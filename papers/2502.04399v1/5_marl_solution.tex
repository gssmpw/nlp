
%\section{PROPOSED SOLUTION: GNN-BASED MARL}
\section{Proposed Solution: GNN-enhanced MARL}
%\xiaoxi{Boken, please carefully read this entire section, and then modify anything that can be enhanced and used in your current implementations.}

In this section, we provide a comprehensive description of the algorithms designed to address the challenges posed by the joint scenarios of ride-hailing vehicle dispatching and crowdsensing. Our algorithm leverages MARL and integrates with other technologies to optimize the QoS as defined in (\ref{max obj}). 

% First, we assume that the joint optimization problem is a Markov decision process (MDP) and introduce the design of state, action, and reward functions in reinforcement learning. After that, we will introduce mean-field multi-agent reinforcement learning to solve the problem of training a large-scale number of agents.
\subsection{Algorithm Overview}
Due to the complexity of joint optimizing decision variables for fleets in urban environments, we employ a decentralized optimization framework. For each time slot, decision variables $\textbf{x}$, $\textbf{y}$ and $\textbf{z}$ can be independently determined by each available vehicle. Optimizing the QoS becomes feasible if the framework can learn the statistical distribution of orders and PoIs while gaining insights into their time sensitivity. The utilization of a distributed optimization framework effectively mitigates the challenge of the decision space rapidly expanding with the size of the vehicle set. The demand for online optimization and distributed decision-making motivates us to use MARL, renowned for its excellent performance in long-term and coupled decision-making \cite{why_use_MARL}. We illustrate the structure of our framework in Fig. \ref{fig: module intro}. Our system is based on the actor-critic MARL algorithm, with each agent making independent decisions. Additionally, we incorporate GNN for raw state embedding. 
% \broken{
Furthermore, the framework integrates the RankTuner module, enabling dynamic adjustment of LoRA ranks to balance fine-tuning accuracy and efficiency. 
% }
In the subsequent sections, we provide a concise overview of key settings such as states, actions, and rewards, followed by an explanation of GNN-based state embedding. 
\begin{figure*}[t]
\centerline{\includegraphics[width=1.0\linewidth]{figure/fig/rank_framework.png}}
\caption{GNN-based MARL framework. It includes the environment, a GNN embedding module for processing raw state information, an actor-critic module for decision-making, a replay buffer for experience storage, and the RankTuner module for dynamically adjusting LoRA ranks to balance fine-tuning accuracy and efficiency. These components work together to enable agents to make independent, informed decisions while optimizing their actions based on the dynamic environment.
% \xiaoxi{Please summarize the names and usages of the key modules and their in-between relationships in a few sentences in this caption.}
}
\label{fig: module intro}
\end{figure*}
\subsection{MARL Statement}
Formally, we model the joint optimization problem as a Markov game, which is represented by a
tuple $(\mathcal{S}, \mathcal{A}, \mathcal{P}, \mathcal{R})$, where $\mathcal{S}$, $\mathcal{A}$, $\mathcal{R}$ and $\mathcal{P}$ are the set of states, actions, rewards, and state transition probability. We define the important components in the MARL framework as follows.
% Since fleet sizes tend to be large, treating a cloud platform as an agent to manage all the vehicles will create a large state space and action space, leading to poor training results and affecting
% the algorithm’s scalability.
% To avoid this problem, we consider each vehicle as an agent and design the algorithm accordingly.
% Each agent is trained centrally and takes actions distributedly.

% Since both ride-haling and for-hire vehicles are managed by the platform or company, and the decision of each vehicle have an impact on the decisions of other vehicles. It is natural to think of a cloud platform as an agent that makes decisions about all vehicles, as most work does. However, since fleet sizes tend to be large, treating a cloud platform as an agent to manage all the vehicles will create a large state space and action space, leading to poor training results and affecting algorithm's scalability. To avoid this problem, we consider each vehicle as an agent and design the algorithm accordingly. Each agent is trained centrally and takes actions distributedly.

% We consider the state transfer of the vehicle as a Markov decision process, represented by a tuple $(\mathcal{S}, \mathcal{O}, \mathcal{A},  \mathcal{P}, \mathcal{R})$, where $\mathcal{S}$, $\mathcal{O}$, $\mathcal{A}$, $\mathcal{R}$ are the set of states, observations, actions and reward, and $\mathcal{P}$ is state transition probability. 

\noindent\textbf{Agent.} We consider a vehicle as an agent. As our objective is to optimize the overall income and data utility of all vehicles, each vehicle can be considered as a homogeneous agent performing cooperative tasks. We still use $\mathcal{M}$ to represent the set of agents.

\noindent\textbf{State space.} Intuitively, a global environmental state should encompass factors such as the distribution of drivers, the distribution of orders, the distribution of PoIs, the current time slot $t$, the generation time and estimated travel time for each order, and the data volume and AoI for each PoI. At the beginning of each time slot $t$, each vehicle $m$ gets a local state correlated with the global environment state $s_t \in \mathcal{S}$, which can be written as ${s_{m,t}}$. The challenge arises in determining whether to aggregate all the relevant factors into a large state space for every agent or to partition the state space into subspaces for each agent. Both approaches prove to be inefficient. Moreover, the continuous changes in the number of orders and PoIs lead to a dynamic change in the state space dimension over time. Selecting a fixed dimension for the state space becomes impractical, posing challenges for implementing the MARL algorithm. To tackle these challenges, we leverage Relational Graph Convolutional Networks (R-GCN) \cite{R-GCN} to encode the features of each agent. The output of R-GCN, denoted as $s^{\prime}_{m,t}$, serves as the embedding state of each agent, integrating the raw state and reducing the raw state dimension to a fixed dimension. The concrete representation of the state space and the detailed process of state embedding will be presented in the subsequent subsection.

% \textbf{Observation space.} 
% At the begin of each time slot $t$, each vehicle $m$ gets a local observation correlated with the environment state $s_t \in \mathcal{S}$, which can be written as ${o_t^m}$. Intuitively, the observation ${o_t^m}$ of vehicle $m$ should include the index of grid $g$ where vehicle $m$ is located, the current time slot $t$, the number of orders to be matched in grid $g$, the generation time and the estimated travel time of each order, the number of uncollected PoIs, the data volume and AoI of each PoI, the number of idle vehicles, and the data collection rate of vehicle $m$. 


% \textbf{Observation space.} At the begin of each time slot, each vehicle $m$ gets a local observation correlated with the environment state $s_t \in \mathcal{S}$, which can be written as $o_t^m = \left\{g_t^m, f_{t}^m, \big |\mathcal{W}_t^{g_t^m} \big |, \big | \mathcal{C}_t^{g_t^m} \big | , t, n\right\}$. In observation $o_t^m$, $g_t^m$ denotes the index of grid that vehicle $n$ is in at time slot $t$; $f_{t}^m$ is a flag bit used to indicate whether vehicle $m$ is serving the order or not, if vehicle $m$ is serving an order then $f_{t}^m$ is 1, otherwise 0; $\big |\mathcal{W}_t^{g_t^m} \big |$ and $\big | \mathcal{C}_t^{g_t^m} \big |$ denotes the number of orders waiting to be served and vehicles which are idle respectively in grid $g_t^m$; $t$ is the time slot and $m$ is the index of the vehicle.

\noindent\textbf{Action space.}
At every time slot, each available agent $m$ takes an action $a_{m,t}$ according to its policy after getting the embedding state $s^{\prime}_{m,t}$. The action $a_{m,t}$ indicates whether the agent should be dispatched to a neighboring grid, remain in the current grid and whether it should accept an order or collect data from a PoI. We denote all agents' joint action as $a_t = \left\{a_{1,t}, a_{2,t}, ..., a_{M,t}\right\}$.


\noindent\textbf{State transition probability.}
Based on the environmental state $s_t$ and the joint action $a_t$ of all agents, the environmental state will transit to the next state $s_{t+1}$ with probability $p(s_{t+1}|s_t, a_t)$.

\noindent\textbf{Reward.}
We calculate an immediate reward for each available agent according to its action. Given the three distinct action types, we formulate distinct reward functions corresponding to each action type.

\begin{itemize}
\item[$\bullet$] If dispatching agent $m$ to a neighboring grid at time slot $t$, we calculate the immediate reward of $m$ as: 
\begin{align}
    r_{m,t} = 0.
\end{align}
In this paper, remaining in the current grid is conceptualized as a special form of dispatching. We assign a reward of 0 for dispatching since dispatching doesn't directly yield rewards, although it can influence subsequent actions. Simultaneously, this is implemented to discourage agents from repeatedly dispatching between specific grids to gain rewards. Despite the reward value being 0 for dispatching, our expectation is that, through MARL, agents can still learn the influence of dispatching on order-serving and data collection, enabling a more effective dispatching policy.
% \begin{align}
% & {\delta}_t^g = \big |\mathcal{W}_t^{g} \big | - \big | \mathcal{C}_t^{g} \big | + b_t^{g},\\
% & r_t^n = max({\delta}_t^{des} - {\delta}_t^{g_t^m}, 0),
% \end{align}
% where $des$ represents the index of the destination grid of vehicle $n$. From (14) and (15), it can be seen that the agent will receive a positive reward when the sum of the supply-demand gap and the amount of data at the destination is larger than those at the origin. 
\end{itemize}

\begin{itemize}
\item[$\bullet$] If agent $m$ decides to accept an order at time slot $t$, the reward of $m$ is written as: 
\begin{align}
& r_{m,t} = \alpha \cdot \sigma(o_t^m),
\end{align}
where $o_t^m$ represents the order that be accepted by $m$, and $\sigma(o_t^m)$ represents the price of $o_t^m$. Here, $\alpha$ is a weight, which is the same as in expression (\ref{equ: qos def}).
\end{itemize}

\begin{itemize}
\item[$\bullet$] 
% \broken{
If agent $m$ decides to collect data from a PoI at time slot $t$, the reward of $m$ is written as: 
\begin{align}
& r_{m,t} = \beta u_t^{p_t^m} = \beta f_k(d_t^{p_{t,k}^m},{\lambda}_t^{{p_{t,k}^m}}).
\end{align}
Here, $r_{m,t}$ equals the utility of the data collected by $m$, and $p_t^m$ is the PoI collected by agent $m$ at time slot $t$ with task $k$. $\beta$ is also a weight like $\alpha$. The function \( f_k(d, \lambda) \) encapsulates the fine-tuning accuracy under the combined influence of two key factors: \( d \), the amount of collected data, and \( \lambda \), the degree of data freshness (also referred to as AoI). The subscript \( k \) indicates the task-specific nature of the function, as different tasks may exhibit unique dependencies on data quantity and freshness. This function quantifies how the accuracy of the fine-tuned model varies based on these two variables. As both data quantity and freshness directly impact the utility function, the interplay between these variables determines the model’s fine-tuning performance, with \( f(d, \lambda) \) capturing this dependency in a nuanced and task-specific manner.
% }
\end{itemize}

%（订单、数据。aoi对reward的随时间的contribution图）
%（添加新的方法，新的技术）

%In the reinforcement learning setup above, we consider each vehicle as an agent. Using algorithms based on the Centralized training with decentralized execution (CTDE) architecture, such as MAPPO, each agent can make independent decisions. However, in practice, the size of the fleet in an area is often more than dozens, or even hundreds. In a system with hundreds of agents, it is often difficult for ordinary multi-agent reinforcement learning to achieve satisfactory results because the actions of each agent may interfere with other agents. Learning for efficient coordination in large-scale multi-agent systems suffers from the problem of the curse of dimensionality due to the exponential growth of agent interactions. Mean-Field (MF)-based methods address this issue by transforming the interactions within the whole system into a single agent played with the average effect of its neighbors \cite{AAAI23}.
\subsection{State Embedding}
Reviewing the structure and influencing factors of our optimization problem, the arrival of ride orders and the generation of PoIs follow a prior but unknown distribution. To ensure adaptability to the dynamic and highly stochastic environment, each agent requires a policy network with high generalization, utilizing ample state information to formulate decisions. These decisions aim to optimize the overall QoS throughout numerous time slots from a long-term perspective. Therefore, continuous monitoring of the states of vehicles, orders, and PoIs is crucial for each agent. First, we define the features of raw state for the agent $m$, namely $s_{m,t} = \left\{ \mathcal{I}_{m,t}, \mathcal{U}_{m,t}, v_{m,t}, h_{m,t} \right\}$, serving as the input of RGCN and is formalized as follows.

\begin{itemize}
\item[$\bullet$] 
\textbf{Order feature set $\mathcal{I}_{m,t}$} contains some entities $i_{m,t} \in \mathcal{I}_{m,t}$, each of which represents the states of a order. Specifically, $i_{m,t}$ is the concatenation of the price, generation time, estimated travel time, origin, and destination of an order within the grid where $m$ is located. The feature set $\mathcal{I}_{m,t}$ not only provides information on the number and price of orders within the grid but also indicates the impact of orders on vehicle distribution due to their different destinations and travel times.
\end{itemize}

\begin{itemize}
\item[$\bullet$] 
\textbf{PoI feature set ${\mathcal{U}}_{m,t}$} is a set contains entities $u_{m,t} \in \mathcal{U}_{m,t}$. Each element $u_{m,t}$ represents the data volume, AoI, and generation location of a PoI within the grid where $m$ is located. By capturing the data volume and AoI of each PoI, the set ${\mathcal{P}}_{m,t}$ effectively characterizes the present data utility associated with individual PoIs.
\end{itemize}

\begin{itemize}
\item[$\bullet$] 
\textbf{Vehicle feature vector $v_{m,t}$} represents the location, working states (serving orders, collecting data or idle), current time slot, and index of agent $m$ (using one-hot encoding). 
\end{itemize}

\begin{itemize}
\item[$\bullet$] 
\textbf{Grid feature vector $h_{m,t}$} contains the number of orders, available vehicles, and PoIs in the current grid where the agent $m$ is located. To distinguish different grids, $h_{m,t}$ includes the grid index encoded by one-hot. Grid feature reflects the distribution of orders, vehicles, and PoIs in the environment.
\end{itemize}

In our MARL framework, if each agent $m$ can only observe the local features $\left\{ \mathcal{I}_{m,t}, \mathcal{U}_{m,t}, v_{m,t}, h_{m,t}\right\}$, there is an increased susceptibility to becoming ensnared in local optima \cite{liyihong}. A global representation of features is thus needed. However, a straightforward concatenation of all $s_{m,t}$ across all agents into a global state would result in a rapid expansion of the agent's input dimension, potentially compromising algorithm performance. In addition, since the size of the order set and PoI set are not fixed, we cannot specify a fixed input dimension for the agent. We try to use GNN to solve these problems. In our approach, we embed graph neural networks into the agent's policy network, enhancing the fusion and interaction between the agent's state and the environment's state. Below we define the topology graph used by GNN.

\begin{figure}[t]
\centering  %图片全局居中
\subfigure[Vehicles, orders, and PoIs in grids $g_1$, $g_2$, $g_3$ and $g_4$.]{
\label{fig: sub.1 distribution}
\includegraphics[width=0.3\textwidth]{figure/fig/grid.png}}
\subfigure[Topology graph based on (a).]{
\label{fig: sub.2 topo}
\includegraphics[width=0.32\textwidth]{figure/topology_example2}}
\caption{An example of constructing a topology graph.}
\label{fig: example of topo}
\end{figure}

\noindent\textbf{Definition 4. (Topology Graph.)} Due to the advantages of GNN in topology-based node information transferring and information processing, we represent vehicles, orders, PoIs, and grids as nodes within a topology graph. The topology graph is used to describe the relationship between urban states and vehicles. Our topology graph is $Gr(\mathcal{N}_t,\mathcal{E}_t)$, where the node set $\mathcal{N}_t$ consists of order nodes $\mathcal{O}_t$, PoI nodes $\mathcal{P}_t$, grid nodes $\mathcal{G}$, vehicle nodes $\mathcal{M}$, and a shortcut node. We abuse the notation of the order, PoI, grid, and vehicle indices to denote the corresponding nodes as well. $\mathcal{E}_t$ is the set of edges formed by connecting nodes, and its definition is as follows. As shown in Fig. \ref{fig: example of topo}, a vehicle node $m \in \mathcal{M}$ connects with a grid node $g \in \mathcal{G}$ only if vehicle $m$ is within the range of gird $g$. If the generation location of order $o \in \mathcal{O}_t$ is grid $g$, node $o$ is connected to node $g$. Similarly, PoI nodes are interconnected with grid nodes using a similar criterion. Grid node $g$ is only connected to the grid nodes which are its neighboring grid. Specifically, a shortcut node is introduced, linked to each grid node, expediting the information propagation within the graph neural network. Through the connection of grid nodes, we can capture the connectivity and topological structure between grids. Meanwhile, GNN facilitates each vehicle node's awareness of the features associated with orders and PoIs.

The topology graph contains multiple types of nodes, so we need to distinguish between different types of nodes. Since the R-GCN model can process topology graphs with different types of nodes and generate embedding information for the nodes, we use R-GCN to generate the state representation for agents. 
The information propagation is that each node $n \in \mathcal{N}_t$ passes the features as messages to its neighboring node and $n$ aggregates the features which are from its neighbors. The aggregation method and update steps of our R-GCN follow the steps in \cite{R-GCN}. The propagation model of our R-GCN is described as:
\begin{align}
    & {Gr}_{(0)}^{shortcut} = \textbf{0} \\
    & {Gr}_{(0)}^{\mathcal{M}} = \cup_{m = 1}^M v_{m,t}  \\
    & {Gr}_{(0)}^{\mathcal{G}} = \cup_{m = 1}^M h_{m,t}  \\
    & {Gr}_{(0)}^{\mathcal{O}} = \cup_{m = 1}^M \mathcal{I}_{m,t} \\
    & {Gr}_{(0)}^{\mathcal{P}} = \cup_{m = 1}^M \mathcal{U}_{m,t} 
\end{align}

The initial input ${Gr}_{(0)}$ is written as: 
\begin{align}
    {Gr}_{(0)} = \left\{ {Gr}_{(0)}^{shortcut}, {Gr}_{(0)}^{\mathcal{M}}, {Gr}_{(0)}^{\mathcal{G}}, {Gr}_{(0)}^{\mathcal{O}},{Gr}_{(0)}^{\mathcal{P}} \right \}.
\end{align} 

The node embedding is propagated in each layer $l$, i.e., ${Gr_{(l)}} = f({Gr_{(l-1)}})$, where $f(\cdot)$ represents the graph convolution network aggregating the features of each node with its neighbors. After $L$ layers of graph message passing, we get the final graph embedding $Gr_{(L-1)}$. We then map $Gr^{\mathcal{M}}_{(L- 1)}$ to the corresponding agents as the input of their actor networks.
\begin{table}[ht]
\centering
\caption{Impact of Rank on Fine-Tuning Accuracy and Time}
\resizebox{\linewidth}{!}{
\begin{tabular}{|c|c|c|}
\hline
\textbf{Rank} & \textbf{Fine-Tune Time (Normalized)} & \textbf{Accuracy Factor} \\
\hline
1 & 1.00  & 0.70     \\
2 & 1.45  & 0.76     \\
3 & 3.71  & 0.98     \\
4 & 5.05  & 0.99     \\
5 & 6.00  & 0.99365  \\
6 & 5.31  & 1.00     \\
\hline
\end{tabular}
}
\label{tab:rank}
\end{table}
\subsection{Heuristic-based Rank Selection Integrated with MARL}
% \broken{
Since LoRA is employed for task fine-tuning, selecting an appropriate rank is crucial. According to relevant studies~\cite{bai2024federated}, a larger rank generally leads to better fine-tuning accuracy but at the cost of increased fine-tuning time. An interesting and important trade-off here is that if choosing a larger rank greedily for obtaining a higher accuracy per task, the longer fine-tuning time will very likely reduce the expected total number of fine-tuning tasks that a vehicle can accomplish in the entire time span. Therefore, the best rank of a fine-tuning adaptor should be carefully chosen to achieve the total utility. 

Our experiments, as summarized in Table~\ref{tab:rank}, show that the choice of rank significantly impacts both accuracy (in terms of accuracy discounts compared to the best accuracy) and fine-tuning time across different tasks, such as image classification and image segmentation. To address this, we propose the RankTuner, a dynamic adjustment mechanism designed to determine the most suitable rank for fine-tuning when the optimal rank is unknown. The RankTuner operates as follows: Initially, a rank is randomly selected as the benchmark. During each iteration, the algorithm compares the current ADU to the previous round. If the ADU improves, the algorithm maintains the current direction (increasing or decreasing the rank) to further explore better settings. If the ADU decreases, the algorithm reverts to the previous rank and switches the direction. The rank is constrained within a predefined allowable range to ensure feasibility. The pseudocode for the RankTuner is presented in Algorithm~\ref{alg:rank_tuner}.
% }

\begin{algorithm}
\caption{A Rank Selection Algorithm (RankTuner)}
\label{alg:rank_tuner}
\begin{algorithmic}[1]
\State \textbf{Input:} Allowed rank range $[\eta_{\text{min}}, \eta_{\text{max}}]$, initial rank $\eta_0$, initial direction $d$ (\texttt{+1} or \texttt{-1}).
\State Initialize $\eta \leftarrow \eta_0$, $d \leftarrow \texttt{+1}$, previous ADU $\text{ADU}_{\text{prev}} \leftarrow 0$.
\For{each fine-tuning iteration}
    \State Fine-tune model with rank $rank$ and compute current ADU $\text{ADU}_{\text{curr}}$.
    \If{$\text{ADU}_{\text{curr}} > \text{ADU}_{\text{prev}}$}
        \State $\eta \leftarrow \eta + d$ {\Comment{Keep direction and adjust rank.}}
    \Else
        \State $d \leftarrow -d$ {\Comment{Reverse direction.}}
        \State $\eta \leftarrow \eta + d$ {\Comment{Revert to previous rank.}}
    \EndIf
    \State $\eta \leftarrow \max(\eta_{\text{min}}, \min(\eta, \eta_{\text{max}}))$ 
    \State Update $\text{ADU}_{\text{prev}} \leftarrow \text{ADU}_{\text{curr}}$.
\EndFor
\end{algorithmic}
\end{algorithm}

The RankTuner 
% \xiaoxi{We don't commonly use separated words to name an algorithm. Please see the revised wording of the algorithm title. I changed the name to RankTuner. I also revised the title of Section IV-D.}
effectively balances fine-tuning accuracy and time efficiency by dynamically adapting the rank based on real-time feedback. This mechanism ensures that the fine-tuning process remains efficient and yields high-quality results across various tasks and environmental conditions.



\subsection{Training}
% \begin{algorithm}[t]
% \caption{Pseudocode: GNN-MAPPO} 
% \label{alg1} \begin{algorithmic}
% \STATE Initialize policy network $\pi_\theta$ and value network $V_\phi$
% \STATE Initialize a memory buffer $D$
% \FOR{episode $i$ = $1$, $2$, $\dots$, $max\underline{~}episode$}
% \STATE Initialize environment
% \FOR{time-slot $t$ = $1$, $2$, $\dots$, $T$}
% \STATE Construct topology graph $Gra(\mathcal{N}_t, \mathcal{E}_t)$
% \FOR{for agent $m$ = $1$, $2$, $\dots$, $M$}
% % \STATE Agent $m$ get its raw state  $s_{m,t}$
% \STATE Input raw state $s_{m,t}$ into RGCN and output the embedding state $s^{'}_{m,t}$
% \STATE Agent $m$ executes action according to $\pi_\theta(a_{m,t} |s^{'}_{m,t})$
% \STATE Get the reward $r_{m,t}$
% \ENDFOR
% \STATE Get the next state $s_{t+1}$
% \ENDFOR
% \ENDFOR
 
		
% 	\end{algorithmic} 
% \end{algorithm}
Our MARL model is based on multi-agent proximal policy optimization (MAPPO) \cite{neurips22}. 
% PPO is a stable on-policy learning algorithm that can handle the large combinatorial action space. We treat agents as homogeneous agents (agents have
% identical state and action spaces), so all agents share actor and critical network parameters during the training process.
Each agent has a flag bit to indicate whether the agent is available or not. We ignore the output of the action by non-available agents. Due to the fewer dispatching destinations in the boundary grids than in the non-boundary grids, we mask the corresponding dispatching action for agents in boundary grids. For every policy update step, we collect a batch of trajectories from the environment and compute the loss function according to \cite{PPO}. Our MARL model is trained online because online training has a higher utilization rate of sampled data. 
% Agents produce an action $a^i$ \tianxiang{what is i} from the state $s^i$ to jointly optimize the discounted accumulated reward $J(\theta) = \mathbb{E}_{A_t, S_t}({\sum \limits^{T}_{t}} \gamma^t R(S_t, A_t))$ by using a policy ${\pi}_\theta (a^i
% |s^i)$ parameterized by $\theta$, where $R(S_t, A_t)$ denotes the shared reward function. 
We apply some implementation techniques, including Generalized Advantage Estimation (GAE) with advantage normalization and value-clipping. 


% \subsection{Mean Field MARL}
% In the vehicles dispatching and VSC task, agents interact with other agents by choosing when to dispatch, take orders, or collect data. Obviously, the actions made by each agent affect the state of the environment. In terms of order taking, when many vehicles are dispatched to the same target area, the ratio of idle vehicles to orders (or the ratio of supply and demand) in that area will become larger, resulting in many arriving vehicles not being able to successfully receive orders. For VCS tasks, the amount of data that needs to be collected in PoI decreases with the collection of vehicles. When a group of vehicles choose to collect data, there may be no more available data to collect. In traditional multi-agent reinforcement learning, in order to simulate the interaction and coordinate the action, the joint action of all agents and the environmental state are required to calculate the state action value, i.e., $Q(s_t,a_t)$, where $a_t=\left\{a_t^1, a_t^2, ..., a_t^M\right\}$. Since each vehicle is regarded as an agent in our reinforcement learning scenario setting, when the number of agents expands, the expansion of the joint action space will follow, which brings difficulties to the coordination of agents. Learning for efficient coordination in large-scale multi-agent systems suffers from the problem of the curse of dimensionality due to the exponential growth of agent interactions. 
% \begin{figure}[t]
% \centerline{\includegraphics[width=1.0\linewidth]{figure/marl_algorithm.png}}
% \caption{Mean Field RL}
% \label{fig}
% \end{figure}

% Mean-Field (MF)-based methods address this issue by transforming the interactions within the whole system into a single agent played with the average effect of its neighbors \cite{AAAI23}.
% Since vehicles are entities with specific locations on the map, it is natural to define the neighbor vehicles of vehicle $m$ as a set of vehicles which is in the current grid and the neighboring grid, and we denote this set as ${nei}_t^m$. The actions of each agent are encoded with one-hot codes in the discrete action space. The state of the simulation environment is $s^t = \left\{o^t_1, o^t_2, ..., o^t_m\right\}$, i.e., the splicing of the observations of each vehicle. We calculate the mean action $\hat{a}^t_m$ based on the neighborhood ${nei}_t^m$ of agent m as
% \begin{align}
%     \hat{a}^t_m = \frac{1}{\big |{{nei}_t^n} \big |} \sum\limits^{}_{k \in {{nei}_t^n}} a^t_k,
% \end{align}
% and the state-action value of each agent is written as 
% \begin{align}
%     Q_t^m(s_t,a_t) =  Q_t^m(s_t, a_t^m, \hat{a}_t^m).
% \end{align}
% The actor is trained by the sampled policy gradient:
% \begin{align}
% \nabla_{{\theta}_i}\mathcal{J}({\theta}_i) = \nabla_{{\theta}_i}log {\pi}_{{\theta}_i}(s)Q_{{\phi}^i}(s, a_i,\hat{a}_i) \big|_{a=\pi_{{\theta}_i}(s)},
% \end{align}
% and update the parameter of the critic network by minimizing the loss $\mathscr{L}$ as follows:
% \begin{align}
% \mathcal{L}({\phi}_i) = \sum{(y_i - Q_{{\phi}^i}(s, a_i, \hat{a}_j))}^2 \\
% y_i = r_i + \gamma {V_{{\phi}_i}}^{MF}(s),
% \end{align}
% where $\theta_i$, $\phi_i$, ${V_{{\phi}_i}}^{MF}(s)$ is the parameter of actor network, the parameter of critic network and mean field value function.
%{Mean-Field (MF)-based methods address this issue by transforming the interactions within the whole system into a single agent played with the average effect of its neighbors \cite{AAAI23}. 由于车辆是在地图上有具体位置的实体，我们很自然地将每辆车的邻居车辆定义为在目前区域以及邻居区域的车辆的集合,将这个集合记作${nei}_t^n$. 在离散的动作空间中用one-hot码编码每个智能体的动作，我们将时隙t时刻所有车辆的联合动作表示为$a^t = {a^t_1, a^t_2, ..., a^t_N}$。环境的状态为$s^t = {o^t_1, o^t_2, ..., a^t_N}$,即每辆车的observation的拼接。We calculate the mean action $a^t_i$ based on the neighborhood ${nei}_t^n$ of agent i as}
%{
%\begin{align}
%& a^t_i = \frac{1}{N^i} \sum,
%\end{align}
%}

%（图进行修改，状态包括什么，数据utility，三种动作用文字描述，文字用框框住，图中体现提到的指标）
%(还可以画出设置数据分布示意图）




