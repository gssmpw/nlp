\section{Evaluation}
In this section, we assess the efficacy of the proposed methodology through a comprehensive evaluation. We conducted experiments based on real-world datasets of orders and compared the results with several comparison algorithms. The experimental results show that our method has better performance.
%(可以加数值实验，画小规模拓扑结构，实验结果，我们的实验的选择和最优的对比）

\subsection{Experiment Settings}

\noindent\textbf{Model configurations.} Our algorithm is implemented by PyTorch and DGL. The actor network of each agent is a three-layer fully connected network, where the size of the hidden layer is set to 64. The structure of the Critic network is the same as that of the actor network. The hidden layer dimension of R-GCN is 128, and the output embedding state dimension is 10. To speed up the training process, we adopted a parameter-sharing method commonly used in multi-agent training, which can reduce the computationalas overhead in experiments.

% \broken{
\noindent\textbf{UFM fine-tuning task configurations.} 
In the experimental settings, we consider three representative fine-tuning tasks, each designed to evaluate the effectiveness of our approach across diverse model architectures and datasets:  
\begin{itemize}
\item[$\bullet$] Image Classification Task: This task leverages the ViT pre-trained model, fine-tuned on the CIFAR-100 dataset~\cite{krizhevsky2009learning} using the LoRA technique.
\end{itemize}
\begin{itemize}
\item[$\bullet$] Image Segmentation Task: For image segmentation, we employ the SAM pre-trained model, fine-tuned on a satellite imagery dataset~\cite{ji2018fully} using LoRA.
\end{itemize}
\begin{itemize}
\item[$\bullet$] Object Detection Task: This task utilizes the YOLOv7~\cite{wang2023yolov7} model, fine-tuned on a vehicle detection dataset~\cite{vehicle-detection-nmzlp_dataset}. The dataset contains annotated images of vehicles in various environments, emphasizing diverse perspectives, sizes, and contexts. As YOLO fine-tuning involves full fine-tuning, rank selection is not applicable, and RankTuner is not utilized for this task.
\end{itemize}
Each fine-tuning configuration was carefully crafted to reflect real-world scenarios, emphasizing the utility of our framework in handling diverse data modalities and model architectures while maintaining computational efficiency.
% }


% \xiaoxi{\noindent\textbf{UFM fine-tuning task configurations.} Boken, please describe the model, dataset, and any experimental information about each model fine-tuning task.}

\noindent\textbf{Order serving dataset.}
The detailed information of the ride order settings comes from the New York City Taxi dataset \cite{newyork_taxi}. The real-world dataset records the travel data of taxis in New York, including the latitude and longitude of passengers boarding and alighting, the start and end time of each order, the fare paid by passengers, and so on. 

\noindent\textbf{Simulator design.}
% We implemented a simulator that simulates ride-hailing vehicles dispatching and crowdsensing. 
At the beginning of each time slot, the simulator dynamically generates ride requests and PoIs. The data volume associated with each PoI is generated by random sampling within a range of 3 to 12 packages, while we manually construct a probability distribution for the PoIs. We have defined three distinct probability distributions for PoIs, labeled as distribution 1, distribution 2, and distribution 3. The distribution of orders and PoI is shown in Fig. \ref{fig: dis of order and poi}. Distribution 1 exhibits a significant disparity between the distributions of PoIs and ride orders. As for distribution 2, PoIs follow a distribution similar to that of orders. Distribution 3 indicates a uniform distribution of PoIs within the target area. Our fleet includes 100 ride-hailing vehicles, each with a set data collection rate of 1 package per time slot. To simplify the problem, we assume that the vehicle can reach the dispatch destination after one time slot like most research does \cite{KDD18, assume_cost_one_time_slot_to_reach_dispatched_destination}. We set that if an order is not accepted by a vehicle within 15 time slots from the time it is generated, the order will be invalid. 
\begin{figure}[t]
\centering  %图片全局居中
\subfigure[Distribution of orders.]{
\label{fig: sub.1 order distribution}
\includegraphics[width=0.23\textwidth]{figure/fig/order_dis.png} }
\hfil
\subfigure[Distribution 1 of PoIs]{
\label{fig: sub.2 poi distribution 1}
\includegraphics[width=0.23\textwidth]{figure/fig/poi_dis1.png}} 

\subfigure[Distribution 2 of PoIs]{
\label{fig: sub.2 poi distribution 2}
\includegraphics[width=0.23\textwidth]{figure/fig/poi_dis2.png}}
\hfil
\subfigure[Distribution 3 of PoIs]{
\label{fig: sub.2 poi distribution 3}
\includegraphics[width=0.23\textwidth]{figure/fig/poi_dis3.png}} 

\caption{
% \broken{
The distribution probability of orders and PoIs in the target area. Darker grid colors indicate higher probabilities of generating orders or PoIs. PoIs are shown as points, with colors representing task types: red for image classification and purple for image segmentation. Point sizes are proportional to data volumes, reflecting task intensity.}
% }
\label{fig: dis of order and poi}
\end{figure}

\begin{figure}[t]
\centerline{\includegraphics[width=1.0\linewidth]{figure/dis_reward.png}}
\caption{Different results of different distributions. }
% \xiaoxi{Put this figure and its text explanation right below Figure 6.}}
\label{different result of different distribution}
\end{figure}

% \begin{figure*}[t]
%   \centering
%   \includegraphics[width=0.23\textwidth]{figure/qos.png}
%     \caption{QoS (Reward) comparison with baselines}
%     \label{chutian1}%文中引用该图片代号
%   \includegraphics[width=0.23\textwidth]{figure/ADI.png}
%     \caption{ADI comparison with baselines}
%     \label{chutian2}%文中引用该图片代号
%   \includegraphics[width=0.23\textwidth]{figure/ADU.png}
%     \caption{ADU comparison with baselines}
%     \label{chutian3}%文中引用该图片代号
%   \includegraphics[width=0.23\textwidth]{figure/AOI.png}
%     \caption{AoI}
%     \label{chutian4}%文中引用该图片代号
%   \caption{Caption for your images.}
%   \label{fig:four_images}
% \end{figure*}

% \begin{figure*}[t]
% 	\centering
% 	\begin{minipage}{0.25\linewidth}
% 		\centering
% 		\includegraphics[width=0.9\linewidth]{figure/qos.png}
% 		\caption{QoS (Reward) comparison with baselines}
% 		\label{chutian1}%文中引用该图片代号
% 	\end{minipage}
% 	\begin{minipage}{0.25\linewidth}
% 		\centering
% 		\includegraphics[width=0.9\linewidth]{figure/ADI.png}
% 		\caption{ADI comparison with baselines}
% 		\label{chutian2}%文中引用该图片代号
% 	\end{minipage}
% 	%\qquad
% 	%让图片换行，
	
% 	\begin{minipage}{0.25\linewidth}
% 		\centering
% 		\includegraphics[width=0.9\linewidth]{figure/ADU.png}
% 		\caption{ADU comparison with baselines}
% 		\label{chutian3}%文中引用该图片代号
% 	\end{minipage}
% 	\begin{minipage}{0.25\linewidth}
% 		\centering
% 		\includegraphics[width=0.9\linewidth]{figure/AOI.png}
% 		\caption{AoI}
% 		\label{chutian4}%文中引用该图片代号
% 	\end{minipage}
% \end{figure*}

% \broken{\noindent\textbf{Rank selection strategy.}
% In our experiments, we evaluate two alternative rank selection strategies to compare their effectiveness in fine-tuning tasks. \xiaoxi{The main problem is that Ours does not show advantages with Thompson Sampling? You need to try to summerzie any advantages of Ours in Fig.~\ref{fig:rank}. Usually, the common perspectives can range from the optimality (which seems similar, in this figure, between Ours and Thompson Sampling), or computation complexity, or cost in getting data to run the algorithm, etc.} 
% \begin{itemize}
% \item[$\bullet$] 
% Greedy: It adjusts the LoRA rank by comparing the ADU of the current step with the previous step. If the ADU improves, the rank is further adjusted in the same direction; otherwise, no change is made.
% \end{itemize}

% \begin{itemize}
% \item[$\bullet$] 
% Thompson sampling~\cite{chapelle2011empirical}: This strategy employs a probabilistic method to balance exploration and exploitation. By sampling ranks according to their estimated likelihood of enhancing ADU, it facilitates broader exploration of potential rank configurations.
% \end{itemize}}

% \xiaoxi{Here, you just show itemized baselines of Rank Selection, and then it directly transitions to the following ``Baselines'' without any explanation -- readers might get confused. I think you should put the entire paragraph starting with ``Rank selection strategy'' and the two baselines into subsection C ``Effect of Rank Tuner''.}

\noindent\textbf{Baselines.}
We consider the following baselines for our algorithm to compare.
\begin{itemize}
\item[$\bullet$] 
Random: The vehicle randomly selects actions from a set of actionable actions based on the grid it is currently in.
\end{itemize}

\begin{itemize}
\item[$\bullet$] 
Greedy-in-OS (with OS representing order serving): If there are orders distributed in the grid, available vehicles will choose to prioritize accepting orders. If there are no orders available, vehicles will go to other grids, collect data or stay.
\end{itemize}

\begin{itemize}
\item[$\bullet$] 
Greedy-in-FT (with FT representing fine-tuning): When fine-tuning tasks are associated with PoIs within a grid, available vehicles prioritize data collection from these PoIs. In the absence of such PoIs, vehicles adapt by either traveling to adjacent grids, fulfilling passenger orders, or remaining stationary.
\end{itemize}

\begin{itemize}
\item[$\bullet$] 
Multi-armed bandits (MAB): For any action vector, MAB uses the sum of the empirical average reward and an upper-confidence bound as the final reward function \cite{UCB}. 
\end{itemize}

\begin{itemize}
\item[$\bullet$] 
IQL: IQL extends DQN to a decentralized multi-agent reinforcement learning environment, with the ability to handle high-dimensional and complex environments.
% A vanilla MAPPO algorithm without mean-field. The state-action value $Q(s, a)$ is calculated by stitching together the state and action of each vehicle.
\end{itemize}

% \begin{itemize}
% \item[$\bullet$] 
% DQN. We use the single-agent algorithm DQN as one of the baselines. The input of the DQN policy network is the splicing of the states of all vehicles, and the output is the action of each vehicle.
% \end{itemize}

% All the experiments are run on a workstation equipped with one Intel(R) Core(TM) i9-12900K CPU and 32GB RAM. 
Our performance evaluation metrics include 1) QoS, which reflects the overall performance of our framework. 2) ADI, which reflects the quality of order-serving, 3) ADU, which reflects the quality of data collection and 4) the average AoI, which is the average of the AoI of the collected PoIs and reflects the overall data freshness.

\subsection{Performance}
We analyze the experimental results as follows. The distribution of PoI is distribution 1.

1) QoS: We demonstrate the QoS of our method and the other 5 baselines during the training in Fig. \ref{QoS}. The experimental results indicate that our framework outperforms all baselines in terms of QoS. Although we lead to GNN, which increases the complexity of the model to some extent, our algorithm still maintains satisfactory convergence. MAB does not consider the state of the environment and the results of each interaction are independent of past actions, causing poor QoS performance in complex environments. Under the IQL algorithm, agents that make independent decisions based on local states are difficult to adapt to environments with high randomness and complexity, resulting in their inability to learn better decisions.


\begin{figure*}[htbp] %这里使用的是强制位置，除非真的放不下，不然就是写在哪里图就放在哪里，不会乱动
	\centering  %图片全局居中
	\vspace{-0.35cm} %设置与上面正文的距离
	\subfigtopskip=2pt %设置子图与上面正文或别的内容的距离
	\subfigbottomskip=2pt %设置第二行子图与第一行子图的距离，即下面的头与上面的脚的距离
	\subfigcapskip=-5pt %设置子图与子标题之间的距离
	\subfigure[QoS comparison with baselines.]{
		\label{QoS}
		\includegraphics[width=0.32\linewidth]{figure/QoS.png}}
	\subfigure[ADI comparison with baselines.]{
		\label{ADI}
		\includegraphics[width=0.32\linewidth]{figure/adi.png}}%这里是空了一行，能够实现强制将四张图分成两行两列显示，而不是放不下图了再换行，使用\\也行。
        \subfigure[ADU comparison with baselines.]{
		\label{ADU}
		\includegraphics[width=0.32\linewidth]{figure/adu.png}}
	\caption{Performance comparison with baselines using Distribution 1 of the data}
	\label{contrast}
\end{figure*}


\begin{figure}[t]
\centerline{\includegraphics[width=0.7\linewidth]{figure/fig/rank_comparison.png}}
\caption{Rank Variation Over Training Steps.
% \xiaoxi{Make sure the color style, the fontsize of labels and legends of this figure is consistent with others, e.g., Fig. 8.}
}
\label{fig:rank}
\end{figure}

% \begin{table}[ht]
% \centering
% \caption{QoS Comparison of Different Rank Strategies \xiaoxi{Please make the displayed fontsize of this table consistent with that in Table V. }}
% \label{tab:qos_comparison}
% \begin{tabular}{|c|c|}
% \hline
% \footnotesize
% \textbf{Method} & \textbf{QoS} \\ \hline
% Ours            & 72.78        \\ \hline
% Greedy          & 68.93        \\ \hline
% Thompson Sampling & 63.49      \\ \hline
% \end{tabular}
% \end{table}
\begin{table}[ht]
\centering
\caption{QoS Comparison of Different Rank Strategies}
\label{tab:qos_comparison}
\resizebox{0.61\linewidth}{!}{ 
\begin{tabular}{|c|c|}
\hline
\textbf{Method} & \textbf{QoS} \\ \hline
Ours            & 72.78        \\ \hline
Greedy          & 68.93        \\ \hline
Thompson Sampling & 63.49      \\ \hline
\end{tabular}
}
\end{table}


2) ADI: 
We recorded the ADI during the training process and displayed it in Fig. \ref{ADI}. As the number of training steps increases, the ADI obtained based on our algorithm continues to rise and eventually converges around 2.5 million steps. In contrast to the baseline algorithms, our algorithm achieves the maximum income. The ADI obtained by Greedy-in-OS is higher than that based on Greedy-in-FT, which is in line with our expectations because if the priority of accepting orders is higher, the frequency of vehicles choosing to accept orders will be higher compared to collecting data. Our algorithm not only learns the distribution of orders and schedules vehicles based on it but also selects the best time to accept orders, which makes our method superior to these baselines in order income.
% \begin{figure}[t]
% \centerline{\includegraphics[width=0.75\linewidth]{figure/ADI.png}}
% \caption{ADI comparison with baselines}
% \label{ADI}
% \end{figure}
\begin{table}[ht]
\centering
\caption{The average accuracy of different methods.
}
\resizebox{0.61\linewidth}{!}{
\begin{tabular}{|c|c|}
\hline
\textbf{Method} & \textbf{Accuracy(\%)} \\
\hline
Ours   & 69.03 \\
Random & 60.77 \\
Greedy-in-FT    & 62.97 \\
Greedy-in-OS  & 60.14 \\
IQL    & 66.48 \\
MAB    & 65.59 \\
\hline
\end{tabular}
}

\label{tab:accuracy_methods}
\end{table}
\begin{table*}[ht]
\centering
\caption{Performance on Multiple Tasks (Image Classification, Object Detection, Image Segmentation)}
\resizebox{\linewidth}{!}{
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|}
\hline
\textbf{Method} & \multicolumn{3}{c|}{\textbf{One Task}} & \multicolumn{3}{c|}{\textbf{Two Tasks}} & \multicolumn{3}{c|}{\textbf{Three Tasks}} \\
\cline{2-10}
                & \textbf{QoS} & \textbf{ADI} & \textbf{ADU} & \textbf{QoS} & \textbf{ADI} & \textbf{ADU} & \textbf{QoS} & \textbf{ADI} & \textbf{ADU} \\
\hline
\textbf{Ours}   & \textbf{66.24} & \textbf{2,834.13} & \textbf{8,474.18} & \textbf{78.57} & \textbf{2,943.96} & \textbf{11,914.06} & \textbf{67.97} & \textbf{2,896.53} & \textbf{8,747.41} \\
Random          & 30.07         & 1,009.86         & 5,081.45          & 31.35          & 1,014.28         & 5,469.35          & 30.75          & 1,009.86         & 5,300.96          \\
greedy-in-FT    & 36.92          & 965.66          & 7,465.04          & 43.78          & 993.40          & 9,527.69          & 37.74          & 1,000.57          & 7,570.97          \\
greedy-in-OS    & 47.49          & 2,437.77         & 4,262.12          & 49.16          & 2,540.21         & 4,338.30          & 48.02          & 2,437.77         & 4,432.99          \\
IQL             & 26.57          & 733.97          & 5,199.41          & 27.89          & 650.55          & 5,680.78          & 26.58          & 649.82          & 5,578.69          \\
MAB             & 34.19          & 784.63          & 7,402.70          & 33.83          & 571.07          & 8,241.65          & 34.54          & 683.61          & 7,965.09          \\
\hline
\end{tabular}
}
\label{tab:task scalability}
\end{table*}
3) ADU: According to Fig. \ref{ADU}, our algorithm also achieved the best performance in data collection. Although the MAB algorithm also has good performance in ADU, combined with Fig. \ref{ADI} and Fig. \ref{ADU}, it doesn't learn the effects of accepting orders and is overly focused on data collection. As the training progresses, the ADU of IQL continuously increases and converges to a local optimum.

% \begin{figure}[htbp]
% \centerline{\includegraphics[width=0.75\linewidth]{figure/ADU.png}}
% \caption{ADU comparison with baselines}
% \label{ADU}
% \end{figure}
4) 
% \broken{
The average accuracy:
To assess the impact of data freshness and quantity on model fine-tuning, we evaluate the inference accuracy achieved on the test set after fine-tuning, as shown in Table \ref{tab:accuracy_methods}. Our approach achieves the highest accuracy, highlighting the advantages of effectively managing both data freshness and volume during the fine-tuning process. In comparison, baseline methods such as Random, Greedy-in-FT, and Greedy-in-OS strategies exhibit lower accuracy due to their suboptimal handling of data freshness and quantity. While the Greedy-in-FT strategy initially reduces AoI, its inability to implement an effective dispatching strategy limits its ability to optimize data collection and fine-tuning accuracy. Our method, which incorporates GNNs to fuse PoI attributes, captures a more detailed and dynamic state representation, enabling better utilization of data freshness and quantity, and thereby outperforming methods like IQL, which lacks sufficient PoI information and struggles to fully capture the sensitivity of data value to freshness. 
% \xiaoxi{The baseline names are not consistent with those in tables and figures. Besides, all the ``Greedy in pois'' in this paper and figures should be revised as ``Greedy-in-FT'' (with FT representing fine-tuning). ``Greedy in orders'' should be ``Greedy-in-order-serving''.}
% }

% \begin{figure}[t]
% \centerline{\includegraphics[width=1.0\linewidth]{figure/AOI.png}}
% \caption{The average AoI comparison with baselines.}
% \label{AoI}
% \end{figure}

Overall, our algorithm achieves a balance between order income and data utility, ensuring the completion quality of both tasks as much as possible through reasonable decision-making in the context of limited vehicle numbers.

\subsection{Effect of RankTuner}
% \broken{
Our experiments evaluate the effectiveness of the RankTuner by comparing it with two alternative rank selection strategies:
\begin{itemize}
\item[$\bullet$] 
Greedy: It adjusts the LoRA rank by comparing the ADU of the current step with the previous step. If the ADU improves, the rank is further adjusted in the same direction; otherwise, no change is made.
\end{itemize}
\begin{itemize}
\item[$\bullet$] 
Thompson sampling~\cite{chapelle2011empirical}: This strategy employs a probabilistic method to balance exploration and exploitation. By sampling ranks according to their estimated likelihood of enhancing ADU, it facilitates broader exploration of potential rank configurations.
\end{itemize}
According to Table~\ref{tab:rank}, rank 3 provides the best trade-off between fine-tuning time and accuracy. While rank 6 achieves the highest precision, its fine-tuning time is nearly double that of rank 3, limiting the number of fine-tuning tasks that can be completed within a given timeframe. The reduced fine-tuning time of rank 3 enables agents to perform more tasks, improving overall ADU and reinforcing the need for a strategy that balances efficiency and accuracy. Fig.~\ref{fig:rank} demonstrates that our RankTuner converges to the optimal rank 3 faster than the Greedy and Thompson Sampling strategies. The Greedy strategy often suffers from limited exploration, resulting in slower adaptation to dynamic environments. Thompson Sampling, while more exploratory, is prone to settling into local optima when the benefits of the local and global solutions are close~\cite{phan2019thompson}. This issue arises because Thompson Sampling assigns higher probabilities to seemingly optimal ranks early on, making it challenging to escape suboptimal configurations. As shown in Table~\ref{tab:qos_comparison}, the RankTuner achieves the highest QoS after 2.5 million training steps, outperforming both baselines. These results validate the RankTuner's ability to dynamically adjust ranks, balance exploration and exploitation, and optimize fine-tuning tasks in diverse and uncertain environments.
% }
% 5) \textbf{Impact of varying number of agents}:

\subsection{Impact of varying distribution of PoIs} %\xiaoxi{We still need this type of varying distribution experiments.}
To verify the robustness of the algorithm under different distributions, we set different distribution types for PoI in our experiment. As shown in Fig. \ref{different result of different distribution}, our method exhibits better performance compared to other baselines under different PoI distributions. We find that when the distribution of PoIs is roughly consistent with the distribution of orders, the total QoS obtained by the fleet will be higher. We speculate that this is because the consistent distribution reduces the complexity of the algorithm in learning the distribution of the environment and making dispatching decisions. There are some grids within which there are a considerable number of orders and PoI simultaneously, allowing for better maximization of total QoS by scheduling vehicles to these types of grids. Compared to other distribution types, the rule-based methods of random, greedy on orders, and greedy on PoIs achieve higher QoS when PoIs are uniformly distributed. This is mainly because in the case of uniformly distributed PoIs, there is a similar amount of PoIs waiting to be collected in each grid. These rule-based methods generate dispatching actions randomly, making the probability of vehicles traveling to each grid roughly the same, thereby increasing the probability of vehicles collecting PoI data from each grid.


\subsection{Scalability: impact of the number of tasks}
% \broken{
In this section, we analyze the scalability of our proposed method in terms of the number of tasks, examining its performance across one, two, and three tasks. As shown in Table \ref{tab:task scalability}, we observe that as the number of tasks increases, there are variations in the metrics of QoS, ADI and ADU. Specifically, while the QoS improves when moving from one to two tasks, a slight decrease occurs when transitioning to three tasks. This decline can be attributed to the increased complexity of handling multiple task types, each with varying fine-tuning accuracy. The fine-tuning accuracy for each task can differ significantly, and some tasks may have lower maximum accuracy, leading to a reduced overall QoS. Additionally, the growing diversity of tasks introduces more challenging conditions for optimizing data collection and model fine-tuning. Despite these challenges, our method still outperforms other approaches, demonstrating its ability to efficiently manage multiple tasks while maintaining high ADI and ADU values. The stable performance across tasks highlights the scalability of our framework in environments with varying task complexities.
% }

% \xiaoxi{We need a section about Scalability: impact of the number of tasks.}