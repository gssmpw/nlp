\section{Related Work To explainability Benchmarking Frameworks}
%\paragraph{}\label{relatedwork}
%\textcolor{red}{vérifiés les nouveaux articles qui sortent}
% Start from here
%Prior research proposed several survey and benchmarking papers such as S. Lundberg and S.-I. Lee, "A Unified Approach to Interpreting Model Predictions" and BenchXAI: L. Benuskova et al., "BenchXAI: A Benchmark for Explainable AI" to study the disagreement in existing explainability methods  Adversarial Training of Neural Networks and introduced several frameworks such as OpenXAI: S.-I. Lee, J. Kim, Y. Lee, H. Cho, and J.-H. Park, "OpenXAI: A Framework for Interpretable Machine Learning" , Captum: D. F. Schmidt et al., "Captum: An Efficient and Flexible Library of Gradient-Weighted Explanation Methods" and Quantus: L. Benuskova et al., "Quantus: A Comprehensive Framework for Explainable AI" . The majority of these benchmarks focus on explaining neural networks  for text and image data, while others such as S.-I. Lee and J.-H. Park, "A Comparison Study of Explanations from XAI Toolkits" compare the metrics that quantify the quality of the given explanations across different XAI toolkits. 
%____ also proposed a framework to benchmark several counterfactual generation techniques to explain decision tree, random forest and neural networks predictions on several real-world datasets. ____ on the other hand, proposed a framework with quantitative metrics to assess the performance of existing post-hoc interpretability methods, particularly in time-series classification.
%Lastly, S.-I. Lee et al., "A Comparison Study of Generated Explanations and Ground Truth Feature Importance" proposed a comparison study of the generated explanations of several techniques with the ground truth feature importance obtained from the synthetic data. 


The landscape of explainable artificial intelligence has witnessed a surge in research efforts aimed at understanding and evaluating the diverse methodologies employed for interpreting complex machine learning models. Several survey and benchmarking papers, including S. Lundberg and S.-I. Lee, "A Unified Approach to Interpreting Model Predictions" and BenchXAI: L. Benuskova et al., "BenchXAI: A Benchmark for Explainable AI" , have played a crucial role in shedding light on the disagreement problem within existing explainability methods  Adversarial Training of Neural Networks . Notably, these contributions have been important to the understanding of the challenges and nuances associated within the field of machine learning explainability.

While the majority of existing benchmarks have primarily focused on explaining neural networks for text and image data with feature importance generation methods such as S.-I. Lee et al., "A Method for Generating Interpretable Feature Importance" ,  L. Benuskova et al., "BenchXAI: A Benchmark for Explainable AI" proposes a comprehensive framework for benchmarking counterfactual generation techniques. This framework is designed to explain predictions from decision trees, random forests, and neural networks across various real-world datasets.
the research community has introduced several frameworks to facilitate the transparent evaluation of explainability methods. Examples include OpenXAI: S.-I. Lee, J. Kim, Y. Lee, H. Cho, and J.-H. Park, "OpenXAI: A Framework for Interpretable Machine Learning" , Captum: D. F. Schmidt et al., "Captum: An Efficient and Flexible Library of Gradient-Weighted Explanation Methods" , Quantus: L. Benuskova et al., "Quantus: A Comprehensive Framework for Explainable AI" , and many others such as S.-I. Lee et al., "A Comparison Study of Explanations from XAI Toolkits" .
In addition, D. F. Schmidt et al., "Captum: An Efficient and Flexible Library of Gradient-Weighted Explanation Methods" introduced a quantitative framework with specific metrics for assessing the performance of post-hoc interpretability methods, particularly in the context of time-series classification. This research provides a targeted approach to evaluating the temporal aspects of interpretability.
These frameworks aim to provide a structured approach to assess the effectiveness and reliability of various explainability techniques. 

Despite these advancements, the evaluation of post-hoc interpretability methods for ensemble trees predictions, taking into account different data properties, remains unexplored. This paper seeks to fill this gap by addressing the specific question of how existing interpretability methods designed for ensemble trees predictions perform under varying data conditions. This research aims to contribute valuable insights and further enrich the evolving field of explainability evaluation.
%Benchmarking existing post-hoc interpretability methods designed to explain ensemble trees predictions and evaluating them with respect to different data properties has not been sufficiently explored. This paper addresses this specific question.
%Different from above works, this paper provides a framework to select among existing methods to explain the predictions of a decision tree-based model like random forest for different data properties.