\section{Related Work To explainability Benchmarking Frameworks}
%\paragraph{}\label{relatedwork}
%\textcolor{red}{vérifiés les nouveaux articles qui sortent}
% Start from here
%Prior research proposed several survey and benchmarking papers such as XAI-survey \cite{bodriaBenchmarkingSurveyExplanation2023a} and BenchXAI \cite{liuSyntheticBenchmarksScientific2021, liu2021synthetic} to study the disagreement in existing explainability methods \cite{krishnaDisagreementProblemExplainable2022,neelyOrderCourtExplainable2021},  \cite{camburuCanTrustExplainer2019,hanWhichExplanationShould2022,turbeEvaluationPosthocInterpretability2023}, and introduced several frameworks such as OpenXAI \cite{agarwalOpenXAITransparentEvaluation}, Captum \cite{kokhlikyanCaptumUnifiedGeneric2020} and Quantus \cite{hedstromQuantusExplainableAI2022}. The majority of these benchmarks focus on explaining neural networks  \cite{ismailBenchmarkingDeepLearning2020,attanasioFerretFrameworkBenchmarking2022,bodriaBenchmarkingSurveyExplanation2021,yang2019benchmarking, zhong2023clock,han2022explanation}
% for text and image data, while others such as \cite{guidotti2021evaluating, le2023benchmarking} compare the metrics that quantify the quality of the given explanations across different XAI toolkits. 
%\cite{moreira2022benchmarking} also proposed a framework to benchmark several counterfactual generation techniques to explain decision tree, random forest and neural networks predictions on several real-world datasets. \cite{turbe2023evaluation} on the other hand, proposed a framework with quantitative metrics to assess the performance of existing post-hoc interpretability methods, particularly in time-series classification.
%Lastly, \cite{guidotti2021evaluating} proposed a comparison study of the generated explanations of several techniques with the ground truth feature importance obtained from the synthetic data. 


The landscape of explainable artificial intelligence has witnessed a surge in research efforts aimed at understanding and evaluating the diverse methodologies employed for interpreting complex machine learning models. Several survey and benchmarking papers, including XAI-survey \cite{bodriaBenchmarkingSurveyExplanation2023a} and BenchXAI \cite{liuSyntheticBenchmarksScientific2021, liu2021synthetic}, have played a crucial role in shedding light on the disagreement problem within existing explainability methods \cite{krishnaDisagreementProblemExplainable2022, neelyOrderCourtExplainable2021, camburuCanTrustExplainer2019, hanWhichExplanationShould2022, turbeEvaluationPosthocInterpretability2023}. Notably, these contributions have been important to the understanding of the challenges and nuances associated within the field of machine learning explainability.

While the majority of existing benchmarks have primarily focused on explaining neural networks for text and image data with feature importance generation methods such as \cite{ismailBenchmarkingDeepLearning2020, attanasioFerretFrameworkBenchmarking2022, bodriaBenchmarkingSurveyExplanation2021, yang2019benchmarking, zhong2023clock, han2022explanation}, %\cite{moreira2022benchmarking} proposes a comprehensive framework for benchmarking counterfactual generation techniques. This framework is designed to explain predictions from decision trees, random forests, and neural networks across various real-world datasets.
the research community has introduced several frameworks to facilitate the transparent evaluation of explainability methods. Examples include OpenXAI \cite{agarwalOpenXAITransparentEvaluation}, Captum \cite{kokhlikyanCaptumUnifiedGeneric2020}, Quantus \cite{hedstromQuantusExplainableAI2022}, and many others such as \cite{guidotti2021evaluating, le2023benchmarking}.
In addition, \cite{turbe2023evaluation} introduced a quantitative framework with specific metrics for assessing the performance of post-hoc interpretability methods, particularly in the context of time-series classification. This research provides a targeted approach to evaluating the temporal aspects of interpretability.
These frameworks aim to provide a structured approach to assess the effectiveness and reliability of various explainability techniques. 

Despite these advancements, the evaluation of post-hoc interpretability methods for ensemble trees predictions, taking into account different data properties, remains unexplored. This paper seeks to fill this gap by addressing the specific question of how existing interpretability methods designed for ensemble trees predictions perform under varying data conditions. This research aims to contribute valuable insights and further enrich the evolving field of explainability evaluation.
%Benchmarking existing post-hoc interpretability methods designed to explain ensemble trees predictions and evaluating them with respect to different data properties has not been sufficiently explored. This paper addresses this specific question.
%Different from above works, this paper provides a framework to select among existing methods to explain the predictions of a decision tree-based model like random forest for different data properties.