\section{Related Work}
\label{sec:related-work}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Learning-Based Collision Detection in Motion Planning}
Learning-based approaches for CD have gained significant attention for improving the efficiency and adaptability of MP algorithms. These methods aim to reduce the computational burden of traditional CD by leveraging ML techniques to approximate the geometry of the configuration space.

A prominent example is using Support Vector Machines (SVMs). 
%
Das and Yip~\cite{das2020learning} introduced the Fastron algorithm, which employs SVM and active learning~\cite{settles2009active} to quickly identify forbidden configurations even when the obstacles move in the environment. 
This was later enhanced by incorporating forward kinematics into the learning process~\cite{das2020forward}, improving both accuracy and efficiency of LCD.


Alternative ML techniques beyond SVMs have also been introduced. 
For example, Yu and Gao~\cite{yu2021reducing} proposed using Graph Neural Networks (GNNs)~\cite{zhou2020graph} to reduce the computational cost of CD in MP.
This was later extended to dynamic environments by incorporating temporal encoding~\cite{zhang2022learning}.

Unfortunately, none of the aforementioned LCDs provide formal guarantees on the classification accuracy of new configurations.

{Looking beyond CD, it is worth noting that recent works used ML for configuration-space representations: 
Li et al.~\cite{Li2024} introduced a method for configuration space distance fields, 
while Koptev et al.~\cite{Koptev2022} developed a neural implicit function for reactive manipulator control.
These approaches focus on approximating configuration space distances or using neural models for implicit 
collision representations.}

\subsection{Sample Complexity}
Sample complexity refers to the minimum number of training samples that an ML algorithm needs in order to successfully learn a target function or achieve a desired level of performance. It is typically expressed as a function of the desired accuracy ($\varepsilon$) and confidence ($\xi$).
%
Sample complexity is closely related to the notion of VC dimension~\cite{vapnik2015uniform} which, roughly speaking, measures the complexity of a hypothesis set or classification model.

For binary classification problems, the fundamental theorem of statistical learning states that the sample complexity is linearly related to the VC dimension of the hypothesis class~\cite{shalev2014understanding}. 
Specifically, for a hypothesis class $H$ with VC dimension $d$, the sample complexity $m(\varepsilon, \xi)$ to achieve an error of at most $\varepsilon$ with probability at least $1-\xi$ is bounded by 
$
m(\varepsilon, \xi) = O\left({(d + \log(1/\xi))} / {\varepsilon^2}\right)
$.

The VC dimension of linear SVMs in an $n$-dimensional space is $n+1$, immediately leading to sample complexity bounds for these classifiers~\cite{shalev2014understanding}.
%
Importantly, for certain  settings which contain some \emph{margin} between different classes (see Sec.~\ref{sec:theory} for a precise definition), the sample complexity of SVMs can be independent of the input space's dimension.

Sample complexity was recently used for robotics-related problems.
%
For instance, recent work~\cite{TsaoSP20, DayanSPH23}  used geometric techniques to such  bounds for PRM~\cite{kavraki1996probabilistic} and related methods to achieve a desired solution quality using deterministic sampling. Another work~\cite{shaw2024practicalfinitesamplebounds} considers probabilistic sample complexity bounds for randomized sampling in the context of task and motion planning~\cite{garrett2021integrated}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%