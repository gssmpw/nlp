\section{Related Work}
\label{sec:related-work}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Learning-Based Collision Detection in Motion Planning}
Learning-based approaches for CD have gained significant attention for improving the efficiency and adaptability of MP algorithms. These methods aim to reduce the computational burden of traditional CD by leveraging ML techniques to approximate the geometry of the configuration space.

A prominent example is using Support Vector Machines (SVMs). 
%
Das and Yip____ introduced the Fastron algorithm, which employs SVM and active learning____ to quickly identify forbidden configurations even when the obstacles move in the environment. 
This was later enhanced by incorporating forward kinematics into the learning process____, improving both accuracy and efficiency of LCD.


Alternative ML techniques beyond SVMs have also been introduced. 
For example, Yu and Gao____ proposed using Graph Neural Networks (GNNs)____ to reduce the computational cost of CD in MP.
This was later extended to dynamic environments by incorporating temporal encoding____.

Unfortunately, none of the aforementioned LCDs provide formal guarantees on the classification accuracy of new configurations.

{Looking beyond CD, it is worth noting that recent works used ML for configuration-space representations: 
Li et al.____ introduced a method for configuration space distance fields, 
while Koptev et al.____ developed a neural implicit function for reactive manipulator control.
These approaches focus on approximating configuration space distances or using neural models for implicit 
collision representations.}

\subsection{Sample Complexity}
Sample complexity refers to the minimum number of training samples that an ML algorithm needs in order to successfully learn a target function or achieve a desired level of performance. It is typically expressed as a function of the desired accuracy ($\varepsilon$) and confidence ($\xi$).
%
Sample complexity is closely related to the notion of VC dimension____ which, roughly speaking, measures the complexity of a hypothesis set or classification model.

For binary classification problems, the fundamental theorem of statistical learning states that the sample complexity is linearly related to the VC dimension of the hypothesis class____. 
Specifically, for a hypothesis class $H$ with VC dimension $d$, the sample complexity $m(\varepsilon, \xi)$ to achieve an error of at most $\varepsilon$ with probability at least $1-\xi$ is bounded by 
$
m(\varepsilon, \xi) = O\left({(d + \log(1/\xi))} / {\varepsilon^2}\right)
$.

The VC dimension of linear SVMs in an $n$-dimensional space is $n+1$, immediately leading to sample complexity bounds for these classifiers____.
%
Importantly, for certain  settings which contain some \emph{margin} between different classes (see Sec.~\ref{sec:theory} for a precise definition), the sample complexity of SVMs can be independent of the input space's dimension.

Sample complexity was recently used for robotics-related problems.
%
For instance, recent work____  used geometric techniques to such  bounds for PRM____ and related methods to achieve a desired solution quality using deterministic sampling. Another work____ considers probabilistic sample complexity bounds for randomized sampling in the context of task and motion planning____.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%