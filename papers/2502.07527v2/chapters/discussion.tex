\section{Ablation study}\label{sec:ablation_study}
To better understand the contributions of different components in our model and training process, we conducted ablation studies focusing on two aspects: (1) the impact of including general text-based instruction tuning data in post-training, and (2) the effectiveness of continuing pretraining on scientific data before post-training, as opposed to directly fine-tuning from the baseline model. We evaluated the results across 7 tasks, including four small molecule tasks (Molecular property prediction, IUPAC to SMILES translation, Retrosynthesis, Metabolism optimization), two protein tasks (Protein description generation, CDR-H3 generation) and two DNA tasks (DNA property prediction). The results are reported in Fig. \ref{fig:ablation_study}. 

\begin{figure}[!htbp]
\centering
\includegraphics[width=0.8\linewidth]{figures/ablation_study.pdf}
\caption{Ablation study results. NatureLM models are denoted as ``(w/ text)''. The NatureLM models without text instruction data for post-training are denoted as ``(w/o text)''. Additionally, we fine-tuned the original Llama 3 model, denoted as ``Llama 3 8B (w/ text)''. Performance metrics are displayed in the cells (larger values indicate better performance), with rank represented by the color intensity (darker colors signify higher rankings).}
\label{fig:ablation_study}
\end{figure}

\subsection{Impact of text-based post-training data}
We investigated whether incorporating general text instruction tuning data into the post-training phase affects the performance of \ourM{}. To this end, we compared the results of \ourM{} pest-trained with or without the inclusion of general text-based instruction data during the post-training phase. \ourM{} (1B w/o text) and \ourM{} (8B w/o text) denote the models without text data for post-training. We have several observations from Fig. \ref{fig:ablation_study}:
\begin{itemize}
\item For the 1B parameter models, post-training without general text-based instruction data leads to better performance on scientific tasks, as evidenced by the superior performance on 5 out of 8 tasks. This suggests that at smaller scales, including text-based instruction data may not provide benefits and could potentially dilute the model's focus on scientific instructions due to limited model capacity.
\item In contrast, for the 8B parameter models, post-training with text-based instruction data does not adversely affect performance. This indicates that the larger model has sufficient capacity to incorporate both general text-based and science-based instruction data without detrimental effects on its performance on scientific tasks.
\end{itemize}

\subsection{Impact of continued pre-training on scientific data}
One might wonder whether it is necessary to pre-train a foundation model on scientific data or if directly fine-tuning a large language model (LLM) with scientific instruction data suffices. To address these questions, we compared our NatureLM (8B) model, which initially continues pre-training of Llama 3 8B on scientific data before undergoing post-training with scientific instruction data, against a model that directly fine-tunes the Llama 3 8B model without the pre-training step. As shown in Fig. \ref{fig:ablation_study}, \ourM{} (8B) outperforms the directly fine-tuned Llama 3 8B across all tasks, highlighting the importance of pre-training on a scientific corpus.


\section{Discussions}
\subsection{Summary}
In this work, we developed Nature Language Model (\ourM{}), a sequence-based science foundation model for scientific discovery across multiple domains. Users can interact with the model using text-based instructions to generate novel scientific entities. It supports cross-domain generation and has been demonstrated in phases of drug discovery, protein generation, RNA generation, and enables predictive capabilities for small molecules, proteins, and DNA. Among the 22 tasks tested, larger models showed better performance on 18 tasks. We believe \ourM{} is a significant step towards transforming scientific discovery with foundation model approaches. 

\subsection{Limitations}
Despite the progress of \ourM{}, we have identified several limitations and are committed to addressing them in future versions:

{\em Language Capabilities}: Interacting with scientific models using human language will be an essential feature to enable scientific discoveries. Although \ourM{} demonstrates general language capabilities, it achieves only a 31.8\% winning rate on the AlpacaEval benchmark when compared to the original Mixtral 8x7B. To enhance this, we plan to incorporate more high-quality text data in pre-training in the future.

{\em Few-shot Capabilities}: The capability of few-shot learning is critical for a foundation model. Currently, our NatureLM does not exhibit strong few-shot capabilities. We aim to enhance this by refining our training strategies and increasing the model size.

\subsection{Cross-domain applications}
\ourM{} is a unified model that spans multiple domains, including text, small molecules, proteins, materials, and nucleotides. One significant advantage of this multi-domain unification is that it allows for the integration of knowledge from diverse fields, enabling us to tackle important cross-domain tasks that domain-specific models cannot address. While we have already provided a few examples of cross-domain tasks, here are several more that we plan to study in the future:
\begin{enumerate}
\item \emph{Design of Biocompatible Materials:} Developing biocompatible materials requires the simultaneous consideration of material properties and protein interactions. Examples included the Titanium Alloys and Cobalt-Chromium Alloys used in hip replacement.
\item \emph{Ribozyme and Bio-Catalyst Development:} Designing effective ribozymes and bio-catalysts necessitates a detailed understanding of RNA structures, protein functions, and small molecule interactions.
\item \emph{Enabling Complex System Understanding:} Systems biology aims to understand the complex interplay of various components in a system, including biomolecules such as proteins, DNA, RNA, lipids, carbohydrates, and small molecules like metabolites.
\end{enumerate}



\clearpage

\section*{Author list}\label{sec:authorlist}
%% Explicitly add a new line to ensure that authors' names are not split across two lines.
Yingce Xia$^{*}$, Peiran Jin$^{*}$, Shufang Xie$^{*}$, Liang He$^{*}$, Chuan Cao$^{*}$, 

Renqian Luo$^{*}$, Guoqing Liu$^{*}$, Yue Wang$^{*}$, Zequn Liu$^{*}$, Yuan-Jyue Chen$^{*}$, 

Zekun Guo$^{*}$, Yeqi Bai, Pan Deng, Yaosen Min, Ziheng Lu, 

Hongxia Hao, Han Yang, Jielan Li, Chang Liu, Jia Zhang, 

Jianwei Zhu, Ran Bi, Kehan Wu, Wei Zhang, Kaiyuan Gao, Qizhi Pei, 

Qian Wang, Xixian Liu, Yanting Li, Houtian Zhu, Yeqing Lu, 

Mingqian Ma, Zun Wang, Tian Xie, Krzysztof Maziarz, Marwin Segler, 

Zhao Yang, Zilong Chen, Yu Shi, Shuxin Zheng, Lijun Wu, 

Chen Hu, Peggy Dai, Tie-Yan Liu, Haiguang Liu, Tao Qin

\vspace{0.5cm}


\noindent$^*$ indicates co-first authors. 
 
\noindent Corresponding authors: Tao Qin and Yingce Xia 

\noindent Contact emails: \{taoqin, yingce.xia\}@microsoft.com

\noindent This work was conducted in Microsoft Research AI for Science.

\section*{Acknowledgements}
We extend our gratitude to Dr. Fan Yang and Dr. Jilong Xue for their support with large-scale model training. We thank Likun Dong and Junren Li for conducting the case study on retrosynthesis and the SMILES-to-IUPAC translation. Our thanks also go to Dr. Claudio Zeni, Dr. Robert Pinsler, Dr. Daniel Z\"{u}gner, Dr. Andrew Fowler, Dr. Matthew Horton, and Dr. Ryota Tomioka for their assistance with material tasks. We appreciate the constructive feedback from Dr. Bichlien Nguyen, Dr. Jake Smith, and Dr. Frank No\'{e}. We thank Dr. Han Guo for visualizing the molecules in our paper. We acknowledge Jingyun Bai for  improving the quality of the figures. We thank Dr. Christopher Bishop for his invaluable guidance and sponsorship of this work.

This work was done when Zekun Guo, Kehan Wu, Wei Zhang, Kaiyuan Gao, Qizhi Pei, Qian Wang, Xixian Liu, Yanting Li, Houtian Zhu, Yeqing Lu, Mingqian Ma, Zhao Yang, Zilong Chen were interns at Microsoft Research AI for Science. % This work was conducted by Microsoft.


