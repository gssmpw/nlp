\section{Strategies to further improve performance}
In this section, we examine two strategies to improve the model's performance: reinforcement learning for scenarios with limited labeled data for fine-tuning specific tasks, and dedicated fine-tuning for cases where sufficient labeled data is available for particular tasks.
% In this section, we explore two strategies to further enhance the model's performance: reinforcement learning and dedicated fine-tuning. Reinforcement learning is particularly effective when training data is limited, while dedicated fine-tuning can be leveraged when ample downstream data is available.

\subsection{Reinforcement enhanced \ourM{}}
\label{sec:RL}
Reinforcement Learning with Human Feedback (RLHF) is well-established approach to enhance foundation models. This section explores how to utilize preference signals in RLHF, moving beyond reliance on direct supervised signals\footnote{It is important to note that direct signals have already been used in post-training (see Section~\ref{sec:post_train}.}. For many generative tasks, where answers are open-ended and do not have a single correct solution, training with preference signals offers a more intuitive approach.

For RLHF training, we curated preference data from nine property optimization tasks related to small molecules: BBBP, BACE, LogP, Donor, QED, CYP1A2, CYP2C9, CYP2D6, and CYP3A4. Detailed descriptions of each task and the corresponding data quantities can be found in Table \ref{tab:data-rlhf}. In total, we compiled 179.5k data points. Note that we used all the data to enhance the post-trained \ourM{} (8B), resulting in a single model for the nine tasks after RLHF.

The data is structured in a preference-based format, where each sample consists of a prompt, along with both an accepted and a rejected response. An example of this format is presented below:

\begin{example}
\textbf{Instruction: }\\\texttt{Enhance the effectiveness of the molecule }\mol{}COc1cc2c(c(OC)c1OC)-c1ccc(OC)c(=O)cc1[C@@H](NC(C)=O)CC2 \emol{}\texttt{ in penetrating the blood-brain barrier.}\\
\textbf{Accepted Response: }\\\mol{}COc1cc2c(c(OC)c1OC)-c1ccc(OC)c(=O)cc1C(NC(C)=O)CC2\emol{}\\
\textbf{Reject Response: }\\\mol{}COc1cc2c(c(OC)c1OC)-c1ccc(OC)c(=O)cc1[C@@H](NC(C)=O)\\CC2\emol{}
\end{example}  

In the example above, the compound in the accepted response is capable of crossing the BBB, whereas the compounds in the instruction and rejected response cannot.

We leveraged Direct Preference Optimization (DPO)~\cite{rafailov2024direct} to enhance the molecule optimization ability of \ourM{}. The loss of DPO algorithm is written as follows:

\begin{align}
    \mathcal{L}_{\text{DPO}}(\pi_\theta; \pi_{\text{ref}}) = -\mathbb{E}_{(x, y_w, y_l) \sim \mathcal{D}} \left[ \log \sigma \left( \beta \log \frac{\pi_\theta(y_w \mid x)}{\pi_{\text{ref}}(y_w \mid x)} - \beta \log \frac{\pi_\theta(y_l \mid x)}{\pi_{\text{ref}}(y_l \mid x)} \right) \right],
\end{align}
where $\pi_{\rm ref}$ is the model after post-training and fixed during DPO training, $\pi_\theta$ is the model to optimize and set to $\pi_{\rm ref}$ before DPO training, $x$ is the prompt, $y_w$ is the accepted response, $y_l$ is the reject response, and $\beta$ is a hyper-parameter. 
 

\begin{table}[h]
\centering
\begin{tabular}{lc}
\toprule
Property & $\Delta$  \\
\midrule
QED  & 0.6 \\
LogP  & 0.6 \\
Donor  & 0.6 \\
BBBP  & 2.9 \\
BACE  & 3.5 \\
CYP1A2  & 2.3 \\
CYP2C9  & 0.7 \\
CYP2D6  & 0.7 \\
CYP3A4  & 1.0 \\
\bottomrule
\end{tabular}
\caption{Results of the reinforcement optimization. Let $r_1$ and $r_2$ represent the outcomes before and after applying reinforcement, respectively, and let $\Delta$ denote the percentage improvement, i.e., $\Delta = (r_2 - r_1)/r_1 \times 100\%$. }
\label{tab:dpo-result}
\end{table}


%%% do not delete; Yue's original data
% \begin{table}[h]
% \centering
% \begin{tabular}{ c|c|c }
% \hline
% Property Name & SFM-Seq  & SFM-Seq-DPO \\ \hline
% BBBP          & 0.380   & 0.391       \\
% BACE          & 0.434   & 0.449       \\
% LogP          & 0.477   & 0.480       \\
% Donor         & 0.817   & 0.822       \\
% QED           & 0.845   & 0.850       \\
% CYP1A2        & 0.896   & 0.917       \\
% CYP2C9        & 0.878   & 0.884       \\
% CYP2D6        & 0.829   & 0.835       \\
% CYP3A4        & 0.890   & 0.899        \\ \hline
% \end{tabular}
% \caption{Results of Text-guided Property Optimization Tasks }
% \label{tab:dpo-result}
% \end{table}
Table \ref{tab:dpo-result} shows the improvements of DPO training over the 9 property optimization tasks. Notably, the model had already undergone instruction tuning (the post-training in Section~\ref{sec:smallmol}) prior to DPO training, and no new data was introduced during the DPO process. The results highlight how reformatting the data into a preference-based structure allows the DPO algorithm to improve performance across multiple tasks simultaneously.

Looking ahead, we plan to generate data on the fly in RLHF and utilize additional reward models to evaluate the properties of newly generated molecules, thereby creating better preference-based data.

\subsection{Dedicated fine-tuning on retrosynthesis}\label{sec:dedicate_tune}
% We dedicatedly fine-tuned our \ourM{} model to evaluate whether it can match or exceed the performance of specialist models when trained on a large-scale downstream dataset. 
% For the retrosynthesis prediction task, we used the Pistachio commercial reaction dataset~\cite{mayfield2017pistachio}.
% The raw dataset, extracted from U.S., European and WIPO patents, comprises a total of 15 million reactions. 
% We carefully eliminated any invalid or duplicated reactions to ensure the dataset's quality.
% After this process, the dataset was randomly divided into a training set, with 3.1 million reactions, and a test set, consisting of 34,000 reactions.

We dedicatedly fine-tuned our \ourM{} model to evaluate its performance against specialized models in the retrosynthesis prediction task, using a large-scale labeled dataset, the Pistachio reaction dataset~\cite{mayfield2017pistachio}, with 15 million reactions from U.S., European, and WIPO patents. 
% We removed any invalid or duplicated reactions to ensure data quality. 
To ensure data quality, we removed any invalid or duplicate reactions.
The cleaned dataset was then randomly split into a training set with 3.1 million reactions and a test set with 34,000 reactions.
% Any invalid or duplicate reactions are removed from the dataset.
% Subsequently, the cleaned dataset is randomly divided into a training set consisting of 3.1M reactions and a test set comprising 0.2M reactions.

% Before training our \ourM{} model,
Before training, we preprocessed the input products and output reactants using a root-aligned SMILES format~\cite{Zhong2022rsmiles}.
This format offers a clear one-to-one mapping between the product and reactant SMILES, 
% improving prediction efficiency.
thereby enhancing prediction efficiency.
Additionally, we augmented the training dataset tenfold to further improve the model's performance.
% During the inference stage, we leverage a well-trained forward reaction prediction model (can be \ourM{} itself) to re-rank the predicted reactants obtained from beam search~\cite{lin2022improving, xia2017dual}.
% Specifically, we initially obtain $K$ candidates $\hat{y}_i$ ($i\in [K]$) from the backward model (\ourM{}) for product $x$ using beam search.
% We then re-rank all candidates using the following rank function:
% \begin{equation}
%     \alpha l_{f}(\hat{y}_i, x) + (1-\alpha)l_{b}(x,\hat{y}_i),
% \end{equation}
% where $\alpha$ is a hyperparameter to balance the tradeoff between two functions, $l_{f}(\hat{y}_i, x)$ denotes the relative rank of $x$ in the output beam of input $\hat{y}_i$, as determined by the forward model.
% $l_{b}(x, \hat{y}_i)$ denotes the original relative rank of $\hat{y}_i$, that is, $i$.
% Table~\ref{tab:retro-pistachio} clearly shows that \ourM{}~(1B) competes favorably with leading template-based (LocalRetro) and template-free (R-SMILES) specialist models on the large Pistachio dataset.
As shown in Table~\ref{tab:retro-pistachio}, NatureLM (1B) demonstrates competitive performance, rivaling leading template-based models (e.g., LocalRetro) and template-free models (e.g., R-SMILES) on the large Pistachio dataset.

% \begin{table}[!htbp]
\begin{table}[!h]

\centering
\begin{tabular}{lcc}
\toprule
& Top-1 accuracy & Top-3 accuracy \\
\midrule
% GPT-4 &  &  \\
LocalRetro~\cite{chen2021localretro} & 40.8\% & 56.6\% \\
R-SMILES~\cite{Zhong2022rsmiles} & 51.2\% & 67.1\% \\ 
\midrule
\ourM{}~(1B) & 51.4\% & 66.0\% \\
\bottomrule
\end{tabular}
\caption{Retrosynthesis prediction results on Pistachio dataset.}
\label{tab:retro-pistachio}
\end{table}

\subsection{Dedicated fine-tuning on Matbench}\label{sec:dedicate_tune_matbench}
We fine-tuned our \ourM{} 8B model on Matbench~\cite{matbench}, a benchmark for state-of-the-art machine learning algorithms that predict various properties of solid materials. Matbench is hosted and maintained by the Materials Project~\cite{materialsproject}. Following the approach in~\cite{xie2023darwin}, we fine-tuned a single model for three tasks from Matbench, rather than developing separate models for each task. 

The results are presented in Table~\ref{tab:pi-matbench-expt-gap} to  \ref{tab:pi-matbench-glass}. Results of baseline models are collected from the official leader board\footnote{\url{https://matbench.materialsproject.org/}}. As can be seen, \ourM{} achieves state-of-the-art performance on matbench\_expt\_gap and matbench\_is\_metal. 

\begin{table}[h]
  \centering
  \begin{minipage}{0.3\linewidth}
    \centering
    %\renewcommand{\arraystretch}{1.4} % Adjust row height  
    \begin{tabular}{lc}
    \toprule
    Model &  MAE $\downarrow$ \\
    \midrule
    Dummy\cite{matbench} & 1.1435\\
    gptchem\cite{Jablonka_2023} & 0.4544\\
    RF-SCM/ & \multirow{2}{*}{0.4461}\\
    Magpie\cite{matbench} & \\
    AMMExpress\cite{matbench} & 0.4161\\
    %CrabNet\cite{Wang2021crabnet} & 0.3463\\
    MODNet\cite{De_Breuck_2021} & 0.3327\\
    Ax/SAASBO & \multirow{2}{*}{0.3310} \\
     CrabNet\cite{Wang2021crabnet,erikssonHighDimensionalBayesianOptimization2021} & \\
    DARWIN\cite{xie2023darwin} & 0.2865 \\
    \midrule
    \ourM{} & \textbf{0.2858} \\
    \bottomrule
    \end{tabular}  
    \vspace{0.35cm} % Add vertical space to match heights 
    \caption{Results on matbench\_expt\_gap.}
    \label{tab:pi-matbench-expt-gap}
  \end{minipage}
  \hfill  
  \begin{minipage}{0.3\linewidth}
    \centering
    %\renewcommand{\arraystretch}{1.4} % Adjust row height  
    \begin{tabular}{lc}
    \toprule
    Model &  F1 $\uparrow$ \\
    \midrule
    Dummy\cite{matbench} & 0.4913\\
    gptchem\cite{Jablonka_2023} & 0.8953\\
    MODNet\cite{De_Breuck_2021} & 0.9153\\
    RF-SCM/ & \multirow{2}{*}{0.9159}\\
    Magpie\cite{matbench} & \\
    AMMExpress\cite{matbench} & 0.9200\\
    DARWIN\cite{xie2023darwin} & 0.9599 \\
    \midrule
    \ourM{} & \textbf{0.9630} \\
    \bottomrule
    \end{tabular}
    \vspace{1cm} % Add vertical space to match heights  
    \caption{Results on matbench\_is\_metal.}  
    \label{tab:pi-matbench-is-metal}
  \end{minipage}
  \hfill  
  \begin{minipage}{0.3\linewidth}
    \centering
    %\renewcommand{\arraystretch}{1.4} % Adjust row height  
    \begin{tabular}{lc}
    \toprule
    Model &  F1 $\uparrow$ \\
    \midrule
    Dummy\cite{matbench} & 0.7127 \\
    DARWIN\cite{xie2023darwin} & 0.8722 \\
    gptchem\cite{Jablonka_2023} & 0.8782\\
    RF-SCM/ & \multirow{2}{*}{0.9278}\\
    Magpie\cite{matbench} & \\
    AMMExpress\cite{matbench} & 0.9043\\
    MODNet\cite{De_Breuck_2021} & \textbf{0.9784}\\
    \midrule
    \ourM{} & 0.8720 \\
    \bottomrule
    \end{tabular}
    \vspace{1cm} % Add vertical space to match heights  
    \caption{Results on matbench\_glass.}  
    \label{tab:pi-matbench-glass}
  \end{minipage}
\end{table}  




% \begin{table}[!h]
% \centering
% \begin{tabular}{lc}
% \toprule
% Model &  MAE $\downarrow$ \\
% \midrule
% %LLAMA & 0.31 & 96.5 & 87.7 \\
% DARWIN-MDP~\cite{xie2023darwin} & 0.317 \\ 
% \midrule
% \ourM{} (8B) & 0.286 \\
% \bottomrule
% \end{tabular}
% \caption{Results on matbench\_expt\_gap.}
% \label{tab:pi-matbench-expt-gap}
% \end{table}

% \begin{table}[!h]
% \centering
% \begin{tabular}{lc}
% \toprule
% Model &  F1 $\downarrow$) \\
% \midrule
% DARWIN-MDP~\cite{xie2023darwin} & 95.9 \\ 
% \midrule
% \ourM{} (8B) & 96.3 \\
% \bottomrule
% \end{tabular}
% \caption{Results on matbench\_is\_metal.}
% \label{tab:pi-matbench-is-metal}
% \end{table}

% \begin{table}[!h]
% \centering
% \begin{tabular}{lc}
% \toprule
% Model &  (MAE $\downarrow$) \\
% \midrule
% DARWIN-MDP~\cite{xie2023darwin} & 88.0 \\ 
% \midrule
% \ourM{} (8B) & 87.2 \\
% \bottomrule
% \end{tabular}
% \caption{Results on matbench\_glass.}
% \label{tab:pi-matbench-glass}
% \end{table}