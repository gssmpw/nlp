\section{Reinforcement enhanced \ourM{}}
\label{sec:RL}

To further enhance the model's performance on specific tasks, we fine-tune it using the Reinforcement Learning with Human Feedback (RLHF) method. This section explores how leveraging preference signals, rather than relying solely on direct supervised signals, can improve the model's effectiveness. For many generative tasks, where answers are inherently open-ended and lack a single correct solution, training with preference signals provides a more natural approach. Our experimental results demonstrate that this method yields superior outcomes.
% To further improve the model's performance on specific tasks, we fine-tune it using the RLHF method. The aim of this section is to explore the potential of enhancing the model's performance by leveraging preference signals rather than direct supervised signals through RLHF. For most generative tasks, the answers are open-ended, and there is no single correct solution. Training with preference signals is more natural, and experimental results indicate that this approach yields better outcomes.

% \subsection{Data and Algorithm}

For the RLHF training, we prepare preference-based data derived from $9$ text-guided property optimization tasks involving small molecules. These tasks include optimizing properties such as BBBP, BACE, LogP, Donor, QED, CYP1A2, CYP2C9, and CYP2D6. The corresponding task names and data quantities are detailed in Table \ref{tab:data-rlhf}.


% To prepare the preference data for RLHF training, we utilize data from small molecules across nine text-guided property optimization tasks. These nine properties include BBBP, BACE, LogP, Donor, QED, CYP1A2, CYP2C9, and CYP2D6. We have collected data from each of these tasks, with the corresponding task names and data quantities presented in Table \ref{tab:data-rlhf}.

\begin{table}[b]
\centering
\begin{tabular}{ c|c|c }
\hline
Property Name & Training samples & Testing samples \\ \hline
BBBP          & 1272             & 199             \\  
BACE          & 90677            & 152             \\  
LogP          & 8491             & 473             \\  
Donor         & 8526             & 478             \\  
QED           & 8466             & 476             \\  
CYP1A2        & 8076             & 103             \\  
CYP2C9        & 21589            & 199             \\  
CYP2D6        & 8067             & 165             \\  
CYP3A4        & 24376            & 171             \\ \hline
Total         & 179540           & 2416            \\ \hline
\end{tabular}
\caption{Statistics of preference data used in RLHF}
\label{tab:data-rlhf}
\end{table}

The data is structured in a preference-based format, where each sample consists of a prompt, along with both an accepted and a rejected response. An example of this format is presented below:

% The data is structured in a preference-based format. For each sample, we provide a prompt, along with an accepted response and a rejected response. An example is shown in Figure \ref{fig:dpo data example}. 
% For each property, we construct the preference dataset as follows: we 

 
\begin{example}
\textbf{Instruction: }\\Enhance the effectiveness of the molecule COc1cc2c(c(OC)c1OC)-c1ccc(OC)c(=O)cc1[C@@H](NC(C)=O)CC2  in penetrating the blood-brain barrier.\\
\textbf{Accepted Response: }\\COc1cc2c(c(OC)c1OC)-c1ccc(OC)c(=O)cc1C(NC(C)=O)CC2\\
\textbf{Reject Response: }\\COc1cc2c(c(OC)c1OC)-c1ccc(OC)c(=O)cc1[C@@H](NC(C)=O)CC2
\end{example}  



To further enhance the model's capabilities across several tasks, we apply the Direct Preference Optimization (DPO) technique \cite{rafailov2024direct}. 

We leverage Direct Preference Optimization(DPO)\cite{rafailov2024direct} to further enhance our foundation model's capability on several tasks listing below. 
The loss of DPO algorithm is 
\begin{align}
    \mathcal{L}_{\text{DPO}}(\pi_\theta; \pi_{\text{ref}}) = -\mathbb{E}_{(x, y_w, y_l) \sim \mathcal{D}} \left[ \log \sigma \left( \beta \log \frac{\pi_\theta(y_w \mid x)}{\pi_{\text{ref}}(y_w \mid x)} - \beta \log \frac{\pi_\theta(y_l \mid x)}{\pi_{\text{ref}}(y_l \mid x)} \right) \right].  
\end{align}
where $\pi_{\rm ref}$ is the pre-given model, it will be fixed during the training, $x$ is the prompt, $y_w$ is the accepted response, $y_l$ is the reject response. $\beta$ is the hyper-parameters. 
 

\begin{table}[h]
\centering
\begin{tabular}{lc}
\toprule
Property & $\Delta$  \\
\midrule
QED  & 0.6 \\
LogP  & 0.6 \\
Donor  & 0.6 \\
BBBP  & 2.9 \\
BACE  & 3.5 \\
CYP1A2  & 2.3 \\
CYP2C9  & 0.7 \\
CYP2D6  & 0.7 \\
CYP3A4  & 1.0 \\
\bottomrule
\end{tabular}
\caption{Results of the reinforcement optimization. Let $r_1$ and $r_2$ represent the outcomes before and after applying reinforcement, respectively, and let $\Delta$ denote the percentage improvement, i.e., $\Delta = (r_2 - r_1)/r_1 \times 100\%$. }
\label{tab:dpo-result}
\end{table}


%%% do not delete; Yue's original data
% \begin{table}[h]
% \centering
% \begin{tabular}{ c|c|c }
% \hline
% Property Name & SFM-Seq  & SFM-Seq-DPO \\ \hline
% BBBP          & 0.380   & 0.391       \\
% BACE          & 0.434   & 0.449       \\
% LogP          & 0.477   & 0.480       \\
% Donor         & 0.817   & 0.822       \\
% QED           & 0.845   & 0.850       \\
% CYP1A2        & 0.896   & 0.917       \\
% CYP2C9        & 0.878   & 0.884       \\
% CYP2D6        & 0.829   & 0.835       \\
% CYP3A4        & 0.890   & 0.899        \\ \hline
% \end{tabular}
% \caption{Results of Text-guided Property Optimization Tasks }
% \label{tab:dpo-result}
% \end{table}
Table \ref{tab:dpo-result} illustrates the improvement in our SFM-Seq model's ability to optimize molecular properties through text guidance, comparing performance before and after DPO training. Notably, the model had already undergone instruction tuning prior to DPO training, and no new data was introduced during the DPO process. The results highlight how reformatting the data into a preference-based structure allows the DPO algorithm to improve performance across multiple tasks simultaneously.
% The numbers in the table \ref{tab:dpo-result} represent the success rates of our SFM-Seq model in optimizing molecular properties based on text guidance, both before and after DPO training. It is important to note that the model had already undergone instruction tuning prior to DPO training, and no new data was introduced during the DPO process. The results in the table demonstrate that simply reformatting the data into a preference-based structure allows the DPO algorithm to improve performance across multiple tasks simultaneously.
% In the future, we plan to generate data online and use additional scoring models to evaluate the properties of new molecules, creating more realistic preference-based data. We also aim to make targeted improvements to the DPO algorithm, with the expectation of achieving even better results.
Looking ahead, we plan to generate data online and utilize additional scoring models to evaluate the properties of newly generated molecules, thereby creating more realistic preference-based data. Furthermore, we aim to make targeted improvements to the DPO algorithm, with the goal of achieving even greater performance enhancements.