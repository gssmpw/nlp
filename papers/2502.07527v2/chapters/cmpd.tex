\section{Small molecule tasks}\label{sec:smallmol}
We assess the capabilities of \ourM{} in terms of small molecule generation from the following perspectives: 
\begin{enumerate}
    \item The unconditional generation ability (Section \ref{sec:smallmol_uncondition});
    \item The basic properties (such as QED, TSPA, etc.) to small molecule generation (Section \ref{sec:basic_property_to_mol});
    \item The translation between small molecule SMILES and IUPAC (Section \ref{sec:smiles_iupac}); %, a methodology that has gained considerable popularity in recent studies (Section \ref{sec:smiles_iupac});
    \item Utilize \ourM{} to aid the drug discovery pipeline, which encompasses the generation and optimization of hit compounds (Section \ref{sec:tamgen}), optimization of binding affinity (Section \ref{sec:binding_affinity}), ADMET optimization (Section \ref{sec:admet}), and the synthesis routes of the compounds (Section \ref{sec:retro}).
\end{enumerate}



\subsection{Unconditional molecular generation}\label{sec:smallmol_uncondition}
We input the special token \mol{} to \ourM{} and let the model generate SMILES. The generation process stops upon encountering the special token \emol{}. We assess the validity of the generated SMILES by checking if they can be converted into molecules using RDKit. Additionally, we evaluate the uniqueness of the valid SMILES by calculating the ratio of unique valid SMILES to the total valid SMILES.

The evaluation results are presented in Table \ref{tab:unconditional_mol_eval}. The results demonstrate a clear trend: as the model size increases, the performance in terms of validity improves. \ourM{} exhibits a consistent increase in uniqueness as the model's capacity grows.  We also establish comparisons between \ourM{} and three generalist models: Llama 3 (8B), Mixtral (8x7B), and GPT-4. Our \ourM{} significantly outperforms the others in terms of uniqueness. As for validity, the results show that GPT-4 demonstrates a remarkable ability to generalize chemically valid SMILES.

\begin{table}[!htbp]
\centering
\begin{tabular}{lccccc}
\toprule 
& Validity (\%) & Unique (\%) \\
\midrule
Llama 3 (8B)    & 77.9 & 35.1  \\ 
Mixtral (8x7B) & 72.6 & 35.1  \\
GPT-4          & \textbf{99.6} & 54.6  \\
\midrule
\ourM{} (1B)       & 94.9 & 91.1 \\
\ourM{} (8B)       & 96.8 & 96.6 \\
\ourM{} (8x7B)     & 98.8 & \textbf{98.8} \\
\bottomrule
\end{tabular}
\caption{Unconditional evaluation of small molecules generation.}
\label{tab:unconditional_mol_eval}
\end{table}


\subsection{Property-to-molecule generation}
\label{sec:basic_property_to_mol}
The task is to generate molecules with specified properties, which is a critical aspect of molecular design. An example is shown as follows:
\begin{example}
 \textbf{Instruction: }\\\texttt{Generate a molecule with four hydrogen bond donors.}\\
\textbf{Response: }\\\mol{}C(C[C@@H](C(=O)O)N)CN=C(N)N\emol{}
\end{example}  

We conduct evaluations of \ourM{} on six distinct properties: Quantitative Estimate of Drug-likeness (QED), hydrogen bond acceptors (HBA), hydrogen bond donors (HBD), fraction of sp3 hybridized carbons (FSP3), rotatable bonds (RotBonds), and topological polar surface area (TPSA). All these properties can be calculated using RDKit. For each property, we select multiple values as inputs to the model (see Table \ref{tab:property_values}). We generate 100 molecules for each input and evaluate them with metrics including the Spearman correlation (Fig. \ref{fig:property2mol}a) and the correct ratio (Fig. \ref{fig:prop2mol_correct}). % Specifically, we calculate the corresponding property values for the generated molecules using RDKit and compare them with the input properties to calculate correct ratio and Spearman correlation. 
%For correct ratio, we treat the generated molecule as a correct one if $\lvert v' - v \lvert \leq \delta$, where $v'$ is its property value and $v$ is the input value. $\delta$ is set to 0 for HBA, HBD, RotBonds, 0.05 for QED and FSP3, and 5 for TPSA. 
Our findings reveal that on certain property, such as TPSA, the model demonstrates a Spearman correlation greater than 0.8, illustrating the consistency between the generated molecules and the input specifications (Fig. \ref{fig:property2mol}b).
\iffalse
\begin{figure}
    \centering
    \includegraphics[width=0.6\linewidth]{figures/property2mol.png}
    \caption{property-to-molecule generation \textcolor{blue}{check novel only; tspa relaxed}}
    \label{fig:property2mol}
\end{figure}
\fi

Additionally, our model can handle the combination of multiple properties. For example, when given the command ``\texttt{Generate a compound with QED 0.5 and TPSA 40}'', the model generates compounds that meet both specified criteria. The results are shown in Fig. \ref{fig:property2mol}c. The majority of the generated compounds have QED and TPSA values centered around our desired properties (i.e., 0.5 and 40), demonstrating the versatility and effectiveness of \ourM{} in multi-property molecular generation. 


\begin{figure}[!htbp]
    \centering
    \includegraphics[width=\linewidth]{figures/prop2mol.pdf}
    \caption{Evaluation of property-to-molecule generation. (a) Bar plot of the Spearman correlation coefficients between the input property values and generated molecules' property values. (b) Violin plot showing the input TPSA values and generated molecules' TPSA values. More properties are left in Fig. \ref{fig:basic_to_cmpd_violinplot}. (c) The joint distribtion of the generated molecules' TPSA and QED given the input ``TPSA=40, QED=0.5'' (see Fig. \ref{fig:qed_fsp3_joint_optim} for more cases).}
    \label{fig:property2mol}
\end{figure}


\subsection{Translation between SMILES and IUPAC}\label{sec:smiles_iupac}
We evaluate NatureLM on the translation between SMILES and IUPAC on NC-I2S and NC-S2I \cite{yu2024llasmol}, the bidirectional IUPAC-SMILES translation dataset comprising 2993 pairs of SMILES and their corresponding IUPAC names (Table \ref{tab:text_to_cmpd_eval}). We ensure that there is no test set leakage in this setting. On both text-to-SMILES and SMILES-to-text translation tasks, NatureLM (8x7B) outperforms all competing language models in terms of accuracy, demonstrating our model's strong learning capability for text-molecule correspondence. NatureLM significantly outperforms GPT-4 and Claude 3 Opus \cite{claude3}, strong generalist large language models (LLMs), highlighting the necessity of training on scientific data. Compared with another LLM trained on text and SMILES corpus LlaSMol$_{\rm Mistral}$ \cite{yu2024llasmol}, NatureLM also obtains significantly better performance. Moreover, NatureLM (8x7B) performs comparably with STOUT \cite{rajan2024stout},  the widely-used model trained specially for IUPAC-SMILES translation task, demonstrating NatureLM's potential as a scientific generalist in specific domains. The performance increases from NatureLM (1B) to NatureLM (8x7B), exhibiting the scaling benefits of larger models. A case study is presented in Fig. \ref{fig:case_study_iupac_to_smiles}, comparing \ourM{} with general large language models and highlighting the advantages of \ourM{} in scientific tasks.

\begin{table}[!htbp]
\centering
\begin{tabular}{lcc}
\toprule
& IUPAC-to-SMILES & SMILES-to-IUPAC\\
\midrule
STOUT &\textbf{0.735}&0.565\\ 
GPT-4&0.033&0\\ 
Claude 3 Opus & 0.177 & 0\\
%ChemLLM & 0 & 0\\
LlaSMol$_{\rm Mistral}$ & 0.701 & 0.290\\
\ourM{} (1B) & 0.476 & 0.284\\
\ourM{} (8B) & 0.679 & 0.517\\
\ourM{} (8x7B) & 0.704 & \textbf{0.607}\\
\bottomrule
\end{tabular}
\caption{IUPAC-SMILES translation results. Models are evaluated by top-5 accuracy.}
\label{tab:text_to_cmpd_eval}
\end{table}

\subsection{Target-aware hit generation and optimization}\label{sec:tamgen}
The task is to generate small molecule compounds given the target protein sequence. The combination of \ourM{} and structure-based compound design will be explored in the future. We test \ourM{} within two distinct scenarios: 

(1) Generate compounds from the target protein sequences. This process is crucial for the hit identification stage of drug discovery, with the goal of discovering  chemical entities that exhibit specific interactions with the target protein. 

(2) Generate molecular fragments based on the target protein sequences and partial molecular structures as inputs. This method is instrumental during the lead optimization phase, where we scrutinize and refine the molecular architecture to amplify efficacy and precision.

The examples are shown below:

\begin{example}  

\noindent$\rhd${\em Scenario 1}: Complete molecule generation

\noindent\textbf{Instruction: }\\Produce a compound guided by the target \\
\noindent\pro{}LALSLTADQMVSALL...SYDLLLEMLDAH\epro{} \\
\noindent\textbf{Response:}\mol{}CC1=C(c2cccc(O)c2)C(c2ccc(I)cc2)Oc2ccc(O)cc21\emol{}

%Produce a compound guided by the target <protein>LALSLTADQMVSALLDAEPPILYSEYDPTRPFSEASMMGLLTNLADRELVHMINWAKRVPGFVDLTSHDQVHLLECAWLEILMIGLVWRSMEHPGKLLFAPNLLLDRNQGKCVEGMVEIFDMLLATSSRFRMMNLQGEEFVCLKSIILLNSGVYTFTLKSLEEKDHIHRVLDKITDTLIHLMAKAGLTLQQQHQRLAQLLLILSHIRHMSNKGMEHLYSMKCKNVVPSYDLLLEMLDAH</protein>.	<mol>CC1=C(c2cccc(O)c2)C(c2ccc(I)cc2)Oc2ccc(O)cc21</mol>

\noindent$\rhd${\em Scenario 2}: Fragment generation

\noindent\textbf{Instruction: }\\Design a compound with reference to the target \\
\pro{}DTKEQRILR$\cdots$EKAIYQGP\epro{} and the fragment  $\langle$\texttt{fragA}$\rangle$O=c1[nH]cnc2c(O)cc([*:1])c([*:2])c12$\langle$\texttt{/fragA}$\rangle$\\
        \textbf{Response: }\\$\langle$\texttt{fragB}$\rangle$Fc1ccc([*:1])cc1.Fc1ccc([*:2])cc1$\langle$\texttt{/fragB}$\rangle$
\end{example}  

Here, ``[*:digit]'' refers to the connection point of the molecular fragment, like the R1 and R2 in Fig. \ref{fig:target_to_frag}.

In the first scenario, we compare \ourM{} with a sequence generation method, TamGen \cite{TamGen}, and two approaches that design compounds in 3D space based on the input target: a diffusion-based method, TargetDiff \cite{targetdiff}, and an autoregressive generation method in 3D space, Pocket2Mol \cite{pocket2mol}. We follow the evaluation procedure outlined in the TamGen paper \cite{TamGen}, which includes calculating the docking score using AutoDock Vina, as well as assessing the QED, synthetic accessibility scores (SAS), diversity of the generated compounds, the percentage of compounds with logP in the range [0,5], and the percentage of compounds satisfying the rule-of-five. The results are presented in Table \ref{tab:tamgen_results}. We can see that in terms of docking score, QED and synthesis ability, \ourM{} surpasses previous baselines, highlighting its effectiveness.  

% backup data; do not delete
% \begin{table}[!htbp]
% \centering
% \begin{tabular}{lccccccc}
% \toprule
% & Vina & QED & SAS & Diversity & LogP$\in[0,5]$ & Ro5  \\
% \midrule
% Pocket2Mol    & -4.90 & 0.52 & 0.84 & 0.87 & 0.76 & 1     \\
% TargetDiff    & -6.08 & 0.55 & 0.67 & 0.83 & 0.74 & 0.98  \\
% TamGen        & -6.66 & 0.56 & 0.76 & 0.75 & 0.84 & 0.99  \\
% NatureLM (1B)  & -6.86 & 0.62 & 0.82 & 0.74 & 0.84 & 0.99  \\
% NatureLM (8B)  & -6.94 & 0.63 & 0.82 & 0.65 & 0.86 & 0.99  \\
% NatureLM (8x7B)& -6.84 & 0.62 & 0.82 & 0.70 & 0.84 & 0.99 \\
% \bottomrule
% \end{tabular}
% \caption{Statistics of target to complete compound sequence generation.}
% \label{tab:tamgen_results}
% \end{table}

\begin{table}[!htbp]
\centering
\begin{tabular}{lccccccc}
\toprule
& Vina ($\downarrow$) & QED & SAS & Diversity & LogP$\in[0,5]$ & Ro5  \\
\midrule
Pocket2Mol    & -4.90 & 0.52 & \textbf{0.84} & \textbf{0.87} & 0.76 & \textbf{1}     \\
TargetDiff    & -6.08 & 0.55 & 0.67 & 0.83 & 0.74 & 0.98  \\
TamGen        & -6.66 & 0.56 & 0.76 & 0.75 & 0.84 & 0.99  \\
\ourM{} (1B)  & -6.80 & \textbf{0.64} & 0.82 & 0.77 & \textbf{0.85} & 0.99  \\
\ourM{} (8B)  & -6.92 & 0.62 & 0.81 & 0.73 & 0.84 & 0.99  \\
\ourM{} (8x7B)& \textbf{-6.95} & 0.62 & 0.82 & 0.75 & 0.84 & 0.99 \\
\bottomrule
\end{tabular}
\caption{Statistics of target to complete compound sequence generation.}
\label{tab:tamgen_results}
\end{table}

Additionally, we utilize \ourM{} for fragment generation. We selected three papers published after May 2024 \cite{Tangallapally2024pdb6PE6,Tarr2024pdb9BCG,Mammoliti2024pdb3xln}, where part of their task is to solve the issue of compound optimization. In this context, the input includes a target protein and a backbone that needs optimization. The results are illustrated in Fig. \ref{fig:target_to_frag}. In this instance, it is evident that larger models typically yield superior docking scores.
%, contrasting with the full molecular generation where the three models show minimal variation. Target-aware fragment generation is a more intricate process, necessitating the model to engineer specific molecular fragments that not only bind to the provided target but also complement a given backbone. In contrast, complete molecule generation mainly requires the model to identify overall patterns and common molecular structures that interact with the specified protein. Larger models possess a greater capacity to comprehend complex relationships and subtle patterns necessary for fragment generation.


    


\begin{figure}[!htbp]
\centering
\includegraphics[trim=1cm 0 1cm 0, clip, width=\linewidth]{figures/fragment_docking3_rdkit.pdf}
\caption{Violin plot of docking scores for molecules in target-to-fragment generation.  This violin plot presents the docking scores of molecules involved in target-to-fragment generation. We selected three recent papers that focus on fragment optimization: \cite{Tarr2024pdb9BCG}, \cite{Mammoliti2024pdb3xln} and \cite{Tangallapally2024pdb6PE6}, which utilize PDB IDs 9BCG, 3LXN, and 6PE6, respectively. The input fragment is visualized alongside its corresponding PDB ID for clarity.}
\label{fig:target_to_frag}
\end{figure}

\subsection{Text-guided binding affinity optimization}\label{sec:binding_affinity}
To further improve the binding affinity between a target and a molecule, we propose a text-guided binding affinity optimization task. Given a target name and a molecule with a known binding affinity for that target, we aim to generate molecules with higher binding affinity, which is crucial for lead optimization. An example is shown below:
\begin{example}
    \noindent\textbf{Instruction: }\\\texttt{Improve the binding affinity on Uridine-cytidine kinase 2 of }\mol{}Cc1ccc(-c2nc3c(c(SCC(=O)Nc4ccccc4)n2)Cc2cccc(C)c2O3)cc1\emol{}\\
    \textbf{Response: }\\\mol{}Cc1ccc(-c2nc3c(c(SCC(=O)Nc4cccc(C(=O)O)c4)n2)Cc2cccc(C)
    c2O3)cc1\emol{}
\end{example} 
Here, the target information is provided in text format, which complements the FASTA representation used in Section \ref{sec:tamgen}. We will combine them in the future. 


We test \ourM{} on 12 targets that are not present in the post-training data and use a hybrid retrieval and docking approach for evaluation. Specifically, for the generated molecules, if we can retrieve their binding affinity values from the ChEMBL database, we compare these values with the original molecule's binding affinity. Otherwise, we compare their docking scores with the original molecule. For the 12 selected targets, their Spearman correlation between the docking score and the actual binding affinity for known molecules exceeds 0.5, indicating the reliability of using docking for assessment (Table \ref{tab:targets}).


\iffalse
\begin{table}[!htbp]
\centering
\begin{tabular}{lcc}
\toprule
Model & Correct ratio\\
\midrule
GPT-4 & 0.436\\
NatureLM (1B) & 0.486\\
NatureLM (8B) & 0.542\\
NatureLM (8x7B) & 0.563\\
\bottomrule
\end{tabular}
\caption{Correct ratios of GPT-4 and NatureLM for binding affinity optimization.}
\label{tab:binding_affinity_optimization}
\end{table}
\fi

We observe that \ourM{} can successfully improve the molecule's binding affinity by making small modifications on its chemical components, much like what a chemist would typically do (Fig. \ref{fig:affnity}b). Compared with GPT-4, \ourM{} can generate more molecules (Fig. \ref{fig:affnity}a) with higher binding affinity, making it a better tool for molecule optimization than general domain LLM. Another observation is that more than 90\% molecules generated by \ourM{} do not have known binding affinity score in ChEMBL database. For 8 out of the 12 targets, over 50\% of the generated novel molecules successfully decreased the docking scores (Fig. \ref{fig:binding_docking}),  demonstrating the model's potential in exploring chemical spaces and discovering new drugs. We observe that \ourM{} (8x7B) and \ourM{} (8B) outperform \ourM{} (1B) as they generate more correct molecules for the majority of targets (Fig. \ref{fig:binding_correct}).

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/binding_new5.pdf}
    \caption{Evaluation of text-guided binding affinity optimization. (a) Box plot of the correct ratios of GPT-4, NatureLM (1B), NatureLM (8B) and NatureLM (8x7B) on 12 targets. (b) A case on the binding affinity optimization for Catechol O-methyltransferase. With small modifications, the binding affinity improves from 410nM to 53nM.}
    \label{fig:affnity}
\end{figure}


\subsection{Text-guided metabolism \& distributional property optimization}\label{sec:admet}

We next assess the molecular optimization capabilities of \ourM{} regarding metabolism and its potential to cross the blood-brain barrier (BBB), a crucial factor in drug distribution. For metabolism, the goal is to reduce the inhibition of five key metabolic enzymes: CYP1A2, CYP2C19, CYP2C9, CYP2D6 and CYP3A4. An example is shown below:

\begin{example} 
{{
\small
\textbf{Instruction: }\\\texttt{Transform} \mol{}COCCn1c(=O)c(-c2ccccc2)nc2cnc(N3CCOCC3)nc21\emol{} \texttt{to yield lower CYP1A2 inhibition and present a better metabolic safety record.}\\
    \textbf{Response: }\\\mol{}COc1ccc(-c2nc3cnc(N4CCOCC4)nc3n(Cc3cccc(OC)c3)c2=O)cc1\emol{}
}}
\end{example}  

In terms of BBB permeability (BBBP), we evaluate the enhancement BBB permeability. An example is provided below:

\begin{example}
{{
\small
\noindent\textbf{Instruction:}\\ \texttt{Adjust the molecule} \mol{}CC[C@H](NC(=O)c1c(OCCCC(=O)O)c

\noindent(-c2ccccc2)nc2ccccc12)c1ccccc1\emol{} \texttt{to facilitate its passage through the blood-brain barrier.}
\newline
\noindent\textbf{Response:} \\\mol{}CC[C@H](NC(=O)c1c(O)c(-c2ccccc2)nc2ccccc12)c1ccccc1\emol{}
}}
\end{example}

For each test sample, we used random search to generate four cases. To determine whether NatureLM effectively refined the input molecule, we trained six groups of deep learning models for this evaluation. For assessing BBBP, we utilized the state-of-the-art model, BioT5 \cite{PeiQizhi2023BioT5}, to determine whether a compound is capable of crossing the BBB. For metabolism optimization, we used ChemProp \cite{yang2019analyzing} to train classifiers to test if a molecule has the ability to inhibit enzymes from the cytochrome P450 (CYP) superfamily. We evaluated the percentage of molecules that were successfully optimized according to the specified criteria (see Section \ref{app:more_eval_method} for details). 

Table \ref{tab:cyp_optimization} displays the outcomes of BBBP and metabolism optimization. The success rates for optimizing BBBP with the 1B, 8B, and 8x7B versions of NatureLM are 0.482, 0.549, and 0.552, respectively. Larger models show better performance, though the improvement is not substantial. This suggests potential for enhancement opportunities in the future. For metabolism optimization, generally, the 8B model outperforms the others in terms of success rate, followed by the 8x7B model and lastly the 1B model. The 1B and 8B models share the same architecture (dense models, large vocabulary size), whereas the 8x7B model has a distinct one (mixture-of-expert model, relative small vocabulary size). In this particular task, the progression from the 1B model to the 8B model is consistent. However, a detailed analysis contrasting the 8x7B model is to be conducted in subsequent studies. Additionally, we jointly optimized metabolism and a basic property. The findings indicate that larger models generally yield better results (see Table \ref{tab:joint_basic_cyp}).

\begin{table}[!htpb]
\centering
\resizebox{\columnwidth}{!}{
\begin{tabular}{lccccccc}
\toprule
 & BBBP & CYP1A2 & CYP2C19 & CYP2C9 & CYP2D6 & CYP3A4 & CYP Average \\
\midrule
1B  & 0.482 & 0.805 & 0.815 & 0.770 & 0.750 & 0.831 & 0.794\\
8B & 0.549 & \textbf{0.882} & 0.813 & \textbf{0.882} & \textbf{0.833} & \textbf{0.913} & \textbf{0.865}\\
8x7B & \textbf{0.552} &  0.837 & \textbf{0.834} & 0.838 & 0.812 & 0.853 & 0.835\\
\bottomrule
\end{tabular}
}
\caption{Optimization results of BBBP metabolism and CYP enzymes. Measured by success rate. }
\label{tab:cyp_optimization}
\end{table}


\subsection{Retrosynthesis prediction}\label{sec:retro}
Retrosynthesis aims to identify synthesis routes for target molecules using commercially available compounds  as starting points, a critical task in the discovery and manufacture of functional small molecules~\cite{corey1969computer, segler2018planning, maziarz2024chimera}.
The applicability of ML-based retrosynthesis tools largely depends on the accuracy of single-step retrosynthesis prediction.
We evaluate the capability of \ourM{} for single-step retrosynthesis prediction on USPTO-50K ~\cite{schneider2016uspto50k}.
\ourM{} is prompted with the task description and the chemical SMILES of the product molecule, and is expected to generate potential reactants. 

We followed the common practice for splitting the USPTO-50K dataset~\cite{dai2019retrosynthesis, maziarz2024re}, and evaluated the performance using the 5007 reactions included in the test set.
We ensured that there is no test set leakage in this setting.
% According to Table~\ref{tab:retro-uspto50k}, 
As outlined in Table~\ref{tab:retro-uspto50k},
all sizes of \ourM{} models surpass other methods in terms of top-$k$ accuracy, demonstrating our
model’s accurate predictive ability for retrosynthesis prediction.
% On the one hand, 
\ourM{} significantly outperforms GPT-4, a general LLM trained on human languages. 
This suggests that training on scientific data is crucial for models to excel in scientific tasks. 
% Compared with other text and SMILES corpus including ChemLLM [7]
% and LlaSMolMistral [5], NatureLM also obtains significantly better performance.
Furthermore, \ourM{} outperforms the state-of-the-art domain-specific models such as LocalRetro~\cite{chen2021localretro} and R-SMILES~\cite{Zhong2022rsmiles}, showing NatureLM’s potential as a scientific generalist in critical scientific tasks. We also note an increase in performance from \ourM{} (1B) to \ourM{} (8x7B), demonstrating the scaling advantages of larger models.

\begin{example}
{{
\small
        \textbf{Instruction: }\\
        \texttt{Please suggest possible reactants for the given product}
        $\langle$\texttt{product}$\rangle$CC(=O)c1ccc2c(ccn2C(=O)OC(C)(C)C)c1$\langle$\texttt{/product}$\rangle$ \\
        \textbf{Response: }\\ 
        $\langle$\texttt{reactant}$\rangle$
        CC(=O)c1ccc2[nH]ccc2c1.CC(C)(C)OC(=O)OC(=O)OC(C)(C)C
        $\langle$\texttt{/reactant}$\rangle$
        % }
}}
\end{example}



% \subsection{USPTO-50K}

% Instruction tuning. Training data: USPTO-50K train dataset - 40k reactions. From table~\ref{tab:retro-uspto50k}, we observe that xxx.

% \begin{table}[!htbp]
\begin{table}[!h]
\centering
\begin{tabular}{lcc}
\toprule
& Top-1 accuracy & Top-3 accuracy \\
\midrule
GPT-4 & 22.4\% & N/A \\
LocalRetro~\cite{chen2021localretro} & 51.5\% & 76.5\% \\ 
R-SMILES~\cite{Zhong2022rsmiles} & 56.0\%  & 79.1\% \\
EditRetro~\cite{han2024editretro} & 60.8\% & 80.6\% \\ 
\midrule
\ourM{} (1B) & 68.6\% & 86.8\%\\
\ourM{} (8B) & 70.2\% & 85.9\% \\
\ourM{} (8x7B) & 71.9\% & 87.4\% \\
\bottomrule
\end{tabular}
\caption{Retrosynthesis prediction results on USPTO-50K dataset.}
% \caption{Retrosynthesis prediction results on USPTO-50K dataset. }
\label{tab:retro-uspto50k}
\end{table}


% For our case study, we selected a reaction from a U.S. patent with ID US12018024B2, granted to Novartis on June 25, 2024. As shown in Figure~\ref{fig:case_study_reaction}, the product is a brominated heterocyclic compound. In synthesizing such molecules, organic chemists typically begin by constructing the ring system from the substrate, followed by a halogenation reaction on the ring, which is subsequently used in further syntheses, often for coupling with other ring systems. In this context, the NatureLM model accurately predicted the use of the classic brominating agent NBS (N-Bromosuccinimide) for this step, aligning perfectly with experimental results. NBS is preferred for its ability to perform selective bromination, especially when mild and controlled halogenation is required. This demonstrates NatureLM's capability to effectively predict useful reactants in chemical reactions.


% In contrast, both the DeepSeek-R1 and o3-mini-high models selected incorrect reactive sites. DeepSeek-R1 focused on constructing a heterocyclic ring system; however, it incorrectly positioned the nitrogen atom in the substrate, preventing the reaction from yielding the desired outcome. Meanwhile, o3-mini-high attempted to construct the acetyl side chain, but in the actual synthesis, this step should occur earlier. Introducing the side chain at this stage not only decreases efficiency but also leads to a side reaction where the bromine atom on the opposite side is substituted. These cases illustrate that, so far, general LLMs cannot fully grasp the rationale behind chemical synthesis. Although they can generate some reasons and predictions, these do not align with chemist strategies.

\begin{figure}[!htbp]
\centering
\includegraphics[width=0.8\linewidth]
{figures/NatureLM_retro_example1_rdkit.pdf}
% {figures/NatureLM_retro_example1.pdf}
\caption{Case study on retrosynthesis prediction. We evaluated the performance of \ourM{}, DeepSeek-R1, and o3-mini-high for retrosynthesis prediction using a reaction from U.S. Patent ID US12018024B2 (not included in our training set), granted to Novartis on June 25, 2024. \ourM{} successfully proposed the ground-truth reactants from the patent, while the outputs from the other two methods required further refinement to achieve the same level of accuracy.
%The SMILES representation of the product is CCOC(=O)c1cc2c(Br)cnn2cn1, while the smiles representations of the reactants are CCOC(=O)c1cc2ccnn2cn1 and BrN1C(=O)CCC1=O.
}
\label{fig:case_study_reaction}
\end{figure}

We selected a reaction from a U.S. patent with ID US12018024B2 (granted to Novartis on June 25, 2024) as the case study. As shown in Figure~\ref{fig:case_study_reaction}, the product is a brominated heterocyclic compound. To synthesize such molecules, organic chemists typically begin by constructing the ring system, then followed by a halogenation reaction on the ring. The halogenated site is subsequently used in further syntheses, often coupling with other ring systems. In this context, our NatureLM model accurately predicted one of the most common brominating agent NBS (\textit{N}-Bromosuccinimide) in this step, aligning perfectly with experimental results. DFT optimization calculations to reactant CCOC(=O)c1cc2ccnn2cn1 show that 3-position is the most nucleophilic. So bromination prefers at 3-position. This demonstrates NatureLM’s capability to effectively predict useful reactants in chemical reactions.

In contrast, DeepSeek-R1 \cite{deepseekai2025r1} model selected incorrect reactants. DeepSeek-R1 focused on constructing a heterocyclic ring system in this step. However, it incorrectly positioned the nitrogen and carbon atoms in the substrate, leading to the wrong outcome. Meanwhile, o3-mini-high model selected another correct route but not so convenient for the whole synthetic process. o3-mini-high might attempt to construct the acetyl side chain by transition-metal-catalyzed CO insertion reaction. Although radical mechanism can achieve this since iodo atom is more reactive than bromo atom, introducing the side chain at this stage may still lead to a side reaction at the bromo atom. In the actual synthesis, this acetyl side chain is generally constructed in previous steps. These cases illustrate that, so far, general LLMs cannot fully grasp the rationale behind chemical synthesis. Although they can generate some reasons and predictions, these do not align with common chemist strategies.



Another example is shown in Fig. \ref{fig:case_study_reaction2}, where \ourM{} accurately predicts the reactants, while the outputs from DeepSeek-R1 and o3-mini-high require additional refinement.