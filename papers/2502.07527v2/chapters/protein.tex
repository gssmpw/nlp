\section{Protein tasks}
\label{sec:protein}

Our model's capabilities with respect to proteins are assessed through several distinct types of tasks:

\begin{enumerate}
\item Unconditioned protein generation: The model generates protein sequences from scratch without any specific conditions or prompts.
\item Text-guided protein generation: This task involves guiding the model to generate protein sequences based on given natural language descriptions.
% \item Protein Understanding - Classification and Regression: The model utilizes its generative abilities to perform understanding tasks, specifically classification and regression.
\item Antibody design: The model designs the Complementary-Determining Region H3 (CDR-H3) of antibodies to effectively bind to target antigens.
\item Protein description generation: This task focuses on generating explanations or uncovering properties and functions of protein sequences, articulating them in natural language.
\end{enumerate}

\subsection{Unconditioned generation}\label{sec_prot_generation}

The first capability of the model is generating protein sequences from scratch freely, prompted by the start token for proteins only, i.e., \text{$\langle$protein$\rangle$}. 
However, since there is no golden standard for evaluating proteins when no conditions are specified, it is difficult to measure the generation results. We focus on foldability, measured by pLDDT score \cite{Mariani2013-av}, as well as lengths and diversity of the sequences, for the valid sequences.

\begin{table}[!h]
\centering
\begin{tabular}{lccc}
\toprule
Model & Avg Length & Diversity & AVG pLDDT \\
\midrule
Mixtral 8x7b & 53.3 & 0.906 & 69.9 \\
GPT-4 & 45.7 & 0.816 & 65.1 \\
\midrule
\ourM{} (1B) & 288.3 & 0.985 & 69.8 \\
\ourM{} (8B) & 284.5 & 0.973 & 71.8 \\
\ourM{} (8x7B) & 318.4 & 0.989 & 75.9 \\
\bottomrule
\end{tabular}
\caption{Protein Sequence Generation Comparison. The average length of natural proteins (calculated from a subset of proteins randomly sampled from UR50) is about 311. The diversity was calculated by the number of clusters with 50\% sequence identity divided by the total generated sequence count. The pLDDT scores were calculated by OmegaFold \cite{omegafold} on the generated sequences with length less than 100 for a fair comparison. The length distribution is left in Figure \ref{fig:protein:unconditioned_generation_sequence_length}. }
\label{tab:protein:unconditioned_generation}
\end{table}

As shown in Table \ref{tab:protein:unconditioned_generation}, \ourM{} consistently outperform Mixtral 8x7b and GPT-4 in terms of average sequence length, diversity, and average pLDDT score. The \ourM{} (8x7B) model achieves the best performance across all metrics, with an average length of 318.4, diversity of 0.989, and average pLDDT score of 75.9. ProLLAMA~\cite{lv2024prollamaproteinlanguagemodel} a fine-tuned LLM for protein. It generates proteins without explicitly defined constraints on length, achieving a pLDDT score of 66.5. In contrast, our approach, which does not impose length constraints, results in pLDDT scores of 69.8 and 78.1 for the 8B and 8x7B models, respectively, demonstrating our significant advancement in this area.



\subsection{Text-guided protein generation}\label{sec:text_guided_protein_design}

% For text-guided protein generation, we evaluated our models' ability to generate proteins with specific properties based on natural language prompts. We have selected solubility and stability for this assessment and leave more properties as future work. In terms of stability, the models are tasked with generating stable protein sequences. Regarding solubility, given the prevalence of both soluble and insoluble proteins in natural sequences, we have instructed \ourM{} to generate both types of sequences. Exemplary prompts are depicted in Figure \ref{fig:protein:conditioned_prompts} while a comprehensive list of prompts can be found in Figure \ref{fig:protein:conditioned_prompts_full}. 

For text-guided protein generation, we evaluated our models' ability to generate proteins with specific properties based on natural language prompts. In this study, we focused on two key properties: solubility and stability, leaving the exploration of additional properties for future work.
%
For stability, the models were tasked with generating protein sequences that exhibit stable properties. Regarding solubility, since both soluble and insoluble proteins are common in natural sequences, we instructed \ourM{} to generate sequences of both types. 
% To build the dataset used by our models, we adapted prompt templates with data from the PEER benchmark~\cite{xu2022peer}, utilizing only the training data during instruction tuning. 
Sample prompts are shown below, and a full list of prompts can be found in Figure~\ref{fig:protein:conditioned_prompts_full}.

\begin{example} 
$\rhd$ An example prompt for ``stable protein generation''\\
\texttt{I require a stable protein sequence, kindly generate one.}\\
$\rhd$ An example prompt for ``soluble protein generation''\\
\texttt{Generate a soluble protein sequence.}\\
$\rhd$ An example prompt for ``insoluble protein generation''\\
\texttt{Produce a protein sequence that is not soluble.}
\end{example} 

To evaluate the stability and solubility of a generated protein sequence, we utilized two specialist models fine-tuned from the protein foundation model, SFM-Protein~\cite{he2024sfm}, as oracle models. One model was used for stability classification, while the other was used for solubility classification. %  (see Section \TODO{xxxx} for details)
The oracle models provide probabilities that suggest the likelihood of the sequence possessing the desired property. To verify the efficiency of our model against random sampling, we have also chosen a subset of 1000 natural protein sequences from the UR50 dataset and assessed them using the same oracle models.

% \begin{figure}[h]  
%     \centering  
%     \begin{mdframed}[backgroundcolor=white, linecolor=black, linewidth=1pt]  
%     \textbf{Instruction:}
%     \textit{Produce a protein sequence that is soluble.} \\
%     \textbf{Response:} \\
%     \\
%     \underline{\text{$\langle$protein$\rangle$}\text{MSLSELSLQL \ldots KGVLVNK}\text{$\langle$/protein$\rangle$}}
%     \end{mdframed}  
%     \caption{Templates for conditioned generation} \label{fig:protein:conditioned_example}
% \end{figure} 

\begin{figure}[!htbp]
\centering
\subfigure[\ourM{} (1B)]{
% \includegraphics[width=0.33\linewidth]{figures/SFM-Seq_1B.stability.pdf}
\includegraphics[width=0.33\linewidth]{figures/NatureLM_1B.stability.pdf}
}%
\subfigure[\ourM{} (8B)]{
% \includegraphics[width=0.33\linewidth]{figures/SFM-Seq_8B.stability.pdf}
\includegraphics[width=0.33\linewidth]{figures/NatureLM_8B.stability.pdf}
}%
\subfigure[\ourM{} (8x7B)]{
% \includegraphics[width=0.33\linewidth]{figures/SFM-Seq_8x7B.stability.pdf}
\includegraphics[width=0.33\linewidth]{figures/NatureLM_8x7B.stability.pdf}
}
\caption{Stability score distribution of the generated sequences.}
\label{fig:protein:conditioned_generation_stability}
\end{figure}

\begin{table}[!h]
\centering
\begin{tabular}{ccc}
\toprule
Source & AVG Prediction & Data Ratio (Score $>0.5$) \\
\midrule
Natural & 0.552 & 0.704 \\
\ourM{} (1B) & 0.559 & 0.644 \\
\ourM{} (8B) & 0.619 & 0.757 \\
\ourM{} (8x7B) & 0.655 & 0.812 \\
\bottomrule
\end{tabular}
\caption{Stability score ratio of the generated sequences.}
\label{tab:protein:conditioned_generation_stability}
\end{table}

\begin{figure}[!htbp]
\centering
\subfigure[\ourM{} (1B)]{
% \includegraphics[width=0.33\linewidth]{figures/SFM-Seq_1B.solubility.pdf}
\includegraphics[width=0.33\linewidth]{figures/NatureLM_1B.solubility.pdf}
}%
\subfigure[\ourM{} (8B)]{
% \includegraphics[width=0.33\linewidth]{figures/SFM-Seq_8B.solubility.pdf}
\includegraphics[width=0.33\linewidth]{figures/NatureLM_8B.solubility.pdf}
}%
\subfigure[\ourM{} (8x7B)]{
% \includegraphics[width=0.33\linewidth]{figures/SFM-Seq_8x7B.solubility.pdf}
\includegraphics[width=0.33\linewidth]{figures/NatureLM_8x7B.solubility.pdf}
}
\caption{Solubility score distribution of the generated sequences.}
\label{fig:protein:conditioned_generation_solubility}
\end{figure}

% \begin{table}[!h]
% \centering
% \begin{tabular}{c|c|c|c}
% \hline
% Source & AVG Prediction & Score $>0.2$ & Score $>0.5$ \\
% \hline
% Natural & 0.221 & 0.452 & 0.054 \\
% SFM-Seq (1B) [Insoluble] & 0.203 & 0.409 & 0.131 \\
% SFM-Seq (8B) [Insoluble] & 0.229 & 0.479 & 0.151 \\
% SFM-Seq (8x7B) [Insoluble] & 0.207 & 0.436 & 0.112 \\
% SFM-Seq (1B) [Soluble] & 0.467 & 0.845 & 0.372 \\
% SFM-Seq (8B) [Soluble] & 0.539 & 0.900 & 0.493 \\
% SFM-Seq (8x7B) [Soluble] & 0.515 & 0.868 & 0.461 \\
% \hline
% \end{tabular}
% \caption{Solubility score ratio of the generated sequences}
% \label{tab:protein:conditioned_generation_stability}
% \end{table}

Figures \ref{fig:protein:conditioned_generation_stability} and \ref{fig:protein:conditioned_generation_solubility} show the distributions of stability and solubility scores for the generated sequences, respectively. The \ourM{} models demonstrate controlled distribution shift in generating proteins with desired properties compared to the natural sequences. 
%
In the task of generating more stable proteins, as shown in Figure~\ref{fig:protein:conditioned_generation_stability}, a clear trend emerges: as the model size increases, the proportion of sequences classified as stable grows, with a pronounced peak in the \ourM{} (8x7B) results. The quantified data, summarized in Table~\ref{tab:protein:conditioned_generation_stability}, further supports this observation. All three models produce proteins that are more stable than natural sequences based on average stability scores. Additionally, two of the models outperform natural proteins in terms of the number of sequences that exceed a stability threshold of 0.5.
%
For the solubility condition, Figure~\ref{fig:protein:conditioned_generation_solubility} reveals a similar trend. As the model size increases, the separation between the distributions of soluble and insoluble scores becomes more distinct, with less overlap. 
% Given that the oracle model tends to classify proteins as insoluble, it is notable that the SFM-Seq (8x7B), when conditioned on solubility, generates sequences with solubility scores that are consistently higher than the median solubility score of natural sequences.

\subsection{Antigen-binding CDR-H3 design}

The task of antigen-binding CDR-H3 design focuses on constructing the Complementary-Determining Region H3 (CDR-H3) of an antibody to bind effectively to a target antigen. We employed the RAbD benchmark dataset~\cite{adolf2018rabd}, comprising 60 antibody-antigen complexes. The example is shown below:

\begin{mdframed}
\noindent\textbf{Instruction: }\\
\texttt{Using antigen} \pro{}TQVCTGTDMKLR$\cdots$GESSEDCQS\epro{} \texttt{and antibody frameworks} \ant{}IVLTQTPS$\cdots$LAVYYC\eant{} \texttt{and} \ant{}FGGGTRLEIEVQ\eant{}, \texttt{create the CDR3 regions.}\\
\textbf{Response: }\\ 
\ant{}QQYSNYPWT\eant{}
\end{mdframed}  



The generation quality is evaluated by the Amino Acid Recovery (AAR) scores for the CDR-H3 design task. We use $r$ and $\hat{r}$ to represent the reference and generated sequences respectively, while $L(r)$ and $L(\hat{r})$ denote the number of amino acids in $r$ and $\hat{r}$. The $i$-th residue in the two sequences is denoted by $r_i$ and $\hat{r}_i$. The AAR is defined as follows:
\begin{equation}
{\rm AAR}(r,\hat{r}) = \frac{1}{L(r)}\sum_{i=1}^{L(r)}\mathbb{I}(r_i = \hat{r}_i). 
\end{equation}
In case $L(\hat{r})>L(r)$, only the first $L(r)$ elements are verified. If $L(\hat{r})<L(r)$, we assign $\mathbb{I}(r_i = \hat{r}_i)=0$ for $i>L(\hat{r})$.


\begin{table}[!h]
\centering
\begin{tabular}{lc}
\toprule
Method & AAR ($\uparrow$) \\
\midrule
GPT-4 & 0.312 \\
RefineGNN~\cite{jin2021refinegnn} & 0.298 \\
HSRN~\cite{jin2022hsrn} & 0.327 \\
MEAN~\cite{kong2022mean} & 0.368 \\
ABGNN~\cite{gao2023abgnn} & 0.396 \\
% SFM-Protein \cite{he2024sfm} & 0.549\\
\midrule
Llama 3 (8B) & 0.275 \\
\ourM{} (1B) & 0.273 \\
\ourM{} (8B) & 0.368 \\
\ourM{} (8x7B) & 0.376 \\
\bottomrule
\end{tabular}
\caption{AAR of the CDR-H3 design. Please note that the \ourM{} models utilize sequence-only input for this task, whereas the baseline models may incorporate additional information, such as structural data.}
\label{tab:protein:antibody:cdr3GenGiven_antigen}
\end{table}

Table \ref{tab:protein:antibody:cdr3GenGiven_antigen} presents the Amino Acid Recovery (AAR) scores for the CDR-H3 design task. As the model size of \ourM{} increase, the AAR gradually increases. The \ourM{} (8x7B) model achieves competitive performance with an AAR of 0.376, outperforming several specialized GNN-based models. While SFM-protein, a BERT-like model trained on protein sequences, holds the top performance, our results demonstrate the potential of \ourM{} in CDR-H3 design, particularly as the model scales and undergoes further refinement.

\subsection{Protein description generation}\label{sec:protein_to_desc}
Despite the rapid discovery of natural protein sequences facilitated by advanced sequencing techniques, the functions of many of these proteins remain largely unknown. This knowledge gap restricts our ability to exploit these proteins for engineering and therapeutic purposes. In this study, we explored the annotation generation capabilities of the \ourM{} series.

To achieve this, we compiled pairs of protein sequences and their human-readable annotations from various species, sourced from the NCBI database. We divided the dataset temporally: historical data were utilized for training the \ourM{} models, while annotation data from the most recent four months were reserved for testing. Model performance was evaluated using Rouge-L scores. As shown in Table \ref{tab:protein:ncbi_description}, \ourM{} models consistently outperformed Llama 3 8B in Rouge-L scores, with performance differences widening as model size increased. Notably, the \ourM{} (8x7B) model achieved the highest score of 0.585. A detailed analysis presented in Figure \ref{fig:protein:protein_understanding} revealed that the \ourM{} (8x7B) model not only generates protein annotations with greater accuracy but also successfully identifies orthologues and functions of proteins, while \ourM{} (8B) is also able to generate reasonable results in many cases.



\begin{table}[!htbp]
\centering
\begin{tabular}{lc}
\toprule
Model Setting & Rouge-L\\
\midrule
Fine-tuned Llama 3 (8B) & 0.324 \\
\ourM{} (1B) & 0.548 \\
\ourM{} (8B) & 0.572 \\
\ourM{} (8x7B) & 0.585 \\
\bottomrule
\end{tabular}
\caption{Performance of protein description generation, measured by Rouge-L. Llama 3 (8B) serves as a baseline, which is directly fine-tuned on the data collection described in Section \ref{sec:supervised_ft_data}. More details about this baseline in Section \ref{sec:ablation_study}.}
\label{tab:protein:ncbi_description}
\end{table}

\begin{figure}[!htbp]
\centering
\includegraphics[trim=3cm 2cm 7cm 1cm, clip, width=0.85\linewidth]{figures/proteinAnnotation_v3.pdf}
\caption{Generated descriptions for different proteins.}
\label{fig:protein:protein_understanding}
\end{figure}


In conclusion, \ourM{} demonstrates strong performance across a wide range of protein-related tasks, from unconditioned generation to specific design tasks like CDR-H3 design. The scalability of our approach is evident, with larger models consistently outperforming smaller versions and often achieving state-of-the-art results.


\subsection{Heme-binding protein design driven by text and SMILES}\label{sec:heme_case_study}
\ourM{} is capable of designing proteins to meet various requirements. In this case study, we demonstrate how to use \ourM{} to design a protein that binds to heme cofactor.

Heme is an essential molecule, playing critical roles in oxygen transport and various enzymatic processes. Designing a protein that can bind to heme can contribute to advancements in therapeutic development, biochemical research, and synthetic biology.

The protein design practice is outlined as the following 4 steps:

\noindent{\textbf{Step 1: protein generation}}. We prompt \ourM{} to generate proteins using two approaches:
\begin{enumerate}
\item Text-based protein design, where we provide a text instruction (here in English);
\item Small-molecule-based protein design, where we use the SMILES representation of heme as part of the input instruction.
\end{enumerate}

Examples of these prompts and their corresponding responses are shown below:

\begin{example}
{{
\small
\noindent$\rhd$ {Generate proteins based on text} (Section \ref{sec:text_guided_protein_design})

\noindent\textbf{Instruction:} \texttt{Design a heme-binding protein sequence.} 

\noindent\textbf{Response:} \pro{}MSAAEGAVVFSEEKEALVLK$\cdots$\epro{}


\noindent$\rhd$ { Generate proteins based on the SMILES of the heme}

\noindent\textbf{Instruction:} \texttt{Generate a protein that can bind to} \\
\mol{}[Fe+2].C=CC1=C(C)c2cc3[n-]c(cc4[n-]c(cc5nc(cc1n2)C(C)$\cdots$\emol{}

\noindent\textbf{Response:} \pro{}ETIGKRVFVHYCHGCHSQNALGI$\cdots$\epro{}
}}
\end{example}

\noindent{\textbf{Step 2: description generation}}. For each generated protein, we utilize the protein-to-text functionality in \ourM{} (Section \ref{sec:protein_to_desc}), to obtain a description of the properties and potential functions of the generated protein.

\noindent{\textbf{Step 3: screen proteins through keyword matching}}. In this step, we use GPT-4o to generate a keyword list, called \texttt{HemeList}, containing characteristics associated with heme-binding proteins. For every protein description generated in Step 2, we check whether it contains keywords from \texttt{HemeList}. If a description matches these criteria, the corresponding protein is added to a list called \texttt{HemeProtein}.

\noindent{\textbf{Step 4: structure generation and validation}}. For each protein in \texttt{HemeProtein}, we use Protenix~\cite{Protenix2025} to predict the complex structure of the generated proteins bound to heme. The predicted structures are then inspected to ensure that the proteins can form the critical interaction with heme for stable binding.

\begin{figure}[!htpb]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/heme_showCase.pdf}
    \caption{Two examples of proteins with plausibility of binding to heme. The yellow models represent the generated protein structures, while the blue models correspond to the reference structures retrieved using the built-in ``blast protein'' function in ChimeraX \cite{chimerax2023}. In each model, the heme binding region is highlighted by showing the nearby residues in stick representations. We use the protein-to-text functionality of \ourM{} to generate  functional annotations for these proteins, and the original outputs are provided here: (left protein) ``Heme-binding protein''; (right protein) ``Transfers electrons from cytochrome c551 to cytochrome oxidase; C-type cytochrome; Part of the cbb3-type cytochrome c oxidase complex.''}
    \label{fig:heme_bind_prot}
\end{figure}

In the text-based design case study (Fig.\ref{fig:heme_bind_prot} left), two histidine residues are positioned in close proximity to the iron located in the center of heme, enabling the formation of coordinated bonding interactions with the heme group. Similarly, in the SMILES-based design, the algorithm can output proteins with binding motifs similar to those generated in the text-based example (Fig. \ref{fig:SI:moreHemeCases}). However, as shown in Fig. \ref{fig:heme_bind_prot} (right), we show a representative case where a methionine and histidine residue are observed to interact closely with the iron ion (see Fig. \ref{fig:SI:prot_hem_hec} for more discussion on this case). These residues effectively coordinate the metal ion through their respective side chains, demonstrating alternative structural strategies for heme binding. Furthermore, the designed protein sequences differ significantly from those present in the database, indicating that our approach can generate novel sequences with distinct structural properties. Collectively, these results demonstrate the effectiveness of \ourM{} in designing functional heme-binding proteins with diverse and novel structural features. We also compare the apo and holo structures of the generated proteins in Fig. \ref{fig:compare_apo_holo}, which shows that the key residues involved in heme binding, such as histidine and methionine, occupy similar positions in both structures.










