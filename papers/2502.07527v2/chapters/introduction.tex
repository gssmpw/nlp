\section{Introduction}
\label{sec:introduction}
Foundation models, including the GPT \cite{brown2020languagemodelsfewshotlearners,gpt4technicalreport,openai2024gpt4ocard}, Gemini \cite{geminiteam2024geminifamilyhighlycapable,geminiteam2024gemini15unlockingmultimodal}, Phi \cite{abdin2024phi3technicalreporthighly,phi4report}, Llama \cite{dubey2024llama3herdmodels}, Mistral \cite{jiang2023mistral7b,jiang2024mixtralexperts}, DeepSeek \cite{deepseekv3,deepseekai2025r1}, and Qwen \cite{yang2024qwen2technicalreport,qwen2025qwen25technicalreport}, represent a transformative advancement in artificial intelligence. %Each of these encompasses a series of models designed for various applications. Foundation models, such as GPT-4 \cite{gpt4technicalreport}, GPT-4o \cite{openai2024gpt4ocard}, LLAMA-3 series\cite{dubey2024llama3herdmodels}, Mixtral \cite{jiang2024mixtralexperts} and DeepSeek \cite{deepseekv3,deepseekai2025r1}, represent a transformative advancement in artificial intelligence. 
These models, trained on massive web-scale datasets, are designed to serve as general-purpose tools, capable of handling a wide range of tasks with a single architecture. The most notable capabilities of foundation models include their abilities to perform tasks without fine-tuning, a phenomenon known as zero-shot learning, and their few-shot learning abilities which allow them to adapt to new tasks by drawing inferences from just a few examples. 

Despite their success in general-purpose tasks, early investigations \cite{ai4science2023impactlargelanguagemodels} highlight significant room for improvement in scientific tasks involving small molecules, proteins, DNA, RNA, or materials. In particular, foundation models struggle with precise quantitative predictions (e.g., ligand-protein binding affinity, protein-protein interactions, DNA properties) \cite{ai4science2023impactlargelanguagemodels}, as well as the rational design of small molecule compounds, proteins, or materials. Ensuring the scientific accuracy of outputs from these models remains a grand challenge.

Recently, there has been a concerted effort to develop large-scale foundation models specifically tailored for scientific tasks. These approaches can be broadly divided into four categories:
\begin{enumerate}
\item Domain-specific foundation models. These models, such as ProGen \cite{ProGen} and ESM3 \cite{esm3} for proteins, DNABERT \cite{zhou2024dnabert2} and Evo \cite{evo2024science} for DNA sequences, scGPT \cite{Cui2024scgpt} for single-cell data, and chemical language models \cite{liu2023molxpt,segler2018generating} for small molecules, are trained specifically on token sequence representations for individual scientific domains.
\item Fine-tuned general-purpose models. This approach adapts well-trained large language models for specific scientific domains, as demonstrated by Tx-LLM \cite{chaves2024tx} for small molecules and ProLLAMA \cite{lv2024prollamaproteinlanguagemodel} for proteins.
\item Scientific data enhanced large language models (LLMs). This approach, exemplified by works such as  \cite{BioGPT2022Luo,liu2023molxpt,galactica2022}, trains LLMs from scratch mainly with text data and a small portion of scientific data.
\item Integration of specific scientific modules. In this approach, external modules, such as pre-trained molecular or protein encoders, are integrated into general-purpose models (e.g., Llama) via lightweight adapters \cite{drugchat,proteinchat}.
\end{enumerate}

While these approaches have made considerable progress, they do face notable limitations. Domain-specific models (approach \#1) are restricted to their respective fields, limiting their ability to capture interdisciplinary insights for cross-domain applications. Fine-tuning general-purpose models (approach \#2) and scientific data enhanced LLMs (approach \#3) show promise but are often constrained by small-scale scientific datasets, e.g., around 90\% text data and only 10\% scientific data in \cite{galactica2022}, which hinders the models' capacity to capture the complexity of scientific tasks. The integration of external modules (approach \#4) faces challenges in aligning inputs effectively with large language models, and most implementations opt for limited fine-tuning with small datasets, leaving the core models largely unchanged.

These limitations emphasize the necessity for a science foundation model, to fulfill the sophisticated demands of scientific research. A model of this kind must not only be highly proficient in producing precise scientific predictions, but also adept at designing and optimizing scientific entities conditioned on context information. A good science foundation model ought to have the capacity to handle a diverse range of inputs. These inputs can span from literature text, to scientific sequence data such as protein or DNA sequences, and further to structural data like 3D protein/DNA structures and their dynamic behaviors. In the present study, our focus is on sequence-based data for representing biological, chemical, material systems, and textual human language (e.g., English):
\begin{itemize}
\item DNA, RNA, and proteins, which are often referred to as the ``language of nature", are intrinsically represented by sequences. Additionally, many other scientific entities like small molecules and materials can be effectively represented as sequences through well-established domain-specific techniques \cite{Weininger1988}.
\item Sequence data is highly compatible with the current mainstream large language models (LLMs). Through the continuous pre-training of LLMs, we are able to utilize the scientific knowledge embedded in these general-purpose LLMs to tackle complex scientific challenges.  
\item Sequential data provides remarkable flexibility when combined with autoregressive paradigms \cite{bond2021deep,yenduri2024gpt}%
%\cite{add some survey papers about autoregressive LLMs or paradigms}
. These paradigms, which are extensively employed in generative models, are capable of effectively modeling the highly complex distributions of any scientific object that can be presented in the form of a sequence.  
\end{itemize}

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=\linewidth]{figures/NatureLM-overview-arch.pdf}
    \caption{\ourM{} is a GPT-style generative model trained on a diverse range of data, including small molecule compounds, proteins, DNA, RNA, materials, and both general and scientific texts, amounting to a total of 143 billion tokens. It is built on existing large language models by integrating new vocabularies for scientific entities and jointly pre-training all components. After the pre-training, the model undergoes additional instruction tuning using millions of curated instructions from scientific fields. Options for reinforcement learning and dedicated fine-tuning are also available to boost performance on specific tasks. Users can engage with \ourM{} through natural language inputs. The model excels in various domains, achieving top results in tasks such as retrosynthesis (Section \ref{sec:retro}), SMILES-to-IUPAC translation (Section \ref{sec:smiles_iupac}), protein generation (Section \ref{sec_prot_generation}) and material property prediction (Section \ref{sec:dedicate_tune_matbench}), often matching or exceeding the capabilities of state-of-the-art specialized models.}
    \label{fig:naturelm_overview}
\end{figure}

\ourM{} is designed to handle the complexity of small molecules, proteins, DNA, RNA, materials, and their associated textual information. An overview of \ourM{} is in Fig. \ref{fig:naturelm_overview}. \ourM{} follows the Transformer decoder architecture and is trained on a corpus of 143 billion tokens collected from various scientific domains (Fig. \ref{fig:pretrain_data_pie}). Our experiments demonstrate that \ourM{} significantly outperforms general-purpose foundation models for scientific tasks. Specifically, \ourM{} excels in tasks such as:
\begin{enumerate}
    \item Following textual instructions to generate and optimize scientific molecular entities.
\item Performing cross-domain generation tasks, such as designing small molecules or RNA binders for proteins as well as designing guide RNA sequences given a DNA template for CRISPR systems.
\item  Achieving top performance on generation and translation tasks, such retrosynthesis (Section \ref{sec:retro}), SMILES-to-IUPAC translation (Section \ref{sec:smiles_iupac}), protein generation (Section \ref{sec_prot_generation}), matching or surpassing state-of-the-art specialist models. %By fintuning \ourM{}, it achieves top performance on prediction tasks, such as material property prediction (Section \ref{sec:dedicate_tune_matbench}).%"specialist models.
\end{enumerate}

To investigate the scalability of \ourM{} with respect to model size, we trained three versions of \ourM{} with varying parameter configurations. As illustrated in Fig. \ref{fig:overallrank}, among the 22 categories of tasks evaluated, 18 categories exhibited clear improvements with increasing model size (i.e., 8x7B demonstrated the best performance\footnote{The 8x7B model is a Mixture-of-Experts (MoE) model \cite{jiang2024mixtralexperts}, composed of eight expert models, each with 7 billion parameters. A portion of these expert models is shared across all models, resulting in a total parameter count of 46.7 billion.}, followed by 8B, and then 1B), underscoring the potential of large foundation models for scientific discovery. Additionally, we demonstrate the efficacy of \emph{reinforcement learning} in enhancing the post-training performance of \ourM{} for molecular property optimization and \emph{dedicated finetuning} for retrosynthesis.


\begin{figure}[!htb]
\centering
\includegraphics[trim= 1cm 2cm 3cm 4cm, clip, width=\linewidth]{figures/overall_rank3.pdf}
\caption{The scaling effect in \ourM{} is obvious. The chart depicts the overall ranking of models with varying sizes, where a better rank is represented by the ``outsider'' bar. The 8x7B model achieves top performance in 19 tasks, while the 8B model excels in 3 tasks. 18 categories exhibited performance improvements with increasing model size (i.e., 8x7B demonstrated the best performance, followed by 8B, and then 1B), highlighting the potential of large foundation models for scientific applications. }
\label{fig:overallrank}
\end{figure}

In summary, \ourM{} represents a significant step towards building a generalist model across multiple scientific domains. By harnessing the capabilities of text-based instructions, \ourM{} serves as a powerful tool for scientific discovery, enabling cross-domain generation and optimization in areas such as drug discovery, materials science, and the development of therapeutic proteins and nucleotides. 
Ideally, a foundation model should support a broad range of tasks while demonstrating strong zero-shot and few-shot capabilities.
\ourM{} shows great promise, but its language capabilities and few-shot learning skills still lag behind leading large language models. We will address these limitations in future iterations, positioning \ourM{} as an essential component in the continued evolution of science foundation models.
