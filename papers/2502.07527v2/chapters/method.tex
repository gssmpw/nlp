\section{Method}

\subsection{Pre-training data}\label{sec:pretraining}
The pre-training data includes text, small molecules, proteins, materials, DNA, and RNA, all in the format of sequences:
\begin{enumerate}
\item Small molecules are converted into Simplified Molecular Input Line Entry System (SMILES) notations, obtained by applying depth-first search algorithm on molecular graphs to yield a linear representation of the chemical structure \cite{Weininger1988}. The SMILES are tokenized by the commonly used regular expression for molecules\footnote{\url{https://github.com/microsoft/DVMP/blob/main/molecule/tokenize_re.py\#L11}}.
\item Proteins, DNA and RNA are depicted using FASTA format, which sequentially lists the amino acids or nucleotides. The sequences are tokenized into individual units, with proteins broken down into their constituent amino acids and DNA/RNA into their respective nucleotides.
\item For crystal material data, both the chemical composition and the associated space group number\footnote{\url{https://en.wikipedia.org/wiki/List_of_space_groups}} are flattened into a sequence. For example, consider the material from the material project with ID mp-1960, as shown in Fig. \ref{fig:example_pretrain_data}. This material has 12 atoms in its cell, consisting of 4 Li and 8 O atoms. We flatten this information as depicted in the figure. The space group is Fm3m, which corresponds to the International Space Group Number 225, and we represent it with $\langle$sg$\cdots$$\rangle$. %We opt for this flattening method instead of using the formula Li$_4$O$_8$ to better align with the prediction of 3D structures of materials (see Section xxxx).
\end{enumerate}
\begin{figure}[!htpb]
\centering
%\includegraphics[trim=3cm 0cm 6cm 0cm, clip, width=0.8\linewidth]{figures/data_example.pdf}
\includegraphics[trim=3cm 0cm 6cm 0cm, clip, width=0.8\linewidth]{figures/example_img.pdf}
\caption{Example data from each domain. The small molecule is Aspirin (PubChem CID: 2244) and visualized by RDKit \cite{greg_landrum_2025_14779836}. The protein snapshot is from the PDB bank with ID 7CAM \cite{Wang2020-vw}. The DNA structure is split into chain I and chain J from PDB 1KX5 \cite{Davey2002im} and visualized by UCSF Chimera \cite{chimera2004software}. The material snapshot is from the material project with ID mp-1960 \cite{osti_1194803}.}
\label{fig:example_pretrain_data}
\end{figure}

An example of the data is in Fig. \ref{fig:example_pretrain_data}. The vocabulary sizes of small molecules, proteins, material, DNA and RNA are 1401, 26, 396, 16 and 16 respectively. To differentiate scientific entities from regular text, each scientific sequence is enclosed by a pair of special tokens: \mol{}$\cdots$\emol{} for small molecules, \pro{}$\cdots$\epro{} for proteins, \material{}$\cdots$\ematerial{} for materials, \dna{}$\cdots$\edna{} for DNA and \rna{}$\cdots$\erna{} for RNA. Specifically, we use \product{}$\cdots$\eproduct{} and \reactant{}$\cdots$\ereactant{} to represent products and reactants for small molecules in chemical reactions. We use \ant{}$\cdots$\eant{} to denote antibodies. For example, benzene is represented by \mol{}c1ccccc1\emol{}. More examples can be found within the following sections.

The pre-training data contains single-domain sequences and cross-domain sequences. A single-domain sequence comes from one domain, such as pure text sequences, SMILES sequences for small molecules, and FASTA sequences for proteins. A cross-domain sequence includes data from two different domains, building connections across domains. The distribution of our pre-training data is visualized in Fig. \ref{fig:pretrain_data_pie} and more details are left in Table \ref{tab:statistics_pretrain_data}. 

\begin{figure}[!htpb]
\centering
\includegraphics[width=0.7\linewidth]{figures/data-pie-pretrain.pdf}
\caption{Distribution of the pre-training data, measured by the number of tokens of each category.}
\label{fig:pretrain_data_pie}
\end{figure}

Our cross-domain data is organized into three categories.
\begin{enumerate}
    \item Interleaved Sequences: Inspired by \cite{liu2023molxpt}, we process scientific literature by initially employing a named entity recognition tool, BERN2 \cite{bern2}, to identify the mentions of small molecules and proteins within the corpus. These entities are then converted into their corresponding SMILES and FASTA sequences.
    Consequently, the small molecules and proteins are wrapped by text, creating an interleaved data structure that bridges the gap between textual information and scientific data. We also develop a quality filter to remove low-quality sentences.
    This formulation is also similar to the one that has been used in multi-modal LLMs where image tokens are wrapped inside text~\cite{Zhu2023MultimodalCA,Zhan2024AnyGPTUM,team2024chameleon}. We provide an example of interleaved sequences.
    \begin{example}
        A prospective, randomized clinical trial was performed to study the efficacy of povidone iodine \mol{}C=CN1CCCC1=O.II\emol{} ( Betadine \mol{}C=CN1CCCC1=O.II\emol{}) suppositories for the treatment of bacterial vaginosis (BV) in comparison to capsules containing lactobacilli (Dderlein Med).
    \end{example}
    \item Parallel Text and Scientific Entities: Leveraging databases such as PubChem\footnote{\url{https://pubchem.ncbi.nlm.nih.gov/}}, UniProt\footnote{\url{https://www.uniprot.org/}}, and NCBI\footnote{\url{https://www.ncbi.nlm.nih.gov/}}, we extract descriptive information about specific proteins and small molecules. Additionally, from the Materials Project website\footnote{\url{https://next-gen.materialsproject.org/}}, material-related data such as bandgap, energy above hull, and other properties are gathered and translated into textual descriptions. This process results in parallel datasets that align scientific facts with their textual counterparts, enhancing the richness of the information. 
    \item Linking DNA with Proteins Through the Central Dogma: For DNA sequences, we identify segments that can be transcribed and translated into proteins, following the central dogma of molecular biology. These identified DNA segments are then replaced with the equivalent protein sequences, establishing a direct connection between the genetic blueprint and its functional protein products. This method not only reflects the biological process but also creates a dataset that encapsulates the relationship between nucleotide and amino acid sequences.
\end{enumerate}


\begin{table}[!htbp]
\centering
\begin{tabular}{lcccc}
\toprule
& Samples & Tokens & Samples & Tokens \\
& (by million) & (by billion) & (\%) & (\%) \\ 
\midrule
Interleaved Sequence & 4.3 & 4.0 & 10.2 & 31.3 \\
Text-SMILES & 33.0 & 3.0 & 78.8 & 24.0 \\
Text-protein & 1.9 & 1.4 & 4.6 & 10.8\\
Text-material & 1.7 & 0.2 & 4.0 & 1.6 \\
DNA-protein & 1.0 & 4.1 & 2.4 & 32.3 \\
\midrule 
Total& 41.9 & 12.7 & 100 & 100 \\
\bottomrule
\end{tabular}
\caption{Statistics of cross-domain data.}
\label{tab:statistics_multimodal_data}
\end{table}

The statistics of cross-domain data is in Table \ref{tab:statistics_multimodal_data}. Both interleaved sequences and text-science parallel data are types of cross-domain data that aim to facilitate cross-domain interaction. For interleaved sequences, the sources are literature, which can cover a broader range of general topics and wider domains. In contrast, parallel data sources are existing databases that focus on specific properties. Although the topics covered by parallel data are not as diverse as those in interleaved sequences, the amount of data available for each given property is greater. These distinctions highlight the complementary nature of the two types of cross-domain data.



\subsection{Post-training data}\label{sec:supervised_ft_data}
We curated a dataset for post-training with about 5.1 million instruction-response pairs encompassing six domains, small molecules, proteins, materials, DNA, RNA and general text (Figure \ref{fig:sft_data}). The dataset includes over 60 sub-tasks. For each sub-task, multiple prompts were manually crafted to form diverse instruction-response pairs, covering essential scientific tasks such as molecular optimization, antibody design, and guide RNA design. We provide two examples below:
\begin{example} 
$\rhd$ {\bf Example 1:}

\noindent\textbf{Instruction:}\\ 
\texttt{Create a guiding RNA to interact with the DNA sequence }\\
\noindent\dna{}CCCAGAGC$\cdots$GGGCCTGTC\edna{}.\\   \noindent\textbf{Response}:\rna{}AGGGGACAAACCTTCATCCA\erna{}



\noindent$\rhd$ {\bf Example 2}

\noindent\textbf{Instruction: }\\\texttt{What product could potentially form from the reaction of the given reactants? }$\langle$\texttt{product}$\rangle$C([C@H]1N(c2c(C(N[C@@H](CC)c3ccccc3)=O)c3c(nc2-c2ccccc2)cccc3)CCC1)(=O)OC$\langle$\texttt{/product}$\rangle$\\   
\textbf{Response: }\\$\langle$\texttt{reactant}$\rangle$C([C@H]1N(c2c(C(N[C@@H](CC)c3ccccc3)=O)c3c(nc2-c2ccccc2)cccc3)CCC1)O$\langle$\texttt{/reactant}$\rangle$
    \end{example}  
\begin{figure}[!htbp]
    \centering
    \includegraphics[width=0.7\linewidth]{figures/data-pie-instruct.pdf}
    \caption{Statistics of post-training data, measured by the number of sequences.}
    \label{fig:sft_data}
\end{figure}


The text data were sourced from open-source instruction tuning datasets like OIG \footnote{\url{https://huggingface.co/datasets/laion/OIG}}, aiming to ensure that the model not only excels in scientific tasks but also maintains general language capabilities.



\iffalse
\begin{table}[!htbp]
\centering
\begin{tabular}{cccccccc}
\toprule
& Generation samples & Prediction samples \\
\midrule
Small molecule & 636k & 182k \\
Protein & 1.3M & xx \\
Material & 307k & 0  \\
DNA & 90k & 0 \\
Cross-domain & 225k & 248k\\
General instruction & 800k \\
\bottomrule
\end{tabular}
\caption{Instruction tuning task distribution.}
\label{tab:instruction_task_distr}
\end{table}
\fi

\subsection{Model architecture}
\ourM{} models are built upon well-trained large language models (LLMs) with some additional parameters for newly introduce scientific tokens. We used Llama 3 8B \cite{dubey2024llama3herdmodels} and Mixtral 8x7B \cite{jiang2024mixtralexperts} to initialize the main part of \ourM{} and continued pre-training using the science data described in Section \ref{sec:pretraining}. Additionally, we trained a model with 1B parameters, which replicates the structural design of Llama 3 but with a reduced number of layers and smaller hidden dimensions. Its pre-training begins with a random selection of 300 billion pure text tokens from the SlimPajama dataset \cite{cerebras2023slimpajama}, followed by the science data we collected in Section \ref{sec:pretraining}. This approach ensures a consistent training methodology across all three models. The details of the model architecture are provided in Table \ref{tab:model_arch}.


\begin{table}[!htbp]
\centering
\begin{tabular}{cccccccc}
\toprule
Model Parameters & 1B & 8B & 8x7B \\
\midrule
Hidden Dimensions & 2048 & 4096 & 4096 \\
FFN Dimensions & 5504 & 14336 & 14336 \\
Attention Heads & 32 & 32 & 32 \\
KV Heads & 8 & 8 & 32 \\
Number of Layers & 16 & 32 & 32 \\
Vocabulary Size & 130,239 & 130,239 & 38,078 \\
\bottomrule
\end{tabular}
\caption{Model parameters of different sizes of \ourM{}.}
\label{tab:model_arch}
\end{table}

\subsection{Continued pre-training}
To address the intricate comprehension required for scientific tasks, \ourM{} introduces specific tokens for scientific entities. Consequently, we augment the vocabulary of the chosen LLMs. The embedding weights for these newly introduced tokens are randomly initialized. Directly tuning from pre-training usually causes instability and potentially compromises the language capabilities of the original LLMs. This is primarily due to the introduction of new tokens and the mismatch between the well-trained text tokens and randomly initialized scientific tokens. 

To circumvent this issue, we have devised a two-stage pre-training procedure:

Stage 1: Training is exclusively concentrated on the newly introduced tokens. During this phase, the parameters of the existing model are frozen. This allows the new tokens to adapt to the model gradually, mitigating the risk of instability.

Stage 2: Once the new tokens are adequately trained, we proceed to the second phase where the entire network, including both new and existing parameters, is trained. This joint optimization process ensures that the new tokens are seamlessly integrated with the existing ones, enhancing the model's overall performance.

This two-stage training approach not only fosters a thorough understanding of the scientific domain but also preserves the integrity and robustness of the underlying language model by preventing potential instabilities. The detailed training recipe is summarized in Table \ref{tab:training_recipe}. 


The validation loss for the three versions of the models is illustrated in Fig. \ref{fig:dev_loss}. All validation losses decrease as the model size increases from 1 billion to 8 billion, and 8x7 billion. This indicates that larger models are better at capturing the underlying patterns in the data, which is expected due to their increased capacity. The most significant decreases are observed in the text and protein data, suggesting that these datasets benefit more from larger models.

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=0.75\linewidth]{figures/pretrain_validation_loss.pdf}
    \caption{Validation loss for the 1B, 8B, and 8x7B models of \ourM{}. Larger models result in smaller validation losses across each domain. \ourM{} (8B) is short for ``Llama 3 8B \ourM{}'' throughout this paper.}
    \label{fig:dev_loss}
\end{figure}

% original dev loss curve; do not show after discussion
% \begin{figure}[!htbp]
%     \centering
%     \includegraphics[width=0.5\linewidth]{figures/mixtral_dev_loss.png}
%     \caption{8x7B dev loss on each dev set}
%     \label{fig:8x7b_dev}
% \end{figure}

\subsection{Post-training}\label{sec:post_train}
In the post-training phase, we mainly employ supervised fine-tuning (SFT) using the instruction-response pair data outlined in Section~\ref{sec:supervised_ft_data}. These pairs are structured into sequences utilizing the template  ``\texttt{Instruction: \{instruction\}\textbackslash{}n\textbackslash{}n\textbackslash{}nResponse: \{response\}}'' where ``\{\texttt{instruction}\}'' and ``\{\texttt{response}\}'' serve as placeholders. During the model optimization, the training loss is computed solely on the response part of the sequence. Unlike in the pre-training phase, each sequence contains a single instruction-response pair rather than multiple pairs packed into one sequence. Empirical evidence suggests that this approach aids in stabilizing the post-training process. The 1B and 8B models are trained for 20k steps, while the 8x7B model is trained for 7.8k steps (due to resource constraint). We also explore using RLHF after supervised finetuning and results are discussed in Section \ref{sec:RL}.


\subsection{Inference acceleration}
As \ourM{} will be tested on many downstream tasks, we need to accelerate inference speed to save computational cost. We adopted the following approaches: (1) PagedAttention \cite{pagedattention}, which optimizes LLM serving by partitioning the key-value (KV) cache into fixed-size, non-contiguous blocks, reducing memory fragmentation and enabling efficient memory sharing; and (2) Selective Batching \cite{orca}, which batches compatible operations while handling attention separately, allowing for flexible and efficient processing of requests with varying input lengths. We employed the vLLM framework \cite{vllm2024} to serve \ourM{} models, leveraging its implementations of both PagedAttention and Selective Batching. These optimizations were applied to the 1B, 8B, and 8×7B models. Consequently, the inference speed for the \ourM{} 8×7B model reached approximately 525 tokens per second with Brain Float 16 (BF16) precision on two NVIDIA A100 GPUs.