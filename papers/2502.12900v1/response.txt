\section{Related Work}
Speech contains rich non-semantic information compared to text**Baevski, "Unsupervised Speech Recognition"**. For LLMs to achieve an accurate understanding of audio, they must have a comprehensive perception of speech rather than relying solely on text**Conneau, "On the Dangers of Stochastic Parrots: Can Transfer Learning Improve Model Robustness against Adversarial Attacks?"**. As a result, many researchers have studied how to build end-to-end speech LLMs**Carson et al., "Super Glue: Towards Efficient and Versatile Visual Features"**.

Some studies have found the \textit{less is more} phenomenon in LLMs with respect to data usage **Li et al., "Dense Connection for Image Super-Resolution"** , meaning that efficient use of data can also achieve good performance. However, for speech LLMs, data efficiency has not been fully explored. Therefore, this work addresses this issue by focusing on the key challenge of speech-text alignment.

The acoustic features and text features differ significantly in both their representation space and length. To address this issue, **Baevski et al., "Unsupervised Speech Recognition"** employ convolution network to down-sample the speech, while others opt for solutions with more learnable parameters, such as Q-Former **Carson et al., "Super Glue: Towards Efficient and Versatile Visual Features"** and linear layers **Brown et al., "Language Models are Few-Shot Learners"**. Unlike previous work, the proposed Soundwave implements two adapters to address differences in representation and length, which also make training more efficient.


Speech LLMs are primarily designed for two capabilities: Speech and Sound. **Carson et al., "Super Glue: Towards Efficient and Versatile Visual Features"** combine Whisper with other feature extractors, such as BEATs **Baevski et al., "Unsupervised Speech Recognition"**  and WavLM **Brown et al., "Language Models are Few-Shot Learners"** , to process sound features. 
**Li et al., "Dense Connection for Image Super-Resolution"** show that a fully fine-tuned encoder can also capture sound information. Our work demonstrates that a frozen encoder can efficiently process both types of features when provided with the proper data and training strategy.