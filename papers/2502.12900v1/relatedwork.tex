\section{Related Work}
Speech contains rich non-semantic information compared to text~\citep{audiobench, bu2024roadmap, dynamic_superb}. For LLMs to achieve an accurate understanding of audio, they must have a comprehensive perception of speech rather than relying solely on text~\citep{ji2024wavchat, sdeval}. As a result, many researchers have studied how to build end-to-end speech LLMs~\citep{wavllm, SALMONN, qwen2audio, GAMA, fang2024llama, geng2025osum}. 

Some studies have found the \textit{less is more} phenomenon in LLMs with respect to data usage \citep{zhou2024lima,song-etal-2025-less}, meaning that efficient use of data can also achieve good performance. However, for speech LLMs, data efficiency has not been fully explored. Therefore, this work addresses this issue by focusing on the key challenge of speech-text alignment.

The acoustic features and text features differ significantly in both their representation space and length. To address this issue, \citet{qwen2audio, qwenaudio} employ convolution network to down-sample the speech, while others opt for solutions with more learnable parameters, such as Q-Former~\citep{SALMONN} and linear layers~\citep{wavllm}. Unlike previous work, the proposed Soundwave implements two adapters to address differences in representation and length, which also make training more efficient.


Speech LLMs are primarily designed for two capabilities: Speech and Sound. \citet{SALMONN,wavllm} combine Whisper with other feature extractors, such as BEATs~\citep{BEATs} and WavLM~\citep{WavLM}, to process sound features. 
\citet{qwen2audio} show that a fully fine-tuned encoder can also capture sound information. Our work demonstrates that a frozen encoder can efficiently process both types of features when provided with the proper data and training strategy.