[
  {
    "index": 0,
    "papers": [
      {
        "key": "audiobench",
        "author": "Wang, Bin and Zou, Xunlong and Lin, Geyu and Sun, Shuo and Liu, Zhuohan and Zhang, Wenyu and Liu, Zhengyuan and Aw, AiTi and Chen, Nancy F",
        "title": "Audiobench: A universal benchmark for audio large language models"
      },
      {
        "key": "bu2024roadmap",
        "author": "Bu, Fan and Zhang, Yuhao and Wang, Xidong and Wang, Benyou and Liu, Qun and Li, Haizhou",
        "title": "Roadmap towards superhuman speech understanding using large language models"
      },
      {
        "key": "dynamic_superb",
        "author": "Chien{-}Yu Huang and\nKe{-}Han Lu and\nShih{-}Heng Wang and\nChi{-}Yuan Hsiao and\nChun{-}Yi Kuan and\nHaibin Wu and\nSiddhant Arora and\nKai{-}Wei Chang and\nJiatong Shi and\nYifan Peng and\nRoshan S. Sharma and\nShinji Watanabe and\nBhiksha Ramakrishnan and\nShady Shehata and\nHung{-}Yi Lee",
        "title": "Dynamic-Superb: Towards a Dynamic, Collaborative, and Comprehensive\nInstruction-Tuning Benchmark For Speech"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "ji2024wavchat",
        "author": "Ji, Shengpeng and Chen, Yifu and Fang, Minghui and Zuo, Jialong and Lu, Jingyu and Wang, Hanting and Jiang, Ziyue and Zhou, Long and Liu, Shujie and Cheng, Xize and others",
        "title": "WavChat: A Survey of Spoken Dialogue Models"
      },
      {
        "key": "sdeval",
        "author": "Ao, Junyi and Wang, Yuancheng and Tian, Xiaohai and Chen, Dekun and Zhang, Jun and Lu, Lu and Wang, Yuxuan and Li, Haizhou and Wu, Zhizheng",
        "title": "SD-Eval: A Benchmark Dataset for Spoken Dialogue Understanding Beyond Words"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "wavllm",
        "author": "Hu, Shujie and Zhou, Long and Liu, Shujie and Chen, Sanyuan and Hao, Hongkun and Pan, Jing and Liu, Xunying and Li, Jinyu and Sivasankaran, Sunit and Liu, Linquan and others",
        "title": "Wavllm: Towards robust and adaptive speech large language model"
      },
      {
        "key": "SALMONN",
        "author": "Tang, Changli and Yu, Wenyi and Sun, Guangzhi and Chen, Xianzhao and Tan, Tian and Li, Wei and Lu, Lu and Ma, Zejun and Zhang, Chao",
        "title": "Salmonn: Towards generic hearing abilities for large language models"
      },
      {
        "key": "qwen2audio",
        "author": "Chu, Yunfei and Xu, Jin and Yang, Qian and Wei, Haojie and Wei, Xipin and Guo, Zhifang and Leng, Yichong and Lv, Yuanjun and He, Jinzheng and Lin, Junyang and others",
        "title": "Qwen2-Audio Technical Report"
      },
      {
        "key": "GAMA",
        "author": "Sreyan Ghosh and\nSonal Kumar and\nAshish Seth and\nChandra Kiran Reddy Evuru and\nUtkarsh Tyagi and\nSakshi Singh and\nOriol Nieto and\nRamani Duraiswami and\nDinesh Manocha",
        "title": "{GAMA:} {A} Large Audio-Language Model with Advanced Audio Understanding\nand Complex Reasoning Abilities"
      },
      {
        "key": "fang2024llama",
        "author": "Fang, Qingkai and Guo, Shoutao and Zhou, Yan and Ma, Zhengrui and Zhang, Shaolei and Feng, Yang",
        "title": "Llama-omni: Seamless speech interaction with large language models"
      },
      {
        "key": "geng2025osum",
        "author": "Geng, Xuelong and Wei, Kun and Shao, Qijie and Liu, Shuiyun and Lin, Zhennan and Zhao, Zhixian and Li, Guojian and Tian, Wenjie and Chen, Peikun and Li, Yangze and others",
        "title": "OSUM: Advancing Open Speech Understanding Models with Limited Resources in Academia"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "zhou2024lima",
        "author": "Zhou, Chunting and Liu, Pengfei and Xu, Puxin and Iyer, Srinivasan and Sun, Jiao and Mao, Yuning and Ma, Xuezhe and Efrat, Avia and Yu, Ping and Yu, Lili and others",
        "title": "Lima: Less is more for alignment"
      },
      {
        "key": "song-etal-2025-less",
        "author": "Song, Dingjie  and\nWang, Wenjun  and\nChen, Shunian  and\nWang, Xidong  and\nGuan, Michael X.  and\nWang, Benyou",
        "title": "Less is More: A Simple yet Effective Token Reduction Method for Efficient Multi-modal {LLM}s"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "qwen2audio",
        "author": "Chu, Yunfei and Xu, Jin and Yang, Qian and Wei, Haojie and Wei, Xipin and Guo, Zhifang and Leng, Yichong and Lv, Yuanjun and He, Jinzheng and Lin, Junyang and others",
        "title": "Qwen2-Audio Technical Report"
      },
      {
        "key": "qwenaudio",
        "author": "Chu, Yunfei and Xu, Jin and Zhou, Xiaohuan and Yang, Qian and Zhang, Shiliang and Yan, Zhijie and Zhou, Chang and Zhou, Jingren",
        "title": "Qwen-audio: Advancing universal audio understanding via unified large-scale audio-language models"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "SALMONN",
        "author": "Tang, Changli and Yu, Wenyi and Sun, Guangzhi and Chen, Xianzhao and Tan, Tian and Li, Wei and Lu, Lu and Ma, Zejun and Zhang, Chao",
        "title": "Salmonn: Towards generic hearing abilities for large language models"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "wavllm",
        "author": "Hu, Shujie and Zhou, Long and Liu, Shujie and Chen, Sanyuan and Hao, Hongkun and Pan, Jing and Liu, Xunying and Li, Jinyu and Sivasankaran, Sunit and Liu, Linquan and others",
        "title": "Wavllm: Towards robust and adaptive speech large language model"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "SALMONN",
        "author": "Tang, Changli and Yu, Wenyi and Sun, Guangzhi and Chen, Xianzhao and Tan, Tian and Li, Wei and Lu, Lu and Ma, Zejun and Zhang, Chao",
        "title": "Salmonn: Towards generic hearing abilities for large language models"
      },
      {
        "key": "wavllm",
        "author": "Hu, Shujie and Zhou, Long and Liu, Shujie and Chen, Sanyuan and Hao, Hongkun and Pan, Jing and Liu, Xunying and Li, Jinyu and Sivasankaran, Sunit and Liu, Linquan and others",
        "title": "Wavllm: Towards robust and adaptive speech large language model"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "BEATs",
        "author": "Sanyuan Chen and\nYu Wu and\nChengyi Wang and\nShujie Liu and\nDaniel Tompkins and\nZhuo Chen and\nWanxiang Che and\nXiangzhan Yu and\nFuru Wei",
        "title": "BEATs: Audio Pre-Training with Acoustic Tokenizers"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "WavLM",
        "author": "Sanyuan Chen and\nChengyi Wang and\nZhengyang Chen and\nYu Wu and\nShujie Liu and\nZhuo Chen and\nJinyu Li and\nNaoyuki Kanda and\nTakuya Yoshioka and\nXiong Xiao and\nJian Wu and\nLong Zhou and\nShuo Ren and\nYanmin Qian and\nYao Qian and\nJian Wu and\nMichael Zeng and\nXiangzhan Yu and\nFuru Wei",
        "title": "WavLM: Large-Scale Self-Supervised Pre-Training for Full Stack Speech\nProcessing"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "qwen2audio",
        "author": "Chu, Yunfei and Xu, Jin and Yang, Qian and Wei, Haojie and Wei, Xipin and Guo, Zhifang and Leng, Yichong and Lv, Yuanjun and He, Jinzheng and Lin, Junyang and others",
        "title": "Qwen2-Audio Technical Report"
      }
    ]
  }
]