[
  {
    "index": 0,
    "papers": [
      {
        "key": "bordes2024introduction",
        "author": "Bordes, Florian and Pang, Richard Yuanzhe and Ajay, Anurag and Li, Alexander C and Bardes, Adrien and Petryk, Suzanne and Ma{\\~n}as, Oscar and Lin, Zhiqiu and Mahmoud, Anas and Jayaraman, Bargav and others",
        "title": "An introduction to vision-language modeling"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "changpinyo2021cc12m",
        "author": "Changpinyo, Soravit and Sharma, Piyush and Ding, Nan and Soricut, Radu",
        "title": "{Conceptual 12M}: Pushing Web-Scale Image-Text Pre-Training To Recognize Long-Tail Visual Concepts"
      },
      {
        "key": "schuhmann2022laion5bopenlargescaledataset",
        "author": "Christoph Schuhmann and Romain Beaumont and Richard Vencu and Cade Gordon and Ross Wightman and Mehdi Cherti and Theo Coombes and Aarush Katta and Clayton Mullis and Mitchell Wortsman and Patrick Schramowski and Srivatsa Kundurthy and Katherine Crowson and Ludwig Schmidt and Robert Kaczmarczyk and Jenia Jitsev",
        "title": "LAION-5B: An open large-scale dataset for training next generation image-text models"
      },
      {
        "key": "ilharco2021openclip",
        "author": "Gabriel Ilharco and Mitchell Wortsman and Ross Wightman and Cade Gordon and Nicholas Carlini and Rohan Taori and Achal Dave and Vaishaal Shankar and Hongseok Namkoong and John Miller and Hannaneh Hajishirzi and Ali Farhadi and Ludwig Schmidt",
        "title": "OpenCLIP"
      },
      {
        "key": "zeng2022multigrainedvisionlanguagepretraining",
        "author": "Yan Zeng and Xinsong Zhang and Hang Li",
        "title": "Multi-Grained Vision Language Pre-Training: Aligning Texts with Visual Concepts"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "radford2021learning",
        "author": "Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others",
        "title": "Learning transferable visual models from natural language supervision"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "liu2023llava",
        "author": "Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae",
        "title": "Visual Instruction Tuning"
      },
      {
        "key": "cai2023vipllava",
        "author": "Cai, Mu and Liu, Haotian and Mustikovela,  Siva Karthik and Meyer, Gregory P. and Chai, Yuning and Park, Dennis and Lee, Yong Jae",
        "title": "Making Large Multimodal Models Understand Arbitrary Visual Prompts"
      },
      {
        "key": "liu2024llavanext",
        "author": "Liu, Haotian and Li, Chunyuan and Li, Yuheng and Li, Bo and Zhang, Yuanhan and Shen, Sheng and Lee, Yong Jae",
        "title": "LLaVA-NeXT: Improved reasoning, OCR, and world knowledge"
      },
      {
        "key": "dai2023instructblipgeneralpurposevisionlanguagemodels",
        "author": "Wenliang Dai and Junnan Li and Dongxu Li and Anthony Meng Huat Tiong and Junqi Zhao and Weisheng Wang and Boyang Li and Pascale Fung and Steven Hoi",
        "title": "InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning"
      },
      {
        "key": "zhai2022litzeroshottransferlockedimage",
        "author": "Xiaohua Zhai and Xiao Wang and Basil Mustafa and Andreas Steiner and Daniel Keysers and Alexander Kolesnikov and Lucas Beyer",
        "title": "LiT: Zero-Shot Transfer with Locked-image text Tuning"
      },
      {
        "key": "chen2022pali",
        "author": "Chen, Xi and Wang, Xiao and Changpinyo, Soravit and Piergiovanni, AJ and Padlewski, Piotr and Salz, Daniel and Goodman, Sebastian and Grycner, Adam and Mustafa, Basil and Beyer, Lucas and others",
        "title": "Pali: A jointly-scaled multilingual language-image model"
      },
      {
        "key": "beyer2024paligemmaversatile3bvlm",
        "author": "Lucas Beyer and Andreas Steiner and Andr\u00e9 Susano Pinto and Alexander Kolesnikov and Xiao Wang and Daniel Salz and Maxim Neumann and Ibrahim Alabdulmohsin and Michael Tschannen and Emanuele Bugliarello and Thomas Unterthiner and Daniel Keysers and Skanda Koppula and Fangyu Liu and Adam Grycner and Alexey Gritsenko and Neil Houlsby and Manoj Kumar and Keran Rong and Julian Eisenschlos and Rishabh Kabra and Matthias Bauer and Matko Bo\u0161njak and Xi Chen and Matthias Minderer and Paul Voigtlaender and Ioana Bica and Ivana Balazevic and Joan Puigcerver and Pinelopi Papalampidi and Olivier Henaff and Xi Xiong and Radu Soricut and Jeremiah Harmsen and Xiaohua Zhai",
        "title": "PaliGemma: A versatile 3B VLM for transfer"
      },
      {
        "key": "finiimproved",
        "author": "Fini, Enrico and Astolfi, Pietro and Romero-Soriano, Adriana and Verbeek, Jakob and Drozdzal, Michal",
        "title": "Improved baselines for vision-language pre-training"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "zhai2022lit",
        "author": "Zhai, Xiaohua and Wang, Xiao and Mustafa, Basil and Steiner, Andreas and Keysers, Daniel and Kolesnikov, Alexander and Beyer, Lucas",
        "title": "Lit: Zero-shot transfer with locked-image text tuning"
      },
      {
        "key": "radford2021learning",
        "author": "Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others",
        "title": "Learning transferable visual models from natural language supervision"
      },
      {
        "key": "metzen2024autoclipautotuningzeroshotclassifiers",
        "author": "Jan Hendrik Metzen and Piyapat Saranrittichai and Chaithanya Kumar Mummadi",
        "title": "AutoCLIP: Auto-tuning Zero-Shot Classifiers for Vision-Language Models"
      },
      {
        "key": "gao2021clipadapterbettervisionlanguagemodels",
        "author": "Peng Gao and Shijie Geng and Renrui Zhang and Teli Ma and Rongyao Fang and Yongfeng Zhang and Hongsheng Li and Yu Qiao",
        "title": "CLIP-Adapter: Better Vision-Language Models with Feature Adapters"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "podell2023sdxlimprovinglatentdiffusion",
        "author": "Dustin Podell and Zion English and Kyle Lacey and Andreas Blattmann and Tim Dockhorn and Jonas M\u00fcller and Joe Penna and Robin Rombach",
        "title": "SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis"
      },
      {
        "key": "abdal2021clip2styleganunsupervisedextractionstylegan",
        "author": "Rameen Abdal and Peihao Zhu and John Femiani and Niloy J. Mitra and Peter Wonka",
        "title": "CLIP2StyleGAN: Unsupervised Extraction of StyleGAN Edit Directions"
      },
      {
        "key": "ramesh2022hierarchical",
        "author": "Ramesh, Aditya and Dhariwal, Prafulla and Nichol, Alex and Chu, Casey and Chen, Mark",
        "title": "Hierarchical text-conditional image generation with clip latents"
      },
      {
        "key": "saharia2022photorealistic",
        "author": "Saharia, Chitwan and Chan, William and Saxena, Saurabh and Li, Lala and Whang, Jay and Denton, Emily and Ghasemipour, Seyed Kamyar Seyed and Ayan, Burcu Karagol and Mahdavi, S Sara and Lopes, Rapha Gontijo and others",
        "title": "Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "hessel-etal-2021-clipscore",
        "author": "Hessel, Jack  and\nHoltzman, Ari  and\nForbes, Maxwell  and\nLe Bras, Ronan  and\nChoi, Yejin",
        "title": "{CLIPS}core: A Reference-free Evaluation Metric for Image Captioning"
      },
      {
        "key": "cho2023finegrainedimagecaptioningclip",
        "author": "Jaemin Cho and Seunghyun Yoon and Ajinkya Kale and Franck Dernoncourt and Trung Bui and Mohit Bansal",
        "title": "Fine-grained Image Captioning with CLIP Reward"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "radford2021learning",
        "author": "Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others",
        "title": "Learning transferable visual models from natural language supervision"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "kamath2023whatsupvisionlanguagemodels",
        "author": "Amita Kamath and Jack Hessel and Kai-Wei Chang",
        "title": "What's \"up\" with vision-language models? Investigating their struggle with spatial reasoning"
      },
      {
        "key": "yuksekgonul2023visionlanguagemodelsbehavelike",
        "author": "Mert Yuksekgonul and Federico Bianchi and Pratyusha Kalluri and Dan Jurafsky and James Zou",
        "title": "When and why vision-language models behave like bags-of-words, and what to do about it?"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "zhao2022vl",
        "author": "Zhao, Tiancheng and Zhang, Tianqi and Zhu, Mingwei and Shen, Haozhan and Lee, Kyusong and Lu, Xiaopeng and Yin, Jianwei",
        "title": "VL-CheckList: Evaluating Pre-trained Vision-Language Models with Objects, Attributes and Relations"
      },
      {
        "key": "yuksekgonul2023visionlanguagemodelsbehavelike",
        "author": "Mert Yuksekgonul and Federico Bianchi and Pratyusha Kalluri and Dan Jurafsky and James Zou",
        "title": "When and why vision-language models behave like bags-of-words, and what to do about it?"
      },
      {
        "key": "Parcalabescu_2022",
        "author": "Parcalabescu, Letitia and Cafagna, Michele and Muradjan, Lilitta and Frank, Anette and Calixto, Iacer and Gatt, Albert",
        "title": "VALSE: A Task-Independent Benchmark for Vision and Language Models Centered on Linguistic Phenomena"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "zhang2024countercurateenhancingphysicalsemantic",
        "author": "Jianrui Zhang and Mu Cai and Tengyang Xie and Yong Jae Lee",
        "title": "CounterCurate: Enhancing Physical and Semantic Visio-Linguistic Compositional Reasoning via Counterfactual Examples"
      },
      {
        "key": "awal2024visminvisualminimalchangeunderstanding",
        "author": "Rabiul Awal and Saba Ahmadi and Le Zhang and Aishwarya Agrawal",
        "title": "VisMin: Visual Minimal-Change Understanding"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "vqa2",
        "author": "Goyal, Yash and Khot, Tejas and Summers-Stay, Douglas and Batra, Dhruv and Parikh, Devi",
        "title": "Making the {V} in {VQA} matter: Elevating the role of image understanding in visual question answering"
      },
      {
        "key": "lin2024revisiting",
        "author": "Lin, Zhiqiu and Chen, Xinyue and Pathak, Deepak and Zhang, Pengchuan and Ramanan, Deva",
        "title": "Revisiting the Role of Language Priors in Vision-Language Models"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "hsieh2023sugarcrepe",
        "author": "Cheng-Yu Hsieh and Jieyu Zhang and Zixian Ma and Aniruddha Kembhavi and Ranjay Krishna",
        "title": "SugarCrepe: Fixing Hackable Benchmarks for Vision-Language Compositionality"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "yuksekgonul2023when",
        "author": "Mert Yuksekgonul and Federico Bianchi and Pratyusha   Kalluri and Dan Jurafsky and James Zou",
        "title": "When and why Vision-Language Models behave like  Bags-of-Words, and what to do about it?"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "kamath2023whatsupvisionlanguagemodels",
        "author": "Amita Kamath and Jack Hessel and Kai-Wei Chang",
        "title": "What's \"up\" with vision-language models? Investigating their struggle with spatial reasoning"
      },
      {
        "key": "yuksekgonul2023when",
        "author": "Mert Yuksekgonul and Federico Bianchi and Pratyusha   Kalluri and Dan Jurafsky and James Zou",
        "title": "When and why Vision-Language Models behave like  Bags-of-Words, and what to do about it?"
      },
      {
        "key": "zhang2024countercurateenhancingphysicalsemantic",
        "author": "Jianrui Zhang and Mu Cai and Tengyang Xie and Yong Jae Lee",
        "title": "CounterCurate: Enhancing Physical and Semantic Visio-Linguistic Compositional Reasoning via Counterfactual Examples"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "kamath2023whatsupvisionlanguagemodels",
        "author": "Amita Kamath and Jack Hessel and Kai-Wei Chang",
        "title": "What's \"up\" with vision-language models? Investigating their struggle with spatial reasoning"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "Johnson2016CLEVRAD",
        "author": "Justin Johnson and Bharath Hariharan and Laurens van der Maaten and Li Fei-Fei and C. Lawrence Zitnick and Ross B. Girshick",
        "title": "CLEVR: A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "bordes2023pugphotorealisticsemanticallycontrollable",
        "author": "Bordes, Florian and Shekhar, Shashank and Ibrahim, Mark and Bouchacourt, Diane and Vincent, Pascal and Morcos, Ari",
        "title": "PUG: Photorealistic and Semantically Controllable Synthetic Data for Representation Learning"
      }
    ]
  },
  {
    "index": 18,
    "papers": [
      {
        "key": "yuksekgonul2023visionlanguagemodelsbehavelike",
        "author": "Mert Yuksekgonul and Federico Bianchi and Pratyusha Kalluri and Dan Jurafsky and James Zou",
        "title": "When and why vision-language models behave like bags-of-words, and what to do about it?"
      }
    ]
  },
  {
    "index": 19,
    "papers": [
      {
        "key": "tang2023when",
        "author": "Yingtian Tang and Yutaro Yamada and Yoyo Minzhi Zhang and Ilker Yildirim",
        "title": "When are Lemons Purple? The Concept Association Bias of Vision-Language Models"
      }
    ]
  },
  {
    "index": 20,
    "papers": [
      {
        "key": "locatello2020objectcentriclearningslotattention",
        "author": "Francesco Locatello and Dirk Weissenborn and Thomas Unterthiner and Aravindh Mahendran and Georg Heigold and Jakob Uszkoreit and Alexey Dosovitskiy and Thomas Kipf",
        "title": "Object-Centric Learning with Slot Attention"
      },
      {
        "key": "wu2023slotdiffusionobjectcentricgenerativemodeling",
        "author": "Ziyi Wu and Jingyu Hu and Wuyue Lu and Igor Gilitschenski and Animesh Garg",
        "title": "SlotDiffusion: Object-Centric Generative Modeling with Diffusion Models"
      },
      {
        "key": "seitzer2023bridginggaprealworldobjectcentric",
        "author": "Maximilian Seitzer and Max Horn and Andrii Zadaianchuk and Dominik Zietlow and Tianjun Xiao and Carl-Johann Simon-Gabriel and Tong He and Zheng Zhang and Bernhard Sch\u00f6lkopf and Thomas Brox and Francesco Locatello",
        "title": "Bridging the Gap to Real-World Object-Centric Learning"
      },
      {
        "key": "pmlr-v177-assouel22a",
        "author": "Assouel, Rim and Castrejon, Lluis and Courville, Aaron and Ballas, Nicolas and Bengio, Yoshua",
        "title": "{VIM}: Variational Independent Modules for Video Prediction"
      }
    ]
  },
  {
    "index": 21,
    "papers": [
      {
        "key": "locatello2020objectcentriclearningslotattention",
        "author": "Francesco Locatello and Dirk Weissenborn and Thomas Unterthiner and Aravindh Mahendran and Georg Heigold and Jakob Uszkoreit and Alexey Dosovitskiy and Thomas Kipf",
        "title": "Object-Centric Learning with Slot Attention"
      }
    ]
  },
  {
    "index": 22,
    "papers": [
      {
        "key": "WuInvertedAttentionTC",
        "author": "Yi-Fu Wu and Klaus Greff and Google Deepmind and Gamaleldin F. Elsayed and Michael C. Mozer and Thomas Kipf and Sjoerd van Steenkiste",
        "title": "Inverted-Attention Transformers can Learn Object Representations: Insights from Slot Attention"
      }
    ]
  }
]