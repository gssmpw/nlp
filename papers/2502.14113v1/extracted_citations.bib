@article{Johnson2016CLEVRAD,
  title={CLEVR: A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning},
  author={Justin Johnson and Bharath Hariharan and Laurens van der Maaten and Li Fei-Fei and C. Lawrence Zitnick and Ross B. Girshick},
  journal={2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2016},
  pages={1988-1997},
  url={https://api.semanticscholar.org/CorpusID:15458100}
}

@inproceedings{Parcalabescu_2022,
   title={VALSE: A Task-Independent Benchmark for Vision and Language Models Centered on Linguistic Phenomena},
   url={http://dx.doi.org/10.18653/v1/2022.acl-long.567},
   DOI={10.18653/v1/2022.acl-long.567},
   booktitle={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
   publisher={Association for Computational Linguistics},
   author={Parcalabescu, Letitia and Cafagna, Michele and Muradjan, Lilitta and Frank, Anette and Calixto, Iacer and Gatt, Albert},
   year={2022},
   pages={8253–8280} }

@inproceedings{WuInvertedAttentionTC,
  title={Inverted-Attention Transformers can Learn Object Representations: Insights from Slot Attention},
  author={Yi-Fu Wu and Klaus Greff and Google Deepmind and Gamaleldin F. Elsayed and Michael C. Mozer and Thomas Kipf and Sjoerd van Steenkiste},
  url={https://api.semanticscholar.org/CorpusID:266090680}
}

@misc{abdal2021clip2styleganunsupervisedextractionstylegan,
      title={CLIP2StyleGAN: Unsupervised Extraction of StyleGAN Edit Directions}, 
      author={Rameen Abdal and Peihao Zhu and John Femiani and Niloy J. Mitra and Peter Wonka},
      year={2021},
      eprint={2112.05219},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2112.05219}, 
}

@misc{awal2024visminvisualminimalchangeunderstanding,
      title={VisMin: Visual Minimal-Change Understanding}, 
      author={Rabiul Awal and Saba Ahmadi and Le Zhang and Aishwarya Agrawal},
      year={2024},
      eprint={2407.16772},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2407.16772}, 
}

@misc{beyer2024paligemmaversatile3bvlm,
      title={PaliGemma: A versatile 3B VLM for transfer}, 
      author={Lucas Beyer and Andreas Steiner and André Susano Pinto and Alexander Kolesnikov and Xiao Wang and Daniel Salz and Maxim Neumann and Ibrahim Alabdulmohsin and Michael Tschannen and Emanuele Bugliarello and Thomas Unterthiner and Daniel Keysers and Skanda Koppula and Fangyu Liu and Adam Grycner and Alexey Gritsenko and Neil Houlsby and Manoj Kumar and Keran Rong and Julian Eisenschlos and Rishabh Kabra and Matthias Bauer and Matko Bošnjak and Xi Chen and Matthias Minderer and Paul Voigtlaender and Ioana Bica and Ivana Balazevic and Joan Puigcerver and Pinelopi Papalampidi and Olivier Henaff and Xi Xiong and Radu Soricut and Jeremiah Harmsen and Xiaohua Zhai},
      year={2024},
      eprint={2407.07726},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2407.07726}, 
}

@INPROCEEDINGS{bordes2023pugphotorealisticsemanticallycontrollable,
 author = {Bordes, Florian and Shekhar, Shashank and Ibrahim, Mark and Bouchacourt, Diane and Vincent, Pascal and Morcos, Ari},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
 pages = {45020--45054},
 publisher = {Curran Associates, Inc.},
 title = {PUG: Photorealistic and Semantically Controllable Synthetic Data for Representation Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/8d352fd0f07fde4a74f9476603b3773b-Paper-Datasets_and_Benchmarks.pdf},
 volume = {36},
 year = {2023}
}

@article{bordes2024introduction,
  title={An introduction to vision-language modeling},
  author={Bordes, Florian and Pang, Richard Yuanzhe and Ajay, Anurag and Li, Alexander C and Bardes, Adrien and Petryk, Suzanne and Ma{\~n}as, Oscar and Lin, Zhiqiu and Mahmoud, Anas and Jayaraman, Bargav and others},
  journal={arXiv preprint arXiv:2405.17247},
  year={2024}
}

@inproceedings{cai2023vipllava,
    author      = {Cai, Mu and Liu, Haotian and Mustikovela,  Siva Karthik and Meyer, Gregory P. and Chai, Yuning and Park, Dennis and Lee, Yong Jae},
    title       = {Making Large Multimodal Models Understand Arbitrary Visual Prompts},
    booktitle   = {CVPR 2024},
    year        = {2024}
  }

@inproceedings{changpinyo2021cc12m,
  title = {{Conceptual 12M}: Pushing Web-Scale Image-Text Pre-Training To Recognize Long-Tail Visual Concepts},
  author = {Changpinyo, Soravit and Sharma, Piyush and Ding, Nan and Soricut, Radu},
  booktitle = {CVPR},
  year = {2021},
}

@article{chen2022pali,
  title={Pali: A jointly-scaled multilingual language-image model},
  author={Chen, Xi and Wang, Xiao and Changpinyo, Soravit and Piergiovanni, AJ and Padlewski, Piotr and Salz, Daniel and Goodman, Sebastian and Grycner, Adam and Mustafa, Basil and Beyer, Lucas and others},
  journal={arXiv preprint arXiv:2209.06794},
  year={2022}
}

@misc{cho2023finegrainedimagecaptioningclip,
      title={Fine-grained Image Captioning with CLIP Reward}, 
      author={Jaemin Cho and Seunghyun Yoon and Ajinkya Kale and Franck Dernoncourt and Trung Bui and Mohit Bansal},
      year={2023},
      eprint={2205.13115},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2205.13115}, 
}

@misc{dai2023instructblipgeneralpurposevisionlanguagemodels,
      title={InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning}, 
      author={Wenliang Dai and Junnan Li and Dongxu Li and Anthony Meng Huat Tiong and Junqi Zhao and Weisheng Wang and Boyang Li and Pascale Fung and Steven Hoi},
      year={2023},
      eprint={2305.06500},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2305.06500}, 
}

@article{finiimproved,
  title={Improved baselines for vision-language pre-training},
  author={Fini, Enrico and Astolfi, Pietro and Romero-Soriano, Adriana and Verbeek, Jakob and Drozdzal, Michal},
  year={2023},
  journal={Transactions on Machine Learning Research}
}

@misc{gao2021clipadapterbettervisionlanguagemodels,
      title={CLIP-Adapter: Better Vision-Language Models with Feature Adapters}, 
      author={Peng Gao and Shijie Geng and Renrui Zhang and Teli Ma and Rongyao Fang and Yongfeng Zhang and Hongsheng Li and Yu Qiao},
      year={2021},
      eprint={2110.04544},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2110.04544}, 
}

@proceedings{hessel-etal-2021-clipscore,
    title = "{CLIPS}core: A Reference-free Evaluation Metric for Image Captioning",
    author = "Hessel, Jack  and
      Holtzman, Ari  and
      Forbes, Maxwell  and
      Le Bras, Ronan  and
      Choi, Yejin",
    editor = "Moens, Marie-Francine  and
      Huang, Xuanjing  and
      Specia, Lucia  and
      Yih, Scott Wen-tau",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.595",
    doi = "10.18653/v1/2021.emnlp-main.595",
    pages = "7514--7528",
    abstract = "Image captioning has conventionally relied on reference-based automatic evaluations, where machine captions are compared against captions written by humans. This is in contrast to the reference-free manner in which humans assess caption quality. In this paper, we report the surprising empirical finding that CLIP (Radford et al., 2021), a cross-modal model pretrained on 400M image+caption pairs from the web, can be used for robust automatic evaluation of image captioning without the need for references. Experiments spanning several corpora demonstrate that our new reference-free metric, CLIPScore, achieves the highest correlation with human judgements, outperforming existing reference-based metrics like CIDEr and SPICE. Information gain experiments demonstrate that CLIPScore, with its tight focus on image-text compatibility, is complementary to existing reference-based metrics that emphasize text-text similarities. Thus, we also present a reference-augmented version, RefCLIPScore, which achieves even higher correlation. Beyond literal description tasks, several case studies reveal domains where CLIPScore performs well (clip-art images, alt-text rating), but also where it is relatively weaker in comparison to reference-based metrics, e.g., news captions that require richer contextual knowledge.",
}

@inproceedings{hsieh2023sugarcrepe,
      title={SugarCrepe: Fixing Hackable Benchmarks for Vision-Language Compositionality}, 
      author={Cheng-Yu Hsieh and Jieyu Zhang and Zixian Ma and Aniruddha Kembhavi and Ranjay Krishna},
      year={2023},
      booktitle={NeurIPS 2023}
}

@misc{ilharco2021openclip,
        author={Gabriel Ilharco and Mitchell Wortsman and Ross Wightman and Cade Gordon and Nicholas Carlini and Rohan Taori and Achal Dave and Vaishaal Shankar and Hongseok Namkoong and John Miller and Hannaneh Hajishirzi and Ali Farhadi and Ludwig Schmidt},
        title={OpenCLIP},
        year={2021},
        version={0.1},
        doi={10.5281/zenodo.5143773},
        url={https://doi.org/10.5281/zenodo.5143773}
}

@misc{kamath2023whatsupvisionlanguagemodels,
      title={What's "up" with vision-language models? Investigating their struggle with spatial reasoning}, 
      author={Amita Kamath and Jack Hessel and Kai-Wei Chang},
      year={2023},
      eprint={2310.19785},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2310.19785}, 
}

@article{lin2024revisiting,
  title={Revisiting the Role of Language Priors in Vision-Language Models},
  author={Lin, Zhiqiu and Chen, Xinyue and Pathak, Deepak and Zhang, Pengchuan and Ramanan, Deva},
  journal={arXiv preprint arXiv:2306.01879},
  year={2024}
}

@article{liu2023llava,
    author      = {Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae},
    title       = {Visual Instruction Tuning},
    journal   = {NeurIPS 2023},
    year        = {2023}
  }

@misc{liu2024llavanext,
    title={LLaVA-NeXT: Improved reasoning, OCR, and world knowledge},
    url={https://llava-vl.github.io/blog/2024-01-30-llava-next/},
    author={Liu, Haotian and Li, Chunyuan and Li, Yuheng and Li, Bo and Zhang, Yuanhan and Shen, Sheng and Lee, Yong Jae},
    month={January},
    year={2024}
}

@misc{locatello2020objectcentriclearningslotattention,
      title={Object-Centric Learning with Slot Attention}, 
      author={Francesco Locatello and Dirk Weissenborn and Thomas Unterthiner and Aravindh Mahendran and Georg Heigold and Jakob Uszkoreit and Alexey Dosovitskiy and Thomas Kipf},
      year={2020},
      eprint={2006.15055},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2006.15055}, 
}

@misc{metzen2024autoclipautotuningzeroshotclassifiers,
      title={AutoCLIP: Auto-tuning Zero-Shot Classifiers for Vision-Language Models}, 
      author={Jan Hendrik Metzen and Piyapat Saranrittichai and Chaithanya Kumar Mummadi},
      year={2024},
      eprint={2309.16414},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2309.16414}, 
}

@InProceedings{pmlr-v177-assouel22a,
  title = 	 {{VIM}: Variational Independent Modules for Video Prediction},
  author =       {Assouel, Rim and Castrejon, Lluis and Courville, Aaron and Ballas, Nicolas and Bengio, Yoshua},
  booktitle = 	 {Proceedings of the First Conference on Causal Learning and Reasoning},
  pages = 	 {70--89},
  year = 	 {2022},
  editor = 	 {Schölkopf, Bernhard and Uhler, Caroline and Zhang, Kun},
  volume = 	 {177},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {11--13 Apr},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v177/assouel22a/assouel22a.pdf},
  url = 	 {https://proceedings.mlr.press/v177/assouel22a.html},
  abstract = 	 {We introduce a variational inference model called VIM, for Variational Independent Modules, for sequential data that learns and infers latent representations as a set of objects and discovers modular causal mechanisms over these objects. These mechanisms - which we call modules - are independently parametrized, define the stochastic transitions of entities and are shared across entities.  At each time step, our model infers from a low-level input sequence a high-level sequence of categorical latent variables to select which transition modules to apply to which high-level object. We evaluate this model in video prediction tasks where the goal is to predict multi-modal future events given previous observations. We demonstrate empirically that VIM can model 2D visual sequences in an interpretable way and is able to identify the underlying dynamically instantiated mechanisms of the generation process.  We additionally show that the learnt modules can be composed at test time to generalize to out-of-distribution observations.}
}

@misc{podell2023sdxlimprovinglatentdiffusion,
      title={SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis}, 
      author={Dustin Podell and Zion English and Kyle Lacey and Andreas Blattmann and Tim Dockhorn and Jonas Müller and Joe Penna and Robin Rombach},
      year={2023},
      eprint={2307.01952},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2307.01952}, 
}

@inproceedings{radford2021learning,
  title={Learning transferable visual models from natural language supervision},
  author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
  booktitle={International conference on machine learning},
  pages={8748--8763},
  year={2021},
  organization={PMLR}
}

@article{ramesh2022hierarchical,
  title={Hierarchical text-conditional image generation with clip latents},
  author={Ramesh, Aditya and Dhariwal, Prafulla and Nichol, Alex and Chu, Casey and Chen, Mark},
  journal={arXiv preprint arXiv:2204.06125},
  year={2022}
}

@article{saharia2022photorealistic,
  title={Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding},
  author={Saharia, Chitwan and Chan, William and Saxena, Saurabh and Li, Lala and Whang, Jay and Denton, Emily and Ghasemipour, Seyed Kamyar Seyed and Ayan, Burcu Karagol and Mahdavi, S Sara and Lopes, Rapha Gontijo and others},
  journal={arXiv preprint arXiv:2205.11487},
  year={2022}
}

@misc{schuhmann2022laion5bopenlargescaledataset,
      title={LAION-5B: An open large-scale dataset for training next generation image-text models}, 
      author={Christoph Schuhmann and Romain Beaumont and Richard Vencu and Cade Gordon and Ross Wightman and Mehdi Cherti and Theo Coombes and Aarush Katta and Clayton Mullis and Mitchell Wortsman and Patrick Schramowski and Srivatsa Kundurthy and Katherine Crowson and Ludwig Schmidt and Robert Kaczmarczyk and Jenia Jitsev},
      year={2022},
      eprint={2210.08402},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2210.08402}, 
}

@misc{seitzer2023bridginggaprealworldobjectcentric,
      title={Bridging the Gap to Real-World Object-Centric Learning}, 
      author={Maximilian Seitzer and Max Horn and Andrii Zadaianchuk and Dominik Zietlow and Tianjun Xiao and Carl-Johann Simon-Gabriel and Tong He and Zheng Zhang and Bernhard Schölkopf and Thomas Brox and Francesco Locatello},
      year={2023},
      eprint={2209.14860},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2209.14860}, 
}

@inproceedings{vqa2,
  title={Making the {V} in {VQA} matter: Elevating the role of image understanding in visual question answering},
  author={Goyal, Yash and Khot, Tejas and Summers-Stay, Douglas and Batra, Dhruv and Parikh, Devi},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={6904--6913},
  year={2017}
}

@misc{wu2023slotdiffusionobjectcentricgenerativemodeling,
      title={SlotDiffusion: Object-Centric Generative Modeling with Diffusion Models}, 
      author={Ziyi Wu and Jingyu Hu and Wuyue Lu and Igor Gilitschenski and Animesh Garg},
      year={2023},
      eprint={2305.11281},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2305.11281}, 
}

@misc{yuksekgonul2023visionlanguagemodelsbehavelike,
      title={When and why vision-language models behave like bags-of-words, and what to do about it?}, 
      author={Mert Yuksekgonul and Federico Bianchi and Pratyusha Kalluri and Dan Jurafsky and James Zou},
      year={2023},
      eprint={2210.01936},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2210.01936}, 
}

@misc{zeng2022multigrainedvisionlanguagepretraining,
      title={Multi-Grained Vision Language Pre-Training: Aligning Texts with Visual Concepts}, 
      author={Yan Zeng and Xinsong Zhang and Hang Li},
      year={2022},
      eprint={2111.08276},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2111.08276}, 
}

@inproceedings{zhai2022lit,
  title={Lit: Zero-shot transfer with locked-image text tuning},
  author={Zhai, Xiaohua and Wang, Xiao and Mustafa, Basil and Steiner, Andreas and Keysers, Daniel and Kolesnikov, Alexander and Beyer, Lucas},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={18123--18133},
  year={2022}
}

@misc{zhai2022litzeroshottransferlockedimage,
      title={LiT: Zero-Shot Transfer with Locked-image text Tuning}, 
      author={Xiaohua Zhai and Xiao Wang and Basil Mustafa and Andreas Steiner and Daniel Keysers and Alexander Kolesnikov and Lucas Beyer},
      year={2022},
      eprint={2111.07991},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2111.07991}, 
}

@misc{zhang2024countercurateenhancingphysicalsemantic,
      title={CounterCurate: Enhancing Physical and Semantic Visio-Linguistic Compositional Reasoning via Counterfactual Examples}, 
      author={Jianrui Zhang and Mu Cai and Tengyang Xie and Yong Jae Lee},
      year={2024},
      eprint={2402.13254},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2402.13254}, 
}

@article{zhao2022vl,
  title={VL-CheckList: Evaluating Pre-trained Vision-Language Models with Objects, Attributes and Relations},
  author={Zhao, Tiancheng and Zhang, Tianqi and Zhu, Mingwei and Shen, Haozhan and Lee, Kyusong and Lu, Xiaopeng and Yin, Jianwei},
  journal={arXiv preprint arXiv:2207.00221},
  year={2022}
}

