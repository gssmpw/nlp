\newif\ifarxiv
\arxivtrue % compiles arxiv version 
% \arxivfalse % compiles ICML version

\ifarxiv
    \documentclass[]{fairmeta} % Option "twocolumn" available, but please prioritize single-column
    \usepackage{algorithm}
    %\usepackage{algorithmic}
    %\usepackage{algpseudocode}
    \usepackage{amsmath, amssymb}

\else 
    \documentclass{article} % For LaTeX2e
    \usepackage{icml2025,times}

    % Optional math commands from https://github.com/goodfeli/dlbook_notation.
    \input{math_commands.tex}
    
    % Attempt to make hyperref and algorithmic work together better:
    \newcommand{\theHalgorithm}{\arabic{algorithm}}
    
    % Use the following line for the initial blind version submitted for review:
    \usepackage{icml2025}

    % If accepted, instead use the following line for the camera-ready submission:
    %\usepackage[accepted]{icml2025}

    % if you use cleveref..
    \usepackage[capitalize,noabbrev]{cleveref}

\fi

    
\usepackage{microtype}
\usepackage{graphicx}
% \usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{pgfplots}
% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{tabularx}
\usepackage{booktabs} % For better table lines
\usepackage{caption} % For improved caption styling
\usepackage{xcolor}
\usepackage{natbib}
\usepackage{adjustbox}
\usepackage{xcolor}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{wrapfig}
\usepackage{multirow} 
\usepackage{listings}
\usepackage{algpseudocode}
\usepackage{enumitem}
%\usepackage{minted}
%\usepackage{tcolorbox}
\PassOptionsToPackage{table}{xcolor}\usepackage{tcolorbox}
\usepackage{sidecap}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}
\def\myparagraph#1{\noindent\textbf{#1}\hspace{0mm}}

\newcommand{\ars}[1]{{\color{blue} #1}}
\newcommand{\md}[1]{{\color{green} #1}}
\newcommand{\rim}[1]{{\color{red} #1}}
\newcommand{\rot}[1]{\rotatebox{90}{#1}}
\newcommand{\new}[1]{{\color{black}#1}}
% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


\ifarxiv
    \author[1, 2, 3]{Rim Assouel}
    \author[1]{Florian Bordes}
    \author[1]{Pietro Astolfi}
    \author[1]{Michal Drozdzal}
    \author[1, 2, 4, 5]{Adriana Romero-Soriano}
    
    \affiliation[1]{FAIR at Meta - Montreal}
    \affiliation[2]{Mila}
    \affiliation[3]{Universit\'{e} de Montr\'{e}al}
    \affiliation[4]{McGill University}
    \affiliation[5]{Canada CIFAR AI chair}
    
    % \contribution[*]{Equal contribution.}
    
    \abstract{
    \input{abstract}
    }
    
    \date{Feb. 5, 2025}
    \correspondence{\email{assouelr@mila.quebec}}
    
    % \metadata[Code]{\url{https://github.com/facebookresearch/repo}}
    % \metadata[Blogpost]{\url{https://ai.meta.com/blog/?page=1}}
\fi

\ifarxiv
    \title{Object-centric Binding in Contrastive Language-Image Pretraining}
\else
% A short form for the running title is supplied here:
    \icmltitlerunning{Submission and Formatting Instructions for ICML 2025}
\fi


\newcommand{\fix}{\marginpar{FIX}}
%\newcommand{\new}{\marginpar{NEW}}

\begin{document}

\ifarxiv
    \maketitle
\else
    \twocolumn[
    \icmltitle{Object-centric Binding in Contrastive Language-Image Pretraining}

    % It is OKAY to include author information, even for blind
    % submissions: the style file will automatically remove it for you
    % unless you've provided the [accepted] option to the icml2025
    % package.
    
    % List of affiliations: The first argument should be a (short)
    % identifier you will use later to specify author affiliations
    % Academic affiliations should list Department, University, City, Region, Country
    % Industry affiliations should list Company, City, Region, Country
    
    % You can specify symbols, otherwise they are numbered in order.
    % Ideally, you should not use this facility. Affiliations will be numbered
    % in order of appearance and this is the preferred way.
    \icmlsetsymbol{equal}{*}

    
    \begin{icmlauthorlist}
    \icmlauthor{Rim Assouel}{yyy,comp,uni}
    \icmlauthor{Florian Bordes}{yyy}
    \icmlauthor{Pietro Astolfi}{yyy}
    \icmlauthor{Michal Drozdzal}{yyy}
    \icmlauthor{Adriana Romero-Soriano}{yyy,comp,sch,cifar}
    \end{icmlauthorlist}
    
    \icmlaffiliation{yyy}{FAIR at Meta - Montreal}
    \icmlaffiliation{comp}{Mila}
    \icmlaffiliation{uni}{Universit\'{e} de Montr\'{e}al}
    \icmlaffiliation{sch}{McGill University}
    \icmlaffiliation{cifar}{Canada CIFAR AI chair}
    
    \icmlcorrespondingauthor{Rim Assouel}{assouelr@mila.quebec}

    % You may provide any keywords that you
    % find helpful for describing your paper; these are used to populate
    % the "keywords" metadata in the PDF but will not be shown in the document
    \icmlkeywords{Machine Learning, ICML}
    
    \vskip 0.3in]
    
    \begin{abstract}
    \input{abstract}
    \end{abstract}

\fi


\section{Introduction}

Recent advancements in multi-modal representation learning have primarily been enabled by the introduction of CLIP~\citep{radford2021learning}. CLIP learns aligned image-text representations from Internet-scale data. Despite its success, CLIP exhibits limitations in understanding complex scenes composed of multiple objects~\citep{kamath2023whatsupvisionlanguagemodels, yuksekgonul2023visionlanguagemodelsbehavelike,doveh2023teachingstructuredvisionlanguageconcepts,paiss2023teaching}. For instance, while capable of recognizing individual objects, CLIP struggles with interpreting spatial relationships among objects in the scene(\textit{e.g.}, ``the cat is to the left of the mat'' \textit{vs.} ``the cat is to the right of the mat'') and adequately associating objects with their corresponding attributes (\textit{e.g.}, ``a red square and a blue circle'' \textit{vs.} ``a blue square and a red circle''). The process of acquiring this compositional understanding of the world is known as the \emph{binding problem} in the literature, and may be decomposed into \emph{segregation}, \emph{representation}, and \emph{composition} problems~\citep{greff2020bindingproblemartificialneural}.\looseness-1

Efforts to improve the compositional understanding of CLIP-like models have largely relied on leveraging \textit{hard negative examples}\footnote{Hard-negatives are additional samples that either contain subtle visual changes in the image and/or subtle linguistic/semantic difference in the caption and are sampled as negatives in the same batch.}, either in the text space~\citep{kalantidis2020hardnegativemixingcontrastive, yuksekgonul2023when, zhang2024contrastingintramodalrankingcrossmodal,doveh2023teachingstructuredvisionlanguageconcepts,paiss2023teaching} -- to improve sensitivity to the order of words and subtle textual differences -- or the image space~\citep{awal2024visminvisualminimalchangeunderstanding, le2023cococounterfactuals, zhang2024countercurateenhancingphysicalsemantic} -- to improve sensitivity to subtle visual differences. 
Although these methods have somewhat improved CLIP-like models' performance on scene compositionality benchmarks~\citep{Parcalabescu_2022,zhao2022vl,yuksekgonul2023when,hsieh2023sugarcrepefixinghackablebenchmarks}, they do not explicitly address the binding problem as they focus mainly on enhancing the model’s representation capabilities with additional data, hindering their generalization to unseen scene compositions.\looseness-1% Thus, endowing CLIP-like models with compositional understanding of the scenes remains an open challenge.\looseness-1

Yet, the literature on object-centric representation learning~\citep{eslami2016attendinferrepeatfast, greff2020multiobjectrepresentationlearningiterative, 
locatello2020objectcentriclearningslotattention, wu2023slotdiffusionobjectcentricgenerativemodeling, seitzer2023bridginggaprealworldobjectcentric} has long focused on devising methods to address the segregation and representation problems as a way to facilitate the subsequent compositional processing of images. This has led to the development of inductive biases to segregate different objects in a scene into distinct representational \emph{slots}, which have been shown to naturally scale to an increasing number of visual objects and relations~\citep{locatello2020objectcentriclearningslotattention, NEURIPS2023_e3cdc587, mondal2024slotabstractorsscalableabstract, savi++}. 
% These works often propose architectural improvements to learn separate distributed embeddings for each object in the scene in an unsupervised manner, and have been shown to naturally scale to an increasing number of visual objects. 
To the best of our knowledge, advances in object-centric representation learning are yet to be explored in the vision-language domain.\looseness-1

Therefore, in this paper, we focus on enhancing the compositional scene understanding of CLIP-like models by leveraging advances from object-centric representation learning. In particular, we propose to endow CLIP-based vision-language architectures with segregation and composition capabilities. Our core idea is to adapt the slot-centric representation paradigm for CLIP architectures and dynamically align each representational slot with the object entities mentioned in the text. To do so, we design a binding module that connects a scene graph, derived from the textual description, with a slot-structured image representation. We utilize the scene graph's relationships as constraints to effectively capture the complex interactions among the visual entities represented as slots. Our enhanced model, which we refer to as Object-Centric CLIP (OC-CLIP), not only boosts CLIP's performance in understanding multi-object compositional scenes but also improves the sample efficiency of the model when trained from scratch.\looseness-1 %accuracy of image-text matching in complex and highly compositional visual scenarios.\looseness-1


Our contributions are summarized as follows:
\ifarxiv
\else
    \vspace{-2em}
\fi
\begin{itemize}[noitemsep]
    \item We introduce OC-CLIP, a model which endows CLIP-based architectures with segregation and composition capabilities to address the binding problem.\looseness-1
    \item We evaluate the sample efficiency of our approach against methods leveraging hard negative augmentations in a controlled 3D environment and show the overall efficiency of OC-CLIP compared to both text and image based a hard-negative augmentations.\looseness-1
    \item We demonstrate that OC-CLIP significantly enhances the binding of object-centric attributes and spatial relationships across a representative set of challenging real-world compositional image-text matching benchmarks. Notably, we report an increase of $\textbf{16.5\%}$ accuracy in the challenging \emph{swap-attribute} split of SugarCrepe compared to OpenCLIP~\citep{ilharco2021openclip} finetuned in-domain, and go from random chance to more than $\textbf{89\%}$ on COCO-spatial and $\textbf{92\%}$on GQA-spatial from the Whatsup benchmark~\citep{kamath2023whatsupvisionlanguagemodels}.\looseness-1
    \item We show the scaling potential of OC-CLIP when trained from scratch on noisy data~\citep{changpinyo2021cc12m, sharma-etal-2018-conceptual} datasets. We report an increase of \textbf{12.7\%} accuracy in zero-shot ImageNet classification compared to OpenCLIP.
\end{itemize}

\iffalse
Recent advancements in zero-shot image classification have primarily been driven by models such as CLIP~\cite{radford2021learning}, which learn to associate visual entities with their corresponding textual descriptions without explicit supervision.
Despite their success, these models exhibit limitations in understanding complex compositional scenes involving multiple objects and their spatial relationships~\cite{kamath2023whatsupvisionlanguagemodels, yuksekgonul2023visionlanguagemodelsbehavelike}. For instance, while capable of recognizing individual objects, CLIP-like models still struggle with interpreting spatial relationships~\cite{} (``the cat is to the left of the mat'' against ``the cat is to the right of the mat'') and binding attributes and correctly associating attributes with the right objects in scenes where multiple objects are present (e.g., differentiating ``a red square and a blue circle'' from ``a blue square and a red circle'').  \todo{We could consider adding a visual highlighting problems of CLIP, e.g. winoground or PUG and show how OC-CLIP works there.}


Efforts to enhance the compositional understanding in models like CLIP have largely been centered around data-centric methods and involve using \textit{hard negative examples} which improve CLIP’s sensitivity to the order of words and subtle visual differences by swapping caption's linguistic elements (nouns, attributes, verb phrases, etc.)~\cite{}, and sampling images with minor visual modifications within the same batch~\cite{}. 
%Efforts to enhance the compositional understanding in models like CLIP have largely been centered around data-centric methods, such as fine-tuning on datasets with minimal semantic differences in textual and/or visual content~\cite{}. Leading approaches involve using \textit{hard negative examples} which has shown potential in improving CLIP’s sensitivity to the order of words and subtle visual differences by swapping caption's linguistic elements (nouns, attributes, verb phrases, etc.)~\cite{}, and sampling images with minor visual modifications within the same batch~\cite{}. 
% Though these methods have somewhat improved CLIP's ability to distinguish between incorrectly replaced  attributes, objects, or relationships, challenges remain, particularly in tasks demanding precise attribute binding~\cite{} and understanding spatial relations~\cite{}.
Though these methods have somewhat improved CLIP's ability to distinguish between scenes -- with replaced  attributes, objects, or relationships -- precise attribute object assignment~\cite{} and understanding spatial relations~\cite{} remain to be an open challenge. 

% challenges remain, particularly in tasks demanding precise attribute binding~\cite{} and understanding spatial relations~\cite{}.

In this work, we explore an alternative approach and %to the prevalent data-centric paradigm by examining 
examine the inherent inductive biases in CLIP's architecture, focusing on the binding problem. The binding problem in the context of deep learning representation %, as discussed in deep learning literature 
\citep{greff2020bindingproblemartificialneural} highlights the difficulty of keeping the right assignment of features and objects within a distributed neural network representation. In order to scale to an increasing number of objects in a scene, a compositional representation needs a mechanism to separate the information relative to each object and as a result prevent any interferences between objects. Such feature interferences, known as the 'superposition catastrophe', can be particularly pronounced in non structured vector-like representation like the ones used in CLIP. Orthogonally, recent advances~\citep{locatello2020objectcentriclearningslotattention} in object-centric representation learning have led to the development of inductive biases that help segregate different objects into distinct representational \emph{slots}. They focus on architectural improvements to push vision models to learn a separate distirbuted vector for each object in the scene in a unsupervised way. These models have been shown to naturally scale to a increasing number of visual objects.

%In this work, we explore an alternative approach and %to the prevalent data-centric paradigm by examining 
%examine the inherent inductive biases in CLIP's architecture, focusing on the binding problem. In particular, we exploit recent advances in object-centric representation learning such as slot attention~\cite{} that segregates different objects into distinct representational \emph{slots}.

Inspired by these advances, our work aims to modify CLIP’s architecture by incorporating \emph{perceptual grouping} inductive biases  conditioned on the candidate caption.  The core idea is to adapt the slot-centric representation paradigm for CLIP, where each slot is dynamically aligned with the textual entities mentioned in the caption.
To do so, we have designed a binding module that connects a scene graph derived from the input caption with a slot-structured image representation. Additionally, we utilize relationships as text-conditioned visual constraints to effectively capture the complex interactions between objects and their relationships. Our enhanced model, which we refer to as OC-CLIP, not only boosts CLIP's performance in understanding multi-object compositional scenes but also improves the accuracy and efficiency of image-text matching in complex and highly compositional visual scenarios.
Our contributions are as follows : 
\begin{itemize}
    \item We introduce a versatile binding module, complete with a structured similarity score, that links a patch-based visual representation with a scene graph parsed from a candidate caption.
    \item We evaluate the sample efficiency of our approach against the traditional data-centric method of hard negative augmentation in a controlled 3D environment.
    \item We demonstrate that OC-CLIP significantly enhances the binding of object-centric attributes and spatial relationships across a representative set of challenging real-world compositional image-to-text matching benchmarks.
\end{itemize}

\fi






\section{Related Work}
\myparagraph{Contrastive Pretraining of VLMs.}
Vision-language models (VLMs) have made substantial strides in both the vision and multi-modal domains~\citep{bordes2024introduction}. Modern VLMs are pretrained on vast, diverse and oftentimes noisy multi-modal datasets~\citep{changpinyo2021cc12m,schuhmann2022laion5bopenlargescaledataset,ilharco2021openclip, zeng2022multigrainedvisionlanguagepretraining}, and have shown substantial improvements when applied to various zero-shot tasks. CLIP~\citep{radford2021learning} presented a contrastive learning approach used for pretraining, which involves training the model to differentiate between similar and dissimilar image-text pairs. This approach encourages the model to learn a shared representation space for images and text, where semantically similar pairs are close together and dissimilar pairs are far apart. Following CLIP's lead, image-text contrastive learning has become a prevalent strategy for VLM pretraining~\citep{liu2023llava,cai2023vipllava,liu2024llavanext, dai2023instructblipgeneralpurposevisionlanguagemodels, zhai2022litzeroshottransferlockedimage, chen2022pali, beyer2024paligemmaversatile3bvlm, finiimproved}. Contrastive vision-language pretraining spans numerous downstream applications, including zero-shot image classification~\citep{zhai2022lit,radford2021learning, metzen2024autoclipautotuningzeroshotclassifiers,gao2021clipadapterbettervisionlanguagemodels},  text-to-image generation~\citep{podell2023sdxlimprovinglatentdiffusion,abdal2021clip2styleganunsupervisedextractionstylegan, ramesh2022hierarchical,saharia2022photorealistic}, as well as assessing text-image alignment~\citep{hessel-etal-2021-clipscore, cho2023finegrainedimagecaptioningclip}. In this work, we are particularly interested in the ability of CLIP-based models to evaluate compositional text-image alignment.\looseness-1

\iffalse
Contrastive-Language Image Pretraining (CLIP)~\citep{radford2021learning} models are trained by defining an image and its corresponding caption as positive pairs while the same image and all the other captions present in a mini-batch are defined as negative pairs. By discriminating the positive pair from the negative one, CLIP models can learn an alignment between visual clues in images and text. Despite having impressive results on zero-shot object detection tasks, such approach has been shown to be limited in learning more abstract concepts such as relations between objects or attributes~\citep{kamath2023whatsupvisionlanguagemodels, yuksekgonul2023visionlanguagemodelsbehavelike}.
\fi

\myparagraph{Compositional Understanding Benchmarks.} 
Several benchmarks have been developed to assess the compositional understanding of VLMs. In this work, we focus on benchmarks structured as cross-modal retrieval tasks where the model needs to distinguish between correct and incorrect text descriptions given an image, and evaluations are based on accuracy metrics. The majority of these benchmarks~\citep{zhao2022vl, yuksekgonul2023visionlanguagemodelsbehavelike, Parcalabescu_2022} rely on the rule-based construction of negative captions and the generation of their associated image counter-factuals~\citep{zhang2024countercurateenhancingphysicalsemantic, awal2024visminvisualminimalchangeunderstanding}. Yet, many of these benchmarks may be solved by leveraging the language prior exclusively~\citep{vqa2, lin2024revisiting}, hence disregarding the information from the visual input. To address this, benchmarks such as SugarCrepe~\citep{hsieh2023sugarcrepe} leverage large language models to generate plausible and linguistically correct hard negatives, and show that previously introduced text-based hard negative strategies are not always effective~\citep{yuksekgonul2023when} -- \textit{e.g.}, when considering attribute and object swaps between textual descriptions.
Other benchmarks focus on assessing the VLMs' spatial understanding~\citep{kamath2023whatsupvisionlanguagemodels, yuksekgonul2023when, zhang2024countercurateenhancingphysicalsemantic}, and propose to finetune CLIP-based models on data containing a high proportion of spatial relationships since these relationships tend to be under-represented in commonly used pretraining datasets. Interestingly,~\citet{kamath2023whatsupvisionlanguagemodels} show that even when finetuning with in-domain data containing an over-representation of spatial relationships, state-of-the-art models still exhibit a close to random chance performance. In this work, we test the hypothesis that spatial relationship failures are due to the lack of composition in the similarity score computation used to train CLIP-like models.\looseness-1


\iffalse
\myparagraph{PUG environment?}

\todo{Difficulty of evaluate the sample-efficiency of the effect hard negatives at scale so we first validate it in a controlled environment where we have access to the entire vocab + existing hard negatives }


Because of the difficulty of getting negative ground truth images, researchers often use synthetic data such like CLEVR~\citep{Johnson2016CLEVRAD} to generate a balanced set of negative images and captions. In this work, we decided to leverage the PUG framework~\citep{bordes2023pugphotorealisticsemanticallycontrollable} to make a synthetic image dataset with realistic 
\fi


\myparagraph{Object-centric Binding Inductive Biases.}
CLIP has been shown~\citep{yuksekgonul2023visionlanguagemodelsbehavelike} to be pushed to learn disentangled, bag-of-words-style representations from the contrastive loss and the easily distinguishable negatives typically used for pretraining. Although the learned representations might be effective for objects presented in isolation, they struggle with scenes containing multiple objects~\citep{tang2023when}. For example, consider a simple scene with a green apple and a yellow banana. In this case, the model must maintain and correctly link the attributes (``green'', ``yellow'') to the objects (``apple'', ``banana''), without mixing the concepts  -- \textit{e.g.}, ``yellow apple'' or `green banana''. This exemplifies the importance of devising robust mechanisms within the CLIP architecture and/or training to accurately handle multiple objects, while preventing feature interferences. 
% Ideally, the model should represent ``green'' and ``apple'' distinctly when only the apple is present, and "yellow" and ``banana'' when only the banana is visible. However, challenges arise when both fruits appear together. The network must then maintain and correctly link each attribute (``green'', ``yellow'', ``apple'', ``banana'') without mixing them up, such as mistakenly creating a ``yellow apple'' or a ``green banana''.This example underscores the need for robust mechanisms within the CLIP model to accurately handle multiple objects and prevent feature interferences. 
In this work, we focus on equipping CLIP with object-centric binding inductive biases and take inspiration from the architectures proposed in the unsupervised object-centric visual representation learning literature~\citep{
locatello2020objectcentriclearningslotattention,wu2023slotdiffusionobjectcentricgenerativemodeling,seitzer2023bridginggaprealworldobjectcentric,pmlr-v177-assouel22a}. Many recent image-only approaches follow a simple inductive bias introduced by slot attention~\citep{locatello2020objectcentriclearningslotattention}, where  an image -- encoded as a set of input tokens -- is soft partitioned into
K slots. In particular, attention maps are computed via an \textbf{inverted cross attention} mechanism~\citep{WuInvertedAttentionTC}, where the softmax is applied along the query dimension in order to induce a competition between the slots to explain different groups of input tokens. In this work, we extend these inductive biases to define text-conditioned visual slots from the input image.\looseness-1

\section{Method}
Our goal is to enhance CLIP-based architectures with object-centric binding and composition capabilities. %In particular, our method aims to bind relevant parts of an image with objects mentioned in a text without supervision. 
% This is achieved through a text-to-image \emph{binding module} and a \emph{structured similarity score} used to train the model. 
Our method starts by extracting representations of distinct open-ended objects and relationships in a textual description, as well as representations of patches in an image. Next, a binding module matches the text representation of objects to the relevant image patches, producing a slot-centric representation of the image. Finally, a structured similarity score compares the slot-centric representation with the textual representations of different objects, and leverages the extracted relationships as constraints applied to the visual slots. 
%and uses the relationships mentionned in the caption as additional contraints applied to the corresponding subject slot and object slot. 
% bind text representations of \textit{objects} mentioned in a description to their corresponding relevant parts in an image without supervision. 
%Overall, our model consists of four main components: a visual patch encoder, a text encoder, a binding module, and a multimodal scoring function. 
Our key contributions lie in the design of the \emph{binding module}\footnote{Code  for the binding module is given in the Appendix Fig~\ref{fig:code}.} and the proposal of the \emph{structured similarity score}, which we detail in sections~\ref{ssec:bindingmod} and~\ref{sec:our_score}, respectively. Figure~\ref{fig:model} presents an overview of the proposed approach. Our approach relies on a scene-graph representation of the text modality. We assume the parser is given and 
orthogonal to our approach and discuss the choice of the parsing method in Appendix~\ref{app:parsing}.\looseness-1

\myparagraph{Notation.}
We denote as $\mathbf{x}$ an image of shape $\mathbb{R}^{h \times w \times 3}$ and as $\mathbf{\bar{x}} = [\mathbf{\bar{x}}^1, ..., \mathbf{\bar{x}}^N] = E_\phi(\mathbf{x}) \in \mathbb{R}^{N \times d} $ its patch-level encoding, where $E_\phi$ is an image encoder -- typically a pre-trained ViT~\citep{dosovitskiy2020image} -- $N$ is the number of patches and $d$ the dimensionality of the patch embeddings. We denote as $t$ the text description, or caption, associated with $\mathbf{x}$. We extract a scene graph. 
%used to extract scene graph from the text description is orthogonal to our work. We discuss the choice of parser in Appendix~\ref{app:parsing}}}, $\mathcal{G}$ from $t$ by leveraging an LLM-based parsing approach. $\mathcal{G}$ is composed of a set of nodes $\mathcal{N} = \{N^1, ..., N^M\}$ representing the $M$ objects in $t$ and of a set of edges $\mathcal{E} = \{(\mathbf{r}^1, s^1, o^1),..., (\mathbf{r}^P, s^P, o^P)\}$ representing the $P$ relationships in $t$. Each relationship is represented by a tuple $(\mathbf{r}, s, o)$, where $\mathbf{r}$ is the embedding of the predicate, $s$ the subject and $o$ the object of the relationship. 
For example, the scene graph of ``A red apple to the left of a blue car'' will be represented with the set of nodes \{``red apple'', ``blue car''\} and the set of edges \{(``to the left of'', ``red apple'', ``blue car'')\}. In practice, we represent $\mathcal{N}$ as a matrix of node features $\mathbf{N}$, where each row contains the embedding of a node in the graph. Moreover, we represent each $s^i$ and $o^i$ in the relationship tuples as indices referencing the nodes (rows) in $\mathbf{N}$.\looseness-1
% For simplicity, we keep the same notation for the text form of the predicate and its embedding (given by a text encoder).


\begin{figure*}
    \centering
    \includegraphics[ width=1.0\linewidth]{figures/oc-clip-diagram.pdf}
    \caption{\textbf{Object-Centric CLIP (OC-CLIP) overview.} OC-CLIP begins with scene parsing, where we utilize a text parser (\textit{e.g.}, Llama3-based) to extract objects and relations from the input caption. The extracted text objects and relations are then fed into a text encoder, which generates distinct text embeddings for both nodes and relations. In parallel, the corresponding image is processed by an image encoder to produce patch-level image embeddings. These image embeddings are then combined with the text entity embeddings and passed through a \emph{binding module}, which outputs visual token slots embeddings. Both modality are aligned in a \emph{new space} using a structured similarity score that matches nodes embeddings to visual slots and models relational constraints between them.\looseness-1} %To align the text entity embeddings with the visual token slots, we use an \emph{object scoring} function that learns to map the text entities to their corresponding visual slots. Furthermore, we introduce a \emph{relation scoring} function that encourages the visual slots to incorporate relationship information, thereby enriching the representation.\looseness-1}
    \label{fig:model}
    \vspace{-1em}
\end{figure*}



\subsection{Binding Module}
\label{ssec:bindingmod}
Our first contribution resides in the binding module. The idea is that when comparing the content of a caption and an image we do not want the features of different objects to interfere with each other but rather keep them separate at a representational level. The role of the binding module is thus to extract a slot-centric representation of an image where the content of the slots are pushed to represent the nodes of the associated scene graph.\looseness-1

To do so, we implement the binding module using a \textit{inverted} cross-attention layer~\citep{WuInvertedAttentionTC}, where the queries are the nodes from our scene graph and the keys and values are the image patches. We normalize the attention coefficients over
the queries' dimension in order to introduce a competition between queries to explain different parts of the visual input.
% To do so, we implement the binding module using a simple \textit{inverted} cross-attention layer between the query nodes $\mathcal{N}$ and the image patches $\mathbf{\bar{x}}$ as keys and values. Similar to Slot Attention \citep{locatello2020objectcentriclearningslotattention}, we normalize the attention coefficients over
% the queries dimension in order to introduce a competitions between those those questions to explain different parts of the visual input. 
% Hence the "inverted" cross attention, term first coined by \citet{WuInvertedAttentionTC} in the context of unsupervised object-centric representation learning. 
We follow common practice and set the attention's softmax temperature to $\sqrt{D}$, with $D$ being the dimensionality of the dot-product operation. 
Applying the softmax along the queries' dimension pushes all the candidate keys to be softly matched to at least one query. However, captions mostly describe specific parts of the image, and rarely capture all the visual information. Since we want only the relevant visual information to be captured by the queries, we add a set of default query tokens, stored in a matrix $\mathbf{Q}_{\text{default}}$, which participate in the competitive attention mechanism -- with the goal of absorbing the visual information not captured in the caption. These default query tokens are dropped in the subsequent computation steps of our model (akin to registers in ViT backbones~\citep{darcet2024visiontransformersneedregisters}). We find the default query tokens crucial to stabilize the training our model.\looseness-1

The binding module computations are formalized as follows:
\begin{align}
\mathbf{Q} &= \mathbf{W}_q \mathbf{N}, \nonumber\\
\mathbf{K},\mathbf{V} &= \mathbf{W}_k\mathbf{\bar{x}},\mathbf{W}_v \mathbf{\bar{x}}, \nonumber\\
\mathbf{Q'} &= [\mathbf{Q}; \mathbf{Q}_{\text{default}}], \nonumber\\
\text{Attn}(\mathbf{Q'}, \mathbf{K}, \mathbf{V}) &= \text{softmax}\left(\frac{\mathbf{Q'} \cdot \mathbf{K}^T}{\sqrt{D}}, \text{dim='Q'}\right) \cdot \mathbf{V},\nonumber\\
\mathbf{S}, \mathbf{S}_{\text{default}}&= \text{Attn}(\mathbf{Q}', \mathbf{K}, \mathbf{V}).
\end{align}
Here, $\mathbf{W}_q$, $\mathbf{W}_k$, and $\mathbf{W}_v$ are the linear projection weight matrices for the queries, keys, and values, respectively, $\mathbf{S}$ are the visual slots, $\mathbf{S}_\text{default}$ are the visual slots from default query tokens, which are discarded for subsequent steps, and [.] denotes the concatenation operation.

Thus, the output of this binding module are the visual slots $\mathbf{S}$. Intuitively, these slots are pushed to represent the visual objects, or entities, that correspond to the nodes of the scene graph. Their object-centric learning is driven by the structured similarity that we detail in the next section.\looseness-1


\subsection{Structured similarity score}\label{sec:our_score}
Our second contribution resides in the introduction of a structured similarity score, whose goal is to promote the constraints imposed by the scene graph on the learnable visual slots. Our proposed structured similarity score is composed of an \emph{object scoring} function and a \emph{relationship scoring} function. The object scoring function assesses the presence of each node in the scene graph (objects present in the caption). We model this function as the sum of the cosine similarity between each textual node representation $\textbf{N}^i$ and its assigned visual slot $\textbf{S}^i$. The relationship scoring function encourages the relational constraints imposed by each edge in the scene graph and is defined as a learnable function $f_\phi$ of the relationship embedding $\mathbf{r}^{i}$, and the visual slot representations $\textbf{S}^{s^i}$ and  $\textbf{S}^{o^i}$ corresponding to the subject and object of the relationship, respectively.
%that models the graphical structure given by the scene graph of the caption. 
%The idea is that overall similarity score between a scene graph and an image should correspond to the sum of the different constraints imposed by the scene graph: 
%\begin{itemize}
%    \item Over \textbf{nodes}: this part should asses the presence of each entity mentioned in the caption. We model it as the sum of the cosine similarity score between each textual node representation $N_i$ and its assigned visual slot $S_i$.
    % \item Over \textbf{edges}: the other part of the similarity score should correspond to the relational constraint imposed by each edge predicate $R^i$ of the scene graph. We model it of some learned function $f_\phi$  of the the visual slots corresponding to the subject $S_{s_i}$ and object  $S_{o_i}$of the relationship.
%\end{itemize}
We derive the overall structured similarity score over the visual slots $\mathbf{S}$ from an image $\mathbf{x}$ and a graph $\mathcal{G} = (\{N^i\}_{i=1..M}, \{(\mathbf{r}^i, s^i, o^i)\}_{i=1..P})$ such that: $S(\mathbf{x}, \mathcal{G}) = \frac{ \alpha \sum_{i=1..M}\text{cosine}(\mathbf{N}^i, \mathbf{S}^i) + \beta\sum_{i=1..P}f_{\phi}(\mathbf{r}^i, \mathbf{S}^{s^i}, \mathbf{S}^{o^i})} {\alpha M + \beta P} $\label{eq:score}
where $\alpha$ and $\beta$ are \new{learned} parameters controlling the strength of each score. $M$ and $P$ are the number of nodes and relationships in the scene graph $\mathcal{G}$, respectively.


We define $f_{\phi}$ as follows:
%\myparagraph{Relationship Scoring Module.}  Given the embedding of the relationship predicate $\mathbf{r}$ and the visual representation of the subject $\mathbf{S}^s$ and the object $\mathbf{S}^o$ of the relationship, we model the relationship score so that it keeps the same scale as the entity score constraint and can take the order of the relationship into account. The final score is modeled as: 
\begin{equation}\label{add_mlp}
  f_\phi(\mathbf{r}, \mathbf{S}^s, \mathbf{S}^o) = \text{cosine}\left(\mathbf{r}, f_s([\mathbf{r}, \mathbf{S}^s]) + f_o([\mathbf{r}, \mathbf{S}^o])\right),
\end{equation}
where [.] denotes the concatenation of two vectors and $f_s$ and $f_o$ are MLPs that reduce the dimensionality of their inputs. 
%that project the visual slot representations into a lower relational concept dimensionality.
Note that we model the relationship scoring function so that it keeps the same scale as the object scoring function and can take the order of the relationship into account. 

\subsection{Training}
The model is trained using the following loss:
\begin{equation}
 \mathcal{L} =  \mathcal{L}_{itc} + \mathcal{L}_{rel}.
\end{equation}
$\mathcal{L}_{itc}$ is the image-text contrastive loss defined to minimize the distance between image and scene graph representations from paired text-image data while maximizing the distance between image and scene graph representations from unpaired text-image data as: 
% for each element (x,G) in the batch, compute structured similarity score, and structured similarity score between graph and all other images in the batch and between image and all other graphs in the batch
\ifarxiv
\begin{equation}
 \mathcal{L}_{itc} = - \sum_{i=1}^B \left ( \log\frac{\exp^{S(\mathbf{x}_i, \mathcal{G}_i)}}{\sum_{j=1}^B \exp^{S(\mathbf{x}_j, \mathcal{G}_i)} }  + \log\frac{\exp^{S(\mathbf{x}_i, \mathcal{G}_i)}}{\sum_{j=1}^B \exp^{S(\mathbf{x}_i, \mathcal{G}_j)} } \right ),
\end{equation}
\else
\begin{equation}
\resizebox{\hsize}{!}{$
 \mathcal{L}_{itc} = - \sum_{i=1}^B \left ( \log\frac{\exp^{S(\mathbf{x}_i, \mathcal{G}_i)}}{\sum_{j=1}^B \exp^{S(\mathbf{x}_j, \mathcal{G}_i)} }  + \log\frac{\exp^{S(\mathbf{x}_i, \mathcal{G}_i)}}{\sum_{j=1}^B \exp^{S(\mathbf{x}_i, \mathcal{G}_j)} } \right ),$}
\end{equation}
\fi
where $B$ is the number of elements in the batch. Note that the $S$ is the structured similarity score defined in Eq.~\ref{eq:score}. \looseness-1
%\begin{equation}
% \mathcal{L}_{itc} = \sum_{(\mathbf{x}, \mathcal{G}) \in \mathcal{B}} -\log\frac{\exp^{S(\mathbf{x}, \mathcal{G})}}{\sum_{\mathbf{x}_j \in \mathcal{B}} \exp^{S(\mathbf{x}_j, \mathcal{G})} }  -\log\frac{\exp^{S(\mathbf{x}, \mathcal{G})}}{\sum_{\mathcal{G}_j \in \mathcal{B}} \exp^{S(\mathbf{x}, \mathcal{G}_j)} }.  
%\end{equation}
$\mathcal{L}_{rel}$ is the loss that pushes the model to learn a non-symmetric relationship scores: 
\begin{equation}
  \mathcal{L}_{rel} = - \sum_{i=1}^B  \log\frac{\exp^{S(\mathbf{x}_i, \mathcal{G}_i)}}{\exp^{S(\mathbf{x}_i, \mathcal{G}_i)} + \exp^{S(\mathbf{x}_i, \bar{\mathcal{G}_i})} +\exp^{S(\mathbf{x}_i, \tilde{\mathcal{G}_i)}}},
\end{equation}
where $\bar{\mathcal{G}}$ and $\tilde{\mathcal{G}}$ are altered scene graphs. In $\bar{\mathcal{G}}$, we swap the order of the subject and the object of a relationship, whereas in $\tilde{\mathcal{G}}$, we randomly chose the relationship's subject and object from the nodes in the scene graph. \new{We ablate the main components of OC-CLIP in Table \ref{tab:ablation-main}  and give a more extensive ablation analysis in Appendix \ref{sec:abla}}\looseness-1

\section{Results}
We evaluate OC-CLIP's inductive biases in 3 different settings:
%In the first two settings the binding and relationship scoring module are trained from scratch \ars{whereas the text and image backbones are initialized with OpenCLIP}. In the third setting, the whole model is trained from scratch. 

\begin{itemize}[leftmargin=*,noitemsep]
\ifarxiv
\else
    \vspace{-1em}
\fi
    \item \textbf{Addressing CLIP's binding problem.} We show the efficiency of OC-CLIP in addressing the binding problem compared to hard-negative based augmentation on a synthetic dataset.(Section~\ref{sec:PUG}). %\ars{In this setting, we initialize both CLIP and OC-CLIP text and vision backbones with OpenCLIP weights, and train OC-CLIP's binding module from scratch.}
    \item \textbf{Compositional understanding.} We showcase OC-CLIP's compositional understanding on real-world object-centric attribute binding and spatial relationship understanding benchmarks (Section~\ref{sec:comp}).\looseness-1 %In this setting, we train OC-CLIP's binding module from scratch on real-word data \ars{while initializing text and vision backbones of both CLIP-like models and OC-CLIP with OpenCLIP weights}. \looseness-1
    \item \textbf{Scaling on noisy data.} We show that OC-CLIP consistently outperforms a CLIP-based model in both zero-shot single object classification and zero-shot compositional understanding multi-object text retrieval, when training both models \emph{fully} from scratch on larger-scale and noisy dataset (Section~\ref{sec:noisy}).\looseness-1
\end{itemize}

% \ars{Since OC-CLIP's binding module is trained from scratch in all settings, OC-CLIP's learned vision-language-aligned space does not rely on the vision-language alignment captured by the CLS token of OpenCLIP's backbone (in fact, we drop the CLS token). Therefore, the representation space learned by OC-CLIP can only be expected to generalize within the vocabulary it has been trained on.\looseness-1}% (it does not rely on the previously aligned CLS tokens since it is dropped in the visual backbone) and can only be expected to generalize within the vocabulary it has been trained on.\looseness-1

\subsection{Addressing CLIP's bag-of-words behavior}%Addressing CLIP's bag-of-words behavior}\label{sec:pug}
\label{sec:PUG}


In this section, we aim to assess the efficiency and effectiveness of leveraging hard-negatives in OC-CLIP and CLIP-like models in addressing the binding problem. To do so, we use a synthetic dataset with a closed-set vocabulary, from which we can enumerate all possible \emph{object-attribute} conjunctions and systematically evaluate the potential of CLIP-like models and OC-CLIP in addressing \emph{simple} swap-attribute retrieval tasks under varying hard-negative sample sizes.

\textbf{Dataset.} We consider a controlled 3D environment based on PUG~\citep{bordes2023pugphotorealisticsemanticallycontrollable} and build a dataset composed of a single textured animal, or pairs of animals, in different backgrounds. We use a combination of 4 textures, 20 animal classes, and 5 different backgrounds -- \textit{e.g.}, see example in Figure~\ref{fig:binding_pug:image}. We follow prior benchmarks~\citep{hsieh2023sugarcrepe} and perform a text-retrieval task between the correct caption and the associated negative caption. We give additional details about the subsets compositions in Appendix~\ref{sec:pug_app}.\looseness-1 

\textbf{Baseline and OC-CLIP training.} We finetune models on data splits from our synthetic data, while considering an increasing proportion of hard-negative samples. We consider a CLIP model initialized with OpenCLIP weights~\cite{ilharco2021openclip}. We also initialize OC-CLIP's text and vision backbones with OpenCLIP weights, but train OC-CLIP's binding module from scratch.

\textbf{Results.} Our results, presented in Figures~\ref{fig:binding_pug:seen} and \ref{fig:binding_pug:unseen}, show that simply adding more hard-negatives to OpenCLIP's training plateaus and is not sample-efficient, as the swap-attribute binding performance always underperforms OC-CLIP trained on less data without any hard-negatives in a simple object-attribute binding task. On seen object pairs, with 70\% of the possible pairs and 70\% of their corresponding swap-attribute hard-negatives CLIP plateaus at 81\% compared to OC-CLIP which solves the task at 97\% on the same training data size and \emph{no} hard-negatives. We hypothesize that the root cause of this issue lies in the representation format used in CLIP's original formulation, which relies on a single vector to capture complex semantic relationships. Our proposed method introduces inductive biases that allow the model to learn more structured representations, avoiding superposition of features~\citep{greff2020bindingproblemartificialneural} and effectively mitigating the bag-of-words behavior. 

\iffalse
\begin{figure*}[ht]
  \centering
  \begin{minipage}[b]{0.33\textwidth}
    \centering
    \includegraphics[width=0.9\textwidth, height=0.15\textheight, keepaspectratio]{figures/ElephantFishAttributes.pdf}
    \caption{Example image}
  \end{minipage}%
  \hfill
  \begin{minipage}[b]{0.33\textwidth}
    \centering
    \includegraphics[width=0.9\textwidth, height=0.4\textheight, keepaspectratio]{figures/pairs_1.pdf}
    \caption{Seen object pairs}
  \end{minipage}%
  \hfill
  \begin{minipage}[b]{0.33\textwidth}
    \centering
    \includegraphics[width=0.9\textwidth, height=0.4\textheight, keepaspectratio]{figures/pairs_3.pdf}
    \caption{Unseen object pairs}
  \end{minipage}
  \caption{\textbf{Attribute Binding.}
  Performance of the finetuned OpenCLIP and OC-CLIP models on a binary classification task between a caption and its corresponding hard-negative, as shown in Figure (a). To assess the models' performance, we compute the accuracy across two dimensions. The first one is the percentage of animal pairs (y-axis) seen during training (animals like elephants and fish could be seen either alone or with other animals but never together). The second dimension (x-axis) is the number of hard-negatives used in the training data. For instance, whether we have the combination ``red elephant'' and ``white fish'' in the training data while we only have ``white elephant'' and ``red fish'' in the test data.}\label{fig:binding_pug}
\end{figure*}
\fi
% \iffalse
\begin{figure*}[ht]
\centering
    \begin{subfigure}[b]{0.33\textwidth}
        \centering    \includegraphics[width=0.9\textwidth, height=0.15\textheight, keepaspectratio]{figures/ElephantFishAttributes.pdf}
        \caption{Synthetic data example}
        \label{fig:binding_pug:image}
    \end{subfigure}%
    \hfill
    \begin{subfigure}[b]{0.33\textwidth}
        \centering    \includegraphics[width=0.9\textwidth, height=0.4\textheight, keepaspectratio]{figures/pairs_1.pdf}
        \caption{Seen object pairs}
        \label{fig:binding_pug:seen}
    \end{subfigure}%
    \hfill
    \begin{subfigure}[b]{0.33\textwidth}
        \centering    \includegraphics[width=0.9\textwidth, height=0.4\textheight, keepaspectratio]{figures/pairs_3.pdf}
        \caption{Unseen object pairs}
        \label{fig:binding_pug:unseen}
    \end{subfigure}
  \caption{\textbf{Efficiency and effectiveness of OC-CLIP: Analysis on synthetic data.}
  Performance of the finetuned OpenCLIP and OC-CLIP models on a binary classification task between a caption and its corresponding hard-negative given a synthetic image, as shown in (a). 
  Performance is shown as a function of the percentage of animal pairs (y-axis) seen during training and the proportion of hard-negatives used in the training data (x-axis). Results shown for (a) seen and (b) unseen object pairs.
  % We compute the accuracy across two dimensions. The first one is the percentage of animal pairs (y-axis) seen during training. The second dimension (x-axis) is the proportion of hard-negatives used in the training data. 
  \looseness-1}
  \label{fig:binding_pug}
\vspace{-1em}
\end{figure*}

\subsection{Compositional Understanding}\label{sec:comp}
\label{ssec:comp_real}
In this section, we verify that the observations made in the controlled environment presented in Section~\ref{sec:PUG} also transfer to real-word datasets, thereby assessing the real-world compositional understanding of OC-CLIP.\looseness-1

\myparagraph{Datasets.}
We train OC-CLIP and finetune OpenCLIP \emph{in-domain} on a set of datasets relevant for real-world compositional understanding. 
%Previous  work~\citep{kamath2023whatsupvisionlanguagemodels} highlighted the lack of spatial prepositions and proposed to finetune and evaluate CLIP on datasets with high spatial relationships coverage (eg. GQA~\citep{hudson2019gqa}).
%Thus, we include these datasets in the training/finetuning experiments and 
The training text descriptions representing positive samples are taken from COCO~\citep{coco}, Visual-Genome (VG)~\citep{krishna2017visual} and GQA~\citep{hudson2019gqa}. The latter annotates images coming from Visual Genome~\citep{krishna2017visual} with objects and both spatial and non-spatial relationships, and thus contains a high representation of spatial prepositions. We evaluate the different models on the most challenging benchmarks representative of compositional understanding, ensuring that we validate both their \emph{attribute binding} and \emph{spatial relationship} understanding capabilities. In particular, we use SugarCrepe~\citep{hsieh2023sugarcrepefixinghackablebenchmarks} and ARO-A ~\citep{yuksekgonul2023visionlanguagemodelsbehavelike} for attribute binding and ARO-Relation (ARO-R)~\citep{yuksekgonul2023visionlanguagemodelsbehavelike}, COCO-spatial and GQA-spatial ~\citep{kamath2023whatsupvisionlanguagemodels} for spatial relationship understanding. Although \citet{hsieh2023sugarcrepefixinghackablebenchmarks} showed that other benchmarks such as VL-Checklist~\citep{zhao2023vlchecklistevaluatingpretrainedvisionlanguage}, COCO-Order and Flickr-Order splits of ARO~\citep{yuksekgonul2023visionlanguagemodelsbehavelike} were easily hackable because the negatives are not semantically correct, we include the results on those benchmarks for reference in Appendix~\ref{app:vl}.\looseness-1

\myparagraph{Training.}
As in section~\ref{sec:PUG}, we initialize the text and vision backbones of OC-CLIP with pre-trained model weights, and train the binding module of OC-CLIP from scratch. In particular, we initialize the text backbone with OpenCLIP %\emph{laion400m} 
weights~\citep{ilharco2021openclip} and consider two different vision backbones, OpenCLIP (ViT-B-16)~\citep{ilharco2021openclip} and DinoV2 (ViT-B-14)~\citep{oquab2024dinov2learningrobustvisual}, to show the flexibility of our binding module and learned structured similarity score. We noticed that taking the patches from earlier layers in OpenCLIP helps the training and ablate it in Appendix~\ref{sec:abla}. We use a batch size of 128 and a learning rate of $2\cdot 10^{-4}$ to train OC-CLIP for 100 epochs. We use a batch size of 256 -- following previous finetuning approaches~\citep{kamath2023whatsupvisionlanguagemodels, yuksekgonul2023when} -- and a learning rate of $4\cdot 10^{-6}$ for 20 epochs to finetune the OpenCLIP baseline. We run all the models for 3 seeds and report the mean performance along with their standard deviation. Note that since OC-CLIP's binding module is trained from scratch, OC-CLIP's learned vision-language-aligned space does not rely on the vision-language alignment captured by the CLS token of OpenCLIP's backbone (in fact, we drop the CLS token). Therefore, \emph{the new representation space learned by OC-CLIP can only be expected to generalize within the vocabulary it has been trained on.}\looseness-1%  \looseness-1

\myparagraph{Baselines.} We report the performance of a representative set of strong baselines which we separate in two groups: the first group of baselines are models trained contrastively and finetuned in-domain (on COCO/VG) and the second group are hard-negative-based and recaptioning-based methods, further divided into small scale and large scale. For the first group, we include OpenCLIP -- referred to as OpenCLIP-FT --, BLIP~\citep{li2023blip}, and XVLM~\citep{zeng2022multigrainedvisionlanguagepretraining}. BLIP is augmented with an image-text matching loss and XVLM uses bounding boxes to assist the object-centric binding. Note that these two baselines are also equipped with a language modeling objective which may help identify unplausible captions. %These augmentation are orthogonal to our work as we study the effect of a binding mechanism using a contrastive objective only but we still consider them as reference. 
For the second group, we select %a representative set of hard-negative-based methods to compare. These
methods that augment the dataset with rule-based text hard-negatives (NegCLIP~\citep{yuksekgonul2023when}), language-model-based hard-negatives (CE-CLIP~\cite{zhang2020contrastive} and CLIP-SVLC~\citep{doveh2023teachingstructuredvisionlanguageconcepts}), and image-\&-language-model-based hard-negatives (CLIP-CC~\citep{zhang2024countercurateenhancingphysicalsemantic}). We also include dense recaptioning baselines such as DAC~\citep{doveh2023dense} for reference.\looseness-1

\myparagraph{Attribute Binding Results.}
We evaluate the attribute binding capabilities of OC-CLIP and baselines on SugarCrepe~\citep{hsieh2023sugarcrepefixinghackablebenchmarks} and ARO-A~\citep{yuksekgonul2023when} benchmarks. 
We report the results in Table~\ref{tab:sugarcrepe}. When comparing OpenCLIP$_{\text{FT}}$ to OC-CLIP (ours -- both models), we observe notable performance boosts on ARO-A and SugarCrepe's swap-attribute, and swap-object. In particular, {OC-CLIP\tiny{B-14}} shows a performance boost of +24\% on ARO-A, while in SugarCrepe, our model achieves improvements of +16.5\% on the hard swap attribute split, +20.4\% on the swap object split, and a smaller +4.1\% on the replacement relationship split. Moreover, both OC-CLIP models perform similarly to OpenCLIP$_{\text{FT}}$ on the remaining SugarCrepe splits. This is to be expected since the remaining splits do not require precise binding to distinguish between positive and negative captions and may therefore be solved with a bag-of-words-like representation. 
We additionally compare OC-CLIP to finetuned versions of CLIP that rely on in domain hard-negatives (NegCLIP, CE-CLIP, CC-CLIP) and with dense recaptioning (DAC-LLM and DAC-SAM). In particular DAC finetunes OpenCLIP with $\sim3$M VLM-generated dense captions (along with their corresponding hard negatives) that significantly increase the vocabulary coverage compared to methods that only finetune in domain (\textit{e.g.}, on COCO). Interestingly, OC-CLIP still outperforms them on both swap-attribute and swap-object, showing improvements of +13.6\% and +8.4\% over the second best performing method, respectively. Those results confirm the behavior that we observed in Section~\ref{sec:PUG} and the inefficiency of hard-negative methods in solving the binding problem of CLIP-like models, even at the scale of DAC finetuning.\looseness-1
\begin{table*}[ht]
%\begin{adjustbox}{width=\textwidth}
\begin{small}
\begin{sc}

\ifarxiv
  \scalebox{0.92}{%
    \begin{tabular}{ccccccccc}%{}{@{}>{\raggedright\arraybackslash\small}p{2cm} *{9}{>{\small}c}@{}}
    \cmidrule(lr){1-9}
    % \toprule
    \multirow{2}{*}{Model} & \multicolumn{2}{c}{Swap} & \multicolumn{2}{c}{Add} & \multicolumn{3}{c}{Replace} & \multicolumn{1}{c}{ARO} \\
    \cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-8} \cmidrule(lr){9-9}
    & Object & Attribute & Object & Attribute & Object & Attribute & Relation & Attribution \\
    %\cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-8} \cmidrule(l){9-9}
    \cmidrule(lr){1-9}
    \multicolumn{9}{l}{
    \textit{Zero-shot }} \\
    OpenCLIP   & 68.2 & 66.2 & 82.7& 80.3 & 93.8 & 82.8 & 67.3 & 63.2 \\
    %\addlinespace
    \cmidrule(lr){1-9}
    \multicolumn{9}{l}{
    \textit{In-domain ft baselines} }\\
    BLIP & 66.2 & 76.2 & - & - & \textbf{96.5} & 81.9 & 68.35 & \textbf{88.0} \\
    XVLM  & 64.9 & 73.9 & - & - & 95.2 & 87.7 & 77.4 & 73.4 \\
    OpenCLIP$_{\text{FT}}$ & 63.1 \tiny{$\pm 0.6$} & 72.4\tiny{$\pm 1.1$} & \textbf{93.4} \tiny{$\pm 0.2$} & 83.1 \tiny{$\pm 0.5$} & 95.4 & 87.0 \tiny{$\pm 0.6$} & 75.5 \tiny{$\pm 0.6$} & 60.0 \\
    %\addlinespace
    \cmidrule(lr){1-9}
    \multicolumn{9}{l}{
    \textit{Hard-Negative - small scale}} \\
    NegCLIP  & 75.2 & 75.4 & 88.8 & 82.8 & 92.7 & 85.9 & 76.5 & 71 \\
    CE-CLIP  & 72.8 & 77 & 92.4 & 93.4 & 93.1 & 88.8 & 79 & 76.4 \\
    CC-CLIP & 68.6 & 73.6 & 86.7  & 90.3  & 95.9  & 87.9  & 76.2 & - \\
    CLIP-SVLC& - & - & - & - & - & - & - & 73.0 \\
    %\addlinespace
    \cmidrule(lr){1-9}
    \multicolumn{9}{l}{
    \textit{Hard-Negative/Dense Captioning - large scale}} \\
    %DCI$_\text{neg}$ & 66.5&66.8 & 68.7& 67.3&91.0 &81.9 &66.2 \\
    DAC-LLM & 75.1& 74.1& 89.7&\textbf{97.7} & 94.4& \textbf{89.3}&\textbf{84.4}& 73.9 \\
    DAC-SAM &71.8 & 75.3& 87.5& 95.5& 91.2&85.9 &83.9& 70.5 \\
    \cmidrule(lr){1-9}
    \multicolumn{9}{l}{\textit{Ours} }
    \\
    OC-CLIP \tiny{B-16}  & 76.6 \tiny{$\pm 0.6$} & 87.5 \tiny{$\pm 0.5$} & 91.1\tiny{$\pm 0.4$}  & 83.8 \tiny{$\pm 1.0$} & 94.6 \tiny{$\pm 0.4$} & 87.9 \tiny{$\pm 0.1$} & 76.0 \tiny{$\pm 0.4$}  & 83.2\tiny{$\pm 0.3$}\\
    OC-CLIP \tiny{B-14}  & \textbf{83.5} \tiny{$\pm 0.2$} & \textbf{88.9} \tiny{$\pm 0.6$} & 92.8\tiny{$\pm 0.1$} & 84.8 \tiny{$\pm 0.1$} & 95.9 \tiny{$\pm 0.4$} & 89.2\tiny{$\pm 0.1$}  & 79.6 \tiny{$\pm 0.3$} & 84.0\tiny{$\pm 0.$}\\
    %\addlinespace
    \cmidrule(lr){1-9}
    \end{tabular}
    }
\else
    \begin{tabularx}{}{@{}>{\raggedright\arraybackslash\small}p{2cm} *{9}{>{\small}c}@{}}
    \cmidrule(lr){1-9}
    % \toprule
    \multirow{2}{*}{Model} & \multicolumn{2}{c}{Swap} & \multicolumn{2}{c}{Add} & \multicolumn{3}{c}{Replace} & \multicolumn{1}{c}{ARO} \\
    \cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-8} \cmidrule(lr){9-9}
    & Object & Attribute & Object & Attribute & Object & Attribute & Relation & Attribution \\
    %\cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-8} \cmidrule(l){9-9}
    \cmidrule(lr){1-9}
    \multicolumn{9}{l}{
    \textit{Zero-shot }} \\
    OpenCLIP   & 68.2 & 66.2 & 82.7& 80.3 & 93.8 & 82.8 & 67.3 & 63.2 \\
    %\addlinespace
    \cmidrule(lr){1-9}
    \multicolumn{9}{l}{
    \textit{In-domain ft baselines} }\\
    BLIP & 66.2 & 76.2 & - & - & \textbf{96.5} & 81.9 & 68.35 & \textbf{88.0} \\
    XVLM  & 64.9 & 73.9 & - & - & 95.2 & 87.7 & 77.4 & 73.4 \\
    OpenCLIP$_{\text{FT}}$ & 63.1 \tiny{$\pm 0.6$} & 72.4\tiny{$\pm 1.1$} & \textbf{93.4} \tiny{$\pm 0.2$} & 83.1 \tiny{$\pm 0.5$} & 95.4 & 87.0 \tiny{$\pm 0.6$} & 75.5 \tiny{$\pm 0.6$} & 60.0 \\
    %\addlinespace
    \cmidrule(lr){1-9}
    \multicolumn{9}{l}{
    \textit{Hard-Negative - small scale}} \\
    NegCLIP  & 75.2 & 75.4 & 88.8 & 82.8 & 92.7 & 85.9 & 76.5 & 71 \\
    CE-CLIP  & 72.8 & 77 & 92.4 & 93.4 & 93.1 & 88.8 & 79 & 76.4 \\
    CC-CLIP & 68.6 & 73.6 & 86.7  & 90.3  & 95.9  & 87.9  & 76.2 & - \\
    CLIP-SVLC& - & - & - & - & - & - & - & 73.0 \\
    %\addlinespace
    \cmidrule(lr){1-9}
    \multicolumn{9}{l}{
    \textit{Hard-Negative/Dense Captioning - large scale}} \\
    %DCI$_\text{neg}$ & 66.5&66.8 & 68.7& 67.3&91.0 &81.9 &66.2 \\
    DAC-LLM & 75.1& 74.1& 89.7&\textbf{97.7} & 94.4& \textbf{89.3}&\textbf{84.4}& 73.9 \\
    DAC-SAM &71.8 & 75.3& 87.5& 95.5& 91.2&85.9 &83.9& 70.5 \\
    \cmidrule(lr){1-9}
    \multicolumn{9}{l}{\textit{Ours} }
    \\
    OC-CLIP \tiny{B-16}  & 76.6 \tiny{$\pm 0.6$} & 87.5 \tiny{$\pm 0.5$} & 91.1\tiny{$\pm 0.4$}  & 83.8 \tiny{$\pm 1.0$} & 94.6 \tiny{$\pm 0.4$} & 87.9 \tiny{$\pm 0.1$} & 76.0 \tiny{$\pm 0.4$}  & 83.2\tiny{$\pm 0.3$}\\
    OC-CLIP \tiny{B-14}  & \textbf{83.5} \tiny{$\pm 0.2$} & \textbf{88.9} \tiny{$\pm 0.6$} & 92.8\tiny{$\pm 0.1$} & 84.8 \tiny{$\pm 0.1$} & 95.9 \tiny{$\pm 0.4$} & 89.2\tiny{$\pm 0.1$}  & 79.6 \tiny{$\pm 0.3$} & 84.0\tiny{$\pm 0.$}\\
    %\addlinespace
    \cmidrule(lr){1-9}
    \end{tabularx}
\fi

\end{sc}
\end{small}
%\end{adjustbox}
\caption{\textbf{Attribute binding: Performance on SugarCrepe and ARO-A.} Both OpenCLIP-FT and OC-CLIP are initialized with the same OpenCLIP checkpoints. OC-CLIP is trained with two ViT base backbones with different resolutions: OpenCLIP's backbone (B-16) and Dinov2 (B-14).OC-CLIP's bidning module is always trained from scratch.\looseness-1 }
\label{tab:sugarcrepe}
\ifarxiv
\else
    \vspace{-1em}
\fi
\end{table*}\looseness-1

\myparagraph{Relationship Understanding Results.}
We evaluate the spatial relationship understanding capabilities of OC-CLIP and baselines on COCO-spatial, GQA-spatial, and ARO-Relation (ARO-R). Note that ARO-Relation contains both spatial and non-spatial relations but about half of the test examples consists of left/right relationships understanding. We report the results in Table~\ref{tab:rel} and show consistent improvements of both OC-CLIP models over the baseline models and across the 3 datasets. In particular, the best OC-CLIP model outperforms OpenCLIP-FT by +44.1\% on COCO-spatial, +43.6\% on GQA-spatial, and +34.8\% on ARO-R. When compared to contrastive VLMs finetuned with in-domain data (XVLM, BLIP), OC-CLIP models exhibit superior performance, with improvements between +10\% and +27\% over the strongest contrastive finetuned VLM. Finally, when compared to baselines leveraging hard-negatives (NegCLIP), OC-CLIP remains the highest performer. Additional results on the ARO benchmark are reported in Table~\ref{tab:vl_aro} of the Appendix.\looseness-1

\begin{table}
\begin{small}
\begin{sc}
% \vspace{-1em}
  \centering
  \scalebox{0.9}{%
  \begin{tabular}{c c c c}
    \toprule
    Model & COCO-spatial & GQA-spatial& ARO-R \\
    \cmidrule(lr){1-1}\cmidrule(lr){2-4}
    XVLM & 73.6 & 67  & 73.4\\
    BLIP & 56.4 & 52.6 & 59\\
    NegCLIP & 46.4 & 46.7& 80.2 \\
    OpenCLIP$_{\text{FT}}$&45.6 \tiny{$\pm 0.2$}&49.1\tiny{$\pm 1.1$} & 50.1\tiny{$\pm0.4$}\\
    %CLIP (OpenCLIP) + CC & 52.7 & 51.7 & 54.8 \\
    \cmidrule(lr){1-1}\cmidrule(lr){2-4}
    OC-CLIP \tiny(B-16) & 86.3 & 90.0 & 84.3 \\
    OC-CLIP \tiny(B-14) & \textbf{89.7} & \textbf{92.7} &\textbf{84.9} \\
    %OC-CLIP \tiny(B-14 + CC) & \textbf{93.5} & 93.9 & \textbf{84.9} \\
    \bottomrule
  \end{tabular}%
  }
  \caption{\textbf{Spatial relationship understanding: Performance on COCO-spatial, GQA-spatial from the Whats'up Benchmark and ARO-R.} We finetune both OpenCLIP (OpenCLIP$_{\text{FT}}$ here) and OC-CLIP in-domain on COCO, Visual Genome, and GQA data. Both models are initialized with the same OpenCLIP checkpoints.\looseness-1 }
  \label{tab:rel}
  \ifarxiv
  \else
      \vspace{-2em}
  \fi
  \end{sc}
  \end{small}
  \looseness-1
\end{table}
\subsection{Training OC-CLIP from scratch}\label{sec:noisy}
In this section, we aim to assess the potential of OC-CLIP when trained \emph{fully} from scratch from scene-graphs obtained from large scale non-human-curated captioning dataset.\looseness-1

\textbf{Datasets.} We train both ViT-B-16 OpenCLIP model and OC-CLIP \emph{fully} from scratch on increasingly large dataset sizes using CC3M~\citep{sharma-etal-2018-conceptual}, CC12M~\citep{changpinyo2021cc12m} and the combination of both datasets. We evaluate all models on ImageNet~\citep{Deng2009ImageNetAL} zero-shot classification in this section, and report results on the ELEVATER suite~\citep{li2022elevaterbenchmarktoolkitevaluating} in Appendix (Table~\ref{tab:classification-downstream}). We also evaluate zero-shot compositional understanding of the models on the challenging swap-object and swap-attribute splits of SugarCrepe, and on Winoground~\citep{thrush2022winoground}.\looseness-1

\textbf{Baseline and OC-CLIP training.} Both CLIP and OC-CLIP architectures are trained \emph{fully} from scratch for 5, 15, or 25 epochs, using a batch size of 4096, a learning rate of $1\cdot 10^{-3}$, 2k steps of learning rate warm-up, and a cosine decay. As recommended by~\citet{mu2021slipselfsupervisionmeetslanguageimage}, we use AdamW optimizer with $0.5$ of weight decay and $\beta_2$ set to 0.98. We use a ViT-B-16 backbone for both models. Since OC-CLIP's text bakcbone only needs to encode single objects and relationships, we use a smaller text bakcbone with a context length of $20$ and only $6$ layers instead of 12 . Note that we do not tune the hyper-parameters in this experiment. We further discuss those design choices in Appendix~\ref{sec:scale}. 

\sidecaptionvpos{figure}{t}
\begin{SCfigure*}[.55]
\begin{wide} % Essential for SCfigure*
    \centering
    \subfloat[Sample efficiency]{
        \centering
        \includegraphics[height=4.5cm]{figures/ze_sample_effi.png}
        \label{fig:noisy_eff}}
    \hfill
    \subfloat[Zero-shot accuracy]{
        \centering
        \includegraphics[height=4.5cm]{figures/noisy_comp.png}
        \label{fig:noisy_cls}}
    \hspace{-.2cm}
    \hfill
    \caption{\textbf{Scaling the training on noisy data.} CLIP and OC-CLIP  are trained \emph{from scratch} on varying sizes of data (3M, 12M and 15M) 
    for a varying number of epochs. OC-CLIP shows (b) better zero-shot compositional understanding performance on SugarCrepe's swap-attribute and swap-object, and on Winoground (I = Image score and T = Text score), as well as (a) better sample efficiency shown on zero-shot ImageNet classification.\looseness-1}
    \label{fig:noisy}
\end{wide} % Close the wide environment
\ifarxiv
\else
    \vspace{-3em}
\fi
\end{SCfigure*}

\textbf{Results.} We start by verifying the sample efficiency of OC-CLIP using ImageNet~\citep{Deng2009ImageNetAL} zero-shot classification performance in Figure~\ref{fig:noisy_eff}. We show that OC-CLIP shows better sample-efficiency than the baseline trained on the same data, while using a smaller text backbone. We then evaluate OC-CLIP on zero-shot classification and compositional understanding in Figure~\ref{fig:noisy_cls}. Interestingly, OC-CLIP shows performance gains in general zero-shot classification ($+12.8\%$ on ImageNet, when trained from scratch on CC3M+CC12M) while also showcasing substantial improvements in zero-shot compositional understanding. For example, OC-CLIP exhibits a notable $+12.7\%$ and $+6.6\%$ in SugarCrepe's swap-attribute and swap-object splits, respectively. This experiment shows that the structured training of OC-CLIP is also effective when scaling to noisy image-caption dataset and, therefore, does not solely rely on high-quality human captions. %\ars{The discussion of results does not factor in the effect of dataset size on the final performance.} will put more discussion in the appendix
We additionally report extensive zero-shot downstream classification performance on the ELEVATER~\citep{li2022elevaterbenchmarktoolkitevaluating} suite and discuss the computation trade-off of our approach in Appendix~\ref{sec:scale}.
We leave further scaling for future work.
\subsection{Ablations}
In Table \ref{tab:ablation-main} we ablate the key design choices of our model. Specifically, we investigate two key components of the model: the use of competitive (inverted) cross-attention and the local graph contrastive loss. On the one hand, results show that removing the competitive cross-attention mechanism greatly affects fine-grained attribute binding (decreasing from $89.0$ to $85.9$). On the other hand, removing the local graph contrastive loss significantly impacts downstream relational understanding, with accuracy decreasing from $80.5$ to $72.8$. Adding attention layers helps relational understanding (boosting performance from  $77.6$ to $80.5$), while adding more default tokens does not necessarily help with attribute binding. These findings highlight the importance of the main design choices behind OC-CLIP. More extensive ablations are presented in Appendix~\ref{sec:abla}.\looseness-1
\begin{table}[htbp]
\ifarxiv
\else
    \vspace{-1em}
\fi
\begin{small}
\begin{sc}
  \centering
  \scalebox{0.8}{%
  \begin{tabular}{cccc|cc}
    \toprule
     Loc Loss & Comp. X-Att & Attn Lay & Default & Rel & Att \\
    \midrule
     \checkmark & \checkmark &  \checkmark &1 & 80.5 & 89.0 \\
    \midrule
     \checkmark & \checkmark  & \checkmark& 4 & 79.2 & 87.6 \\
      -& \checkmark & \checkmark &1 &  72.8 & 87.7 \\
    % \hline
      \checkmark & - & \checkmark &- & 78.3 & 85.9 \\
    % \hline
      \checkmark & \checkmark  &-& 1 &77.6 & 87.8 \\
    % \hline
    \bottomrule
  \end{tabular}%
  }
  \caption{\textbf{Ablation of OC-CLIP's main components.} Fine-grained accuracy on attribute binding and relational splits  of SugarCrepe.\looseness-1}
  \label{tab:ablation-main}
  \end{sc}
  \end{small}
  \vspace{-2em}
\end{table} 

\section{Conclusion and limitations}

\myparagraph{Conclusion.} In this paper, we proposed Object-Centric CLIP (OC-CLIP), a method to enhance the compositional scene understanding of CLIP-like models by leveraging advances from object-centric representation learning. Our approach adapts the slot-centric representation paradigm to CLIP and dynamically aligns each representational slot with the objects mentioned in the text description. This is achieved by the introduction of a binding module and a structured similarity score that allows to train OC-CLIP in a contrastive way. We evaluated our approach against common hard-negative augmentation strategies and demonstrated that OC-CLIP significantly enhances the binding of object-centric attributes and spatial relationships across a representative set of challenging real-world compositional image-text matching benchmarks. Notably, we reported an increase of +16.5\% accuracy in the challenging swap-attribute split of SugarCrepe compared to OpenCLIP finetuned with in-domain data and drastically improved performance on COCO-spatial and GQA-spatial from the Whatsup benchmark, moving from random chance to more than $89\%$. Finally we show the scaling potential of OC-CLIP to be trained from scratch on a noisy dataset~\citep{changpinyo2021cc12m, sharma-etal-2018-conceptual} datastet. Notably we report performance gain in zero-shot classfication ($+12.8\%$ in ImageNet \ref{tab:classification-downstream}) while maintaining a  significant  gap in zero-shot SugarCrepe swap attribute ($+12.7\%$) and swap obj ($+6.6\%$) splits.\looseness-1

\myparagraph{Limitations and Future Work.} 
Our proposed Object-Centric CLIP (OC-CLIP) model has several limitations and avenues for future work. Notably, our approach relies on a parser to extract object-centric attributes and spatial relationships from text descriptions. While we have chosen an LLM-based parser, which is discussed in Appendix~\ref{app:parsing}, studying the different biases of LLM-based parser families could be interesting. A related promising direction for future research is also to explore the possibility of parsing scene graphs directly from Visual Language Models (VLMs), using both visual and textual inputs. Additionally, we plan to investigate the synergy between long-captioning and our scene graph-based training approach, aiming to study the complementary strengths of these two data-centric and model-centric paradigms.

\ifarxiv
\else
    \myparagraph{Impact Statement.}
    This paper presents work whose goal is to advance the field
    of machine learning, specifically the multi-modal architecture of CLIP. CLIP itself has broad applications and impact thus our proposed technique shall be considered when such models are used.
\fi

\iffalse
Our current implementation builds upon existing pre-trained backbones and only trains the binding and scoring modules from scratch. This allows us to leverage the knowledge captured by these pre-trained backbones while still adapting to the specific task of compositional scene understanding.  Future work could explore ways to improve the scalability of our approach, such as developing more efficient training methods or exploring alternative architectures with similar object-centric  inductive biases. \new{We also expect the capacity needed for the text encoder to be reduced since it does not need to encode whole scene configuration but rather single objects and relationships as shown in our CC12M experiemnts and further explained in Appendix \ref{sec:scale}}.\looseness-1
\fi



\bibliographystyle{plainnat}
\bibliography{iclr2025_conference}
\clearpage
\onecolumn


\appendix
\input{iclr_2025_appendix}



\end{document}

