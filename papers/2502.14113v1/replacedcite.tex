\section{Related Work}
\myparagraph{Contrastive Pretraining of VLMs.}
Vision-language models (VLMs) have made substantial strides in both the vision and multi-modal domains____. Modern VLMs are pretrained on vast, diverse and oftentimes noisy multi-modal datasets____, and have shown substantial improvements when applied to various zero-shot tasks. CLIP____ presented a contrastive learning approach used for pretraining, which involves training the model to differentiate between similar and dissimilar image-text pairs. This approach encourages the model to learn a shared representation space for images and text, where semantically similar pairs are close together and dissimilar pairs are far apart. Following CLIP's lead, image-text contrastive learning has become a prevalent strategy for VLM pretraining____. Contrastive vision-language pretraining spans numerous downstream applications, including zero-shot image classification____,  text-to-image generation____, as well as assessing text-image alignment____. In this work, we are particularly interested in the ability of CLIP-based models to evaluate compositional text-image alignment.\looseness-1

\iffalse
Contrastive-Language Image Pretraining (CLIP)____ models are trained by defining an image and its corresponding caption as positive pairs while the same image and all the other captions present in a mini-batch are defined as negative pairs. By discriminating the positive pair from the negative one, CLIP models can learn an alignment between visual clues in images and text. Despite having impressive results on zero-shot object detection tasks, such approach has been shown to be limited in learning more abstract concepts such as relations between objects or attributes____.
\fi

\myparagraph{Compositional Understanding Benchmarks.} 
Several benchmarks have been developed to assess the compositional understanding of VLMs. In this work, we focus on benchmarks structured as cross-modal retrieval tasks where the model needs to distinguish between correct and incorrect text descriptions given an image, and evaluations are based on accuracy metrics. The majority of these benchmarks____ rely on the rule-based construction of negative captions and the generation of their associated image counter-factuals____. Yet, many of these benchmarks may be solved by leveraging the language prior exclusively____, hence disregarding the information from the visual input. To address this, benchmarks such as SugarCrepe____ leverage large language models to generate plausible and linguistically correct hard negatives, and show that previously introduced text-based hard negative strategies are not always effective____ -- \textit{e.g.}, when considering attribute and object swaps between textual descriptions.
Other benchmarks focus on assessing the VLMs' spatial understanding____, and propose to finetune CLIP-based models on data containing a high proportion of spatial relationships since these relationships tend to be under-represented in commonly used pretraining datasets. Interestingly,____ show that even when finetuning with in-domain data containing an over-representation of spatial relationships, state-of-the-art models still exhibit a close to random chance performance. In this work, we test the hypothesis that spatial relationship failures are due to the lack of composition in the similarity score computation used to train CLIP-like models.\looseness-1


\iffalse
\myparagraph{PUG environment?}

\todo{Difficulty of evaluate the sample-efficiency of the effect hard negatives at scale so we first validate it in a controlled environment where we have access to the entire vocab + existing hard negatives }


Because of the difficulty of getting negative ground truth images, researchers often use synthetic data such like CLEVR____ to generate a balanced set of negative images and captions. In this work, we decided to leverage the PUG framework____ to make a synthetic image dataset with realistic 
\fi


\myparagraph{Object-centric Binding Inductive Biases.}
CLIP has been shown____ to be pushed to learn disentangled, bag-of-words-style representations from the contrastive loss and the easily distinguishable negatives typically used for pretraining. Although the learned representations might be effective for objects presented in isolation, they struggle with scenes containing multiple objects____. For example, consider a simple scene with a green apple and a yellow banana. In this case, the model must maintain and correctly link the attributes (``green'', ``yellow'') to the objects (``apple'', ``banana''), without mixing the concepts  -- \textit{e.g.}, ``yellow apple'' or `green banana''. This exemplifies the importance of devising robust mechanisms within the CLIP architecture and/or training to accurately handle multiple objects, while preventing feature interferences. 
% Ideally, the model should represent ``green'' and ``apple'' distinctly when only the apple is present, and "yellow" and ``banana'' when only the banana is visible. However, challenges arise when both fruits appear together. The network must then maintain and correctly link each attribute (``green'', ``yellow'', ``apple'', ``banana'') without mixing them up, such as mistakenly creating a ``yellow apple'' or a ``green banana''.This example underscores the need for robust mechanisms within the CLIP model to accurately handle multiple objects and prevent feature interferences. 
In this work, we focus on equipping CLIP with object-centric binding inductive biases and take inspiration from the architectures proposed in the unsupervised object-centric visual representation learning literature____. Many recent image-only approaches follow a simple inductive bias introduced by slot attention____, where  an image -- encoded as a set of input tokens -- is soft partitioned into
K slots. In particular, attention maps are computed via an \textbf{inverted cross attention} mechanism____, where the softmax is applied along the query dimension in order to induce a competition between the slots to explain different groups of input tokens. In this work, we extend these inductive biases to define text-conditioned visual slots from the input image.\looseness-1