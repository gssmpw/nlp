@article{martin2021nerf,
  title={Nerf in the wild: Neural radiance fields for unconstrained photo collections},
  author={Martin-Brualla, Ricardo and Radwan, Noha and Sajjadi, Mehdi SM and Barron, Jonathan T and Dosovitskiy, Alexey and Duckworth, Daniel},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={7210--7219},
  year={2021}
}

@inproceedings{turki2022mega,
  title={Mega-nerf: Scalable construction of large-scale nerfs for virtual fly-throughs},
  author={Turki, Haithem and Ramanan, Deva and Satyanarayanan, Mahadev},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={12922--12931},
  year={2022}
}

@inproceedings{xu2023grid,
  title={Grid-guided neural radiance fields for large urban scenes},
  author={Xu, Linning and Xiangli, Yuanbo and Peng, Sida and Pan, Xingang and Zhao, Nanxuan and Theobalt, Christian and Dai, Bo and Lin, Dahua},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={8296--8306},
  year={2023}
}

@inproceedings{ost2021neural,
  title={Neural scene graphs for dynamic scenes},
  author={Ost, Julian and Mannan, Fahim and Thuerey, Nils and Knodt, Julian and Heide, Felix},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={2856--2865},
  year={2021}
}

@article{song2022towards,
  title={Towards efficient neural scene graphs by learning consistency fields},
  author={Song, Yeji and Kong, Chaerin and Lee, Seoyoung and Kwak, Nojun and Lee, Joonseok},
  journal={arXiv preprint arXiv:2210.04127},
  year={2022}
}

@inproceedings{turki2023suds,
  title={Suds: Scalable urban dynamic scenes},
  author={Turki, Haithem and Zhang, Jason Y and Ferroni, Francesco and Ramanan, Deva},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={12375--12385},
  year={2023}
}

@article{xie2023s,
  title={S-nerf: Neural radiance fields for street views},
  author={Xie, Ziyang and Zhang, Junge and Li, Wenye and Zhang, Feihu and Zhang, Li},
  journal={arXiv preprint arXiv:2303.00749},
  year={2023}
}

@inproceedings{wu2023mars,
  title={Mars: An instance-aware, modular and realistic simulator for autonomous driving},
  author={Wu, Zirui and Liu, Tianyu and Luo, Liyi and Zhong, Zhide and Chen, Jianteng and Xiao, Hongmin and Hou, Chao and Lou, Haozhe and Chen, Yuantao and Yang, Runyi and others},
  booktitle={CAAI International Conference on Artificial Intelligence},
  pages={3--15},
  year={2023},
  organization={Springer}
}



@article{yang2023emernerf,
  title={Emernerf: Emergent spatial-temporal scene decomposition via self-supervision},
  author={Yang, Jiawei and Ivanovic, Boris and Litany, Or and Weng, Xinshuo and Kim, Seung Wook and Li, Boyi and Che, Tong and Xu, Danfei and Fidler, Sanja and Pavone, Marco and others},
  journal={arXiv preprint arXiv:2311.02077},
  year={2023}
}

@article{kerbl20233d,
  title={3D Gaussian Splatting for Real-Time Radiance Field Rendering.},
  author={Kerbl, Bernhard and Kopanas, Georgios and Leimk{\"u}hler, Thomas and Drettakis, George},
  journal={ACM Trans. Graph.},
  volume={42},
  number={4},
  pages={139--1},
  year={2023}
}

@inproceedings{yang2024deformable,
  title={Deformable 3d gaussians for high-fidelity monocular dynamic scene reconstruction},
  author={Yang, Ziyi and Gao, Xinyu and Zhou, Wen and Jiao, Shaohui and Zhang, Yuqing and Jin, Xiaogang},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={20331--20341},
  year={2024}
}

@inproceedings{wu20244d,
  title={4d gaussian splatting for real-time dynamic scene rendering},
  author={Wu, Guanjun and Yi, Taoran and Fang, Jiemin and Xie, Lingxi and Zhang, Xiaopeng and Wei, Wei and Liu, Wenyu and Tian, Qi and Wang, Xinggang},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={20310--20320},
  year={2024}
}

@article{luiten2023dynamic,
  title={Dynamic 3d gaussians: Tracking by persistent dynamic view synthesis},
  author={Luiten, Jonathon and Kopanas, Georgios and Leibe, Bastian and Ramanan, Deva},
  journal={arXiv preprint arXiv:2308.09713},
  year={2023}
}

@inproceedings{zhou2024drivinggaussian,
  title={Drivinggaussian: Composite gaussian splatting for surrounding dynamic autonomous driving scenes},
  author={Zhou, Xiaoyu and Lin, Zhiwei and Shan, Xiaojun and Wang, Yongtao and Sun, Deqing and Yang, Ming-Hsuan},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={21634--21643},
  year={2024}
}

@article{yan2024street,
  title={Street gaussians for modeling dynamic urban scenes},
  author={Yan, Yunzhi and Lin, Haotong and Zhou, Chenxu and Wang, Weijie and Sun, Haiyang and Zhan, Kun and Lang, Xianpeng and Zhou, Xiaowei and Peng, Sida},
  journal={arXiv preprint arXiv:2401.01339},
  year={2024}
}

@article{huang2024textit,
  title={S3 Gaussian: Self-Supervised Street Gaussians for Autonomous Driving},
  author={Huang, Nan and Wei, Xiaobao and Zheng, Wenzhao and An, Pengju and Lu, Ming and Zhan, Wei and Tomizuka, Masayoshi and Keutzer, Kurt and Zhang, Shanghang},
  journal={arXiv preprint arXiv:2405.20323},
  year={2024}
}

@article{mildenhall2021nerf,
  title={Nerf: Representing scenes as neural radiance fields for view synthesis},
  author={Mildenhall, Ben and Srinivasan, Pratul P and Tancik, Matthew and Barron, Jonathan T and Ramamoorthi, Ravi and Ng, Ren},
  journal={Communications of the ACM},
  volume={65},
  number={1},
  pages={99--106},
  year={2021},
  publisher={ACM New York, NY, USA}
}

@inproceedings{wang2021birds,
  title={Birds of a feather: Capturing avian shape models from images},
  author={Wang, Yufu and Kolotouros, Nikos and Daniilidis, Kostas and Badger, Marc},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={14739--14749},
  year={2021}
}

@article{liu2024endogaussian,
  title={Endogaussian: Gaussian splatting for deformable surgical scene reconstruction},
  author={Liu, Yifan and Li, Chenxin and Yang, Chen and Yuan, Yixuan},
  journal={arXiv preprint arXiv:2401.12561},
  year={2024}
}

@inproceedings{jiang2024vr,
  title={VR-GS: a physical dynamics-aware interactive gaussian splatting system in virtual reality},
  author={Jiang, Ying and Yu, Chang and Xie, Tianyi and Li, Xuan and Feng, Yutao and Wang, Huamin and Li, Minchen and Lau, Henry and Gao, Feng and Yang, Yin and others},
  booktitle={ACM SIGGRAPH 2024 Conference Papers},
  pages={1--1},
  year={2024}
}

@inproceedings{abdal2024gaussian,
  title={Gaussian shell maps for efficient 3d human generation},
  author={Abdal, Rameen and Yifan, Wang and Shi, Zifan and Xu, Yinghao and Po, Ryan and Kuang, Zhengfei and Chen, Qifeng and Yeung, Dit-Yan and Wetzstein, Gordon},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={9441--9451},
  year={2024}
}

@inproceedings{mescheder2019occupancy,
  title={Occupancy networks: Learning 3d reconstruction in function space},
  author={Mescheder, Lars and Oechsle, Michael and Niemeyer, Michael and Nowozin, Sebastian and Geiger, Andreas},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={4460--4470},
  year={2019}
}

@inproceedings{Dosovitskiy17,
  title = {{CARLA}: {An} Open Urban Driving Simulator},
  author = {Alexey Dosovitskiy and German Ros and Felipe Codevilla and Antonio Lopez and Vladlen Koltun},
  booktitle = {Proceedings of the 1st Annual Conference on Robot Learning},
  pages = {1--16},
  year = {2017}
}

@inproceedings{cao2022monoscene,
  title={Monoscene: Monocular 3d semantic scene completion},
  author={Cao, Anh-Quan and De Charette, Raoul},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={3991--4001},
  year={2022}
}

@inproceedings{huang2023tri,
  title={Tri-perspective view for vision-based 3d semantic occupancy prediction},
  author={Huang, Yuanhui and Zheng, Wenzhao and Zhang, Yunpeng and Zhou, Jie and Lu, Jiwen},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={9223--9232},
  year={2023}
}

@inproceedings{wei2023surroundocc,
  title={Surroundocc: Multi-camera 3d occupancy prediction for autonomous driving},
  author={Wei, Yi and Zhao, Linqing and Zheng, Wenzhao and Zhu, Zheng and Zhou, Jie and Lu, Jiwen},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={21729--21740},
  year={2023}
}

@article{tian2024occ3d,
  title={Occ3d: A large-scale 3d occupancy prediction benchmark for autonomous driving},
  author={Tian, Xiaoyu and Jiang, Tao and Yun, Longfei and Mao, Yucheng and Yang, Huitong and Wang, Yue and Wang, Yilun and Zhao, Hang},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{zheng2023occworld,
  title={Occworld: Learning a 3d occupancy world model for autonomous driving},
  author={Zheng, Wenzhao and Chen, Weiliang and Huang, Yuanhui and Zhang, Borui and Duan, Yueqi and Lu, Jiwen},
  journal={arXiv preprint arXiv:2311.16038},
  year={2023}
}

@inproceedings{li2024occ,
  title={Occ-vo: Dense mapping via 3d occupancy-based visual odometry for autonomous driving},
  author={Li, Heng and Duan, Yifan and Zhang, Xinran and Liu, Haiyi and Ji, Jianmin and Zhang, Yanyong},
  booktitle={2024 IEEE International Conference on Robotics and Automation (ICRA)},
  pages={17961--17967},
  year={2024},
  organization={IEEE}
}

@inproceedings{boulch2023also,
  title={Also: Automotive lidar self-supervision by occupancy estimation},
  author={Boulch, Alexandre and Sautier, Corentin and Michele, Bj{\"o}rn and Puy, Gilles and Marlet, Renaud},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={13455--13465},
  year={2023}
}

@inproceedings{cheng2021mask2former,
  title={Masked-attention Mask Transformer for Universal Image Segmentation},
  author={Bowen Cheng and Ishan Misra and Alexander G. Schwing and Alexander Kirillov and Rohit Girdhar},
  journal={CVPR},
  year={2022}
}

@misc{wu2019detectron2,
  author =       {Yuxin Wu and Alexander Kirillov and Francisco Massa and
                  Wan-Yen Lo and Ross Girshick},
  title =        {Detectron2},
  howpublished = {\url{https://github.com/facebookresearch/detectron2}},
  year =         {2019}
}

@InProceedings{Sun_2020_CVPR, author = {Sun, Pei and Kretzschmar, Henrik and Dotiwalla, Xerxes and Chouard, Aurelien and Patnaik, Vijaysai and Tsui, Paul and Guo, James and Zhou, Yin and Chai, Yuning and Caine, Benjamin and Vasudevan, Vijay and Han, Wei and Ngiam, Jiquan and Zhao, Hang and Timofeev, Aleksei and Ettinger, Scott and Krivokon, Maxim and Gao, Amy and Joshi, Aditya and Zhang, Yu and Shlens, Jonathon and Chen, Zhifeng and Anguelov, Dragomir}, title = {Scalability in Perception for Autonomous Driving: Waymo Open Dataset}, booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, month = {June}, year = {2020} }

@inproceedings{schoenberger2016sfm,
    author={Sch\"{o}nberger, Johannes Lutz and Frahm, Jan-Michael},
    title={Structure-from-Motion Revisited},
    booktitle={Conference on Computer Vision and Pattern Recognition (CVPR)},
    year={2016},
}

@article{wang2004image,
  title={Image quality assessment: from error visibility to structural similarity},
  author={Zhou Wang and Bovik, Alan C. and Sheikh, Hamid R. and Simoncelli, Eero P.},
  journal={IEEE Transactions on Image Processing},
  volume={13},
  number={4},
  pages={600--612},
  year={2004},
  publisher={IEEE}
}

@inproceedings{zhang2018unreasonable,
  title={The Unreasonable Effectiveness of Deep Features as a Perceptual Metric},
  author={Zhang, Richard and Isola, Phillip and Efros, Alexei A. and Shechtman, Eli and Wang, Oliver},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2018},
  pages={586--595}
}

@inproceedings{liu2024difflow3d,
  title={DifFlow3D: Toward Robust Uncertainty-Aware Scene Flow Estimation with Iterative Diffusion-Based Refinement},
  author={Liu, Jiuming and Wang, Guangming and Ye, Weicai and Jiang, Chaokang and Han, Jinru and Liu, Zhe and Zhang, Guofeng and Du, Dalong and Wang, Hesheng},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={15109--15119},
  year={2024}
}

@inproceedings{liu2025dvlo,
  title={Dvlo: Deep visual-lidar odometry with local-to-global feature fusion and bi-directional structure alignment},
  author={Liu, Jiuming and Zhuo, Dong and Feng, Zhiheng and Zhu, Siting and Peng, Chensheng and Liu, Zhe and Wang, Hesheng},
  booktitle={European Conference on Computer Vision},
  pages={475--493},
  year={2025},
  organization={Springer}
}

@article{duan2024cellmap,
  title={CELLmap: Enhancing LiDAR SLAM through Elastic and Lightweight Spherical Map Representation},
  author={Duan, Yifan and Zhang, Xinran and Li, Yao and You, Guoliang and Chu, Xiaomeng and Ji, Jianmin and Zhang, Yanyong},
  journal={arXiv preprint arXiv:2409.19597},
  year={2024}
}

@inproceedings{10.1145/3636534.3649386,
author = {Zhang, Xinran and Zhu, Hanqi and Duan, Yifan and Zhang, Wuyang and Shangguan, Longfei and Zhang, Yu and Ji, Jianmin and Zhang, Yanyong},
title = {Map++: Towards User-Participatory Visual SLAM Systems with Efficient Map Expansion and Sharing},
year = {2024},
isbn = {9798400704895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636534.3649386},
doi = {10.1145/3636534.3649386},
abstract = {Constructing precise 3D maps is crucial for the development of future map-based systems such as self-driving and navigation. However, generating these maps in complex environments, such as multi-level parking garages or shopping malls, remains a formidable challenge. In this paper, we introduce a participatory sensing approach that delegates map-building tasks to map users, thereby enabling cost-effective and continuous data collection. The proposed method harnesses the collective efforts of users, facilitating the expansion and ongoing update of the maps as the environment evolves.We realized this approach by developing Map++, an efficient system that functions as a plug-and-play extension, supporting participatory map-building based on existing SLAM algorithms. Map++ addresses a plethora of scalability issues in this participatory map-building system by proposing a set of lightweight, application-layer protocols. We evaluated Map++ in four representative settings: an indoor garage, an outdoor plaza, a public SLAM benchmark, and a simulated environment. The results demonstrate that Map++ can reduce traffic volume by approximately 46\% with negligible degradation in mapping accuracy, i.e., less than 0.03m compared to the baseline system. It can support approximately 2\texttimes{} as many concurrent users as the baseline under the same network bandwidth. Additionally, for users who travel on already-mapped trajectories, they can directly utilize the existing maps for localization and save 47\% of the CPU usage.},
booktitle = {Proceedings of the 30th Annual International Conference on Mobile Computing and Networking},
pages = {633â€“647},
numpages = {15},
keywords = {user-participatory, SLAM, map sharing, map expansion},
location = {<conf-loc>, <city>Washington D.C.</city>, <state>DC</state>, <country>USA</country>, </conf-loc>},
series = {ACM MobiCom '24}
}

@article{wu2024mm,
  title={MM-Gaussian: 3D Gaussian-based Multi-modal Fusion for Localization and Reconstruction in Unbounded Scenes},
  author={Wu, Chenyang and Duan, Yifan and Zhang, Xinran and Sheng, Yu and Ji, Jianmin and Zhang, Yanyong},
  journal={arXiv preprint arXiv:2404.04026},
  year={2024}
}