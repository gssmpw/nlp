\section{RELATED WORKS}
\noindent\textbf{Occupancy Network and applications. }Obtaining accurate semantic 3D Occupancy Grids is crucial for downstream tasks, making research on the Occupancy Prediction Network highly valuable\cite{duan2024cellmap}. MonoScene~\cite{cao2022monoscene} completes dense 3D grids from a single RGB image, it ensures spatial consistency via 2D-3D projection and 3D context prior. By extending BEV representation to 3D, TPVFormer~\cite{huang2023tri} proposes a new representation named TPV and combines it with a transformer to predict 3D semantic Occupancy Grids. SurroundOcc~\cite{wei2023surroundocc} utilizes an innovative method to generate dense occupancy labels and employed a 2D-3D attention mechanism to predict Occupancy Grids from multi-camera images. To facilitate future research, Occ3D~\cite{tian2024occ3d} introduces two benchmark datasets (Occ3D-Waymo and Occ3D-nuScenes) and a new model (CTF-Occ network). As the accuracy of occupancy prediction networks improves, many related applications are also emerging. OccWorld~\cite{zheng2023occworld} introduces a world model framework based on 3D occupancy space. It demonstrates an effective capability in modeling scene evolution on the nuScenes benchmark. OCC-VO~\cite{li2024occ} transforms 2D camera images into 3D semantic occupancy to address the depth information challenge in Visual Odometry(VO). 

\noindent\textbf{3D scenes reconstruction for Autonomous Driving. } 
Simulating real-world street scene is essential for developing and testing autonomous driving systems. A prime example is CARLA~\cite{dosovitskiy17}, a well-known open-source driving simulator that's widely used for creating complex 3D environments. But the scenes created by these simulators often lack the realism needed to fully immerse users in real-world environments. 
% Consequently, NeRF and 3DGS have gained attention for reconstructing detailed 3D scenes from 2D images. 
To adapt NeRF for unbounded and dynamic field like autonomous driving, ~\cite{martin2021NeRF, turki2022mega} improves NeRF to model multi-scale urban scenarios. ~\cite{xu2023grid} combines compact multi-resolution ground features with NeRF to achieve high-fidelity rendering. Research in ~\cite{ost2021neural, song2022towards} begin to explore handling dynamic scenes with multiple objects, leading to Suds~\cite{turki2023suds} processing scenes into static backgrounds and dynamic objects. While S-NeRF~\cite{xie2023s} introduces LiDAR as a form of supervision, MARS and EmerNeRF~\cite{wu2023mars, yang2023emernerf} further optimize NeRF for outdoor scene reconstruction to enhance its performance. However, NeRF is limited by its high training costs and slow rendering speed, so attention of industry is gradually shifting toward 3DGS due to its lower training costs and faster rendering speed~\cite{kerbl20233d}. \cite{yang2024deformable} uses a transformer to model Gaussian motion from monocular images. ~\cite{luiten2023dynamic} parametrizes the entire scene using a set of variable dynamic Gaussians. DrivingGaussian~\cite{zhou2024drivinggaussian} firstly introduces LiDAR point clouds as a prior and incrementally reconstructs the entire scene. Street Gaussians~\cite{yan2024street} reconstructs dynamic scenes with separate rendering and LiDAR. S3 Gaussian~\cite{huang2024textit} improves this using self-supervised vehicle bounding boxes. 
% Street Gaussians~\cite{yan2024street} achieves precise reconstruction of dynamic scenes by separately rendering the background model and dynamic objects, they also use LiDAR. Building on~\cite{yan2024street}, S3 Gaussian~\cite{huang2024textit} effectively reconstructs outdoor scenes by leveraging self-supervised vehicle bounding boxes.Â  

% , each of which contains 59 parameters that adequately represent the state of each Gaussian. Of these 59, 3 parameters are used to represent the center position of the Gaussian, 7 parameters are used to represent the covariance matrix, 1 parameter is used to represent the opacity, and 48 parameters are used to represent the spherical harmonic coefficients.~\textcolor{orange}{Xinran: If you want to introduce the 59 parameters, give the formula of 3DGS, which including these parameters.}
\begin{figure*}[h]
\centering
\includegraphics[width=0.95\textwidth]{jiagou_compressed.pdf}
\vspace{-3pt}
\caption{\textbf{Overview of OG-Gaussian. }OG-Gaussian utilizes a trained 3D Occupancy Prediction network to obtain Occupancy Grid data for the scene. It separates static and dynamic objects into different initial point cloud models using semantic information. After the separated reconstruction, we globally render both static and dynamic objects, producing 3D scenes, depth maps and so on.}
\label{fig:overview}
% Fig.~\ref{fig:overview}
\vspace{-14pt}
\end{figure*}