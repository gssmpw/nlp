% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[preprint]{acl}

% Standard package includes


\usepackage{algorithm}
\usepackage{algpseudocode} 
\usepackage{amsmath}       
\usepackage{graphicx}      
\usepackage{caption}       
\usepackage{multirow}
\usepackage[most]{tcolorbox}
\usepackage{underscore}
\usepackage{times}
\usepackage{latexsym}
\usepackage{amsthm}
\usepackage{booktabs}
\usepackage{atbegshi}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{microtype}
\usepackage{inconsolata}
\usepackage{pifont}

\usepackage{graphicx}
\usepackage{hyperref}
% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{QA-Expand: Multi-Question Answer Generation for Enhanced Query Expansion in Information Retrieval}

\author{
 \textbf{Wonduk Seo\textsuperscript{1}}\hspace{0.5em}
 \textbf{Seunghyun Lee\textsuperscript{1}}\thanks{Corresponding author.}\hspace{0.5em}
\\
\\
 \textsuperscript{1}Enhans.ai\hspace{0.5em}
\\
\texttt{\{seowonduk,seunghyun\}@enhans.ai}
}



\begin{document}
\maketitle
\begin{abstract}
Query expansion is widely used in Information Retrieval (IR) to improve search outcomes by enriching queries with additional contextual information. Although recent Large Language Model (LLM) based methods generate pseudo-relevant content and expanded terms via multiple prompts, they often yield repetitive, narrow expansions that lack the diverse context needed to retrieve all relevant information. In this paper, we introduce \textbf{\emph{QA-Expand, a novel and effective framework for query expansion.}} It first generates multiple relevant questions from the initial query and subsequently produces corresponding pseudo-answers as surrogate documents. A feedback model further filters and rewrites these answers to ensure only the most informative augmentations are incorporated. Extensive experiments on benchmarks such as \emph{BEIR} and \emph{TREC} demonstrate that \emph{QA-Expand} enhances retrieval performance by up to 13\% over state-of-the-art methods, offering a robust solution for modern retrieval challenges.

\end{abstract}

\section{Introduction}
Query expansion is widely used in Information Retrieval (IR) for effectively improving search outcomes by enriching the initial query with additional contextual information~\cite{carpineto2012survey,azad2019novel,jagerman2023query}. Traditional methods as Pseudo-Relevance Feedback (PRF) expand queries by selecting terms from top-ranked documents~\cite{robertson1990term,jones2006generating,lavrenko2017relevance}. While these conventional approaches have been successful to some extent, their reliance on static term selection limits the scope of expansion~\cite{roy2016using,imani2019deep}. 


\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{real-QA.png}
    \caption{\textbf{The overview of the novel \emph{QA-Expand} framework.} Given an initial query, the framework generates diverse relevant questions, produces corresponding pseudo-answers, and selectively rewrites and filters relevant answers to enhance query expansion.}
    \label{fig:framework}
\end{figure}


In recent years, Large Language Models (LLMs) have enabled dynamic query rewriting techniques that overcome traditional limitations by harnessing their generative ability~\cite{zhao2023survey,ye2023enhancing,liu2024query,lei2024corpus,seo2024gencrf,chen2024analyze}. For instance, \emph{Q2D}~\cite{wang2023query2doc} expands queries with pseudo-documents generated via few-shot prompting, while \emph{Q2C}~\cite{jagerman2023query} uses Chain-of-Thought (CoT) prompting~\cite{wei2022chain} for reformulation. Moreover, \emph{GenQREnsemble}~\cite{dhole2024genqrensemble} concatenates multiple keyword sets produced through zero-shot paraphrasing with the original query, and \emph{GenQRFusion}~\cite{dhole2024generative} retrieves documents for each keyword set and fuses the rankings.

% In recent years, the emergence of Large Language Models (LLMs) has paved the way for more dynamic and sophisticated query rewriting techniques that harness the generative power of these models to overcome traditional limitations~\cite{zhao2023survey,ye2023enhancing,liu2024query,lei2024corpus,seo2024gencrf,chen2024analyze}. Recent methods such as \emph{Q2D}~\cite{wang2023query2doc} generates pseudo-documents through few-shot prompting and expands the query with the generated content; \emph{Q2C}~\cite{jagerman2023query} employs Chain-of-Thought (CoT) prompting~\cite{wei2022chain} to guide query reformulation; \emph{GenQREnsemble}~\cite{dhole2024genqrensemble} concatenates multiple sets of keywords generated via paraphrases of a zero-shot instruction with the initial query; and \emph{GenQRFusion}~\cite{dhole2024generative} retrieves documents for each initial query paired with each set of keywords, and then fuses the resulting rankings to enhance retrieval performance.

% \emph{GenQREnsemble} and \emph{GenQRFusion}~\cite{dhole2024genqrensemble,dhole2024generative} both generating 10 keyword sets through paraphrases of a zero-shot instruction, each applying a distinct retrieval strategy with the former concatenating these keywords with the original query and the latter retrieving documents for each set and fusing the resulting rankings.

Despite these advances, several significant challenges remain: \ding{172} simplistic prompt variations yield repetitive, narrowly focused expansions that miss the full range of contextual nuances; \ding{173} many approaches lack a dynamic evaluation mechanism, leading to redundant or suboptimal term inclusion; and \ding{174} these methods do not reformulate the query into distinct questions with corresponding answers, limiting their ability to capture diverse, insightful facets of the underlying information need.
% Despite these promising advances, several significant challenges remain: \ding{172} The reliance on simplistic prompt variations often results in repetitive and narrowly focused expansions that fail to capture the full spectrum of contextual nuances; \ding{173} Many of these approaches lack a dynamic evaluation mechanism to assess the quality and relevance of the generated content, resulting in the incorporation of redundant or suboptimal terms; \ding{174} Although these methods engage in query expansion, they do not perform a deeper analysis by reformulating the initial query into distinct questions followed by corresponding answer generation, thereby limiting their ability to capture the diverse and richly insightful facets of the underlying information need.

To overcome these limitations, we propose \emph{QA-Expand}, a novel framework that leverages Large Language Models (LLMs) to generate diverse question-answer pairs from an initial query. Specifically, \emph{QA-Expand} first generates multiple relevant questions derived from the initial query and subsequently produces corresponding pseudo-answers that serve as surrogate documents to enrich the query representation. A feedback model is furthermore 
integrated to selectively rewrite and filter these generated answers, ensuring that the final query augmentation robustly captures a multi-faceted view of the underlying information need. 

% Extensive experiments on benchmark datasets, including four from the BEIR Benchmark~\cite{thakur2021beir} and two from the TREC Deep Learning 2019 and 2020 tracks~\cite{craswell2020overview}, demonstrate that \emph{QA-Expand} significantly outperforms existing query expansion techniques. Our contributions are threefold: (1) a novel paradigm that reformulates the initial query into multiple targeted questions and generates corresponding pseudo-answers to capture diverse aspects of the information need; (2) a dynamic feedback model that selectively rewrites and filters only the most informative pseudo-answers during retrieval to ensure effective query augmentation; and (3) comprehensive empirical validation confirming the robustness and superiority of our approach in modern information retrieval scenarios.
% 

Extensive experiments on four datasets from \emph{BEIR Benchmark}~\cite{thakur2021beir} and two datasets from the \emph{TREC Deep Learning Passage} $2019$ and $2020$~\cite{craswell2020overview} demonstrate that \emph{QA-Expand} significantly outperforms existing query expansion techniques. Our contributions include: (1) \emph{a novel paradigm} that reformulates the query into multiple targeted questions and generates corresponding pseudo-answers to capture diverse aspects of the information need; (2) \emph{a dynamic feedback model} that selectively rewrites and filters only the most informative pseudo-answers for effective query augmentation; and (3) \emph{comprehensive empirical validation} confirming the robustness and superiority of our approach.\footnote{Background is detailed in Appendix A.}

% Extensive experiments on benchmark datasets, including four frequently used datasets from the BEIR Benchmark~\cite{thakur2021beir} and two datasets from the TREC Deep Learning $2019$ and $2020$ tracks~\cite{craswell2020overview}, demonstrate that \emph{QA-Expand} significantly outperforms existing query expansion techniques. Our contributions include: (1) \emph{a novel paradigm} that reformulating the initial query into multiple targeted questions and generates corresponding pseudo-answers to capture diverse aspects of the information; (2) \emph{a dynamic feedback model} that selectively rewrites and filters only the most informative pseudo-answers during retrieval, ensuring effective query augmentation; and (3) \emph{comprehensive empirical validation} that confirms the robustness and superiority of our approach in modern information retrieval scenarios.

\section{Methodology}
In this section, we detail our proposed \emph{QA-Expand} framework. An overview of the \emph{QA-Expand} framework is provided in Figure~\ref{fig:framework}.

% It mainly consists of three steps: (1) generation of multiple questions from the initial query, (2) generation of corresponding pseudo-answers for each question, and (3) a feedback-driven process for rewriting and selecting the most relevant question-answer pairs. 
% An overview of the \emph{QA-Expand} framework is provided in Figure~\ref{fig:framework}.

\subsection{Multiple Question Generation}
Given an initial query \(Q\), a single inference call is made to an LLM using a fixed prompt \(P\) to generate a set of diverse questions relevant to initial query. This process is formalized as:
\begin{equation}\label{eq:question-generation}
\mathcal{Q} = \{q_1, q_2, \dots, q_N\} = G_\mathcal{Q}(Q, P),
\end{equation}
where \(G_\mathcal{Q}\) denotes the question generation module and \(N\) is the number of generated questions. Each \(q_i\) is designed to capture a distinct aspect of the information need expressed in \(Q\).


\subsection{Pseudo-Answer Generation}
For each generated question \(q_i \in \mathcal{Q}\), the answer generation module subsequently produces a corresponding pseudo-answer. This module generates an answer for each question in \(\mathcal{Q}\). This results in a complete set of pseudo-answers:
\begin{equation}\label{eq:answer-generation}
\mathcal{A} = \{a_1, a_2, \dots, a_N\} = G_\mathcal{A}(\mathcal{Q}),
\end{equation}
where \(G_\mathcal{A}\) denotes the answer generation process implemented via an LLM. This design ensures that all generated questions are paired with an answer, providing a comprehensive candidate set for subsequent evaluation.

\subsection{Feedback-driven Rewriting and Selection}
After generating the pseudo-answers, the feedback module \(G_\mathcal{S}\) processes the complete set of question-answer pairs \(\{(q_i, a_i)\}_{i=1}^N\) in the context of the initial query \(Q\) and directly produces the refined pseudo-answer set:
\begin{equation}\label{eq:refined-answers}
\mathcal{S} = G_\mathcal{S}(\{(q_i, a_i)\}_{i=1}^N, Q).
\end{equation}
Here, \(G_\mathcal{S}\) denotes the selective rewriting and filtering operation implemented via an LLM. In this process, any refined pseudo-answer deemed irrelevant or too vague is omitted from \(\mathcal{S}\). Thus, the final set of refined pseudo-answers can be represented as:
\begin{equation}\label{eq:final-set}
\mathcal{S} = \{a'_1, a'_2, \dots, a'_j\}, \quad \text{with } 0 \leq j \leq N.
\end{equation}
Finally, the refined pseudo-answers \(\mathcal{S}\) are integrated with the initial query \(Q\) using various aggregation strategies (e.g., sparse concatenation, dense weighted fusion, and Reciprocal Rank Fusion), as detailed in Section~\ref{implementation-details}.\footnote{Prompts are detailed in Appendix B.}


\section{Experiments}
\subsection{Setup}

Our experimental setup includes a description of the datasets, model specifications, and the baseline methods used for comparison in our framework.

% \subsubsection{Experimental Datasets}
\paragraph{Datasets.} We evaluate QA-Expand on two benchmark collections: (1) \emph{BEIR  Benchmark}~\cite{thakur2021beir} and (2) \emph{TREC Passage Datasets}~\cite{craswell2020overview}. Specifically, for \emph{BEIR Benchmark}, we select four frequently used datasets: \emph{webis-touche2020}, \emph{scifact}, \emph{trec-covid-beir}, and \emph{dbpedia-entity}. For \emph{TREC Datasets}, we employ the \emph{Deep Learning Passage Tracks} from $2019$ and $2020$, which consist of large-scale passage collections to ensure that our approach performs well in challenging retrieval scenarios.~\footnote{Statistical description of the datasets is detailed in Appendix C.}

% \subsubsection{Models}
\paragraph{LLM and Retrieval Models.} For generating the question-answer pairs in \emph{QA-Expand}, we utilize \emph{Qwen2.5-7B-Instruct Model}\footnote{\url{https://huggingface.co/Qwen/Qwen2.5-7B-Instruct}}~\cite{qwen2.5}, which is a high-performance language model and, for the retrieval task, we employ \emph{multilingual-e5-base}\footnote{\url{https://huggingface.co/intfloat/multilingual-e5-base}}~\cite{wang2024multilingual} to encode both queries and documents into dense representations. Additionally, we incorporate \emph{BM25}~\cite{robertson2009probabilistic} as a sparse retrieval baseline, specifically using BM25s\footnote{\url{https://github.com/xhluca/bm25s}}~\cite{lu2024bm25s}, a pure-Python implementation that leverages Scipy~\cite{virtanen2020scipy} sparse matrices for fast, efficient scoring.

% \paragraph{Baselines and Our Approach.} We compare \emph{QA-Expand} with standard retrieval baselines—\emph{BM25} for sparse and \emph{multilingual-e5-base} for dense retrieval—and with SOTA query expansion methods such as \emph{Q2D} (few-shot pseudo-document generation), \emph{Q2C} (chain-of-thought guided reformulation), and GenQR-based methods that generate 10 prompt-based keyword sets (either concatenated with the query or used for ranking fusion), all using their original settings. In contrast, our \emph{QA-Expand} enriches each query by generating 3 distinct questions with corresponding refined pseudo-answers—a configuration chosen to balance diversity and relevance by capturing multiple facets of the query without redundancy.

% \subsubsection{Baseline Methods}
\paragraph{Baselines and Our Approach.} We compare \emph{QA-Expand} with standard retrieval baselines and query expansion methods. Retrieval baselines include \emph{BM25} for sparse retrieval and \emph{multilingual-e5-base} for dense retrieval using cosine similarity. We also evaluate query expansion methods such as \emph{Q2D}~\cite{wang2023query2doc}, which generates pseudo-documents via few-shot prompting, and \emph{Q2C}~\cite{jagerman2023query}, which uses chain-of-thought guided reformulation. In addition, we compare with \emph{GenQR-based} methods~\cite{dhole2024genqrensemble,dhole2024generative} that generate 10 prompt-based keyword sets, with one variant concatenating these keywords with the original query and the other retrieving documents for each set and fusing the rankings. All baselines use their original settings. In contrast, our \emph{QA-Expand} enriches each query by generating $3$ distinct while diverse questions and corresponding refined pseudo-answers—a configuration chosen to balance diversity and relevance by capturing multiple facets of the query without excessive redundancy.
% \paragraph{Baselines and Our Approach.} We compare \emph{QA-Expand} with standard retrieval baselines and state-of-the-art query expansion methods. Retrieval baselines include \emph{BM25} for sparse retrieval and \emph{multilingual-e5-base} for dense retrieval via cosine similarity. We also evaluate query expansion methods: \emph{Q2D}~\cite{wang2023query2doc} (pseudo-document generation via few-shot prompting), \emph{Q2C}~\cite{jagerman2023query} (chain-of-thought guided reformulation), \emph{GenQREnsemble}~\cite{dhole2024genqrensemble} (concatenating 10 prompt-based keyword sets), and \emph{GenQRFusion}~\cite{dhole2024generative} (retrieving documents for each keyword set and fusing the rankings). All baselines use their original settings. In contrast, \emph{QA-Expand} enriches each query by generating 3 distinct questions and corresponding refined pseudo-answers\footnote{Prompts are detailed in Appendix A.}.

% \paragraph{Baselines and Our Approach.} We compare \emph{QA-Expand} against retrieval baselines and  query expansion methods. Specifically, retrieval baselines include \emph{BM25}, which performs sparse retrieval using the initial query, and \emph{multilingual-e5-base}, which retrieves documents based on cosine similarity of dense embeddings. We also compare with state-of-the-art (SOTA) query expansion methods: \ding{172} \emph{Q2D}~\cite{wang2023query2doc} (pseudo-document generation via few-shot prompting), \ding{173} \emph{Q2C}~\cite{jagerman2023query} (chain-of-thought (COT) guided reformulation), \ding{174} \emph{GenQREnsemble}~\cite{dhole2024genqrensemble} (a concatenation of $10$ prompt-based keyword sets), and \ding{175} \emph{GenQRFusion}~\cite{dhole2024generative} (retrieves documents for each initial query paired with each set of keywords, and then fuses the resulting rankings to enhance retrieval performance). All baselines follow their original configurations. Additionally, in our \emph{QA-Expand} approach, each input query is enriched by generating $3$ distinct relevant questions and their corresponding refined pseudo-answers\footnote{Prompts used in our method are detailed in Appendix A.}.


\begin{table*}[ht]
\centering
\renewcommand{\arraystretch}{1.2} % Increase vertical spacing between rows
\resizebox{\textwidth}{!}{%
\begin{tabular}{l|ccccc|ccc|ccc}
\toprule
\multirow{3}{*}{\textbf{Methods}} & \multicolumn{5}{c|}{\textbf{BEIR Benchmark (nDCG@10)}} & \multicolumn{3}{c|}{\textbf{TREC DL'19}} & \multicolumn{3}{c}{\textbf{TREC DL'20}} \\
\cmidrule(lr){2-6} \cmidrule(lr){7-9} \cmidrule(lr){10-12}
 & Webis & SciFact & TREC-COVID & DBpedia & Avg. Score & nDCG@10 & R@1000 & Avg. Score & nDCG@10 & R@1000 & Avg. Score \\
\midrule
\multicolumn{12}{c}{\textbf{Sparse Results}} \\
\hline
\emph{BM25} & 0.2719 & 0.6694 & 0.5868 & 0.2831 & 0.4528 & 0.4239 & 0.3993 & 0.4116 & 0.4758 & 0.4240 & 0.4500 \\
\emph{Q2C} (\citeyear{jagerman2023query}) & 0.3546 & \underline{0.6876} & 0.6954 & 0.3252 & 0.5157 & 0.5439 & 0.4814 & 0.5127 & 0.5357 & 0.4941 & 0.5149 \\
\emph{Q2D} (\citeyear{wang2023query2doc}) & \underline{0.3679} & 0.6794 & \underline{0.6957} & \textbf{0.3378} & \underline{0.5202} & \underline{0.5732} & \underline{0.4890} & \underline{0.5311} & \underline{0.5486} & \underline{0.4958} & \underline{0.5222} \\
\emph{GenQREnsemble} (\citeyear{dhole2024genqrensemble}) & 0.2887 & 0.5560 & 0.5104 & 0.2302 & 0.3963 & 0.4109 & 0.4110 & 0.4110 & 0.4261 & 0.4163 & 0.4207 \\
\emph{\textbf{QA-Expand*}} (Sparse, Ours) & \textbf{0.3919*} & \textbf{0.6965*} & \textbf{0.7050*} & \underline{0.3273*} & \textbf{0.5302*} & \textbf{0.5811*} & \textbf{0.4932*} & \textbf{0.5372*} & \textbf{0.5803*} & \textbf{0.5000*} & \textbf{0.5402*} \\
\midrule
\multicolumn{12}{c}{\textbf{Dense Results}} \\
\hline
\emph{E5-Base} & 0.1786 & 0.6924 & 0.7098 & 0.4002 & 0.4953 & 0.7020 & 0.5185 & 0.6103 & 0.7029 & 0.5648 & 0.6339 \\
\emph{Q2C} (\citeyear{jagerman2023query}) & 0.1841 & 0.7028 & 0.7238 & \underline{0.4250} & 0.5112 & 0.5517 & 0.4891 & 0.5204 & \underline{0.7084} & 0.5715 & \underline{0.6400} \\
\emph{Q2D} (\citeyear{wang2023query2doc}) & \textbf{0.1931} & \underline{0.7108} & \underline{0.7284} & 0.4229 & \underline{0.5133} & \underline{0.7472} & \textbf{0.5565} & \textbf{0.6519} & 0.6971 & \underline{0.5799} & 0.6385 \\
\emph{\textbf{QA-Expand*}} (Dense, Ours) & \underline{0.1911*} & \textbf{0.7147*} & \textbf{0.7342*} & \textbf{0.4278*} & \textbf{0.5387*} & \textbf{0.7476*} & \underline{0.5527*} & \underline{0.6502*} & \textbf{0.7184*} & \textbf{0.5831*} & \textbf{0.6508*} \\
\midrule
\multicolumn{12}{c}{\textbf{RRF Fusion (BM25) Results}} \\
\hline
\emph{GenQRFusion} (\citeyear{dhole2024generative}) & \textbf{0.3815} & \underline{0.6518} & \underline{0.6594} & \underline{0.2726} & \underline{0.4913} & \underline{0.4418} & \underline{0.4205} & \underline{0.4312} & \underline{0.4375} & \underline{0.4654} & \underline{0.4515} \\
\emph{\textbf{QA-Expand*}} (RRF, Ours) & 0.3533 & \textbf{0.6777*} & \textbf{0.6698*} & \textbf{0.3009*} & \textbf{0.5004*} & \textbf{0.5048*} & \textbf{0.4734*} & \textbf{0.4891*} & \textbf{0.5211*} & \textbf{0.4795*} & \textbf{0.5003*} \\
\bottomrule
\end{tabular}%
}
\caption{\emph{Combined retrieval performance on BEIR Benchmark (nDCG@10) and TREC DL'19/TREC DL'20 (nDCG@10 / R@1000). For BEIR, the Avg. column is the average across Webis, SciFact, TREC-COVID, and DBpedia}. For TREC DL, the Avg. Score is computed as the average of nDCG@10 and R@1000. Bold indicates the best score and underline indicates the second-best score. * denotes significant improvements (paired t-test with Holm-Bonferroni correction, p $<$ 0.05) over the average baseline value for the metric.}
\label{tab:combined-results}
\end{table*}

\subsection{Implementation Details}
\label{implementation-details}

\paragraph{Sparse Query Aggregation.}  
In the sparse retrieval setting, following previous work~\cite{wang2023query2doc,jagerman2023query}, we replicate the initial query \(Q\) three times and append all refined pseudo-answers \(a'_i\). Specifically, let \(Q_i = Q\) for \(i=1,2,3\). The expanded query is formulated as:
\begin{equation}
Q^*_{\text{sparse}} = \sum_{i=1}^{3} Q_i + \sum_{j=1}^{|\mathcal{S}|} a'_j,
\end{equation}
where the “+” operator denotes the concatenation of query terms (with [SEP] tokens as separators).

\paragraph{Dense Query Aggregation.}  
    For dense retrieval, let \(\text{emb}(Q)\) be the embedding of the initial query and \(\text{emb}(a'_i)\) the embedding of each refined pseudo-answer. Following previous work in weighted query aggregations~\cite{seo2024gencrf}, which employed a weight of \(0.7\) for the initial query embedding, we adopted the same weighting scheme and compute the final query embedding \(Q^{*}_{dense}\):
\begin{equation}
Q^*_{\text{dense}} = 0.7 \cdot \text{emb}(Q) + 0.3 \cdot \frac{1}{|\mathcal{S}|} \sum_{i=1}^{|\mathcal{S}|} \text{emb}(a'_i).
\end{equation}

\paragraph{Reciprocal Rank Fusion (RRF).}  
In the RRF setting~\cite{cormack2009reciprocal}, each refined pseudo-answer \(a'_i\) is used to form an individual expanded query \(Q^*_i\). For each document \(d\), let \(r_{i,d}\) denote its rank when retrieved with \(Q^*_i\). The final score for \(d\) is computed as:
\begin{equation}
\text{score}(d) = \sum_{i=1}^{|\mathcal{S}|} \frac{1}{k + r_{i,d}},
\end{equation}
where \(k\) is a constant (e.g., \(k = 60\)) to dampen the influence of lower-ranked documents. Documents are then re-ranked based on their aggregated scores.\footnote{Algorithm of \emph{QA-Expand} is detailed in Appendix D.}



\subsection{Main Results}

In our experiments on both sparse and dense retrieval settings (see Table~\ref{tab:combined-results}), we found that while methods such as \emph{GenQREnsemble}, \emph{Q2C}, and \emph{Q2D} yield incremental improvements through query reformulations, each exhibits notable shortcomings. \emph{GenQREnsemble} uses multiple prompt configurations to produce pseudo-relevant term expansions, yet its repeated and narrowly focused outputs often miss the full spectrum of user intent. Similarly, \emph{Q2C} leverages chain-of-thought (CoT) reasoning but tends to generate repetitive expansions with limited contextual diversity, and although \emph{Q2D} produces pseudo-documents that better capture the underlying information need, it falls short in filtering out less informative content. In contrast, our \emph{QA-Expand} framework reformulates the query into diverse targeted questions and generates corresponding pseudo-answers that are dynamically evaluated, resulting in a 13\% improvement in average retrieval performance.

% In the fusion-based retrieval scenario, conventional methods like \emph{GenQRFusion} generate candidates via up to ten separate prompts and fuse the resulting rankings using Reciprocal Rank Fusion (RRF), often aggregating redundant or low-quality candidates. Our approach instead leverages a dedicated evaluation module to filter out inferior pseudo-answers before fusion, ensuring that only those expansions which robustly capture the multifaceted nature of the query are integrated, thereby reducing computational overhead while delivering significantly improved performance.

% In our experiments on both sparse and dense retrieval settings (see Table~\ref{tab:combined-results}), we observed that methods such as \emph{GenQREnsemble}, \emph{Q2C}, and \emph{Q2D} yield incremental improvements through query reformulations, yet each exhibits notable shortcomings. For instance, \emph{GenQREnsemble} relies on multiple prompt configurations to produce pseudo-relevant term expansions,while its repeated, narrowly focused outputs often overlook the full spectrum of user intent. Although \emph{Q2C} similarly leverages chain-of-thought (CoT) reasoning to enrich queries, its expansions tend to be repetitive and lack sufficient contextual diversity. Moreover, while \emph{Q2D} generates pseudo-documents that better capture the underlying information need, it falls short in filtering out less informative content. By comparison, our \emph{QA-Expand} framework reformulates the initial query into diverse relevant questions and generates corresponding pseudo-answers that are dynamically evaluated for quality. This two-stage process ensures that only the most relevant and diverse expansions are integrated and thus lead to markedly superior performance in both sparse and dense settings. In fact, our approach improves average retrieval performance by 13\% over the best-performing baselines.

In the fusion-based retrieval scenario, conventional methods such as \emph{GenQRFusion} typically generate candidates through up to ten separate prompts and then fuse the resulting rankings using Reciprocal Rank Fusion (RRF). Although this approach is intended to capture a wide range of query facets, it often aggregates redundant or low-quality candidates, resulting in an overall ineffective expansion. Our \emph{QA-Expand} framework, on the other hand, employs a more discerning selection process prior to fusion. Our method integrates only those expansions that robustly encapsulate the multifaceted nature of the initial query by leveraging a dedicated evaluation module to filter out inferior pseudo-answers. This targeted fusion strategy minimizes computational overhead while delivering significantly improved retrieval performance, as evidenced by our experimental results.

\subsection{Ablation Study and Analysis}
To evaluate the effectiveness of the evaluation module, we conducted an ablation study on two datasets. Table~\ref{tab:combined-avg} compares the full \emph{QA-Expand} framework with a variant that omits the evaluation module. The results show that including the evaluation module improves the average score by effectively filtering out redundant and less informative pseudo-answers, ensuring that only high-quality expansions contribute to query augmentation.

\begin{table}[htbp]
    \centering
    \resizebox{\columnwidth}{!}{%
        \begin{tabular}{llccc}
            \toprule
            \textbf{Methods} & \textbf{Feedback} & \textbf{BEIR} & \textbf{TREC DL'19} & \textbf{TREC DL'20} \\
            \midrule
            \multirow{2}{*}{BM25} 
                & w/o feedback & 0.5266 & 0.5342 & 0.5373 \\
                & w feedback   & \textbf{0.5302} & \textbf{0.5372} & \textbf{0.5402} \\
            \midrule
            \multirow{2}{*}{Dense} 
                & w/o feedback & 0.5115 & 0.6404 & 0.6474 \\
                & w feedback   & \textbf{0.5387} & \textbf{0.6502} & \textbf{0.6508} \\
            \midrule
            \multirow{2}{*}{BM25/RRF} 
                & w/o feedback & \textbf{0.5099} & 0.4766 & 0.5001 \\
                & w feedback   & 0.5004 & \textbf{0.4891} & \textbf{0.5003} \\
            \bottomrule
        \end{tabular}
    }
    \caption{\emph{Combined average retrieval performance on BEIR Benchmark and TREC DL datasets, with and without feedback}. Scores are averaged over four BEIR datasets and computed separately for TREC DL'19 and DL'20. Bold values denote the best performance.}
    \label{tab:combined-avg}
\end{table}

Furthermore, the evaluation module not only boosts overall performance but also enhances robustness. Without it, performance variability increases and more noise from less relevant pseudo-answers is observed, whereas the refined feedback mechanism maintains stable and superior retrieval effectiveness across diverse datasets. These findings highlight the importance of dynamically selecting high-quality expansions to capture the multifaceted nature of user intent. Notably, even the variant without the evaluation module outperforms other baselines, as shown in Table~\ref{tab:combined-results}.


% To assess the effectiveness of the evaluation module, an ablation study was conducted on the BEIR Benchmark and TREC datasets. Table~\ref{tab:combined-avg} compares the retrieval performance of the full \emph{QA-Expand} framework against a variant without the evaluation module. The results indicate that incorporating the \emph{evaluation module} improves the average score, which showcase its ability to effectively filter out redundant and less informative pseudo-answers, ensuring that only high-quality expansions contribute to query augmentation.

% Moreover, the analysis reveals that the evaluation module not only boosts overall performance but also enhances robustness. In the absence of this component, performance variability increases and more noise from less relevant pseudo-answers is observed. In contrast, the refined feedback mechanism consistently maintains stable and superior retrieval effectiveness across diverse datasets. These findings underscore the importance of the integrated evaluation strategy in \emph{QA-Expand}, confirming that dynamically selecting high-quality expansions is crucial for capturing the multifaceted nature of user intent. Notably, even the framework without the evaluation module achieves a higher average score than the other baselines, which are shown in Table~\ref{tab:combined-results}.

\section{Conclusion}
In this paper, we present our novel framework \emph{QA-Expand} which addresses query expansion by generating diverse question-answer pairs and employing a feedback model for selective rewriting and filtering. Our approach yields significant performance gains and better captures the multifaceted nature of user intent.  Experimental results on BEIR and TREC benchmarks demonstrate the effectiveness and robustness of \emph{QA-Expand}.
% In this paper, we presented our novel framework \emph{QA-Expand} which addresses query expansion by generating diverse question-answer pairs and employing a feedback model for selective rewriting and filtering. We showed that adopting this generation-based approach yields a significant performance boost and better captures the multifaceted nature of user intent. Experimental results on BEIR and TREC benchmarks demonstrate the effectiveness and robustness of \emph{QA-Expand}.

\section{Limitations}
One limitation is the persistence of residual noise and redundancy in the expanded queries. Although our feedback module is designed to filter out irrelevant or repetitive pseudo-answers, some less informative content may still be included, particularly for queries with ambiguous or complex information needs. Such residual noise can degrade retrieval precision by diluting the core intent of the initial query. Further research is needed to develop more robust filtering methods that can better discern and eliminate spurious information. Addressing this issue is an important direction for future work, as it could significantly improve the effectiveness of the query expansion process.
\bibliography{custom}


\appendix
\newpage
% \onecolumn

\section{Appendix A. Background}
\paragraph{Query Expansion with LLM.} Let \(Q\) denote the initial query and \(G\) be a Large Language Model (LLM) used for generation. Query expansion enhances retrieval by enriching \(Q\) with additional context~\cite{azad2019query,claveau2020query,naseri2021ceqe,jia2023mill}. Two predominant LLM-based strategies have emerged: (1) \emph{Pseudo-Document Generation}, where \(G\) produces a surrogate document \(D\) or an expanded query \(Q^*\) using a prompt \(P\) to capture latent information~\cite{wang2023query2doc,jagerman2023query,zhang2024exploring}, and (2) \emph{Term-Level Expansion}, where \(G\) generates a set of terms \(T = \{t_1, t_2, \dots, t_M\}\) that reflect diverse aspects of \(Q\)~\cite{dhole2024genqrensemble,dhole2024generative,li2024can,nguyen2024exploiting}.

\paragraph{Retrieval with Expanded Queries.} In query expansion, Information Retrieval (IR) integrates the initial query with generated augmentations using various strategies. For \emph{sparse retrieval}, the common method concatenates multiple copies of the initial query with generated terms to reinforce core signals~\cite{wang2023query2doc,zhang2024exploring}. In \emph{dense retrieval}, one strategy directly combines the query and its expansions into a unified embedding~\cite{li2023pseudo,wang2023query2doc}, while another fuses separate embeddings from each component~\cite{seo2024gencrf,kostric2024surprisingly}. Additionally, \emph{Reciprocal Rank Fusion (RRF)} aggregates rankings from individual expanded queries by inversely weighting document ranks~\cite{mackie2023generative}.


\section{Appendix B. Prompts}
\begin{tcolorbox}[
    colback=lightgray!20,
    colframe=darkgray!80,
    title=Prompt for Multiple Question Generation,
    halign=flush left
]
You are a helpful assistant. Based on the following query, generate $3$ possible related questions that someone might ask. \\ Format the response as a JSON object with the following structure:
\begin{verbatim}
{"question1":"First question ..."
"question2":"Second question ..."
"question3":"Third question ..."}
\end{verbatim}
Only include questions that are meaningful and logically related to the query. Here is the query: \{\}
\end{tcolorbox}

% Here, the number of generated answers should be up to 3, not answering all the questions which were generated previously .
\begin{tcolorbox}[
    colback=lightgray!20,
    colframe=darkgray!80,
    title=Prompt for Pseudo-Answer Generation,
    halign=flush left
]
You are a knowledgeable assistant. The user provides $3$ questions in JSON format. For each question, produce a document style answer. Each answer must:
Be informative regarding the question. Return all answers in JSON format with the keys \texttt{answer1}, \texttt{answer2}, and \texttt{answer3}. For example:
\begin{verbatim}
{"answer1": "...",
"answer2": "...",
"answer3": "..."}
\end{verbatim}

Text to answer: \{\}
\end{tcolorbox}

% \vspace{1em}
% \hrule
% \vspace{1em}


\begin{tcolorbox}[
    colback=lightgray!20,
    colframe=darkgray!80,
    title=Prompt for Feedback-driven Rewriting and Selection,
    halign=flush left
]
You are an evaluation assistant. You have an initial query and  answers provided in JSON format. Your role is to check how relevant and correct each answer is. Return only those answers that are relevant and correct to the initial query. Omit or leave blank any that are incorrect, irrelevant, or too vague. If needed, please rewrite the answer in a better way.

Return your result in JSON with the same structure:
\begin{verbatim}
{"answer1": "Relevant/correct...",
"answer2": "Relevant/correct...",
"answer3": "Relevant/correct..."}
\end{verbatim}

If an answer is irrelevant, do not include it at all or leave it empty. Focus on ensuring the final JSON only contains the best content for retrieval. Here is the combined input (initial query and answers): \{\}
\end{tcolorbox}



\section{Appendix C. Dataset Details}
\begin{table}[htbp]
    \centering
    \begin{tabular}{lcc}
        \toprule
        \textbf{Dataset} & \textbf{Test Queries} & \textbf{Corpus} \\
        \midrule
        Webis            & 49   & 382,545    \\
        SciFact          & 300  & 5,183      \\
        TREC-COVID       & 50   & 171,332    \\
        DBpedia-Entity   & 400  & 4,635,922  \\
        Trec DL'19 Passage             & 43   & 8,841,823  \\
        Trec DL'20 Passage            & 54   & 8,841,823  \\
        \bottomrule
    \end{tabular}
    \caption{Test Queries and Corpus Sizes for the Different Datasets from \emph{BEIR Benchmark} and \emph{TREC Track}.}
    \label{tab:dataset-details}
\end{table}



\section{Appendix D. Algorithm}
\begin{algorithm}
% \small
\caption{\emph{QA-Expand}: Query Expansion via Question-Answer Generation}\label{algo:qa-expand}
\begin{algorithmic}[1]
\Require Initial query $Q$, LLM models: Question Generator $G_{\mathcal{Q}}$, Answer Generator $G_{\mathcal{A}}$, Feedback Filter $G_{\mathcal{S}}$, Aggregation Strategy $Agg$
\Ensure Expanded query $Q^*$
    
\State \textbf{// Step 1: Multiple Question Generation}
\State $\mathcal{Q} \gets G_{\mathcal{Q}}(Q, P)$ 
\Comment Generate a set of diverse questions $\{q_1, q_2, \dots, q_N\}$ from $Q$

\State \textbf{// Step 2: Pseudo-Answer Generation}
\State $\mathcal{A} \gets G_{\mathcal{A}}(\mathcal{Q})$
\Comment Generate pseudo-answers concurrently for all questions, yielding $\mathcal{A} = \{a_1, a_2, \dots, a_N\}$

\State \textbf{// Step 3: Feedback-driven Rewriting and Selection}
\State $\mathcal{S} \gets G_{\mathcal{S}}(\{(q_i, a_i)\}_{i=1}^{N}, Q)$
\Comment Refine and filter to obtain $\mathcal{S} = \{a'_1, a'_2, \dots, a'_j\}$ with $0 \leq j \leq N$

\State \textbf{// Retrieval via Diverse Aggregation Methods}
\If{Method = Sparse}
    \State Compute: \(Q^*_{\text{sparse}} = \sum_{i=1}^{3} Q_i + \sum_{j=1}^{|\mathcal{S}|} a'_j\)
\ElsIf{Method = Dense}
    \State Compute: \(Q^*_{\text{dense}} = 0.7\cdot\text{emb}(Q) + 0.3\cdot \frac{1}{|\mathcal{S}|}\sum_{i=1}^{|\mathcal{S}|}\text{emb}(a'_i)\)
\ElsIf{Method = RRF}
    \For{each document \(d\)}
        \State Compute: \(\text{score}(d) = \sum_{i=1}^{|\mathcal{S}|}\frac{1}{k + r_{i,d}}\)
    \EndFor
\EndIf
\State \Return Ranked documents

\end{algorithmic}
\end{algorithm}

\end{document}
