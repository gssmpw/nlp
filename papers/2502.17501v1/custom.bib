
@ARTICLE{sun2024shapley,
  author={Sun, Qiheng and Zhang, Jiayao and Liu, Jinfei and Xiong, Li and Pei, Jian and Ren, Kui},
  journal={IEEE Transactions on Knowledge and Data Engineering}, 
  title={Shapley Value Approximation Based on Complementary Contribution}, 
  year={2024},
  volume={36},
  number={12},
  pages={9263-9281},
  keywords={Resource management;Heuristic algorithms;Games;Data models;Approximation algorithms;Estimation;Pricing;Approximate algorithms;dynamic datasets;shapley value},
  doi={10.1109/TKDE.2024.3438213}}


@inproceedings{
H2O,
title={H2O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models},
author={Zhenyu Zhang and Ying Sheng and Tianyi Zhou and Tianlong Chen and Lianmin Zheng and Ruisi Cai and Zhao Song and Yuandong Tian and Christopher Re and Clark Barrett and Zhangyang Wang and Beidi Chen},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
year={2023},
url={https://openreview.net/forum?id=RkRrPp7GKO}
}

@inproceedings{
StreamingLLM,
title={Efficient Streaming Language Models with Attention Sinks},
author={Guangxuan Xiao and Yuandong Tian and Beidi Chen and Song Han and Mike Lewis},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=NG7sS51zVF}
}

@inproceedings{
SnapKV,
title={Snap{KV}: {LLM} Knows What You are Looking for Before Generation},
author={Yuhong Li and Yingbing Huang and Bowen Yang and Bharat Venkitesh and Acyr Locatelli and Hanchen Ye and Tianle Cai and Patrick Lewis and Deming Chen},
booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
year={2024},
url={https://openreview.net/forum?id=poE54GOq2l}
}


@misc{PyramidKV,
      title={PyramidKV: Dynamic KV Cache Compression based on Pyramidal Information Funneling}, 
      author={Zefan Cai and Yichi Zhang and Bofei Gao and Yuliang Liu and Tianyu Liu and Keming Lu and Wayne Xiong and Yue Dong and Baobao Chang and Junjie Hu and Wen Xiao},
      year={2024},
      eprint={2406.02069},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2406.02069}, 
}

@misc{AdaKV,
      title={Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for Efficient LLM Inference}, 
      author={Yuan Feng and Junlin Lv and Yukun Cao and Xike Xie and S. Kevin Zhou},
      year={2025},
      eprint={2407.11550},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2407.11550}, 
}


@inproceedings{
HeadKV,
title={Not All Heads Matter: A Head-Level {KV} Cache Compression Method with Integrated Retrieval and Reasoning},
author={Yu Fu and Zefan Cai and Abedelkadir Asi and Wayne Xiong and Yue Dong and Wen Xiao},
booktitle={The Thirteenth International Conference on Learning Representations},
year={2025},
url={https://openreview.net/forum?id=FJFVmeXusW}
}

@misc{gpt4,
      title={GPT-4 Technical Report}, 
      author={OpenAI},
      year={2024},
      eprint={2303.08774},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2303.08774}, 
}
@article{llama3,
  author       = {Abhimanyu Dubey and
                  Abhinav Jauhri and
                  Abhinav Pandey and
                  Abhishek Kadian and
                  Ahmad Al{-}Dahle and
                  Aiesha Letman and
                  Akhil Mathur and
                  Alan Schelten and
                  Amy Yang and
                  Angela Fan and
                  Anirudh Goyal and
                  Anthony Hartshorn and
                  Aobo Yang and
                  Archi Mitra and
                  Archie Sravankumar and
                  Artem Korenev and
                  Arthur Hinsvark and
                  Arun Rao and
                  Aston Zhang and
                  Aur{\'{e}}lien Rodriguez and
                  Austen Gregerson and
                  Ava Spataru and
                  Baptiste Rozi{\`{e}}re and
                  Bethany Biron and
                  Binh Tang and
                  Bobbie Chern and
                  Charlotte Caucheteux and
                  Chaya Nayak and
                  Chloe Bi and
                  Chris Marra and
                  Chris McConnell and
                  Christian Keller and
                  Christophe Touret and
                  Chunyang Wu and
                  Corinne Wong and
                  Cristian Canton Ferrer and
                  Cyrus Nikolaidis and
                  Damien Allonsius and
                  Daniel Song and
                  Danielle Pintz and
                  Danny Livshits and
                  David Esiobu and
                  Dhruv Choudhary and
                  Dhruv Mahajan and
                  Diego Garcia{-}Olano and
                  Diego Perino and
                  Dieuwke Hupkes and
                  et al.},
  title        = {The Llama 3 Herd of Models},
  journal      = {CoRR},
  volume       = {abs/2407.21783},
  year         = {2024},
  url          = {https://doi.org/10.48550/arXiv.2407.21783},
  doi          = {10.48550/ARXIV.2407.21783},
  eprinttype    = {arXiv},
  eprint       = {2407.21783},
  timestamp    = {Wed, 11 Dec 2024 17:20:48 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2407-21783.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@misc{mistral,
      title={Mistral 7B}, 
      author={Albert Q. Jiang and Alexandre Sablayrolles and Arthur Mensch and Chris Bamford and Devendra Singh Chaplot and Diego de las Casas and Florian Bressand and Gianna Lengyel and Guillaume Lample and Lucile Saulnier and Lélio Renard Lavaud and Marie-Anne Lachaux and Pierre Stock and Teven Le Scao and Thibaut Lavril and Thomas Wang and Timothée Lacroix and William El Sayed},
      year={2023},
      eprint={2310.06825},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2310.06825}, 
}

@misc{claude3,
  author = {Anthropic},
  title = {The Claude 3 Model Family: Opus, Sonnet, Haiku},
  year = {2024},
  howpublished = {\url{https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf}},
  note = {Accessed: 2025-02-04}
}

@article{cc,
author = {Zhang, Jiayao and Sun, Qiheng and Liu, Jinfei and Xiong, Li and Pei, Jian and Ren, Kui},
title = {Efficient Sampling Approaches to Shapley Value Approximation},
year = {2023},
issue_date = {May 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {1},
url = {https://doi.org/10.1145/3588728},
doi = {10.1145/3588728},
abstract = {Shapley value provides a unique way to fairly assess each player's contribution in a coalition and has enjoyed many applications. However, the exact computation of Shapley value is #P-hard due to the combinatoric nature of Shapley value. Many existing applications of Shapley value are based on Monte-Carlo approximation, which requires a large number of samples and the assessment of utility on many coalitions to reach high quality approximation, and thus is still far from being efficient. Can we achieve an efficient approximation of Shapley value by smartly obtaining samples? In this paper, we treat the sampling approach to Shapley value approximation as a stratified sampling problem. Our main technical contributions are a novel stratification design and two sample allocation methods based on Neyman allocation and empirical Bernstein bound, respectively. Experimental results on several real data sets and synthetic data sets demonstrate the effectiveness and efficiency of our novel stratification design and sampling approaches.},
journal = {Proc. ACM Manag. Data},
month = may,
articleno = {48},
numpages = {24},
keywords = {Shapley value, data market, sampling}
}

@inproceedings{longbench,
    title = "{L}ong{B}ench: A Bilingual, Multitask Benchmark for Long Context Understanding",
    author = "Bai, Yushi  and
      Lv, Xin  and
      Zhang, Jiajie  and
      Lyu, Hongchang  and
      Tang, Jiankai  and
      Huang, Zhidian  and
      Du, Zhengxiao  and
      Liu, Xiao  and
      Zeng, Aohan  and
      Hou, Lei  and
      Dong, Yuxiao  and
      Tang, Jie  and
      Li, Juanzi",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-long.172/",
    doi = "10.18653/v1/2024.acl-long.172",
    pages = "3119--3137",
    abstract = "Although large language models (LLMs) demonstrate impressive performance for many language tasks, most of them can only handle texts a few thousand tokens long, limiting their applications on longer sequence inputs, such as books, reports, and codebases. Recent works have proposed methods to improve LLMs' long context capabilities by extending context windows and more sophisticated memory mechanisms. However, comprehensive benchmarks tailored for evaluating long context understanding are lacking. In this paper, we introduce LongBench, the first bilingual, multi-task benchmark for long context understanding, enabling a more rigorous evaluation of long context understanding. LongBench comprises 21 datasets across 6 task categories in both English and Chinese, with an average length of 6,711 words (English) and 13,386 characters (Chinese). These tasks cover key long-text application areas including single-doc QA, multi-doc QA, summarization, few-shot learning, synthetic tasks, and code completion. All datasets in LongBench are standardized into a unified format, allowing for effortless automatic evaluation of LLMs. Upon comprehensive evaluation of 8 LLMs on LongBench, we find that: (1) Commercial model (GPT-3.5-Turbo-16k) outperforms other open-sourced models, but still struggles on longer contexts. (2) Scaled position embedding and fine-tuning on longer sequences lead to substantial improvement on long context understanding. (3) Context compression technique such as retrieval brings improvement for model with weak ability on long contexts, but the performance still lags behind models that have strong long context understanding capability."
}

@article{shapley,
  title={A value for n-person games},
  author={Shapley, Lloyd S},
  journal={Contribution to the Theory of Games},
  volume={2},
  year={1953}
}

@inproceedings{
scissorhands,
title={Scissorhands: Exploiting the Persistence of Importance Hypothesis for {LLM} {KV} Cache Compression at Test Time},
author={Zichang Liu and Aditya Desai and Fangshuo Liao and Weitao Wang and Victor Xie and Zhaozhuo Xu and Anastasios Kyrillidis and Anshumali Shrivastava},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
year={2023},
url={https://openreview.net/forum?id=JZfg6wGi6g}
}

@inproceedings{
FastGen,
title={Model Tells You What to Discard: Adaptive {KV} Cache Compression for {LLM}s},
author={Suyu Ge and Yunan Zhang and Liyuan Liu and Minjia Zhang and Jiawei Han and Jianfeng Gao},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=uNrFpDPMyo}
}

@inproceedings{
Razorattention,
title={RazorAttention: Efficient {KV} Cache Compression Through Retrieval Heads},
author={Hanlin Tang and Yang Lin and Jing Lin and Qingsen Han and Shikuan Hong and Danning Ke and Yiwu Yao and Gongyi Wang},
booktitle={The Thirteenth International Conference on Learning Representations},
year={2025},
url={https://openreview.net/forum?id=tkiZQlL04w}
}

@inproceedings{
Duoattention,
title={DuoAttention: Efficient Long-Context {LLM} Inference with Retrieval and Streaming Heads},
author={Guangxuan Xiao and Jiaming Tang and Jingwei Zuo and junxian guo and Shang Yang and Haotian Tang and Yao Fu and Song Han},
booktitle={The Thirteenth International Conference on Learning Representations},
year={2025},
url={https://openreview.net/forum?id=cFu7ze7xUm}
}

@inproceedings{
retrievalheads,
title={Retrieval Head Mechanistically Explains Long-Context Factuality},
author={Wenhao Wu and Yizhong Wang and Guangxuan Xiao and Hao Peng and Yao Fu},
booktitle={The Thirteenth International Conference on Learning Representations},
year={2025},
url={https://openreview.net/forum?id=EytBpUGB1Z}
}

@misc{mqa,
      title={Fast Transformer Decoding: One Write-Head is All You Need}, 
      author={Noam Shazeer},
      year={2019},
      eprint={1911.02150},
      archivePrefix={arXiv},
      primaryClass={cs.NE},
      url={https://arxiv.org/abs/1911.02150}, 
}

@inproceedings{
gqa,
title={{GQA}: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints},
author={Joshua Ainslie and James Lee-Thorp and Michiel de Jong and Yury Zemlyanskiy and Federico Lebron and Sumit Sanghai},
booktitle={The 2023 Conference on Empirical Methods in Natural Language Processing},
year={2023},
url={https://openreview.net/forum?id=hmOwOZWzYE}
}

@inproceedings{
flashattention,
title={FlashAttention: Fast and Memory-Efficient Exact Attention with {IO}-Awareness},
author={Tri Dao and Daniel Y Fu and Stefano Ermon and Atri Rudra and Christopher Re},
booktitle={Advances in Neural Information Processing Systems},
editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
year={2022},
url={https://openreview.net/forum?id=H4DqfPSibmx}
}

@inproceedings{vllm,
  title={Efficient memory management for large language model serving with pagedattention},
  author={Kwon, Woosuk and Li, Zhuohan and Zhuang, Siyuan and Sheng, Ying and Zheng, Lianmin and Yu, Cody Hao and Gonzalez, Joseph and Zhang, Hao and Stoica, Ion},
  booktitle={Proceedings of the 29th Symposium on Operating Systems Principles},
  pages={611--626},
  year={2023}
}

@inproceedings{shap,
 author = {Lundberg, Scott M and Lee, Su-In},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {A Unified Approach to Interpreting Model Predictions},
 url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/8a20a8621978632d76c43dfd28b67767-Paper.pdf},
 volume = {30},
 year = {2017}
}

@InProceedings{jia2019towards,
  title = 	 {Towards Efficient Data Valuation Based on the Shapley Value},
  author =       {Jia, Ruoxi and Dao, David and Wang, Boxin and Hubis, Frances Ann and Hynes, Nick and G\"{u}rel, Nezihe Merve and Li, Bo and Zhang, Ce and Song, Dawn and Spanos, Costas J.},
  booktitle = 	 {Proceedings of the Twenty-Second International Conference on Artificial Intelligence and Statistics},
  pages = 	 {1167--1176},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Sugiyama, Masashi},
  volume = 	 {89},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {16--18 Apr},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v89/jia19a/jia19a.pdf},
  url = 	 {https://proceedings.mlr.press/v89/jia19a.html},
  abstract = 	 {{\em “How much is my data worth?”} is an increasingly common question posed by organizations and individuals alike. An answer to this question could allow, for instance, fairly distributing profits among multiple data contributors and determining prospective compensation when data breaches happen. In this paper, we study the problem of \emph{data valuation} by utilizing the Shapley value, a popular notion of value which originated in coopoerative game theory. The Shapley value defines a unique payoff scheme that satisfies many desiderata for the notion of data value. However, the Shapley value often requires \emph{exponential} time to compute. To meet this challenge, we propose a repertoire of efficient algorithms for approximating the Shapley value. We also demonstrate the value of each training instance for various benchmark datasets.}
}

@article{mitchell2022sampling,
  author       = {Rory Mitchell and
                  Joshua Cooper and
                  Eibe Frank and
                  Geoffrey Holmes},
  title        = {Sampling Permutations for Shapley Value Estimation},
  journal      = {J. Mach. Learn. Res.},
  volume       = {23},
  pages        = {43:1--43:46},
  year         = {2022},
  url          = {https://jmlr.org/papers/v23/21-0439.html},
  timestamp    = {Wed, 11 Sep 2024 14:41:27 +0200},
  biburl       = {https://dblp.org/rec/journals/jmlr/MitchellCFH22.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@article{deng1994complexity,
  author       = {Xiaotie Deng and
                  Christos H. Papadimitriou},
  title        = {On the Complexity of Cooperative Solution Concepts},
  journal      = {Math. Oper. Res.},
  volume       = {19},
  number       = {2},
  pages        = {257--266},
  year         = {1994},
  url          = {https://doi.org/10.1287/moor.19.2.257},
  doi          = {10.1287/MOOR.19.2.257},
  timestamp    = {Sun, 28 May 2017 13:24:36 +0200},
  biburl       = {https://dblp.org/rec/journals/mor/DengP94.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{li2024pre,
  author       = {Junyi Li and
                  Tianyi Tang and
                  Wayne Xin Zhao and
                  Jian{-}Yun Nie and
                  Ji{-}Rong Wen},
  title        = {Pre-Trained Language Models for Text Generation: {A} Survey},
  journal      = {{ACM} Comput. Surv.},
  volume       = {56},
  number       = {9},
  pages        = {230:1--230:39},
  year         = {2024},
  url          = {https://doi.org/10.1145/3649449},
  doi          = {10.1145/3649449},
  timestamp    = {Tue, 18 Jun 2024 09:25:09 +0200},
  biburl       = {https://dblp.org/rec/journals/csur/LiTZNW24.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{chen2024large,
  author       = {Jin Chen and
                  Zheng Liu and
                  Xu Huang and
                  Chenwang Wu and
                  Qi Liu and
                  Gangwei Jiang and
                  Yuanhao Pu and
                  Yuxuan Lei and
                  Xiaolong Chen and
                  Xingmei Wang and
                  Kai Zheng and
                  Defu Lian and
                  Enhong Chen},
  title        = {When large language models meet personalization: perspectives of challenges
                  and opportunities},
  journal      = {World Wide Web {(WWW)}},
  volume       = {27},
  number       = {4},
  pages        = {42},
  year         = {2024},
  url          = {https://doi.org/10.1007/s11280-024-01276-1},
  doi          = {10.1007/S11280-024-01276-1},
  timestamp    = {Mon, 10 Feb 2025 10:29:52 +0100},
  biburl       = {https://dblp.org/rec/journals/www/ChenLHWLJPLCWZLC24.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{bao2023tallrec,
  author       = {Keqin Bao and
                  Jizhi Zhang and
                  Yang Zhang and
                  Wenjie Wang and
                  Fuli Feng and
                  Xiangnan He},
  editor       = {Jie Zhang and
                  Li Chen and
                  Shlomo Berkovsky and
                  Min Zhang and
                  Tommaso Di Noia and
                  Justin Basilico and
                  Luiz Pizzato and
                  Yang Song},
  title        = {TALLRec: An Effective and Efficient Tuning Framework to Align Large
                  Language Model with Recommendation},
  booktitle    = {Proceedings of the 17th {ACM} Conference on Recommender Systems, RecSys
                  2023, Singapore, Singapore, September 18-22, 2023},
  pages        = {1007--1014},
  publisher    = {{ACM}},
  year         = {2023},
  url          = {https://doi.org/10.1145/3604915.3608857},
  doi          = {10.1145/3604915.3608857},
  timestamp    = {Sun, 19 Jan 2025 13:14:39 +0100},
  biburl       = {https://dblp.org/rec/conf/recsys/BaoZZWF023.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{yang2024no,
  author       = {June Yong Yang and
                  Byeongwook Kim and
                  Jeongin Bae and
                  Beomseok Kwon and
                  Gunho Park and
                  Eunho Yang and
                  Se Jung Kwon and
                  Dongsoo Lee},
  title        = {No Token Left Behind: Reliable {KV} Cache Compression via Importance-Aware
                  Mixed Precision Quantization},
  journal      = {CoRR},
  volume       = {abs/2402.18096},
  year         = {2024},
  url          = {https://doi.org/10.48550/arXiv.2402.18096},
  doi          = {10.48550/ARXIV.2402.18096},
  eprinttype    = {arXiv},
  eprint       = {2402.18096},
  timestamp    = {Tue, 26 Mar 2024 10:51:46 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2402-18096.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{liukivi,
  author       = {Zirui Liu and
                  Jiayi Yuan and
                  Hongye Jin and
                  Shaochen Zhong and
                  Zhaozhuo Xu and
                  Vladimir Braverman and
                  Beidi Chen and
                  Xia Hu},
  title        = {{KIVI:} {A} Tuning-Free Asymmetric 2bit Quantization for {KV} Cache},
  booktitle    = {Forty-first International Conference on Machine Learning, {ICML} 2024,
                  Vienna, Austria, July 21-27, 2024},
  publisher    = {OpenReview.net},
  year         = {2024},
  url          = {https://openreview.net/forum?id=L057s2Rq8O},
  timestamp    = {Thu, 24 Oct 2024 16:57:14 +0200},
  biburl       = {https://dblp.org/rec/conf/icml/LiuYJZXBC024.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{chen2025arkvale,
  author       = {Renze Chen and
                  Zhuofeng Wang and
                  Beiquan Cao and
                  Tong Wu and
                  Size Zheng and
                  Xiuhong Li and
                  Xuechao Wei and
                  Shengen Yan and
                  Meng Li and
                  Yun Liang},
  editor       = {Amir Globersons and
                  Lester Mackey and
                  Danielle Belgrave and
                  Angela Fan and
                  Ulrich Paquet and
                  Jakub M. Tomczak and
                  Cheng Zhang},
  title        = {ArkVale: Efficient Generative {LLM} Inference with Recallable Key-Value
                  Eviction},
  booktitle    = {Advances in Neural Information Processing Systems 38: Annual Conference
                  on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver,
                  BC, Canada, December 10 - 15, 2024},
  year         = {2024},
  url          = {http://papers.nips.cc/paper\_files/paper/2024/hash/cd4b49379efac6e84186a3ffce108c37-Abstract-Conference.html},
  timestamp    = {Thu, 13 Feb 2025 16:56:44 +0100},
  biburl       = {https://dblp.org/rec/conf/nips/ChenWCW0LWYL024.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}