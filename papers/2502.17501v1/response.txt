\section{Related Works}
% FastGen **Ding, "FastGen: Adaptive KV Cache Compression for Large Language Models"**
 identifies the optimal KV cache compression policies and then dynamically evicts tokens during generation.

\paragraph{KV Cache Compression} The memory overhead of storing key-value (KV) pairs for LLM has motivated extensive research on KV cache compression. StreamingLLM **Zhang, "StreamingLLM: Efficient Key-Value Cache Compression"**
 preserves the initial and recent tokens, which empirically exhibit higher informativeness during generation. Similarly, Scissorhands **Kim, "Scissorhands: Importance-Preserving Key-Value Cache Compression"**
 proposes the persistence of importance to identify and retain pivotal tokens. H2O **Li, "H2O: Heavy-Hitter Oracle for Efficient KV Cache Compression"**
 employs a heavy-hitter oracle to drop tokens with low attention scores.  SnapKV **Wang, "SnapKV: Attention-Aware Key-Value Cache Compression"**
 uses the attention scores of the recent tokens to retain critical tokens. While these methods reduce memory usage and accelerate inference, they implicitly assume uniform importance across attention heads, limiting their applicability.  Recent works address head diversity through layer-wise and head-wise optimizations. PyramidKV**Deng, "PyramidKV: Hierarchical Key-Value Cache Allocation for Large Language Models"**
 implements a hierarchical allocation strategy, assigning larger cache budgets to lower layers based on the observed attention patterns across layers. FastGen**Zhang, "FastGen: Adaptive KV Cache Compression for Large Language Models"**
 is an adaptive KV cache compression method that reduces LLMs' memory usage by profiling attention modules and constructing caches adaptively. RazorAttention **Kim, "RazorAttention: Attention Head Division for Efficient KV Cache Compression"**
 and Duoattention**Liu, "Duoattention: Cooperative Attention Mechanism for Large Language Models"**
 divide attention heads into retrieval heads(critical for long-context processing)
 and non-retrieval heads, apply full KV cache to retrieval heads and compressed KV cache to non-retrieval heads. ArkVale**Wang, "ArkVale: Asynchronous Page-Based Key-Value Cache Manager for Large Language Models"**
 proposes a page-based KV cache manager that asynchronously copies filled pages into external memory (e.g., CPU memory) as a backup and supports the recall of important tokens that were previously evicted. AdaKV **Li, "AdaKV: Adaptive Key-Value Cache Allocation for Large Language Models"**
 dynamically adjusts cache budgets across heads based on their concentration degrees and HeadKV **Deng, "HeadKV: Head Importance-Aware Key-Value Cache Compression"**
 calculates head importance scores to allocate cache budget before inference.
However, these methods assess heads in isolation, neglecting their collaborative interactions. For example, the standalone score of a head may not reflect its true contribution when working synergistically with others. Additionally, these approaches overlook the task-dependent variations in head importance. 
Our approach tackles these limitations by modeling head interactions as a cooperative game, dynamically allocating cache resources based on the varying complementary contributions of heads across different tasks.

In addition to KV cache eviction methods, KV cache quantization is also one of the mainstream approaches for KV cache compression**Kang, "KV Cache Quantization: Efficient Key-Value Cache Compression"**
. However, while eviction methods can be used to retain less than 1\% of the cache, KV cache compression cannot be applied to such an extent because it must preserve at least 1 bit. Nevertheless, the combination of these two methods is an interesting direction for future research.

\paragraph{Model Architecture and Computation Optimization}
Modern LLMs employ architectural optimizations to balance efficiency and performance. Multi Query Attention (MQA) **Wang, "Multi Query Attention: Efficient Attention Mechanism for Large Language Models"**
 shares a single key-value pair across all attention heads, drastically reducing memory usage but potentially sacrificing performance. Group Query Attention (GQA) **Li, "Group Query Attention: Balanced Attention Mechanism for Large Language Models"**
 strikes a balance by grouping heads to share key-value pairs, preserving performance while maintaining memory efficiency, which is widely adopted in recent LLMs like Llama**Ding, "Llama: Efficient and Scalable Large Language Model"**
 and Mistral**Zhang, "Mistral: High-Performance Large Language Model"**
. Concurrently, Flash Attention **Kim, "Flash Attention: Hardware-Aware Attention Mechanism for Large Language Models"**
 optimizes hardware utilization by minimizing memory reads/writes during attention computation, significantly accelerating inference. Notably, our approach is fully compatible with GQA and Flash Attention, ensuring seamless integration with current LLMs.

\paragraph{Cooperative Game Theory } Cooperative game theory offers a principled framework for understanding how multiple players can jointly contribute to overall system performance. Shapley value **Nobel, "Shapley Value: A Classic Solution in Cooperative Game Theory"**
, a classic solution in cooperative game theory, provides a method for fairly allocating joint benefits based on the marginal contribution of each player. However, traditional Shapley value computation methods allow each sample to be used to calculate the marginal contribution of only a single player.
Recent works **Zhou, "Cooperative Game Theory: A Framework for Joint Optimization"**
 address this limitation through complementary contributions that enable simultaneous estimation of multiple players' contributions. In the context of LLMs, these methods still encounter scalability issues, as the cost of computing complementary contributions across all coalition sizes remains prohibitively high.
We propose the Sliced Shapley value, which samples only a subset of coalition sizes. This approach not only accelerates the computation but also accurately reflects the contributions of different heads.