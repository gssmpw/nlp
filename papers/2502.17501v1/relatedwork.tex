\section{Related Works}
% FastGen ~\cite{FastGen} identifies the optimal KV cache compression policies and then dynamically evicts tokens during generation.

\paragraph{KV Cache Compression} The memory overhead of storing key-value (KV) pairs for LLM has motivated extensive research on KV cache compression. StreamingLLM ~\cite{StreamingLLM} preserves the initial and recent tokens, which empirically exhibit higher informativeness during generation. Similarly, Scissorhands ~\cite{scissorhands} proposes the persistence of importance to identify and retain pivotal tokens. H2O ~\cite{H2O} employs a heavy-hitter oracle to drop tokens with low attention scores.  SnapKV ~\cite{SnapKV} uses the attention scores of the recent tokens to retain critical tokens. While these methods reduce memory usage and accelerate inference, they implicitly assume uniform importance across attention heads, limiting their applicability.  Recent works address head diversity through layer-wise and head-wise optimizations. PyramidKV~\cite{PyramidKV} implements a hierarchical allocation strategy, assigning larger cache budgets to lower layers based on the observed attention patterns across layers. FastGen~\cite{FastGen} is an adaptive KV cache compression method that reduces LLMs' memory usage by profiling attention modules and constructing caches adaptively. RazorAttention ~\cite{Razorattention} and Duoattention~\cite{Duoattention} divide attention heads into retrieval heads(critical for long-context processing~\cite{retrievalheads}) and non-retrieval heads, apply full KV cache to retrieval heads and compressed KV cache to non-retrieval heads. ArkVale~\cite{chen2025arkvale} proposes a page-based KV cache manager that asynchronously copies filled pages into external memory (e.g., CPU memory) as a backup and supports the recall of important tokens that were previously evicted. AdaKV ~\cite{AdaKV} dynamically adjusts cache budgets across heads based on their concentration degrees and HeadKV ~\cite{HeadKV} calculates head importance scores to allocate cache budget before inference.
However, these methods assess heads in isolation, neglecting their collaborative interactions. For example, the standalone score of a head may not reflect its true contribution when working synergistically with others. Additionally, these approaches overlook the task-dependent variations in head importance. 
Our approach tackles these limitations by modeling head interactions as a cooperative game, dynamically allocating cache resources based on the varying complementary contributions of heads across different tasks.

In addition to KV cache eviction methods, KV cache quantization is also one of the mainstream approaches for KV cache compression~\cite{yang2024no,liukivi}. However, while eviction methods can be used to retain less than 1\% of the cache, KV cache compression cannot be applied to such an extent because it must preserve at least 1 bit. Nevertheless, the combination of these two methods is an interesting direction for future research.

\paragraph{Model Architecture and Computation Optimization}
Modern LLMs employ architectural optimizations to balance efficiency and performance. Multi Query Attention (MQA) ~\cite{mqa} shares a single key-value pair across all attention heads, drastically reducing memory usage but potentially sacrificing performance. Group Query Attention (GQA) ~\cite{gqa} strikes a balance by grouping heads to share key-value pairs, preserving performance while maintaining memory efficiency, which is widely adopted in recent LLMs like Llama~\cite{llama3} and Mistral~\cite{mistral}. Concurrently, Flash Attention ~\cite{flashattention} optimizes hardware utilization by minimizing memory reads/writes during attention computation, significantly accelerating inference. Notably, our approach is fully compatible with GQA and Flash Attention, ensuring seamless integration with current LLMs.

\paragraph{Cooperative Game Theory } Cooperative game theory offers a principled framework for understanding how multiple players can jointly contribute to overall system performance. Shapley value ~\cite{shapley}, a classic solution in cooperative game theory, provides a method for fairly allocating joint benefits based on the marginal contribution of each player. However, traditional Shapley value computation methods allow each sample to be used to calculate the marginal contribution of only a single player.
Recent works ~\cite{cc,sun2024shapley} address this limitation through complementary contributions that enable simultaneous estimation of multiple players' contributions. In the context of LLMs, these methods still encounter scalability issues, as the cost of computing complementary contributions across all coalition sizes remains prohibitively high.
We propose the Sliced Shapley value, which samples only a subset of coalition sizes. This approach not only accelerates the computation but also accurately reflects the contributions of different heads.