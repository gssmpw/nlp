% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[preprint]{acl}

% Standard package includes
\usepackage{CJKutf8} % 使用CJKutf8宏包
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage{times}
\usepackage{latexsym}
\usepackage{makecell} % 在导言区添加
\usepackage{amsmath,amssymb,units}
\usepackage[ruled,vlined,linesnumbered]{algorithm2e}
\usepackage{subfigure}
\usepackage{enumitem}
\usepackage{import}
\usepackage{bm}
\usepackage{booktabs}
\usepackage{multirow}
%\usepackage[table]{xcolor}
\usepackage{array}
\usepackage{caption}
\usepackage{adjustbox}
\usepackage{balance}
\usepackage{hyperref}
\newcolumntype{C}{>{\centering\arraybackslash}p{0.7cm}}
% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}
\newcommand{\todo}[1]{\textcolor{red}{\textbf{[#1]}}}
\newcommand{\checks}[1]{\textcolor{blue}{\textbf{[#1]}}}
\newcommand{\placehold}{\textcolor{white}{0}}

\newtheorem{definition}{Definition}
\newtheorem{Theorem}{Theorem}
% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{CoKV: Optimizing KV Cache Allocation via Cooperative Game}

\author{
Qiheng Sun$^{1,2}$, Hongwei Zhang$^{1,2}$, Haocheng Xia$^3$, Jiayao Zhang$^{1,2}$, Jinfei Liu$^{1,2}$\thanks{Jinfei Liu is the corresponding author.} Kui Ren$^1$\\
$^1$the State Key Laboratory of Blockchain and Data Security, Zhejiang University\\
$^2$Hangzhou High-Tech Zone (Binjiang) Institute of Blockchain and Data Security\\
$^3$Siebel School of Computing and Data Science\\
University of Illinois Urbana-Champaign\\
\texttt{\{qiheng\_sun,hongweizhang, jiayaozhang, jinfeiliu, kuiren\}@zju.edu.cn}\\
\texttt{hxia7@illinois.edu}\\
}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

% \author{First Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\\And
%   Second Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\}

%\author{
%  \textbf{First Author\textsuperscript{1}},
%  \textbf{Second Author\textsuperscript{1,2}},
%  \textbf{Third T. Author\textsuperscript{1}},
%  \textbf{Fourth Author\textsuperscript{1}},
%\\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
%\\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
%\\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
%\\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
%\\
%\\
%  \textsuperscript{1}Affiliation 1,
%  \textsuperscript{2}Affiliation 2,
%  \textsuperscript{3}Affiliation 3,
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
%\\
%  \small{
%    \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
%  }
%}

\begin{document}
\maketitle
\begin{abstract}
Large language models (LLMs) have achieved remarkable success on various aspects of human life. However, one of the major challenges in deploying these models is the substantial memory consumption required to store key-value pairs (KV), which imposes significant resource demands. Recent research has focused on KV cache budget allocation, with several approaches proposing head-level budget distribution by evaluating the importance of individual attention heads. These methods, however, assess the importance of heads independently, overlooking their cooperative contributions within the model, which may result in a deviation from their true impact on model performance. In light of this limitation, we propose CoKV, a novel method that models the cooperation between heads in model inference as a cooperative game. By evaluating the contribution of each head within the cooperative game, CoKV can allocate the cache budget more effectively. Extensive experiments show that CoKV achieves state-of-the-art performance on the LongBench benchmark using LLama-3-8B-Instruct and Mistral-7B models. Code is provided in \url{https://github.com/nawei1010/CoKV}.
\end{abstract}

\section{Introduction}
Large language models (LLMs) are widely applied across various domains, including content generation~\cite{li2024pre}, automated services~\cite{chen2024large}, and decision support systems~\cite{bao2023tallrec}. To enhance the application capabilities of large language models, it is essential for them to handle long texts. For example, GPT-4~\cite{gpt4} and Llama-3~\cite{llama3} support a context size of 128k tokens, while the context size of Claude 3~\cite{claude3} is up to 200k tokens. LLMs consist of multiple transformer blocks that store key and value states (KV) during inference. KV cache allows efficient decoding in token generation without recomputing key and value states by using previously cached KV pairs. However, the KV cache grows excessively large when dealing with long texts, inevitably straining GPU memory and increasing decoding latency.

Eviction of less important key and value states in the cache has garnered significant attention. Many studies have explored methods for ranking the importance of tokens within a single attention head, retaining only the top $k$ most significant ones. For example, H2O~\cite{H2O} evaluates token importance using the sum of attention weights. StreamingLLM~\cite{StreamingLLM} directly removes KV from the middle segment of the cache to reduce the cache size as they incorporate less information. SnapKV~\cite{SnapKV} calculates token scores by pooling the attention weights between tokens in the local window and those in the cache. Recently, some studies have recognized that the importance of each attention head varies, enabling methods like AdaKV~\cite{AdaKV} and HeadKV~\cite{HeadKV}. AdaKV improves budget utilization by adaptively allocating the overall budget across different attention heads based on their varied concentration degrees. Heads with sparse concentrations require a small cache proportion, whereas more dispersed heads demand larger allocations. HeadKV evaluates the retrieval-reasoning scores of different heads and allocates a larger cache size to those with higher scores.

Motivated by evidence that attention heads vary in importance, we propose a novel approach to better evaluate and utilize this variability. We identify two key insights. First, existing methods evaluate attention head importance independently. For example, AdaKV evaluates the concentration degrees of heads while HeadKV assesses the retrieval-reasoning capability of each head in isolation as a measure of importance. However, these approaches treat heads as isolated units, overlooking the fact that their true importance emerges from their cooperation rather than individual capabilities. As a result, independently assessing head importance may lead to suboptimal allocation. Second, existing methods evaluate head importance in a task-agnostic manner. However, heads that play a critical role in query answering may not hold the same level of significance in code generation. Consequently, applying the same importance scores to heads across all tasks within a model may fail to reflect the practical need of each specific task accurately. Based on these insights, we propose \textbf{CoKV} 
 (\underline{\textbf{Co}}operation-based \underline{\textbf{K}}ey-\underline{\textbf{V}}alue), a method that evaluates the contribution of all attention heads in their cooperation within the model and dynamically allocates cache budgets based on their contribution to the specific task.

CoKV is inspired by the Shapley value~\cite{shapley} from cooperative game theory. The Shapley value of a player $p_i$ measures the expected marginal contribution that $p_i$ provides to a coalition of players. Similarly, we can use the Shapley value to assess the importance of each attention head by viewing each head as a player. Marginal contribution is defined as $\mathcal{U}(\mathcal{S} \cup \{p_i\}) - \mathcal{U}(\mathcal{S})$ where \( \mathcal{S} \) is a coalition of players excluding $i$ and \( \mathcal{U} \) is the utility function. A simple intuition for computing the Shapley value of each head in the model is to define $\mathcal{U}$ as the model performance metric. However, calculating the Shapley value is \#P-hard~\cite{deng1994complexity}, as there are an exponential number of coalitions and corresponding marginal contributions. As a result, evaluating the Shapley value for each head in LLMs requires an enormous number of model inferences. Although many studies~\cite{jia2019towards, mitchell2022sampling} have explored approximating the Shapley value to reduce computational costs, the process remains costly.


The computational bottleneck in calculating the Shapley value arises from the fact that each sample of the marginal contribution only can be applied to a single player. Fortunately, Shapley value can be expressed as the expectation of the weighted complementary contribution, defined as \( \mathcal{U}(\mathcal{S}) - \mathcal{U}(\mathcal{N} \setminus \mathcal{S}) \), where \( \mathcal{N} \) represents the set of all players~\cite{cc}. Complementary contribution has an advantage over marginal contribution is that \( \mathcal{U}(\mathcal{S}) - \mathcal{U}(\mathcal{N} \setminus \mathcal{S}) \) can be used to update the Shapley values for all players \( i \in \mathcal{S} \). By expressing the Shapley value in terms of complementary contributions, we can interpret it as an expectation over these contributions computed at different coalition sizes \( |\mathcal{S}| \). However, in the LLM setting, the cost of computing the complementary contributions in all coalition sizes is still prohibitively high. We observe that the average complementary contribution at each coalition size exhibits a strong correlation with the Shapley value of the players in Appendix Section~\ref{sec:coalition_distribution}. This insight allows us to approximate attention head importance by computing complementary contributions at only a few selected coalition sizes, rather than evaluating all possible sizes (i.e., from 1 to $|\mathcal{N}|$). By focusing on a few representative coalition sizes, we can significantly reduce the computational cost of estimating the contributions of heads. Additionally, we provide a theoretical analysis of this approach and demonstrate its efficiency.

CoKV is a simple-yet-effective method and can integrate well with other inference optimization techniques. We integrate CoKV with widely used methods including FlashAttention~\cite{flashattention} and grouped-query attention (GQA)~\cite{gqa}. CoKV achieves state-of-the-art performance in LongBench~\cite{longbench} using Llama-3-8B-Instruct~\cite{llama3} and Mistral-7B~\cite{mistral} models. Results from the Llama-3-8B-Instruct model show that when each KV cache retains an average of 128 KV pairs ($1.6\%$ of the full cache), it achieves $97.29\%$ of the performance of the full KV cache. Furthermore, when each cache retains just 512 tokens on average, CoKV outperforms the full KV cache in terms of average accuracy. This demonstrates that CoKV not only reduces computational costs but also improves inference performance by identifying which heads benefit from cache retention and which may have a detrimental effect. Additionally, we evaluate all methods within the token range of 1k to 31k in the Needle-in-a-Haystack test, where CoKV also demonstrated the best retrieval capability.



\section{Preliminaries}
\subsection{Key-Value Caching and Compression} 
In Multi-Head Attention (MHA), for each attention head $h_i$ in one layer, the embedded input $X = \{x_1, x_2, \dots, x_m\} \in \mathbb{R}^{m \times d_{\text{model}}}$ of $m$ tokens is mapped into different subspaces using query $W^Q_i$, key $W^K_i$, and value $W^V_i \in \mathbb{R}^{d_{\text{model}} \times d_h}$ matrices:
\begin{align}\nonumber
    Q_i = X W^Q_i, K_i = X W^K_i, V_i = X W^V_i \in \mathbb{R}^{m \times d_h}
\end{align}
where $d_h$ is the dimension of attention heads, $d_h=d/{\tau}$, and $\tau$ is the number of heads in one layer.

All the computed $KV$ for the input sequence are cached to avoid recalculating them during the subsequent decoding stages. Assume there is a new input token $x\in \mathbb{R}^{1 \times d_{\text{model}}}$, then it will be mapped to a new query, key, and value as follows,
\begin{align}\nonumber
    q_i = x W^Q_i, k_i = x W^K_i, v_i = x W^V_i \in \mathbb{R}^{1 \times d_h}.
\end{align}

The KV cache is updated by adding the new key and value pair
$$K_i = \operatorname{Cat}[K_i,k_i], V_h=\operatorname{Cat}[V_i,v_i].$$
The attention output is computed as follows,
$$O_i=A_iV_i$$ where $A_i=\operatorname{softmax}(q_iK_i^T/\sqrt{d_h})$. The final output $y \in \mathbb{R}^{1\times d_{\text{model}}}$ is obtained through a linear transformation $$
y=\operatorname{Cat}[O_1,\cdots,O_\tau]W^O$$ where $W^O\in \mathbb{R}^{d \times d_{\text{model}}}$ output weight matrix.

Furthermore, KV cache eviction methods can be employed to discard unimportant KV cache entries while preserving performance. For each head $h_i$, the compressed KV cache is reduced to $\hat{K}_i \in \mathbb{R}^{s \times d_h}$ and $\hat{V}_i \in \mathbb{R}^{s \times d_h}$, where some unimportant KV pairs are evicted and $s \ll m$, resulting in a significant improvement in computational efficiency and memory usage. Specifically, the compressed KV cache is updated by appending the new key and value pair:
   \[
   \hat{K}_i = \text{Cat}[\hat{K}_i, k_i], \quad \hat{V}_i = \text{Cat}[\hat{V}_i, v_i].
   \]
The attention output for each head \( h_i \) is computed using the compressed KV cache:
   \[
   \hat{O}
   _i = \hat{A}_i \hat{V}_i,
   \]
   where the attention weights \( A_i \) are calculated as:
   $\hat{A}_i = \text{softmax}(q_i \hat{K}_i^T/\sqrt{d_h}).$



\subsection{Shapley Value}

Consider a set of players $\mathcal{N}=\{p_1,\ldots,p_n\}$. A \emph{coalition} $\mathcal{S}$ is a subset of $\mathcal{N}$ that cooperates to complete a task. A utility function $\mathcal{U}(\mathcal{S})$ $(\mathcal{S} \subseteq \mathcal{N})$ is the utility of coalition $\mathcal{S}$ for the task. The marginal contribution of player $p_i$ with respect to a coalition $\mathcal{S}$ is $\mathcal{U}(\mathcal{S}\cup \{p_i\})-\mathcal{U}(\mathcal{S})$. 
The Shapley value measures the expectation of marginal contribution of player $p_i$ in all possible coalitions. That is
\begin{equation}\label{equ:SV}
  \mathcal{SV}_i=\frac{1}{n} \sum_{\mathcal{S}\subseteq \mathcal{N} \setminus \{p_i\}}\frac{\mathcal{U}(\mathcal{S}\cup \{p_i\})-\mathcal{U}(\mathcal{S})}{\binom{n-1}{|\mathcal{S}|}}.
\end{equation} 
According to Equation~\ref{equ:SV}, it is evident that computing the exact Shapley value requires enumerating the utilities for all possible subsets of players and each marginal contribution can only be used to update the Shapley value of a single player. Therefore, the computational complexity of exactly calculating the Shapley value is exponential. Recently, the Shapley value of player $p_i$ is proven to be equal to the weighted complementary contributions~\cite{cc} as follows,
\begin{equation}\label{equ:svs}
    \mathcal{SV}_i = \frac{1}{n}\sum_{\mathcal{S}\subseteq \mathcal{N}\setminus\{p_i\}}\frac{\mathcal{U}(\mathcal{S})-\mathcal{U}(\mathcal{N} \setminus\mathcal{S})}{\binom{n-1}{|\mathcal{S}|}}.
\end{equation}
$\mathcal{U}(\mathcal{S})-\mathcal{U}(\mathcal{N} \setminus\mathcal{S})$ is called complementary contribution which has an advantage that can be reused to update Shapley value estimation for all players in $\mathcal{S}$. In the context of KV caches, attention heads are treated as players for evaluating their importance to each specific task. $\mathcal{U}(\mathcal{S})$ is defined as the model accuracy when the attention heads in $\mathcal{N}\setminus{\mathcal{S}}$ are masked, we retain only the KV pairs within the local window for masked heads.


\section{Method}
Our method consists of two phases. First, we precompute the importance scores for each attention head. Second, these scores are utilized for KV cache compression during inference. The overview of our approach is illustrated in Figure~\ref{fig:overview}.

 \begin{figure*}[htbp] 
    \centering
    \includegraphics[width=1\textwidth]{pics/overview.pdf} 
    % \caption{ The figure shows how the contribution of each head is evaluated by sampling sets of masked heads and calculating complementary contributions. The average complementary contribution of each head is then used to assess its importance.}
    \caption{Overview of our proposed method:
    (1) \textbf{Head Importance Evaluation (Upper Part):} For a 4-layer × 4-head model, We measure head importance using the Sliced Shapley Value (SSV). To approximate SSV, we sample $M$ different sets of masked heads and compute their complementary contributions. The average complementary contribution of each head is its estimated SSV.
    % The average complementary contribution of each head, termed Sliced Shapley Value (SSV), quantifies its importance.
    (2) \textbf{KV Cache Compression (Lower Part):} Using the 4 heads in Layer 3 as an example, all heads store KV pairs for a small local window of recent tokens, while heads with higher SSV (darker in the heatmap) are allocated more cache size to retain KV pairs before the local window.
    }  
    \label{fig:overview}
\end{figure*}
\subsection{Head Importance Evaluation}
Although the complementary contribution helps in increasing efficiency when approximating the Shapley value, it is still computationally costly, especially in the LLM setting. Given a set of players $\mathcal{N}=\{p_1,\ldots,p_n\}$, a coalition of $j$ players $(1 \leq j \leq n)$ is called a \textit{$j$-coalition}. Moreover, for a player $p_i$ $(1\leq i \leq n)$, a $j$-coalition that contains $p_i$ is called a \textit{$(i, j)$-coalition}. Denote by $\mathfrak{S}_{i,j} = \{\mathcal{S}\cup\{p_i\}|\mathcal{S}\subseteq\mathcal{N}\setminus\{p_i\},|\mathcal{S}|=j-1\}$ the set of $(i, j)$-coalitions, and by $\mathcal{SV}_{i,j}$ \textit{the expected complementary contributions of} $(i, j)$-coalitions. That is,
$$\mathcal{SV}_{i,j} = \sum_{\mathcal{S} \in \mathfrak{S}_{i,j}} \frac{\mathcal{U}(\mathcal{S})-\mathcal{U}(\mathcal{N}\setminus\mathcal{S})}{\binom{n-1}{j-1}}.$$
It is clear that $\mathcal{SV}_i = \frac{1}{n}\sum_{j=1}^{n}\mathcal{SV}_{i, j}$. Computing the Shapley value needs to calculate $\mathcal{SV}_{i,j}$ for $j$ ranging from 1 to $n$, which becomes costly when $n$ is large. 

We observe that the expected complementary contributions of $j$-coalitions for heads in LLMs follow a similar distribution across different $j$ values, as shown in Appendix Section~\ref{sec:coalition_distribution}. This suggests that the contributions of heads can be effectively captured using a subset of $j$-coalitions.  Based on this insight, we propose assessing the importance of heads using the expected complementary contribution of several $j$-coalitions, which can significantly reduce the computation cost while maintaining effectiveness. Formally, we introduce a new definition called the Sliced Shapley value as follows.

\begin{definition}
\normalfont \textbf{(Sliced Shapley Value)} Let $\mathcal{H} \subseteq \{1,\cdots,n\}$ denote the selected set of $j$-coalitions, representing a specific slice of the coalition size space. The \emph{Sliced Shapley value} of head $h_i$ with respect to $\mathcal{H}$ is defined as:
\[
\mathcal{SSV}_i^{\mathcal{H}} = \frac{1}{|\mathcal{H}|} \sum_{j=1}^{n} \mathcal{SV}_{i,j} \cdot \mathbb{I}_j^{|\mathcal{H}|},
\]
where $\mathbb{I}_j^{\mathcal{H}}$ is an indicator function, which is 1 if $j$ is the element in $\mathcal{H}$ and 0 otherwise. 
\end{definition}

\paragraph{Algorithm Description.}The detailed steps of approximating $\mathcal{SSV}_i^{\mathcal{H}}$ are shown in Algorithm~\ref{Alg:approSV}. In each iteration, sample a random permutation \( \pi^k \) of the heads \( \{h_1, \dots, h_n\} \), which defines a random ordering of the heads. Randomly select a split point and create a set \( \mathcal{S} \) of selected heads. Mask heads in the set \( \mathcal{N} \setminus \mathcal{S} \), and evaluate the model accuracy after masking, which is denoted as \( \mathcal{U}(\mathcal{S}) \). Similarly, calculate $\mathcal{U}(\mathcal{N} \setminus \mathcal{S})$ by masking heads in $\mathcal{S}$ (Lines 3-6). For each head in $\mathcal{S}$, update \( \mathcal{SV}_{\pi^k(j), i} \) and count matrix \( m_{\pi^k(j), i} \) (Lines 7-10). After $\mathcal{M}$ iterations are completed, calculate the approximated Sliced Shapley value for each head by averaging the complementary contributions. 



\begin{Theorem}\label{theorem:SSV_sampling}
    Algorithm~\ref{Alg:approSV} returns an $(\epsilon,\delta)$-approximation of Sliced Shapley value with time complexity $\mathcal{O}( \frac{T|\mathcal{H}|ln\frac{2|\mathcal{H}|}{\delta}}{\epsilon^2})$ where T is the time cost of evaluating a complementary contribution which is the time to inference on the validation dataset of each task in our setting. In contrast, Shapley value requires the time complexity of $\mathcal{O}( \frac{Tnln\frac{2n}{\delta}}{\epsilon^2})$ to achieve an $(\epsilon,\delta)$-approximation. The proof is provided in Appendix Section~\ref{sec:proof}.
\end{Theorem}{} 




\begin{algorithm}[t] \caption{Evaluating Head Importance in LLMs.}\label{Alg:approSV}
\SetKwInOut{Input}{input}\SetKwInOut{Output}{output}

\Input{Heads $\mathcal{N} = \{h_1, \ldots, h_n\}$ and sampling number $\mathcal{M} >0$}
\Output{approximate Sliced Shapley value $\overline{\mathcal{SSV}^\mathcal{H}_i}$ for each head $h_i$ $(1 \leq i \leq n)$}

    $\overline{\mathcal{SV}_i^{\mathcal{H}}} \gets 0$ $(1 \leq i \leq n)$;
    $\overline{\mathcal{SV}_{i,j}},m_{i,j} \gets 0$ $(1 \leq i,j \leq n)$;

    \For{k=1 to $\mathcal{M}$}{
        let $\pi^k$ be a random permutation of $\{1, \ldots, n\}$;
        
        let $i$ be a randomly selected element from the set $\mathcal{H}$;
        
        $\mathcal{S} \gets \{\pi^k(1), \ldots,\pi^k(i)\}$;
        
        $\mathcal{N}\setminus\mathcal{S} \gets \{\pi^k(i+1), \ldots, \pi^k(n)\}$;\\
        {\scriptsize \tcp{ $\mathcal{U}(\mathcal{S})$ is the model performance when heads in $\mathcal{N} \setminus \mathcal{S}$ are masked and vice versa for $\mathcal{U}(\mathcal{N} \setminus \mathcal{S})$.}}
        
        $u \gets \mathcal{U}(\mathcal{S})-\mathcal{U}(\mathcal{N}\setminus\mathcal{S})$;\\
        
        \For{j=1 to i}{
            $\overline{\mathcal{SV}_{\pi^k(j),i}}+=u$;
            
            $m_{\pi^k(j),i} += 1$;
        }
    }
    \For{i = 1 to $n$}{
        $\overline{\mathcal{SSV}_{i}^{\mathcal{H}}}=\frac{1}{\mathcal{H}}\sum_{j=1}^{n} \overline{\mathcal{SV}_{i,j}} / m_{i, j}$\;
    }
    \Return $\overline{\mathcal{SSV}_1^{\mathcal{H}}}, \ldots, \overline{\mathcal{SSV}_n^{\mathcal{H}}}$.
\end{algorithm}




\subsection{KV Cache Compression}

Existing KV cache compression methods have partially addressed the importance of layers, yet this consideration remains insufficient during cache allocation. While AdaKV attempts to preserve tokens with larger attention weights across all heads when allocating cache size, it overlooks the varying importance of different attention heads. Conversely, HeadKV acknowledges the differential importance of attention heads but suffers from several limitations. First, its evaluation primarily relies on the retrieval capability of individual heads, incorporating only basic reasoning abilities that prove inadequate for more complex scenarios, such as few-shot learning. Second, it assesses each head in isolation, ignoring the discrepancy between a head's individual contribution and its collaborative importance when working in conjunction with other heads. Our proposed method addresses these limitations by introducing a SSV-based scoring mechanism, which evaluates each head's importance based on its collaborative contribution to the task. This approach offers a more comprehensive and accurate representation of each head's significance in the overall model inference process.

\paragraph{Budget Allocation.}
An intuitive approach suggests that the least important heads, which contribute minimally or even negatively to the model performance, may not require cache allocation. Let \( \alpha \) represent the number of such heads, which serves as the sole hyperparameter in our allocation scheme. For the remaining \( n - \alpha \) heads, we employ a 
normalization method to normalize their importance scores and allocate the cache size proportionally based on their normalized scores.

Specifically, we normalize their contributions using min-max normalization for the \( n - \alpha\) heads:
\[
\mathcal{NSV}_i^{\mathcal{H}} = \frac{\mathcal{SSV}_i^{\mathcal{H}} - \min^{\alpha}(\mathcal{SSV}^{\mathcal{H}})}{\max(\mathcal{SSV}^{\mathcal{H}}) - \min^{\alpha}(\mathcal{SSV}^{\mathcal{H}})},
\]
where \( \min^{\alpha}(\cdot) \) and \( \max(\cdot) \) extract the $\alpha$-th smallest and maximum value, respectively. For the $\alpha$ heads with the smallest Sliced Shapley values, we set the normalized score as 0. This ensures that all normalized scores lie in the range \([0, 1]\).

Next, the cache size \( c_i \) allocated to head \( h_i \) is determined by the local window size $s$ and linearly distributing the remaining shared cache size \( B \) based on the normalized scores:

\begin{equation}\label{equ:alloc}
    c_i = B \cdot \frac{\mathcal{NSV}_i^{\mathcal{H}}}{\sum_{j=1}^{n} \mathcal{NSV}_j^{\mathcal{H}}}+s.
\end{equation}


\paragraph{Algorithm Description.}First, we allocate the KV cache size for each head based on their normalized Sliced Shapley values. Next, we rank the importance of KV pairs within each head using SnapKV. Specifically, the most recent tokens within local windows guide the KV cache selection. Attention scores from these local windows to the remaining tokens are aggregated via pooling, with higher-scoring tokens retained in the cache for each head. The detailed eviction steps for a single head are outlined in Algorithm~\ref{Alg:CoKV}.

\begin{algorithm}[t] \caption{Token Eviction Using CoKV.}\label{Alg:CoKV}
\SetKwInOut{Input}{input}\SetKwInOut{Output}{output}

\Input{Shared budget size $B$, local window size $s$, tokens in local window $X^{win}\in \mathbb{R}^{s \times d}$, KV in local window $\{K_i^{win},V_i^{win}\}$, KV outside local window $\{K_i^{out},V_i^{out}\}$}
\Output{Retained KV Cache $\{\hat{K}_i,\hat{V}_i\}$}
    $Q_i^{win} = X^{win}W_i^Q$;\\
    {\scriptsize \tcp{Compute attention weights of queries in local window and prefix Keys.}}
    $\overline{A}_i=\operatorname{softmax}(Q_i^{win}K_i^T)$;\\
    $\overline{A}_i=\overline{A}_i.maxpooling(dim=1).mean(dim=0)$;\\
    {\scriptsize \tcp{Calculate token scores outside the local window.}}
    Get $c_i$ using Algorithm~\ref{Alg:approSV} and Equation~\ref{equ:alloc};\\
    $indices=\overline{A}_i.topk(c_i).indices$;\\
    Select $\{\hat{K}_i,\hat{V}_i\}$ from $\{K_i^{out},V_i^{out}\}$ according $indices$;\\
    $\{\hat{K}_i,\hat{V}_i\}=$  Cat($\{\hat{K}_i,\hat{V}_i\},\{K_i^{win},V_i^{win}\}$);\\
    {\scriptsize \tcp{Keep top $c_i$ KV pairs in the cache.}}
    \Return Retained KV Cache $\{\hat{K}_i,\hat{V}_i\}$.
\end{algorithm}

\section{Experiments}

\subsection{Experiment Settings}
\paragraph{Datasets.}LongBench 
is a multitask benchmark for long context understanding and exhibits a wide range of average input lengths, spanning from 1,235 to 18,409 tokens.

\paragraph{Baselines and Settings.}
We compare CoKV with four strong KV cache compression methods. All methods keep the same total cache size for fair comparison. Besides, we implement all methods with GQA~\cite{gqa} and FlashAttention~\cite{flashattention} for efficient computation.
\begin{itemize}[leftmargin=*, itemsep=0pt, parsep=0pt]
\item \textbf{SnapKV}~\cite{SnapKV} uses the last several tokens as local windows to guide KV cache selection. Attention scores from these windows to the remaining tokens are pooled to cluster and guide the selection process.
\item \textbf{PyramidKV}~\cite{PyramidKV} allocates more KV cache to lower layers to retain key information while reducing the budget for higher layers where information is already aggregated.
\item  \textbf{Ada-KV}~\cite{AdaKV} dynamically allocates budgets to heads within each layer based on their concentration degrees, and can be combined with SnapKV or PyramidKV. Ada-SnapKV is used as the baseline due to its superior performance over Ada-PyramidKV.
\item \textbf{HeadKV-R2}~\cite{HeadKV} allocate budgets to heads based on their retrieval-reasoning score, and it uses SnapKV to rank the importance of KV pairs in each head.
\end{itemize}

In CoKV, we allocate the KV cache size for each head based on the normalized Sliced Shapley value of $\mathcal{H}=\{32, 64, 96, 128\}$. Following HeadKV-R2, we set the local window size to 8, and randomly split each dataset into a validation dataset and a test dataset, with proportions of 15\% and 85\%, respectively. The hyperparameter $\alpha$ is selected from the set $\{1, 5, 10, 15, 20, 30, 40\}$. The validation dataset is used to compute Sliced Shapley value and determine the optimal $\alpha$ for each task. We evaluate CoKV on the Llama-3-8B-Instruct and Mistral-7B-Instruct-v0.2 models. Due to the page limit, the Mistral-7B-Instruct-v0.2 results are provided in \nameref{sec:appendix}.  For test data that exceeds the maximum input length of Llama-3-8B-Instruct, we adopt the approach of HeadKV by utilizing the first 4k tokens and the last 4k tokens.  Following standard practices in prior studies~\cite{AdaKV,HeadKV}, we perform cache eviction after the prefilling phase of each layer for consistent comparison. In GQA, a group of 4 heads shares the same KV cache. We treat each cache within a group as a player in a cooperative game, evaluating their Sliced Shapley value to determine their importance scores. For HeadKV-R2, we calculate the importance score of each group by averaging the retrieval-reasoning scores of the 4 heads within the group. This adaptation ensures compatibility with GQA, as HeadKV is implemented with MHA in the original paper. For the efficiency and computation cost analysis of Sliced Shapley value, please refer to Appendix Section~\ref{section:compute_efficiency}. For the test in Needle-in-a-Haystack, please refer to Appendix Section~\ref{sec:needle_test}.

\subsection{Main Results}
\paragraph{Benchmark Results.} 
The complete benchmark results are presented in Tables~\ref{tab:llama_results} and~\ref{tab:mistral_results} in the appendix. We include a simplified table (Table~\ref{tab:short_llama_results}), showing the performance of Llama-3-8B-Instruct when keeping 64-128 KV pairs on average.
\subimport{table_and_figure_tex}{short_benchmark_llama_table}
The results demonstrate that CoKV consistently outperforms all baseline methods. The average accuracy of the two models on 16 datasets are presented in Figure ~\ref{fig:Llama_Mistral_Longbench}. 
 \begin{figure}[h] 
    \centering
    \includegraphics[width=0.4\textwidth]{pics/Llama_LongBench.png}
    \includegraphics[width=0.4\textwidth]{pics/Mistral_LongBench.png} 
    \caption{Results for varying KV cache sizes (64, 128, 256, 512, 1024), showing the average accuracy across 16 datasets from the LongBench benchmark.}
    \label{fig:Llama_Mistral_Longbench}
\end{figure}
Notably, in Llama-3-8B-Instruct, with an average of 128 tokens cached per group KV cache, CoKV retains 97.29\% of the model performance. Furthermore, CoKV significantly surpasses FullKV when it maintains an average of over 512 KV pairs per group cache. When retains an average of 1024 KV, the average results of both models outperform FullKV. This demonstrates that CoKV achieves near-lossless performance under resource-constrained settings. The superior performance of CoKV arises from its ability to effectively evaluate the importance of each cache within a group while considering the cooperation among all groups. It is not only capable of identifying which groups are important but also able to recognize those groups that do not contribute or even have a negative contribution. By optimizing the cache size to enhance overall cooperation, CoKV ensures efficient and high-quality inference.



\paragraph{Hyperparameter Free Results.}\label{experiment:mask} Since both HeadKV-R2 and CoKV provide importance scores for each group, we conduct an experiment to compare their effectiveness without introducing any additional hyperparameters. In this experiment, we mask the caches of groups based on the importance scores assigned by each algorithm. Specifically, we mask the caches of both the highest-ranked (\textbf{top}) and lowest-ranked groups (\textbf{low}). The complete results are shown in Tables~\ref{tab:mask_llama} and~\ref{tab:mask_mistral} in the appendix. We include a simplified table for the results of masking 16,128 groups of Llama-3-8B-Instruct model in Table~\ref{tab:short_mask_llama}. 
\subimport{table_and_figure_tex}{short_mask_llama_table}
The results show that when masking the top-ranked groups identified by each method, the performance of CoKV degrades more significantly than that of HeadKV-R2. Conversely, when masking the unimportant groups (\textbf{low}), the performance of CoKV declines more gradually than HeadKV-R2. This suggests that CoKV is more effective at ranking group importance, as it better distinguishes between critical and non-critical caches. The results of masking 16 groups in both models outperformed the FullKV approach as shown in Figure~\ref{fig:mask_llama_mistral_average}. 
\begin{figure}[h] 
    \centering
    \includegraphics[width=0.4\textwidth]{pics/mask_llama.png} 
    \includegraphics[width=0.4\textwidth]{pics/mask_mistral.png} 
    \caption{Results for varying masked groups (16,32,64,96,128), showing the average accuracy across 16 datasets from the LongBench benchmark.}
    \label{fig:mask_llama_mistral_average}
\end{figure}
This further demonstrates that CoKV can identify groups that have a negative impact on the model. By removing the KV pairs from these groups, the model inference not only optimizes storage and decoding speed but also enhances overall performance.


\subsection{Decoding Latency and Memory Usage}
We conduct experiments using the Mistral-7B-Instruct-v0.2 model, which supports a maximum context window of 32k tokens, with FlashAttention enabled as the default setting, on an A100 GPU with 40GB of memory. We design two key experiments with the average KV cache size set to 128 tokens(comparative experiments showed less than 2\% variation across 64/256/512/1024 tokens).

\begin{figure}[h]
    \centering
    \includegraphics[width=0.4\textwidth]{pics/decoding_latency.png} \\[1em] % 第一张图片
    \includegraphics[width=0.4\textwidth]{pics/peak_memory_usage.png} % 第二张图片
    \caption{Results of Decoding Latency and Peak Memory Usage, demonstrating that CoKV maintains comparable performance with other baseline methods while achieving significant improvements over FullKV.}
    \label{fig:decoding}
\end{figure}

\paragraph{Decoding Latency} With a fixed input context length of 28k tokens, we measure decoding latency (including both the pre-filling time and the decoding time)  across different generation lengths (1/512/1024/2048/4096 tokens). As shown in the Decoding Latency of Figure~\ref{fig:decoding},  CoKV achieves less than 50\% of the total latency compared to the FullKV baseline, with negligible differences observed between the other baselines.

\paragraph{Peak Memory Usage} Under fixed generation length (1 token), we measure the peak GPU memory usage (including model parameters and runtime states) across varying input contexts (1k/2k/4k/8k/16k/32k tokens). As shown in the Peak Memory Usage of Figure~\ref{fig:decoding}, CoKV reduces memory usage by 64\% compared to FullKV baseline at 32k input length.




\section{Conclusion}
Large language models (LLMs) face significant challenges in handling long texts due to the excessive memory and latency overhead caused by the growing size of the KV cache. To this end, we introduce the Sliced Shapley value (SSV) to evaluate the collaborative importance of attention heads and a novel method called  CoKV to dynamically allocate cache sizes based on SSV. Our experimental results demonstrate that CoKV achieves state-of-the-art performance across 16 LongBench datasets, outperforming the full KV cache in 9 datasets while reducing memory and latency overhead. CoKV provides a scalable and practical solution for enhancing the efficiency of LLMs in real-world applications.

\section*{Limitations}
Our work has two main limitations that suggest future research directions:

\paragraph{Task-specific constraint:}CoKV requires calculating head importance scores for different tasks. While experiments in Appendix Section~\ref{section:generalization} demonstrate strong generalizability across datasets within the same task category. Despite this constraint, CoKV is highly practical for LLM providers serving diverse users. Users can simply select their task type, and the model will apply the corresponding head importance scores for KV cache compression. Importantly, the underlying inference process remains consistent across all tasks; only the cache budget allocation varies based on the task-specific importance scores. This ensures both flexibility and efficiency, enabling the model to adapt to various user needs without requiring significant changes to its core architecture.

\paragraph{Precomputation cost:}The computation of importance based on cooperative game theory for attention heads is computationally intensive. Although we propose the Sliced Shapley Value (SSV), which significantly reduces the computational cost, our precomputation overhead remains higher than that of baseline methods. However, our experiments in Appendix Section~\ref{section:compute_efficiency} demonstrate that this precomputation is still entirely acceptable. We plan to address optimizing computational complexity as one of our future research directions by developing efficient approximation algorithms and parallel computing strategies.

\bibliography{custom}

\clearpage

\appendix

\section*{Appendix}

\label{sec:appendix}



\section{Related Works}
% FastGen ~\cite{FastGen} identifies the optimal KV cache compression policies and then dynamically evicts tokens during generation.

\paragraph{KV Cache Compression} The memory overhead of storing key-value (KV) pairs for LLM has motivated extensive research on KV cache compression. StreamingLLM ~\cite{StreamingLLM} preserves the initial and recent tokens, which empirically exhibit higher informativeness during generation. Similarly, Scissorhands ~\cite{scissorhands} proposes the persistence of importance to identify and retain pivotal tokens. H2O ~\cite{H2O} employs a heavy-hitter oracle to drop tokens with low attention scores.  SnapKV ~\cite{SnapKV} uses the attention scores of the recent tokens to retain critical tokens. While these methods reduce memory usage and accelerate inference, they implicitly assume uniform importance across attention heads, limiting their applicability.  Recent works address head diversity through layer-wise and head-wise optimizations. PyramidKV~\cite{PyramidKV} implements a hierarchical allocation strategy, assigning larger cache budgets to lower layers based on the observed attention patterns across layers. FastGen~\cite{FastGen} is an adaptive KV cache compression method that reduces LLMs' memory usage by profiling attention modules and constructing caches adaptively. RazorAttention ~\cite{Razorattention} and Duoattention~\cite{Duoattention} divide attention heads into retrieval heads(critical for long-context processing~\cite{retrievalheads}) and non-retrieval heads, apply full KV cache to retrieval heads and compressed KV cache to non-retrieval heads. ArkVale~\cite{chen2025arkvale} proposes a page-based KV cache manager that asynchronously copies filled pages into external memory (e.g., CPU memory) as a backup and supports the recall of important tokens that were previously evicted. AdaKV ~\cite{AdaKV} dynamically adjusts cache budgets across heads based on their concentration degrees and HeadKV ~\cite{HeadKV} calculates head importance scores to allocate cache budget before inference.
However, these methods assess heads in isolation, neglecting their collaborative interactions. For example, the standalone score of a head may not reflect its true contribution when working synergistically with others. Additionally, these approaches overlook the task-dependent variations in head importance. 
Our approach tackles these limitations by modeling head interactions as a cooperative game, dynamically allocating cache resources based on the varying complementary contributions of heads across different tasks.

In addition to KV cache eviction methods, KV cache quantization is also one of the mainstream approaches for KV cache compression~\cite{yang2024no,liukivi}. However, while eviction methods can be used to retain less than 1\% of the cache, KV cache compression cannot be applied to such an extent because it must preserve at least 1 bit. Nevertheless, the combination of these two methods is an interesting direction for future research.

\paragraph{Model Architecture and Computation Optimization}
Modern LLMs employ architectural optimizations to balance efficiency and performance. Multi Query Attention (MQA) ~\cite{mqa} shares a single key-value pair across all attention heads, drastically reducing memory usage but potentially sacrificing performance. Group Query Attention (GQA) ~\cite{gqa} strikes a balance by grouping heads to share key-value pairs, preserving performance while maintaining memory efficiency, which is widely adopted in recent LLMs like Llama~\cite{llama3} and Mistral~\cite{mistral}. Concurrently, Flash Attention ~\cite{flashattention} optimizes hardware utilization by minimizing memory reads/writes during attention computation, significantly accelerating inference. Notably, our approach is fully compatible with GQA and Flash Attention, ensuring seamless integration with current LLMs.

\paragraph{Cooperative Game Theory } Cooperative game theory offers a principled framework for understanding how multiple players can jointly contribute to overall system performance. Shapley value ~\cite{shapley}, a classic solution in cooperative game theory, provides a method for fairly allocating joint benefits based on the marginal contribution of each player. However, traditional Shapley value computation methods allow each sample to be used to calculate the marginal contribution of only a single player.
Recent works ~\cite{cc,sun2024shapley} address this limitation through complementary contributions that enable simultaneous estimation of multiple players' contributions. In the context of LLMs, these methods still encounter scalability issues, as the cost of computing complementary contributions across all coalition sizes remains prohibitively high.
We propose the Sliced Shapley value, which samples only a subset of coalition sizes. This approach not only accelerates the computation but also accurately reflects the contributions of different heads.
\section{Supplementary experiments}

We introduce the detailed information of LongBench in Table~\ref{tab:longbench_metric}, including the task types, evaluation metrics, average length, languages, and the number of samples for each task.
\subimport{table_and_figure_tex}{longbench_metric_table}.

\subsection{Computation Efficiency}
\label{section:compute_efficiency}
We conduct experiments to demonstrate the efficiency of approximating the Sliced Shapley value using the \emph{qasper} dataset with the Llama-3-8B-Instruct model. We randomly select 15\% of the \emph{qasper} dataset as the validation set to compute the Sliced Shapley value. The experiments are performed on a server equipped with 8 RTX 3090 GPUs. We compute the Sliced Shapley value for coalition sizes of $\{32, 64, 96, 128\}$. GPUs 0-3 are assigned to compute the complementary contributions for coalitions of sizes $\{32, 64, 96, 128\}$, respectively, while GPUs 4-7 compute another independent Sliced Shapley value. Table~\ref{table:qasper_mse} shows the computation time for each GPU from 50 to 500 samples of complementary contributions, as well as the mean absolute error (MAE) between the two independently computed Sliced Shapley values. The MAE is calculated as:
\[
MAE = \frac{\sum_{i=1}^n |\overline{\mathcal{SSV}}_i^{\mathcal{H}} - \overline{\mathcal{SSV}}_i^{\mathcal{H}'}|}{n},
\]
where $\overline{\mathcal{SSV}}_i^{\mathcal{H}}$ and $\overline{\mathcal{SSV}}_i^{\mathcal{H}'}$ represent the Sliced Shapley values from the two independent computations. The experimental results show that when the number of samples reaches 250 for each coalition size, the MAE is $3.8e-3 \leq 1/256$ with 20.93 hours. In GQA inference, the Llama-3-8B-Instruct model has a total of $32 \times 8 = 256$ groups. Since the model accuracy lies in the range $[0, 1]$, when the MAE between two sampling runs is less than $1/256$, the sum of absolute errors across all groups is less than 1. At this point, the Sliced Shapley value can reliably reflect the contributions of the groups. 

We recommend performing two independent sampling runs when computing the Sliced Shapley value for a task. The sampling results are considered stable when the mean absolute error between the two runs is less than $1/n$, where $n$ represents the number of players in the cooperative game. At this point, the results from the two sampling runs can be averaged and used as the importance scores of the heads in the model.

\subsection{Distribution of Sliced Shapley Value}
Figures~\ref{fig:heatmap_llama} and~\ref{fig:heatmap_mistral} illustrate the distribution of the Sliced Shapley values computed for selected coalition sizes 
$H=\{32,64,96,128\}$ in our experiment. We observe that the distributions of Sliced Shapley values exhibit significant differences across datasets of different task categories, while showing relatively smaller variations within datasets of the same domain type.


\subsection{Distribution of j-coalition Complementary Contribution}\label{sec:coalition_distribution}
In Figures~\ref{fig:heatmap_lcc_coalition} and~\ref{fig:heatmap_hotpotqa_coalition}, we present the distributions of the expected complementary contributions of heads in Llama-3-8B-Instruct model on the \emph{hotpotqa} dataset (multi-document question answering) and the \emph{lcc} dataset (code generation), with coalition sizes of 
$\{32,64,96,128,160,192,224\}$. We observe strong correlations in the distributions across all coalition sizes. Additionally, the distributions of the expected complementary contributions for coalition sizes $S$ and $n-|S|$ are nearly identical, exhibiting symmetry around the size of 128. To optimize computational efficiency, we restrict the calculation of complementary contributions to coalitions with sizes below 128. These observations provide a justification for our approach of computing complementary contributions using only a small subset of coalition sizes, as it effectively captures the contributions of the heads.

\subsection{Generalization}
\label{section:generalization}
To validate the generalization capability of our method, we conduct cross-dataset evaluations on two task categories: 1. Multi-Document QA including 2WikiMQA and Musique datasets. 2. Code Processing including Lcc and RB-P datasets.

Following Section 4.2, we mask top and low-ranked attention heads but cross-apply head importance scores between datasets within the same task (e.g., mask 2WikiMQA using Musique-derived scores). As shown in Table~\ref{tab:generalization_llama} and Table~\ref{tab:generalization_mistral}, our method maintains superior accuracy over baselines across both models, confirming that learned importance scores can generalize across datasets within shared task domains.

\subsection{Needle-in-a-Haystack Test}\label{sec:needle_test}
To evaluate the performance of different KV cache compression methods in long-context retrieval tasks, we conduct a Needle-in-a-Haystack benchmark test using the Mistral-7B-v0.2 model. With the average KV cache size 128, we systematically insert target texts (needles) at ten equidistant positions (11\%, 22\%, ..., 100\%) across varying context lengths ranging from 1,000 to 31,000 tokens (in 1,000-token increments). Experimental results demonstrate that CoKV outperforms other baseline methods, achieving an average score of 95.89\% - the closest performance to the uncompressed FullKV benchmark.



\section{Proof}\label{sec:proof}
\subimport{proofs_tex}{proof_unbiased_and_error}



\subimport{table_and_figure_tex/}{benchmark_llama_table}
\subimport{table_and_figure_tex/}{benchmark_mistral_table}
\subimport{table_and_figure_tex}{mask_llama_table}
\subimport{table_and_figure_tex/}{mask_mistral_table}
\subimport{table_and_figure_tex/}{qasper_sv_efficieny_table}
\subimport{table_and_figure_tex/}{heatmap_llama}
\subimport{table_and_figure_tex/}{heatmap_mistral}
\subimport{table_and_figure_tex/}{coalition_size_heatmap}
\subimport{table_and_figure_tex/}{generalization_llama.tex}
\subimport{table_and_figure_tex/}{generalization_mistral.tex}
\subimport{table_and_figure_tex/}{niah.tex}

\end{document}
