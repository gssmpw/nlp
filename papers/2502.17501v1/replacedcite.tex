\section{Related Works}
% FastGen ____ identifies the optimal KV cache compression policies and then dynamically evicts tokens during generation.

\paragraph{KV Cache Compression} The memory overhead of storing key-value (KV) pairs for LLM has motivated extensive research on KV cache compression. StreamingLLM ____ preserves the initial and recent tokens, which empirically exhibit higher informativeness during generation. Similarly, Scissorhands ____ proposes the persistence of importance to identify and retain pivotal tokens. H2O ____ employs a heavy-hitter oracle to drop tokens with low attention scores.  SnapKV ____ uses the attention scores of the recent tokens to retain critical tokens. While these methods reduce memory usage and accelerate inference, they implicitly assume uniform importance across attention heads, limiting their applicability.  Recent works address head diversity through layer-wise and head-wise optimizations. PyramidKV____ implements a hierarchical allocation strategy, assigning larger cache budgets to lower layers based on the observed attention patterns across layers. FastGen____ is an adaptive KV cache compression method that reduces LLMs' memory usage by profiling attention modules and constructing caches adaptively. RazorAttention ____ and Duoattention____ divide attention heads into retrieval heads(critical for long-context processing____) and non-retrieval heads, apply full KV cache to retrieval heads and compressed KV cache to non-retrieval heads. ArkVale____ proposes a page-based KV cache manager that asynchronously copies filled pages into external memory (e.g., CPU memory) as a backup and supports the recall of important tokens that were previously evicted. AdaKV ____ dynamically adjusts cache budgets across heads based on their concentration degrees and HeadKV ____ calculates head importance scores to allocate cache budget before inference.
However, these methods assess heads in isolation, neglecting their collaborative interactions. For example, the standalone score of a head may not reflect its true contribution when working synergistically with others. Additionally, these approaches overlook the task-dependent variations in head importance. 
Our approach tackles these limitations by modeling head interactions as a cooperative game, dynamically allocating cache resources based on the varying complementary contributions of heads across different tasks.

In addition to KV cache eviction methods, KV cache quantization is also one of the mainstream approaches for KV cache compression____. However, while eviction methods can be used to retain less than 1\% of the cache, KV cache compression cannot be applied to such an extent because it must preserve at least 1 bit. Nevertheless, the combination of these two methods is an interesting direction for future research.

\paragraph{Model Architecture and Computation Optimization}
Modern LLMs employ architectural optimizations to balance efficiency and performance. Multi Query Attention (MQA) ____ shares a single key-value pair across all attention heads, drastically reducing memory usage but potentially sacrificing performance. Group Query Attention (GQA) ____ strikes a balance by grouping heads to share key-value pairs, preserving performance while maintaining memory efficiency, which is widely adopted in recent LLMs like Llama____ and Mistral____. Concurrently, Flash Attention ____ optimizes hardware utilization by minimizing memory reads/writes during attention computation, significantly accelerating inference. Notably, our approach is fully compatible with GQA and Flash Attention, ensuring seamless integration with current LLMs.

\paragraph{Cooperative Game Theory } Cooperative game theory offers a principled framework for understanding how multiple players can jointly contribute to overall system performance. Shapley value ____, a classic solution in cooperative game theory, provides a method for fairly allocating joint benefits based on the marginal contribution of each player. However, traditional Shapley value computation methods allow each sample to be used to calculate the marginal contribution of only a single player.
Recent works ____ address this limitation through complementary contributions that enable simultaneous estimation of multiple players' contributions. In the context of LLMs, these methods still encounter scalability issues, as the cost of computing complementary contributions across all coalition sizes remains prohibitively high.
We propose the Sliced Shapley value, which samples only a subset of coalition sizes. This approach not only accelerates the computation but also accurately reflects the contributions of different heads.