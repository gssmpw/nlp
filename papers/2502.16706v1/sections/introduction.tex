% \section{Introduction}

% Scaling inference efficiency is a fundamental challenge in large language models (LLMs). Many existing techniques improve inference by decomposing problems into smaller steps and systematically exploring different solutions ~\citep{feng2023alphazero, zeng2024scaling, wu2024inferencescalinglawsempirical, nori2024medprompto1explorationruntime, snell2024scaling, brown2024large, gandhi2024stream, lee2025evolvingdeeperllmthinking, anonymous2025scattered, anonymous2025planning}. 

% Some decomposition methods involve domain specific heuristics and hand-crafted rules ~\citep{yao2024tree, zelikman2023parsel, zhou2022least}. However, manually partitioning the problem or using domain specific heuristics is ungeneralizable and presents significant human costs. 
% Moreover, it is difficult for humans to understand what steps might be important for an LLM. As shown in the decomposition examples presented in Section \ref{sec:example_decomp}, LLMs often find single words steps (such as `therefore' and `which') important, which may be counterintuitive to humans but make great sense to LLMs trained autoregressively. 
% Other approaches involved fixed decompositions that apply uniform step sizes, such as token-level or sentence-level decomposition ~\citep{feng2023alphazero, guo2025deepseek}. All these methods rely on \textbf{static decomposition strategies}, where step sizes are predefined or determined using domain heuristics. This rigidity limits adaptability and wastes compute on  steps that are easy for the LLM (but perhaps difficult for humans) while undersampling difficult ones.

% To address these issues, we propose the \decomp (\underline{D}ynamic decomposition \underline{I}mproves \underline{S}caling \underline{C}ompute) recursive inference algorithm that dynamically partitions solutions steps based on difficulty. Unlike prior methods, \decomp \textbf{adapts decomposition granularity} during inference based on the inference budget and problem difficulty, ensuring finer granularity for harder steps. \decomp leverages the autoregressive nature of LLMs to quickly \textbf{locate difficult steps} through binary partitioning, allowing us to spend more compute on difficult steps instead of trivial steps. 
% \decomp is \emph{generalizable and does not require any human guidance or domain specific heuristics or adjustments.} This includes domain specific prompts, scaffolding, and process annotations. 

% To summarize, our main contributions are:
% \vspace{-0.1in}
% \begin{itemize}[label=\quad\textbullet]
%     \item We introduce a method to recursively partition and decompose a solution into steps during inference called \decomp without any human supervision, domain specific heuristics, prompt engineering, or a process reward model.
%     \item We show how \decomp combines decomposition with inference time search, \textbf{focusing compute on high impact, difficult steps}.
%     \item We demonstrate the performance gains that \decomp and decomposition bring to inference scaling, both in terms of sample efficiency and token efficiency.
%     \item We show how \decomp can lead to better understanding and interpretation of how LLMs reason and plan, and what steps are critical for the LLM.
% \end{itemize}

\section{Introduction}

Scaling inference efficiency remains a fundamental challenge for large language models (LLMs). Many existing approaches improve inference by decomposing problems into smaller steps and systematically exploring different solutions~\citep{feng2023alphazero, zeng2024scaling, wu2024inferencescalinglawsempirical, nori2024medprompto1explorationruntime, snell2024scaling, brown2024large, gandhi2024stream, lee2025evolvingdeeperllmthinking, light2024strategist, anonymous2025planning}. 

Some decomposition methods rely on domain-specific heuristics and hand-crafted rules~\citep{yao2024tree, zelikman2023parsel, zhou2022least}. However, manually partitioning problems or designing task-specific heuristics is costly and lacks generalization. 
Moreover, identifying critical steps for an LLM can be non-trivial for humans. As shown in Sec. 3.5, LLMs may assign importance to seemingly trivial words (e.g., therefore or which), which, while counterintuitive to humans, play a crucial role in autoregressive generation \citep{lin2025criticaltokens}.
% Moreover, humans may struggle to determine which steps are critical for an LLM. As shown in Sec.~\ref{sec:example_decomp}, LLMs often assign importance to seemingly trivial words (e.g., \texttt{therefore} or \texttt{which}), which may appear counterintuitive but are crucial in an autoregressive setting \citep{lin2025criticaltokens}. 
Other approaches employ fixed, uniform step sizes, such as token- or sentence-level decomposition~\citep{feng2023alphazero, guo2025deepseek}. All these methods rely on \textbf{static decomposition strategies}, where step sizes are predefined or determined via heuristics. Such rigidity wastes compute on steps that are easy for the LLM (but potentially difficult for humans) while undersampling more challenging steps.

To overcome these limitations, we propose \decomp (\underline{D}ynamic decomposition \underline{I}mproves \underline{S}caling \underline{C}ompute), a recursive inference algorithm that dynamically partitions solution steps based on difficulty. Unlike prior methods, \decomp \textbf{adapts decomposition granularity} during inference based on both the available budget and problem complexity, ensuring finer granularity for more difficult steps. By leveraging the autoregressive nature of LLMs, \decomp efficiently \textbf{locates difficult steps} through binary partitioning, focusing compute on challenging regions rather than wasting resources on trivial steps. \decomp is \emph{generalizable} and requires \emph{no human supervision, domain-specific heuristics, prompt engineering, or process annotations}, making it widely applicable across tasks.

Our main contributions are:
\vspace{-0.1in}
\begin{itemize}[leftmargin=*,itemsep=0pt,topsep=0pt,parsep=0.4pt, partopsep=0pt]%[label=\quad\textbullet]
    \item We introduce \decomp, a method for recursive partitioning and decomposing solutions during inference \emph{without human supervision, domain-specific heuristics, or process reward models}.
    \item We demonstrate how \decomp integrates decomposition with inference-time search, \textbf{allocating compute to high-impact, difficult steps}.
    \item We show that \decomp improves inference scaling in terms of both \textbf{sample efficiency} and \textbf{token efficiency}.
    \item We provide insights into how LLMs reason and plan by identifying \textbf{critical steps} in their generation process.
\end{itemize}




% Inference scaling methods often involve involve dividing the solution into smaller parts and systematically searching over these smaller parts ~\citep{feng2023alphazero, zeng2024scaling, wu2024inferencescalinglawsempirical, nori2024medprompto1explorationruntime, snell2024scaling, brown2024large, gandhi2024stream, anonymous2025scaling, lee2025evolvingdeeperllmthinking, anonymous2025scattered, anonymous2025planning}.  anonymous2025scaling, 
% % One main problem with current search methods is that it is difficult 
% Recent works have shown the difficulty of applying search and inference scaling techniques when taking token level steps ~\citep{guo2025deepseek}.

% \santiago{This paragraph is out of place.}




% \santiago{Shouldn't the word dynamic be in the acronym? The text in red can be improved. The sentence after the coma seems redundant. But I may be missing the point.}



% to dynamically decompose LLM solutions into textual steps and helps conduct inference scaling. 
% \decomp recursively divides difficult steps into smaller ones proportionally to the compute budget. 
% {\color{red}{It utilizes inference compute efficiency to both help decomposition and generate more possible solutions, leveraging the autoregressive nature of LLMs to both decompose and sample.}} 

% We leverage the autoregressive nature of LLMs to dynamically decompose the solution. 
