\vskip -0.1in
\section{Methodology}
\begin{figure}
\vspace{-0.2cm}
    \centering
    \includegraphics[width=0.72\columnwidth]{graphics/icmlsquare-01}
    \vskip -0.1in
    % \caption{\decomp conducts decomposition through \textbf{recursive binary partitioning} of the `hardest' step, i.e. the step with the lowest priority metric $h$. This process decomposes the optimal solution. }
    \caption{\decomp recursively partitions the most challenging step -- i.e., the one with the lowest priority metric $h$ -- to progressively refine and find the optimal solution.}
    \label{fig:recursive_partition}\vspace{-0.3cm}
\end{figure}


% \vskip -0.1in
\subsection{\decomp Algorithm}

The \decomp algorithm employs recursive binary decomposition to iteratively break down complex solutions into smaller, more manageable steps.
% The \decomp algorithm is a recursive binary decomposition approach that iteratively breaks down complex solutions into smaller, more manageable steps. 
Given a problem prompt $\boldsymbol{x}$, the algorithm outputs a decomposition of a solution, $\boldsymbol{y} = (\boldsymbol{y}_1, \boldsymbol{y}_2, \dots, \boldsymbol{y}_K)$, such that the concatenation $\boldsymbol{y}_{1...K}$ forms a complete solution to $\boldsymbol{x}$. 

% \yue{the notation is inconsistent from Section 2.1?}

The algorithm operates in two key stages:

\textbf{1. Solution sampling.} 
Starting from $\boldsymbol{x}$ and $\boldsymbol{y}_0 = \emptyset$, the algorithm generates complete solutions $\boldsymbol{y}_k \sim \pi(\cdot | \boldsymbol{x} \cdot \boldsymbol{y}_{1...(1-k)})$ using a policy $\pi$ like in single step generation. The best solution, $\boldsymbol{y}^*_k$, is selected based on the reward model $R$.

\textbf{2. Recursive partitioning.}
The selected solution $\boldsymbol{y}^*_k$ is partitioned into two segments, $\boldsymbol{y}^*_k = \boldsymbol{y}^*_a \cdot \boldsymbol{y}^*_b$, based on a predefined partition fraction $\alpha$, where $|\boldsymbol{y}^*_a| \approx \alpha |\boldsymbol{y}^*_k|$. For each part, a priority metric $h$ is estimated: $\hat{h}(\boldsymbol{y}^*_a | \boldsymbol{x} \cdot \boldsymbol{y}_{1...(k-1)})$ and $\hat{h}(\boldsymbol{y}^*_b | \boldsymbol{x} \cdot \boldsymbol{y}^*_{1...a})$, usually through rollouts of the step using $\pi$. The part with the lower priority is further partitioned.

% \yue{Better use $\hat{h}(\boldsymbol{x})$ and $\hat{h}(\boldsymbol{x} \cdot \boldsymbol{y}^*_1)?$}
% \jon{Yeah I was debating which one to use -- $y^*$ implies it is associated with a step but probably $h(x)$ is clearer}
% \jon{Actually we still need $\boldsymbol{y}^*_1$ in the first metric for Z-score based metrics}


\begin{itemize}[leftmargin=*,itemsep=0pt,topsep=0pt,parsep=0pt, partopsep=0pt]
    \item If $\hat{h}(\boldsymbol{y}^*_a | \boldsymbol{x} \cdot \boldsymbol{y}_{1...(k-1)}) \ge \hat{h}(\boldsymbol{y}^*_b | \boldsymbol{x} \cdot \boldsymbol{y}^*_{1...a})$, additional samples are sampled for $\boldsymbol{y}^*_b$, with the process repeating on the new best solution, $\boldsymbol{y}'^*_b$.
    \item Conversely, if $\hat{h}(\boldsymbol{y}^*_a | \boldsymbol{x}\cdot \boldsymbol{y}_{1...(k-1)}) < \hat{h}(\boldsymbol{y}^*_b | \boldsymbol{x} \cdot \boldsymbol{y}^*_{1...a})$, the first segment $\boldsymbol{y}^*_a$ is further partitioned. The first step corresponds to the $\alpha$ fraction of $\boldsymbol{y}'^*_a$, with the remaining part of the full solution forming the second step.
\end{itemize}



This recursive process is illustrated in Fig.~\ref{fig:recursive_partition}. The pseudocode for \decomp is provided in Alg.~\ref{alg:dynamic_decomposition}, with an annotated Python implementation in App.~\ref{app:code_impl}. 



The \textbf{priority metric} $h$ serves as the central heuristic for determining which solution steps to prioritize. It estimates the ``difficulty'' or ``potential for improvement'' of a step $\boldsymbol{y}_k$ given the context $\boldsymbol{x} \cdot \boldsymbol{y}_{1...(k-1)}$, computed via rollouts of the policy $\pi$. Specifically, $h(\boldsymbol{y}_k| \boldsymbol{x} \cdot \boldsymbol{y}_{1...(k-1)})$ is estimated by sampling continuations and evaluating their outcomes.

In practice, estimating $h$ and generating new samples occur simultaneously, as both rely on rollout-based computations (Sec.~\ref{sec:priority_metric}). Unlike standard decomposition methods, \decomp does not process steps in strict temporal order, resembling goal-directed planning~\citep{parascandolo2020divide} and \emph{backtracking}. 



\textbf{Intuition and Benefits.} \decomp partitions difficult steps into smaller, simpler sub-steps, allocating additional resources to refine them. This approach is particularly effective for inference scaling, where computing must be used judiciously. A binary decomposition strategy enables fast identification of difficult or high-potential steps.

\vspace{-0.2cm}
\begin{tcolorbox}[title=Key Insight: Recursive partitioning, colframe=low,boxsep=0.5mm]
\footnotesize{
    Top-down, recursive partitioning means that we can both efficiently locate critical steps and also dynamically determine step sizes based on our budget.}
\end{tcolorbox}
\vspace{-0.2cm}
\textbf{Dynamic Compute Allocation.}  
A key advantage of \decomp is its ability to prioritize challenging or high-potential steps, improving solution quality while minimizing compute waste. By iteratively refining steps with low priority scores, \decomp adaptively allocates more resources to difficult steps and less to simpler ones, optimizing inference efficiency.
\vspace{-0.2cm}
\begin{tcolorbox}[title=Key Insight: Adaptive compute allocation, colframe=low,boxsep=0.5mm]\footnotesize{
    \decomp dynamically allocates inference compute to harder steps, optimizing solution quality and resource efficiency.}
\end{tcolorbox}


% The \textbf{priority metric} $h$ is the central heuristic for determining which part of the solution to prioritize. It evaluates the ``difficulty'' or importance of a step $\boldsymbol{y}_k$ given the context $\boldsymbol{x} \cdot \boldsymbol{y}_{1...(k-1)}$. The metric can be estimated through rollouts of the policy $\pi$. Specifically, $h(\boldsymbol{y}_k| \boldsymbol{x} \cdot \boldsymbol{y}_{1...(k-1)})$ is computed by sampling potential continuations and evaluating their outcomes.



% In practice, the estimation of $h$ and the generation of new solution samples are performed simultaneously, as both involve rollout-based computations. We explain this in more detail in the section \ref{sec:priority_metric}. An interesting difference between the sequence by which \decomp processes steps and other decomposition methods is that \decomp may not sample the steps in temporal sequential order, similar to works in goal directed planning ~\citep{parascandolo2020divide}. 

% The pseudocode for our algorithm is shown in Alg. \ref{alg:dynamic_decomposition}, with a corresponding annotated python implementation shown in App. \ref{app:code_impl}. 

% \textbf{Intuition and Benefits.} The intuition behind \decomp is straightforward: partition difficult steps into smaller, simpler sub-steps and allocate additional resources to refine these sub-steps. This approach is particularly effective for inference scaling, where computational resources are limited and must be allocated judiciously. A binary decomposition approach allows us to locate difficult steps quickly. 


% \begin{tcolorbox}[title=Key Insight: Recursive partitioning, colframe=low]
% \footnotesize{
%     Top down, recursive partitioning means that we can both efficiently locate critical steps and also dynamically determine step sizes based on our budget.}
% \end{tcolorbox}


% \textbf{Dynamic Compute Allocation.}
% One significant advantage of \decomp is its ability to dynamically prioritize challenging steps, ensuring a higher likelihood of finding optimal solutions while minimizing compute waste on trivial steps. By iteratively refining ``hard'' steps with lower priority metrics, the algorithm adaptively allocates more computational resources to challenging parts of the solution while spending less on simpler steps. This dynamic allocation optimizes the use of inference compute time. 

% \begin{tcolorbox}[title=Key Insight: Adaptive Compute Allocation, colframe=low]\footnotesize{
%     \decomp dynamically allocates inference compute to harder steps, optimizing solution quality and resource efficiency.}
% \end{tcolorbox}

% \subsection{Priority Metric}

% \santiago{$h$ or $\hat{h}$ is computed in this form?}
% \santiago{More intuition as to what this metric is and how it relates to difficulty is needed.}
% \jon{Yeah this should be in the next section}


% \santiago{Top down?}
% \jon{We partition large steps into smaller steps}

% The \decomp algorithm recursively breaks solutions down using a binary decomposition approach that iteratively splits a larger solution step into smaller solution steps. Specifically, the input of the algorithm is a problem prompt $\boldsymbol{x}$, and the output is a decomposition of a solution $\boldsymbol{Y} = \{\boldsymbol{y}_1, ..., \boldsymbol{y}_k\}$ to $\boldsymbol{x}$, where the concatenation $\boldsymbol{y}_{12...k} = \boldsymbol{y}_1 \cdot \boldsymbol{y}_2 \cdot ... \cdot \boldsymbol{y}_k$ forms a complete solution to the problem. 

% \decomp works first generating $k_1$ complete solutions $\boldsymbol{y} \sim \pi(\cdot | \boldsymbol{x})$ starting from the prompt $\boldsymbol{x}$. We then select the optimal solution $\boldsymbol{y}^*$ out of the $k_1$ solutions to partition into two parts, $\boldsymbol{y}^* = \boldsymbol{y}^*_1 \cdot \boldsymbol{y}^*_2$, based on the \textbf{partition fraction} $\alpha$, where $|\boldsymbol{y}^*_1| \approx \alpha |\boldsymbol{y}^*|$. We then estimate the priority metric $h$ for the two parts, $\hat{h}(\boldsymbol{y}^*_1 | \boldsymbol{x})$ and $\hat{h}(\boldsymbol{y}^*_2 | \boldsymbol{x} \cdot \boldsymbol{y}^*_1)$. 

% We partition the step with lower priority. 
% If $\hat{h}(\boldsymbol{y}^*_1 | \boldsymbol{x}) \ge \hat{h}(\boldsymbol{y}^*_2 | \boldsymbol{x} \cdot \boldsymbol{y}^*_1)$, then we generate more samples of step $\boldsymbol{y}'_2 \sim \pi(\cdot | \boldsymbol{x} \cdot \boldsymbol{y}^*_1)$, pick the new best one $\boldsymbol{y}'^{*}_2$ repeat the partition process on $\boldsymbol{y}'^{*}_2$ where we split $\boldsymbol{y}'^{*}_2$ into two steps. 
% If $\hat{h}(\boldsymbol{y}^*_1 | \boldsymbol{x}) < \hat{h}(\boldsymbol{y}^*_2 | \boldsymbol{x} \cdot \boldsymbol{y}^*_1)$, then we can generate more samples of $\boldsymbol{y}'_1 \sim \pi(\cdot | \boldsymbol{x})$ repeat the partition process on $\boldsymbol{y}'^{*}_1$, where the first step consists of the $\alpha$ fraction of $\boldsymbol{y}'^{*}_1$ and the second step consists of the rest of the solution. 
% This recursive partitioning process is visualized in Fig. \ref{fig:recursive_partition}. 



% The \textbf{priority metric} $h$ is what we use to determine which part of the solution to partition and spend more compute on, where $h(\boldsymbol{y}'| \boldsymbol{x} \cdot \boldsymbol{y})$ denotes the priority metric for step $\boldsymbol{y}'$ starting from state $\boldsymbol{x} \cdot \boldsymbol{y}$, where $\boldsymbol{x}$ is the prompt and $\boldsymbol{y}$ is the intermediate solution so far. We can estimate the priority metric using rollout samples. 
% In practice, estimation of the priority metric and searching over new possible best steps \textbf{can be combined}, since both involve rollouts of the policy. We explain this in more detail in the next subsection and App. \ref{app:code_impl}. 

% The intuition behind \decomp is that we want to split steps that are `hard' into smaller steps and generate more samples of these smaller steps in hopes of finding a more optimal step to take. A big advantage of the \decomp algorithm, when used for inference scaling, is that it allows us to dynamically allocate more compute towards steps that are hard and less towards steps that are easy. The pseudocode for our algorithm is shown in Alg. \ref{alg:dynamic_decomposition}, with a corresponding annotated python implementation shown in App. \ref{app:code_impl}. 

% \begin{tcolorbox}[title=Takeaway: spend more compute on hard steps]
%     \decomp dynamically allocates more inference time compute towards harder steps
% \end{tcolorbox}


% The split metric $h$ is what we use to determine which part of the solution to split and spend more compute on, where $h(\boldsymbol{x} \cdot \boldsymbol{y})$ denotes the metric at intermediate solution $\boldsymbol{y}$ given prompt $\boldsymbol{x}$. 


% \decomp works by first conducting rollouts starting from the prompt $\boldsymbol{x}$ until we receive enough samples $\boldsymbol{y}$ to estimate the split metric $h()$

% The priority metric $h$ is what we use to determine which part of the solution to split and spend more compute on, where $h(\boldsymbol{x} \cdot \boldsymbol{y}, k)$ denotes the split metric for a step of size $k$ starting from state $\boldsymbol{x} \cdot \boldsymbol{y}$, where $\boldsymbol{x}$ is the prompt and $\boldsymbol{y}$ is the intermediate solution so far. If $k=\infty$, then there is no constraint on how large the step can be. 


% Recall that $h(\boldsymbol{y}_1)$ is actually an unbiased estimator for 
% Choices for the priority metric $h$ include the Q-value and Z-score. Below, we elaborate on the key metrics used in our framework:
\subsection{Priority Metric}
\label{sec:priority_metric}

We consider two intuitive priority metrics for step selection: \textbf{Q-value priority} and \textbf{Z-score priority}, which are visualized in Fig. \ref{fig:priority_metrics}.

\textbf{Q-value based priority (\decomp-Q).}  
Given a partial solution $\boldsymbol{y}^*_{1...k} = \boldsymbol{y}^*_{1...(k-1)} \cdot \boldsymbol{y}^*_a \cdot \boldsymbol{y}^*_b$, we aim to prioritize either $\boldsymbol{y}^*_a$ or $\boldsymbol{y}^*_b$ for refinement. 
The core intuition behind Q-value prioritization is that \emph{steps with lower Q-values indicate areas needing refinement}, directing compute toward the most challenging parts of the solution. 
% The \textbf{Q-priority metric} quantifies the difficulty of a step based on its expected Q-value under $\pi$:
More formally, we define the Q-priority metric under policy $\pi$ as:
\[
h_Q(\boldsymbol{y}^*_k \mid \boldsymbol{x} \cdot \boldsymbol{y}_{1...(k-1)}) = \mathbb{E}_{\boldsymbol{y}_k}\left[ Q^\pi(\boldsymbol{y}_k \mid  \boldsymbol{x} \cdot \boldsymbol{y}_{1...(k-1)}) \right].
\]
Here, $\boldsymbol{y}_k$ represents alternative steps sampled from $\pi$, and $Q^\pi(\boldsymbol{y}_k \mid \boldsymbol{x} \cdot \boldsymbol{y}_{1...(k-1)})$ denotes the Q-value of $\boldsymbol{y}_k$, conditioned on the partial solution $\boldsymbol{x} \cdot \boldsymbol{y}_{1...(k-1)}$. Equivalently, $h_Q(\boldsymbol{y}^*_k)$ can be interpreted as the value function $V^\pi(\boldsymbol{x} \cdot \boldsymbol{y}_{1...(k-1)})$, where $V^\pi$ represents the expected reward achievable from the given partial solution. The expectation is taken over sampled candidates $\boldsymbol{y}_k \sim \pi(\cdot \mid \boldsymbol{x} \cdot \boldsymbol{y}_{1...(k-1)})$, constrained by $|\boldsymbol{y}_k| = |\boldsymbol{y}^*_k|$. This metric helps identify \emph{difficult steps} that the LLM is likely to get wrong, guiding partitioning toward the most critical refinements.

To estimate $h_Q$ for the second step $\boldsymbol{y}^*_b$, we sample $\boldsymbol{y}_{b}$ from $\pi$, generate rollouts $\boldsymbol{y}_{(k+1)+}$, compute rewards, and average the outcomes. For the previous step $\boldsymbol{y}^*_a$, we reuse rollouts from earlier partitioning, as the mean of these previously generated samples provides an unbiased estimate of the Q-priority metric:
% \[
% h_Q(\boldsymbol{y}^*_a \mid \boldsymbol{x} \cdot \boldsymbol{y}^*_{1...(k-1)}) = \mathbb{E} \left[ \hat{h}_Q(\boldsymbol{y}^*_{(k-1)+} \mid \boldsymbol{x} \cdot \boldsymbol{y}^*_{1...(k-1)}) \right].
% \]
\[
h_Q(\boldsymbol{y}^*_a \mid \boldsymbol{x} \cdot \boldsymbol{y}^*_{1...(k-1)}) = \mathbb{E} \left[ R(\boldsymbol{y}^*_{1...(k-1)+}) \right].
\]
By leveraging existing rollouts, \decomp avoids redundant sampling, improving computational efficiency. Once the rollouts for $\boldsymbol{y}^*_{(k+1)}$ are available, the best completion $\boldsymbol{y}^*_{(k+1)+}$ is selected as the next step to partition. This dual use of rollouts optimizes both metric estimation and inference sampling.


\vspace{-0.2cm} 
\begin{tcolorbox}[title=Combining priority estimation and test-time search, colframe=low, boxsep=0.5mm]\footnotesize{
    By integrating LLM policy sampling for both metric estimation and search, we can significantly enhance computational efficiency.}
\end{tcolorbox}
\vspace{-0.2cm} 

% \textbf{Q-value based priority (\decomp-Q).}  
% Given a partial solution $\boldsymbol{y}^*_{1...k} = \boldsymbol{y}^*_{1...(k-1)} \cdot \boldsymbol{y}^*_a\cdot \boldsymbol{y}^*_b$, we aim to prioritize either $\boldsymbol{y}^*_a$ or $\boldsymbol{y}^*_b$ for refinement. The \textbf{Q-priority metric} quantifies the importance  or ``difficulty'' of a step based on its expected Q-value under the policy $\pi$:
% \[
% h_Q(\boldsymbol{y}^*_k \mid \boldsymbol{x} \cdot \boldsymbol{y}_{1...(k-1)}) = \mathbb{E}_{\boldsymbol{y}_k}\left[ Q^\pi(\boldsymbol{y}_k \mid  \boldsymbol{x} \cdot \boldsymbol{y}_{1...(k-1)}) \right].
% \]
% Here, $\boldsymbol{y}_k$ represents possible alternative steps for $\boldsymbol{y}^*_k$, sampled from the model $\pi$, and $Q^\pi(\boldsymbol{y}_k \mid \boldsymbol{x} \cdot \boldsymbol{y}_{1...(k-1)})$ denotes the Q-value of a candidate step $\boldsymbol{y}_k$, conditioned on the partial solution $\boldsymbol{x} \cdot \boldsymbol{y}_{1...(k-1)}$. Equivalently, $h_Q(\boldsymbol{y}^*_k \mid \boldsymbol{x} \cdot \boldsymbol{y}_{1...(k-1)})$ can be viewed as the value function $V^\pi(\boldsymbol{x} \cdot \boldsymbol{y}_{1...(k-1)})$ under policy $\pi$, where $V^\pi$ represents the expected reward achievable from the given partial solution. The expectation is taken over candidate steps $\boldsymbol{y}_k$ sampled as $\boldsymbol{y}_k \sim \pi(\cdot \mid \boldsymbol{x} \cdot \boldsymbol{y}_{1...(k-1)})$, constrained by $|\boldsymbol{y}_k| = |\boldsymbol{y}^*_k|$. Thus, the Q-priority metric measures the average Q-value of refinements of $\boldsymbol{y}^*_k$, guiding optimization towards the most critical steps, difficult steps that the LLM often gets wrong. 

% To estimate $h_Q$ for the second step $\boldsymbol{y}^*_b$, we sample $\boldsymbol{y}_{b}$ from $\pi$, generate rollouts $\boldsymbol{y}_{(k+1)+}$, compute rewards, and average the outcomes. For the previous step $\boldsymbol{y}^*_{a}$, we can reuse rollouts from earlier partitioning since the mean of the previously generated samples provides an unbiased estimate of the Q-priority metric:
% \[
% h_Q(\boldsymbol{y}^*_k \mid \boldsymbol{x} \cdot \boldsymbol{y}^*_{1...(k-1)}) = \mathbb{E} \left[ \hat{h}_Q(\boldsymbol{y}^*_{k+} \mid \boldsymbol{x} \cdot \boldsymbol{y}^*_{1...(k-1)}) \right].
% \]
% By leveraging existing rollouts, \decomp avoids redundant sampling, improving computational efficiency. Once the rollouts for $\boldsymbol{y}^*_{(k+1)}$ are available, we select the best completion $\boldsymbol{y}^*_{(k+1)+}$ as the next step to partition. This dual use of rollouts optimizes both metric estimation and decision-making. 

% The core intuition behind Q-value prioritization is that lower Q-values indicate areas needing refinement, directing compute toward the most challenging steps.

% \begin{tcolorbox}[title=Takeaway: Combining step priority estimation and inference-time search, colframe=low, boxsep=0.1mm]\footnotesize{
%     By integrating LLM policy sampling for both metric estimation and search, we can significantly enhance computational efficiency.}
% \end{tcolorbox}

% Here, $\boldsymbol{y}_k$ represents candidate alternatives to $\boldsymbol{y}^*_k$ sampled from $\pi$, and $Q^\pi(\boldsymbol{y}_k \mid \boldsymbol{x} \cdot \boldsymbol{y}_{1...(k-1)})$ is the estimated Q-value for each candidate step. Intuitively, $h_Q$ measures how much a step contributes to solution quality, guiding partitioning toward steps requiring further refinement.

% To estimate $h_Q(\boldsymbol{y}^*_{(k+1)} \mid \boldsymbol{x} \cdot \boldsymbol{y}^*_{1...k})$, we sample $\boldsymbol{y}_{(k+1)} \sim \pi(\cdot \mid \boldsymbol{x} \cdot \boldsymbol{y}^*_{1...k})$, perform rollouts $\boldsymbol{y}_{(k+1)+}$ to completion, compute the rewards, and take the average of the outcomes. For the preceding step $\boldsymbol{y}^*_{k}$, recall that its rollout samples $\boldsymbol{y}_{k+}$ were already generated during the earlier partitioning. The mean of these previously generated samples provides an unbiased estimate of the Q-priority metric for $\boldsymbol{y}^*_{k}$:

\begin{figure}
    \centering
\vspace{-0.3cm}    \includegraphics[width=0.72\linewidth]{graphics/icmlsquare-03}
   \vspace{-0.3cm}  
    \caption{\textbf{Priority metric estimation.} We can estimate the priority metric $\hat{h}$ using Monte Carlo rollouts of the LLM policy $\pi$ for each step.}
    \label{fig:metric_estimation}
\vspace{-0.3cm}  
\end{figure}

\textbf{Z-score based priority (\decomp-Z).}  
To allocate more compute to steps with \emph{higher potential for improvement}, we estimate the probability of sampling a better step given existing samples. Assuming the Q-values of sampled steps follow a normal distribution, we model this probability using the cumulative distribution function (CDF). Given a mean $\mu_r$ and standard deviation $\sigma_r$ of sampled Q-values, the probability of sampling better than the best-observed step $\boldsymbol{y}^*_k$ with Q-value $q^*$ is:
\[
1 - \operatorname{CDF}\left(\frac{q^* - \mu_r}{\sigma_r}\right) = 1 - \operatorname{CDF}(z^*),
\]
where $z^*$ is the Z-score. Since the CDF is monotonic, we compare steps based on their Z-scores.

We formally define the Z-score priority metric as:
\begin{equation*}
    h_Z(\boldsymbol{y}^*_k \mid \boldsymbol{x} \cdot \boldsymbol{y}_{1...(k-1)}) = \frac{q^* - \mathbb{E}[Q^\pi(\boldsymbol{y}_k \mid \boldsymbol{x} \cdot \boldsymbol{y}_{1...(k-1)})]}{\operatorname{Std}[Q^\pi(\boldsymbol{y}_k \mid \boldsymbol{x} \cdot \boldsymbol{y}_{1...(k-1)})]}.
\end{equation*}
Since $q^* - \mu_r$ represents the advantage of step $\boldsymbol{y}^*_k$, the Z-score metric can be interpreted as a \emph{standard deviation-scaled advantage}. Lower $h_Z$ values indicate steps with greater room for improvement, guiding decomposition toward those with higher variance in performance.

\begin{figure}
    \centering
\vspace{-0.3cm}      \includegraphics[width=0.72\linewidth]{graphics/icmlsquare-06}
    \vspace{-0.3cm}  
    \caption{\textbf{Different priority metrics.} Sampled rewards of different steps can be visualized as a distribution.}
    \label{fig:priority_metrics}
\vspace{-0.6cm}  
\end{figure}

% \subsection{Priority Metric}
% \label{sec:priority_metric}


% We focus on two intuitive priority metrics in our study -- Q-value priority and Z-score priority. 


% \textbf{Q-Value Based Priority (\decomp-Q).} 
% Suppose we have a partial solution $\boldsymbol{y}^*_{1...(k+1)} = \boldsymbol{y}^*_{1...k} \cdot \boldsymbol{y}^*_{(k+1)} = \boldsymbol{y}^*_{1} \cdot \boldsymbol{y}^*_{2} \cdot \dots \cdot \boldsymbol{y}^*_{(k+1)}$ and aim to decide between $\boldsymbol{y}^*_{k}$ and $\boldsymbol{y}^*_{(k+1)}$ as the next step to refine. In a Q-value-based prioritization, we evaluate and compare the average Q-values of these steps to guide the partitioning process.

% For any solution step $\boldsymbol{y}^*_{k}$, the Q-priority metric quantifies its importance for refinement and is defined as:
% \[
% h_Q(\boldsymbol{y}^*_k \mid \boldsymbol{x} \cdot \boldsymbol{y}_{1...(k-1)}) = \mathbb{E}_{\boldsymbol{y}_k}\left[ Q^\pi(\boldsymbol{y}_k \mid  \boldsymbol{x} \cdot \boldsymbol{y}_{1...(k-1)}) \right].
% \]
% Here, $\boldsymbol{y}_k$ represents possible alternative steps for $\boldsymbol{y}^*_k$, sampled from the model $\pi$, and $Q^\pi(\boldsymbol{y}_k \mid \boldsymbol{x} \cdot \boldsymbol{y}_{1...(k-1)})$ denotes the Q-value of a candidate step $\boldsymbol{y}_k$, conditioned on the partial solution $\boldsymbol{x} \cdot \boldsymbol{y}_{1...(k-1)}$. Equivalently, $h_Q(\boldsymbol{y}^*_k \mid \boldsymbol{x} \cdot \boldsymbol{y}_{1...(k-1)})$ can be viewed as the value function $V^\pi(\boldsymbol{x} \cdot \boldsymbol{y}_{1...(k-1)})$ under policy $\pi$, where $V^\pi$ represents the expected reward achievable from the given partial solution. The expectation is taken over candidate steps $\boldsymbol{y}_k$ sampled as $\boldsymbol{y}_k \sim \pi(\cdot \mid \boldsymbol{x} \cdot \boldsymbol{y}_{1...(k-1)})$, constrained by $|\boldsymbol{y}_k| = |\boldsymbol{y}^*_k|$. Thus, the Q-priority metric measures the average Q-value of refinements of $\boldsymbol{y}^*_k$, guiding optimization towards the most critical steps.

% To estimate $h_Q(\boldsymbol{y}^*_{(k+1)} \mid \boldsymbol{x} \cdot \boldsymbol{y}^*_{1...k})$, we sample $\boldsymbol{y}_{(k+1)} \sim \pi(\cdot \mid \boldsymbol{x} \cdot \boldsymbol{y}^*_{1...k})$, perform rollouts $\boldsymbol{y}_{(k+1)+}$ to completion, compute the rewards, and take the average of the outcomes. For the preceding step $\boldsymbol{y}^*_{k}$, recall that its rollout samples $\boldsymbol{y}_{k+}$ were already generated during the earlier partitioning. The mean of these previously generated samples provides an unbiased estimate of the Q-priority metric for $\boldsymbol{y}^*_{k}$:
% \[
% h_Q(\boldsymbol{y}^*_k \mid \boldsymbol{x} \cdot \boldsymbol{y}^*_{1...(k-1)}) = \mathbb{E} \left[ \hat{h}_Q(\boldsymbol{y}^*_{k+} \mid \boldsymbol{x} \cdot \boldsymbol{y}^*_{1...(k-1)}) \right]
% \]

% By reusing rollouts generated for earlier steps, this approach eliminates the need to re-sample $\boldsymbol{y}^*_{k}$ for Q-value estimation, saving computational resources. Moreover, once the rollouts for $\boldsymbol{y}^*_{(k+1)}$ are available, the best completion $\boldsymbol{y}^*_{(k+1)+}$ can be selected as the next optimal step to partition. This dual use of rollouts for both metric estimation and decision-making enhances compute efficiency.

% The core intuition behind Q-value-based prioritization is that solution steps with lower average Q-values indicate areas where further refinement and optimization are most critical. By prioritizing such steps, the method efficiently focuses computational resources on challenging parts of the solution.



% \begin{tcolorbox}[title=Takeaway: Combining step priority estimation and inference time search, colframe=low,boxsep=0.1mm]\footnotesize{
%     By integrating LLM policy sampling for both metric estimation and search, we can significantly enhance computational efficiency.}
% \end{tcolorbox}

% \textbf{Z-score Based Priority (\decomp-Z).}
% Ideally we would like to allocate more compute (through decomposition) to steps that have a higher \emph{potential} for improvement. We can estimate the potential through estimating the \emph{probability of sampling a better step} based on the existing samples. 
% If we assume that the q-value of the sampled steps, conditional on the same context $\boldsymbol{x} \cdot \boldsymbol{y}_{1...(k-1)}$, follow a normal distribution, then, given mean $\mu_r$ and standard deviation $\sigma_r$ the probability of sampling better than the best sample $\boldsymbol{y}^*_k$ and associated q-value $q^* = R(\boldsymbol{x} \cdot \boldsymbol{y}_{1...(k-1)}\cdot \boldsymbol{y}^*_k)$ is $1-\operatorname{CDF}(\frac{r^*-\mu_r}{\sigma_r}) = 1-\operatorname{CDF}(z^*) $, where $z^*$ is the z-score. Since the CDF is monotonic, for comparison purposes, we just need to compare the z-score of the two steps. 

% We formally define the z-score priority metric as follows:
% \begin{equation}
%     h_Z(\boldsymbol{y}^*_k \mid \boldsymbol{x}\cdot\boldsymbol{y}_{1...(k-1)}) = \frac{q^* - \mathbb{E}[Q^\pi(\boldsymbol{y}_k \mid \boldsymbol{x}\cdot\boldsymbol{y}_{1...(k-1)})]}{\operatorname{Std}[Q^\pi(\boldsymbol{y}_k \mid \boldsymbol{x}\cdot\boldsymbol{y}_{1...(k-1)})]}
% \end{equation}
% Where we take the expectation and standard deviation across $\boldsymbol{y}_k \mid \boldsymbol{x}\cdot\boldsymbol{y}_{1...(k-1)}$. 
% Hence, the z-score metric can be interpreted as a standard deviation scaled version of the advantage metric. Recall that $q^*-\mu_r$ is how the advantage for action $\boldsymbol{y}^*_k$ is defined. Hence, $h_Z$ can also be interpreted as \emph{standard deviation scaled advantage}. 


% \begin{figure}
%     \centering
%     \includegraphics[width=0.72\linewidth]{graphics/icmlsquare-06.png}
%     \caption{\textbf{Different priority metrics.} The sampled rewards of different steps from different states can be visualized as a distribution. }
%     \label{fig:priority_metrics}
% \vskip -0.2in
% \end{figure}

% \begin{equation}
%     h_Z(\boldsymbol{y}^* | \boldsymbol{x}\cdot\boldsymbol{y}) = \frac{h_A(\boldsymbol{y}^* | \boldsymbol{x}\cdot\boldsymbol{y})}{\operatorname{Std}(R(\boldsymbol{y}' | \boldsymbol{x}\cdot\boldsymbol{y}))}
% \end{equation}


% For any solution step $\boldsymbol{y}^*_{k}$, the Q-priority metric quantifies its importance for refinement and is defined as:
% \[
% h_Q(\boldsymbol{y}^*_k \mid \boldsymbol{x} \cdot \boldsymbol{y}_{1...(k-1)}) = \mathbb{E}_{\boldsymbol{y}_k}\left[ Q^\pi(\boldsymbol{y}_k \mid  \boldsymbol{x} \cdot \boldsymbol{y}_{1...(k-1)}) \right].
% \]
% Here, $\boldsymbol{y}_k$ represents possible alternative steps for $\boldsymbol{y}^*_k$, sampled from the model $\pi$, and $Q^\pi(\boldsymbol{y}_k \mid \boldsymbol{x} \cdot \boldsymbol{y}_{1...(k-1)})$ denotes the Q-value of a candidate step $\boldsymbol{y}_k$, conditioned on the partial solution $\boldsymbol{x} \cdot \boldsymbol{y}_{1...(k-1)}$. Thus, $h_Q(...[todo]) = V^\pi(x dot y_{...[to fill]...})$ where $V^\pi$ is the ...[to fill]. The expectation is taken over candidate steps $\boldsymbol{y}_k$ sampled as $\boldsymbol{y}_k \sim \pi(\cdot \mid \boldsymbol{x} \cdot \boldsymbol{y}_{1...(k-1)})$, with the additional constraint that $|\boldsymbol{y}_k| = |\boldsymbol{y}^*_k|$. In essence, the Q-priority metric evaluates the average Q-value of all possible refinements of $\boldsymbol{y}^*_k$, providing a measure of its priority for further optimization.


% For any solution step $\boldsymbol{y}^*_{k}$, the Q-priority metric is defined as:
% \[
% h_Q(\boldsymbol{y}^*_k \mid \boldsymbol{x} \cdot \boldsymbol{y}_{1...(k-1)}) = \mathbb{E}_{\boldsymbol{y}_k}\left[ Q^\pi(\boldsymbol{y}_k \mid  \boldsymbol{x} \cdot \boldsymbol{y}_{1...(k-1)}) \right]
% \]
% where $\boldsymbol{y}_k$ represents possible replacement steps for $\boldsymbol{y}^*_k$ sampled from the model $\pi$, and $Q^\pi$ denotes the Q-value under policy $\pi$ and the expectation is taken over ... $\boldsymbol{y}_k \sim \pi(\cdot \mid \boldsymbol{x} \cdot \boldsymbol{y}), |\boldsymbol{y}_k| = |\boldsymbol{y}^*_k|$


% \textbf{Q-Value Based Priority (\decomp-Q).}
% Suppose we have a partial solution $\boldsymbol{y}^*_{0...(k+1)} = \boldsymbol{y}_{0...(k-1)}^* \cdot \boldsymbol{y}^*_{k(k+1)} = \boldsymbol{y}_{0...(k-1)}^* \cdot \boldsymbol{y}^*_{k} \cdot \boldsymbol{y}^*_{(k+1)}$ and would like to choose between $\boldsymbol{y}^*_k$ and $\boldsymbol{y}^*_{(k+1)}$ to further partition. 
% % Let $\boldsymbol{y}^*_{012} = \boldsymbol{y}_0^* \cdot \boldsymbol{y}^*_{12} = \boldsymbol{y}_0^* \cdot \boldsymbol{y}^*_{1} \cdot \boldsymbol{y}^*_2$ denote the optimal solution, where we would like to partition the latter step $\boldsymbol{y}^*_{12}$. 
% In a Q-value-based partition, we compare the average Q-value of the first step $\boldsymbol{y}^*_1$ with that of the second step $\boldsymbol{y}^*_2$. The Q-priority metric for any step $\boldsymbol{y}_k^*$ is defined as:
% \begin{equation}
%     h_Q(\boldsymbol{y}^*_k | \boldsymbol{x}\cdot\boldsymbol{y}_{0...(k-1)}) = \underset{\boldsymbol{y}' \sim \pi(\cdot | \boldsymbol{x}\cdot\boldsymbol{y}), |\boldsymbol{y}'| = |\boldsymbol{y}^*|}{\mathbb{E}} \left[Q^\pi(\boldsymbol{y}' | \boldsymbol{x}\cdot\boldsymbol{y})\right]
% \end{equation}

% To estimate the average Q-value for the second step, $h_Q(\boldsymbol{y}_2^* | \boldsymbol{x}\cdot\boldsymbol{y}_{01}^*)$, we generate samples $\boldsymbol{y}_2 \sim \pi(\cdot | \boldsymbol{x}\cdot\boldsymbol{y}_{01}^*)$, perform rollouts $\boldsymbol{y}_{2+}$ to completion, calculate the outcome rewards, and take the mean. For the first step, recall that we already generated rollout samples for $\boldsymbol{y}_{12}$ in the previous partitioning. 
% The mean of these samples actually serves as an unbiased estimator for $h_Q(\boldsymbol{y}^*_1, \boldsymbol{x}\cdot\boldsymbol{y}_0^*)$:
% \begin{equation}
%     h_Q(\boldsymbol{y}^*_1, \boldsymbol{x}\cdot\boldsymbol{y}_0^*) = \mathbb{E}\left[\hat{h}_Q(\boldsymbol{y}^*_{12}, \boldsymbol{x}\cdot\boldsymbol{y}_0^*)\right]
% \end{equation}

% This approach eliminates the need to re-sample the first step for metric estimation. Furthermore, since we have already generated full rollouts for $\boldsymbol{y}_2$, we can simply select the best rollout $\boldsymbol{y}^*_{2+}$ as the next optimal step to partition. By combining the rollouts for estimation and search, we can increase compute efficiency. 

% The intuition behind a Q-value-based split is that steps with lower average Q-values are more critical to refine and optimize further.
% we utilize samples $\boldsymbol{y}_{12}$ generated in the previous partition step. 


% \textbf{Maximum Advantage Based Priority (\decomp-A).}
% The advantage-based priority metric is defined as:
% \begin{equation}
%     h_A(\boldsymbol{y}^* | \boldsymbol{x}\cdot\boldsymbol{y}) = \max_{\boldsymbol{y}', |\boldsymbol{y}'| = |\boldsymbol{y}^*|}\left[Q^\pi(\boldsymbol{y}' | \boldsymbol{x}\cdot\boldsymbol{y})\right] - h_Q(\boldsymbol{y}^* | \boldsymbol{x}\cdot\boldsymbol{y})
% \end{equation}

% This metric quantifies the maximum advantage achievable for a given step, measuring how much better we can perform compared to the average sample. The intuition is that steps with lower advantage have greater potential for improvement, warranting more granular decomposition and search. Conversely, steps with higher maximum advantage are likely already optimal and require less refinement. We can estimate the maximum advantage through rollouts, same as how we estimated $\hat{h}_Q$. 




% Where we take the negative since we want to choose the step with lower z-score and thus higher potential. 

% In summary, the Q-value and Zscore-based metrics provide complementary perspectives for prioritizing steps in our framework, enabling efficient and targeted optimization of the solution space.



% \subsection{Priority metric}
% \label{sec:priority_metric}

% Choices for priority metric $h$ include the Q-value, maximum advantage, and variance. We elaborate more on some of the metrics we used below:

% \textbf{Q-value based priority.} Let $\boldsymbol{y}^*_{012} = \boldsymbol{y}_0^* \cdot \boldsymbol{y}^*_{12} = \boldsymbol{y}_0^* \cdot \boldsymbol{y}^*_{1} \cdot \boldsymbol{y}^*_2$ be the best solution of which we would like to partition the latter step $\boldsymbol{y}^*_{12}$ in two. When using a Q-value based partition, we would like to compare the average Q-value of the first step $\boldsymbol{y}^*_1$ with the average Q-value of the second step $\boldsymbol{y}^*_2$. In other words, the split metric is:
% \begin{equation}
%     h_Q(\boldsymbol{y}^* | \boldsymbol{x}\cdot\boldsymbol{y}) = \underset{\boldsymbol{y}' \sim \pi(\cdot | \boldsymbol{x}\cdot\boldsymbol{y} ), |\boldsymbol{y}'| = |\boldsymbol{y}^*|}{\mathbb{E}} \left[Q^\pi(\boldsymbol{y}' | \boldsymbol{x}\cdot\boldsymbol{y})\right]
% \end{equation}
% To estimate the the average Q-value for second step, $h_Q(\boldsymbol{y}_2^* | \boldsymbol{x}\cdot\boldsymbol{y}_{01}^*) $, we can generate samples $\boldsymbol{y}_2 \sim \pi(\cdot | \boldsymbol{x}\cdot\boldsymbol{y}_{01}^*)$, then rollout the samples $\boldsymbol{y}_{2+}$ until completion, get the outcome reward, and take the mean reward. 
% To calculate the average Q-value for the first step, recall that previously we have already generated samples $\boldsymbol{y}_{12}$ for the step we are trying to split as part of the previous partition step. The mean of these samples is actually an unbiased estimator for $h_Q(\boldsymbol{y}^*_1, \boldsymbol{x}\cdot\boldsymbol{y}_0^*)$.
% In other words, 
% \begin{equation}
%     h_Q(\boldsymbol{y}^*_1, \boldsymbol{x}\cdot\boldsymbol{y}_0^*) = \mathbb{E}\left[\hat{h}_Q(\boldsymbol{y}^*_{12}, \boldsymbol{x}\cdot\boldsymbol{y}_0^*)\right] 
% \end{equation}
% This trick means that we \emph{do not need to spend compute sampling the first step for metric estimation}. Moreover, since we already generated full solution rollouts when sampling $\boldsymbol{y}_2$, which can simply take the best of those rollouts $\boldsymbol{y}^*_{2+}$ as the next best step to partition. 

% The intuition behind a Q-value based split is that steps with lower average Q-values 

% \begin{tcolorbox}[title=Takeaway: combine step priority estimation and search]
%     We can combine the LLM policy sampling steps for both metric estimation and searching for the optimal step to increase compute efficiency
% \end{tcolorbox}


% \textbf{Maximum advantage based priority.} 
% The advantage based priority metric is defined as
% \begin{equation}
%     h_A(\boldsymbol{y}^* | \boldsymbol{x}\cdot\boldsymbol{y}) = \max_{\boldsymbol{y}', |\boldsymbol{y}'| = |\boldsymbol{y}^*|}\left[Q^\pi(\boldsymbol{y}' | \boldsymbol{x}\cdot\boldsymbol{y})\right] - h_Q(\boldsymbol{y}^* | \boldsymbol{x}\cdot\boldsymbol{y})
% \end{equation}
% Which measures the maximum advantage possible for the given step. The intuition behind an advantage based split is that it measures \emph{how much better we can do} compared to the average sample. Steps with lower advantage have higher room for improvement, and thus more fine-grained, step-wise search and decomposition across those steps should lead to better results. Meanwhile, steps with higher maximum advantage are likely steps where we have already discovered the optimal step and thus do not require more search. 


% \begin{equation}
%     \underset{\boldsymbol{y}' \sim \pi(\cdot | \boldsymbol{x}\cdot\boldsymbol{y} ), |\boldsymbol{y}'| = |\boldsymbol{y}^*|}{\mathbb{E}} \left[Q^\pi(\boldsymbol{y}' | \boldsymbol{x}\cdot\boldsymbol{y})\right] = \underset{\boldsymbol{y}' \sim \pi(\cdot | \boldsymbol{x}\cdot\boldsymbol{y} ), |\boldsymbol{y}'| = |\boldsymbol{y}^*|}{\mathbb{E}} \left[Q^\pi(\boldsymbol{y}' | \boldsymbol{x}\cdot\boldsymbol{y})\right]
% \end{equation}


% \textbf{Q-value based split.} Let $\boldsymbol{y}^* = \boldsymbol{y}_0^* \cdot \boldsymbol{y}^*_{12} = \boldsymbol{y}_0^* \cdot \boldsymbol{y}^*_{1} \cdot \boldsymbol{y}^*_2$ be the best solution of which we would like to split the latter step $\boldsymbol{y}^*_{12}$ in two. When using a Q-value based split, we would like to compare the average Q-value of the first step $\boldsymbol{y}^*_1$ with the average Q-value of the second step $\boldsymbol{y}^*_2$. In other words, the split metric is:
% \begin{equation}
%     h_Q(\boldsymbol{x}\cdot\boldsymbol{y}, k) = \underset{\boldsymbol{y}' \sim \pi(\cdot | \boldsymbol{x}\cdot\boldsymbol{y} ), |\boldsymbol{y}'| = k}{\mathbb{E}} \left[Q^\pi(\boldsymbol{y}' | \boldsymbol{x}\cdot\boldsymbol{y})\right]
% \end{equation}
% To calculate the the average Q-value for second step, $h_Q(\boldsymbol{x}\cdot\boldsymbol{y}_0^* \cdot \boldsymbol{y}_1^*, \infty) $, we can generate samples $\boldsymbol{y}_2 \sim \pi(\cdot | \boldsymbol{x}\cdot\boldsymbol{y}_0^*\cdot \boldsymbol{y}_1^*)$ and take the mean. To calculate the average Q-value for the first step, recall that previously we have already generated samples $\boldsymbol{y}_{12}$ for the step we are trying to split. The mean of these samples is actually an unbiased estimator for $h_Q(\boldsymbol{x}\cdot\boldsymbol{y}_0^*, |\boldsymbol{y}^*_1|) $

% Suppose we use the mean Q-value of the next step $\boldsymbol{y}'$ as the split metric
% \begin{equation}
%     h_Q(\boldsymbol{y}) = \underset{\boldsymbol{y}' \sim \pi(\cdot | \boldsymbol{x}\cdot\boldsymbol{y} )}{\mathbb{E}} \left[Q(\boldsymbol{y}' | \boldsymbol{x}\cdot\boldsymbol{y})\right]
% \end{equation}
% Let $\boldsymbol{y}^*$


% We would now like to calculate 
% Recall that 









% \textbf{Variance based split. }






\subsection{\decomp and Search Methods}
\decomp can also be used to enhance \textbf{Monte Carlo Tree Search (MCTS)} and other inference scaling and search methods. Recall that in our partition step, we greedily partition the best solution step $\boldsymbol{y}^*$. Instead of greedily partitioning the best step, we can partition the top $k$ best steps instead, and select which step to partition using the upper confidence tree (UCT) formula. We can also use \textbf{beam search} to prune out steps we do not want to partition further. 
We explain MCTS and beam search in detail in App. \ref{sec:search_extended} and present results of combining \decomp with search in Sec. \ref{sec:search_results}.



% One major problem with using step-by-step MCTS in language settings where the action space is large is that it is difficult to determine the step size

\vspace{-0.2cm}  
\begin{tcolorbox}[title=Takeaway: Dynamic step sizes can improve search,boxsep=0.5mm, colframe=low,]\footnotesize{
    \decomp can enhance search based inference scaling methods by determining what step size to search across.}
\end{tcolorbox}
\vspace{-0.3cm}  

\vspace{-0.1cm}  
\subsection{A Motivating Example on DISC-Z}
\vspace{-0.1cm}  
We use the Wiener process $W(t)$ as an example where there are intractably many actions and steps. Suppose we start at $t=0$ with $W(0) = 0$. 
At each round $k$, the algorithm can choose one of the two options:
\begin{enumerate}[leftmargin=*,itemsep=0pt,topsep=0pt,parsep=0pt, partopsep=0pt]
    \item samples a trajectory and observe the final value $W(T)$ at time $t=T$, as the reward signal. Denote the whole trajectory as $w_k(\cdot)$.
    \item chooses one trajectory from the previous rounds (denoted as $w_s(t)$ for round $s$), and time $t_0$; then sample a trajectory at $t=t_0$ with $W(t_0)=w_s(t_0)$. Denote the concatenated trajectory as $w_k(\cdot)$ with $w_k(t)=w_s(t)$ when $t\le t_0$.
\end{enumerate}
Note that we are only able to observe the final reward $W(t)$. At any intermediate time $t \in (0, T)$, the current value $W(t)$ is not observable. The goal is to design an algorithm that can reach the highest reward among the $K$ trajectories. Formally speaking, we aim to maximize the maximum:
\begin{align*}
    \max_{k \in K} w_k(T).
\end{align*}
% The optimal solution to the toy example above may be of independent interest but we present it mainly to motivate our algorithmic design for LLM test-time search. 
One naive solution is to call option 1 for $K$ times and return the best-of-$K$ reward, each following:
\begin{align*}
    W(T) & \sim \mathcal{N}(0, T).
\end{align*}


Alternatively, suppose there is a promising path $w(\cdot)$ with a high final reward $w(T)=R$.
It is natural to consider starting at some midpoint $\alpha T$ ($0 < \alpha < 1$) and perform more completions to obtain an even higher reward than $R$. 
The reward distribution sampled this way is
\begin{align*}
    W'(T) \sim \mathcal{N}(w(\alpha T) , (1-\alpha)T).
\end{align*}
The remaining question is which $\alpha$ we should choose. One option is to maximize the probability that the newly sampled reward is higher than $R$:
\begin{align*}
    \mathbb{P}(W'(T) > R) = 1 - \Phi\bigg(  
    \frac{R - w(\alpha T)}{\sqrt{(1-\alpha)T}}
    \bigg).
\end{align*}


% To facilitate the 
% More formally, we are going to compare the following two


% One caveat is that, the high reward $R$ can be attributed to the first part $t \in [0, \alpha T]$ and the second part $t \in [\alpha T, T]$. Conditioned on that $W(T)=R$, we have
% \begin{align*}
%     W(\alpha T)|_{W(T)=R} & \sim \mathcal{N}
%     \big(\alpha R, \alpha (1-\alpha)T \big), \\
%     W(T) - W(\alpha T)|_{W(T)=R}  &\sim \mathcal{N}\big((1-\alpha) R, \alpha (1-\alpha)T \big).
% \end{align*}
% In expectation, we


% On the other hand, 






% \begin{figure}
%     \centering
%     \caption{\textbf{Priority metric estimation.} We can estimate the priority metric $\hat{h}$ using Monte Carlo rollouts of the LLM policy $\pi$ for each step}
%     \label{fig:leftmost-tree}
% \end{figure}

\vspace{-0.2cm}  
\subsection{Example Decomposition}
\vspace{-0.1cm}  
\label{sec:example_decomp}

With sample budget 100, the decomposition of a MATH problem is as follows, where color indicates the value of the priority metric $h$ of each step (yellow low, purple high).
\vspace{-0.2cm}  
\begin{tcolorbox}[colback=white, title=\decomp example decomposition, colframe=black, boxsep=0.1mm]
\scriptsize
\textcolor{high}{Let the length of the rectangle be \( l \)} 
    \textcolor{medium}{and the width of the rectangle be \( w \). Since the perimeter of the rectangle is 24 inches, we have that \( 2l + 2w = 24 \), so \( l + w = 12 \). We wish to maximize the area of the rectangle,}
    \textcolor{high}{which}
    \textcolor{low}{is \( A = lw \). Let \( l = 12 - w \) and plug into the area:}
    \[
    \textcolor{low}{A = (12 - w)w \Rightarrow \qquad A = 12w - w^2}
    \]
    \textcolor{low}{Now, we differentiate \( A \) with respect to \( w \):}
    \[
    \textcolor{low}{A'(w) = 12 - 2w}
    \]
    \textcolor{low}{We wish to maximize \( A \), so we set \( A'(w) = 0 \), and solve for \( w \):}
    \[
    \textcolor{low}{12 - 2w = 0 \Rightarrow \qquad w = 6}
    \]
    \textcolor{low}{Since \( l = 12 - w \), we have that \( l = 12 - 6 = 6 \). Therefore, the area of the rectangle is \( A = lw = 6 \cdot 6 = \boxed{36} \).}
\end{tcolorbox}
\vspace{-0.1cm}  
\normalsize
Once the LLM generates the first three steps, the rest is easy. Interestingly, `which' is an important decision point which helps decide how the LLM will complete the solution. 
\vspace{-0.2cm}  
\begin{tcolorbox}[title=Takeaway: Autoregressive models require autoregressive decomposition,boxsep=0.5mm, colframe=low,]\footnotesize{
    While words such as `which', `therefore', etc. may not seem like important steps to humans, they actually represent important steps for autoregressive LLMs which are trained on next token prediction.}
\end{tcolorbox}
\vspace{-0.2cm}  