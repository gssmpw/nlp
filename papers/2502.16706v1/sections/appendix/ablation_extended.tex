\section{Ablation studies}

\subsection{Ablation on Temperature}
\label{sec:ab_temp}

We conduct an ablation study to analyze the effects of temperature on \decomp and BoN. Temperature controls the randomness of token sampling in autoregressive models, influencing both exploration and consistency. Higher temperatures encourage more diverse outputs, whereas lower temperatures yield more deterministic generations. To examine its impact, we evaluate \decomp and BoN on a 100-problem subset of APPS (the first 100 problems) using gpt-4o-mini.

\Cref{fig:temp_token} presents the Pass@token scaling curve for \decomp across different temperatures. The results indicate that lower temperatures lead to improved performance, as \decomp benefits from more deterministic step selection. Unlike BoN, which relies on broad solution sampling, \decomp dynamically refines steps, making stable token probabilities advantageous.

\Cref{fig:temp_actualpart} illustrates the frequency of actual partitions made by \decomp at different temperatures. As temperature increases, the number of partitions fluctuates more, suggesting that high temperature introduces instability in step selection. Lower temperatures provide more structured decomposition, reducing unnecessary subdivisions.

In \Cref{fig:temp_rewardstep}, we visualize the mean reward per step. The trend shows a linear increase in reward as step number grows, demonstrating that deeper decomposition results in progressively better solutions. This reinforces that \decomp effectively allocates computation towards refining difficult steps.

The mean standard deviation per step is shown in \Cref{fig:temp_stdstep}. Lower temperatures yield lower standard deviations, confirming that \decomp benefits from reduced variability in sample quality. This consistency allows for more reliable prioritization of difficult steps, enhancing overall inference efficiency.

For comparison, \Cref{fig:temp_bon_token} and \Cref{fig:temp_bon_passk} display Pass@token and Pass@k scaling curves for BoN across different temperatures. Unlike \decomp, BoN achieves peak performance at a temperature around 0.6-0.8, balancing diversity and consistency. Higher temperatures increase exploration but degrade precision, while lower temperatures hinder sample diversity, reducing the probability of obtaining high-quality completions.

These findings highlight the fundamental difference between \decomp and BoN: \decomp benefits from lower variance and stable decomposition, while BoN relies on broader exploration facilitated by moderate temperature settings. As a result, optimal temperature settings differ significantly between these methods, with \decomp favoring deterministic sampling and BoN requiring a balance between diversity and coherence.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.5\linewidth]{graphics/temp_token.pdf}
    \caption{\textbf{Pass@token scaling curve for different temperatures on APPS using gpt-4o-mini}. The lower the temperature, the stronger the \decomp performance.}
    \label{fig:temp_token}
\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.5\linewidth]{graphics/actualpart_temp.pdf}
    \caption{\textbf{Partition frequency of \decomp with different temperatures on APPS using gpt-4o-mini}}
    \label{fig:temp_actualpart}
\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.5\linewidth]{graphics/meanreward_temp.pdf}
    \caption{\textbf{Mean reward per step of \decomp with different temperatures on APPS using gpt-4o-mini}. The mean reward scales linearly with step number.}
    \label{fig:temp_rewardstep}
\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.5\linewidth]{graphics/std_temp.pdf}
    \caption{\textbf{Mean standard deviation per step of \decomp with different temperatures on APPS using gpt-4o-mini}. Lower temperature means lower average standard deviation.}
    \label{fig:temp_stdstep}
\end{figure}


\begin{figure}[ht]
    \centering
    \includegraphics[width=0.5\linewidth]{graphics/temp_bon_passk.pdf}
    \caption{\textbf{Pass@k scaling curve for different temperatures on APPS using gpt-4o-mini for BoN}. A temperature around 0.6-0.8 leads to the best performance and balance between diversity and consistency.}
    \label{fig:temp_bon_passk}
\end{figure}


\begin{figure}[ht]
    \centering
    \includegraphics[width=0.5\linewidth]{graphics/temp_bon_token.pdf}
    \caption{\textbf{Pass@token scaling curve for different temperatures on APPS using gpt-4o-mini for BoN}. A temperature around 0.6-0.8 leads to the best performance and balance between diversity and consistency.}
    \label{fig:temp_bon_token}
\end{figure}


\newpage
% \subsection{Ablation on priority metric $h$}
% \label{sec:ab_prioritymetric}
% We include some more analysis on how the priority metric affects \decomp here. We tested them the first 200 competition level APPS problems with gpt-4o-mini. All methods used a temperature of 0.8.
% % describe the figures here in more detail and what they mean. 
% We see that the choice of priority metric does have a strong affect on performance as seen in Fig. \ref{fig:metric_token}, where both \decomp-Q and \decomp-Z perform better than random and their opposites. 

\subsection{Ablation on Priority Metric $h$}
\label{sec:ab_prioritymetric}

We analyze the effect of different priority metrics on \decomp performance. We evaluate \decomp using the first 200 competition-level APPS problems with gpt-4o-mini, setting the temperature to 0.8 for all experiments. The priority metric determines which steps are refined during recursive decomposition, impacting both efficiency and final solution quality.

\Cref{fig:metric_token} presents a token-level comparison of different priority metrics. Both \decomp-Q and \decomp-Z significantly outperform random selection and their inverse counterparts, demonstrating the importance of prioritizing high-value steps.

\Cref{fig:metric_actualpart} illustrates the partition frequency under different priority metrics. We observe that effective metrics such as \decomp-Q and \decomp-Z lead to fewer, more meaningful partitions, whereas suboptimal strategies result in excessive, redundant partitioning.

The relationship between mean reward and step number is shown in \Cref{fig:metric_rewardstep}. All tested metrics exhibit a strong correlation between increasing step depth and mean reward, indicating that decomposition progressively refines solutions. However, \decomp-Q and \decomp-Z achieve higher reward gains at earlier stages, suggesting that they prioritize the most impactful refinements.

Finally, \Cref{fig:metric_stdstep} reports the standard deviation of rewards per step. Lower standard deviation suggests more stable solution quality, a property that \decomp-Q and \decomp-Z maintain better than random selection methods. This highlights their effectiveness in identifying and refining challenging steps efficiently.

Overall, these results confirm that choosing an appropriate priority metric is crucial for \decomp. While \decomp-Q and \decomp-Z consistently enhance inference efficiency and quality, random or inverse strategies lead to poorer performance due to misallocation of compute resources.


\begin{figure}[ht]
    \centering
    \includegraphics[width=0.5\linewidth]{graphics/metric_token.pdf}
    \caption{\textbf{Token level comparison of different priority metrics on \decomp in the APPS setting with gpt-4o-mini.} Both Q and Z based priority metrics perform well.}
    \label{fig:metric_token}
\end{figure}


\begin{figure}[ht]
    \centering
    \includegraphics[width=0.5\linewidth]{graphics/metric_actual.pdf}
    \caption{\textbf{Partition frequency of \decomp with different priority metrics on APPS using gpt-4o-mini}}
    \label{fig:metric_actualpart}
\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.5\linewidth]{graphics/metric_mean.pdf}
    \caption{\textbf{Mean reward per step of \decomp with different priority metrics on APPS using gpt-4o-mini}. All metrics display strong correlation between step depth and the mean reward.}
    \label{fig:metric_rewardstep}
\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.5\linewidth]{graphics/metric_std.pdf}
    \caption{\textbf{Mean standard deviation per step of \decomp with different priority metrics on APPS using gpt-4o-mini}. All metrics display correlation between step depth and the standard deviation.}
    \label{fig:metric_stdstep}
\end{figure}


\newpage
% \subsection{Model ablation}
% \label{sec:model_ablation}
% Different models were tested with \decomp on 200 APPS competition level problems with a sample budget of 30. The groundtruth reward model was used. LLMs were set to temperature 0.8. Due to the challenging nature of the benchmark, open source models performed poorly on their own. However, with \decomp, performance dramatically increased. 
\subsection{Model Ablation}
\label{sec:model_ablation}

We investigate how different LLMs perform when used with \decomp on 200 competition-level APPS problems, given a sample budget of 30. The groundtruth reward model was used to evaluate correctness, and all models were set to a temperature of 0.8. Due to the challenging nature of the benchmark, open-source models struggled to achieve strong performance independently. However, when paired with \decomp, their performance significantly improved.

\Cref{fig:opensource_token} presents the Pass@token scaling curve for open-source models using \decomp. The results demonstrate that \decomp substantially enhances the capabilities of these models, closing the gap between them and proprietary alternatives.

\Cref{fig:open_actualpart} visualizes the partition frequency of \decomp with different open-source models. Compared to their standalone performance, the use of \decomp led to more structured and effective decomposition, highlighting its adaptability to different architectures.

The mean reward per step is shown in \Cref{fig:open_rewardstep}. Similar to prior findings, we observe that deeper decomposition leads to increasingly higher rewards. Notably, even lower-capacity models benefit from \decomp’s ability to iteratively refine their solutions.

Finally, \Cref{fig:open_stdstep} presents the mean standard deviation per step. With \decomp, the variance in performance is significantly reduced, resulting in more stable and reliable inference.

Overall, these findings emphasize that \decomp is a robust framework capable of enhancing inference performance across diverse LLMs, particularly those with limited standalone capabilities.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.5\linewidth]{graphics/opensource_token.pdf}
    \caption{\textbf{Pass@token scaling curve for open source models with \decomp on APPS}. \decomp also demonstrates strong performance gains with open source models.}
    \label{fig:opensource_token}
\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.5\linewidth]{graphics/open_actualpart.pdf}
    \caption{\textbf{Partition frequency of \decomp with open source models on APPS}}
    \label{fig:open_actualpart}
\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.5\linewidth]{graphics/open_rewardstep.pdf}
    \caption{\textbf{Mean reward per step of \decomp with open source models on APPS}}
    \label{fig:open_rewardstep}
\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.5\linewidth]{graphics/open_stdstep.pdf}
    \caption{\textbf{Mean standard deviation per step of \decomp with open source models on APPS}}
    \label{fig:open_stdstep}
\end{figure}


\newpage
\subsection{Ablation on partition fraction $\alpha$}
\label{sec:ab_alphafraction}
We include some more analysis on the partition fraction here. 
% describe the figures here in more detail and what they mean. 

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.5\linewidth]{graphics/alpha_token.pdf}
    \caption{\textbf{Token level comparison of different \decomp splitting fraction $\alpha$ on APPS competition level.} $0.15 \le \alpha \le 0.25$ seems to be optimal.}
    \label{fig:alpha_token}
\end{figure}