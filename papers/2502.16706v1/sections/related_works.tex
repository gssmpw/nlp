% \section{Additional Related Work}
% \textbf{Inference scaling.} Inference scaling \citep{snell2024scaling, brown2024large, manvi2024adaptive, leea2025evolving} has quickly become a new dominant paradigm with the introduction of o1 and r1 like chain of thought reasoning models. 
% Many works have proposed a trade-off between scaling inference compute and training compute \citep{ guan2025rstarmathsmallllmsmaster, chen2024think}. Typically, LLM inference involves decomposing complex questions into sequential intermediate steps that lead to the final
% answer, exemplified by chain-of-thought (CoT) prompting~\cite{wei2022chain, sprague2024cotcot, wang2024chainofthoughtr} and its variants~\cite{kojima2022large,zhouleast, wangself, li2023making}.

% \textbf{LLM reasoning and decomposition} LLM reasoning and code generation are standard tasks for inference scaling. There are also prior works that explore evolutionary inference scaling methods, including for program generation ~\citep{liventsev2023fully, chen2023evoprompting, romera2024mathematical, lehman2023evolution, hemberg2024evolving}. There are also works that use domain specific decomposition strategies for code such as decomposing based on functions ~\citep{chen2024divide, zenkner2024abstractbeam, levin2025effective}. Often, decomposition refers to using prompts to prompt for completion of subtasks from a LLM agent ~\citep{hernandez2recursive, khot2022decomposed, dua2022successive}, which is different from decomposing a single LLM generation.  

% \textbf{Reinforcement Learning (RL) and Monte-Carlo method} Unlike standard RL, the setting we considered is closer to the search problem, where the primary goal is to find a single path that has the maximum reward.   \citet{cazenave2009nested} showing that the nested Monte-Carlo search can accelerate exploration for the best path.
% Under the bandit setting, the problem can be formulated as identifying the arm with the highest \textbf{maximum} reward rather than the one with the highest mean reward~\citep{cicirello2005max,carpentier2014extreme}. 

\vspace{-0.2cm}
\section{Related Work}
\vspace{-0.2cm}
% \textbf{Inference scaling.}  
% Inference scaling has emerged as a dominant paradigm, driven by the introduction of o1- and r1-like chain-of-thought reasoning models~\citep{snell2024scaling, brown2024large, manvi2024adaptive, leea2025evolving}. Several works examine the trade-off between inference compute and training compute~\citep{guan2025rstarmathsmallllmsmaster, chen2024think}. LLM inference often relies on decomposing complex problems into intermediate reasoning steps, as seen in chain-of-thought (CoT) prompting~\citep{wei2022chain, sprague2024cotcot, wang2024chainofthoughtr} and its variants~\citep{kojima2022large, zhouleast, wangself, li2023making}. We build upon the inference scaling literature by introducing a new way to adapt inference compute ~\citep{manvi2024adaptive}.
\textbf{Inference scaling.}  
Inference scaling has emerged as a dominant paradigm, driven by the introduction of o1- and r1-like chain-of-thought reasoning models~\citep{snell2024scaling, brown2024large, manvi2024adaptive, leea2025evolving}. Several works examine the trade-off between inference compute and training compute~\citep{guan2025rstarmathsmallllmsmaster, chen2024think}. LLM inference often relies on decomposing complex problems into intermediate reasoning steps, as seen in chain-of-thought (CoT) prompting~\citep{wei2022chain, sprague2024cotcot, wang2024chainofthoughtr} and its variants~\citep{kojima2022large, zhouleast, wangself, li2023making}. We extend inference scaling by introducing a new approach for adaptive compute allocation~\citep{manvi2024adaptive}.


\textbf{LLM reasoning and code generation.}  
LLM reasoning and code generation are central tasks for inference scaling. Evolutionary inference scaling methods have been explored in program generation~\citep{liventsev2023fully, chen2023evoprompting, romera2024mathematical, lehman2023evolution, hemberg2024evolving}. Domain-specific decomposition strategies have been applied in code generation, such as function-based decomposition~\citep{chen2024divide, zenkner2024abstractbeam, levin2025effective}. More broadly, decomposition often involves prompting LLMs to generate subtask completions~\citep{hernandez2recursive, khot2022decomposed, dua2022successive}, which differs from methods that refine a single LLM generation.

\textbf{Reinforcement learning and Monte Carlo methods.}  
Unlike standard RL, our setting resembles a search problem where the goal is to identify the single highest-reward path. \citet{cazenave2009nested} demonstrated that nested Monte Carlo search can accelerate optimal pathfinding. Under the bandit setting, this can be formulated as identifying the arm with the highest \emph{maximum} reward rather than the highest mean reward~\citep{cicirello2005max, carpentier2014extreme}.
