\vspace{-0.4cm}
\section{Conclusion}  
\vspace{-0.2cm}
We introduce \decomp, a dynamic decomposition framework that adaptively partitions solution steps based on difficulty, improving inference scaling by directing compute toward critical steps while balancing exploration and resource allocation. 
\decomp seamlessly integrates with search-based methods such as MCTS and beam search, further enhancing performance.
% \decomp integrates with search-based methods like MCTS and beam search, further enhancing performance. 
It also identifies challenging steps for LLMs, aiding curriculum learning, fine-tuning, and dataset augmentation. By dynamically adjusting partitioning based on available compute, \decomp enables more adaptive and efficient reasoning in large language models, with broad implications for both training and inference optimization.

% We introduce \decomp, a dynamic decomposition framework that adaptively partitions solution steps based on difficulty, significantly improving inference scaling by directing compute toward critical steps while efficiently balancing exploration and resource allocation. Beyond inference scaling, \decomp can be integrated with existing search-based methods such as MCTS and beam search, further enhancing their performance. Additionally, \decomp provides insights into which steps are most challenging for LLMs, offering a principled way to identify key steps for curriculum learning, fine-tuning, and targeted dataset augmentation. By dynamically adjusting partitioning based on available compute, \decomp lays the foundation for more adaptive and efficient reasoning strategies in large language models, with broad implications for both training and inference optimization.

