% \section{Experimental Results}
% \subsection{Benchmarks}
% We conducted experiments on APPS (competition level), LiveCodeBench, and MATH to demonstrate the effectiveness of decomposition on inference scaling. \textbf{APPS} is a set of 5000 challenging, code competition problems ~\citep{hendrycks2021measuring} with three levels of difficulty, competition level being the most difficult. 
% There is still much room for improvement on APPS, making it a great benchmark for evaluation. 
% % The SOTA performance for a base LLM on APPS competition level is around [number] [cite], indicating room for inference scaling improvements. 
% Due to computational constraints, we test on a 200 problem subset of APPS. 

% MATH ~\citep{hendrycks2021measuring-MATH} is a set of 12,500 mathematical problems. 
% Since the groundtruth verfier outputs binary rewards, hence offering no guidance signal, we used a pretrained ORM \citep{xiong2024rlhflowmath} to provide the reward signal. This ORM was trained using the same method as in \citep{wang2024math}, with Llama-3.1-8B-Instruct as the base model. 
% Due to computational constraints, we test on a 500 problem subset, \textbf{MATH500}, of the entire 5000 test set for MATH, that is identical to the subset used in prior works \citep{wang2024math, lightman2023let}.

% LiveCodeBench is a set of continuously updated competition code problems from Leetcode, atCoder, and codeForces ~\citep{jain2024livecodebench}. Since LiveCodeBench is continuously updated with new problems, it is contamination free (LLMs have not seen the problems before), making it a great benchmark for accurate assessment.
% We evaluate on 108 problems selected from 10/01/2024 until 12/01/2024, which guarantees that the models we tested have not been contaminated by the problems. 
\vspace{-0.2cm}  
\section{Experimental Results}
\vspace{-0.2cm}  
\subsection{Benchmarks}
We evaluate DISC on three benchmarks: \textbf{APPS}, \textbf{MATH}, and \textbf{LiveCodeBench}, to assess its impact on inference scaling for both coding and reasoning. 

\textbf{APPS}~\citep{hendrycks2021measuring} consists of 5000 competitive programming problems across three difficulty levels, with the competition-level subset being the hardest. We evaluate on a 200-problem subset due to computational constraints.
\textbf{MATH}~\citep{hendrycks2021measuring-MATH} comprises 12,500 math problems. Since the ground-truth verifier provides only binary rewards, we use a pretrained ORM~\citep{xiong2024rlhflowmath}, trained via the method in~\citep{wang2024math}, with Llama-3.1-8B-Instruct as the base model. We test on a 500-problem subset (\textbf{MATH500}), identical to prior work~\citep{wang2024math, lightman2023let}.
\textbf{LiveCodeBench}~\citep{jain2024livecodebench} is a continuously updated dataset from Leetcode, AtCoder, and CodeForces, ensuring LLMs have not been exposed to test problems. We evaluate on the 108 problems uploaded between 10/01/2024 and 12/01/2024 to prevent contamination.
\begin{figure}[ht]
\vspace{-0.3cm}
    \centering
    \includegraphics[width=0.75\linewidth]{graphics/apps_token.pdf}
   \vspace{-0.3cm}
    \caption{\textbf{Token-level comparison on APPS competition level.} \decomp consistently outperforms prior decomposition methods in efficiency.}
    \label{fig:apps_token}
\vskip -0.1in
\end{figure}

\begin{figure}[ht]
\vspace{-0.2cm}
    \centering
    \includegraphics[width=0.7\linewidth]{graphics/math_token.pdf}
 \vspace{-0.3cm}
    \caption{\textbf{Token-level comparison on MATH500.} \decomp outperforms other decomposition methods, demonstrating its effectiveness in mathematical reasoning.}
    \label{fig:math_token}
\vskip -0.1in
\end{figure}

\begin{figure}[ht]
    \centering
    \vspace{-0.3cm}\includegraphics[width=0.75\linewidth]{graphics/live_token.pdf}\vspace{-0.3cm}
    \caption{\textbf{Token-level comparison on LiveCodeBench.} \decomp achieves superior inference scaling over TokenSplit, LineSplit, and BoN.}
    \label{fig:live_token}\vspace{-0.3cm}
\end{figure}
\subsection{Decomposition Comparison}
\label{sec:scaling_methods}

We compare \decomp against three prior decomposition methods from Sec.~\ref{sec:prior_decomp}: \textbf{TokenSplit} (token-level decomposition), \textbf{LineSplit} (newline-based decomposition), and \textbf{BoN} (treating the entire solution as a single step). 

Across all benchmarks, \decomp achieves superior scaling and performance under both fixed token budgets (Fig.~\ref{fig:live_token}) and sample budgets (Fig.~\ref{fig:passk_val_comparison}). We evaluate two key metrics: \textbf{Pass@k}, the proportion of problems solved within a sample budget $k$, and \textbf{Pass@token}, the proportion solved within a given token budget. 
Notably, \decomp consistently outperforms static decomposition methods on APPS, MATH, and LiveCodeBench (Fig.~\ref{fig:apps_token},~\ref{fig:math_token},~\ref{fig:live_token}), demonstrating its ability to allocate compute adaptively for improved inference efficiency. Extended results and analyses for each benchmark are provided in App.~\ref{sec:apps_extended},~\ref{sec:math500_extended}, and~\ref{sec:livecodebench_extended}.  





% \subsection{Decomposition comparison}
% \label{sec:scaling_methods}
% We compare against the three prior decomposition methods described in Sec. \ref{sec:prior_decomp} as baselines. \textbf{TokenSplit} decomposes the solution into tokens. \textbf{LineSplit} treats each newline as a step. \textbf{BoN} treats the entire solution as a single step. 
% On all three benchmarks, \decomp consistently displays better scaling and performance given the same budget (Fig. \ref{fig:apps_passk}, \ref{fig:math_token}, \ref{fig:apps_token}). 
% This includes both scaling given the same token budget (Fig. \ref{fig:live_token}) and the same sample budget (Fig. \ref{fig:live_passk}). 
% % Show how \decomp performs better than other decomposition methods on APPS, MATH, and LiveCodeBench.
% More details and analysis on APPS, MATH, and LiveCodeBench experiments can be found in App. \ref{sec:apps_extended}, App. \ref{sec:math500_extended}, and App. \ref{sec:livecodebench_extended} respectively. 

% \begin{figure}
% \vskip -0.1in
%     \centering
%     \includegraphics[width=0.8\linewidth]{graphics/decomp_passk.pdf}
% \vskip -0.1in
%     \caption{\textbf{Pass@k comparison of different decomposition methods on APPS competition level.} \decomp consistently scales better than other decomposition methods.}
%     \label{fig:apps_passk}
% \end{figure}

% \begin{figure}
% \vskip -0.1in
%     \centering
%     \includegraphics[width=0.8\linewidth]{graphics/decomp_token.pdf}
%     \vskip -0.1in
%     \caption{\textbf{Token level comparison of different decomposition methods on APPS competition level.} \decomp consistently scales better than other decomposition methods.}
%     \label{fig:apps_token}
% \vskip -0.1in
% \end{figure}




% \begin{figure}
% \vskip -0.1in
%     \centering
%     \includegraphics[width=0.8\linewidth]{graphics/math_token.pdf}
%     \vskip -0.1in
%     \caption{\textbf{Token level comparison of different decomposition methods on MATH500.} \decomp consistently scales better than other decomposition methods.}
%     \label{fig:math_token}
% \vskip -0.1in
% \end{figure}

% \begin{figure}
%     \centering
%     \includegraphics[width=0.8\linewidth]{graphics/live_token.pdf}
%     \caption{\textbf{Token level comparison of different decomposition methods on LiveCodeBench.} \decomp consistently scales better than other decomposition methods.}
%     \label{fig:live_token}
% \end{figure}

% \begin{figure}
%     \centering
%     \includegraphics[width=0.85\linewidth]{graphics/decomp_passk.pdf}
%     \caption{\textbf{Pass@k comparison of different decomposition methods on APPS competition level.} \decomp consistently scales better than other decomposition methods.}
%     \label{fig:metric_estimation}
% \end{figure}

\subsection{Decomposition Analysis and Interpretation}

Our results strongly indicate that decomposition—whether line-based, token-based, or \decomp—improves sample quality. 
Figures~\ref{fig:avg_reward_step} and~\ref{fig:std_per_step} illustrate how the mean and variance of sampled rewards evolve with the \textbf{step number}, which represents the order in which a step is explored. Higher step numbers correspond to deeper search levels, where solutions are partitioned into finer-grained steps. As shown in Fig.~\ref{fig:avg_reward_step}, increasing step number correlates with higher-quality solutions, demonstrating that finer-grained decomposition improves sample quality. 
Additionally, Fig.~\ref{fig:std_per_step} shows that reward variance decreases as step count increases, highlighting how decomposition enhances sampling precision.

Furthermore, \decomp achieves better performance with fewer partitions under a fixed sampling budget (Fig.~\ref{fig:partition_comparison}). We distinguish between \textbf{actual partitions}, the number of steps effectively explored, and \textbf{planned partitions}, the number of partitions intended by the method. Token and line split methods generate a large number of planned partitions (Fig.~\ref{fig:partition_comparison}) but search over at most 15 steps due to budget constraints. In contrast, \decomp dynamically adjusts the number of partitions based on available budget, efficiently identifying and focusing on critical steps.

\begin{tcolorbox}[title=Takeaway: Decomposition and sample quality,boxsep=0.5mm,  colframe=low,]\footnotesize{
    Finer-grained decomposition improves sampled solution quality and reduces reward variance.}
\end{tcolorbox}

\begin{figure}
    \centering
    \vspace{-0.3cm}\includegraphics[width=0.7\linewidth]{graphics/reward_per_step.pdf}
    \vspace{-0.3cm}
    \caption{\textbf{Average reward per step, averaged over APPS problems.} From step 3 onward, higher step counts strongly correlate with increased average reward, demonstrating the effectiveness of decomposition. The dip between steps 1 and 3 likely occurs because simple problems are solved early, preventing further search.}
    \vspace{-0.3cm}
    \label{fig:avg_reward_step}
\end{figure}

\begin{figure}
    \centering
\vspace{-0.3cm}    
\includegraphics[width=0.7\linewidth]{graphics/standard_deviation_of_rewards_per_step.pdf}
    \vspace{-0.3cm}
    \caption{\textbf{Standard deviation of rewards per step, averaged over APPS problems.} Decomposition reduces sampling variance, improving precision at deeper search depths.}
    \label{fig:std_per_step}
\end{figure}

\begin{figure}[t]
    \centering
    \begin{minipage}{0.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{graphics/frequency_of_actual_partitons.pdf}
        \label{fig:actual_part_freq}
    \end{minipage}
    \hfill
    \begin{minipage}{0.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{graphics/frequency_of_planned_partitons.pdf}
        \label{fig:planned_part_freq}
    \end{minipage}
 \vspace{-0.7cm}
    \caption{\textbf{Comparison of actual and planned partitions on APPS.}  
    \decomp outperforms other methods with fewer partitions by efficiently identifying critical steps. Unlike token and line split methods, which plan many partitions but search only a subset, \decomp dynamically adjusts partitioning based on budget.}
    \vspace{-0.3cm}
    \label{fig:partition_comparison}
\end{figure}

\subsection{Interaction Between Temperature and \decomp}

\begin{figure}[ht]
    \centering
    \vspace{-0.1cm}
    \includegraphics[width=0.75\linewidth]{graphics/temp_passk.pdf}
    \vspace{-0.3cm}
    \caption{\textbf{Effect of temperature on \decomp performance on APPS with gpt-4o-mini.} Unlike BoN and other inference scaling methods, \decomp achieves higher performance at lower temperatures.}
    \label{fig:disc_temp_passk}\vspace{-0.5cm}
\end{figure}

We perform ablation studies to analyze the impact of temperature on \decomp. 
Typically, inference scaling methods achieve optimal performance at temperatures around 0.6–0.8, as increased temperature promotes sample diversity~\citep{wang2024planning}. 
Surprisingly, however, \decomp performs \emph{better at lower temperatures}, as shown in Fig.~\ref{fig:disc_temp_passk}. This trend is in stark contrast to BoN (Fig.~\ref{fig:temp_bon_token}), where higher temperatures are generally beneficial.

We believe this phenomenon arises because \decomp depends on accurately estimating the priority metric $h$ at each step. Lower temperatures reduce sample variance, leading to more reliable estimates of $h$, which in turn improves step selection. 
This is further supported by Fig.~\ref{fig:temp_stdstep}, which shows that lower temperatures yield lower standard deviations per step, indicating increased sampling consistency.
Additional details and analyses can be found in App.~\ref{sec:ab_temp}.


% \subsection{Interaction between temperature and \decomp}

% \begin{figure}[ht]
%     \centering
%     \includegraphics[width=0.8\linewidth]{graphics/temp_passk.pdf}
%     \caption{\textbf{Interaction between temperature and \decomp on APPS with gpt-4o-mini.} Remarkably, unlike BoN and other inference scaling methods, the lower the temperature, the better the performance for \decomp.}
%     \label{fig:disc_temp_passk}
% \end{figure}

% We conducted ablation studies on the effects of temperature on \decomp. 
% Conventionally, temperatures of around 0.6-0.8 provide the best performance for inference scaling methods since they rely on the temperature to produce diverse samples ~\citep{wang2024planning}. 
% Counterintuitively, \emph{the lower temperature, the better the performance for} \decomp, as shown in Fig. \ref{fig:disc_temp_passk}, unlike BoN as shown in Fig. \ref{fig:temp_bon_token}. 
% We suspect this because \decomp relies on accurate estimation of the priority metric $h$ when sampling. Lower temperatures increase the accuracy of samples. 
% This is supported by the fact that the standard deviation per step is lower for lower temperatures than for higher temperatures, as shown in Fig. \ref{fig:temp_stdstep}.
% More details are included in App. \ref{sec:ab_temp}. 
\subsection{Self-Generated Validation Tests}

We also evaluate \decomp in a more practical setting where a ground-truth reward model is unavailable for code generation ~\citep{chen2022codet, chen2023teaching, zhou2023language}. Instead of relying on predefined test cases, we prompt the LLM to generate validation test cases based on the problem prompt. In real-world applications, manually curated ground-truth test cases are often costly to obtain, making self-generated validation a more scalable approach. The results, shown in Fig.~\ref{fig:passk_val_comparison}, indicate that \decomp continues to scale better than other methods in this setting. Additional results and details are provided in App.~\ref{sec:val_tests}. 

% Following standard practice in code generation research ~\citep{ref}, We report \textbf{Pass@any}, where a problem is considered solved if at least one generated solution passes the self-generated tests.
\begin{figure}[t]
    \centering
    \begin{minipage}{0.465\linewidth}
        \centering
        % \vspace{-0.2cm}
        \includegraphics[width=\linewidth]{graphics/val_passk_scaling.pdf}
        \vspace{-0.3cm}
        % \caption{\textbf{Pass@k comparison of decomposition methods on APPS competition level with self-generated validation tests using gpt-4o-mini.} \decomp still scales better than other methods, though its improvement rate is slightly lower compared to settings with ground-truth validation.}
        % \label{fig:val_passk}
    \end{minipage}
    \hfill
    \begin{minipage}{0.515\linewidth}
        \centering
        \vspace{-0.2cm}
        \includegraphics[width=\linewidth]{graphics/decomp_passk.pdf}
        % \vspace{-0.3cm}
        % \caption{\textbf{Pass@k comparison of decomposition methods on APPS competition level.} \decomp scales more effectively than TokenSplit, LineSplit, and BoN.}
        % \label{fig:apps_passk}
    \end{minipage}\vspace{-0.3cm}
    \caption{\textbf{Comparison of Pass@k performance on APPS with ground truth tests (right) and self generated validation tests (left) using gpt-4o-mini.} \decomp scales more effectively  in both.}
    % However, its improvement rate is slightly lower when using self-generated validation compared to ground-truth validation.
    \label{fig:passk_val_comparison}
    \vspace{-0.3cm}
\end{figure}


% \begin{figure}
%     \centering
%     \vspace{-0.3cm}\includegraphics[width=0.73\linewidth]{graphics/val_passk_scaling.pdf}
%     \vspace{-0.3cm}
%     \caption{\textbf{Pass@k comparison of decomposition methods on APPS competition level with self-generated validation tests using gpt-4o-mini.} \decomp still scales better than other methods, though its improvement rate is slightly lower compared to settings with ground-truth validation.}
%     \label{fig:val_passk}\vspace{-0.3cm}
% \end{figure}

% \begin{figure}[ht]\vspace{-0.3cm}
%     \centering
%     \includegraphics[width=0.8\linewidth]{graphics/decomp_passk.pdf}
% \vspace{-0.3cm}
%     \caption{\textbf{Pass@k comparison of decomposition methods on APPS competition level.} \decomp scales more effectively than TokenSplit, LineSplit, and BoN.}
%     \label{fig:apps_passk}
% \end{figure}

% \subsection{Self-generated validation tests}
% We also conduct experiments when the ground truth reward model is not available for code generation. Instead, we ask the LLM to generate validation test cases based on the problem prompt instead. In most practical applications, we do not have access to ground-truth test cases since they involve significant manual costs to generate, making this a more practical experimental setup. We report the pass@any rate here -- if any of the solutions generated by the method is correct that is considered a pass. This is the same setup adopted in many other code generation papers [ref]. These results are shown in Fig. \ref{fig:val_passk}, with additional results and details in App. \ref{sec:val_tests}. 

% \begin{figure}
%     \centering
%     \includegraphics[width=0.8\linewidth]{graphics/val_passk_scaling.pdf}
%     \caption{\textbf{Pass@k comparison of different decomposition methods on APPS competition level when validation tests are self-generated with gpt-4o-mini.} \decomp still scales better than other methods in this setting, albeit at a lower rate.}
%     \label{fig:val_passk}
% \end{figure}

\subsection{Ablation on Base LLM Model}

We evaluate \decomp across different LLMs, including open-source models. As shown in Fig.~\ref{fig:opensource_passk} and Fig.~\ref{fig:opensource_token}, \decomp significantly enhances performance even for weaker models. Specifically, it improves Llama's pass rate from 1\% to 5.5\%, a \emph{550\% relative increase}, and Mistral’s from 0\% to 3.5\%, demonstrating substantial gains even from a nonzero baseline. Additional details and analyses are in App.~\ref{sec:model_ablation}.

\begin{figure}
    \centering
    \vspace{-0.3cm}\includegraphics[width=0.75\linewidth]{graphics/opensource_passk.pdf}
    \vspace{-0.3cm}
    \caption{\textbf{Pass@k scaling curve for open-source models with \decomp on APPS.} \decomp substantially improves performance across different LLMs, including Llama and Mistral.}
    \vspace{-0.6cm}
    \label{fig:opensource_passk}
\end{figure}


% \subsection{Ablation on base LLM model}
% We also tested \decomp with other LLM models, including open source ones as shown in Fig. \ref{fig:opensource_passk} and \ref{fig:opensource_token}, where \decomp improves the performance of Llama from 1\% to 5.5\%, a 550\% increase in performance, and Mistral from 0\% to 3.5\%, a massive increase in performance. 
% More details are included in App. \ref{sec:model_ablation}. 
% \begin{figure}
%     \centering
%     \includegraphics[width=0.8\linewidth]{graphics/opensource_passk.pdf}
%     \caption{\textbf{Pass@k scaling curve for open source models with \decomp on APPS}. \decomp can also improve the performance of open source models.}
%     \label{fig:opensource_passk}
% \end{figure}

\subsection{Interaction Between Priority Metric and \decomp}

We conduct an ablation study to examine how the choice of priority metric affects \decomp performance. In addition to the \textbf{Q-based} and \textbf{Z-based} priority metrics (\decomp-Q and \decomp-Z) introduced in Sec.~\ref{sec:priority_metric}, we evaluate three baselines: \textbf{\decomp-R} (random step selection), \textbf{\decomp-negQ}, and \textbf{\decomp-negZ} (which prioritize the opposite steps of \decomp-Q and \decomp-Z, respectively). As shown in Fig.~\ref{fig:metric_passk}, the selection of a priority metric significantly impacts performance. Both \decomp-Q and \decomp-Z \emph{significantly} outperform random selection and their inverse counterparts, demonstrating the effectiveness of their priority heuristics. Additional details and analysis are in App.~\ref{sec:ab_prioritymetric}.

\begin{figure}[ht]
    \centering
    \vspace{-0.2cm}\includegraphics[width=0.75\linewidth]{graphics/metric_passk.pdf}
    \vspace{-0.3cm}
    \caption{\textbf{Effect of priority metrics on \decomp performance on APPS with gpt-4o-mini.} Both Q-based and Z-based priority metrics outperform random selection and their inverses, highlighting their effectiveness.}
    \label{fig:metric_passk}
\end{figure}


% \subsection{Interaction between priority metric and \decomp}
% We also conducted an ablation study on how the priority metric affects \decomp performance. In addition to Q and Z based priority metrics (\decomp-Q and \decomp-Z) that we introduced in Sec. \ref{sec:priority_metric}, we also tested out random selection (\decomp-R) and splitting the opposite step as Q and Z based priority metrics (\decomp-negQ, \decomp-negZ) as baselines and controls. 
% We see that the choice of priority metric does have a strong affect on performance as seen in Fig. \ref{fig:metric_passk}, where both \decomp-Q and \decomp-Z perform better than random and their opposites. 
% More details in App. \ref{sec:ab_prioritymetric}.
% \begin{figure}[ht]
%     \centering
%     \includegraphics[width=0.8\linewidth]{graphics/metric_passk.pdf}
%     \caption{\textbf{Interaction between the priority metric and \decomp on APPS with gpt-4o-mini.} Both Q and Z based priority metrics perform well.}
%     \label{fig:metric_passk}
% \end{figure}

\subsection{Ablation on Partition Fraction $\alpha$}

We conduct an ablation study to analyze the effect of the partition fraction $\alpha$ on \decomp performance. As shown in Fig. ~\ref{fig:alpha_passk} and ~\ref{fig:alpha_token}, the optimal range appears to be $0.15 \leq \alpha \leq 0.25$. Lower partition fractions ($\alpha < 0.5$) tend to perform better due to the \emph{asymmetric cost of sampling from different halves of the partition}. Sampling from the first half requires generating more tokens, while the second half requires fewer, making it crucial to partition the first half more conservatively. Additional analysis are in App.~\ref{sec:ab_alphafraction}.

\begin{figure}
    \centering
    \vspace{-0.35cm}\includegraphics[width=0.75\linewidth]{graphics/alpha_passk.pdf}
    \vspace{-0.3cm}
    \caption{\textbf{Pass@k comparison of different \decomp partition fractions $\alpha$ on APPS competition level.} The range $0.15 \leq \alpha \leq 0.25$ appears optimal.}\vspace{-0.35cm}
    \label{fig:alpha_passk}
\end{figure}


% \subsection{Ablation on partition fraction $\alpha$}
% We conducted an ablation study on the effects of the partition fraction $\alpha$ on the performance. We found that a partition fraction of $0.15 \le \alpha \le 0.25$ seems to be optimal as shown in Fig. \ref{fig:alpha_passk} and Fig. \ref{fig:alpha_token}. The reason why lower $\alpha < 0.5$ partition fractions tend to perform better is because of the higher cost associated with sampling from the first half compared with the second half of the partition. Sampling from the former half means sampling more tokens, while the latter half requires less, meaning we need to be more conservative when partitioning out the former half. 
% More details are included in App. \ref{sec:ab_alphafraction}. 
% \begin{figure}
%     \centering
%     \includegraphics[width=0.8\linewidth]{graphics/alpha_passk.pdf}
%     \caption{\textbf{Pass@k comparison of different \decomp splitting fraction $\alpha$ on APPS competition level.} $0.15 \le \alpha \le 0.25$ seems to be optimal.}
%     \label{fig:alpha_passk}
% \end{figure}



% \subsection{Ablation on sampling threshold $\sigma$}
% \label{sec:ab_sampling}

% \subsection{Search and \decomp}
% \label{sec:search_results}
% We show that search methods such as MCTS and beam search can also be combined with \decomp. 
% As seen in Fig. \ref{fig:search_actualpart}, greedy search is able to search to higher depths and thus higher partitions given the same search budget, while both MCTS and beam search reach similar search depths. However, MCTS allocates the search budget more effectively than beam search, resulting in higher performance as shown in Fig. \ref{fig:search_passk}.
% More details in App. \ref{sec:search_extended}.

% \begin{figure}
%     \centering
%     \includegraphics[width=0.8\linewidth]{graphics/search_passk.pdf}
%     \caption{\textbf{Pass@k comparison of different search methods when combined with \decomp on APPS with gpt-4o-mini.} MCTS scales best, followed by greedy, followed by beam search.}
%     \label{fig:search_passk}
% \end{figure}
\vspace{-0.2cm}
\subsection{Search and \decomp}
\vspace{-0.1cm}
\label{sec:search_results}

We demonstrate that search methods such as \textbf{MCTS} and \textbf{beam search} can be combined with \decomp. As shown in Fig.~\ref{fig:search_actualpart} in the Appendix, greedy search explores deeper partitions given the same search budget due to its greedy nature, while MCTS and beam search reach similar, shallower depths. However, MCTS allocates the search budget more effectively than beam search, leading to higher performance, as seen in Fig.~\ref{fig:search_passk}. Additional details and analysis are in App.~\ref{sec:search_extended}.

\begin{figure}
    \centering
    \vspace{-0.2cm}\includegraphics[width=0.75\linewidth]{graphics/search_passk.pdf}
    \vspace{-0.3cm}
    \caption{\textbf{Pass@k comparison of search methods combined with \decomp on APPS with gpt-4o-mini.} MCTS scales best, followed by greedy search, then beam search (with beam size 2).}
    \label{fig:search_passk}\vspace{-0.3cm}
\end{figure}
