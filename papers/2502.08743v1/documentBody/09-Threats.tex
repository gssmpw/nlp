\section{Threats to Validity
\draftStatus{barring audits, R+R DONE}
}

We follow the framework from Yin~\cite{yinBook} to analyze threats to validity, since every study has threats~\cite{Wohlin-2012}.

\subsection{Construct Validity - did we actually measure what we thought we measured?}

The first threat in this category is that our case text may not accurately capture the concept it is supposed to represent.
To mitigate this threat, we appealed to a known taxonomy that experts at \tosdr{} curated.
Additionally, our task was to provide perception in a context-free manner.
As a result, the participants may not have felt any sense of stakes, which might exist if we were able to access people who were actually handling a policy document for a real purpose, with the intent of deciding whether or not to agree.
\revised{%
Last, it is important to collect additional information about what each case means for the user or service provider and whom does it \textit{actually} favor.
As such, validated answers from subject matter experts who can confirm or deny the party favored and the severity could provide ground truth.
Note that this is a normative approach, while our work follows a descriptive approach---the combination helps triangulate results.
} % end revised

\subsection{External Validity}

Based on our inclusion/exclusion criteria, we focused on English speakers residing in the US, who were working on the crowdsourcing platform Prolific.
As such, our results definitely do not generalize to other samples (i.e., the standard limitations of frequentist statistics).
\revised{%
Further, crowdsourcing introduces sampling biases, such as those Tang et al.~\cite{Tang2022crowdsourcing} revealed.
The most important one for the present study is that Prolific workers are not representative of the general population with respect to security and privacy knowledge.
} % end revised
However, the hope with doing a study like this is that some findings may generalize to samples that are sufficiently similar to ours.
Thus, it is highly unlikely that our observations generalize to other countries or languages, though it may be possible to generalize to more similar samples, such as English speakers residing in the US who are not working on Prolific).

\subsection{Reliability}

The biggest threat to reliability comes from our sampling procedure.
Ultimately, we wanted to gather data on a total of 243 unique cases.
However, having a single person observe all the cases would result in a very long task, so we randomly chose 5 cases for each participant to review.
This sampling strategy meant that not all cases received equal representation among the responses.
Specifically, while it is possible to write the webserver to cap the number of samples for each case, we have to sample before we assess the work quality.
We could semi-automate this based on our attention check question that we included to improve reliability, but we also based decisions to accept or reject work on the apparent quality of the rest of the responses as well.
As an example, some participants got the attention check correct but provided strange answers and a rewrite that included a ChatGPT prompt.

\subsection{Internal Validity}

While our study is descriptive in nature and thus does not have treatments, we did perform comparative statistics based on splitting the sample.
Hence, all the typical concerns associated with doing so remain.
As an example, employment status may be a proxy for a different, more important variable, such as level of education.
