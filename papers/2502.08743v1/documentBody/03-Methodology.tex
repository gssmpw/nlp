\section{Methodology
\draftStatus{few fixes remain, revision highlights are in}
}

\boldify{at a high level, what did we do and who did it?}

In the third quarter of 2023, we crawled and scraped data via the \tosdr{} streaming API and BeautifulSoup~\cite{beautifulSoup}.
This data set contains numerous attributes, such as \textit{Description}, \textit{Case}, \textit{DocType}, \textit{Title}, \textit{Status}, \textit{Comments}, and \textit{Authors}.
\revised{%
While the scrape provides labels for fragments of privacy policies, this study will focus on just the \emph{Case} attribute: text that the \tosdr{} team has developed for a privacy concept, about a sentence in length.
This team has developed 245 such cases, which have more detail in a longer paragraph describing the case and offer a ``summary'' of the main ideas in the document.
} % end revised
However, our survey will only focus on 243 of these cases because there were two cases that the \tosdr{} team had not confirmed are actually present in any documents since they rejected all submitted identifications.

In order for people to learn about their ability to understand privacy-related concepts, as well as their impression of how negative or positive that concept was to the affected parties, we conducted a crowd worker survey%
\footnote{The survey is still available at \REDACT{\url{https://severityandunderstandability.ist.psu.edu/}}.}.
To that end, upon obtaining IRB approval, we recruited 519 individuals from Prolific, with inclusion/exclusion criteria that they must speak English fluently and reside in the USA.
The survey contained one attention check question, which would cause rejected work.
\revised{%
Survey responses took FIXME minutes and we paid \$FIXME for accepted work, for an average pay rate of \$FIXME/hr.
} % end revised
Out of the 519 participants, we rejected work from 19 of them and could not recover data from one more, leaving 499 participants.
\revised{%
We chose this sample size based on budget constraints, with a focus on understanding groups of people, meaning we had no intent to compute comparative statistics between cases.
The average time taken by the participants to fill this survey was about 5 minutes and the payment was \$3.} % end revised



\boldify{What core questions did we ask?}

The main task of the survey was to evaluate five random cases.
For each case, we presented the case label and then asked several questions about it.
We first asked about understandability: \textit{``On a scale of 1 to 10, how well do you understand this case? (1 means `Not understanding at all' and 10 means `Understanding this case completely')''}.
When asking about severity, we split the question into two pieces.
The first piece asked: \textit{``Which party does this case tend to favor?''} and participants responded by checking one of three mutually exclusive boxes: \texttt{User}, \texttt{Service Provider}, or \texttt{Neither}.
The second piece then asked: \textit{``How severe is the favoritism that party gains from this case?
(1 means `Not severe at all' and 10 means `Very severe')''}.
\revised{%
We chose this two-step question format to help clarify the participants' thinking by letting them subdivide the problem, akin to determining a vector's direction and then its magnitude.
} % end revised
For the last case, participants saw an additional question that asked them to rewrite the case based on both the case label and its description (\textit{Rewrite the last case (Case \#) in your own words.}).
The minimum requirement for text entry was 21 characters, which helps ensure that participants make a meaningful response.

\boldify{How did the survey flow and what are its constituent parts?}

After obtaining informed consent, the first page collects demographic information from the participant.
The second page gives instructions for the task alongside a single dummy case to clarify the task.
In particular, they answered our questions by checking boxes, repositioning sliders from 0 into the range 1--10, and rewriting one case.
Participants were not able to advance to the next page if any sliders remained at 0 since we consider default values a non-response.
Last comes the page containing the main task (i.e., the five random cases and the aforementioned questions for each case).
The main task page contains one attention check question, which was \textit{``Set the slider to position 7 (This is a special question not related to any case.)''}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Demographic and Attitude Information}

\boldify{Now we describe what demographic information we collect and where we got the questions}

In order to learn about what human traits might moderate responses, we asked about fundamental information such as age, occupation, student status, and gender\footnote{Though we provided multiple gender options in accordance with best practices~\cite{scheuerman2020gender}, nearly all participants identified as male/female, with a few preferring not to state.
} % end footnote

We also posed 11 questions to discover their level of awareness of privacy policies, listed later in Table~\ref{table: demoStats}.
We based several questions on language and categories from the US Census~\cite{UScensus}.
Furthermore, we adapted 11 questions adapted from GenderMag~\cite{burnett2016gendermag}, which specifically identifies five facets of users' problem-solving style: motivation, information processing style, learning style, computer self-efficacy, and risk aversion.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Analysis}
\label{secAnalysis}

\boldify{how did we prepare the data?}
In order to collect the information on understandability, we asked participants their perceived understanding of each case presented to them, collecting the response on a Likert scale [1, 10].
We then calculated descriptive statistics to assess the understandability with respect to participants, as well as the cases.
After collecting the data, we applied a simple transformation to combine the two questions about severity.
We interpret the response to the second question (which party the case favors) as giving the \emph{sign}, while the Likert response on [1, 10] range gives the \textit{magnitude}.
Therefore, if a participant stated a case favored the \texttt{User}, we would use their severity response directly as a positive value.
However, if they said a case favored the \texttt{Service Provider}, we would negate their severity response.
Finally, if they said a favored \texttt{Neither}, then we would multiply their severity response by 0.
Ultimately, this creates a severity range from [-10, 10], with 10 indicating the case fully favors the User, and -10 fully in favor of the Service Provider.
However, at times we considered the absolute value to assess the statistics with respect to individual cases, each of which are marked clearly in the results.
We analyzed severity similar to understandability, with respect to participants and cases.
We also correlated understandability and severity to assess their impact on each other.

\boldify{And then what kind of analysis did we do?}

Much of our analysis relies on descriptive statistics, including mean, median, standard deviation, count of extremities (values of 1 and 10), and a direct comparison between the scores of understandability and severity and their corresponding means.
Since we used different lenses (per participant, per case, etc.), we extracted data from the raw data in different ways to support each perspective, resulting in different sets of data specific to the analysis variable.
\revised{%
Our last quantitative analysis was to perform hypothesis testing about the influence of demographic traits on participant responses.
Specifically, we checked normality with the Kolmogorov-Smirnov test and equivariance with Levene's test.
Fortunately, none of the distributions violated assumptions and we were able to proceed with parametric statistics, such as two-sample t-test and one-way ANOVA.
} % end revised


%Before conducting the analysis, we tested the assumptions necessary for parametric statistics to ensure the validity of the two-sample t-test. Leveneâ€™s test for equality of variances confirmed that the variance in understandability scores between the two groups was equal, while the Kolmogorov-Smirnov (KS) test for normality indicated that the distribution of scores met the assumption of normality. These results justified the use of a two-sample t-test to compare the means of the two groups.
% JED: commented because
