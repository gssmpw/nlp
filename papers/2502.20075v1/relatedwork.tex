\section{Related work}
The methodology to measure the CPU core frequency transition latency is well described and implemented in the FTaLaT benchmark~\cite{cpulat}. This benchmark is done in two phases, which both utilize an artificial, compute-bound workload repeated in a loop. First, the iteration average execution time is measured for each frequency separately. Secondly, the loop is run again with the frequency change from an initial to a target frequency is performed. The time between the frequency change call and the first iteration executed in run time corresponding to the average execution time under the target frequency (measured in the first phase) is considered the transition latency of the initial to the target frequency change. We must consider that every couple of frequencies may show a different transition latency, as well as be non-symmetrical.

The energy efficiency can be improved using static tuning, which applies a configuration at the beginning of an application execution and persists till the execution end. If applying a constraint of no runtime extension, the static tuning can bring only a limited amount of energy savings~\cite{READEXbook}. Complex applications usually have different hardware requirements in time, their performance is bounded by a different subsystem (compute, memory, IO, etc.). Thus, dynamic configuration tuning brings way more opportunities to save energy.


Suitable places for the frequency adjustments are the boundaries of code regions where workload changes. One such example of such boundary is used in COUNTDOWN~\cite{COUNTDOWN}, which focuses on DVFS used on MPI applications. It examines the usage of DVFS within the MPI communication regions and the regions where actual computations are performed. The paper introduces a classification of the boundary depending on the code regions in one out of four types, depending on whether one or both of the code regions are shorter or longer than 500\,us. In terms of GPU programming, this would correspond to the parts of code where memory operations take place or data is fetched between host and device. In the case of the Intel Haswell E5-2630 v3 CPUs mentioned in this paper, if another frequency change request is made before the previous frequency transition ends, the actual CPU core frequency is undefined. As a result, energy savings in these cases are minimal, and attempting DVFS in such short regions can lead to inefficient power-state transitions. Therefore, considering the hardware reactivity to requested frequency changes is crucial in achieving meaningful energy savings.

Energy efficiency optimization using dynamic tuning of accelerators is a distinct topic from CPU energy efficiency approaches, as the architectures of CPUs and ACCs differ significantly. While both rely on transistor-based microchips, the impact of frequency and voltage scaling on power consumption and runtime can vary. The energy consumption and execution time impacts of static frequency tuning were compared for the AMD MI100 and Nvidia A100 GPUs using the hipBone and Stream benchmarks, as outlined in \cite{eef-gpu}. Regardless of the underlying architecture or the benchmark, it was shown that operating at approximately 75\,\% of the maximum frequency represents an optimal balance between significant energy savings and minimal performance penalties for these executed codes. Another study presents up to 27\,\% power saving without impacting the performance of Nvidia DGX-A100 for both compute and memory bound artificial benchmarks~\cite{DGXvsDGX}. 
These papers show that GPU's SMs' frequency scaling can potentially bring decent energy savings.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%