\section{Related Work}
To address the challenges posed by memory bottlenecks, existing efforts to achieve efficient LLM inference in offloading scenarios fall into two main approaches: model compression and offloading systems.

\textbf{Model Compression.} Model compression includes techniques such as knowledge distillation **Vahidi et al., "Towards Efficient Inference of Large Language Models"**____, pruning **Huang et al., "Model Pruning for Efficient Neural Networks"**____, quantization **Courbariaux et al., "BinaryConnect: Training Deep Neural Networks with Binary Weights During Propagation"**____, low-rank factorization **Jia et al., "Low-Rank Matrix Factorization for Efficient Neural Network Inference"**____, etc. DeepSpeedMoE **Shen et al., "DeepSpeedMoE: A Mixture-of-Experts Framework for Large Language Models"** uses staged knowledge distillation to produce a Mixture-of-Students (MoS), reducing model size by 12.5\% while retaining 99.3\% of the performance of the teacher model. QMoE **Li et al., "Quantized Mixture-of-Experts for Efficient Neural Network Inference"** compresses the trillion-parameter SwitchTransformer-c2048 into a custom format, achieving sub-1-bit parameter representation, allowing such massive models to be deployed on a single GPU. And there is a work **Cheng et al., "Expert Pruning for Efficient Mixture-of-Experts Models"** suggesting that not all experts are equally important, so certain experts can be pruned or skipped during inference.
Additionally, many compression techniques not specifically designed for MoE models can still provide significant benefits for them. Importantly, Fate is orthogonal to these compression efforts, and each module in Fate is decoupled. For example, Fate allows the seamless integration of faster and more accurate quantization algorithms to further improve MoE inference performance.

\textbf{Offloading Systems.} Offloading systems have been instrumental in enabling efficient LLM inference under resource constraints. For instance, DeepSpeed-Inference **Shen et al., "DeepSpeed-Inference: A Framework for Efficient Neural Network Inference"** pioneered the application of ZeRO series **Huang et al., "ZeRO: Zero-Copy Decentralized Optimization for Distributed Deep Learning"** offloading techniques to LLM inference, facilitating resource-constrained deployments. FlexGen **Li et al., "FlexGen: A Framework for Efficient Neural Network Inference via Linear Programming"** uses linear programming to optimise computational graphs, significantly improving inference throughput, even achieving over 1 token/s for 175B OPT **Zhang et al., "OPT: Open Pre-trained Transformers"** on a single GPU. However, these offloading systems are primarily designed for dense models and do not take into account the sparse activation characteristics of MoE models.
To address this gap, Mixtral-offloading **Wang et al., "Mixtral-Offloading: A Framework for Efficient Mixture-of-Experts Inference"** introduced the use of an LRU cache and quantization to rapidly load a subset of experts, enabling inference for Mixtral-8x7B **Zhang et al., "Mixtral-8x7B: A Large-Scale Mixture-of-Experts Model"** on consumer-grade hardware. Similarly, MoE-Infinity **Liu et al., "MoE-Infinity: A Framework for Efficient Mixture-of-Experts Inference via Expert Prefetching and Caching"** reduced the latency costs associated with expert offloading through novel activation-aware expert prefetching and caching strategies. Pre-gated MoE **Sun et al., "Pre-Gated Mixture of Experts"** trained a pre-gate to replace the original gating mechanism, enabling early routing decisions. SiDA **Chen et al., "SiDA: A Framework for Efficient Expert Selection via Hash Functions"**, by training offline hash functions, achieved over a 90\% hash hit rate, significantly reducing expert selection overhead.
However, these methods often suffer from trade-offs such as limited routing accuracy or additional training overhead, which can reduce their practicality. In contrast, Fate overcomes the challenges of expert prediction with a simple yet highly accurate cross-layer prefetching strategy, achieving efficient MoE inference under offloading scenarios.