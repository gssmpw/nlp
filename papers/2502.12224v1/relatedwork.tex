\section{Related Work}
To address the challenges posed by memory bottlenecks, existing efforts to achieve efficient LLM inference in offloading scenarios fall into two main approaches: model compression and offloading systems.

\textbf{Model Compression.} Model compression includes techniques such as knowledge distillation~\cite{agarwal2023gkd, gu2023knowledge, li2023symbolic}, pruning~\cite{frantar2023sparsegpt, ma2023llm, sun2023simple}, quantization~\cite{frantar2022gptq, lin2024awq, xiao2023smoothquant}, low-rank factorization~\cite{xu2023tensorgpt}, etc. DeepSpeedMoE~\cite{rajbhandari2022deepspeed} uses staged knowledge distillation to produce a Mixture-of-Students (MoS), reducing model size by 12.5\% while retaining 99.3\% of the performance of the teacher model. QMoE~\cite{frantar2023qmoe} compresses the trillion-parameter SwitchTransformer-c2048 into a custom format, achieving sub-1-bit parameter representation, allowing such massive models to be deployed on a single GPU. And there is a work~\cite{lu2024not} suggesting that not all experts are equally important, so certain experts can be pruned or skipped during inference. 
Additionally, many compression techniques not specifically designed for MoE models can still provide significant benefits for them. Importantly, Fate is orthogonal to these compression efforts, and each module in Fate is decoupled. For example, Fate allows the seamless integration of faster and more accurate quantization algorithms to further improve MoE inference performance.

\textbf{Offloading Systems.} Offloading systems have been instrumental in enabling efficient LLM inference under resource constraints. For instance, DeepSpeed-Inference~\cite{aminabadi2022deepspeed} pioneered the application of ZeRO series~\cite{ren2021zero, rajbhandari2021zero} offloading techniques to LLM inference, facilitating resource-constrained deployments. FlexGen~\cite{sheng2023flexgen} uses linear programming to optimise computational graphs, significantly improving inference throughput, even achieving over 1 token/s for 175B OPT~\cite{zhang2022opt} on a single GPU. However, these offloading systems are primarily designed for dense models and do not take into account the sparse activation characteristics of MoE models.
To address this gap, Mixtral-offloading~\cite{eliseev2023fast} introduced the use of an LRU cache and quantization to rapidly load a subset of experts, enabling inference for Mixtral-8x7B~\cite{jiang2024mixtral} on consumer-grade hardware. Similarly, MoE-Infinity~\cite{xue2024moe} reduced the latency costs associated with expert offloading through novel activation-aware expert prefetching and caching strategies. Pre-gated MoE~\cite{hwang2024pre} trained a pre-gate to replace the original gating mechanism, enabling early routing decisions. SiDA~\cite{du2024sida}, by training offline hash functions, achieved over a 90\% hash hit rate, significantly reducing expert selection overhead.
However, these methods often suffer from trade-offs such as limited routing accuracy or additional training overhead, which can reduce their practicality. In contrast, Fate overcomes the challenges of expert prediction with a simple yet highly accurate cross-layer prefetching strategy, achieving efficient MoE inference under offloading scenarios.