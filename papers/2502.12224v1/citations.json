[
  {
    "index": 0,
    "papers": [
      {
        "key": "agarwal2023gkd",
        "author": "Agarwal, Rishabh and Vieillard, Nino and Stanczyk, Piotr and Ramos, Sabela and Geist, Matthieu and Bachem, Olivier",
        "title": "Gkd: Generalized knowledge distillation for auto-regressive sequence models"
      },
      {
        "key": "gu2023knowledge",
        "author": "Gu, Yuxian and Dong, Li and Wei, Furu and Huang, Minlie",
        "title": "Knowledge distillation of large language models"
      },
      {
        "key": "li2023symbolic",
        "author": "Li, Liunian Harold and Hessel, Jack and Yu, Youngjae and Ren, Xiang and Chang, Kai-Wei and Choi, Yejin",
        "title": "Symbolic chain-of-thought distillation: Small models can also\" think\" step-by-step"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "frantar2023sparsegpt",
        "author": "Frantar, Elias and Alistarh, Dan",
        "title": "Sparsegpt: Massive language models can be accurately pruned in one-shot"
      },
      {
        "key": "ma2023llm",
        "author": "Ma, Xinyin and Fang, Gongfan and Wang, Xinchao",
        "title": "Llm-pruner: On the structural pruning of large language models"
      },
      {
        "key": "sun2023simple",
        "author": "Sun, Mingjie and Liu, Zhuang and Bair, Anna and Kolter, J Zico",
        "title": "A simple and effective pruning approach for large language models"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "frantar2022gptq",
        "author": "Frantar, Elias and Ashkboos, Saleh and Hoefler, Torsten and Alistarh, Dan",
        "title": "Gptq: Accurate post-training quantization for generative pre-trained transformers"
      },
      {
        "key": "lin2024awq",
        "author": "Lin, Ji and Tang, Jiaming and Tang, Haotian and Yang, Shang and Chen, Wei-Ming and Wang, Wei-Chen and Xiao, Guangxuan and Dang, Xingyu and Gan, Chuang and Han, Song",
        "title": "AWQ: Activation-aware Weight Quantization for On-Device LLM Compression and Acceleration"
      },
      {
        "key": "xiao2023smoothquant",
        "author": "Xiao, Guangxuan and Lin, Ji and Seznec, Mickael and Wu, Hao and Demouth, Julien and Han, Song",
        "title": "Smoothquant: Accurate and efficient post-training quantization for large language models"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "xu2023tensorgpt",
        "author": "Xu, Mingxue and Xu, Yao Lei and Mandic, Danilo P",
        "title": "Tensorgpt: Efficient compression of the embedding layer in llms based on the tensor-train decomposition"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "rajbhandari2022deepspeed",
        "author": "Rajbhandari, Samyam and Li, Conglong and Yao, Zhewei and Zhang, Minjia and Aminabadi, Reza Yazdani and Awan, Ammar Ahmad and Rasley, Jeff and He, Yuxiong",
        "title": "Deepspeed-moe: Advancing mixture-of-experts inference and training to power next-generation ai scale"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "frantar2023qmoe",
        "author": "Frantar, Elias and Alistarh, Dan",
        "title": "Qmoe: Practical sub-1-bit compression of trillion-parameter models"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "lu2024not",
        "author": "Lu, Xudong and Liu, Qi and Xu, Yuhui and Zhou, Aojun and Huang, Siyuan and Zhang, Bo and Yan, Junchi and Li, Hongsheng",
        "title": "Not All Experts are Equal: Efficient Expert Pruning and Skipping for Mixture-of-Experts Large Language Models"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "aminabadi2022deepspeed",
        "author": "Aminabadi, Reza Yazdani and Rajbhandari, Samyam and Awan, Ammar Ahmad and Li, Cheng and Li, Du and Zheng, Elton and Ruwase, Olatunji and Smith, Shaden and Zhang, Minjia and Rasley, Jeff and others",
        "title": "Deepspeed-inference: enabling efficient inference of transformer models at unprecedented scale"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "ren2021zero",
        "author": "Ren, Jie and Rajbhandari, Samyam and Aminabadi, Reza Yazdani and Ruwase, Olatunji and Yang, Shuangyan and Zhang, Minjia and Li, Dong and He, Yuxiong",
        "title": "Zero-offload: Democratizing $\\{$billion-scale$\\}$ model training"
      },
      {
        "key": "rajbhandari2021zero",
        "author": "Rajbhandari, Samyam and Ruwase, Olatunji and Rasley, Jeff and Smith, Shaden and He, Yuxiong",
        "title": "Zero-infinity: Breaking the gpu memory wall for extreme scale deep learning"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "sheng2023flexgen",
        "author": "Sheng, Ying and Zheng, Lianmin and Yuan, Binhang and Li, Zhuohan and Ryabinin, Max and Chen, Beidi and Liang, Percy and R{\\'e}, Christopher and Stoica, Ion and Zhang, Ce",
        "title": "Flexgen: High-throughput generative inference of large language models with a single gpu"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "zhang2022opt",
        "author": "Zhang, Susan and Roller, Stephen and Goyal, Naman and Artetxe, Mikel and Chen, Moya and Chen, Shuohui and Dewan, Christopher and Diab, Mona and Li, Xian and Lin, Xi Victoria and others",
        "title": "Opt: Open pre-trained transformer language models"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "eliseev2023fast",
        "author": "Eliseev, Artyom and Mazur, Denis",
        "title": "Fast inference of mixture-of-experts language models with offloading"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "jiang2024mixtral",
        "author": "Jiang, Albert Q and Sablayrolles, Alexandre and Roux, Antoine and Mensch, Arthur and Savary, Blanche and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Hanna, Emma Bou and Bressand, Florian and others",
        "title": "Mixtral of experts"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "xue2024moe",
        "author": "Xue, Leyang and Fu, Yao and Lu, Zhan and Mai, Luo and Marina, Mahesh",
        "title": "Moe-infinity: Activation-aware expert offloading for efficient moe serving"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "hwang2024pre",
        "author": "Hwang, Ranggi and Wei, Jianyu and Cao, Shijie and Hwang, Changho and Tang, Xiaohu and Cao, Ting and Yang, Mao",
        "title": "Pre-gated moe: An algorithm-system co-design for fast and scalable mixture-of-expert inference"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "du2024sida",
        "author": "Du, Zhixu and Li, Shiyu and Wu, Yuhao and Jiang, Xiangyu and Sun, Jingwei and Zheng, Qilin and Wu, Yongkai and Li, Ang and Li, Hai and Chen, Yiran",
        "title": "SiDA: Sparsity-Inspired Data-Aware Serving for Efficient and Scalable Large Mixture-of-Experts Models"
      }
    ]
  }
]