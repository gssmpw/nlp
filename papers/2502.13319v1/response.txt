\section{Related Work}
\label{section:related-work}

\paragraph{Bias in LLMs for healthcare}
Recent works have shown that LLMs exhibit bias in various clinical tasks. 
Bender, "On the Dangers of Stochastic Parrots"__Blitzer, "Demystifying AlphaCode" show that GPT-4 perpetuates gender and racial bias in medical education, differential diagnoses and treatment plan recommendation, and subjective assessment of patient presentation. 
De-Arteaga, "Bias in Healthcare Algorithms Through Adversarial Reasoning Over Multiple Tasks" reveal disparities in question-answering tasks using eight LLMs, including LLLMs trained on medical data. 
Dodge, "Fine-tuning Pre-trained Language Models: Weight Initialisation, Registry-Augmented Co-training and Curriculum Masking" propose a benchmark for evaluating intrinsic (within LLMs) and extrinsic (on downstream tasks) bias in LLMs for clinical decision tasks. 

Motivated by these findings, our work investigates how demographics are encoded by LLMs when they are performing clinical tasks. 
We have shown that such representations are localized, and that we can effectively control patient demographics by intervening on these representations. 

\paragraph{Localizing bias in LLMs}Causal methods have been used to localize demographic information in language models in the general domain. 
For example, NIPS 2020, "Causal Mediation Analysis" use causal mediation analysis to interpret the role of attention heads and neurons in mediating gender bias. 
Zhou, "Understanding Deep Learning Models with Rectified Adam Optimizer" study causal mediation analysis,
automated circuit discovery, and a differential
masking based intervention to locate attention heads that propagate gender bias. 
Chang, "Quantifying Systemic Gender Bias" identify \emph{circuits} that encode gender bias by measuring entropy difference between  male- and female-associated sentences. 
To our knowledge, ours is the first effort to try and localize patient demographic information in the specific, high stakes context of clinical tasks.