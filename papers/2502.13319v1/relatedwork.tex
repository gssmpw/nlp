\section{Related Work}
\label{section:related-work}

\paragraph{Bias in LLMs for healthcare}
Recent works have shown that LLMs exhibit bias in various clinical tasks. 
\citet{zack2024assessing} demonstrate that GPT-4 perpetuates gender and racial bias in medical education, differential diagnoses and treatment plan recommendation, and subjective assessment of patient presentation. 
\citet{Yang_2024} show that GPT-3.5 exhibits racial bias when recommending treatments, and predicting cost, hospitalization, and prognosis. \citet{poulain2024biaspatternsapplicationllms} reveal disparities in question-answering tasks using eight LLMs, including LLMs trained on medical data. 
\citet{zhang2024climb} propose a benchmark for evaluating intrinsic (within LLMs) and extrinsic (on downstream tasks) bias in LLMs for clinical decision tasks. 

Motivated by these findings, our work investigates how demographics are encoded by LLMs when they are performing clinical tasks. 
We have shown that such representations are localized, and that we can effectively control patient demographics by intervening on these representations. 

\paragraph{Localizing bias in LLMs}Causal methods have been used to localize demographic information in language models in the general domain. 
For example, \citet{vig2020causal} use causal mediation analysis to interpret the role of attention heads and neurons in mediating gender bias. 
\citet{chintam2023identifying} study causal mediation analysis,
automated circuit discovery, and a differential
masking based intervention to locate attention heads that propagate gender bias. \citet{yu2025understanding} identify \emph{circuits} that encode gender bias by measuring entropy difference between  male- and female-associated sentences. 
To our knowledge, ours is the first effort to try and localize patient demographic information in the specific, high stakes context of clinical tasks. 
%Our work looks into localizing gender and racial information specifically in the clinical context.