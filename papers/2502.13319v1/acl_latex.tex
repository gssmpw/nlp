% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[final]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{subcaption}
% \documentclass{article}
\usepackage{amssymb}
\usepackage{url}
% \usepackage{authblk}
%
% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{Elucidating Mechanisms of Demographic Bias \\ in LLMs for Healthcare}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Hiba Ahsan      Arnab Sen Sharma}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

% \author{First Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\\And
%   Second Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\}

% \author{Author 1 \And ... \And Author n \\
%         Address line \\ ... \\ Address line}

% \author{
%  Hiba Ahsan\And Arnab Sen Sharma\And Silvio Amir \\ Northeastern University \\ \small{\textbf{Correspondence:} \href{mailto:ahsan.hi@northeastern.edu}{ahsan.hi@northeastern.edu}} \And David Bau \And Byron C. Wallace\\
% }

\author{Hiba Ahsan\qquad Arnab Sen Sharma\qquad Silvio Amir \qquad David Bau \qquad Byron C. Wallace  \\Northeastern University\\ {\small{\tt \{ahsan.hi, sensharma.a, s.amir, d.bau, b.wallace\}@northeastern.edu}}}

\begin{document}
\maketitle
\begin{abstract}
We know from prior work that LLMs encode social biases, and that this manifests in clinical tasks. %language models specifically.
In this work we adopt tools from  \emph{mechanistic interpretability} to  unveil sociodemographic representations and biases within LLMs in the context of healthcare. 
Specifically, we ask: \emph{Can we identify activations within LLMs that encode sociodemographic information (e.g., gender, race)?} 
We find that gender information is highly localized in middle MLP layers and can be reliably manipulated at inference time via patching. 
Such interventions can surgically alter generated \emph{clinical vignettes} for specific conditions, and also influence downstream clinical predictions which correlate with gender, e.g., patient risk of depression. 
We find that representation of patient race is somewhat more distributed, but can also be intervened upon, to a degree. 
To our knowledge, this is the first application of mechanistic interpretability methods to LLMs for healthcare. 
\end{abstract}

\section{Introduction}

LLMs are poised to transform the practice of healthcare in many ways \citep{nori2023capabilities,dash2023evaluation,singhal2023large}, given the volume of unstructured health data and limited provider bandwidth \citep{zhou2023survey}.  
Such models are capable of a wide range of tasks related to processing and making sense of healthcare data \citep{thirunavukarasu2023large}, from summarizing published medical literature \citep{shaib-etal-2023-summarizing} to extracting key information from the notes within patient electronic health record (EHR) data \citep{agrawal-etal-2022-large,ahsan2024retrieving}. 
Indeed, excitement around such uses is driving fast adoption: Epic---a major vendor of EHR software---has hastily integrated GPT-4 into its platform, making it directly accessible to caregivers \citep{epic_gpt4_ehr}. 

But enthusiasm around the uptake of LLMs in this space has been tempered by concerns over fairness and the opaque nature of large generative models \citep{haltaufderheide2024ethics}.
One salient concern---which preliminary work suggests is very much warranted---is that %the uptake of 
such models might exacerbate existing biases in healthcare.

For instance, recent work by \citet{zack2024assessing} found that GPT-4 exaggerates associations between conditions and sociodemographic groups. 
Specifically, when asked to generate \emph{clinical vignettes} of patients with particular conditions, GPT-4 will \emph{nearly exclusively} assume certain demographics (e.g., race, gender). 
For example, asked to generate vignettes for patients with \emph{rheumatoid arthritis}, GPT-4 generates cases featuring female patients 97\% of the time (the actual percent of individuals with rheumatoid arthritis who are female is about 66\%; \citealt{linos1980epidemiology}). 
Similarly, GPT-4 associates sarcoidosis with Black patients and hepatitis B with Asian patients more strongly than actual population-wide correlations. 

\begin{figure}
    \centering
    \includegraphics[scale=0.8]{images/hiba-fig1.pdf}
    \caption{We show that we can localize patient gender information in LLM representations for clinical tasks.}
    \label{fig:fig1}
\end{figure}


In this work, we ask: Is the internal LLM encoding of patient demographics like gender and race localized? 
And, can we intervene upon this? 
To answer this we perform \emph{activation patching} \citep{heimersheim2024use} on {\tt OLMo-7B-Instruct} \cite{groeneveld2024olmo} in the context of clinical vignette generation.
The idea is to identify a small set of internal activations which code for patient characteristics like gender, and then verify these by intervention. 
% (i.e., we ought to be able to ``change'' the gender of a patient in a generated vignette by manipulating these activations). 
We offer the following contributions.

\vspace{0.35em}
% \noindent (1) We find that gender\footnote{We assume that texts in general communicate patient gender (e.g., via pronouns). We will refer to certain conditions---like pregnancy---as being associated with biological sex. Unfortunately, as in prior work \cite{zack2024assessing}, the data we have only permits analysis of binary gender categories.} information is localized to middle MLP layers; patching MLP activations of a single layer consistently alters patient gender in generated texts.
\noindent (1) We find that gender\footnote{Following prior work \cite{zack2024assessing}, we use ``gender'' instead of ``sex'' because of the ambiguity in how LLMs use the terms ``male'' and ``female'' (biological vs sociocultural). Most ($\sim 60\%$) vignettes generated by {\tt OLMo} state ``gender'' followed by ``male'' or ``female''. Our analysis is also limited to binary gender categories, owing to limitations of the data.} information is localized to middle MLP layers; patching MLP activations of a single layer consistently alters patient gender in generated texts.
Gender information can also be localized in conditions such as prostate cancer (exclusive to males) and preeclampsia (females); these patches generalize to non-clinical domains.

\vspace{0.25em} 
\noindent (2) Race representations are more complicated: Multiple token activations in early and middle MLP layers correspond to patient race. We are able to intervene to ``alter'' race to a degree. 

\vspace{0.25em} 
\noindent (3) We use two downstream clinical tasks to show how patching demographic information can be used to study implicit biases encoded in LLMs. 


\vspace{0.5em}
To our knowledge, this is the first investigation of mechanistic interpretability methods for healthcare. 




\section{Localizing patient gender}
\label{sec:gender-bias}



\subsection{Vignette Generation}

%We first introduce the task we use to investigate localization of patient gender information: \emph{vignette generation}. 
%Specifically, we start by reproducing prior work on bias in patient vignette generated under GPT-4 \citep{zack2024assessing}, here using the open-source {\tt Olmo-7B-Instruct} \citep{groeneveld2024olmo} model.

\citet{zack2024assessing} found that GPT-4 exaggerates differences between demographic groups with respect to clinical conditions. 
Specifically, when asked to generate \emph{clinical vignettes} of patients with particular conditions, GPT-4 will \emph{nearly exclusively} assume certain demographics (e.g., race, gender).  
For instance, asked to generate vignettes for patients with \emph{rheumatoid arthritis}, GPT-4 generates cases featuring female patients 97\% of the time.\footnote{The actual percent of individuals with rheumatoid arthritis who are female is about 66\% \cite{linos1980epidemiology}.} 

%This prior work \cite{zack2024assessing} focussed on GPT-4, a proprietary model. 
%Elucidating the mechanics of models requires access to their internals, i.e., working with models for which weights are accessible. 
%As preliminary work, we have reproduced the results concerning exaggerating associations in clinical vignettes using OLMo \cite{groeneveld2024olmo}, an open-source LLM. 
%We find that {\tt OLMo-7B-Instruct} exhibits even stronger patterns of exaggeration than GPT-4, e.g. describing female patients in 99\% of vignettes for \emph{sarcoidosis}. 




Is the encoding of patient gender localized or distributed in the LLM? 
%Answering this requires access to model internals, which is not possible with GPT-4. 
To answer this we use \emph{activation patching} \cite{heimersheim2024use} in the context of vignette generation. 
This requires access to model internals, so we adopt the open-source {\tt OLMo-7B-Instruct} model in place of the opaque GPT-4.   
% {\tt OLMo-7B-Instruct} model used for vignette generation. 
The idea is to identify a small set of internal activations which code for patient gender, and then verify these by intervention:  
We ought to be able to ``change'' the gender of a patient in a generated vignette by manipulating these activations. 

\begin{table}
    \centering
    \small
    \begin{tabular}{lcc}
    % Condition&\multicolumn{2}{c}{Olmo-7B-Instruct}\\
    \hline
    Condition&M&F\\
    \hline
    Sarcoidosis&0.02&0.98\\
    Multiple Sclerosis&0.06&0.94\\ 
    \end{tabular}
    \caption{Proportion of Male (M) and Female (F) patients in {\tt OLMo-7B-Instruct}-generated vignettes.}
    \label{tab:gender-bias}
    \vspace{-2mm}
\end{table}


\begin{table*}
\small
\centering
\renewcommand{\arraystretch}{1.2}
\begin{tabular}{@{}p{12.5cm}@{}}
{\bf Prompt} \emph{Compose a brief presentation of a patient presenting with Multiple Sclerosis. Please include complete demographic information and past medical history.} 
\end{tabular}
\begin{tabular}{@{}p{6.25cm}p{6.25cm}@{}}
\hline
\emph{Before intervention} & \emph{After activation patching} \\
\hline
\textbf{Ms. Sarah Johnson} & \textbf{Mr. John Smith} \\
%\midrule
%\multicolumn{2}{@{}c}{\textbf{Demographic Information}} \\
%Gender: Female Age: 42 years & Gender: Male; Age: 45 years \\
Gender: Female; Age: 42 & Gender: Male; Age: 45 \\ 
Race: White & Birthplace: New York, USA \\
... & ... \\
%Ethnicity: Black \\
%Marital status: Married & Marital status: Married with two children \\
%Education: Bachelor’s degree & Occupation: Retired from a career in IT \\
%Occupation: Retired (formerly computer programmer) & \\
%Residence: Suburban USA & \\
%\midrule
%\multicolumn{2}{@{}l}{\textbf{Past Medical History}} \\
{\bf Past medical history} & {\bf Past medical history} \\
Multiple Sclerosis (MS) & Multiple Sclerosis (MS) \\
Hypertension: Diagnosed at 40 & Hypertension (high blood pressure) \\
Diabetes Mellitus Type 2 & Diabetes Mellitus Type 2 \\
%... & ... \\
 %& Gastroesophageal Reflux Disease (GERD) \\
 %& Obstructive Sleep Apnea (OSA) \\
 %& Coronary Artery Disease (CAD) \\
%\bottomrule
\end{tabular}
\caption{Patient vignettes generated by {\tt OLMo-7B-Instruct} for a patient with \emph{Multiple Sclerosis} before (left) and after (right) patching in the ``male'' activation pattern. 
This intervention alters patient gender, but not other attributes.}%consistently alters the gender of the patient described in the vignette, but appears to leave other key information unaltered.}
\vspace{-.75em}
\label{table:vignette-itnervention}
\end{table*}


To find such activations, we first prompt the LLM to generate a vignette for a condition (strongly) associated with females (or males).
Specifically, following prior work \cite{zack2024assessing}, we prompt the LLM to provide a succinct description of the patient---including symptoms, medical history, and demographic information---using the same $10$ prompts introduced in this prior study. 
One of the prompts, e.g., is:
\begin{quote} 
\vspace{-.2em}
    \textit{Compose a brief presentation of a patient presenting with} \texttt{[CONDITION]}\textit{. Please include complete demographic information and past medical history.}
\vspace{-.2em}
\end{quote}

% To study gender bias in vignette generation, we pick two conditions that [X] found GPT-4 over-represented female patients: 



We run each prompt for every condition through the model $100$ times, yielding $1000$ vignettes per condition. 
We pick two conditions for which GPT-4 exaggerates the association between gender and incidence \citep{zack2024assessing}: 
%that \cite{zack2024assessing} found GPT-4 over-represented female patients: 
Sarcoidosis and multiple sclerosis (MS). 
While these conditions are indeed more prevalent in women than men,\footnote{In US-based studies, about $76\%$ of individuals with MS and $64\%$ of individuals with sarcoidosis are female \citep{baughman2016sarcoidosis, hittle2023population}.} GPT-4 generates cases with female patients in 97\% and 96\% of the cases where gender was specified. 
% (closer to a $3:100$ ratio). 

%($3:1$ ratio in the US population)
%(TODO: Gender bias can also occur in the other direction: over-representing the male population. Need to work on this as well).    

We first confirm that {\tt OLMo-7B-Instruct} behaves similarly. 
Table \ref{tab:gender-bias} reports the proportion of male/female patients in generated vignettes; %both models 
{\tt OLMo} over-represents females for sarcoidosis and MS. %, even moreso than GPT-4.
%and LLama-3.1-8b-Instruct.



\subsection{Activation Patching}
\label{sec:gender-patching}

We use activation patching to localize gender information encoded by LLMs in clinical contexts. 
We first consider conditions for which the LLM over-represents females.
%female-biased conditions, i.e., conditions associated with (biological) females. 
We denote the vignette generation prompt by $x_{\text{vignette}}$ and define the simple prompt $x_{\text{male}}$ as ``The patient is Male''. 
While prompting the LLM using $x_{\text{vignette}}$, we replace or ``patch'' the MLP activation of the $i^{th}$ token at layer $\ell$, with the MLP activations of the `Male' token at layer $\ell$ from $x_{\text{male}}$. 
We choose MLP activations over residual stream or attention as prior work \citep{meng2022locating, geva2023dissecting} has shown that MLPs play a crucial role in \emph{detokenization}---enriching token embeddings with relevant semantics.
The idea is to locate activations $a_{\text{gender}}$ that encode gender information. 
Replacing activations $a_{\text{gender}}$ in $x_{\text{vignette}}$ with activations from $x_{\text{male}}$ should then increase the likelihood of a male vignette generation. 

For each intervention at the $i^{th}$ token at layer $\ell$, we compute a \emph{rewrite score} \cite{hase2024does}:
\begin{equation}
    \frac{p_{*}(\text{`Male'}) - p(\text{`Male'})}{1-p(\text{`Male'})}
\end{equation}
Where $p(\text{`Male'})$ is the probability of generating the token `Male' for gender when prompting using $x_{\text{vignette}}$ before intervention and $p_{*}(\text{`Male'})$ is the probability of generating `Male' after it. 


\begin{figure}
    \centering
    \includegraphics[scale=0.4]{images/rewrite_score_gender_mean_condition.pdf}
    \caption{Rewrite score distribution averaged over six conditions for which {\tt OLMo} over-represents females (starting from $\ell$=$1$). Middle layer ($\ell$=$18$) MLP activations of the last subtoken of the condition encodes gender information.} 
    \label{fig:rewrite_score_gender}
\end{figure}

Figure \ref{fig:rewrite_score_gender} shows the rewrite score distribution averaged over six conditions for which {\tt OLMo-7B-Instruct} over-represents females (see Appendix \ref{app:rewrite-score} for conditions and prompts). 
We observe that middle layer ($\ell = 18$) MLP activations of the last subtoken of the condition encodes gender. 
Based on this, we proceed with patching the last subtoken, at layer $18$. 

%using a single prompt (see Appendix \ref{app:vignette})
For a condition, we generate $1000$ vignettes before and after activation patching at temperature $0.7$. We also experiment with scaling up the MLP activations by a factor, $c$. %$c\in\mathbb{Z}$.
% using different temperatures ($t$) and scaling factors ($c$). 
 Figure \ref{fig:male-patching} shows the ratio of male vignettes after patching for sarcoidosis and multiple sclerosis. 
 Patching is ineffective in flipping the gender when $c=1$ but scaling activations with $c \geq 2$ flips the gender for all the vignettes. 
 % We observe the same trend for temperatures $0.3$ and $1.0$. 

As a qualitative example, the left side of Table \ref{table:vignette-itnervention} reproduces (a snippet of) the vignette generated by {\tt OLMo-7B-Instruct} for MS; as expected, the patient is described as female. 
When we intervene by patching in the ``male activation pattern'' at layer $\ell=18$ at the token position corresponding to \emph{sclerosis} (right side of table), the gender is switched to male, but the rest is not meaningfully altered. %the final subword token of 

\begin{figure*}[h]
    \centering
    \scalebox{0.85}{ 
    \begin{subfigure}[b]{0.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/male_ratio_line_plot_llama.pdf}
        \caption{Male vignette ratio}
        \label{fig:male-patching}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/gender_perplexity_line_plot_llama.pdf}
        \caption{Perplexity}
        \label{fig:gender-perplexity}
    \end{subfigure}
    }
    \caption{(a) Male vignette ratio after patching. Patching in and scaling ($c\geq2$) alters the gender 100\% of the time. (b) Average vignette perplexity after patching. The black dotted line corresponds to perplexity before patching. }
    \label{fig:male-patching-perplexity} %stated gender in the vignette
\end{figure*}

\vspace{-0.5em}
\paragraph{Do our interventions deteriorate text quality?} 
A natural concern here is that the activation patching we have performed may degrade output quality, even while being ``successful'' in altering the patient characteristic of interest. 
To assess if this is the case, we compute the perplexity using {\tt Llama-3.1-8B}\footnote{We use {\tt Llama} as an external judge of perplexity rather than {\tt OLMo} since the latter generated the text and so likely finds it high likelihood.} \cite{grattafiori2024llama3herdmodels} of $500$ vignettes LLM generated before and after patching. 
Figure \ref{fig:male-patching} reports average perplexities as a function of $c$ for sarcoidosis and MS. 
Perplexity is minimally impacted by the interventions, indicating that generation quality is not compromised. 
For context, a distorted vignette after faulty patching has an average perplexity of $15.54$ (Appendix \ref{app:distort-perplexity}).




% \begin{figure*}
%     \centering
%     \includegraphics[scale=0.55]{images/male_ratio_perplexity_line_plot.pdf}
%     \caption{Left: male vignette ratio after activation patching. Patching in and scaling ($>$2x) alters the stated gender in the vignette 100\% of the time. Right: average vignette perplexity after patching. The black dotted line corresponds to perplexity before patching. } 
%     \label{fig:male-patching}
% \end{figure*}

% \begin{figure}
%     \centering
%     \includegraphics[scale=0.35]{images/male_prevalence.pdf}
%     \caption{Male vignette ratio before and after activation patching. Patching in and scaling up flips the stated gender in the vignette 100\% of the time.} 
%     \label{fig:male-patching}
% \end{figure}



\subsection{Sexed Conditions}
\label{sec:gender-localize}
%It is reasonable to hypothesize that conditions that are exclusive to a gender encode gender information. 
We have shown that we can extract a ``male activation'' which can consistently induce ``maleness'' via patching in clinical vignettes. 
This activation pattern was extracted from the $x_{\text{male}}$ prompt, ``The patient is Male''. 
Is such gender information also encoded when processing less explicitly gendered texts? 
Here we consider the case of conditions which are inherently sexed. 
For instance, activations corresponding to prostate cancer may implicitly encode `male-ness'. 
We test this hypothesis by following the patching set up discussed in Section \ref{sec:gender-patching}, but change $x_{\text{male}}$ from `The patient is Male' to $x_{\text{vignette}}$ for `prostate cancer'. 
In other words, the prompt that we patch from and the prompt that we patch into differ only in terms of the clinical condition. 
We patch MLP activations of `prostate' to the last sub-token of the condition in $x_{\text{vignette}}$ at layer $18$. We observe the same phenomenon as shown in Figure \ref{fig:male-patching}: patching after scaling up activations alters the gender $100\%$ of the time (Appendix \ref{app:sexed-condition}). 
%Figure \ref{fig:male-patching-prostate} shows the ratio of male vignettes after patching using the token `prostate'. As observed in Figure \ref{fig:male-patching}, patching after scaling up the activations flips the gender to male $100\%$ of the time. 

\vspace{-2.5mm}
\paragraph{Patching to other domains}
We next evaluate generalization: Does the `male-ness' patch derived from inserting `prostate cancer' as a condition work in other domains (beyond healthcare)?  
This would imply that the gender inferred implicitly from certain (sexed) medical conditions is `universal' in some sense. 
To assess this,  we use the Professions dataset \citep{vig2020causal} which comprises templates designed to elicit gender bias. 
An example from the dataset is `The \{profession\} was fired because', where \{profession\} is replaced with a profession such as nurse.
In this case, a stereotypical completion would begin with \textit{she}. 

We prompt {\tt Olmo-7B-Instruct} to complete each of the $17$ templates for the $28$ `female' professions provided in the dataset. We select the sentences for which the model generates female pronouns.
% We select professions traditionally associated with females for which {\tt Olmo-7B-Instruct} generates female pronouns when prompted to complete a sentence. 
This yields $46$ sentences. 
We use $20$ sentences to pick a scaling factor $c=5$. 
Patching over the remaining $26$ sentences flips the gender in all but one (scaling up $c$ to $7$ flips the gender for this sentence as well). 
Table \ref{tab:gender-other-domain} provides examples.

\begin{table*}[h]
    \centering
    \small
    \begin{tabular}{p{5cm}p{5cm}p{4cm}}
    \hline
    \textbf{prompt}&\textbf{pre-patching}&\textbf{post-patching}\\
    \hline
    The nurse was fired because&\textit{\textbf{she}} repeatedly failed to follow..&\textit{\textbf{he}} repeatedly failed to follow...\\
    The secretary slept because&\textit{\textbf{she}} had worked...&\textit{\textbf{he}} had worked...\\
    The teacher was promoted because&\textit{\textbf{she}} consistently demonstrated...&of \textit{\textbf{his}} exceptional teaching..\\
    \end{tabular}
    \caption{Examples from Professions dataset of generations pre and post MLP activation patching from `prostate'.}
    \label{tab:gender-other-domain}
     \vspace{-2mm}
\end{table*}

\begin{figure*}%[htbp]
    \centering
    % \scalebox{1.0}{ 
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/rewrite_score_race_hepB.pdf}
        \caption{hepatitis B}
        \label{fig:rewrite_score_race_hepB}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/rewrite_score_race_sarcoidosis.pdf}
        \caption{sarcoidosis}
        \label{fig:rewrite_score_race_sarcoidosis}
    \end{subfigure}
    % }
    \caption{Rewrite score distribution for hepatitis B and sarcoidosis (starting from $\ell$=$1$). Early (layer $4$) as well as middle (layer $18-20$) MLP layers affect racial distribution.}
    \label{fig:race-rewrite-score}
\end{figure*}

\section{Localizing Race}
\label{sec:race-bias}


We have found that patient gender information is localized within LLM representations; is race similarly? 
We repeat the exercise, 
%We follow the approach outlined in Section \ref{sec:gender-patching} to localize race information. 
using two conditions that correlate with race: Sarcoidosis and hepatitis B. 
We again first reproduce \citet{zack2024assessing}'s result using {\tt Olmo-7B-Instruct}, confirming that the model disproportionately generates vignettes of Black patients in the case of sarcoidosis and Asian patients for hepatitis B (Table \ref{tab:race-distribution}). 

% \begin{table}[h]
%     \centering
%     \small
%     \begin{tabular}{cccccc}
%     \hline
%     Condition&Black&White&Asian&Hispanic&Other\\
%     \hline
%     Sarcoidosis&$0.70$&$0.13$&$0.06$&$0.03$&$0.09$\\
%     Hepatitis B&$0.02$&$0.13$&$0.70$&$0.01$&$0.14$
%     \end{tabular}
%     \caption{Race distribution of {\tt OLMo-7B-Instruct}-generated vignettes.}
% \end{table}

% \begin{figure}
%     \centering
%     \includegraphics[scale=0.4]{images/rewrite_score_race_hepB.pdf}
%     \caption{Rewrite score distribution} 
%     \label{fig:rewrite_score_hepB}
% \end{figure}

% \begin{figure}
%     \centering
%     \includegraphics[scale=0.4]{images/rewrite_score_race_sarcoidosis.pdf}
%     \caption{Rewrite score distribution} 
%     \label{fig:rewrite_score_sarc}
% \end{figure}


% \begin{figure}[htbp]
%     \centering
%     \scalebox{0.55}{ 
%     \begin{subfigure}[b]{0.45\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{images/rewrite_score_race_hepB.pdf}
%         \caption{hepatitis B}
%         \label{fig:rewrite_score_race_hepB}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[b]{0.45\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{images/rewrite_score_race_sarcoidosis.pdf}
%         \caption{sarcoidosis}
%         \label{fig:rewrite_score_race_sarcoidosis}
%     \end{subfigure}
%     }
%     \caption{Race rewrite score distribution}
%     \label{fig:all}
% \end{figure}

\begin{figure*}[htbp]
    \centering
    \scalebox{1.0}{ 
    \begin{subfigure}[b]{0.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/race_hepatitis-B_4_ratio.pdf}
        \caption{Layer 4}
        \label{fig:race_hepatitis B_4_prevalence}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/race_hepatitis-B_19_ratio.pdf}
        \caption{Layer 19}
        \label{fig:race_hepatitis B_19_prevalence}
    \end{subfigure}
    }
    \caption{Ratio of target race vignettes before and after activation patching in the case of hepatitis B. We report the maximum improvement for scaling and sliding window patching here. Refer to the Appendix for other scaling factors and window sizes.}
    \label{fig:race-target-race-ratio}
\end{figure*}

\begin{figure*}[htbp]
    \centering
    \scalebox{1.0}{ 
    \begin{subfigure}[b]{0.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/race_sarcoidosis_4_ratio.pdf}
        \caption{Layer 4}
        \label{fig:race_sarcoidosis_4_prevalence}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/race_sarcoidosis_19_ratio.pdf}
        \caption{Layer 19}
        \label{fig:race_sarcoidosis_19_prevalence}
    \end{subfigure}
    }
    \caption{Ratio of target race vignettes before and after activation patching in the case of sarcoidosis.}%  We report the maximum improvement for scaling and sliding window patching here. Refer Appendix for other scaling factors and window sizes.}
    \label{fig:all}
\end{figure*}

 %We observe that 
As done in Section \ref{sec:gender-patching}, for a condition, we generate $1000$ vignettes using a single prompt before and after activation patching. In the case of hepatitis B, we aim to flip the over-represented race, Asian, to another race. Specifically, we experiment with three combinations: (Asian$\rightarrow$Black),  (Asian$\rightarrow$Caucasian), and (Asian$\rightarrow$Hispanic). Similarly, for sarcoidosis, for which Black patients are over-represented, we experiment with: (Black$\rightarrow$Asian), (Black$\rightarrow$Hispanic), and (Black$\rightarrow$Caucasian).


Figures \ref{fig:rewrite_score_race_hepB} and \ref{fig:rewrite_score_race_sarcoidosis} depict the average rewrite scores for hepatitis B and sarcoidosis respectively. 
Patching in early ($\ell$=$4$) as well as middle ($\ell$=$18-20$) layers affects racial distribution. Figures \ref{fig:race_hepatitis B_4_prevalence} and \ref{fig:race_hepatitis B_19_prevalence} show the ratio of the target race (race we aim to flip to) vignettes before and after intervention at layers $4$ and $19$ respectively. Patching a single layer, even with scaling, has a less pronounced effect than for gender. 

Does patching multiple layers at a time help? We explore sliding window patching \cite{meng2022locating}, in which neighboring layers are simultaneously patched. For instance, patching at layer $\ell$ with a window size of $1$ implies patching layers $\ell-1$, $\ell$, and $\ell+1$ simultaneously. In Figures \ref{fig:race_hepatitis B_4_prevalence} and \ref{fig:race_hepatitis B_19_prevalence}, we show results for  window size $5$, which results in the highest improvement in the target race ratio (averaged over the $3$ target races). 
We report results of window sizes $1$ and $3$ in Tables \ref{app:hepb-race-table} and \ref{app:sarc-race-table}. 
Sliding window patching improves target race ratio, suggesting race information is more dispersed.

\paragraph{Is generation quality impacted?} As we did for gender, we now evaluate if patching in race affects text quality. 
Specifically, we compute average perplexity under {\tt Llama-3.1-8B} over the three target races for $500$ vignettes before and after patching for sarcoidosis and hepatitis B. 
Perplexities before and after patching are comparable (Figure \ref{fig:race_perplexity}). 

\begin{figure}
    \centering
    \includegraphics[scale=0.35]{images/race_perplexity_llama3.pdf}
    \caption{Mean and standard deviation of vignette perplexity before and after activation patching for race. Perplexity is minimally impacted.} 
    \label{fig:race_perplexity}
    \vspace{-0.5em}
\end{figure}


\begin{table*}[h]
    \centering
    \small
    \begin{tabular}{p{7cm}p{7cm}}
    \hline
    \textbf{Male}&\textbf{Female}\\
    \hline
    ...there is no direct indication that the patient is at risk of depression....&...the patient's history of anxiety and alcohol withdrawal symptoms, as well as her eviction from her apartment, suggest that she may be at risk of depression....\\
    \\
    ...The patient experienced anxiety, agitation, and delirium, but these were managed...&...the patient had persistent difficulties with anxiety over the weeks preceding admission...\\
    \\
    \hline
    \textbf{White}&\textbf{Black}\\
    \hline
    ...there is no indication that she is at risk of depression...&...is at risk of depression, given her anxiety and interest in complementary/alternative medicine for managing her mental health...\\
    \\
    ...the patient is not at risk of depression...&...at a higher risk of depression...denied suicidality, and she denied prior hospitalizations and incarcerations for which we have documentation...
    % ...the patient is not at risk of depression...&...at a higher risk of depression due to a combination of genetic, social, and cultural factors...denied suicidality, and she denied prior hospitalizations and incarcerations for which we have documentation... 
    
    % that these events occurred...
    \vspace{-3mm}
    \end{tabular}
    \caption{Sample {\tt OLMo} outputs when prompted to assess depression risk, after patching in the target demographic.}
    \label{tab:depression-examples}
\end{table*}

\section{Clinical Applications}
\label{section:applications}
% We have established that patient gender information is localized in LLMs using the task of clinical vignette generation. 
% Vignette generation is a practical, straightforward setting to investigate localized encodings of demographic information and mechanistic interventions on such attributes. 
% Next we deepen this analysis by investigating how demographics is encoded and used in the context of clinical tasks for which LLMs might be used. 

We have established that certain patient demographic information is localized in LLM representations using the task of clinical vignette generation, a straightforward setting that focuses on a single condition and has limited confounding variables. Next, we broaden our analysis and look at how mechanistic interventions can be used to detect implicit biases in the context of clinical tasks where LLMs might be used.\footnote{The reader might ask: Can we instead study disparities by simply stating demographics explicitly in prompts (e.g., ``Below is the brief hospital course of a \textit{Black} patient...'')? Perhaps, but recent work has shown that LLMs have implicit biases which may not be apparent with explicit prompts \cite{bai2024measuring}. Moreover, LLMs can discern patient race from clinical notes even when explicit mentions of race are removed \cite{adam2022write}. 
Assigning demographics with causal interventions provides an alternate approach to study implicit biases in LLMs. 
% and how these affect downstream predictions.
}

\subsection{Depression Risk}
\label{sec:depression}
Prior research shows that racial and gender disparities exist in depression diagnosis. In particular, it is diagnosed significantly more commonly in female \cite{brody2018prevalence} and Black adults \cite{vyas2020association}.
We investigate whether demographics affect LLM outputs as to whether a patient with anxiety is at risk of depression, and if we can control this mechanistically, via patching. 

Specifically, given a brief hospital course of a patient with anxiety, we follow \citet{ahsan2024retrieving} and prompt the LLM to determine whether the patient is at risk of depression. For patient notes, we use the dataset introduced by \citet{hegselmann2024medical} and select brief hospital courses (BHCs) of female patients that include the term `anxiety'. 
We filter out BHCs with the term `depression' to eliminate patients that may already have depression. 
We also exclude BHCs that discuss sexed conditions, such as pregnancy. We sample $1000$ BHCs from this filtered set to create our final evaluation set, $S$.
% We sample $1000$ BHCs from this filtered set and create corresponding gender-neutral instances by replacing gendered terms such as `F', `female', `Mrs.', and `she' with `patient'. 

% \begin{quote}
% \textit{Below is the brief hospital course of a patient.}

% \texttt{[BHC]}

% \textit{Based on the course, is the patient at risk of depression? Choice: -Yes -No}
% \end{quote}



\paragraph{Gender}
\label{sec:depression-gender}
To study whether gender affects LLM outputs for the task, we first create a gender-neutral evaluation set using $S$. Specifically, we replace gendered terms such as `F', `female', `Mrs.', and `she' with `patient' in every BHC in $S$. 
% To implicitly assign gender to an explicitly gender-neutral BHC, we perform activation patching.
Then we perform activation patching to implicitly assign gender to an explicitly gender-neutral BHC.
% We use `The patient is Male' as the source prompt to assign `male' to the BHC. Similarly, we use `The patient is Female' as the source prompt to assign `female' to the BHC.


% We use the risk prompt with the gender-neutral BHC as the target prompt, but modify it to first generate the gender of the patient. 
% This allows us to determine if patching was successful. 

% \begin{quote}
% \textit{Below is the brief hospital course of a patient. Based on the course, is the patient at risk of depression? \\Choice: -Yes -No}

% \textbf{\textit{You must start your answer with ``Gender:", followed by the patient's gender.}}

% \texttt{[NOTE]}
% \end{quote}
Similar to the setup in Sections \ref{sec:gender-bias} and \ref{sec:race-bias}, we extract the `male' and `female' representations using two simple source prompts, ``The patient is Male'' and ``The patient is Female'', respectively. However, this time the activations are extracted from the \emph{residual stream} (not MLPs) of the token `Male' (or `Female') from the source prompts. Also, the extracted activations are patched at the \emph{last} token of the target prompt. See Appendix \ref{app:depression-risk} for examples of the prompt.
% We patch \textit{residual stream} activations of the token `Male' (`Female') to the residual activations of the last token of the target prompt 
% (specifically the token `|>'. In instruction-tuned models, the <|assistant|> token added at the end indicates the start of the model’s generation). 
% We chose the last token as our intervention position because our experiments showed that patching on this token position had the most causal influence on LLM outputs (see Appendix \ref{app:depression-risk}). 
Our choice of last token is informed by and aligns with the claims of previous research \cite{marks2023geometry, todd2023function} suggesting that the last token representation encodes information about the entire prompt. 
% and influences the generation behavior of the model.
% The last token representation encodes information about the entire prompt and influences the generation behavior of the model \cite{marks2023geometry, todd2023function}. 
We patch the target prompt at layer $18$, and scale the activations with a factor of $2$; We picked these values using a validation set of $100$ BHCs. 


% We patch \textit{residual stream} activations of the token `Male' (`Female') to the residual activations of the last token of the target prompt at layer $18$, and scale the activations with a factor of $2$. The layer and scaling factor are determined using a validation set of $100$ BHCs. 



% Table \ref{tab:depression-gender} shows gender ratio and risk after patching. Patching successfully assigns gender to the gender-neutral BHCs. Moreover, we observe for the same BHC, {\tt Olmo} on an average considers females to be at higher risk than males.


% \begin{table}[h]
%     \centering
%     \begin{tabular}{ccc}
%     \hline
%     &Female&Male\\
%     \hline
%     Gender&$1.0$&$1.0$\\
%     Risk&$0.37$&$0.33$\\
%     % \hline
%     % \\
%     % \hline
%     % &Black&White\\
%     % \hline
%     % Race&$0.79$&$0.75$\\
%     % Risk&$0.43$&$0.37$\\
%     \end{tabular}
%     \caption{The first row (Gender) indicates the ratio of female (male) patients after patching gender-neutral BHCs. Patching successfully assigns gender to the BHCs. The second row (Risk) indicates the ratio of  `yes' generations  for whether a patient with anxiety is at risk of depression. {\tt Olmo} generates `yes' more often when the gender is patched to be Female than Male.}
%     \label{tab:depression-gender}
% \end{table}


\paragraph{Race}
\label{sec:depression-race}
%To study the effect of patient race, we use the evaluation set $S$ described above in Section \ref{sec:depression}. 
Next we use $S$ to evaluate the effect of altering race implicitly via patching. 
Specifically, we measure disparity between white and Black patients. 
We use source prompts ``The patient is White.'' and ``The patient is Black.'' to assign race to a BHC. %as the source prompt to assign the race `white' to a BHC, and  to assign the race `Black'. 
We set the target patching layer and scaling factor to $20$ and $2$, respectively, based on the the validation set from Section \ref{sec:depression-gender}. 
%Table \ref{tab:depression-race} shows race ratios and risk after patching. {\tt Olmo} on an average considers Black patients to be at higher risk than White patients. Note that the race ratio is computed based on explicit mention of race in the LLM output. 


% \begin{table}[h]
%     \centering
%     \begin{tabular}{ccc}
%     &Black&White\\
%     \hline
%     Race&$0.79$&$0.75$\\
%     Risk&$0.43$&$0.37$\\
%     \end{tabular}
%     \caption{The first row (Race) indicates the ratio of black (white) patients (explicitly stated in the LLM output) after patching BHCs. The second row (Risk) indicates the ratio of  `yes' generations  for whether a patient with anxiety is at risk of depression. {\tt Olmo} generates `yes' more often when the race is patched to be Black than White.}
%     \label{tab:depression-race}
% \end{table}


\paragraph{Results}
We treat LLM output as a binary variable and compute the difference in risk prediction between demographic groups (female/male or Black/white) as follows:

\begin{equation}
  \Delta_{\text{risk}} = \frac{1}{|S|} \sum_{i=1}^{|S|} (u_i -  v_i)  
\end{equation}

% \begin{align}
%   \Delta_{\text{risk}} &= \frac{1}{|S|} \sum_{i=1}^{|S|} (\female_i -  \mars_i)\\
%   \text{where,}\;  \female_i &= p(\texttt{`` Yes"} \,|\, \text{BHC}_i  \,\wedge\, h_{-1}^{\ell} \leftarrow a_{\text{female}}) \nonumber
% \end{align}

\noindent where for gender $u_i$ and $v_i$ indicate the risk prediction for the $i^{th}$ BHC when assigned female and male gender respectively. In the case of race, $u_i$ and $v_i$ indicate the risk prediction for the $i^{th}$ BHC when assigned Black and white, respectively. 

Instruction-tuned LLMs are sensitive to instruction phrasings \cite{sun2023evaluating,ceballos2024open}. 
To ensure our results are robust, we perform the intervention on four different target prompts (see Appendix \ref{app:depression-prompts}) to elicit risk of depression prediction. 
%that elicit a patient's risk of depression in varied ways. 
Table \ref{tab:depression} reports the difference in risk prediction averaged over four prompts for each demographic. {\tt OLMo-7B-Instruct} on average considers females to be at higher risk of depression than males. 
With respect to race, the LLM considers Black patients to be at higher risk than white patients. 
Table \ref{tab:depression-examples} shows some sample outputs. 
While implicit and explicit biases may manifest differently, we observe the same trend in disparity with explicit prompts as well (Appendix \ref{app:depression-risk}).
 
We evaluate if the target demographic (e.g., Black for race) is successfully assigned after patching in two ways. 
\textbf{Strict} is calculated by checking if the target demographic (e.g., `White' or `Caucasian' for white) is explicitly mentioned in the LLM output. 
\textbf{Relaxed} is calculated by checking if the counterfactual demographic is not mentioned in the output; outputs in which the target demographic is \textit{not} mentioned are thus also considered successful assignments. 
Table \ref{tab:depression-assignment} shows the ratio of successful demographic assignment averaged over two prompts of the four prompts that ask for the demographic to be stated (in addition to risk evaluation). 
In the strict case, the target gender and race assignment are $\sim0.95$ and $0.78$, respectively. 
Relaxed evaluation is $1.0$ across combinations.

\begin{table}[h]
\small
    \centering
    \begin{tabular}{lc}
    \hline
    Demographic&$\Delta_{\text{risk}}$\\
    \hline
    Gender&$3.50\pm2.2\%$\\
    % Gender&$0.04\pm0.02$\\
    % Race&$0.08\pm0.06$
    Race&$8.25\pm5.8\%$
    \end{tabular}
    \caption{Difference in risk depression averaged over four prompts for each demographic. {\tt OLMo-7B-Instruct} on an average considers females to be at higher risk of depression than males, and Black patients to be at higher risk than white patients.}
    %In terms of race, the LLM considers Black patients to be at higher risk than White patients.
    \label{tab:depression}
    \vspace{-14pt}
\end{table}

\begin{table}[h]
\small
    \centering
    \begin{tabular}{l cc cc}
    \hline
    Assignment&Female&Male&Black&White\\
    \hline
    Strict&$0.96$&$0.94$&$0.79$&$0.76$\\
    Relaxed&$1.0$&$1.0$&$1.0$&$1.0$\\
    \end{tabular}
    \caption{Ratio of successful demographic assignment averaged over two prompts.} %`Strict' is calculated by checking for explicit mention of the demographic. `Relaxed' is calculated by checking if the counterfactual demographic is \textit{not} mentioned.
    \label{tab:depression-assignment}
\vspace{-14pt}
\end{table}

\begin{figure*}%[htbp]
    \centering
    \scalebox{0.85}{
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/diff_diag_gender_explicit.pdf}
        \caption{Gender -- explicit}
        \label{fig:diff-diag-gender-explicit}
    \end{subfigure}
    % \hfill
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/diff_diag_gender_implicit.pdf}
        \caption{Gender -- implicit}
        \label{fig:diff-diag-gender-implicit}
    \end{subfigure}
    }
    \caption{Rank distribution of the correct diagnosis for explicit and implicit gender assignment. We see a similar trend in rank difference in both strategies. }
    \label{fig:diff-diag}
    \vspace{-1em}
\end{figure*}


\subsection{Differential Diagnosis}
\label{sec:diff-diagnosis}
We explore how demographics affects LLM diagnostic accuracy, specifically its ability to rank the correct diagnosis when asked for a list of differentials for a given patient case. We follow \citet{zack2024assessing}'s setup and prompt the LLM to list differentials for medical education cases from NEJM Healer \citep{abdulnour2022deliberate}.  NEJM Healer is a medical education tool that provides expert-created cases, enabling medical trainees to compare their differential diagnoses with the expected ones. 

We select one case each to study disparities between male/female patients and Black/white patients (see Appendix  \ref{app:diff-diagnosis} for prompts and cases). We follow the set up described in Section \ref{sec:depression} to implicitly assign the target demographic via activation patching. We use the same source prompts, target layers, and scaling factor. 

% \paragraph{Gender}
% We follow \citet{zack2024assessing}'s setup and prompt the LLM to list differentials for medical education cases from NEJM Healer.  NEJM Healer is a medical education tool that provides expert-created cases, enabling medical trainees to compare their differential diagnoses with the expected ones. We use the following case: 

% \begin{quote}
% \textit{A 63-year-old patient presents with acute-on-chronic cough with a change in sputum character and trace hemoptysis and is found to have tachycardia, tachypnea, and hypoxemia.}
% \end{quote}

% \paragraph{Race}
% To study disparity between Black and White patients, we use the following case:

% \begin{quote}
% \textit{A 54-year-old patient with a history of aortic stenosis and travel to South America presents with subacute progressive dyspnea, intermittent fevers, a cough that produces pink sputum, orthopnea, and unintentional weight loss. They are found to be febrile, hypoxemic, tachypneic, and tachycardic.}
% \end{quote}

% We follow the set up described in Section \ref{sec:depression} to implicitly assign the target demographic via activation patching. We use the same source prompts, target layers, and scaling factor. 

% \begin{figure*}%[htbp]
%     \centering
%     % \scalebox{0.55}{ 
%     \begin{subfigure}[b]{0.48\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{images/diff_diag_gender.pdf}
%         \caption{Gender}
%         \label{fig:diff-diag-gender}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[b]{0.48\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{images/diff_diag_race.pdf}
%         \caption{Race}
%         \label{fig:diff-diag-race}
%     \end{subfigure}
%     % }
%     \caption{Rank distribution of the correct diagnosis. `(explicit)' indicates the demographic was explicitly stated in the prompt. `(implicit)' indicates the demographic was implicitly assigned by activation patching. }
%     \label{fig:diff-diag}
% \end{figure*}
\vspace{-5pt}

\paragraph{Results}
We sample $500$ differential lists at temperature $0.7$ and check for significant difference in the ranking of the correct diagnosis (between male/female and Black/white patients) using Mann-Whitney tests. We observe significant differences: The mean rank difference between male and female patients is $0.24$ ($p=$$0.004$), and between Black and white patients this is $0.08$ ($p$=$0.02$). Figure \ref{fig:diff-diag} shows the rank distribution of the correct diagnosis when gender is explicitly stated and when it is patched; the trend in rank difference is similar whether gender is explicitly or implicitly assigned. (See Figure \ref{fig:diff-diag-race} in the Appendix for race). 

% %an LLM's ability to prioritize the correct diagnosis when asked for a list of differentials for a given case. 
% We follow \cite{zack2024assessing}'s setup and choose the following case to study: 

% \begin{quote}
% \textit{A 21-year-old patient with multiple recent sexual partners and residence in a dormitory with known sick contacts presents with acute pharyngitis, fatigue, fever, rash, and headache but no coryza, congestion, or cough.}
% \end{quote}

% We prompt the LLM to generate a list of differential diagnoses, ranking them from most likely to least likely.

% % after reproducing a significant difference using Mann-Whitney test in the correct diagnosis (mononucleosis) being in the top $3$ diagnoses for male and female when gender is explicitly stated.

% \paragraph{Gender}
% % We first reproduce \cite{zack2024assessing}'s result using \texttt{Olmo} of a significant difference in the correct diagnosis (mononucleosis) being in the top $3$ diagnoses for male and female, when gender is explicitly stated in the prompt. 
% To assign gender to a patient, we perform activation patching. 
% As in section \ref{sec:gender-patching}, we use `The patient is Male' as the source prompt to assign `male' to the patient. 
% Similarly, we use `The patient is Female' as the source prompt to assign `female' to the patient. We patch residual stream activations of the token `Male' (`Female') to the residual activations of the last token of the target prompt at layer $18$, and scale the activations with a factor of $2$. We generate diagnoses list $2000$ times and observe a statistically significant difference (p-value$=0.009$) in the correct diagnosis being in the top $3$ diagnoses for male and female. 
% We also compare whether the difference observed is the same as the difference when gender is explicitly stated using Mann-Whitney test, and observe no difference.  

% \paragraph{Race} We study the disparity between White and Black patients. We first reproduce a significant difference in mononucleosis being in the top $3$ diagnoses for Black and White patients when race is explicitly mentioned in the prompt. We then proceed with implicitly assigning race using activation patching.  As in Section \ref{sec:depression-race}, we use `The patient is White.' as the source prompt to assign the race `White' to the patient, and `The patient is Black.' to assign the race `Black'. We generate diagnoses list $2000$ times and observe a statistically significant difference (p-value$=0.0004$) in the correct diagnosis being in the top $3$ diagnoses for Black and White patients. 
% We also compare whether the difference observed is the same as the difference when race is explicitly stated using Mann-Whitney test, and observe no difference. 

\section{Related Work}
\label{section:related-work}

\paragraph{Bias in LLMs for healthcare}
Recent works have shown that LLMs exhibit bias in various clinical tasks. 
\citet{zack2024assessing} demonstrate that GPT-4 perpetuates gender and racial bias in medical education, differential diagnoses and treatment plan recommendation, and subjective assessment of patient presentation. 
\citet{Yang_2024} show that GPT-3.5 exhibits racial bias when recommending treatments, and predicting cost, hospitalization, and prognosis. \citet{poulain2024biaspatternsapplicationllms} reveal disparities in question-answering tasks using eight LLMs, including LLMs trained on medical data. 
\citet{zhang2024climb} propose a benchmark for evaluating intrinsic (within LLMs) and extrinsic (on downstream tasks) bias in LLMs for clinical decision tasks. 

Motivated by these findings, our work investigates how demographics are encoded by LLMs when they are performing clinical tasks. 
We have shown that such representations are localized, and that we can effectively control patient demographics by intervening on these representations. 

\paragraph{Localizing bias in LLMs}Causal methods have been used to localize demographic information in language models in the general domain. 
For example, \citet{vig2020causal} use causal mediation analysis to interpret the role of attention heads and neurons in mediating gender bias. 
\citet{chintam2023identifying} study causal mediation analysis,
automated circuit discovery, and a differential
masking based intervention to locate attention heads that propagate gender bias. \citet{yu2025understanding} identify \emph{circuits} that encode gender bias by measuring entropy difference between  male- and female-associated sentences. 
To our knowledge, ours is the first effort to try and localize patient demographic information in the specific, high stakes context of clinical tasks. 
%Our work looks into localizing gender and racial information specifically in the clinical context.


\section{Conclusions}
We investigated if patient demographic information can be localized in LLMs. 
We found that gender information is highly localized. 
Patient race is somewhat localized, but less so (it is somewhat distributed across model activations). 
We showed that implicit biases in clinical tasks can be studied by mechanistically controlling demographics, pointing to directions for future work, and potentially methods to mitigate bias in clinical tasks.  

\section*{Limitations}
This work has several important limitations.
First, we focussed our analyses on a single open-source LLM, {\tt OLMo-7B-Instruct}; other LLMs may encode patient demographic information differently. 
Second, we did not extensively edit the prompts used in this work, and this can substantially affect results. 
Third, we took a simplistic view of illustrative demographic categories, and in particular---following prior related analyses \citep{zack2024assessing}---focussed on patients conforming to binary gender categories; future work might extend this to be more inclusive in analyses. 



% Bibliography entries for the entire Anthology, followed by custom entries
%\bibliography{anthology,custom}
% Custom bibliography entries only
\bibliography{acl_latex}

\appendix

\section{Vignette Generation}
\label{app:vignette}
We conduct experiments with {\tt HuggingFace} implementations of models and use {\tt NNsight} \citep{fiotto2024nnsight} to perform interventions. We use two A100 GPUs for all our experiments.

\label{sec:appendix}

\subsection{Rewrite Score}
Layers are numbered from $0$ to $L-1$, where $L$ is the number of layers in the LLM.
\label{app:rewrite-score}

To plot Figure \ref{fig:rewrite_score_gender} for gender, we use the following prompt:
\begin{quote}
\textit{You are a helpful clinical assistant.\newline Compose a brief presentation of a patient presenting with {\tt [CONDITION]}. Please include complete demographic information and past medical history. You must start with the following: "Gender:".}
\end{quote}

\noindent To plot Figure \ref{fig:rewrite_score_race_hepB} and \ref{fig:rewrite_score_race_sarcoidosis} for race, we use the following prompt:
\begin{quote}
\textit{You are a helpful clinical assistant.\newline Please provide a concise example patient presentation of a patient presenting with {\tt [CONDITION]}, including complete demographic details and relevant past medical history. You must start with the following: "Race:".}
\end{quote}

The phrase \textit{You must start with the following: "Gender:"} or \textit{"Race:"} is added to consistently get Male/Female (or race) logits at a fixed position, so that we can index the values and subsequently compute the rewrite score. Note, however, that the intervention is effective even if this phrase is removed. 
% We exclude layer $0$ from the plots; the first layer typically captures basic linguistic features, which are less useful when trying to localize concepts.

For Figure \ref{fig:rewrite_score_gender}, we  average scores over six conditions for which the LLM over-represents females: multiple sclerosis, sarcoidosis, rheumatoid arthristis, asthma, bronchitis, and essential hypertension.

For certain clinical conditions, such as rheumatoid arthritis (Figure \ref{fig:rewrite_score_gender_arthritis}) and sarcoidosis (Figure \ref{fig:rewrite_score_gender_sarcoidosis}), patching at an earlier subtoken of the condition is more effective. This phenomenon is also observed by \cite{meng2022locating} perhaps because the model recognizes the condition when it sees ``rheumatoid'' (or ``sar'' and ``coi'') before seeing ``arthritis'' or (``osis'') \citep{feucht2024token}.

\begin{figure*}[h]
    \centering
    % \scalebox{0.85}{ 
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/rewrite_score_gender_arthritis.pdf}
        \caption{rheumatoid arthritis}
        \label{fig:rewrite_score_gender_arthritis}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/rewrite_score_gender_sarcoidosis.pdf}
        \caption{sarcoidosis}
        \label{fig:rewrite_score_gender_sarcoidosis}
    \end{subfigure}
    % }
    \caption{Rewrite score distribution for rheumatoid arthritis and sarcoidosis. Patching at an earlier subtoken is more effective.}
    \label{fig:rewrite-scores-exceptions} %stated gender in the vignette
\end{figure*}




\subsection{Perplexity}
\label{app:distort-perplexity}
We create a baseline reference for high perplexity, indicating that the patch compromises generation quality. We randomly select $50\%$ of the tokens in $x_{\text{vignette}}$ and patch their MLP representations in $7$ layers ($\ell=[0, 4, 8, 12, 16, 20, 24]$) with that of the token `Male' from $x_{\text{male}}$ in layer $18$. We amplify the activations with $c=20$. We sample $50$ vignettes with temperature $0.7$ and compute the mean perplexity. Below is an example vignette for multiple sclerosis:

\begin{quote}
    \textit{Marcia, age 24, with a history of depression (depression is not a feature of multiple sclerosis).(So this clinical history is not typical for multiple sclerosis)."I took medication for my depression but it made my symptoms of multiple sclerosis worse.I then decided to stop taking the medication and have been feeling better since...}
\end{quote}

\subsection{Sexed Conditions}
\label{app:sexed-condition}
\paragraph{Male} Figure \ref{fig:male-patching-prostate} shows male vignette ratio after activation patching using ` prostate'. Scaling up patched activations flips the gender $100\%$ of the time.

\begin{figure}[ht]
    \centering
    \includegraphics[scale=0.4]{images/male_ratio_line_plot_prostate.pdf}
    \caption{Male vignette ratio after activation patching using ` prostate'. Patching in and scaling up flips the stated gender in the vignette 100\% of the time.} 
    \label{fig:male-patching-prostate}
\end{figure}

\paragraph{Female} We observe the same phenomenon when patching using a female-sexed condition. Concretely, we use $x_{\text{vignette}}$ to generate vignettes for a condition for which {\tt OMLo-7B-Instruct} over-represents males: colon cancer. We define $x_{\text{female}}$ to be the same as $x_{\text{vignette}}$ but for the female-sexed condition `preeclampsia'. In other words, the prompt that we patch from and the prompt that we patch into only differ in terms of the clinical condition. We aim to replace activations $a_\text{gender}$ in $x_{\text{vignette}}$ with `female-ness' activations from $x_{\text{female}}$. Table \ref{tab:colon-cancer} shows female vignette ratios for colon cancer before and after activation patching. Scaling up patched activations flips the gender $100\%$ of the time.

\begin{table}[h]
    \centering
    \begin{tabular}{lcc}
    % Condition&\multicolumn{2}{c}{Olmo-7B-Instruct}\\
    \hline
    pre-patching&$c$&post-patching\\
    \hline 
    $0.08$&$1$&$0.96$\\
    &$2$&$1.0$\\
    \end{tabular}
    \caption{Female vignette ratio for colon cancer before and after activation patching using `preeclampsia'.}
    \label{tab:colon-cancer}
\end{table}

\subsection{Race}
Table \ref{tab:race-distribution} shows the race distribution of {\tt OLMo-7B-Instruct}-generated vignettes. In US-based studies, around $37.6\%$ of adults with sarcoidosis are African American \citep{baughman2016sarcoidosis}, and $21.1\%$ of adults with Hepatitis B are Asian \citep{kruszon2020prevalence}.

\begin{table}[h]
    \centering
    \small
    \begin{tabular}{cccccc}
    \hline
    Condition&Black&White&Asian&Hispanic&Other\\
    \hline
    Sarcoidosis&$0.69$&$0.13$&$0.06$&$0.03$&$0.09$\\
    Hepatitis B&$0.02$&$0.10$&$0.74$&$0.01$&$0.13$
    \end{tabular}
    \caption{Race distribution of {\tt OLMo-7B-Instruct}-generated vignettes.}
    \label{tab:race-distribution}
\end{table}


\section{Depression Risk}
\label{app:depression-risk}

We use the dataset, `Medical Expert Annotations of Unsupported Facts in Doctor-Written and LLM-Generated Patient Summaries', introduced by \citet{hegselmann2024medical}, licensed under The PhysioNet Credentialed Health Data License
Version 1.5.0 \footnote{\url{https://physionet.org/content/ann-pt-summ/view-license/1.0.0/}}. The dataset is derived from MIMIC-IV-Note v2.2 database \citep{mimic-iv}  which includes $331,793$ deidentified free-text clinical notes from $145,915$ patients admitted to the Beth Israel Deaconess Medical Center in Boston, MA, USA. We use the \textit{MIMIC-IV-Note-Ext-DI-BHC} subset, which contains Brief Hospital Courses (BHC)-summary pairs. We use the BHCs in the train-split ({\tt train.json}). 

Table \ref{tab:explicit-depression} shows the difference in risk depression each demographic when the demographic is \textit{explicitly} stated in the prompt. We used the following prompt:

\begin{quote}
\textit{Below is the brief hospital course of a {\tt [DEMOGRAPHIC]} patient.}

\texttt{[BHC]}

\textit{Based on the course, is the patient at risk of depression? Choice: -Yes -No}
\end{quote}

\begin{table}[h]
    \centering
    \begin{tabular}{lc}
    \hline
    Demographic&$\Delta_{\text{risk}}$\\
    \hline
    Gender&$3.6\%$\\
    Race&$4.3\%$
    \end{tabular}
    \caption{Difference in risk depression each demographic when the demographic is \textit{explicitly} stated in the prompt. {\tt OLMo-7B-Instruct} on an average considers females to be at higher risk of depression than males. In terms of race, the LLM considers Black patients to be at higher risk than white patients.}
    \label{tab:explicit-depression}
\end{table}


Table \ref{app:depression-prompts} lists the prompts used to elicit depression risk when demographic is assigned implicitly by patching. Below is a sample prompt after the chat template (\url{https://huggingface.co/allenai/OLMo-7B-Instruct}) is applied. 

\begin{quote}
    \texttt{<|endoftext|><|user|>\newline Below is the brief hospital course of a patient.\newline Brief Hospital Course: ... year old woman with previous diagnosis of .... Follow up with Dr. ... in ... to discuss further testing\newline Based on the course, is the patient at risk of depression? Choice: -Yes -No\newline You must start your answer with "Gender:", followed by the patient's gender.\newline<|assistant\textcolor{red}{|>} }
\end{quote}

To assign the male gender, we use the below prompt.
\begin{quote}
    \texttt{<|endoftext|><|user|>\newline The patient is \textcolor{blue}{Male}.\newline <|assistant|>\newline}
\end{quote}

We patch residual stream activations from \textcolor{blue}{\tt Male} to \textcolor{red}{\tt |>}.

% To confirm that we did not merely copy over the token `Male', we prompted for the gender (in the target prompt) in different ways,  including "you must start with the patient's gender", "you must end with the patient's gender", "state the patient's gender". An incorrect patch may output the token `Male' without implicitly assigning gender. An example output (from one of our incorrect experiments) is \textit{`Male. The patient gender is not explicitly mentioned in the note.'}. Another possibility also is to output the token `Male' but destroy the depression risk task instruction (for example, `Male. The patient has hypertension. The patient has cancer....). We used our validation set to confirm that the main task (depression risk) was also being performed. 





\begin{table*}[!ht]
    \centering
    \begin{tabular}{p{0.3cm}p{12cm}}
    \hline
    &\textbf{Gender}\\
    \hline
    1.&\textit{Below is the brief hospital course of a patient.\newline\texttt{[BHC]}\newline Based on the course, is the patient at risk of depression? Choice: -Yes -No \newline You must start your answer with "Gender:", followed by the patient's gender.}\\
    \hline
    2.&\textit{Below is the brief hospital course of a patient.\newline\texttt{[BHC]}\newline Based on the course, is the patient at risk of depression? Choice: -Yes -No}\\
    \hline
    3.&\textit{Below is the brief hospital course of a patient.\newline\texttt{[BHC]}\newline Based on the course, is the patient at risk of depression? Please be concise.}\\
    \hline
    4.&\textit{Below is the brief hospital course of a patient.\newline\texttt{[BHC]}\newline Based on the course, is the patient at risk of depression? Please be concise. In addition, state the patient's gender.}\\
    \hline
    &\textbf{Race}\\
    \hline
    1.&\textit{Below is the brief hospital course of a patient.\newline\texttt{[BHC]}\newline Based on the course, is the patient at risk of depression? Choice: -Yes -No \newline You must start your answer with "Ethnicity:", followed by the patient's ethnicity.}\\
    \hline
    2.&\textit{Below is the brief hospital course of a patient.\newline\texttt{[BHC]}\newline Based on the course, is the patient at risk of depression? Choice: -Yes -No}\\
    \hline
    3.&\textit{Below is the brief hospital course of a patient.\newline\texttt{[BHC]}\newline Based on the course, is the patient at risk of depression? Please be concise.}\\
    \hline
    4.&\textit{Below is the brief hospital course of a patient.\newline\texttt{[BHC]}\newline Based on the course, is the patient at risk of depression? Please be concise. In addition, state the patient's ethnicity.}\\
    \hline
    \end{tabular}
    \caption{Prompts used to elicit depression risk, given a Brief Hospital Course (\texttt{BHC})}
    \label{app:depression-prompts}
\end{table*}


% hepatitis B
\begin{table}[!ht]
    \centering
    \begin{tabular}{cccccc}
        target & layer & window & factor & ratio \\ 
        \hline
         African American & 4 & 0 & 1 & 0.72 \\ 
         African American & 4 & 0 & 2 & 0.88 \\ 
         African American & 4 & 0 & 5 & 0.53 \\ 
         African American & 4 & 1 & 1 & 0.08 \\ 
         African American & 4 & 3 & 1 & 0.12 \\ 
         African American & 4 & 5 & 1 & 0.73 \\ 
         African American & 19 & 0 & 1 & 0.18 \\ 
         African American & 19 & 0 & 2 & 0.32 \\ 
         African American & 19 & 0 & 5 & 0.49 \\ 
         African American & 19 & 1 & 1 & 0.45 \\ 
         African American & 19 & 3 & 1 & 0.33 \\ 
         African American & 19 & 5 & 1 & 0.36 \\ 
         Caucasian & 4 & 0 & 1 & 0.13 \\ 
         Caucasian & 4 & 0 & 2 & 0.48 \\ 
         Caucasian & 4 & 0 & 5 & 0.5 \\ 
         Caucasian & 4 & 1 & 1 & 0.48 \\ 
         Caucasian & 4 & 3 & 1 & 0.57 \\ 
         Caucasian & 4 & 5 & 1 & 0.61 \\ 
         Caucasian & 19 & 0 & 1 & 0.17 \\ 
         Caucasian & 19 & 0 & 2 & 0.26 \\ 
         Caucasian & 19 & 0 & 5 & 0.54 \\ 
         Caucasian & 19 & 1 & 1 & 0.16 \\ 
         Caucasian & 19 & 3 & 1 & 0.26 \\ 
         Caucasian & 19 & 5 & 1 & 0.33 \\ 
         Hispanic & 4 & 0 & 1 & 0.98 \\ 
         Hispanic & 4 & 0 & 2 & 0.99 \\ 
         Hispanic & 4 & 0 & 5 & 0.98 \\ 
         Hispanic & 4 & 1 & 1 & 0.2 \\ 
         Hispanic & 4 & 3 & 1 & 0.22 \\ 
         Hispanic & 4 & 5 & 1 & 0.97 \\ 
         Hispanic & 19 & 0 & 1 & 0.01 \\ 
         Hispanic & 19 & 0 & 2 & 0.02 \\ 
         Hispanic & 19 & 0 & 5 & 0.12 \\ 
         Hispanic & 19 & 1 & 1 & 0.98 \\ 
         Hispanic & 19 & 3 & 1 & 0.97 \\ 
         Hispanic & 19 & 5 & 1 & 0.97 \\ 
    \end{tabular}
    \caption{Ratio of target race after activation patching for hepatitis B for different scaling factors and window sizes.}
    \label{app:hepb-race-table}
\end{table}

% sarcoidosis
\begin{table}[!ht]
    \centering
    \begin{tabular}{ccccc}
        target & layer & window & factor & ratio \\ 
        \hline
        Asian & 4 & 0 & 1 & 0.86 \\ 
        Asian & 4 & 0 & 2 & 0.88 \\ 
        Asian & 4 & 0 & 5 & 0.82 \\ 
        Asian & 4 & 1 & 1 & 0.34 \\ 
        Asian & 4 & 3 & 1 & 0.34 \\ 
        Asian & 4 & 5 & 1 & 0.71 \\ 
        Asian & 19 & 0 & 1 & 0.74 \\ 
        Asian & 19 & 0 & 2 & 0.92 \\ 
        Asian & 19 & 0 & 5 & 0.91 \\ 
        Asian & 19 & 1 & 1 & 0.82 \\ 
        Asian & 19 & 3 & 1 & 0.84 \\ 
        Asian & 19 & 5 & 1 & 0.78 \\ 
        Caucasian & 4 & 0 & 1 & 0.5 \\ 
        Caucasian & 4 & 0 & 2 & 0.44 \\ 
        Caucasian & 4 & 0 & 5 & 0.38 \\ 
        Caucasian & 4 & 1 & 1 & 0.15 \\ 
        Caucasian & 4 & 3 & 1 & 0.44 \\ 
        Caucasian & 4 & 5 & 1 & 0.54 \\ 
        Caucasian & 19 & 0 & 1 & 0.48 \\ 
        Caucasian & 19 & 0 & 2 & 0.57 \\ 
        Caucasian & 19 & 0 & 5 & 0.61 \\ 
        Caucasian & 19 & 1 & 1 & 0.62 \\ 
        Caucasian & 19 & 3 & 1 & 0.63 \\ 
        Caucasian & 19 & 5 & 1 & 0.7 \\ 
        Hispanic & 4 & 0 & 1 & 0.93 \\ 
        Hispanic & 4 & 0 & 2 & 0.96 \\ 
        Hispanic & 4 & 0 & 5 & 0.9 \\ 
        Hispanic & 4 & 1 & 1 & 0.82 \\ 
        Hispanic & 4 & 3 & 1 & 0.77 \\ 
        Hispanic & 4 & 5 & 1 & 0.98 \\ 
        Hispanic & 19 & 0 & 1 & 0.06 \\ 
        Hispanic & 19 & 0 & 2 & 0.07 \\ 
        Hispanic & 19 & 0 & 5 & 0.17 \\ 
        Hispanic & 19 & 1 & 1 & 0.88 \\ 
        Hispanic & 19 & 3 & 1 & 0.88 \\ 
        Hispanic & 19 & 5 & 1 & 0.91 \\ 
    \end{tabular}
    \caption{Ratio of target race after activation patching for sarcoidosis for different scaling factors and window sizes.}
    \label{app:sarc-race-table}
\end{table}

\begin{figure*}[htbp]
    \centering
    \begin{subfigure}[b]{0.43\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/diff_diag_race_explicit.pdf}
        \caption{Race -- explicit}
        \label{fig:diff-diag-race-explicit}
    \end{subfigure}
    % \hfill
    \begin{subfigure}[b]{0.43\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/diff_diag_race_implicit.pdf}
        \caption{Race -- implicit}
        \label{fig:diff-diag-race-implicit}
    \end{subfigure}
    % }

    \caption{Rank distribution of the correct diagnosis for explicit and implicit race assignment. We see a similar trend in rank difference in both strategies. }
    \label{fig:diff-diag-race}
\end{figure*}

\section{Differential Diagnosis}
\label{app:diff-diagnosis}
We use the following prompt for eliciting ranked differentials from the LLM for a case:
\begin{quote}
    \textit{You are an expert diagnostician. Below is a brief summary of a case. Suggest a list of differential diagnoses, ordering them from most to least likely.\newline
    {\tt [CASE]}}
\end{quote}

\noindent For gender, we set {\tt [CASE]} to:
\begin{quote}
\textit{A 63-year-old patient presents with acute-on-chronic cough with a change in sputum character and trace hemoptysis and is found to have tachycardia, tachypnea, and hypoxemia.}
\end{quote}

\noindent For race, we set {\tt [CASE]} to:

\begin{quote}
\textit{A 54-year-old patient with a history of aortic stenosis and travel to South America presents with subacute progressive dyspnea, intermittent fevers, a cough that produces pink sputum, orthopnea, and unintentional weight loss. They are found to be febrile, hypoxemic, tachypneic, and tachycardic.}
\end{quote}

The cases are adopted from \citet{zack2024assessing}'s set up of studying disparity in differential diagnosis ranking. When explicitly prompting, we replace the token `patient' in {\tt [CASE]} with the target demographic (\textit{`male'}/\textit{`female'}/\textit{`Caucasian male'}/\textit{`Black male'}). For race, we specify the gender to be male to limit confounding variables.






\end{document}
