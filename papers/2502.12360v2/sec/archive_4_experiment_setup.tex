% \section{Experiment Setup}
% \label{sec:experiment_setup}

% \textbf{Datasets:} We use four datasets in our experiments to test different black-box DNNs-under-test. First, as a proof-of-concept for our overall approach, we use the CelebA~\citep{liu2015faceattributes} dataset with a rich collection of 40 facial attributes (metadata) for $202\,599$ images of celebrity faces. We use the aligned PNG images provided by the authors, which are at a resolution of $178 \times 218$ pixels.
% Next, for the pedestrian detection tasks, we consider BDD100k~\citep{yu2020bdd100k}, Cityscapes~\citep{cordts2016cityscapes}, and RailSem19~\citep{Zendel_2019_CVPR_Workshops}. In these datasets, we focus only on the pedestrian class in the 2D-bounding box and semantic segmentation tasks. For BDD100k, we consider the predefined validation set of 10k samples of resolution $1280 \times 720$, while in Cityscapes and RailSem19, due to their smaller dataset sizes, we use the entire train and val sets containing 3475 and 8500 samples with image resolutions $2048 \times 1024$ and $1920 \times 1080$ respectively.
% % \ma{Comparison to SOTA approaches, namely PromptAttack \cite{Metzen_2023_ICCV} and CounTEX \cite{kim2023grounding}, is provided using above CelebA and ``Animals with Attributes 2'' (AwA2) dataset~\cite{xian2018zero}, where for the latter we focus on the ``hippopotamus'' class.}
% %In addition, to compare against SOTA approaches, we use results from CelebA and Animals with Attributes2 (AwA2) dataset~\cite{xian2018zero} which contains 50 animal classes and 85 attributes. However, for our benchmarking experiment, we focus primarily on the "hippopotamus" class. 

% \textbf{Models (DNNs-under-test):} We evaluate four black-box models for ODD aligned systematic weaknesses. For the first experiment, we consider the publicly available ViT-B-16~\citep{dosovitskiy2020image} model pre-trained on ImageNet21k~\citep{ridnik2021imagenet} from the python library timm, \mar{\cite{timm_github}}.\footnote{\url{https://github.com/huggingface/pytorch-image-models}}
% % Although this model is trained to classify~11k classes, we focus on the hierarchy level 1 semantic classes as proposed in \cite{ridnik2021imagenet}.
% % As we only focus on data subset~\textit{"n00007846"}, all images do in fact contain "person". 
% Second, we use the pre-trained publicly available Faster R-CNN~\citep{ren2015faster} object detector with ConvNeXt-T~\citep{liu2022convnet} backbone. The model weights are available on the BDD100k model Zoo.\footnote{\url{https://github.com/SysCV/bdd100k-models/tree/main/det}} Third, for the Cityscapes dataset, we use a pre-trained SETR PUP~\citep{zheng2021rethinking} semantic segmentation model. The model weights are available on the mmsegmentation codebase, \mar{\cite{mmseg2020}}.\footnote{\url{https://github.com/open-mmlab/mmsegmentation}}
% Finally, from the railway domain, we use a PanopticFCN~\citep{li2021fully} model, which has been trained by an industrial partner on a large proprietary dataset also including  RailSem19~\citep{Zendel_2019_CVPR_Workshops}. We consider this as a complete black box and have no details on the concrete training. 
% For the autonomous datasets, the black-box model performance per object (i.e., pedestrian) is measured by the intersection-over-union (IoU).

% \textbf{Parameters} of CLIP and SliceLine:
% For the metadata generation, we use a pre-trained CLIP~\citep{radford2021learning} with image encoder (ViT-L/14~\citep{dosovitskiy2020image}).
% For SliceLine, we use a python implementation\footnote{\url{https://github.com/DataDome/sliceline}} and choose default $\alpha$ and $\sigma$ values of $0.95$ and $n/100$ \sg{move this to the appendix} where $n$ defines the size of the structured data as proposed in \citep{sagadeeva2021sliceline}. We run the experiments for a depth level of 5 and present the top-5 weak slices. 
% To foster reproducibility, we will provide the overall code and the prompts used for metadata generation with CLIP.
% %\sg{code and prompts will be provided}


