\section{Method}
\label{sec:method}


\begin{figure*}[!bht] % failed to  change figure position -MA
  \centering
  % \includegraphics[width=\textwidth]{images/workflow.pdf}
  
    \input{images/pipeline}
  \caption{Our algorithm for finding systematic weaknesses of CV models. Given a model, a test dataset, and an ODD description for the objects we are interested in, we build a database of object-level performance and metadata in a structured format. Weak slice discovery methods are then applied to this database to identify top-k weak slices of the model.}
  \label{fig:workflow}
\end{figure*}

In this section, we present our algorithm for weakness detection on the basis of human-understandable semantic dimensions. To this end, we introduce notation regarding metadata and slicing, discuss the generation of metadata, formulate DNN weakness within a Bayesian framework to account for the impact of noise, and lastly detail how such impact can be acknowledged within slice discovery algorithms.




% \subsection{Notation}

\textbf{Notation}: Consider a DNN-under-test (\textbf{DuT}) $M$ trained on some computer vision task. Let $\mathcal D$ be the (test) dataset containing the inputs and the corresponding task-related ground truth.
For each sample $s_i\in\mathcal D$, using some per-sample performance metric (e.g.,\ intersection over union (IoU)) and, if applicable, applying some threshold, we obtain binarized \textbf{DuT} errors, defined as $e_i \in \{0, 1\}$. Each sample is either correctly ($e_i=0$) or incorrectly ($e_i=1$) predicted by the \textbf{DuT}.
Here, we slightly deviate from conventional notation by considering individual samples rather than the \textbf{DuT} inputs.
Although identical for image classification, in the case of object detection, multiple samples (i.e.,\ objects) may be present in a given input image, over which inference is performed.
% For each sample $s$, a per-sample performance metric $p_s$ of model $M$ (e.g., intersection over union IoU) is defined with $0 \leq p_s \leq 1$.
Using a set of samples with individual errors $e_i$ allows us to define slices $\mathcal S\subseteq\mathcal D$ of the data and their corresponding (average) error rate $\restr{\bar{e}}{\mathcal S}$, defined as $\frac{1}{|\mathcal{S}|}\sum_{s\in\mathcal{S}}{e_s}$.
One of the goals of slice discovery methods is to find slices where $\restr{\bar{e}}{\mathcal S}$ is significantly worse than the global average $\restr{\bar{e}}{\mathcal D}$.
However, this constraint alone could be trivially satisfied by selecting all samples where $e_i=1$.
But, this, in general, would reveal no further information than the known data-points with bad performance.

As motivated in~\cref{sec:intro}, slice discovery methods aim to provide further information about DNN weaknesses by attempting to find semantically coherent slices.
This is typically achieved using some scheme that determines whether a specific sample $s_i$ belongs to the set $\mathcal S$. The scheme relies on additional information beyond what is provided solely by $e_i$.
This additional information may then be used to infer the cause of the weakness.
Several existing works~\citep{d2022spotlight, eyuboglu2022domino, jain2023distilling} base their schemes on distance measures in the latent spaces of the samples, so that the resulting clusters, i.e.,\ slices, require further interpretation.
In contrast, this work bases the information on metadata, more concretely a predefined set $\mathcal Z$ of semantic dimensions and corresponding attributes that describe the samples. For instance, in pedestrian detection, such dimensions can be fairness-related, such as ``gender'' or ``age'', with attributes like ``young'' or ``old'' but may also include other safety-relevant aspects such as ``occlusion'' or ``clothing-color''.
Such an algorithm offers direct interpretability of the slices, and selected dimensions can be aligned, e.g., with existing safety considerations in the form of Operational Design Domains (ODDs) for the respective systems. The set $\mathcal Z$ used in this work was inspired by \citet{herrmann2022using} in the context of automotive ODDs.




In~\cref{fig:workflow}, we present our algorithm, where, using inputs such as a dataset, a predefined ODD, and a \textbf{DuT}, we transform the task of finding systematic weaknesses in the unstructured data domain into a problem in the structured data domain. The algorithm is designed with modular components for adaptability. The first module handles the generation of structured metadata, while the second module applies the weak slice search algorithm to the generated structured metadata. With a structured description of the data, we can formulate slices as rules over $\mathcal Z$, e.g.,\ $\text{gender} = \text{male}\wedge \text{occlusion} = (0.9, 1.0]$. This allows for a more probabilistic notation $p(e|\mathcal{S})$ of the expected error given the slice. Slice discovery is then the task of finding (coherent) conditions $\mathcal S$ such that the conditional expectation is maximized.


% \subsection{ODD Descriptions and Metadata Generation}
\textbf{Metadata Generation}:  While there is great interest from safety experts and certification bodies in ODDs for safety argumentation, metadata that align with the ODD are scarcely available for most, particularly image, domains.
Human annotation of such metadata is often out of scope for large datasets due to cost and time constraints.
However, an automated metadata generation module that captures different semantic dimensions of $\mathcal{Z}$ is feasible with existing technologies. 
For example, a multi-modal foundational model like CLIP~\citep{radford2021learning} with its joint image and text embedding space could be a potential candidate for such automated annotation out of the box or after fine-tuning. 
For a given attribute $a$ we can use CLIP as a zero-shot classification function $\mathcal G$, which maps a given sample $s_i$ onto the attributes, which represent the potential classes. As such, it therefore provides the coherence of the slices discussed above.

Taking the ontology for pedestrians from the automotive domain as a baseline, a qualitative evaluation of CLIP's capability was performed by~\citet{gannamaneni2023investigating}.
While CLIP achieved SOTA level zero-shot performance on different dimensions such as gender, skin-color, and age for portrait shots of human faces in the celebA~\citep{liu2015faceattributes} dataset, they observed a drop in performance on real-world datasets containing pedestrians like in the Cityscapes~\citep{cordts2016cityscapes} dataset.
The drop in performance can be attributed to more challenging conditions, such as complex poses, low illumination, and high occlusion. 
These observations, along with our experiments, show that the classification function $\mathcal G$ is subject to varying degrees and types of uncertainty, depending on the dimensions of $\mathcal Z$:
(i) the presence of data-based (aleatoric) uncertainties, i.e.,\ where the image resolution is low or the object in question is heavily occluded or distant, leading to errors in the generated metadata. (ii) the presence of model-based (epistemic) uncertainties, i.e.,\ where the function $\mathcal G$ exhibits suboptimal performance. While (i) can occur in the case of both human and CLIP-based annotation, (ii) occurs more prominently in non-human, automated labeling.\footnote{High-quality human labeling typically requires multiple measures to reduce inter-observer variability or epistemic uncertainty in general (e.g.\ via labeling guides). However, in this work, we consider human labeling as high-quality compared to DNN-based labeling.} Therefore, any method that aims to consider metadata generated using such techniques should take into account the incurred noise in downstream tasks.

\textbf{Bayesian Framework to Account for
the Impact of Noise}: To address the uncertain nature of classification, we extend the previous slice notation of the error to the joint probability $p(e,\mathcal C, \mathcal S)$, where $\mathcal C$ represents the outcome of automated labeling for some attribute of a dimension, while $\mathcal S$ denotes the corresponding ground truth. 
For simplicity, we drop the indices and make the additional assumption that $\mathcal S,\mathcal C$ can be seen as binary, i.e.\ they may either be true ($\mathcal S$, $\mathcal C$) or not true ($\neg\mathcal S$, $\neg\mathcal C$), respectively (for details on the non-binary case, see~\cref{appendix:level1_precisions}).
Using Bayes' Theorem and marginalizing over $\mathcal C$ or $\mathcal S$, we can express
\begin{align}
    p(e|\mathcal{S}) = & p(e|\mathcal C, \mathcal S)r_\mathcal C + p(e|\neg\mathcal C, \mathcal S) \,(1-r_\mathcal C)\,,
    \label{eq:pEgivenS}
    \\
    p(e|\mathcal C)=&p(e|\mathcal C,\mathcal S)p_\mathcal C+p(e|\mathcal C,\neg \mathcal S)\,(1-p_\mathcal C)\,.
    \label{eq:pEgivenC}
\end{align}
Here, $p(e|\mathcal{S})$ represents the true slice error, while $p(e|\mathcal{C})$ denotes the observed slice error. Furthermore, $p_\mathcal C = p(\mathcal S|\mathcal C)$ and $r_\mathcal C= p(\mathcal C|\mathcal S)$ are shorthand for precision and recall of the labeling function $\mathcal G$ measured towards the ground truth, and are used in our algorithm,~\cref{fig:workflow}, for the quality check.
%{represented in~\cref{fig:workflow} as the sample based metadata quality check}.
A detailed derivation of the equations is provided in~\cref{appendix:derivation_1}.
Making these relations explicit allows us to investigate the hypothesis typically underlying Slice Discovery Methods in more detail. Specifically, based solely on the observed slice performance/weakness $p(e|\mathcal C)$, one may conclude that a related data property $\mathcal S$ represents a weakness of the \textbf{DuT}, i.e.,\ we assume that $p(e|\mathcal S)$ also has a comparable performance/weakness. While in our algorithm the relation between $\mathcal S$ and $\mathcal C$ is explicit as the latter is given by a classifier for the former, in other approaches~\citep{d2022spotlight, eyuboglu2022domino, jain2023distilling} the relation is implicit, as observed sets $\mathcal C$ are interpreted to indicate a meaning of $\mathcal S$ (typically referred to as a slice label).
Another assumption typically made is the independence between the labeling function $\mathcal G$ and \textbf{DuT}. This independence would imply that the errors of the DuT do not depend on the noise (errors) of $\mathcal G$. Specifically, for a semantic attribute, the error rates $p(e|\mathcal C, \mathcal S)$ when $\mathcal G$ is correct and the error rate $p(e|\neg\mathcal C,\mathcal S)$ when it is not should be (approximately) equal. However, our experiments indicate that this is not always the case;
therefore, we denote the difference by
\begin{equation}
    \delta p(e|\mathcal S) = p(e|\neg\mathcal C, \mathcal S)-p(e|\mathcal C, \mathcal S)\,.
\end{equation}

Please note that $\delta p$ describes intra-set variances of the error rate in the set $\mathcal S$ and is not a conditional probability on its own.
Taking into account this potential dependence, we can derive the true error from the observed error exactly given the performance of the annotation process using
\begin{equation}\label{eq:correction_equation}
    p(e|\mathcal S) = \underbrace{\frac{p(e|\mathcal C)\,p_{\neg\mathcal C}+p(e|\neg\mathcal C)\,(p_\mathcal C-1)}{p_\mathcal C + p_{\neg\mathcal C}-1}}_\text{independence assumption}
    +\underbrace{\delta p(e|\mathcal S) \overbrace{\left(\frac{p_\mathcal C p_{\neg\mathcal C}}{p_\mathcal C + p_{\neg\mathcal C}-1}-r_\mathcal C\right)}^{\kappa_\mathcal S} 
    +\delta p(e|\neg\mathcal S)\overbrace{\frac{(p_\mathcal C -1)p_{\neg\mathcal C}}{p_\mathcal C + p_{\neg\mathcal C}-1}}^{\kappa_{\neg\mathcal S}}}_\text{correction terms}\,.
\end{equation}
As long as the independence assumption is (approximately) valid, implying $\delta p(e|\mathcal S)\approx \delta p(e|\neg\mathcal S)\approx 0$, the slice error given the semantic attribute $\mathcal S$ is obtained by separating the two types of observed error probabilities $p(e|\mathcal C)$, which is possible as long as the denominator is non-zero.\footnote{For the sake of numeric stability, also denominators which are only approximately zero should be discarded.}
An analysis of properties of this equation w.r.t.\ the denominator allows us to automatically create quality indicators on the validity or invalidity of the obtained corrected slices for attribute $\mathcal S$. The full derivation and further details on quality indicators can be found in~\cref{appendix:derivation_2}.


\textbf{Weak Slice discovery on Structured ODD Data with SliceLine}: We have now established methods to generate metadata and correct for noise during the metadata generation.
With this background, in~\cref{algo:combined_sliceline}, we propose three-stages for Systematic Weakness Detection (SWD-1,2,3). In SWD-1, using the generated structured metadata and observed errors $p(e|\mathcal C)$, we employ algorithms such as SliceLine~\citep{sagadeeva2021sliceline} to provide a ranked list of top-$k$ worst performing slices based on a scoring function that takes into account the errors and sizes of the slices (see~\cref{eq:scoring_function_orig} in~\cref{appendix:sliceline_workflow} for details on how SliceLine works).
As we have motivated, observed errors may not always provide a sufficient signal to identify the underlying error (see the top row in~\cref{fig:synthetic_data}). Therefore, in SWD-2, using~\cref{eq:correction_equation} to compensate for noise in the metadata, we provide corrected errors instead of observed errors to SliceLine to provide a second ranked list of top-$k$ worst-performing slices $\mathcal{S}$. However, as it requires extensive human effort to identify certain parameters, i.e., $\delta p(e|\mathcal S)$,\ $\delta p(e|\neg\mathcal S)$ in~\cref{eq:correction_equation}, in particular for combinations of semantics, we make a cheaper approximation only considering the independence assumption part of the equation. This is implemented in \texttt{computeCorrectedError()} in \cref{algo:combined_sliceline}. To operationalize this part of the equation, we estimate precision values based on human evaluation of metadata quality on only $n=60$ samples per attribute (see~\cref{appendix:level1_precisions}). The subsequent corrected errors from this independence assumption are used in the SliceLine scoring function.
Based on the slice quality indicators discussed above, we are also able to discard invalid slices due to denominator values close to zero.
In addition to SWD-1 and SWD-2, we also consider a merge of the resulting slices from SWD-1 and SWD-2, as this might provide a complementary effect. We refer to this merged list as the output of SWD-3. The merge step includes sorting based on the score of the slice from the scoring function, removal of duplicate slices, and filtering of invalid slices. 
The SliceLine hyperparameters include the level (maximal search depth), i.e. the maximal number of semantic dimensions considered simultaneously, as well as a cut-off for the necessary slice error $\restr{\bar{e}}{\mathcal S}$ to consider $\mathcal S$ a valid slice.



\IncMargin{1em}
\begin{algorithm}[H]
\caption{Systematic Weakness Detector (SWD)}\label{algo:combined_sliceline}
\KwIn{Metadata $\{\mathcal{C}_{\mathcal Z_1}, \mathcal{C}_{\mathcal Z_2}, \dots\}$, errors $e$, Precision vectors $\{p_{\mathcal{C}}\}$, SliceLine hyper-parameters}
\KwOut{Top-K slices $TS$}

\textbf{SWD-1: SliceLine with observed errors $p(e|\mathcal{C}_i)$}\;
\Indp
    $[TS_1] \gets \text{SliceLine}(\{\mathcal{C}_{\mathcal Z_1}, \mathcal{C}_{\mathcal Z_2}, \dots \}, e, \text{hyperparameters})$\; 
\Indm  
\textbf{SWD-2: SliceLine with corrected errors (approximations to $p(e|\mathcal{S}_i)$)}\;
\Indp
 $[TS_2, \text{Quality Indicators}] \gets \text{SliceLine}(\{\mathcal{C}_{\mathcal Z_1}, \mathcal{C}_{\mathcal Z_2}, \dots\}, \textbf{computeCorrectedError}(e, p_{\mathcal{C}}) , \text{hyperparameters})$\;
\Indm  
\textbf{SWD-3: Combined Slices}\;
\Indp
    $[TS] \gets \text{Merge}(TS_1 \cup TS_2)$\; 
\Indm

\Return TS\;
\end{algorithm}
\DecMargin{1em}

