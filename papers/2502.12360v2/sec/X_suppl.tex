\clearpage
\setcounter{page}{1}
% \maketitlesupplementary
\begin{center}
    \Large
    \textbf{Appendix}
   
\end{center}

% \section{Rationale}
% \label{sec:rationale}
% % 
% Having the supplementary compiled together with the main paper means that:
% % 
% \begin{itemize}
% \item The supplementary can back-reference sections of the main paper, for example, we can refer to \cref{sec:intro};
% \item The main paper can forward reference sub-sections within the supplementary explicitly (e.g. referring to a particular experiment); 
% \item When submitted to arXiv, the supplementary will already included at the end of the paper.
% \end{itemize}
% % 
% To split the supplementary pages from the main paper, you can use \href{https://support.apple.com/en-ca/guide/preview/prvw11793/mac#:~:text=Delete%20a%20page%20from%20a,or%20choose%20Edit%20%3E%20Delete).}{Preview (on macOS)}, \href{https://www.adobe.com/acrobat/how-to/delete-pages-from-pdf.html#:~:text=Choose%20%E2%80%9CTools%E2%80%9D%20%3E%20%E2%80%9COrganize,or%20pages%20from%20the%20file.}{Adobe Acrobat} (on all OSs), as well as \href{https://superuser.com/questions/517986/is-it-possible-to-delete-some-pages-of-a-pdf-document}{command line tools}.
\appendix

\section{Experiment Setup}
\subsection{Datasets, DNNs, Hyperparamters}
\label{appendix:experiment_setup}

\textbf{Datasets:} We use four datasets in our experiments to test different black-box DNNs-under-test. First, as a proof-of-concept for our overall approach, we use the CelebA~\citep{liu2015faceattributes} dataset with a rich collection of 40 facial attributes (metadata) for $202\,599$ images of celebrity faces. We used the aligned PNG images provided by the authors, which have a resolution of $178 \times 218$ pixels.
Next, for pedestrian detection tasks, we consider BDD100k~\citep{yu2020bdd100k}, Cityscapes~\citep{cordts2016cityscapes}, and RailSem19~\citep{Zendel_2019_CVPR_Workshops}. In these datasets, we focus only on the pedestrian class in the 2D-bounding box and semantic segmentation tasks. For BDD100k, we consider the predefined validation set of 10k samples of resolution $1280 \times 720$, while in Cityscapes and RailSem19, due to their smaller dataset sizes, we use the entire train and validation sets containing 3475 (the test set is not considered due to the lack of GT) and 8500 samples with image resolutions $2048 \times 1024$ and $1920 \times 1080$ respectively.


\textbf{Models (DNNs-under-test):} We evaluate four black-box models for ODD aligned systematic weaknesses. For the first experiment, we consider the publicly available ViT-B-16~\citep{dosovitskiy2020image} model pre-trained on ImageNet21k~\citep{ridnik2021imagenet} from the python library timm.\footnote{https://github.com/huggingface/pytorch-image-models}
Second, we use the pre-trained publicly available Faster R-CNN~\citep{ren2015faster} object detector with ConvNeXt-T~\citep{liu2022convnet} backbone. The model weights are available on the BDD100k model Zoo.\footnote{https://github.com/SysCV/bdd100k-models/tree/main/det} Third, for the Cityscapes dataset, we use a pre-trained SETR PUP~\citep{zheng2021rethinking} semantic segmentation model. The model weights are available on the mmsegmentation codebase.\footnote{https://github.com/open-mmlab/mmsegmentation}
Finally, from the railway domain, we use a PanopticFCN~\citep{li2021fully} model, which has been trained by an industrial partner on a large proprietary dataset also including RailSem19~\citep{Zendel_2019_CVPR_Workshops}. We consider this as a complete black box and have no details on the concrete training procedure. 
For the autonomous datasets, the black-box model performance per-object (i.e., pedestrian) is measured by the intersection-over-union (IoU).

\textbf{Parameters} of CLIP and SliceLine:
For metadata generation, we use a pre-trained CLIP~\citep{radford2021learning} with image encoder (ViT-L/14~\citep{dosovitskiy2020image}).
For SliceLine, we use a python implementation and choose default $\alpha$ and $\sigma$ values of $0.95$ and $n/100$ where $n$ defines the size of the structured data as proposed in \citet{sagadeeva2021sliceline}. For the synthetic data experiment, we incrementally increase k from 1 to 60. 

\subsection{Synthetic Data Generation Parameters}
\label{appendix:synthetic_data}

The purpose of the synthetic data experiment is to evaluate the algorithm with control over the quality of labeling and without the influence of correlations. Therefore, we build a tabular dataset with 9 ``real'' semantic dimensions (dim1, \dots, dim9) containing 200,000 rows. For each of these dimensions, we generate a synthetic dimension as a proxy for labeling by CLIP. All dimensions contain binary attributes. For the first five dimensions, the distribution of true attribute is imbalanced, i.e., only 5\% of overall samples ([8000, 9000, 10000, 11000, 12000]), respectively. The other dimensions are balanced between both attributes. The final column contains errors simulating the \textbf{DuT} performance. Next, we define a set of slices and induce errors for each of the slices. For our experiments, we induce the following errors: \{dim1: 0.19, dim2 \& dim3: 0.18, dim3: 0.23, dim4: 0.3, dim5: 0.07, dim6: 0.04, dim7: 0.01, dim8: 0.05, dim9: 0.02\}. As we have 100 runs for different labeling qualities, we introduce random fluctuations between -0.01 and 0.01 to these error values. The choice of errors and number of dimensions is to align the synthetic data with the celebA experiment and also to effectively induce errors. If all dimensions contribute roughly equally to the error rate, no strong signal for a specific slice, in contrast to the others, could be found. We generate 100 runs each for 3 different labeling qualities.
That is, we generate the ``observed'' metadata from the ``real'' one using a random predefined ``precision'' value.
For good quality, this precision value to detect attribute 1 of each dimension is sampled from a uniform distribution between 0.8 and 1.0. For attribute 0, we sample between 0.8 and 1.0. For medium quality and attribute 1, we sample from 0.4 and 0.6 and for attribute 0 between 0.4 and 0.7. For bad quality, for attribute 1, we sample between 0.1 and 0.4 and for attribute 0 between 0.3 and 0.6.



\subsection{Human-understandable Dimensions}
%\section{Safety Relevant Dimensions}
\label{appendix:odds_used}

\citet{herrmann2022using} have proposed ontologies for different dynamic objects (e.g., pedestrians) to build ODDs for AD vehicles. Although these proposed ontologies do not yet completely capture all safety-relevant features, they provide a reference to the direction safety experts intend to take to build evidences for safety augmentations of AD vehicles. To enable such a formulation of evidence, we performed our experiments on a subset of the concepts discussed in these ontologies as shown in~\cref{table:AD_ontology,table:celebA_ontology}. In the case of BDD100k, as information about occlusion is provided in the dataset, we combine our generated metadata with this additional information. For the CelebA experiment, as the input distribution is not directly related to the AD domain, we consider semantic concepts that are more suitable for this dataset as shown in~\cref{table:celebA_ontology}. Similar to~\citet{gannamaneni2023investigating}, we encode the input image using the CLIP image encoder. For celebA, we encode the entire input image, while for the AD experiments, we encode individual pedestrian crops as a single input. We consider each semantic dimension and its corresponding attributes to generate metadata for an input image. 

\begin{table}[htbp!]
\centering
\setlength{\tabcolsep}{4pt} % set column spacing

\begin{tabular}{c|cc} 
 \hline
 Semantic dimension & \multicolumn{2}{c}{Attributes}   \\ 
 \hline\hline
 Gender             & Male & Female \\
 Pale-skin         & True & False \\
 Age                & Young & Adult \\
 % Hair color	       & Black & Blond & Gray & Brown\\ 
 Beard              & True & False   \\ 
 Goatee             & True & False \\
 Bald               & True & False \\
 Wearing-Hat        & True & False \\
 Wearing-Eyeglasses & True & False \\
 Smiling            & True & False \\
\hline
\end{tabular}
\caption{The ODD used for the celebA experiment. The first column represents the different semantic dimensions (in analogy to safety-relevant features). For each dimension, different attributes are considered and generated as metadata using our metadata generation process.}
\label{table:celebA_ontology}
\end{table}


\begin{table*}[htbp!]
\centering
% \setlength{\tabcolsep}{3pt} % set column spacing

\begin{tabular}{c|ccccc} 
 \hline
 Semantic dimension & \multicolumn{2}{c}{Attributes}   \\ 
 \hline\hline
 Gender              & Male & Female \\
 Skin color          & White & Dark \\
 Age                 & Young & Adult \\
 % Hair color	        & Black & Blond & Gray & Brown\\ 
 Clothing color      & Bright-color & Dark-color \\
 Blurry              & True & False \\
 $\text{Occlusion}^\dag$         &    True   &   False      \\ 
 $\text{Construction-worker}^{\ddag}$ & True & False \\
 Size                & 10 quantile binned values of bounding box pixel area\\ 

\hline
\end{tabular}
\caption{A sample ontology for pedestrians used in our AD dataset experiments. The first column represents the different semantic dimensions (safety relevant features). For each dimension, different attributes are considered and generated as metadata using our metadata generation process. Metadata that is generated from CLIP but from available through other sources (e.g., GT) is not considered noisy and, therefore, we do not perform precision and recall estimation by human sampling. $\dag$ Occlusion is available as GT from the BDD100k dataset and we only consider it in corresponding experiment. ${\ddag}$ RailSem19 dataset contains several images where construction-workers are present near railway tracks. Therefore, we additionally consider this dimension for CLIP labeling to identify if models have weaknesses identifying construction workers. Size of pedestrian is estimated by calculating product of bounding width and height.}
\label{table:AD_ontology}
\end{table*}


\subsection{SliceLine Workflow}
\label{appendix:sliceline_workflow}

SliceLine works on individual errors $e_i$ of data samples $i$. These, in the original work, can be defined as $e_i=1-p_i$ with the DuT predicted probability $p_i$ for the correct class. In the remainder, we make the simplifying assumption that $e_i\in\{0,1\}$ indicates whether $i$ was classified correctly, $e_i=0$, or not, $e_i=1$.
The workflow of SliceLine to identify weak slices is as follows: Initially, for depth level 1, a breadth search is performed on all attributes in the metadata such that only single features form a slice (e.g., a slice containing all data points with condition $(gender: male)$).
Checks are performed over these slices to ensure that thresholds are met (e.g., minimum slice size specified via some parameter $\sigma$). 
Next, based on the slice scores from~\cref{eq:scoring_function_orig}, the slices are ordered, and a list of top-k weak slices is populated.
The hyperparameter $\alpha$ in \cref{eq:scoring_function_orig} allows us to weight the size of the slice as well as the error signal.
At depth level 2 and above, combinations of two attributes are chosen to form a slice (e.g., slice containing all data points with condition $(gender = male) \& (occlusion = (0.9, 1.0])$).
The list of weak slices is updated after each depth level. The maximum depth level is a hyperparameter.
In addition, pruning steps are also performed at each depth level in the original implementation.  
The conditions for pruning have a monotonicity property, which ensures that all potential sub-slices of a pruned slice would also fulfill the pruning condition. 
Due to the limited sizes of the ODDs for our experiment, we do not consider the pruning step in our implementation.
Once the maximum depth level has been reached, the algorithm is terminated and the final list of top-$k$ weak slices are available. 


\begin{equation}\label{eq:scoring_function_orig}
    \text{Scoring Function}(\mathcal{S}) = \alpha\,\frac{\restr{e}{\mathcal{S}}-\restr{e}{\mathcal{D}}}{\restr{e}{\mathcal{D}}} - \left(1 - \alpha\right)\,\frac{|\mathcal{D}|-|\mathcal{S}|}{{|\mathcal{S}|}}
    %\text{Scoring Function}(\mathcal{S}) = \alpha\left(\cfrac{\bar{e}_\mathcal{S}}{\bar{e}_{\mathcal{D}}} - 1\right) - \left(1 - \alpha\right)\left(\cfrac{|\mathcal{D}|}{{|\mathcal{S}|}} - 1\right)
\end{equation}    




\section{Derivations}
\subsection{Derivation of \texorpdfstring{$p(e|\mathcal C)$}{p(e|C)} and  \texorpdfstring{$p(e|\mathcal S)$}{p(e|S)}}
\label{appendix:derivation_1}


To derive \cref{eq:pEgivenS} and \cref{eq:pEgivenC} from \cref{sec:method}, we first consider the joined probability $p(e,\mathcal C,\mathcal S)$, where $e$ denotes the DuT error, $\mathcal{C}$ labeling, and $\mathcal{S}$ the ground truth for some semantic attribute. Using Bayes' Theorem we can rewrite this as
\begin{equation}
    p(e,\mathcal C,\mathcal S) =
    p(e|\mathcal{C}, \mathcal S) p(\mathcal{C}, \mathcal S) =
    p(e|\mathcal{C}, \mathcal S) p(\mathcal{C}| \mathcal S) p(\mathcal S)
    \,.
\end{equation}
Looking additionally at marginal distributions
\begin{equation}
    p(e,\mathcal S) = \sum_\mathcal{C} p(e,\mathcal C, \mathcal S) 
    = p(\mathcal S) \sum_\mathcal{C} p(e|\mathcal C, \mathcal S)p(\mathcal C| \mathcal S)
    \,,
\end{equation}
where the sum goes over all possible values $\mathcal C$ can take.
We can write the conditional error probability (or rate if considered over finite data) as
\begin{equation}
    p(e|\mathcal S) = \sum_\mathcal{C} p(e|\mathcal C, \mathcal S)p(\mathcal C| \mathcal S)\,
\end{equation}
At this point, using that $\mathcal C$ takes only binary values, which, for brevity, we denote as $\mathcal C$ if the attribute was detected and as $\neg\mathcal{C}$ else,\footnote{This is a slight over-use of the notation, but it is apparent from context whether $\mathcal C$ is meant as the random variable for the labeling, or as its value in the sense of positive detection.} we can expand the sum:
\begin{equation}
    p(e|\mathcal S) = p(e|\mathcal C, \mathcal S)p(\mathcal C| \mathcal S)
    + p(e|\neg\mathcal C, \mathcal S)p(\neg\mathcal C| \mathcal S)
    \,
\end{equation}
Within this expression, we can identify the recall
\begin{equation}
    r_\mathcal{C} \equiv p(\mathcal C | \mathcal S)
\end{equation}
of the labeling method, that is the probability we will obtain correct identification of the semantic attribute given its presence.
Using further the normalisation property
\begin{equation}
    1=\sum_\mathcal{C}p(\mathcal C| \mathcal S)
    \quad\rightarrow\quad
    p(\neg\mathcal C|\mathcal S) = 1-p(\mathcal C| \mathcal S)
    \,,
\end{equation}
we arrive at the originally presented \cref{eq:pEgivenS}:
\begin{equation}
    \label{eq:prec_and_recall}
    p(e|\mathcal S) = r_\mathcal{C}\,p(e|\mathcal C, \mathcal S)
    + \left(1-r_\mathcal{C}\right)\,p(e|\neg\mathcal C, \mathcal S)
    \,
\end{equation}
Along the same lines \cref{eq:pEgivenC},
\begin{equation}
     p(e|\mathcal C) \equiv  p_\mathcal{C}\,p(e|\mathcal C, \mathcal S)
    + \left(1-p_\mathcal{C}\right)\,p(e|\mathcal C, \neg\mathcal S)
    \,,
\end{equation}
can be derived, however with the identification
\begin{equation}
    p_\mathcal{C}= p(\mathcal S|\mathcal C)\,,
\end{equation}
i.e., the precision of the labeling process.




\subsection{Derivation of  Correction Equation}
\label{appendix:derivation_2}


As discussed in~\cref{sec:method}, the annotation process may not be a perfect process. Furthermore, there is no guarantee that the failure modes within this process do not overlap the failures of \textbf{DuT}, i.e., there is a possibility that some amount of correlation could exist between the errors of annotation process and the errors of \textbf{DuT}. Therefore, we frame this using the following
\begin{equation}\label{eq:correlation_correction_term}
    \delta p(e|\mathcal S) = p(e|\neg\mathcal C, \mathcal S)-p(e|\mathcal C, \mathcal S)
\end{equation}
By considering earlier equations and their complementary forms for $\neg\mathcal S$ and reducing the equation set, we obtain

\[
A = 
\begin{pmatrix}
  p_\mathcal C & 1 - p_\mathcal C \\
  1 - p_{\neg\mathcal C} & p_{\neg\mathcal C}
\end{pmatrix},
\quad
B = 
\begin{pmatrix}
  p(e|\mathcal C) + (p_\mathcal C)\, \delta p(e|\neg\mathcal S) \\[6pt]
  p(e|\neg\mathcal C) + (p_{\neg\mathcal C})\, \delta p(e|\mathcal S)
\end{pmatrix},
\]

\[
A
\begin{pmatrix}
  p(e|\mathcal C, \mathcal S) \\[4pt]
  p(e|\neg\mathcal C, \neg\mathcal S)
\end{pmatrix}
=
B
\]

Here, the $det(A)$ is given by $p_c + p_{-c} - 1$ and inverse of $A$ is given by

\begin{equation}\label{eq:matrix_corr_equation}
A^{-1} 
= 
\frac{1}{p_c + p_{-c} - 1}
\begin{pmatrix}
  p_{-c} & -\!\bigl(1-p_c\bigr) \\[4pt]
  -\!\bigl(1-p_{-c}\bigr) & p_c
\end{pmatrix}.
\end{equation}

Solving for the intermediate value of $p(e|\neg\mathcal C,\mathcal S)$ and plugging this in~\cref{eq:prec_and_recall} along with~\cref{eq:correlation_correction_term}, we obtain the final equation~\cref{eq:correction_equation}:

\begin{equation}\label{eq:correction_equation_appendix}
    p(e|\mathcal S) = \underbrace{\frac{p(e|\mathcal C)\,p_{\neg\mathcal C}+p(e|\neg\mathcal C)\,(p_\mathcal C-1)}{p_\mathcal C + p_{\neg\mathcal C}-1}}_\text{independence assumption}
    +\underbrace{\delta p(e|\mathcal S) \overbrace{\left(\frac{p_\mathcal C p_{\neg\mathcal C}}{p_\mathcal C + p_{\neg\mathcal C}-1}-r_\mathcal C\right)}^{\kappa_\mathcal S} 
    +\delta p(e|\neg\mathcal S)\overbrace{\frac{(p_\mathcal C -1)p_{\neg\mathcal C}}{p_\mathcal C + p_{\neg\mathcal C}-1}}^{\kappa_{\neg\mathcal S}}}_\text{correction terms}\,.
\end{equation}

Regarding the denominator $p_\mathcal C + p_{\neg\mathcal C}-1$, it can be zero (or approximately zero) for some combinations of precision of the metadata annotation process. In these cases, no statement can be made on $\mathcal S$ as the performance of the annotation classification does not allow separation of $\mathcal S$ from the rest of the data and any observable error differences on $\mathcal C$ potentially stems only from the correction factors.
Besides this technical breakdown of the hypothesis, it should be pointed out that the scaling factors $\kappa_{\mathcal S,\neg\mathcal S}$ depend only on the performance of the annotation process and thus can be determined without knowing the correction factors $\delta p$ themselves. While the latter are challenging to determine in practice they are rarely non-zero, even in cases where the hypothesis holds, due to fluctuations (e.g.\ when errors are determined on finite sample sizes). Knowing the magnitude of $\kappa$ therefore allows us a degree of certainty on the statements of the hypothesis. 




\subsection{Quantitative Evaluation of Metadata Generation Process}
\label{appendix:quantitaive_metadata_evaluation}


Our metadata generation is a form of data labeling process. 
Within this work, we chose CLIP~\citep{radford2021learning} to generate the metadata but know that for certain attributes of the ODDs the performance might be far below human capabilities, compare, e.g.,~\citet{gannamaneni2023investigating}.
To estimate the performance of our metadata generation process without large-scale evaluation or manual labeling, we take a simplifying view.
For each slice $\mathcal{C}$ containing a semantic concept identified by CLIP, for instance, images containing gender ``female'', we randomly draw a few samples to create a smaller subset $\mathcal{R}$.
% we can obtain from our data, iConsider that when we randomly draw samples from a data slice $\mathcal{S}$ that contains a specific concept, for instance, images containing gender ``female'', 
Let $q$ denote the probability that images within $\mathcal{C}$ contain the correct semantic concept.
By manually evaluating the smaller sample of images $\mathcal{R}\subset\mathcal{C}$ (drawn with replacement), we can model the posterior distribution for $q$ using Bayes theorem, that is
\begin{equation}
    p(q|\mathcal{R})=\frac{p(q)p(\mathcal{R}|q)}{p(\mathcal{R})}\propto p(\mathcal{R}|q)\,.
    \label{eq:bayes}
\end{equation}
Therein, we assumed a flat prior, i.e. $p(q)=\text{const.}$.
The probability of the observed sample $\mathcal{R}$ is given by
\begin{equation}
    p(\mathcal{R}|q)=\binom{n}{l} q^l (1-q)^{n-l}\,,
    \label{eq:bernoulli}
\end{equation}
where $n=|\mathcal{R}|$ is the size of the observed sample taken from $\mathcal{S}$ and $l\leq n$ is the number of observed positive, i.e., correct instances.
The true value for $q$ for the entire slice would describe the precision of the labeling of the concept as it is the ratio of true instances to the overall number of samples. We can approximate it using the small set using
\begin{equation}
    p_\text{precision}(q|\mathcal{R}) = \frac{(n+1)!}{l!(n-l)!} q^l (1-q)^{n-l}\,,
    \label{eq:approx:precison}
\end{equation}
where the factorials serve as the normalisation.
Using~\cref{eq:approx:precison}, we can, therefore, determine both the expected value of $q$ as well as our uncertainty of its value, which we report in terms of the standard deviation $\sigma$.
As a side note, for values of $q$ near 0 or 1, the Binomial distribution is asymmetric and the standard deviation is not always a faithful measure of ``true'' deviation. However, we compared with a quantile based approach, taking the range from the $1/6^\text{th}$ to $5/6^\text{th}$ quantile, and found only minor discrepancies.


Besides estimating the precision, we are also interested in estimating the recall of the labeling process.
This latter quantity is harder to evaluate as it depends both on the number of true positives and false negatives.
Let $P$ and $N$ denote the total number of data points that are classified as containing, or respectively, as not containing, the semantic concept.
Then the probability over the total number of true positives is given by  $p_\text{precision}(q| \mathcal{R}_P)$, where $\mathcal{R}_{P}$ is a random sample taken from the set $\mathcal{C}_P$ of positively classified elements.
A similar statement holds for the number of false negatives, where a sample $\mathcal{R}_N$ from the non-detected set can be used.
However, in this case, we either have to count (for $l$) the number of prediction errors or use the inverse outcome $1-q$.
Given that both samples are free of intersection, that is $\mathcal{R}_P \cap \mathcal{R}_N = \emptyset$, we make the assumption that the obtained probabilities $q_P$ and $q_N$ are independent from one another.
In this case, we can formulate the recall as
\begin{equation}
\begin{split}
    p_\text{recall}(q|\mathcal{R}_P, \mathcal{R}_N) = & \int_0^1\!\mathrm{d}q_P\int_0^1\!\mathrm{d}q_N \\
    \times & \delta\left(q-\frac{P q_P}{P q_P + N (1-q_N)} \right) \\
    \times & p_\text{precision}(q_P|\mathcal{R}_P)p_\text{precision}(q_N|\mathcal{R}_N)\,,  
\end{split}
\label{eq:approx:recall}
\end{equation}
where we interpret $p_\text{precision}$ such that in both cases correct predictions are counted while $\delta$ denotes a Dirac-Delta Distribution.
We evaluate this function numerically and use the results of $p_\text{recall}$ in the same way as for the precision above regarding, e.g., the reported standard deviation.




\subsection{Precision Sampling at different levels}
\label{appendix:level1_precisions}

In~\cref{appendix:quantitaive_metadata_evaluation}, we provide the framework for how precision and recall can be estimated by sampling data in slices. 
In this section, we present the concrete steps taken at level 1 to operationalize it and also the steps taken to calculate precision and recall at higher levels. At level 1, in synthetic and celebA experiments, as GT labels are available in addition to classification function $\mathcal G$ labels, human evaluation of slices is not necessary. For each slice in the data, before running SliceLine, we sample with replacement (n=60), and using GT slice labels, calculate precision and recall based on~\cref{appendix:quantitaive_metadata_evaluation}. This gives us mean and standard deviations of precision and recall that can be used with~\cref{eq:correction_equation}. For AD datasets, as GT labels are not available, we performed human evaluation by first taking 60 samples for each level 1 slice. The results of this are shown in~\cref{table:estimation_metadata_quality}.



At level 2 and higher, human sampling of precisions gets very labor-intensive even if considering only 9 semantic dimensions with binary attributes. Therefore, we incorporate the parent-level precisions calculated earlier to estimate corrected errors by accounting for their contributions. From level 2 onward, we construct a composite inverse matrix,
\begin{equation}
    A^{-1}_{\mathcal{S}_1 \mathcal{S}_2}=A^{-1}_{\mathcal{S}_1} \otimes A^{-1}_{\mathcal{S}_2}\,,
    \label{eq:compositeAinv}
\end{equation}
which is a direct product of the inverse matrices given in \cref{{eq:matrix_corr_equation}} for the respective semantic dimensions $\mathcal S_1$ and $\mathcal S_2$. The direct product implies an element-wise multiplication of the differing elements of $A^{-1}_{\mathcal{S}_i}$ in all possible combinations. 
This approach can be understood by first considering that in the approximation the precision values in $A_{\mathcal{S}_1 \mathcal{S}_2}$ are given by products of the respective precisions for $\mathcal S_{1,2}$ or its negations $\neg\mathcal{S}_{1,2}$.
That is, for two $2\times 2$ matrices $A_{\mathcal{S}_i}$ the resulting $A_{\mathcal{S}_1 \mathcal{S}_2}$ will be $4\times 4$ dimensional.
Second, the inverse of this direct product matrix is given by the direct product of its constituent matrices, leading to~\cref{eq:compositeAinv}.
%similar to the one in~\cref{{eq:matrix_corr_equation}}, and apply it to the observed errors. This transformation yields the corrected errors for each combination. 

We can also extend the binary case of ~\cref{{eq:matrix_corr_equation}} to a multi-class setting by taking into account that the matrices $A$ are based on normalized confusion matrices. That is, row-wise the entries in $A$ give the rate or probability with which the classifier $\mathcal G$ will mistake a given element for an element of a foreign class. This notion easily generalizes to arbitrary classes by taking the full confusion matrix for all classes, thereby introducing all combinations besides ``False Positives'' or ``False Negatives'' from the binary case.
For $n$ classes this would result in a $n\times n$ matrix for $A$, the inverse of which can be used to obtain the hypothesis part of~\cref{eq:correction_equation_appendix}.


\section{Results}
\subsection{Further results: CelebA Evaluations}
\label{appendix:celeba_further_eval}
In the synthetic data experiment, we provide the spread of errors and the precision and recall of slice recovery in comparison to an Oracle for different values of $k$. With the GT metadata in the celebA dataset, we build a similar Oracle for comparison and provide similar error spread and precision and recall values of SWD-1,2,3 in~\cref{fig:celebA_furthereval}. Here, the plot depicting the spread of errors is restricted to level 1 errors for better visualization. However, the precision and recall plot is based on the full level 2 slices.% under consideration. 
\begin{figure}[htbp]
    \centering
    \begin{minipage}{\textwidth}
      \includegraphics[width=\linewidth]{images/spread_single_runs.png} % Replace with your actual image file
      % \caption{First Image}
    \end{minipage}%
    \hfill % adds horizontal space between the images
    \begin{minipage}{\textwidth}
      \includegraphics[width=\linewidth]{images/recall_results_single_run.png} % Replace with your actual image file
      % \caption{Second Image}
  \end{minipage}
    \caption{Similar to synthetic data experiment, we provide spread of error (top) and Precision and Recall at different levels of k for SWD-1,2,3 of~\cref{algo:combined_sliceline} in comparison to the Oracle (bottom). The \textbf{DuT} is a ViT-B-16 classification model trained on ImageNet21k and evaluated on celebA dataset. Note that here precision and recall are quality metrics of weak slice discovery and not of labeling quality.}
    \label{fig:celebA_furthereval}
\end{figure}


\subsection{Evaluation of Top-5 Weak Slices}
\label{appendix:qualitative_metadata_evaluation}

In this section, we provide both the quantitative and qualitative results of our experiments. For the celebA dataset, \cref{fig:appendix_celebA_clip,fig:domino_results,fig:spotlight_results,fig:svmfd_results} contain the identified top-5 weak slices in the experiments SWD-3, DOMINO, Spotlight, and SVM FD respectively. We provide 8 samples from each of the top-5 slices found by the methods and 8 samples from the remaining data, except SVM FD which only provides 1 weak slice. In addition, we provide four slice descriptions given by DOMINO for each slice and the single slice description of SVM FD. While the actionability of our proposed approach is inherent as the identified weak slices are based on semantic concepts from the ODD, the textual descriptions from DOMINO are comparatively less useful. Furthermore, by focusing only on the samples from DOMINO, it is still hard to identify which semantic concepts uniquely constitute a slice. For example, if we consider an image from the remaining data (rightmost column), it is not straightforward to say if this image does or does not belong to any of the weak slices. Although the fifth slice does appear to capture a coherent slice, images of sports persons, the observed error $p(e|\mathcal{C})$ is significantly lower than what is identified by our approach. 
It is important to note that both DOMINO and SliceLine judge performance in terms of class probabilities, not false negative counts. Therefore, weak slices can have slightly better performance in terms of $p(e|\mathcal{C})$ compared to overall data, as observed for slice 3 found by DOMINO.

In~\cref{fig:appendix_bdd100k,fig:appendix_cityscapes,fig:appendix_railsem}, pedestrian crop samples from the top-5 weak slices obtained using our method are provided for each autonomous driving experiment. The quantitative evaluation of the top-5 slices for the three experiments can be found in~\cref{tab:appendix:bdd100k,tab:appendix:cityscapes,tab:appendix:railsem}. 

% \begin{figure*}[htbp!]
%      \centering  
%      \begin{subfigure}{0.49\textwidth}
%          \centering
%          \includegraphics[width=\linewidth]{images/e1_s0.pdf}        
%      \end{subfigure}     
%      \hfill     
%      \begin{subfigure}{0.49\textwidth}
%          \centering
%          \includegraphics[width=\linewidth]{images/e1_s1.pdf}        
%      \end{subfigure}
%      \\
%      \begin{subfigure}{0.49\textwidth}
%          \centering
%          \includegraphics[width=\linewidth]{images/e1_s2.pdf}        
%      \end{subfigure}     
%      \hfill     
%      \begin{subfigure}{0.49\textwidth}
%          \centering
%          \includegraphics[width=\linewidth]{images/e1_s3.pdf}        
%      \end{subfigure}
%      \\
%      \begin{subfigure}{0.49\textwidth}
%          \centering
%          \includegraphics[width=\linewidth]{images/e1_s4.pdf}        
%      \end{subfigure}
%      \caption{Samples from top-5 weak slices of a ViT-B-16 classification model trained on ImageNet21k and evaluated on the full celebA dataset with metadata generated from CLIP (CLIP-M+SL). The statistics provide a quantitative evaluation of the entire slice. For qualitative evaluation, we provide some sample images from the slice. \sg{add level 2 and level 1 for sliceilne GT and ours if also GT captures smiling 0 as well, }} \sg{TODO tomorrow: 3 tables. run sliceline on GT and obs and sliceline star. get real errors and observed for all identified slices. }
%      \label{fig:appendix_celebA_clip}
% \end{figure*}


\begin{figure*}[htbp!]
     \centering  
     \begin{subfigure}{0.49\textwidth}
         \centering
         \includegraphics[width=\linewidth]{images/e2_s0.pdf}        
     \end{subfigure}     
     \hfill     
     \begin{subfigure}{0.49\textwidth}
         \centering
         \includegraphics[width=\linewidth]{images/e2_s1.pdf}        
     \end{subfigure}
     \\
     \begin{subfigure}{0.49\textwidth}
         \centering
         \includegraphics[width=\linewidth]{images/e2_s2.pdf}        
     \end{subfigure}     
     \hfill     
     \begin{subfigure}{0.49\textwidth}
         \centering
         \includegraphics[width=\linewidth]{images/e2_s3.pdf}        
     \end{subfigure}
     \\
     \begin{subfigure}{0.49\textwidth}
         \centering
         \includegraphics[width=\linewidth]{images/e2_s4.pdf}        
     \end{subfigure}     
     \caption{Samples from top-5 weak slices obtained using SWD-3 for the ViT-B-16 classification model trained on ImageNet21k and evaluated on the full celebA dataset with metadata generated from CLIP using step 3 in~\cref{algo:combined_sliceline}. The statistics provide a quantitative evaluation of the entire slice. For qualitative evaluation, we provide some sample images from the slice.}
     \label{fig:appendix_celebA_clip}
\end{figure*}

\begin{figure*}
\centering
     \includegraphics[width=0.95\textwidth]{images/celebA/domino_output_plot.pdf}     
     \caption{Samples from top-5 weak slices of a ViT-B-16 classification model trained on ImageNet21k and evaluated on the full celebA dataset (DOMINO). From the 8 samples in each slice, 4 are true positives (green outline) and 4 are false negatives (red outline).}
     \label{fig:domino_results}     
\end{figure*}

\begin{figure*}
\centering
     \includegraphics[width=0.95\textwidth]{images/celebA/spotlight_output_plot.pdf}     
     \caption{Samples from top-5 weak slices of a ViT-B-16 classification model trained on ImageNet21k and evaluated on the full celebA dataset (Spotlight). From the 8 samples in each slice, 4 are true positives (green outline) and 4 are false negatives (red outline). Spotlight does not provide automatic descriptions of the slices}
     \label{fig:spotlight_results}     
\end{figure*}

\begin{figure*}
\centering
     \includegraphics[width=0.95\textwidth]{images/celebA/svm_output_plot.pdf}     
     \caption{Samples from top-1 weak slices of a ViT-B-16 classification model trained on ImageNet21k and evaluated on the full celebA dataset (SVM-FD). From the 10 samples in each slice, 5 are true positives (green outline) and 5 are false negatives (red outline). Unlike other SDMs, SVM-FD only outputs one weak slice.}
     \label{fig:svmfd_results}     
\end{figure*}


\begin{figure*}[htbp!]
     \centering  
     \begin{subfigure}{0.49\textwidth}
         \centering
         \includegraphics[width=\linewidth]{images/e3_s0.pdf}        
     \end{subfigure}     
     \hfill     
     \begin{subfigure}{0.49\textwidth}
         \centering
         \includegraphics[width=\linewidth]{images/e3_s1.pdf}        
     \end{subfigure}
     \\
     \begin{subfigure}{0.49\textwidth}
         \centering
         \includegraphics[width=\linewidth]{images/e3_s2.pdf}        
     \end{subfigure}     
     \hfill     
     \begin{subfigure}{0.49\textwidth}
         \centering
         \includegraphics[width=\linewidth]{images/e3_s3.pdf}        
     \end{subfigure}
     \\
     \begin{subfigure}{0.49\textwidth}
         \centering
         \includegraphics[width=\linewidth]{images/e3_s4.pdf}        
     \end{subfigure}     
     \caption{Samples from top-5 weak slices obtained using SWD-3 for the Faster R-CNN object detector trained and evaluated on BDD100k dataset.}
     \label{fig:appendix_bdd100k}
\end{figure*}

\begin{table}[htbp!]
\setlength{\tabcolsep}{3pt}

    \centering
    \begin{tabular}{c|cccl}
    Slice No. & $\mathcal{|S|}$ & $p_\text{corr}(e|\mathcal{S})$ & \makecell{Avg. Perf. \\ Degra.} & Slice Description \\
    \hline
    $\mathcal{S}_1$   &  319	& 0.2206   & -0.1636 & \messagebubble{\makecell[l]{\textbf{blurry}: false \\
\textbf{occluded}: true}} \\
\hline
    $\mathcal{S}_2$   &  508	& 0.2099   & -0.1528 & \messagebubble{\makecell[l]{\textbf{blurry}: false \\
    \textbf{cloth.-color}: dark-color \\
}} \\
\hline

    $\mathcal{S}_3$   &  466	& 0.147	   & -0.0899 & \messagebubble{\makecell[l]{\textbf{blurry}: false \\
\textbf{age}: adult \\
}} \\
\hline

    $\mathcal{S}_4$   &  773	& 0.1263   & -0.0693 & \messagebubble{\makecell[l]{\textbf{blurry}: false}} \\
\hline

    $\mathcal{S}_5$   &  582	& 0.1263   & -0.0693 & \messagebubble{\makecell[l]{\textbf{blurry}: false \\
    \textbf{gender}: Male
}} \\

    \end{tabular}
    \caption{Quantitative analysis of the top-5 weak slices obtained using SWD-3 for the Faster R-CNN object detector trained and evaluated on BDD100k dataset.}
    \label{tab:appendix:bdd100k}
\end{table}

\begin{figure*}[htbp!]
     \centering  
     \begin{subfigure}{0.49\textwidth}
         \centering
         \includegraphics[width=\linewidth]{images/e4_s0.pdf}        
     \end{subfigure}     
     \begin{subfigure}{0.49\textwidth}
         \centering
         \includegraphics[width=\linewidth]{images/e4_s1.pdf}        
     \end{subfigure}
     \\
     \begin{subfigure}{0.49\textwidth}
         \centering
         \includegraphics[width=\linewidth]{images/e4_s2.pdf}        
     \end{subfigure}     
     \begin{subfigure}{0.49\textwidth}
         \centering
         \includegraphics[width=\linewidth]{images/e4_s3.pdf}        
     \end{subfigure}
     \\
     \begin{subfigure}{0.49\textwidth}
         \centering
         \includegraphics[width=\linewidth]{images/e4_s4.pdf}        
     \end{subfigure}         
    
     
     \caption{Samples from top-5 weak slices obtained using SWD-3 for the SETR semantic segmentation model trained and evaluated on Cityscapes dataset.}
     \label{fig:appendix_cityscapes}
\end{figure*}

\begin{table}[htbp!]
\setlength{\tabcolsep}{3pt}

    \centering
    \begin{tabular}{c|cccl}
    Slice No. & $\mathcal{|S|}$ & $p_\text{corr}(e|\mathcal{S})$ & \makecell{Avg. Perf. \\ Degra.} & Slice Description \\
        \hline

    $\mathcal{S}_1$     & 690 & 0.1046 & -0.0897 & \messagebubble{\makecell[l]{\textbf{age}: adult \\
        \textbf{skin-color}: dark}} \\       
    \hline

    $\mathcal{S}_2$     & 591 & 0.0921 & -0.0773 & \messagebubble{\makecell[l]
    {
        \textbf{skin-color}: dark \\
        \textbf{cloth.-color}: dark-color}} \\
    \hline

    $\mathcal{S}_3$     & 349 & 0.0896 & -0.0748 & \messagebubble{\makecell[l]{\textbf{gender}: female \\
        \textbf{skin-color}: dark \\
        }}\\
    \hline

    $\mathcal{S}_4$     & 766 & 0.0778 & -0.0630 & \messagebubble{\makecell[l]
    {\textbf{skin-color}: dark \\
       \textbf{blurry}: false}}\\
    \hline

    $\mathcal{S}_5$     & 997 & 0.0594 & -0.0446 & \messagebubble{\makecell[l]{\textbf{skin-color}: dark \\
        }} \\
    \end{tabular}
    \caption{Quantitative analysis of the top-5 weak slices obtained using SWD-3 for the SETR semantic segmentation model trained and evaluated on Cityscapes dataset.}
    \label{tab:appendix:cityscapes}
\end{table}


\begin{figure*}[htbp!]

     \centering  
     \begin{subfigure}{0.49\textwidth}
         \centering
         \includegraphics[width=\linewidth]{images/e5_s0.pdf}        
     \end{subfigure}     
     \hfill     
     \begin{subfigure}{0.49\textwidth}
         \centering
         \includegraphics[width=\linewidth]{images/e5_s1.pdf}        
     \end{subfigure}
     \\
     \begin{subfigure}{0.49\textwidth}
         \centering
         \includegraphics[width=\linewidth]{images/e5_s2.pdf}        
     \end{subfigure}     
     \hfill     
     \begin{subfigure}{0.49\textwidth}
         \centering
         \includegraphics[width=\linewidth]{images/e5_s3.pdf}        
     \end{subfigure}
     \\
     \begin{subfigure}{0.49\textwidth}
         \centering
         \includegraphics[width=\linewidth]{images/e5_s4.pdf}        
     \end{subfigure}         
     
     \caption{Samples from top-5 weak slices obtained using SWD-3 for the Panoptic-FCN model trained and evaluated on RailSem19 dataset.}
     \label{fig:appendix_railsem}
\end{figure*}


\begin{table}[htbp!]
\setlength{\tabcolsep}{3pt}

    \centering
    \begin{tabular}{c|cccl}
    Slice No. & $\mathcal{|S|}$ & $p_\text{corr}(e|\mathcal{S})$ & \makecell{Avg. Perf. \\ Degra.} & Slice Description \\
        \hline

     $\mathcal{S}_1$     & 541 & 0.8663 & -0.222 & \messagebubble{\makecell[l]{\textbf{age}: young}} \\      
    \hline

    $\mathcal{S}_2$     & 510 & 0.8723 & -0.228 & \messagebubble{\makecell[l]{\textbf{age}: young \\
        \textbf{construction-worker}: false}} \\       
    \hline

    $\mathcal{S}_3$     & 405 & 0.8819 & -0.2376 & \messagebubble{\makecell[l]{\textbf{skin-color}: dark \\
        \textbf{cloth.-color}: dark-color}} \\       
    \hline

    $\mathcal{S}_4$     & 349 & 0.9095 & -0.2652 & \messagebubble{\makecell[l]{\textbf{age}: young \\
        \textbf{blurry}: false}} \\       
    \hline

    $\mathcal{S}_5$     & 173 & 1.00 & -0.4602 & \messagebubble{\makecell[l]{\textbf{age}: young \\
        \textbf{skin-color}: dark}} \\       
    \hline

   
    \end{tabular}
    \caption{Quantitative analysis of the top-5 weak slices obtained using SWD-3 for the Panoptic-FCN model trained and evaluated on RailSem19 dataset.}
    \label{tab:appendix:railsem}
\end{table}



