\section{Proof of Concept with Synthetic Data}
\label{sec:synthetic_data}



To demonstrate the efficacy of our algorithm~\cref{fig:workflow}, we first present evaluations on a synthetic dataset. This is done to evaluate the impact of noise on the labeling process and to determine the degree to which our algorithm can compensate for it.  
The synthetic data is a tabular dataset containing columns for nine ``real'' semantic dimensions for $200\,000$ samples each containing binary attributes. For each of the ``real'' dimensions (GT), a ``predicted'' metadata column is included as a proxy for the metadata that would be generated by CLIP in our algorithm (see~\cref{fig:workflow}). In addition, one final column contains the binarized \textbf{DuT} errors ($e$). The first four dimensions are generated to be imbalanced with only $\sim5\%$ of the samples belonging to the attribute ``1''. The other five dimensions are generated such that both attributes have equal distribution. The error column is designed such that weak slices are induced for the specified ground-truth attributes.

We consider three regimes of noise, i.e., different quality of labeling of the simulated annotation process:
%100 runs for each of three variants of the synthetic data are generated in:
(i) a regime of ``good'' quality CLIP labeling, represented with $p_\mathcal C$ above $80\%$, (ii) a regime of ``medium'' quality CLIP labeling, represented with $p_\mathcal C$ between $40\%$ and $70\%$, and (iii) a regime of ``bad'' quality CLIP labeling, represented with $p_\mathcal C$ between $10\%$ and $40\%$.
For all three regimes, we considered 100 runs of the experiments to account for statistical influence. Further details about the dataset generation can be found in~\cref{appendix:synthetic_data}.
%(see \sg{appendix TODO} for further details about the dataset generation). 
In~\cref{fig:synthetic_data}, in the top row, the error distributions show how labeling quality impacts the spread of error between attributes for each semantic dimension, i.e.,\ the upper and lower ends of the bars are given by the error rates for $\restr{\bar{e}}{\mathcal S}$, $\restr{\bar{e}}{\neg\mathcal S}$ and similarly using $\mathcal C$ or the corrected errors. In the good labeling quality regime, as expected, observed errors and corrected errors both display the same spread as the GT error. But when labeling quality is medium or bad (where the impact of ~\cref{eq:correction_equation} is stronger), the spread of the observed error is significantly lower than that of the corrected error. In contrast, the corrected error either has close estimates to the true error or overestimates the true error (GT). From a safety perspective, we argue that overestimating the error within a DNN is better than underestimating it. In the bottom row, we evaluate the results of SWD-1,2,3. This is shown by comparing how well SWD-1,2,3 recover the top-$k$ weak slices in comparison to top-$k$ slices from Oracle, i.e., a situation where we have access to perfect ``GT'' labeling quality annotation.
Precision and recall are calculated for the three data quality regimes w.r.t.\ the Oracle case by considering the overlap of identified weak slices at an increasing number of top-slices $k$. Note that precision and recall in this figure refer to quality metrics on weak slice discovery and not precision and recall of the CLIP labeling.
While, at level 2, the maximum number of slices $k$ is 162 for 9 binarized dimensions\footnote{$9 \times 2 + \binom{9}{2}\times{2^2}$}, we consider only slices fulfilling the cut-off requirement as a weak slice. Of these 162 slices, only $\sim 30$ are identified as weak slices. Although under good labeling quality, the slices identified by SWD-1,2,3 basically have 100\% overlap with the slices from the Oracle, under medium and strong label noise, SWD-3 shows significantly more recall than SWD-1 and marginally over SWD-2. However, this comes with a small loss in precision. 
In cases of strong noise, SWD-1 only recovers a few slices where the error signal is dominant, which explains the high precision at the cost of low recall. SWD-3, on the other hand, has a reduced precision, but recovers most of the weak slices identified by Oracle. For the rest of this work, we focus primarily on the slices identified by SWD-3.









\begin{figure}[htbp]
    \centering
    % Row 1
    \begin{tikzpicture}
        Define spacing
        \def\figwidth{5cm}
        \def\figheight{3cm}
        \def\hspace{1cm}
        \def\vspace{-3.5cm}

        % Row 1
        \node[inner sep=0pt, outer sep=0pt] (fig1) at (0, 0.0)
            {\includegraphics[width=0.30\textwidth, height=2.5cm]{images/spread_multiple_runs_good.png}};
        \node[above=0.12cm of fig1] {Good Quality};

        \node[inner sep=0pt, outer sep=0pt] (fig2) at (\figwidth , -0.02)
            {\includegraphics[width=0.30\textwidth, height=2.42cm]{images/spread_multiple_runs_medium.png}};
        \node[below=0.2cm of fig2] {Dimensions};
        \node[above=0.2cm of fig2] {Medium Quality};

        \node[inner sep=0pt, outer sep=0pt] (fig3) at (2 * \figwidth, -0.02)
            {\includegraphics[width=0.30\textwidth, height=2.42cm]{images/spread_multiple_runs_bad.png}};
        \node[above=0.2cm of fig3] {Bad Quality};

        % % Row 2
        \node[inner sep=0pt] (fig4) at (0, \vspace)
            {\includegraphics[width=0.31\textwidth]{images/recall_plots_precisions_good.png}};

        \node[inner sep=0pt] (fig5) at (\figwidth, \vspace)
            {\includegraphics[width=0.31\textwidth]{images/recall_plots_precisions_medium.png}};
        \node[below=0.2cm of fig5] {Number of slices under consideration (k)};

        \node[inner sep=0pt] (fig6) at (2 * \figwidth, \vspace)
            {\includegraphics[width=0.31\textwidth]{images/recall_plots_precisions_bad.png}};

        % Adding axis labels manually if needed
        \node[rotate=90] at (-2.8cm, 0.1cm) {Errors};
        \node[rotate=90] at (-3cm, -3.4cm) {\parbox{2.3cm}{\centering Precision\\Recall}};

        % \node at (5cm, -6cm) {X-axis Label};
    \end{tikzpicture}
    \caption{Based on labeling quality, we divide the generated datasets into (i) good quality (left), (ii) medium quality (middle), and (iii) bad quality (right). In three cases, we look at the spread of error in GT ($p(e|\mathcal{S})$), Observed ($p(e|\mathcal{C})$), and Corrected ($p(e|\mathcal{S})$). In the second row, corresponding performance in terms of precision and recall of SWD-1,2,3 are shown. Precision and Recall in this figure are metrics to evaluate weak slice recovery and are not related to labeling quality. The legend for both rows are presented on the figures on left.}
    \label{fig:synthetic_data}
\end{figure}

