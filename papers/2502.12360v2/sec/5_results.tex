\section{Evaluations of real-world DNNs}
\label{sec:results}

In this section, we first present our experimental setup. We then show the evaluation of our systematic weaknesses detection method on a publicly available pre-trained model for the CelebA dataset. 
Here, the dataset's rich metadata annotation allows us to investigate the influence of noisy metadata annotation. In addition, we
compare against SOTA SDM methods to evaluate our claim that adherence to the ODD descriptions is useful to end users (e.g., safety experts, ML developers). Subsequently, we present the insights gained by using our approach on DNNs trained on autonomous driving datasets.



\subsection{Experimental Setup}
\textbf{Datasets and Models:} Four pre-trained models, ViT-B-16~\citep{dosovitskiy2020image}\footnote{https://github.com/huggingface/pytorch-image-models}, Faster R-CNN~\citep{ren2015faster}\footnote{https://github.com/SysCV/bdd100k-models/tree/main/det}, SETR PUP~\citep{zheng2021rethinking}\footnote{https://github.com/open-mmlab/mmsegmentation}, PanopticFCN~\citep{li2021fully} are evaluated using four public datasets (CelebA~\citep{liu2015faceattributes}, BDD100k~\citep{yu2020bdd100k}, Cityscapes~\citep{cordts2016cityscapes}, and RailSem19~\citep{Zendel_2019_CVPR_Workshops}), respectively. 
% For SliceLine, the original python implementation is utilized.\footnote{https://github.com/DataDome/sliceline} 
We restrict the number of combinations (level) to 2 in this work. However, as presented in~\cref{appendix:level1_precisions}, our approach allows correction of errors even at higher levels of combinations. We used the cutoff for the slice error as $1.5 \, \restr{\bar{e}}{\mathcal D}$ for all experiments except the PanopticFCN model evaluation. In the PanopticFCN evaluation, we utilize the cut-off point for the slice error as $1.0 \, \restr{\bar{e}}{\mathcal D}$ as the global average error is already quite high. For a detailed experimental setup, see~\cref{appendix:experiment_setup}. To foster reproducibility, code and the prompts used for metadata generation with CLIP will be provided. 


\subsection{Evaluation of our Systematic Weaknesses Detection Method}
\label{sec:results:celebA}
\textbf{Evaluating a ViT Model on CelebA:}
As our first experiment, we evaluated the weaknesses of the ViT-B-16~\citep{dosovitskiy2020image} model (\textbf{DuT}) trained on ImageNet21k~\citep{ridnik2021imagenet}. We use the model for the targeted task of identifying the class ``person'' in the CelebA dataset~\citep{liu2015faceattributes} as a real-world proof of concept for our approach. 
Due to the extensive range of label categories in ImageNet~\citep{deng2009imagenet} and the significant noise in the labeling style, models trained on the full ImageNet dataset or its standard subset ImageNet1k~\citep{russakovsky2015imagenet} can suffer from systematic weaknesses.
For example, although the primary foreground object in an image might be a human, in some instances the image can be labeled as belonging to the class ``person'' while in other similar instances the label might be about more granular classes like ``bride'' or ``guitarist''.
To fix this issue,~\citep{ridnik2021imagenet} proposed 11 hierarchies based on WordNet~\citep{miller1995wordnet} semantic trees such that classes at higher hierarchy levels are superclasses that subsume classes at lower hierarchy levels.
However, despite these efforts, considerable label noise in terms of class overlap still persists. For example, humans holding specific objects might occur at the same hierarchy level as the class ``artifact'' or ``person''. Similar problems exist, for example, for hairstyles (see ``pompadour'' existing at the same level as ``person''). 
For a further analysis, also see the work of \citep{northcutt2021confident}.


Earlier works~\citep{beyer2020we, shankar2020evaluating} have proposed using multi-label evaluation metrics as a way to deal with label noise. However, we consider the simplified task of identifying a dedicated class, ``Person'', in a dataset with only human faces (celebA) by focusing on the top-1 class predictions for level-0 of the label hierarchy proposed in ImageNet21k.
We obtain an accuracy of $94.44\%$ on the $202\,599$ images in the CelebA dataset. The softmax of the top-1 prediction, see~\cref{fig:celebA}, shows, besides the ``person'' class, the presence of several other classes, most prominently ``artifact'' and ``pompadour''.
%The softmax values of the classes at the top level hierarchy, level-0, as proposed in ImageNet21k for the DNN-under-test are used, and we obtain an accuracy of $94.44\%$ out of $202\,599$ images in the CelebA dataset with the top-1 predictions distributed as shown in~\cref{fig:celebA}. 
As this model is commonly used as a pre-trained backbone for various applications, uncovering potential shortcomings might also be beneficial for potential downstream use cases of various types.
Furthermore, the CelebA dataset serves as an ideal testing ground for approaches identifying systematic weaknesses due to the availability of the ground-truth metadata attributes.
As an ODD for this test case, we propose a simplified subset of these available metadata attributes in analogy to the work of~\citet{gannamaneni2023investigating}, for details see~\cref{appendix:odds_used}.
As proposed in our algorithm, we generate metadata using CLIP for the given ODD dimensions.
Subsequently, the generated metadata is combined with the errors of the \textbf{DuT}. 

\begin{figure}
\begin{minipage}{0.45\textwidth}
  \includegraphics[width=\linewidth]{images/celebA_top_10_preds.pdf} % Replace with your actual image file
  % \caption{First Image}
\end{minipage}%
\hfill % adds horizontal space between the images
\begin{minipage}{0.45\textwidth}
  \includegraphics[width=\linewidth]{images/e2_s0.pdf} % Replace with your actual image file
  % \caption{Second Image}
  
\end{minipage}
\caption{Left: The hierarchy level-0~\citep{ridnik2021imagenet} predictions of the pre-trained ViT-B-16 model on the full CelebA dataset converted into a binary classification problem. While a majority of the predictions are correct, there is a non-trivial subset of images with systematic errors due to label overlap issues. Right: Top-1 weak slice, identified by SWD-3, of a ViT-B-16 classification model trained on ImageNet21k and evaluated on the full celebA dataset. The statistics provide a quantitative evaluation of the entire slice. For qualitative evaluation, we provide some sample images from the slice.}
\label{fig:celebA}
\end{figure}




\textbf{Weak Slice Discovery}
Since the CelebA dataset contains annotated metadata for 40 attributes, we have access to noiseless metadata which, when used with SliceLine, can be considered as the ``Oracle'' approach. In~\cref{tab:first_celebA_eval}, we present the quantitative comparison of the top-7 slices identified by SWD-3 against corresponding slices in SWD-1 and Oracle. Basically, we list the top-7 slices of SWD-3 and evaluate where these slices would be ranked by SWD-1 and Oracle and what the corresponding statistics would be to highlight the importance of error correction. 
% We apply SliceLine on this combined structured data, and the resulting top-1 weak slice is shown in. 
From the slice descriptions, all the identified weak slices contain some variation of the semantic concept ``wearing hat''. 
The discovery of these slices can be seen within the context of the frequent misclassification of images as class ``artifact'' by the \textbf{DuT} (see~\cref{fig:celebA}). 
In these cases, the model likely focuses on the hats as the foreground object and predicts the class ``artifact''.
To evaluate the quality of the identified slices, we utilize the errors of the slices, i.e.,\ $p_{corr}(e|\mathcal{S})$, $p(e|\mathcal{C})$, and $p(e|\mathcal{S})$ defined in~\cref{sec:method}.
We observe, based on the rank column, that the top-6 Oracle slices are captured in top-7 SWD-3 slices. Notably, while the observed error $p(e|\mathcal{C})$ of SWD-1 underestimates the true error $p(e|\mathcal{S})$ of Oracle, SWD-3 effectively corrects this in $p_{corr}(e|\mathcal{S})$. For instance, in the third row, which corresponds to the top-ranked weak slice identified by the Oracle, the difference between the Oracle slice error and the SWD-1 slice error is 0.3, while between SWD-3 and Oracle it is only 0.07. A thorough evaluation of our approach on the top-60 slices shown in~\cref{fig:celebA_furthereval} in~\cref{appendix:celeba_further_eval} reveals that SWD-3 obtains $100\%$ recall of weak slices at the cost of a reduction in precision. Note that precision and recall in this figure refer to quality metrics on weak slice discovery and not precision and recall of the CLIP labeling. From a safety perspective, given the noisy labeling, high recall (detection of all weak slices) at the cost of some reduction in precision can be considered acceptable. Interestingly, the top-1 slice of SWD-1 (not shown in table) refers to slice description ``wearing hat: true'' and ``pale-skin: true''.~\citet{gannamaneni2023investigating} discussed the limitations of CLIP in separating the latter dimension and corresponding low performance. This high level of noise in the generated metadata leads to SWD-1 identifying ``pale-skin'' as a top-1 slice while SWD-3 effectively corrects for this by discarding the wrongly detected slice as ``invalid'' using the quality indicators (see~\cref{algo:combined_sliceline}) and hence does not identify this dimension in top-7. For a qualitative evaluation of SWD-3, the top-1 slice with sample images from the slice are available in~\cref{fig:celebA} (see~\cref{fig:appendix_celebA_clip} in~\cref{appendix:qualitative_metadata_evaluation} for a qualitative evaluation of the top-5 slices).

\begin{table}[h]
    \centering
    \renewcommand{\arraystretch}{0.9} % Reduce row height
    \setlength{\tabcolsep}{3pt} % Reduce column spacing
    \begin{tabular}{c@{\hskip 4pt}|c@{\hskip 4pt}|ccc@{\hskip 4pt}|ccc@{\hskip 4pt}|ccc}
        Slice & Slice Description & \multicolumn{3}{c|}{SWD-3} & \multicolumn{3}{c|}{SWD-1} & \multicolumn{3}{c}{Oracle} \\
        \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} & $\operatorname{rank}(S)$ & $|\mathcal{S}|_\text{corr}$ & $p_\text{corr}(e|\mathcal{S})$ &  $\operatorname{rank}(S)$ & $|\mathcal{S}|$ & $p(e|\mathcal{C})$ & $\operatorname{rank}(S)$ & $|\mathcal{S}|$ & $p(e|\mathcal{S})$  \\
        \hline
        $\mathcal{S}_1$ & \messagebubble{\makecell[l]{\textbf{Wearing-Hat}: True \\ \textbf{Beard}: False}} & 1 & 7600 & 0.69 & 6 & 12152 & 0.33 & 2 & 6267 & 0.51 \\
        \hline
        $\mathcal{S}_2$ & \messagebubble{\makecell[l]{\textbf{Wearing-Hat}: True \\ \textbf{Smiling}: False}} & 2 & 5132 & 0.60 & 3 & 8573 & 0.36 & 9 & 6476 & 0.45 \\
        \hline
        $\mathcal{S}_3$ & \messagebubble{\makecell[l]{\textbf{Wearing-Hat}: True \\ \textbf{Gender}: Female}} & 3 & 4435 & 0.61 & 2 & 7393 & 0.38 & 1 & 2947 & 0.69 \\
        \hline
        $\mathcal{S}_4$ & \messagebubble{\makecell[l]{\textbf{Wearing-Hat}: True \\ \textbf{Age}: Young}} & 4 & 7974 & 0.54 & 4 & 12758 & 0.34 & 3 & 6937 & 0.50 \\
        \hline
        $\mathcal{S}_5$ & \messagebubble{\makecell[l]{\textbf{Wearing-Hat}: True \\ \textbf{Eyeglasses}: False}} & 5 & 8606 & 0.54 & 5 & 12594 & 0.33 & 6 & 8417 & 0.45 \\
        \hline
        $\mathcal{S}_6$ & \messagebubble{\makecell[l]{\textbf{Wearing-Hat}: True \\ \textbf{Goatee}: False}} & 6 & 8845 & 0.53 & 7 & 11453 & 0.33 & 4 & 8284 & 0.46 \\
        \hline
        $\mathcal{S}_7$ & \messagebubble{\makecell[l]{\textbf{Wearing-Hat}: True \\ \textbf{Bald}: False}} & 7 & 9676 & 0.51 & 8 & 15501 & 0.32 & 5 & 9795 & 0.44 \\
    \end{tabular}
    \caption{Evaluation of top-7 slices of SWD-3 (see~\cref{algo:combined_sliceline}) by comparing its statistics with corresponding slice statistics of SWD-1 and Oracle. The rank column indicates the slice ranking in each approach. }
    \label{tab:first_celebA_eval}
\end{table}





\textbf{Comparison to SOTA SDM method}
In addition to the evaluation of SWD-3, we compare three SOTA methods DOMINO~\citep{eyuboglu2022domino}, Spotlight~\citep{d2022spotlight}, and SVM FD~\citep{jain2023distilling} against Oracle.
Similarly to our work, DOMINO and SVM FD use CLIP (ViT L/14) in their workflows. However, they encode the images in the CLIP embedding space and then search for weak slices without explicitly enforcing any semantic concepts. To describe the slices, both approaches perform an additional step, where the identified slices are explained using text from large language models. In contrast, Spotlight directly uses the embedding space of the \textbf{DuT} to cluster weak slices and provides no descriptions of the identified slices. The former methods follow a broader trend (like us) of using foundational models like CLIP in testing smaller models. However, they do not thoroughly address limitations in CLIP's capabilities and the limitations of their approaches w.r.t.\ actionability when the slice descriptions are not very meaningful. Our approach tackles both these limitations as we address noise in CLIP labeling and also correctness of descriptions. However, our approach also has a limitation as it can only identify weaknesses w.r.t.\ dimensions in $\mathcal Z$ while the other methods could identify more novel weaknesses. However, this advantage of SOTA methods, as will be shown below, can only be realized if the description or coherence of a slice is understandable and actionable to the end-user. To assess the actionability of the SOTA methods, we consider (i) slice descriptions based on the methods themselves, (ii) slice coherence based on human inspection, and (iii) slice coherence based on overlap with top-5 slices of Oracle.


Qualitative results and slice descriptions are provided for the three methods in~\cref{appendix:qualitative_metadata_evaluation}. We identified that DOMINO descriptions can be very generic and not helpful in identifying the unique attributes of a slice. This problem was also discussed in other works~\citep{jain2023distilling, gao2023adaptive}. For Spotlight, descriptions are not available as part of the method. In contrast, in SVM FD, the slice description is targeted and covers one dimension of the weak slice identified by Oracle, namely, ``wearing hat''. However, as shown earlier, the weaknesses identified from Oracle stem from the combination of semantics. Therefore, slice descriptions from the SOTA methods are not enough for actionability. 
Second, to further evaluate the coherence of the slices, we manually inspect a sample of the images from a slice to identify the semantics. Such an approach is necessary for all methods that do not provide slice descriptions. For such manual inspection to identify the coherence of the slice, we consider samples from the slice and samples from the remaining data (last column) as a form of control group. For top-1 slices of all three approaches, it is hard to determine what uniquely constitutes the top-1 slice when considering the combination of semantics. Furthermore, such an exercise is time intensive and might potentially uncover spurious patterns.

Finally, to evaluate the coherence of the slice based on overlap with Oracle slices, we present in~\cref{tab:celebA_results} the top-1 slice identified by each method, their corresponding statistics and the overlap (Jaccard Similarity Coefficient) of the top-1 slice with the top-5 Oracle slices. From the slice statistics, it can be observed that the methods recover slices with significant performance degradation and observed error $p(e|\mathcal{C})$. However, the overlap of the top-1 slices with top-5 of Oracle is quite low. This indicates that the methods might be uncovering weaknesses w.r.t.\ dimensions not present in the ODD. However, without useful descriptions, the actionability of these slices is low. Furthermore, we evaluate the overlap of the top-1 slice with a slice that is purely made up of FNs of the \textbf{DuT}. High values in this column might indicate that priority is given to identifying FNs rather than semantic coherence, as it is unlikely that all weaknesses of a DNN can be explained by one semantic concept. Therefore, grouping all FNs into one slice would be counterproductive. As DOMINO captures $64$\% of all false negatives in its top-1 slice, it is unlikely that such a slice is actionable. In contrast, Spotlight and SVM FD capture fewer FNs in the top-1 slice. Therefore, they might be capturing some form of combination of semantics. Based on these evaluations, we conclude that the SOTA methods, when integrated with improved slice description techniques, could complement our approach. However, in their current form, our approach offers greater actionability due to its inherent slice descriptions. 







\begin{table}
    \centering
    \setlength{\tabcolsep}{3pt}
    \begin{tabular}{@{}c|ccc|ccccc|c@{}}
        Method  & \multicolumn{3}{c|}{Slice Statistics} & \multicolumn{5}{c|}{Slice Coherence with Attributes} \\
        \hline 
        \multicolumn{1}{c|}{} & \makecell{Perf. \\ degr.} & Size & \multicolumn{1}{c|}{} & \multicolumn{5}{c|}{\makecell{Overlap with \\ Oracle top-5 \\ slices}} & \makecell{Overlap \\ with \\ FNs}  \\

        
        \multicolumn{1}{c|}{} & \makecell{$\restr{\bar{e}}{\mathcal{D}}$ \\ - \\ $p(e|\mathcal{C})$} & $|\mathcal{S}_1|$ & $p(e|\mathcal{C})$ & $J(\mathcal{S}_1, \mathcal{S}^{O}_{1})$ & $J(\mathcal{S}_1,\mathcal{S}^{O}_{2})$ & $J(\mathcal{S}_1,\mathcal{S}^{O}_{3})$ & $J(\mathcal{S}_1,\mathcal{S}^{O}_{4})$ & $J(\mathcal{S}_1,\mathcal{S}^{O}_{5})$ & $\frac{|\mathcal{S}_1 \cap \mathcal{S}_{FN}|}{|\mathcal{S}_{FN}|}$ \\
        
        % \makecell{Oracle \\ (SL+GT)} & -0.6316 & 2947  &  0.6868 &  1 & 1 & 1 & 1 & 1 & 0.18   \\
        % \makecell{Ours \\ (SWD-1)} & -0.3115 & 9616 & 0.3668 & 1704 & 0.32 &  \makecell{Available \\ Good} & \textbf{Yes}   \\
        % \hline      
        % \makecell{Ours \\ (SWD-2)} & -0.2098 & 4116 & 0.2651 & 507 & 0.10 &  \makecell{Available \\ Good} & \textbf{Yes}   \\
        % \hline   
        % \makecell{Ours \\ (SWD-3)} & -0.3115 & 9616 & 0.3668 & 1704 & 0.32 &  \makecell{Available \\ Good} & \textbf{Yes}   \\
        \hline   
        \makecell{DOMINO} & -0.5629 & 11726 & 0.6181 & 0.13  & 0.19 & 0.20 & 0.21 & 0.24 & 0.64   \\
        \hline        
        \makecell{Spotlight} & -0.8622  & 4050 & 0.9179 & 0.32 & 0.32 & 0.32 & 0.31 & 0.31 & 0.33   
        \\
        \hline        
        \makecell{SVM FD} & -0.3844 & 2642 & 0.4295 & 0.11 & 0.15 & 0.16 & 0.16 & 0.16 & 0.24    \\

    \end{tabular}
    \caption{Comparison of three metadata-free SOTA methods with top-5 slices of Oracle. $J(\mathcal S_1, \mathcal S^O_x)$ indicates the Jaccard similarity coefficient between the two slices.~\cref{appendix:qualitative_metadata_evaluation} contains samples from each slice of the SOTA methods along with slice descriptions and statistics. For overlap with the oracle slice, higher values are better. For the overlap with the FNs, low values indicate that a slice does not contain ``significant'' weaknesses or is highly specific, while high values indicate that potentially all weaknesses of the \textbf{DuT} are in one slice and it might, therefore, be too generic. This implies that in general one would expect or desire medium overlap ranges.}
    \label{tab:celebA_results}
\end{table}





\subsection{Insights on SOTA Pedestrian Detection Models}
\label{sec:results:ad_results}
Having shown the benefits of our proposed method, we evaluate a more safety-relevant task of pedestrian detection using models trained in real-world autonomous driving (AD) datasets to identify their systematic weaknesses when predicting the class ``pedestrian''. For this, we require pedestrian level performances (intersection-over-union (IoU)) and metadata.
To avoid noisy labeling in our metadata generation step, we perform some additional steps which were not required for the previous experiment. First, we cropped all pedestrians from the images and considered these crops as $\mathcal{D}$. This is done to focus the CLIP model only on pedestrians during metadata generation.\footnote{To avoid that the aspect ratio of pedestrians is changed by the CLIP pre-processing, we use padding to obtain square crops.} 



Second, we calculate the pixel area of the pedestrians based on the ground truth bounding box area and use this to filter $\mathcal{D}$ by removing pedestrians that occupy small pixel areas (``smaller'' pedestrians).
Such filtering is necessary as: %\sg{tie with aleatoric from sec3}
(i) Due to low resolution and high pixelation of ``smaller'' sized pedestrians, i.e., there is a high aleatoric uncertainty regarding the correct labels affecting both CLIP and human labelers in understanding the image content (e.g., to determine gender, age, etc.).
(ii) ``Smaller'' pedestrians are more likely to be farther from the ego-vehicle~\footnote{Unless if small size is due to occlusion. For BDD100k dataset, where occlusion is available as annotation, we show impact of occlusion as well} and, therefore, might be considered less safety-relevant (in terms of vehicle breaking time).
(iii) As the small size can be strongly correlated to performance (due to distance~\citep{Gannamaneni_2021_ICCV, Lyssenko_2021_CVPR} or occlusion), this signal can strongly dominate the search for systematic weaknesses by SliceLine, thus not providing any novel insights in terms of systematic weaknesses. For this reason, we remove the low-resolution ``smaller'' pedestrians to improve the quality of metadata generation and gain further novel insights about model failures w.r.t.\ more safety-relevant pedestrians.  

The metadata generation using CLIP is performed using ODDs more suitable for automotive context (see~\cref{appendix:odds_used}).
We also perform a manual evaluation of a subset of images ($n=60$) for each attribute in each dataset to evaluate the quality of the generated metadata by estimating the precision $p_\mathcal C$ and recall $r_\mathcal C$ (as discussed in~\cref{sec:method}) and show the results in~\cref{table:estimation_metadata_quality}.

\begin{table*}[htbp!]
\centering
% \setlength{\tabcolsep}{4pt} % set column spacing
% \rotatebox{90}{
\begin{tabular}{@{}cc|ccc|ccc@{}}
  
  \multirow{2}{*}{\makecell{Sem. \\ dim.}} & \multirow{2}{*}{Attri.} &  \multicolumn{3}{c}{Estimated Precision $p_\mathcal C$} & \multicolumn{3}{c}{Estimated Recall $r_\mathcal C$} \\ 
 & & BDD100k & Cityscapes & RailSem19 & BDD100k & Cityscapes & RailSem19\\ 
 \hline\hline
 
 \multirow{2}{*}{Age} & Adult                    & $ 0.95 \pm 0.03 $& $ 0.99 \pm 0.02 $ & $ 0.97 \pm 0.02 $           & $ 0.76 \pm 0.03$ &$ 0.70 \pm 0.02$ & $ 0.55 \pm 0.02$\\
                      & Young                      & $ 0.69 \pm 0.06 $ & $ 0.56 \pm 0.06 $ & $ 0.42 \pm 0.06 $         & $ 0.93 \pm 0.06$ &$ 0.97 \pm 0.06$ & $ 0.94 \pm 0.06$\\
  \hline
 \multirow{2}{*}{Gender} & Female                 & $ 0.84 \pm 0.05 $ & $ 0.97 \pm 0.02 $ & $ 0.85 \pm 0.04 $         & $ 0.90 \pm 0.05$ &$ 0.95 \pm 0.02$ & $ 0.87 \pm 0.04$\\
                         & Male                 & $ 0.94 \pm 0.03 $ & $ 0.97 \pm 0.02 $  & $ 0.94 \pm 0.03 $          & $ 0.88 \pm 0.03$ & $ 0.97 \pm 0.02$ & $ 0.92 \pm 0.03$\\
 \hline
 \multirow{2}{*}{\makecell{Cloth.-\\color}} & \makecell{Bright-\\color}  & $ 0.81 \pm 0.05 $ & $ 0.85 \pm 0.04 $  & $ 0.79 \pm 0.05 $         & $ 0.30 \pm 0.05$ &$ 0.23 \pm 0.04$ & $ 0.66 \pm 0.05$\\
                         & \makecell{Dark-\\color}            & $ 0.76 \pm 0.05 $ & $ 0.65 \pm 0.06 $ & $ 0.82 \pm 0.05 $         & $ 0.96 \pm 0.05$&$ 0.97 \pm 0.06$ & $ 0.89 \pm 0.05$\\
 \hline
 \multirow{2}{*}{\makecell{Skin-\\color}} & Dark             & $ 0.82 \pm 0.05 $&  $ 0.55 \pm 0.06$ & $ 0.56 \pm 0.06 $           & $ 0.92 \pm 0.05$& $ 0.71 \pm 0.06$ & $ 0.76 \pm 0.06$\\
                         & White                  & $ 0.99 \pm 0.02 $&  $ 0.95 \pm 0.03$ & $ 0.89 \pm 0.04 $           & $ 0.96 \pm 0.02$ & $ 0.91 \pm 0.03$ & $ 0.75 \pm 0.04$\\
 \hline
 \multirow{2}{*}{Blurry}                         & True &  $ 0.71 \pm 0.06 $ &  $ 0.63 \pm 0.06$ & $ 0.87 \pm 0.04$  & $ 0.42 \pm 0.06$& $ 0.87 \pm 0.06$ & $ 0.64 \pm 0.04$\\
                                                 & False  &  $ 0.48 \pm 0.06 $ &  $ 0.95 \pm 0.03$ &$ 0.84 \pm 0.05$ & $ 0.74 \pm 0.06$ & $ 0.82 \pm 0.03$ & $ 0.95 \pm 0.05$\\
                                                  \hline
 \multirow{2}{*}{\makecell{Constru.- \\ Worker}}                         & False &  - &  - & $ 0.97 \pm 0.02$  &  -& - & $ 0.98 \pm 0.02$\\
                                                 & True  &  - & - &$ 0.65 \pm 0.06$ & - & - & $ 0.55 \pm 0.06$\\
\end{tabular}
% }
\caption{The estimated precision and recall using our proposed approach for evaluating the quality of the generated metadata. Here, we provide the mean and $\sigma/2$, for $n$ of 60, of the estimated precision and recall. Certain dimensions like occlusion are available as part of the datasets themselves. We do not perform human-evaluation for these dimensions but these are considered in the weak slice search.}
\label{table:estimation_metadata_quality}
\end{table*}

In these experiments, using SWD-3, we evaluate the weaknesses of an object detection model (Faster R-CNN), a segmentation model (SeTR PUP), and a panoptic segmentation model (Panoptic-FCN). The models are evaluated on their respective datasets, i.e., BDD100k, Cityscapes, and RailSem19. Samples of image crops of the identified top-1 weak slice for each experiment are shown in~\cref{fig:ad_results} (see~\cref{fig:appendix_bdd100k,fig:appendix_cityscapes,fig:appendix_railsem} in~\cref{appendix:qualitative_metadata_evaluation} for top-5 weak slices). In~\cref{tab:summary_ad}, we present the largest and worst performing slice of the top-5 to provide insights about the three models. In all three experiments, the performance degradation of the identified slices is significant. ``Occlusion'', skin-color and clothing-color are reoccurring slice descriptions for the first two models, which are tested on datasets that contain images with many nighttime scenes (BDD100k) or relatively high gray-toned scenes (Cityscapes). In contrast, the third model, which contains relatively brighter scenes, has a significant weakness for the dimension ``age''. The estimated precision $p_\mathcal C$ and recall $r_\mathcal C$ in~\cref{table:estimation_metadata_quality} were provided as input to~\cref{algo:combined_sliceline} to obtain these slices and to determine the quality of the identified weaknesses. Therefore, in contrast to SOTA SDMs, our approach identifies human-understandable safety-relevant systematic weaknesses in DNNs used for real-world applications. 


\begin{figure}[ht]
\begin{minipage}{0.33\textwidth}
  \includegraphics[width=\linewidth]{images/e3_s0.pdf} % Replace with your actual image file
  % \caption{First Image}
   \subcaption[]{Faster R-CNN}
\end{minipage}%
\begin{minipage}{0.33\textwidth}
  \includegraphics[width=\linewidth]{images/e4_s0.pdf} % Replace with your actual image file
   \subcaption[]{SeTR}
\end{minipage}
\begin{minipage}{0.33\textwidth}
  \includegraphics[width=\linewidth]{images/e5_s0.pdf} % Replace with your actual image file
   \subcaption[]{Panoptic-FCN}
\end{minipage}
  \caption{Left: Samples from top-1 weak slice of a Faster R-CNN object detector trained and evaluated on BDD100k dataset. Middle: Samples from top-1 weak slice of SeTR model trained and evaluated on Cityscapes dataset. Right: Samples from top-1 weak slice of a Panoptic-FCN model trained and evaluated on RailSem19 dataset.}
\label{fig:ad_results}

\end{figure}




\begin{table}[bp!]
\setlength{\tabcolsep}{7pt}
    \centering
    \begin{tabular}{c|ccc|ccc}
        \multirow{2}{*}{\makecell{Model \&\\ Dataset}} &  \multicolumn{3}{c}{\makecell{Largest Slice \\ (in top-5)}} & \multicolumn{3}{c}{\makecell{Worst Performing Slice \\ (in top-5)}} \\
        
        & $\frac{|\mathcal{S}|}{|\mathcal{D}|}$\% & $p_\text{corr}(e|\mathcal{S})$ & \makecell{Perf. \\ Degr.} & $\frac{|\mathcal{S}|}{|\mathcal{D}|}$\% & $p_\text{corr}(e|\mathcal{S})$ & \makecell{Perf. \\ Degr.}   \\
         
        \hline 
        {\makecell{Faster R-CNN \\ BDD100k}} & 34.44\% & 0.1263 & -0.0693 &  14.22\% & 0.2206 & -0.1636 \\ 
        
        
        \hline 
        {\makecell{SeTR \\ Cityscapes}} & 13.34\% & 0.0594 & -0.0446 &  9.24\% & 0.1046 & -0.0897 \\ 
        
        \hline
        {\makecell{Panoptic-FCN \\ RailSem19}} & 25.49\% & 0.8663 & -0.222 &  8.13\% & 1.0 & -0.4602 \\
        
    \end{tabular}
    \caption{Quantitative analysis of three pre-trained autonomous driving models (results are only for SWD-3). From the top-5 weak slices, we show the largest slice and the weakest performing slice. Please refer to the~\cref{appendix:qualitative_metadata_evaluation} for the top-5 slices.}
    \label{tab:summary_ad}
\end{table}






