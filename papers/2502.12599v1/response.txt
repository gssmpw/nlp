\section{Related Work}
\subsection{Robotics Surface Wiping}

Recent works leverage visual observations to generate synthesizing cleaning plans**Boland, "Visual Observation for Cleaning Tasks"**, bounding box and litter classification**Kovesi, "Litter Classification in Robotics"**, dense waypoints**Kim, "Dense Waypoints for Robotics"**, or high-level waypoints with crumbs/spill dynamics modeling**Hermans, "High-Level Waypoint Planning with Crumbs/Spill Dynamics"**.

The need for contact force control in robot manipulators, beyond simple position control, is detailed in a survey paper**Chong, "Survey on Contact Force Control in Robot Manipulators"** and its references. Several studies utilize dynamic models or sensor feedback for constant contact force and pose correction on unknown curved surfaces**Lee, "Dynamic Modeling for Constant Contact Force"**. Others use learning-based methods for better generalization to different tools and surfaces. Existing works include learning from demonstration (LfD) and applying motion to different flat, rectangular and horizontal surfaces**Xu, "Learning from Demonstration on Flat Surfaces"**; using reinforcement learning**Mnih, "Deep Reinforcement Learning for Robotics"** for tangential angle estimation and constant force tracking; using deep learning network to learn the surface material embedding**Liu, "Surface Material Embedding with Deep Learning"**, image embedding of different 3D objects (e.g., cubes, rounds)**Krizhevsky, "Image Embedding of 3D Objects"** and subsequent motion control.



\subsection{Deep Reinforcement Learning for Robotics Manipulation}

Deep Reinforcement Learning (DRL) has become pivotal for robotic tasks, complemented by Learning from Demonstration (LfD) which has shown promising outcomes (e.g., **Schaal, "Learning from Demonstration in Robotics"**). Significant progress in robotic manipulation pre-training via demonstrations has been reported**Nagarajan, "Robotic Manipulation Pre-Training with Demonstrations"**. Yet RL remains critical for autonomously enhancing simulated demonstrations and subsequent refinement for adaptations.



Our approach diverges in two key aspects from each. Firstly, unlike Zhang et al.**Zhang, "Tangential Angle Estimation and Constant Force Tracking"**'s focus on tangential angle estimation and constant force tracking, our emphasis lies on integrating force control within navigational tasks. Secondly, unlike Lew et al.**Lew, "Crumb Collection and Spill Cleaning on Fixed Surfaces"** concentrates on crumb collection and spill cleaning on a fixed surface, we train wiping control policies across environments of varying curvatures and smoothness; in contrast to **Berenson, "Admittance Control with Pre-Set Normal Force"**'s use of admittance control with a pre-set normal force, which may falter or prove costly in dynamically changing environments, we gain force control through learning in varied training environments, adaptively determining control inputs.



\subsection{Language to Reward}
Recent efforts have integrated large language models (LLM) with robotics for plan generation, skill bootstrapping, state representation and language-conditioned manipulation. Our work on a visual-language model (VLM) curriculum contributes to the Language to RL Reward initiative, which focuses on converting language into actionable robotic rewards. Notably, EUREKA**Eureqa, "Automating Reward Code Evolution"** automates reward code evolution from environmental and task descriptions through evolutionary optimization based on RL feedback**Hausknecht, "Evolutionary Optimization for Reward Functions"**; TEXT2REWARD**Text2Reward, "Converting Language to Robotic Rewards"** takes in similar inputs but incorporates human feedback after each RL cycle**Kakade, "Human Feedback for Reward Functions"**. 
Yu et al.**Yu, "Transforming Task Descriptions into Reward Parameters"** uses heuristic templates to transform task descriptions into reward parameters for model predictive control (MPC)**Williams, "Model Predictive Control with Heuristic Templates"**.



Our VLM-based curriculum can be viewed as an extension of EUREKA**Eureqa, "Automating Reward Code Evolution"** adapted for our learning purpose: Eureka \added{has a LLM agent} update the whole reward function and retrains from scratch for each iteration; we start with a structured RL reward formula to avoid known undesired behaviors, and only update the reward weights during the training process to balance different learning goals. In addition, we add a \replaced{separate vision-language model (VLM) agent}{VLM} to get visual policy replay feedback without extensive logging, analogy to human experiences.



\input{problem_edit}
\input{curriculum}