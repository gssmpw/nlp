\section{Robotic Wiping as Quality-critical MDP}
We will first formalize the problem of robotic wiping as a common Markov Decision Processes (MDPs) with dense rewards provided per step and sparse reward per episode. Then, we will show the infeasibility of the given wiping task because it is a quality-critical task. Then, we propose a new bounded formulation that makes the problem feasible.


\subsection{Initial Formulation of Markov Decision Process}
We formulate robotic wiping as a Partially Observable Markov Decision Process (POMDP), which is a tuple of the state space $S$, the observation space $O$, the action space $A$, the reward function $r$, the initial state distribution $\rho_0$, the transition function $P(\stp | \st, \at)$, and the discount factor $\gamma$. Our problem is partially observable because certain information, such as the tabletop's curvature and smoothness, is inaccessible due to limited sensory feedback. Then our goal is to find the optimal policy $\pi: {O} \mapsto {A}$ that maximizes the expected episodic reward: $E_{\vc{s}_0 \sim D}[\sum_{t=0}^T \gamma^t r(\st, \at)]$.

\textbf{State:} the state $\vc{s} \in S$ is defined as the internal state of the physics-based simulation.

\textbf{Observation:} A $46$ dimensional observation vector $\vc{o} \in O $ includes waypoint information, joint positions and velocities encoded as sine and cosine of their values, end-effector position and orientation, and force/torque sensor values. 



\textbf{Action:} we use a six dimensional pose control to directly adjust the precise position and orientation of the end-effector, which also indirectly adjusts the forces. 


\textbf{Reward:}
The reward function $r$ is defined as a weighted sum of the five terms:
\begin{equation}
r(\st, \at, \stp) = 
\begin{cases} 
r_{\text{col}} & \text{if collides} , \\
r_{\text{con}} + r_{\text{force}} + r_{\text{way}} + r_{\text{ac}} & \text{otherwise},
\end{cases}
\label{eq:reward}
\end{equation}
where we omit their arguments for brevity. We also encapsulate all the weights inside of the terms.
If collision happens, agent will receive a negative scalar reward $r_{\text{col}} = -w_{col}$ to penalize collision with the episode terminates immediately. 
Otherwise, we consider four terms that are contact flag, contact force, waypoint, and acceleration penalization rewards.
First, the contact flag reward is defined as $r_{\text{con}} = w_{\text{con}} \mathbf{I}_{\text{con}}$, where $\mathbf{I}_{\text{con}}$ is a zero or one binary flag whether the end effector makes any contact with the table.
The second force term, $r_{\text{force}}$ encourages force control while moving towards the target, which is defined as:
\begin{equation}
r_{\text{force}} = w_{\text{force}} \exp\left( -\frac{(f_z-\mu)^2}{2\sigma^2} \right) \mathbf{I}_{\text{align}},
\label{eq:rt_force}
\end{equation}
Where $w_{\text{force}}$ is the weight, $f_z$ is the upward/downward force applied at the force sensor at EE, and  $e^{-\frac{(f_z-\mu)^2}{2\sigma^2}}$ is a Gaussian shape reward centering at target force $\mu$ (in our case, $\mu =60\text{N}$). 
$\mathbf{I}_{\text{align}}$ is a binary flag which checks the alignment between EE's movement direction and the direction toward the next way point, which returns one when their cosine similarity is greater than 0.8.

The waypoint reward $r_{\text{way}} = w_{\text{way}}\mathbf{I}_{\text{way}}$ denotes the positive episodic reward agent receives for wiping each way point, as $\mathbf{I}_{\text{way}}$ indicates the completion of the waypoint. If the last waypoint is wiped, an extra sparse episodic reward will be provided, end the episode ends. Finally, the term $r_{ac}(\at) := w_{\textbf{ac}}(|a_x| + |a_y| + |a_z|)$ penalizes excessive actions, where $a_x$, $a_y$, $a_z$ are agent's accelerations at $x$, $y$, $z$ axis respectively. 













\subsection{Convergence Analysis of Quality-critical MDP} \label{sec:undesired-behavior}

The reward formulation in the previous section consists of common terms in robot learning: dense stepwise feedback to promote desired behaviors and substantial completion rewards to encourage the fast completion. In practice, many researchers typically tune the ratios with many rounds of trial and error to obtain the desirable behaviors. However, tuning hyperparameters for tasks requiring both in-process quality and rapid completion presents significant challenges.

Let us simplify two rewards: a continuous quality reward $W_q$ and an episodic terminal reward for wiping all waypoints, $W_T$. In our case, $W_q$ considers $r_{\text{con}}$, $r_{\text{force}}$, and $r_{\text{ac}}$ while $W_T$ corresponds to the waypoint reward $r_{way}$. We have $W_q^{\text{max}} > 0$ for constant contact with target force and small accelerations, $W_q^{\text{poor}} < W_q^{\text{max}}$ for all other undesired qualities, and $W_T > 0$ to encourage completion. Then, the agent can learn one of three possible strategies, and get respective accumulated rewards:


\begin{itemize}
    \item \textbf{optimal}: takes the best quality wipe and terminates at minimum required time $\text{T}_2$ steps: $\sum_{t=0}^{T_2} \gamma^t W_q^{\text{max}} + \gamma^{T_2} W_T$.
    \item \textbf{lazy}: suboptimal, finishes episode as early as possible without maintaining wiping qualities (e.g., jumping between waypoints with high accelerations and no constant contacts): $\sum_{t=0}^{T_1} \gamma^t W_q^{\text{poor}} + \gamma^{T_1} W_T$.
    \item \textbf{forever}: suboptimal, keeps getting a quality reward without task completion: $\sum_{t=0}^{\infty} \gamma^t W_q^{\text{max}} = W_q^{\text{max}}/(1-\gamma)$.
    
\end{itemize}


For stable learning, it's crucial to establish a feasible relationship between \(W_T\) and \(W_q^{\max}\) so that accumulated rewards meet the constraints for episodes of varying lengths \(\text{T}_1 < \text{T}_2\) and for any \(W_q^{\text{poor}} < W_q^{\text{max}}\). From $\textbf{R}_{\textbf{optimal}} \gg \textbf{R}_{\textbf{lazy}}$, we get the relation below.
\begin{equation}
W_T \ll (\sum_{t=0}^{T_2} \gamma^t W_q^{\text{max}} - \sum_{t=0}^{T_1} \gamma^t W_q^{\text{poor}})/(\gamma^{T_1} - \gamma^{T_2}), 
\label{eq:cond1}
\end{equation}
And from $\textbf{R}_{\textbf{optimal}} \gg \textbf{R}_{\textbf{forever}}$, we get the relation below.
\begin{equation}
W_T \gg (\sum_{t=T_{2+1}}^{\infty} \gamma^t W_q^{\text{max}})/\gamma^{T_2} ,
\label{eq:cond2}
\end{equation}






By combining Eqs.~\ref{eq:cond1} and \ref{eq:cond2}, we want to find a feasible range of $W_T$ regarding $W_q^{\text{max}}$:






\begin{equation}
\text{L}(\text{W}_q^{\text{max}}) \ll W_T \ll \text{U}(\text{W}_q^{\text{max}})
\label{eq:cond3}
\end{equation}


Where $\text{U}(\text{W}_q^{\text{max}}) = (\sum_{t=0}^{T_2} \gamma^t W_q^{\text{max}} - \sum_{t=0}^{T_1} \gamma^t W_q^{\text{poor}})/(\gamma^{T_1} - \gamma^{T_2})$ and $\text{L}(\text{W}_q^{\text{max}}) = (\sum_{t=T_{2+1}}^{\infty} \gamma^t W_q^{\text{max}})/\gamma^{T_2}$.
Finding the lower bound of $\text{U}(\text{W}_q^{\text{max}})$ is more straightforward, as $\text{T}_2/\text{T}_1$ predominantly influences the exponential terms, while $\text{W}_q^{\text{poor}}/\text{W}_q^{\text{max}}$ affects only the linear terms.
We can approximate the lower bound of $\text{U}(\text{W}_q^{\text{max}})$ by setting $\text{T}_1 = 1$ and $\text{T}_2 = \text{H}$, where $\text{H}$ denotes the episode horizon (in our case, $\text{H} = 200$). Then $\text{U}(\text{W}_q^{\text{max}}) \in [99.02\text{W}_q^{\text{max}}, 101.30\text{W}_q^{\text{max}}]$ for $\text{W}_q^{\text{poor}}/\text{W}_q^{\text{max}} \in [0.01, 0.99]$. 


On the contrary, finding a feasible $\text{L}(\text{W}_q^{\text{max}})$ applicable for all $\text{T}_2$ is more challenging and prevents the the feasible range of current formulation, which motivates the next section.





\subsection{Bounded Reward Design for Improved Feasibility} \label{sec:bounded-reward-design}

 

\begin{wrapfigure}{r}{0.4\linewidth}
  \centering
  \includegraphics[width=0.80\linewidth]{figs/checkpoint_region.jpg}
  \caption{Illustration of Checkpoint Regions.}
  \label{fig:checkpoint}
  \vspace{-1em}
\end{wrapfigure}

To address $\text{L}(\text{W}_q^{\text{max}})$, we introduce concentric circular checkpoint regions between waypoints to promote navigation, inspired by the horizontal checkpoints in the Google research football environment \cite{kurach2020google}. This setup introduces a bounded reward mechanism for target force control $r_{\text{force}}$ and constant contact $r_{\text{con}}$, as outlined in equation~(\ref{eq:reward}). %

Fig.~\ref{fig:checkpoint} illustrates these checkpoint regions around a waypoint, with the next waypoint marked by a green point at the center of equally distanced concentric circles. Rewards $r_{\text{force}}$ and $r_{\text{con}}$ are granted per checkpoint region rather than per step. The updated reward function is similar to equation~(\ref{eq:reward}) but with an additional indicator function:













\begin{equation}
r = 
\begin{cases} 
r_{\text{col}} & \text{if collides} , \\
\mathbf{I}_{\text{check}} (r_{\text{con}} + r_{\text{force}}) + r_{\text{way}} + r_{\text{ac}} & \text{otherwise}.
\end{cases}
\label{eq:reward1}
\end{equation}

Now, two positive terms, $r_{\text{con}}$ and $r_{\text{force}}$ are controlled by the checkpoint indicator $\mathbf{I}_{\text{check}}$, which limits the occurrence of those terms to the number of the checkpoints. This gives us a direct way to bound the cumulative reward $\text{R}_\text{forever}$, which is re-defined from  $\sum_{t=0}^{\infty} \gamma^t W_q^{\text{max}}$ to  $\sum_{t=0}^{T_2} \gamma^t W_{q1}^{\text{max}} + \sum_{t={T_2+1}}^{\infty} \gamma^t W_{q2}^{\text{max}}$, where $\text{T}_2$ is approximated by the time to traverse each checkpoint region only once. $W_{q1}^{\text{max}}$ is identical to $W_{q}^{\text{max}}$ within checkpoint regions, but $W_{q2}^{\text{max}} < 0$ only contains acceleration penalties when all checkpoint regions have been visited. And hence $\text{L}(\text{W}_q^{\text{max}})$ is re-defined as: 

\begin{equation}
\text{L}(\text{W}_q^{\text{max}}) = (\sum_{t={T_2+1}}^{\infty} \gamma^t W_{q2}^{\text{max}})/ \gamma^{T_2}
\label{eq:reward1}
\end{equation}



Given $\text{L}(\text{W}_q^{\text{max}}) \ll 0$, equation~\ref{eq:cond3} holds for $0 < \text{W}_T \ll 99\text{W}_q^{\text{max}}$, altering the policy convergence landscape. Our experiments demonstrate this effectively prevents the convergence to perpetual wiping, as elaborated in Section~\ref{sec:res}.





































\
