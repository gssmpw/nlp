\section{Visual-Language Model based Curriculum} \label{sec:vlm}

\begin{figure*}[t]
  \centering
    \includegraphics[width=0.83\textwidth]{figs/diagram-0915.png}
  \caption{Diagram of the Proposed VLM Algorithms, simulating human decision process on reward scale engineering.
  }
  \label{fig:alog}
  \vspace{-1.5em}
\end{figure*}


While the new formulation makes the quality-critical problem feasible, learning is still hyperparameter sensitive.
To ensure successful trajectories exist and hence can be learned subsequently, we propose a novel Vision-Language Models (VLM) based curriculum learning system, which automatically monitors training metrics and adjusts relative weights of reward terms during the learning process, which resembles the parameter tuning process of human experts.








\subsection{VLM-based Curriculum}

Our learning framework calls the VLM-based curriculum every K steps after the initial M training steps, where K and M are hyper parameters.
The curriculum module auto-adjusts reward weights
for the next cycle with following steps:

\textbf{Step1: Inspection.} In this step, our goal is to collect the initial set of information, which includes success rates, landing pressure profiles, and navigation pressure stats. These stats can be collected by expanding the rollout of the current policy $\pi$ for $N$ times. We maintain the history of the previous information for reference. Once the information is collected, the system checks the pre-defined predicates (e.g., force variance decreased without a significant reduction in navigation success rates) to see if it wants to call the VLM-based hyperparameter tuning.


\textbf{Step2: Update} 
In this step,\added{ there are two large model agents involved: a LLM agent and a VLM agent. } The LLM takes in provided metric \added{from Step 1} and updates reward weights. Depending on the training progress, the LLM could request for different extra information before updating. If the completion rate is low, vision feedback of ending scene summarized by a separate VLM will be provided to describe failure reasons (e.g., no contact, or close to endpoint without finish wiping). If the force metrics require improvements, detailed force percentiles will be provided. This step is desired with multiple purposes: 1) Only providing necessary details into prompts to avoid LLM's catastrophic forgetting on important information. 2) Navigation failures can arise from various scenarios. Leveraging VLM’s semantic capabilities allows us to understand the causes of failures, reducing the need for labor-intensive monitoring and iterative metric development. 3) This hierarchical approach enhances system's extensibility. \added{4) Separating LLM and VLM optimizes reasoning and visual data interpretation respectively.}

\added{The final metrics and extra information will be feed to the LLM. The output from LLM consists of two parts: 1) A 1-2 sentences step-by-step analysis on logs and focus-learning areas; 2) python code for updated reward parameters.} 





\added{Detailed prompts can be found at our website noted in the abstract.} The high-level description is summarized in Algorithm~\ref{algo:vlm-curriculum} and Figure~\ref{fig:alog}.

\makeatletter
\algnewcommand{\LineComment}[1]{\Statex \hskip\ALG@thistlm \(\triangleright\) #1}
\algnewcommand{\IndentLineComment}[1]{\Statex \hskip\ALG@tlm \(\triangleright\) #1}
\makeatother


\begin{algorithm}
\caption{VLM-based Curriculum Learning}
\label{algo:vlm-curriculum}
\begin{algorithmic}[1]
\State \textbf{Data:} \added{pre-trained LLM $L$ and }VLM $V$
\State \textbf{Data:} a \added{RL} policy $\pi$
\State \textbf{Data:} a \added{reward weights} parameter vector $\mathbf{w}$
\State $d \gets$ dict(), $i \gets 0$
\While{not converged}
    \State $\pi \gets$ learn($\pi$, $\mathbf{w}$, K) \Comment{Learn a policy for $K$ steps}
    \LineComment{Step 1. Inspection}
    \State $d \gets$ eval($d$, $\pi$, $i$) \Comment{Eval $\pi$ and update $i$th iter data}

    \If{not $\text{maintain}()$}
        \IndentLineComment{Step 2. Update}
        \State $d \gets$ request\_extra\_info\_if\_needed($V$, $d$, $i$)
        \State $\mathbf{w} \gets$ update\_reward\_params(\replaced{$L$}{$V$}, $d$)
    \EndIf
    \State $i += 1$
\EndWhile
\end{algorithmic}
\end{algorithm}
\vspace{-1.5em}





























\subsection{Implementation details}

\replaced{We used gpt-4~\cite{achiam2023gpt} as LLM and gpt-4-vision-preview~\cite{openai2023gptv} as VLM.}{We implemented GPT-4~\cite{achiam2023gpt} as the VLM.} To ensure thorough exploration of initial parameters, we initiate our module at 300k steps. We evaluate $N=50$ episodes every $K=100k$ steps and invoke the LLM curriculum module unless evaluation metrics meet the maintenance criteria: an improvement in force profiles—defined by a mean force deviation from the target of less than 5N with reduced variance—without significantly compromising the navigation completion rate (a permissible change of less than 15\%). Initially, $\text{W}_T = 1000$ and $\text{W}_q^{\text{max}} = 29$, which is far from the upper limit of the feasible range $0 < \text{W}_T \ll 99\text{W}_q^{\text{max}}$ outlined in Section \ref{sec:bounded-reward-design}. \added{Optionally, researchers can clip the reward weights for each goal do not exceed twice the weight of any other goal for safeguard.} Throughout the fine-tuning process, the ratio consistently remains within this range.
