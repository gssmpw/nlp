\section{Related Work}
\subsection{Robotics Surface Wiping}

Recent works leverage visual observations to generate synthesizing cleaning plans~\cite{hess2011learning, elliott2018robotic}, bounding box and litter classification~\cite{yin2020table}, dense waypoints~\cite{cauli2018autonomous, kim2018icub}, or high-level waypoints with crumbs/spill dynamics modeling~\cite{lew2023robotic}. 

The need for contact force control in robot manipulators, beyond simple position control, is detailed in a survey paper~\cite{suomalainen2022survey} and its references. Several studies utilize dynamic models or sensor feedback for constant contact force and pose correction on unknown curved surfaces~\cite{qian2019sensorless, lin2022real, li2022real, zapico2024semi}. Others use learning-based methods for better generalization to different tools and surfaces. Existing works include learning from demonstration (LfD) and applying motion to different flat, rectangular and horizontal surfaces~\cite{elliott2017learning}; using reinforcement learning~\cite{zhang2020robotic} for tangential angle estimation and constant force tracking; using deep learning network to learn the surface material embedding~\cite{kawaharazuka2022learning}, image embedding of different 3D objects (e.g., cubes, rounds)~\cite{saito2020wiping, saito2021utilization} and subsequent motion control. 









\subsection{Deep Reinforcement Learning for Robotics Manipulation}



Deep Reinforcement Learning (DRL) has become pivotal for robotic tasks, complemented by Learning from Demonstration (LfD) which has shown promising outcomes (e.g., \cite{kim2018icub, gams2016adaptation}). Significant progress in robotic manipulation pre-training via demonstrations has been reported \cite{brohan2023rt}. 
Yet RL remains critical for autonomously enhancing simulated demonstrations and subsequent refinement for adaptations. 



Our approach diverges in two key aspects from each. Firstly, unlike Zhang et al.~\cite{zhang2020robotic}'s focus on tangential angle estimation and constant force tracking, our emphasis lies on integrating force control within navigational tasks. 
Secondly, unlike Lew et al.~\cite{lew2023robotic} concentrates on crumb collection and spill cleaning on a fixed surface, we train wiping control policies across environments of varying curvatures and smoothness; in contrast to \cite{lew2023robotic}'s use of admittance control with a pre-set normal force, which may falter or prove costly in dynamically changing environments, we gain force control through learning in varied training environments, adaptively determining control inputs.






\subsection{Language to Reward}
Recent efforts have integrated large language models (LLM) with robotics for plan generation, skill bootstrapping, state representation and language-conditioned manipulation. Our work on a visual-language model (VLM) curriculum contributes to the Language to RL Reward initiative, which focuses on converting language into actionable robotic rewards. Notably, EUREKA~\cite{ma2023eureka} automates reward code evolution from environmental and task descriptions through evolutionary optimization based on RL feedback~\cite{ma2023eureka}; TEXT2REWARD~\cite{xie2023text2reward} takes in similar inputs but incorporates human feedback after each RL cycle~\cite{xie2023text2reward}. 
Yu et al.~\cite{yu2023language} uses heuristic templates to transform task descriptions into reward parameters for model predictive control (MPC)~\cite{yu2023language}.

Our VLM-based curriculum can be viewed as an extension of EUREKA~\cite{ma2023eureka} adapted for our learning purpose: Eureka \added{has a LLM agent} update the whole reward function and retrains from scratch for each iteration; we start with a structured RL reward formula to avoid known undesired behaviors, and only update the reward weights during the training process to balance different learning goals. In addition, we add a \replaced{separate vision-language model (VLM) agent}{VLM} to get visual policy replay feedback without extensive logging, analogy to human experiences. 






\input{problem_edit}
\input{curriculum}