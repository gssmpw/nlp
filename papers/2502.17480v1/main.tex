\documentclass[]{fairmeta}

% \usepackage[top=2cm, bottom=2cm, left=1cm, right=1cm]{geometry}
% \setlength{\columnsep}{1cm}

% XXX To uncomment:
% \newcommand{\citet}[1]{\textcite{#1}}
% \newcommand{\citep}[1]{\parencite{#1}}

\usepackage{hyperref}
\usepackage{url}
% \usepackage{authblk}  % authors with affiliations
            
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{enumitem}
\usepackage{subcaption}
%\usepackage[dvipsnames]{xcolor}
\usepackage{wasysym}

% XXX Use bibtex instead?
% \usepackage[sorting=none,doi=false,isbn=false,giveninits=true,style=nature]{biblatex} 
% \bibliography{sample.bib}

\usepackage{microtype}      % microtypography
\usepackage{graphicx}
\usepackage{adjustbox}
\usepackage{float}               % For float placement control

\usepackage{amsmath,amssymb,amsthm,bm,dsfont}
\usepackage{array, booktabs, makecell}
\usepackage{multirow}
\usepackage{cleveref}

\usepackage{pifont}

\usepackage[T1]{fontenc} % Ensures proper font encoding
\usepackage{courier}    % For Courier font
\usepackage{relsize}

% Define the macro
\newcommand{\typing}[1]{\textsc{\smaller #1}}


% \usepackage{lineno}
% \linenumbers

% \title{Brain2Qwerty: Decoding Typing from Brain Activity}
% \title{Brain2Qwerty: Decoding Language Production from Brain Activity}
% \title{Brain2Qwerty: Decoding Sentence Typing from Brain Activity}

% \title{A Non-Invasive Approach to Brain-Computer Typing}
% \title{A Non-Invasive Approach to Brain-to-Text Decoding}
% \title{A Non-Invasive Method to Decode Sentences from Brain Activity}

% \title{Decoding Typing from Brain Activity: A Non-Invasive Method}
% \title{A Non-Invasive Method for Brain-to-Text Communication through typing}
% \title{A Non-Invasive Method for Brain-to-Text Decoding via Typing}
% \title{Non-Invasive Decoding Brain-to-Text Decoding via Typing}
% \title{Decoding of Text from Non-invasive recordings of brain activity}
% \title{Decoding Text from Brain Activity: A Non-Invasive Approach via Typing}
% \title{Decoding Language from Brain Activity: A Non-Invasive Approach via Typing}
\title{Brain-to-Text Decoding: \\
A Non-invasive Approach via Typing}


% \title{Towards an AI Model to Decode Non-Invasive Brain-to-Text Communication}
% \title{Towards a Non-Invasive Brain-to-Text Communication Device}
%\title{Towards decoding typing from non-invasive brain recordings}

\author[1]{Jarod Lévy}
\author[2,3]{Mingfang (Lucy) Zhang}
\author[4,5]{Svetlana Pinet}
\author[1]{Jérémy Rapin}
\author[1]{Hubert Banville}
\author[1*]{Stéphane d'Ascoli}
\author[1*]{Jean-R\'emi King}

\affiliation[1]{Meta AI}
\affiliation[2]{École Normale Supérieure, Université PSL, CNRS}
\affiliation[3]{Hospital Foundation Adolphe de Rothschild}
\affiliation[4]{Basque Center on Cognition, Brain and Language, San Sebastian}
\affiliation[5]{Ikerbasque, Basque Foundation for Science, Bilbao}


\contribution[*]{equal contribution}

\abstract{Modern neuroprostheses can now restore communication in patients who have lost the ability to speak or move. However, these invasive devices entail risks inherent to neurosurgery. Here, we introduce a non-invasive method to decode the production of sentences from brain activity and demonstrate its efficacy in a cohort of 35 healthy volunteers. For this, we present Brain2Qwerty, a new deep learning architecture trained to decode sentences from either electro- (EEG) or magneto-encephalography (MEG), while participants typed briefly memorized sentences on a QWERTY keyboard. With MEG, Brain2Qwerty reaches, on average, a character-error-rate (CER) of 32\% and substantially outperforms EEG (CER: 67\%). For the best participants, the model achieves a CER of 19\%, and can perfectly decode a variety of sentences outside of the training set. While error analyses suggest that decoding depends on motor processes, the analysis of typographical errors suggests that it also involves higher-level cognitive factors. Overall, these results narrow the gap between invasive 
and non-invasive methods and thus open the path for developing safe brain-computer interfaces for non-communicating patients.

% While decoding remains consistent across sentences, CER correlates with the frequency of characters and words. Finally, our model makes most mistakes when participants effectively produce typographical errors. Overall, these results narrow the gap with current invasive approaches and thus open the path for developing safe brain-computer interfaces for non-communicating patients.

% Deciphering how the brain encodes complex behaviors, such as language production, remains a fundamental challenge in neuroscience.
% While invasive techniques like electrocorticography (ECoG) have demonstrated significant success in decoding neural signals during tasks such as speech and handwriting, achieving similar performance with noninvasive methods has remained elusive. In this study, we introduce \textit{Brain2Qwerty}, an innovative framework for decoding typed sentences from noninvasive neural signals recorded via electroencephalography (EEG) and magnetoencephalography (MEG). By employing a novel deep learning pipeline, we achieve unprecedented accuracy in decoding continuous language production from limited data. Our approach decodes 1 out of 3 correct characters from EEG signals and 2 out of 3 correct characters from MEG signals. Furthermore, our findings reveal the ability to decode not only motor functions but also cognitive processes, with a clear improvement in distinguishing correct characters from typing errors. These results mark a significant step forward for noninvasive brain-computer interfaces (BCI) opening the door to real-world applications.
}

\date{\today}
\correspondence{\email{\{jarod,sdascoli,jeanremi\}@meta.com}}

% You can add additional metadata fields as follows 
% \metadata[Code]{\url{https://github.com/fairinternal/brainai}}
% \metadata[Blogpost]{\url{https://ai.meta.com/blog/?page=1}}


\begin{document}

% \twocolumn
\maketitle
\input{figures/fig1.tex}
\section{Introduction}

% General motivation
The past decade has been marked by rapid progress in brain-computer interfaces (BCIs) for individuals who, after a brain lesion, have lost their ability to speak or communicate. 
% SOTA intracranial
In particular, several patients suffering from anarthria \citep{Metzger2022, moses2021neuroprosthesis}, Amyotrophic Lateral Sclerosis (ALS) \citep{willett2023neuroprosthesis}, or severe paralysis \citep{Hochberg2012} have now been able to produce full sentences via a neuroprosthesis, which records and decodes neural activity from motor regions of the brain. Originally limited to decoding small sets of linguistic features  \citep{herff2019generating, angrick2019speech, anumanchipalli2019speech, moses2021neuroprosthesis, card2024accurate}, words \citep{Metzger2022}, and gestures \citep{willett2021handwriting}, the recent development of AI models has improved the precision and rapidity of brain-to-text decoding to a point of enabling natural language production at rates close to normal speech \citep{metzger2023neuroprosthesis, wairagkar2024voice}.

% Challenge 1: invasive is dangerous and does not scale
However, such invasive neuroprostheses require a neurosurgical procedure, and thus expose patients to non-negligible risks of brain hemorrhage and infection \citep{chung2019highdensity, Bullard2020, Leuthardt2021, Baranauskas2014}. Additionally, maintaining functional cortical implants over extended time periods remains challenging~\citep{Fekete2023, Zhou2024, Yasar2024}. As a result, in their current form, \emph{invasive} BCIs are not easily scalable for diagnosing or restoring communication in the large groups of non- or poorly-responsive patients \citep{owen2006detecting, Claassen2019}.

% Challenge 2: non invasive is too noisy
\emph{Non-invasive} BCIs could potentially address this challenge. However, they are usually based on scalp electroencephalography (EEG), whose limited signal-to-noise ratio \citep{Mak2009} requires users to perform complex tasks. For example, EEG-based BCIs typically require individuals to maintain their attention on flickering stimuli \citep{abiri2019review} or to imagine moving their hand or foot over long time periods \citep{bodien2024cognitive} -- two tasks known to produce EEG patterns that can be relatively easily detected by a linear classifier. Even so, decoding performance remains moderate. For instance, a public BCI benchmark \citep{chevallier2024} using EEG achieves an accuracy of only 43.3\% on a four-class classification task with a motor imagery dataset \citep{Yi2014}. In sum, current non-invasive methods fall short of providing a fast and reliable BCI.

Two elements could address these challenges. First, magnetoencephalography (MEG), which measures the fluctations of magnetic fields elicited in the cortex, has higher signal-to-noise ratio than EEG \citep{Hamalaainen1993, Goldenholz2009, Baillet2017}. Second, deep learning models trained to reconstruct natural language from MEG signals in language \emph{comprehension} paradigms have recently demonstrated major improvements, especially in comparison to EEG \citep{defossez2023decoding}. Together, these elements thus indicate that with modern AI techniques, high-quality MEG signals and natural language tasks could be combined to decode the \emph{production} of language from non-invasive recordings of the brain.

% Approach
In this study, we introduce Brain2Qwerty, an AI model trained to decode text production from non-invasive recordings of brain activity (Fig. \ref{fig:approach}). For this, we tasked 35 participants to type briefly memorized sentences on a keyboard, while their brain activity was recorded with either EEG or MEG. We then train Brain2Qwerty, a three-stage deep neural network trained to decode text from these brain signals and evaluate it on both EEG (20 participants, 146K characters, 23K words and 4K sentences) and MEG recordings (20 participants, 193K characters, 30K words and 5K sentences).
Note that the present study does \emph{not} delve into \emph{how} the brain produce language during  typing. This neuroscientific issue is addressed in a companion paper \citep{lucy2025}.

% \begin{itemize}
%     \item Non-invasive techniques, such as electroencephalography (EEG) and magnetoencephalography (MEG), require users to engage in prolonged and demanding tasks like sustained attention or motor imagery \cite{bodien2024cognitive}.
%     \item Example: The P3-Speller allows participants to spell letters by focusing on flickering stimuli on a screen but is slow and demanding \cite{abiri2019review}.
%     % \item Extensive literature exists on EEG-based brain decoding; however, most models heavily rely on teacher-forcing during evaluation. This reliance limits their applicability in real-world scenarios where such controlled conditions are unavailable \cite{wang2022open, duan2023dewave}.
% \end{itemize}


%%%



% \begin{table*}
%     \footnotesize
%     \centering
%     \begin{tabular}{lccccccccc}  % Corrected to 10 columns (one less 'c')
%     \hline
%     Dataset & Subjects & \makecell{Blocks} & Time (h) & \makecell{Avg\\Duration (h)} & Words & \makecell{Unique\\Words} & \makecell{Unique\\Sentences} & Keystrokes & Sensors \\ \hline
%     EEG & 20 & 62 & 17.7 & 0.88 & 23,040 & 351 & 128 & 146,551 & 64  \\
%     MEG & 19 & 81 & 21.5 & 0.93 & 30,626 & 351 & 128 & 193,094 & 306  \\ \hline
%     \end{tabular}
%     \caption{\textbf{Dataset Characteristics.}
%     %This table summarizes the key attributes of the datasets analyzed in this study.
%     }
%     \label{tab:dataset}
% \end{table*}

\section{Results}

% \begin{figure*}[!ht]
%     \centering 
%     \hspace{-0.8cm}
%     \includegraphics[width=0.7\textwidth]{figures/fig2.pdf}
%     \caption{
%         \textbf{Linear Decoding Performance.} \\
% A linear ridge classifier model is trained to predict either the hand used to type (A) or the character typed (B). Results for EEG are shown in blue, while MEG results are displayed in green. The thin colored lines represent chance level. Intervals where decoding accuracy is significantly above chance (p < 0.05) are marked with a star.
%     }
%     \label{fig:linear_perf}
% \end{figure*}

% \begin{figure}[!ht]
%     \hspace{-0.8cm}
%     \includegraphics[width=0.5\textwidth]{figures/fig3.2.pdf}
%     \caption{
%         \textbf{Model Decoding Performance.} \\
% Comparison of decoding performance across \textbf{EEG (left)} and \textbf{MEG (right)} for baseline models (Linear and EEGNet \cite{lawhern2018EEGNet}) and our proposed model (Ours). Each point represents a participant and the mean value is displayed. Statistical significance (one-sided Wilcoxon tests across the participants) are denoted as follows: p < 0.05 (*), p < 0.01 (**), and p < 0.001 (***).
%     }
%     \label{fig:main_results}
% \end{figure}

\input{figures/fig2.tex}

\subsection{Linear Decoding}
To verify that our typing protocol leads to the expected brain responses, we first focus on the differences in evoked responses elicited by left- and right-handed key presses (Fig. \ref{fig:model_perf} A-B). The resulting topographies are typical of those associated with motor activity in the cortex \citep{Donner2009}. In addition, we trained a linear ridge classifier per subject to categorize left- vs right-handed responses at each time-sample relative to key presses. The classification accuracy peaks t=40\,ms after the key press (Fig. \ref{fig:model_perf}). MEG achieves a peak accuracy of 74$\pm$1.3\% ($\pm$ reports standard-error-of-the-mean (SEM) across subjects), significantly outperforming EEG which achieves 64$\pm$0.8\% (Mann-Withney U test: p<$10^{-7}$). We then trained the same model to classify the keys pressed. Character accuracy peaks around the same time, reaching a value of 22$\pm$0.8\% for MEG and 16$\pm$0.5\% for EEG, significantly above the chance level (14\%). Overall, these findings confirm that the present protocol leads to the expected brain responses to key presses 
\citep{Pinet2020}.


\subsection{Brain2Qwerty Performance}
We next trained Brain2Qwerty, a new deep learning architecture, to decode individual characters from these M/EEG signals (see Methods in \cref{subsection:decoder}) and evaluated both the hands-error-rate (HER) and the character-error-rate (CER).
Brain2Qwerty achieves a CER of 32$\pm$0.6\% with MEG and 67$\pm$1.5\% with EEG. 
This performance reflects a substantial difference across recording devices (p<$10^{-8}$). The best and worst EEG subjects reach a CER of 61$\pm$2.0\% across sentences and 71$\pm$2.3\%, respectively. Similarly, the best and worst MEG subjects reach a CER of 19$\pm$1.1\% and 45$\pm$1.2\%, respectively. 

% For the best-performing subject, the error rate ranges from 0.0 to 0.6, while for the worst-performing subject, it ranges from 0.2 to 0.8. Despite this variability, these results demonstrate that even for the subject with the highest error rates, the model is still capable of decoding sentences with reasonable accuracy in most cases. 


\subsection{Comparing Brain2Qwerty to Baseline Models}
How does Brain2Qwerty perform in comparison to classic baseline architectures? To address this issue we trained a linear model as well as EEGNet --  a popular architecture used in BCIs \citep{lawhern2018EEGNet} -- with the same approach, and compared their decoding performance to Brain2Qwerty's with a Wilcoxon test across subjects (Fig. \ref{fig:model_perf}~E-H).
%
EEGNet outperforms the linear model on both HER (p=0.008) and CER (p<$10^{-4}$) for MEG  -- although only on HER for EEG (p=0.03). %, hence confirming that deep learning is effective at extracting non-linear patterns embedded in the M/EEG signals
However, EEGNet remains less effective than our model, which achieves, in comparison, a 1.14-fold improvement in CER with EEG (p<$10^{-5}$) and a 2.25-fold improvement for MEG (p<$10^{-6}$), respectively.

\subsection{Brain2Qwerty Ablations}
To validate our design choices, we then retrained different ablated versions of our model. Specifically, we re-trained and evaluated (i) the Convolutional Module (i.e. no transformer, no language model) and (ii) the Conv+Transformer (i.e. without Language Model) with the same hyperparameters. 
%
% The difference in CER between the baseline models and our pipeline is illustrated by a p of $10^{-6}$ for EEG and $10^{-7}$ for MEG (Wilcoxon signed-rank text).
%
%In \cref{fig:model_perf}, we initially present the performance of our model on a conventional Brain-Computer Interface (BCI) task, specifically hand recognition. %Notably, our model achieves character-error-rates (CERs) of 26\% and 9\% for this task.
%
The Convolutional Module alone outperforms EEGNet both on EEG (HER: p=0.009, CER: p=0.03) and MEG (HER: p<$10^{-5}$, CER: p<$10^{-6}$). %This result suggests that the subject-embedding layer of our Convolutional Module is important to maximize decoding performance. 
%
Adding the transformer only appears beneficial to CER, both for EEG (p<$10^{-4}$) and MEG (p<$10^{-6}$). 
%
Finally, the use of a Language Model module leads to an additional improvement of the CER of EEG (p<$10^{-5}$) and MEG (p<$10^{-6}$).
Overall, these results show that the sentence-level contextualization provided by the transformer together with the leverage of natural language's statistical regularities effectively improves the decoding of individual characters. 
% This result shows that the statistical regularities of natural language can be effectively used to improve decoding performance. 

%The difference in performance between EEG and MEG in the final step of our model is illustrated by a p of $10^{-8}$ (one-sided Mann-Whitney U test across participants). The results confirm that MEG data exhibits a higher signal-to-noise ratio and provides richer temporal and spatial information, enabling more accurate predictions. 

% We also note the critical role of each part of our pipeline in improving model performance. In the MEG setup, each step contributes to a substantial reduction in the CER, with a p below $10^{-6}$ observed between performances at consecutive stages. In particular, the inclusion of the Transformer module leads to a 15\% reduction in the CER for MEG data. The language model provides an additional 6\% improvement, demonstrating its effectiveness when the predictions from previous steps are already relatively accurate.

% Regarding the WER, the high value obtained by the EEGNet model can be attributed to its lack of a subject embedding layer, whereas the linear model is fitted to each individual subject.

% In \cref{fig:performance_sentence}, we give a more detailed view of the performance of our model across held-out (via cross-validation) sentences of the dataset. In our protocol, participants could type the same sentence multiple times. To account for this, we averaged performance metrics for each unique sentence.

% Overall, these results demonstrate unprecedented performance levels, particularly for MEG, and provide evidence supporting the utility of our multi-step framework for improving decoding accuracy.

\input{figures/fig3.tex}


% \begin{table*}[!ht]
%      \centering
%      \begin{minipage}{0.49\textwidth}
%         \centering
%         \begin{tabular}{p{0.3\textwidth} p{0.7\textwidth}}
%             \textbf{Read:} & \typing{las teorias reducen los numeros} \\
%             \textbf{Best subject:} & \typing{\textcolor[rgb]{0.204, 0.596, 0.859}{las} \textcolor[rgb]{0.882, 0.071, 0.188}{pr}\textcolor[rgb]{0.204, 0.596, 0.859}{orias reducen los numeros}} \\
%             \textbf{Median subject:} & \typing{\textcolor[rgb]{0.204, 0.596, 0.859}{las teorias }\textcolor[rgb]{0.882, 0.071, 0.188}{exig}\textcolor[rgb]{0.204, 0.596, 0.859}{en los }\textcolor[rgb]{0.882, 0.071, 0.188}{homb}\textcolor[rgb]{0.204, 0.596, 0.859}{ros}} \\
%             \textbf{Worst subject:} & \typing{\textcolor[rgb]{0.204, 0.596, 0.859}{las} \textcolor[rgb]{0.882, 0.071, 0.188}{ranc}\textcolor[rgb]{0.204, 0.596, 0.859}{ias re}\textcolor[rgb]{0.882, 0.071, 0.188}{vis}\textcolor[rgb]{0.204, 0.596, 0.859}{en los numer}\textcolor[rgb]{0.882, 0.071, 0.188}{ad}} \\
%             \addlinespace

%         \end{tabular}
%     \end{minipage}
%     \hfill
%     \begin{minipage}{0.49\textwidth}
%         \centering
%         \begin{tabular}{p{0.3\textwidth} p{0.7\textwidth}}

%             \textbf{Read:} & \typing{la estadistica sigue la distribucion} \\
%             \textbf{Best subject:} & \typing{\textcolor[rgb]{0.204, 0.596, 0.859}{la estadistica sigue la distribucion}} \\
%             \textbf{Median subject:} & \typing{\textcolor[rgb]{0.882, 0.071, 0.188}{stamistosa} \textcolor[rgb]{0.204, 0.596, 0.859}{sigue la distribucion}} \\
%             \textbf{Worst subject:} & \typing{\textcolor[rgb]{0.204, 0.596, 0.859}{la estadistica} \textcolor[rgb]{0.882, 0.071, 0.188}{f}\textcolor[rgb]{0.204, 0.596, 0.859}{igu}\textcolor[rgb]{0.882, 0.071, 0.188}{ra de petrilla lo}} \\

%             \addlinespace
%     % \textbf{Read:} & \typing{el centro describe las parabolas }\\
%     % \textbf{Type:} & \typing{\underline{w}l centro describe las parabolas }\\
%     % \textbf{Decode:} & \typing{\textcolor[rgb]{0.882, 0.071, 0.188}{\underline{e}}\textcolor[rgb]{0.204, 0.596, 0.859}{l centro} \textcolor[rgb]{0.882, 0.071, 0.188}{trece de} \textcolor[rgb]{0.204, 0.596, 0.859}{las} \textcolor[rgb]{0.882, 0.071, 0.188}{carabin}\textcolor[rgb]{0.204, 0.596, 0.859}{as} }\\
%     % \addlinespace
%     % \textbf{Read:} & \typing{los usos ofrecen las ventajas energeticas }\\
%     % \textbf{Type:} & \typing{los usos ofrecen las ventajas energeticas }\\
%     % \textbf{Decode:} & \typing{\textcolor[rgb]{0.204, 0.596, 0.859}{los} \textcolor[rgb]{0.882, 0.071, 0.188}{para me}\textcolor[rgb]{0.204, 0.596, 0.859}{recen las venta}\textcolor[rgb]{0.882, 0.071, 0.188}{n}\textcolor[rgb]{0.204, 0.596, 0.859}{as e}\textcolor[rgb]{0.882, 0.071, 0.188}{structur}\textcolor[rgb]{0.204, 0.596, 0.859}{as} }\\
%     % \addlinespace
%     % \textbf{Read:} & \typing{la explicacion aclara la pregunta de la evaluacion }\\
%     % \textbf{Type:} & \typing{la explicacion aclara la pregunta de la evaluacion }\\
%     % \textbf{Decode:} & \typing{\textcolor[rgb]{0.204, 0.596, 0.859}{la explicacion}\textcolor[rgb]{0.882, 0.071, 0.188}{es para} \textcolor[rgb]{0.204, 0.596, 0.859}{la pre}\textcolor[rgb]{0.882, 0.071, 0.188}{sentan a} \textcolor[rgb]{0.204, 0.596, 0.859}{la }\textcolor[rgb]{0.882, 0.071, 0.188}{respirar}\textcolor[rgb]{0.204, 0.596, 0.859}{on} }\\
%     \end{tabular}
%     \end{minipage}
%     \caption{\textbf{Decoding of representative sentences across subjects for MEG data.} \\
%     \textbf{(Left)}. Selection of the best, median and worst subjects for two representatives sentences. 
%     \textbf{(Right)}. Median performance across sentences and subjects for MEG data. 
%     Correct characters are highlighted in \textcolor[rgb]{0.204, 0.596, 0.859}{blue}, mistakes in \textcolor[rgb]{0.882, 0.071, 0.188}{red}, and typing errors \underline{underlined}. 
%     }
%     \label{tab:decoded_sentences_per_subject}
% \end{table*}



% \begin{table*}
%     \centering
%     \begin{minipage}{0.46\textwidth}
%         \centering
%         \begin{tabular}{p{0.12\textwidth} p{0.88\textwidth}}
%             \textbf{Read:} & las teorias reducen los numeros \\
%             \textbf{Best:} & \textcolor[rgb]{0.204, 0.596, 0.859}{las} \textcolor[rgb]{0.882, 0.071, 0.188}{pr}\textcolor[rgb]{0.204, 0.596, 0.859}{orias reducen los numeros} \\
%             \textbf{Median:} & \textcolor[rgb]{0.204, 0.596, 0.859}{las teorias }\textcolor[rgb]{0.882, 0.071, 0.188}{exig}\textcolor[rgb]{0.204, 0.596, 0.859}{en los }\textcolor[rgb]{0.882, 0.071, 0.188}{homb}\textcolor[rgb]{0.204, 0.596, 0.859}{ros} \\
%             \textbf{Worst:} & \textcolor[rgb]{0.204, 0.596, 0.859}{las} \textcolor[rgb]{0.882, 0.071, 0.188}{ranc}\textcolor[rgb]{0.204, 0.596, 0.859}{ias re}\textcolor[rgb]{0.882, 0.071, 0.188}{vis}\textcolor[rgb]{0.204, 0.596, 0.859}{en los numer}\textcolor[rgb]{0.882, 0.071, 0.188}{ad} \\
%             \addlinespace
%         \end{tabular}
%     \end{minipage}
%     \hfill
%     \begin{minipage}{0.50\textwidth}
%         \centering
%         \begin{tabular}{p{0.1\textwidth} p{0.9\textwidth}}
%             \textbf{Read:} & el beneficio supera los riesgos \\
%             \textbf{Type:} & e\underline{k} benef\underline{u}c\underline{ui} s\underline{yo}era \underline{kis} r\underline{u}esg\underline{i}s \\
%             \textbf{Pred:} & \textcolor[rgb]{0.204, 0.596, 0.859}{el beneficio supera los riesgos} \\
%             \addlinespace
%         \end{tabular}
%     \end{minipage}
%     \caption{\textbf{(Left) One sentence across subject.} 
%     \textbf{(Right) Example showing Model Robustness} \\
%     Correct characters are highlighted in \textcolor[rgb]{0.204, 0.596, 0.859}{blue}, mistakes in \textcolor[rgb]{0.882, 0.071, 0.188}{red}, and typing errors \underline{underlined}. 
%     \textbf{(Left)} Best decoded sentences. \textbf{(Right)} Median decoded sentences.
%     % \textbf{(Right)} Step-by-step decoding using CNN, CNN+Transformer, and CNN+Transformer+Ngram.
%     }
%     \label{tab:per_subject_interesting examples}
% \end{table*}



\subsection{Analyses of Decoded Sentences}
The CER of all sentences for three representative participants recorded with MEG along with two example sentences from these subjects are displayed in Fig. \ref{fig:performance_sentence}. 
% (CERs of 0.28, 0.31 and 0.32). %In this example, we present the decoded sentences for a single trial per subject. 
More decoding examples show that several sentences can be perfectly decoded for MEG (Tab. \ref{tab:combined_decoded_sentences}, right). Interestingly, some of these examples show that Brain2Qwerty's language model can correct the typographical errors of the participant. For example, \textsc{el beneficio supera los riesgos} was perfectly decoded, even though the participants typed: \textsc{ek benefucui syoera kis ruesgis}. In comparison, the poor EEG decoding (Tab. \ref{tab:combined_decoded_sentences}, left) rarely leads to comprehensible text. % respectively CERs of 0.32, 0.42 and 0.43 underscoring again the superior quality of results obtained with MEG.
%
%Shorter words, such as determiners, are consistently well-predicted. However, decoding longer words proves more challenging, especially when the output of the Transformer module significantly deviates from the intended word. In such cases, the N-gram model tends to predict a word that is far from the correct one. This highlights a trade-off inherent to the N-gram model: while it performs well when the Transformer provides reasonably accurate predictions, its performance deteriorates when the input is highly erroneous.
%
%We presents two cherry-picked 
% Other representative examples of sentences decoded with each ablation of our model are displayed in \cref{tab:step_by_step_decoded_sentences}. 
%In the first example, the CER starts at 0.52 after the CNN module, decreases to 0.16 after the Transformer, and ultimately reaches 0.0 after the final N-gram module. Similarly, in the second example, the CER begins at 0.34 after the CNN, reduces to 0.16 following the Transformer, and is perfectly decoded by the end of the pipeline.
Consistent with the statistical effects reported earlier, the examples in Tab. \ref{tab:step_by_step_decoded_sentences} highlight the impact of each module of our model, which together leads to perfect decoding after the language model. %This last step is particularly striking, as it can reconstruct complex words like \textit{teorías} from highly erroneous intermediate predictions such as \textit{geoolas}. Overall, these results showcase the feasibility of leveraging context and prior linguistic knowledge to improve brain-to-text decoding.

\begin{table*}
    \centering
    \begin{minipage}{0.50\textwidth}
        \centering
        \textbf{EEG}\\\vspace{.2cm}
        \begin{tabular}{p{0.12\textwidth} p{0.88\textwidth}}
    \textbf{Read:} & \typing{la ciencia de la idea rompe la vision} \\
    \textbf{Typed:} & \typing{la ciencia de la idea rompe la \underline{b}ision} \\
    \textbf{Decode:} & \typing{\textcolor[rgb]{0.204, 0.596, 0.859}{la ciencia de la idea} \textcolor[rgb]{0.882, 0.071, 0.188}{las mas de es}\textcolor[rgb]{0.204, 0.596, 0.859}{o}\textcolor[rgb]{0.882, 0.071, 0.188}{s}}\\
    
    \addlinespace
    
    \textbf{Read:} & \typing{el procesador ejecuta la instruccion} \\
    \textbf{Typed:} & \typing{\underline{orden}ador ejecuta la instruccion} \\
    \textbf{Decode:} & \typing{\textcolor[rgb]{0.882, 0.071, 0.188}{las c}\textcolor[rgb]{0.204, 0.596, 0.859}{o}\textcolor[rgb]{0.882, 0.071, 0.188}{rrida peri}\textcolor[rgb]{0.204, 0.596, 0.859}{ta la instruccion}} \\
    
    \addlinespace
    
    \textbf{Read:} & \typing{la presencia de los tipos impone los retos} \\
    \textbf{Type:} & \typing{la presencia de los tipos impone los retos} \\
    \textbf{Decode:} & \typing{\textcolor[rgb]{0.204, 0.596, 0.859}{la }\textcolor[rgb]{0.882, 0.071, 0.188}{declarad}\textcolor[rgb]{0.204, 0.596, 0.859}{a de los }\textcolor[rgb]{0.882, 0.071, 0.188}{cel}\textcolor[rgb]{0.204, 0.596, 0.859}{os }\textcolor[rgb]{0.882, 0.071, 0.188}{eran a }\textcolor[rgb]{0.204, 0.596, 0.859}{los }\textcolor[rgb]{0.882, 0.071, 0.188}{ac}\textcolor[rgb]{0.204, 0.596, 0.859}{tos}} \\
    
    \addlinespace
    
    \end{tabular}
    \end{minipage}
    \hfill
    \begin{minipage}{0.46\textwidth}
        \centering
        \textbf{MEG}\\\vspace{.2cm}
        \begin{tabular}{p{0.12\textwidth} p{0.88\textwidth}}            
            \textbf{Read:} & \typing{la silla ocasiona las lesiones }\\
            \textbf{Type:} & \typing{la silla ocasio\underline{m}a las lesio\underline{m}es }\\
            \textbf{Decode:} & \typing{\textcolor[rgb]{0.204, 0.596, 0.859}{la silla ocasiona las lesiones} }\\
            
            \addlinespace
            
            \textbf{Read:} & \typing{las teorias reducen los numeros }\\
            \textbf{Type:} & \typing{las teorias reducen los numeros }\\
            \textbf{Decode:} & \typing{\textcolor[rgb]{0.204, 0.596, 0.859}{las teorias reducen los numeros} }\\
            
            \addlinespace

            \textbf{Read:} & \typing{el beneficio supera los riesgos }\\
            \textbf{Type:} & \typing{e\underline{k} benef\underline{u}c\underline{ui} s\underline{yo}era \underline{ki}s r\underline{u}esg\underline{i}s }\\
            \textbf{Decode:} & \typing{\textcolor[rgb]{0.204, 0.596, 0.859}{el beneficio supera los riesgos} }\\
            \addlinespace
        \end{tabular}
    \end{minipage}
    \caption{\textbf{Examples of best-decoded sentences across subjects for EEG (left) and MEG (right) data.} \\
    Correct characters are highlighted in \textcolor[rgb]{0.204, 0.596, 0.859}{blue}, mistakes in \textcolor[rgb]{0.882, 0.071, 0.188}{red}, and typing errors \underline{underlined}. Note that correct vs incorrect spaces are not visualized here.\\
    % \textbf{(Right)} Step-by-step decoding using CNN, CNN+Transformer, and CNN+Transformer+Ngram.
    }
    \label{tab:combined_decoded_sentences}
\end{table*}

\begin{table*}
    \centering
    \begin{minipage}{0.45\textwidth}
        \centering
        \begin{tabular}{p{0.05\textwidth} p{0.85\textwidth}}
            \textbf{Read:} & \hspace{1.5cm} \typing{las teorias reducen los numeros} \\
            \textbf{Typed:} & \hspace{1.5cm} \typing{las teorias reducen los numeros} \\
            \textbf{Conv:} & \hspace{1.5cm} \typing{\textcolor[rgb]{0.204, 0.596, 0.859}{las t}\textcolor[rgb]{0.882, 0.071, 0.188}{angp}\textcolor[rgb]{0.204, 0.596, 0.859}{as re u}\textcolor[rgb]{0.882, 0.071, 0.188}{dn}\textcolor[rgb]{0.204, 0.596, 0.859}{n}\textcolor[rgb]{0.882, 0.071, 0.188}{d}\textcolor[rgb]{0.204, 0.596, 0.859}{l s}\textcolor[rgb]{0.882, 0.071, 0.188}{lindiis}} \\
            \textbf{Conv+Trans:} & \hspace{1.5cm} \typing{\textcolor[rgb]{0.204, 0.596, 0.859}{las} \textcolor[rgb]{0.882, 0.071, 0.188}{g}\textcolor[rgb]{0.204, 0.596, 0.859}{eo}\textcolor[rgb]{0.882, 0.071, 0.188}{ol}\textcolor[rgb]{0.204, 0.596, 0.859}{as red}\textcolor[rgb]{0.882, 0.071, 0.188}{i}\textcolor[rgb]{0.204, 0.596, 0.859}{cen los nume}\textcolor[rgb]{0.882, 0.071, 0.188}{i}\textcolor[rgb]{0.204, 0.596, 0.859}{os}}\\
            \textbf{Brain2Qwerty:} & \typing{\hspace{1.5cm} \textcolor[rgb]{0.204, 0.596, 0.859}{las teorias reducen los numeros}}\\
    \end{tabular}
    \end{minipage}
    \hfill
    \begin{minipage}{0.45\textwidth}
        \centering
        \begin{tabular}{p{0.05\textwidth} p{0.85\textwidth}}
            \textbf{Read:} & \hspace{1.5cm} \typing{el beneficio supera los riesgos} \\
            \textbf{Typed:} & \hspace{1.5cm} \typing{el be\underline{m}eficio supera los ries\underline{f}os} \\
            \textbf{Conv:} & \hspace{1.5cm} \typing{\textcolor[rgb]{0.204, 0.596, 0.859}{el} \textcolor[rgb]{0.882, 0.071, 0.188}{g}\textcolor[rgb]{0.204, 0.596, 0.859}{e}\textcolor[rgb]{0.882, 0.071, 0.188}{f}\textcolor[rgb]{0.204, 0.596, 0.859}{e}\textcolor[rgb]{0.882, 0.071, 0.188}{d}\textcolor[rgb]{0.204, 0.596, 0.859}{i}\textcolor[rgb]{0.882, 0.071, 0.188}{s}\textcolor[rgb]{0.204, 0.596, 0.859}{io su}\textcolor[rgb]{0.882, 0.071, 0.188}{i}\textcolor[rgb]{0.204, 0.596, 0.859}{era} \textcolor[rgb]{0.882, 0.071, 0.188}{n}\textcolor[rgb]{0.204, 0.596, 0.859}{o}\textcolor[rgb]{0.882, 0.071, 0.188}{a} \textcolor[rgb]{0.204, 0.596, 0.859}{ries}\textcolor[rgb]{0.882, 0.071, 0.188}{tii} }\\
            \textbf{Conv+Trans:} & \hspace{1.5cm} \typing{\textcolor[rgb]{0.204, 0.596, 0.859}{el} \textcolor[rgb]{0.882, 0.071, 0.188}{g}\textcolor[rgb]{0.204, 0.596, 0.859}{enefic}\textcolor[rgb]{0.882, 0.071, 0.188}{on} \textcolor[rgb]{0.882, 0.071, 0.188}{c}\textcolor[rgb]{0.204, 0.596, 0.859}{upera los riesgo}\textcolor[rgb]{0.882, 0.071, 0.188}{o} }\\
            \textbf{Brain2Qwerty:} & \hspace{1.5cm} \typing{\textcolor[rgb]{0.204, 0.596, 0.859}{el beneficio supera los riesgos} }\\
            \addlinespace
        \end{tabular}
    \end{minipage}
    \caption{\textbf{Example of decoded sentences across ablations for MEG data.} 
    Color coding identical to Tab. \ref{tab:combined_decoded_sentences}.
    %Correct characters are highlighted in \textcolor[rgb]{0.204, 0.596, 0.859}{blue}, mistakes in \textcolor[rgb]{0.882, 0.071, 0.188}{red}, and typing errors are \underline{underlined}. 
    \\
    % \textbf{(Right)} Step-by-step decoding using CNN, CNN+Transformer, and CNN+Transformer+Ngram.
    }
    \label{tab:step_by_step_decoded_sentences}
\end{table*}

\subsection{Impact of Word Type and Frequency}

%In this section, we analyze the factors impacting the performance of our model. %Each analysis is conducted on the test set of each participant and averaged across participants. Results are reported in \cref{fig:model_analysis}.

To test whether Brain2Qwerty decodes words irrespectively of their grammatical type, we evaluated the CER for each part-of-speech (POS) categories separately (Fig. \ref{fig:model_analysis}A). All POS categories are significantly better decoded than chance, with determiners exhibiting a remarkably low CER (17$\pm$1.9\%). This phenomenon may be due to two factors: their short length and their high frequency. %However, WER is relatively stable across all categories, as long words benefit more from the language model correction step.
% 
To formally test this hypothesis, we first analyzed the impact of word frequency on CER (Fig. \ref{fig:model_analysis}B). The results confirm that frequent words are better decoded than rare words (p=$10^{-7}$). Interestingly, we verify that the words absent from the training set (out-of-vocabulary, OOV) can also be decoded, although with a relatively poor CER (68$\pm$2.1\%). Note that this may be due to the fact that on a random partition of train/validation/test splits, OOV words tend to be rare words.

Second, we evaluated whether the frequency of each character also impacts decoding. The results show a significant correlation between character frequency and decoding accuracy: R=0.85, p < $10^{-8}$ (Fig. \ref{fig:model_analysis}C). Rare characters, such as "z," "k," and "w" in Spanish, are not decoded above chance level but only account for 0.08\%, 0.08\%, and 0.05\% of the characters in our sentences. These results suggest that the number of repetitions (of words and characters) encountered during training directly affects performance. 

To confirm this, we explore how decoding performance scales with the amount of data. For this, we re-trained our model on uniformly sampled subsets of the training set (Fig. \ref{fig:model_analysis}D). Our results show that CER decreases as a function of the amount of training data: R=0.93, p<$10^{-7}$.

\input{figures/fig4.tex}


\input{figures/fig5.tex}

\subsection{Impact of Keyboard Layout}
% hypothesis
If Brain2Qwerty relies on brain activity from the motor cortex (as opposed to some amodal representations of language), then we expect its decoding errors to relate to the specific layout of the QWERTY keyboard.
% supervised
To test this hypothesis, we evaluated the confusion patterns of incorrectly predicted characters by analyzing the keyboard distance between decoded and actual key presses. %Specifically, for each key, we identify all instances of misclassifications, categorize them based on their corresponding keyboard distances (defined as the minimum number of steps required to transition from one key to another). We then normalize across the keyboard distances. 
The results show a strong Pearson correlation between physical distance and confusion rate: R=0.73, p=0.02 (Fig. \ref{fig:motor_analysis}A). 
%This finding is intuitive, as participants are more likely to make errors with keys located near one another on the keyboard. 
% clustering
To complement this analysis, we further perform a clustering of the last-layer embeddings of the convolutional module using scikit-learn's K-means clustering algorithm. %  straightforward algorithm like K-means. 
When trained on two clusters, this unsupervised model fully separates the left-hand and right-hand keys. When using up to 10 clusters, the resulting partition remains consistent with the keyboard layout (Fig. \ref{fig:motor_analysis}B). This shows that the spatial layout of the keyboard is well encoded in the high-dimensional representations learned by our model.
% conclusion
Overall, these results show that the decoder's mistakes tend to be confused with the keys that are physically close to the target letter on the QWERTY keyboard, suggesting that the decoder primarily relies on motor representations.

\subsection{Impact of Typing Errors}
Our protocol does not allow participants to correct their mistakes. Typing errors account for 3.9\% of the keystrokes and are present in 65\% of the sentences. Furthermore, they are associated with a specific behavior (Fig. \ref{fig:motor_analysis}C): the time taken to type a character -- as measured by the inter-key interval -- doubles between correctly- (50$\pm$7\,ms) and incorrectly-typed characters (114$\pm$12\,ms, p=$10^{-7}$). This well-known phenomenon \citep{logan2010cognitive} likely reflects hesitation or monitoring of mistakes.  
%For sentences containing at least one typing mistake, the average number of typos per sentence is 2.8. 
To evaluate the impact of typing error on decoding performance, we separately evaluated CER for correctly- and incorrectly-typed characters (Fig. \ref{fig:motor_analysis}D). %Typing mistakes can be categorized as either additions or substitutions, both types of errors are considered. %To align the true sentence with the typed sentence, we use the \textit{SequenceMatcher} from the \texttt{difflib} Python library.
%
%Typing mistakes are significantly more frequent at the ends of sentences (p = $10^{-6}$) as shown in panel A. 
%
% 
The results show that with our Conv+Trans model, correctly-typed characters lead to a better CER (38\%) than incorrectly-typed characters (65\%, p=$10^{-7}$). This result, however, may be partly driven by the contextualization enabled by the transformer. 
To minimize the impact of sentence context on this error analysis, we thus evaluate the performance of the Convolutional Module (Fig. \ref{fig:motor_analysis}D, right). Again, correctly-typed characters leads to a better CER (52\%) than incorrectly-typed characters (71\%, p=$10^{-7}$). This result suggests that decoding performance diminishes when motor processes are inaccurately executed.

% In Panel B, we define the sum of keypress intervals as the combined duration between the current keystroke and the adjacent ones (i.e., the preceding and following keystrokes), which serves as an indicator of participant hesitation. The analysis reveals that the keypress intervals for correct characters are significantly shorter than those for typing errors, with the mean hesitation time for errors being three times greater. This illustrates the increased cognitive load associated with typing mistakes.  

% A key observation is the novel finding that errors are more frequent at the ends of sentences. This pattern may be attributed to the experimental protocol, where sentences are presented one word at a time, potentially increasing the cognitive load required to retain sentence endings. Nevertheless, this pattern remains consistent across sentences of varying lengths. The significantly higher decoding accuracy for correctly typed characters, even in the CNN-only model, highlights the distinct representations of accurate typing compared to errors. This may be explained by the hesitation or realization of mistakes by participants during typing, which complicates the decoding process for errors.  

% \subsection{Model Embeddings Analysis via Linear Probing.}
% \begin{itemize}
%     \item This analysis is presented in Figure 6.
%     \item \textbf{Purpose:} To determine the optimal model depth for minimizing character-error-rate (CER) and to examine the richness of the model's embeddings and optimization strategies.
%     \item \textbf{Methods:}
%         \begin{itemize}
%             \item \textbf{Panel A:} We trained 12 models of varying depths and assessed their performance on the test set.
%             \item \textbf{Panels B and D:} Last-layer embeddings were extracted and treated as fixed representations. These embeddings were then used as input features for a binary classifier tasked with addressing two questions: (B) Is the character a typo? (D) Will the character be misclassified by the model?
%             \item \textbf{Panel C:} Eight convolutional models with different depths were trained, and their embeddings were used as input features for a linear classifier to predict seven distinct attributes. This analysis investigated how representations evolve across layers. The transformer was excluded due to its implicit knowledge of sentence length and positional encoding.
%         \end{itemize}
%     \item \textbf{Results:}
%         \begin{itemize}
%             \item \textbf{Panel A:} Performance improved with increasing model depth, reaching the lowest CER at transformer depth 3. Beyond this depth, performance began to decline.
%             \item \textbf{Panel B:} Using last-layer embeddings, a linear model could decode typos at above-chance levels.
%             \item \textbf{Panel C:} Correlation with sentence position and sentence length initially increased, peaking at layer 2, and subsequently decreased.
%             \item \textbf{Panel C:} For character decoding, the correlation with the correct character increased consistently across layers, even when considering previous-2 and next+2 character positions.
%             \item \textbf{Panel D:} Using last-layer embeddings, a linear model could predict above chance whether a character would be misclassified by the model.
%         \end{itemize}
%     \item \textbf{Observations:}
%         \begin{itemize}
%             \item \textbf{Panel A:} This panel demonstrates how each additional layer contributes to performance. 
%             \item \textbf{Panel A:} While this analysis uses separate models for each depth, another experiment was conducted on the full model, extracting embeddings at each layer during training. This revealed a periodic pattern repeating every three layers. Understanding the optimization strategies underlying this periodicity is challenging.
%             \item \textbf{Panels B and D:} These panels underscore the richness of the model's representations. Last-layer embeddings contain sufficient information to identify whether a character is a typo or will be misclassified.
%             \item \textbf{Panel C:} Low-level features such as sentence position and length diminish with increasing depth, whereas high-level features such as character representations become more salient.
%             \item \textbf{Panel C:} The correlation with key duration increases across layers, contrary to the expected pattern typically associated with low-level features.
%             \item \textbf{Panel C:} The correlation with the correct character increases across layers, even for previous-2 and next+2 character positions. This likely reflects the effect of the 0.5s temporal window, where the average keystroke duration of 0.1s results in overlapping representations of adjacent characters. Interestingly, representations of non-central characters do not diminish over time as anticipated. This suggests that further analysis with varying temporal window could yield deeper insights.
%         \end{itemize}
% \end{itemize}

\section{Discussion}
% Summary / Contribution
The present study introduces a new method to decode the production of sentences from non-invasive brain recordings. With MEG, our Brain2Qwerty model achieves a character-error-rate (CER) of 32$\pm$0.6\% on average across subjects, with the best-performing participants reaching a CER as low as 19\%.
Our analyses indicate that this decoding benefits from two main factors. First, the use of MEG signals instead of EEG signals resulted in a two-fold improvement. Second, our deep learning architecture, combined with a pretrained character-level language model, substantially outperforms standard models.


%Projecting the embeddings from the last layer onto a QWERTY keyboard, as shown in \ref{fig:typing_analysis}, reveals clear patterns, suggesting that a significant portion of the model's representations can be attributed to hand movements. 
%It is not clear whether the analysis of typographical mistakes in \ref{fig:typo_analysis} could be explained by hesitation in hand movements during errors or may indicate more complex encoding patterns, such as the participant's underlying intentions.

% Comparison to current non-invasive decoding
This work directly stems from the recent progress in decoding natural language from non-invasive recordings of brain activity. In particular, \citet{defossez2023decoding} showed that the perception of natural speech segments could be decoded, from MEG signals, with up to 41\% top-10 accuracy (chance level=0.1\%). Similarly, \citet{tang2023semantic} showed that the meaning of perceived sentences could be decoded from functional Magnetic Resonance Imaging (fMRI). Our Brain2Qwerty model shares several elements with these approaches, notably through the use of both a subject layer \citep{defossez2023decoding} and the use of a language model \citep{tang2023semantic}, although here restricted to a pretrained 9-gram character-level model. However, these two studies, which focus on decoding the \emph{perception} of language rather than its \emph{production}, remained limited in their downstream clinical applications.

% Recently, \cite{sivakumar2024emgqwerty} introduced a method for decoding EMG signals during a typing task. The signals were recorded directly from the wrist, comprising 1,135 sessions, 108 participants, and a total of 346 hours of data. EMG signals are less noisy than MEG signals due to their direct recording from the wrist, which reduces artifacts, and their distinctive waveform characterized by clear spikes. By fine-tuning the model on individual participants, the method achieved an impressive error rate of just 7\%. While EMG signals appear highly suitable for decoding, they are inapplicable to individuals with severe motor impairments. 

Studies that directly decode text production from non-invasive recordings remain rare. For example, \citet{crell2024decoding} employed EEG to decode only 10 letters and achieved a character-error-rate of 75.8\%, substantially higher than our 68\% in the EEG setting with 29 characters. Similarly, EEG-based BCI benchmarks currently emphasize the constraints of poor signal quality and the variability across subjects \citep{chevallier2024}. Our EEG findings are consistent with these observations. Beyond its mere metric performance, our approach is more efficient than traditional protocols used in non-invasive BCIs, such as the P300-speller \citep{marchetti2014effectiveness}, SSVEP \citep{cheng2002design}, and fMRI-localizer \citep{owen2006detecting}. These methods have historically relied on handcrafted signal processing and shallow classifiers. In contrast,  our approach is based on a task that is comparatively easier to use.

% Comparison to invasive methods
While the decoding performance of Brain2Qwerty narrows the gap between non-invasive and invasive BCIs, this gap remains significant. In particular, for speech decoding, \citet{metzger2023neuroprosthesis} achieved a rate of 79 words per minute and reported a CER of 15.2\% on a dataset with 372 unique words, a vocabulary size comparable to ours.
\citet{willett2021handwriting} demonstrated typing speeds of 90 characters per minute with a CER below 6\% and an offline CER under 1\% when using a correction model. Both approaches rely on intracortical setups and require extensive recording sessions (11 hours per participant for \citet{willett2021handwriting}). Consequently an important research avenue will be to scale and adapt those tasks for MEG experiments. 

% Limits
Several challenges remain to be solved before the present method could be adapted to clinical applications. 
% real time
First, our model does not operate in real time. In particular, the transformer and language model here operate at the sentence level and thus require the trial to conclude before an output can be produced. 
In addition, the input of Brain2Qwerty requires the MEG segments to be aligned to keystrokes. Overall, a real-time architecture, akin to what is done with electromyography \citep{sivakumar2024emgqwerty} and speech recognition \citep{defossez2023decoding}, remains necessary to make the present proof-of-concept applicable in real time.  
% Second, due to the presence of subject-specific layers, our model cannot generalize to unseen subjects without some form of fine-tuning. 
% Eliminating the dependency on sentence-level correction, keystroke onsets and subject-specific layers are important endeavors towards real-time decoding. 
% patients

Second, our study was conducted exclusively with healthy participants and with a strictly supervised model. Training indeed requires knowing both the timing and identity of each character. While this setup may be suitable for patients with neurodegenerative conditions who may still possess  motor abilities, it is not applicable to locked-in individuals, who are completely unable to perform a typing task on a keyboard. Addressing this challenge may involve either adapting our typing task into an imagination task or designing AI systems capable of robust generalization across participants \citep{scotti2024mindeye2}.

Finally, while MEG outperforms EEG, current MEG systems, including the one used in the present study, are not wearable. This, however, may be resolved by the development of new MEG sensors based on optically pumped magnetometers (OPMs)~\citep{shah2013compact,schofield2022quantum,brickwedde2024applications}.

% Opening
% Finally, while our study demonstrates the feasibility of decoding typing, it does not offer a clear understanding of the underlying neural mechanisms driving these outcomes. This aspect is addressed in greater detail in a companion paper. 

Overall, the present results serve as a stepping stone toward developing safer and more accessible non-invasive brain-computer interfaces, ultimately enabling solutions for individuals who have partially or completely lost the ability to communicate.

\section{Methods} \label{section:decoder}
We aim to decode language production from non-invasive brain recordings. To achieve this, participants typed sentences on a keyboard while their brain activity was recorded using EEG or MEG. These two devices measure neural activity at a millisecond level, with EEG capturing electric fields and MEG detecting magnetic fields that are both generated by cortical neurons and recorded from sensors distributed across the scalp.

\subsection{Experimental Protocol}
\paragraph{Cohort. }
% We present a new dataset collected at the Basque Center on Cognition, Brain and Language (BCBL). Table~\ref{tab:dataset} summarizes the key characteristics of the dataset after preprocessing. 
We recruited 35 healthly adult volunteers to participate in our study at the Basque Center on Cognition, Brain and Language (BCBL) in Spain. This group was composed of 23\% of men and 77\% of women with an average age of 31.6$\pm$5.2 years. All participants were right-handed and skilled at typing. Participants had to type words they were hearing on a keyboard covered by a cardboard box. They were selected if their typing accuracy was above or equal to 80\%. They are all native Spanish speakers with no declared prior history of neurological or psychiatric disorders. Their brain activity was recorded with either EEG or MEG for 0.88$\pm$0.02 and 0.93$\pm$0.01 hours respectively, amounting to a total of 17.7 and 21.5 hours of typing. Five participants took part in both EEG and MEG sessions. One participant was excluded from the MEG study due to the presence of a metallic component during the recording. Participants gave their informed consent and were compensated 12 euros per hour for their participation. This study was approved by the local ethics committee. The same dataset is used to investigate the underlying neural mechanisms driving this task in a companion paper \citep{lucy2025}. 

\paragraph{Devices. }
% MEG system
The MEG system is a Megin system with 306 channels (102 magneto-meters and 204 planar gradiometers) recording at a sampling rate of 1\,kHz, with an online high-pass filter set at 0.1\,Hz and a low-pass filter at 330\,Hz.
% EEG system
The EEG system is an actiCAP slim from BrainVision\footnote{\url{https://brainvision.com/products/acticap-slim-acticap-snap/}}, with 64 channels (61 EEG channels and 3 ocular channels), a BRAINAMP DC amplifier, and sampled at 1\,kHz with an online high-pass filter set at 0.02\,Hz.

% The EEG system is composed of with 61 wet-electrodes, sampled at 1\,kHz with an online high-pass filter set at 0.02\,Hz. 
% Keyboard
Standard keyboards contain electronic and metallic parts that generate artifacts in the MEG. Consequently, we used a custom Magnetic-Resonance-compatible QWERTY keyboard from HybridMojo (LLC), and further modified it to replace the standard metallic springs with non-ferromagnetic silver-spring mechanisms.

\paragraph{Task. }
Participants were seated in front of a projected screen (100\,cm for MEG and 70\,cm for EEG away from their eyes), and with our custom keyboard placed on a stable platform. The distance between M/EEG sensors and the keyboard was 70\,cm.
% Participants were seated in front of a projected screen with our custom keyboard placed on a stable platform. 
This setup ensured participants could type in a natural position. Each trial consisted of three steps: read, wait, type. 
%
First, a sentence was presented on the screen, with a rapid serial visual presentation protocol (RSVP; i.e. one word at a time). Each word was presented in a black font, in all upper-case, on a 50\% gray background for a random duration between 465 and 665\,ms without intervals between words.
%
Second, after the disappearance of the last word of each sentence, a black fixation cross was displayed on the screen for 1.5 seconds. 
%
Third, the disappearance of the fixation cross signaled the start of the typing phase. No letters were presented on the screen during typing. Nevertheless, we added minimal visual feedback: a small black square at the center of the screen rotated clockwise by 10 degrees on every keystroke. This feedback ensured that eye movements were not correlated with linguistic features, as it is usually the case in left-to-right reading.
%
Each session consisted of two blocks of 64 sentences each. The first four sentences of each session were training sentences and were different from the 128 unique sentences in the protocol. During the first two training sentences, participants received visual feedback while typing. The other two sentences were used to train them on the task (typing with minimal visual feedback).

\paragraph{Instruction and stimuli. }
% instruction
Participants were instructed to type the sentence that was presented in RSVP as accurately as possible without using backspaces to correct errors and while fixating on the center of the screen. To avoid using diacritic marks that occur in Spanish (e.g., é, á, í, ó, ú, ü, and ñ), all words were presented in upper case, and participants were instructed to think of writing in upper case, and without accents. Participants pressed the return key at the end of the trial.
% sentences
Each session consisted of 128 unique Spanish sentences. All sentences were declarative Spanish sentences that contained between 5 and 8 words. They consisted of determiners, nouns, adjectives, prepositions and verbs. This led to a total of 4K sentences and 146K characters across participants for EEG, and 5.1K sentences and 193K characters for MEG. 
% The use of the backspace key was not allowed, preventing participants from correcting typographical errors. As a result, the typed sentences could contain mistakes. This setup enabled two definitions of a "correct" sentence: either the intended Spanish sentence that participants were instructed to type or the actual typed sentence, which could include errors.

\paragraph{MEG and EEG preprocessing. }
In our pipeline, the EEG and MEG recordings were bandpass filtered between 0.1 and 20\,Hz, and resampled to 50\,Hz, using MNE-Python's default parameters~\citep{gramfort2014mne}. These continuous recordings were then segmented into 0.5\,s time windows around each key press from -0.2\,s to +0.3\,s. We applied baseline correction by subtracting the average channel-wise value in the (-0.2, 0) interval from each window. To ensure that the data was on the same scale, a \texttt{RobustScaler} was applied across time from \texttt{scikit-learn} \citep{pedregosa2011scikit} followed by a clamping operation. The scaler removes the median and scales the data according to the interquantile range.

\paragraph{Text preprocessing. }
We removed sentences that contained strictly more than 10 typographical errors (less than 5\% for both EEG and MEG).
To determine whether the participants performed typographical errors, we first used the \texttt{SequenceMatcher} from \texttt{difflib }\footnote{https://docs.python.org/3/library/difflib.html} to align the typed sentence to the original sentence using a Gestalt pattern matching algorithm \citep{Ratcliff1988} that determines the minimal number of character-edits. This approach yields, for each keystroke event, two variables: the key pressed and the key that should have been pressed (target). Unless stated otherwise, all analyses are based on the target key. 
On average, participants typed 152.0$\pm$3.2 characters per minute, leading to an average sentence production time of 5.7$\pm$0.2\,s. 3.6 $\pm$ 0.7\% of the keystrokes were typographical errors.

\paragraph{Train/validation/test splits.}
To ensure that our AI model does not memorize sentences, we divided the unique sentences into train (80\%), validation (10\%) and test splits (10\%). As sentences may be similar (e.g. same sentence with an additional component), we adopted a maximally diverse splitting strategy by implementing a custom data splitter based on \texttt{sklearn}’s \texttt{AgglomerativeClustering} \citep{pedregosa2011scikit}. This method clusters unique sentences into groups of similar data points, where similarity is determined using Term Frequency-Inverse Document Frequency (TF-IDF) \citep{SparckJones1972} cosine similarity with a threshold of 0.5 (i.e., sentences with a similarity score above 0.5 are grouped in the same split). These clusters are then assigned to the training, validation and testing splits with relative ratios of 80/10/10. This approach prevents the model from artificially improving test performance by memorizing similar sentences across participants. 
Throughout the study, we presented our results using a fixed splitting seed, except in Fig. \ref{fig:performance_sentence} where we evaluated performance across sentences. For this figure, we aggregated results from multiple models with different splitting seeds to ensure broader sentence coverage, similar to a cross-validation approach.

\subsection{Decoder} \label{subsection:decoder}
The goal for our decoding model is to predict each keystroke based on 0.5\,s windows of M/EEG signals. Formally, the objective is to learn a mapping from brain recordings to class probabilities: $
f: \mathbb{R}^{s \times t} \rightarrow $[0,1]$^C
$, where \( s \) represents the number of M/EEG sensors, \( t \) denotes the number of M/EEG time samples in the window, and \( C \) is the number of available keys. We consider $C=29$ distinct classes, which include all the letters of the Latin alphabet as well as three special classes: one for space, another for numbers, and the last for all other special characters. 

% The model predicts \(\hat{Y}\), such that:

% \[
% \hat{Y} = f(X) \approx Y,
% \]

% where \( X \in \mathbb{R}^{c \times t} \) is the input and \( Y \in \mathbb{R}^C \) is the one-hot encoded ground truth. The cross-entropy loss is then used to minimize the difference between the predicted \(\hat{Y}\) and the true \(Y\), ensuring accurate classification.


\subsubsection{Architecture}
Our Brain2Qwerty model is composed of three successive modules (Fig. \ref{fig:approach}). 

\paragraph{Convolutional Module.}
The first building block is a modified convolutional model originally introduced by \cite{defossez2023decoding}. This model consists of four main parts. The first component employs a spatial attention mechanism to encode the relative positions of the sensors. The second introduces a subject-specific linear layer to account for differences between subjects. The third component is a convolutional neural network architecture, comprising 8 sequential blocks that employ a kernel size of 3 and a dilation period of 3, and incorporate skip connections, dropout regularization, and GELU activation functions. Finally, the temporal dimension is pooled with a single-head self-attention layer. Thus, for each window $\mathbf{X}\in \mathbb R^{s\times t}$, the CNN outputs $\mathbf{z}\in \mathbb R^{h}$, with $h=2,048$. For clarity, this module will hereafter be referred to as Convolutional Module (Conv).

\paragraph{Transformer Module.}
The outputs of the Convolutional Module are then input to a transformer Module (Trans). The receptive field of the transformer is restricted to a unique sentence: $\mathbf{Z}\in \mathbb R^{n\times h}$. The transformer is used to refine the keystroke predictions by exploiting contextual information and consists of 4 layers with 2 attention heads per layer, maintaining consistent input and output dimensions. Finally, a linear layer projects the transformer's outputs to obtain the logits of each character \(\mathbf{\tilde{Y}} \in  \mathbb{R}^{n\times C} \).


\paragraph{Language Model.}
Finally, the output of the transformer \(\mathbf{\tilde{Y}}\) is input to a language model, so as to leverage the statistical regularities of natural language. 
% pretraining
For this, we used a 9-gram character-level model constructed using the KenLM library \citep{KenLM} and pretrained on the Spanish Wikipedia Corpus \citep{wikidump}. This library optimizes both speed and memory efficiency by employing a prefix tree structure. 
%This design choice was inspired by the pipeline presented in \cite{sivakumar2024emgqwerty}, where EMG signals are used to decode typing. 
% Our goal is to develop a Spanish language-aware spelling mistake corrector that does not require extensive training or inference time, unlike character-level transformer models.
%
% inference
At inference time, the language model is input with a sequence of predicted characters, and causally predicts the most likely next character given the preceding predicted ones.

Formally, let \( i \in [1, 2, ..., n]\) denote the position of the character in a sequence of up to $n$ characters and $m=9$ be the order of the $m$-gram model. The probability of the next character \( \hat{c}_i \) given its preceding predictions \( \hat{c}_{i-m}, \ldots, \hat{c}_{i-1} \) is estimated as a weighted combination between the transformer's logits and the probabilities of the language model:

\[
\mathbf{P}(\hat{c}_i | \hat{c}_{i-m}, \ldots, \hat{c}_{i-1}) = \mathbf{P}_{\text{trans}}(\hat{c}_i) + \alpha \cdot \mathbf{P}_{\text{lm}}(\hat{c}_i | \hat{c}_{i-m}, \ldots, \hat{c}_{i-1}),
\]

where \( \mathbf P_{\text{trans}}(\hat{c}_i )= \log\left(\text{softmax} \left[(\mathbf{\tilde Y})\right]_i\right)\) represents the contribution from the core model, \( \mathbf P_{\text{lm}}(\hat{c}_i | \hat{c}_{i-m}, \ldots, \hat{c}_{i-1}) \) represents the probabilities from the language model, and \( \alpha \) is the language model weight. We use a beam search of size 30 and a language model weight of 5 found using a grid search, ensuring a tradeoff between inference time and decoding accuracy. This language modeling module outputs a sequence of $n$ characters that aim to regularize the predictions of the transformer with the statistics of natural language. Brain2Qwerty's final prediction is denoted as $\hat{\mathbf{Y}} \in \mathbb R^{n\times C}$. %Importantly, the language model does not alter the sequence length; it can only replace characters while preserving the original length.



\subsubsection{Training}
The Convolutional and Transformer modules are trained jointly with an unweighted cross-entropy loss in an end-to-end manner across all subjects, with the same hyperparameters for EEG and MEG recordings. This leads to a total of $\sim$ 400M parameters (258M for Conv, 138M for Trans). The model is trained for 100 epochs with a batch size of 128 using the AdamW optimizer \citep{loshchilov2019decoupledweightdecayregularization} with early stopping. We use the OneCycleLR scheduler \citep{smith2018superconvergencefasttrainingneural} (weight decay=$10^{-4}$; pct\_start=0.1), warming up the learning rate to $10^{-4}$ over the first 10 epochs then decaying linearly. Training was conducted on a single NVIDIA Tesla V100 Volta GPU with 32 GB of memory. The total runtime for training one model is $\sim$12 hours.

\subsubsection{Evaluation}

\paragraph{Hand Error Rate (HER).}
For analysis purposes and comparison with the classic BCI literature (e.g. \cite{lebedev2006brain}), we first consider HER. This metric estimates whether the target and the predicted characters correspond to the same left/right-hand split of the keyboard. Specifically, keys to the left of Y, H, and B are assigned to the left-hand category, while those to the right (including Y, H, and B) are assigned to the right-hand category. For this evaluation, special characters, numbers and space are excluded, as participants may use both hands to type them.

% \subsubsection{Character Accuracy}
% Character Accuracy evaluates the model's ability to correctly predict the typed character for each keystroke. For this metric, we use \textit{balanced accuracy} from the scikit-learn library \cite{pedregosa2011scikit}, which ensures fair performance evaluation across all 29 classes, regardless of class imbalance.


\paragraph{Character-error-rate (CER).}
CER is based on the Levenshtein distance which quantifies the minimum number of single-character edits required to transform the predicted sequence of keystrokes to the target sentence. 
A CER of 0 indicates perfect character-level accuracy. The formula for CER is given by \(\text{CER} = (s + d + a)\times\frac{1}{n}\), where \( s \), \( d \) and \( a \) stand for substitutions, deletions, and additions in the $n$-character sentence, respectively. %This metric provides a comprehensive measure of prediction errors, capturing all possible discrepancies between the predicted and reference text. 
Unless stated otherwise, we compute the CER at the sentence level using the $\texttt{Levenshtein}$\footnote{\url{https://pypi.org/project/Levenshtein/}} python library, and report the average CER across sentences.

% \subsubsection{Word Error Rate (WER)}
% WER is analogous to CER but operates at the word level. It measures the number of words in the predicted text that differ from those in the reference text, also utilizing the Levenshtein distance. \\

\subsubsection{Model comparison}

\paragraph{Statistics.}
For statistical comparisons, we employed the non-parametric tests provided by the \texttt{scipy} package~\citep{virtanen2020scipy}. For comparison across models within the same subjects, we used the Wilcoxon test. For comparison across subjects (e.g. for EEG versus MEG), we used the Mann Whitney U test. For time-course decoding, we further applied a false-discovery-rate (FDR) correction for multiple comparisons across time samples. 

\paragraph{Baseline Models.}
We consider two baseline models. 
% linear
The first one is a linear model implemented using the \texttt{RidgeClassifierCV} function from the scikit-learn library \citep{pedregosa2011scikit}, which was trained to predict characters from a single time sample of recording for each subject separately. The regularization parameter \( \alpha \) was selected through a nested cross-validation using a grid search logarithmic spanning from \( 10^{-2} \) to \( 10^8 \). This operation was repeated for each time sample between -0.5 and 0.5 seconds relative to the character onset. 

% EEGNet
As a second baseline, we need a model which uses the same setup as Brain2Qwerty, i.e. where all subjects are trained collectively and the temporal dimension is collapsed. We used EEGNet \citep{lawhern2018EEGNet}, a highly parameter-efficient model classically used in BCIs. We trained EEGNet with the same approach as our model. For our experiments, EEGNet is configured with a depth of 6 and a dropout rate of 0.3, which were selected based on a grid search over the validation set. EEGNet does not incorporate a subject-specific linear layer, which may compromise its performance in this particular setup where all subjects were trained collectively.
%
To compute the chance level while accounting for the imbalance across characters, we evaluated the performance of a dummy model that always predicts the most frequent character.

\paragraph{Companion paper.} We explore \emph{how} the brain produce a hierarchy of language representations in a companion paper \citep{lucy2025}. Note that Fig. \ref{fig:approach} (left) and Fig. \ref{fig:model_perf}B are shared between these two studies.

\section{Acknowledgments}
The authors would like to thank Maite Kaltzakorta, Manex Lete, Jessi Jacobsen, Daniel Nieto, Jone Iraeta, Araitz Garnika, Jaione Bengoetxea, Natalia Louleli, Naroa Miralles, Eñaut Zeberio, Craig Richter, Amets Esnal, and Olatz Andonegui, as well as Abhishek Charnalia and Pierre-Louis Xech for their critical help.
This research is supported by the Basque Government through the BERC 2022-2025 program and Funded by the Spanish State Research Agency through BCBL Severo Ochoa excellence accreditation CEX2020-001010/AEI/10.13039/501100011033.
Parts of this research were carried within the European Union's Horizon 2020 research and innovation programme under the Marie Skłodowska-Curie grant agreement No 945304 - Cofund AI4theSciences hosted by PSL University.

\clearpage
% \printbibliography  % biblatex not supported by template
\newpage
\bibliographystyle{assets/plainnat}
\bibliography{main}

% While MEG offers superior signal quality compared to EEG, its application in brain decoding remains underexplored \cite{dash2020decoding, csaky2023interpretable, ghazaryan2023trials}. 
%For example, \cite{yang2024decode} introduced the NeuSpeech model for MEG-to-text decoding. However, their evaluation was restricted to text seen during training, which does not meet the requirements for open-vocabulary translation. 


% Willet 2023: 9.1\% WER on 50 words vocabulary and 23.8\% on a 125K word vocabulary and 62 words per minute to text decoding with intracortical microelectrode arrays.

% Willet 2021 Handwriting - 90 characters per minute at 6\% error rate - intracortical

% Metzger 2022 - decoded sentences using
% words from a 1,152-word vocabulary at a median character-error-rate of 6.13%
% and speed of 29.4 characters per minute



% \appendix
% \section{Appendix}

% \begin{table*}[!htbp]
%     \centering
%     \vspace{-10cm}
%     \begin{tabular}{p{0.05\textwidth} p{0.95\textwidth}}
%     \textbf{Read:} & el centro describe las parabolas \\
%     \textbf{Typed:} & \underline{w}l centro describe las parabolas \\
%     \textbf{Pred:} & \textcolor[rgb]{0.882, 0.071, 0.188}{\underline{e}}\textcolor[rgb]{0.204, 0.596, 0.859}{l centro} \textcolor[rgb]{0.882, 0.071, 0.188}{trece de} \textcolor[rgb]{0.204, 0.596, 0.859}{las} \textcolor[rgb]{0.882, 0.071, 0.188}{carabin}\textcolor[rgb]{0.204, 0.596, 0.859}{as} \\
%     \addlinespace
%     \textbf{Read:} & los usos ofrecen las ventajas energeticas \\
%     \textbf{Typed:} & los usos ofrecen las ventajas energeticas \\
%     \textbf{Pred:} & \textcolor[rgb]{0.204, 0.596, 0.859}{los} \textcolor[rgb]{0.882, 0.071, 0.188}{para me}\textcolor[rgb]{0.204, 0.596, 0.859}{recen las venta}\textcolor[rgb]{0.882, 0.071, 0.188}{n}\textcolor[rgb]{0.204, 0.596, 0.859}{as e}\textcolor[rgb]{0.882, 0.071, 0.188}{structur}\textcolor[rgb]{0.204, 0.596, 0.859}{as} \\
%     \addlinespace
%     \textbf{Read:} & la explicacion aclara la pregunta de la evaluacion \\
%     \textbf{Typed:} & la explicacion aclara la pregunta de la evaluacion \\
%     \textbf{Pred:} & \textcolor[rgb]{0.204, 0.596, 0.859}{la explicacion}\textcolor[rgb]{0.882, 0.071, 0.188}{es para} \textcolor[rgb]{0.204, 0.596, 0.859}{la pre}\textcolor[rgb]{0.882, 0.071, 0.188}{sentan a} \textcolor[rgb]{0.204, 0.596, 0.859}{la }\textcolor[rgb]{0.882, 0.071, 0.188}{respirar}\textcolor[rgb]{0.204, 0.596, 0.859}{on} \\
%     \addlinespace
%     \end{tabular}
%     \caption{\textbf{Median performance decoded sentences.} 
%     Correct characters are highlighted in \textcolor[rgb]{0.204, 0.596, 0.859}{blue}. Mistakes are highlighted in \textcolor[rgb]{0.882, 0.071, 0.188}{red}. Typing errors are \underline{underlined}. The "Read" line shows the original reference, while "Typed" shows user input, and "Pred" shows the model prediction.
%     }
%     \label{tab:median_decoded_sentences}
% \end{table*}


% \begin{table*}[!htbp]
%     \centering
%     \vspace{-20cm}
%     \begin{minipage}{0.46\textwidth}
%         \centering
%         \begin{tabular}{p{0.2\textwidth} p{0.8\textwidth}}
%             \textbf{Read:} & la explicacion aclara la pregunta de la evaluacion \\
%             \textbf{Typed:} & explicacion aclara la pregunta de la evaluacu\underline{j}on \\
%             \textbf{Median 1:} & \textcolor[rgb]{0.882, 0.071, 0.188}{a a}\textcolor[rgb]{0.204, 0.596, 0.859}{plicacion }\textcolor[rgb]{0.882, 0.071, 0.188}{exist}\textcolor[rgb]{0.204, 0.596, 0.859}{a la pregunta de la}\textcolor[rgb]{0.882, 0.071, 0.188}{s c}\textcolor[rgb]{0.204, 0.596, 0.859}{a}\textcolor[rgb]{0.882, 0.071, 0.188}{s}\textcolor[rgb]{0.204, 0.596, 0.859}{u}\textcolor[rgb]{0.882, 0.071, 0.188}{ccip}\textcolor[rgb]{0.204, 0.596, 0.859}{on} \\
%             \addlinespace
%             \textbf{Median 2:} & \textcolor[rgb]{0.204, 0.596, 0.859}{la explicacion}\textcolor[rgb]{0.882, 0.071, 0.188}{es p}\textcolor[rgb]{0.204, 0.596, 0.859}{ara la pre}\textcolor[rgb]{0.882, 0.071, 0.188}{sen}\textcolor[rgb]{0.204, 0.596, 0.859}{ta}\textcolor[rgb]{0.882, 0.071, 0.188}{n a} \textcolor[rgb]{0.204, 0.596, 0.859}{la} \textcolor[rgb]{0.882, 0.071, 0.188}{respirar}\textcolor[rgb]{0.204, 0.596, 0.859}{on} \\
%             \addlinespace
%             \textbf{Median 3:} & \textcolor[rgb]{0.204, 0.596, 0.859}{la explicacion} \textcolor[rgb]{0.882, 0.071, 0.188}{esper}\textcolor[rgb]{0.204, 0.596, 0.859}{a}\textcolor[rgb]{0.882, 0.071, 0.188}{r} \textcolor[rgb]{0.204, 0.596, 0.859}{a} \textcolor[rgb]{0.882, 0.071, 0.188}{la line}\textcolor[rgb]{0.204, 0.596, 0.859}{a de la }\textcolor[rgb]{0.882, 0.071, 0.188}{desol}\textcolor[rgb]{0.204, 0.596, 0.859}{acion} \\
%             \addlinespace
%         \end{tabular}
%     \end{minipage}
%     \hfill
%     \begin{minipage}{0.50\textwidth}
%         \centering
%         \begin{tabular}{p{0.15\textwidth} p{0.85\textwidth}}
%             \textbf{Read:} &  la explicacion aclara la pregunta de la evaluacion \\
%             \textbf{Best:} & \textcolor[rgb]{0.204, 0.596, 0.859}{la explicacion aclara la pregunta de la evaluacion} \\
%             \textbf{Median:} & \textcolor[rgb]{0.204, 0.596, 0.859}{la explicacion }\textcolor[rgb]{0.882, 0.071, 0.188}{recobra} \textcolor[rgb]{0.204, 0.596, 0.859}{la pregunta de la }\textcolor[rgb]{0.882, 0.071, 0.188}{propiaccion} \\
%             \textbf{Worst:} & \textcolor[rgb]{0.204, 0.596, 0.859}{la explicacion }\textcolor[rgb]{0.882, 0.071, 0.188}{esperan el resultan el concilidad es} \\
%             \addlinespace
%             \textbf{Read:} & las teorias reducen los numeros \\
%             \textbf{Best:} & \textcolor[rgb]{0.204, 0.596, 0.859}{las teorias reducen los numeros} \\
%             \textbf{Median:} & \textcolor[rgb]{0.204, 0.596, 0.859}{las teorias re}\textcolor[rgb]{0.882, 0.071, 0.188}{sid}\textcolor[rgb]{0.204, 0.596, 0.859}{en los }\textcolor[rgb]{0.882, 0.071, 0.188}{liberia} \\
%             \textbf{Worst:} & \textcolor[rgb]{0.882, 0.071, 0.188}{ella l}\textcolor[rgb]{0.204, 0.596, 0.859}{o}\textcolor[rgb]{0.882, 0.071, 0.188}{g}\textcolor[rgb]{0.204, 0.596, 0.859}{r}\textcolor[rgb]{0.882, 0.071, 0.188}{ar} \textcolor[rgb]{0.204, 0.596, 0.859}{re}\textcolor[rgb]{0.882, 0.071, 0.188}{p}\textcolor[rgb]{0.204, 0.596, 0.859}{i}\textcolor[rgb]{0.882, 0.071, 0.188}{carona templadas} \\
%             \addlinespace
%         \end{tabular}
%     \end{minipage}
%     \caption{\textbf{Decoded Sentences Showcase across subjects.} \\
%     Correct characters are highlighted in \textcolor[rgb]{0.204, 0.596, 0.859}{blue}, mistakes in \textcolor[rgb]{0.882, 0.071, 0.188}{red}, and typing errors \underline{underlined}. \\
%     \textbf{(Left)} Median performances for three participants on a single sentence. \\
%     \textbf{(Right)} Best, median, and worst performances for two different sentences across participants.
%     }
%     \label{tab:combined_performance_decoded_sentences}
% \end{table*}

\end{document}