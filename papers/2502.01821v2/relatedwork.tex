\section{Related Work}
\label{sec:relatedwork}

We first introduce related work in \llm for test generation. 
Next, we discuss recent automated \brt generation techniques, specifically \libro and \sweagentplus from \swtbench. 
We then highlight the similarities and differences between these works and this paper.

\subsection{\llm for Test Generation}
\label{sec:relatedwork:llmtestgen}

The application of \llm{}s to automated test generation has garnered significant attention recently, showcasing the ability of \llm{}s to support developers during testing activities. 
\citet{yuan2023no} employs a conversational approach, iteratively generating unit tests through interaction with \chatgpt. 
This method leverages the conversational abilities of \llm{}s to refine test cases based on ongoing feedback. 
Similarly, \citet{xie2023chatunitest} adopts a feedback-based generation process with \chatgpt, focusing on iterative improvements through continuous interaction.
\citet{lemieux2023codamosa} takes a hybrid approach that combines evolutionary search with the code understanding capabilities of \codex~\cite{chen2021evaluating} to overcome the prior limitation in generating tests that improve coverage.  
\citet{schafer2023empirical} incorporates API documentation alongside \gptthreepfive to generate unit tests, highlighting the importance of providing additional context to \llm for improved test generation.
Further, \citet{li2023nuances} demonstrated that guiding \chatgpt with subtle differences between fixed and buggy codebases can effectively generate failure-inducing tests. 
\Space{Several studies\samcheng{I dont see more than one}}Prior work has empirically evaluated the effectiveness of \llm{}s in unit test generation, e.g., \citet{siddiq2023exploring} investigated and provided insights of \gptthreepfive and \codex on their test generation capabilities. 

The aforementioned studies collectively demonstrate the potential of \llm for automated test generation, showcasing various strategies for prompting, interaction, and context integration.
While these studies offer valuable insights into the application of \llm for test generation, they predominantly study the generation of generic tests, primarily unit tests, whereas we focus on the specific task of generating \brt{}s from bug reports in a production environment.

\subsection{\llm for \brt Generation}
\label{sec:relatedwork:brt}

\subsubsection{\libro}
\label{sec:relatedwork:brt:libro}

\libro~\cite{kang2023large,kang2024evaluating} was one of the first approaches to use \llm{}s for general bug reproduction. 
It frames the task of \brt generation as a few-shot code generation problem, where an \llm is prompted with a few $\langle$bug report, test$\rangle$ examples before being asked to generate a test for the targeting bug report. 
The key assumption of \libro is that \llm{}s, due to their extensive pre-training on vast amounts of code and natural language data, can understand the relationship between a bug description and the test code required to reproduce it.
\libro has the following components~\cite{kang2023large}:
\begin{enumerate}
    \item \textit{Prompt Engineering.} Given a bug report, construct the input prompt with the bug report title and description, and a few $\langle$bug report, test$\rangle$ examples. 
    \item \textit{Querying an \llm.} Query an \llm multiple times with the same input prompt and non-zero temperature to allow variation in the \llm's generated test code each time.
    \item \textit{Test Postprocessing.} Find a test file $F$ that is the most textually similar to each generated test $t$, inject $t$ into $F$ and resolve any new dependencies. \libro considers $t$ a candidate \brt if it compiles and fails on the buggy code.
    \item \textit{Selection and Ranking.} Candidate \brt{}s are clustered and de-duplicated by their failure traces and lines of code. \libro presents one test per cluster in a round-robin fashion to developers, prioritizing tests from the larger clusters. 
\end{enumerate}

\libro shows promising results on \defectsfj 2.0, a benchmark consisting of Java projects, demonstrating the ability of \llm{}s to understand bug descriptions and generate corresponding \brt{}s.\Space{ However, the approach also relied heavily on the quality of the retrieved context and the few-shot examples provided in the prompt, indicating room for improvement in these areas.\samcheng{this is incorrect}}
\citet{ahmed2024tdd} proposed an approach on top of \libro, composed of three \llm calls to find a test file, get function names from bug report, and then generate tests.
Our work, developed independently and concurrently to \citet{ahmed2024tdd}, also found that selecting test file(s) as input to \brt generation is more practical (\S\ref{sec:approach:libro}).

\subsubsection{\sweagentplus}
\label{sec:relatedwork:brt:swtbench}

\Space{\sweagentplus is the best-performing approach on \swtbench~}\citet{mundler2024swt} recently proposed \swtbench, a benchmark built on top of \swebench specifically for \brt generation in Python.
With \swtbench, \citet{mundler2024swt} evaluated \libro. 
They also\Space{ adapted and} evaluated \autopr agents that can interact with the codebase, i.e., \aider~\cite{aider}, \autocoderover~\cite{zhang2024autocoderover}, and \sweagent~\cite{yang2024swe}---these agents were adapted to the \brt generation task via changes in their system and instruction prompts.
On \swtbench, the adapted \sweagent and its proposed variant \sweagentplus perform the best, successfully generating \failtopass tests on 15.9\% and 18.5\% of the instances, respectively. 
They are also the most related to our work.


\sweagent provides an \llm with direct access to augmented command-line tools for searching, viewing, and editing files. 
The agent operates within a limited shell environment and, beyond initial instructions, offers minimal guardrails or structure to its \llm's actions.
Note that one of the tips to \sweagent in its instructions is to start by trying to replicate the bug~\cite{yang2024swe}.
\sweagentplus is the \sweagent variant proposed in \citet{mundler2024swt}, in additional to changes in the system and instruction prompts, it is also explicitly instructed to execute the generated tests before finishing.\Space{Importantly, the instructions do not provide information on how to run the tests. This simple addition was shown to increase the success rate from 15.9\% to 18.5\% in the \swtbench evaluation.}

\subsection{Similarities and Differences with Our Work}
\label{sec:relatedwork:compare}

Like \libro and \swtbench, we focus on \llm for \brt generation.
We adapted and evaluated \libro in our context (\S\ref{sec:approach:libro}).
We also developed an  agent-based approach for automated \brt generation (\S\ref{sec:approach:agent}).
While our approach is developed independently and concurrently to \sweagentplus, they both share the same high-level philosophy in its use of an \llm agent interacting with a codebase via a set of commands to generate \brt{}s.

Our work has the following key differences from prior work:

\myparagraph{Industrial Context}
\libro and \swtbench focus on a set of open-source Java and Python projects that are relatively well-studied~\cite{yang2024swe,Ernst2014Defects4J}, while we focus on \brt generation for production bugs within \google's internal ecosystem~\cite{rondon2025passerine}.
These production bugs are recent, from diverse projects, and span multiple languages\Space{: Java, C++, Go, Python, Kotlin, Dart, and TypeScript~\cite{rondon2025passerine}} (\S\ref{sec:empiricalstudy:datasetandconfig:dataset}).

Developing effective \brt generation for \google's internal environment also poses unique challenges and opportunities, such as dealing with a massive proprietary codebase\Space{ and leveraging}, internal-specific tooling and infrastructure.
For example, when developing our \brt generation agent, we use an \llm fine-tuned on \google's code for the agent's code editing command---this design addresses the confidential nature of \google's code and substantially improves the agent's performance and the effectiveness of its generated \brt{}s.


\myparagraph{Usefulness of the Generated \brt{}s}
In addition to studying the effectiveness of standalone \llm or \llm agent for automated \brt generation, we investigate the practical impact of the generated \brt{}s on an industrial-scale \autopr system---a dimension not explored in detail by \libro or \swtbench.

Specifically, we study the generated \brt{}s on\Space{ two use-cases}: (1) their effectiveness on improving the performance of \autopr system; (2) their effectiveness on selecting the most plausible \autopr-generated fixes.
Our study adds another layer of practical application not explicitly addressed in the related work in \llm-based \brt generation.