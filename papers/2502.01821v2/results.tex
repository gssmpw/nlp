\section{Empirical Results}
\label{sec:results}
We now present results of our research questions defined in \S\ref{sec:empiricalstudy:rq}:
\begin{itemize}
    \item \textbf{RQ1: \rqonetitle}: How effective is our \tool in generating \brt{}s for \google's internal human-reported bugs?
    \item \textbf{RQ2: \rqtwotitle}: How effective are the generated \brt{}s for improving the bug-fixing performance of an \autopr system?
    \item \textbf{RQ3: \rqthreetitle}: How effective are the generated \brt{}s for selecting plausible fixes generated by an \autopr system?
\end{itemize}

\subsection{\rqonetitle\Space{Bug Reproduction Effectiveness}}
\label{sec:results:brt}


\subsubsection{Overall Effectiveness}
Table \ref{tab:rq1_results} presents the \brt generation effectiveness of the evaluated techniques.  
The adapted \libro achieves a candidate \brt generation rate of 41\%. Its plausible \brt generation rate is 10\%\Space{. This}, which is lower than the 33\% reported on \defectsfj~\cite{kang2023large}, potentially due to the increased complexity of Google's internal bugs~\cite{rondon2025passerine}. 
The main failure mode is represented by \libro generating \brt{}s with build errors, from which it cannot recover.

On the other hand,\Space{ Our} \tool demonstrates superior performance. 
It achieves an 85\% candidate \brt generation rate and a 28\% plausible \brt generation rate, significantly outperforming \libro. 
The higher candidate-to-plausible \brt rate (34\%) indicates that \tool is more effective at generating tests that are likely to be actual \brt{}s.

Table~\ref{tab:rq1_languages} provides a detailed breakdown of plausible \brt{}s by programming languages.
Each value in Table~\ref{tab:rq1_languages} is computed relative to the number of evaluated bugs in a language.
Our \tool generates plausible \brt{}s across 6 languages\Space{: Java, C++, Go, Python, Kotlin, and TypeScript}.
It generates plausible \brt{}s on the same and higher number of bugs than \libro in 3 and 4 languages, respectively.
The ability to generalize across multiple languages is a particularly important characteristic of agent-based systems, as it suggests a broader applicability to diverse codebases and development environments~\cite{yang2024swebenchmultimodalaisystems}.\Space{\samcheng{i am not sure what this sentence means but fine for me to keep}}


Our results show that, \tool, by generating \brt in an agentic fashion on top of a \codeeditingllm specialized on \google's internal code, is significantly more effective in the industrial setting.\Space{\samcheng{add that \tool did not find the right test in 10\% of the cases}---\samcheng{this one is hard to argue, not mention}}

\input{tables/brt_generation_effectiveness}

\input{tables/brt_generation_language}

\myparagraph{Manual Inspection}

To further investigate \tool's generation quality, we manually inspect the generated patches of all plausible \brt{}s, by comparing them against the oracle \brt{}s.
Specifically, two authors inspect the \brt{}s and a third author resolves any disagreement.\Space{michele: we should give a bit of information about how this manual analysis was conducted: 2 authors analyzed the BRTs + 1 author resolved the conflicts on 5 cases.}
We consider a plausible generated \brt patch to be \textit{valid} if (1) it is identical or semantically equivalent to the oracle \brt patch, and (2) it did not modify any existing test.

Through inspection, we find that 86\% of the plausible generated \brt patches are valid, or 25\% of the bugs have at least one valid \brt patch.
The percentage of plausible \brt patches that are identical and semantically equivalent to the oracle \brt{}s are 19\% and 48\%, respectively.
Meanwhile, 16\% of the plausible \brt patches, despite being valid, add irrelevant assertion(s) or new test(s) that pass on the bug.
We reason these irrelevant code segments are by-products of the agent's attempts into iteratively improving its previous edits~\cite{jimenez2023swe,rondon2025passerine} before producing a test failure.
We also find that 11\% of the plausible \brt patches to be invalid because they have modified existing test(s).
Our inspection results imply that the majority of generated plausible \brt patches are readily-usable, and future work can further improve \tool by systematically resolving its suboptimal behavior of changing existing tests into valid \brt{}s.

\subsubsection{Agent Behaviors}
\label{sec:results:brt:behave}
We also study the statistics on agent behaviors, specifically the action distribution and termination reasons.

\begin{figure*}[t!]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/command_index_chart.pdf} 
    \caption{Action distribution by step for the \tool.\label{fig:action_distribution}}
\end{figure*}

Figure \ref{fig:action_distribution} shows the action distribution of each step for all 1600 (80 * 20) runs of \tool. 
In each run, the agent can take at most 25 steps (\S\ref{sec:empiricalstudy:datasetandconfig}).
The ``other'' category includes actions output by the agent that are not part of the provided action set---in a few cases, the agent hallucinated and output actions, such as \CodeIn{grep} or \CodeIn{find}, that are not listed in its system prompt.
Figure~\ref{fig:action_distribution} also shows \tool frequently starts by finding related files via \actioncodesearch, inspecting their content via \actioncat, and  proceeding to edit and run tests.


\begin{table}[t!]
\centering
\begin{tabular}{lc}
\toprule
Top-5 action bigram & Frequency \\
\midrule
(\CodeIn{edit}, \CodeIn{bazel test}) & 19\% \\
(\CodeIn{cat}, \CodeIn{edit}) & 10\% \\
(\CodeIn{bazel test}, \CodeIn{edit}) & 9\% \\
(\CodeIn{code\_search}, \CodeIn{code\_search}) & 9\% \\
(\CodeIn{code\_search}, \CodeIn{cat}) & 8\% \\
\bottomrule
\end{tabular}
\caption{Top-5 most frequent action bigrams of \tool.}
\label{tab:action_bigrams}
\end{table}


Table \ref{tab:action_bigrams} shows the top-5 most frequent action bigrams from \tool. 
The frequency column indicates the occupancy of a specific bigram among all bigrams. 
The top-3 bigrams mainly represent the agent's core task of \brt generation: editing, analyzing, and running test files. 
The two remaining bigrams mainly represent the agent's file localization and debugging activities.

\begin{table}[t!]
\centering
\begin{tabular}{lcc}
\toprule
Termination reason & \libro & \tool (Ours) \\
\midrule
Requested termination & 83\% & 72\% \\
Steps exhausted & - & 21\% \\
Framework exception & 17\% & 7\% \\
\bottomrule
\end{tabular}
\caption{Termination reasons of techniques.}
\label{tab:termination_reasons}
\end{table}

Table \ref{tab:termination_reasons} shows the category of reasons that the evaluated \brt generation techniques terminated.
\tool\Space{ frequently} terminates gracefully by invoking the \actionfinish action in 72\% of the cases, while exhausting the configured total number of steps (i.e., 25) in 21\% of the cases. 
There are also cases where \tool and \libro throw framework-level exceptions; common reasons for framework-level exceptions are the \llm input exceeding the input context window size, server rate limit exceptions, and arbitrary exceptions thrown from imported libraries outside the framework (\tool only).


\subsection{\rqtwotitle\Space{Impact on Automated Patch Generation}}
\label{sec:results:generation}

We now present results of \passerine's bug-fixing effectiveness with versus without providing generated plausible \brt{}s (\S\ref{sec:empiricalstudy:rq2}).

\myparagraph{Plausible Fixes}
Generated \brt{}s from \tool can substantially improve \passerine's performance in generating plausible fixes.
Figure~\ref{fig:rq2_plausible_fixes} shows the sets of bugs resolved by \passerine with versus without providing \brt{}s as initial input. 
\passerine generated plausible fixes in 74\% (17/23) of the bugs when provided with a generated \brt as initial input, compared to 57\% (13/23) when not provided a \brt. 
Providing \brt{}s help \passerine solve 6 new, unique bugs that it would have not solved if without a \brt.

\myparagraph{Steps to Plausible Fixes}
\passerine took fewer steps on average to generate plausible fixes when provided with a \brt. 
Figure \ref{fig:rq2_steps_to_fix} depicts the frequency distributions of \passerine's runs completed within $k$ steps for both settings (i.e., with and without \brt), with a maximum of $k=25$ steps. 
The leftward shift from the distribution of ``without \brt'' to the distribution of ``with \brt'' indicates that Passerine resolved bugs with fewer steps when provided a \brt.


\myparagraph{Probability of Plausible Fixes}
The probability of \passerine generating a plausible fix given that the generated \brt was used is 33\%, compared to only 2\% when the \brt was not used. 
This observation implies that a plausible fix is more likely to be generated by the \autopr agent in runs where the \brt is provided and used.


Overall, providing generated \brt{}s to an \autopr system like \passerine could improve its bug-fixing performance, both in terms of the number of bugs fixed and the fix generation efficiency. 
Meanwhile, \autopr agents should also attempt to generate multiple fixes both with and without providing \brt{}s through sampling to cover more inference trajectories and unlock new possibilities.


\begin{figure}[t!]
    \centering
    \includegraphics[width=0.8\columnwidth]{figures/rq2_bugs.pdf}
    \caption{The number of plausible fixes generated by \passerine with (red) and without (green) \brt as input.}
    \label{fig:rq2_plausible_fixes}
\end{figure}



\begin{figure}[t!]
    \centering
    \includegraphics[width=\columnwidth]{figures/rq2_steps.pdf}
    \caption{Average number of steps per run for \passerine to generate plausible fix with and without \brt as input.}
    \label{fig:rq2_steps_to_fix}
\end{figure}

\subsection{\rqthreetitle\Space{Impact on Automated Patch Selection}}
\label{sec:results:selection}




\subsubsection{Top-K Selection}
Figure~\ref{fig:rq3_top_k} shows precision@$K$ and recall@$K$ achieved with the \enpassratefull (\enpassrate) computed on the generated \brt{}s, $K$ increments from 1 to 5. 
We can see from Figure~\ref{fig:rq3_top_k} that \enpassrate allows for different \fixpatch selection strategies. 

When prioritizing maximum precision, selecting the top-ranked \fixpatch ($K=1$) correctly identifies a plausible \fixpatch from a pool of 20 candidates in 70\% of cases, achieving a precision of 0.7. 
This means that, on average, the correct \fixpatch is ranked first in 70\% of the cases (an MRR of 0.7). However, this approach yields a lower recall of 0.3.  

Alternatively, a more balanced approach, considering the top-3 ranked \fixpatch{}es ($K=3$), achieves a precision of 0.6 and a recall of 0.5 obtaining the highest F1-score among the different K values. 
Varying $K$ between 1 and 5 reveals a trade-off: precision decreases from 0.7 to 0.5, while recall increases from 0.3 to 0.6. 

Overall, Top-K selection via \enpassrate allows for only the most likely plausible \fixpatch{}es to be presented to the \autopr user\Comment{/developer\samcheng{user in this paper is developer}}, minimizing the need to review incorrect \fixpatch{}es.

\begin{figure}[t!]
    \centering
    \includegraphics[width=\columnwidth]{figures/rq3_1.pdf}
    \caption{Results of Top-K \fixpatch selection via \enpassrate.}
    \label{fig:rq3_top_k}
\end{figure}

\subsubsection{Threshold-Based Selection}
Figure \ref{fig:rq3_threshold} illustrates the trade-off between precision and recall when varying the \enpassrate threshold for \fixpatch selection. 
As the \enpassrate threshold increases from 0 to 1 in increments of 0.01, precision increases from 0.3 to 1, while recall decreases from 1 to 0.1. 
A balance point is when the threshold is set to 0.1 (e.g., \fixpatch{}es passed at least 1-2 generated \brt{}s), where we achieve a precision of 0.9, and a recall of 0.6. This threshold also yields an MRR of 0.6, indicating that a correct \fixpatch is frequently ranked highly. 
Additionally, with an F1-score of 0.6, this threshold offers a good balance between precision and recall for \fixpatch selection.


\begin{figure}[t!]
    \centering
    \includegraphics[width=\columnwidth]{figures/rq3_2.pdf}
    \caption{Results of threshold-based \fixpatch selection via \enpassrate.}
    \label{fig:rq3_threshold}
\end{figure}

These results demonstrate that the \enpassratefull (\enpassrate), derived from generated candidate \brt{}s, is an effective metric for selecting plausible fixes. 
While it exhibits good precision, the recall can be improved by generating more candidate fixes per bug.


