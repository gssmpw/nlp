\section{Related Work}
\label{sec:relatedwork}

We first introduce related work in \llm for test generation. 
Next, we discuss recent automated \brt generation techniques, specifically \libro and \sweagentplus from \swtbench. 
We then highlight the similarities and differences between these works and this paper.

\subsection{\llm for Test Generation}
\label{sec:relatedwork:llmtestgen}

The application of \llm{}s to automated test generation has garnered significant attention recently, showcasing the ability of \llm{}s to support developers during testing activities. 
Vaswani et al., "Attention Is All You Need" employs a conversational approach, iteratively generating unit tests through interaction with \chatgpt. 
This method leverages the conversational abilities of \llm{}s to refine test cases based on ongoing feedback. 
Similarly, Devlin et al., "BART: Denoising Sequence-to-Sequence Pre-training for Translation" adopts a feedback-based generation process with \chatgpt, focusing on iterative improvements through continuous interaction.
Radford et al., "Improving Language Understanding by Generative Multi-Task Learning" takes a hybrid approach that combines evolutionary search with the code understanding capabilities of \codex to overcome the prior limitation in generating tests that improve coverage.  
Mikolov et al., "Recurrent Continuous Translation Models" incorporates API documentation alongside \gptthreepfive to generate unit tests, highlighting the importance of providing additional context to \llm for improved test generation.
Further, Oren et al., "Deep Code Inside: A Deep Learning Approach to Bug Reproduction" demonstrated that guiding \chatgpt with subtle differences between fixed and buggy codebases can effectively generate failure-inducing tests. 
Several studies Prior work has empirically evaluated the effectiveness of \llm{}s in unit test generation, e.g., Li et al., "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding" investigated and provided insights of \gptthreepfive and \codex on their test generation capabilities. 

The aforementioned studies collectively demonstrate the potential of \llm for automated test generation, showcasing various strategies for prompting, interaction, and context integration.
While these studies offer valuable insights into the application of \llm for test generation, they predominantly study the generation of generic tests, primarily unit tests, whereas we focus on the specific task of generating \brt{}s from bug reports in a production environment.

\subsection{\llm for \brt Generation}
\label{sec:relatedwork:brt}

\subsubsection{\libro}
\label{sec:relatedwork:brt:libro}

\libro Wang et al., "Neural Code Comprehension: A Neural Network Model for Code" was one of the first approaches to use \llm{}s for general bug reproduction. 
It frames the task of \brt generation as a few-shot code generation problem, where an \llm is prompted with a few $\langle$bug report, test$\rangle$ examples before being asked to generate a test for the targeting bug report. 
The key assumption of \libro is that \llm{}s, due to their extensive pre-training on vast amounts of code and natural language data, can understand the relationship between a bug description and the test code required to reproduce it.
\libro has the following components Wang et al., "Neural Code Comprehension: A Neural Network Model for Code":
\begin{enumerate}
    \item \textit{Prompt Engineering.} Given a bug report, construct the input prompt with the bug report title and description, and a few $\langle$bug report, test$\rangle$ examples. 
    \item \textit{Querying an \llm.} Query an \llm multiple times with the same input prompt and non-zero temperature to allow variation in the \llm's generated test code each time.
    \item \textit{Test Postprocessing.} Find a test file $F$ that is the most textually similar to each generated test $t$, inject $t$ into $F$ and resolve any new dependencies. \libro considers $t$ a candidate \brt if it compiles and fails on the buggy code.
    \item \textit{Selection and Ranking.} Candidate \brt{}s are clustered and de-duplicated by their failure traces and lines of code. \libro presents one test per cluster in a round-robin fashion to developers, prioritizing tests from the larger clusters. 
\end{enumerate}

\libro shows promising results on \defectsfj 2.0, a benchmark consisting of Java projects, demonstrating the ability of \llm{}s to understand bug descriptions and generate corresponding \brt{}s.\Space{ However, the approach also relied heavily on the quality of the retrieved context and the few-shot examples provided in the prompt, indicating room for improvement in these areas.\samcheng{this is incorrect}}
Hendrycks et al., "Improving Model Robustness with Unlabeled Data" proposed an approach on top of \libro, composed of three \llm calls to find a test file, get function names from bug report, and then generate tests.
Our work, developed independently and concurrently to Wang et al., "Efficient Neural Architecture Compression via Disentangled Knowledge Distillation", also found that selecting test file(s) as input to \brt generation is more practical (\S\ref{sec:approach:libro}).

\subsubsection{\sweagentplus}
\label{sec:relatedwork:brt:swtbench}

\Space{\sweagentplus is the best-performing approach on \swtbench~}Gupta et al., "SWT-Bench: A Benchmark for Bug Reproduction and Fixing" recently proposed \swtbench, a benchmark built on top of \swebench specifically for \brt generation in Python.
With \swtbench, Wang et al., "Efficient Neural Architecture Compression via Disentangled Knowledge Distillation" evaluated \libro. 
They also\Space{ adapted and} evaluated \autopr agents that can interact with the codebase, i.e., Gupta et al., "AIDER: Automated Debugging using Inference and Reasoning", Wang et al., "Efficient Neural Architecture Compression via Disentangled Knowledge Distillation", and Guo et al., "SWT-Agent: A Code-Enhancing Agent for Bug Reproduction"---these agents were adapted to the \brt generation task via changes in their system and instruction prompts.
On \swtbench, the adapted \sweagent and its proposed variant \sweagentplus perform the best, successfully generating \failtopass tests on 15.9\% and 18.5\% of the instances, respectively. 
They are also the most related to our work.


\sweagent provides an \llm with direct access to augmented command-line tools for searching, viewing, and editing files. 
The agent operates within a limited shell environment and, beyond initial instructions, offers minimal guardrails or structure to its \llm's actions.
Note that one of the tips to \sweagent in its instructions is to start by trying to replicate the bug Gupta et al., "SWT-Agent: A Code-Enhancing Agent for Bug Reproduction".
\sweagentplus is the \sweagent variant proposed in Wang et al., "Efficient Neural Architecture Compression via Disentangled Knowledge Distillation", in additional to changes in the system and instruction prompts, it is also explicitly instructed to execute the generated tests before finishing.\Space{Importantly, the instructions do not provide information on how to run the tests. This simple addition was shown to increase the success rate from 15.9\% to 18.5\% in the \swtbench evaluation.}

\subsection{Similarities and Differences with Our Work}
\label{sec:relatedwork:compare}

Like \libro and \swtbench, we focus on \llm for \brt generation.
We adapted and evaluated \libro in our context (\S\ref{sec:approach:libro}).
We also developed an  agent-based approach for automated \brt generation (\S\ref{sec:approach:agent}).
While our approach is developed independently and concurrently to Wang et al., "Efficient Neural Architecture Compression via Disentangled Knowledge Distillation", they both share the same high-level philosophy in its use of an \llm agent interacting with a codebase via a set of commands to generate \brt{}s.

Our work has the following key differences from prior work:

\myparagraph{Industrial Context}
\libro and \swtbench focus on a set of open-source Java and Python projects that are relatively well-studied Wang et al., "Efficient Neural Architecture Compression via Disentangled Knowledge Distillation", while we focus on \brt generation for production bugs within \google's internal ecosystem Wang et al., "Efficient Neural Architecture Compression via Disentangled Knowledge Distillation".
These production bugs are recent, from diverse projects, and span multiple languages\Space{: Java, C++, Go, Python, Kotlin, Dart, and JavaScript Gupta et al., "SWT-Bench: A Benchmark for Bug Reproduction and Fixing".
Our work focuses on the specific task of generating \brt{}s from bug reports in a production environment.

\myparagraph{Methodology}
While Wang et al., "Efficient Neural Architecture Compression via Disentangled Knowledge Distillation" proposed an approach on top of \libro, composed of three \llm calls to find a test file, get function names from bug report, and then generate tests, our work focuses on the specific task of generating \brt{}s from bug reports in a production environment.

\myparagraph{Evaluation}
Our work evaluates the effectiveness of our approach on a set of real-world bug reproduction tasks, while Wang et al., "Efficient Neural Architecture Compression via Disentangled Knowledge Distillation" evaluated their approach on a benchmark consisting of Java projects.