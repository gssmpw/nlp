\section{Empirical Study Design}
\label{sec:empiricalstudy}

This section outlines the design of our empirical study\Space{ on the effectiveness of our \brt generation techniques}. 
We describe the evaluation dataset, experiment configurations, research questions (RQs)\Space{ guiding our investigation}, and evaluation metrics for each RQ\Space{ employed to assess the performance of our approaches}.


\subsection{Dataset and Experiment Configurations}
\label{sec:empiricalstudy:datasetandconfig}

\subsubsection{Dataset}
\label{sec:empiricalstudy:datasetandconfig:dataset}
Our evaluation is conducted on a dataset of\Space{ approximately} 80\footnote{Concurrent work~\cite{rondon2025passerine} reports 78 bugs in the same set due to recent, unrelated infrastructure changes rendering two bugs' tests no longer executable; this work was performed before the change.} production bugs in\Space{collected from} \google's internal issue tracking system (\gits).
All the bugs were reported by human developers, and were fixed by human developers from corresponding teams.
This dataset was constructed via automated extraction and filtering phases as well as manual curation\Space{, in previous research conducted at Google}~\cite{rondon2025passerine}. 
The bugs are recent (since June 2024), from a wide array of \google projects, and span seven programming languages: Java, C++, Go, Python, Kotlin, Dart, and TypeScript.\Space{Specifically, we focus on\Space{ the subset of} bugs reported by human developers, and exclude those reported by automated tools from the dataset.}
In the manual curation process, each bug and its corresponding fix were carefully reviewed to ensure that the fix genuinely addresses the underlying issue reported in the bug report, and not merely a workaround or masking the bug. 

Each bug sample in the evaluation dataset has:

\begin{itemize}
    \item \textbf{\gits Issue}: A bug report with a title and a description\Space{ of the issue}, reported by a human developer.
    \item \textbf{Ground Truth Fix}: A code change (CL) that resolves the reported bug. 
    This fix includes an ``oracle'' \brt---a manually written test that reproduces the bug and validates the fix~\cite{frommgen2024resolving}. 
    \Space{Importantly, due to the manual curation, these fixes are}Each fix has been manually verified to address the root cause of the corresponding issue as identified in the bug report~\cite{rondon2025passerine}.
    \item \textbf{Ground Truth Buggy Files}: The files modified by the ground truth fix, representing the buggy version of the code.
    \item \textbf{Ground Truth Test File}: The test file modified by the ground truth fix to include the oracle \brt.
\end{itemize}



\subsubsection{Experiment Configurations}
We evaluate the adapted \libro (\S\ref{sec:approach:libro}) and our \tool (\S\ref{sec:approach:agent}) for \brt generation.

We provide the \gits issue title and description, ground truth buggy files, and the ground truth test file to the adapted \libro as input. 
We provide only the \gits issue title and description, along with the ground truth buggy files, to \tool as input.

\libro uses only one \llm, while \tool uses a reasoning \llm and a \codeeditingllm (\S\ref{sec:approach}).
We use the same \gemini model fine-tuned on \google's internal code as \libro's \llm and \tool's \codeeditingllm, and use a publicly available \gemini as \tool's reasoning \llm. 
For each $\langle$technique, bug$\rangle$, we perform multiple experiment runs to account for the stochastic nature of \llm{}s. 
For \libro, we set a temperature of 0.7 and a top P of 0.95 for the \llm, and perform 50 runs per bug~\cite{kang2023large}. 
For \tool, we set a temperature of 0.2 and a top P of 0.95 for both the reasoning and the \codeeditingllm, and perform 20 runs per bug~\cite{rondon2025passerine}. 
We set the total number of steps the \tool can take per run to 25. 

We provide three synthetic \brt generation examples in the system prompt of \tool to provide additional context of the \brt generation task to \gemini\Space{: an example where a \brt generation is done with one \actionedit{}-\actiontest iteration, and two examples where multiple \actionedit{}-\actiontest iterations were performed before the \brt is generated}.  
These examples demonstrate the desired agent behaviors in a \react format, showcasing trajectories of successful and unsuccessful \brt generation attempts, and were unchanged across all experiments. 
We did not provide these examples to the adapted \libro because it is not an agent-based approach. 
We need not provide code generation examples to \libro because it already uses a \codeeditingllm trained for \google internal code generation tasks, including test generation.\Comment{We drop the selection and ranking component of \libro because we evaluate all of its generated \brt{}s.}



\subsection{Research Questions}
\label{sec:empiricalstudy:rq}
Our study aims to answer the following research questions:

\begin{itemize}
    \item \textbf{RQ1: \rqonetitle}: How effective is our \tool in generating \brt{}s for \google's internal human-reported bugs?
    \item \textbf{RQ2: \rqtwotitle}: How effective are the generated \brt{}s for improving the bug-fixing performance of an \autopr system?
    \item \textbf{RQ3: \rqthreetitle}: How effective are the generated \brt{}s for selecting plausible fixes generated by an \autopr system?
\end{itemize}

\subsubsection{RQ1: \rqonetitle}
\label{sec:empiricalstudy:rq1}
A high success rate in generating \brt{}s in an industrial context is crucial, because it provides developers with a valuable tool for understanding, debugging, and ultimately fixing software defects, thereby\Space{ significantly} reducing the\Space{ manual} effort\Space{ typically} required in the debugging process and accelerating the overall software development lifecycle. 
Successfully developing \brt generation techniques for an industrial environment like \google's would demonstrate their practical applicability and potential for broader impact beyond\Space{ the realm of} open-source projects.

\myparagraph{Setup}
We run \libro and \tool with settings as described in \S\ref{sec:empiricalstudy:datasetandconfig}.
We compute their \brt generation effectiveness with the same metrics as recent studies~\cite{mundler2024swt,kang2023large}:\Space{\samcheng{lets not mention inspection here just like agent behavior analysis}}

\begin{itemize}
    \item \textbf{Candidate \brt{}s}: Percentage of bugs for which at least one test is generated that fails on the buggy version of the code (\failtoany). 
    These are considered candidate \brt{}s because failing on the buggy version is a necessary, but not sufficient, condition for a test to be a true \brt.
    \item \textbf{Plausible \brt{}s}: Percentage of bugs for which at least one test is generated that fails on the buggy version and passes on the fixed version of the code (\failtopass). 
    These are considered plausible \brt{}s because they satisfy both necessary conditions for reproducing a bug and validating the fix.
    \item \textbf{Candidate to Plausible \brt Rate}: Percentage of bugs with candidate \brt{}s that are confirmed to be as plausible \brt{}s when applied to the ground truth fix. 
    This metric provides insight into the precision of the \brt generation technique.
\end{itemize}



\subsubsection{RQ2: \rqtwotitle}
\label{sec:empiricalstudy:rq2}

This research question investigates the practical impact of the automatically generated \brt{}s on fix generation. 
The hypothesis is that \brt{}s provide additional context to better understand the buggy behaviors, which can improve bug-fixing efficiency (\S\ref{sec:motivation}).\Space{the hypothesis should be kept because we use plausible \brt not candidate \brt.}

Specifically, we want to understand how providing \brt{}s to \passerine~\cite{rondon2025passerine}, an industrial-scale, \llm-agent-based \autopr system designed to work with \google's internal codebase and framework, can improve its ability in generating plausible fixes. 


\myparagraph{Setup}
For each bug in which \tool did generate plausible \brt{}(s), we provide a generated plausible \brt{} to \passerine by (1) applying the generated \brt patch to the buggy codebase version where \passerine will perform the bug-fixing activities, and (2) mentioning a \brt (with test path provided) is available in the \gits issue.
We let \passerine decide whether to use the provided \brt\Space{ during its bug-fixing run}.
Note that the plausibility of a \brt is determined with a fix (\S\ref{sec:empiricalstudy:rq1}), results from this research question thus represent an upper bound on the impact of generated \brt{}s on \autopr\Space{, independent of the \brt generation strategy\samcheng{we use brts from \tool}}.



To properly sample a generated plausible \brt for each bug, and allow such sampling procedure to be applicable in practice where a fix would not be available prior to the sampling, we use \gemini as \llm-as-a-Judge~\cite{zheng2023judging}.
We prioritize selecting a plausible \brt{} that (1) has a test failure judged to be caused by the bug, (2) was generated from a gracefully terminated \tool run, and (3) its last execution in the \tool run produced a test failure. 
We sample randomly if multiple or no candidate meet these criteria.


We compare \passerine's performance with versus without providing a generated \brt, on the subset of 23 bugs where \tool generated plausible \brt{}s. 
We run \passerine with \gemini on temperature 0.2 and top P 0.95 for 20 runs per bug~\cite{rondon2025passerine}. 
We determine the plausibility of a generated fix by running the oracle \brt on it.




We use these metrics to measure \passerine's performance:
\label{sec:empiricalstudy:rq3}
\begin{itemize}
    \item \textbf{Number of Bugs with Plausible Fixes}: Number of bugs for which \passerine generates at least one plausible fix (a fix that passes the oracle \brt which \passerine cannot access).\Space{ We compare this number between runs with and without the generated \brt provided as input.}
    \item \textbf{Steps to Plausible Fix}: The\Space{ average} number of steps taken by \passerine to generate a plausible fix\Space{, comparing runs with and without the generated \brt}.
    \item \textbf{Plausibility Given \brt Usage}: The probability that a generated fix is plausible given that the generated \brt was used by \passerine in the bug-fixing run.
\end{itemize}


\subsubsection{RQ3: \rqthreetitle}

This research question explores the utility of generated \brt{}s as a mechanism for selecting plausible fixes from a pool of possible fixes generated by an \autopr system. 
\llm-based \autopr systems can generate a large number of potential fixes per bug~\cite{jiang2023impact,fan2023automated,xia2023automated,jimenez2023swe,rondon2025passerine}, efficiently identifying a correct fix from all potential fixes is a crucial challenge when \textit{no existing test} had failed because of the bug\Space{, i.e., no existing test is a \brt}. 
We thus examine whether\Space{ the suite of} generated \brt{}s can be used to effectively discriminate between correct and incorrect fixes, and to rank fixes based on their likelihood of being correct. 

\newcommand{\passtopass}{$P \rightarrow P$\xspace}

We introduce \emph{\enpassratefull (\enpassrate)}, defined as the percentage of candidate \brt{}s that pass when executed against a given fix. 
The intuition is to use the generated tests (i.e., candidate \brt{}s with \failtoany behavior) as a ``synthetic test suite'' to assess the effectiveness for all potential fixes, and leverage the pass/fail test results to inform fix selection.
We omit Pass-to-Pass tests~\cite{mundler2024swt} because they do not fail on the bug.
\Comment{This approach aims to balance between checking the validity of fix with only one~\cite{kang2023large} or all~\cite{mundler2024swt} generated \brt{}s.\samcheng{not sure if this has been mentioned, lets not draw attention}}
If successful, this approach would be a useful tool for improving the accuracy of \autopr systems and reducing the manual effort required to validate their outputs.


\myparagraph{Setup} 
Different from \S\ref{sec:empiricalstudy:rq2}, in this research question, \passerine is not provided a \brt for its bug-fixing run---we only evaluate the usefulness of the generated \brt{}s for fix selection,\Space{ in isolation} after \passerine has generated fixes.
We use standard \irfull metrics to measure the efficacy of \enpassrate in selecting plausible fixes: 

\begin{itemize}
    \item \textbf{Precision, Recall, F1-score, and Mean Reciprocal Rank (MRR)}.
    We define a true positive as a selected fix that passes the oracle \brt, a false positive as a selected fix that fails the oracle \brt, and a false negative as an unselected fix that passes the oracle \brt. 
\end{itemize}

\Space{We analyze how these metrics vary with different values of an integer or a float threshold. }We compute values of these metrics for \enpassrate-based fix selection under two settings: (1) \textit{Top-K Selection}, where we select the top-$K$ fixes with the highest \enpassrate, and (2) \textit{Threshold-Based Selection}, where we select all fixes with an \enpassrate above a certain threshold.

