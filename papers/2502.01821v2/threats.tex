\section{Threats to Validity}
\label{sec:threats}

While our study provides promising results for automated \brt generation in an industrial setting, several factors could potentially influence the validity of our findings.\Space{We categorize these threats into internal, external, and construct validity.}

\myparagraph{Internal Validity}
One potential threat\Space{ to internal validity} is the evaluation dataset size.
We study \brt generation on a relatively small dataset of 80 bugs from \google's internal issue tracking system (\S\ref{sec:empiricalstudy:datasetandconfig}). 
Although this dataset has been carefully curated and studied for prior work on \autopr~\cite{rondon2025passerine}, its size could limit the generalizability of our results to the broader spectrum of bugs encountered within \google.\Space{The specific distribution of bug types and affected projects within the dataset might not perfectly reflect the overall bug landscape.}
Another potential threat is implementation bias. 
Our adaptation of \libro may have introduced subtle differences that could affect its performance.
To mitigate such threat, we ensure each change we made to \libro improves its experiment performance in \google's environment. 
The difference in \llm{}s can also pose a threat.
To mitigate such threat, we consistently use the same \gemini models(s) for code generation for all evaluated techniques.
Finally, the inherent randomness in \llm{}s could influence performance of \llm-based techniques. 
To mitigate the threat from such randomness, we run each evaluated technique on the same input multiple times (\S\ref{sec:empiricalstudy:datasetandconfig}).

\myparagraph{External Validity}
A key threat to external validity is the generalizability of our findings to other industrial settings. 
Our study focuses exclusively on \google's internal development environment. 
While this provides valuable insights into a large-scale industrial setting, the specific tools, processes, and codebase characteristics may differ significantly from those in other companies. 
Therefore, the generalizability of our findings to other industrial settings requires further investigation.

\myparagraph{Construct Validity}
One potential threat\Space{ to construct validity} lies in our evaluation.
We use metrics like Candidate \brt{}s and Plausible \brt{}s from prior work~\cite{mundler2024swt} to measure \brt generation success. 
However, these metrics may not fully capture all aspects of a\Space{ ``good''} \brt, such as its readability, maintainability, or ability to trigger subtle, hard-to-reproduce bugs. 
To mitigate this threat, we perform manual inspection on the plausible generated \brt{}s  (\S\ref{sec:results:brt}). 
Another threat to construct validity lies in our proposed metric \enpassrate (\S\ref{sec:empiricalstudy:rq3}) for assessing fix correctness in \S\ref{sec:empiricalstudy:rq3}. 
While \enpassrate shows promising results, it is inherently an indirect measure, and may not always perfectly correlate with the actual correctness of a fix as determined by human developers.
