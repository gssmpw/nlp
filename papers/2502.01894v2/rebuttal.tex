\documentclass[10pt, twocolumn, letterpaper]{article}
\usepackage[rebuttal]{cvpr}

\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}

\input{preamble}

\definecolor{cvprblue}{rgb}{0.21, 0.49, 0.74}
\usepackage[pagebackref, breaklinks, colorlinks, allcolors=cvprblue]{hyperref}

%\setcounter{figure}{2}
%\setcounter{table}{1}
%\setcounter{equation}{2}

%\makeatletter
%\apptocmd{\thebibliography}{\global\c@NAT@ctr 100\relax}{}{}
%\makeatother

\def\paperID{8035}
\def\confName{CVPR}
\def\confYear{2025}

\begin{document}

\title{SimBEV: A Synthetic Multi-Task Multi-Sensor Driving Data Generation Tool and Dataset}

\maketitle
\thispagestyle{empty}
\appendix

We thank the reviewers for their thoughtful feedback and are grateful for their recommendations. We are encouraged that the reviewers find our idea simple and easy to understand ({\color{red} R1}), think our dataset shows significant advantages over other datasets in supporting BEV segmentation tasks ({\color{green} R2}), and find our work beneficial to the development of the field of autonomous driving ({\color{green} R2}, {\color{blue} R3}). We address reviewer comments and concerns below and will incorporate all feedback into the final version of the paper.

\paragraph{SimBEV v. CARLA.} At the start of Sec. 3, we state that ``SimBEV relies on a customized version of CARLA 0.9.15 \ldots to simulate the environment, perception sensors, and traffic behavior\ldots" While we explain our enhancements to the CARLA content library (and not CARLA itself, which we use as is) in the Supplementary Material (SM), we understand how our wording may have caused some confusion and indicated that we were taking credit for CARLA's customization capabilities, when we were not ({\color{red} R1}, {\color{green} R2}).

SimBEV and CARLA's relationship is somewhat akin to that of Keras and TensorFlow. SimBEV streamlines, automates, and manages the entire data collection process for the user (which controls it through a single config file) by creating randomized scenarios, collecting and saving sensor data, and calculating the ground truth, doing so efficiently by manipulating simulation elements through CARLA's Python API. SimBEV equips the user with CARLA's configurability when necessary and takes charge when not. For instance, it is SimBEV that decides, based on a desired probability set by the user, what door(s) of which stopped car(s) should be open when and for how long ({\color{red} R1}, {\color{green} R2}).

This flexibility and ease of use enables, for instance, one user to quickly create a lidar dataset of reckless driving on highways, another a dataset of night-time camera images for BEV segmentation, and a third an offroad dataset for 3D object detection from a custom map. In our case, we used SimBEV on a machine with a single Nvidia RTX 3090 graphics card to create the SimBEV dataset in the span of just 72 hours (1.8 TB, more than half comprised of depth camera images and dense semantic lidar point clouds) ({\color{blue} R3}). Had we more storage space, or used fewer sensor types, SimBEV could have easily created a dataset that eclipses even datasets as large as the SHIFT dataset. In short, SimBEV (which will be open-sourced soon ({\color{green} R2})) facilitates and democratizes the creation of driving datasets, and along with the accurate BEV ground truth that it provides, makes a worthy contribution to the field ({\color{red} R1}, {\color{green} R2}).

\paragraph{SimBEV dataset benchmarks.} We would have liked to benchmark more algorithms and tasks on the SimBEV dataset, but given the small size of our team (1 researcher and 1 advisor, compared to e.g. 8 contributers to SHIFT), our priority was to streamline SimBEV and make it as fast and as efficient as possible, as we felt it would be a bigger contribution to the field. We chose BEVFusion as our baseline model because it excels at both 3D object detection and BEV segmentation, and because it lets us compare different sensing modalities. It is worth noting that for 3D object detection, while under the BEVFusion umbrella, the camera-only model is a variant of BEVDet-Tiny with a much heavier view transformer, while the lidar-only model is the lidar-only variant of TransFusion ({\color{red} R1}, {\color{green} R2}, {\color{blue} R3}).

The SimBEV dataset uses CARLA's latest, biggest, most diverse maps not used in other datasets (including SHIFT), and we have provided ample statistics about the SimBEV dataset in the SM which, while not perfect, should give the reviewers a sense of how the SimBEV dataset compares to others such as nuScenes and SHIFT ({\color{green} R2}, {\color{blue} R3}). We will work to include the results of benchmarking UniTR on the SimBEV dataset, along with BEVFormer and BEVDepth as suggested by {\color{red} R1}, in the final version of the paper.

\paragraph{Sim-to-real gap.}

As noted by {\color{red} R1}, the CARLA environment and the real world are statistically different domains. However, the domain gap is not the same for every type of data. While camera-only models trained on CARLA images are practically unusable in the real world without domain adaptation, for other types of data (such as depth camera images) this gap may be smaller ({\color{red} R1}, {\color{blue} R3}).

To test this, we trained BEVFusion's lidar-only model on both the SimBEV dataset and nuScenes, and evaluated both on the nuScenes \textit{val} set ({\color{blue} R3}). As shown in \cref{table:sim-to-real}, the former has an acceptable performance. The gap between the two models can be attributed to the limited variety of car and pedestrian models in CARLA, the fact that lidar intensity values cannot be replicated by CARLA and are not used by the first model, and differences in lidar noise pattern between CARLA and the real world.

\begin{table}[t]
    \centering
    \footnotesize
    \begin{tabular}{c c c}
        \toprule
        \textbf{Model} & \textbf{Car} & \textbf{Pedestrian} \\
        \toprule
        BEVFusion (SimBEV) & 39.5 & 30.2 \\
        BEVFusion (nuScenes) & \textbf{75.8} & \textbf{70.0} \\
        \bottomrule
    \end{tabular}
    \setlength{\belowcaptionskip}{-10 pt}
    \caption{mAP values (\%) for the lidar-only BEVFusion model evaluated on the nuScenes \textit{val} set.} \label{table:sim-to-real}
\end{table}

We hope that the SimBEV dataset can encourage further research into domain adaptation algorithms. Furthermore, as the dynamics of image-to-BEV transformation are similar in CARLA and in the real world, we think the accurate BEV ground truth provided by the SimBEV dataset can be helpful in the development of better depth estimation models for camera-to-BEV transformation. We will include this discussion in the final version of the paper ({\color{blue} R3}).

\end{document}
