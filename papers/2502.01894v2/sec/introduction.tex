\section{Introduction} \label{sec:intro}

Autonomous driving promises a future with safer, cleaner, more efficient and reliable transportation systems \cite{eskandarian2019research, wang2023multimodal}. As development of autonomous vehicle (AV) technology has accelerated in recent years, so has the need for perception algorithms capable of understanding complex driving scenarios in diverse environments \cite{zhao2023autonomous, cui2024survey}. High-quality driving datasets have been at the center of recent progress, serving as a foundation for training and benchmarking novel perception algorithms. It is vital for such datasets to encompass a wide variety of driving scenarios and encapsulate a diverse set of road types, weather conditions, and traffic patterns, so perception models can effectively generalize to real-world situations \cite{liu2024survey, song2023synthetic, guo2019safe}.

As essential in this context is multimodal sensor fusion, which enhances the performance of perception algorithms by compensating for the weaknesses of one modality with the strengths of others \cite{zhuang2021perception, chitta2022transfuser, xu2018pointfusion}. Sensor fusion improves an AV's understanding of its environment \cite{wang2023multisensor} (especially in adverse weather conditions \cite{bijelic2020seeing}), enables robust decision making in dynamic scenarios \cite{huang2020multi, shao2023safety}, and opens the door to perception models capable of performing multiple tasks simultaneously \cite{liu2022bevfusion, natan2022towards, phillips2021deep, huang2023fuller}. Consequently, it is imperative for driving datasets to support a wide array of sensors and perception tasks to facilitate the development of multifaceted perception systems that take advantage of the strengths of different sensing modalities.

Bird's-eye view (BEV) perception has attracted significant attention in recent years for two main reasons \cite{ma2024vision}. First, BEV representation is conducive to the fusion of information from different modalities, perspectives, and agents, and extracted BEV features can be used for various perception tasks. Second, BEV segmentation offers a concise, geometrically accurate, and semantically rich view of the environment, and can be used by motion planning, behavior prediction, and control algorithms. These two factors have led to the proliferation of perception algorithms that use BEV representation for 3D object detection, BEV segmentation, or both \cite{liu2022bevfusion, huang2023fuller, wang2023bi, xiong2023lxl, lin2024rcbevdet, jiao2023msmdfusion, li2022time3d, li2024unimode, zhang2023uni3d, xiong2023cape, man2023bev, wang2023unitr, yang2023bevformer, cai2023bevfusion4d, liu2023petrv2, mohapatra2021bevdetnet, gunn2024lift, liu2024seed, huang2021bevdet, wang2022detr3d, liang2022bevfusion, luo2022detr4d, li2024bevnext, zhao2024maskbev, dutta2022vit, gong2022gitnet, peng2023bevsegformer, xie2022m, xu2023cobevt, li2024fast, zhao2024improving}.

\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/Collage.pdf}
    \setlength{\abovecaptionskip}{-8 pt}
    \setlength{\belowcaptionskip}{-14 pt}
    \caption{A data sample generated by SimBEV. The left half depicts a 360-degree view of the ego vehicle's surroundings using different camera types (from top to bottom RGB, sematic segmentation, instance segmentation, depth, and optical flow cameras, respectively). On the right half, views of lidar, semantic lidar, radar, and the BEV ground truth are shown from top to bottom, respectively. Some images also contain 3D object bounding boxes colored according to the object's class.}\label{fig:dataset-collage}
\end{figure*}

Despite growing interest in BEV perception, few existing datasets support BEV segmentation. For the ones that do, either BEV ground truth is limited to static map elements (drivable area, pedestrian crossing, etc.) \cite{caesar2020nuscenes}, or BEV ground truth is only provided for a small window around the ego vehicle (obtained by combining map elements with 3D object bounding boxes \cite{houston2021one}) as objects further away may be occluded from the ego vehicle's view.

Creating a new dataset to fill this gap is a challenging endeavor. Real-world driving data require (at least in part) labor-intensive hand annotation and need to be collected over a long period of time to ensure that weather conditions and traffic patterns present in the dataset are diverse \cite{uricar2019challenges, liu2024survey}. On the other hand, synthetic driving data often consist of user-designed scenarios that in most cases do not capture the full diversity of the environment. Moreover, simply capturing the overhead view of the ego vehicle in either case may not be enough to obtain the BEV ground truth due to the presence of vegetation and other structures (traffic lights, bridges, etc.) that obstruct that view \cite{liu2025h}.

\begin{table*}[t]
    \centering
    \footnotesize
    % \setlength{\abovecaptionskip}{2 pt}
    % \setlength{\belowcaptionskip}{-12 pt}
    \begin{tabular}{c l c c c c c c c c}
        \toprule
         & \textbf{Dataset} & \textbf{Year} & \textbf{Scenes} & \textbf{Annotated frames} & \textbf{2D Det} & \textbf{3D Det} & \textbf{2D Seg} & \textbf{3D Seg} & \textbf{BEV Seg} \\
        \toprule
        \multirow{11}*{\rotatebox[origin=c]{90}{\textbf{Real-world}}} & KITTI \cite{geiger2013vision} & 2012 & 22 & 41K & \checkmark & \checkmark & - & - & - \\
         & Cityscapes \cite{cordts2016cityscapes} & 2016 & - & 25K & \checkmark & \checkmark & \checkmark & - & - \\
         & Mapillary \cite{neuhold2017mapillary} & 2017 & - & 25K & - & - & \checkmark & - & - \\
         & ApolloScape \cite{huang2018apolloscape} & 2018 & 103 & 144K & \checkmark & \checkmark & \checkmark & \checkmark & - \\
         & Argoverse \cite{chang2019argoverse} & 2019 & 113 & 22K & \checkmark & \checkmark & - & - & limited \\
         & Waymo Open \cite{sun2020scalability} & 2019 & 1150 & 230K & \checkmark & \checkmark & \checkmark & - & - \\
         & nuScenes \cite{caesar2020nuscenes} & 2019 & 1000 & 40K & \checkmark & \checkmark & \checkmark & \checkmark & limited \\
         & A*3D \cite{pham20203d} & 2020 & - & 39K & - & \checkmark & - & - & - \\
         & BDD100K \cite{yu2020bdd100k} & 2020 & 100K & 12M & \checkmark & - & \checkmark & - & - \\
         & Lyft Level 5 \cite{houston2021one} & 2021 & 366 & 46K & - & \checkmark & - & - & limited \\
         & Argoverse 2 \cite{wilson2023argoverse} & 2021 & 1000 & 6M & \checkmark & \checkmark & - & - & limited \\
        \midrule
        \multirow{6}*{\rotatebox[origin=c]{90}{\textbf{Synthetic}}} & SYNTHIA \cite{ros2016synthia} & 2016 & - & 13K & \checkmark & \checkmark & \checkmark & - & - \\
         & GTA-V \cite{richter2016playing} & 2016 & - & 25K & - & - & \checkmark & - & - \\
         & ViPER \cite{simon2005viper} & 2017 & - & 254K & \checkmark & \checkmark & \checkmark & - & - \\
         & All-in-One Drive \cite{weng2023all} & 2021 & 100 & 100K & \checkmark & \checkmark & \checkmark & \checkmark & - \\
         & SHIFT \cite{sun2022shift} & 2022 & 4850 & 2.5M & \checkmark & \checkmark & \checkmark & - & - \\
         & \textbf{SimBEV} & 2024 & 320 & 102K & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark \\
        \bottomrule
    \end{tabular}
    \caption{Comparison of the size and supported tasks of the most notable existing single-vehicle driving datasets. SimBEV is the only dataset that provides full support for BEV perception.} \label{table:dataset-comparison}
\end{table*}

To overcome these challenges, our paper makes two main contributions. First, we introduce SimBEV, a synthetic data generation tool based on CARLA Simulator \cite{dosovitskiy2017carla} that uses domain randomization to create diverse driving scenarios. SimBEV supports a comprehensive array of sensors and incorporates information from multiple sources to capture accurate BEV ground truth and 3D object bounding boxes. It enables a variety of perception tasks, including BEV segmentation and 3D object detection, making it an invaluable tool for computer vision researchers and helping accelerate the development of more capable autonomous driving systems. Second, we use SimBEV to create the SimBEV dataset, a comprehensive large-scale dataset that can serve as a benchmark for a variety of perception tasks. A data sample generated by SimBEV is shown in \cref{fig:dataset-collage}.