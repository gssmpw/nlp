\section{Evaluation and Analysis} \label{sec:eval}

The SimBEV dataset can be used for a variety of perception tasks, including 2D/3D segmentation, depth and optical flow estimation, and motion tracking and prediction. Here, we focus on BEV segmentation and 3D object detection.

\subsection{Tasks and metrics} \label{subsec:eval-metrics}

BEV segmentation results are evaluated using intersection over union (IoU), where for each class, a prediction is considered positive if its probability (score) is above a certain threshold (here 0.5). Our 3D object detection metrics are inspired by \cite{caesar2020nuscenes}, with average precision (AP) as the main metric. We consider two approaches to matching a predicted bounding box with a ground truth one. In the first, two boxes are matched if their 3D IoU is above a certain threshold \cite{geiger2013vision, cordts2016cityscapes}. In the second, two boxes are matched if the distance between their centers is below a certain threshold. As \cite{caesar2020nuscenes} notes, in the former, small translation errors for small objects (such as pedestrians) result in low or even zero IoU, making performance comparison of camera-only models that tend to have large localization errors difficult. More information about the metrics used for 3D object detection evaluation is available in the Supplementary Material.

\subsection{Evaluation results} \label{subsec:eval-results}

\begin{table*}[t]
    \centering
    \footnotesize
    \begin{tabular}{c c c c c c c c c c c}
        \toprule
        \textbf{Model} & \textbf{Modality} & \textbf{Road} & \textbf{Car} & \textbf{Truck} & \textbf{Bus} & \textbf{Motorcycle} & \textbf{Bicycle} & \textbf{Rider} & \textbf{Pedestrian} & \textbf{Mean} \\
        \toprule
        BEVFusion-C & C & 76.0 & 17.2 & 5.1 & 22.9 & 0.0 & 0.0 & 0.0 & 0.0 & 15.2 \\
        BEVFusion-L & L & 87.7 & 70.6 & \gb{73.5} & \rb{81.5} & 32.5 & \bb{3.6} & 18.4 & \bb{18.9} & \bb{48.3} \\
        BEVFusion & C + L & \bb{88.4} & \bb{72.7} & \rb{74.5} & \gb{80.0} & \gb{36.3} & \bb{3.6} & \bb{23.3} & \gb{20.0} & \rb{50.0} \\
        UniTR & C + L & \gb{92.8} & \rb{73.8} & 67.7 & 51.7 & \rb{36.5} & \rb{11.4} & \rb{36.2} & \rb{27.5} & \gb{49.7} \\
        UniTR+LSS & C + L & \rb{93.3} & \gb{72.8} & \bb{69.4} & \bb{58.5} & \bb{35.9} & \gb{6.3} & \gb{31.6} & 12.9 & 47.6 \\
        \bottomrule
    \end{tabular}
    \setlength{\abovecaptionskip}{4 pt}
    \setlength{\belowcaptionskip}{-4 pt}
    \caption{BEV segmentation IoUs (in \%) for different models evaluated on the SimBEV dataset \textit{test} set. The top three values are indicated in \rb{red}, \gb{green}, and \bb{blue}, respectively.} \label{table:seg-results}
\end{table*}
\begin{table*}[t]
    \centering
    \footnotesize
    \begin{tabular}{c c c c c c c c}
        \toprule
        \multirow{2}*{\textbf{Model}} & \multirow{2}*{\textbf{Modality}} & \textbf{mAP} & \textbf{mATE} & \textbf{mAOE} & \textbf{mASE} & \textbf{mAVE} & \textbf{SDS} \\
         & & \textbf{(\%) $\uparrow$} & \textbf{(m) $\downarrow$} & \textbf{(rad) $\downarrow$} & \textbf{$\downarrow$} & \textbf{(m/s) $\downarrow$} & \textbf{(\%) $\uparrow$} \\
        \toprule
        BEVFusion-C & C & 7.0 & 0.337 & 0.943 & 0.106 & 4.98 & 23.7 \\
        BEVFusion-L & L & 33.9 & \bb{0.105} & \gb{0.086} & 0.107 & 1.49 & 50.8 \\
        BEVFusion & C + L & \gb{34.1} & 0.107 & \rb{0.077} & \bb{0.101} & \bb{1.46} & \bb{51.0} \\
        UniTR & C + L & \bb{33.0} & \rb{0.081} & 0.140 & \gb{0.071} & \gb{0.51} & \gb{56.5} \\
        UniTR+LSS & C + L & \rb{34.2} & \gb{0.083} & \bb{0.131} & \rb{0.069} & \rb{0.49} & \rb{57.5} \\
        \bottomrule
    \end{tabular}
    \setlength{\abovecaptionskip}{4 pt}
    \setlength{\belowcaptionskip}{-4 pt}
    \caption{3D object detection results for different models evaluated on the SimBEV dataset \textit{test} set using the first (IoU-based) method. The top three values are indicated in \rb{red}, \gb{green}, and \bb{blue}, respectively.} \label{table:det-results-iou}
\end{table*}
\begin{table*}[ht!]
    \centering
    \footnotesize
    % \setlength{\tabcolsep}{4pt}
    \begin{tabular}{c c c c c c c c}
        \toprule
        \multirow{2}*{\textbf{Model}} & \multirow{2}*{\textbf{Modality}} & \textbf{mAP} & \textbf{mATE} & \textbf{mAOE} & \textbf{mASE} & \textbf{mAVE} & \textbf{SDS} \\
         & & \textbf{(\%) $\uparrow$} & \textbf{(m) $\downarrow$} & \textbf{(rad) $\downarrow$} & \textbf{$\downarrow$} & \textbf{(m/s) $\downarrow$} & \textbf{(\%) $\uparrow$} \\
        \toprule
        BEVFusion-C & C & 22.1 & 0.744 & 1.044 & 0.137 & 4.65 & 25.1 \\
        BEVFusion-L & L & \rb{48.1} & \gb{0.144} & \gb{0.133} & 0.134 & 1.56 & 56.4 \\
        BEVFusion & C + L & \rb{48.1} & \bb{0.146} & \rb{0.122} & \bb{0.127} & \bb{1.54} & \bb{56.6} \\
        UniTR & C + L & \bb{47.7} & \rb{0.113} & 0.224 & \gb{0.090} & \gb{0.55} & \gb{61.7} \\
        UniTR+LSS & C + L & \gb{47.8} & \rb{0.113} & \bb{0.207} & \rb{0.085} & \rb{0.53} & \rb{62.2} \\
        \bottomrule
    \end{tabular}
    \setlength{\abovecaptionskip}{4 pt}
    \setlength{\belowcaptionskip}{-14 pt}
    \caption{3D object detection results for different models evaluated on the SimBEV dataset \textit{test} set using the second (distance-based) method. The top three values are indicated in \rb{red}, \gb{green}, and \bb{blue}, respectively.} \label{table:det-results-distance}
\end{table*}

We benchmark BEVFusion \cite{liu2022bevfusion} and UniTR \cite{wang2023unitr} - both multi-sensor models for multi-task perception - on the SimBEV dataset. BEVFusion has camera-only (BEVFusion-C), lidar-only (BEVFusion-L), and fused (camera + lidar) variants for each task (six variants in total), allowing us to compare the performance of different modalities. BEVFusion-C is a variant of BEVDet-Tiny \cite{huang2021bevdet} using a much heavier view transformer, while BEVFusion-L is the lidar-only variant of TransFusion (TransFusion-L) \cite{bai2022transfusion}. UniTR, along with the base model for each task, has a variant augmented by an additional LSS-based BEV fusion step (four variants in total) \cite{liu2022bevfusion, liang2022bevfusion, philion2020lift}.

\Cref{table:seg-results} shows BEV segmentation IoUs (in \%) for different models evaluated on the SimBEV dataset \textit{test} set. As expected, all models achieve higher IoUs for larger objects compared to smaller ones (\textit{motorcycle}, \textit{bicycle}, \textit{rider}, and \textit{pedestrian}). In addition, the IoUs for the \textit{road} class (which is the only BEV segmentation class shared between SimBEV and nuScenes) are consistent with \cite{liu2022bevfusion}.

\Cref{table:seg-results} shows that BEVFusion outperforms BEVFusion-L only by a small margin, probably because of SimBEV's dense lidar point cloud. Notably, BEVFusion gets ahead when it comes to detecting smaller objects, probably because of the extra semantic information obtained from camera images. However, both models perform poorly when it comes to the \textit{bicycle} class, though we found that BEVFusion achieves a 12.7\% IoU for that class when the threshold is lowered to 0.4. It seems that because bicycles are always accompanied by a rider (and are smaller than motorcycles), the model has difficulty distinguishing between the two and has lower confidence in its predictions.

\Cref{table:seg-results} also shows that BEVFusion outperforms UniTR, because the latter significantly underperforms in the \textit{bus} class, even though it is much better at detecting smaller objects than the former. We think that this is likely because UniTR's transformer backbone is unable to effectively utilize information in the $z$ direction. We can also see from \cref{table:seg-results} that BEVFusion-C performs poorly (except for the \textit{road} class) compared to the others. As noted above, because images lack explicit geometric information, camera-only models have difficulty localizing objects.

3D object detection results using the first and second methods are shown in \cref{table:det-results-iou} and \cref{table:det-results-distance}, respectively. In contrast to nuScenes benchmarks \cite{caesar2020nuscenes, wang2023unitr}, BEVFusion slightly outperforms UniTR and UniTR+LSS in mAP here. However, the UniTR variants score a much higher SDS because they do a much better job at predicting object velocities. We can also see that the second matching method (distance-based) produces higher mAP values. This is due to its more permissive nature, where, unlike the first matching method, two boxes can be matched even if they do not intersect at all. This permissiveness, which makes BEVFusion-C more comparable to the others, can be seen when juxtaposing the mATE, mAOE, and mASE values of the two methods, with those for the second method considerably higher. A breakdown of the results by class is available in the Supplementary Material.

Finally, we should note that while CARLA and the real world are statistically different domains, our results and those of \cite{sun2022shift} indicate that trends in CARLA are compatible with real-world observations and SimBEV, with its accurate BEV ground truth, can be a useful tool for evaluating both novel perception methods and domain adaptation strategies.


