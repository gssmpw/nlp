\section{Related Work} \label{sec:related-work}

Real-world driving datasets often target specific subsets of perception tasks, as the high costs associated with data collection and labeling limit their scope. One of the oldest and most prominent driving datasets is the KITTI dataset \cite{geiger2013vision}, which supports depth estimation and 2D/3D object detection and tracking. Other notable image-based datasets include Cityscapes \cite{cordts2016cityscapes}, and Mapillary \cite{neuhold2017mapillary}, which are geared towards segmentation, while A*3D \cite{pham20203d} focuses on 3D object detection. More recently, large-scale datasets such as BDD100K \cite{yu2020bdd100k}, Waymo Open \cite{sun2020scalability}, ApolloScape \cite{huang2018apolloscape}, Argoverse 2 \cite{wilson2023argoverse}, and nuScenes \cite{caesar2020nuscenes} have emerged, offering multi-modal data and multi-task annotations but primarily emphasizing object detection and tracking.

Synthetic driving datasets are compiled using graphics engines and physics simulators. For example, SYNTHIA \cite{ros2016synthia} incorporates RGB and semantically segmented images generated by its dedicated simulator. Video games have also served as a source of data. For instance, GTA-V \cite{richter2016playing} offers RGB and semantically segmented images extracted from GTA. ViPER \cite{simon2005viper} expands on GTA-V by including optical flow images and discrete environmental labels. The introduction of CARLA \cite{dosovitskiy2017carla} fostered systemic generation of driving datasets. All-in-One Drive \cite{weng2023all} is one such dataset, providing support for multiple sensors with a focus on simulating SPAD (Single-Photon Avalanche Detector)-lidars. Another is SHIFT \cite{sun2022shift}, a large-scale multi-task multi-modal dataset for autonomous driving, designed to simulate discrete and continuous changes in weather and traffic conditions to evaluate domain adaptation strategies.

Existing datasets offer limited support for BEV segmentation. In nuScenes \cite{caesar2020nuscenes}, BEV segmentation is only supported for static map elements (drivable area, lane line, pedestrian crossing, etc.). In Lyft Level 5 \cite{houston2021one} and Argoverse \cite{chang2019argoverse, wilson2023argoverse}, BEV ground truth is obtained by combining map elements and vehicle bounding boxes observable by the ego vehicle's perception sensors, limiting BEV ground truth area and/or missing occluded objects.

Some vehicle-to-everything (V2X) datasets provide limited support for BEV segmentation as well. H-V2X \cite{liu2025h} captures the BEV ground truth using overhead cameras installed along a 100 km highway, with data limited to highway driving and mostly suitable for highway motion forecasting. CARLA-based OPV2V \cite{xu2022opv2v} dataset provides the BEV ground truth, but only for the drivable area, lane line, and vehicle classes, and the one for drivable area can be inaccurate due to its sole reliance on CARLA-generated waypoints. Finally, CARLA-based V2X-Sim \cite{li2022v2x} dataset captures the BEV ground truth for several classes using an overhead camera, which can be inaccurate due to the presence of vegetation, traffic light poles, and other structures that obstruct the overhead view.