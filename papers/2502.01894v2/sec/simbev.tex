\section{SimBEV} \label{sec:simbev}

\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/SimBEV.pdf}
    \setlength{\abovecaptionskip}{-8 pt}
    \setlength{\belowcaptionskip}{-8 pt}
    \caption{SimBEV's logic flow when creating a new dataset. The arrow exiting green nodes at the top indicates the action taken when the condition in that node is no longer satisfied.}\label{fig:simbev}
\end{figure*}

SimBEV relies on CARLA 0.9.15 \cite{dosovitskiy2017carla} equipped with a custom content library (see the Supplementary Material) to simulate the environment, perception sensors, and traffic behavior, although it is compatible with the standard release of CARLA as well. SimBEV streamlines, automates, and manages the entire data collection process for the user (who controls it through a single configuration file) by manipulating simulation elements through CARLA's Python API. It equips the user with CARLA's customizability when necessary and takes charge when not. This flexibility and ease of use enables researchers to quickly create custom datasets that suit their needs.

\subsection{Design} \label{subsec:simbev-overview}

SimBEV works by randomizing (within some bounds) as many simulation parameters as possible to create a statistically diverse yet realistic set of scenarios. To create a dataset, SimBEV generates and collects data from consecutive episodes, or scenes, each with a unique configuration. 

SimBEV's logic flow is shown in \cref{fig:simbev}. To start, the user configures the desired number of scenes for each map (i.e., the driving environment, can be an existing CARLA map or a custom one) for the training, validation, and test sets. SimBEV checks to see if a SimBEV dataset already exists. If so, it subtracts the number of existing scenes in that dataset from the number of desired scenes for each map. This lets the user expand an already existing SimBEV dataset, or continue with dataset creation in the event of a crash. SimBEV also lets the user replace individual scenes.

At the start of each scene, SimBEV creates uniformly distributed waypoints a certain distance (specified by the user) apart from each other across the map's roads. It selects one at random and spawns the ego vehicle and attached sensors there, though the user has the option to specify a set of spawn coordinates that SimBEV must choose from instead. SimBEV then configures the weather randomly and, if at night, changes the intensity of street lights at random.

For background traffic, SimBEV randomly selects the desired number of vehicles and pedestrians, although the user can specify each. SimBEV then uses the waypoints mentioned earlier to spawn random vehicles at random locations, and spawns pedestrians randomly on walkable areas of the map. CARLA's traffic manager controls the behavior of vehicles and pedestrians throughout the scene.

Because all vehicles and pedestrians start from rest, SimBEV runs the simulation for a user-specified period of time (called warm-up duration) to reach a more realistic state, before collecting data for a user-specified period of time. During that period, SimBEV saves data from the desired sensors at each time step, and calculates and saves both 3D object bounding boxes and the BEV ground truth. Following that, it saves meta-information about the collected data and a log of the scene, destroys the vehicles, pedestrians, and sensors, and moves on to the next scene.

\subsection{Sensors} \label{subsec:simbev-sensors}

SimBEV supports a variety of sensors available in CARLA, including five different camera types (RGB, semantic segmentation, instance segmentation, depth, and optical flow), two different lidar types (regular and semantic), radar, GNSS, and IMU, as shown in \cref{fig:dataset-collage}. The user has full control over each sensor type's parameters (e.g. a camera's resolution or FoV), but the placement of sensors is fixed for now. Similar to \cite{caesar2020nuscenes}, cameras are placed at six locations above the vehicle to offer a 360-degree view of the vehicle's surroundings, while a radar is placed on each of the four sides of the vehicle. The GNSS and IMU are placed at the center of the vehicle (the origin of the vehicle's coordinate system), and a lidar is placed high above that center.

\subsection{Scene configuration} \label{subsec:simbev-config}

As will be discussed in what follows, numerous parameters are randomized for each scene to ensure that generated scenes are as unique and diverse as possible.

\paragraph{Weather.} \label{par:weather}

Weather in CARLA is controlled using several parameters such as fog density, sun altitude angle, wind intensity, etc. By default, SimBEV randomly selects these parameters for each scene to create diverse weather conditions (subject to some constraints to ensure the realism of the weather), but the user also has the option to set any of the parameters to a fixed value. For instance, setting the sun altitude angle to anything less than zero creates a dataset of night-time scenes.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{images/reckless.jpg}
    \setlength{\abovecaptionskip}{-8 pt}
    \setlength{\belowcaptionskip}{-8 pt}
    \caption{In a scene generated by SimBEV, a reckless ego vehicle runs over a cyclist.}\label{fig:reckless}
\end{figure}

\paragraph{Traffic.} \label{par:traffic}

SimBEV randomly selects background vehicles from CARLA's vehicle library, which includes sedans, vans, trucks, heavy goods vehicles (HGVs), buses, bicycles, motorcycles, and emergency vehicles, whose emergency lights are turned on randomly. When possible, vehicle colors are selected at random from a vast set of available colors (e.g. a sedan can change colors but a firetruck will only be red). Some vehicles support articulated doors, so when these vehicles come to a stop - e.g. at a traffic light - SimBEV may randomly open one or all of their doors.

SimBEV randomly chooses each vehicle's maximum speed (relative to the speed limit, e.g. 10\% over/under) and how close vehicles can get to each other when coming to a stop. It also randomly selects how long each traffic light stays green. However, the user always has the option to set any of these parameters to a fixed value.

SimBEV randomly chooses pedestrians from CARLA's walker library (which contains models of different age, gender, race, and body type), sets their walking speed at random, and gives each a random destination to go to.

\paragraph{Lights.} \label{par:lights}

SimBEV gives the user the option to turn off all street and/or building lights at night. It also lets the user randomize building light colors, and/or change the intensity of all street lights by a fixed, if desired random, value. In addition, SimBEV randomly turns off individual street lights based on a probability set by the user to simulate broken street lights in the real world.

\paragraph{Reckless driving and jaywalking.} \label{par:reckless}

If desired by the user, some vehicles (including the ego vehicle) can drive recklessly, ignoring traffic lights, traffic signs, and collisions with other vehicles and pedestrians, as shown in \cref{fig:reckless}. The user controls the likelihood of reckless driving, which can result in interesting edge cases. The user also has control over the share of pedestrians allowed to jaywalk and cross the road at any point, not just at crosswalks.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{images/invis-road.jpg}
    \setlength{\abovecaptionskip}{-8 pt}
    \setlength{\belowcaptionskip}{-8 pt}
    \caption{Ground elements (roads, sidewalks, etc.) in CARLA use one-way visible materials, appearing invisible to a camera placed below them. We use this property to capture accurate BEV ground truth by placing a camera below the ego vehicle looking up.}\label{fig:invis-road}
\end{figure}

\subsection{Data annotation} \label{subsec:simbev-gt}

SimBEV offers two main types of data annotation: 3D object bounding boxes and BEV ground truth. The output of some perception sensors such as segmentation, depth, and optical flow cameras and semantic lidar can serve as annotation as well, but we do not discussed them here.

\paragraph{3D object bounding boxes.} \label{par:bev-bbox}

At each time step, SimBEV collects 3D object bounding boxes that are within a user-configurable radius of the ego vehicle for the following six classes: \textit{car}, \textit{truck} (includes trucks, vans, HGVs, etc., but not buses), \textit{bus}, \textit{motorcycle}, \textit{bicycle}, and \textit{pedestrian}. Other object attributes are also collected alongside each bounding box, such as the object's ID, its linear and angular velocity, and its make, model, and color if the object is a vehicle. An optional post-processing step calculates the number of lidar and radar points that fall within each bounding box and adds a \textit{valid} label to boxes with at least one point inside, \textit{invalid} otherwise. This labeling is useful for training 3D object detection algorithms, as it can filter out objects that may not be visible to perception sensors \cite{caesar2020nuscenes}.

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.32\linewidth]{images/RoadWaypointC.jpg}
    \includegraphics[width=0.32\linewidth]{images/RoadCameraC.jpg}
    \includegraphics[width=0.32\linewidth]{images/RoadCombinedC.jpg}
    \setlength{\abovecaptionskip}{8 pt}
    \setlength{\belowcaptionskip}{-8 pt}
    \caption{Left: BEV road data calculated using CARLA-generated waypoints; there are clear gaps where lanes diverge. Middle: BEV road data obtained from the overhead camera; vehicles and vegetation obstruct a portion of the view. Right: BEV \textit{road} ground truth obtained by combining the two sources and performing \textit{binray closing}.}\label{fig:road-mask}
\end{figure*}

\paragraph{BEV ground truth.} \label{par:bev-gt}

SimBEV supports the following eight classes for BEV segmentation: \textit{road}, \textit{car}, \textit{truck}, \textit{bus}, \textit{motorcycle}, \textit{bicycle}, \textit{rider} (human on a \textit{motorcycle} or \textit{bicycle}), and \textit{pedestrian}. At each time step, the BEV ground truth is saved as a $C \times l \times l$ binary array, where $C$ is the number of classes and $l$ is the dimension of the BEV grid that is centered on the ego vehicle.

To calculate the BEV ground truth for non-\textit{road} classes, we take advantage of the fact that ground elements in CARLA (roads, sidewalks, etc.) use one-way visible materials, appearing solid from one direction and see-through from the opposite, as shown in \cref{fig:invis-road}. This means that we can place a semantic segmentation camera 1 km above the ego vehicle facing down (far enough to minimize perspective distortion) and another 1 km below the ego vehicle facing up to catch what the overhead camera cannot see due to obstructions. Both cameras have a $l \times l$ resolution and their field of view (FoV) is calculated so that each pixel represents a $d \times d$ area on the ground. The BEV ground truth for each non-\textit{road} class is obtained by merging data from the two cameras using a \textit{logical or} operation. By default, $l$ is set to 360 and $d$ is set to 0.4 m, creating a 144 m $\times$ 144 m box around the ego vehicle. This area is larger than what is typically used for BEV segmentation (100 m $\times$ 100 m), but it can help with data augmentation (rotation, translation, scaling) during training.

We follow an approach similar to \cite{xu2022opv2v} to obtain the ground truth for the \textit{road} class. Specifically, we use CARLA-generated waypoints a small distance apart from each other (specified by the user, we recommend setting it to $d$) across the map's roads and note each waypoint's lane width. We then calculate the mutual distance between these waypoints and the center of each cell of a $l \times l$ BEV grid that is centered on the ego vehicle, where each cell represents a $d \times d$ area. For each grid cell, if a waypoint exists whose distance to the center of that cell is less than that waypoint's lane width, that cell is labeled as \textit{road}. Where our approach differs from \cite{xu2022opv2v} is that we then combine this information with data from the overhead camera and perform \textit{binary closing} to patch any potential gaps, obtaining a much more accurate ground truth. This process is illustrated in \cref{fig:road-mask}.

In general, our method allows us to assign multiple labels to the same cell. For example, a cell occupied by a cyclist will have a \textit{rider} (obtained from the overhead camera), a \textit{bicycle} (obtained from the underground camera), and a \textit{road} (calculated using CARLA-generated waypoints) label.

Our approach works everywhere except when roads with large elevation differences are near the ego vehicle, e.g. when the ego vehicle is traveling under an overpass. In those situations, we do not use the overhead or underground cameras. Instead, we rely on CARLA-generated waypoints to calculate the BEV ground truth for the \textit{road} class and use 3D object bounding boxes to calculate the BEV ground truth for other classes. Although not as accurate as our overall approach, the resulting ground truth is still acceptable. SimBEV switches to this method when it detects two waypoints within 48.0 m of each other that have an elevation difference greater than 6.4 m, signaling that they are on two different roads.

