\section{Related Work}
\subsection{Text-to-Image Generation}
The U-Net-based Stable Diffusion models~\cite{Rombach_2022_CVPR,podell2023sdxl} first perform universal image generation from text prompts, which typically trained on large scale text-image paired dataset, \aka, LIANG 5B~\cite{schuhmann2022laion}. After the proposal of the Diffusion Transformer (DiT)~\cite{peebles2023scalable}, some research, such as Stable Diffusion V3~\cite{esser2024scaling} and Flux~\cite{flux}, utilize DiT as the backbone to develop DiT-based Diffusion Models for text-to-image generation. Recently, with the success of auto-regressive (AR) modeling in the natural language processing~\cite{openai2023gpt4,touvron2023llama}, some works have explored how to combine auto-regressive models with diffusion models to improve the understanding capability and further build a unified multi-modal model for both understanding and generation~\cite{chen2024diffusion,xie2024show,zhou2024transfusion}. These AR-based models, such as OmniGen~\cite{xiao2024omnigen} and Emu~\cite{sun2024generative}, also show notable performance on the text-to-image task. While these methods have achieved strong performance in text-to-image generation, they store all the knowledge in their pre-trained parameters, which leads to hallucinations and distortions when generating realistic objects. To address this limitation, we propose the real-object-based RAG framework to integrate missing knowledge and improve the ability to generate realistic images.

\subsection{Retrieval-augmented Generation}
Retrieval-augmented generation has shown promise with NLP~\cite{lewis2020retrieval,guu2020retrieval}. To incorporate external knowledge into a LLM~\cite{gao2023retrieval,jiang2023active}, these methods retrieve documents relevant to inputs from an external database, subsequently, the LLM utilizes the recalled documents as references to generate accurate results. The external knowledge used is typically a text database~\cite{hashimoto2018retrieve,khandelwal2019generalization,shi2023replug}. However, the text database is not direct and controllable for realistic image generation~\cite{blattmann2022retrieval}. In this paper, we conduct a vision-based, real-object-based database, which is collected by realistic images from public real-world datasets, including ImageNet~\cite{deng2009imagenet}, Stanford Cars~\cite{krause20133d}, Stanford Dogs~\cite{dataset2011novel}, and Oxford Flowers~\cite{nilsback2008automated}. In this way, we augment the realism of the generative images with the real-object-based database.
% with the real-object database.

\subsection{Contrastive Learning for Retrieval}
Contrastive learning has emerged as a powerful method for retrieval tasks, leveraging the principle of learning representations by contrasting positive and negative samples~\cite{khosla2020supervised,le2020contrastive}. The approach aims to map semantically similar data points closer in the embedding space while simultaneously pushing dissimilar data points apart. Methods such as SimCLR~\cite{chen2020simple} and MoCo~\cite{he2020momentum} have popularized this framework in vision tasks, whereas in the domain of multi-modal retrieval, models including CLIP~\cite{radford2021learning} have demonstrated their effectiveness by aligning textual and visual representations. Recent research has extended contrastive learning to various modalities, such as audio~\cite{radford2021learning,sun2023learning,likhosherstov2021polyvit,guzhov2022audioclip,mahmud2023ave,girdhar2023imagebind}, video~\cite{huang2023clover,fang2021clip2video,luo2022clip4clip,xue2022clip,zhu2023languagebind}, point cloud~\cite{zhang2022pointclip,zhu2022pointclip,huang2022clip2point,guo2023point}, and tactile data~\cite{yang2024binding, lei2024vit}, thereby enhancing cross-modal retrieval capabilities. These methods often incorporate techniques (e.g., hard negative mining~\cite{kalantidis2020hard,robinson2020contrastive} and balanced learning~\cite{zhu2022balanced,liu2022universal}) to improve the quality of the learned embedding space.
Despite its success in query-document matching, images that best match a text prompt may not be the most valuable references for text-to-image generative models~\cite{zhang2021cross}. Consequently, we propose a self-reflective contrastive learning approach, which retrieves images containing the missing knowledge of the generative models rather than selecting the most relevant images.