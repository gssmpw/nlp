%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{graphicx}
% \usepackage[table]{xcolor}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

\newcommand{\aka}{\textit{a.k.a.}}
\newcommand{\awa}{\textit{a.w.a.}}
\newcommand{\eg}{\textit{e.g.}}
\newcommand{\ie}{\textit{i.e.}}
% \usepackage[table]{xcolor}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{RealRAG: Retrieval-augmented Realistic Image Generation via Self-reflective Contrastive Learning}

\begin{document}

\twocolumn[
\icmltitle{RealRAG: Retrieval-augmented Realistic Image Generation via \\ Self-reflective Contrastive Learning}
% \icmltitle{Learn and Retrieve `What Really Augments" Image Synthesis via Self-negative Contrastive Learning}
% Bootstrapping Realistic Image Generation with Retrieval-augmented Self-negative Contrastive Learning

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
% \icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Yuanhuiyi Lyu}{hkustgz}
\icmlauthor{Xu Zheng}{hkustgz}
\icmlauthor{Yibo Yan}{hkustgz}
\icmlauthor{Lutao Jiang}{hkustgz}
\icmlauthor{Xin Zou}{hkustgz}
\icmlauthor{Huiyu Zhou}{gxi} \\
\icmlauthor{Linfeng Zhang}{sjtu}
\icmlauthor{Xuming Hu}{hkustgz,hkust}
\end{icmlauthorlist}

\icmlaffiliation{hkustgz}{The Hong Kong University of Science and Technology (Guangzhou)}
\icmlaffiliation{hkust}{The Hong Kong University of Science and Technology}
\icmlaffiliation{sjtu}{Shanghai Jiao Tong University}
\icmlaffiliation{gxi}{Guangxi Zhuang Autonomous Region Big Data Research Institute}
% \icmlcorrespondingauthor{Linfeng Zhang}{zhanglinfeng@sjtu.edu.cn}
% \icmlcorrespondingauthor{Xuming Hu}{xuminghu@hkust-gz.edu.cn}
% \icmlcorrespondingauthor{Firstname2 Lastname2}{first2.last2@www.uk}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
% \icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
% \printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
Recent text-to-image generative models, \eg, Stable Diffusion V3 and Flux, have achieved notable progress. 
However, these models are strongly restricted to their limited knowledge, \aka, their own fixed parameters, that are trained with closed datasets. This leads to significant hallucinations or distortions when facing fine-grained and unseen novel real-world objects, \eg, the appearance of the Tesla Cybertruck. To this end, we present \textbf{the first} real-object-based retrieval-augmented generation framework (\textbf{RealRAG}), which augments fine-grained and unseen novel object generation by learning and retrieving real-world images to overcome the knowledge gaps of generative models. Specifically, to integrate missing memory for unseen novel object generation, we train a reflective retriever by \textbf{self-reflective contrastive learning}, which injects the generator's knowledge into the sef-reflective negatives, ensuring that the retrieved augmented images compensate for the model's missing knowledge. Furthermore, the real-object-based framework integrates fine-grained visual knowledge for the generative models, tackling the distortion problem and improving the realism for fine-grained object generation. Our Real-RAG is superior in its modular application to \textbf{all types} of state-of-the-art text-to-image generative models and also delivers \textbf{remarkable} performance boosts with all of them, such as a \textbf{gain of \textit{16.18\%}} FID score with the auto-regressive model on the Stanford Car benchmark.
% \textit{The project code will be available upon acceptance.}
\end{abstract}

\begin{figure}[t!]
    \centering
    \includegraphics[width=\linewidth]{figs/teaser_fig.pdf}
    \vspace{-10pt}
    \caption{\textbf{(a)} The pipeline of text-to-image generative models. \textbf{(b)} The framework of existing retrieval-augmented methods. \textbf{(c)} The framework of our proposed RealRAG.}
    \vspace{-10pt}
    \label{tease_fig}
\end{figure}





\section{Introduction}
% literature review of AR-based, U-Net-based, DiT-based models and the limitation
Recent text-to-image generators have achieved notable progress in image synthesis from the given textual prompts. There are three mainstream types of generative models, including the U-Net-based diffusion model~\cite{Rombach_2022_CVPR, podell2023sdxl}, the DiT-based diffusion model~\cite{xiao2024omnigen, sun2024generative}, and the auto-regressive model~\cite{esser2024scaling, flux}. Typically, these models store all their visual memory (\eg, the appearance of Big Ben) implicitly in the parameters of the underlying neural network, requiring a lot of parameters(\eg, 10B).
Furthermore, similar to the hallucination problem of Large Language Models (LLMs)~\cite{openai2023gpt4,touvron2023llama}, the large-scale text-to-image generative models also show the same problem. Some generated images include ghosting, distortions, and unnatural elements when generating specific real-world objects.
Therefore, these problems motivate the development of text-to-image generation models, which can integrate external visual knowledge (\eg, images from the web) to augment generative realism and accuracy for fine-grained and unseen novel object generation. 

% literature review of RAG and challenges in t2i generation (with teaser figure)
Retrieval-augmented generation (RAG) has shown promise in natural language processing (NLP)~\cite{gao2023retrieval}. 
To enhance the specific knowledge and minimize the hallucination of LLMs, a retrieval model first retrieves the most relevant documents for the input prompts (\eg, the questions), and then LLMs generate predictions powered by the recalled documents. However, in text-to-image generation, these RAG methods, which rely primarily on similarity matching, remain challenging. Specifically, as shown in Fig.~\ref{tease_fig}\textbf{ (b)} and \textbf{(c)}, the candidate image \textbf{(1)}, with the highest similarity score of the text prompt, fails to improve the image generation. In contrast, candidate image \textbf{(2)}, despite having a lower similarity score, complements the missing knowledge and augments the image generation. 
%although it has the highest similarity score with the text prompt—fails to \textbf{improve} image generation. In contrast, candidate image \textbf{(2)}, despite having a lower similarity score, complements the \textbf{missing information} in the generative model and thereby \textbf{enhances} the generated output.
% In this work, we train the reflective retriever by self-reflective contrastive learning, to recall the augmented images with missing knowledge instead of the most relevant one.
In this work, we propose a reflective retriever trained via self-reflective contrastive learning, \textbf{aimed at retrieving images with missing knowledge instead of the most relevant one}.


% In this work
% In this paper, we present the first \textit{real-object-based retrieval augmented generation framework (\textbf{RealRAG})}, which learns and recalls real-world images with the missing memory of specific generative models to augment realistic image generation. The core insight of our proposed RealRAG is to augment the realism and minimize the hallucination for the text-to-image generative models by the images with missing memory, which are recalled by the retriever training with self-reflective contrastive learning. Our UniBind is superior in its flexible application to all types of text-to-image generative models and delivers remarkable performance boosts (+15.00\% with Flux on the Stanford Cars benchmark).
In this paper, we present the \textbf{first} real-object-based retrieval-augmented generation framework (RealRAG), which leverages real-world images to compensate for the missing knowledge inherent in generative models and improve realistic image generation. \textit{The core insight of RealRAG is to enhance realism and reduce hallucination in text-to-image generative models powered by the reflective retriever, which is trained via self-reflective contrastive learning.} In addition, our proposed RealRAG demonstrates \textbf{remarkable flexibility} and can be applied across diverse text-to-image generative models, yielding significant performance gains, \eg, a \textbf{\textit{+16.18\% gain}} with Emu~\cite{sun2024generative} on the Stanford Cars benchmark~\cite{krause20133d}.

% Technical design
Specifically, we \textbf{first} generate images from the given text prompts using the specific text-to-image generative models. Given these generated images, we then sample the reflective negatives in the image database by selecting images with the highest cosine similarity of the generated images. In this way, these sampled negatives are similar to the generated images that store the generative models' visual memory. \textbf{Second}, we train the \textit{reflective retriever} with self-reflective contrastive learning, which utilizes text prompts as positives and trains with the sampled reflective negatives. In this way, the well-trained retriever can recall the prompt-relevant images also with missing knowledge for the generative models. For example, as shown in Fig.~\ref{tease_fig}, our RealRAG retrieves the image {\fontfamily{qcr}\selectfont ["Cybertruck"]}, instead of the image with highest similarity ({\fontfamily{qcr}\selectfont["A truck is speeding along the Great Wall"]}), which integrates missing knowledge for generative models. 

% Performance
We apply our \textbf{RealRAG} to \textit{all types of state-of-the-art text-to-image generative models}, including U-Net-based diffusion models (SD V2.1~\cite{Rombach_2022_CVPR}, SD XL~\cite{podell2023sdxl}), DiT-based diffusion models (SD V3~\cite{esser2024scaling}, Flux~\cite{flux}), and auto-regressive models (OmniGen~\cite{xiao2024omnigen}, Emu~\cite{sun2024generative}). Note that, our RealRAG is the first work to build \textbf{a unified RAG framework for all types of text-to-image generative models}. Our RealRAG consistently delivers significant performance improvements with all these models, such as \textit{\textbf{+5.77\%}} on the Stanford Cars benchmark with Flux~\cite{flux} and \textit{\textbf{+20.48\%}} on the Oxford Flowers benchmark with Emu~\cite{sun2024generative}. 
Additionally, to evaluate the capability of unseen novel object generation, we collect recent novel objects from news web pages and construct human evaluations. Our RealRAG demonstrates a substantial performance improvement in human evaluation. \textit{Code will be available upon acceptance.}



\begin{figure*}[t!]
    \centering
    \includegraphics[width=\textwidth]{figs/overall.pdf}
    \vspace{-20pt}
    \caption{An overview of our RealRAG. \textbf{(a)} The pipeline of the real-object-based RAG. We propose the first real-object-based retrieval-augmented generation framework, which leverages real-world images to compensate for the knowledge gap inherent in generative models and augment realistic image generation, \textbf{(b)} The framework of self-reflective contrastive learning, which injects the generator's knowledge into the self-reflective negatives ensuring that the retrieved images compensate for the model's missing knowledge.}
    % \vspace{-6pt}
    \label{overall}
\end{figure*}

% \vspace{-4pt}
\section{Related Work}
\subsection{Text-to-Image Generation}
The U-Net-based Stable Diffusion models~\cite{Rombach_2022_CVPR,podell2023sdxl} first perform universal image generation from text prompts, which typically trained on large scale text-image paired dataset, \aka, LIANG 5B~\cite{schuhmann2022laion}. After the proposal of the Diffusion Transformer (DiT)~\cite{peebles2023scalable}, some research, such as Stable Diffusion V3~\cite{esser2024scaling} and Flux~\cite{flux}, utilize DiT as the backbone to develop DiT-based Diffusion Models for text-to-image generation. Recently, with the success of auto-regressive (AR) modeling in the natural language processing~\cite{openai2023gpt4,touvron2023llama}, some works have explored how to combine auto-regressive models with diffusion models to improve the understanding capability and further build a unified multi-modal model for both understanding and generation~\cite{chen2024diffusion,xie2024show,zhou2024transfusion}. These AR-based models, such as OmniGen~\cite{xiao2024omnigen} and Emu~\cite{sun2024generative}, also show notable performance on the text-to-image task. While these methods have achieved strong performance in text-to-image generation, they store all the knowledge in their pre-trained parameters, which leads to hallucinations and distortions when generating realistic objects. To address this limitation, we propose the real-object-based RAG framework to integrate missing knowledge and improve the ability to generate realistic images.

\subsection{Retrieval-augmented Generation}
Retrieval-augmented generation has shown promise with NLP~\cite{lewis2020retrieval,guu2020retrieval}. To incorporate external knowledge into a LLM~\cite{gao2023retrieval,jiang2023active}, these methods retrieve documents relevant to inputs from an external database, subsequently, the LLM utilizes the recalled documents as references to generate accurate results. The external knowledge used is typically a text database~\cite{hashimoto2018retrieve,khandelwal2019generalization,shi2023replug}. However, the text database is not direct and controllable for realistic image generation~\cite{blattmann2022retrieval}. In this paper, we conduct a vision-based, real-object-based database, which is collected by realistic images from public real-world datasets, including ImageNet~\cite{deng2009imagenet}, Stanford Cars~\cite{krause20133d}, Stanford Dogs~\cite{dataset2011novel}, and Oxford Flowers~\cite{nilsback2008automated}. In this way, we augment the realism of the generative images with the real-object-based database.
% with the real-object database.

\subsection{Contrastive Learning for Retrieval}
Contrastive learning has emerged as a powerful method for retrieval tasks, leveraging the principle of learning representations by contrasting positive and negative samples~\cite{khosla2020supervised,le2020contrastive}. The approach aims to map semantically similar data points closer in the embedding space while simultaneously pushing dissimilar data points apart. Methods such as SimCLR~\cite{chen2020simple} and MoCo~\cite{he2020momentum} have popularized this framework in vision tasks, whereas in the domain of multi-modal retrieval, models including CLIP~\cite{radford2021learning} have demonstrated their effectiveness by aligning textual and visual representations. Recent research has extended contrastive learning to various modalities, such as audio~\cite{radford2021learning,sun2023learning,likhosherstov2021polyvit,guzhov2022audioclip,mahmud2023ave,girdhar2023imagebind}, video~\cite{huang2023clover,fang2021clip2video,luo2022clip4clip,xue2022clip,zhu2023languagebind}, point cloud~\cite{zhang2022pointclip,zhu2022pointclip,huang2022clip2point,guo2023point}, and tactile data~\cite{yang2024binding, lei2024vit}, thereby enhancing cross-modal retrieval capabilities. These methods often incorporate techniques (e.g., hard negative mining~\cite{kalantidis2020hard,robinson2020contrastive} and balanced learning~\cite{zhu2022balanced,liu2022universal}) to improve the quality of the learned embedding space.
Despite its success in query-document matching, images that best match a text prompt may not be the most valuable references for text-to-image generative models~\cite{zhang2021cross}. Consequently, we propose a self-reflective contrastive learning approach, which retrieves images containing the missing knowledge of the generative models rather than selecting the most relevant images.



\section{Methodology}

\noindent \textbf{Problem Setting: }
Given a textual prompt $T$ drawn from a text space $\mathcal{T}$, the text-to-image generation task aims to produce a corresponding image $I \in \mathcal{I}$ that accurately reflects the semantics of $T$. Formally, we model this task as learning a conditional distribution $p(I \mid T)$, which describes how likely an image $I$ is given the text $T$. In practice, we often parameterize this distribution with a neural generator $G_\theta$ (with parameters~$\theta$), yielding:
\begin{equation}
    P_\theta(I \mid T) \;\approx\; P(I \mid T),
\end{equation}
given a new text prompt $T$, the learned generator $G_\theta$ generate an image:
\begin{equation}
    \hat{I} \;\sim\; P_\theta\bigl(I \mid T\bigr),
\end{equation}
Alternatively, one might produce a deterministic output by taking, for example, the mode of $P_\theta(I \mid T)$. In either case, the goal is to ensure that $\hat{I}$ visually manifests the semantics conveyed by $T$.

\noindent \textbf{Overview: }
An overview of RealRAG is shown in Fig.~\ref{overall}. Specifically, as shown in Fig.~\ref{overall} (b), we first train the reflective retriever via self-reflective contrastive learning (\textbf{Sec.~\ref{Self-reflective}}). Powered by self-reflective contrastive learning, the reflective retriever retrieves images with the generator's missing knowledge. Subsequently, as shown in Fig.~\ref{overall} (a), we utilize the recalled image as the reference for the realistic image generation, which effectively addresses the text-to-image generation's hallucinations (\textbf{Sec.~\ref{Real-object-based}}). We now describe these technical components in detail. 


\subsection{Self-reflective Contrastive Learning}
\label{Self-reflective}
We propose self-reflective contrastive learning to retrieve useful images for the generator. \textbf{Our key insight} is to train a retriever that \textit{retrieves images staying off the generation space of the generator, yet closing to the representation of text prompts.} To this end, as shown in Fig.~\ref{overall} (b), we first generate images from the given text prompts and then utilize the generated images as queries to retrieve the most relevant images in the real-object-based database. These most relevant images are utilized as reflective negatives. 
Concretely, we denote the generator as $G$, given a text prompt $T$, we generate image $I_{gen}$ and extract its feature embedding $Z_{gen}$ is by:
\begin{equation}
    I_{gen} = G(T), \quad Z_{gen} = F_{i}(I_{gen}),
\end{equation}
where $F_{i}$ is the vision encoder for extracting visual embedding from input images. 
We then select the self-reflective negative by ranking the images from the real-object-based database $\mathcal{I}$ with the cosine-similarity. We choose the highest one:
\begin{equation}
    I_{neg} 
    = 
    \underset{ I_{obj}^k \in \mathcal{I} }{\arg\max} 
    \ \text{sim}\Big( F_i(I_{obj}^k), Z_{gen} \Big).
\end{equation}
The reflective negative, with the highest cosine-similarity score, includes the original knowledge of the generator and empowers the retriever to capture the real-object-based images with missing knowledge of the generator.
For training the retriever via self-reflective contrastive learning, we first extract object feature embeddings $\{ Z_{obj}^{1}, Z_{obj}^{2}, ..., Z_{obj}^{n}\}$ for the real-object-based images $\{ I_{obj}^{1}, I_{obj}^{2}, ..., I_{obj}^{n}\} \in \mathcal{I}$:
\begin{equation}
    \{ Z_{obj}^{1}, Z_{obj}^{2}, ..., Z_{obj}^{n}\} = F_{i}(\{ I_{obj}^{1}, I_{obj}^{2}, ..., I_{obj}^{n}\}),
\end{equation}
we select the embedding $Z_{obj}^{pos}$ of ground truth image, which is matched with the input text prompt $T$, from the object feature embeddings $\{ Z_{obj}^{1}, Z_{obj}^{2}, ..., Z_{obj}^{n}\}$.

Subsequently, we encode text prompt $T$ as the query embedding $Z_{q}$ and the reflective negative $I_{neg}$ as the negative feature embedding $Z_{neg}$:
\begin{equation}
    Z_{q} = F_{t}(T), \quad Z_{neg} = F_{i}(I_{neg}),
\end{equation}
where $F_{i}$ is the vision encoder and $F_{t}$ is the text encoder (As shown in Fig.~\ref{overall} (b)).

We perform our self-reflective contrastive learning by using normal in-batch negative and the reflective negative at the same time. We calculate the similarity between the text prompt and negatives:
\begin{small}
\begin{equation}
    \mathcal{D}_{neg}^{nor}=\sum_{j \ne pos}^{N}(Z_{q}^T \cdot Z_{obj}^{j}/\tau), \quad \mathcal{D}_{neg}^{ref} = exp(Z_{q}^T \cdot Z_{neg}),
\end{equation}
where $N$ is the training batch size, $\mathcal{D}_{neg}^{nor}$ is the similarity between the text prompt and the in-batch negative, $\mathcal{D}_{neg}^{ref}$ is the similarity between the text prompt and the reflective negative.

Lastly, the overall loss of self-reflective contrastive learning is:
\end{small}
\begin{equation}
    \mathcal{L} = -log\frac{exp(Z_{q}^T \cdot Z_{obj}^{pos})/\tau}{exp(Z_{q}^T \cdot Z_{obj}^{pos}/\tau)+\mathcal{D}_{neg}^{nor} +\mathcal{D}_{neg}^{ref}},
\label{core}
\end{equation}
where \( \tau \) is a temperature hyperparameter.

Powered by self-reflective contrastive learning, the reflective retriever integrates the missing knowledge of the generator, augmenting the realism and accuracy of the generated images and effectively addressing the hallucination problems in the text-to-image generation.

\begin{figure}[t!]
    \centering
    \includegraphics[width=\linewidth]{figs/space.pdf}
    \vspace{-15pt}
    \caption{The comparison of generation space. We show the generation space of the normal RAG (\textbf{left}) and our RealRAG (\textbf{right}).}
    \vspace{-4pt}
    \label{space}
\end{figure}


\subsection{Real-object-based Retrieval-augmented Generation}
\label{Real-object-based}
Expanding upon our trained reflective retriever, we subsequently retrieve the augmented real-object-based images $I_{ref}$ based on the given text prompt $T$ for the generator:
\begin{equation}
    I_{ref} = Retriever(T, \{I_{obj}^{1},I_{obj}^{2},...I_{obj}^{n}\}),
\end{equation}
where $Retriever$ is the trained reflective retriever, and $n$ in the scale of real-object-base database.

We then generate image $I_{res}$ based on the text prompt $T$ and the augmented image $I_{ref}$:
\begin{equation}
    I_{res} = G(T,I_{ref}),
\end{equation}
where $G$ is the generator.
Our real-object-based RAG improves the knowledge of the generator and significantly extends the generation space for the frozen generator. As shown in Fig.~\ref{space}, powered by Eq.~\ref{core}, the reference image is distributed in the distribution space outside the generator's generation space and close to the text prompt embedding. In this way, the generation space of our real-object-based RAG can effectively expand toward the text prompt embedding.


\begin{table*}[t!]
\renewcommand{\tabcolsep}{3pt}
\caption{Evaluation of fine-grained object generation. We report the quantitative results on the Stanford Cars~\cite{krause20133d}, Stanford Dogs~\cite{dataset2011novel}, and Oxford Flowers~\cite{nilsback2008automated} benchmarks.}
\vspace{8pt}
\resizebox{\linewidth}{!}{
\begin{tabular}{c|l|ccc|ccc|ccc}
\toprule 
Type & Method & \multicolumn{3}{c|}{Stanford Cars} & \multicolumn{3}{c|}{Stanford Dogs} & \multicolumn{3}{c}{Oxford Flowers} \\
% \cmidrule{3-11}
 & & CLIP-I $\uparrow$ & CLIP-T $\uparrow$ & FID $\downarrow$ & CLIP-I $\uparrow$ & CLIP-T $\uparrow$ & FID $\downarrow$ & CLIP-I $\uparrow$ & CLIP-T $\uparrow$ & FID $\downarrow$ \\
\midrule
 & OmniGen~\cite{xiao2024omnigen} & 55.69 & 11.25& 77.35 & 72.47 & 17.92 & 67.94 & 82.78 & 20.72 & 66.22 \\
\rowcolor{gray!10} \cellcolor{white}AR & OmniGen W. Ours & 57.11 & 14.48 & 74.98 & 78.74 & 18.30 & 63.10 & 84.65 & 20.98 & 54.42 \\ 
Model & Emu~\cite{sun2024generative} & 61.41 & 13.49 & 86.73 & 80.51 & 19.52 & 48.75 & 81.88 & 21.17 & 84.33 \\
 \rowcolor{gray!10} \cellcolor{white} & Emu W. Ours & 63.07 & 15.30 & 70.55 & 80.39 & 19.60 & 45.04 & 85.52 & 22.42 & 63.85 \\
\rowcolor{gray!20} \cellcolor{white} &$\Delta $ & \textcolor{blue}{+1.54} & \textcolor{blue}{+2.52} & \textcolor{blue}{-9.28} & \textcolor{blue}{+3.08} & \textcolor{blue}{+0.23} & \textcolor{blue}{-4.28} & \textcolor{blue}{+2.76} & \textcolor{blue}{+0.75} & \textcolor{blue}{-16.14} \\ \midrule

 & SD V2.1!\cite{Rombach_2022_CVPR} & 61.48 & 14.16 & 64.99 & 68.80 & 17.69 & 49.60 & 82.97 & 20.76 & 69.17 \\
\rowcolor{gray!10} \cellcolor{white} U-Net-based & SD V2.1 W. Ours & 64.50 & 15.84 & 58.98 & 79.06 & 18.57 & 43.93 & 88.43 & 21.77 & 60.76 \\
Diffusion Model & SDXL~\cite{podell2023sdxl} & 59.53 & 13.03 & 61.98 & 77.10 & 18.05 & 49.28 & 81.08 & 19.59 & 55.20 \\
\rowcolor{gray!10} \cellcolor{white} & SDXL W. Ours & 64.71 & 15.16 & 60.95 & 77.79 & 18.31 & 45.31 & 85.63 & 20.92 & 47.41 \\
\rowcolor{gray!20} \cellcolor{white} &$\Delta $ & \textcolor{blue}{+4.10} & \textcolor{blue}{+1.91} & \textcolor{blue}{-3.52} & \textcolor{blue}{+5.48} & \textcolor{blue}{+0.57} & \textcolor{blue}{-4.82} & \textcolor{blue}{+5.01} & \textcolor{blue}{+1.17} & \textcolor{blue}{-8.10} \\ \midrule

 & SD V3~\cite{esser2024scaling} & 59.43 & 12.61 & 59.94 & 80.28 & 18.56 & 53.37 & 83.23 & 20.19 & 63.46 \\
\rowcolor{gray!10} \cellcolor{white} DiT-based & SD V3 W. Ours & 60.01 & 14.80 & 54.60 & 81.04 & 18.62 & 53.28 & 82.68 & 20.73 & 60.89 \\
Diffusion Model & Fux~\cite{flux} & 60.50 & 13.70 & 58.47 & 80.16 & 18.36 & 45.71 & 82.18 & 20.19 & 55.93 \\
\rowcolor{gray!10} \cellcolor{white} & Flux W. Ours & 62.81 & 14.46 & 52.28 & 83.53 & 18.64 & 42.25 & 86.18 & 21.46 & 53.32 \\
\rowcolor{gray!20} \cellcolor{white} &$\Delta $ & \textcolor{blue}{+1.45} & \textcolor{blue}{+1.48} & \textcolor{blue}{-5.77} & \textcolor{blue}{+2.07} & \textcolor{blue}{+0.17} & \textcolor{blue}{-1.78} & \textcolor{blue}{+1.73} & \textcolor{blue}{+0.91} & \textcolor{blue}{-2.59} \\
\bottomrule
\end{tabular}}
\vspace{-12pt}
\label{Table_main_1}
\end{table*}



\begin{table}[t!]
\renewcommand{\tabcolsep}{3pt}
\caption{The classification results of fine-grained object generation. We use the pre-trained classification model (OpenCLIP~\cite{cherti2022reproducible}) to test the classification accuracy of the generated images from the three datasets. A higher accuracy indicates the generated images are more similar to real images. We report the average accuracy score (\textit{the full results of the three datasets are shown in Tab.~\ref{Table_appendix_2} of the Appendix}).}
\vspace{8pt}
\resizebox{\linewidth}{!}{
\begin{tabular}{c|l|c}
\toprule 
Type & Method & Average Acc. $\uparrow$ \\
\midrule
 & OmniGen~\cite{xiao2024omnigen} & 35.26  \\
\rowcolor{gray!10} \cellcolor{white}Autoregressive & OmniGen W. Ours & 38.96  \\ 
Model & Emu~\cite{sun2024generative} & 35.81  \\
 \rowcolor{gray!10} \cellcolor{white} & Emu W. Ours & 39.90  \\
\rowcolor{gray!20} \cellcolor{white} &$\Delta $ & \textcolor{blue}{+3.89}  \\ \midrule

 & SD V2.1!\cite{Rombach_2022_CVPR} & 37.45  \\
\rowcolor{gray!10} \cellcolor{white} U-Net-based & SD V2.1 W. Ours & 40.23  \\
Diffusion Model & SDXL~\cite{podell2023sdxl} & 36.32  \\
\rowcolor{gray!10} \cellcolor{white} & SDXL W. Ours & 40.49 \\
\rowcolor{gray!20} \cellcolor{white} &$\Delta $ & \textcolor{blue}{+3.48} \\ \midrule

 & SD V3~\cite{esser2024scaling} & 36.09 \\
\rowcolor{gray!10} \cellcolor{white} DiT-based & SD V3 W. Ours & 39.03 \\
Diffusion Model & Fux~\cite{flux} & 37.29 \\
\rowcolor{gray!10} \cellcolor{white} & Flux W. Ours & 40.37 \\
\rowcolor{gray!20} \cellcolor{white} &$\Delta $ & \textcolor{blue}{+2.99} \\
\bottomrule
\end{tabular}}
\vspace{-12pt}
\label{Table_main_2}
\end{table}



\subsection{Implementation}
Our RealRAG can be flexibly implemented with different existing generators, including U-Net-based diffusion models, DiT-based diffusion models, and auto-regressive models. It is the first work to build \textit{a unified RAG framework for all types of text-to-image generative models}.

\noindent \textbf{Real-object-based Database.} 
We collect our real-object-based database from wide-use real-world datasets, including ImageNet~\cite{deng2009imagenet}, Stanford Cars~\cite{krause20133d}, Stanford Dogs~\cite{dataset2011novel}, and Oxford Flowers~\cite{nilsback2008automated}. We use the training set of these datasets to conduct our database.

\noindent \textbf{Generator.} We use existing text-to-image generative models as the generator to implement our RealRAG. Concretely, we implement our RealRAG with the following models: U-Net-based diffusion models (SD V2.1~\cite{Rombach_2022_CVPR}, SD XL~\cite{podell2023sdxl}), DiT-based diffusion models (SD V3~\cite{esser2024scaling}, Flux~\cite{flux}), and auto-regressive models (OmniGen~\cite{xiao2024omnigen}, Emu~\cite{sun2024generative}). 

\noindent \textbf{Training Details.} 
We train our reflective retriever based on the pre-trained CLIP model~\cite{radford2021learning}. We add a simple MLP layer at the end of the vision encoder of the CLIP model, to map the visual embeddings outside the generation space of the generator and close to the input prompt embedding. We utilize the frozen text encoder from the CLIP model as our text encoder. 





\section{Experiment}

\begin{figure*}[t!]
    \centering
    \includegraphics[width=\textwidth]{figs/real_1.pdf}
    \vspace{-15pt}
    \caption{The visual results of fine-grained object generation. We visually compare the images generated by the original generators and our RealRAG. We also add real-world images for reference.}
    \vspace{-6pt}
    \label{result_real}
\end{figure*}


\subsection{Datasets and Implementation Details}

\noindent \textbf{Datasets and Benchmarks.} 
We evaluate our proposed RealRAG on three fine-grained real-world image datasets, including Stanford Cars~\cite{krause20133d}, Stanford Dogs~\cite{dataset2011novel}, and Oxford Flowers~\cite{nilsback2008automated}. We use the test sets of these datasets to validate the realism of the generated images (\textbf{Sec.~\ref{real_setting}}). 
% Furthermore, to validate thabilityof our RealRAG to generate unseen novel objects, we also test our model on generating recent novel objects. (Sec.~\ref{novel_setting})
Furthermore, to validate the ability of RealRAG to generate unseen novel objects, we also test our model on recently introduced novel objects (\textbf{Sec.\ref{novel_setting}}).

\noindent \textbf{Evaluation Metrics.}
We employ FID, CLIP-T, and CLIP-I to compare the visual quality and realism of generated images from different methods. Specifically, FID measures how closely the distribution of generated images matches that of real images by comparing their feature statistics in the latent space of Inception V3. CLIP-T uses CLIP model~\cite{} to assess how well the generated image matches with its text prompt, essentially quantifying text-image correspondence. CLIP-I, on the other hand, focuses on measuring image-image similarity through CLIP, often comparing a generated image to a reference or target image, thereby evaluating visual fidelity or consistency across images.

\begin{figure*}[t!]
    \centering
    \includegraphics[width=\textwidth]{figs/novel.pdf}
    \vspace{-18pt}
    \caption{The visual results of unseen novel object generation.}
    \vspace{-8pt}
    \label{result_novel}
\end{figure*}


\subsection{Fine-grained Object Generation}
\label{real_setting}

\subsubsection{Experiment Setup}
To evaluate the realism of the generated images, we use the text prompt {\fontfamily{qcr}\selectfont["A photo of a [CLASS NAME]"]} for image generation. We generate ten images for each class to get more reliable results by multiple sampling. We use fine-grained real-world datasets to evaluate the realism of the generated images by calculating the FID score, CLIP-T score, and CLIP-I score between the ground truth images and the generated images. 

\subsubsection{Generative Result}


\noindent \textbf{Quantitative Results.}
In Tab.~\ref{Table_main_1}, we apply our method with all the types of SoTA text-to-image generators. Our RealRAG demonstrates significant performance gains with all these models on the three benchmarks, including an average \textbf{gain of \textit{6.19\%}} on the Stanford Cars, \textbf{a \textit{3.62\%} gain} on the Stanford Dogs, and \textbf{an \textit{8.94\%} gain} on the Oxford Flowers. As shown in Tab.~\ref{Table_main_1}, our RealRAG achieves the \textbf{most significant improvement} (a \textbf{\textit{9.90\%}} gain on average) with the auto-regressive model, which shows the potential of our RealRAG to enhance the development of the large-scale auto-regressive model. 
To further evaluate the generative quality and realism of fine-grained objects, we utilize a pre-trained classification model (OpenCLIP~\cite{cherti2022reproducible}) to calculate the classification accuracy for the generated images. As shown in Tab.~\ref{Table_main_2}, our RealRAG achieves considerable improvements of the classification performance, \eg, \textbf{a \textit{3.89\%} gain} for the auto-regressive model. 
% The results demonstrate that RealRAG enhances the generative quality and realism effectively.
% Notably, the augment of our RealRAG is not limited to the original generative ability of the generators. For example, although the SoTA model, Flux~\cite{flux} achieves the best performance on the Stanford Car, \eg 58.47 FID score, our RealRAG also effectively enhances the generative quality and realism (a \textbf{\textit{6.19\%}} gain).

\noindent \textbf{Qualitative Results.}
We show the qualitative results in Fig.~\ref{result_real}. The visual results show the ability of our RealRAG to \textit{overcome hallucinations and significantly improve the realism and quality of fine-grained realistic image generation}. Specifically, for the autoregressive model, In the prompt {\fontfamily{qcr}\selectfont["Audi 100 Wagon 1994"]}, the RealRAG model produces a highly realistic rendering of the vehicle, capturing intricate details like the side mirrors and headlights, which are closer to the real object compared to the original AR model with significant hallucination in the car's side. 
For the U-Net-based diffusion model, In prompt {\fontfamily{qcr}\selectfont["Silverbush"]}, RealRAG ensures the correct flower structure and vibrant color tones, whereas the original model struggles with maintaining fine-grained details like petal shape. 
Lastly, compared with the original generator, RealRAG delivers a compelling representation of the breed’s signature coat patterns and body posture from the {\fontfamily{qcr}\selectfont["Bluetick"]} prompt. The results demonstrate that RealRAG generates high realism and correct objects and show the flexible application of the SoTA generators.

\subsection{Unseen Novel Object Generation}
\label{novel_setting}

\subsubsection{Experiment Setup}
% To evaluate the ability of unseen novel object generation, we collect several recent novel objects, \eg the cybertruck, to conduct the input prompts. Specifically, we use LLM, \eg ChatGPT, to create the input prompts based on the novel objects. To ensure fairness in the comparison, we conduct both qualitative evaluation and human evaluation.
To evaluate the ability to generate unseen novel objects, we collect several recently introduced objects (e.g., the Cybertruck) to form the input prompts. Specifically, we use a LLM (e.g., ChatGPT) to generate prompts based on these novel objects. To ensure fairness in our comparison, we conduct both qualitative evaluations and human evaluations.

\subsubsection{Generative Result}

\begin{figure}[t!]
    \centering
    \includegraphics[width=\linewidth]{figs/human.pdf}
    \vspace{-22pt}
    \caption{The 7-point Likert scale of the human evaluation. The participant is asked to score the generated images from 1 (low accuracy) to 7 (high accuracy), according to given text prompts.}
    \vspace{-12pt}
    \label{result_human}
\end{figure}

\noindent \textbf{Qualitative Results.}
In Fig.~\ref{result_novel}, we present the visual comparison of the SoTA model Flux~\cite{flux} and Our RealRAG. The visual results demonstrate the significant improvement in the unseen object generation of our RealRAG. As the case {\fontfamily{qcr}\selectfont["Cybertruck"]} shows, compared with the original result generated by Flux, our RealRAG generates the realistic shape for the Cybertruck and also synthesis the sense of {\fontfamily{qcr}\selectfont["speeding along the Great Wall"]}; As the case {\fontfamily{qcr}\selectfont["Boston Dynamics Spot"]}, although the Flux can generate the sense of {\fontfamily{qcr}\selectfont["a robot walks on mountains"]}, the shape and type of the {\fontfamily{qcr}\selectfont["Boston Dynamics Spot"]} is inaccurate. Our RealRAG can generate the correct and realistic objects, which shows the significant improvement of our RealRAG for unseen noval object generation.

\noindent \textbf{Human Evaluation.}
We conducted a human evaluation to assess the accuracy of the generated images. Data acquisition primarily revolved around participants' subjective assessments of the generative accuracy of the unseen novel objects. We involved \textbf{26 participants} in our evaluation (\textit{More details are included in the Appendix.}). We use a \textbf{7-point Likert scale} to evaluate the accuracy of the generated images in the human's view. As shown in Fig.~\ref{result_human}, we ask participants to give a score from 1 (low accuracy) to 7 (high accuracy) for the 4 sets of images. The results show that all participants consistently rated the quality of RealRAG-generated images with a mean score exceeding 5, indicating significant performance gains compared to Flux. RealRAG exhibited a relatively small variance across samples, highlighting its proficiency in unseen novel object generation. Overall, \textit{the combination of higher mean scores and lower variance demonstrates that RealRAG is not only better at generating higher-realism images but also more reliable, as participants consistently rated its outputs highly.}

% \subsection{Human Evaluation}

% \section{Analysis}

\begin{table}[t!]
\renewcommand{\tabcolsep}{3pt}
\caption{Evaluation of fine-grained object generation. We report the quantitative results on the Stanford Cars~\cite{krause20133d}.}
\vspace{4pt}
\resizebox{\linewidth}{!}{
\begin{tabular}{c|l|ccc}
\toprule 
Generator & Method & CLIP-I $\uparrow$ & CLIP-T $\uparrow$ & FID $\downarrow$  \\ 
\midrule
 & Zero-RAG & 61.84 & 14.06 & 80.75 \\
Emu~\cite{sun2024generative} & Normal-RAG & 62.20 & 14.31 & 76.85 \\
 & RealRAG & \textbf{63.07} & \textbf{15.30} & \textbf{70.55} \\
\midrule
 & Zero-RAG & 62.59 & 14.87 & 61.67 \\
SD V2.1~\cite{Rombach_2022_CVPR} & Normal-RAG & 63.05 & 15.03 & 60.77 \\
 & RealRAG & \textbf{64.50} & \textbf{15.84} & \textbf{58.98} \\
\midrule
 & Zero-RAG & 61.20 & 13.96 & 54.25 \\
Flux~\cite{flux} & Normal-RAG & 61.72 & \textbf{14.52} & 54.04 \\
 & RealRAG & \textbf{62.81} & 14.46 & \textbf{52.28} \\
\bottomrule
\end{tabular}}
\vspace{-16pt}
\label{Table_ab_1}
\end{table}


\section{Ablation Study}
% To investigate and analyze the effectiveness of our proposed RealRAG, we first conduct the ablation study on the fine-grained object generation setting. As shown in Tab.~\ref{Table_ab_1}, we compare the generative performance of the Zero-shot RAG, the Normal RAG, and our RealRAG. Specifically, Zero-shot RAG is the straightforward pipeline of the RAG, which simply uses the pre-trained CLIP model to retrieve relevant images by cosine-similarity; Normal RAG trains the retriever via contrastive learning to retrieve the most relevant images. We experiment with the generators from three types, including Emu~\cite{sun2024generative} for the AR model, SDXL~\cite{podell2023sdxl} for the U-Net-based diffusion model, Flux~\cite{flux} for the DiT-based diffusion model. The results show the effectiveness of our proposed self-reflective contrastive learning. Compared with the normal RAG framework, our RealRAG achieves significant improvements in generation realism and accuracy. We also use a classification model to calculate the classification accuracy of the generated images (\textit{Please check Tab.~\ref{} in Appendix.}), which shows the strong performance of our RealRAG for fine-grained realistic image generation.

To investigate and analyze the effectiveness of our proposed RealRAG, we first perform an ablation study on the fine-grained object generation setting. As shown in Table~\ref{Table_ab_1}, we compare the generative performance of three variants: Zero-shot RAG, Normal RAG, and RealRAG. Specifically, Zero-shot RAG is a baseline approach that employs a pre-trained CLIP model to retrieve relevant images based on cosine similarity, whereas Normal RAG trains the retriever via contrastive learning to retrieve the most relevant images. We evaluate each approach using three different generators—Emu~\cite{sun2024generative} (AR model), SDXL~\cite{podell2023sdxl} (U-Net-based diffusion model), and Flux~\cite{flux} (DiT-based diffusion model). The results highlight the effectiveness of our proposed self-reflective contrastive learning. Compared with the Normal RAG framework, RealRAG achieves substantial improvements in both generation realism and accuracy. In addition, we employ a classification model to measure the classification accuracy of the generated images (\textit{see Table~\ref{Table_appendix_2} in the Appendix}), which further demonstrates the robust performance of RealRAG for fine-grained, realistic image generation.


% \begin{table}[t!]
% \renewcommand{\tabcolsep}{3pt}
% \caption{Evaluation of fine-grained object generation. We report the quantitative results on the Stanford Car.}
% \vspace{2pt}
% \resizebox{\linewidth}{!}{
% \begin{tabular}{c|l|ccc}
% \toprule 
% Generator & Method & CLIP-I & CLIP-T & FID  \\ 
% \midrule
%  & Zero-RAG & 55.69 & 11.25& 77.35 \\
% Emu~\cite{sun2024generative} & Normal-RAG & 55.69 & 11.25& 77.35 \\
%  & RealRAG & 55.69 & 11.25& 77.35 \\
% \midrule
%  & Zero-RAG & 55.69 & 11.25& 77.35 \\
% SDXL~\cite{podell2023sdxl} & Normal-RAG & 55.69 & 11.25& 77.35 \\
%  & RealRAG & 55.69 & 11.25& 77.35 \\
% \midrule
%  & Zero-RAG & 55.69 & 11.25& 77.35 \\
% Flux~\cite{flux} & Normal-RAG & 55.69 & 11.25& 77.35 \\
%  & RealRAG & 55.69 & 11.25& 77.35 \\
% \bottomrule
% \end{tabular}}
% \vspace{-10pt}
% \label{Table_ab_1}
% \end{table}


% To further explore the effectiveness of the reflective negative, we evaluate the performance of the checkpoint from the second, fourth, sixth, eighth, and tenth epoch. As shown in Fig.~\ref{result_tend}, Our RealRAG is not as fast in performance improvement as the retrieval frameworks relying only on similarity in the early stage of training, but in the late stage of training, powered by the reflective negative, our RealRAG can better break through the generative bottleneck.
To further investigate the effectiveness of the reflective negative, we evaluate the performance of the checkpoints from the second, fourth, sixth, eighth, and tenth epochs. As shown in Fig.~\ref{result_tend}, while our RealRAG does not improve as rapidly as retrieval frameworks relying solely on similarity in the early stages of training, it ultimately surpasses the generative bottleneck in later training stages, powered by the reflective negative.


\begin{figure}[t!]
    \centering
    \includegraphics[width=\linewidth]{figs/tend.pdf}
    \vspace{-24pt}
    \caption{The results present the performance comparison of Normal RAG and our RealRAG with the checkpoint from the second, fourth, sixth, eighth, and tenth epoch.}
    \vspace{-8pt}
    \label{result_tend}
\end{figure}

Lastly, the results of the t-SNE visualization in Fig.~\ref{result_tsne} reveal the differences between the representation spaces constructed by normal RAG and our RealRAG. The visual results demonstrate that the generative space of RealRAG expands more in the direction of ground-truth images.

\begin{figure}[t!]
    \centering
    \includegraphics[width=\linewidth]{figs/tsne.pdf}
    \vspace{-20pt}
    \caption{The t-SNE visualization of the generated images.}
    \vspace{-12pt}
    \label{result_tsne}
\end{figure}


\section{Conclusion}
In this paper, we proposed RealRAG, the \textbf{first real-object-based retrieval-augmented generation framework}. Our RealRAG augmented fine-grained and unseen novel object generation by learning and retrieving real-world images to overcome the knowledge gaps of generative models. Our RealRAG achieved \textbf{remarkable performance boosts} and is compatible with \textbf{all types of generative models}. Additionally, we examined the potential of the RAG framework for text-to-image generation. For future work, we will continue to explore more efficient RAG frameworks for the text-to-image generation task and other generation tasks.

% \begin{algorithm}[tb]
%    \caption{Bubble Sort}
%    \label{alg:example}
% \begin{algorithmic}
%    \STATE {\bfseries Input:} data $x_i$, size $m$
%    \REPEAT
%    \STATE Initialize $noChange = true$.
%    \FOR{$i=1$ {\bfseries to} $m-1$}
%    \IF{$x_i > x_{i+1}$}
%    \STATE Swap $x_i$ and $x_{i+1}$
%    \STATE $noChange = false$
%    \ENDIF
%    \ENDFOR
%    \UNTIL{$noChange$ is $true$}
% \end{algorithmic}
% \end{algorithm}

\section*{Impact Statement}
% This paper advances machine learning, specifically in text-to-image generation. Our work addresses the challenge of hallucinations and distortions of the generated images caused by the limited knowledge of the fixed parameters, a common issue in real-world applications. While there are potential societal implications, we feel none need to be specifically highlighted here.
This paper advances the field of machine learning, with a particular focus on text-to-image generation. It addresses the prevalent challenge of hallucinations and distortions in generated images, which arise from the limited knowledge stored in the fixed parameters of generative models— an issue frequently encountered in real-world applications. While there are potential societal implications, we feel none need to be specifically highlighted here.

% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
% \nocite{langley00}

\bibliography{main}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn

\section{Appendix}

\subsection{More Experimental Results}
We show the full classification results of fine-grained object generation in Tab.~\ref{Table_appendix_1}. The results demonstrate that RealRAG effectively enhances the generative quality and realism. Notably, the augment of our RealRAG is not limited to the original generative ability of the generators. For example, although the SoTA model, Flux~\cite{flux} achieves the best performance on the Stanford Car, \eg 35.78 Accuracy score, our RealRAG also effectively enhances the generative quality and realism (a \textbf{\textit{4.20\%}} gain).

\begin{table}[h!]
\renewcommand{\tabcolsep}{8pt}
\vspace{-8pt}
\caption{The full classification results of fine-grained object generation.}
\vspace{8pt}
\resizebox{\linewidth}{!}{
\begin{tabular}{c|l|cccc}
\toprule 
Type & Method & Stanford Cars & Stanford Dogs & Oxford Flowers & Average Acc. $\uparrow$ \\
\midrule
 & OmniGen~\cite{xiao2024omnigen} & 32.80 & 40.93 & 32.04 & 35.26  \\
\rowcolor{gray!10} \cellcolor{white}Autoregressive & OmniGen W. Ours & 37.45 & 44.71 & 34.72 & 38.96  \\ 
Model & Emu~\cite{sun2024generative} & 33.94 & 41.32 & 32.18 & 35.81  \\
 \rowcolor{gray!10} \cellcolor{white} & Emu W. Ours & 40.24 & 44.33 & 35.13 & 39.90  \\
\rowcolor{gray!20} \cellcolor{white} &$\Delta $ & \textcolor{blue}{+5.48} & \textcolor{blue}{+3.40} & \textcolor{blue}{+2.82} & \textcolor{blue}{+3.89}  \\ \midrule

 & SD V2.1\cite{Rombach_2022_CVPR} & 35.21 & 42.35 & 34.79 & 37.45  \\
\rowcolor{gray!10} \cellcolor{white} U-Net-based & SD V2.1 W. Ours & 39.43 & 46.10 & 35.15 & 40.23  \\
Diffusion Model & SDXL~\cite{podell2023sdxl} & 35.20 & 41.06 & 32.69 & 36.32  \\
\rowcolor{gray!10} \cellcolor{white} & SDXL W. Ours & 40.55 & 44.76 & 36.17 & 40.49 \\
\rowcolor{gray!20} \cellcolor{white} &$\Delta $ & \textcolor{blue}{+4.79} & \textcolor{blue}{+3.73} & \textcolor{blue}{+1.92} & \textcolor{blue}{+3.48} \\ \midrule

 & SD V3~\cite{esser2024scaling} & 33.90 & 41.26 & 33.12 & 36.09 \\
\rowcolor{gray!10} \cellcolor{white} DiT-based & SD V3 W. Ours & 38.72 & 44.24 & 34.05 & 39.03 \\
Diffusion Model & Fux~\cite{flux} & 35.78 & 42.11 & 33.98 & 37.29 \\
\rowcolor{gray!10} \cellcolor{white} & Flux W. Ours & 39.98 & 45.84 & 35.29 & 40.37 \\
\rowcolor{gray!20} \cellcolor{white} &$\Delta $ & \textcolor{blue}{+4.51} & \textcolor{blue}{+3.36} & \textcolor{blue}{+1.12} & \textcolor{blue}{+2.99} \\
\bottomrule
\end{tabular}}
% \vspace{-12pt}
\label{Table_appendix_1}
\end{table}

\subsection{More Results in Ablation Study}
We show the ablation study of fine-grained classification in Tab.~\ref{Table_appendix_2}. Specifically, Zero-shot RAG is the straightforward pipeline of the RAG, which simply uses the pre-trained CLIP model to retrieve relevant images by cosine-similarity; Normal RAG trains the retriever via contrastive learning to retrieve the most relevant images. The results show the effectiveness of our proposed self-reflective contrastive learning. Compared with the normal RAG framework, our RealRAG achieves significant improvements in generation realism and accuracy.

\begin{table}[h!]
\renewcommand{\tabcolsep}{8pt}
\vspace{-8pt}
\caption{The ablation study of fine-grained classification on the three datasets.}
\vspace{4pt}
\resizebox{\linewidth}{!}{
\begin{tabular}{c|l|cccc}
\toprule 
Generator & Method & Stanford Cars & Stanford Dogs & Oxford Flowers & Average Acc. $\uparrow$ \\
% \cmidrule{3-11}
\midrule
 & Zero-RAG & 37.18 & 42.88 & 34.27 & 38.11 \\
Emu~\cite{sun2024generative} & Normal-RAG & 38.75 & 43.09 & 35.10 & 38.98\\
 & RealRAG & \textbf{40.24} & \textbf{44.33} & \textbf{35.13} &\textbf{ 39.90} \\
\midrule
 & Zero-RAG & 38.64 & 44.07 & 33.70 & 38.80\\
SD V2.1~\cite{Rombach_2022_CVPR} & Normal-RAG & 39.07 & 44.92 & \textbf{35.21} & 39.73 \\
 & RealRAG & \textbf{39.43} & \textbf{46.10} & 35.15 & \textbf{40.23} \\
\midrule
 & Zero-RAG & 36.27 & 44.31 & 34.59 & 38.39 \\
Flux~\cite{flux}& Normal-RAG & 37.84 & 45.01 & 34.96 & 39.27 \\
 & RealRAG  & \textbf{39.98} & \textbf{45.84} & \textbf{35.29} & \textbf{40.37} \\
\bottomrule
\end{tabular}}
% \vspace{-16pt}
\label{Table_appendix_2}
\end{table}






\subsection{Details and More Results in Human Evaluation}
We conducted a human evaluation to assess the realism and accuracy of the generated images. Data acquisition primarily revolved around participants' subjective assessments of the realism in fine-grained image generation and accuracy in unseen novel image generation. The evaluation consists of two tasks: \textbf{(1)} Text-to-image matching task and \textbf{(2)} Real/False task. The human evaluation procedures were approved by the ethical committee.

\noindent \textbf{Participants.}
The evaluation involved 26 participants, of whom 69.23\% were aged 18-24 and 30.77\% were aged 25-34. The gender distribution was 53.85\% male and 46.15\% female. Also, 57.69\% of the participants had previous experience with generative models, and 42.31\% had no experience using AI generative models in the last six months. 
% As shown in Fig.~\ref{fig: participants}, the experiments were conducted using the Meta Quest 2 VR Headset, and the Oculus software Cenario VR was utilized as our player.

\noindent \textbf{Tasks and Measurement.}
We designed two tasks to evaluate the effectiveness of RealRAG for its ability to generate fine-grained and unseen novel objects. The two tasks include: \textbf{(1)} Text-to-image matching task and \textbf{(2)} Real/False task.
% task image clear.
% check the number of samples
\begin{itemize}
    \item \textbf{Text-to-image matching task.} In this task, we collected four sets of images, and for each set, there were two images, one was generated by the original Flux model~\cite{flux}, and the other was generated by our RealRAG. We then asked the participants to rate how well the text prompt and the generative image matched via a 7-point Likert scale. Images were presented in a random order, and the specific cases are shown in Fig.~\ref{task1}.
    \item \textbf{Real/Fake task.} In this task, we selected eight cases, of which, four of them are generated by original generators and others are generated by our RealRAG. Furthermore, we asked the participants whether the images were real or fake images (generated by AI). We show the task details in Fig.~\ref{task2}.
\end{itemize}

\noindent \textbf{Results.} 
We show the evaluation results of task \textbf{(1)} in Fig.\ref{result_human} of the main paper. For the task (2), we present the human-evaluation results in Fig.\ref{result_huamn_2}. The results show that the images generated by our RealRAG have got higher scores from the participants. To summarize, more than 70\% of the participants found our images to be realistic, which demonstrates the strong performance of our RealRAG for realistic image generation.

\begin{figure}[t!]
    \centering
    \includegraphics[width=0.95\linewidth]{figs/human_2.pdf}
    \vspace{-12pt}
    \caption{The results of the Real/fake task. Case 1,2,3,4 are generated by our RealRAG and case 5,6,7,8 are generated by the original generators.}
    % \vspace{-8pt}
    \label{result_huamn_2}
\end{figure}

\begin{figure}[t!]
    \centering
    \includegraphics[width=\linewidth]{figs/task1.pdf}
    \vspace{-12pt}
    \caption{The cases in \textbf{Text-to-image matching task} of the human evaluation.}
    % \vspace{-8pt}
    \label{task1}
\end{figure}

\begin{figure}[t!]
    \centering
    \includegraphics[width=\linewidth]{figs/task2.pdf}
    \vspace{-12pt}
    \caption{The cases in \textbf{Real/Fake task} of the human evaluation. Case 1,2,3,4 are generated by our RealRAG and case 5,6,7,8 are generated by the original generators.} 
    % \vspace{-8pt}
    \label{task2}
\end{figure}



\section{More Visualization}

\subsection{Qualitative Feature Visualization}
We show more t-SNE visualization results in Fig.~\ref{tsne_supp}, which shows that the generative space of RealRAG expands more in the direction of ground-truth images.

\subsection{Generative Results}
We show more generative results for fine-grained generative results in Fig.~\ref{real_supp}. The results show the generative ability of our RealRAG in realistic image generation.


\begin{figure}[t!]
    \centering
    \includegraphics[width=\linewidth]{figs/tsne_supp.pdf}
    \vspace{-12pt}
    \caption{More t-SNE visualization results of generated images.} 
    % \vspace{-8pt}
    \label{tsne_supp}
\end{figure}


\begin{figure}[t!]
    \centering
    \includegraphics[width=\textwidth]{figs/real_supp.pdf}
    \vspace{-12pt}
    \caption{More visual results of fine-grained object generation.} 
    % \vspace{-8pt}
    \label{real_supp}
\end{figure}


% You can have as much text here as you want. The main body must be at most $8$ pages long.
% For the final version, one more page can be added.
% If you want, you can use an appendix like this one.  

% The $\mathtt{\backslash onecolumn}$ command above can be kept in place if you prefer a one-column appendix, or can be removed if you prefer a two-column appendix.  Apart from this possible change, the style (font size, spacing, margins, page numbering, etc.) should be kept the same as the main body.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.




Multimodal matching and retrieval is ubiquitous and in dispensable in our daily lives, from online shopping, visual news search, to question answering. With growing research interest, existing approaches to multimodal retrieval are typically divided into generative-based methods [33, 40] and embedding-based methods [24, 45, 57]. Generative approaches use a vision encoder followed by a text decoder to retrieve target entities through next-token prediction [2, 33, 63]. In contrast, embedding-based methods rely on separate vision and text encoders to represent query and target entities, followed by similarity computation and K-nearest neighbor (KNN) search. Given the broad applicability of embedding-based methods, this paper focuses on advancing embedding model development.

Although subsequent work, such as BLIP [29, 30] and InternVL [10], has aimed to increase parameter sharing between modalities, the optimal balance of encoder sharing remains unclear. Furthermore, scal056 ing up these encoder-based models is challenging, partly due to the scarcity of generalizable pretrained modality encoders, unlike pretrained decoder-based large language
models (LLMs) that offer extensive open-world knowl060 edge. Additionally, the reliance on the InfoNCE contrastive loss [43] in these encoder-based approaches presents further challenges. This loss function is limited in several ways: 1) it can produce biased gradients, leading to suboptimal solutions [7]; 2) it struggles to handle noisy data pairs effectively as studied in [7, 44]; and 3) it is specifically tailored for single-task contrastive learning, lacking adaptability for multi-task contexts. These inherent limitations make the InfoNCE-based contrastive loss inadequate for achieving our goals on effective multi-modal multi-task representation learning

In light of the need for scalable embedding models, recent efforts have focused on leveraging pretrained decoder-only LLMs for embedding tasks. While initial research on single-text-modality embedding models has shown that decoder-based LLMs provide greater flexibility and improved performance over conventional mod077 els [39, 40, 53], recent studies, such as MM-GEM [37] and VLM2VEC [26], have extended this concept to multi-modal embedding models. However, these models still rely on the single-task InfoNCE contrastive loss, limiting their adaptability, and, as a result, their performance remains slightly behind (if not comparable to) advanced encoder-based models like CLIP in some tasks.

To address these limitations, we propose the Multi085 Modal Multi-Task Unified Embedding Model (M3T-UEM), a unified, decoder-only LLM that serves as both a text and image encoder. Through multi-task learning, M3T-UEM can process any mixed modality input and retrieve mixed modality targets, as illustrated in Figure 1. Our model incorporates task-specific instructions (see Section 4.1 for detailed instructions on each retrieval task) and a novel task-aware contrastive loss designed to overcome the limitations of the standard InfoNCE loss (see Section 3.2). Our reformulated contrastive loss includes task-level and sample095 specific weights for each similarity score, improving multi096 task adaptability. Developed from a Bayesian modeling perspective, our task-aware contrastive loss enables efficient optimization through a stochastic expectation099 maximization (EM) framework.