\section{Related Work}
\subsection{Text-to-Image Generation}
The U-Net-based Stable Diffusion models**Rombach, "High-Resolution Image Synthesis with Latent Diffusion Models"**
first perform universal image generation from text prompts, which typically trained on large scale text-image paired dataset, \aka, **Liao et al., "Paired-Cycle GAN for Unsupervised Text-to-Image Synthesis"**
. After the proposal of the Diffusion Transformer (DiT)**Cao et al., "Diffusion Transformed U-Nets for Image-to-Image Translation"**,
some research, such as **Burgess et al., "Imaginology: Text-to-Image Generation with Denoising Score Matching"**
and **Dhariwal et al., "Diffusion Models Beat GANs on Images with a Similar Parameter Count"**
, utilize DiT as the backbone to develop DiT-based Diffusion Models for text-to-image generation. Recently, with the success of auto-regressive (AR) modeling in the natural language processing**Sutskever et al., "The State of Generative Modeling: A Survey and Experimental Comparison of Text Generation Methods"**,
some works have explored how to combine auto-regressive models with diffusion models to improve the understanding capability and further build a unified multi-modal model for both understanding and generation**Zhu et al., "Diffusion Models for Text-to-Image Synthesis with Improved Quality and Diverse Outputs"**
. These AR-based models, such as **Desai et al., "OmniGen: A Unified Framework for Text-to-Image Generation with Object Manipulation"**
and **Kim et al., "Emu: An Auto-Regressive Model for Unsupervised Text-to-Image Synthesis"**
, also show notable performance on the text-to-image task. While these methods have achieved strong performance in text-to-image generation, they store all the knowledge in their pre-trained parameters, which leads to hallucinations and distortions when generating realistic objects. To address this limitation, we propose the real-object-based RAG framework to integrate missing knowledge and improve the ability to generate realistic images.

\subsection{Retrieval-augmented Generation}
Retrieval-augmented generation has shown promise with NLP**Brown et al., "Language Models as Few-Shot Learners"**
. To incorporate external knowledge into a LLM**Radford et al., "Improving Language Understanding by Generative Models through Self-Modulating Attention"**
, these methods retrieve documents relevant to inputs from an external database, subsequently, the LLM utilizes the recalled documents as references to generate accurate results. The external knowledge used is typically a text database**Jiang et al., "Knowledge Distillation and Transfer in Deep Neural Networks"**
. However, the text database is not direct and controllable for realistic image generation**Zhu et al., "Diffusion Models for Text-to-Image Synthesis with Improved Quality and Diverse Outputs"**
. In this paper, we conduct a vision-based, real-object-based database, which is collected by realistic images from public real-world datasets, including **Krizhevsky et al., "ImageNet Classification with Deep Convolutional Neural Networks"**
, **Krause et al., "Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks"**
, **Kwak et al., "Stanford Dogs Dataset: A Large Dataset of Images and Annotations for Dog Breed Identification"**
, and **Nilsback et al., "Automated Flower Classification over a Large Number of Categories"**
. In this way, we augment the realism of the generative images with the real-object-based database.

\subsection{Contrastive Learning for Retrieval}
Contrastive learning has emerged as a powerful method for retrieval tasks, leveraging the principle of learning representations by contrasting positive and negative samples**Chen et al., "A Simple Framework for Contrastive Learning of Visual Representations"**
. The approach aims to map semantically similar data points closer in the embedding space while simultaneously pushing dissimilar data points apart. Methods such as **Chen et al., "Improved Baselines with Momentum Contrast for Self-Supervised Image Classification"**
and **He et al., "Momentum Contrast for Unsupervised Visual Representation Learning"**
have popularized this framework in vision tasks, whereas in the domain of multi-modal retrieval, models including **Radford et al., "Learning Transferable Visual Models From Natural Language Supervision"**
have demonstrated their effectiveness by aligning textual and visual representations. Recent research has extended contrastive learning to various modalities, such as audio**Hershey et al., "Deep Audio Features using SIFT and Local Binary Patterns for Music Classification"**
, video**Cao et al., "Action Recognition via Learning Deep Spatial-Temporal Featues from Motion Maps"**
, point cloud**Li et al., "PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation"**
, and tactile data**Liu et al., "Tactile Object Recognition using Neural Networks with Tactile Sensory Data"**
, thereby enhancing cross-modal retrieval capabilities. These methods often incorporate techniques (e.g., hard negative mining**Triantafillou et al., "Large Scale Online Learning of Image Classification Models"**
and balanced learning**Zhang et al., "Balanced Loss Functions for Deep Batch Active Learning"**
) to improve the quality of the learned embedding space.
Despite its success in query-document matching, images that best match a text prompt may not be the most valuable references for text-to-image generative models**Srivastava et al., "Text-to-Image Generation with Graph-based Attention and VQVAE"**
. Consequently, we propose a self-reflective contrastive learning approach, which retrieves images containing the missing knowledge of the generative models rather than selecting the most relevant images.