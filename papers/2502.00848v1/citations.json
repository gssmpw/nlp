[
  {
    "index": 0,
    "papers": [
      {
        "key": "Rombach_2022_CVPR",
        "author": "Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj\\\"orn",
        "title": "High-Resolution Image Synthesis With Latent Diffusion Models"
      },
      {
        "key": "podell2023sdxl",
        "author": "Podell, Dustin and English, Zion and Lacey, Kyle and Blattmann, Andreas and Dockhorn, Tim and M{\\\"u}ller, Jonas and Penna, Joe and Rombach, Robin",
        "title": "Sdxl: Improving latent diffusion models for high-resolution image synthesis"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "schuhmann2022laion",
        "author": "Schuhmann, Christoph and Beaumont, Romain and Vencu, Richard and Gordon, Cade and Wightman, Ross and Cherti, Mehdi and Coombes, Theo and Katta, Aarush and Mullis, Clayton and Wortsman, Mitchell and others",
        "title": "Laion-5b: An open large-scale dataset for training next generation image-text models"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "peebles2023scalable",
        "author": "Peebles, William and Xie, Saining",
        "title": "Scalable diffusion models with transformers"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "esser2024scaling",
        "author": "Esser, Patrick and Kulal, Sumith and Blattmann, Andreas and Entezari, Rahim and M{\\\"u}ller, Jonas and Saini, Harry and Levi, Yam and Lorenz, Dominik and Sauer, Axel and Boesel, Frederic and others",
        "title": "Scaling rectified flow transformers for high-resolution image synthesis"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "flux",
        "author": "BlackForest",
        "title": "Black Forest Labs; Frontier AI Lab"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "openai2023gpt4",
        "author": "OpenAI",
        "title": "GPT-4 Technical Report"
      },
      {
        "key": "touvron2023llama",
        "author": "Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\\'e}e and Rozi{\\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others",
        "title": "Llama: Open and efficient foundation language models"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "chen2024diffusion",
        "author": "Chen, Boyuan and Monso, Diego Marti and Du, Yilun and Simchowitz, Max and Tedrake, Russ and Sitzmann, Vincent",
        "title": "Diffusion forcing: Next-token prediction meets full-sequence diffusion"
      },
      {
        "key": "xie2024show",
        "author": "Xie, Jinheng and Mao, Weijia and Bai, Zechen and Zhang, David Junhao and Wang, Weihao and Lin, Kevin Qinghong and Gu, Yuchao and Chen, Zhijie and Yang, Zhenheng and Shou, Mike Zheng",
        "title": "Show-o: One single transformer to unify multimodal understanding and generation"
      },
      {
        "key": "zhou2024transfusion",
        "author": "Zhou, Chunting and Yu, Lili and Babu, Arun and Tirumala, Kushal and Yasunaga, Michihiro and Shamis, Leonid and Kahn, Jacob and Ma, Xuezhe and Zettlemoyer, Luke and Levy, Omer",
        "title": "Transfusion: Predict the next token and diffuse images with one multi-modal model"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "xiao2024omnigen",
        "author": "Xiao, Shitao and Wang, Yueze and Zhou, Junjie and Yuan, Huaying and Xing, Xingrun and Yan, Ruiran and Wang, Shuting and Huang, Tiejun and Liu, Zheng",
        "title": "Omnigen: Unified image generation"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "sun2024generative",
        "author": "Sun, Quan and Cui, Yufeng and Zhang, Xiaosong and Zhang, Fan and Yu, Qiying and Wang, Yueze and Rao, Yongming and Liu, Jingjing and Huang, Tiejun and Wang, Xinlong",
        "title": "Generative multimodal models are in-context learners"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "lewis2020retrieval",
        "author": "Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and K{\\\"u}ttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rockt{\\\"a}schel, Tim and others",
        "title": "Retrieval-augmented generation for knowledge-intensive nlp tasks"
      },
      {
        "key": "guu2020retrieval",
        "author": "Guu, Kelvin and Lee, Kenton and Tung, Zora and Pasupat, Panupong and Chang, Mingwei",
        "title": "Retrieval augmented language model pre-training"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "gao2023retrieval",
        "author": "Gao, Yunfan and Xiong, Yun and Gao, Xinyu and Jia, Kangxiang and Pan, Jinliu and Bi, Yuxi and Dai, Yi and Sun, Jiawei and Wang, Haofen",
        "title": "Retrieval-augmented generation for large language models: A survey"
      },
      {
        "key": "jiang2023active",
        "author": "Jiang, Zhengbao and Xu, Frank F and Gao, Luyu and Sun, Zhiqing and Liu, Qian and Dwivedi-Yu, Jane and Yang, Yiming and Callan, Jamie and Neubig, Graham",
        "title": "Active retrieval augmented generation"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "hashimoto2018retrieve",
        "author": "Hashimoto, Tatsunori B and Guu, Kelvin and Oren, Yonatan and Liang, Percy S",
        "title": "A retrieve-and-edit framework for predicting structured outputs"
      },
      {
        "key": "khandelwal2019generalization",
        "author": "Khandelwal, Urvashi and Levy, Omer and Jurafsky, Dan and Zettlemoyer, Luke and Lewis, Mike",
        "title": "Generalization through memorization: Nearest neighbor language models"
      },
      {
        "key": "shi2023replug",
        "author": "Shi, Weijia and Min, Sewon and Yasunaga, Michihiro and Seo, Minjoon and James, Rich and Lewis, Mike and Zettlemoyer, Luke and Yih, Wen-tau",
        "title": "Replug: Retrieval-augmented black-box language models"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "blattmann2022retrieval",
        "author": "Blattmann, Andreas and Rombach, Robin and Oktay, Kaan and M{\\\"u}ller, Jonas and Ommer, Bj{\\\"o}rn",
        "title": "Retrieval-augmented diffusion models"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "deng2009imagenet",
        "author": "Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li",
        "title": "Imagenet: A large-scale hierarchical image database"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "krause20133d",
        "author": "Krause, Jonathan and Stark, Michael and Deng, Jia and Fei-Fei, Li",
        "title": "3d object representations for fine-grained categorization"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "dataset2011novel",
        "author": "Dataset, E",
        "title": "Novel datasets for fine-grained image categorization"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "nilsback2008automated",
        "author": "Nilsback, Maria-Elena and Zisserman, Andrew",
        "title": "Automated flower classification over a large number of classes"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "khosla2020supervised",
        "author": "Khosla, Prannay and Teterwak, Piotr and Wang, Chen and Sarna, Aaron and Tian, Yonglong and Isola, Phillip and Maschinot, Aaron and Liu, Ce and Krishnan, Dilip",
        "title": "Supervised contrastive learning"
      },
      {
        "key": "le2020contrastive",
        "author": "Le-Khac, Phuc H and Healy, Graham and Smeaton, Alan F",
        "title": "Contrastive representation learning: A framework and review"
      }
    ]
  },
  {
    "index": 18,
    "papers": [
      {
        "key": "chen2020simple",
        "author": "Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey",
        "title": "A simple framework for contrastive learning of visual representations"
      }
    ]
  },
  {
    "index": 19,
    "papers": [
      {
        "key": "he2020momentum",
        "author": "He, Kaiming and Fan, Haoqi and Wu, Yuxin and Xie, Saining and Girshick, Ross",
        "title": "Momentum contrast for unsupervised visual representation learning"
      }
    ]
  },
  {
    "index": 20,
    "papers": [
      {
        "key": "radford2021learning",
        "author": "Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others",
        "title": "Learning transferable visual models from natural language supervision"
      }
    ]
  },
  {
    "index": 21,
    "papers": [
      {
        "key": "radford2021learning",
        "author": "Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others",
        "title": "Learning transferable visual models from natural language supervision"
      },
      {
        "key": "sun2023learning",
        "author": "Sun, Weixuan and Zhang, Jiayi and Wang, Jianyuan and Liu, Zheyuan and Zhong, Yiran and Feng, Tianpeng and Guo, Yandong and Zhang, Yanhao and Barnes, Nick",
        "title": "Learning Audio-Visual Source Localization via False Negative Aware Contrastive Learning"
      },
      {
        "key": "likhosherstov2021polyvit",
        "author": "Likhosherstov, Valerii and Arnab, Anurag and Choromanski, Krzysztof and Lucic, Mario and Tay, Yi and Weller, Adrian and Dehghani, Mostafa",
        "title": "Polyvit: Co-training vision transformers on images, videos and audio"
      },
      {
        "key": "guzhov2022audioclip",
        "author": "Guzhov, Andrey and Raue, Federico and Hees, J{\\\"o}rn and Dengel, Andreas",
        "title": "Audioclip: Extending clip to image, text and audio"
      },
      {
        "key": "mahmud2023ave",
        "author": "Mahmud, Tanvir and Marculescu, Diana",
        "title": "Ave-clip: Audioclip-based multi-window temporal transformer for audio visual event localization"
      },
      {
        "key": "girdhar2023imagebind",
        "author": "Girdhar, Rohit and El-Nouby, Alaaeldin and Liu, Zhuang and Singh, Mannat and Alwala, Kalyan Vasudev and Joulin, Armand and Misra, Ishan",
        "title": "Imagebind: One embedding space to bind them all"
      }
    ]
  },
  {
    "index": 22,
    "papers": [
      {
        "key": "huang2023clover",
        "author": "Huang, Jingjia and Li, Yinan and Feng, Jiashi and Wu, Xinglong and Sun, Xiaoshuai and Ji, Rongrong",
        "title": "Clover: Towards a unified video-language alignment and fusion model"
      },
      {
        "key": "fang2021clip2video",
        "author": "Fang, Han and Xiong, Pengfei and Xu, Luhui and Chen, Yu",
        "title": "Clip2video: Mastering video-text retrieval via image clip"
      },
      {
        "key": "luo2022clip4clip",
        "author": "Luo, Huaishao and Ji, Lei and Zhong, Ming and Chen, Yang and Lei, Wen and Duan, Nan and Li, Tianrui",
        "title": "Clip4clip: An empirical study of clip for end to end video clip retrieval and captioning"
      },
      {
        "key": "xue2022clip",
        "author": "Xue, Hongwei and Sun, Yuchong and Liu, Bei and Fu, Jianlong and Song, Ruihua and Li, Houqiang and Luo, Jiebo",
        "title": "Clip-vip: Adapting pre-trained image-text model to video-language representation alignment"
      },
      {
        "key": "zhu2023languagebind",
        "author": "Zhu, Bin and Lin, Bin and Ning, Munan and Yan, Yang and Cui, Jiaxi and Wang, HongFa and Pang, Yatian and Jiang, Wenhao and Zhang, Junwu and Li, Zongwei and others",
        "title": "Languagebind: Extending video-language pretraining to n-modality by language-based semantic alignment"
      }
    ]
  },
  {
    "index": 23,
    "papers": [
      {
        "key": "zhang2022pointclip",
        "author": "Zhang, Renrui and Guo, Ziyu and Zhang, Wei and Li, Kunchang and Miao, Xupeng and Cui, Bin and Qiao, Yu and Gao, Peng and Li, Hongsheng",
        "title": "Pointclip: Point cloud understanding by clip"
      },
      {
        "key": "zhu2022pointclip",
        "author": "Zhu, Xiangyang and Zhang, Renrui and He, Bowei and Zeng, Ziyao and Zhang, Shanghang and Gao, Peng",
        "title": "Pointclip v2: Adapting clip for powerful 3d open-world learning"
      },
      {
        "key": "huang2022clip2point",
        "author": "Huang, Tianyu and Dong, Bowen and Yang, Yunhan and Huang, Xiaoshui and Lau, Rynson WH and Ouyang, Wanli and Zuo, Wangmeng",
        "title": "Clip2point: Transfer clip to point cloud classification with image-depth pre-training"
      },
      {
        "key": "guo2023point",
        "author": "Guo, Ziyu and Zhang, Renrui and Zhu, Xiangyang and Tang, Yiwen and Ma, Xianzheng and Han, Jiaming and Chen, Kexin and Gao, Peng and Li, Xianzhi and Li, Hongsheng and others",
        "title": "Point-Bind \\& Point-LLM: Aligning Point Cloud with Multi-modality for 3D Understanding, Generation, and Instruction Following"
      }
    ]
  },
  {
    "index": 24,
    "papers": [
      {
        "key": "yang2024binding",
        "author": "Yang, Fengyu and Feng, Chao and Chen, Ziyang and Park, Hyoungseob and Wang, Daniel and Dou, Yiming and Zeng, Ziyao and Chen, Xien and Gangopadhyay, Rit and Owens, Andrew and others",
        "title": "Binding touch to everything: Learning unified multimodal tactile representations"
      },
      {
        "key": "lei2024vit",
        "author": "Lei, Weixian and Ge, Yixiao and Yi, Kun and Zhang, Jianfeng and Gao, Difei and Sun, Dylan and Ge, Yuying and Shan, Ying and Shou, Mike Zheng",
        "title": "Vit-lens: Towards omni-modal representations"
      }
    ]
  },
  {
    "index": 25,
    "papers": [
      {
        "key": "kalantidis2020hard",
        "author": "Kalantidis, Yannis and Sariyildiz, Mert Bulent and Pion, Noe and Weinzaepfel, Philippe and Larlus, Diane",
        "title": "Hard negative mixing for contrastive learning"
      },
      {
        "key": "robinson2020contrastive",
        "author": "Robinson, Joshua and Chuang, Ching-Yao and Sra, Suvrit and Jegelka, Stefanie",
        "title": "Contrastive learning with hard negative samples"
      }
    ]
  },
  {
    "index": 26,
    "papers": [
      {
        "key": "zhu2022balanced",
        "author": "Zhu, Jianggang and Wang, Zheng and Chen, Jingjing and Chen, Yi-Ping Phoebe and Jiang, Yu-Gang",
        "title": "Balanced contrastive learning for long-tailed visual recognition"
      },
      {
        "key": "liu2022universal",
        "author": "Liu, Zhenghao and Xiong, Chenyan and Lv, Yuanhuiyi and Liu, Zhiyuan and Yu, Ge",
        "title": "Universal vision-language dense retrieval: Learning a unified representation space for multi-modal retrieval"
      }
    ]
  },
  {
    "index": 27,
    "papers": [
      {
        "key": "zhang2021cross",
        "author": "Zhang, Han and Koh, Jing Yu and Baldridge, Jason and Lee, Honglak and Yang, Yinfei",
        "title": "Cross-modal contrastive learning for text-to-image generation"
      }
    ]
  }
]