\section{Expanded Discussion of Heuristics for Scalability and Efficient Implementation}\label{sec:expanded-heuristics-discussion}

In this section, we expand on the heuristics proposed in Section~\ref{ssec:efficient-heuristics}.

\paragraph{Sampling gradients} Gradient sampling can be performed in parallel, since we assume gradients are evaluated at points drawn i.i.d.
Moreover, as discussed below, we hypothesize that to obtain a benefit from EGOP reparameterization it suffices to obtain an accurate estimate of only the leading eigenvectors of the EGOP eigenmatrix. Thus for functions with strong spectral decay it suffices to use a conservative number of gradient samples ($M\lesssim d$ for $M$ the number of samples and $d$ the number of problem parameters).

\paragraph{Forming EGOP Eigenbasis} 
For applications with large numbers of parameters, computing a full, exact eigendecomposition of the empirical EGOP matrix may pose a significant expense. However, we hypothesis that for functions with strong spectral decay it suffices to obtain accurate estimates of only the leading EGOP eigenvectors, and that use of any random orthonormal basis for the eigenspaces of smaller eigenvalues suffices. Indeed when using random sampling to empirically estimate the EGOP, the error in the estimate of the subspace of the leading $k$ eigenvectors is proportionate to the eigengap $\lambda_k/\lambda_{k+1}$ (for a formal statement of this result, see Corollary 3.10 in \citet{constantine2015active}). Thus when executing Algorithm~\ref{alg:meta-algorithm-block} with a small number of gradient samples (e.g. $M\lesssim d$), one is effectively only obtaining an estimate of the leading EGOP eigenvectors. In Section~\ref{sec:experimental-results}, we include results obtained by using a small number of gradient samples ($M=d$) to estimate the EGOP, placing these empirical studies within this regime.

In light of this differential error between the leading and smaller eigenspaces, we propose that to save computational expense one can employ techniques like randomized SVD in order to form $V_k \in \R^{d\times k}$, a matrix whose columns contain the estimated $k$ leading eigenvectors of the EGOP, and then use any random orthonomal basis for the orthogonal complement to form a complete $d\times d$ change-of-basis matrix.%\footnote{\AD{If we have time, we could elaborate on this procedure in the appendix}}. 

\paragraph{Storing and applying change-of-basis matrix.}
In key applications of adaptive algorithms, particularly training neural networks, the number of problem parameters may be so large that storing and applying change-of-basis matrices on iterations becomes a nontrivial expense. In this section we consider several heuristics tailored to these applications and we note that many of the heuristics from this section--particularly the \emph{block reparameterization} described below--also reduce the cost of sampling gradients and forming the EGOP eigenbasis.

For any objective, one can reduce the cost of maintaining and applying $V \in \R^{d\times d}$ by forming some sparse or factorized approximation of $V$. For example, consider computing a Kronecker-product approximation to $V$, i.e. finding matrices $V_1 \in \R^{m_1\times n_1}, V_2 \in \R^{m_2 \times n_2}$ for $m_1 m_2 = n_1 n_2 = d$ such that $V \approx V_1\otimes V_2$. This reduces the storage cost from $d^2$ to $m_1 n_1 + m_2 n_2$, which can be significantly lower depending on choice of the factorization dimensions $m_i, n_i$. Such a factorization is similar to the method proposed by \citet{vyas2024soap}.

Instead of computing and retaining a full orthogonal basis $V\in R^{d\times d}$, one could form a projection matrix $V\in \R^{d\times k}$ for some user-chosen reduced dimension $k$. This would require storing and applying a matrix of size  $\R^{n_{in} n_{out} \times k}$ instead of $\R^{n_{in} n_{out} \times n_{in} n_{out}}$, and the $k$ leading eigenvectors can be efficiently estimated using randomized SVD, as discussed earlier. \citet{zhao2024galore} studies a related method for reparameterizing adaptive methods using a partial basis, but find that it is necessary to periodically re-compute the projection in order to achieve good performance.

\paragraph{Block Reparameterization} Here we instantiate block reparameterization for multilayer neural networks. Given an $L$-layer neural network whose $i$th layer is parameterized by weight matrix $W_i\in \R^{\nin^{(i)}\times \nout^{(i)}}$ and bias vector $b_i\in \R^{\nout^{(i)}}$, we consider optimizing $f(\theta) = \textrm{loss}(\theta; X_{\textrm{train}}, y_{\textrm{train}})$ with respect to parameters
$ \theta = [(W_1, b_1),\dots,(W_L, b_L)].$

Rather than forming the full EGOP as described in Algorithm~\ref{alg:meta-algorithm-block}, we consider estimating the \textit{layer} EGOP matrices%\footnote{\AD{Notation: should develop separate commands for empirical vs exact EGOP matrices, since technically this is defining an empirical EGOP.}}
\[
    \EGOP^{(i)} \defeq \frac{1}{M}\sum_{k=1}^M \nabla_{W_i} f(\theta_k)\nabla_{W_i} f(\theta_k)^\T
\]
where $\nabla_{W_i} f(\theta_k) \in \R^{\nin^{(i)}\nout^{(i)}}$ is the vector of partial derivatives of $f$ w.r.t. the entries of $W_i$ evaluated at $\theta_k$, and the points $\{\theta_k\}_{k=1}^M \sim \rho$ are drawn i.i.d.. 

Given these $L$ empirical layer EGOP matrices, one can obtain $L$ change-of-basis matrices 
\[
    V^{(i)}\in \R^{\nin^{(i)}\nout^{(i)}\times \nin^{(i)}\nout^{(i)}}
\]
and form the objective
\[
    \tf(\theta) = f([(V^{(1)}W_1, b_1),\dots,(V^{(L)}W_L, b_L)]).
\]

Block EGOP reparameterization requires storing and applying $L$ orthonormal matrices, each of size $\nin^{(i)}\nout^{(i)}\times \nin^{(i)}\nout^{(i)}$. For deep neural networks, this can be considerably less expensive than storing and applying the global change-of-basis matrix, which would be of size $\sum_{i=1}^L \nin^{(i)}\nout^{(i)}\times \sum_{i=1}^L \nin^{(i)}\nout^{(i)}$. This may also reduce the sampling cost; each layer EGOP is of smaller dimension than the global EGOP and thus has a smaller number of eigenvectors to estimate. Thus the layer EGOP eigenbases may be estimated with fewer total gradient samples.

Another benefit to block EGOP reparameterization is that it offers an easy way to reduce cost by choosing to only reparameterize a subset of layers. Thus for example if a network has a subset of layers which are too wide to efficiently reparameterize, one can choose to only reparameterize the narrower layers. 

For networks where the first layer involves a linear transformation of the input data, reparameterizing only the first layer corresponds to an orthogonal transformation of the data. Thus pre-computing the EGOP eigenbasis and applying this matrix to the input data up-front would allow one to reparameterize the first layer, without requiring one to store and apply the change-of-basis matrix during training.