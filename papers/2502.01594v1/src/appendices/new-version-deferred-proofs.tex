\section{Deferred Proofs}\label{sec:deferred-proofs}

\paragraph{Notation for proofs} Throughout this section, we denote the unit sphere in $d$ dimensions by $\Sbb^{d-1}$ and the Euclidean ball of radius $r$ centered at $\bar{x}$ by $\mathcal{B}(\bar{x}; r)$. Given a matrix $A \in \R^{m \times d}$, we write $\frobnorm{A} := \sqrt{\ip{A, A}}$ for its \emph{Frobenius} norm and $\opnorm{A} = \sup_{x \in \Sbb^{d-1}} \norm{Ax}$ for its \emph{spectral} norm. For matrices $A, B \in \R^{d\times d}$, we denote the L\"owner ordering by $A\preceq B$. We write $A \lesssim B$ to indicate the existence of a dimension-independent positive constant $c > 0$ such that $A \leq cB$.  We write $\mathcal{O}(d, r) := \set{X \in \R^{d \times r} \mid X^{\T} X = I_{r}}$ and $\mathcal{O}(d) \equiv \mathcal{O}(d, d)$ for the set of matrices with orthogonal columns.

We write $\ip{X, Y} \defeq \tr(X^{\T} Y)$ for the Euclidean inner product and $\norm{X} = \sqrt{\ip{X, X}}$ for its induced norm. Given a PSD matrix $H$ we write $\ip{X, Y}_{H}$ for the weighted inner product $\ip{HX, Y} = \ip{H^{1/2} X, H^{1/2} Y}$.  We write $\mathcal{O}(d)$ for the set of matrices with orthogonal columns. Given a set of scalars $\{L_i\}_{i=1}^d$, we let $\diag(L_1,\dots,L_d)$ denote the $d\times d$ diagonal matrix with values $L_1,\dots, L_d$ along the diagonal.  We let $\mat(\theta)\in \R^{m\times n}$ denote the reshaping of $\theta$ into a matrix for some $m$ and $n$ such that $m n = d$, and similarly we let $\vec{M}$ denote the reshaping of a matrix into a vector. 

Recall the definition of coordinate-wise smoothness constants in \cref{eq:def-coordinate-wise-smoothness}. If there exist constants $\{L_i\}_{i=1}^d$ such that $f(\cdot)$ satisfies \cref{eq:def-coordinate-wise-smoothness} for all $\theta \in \Theta\subseteq \R^d$ with respect to $\{L_i\}_{i=1}^d$, then we will refer to these $\{L_i\}_{i=1}^d$ as coordinate-wise smoothness constants of $f(\cdot)$ \textit{within domain } $\Theta$.

\subsection{Proofs from Section~\ref{sec:convergence-analysis}}

\input{src/appendices/proof-of-main-result}

Theorem~\ref{thm:simplified-smoothness-ratio} follows from Lemmas~\ref{claim:OG-coor-LB} and \ref{claim:reparam-coor-UB}. In order to prove these lemmas, we first note some consequences of Assumption~\ref{assume:Lipschitz-Hessian} and establish some helper lemmas.

\paragraph{Helper Lemmas} In order to prove Claims~\ref{claim:OG-coor-LB} and \ref{claim:reparam-coor-UB}, we establish the following intermediary results. The first lemma shows that Assumption~\ref{assume:Lipschitz-Hessian} implies the following bounds on the Hessian and third-derivatives of $f(\cdot)$:
\begin{lemma}\label{lemma:Lipschitz-Hessian-consequences}
    Given $f(\cdot)$ satisfying Assumption~\ref{assume:Lipschitz-Hessian}, it follows that for all $\theta, v\in \R^d$,
    \[
        \opnorm{\nabla^3 f(\theta)[v,v]} \leq H \sqrt{d}\norm{v}^2
    \]
    where $\nabla^3 f(\theta)[v,v]$ denotes the action of the third-derivative tensor at $\theta$ on two copies of the vector $v$. Also, for any $\forall v, \theta_1, \theta_2\in \R^d$, $f(\cdot)$ satisfies 
    \[
        \abs{\ip{v, \nabla^2 f(\theta_1)v}} \leq \langle v, \nabla^2 f(\theta_2)\rangle  + H\norm{\theta_1-\theta_2}_2 \norm{v}^2_2.
    \]
\end{lemma}
\begin{proof}
    By definition, the third derivative tensor $\nabla^3 f(\theta)[v,v]$ satisfies
    \begin{align*}
        \norm{\nabla^3 f(\theta)[v,v]}^2_2 = \sum_{i=1}^d \left(\sum_{j,k=1}^d v_j v_k \frac{\partial^3 f(\theta)}{\partial \theta_i \partial \theta_j \partial \theta_k}\right)^2 = \sum_{i=1}^d \left(\ip{v, \left(\frac{\partial }{\partial \theta_i} \nabla^2 f(\theta)\right)v}\right)^2
    \end{align*}
    By the limit definition of the derivative,
    \[
        \left|\ip{v, \left(\frac{\partial }{\partial \theta_i} \nabla^2 f(\theta)\right)v}\right| \leq \opnorm[\Big]{\lim_{h\rightarrow 0} \frac{\nabla^2 f(\theta + he_i)- \nabla^2 f(\theta)}{h}} \norm{v}^2
    \]
    Assumption~\ref{assume:Lipschitz-Hessian} implies
    \[
        \norm{\lim_{h\rightarrow 0} \frac{\nabla^2 f(\theta + he_i)- \nabla^2 f(\theta)}{h}}_2 \leq  \lim_{h\rightarrow 0} \frac{\norm{\nabla^2 f(\theta + he_i)- \nabla^2 f(\theta_2)}_2}{\abs{h}} \leq H
    \]
    We can thus bound the action of the third-derivative tensor as
    \begin{align*}
        \norm{\nabla^3 f(\theta)[v,v]}^2_2 &= \sum_{i=1}^d \left(\ip{v, \left(\frac{\partial }{\partial \theta_i} \nabla^2 f(\theta)\right)v}\right)^2
        \leq \sum_{i=1}^d \left(H \norm{v}^2_2\right)^2 = H^2 d \norm{v}^4_2
    \end{align*}
    which yields the first part of the lemma.

    For the second result, we note that Assumption~\ref{assume:Lipschitz-Hessian} implies that $\forall v, \theta_1, \theta_2\in \R^d$, 
    \[
        \abs{\ip{v, \left(\nabla^2 f(\theta_1) - \nabla^2 f(\theta_2)\right) v}} \leq \norm{\nabla^2 f(\theta_1)-\nabla^2 f(\theta_2)}_2 \norm{v}^2_2 \leq H \norm{\theta_1 - \theta_2}_2 \norm{v}^2_2.
    \]
    Thus
    \[
        \abs{\ip{v, \nabla^2 f(\theta_1)v}} - \ip{v, \nabla^2 f(\theta_2)v} \leq \abs{\ip{v, \left(\nabla^2 f(\theta_1) - \nabla^2 f(\theta_2)\right) v}} \leq  H \norm{\theta_1 - \theta_2}_2 \norm{v}^2_2
    \]
    and re-arranging the last line yields the stated bound.
\end{proof}

The second lemma gives an alternative characterization of the coordinate wise smoothness constants defined in Eq.~\ref{eq:def-coordinate-wise-smoothness}. Throughout the proofs presented in this section, will use the following notation: given constants $L_1,\dots,L_d$, we denote by $\diag(L)$ the $d\times d$ diagonal matrix with values $L_1,\dots,L_d$ along the diagonal. 

\begin{lemma}\label{lem:Hessian-coor-wise-smooth}
    A twice-differentiable function $f:\R^d\rightarrow \R$ satisfies Eq.~\ref{eq:def-coordinate-wise-smoothness} with respect to smoothness constants $L_1,\dots,L_d$ if and only if $\forall \theta\in \R^d, \forall v\in \R^d$,
    \[
        \abs{\ip{v, \nabla^2 f(\theta) v}} \leq \ip{v, \diag(L) v}
    \]
    where $\diag(L) \in \R^{d\times d}$ is the diagonal matrix with diagonal entries $L_i$.
\end{lemma}
\begin{proof}[Proof of Lemma~\ref{lem:Hessian-coor-wise-smooth}]
    By definition, $f$ is twice-differentiable if and only if satisfies
    \begin{equation}
        f(y) = f(x) + \ip{\grad f(x), y - x} + \ip{y - x, \grad^2 f(x)(y - x)} + o(\|y - x\|^2),
        \quad \text{for all $x, y \in \R^d$.}
        \label{eq:2nd-order-frechet}
    \end{equation}
    We first prove the ``if'' version. By the mean value theorem,
    \begin{align*}
        f(y) - f(x) - \ip{\grad f(x), y - x} &=
        \int_{0}^1 t \ip{y - x, \grad^2 f(x + t(y - x)) (y - x)} \mathrm{d}t \\
        &\leq
        \int_{0}^1 t \ip{y - x, \diag(L)(y - x)} \mathrm{d}t \\
        &=
        \ip{y - x, \diag(L)(y - x)},
    \end{align*}
    where the inequality follows by assumption. Taking absolute values on both sides yields the claim.

    We now prove the ``only if'' part. In particular, writing $y = x + tv$ and invoking~\eqref{eq:2nd-order-frechet}, we obtain
    \begin{align*}
        \abs{\ip{y - x, \grad^2 f(x)(y - x)}} &\leq
        \abs{f(y) - f(x) - \ip{\grad f(x), y - x}} +
        o(\|y - x\|^2) \\
        &\leq
        \ip{y - x, \diag(L)(y - x)} + o(\|y - x\|^2) \\
        \Leftrightarrow
        t^2 \abs{\ip{v, \grad^2 f(x) v}} &\leq
        t^2 \ip{v, \diag(L)v} + o(t^2).
    \end{align*}
    Dividing both sides by $t$ and letting $t \downarrow 0$ implies the result.
\end{proof}

The third intermediate result relates the EGOP to the Hessian.

\begin{lemma}\label{lem:EGOP-equation}
    Consider thrice-differentiable function $f:\R^d \rightarrow \R$ satisfying Assumption~\ref{assume:Lipschitz-Hessian}, and sampling distribution $\rho$ satisfying Assumption~\ref{assumption:sampling-distribution} with respect to local minimum $\theta^*$. The EGOP of $f(\cdot)$ with respect to $\rho$ satisfies
    \[
        \mathbb{E}_{\theta\sim \rho}[\nabla f(\theta)\nabla f(\theta)^\T] = H(\theta^*) + E_f(\theta^*)
    \]
    where $H(\theta^*)$ is a PSD matrix satisfying
    \[
        c^2 \nabla^2 f(\theta^*)\nabla^2 f(\theta^*)^\T \preceq H(\theta^*) \preceq 2c^2 \nabla^2 f(\theta^*)\nabla^2 f(\theta^*)^\T
    \]
    and the matrix $E_f(\theta^*)$ satisfies
    \[
    \abs{\ip{v, E_f (\theta^*) v}} \leq \left(2\sqrt{d}H \lambda_{\max}(\nabla^2 f(\theta^*)) M_3 + dH^2 M_4\right)\norm{v}^2_2
   \]
   where
   \[
    M_p \defeq \mathbb{E}_{\theta\sim\rho}[\norm{\theta-\theta^*}^{p}_2].
   \]
\end{lemma}

\begin{proof}[Proof of Lemma~\ref{lem:EGOP-equation}]
    Given $f(\cdot)$ thrice-differentiable, the gradient $\nabla f:\R^d \rightarrow \R^d$ is a twice differentiable map. Thus applying Taylor's theorem to the gradient map implies that for any point $\theta$ there exists $\xi\in \R^d$ with entries $\xi_i = \alpha_i \theta_i + (1-\alpha_i)\theta^*_i$ for some $\alpha_i\in [0,1]$ such that
    \[
        \nabla f(\theta) = \nabla f(\theta^*) + \nabla^2 f(\theta^*)(\theta-\theta^*) + \frac{1}{2}\nabla^3 f(\xi)[\theta-\theta^*, \theta-\theta^*]
   \]
   where $\nabla^3 f(\xi)[\cdot, \cdot]$ denotes the action of the derivative of third-derivatives of $f(\cdot)$: for any $v\in \R^d$, the vector $\nabla^3 f(\xi)[v, v]\in \R^d$ has entries
   \[
        \big(\nabla^3 f(\xi)[v, v]\big)_i \defeq \sum_{j=1}^d v_j \sum_{k=1}^d v_k\cdot \frac{\partial^3 f}{\partial \theta_i \partial \theta_j \partial \theta_k}\Bigg|_{\theta=\xi}.
   \]
   For ease of notation, we let $R_f(\theta)\in \R^d$ denote the vector $R_f(\theta)\defeq \frac{1}{2}\nabla^3 f(\xi)[\theta-\theta^*, \theta-\theta^*]$ satisfying
   \[
        \nabla f(\theta) = \nabla f(\theta^*) + \nabla^2 f(\theta^*)(\theta-\theta^*) + R_f(\theta).
   \]
   Given $\theta^*$ a stationary point of $f(\cdot)$, $\nabla f(\theta^*) = 0$, so the above simplifies to
   \[
     \nabla f(\theta) = \nabla^2 f(\theta^*)(\theta-\theta^*) + R_f(\theta).
   \]
   We can substitute the above expression into the definition of the EGOP matrix:
   \begin{align*}
       \mathbb{E}_{\theta\sim \rho}[\nabla f(\theta)\nabla f(\theta)^\T] &= \mathbb{E}_{\theta\sim \rho}\left[\left( \nabla^2 f(\theta^*)(\theta-\theta^*) + R_f(\theta)\right)\left( \nabla^2 f(\theta^*)(\theta-\theta^*) + R_f(\theta)\right)^\T  \right]\\
       &=\mathbb{E}_{\theta\sim \rho}\left[\nabla^2 f(\theta^*)(\theta-\theta^*)(\theta-\theta^*)^\T \nabla^2 f(\theta^*)^\T\right] + E_f(\theta^*)
   \end{align*}
   where
   \[
        E_f(\theta^*) \defeq \mathbb{E}_{\theta\sim \rho}[R_f(\theta)(\theta-\theta^*)^\T\nabla^2 f(\theta^*)^\T + \nabla^2 f(\theta^*)(\theta-\theta^*)R_f(\theta)^\T + R_f(\theta) R_f(\theta)^\T].
   \]

   We can simplify the first term by leveraging Assumption~\ref{assumption:sampling-distribution}: 
   \begin{align*}
       \mathbb{E}_{\theta\sim \rho}\left[\nabla^2 f(\theta^*)(\theta-\theta^*)(\theta-\theta^*)^\T \nabla^2 f(\theta^*)^\T\right] &= \nabla^2 f(\theta^*) \mathbb{E}_{\theta\sim \rho}\left[(\theta-\theta^*)(\theta-\theta^*)^\T\right] \nabla^2 f(\theta^*)^\T\\
       &=\nabla^2 f(\theta^*) \mathbb{E}_{\theta\sim \rho}\left[\theta\theta^\T-\theta(\theta^*)^\T -\theta^* \theta^\T + \theta^* (\theta^*)^\T\right] \nabla^2 f(\theta^*)^\T\\
       &=\nabla^2 f(\theta^*) \left(\mathbb{E}_{\theta\sim \rho}\left[\theta\theta^\T\right]+ \theta^* (\theta^*)^\T\right) \nabla^2 f(\theta^*)^\T\\
       &=\nabla^2 f(\theta^*) \left(c^2 \mathbb{I}+ \theta^* (\theta^*)^\T\right) \nabla^2 f(\theta^*)^\T
   \end{align*}
   where the penultimate equality follows from the assumption that $\rho$ is mean-zero, and the last inequality follows from the assumption that $\rho$ is isotropic. Because $\theta^*$ is a local minimum, $\nabla^2 f(\theta^*)$ is PSD, and further the rank-1 outer product $\theta^* (\theta^*)^\T$ is also PSD,  so we have
   \[
    \nabla^2 f(\theta^*) \left(c^2 \mathbb{I}+ \theta^* (\theta^*)^\T\right) \nabla^2 f(\theta^*)^\T \succeq c^2 \nabla^2 f(\theta^*)\nabla^2 f(\theta^*)^\T.
   \]
   Moreover, Assumption~\ref{assumption:sampling-distribution}, $\norm{\theta^*}^2_2\leq c^2$, which implies
   \[
        \theta^* (\theta^*)^\T \preceq c^2 \mathbb{I}.
   \]
   This implies
   \[
    \nabla^2 f(\theta^*) \left(c^2 \mathbb{I}+ \theta^* (\theta^*)^\T\right) \nabla^2 f(\theta^*)^\T \preceq 2c^2 \nabla^2 f(\theta^*)\nabla^2 f(\theta^*)^\T.
   \]
   Combining these two matrix inequalities yields the matrix inequalities for 
   \[
    H(\theta^*) \defeq \nabla^2 f(\theta^*) \left(c^2 \mathbb{I}+ \theta^* (\theta^*)^\T\right) \nabla^2 f(\theta^*)^\T.
   \]

   Lastly, we bound $\abs{\ip{v, E_f(\theta^*)v}}$ for $E_f(\theta^*)$ defined above. Applying triangle inequality, Jensen's inequality,  and Cauchy-Schwarz, to the definition of $E_f(\theta^*)$ yields
   \begin{align*}
       \abs{\ip{v, E_f (\theta^*) v}} &\leq \mathbb{E}_{\theta\sim \rho} \left[ 2\abs{\ip{v, R_f(\theta)}}\cdot \abs{\langle \theta-\theta^*, \nabla^2 f(\theta^*) v\rangle}+ \abs{\ip{v, R_f(\theta)}^2}\right]\\
       &\leq \mathbb{E}_{\theta\sim \rho} \left[ 2 \norm{R_f(\theta)}_2 \norm{\theta-\theta^*}_2 \lambda_{\max}(\nabla^2 f(\theta^*))\norm{v}^2_2 + \norm{R_f(\theta)}^2_2\norm{v}^2_2\right].
   \end{align*}
   Lemma~\ref{lemma:Lipschitz-Hessian-consequences} establishes that under Assumption~\ref{assume:Lipschitz-Hessian}, $\norm{R_f(\theta)}_2 \leq H\sqrt{d}\norm{\theta-\theta^*}^2_2.$ Substituting this into the above bound yields
   \[
    \abs{\ip{v, E_f (\theta^*) v}} \leq \left(2\sqrt{d}H \lambda_{\max}(\nabla^2 f(\theta^*)) M_3 + dH^2 M_4\right)\norm{v}^2_2
   \]
   where
   \[
    M_p \defeq \mathbb{E}_{\theta\sim\rho}[\norm{\theta-\theta^*}^{p}_2].
   \]
\end{proof}

We'll also use the following basic results from linear algebra, which we prove here for completeness.
\begin{lemma}\label{lemma:relate-square-mat-to-self}
    Given $A \in \R^{d\times d}$ a symmetric PSD matrix, for any vector $v\in \R^d$, 
    \[
        \ip{v, A^\T A v} \leq \lambda_{\max}(A)\ip{v, A v}
    \]
    where $\lambda_{\max}(A)$ denotes the largest eigenvalue of $A$.
\end{lemma}
\begin{proof}[Proof of Lemma~\ref{lemma:relate-square-mat-to-self}]
    Since $A$ is PSD, it admits a square root $A^{1/2}$ which is itself PSD. Consequently,
    \[
        \ip{Av, Av} =
        \norm{Av}^2 \leq
        \opnorm{A^{1/2}}^2 \| A^{1/2} v \|^2 =
        \lambda_{\max}(A) \ip{v, Av}.
    \]
\end{proof}

\begin{lemma}[Order-preserving square root]\label{lemma:order-preserving-square-root}
For any positive-semidefinite matrices $X, Y$, the following holds:
\begin{equation}
    X \preceq Y \implies X^{1/2} \preceq Y^{1/2}.
    \label{eq:sqrt-order-preserving}
\end{equation}
\end{lemma}
\begin{proof}
    Since $Y - X \succeq 0$, any unit vector $v$ satisfies
    \[
    0 \leq \ip{v, (Y - X) v} = \ip{v, (Y^{1/2} + X^{1/2})(Y^{1/2} - X^{1/2})v}.
    \]
    Since $X, Y$ are PSD, their square roots are well-defined and
    symmetric. In particular, the matrix $Y^{1/2} - X^{1/2}$ is symmetric and, as such, has real eigenvalues. Therefore, it suffices to prove all eigenvalues of this matrix are nonnegative.

    To that end, let $(\lambda, v)$ be an eigenpair of $Y^{1/2} - X^{1/2}$. We have
    \begin{equation}
    0 \leq \ip{(Y^{1/2} - X^{1/2})v, (Y^{1/2} + X^{1/2}) v} =
    \lambda \ip{v, (Y^{1/2} + X^{1/2}) v}.
    \label{eq:contradiction-stmt}
    \end{equation}
    In particular, assume $\lambda < 0$ (since otherwise there is nothing to show). Consequently,
    \[
    0 > \lambda = \ip{v, (Y^{1/2} - X^{1/2})v} \implies
    \ip{v, X^{1/2} v} > \ip{v, Y^{1/2} v} \geq 0,
    \]
    where the last inequality follows from the fact that $Y$ is PSD.
    In particular, this implies that $\ip{v, (Y^{1/2} + X^{1/2}) v} > 0$;
    combined with~\eqref{eq:contradiction-stmt} and the assumption
    $\lambda < 0$, this yields a contradiction. Therefore, $\lambda \geq 0$. Since the choice of eigenpair was arbitrary, the claim
    follows.
\end{proof}

\paragraph{Dense EGOP eigenvectors lead to large $\mathbf{L_f}$} Given the above helper lemmas, we are now equipped to prove Lemma~\ref{claim:OG-coor-LB}.
\begin{proof}[Proof of Lemma~\ref{claim:OG-coor-LB}]
    By Lemma~\ref{lem:Hessian-coor-wise-smooth},  the smoothness constants $L_1,\dots,L_d$ must satisfy
    \[
        \forall \theta, \forall v, \quad \ip{v, \diag(L) v} \geq \abs{\ip{v, \nabla^2 f(\theta) v}}.
    \]
    where $\diag(L)\in \R^{d\times d}$ is the diagonal matrix with entries $\diag(L)_{i,i} = L_i$. In particular, consider $\theta=\theta^*$, and let $\nu \in \{\pm d^{-1/2}\}^d$ be a dense unit vector whose entries all satisfy $\abs{\nu(i)} = d^{-1/2}$. Then the smoothness constants must satisfy
    \[
        \ip{\nu, \diag(L)\nu} =\frac{1}{d}\sum_{i=1}^d L_i \geq \ip{\nu, \nabla^2 f(\theta^*) \nu}.
    \]
    Given $\theta^*$ a local minimum, $\nabla^2 f(\theta^*)$ is PSD. Thus by Lemma~\ref{lemma:relate-square-mat-to-self}, 
    \[
        \ip{\nu, \nabla^2 f(\theta^*) \nu} \geq \frac{1}{\lambda_{\max}(\nabla^2 f(\theta^*))}\ip{\nu, \nabla^2 f(\theta^*)\nabla^2 f(\theta^*)^\T \nu}
    \]
    where $\lambda_{\max}(\nabla^2 f(\theta^*))$ denotes the leading eigenvalue of $\nabla^2 f(\theta^*)$. Moreover,  Lemma~\ref{lem:EGOP-equation} implies
    \begin{align*}
        \ip{\nu, \nabla^2 f(\theta^*) \nabla f(\theta^*)^\T \nu} &\geq \frac{1}{2c^2} \ip{\nu, H(\theta^*) \nu}=\frac{1}{2c^2}\ip{ \nu,\left(\mathbb{E}[\nabla f(\theta) \nabla f(\theta)^\T] - E_f(\theta^*)\right)\nu}.
    \end{align*}
    \[
        \ip{v_1, \nabla^2 f(\theta^*) \nabla f(\theta^*)^\T v_1} \geq \frac{\lambda_{\max}(\operatorname{EGOP}) - \ip{v_1, E_f(\theta^*) v_1}}{2 c^2}.
    \]
    
    Chaining the preceding inequalities yields
    \begin{equation}\label{eq:L-quadratic-LB}
        \ip{\nu, \diag(L) \nu} \geq \frac{\ip{\nu,\operatorname{EGOP}\nu} - \ip{\nu,E_f(\theta^*) \nu}}{2 c^2 \lambda_{\max}(\nabla^2 f(\theta^*))}.
    \end{equation}
    Given $\nabla^2 f(\theta^*)$ PSD, we have
    \[
        \lambda_{\max}(\nabla^2 f(\theta^*)) = \sqrt{\lambda_{\max}(\nabla^2 f(\theta^*))^2} = \sqrt{\lambda_{\max}(\nabla^2 f(\theta^*)\nabla^2 f(\theta^*)^\T)}.
    \]
    Moreover, by Lemma~\ref{lem:EGOP-equation} we have the following upper bound:
    \begin{align*}
        \lambda_{\max}(\nabla^2 f(\theta^*)\nabla^2 f(\theta^*)^\T) &\leq \frac{1}{c^2}\lambda_{\max}(H(\theta^*))\\
        &= \frac{1}{c^2} \max_{v\in \R^d} \ip{v, (\mathbb{E}[\nabla f(\theta)\nabla f(\theta)^\T] - E_f(\theta^*)) v}\\
        &\leq \frac{1}{c^2}\left(\lambda_{\max}(\operatorname{EGOP}) + \lambda_{\max}(E_f(\theta^*))\right).
    \end{align*}
    Combining this inequality with the preceding equation implies
    \[
        \lambda_{\max}(\nabla^2 f(\theta^*)) \leq \frac{1}{c}\sqrt{\lambda_{\max}(\operatorname{EGOP}) + \lambda_{\max}(E_f(\theta^*))}.
    \]
    Substituting this inequality into Eq.~\ref{eq:L-quadratic-LB} implies
    \[
        \ip{\nu, \diag(L) \nu} \geq \frac{\ip{\nu,\operatorname{EGOP}\nu} - \ip{\nu, E_f(\theta^*)\nu}}{2 c \sqrt{\lambda_{\max}(\operatorname{EGOP}) + \lambda_{\max}(E_f(\theta^*))}}.
    \]
    Recall that $\nu \in \{\pm d^{-1/2}\}^d$. Thus
    \[
        \ip{\nu, \diag(L) \nu} = \sum_{i=1}^d L_i v(i)^2 = \frac{1}{d}\sum_{i=1}^d L_i.
    \]
    Combining this with the above inequality implies
    \[
        \sum_{i=1}^d L_i \geq \frac{d\left(\ip{\nu,\operatorname{EGOP}\nu} - \ip{\nu, E_f(\theta^*)\nu}\right)}{2 c \sqrt{\lambda_{\max}(\operatorname{EGOP}) + \lambda_{\max}(E_f(\theta^*))}}.
    \]
    By Assumption~\ref{assume:Lipschitz-Hessian}, $\forall v\in \R^d$ it holds that
    \[
        \abs{\ip{v, E_f(\theta^*) v}} \leq \gamma \norm{v}^2_2
    \]
    where
    \[
        \gamma \defeq  2\sqrt{d}H \lambda_{\max}(\nabla^2 f(\theta^*)) M_3 + dH^2 M_4
    \]
    Thus
    \[
        \sum_{i=1}^d L_i \geq \frac{d}{2c}\frac{\ip{\nu,\operatorname{EGOP}\nu} - \gamma}{\sqrt{\lambda_{\max}(\operatorname{EGOP}) + \gamma}}.
    \]
    As this holds for all $\nu\in \{\pm d^{-1/2}\}^d$, we conclude 
    \[
        \sum_{i=1}^d L_i \geq \frac{d}{2c}\cdot \max_{\nu\in \{\pm d^{-1/2}\}^d}\frac{\ip{\nu,\operatorname{EGOP}\nu} - \gamma}{\sqrt{\lambda_{\max}(\operatorname{EGOP}) + \gamma}}.
    \]
    We observe that for any $v\in \R^d$,
    \[
        \ip{v, \EGOP v} = \sum_{i=1}^d \lambda_i \langle v, v_i\rangle^2 \geq \max_{k} \lambda_{k} \langle v, v_k\rangle^2
    \]
    where $(\lambda_i, v_i)$ denote the $i$\ts{th} eigenvalue and eigenvector of the EGOP indexed in decreasing order of magnitude. 
    
    In particular, for any $v_k$, consider $\nu_k\in \R^d$ defined to have entries $\nu_k(i) = \operatorname{sign}(v_k(i))d^{-1/2}$. Observe that 
    \[
        \nu_k \in \operatorname{argmax}\{\langle \nu, v_k\rangle^2 \mid  \nu\in \{\pm d^{-1/2}\}^d \}.
    \]
    For this $\nu_k$,
    \[
        \langle \nu, v_k\rangle^2 = \left(\sum_{i=1}^d \operatorname{sign}(v_k(i))d^{-1/2} v_k(i)\right)^2 = \frac{1}{d}\left(\sum_{i=1}^d \abs{v_k(i)}\right)^2 = \frac{1}{d}\norm{v_k}^2_1.
    \]
    Thus in general,
    \[
        \max_{\nu\in \{\pm d^{-1/2}\}^d} \ip{\nu, \EGOP \nu} = \max_{\nu\in \{\pm d^{-1/2}\}^d} \sum_{k=1}^d \lambda_k \langle \nu, v_k\rangle^2 \geq \max_k \frac{\lambda_k}{d}\norm{v_k}^2_1.
    \]
    This implies the lower bound
    \[
        \sum_{i=1}^d L_i \geq \frac{d}{2c}\cdot \max_{k}\frac{\lambda_k \norm{v_k}^2_1/d - \gamma}{\sqrt{\lambda_{\max}(\operatorname{EGOP}) + \gamma}}.
    \]
    When interpreting the above inequality, we note that as the $k$\ts{th} eigenvector becomes dense, $\norm{v_k}^2_1/d\rightarrow 1$. In particular, if $v_1 \in \{\pm d^{-1/2}\}^d$, then
    $
        \norm{v_1}^2_1 = (d^{-1/2}\cdot d)^2 = d
    $
    and thus
    $
        \max_{\nu\in \{\pm d^{-1/2}\}^d} \langle \nu, v_1\rangle^2 = 1.
    $ Thus under this assumption, the lower bound can be simplified to
    \[
        \sum_{i=1}^d L_i \geq \frac{d}{2c}\cdot \frac{\lambda_{\max}(\EGOP) - \gamma}{\sqrt{\lambda_{\max}(\operatorname{EGOP}) + \gamma}}.
    \]
\end{proof}

\paragraph{Reparameterization by EGOP eigenbasis produces small $\mathbf{L_{\tf}}$} In order to prove Lemma~\ref{claim:reparam-coor-UB}, we first establish the equivariance of the EGOP matrix: 
\begin{lemma}\label{lemma:equivariant-EGOP}
    For isotropic sampling distribution $\rho$, the EGOP matrix with respect to $\rho$ is equivariant under reparameterization by an orthonormal matrix: for $\tf(\theta)\defeq f(V\theta)$, 
    \[
        \mathbb{E}_{\theta\sim \rho}[\nabla \tf(\theta)\nabla \tf(\theta)^\T] = V^\T \mathbb{E}_{\theta\sim \rho}[\nabla f(\theta)\nabla f(\theta)^\T] V
    \]
\end{lemma}
\begin{proof}
    By the chain rule,
    \[
        \nabla \tf(\theta) = V^\T \nabla f(V\theta).
    \]
    Moreover, given $\rho$ isotropic, for any orthonormal $V$ we have $\rho(\theta)=\rho(V\theta)$. Thus 
    \begin{align*}
        \mathbb{E}_{\theta\sim \rho}[\nabla \tf(\theta)\nabla \tf(\theta)^\T]=V^\T\mathbb{E}_{\theta\sim \rho}[\nabla f(V\theta)f(V\theta)^\T]V=V^\T \mathbb{E}_{\theta\sim \rho}[\nabla f(\theta)\nabla f(\theta)^\T]V.
    \end{align*}
\end{proof}

An immediate corollary of Lemma~\ref{lemma:equivariant-EGOP} is that reparameterizing by the EGOP eigenbasis yields a diagonalized EGOP:

\begin{corollary}\label{cor:EGOP-reparam-diagonalizes-EGOP}
    Given $f(\cdot)$ whose EGOP with respect to isometric distribution $\rho$ has eigenvalue decomposition
    \[
        \mathbb{E}_{\theta\sim \rho}[\nabla f(\theta)\nabla f(\theta)^\T] = V \Lambda V^\T, 
    \]
    the reparameterized function $\tf(\theta)\defeq f(V\theta)$ satisfies
    \[
        \mathbb{E}_{\theta\sim \rho}[\nabla \tf(\theta)\nabla \tf(\theta)^\T] = V^\T \mathbb{E}_{\theta\sim \rho}[\nabla f(\theta)\nabla f(\theta)^\T]V = \Lambda.
    \]
\end{corollary}

Using Corollary~\ref{cor:EGOP-reparam-diagonalizes-EGOP}, we now prove Lemma~\ref{claim:reparam-coor-UB}.

\begin{proof}[Proof of Lemma~\ref{claim:reparam-coor-UB}]
    Given reparameterized function $\tf(\theta)\defeq f(V\theta)$, Corollary~\ref{cor:EGOP-reparam-diagonalizes-EGOP} implies $\mathbb{E}_{\theta\sim \rho}[\nabla \tf(\theta)\nabla \tf(\theta)^\T]$ is diagonal, and its entries are given by the eigenvalues of $\operatorname{EGOP}(f)\defeq \mathbb{E}_{\theta\sim \rho}[\nabla f(\theta)\nabla f(\theta)^\T]$. As the EGOP is an expectation of PSD matrices, these eigenvalues are all non-negative, and thus
    \[
        \mathbb{E}_{\theta\sim \rho}[\nabla \tf(\theta)\nabla \tf(\theta)^\T]^{1/2}_{i,i} = \sqrt{\lambda_i(\operatorname{EGOP}(f))}.
    \]
    
    
    Thus $\forall v\in \R^d$,
    \[
        \ip{v, \mathbb{E}_{\theta\sim \rho}[\nabla \tf(\theta)\nabla \tf(\theta)^\T]^{1/2} v} = \sum_{i=1}^d v(i)^2 \sqrt{\lambda_i(\operatorname{EGOP}(f))}.
    \]

    Applying Lemma~\ref{lem:EGOP-equation} to the function $\tf(\cdot)$ and its corresponding local minimum $V^\T\theta^*$ implies
    \[
        c^2 \nabla^2 \tf(V\theta^*)\nabla^2\tf (V\theta^*) \preceq H_{\tf}(\theta^*) = \mathbb{E}_{\theta\sim\rho}[\nabla\tf(\theta)\nabla\tf(\theta)^\T] - E_{\tf}(V\theta^*)
    \]
    where $E_{\tf}(V\theta^*)$ satisfies $\forall v\in \R^d$, $\abs{\ip{v, E_{\tf}(\theta^*)v}} \leq \gamma \norm{v}^2_2$ for
    \[
        \gamma \defeq  2\sqrt{d}H \lambda_{\max}(\nabla^2 f(\theta^*)) M_3 + dH^2 M_4
    \]
    Thus
    \[
        \mathbb{E}_{\theta\sim\rho}[\nabla\tf(\theta)\nabla\tf(\theta)^\T] - E_{\tf}(V\theta^*) \preceq \mathbb{E}_{\theta\sim\rho}[\nabla\tf(\theta)\nabla\tf(\theta)^\T] +\gamma\mathbb{I}.
    \]
    Chaining the above matrix inequalities implies
    \[
        c^2\nabla^2 \tf(V\theta^*)\nabla^2\tf (V\theta^*)  \preceq \mathbb{E}_{\theta\sim\rho}[\nabla\tf(\theta)\nabla\tf(\theta)^\T] +\gamma\mathbb{I}.
    \]

    Recall that the PSD ordering $X\preceq Y$ implies $X^{1/2} \preceq Y^{1/2}$ (for completeness, see statement and proof in Lemma~\ref{lemma:order-preserving-square-root} below). Thus the preceding matrix inequality implies
    \[
        \nabla^2 \tf(V^\T\theta^*) \preceq \frac{1}{c}\left(\mathbb{E}_{\theta\sim\rho}[\nabla\tf(\theta)\nabla\tf(\theta)^\T] +\gamma\mathbb{I}\right)^{1/2}
    \]
    Since $\mathbb{E}_{\theta\sim\rho}[\nabla\tf(\theta)\nabla\tf(\theta)^\T]$ is diagonal and $\gamma\mathbb{I}$ is diagonal, 
    \[
        \left(\mathbb{E}_{\theta\sim\rho}[\nabla\tf(\theta)\nabla\tf(\theta)^\T] +\gamma\mathbb{I}\right)^{1/2}_{i,i} = \sqrt{\mathbb{E}_{\theta\sim\rho}[\nabla\tf(\theta)\nabla\tf(\theta)^\T]_{i,i} + \gamma}.
    \]
    Thus for all $v\in \R^d$, we have
    \begin{equation}\label{eq:UB-intermediate-quadratic}
        \ip{v, \nabla^2 \tf(V^\T \theta^*) v} \leq \frac{1}{c}\sum_{i=1}^d v(i)^2 \sqrt{\mathbb{E}_{\theta\sim\rho}[\nabla\tf(\theta)\nabla\tf(\theta)^\T]_{i,i} + \gamma}.
    \end{equation}
    
    Lemma~\ref{lemma:Lipschitz-Hessian-consequences} states that under Assumption~\ref{assume:Lipschitz-Hessian}, $\forall \theta,v\in \R^d$,
    \[
        \abs{\ip{v, \nabla^2 f(\theta)v}} \leq \ip{v, \nabla^2 f(\theta^*)v} + H \norm{\theta-\theta^*}_2 \norm{v}^2_2.
    \]

    Applying chain rule and using the fact that orthonormal transformations are bijections on $\R^d$, this implies that $\forall \theta,v\in \R^d$,
    \[
        \abs{\ip{v, \nabla^2 \tf(\theta)v}} \leq
        \ip{v, \nabla^2 \tf(V\theta^*)v} + H \norm{\theta-\theta^*}_2 \norm{v}^2_2.
    \]
    Combining this with the inequality in Eq.~\ref{eq:UB-intermediate-quadratic} implies that $\forall v\in \R^d$,
    \[
        \abs{\ip{v, \nabla^2 \tf(\theta)v}} \leq\sum_{i=1}^d v(i)^2 \left(\frac{1}{c}\sqrt{\mathbb{E}_{\theta\sim\rho}[\nabla\tf(\theta)\nabla\tf(\theta)^\T]_{i,i} + \gamma} + H\norm{\theta-\theta^*}_2\right).
    \]
    Moreover, Lemma~\ref{lemma:equivariant-EGOP} implies
    \[
        \mathbb{E}_{\theta\sim\rho}[\nabla\tf(\theta)\nabla\tf(\theta)^\T]_{i,i} = \lambda_i(\operatorname{EGOP}(f)).
    \]
    Thus by Lemma~\ref{lem:Hessian-coor-wise-smooth}, this implies that in a ball of radius $B$ centered at the origin, $\tf(\cdot)$ satisfies coordinate-wise smoothness with respect to constants
    \[
        L_i \leq \frac{1}{c}\sqrt{\lambda_i(\operatorname{EGOP}(f))+\gamma} + H(B+c).
    \]
    This implies the sum of the coordinate-wise smoothness constants is bounded above as
    \begin{align*}
        L_{\tf} &\leq  \sum_{i=1}^d \left(\frac{1}{c}\sqrt{\lambda_i(\operatorname{EGOP}(f))+\gamma}  + H(B+c)\right)\\
        &\leq d\left(\frac{\sqrt{\gamma}}{c} + H(B+c)\right)+ \frac{1}{c}\sum_{i=1}^d \sqrt{\lambda_i(\operatorname{EGOP}(f))}.
    \end{align*}
\end{proof}

\paragraph{Proof of Theorem~\ref{thm:simplified-smoothness-ratio}} Lemmas~\ref{claim:OG-coor-LB} and \ref{claim:reparam-coor-UB} imply Theorem~\ref{thm:simplified-smoothness-ratio}:
\begin{proof}[Proof of Theorem~\ref{thm:simplified-smoothness-ratio}]    
    By definition of $\gamma$, if $H$ satisfies
    \[
        H\leq \frac{1}{M_4 \sqrt{d}}\left(\sqrt{\lambda^2_{\max}(\nabla^2 f(\theta^*)) M_3^2 + \Delta M_4} - \lambda_{\max}(\nabla^2 f(\theta^*)) M_3\right),
    \]
    then $\gamma\leq \Delta$. Thus given $H$ satisfying
    \begin{equation}\label{eq:H-cond-1}
        H\leq \frac{\HessLambdaMax}{M_4 \sqrt{d}}\left(\sqrt{M_3^2 + \zeta c^2 M_4} -  M_3\right)
    \end{equation}
    we have $\gamma \leq \zeta c^2 \HessLambdaMax$. In particular, \cref{lem:EGOP-equation} implies
    \[
        c^2 \left(\HessLambdaMax\right)^2 -\gamma \leq \EGOPLambdaMax \leq c^2 \left(\HessLambdaMax\right)^2 +\gamma. 
    \]
    Thus if $H$ satisfies \cref{eq:H-cond-1} for some $\zeta \in [0,1]$, then this implies
    \[
        (1-\zeta)c^2 \left(\HessLambdaMax\right)^2 \leq \EGOPLambdaMax \leq (1+\zeta)c^2 \left(\HessLambdaMax\right)^2
    \]
    and thus that $\gamma \leq \delta \EGOPLambdaMax$ for $\delta \defeq \zeta/(1-\zeta)$.

    Recall that by \cref{lem:general-LB}, if $\norm{v_1}^2_1/d\geq \beta$, then 
    \[
        L_f \geq \frac{d}{2c}\cdot \frac{\beta \EGOPLambdaMax - \gamma}{\sqrt{\lambda_{\max}(\operatorname{EGOP}) + \gamma}}.
    \]
    Under \cref{eq:H-cond-1},
    \[
        \frac{\beta \EGOPLambdaMax - \gamma}{\sqrt{\lambda_{\max}(\operatorname{EGOP}) + \gamma}} \geq \frac{(\beta-\delta)}{\sqrt{1+\delta}}\sqrt{\EGOPLambdaMax}.
    \]
    Thus under \cref{eq:H-cond-1}, \cref{lem:general-LB} and \cref{claim:reparam-coor-UB} jointly imply
    \begin{align}
        \frac{L_{\tf}}{L_f} &\leq \left(d\left(\frac{\sqrt{\gamma}}{c} + H(B+c)\right)+ \frac{1}{c}\sum_{i=1}^d \sqrt{\lambda_i}\right)\left(\frac{d}{2c}\cdot\frac{(\beta-\delta)}{\sqrt{1+\delta}}\sqrt{\EGOPLambdaMax}\right)^{-1}\\
        &\label{eq:3-terms}=\frac{2\sqrt{1+\delta}}{(\beta-\delta)}\frac{\sr_f}{d} + \frac{2\sqrt{\gamma(1+\delta)}}{(\beta-\delta)\sqrt{\EGOPLambdaMax}} + \frac{2Hc(B+c)\sqrt{1+\delta}}{(\beta-\delta)\sqrt{\EGOPLambdaMax}}.
    \end{align}
    We first simplify and bound the central term in \cref{eq:3-terms}. Recall that under \cref{eq:H-cond-1}, $\gamma \leq \delta \EGOPLambdaMax$. Thus
    \[
        \frac{2\sqrt{\gamma(1+\delta)}}{(\beta-\delta)\sqrt{\EGOPLambdaMax}} \leq \frac{2\sqrt{\delta(1+\delta)}}{\beta-\delta}.
    \]
    In particular, if $H$ satisfies \cref{eq:H-cond-1} for the value
    \[
        \zeta \leq \frac{\beta \varepsilon\sqrt{1-\varepsilon^2}}{\beta \varepsilon\sqrt{1-\varepsilon^2} + 4-\varepsilon^2}
    \]
    then
    \[
        \delta  = \frac{\zeta}{1-\zeta}\leq \frac{2\beta \varepsilon}{4-\varepsilon^2}\sqrt{1-\varepsilon^2}
    \]
    which in turn implies
    \[
        \frac{2\sqrt{\delta(1+\delta)}}{\beta-\delta} \leq \varepsilon.
    \]

    We now bound the last term in \cref{eq:3-terms}. Recall if $H$ satisfies \cref{eq:H-cond-1}, then
    \[
        \frac{c^2\left(\HessLambdaMax\right)^2}{1+\delta} \leq \EGOPLambdaMax.
    \]
    Thus if $H$ also satisfies
    \begin{equation}\label{eq:H-cond-2}
        H \leq \frac{\varepsilon\HessLambdaMax (\beta-\delta)}{2(B+c)(1+\delta)}
    \end{equation}
    then
    \[
        \frac{2Hc(B+c)\sqrt{1+\delta}}{(\beta-\delta)\sqrt{\EGOPLambdaMax}} \leq \varepsilon.
    \]
    Thus for $H$ satisfying \cref{eq:H-cond-1} and \cref{eq:H-cond-2},
    \[
        \frac{L_{\tf}}{L_f}\leq \frac{2\sqrt{1+\delta}}{(\beta-\delta)}\frac{\sr_f}{d} +2\varepsilon.
    \]
\end{proof}

 \subsubsection{Lipschitz Constants of Hessians in Machine Learning}\label{ssec:ML-lipschitz-Hessians}

 In this section, we note one family of naturally-motivated non-convex objectives satisfying Assumption~\ref{assume:Lipschitz-Hessian} that arise in machine learning problems. We consider the over-parameterized matrix factorization problem: let $\theta \in \R^{(d_1+d_2)k}$ be parameters, whose entries can be grouped into two matrices as $\theta = (L,R)$ for $L\in \R^{d_1\times k}, R\in \R^{d_2\times k}$. We define the objective 
\begin{equation}\label{eq:linear-measurement-overparam-objective}
    f(\theta) = \norm{\mathcal{A}(LR^\T) - b}^2_2
\end{equation}
where $b \in \R^m$ correspond to measurements of some matrix under map $\mathcal{A}(\cdot)$, and where  $\mathcal{A}:\R^{d_1\times d_2}\rightarrow \R^m$ denotes a map 
\[
    \mathcal{A}(X) \defeq (\langle A_1, X\rangle, \dots, \langle A_m, X\rangle) \in \R^m.
\]
We note that one a special case of Eq.~\ref{eq:linear-measurement-overparam-objective} is the set of objectives that arise when training a two-layer linear feed-forward network using mean-squared-error loss.

We can bound the Lipschitz constant of the Hessian of this objectives in this family:
\begin{lemma}\label{lemma:overparam-linear-measurement-Lipschitz-Hessian}
    Consider the overparameterized matrix factorization objective with a linear measurement map, as defined in Eq.~\ref{eq:linear-measurement-overparam-objective}. Let $\theta \in \R^{(d_1+d_2)k}$ be parameters, whose entries can be grouped into two matrices: $\theta = (L,R)$ for $L\in \R^{d_1\times k}, R\in \R^{d_2\times k}$. Consider any measurement vector $b \in \R^{m}$. Then in the ball $\norm{\theta}_2 \leq B$, the objective in Eq.~\ref{eq:linear-measurement-overparam-objective} satisfies
     \[
        \opnorm{\nabla^2 f(\theta_1)-\nabla^2 f(\theta_2)} \leq 12 B \left(\sum_{i=1}^m \frobnorm{A_i}^2 \right)\norm{\theta_1 - \theta_2}_2.
     \]
\end{lemma}

 We first characterize the quadratic form of the Hessian of Eq.~\ref{eq:linear-measurement-overparam-objective}.
\begin{lemma}\label{lemma:linear-measurement-overparam-Hessian-bilinear}
    For the objective $f(\cdot)$ as defined in Eq.~\ref{eq:linear-measurement-overparam-objective}, for any $\theta, v\in \R^{(d_1\times d_2)k)}$ with entries denoted $\theta = (L,R)$ and $v=(U,V)$, the Hessian quadratic form can be expressed as
    \[
        D^2 f(L, R)[U,V] = 2\norm{\mathcal{A}(UR^\T + LV^\T)}^2_2 + 4 \langle \mathcal{A}(LR^\T)-b, \mathcal{A}(UV^\T)\rangle.
    \]
\end{lemma}
\begin{proof}
    We begin by deriving the gradient form using the limit definition:
    \begin{align*}
        \nabla f(L,R)[U,V] &\defeq \lim_{t\rightarrow 0}\frac{1}{t}\left(f(L+tU, R+tV)-f(L,R)\right)\\
        &=\lim_{t\rightarrow 0}\frac{1}{t}\left(\norm{\mathcal{A}((L+tU)(R+tV)^\T) - b}^2_2 - \norm{\mathcal{A}(LR^\T) - b}^2_2\right).
    \end{align*}
    By linearity of the map $\mathcal{A}(\cdot)$,
    \begin{align*}
        \mathcal{A}((L+tU)(R+tV)^\T) &= \mathcal{A}(LR^\T + tUR^\T + tLV^\T + t^2 UV^\T)\\
        &= \mathcal{A}(LR^\T) + t \mathcal{A}(UR^\T + LV^\T) + t^2 \mathcal{A}(UV^\T).
    \end{align*}
    Substituting this into the above limit yields
    \begin{align*}
        \nabla f(L,R)[U,V] &=\lim_{t\rightarrow 0}\frac{1}{t}\left(\norm{\Delta + t\mathcal{A}(UR^\T + LV^\T) + t^2 \mathcal{A}(UV^\T)}^2_2 -\norm{\Delta}^2_2\right)
    \end{align*}
    where $\Delta \defeq \mathcal{A}(LR^\T)-b$. Expanding the squared norm, this yields
    \begin{align*}
        \nabla f(L,R)[U,V] &=\lim_{t\rightarrow 0}\frac{1}{t}\Big(\norm{\Delta}^2_2 + \norm{t\mathcal{A}(UR^\T + LV^\T) + t^2 \mathcal{A}(UV^\T)}^2_2\\
        &\qquad\qquad\quad + 2 \langle \Delta,t\mathcal{A}(UR^\T + LV^\T) + t^2 \mathcal{A}(UV^\T)\rangle -\norm{\Delta}^2_2\Big)\\
        &=\lim_{t\rightarrow 0}\frac{1}{t}\Big(\norm{t\mathcal{A}(UR^\T + LV^\T) + t^2 \mathcal{A}(UV^\T)}^2_2\\
        &\qquad\qquad\quad +2 \langle \Delta,t\mathcal{A}(UR^\T + LV^\T) + t^2 \mathcal{A}(UV^\T)\rangle\Big)\\
        &=\lim_{t\rightarrow 0}\Big(t \norm{\mathcal{A}(UR^\T + LV^\T) + t \mathcal{A}(UV^\T)}^2_2+2 \langle \Delta,\mathcal{A}(UR^\T + LV^\T) + t \mathcal{A}(UV^\T)\rangle\Big)\\
        &=0+2 \langle \Delta,\mathcal{A}(UR^\T + LV^\T) + 0\rangle\\
        &=2\langle \mathcal{A}(LR^\T)-b), \mathcal{A}(UR^\T + LV^\T\rangle
    \end{align*}
    where the last line follows from $\Delta \defeq\mathcal{A}(LR^\T)-b$. Given this expression for the gradient, we can then define the Hessian quadratic form on input $v=(U,V)$ as
    \[
        \nabla^2 f(L,R)[U,V]\defeq \lim_{t\rightarrow 0}\frac{1}{t} \left(\nabla f(L+tU, R+tV)[U,V]-\nabla f(L, R)[U,V]\right).
    \]
    Given the above expression for the gradient, this yields
    \begin{align*}
        \nabla^2 f(L,R)[U,V] &= \lim_{t\rightarrow 0}\frac{1}{t}\Big(2\langle \mathcal{A}((L+tU)(R+tV)^\T) - b, \mathcal{A}(U (R+tV)^\T + (L+tU)V^\T) \rangle\\
        &\qquad \qquad \quad - 2\langle \Delta, \mathcal{A}(UR^\T + LV^\T\Big).
    \end{align*}
    As noted above, 
    \begin{align*}
        \mathcal{A}((L+tU)(R+tV)^\T) = \mathcal{A}(LR^\T) + t \mathcal{A}(UR^\T + LV^\T) + t^2 \mathcal{A}(UV^\T).
    \end{align*}
    Substituting this in to the first term in the expression for the Hessian, and using linearity of $\mathcal{A}(\cdot)$ to simplify terms, yields
    \begin{align*}
        \nabla^2 f(L,R)&[U,V] = \lim_{t\rightarrow 0}\frac{2}{t}\Big(\langle \Delta  +t \mathcal{A}(UR^\T + LV^\T) + t^2 \mathcal{A}(UV^\T), \mathcal{A}(UR^\T + LV^\T) + 2t\mathcal{A}(UV^\T) \rangle\\
        &\qquad \qquad \quad - \langle \Delta, \mathcal{A}(UR^\T + LV^\T\Big)\\
        &= \lim_{t\rightarrow 0}\frac{2}{t}\Big(\langle \Delta,\mathcal{A}(UR^\T + LV^\T)\rangle   + \langle t \mathcal{A}(UR^\T + LV^\T) + t^2 \mathcal{A}(UV^\T), \mathcal{A}(UR^\T + LV^\T)\rangle \\
        &\qquad \qquad \quad +\langle \Delta, 2t\mathcal{A}(UV^\T)\rangle + \langle t \mathcal{A}(UR^\T + LV^\T) + t^2 \mathcal{A}(UV^\T),2t\mathcal{A}(UV^\T)\rangle \\
        &\qquad \qquad \quad - \langle \Delta, \mathcal{A}(UR^\T + LV^\T\Big)\\
        &= \lim_{t\rightarrow 0}\frac{2}{t}\Big(\langle t \mathcal{A}(UR^\T + LV^\T) + t^2 \mathcal{A}(UV^\T), \mathcal{A}(UR^\T + LV^\T)\rangle \\
        &\qquad \qquad \quad +\langle \Delta, 2t\mathcal{A}(UV^\T)\rangle + \langle t \mathcal{A}(UR^\T + LV^\T) + t^2 \mathcal{A}(UV^\T),2t\mathcal{A}(UV^\T)\rangle\Big)\\
        &= \lim_{t\rightarrow 0}\frac{2}{t}\Big(t\langle  \mathcal{A}(UR^\T + LV^\T) + t \mathcal{A}(UV^\T), \mathcal{A}(UR^\T + LV^\T)\rangle \\
        &\qquad \qquad \quad +2t \langle \Delta, \mathcal{A}(UV^\T)\rangle + 2t^2\langle \mathcal{A}(UR^\T + LV^\T) + t^2 \mathcal{A}(UV^\T),\mathcal{A}(UV^\T)\rangle\Big).
    \end{align*}
    Taking the limit as $t\rightarrow 0$,
    \begin{align*}
        \nabla^2 f(L,R)[U,V] &= 2\Big(\langle \mathcal{A}(UR^\T + LV^\T), \mathcal{A}(UR^\T + L^\T)\rangle + 2\langle \Delta, \mathcal{A}(UV^\T)\rangle\Big)\\
        &= 2\Big(\norm{\mathcal{A}(UR^\T + LV^\T)}^2_2 + 2\langle \mathcal{A}(LR^\T)-b, \mathcal{A}(UV^\T)\rangle\Big)\\
        &= 2\norm{\mathcal{A}(UR^\T + LV^\T)}^2_2 + 4\langle \mathcal{A}(LR^\T)-b, \mathcal{A}(UV^\T)\rangle
    \end{align*}
    which yields the result.
\end{proof}

We can prove Lemma~\ref{lemma:overparam-linear-measurement-Lipschitz-Hessian}, which bounds the Lipschitz constant of the Hessian of $f(\cdot)$ as defined in Eq.~\ref{eq:linear-measurement-overparam-objective}.

\begin{proof}[Proof of Lemma~\ref{lemma:overparam-linear-measurement-Lipschitz-Hessian}]
    Given some vector $v\in \R^{(d_1 + d_2)k}$, group the entries of the vector $v$ into two matrices: let $v = (U, V)$ for matrices $U\in \R^{d_1\times k}, V\in \R^{d_2\times k}$. Then the quadratic form of the Hessian is
     \[
        \ip{v, \nabla^2 f(\theta) v} = D^2 f(L,R)[U,V].
     \]
     The objective $f(\cdot)$ satisfies Assumption~\ref{assume:Lipschitz-Hessian} with respect to Lipschitz constant $H$ if  $\forall \theta_1, \theta_2, v\in \R^{(d_1 + d_2)k}$,
     \[
        \abs{\ip{v, (\nabla^2 f(\theta_1)-\nabla^2 f(\theta_2)) v}} \leq H \norm{v}^2_2 \norm{\theta_1-\theta_2}_2.
     \]
     For any pair $\theta_1, \theta_2\in \R^{(d_1 + d_2)k}$, denote the entries by $\theta_1 = (L_1, R_1)$ and $\theta_2 = (L_2, R_2)$, and for any $v\in \R^{(d_1 + d_2)k}$, denote the entries by $v = (U, V)$ as above. Then the above inequality is equivalent to
     \[
        \abs{D^2f(L_1, R_1)[U,V] - D^2f(L_2, R_2)[U,V]} \leq H \left(\frobnorm{U}^2 + \frobnorm{V}^2 \right)\norm{(L_1, R_1)-(L_2, R_2)}_2.
     \]
     where
     \[
        \norm{(L,R)}_2 \defeq \sqrt{\frobnorm{L}^2 + \frobnorm{R}^2} = \norm{\theta}_2.
     \]
    By Lemma~\ref{lemma:linear-measurement-overparam-Hessian-bilinear}, for any $\theta = (L,R)$ and any $v=(U,V)$, the Hessian satisfies
    \begin{align*}
        D^2 f(L, R)[U,V] &= 2\norm{\mathcal{A}(UR^\T + LV^\T)}^2_2 + 4 \langle \mathcal{A}(LR^\T)-b, \mathcal{A}(UV^\T)\rangle.
    \end{align*}
    By linearity of $\mathcal{A}(\cdot)$, we can expand the squared norm:
    \begin{align*}
        D^2 f(L, R)&[U,V] = 2\norm{\mathcal{A}(UR^\T) + \mathcal{A}(LV^\T)}^2_2 + 4 \langle \mathcal{A}(LR^\T)-b, \mathcal{A}(UV^\T)\rangle\\
        &= 2\left(\norm{\mathcal{A}(LV^\T)}^2_2+\norm{\mathcal{A}(UR^\T)}^2_2\right) +4\langle \mathcal{A}(UR^\T), \mathcal{A}(LV^\T)\rangle+ 4 \langle \mathcal{A}(LR^\T)-b, \mathcal{A}(UV^\T)\rangle.
    \end{align*}
    Thus
    \begin{align}
        D^2f(L_1, R_1)[U,V] - D^2f(L_2, R_2)&[U,V] = 2\left(\norm{\mathcal{A}(L_1V^\T)}^2_2-\norm{\mathcal{A}(L_2V^\T)}^2_2\right)\label{eq:quadratic-diff-line-1}\\ 
        &\quad + 2\left(\norm{\mathcal{A}(UR_1^\T)}^2_2-\norm{\mathcal{A}(UR_2^\T)}^2_2\right)\label{eq:quadratic-diff-line-2}\\ 
        &\quad + 4\left(\langle \mathcal{A}(UR_1^\T), \mathcal{A}(L_1V^\T)\rangle-\langle \mathcal{A}(UR_2^\T), \mathcal{A}(L_2V^\T)\rangle\right)\label{eq:quadratic-diff-line-3}\\ 
        & \quad + 4\langle \mathcal{A}(L_1 R_1^\T)-\mathcal{A}(L_2 R_2^\T), \mathcal{A}(UV^\T)\rangle. \label{eq:quadratic-diff-line-4}
    \end{align}
    We bound the magnitude of each term in sequence. For the first term in Line~\ref{eq:quadratic-diff-line-1}, we observe
    \begin{align*}
        \left|\norm{\mathcal{A}(L_1V^\T)}^2_2-\norm{\mathcal{A}(L_2V^\T)}^2_2\right| &= \abs{\langle \mathcal{A}(L_1V^\T)+\mathcal{A}(L_2V^\T), \mathcal{A}(L_1V^\T)-\mathcal{A}(L_2V^\T) \rangle}\\
        &= \abs{\langle \mathcal{A}\left((L_1+L_2)V^\T\right), \mathcal{A}\left((L_1-L_2)V^\T\right) \rangle}\\
        &\leq \norm{\mathcal{A}\left((L_1+L_2)V^\T\right)}_2 \norm{\mathcal{A}\left((L_1-L_2)V^\T\right)}_2.
    \end{align*}
    The operator norm of $\mathcal{A}(\cdot)$ can be bounded as
    \[
        \norm{\mathcal{A}(X)}^2_2 = \sum_{i=1}^m \langle A_i, X\rangle^2 \leq \frobnorm{X}^2 \sum_{i=1}^m \frobnorm{A_i}^2.
    \]
    Thus
    \begin{equation}\label{eq:linear-measurement-map-2-norm}
        \norm{\mathcal{A}}_2 \leq \left(\sum_{i=1}^m \frobnorm{A_i}^2\right)^{1/2}.
    \end{equation}
    We can thus bound the term in Line~\ref{eq:quadratic-diff-line-1} as
    \begin{align*}
        \left|\norm{\mathcal{A}(L_1V^\T)}^2_2-\norm{\mathcal{A}(L_2V^\T)}^2_2\right| &\leq \norm{\mathcal{A}\left((L_1+L_2)V^\T\right)}_2 \norm{\mathcal{A}\left((L_1-L_2)V^\T\right)}_2\\
        &\leq \norm{\mathcal{A}}^2_2 \frobnorm{(L_1+L_2)V^\T} \frobnorm{(L_1-L_2)V^\T} \\
        &\leq \norm{\mathcal{A}}^2_2 \frobnorm{L_1+L_2} \frobnorm{V} \frobnorm{L_1-L_2} \frobnorm{V}.
    \end{align*}
    In the ball $\norm{\theta}_2 \leq B$, we have that for $\theta = (L, R)$
     \[
        \frobnorm{L} \leq \norm{\theta}_2 \leq B.
     \]
    Thus $\frobnorm{L_1+L_2} \leq 2B$, so we can bound
    \begin{align*}
        \left|\norm{\mathcal{A}(L_1V^\T)}^2_2-\norm{\mathcal{A}(L_2V^\T)}^2_2\right| &\leq 2B \norm{\mathcal{A}}^2_2 \norm{V}^2_F \frobnorm{L_1-L_2} \\
        &\leq 2B \norm{\mathcal{A}}^2_2 \frobnorm{V}^2 \norm{(L_1, R_1)-(L_2, R_2)}_2
    \end{align*}
    where the last line follows by
     \[
        \norm{(L_1, R_1)-(L_2, R_2)}_2 \defeq \sqrt{\frobnorm{L_1-L_2}^2 + \frobnorm{R_1-R_2}^2} \geq \frobnorm{L_1 - L_2}.
     \]
     An analogous argument bounds the term in Line~\ref{eq:quadratic-diff-line-2} as
     \[
        \left|\norm{\mathcal{A}(UR_1^\T)}^2_2-\norm{\mathcal{A}(UR_2^\T)}^2_2\right|\leq 2B \norm{\mathcal{A}}^2_2 \frobnorm{U}^2 \norm{(L_1, R_1)-(L_2, R_2)}_2.
     \]
     To bound the term in Line~\ref{eq:quadratic-diff-line-3}, we first observe that
     \begin{align*}
        \langle &\mathcal{A}(UR_1^\T), \mathcal{A}(L_1V^\T)\rangle-\langle \mathcal{A}(UR_2^\T), \mathcal{A}(L_2V^\T)\rangle\\
        &=\langle \mathcal{A}(UR_1^\T), \mathcal{A}(L_1V^\T)\rangle- \langle\mathcal{A}(L_1V^\T), \mathcal{A}(U R_2^\T)\rangle+\langle\mathcal{A}(L_1V^\T), \mathcal{A}(U R_2^\T)\rangle - \langle \mathcal{A}(UR_2^\T), \mathcal{A}(L_2V^\T)\rangle\\
        &=\langle \mathcal{A}(UR_1^\T) - \mathcal{A}(U R_2^\T), \mathcal{A}(L_1V^\T)\rangle+\langle\mathcal{A}(L_1V^\T)-\mathcal{A}(L_2V^\T), \mathcal{A}(U R_2^\T)\rangle\\
        &=\langle \mathcal{A}\left(U(R_1-R_2)^\T\right), \mathcal{A}(L_1V^\T)\rangle+\langle\mathcal{A}\left((L_1-L_2)V^\T\right), \mathcal{A}(U R_2^\T)\rangle
     \end{align*}
     Thus
     \begin{align*}
        & \abs{\ip{\mathcal{A}(UR_1^\T), \mathcal{A}(L_1V^\T)}-\ip{ \mathcal{A}(UR_2^\T), \mathcal{A}(L_2V^\T)}}\\
        &=\abs{\langle \mathcal{A}\left(U(R_1-R_2)^\T\right), \mathcal{A}(L_1V^\T)\rangle+\langle\mathcal{A}\left((L_1-L_2)V^\T\right), \mathcal{A}(U R_2^\T)\rangle}\\
        &\leq \norm{\mathcal{A}\left(U(R_1-R_2)^\T\right)}_2\norm{\mathcal{A}(L_1V^\T)}_2+\norm{\mathcal{A}\left((L_1-L_2)V^\T\right)}_2 \norm{\mathcal{A}(U R_2^\T)}_2\\
        &\leq \norm{\mathcal{A}}^2_2\frobnorm{U(R_1-R_2)^\T}\frobnorm{L_1V^\T}+\norm{\mathcal{A}}^2_2\frobnorm{(L_1-L_2)V^\T} \frobnorm{U R_2^\T}\\
        &\leq \norm{\mathcal{A}}^2_2\frobnorm{U}\frobnorm{R_1-R_2}\frobnorm{L_1}\frobnorm{V}+\norm{\mathcal{A}}^2_2\frobnorm{L_1-L_2}\frobnorm{V} \frobnorm{U} \frobnorm{R_2}\\
        &= \norm{\mathcal{A}}^2_2\frobnorm{U}\frobnorm{V} \left(\frobnorm{R_1-R_2}\frobnorm{L_1}+\frobnorm{L_1-L_2}\frobnorm{R_2}\right)
    \end{align*}
    In the ball $\norm{\theta}_2 \leq B$, we have that for $\frobnorm{L_1} \leq B$ and $\frobnorm{R_2}\leq B$. Thus
    \begin{align*}
        \abs{\langle \mathcal{A}(UR_1^\T), \mathcal{A}(L_1V^\T)\rangle&-\langle \mathcal{A}(UR_2^\T), \mathcal{A}(L_2V^\T)\rangle}\\
        &\leq B \norm{\mathcal{A}}^2_2\frobnorm{U}\frobnorm{V}\left(\frobnorm{R_1-R_2}+\frobnorm{L_1-L_2}\right).
    \end{align*}
    Recall that for any $a, b\in R$, it holds that $a+b \leq 2\sqrt{a^2 + b^2}$ and $2ab \leq a^2 + b^2$. Thus
     \[
        \frobnorm{R_1-R_2} + \frobnorm{L_1-L_2} \leq 2 \sqrt{\frobnorm{L_1 - L_2}^2 + \frobnorm{R_1 - R_2}^2} = 2\norm{(L_1,R_1) - (L_2, R_2)}_2
     \]
     and
     \[
        \frobnorm{U} \frobnorm{V} \leq \frac{1}{2}\left(\frobnorm{U}^2  + \frobnorm{V}^2\right).
     \]
     Combining these bounds yields
     \begin{align*}
        \abs{\langle \mathcal{A}(UR_1^\T), \mathcal{A}(L_1V^\T)\rangle&-\langle \mathcal{A}(UR_2^\T), \mathcal{A}(L_2V^\T)\rangle}\\
        &\leq B \norm{\mathcal{A}}^2_2\left(\frobnorm{U}^2 + \frobnorm{V}^2\right)\norm{(L_1,R_1) - (L_2, R_2)}_2.
    \end{align*}
    Lastly, we bound the term in Line~\ref{eq:quadratic-diff-line-4}. We begin by observing that
    \begin{align*}
        \mathcal{A}(L_1 R_1^\T)-\mathcal{A}(L_2 R_2^\T) &= \mathcal{A}(L_1 R_1^\T)-\mathcal{A}(L_1 R_2^\T)+\mathcal{A}(L_1 R_2^\T)-\mathcal{A}(L_2 R_2^\T)\\
        &=\mathcal{A}\left(L_1 (R_1-R_2)^\T\right)+\mathcal{A}\left((L_1-L_2)R_2^\T\right)
    \end{align*}
    Thus
    \begin{align*}
        \abs{\langle \mathcal{A}(L_1 R_1^\T)&-\mathcal{A}(L_2 R_2^\T), \mathcal{A}(UV^\T)\rangle} =\abs{\langle \mathcal{A}\left(L_1 (R_1-R_2)^\T\right)+\mathcal{A}\left((L_1-L_2)R_2^\T\right), \mathcal{A}(UV^\T)\rangle}\\
        &=\abs{\langle \mathcal{A}\left(L_1 (R_1-R_2)^\T\right),\mathcal{A}(UV^\T)\rangle +\langle \mathcal{A}\left((L_1-L_2)R_2^\T\right), \mathcal{A}(UV^\T)\rangle}\\
        &\leq \norm{\mathcal{A}\left(L_1 (R_1-R_2)^\T\right)}_2\norm{\mathcal{A}(UV^\T)}_2 +\norm{\mathcal{A}\left((L_1-L_2)R_2^\T\right)}_2\norm{ \mathcal{A}(UV^\T)}_2\\
        &\leq \norm{\mathcal{A}}^2_2\norm{L_1 (R_1-R_2)^\T}_F\norm{UV^\T}_F +\norm{\mathcal{A}}^2_2\norm{(L_1-L_2)R_2^\T}_F\norm{ UV^\T}_F\\
        &\leq \norm{\mathcal{A}}^2_2\left(\frobnorm{L_1} \frobnorm{R_1-R_2}\frobnorm{U}\frobnorm{V} +\frobnorm{L_1-L_2}\frobnorm{R_2}\frobnorm{U}\frobnorm{V}\right)\\
        &= \norm{\mathcal{A}}^2_2\frobnorm{U}\frobnorm{V}\left(\frobnorm{L_1} \frobnorm{R_1-R_2} +\frobnorm{L_1-L_2}\frobnorm{R_2}\right).
    \end{align*}
    In the ball $\norm{\theta}_2 \leq B$, we have that for $\frobnorm{L_1} \leq B$ and $\frobnorm{R_2}\leq B$. Thus
    \begin{align*}
        \abs{\langle \mathcal{A}(L_1 R_1^\T)-\mathcal{A}(L_2 R_2^\T), \mathcal{A}(UV^\T)\rangle} &\leq B\norm{\mathcal{A}}^2_2\frobnorm{U}\frobnorm{V}\left(\frobnorm{R_1-R_2} +\frobnorm{L_1-L_2}\right).
    \end{align*}
    Recalling the bounds $\frobnorm{V} \frobnorm{U}$ and $\left(\frobnorm{L_1 - L_2}+\frobnorm{R_1 - R_2}\right)$ established above, we have
    \[
        \abs{\langle \mathcal{A}(L_1 R_1^\T)-\mathcal{A}(L_2 R_2^\T), \mathcal{A}(UV^\T)\rangle} \leq B\norm{\mathcal{A}}^2_2\left(\frobnorm{U}^2+\frobnorm{V}^2\right)\norm{(L_1, R_1)-(L_2,R_2)}_2.
    \]
    Employing triangle inequality and each of the above bounds implies
    \begin{align*}
        \left|D^2f(L_1, R_1)[U,V] - D^2f(L_2, R_2)[U,V]\right|&\leq 2\cdot 2B \norm{\mathcal{A}}^2_2 \frobnorm{V}^2 \norm{(L_1, R_1)-(L_2, R_2)}_2\\
        &\quad + 2\cdot 2B \norm{\mathcal{A}}^2_2 \frobnorm{U}^2 \norm{(L_1, R_1)-(L_2, R_2)}_2\\
        &\quad + 4B \norm{\mathcal{A}}^2_2\left(\frobnorm{U}^2 + \frobnorm{V}^2\right)\norm{(L_1,R_1) - (L_2, R_2)}_2\\
        &\quad +4B\norm{\mathcal{A}}^2_2\left(\frobnorm{U}^2+\frobnorm{V}^2\right)\norm{(L_1, R_1)-(L_2,R_2)}_2\\
        &\leq 12 B \norm{\mathcal{A}}^2_2\left(\frobnorm{U}^2+\frobnorm{V}^2\right)\norm{(L_1, R_1)-(L_2,R_2)}_2
    \end{align*}
    which implies the result.
\end{proof}

\subsection{Proofs from Section~\ref{sec:EGOP-spectral-decay}}\label{ssec:spectral-decay-proofs}

In Section~\ref{sec:EGOP-spectral-decay}, we noted that for objectives of the form $f(\theta) = h(A\theta)$, for some loss function $h(\cdot)$ and data matrix $A \in \R^{d\times n}$, the EGOP of $f(\cdot)$ is
    \begin{equation}\label{eq:EGOP-chain-rule}
        \EGOP(f) = A^\T \mathbb{E}_{\theta\sim \rho}\left[\nabla_\theta h(A\theta) \nabla_\theta h(A\theta)^\T\right] A
    \end{equation}

For general loss functions $h(\cdot)$, one can establish the following upper bounds showing how the singular values of $A$ control the EGOP eigepsectrum of $f(\cdot)$.    
    \begin{lemma}\label{lem:data-decay-induces-EGOP-decay}
        Consider $f:\R^n\rightarrow \R$ satisfying $f(\theta) = h(A\theta)$ for some loss function $h:\R^n\rightarrow \R$ and nonsingular data matrix $A \in \R^{n\times n}$. Denote by  $\sigma_i(\cdot)$ and $\lambda_i(\cdot)$ the $i$th singular value and eigenvalue of a matrix respectively, indexed by decreasing value. Then all nonzero eigenvalues of the EGOP of $f(\cdot)$ satisfy
        \[
            \frac{\lambda_k(\EGOP(f))}{ \lambda_1(\EGOP(f))} \leq \left(\frac{\sigma_k(A)}{\sigma_1(A)}\right)^2\frac{\lambda_1(M)}{\lambda_n(M)}.
        \]
        where
        \[
            M\defeq \mathbb{E}_{\theta\sim\rho}[\nabla_\theta h(A\theta) \nabla_\theta h(A\theta)^\T].
        \]
    \end{lemma}
    If the matrix $M$ has some finite condition number that does not go to infinity as the spectral decay in $A$ increases, then Lemma~\ref{lem:data-decay-induces-EGOP-decay} shows that increasing spectral decay in the data matrix $A$ induces spectral decay in the EGOP eigenvalues of $f(\cdot)$.

    For specific choices of $h(\cdot)$, such as $h(\cdot)\defeq \shortnorm{\cdot}^2_2$, one can more precisely characterize how spectral decay in the matrix $A$ induces decay in the EGOP of $f\defeq h\circ A$.
    \begin{lemma}\label{lem:spectral-decay-least-squares}
        Consider $A \in \R^{d\times n}$ and let $f(\theta) = \frac{1}{2}\norm{A\theta- y}^2_2$ where $y = A\theta^* + \eta$, for $\eta$ some mean-zero measurement noise. Assume sampling density $\rho$ is a standard Gaussian distribution. Then the eigenvalues of $\EGOP(f)$,  $\{\lambda_k\}_{k=1}^d$ indexed in decreasing order, satisfy
        \begin{equation}
            \frac{\lambda_k}{\lambda_1} \leq \left(\frac{\sigma_{k-1}(A)}{\sigma_1(A)}\right)^4 \ \forall k\in [2,\dots,n]
        \end{equation}
        where $\sigma_i(A)$ denotes the $i$\ts{th} singular value of $A$, indexed in decreasing order.
    \end{lemma}
    \begin{proof}
        Observe that for any $\theta\in \R^d$,
        \[
            \nabla f(\theta) = A^{\T} (A\theta-y) = A^{\T}(A\theta - A\theta^* - A\eta) = A^{\T} A(\theta-\theta^*) + A\eta.
        \]
        Thus for $\rho$ a standard Gaussian,
        \begin{align*}
            \EGOP(f) &= \mathbb{E}_{\theta\sim \rho}\left[\left(A^{\T} (A\theta-y)\right) \left(A^{\T} (A\theta-y)\right)^{\T}\right]\\
            &=A^{\T} \mathbb{E}_{\theta\sim \rho}\left[A \theta \theta^{\T} A + yy^{\T}\right]A\\
            &=\left(A^{\T} A\right)^2 + (A^{\T}y)(A^{\T} y)^{\T}
        \end{align*}
        using the fact that $\rho$ is mean-zero and isotropic. Thus $\EGOP(f)$ is a rank-1 perturbation of the matrix $(A^{\T} A)^2$, which is itself PSD and has eigenvalues $\sigma_k(A)^4$. Moreover if we let $A = Q_1 \Sigma Q_2^\T$ denote the SVD of $A$, we can rewrite
        \[
            \EGOP(f) = Q_2\left(\Sigma^4 + \Sigma Q_1^{\T} y y^{\T} Q_1 \Sigma \right)V^{\T}
        \]
        where $\Sigma^4$ is diagonal and has entries $\{\sigma_i^4(A)\}_{i=1}^n$. By \citet{golub1973some}, the eigenvalues of $\EGOP(f)$ thus satisfy
        \[
            \sigma_i^4(A) \geq \lambda_{i}(\EGOP(f)) \geq \sigma^4_{i+1}(A) \ \forall i\in[2,\dots,n] 
        \]
        and
        \[
            \sigma_1^4(A) + \norm{\Sigma Q_1^{\T} y}^2_2 \geq \lambda_1(\EGOP)\geq \sigma^4_2(A)
        \]
        which implies the result.
    \end{proof}



