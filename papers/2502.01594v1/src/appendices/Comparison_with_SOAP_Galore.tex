\section{Comparison with Other Reparameterization Methods}\label{sec:compare-w-SOAP-Galore}

    In this section, we compare the reparameterization method proposed in Algorithm~\ref{alg:meta-algorithm-block} with SOAP and GaLore, two related methods that involve optimizing with Adam under change-of-basis \cite{vyas2024soap, zhao2024galore, maes2024understanding}. To facilitate this discussion, we begin by instantiating an example of EGOP reparameterization for an objective whose parameters can be viewed as matrices, as this highlights some of the conceptual differences between the methods.

    In many key applications of adaptive algorithms, including training neural networks, the parameters over which optimization occurs are matrix-valued. As a simple illustration, consider optimizing a single-layer linear fully-connected network. Let $\nin$ denote the number of input features to the layer, and $\nout$ denote the number of output features. This network then has $\nin\cdot\nout$ parameters, which can be expressed either as a vector, $\theta\in \R^{\nin\nout}$, or as a matrix, denoted $\mat(\theta)\in\R^{\nin\times \nout}$.
    
    Denote the training data and labels by $A \in \R^{\nin\times\nsamples}$ and $Y \in \R^{\nout \times \nsamples}$, and consider minimizing loss function
    \[
        f(\theta) = \frac{1}{2}\frobnorm{\mat(\theta)^\T A - Y}^2.
    \]
    Similarly, the vector-valued gradients $\nabla f(\theta) \in \R^{\nin \nout}$ can also be viewed as matrices, $\mat(\nabla f(\theta)) \in \R^{\nin\times \nout}$.
    
    In the method proposed in this work, we consider gradients to be vector-valued when forming the EGOP. Thus for this single-layer objective, $\EGOP(f) \in \R^{\nin\nout \times \nin\nout}$. We emphasize this vector-view of gradients for clarity, because the EGOP matrix has distinct eigenvectors from the expectation of the matrix product, $\mat(\nabla f(\theta))\mat(\nabla f(\theta))^{\T}$. The related methods discussed in the introduction, including SOAP and GaLore, consider transformations by the matrices
    \begin{align*}
        Q_L &= \operatorname{eigenvectors}(\mathbb{E}_{\theta\sim\rho} [\mat\left(\nabla f(\theta)\right)\mat\left(\nabla f(\theta)\right)^{\T}])\\
        Q_R &= \operatorname{eigenvectors}(\mathbb{E}_{\theta\sim\rho} [\mat\left(\nabla f(\theta)\right)^{\T}\mat\left(\nabla f(\theta)\right)])
    \end{align*}
    and closely related transformations \cite{vyas2024soap, zhao2024galore, maes2024understanding}. In general the eigenvectors in $Q_L, Q_R$ correspond to different transformations than the eigenvectors of the EGOP. In particular, letting $V$ denote the eigenbasis of the EGOP formed from vector-valued gradients, $V\neq Q_L\otimes Q_R$. Moreover, the class of orthonormal matrices obtainable from the EGOP eigenbasis is strictly more general than the class $Q_L \otimes Q_R$ for pairs of orthogonal matrices $Q_L, Q_R$, as formalized below:
    \begin{lemma}
        For any orthogonal matrices $Q_L\in\R^{\nin\times \nin},Q_R\in\R^{\nout\times\nout}$ and any objective function $f:\R^{\nin\nout}\rightarrow \R$, there exists orthogonal $V\in \R^{\nin\nout\times\nin\nout}$ such that
        \[
            \forall \theta \in \R^{\nin\nout}, \quad Q_L^\T \mat(\nabla f(\theta)) Q_R = \mat(V\nabla f(\theta)).
        \]
        However, there exist values of $\nin, \nout$, objective functions $f:\R^{\nin\nout}\rightarrow \R$, and orthogonal matrices $V\in \R^{\nin\nout\times \nin\nout}$ such that no orthogonal matrices $Q_L\in\R^{\nin\times \nin},Q_R\in\R^{\nout\times\nout}$satisfy
        \[
            \forall \theta \in \R^{\nin\nout}, \quad Q_L^\T \mat(\nabla f(\theta)) Q_R = \mat(V\nabla f(\theta)).
        \]
    \end{lemma}
    \begin{proof}
        First we show that for any $Q_L, Q_R$, there exists suitable $V$ satisfying the property. We begin by noting that for any matrices $A, B, C$ of compatible dimension,
        \[
            \matvec(ABC) = (C^{\T} \otimes A) \matvec(B).
        \]
        Thus for any $Q_L, Q_R$,
        \[
            \matvec(Q_L^{\T} \mat(\nabla f(\theta)) Q_R) = (Q_R^\T \otimes Q_L^\T)\nabla f(\theta)
        \]
        where the second equality uses the fact that trivially $\matvec(\mat(\nabla f(\theta))) = \nabla f(\theta)$. The Kronecker product of orthogonal matrices is orthogonal, so choice of $V = (Q_R^\T \otimes Q_L^\T)$ satisfies the desired property.

        We now show there exist orthogonal matrices $V\in \R^{\nin\nout\times \nin\nout}$ such that no orthogonal matrices $Q_L\in\R^{\nin\times \nin},Q_R\in\R^{\nout\times\nout}$ satisfy
        \[
           Q_R \otimes Q_L^{\T} = V.
        \]
        This is a consequence of the fact that not all orthogonal matrices admit Kronecker product factorizations. Here we give one specific construction.
        
        Let $\vec{1}_d$ denote the all-ones vector in $\R^d$. Consider $V$ with leading column $v_1 = \vec{1}_{\nin\nout}/\sqrt{\nin \nout}$, and second column $v_2 = (\nin\nout)^{-1/2}\cdot\operatorname{concatenate}(\vec{1}_{\nin\nout/2}, -\vec{1}_{\nin\nout/2})$. Given the entries of $v_1$ are identical, this implies all the entries in the first column of $Q_R$ are identical, and thus that the first $\nin$ columns of $V$ comprise concatenated copies of $Q_L^T$. However this contradicts the fact that the entries of $v_2$ are identical in magnitude and nonzero but have positive sign for the first $\nin\nout/2$ entries and negative sign for the rest. Thus no orthogonal matrices $V$ with such first and second columns can be decomposed into $Q_R \otimes Q_L^T$, so in particular there exists some vector $z\in \R^{\nin\nout}$ such that
        \[
            Q_R \otimes Q_L^{\T} z \neq V z.
        \]
        Thus for any $f(\cdot)$ such that there exists $\theta$ with $\nabla f(\theta)=z$, it holds that for this value
        \[
            Q_L^{\T} \mat(\nabla f(\theta)) Q_R = \mat\left((Q_R \otimes Q_L^{\T})\nabla f(\theta)\right)\neq  \mat(V\nabla f(\theta)).
        \]
    \end{proof}

    In both SOAP and GaLore the orthonormal bases are periodically re-computed, and performance of these methods degrades as time between re-computations grows \cite{vyas2024soap, zhao2024galore}. The fewest re-computations performed in the experiments of the original SOAP paper was once every 100 batches \cite{vyas2024soap}. We show that with our proposed choice of basis, a single up-front computation of the change-of-basis suffices to improve the performance of both Adam and Adagrad in a variety of settings, as shown in Section~\ref{sec:experimental-results}. A valuable direction for future research would be to characterize the cost-benefit trade-off of periodic re-estimation of the change-of-basis matrix.