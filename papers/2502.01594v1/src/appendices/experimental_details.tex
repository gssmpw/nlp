\section{Experimental details}\label{sec:experimental-details}

Experiments were implemented in python, and both code and data will be made publicly available upon publication. Experiments with neural networks were implemented in Pytorch, and experiments with convex objectives were implemented using auto-differentiation in Jax and optimizers from Optax \cite{paszke2017automatic, jax2018github, deepmind2020jax}.

\subsection{Dataset Details}\label{ssec:dataset-details}

\paragraph{UCI handwritten digits dataset} The UCI digits dataset \cite{optical_recognition_of_handwritten_digits_80} contains 5620 instances, each an $8\times 8$ pixel greyscale image of a handwritten digit of values $0,\dots, 9$. Each instance has a integer label, $0,\dots, 9$. We split the dataset into a training dataset with 3823 instances, a validation dataset with 598 instances, and a test dataset with 1199 instances.

\paragraph{fashionMNIST dataset} The fashionMNIST dataset consists of $28\times 28$ pixel grayscale images of clothing, each having a label from one of 10 classes. For our experiments, we restrict to only instances corresponding to the first ten classes (labeled ``t-shirt,'' ``trouser,'' ``pullover,'' and ``dress''). This yields a dataset of 28,000 instances, which we subdivide into a training set of 24,000 instances, a validation set of 1,000 instances, and a test set of 3,00 instances.

\subsection{Algorithm Details}\label{ssec:algorithm-details}

We consider four optimization algorithms: Adagrad, Adam, SGD, and SGD with momentum. We use standard settings for all hyperparameters except learning rate: for Adam, we use $\beta_1 = 0.9$ and $\beta_2 = 0.999$, and for SGD with momentum we use momentum parameter $0.9$, matching that of Adam. We use zero weight decay for all algorithms, and for all objectives except the linear feed-forward networks, we use a constant learning rate schedule.

We tune learning rates for each algorithm. Unless otherwise noted, when choosing the range of learning rates to sweep for tuning, we the following two-step procedure. First we sweep a coarse sweep using powers of 10 (e.g. 1e-3, 1e-2, $\dots$ , 100) to find an upper and lower bound on the learning rates that produce best performance. The metric by which we quantify the ``best performance'' for each different experiment is discussed below in \cref{ssec:details-for-main-body-experimental-results}. We then perform a refined sweep, where we discretize each interval between subsequent power of 10 using doubling. For example, if the coarse sweep identified lower bound 0.01 and upper bound 1.0, we would perform the refined sweep over values [0.01, 0.02, 0.04, 0.08, 0.1, 0.2, 0.4, 0.8, 1.0].

\subsection{Details for Figures in Section~\ref{sec:intro}}\label{ssec:details-for-opener-cartoon}

For \cref{fig:opener-cartoon} (left), we generate the loss landscape pictured by taking $d=2$ dimensions and $n=100$ samples. We generate a matrix $A$ with singular values $\sigma_k = k^{-\alpha}$ with $\alpha = 1.5$ and random right- and left-singular vectors. We sampled $\theta^* \sim \mathcal{N}(0, \mathbb{I})$ and used $A, \theta^*$ to define the objective $f(\cdot)$ according to \cref{eq:log-sum-exp-objective}. We generated problem instances at random, and chose a random seed that produced a problem where the primary directions of variation for $f(\cdot)$ were clearly un-aligned with the coordinate axes. We allow Adagrad to take 1000 iterations.

We selected an initial point $\theta_0$ that would produce a clear visual distinction between the two coordinate systems in early iterates. To select the learning rate, we first swept over the values $1, 10, 20,\dots, 100$, and examined the suboptimality of the solution produced after 1000 iterates. For each learning rate we conducted 5 random trials on log-sum-exp objectives generated with identical values of $d, n, \alpha$, and with $\theta^*\sim \mathcal{N}(0,\mathbb{I})$; these trials were initialized at points drawn from $\mathbf{N}(0, \norm{\theta_0}^2_2\mathbb{I})$, where $\theta_0$ is the initial point used in \cref{fig:opener-cartoon} (left). We found that for learning rates 30 through 90, the suboptimality of the solution returned by the algorithm in original coordinates was very comparable across learning rates and very close to zero ($<1e-10$). Thus we selected a learning rate with these properties that was on the lower end (hence 30) because at larger learning rates, the oscillations around the global minimum made it more difficult to visually assess the difference between trajectories before and after reparameterization.


\subsection{Details for Figures in Section~\ref{sec:EGOP-spectral-decay}}\label{ssec:details-for-spectral-decay} 

\cref{fig:tinyMNIST-global-spectral-decay} plots the eigenspectrum of an empirically estimated EGOP matrix. We consider a 2-layer ReLU network. Its first layer is a  fully-connected linear layer with bias, with 64 input nodes and 32 output nodes, followed by a ReLU nonlinearity. Its second layer is a fully-connected linear layer with bias, with 32 input nodes and 10 output nodes, followed by a log-softmax. We form the function $f(\cdot)$ by taking the negative log-likelihood loss on the training dataset. We use the UCI digits dataset and consider the training subset described in \cref{ssec:dataset-details}.

For \cref{fig:tinyMNIST-global-spectral-decay} we use full-batch gradients to estimate the EGOP matrix. We take $\rho$ to be a standard normal distribution: for each gradient sample, we form $\theta$ by sampling the entries of all weights and biases i.i.d. from a standard Gaussian. We estimate $\EGOP(f)$ using $M = 10d$, where $d$ is the total number of parameters (sum of all weight and bias entries) for the network. This is a larger number of EGOP samples than we use in later experiments; this is done intentionally with the goal of clearly resolving the spectral decay, rather than having decay appear as an artifact of numerical estimation with few samples.

For \cref{fig:tinyMNIST-compare-spectral-decay}, we use the same dataset, architecture, objective $f(\cdot)$, and procedure for EGOP estimation, but we use different sampling distributions $\rho$. The ``realistic initialization'' distribution is described in full detail below in \cref{ssec:details-for-main-body-experimental-results}.

For \cref{fig:layer-by-layer-spectra}, we use the same architecture and objective for the UCI digits dataset. For fashionMNIST, we restrict to 4 classes and use the train-validation-test split, as described in \ref{ssec:dataset-details}. Additionally, we down-sample the input to our network from $28\times 28$ pixels to $14\times 14$ pixels using a 2D max-pooling operation as the first layer of our network. The next layer is then a fully-connected linear layer with bias, with 196 input nodes and 20 output nodes, followed by a ReLU activation function. The last layer is a fully-connected linear layer with bias, with 20 input nodes and 4 output nodes. We form the function $f(\cdot)$ by taking the cross entropy loss on the training dataset. We note that this is equivalent to computing the log-softmax on the network output and then using negative log-likelihood loss, which is how we compute $f(\cdot)$ on tinyMNIST.

For \cref{fig:layer-by-layer-spectra}, we use minibatches of size 300 to estimate gradients. For each architecture, we use $M = 5d$ gradient samples to estimate each block EGOP matrix, where $d$ is the number of parameters in the network. This is a larger number of EGOP samples than we use in later experiments; this is done intentionally with the goal of clearly resolving the spectral decay, rather than having decay appear as an artifact of numerical estimation with few samples. We perform block EGOP reparameterization for both networks, where the blocks are defined by the weight matrices of each network. For more details on block reparameterization for neural network weights, see the example in \cref{sec:expanded-heuristics-discussion}.

When drawing gradients to estimate the block EGOP matrices, the distribution over parameters $\rho$ from which we draw is the same as a distribution we later use to initialize the networks during training. For a full description of this initialization distribution, see \cref{ssec:details-for-main-body-experimental-results}.

\subsection{Details for Figures in Section~\ref{sec:experimental-results}}\label{ssec:details-for-main-body-experimental-results}

\paragraph{Linear Feedforward Networks} As described in Section~\ref{sec:experimental-results}, for \cref{fig:global-reparam-linear-layers} we consider parameters $\theta = [\textrm{vec}(W_1), \textrm{vec}(W_2), \textrm{vec}(W_3)]$ where $W_1 \in \R^{50\times 10}$, $W_2 \in \R^{30\times 50}$, $W_3 \in \R^{10\times 30}$. We seek to minimize loss
\[
    f(\theta) = \frobnorm{W_3 W_2 W_1 A - Y}^2 /\nsamples
\]
where $A\in \R^{10\times \nsamples}$, and $Y = M^* A$ for $M^*\in \R^{10\times 10}$ drawn from a standard Gaussian distribution. We induce spectral decay in $\EGOP(f)$ by generating $A$ with singular values $\sigma_k(A) = k^{-2}$ and random right- and left-singular vectors. For each trial, we generate $20,000$ data samples, which we split into a test set of $10,000$ training data instances, $4,000$ validation instances, and $6,000$ test instances. We define the training loss to be $f(\cdot)$ restricted to the training instances and their labels, and use this function when estimating the EGOP matrix. We use stochastic mini-batches of size 500 when estimating the EGOP and when performing optimization.

For each trial, we generate $A, M^*$, and $Y$ as described above. We then estimate $\EGOP(f)$ using $M = 2d$ samples, where $d$ is the total number of parameters in the network ($d=2,300$). When estimating the EGOP, we let $\rho$ be the same distribution used when initializing networks for training. Specifically, we initialize each weight matrix from a mean-zero Xavier normal distribution, also called Glorot initialization, which is widespread in practice \cite{glorot2010understanding}. The Xavier normal distribution is a Gaussian with standard deviation $\sqrt{2/\FIFO}$, where for a matrix $W \in \R^{n\times m}$, $\FIFO = n+m$. We compute the full eigenvalue decomposition to find the change-of-basis matrix $V$.

We use 1000 epochs during training. See \cref{ssec:algorithm-details} for choice of  hyperparameters and choice of the range of learning rates to sweep for tuning. We measure performance using the median minimum validation loss achieved during training. We perform 10 independent trials at each learning rate. For each algorithm, we choose the learning rate at which the algorithm in original achieved lowest median minimum validation loss. Results of this learning rate sweep are in \cref{fig:linear-layers-global-reparam-valloss-vs-LR}.

Because Adam in original coordinates exhibited numerical instability due to the near-zero gradient values encountered, we used cosine annealing to decay the learning rate over the coarse of training. We use Pytorch's default implementation of cosine annealing, and visualize an example of the learning rate schedule corresponding to learning rate 1.0 in \cref{fig:cosine-annealing}. For ease of comparison, we apply the same learning rate decay schedule to all algorithms (Adagrad, Adam, SGD, and SGD with momentum) in both original and reparameterized coordinates. We use this learning rate decay schedule throughout learning rate tuning, and in all results displayed in \cref{fig:global-reparam-linear-layers}.

%%%%%%%%%%%%%%%%%%%%%%%
%% This figure is for EXPERIMENTAL DETAILS/ LINEAR NETWORK
\begin{figure}[]
    \centering    \includegraphics[width=0.4\linewidth]{Images/final_figures/cosine_annealing_schedule.pdf}
    \caption{An example of the cosine annealing learning rate schedule used for the linear feedforward network experiments. This schedule corresponds to learning rate 1.0.}
    \label{fig:cosine-annealing}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%


\paragraph{ReLU Networks for Image Classification} We use the UCI digits dataset and fashionMNIST, with the train-validation-split described in \ref{ssec:dataset-details}. For the UCI digits dataset, we use a 2-layer ReLU network, with the same architecture and objective $f(\cdot)$ detailed in \ref{ssec:details-for-spectral-decay}. We use the same (random) partition of datapoints into train, validation, and test datasets for all trials.

For fashionMNIST, we restrict to 4 classes and use the train-validation-test split, as described in \cref{ssec:dataset-details}. We use the architecture and objective $f(\cdot)$ described in \cref{ssec:details-for-spectral-decay}.

For both networks and objectives, we use minibatches of size 300 to estimate gradients throughout.

We perform block EGOP reparameterization for both networks, where the blocks are defined by the weight matrices of each network. For more details on block reparameterization for neural network weights, see the example in \cref{sec:expanded-heuristics-discussion}. For each network, we use $M=d$ minibatch gradient samples to estimate the EGOP, where $d$ is the number of parameters in the network.

For each objective and network, we use the same distribution over parameters for drawing gradient samples to estimate the EGOP and for initializing the network at training time. For the experiments in \cref{fig:fashionMNIST}, that distribution is Pytorch's default method for re-setting parameters in a linear layer, namely drawing weight entries i.i.d. from a uniform distribution with range $[-(\textrm{in-features})^{-1/2}, (\textrm{in-features})^{-1/2}]$ and drawing bias entries i.i.d. from a uniform distribution with range $[-(\FIFO)^{-1/2},(\FIFO)^{-1/2}]$, where $\FIFO$ for a layer is defined above in \cref{ssec:details-for-spectral-decay}. For the experiments in \cref{fig:opener-cartoon} (right), we draw weight entries i.i.d. from a mean-zero Xavier distribution, and bias entries i.i.d. from a uniform distribution with range $[-(\FIFO)^{-1/2},(\FIFO)^{-1/2}].$ All of these initialization distributions stem from heuristics that are widespread in practice, and the distinction between the weight distributions used with the digits dataset versus the fashionMNIST dataset was the result of using legacy code; we did not tune initializations to different applications.

See \cref{ssec:algorithm-details} for choice of  hyperparameters and choice of the range of learning rates to sweep for tuning. We used a constant learning rate for both datasets. We measure performance using the median maximum validation classification accuracy achieved during training. For each algorithm, we choose the learning rate at which the algorithm in original achieved highest median maximum validation accuracy. For the digits dataset, we performed 50 independent trials at each learning rate. For fashionMNIST, we performed 10 independent trials. We selected the number of epochs over which to train by examining loss and accuracy curves by eye, and choosing a value large enough that all algorithms had converged. For the digits dataset, we trained for 200 epochs, and for fashionMNIST we trained for 40 epochs.

\paragraph{Convex objectives} We study three objectives. For each, we generate a matrix $A \in \R^{\nsamples\times d}$ with singular value decay $\sigma_k = k^{-\alpha}$ and random orthonormal right- and left-singular vectors. The values of $\alpha$ are specified in the caption of \cref{fig:cvx-objectives}. The log-sum-exp objective is defined in Eq.~\ref{fig:log-sum-exp}. The logistic regression objective is defined as
\[
    f(\theta) = \sum_{i=1}^{\nsamples} -y_i \log\left(\frac{1}{1+e^{-\langle a_i, \theta\rangle}}\right) -(1-y_i)\log\left(\frac{e^{-\langle a_i, \theta\rangle}}{1+e^{-\langle a_i, \theta\rangle}}\right)
\]
where $\{a_i\in \R^d\}_{i=1}^{\nsamples}$ are the columns of $A$, and labels $y_i\sim \operatorname{Bernoulli}(\pi(\theta^*)_i)$ where $\theta^* \in \R^d$ is drawn from $\mathcal{N}(0, \mathbb{I})$ and 
\[
    \pi(\theta^*)_i \defeq \frac{e^{-\langle a_i, \theta^*\rangle}}{1+e^{-\langle a_i, \theta^*\rangle}}.
\]
The logistic regression objective is 
\[
    f(\theta) = \frac{1}{2}\norm{A\theta-y}^2_2
\]
where $y=A\theta^*$ and $\theta^*\in \R^d$ is drawn from $\mathcal{N}(0, \mathbb{I})$.

For log-sum-exp and linear least squares, $\nsamples=d=100$. For logistic regression, $\nsamples=100$ and $d=3$.

For each objective, on each trial we generate $A, \theta^*$, and thus $f(\cdot)$ randomly and independently following the above procedure.

For all convex objectives, use deterministic (full-batch) gradients to estimate the EGOP and to optimize.

For all objectives, we use $M = 5d$ gradient samples to estimate the EGOP matrix. We take $\rho$ to be a standard Gaussian distribution, and use the same distribution to initialize when optimizing. See \cref{ssec:algorithm-details} for choice of  hyperparameters and choice of the range of learning rates to sweep for tuning. We used a constant learning rate for all convex objectives. We measure performance using the median final training loss achieved. For each algorithm, we choose the learning rate at which the algorithm in original achieved lowest median training loss. We perform 5 independent trials at each learning rate.

For each objective, we optimize for 1000 iterations, or until the gradient Euclidean norm drops below $1e-8$. The latter termination condition is comparable to default termination conditions employed by \texttt{scipy.optimize} \cite{2020SciPy-NMeth}.


% \newpage
% \AD{PENDING}

% Figure~\ref{fig:tinyMNIST-global-spectral-decay}
% \begin{itemize}
%     \item Distribution: standard Gaussian for both weights and biases
%     \item Number of samples: 10x times total number of parameters
%     \item Full-batch gradients
%     \item Architecture: 2-layer ReLU Network
% \end{itemize}

% Letting $d$ denote the total number of parameters in the network, the empirical EGOP used to generate Fig.~\ref{fig:tinyMNIST-global-spectral-decay} was computed using $10d$ i.i.d. gradient samples. Recalling that the number of samples required to estimate the $k$th eigenvalue to within $\varepsilon$ with high probability scales as $\log(d)\lambda_1/(\lambda_k \varepsilon^2)$, the number of samples used to generate this figure is sufficiently high to have reasonable confidence in the estimates, particularly of the leading eigenvalues\footnote{\AD{Despite the math, this is a vague statement, since the guarantees in \citet{constantine2015active} are derived for differentiable functions, and ReLU networks aren't differentiable.}} \cite{constantine2015active}.

% Figure~\ref{fig:layer-by-layer-spectra}
% \begin{itemize}
%     \item Distribution: realistic init. distributions
%     \item Number of samples: \AD{Check}
%     \item Stochastic gradients \AD{Check batch size}
%     \item Architectures: 2-layer ReLu and 3-layer ReLU repsectively
% \end{itemize}

% Figure~\ref{fig:natural-data-layer-by-layer}, fashionMNIST
% \begin{itemize}
%     \item EGOP oversampling factor = 1
%     \item Current version doesn't use LR decay
%     \item Minibatch size 300
% \end{itemize}

% Figure~\ref{fig:tinyMNIST-compare-spectral-decay}
% \begin{itemize}
%     \item 10x number total parameters.
% \end{itemize}
