\subsubsection{Proof of Theorem~\ref{thm:compare-convergence-guarantees}}\label{sec:proof-of-main-result}

For convenience, we recall the definition of coordinate-wise smoothness (\cref{eq:def-coordinate-wise-smoothness}). A function $f$ satisfies is coordinate-wise smooth with respect to constants $L_1,\dots,L_d > 0$ if $\forall \theta_1, \theta_2 \in \R^d$, 
\[
    \abs{f(\theta_1)-f(\theta_2) - \langle \nabla f(\theta_2), \theta_1 - \theta_2)} \leq \frac{1}{2}\sum_{i=1}^d L_i \left(\theta_1(i)-\theta_2(i)\right)^2.
\]
We note that this is a stronger definition than the notion of coordinate-wise smoothness employed elsewhere in literature, including e.g. the analysis of randomized coordinate descent. There, the smoothness constant of the $i$th coordinate is defined as $L'_i$ satisfying
\[
    \abs{f(\theta + h e_i) - f(\theta) + h \nabla f(\theta)(i)} \leq \frac{1}{2} h^2 L'_i.
\]
Namely, this notion only considers perturbations to $\theta$ along the $i$th coordinate, where as the definition in Eq.~\ref{eq:def-coordinate-wise-smoothness} allows simultaneous perturbations along many coordinates.

\citet{jiang2024convergence} and \citet{liu2024adagrad} establish upper bounds on the convergence rate of Adagrad in terms of the sum
\[
    L_f\defeq \sum_{i=1}^d L_i
\]
For example, in the case of noiseless gradients they show that the iterates $\{\theta_t\}_{t=1}^T$ produced by Adagrad satisfy
\begin{equation}\label{eq:jiang-fullbatch-gradient-guarantee}
    \frac{1}{T} \sum_{t=1}^T \norm{\nabla f(\theta_t)}_1  = O\left(\frac{f(\theta^*)-f(\theta_0)}{\eta \sqrt{T}} + \frac{\eta L_f}{\sqrt{T}}\cdot \log(p(T, L_f, \nabla f(\theta_0)))\right).
\end{equation}
where $p(T, L_f, \nabla f(\theta_0))$ is polynomial in $T$, $L_f$, and $\norm{\nabla f(\theta_0)}_\infty$. In order to prove Theorem~\ref{thm:compare-convergence-guarantees}, we establish that for a class of non-convex functions, the sum of the coordinate-wise smoothness constants reduces significantly under reparameterization by the EGOP eigenbasis. For a function $f:\R^d \rightarrow \R$, denote by $V$ the eigenbasis of $\EGOP(f)$ and by $\lambda_1,\dots, \lambda_d$ the eigenvalues of $\EGOP(f)$, indexed in decreasing order.

We begin with a formal statement of the result in Thm.~\ref{thm:compare-convergence-guarantees}.
\begin{theorem}\label{thm:formal-compare-convergence-guarantees}
    Consider a function $f:\R^d\rightarrow \R$ and a sampling distribution $\rho$ satisfying Assumptions~\ref{assumption:sampling-distribution} and ~\ref{assume:Lipschitz-Hessian}. Let $\Delta_f \defeq \inf_{\theta\in \R^d} f(\theta) -f(\theta_0)$ for some initialization point $\theta_0$, and assume $\Delta_f$ is finite. Let $\{\ttheta_t\}_{t=1}^T$ denote the iterates produced by optimizing the reparameterizing objective $\tf \defeq f\circ V$ using Adagrad initialized at $V^T \theta_0$, where $V$ denotes the eigenbasis of $\EGOP(f)$. Let $\theta^*$ denote the local minimum considered in Assumption~\ref{assumption:sampling-distribution}, and denote
   \[
    M_p \defeq \mathbb{E}_{\theta\sim\rho}[\norm{\theta-\theta^*}^{p}_2].
   \]
   Denote by $v_1$ the leading eigenvector of $\EGOP(f)$, and let $\beta \defeq \shortnorm{v_1}^2_1/d$. Then for any $\delta \in (0, 1)$, $\varepsilon > 0$, if the Lipschitz constant of the Hessian satisfies $H\leq \min(T_1(\delta, \varepsilon), T_2(\delta, \varepsilon))$ defined as
    \[
        T_1 = \frac{\varepsilon\HessLambdaMax (\beta-\delta)}{2(\max_{t}\{\shortnorm{\ttheta_t}_2+c)(1+\delta)}
    \]
    \begin{align*}
        T_2 &= \frac{\HessLambdaMax}{M_4 \sqrt{d}}\left(\sqrt{M_3^2 + \zeta c^2 M_4} -  M_3\right)
    \end{align*}
    where $\zeta \defeq \delta/(1+\delta)$ is sufficiently small, i.e.
    \[
        \zeta \leq \frac{\beta \varepsilon\sqrt{1-\varepsilon^2}}{\beta \varepsilon\sqrt{1-\varepsilon^2} + 4-\varepsilon^2}
    \]
    then the convergence of reparameterized Adagrad is bounded by
    \[
        \frac{1}{T} \sum_{t=1}^T \norm{\nabla \tf(\ttheta_t)}_1 = \tilde{O}\left(\frac{\eta }{\sqrt{T}}\left(\frac{1}{\beta-\delta}\cdot \frac{\sr_f }{d}\cdot L_f + \varepsilon\right) + \frac{\Delta_f}{\eta\sqrt{T}}\right).
    \]
\end{theorem}

Thm.~\ref{thm:formal-compare-convergence-guarantees} follows from combining the following result with the convergence guarantee in Eq.~\ref{eq:jiang-fullbatch-gradient-guarantee}.

\begin{theorem}\label{thm:simplified-smoothness-ratio}
    Consider $f(\cdot)$ satisfying Assumptions~\ref{assumption:sampling-distribution} and  \ref{assume:Lipschitz-Hessian}. Consider the ball in $\R^d$ of radius $B$ centered at the origin. Denote by $v_1$ the leading eigenvector of $\EGOP(f)$, and let $\beta \defeq \shortnorm{v_1}^2_1/d$. Then for any $\delta \in (0, 1)$, $\varepsilon > 0$, if the Lipschitz constant of the Hessian satisfies $H\leq \min(T_1(\delta, \varepsilon), T_2(\delta, \varepsilon))$ defined as
    \[
        T_1 = \frac{\varepsilon\HessLambdaMax (\beta-\delta)}{2(B+c)(1+\delta)}
    \]
    \begin{align*}
        T_2 &= \frac{\HessLambdaMax}{M_4 \sqrt{d}}\left(\sqrt{M_3^2 + \zeta c^2 M_4} -  M_3\right),
    \end{align*}
    where $\zeta \defeq \delta/(1+\delta)$ is sufficiently small, i.e.
    \[
        \zeta \leq \frac{\beta \varepsilon\sqrt{1-\varepsilon^2}}{\beta \varepsilon\sqrt{1-\varepsilon^2} + 4-\varepsilon^2}
    \]
    then within the ball of radius $B$, $f(\cdot)$ and $\tf(\cdot)$ satisfy \cref{eq:def-coordinate-wise-smoothness} with respect to coordinate-wise smoothness constants whose sums $L_f$ and $L_{\tf}$ respectively satisfy
    \[
        \frac{L_{\tf}}{L_f}\leq \frac{2\sqrt{1+\delta}}{(\beta-\delta)}\frac{\sr_f}{d} +2\varepsilon
    \]
    where $\sr_f$ is the stable rank of $\EGOP(f)$, defined in \cref{eq:def-stable-rank}.
\end{theorem}

We prove Theorem~\ref{thm:simplified-smoothness-ratio} by establishing the following pair of claims. The first lower bounds the term $L_f$:
\begin{lemma}\label{lem:general-LB}
    Consider $f(\cdot)$ satisfying Assumptions~\ref{assumption:sampling-distribution} and \ref{assume:Lipschitz-Hessian} and consider the set of dense unit vectors $\nu \in \{\pm d^{-1/2}\}^d$, the collection of vectors whose entries all have magnitude $\abs{\nu(i)} = d^{1/2}$. Then the smoothness constants of $f$ satisfy
    \[
        L_f \geq \frac{d}{2c}\cdot \max_{\nu \in \{\pm d^{-1/2}\}^d}\frac{\ip{\nu, \EGOP\nu} - \gamma}{\sqrt{\lambda_{\max}(\operatorname{EGOP}) + \gamma}}
    \]
    where
    \[
        \gamma \defeq  2\sqrt{d}H \lambda_{\max}(\nabla^2 f(\theta^*)) M_3 + dH^2 M_4
    \]
    where $H$ is the Lipschitz constant of the Hessian of $f(c\dot)$ in Assumption~\ref{assume:Lipschitz-Hessian} and $M_p\defeq \mathbb{E}_{\theta\sim\rho}[\norm{\theta-\theta^*}^p_2]$.
\end{lemma}
If the EGOP has dense leading eigenvectors, then the term $\langle\nu, \EGOP \nu\rangle$ is large. In particular, \cref{lem:general-LB} implies the following:
\begin{corollary}\label{claim:OG-coor-LB}
    For $f(\cdot)$ satisfying Assumptions~\ref{assumption:sampling-distribution} and \ref{assume:Lipschitz-Hessian}, the smoothness constants of $f$ satisfy
    \[
        L_f \geq \frac{d}{2c}\frac{\beta_k\lambda_k(\operatorname{EGOP}) - \gamma}{\sqrt{\lambda_{\max}(\operatorname{EGOP}) + \gamma}}.
    \]
    where $\beta_k \defeq \norm{v_k}^2_1/d$ for $\lambda_k(\operatorname{EGOP}), v_k$ the $k$\ts{th} eigenvalue and eigenvector of $\EGOP(f)$.
\end{corollary}

In contrast, we show that under  reparameterization by the EGOP eigenbasis, the smoothness constants can be upper bounded by the following:
 
\begin{lemma}\label{claim:reparam-coor-UB}
    Let $V$ be the eigenbasis of the EGOP of $f(\cdot)$ with respect to $\rho$, and define $\tf(\ttheta)\defeq f(V\theta)$. Let the function $f(\cdot)$ satisfy Assumptions~\ref{assumption:sampling-distribution},  and \ref{assume:Lipschitz-Hessian}. Then within the ball of radius $B$ centered at the origin, $\tf(\cdot)$ satisfies Eq.~\ref{eq:def-coordinate-wise-smoothness} with respect to smoothness coordinates whose sum is bounded by
    \[
        L_{\tf} \leq d\left(\frac{\sqrt{\gamma}}{c} + H(B+c)\right)+ \frac{1}{c}\sum_{i=1}^d \sqrt{\lambda_i}
    \]
    where $\gamma$ has the same value as in Claim~\ref{claim:OG-coor-LB} and $\lambda_i$ denotes the $i$th eigenvalue of the EGOP of $f$ with respect to $\rho$.
\end{lemma}

Note that as Lipschitz constant of Hessian in Assumption~\ref{assume:Lipschitz-Hessian} goes to zero, so does the value of $\gamma$ in both Claim~\ref{claim:OG-coor-LB} and \ref{claim:reparam-coor-UB}.
