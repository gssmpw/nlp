\section{Supplementary Figures}\label{sec:supplementary-figures}

\subsection{Supplementary Figures for Section~\ref{sec:EGOP-spectral-decay}}\label{ssec:sup-figs-spectral-decay}

\paragraph{Spectral decay is robust to choice of sampling distribution} In Figure~\ref{fig:tinyMNIST-compare-spectral-decay}, we present evidence that spectral decay visualized in Figure~\ref{fig:tinyMNIST-global-spectral-decay} is robust to choice of sampling distribution. The EGOP whose eigenspectrum is displayed in Fig.~\ref{fig:tinyMNIST-global-spectral-decay} was generated from gradient samples $\nabla f(\theta_i)$ for $\theta_i\sim \mathcal{N}(0,\mathbb{I})$, but as visualized in later figures  (Fig.~\ref{fig:tinyMNIST-compare-spectral-decay}), the level of spectral decay is comparable to that obtained with Gaussian distributions of differing scales. We also compare to the spectral decay exhibited by the EGOP matrix when $\rho$ is taken to be an initialization distribution used in practice \cite{glorot2010understanding}. For details on this distribution, see \cref{ssec:details-for-main-body-experimental-results}. We also note that the interesting shelf structure of the leading eigenvalues is also robust to choice of sampling distribution, as illustrated in Fig.~\ref{fig:tinyMNIST-compare-spectral-decay}. Full details for the objective and EGOP estimation procedure for this figure are detailed in \cref{ssec:details-for-spectral-decay}.

\begin{figure*}[h!]
    \centering
    \includegraphics[width=.9\linewidth]{Images/final_figures/comparing_spectral_decay.pdf}
    \caption{Comparing EGOP spectral decay of a 2-layer ReLU network on tinyMNIST dataset. Plot displays the ratio $\lambda_k/\lambda_1$ as a function of eigenvalue index $k$, for eigenvalues indexed in decreasing order. The blue, orange, green, red, and purple colored traces display the eigenspectrum of the EGOP with respect to a mean-zero Gaussian with covariance $\sigma^2 \mathbb{I}$, for varying values of $\sigma$. The brown trace displays the eigenspectrum of the EGOP with respect to a realistic initialization distribution for this architecture: weights for each layer are drawn from a scaled Xavier normal distribution, and biases are initialized from a scaled uniform distribution (see \cref{ssec:details-for-main-body-experimental-results}). We observe that under all sampling distributions, the eigenspectrum exhibits spectral decay, and that the realistic initialization distribution has spectral decay very comparable to that of the standard Gaussian, displayed in Fig.~\ref{fig:tinyMNIST-global-spectral-decay} in the main body. }
    \label{fig:tinyMNIST-compare-spectral-decay}
\end{figure*}

\paragraph{Spectral decay persists in block EGOP matrices} \cref{fig:layer-by-layer-spectra} plots the normalized eigenspectra of the block matrices corresponding to the first and second layers of ReLU networks on the UCI digits dataset and fashionMNIST dataset respectively. For full details on these datasets and the architectures used, see \cref{ssec:details-for-spectral-decay}. Interestingly, both datasets exhibit shared characteristics: the normalized spectral decay in the first layer is strikingly similar, and in both networks the spectral decay in the first layer is more pronounced than in the second layer.

\begin{figure*}[h!]
    \centering        \includegraphics[width=0.9\linewidth]{Images/final_figures/layer_eigenspectra.pdf}
    \caption{Eigenspectra of the layerwise EGOP matrices of neural networks on \texttt{tinyMNIST} and \texttt{fashionMNIST}. The spectral decay observed in~\cref{fig:tinyMNIST-global-spectral-decay} persists for layer EGOP matrices, defined in Section~\ref{ssec:efficient-heuristics}, and across datasets. Y-axes for all figures display identical ranges.}
    \label{fig:layer-by-layer-spectra}
\end{figure*}

\paragraph{Density of leading EGOP eigenvectors} \cref{thm:compare-convergence-guarantees} shows that when the leading EGOP eigenvector is dense, reparameterized Adagrad enjoys much stronger convergence guarantees. Density of the $k$\ts{th} eigenvector is measured by $\beta_k\defeq \norm{v_k}^2_1$. \cref{thm:compare-convergence-guarantees} states a guarantee in terms of $\beta \defeq \beta_1$, the density of the leading eigenvector.

In order for the leading term in \cref{thm:compare-convergence-guarantees} in the bound for reparameterized Adagrad to scale as $\tilde{O}\big((\sr_f/d\big)\cdot(\eta L_f/\sqrt{T}))$, it is sufficient that $O(\beta-1/3)^{-1}=1$, or in particular that $\beta-1/3 \gg1/d$. In \cref{fig:MNIST-global-eigvec-density}, we show that for the UCI digits dataset, this condition is satisfied. Specifically, we show that $\beta > 0.5$, while for this network $1/d < 5e-4$, meaning that indeed $\beta-1/3 \gg1/d$. Moreover we show that not only does the leading eigenvector satisfy this density assumption, but several of the leading eigenvectors satisfy $\beta_k-1/3 \gg1/d$.
\begin{figure*}[h!]
    \centering        \includegraphics[width=0.5\linewidth]{Images/final_figures/tinyMNIST_global_eigvec_density.pdf}
    \caption{Plotting the density measure $\beta_k\defeq \norm{v_k}^2_1$ for the leading 100 eigenvectors of $\EGOP(f)$, where $f(\cdot)$ is the cross-entropy loss of a 2-layer ReLU neural network on the UCI digits training dataset. The leading eigenvector satisfies $\beta > 0.5$ and several have density $\beta_k > 0.3$. We visualize the value $1/d$ in red  (for this example, $d = 2,410$) to verify that for the leading eigenvectors, $\beta_k \gg 1/d$.}
    \label{fig:MNIST-global-eigvec-density}
\end{figure*}

\subsection{Supplementary Figures for Section~\ref{sec:experimental-results}}\label{ssec:sup-figs-main-experiments}

\paragraph{Multilayer linear networks} We compare three methods for EGOP reparameterization in order to examine some heuristics proposed in Section~\ref{ssec:efficient-heuristics}. In \cref{fig:sup-linear-networks}, we consider training a multilayer linear network (\ref{eq:linear-feedforward-objective}) under  global EGOP reparameterization, wherein all parameters are reparameterized simultaneously as in \cref{alg:meta-algorithm-block} (\cref{fig:global-sup});  block reparameterization for all layers, following the procedure defined in Section~\ref{ssec:efficient-heuristics} (\cref{fig:block-sup}); and block reparameterization of only the parameters in the first layer (\cref{fig:first-layer-only-sup}). For all three methods, we estimate the EGOP using the same number of gradient samples: $M = 2d$, where $d$ is the total number of network parameters.

Comparing \cref{fig:global-sup} wiht \cref{fig:block-sup} shows that for this problem, EGOP reparameterization offers comparable benefit when using block reparameterization as when using global reparameterization. This suggests that block EGOP reparameterization, which has a reduced computational cost compared to global EGOP reparameterization, may be an effective way to accelerate adaptive methods when problem instances are too large to permit global reparameterization. Reparameterizing only the first layer (\cref{fig:first-layer-only-sup}) improves Adagrad's performance by a margin comparable to that of global and block reparameterization of all layers, but the benefit to Adam under reparameterization of only the first layer is much less pronounced.


\begin{figure*}
    \centering
    % First subfigure
    \begin{subfigure}[t]{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{Images/final_figures/global_final_train_loss_by_LR-sup.pdf}
        \caption{Global EGOP Reparameterization}
        \label{fig:global-sup}
    \end{subfigure}
    % Second subfigure
    \begin{subfigure}[t]{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{Images/final_figures/layerwise_final_train_loss_by_LR_no_legend.pdf}
        \caption{Block reparameterization, all layers}
        \label{fig:block-sup}
    \end{subfigure}
    % Third subfigure
    \begin{subfigure}[t]{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{Images/final_figures/first-layer-only_final_train_loss_by_LR_no_legend.pdf}
        \caption{First layer only}
        \label{fig:first-layer-only-sup}
    \end{subfigure}
    \caption{Comparing three  EGOP reparameterization methods for training a multilayer linear network (\ref{eq:linear-feedforward-objective}). \cref{fig:global-sup} is a reproduction of \cref{fig:linear-layers-global-reparam-loss-vs-LR} in the main body, and shows results for performing EGOP reparameterization of all parameters simultaneously. \cref{fig:block-sup} shows results when performing block EGOP reparameterization, where each network layer forms a block. \cref{fig:first-layer-only-sup} shows results when block-reparameterizing only the parameters in the first layer.}
    \label{fig:sup-linear-networks}
\end{figure*}

\paragraph{Image Classification with ReLU Networks} \cref{fig:tinyMNIST-sup} expands on the results shown in \cref{fig:opener-cartoon}  (right) for 2-layer ReLU networks on the UCI digits dataset. \cref{fig:tinyMNIST-loss-sup} plots final training loss versus learning rate, and shows that benefit of reparameterization shown in \cref{fig:opener-cartoon} is robust to choice of learning rate. \cref{fig:tinyMNIST-testacc-sup} plots final test accuracy versus learning rate, and shows that the improved training offered by reparameterization does not lead to over-fitting. For full experimental details on the architecture, dataset, and loss function used for these experiments, see \cref{ssec:sup-figs-main-experiments}.

\begin{figure*}[h!]
    \centering
    % First subfigure
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includegraphics[width=\linewidth]{Images/final_figures/tinyMNIST_final_loss_vs_LR.pdf}
        \caption{Median final training loss versus learning rate}
        \label{fig:tinyMNIST-loss-sup}
    \end{subfigure}
    \hspace{0.05\textwidth} % Optional space between subfigures
    % Second subfigure
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includegraphics[width=\linewidth]{Images/final_figures/tinyMNIST_final_testacc_by_LR.pdf}
        \caption{Median final test accuracy versus learning rate}
        \label{fig:tinyMNIST-testacc-sup}
    \end{subfigure}
    \caption{Training loss and test accuracy results for the UCI digits dataset image classification task. Results are aggregated over independent trials corresponding to different random initializations. Medians are plotted as traces, and shaded regions indicate the 25\ts{th}-75\ts{th} percentiles.
    } \label{fig:tinyMNIST-sup}
\end{figure*}

Similarly, \cref{fig:fashionMNIST-sup} expands on the results presented in \cref{fig:fashionMNIST}. \cref{fig:fashionMNIST-loss-sup} shows that benefit of reparameterization on training loss is robust to choice of learning rate. \cref{fig:fashionMNIST-testacc-sup} shows that the improved training does not lead to over-fitting. Reparameterized Adagrad achieves a marginally higher median validation accuracy at all learning rates. Reparameterized Adam achieves a marginally lower median maximum validation accuracy at learning rates between $4\times 10^{-3}$ and $0.02$, but the difference amounts to less than half a percentage. 

\begin{figure*}[h!]
    \centering
    % First subfigure
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includegraphics[width=\linewidth]{Images/final_figures/fashionMNIST_final_train_loss_by_LR.pdf}
        \caption{Median final training loss versus learning rate}
        \label{fig:fashionMNIST-loss-sup}
    \end{subfigure}
    \hspace{0.05\textwidth} % Optional space between subfigures
    % Second subfigure
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includegraphics[width=\linewidth]{Images/final_figures/fashionMNIST_final_testacc_by_LR.pdf}
        \caption{Median final test accuracy versus learning rate}
        \label{fig:fashionMNIST-testacc-sup}
    \end{subfigure}
    \caption{Training loss and test accuracy results for the fashionMNIST image classification task. Results are aggregated over independent trials corresponding to different random initializations. Medians are plotted as traces, and shaded regions indicate the 25\ts{th}-75\ts{th} percentiles.
    }\label{fig:fashionMNIST-sup}
\end{figure*}

We note that both \cref{fig:tinyMNIST-testacc-sup} and \cref{fig:fashionMNIST-testacc-sup} display small discrepancies between SGD in original coordinates and under reparameterization (and similarly for SGD with momentum), despite the fact that SGD is theoretically equivariant. We hypothesize that these discrepancies arise from the fac that matrix multiplication on GPUs is non-deterministic; because reparameterization involves additional matrix multiplications by the change-of-basis matrix $V$, this may lead to discrepancies arising from the nondeterministic accumulation of numerical error. We note that we do verify that our implementation yields identical results for SGD and SGD with momentum on CPU for small numbers of epochs.

\paragraph{Convex objectives} \cref{fig:sup-cvx-objectives} plots the gradient Euclidean norm over iterates for Adam, Adagrad, GD and GD with momentum. Unlike \cref{fig:cvx-objectives}---where each algorithm uses the same learning rate in both original coordinates and under reparameterization, and this learning rate is tuned to produce the best performance in original coordinates---\cref{fig:sup-cvx-objectives} shows results when learning rates are tuned separately in original coordinates versus under reparameterization. As a result, the discrepancy between Adagrad in original coordinates versus under reparameterization is more pronounced in \cref{fig:log-sum-exp-sup} and \cref{fig:linear-least-squares-sup} than in the corresponding plots in \cref{fig:cvx-objectives}, while the qualitative results for logistic regression (\cref{fig:log-reg-sup}) are unchanged.

\begin{figure*}
    \centering
    % First subfigure
    \begin{subfigure}[t]{0.34\textwidth}
        \centering
        \includegraphics[width=\linewidth]{Images/final_figures/log-sum-exp_supplementary-grad_norm_vs_iterate_all_solvers.pdf}
        \caption{Log-sum-exp (\ref{eq:log-sum-exp-objective}) with $\alpha = 2$}
        \label{fig:log-sum-exp-sup}
    \end{subfigure}
    %\hspace{0.05\textwidth} % Optional space between subfigures
    % Second subfigure
    \begin{subfigure}[t]{0.315\textwidth}
        \centering
        \includegraphics[width=\linewidth]{Images/final_figures/log-reg_supplementary-grad_norm_vs_iterate_all_solvers_no_legend.pdf}
        \caption{Logistic regression with $\alpha = 3$}
        \label{fig:log-reg-sup}
    \end{subfigure}
    % Third subfigure
    \begin{subfigure}[t]{0.315\textwidth}
        \centering
        \includegraphics[width=\linewidth]{Images/final_figures/Linear_least_squares_supplementary-grad_norm_vs_iterate_all_solvers_no_legend.pdf}
        \caption{Linear least-squares with $\alpha = 2$}
        \label{fig:linear-least-squares-sup}
    \end{subfigure}
    \caption{Counterpart to \cref{fig:cvx-objectives}, where learning rates are tuned separately for methods in original coordinates versus under reparameterization. The discrepancy between Adagrad in original coordinates versus under reparameterization is more pronounced in \cref{fig:log-sum-exp-sup} and \cref{fig:linear-least-squares-sup} than in the corresponding plots in \cref{fig:cvx-objectives}, while the qualitative results for logistic regression (\cref{fig:log-reg-sup}) are unchanged.}
    \label{fig:sup-cvx-objectives}
\end{figure*}