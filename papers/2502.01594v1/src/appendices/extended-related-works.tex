\section{Extended Discussion of Related Works}\label{sec:extended-related-works}

In this section, we expand on the overview of related work presented in Section~\ref{ssec:related-work}.

\paragraph{Geometric sensitivity of adaptive methods} A large body of research has been devoted to understanding the settings in which the per-coordinate learning rates of adaptive algorithms confer a benefit over more basic methods, such as (S)GD. Recently there has been renewed interest in distinguishing the properties of adaptive algorithms versus SGD because several empirical studies suggest that adaptive methods outperform SGD when training transformer models \cite{kunstner2024heavy,zhang2024transformers}. Traditional analyses of Adagrad in the context of online convex optimization establish regret bounds which can be either better or worse than those enjoyed by SGD by up to a factor of $\sqrt{d}$, for $d$ the number of problem parameters \cite{duchi2011adaptive, pmlr-v247-chen24e}. More recent research has studied the rates at which adaptive methods converge to stationary points. Several works study convergence guarantees, measured in terms of the $\ell_2$ norm of the gradient, on smooth non-convex objectives \cite{ward2020adagrad, defossez2020simple}. These results show that Adagrad, Adam and related variants achieve rates matching those enjoyed by SGD, while having the key distinction that Adagrad and its variants do not require a-priori knowledge of the objective function's Lipschitz constant \cite{ward2020adagrad, defossez2020simple}.

In order to shed light on the question of when adaptive algorithms enjoy provably stronger guarantees than SGD, a recent line of work studies convergence under more refined geometric assumptions, with particular emphasis on assumptions that are \textit{not} rotationally invariant \cite{jiang2024convergence, liu2024adagrad, xie2024adamexploitsellinftygeometryloss}. \citet{xie2024adamexploitsellinftygeometryloss} establish convergence guarantees in terms of the $L_\infty$ smoothness constant of the objective and also in terms of the related Hessian 1-norm. They show that rotationally invariant geometric assumptions do not suffice to capture settings when Adam out-performs SGD through experiments examining the sensitivity of Adam to orthonormal rotations \cite{xie2024adamexploitsellinftygeometryloss}. 

\citet{jiang2024convergence} and \citet{liu2024adagrad} study convergence of Adagrad on objectives that satisfy coordinate-wise smoothness. Both works prove similar convergence guarantees for Adagrad in terms of $\ell_1$ gradient norm, rather than the $\ell_2$ gradient norm measure more widely studied; \citet{jiang2024convergence} establish convergence guarantees showing that SGD remains worst-case optimal even in the setting of coordinate-wise smoothness when measuring stationarity with the $\ell_2$ gradient norm, motivating the need to measure stationarity with the $\ell_1$ norm in order to prove separation between Adagrad and SGD. Using this $\ell_1$ stationarity measure, \citet{jiang2024convergence} establish provable separation between SGD's and Adagrad's convergence to stationary points of non-convex objectives. They show that when objective functions exhibit certain geometric properties, measured by their coordinate-wise smoothness constants, Adagrad's upper bounds are lower than SGD's lower bounds by a factor of $d$ \cite{jiang2024convergence}. Our analysis builds on that of \citet{jiang2024convergence} and \citet{liu2024adagrad}, as a consequence of our results is that the EGOP reparameterization proposed in this work acts to transform objectives into the setting identified by \citet{jiang2024convergence} where Adagrad's convergence guarantees compare most favorably with SGD's. 

\citet{ling2022vectoradam} develop a rotationally equivariant extension of Adam, termed VectorAdam, in an effort to reduce axis-aligned artifacts that arise when using adaptive methods to solve geometric optimization problems such as differentiable rendering. This method targets applications when the problem parameters $\theta\in \R^{r\cdot n}$ represent a collection of $r$ different vectors in $\R^n$. VectorAdam uses the squared gradient norm of each $n$-dimensional vector to rescale the learning rates of all entries in each of the $r$ vectors comprising $\theta$, making the algorithm equivariant to transformations of the form $Q\mat{\theta}$, where $\mat{\theta}\in \R^{n\times r}$ is the reshaping of $\theta\in \R^{r\cdot n}$ and $Q \in \R^{n\times n}$ is orthonormal \cite{ling2022vectoradam}.

\paragraph{Change-of-basis for Optimization}

Several recent works propose that when using Adam and its variants to train neural networks,  different choices of orthonormal transformation can reduce the computational cost associated with these algorithms and improve their performance \cite{vyas2024soap, maes2024understanding, zhao2024galore}. In \cref{ssec:related-work}, we provided an overview of the work of \citet{vyas2024soap}, \citet{zhao2024galore}, and \citet{maes2024understanding}. SOAP and GaLore both call for periodically re-computing the change-of-basis matrices. In their experiments, the fewest re-computations performed by \citet{vyas2024soap} was once every 100 batches, and they show that the performance gap between SOAP and Adam narrows as the number of batches between re-computations increases. In contrast, our we show that with our proposed choice of basis, a single up-front computation of the change-of-basis suffices to improve the performance of both Adam and Adagrad in a variety of settings, as shown in Section~\ref{sec:experimental-results}.

Outside of the domain neural network training, several works have considered data-driven methods for performing dimensionality reduction when optimizing objectives with low-rank EGOP matrices, including the works of \citet{cartis2024learning} and \citet{cosson2023low}. For functions with low-dimensional active subspaces, \citet{cartis2024learning} study using the EGOP eigenbasis to reparameterize and reduce the dimension of optimization problems. They demonstrate that this approach yields computational speedups for loss functions whose EGOP matrix is low-rank. \citet{cosson2023low} develop gradient descent algorithms which leverage the EGOP eigenbasis. Their method first estimates the EGOP and computes the corresponding leading $r$-eigenvectors, and then performs gradient descent steps along the directions of these $r$ vectors. In the setting of exactly low-rank functions, they prove convergence results illustrating that this approach improves the dimensional dependency of gradient descent.  