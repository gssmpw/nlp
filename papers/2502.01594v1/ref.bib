@article{charisopoulos2023superlinearly,
  title={{A superlinearly convergent subgradient method for sharp semismooth problems}},
  author={Charisopoulos, Vasileios and Davis, Damek},
  journal={{Mathematics of Operations Research}},
  year={2023},
  publisher={INFORMS}
}

@article{cartis2024learning,
  title={{Learning the subspace of variation for global optimization of functions with low effective dimension}},
  author={Cartis, Coralia and Liang, Xinzhu and Massart, Estelle and Otemissov, Adilet},
  journal={arXiv preprint arXiv:2401.17825},
  year={2024}
}

@inproceedings{damian2022neural,
  title={Neural networks can learn representations with gradient descent},
  author={Damian, Alexandru and Lee, Jason and Soltanolkotabi, Mahdi},
  booktitle={{Conference on Learning Theory}},
  pages={5413--5452},
  year={2022},
  organization={PMLR}
}

@article{vaswani2017low,
  title={Low-rank phase retrieval},
  author={Vaswani, Namrata and Nayer, Seyedehsara and Eldar, Yonina C},
  journal={{IEEE Transactions on Signal Processing}},
  volume={65},
  number={15},
  pages={4059--4074},
  year={2017},
  publisher={IEEE}
}

@article{duchi2011adaptive,
  title={{Adaptive subgradient methods for online learning and stochastic optimization.}},
  author={Duchi, John and Hazan, Elad and Singer, Yoram},
  journal={{Journal of Machine Learning Research}},
  volume={12},
  number={7},
  year={2011}
}

@article{hazan2016introduction,
  title={{Introduction to online convex optimization}},
  author={Hazan, Elad and others},
  journal={{Foundations and Trends{\textregistered} in Optimization}},
  volume={2},
  number={3-4},
  pages={157--325},
  year={2016},
  publisher={Now Publishers, Inc.}
}

@article{cosson2023low,
  title={{Low-Rank Gradient Descent}},
  author={Cosson, Romain and Jadbabaie, Ali and Makur, Anuran and Reisizadeh, Amirhossein and Shah, Devavrat},
  journal={{IEEE Open Journal of Control Systems}},
  year={2023},
  publisher={IEEE}
}

@book{constantine2015active,
  title={{Active subspaces: Emerging ideas for dimension reduction in parameter studies}},
  author={Constantine, Paul G},
  year={2015},
  publisher={SIAM}
}

@article{ling2022vectoradam,
  title={Vectoradam for rotation equivariant geometry optimization},
  author={Ling, Selena Zihan and Sharp, Nicholas and Jacobson, Alec},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={4111--4122},
  year={2022}
}

@article{PS70,
  title={Free states of the canonical anticommutation relations},
  author={Powers, Robert T and St{\o}rmer, Erling},
  journal={Communications in Mathematical Physics},
  volume={16},
  number={1},
  pages={1--33},
  year={1970},
  publisher={Springer}
}


@article{bunch1978rank,
  title={Rank-one modification of the symmetric eigenproblem},
  author={Bunch, James R and Nielsen, Christopher P and Sorensen, Danny C},
  journal={Numerische Mathematik},
  volume={31},
  number={1},
  pages={31--48},
  year={1978},
  publisher={Springer}
}

@article{golub1973some,
  title={Some modified matrix eigenvalue problems},
  author={Golub, Gene H},
  journal={SIAM Review},
  volume={15},
  number={2},
  pages={318--334},
  year={1973},
  publisher={SIAM}
}

%%%%%%%%%%%%%%%%%%%%%%%%
%% CONVERGENCE ANALYSES
%%%%%%%%%%%%%%%%%%%%%%%%

% HAZAN'S OPEN PROBLEM
@InProceedings{pmlr-v247-chen24e,
  title = 	 {Open Problem: Black-Box Reductions and Adaptive Gradient Methods for Nonconvex Optimization},
  author =       {Chen, Xinyi and Hazan, Elad},
  booktitle = 	 {Proceedings of Thirty Seventh Conference on Learning Theory},
  pages = 	 {5317--5324},
  year = 	 {2024},
  editor = 	 {Agrawal, Shipra and Roth, Aaron},
  volume = 	 {247},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {30 Jun--03 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v247/chen24e/chen24e.pdf},
  url = 	 {https://proceedings.mlr.press/v247/chen24e.html},
  abstract = 	 {We describe an open problem: reduce offline nonconvex stochastic optimization to regret minimization in online convex optimization. The conjectured reduction aims to make progress on explaining the success of adaptive gradient methods for deep learning. A prize of 500 dollars is offered to the winner.}
}

% Rachel Ward's work on Adagrad-norm
@article{ward2020adagrad,
  title={Adagrad stepsizes: Sharp convergence over nonconvex landscapes},
  author={Ward, Rachel and Wu, Xiaoxia and Bottou, Leon},
  journal={Journal of Machine Learning Research},
  volume={21},
  number={219},
  pages={1--30},
  year={2020}
}

% Bach's work with smoothness
@article{defossez2020simple,
  title={{A Simple Convergence Proof of Adam and Adagrad}},
  author={D{\'e}fossez, Alexandre and Bottou, L{\'e}on and Bach, Francis and Usunier, Nicolas},
  journal={arXiv preprint arXiv:2003.02395},
  year={2020}
}

% RECENT ANALYSES OF ADAGRAD WITH GEOMETRIC ASSUMPTIONS
@misc{xie2024adamexploitsellinftygeometryloss,
      title={{Adam Exploits $\ell_\infty$-geometry of Loss Landscape via Coordinate-wise Adaptivity}}, 
      author={Shuo Xie and Mohamad Amin Mohamadi and Zhiyuan Li},
      year={2024},
      eprint={2410.08198},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2410.08198}, 
}

@article{jiang2024convergence,
  title={Convergence analysis of adaptive gradient methods under refined smoothness and noise assumptions},
  author={Jiang, Ruichen and Maladkar, Devyani and Mokhtari, Aryan},
  journal={arXiv preprint arXiv:2406.04592},
  year={2024}
}

@article{liu2024adagrad,
  title={{AdaGrad under Anisotropic Smoothness}},
  author={Liu, Yuxing and Pan, Rui and Zhang, Tong},
  journal={arXiv preprint arXiv:2406.15244},
  year={2024}
}

@article{maes2024understanding,
  title={{Understanding Adam Requires Better Rotation Dependent Assumptions}},
  author={Maes, Lucas and Zhang, Tianyue H and Jolicoeur-Martineau, Alexia and Mitliagkas, Ioannis and Scieur, Damien and Lacoste-Julien, Simon and Guille-Escuret, Charles},
  journal={arXiv preprint arXiv:2410.19964},
  year={2024}
}
%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%
%% Shampoo, SOAP, and Galore
%%%%%%%%%%%%%%%%%%%%%%%%
@inproceedings{gupta2018shampoo,
  title={Shampoo: Preconditioned stochastic tensor optimization},
  author={Gupta, Vineet and Koren, Tomer and Singer, Yoram},
  booktitle={International Conference on Machine Learning},
  pages={1842--1850},
  year={2018},
  organization={PMLR}
}

@article{vyas2024soap,
  title={{SOAP: Improving and stabilizing Shampoo using Adam}},
  author={Vyas, Nikhil and Morwani, Depen and Zhao, Rosie and Shapira, Itai and Brandfonbrener, David and Janson, Lucas and Kakade, Sham},
  journal={arXiv preprint arXiv:2409.11321},
  year={2024}
}

@article{zhao2024galore,
  title={Galore: Memory-efficient llm training by gradient low-rank projection},
  author={Zhao, Jiawei and Zhang, Zhenyu and Chen, Beidi and Wang, Zhangyang and Anandkumar, Anima and Tian, Yuandong},
  journal={arXiv preprint arXiv:2403.03507},
  year={2024}
}
%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%
%% SGD vs Adam for LLMs and Transformers
%%%%%%%%%%%%%%%%%%%%%%%%
@article{kunstner2024heavy,
  title={{Heavy-tailed class imbalance and why Adam outperforms gradient descent on language models}},
  author={Kunstner, Frederik and Yadav, Robin and Milligan, Alan and Schmidt, Mark and Bietti, Alberto},
  journal={arXiv preprint arXiv:2402.19449},
  year={2024}
}
@article{zhang2024transformers,
  title={{Why transformers need Adam: A Hessian perspective}},
  author={Zhang, Yushun and Chen, Congliang and Ding, Tian and Li, Ziniu and Sun, Ruoyu and Luo, Zhi-Quan},
  journal={arXiv preprint arXiv:2402.16788},
  year={2024}
}
%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%
%% Works showing that in ML objectives, a small subset of directions are most influential
%%%%%%%%%%%%%%%%%%%%%%%%

% See Zhang above.

@article{sagun2017empirical,
  title={Empirical analysis of the hessian of over-parametrized neural networks},
  author={Sagun, Levent and Evci, Utku and Guney, V Ugur and Dauphin, Yann and Bottou, Leon},
  journal={arXiv preprint arXiv:1706.04454},
  year={2017}
}

@article{papyan2018full,
  title={The full spectrum of deepnet hessians at scale: Dynamics with sgd training and sample size},
  author={Papyan, Vardan},
  journal={arXiv preprint arXiv:1811.07062},
  year={2018}
}

%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%
%% Works studying EGOP/active subspaces for NNs
%%%%%%%%%%%%%%%%%%%%%%%%
@article{cui2020active,
  title={Active subspace of neural networks: Structural analysis and universal attacks},
  author={Cui, Chunfeng and Zhang, Kaiqi and Daulbaev, Talgat and Gusak, Julia and Oseledets, Ivan and Zhang, Zheng},
  journal={SIAM Journal on Mathematics of Data Science},
  volume={2},
  number={4},
  pages={1096--1122},
  year={2020},
  publisher={SIAM}
}
%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%
%% Belkin EGOP projects
%%%%%%%%%%%%%%%%%%%%%%%%

@article{zhu2023catapults,
  title={{Catapults in SGD: spikes in the training loss and their impact on generalization through feature learning}},
  author={Zhu, Libin and Liu, Chaoyue and Radhakrishnan, Adityanarayanan and Belkin, Mikhail},
  journal={arXiv preprint arXiv:2306.04815},
  year={2023}
}

@article{mallinar2024emergence,
  title={Emergence in non-neural models: grokking modular arithmetic via average gradient outer product},
  author={Mallinar, Neil and Beaglehole, Daniel and Zhu, Libin and Radhakrishnan, Adityanarayanan and Pandit, Parthe and Belkin, Mikhail},
  journal={arXiv preprint arXiv:2407.20199},
  year={2024}
}

% Neural feature ansatz
@article{radhakrishnan2022mechanism,
  title={Mechanism of feature learning in deep fully connected networks and kernel machines that recursively learn features},
  author={Radhakrishnan, Adityanarayanan and Beaglehole, Daniel and Pandit, Parthe and Belkin, Mikhail},
  journal={arXiv preprint arXiv:2212.13881},
  year={2022}
}
%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%
%% Stable rank
%%%%%%%%%%%%%%%%%%%%%%%%

@article{chou2024gradient,
  title={Gradient descent for deep matrix factorization: Dynamics and implicit bias towards low rank},
  author={Chou, Hung-Hsu and Gieshoff, Carsten and Maly, Johannes and Rauhut, Holger},
  journal={Applied and Computational Harmonic Analysis},
  volume={68},
  pages={101595},
  year={2024},
  publisher={Elsevier}
}

@article{rudelson2007sampling,
  title={Sampling from large matrices: An approach through geometric functional analysis},
  author={Rudelson, Mark and Vershynin, Roman},
  journal={Journal of the ACM (JACM)},
  volume={54},
  number={4},
  pages={21--es},
  year={2007},
  publisher={ACM New York, NY, USA}
}

%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%
%% Ada- papers
%%%%%%%%%%%%%%%%%%%%%%%%

@misc{kingma2017adammethodstochasticoptimization,
      title={Adam: A Method for Stochastic Optimization}, 
      author={Diederik P. Kingma and Jimmy Ba},
      year={2017},
      eprint={1412.6980},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1412.6980}, 
}

@misc{reddi2019convergenceadam,
      title={{On the Convergence of Adam and Beyond}}, 
      author={Sashank J. Reddi and Satyen Kale and Sanjiv Kumar},
      year={2019},
      eprint={1904.09237},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1904.09237}, 
}

@misc{zeiler2012adadeltaadaptivelearningrate,
      title={ADADELTA: An Adaptive Learning Rate Method}, 
      author={Matthew D. Zeiler},
      year={2012},
      eprint={1212.5701},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1212.5701}, 
}

@misc{loshchilov2019decoupledweightdecayregularization,
      title={Decoupled Weight Decay Regularization}, 
      author={Ilya Loshchilov and Frank Hutter},
      year={2019},
      eprint={1711.05101},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1711.05101}, 
}

@misc{Crew_2020,
    title={{Google Scholar reveals its most influential papers for 2020}},
    journal={Nature Index},
    publisher={Nature Publishing Group},
    author={Crew, Bec},
    year={2020},
    month={Jul}
} 
% url={https://www.nature.com/nature-index/news/google-scholar-reveals-most-influential-papers-research-citations-twenty-twenty},
%%%%%%%%%%%%%%%%%%%%%%%%



@article{ding2024flat,
  title={Flat minima generalize for low-rank matrix recovery},
  author={Ding, Lijun and Drusvyatskiy, Dmitriy and Fazel, Maryam and Harchaoui, Zaid},
  journal={Information and Inference: A Journal of the IMA},
  volume={13},
  number={2},
  pages={iaae009},
  year={2024},
  publisher={Oxford University Press}
}

@article{lu2000some,
  title={Some new bounds for singular values and eigenvalues of matrix products},
  author={Lu, L-Z and Pearce, Charles EM},
  journal={Annals of Operations Research},
  volume={98},
  number={1},
  pages={141--148},
  year={2000},
  publisher={Springer}
}



@article{udell2019big,
  title={Why are big data matrices approximately low rank?},
  author={Udell, Madeleine and Townsend, Alex},
  journal={SIAM Journal on Mathematics of Data Science},
  volume={1},
  number={1},
  pages={144--160},
  year={2019},
  publisher={SIAM}
}

%% DATASET CITATIONS
@misc{optical_recognition_of_handwritten_digits_80,
  author       = {Alpaydin, E. and Kaynak, C.},
  title        = {{Optical Recognition of Handwritten Digits}},
  year         = {1998},
  howpublished = {UCI Machine Learning Repository},
  note         = {{DOI}: https://doi.org/10.24432/C50P49}
}

@article{xiao2017fashion,
  title={Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms},
  author={Xiao, Han and Rasul, Kashif and Vollgraf, Roland},
  journal={arXiv preprint arXiv:1708.07747},
  year={2017}
}


%%%%%%%%%%%%%%%%%%%%%%%%
%% Citations related to experimental details
%%%%%%%%%%%%%%%%%%%%%%%%

@inproceedings{glorot2010understanding,
  title={Understanding the difficulty of training deep feedforward neural networks},
  author={Glorot, Xavier and Bengio, Yoshua},
  booktitle={Proceedings of the thirteenth international conference on artificial intelligence and statistics},
  pages={249--256},
  year={2010},
  organization={JMLR Workshop and Conference Proceedings}
}

@inproceedings{paszke2017automatic,
  title={{Automatic differentiation in PyTorch}},
  author={Paszke, Adam and Gross, Sam and Chintala, Soumith and Chanan, Gregory and Yang, Edward and DeVito, Zachary and Lin, Zeming and Desmaison, Alban and Antiga, Luca and Lerer, Adam},
  booktitle={NIPS-W},
  year={2017}
}

@software{jax2018github,
  author = {James Bradbury and Roy Frostig and Peter Hawkins and Matthew James Johnson and Chris Leary and Dougal Maclaurin and George Necula and Adam Paszke and Jake Vander{P}las and Skye Wanderman-{M}ilne and Qiao Zhang},
  title = {{JAX}: composable transformations of {P}ython+{N}um{P}y programs},
  url = {http://github.com/jax-ml/jax},
  version = {0.3.13},
  year = {2018},
}

@software{deepmind2020jax,
  title = {The {D}eep{M}ind {JAX} {E}cosystem},
  author = {DeepMind and Babuschkin, Igor and Baumli, Kate and Bell, Alison and Bhupatiraju, Surya and Bruce, Jake and Buchlovsky, Peter and Budden, David and Cai, Trevor and Clark, Aidan and Danihelka, Ivo and Dedieu, Antoine and Fantacci, Claudio and Godwin, Jonathan and Jones, Chris and Hemsley, Ross and Hennigan, Tom and Hessel, Matteo and Hou, Shaobo and Kapturowski, Steven and Keck, Thomas and Kemaev, Iurii and King, Michael and Kunesch, Markus and Martens, Lena and Merzic, Hamza and Mikulik, Vladimir and Norman, Tamara and Papamakarios, George and Quan, John and Ring, Roman and Ruiz, Francisco and Sanchez, Alvaro and Sartran, Laurent and Schneider, Rosalia and Sezener, Eren and Spencer, Stephen and Srinivasan, Srivatsan and Stanojevi\'{c}, Milo\v{s} and Stokowiec, Wojciech and Wang, Luyu and Zhou, Guangyao and Viola, Fabio},
  url = {http://github.com/google-deepmind},
  year = {2020},
}

@ARTICLE{2020SciPy-NMeth,
  author  = {Virtanen, Pauli and Gommers, Ralf and Oliphant, Travis E. and
            Haberland, Matt and Reddy, Tyler and Cournapeau, David and
            Burovski, Evgeni and Peterson, Pearu and Weckesser, Warren and
            Bright, Jonathan and {van der Walt}, St{\'e}fan J. and
            Brett, Matthew and Wilson, Joshua and Millman, K. Jarrod and
            Mayorov, Nikolay and Nelson, Andrew R. J. and Jones, Eric and
            Kern, Robert and Larson, Eric and Carey, C J and
            Polat, {\.I}lhan and Feng, Yu and Moore, Eric W. and
            {VanderPlas}, Jake and Laxalde, Denis and Perktold, Josef and
            Cimrman, Robert and Henriksen, Ian and Quintero, E. A. and
            Harris, Charles R. and Archibald, Anne M. and
            Ribeiro, Ant{\^o}nio H. and Pedregosa, Fabian and
            {van Mulbregt}, Paul and {SciPy 1.0 Contributors}},
  title   = {{{SciPy} 1.0: Fundamental Algorithms for Scientific
            Computing in Python}},
  journal = {Nature Methods},
  year    = {2020},
  volume  = {17},
  pages   = {261--272},
  adsurl  = {https://rdcu.be/b08Wh},
  doi     = {10.1038/s41592-019-0686-2},
}

%%%%%%%%%%%%%%%%%%%%%%%%

@article{halko2011finding,
  title={Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions},
  author={Halko, Nathan and Martinsson, Per-Gunnar and Tropp, Joel A},
  journal={SIAM {R}eview},
  volume={53},
  number={2},
  pages={217--288},
  year={2011},
  publisher={SIAM}
}