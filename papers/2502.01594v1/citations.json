[
  {
    "index": 0,
    "papers": [
      {
        "key": "kunstner2024heavy",
        "author": "Kunstner, Frederik and Yadav, Robin and Milligan, Alan and Schmidt, Mark and Bietti, Alberto",
        "title": "{Heavy-tailed class imbalance and why Adam outperforms gradient descent on language models}"
      },
      {
        "key": "zhang2024transformers",
        "author": "Zhang, Yushun and Chen, Congliang and Ding, Tian and Li, Ziniu and Sun, Ruoyu and Luo, Zhi-Quan",
        "title": "{Why transformers need Adam: A Hessian perspective}"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "duchi2011adaptive",
        "author": "Duchi, John and Hazan, Elad and Singer, Yoram",
        "title": "{Adaptive subgradient methods for online learning and stochastic optimization.}"
      },
      {
        "key": "hazan2016introduction",
        "author": "Hazan, Elad and others",
        "title": "{Introduction to online convex optimization}"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "defossez2020simple",
        "author": "D{\\'e}fossez, Alexandre and Bottou, L{\\'e}on and Bach, Francis and Usunier, Nicolas",
        "title": "{A Simple Convergence Proof of Adam and Adagrad}"
      },
      {
        "key": "ward2020adagrad",
        "author": "Ward, Rachel and Wu, Xiaoxia and Bottou, Leon",
        "title": "Adagrad stepsizes: Sharp convergence over nonconvex landscapes"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "jiang2024convergence",
        "author": "Jiang, Ruichen and Maladkar, Devyani and Mokhtari, Aryan",
        "title": "Convergence analysis of adaptive gradient methods under refined smoothness and noise assumptions"
      },
      {
        "key": "liu2024adagrad",
        "author": "Liu, Yuxing and Pan, Rui and Zhang, Tong",
        "title": "{AdaGrad under Anisotropic Smoothness}"
      },
      {
        "key": "xie2024adamexploitsellinftygeometryloss",
        "author": "Shuo Xie and Mohamad Amin Mohamadi and Zhiyuan Li",
        "title": "{Adam Exploits $\\ell_\\infty$-geometry of Loss Landscape via Coordinate-wise Adaptivity}"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "xie2024adamexploitsellinftygeometryloss",
        "author": "Shuo Xie and Mohamad Amin Mohamadi and Zhiyuan Li",
        "title": "{Adam Exploits $\\ell_\\infty$-geometry of Loss Landscape via Coordinate-wise Adaptivity}"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "jiang2024convergence",
        "author": "Jiang, Ruichen and Maladkar, Devyani and Mokhtari, Aryan",
        "title": "Convergence analysis of adaptive gradient methods under refined smoothness and noise assumptions"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "liu2024adagrad",
        "author": "Liu, Yuxing and Pan, Rui and Zhang, Tong",
        "title": "{AdaGrad under Anisotropic Smoothness}"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "jiang2024convergence",
        "author": "Jiang, Ruichen and Maladkar, Devyani and Mokhtari, Aryan",
        "title": "Convergence analysis of adaptive gradient methods under refined smoothness and noise assumptions"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "jiang2024convergence",
        "author": "Jiang, Ruichen and Maladkar, Devyani and Mokhtari, Aryan",
        "title": "Convergence analysis of adaptive gradient methods under refined smoothness and noise assumptions"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "jiang2024convergence",
        "author": "Jiang, Ruichen and Maladkar, Devyani and Mokhtari, Aryan",
        "title": "Convergence analysis of adaptive gradient methods under refined smoothness and noise assumptions"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "jiang2024convergence",
        "author": "Jiang, Ruichen and Maladkar, Devyani and Mokhtari, Aryan",
        "title": "Convergence analysis of adaptive gradient methods under refined smoothness and noise assumptions"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "liu2024adagrad",
        "author": "Liu, Yuxing and Pan, Rui and Zhang, Tong",
        "title": "{AdaGrad under Anisotropic Smoothness}"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "maes2024understanding",
        "author": "Maes, Lucas and Zhang, Tianyue H and Jolicoeur-Martineau, Alexia and Mitliagkas, Ioannis and Scieur, Damien and Lacoste-Julien, Simon and Guille-Escuret, Charles",
        "title": "{Understanding Adam Requires Better Rotation Dependent Assumptions}"
      },
      {
        "key": "vyas2024soap",
        "author": "Vyas, Nikhil and Morwani, Depen and Zhao, Rosie and Shapira, Itai and Brandfonbrener, David and Janson, Lucas and Kakade, Sham",
        "title": "{SOAP: Improving and stabilizing Shampoo using Adam}"
      },
      {
        "key": "zhao2024galore",
        "author": "Zhao, Jiawei and Zhang, Zhenyu and Chen, Beidi and Wang, Zhangyang and Anandkumar, Anima and Tian, Yuandong",
        "title": "Galore: Memory-efficient llm training by gradient low-rank projection"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "gupta2018shampoo",
        "author": "Gupta, Vineet and Koren, Tomer and Singer, Yoram",
        "title": "Shampoo: Preconditioned stochastic tensor optimization"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "vyas2024soap",
        "author": "Vyas, Nikhil and Morwani, Depen and Zhao, Rosie and Shapira, Itai and Brandfonbrener, David and Janson, Lucas and Kakade, Sham",
        "title": "{SOAP: Improving and stabilizing Shampoo using Adam}"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "vyas2024soap",
        "author": "Vyas, Nikhil and Morwani, Depen and Zhao, Rosie and Shapira, Itai and Brandfonbrener, David and Janson, Lucas and Kakade, Sham",
        "title": "{SOAP: Improving and stabilizing Shampoo using Adam}"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "zhao2024galore",
        "author": "Zhao, Jiawei and Zhang, Zhenyu and Chen, Beidi and Wang, Zhangyang and Anandkumar, Anima and Tian, Yuandong",
        "title": "Galore: Memory-efficient llm training by gradient low-rank projection"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "zhao2024galore",
        "author": "Zhao, Jiawei and Zhang, Zhenyu and Chen, Beidi and Wang, Zhangyang and Anandkumar, Anima and Tian, Yuandong",
        "title": "Galore: Memory-efficient llm training by gradient low-rank projection"
      }
    ]
  },
  {
    "index": 18,
    "papers": [
      {
        "key": "maes2024understanding",
        "author": "Maes, Lucas and Zhang, Tianyue H and Jolicoeur-Martineau, Alexia and Mitliagkas, Ioannis and Scieur, Damien and Lacoste-Julien, Simon and Guille-Escuret, Charles",
        "title": "{Understanding Adam Requires Better Rotation Dependent Assumptions}"
      }
    ]
  },
  {
    "index": 19,
    "papers": [
      {
        "key": "xie2024adamexploitsellinftygeometryloss",
        "author": "Shuo Xie and Mohamad Amin Mohamadi and Zhiyuan Li",
        "title": "{Adam Exploits $\\ell_\\infty$-geometry of Loss Landscape via Coordinate-wise Adaptivity}"
      }
    ]
  },
  {
    "index": 20,
    "papers": [
      {
        "key": "maes2024understanding",
        "author": "Maes, Lucas and Zhang, Tianyue H and Jolicoeur-Martineau, Alexia and Mitliagkas, Ioannis and Scieur, Damien and Lacoste-Julien, Simon and Guille-Escuret, Charles",
        "title": "{Understanding Adam Requires Better Rotation Dependent Assumptions}"
      }
    ]
  },
  {
    "index": 21,
    "papers": [
      {
        "key": "cartis2024learning",
        "author": "Cartis, Coralia and Liang, Xinzhu and Massart, Estelle and Otemissov, Adilet",
        "title": "{Learning the subspace of variation for global optimization of functions with low effective dimension}"
      },
      {
        "key": "cosson2023low",
        "author": "Cosson, Romain and Jadbabaie, Ali and Makur, Anuran and Reisizadeh, Amirhossein and Shah, Devavrat",
        "title": "{Low-Rank Gradient Descent}"
      }
    ]
  },
  {
    "index": 22,
    "papers": [
      {
        "key": "papyan2018full",
        "author": "Papyan, Vardan",
        "title": "The full spectrum of deepnet hessians at scale: Dynamics with sgd training and sample size"
      },
      {
        "key": "sagun2017empirical",
        "author": "Sagun, Levent and Evci, Utku and Guney, V Ugur and Dauphin, Yann and Bottou, Leon",
        "title": "Empirical analysis of the hessian of over-parametrized neural networks"
      },
      {
        "key": "zhang2024transformers",
        "author": "Zhang, Yushun and Chen, Congliang and Ding, Tian and Li, Ziniu and Sun, Ruoyu and Luo, Zhi-Quan",
        "title": "{Why transformers need Adam: A Hessian perspective}"
      }
    ]
  },
  {
    "index": 23,
    "papers": [
      {
        "key": "constantine2015active",
        "author": "Constantine, Paul G",
        "title": "{Active subspaces: Emerging ideas for dimension reduction in parameter studies}"
      }
    ]
  },
  {
    "index": 24,
    "papers": [
      {
        "key": "cui2020active",
        "author": "Cui, Chunfeng and Zhang, Kaiqi and Daulbaev, Talgat and Gusak, Julia and Oseledets, Ivan and Zhang, Zheng",
        "title": "Active subspace of neural networks: Structural analysis and universal attacks"
      },
      {
        "key": "mallinar2024emergence",
        "author": "Mallinar, Neil and Beaglehole, Daniel and Zhu, Libin and Radhakrishnan, Adityanarayanan and Pandit, Parthe and Belkin, Mikhail",
        "title": "Emergence in non-neural models: grokking modular arithmetic via average gradient outer product"
      },
      {
        "key": "radhakrishnan2022mechanism",
        "author": "Radhakrishnan, Adityanarayanan and Beaglehole, Daniel and Pandit, Parthe and Belkin, Mikhail",
        "title": "Mechanism of feature learning in deep fully connected networks and kernel machines that recursively learn features"
      },
      {
        "key": "zhu2023catapults",
        "author": "Zhu, Libin and Liu, Chaoyue and Radhakrishnan, Adityanarayanan and Belkin, Mikhail",
        "title": "{Catapults in SGD: spikes in the training loss and their impact on generalization through feature learning}"
      }
    ]
  },
  {
    "index": 25,
    "papers": [
      {
        "key": "radhakrishnan2022mechanism",
        "author": "Radhakrishnan, Adityanarayanan and Beaglehole, Daniel and Pandit, Parthe and Belkin, Mikhail",
        "title": "Mechanism of feature learning in deep fully connected networks and kernel machines that recursively learn features"
      }
    ]
  },
  {
    "index": 26,
    "papers": [
      {
        "key": "mallinar2024emergence",
        "author": "Mallinar, Neil and Beaglehole, Daniel and Zhu, Libin and Radhakrishnan, Adityanarayanan and Pandit, Parthe and Belkin, Mikhail",
        "title": "Emergence in non-neural models: grokking modular arithmetic via average gradient outer product"
      },
      {
        "key": "radhakrishnan2022mechanism",
        "author": "Radhakrishnan, Adityanarayanan and Beaglehole, Daniel and Pandit, Parthe and Belkin, Mikhail",
        "title": "Mechanism of feature learning in deep fully connected networks and kernel machines that recursively learn features"
      }
    ]
  },
  {
    "index": 27,
    "papers": [
      {
        "key": "zhu2023catapults",
        "author": "Zhu, Libin and Liu, Chaoyue and Radhakrishnan, Adityanarayanan and Belkin, Mikhail",
        "title": "{Catapults in SGD: spikes in the training loss and their impact on generalization through feature learning}"
      }
    ]
  }
]