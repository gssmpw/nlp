\documentclass{article}
\usepackage[margin=1in]{geometry}

\usepackage{arxiv_custom_commands}

% Figure formatting packages
\usepackage{subcaption}


\usepackage{authblk}
\author[1]{Adela DePavia}
\author[2]{Vasileios Charisopoulos}
\author[1,2,3,4]{Rebecca Willett}
\affil[1]{Computational and Applied Mathematics, University of Chicago}
\affil[2]{Data Science Institute, University of Chicago}
\affil[3]{Department of Statistics, Department of Computer Science, University of Chicago}
\affil[4]{NSF-Simons National Institute for Theory and Mathematics in Biology}
\renewcommand{\Affilfont}{\fontsize{9}{10.8}\itshape}

\begin{document}

\title{Faster Adaptive Optimization via Expected Gradient Outer Product Reparameterization}

\maketitle
\begin{abstract}
    Adaptive optimization algorithms---such as Adagrad, Adam, and their variants---have found wide\-spread use in machine learning, signal processing and many other settings. Several methods in this family are not rotationally equivariant, meaning that simple reparameterizations  (i.e. change of basis) can drastically affect their convergence. However, their sensitivity to the choice of parameterization has not been systematically studied;  it is not clear how to identify a ``favorable'' change of basis in which these methods perform best. In this paper we propose a reparameterization method and demonstrate both theoretically and empirically its potential to improve their convergence behavior. Our method is an orthonormal transformation based on the \textit{expected gradient outer product} (EGOP) matrix, which can be approximated using either full-batch or stochastic gradient oracles. We show that for a broad class of functions, the sensitivity of adaptive algorithms to choice-of-basis is influenced by the decay of the EGOP matrix spectrum. We illustrate the potential impact of EGOP reparameterization by presenting empirical evidence and theoretical arguments that common machine learning tasks with ``natural'' data exhibit EGOP spectral decay.
\end{abstract}

% FIGURE NOTES %%%%%%%%%%%%%%%%%%%%%%%%%%
% This figure is the opener cartoon. It's placed here to get latex to render this where I want.
% FIGURE NOTES %%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}
    \centering
    % First subfigure
    \begin{subfigure}[c]{0.58\textwidth}
        \centering
        \includegraphics[width=\linewidth]{Images/final_figures/Adagrad_opener_cartoon_v4.pdf}
    \end{subfigure}
    \hspace{0.01\textwidth} % Optional space between subfigures
    % Second subfigure
    \begin{subfigure}[c]{0.38\textwidth} % Use a valid fraction like 0.4\textwidth
        \centering
        \includegraphics[width=\linewidth]{Images/final_figures/tinyMNIST_loss_vs_epochs_all_solvers.pdf}
    \end{subfigure}
    \caption{(Left) Visualization of optimizing a two-dimensional log-sum-exp objective  (\ref{eq:log-sum-exp-objective}) using Adagrad in both original coordinates and under EGOP reparameterization.  In the EGOP eigenbasis, the primary directions of function variation are axis-aligned. Experimental details in \cref{ssec:details-for-opener-cartoon}. (Right) Negative log-likelihood loss over epochs from training a 2-layer ReLU network in 2.4k dimensions to classify handwritten digits using Adam, Adagrad, SGD, and SGD with momentum, in both original coordinates and under reparameterization. Equivariant methods (e.g. SGD) exhibit no change under reparameterization. See discussion in Section~\ref{sec:experimental-results}.}\label{fig:opener-cartoon}
\end{figure*}

\section{Introduction}\label{sec:intro}

Adaptive optimization algorithms are popular methods in modern machine learning \cite{Crew_2020}. Optimizers in this family include the seminal Adagrad and Adam algorithms as well as variants such as AdamW, Adadelta, and Adamax \cite{kingma2017adammethodstochasticoptimization, loshchilov2019decoupledweightdecayregularization,reddi2019convergenceadam, zeiler2012adadeltaadaptivelearningrate}. These iterative algorithms are termed ``adaptive'' because they maintain and update different learning rates for each coordinate in parameter space.\footnote{In this work we focus on adaptive algorithms with diagonal pre-conditioners, which correspond to the standard implementation of Adagrad, Adam, and variants in popular software packges. Full-matrix Adagrad, proposed by \citet{duchi2011adaptive}, is equivariant but requires computing a matrix root on every iteration and is thus rarely used in practice. We also note that adaptive methods may use learning rates for blocks of coordinates, rather using one learning rate for each individual coordinate.} Despite their popularity, fully understanding the impact these adaptive learning rates have on convergence remains an area of ongoing research \cite{jiang2024convergence, liu2024adagrad, maes2024understanding}. Notably, the coordinate-wise learning rates make these methods sensitive to orthonormal reparameterization, distinguishing them from equivariant methods like gradient descent, which have the same convergence rate regardless of orthonormal reparameterization.

Orthonormal reparameterizations correspond to full-dimensional changes of basis, and include seemingly benign transformations such as rotations of the loss landscape about the origin. The sensitivity of adaptive algorithms to rotation means that changes of basis can affect the rate of convergence to local minima, and even impact the generalization properties of the obtained solutions in the presence of non-convex landscapes.

Given the sensitivity of these ubiquitous optimization methods to choice of basis, we pose the following research question:
\begin{myquote}
    \noindent\it{When using an adaptive algorithm, in what settings and to what extent can change-of-basis improve optimization?}
\end{myquote}
In this work, we address this research question by identifying geometric properties of loss functions that govern the sensitivity of adaptive algorithms to change-of-basis. We propose a reparameterization procedure based the \emph{expected gradient outer product} (EGOP) matrix, which captures these geometric properties, and show this reparameterization can improve convergence of adaptive methods. The geometric properties identified in this work---namely, strong decay of the EGOP eigenvalues---have been observed in a variety of machine learning objectives \cite{ papyan2018full,sagun2017empirical, zhang2024transformers}. We include both empirical evidence and theoretical arguments suggesting that these properties arise when using natural data.

\paragraph{Contributions}
We show that for a large class of objectives, the proposed reparameterization procedure can improve the convergence of adaptive optimization algorithms. Our main contributions are as follows:

\begin{itemize}
    \item We characterize a class of objective functions for which reparameterization can reduce the number of iterations required for adaptive algorithms to converge to first-order stationary points. 
    \item For these functions, we identify a choice of basis in which adaptive algorithms will perform well, and we propose an approximation procedure that only requires access to a (stochastic) gradient oracle, rather than analytical knowledge about the loss function. This procedure is defined in Section~\ref{sec:EGOP-defn}.
    \item We develop theory that proves the proposed reparameterization endows adaptive algorithms with improved convergence guarantees, quantified in terms of the spectrum of the EGOP matrix. Our main results are discussed in Section~\ref{sec:convergence-analysis}.
    \item We empirically examine the performance of this procedure and find that the proposed reparameterization improves the convergence of both Adagrad and Adam, for convex and nonconvex objective functions,
    and machine learning loss functions corresponding to both natural and synthetic data. We present empirical results in Section~\ref{sec:experimental-results}.
\end{itemize}
\subsection{Related Work}\label{ssec:related-work}

Our work intersects with research on theoretical guarantees for adaptive algorithms, the role of orthogonal rotations in algorithmic performance, and the geometry of machine learning objectives with natural data. Here we present a concise overview of relevant works, and include an expanded discussion in Section~\ref{sec:extended-related-works} of the supplementary material. 

\paragraph{Geometric Sensitivity of Adaptive Methods} A large body of research has been devoted to understanding when adaptive algorithms' per-coordinate learning rates confer a benefit over (stochastic) gradient descent ((S)GD). Recently, there has been renewed interest in distinguishing the properties of adaptive algorithms versus SGD, as several empirical studies suggest that adaptive methods outperform SGD when training transformer models \cite{kunstner2024heavy,zhang2024transformers}. Traditional analyses of adaptive algorithms establish regret bounds for online convex optimization \cite{duchi2011adaptive, hazan2016introduction}. More recent work establishes convergence rates for smooth, non-convex objectives in terms of the $\ell_2$ norm of the gradient \cite{defossez2020simple,ward2020adagrad}. However, because SGD is known to have optimal convergence rates in these settings, these theoretical results only show that adaptive algorithms achieve rates matching those of SGD.

In order to understand when adaptive algorithms enjoy provably stronger guarantees than SGD, a recent line of work studies convergence under refined geometric assumptions, with particular emphasis on assumptions that are \textit{not} rotationally invariant \cite{jiang2024convergence, liu2024adagrad, xie2024adamexploitsellinftygeometryloss}. \citet{xie2024adamexploitsellinftygeometryloss} establish convergence guarantees in terms of the $\ell_{\infty}$ smoothness constant of the objective and show experimentally that rotationally invariant geometric assumptions do not suffice to capture settings when Adam out-performs SGD. \citet{jiang2024convergence} and \citet{liu2024adagrad} study convergence of Adagrad on objectives that are \textit{coordinate-wise smooth}, defined in~\cref{ssec:notation}. Both works prove similar convergence guarantees for Adagrad; they measure convergence with the gradient $\ell_1$ norm, rather than $\ell_2$, because in this regime they can prove a gap between convergence guarantees for Adagrad versus SGD that is consistent with the advantage exhibited by adaptive methods in certain settings \cite{jiang2024convergence}.
\citet{jiang2024convergence} show that when objective functions exhibit certain geometric properties, Adagrad enjoys $\ell_1$ convergence upper bounds that are lower than corresponding lower bounds on SGD's convergence by a factor of $d$ \cite{jiang2024convergence}. Our analysis builds on that of \citet{jiang2024convergence} and \citet{liu2024adagrad}; they show that the sum of the coordinate-wise smoothness constants governs Adagrad convergence, and we prove that EGOP reparameterization decreases this value by a factor as large as $1/d$.

\paragraph{Change-of-Basis for Adaptive Algorithms} Recent works propose that using different orthonormal transformations with Adam and its variants can reduce computational costs and improve performance in neural network training \cite{maes2024understanding,vyas2024soap,  zhao2024galore}. \citet{gupta2018shampoo} introduced Shampoo, an efficient preconditioning method for optimizing tensor spaces. \citet{vyas2024soap} formalized a connection between Shampoo and a variant of Adagrad, leading to a new proposed method called SOAP. Designed for training neural networks, SOAP computes an orthonormal reparameterization based on the singular vectors of the matrix-valued network gradients and performs optimization in this basis. \citet{vyas2024soap} empirically examine the performance of SOAP and find that it outperforms both Adam and Shampoo in LLM pre-training. \citet{zhao2024galore} propose GaLore, a method that simultaneously performs reparameterization and dimensionality reduction. GaLore computes a similar orthogonal basis to that used in SOAP, but instead of a full-dimensional change-of-basis GaLore retains only leading basis vectors in order to reduce dimension \cite{zhao2024galore}. \citet{maes2024understanding} empirically study Adam's rotational sensitivity and examine the power of existing geometric assumptions (such as those leveraged by \citet{xie2024adamexploitsellinftygeometryloss}) to explain Adam's performance when training transformer architectures. They propose an orthonormal reparameterization, similar to those used by SOAP and GaLore, and show empirically that this can improve Adam's performance \cite{maes2024understanding}. 

SOAP and GaLore are empirically powerful tools, but their convergence properties are not fully understood. The results in this work may lead to new ways to analyze the performance of SOAP and GaLore. See \cref{sec:compare-w-SOAP-Galore} for further discussion of the main distinctions between our proposed reparameterization methods and SOAP/GaLore.

Outside of training neural networks, several works have considered data-driven dimensionality reduction methods for optimizing more general objectives with low-rank EGOP matrices \cite{cartis2024learning, cosson2023low}. These procedures target objectives with exact low-rank structure, while our method can improve convergence of adaptive algorithms even when the EGOP matrix has strong spectral decay but is full-rank.

\paragraph{EGOP Structure in Machine Learning}
    Increasing empirical evidence across a wide range of applications, including language modeling and image classification, suggests that empirical loss functions used for training machine learning models are approximately low-rank---meaning that the functions vary strongly in only a small subset of directions in parameter space \citet{papyan2018full,sagun2017empirical,  zhang2024transformers}. Such approximate low-rank structure can be detected and analyzed using active subspace methods. Many active subspace methods leverage the EGOP matrix (sometimes referred to as the average gradient outer product (AGOP) matrix), which we define in Section~\ref{sec:EGOP-defn}  \cite{constantine2015active}. Recently, there has been growing interest in the EGOP and in the use of active subspace methods in machine learning research \cite{cui2020active, mallinar2024emergence,radhakrishnan2022mechanism,zhu2023catapults}. \citet{radhakrishnan2022mechanism} provide theoretical and empirical evidence that the weight matrices of deep neural networks trained with gradient descent correlate strongly with a kind of EGOP matrix.\footnote{In \citet{mallinar2024emergence,radhakrishnan2022mechanism} and \citet{zhu2023catapults}, the EGOP is defined using the gradient with respect to the \emph{input data} instead of the optimization parameters.}

\subsection{Notation}\label{ssec:notation}

For a vector $\theta \in \R^d$, we denote its $i$\ts{th} entry by $\theta(i)$. We denote the inner product on $\R^d$ by $\langle \cdot, \cdot \rangle$, and let $\norm{\cdot}_p$ denote the vector $p$-norm on $\R^d$, with $\norm{\theta}_{\infty} \defeq \max_{i}\abs{\theta(i)}$. Given a matrix $A \in \R^{m \times n}$, we write $\frobnorm{A}$ for its \emph{Frobenius} norm and $\opnorm{A} \defeq \sup_{\norm{\theta}_2=1} \norm{A\theta}_2$ for its \emph{spectral} norm. Given a PSD matrix $H\in \R^{d\times d}$, we denote the norm $\norm{\theta}_{H} \defeq \sqrt{\ip{\theta, H\theta}}$. For a matrix $A \in \R^{n\times m}$, we denote the vectorization of $A$ by  $\operatorname{vec}(A) \in \R^{nm}$.

We obtain guarantees in terms of the \textit{coordinate-wise smoothness constants} of the objective $f(\cdot)$. Following \citet{jiang2024convergence} and \citet{liu2024adagrad}, we say that a function $f$ satisfies coordinate wise smoothness with constants $L_1,\dots,L_d > 0$ if the following holds for all
$\theta_1, \theta_2 \in \R^d$: 
\begin{equation}\label{eq:def-coordinate-wise-smoothness}
    \abs{f(\theta_1)-f(\theta_2) - \ip{\nabla f(\theta_2), \theta_1 - \theta_2}} \leq
    \frac{1}{2} \norm{\theta_1 - \theta_2}_{L}^2,
\end{equation}
where $L = \diag(L_1, \dots, L_{d})$. We will denote by $L_f$ the sum of these constants for a particular function $f(\cdot)$: $L_f\defeq \sum_{i=1}^d L_i$.

\section{EGOP Reparameterization}\label{sec:EGOP-defn}

    Given a function $f:\R^d\rightarrow \R$ and a sampling distribution $\rho$, the expected gradient outer product of $f(\cdot)$ with respect to $\rho$ is defined as
    \[
        \EGOP(f) \defeq \mathbb{E}_{\theta\sim \rho}\left[\nabla f(\theta)\nabla f(\theta)^{\T}\right].
    \]
    As $f:\R^d\rightarrow \R$, the EGOP is a $d\times d$ symmetric matrix, and it is positive semi-definite because it is an expectation over PSD matrices. We denote its eigendecomposition by
    \[
        \EGOP(f) = V \Lambda V^{\T}
    \]
    where $V\in \R^{d\times d}$ is an orthonormal matrix. The EGOP eigenbasis captures key geometric qualities of the function $f(\cdot)$. When the sampling distribution $\rho$ is isotropic, the leading eigenvectors of the EGOP matrix capture the directions of greatest variation in $f(\cdot)$, whereas the eigenspaces of the smallest eigenvalues are directions along which $f(\cdot)$ does not vary strongly: for any eigenpair
    $(\lambda_i, v_i)$ of $\EGOP(f)$,
    \begin{equation}\label{eq:constantine-eigvecs-identity}
        \lambda_i = \mathbb{E}_{\theta\sim\rho}[\langle \nabla f(\theta), v_i\rangle^2 ].
    \end{equation}
    In this work, we compare how adaptive optimization algorithms perform when optimizing $f(\cdot)$ versus the reparameterized function $\tf:\R^d\rightarrow \R$ defined $\tf(\ttheta)\defeq f(V\ttheta)$. For any $f$, the objective $\tf(\cdot)$ can be approximated by empirically estimating the EGOP via Monte Carlo sampling and forming the eigenvectors of the empirical EGOP matrix, as summarized in~\cref{alg:meta-algorithm-block}.

    \begin{algorithm}[h]
       \caption{Reparameterization by EGOP Eigenbasis}
       \label{alg:meta-algorithm-block}
    \begin{algorithmic}
       \STATE {\bfseries Input:} $M$ number of gradient samples, distribution $\rho$.
       \STATE Generate $\{\theta_i\}_{i=1}^M \sim \rho$ i.i.d.
       \STATE Form empirical EGOP $\hat{P} = \frac{1}{M}\sum_{i=1}^M \nabla f(\theta_i)\nabla f(\theta_i)^\T$
       \STATE Form eigendecomposition $V\Lambda V^\T = \hat{P}$
       \STATE Define function $\tilde{f}(\cdot) = f\circ V$
       \STATE Optimize $\tf(\cdot)$ with adaptive algorithm of choice.
       \STATE {\bfseries Output:} $\ttheta \in \R^d$ the result of optimizing $\tf(\cdot)$.
    \end{algorithmic}
    \end{algorithm}

    Note that as $V$ is orthogonal, any solution $\ttheta$ obtained by optimizing $\tf(\cdot)$ can be transformed into a solution in original coordinates as $\theta \defeq
    V \ttheta$, which satisfies $f(\theta) = \tf(\ttheta)$.

    \cref{alg:meta-algorithm-block} requires a user-specified sampling distribution $\rho$. Our guarantees in  Section~\ref{sec:convergence-analysis} require that $\rho$ be isotropic and that its scale is large enough with respect to the norm of some local minimum of $f(\cdot)$. In Section~\ref{sec:experimental-results}, we present empirical results when the EGOP is estimated with $\rho$ a standard Gaussian, and when $\rho$ is defined by common neural network initialization distributions \cite{glorot2010understanding}.
    
\section{Convergence Guarantees under EGOP Reparameterization}\label{sec:convergence-analysis}

In this section, we show that for objectives with strong EGOP spectral decay, reparameterization by the EGOP eigenbasis can improve convergence guarantees for adaptive methods. We consider convergence guarantees for Adagrad, and relate the improvement obtained through reparameterization by the EGOP eigenbasis to the \textit{stable rank}\footnote{We call this quantity the stable rank because of the connection to the stable rank considered in numerical linear algebra; the ratio in (\ref{eq:def-stable-rank}) is related to $\norm{G}_*/\opnorm{G}$, where $G$ denotes the empirical gradient bundle matrix $[\nabla f(\theta_1),\dots,\nabla f(\theta_M)]\in \R^{d\times M}$ and $\norm{\cdot}_*$ denotes the nuclear norm. This ratio is often referred to in literature as the \textit{stable} or \textit{effective rank} of $G$ \cite{chou2024gradient,rudelson2007sampling}.} of $f$:
\begin{equation}\label{eq:def-stable-rank}
    {\sr}_f \defeq \frac{\sum_{i=1}^d \sqrt{\lambda_i(\EGOP(f))}}{\sqrt{\lambda_{\max}(\EGOP(f))}}.
\end{equation}
Functions with strong EGOP spectral decay will have constant stable rank (tending towards $1$ as spectral decay increases), while functions without EGOP spectral decay will have a stable rank that scales with $d$.

We now introduce the main assumptions for our theoretical results. Setting the stage,
we consider a twice-differentiable objective $f:\R^d\rightarrow \R$, a sampling distribution $\rho(\cdot)$ and fix a local minimum $\theta^*$.

\begin{assumption}\label{assumption:sampling-distribution}
    The sampling distribution $\rho(\cdot)$ is mean-zero, isotropic, and sufficiently ``spread out''; i.e.,
    \[
        \mathbb{E}_{\theta\sim \rho}[\theta \theta^\T] = c^2 \mathbb{I} \quad \text{ for some }\quad c \geq \norm{\theta^{\ast}}.
    \]
\end{assumption}

\begin{assumption}\label{assume:Lipschitz-Hessian}
    The Hessian of $f(\cdot)$ is $H$-Lipschitz:
    \[
        \opnorm{\nabla^2 f(\theta_1)-\nabla^2 f(\theta_2)} \leq H \norm{\theta_1-\theta_2}, \; \text{for all} \; \theta_1, \theta_2.
    \]
\end{assumption}

Under the above assumptions, we show that EGOP spectral decay governs the improvements conferred by EGOP reparameterization.
\begin{theorem}[Informal]\label{thm:compare-convergence-guarantees}
    Consider a function $f:\R^d\rightarrow \R$ and a sampling distribution $\rho$ satisfying Assumptions~\ref{assumption:sampling-distribution} and ~\ref{assume:Lipschitz-Hessian}. Let $\Delta_f \defeq f(\theta_0) - \inf_{\theta\in \R^d} f(\theta)$ for some initialization point $\theta_0$, and assume $\Delta_f$ is finite. Consider optimizing $f(\cdot)$ with Adagrad using step size $\eta$. Let $\{\ttheta_t\}_{t=1}^T$ denote the iterates produced by optimizing the reparameterizing objective $\tf \defeq f\circ V$ using Adagrad initialized at $V^\T \theta_0$, where $V$ denotes the eigenbasis of $\EGOP(f)$. Let $\beta \defeq \shortnorm{v_1}^2_1/d$ for $v_1$ the leading EGOP eigenvector. Then if the value $H$ in Assumption~\ref{assume:Lipschitz-Hessian} is sufficiently small, reparameterized Adagrad convergence is bounded by
    \[
        \frac{1}{T} \sum_{t=1}^T \norm{\nabla \tf(\ttheta_t)}_1 = \tilde{O}\left(\frac{\sr_f}{\beta d}\cdot \frac{\eta L_f}{\sqrt{T}} + \frac{\Delta_f}{\eta\sqrt{T}}\right)
    \]
    where $\sr_f$ denotes the stable rank (\ref{eq:def-stable-rank}). 
\end{theorem}
We compare the above bound with the convergence guarantee for Adagrad in original coordinates initialized at $\theta_0$:
    \[
        \frac{1}{T} \sum_{t=1}^T \norm{\nabla f(\theta_t)}_1 = \tilde{O}\left(\frac{\eta L_f}{\sqrt{T}} + \frac{\Delta_f}{\eta\sqrt{T}}\right)
    \]
where $\{\theta_t\}_{t=1}^T$ denote the iterates produced by the algorithm in original coordinates, and the term $L_f$ appearing in both this bound and in \cref{thm:compare-convergence-guarantees} denotes the sum of the coordinate-wise smoothness constants of $f(\cdot)$ in original coordinates \cite{jiang2024convergence, liu2024adagrad}.

When $f(\cdot)$ has strong EGOP spectral decay and dense leading eigenvectors, \cref{thm:compare-convergence-guarantees} implies the convergence bounds for reparameterized Adagrad are much stronger: in these settings, $\sr_f/(\beta d) = O(1/d)$. We defer the proof of \cref{thm:compare-convergence-guarantees} to \cref{sec:proof-of-main-result}. We show that the two properties of the EGOP emphasized in this result, namely low stable rank and dense leading eigenvectors, are satisfied empirically in Section~\ref{sec:EGOP-spectral-decay}. Many naturally-motivated objectives in machine learning satisfy Assumption~\ref{assume:Lipschitz-Hessian}, including loss functions used in logistic regression, over-parameterized matrix factorization, and training of multilayer linear networks. We bound the Lipschitz constants of the Hessians of these objectives in \cref{ssec:ML-lipschitz-Hessians}.

Our guarantee in \cref{thm:compare-convergence-guarantees} holds for any step size. If one chooses the step size $\eta$ optimally in order to minimize the upper bounds, the convergence guarantee for the reparameterized iterates becomes $\tilde{O}\left(\sqrt{\frac{\sr_f}{\beta d}\cdot \frac{L_f \Delta_f}{ T}}\right)$, while the convergence bound in original coordinates becomes $\tilde{O}\left(\sqrt{\frac{L_f \Delta_f}{T}}\right)$. This implies that under such choice of $\eta$, reparameterization offers improvement by a factor of $\sqrt{\sr_f/\beta d}$. However, in practice users must select learning rates without a-priori knowledge of the smoothness constants. In these settings $\eta$ is typically set through hyperparameter tuning. \Cref{thm:compare-convergence-guarantees} implies that the optimal step size for reparameterized Adagrad is larger than that of Adagrad in original coordinates by a factor of $\sqrt{\beta d/\sr_f}$. \Cref{thm:compare-convergence-guarantees} also implies that  for large values of $\eta$, reparameterization offers improvement by a factor as large as $\sr_f/(\beta d)$, which agrees with our empirical results in Section~\ref{sec:experimental-results} showing that reparameterized methods perform even better with larger step sizes (see e.g. \cref{fig:linear-layers-global-reparam-loss-vs-LR}). 

The factor $\beta = \norm{v_1}^2_1/d$ tends towards $1$ as the leading EGOP eigenvector gets denser. For the guarantee to reflect a benefit from reparameterization, it suffices to have $\beta \gg 1/d$ and small stable rank. In Section~\ref{sec:EGOP-spectral-decay}, we empirically verify this property on benchmark image datasets.
For simplicity, the guarantee in \cref{thm:compare-convergence-guarantees} only considers the density of $v_1$, but more general guarantees can be obtained in terms of the measure $\max_k \max_{\nu \in \{\pm 1\}^d}\lambda_k \sum_{i=1}^d \langle \nu, v_k\rangle^2/d$. Similarly, one can derive generalizations of~\cref{thm:compare-convergence-guarantees} for approximate versions of the EGOP eigenbasis; such guarantees will scale with the subspace distance between the EGOP eigenbasis and its approximation.

We measure convergence with respect to the $\ell_1$ gradient norm instead of the $\ell_2$ norm\footnote{We note that as the $\ell_2$ norm is upper-bounded by the $\ell_1$ norm, the guarantees in \cref{thm:compare-convergence-guarantees} also imply stronger convergence guarantees under reparameterization in terms of the $\ell_2$ gradient norm measure.} because this is the measure by which convergence guarantees for adaptive algorithms can be provably separated from those for SGD \cite{jiang2024convergence}. 


\section{EGOP Spectral Decay in Machine Learning}\label{sec:EGOP-spectral-decay}

    Our analysis in Section~\ref{sec:convergence-analysis} shows EGOP spectral decay and dense leading EGOP eigenvectors are sufficient for EGOP reparameterization to improve convergence guarantees for Adagrad. Here we present empirical evidence that these conditions occur in benchmark machine learning objectives, and we discuss theoretical reasons for why natural data results in EGOP spectral decay in real-world problems.

    Figure~\ref{fig:tinyMNIST-global-spectral-decay} shows the EGOP eigenspectrum for the objective function $f(\cdot)$ from an image classification problem. We use a 2-layer ReLU neural network to predict 10-class probabilities for handwritten digit images from the UCI ML digits dataset (\citet{optical_recognition_of_handwritten_digits_80}). Here $f(\cdot)$ denotes the negative log-likelihood loss on the training data. Figure~\ref{fig:tinyMNIST-global-spectral-decay} shows strong EGOP spectral decay. We plot $\lambda_k/\lambda_1$ for all EGOP eigenvalues $\lambda_k$ on a logarithmic scale, while the inset zooms in on the leading 100 indices on a linear scale. These plots illustrate roughly exponential eigenvalue decay in regions of the spectrum.
    
    In \cref{ssec:sup-figs-spectral-decay}, we show that these trends are robust to choice of sampling distribution (see Figure~\ref{fig:tinyMNIST-compare-spectral-decay}). In this appendix we also visualize the eigenspectra of the layer block EGOP matrices, defined in Section~\ref{ssec:efficient-heuristics}, for neural networks on both the UCI digits dataset and a subset of the fashionMNIST dataset (see Figure~\ref{fig:layer-by-layer-spectra}) \cite{optical_recognition_of_handwritten_digits_80, xiao2017fashion}. We observe that the spectral decay illustrated in Fig.~\ref{fig:tinyMNIST-global-spectral-decay} persists when considering block EGOP matrices instead of global EGOP matrices, and that this spectral decay occurs across different datasets.

     In \cref{ssec:sup-figs-spectral-decay}, we examine the density of the leading EGOP eigenvectors for a 2-layer ReLU network on the UCI digits dataset. We show that $\beta \geq 0.5$, while for this network the number of parameters $d$ satisfies $1/d < 5 \cdot 10^{-4}$ and thus  $\beta \gg1/d$, and we we show that several of the leading eigenvectors exhibit similar density (see \cref{fig:MNIST-global-eigvec-density}).

%%%%%%%%%%%%%%%%%%%%%%%
%% This figure is for the section on EGOP spectral decay, I'm placing it here so Latex renders it where I want.
\begin{figure}[t!]    \centering\includegraphics[width=0.55\linewidth]{Images/final_figures/tinyMNIST_global_spectrum.pdf}
    \caption{The EGOP eigenspectrum of a 2-layer ReLU network on the UCI handwritten digits dataset. Plot shows ratio $\lambda_k/\lambda_1$ as a function of eigenvalue index $k$, indexed in decreasing order.}
    \label{fig:tinyMNIST-global-spectral-decay}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%
    
    \subsection{Natural Data Induces EGOP Spectral Decay}

    In addition to empirical evidence, simple gradient calculations suggest that natural data may induce EGOP spectral decay in machine learning problems. Many objectives in machine learning can be expressed as $f(\theta) = h(A\theta)$, where $h(\cdot)$ is a loss function and $A \in \R^{n\times d}$ is a data matrix whose rows comprise samples $a_i \in \R^d$. By the chain rule, the EGOP for such objectives satisfies
    \begin{equation}
        \label{eq:EGOP-chain-rule-main}
        \EGOP(f) = A^\T \mathbb{E}_{\theta\sim \rho}\left[\nabla_\theta h(A\theta) \nabla_\theta h(A\theta)^\T\right] A
    \end{equation}
    where $\nabla_{\theta}h(A\theta)$ denotes the gradient $\nabla h(\cdot)$ evaluated at $A\theta$. The above expression shows the EGOP is formed by transforming some PSD matrix $M$ by $A^\T M A$. It suggests that if $A$ has strong spectral decay, this may induce eigenvalue decay in the EGOP matrix. Indeed for many naturally occurring data distributions, the singular values of $A$ exhibit significant decay \cite{udell2019big}.
    
    The inner PSD matrix in the right hand side of Eq.~\ref{eq:EGOP-chain-rule-main} depends on both $A$ and $h(\cdot)$, so without further assumptions on $h(\cdot)$ it is difficult to precisely characterize the  spectral decay induced by the composition with $A^\T$ and $A$. However, the spectral decay induced by $A$ can be quantified for specific choices of $h(\cdot)$; see~\cref{ssec:spectral-decay-proofs} for some examples.

    We note that the density condition $\beta \gg 1/d$ is satisfied with high probability for random unit vectors in high dimensions: in particular, for $v\sim\textrm{Unif}(S^{d-1})$ where $S^{d-1}$ is the unit sphere in $\R^d$, with high probability $\norm{v}^2_1/d \approx 2/\pi > 0.6$.

% FIGURE NOTES %%%%%%%%%%%%%%%%%%%%%%%%%%
% This figure is for the empirical results section. It's placed here to get latex to render this where I want.
% FIGURE NOTES %%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}
    \centering
    % First subfigure
    \begin{subfigure}[t]{0.325\textwidth}
        \centering
        \includegraphics[width=\linewidth]{Images/final_figures/global_loss_vs_epochs_all_solvers_no_legend.pdf}
        \caption{Loss over Epochs}
        \label{fig:linear-layers-global-reparam-loss-vs-epochs}
    \end{subfigure}
    %\hspace{0.05\textwidth} % Optional space between subfigures
    % Second subfigure
    \begin{subfigure}[t]{0.32\textwidth} % Use a valid fraction like 0.4\textwidth
        \centering
        \includegraphics[width=\linewidth]{Images/final_figures/global_final_train_loss_by_LR.pdf}
        \caption{Training Loss over Learning Rates}
        \label{fig:linear-layers-global-reparam-loss-vs-LR}
    \end{subfigure}
    % Third subfigure
    \begin{subfigure}[t]{0.32\textwidth} % Use a valid fraction like 0.4\textwidth
        \centering
        \includegraphics[width=\linewidth]{Images/final_figures/global_min_valloss_by_LR_no_legend.pdf}
        \caption{Validation Loss over Learning Rates}
        \label{fig:linear-layers-global-reparam-valloss-vs-LR}
    \end{subfigure}
    \caption{Training multilayer linear networks (\ref{eq:linear-feedforward-objective}). Both SGD and SGD with momentum are equivariant optimization methods, so their results in original and reparameterized coordinates are exactly superimposed. In \cref{fig:linear-layers-global-reparam-valloss-vs-LR} we consider the minimum validation loss achieved over epochs during training. Results are aggregated over 10 independent trials, with traces showing medians and shading indicating 25th-75th quartile. Asterisks indicate the learning rate used for each method in \cref{fig:linear-layers-global-reparam-loss-vs-epochs}. Learning rates chosen to minimize validation loss of the algorithm in \textit{original} coordinates.
    }
    \label{fig:global-reparam-linear-layers}
\end{figure*}

\section{Heuristics for Scalability}\label{ssec:efficient-heuristics}

Reparameterization with the EGOP eigenbasis incurs three main sources of additional computation: (1) sampling gradients to estimate the EGOP, (2) forming the EGOP eigenbasis, and (3) storing and applying the change-of-basis matrix to compute values and gradients of $f\circ V$. We outline some implementation details and heuristics that reduce the computational cost and enhance the scalability of the proposed framework. For a more detailed discussion, see \cref{sec:expanded-heuristics-discussion}.

We hypothesize that for functions with strong spectral decay, it suffices to accurately estimate  only the leading EGOP eigenvectors. The remaining columns of the matrix $V$ can be completed using any random orthonormal basis for the orthogonal complement of the leading eigenspace. Based on this hypothesis, one can use techniques like randomized SVD \cite{halko2011finding} to form $V_k \in \R^{d\times k}$, a matrix whose columns contain the estimated $k$ leading eigenvectors of the EGOP, and then append any random orthonormal basis for the complement to produce $V\in \R^{d\times d}$. Alternatively, one can employ a partial basis instead of a full orthonormal basis. GaLore, discussed in~\cref{sec:extended-related-works}, uses such a partial basis but requires periodic re-computation of the basis in order to achieve good performance~\cite{zhao2024galore} We also note that for functions with strong spectral decay, a conservative number of gradient samples ($M \leq 2d$, where $M$ is the number of samples and $d$ is the number of problem parameters) is empirically sufficient to yield change-of-basis matrices.

The ``partial basis'' strategy outlined above is one of several possible ways to reduce the cost of storing and/or applying a change of basis. Other heuristics include structured or factorized approximations of $V$. One can group the optimization variables in the vector $\theta$ into subsets $\{S_i\}_{i=1}^L$ and perform \textit{block reparameterization}. For each subset, we obtain a separate change-of-basis matrix $V^{(i)} \in \R^{\abs{S_i} \times \abs{S_i}}$ via the eigenvectors of the \emph{block} EGOP matrix,
%\vspace{-5mm}
\begin{equation}
    \EGOP^{(i)} \defeq \frac{1}{M}\sum_{k=1}^M \nabla_{S_i} f(\theta_k)\nabla_{S_i} f(\theta_k)^\T
    \label{eq:block-egop}
\end{equation}
where $\nabla_{S_i} f(\theta_k) \in \R^{|S_i|}$ is the vector of partial derivatives of $f$ w.r.t. the entries in $S_i$, and the points $\{\theta_k\}_{k=1}^M \overset{\mathrm{i.i.d.}}{\sim} \rho$. Block reparameterization is well-suited to multilayer neural networks, where each layer forms a parameter block; further savings can be obtained by only reparameterizing a subset of blocks (see~\cref{sec:experimental-results}). Another approach is a Kronecker-product factorization, where one finds matrices $V_1 \in \R^{m_1\times n_1}, V_2 \in \R^{m_2 \times n_2}$ for $m_1 m_2 = n_1 n_2 = d$ such that $V \approx V_1\otimes V_2$. This reduces the storage cost from $d^2$ to $m_1 n_1 + m_2 n_2$, and is similar to the method proposed by \citet{vyas2024soap}.

\section{Experimental Results}\label{sec:experimental-results}

We examine the impact of EGOP reparameterization in a variety of settings. We compare how the seminal adaptive algorithms, Adagrad and Adam, perform when optimizing functions in original coordinates versus under reparameterization by the empirical EGOP eigenbasis, both as described in Algorithm~\ref{alg:meta-algorithm-block} and with the block reparameterization described in Section~\ref{ssec:efficient-heuristics}. We include comparisons to equivariant iterative optimization methods, namely (stochastic) gradient descent and (S)GD with momentum. When comparing methods, we always choose equivalent initializations: when a method in original coordinates is initialized at $\theta_0$, chosen randomly, the reparameterized method is initialized at $V^\T\theta_0$. Full experimental details are in \cref{sec:experimental-details}.

\paragraph{Linear Feedforward Networks} We examine the impact of EGOP reparameterization on training 3-layer fully-connected linear feedforward networks using synthetic data. We consider parameters 
\[
    \theta = [\textrm{vec}(W_1), \textrm{vec}(W_2), \textrm{vec}(W_3)]
\]
where $W_1 \in \R^{50\times 10}$, $W_2 \in \R^{30\times 50}$, $W_3 \in \R^{10\times 30}$. We train by minimizing loss function
\begin{equation}\label{eq:linear-feedforward-objective}
    f(\theta) = \frobnorm{W_3 W_2 W_1 A - Y}^2 /\nsamples
\end{equation}
where $A\in \R^{10\times \nsamples}$, and $Y = M^* A$ for $M^*\in \R^{10\times 10}$ drawn from a standard Gaussian distribution. We induce spectral decay in $\EGOP(f)$ by generating $A$ with singular values $\sigma_k(A) = k^{-2}$ and random singular vectors. We use minibatched stochastic gradient samples throughout. Full experimental details are deferred to \cref{sec:experimental-details}. 

In Figure~\ref{fig:global-reparam-linear-layers} we study the impact of global EGOP reparameterization (Algorithm~\ref{alg:meta-algorithm-block}). \cref{fig:linear-layers-global-reparam-loss-vs-epochs} shows training loss over epochs in original coordinates and under reparameterization. Reparameterized Adagrad and reparameterized Adam achieve significantly lower final training loss and faster convergence than their counterparts in original coordinates. The adaptive methods also outperform the equivariant methods (SGD and SGD with momentum) in both reparameterized and original coordinates.

% FIGURE NOTES %%%%%%%%%%%%%%%%%%%%%%%%%%
% This figure is for the empirical results section. It's placed here to get latex to render this where I want.
% FIGURE NOTES %%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}
    \centering
    \includegraphics[width=0.5\linewidth]{Images/final_figures/fashionMNIST_loss_vs_epochs_all_solvers.pdf}
    \caption{Block EGOP reparameterization on fashionMNIST. Results are aggregated over independent trials corresponding to different random initializations. Medians are plotted as traces, and shaded regions indicate the 25\ts{th}-75\ts{th} percentiles. Each algorithm (Adagrad, Adam, etc) uses the same learning rate for both coordinate systems. Full details in \cref{sec:experimental-details}.
    } \label{fig:fashionMNIST}
\end{figure}

\cref{fig:linear-layers-global-reparam-loss-vs-LR} demonstrates the robustness of these results across learning rates. Asterisks along the x-axis indicate the learning rates used in \cref{fig:linear-layers-global-reparam-loss-vs-epochs}. \cref{fig:linear-layers-global-reparam-valloss-vs-LR} confirms that the improved minimization of the training loss enabled by reparameterization does not lead to over-fitting. The figure plots validation loss achieved as a function of learning rate, and shows that reparameterization enables adaptive methods to achieve better performance on validation data.

In \cref{ssec:sup-figs-main-experiments}, we compare the above results to those obtained when performing block reparameterization, as described in Section~\ref{ssec:efficient-heuristics}. We also visualize the impact of reparameterizing only the first layer of the network, corresponding to the parameters in $W_1$ in Eq.~\ref{eq:linear-feedforward-objective}. We find that both block-reparameterized Adagrad and Adam achieve lower final training loss than their counterparts in original coordinates. For Adagrad, reparameterizing only the first layer confers a benefit comparable to block reparameterizing all layers, while for Adam reparameterizing the first layer alone provides only a marginal benefit over original coordinates.

\paragraph{ReLU Networks for Image Classification} We examine the impact of EGOP reparameterization when training networks to perform image classification using real-world data. We consider two benchmark image classification datasets: the UCI hand-written digits dataset ($8\times 8$ pixel images), and the fashionMNIST dataset ($28\times 28$ pixel images of clothing) \cite{optical_recognition_of_handwritten_digits_80,xiao2017fashion}. For fashionMNIST, we down-sample images to $14\times 14$ pixels and restrict our dataset to samples from the first four classes; we do this in order to reduce the problem size to a setting where we can implement procedures most consistent with our theory. We train multilayer fully-connected ReLU networks to perform image classification on each dataset by minimizing the cross-entropy loss. We perform block EGOP reparameterization, as described in Section~\ref{ssec:efficient-heuristics}. We use stochastic minibatch gradients throughout. For these experiments, the number of gradient samples drawn equals the number of model weight parameters. For full experimental details, see~\cref{sec:experimental-details}.

\cref{fig:opener-cartoon} (right) and \cref{fig:fashionMNIST} plot training loss by epoch for the UCI digits dataset and fashionMNIST respectively. On both datasets, the reparameterized adaptive methods out-perform their counterparts in original coordinates, and reparameterized Adam achieves the best performance. We note that all methods achieve comparable accuracy on held-out data: median validation accuracy for all methods ranges between 95-96\% for the digits dataset, and between 93-94\% for fashionMNIST. This indicates that the improved training produced by reparameterization did not lead to over-fitting. In \cref{ssec:sup-figs-main-experiments}, we report test accuracy and show that these results are robust to choice of learning rate.

\paragraph{Convex Objectives} In addition to training neural networks, which is a primary application of adaptive optimization algorithms, we also study reparameterization for convex optimization. \cref{fig:log-sum-exp} shows the result of minimizing
\begin{equation}\label{eq:log-sum-exp-objective}
    f(\theta) = \log\left(\sum_{i=1}^n\exp\left((\langle a_i, \theta\rangle -y_i)^2\right)\right),
\end{equation}
where $a_i\in \R^d$ denote vectors of observations and $y_i \defeq \langle a_i, \theta^*\rangle$ for ground truth $\theta^*$. We also plot results from minimizing logistic regression objectives (\cref{fig:log-reg}) and linear least-squares objectives (\cref{fig:linear-least-squares}) arising from problems with data matrices $A\in \R^{n\times d}$. For all convex objectives, we induce EGOP spectral decay by choosing matrices $A$ with singular value decay. For these objectives, we estimate the EGOP and perform optimization using noiseless (full-batch) gradients. %For full experimental details, see~\cref{sec:experimental-details}.

% FIGURE NOTES %%%%%%%%%%%%%%%%%%%%%%%%%%
% This figure is for the empirical results section. It's placed here to get latex to render this where I want.
% FIGURE NOTES %%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}
    \centering
    % First subfigure
    \begin{subfigure}[t]{0.34\textwidth}
        \centering
        \includegraphics[width=\linewidth]{Images/final_figures/log-sum-exp_grad_norm_vs_iterate_all_solvers.pdf}
        \caption{Log-sum-exp (\ref{eq:log-sum-exp-objective}) with $\alpha = 2$}
        \label{fig:log-sum-exp}
    \end{subfigure}
    %\hspace{0.05\textwidth} % Optional space between subfigures
    % Second subfigure
    \begin{subfigure}[t]{0.315\textwidth} % Use a valid fraction like 0.4\textwidth
        \centering
        \includegraphics[width=\linewidth]{Images/final_figures/log-reg_grad_norm_vs_iterate_all_solvers_no_legend.pdf}
        \caption{Logistic regression with $\alpha = 3$}
        \label{fig:log-reg}
    \end{subfigure}
    % Third subfigure
    \begin{subfigure}[t]{0.315\textwidth} % Use a valid fraction like 0.4\textwidth
        \centering
        \includegraphics[width=\linewidth]{Images/final_figures/Linear_least_squares_grad_norm_vs_iterate_all_solvers_no_legend.pdf}
        \caption{Linear least-squares with $\alpha = 2$}
        \label{fig:linear-least-squares}
    \end{subfigure}
    \caption{Gradient Euclidean norm of solution at $t$\ts{th} iterate. Learning rates were chosen to minimize loss of the algorithm in original coordinates. We induce EGOP spectral decay by choice of data matrix $A$ with singular values $\sigma_k(A) = k^{-\alpha}$. As noted in the prose, in some plots the dotted traces coincide with the solid and are thus not visible (Adagrad in \cref{fig:log-sum-exp}, Adam in  \cref{fig:log-sum-exp}).}
    \label{fig:cvx-objectives}
\end{figure*}

\cref{fig:cvx-objectives} demonstrates that EGOP reparameterization can improve convergence of adaptive algorithms to global minima of convex objectives. For the log-sum-exp objective (\cref{fig:log-sum-exp}), reparameterization accelerates Adam's convergence, but has no impact on Adagrad. In \cref{ssec:sup-figs-main-experiments}, we show that if learning rates are tuned separately for each coordinate setting, reparameterized Adagrad can use a larger learning rate and thus converge more quickly than Adagrad in original coordinates. For logistic regression (\cref{fig:log-reg}), Adam achieves comparable performance in both coordinate settings, while gradient descent with momentum outperforms all other methods. Notably, methods with momentum excel for this objective, and reparameterization boosts the performance of Adagrad (which does not use momentum), making it competitive with the momentum-based optimizers.  For linear least-squares (\cref{fig:linear-least-squares}), reparameterization improves the performance of both Adam and Adagrad. As with log-sum-exp, \cref{ssec:sup-figs-main-experiments} shows that if learning rates are tuned separately for each coordinate system, then reparameterized Adagrad can achieve even better performance.

\section{Conclusions and Limitations}

In this work, we have shown through both analysis and experiments that EGOP reparameterization can improve convergence of adaptive algorithms. There may be opportunities for future work to explore other reparameterizations and additional hyperparameter tuning which we have not examined in this work. One limitation of this work is the engineering required for scalability, as discussed in Section~\ref{ssec:efficient-heuristics}. Fully characterizing the trade-off between improved convergence and the up-front cost of reparameterization remains a direction for future research. Another interesting direction
is whether competing explanations for Adam's empirical out-performance of SGD on LLM training---in terms of achieving lower training error and converging more quickly---may be unified through the lens of EGOP spectral decay \cite{kunstner2024heavy,zhang2024transformers}.

\section*{Acknowledgments}

We thank Lorenzo Orecchia for several helpful conversations about the role of smoothness in analyzing related methods. We gratefully acknowledge the support of AFOSR FA9550-18-1-0166, NSF DMS-2023109, DOE DE-SC0022232, the NSF-Simons National Institute for Theory and Mathematics in Biology (NITMB) through NSF (DMS-2235451) and Simons Foundation (MP-TMPS-00005320), and the Margot and Tom Pritzker Foundation. AD gratefully acknowledges the support of NSF DGE 2140001.

\bibliographystyle{abbrvnat}
\bibliography{ref}
\newpage
\appendix

\input{src/appendices/new-version-deferred-proofs}

\input{src/appendices/extended_heuristics_discussion}

\input{src/appendices/supplementary_figures}

\input{src/appendices/experimental_details}

\input{src/appendices/Comparison_with_SOAP_Galore}

\input{src/appendices/extended-related-works}

\vfill
\end{document}
