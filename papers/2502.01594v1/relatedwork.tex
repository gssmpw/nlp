\section{Related Work}
\label{ssec:related-work}

Our work intersects with research on theoretical guarantees for adaptive algorithms, the role of orthogonal rotations in algorithmic performance, and the geometry of machine learning objectives with natural data. Here we present a concise overview of relevant works, and include an expanded discussion in Section~\ref{sec:extended-related-works} of the supplementary material. 

\paragraph{Geometric Sensitivity of Adaptive Methods} A large body of research has been devoted to understanding when adaptive algorithms' per-coordinate learning rates confer a benefit over (stochastic) gradient descent ((S)GD). Recently, there has been renewed interest in distinguishing the properties of adaptive algorithms versus SGD, as several empirical studies suggest that adaptive methods outperform SGD when training transformer models \cite{kunstner2024heavy,zhang2024transformers}. Traditional analyses of adaptive algorithms establish regret bounds for online convex optimization \cite{duchi2011adaptive, hazan2016introduction}. More recent work establishes convergence rates for smooth, non-convex objectives in terms of the $\ell_2$ norm of the gradient \cite{defossez2020simple,ward2020adagrad}. However, because SGD is known to have optimal convergence rates in these settings, these theoretical results only show that adaptive algorithms achieve rates matching those of SGD.

In order to understand when adaptive algorithms enjoy provably stronger guarantees than SGD, a recent line of work studies convergence under refined geometric assumptions, with particular emphasis on assumptions that are \textit{not} rotationally invariant \cite{jiang2024convergence, liu2024adagrad, xie2024adamexploitsellinftygeometryloss}. \citet{xie2024adamexploitsellinftygeometryloss} establish convergence guarantees in terms of the $\ell_{\infty}$ smoothness constant of the objective and show experimentally that rotationally invariant geometric assumptions do not suffice to capture settings when Adam out-performs SGD. \citet{jiang2024convergence} and \citet{liu2024adagrad} study convergence of Adagrad on objectives that are \textit{coordinate-wise smooth}, defined in~\cref{ssec:notation}. Both works prove similar convergence guarantees for Adagrad; they measure convergence with the gradient $\ell_1$ norm, rather than $\ell_2$, because in this regime they can prove a gap between convergence guarantees for Adagrad versus SGD that is consistent with the advantage exhibited by adaptive methods in certain settings \cite{jiang2024convergence}.
\citet{jiang2024convergence} show that when objective functions exhibit certain geometric properties, Adagrad enjoys $\ell_1$ convergence upper bounds that are lower than corresponding lower bounds on SGD's convergence by a factor of $d$ \cite{jiang2024convergence}. Our analysis builds on that of \citet{jiang2024convergence} and \citet{liu2024adagrad}; they show that the sum of the coordinate-wise smoothness constants governs Adagrad convergence, and we prove that EGOP reparameterization decreases this value by a factor as large as $1/d$.

\paragraph{Change-of-Basis for Adaptive Algorithms} Recent works propose that using different orthonormal transformations with Adam and its variants can reduce computational costs and improve performance in neural network training \cite{maes2024understanding,vyas2024soap,  zhao2024galore}. \citet{gupta2018shampoo} introduced Shampoo, an efficient preconditioning method for optimizing tensor spaces. \citet{vyas2024soap} formalized a connection between Shampoo and a variant of Adagrad, leading to a new proposed method called SOAP. Designed for training neural networks, SOAP computes an orthonormal reparameterization based on the singular vectors of the matrix-valued network gradients and performs optimization in this basis. \citet{vyas2024soap} empirically examine the performance of SOAP and find that it outperforms both Adam and Shampoo in LLM pre-training. \citet{zhao2024galore} propose GaLore, a method that simultaneously performs reparameterization and dimensionality reduction. GaLore computes a similar orthogonal basis to that used in SOAP, but instead of a full-dimensional change-of-basis GaLore retains only leading basis vectors in order to reduce dimension \cite{zhao2024galore}. \citet{maes2024understanding} empirically study Adam's rotational sensitivity and examine the power of existing geometric assumptions (such as those leveraged by \citet{xie2024adamexploitsellinftygeometryloss}) to explain Adam's performance when training transformer architectures. They propose an orthonormal reparameterization, similar to those used by SOAP and GaLore, and show empirically that this can improve Adam's performance \cite{maes2024understanding}. 

SOAP and GaLore are empirically powerful tools, but their convergence properties are not fully understood. The results in this work may lead to new ways to analyze the performance of SOAP and GaLore. See \cref{sec:compare-w-SOAP-Galore} for further discussion of the main distinctions between our proposed reparameterization methods and SOAP/GaLore.

Outside of training neural networks, several works have considered data-driven dimensionality reduction methods for optimizing more general objectives with low-rank EGOP matrices \cite{cartis2024learning, cosson2023low}. These procedures target objectives with exact low-rank structure, while our method can improve convergence of adaptive algorithms even when the EGOP matrix has strong spectral decay but is full-rank.

\paragraph{EGOP Structure in Machine Learning}
    Increasing empirical evidence across a wide range of applications, including language modeling and image classification, suggests that empirical loss functions used for training machine learning models are approximately low-rank---meaning that the functions vary strongly in only a small subset of directions in parameter space \citet{papyan2018full,sagun2017empirical,  zhang2024transformers}. Such approximate low-rank structure can be detected and analyzed using active subspace methods. Many active subspace methods leverage the EGOP matrix (sometimes referred to as the average gradient outer product (AGOP) matrix), which we define in Section~\ref{sec:EGOP-defn}  \cite{constantine2015active}. Recently, there has been growing interest in the EGOP and in the use of active subspace methods in machine learning research \cite{cui2020active, mallinar2024emergence,radhakrishnan2022mechanism,zhu2023catapults}. \citet{radhakrishnan2022mechanism} provide theoretical and empirical evidence that the weight matrices of deep neural networks trained with gradient descent correlate strongly with a kind of EGOP matrix.\footnote{In \citet{mallinar2024emergence,radhakrishnan2022mechanism} and \citet{zhu2023catapults}, the EGOP is defined using the gradient with respect to the \emph{input data} instead of the optimization parameters.}