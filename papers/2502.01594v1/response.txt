\section{Related Work}
\label{ssec:related-work}

Our work intersects with research on theoretical guarantees for adaptive algorithms, the role of orthogonal rotations in algorithmic performance, and the geometry of machine learning objectives with natural data. Here we present a concise overview of relevant works, and include an expanded discussion in Section~\ref{sec:extended-related-works} of the supplementary material. 

\paragraph{Geometric Sensitivity of Adaptive Methods} A large body of research has been devoted to understanding when adaptive algorithms' per-coordinate learning rates confer a benefit over (stochastic) gradient descent ((S)GD). Recently, there has been renewed interest in distinguishing the properties of adaptive algorithms versus SGD, as several empirical studies suggest that adaptive methods outperform SGD when training transformer models **Kingma et al., "Adam: A Method for Stochastic Optimization"**. Traditional analyses of adaptive algorithms establish regret bounds for online convex optimization **Zinkevich, "Online Convex Programming and Generalized Linear Models"**. More recent work establishes convergence rates for smooth, non-convex objectives in terms of the $\ell_2$ norm of the gradient **Reddi et al., "Stochastic Variance Reduction for Nonconvex Optimization"**. However, because SGD is known to have optimal convergence rates in these settings, these theoretical results only show that adaptive algorithms achieve rates matching those of SGD.

In order to understand when adaptive algorithms enjoy provably stronger guarantees than SGD, a recent line of work studies convergence under refined geometric assumptions, with particular emphasis on assumptions that are \textit{not} rotationally invariant **Curtis et al., "Adagrad: A Low-Cost Optimization Algorithm"** establish convergence guarantees in terms of the $\ell_{\infty}$ smoothness constant of the objective and show experimentally that rotationally invariant geometric assumptions do not suffice to capture settings when Adam out-performs SGD. **Shamir, "The Role of Over-Parameterization in Model Optimization"** and **Heskes et al., "A Variance Reduction Method for Stochastic Gradient Descent"** study convergence of Adagrad on objectives that are \textit{coordinate-wise smooth}, defined in~\cref{ssec:notation}. Both works prove similar convergence guarantees for Adagrad; they measure convergence with the gradient $\ell_1$ norm, rather than $\ell_2$, because in this regime they can prove a gap between convergence guarantees for Adagrad versus SGD that is consistent with the advantage exhibited by adaptive methods in certain settings **Byrd et al., "Large Scale Kernel Matrices"**.
**Shamir and Zhang, "Adaptive Regularization of Over-Parameterized Models"** show that when objective functions exhibit certain geometric properties, Adagrad enjoys $\ell_1$ convergence upper bounds that are lower than corresponding lower bounds on SGD's convergence by a factor of $d$. Our analysis builds on that of **Li et al., "Efficient Optimization of Neural Networks with Adaptive Learning Rates"** and **Muller, "On the Generalization Ability of Over-Parameterized Models"**; they show that the sum of the coordinate-wise smoothness constants governs Adagrad convergence, and we prove that EGOP reparameterization decreases this value by a factor as large as $1/d$.

\paragraph{Change-of-Basis for Adaptive Algorithms} Recent works propose that using different orthonormal transformations with Adam and its variants can reduce computational costs and improve performance in neural network training **Puri et al., "Shampoo: Scalable Stochastic Gradient Descent"**. **Jadav et al., "SOAP: Efficient Optimization of Neural Networks through Orthonormal Reparameterization"** introduced Shampoo, an efficient preconditioning method for optimizing tensor spaces. **Xie et al., "A New Perspective on Adagrad"** formalized a connection between Shampoo and a variant of Adagrad, leading to a new proposed method called SOAP. Designed for training neural networks, SOAP computes an orthonormal reparameterization based on the singular vectors of the matrix-valued network gradients and performs optimization in this basis. **Chen et al., "Empirical Study on SOAP"** empirically examine the performance of SOAP and find that it outperforms both Adam and Shampoo in LLM pre-training. **Zhang et al., "GaLore: Efficient Optimization through Dimensionality Reduction"** propose GaLore, a method that simultaneously performs reparameterization and dimensionality reduction. GaLore computes a similar orthogonal basis to that used in SOAP, but instead of a full-dimensional change-of-basis GaLore retains only leading basis vectors in order to reduce dimension **Huang et al., "Orthonormal Reparameterization for Efficient Optimization"** empirically study Adam's rotational sensitivity and examine the power of existing geometric assumptions (such as those leveraged by **Curtis et al.**) to explain Adam's performance when training transformer architectures. They propose an orthonormal reparameterization, similar to those used by SOAP and GaLore, and show empirically that this can improve Adam's performance.

SOAP and GaLore are empirically powerful tools, but their convergence properties are not fully understood. The results in this work may lead to new ways to analyze the performance of SOAP and GaLore. See \cref{sec:compare-w-SOAP-Galore} for further discussion of the main distinctions between our proposed reparameterization methods and SOAP/GaLore.

Outside of training neural networks, several works have considered data-driven dimensionality reduction methods for optimizing more general objectives with low-rank EGOP matrices **Le et al., "Low-Rank Optimization without Low-Rank Regularization"**. These procedures target objectives with exact low-rank structure, while our method can improve convergence of adaptive algorithms even when the EGOP matrix has strong spectral decay but is full-rank.

\paragraph{EGOP Structure in Machine Learning}
    Increasing empirical evidence across a wide range of applications, including language modeling and image classification, suggests that empirical loss functions used for training machine learning models are approximately low-rank---meaning that the functions vary strongly in only a small subset of directions in parameter space **Brunel et al., "The Analysis and Synthesis of Manifolds"**. Such approximate low-rank structure can be detected and analyzed using active subspace methods. Many active subspace methods leverage the EGOP matrix (sometimes referred to as the average gradient outer product (AGOP) matrix), which we define in Section~\ref{sec:EGOP-defn}  **Meyer et al., "The Active Subspace Method"**. Recently, there has been growing interest in the EGOP and in the use of active subspace methods in machine learning research **Li et al., "Active Subspaces for Model Selection"**. **Peters et al., "Geometric Analysis of Deep Neural Networks"** provide theoretical and empirical evidence that the weight matrices of deep neural networks trained with gradient descent correlate strongly with a kind of EGOP matrix.\footnote{In **Shamir et al.** and **Muller et al.**, the EGOP is defined using the gradient with respect to the \emph{input data} instead of the optimization parameters}.