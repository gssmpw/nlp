@article{cartis2024learning,
  title={{Learning the subspace of variation for global optimization of functions with low effective dimension}},
  author={Cartis, Coralia and Liang, Xinzhu and Massart, Estelle and Otemissov, Adilet},
  journal={arXiv preprint arXiv:2401.17825},
  year={2024}
}

@book{constantine2015active,
  title={{Active subspaces: Emerging ideas for dimension reduction in parameter studies}},
  author={Constantine, Paul G},
  year={2015},
  publisher={SIAM}
}

@article{cosson2023low,
  title={{Low-Rank Gradient Descent}},
  author={Cosson, Romain and Jadbabaie, Ali and Makur, Anuran and Reisizadeh, Amirhossein and Shah, Devavrat},
  journal={{IEEE Open Journal of Control Systems}},
  year={2023},
  publisher={IEEE}
}

@article{cui2020active,
  title={Active subspace of neural networks: Structural analysis and universal attacks},
  author={Cui, Chunfeng and Zhang, Kaiqi and Daulbaev, Talgat and Gusak, Julia and Oseledets, Ivan and Zhang, Zheng},
  journal={SIAM Journal on Mathematics of Data Science},
  volume={2},
  number={4},
  pages={1096--1122},
  year={2020},
  publisher={SIAM}
}

@article{defossez2020simple,
  title={{A Simple Convergence Proof of Adam and Adagrad}},
  author={D{\'e}fossez, Alexandre and Bottou, L{\'e}on and Bach, Francis and Usunier, Nicolas},
  journal={arXiv preprint arXiv:2003.02395},
  year={2020}
}

@article{duchi2011adaptive,
  title={{Adaptive subgradient methods for online learning and stochastic optimization.}},
  author={Duchi, John and Hazan, Elad and Singer, Yoram},
  journal={{Journal of Machine Learning Research}},
  volume={12},
  number={7},
  year={2011}
}

@inproceedings{gupta2018shampoo,
  title={Shampoo: Preconditioned stochastic tensor optimization},
  author={Gupta, Vineet and Koren, Tomer and Singer, Yoram},
  booktitle={International Conference on Machine Learning},
  pages={1842--1850},
  year={2018},
  organization={PMLR}
}

@article{hazan2016introduction,
  title={{Introduction to online convex optimization}},
  author={Hazan, Elad and others},
  journal={{Foundations and Trends{\textregistered} in Optimization}},
  volume={2},
  number={3-4},
  pages={157--325},
  year={2016},
  publisher={Now Publishers, Inc.}
}

@article{jiang2024convergence,
  title={Convergence analysis of adaptive gradient methods under refined smoothness and noise assumptions},
  author={Jiang, Ruichen and Maladkar, Devyani and Mokhtari, Aryan},
  journal={arXiv preprint arXiv:2406.04592},
  year={2024}
}

@article{kunstner2024heavy,
  title={{Heavy-tailed class imbalance and why Adam outperforms gradient descent on language models}},
  author={Kunstner, Frederik and Yadav, Robin and Milligan, Alan and Schmidt, Mark and Bietti, Alberto},
  journal={arXiv preprint arXiv:2402.19449},
  year={2024}
}

@article{liu2024adagrad,
  title={{AdaGrad under Anisotropic Smoothness}},
  author={Liu, Yuxing and Pan, Rui and Zhang, Tong},
  journal={arXiv preprint arXiv:2406.15244},
  year={2024}
}

@article{maes2024understanding,
  title={{Understanding Adam Requires Better Rotation Dependent Assumptions}},
  author={Maes, Lucas and Zhang, Tianyue H and Jolicoeur-Martineau, Alexia and Mitliagkas, Ioannis and Scieur, Damien and Lacoste-Julien, Simon and Guille-Escuret, Charles},
  journal={arXiv preprint arXiv:2410.19964},
  year={2024}
}

@article{mallinar2024emergence,
  title={Emergence in non-neural models: grokking modular arithmetic via average gradient outer product},
  author={Mallinar, Neil and Beaglehole, Daniel and Zhu, Libin and Radhakrishnan, Adityanarayanan and Pandit, Parthe and Belkin, Mikhail},
  journal={arXiv preprint arXiv:2407.20199},
  year={2024}
}

@article{papyan2018full,
  title={The full spectrum of deepnet hessians at scale: Dynamics with sgd training and sample size},
  author={Papyan, Vardan},
  journal={arXiv preprint arXiv:1811.07062},
  year={2018}
}

@article{radhakrishnan2022mechanism,
  title={Mechanism of feature learning in deep fully connected networks and kernel machines that recursively learn features},
  author={Radhakrishnan, Adityanarayanan and Beaglehole, Daniel and Pandit, Parthe and Belkin, Mikhail},
  journal={arXiv preprint arXiv:2212.13881},
  year={2022}
}

@article{sagun2017empirical,
  title={Empirical analysis of the hessian of over-parametrized neural networks},
  author={Sagun, Levent and Evci, Utku and Guney, V Ugur and Dauphin, Yann and Bottou, Leon},
  journal={arXiv preprint arXiv:1706.04454},
  year={2017}
}

@article{vyas2024soap,
  title={{SOAP: Improving and stabilizing Shampoo using Adam}},
  author={Vyas, Nikhil and Morwani, Depen and Zhao, Rosie and Shapira, Itai and Brandfonbrener, David and Janson, Lucas and Kakade, Sham},
  journal={arXiv preprint arXiv:2409.11321},
  year={2024}
}

@article{ward2020adagrad,
  title={Adagrad stepsizes: Sharp convergence over nonconvex landscapes},
  author={Ward, Rachel and Wu, Xiaoxia and Bottou, Leon},
  journal={Journal of Machine Learning Research},
  volume={21},
  number={219},
  pages={1--30},
  year={2020}
}

@misc{xie2024adamexploitsellinftygeometryloss,
      title={{Adam Exploits $\ell_\infty$-geometry of Loss Landscape via Coordinate-wise Adaptivity}}, 
      author={Shuo Xie and Mohamad Amin Mohamadi and Zhiyuan Li},
      year={2024},
      eprint={2410.08198},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2410.08198}, 
}

@article{zhang2024transformers,
  title={{Why transformers need Adam: A Hessian perspective}},
  author={Zhang, Yushun and Chen, Congliang and Ding, Tian and Li, Ziniu and Sun, Ruoyu and Luo, Zhi-Quan},
  journal={arXiv preprint arXiv:2402.16788},
  year={2024}
}

@article{zhao2024galore,
  title={Galore: Memory-efficient llm training by gradient low-rank projection},
  author={Zhao, Jiawei and Zhang, Zhenyu and Chen, Beidi and Wang, Zhangyang and Anandkumar, Anima and Tian, Yuandong},
  journal={arXiv preprint arXiv:2403.03507},
  year={2024}
}

@article{zhu2023catapults,
  title={{Catapults in SGD: spikes in the training loss and their impact on generalization through feature learning}},
  author={Zhu, Libin and Liu, Chaoyue and Radhakrishnan, Adityanarayanan and Belkin, Mikhail},
  journal={arXiv preprint arXiv:2306.04815},
  year={2023}
}

