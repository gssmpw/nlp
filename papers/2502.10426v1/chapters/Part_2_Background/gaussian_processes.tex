\chapter{Gaussian Processes}{\label{ch:Gaussian Processes}}
This chapter introduces Gaussian Processes (GPs) and shows how careful choice of covariance function (or kernel) allows us to encode assumptions about distributions over functions. In particular, we detail the Spectral Mixture (SM) covariance function, which allows modelling signals as superposed periodic components. All of the equations and definitions in this chapter come from \cite{rasmussen_2006_gaussian}. 

\section{Definition}
GPs provide a principled, practical and powerful framework for non-parametric regression and classification. They can be thought of as generalisations of multivariate Gaussian distributions, extended to infinitely many variables, $\boldsymbol{x}$. Thus, GPs can be interpreted as modelling distributions of \textit{functions} over infinite input domain. Formally:

\begin{definition}
    A Gaussian Process is a collection of random variables, any finite number of which have a joint Gaussian distribution.
\end{definition}

A GP for a real-valued process $y = f(\boldsymbol{x})$ is fully specified by its mean function, $m(\boldsymbol{x}) = \mathbb{E}[f(\boldsymbol{x})]$, and its covariance function, $k(\boldsymbol{x}, \boldsymbol{x}^\prime) =  \mathbb{E}[(f(\boldsymbol{x}) - m(\boldsymbol{x}))(f(\boldsymbol{x}^\prime) - m(\boldsymbol{x}^\prime))]$. This allows us to express the GP as:
\[
f(\boldsymbol{x}) \sim \mathcal{GP}(m(\boldsymbol{x}), k(\boldsymbol{x},\boldsymbol{x}^\prime))
\]

Any finite collection of function values has a jointly normal distribution: 
\[
[f(\boldsymbol{x_1}), f(\boldsymbol{x_2}), ..., f(\boldsymbol{x_n})]^T \sim \mathcal{N}(\boldsymbol{\mu}, K)
\]
where $\boldsymbol{\mu} = m(\boldsymbol{x})$ is the mean function and the covariance matrix $K$ is an $n \times n$ matrix whose entries are populated by the covariance function: $K_{i,j} = k(\boldsymbol{x}_i, \boldsymbol{x}_j)$. According to \cite{rasmussen_2006_gaussian}, we may assume that $\boldsymbol \mu = \boldsymbol 0$ for simplicity's sake, unless it is necessary to model a mean that varies over time. Under this assumption, a GP is defined entirely by its covariance function.\\
%The flexibility of GPs allows us to assume that $\boldsymbol{\mu} = \boldsymbol{0}$. 

\section{Covariance Function}{\label{subsection:covariance_function}}

The covariance function encodes assumptions about the functions to be modelled. We can think of the covariance function as a measure of \textit{similarity} between data points $\boldsymbol{x}$ and $\boldsymbol{x'}$. One basic assumption is that close inputs of $\boldsymbol{x}$ return similar target values $y$. Note that for $K$ to be a valid covariance matrix, it must be positive semidefinite, such that $K$ satisfies $\boldsymbol{v}^T K \boldsymbol{v} \geq 0$ for all vectors $\boldsymbol{v} \in \mathcal{R}^n$. In \hyperref[fig:kernels]{Figure \ref*{fig:kernels}}, we illustrate how various common covariance functions determine the characteristics of functions randomly sampled from corresponding GPs. 

\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{figs/Part_2/covariance_function.png}
    \caption{Here we present random samples drawn from three different GP models which have different underlying covariance functions. From left to right, we have Rational Quadratic, Exponentiated Quadratic and Periodic kernels \cite{roelants_2019_gaussian}. Hyperparameters are defined above each graph.}
    \label{fig:kernels}
\end{figure}


\section{Spectral Mixture Covariance Function}{\label{section:SM}
As outlined in {\cite{wilson_2013_gaussian}, the Spectral Mixture (SM) kernel is derived from a Mixture of Gaussians (MoG) in the frequency domain. This allows us to encode the underlying frequency components of a function, naturally providing a framework for modelling musical signals as superposed note sources. Furthermore, the SM kernel can approximate any covariance function to arbitrary precision by superposing sufficiently many weighted frequency components.\\

Importantly, the Spectral Mixture (SM) kernel is a stationary covariance function, which means it is a function of $\boldsymbol{x} - \boldsymbol{x'} =  \tau$. Thus it is invariant to translations of the input space. The inverse Fourier transform of the MoG allows us to find the time domain covariance function, with general form {\cite{wilson_2013_gaussian}: 

\begin{equation}{\label{equation:SM}}
 k( \tau) = \sum^Q_{q=1} w_q \prod_{p=1}^P \
\exp(-2\pi^2\tau_p^2 v_q^{(p)})\cos(2\pi\tau_p\mu_q^{(p)})
\end{equation}

where $P$ is the dimension of the inputs and $Q$ is the number of frequency components. The $q$-th Gaussian has weight $w_q$, and its $p$-th dimension has mean $\mu_q^{(p)}$ and variance $v_q^{(p)}$. In this work, we assume $P=1$ since audio is 1-dimensional, and we can omit references to $p$.

\section{Log Marginal Likelihood (LML)}
Assuming Gaussian noise $\sigma_n$ in our observed data $\boldsymbol{y}$ consisting of $n$ samples, we can derive the LML given our model hyperparameters\footnote{We refer to the parameters of the covariance function $k$ as `hyperparameters' to emphasise
that they are parameters of a non-parametric model.} $\boldsymbol{\theta}$ and data input vector $\boldsymbol{x}$:

\begin{equation}\label{eq:LML}
\log p(\boldsymbol{y}|\boldsymbol{x}, \boldsymbol{\theta}) = -\frac{1}{2}\boldsymbol{y}^T[ K+\sigma_n^2  I]^{-1}\boldsymbol{y} -\frac{1}{2} \log| K + \sigma_n^2  I| -\frac{n}{2} \log (2 \pi)
\end{equation}
Here, $ K$ is a function of $\boldsymbol x$ and $\boldsymbol \theta$ since $K_{i,j} = k({x}_i, {x}_j) = k(\tau)$ and $k$ is determined by $\boldsymbol \theta$. The first term of the LML is called the data fit term and the second is called the complexity term.
