% Define a new tcolorbox environment
\newtcolorbox{lemmabox}[2][]{%
    colback=gray!20, % Background color
    colframe=gray!60, % Frame color
    fonttitle=\bfseries, % Title font style
    title=#2, #1, % Use the second argument as the title
    sharp corners, % Optional: makes the corners sharp
    boxrule=0.5mm, % Width of the frame
}
\chapter{Stage 1: GP Model Specification}{\label{ch:model_selection}}
 In this chapter we develop and optimise a Gaussian Process (GP) model for piano audio signals. This model gives us a log marginal likelihood (LML) function, which forms the basis for making predictions about underlying notes. The GP uses a Spectral Mixture (SM) covariance function, which we design through analysis of audio signals and optimise using both theoretical understanding of acoustics and empirical methods. In describing our methods, we discuss design decisions and trade-offs which affect the success of our score follower.      

\section{Aims and Requirements}{\label{section:aims}}
The primary goal of this chapter is to develop a log marginal likelihood (LML) function that estimates the notes present in an \textit{audioframe}. An audioframe is a group of contiguous audio samples, ranging in length between 800 and 2000 samples. Given a list of fundamental frequencies $\boldsymbol{f}$, the LML function should take an audioframe and output a log likelihood which:
\begin{itemize}
    \item is globally maximised when $\boldsymbol{f}$ is equal to the true underlying fundamental frequencies;
    \item is invariant to changes in other properties of the recording;
    \item is robust to the challenges mentioned in \hyperref[section:challenges]{section \ref*{section:challenges}};
    \item is efficient and stable to compute (since we are developing a real-time score follower).
\end{itemize}


\section{Motivation}
Gaussian Processes (GPs) offer a Bayesian framework for computing likelihoods over functions. They can be fine-tuned via their covariance functions to effectively capture the structured characteristics of audio signals (refer to \hyperref[ch:Gaussian Processes]{chapter \ref*{ch:Gaussian Processes}}).  \\

As concluded in \hyperref[ch:problem_definition]{chapter \ref*{ch:problem_definition}}, our objective is to ensure that the model accurately represents the harmonic structure of piano signals. As motivated in \hyperref[section:SM]{section \ref*{section:SM}}, we employ the Spectral Mixture (SM) kernel, which lends itself to being defined in the frequency domain, allowing us to feature engineer harmonic structure into the model. Moreover, the SM kernel incorporates a frequency hyperparameter, facilitating a simple interface for adjusting the model for different notes. Since we know all potential note combinations present in the score, we can systematically determine the LMLs of audioframes corresponding to the frequencies of those notes and determine the notes with maximal LML. By using the SM kernel, we assume that our GPs are stationary, meaning that they possess the same statistical properties throughout each audioframe. This is not strictly true due to decay and other noise transience effects, but it is a reasonable simplifying assumption since our audioframes are short in duration (800 to 2000 samples at $44.1$ kHz $\approx$ 18 to 45 ms). 

\section{Method}

\subsection{Signal Analysis}{\label{subsection:signal_analysis}}
We first took numerous recordings of different piano notes and chords at a sampling rate of $f_s = 44.1$ kHz and extracted various audioframes of 1000 and 2000 audio samples for analysis. See \hyperref[fig: example_audio_samples]{Figure \ref*{fig: example_audio_samples}} for one example of these recordings and an extracted audioframe. 


\begin{figure}[H]
  \centering
  \begin{subfigure}[b]{0.45\textwidth}
    \centering
    \includegraphics[width=0.75\textwidth]{figs/Part_3_Implementation/Stage_1_Model_Selection/single_440_note.png}
    \caption{Time-amplitude graph of a recording of a single piano A4 note, zoomed out to show amplitude over a 1.2-second window containing dozens of audioframes.}
    \label{fig:zoom_out}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.45\textwidth}
    \centering
    \includegraphics[width=0.75\textwidth]{figs/Part_3_Implementation/Stage_1_Model_Selection/audio_frame_440.png}
    \caption{Time-amplitude graph of a single audioframe (1000 audio samples) extracted from \ref{fig:zoom_out}. Note the periodic but non-sinusoidal waveform.}
    \label{fig:zoom_in}
  \end{subfigure}
  \caption{Two time-amplitude graphs from a recording of the note A4 ($f_0 = 440$ Hz) on the piano.}
  \label{fig: example_audio_samples}
\end{figure}


Although \hyperref[fig:zoom_in]{Figure \ref*{fig:zoom_in}} represents a single A4 note with fundamental frequency 440 Hz, the time domain waveform is clearly not a pure sinusoid. Instead, the waveform demonstrates the natural interference patterns that arise from \textit{overtones}. These overtones are largely responsible for the unique \gls{timbre}s of different instruments (see \hyperref[ch:music_preliminaries]{chapter \ref*{ch:music_preliminaries}}). \\

We then plotted the magnitudes of the frequency and power spectra, using a Hanning window to avoid spectral leakage. As seen in \hyperref[fig: frequency_domain]{Figure \ref*{fig: frequency_domain}}, the harmonic content of a single note displays an accentuated peak at $f_0$, as well as several tapered peaks situated at the harmonics of the fundamental frequency, which are integer multiples of $f_0$. The relative magnitudes of these harmonics form the spectral `envelope' which we aim to model (see \hyperref[ch:music_preliminaries]{chapter \ref*{ch:music_preliminaries}}).  We also examined chords and found that their spectra were the superpositions of the underlying frequency spectra of each single note source. 


\begin{figure}[H]
  \centering
  \begin{subfigure}[b]{0.45\textwidth}
    \centering
    \includegraphics[width=0.85\textwidth]{figs/Part_3_Implementation/Stage_1_Model_Selection/FFT.png}
    \caption{Frequency spectrum.}
    \label{fig:FFT}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.45\textwidth}
    \centering
    \includegraphics[width=0.85\textwidth]{figs/Part_3_Implementation/Stage_1_Model_Selection/PSD.png}
    \caption{Power spectrum.}
    \label{fig:PSD}
  \end{subfigure}
  \caption{Examples of audio frequency spectra in a 2000-sample audioframe of a single A4 ($f_0 = 440$ Hz) piano note using a Hanning window. The overlaid pink dashed lines indicate the fundamental frequency and its integer multiples (harmonics).}
  \label{fig: frequency_domain}
\end{figure}

\subsection{Modelling}{\label{subsection:modelling}}
We initially model the above power spectrum as a Mixture of Gaussians:\footnote{We work in terms frequency $f$ since this representation of frequency lends itself better to musical notes.}

\begin{equation}{\label{equation:frequency}}
S(f) = \sum_{q=1}^Q w_q \sum_{m=1}^M \frac{E_m}{2} \left[ \phi(f; m f_q,\sigma_f) + \phi(f; -m f_q,\sigma_f) \right]
\end{equation}

Here, $Q$ represents the number of distinct note sources present, so $Q=1$ for single notes and $Q\geq 2$ for chords. Hence, $f_q$ represents the $q$-th note source's fundamental frequency. $M$ denotes the total number of harmonics that we model (including the fundamental), so the mean of the Gaussian for the $m$-th harmonic is $mf_q $. Thus, each of the peaks in the total power spectrum is represented by a Gaussian $\phi(f; m f_q,\sigma_f) = \frac{1}{\sigma_f\sqrt{2\pi}} \exp({-\frac{(f - m f_q)^2}{2\sigma_f^2}})$ with mean $m f_q$ and variance $\sigma_f^2$. The parameters $w_q$ and $E_m$ represent the relative weights assigned to the $q$-th note and $m$-th harmonic peaks, respectively. The values of $E_m$ are empirically determined to match the spectral envelope observed during analysis (see \hyperref[subsection:signal_analysis]{subsection \ref*{subsection:signal_analysis}}). Following \cite{godsill_2006_bayesian}, we approximate $E_m = \frac{1}{1 + Tm^v}$, where $T$ and $v$ are empirical constants.  

\subsubsection{Improving the Model}{\label{subsubsection:improving_model}}
We noticed several physical phenomena which compromised the model fit. For instance, we found a small unexpected peak at $50$ Hz which was due to a mains hum. This was resolved by simply adding a scaled Gaussian at $50$ Hz.\footnote{In America it was necessary to use a 60 Hz Gaussian.} Another phenomenon encountered was \textit{inharmonicity}, which is the tendency for overtones of a note to depart from whole multiples of $f_0$. This is a common musical phenomenon, occurring due to the physics of unforced oscillations of strings with finite tension \cite{murray_2021_musical}. In general, we noticed that harmonics consistently shifted upward, becoming sharper, with this effect being most pronounced at higher harmonics. 

\begin{figure}[H]
  \centering
  \begin{subfigure}[b]{0.48\textwidth}
    \centering
    \includegraphics[width=1\textwidth]{figs/Part_3_Implementation/Stage_1_Model_Selection/uncorrected.png}
    \caption{Audio spectra of a 2000 sample audio frame of a single B4 note (494 Hz) in red with the simple model overlaid in blue.}
    \label{fig:uncorrected}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.48\textwidth}
    \centering
    \includegraphics[width=1\textwidth]{figs/Part_3_Implementation/Stage_1_Model_Selection/corrected.png}
    \caption{Same as (a), except kernel has been modified to account for inharmonicity.}
    \label{fig:corrected}
  \end{subfigure}
  \caption{Example of inharmonicity effects and how the multiplicative factor $b_{m,f_q}$ improves model fit. Values of $\boldsymbol{B}$ are determined in \hyperref[subsubsection:inharmonicity]{subsubsection \ref*{subsubsection:inharmonicity}}.}
  \label{fig:inharmonicity}
\end{figure}

 To account for inharmonicity, we use the approximation presented in \cite{Godsill2005BayesianCM}, which corrects the harmonic frequency $f_{m,q}$ of the $m$-th harmonic of the $q$-th note in the chord to $f_{m,q} = mf_q \sqrt{1 + B_{f_q}m^2} = m f_q b_{m,f_q}$ (for simplicity, let $b_{m,f_q}=\sqrt{1+B_{f_q}m^2}$). $B_{f}$ is an empirical constant for the note of the piano with fundamental frequency $f$, and all $B_f$ are stored in a vector $\boldsymbol{B}$. \hyperref[fig:inharmonicity]{Figure \ref*{fig:inharmonicity}} illustrates inharmonicity (\hyperref[fig:uncorrected]{Figure \ref*{fig:uncorrected}}) and our corrected harmonics (\hyperref[fig:corrected]{Figure \ref*{fig:corrected}}), where the disparity at higher frequencies between the true spectra (red) and the modelled spectra (blue) is eliminated by the multiplicative factor $b_{m,f_q}$.

\subsection{Covariance Function}{\label{subsection:covariance_function}}
Our model from \hyperref[subsection:modelling]{subsection \ref*{subsection:modelling}} forms the frequency spectrum of the SM kernel for our GPs. To obtain the time domain covariance function, we calculate the inverse Fourier transform of \hyperref[equation:frequency]{equation \ref*{equation:frequency}} (see \hyperref[appendix:iFT]{Appendix \ref*{appendix:iFT}}): 

\begin{equation}{\label{eq:k_tau}}
k(\tau) = e^{-2\pi^2\sigma_f^2 \tau^2} \sum_{q=1}^Q w_q \sum_{m=1}^M E_m \cos(2\pi m f_{q} b_{m,f_q}  \tau)
\end{equation}

where $\tau = x - x'$ represents the time between audio samples $x$ and $x'$ (since we are assuming a stationary covariance function; see \hyperref[section:SM]{section \ref*{section:SM}}). The other variables are hyperparameters and are defined in \hyperref[subsection:modelling]{subsection \ref*{subsection:modelling}}. This result is intuitive, comprising a quasi-periodic function of cosines multiplied by a negative exponential component. Each of these sinusoids has a frequency that depends on the underlying hyperparameters, corresponding to the frequency of each harmonic present in an audio wave. Due to linearity of the Fourier transform, the scalars $w_q$ and $E_m$, corresponding to the relative energy of each note source and defining the harmonic envelope respectively, are the same as in the frequency domain. The negative exponential component characterises the relationship between different audio samples as a function of $\tau$ and $\sigma_f$. Therefore, $\sigma_f$ represents a characteristic `inverse length scale', which determines the scale at which the function values have dependency on each other. \\

Thus far, we have seven hyperparameters: $\boldsymbol{\theta} = [\boldsymbol{f}, M,\sigma_f, \boldsymbol{w}, T, v, \boldsymbol{B}]$, recalling that $Q$ is the dimension of $\boldsymbol{f}$ and $E_m=\frac{1}{1+Tm^v}$ (see \hyperref[subsection:modelling]{subsection \ref*{subsection:modelling}} for definitions). In \hyperref[fig:covariance_functions]{Figure \ref*{fig:covariance_functions}} we plot several graphs demonstrating the effect of different hyperparameter combinations on our covariance function. In \hyperref[fig:random_samples]{Figure \ref*{fig:random_samples}}, we also plot three randomly drawn GP samples from these covariance functions with varying hyperparameters. These samples visually resemble the quasi-periodic audio signals observed in our initial analysis of piano note audio signals (see \hyperref[fig:zoom_in]{Figure \ref*{fig:zoom_in}}). Extending these random samples to seconds allowed us to successfully synthesise some musical sounds, which worked particularly well for the viola. The piano was more challenging to imitate, owing to its time-varying temporal envelope.

\begin{figure}
    \centering
    \includegraphics[width=0.96\textwidth]{figs/Part_3_Implementation/Stage_1_Model_Selection/covariance_functions_upright.png}
    \caption{Illustration of the covariance function defined in \hyperref[subsection:covariance_function]{subsection \ref*{subsection:covariance_function}} and the effects of varying certain hyperparameters. The horizontal axis represents $\tau$, the time between two audio samples, and the vertical axis represents the corresponding covariance function values, intuitively the amount of dependence between two audio samples separated by $\tau$. In the left-to-right direction we have increasing fundamental frequency, which can be seen by the increasing number of periods. In the top row, we have $M=3$ and $\sigma_f = 5$. In the middle row, $M$ has increased to 16, as captured by the greater number of high-frequency fluctuations between the primary peaks, corresponding to the high frequency harmonics. In the top left, the three small peaks between the prominent peaks (corresponding to $M=3$) have been smoothed out in the middle left by 16 barely visible peaks which interfere with each other. Meanwhilst, in the middle and right column, the increase in $M$ results in more high frequency oscillations creating a more jagged waveform. Finally, in the bottom row we have increased $\sigma_f$ to 20. Increasing $\sigma_f$ decreases the length scale, which decreases the maximum $\tau$ at which there exists non-negligble dependencies between audio samples. This can be observed by the narrowness of the bottom row graphs.}
    \label{fig:covariance_functions}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.96\textwidth]{figs/Part_3_Implementation/Stage_1_Model_Selection/random_samples.png}
    \caption{Three randomly drawn GP samples using the covariance function defined in \hyperref[subsection:covariance_function]{subsection \ref*{subsection:covariance_function}}. All three sub-figures have fundamental frequency 440 Hz, but differ in $M$ and $\sigma_f$. Increasing $M$ (centre vs. left) increases the complexity of the waveform due to more high-frequency components. Increasing $\sigma_f$ (right vs. left) decreases non-local dependencies and thus increases irregularity across the signal.      }
    \label{fig:random_samples}
\end{figure}


\subsection{Log Marginal Likelihood}{\label{subsection:LML}}
As detailed in \hyperref[ch:Gaussian Processes]{chapter \ref*{ch:Gaussian Processes}}, we utilise the log marginal likelihood (LML) to compare probabilities of different notes. This is defined in \hyperref[eq:LML]{equation \ref*{eq:LML}} and repeated here (with $\ell$ replacing $n$): 

\[ \log p(\boldsymbol{y}|\boldsymbol{x}, \boldsymbol{\theta}) = -\frac{1}{2}\boldsymbol{y}^T[K+\sigma_n^2 I]^{-1}\boldsymbol{y} -\frac{1}{2} \log|K + \sigma_n^2 I| -\frac{\ell}{2} \log (2 \pi) \]

where $\boldsymbol{y}$ is our audioframe containing $\ell$ samples, $\boldsymbol{x}$ contains the time offset of each audio sample, $\boldsymbol{\theta}$ is our hyperparameters, and $K$ is our covariance matrix, which is populated by our covariance function: $K_{i,j} = k(x_i, x_j)$. Since we know all possible note combinations from the score, we can compare all the LMLs for a given audioframe $\boldsymbol{y}$ given the $K$s computed from all the different corresponding frequency combinations (i.e. states). Note that this LML function includes $\sigma_n$ as an eighth hyperparameter representing Gaussian noise. Although we assume no noise in our GP model, we need to include noise when using the GP model on real-life data due to the inevitability of noise. It is also essential to add $\sigma^2_nI$ to $K$, since this mitigates stabilisation issues when computing the inverse of $K$, especially when $K$ is near singular. We leave $\sigma_n$ to be set by the user at runtime.

\subsection{Implementation of the LML Function}
We reduced calculation time by vectorising and using a ``sum of angles'' trick for the construction of $K$. Additionally we found that $\ell$, the number of audio samples in an audioframe, could be reduced to $800$ without compromising results. The LML requires calculating the inverse of the covariance matrix, $K^{-1}$, which has time complexity $\mathcal{O}(\ell^3)$ and may cause stability issues at small $\sigma_n$. To address these problems, we proceed by using a Cholesky factorisation (which also has complexity $\mathcal{O}(\ell^3)$, but is generally much faster) to reformulate the LML equation.  

\subsubsection{Cholesky Factorisation for the LML Function}
Since $K$ is symmetric positive definite (see \hyperref[subsection:covariance_function]{subsection \ref*{subsection:covariance_function}}), we can calculate $K^{-1}$ using a Cholesky factorisation, which is both faster to compute (the Cholesky factor takes  time $\frac {\ell^3} 6$) and highly numerically stable \cite{rasmussen_2006_gaussian}. We take a brief detour to derive this, since it is key to our stable implementation of the LML function. 
% The corresponding pseudo code is outlined in Algorithm 2.1 of \cite{rasmussen_2006_gaussian}. 


\begin{lemmabox}{Cholesky Factorisation}
\footnotesize
Let $A$ be a Hermitian, positive-definite matrix, $\boldsymbol{b}$ be a vector, and suppose $A\boldsymbol{x}=\boldsymbol{b}$. Factorise $A=LL^T$ for a lower triangular matrix $L$. Then: 

\[  A\mathbf{x} = \mathbf{b} \rightarrow LL^T\mathbf{x} = \mathbf{b} \rightarrow LL^T\mathbf{x} = L\mathbf{y} \rightarrow L^T\mathbf{x} = \mathbf{y} \]

If we solve for $L\boldsymbol{y} = \boldsymbol{b}$ and then solve for $\boldsymbol{x}$ in $L^{T}\boldsymbol{x} = \boldsymbol{y}$, we will have solved for $\boldsymbol{x}$ in our original $A\boldsymbol{x} = \boldsymbol{b}$ equation. Now, let the notation $A \backslash \boldsymbol{b}$ denote the vector $\boldsymbol{x}$ which solves $A\boldsymbol{x} = \boldsymbol{b}$. Then $\boldsymbol{x} = L^{T} \backslash (L \backslash \boldsymbol{b})$.  
\qed
\end{lemmabox}


To approximate the data fit term in \hyperref[eq:LML]{equation \ref*{eq:LML}}, we introduce a parameter $\boldsymbol{\alpha}$ such that $\boldsymbol{\alpha} = L^{T} \backslash (L \backslash \boldsymbol{y})$. Now, let $(K+\sigma^2_n I) = A = LL^T$. Hence, \[
-\frac{1}{2} \mathbf{y}^\top \left( K + \sigma_n^2 \mathbf{I} \right)^{-1} \mathbf{y} = -\frac{1}{2} \mathbf{y}^\top \boldsymbol{\alpha}
\]
For the complexity term of the LML, we can efficiently calculate the determinant: $
\det(K + \sigma_n^2 \mathbf{I}) = \det(LL^\top) = \det(L) \det(L^\top)$. Here, $\det(L) = \prod_{i=1}^\ell L_{i,i}$ because $L$ is a lower triangular matrix. Hence, 
\[\log \det (K + \sigma_n^2 \mathbf{I}) = \log \det (L^2) \\
= \log ( \left[ \prod_{i=1}^\ell L_{i,i} \right]^2 ) \\
=  2 \sum_{i=1}^\ell \log L_{i,i}
\]
Thus we have an efficient implementation of a stable LML function \cite{rasmussen_2006_gaussian}:
\[
\log p(\boldsymbol{y} | \boldsymbol{x}, \boldsymbol{\theta}) = -\frac{1}{2} \mathbf{y}^\top \boldsymbol{\alpha} - \sum_{i=1}^\ell \log L_{i,i} - \frac{\ell}{2} \log 2\pi
\]

\section{Hyperparameter Selection}

% \begin{wraptable}{r}{5.76cm}
% \vspace{-4.5mm} % Adjusts space before the table
% \footnotesize
% \centering
% \renewcommand{\arraystretch}{1}
%   \begin{tabular}{|>{\bfseries}p{0.6cm}|p{4cm}|} % Added borders to columns
%     \hline
%     $\boldsymbol f$ & Fundamental frequencies \\
%     \hline
%     $\sigma_n$ & Noise  \\% TODO change to dim f (no more QQQQQ)
%     \hline
%     $M$ & Number of harmonics \\
%     \hline
%     $\sigma_f$ & Inverse length scale \\
%     \hline
%     $\boldsymbol w$ & Relative weights of notes \\
%     \hline
%     $T$ & Spectral envelope (scalar) \\
%     \hline
%     $v$ & Spectral envelope (power) \\
%     \hline
%     $\boldsymbol B$ & Inharmonicity constants \\
%     \hline
%   \end{tabular}
%   \vspace{-3mm} % Adjusts space before the table
% \end{wraptable}

We aim to identify the true underlying notes of a single audioframe by comparing the values of the LML function over different possible fundamental frequencies $\boldsymbol{f}$ (i.e., states). Specifically, we infer that the true notes are those which correspond to the $\boldsymbol{f}$ that gives the highest LML. However, as established in \hyperref[subsection:covariance_function]{subsections \ref*{subsection:covariance_function}} and \hyperref[subsection:LML]{\ref*{subsection:LML}}, the LML function depends not only on $\boldsymbol{f}$, but also on seven other hyperparameters: $ \sigma_n, M, \sigma_f, \boldsymbol{w}, T, v$, and $\boldsymbol{B}$. Importantly, there are complex non-linear relationships between these hyperparameters, which means that the LML responds to \textit{combinations} of hyperparameters. Thus, if the LML for $\boldsymbol f_1$ is greater than the LML for $\boldsymbol f_2$, this could either be because $\boldsymbol f_1$ is closer to the true underlying notes, or because non-uniform interactions between $\boldsymbol f_1$ and other hyperparameters give $\boldsymbol f_1$ a higher LML despite $\boldsymbol f_1$ being \textit{farther} from the true notes. Hence, the optimal hyperparameters always maximise LML for the \textit{true} $\boldsymbol f$, avoiding the latter case above. However, finding these hyperparameters would require optimising over a highly non-convex 7-dimensional surface. This challenge is a paradigm case of the limitations of SM kernels. As Simpson, Lalchand and Rasmussen postulate, ``the key limitation in the SM kernelâ€™s performance lies not in its stationarity or expressivity, but in the optimisation procedure''  \cite{simpson_2020_marginalised}.\\ 

Interestingly, \cite{simpson_2020_marginalised} proposes stochastic techniques like nested sampling and Monte Carlo methods as computationally tractable procedures to optimise hyperparameters. However, given that our goal is not necessarily true global optimality but rather a \textit{typical} set of hyperparameters that are sufficient for score following, we use our prior investigation of the hyperparameters and their physical bases to determine initial values and then empirically investigate LML graphs, using a mixture of local optimisation and tuning to obtain adequate values. In the subsequent subsections, we discuss the methods for choosing each of the hyperparameters. We begin with hyperparameters with clear physical interpretations, which makes them more straightforward to estimate. We then hold these as constant as we optimise the remaining hyperparameters. Note that we defer selection of $\sigma_n$ to the user, since this value is context-dependent. Even during this selection stage, we significantly varied $\sigma_n$ (ranging between 0.0001 and 10) to account for changing noise levels in different recordings. 

\subsubsection{$M$: Number of Harmonics}
$M$ represents the number of harmonics. Theoretically, $M$ could be very large since the harmonic series is infinite. However, this introduces a trade-off with computational complexity, which gives us reason to approximate the spectral envelope using a smaller value of $M$. As seen in \hyperref[subsection:signal_analysis]{subsection \ref*{subsection:signal_analysis}}, each subsequent harmonic has a lower energy, and visual inspection showed that harmonics above the 10th seem negligible. Therefore, we constrained our optimisation for $M$ ranging between 7 and 14, plotting graphs of LML vs. $M$ for inspection. In \hyperref[fig:M]{Figure \ref*{fig:M}} we show these graphs for two typical audioframes. As observed, we found that across all frequencies, the LML increases monotonically with $M$, but plateaus past a threshold of about 9. Hence, we set $M=9$ as an acceptable tradeoff for all frequencies.
 
 \begin{figure}[H]
  \centering
  \begin{subfigure}[b]{0.45\textwidth}
    \centering
    \includegraphics[width=0.75\textwidth]{figs/Part_3_Implementation/Stage_1_Model_Selection/round_2/M_1.png}
    \caption{LML vs. $M$ for a single A5 note (880 Hz).}
    \label{fig:M_1}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.45\textwidth}
    \centering
    \includegraphics[width=0.75\textwidth]{figs/Part_3_Implementation/Stage_1_Model_Selection/round_2/M_2.png}
    \caption{LML vs. $M$ for a single A3 note (220 Hz).}
    \label{fig:M_2}
  \end{subfigure}
  \caption{Graphs of LML against $M$ for two piano notes 2 octaves apart.}
  \label{fig:M}
\end{figure}
 
\subsubsection{$T$ and $v$: Spectral Envelope}
$T$ and $v$ determine the overall shape of the spectral envelope as the relative weights of the $m$-th harmonic is defined by $E_m=\frac{1}{1+Tm^v}$. To obtain a starting point for joint optimisation of $T$ and $v$, we used visual inspection of typical audioframe frequency spectra, overlaying the covariance function in the frequency domain (\hyperref[equation:frequency]{equation \ref*{equation:frequency}}) and systemically testing $T$ and $v$ values. Once we had obtained a reasonable estimate, we used gradient-based methods to find a jointly optimal pair over several typical audioframes using \verb|scipy.optimize|. Since the general shape of the spectra remained largely the same across frequencies, we settled for the optimised pair of $T = 0.465, v =2.37$ for all frequencies. 



\subsubsection{$\boldsymbol{B}$: Inharmonicity Constants}{\label{subsubsection:inharmonicity}}
The inharmonicity constant $B_{f}$ depends on the behaviour of the piano strings for the note with fundamental frequency $f$. Piano strings vary in stiffness (see \hyperref[subsubsection:improving_model]{subsection \ref*{subsubsection:improving_model}}) not only over different $f$ but also over different pianos. Hence, it is difficult to generalise these values. For the piano used in this project, a dictionary was constructed to assign optimal $B_f$ values to all 88 keys. This was achieved by limited-memory BFGS, which is a quasi-Newton optimisation algorithm designed for large scale optimisation, again using \verb|scipy.optimize|.


\subsubsection{$\sigma_f$: Inverse Length Scale}
$\sigma_f$ is the standard deviation of the Gaussians in the frequency spectra (see \hyperref[subsection:modelling]{subsection \ref*{subsection:modelling}}). Thus, we can think of $\sigma_f$ as the spread of energy around the mean frequency, so higher $\sigma_f$ increases tolerance for deviation from expected frequency. In the time domain, $\sigma_f$ represents the inverse length scale, which defines the minimum time difference at which audio samples have negligible dependency (see \hyperref[fig:covariance_functions]{Figure \ref*{fig:covariance_functions}}). We began by estimating $\sigma_f$ by approximating the standard deviation of the visible peaks in our previous power spectra. This yielded $0.01$ as an order-of-magnitude starting point. Then, for audioframes of varying $\boldsymbol{f}$, we plotted LML vs. fundamental frequency for a logarithmic range of $\sigma_f$ centring around $0.01$, examining the graphs for prominent global maxima at the true fundamental frequency (see \hyperref[fig:sigma]{Figure \ref*{fig:sigma}}). Across all frequencies, we found that the smaller $\sigma_f$, the more prominent the LML peak at the true frequency. For $\sigma_f > 0.01$, there was significant error in the LML and imprecise peaks, and even false peaks at lower frequencies. We conjecture that for high $\sigma_f$, a wide range of low notes have high LML because low notes have many overtones, which allows overfitting if the tolerance for frequency deviation is too high. Finally, we settled for $\sigma_f = 0.005$.

\begin{figure}
  \centering
  \begin{subfigure}[b]{0.45\textwidth}
    \centering
    \includegraphics[width=0.75\textwidth]{figs/Part_3_Implementation/Stage_1_Model_Selection/round_2/sigma_f_1.png}
    \caption{LML vs. frequency for a single D\#4 note (311 Hz).}
    \label{fig:sigma_1}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.45\textwidth}
    \centering
    \includegraphics[width=0.75\textwidth]{figs/Part_3_Implementation/Stage_1_Model_Selection/round_2/sigma_f_2.png}
    \caption{LML vs. frequency for a single G6 note (1568 Hz).}
    \label{fig:sigma_2}
  \end{subfigure}
  \caption{LML vs. frequency for various $\sigma_f$ (red is high, green is low) for notes over two octaves apart.}
  \label{fig:sigma}
\end{figure}

\subsubsection{$\boldsymbol{w}$: Relative Weights of Note Sources}{\label{subsubsection:amplitude}}
In audioframes containing multiple notes, the relative energies of those notes will almost always be non-uniform. This is due to physical phenomena like the properties of the hammers and strings on a piano, intentional musical techniques such as \textit{voicing}\footnote{Voicing is the technique of deliberately playing simultaneous notes at different loudness for emphasis.} and microphones with non-uniform frequency responses. Thus, an LML function that has $\boldsymbol{w}$ closer to the true relative energies of the note sources should yield higher a LML. We developed a method to estimate $\boldsymbol w$ using a least-squares approach to minimise the error between the true power spectrum and the modelled one. This involved a least-squares equation of the form $\boldsymbol{A} \boldsymbol{w} = \boldsymbol{\hat{b}}$, where each column of $\boldsymbol{A}$ is the power spectrum of a constituent note (modelled using our other hyperparameters) and $\boldsymbol{\hat{b}}$ is the $\boldsymbol w$-weighted sum of those power spectra, which estimates the resulting superposition of spectra. Minimising the squared residuals between $\boldsymbol{\hat{b}}$ and the true power spectrum of a multi-note audioframe allowed us to estimate the underlying $\boldsymbol w$ given $\boldsymbol{f}$. After estimating $\boldsymbol w$, we normalise it by dividing by $|w|_1$, so ultimately $|\boldsymbol w|_1=1$. \hyperref[fig:interval_amplitudes]{Figure \ref*{fig:interval_amplitudes}} illustrates the improvements of estimating $\boldsymbol w$ by plotting the modelled spectrum using estimated weights over a true spectrum. 

\begin{figure}[H]
  \centering
  \begin{subfigure}[b]{0.45\textwidth}
    \centering
    \includegraphics[width=0.95\textwidth]{figs/Part_3_Implementation/Stage_1_Model_Selection/amplitude_bare.png}
    \caption{True power spectrum of note sources G3 and D4 with partials labelled in pink and turquoise respectively ($\boldsymbol{f} = [196, 283]$).}
    \label{fig:interval}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.45\textwidth}
    \centering
    \includegraphics[width=0.95\textwidth]{figs/Part_3_Implementation/Stage_1_Model_Selection/amplitude_overlaid.png}
    \caption{\hyperref[fig:interval]{Figure \ref*{fig:interval}} with power spectrum of modelled covariance function overlaid in orange, using amplitudes found via a least squares approach.}
    \label{fig:amplitude}
  \end{subfigure}
  \caption{Graphs of G3, D4 interval, exhibiting large relative amplitude difference (D4 is much louder) and how least squares can be used to estimate relative note source weights.}
  \label{fig:interval_amplitudes}
\end{figure}

 Although this model was closer to the true spectra, the improvements to the LML values were minimal and often inconsistent. For example, in the recording of a soft G3 and loud D4, the LML assuming uniform weighting (i.e. $\frac{\boldsymbol{w}}{{|\boldsymbol{w}|}}=[0.5,0.5]^T$) was 2398, whereas the LML using the normalised relative weights (i.e., $\frac{\boldsymbol{w}}{{|\boldsymbol{w}|}}= [0.216, 0.784]^T$) calculated from the least squares method gave us 2416. This is an insignificant increase compared to the changes in LML observed from changes in $\boldsymbol{f}$. Furthermore, the least squares calculations can only be performed once the audioframes are received, increasing runtime. Hence, we decided to simply assume $\boldsymbol w = \mathbf {\frac 1Q}$. Ultimately, this was of little consequence for score following, since our GP model is much more sensitive to notes having the wrong frequencies rather than the wrong amplitudes, due to the nature of the SM kernel used with a small value of $\sigma_f$. However, estimating $\boldsymbol w$ could be useful in other applications, such as sound synthesis.





 \section{Results and Discussion}
After tuning our hyperparameters, we finally have a fully defined GP model with a SM kernel, which can now be used to infer notes from audioframes. The key result from this chapter is shown in \hyperref[fig:LML]{Figure \ref*{fig:LML}}. The largest peak occurs at the true fundamental frequency of the audioframe, as indicated by the red dashed line. The other peaks correspond to the octaves below, where some of the partials line up. Note that the y-axis scale is logarithmic, implying that the seemingly small delta between adjacent peaks is in fact substantial, with the first difference in likelihood being a factor of $e^{326}$. Similar results hold across a range of notes and chords, with the exception of very similar chords which have many but not all notes in common and therefore share many harmonics.
      

\begin{figure}[H]
    \centering
    \includegraphics{figs/Part_3_Implementation/Stage_1_Model_Selection/LML.png}
    \caption{Final result illustrating the variation in LML for varying frequency of a single F4 piano note (349 Hz). Note the global peak at the true underlying frequency (red dashed lines). }
    \label{fig:LML}
\end{figure}


Overall, we have successfully met the requirements for this chapter as set out in \hyperref[section:aims]{section \ref*{section:aims}}. The results from the likelihood function produce a clear spike when $\boldsymbol{f}$ is the true underlying fundamental frequencies, and this is largely invariant to changes in the recording parameters. Due to our optimisations, the LML function is fairly efficient and stable to compute, as well as robust to the challenges mentioned in \hyperref[section:challenges]{section \ref*{section:challenges}}.
 


% % \section{Side Investigations}

% TODO: its interesting how changing the amplitude/ weights of different components doesn't yield large changes in probability. This will prove as an issue later on, in the wider context of score following.


