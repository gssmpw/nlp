\chapter{Stage 2: Real-Time Alignment}{\label{ch:alignment}}
In this chapter, we implement a statistical algorithm to infer score location in real time from note likelihoods calculated using the log marginal likelihood (LML) function developed in \hyperref[ch:model_selection]{chapter \ref*{ch:model_selection}}. This algorithm uses a Hidden Markov Model (HMM), where audioframes (groups of contiguous audio samples) are the observed variables and notes in the score are the latent states. We use the note LML function as emission probabilities for the HMM, and we develop a state duration model for transition probabilities. This allows us to formulate a Maximum A Posteriori (MAP) estimate for the most likely sub-sequence of latent states ending at the current time. Finally, we solve for the MAP using a `Windowed' Viterbi algorithm, which efficiently returns the most likely current score position.

%In this chapter, we combine different statistical inference methods to develop an online `Windowed' Viterbi algorithm. We start with a Hidden Markov Model (HMM), where audioframes (groups of contiguous audio samples) are our observed variables and notes in the score are our latent states. We use the log marginal likelihood (LML) function from \hyperref[ch:model_selection]{chapter \ref*{ch:model_selection}} for emission probabilities, and we develop a state duration model for transition probabilities. This allows us to formulate a Maximum A Posteriori (MAP) estimate for the most likely sub-sequence of latent states ending at the current time, thus allowing us to predict the current state during a recording. 

\section{Aims and Requirements}{\label{section:aims_and_reqs_Alignment}}
The primary goal of this chapter is to develop a statistical method for real-time score following building on the Gaussian Process (GP) model developed in \hyperref[ch:model_selection]{chapter \ref*{ch:model_selection}}. We require an online alignment algorithm which:
\begin{itemize}
    \item predicts the most probable state, given all audioframes that have been observed so far;
    \item meets the time-sensitive demands of a real-time application with no noticeable lag;
    \item is robust to the challenges mentioned in \hyperref[section:challenges]{section \ref*{section:challenges}}.
\end{itemize}
For the scope of this project, we assume that the pianist starts at the beginning of the piece and makes no significant deviations from the score.


\section{Motivation}
Hidden Markov Models (HMMs) are a natural choice to address the problem of score following, as musical signals can be modelled as a sequence of observations (audioframes) that depends upon a discrete latent state process (musical notes). If we can predict these latent states from the observations, we will have access to score position. Furthermore, HMMs are a highly suitable choice for this particular project since the log marginal likelihood (LML) function developed in \hyperref[ch:model_selection]{chapter \ref*{ch:model_selection}} provides the emission probabilities for this HMM. \\

Additionally, the use of transition probabilities in a HMM presents the opportunity to incorporate knowledge of the score's \gls{rhythm}. This is because rhythm provides information about when the next note is likely to occur, and this information can be encoded in the HMM's transition probabilities. Thus, we make use of both pitch and note duration, the two perceptual features established to be helpful for score following in \hyperref[subsection:score_influences]{subsection \ref*{subsection:score_influences}}.


% \section{Assumptions}{\label{section:assumptions_alignment}}
% We first make a couple of assumptions, in-addition to the assumptions stated in \hyperref[section:assumptions]{section \ref*{section:assumptions}}:
% \begin{itemize}
%     \item 
%     \item The pianist makes no mistakes and starts and finishes at the beginning and end of the piece.  
% \end{itemize}


\section{Hidden Markov Model}{\label{section:HMM}}
A Hidden Markov Model (HMM) is a tool for modelling probability distributions over sequences of observations that depend upon some latent (or `hidden') process. HMMs have been extensively used in many applications, such as speech recognition, gene sequencing, signal processing and, of course, score following (refer to \hyperref[section:literature_review]{section \ref*{section:literature_review}}). 

\subsection{Hidden Markov Model for Score Following }
% Note $\{\boldsymbol{x}_t\}_{t=1}^{T}$ therefore defines our sample times for the corresponding $t$-th audio frame.
Let $\boldsymbol{y}_n$ represent the $n$-th observed variable (i.e. the $n$-th of $N$ audioframes). Then, let $s_n \in \{1, \dots , K\}$ represent the latent states we would like to predict (i.e. the underlying notes), sorted by time in the score. $s_n$ can take one of $K$ discrete states, such that $s_n=k$ means that the latent state at audioframe $n$ is the $k$-th chord or note in the list of possible states. \hyperref[fig:HMM]{Figure \ref*{fig:HMM}} shows a representation of this HMM, where the grey nodes are latent states and the white nodes are observations. The sequence of latent states is a stochastic process, where transitions are constrained such that the system can only either stay in the same state (self-transition) or move to the next state (advance-transition). This is because we assume that the pianist makes no mistakes, so there is no skipping of states or backtracking. Moreover, we can assume a first-order HMM, since the transition probabilities do not depend on previous states.\footnote{Since we ultimately implement a state-duration model, the transition probabilities are determined by the current state and number of self-transitions \textit{d}, but not previous states. Thus, this model might most accurately be called a \textit{time-varying} first-order HMM.} Hence, this a left-to-right first-order Markov chain, which is illustrated in the state transition diagram in \hyperref[fig:latent]{Figure \ref*{fig:latent}}. 

\noindent % Ensures no indentation of paragraphs
\begin{minipage}{0.46\textwidth}
\centering
    \includegraphics[width=0.85\linewidth]{figs/Part_3_Implementation/Stage_2_Alignment/HMM.png}
    \captionof{figure}{Illustration of the HMM. Latent states $s_n$ are represented by grey nodes and the observed variables $\boldsymbol{y}_n$ are represented by white nodes.}
    \label{fig:HMM}
\end{minipage}%
\hfill % Adds horizontal space between the figures
\begin{minipage}{0.46\textwidth}
\centering
    \includegraphics[width=0.85\linewidth]{figs/Part_3_Implementation/Stage_2_Alignment/latent.png}
    \captionof{figure}{Illustration of the left-to-right first-order state transition diagram for the latent states $s$ (represented as grey nodes). $T_{i,j}$ represents the transition probability from state $j$ to state $i$.}
    \label{fig:latent}
\end{minipage}\\


The joint distribution of the sequence of latent states and observations can be written as:

\begin{equation} \label{eq:joint_distribution}
P(\mathbf{y}_{1:N}, s_{1:N}) = P(s_1)P(\mathbf{y}_1|s_1) \prod_{n=2}^N P(s_n|s_{n-1})P(\mathbf{y}_n|s_n)
\end{equation}

 As we assume the pianist starts at the beginning of the piece, we set the initialisation probability to 1, such that $P(s_1) = P(s_1 = 1) = 1$.  



\section{State Duration Model}{\label{section:state_duration_model}
HMMs contain a transition probability function, represented above as $P(s_n|s_{n-1})$. Unlike typical HMMs, whose transition probabilities can be specified by a constant transition matrix, in the HMM for our score follower, the transition probabilities depend on how long the current note has already been played for. This is because the longer a note has been sustained already, the higher the probability of an advance-transition, whilst a note that has just begun is most likely to continue (self-transition). Similar to note pitch information, we can extract note duration information from a MIDI. Hence, we can use estimates of state durations to approximate the likelihood of state transitions. This enables our model to account for not only pitch, but also \gls{rhythm}. We discuss our state duration model and our estimates of note durations, respectively, in the following two subsections. Note that our approach differs from the formal State Duration HMMs in \cite{Dewar_2012}. This presents an area for possible future work. 

\subsection{Geometric Distribution for State Transitions}
We use a geometric distribution (over non-negative support) to model the number of self-transitions, $Z$, before an advance-transition, such that $Z \sim Geo(p)$ for some probability $p$. The mean of this distribution is $\mathbb E [Z]$, the expected number of self-transitions before an advance-transition. Intuitively, for a given note, $\mathbb E [Z]$ equals the expected number of audioframes that fit in that note. $\mathbb E[Z]$ determines $p$, the probability of an advance transition in our geometric distribution, as $p = \frac{1}{1+\mathbb{E}[Z]}=1- \frac{\mathbb{E}[Z]}{1 + \mathbb{E}[Z]}$. \\

The CDF of the geometric distribution allows us to calculate the transition probabilities, given $d$, the number of self-transitions that have already occurred. If $d$ self-transitions have already occurred, the probability of another self-transition equals $P(Z>d) = 1-P(Z\leq d)$. Since we exclude all transitions except self- and advance-transitions, the probability of an advance-transition equals the geometric CDF, or $1-(1-p)^{d+1}$.\\

Substituting $p=1-\frac{\mathbb E[Z]}{1+\mathbb E[Z]}$, we can formulate our transition function between two consecutive states as follows:
\begin{equation}
P_d(s_n=i|s_{n-1}=j) = 
\begin{cases}
\big( \frac{\mathbb E[Z]}{1+ \mathbb E[Z]} \big )^{d+1} & i=j\\[1.5ex]
    1 - \big( \frac{\mathbb E[Z]}{1+ \mathbb E[Z]} \big )^{d+1} & i=j+1 \\[1.5ex]
    0 & \text{otherwise}
\end{cases}
\end{equation}

We can then use this transition function to populate elements of our transition matrix:\footnote{Here, $T^d$ does not refer to exponentiation, but rather the transition matrix $T$ after $d$ self-transitions.} $\boldsymbol{T}^d_{i,j} = P_d(s_n=i|s_{n-1}=j)$. 


\subsection{Estimating $\mathbb E [Z]$}

A rudimentary approximation of $\mathbb E[Z]$ assumes that, on average, the pianist plays perfectly at the tempo specified in the MIDI. Then, the expected number of audioframes in a note is given by $\mathbb{E}[Z] = \verb|time_to_next| \times f_s$, where $\verb|time_to_next|$ is the length of this note in seconds (specified by the MIDI) and $f_s$ is the audioframe sampling frequency. In this method, $\mathbb E[Z]$ can be precomputed for every note state in the score.\\

% To estimate the probability of an advance-transition, we use a cumulative geometric distribution (of the first kind) where the probability of a state-transition after $d$ self-transitions is defined as:
% \begin{equation}
%     P(Z=d) = \sum_{z=1}^d (1-p)^z p
% \end{equation}

% where the variable $Z$ represents the number of self-transitions, and $p$ represents the probability of an advance-transition.  To find $p$, we need to calculate the expected number of transitions for each state, $\mathbb{E}[Z]$, since $\mathbb{E}[Z] = \frac{1-p}{p} \xrightarrow{} p = \frac{1}{1 + \mathbb{E}[Z]}$. One method would be to directly calculate this from the note duration of each state (\verb|time_to_next|) and the sample frequency $f_s$ as:

% \[
% \mathbb{E}[Z] = \verb|time_to_next| \times f_s
% \]


However, the method above does not consider that performers almost always deviate in tempo from the one specified in the MIDI. The performer may continually change the tempo, both due to expression and error, so a good score follower should adjust to local fluctuations in tempo. Thus, we calculate a tempo conversion factor, \verb|conversion_rate|, which provides a local estimate of the number of audioframes per second of MIDI time (not real time). For each of the preceding notes, we know both the MIDI note length in seconds (\verb|time_to_next|), as well as the observed value of how many audioframes corresponded to that note ($Z$). Thus, we obtain the \verb|conversion_rate| by computing a moving average of $\frac {Z}{\texttt{time\_to\_next}}$ over the preceding $h$ notes (where we can adapt $h$ to suit the performance). This allows us to locally estimate $\mathbb{E}[Z] = \texttt{conversion\_rate}\times \texttt{time\_to\_next}$, using the current note's MIDI duration as \verb|time_to_next|. Although this method allows adjusting for tempo, it does not allow precalculating $\mathbb E[Z]$ for all notes. Nonetheless, since the moving average is not a computationally intense calculation, this does not compromise the the HMM's real-time performance.

% \mathbb E[Z]$ by multiplying $\has a time-duration Essentially, this involved appending a counter variable $d$ to list $\textbf{d}$ at each advance-transition, and calculating the element-wise division of $\frac{\textbf{d}}{\texttt{ times\_to\_next}}$ (where \verb|times_to_next| is a list of all state durations). Then, the moving average of these values gave us a local variable \verb|conversion_rate| to calculate the next $\mathbb{E}[Z]$ as:
% \[
    


\section{The Viterbi Algorithm}{\label{section:general_viterbi}}
Given an HMM, the Viterbi algorithm allows us to predict the most likely historical sequence of latent states (notes) given \textit{all} observations (audioframes). In its original form, the Viterbi algorithm is more suitable for score alignment than score following, since it requires all audioframes to optimise the sequence of notes (see \hyperref[subsection:score_following_v_alignment]{subsection \ref*{subsection:score_following_v_alignment}}). However, in our real-time application, we only have access to the first $n$ audioframes. Importantly, upon receiving a new audioframe, we cannot simply append a new predicted state to the previously predicted sequence (i.e., a greedy algorithm), since the new observation may alter our beliefs about past states. Due to the computational complexity of the general Viterbi algorithm, it is infeasible to re-execute the whole algorithm after each audioframe. \\

As outlined in \cite{rmek_2007_the}, there exist online Viterbi algorithms, including ones which are still guaranteed to produce optimal solutions, like the short-time Viterbi algorithm in \cite{bloit_2008_shorttime}. However, the short-time Viterbi algorithm does not provide predictions at \textit{every} observation (audioframe). Another potential solution could be to greedily use a `$k$-best' Viterbi algorithm, where we track the most probable $k$ paths to the current audioframe \cite{Brown2010DecodingHU}. However, given the sequential structure of music, the sparsity of our transition matrix, and the constraints imposed by the state duration model, we know that the $k$ most likely paths are likely close to one another, seldom differing by more than a certain number (\textit{window size}) of states. To take advantage of the structure of our HMM, we opt to implement a `Windowed' Viterbi algorithm, which is still suitable for online use. First, however, we examine the general Viterbi algorithm.

\subsection{The General Viterbi Algorithm}

To predict the sequence $s_{1:N}$ based on the observed data $\boldsymbol{y}_{1:N}$, we use the Maximum A Posteriori (MAP) estimate, which involves finding $s_{1:N}$ that maximises the posterior:
\[
\boldsymbol{s}^* = \underset{\boldsymbol{s}}{\operatorname{arg\,max}} \; P(s_{1:N}|\boldsymbol{y}_{1:N}) = 
 \underset{\boldsymbol{s}}{\operatorname{arg\,max}} \; \frac{P(\boldsymbol{y}_{1:N},s_{1:N})}{P(\boldsymbol{y}_{1:N})} =  \underset{\boldsymbol{s}}{\operatorname{arg\,max}} \; P(\boldsymbol{y}_{1:N},s_{1:N})
\]

Taking logarithms and substituting \hyperref[eq:joint_distribution]{equation \ref*{eq:joint_distribution}} gives:
\begin{equation}
    \boldsymbol{s}^* = \underset{\boldsymbol{s}}{\operatorname{arg\,max}} \bigg\{\log P(s_1) + \log P(\boldsymbol{y}_{1}|s_1) + \sum_{n=2}^N \big[
\log P(\boldsymbol{y}_{n}|s_n) + \log P(s_n|s_{n-1})\big] \bigg\}
\end{equation}
The first two terms are invariant to $s$, since we define $s_1=1$ (we always start at the beginning of the piece). The first term in the sum, $\log P(\boldsymbol{y}_{n}|s_n)$, is the log emission probability, which we obtain using the Gaussian Process (GP) LML function from \hyperref[subsection:LML]{subsection \ref*{subsection:LML}}. Since the GP LML takes hyperparameters $\boldsymbol{\theta}$ and not a state $s_n$, we encode $s_n$ into a corresponding $\boldsymbol{\theta}$ using the hyperparameter $\boldsymbol{f}$ (see \hyperref[subsection:covariance_function]{subsection \ref{subsection:covariance_function}}). The second term in the sum, $\log P(s_n|s_{n-1})$, is the log of the transition probability from \hyperref[section:state_duration_model]{section \ref{section:state_duration_model}}.\\



The Viterbi algorithm solves for $\boldsymbol{s^*}$ with dynamic programming: it uses a bottom-up approach to calculate the most likely sub-sequence until each audioframe. This involves a maximisation routine at each audioframe via an exhaustive search over all possible next states. We start at audioframe 1, which has a single trivial solution. Then, we build up a table of possible paths, calculating the highest probability subsequence to each state after each audioframe. Once the subproblem spans all $N$ audioframes, we have arrived at an optimal solution, and can complete a final traceback to calculate the entire optimal sequence $\boldsymbol{s^*}$. This algorithm has a computational complexity of $\mathcal{O}(K^2 N)$, where $K$ is the number of note states and $N$ is the number of audioframes. Refer to \cite{viterbi} for more details of the Viterbi algorithm.\\

As outlined in \hyperref[section:HMM]{section \ref*{section:HMM}}, our HMM is special in that it is a left-to-right first-order Markov chain. Hence, our Viterbi algorithm can be adapted to be computationally simpler than the general case, as the transition matrix $\boldsymbol{T^d}$ is sparse: $T^d_{i,j}=0$ unless $i-j \in \{ 0,1\}$. We show this in the trellis diagram in \hyperref[fig:trellis]{Figure \ref*{fig:trellis}}, which illustrates all possible sequences of states across the first five audioframes. This diagram shows that at each audioframe except the first, there are only two possible previous states. Nonetheless, the requirement that we provide predictions of score location in real-time requires us to further optimise the Viterbi algorithm.
% \vspace{-0.7cm}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.45 \textwidth]{figs/Part_3_Implementation/Stage_2_Alignment/trellis.png}
    \caption{Trellis diagram of the set of possible paths during the first five audioframes of the Viterbi algorithm. Rows represent the different states $s=k$, and columns represent each audioframe $\boldsymbol{y}_n$. The values in the $n$-th column of the dynamic programming matrix, $\boldsymbol \Pi$, are defined by the total product of the path across the first $n$ audioframes (or alternatively the sum of the log probabilities), where $T^d_{i,j}$ represents the transition probabilities and $E_{i,j}$ represents the emission probabilities. Note that $T^d_{i,j}$ varies with $d$, the number of self transitions that have already occurred, as detailed in \hyperref[section:state_duration_model]{section \ref*{section:state_duration_model}}.  }
    \label{fig:trellis}
\end{figure}

\subsection{Adapting the Viterbi Algorithm}{\label{subsection:adjusting_viterbi}}

For the reasons introduced in \hyperref[section:general_viterbi]{section \ref*{section:general_viterbi}}, we design an approximately optimal `Windowed' Viterbi algorithm, in which we only examine states which sit within a \textit{window} of constant length $\ell$ that advances alongside the current note state, $s_n$. The trellis diagram in \hyperref[fig:windowing]{Figure \ref*{fig:windowing}} depicts the execution of this algorithm with a window length $\ell=6$ that shifts forward once its position within the window exceeds a threshold of $\varphi=4$. 

\begin{figure}[H]
  \centering
  \begin{subfigure}[b]{0.49\textwidth}
    \centering
    \includegraphics[width=0.8\textwidth]{figs/Part_3_Implementation/Stage_2_Alignment/unpruned.png}
    \caption{The trellis diagram of a general Viterbi algorithm for a left-to-right first-order Markov chain for a piece with 10 notes. We see that with each new audioframe, the number of states searched increases, until we reach the last state. }
    \label{fig:unpruned}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.49\textwidth}
    \centering
    \includegraphics[width=0.8\textwidth]{figs/Part_3_Implementation/Stage_2_Alignment/pruned.png}
    \caption{Trellis diagram of our Windowed Viterbi algorithm, where the \textit{window} size is 6 and the threshold position in the window is 4. The red nodes represent the nodes which cross the threshold and cause the next iteration to advance the window by one.}
    \label{fig:pruned}
  \end{subfigure}
  \caption{Two trellis diagrams illustrating the general and Windowed Viterbi algorithm.}
  \label{fig:windowing}
\end{figure}


The inner for loop of this algorithm populates the $n$-th column of the probability matrix $\boldsymbol{\Pi}$, where $\boldsymbol\Pi[i,j]$ is the probability of the most likely path to state $s_i$ at audioframe $\boldsymbol y_j$.

\begin{algorithm}
\footnotesize
\caption{$n$-th inner loop of Windowed Viterbi Algorithm}
\begin{algorithmic}

\Require audioframe $\boldsymbol{y}_{n}$, path probability matrix $\boldsymbol{\Pi}$, window start $w$, window length $\ell$,\\ \ \ \ \ \ \ \ \ \ \ \  window threshold $\varphi$, previous state $s_{n-1}$, transition probabilities $\boldsymbol T^d$
\For{$k \gets w$ to $w+ \ell$}
    \State $\boldsymbol{\Pi}[k, n] \gets \max \big \{\mathrm{LML}(\boldsymbol{y}_{n}|s_n=k) + T_{k,k}^{d} + \boldsymbol{\Pi}[k, n-1] \ \ ,\ \  \mathrm{LML}(\boldsymbol{y}_{n}|s_n=k) + T_{k-1,k}^{d} + \boldsymbol{\Pi}[k-1, n-1]\big \}$
\EndFor

% \State \textsc{send} $new\_state$
% \State \textsc{calculate} $new \ d$
\State $s_n \gets \mathrm{argmax}_k \big \{\boldsymbol{\Pi}[k, n]\big \}$
\If{$s_n > w + \varphi$}
    \State $w \gets s_n - \varphi$
\EndIf
\end{algorithmic}
\end{algorithm}



Owing to the high sampling frequency compared to the relatively slow note changes in real music, we would expect many more self-transitions than in this toy example, resulting in a more horizontal trajectory through the trellis. This is the reason the Windowed Viterbi algorithm's approximated solution is acceptable. In the unmodified Viterbi algorithm, since we have many self-transitions, probabilities outside the window will approach zero since we repeatedly multiply by near-zero probabilities. Hence, we can justify modelling the transition probabilities to and from these states as zero, effectively pruning all paths outside the window.



\section{Results and Discussion}{\label{section:stage_2_results}}
Testing the success of Stage 2: \textit{Real-Time Alignment} of the score follower involved recording short snippets of piano pieces, running the Windowed Viterbi algorithm, then overlaying the predicted states on a time-amplitude graph of the recordings. Visual inspection (and some manual time-mapping) was then used to analyse the correctness of these predictions. Note that this is a limited method for evaluation, since only pieces with visually obvious state transitions can be used.  Moreover, the method of manually checking each audioframe is tedious, highlighting the need for a live score position renderer for evaluation (see \hyperref[section:renderer]{section \ref*{section:renderer}}). \\

\hyperref[fig:intermediate_results]{Figure \ref*{fig:intermediate_results}} shows the key results from this section. We have used Bach's \textit{Prelude in C major BWV 846} because this is a \gls{monophonic} piece, so its states are visually easy to identify. For recordings where no or some \gls{sustain pedal} was used, the results were very successful, as presented in \hyperref[fig:no_pedal]{Figures \ref*{fig:no_pedal}} and \hyperref[fig:some_pedal]{\ref*{fig:some_pedal}} . However, use of very generous pedal caused results to become temperamental, as seen in the erroneous results of \hyperref[fig:loads_pedal]{Figure \ref*{fig:loads_pedal}}. \\

\begin{figure}[H]
  \centering
  \begin{subfigure}[b]{0.33\textwidth}
    \centering
    \includegraphics[width=1\textwidth]{figs/Part_3_Implementation/Stage_2_Alignment/no_pedal.png}
    \caption{Recording with no pedal. All predictions are correct.}
    \label{fig:no_pedal}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.32\textwidth}
    \centering
    \includegraphics[width=1\textwidth]{figs/Part_3_Implementation/Stage_2_Alignment/some_pedal.png}
    \caption{Recording with a realistic amount of pedal. All predictions are correct.}
    \label{fig:some_pedal}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.33\textwidth}
    \centering
    \includegraphics[width=1\textwidth]{figs/Part_3_Implementation/Stage_2_Alignment/loads_pedal.png}
    \caption{Excessive pedal and sharply decreasing loudness causing 8 mistakes.}
    \label{fig:loads_pedal}
  \end{subfigure}
  \caption{We show results from three different recordings of Bach's \textit{Prelude in C major BWV 846}. The blue lines represent the locations of each audioframe (where time between audioframes have been exaggerated to allow for visual inspection). The blue text represents predicted states by the Windowed Viterbi algorithm, and the red and green text below shows the true underlying states (green when correct, red when incorrect).}
  \label{fig:intermediate_results}
\end{figure}
% TODO -- ??


%It's unsurprising that these error cases were generally after some delay of each new note, because the relative energy of the sustained notes became significant after some decay (due to the characteristic temporal envelope of the pianoâ€” refer to \hyperref[section:duration]{section \ref*{section:duration}}). 

Analysing the errors in \hyperref[fig:loads_pedal]{Figure \ref*{fig:loads_pedal}} revealed that each audioframe was misidentified as \textit{previous} note states. Given the excessive use of sustain pedal and rapid decrease in loudness, these errors are unsurprising, because pedal allows notes from previous note states to continue sounding, producing an audioframe that consists of notes from both previous and current note states. One way to mitigate this problem was through the observation that, since the frequencies from previous note states are not present in our hyperparameter $\boldsymbol{f}$, the sustain pedal was essentially adding extra noise. We therefore tried increasing the model noise, $\sigma_n$, which had some improvement on the overall results for pedalled piano pieces. However, considering that sustain pedal is \textit{intended} to cause pitch deviation from notes in the score, the problem should instead be addressed by refining our GP model so that it matches the pitches that actually sound, not just the notes in the score. \\

Hence, we attempted to account for pedal using data present in the MIDI file corresponding to the use of sustain pedal. In the original model, we only detect notes that are notated as ongoing, while in the new model, we additionally detect notes which are currently being sustained by the pedal. However, performers usually deviate from the exact pedal markings in a piece. We also tried adding notes from several consecutive preceding states to current ones to model a   configurable amount of pedal. However, both of these methods failed to provide consistent improvement, so we do not typically utilise them in our model (though there is the option to use the former, by setting a $\verb|sustain|$ parameter to $\verb|True|$).\\


In summary, it is clear that these results offer significant insight into the performance of our completed score follower.  If we constrain our recordings to moderate pedal, the initial aims of this section are met (\hyperref[section:aims_and_reqs_Alignment]{section \ref*{section:aims_and_reqs_Alignment}}), since we have a near-optimal sequence finder that         works well on simple pieces in typical circumstances. Further, the algorithm runs in real-time, at least for short recordings, showing promise and potential for a real-time score follower.     