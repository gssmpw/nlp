\section{Experiments}


The \textbf{MEDQA} dataset is a free-form, multiple-choice open-domain \gls{qa} data set specifically designed for medical \gls{qa}. Derived from professional medical board exams, this dataset presents a significant challenge as it requires both the retrieval of relevant evidence and sophisticated reasoning to answer questions accurately. Each question is accompanied by multiple-choice answers that demand a deep understanding of medical concepts and logical inference, often relying on evidence found in medical textbooks. For this study, the test partition of the MEDQA dataset, comprising approximately 1,200 samples, was used \cite{jin2021disease}.

The \textbf{MedMCQA} dataset is another multiple-choice question-answering dataset tailored for medical \gls{qa}. Unlike MEDQA, which is derived from board exam questions, MedMCQA offers a broader variety of question types, encompassing both foundational and clinical knowledge across diverse medical specialties. In this study, the MedMCQA development set, containing approximately 4,000 questions, was used to benchmark against other models \cite{pmlr-v174-pal22a}.



This study employed the MEDQA and MedMCQA datasets to benchmark and evaluate medical \gls{qa} systems. These datasets serve as challenging testbeds for open-domain \gls{qa} tasks due to their demands for multi-hop reasoning and the integration of domain-specific knowledge. The relevance of MEDQA in the real world, together with the diverse question styles and extensive development set of MedMCQA make them ideal for advancing the development of robust \gls{qa} models capable of addressing medical inquiries. We utilize \textit{GPT-4o-mini} as the backbone of the implementation for both \gls{mkg} and \gls{myrag}, leveraging its capabilities with approximately \(\sim 8B\) parameters. This model serves as the core component, enabling advanced reasoning, \gls{rag}, and structured knowledge integration.


\subsection{Medical Knowledge Graph}
The \gls{kg} was dynamically constructed by integrating search items, contextual information, and relationships derived from textbooks and search queries from the PubMed engine for each question in the dataset. This data was processed and stored in a Neo4j database. The key features of the knowledge graph include:

\begin{enumerate}
    \item \textbf{Dynamic Node and Relationship Creation}: Nodes are dynamically generated for search items, and relationships between these nodes are established based on their relevance and predefined relationship types.
    
    \item \textbf{Bidirectional Relationships}: To ensure a comprehensive representation, the graph includes both forward and reverse relationships between nodes, enhancing its utility for diverse queries.
    
    \item \textbf{Relevance Scoring}: Each relationship is enriched with descriptive annotations and a confidence score, quantifying the strength of the association and aiding in prioritizing relevant connections.
    
    \item \textbf{Summarization}: Concise summaries for each search item are included, derived from contextual data. A confidence score accompanies each summary to indicate its reliability.
    
    \item \textbf{Integration with Neo4j}: The entire graph is stored in a Neo4j database, leveraging its graph-based query capabilities for efficient analysis and retrieval.
\end{enumerate}

A snapshot of a portion of the knowledge graph is shown in Figure \ref{fig:model_schema}.B, illustrating its structure and relationships.

This \gls{mkg} serves as the foundational information source for the \gls{myrag} framework during the inference phase. The evaluation of \gls{mkg} confirmed its robustness and reliability, with experts \glspl{llm} such as GPT-4 achieving high precision (e.g. $~$9/10). These results underscore the effectiveness of \gls{mkg} in supporting medical reasoning and decision-making, as detailed in Appendix \ref{app:mkd-analysis}.

\subsection{Performance Comparison}
\begin{table*}[h!]
\small
\centering
\caption{Comparison of LLM models on the MEDQA Benchmark.}
\label{tab:medqa_comparison}
\renewcommand{\arraystretch}{0.9}
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}lcccccc@{}}
\toprule
\textbf{Model}            & \textbf{Model Size} & \textbf{Acc. (\%)} & \textbf{F1 (\%)} & \textbf{Fine-Tuned} & \textbf{Uses CoT} & \textbf{Uses Search} \\
\midrule
Med-Gemini \cite{saab2024capabilities}               & $\sim$1800B                                     & 91.1                   & 89.5                   & \yesmarker          & \yesmarker        & \yesmarker          \\
GPT-4 \cite{nori2023can}                     & $\sim$1760B                                     & 90.2                   & 88.7                   & \yesmarker          & \yesmarker        & \yesmarker          \\
Med-PaLM 2 \cite{singhal2025toward}               & $\sim$340B                                     & 85.4                   & 82.1                   & \yesmarker          & \yesmarker        & \nomarker           \\
Med-PaLM 2 (5-shot)       & $\sim$340B                                     & 79.7                   & 75.3                   & \nomarker           & \yesmarker        & \nomarker           \\
\gls{myrag}                 & $\sim$8B                                     & 73.9                   & 74.1                   & \nomarker          & \yesmarker        & \yesmarker           \\
Meerkat\cite{kim2024small}              & 7B                                       & 74.3                   & 70.4                   & \yesmarker          & \yesmarker        & \nomarker           \\
Meditron \cite{chen2023meditron}                 & 70B                                      & 70.2                   & 68.3                   & \yesmarker          & \yesmarker        & \yesmarker          \\
Flan-PaLM  \cite{singhal2023large}               & 540B                                     & 67.6                   & 65.0                   & \yesmarker          & \yesmarker        & \nomarker           \\
LLAMA-2 \cite{chen2023meditron}                  & 70B                                      & 61.5                   & 60.2                   & \yesmarker          & \yesmarker        & \nomarker           \\
Shakti-LLM \cite{shakhadri2024shakti}               & 2.5B                                     & 60.3                   & 58.9                   & \yesmarker          & \nomarker         & \nomarker           \\
Codex 5-shot CoT  \cite{lievin2024can}        & --                                     & 60.2                   & 57.7                   & \nomarker           & \yesmarker        & \yesmarker          \\
BioMedGPT  \cite{luo2023biomedgpt}               & 10B                                      & 50.4                   & 48.7                   & \yesmarker          & \nomarker         & \nomarker           \\
BioLinkBERT (base)  \cite{singhal2023large}      & --                                     & 40.0                   & 38.4                   & \yesmarker          & \nomarker         & \nomarker           \\
\bottomrule
\end{tabular}%
}
\end{table*}

Table~\ref{tab:medqa_comparison} presents a comprehensive comparison of state-of-the-art language models on the MEDQA benchmark. The results highlight the critical role of advanced reasoning strategies in achieving higher performance, such as CoT reasoning, fine-tuning, and the integration of search tools. While larger models like Med-Gemini and GPT-4 achieve the highest accuracy and F1 scores, their performance comes at the cost of significantly larger parameter sizes. These models exemplify the power of scaling combined with sophisticated reasoning and retrieval techniques.

Significantly, \gls{myrag}, despite having just 8 billion parameters, attains an F1 score of 74.1\% on the MEDQA benchmark, surpassing models like Meditron, which possess 70 billion parameters without needing any fine tuning. This highlights \gls{myrag}'s exceptional efficiency and proficiency in utilizing CoT reasoning and external evidence retrieval. The model leverages tools such as PubMedSearch and WikiSearch to dynamically integrate domain-specific knowledge dynamically, thereby improving its ability to address medical questions. Examples of \gls{qa} interactions, including detailed search items and reasoning for question samples, are provided in Appendix \ref{app:examples-medqa}. These examples are organized in Tables \ref{table:search_guidance_1}, \ref{table:search_guidance_2}, \ref{table:search_guidance_3}, and \ref{table:search_guidance_4}, drawn from the MEDQA benchmark.

On the MedMCQA benchmark, as shown in Table~\ref{tab:medmcqa_comparison}, \gls{myrag} achieves an accuracy of 66.34\%, even outperforming larger models like Meditron-70B and better than Codex 5-shot CoT. This result underscores \gls{myrag}'s adaptability and robustness, demonstrating that it can deliver competitive performance even against significantly larger models. Its ability to maintain high accuracy on diverse datasets further highlights the effectiveness of its design, which combines CoT reasoning with structured knowledge graph integration and retrieval mechanisms.



\begin{table}[h!]
\small
\centering
\caption{Comparison of Models on the MedMCQA.}
\label{tab:medmcqa_comparison}
\renewcommand{\arraystretch}{0.9}

\begin{tabular}{@{}lcc@{}}
\toprule
\textbf{Model}                              & \textbf{Model Size}           & \textbf{ Acc. (\%)} \\
\midrule
\gls{myrag}                 & $\sim$8B                                     & \textbf{66.34}\\
Meditron \citep{chen2023meditron}                     & 70B                          & 66.0 \\

Codex 5-shot \citep{lievin2024can}                            & --                           & 59.7 \\
VOD \citep{lievin2023variational}                           & --                           & 58.3 \\
Flan-PaLM \citep{singhal2022large}                        & 540B                         & 57.6 \\
PaLM                       & 540B                         & 54.5 \\

GAL                        & 120B                         & 52.9 \\
% Flan-PaLM                   & 62B                          & 46.2 \\
% PaLM                        & 62B                          & 43.4 \\
PubmedBERT \citep{gu2021domain}                & --                           & 40.0 \\
SciBERT \citep{pal2022medmcqa}              & --                           & 39.0 \\
BioBERT \citep{lee2020biobert}                  & --                           & 38.0 \\
BERT \citep{devlin2018bert}                  & --                           & 35.0 \\
% Flan-PaLM                    & 8B                           & 34.5 \\
% BLOOM                       & --                           & 32.5 \\
% OPT                          & --                           & 29.6 \\
% PaLM                         & 8B                           & 26.7 \\
\bottomrule
\end{tabular}%

\end{table}


Overall, \gls{myrag}'s results on MEDQA and MedMCQA benchmarks solidify its position as a highly efficient and effective model for medical \gls{qa}. By leveraging CoT reasoning, search tools, and external knowledge sources, \gls{myrag} not only closes the gap with much larger models but also sets a new standard for performance among smaller-sized models.
\subsection{Impact of Search Tools and CoT Reasoning on AMG-RAG Performance}

Figure~\ref{fig:cot_kg_comparison} and Table~\ref{tab:cot_kg_metrics} demonstrate the impact of integrating search tools such as PubMedSearch and WikiSearch on the performance of \gls{myrag} when applied to the MEDQA dataset. The inclusion of these search capabilities significantly improves accuracy and F1 scores by providing access to relevant external evidence, which is critical for addressing medical questions. Among the search tools, PubMedSearch outperforms WikiSearch, likely due to its more focused and domain-specific content, which better aligns with the nature of medical \gls{qa} tasks.

Additionally, the impact of CoT reasoning and \gls{mkg} integration on \gls{myrag} performance is highlighted in the same figure and table. The results reveal that the removal of either CoT reasoning or KG integration leads to a substantial drop in accuracy and F1 scores. This underscores the indispensable role of structured reasoning and domain-specific retrieval in enhancing the systemâ€™s ability to generate accurate and evidence-backed answers.



\begin{table}[h!]
\small
    \centering
    \caption{Performance metrics for \gls{myrag} model with and without CoT and Knowledge Graph integration with different search tools for MEDQA dataset.}
    \label{tab:cot_kg_metrics}
    \begin{tabular}{lccc}
    \hline
    \textbf{Model}                  & \textbf{Acc. (\%)} & \textbf{F1-Score} & \textbf{Recall} \\
    \hline
    PubMedSearch        & 73.92                  & 0.7410            & 0.7392          \\
    WikiSearch          & 70.62                  & 0.7067            & 0.7062          \\
    No Search           & 67.16                  & 0.6696            & 0.6716          \\

    No Search \& CoT    & 66.69                  & 0.6655            & 0.6669          \\
    \hline
    \end{tabular}
\end{table}
\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{latex/Figures/wiki_conf_matrix.png}
    \caption{Confusion matrix for \gls{myrag} with and without CoT and Knowledge Graph integration on MEDQA dataset.}
    \label{fig:cot_kg_comparison}
\end{figure}
\subsection{Improving QA in Rapidly Changing Medical Domains}

Figure~\ref{fig:performance_clusters} illustrates the performance of various models across different question domains, including Neurology and Genetics. The \gls{myrag} model consistently outperforms other approaches, showcasing its superior adaptability and robustness in these rapidly evolving and knowledge-intensive fields. This exceptional performance stems from its ability to seamlessly integrate external sources of information and evidence. By leveraging PubMed searches, the \gls{myrag} model dynamically retrieves the latest medical research and continuously updates the \gls{mkg}, ensuring that it remains relevant and up-to-date. This dynamic updating process not only enhances the model's ability to reason across multiple domains but also allows it to address complex, multi-hop questions with greater accuracy and depth.

\begin{figure}[t]
    \centering
    \includegraphics[width=.8\columnwidth]{latex/Figures/compariong_clusters_medqa.png}
    \caption{Performance comparison across different question domains in the Neurology and Genetics fields.}
    \label{fig:performance_clusters}
\end{figure}
