\section{Related Work}

\subsection{LVLMs}

With the rapid advancement of large language models (LLMs) \cite{ouyang2022training, zhao2023survey,pan2024fallacy,wu2024akew,wu2024antileak,wu2024fastopic,wu2024survey},
integrating their general intelligence into multimodal domains has garnered significant interest. This has led to the emergence of large vision-language models (LVLMs) \cite{ye2023mplug, zhu2023minigpt, liu2023llava, li2023otter, dai2305instructblip, bai2023qwen}, designed to understand and generate multimodal content under instructions. Most LVLMs adopt a two-stage training paradigm: multimodal alignment pre-training followed by instruction tuning, where an alignment module processes multimodal inputs before passing them to an LLM for response generation. For instance, mPLUG-Owl \cite{ye2023mplug} pre-trains both the encoder and alignment module before fine-tuning LLaMa \cite{touvron2023llama} with low-rank adaptation. LLaVA \cite{liu2023llava} pre-trains the alignment network and fine-tunes it alongside Vicuna \cite{chiang2023vicuna} on constructed instructions. In contrast, MiniGPT-4 \cite{zhu2023minigpt} only fine-tunes the cross-modal alignment network while keeping other components frozen.
\subsection{Hallucination in LVLMs}


\begin{figure*}[t]
    

    \centering
    \includegraphics[width=0.99\textwidth]{images/Model.pdf}
    \caption{The overall architecture of CutPaste\&Find. The system extracts scene graphs from captions, verifies object existence and attributes using a knowledge base, and employs object detection, visual similarity, and VQA for validation. A scaling factor adjusts similarity scores, enhancing reliability, with hallucinated entities and relations highlighted in red. }
    \label{fig:model}
   \vspace{-10pt}
\end{figure*}

Despite their strong capabilities, LVLMs often suffer from hallucination issues. To address this, several benchmarks \cite{fu2023mme, xu2023lvlm, Li-hallucination-2023, lovenia2023negative, jing2023faithscore, chen2024unified} have been proposed to evaluate hallucination severity in LVLMs. 
Existing mitigation strategies can be categorized into three groups. The first and most common approach \cite{liu2023mitigating, gunjal2023detecting, lee2023volcano, wang2023vigc} relies on instruction tuning and retraining. LRV-Instruction \cite{liu2023mitigating} constructs a diverse dataset with both positive and negative instructions, while \cite{wang2023vigc} employs iterative instruction generation to enhance diversity and accuracy. Volcano \cite{lee2023volcano} enables self-feedback training for response revision. However, these methods depend heavily on high-quality instruction datasets and require substantial computation.
The second group focuses on decoding strategies to mitigate hallucinations. OPERA \cite{huang2023opera} introduces a penalty-based decoding method with a rollback strategy to reduce overconfidence, while VCD \cite{leng2023mitigating} applies contrastive decoding to minimize reliance on spurious biases. However, accessing LVLMs' internal states during decoding remains challenging for general users.
The third category integrates external models to refine LVLM outputs. Woodpecker~\cite{yin2023woodpecker} and LURE~\cite{zhou2023analyzing} leverage external detectors or specialized LVLMs as revisors to improve visual understanding. 
However, these methods depend on auxiliary models rather than enhancing the base LVLM itself. 
LogicCheckGPT~\cite{wu2024logical} takes a different approach by probing logical consistency in object-attribute relationships to detect and mitigate object hallucinations.