\section{Method}

% \wxb{In this section, we introduce our approach xxx, including xxx, and xxx.}

In this section, we introduce the framework, first including Scene graph parser extraction, Object detector, Visual verification and then go through Scaling factor with visual-aid prior knowledge. We visualize the overall architecture in Figure~\ref{fig:model}.

% Our objective is to diagnose and correct the hallucinations in the response generated by LVLMs. The key challenges lie in locating the hallucinations and determining the facts, which can be organized in a structured way for final correction. 
% To this end, we break down the whole process into five subtasks: key concept extraction, question formulation, visual knowledge validation, visual claim generation, and hallucination correction. We will illustrate each step in sequence later. 
% An overview of our framework is depicted in.


% Our objective is to diagnose and correct hallucinations in the responses generated by multimodal large language models (LVLMs). The primary challenges lie in pinpointing hallucinated content and determining factual corrections in a structured manner.
% To address this, we decompose the process into five subtasks: key concept extraction, question formulation, visual knowledge validation, visual claim generation, and hallucination correction. An overview of our framework is depicted in Figure \ref{fig:framework}. Below, we detail each component.

\subsection{Scene graph parser extraction}
\label{sec:method_1}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Captions typically focus on key concepts, the initial step involves extracting these concepts from the generated sentence. Different from previous works which mainly focus on main objects mentioned in the sentence, we extract entities along with their attributes, as well as the relationships connecting these entities.
% For example, given a sentence, ``\texttt{The man is wearing a black hat.}'', the entities and the relations are extracted and serve as the center for diagnosis in the following steps. In this step, we utilize the Scene-Graph-Parser module (SGP), which provides an advanced framework for converting unstructured textual descriptions into structured scene graphs. Although LLMs have strong generalization ability and rich world knowledge, using them is more expensive in terms of cost (if calling online api) or resource (if using open source) and redundant for a not so complex task compared to SGP. 

% Given a sentence $s$, the return output of SGP is a list of entities (with attribute if applicable) $E = [ (e_1,a_1), (e_2,a_2), ... ]$ and a list of triplet relations $R = [ (s_1,r-1,o_1), (s_2,r-2,o_2), ... ]$. An example of output from SGP can be referred in Appendix \ref{appendix:sgp}.

Captions often emphasize key concepts, and the first step in our framework involves extracting these concepts from the generated sentences. Unlike prior works that primarily focus on the main objects mentioned in a sentence, we extract entities alongside their attributes and the relationships between these entities. For example, given the sentence, “\texttt{The man is wearing a black hat},” we extract entities (\texttt{man}, \texttt{hat}) and their attributes (\texttt{black}) and relationships (\texttt{wearing}). These serve as the foundation for subsequent hallucination diagnosis.

To achieve this, we leverage a Scene graph parser module (SGP), which efficiently converts unstructured textual descriptions into structured scene graphs. While large language models (LLMs) possess strong generalization capabilities and world knowledge, relying on them for this relatively straightforward task is resource-intensive and costly, particularly when APIs or open-source LLMs are used. SGP provides a more efficient alternative. Given a sentence $s$, SGP outputs two lists (a visualization is shown under Scene graph parser extraction in Figure~\ref{fig:model}):

\begin{align}
    E &= [ (e_1,a_1), (e_2,a_2), ... ] \\
    R &= [ (s_1,r-1,o_1), (s_2,r-2,o_2), ... ]
\end{align}

\noindent $e_i$ represents an entity and $a_i$ represents its attribute (if applicable), $(s_i,r_i,o_i)$ represents a relationship triplet of subject, relation, and object.

\subsection{Object detector}
\label{sec:method_2}

Using the entities extracted by the SGP, we apply an open-set object detector (OD) to locate each entity in the image. For a given entity $e_i$, if OD successfully detects it, we obtain a list of bounding boxes $B_i = \{b_{ia}, b_{ib}, ... \} $, where each bounding box $b_{ij}$  includes the top-left position $(x, y)$ and dimensions $(h, w)$. If $e_i$ cannot be detected, it is flagged as potentially hallucinated and validated using only the Visual Question Answering (VQA) module in the next step.

\subsection{Visual verification}

% This step is responsible for checking the hallucination of entities, attributes of entities and relations. We combine both modules: Visual question answering and Verification with a visual-aid knowledge base for the Validation step. 

This stage validates the existence of entities, attributes, and relationships using two complementary methods: Visual Question Answering (VQA) and Visual Similarity.

% \subsubsection{Visual question answering}

% We apply a pre-trained VQA model~\cite{li2023blip} to answer the questions conditioned on the image. Compared with mainstream LVLMs, the VQA model tends to generate shorter answers but also with fewer hallucinations and thus can be a reasonable choice. We have 3 types of formats:

\subsubsection{Visual verification with VQA}

We utilize a pre-trained VQA model~\cite{li2023blip}  to answer context-specific questions about the image. Compared to LVLMs, the VQA model produces concise responses with fewer hallucinations, making it a suitable choice. We formulate three types of questions:

\paragraph{Attribute asking} Given the entity $e_i$ and attribute $a_i$, we ask the VQA model ``\texttt{Is the $\{e_i\} \{a_i\}$?}''.

\paragraph{Relation asking} Given the triplet $s_i,r_i,o_i$, we ask the VQA model ``\texttt{Is the $\{s_i\} \{r_i\} \{o_i\}$?}''.

\paragraph{Existance asking} This format is used when the entity $e_i$ was missed from previous step~\ref{sec:method_2}. Given the entity $e_i$, we ask the VQA model ``\texttt{Is there \{$s_i$\} in the image}''.

We ask the VQA model in a binary Yes/No question, and instead of using the generated answer (``\texttt{yes}'' or ``\texttt{no}''), we base it on the scoring of ``\texttt{yes}'' token of the predicted probabilities of the output. Thus we can have a soft-predicted score $s^{qa}_i$ instead of a hard prediction.
Instead of relying on the binary outputs (\texttt{yes} or \texttt{no}), we use the probability of the \texttt{yes} token to derive a soft-predicted score $s^{qa}_i$, offering a more nuanced assessment.

\subsubsection{Visual verification with Visual Similarity}

% A second method we used to check the hallucination of the caption is using a visual-aid knowledge base to validate the cropped image of entities and relations. Given a pair of entity $e_i$ and attribute $a_i$, the model first queries a list of images $\hat{C}_i$ represented ($a_i$,$e_i$) in the data store. Then we use the pretrained visual encoder CLIP to extract the visual representation of cropped image $c_i$ and all query images $\hat{C}_i$ to get $f_i, \hat{F}_i$. We then calculate the similarity score $s^{v}_i$ between $f_i$ and $\hat{F}_i$, the less the similarity score, the higher the change of hallucination. Similar setup as attribute validation, the input triplet subject, relation, object $s_i,r_i,o_i$ is first queried to similar images from the data store.

To complement VQA, we validate hallucination using Visual-aid Knowledge Base. For an entity-attribute pair ($a_i$,$e_i$), we retrieve a list of representative images $\hat{C}_i$ from the datastore and compute visual representations $f_i$ (for the cropped image) and $\hat{F}_i$ (for retrieved images) using a pre-trained visual encoder (e.g., CLIP). The similarity score $s^{v}_i$ is calculated between $f_i$ and $\hat{F}_i$; lower scores indicate higher likelihoods of hallucination. A similar approach is applied to validate relationships $s_i,r_i,o_i$.

\subsection{Scaling factor with Visual-aid prior knowledge}

% Detecting multimodal hallucination in captions relies on accurately comparing visual representations of entities and attributes against a knowledge base. Current approaches measure similarity between the cropped image and retrieved images using pretrained visual encoders like CLIP, with similarity scores indicating the likelihood of hallucination. However, these similarity scores often fall short of ideal ranges near 1.0, even for ground-truth images. This inconsistency can misrepresent the true alignment between visual and textual data, leading to potential inaccuracies in hallucination detection.

% We introduce a scaling factor informed by visual-aid prior knowledge to address this challenge. By analyzing the similarity score distribution of ground-truth pairs, we derive a principled adjustment that aligns the similarity scores with realistic expectations. This scaling factor enables a more accurate interpretation of visual-textual alignment, leveraging the rich prior knowledge embedded in the data store. With the visual encoding method, we first calculate the similarity score between $\hat{F}_i$:

Similarity scores often deviate from ideal values near 1.0, even for ground-truth pairs. To address this, we introduce a scaling factor derived from the prior knowledge embedded in the datastore. This adjustment aligns similarity scores with realistic expectations, improving the sensitivity and robustness of hallucination detection.

\begin{figure}[t]

    \centering
    \includegraphics[width=0.45\textwidth]{images/KnowledgeBase.pdf}
    \caption{The figure illustrates a Visual-aid Knowledge Base built from meta-information and images, including objects, attributes, and relationships. A "Cut \& Paste" process segments visual elements such as objects (e.g., van), attributes (e.g., red shirt), and relationships (e.g., tree next to street) into a structured knowledge base. }
    \label{fig:kb}
    \vspace{-10pt}
\end{figure}

\paragraph{Visual Scaling Factor}
The scaling factor for visual encoding is computed as:

\vspace{-5pt}
\begin{align}
    &\tilde{S}^v_i = f(\hat{F}_i, \hat{F}_i^\intercal) \\
    &diag(\tilde{S}^v_i) = 0 \\
    &d^v_i = \max(\tilde{S}^v_i)
\vspace{-5pt}
\end{align}


\noindent where $d^v_i$ is the scaling factor for the visual encoding method. Then we calculate re-scale similarity score of an entity $e_i$:

\vspace{-5pt}
\begin{align}
    \bar{s}^{v}_i = s^{v}_i / d^v_i
\vspace{-5pt}
\end{align}

\paragraph{VQA Scaling Factor}
For the VQA module, we compute the scaling factor:

\vspace{-5pt}
\begin{align}
    & \tilde{S}^{qa}_i = [\tilde{s}^{qa}_{i1}, \tilde{s}^{qa}_{i2}, ...\tilde{s}^{qa}_{ij} ] \\
    & diag(\tilde{S}^qa_i) =0 \\
    & d^{qa}_i = max(\tilde{S}^{qa}_i)
\vspace{-5pt}
\end{align}

\noindent where $d^{qa}_i$ is the scaling factor for the visual question module. Then we calculate the re-scale similarity score of an entity $e_i$:

\vspace{-5pt}
\begin{equation}   
    \bar{s}^{qa}_i = s^{qa}_i / d^{qa}_i
\vspace{-5pt}
\end{equation}

\paragraph{Final output}

% Feeding the input generated caption $s$ and image $I$, our classifer will return the hallucination dictionary:

Given a generated caption $s$ and an image $I$, our classifier outputs a hallucination dictionary:

\begin{itemize}
    
\item Entity: $\left [ e_1: \tilde{s}_{e1}, \cdots, e_i: \tilde{s}_{ei} \right ]$

\item Attribute-entity: $\left [ e_1,a_1: \tilde{s}_{a1}, \cdots, e_i,a_i: \tilde{s}_{ai} \right ]$


\item Relation: $\left [ s_1,r_1,o_1: \tilde{s}_{r1}, \cdots, s_j,r_j,o_j: \tilde{s}_{ri} \right ]$

\end{itemize}

\noindent where $s_e, s_a, s_r$ are the existing, attribute, and relation score respectively. This structured output enables precise identification and correction of hallucinations, ensuring multimodal consistency.

% \begin{equation}
%     \begin{Bmatrix}
%      s_1,r_1,o_1: \tilde{s}_{r1} \\
%      \cdots \\
%      s_j,r_j,o_j: \tilde{s}_{ri} \\
%     \end{Bmatrix} 
% \end{equation}
% \begin{equation}
%     \begin{Bmatrix}
%      e_1,a_1: \tilde{s}_{e1} \\
%      \cdots \\
%      e_i,a_i: \tilde{s}_{ei} \\
%     \end{Bmatrix}
% \end{equation}