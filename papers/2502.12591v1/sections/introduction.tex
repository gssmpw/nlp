

\section{Introduction}

% background & problem
% Multimodal Large Language Models (LVLMs)~\cite{yin2023survey} are now flourishing in the research community, working towards Artificial General Intelligence (AGI). By exploiting powerful Large Language Models (LLMs), researchers align foreign modalities like vision with language, and develop LVLMs with various exciting capabilities~\cite{liu2023visual,ye2023mplug,zhu2023minigpt,zhang2023transfer,Qwen-VL}, such as fully describe the contents of a given image.

Large Vision Language Models (LVLMs) \cite{liu2023llava, ye2023mplug, zhu2023minigpt, li2023otter, bai2023qwen,dai2023instructblip} have exhibited impressive multimodal understanding and reasoning abilities.
% They xxx\wxb{Introduce some cases and examples of LVLMs. 1-2 sentences}.
They excel at various vision-language tasks, such as image captioning, visual question answering (VQA), and image-based dialogue, where they generate descriptive text based on visual inputs.
For example, models such as LLaVA~\cite{liu2023llava} and MiniGPT-4~\cite{zhu2023minigpt} can generate detailed captions from an image, answering complex questions about visual content.
However, LVLMs face one critical challenge: object hallucination.
This means LVLMs generate inconsistent or fabricated descriptions of a given image, such as inventing non-existent objects.
This challenge undermines the reliability and accuracy of LVLMs, severely limiting their broader applications.
To address this challenge, some hallucination detection methods have been proposed.
For example, Woodpecker~\cite{yin2023woodpecker}  is a training-free post-processing method that detects and corrects hallucinations in LVLMs.
LogicCheckGPT~\cite{wu2024logical} is a plug-and-play framework that detects and mitigates object hallucinations in LVLMs by probing their logical consistency in responses.

\begin{table}[t]
\centering
\resizebox{1.0 \columnwidth}{!}{
\begin{tabular}{l|c|c}
& \begin{tabular}[c]{@{}c@{}}Number of time\\ calling OpenAI api\\ for 1 sentence\end{tabular} & Module usage                                                           
\\ \hline
Woodpecker    & 4                                                                                            & \begin{tabular}[c]{@{}c@{}}GroundDINO, \\ BLIP2, QA2Claim\end{tabular}  \\ \hline
LogicCheckGPT & 6                                                                                            & LVLM (MPlug)                                                            \\ \hline
Our           & 0                                                                                            & \begin{tabular}[c]{@{}c@{}}SGP,  GroundDINO,\\ BLIP2, CLIP\end{tabular}
\end{tabular}}
\caption{Statistics of Calling OpenAI API and Module usage of LVLM hallucination detections.}
\label{tab:stat}
\vspace{-10pt}

\end{table}

"However, these methods have several limitations. First, they rely heavily on external API calls.
As reported in \Cref{tab:stat}, Woodpecker and LogicCheckGPT make 4-6 API calls to detect each sentence.
% This 
% xxx \wxb{Explain the bad results of this limitation, like offline use.}
This dependence also limits offline usability, making deployment infeasible in scenarios with restricted API access or privacy concerns.
Second, they require iterative interactions with LVLMs to detect hallucinations.
% For instance, LogicCheckGPT xxx.
% This xxx \wxb{Explain the bad results of this limitation.}
For instance, LogicCheckGPT generates multiple queries and performs consistency checks across responses. This iterative process not only increases latency but also amplifies token consumption, making it impractical for large-scale applications or real-time inference.

% With the remarkable advancements in large language models (LLMs) \cite{ouyang2022training, touvron2023llama, zhao2023survey}, these models have demonstrated impressive capabilities, such as text generation and instruction following.
% Recent research has focused on bringing the powerful capabilities of LLMs to multimodal models. Leveraging LLMs, large vision-language models (LVLMs) \cite{liu2023llava, ye2023mplug, zhu2023minigpt, li2023otter, bai2023qwen,dai2023instructblip} have been developed to exhibit strong multimodal understanding and reasoning abilities.

% Despite these groundbreaking advancements, LVLMs are still plagued by hallucination issues, particularly object hallucination. Object hallucination refers to the generation of inconsistent or fabricated descriptions of a given image, such as inventing non-existent objects. This issue severely limits the broader application of LVLMs, as it undermines their reliability and accuracy.

% % Motivation and Proposed Approach
% Previous works, such as Woodpecker~\cite{yin2023woodpecker} and LogicCheckGPT~\cite{wu2024logical}, have been introduced to detect hallucinations in LVLM-generated descriptions based on given images. These models have proven to be effective in identifying hallucinations. However, as shown in Table~\ref{tab:stat}, both approaches have significant limitations. For each sentence, Woodpecker makes $4$ API calls to OpenAI, and LogicCheckGPT makes 
% $6$ API calls. Furthermore, these methods require iterative interactions with LVLMs, involving multiple rounds of inference to validate hallucinations.




To resolve these limitations, we propose a novel hallucination detection framework, CutPaste\&Find. 
% It is training-free, no cost, and efficient.\wxb{"no cost" here is not specific enough. what kind of cost?}
It is training-free, offline, and computationally efficient.
Previous detection methods rely on external APIs. 
% Differently, we xxx.\wxb{Explain how your method works from the high level.}
Differently, we validate visual entities with the pipeline by utilizing off-the-shelf textual and visual modules which are less hallucinated than LVLM~\cite{Yin_2024}.
% Moreover, we xxx \wxb{Explain how your method solves the second limitation (iterative interactions)}.
Moreover, we build a visual knowledge base to improve the reliability without high-computational interaction.
% \wxb{-------------------------}

While these methods achieve strong performance, they are resource-intensive and costly due to frequent API calls and the computational overhead of LVLM inference. Moreover, they are not suitable for offline use. To address these limitations, we propose a novel framework called CutPaste\&Find that is training-free, and efficient. Our framework leverages off-the-shelf modules to preprocess and perform multi-step inference without relying on powerful APIs. %For instance, it efficiently processes inputs using readily available visual and linguistic tools.

Additionally, to further enhance our framework, we introduce a Visual-aid Knowledge Base as a robust solution for detecting multimodal hallucinations. Unlike traditional methods, our approach is grounded in a curated knowledge base built on human-annotated data, ensuring high accuracy and reliability. Specifically, this knowledge base is constructed through process called Cut-and-Paste using the Visual Genome dataset~\cite{krishna2016visualgenomeconnectinglanguage}, which incorporates rich information, including object names, attributes, relationships, and corresponding images.

The proposed method compares the visual representation similarity score of a cropped image (or the entire image) with entity-attribute pairs or triplet relations retrieved from the knowledge base. To improve the accuracy of alignment interpretation, we introduce a scaling factor with visual-aid prior knowledge. This innovation addresses the observation that similarity scores often deviate from ideal values near 1.0, even for ground-truth pairs. By incorporating this scaling factor, our framework refines similarity scores, enhancing their ability to represent true alignment between model outputs and the underlying image.

In summary, our main contributions are as follows:

\begin{itemize}[leftmargin=*,itemsep=0pt]

\item  We propose a training-free, lightweight, and efficient framework named ABC for detecting hallucinations in LVLMs. This framework employs off-the-shelf modules to perform multi-step detection with minimal resources and without reliance on costly APIs.

\item We introduce a clean and robust Visual-aid Knowledge Base to enhance hallucination validation. This knowledge base integrates prior knowledge to improve detection accuracy.

\item We evaluate the effectiveness of our framework through extensive experiments on benchmark datasets, including POPE~\cite{li2023evaluating} (with the split from \cite{wu2024logical}) and R-Bench~\cite{pmlr-v235-wu24l}. Our results demonstrate the robustness and efficiency of the proposed approach.

\end{itemize}
