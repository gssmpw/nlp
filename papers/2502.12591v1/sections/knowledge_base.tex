

\section{Visual-aid Knowledge Base}

\subsection{Cut-And-Paste}

% Our VaKb is build on Visual Genome dataset. The Visual Genome dataset provides a rich resource for Visual Question Answering (VQA) in a multiple-choice format. It features 101,174 images sourced from MSCOCO, accompanied by 1.7 million question-answer pairs, averaging 17 questions per image. Unlike the traditional VQA dataset, Visual Genome offers a more balanced distribution of six question types: What, Where, When, Who, Why, and How. Additionally, it includes 108,000 images densely annotated with objects, attributes, and relationships, enhancing its utility for fine-grained visual understanding.

Our Visual-aid Knowledge Base (VaKB) is built upon the Visual Genome dataset, a rich resource for tasks requiring fine-grained visual understanding. The VG includes 101,174 images sourced from MSCOCO, accompanied by 1.7 million question-answer pairs, averaging 17 questions per image. Unlike the traditional VQA datasets, Visual Genome offers a balanced distribution of six question types: What, Where, When, Who, Why, and How, making it particularly suitable for diverse multimodal reasoning tasks. Furthermore, it provides 108,000 images densely annotated with objects, attributes, and relationships, making it an invaluable resource for constructing comprehensive knowledge bases that bridge visual and textual modalities. We will pre-process the dataset with its annotation through process call "Cut-And-Paste". 



\input{sections/table_main}




\subsection{Datastore}

% Each image $I$ in Visual Genome contains a set of human annotations: entity, attribute, and relation. Each entity list includes the name of the object and its bounding box in (x, y, w, h) format, the attribute annotation extend with the attribute list of mentioned object, while relation contains a triplet subject, relation name, and object that the subject related to. We cropped its bounding box and saved the cropped in the database, as a result, for each key $k$, we have list of cropped images (and also the original one) corresponding to the key $k$.
% We define the key-value pair ($k_i$, $v_i$), where the key $k_i$, key should be entity $e_i$, attribute-entity $e_i,a_i$, or relation triplet $s_i,r_i,o_i$, and the value $a_i$ is the list cropped images (and also the original images, we build two different mapping). Let $f^T(\cdot)$ be the function that maps a given context $c$ to a fixed-length vector representation computed by the pre-trained text encoder (CLIP-Text).
% The datastore (K, V) is thus the set of all key-value pairs constructed from all the training examples in Visual Genome (VG) dataset $\mathcal{D}$:

% \begin{align}
% \left( \mathcal{K}, \mathcal{V} \right) = \left\{ (f^T(k_i), v_i) | (k_i, v_i) \in \mathcal{D}, k_i \in \left\{ e_i,(e_i,a_i),(s_i,r_i,o_i) \right\} \right\}
% \end{align}


Each image $I$ in the Visual Genome dataset is enriched with human annotations, including entities, attributes, and relationships. These annotations are structured as follows:

\vspace{-5pt}
\paragraph{Entities} Each entity includes the object name and its bounding box in  $(x, y, w, h)$ format.
\vspace{-5pt}
\paragraph{Attributes} Each entity is further extended with a list of associated attributes, providing additional descriptive detail.
\vspace{-5pt}
\paragraph{Relationships} Each relationship annotation is represented as a triplet $(s,r,o)$, where $s$ is the subject, $r$ is the relationship, and $o$ is the object.

To build the VaKb, we crop the bounding boxes of entities and store both the cropped and original images in a database. This ensures that for each key $k$, we have a list of cropped images and their corresponding full images. Keys are constructed from entities, attribute-entity pairs, and relationship triplets.
Formally, we define the datastore as a collection of key-value pairs $(k_i, v_i)$, where the key $k_i$ can represent an entity $e_i$, an attribute-entity pair $(e_i, a_i)$, or a relationship triplet $(s_i, r_i, o_i)$. The value $v_i$ consists of the associated cropped and original images. To facilitate retrieval, each key $k_i$ is mapped to a fixed-length vector representation $f^T(k_i)$, computed by a pre-trained text encoder (e.g., CLIP-Text).
The datastore $\left( \mathcal{K}, \mathcal{V} \right)$ is constructed as:

\vspace{-10pt}
\begin{align}
\left( \mathcal{K}, \mathcal{V} \right) = \{ (f^T(k_i), v_i) | (k_i, v_i) \in \mathcal{D}, \\ k_i \in \{ e_i,(e_i,a_i),(s_i,r_i,o_i) \} \}
\vspace{-10pt}
\end{align}

Here, $\mathcal{D}$ represents the annotated examples in the Visual Genome dataset.


% \subsection{Inference}

% % At test time, given the input context $x$
% % % $c_t$ 
% % the text encoder CLIP$_{Text}$ encode context representation $f(x)$.
% % The model queries the datastore with $f(x)$ to retrieve its $k$-nearest neighbors $\mathcal{N}$ according to a distance function $d(\cdot, \cdot)$ (squared $L^2$ distance in our experiments). As 
% % a result, we obtain a list of images related to the input context $x$. %\footnote{This choice makes our similarity function an RBF kernel.}).
% % % \footnote{In our experiments, $d(k_i,k_j)$ is the L2 distance between keys $k_i$ and $k_j$.}
% % % Then, it computes a distribution over neighbors based on a softmax of their negative distances, while aggregating probability mass for each vocabulary item across all its occurrences in the retrieved targets (items that do not appear in the retrieved targets have zero probability): 

% At inference time, the model is given an input context $x$, such as a textual description or query. The text encoder (e.g., CLIP-Text) encodes this context into a fixed-length vector representation $f^T(x)$. The datastore is then queried with $f^T(x)$ to retrieve the $k$-nearest neighbors based on a distance function $d(\cdot, \cdot)$. In our implementation, we use the squared $L^2$ distance.
% The retrieval process yields a set of images related to the input context $x$, providing visually grounded evidence for downstream tasks such as hallucination detection or multimodal reasoning. This efficient retrieval mechanism ensures that the model can leverage the curated visual-aid knowledge base to validate outputs without incurring the overhead of real-time API calls or additional computational inference from large models.


% \subsection{Prior knowledge base}

% Regarding 