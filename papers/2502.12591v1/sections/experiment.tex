\input{sections/table_main2}

\section{Experiment}

\subsection{Experimental Settings}

\subsubsection{Benchmark}

% \paragraph{POPE}~\cite{Li-hallucination-2023} focuses on assessing hallucinations in LVLMs by employing three distinct sampling strategies: random, popular, and adversarial. These strategies primarily differ in how negative samples are constructed. In the random setting, negative samples are created by randomly selecting objects not present in the image. In the popular setting, non-existent objects are sampled from a pool of the most frequently occurring objects. Meanwhile, the adversarial setting selects negative samples from objects that often co-occur but are not present in the image.

% Following the evaluation setup~\cite{li2023evaluating}, we sampled 50 images and generated six questions per image, maintaining a balanced ratio of positive to negative samples (50\% each). This approach converts object annotations into a series of binary "Yes-or-No" questions, concentrating on object-level hallucinations, specifically the \emph{existence} aspect. The LVLMs are tasked with determining whether an object is present in the image. Evaluation metrics include accuracy, precision, recall, and F1-score.

% \paragraph{R-Bench}~\cite{pmlr-v235-wu24l}

% The Relationship Hallucination Benchmark (R-Bench) is a novel evaluation framework designed to assess relationship hallucinations in large vision-language models (LVLMs). It includes 11,651 binary questions, split into 7,883 image-level questions that assess global relationship understanding and 3,768 instance-level questions that evaluate localized relationships using bounding boxes or masks. Evaluations on popular LVLMs reveal that relationship hallucinations are more pronounced than object hallucinations, stemming from long-tail distributions and co-occurrence biases, and highlight challenges in spatial reasoning and reliance on commonsense over visual cues. R-Bench aims to drive advancements in mitigating relationship hallucinations through fine-grained image-text alignment.

\paragraph{POPE}~\cite{Li-hallucination-2023}: POPE evaluates hallucinations in LVLMs using three sampling strategies: random, popular, and adversarial, each differing in negative sample construction. The random strategy selects objects absent from the image, popular samples frequent but non-existent objects, and adversarial picks commonly co-occurring but missing objects. Following~\cite{li2023evaluating}, we sampled 50 images, generating six questions per image with an equal mix of positive and negative samples (50\%). Object annotations were converted into binary Yes-or-No questions, focusing on existence hallucinations. LVLMs determine object presence, with performance measured via accuracy, precision, recall, and F1-score.

\paragraph{R-Bench}~\cite{pmlr-v235-wu24l}: The Relationship Hallucination Benchmark (R-Bench) evaluates relationship hallucinations in LVLMs. It comprises 11,651 binary questions: 7,883 image-level for global relationships and 3,768 instance-level for localized ones, using bounding boxes or masks. Evaluations on popular LVLMs reveal relationship hallucinations are more prevalent than object-level ones due to long-tail distributions and co-occurrence biases, highlighting challenges in spatial reasoning and reliance on commonsense over visual cues. R-Bench promotes improvements in mitigating relationship hallucinations via fine-grained image-text alignment.

\subsubsection{Implementation}

\paragraph{Baselines.}

We choose mainstream LVLMs as our baseline models, including mPLUG-Owl~\cite{ye2023mplug}, LLaVA~\cite{liu2023visual}, Qwen~\cite{Qwen-VL}. Regarding LVLM hallucination detection, we compare our work with Woodpecker~\cite{yin2023woodpecker} and LogicCheckGPT (LCGPT)~\cite{wu2024logical}.

\paragraph{Implementation Details.}

Our pipeline is training-free and comprises several pre-trained models apart from the LVLM to be corrected. 

\begin{itemize}[leftmargin=*]
    \item Scene Graph Parser: we use FlanT5 pretrained with FACTUAL~\cite{li-etal-2023-factual} to extract textual scene graph.
    \item Open-set object detection: we use Grounding DINO~\cite{liu2023grounding} to extract object counting information with default detection thresholds.
    \item Textual encoder, visual encoder: we use pretrained CLIP~\cite{radford2021learningtransferablevisualmodels} for extracting text and visual representation.
    \item Visual question answering: we use BLIP-2-FlanT5$_\text{XXL}$~\cite{li2023blip} as the VQA model to answer the question about: existence, attribute, and relation.
    \item Datastore: To search over this large datastore, we use FAISS \cite{johnson2017billion}, an open-source library for fast nearest neighbor retrieval in high dimensional spaces.
\end{itemize}

For both POPE and R-Bench benchmark, they are both ``Yes-or-No'' questions, instead of feeding the question and the answer from LVLMs, we use QA2Claim model~\cite{huang-etal-2023-zero} to convert all questions into claims which assume the answer is ``Yes''. For example, given a question, ``\texttt{Is there a dog in the image?}'', we convert into the specific caption ``\texttt{There is a dog in the image.}''.

% We choose the LLM, GPT-3.5-turbo~\cite{brown2020language}, to fulfill the subtasks of key concept extraction, question formulation, and hallucination correction. For open-set object detection, we use Grounding DINO~\cite{liu2023grounding} to extract object counting information with default detection thresholds. Moreover, we utilize BLIP-2-FlanT5$_\text{XXL}$~\cite{li2023blip} as the VQA model to answer the attribute-related questions conditioned on the input image.

% For the ``Yes-or-No'' questions, we find that the instruction-following ability of some LVLMs is somewhat weak, often outputting irrelevant texts such as pure emojis or URLs.
% This is an obstacle to our correction process.
% Besides, some LVLMs only output a single ``Yes'' or ``No'', which also poses a challenge to the correction.
% To deal with these issues, we design two simple measures: (1) we first extract keywords, \ie, ``Yes'' and ``No'' from the responses as the answers, then combine the questions with the answers into more specific claims. For example, given a question, ``\texttt{Is there a dog in the image?}'' and a model answer, ``Yes'', we compose a more specific answer as ``\texttt{Yes, there is a dog in the image.}''; (2) we additionally feed the questions to the LLM in the correction process so that the LLM can have a better grasp of the context and task requirements. 
\subsection{Experimental Results}

% \subsection{}

\subsubsection{POPE}

Table~\ref{tab:main} presents the performance of all methods across the different data splits. Our method consistently outperforms both baselines in all metrics and data splits, demonstrating superior robustness and generalization capabilities.

\vspace{-4pt}
\paragraph{Adversarial Split}

In the adversarial split, which challenges the models with difficult examples designed to exploit their weaknesses, our method achieves an Accuracy of 94.67\%, surpassing Woodpecker (90.67\%) and Logic (83.33\%). Notably, our method maintains a high Precision of 92.95\% and an outstanding Recall of 96.67\%, leading to the highest F1 Score of 94.77\%. This indicates that our model is not only accurate but also reliable in identifying true positives even under adversarial conditions.

\vspace{-4pt}
\paragraph{Popular Split}

For the popular split, which includes frequently occurring patterns, our method achieves an Accuracy of 93.67\%, significantly higher than Woodpecker (89.67\%) and Logic (85.00\%). The F1 Score of 93.65\% highlights our method's balanced performance in terms of Precision (93.96\%) and Recall (93.33\%), confirming its effectiveness in handling common data patterns without overfitting.

\vspace{-4pt}
\paragraph{Random Split}

In the random split, designed to reflect general, unbiased data distribution, our method achieves the highest Accuracy of 95.67\%, outperforming Woodpecker (93.33\%) and Logic (85.67\%). With a Precision of 95.36\% and Recall of 96.00\%, our method attains an exceptional F1 Score of 95.68\%, demonstrating its strong generalization capability across diverse data samples.







\subsubsection{R-Bench}

Table~\ref{tab:main2} summarizes the performance of all methods on the R-Bench benchmark, evaluated at both the image and instance levels. Our method consistently demonstrates superior performance, underscoring its robustness and adaptability across different granularity levels.

\vspace{-4pt}
\paragraph{Image Level}

At the image level, our method achieves an Accuracy of 79.00\%, significantly outperforming Woodpecker (55.00\%) and Logic (62.00\%). The Precision of 74.58\% and outstanding Recall of 88.00\% lead to the highest F1 Score of 80.73\%. These results highlight our model's effectiveness in accurately identifying relevant features within images.

\vspace{-4pt}
\paragraph{Instance Level}

For instance-level evaluation, our method achieves an Accuracy of 72.33\%, compared to Woodpecker (56.67\%) and Logic (60.67\%). The Precision of 66.67\% and remarkable Recall of 89.33\% result in an F1 Score of 76.35\%, indicating our model's strong capability to generalize well at a finer granularity, effectively capturing individual instances within images.

% \subsubsection{GPT-4v Assisted Evaluation}