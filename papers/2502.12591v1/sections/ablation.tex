\begin{table}[t]

\scalebox{0.83}{
\begin{tabular}{lcccc}
\hline
\multicolumn{1}{l|}{Method}    & \multicolumn{1}{c|}{Acc}   & \multicolumn{1}{c|}{Precision} & \multicolumn{1}{c|}{Recall} & F1 Score \\ \hline

\multicolumn{5}{c}{\textit{Adversarial}     }                                                                                                 \\ \hline
\multicolumn{1}{l|}{LLaVA}     & \multicolumn{1}{c|}{50.77} & \multicolumn{1}{c|}{50.39}     & \multicolumn{1}{c|}{\textbf{99.87}}  & 66.98    \\ \hline
\multicolumn{1}{l|}{mPLUG-Owl} & \multicolumn{1}{c|}{50.67} & \multicolumn{1}{c|}{50.34}     & \multicolumn{1}{c|}{\textbf{99.33}}  & 66.82    \\ \hline
\multicolumn{1}{l|}{CutPaste\&Find}       & \multicolumn{1}{c|}{\textbf{81.26}} & \multicolumn{1}{c|}{\textbf{77.92}}     & \multicolumn{1}{c|}{87.27}  & \textbf{82.33}    \\ \hline


\multicolumn{5}{c}{\textit{Popular}        }                                                                                                  \\ \hline
\multicolumn{1}{l|}{LLaVA}     & \multicolumn{1}{c|}{52.43} & \multicolumn{1}{c|}{51.25}     & \multicolumn{1}{c|}{\textbf{99.80}}  & 66.79    \\ \hline
\multicolumn{1}{l|}{mPLUG-Owl} & \multicolumn{1}{c|}{50.63} & \multicolumn{1}{c|}{50.32}     & \multicolumn{1}{c|}{\textbf{99.27}}  & 67.72    \\ \hline
\multicolumn{1}{l|}{CutPaste\&Find}       & \multicolumn{1}{c|}{\textbf{89.83}} & \multicolumn{1}{c|}{\textbf{91.99}}     & \multicolumn{1}{c|}{87.27}  & \textbf{89.57}    \\ \hline


\multicolumn{5}{c}{\textit{Random}    }                                                                                                       \\ \hline
\multicolumn{1}{l|}{LLaVA}     & \multicolumn{1}{c|}{54.43} & \multicolumn{1}{c|}{52.32}     & \multicolumn{1}{c|}{\textbf{99.80}}  & 68.65    \\ \hline
\multicolumn{1}{l|}{mPLUG-Owl} & \multicolumn{1}{c|}{53.30} & \multicolumn{1}{c|}{51.71}     & \multicolumn{1}{c|}{\textbf{99.53}}  & 68.06    \\ \hline
\multicolumn{1}{l|}{CutPaste\&Find}       & \multicolumn{1}{c|}{\textbf{86.80}} & \multicolumn{1}{c|}{\textbf{86.46}}     & \multicolumn{1}{c|}{87.27}  & \textbf{86.86}    \\ \hline

\end{tabular}
}

\caption{Results on POPE (Full version). The best performances within each setting are \textbf{bolded}.}

\label{tab:main3}

\vspace{-5pt}
\end{table}


\section{Ablation study}

To further validate our approach, we conducted an extensive evaluation using the full POPE dataset (9000 samples), and the results are presented in Table 2. Our method is compared against two additional state-of-the-art models: LLaVA and mPLUG-Owl.

\vspace{-4pt}
\paragraph{Random Split}

Our method achieves an Accuracy of 86.80\%, significantly outperforming LLaVA (54.43\%) and mPLUG-Owl (53.30\%). The Precision of 86.46\% and Recall of 87.27\% lead to the highest F1 Score of 86.86\%, demonstrating strong generalization across randomly distributed samples.

\vspace{-4pt}
\paragraph{Popular Split}

For the popular split, our method again achieves superior performance with an Accuracy of 89.83\%, compared to LLaVA (52.43\%) and mPLUG-Owl (50.63\%). The F1 Score of 89.57\% highlights our model’s ability to capture frequently occurring patterns while maintaining high precision (91.99\%) and recall (87.27\%).

\vspace{-4pt}
\paragraph{Adversarial Split}

In the adversarial setting, which is particularly challenging, our method maintains an Accuracy of 81.26\%, a substantial improvement over LLaVA (50.77\%) and mPLUG-Owl (50.67\%). The F1 Score of 82.33\% further confirms its robustness, balancing Precision (77.92\%) and Recall (87.27\%).

\subsection{Experiment on full POPE}

To highlight the necessity of our scaling factor, we analyze similarity scores in the visual-aid knowledge base. As shown in Table~\ref{tab:main3}, ground-truth validation images often receive suboptimal similarity scores despite correctly representing the queried object. For example, a wooden medicine cabinet scores 0.558, with alternative images varying from 0.383 to 0.805. Similarly, white fridge and train on track validation images score 0.922 and 0.655, while some alternatives score even higher (e.g., 0.823 for white fridge and 0.845 for train on track), despite contextual differences. These inconsistencies suggest that raw similarity scores may not fully capture object fidelity, leading to misinterpretations in hallucination detection. Our scaling factor mitigates this by incorporating prior knowledge from the datastore, aligning scores with realistic expectations and improving hallucination sensitivity.

\subsection{VQA scoring visualization }

To illustrate the necessity of our scaling factor, we analyze similarity scores in the visual-aid knowledge base. As shown in Figure~\ref{fig:ab2}, ground-truth validation images often receive lower-than-ideal similarity scores despite correctly representing the queried object. For example, a wooden medicine cabinet scores 0.558, while alternative images range from 0.383 to 0.805. Similarly, white fridge and train on track validation images score 0.922 and 0.655, yet some alternative images score even higher (e.g., 0.823 and 0.845, respectively), despite contextual differences. These inconsistencies indicate that raw similarity scores do not fully capture object fidelity, leading to misinterpretations in hallucination detection. Our scaling factor mitigates this by incorporating prior knowledge from the datastore, aligning scores with realistic expectations and improving sensitivity to hallucinations.



\subsection{Visualization}

\subsubsection{Open-set Object detector based comparison}

% The qualitative examples in Figure~\ref{} demonstrate our model’s effectiveness in detecting hallucinations, particularly where Woodpecker generates incorrect or misleading responses. In the first case, when asked about a sports ball, Woodpecker incorrectly states that none is present, while our model correctly identifies it with an existence score of 1.0, avoiding the hallucination of a missing entity. Similarly, for a dining table, Woodpecker misidentifies a generic table, whereas our model assigns an existence score of 0.0, accurately detecting the hallucination.

% In the black leather couch example, Woodpecker detects a couch but fails to confirm its leather attribute, creating ambiguity. Our model explicitly identifies the leather attribute with high confidence (0.9228) and captures relational understanding by confirming the woman’s sitting position with a score of 0.9431, which Woodpecker lacks.

% Lastly, for the smiling baby, Woodpecker falsely asserts the baby is smiling, while our model assigns a lower smiling attribute score (0.3378), reflecting uncertainty and mitigating hallucinated affirmations.

% These examples highlight our model’s robustness in leveraging existence, attribute confidence scores, and relational reasoning to mitigate hallucinations, outperforming Woodpecker in multimodal verification.

The qualitative examples in Figure~\ref{fig:ab1a} demonstrate our model’s effectiveness in detecting hallucinations, particularly where Woodpecker generates incorrect or misleading responses. In the first case, when asked about a sports ball, Woodpecker incorrectly states that none is present, while our model correctly identifies it with an existence score of 1.0, avoiding the hallucination of a missing entity. Similarly, for a dining table, Woodpecker misidentifies a generic table, whereas our model assigns an existence score of 0.0, accurately detecting the hallucination.
In the black leather couch example, Woodpecker detects a couch but fails to confirm its leather attribute, creating ambiguity. Our model explicitly identifies the leather attribute with high confidence (0.9228) and captures relational understanding by confirming the woman’s sitting position with a score of 0.9431, which Woodpecker lacks.
Lastly, for the smiling baby, Woodpecker falsely asserts the baby is smiling, while our model assigns a lower smiling attribute score (0.3378), reflecting uncertainty and mitigating hallucinated affirmations.
These examples highlight our model’s robustness in leveraging existence, attribute confidence scores, and relational reasoning to mitigate hallucinations, outperforming Woodpecker in multimodal verification.

\subsubsection{Failed cases}

% The examples in Figure~\ref{} illustrate failure cases where our model struggles with understanding complex language structures, particularly in distinguishing fine-grained attributes and relational constraints. In the first case, the question asks whether there are any trees in the image that are not decorated with lights and are located outside of a strip mall. While our model correctly detects the presence of trees (existence score: 1.0) and the strip mall (existence score: 1.0), it fails to differentiate between trees with and without lights. Additionally, while the model correctly identifies a spatial relationship between trees and the strip mall with a relation score of 0.8866, it does not explicitly verify whether there are trees that lack decoration. This suggests a limitation in attribute-based reasoning when dealing with multi-condition queries.

% In the second example, the question asks whether the two women in the image are playing table tennis against each other. The model successfully detects the presence of women and table tennis with high existence scores (1.0). It also captures the women, play, table tennis relation with a moderate confidence score (0.6176). However, the model fails to account for the crucial against each other condition, which requires a nuanced understanding of competitive gameplay rather than just activity recognition. This failure indicates a challenge in capturing multi-step reasoning required to verify interactive actions between multiple entities.

% These failure cases suggest that while our model is effective at detecting explicit entities and relations, it struggles with complex, compositional language that involves negation, fine-grained attribute distinctions, and multi-entity interactions. Addressing these limitations would require improvements in linguistic grounding and multimodal reasoning capabilities to handle more intricate natural language queries.


The examples in Figure~\ref{fig:ab1b} illustrate failure cases where our model struggles with complex language structures, particularly in fine-grained attributes and relational constraints. In the first case, the question asks whether there are trees without lights outside a strip mall. While our model correctly detects trees and the strip mall (existence score: 1.0) and their spatial relationship (0.8866), it fails to distinguish decorated from undecorated trees, revealing a limitation in attribute-based reasoning for multi-condition queries.
In the second case, the question asks whether two women are playing table tennis against each other. The model detects women and table tennis (existence score: 1.0) and identifies the women, play, table tennis relation (0.6176). However, it fails to capture the against each other condition, requiring a deeper understanding of competitive interactions rather than simple activity recognition.
These cases highlight our model’s difficulty with compositional language, negation, and multi-entity interactions. To overcome these limitations, improvements in linguistic grounding and multimodal reasoning are needed to better handle complex queries.