@article{10.1145/3386252,
    author = {Wang, Yaqing and Yao, Quanming and Kwok, James T. and Ni, Lionel M.},
    title = {Generalizing from a Few Examples: A Survey on Few-Shot Learning},
    year = {2020},
    issue_date = {May 2021},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    volume = {53},
    number = {3},
    issn = {0360-0300},
    url = {https://doi.org/10.1145/3386252},
    doi = {10.1145/3386252},
    abstract = {Machine learning has been highly successful in data-intensive applications but is often hampered when the data set is small. Recently, Few-shot Learning (FSL) is proposed to tackle this problem. Using prior knowledge, FSL can rapidly generalize to new tasks containing only a few samples with supervised information. In this article, we conduct a thorough survey to fully understand FSL. Starting from a formal definition of FSL, we distinguish FSL from several relevant machine learning problems. We then point out that the core issue in FSL is that the empirical risk minimizer is unreliable. Based on how prior knowledge can be used to handle this core issue, we categorize FSL methods from three perspectives: (i) data, which uses prior knowledge to augment the supervised experience; (ii) model, which uses prior knowledge to reduce the size of the hypothesis space; and (iii) algorithm, which uses prior knowledge to alter the search for the best hypothesis in the given hypothesis space. With this taxonomy, we review and discuss the pros and cons of each category. Promising directions, in the aspects of the FSL problem setups, techniques, applications, and theories, are also proposed to provide insights for future research.1},
    journal = {ACM Comput. Surv.},
    month = {jun},
    articleno = {63},
    numpages = {34},
    keywords = {low-shot learning, Few-shot learning, small sample learning, one-shot learning, prior knowledge, meta-learning}
}

@InProceedings{Sun_2019_CVPR,
    author = {Sun, Qianru and Liu, Yaoyao and Chua, Tat-Seng and Schiele, Bernt},
    title = {Meta-Transfer Learning for Few-Shot Learning},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month = {June},
    year = {2019}
}

@article{zhu2009active,
  title={Active learning with sampling by uncertainty and density for data annotations},
  author={Zhu, Jingbo and Wang, Huizhen and Tsou, Benjamin K and Ma, Matthew},
  journal={IEEE Transactions on audio, speech, and language processing},
  volume={18},
  number={6},
  pages={1323--1331},
  year={2009},
  publisher={IEEE}
}

@book{wilson2008fine,
	title={Fine-grained subjectivity and sentiment analysis: recognizing the intensity, polarity, and attitudes of private states},
	author={Wilson, Theresa Ann},
	year={2008},
	publisher={University of Pittsburgh}
}

@techreport{settles2009active,
  added-at = {2011-03-25T11:05:49.000+0100},
  author = {Settles, Burr},
  biburl = {https://www.bibsonomy.org/bibtex/211a6f820b613ae8cacc5cccfe41f6b38/beate},
  institution = {University of Wisconsin--Madison},
  interhash = {d21ffc0eaffcf51e86e81779fe2b22c2},
  intrahash = {11a6f820b613ae8cacc5cccfe41f6b38},
  keywords = {active-learning literature-review spam-detection survey},
  number = 1648,
  timestamp = {2011-03-25T11:05:49.000+0100},
  title = {Active Learning Literature Survey},
  type = {Computer Sciences Technical Report},
  url = {http://axon.cs.byu.edu/~martinez/classes/778/Papers/settles.activelearning.pdf},
  year = 2009
}

@article{chen2019closer,
  title={A closer look at few-shot classification},
  author={Chen, Wei-Yu and Liu, Yen-Cheng and Kira, Zsolt and Wang, Yu-Chiang Frank and Huang, Jia-Bin},
  journal={arXiv preprint arXiv:1904.04232},
  year={2019}
}

@article{ren2018meta,
  title={Meta-learning for semi-supervised few-shot classification},
  author={Ren, Mengye and Triantafillou, Eleni and Ravi, Sachin and Snell, Jake and Swersky, Kevin and Tenenbaum, Joshua B and Larochelle, Hugo and Zemel, Richard S},
  journal={arXiv preprint arXiv:1803.00676},
  year={2018}
}

@article{wang2023few,
  title={Few-Shot Learning Meets Transformer: Unified Query-Support Transformers for Few-Shot Classification},
  author={Wang, Xixi and Wang, Xiao and Jiang, Bo and Luo, Bin},
  journal={IEEE Transactions on Circuits and Systems for Video Technology},
  year={2023},
  publisher={IEEE}
}

@article{chang2021training,
  title={On training instance selection for few-shot neural text generation},
  author={Chang, Ernie and Shen, Xiaoyu and Yeh, Hui-Syuan and Demberg, Vera},
  journal={arXiv preprint arXiv:2107.03176},
  year={2021}
}

@article{wiebe2005annotating,
	title={Annotating expressions of opinions and emotions in language},
	author={Wiebe, Janyce and Wilson, Theresa and Cardie, Claire},
	journal={Language resources and evaluation},
	volume={39},
	number={2},
	pages={165--210},
	year={2005},
	publisher={Springer}
}

@inproceedings{lin2022few,
  title={Few-shot Learning with Multilingual Generative Language Models},
  author={Lin, Xi Victoria and Mihaylov, Todor and Artetxe, Mikel and Wang, Tianlu and Chen, Shuohui and Simig, Daniel and Ott, Myle and Goyal, Naman and Bhosale, Shruti and Du, Jingfei and others},
  booktitle={Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing},
  pages={9019--9052},
  year={2022}
}

@inproceedings{schick2021few,
  title={Few-shot text generation with natural language instructions},
  author={Schick, Timo and Sch{\"u}tze, Hinrich},
  booktitle={Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
  pages={390--402},
  year={2021}
}


@inproceedings{zhang2020pegasus,
  title={Pegasus: Pre-training with extracted gap-sentences for abstractive summarization},
  author={Zhang, Jingqing and Zhao, Yao and Saleh, Mohammad and Liu, Peter},
  booktitle={International Conference on Machine Learning},
  pages={11328--11339},
  year={2020},
  organization={PMLR}
}

@article{shin2022active,
  title={Active Instance Selection for Few-shot Classification},
  author={Shin, Junsup and Kang, Youngwook and Jung, Seungjin and Choi, Jongwon},
  journal={IEEE Access},
  year={2022},
  publisher={IEEE}
}

@article{pezeshkpour2020utility,
  title={On the utility of active instance selection for few-shot learning},
  author={Pezeshkpour, Pouya and Zhao, Zhengli and Singh, Sameer},
  journal={NeurIPS HAMLETS},
  year={2020}
}

@inproceedings{boney2019active,
  title={Active one-shot learning with Prototypical Networks.},
  author={Boney, Rinu and Ilin, Alexander and others},
  booktitle={ESANN},
  year={2019}
}

@article{li2021alpn,
  title={ALPN: Active-learning-based prototypical network for few-shot hyperspectral imagery classification},
  author={Li, Xiaorun and Cao, Zeyu and Zhao, Liaoying and Jiang, Jianfeng},
  journal={IEEE Geoscience and Remote Sensing Letters},
  volume={19},
  pages={1--5},
  year={2021},
  publisher={IEEE}
}

@article{shannon1948mathematical,
  title={A mathematical theory of communication},
  author={Shannon, Claude E},
  journal={The Bell system technical journal},
  volume={27},
  number={3},
  pages={379--423},
  year={1948},
  publisher={Nokia Bell Labs}
}

@inproceedings{10.5555/1283383.1283494,
    author = {Arthur, David and Vassilvitskii, Sergei},
    title = {K-Means++: The Advantages of Careful Seeding},
    year = {2007},
    isbn = {9780898716245},
    publisher = {Society for Industrial and Applied Mathematics},
    address = {USA},
    abstract = {The k-means method is a widely used clustering technique that seeks to minimize the average squared distance between points in the same cluster. Although it offers no accuracy guarantees, its simplicity and speed are very appealing in practice. By augmenting k-means with a very simple, randomized seeding technique, we obtain an algorithm that is Θ(logk)-competitive with the optimal clustering. Preliminary experiments show that our augmentation improves both the speed and the accuracy of k-means, often quite dramatically.},
    booktitle = {Proceedings of the Eighteenth Annual ACM-SIAM Symposium on Discrete Algorithms},
    pages = {1027–1035},
    numpages = {9},
    location = {New Orleans, Louisiana},
    series = {SODA '07}
}

@article{10.5555/3455716.3455856,
    author = {Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J.},
    title = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
    year = {2020},
    issue_date = {January 2020},
    publisher = {JMLR.org},
    volume = {21},
    number = {1},
    issn = {1532-4435},
    abstract = {Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pretraining objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new "Colossal Clean Crawled Corpus", we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code.},
    journal = {J. Mach. Learn. Res.},
    month = {jan},
    articleno = {140},
    numpages = {67},
    keywords = {attention based models, deep learning, multi-task learning, natural language processing, transfer learning}
}

@article{chung2022scaling,
  title={Scaling instruction-finetuned language models},
  author={Chung, Hyung Won and Hou, Le and Longpre, Shayne and Zoph, Barret and Tay, Yi and Fedus, William and Li, Eric and Wang, Xuezhi and Dehghani, Mostafa and Brahma, Siddhartha and others},
  journal={arXiv preprint arXiv:2210.11416},
  year={2022}
}

@article{lewis2019bart,
  title={Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension},
  author={Lewis, Mike and Liu, Yinhan and Goyal, Naman and Ghazvininejad, Marjan and Mohamed, Abdelrahman and Levy, Omer and Stoyanov, Ves and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:1910.13461},
  year={2019}
}

@inproceedings{muller2022active,
  title={Active few-shot learning with fasl},
  author={M{\"u}ller, Thomas and P{\'e}rez-Torr{\'o}, Guillermo and Basile, Angelo and Franco-Salvador, Marc},
  booktitle={International Conference on Applications of Natural Language to Information Systems},
  pages={98--110},
  year={2022},
  organization={Springer}
}

@misc{Gulli_2005,
    url="http://groups.di.unipi.it/~gulli/AG_corpus_of_news_articles.html",
    title={AG’s corpus of news articles},
    author={Gulli, Antonio},
    year={2005}
}

@inproceedings{NEURIPS2020_1457c0d6,
 author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {1877--1901},
 publisher = {Curran Associates, Inc.},
 title = {Language Models are Few-Shot Learners},
 url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf},
 volume = {33},
 year = {2020}
}

@article{reid2024gemini,
  title={Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context},
  author={Reid, Machel and Savinov, Nikolay and Teplyashin, Denis and Lepikhin, Dmitry and Lillicrap, Timothy and Alayrac, Jean-baptiste and Soricut, Radu and Lazaridou, Angeliki and Firat, Orhan and Schrittwieser, Julian and others},
  journal={arXiv preprint arXiv:2403.05530},
  year={2024}
}

@article{dubey2024llama,
  title={The llama 3 herd of models},
  author={Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024}
}

@article{pecher2024automatic,
  title={Automatic Combination of Sample Selection Strategies for Few-Shot Learning},
  author={Pecher, Branislav and Srba, Ivan and Bielikova, Maria and Vanschoren, Joaquin},
  journal={arXiv preprint arXiv:2402.03038},
  year={2024}
}

@article{team2024gemma,
  title={Gemma 2: Improving open language models at a practical size},
  author={Team, Gemma and Riviere, Morgane and Pathak, Shreya and Sessa, Pier Giuseppe and Hardin, Cassidy and Bhupatiraju, Surya and Hussenot, L{\'e}onard and Mesnard, Thomas and Shahriari, Bobak and Ram{\'e}, Alexandre and others},
  journal={arXiv preprint arXiv:2408.00118},
  year={2024}
}

@article{jiang2023mistral,
  title={Mistral 7B (2023)},
  author={Jiang, AQ and Sablayrolles, A and Mensch, A and Bamford, C and Chaplot, DS and de las Casas, D and Bressand, F and Lengyel, G and Lample, G and Saulnier, L and others},
  journal={arXiv preprint arXiv:2310.06825},
  year={2023}
}

@article{gunel2020supervised,
  title={Supervised contrastive learning for pre-trained language model fine-tuning},
  author={Gunel, Beliz and Du, Jingfei and Conneau, Alexis and Stoyanov, Ves},
  journal={arXiv preprint arXiv:2011.01403},
  year={2020}
}