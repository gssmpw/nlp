\section{Related Work}
\label{sec:rel_work_section}

In previous studies, the few-shot scenario has been simulated by randomly sampling a subset from the complete training data ____. Among different \gls*{fsl} methods in \gls*{nlp}, there are few methods that have paid attention to the sample selection strategies.
However, some recent studies in the field of image processing have demonstrated the effectiveness of incorporating \gls*{al} strategies in the context of \gls*{fsl} ____.

The study conducted by ____ is one of the few works that specifically addresses sample selection in \gls*{nlp}.
Their research focuses on \gls*{fs} training instance selection and using it in three text generation tasks with BART.
Their approach is motivated by the idea that few-shot training instances should exhibit diversity and representativeness. To achieve this, the authors utilized K-Means clustering for choosing data points closer to the center of clusters as important (i.e., informative) samples. Their results demonstrate the success of this method even with this simple non-iterative clustering-based approach. 
In contrast, our research specifically targets classification tasks. Furthermore, our active learning approach incorporates a wider range of tasks and selection strategies (i.e., uncertainty, diversity, and representativeness) compared to their study. We then extend the usage of this idea to iteratively expand the support set.

The only recent study that incorporates \gls*{fs} and \gls*{al} approaches in \gls*{nlp} is conducted by ____, using a zero-shot approach for XLM-RoBERTa-based Siamese networks. They use label tuning to fine-tune the label embeddings for faster training.
On the contrary, we use multiple language models (i.e., BART and FLAN-T5) and fine-tune the entire model on the support set in every iteration as we prioritize the \gls*{fs} instance selection quality and model performance, which is shown to produce significantly better performance. Also, the related work does not specify how they select samples in the first iteration using an uncertainty approach while the model has never seen any related data before. In contrast, we propose using representative sampling in the first step to boost the initial performance of the model, even in uncertainty sampling methods. Moreover, we introduce four new sampling methods compared to the mentioned work. Importantly, our methods and implementation are open source and publicly available to be freely used by fellow researchers. To the best of our knowledge, the previous workâ€™s methods are not freely available to the public.