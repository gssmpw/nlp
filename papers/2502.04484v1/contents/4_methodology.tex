\label{sec:methodology}

The \emph{goal} of this study was to evaluate the current state of documentation, structure, and licensing of the ML supply chain. The study \emph{context} consisted of 760,460 models and 175,000 datasets mined from Hugging Face. We aim to address the following research questions (RQs):

\begin{enumerate}[label=\textbf{RQ$_\arabic*$:}, ref=\textbf{RQ$_\arabic*$}, wide, labelindent=10pt,leftmargin=10pt, start=0]\setlength{\itemsep}{0.2em} 

    \item \label{rq:2}{\textit{What documentation deficiencies exist in Hugging Face model and data cards that potentially complicate mapping the ML supply chain?}} %
    To understand the ML supply chain, we needed to collect and analyze Hugging Face metadata. Using this data, we identified several issues and challenges related to documentation that introduced hurdles to obtaining a comprehensive understanding of the ecosystem. We examine these challenges in detail because with poorly and inconsistently described models and datasets, developers may struggle to leverage the ML supply chain effectively. This preliminary research question discusses encountered examples of noisiness issues occurring in Hugging Face model and dataset documentation.
  
    \item \label{rq:1}{\textit{What is the structure of the ML supply chain?}} The ML supply chain, like any other supply chain, is characterized by dependencies across components. This research question looks into the complexities of such a dependency graph.%
  
    \item \label{rq:3}{\textit{What is the current licensing landscape for models and datasets and what potential compliance challenges does it pose?}}  ML models are software, so software licenses govern their use and redistribution. %
    As such, these models present new challenges in the area of license compliance. To understand these challenges, we examine the state of licensing in the ML supply chain by 1) analyzing common licenses and license categories, 2) exploring licensing decisions made in the supply chain, %
    and 3) investigating potential compliance challenges, such as incompatibilities between parent/child model licenses, ambiguities introduced by undeclared, unknown, or custom licenses, and uncertainty surrounding how license terms apply in context.  %
\end{enumerate}

\begin{figure}[ht!]
\centering
\includegraphics[width=\linewidth]{images/model_supply_chain_methodology_v4.drawio.pdf}
\caption{Study Methodology} %
\label{fig:methodology}
\end{figure}

In the following subsections, we describe the study methodology, which is depicted in Fig. \ref{fig:methodology}. We start by mining model and dataset lists.  Next, for each model/dataset, we extract information necessary to construct the ML supply chain graph, recording and analyzing documentation deficiencies encountered during the process (RQ0). Then, we systematically analyze and evaluate the structure of the resulting supply chain graph (RQ1).  %
This same graph, along with licensing information, is then used to address RQ2.

\subsection{Data collection}
To answer the research questions, we used the Hugging Face API to extract a list of all models publicly hosted on the platform.  An initial list of 500,000 models was pulled on July 9, 2024, and a full list of all available models was obtained on July 11, 2024.  Using the same APIs, we mined data (model cards and metadata) for each model on this list.  For models that were unavailable through the API (as described in \Cref{sec:no_api_access}), we employed a web-scraping technique relying on a combination of Python's \texttt{request} module and the BeautifulSoup HTML parser.  Mining took place between July 9 and July 12, 2024, yielding 760,460 mined models. We also used the Hugging Face API to gather the list of all datasets publicly hosted on the platform and available through the API as of July 9, 2024.  We make all the raw data acquired from mining available in our replication package \cite{anonymous_repo}. 

While a handful of research datasets for Hugging Face models already exist \cite{jiang2024peatmoss, jiang2023ptmtorrent}, we opted to create our own dataset for this work for three reasons. First, we aimed to analyze the most current data, which includes new models and model families (\eg new models recently added to the Llama family of models) %
and changes in licensing frequencies (see \Cref{sec:licensing}). Second, we aimed to collect and analyze the most comprehensive and complete dataset; as shown in \Cref{tab:dataset-comparison}, our dataset is significantly larger than existing datasets. %
Third, unlike Jiang \etal \cite{jiang2024peatmoss} and Pepe \etal \cite{pepe2024hugging}, %
we were not focused on model-to-GitHub repository dependencies; accordingly, we aimed to design a dataset tailored for mapping model-to-model dependencies.%

\input{tables/dataset_comparison}

\subsection{Data normalization and cleaning}
Once the data for models and datasets was downloaded, it needed to be cleaned, standardized, and stored in a more usable form.  This process involved standardizing the names of declared base models and datasets 
and mapping to unique identifiers where possible. 
For example, \model{xlm-roberta-base} is a shorthand for \model{FacebookAI/xlm-roberta-base}, which has the unique internal ID \textsc{621ffdc036468d709f174364}. We relied on Hugging Face's internal mechanisms to resolve these differences where possible. For example, attempting to load the model page for \model{xlm-roberta-base} on Hugging Face redirects to the page for \model{FacebookAI/xlm-roberta-base}. Additionally, we resolved inconsistencies in field inputs. For example, [], ``'', and None were all being used to denote no declared licenses. We standardized these variations to the empty list to make later computation simpler. %
We further elaborate on documentation challenges that required special attention and resolution in our discussion of \ref{rq:2}. %

\subsection{Extracting licensing information}
\label{sec:license_extraction}
We extracted licensing information from the tags present in model metadata. While licenses can also be declared in the \textsc{CardData} attribute of the model metadata, we found through an analysis of our dataset that the tags were more complete in every instance than the \textsc{CardData} attribute.  %
In nearly all instances, licensing information was in both locations (99.9\%). However, for 0.1\% of models, the licensing information was exclusively in the tags, and there were no instances that relied only on the \textsc{CardData} attribute.  We also looked for discrepancies in those cases where both locations were utilized.  We identified only 134 such discrepancies, and in all cases, an additional license was declared in the tags while it was not declared in the \textsc{CardData}. There were no irreconcilable differences (\ie \license{MIT} vs \license{GPL}).  From this, we concluded that the model tags are the most complete machine-readable source for licensing information.%

Although licensing information can also be found in other, harder to programmatically extract %
sections of a model card, we sought to further confirm that reliance on tags to identify the declared license used by the model %
was justified. Accordingly, one author manually reviewed the model cards for the top 100 models (sorted by number of downloads) marked as having no license in the tags.  We anticipated that these 100 models were more likely to have complete and reliable documentation because of their popularity as compared with other models in our dataset, consistent with the findings of Jones \etal \cite{Jones2024WhatDW}.  From this sample of 100 models, we identified only 16 models that had declared licensing information elsewhere. Of those, one had licensing information added to its tags after our initial mining, bringing the total to 15 models. For nine of these models, the license could only be found transitively by following a link to the model's associated GitHub page.  \Cref{tab:manual_analysis} provides a more detailed breakdown of where the licensing information was located for each example.%

\input{tables/manual_analysis}

Detecting each model's declared license %
would involve more complicated mining techniques, such as those that could traverse external links, and would require NLP-based approaches that extract licensing information from the text.  These NLP approaches would have to not only recognize license names, which may not be in a standardized form but also pattern-match embedded license text. An LLM tool, like the one employed by Jiang \etal \cite{jiang2024peatmoss}, could potentially be used to extract licensing information, but at a high cost (between \$0.01 and \$0.03 per model--- or between \$7,000 and \$22,800 for our dataset). To ensure that our analysis captured the most accurate information possible and to better understand the formal documentation structure of Hugging Face, rather than employ such an approach, we focus on explicitly declared licenses.
Given the likelihood that licenses are declared in tags, the infeasibility of manual annotation for all examples, and the inherent difficulties of automatically obtaining license information from anywhere in a model card, our reliance on tags for license detection appears to be an acceptable approach. Additionally, existing supply chain analysis tools rely heavily on machine-readable metadata, so we further justify that this is the appropriate locus of analysis given that the ultimate audience for such tools is the end user. %


\subsection{Data analysis}


To address \ref{rq:2}, we quantitatively and qualitatively analyzed and discussed the different types of documentation issues that we observed in our attempt to understand the ML supply chain on Hugging Face.

Once the dataset had been cleaned, to address \ref{rq:1}, we used the \texttt{networkx} Python library \cite{networkx} to create a directed %
graph representing the observable  %
ML supply chain.  Each node in the graph represents a distinct model, and an edge represents a dependency relationship. For example, if model $A$ listed model $B$ as a base model, there would be an edge from $B$ to $A$. We would describe model $B$ as the parent of model $A$ and model $A$ as the child of model $B$. %
Due to the inherent issues and incompleteness of model documentation, this graph, by its nature, only represents a subset of the total ML supply chain on Hugging Face.%


For the sake of analysis, we flatten the individual model supply chains by extracting all longest paths (\eg lineage chains) %
from the graph that go from source nodes to sinks.  Each of these paths goes %
from a root base model to a model at the end of the ML supply chain (\ie a leaf node with no dependents or outgoing connections).

Using the paths resulting from \ref{rq:1}, we addressed \ref{rq:3} by analyzing the state of licensing in the ML supply chain, developing scripts and heuristics to look for parent/child license differences, %
and potential license compliance problems related to model/dataset dependencies.  %
We define a \textit{parent/child license difference} to refer to the difference between the license(s) of a parent and a child model/dataset. Assume, for example, that model $B$ is the child of model $A$. If model $B$ was licensed under \license{MIT} and model $A$ under \license{Apache-2.0}, we would say there is a parent/child license difference between $A$ and $B$. We avoid stronger terms such as ``inconsistencies'' and ``incompatibilities'', since some of these differences may in fact be consistent and compliant with licensing terms. %
Two authors reviewed all distinct licenses in our dataset and grouped them into six categories, which can provide the basis for future research into license compatibility and licensing trends in the ML supply chain. 

All scripts used for mining, data clean-up, and analysis can be found in our replication package~\cite{anonymous_repo} and can be used to foster further research in this area.

