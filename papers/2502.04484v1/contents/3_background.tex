
The ML supply chain has emerged as a complex structure with many steps and components, each of which can affect the final output of a given model. Lee \etal identify eight stages in the generative AI supply chain, but most apply more broadly: 1) the creation of expressive works that will eventually be used to train a model, 2) the conversion of these expressive works into digital data, 3) the compilation of these data points into training datasets, 4) the creation of an ML model by selecting an architecture, training datasets, and a training algorithm, 5) the fine-tuning of existing ``base'' models, 6) the deployment of the trained/fine-tuned model as a service, 7) the use of the model to generate output, and finally 8) applying additional alignment to further improve the model or meet user needs \cite{lee2023talkin}. %
Modern ML models are also becoming increasingly complex with respect to their architectures.  For example, Mixture of Expert (MoE) models and ensemble models comprise multiple specialized models trained by dividing the problem space into homogeneous regions \cite{masoudnia2014mixture, mixtral}.  Recent work has also proposed using smaller ML models to replace the hidden states within a Recurrent Neural Network (RNN) architecture to facilitate better recall and compression of context \cite{sun2024learning}.  Both of these examples demonstrate that understanding the relationships among models and among model architectures will become increasingly difficult.

As a result, sharing information about models 
has become critically important. ML components do not, however, always disclose data sources, making it difficult to comply with or even keep track of licensing obligations associated with the data~\cite{hassan2024rethinking}.  This creates conditions that could lead to legal disputes over the use of copyrighted material in training ML components~\cite{ai_copyright}. Model cards \cite{mitchell2019model}
have become the standard method for sharing information 
about ML models hosted on Hugging Face \cite{what_are_model_cards}, including intended uses, limitations, and datasets used in training 
\cite{model_card_guidebook}. Although tools are in development to streamline the process \cite{model_card_creation_tool}, model card creation on Hugging Face is primarily a manual process in which a user enters documentation about a model into a predefined markdown template \cite{model_card_template}. This template is robust, including spaces for model description, uses, bias, limitations, testing, \etc, as well as optional fields for citation, technical specifications, and environmental impact.   %
The manual nature of documentation, however, introduces a significant potential for human error, ambiguity, and incompleteness \cite{jiang2024peatmoss, jiang2023ptmtorrent}. Indeed, the quality and adoption of model cards remains low \cite{bhat2023aspirations}, despite user studies and other efforts by Hugging Face \cite{model_card_user_studies} to ameliorate the situation, and despite researchers proposing tools such as DocML \cite{bhat2023aspirations} to create and evolve model cards.



Hugging Face has previously been mined to study the maintenance and evolution of ML models~\cite{castano2024analyzing}, security risks in the ML supply chain \cite{jiang2022empirical}, the carbon footprint of ML models \cite{castano2023exploring}, and the documentation of ML models in terms of datasets, bias, and licenses \cite{pepe2024hugging}. Through an analysis of discussion forums and a mining study of Hugging Face, Taraghi \etal \cite{taraghi2024deep} %
identified challenges and benefits associated with model reuse. Also, previous work identified challenges and deficiencies in the organization of the ML supply chain, including poorly chosen or erroneous model names~\cite{jiang2023exploring} and frequently missing or incorrect licensing information~\cite{longpre2023data} on Hugging Face and other platforms. 

While prior work has previously noted the incompleteness of model/dataset documentation on Hugging Face \cite{Jiang2023AnES, pepe2024hugging, yang2024navigating}, our work goes beyond this observation and provides a detailed analysis of specific examples in which documentation is incomplete and erroneous.  These real-world observations can help model owners avoid common documentation pitfalls, empower model hubs to implement automated checks for common errors, and make researchers aware of potential noise in data mined from Hugging Face.

Oreamuno \etal~\cite{oreamuno2024state} also observed that the majority of models and datasets on Hugging Face lacked documentation. When present, this documentation was in the form of model/data cards, was often incomplete, and followed standards inconsistently.  Whereas the work of Oreamuno \etal focused on exploring the presence and completeness of documentation, we report various inconsistencies and anomalies that are introduced by Hugging Face users during the process of documenting their models and datasets.  The anomalies we encountered can exist even in cases where the model/dataset documentation is complete. %

Yang \etal \cite{yang2024navigating} conducted an in-depth analysis of the state of dataset documentation on Hugging Face, focusing primarily on the content included in the documentation (\eg description, structure, creation).  Our study expands on this work by also considering the documentation of ML models in addition to datasets and by noting errors or inconsistencies introduced by model/dataset owners.

Jiang \etal \cite{jiang2024naming} studied naming practices and naming anomalies for models on Hugging Face.  They found that model names carry more semantic information than do the names of traditional software packages, so anomalies, such as misrepresenting the model architecture, can lead to greater confusion in the ML context.  %
Our work does not consider the accuracy of pre-trained model (PTM) names when compared against the metadata they provide; rather we note difficulties in constructing the ML supply chain given naming inconsistencies and ambiguities in metadata references. %

Pepe \etal \cite{pepe2024hugging} conducted the study most closely related to ours. However, instead of focusing on the structure of the ML supply chain, they explore the extent to which models document datasets, potential biases, and licenses.  Both Pepe \etal \cite{pepe2024hugging} and Jiang \etal \cite{jiang2024peatmoss} analyze a supply chain, but they focus on the relationship between models and downstream GitHub repositories that make use of those models.  We instead focus on the relationships among PTMs, dependent models, and datasets.

Both Pepe \etal \cite{pepe2024hugging} and Jiang \etal \cite{jiang2024peatmoss} also explore model licensing on Hugging Face.  In the case of Jiang \etal \cite{jiang2024peatmoss}, this is done as a short demonstration illustrating the utility of the PEATMOSS dataset.  %
We extend these prior works by collecting and analyzing more recent data, which reflects the current state of model/dataset licensing.  %
Both works also consider the interplay of model licenses with the licenses of dependent GitHub repositories.  Neither work, however, evaluates license incompatibilities between models or between models and datasets.  We further expand on prior work by considering various license classes, multi-licensing, and the failure to comply with license-imposed naming requirements. Where previous work examines the relationships between projects and models, we examine the relationships between multiple models. Thus, our work is useful not only to software developers, as is the case for prior work, but also to data scientists, those who want to reuse models that have complex dependencies, and those who want to develop new models. By providing an ecosystem-level examination of the ML supply chain, we provide these groups with the information necessary to understand how models relate to each other and evolve, licensing concerns regarding ML components on Hugging Face, and potential challenges that might be encountered regarding model documentation. %



In conclusion, while our study %
has been informed by prior work \cite{castano2024lessons,yang2024navigating,jiang2024naming,jiang2023ptmtorrent,oreamuno2024state,Jiang2023AnES,pepe2024hugging,taraghi2024deep,jiang2023peatmoss}, %
our investigation focuses on the structure and characteristics of the ML supply chain itself and the challenges encountered in managing it. More specifically, while previous work examined deficiencies in documentation and licensing at the level of individual models, we investigate the ML supply chain at a higher level by examining how it is structured and how models relate to each other. %





