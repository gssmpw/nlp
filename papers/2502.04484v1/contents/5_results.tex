
\section{RQ$_0$: Documentation challenges encountered on Hugging Face}
\label{sec:rq0}
In this section, we outline and discuss documentation issues that were observed on the Hugging Face platform while mapping connections in the ML supply chain.  Specifically, this preliminary RQ outlines the observed shortcomings and challenges to 1) elaborate potential pitfalls future researchers may face in mining Hugging Face and 2) to more thoroughly report documentation issues extant on Hugging Face that have been left unexplored by prior work.

\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{images/gated.PNG}
\caption{Example of model gated behind Terms of Service requirement}
\label{fig:gated}
\end{figure}

\subsection{Inability to Access Metadata}
\label{sec:no_api_access}
Not all model metadata was accessible through the Hugging Face API.  There were 7,258 model cards (0.95\%) %
that had to be scraped using a combination of Python's \texttt{requests} \cite{requests} and BeautifulSoup \cite{bs4} libraries.  The most frequent challenge (\ie found in 99.4\% of the cases) resulted from gating based on the acceptance of terms and conditions (see \Cref{fig:gated}).  This was particularly the case for models owned by or relying on models created by larger entities such as Google or Meta.  Twenty-two models and their metadata were gated behind age restrictions imposed by the model owner\footnote{Throughout the paper, by "model owner", we refer to the entity that uploaded the model to Hugging Face.  We make no claims as to whether that entity had the legal right to do so or if they are the original author/creator of the model.}, requiring a Hugging Face account to access. Accessing the full repository, and thus metadata, required logging in with a Hugging Face account and then, in most cases, providing an email address.  This hurdle to even access the metadata for a model could potentially complicate the creation of fully automated analyzers and ML/AIBOM (Machine Learning/Artificial Intelligence Bill of Materials) %
generators for an ML supply chain.


\subsection{Incomplete Metadata}
\label{sec:incomplete_metadata}
We observed that the documentation provided by model owners on Hugging Face is often incomplete. %
This is a trend not just for smaller, infrequently used projects but also for large projects with tens of thousands of downloads. For example, OpenAI's \model{clip-vit-large-patch14-336} model, with 5,827,027 downloads, has a model card that is almost entirely incomplete, save for basic hyper-parameter and framework version information  \cite{incomplete_docs_example}.  %
Only 37.8\% of models and 27.6\% of datasets declared any kind of licensing information in a machine-readable way.  We discuss the prevalence of declared components further in \Cref{sec:rq1}. %

Some models may provide information on training data and architecture by linking to scientific papers that include these details, but such a practice introduces an additional hurdle in easily and programmatically obtaining that data.  Ultimately, the reason why many models do not even mention the datasets used would require further investigation, including interviewing/surveying developers, which is out of the scope of this work.   %

During the manual analysis of 100 model cards described in \Cref{sec:license_extraction}, we observed two examples where
the Hugging Face team wrote documentation on behalf of model owners.  The first instance was an image-to-text model 
uploaded by Microsoft with the disclaimer: ``The team releasing TrOCR did not write a model card for this model so this 
model card has been written by the Hugging Face team'' \cite{hugging_face_authored_1}. The second was a larger version of
the same model, also owned by Microsoft \cite{hugging_face_authored_2}.  In both cases, the models were described by a
research paper and released on GitHub.  Understanding the criteria that the Hugging Face team uses to determine when and 
how to intervene with model cards requires further investigation, but we do note that according to a blog written by 
members of the Hugging Face team, they created/updated model cards in some instances to inform design decisions surrounding a new 
model card template~\cite{hugging_face_model_card_blog}. %


\subsection{Additional Metadata Issues}


\subsubsection{The "unknown" license.}
The Hugging Face platform's documentation provides a list of recognized licenses each with a unique, standardized short-hand identifier that can be utilized by users when creating their model/data cards~\cite{hugging_face_licenses}. %
This list includes common Open Source Software (OSS), Creative Commons (CC), and ML-specific licenses as well as an ``Other'' catch-all category which encompasses less common, custom, or modified licenses not included in Hugging Face's predefined license list %
An ``unknown'' licensing option is also provided, but there is no guidance provided as to when this option should be selected. Future work will need to explore when and why model owners select this option, particularly given that acknowledging that the license is ``unknown'' suggests, at best, a lack of due diligence or understanding on the part of the owner or, at worst, that the model/dataset raises copyright infringement issues.  We observed 4,419 (1.5\%) models and 2,194 (4.5\%) datasets that used this ``unknown'' license tag, which inherently raises compliance challenges for dependent models.
\looseness=-1

\subsubsection{Naming problems.}
\label{sec:naming-problems} 
Hugging Face users primarily use human-readable names, not unique identifiers, when supplying the metadata information for their base models/datasets.  Unique identifiers do exist for all models and datasets on Hugging Face, but we did not observe any instances where this information was being used by model/dataset owners when referring to models or datasets they depended on. Instead, these references typically, but not always, followed the ``owner/model'' naming convention.  For example, a model owner would refer to a base model as ``FacebookAI/xlm-roberta-base'' and not by its unique Hugging Face ID: 621ffdc036468d709f174364. This is problematic because these model references are not automatically updated when name changes occur. A change to the model, dataset, or owner name results in a new human-readable identifier that no longer matches the previous references to the model/dataset.  As a result, documentation can easily become outdated, unhelpful, and potentially confusing.

We observed 596 models that had their human-readable identifiers changed. %
Instances such as \model{222gate/Blurdus-7b-v0.1} $\rightarrow$ \model{gate369/Blurdus-7b-v0.1} involve a change to the name of the model owner.  Other instances, such as \model{aaditya/openbiollm-llama3-70b} $\rightarrow$ \model{aaditya/Llama3-OpenBioLLM-70B} involve a change to the model name itself. In both cases, manual effort is required to track down and map the original references to the new names, which can make dependency management tasks increasingly difficult.  Often the only way to determine the correct mapping is by relying on Hugging Face to redirect the page associated with a former name to the one associated with the most current name.  This suggests that Hugging Face maintains some internal mapping, but it is unclear how long this mapping persists or what might happen in the event of a name collision.

\input{tables/common_names}

Model names, without the additional owner information, are not guaranteed to be unique.  Similar to how repository forks have the same name (but different owners) on GitHub, fine-tuned or forked models may also have the same name yet different owners on Hugging Face.  For this reason, developers often specify models using the ``owner/model'' naming convention described previously. If this happens consistently, ambiguities can be avoided. However, we observed 16,477 cases where developers referred to base models only by the model name, excluding owner information. For example, a developer might use ``roberta-base'' to refer to ``FacebookAI/roberta-base.''  While it could be reasonable to assume that this shorthand refers to the popular model, this may not be the case: in our dataset, there were 39 other models also named ``roberta-base,'' all with different owners.  \Cref{tab:common_names} shows the top-10 most common model names in the Hugging Face ecosystem and their frequencies.  If one of these models were referred to by name only, it would be nearly impossible to determine which model was being referenced. For example, more than 5,600 models share the model name ``ppo-LunarLander-v2.''  This further stresses the need for widespread adoption of unique model identifiers. %

\subsubsection{Missing or nonsensical references.} 
\label{sec:missing_refs}
There were 34,159 instances where we were unable to map a declared dataset to a dataset found publicly on Hugging Face.  These consisted of 4,755 unique declarations.  It is impossible to know with certainty, but such declarations likely consist of datasets that have been removed or made private, are from external sources, are actually dataset descriptors, contain typos, or, in some cases, are Hugging Face usernames. 

We were also unable to map 2,501 base model declarations to known IDs, representing a possible 1,371 unique missing models in total. %
As with the datasets, these models may have been removed, made private, renamed in a way difficult to trace, or may reference a model outside the ecosystem.  It is also possible that developer typos result in the inability to identify the models. 

In other cases, some of the references were nonsensical: in our analysis of the graph of all collected components in the ML supply chain (described further in \Cref{sec:rq1}), we detected 684 cycles in total, including 675 (98.7\%) trivial cycles in which a model declares itself as a base model, despite the fact that a model should not be able to be its own ancestor. All of this further motivates the need to use unique and standardized identifiers when referring to models and datasets, as well as a need for validating reference information.

\subsubsection{Models as datasets.} \label{sec:models_as_data} We observed 1,416 instances in which a model was listed as a dataset in the \textsc{datasets} field. In other words, a model reference was included in the metadata field specific to training datasets.  Without more information, it is impossible to know whether this indicates a mistake on the part of the model owner or that the output of the declared model was somehow used for training. 

\subsubsection{Shortcomings of Hugging Face.} 
\label{sec:no_relationship}
While base models are declared by some model owners, in many cases, the precise relationship between a derived model and its base model is left ambiguous, at least in the available metadata. Hugging Face does provide a separate field for architecture relationships, but there is no standardized way to specify situations involving fine-tuning, quantization, or using outputs for training.  This ambiguity can have significant consequences since, particularly in the licensing context, the nature of the relationship is important in determining how licensing terms should be applied. For example, it is permissible to use a model licensed under the \license{llama3} license for fine-tuning but impermissible to use its outputs to train a competing model~\cite{llama3_license}.  Additional fields specifying the nature of relationships between models would be useful, but their inclusion would also introduce additional overhead for model owners.

While uncommon and not a direct shortcoming of Hugging Face, we observed examples where model owners would declare the same dataset or base model multiple times (\ie the same model/dataset had multiple entries in the respective metadata field). Specifically, 178 models declared a dataset at least twice, and 310 models declared a base model at least twice.  One model, with otherwise robust documentation, declared the same base model 64 times. Again, while there are few instances of this duplication, they still serve as something of a canary in the coal mine, indicating a lack of validation on behalf of Hugging Face.


\section{RQ$_1$: The structure of the ML supply chain}
\label{sec:rq1}

\input{tables/top_models}

Despite the challenges outlined in \ref{rq:2}, we were still able to construct an ecosystem-level supply chain graph mapping the relationships between models and datasets. While we acknowledge that our supply chain graph represents only a subset of the total components %
in the full Hugging Face ecosystem, as noted previously, we are confident that we can still rely upon it to make observations about recurring trends and patterns. %


To analyze the structure of the ML supply chain, we began by addressing the issues identified in \ref{rq:2} using different strategies depending on the issue. %
Where possible, we employed web-scraping techniques to pull metadata for models that were inaccessible through the Hugging Face API (\Cref{sec:no_api_access}).  Models where even this analysis was impossible were excluded from the analysis.  We standardized model/dataset names, preferring the unique Hugging Face ID (\Cref{sec:naming-problems}).  Where naming ambiguities were present (multiple models with the same name, but different owners), we assumed the most popular model (\Cref{sec:naming-problems}).  References that could not be mapped to existing models/datasets on Hugging Face were also excluded from analysis (\Cref{sec:missing_refs}).  Models that were listed as datasets were excluded from the dataset analysis (\Cref{sec:models_as_data}). %

\subsection{Most popular models and datasets}

\input{tables/top_datasets}

We begin with an overview of the landscape by describing the most popular models and datasets. We measure popularity in terms of engagement via the metrics provided by Hugging Face, likes and recent downloads. These metrics also serve our goal of capturing the current state of the ecosystem by highlighting what models and datasets are currently being engaged with. %
\Cref{tab:top_models_likes,tab:top_models_downloads} show the ten most engaged with models in our dataset by these metrics. Organizations such as OpenAI and Meta occupy several top positions. The models span a variety of use cases, including image and text generation.

Similarly, \Cref{tab:top_datasets_likes,tab:top_datasets_downloads} display the most engaged with datasets. The most common \emph{Task} field is ``Question Answering'' for the most downloaded datasets  and ``Text Generation'' for the most liked datasets. However, a variety of \emph{Data Modalities} and \emph{Tasks} are represented across both groups. For example, \emph{Data Modalities} included both text and images, while \emph{Tasks} included other specifics such as ``Token Classification.'' In fact, the most liked dataset by far, \model{fka/awesome-chatgpt-prompt},
is a repository of curated ChatGPT prompts rather than a training dataset for models, which suggests that it may be more prudent to examine download count to determine the most popular datasets. This is further supported by its correlation with the number of client projects on GitHub \cite{pepe2024hugging}.%


\subsection{Most depended upon datasets and base models}
\label{sec:most-dependend-upon}



Our ability to evaluate the ML supply chain relies on the dependency information provided by model owners.  On Hugging Face, this information is found in the model metadata, where model owners declare base models and datasets. Although many models on Hugging Face were not created independently, we note that only 117,245 out of 760,460  models (15.4\%) declare any base model and, of these, most (111,568) declare only one base model. Consequently, the average number of declared base models is 0, as are the first quartile, median, and third quartile, as shown in \Cref{fig:num_declared_base_models}.

\begin{figure}[t]
\centering
\includegraphics[width=\linewidth* 1/2]{images/num_declared_base_models.png}
\caption{Models' Number of Declared Base Models}
\label{fig:num_declared_base_models}
\end{figure}

\input{tables/top_base_models}

\Cref{tab:top_base_models} lists the ten models that were most frequently declared as a base model by the models in our dataset, \ie the models that were most frequently cited as dependencies by other models. They exhibit a varying number of likes and downloads and primarily come from large providers such as Meta, Google, and OpenAI. Notably, none of these models declare that they are derived from any base models. We observe some overlap with the ten most engaged with models by downloads%
, suggesting, not surprisingly, that popular models are also frequently used as base models. %

In addition to base models, we investigate the datasets used for model training. We observe that 75,516 (9.9\%) models declare at least one dataset. Of these, 64,343 (85.2\%) declare a single dataset, 5,066 (6.7\%) declare two datasets, and 1,637 (2.2\%) declare three datasets. The number of datasets also exhibits long-tail behavior, with one model declaring 287 distinct datasets. However, the vast majority of models declare no dataset, with 0 declared models as the average and first three quartiles, as shown in \Cref{fig:num_declared_datasets}. We observe that this low percentage of declared datasets is particularly notable since, by their nature, nearly all ML models on Hugging Face should have been trained on at least one dataset, even if that dataset is not publicly available. %



\begin{figure}[t]
\centering
\includegraphics[width=\linewidth* 1/2]{images/num_declared_datasets.png}
\caption{Models' Number of Declared Datasets}
\label{fig:num_declared_datasets}
\end{figure}

\input{tables/top_declared_datasets}

\Cref{tab:top_declared_datasets} describes the top-ten declared datasets. We observe no overlap with the top-ten most liked or downloaded datasets.  This may be a consequence of the low declaration rate for datasets, a function of a dataset's age, or the fact that popular ``datasets'' uploaded to Hugging Face may not be intended for model training, as observed above.

License declarations vary among the datasets. Half of the top-ten most declared datasets are made available under a license declared as ``other.'' One example from Google appears under six distinct licenses, including the \license{Apache-2.0} license, several Creative Commons licenses, and an ``other'' license. While these are all listed in the dataset's metadata, the dataset card's field for licensing information simply says, ``More Information Needed,'' leaving it unclear as to how these licenses jointly apply to the dataset. This suggests that even very popular components can contain ambiguities that make the license compliance process more difficult. %



\subsection{Structure of the supply chain}

\subsubsection{Lengths of model supply chains}

We define a lineage chain as a path from some root base model (\ie one with no model dependencies) to a final sink node model (\ie one without any model dependents).  We examine 53,151 lineage chains for models that declare at least one base model, including cases where multiple chains lead to the same model. Of these, the average chain length to reach that model is 6.2 models. The most frequent chain length is of three models (12,480 chains), and, as shown in \Cref{fig:chain_lengths}, the first quartile is 3 models, the median is 4 models, and the third quartile is 9 models. %
The longest chain, beginning with \model{cohereforai/c4ai-command-r-v01} and ending with \model{Citaman/command-r-1-layer}, contains 40 models.  Excluding the first model, all models in this chain are owned by Citaman and incrementally count down from 39. 
That is, the models in the chain range from \model{Citaman/command-r-39-layer} down to \model{Citaman/command-r-1-layer}, suggesting that each model may represent an incremental improvement or an updated version.  %

\begin{figure}[t]
\centering
\begin{minipage}[c]{0.45\linewidth}
\includegraphics[width=\linewidth]{images/chain_lengths.png}
\caption{Model Chain Lengths}
\label{fig:chain_lengths}
\end{minipage}
\hfill
\centering
\begin{minipage}[c]{0.45\linewidth}
\includegraphics[width=\linewidth]{images/chain_licenses.png}
\caption{Number of Licenses in Model Chains}
\label{fig:chain_licenses}
\end{minipage}
\end{figure}


\subsubsection{Licenses present in model supply chains}

There are typically relatively few parent/child license differences %
between models in these chains, with an average of 1.9 distinct licenses in a given chain, including unknown or undeclared licenses. As \Cref{fig:chain_licenses} illustrates, the first quartile is just 1 license, and both the median and third quartile are 2 licenses. %
A chain most commonly contains 2 licenses (24,069 chains), and the highest number of licenses observed was 6, occurring in 17 different chains. A relatively small number of parent/child license differences simplifies the task of managing license compliance. However, these differences within a lineage necessitate examination of license compatibility, as well as a possible need to find mitigations for items in model chains when required by the license of a base model. %

\begin{figure}[b]
\centering
\includegraphics[width=\linewidth* 1/2]{images/owner_appearances.png}
\caption{Number of Times the Owner of a Chain's Final Model Appears in that Chain}
\label{fig:owner_appearances}
\end{figure}

\subsubsection{Model ownership}

We also observe that developers %
using Hugging Face often build off each other's work rather than off their own models. The owners of nodes at the end of a chain appeared within that node's chain only 1.4 times on average, showing that base models are frequently sourced from the community and supporting the idea that a common approach to training a model involves building off of previous work. In another interpretation, it could be that model trainers do not fill in base model information when building on their own work at least some of the time. Future work involving discussions with developers could shed light on the reasons behind this phenomenon. %

In 82.9\% of chains (43,507), the owner of the final model in the chain owned no other model in that chain. In the maximum case, the chain contained 39 models from the final model's owner. %
Illustrating this further, as shown in \Cref{fig:owner_appearances}, the first quartile, median, and third quartile are all a single occurrence of the final model's owner in a given chain: only the final model itself. 






Based on our observations, %
a given owner publishes relatively few models to Hugging Face, owning, on average, 4 models in our dataset, though the data is skewed heavily toward relatively few highly-prolific owners. Of 190,136 distinct model owners in our dataset, 61.5\% owned just one public model (117,016), while the most prolific account owned 4,610 models. The first quartile and median are both just 1 owned model, and the third quartile is 2 models. Notably, the large players mentioned above, including Meta and OpenAI, are not present in the top-10 most prolific model owners: those spots instead go to smaller stakeholders in the AI market. The top-ten model owners on Hugging Face are shown in \Cref{tab:top-owners} and the top-ten dataset owners in \Cref{tab:top-dataset-owners}. There is only one overlap between the two lists. 

\input{tables/top_owners}



\section{RQ$_2$: licensing of models and datasets on Hugging Face}
\label{sec:licensing}

The majority of models (62.2\%) and datasets (72.4\%) did not declare licensing information in a machine-readable way (see \Cref{sec:license_extraction}). To better illustrate how models and datasets are licensed, we organize the licenses that we observed into six categories. The categories are intended to be indicative of the origin or purpose of the licenses (whether the licenses were designed for open source software, for ML components specifically, or other purposes) and are not intended to convey any legal characteristics, particularly given the uncertainty involved in applying these licenses in the ML context. %
We define the following categories: %
\begin{itemize}
    \item OSS: open source software licenses such as \license{MIT}, \license{Apache}, and \license{GPL}
    \item CC: Creative Commons licenses
    \item ML: ML-specific licenses such as \license{open-rail} and \license{llama}
    \item Data: licenses that are intended to specifically protect data such as \license{odbl} and \license{c-uda}
    \item Other: the ``other'' license category on Hugging Face %
    \item Unknown: the ``unknown'' license category on Hugging Face
\end{itemize}


\subsection{Most common licenses}
\label{sec:common_licenses}



\input{tables/top-licenses}

There were 71 unique declared licenses for models and 70 for datasets. %
\Cref{tab:top_licenses_datasets,tab:top_licenses_models} provide an overview of the most frequently observed licenses. %

We classify licenses by the classes defined above. %
A breakdown of the prevalence of these classes can be found in \Cref{tab:class_prevalence}. Both models (62.5\%) and datasets (58\%) are most likely to be licensed under approved OSS licenses. %
This is particularly interesting for datasets, where we would suspect to see mostly CC or Data licenses, as datasets are not ``software'' in a strict sense of the word. %
This may reflect that Hugging Face developers view models and datasets more similarly to programs than to data or that developers rely on OSS licenses because they are already familiar with their terms. However, the relationships between these preexisting licenses and these new types of components in the ML supply chain, as well as the implications of licensing such components with such licenses, are currently unknown. Further research is needed to understand the reasoning behind these licensing choices, as well as to understand the interactions between many of these licenses and ML components.

\input{tables/license_classes}

Both models and datasets are distributed with (i) OSS licenses specific to software (both restrictive and permissive), (ii) Creative Commons (CC) licenses, that are not software-specific yet are being used for software (e.g., Stack Overflow's adoption of CC licenses), and ML-specific licenses. For models, our findings are consistent with those of Pepe \etal  \cite{pepe2024hugging}, %
showing that permissive OSS licenses such as \license{Apache-2.0} and \license{MIT} are popular in this space. This is not surprising, as such licenses are also among the most popular for open-source projects~\cite{top_licenses,vendome2015large} 
and allow commercial/closed-source exploitation.  We note that, in general, the top licenses adopted on Hugging Face %
are traditionally understood to be more permissive, even if their exact application in this novel context is not fully understood.

We also note the adoption (for models, but especially for datasets) of different variants of the CC license, including a permissive one (\license{CC-BY}), restrictive ones (\license{CC-BY-SA} and \license{CC-BY-NC-SA}), and licenses limiting non-commercial use only (\license{CC-BY-NC}).%

Besides OSS licenses, we see a large proportion of different kinds of ML-specific licenses. These include both licenses originating from open-source initiatives (\license{openrail}, \license{creativeml-openrail-m}), and licenses originating from companies (\eg Meta's \license{llama2} and \license{llama3} licenses). 
Such licenses tend to differ from typical redistribution terms established by software licenses by, for example, introducing ``behavioral'' constraints; requiring responsible model usage; and prohibiting adoption for harmful or unethical uses or uses that do not take models' limitations into account. Likewise, the \license{llama2} and \license{llama3} licenses %
impose limitations related to commercial use,  use in products or services having %
more than 700 million monthly active users, and use to train other models, unless such models are redistributed, under the same licenses, as derivative works of \license{llama} models. %







 Last but not least, we note the increased prevalence of Hugging Face's ``Other'' license %
 which was ranked 7th in Pepe \etal \cite{pepe2024hugging} but has moved to the 5th spot since.  %
 In fact, 3.8\% of datasets and 5.7\% of models are licensed with some ``Other'' license. Prior work has found that such license proliferation can make license compliance tasks more time-consuming and difficult, since compliance teams are no longer dealing with known quantities  \cite{wintersgill2024law}.


\subsection{Parent/child license differences} %
\label{sec:base-to-deriv-variations} %

\input{tables/change-frequencies}
\input{tables/license_change_frequencies}

Our dataset contains 274,104 distinct parent/child relationships.  We observed 66,460 instances (24\%) where the licensing of a child model (\ie derivative model) was different from that of its parent (\ie base model). \Cref{tab:license_change_frequencies} provides an overview of the types of differences observed. In nearly a third of cases, child models specified no licensing information despite information being available for their parent(s), leading to a potential compliance violation. We also observe that once license information has been dropped, it is unlikely to be restored by later links in the chain, with license restoration behavior observed in only 15.9\% of instances.  A complete shift in licensing between parent and child was seen in 54.8\% of cases (\ie no licenses were shared between the two).  Of these shifts, 18.5\% involved the ``unknown'' or ``other'' license category.  \Cref{tab:license_changes} shows the top ten such shifts, and \Cref{fig:changes_across_classes} provides more information on how licensing decisions shifted across class boundaries between parents and their children. %
While ML-specific licenses seem a good fit for models, model owners may be opting for licenses they are more familiar, and comfortable with (OSS for software developers and CC for data scientists). We observe that most shifts occur across the OSS and CC boundary (68.3\%), perhaps suggesting some tension between the licensing preferences of those with data science and software development backgrounds. Additionally, depending on the specifics of the situation and the licenses involved, these shifts can also potentially introduce license incompatibilities, such as dropping non-commercial requirements or failing to apply the requirements of a copyleft license. 


\begin{figure}[t]
\centering
\includegraphics[width=0.4\linewidth]{images/license_changes_across_classes.drawio.pdf}
\caption{Parent/child license differences across class boundaries (Total differences: 29,708)}%
\label{fig:changes_across_classes}
\end{figure}

\subsection{Dataset license and model license combinations} 

\input{tables/dataset-and-model-licenses}

The licenses of datasets that are used during training can have an impact on the licensing decisions for the final ML model.  Here we consider the pairings of dataset licenses and the resultant model licenses.  There are 623 distinct model/dataset license combinations within our dataset across 43,455 such pairings. %
(The top ten most frequent can be found in \Cref{tab:dataset_model_licenses}.)  However, since we (and Hugging Face) aggregated all ``other'' licenses and treated them as one quantity, these numbers likely overestimate the consistency in the space. In 41 of the 623 combinations, the license of the model exactly matches the license of at least one dataset it was trained on. %
The most common combination is a model licensed under the \license{apache-2.0} license and a dataset under a custom or ``other'' license.  In total, 11,731 (27\%) pairs involve a dataset under a custom or ``other'' license. This can make using these datasets and models problematic since retrieving and evaluating the licensing terms can be difficult and time-consuming. %

\subsection{Other licensing findings}


\subsubsection{Multi-licensing.}
\label{sec:multilicensing}
A few models (188) %
and datasets (104) were released under more than one license. Multi-licensing is an existing phenomenon in the world of open-source software but is made more complex in the ML model context by the presence of novel license combinations that are not yet well understood.  Examples include the 48 models we observed multi-licensed under \license{apache-2.0} and \license{cc-by-nc-4.0} as well as models under both OSS and ML licenses, such as \license{MIT} and \license{OpenRAIL}. We observe examples of such multilicensing in the most declared datasets in \Cref{tab:top_declared_datasets}. %
We also observed datasets that were released under as many as six distinct licenses. %
The Hugging Face metadata provides no way of determining the relationship among these licenses, which could be either an AND or an OR. This distinction is critical to determining compliance.  For example, if a user who uploaded a \license{llama2} derivative model chose to make it available with an OR relationship between the \license{apache-2.0} and \license{llama2} licenses, this would contradict the exclusive \license{llama2} licensing of its base model, resulting in a violation that could propagate to models further down the chain. %
While we observed only a few instances of multi-licensing, such activity motivates the need to understand the interplay between the various license classes and for Hugging Face to supply a standardized way to specify the intended relationship between licenses.

\subsubsection{Naming requirements.} 
\label{sec:naming_requirements}
Some licenses, like \license{Llama3}, impose a %
generational limitation \cite{oss_book} that specifies naming requirements for derivative works.  According to the license, models that are built on a model under this license must have names prefixed with ``llama3'' \cite{llama3_license}.  In our dataset, there are 3,335 models licensed under \license{llama3}. However, of these, only 473 (14.2\%) models correctly have names beginning with ``llama3'' as required by the license.  Only 768 (23\%) models even contain the term ``llama3'' in their name.  Just 2,421 (72.6\%) models contain the term ``llama'' at all, and 384 (11.5\%) only contain the string ``l3.'' %
The problem is likely even worse in practice, as these statistics only include models that documented that they were under the \license{llama3} license.  %

\subsubsection{Attribution.}
As was observed during the manual model card analysis described in \Cref{sec:license_extraction}, attribution appears to be important to model owners regardless of licensing status: rather than requesting or requiring attribution through a license, even models without a license often asked for some form of attribution (29/84), typically in the form of citations. %
In fact, 83,628 models (11.1\%) mined using the Hugging Face API included a link to a paper on \textit{arxiv} \cite{arxiv} in their tags.  In our manual analysis of model cards, we observed that the model owner was typically also affiliated with (\eg an author on) the linked research paper. %
This desire for recognition is reflected by the top licenses in the space, namely \license{MIT} and \license{Apache-2.0}, each of which specifies attribution requirements. (A similar historical example is the evolution of Creative Commons licenses in the early 2000s, where the overwhelming preference for attribution led to its becoming a default term \cite{standard_attribution}.) Given the research community's adoption of Hugging Face, %
it may be worthwhile to add dedicated metadata fields to facilitate the standardization of citation information. 
