In this section we discuss the implications of our findings and potential directions for future work. %

\subsection{License compliance in the ML supply chain}
In \Cref{sec:rq1}, we showed that the structure of the ML supply chain consists primarily of relatively short component chains. %
This presents a radically different landscape from the traditional software supply chain, in which a single piece of software can depend on potentially hundreds or even thousands of components. However, just because fewer dependencies are declared does not mean that ensuring license compliance is any easier in this ecosystem. In fact, as we observed, licensing information is often missing (\Cref{sec:licensing}) and there can be licensing differences between parent and child models in a model's lineage (\Cref{sec:base-to-deriv-variations}). Compounded with our observation that datasets may also have unclear licensing (\Cref{sec:common_licenses}) and may go undeclared by models (\Cref{sec:most-dependend-upon}), the state of documentation on Hugging Face appears to be counterproductive to license compliance tasks. 
\looseness=-1

The task of managing different licenses across a model's components presents a novel challenge due to the advent of ML-focused licenses and a heavy reliance on \license{CC} licenses. 
While previous work has investigated compatibility between various traditional software licenses~\cite{kapitsaki2022towards,liu2024catch}, new licenses, such as the \license{OpenRAIL} family and the \license{Llama} licenses, require %
analysis to understand how they interact with existing licenses and with each other. Additionally, ML-focused licenses also indicate legitimate uses for the model and any derivatives, further separating them from the traditional open source definition. %
Proponents of \license{OpenRAIL} argue that existing open-source licenses are ``ill-adapted'' to handle ML components' differences from source code, particularly with respect to the responsible use of such components~\cite{openrailTowards}. %
This necessitates future work to provide a detailed analysis of software licenses with respect to ML components, including an analysis of these licenses and the ways in which licenses interact with ML components differently from traditional software.

It also becomes crucial to educate %
model owners and users on the terms associated with licenses.  For example, we observed that the simple naming requirements imposed by the \license{Llama3} license were followed in only 14.2\% of cases (\Cref{sec:naming_requirements}).  This suggests either extensive noncompliance or, more likely, an ignorance of the terms being violated. To concretely identify the cause and provide potential mitigations, future work should investigate developers' views and understanding of licenses in the ML supply chain.

It also remains unclear, particularly with the relationship between datasets and models, how exactly license terms, such as \license{GPL}'s copyleft, should be applied.  To what extent should a model be considered a derivative work of the dataset(s) on which it was trained?  More concretely, if a model trainer trains a model on a dataset governed by a copyleft license and then distributes the model, but not the dataset, has the trainer violated the dataset license?  These questions and many others will likely need to be resolved by legal teams, regulators, and potentially the courts.


Lastly, in \Cref{sec:multilicensing}, we also observed instances of ambiguity introduced through multi-licensing.  Determining the intended relationship among the multiple licenses assigned by the model/dataset owner can be a difficult task, and different pitfalls come with the assumption of an OR versus an AND relationship.  Model hubs should include standardized ways for model/dataset owners to express these relationships and educate users on why they might want to use a multi-licensing scheme.  It is left to future work to explore why some developers are presently releasing their models/datasets under multiple licenses. %





\subsection{Potential difficulties in using model cards as ML/AIBOM}
Amidst the growing push for better transparency and security in software through Software Bills of Materials (SBOMs)~\cite{xia2023empirical, stalnaker2024boms}, calls have been made for similar BOMs for ML components~\cite{stalnaker2024boms, xia2023empirical, barclay2019towards, barclay2022providing}. These documents serve as inventories of all components within a piece of software and can document a variety of the software's traits, including dependencies, licensing, and security. Distinct ML/AI Bills of Materials (ML/AIBOMs) may be necessary to address such components' different inputs, security concerns (such as model poisoning), and ethical considerations~\cite{xia2023empirical, xia2023trust, bi2024way, lu2024taxonomy}. The SPDX \cite{spdx} Working Group, operating under the auspices of the Linux Foundation, is developing guidelines and a proposed standard for such an ML/AIBOMs~\cite{aibom2024} that will complement the SPDX ISO Standard ISO/IEC 5962:2021, which describes the use of SBOMs to document the components used in creating a software system~\cite{ISO5962-2021}. CycloneDX \cite{cyclonedx} is also working on support for ML/AIBOMs~\cite{cyclonedx-mlbom}. %

Prior work has also suggested that model and data cards can serve as an ML/AIBOM~\cite{stalnaker2024boms}, or the information they provide can assist with their creation~\cite{xia2023trust, pepe2024hugging}, but our work suggests that these tools are not yet robust enough to serve this purpose. As we highlight in \Cref{sec:rq0}, the information provided by model cards is often missing key elements, including datasets that the models were trained on. This immediately obviates many of the benefits of ML/AIBOMs, including understanding licensing obligations that might be associated with that dataset, being aware of potential model poisoning attacks, and providing the ability to select models trained on ethically sourced data. Additionally, the cases where model cards were locked behind Terms of Service agreements or other restrictions (\Cref{sec:no_api_access}) could further limit their usefulness as ML/AIBOMs to consumers. Our work provides additional evidence that, in practice, model cards contain little actionable information, making them difficult to use as ML/AIBOMs~\cite{bhat2023aspirations}.

\subsection{The need for follow up surveys and interviews}
Our study raises many questions concerning what motivates developers when they make decisions concerning documentation, model reuse, and licensing.  %
Conversations with Hugging Face developers through surveys and interviews can help to bridge this gap.  There are numerous key questions such work should consider. What factors influence developers when choosing a license for their dataset/model (\Cref{sec:common_licenses})? Why do developers so commonly opt to use an ``other'' license (\Cref{sec:common_licenses})? What motivates developers to multi-license a model/dataset and what is the intention behind the license interaction (\Cref{sec:multilicensing})?  Why do so few developers fine-tuning the \model{llama3} model comply with the naming requirements imposed by the \license{llama3} license (\Cref{sec:naming_requirements})? What is the primary motivation for developers to write documentation for their models?  For each of the various stakeholders in the space (model owners, fine-tuners, and users), which metadata fields do they perceive as most important?  Why are some models listed as datasets (\Cref{sec:models_as_data})? Answers to these questions will inform how future tools, practices, techniques, and even educational materials should be developed to have maximal impact on the emerging ML community. %


\subsection{The addition of fields for recording model relationship information}


As the relationships between ML models become more complex, it becomes increasingly important to keep a record of those relationships (\eg fine-tuned, quantized, ensemble, student/teacher, \etc).  This information is crucial not only for transparency and understanding how the model was created, but also for license compliance.  Just as in traditional software, licenses may have terms that govern the manner in which the component may be (re)used and distributed. For example, many generative models disallow using generated content to train a derivative model~\cite{openai_terms}. As noted in \Cref{sec:no_relationship}, Hugging Face provides no standardized way to express relationship information.  This inability to specify relationships may have resulted in some model owners declaring models used in student-teacher relationships as datasets (\Cref{sec:models_as_data}).  We suggest that model hubs provide a standardized, machine-readable way of conveying this type of information.  Just as Hugging Face currently provides a standard list of architectures, it could also provide a curated list of relationship types.  
\looseness=-1

\subsection{The need to use unique identifiers}
While unique identifiers already exist on Hugging Face, they are not currently being used by model/dataset owners.  This can result in problems, particularly when trying to trace model/dataset provenance.  Model hubs should encourage their users to use these existing unique identifiers, perhaps by including tools that facilitate the creation of model cards or the provision of metadata.  For example, a developer could supply a colloquial name for a model hosted on the platform, such as the familiar owner/model convention, and the tool could automatically conduct a search and, where possible, map the provided name to the unique ID.  This would immediately alert the developer of typos, remove ambiguities, and also prevent the name drift issues we discussed in \Cref{sec:naming-problems,sec:missing_refs}. Furthermore, such a system could potentially allow for automatic updates (or request manual developer updates) when the information for downstream elements changes, thereby facilitating an ecosystem which is more supportive of supply chain management tasks and lifting the burden from downstream developers.


\subsection{Differences between the ML supply chain and traditional software supply chain}
Our findings highlight a notable difference between the ML supply chain and the traditional software supply chain.
In many software ecosystems with dependency management, dependency information for components in the software supply chain is organized in manifest files such as \texttt{pom.xml}, \texttt{package.json}, or \texttt{requirements.txt}.
These files can then be used to generate other dependency tracking documents like SBOMs and have also been used by forges like GitHub to build dependency graphs \cite{dep_graph}.  Notably, however, these manifest files have utility %
beyond dependency tracking.  They are necessary for setting up a fresh installation of the software -- that is, the list of dependencies must be downloaded in order for the software to function properly.  This is not the case with the information supplied by developers in model cards.  Because erroneous mappings to datasets and base models do not leave the model in question unusable as a practical matter, it is therefore easier for typos and other mistakes to go unnoticed, and thus uncorrected.
This disconnect between correctness and functionality necessitates tooling that has similar functionality and richness to dependency management tools for traditional software, as it leaves ample opportunity for errors without comparable means of checking for and correcting them.



\subsection{Quality control for model hubs}
Our analysis of the model documentation available on Hugging Face shows that the information that developers provide is often incomplete, malformed, or only available in human-readable text. We observed some documentation issues, such as the presence of duplicates in lists of datasets and base models (\Cref{sec:no_relationship}) or the declaration of a model as its own base (\Cref{sec:missing_refs}), that could be easily corrected by automated tools like linters. Additionally, an automated tool could also provide standardization for data fields to facilitate metadata analysis.  For example, the variants for how one can specify that there are no datasets or base models ([], ``'', and \textsc{None}) should be standardized to a single type.  We are not suggesting that model hubs should be responsible for curating the metadata of all models on their platforms, but where automated checks can efficiently be put into place, they should be employed. 

Potential solutions could closely resemble approaches to automatically check the quality of bug reports. Similarly to how tools can check for important components of a bug report (\eg, expected behavior, steps to reproduce, \etc)~\cite{chaparro2017detecting}, in theory, tools may be able to check for missing information in model cards. %
Some work is already emerging in this area, such as DocML \cite{bhat2023aspirations} and AIMML \cite{TsayBHSM20}, however, more work needs to be done to understand what suggestions should be made to model owners, how to handle complex licensing scenarios, and how to best implement tools that assess ML documentation. %




\subsection{Potential pitfalls when mining Hugging Face}
The numerous documentation challenges that we discuss in \Cref{sec:rq0}, demonstrate that researchers relying on Hugging Face mining to collect information on the ML supply chain must proceed thoughtfully, with caution and care to avoid potential pitfalls. %
Unlike source code forges like GitHub, it seems that some model owners may be uploading and publishing models as a by-product of their workflows. As such, we speculate that some models are made publicly available not because the owner intends them to be reused by the community, but rather because the owner is using the resources of Hugging Face, Google Colab, or some other cloud provider. %
(This might explain why we observed so many models with the same names.  See \Cref{sec:naming-problems}.) More research is required to understand how such tools might populate the supply chain with such ``intermediate'' models, or those that are otherwise not intended for public consumption. To mitigate the risk such models pose, future studies should be mindful of the models they include in their analysis, erring toward those that are more popular.  However, determining popularity may not be as easy as sorting by downloads or likes.  As shown in \Cref{tab:top_datasets_likes}, the most liked ``dataset'' on Hugging Face is in reality a repository curating ChatGPT prompts.  Future studies will be left to explore the prevalence of curated repositories hosted as datasets and the exact nature of potentially ad hoc developer workflows.
\looseness=-1








