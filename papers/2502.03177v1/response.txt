\section{Background and Related Work}
\label{sec:background}

Simply put, IP cameras are network-connected devices that convert physical movement in their field of view into packets. 
The more movement in the scene, the more packets the cameras generate and transmit to the network. In this section, we provide background material on generating a network denial-of-service (DoS) attack with IP cameras, as well as relevant related work.

\subsection{IP Cameras as Network Devices}
\label{sec:compression}
Modern digital cameras convert light into electronic signals to capture images or video footage. Each video recording is a sequence of frames, typically captured at rates of 24-120 FPS, which, when displayed in sequence to the human eye, create the illusion of motion. These frames generate substantial raw data. For example, an uncompressed 1080×1920 video at 60 FPS requires $\approx 2.986Gb/s$, making raw storage and transmission impractical. To address this, video compression techniques reduce the data size while maintaining a balance between visual quality and computational efficiency **Wiegand et al., "On Multi-Hypothesis Prediction and Motion Compensated Temporal Filtering"**.

Video encoding takes advantage of the fact that our eyes are more sensitive to variations in brightness (luminance) than to changes in color (chrominance) in order to reduce the resolution of color information through a process called \textit{chroma subsampling}, where color data are sampled at a lower resolution than brightness data. Common subsampling schemes like 4:2:2 and 4:2:0 keep all of the luminance data while sampling chrominance at $1/2$ and $1/4$ resolution, respectively.

Another aspect of video encoding is exploiting redundancies within and between frames. Spatial redundancy arises from similarities between neighboring pixels in a single frame, while temporal redundancy stems from similarities between consecutive frames. Techniques such as discrete cosine transform (DCT) address spatial redundancy by transforming image blocks into frequency components. High-frequency components, which represent fine details less noticeable to human viewers, are quantized or discarded. This reduces the data required to represent a frame.

Temporal redundancy arises because consecutive frames in a video sequence often contain similar content, especially in scenes with minimal motion. Instead of encoding each frame in full, only the changes between frames are encoded. This is achieved through motion estimation and compensation.

Regular motion events in an office environment have a negligible impact on the stream size, as modern video compression algorithms are based on motion vectors. 
Motion estimation identifies blocks of pixels in one frame corresponding to blocks in a reference frame. The displacement of these blocks is represented by \textit{motion vectors}, which describe the direction and magnitude of movement. Motion compensation uses these vectors to predict the content of the current frame based on reference frames. By encoding only the differences between the actual and predicted frames, the data size is significantly reduced.
This means that a person walking in a corridor has a similar impact on the generated traffic as a person standing in place. 
Therefore, to generate additional traffic, unexpected or unusual changes in the scene are required. 


% For example, consider a typical security camera monitoring a hallway. While the walls, floor, and doors remain static, they are occasionally interrupted by the movement of a person walking through. Sending the entire scene 30 times per second would be highly inefficient. Instead, the system establishes periodic "key frames" (also known as I-frames), which serve as complete reference images and are typically sent every few seconds or whenever significant scene changes occur. Between these key frames, the camera transmits only the differences, referred to as delta frames (P-frames and B-frames). These delta frames contain information about motion vectors that describe how objects have moved, any changed pixel data, and references to the static parts of the previous frame.
% In that sense, the bitrate of the network traffic that IP cameras generate is adaptive, and it is based on the physical changes in the scene.
% This approach substantially reduces bandwidth requirements while maintaining visual quality. In our hallway example, the codec might only need to transmit data about the moving person while referencing the static background from the previous key frame. This solution also explains why video can sometimes display artifacts during packet loss—if a key frame is missed, the subsequent delta frames lack a crucial reference point for reconstructing the complete image. 


Video encoding employs three types of frames, which work together to balance compression efficiency and video quality:

\begin{itemize} 
    \item \textbf{I-Frames (Intra-coded):} These frames serve as independent reference points, and they are fully encoded without relying on other frames.
    \item \textbf{P-Frames (Predicted):} These frames only store the differences from a preceding frame, reducing data size.
    \item \textbf{B-Frames (Bidirectional):} These frames use data from preceding and subsequent frames to increase encoding efficiency.
\end{itemize}

When encoding a live video stream, B-frames are typically avoided, as the successive frames necessary to define a B-frame are in the yet-unknown future.

Modern video encoding algorithms typically perform a multi-stage process consisting of the following steps:
\begin{enumerate} 
\item \textbf{Color Space Conversion and Chroma Subsampling:} Convert the raw RGB data into a suitable format (e.g., YCbCr) and reduce chrominance resolution.
\item \textbf{Partitioning:} Divide each frame into blocks (e.g., $8\times8$ or $16\times16$ pixels) for efficient and independent processing.
\item \textbf{Motion Estimation and Compensation:} Identify motion vectors and encode only the changes between frames.
\item \textbf{Transformation and Quantization:} Apply DCT to reduce spatial redundancy, followed by quantization to lower precision when less detail is perceived.
\item \textbf{Entropy Coding:} Use lossless compression techniques such as Huffman coding and run-length encoding (RLE) to efficiently represent the quantized data.
\end{enumerate}

\subsection{Adaptive Behavior and Variable Bitrate Traffic}
In this context, consider industrial vibration sensors that increase their sampling and transmission rates upon detecting anomalous machinery behavior **Guldstrand Larsen et al., "On adaptive filtering of sensor measurements for reduced energy consumption"**. 
Under normal operating conditions, these sensors may transmit minimal data. However, when they detect unusual vibrations, they can flood the network with high-frequency measurements and alerts.
Some audio monitoring systems in industrial environments also demonstrate this characteristic, generating variable bitrate traffic that correlates with changes in noise levels **Huang et al., "A Novel Adaptive Sampling Scheme for Industrial Sensor Networks"**.

In cases where the sensors are battery-powered, which is quite common for in-field deployed sensors, increased activity may lead to quicker battery exhaustion. 
For example, to conserve sensor battery life, it is beneficial to reduce the amount of data sampled by water sensors when the monitored environment is stable. 
Submerged sensors that monitor water quality may increase their water sampling rate rapidly once a change in pH value, dissolved oxygen (DO), conductivity, oxidation-reduction potential (ORP), turbidity, or temperature is detected **Kim et al., "Adaptive Water Quality Monitoring System for Real-time Detection of Contaminants"**.

While adaptive behavior optimizes power and bandwidth usage under normal operating conditions, it also results in the direct influence of physical world events on network utilization. 
This relationship creates a potentially exploitable connection between environmental stimuli and resource exhaustion, such as battery depletion.