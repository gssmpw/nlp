\section{Related Work}
\textbf{LLM Reasoning}
Leveraging the Chain-of-Thought~(CoT) technique~\citep{cot,auto-cot}, LLMs have demonstrated impressive performance on various reasoning tasks~\citep{codellama,gpqa,deepseekmath}. CoT enables LLMs to decompose the entire problem into several sub-goals and then reason step-by-step to achieve a more reliable answer. Among various LLM reasoning tasks, mathematical reasoning has become one of the most widely studied and important tasks. Current work on LLM math reasoning primarily focuses on: synthesizing large-scale and diverse math data~\citep{star,metamath, numinamath}, constructing challenging math reasoning benchmarks~\citep{omni-math,frontiermath}, training powerful process reward models~(PRMs)~\citep{prm800k,math-shepherd,skywork-o1}, and designing more effective alignment or reinforcement learning algorithms to improve math reasoning capabilities of LLMs~\citep{step-dpo,rstar-math,prime}.~\looseness=-1

\textbf{Test-Time Scaling}
Recently, scaling test-time compute of LLMs has shown significant potential for further improving their reasoning performance~\citep{large-language-monkeys,more-llm-calls}. Existing test-time scaling studies can be divided into several categories: (1) \textbf{Sampling-based scaling} aims to increase the number of individual reasoning paths of LLMs when solving a given problem. Then the most reliable answer is selected from all the generated options using mechanisms such as majority voting~\citep{self-consistency}, weighted majority voting~\citep{weighted-majority-voting}, or best-of-N selection~\citep{prm800k}. (2) \textbf{Tree search-based scaling} expands reasoning paths by constructing tree-like trajectories, allowing LLMs to explore diverse options at each state and continue reasoning along the most promising directions. Tree-of-Thoughts~(ToT)~\citep{tot} and Monte Carlo Tree Search~(MCTS)~\citep{empirical-compute-optimal-inference,mcts-refine,scaling-optimally,marco-o1} are two typical tree search-based test-time scaling methods. (3) \textbf{In-context search-based scaling} enables LLMs to learn to search, backtrack and re-explore within one single CoT path~\citep{stream-of-search}.  %making the reasoning process more akin to that of humans. 
Recently, OpenAI's o1 model~\citep{o1} have made a significant breakthrough in this line. It leverages reinforcement learning to scale the lengths of CoT to enable LLMs to perform thorough thinking through reflection, verification and re-exploration when solving problems. Following the same line, a series of studies~\citep{skywork-o1,r1,qwq,gemini-flash-thinking,still,o1-journey2} have been proposed to scale CoT lengths during inference time. Our work also primarily focuses on the scaling properties of o1-like models.



\begin{figure*}[t]
  \centering
  \subfigure[Results on MATH500]{\includegraphics[width=0.48\linewidth]{figures/math500_o1_like_models.pdf}
  }
  \subfigure[Results on AIME2024]{
    \includegraphics[width=0.45\linewidth]{figures/aime24_o1_like_models.pdf}
  }
  \caption{The accuracy and the average number of tokens for each model on MATH500 and AIME2024. To ensure a fair comparison, we tokenized all model outputs using the Qwen2.5 tokenizer.}
  \label{fig: o1-like models acc and tokens}
  \vskip -0.05in
\end{figure*}


We notice that there is a few concurrent studies~\citep{o1-overthinking, o1-pruner} highlighting that existing o1-like models exhibit overthinking issues, often generating an excessive number of tokens for simple problems with minimal benefit. Thus, they aim to shortening the CoT lengths of o1-like models while preserving their performance. However, our work differs in that we aim to uncover a deeper and more critical issue: scaling with more tokens can, in some cases, even degrade the model's performance. Thus, our work focuses on achieving optimal test-time scaling from base models in both aspects of effectiveness and efficiency.