\documentclass{article}


% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2024


% ready for submission
% \usepackage{neurips_2024}


% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
\usepackage[nonatbib,preprint]{neurips_2024}


% to compile a camera-ready version, add the [final] option, e.g.:
    % \usepackage[final]{neurips_2024}


% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2024}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{multirow} 
\newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}
% \usepackage{natbib}
\bibliographystyle{plainnat}
\usepackage[numbers]{natbib}
\usepackage{wrapfig}

\usepackage{tcolorbox}
\newtcolorbox{prompt}[1]{
    % enhanced,
    % drop shadow=black!5!white,
    left=4mm,
    right=4mm,
    top=1mm,
    bottom=1mm,
    boxsep=0mm,
    rounded corners,
    title=#1,    fontupper=\scriptsize\linespread{0.7}\fontfamily{lmr}\selectfont,
}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}



\title{Towards Thinking-Optimal Scaling of Test-Time Compute for LLM Reasoning}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.



\author{Wenkai Yang\thanks{Work was done while Wenkai Yang was at internship in MSRA.} \,$^1$, Shuming Ma$^2$, Yankai Lin\thanks{Corresponding Author} \,$^1$, Furu Wei$^2$ \\
  $^1$Gaoling School of Artificial Intelligence, Renmin University of China \\
  $^2$Microsoft Research, Asia \\
    \texttt{\{wenkaiyang, yankailin\}@ruc.edu.cn } \\ \texttt{\{shuming.ma, fuwei\}@microsoft.com  } 
    }



\begin{document}


\maketitle


\begin{abstract}
Recent studies have shown that making a model spend more time thinking through longer Chain of Thoughts~(CoTs) enables it to gain significant improvements in complex reasoning tasks. While current researches continue to explore the benefits of increasing test-time compute by extending the CoT lengths of Large Language Models~(LLMs), 
we are concerned about a potential issue hidden behind the current pursuit of test-time scaling: \textit{Would excessively scaling the CoT length actually bring adverse effects to a model's reasoning performance?} 
Our explorations on mathematical reasoning tasks reveal an unexpected finding that scaling with longer CoTs can indeed impair the reasoning performance of LLMs in certain domains. 
Moreover, we discover that there exists an optimal scaled length distribution that differs across different domains. 
Based on these insights, we propose a Thinking-Optimal Scaling strategy. Our method first uses a small set of seed data with varying response length distributions to teach the model to adopt different reasoning efforts for deep thinking. Then, the model selects its shortest correct response under different reasoning efforts on additional problems for self-improvement. Our self-improved models built upon Qwen2.5-32B-Instruct outperform other distillation-based 32B o1-like models across various math benchmarks, and achieve performance on par with QwQ-32B-Preview.

\end{abstract}

\section{Introduction}
\label{introduction}
Recently, System-2 thinking~\citep{system-2-thinking} has become an important research area for enhancing the reasoning capabilities of Large Language Models~(LLMs). Unlike previous System-1 thinking systems~\citep{qwenmath,deepseekmath} that perform fast thinking, such a slow thinking system aims to increase the test-time compute of LLMs to make them think more thoroughly before responding to a question. OpenAI' o1 model~\citep{o1} has demonstrated a promising potential in this direction. By incentivizing the model to employ longer internal Chain of Thoughts~(CoTs)~\citep{cot} for thinking, o1 shows human-like reasoning capabilities, including searching, reflecting, backtracking, and re-exploring in its reasoning process, and achieves outstanding performance on complex reasoning tasks~\cite{math,gpqa}.

Subsequently, a series of follow-up studies~\citep{qwq,skywork-o1,gemini-flash-thinking} have been proposed to imitate and explore o1-like thinking systems. These studies try to scale the number of reasoning tokens of LLMs either by distilling from existing o1-like models~\citep{o1-journey2,still,sky-t1} or reinforcement learning~\citep{r1,k15}, and gain significant improvements compared to earlier reasoning models~\citep{qwenmath, deepseekmath}. 

Behind the promising paradigm of test-time scaling, there is a few concurrent studies~\citep{o1-overthinking, o1-pruner} highlighting an efficiency issue of overthinking in existing o1-like models, where they tend to generate an excessive number of tokens, even for simple questions that could be answered correctly with just a few tokens.  However, we are concerned about a more critical issue that \textit{could the excessive pursuit of longer CoTs have negative impacts on the model's reasoning performance?} 
%The recent study~\citep{o1-overthinking} highlights an issue of overthinking in existing o1-like models, where they tend to generate an excessive number of tokens, even for simple questions that could be answered with just a few tokens. This work, instead, takes a step further and deeper to 
That is, besides the efficiency issues, our aim is to explore and study whether and how overly test-time scaling could potentially impair the reasoning performance of LLMs, typically in the math domain.
%our aim is to understand the  of test-time scaling: whether and how overly scaling could potentially impair the reasoning performance, typically in the math domain.


To study the problem, we first calculate and compare the accuracies and used reasoning tokens of several o1-like models and their corresponding System-1 thinking models on MATH500~\citep{prm800k} and AIME2024\footnote{\url{https://huggingface.co/datasets/AI-MO/aimo-validation-aime}} (see Figure~\ref{fig: o1-like models acc and tokens}). We find that subsequent o1-like models, QwQ-32B-Preview~\citep{qwq} as an typical example, generate much more tokens but gain only limited improvements in model performance. This preliminary exploration indicates that scaling to more reasoning tokens might not consistently lead to better performance. Then, to fairly investigate the performance comparison of the same base model after scaling with different lengths of CoTs, we conduct additional experiments on LLaMA3.1-8B-Instruct~\citep{llama3.1} and Qwen2.5-32B-Instruct~\citep{qwen2.5}. Specifically, we utilize QwQ-32B-Preview~\citep{qwq} to generate and filter three types of reasoning paths with different lengths for the same set of prompts. Then, we teach the base model to use different reasoning efforts (i.e., different numbers of reasoning tokens) to solve a given problem based on learning on different subsets. Surprisingly, we find that training with longer reasoning paths leads to worse performance especially in easier tasks, and there exists an optimal reasoning effort that varies across tasks of different difficulty levels. Our further analysis reveals that longer CoTs may contain more erroneous steps. Though including a certain incorrect steps and subsequent reflective steps
can teach the model how to correct errors during inference, excessive erroneous steps can have a negative impact.

Based on the above findings, we propose a \underline{T}hinking-\underline{OP}timal \underline{S}caling strategy (TOPS) that allows LLMs to decide by themselves how many tokens are needed to solve a given problem. The motivation is, if an LLM can already answer a question correctly under the given reasoning effort, increasing the response length with additional tokens may have adverse effects as longer responses are more likely to include erroneous steps. On the other hand, encouraging LLMs to spend more time thinking brings benefits to tackling more challenging problems. Therefore, we first use a small set of o1-like responses under different reasoning efforts (i.e., of varying lengths) to train a ``tag'' model, which is used to generate responses for a large set of math problems under different reasoning efforts. Then, we select the shortest correct response generated across all reasoning efforts given the same problem to create a thinking-optimal dataset, which is used for the self-improvement of the base model. Our self-improved model based on Qwen2.5-32B-Instruct achieves better performance than existing distillation-based 32B o1-like models in various benchmarks with varying levels of difficulty, including GSM8K~\citep{gsm8k}, MATH500 and AIME2024. Furthermore, we perform iterative self-improvement and obtain a reasoning model that achieves comparable performance to QwQ-32B-Preview.







\section{Related Work}


\textbf{LLM Reasoning}
Leveraging the Chain-of-Thought~(CoT) technique~\citep{cot,auto-cot}, LLMs have demonstrated impressive performance on various reasoning tasks~\citep{codellama,gpqa,deepseekmath}. CoT enables LLMs to decompose the entire problem into several sub-goals and then reason step-by-step to achieve a more reliable answer. Among various LLM reasoning tasks, mathematical reasoning has become one of the most widely studied and important tasks. Current work on LLM math reasoning primarily focuses on: synthesizing large-scale and diverse math data~\citep{star,metamath, numinamath}, constructing challenging math reasoning benchmarks~\citep{omni-math,frontiermath}, training powerful process reward models~(PRMs)~\citep{prm800k,math-shepherd,skywork-o1}, and designing more effective alignment or reinforcement learning algorithms to improve math reasoning capabilities of LLMs~\citep{step-dpo,rstar-math,prime}.~\looseness=-1

\textbf{Test-Time Scaling}
Recently, scaling test-time compute of LLMs has shown significant potential for further improving their reasoning performance~\citep{large-language-monkeys,more-llm-calls}. Existing test-time scaling studies can be divided into several categories: (1) \textbf{Sampling-based scaling} aims to increase the number of individual reasoning paths of LLMs when solving a given problem. Then the most reliable answer is selected from all the generated options using mechanisms such as majority voting~\citep{self-consistency}, weighted majority voting~\citep{weighted-majority-voting}, or best-of-N selection~\citep{prm800k}. (2) \textbf{Tree search-based scaling} expands reasoning paths by constructing tree-like trajectories, allowing LLMs to explore diverse options at each state and continue reasoning along the most promising directions. Tree-of-Thoughts~(ToT)~\citep{tot} and Monte Carlo Tree Search~(MCTS)~\citep{empirical-compute-optimal-inference,mcts-refine,scaling-optimally,marco-o1} are two typical tree search-based test-time scaling methods. (3) \textbf{In-context search-based scaling} enables LLMs to learn to search, backtrack and re-explore within one single CoT path~\citep{stream-of-search}.  %making the reasoning process more akin to that of humans. 
Recently, OpenAI's o1 model~\citep{o1} have made a significant breakthrough in this line. It leverages reinforcement learning to scale the lengths of CoT to enable LLMs to perform thorough thinking through reflection, verification and re-exploration when solving problems. Following the same line, a series of studies~\citep{skywork-o1,r1,qwq,gemini-flash-thinking,still,o1-journey2} have been proposed to scale CoT lengths during inference time. Our work also primarily focuses on the scaling properties of o1-like models.



\begin{figure*}[t]
  \centering
  \subfigure[Results on MATH500]{\includegraphics[width=0.48\linewidth]{figures/math500_o1_like_models.pdf}
  }
  \subfigure[Results on AIME2024]{
    \includegraphics[width=0.45\linewidth]{figures/aime24_o1_like_models.pdf}
  }
  \caption{The accuracy and the average number of tokens for each model on MATH500 and AIME2024. To ensure a fair comparison, we tokenized all model outputs using the Qwen2.5 tokenizer.}
  \label{fig: o1-like models acc and tokens}
  \vskip -0.05in
\end{figure*}


We notice that there is a few concurrent studies~\citep{o1-overthinking, o1-pruner} highlighting that existing o1-like models exhibit overthinking issues, often generating an excessive number of tokens for simple problems with minimal benefit. Thus, they aim to shortening the CoT lengths of o1-like models while preserving their performance. However, our work differs in that we aim to uncover a deeper and more critical issue: scaling with more tokens can, in some cases, even degrade the model's performance. Thus, our work focuses on achieving optimal test-time scaling from base models in both aspects of effectiveness and efficiency.


\section{The Impact of Scaling Efforts on the Effectiveness of Test-Time Scaling}

\subsection{Preliminary Analysis on Existing o1-Like Models}
\label{subsec: preliminary analysis on o1-like models}




Though o1-like models has proven to be much more effective on reasoning tasks than previous System-1 thinking models, we are curious about the scaling process behind the these o1-like models. That is, we want to explore that: \textit{How effective has their scaling achieved compared to their corresponding System-1 thinking models(e.g., QwQ-32B-Instruct v.s.\
Qwen2.5-32B-Instruct)?}

We first conduct a preliminary analysis on several existing typical o1-like models along with their corresponding System-1 thinking models. Specifically, we choose o1-mini~\citep{o1} v.s.\ gpt-4o/4o-mini~\citep{gpt4o},\footnote{Note that o1-mini and 4o/4o-mini do not have equivalent number of parameters, but we make a rough comparison here.} Gemini2.0-Flash-Thinking-Exp.-1219~\citep{gemini-flash-thinking} v.s.\ Gemini2.0-Flash-Exp.~\citep{gemini-flash}, and QwQ-32B-Preview~\citep{qwq} v.s.\ Qwen2.5-32B-Instruct~\citep{qwen2.5} as our experimental models. We calculate the accuracy and the average number of generated tokens of each model on two typical benchmarks: \textbf{MATH500}~\citep{prm800k}: 500 high school math competition problems across various subjects, sampled from MATH benchmark~\citep{math}; \textbf{AIME2024}: 30 challenging problems from the American Invitational Mathematics Examination~(AIME). To address the issue of token counts not being directly comparable due to the different tokenizers used by different models, we standardize by using Qwen2.5 tokenizer to tokenize the reasoning completions of different models and then calculate the number of tokens. As the internal CoT of o1-mini is not available to users, we use an estimation strategy based on the summary part, the number of reasoning tokens and total number of completion tokens returned from the o1-mini model to estimate the number of tokens of hidden CoT tokenized by Qwen2.5 tokenizer. Details are in Appendix~\ref{appendix: estimate o1 model tokens}. We set the maximum number of generation tokens to 16,384 for each model in all evaluations in this paper.


\begin{figure*}[t]
    \centering
    \input{tag_prompts}
    \caption{System prompts for QwQ-32B-Preview with varying levels of reasoning effort.}
    \label{fig: tag system prompts}
    \vskip -0.05in
\end{figure*}


We put the visualization results in Figure~\ref{fig: o1-like models acc and tokens}. As we can see, subsequent o1-like models (QwQ-32B-Preview and Gemini2.0-Flash-Thinking) show less effective scaling effects compared with o1-mini, as they generate much more tokens but gain less improvements when scaling from their corresponding System-1 thinking models. QwQ-32B-Preview has the most severe issue in this regard. This preliminary analysis suggests, to some extent, that excessively scaling to longer CoTs does not maximize test-time scaling effects.


\subsection{Deeper Explorations on The Scaling Process of CoT Length}
\label{subsec: experiments on tag models}
\input{tables/tag_data_statistics}




\begin{figure*}[t]
  \centering

  \subfigure[Results on GSM8K]{\includegraphics[width=0.32\textwidth]{figures/llama_tag_model_gsm8k.pdf}
  }
  \subfigure[Results on MATH500]{\includegraphics[width=0.32\textwidth]{figures/llama_tag_model_math500.pdf}
  }
  \subfigure[Results on AIME2024]{
    \includegraphics[width=0.32\textwidth]{figures/llama_tag_model_aime2024.pdf}
  }
  \caption{The accuracy and the average number of tokens of LLaMA3.1-8B-Instruct and LLaMA3.1-8B-Tag under different reasoning efforts (``Low'', ``Medium'' and ``High'') on different benchmarks with varying levels of difficulty.}
  \label{fig: llama3.1 tag model acc and tokens}
\end{figure*}

\begin{figure*}[t]
  \centering

  \subfigure[Results on GSM8K]{\includegraphics[width=0.32\textwidth]{figures/qwen_tag_model_gsm8k.pdf}
  }
  \subfigure[Results on MATH500]{\includegraphics[width=0.32\textwidth]{figures/qwen_tag_model_math500.pdf}
  }
  \subfigure[Results on AIME2024]{
    \includegraphics[width=0.32\textwidth]{figures/qwen_tag_model_aime2024.pdf}
  }
  \caption{The accuracy and the average number of tokens of Qwen2.5-32B-Instruct and Qwen2.5-32B-Tag under different reasoning efforts (``Low'', ``Medium'' and ``High'') on different benchmarks with varying levels of difficulty.}
  \label{fig: qwen2.5 tag model acc and tokens}
  \vskip -0.05in
\end{figure*}


The above analysis still faces a problem that the base models of different o1-like models are not identical, making it unfairly to compare the impacts of scaled CoT lengths on test-time scaling effects of different models. Therefore, we conduct experiments on LLaMA3.1-8B-Instruct~\citep{llama3.1} and Qwen2.5-32B-Instruct~\citep{qwen2.5} to fairly investigate the impact of the scaling efforts on the effectiveness of test-time scaling. Specifically, we first use three system prompts (refer to Figure~\ref{fig: tag system prompts}), corresponding to different levels of reasoning effort (``Low'', ``Medium'' and ``High''), to prompt QwQ-32B-Preview to generate solutions of different numbers of tokens for the same set of math problems sampled
%, constructed by a small portion of GSM8K, MATH, and Olympiads problems 
from NuminaMath~\citep{numinamath}. We then filter out the problems that can be answered correctly under all three reasoning efforts, along with the corresponding three reasoning paths of different lengths. However, we find that QwQ-32B-Preview has relatively poor instruction-following abilities, reflected in that the length distributions of the generated responses for the same question does not closely match the specified system prompts. Therefore, for a given problem, we further reorder the three responses based on their lengths and keep them if their pairwise length difference consistently exceeds 300 tokens. The length is determined either by LLaMA3.1 tokenizer or Qwen2.5 tokenizer depending on the chosen experimental model. Finally, we curate a set of 1.3K problems, each accompanied by three o1-like responses of varying lengths. The data statistics of each set for each model is shown in Table~\ref{tab: tag data statistics}. We assign different system prompts (the same in Figure~\ref{fig: tag system prompts}) to each type of responses and train the base model on all three types of samples. By doing so, we ensure the consistency between the base model and the training problems, allowing for a fair comparison on the impacts of different scaling efforts on the effectiveness of test-time scaling. We refer to the fine-tuned model as the ``tag" model (LLaMA3.1-8B-Tag and Qwen2.5-32B-Tag). During inference, we can use different system prompts to guide the tag model in applying varying levels of reasoning effort to answer the questions.


%We call the fine-tuned model the ``tag'' model (LLaMA3.1-8B-Tag and Qwen2.5-32B-Tag). During inference, we can use different system prompts to enable the tag model to use different reasoning efforts to perform the reasoning and answer the question. 

For training, the learning rate is $1\times 10^{-5}$, the batch size is $32$, the number of epochs is 3 for Qwen2.5-32B-Tag and 5 for LLaMA3.1-8B-Tag. For evaluation, besides \textbf{MATH500} and \textbf{AIME2024} introduced before, we further include \textbf{GSM8K}~\citep{gsm8k} that contains 1319 grade school math word problems. In the following, unless otherwise specified, we set the decoding temperature to 1.0 for o1-like models, by following the recommended setting   
for o1 model,\footnote{\url{https://platform.openai.com/docs/guides/reasoning}} which we also find is more suitable for o1-like model's reasoning. For System-1 thinking models LLaMA3.1-8B-Instruct and Qwen2.5-32B-Instruct, we report the results for both decoding temperatures, 1.0 and 0.0, for comprehensive comparison. Each result under 1.0 temperature is averaged over 5 random seeds. The full results on LLaMA3.1-8B-Instruct and Qwen2.5-32B-Instruct are shown in Figure~\ref{fig: llama3.1 tag model acc and tokens} and Figure~\ref{fig: qwen2.5 tag model acc and tokens} respectively. We can draw several interesting conclusions from these results: (1) \textbf{A small number o1-like responses is already highly effective in enhancing the reasoning performance of LLMs.} This is also consistent with the findings in previous studies~\citep{still,o1-journey2}. (2) \textbf{Scaling with longer CoTs can bring negative effects to the model's reasoning performance in certain domains}, especially on easy tasks. For example, both LLaMA3.1-8B-Tag and Qwen2.5-32B-Tag perform worse under high reasoning effort compared to the other two reasoning efforts, while consuming significantly more tokens, particularly on GSM8K and MATH500. (3) \textbf{There exists an optimal reasoning efforts that varies across different tasks of varying difficulty levels.} As we can see, low reasoning effort consistently works best on GSM8K, while medium and high reasoning efforts are more beneficial for harder question.




% \begin{figure*}[t]
% % \vskip 0.2in
% \begin{center}
% \centerline{\includegraphics[width=0.95\linewidth]{figures/demo.pdf}}
% \caption{The illustration of our Thinking-Optimal Scaling method. Our method includes three stages: \textbf{Format Imitation} enables the base model to learn how to adopt different levels of reasoning effort $e_{i}$ to perform System-2 thinking, using a small set of seed data. \textbf{Reasoning Effort-Conditioned Generation} requires the model to apply System-2 thinking to a large set of problems under different reasoning efforts. \textbf{Self-Improvement} select the shortest correct response for each problem among all responses to fine-tune the base model to achieve thinking-optimal test-time scaling.}
% \label{fig: demo}
% \end{center}
% \vskip -0.2in
% \end{figure*}

% \begin{figure}[ht]
% % \vskip 0.2in
% \begin{center}
% \centerline{\includegraphics[width=0.98\linewidth]{figures/tag_data_stat.pdf}}
% \caption{The statistics of responses under different reasoning efforts for training the tag models.}
% \label{fig: step analysis}
% \end{center}
% \vskip -0.25in
% \end{figure}


\subsection{Analysis on The Adverse Effects of Excessive Length Scaling}

\begin{wrapfigure}{R}{0.5\linewidth}
\begin{center}
\vskip -0.15in
\centerline{\includegraphics[width=1.0\linewidth]{figures/tag_data_stat.pdf}}
\caption{The statistics of responses under different reasoning efforts for training the tag models.}
\label{fig: step analysis}
%\vskip -0.1in
\end{center}
\vskip -0.15in
\end{wrapfigure}

Here, we take a deeper step to explore why training on longer CoTs leads to a decline in model's reasoning performance. We randomly selected 100 problems from the tag model's training set %(50 problems for each of GSM8K, MATH and Olympiads sets), 
along with their responses under three different reasoning efforts. We first follow the existing study~\citep{o1-overthinking} to use \texttt{gpt-4o} to determine the total number of reasoning rounds contained in each response. Each reasoning round is defined as a complete reasoning process or verification process that contains a final answer. Besides, we further use \texttt{gpt-4o} to determine the number of reasoning rounds that contain erroneous steps or wrong final answers. The utilized prompt is shown in Appendix~\ref{appendix: gpt4o prompt}. 
We visualize the average number of reasoning rounds and the average number of erroneous reasoning rounds on each problem under each reasoning effort in Figure~\ref{fig: step analysis}.


First, we can see that the number of reasoning rounds consistently increases from low reasoning effort to high reasoning effort. It could lead to the overthinking issue in reasoning models~\citep{o1-overthinking}, where redundant tokens are used to answer the question through repetitive verification. This, however, should not affect the accuracy if all reasoning rounds are correct. Unfortunately, we observe a pattern that \textbf{the number of erroneous reasoning rounds also increases when the reasoning effort becomes higher}. Training the model on more wrong steps would bring adverse effects to the model's reasoning abilities, which can explain why scaling with high reasoning effort leads to worse results. Therefore, we can conclude that while including a certain incorrect and reflective steps can help the model learn to correct errors during inference, an excess of erroneous steps can have a detrimental impact on model's learning.

\section{Thinking-Optimal Test-Time Scaling}


%The above analysis reveals that excessively scaling the response lengths of LLMs can lead to negative consequences. We are motivated that an optimal way to achieve test-time scaling is to let the model itself decide how many tokens are needed to solve the problem. That is, for easier questions, if the model can already answer the question correctly within a certain number of tokens, further extending the CoT length is not necessary and sub-optimal as it may include more erroneous steps into the reasoning process. On the other hand,

The above analysis reveals that excessively increasing the response lengths of LLMs can result in negative consequences. This motivates us that an optimal approach to achieve test-time scaling is allowing the model to determine by itself the number of tokens needed to solve each problem. Specifically, for a simple question, if the model can provide a correct answer within a certain number of tokens, further extending the CoTs becomes suboptimal, as it may introduce unnecessary overthinking or even additional erroneous steps into the reasoning process. Conversely, the model should be encouraged to use more tokens for difficult problems if additional reasoning effort can help it to obtain a more reliable and accurate answer.

Thus, we propose a \textbf{\underline{T}hinking-\underline{OP}timal \underline{S}caling} (\textbf{TOPS}) strategy aiming to achieve more effective and efficient test-time scaling for LLM reasoning. We define \textbf{a System-2 thinking-optimal response as the shortest correct response that the model can generate using System-2 thinking}: fewer tokens may lead to wrong answer while more tokens causes overthinking. Then, our method includes three stages: Format Imitation, Reasoning Effort-Conditioned Generation, and Self-Improvement.


\begin{figure*}[t]
% \vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=0.95\linewidth]{figures/demo.pdf}}
\caption{The illustration of our Thinking-Optimal Scaling method. Our method includes three stages: \textbf{Format Imitation} enables the base model to learn how to adopt different levels of reasoning effort $e_{i}$ to perform System-2 thinking, using a small set of seed data. \textbf{Reasoning Effort-Conditioned Generation} requires the model to apply System-2 thinking to a large set of problems under different reasoning efforts. \textbf{Self-Improvement} select the shortest correct response for each problem among all responses to fine-tune the base model to achieve thinking-optimal test-time scaling.}
\label{fig: demo}
\end{center}
\vskip -0.2in
\end{figure*}

\textbf{Format Imitation} First, we require a small set of o1-like responses for a cold start, enabling the model to learn the format of System-2 thinking patterns, including searching, reflecting, verification, backtracking, etc. Preliminary results in Section~\ref{subsec: experiments on tag models} and previous studies~\citep{still, o1-journey2} have shown that a small number of o1-like responses is sufficient for the model to effectively learn the format of System-2 reasoning. Such a small set of seed data can be manually human-written or generated by existing o1-like models. However, different from previous studies~\citep{still,sky-t1} that use a fixed length distribution of seed samples (i.e., directly generated by existing o1-like models), which may not be a thinking-optimal distribution for our base model, we instead create the seed data containing responses under different reasoning efforts (i.e., different length distributions). Specifically, we define a small number of seed problems as $\mathcal{P}_{s}$, and our goal is to curate a seed dataset $\mathcal{D}_{s}=\mathcal{D}_{s_1} \cup \cdots \cup \mathcal{D}_{s_n}$, where $\mathcal{D}_{s_{i}}=\{  (e_i,x,y_{e_i})|x\sim \mathcal{P}_{s}\}$ represents the responses to seed problems under a specific reasoning effort $e_i$. %This is the same procedure as we train the tag model in Section~\ref{subsec: experiments on tag models}. By doing so, we can teach the base model to adopt different levels of reasoning efforts to perform System-2 thinking on a given problem.
This follows the same procedure as data curation for the tag model in Section~\ref{subsec: experiments on tag models}. Then, we train the base model on this dataset to obtain the tag model that can apply different levels of reasoning effort to perform System-2 thinking on a given problem:~\looseness=-1
\begin{equation}
\label{eq: train tag model}
% \small
\begin{aligned}
    \boldsymbol{\theta}_{tag} = \mathop{\arg\max}_{\boldsymbol{\theta}} \mathbb{E}_{(e_i,x,y_{e_i})\sim \mathcal{D}_{s}} [ P(y_{e_i} | e_i,x,\boldsymbol{\theta})].
\end{aligned}
\end{equation}
%which paves the way to find the thinking-optimal response for the problem.

\textbf{Reasoning Effort-Conditioned Generation} We then use the tag model to generate the solutions on a large number of additional math problems $\mathcal{P}_{a}$ under different reasoning efforts $e_i$:
\begin{equation}
\label{eq: train response generation}
% \small
\begin{aligned}
    y_{e_i} \sim \pi(e_{i},x|\boldsymbol{\theta}_{tag}), \quad x\sim \mathcal{P}_{a},
\end{aligned}
\end{equation}
where $\pi(\cdot|\boldsymbol{\theta}_{tag})$ denotes the output distribution of the tag model.  
We select the shortest correct solution $y_{sc}$ among all generations $\{y_{e_1} ,\cdots,y_{e_{n}}\}$ as the thinking-optimal response for problem $x$, and obtain a thinking-optimal self-improvement dataset $\mathcal{D}_{TOP} = \{(x,y_{sc})|x\sim \mathcal{P}_{a}) \}$.

\textbf{Self-Improvement} After obtaining the thinking-optimal dataset determined by the model itself, we can use it to train the base model, enabling the base model to achieve better self-improvement on System-2 thinking. Specifically, we perform Supervised Fine-Tuning~(SFT) to the base model on $\mathcal{D}_{TOP}$:
\begin{equation}
\label{eq: sft}
% \small
\begin{aligned}
    \boldsymbol{\theta}_{TOP} = \mathop{\arg\max}_{\boldsymbol{\theta}} \mathbb{E}_{(x,y_{sc})\sim \mathcal{D}_{TOP}} [ P(y_{sc} | x,\boldsymbol{\theta})].
\end{aligned}
\end{equation}


The full illustration of our method is shown in Figure~\ref{fig: demo}.


\section{Experiments and Analysis}
\subsection{Experimental Settings}
\textbf{Base Model} We perform our Thinking-Optimal Scaling strategy mainly on Qwen2.5-32B-Instruct,  as it serves as an appropriate base model for exploration on test-time scaling according to previous works~\citep{qwq, still,sky-t1}.

\textbf{Datasets} First, we directly use the model Qwen2.5-32B-Tag created in Section~\ref{subsec: experiments on tag models} as the tag model for reasoning effort-conditioned generation. As mentioned before, this tag model is trained on the seed data that contains 1.3K problems from a subset of NuminaMath, and totally 3.9K responses under three types of reasoning efforts generated by QwQ-32B-Preview. We then use this tag model to generate responses on an additional subset of NuminaMath containing extra 50K problems %\footnote{We only keep the problems from GSM8K, MATH and Olympiads sets.} 
under different reasoning efforts. On each problem, we sample only 1 response for each reasoning effort, though we believe that performing multiple samplings could further enhance effectiveness. For each problem, we select the shortest correct response among all three responses to the problem as the thinking-optimal response. Finally, we incorporate the responses corresponding to low reasoning effort from the seed data into the above generated dataset, resulting in a thinking-optimal dataset of about 26K samples for self-improvement. We denote the self-improved model created by our method as \textbf{Qwen2.5-32B-TOPS}. We then evaluate the performance of our model on three typical math reasoning benchmarks: GSM8K, MATH500, and AIME2024, which are introduced in Section~\ref{subsec: preliminary analysis on o1-like models} and Section~\ref{subsec: experiments on tag models}.


\textbf{Training and Evaluation Details}
In the SFT stage, the learning rate is $1\times 10^{-5}$, the batch size is $96$,  and the number of epochs is 2. In inference, the decoding temperature is $1.0$, the maximum generation length is 16,384. We report the average accuracy across 5 random seeds in each experiment. In the evaluation of MATH500, we also use \texttt{gpt-4o} to assist in identifying missed cases caused by format issues by following~\citet{omni-math}. 

\textbf{Baselines} Besides the base model Qwen2.5-32B-Instruct, we compare our self-improved model with several existing o1-like models that are based on the same base model:
\begin{itemize}
    \item \textbf{QwQ-32B-Preview}: One of the most popular o1-like reasoning models developed by Qwen Team.
    \item  \textbf{STILL-2-32B}~\citep{still}: A System-2 thinking model trained on 3.9K challenging math examples generated by QwQ-32B-Preview and DeepSeek-R1-Lite\footnote{\url{https://api-docs.deepseek.com/news/news1120}}.
    \item \textbf{Sky-T1-32B-Preview}~\cite{sky-t1}: The reasoning model trained on 17K examples generated by QwQ-32B-Preview, including 10K math examples.~\looseness=-1
    \item \textbf{Qwen2.5-32B-Random}: After the reasoning effort-based generation by the tag model, we randomly select a correct solution on each problem rather than the shortest one to form a thinking-suboptimal dataset, and train the base model on this dataset.
\end{itemize}


\subsection{Main Results}
\input{tables/self_improvement_results}
The results of each model are displayed in Table~\ref{tab: self-improvement results}. Besides the accuracy, we also report the number of CoT tokens used by each model on each dataset for reference.\footnote{For STILL-2-32B and Sky-T1-32B-Preview, we only calculate the number of tokens in the thought part and do not include the tokens in the summary part.
}

First, we can see that the model trained under thinking-optimal samples (Qwen2.5-32B-TOPS) consistently performs better than the model trained under thinking-suboptimal samples (Qwen2.5-32B-Random). This helps to revalidate our motivation that scaling with shortest correct responses, as determined by the base model itself using System-2 thinking, is the most effective approach to achieve optimal test-time scaling. Second, compared to distillation-based models STILL-2-32B and Sky-T1-32B-Preview, our self-improvement-based model Qwen2.5-32B-TOPS achieves better results across the board, except for AIME2024, where it slightly underperforms STILL-2-32B. However, note that STILL-2-32B uses a greater number of high-quality distilled samples (3.9K) including more challenging problems from AIME1983-2023, whereas our model achieves comparable performance using only 1.3K seed samples and effective self-improvement strategy. 
%Finally, our self-improved model achieves performance comparable to QwQ-32B-Preview.

Regarding the reasoning efforts (i.e., the number of reasoning tokens) used by each model to solve different difficulty levels of tasks, we observe that Qwen2.5-32B-TOPS uses fewer tokens on easier tasks like GSM8K compared to other models, effectively mitigating the issue of overthinking~\citep{o1-overthinking}. On the other hand, it tends to spend more time thinking on harder problems such as AIME2024, demonstrating adaptive reasoning depths.

\subsection{Results of Iterative Self-Improvement}
To further enhance the reasoning performance of our model on challenging problems, we perform iterative self-improvement on Qwen2.5-32B-TOPS. Specifically, we select additional 4500 MATH problems~\citep{math} (which have not appeared in the previously used problems) and the problems from AIME1983-2023. On each problem, we sample 8 responses from Qwen2.5-32B-TOPS. Then, we select the shortest correct response among 8 responses as the chosen response. One iterative self-improvement approach is to further supervised fine-tune Qwen2.5-32B-TOPS on the dataset composed of all chosen responses (shortest correct responses), resulting in \textbf{Qwen2.5-32B-TOPS-Iter-SFT}. Besides, we can also perform preference optimization. Specifically, if there are responses with incorrect final answers, we select the longest incorrect response as the rejected response to improve reasoning capability. 
%Otherwise, the longest correct response is chosen as the rejected one to mitigate overthinking. 
Additionally, we include preference pairs where the rejected response is the shortest wrong response if there exists a wrong response that is shorter than the shortest correct response, to avoid causing the model to underthink. After obtaining the preference dataset, we perform Direct Preference Optimization~(DPO)~\citep{dpo} on Qwen2.5-32B-TOPS to get \textbf{Qwen2.5-32B-TOPS-Iter-DPO}. Detailed experimental settings are in Appendix~\ref{appendix: experimental settings}.

The results of iterative self-improvement are in Table~\ref{tab: self-improvement results}. As we can observe, further SFT is mainly effective in shortening the CoT lengths but does not necessarily improve reasoning performance. Preference optimization improves both the efficiency and the effectiveness, resulting in a reasoning model that is comparable to QwQ-32B-Preview.

\section{Conclusion}
In this work, we explore a potential issue under current pursuit of test-time scaling. Through empirical analysis on mathematical reasoning tasks, we first demonstrate that overly long CoTs can negatively impact the model's reasoning performance in certain domains, emphasizing the need for an optimal length distribution during CoT length scaling. To tackle this, we propose a Thinking-Optimal Scaling strategy, which leverages a small set of seed data to teach LLMs to adopt varying levels of reasoning effort to perform System-2 thinking. Then, our approach allows models to identify the shortest correct response for self-improvement, leading to a more efficient and effective System-2 thinking. Experimental results show that our self-improved and further iteratively self-improved models, based on Qwen2.5-32B-Instruct, outperform existing distillation-based o1-like models and achieve performance that is on par with QwQ-32B-Preview across various math benchmarks. 




















% If using bibtex, please protect capital letters of names and
% abbreviations in titles, for example, use \{B\}ayesian or \{L\}ipschitz
% in your .bib file.



% Acknowledgements should only appear in the accepted version.
% \section*{Acknowledgements}


\section*{Impact Statement}

This work aims to explore the effectiveness and limitations of o1-like test-time scaling. Our goal is to enhance both the efficiency and effectiveness of test-time scaling in a more thinking-optimal way. These findings highlight the importance of adaptive reasoning efforts and provide a promising direction for more effectively enhancing LLM reasoning capabilities.




\bibliography{neurips}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage
\clearpage
\appendix
% \onecolumn

\section{Details on Estimating The Number of Tokens in Hidden CoT of o1-mini by Qwen2.5 Tokenizer}
\label{appendix: estimate o1 model tokens}

In the preliminary analysis in Section~\ref{subsec: preliminary analysis on o1-like models}, we use Qwen2.5 tokenizer to calculate the number of tokens in the CoTs generated by each model for a fair comparison. However, o1-mini does not expose the internal CoTs to users, but only shows the summary parts $S$, along with the number of reasoning tokens $n_{r}^{o1}$ and total number completion tokens $n_{c}^{o1}$ (the sum of reasoning tokens and summary tokens) measured by o1-mini tokenizer. Therefore, we choose to estimate the number of tokens in its hidden CoTs measured by the Qwen2.5 tokenizer using $S$, $n_{r}^{o1}$ and $n_{c}^{o1}$. Specifically, we denote the number of tokens of the summary part measured by Qwen2.5 tokenizer as $n_{s}^{qwen}$, then the estimated number of tokens of hidden CoT by Qwen2.5 tokenizer can be calculated as $n_{s}^{qwen} \times \frac{n_{r}^{o1}}{n_{c}^{o1} - n_{r}^{o1}}$.


\section{User Prompt for \texttt{gpt-4o} to Determine the Number of (Erroneous) Reasoning Rounds}
\label{appendix: gpt4o prompt}
\input{gpt_step_analysis_prompt}

\section{Experimental Settings}
\label{appendix: experimental settings}
\subsection{Training Settings in Format Imitation}
In the format imitation stage, we perform SFT on the base model Qwen2.5-32B-Instruct on a small subset of seed data containing 1.3K problems sampled from NuminaMath along with responses with varying lengths for each problem. The statistics of the seed data is shown in Table~\ref{tab: tag data statistics}. In SFT stage, the learning rate is $1\times 10^{-5}$, the batch size is $32$, the number of epochs is $3$. 

\subsection{Training Settings in Self-Improvement}
In the self-improvement stage, we perform SFT on Qwen2.5-32B-Instruct on the curated thinking-optimal dataset for 2 epochs. The learning rate is $1\times 10^{-5}$, and the batch size is 96. 

\subsection{Training Settings in Iterative Self-Improvement}
In the iterative self-improvement stage, for Qwen2.5-32B-TOPS-Iter-SFT, the learning rate is $1\times10^{-6}$, the batch size is 32, and we set the training epoch to 1. For Qwen2.5-32B-TOPS-Iter-DPO, the learning rate is $5\times10^{-7}$, the batch size is 32, the training epoch is 3. 

\subsection{Evaluation Settings}
For all o1-like models, we set the decoding temperature to 1.0 and average the results over 5 random seeds for each evaluation experiment. The maximum generation
length is 16,384. 

\end{document}