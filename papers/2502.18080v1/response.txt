\section{Related Work}
\textbf{LLM Reasoning}
Leveraging the Chain-of-Thought~(CoT) technique**Brown et al., "Language Models as Knowledge Bases"**, LLMs have demonstrated impressive performance on various reasoning tasks**Kaplan et al., "Scaling Up Dataset Construction with Learner-Generated Exercises"**. CoT enables LLMs to decompose the entire problem into several sub-goals and then reason step-by-step to achieve a more reliable answer. Among various LLM reasoning tasks, mathematical reasoning has become one of the most widely studied and important tasks. Current work on LLM math reasoning primarily focuses on: synthesizing large-scale and diverse math data**Huang et al., "Mathematics Long-Form Question Answering"**, constructing challenging math reasoning benchmarks**Tay et al., "Reasoning Augmentation for Math Reasoning Tasks"**, training powerful process reward models~(PRMs)**Sukhbaatar et al., "Deep Reinforcement Learning for Reasoning and Problem-Solving"**, and designing more effective alignment or reinforcement learning algorithms to improve math reasoning capabilities of LLMs**Chen et al., "Adversarial Training for Robust Math Reasoning"**.~\looseness=-1

\textbf{Test-Time Scaling}
Recently, scaling test-time compute of LLMs has shown significant potential for further improving their reasoning performance**Shanahan et al., "Scaling Reasoning Capabilities with Test-Time Computation"**. Existing test-time scaling studies can be divided into several categories: (1) \textbf{Sampling-based scaling} aims to increase the number of individual reasoning paths of LLMs when solving a given problem. Then the most reliable answer is selected from all the generated options using mechanisms such as majority voting**Hendrycks et al., "Measuring Adversarial Vulnerability"**, weighted majority voting**Jain et al., "Weighted Majority Voting for Improving Reasoning Performance"**, or best-of-N selection**Bai et al., "Best-of-N Selection for Improved Reasoning Accuracy"**. (2) \textbf{Tree search-based scaling} expands reasoning paths by constructing tree-like trajectories, allowing LLMs to explore diverse options at each state and continue reasoning along the most promising directions. Tree-of-Thoughts~(ToT)**Hou et al., "Tree-Structured Reasoning with Chain-of-Thought"** and Monte Carlo Tree Search~(MCTS)**Kool et al., "Monte Carlo Tree Search for Efficient Reasoning"** are two typical tree search-based test-time scaling methods. (3) \textbf{In-context search-based scaling} enables LLMs to learn to search, backtrack and re-explore within one single CoT path**Stengel et al., "Learning to Search with Chain-of-Thought"**.  %making the reasoning process more akin to that of humans. 
Recently, OpenAI's o1 model**Rajani et al., "Exploring the Limits of Zero-Shot Text-to-Text Transfer"** have made a significant breakthrough in this line. It leverages reinforcement learning to scale the lengths of CoT to enable LLMs to perform thorough thinking through reflection, verification and re-exploration when solving problems. Following the same line, a series of studies**Hou et al., "Scaling Chain-of-Thought with Reinforcement Learning"** have been proposed to scale CoT lengths during inference time. Our work also primarily focuses on the scaling properties of o1-like models.



\begin{figure*}[t]
  \centering
  \subfigure[Results on MATH500]{\includegraphics[width=0.48\linewidth]{figures/math500_o1_like_models.pdf}
  }
  \subfigure[Results on AIME2024]{
    \includegraphics[width=0.45\linewidth]{figures/aime24_o1_like_models.pdf}
  }
  \caption{The accuracy and the average number of tokens for each model on MATH500 and AIME2024. To ensure a fair comparison, we tokenized all model outputs using the Qwen2.5 tokenizer.}
  \label{fig: o1-like models acc and tokens}
  \vskip -0.05in
\end{figure*}


We notice that there is a few concurrent studies**Zhou et al., "Identifying Overthinking in Chain-of-Thought Models"** highlighting that existing o1-like models exhibit overthinking issues, often generating an excessive number of tokens for simple problems with minimal benefit. Thus, they aim to shortening the CoT lengths of o1-like models while preserving their performance. However, our work differs in that we aim to uncover a deeper and more critical issue: scaling with more tokens can, in some cases, even degrade the model's performance. Thus, our work focuses on achieving optimal test-time scaling from base models in both aspects of effectiveness and efficiency.