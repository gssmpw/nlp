\section{Related Work}
\subsection{Task-oriented Human-Robot Communication}


Task-oriented communication between humans and robots is becoming increasingly important, especially in the context of end-user robot programming. Natural language programming allows non-expert users to specify robot tasks through verbal instructions \cite{connell_verbal_2019, matuszek_learning_2013, thomason_improving_2019,ionescu_programming_2021,lauria_mobile_2002}. This approach enables users to communicate complex task requirements and specify reusable programs through multi-turn dialogues \cite{gorostiza_end-user_2011,stenmark_natural_2013,thomason_improving_2019}, making it a fundamental aspect of end-user robot programming \cite{ajaykumar_survey_2022,lieberman_end-user_2006}.

The integration of artificial intelligence with natural language programming is considered a key method for future household robots and intelligent agents to provide personalized services \cite{fischer_adaptive_2023}. 
Through natural dialogue, this approach enables untrained users without programming knowledge to define reusable robot programs that align with their practical needs.
Recent advancements in large language models (LLMs) 
\cite{wei_emergent_2022} have further accelerated developments in this field. Several studies have explored how LLM capabilities can support end-users in defining robot tasks using natural language \cite{fang_enabling_2024, gargioni_integrating_2024}. 
However, specifying robot tasks based on natural language descriptions of desired program outcomes still poses challenges due to the abstraction gap between natural language and program code \cite{liu_what_2023}.
Recent advancements in LLM-based end-user programming have investigated structured and visualized “intermediate-level” representations to address the abstraction gap \cite{liu_what_2023,ge_cocobo_2024}.

LLM-based end-user development (EUD) transforms programming into a collaborative and iterative communication process \cite{karli_alchemist_2024,fischer_adaptive_2023}.
Effective communication between users and robots often requires a continuous cycle of ``intention expression → result feedback → intention adjustment'' to complete task specification \cite{glassman_designing_2023}. This process typically integrates multiple modalities to provide user feedback and represent program or task context \cite{huang_vipo_2020,zhang_patterns_2021,fang_enabling_2024}, or supports users in expressing intentions through multimodal interactions \cite{porfirio_sketching_2023}. Furthermore, human-like multimodal interactions have been explored to facilitate task communication between users and robots \cite{higger_toward_2023,huang_gestures_2024}.



In light of these advancements and the challenges inherent in LLM-based EUD systems, there is an opportunity to enhance verbal programming. This motivates us to draw inspiration from human-to-human verbal communication and explore natural, intuitive interactions that leverage large language models to improve the usability of verbal programming.



 
\subsection{Visual Aids in Human-Robot Communication}



Screens on robots serve an important function in enhancing communication by displaying facial expressions and complementing voice interactions \cite{glauser_how_2023, you_emi_2020, chen_teaching_2020}. Beyond expressive functions, screens also facilitate the communication of complex messages, yet their integration with verbal communication remains an area requiring further exploration.

Currently, the way touch screens are used in service robots has led to their perception as mere “screen bearers”, diminishing the sense of rich interaction with an autonomous agent.
In most cases, they are employed as a means to circumvent the current limitations of full speech interaction while also providing an effective way to enable complex interactions \cite{bonarini_communication_2020}. 
However, balancing this with other interaction modalities is important to maintain a rich interaction experience. The consistency between voice interactions and graphical user interfaces is crucial for improving system usability and user satisfaction \cite{okamoto_usability_2009}, making it easier for users to understand and operate the system.
Research indicates that providing graphical feedback during dialogues significantly enhances user comprehension of the robot’s responses and intentions \cite{peng_understanding_2020}.

Visual media can support task communication through various forms, including robot-mounted screens, external display devices, and extended reality (XR) interfaces \cite{carriero_human-robot_2023,maccio_mixed_2022,carriero_human-robot_2023}. Additionally, external objects and gestures can serve as references to assist in task communication, enhancing naturalness and comprehension \cite{huang_gestures_2024, higger_toward_2023}. 
Some researchers have explored projection techniques that allow robots to visualize task information on physical objects in the environment \cite{andersen_projecting_2016}. 
Compared to single-modality interactions, interactions with robots that exhibit natural multimodal expression provide a more engaging and intuitive communication experience \cite{breazeal_social_2016}. 
Drawing inspiration from human-to-human interaction patterns has proven effective in refining robot communication strategies \cite{huang_gestures_2024}. Since human-AI agent communication is inherently dialogic, requiring iterative exchanges to refine intent expression, the selection of interaction modalities should be contextually adapted to different scenarios \cite{glassman_designing_2023}.





\subsection{LLM-driven Dynamic UI Generation}

Recent work has demonstrated that large language models can serve as intermediaries for multimodal interfaces, enabling both understanding and generation beyond purely natural language content. For example, in graphical user interfaces, LLMs can interpret the semantics and structure of UIs \cite{you_ferret-ui_2024,duan_towards_2023} and generate UI designs \cite{lu_ui_2023,kargaran_menucraft_2023}, demonstrating their potential in processing and generating non-linguistic visual representations.

Furthermore, the world knowledge and in-context learning capabilities of LLMs \cite{zhao_survey_2024} enable the dynamic generation of multimodal interactions tailored to the context on the fly. For instance, GenEM \cite{mahadevan_generative_2024} utilizes LLMs to flexibly generate and adapt robot expressive behaviors based on natural language instructions and user preferences. Similarly, SiSCo \cite{sonawani_sisco_2024} showcases the ability of LLMs to synthesize both natural language and visual signals adaptively for efficient collaboration. In the context of end-user programming, Cocobo \cite{ge_cocobo_2024} illustrates how LLMs can dynamically translate between natural language and visual programming representations.

Unlike traditional rule-based or template-based approaches, these LLM-based methods show the potential for adaptive contextual understanding and immediate generation across multiple modalities. This motivates our exploration of leveraging LLMs to dynamically generate visual aids for task-oriented communication between humans and robots.







\begin{figure*}
  \centering
  \includegraphics[width=\textwidth]{figures/mapDraw.jpg}
  \caption{Sample maps drawn by participants during the formative study, showing how they used visual elements to represent and communicate spatial tasks through annotations, paths, and markers.}
  \Description{The figure displays a collection of hand-drawn maps created by study participants during the formative study. Each map shows how participants used different visual elements like arrows, circles, text labels, and color coding to represent spatial tasks and communicate their understanding of task requirements.}
  \label{fig:paper map}
\end{figure*}