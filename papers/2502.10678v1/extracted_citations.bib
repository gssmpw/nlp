@article{ajaykumar_survey_2022,
	title = {A {Survey} on {End}-{User} {Robot} {Programming}},
	volume = {54},
	issn = {0360-0300, 1557-7341},
	url = {https://dl.acm.org/doi/10.1145/3466819},
	doi = {10.1145/3466819},
	abstract = {As robots interact with a broader range of end-users, end-user robot programming has helped democratize robot programming by empowering end-users who may not have experience in robot programming to customize robots to meet their individual contextual needs. This article surveys work on end-user robot programming, with a focus on end-user program specification. It describes the primary domains, programming phases, and design choices represented by the end-user robot programming literature. The survey concludes by highlighting open directions for further investigation to enhance and widen the reach of end-user robot programming systems.},
	language = {en},
	number = {8},
	urldate = {2023-03-02},
	journal = {ACM Computing Surveys},
	author = {Ajaykumar, Gopika and Steele, Maureen and Huang, Chien-Ming},
	month = nov,
	year = {2022},
	keywords = {ccfInfo: CCF-None CSUR, citationNumber: 26, ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê},
	pages = {1--36},
}

@inproceedings{andersen_projecting_2016,
	address = {New York, NY, USA},
	title = {Projecting robot intentions into human environments},
	isbn = {978-1-5090-3929-6},
	url = {http://ieeexplore.ieee.org/document/7745145/},
	doi = {10.1109/ROMAN.2016.7745145},
	urldate = {2024-09-12},
	booktitle = {2016 25th {IEEE} {International} {Symposium} on {Robot} and {Human} {Interactive} {Communication} ({RO}-{MAN})},
	publisher = {IEEE},
	author = {Andersen, Rasmus S. and Madsen, Ole and Moeslund, Thomas B. and Amor, Heni Ben},
	month = aug,
	year = {2016},
	keywords = {\#EX: ‰∏çÂåÖÂê´HRI, \#EX:‰∏çÂåÖÂê´Â§ßÊ®°Âûã, \#EX:Â∑•‰∏öÊú∫Âô®‰∫∫, \#‰∏çÁ°ÆÂÆö, \#Áü≠Êñá, \#ÁªºËø∞, /unread, Design, Users, ccfInfo: CCF-None RO-MAN, citationNumber: 77},
	pages = {294--301},
}

@article{bonarini_communication_2020,
	title = {Communication in {Human}-{Robot} {Interaction}},
	volume = {1},
	issn = {2662-4087},
	url = {https://doi.org/10.1007/s43154-020-00026-1},
	doi = {10.1007/s43154-020-00026-1},
	abstract = {To present the multi-faceted aspects of communication between robot and humans (HRI), putting in evidence that it is not limited to language-based interaction, but it includes all aspects that are relevant in communication among physical beings, exploiting all the available sensor channels.},
	language = {en},
	number = {4},
	urldate = {2024-09-11},
	journal = {Current Robotics Reports},
	author = {Bonarini, Andrea},
	month = dec,
	year = {2020},
	keywords = {\#EX: ‰∏çÂåÖÂê´HRI, \#EX:‰∏çÂåÖÂê´Â§ßÊ®°Âûã, \#EX:Â∑•‰∏öÊú∫Âô®‰∫∫, \#‰∏çÁ°ÆÂÆö, \#Áü≠Êñá, \#ÁªºËø∞, /unread, Artificial Intelligence¬†‰∫∫Â∑•Êô∫ËÉΩ, Communication, Design, Human-robot interaction, Robot design, Users, ccfInfo: Not Found, ‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è},
	pages = {279--285},
}

@incollection{breazeal_social_2016,
	address = {Cham},
	title = {Social {Robotics}},
	isbn = {978-3-319-32552-1},
	url = {https://doi.org/10.1007/978-3-319-32552-1_72},
	abstract = {This chapter surveys some of the principal research trends in Social Robotics and its application to human‚Äìrobot interaction (HRIhuman‚Äìrobotinteraction). Social (or Sociable) robots are designed to interact with people in a¬†natural, interpersonal manner¬†‚Äì often to achieve positive outcomes in diverse applications such as education, health, quality of life, entertainment, communication, and tasks requiring collaborative teamwork. The long-term goal of creating social robots that are competent and capable partners for people is quite a¬†challenging task. They will need to be able to communicate naturally with people using both verbal and nonverbal signals. They will need to engage us not only on a¬†cognitive level, but on an emotional level as well in order to provide effective social and task-related support to people. They will need a¬†wide range of social-cognitive skills and a¬†theory of other minds to understand human behavior, and to be intuitively understood by people. A¬†deep understanding of human intelligence and behavior across multiple dimensions (i.‚ÄØe., cognitive, affective, physical, social, etc.) is necessary in order to design robots that can successfully play a¬†beneficial role in the daily lives of people. This requires a¬†multidisciplinary approach where the design of social robot technologies and methodologies are informed by robotics, artificial intelligence, psychology, neuroscience, human factors, design, anthropology, and more.},
	language = {en},
	urldate = {2024-09-12},
	booktitle = {Springer {Handbook} of {Robotics}},
	publisher = {Springer International Publishing},
	author = {Breazeal, Cynthia and Dautenhahn, Kerstin and Kanda, Takayuki},
	editor = {Siciliano, Bruno and Khatib, Oussama},
	year = {2016},
	doi = {10.1007/978-3-319-32552-1_72},
	keywords = {\#EX: ‰∏çÂåÖÂê´HRI, \#EX:‰∏çÂåÖÂê´Â§ßÊ®°Âûã, \#EX:Â∑•‰∏öÊú∫Âô®‰∫∫, \#‰∏çÁ°ÆÂÆö, \#Áü≠Êñá, \#ÁªºËø∞, /unread, Ambient Assisted Living, Design, False Belief Task, Humanoid Robot, Robot Interaction, Social Robot, Users, ccfInfo: CCF-None ROBO},
	pages = {1935--1972},
}

@article{carriero_human-robot_2023,
	title = {Human-{Robot} {Collaboration}: {An} {Augmented} {Reality} {Toolkit} for {Bi}-{Directional} {Interaction}},
	volume = {13},
	copyright = {https://creativecommons.org/licenses/by/4.0/},
	issn = {2076-3417},
	shorttitle = {Human-{Robot} {Collaboration}},
	url = {https://www.mdpi.com/2076-3417/13/20/11295},
	doi = {10.3390/app132011295},
	abstract = {This work proposes an Augmented Reality (AR) application designed for HoloLens 2 which allows human operators, without particular experience or knowledge of robotics, to easily interact with collaborative robots. Building on the application presented in a previous work of the authors, the novel contributions are focused on a bi-directional interaction that manages the exchange of data from the robot to the human operator and, in the meantime, the Ô¨Çow of commands in the opposite direction. More in detail, the application includes the reading of the robot state, in terms of joint positions, velocities and torques, the visualization of the workspace and the generation and manipulation of the end-effector trajectory by directly moving a set of way-points displayed in the AR environment. Finally, the trajectory feasibility is veriÔ¨Åed and notiÔ¨Åed to the user by taking into account the workspace limits. A usability study of the AR platform has been conducted involving 45 participants with different ages and expertise in robot programming and Extended Reality (XR) platforms, comparing two programming methods: a classical kinesthetic teaching interface, provided by the Franka Emika Panda cobot, and the presented AR platform. Participants have reported the effectiveness of the proposed platform, experiencing less physical demand and higher intuitiveness and usability.},
	language = {en},
	number = {20},
	urldate = {2024-09-11},
	journal = {Applied Sciences},
	author = {Carriero, Graziano and Calzone, Nicolas and Sileo, Monica and Pierri, Francesco and Caccavale, Fabrizio and Mozzillo, Rocco},
	month = oct,
	year = {2023},
	keywords = {\#EX: ‰∏çÂåÖÂê´HRI, \#EX:‰∏çÂåÖÂê´Â§ßÊ®°Âûã, \#EX:Â∑•‰∏öÊú∫Âô®‰∫∫, \#‰∏çÁ°ÆÂÆö, \#Áü≠Êñá, \#ÁªºËø∞, Design, Users, ccfInfo: Not Found, citationNumber: 0},
	pages = {11295},
}

@article{chen_teaching_2020,
	title = {Teaching and learning with children: {Impact} of reciprocal peer learning with a social robot on children‚Äôs learning and emotive engagement},
	volume = {150},
	issn = {0360-1315},
	shorttitle = {Teaching and learning with children},
	url = {https://www.sciencedirect.com/science/article/pii/S0360131520300373},
	doi = {10.1016/j.compedu.2020.103836},
	abstract = {Pedagogical agents are typically designed to take on a single role: either as a tutor who guides and instructs the student, or as a tutee that learns from the student to reinforce what he/she knows. While both agent-role paradigms have been shown to promote student learning, we hypothesize that there will be heightened benefit with respect to students‚Äô learning and emotional engagement if the agent engages children in a more peer-like way ‚Äî adaptively switching between tutor/tutee roles. In this work, we present a novel active role-switching (ARS) policy trained using reinforcement learning, in which the agent is rewarded for adapting its tutor or tutee behavior to the child‚Äôs knowledge mastery level. To investigate how the three different child‚Äìagent interaction paradigms (tutee, tutor, and peer agents) impact children‚Äôs learning and affective engagement, we designed a randomized controlled between-subject experiment. Fifty-nine children aged 5‚Äì7 years old from a local public school participated in a collaborative word-learning activity with one of the three agent-role paradigms. Our analysis revealed that children‚Äôs vocabulary acquisition benefited from the robot tutor‚Äôs instruction and knowledge demonstration, whereas children exhibited slightly greater affect on their faces when the robot behaves as a tutee of the child. This synergistic effect between tutor and tutee roles suggests why our adaptive peer-like agent brought the most benefit to children‚Äôs vocabulary learning and affective engagement, as compared to an agent that interacts only as a tutor or tutee for the child. This work sheds light on how fixed role (tutor/tutee) and adaptive role (peer) agents support children‚Äôs cognitive and emotional needs as they play and learn. It also contributes to an important new dimension of designing educational agents ‚Äî actively adapting roles based on the student‚Äôs engagement and learning needs.},
	urldate = {2024-12-11},
	journal = {Computers \& Education},
	author = {Chen, Huili and Park, Hae Won and Breazeal, Cynthia},
	month = jun,
	year = {2020},
	keywords = {/unread, Cooperative/collaborative learning, Evaluation of CAL systems, Intelligent tutoring systems, Interactive learning environments},
	pages = {103836},
}

@misc{connell_verbal_2019,
	title = {Verbal {Programming} of {Robot} {Behavior}},
	url = {http://arxiv.org/abs/1911.09782},
	doi = {10.48550/arXiv.1911.09782},
	abstract = {Home robots may come with many sophisticated built-in abilities, however there will always be a degree of customization needed for each user and environment. Ideally this should be accomplished through one-shot learning, as collecting the large number of examples needed for statistical inference is tedious. A particularly appealing approach is to simply explain to the robot, via speech, what it should be doing. In this paper we describe the ALIA cognitive architecture that is able to effectively incorporate user-supplied advice and prohibitions in this manner. The functioning of the implemented system on a small robot is illustrated by an associated video.},
	language = {en-US},
	urldate = {2024-09-04},
	publisher = {arXiv},
	author = {Connell, Jonathan},
	month = nov,
	year = {2019},
	note = {arXiv:1911.09782 [cs]},
	keywords = {\#EX: ‰∏çÂåÖÂê´HRI, \#EX:‰∏çÂåÖÂê´Â§ßÊ®°Âûã, \#EX:Â∑•‰∏öÊú∫Âô®‰∫∫, \#‰∏çÁ°ÆÂÆö, \#Áü≠Êñá, \#ÁªºËø∞, /unread, Computer Science - Artificial Intelligence, Computer Science - Robotics, Design, Users, ccfInfo: CCF-None CORR, citationNumber: 0},
}

@inproceedings{duan_towards_2023,
	address = {San Francisco CA USA},
	title = {Towards {Generating} {UI} {Design} {Feedback} with {LLMs}},
	isbn = {979-8-4007-0096-5},
	url = {https://dl.acm.org/doi/10.1145/3586182.3615810},
	doi = {10.1145/3586182.3615810},
	language = {en},
	urldate = {2024-03-22},
	booktitle = {Adjunct {Proceedings} of the 36th {Annual} {ACM} {Symposium} on {User} {Interface} {Software} and {Technology}},
	publisher = {ACM},
	author = {Duan, Peitong and Warner, Jeremy and Hartmann, Bjoern},
	month = oct,
	year = {2023},
	keywords = {ccfInfo: CCF-A UIST, citationNumber: 0},
	pages = {1--3},
}

@inproceedings{fang_enabling_2024,
	title = {Enabling {Waypoint} {Generation} for {Collaborative} {Robots} using {LLMs} and {Mixed} {Reality}},
	url = {https://openreview.net/forum?id=89F2jYzASY},
	abstract = {Programming a robotic is a complex task, as it demands the user to have a good command of specific programming languages and awareness of the robot's physical constraints. We propose a framework that simplifies robot deployment by allowing direct communication using natural language. It uses large language models (LLM) for prompt processing, workspace understanding, and waypoint generation. It also employs Augmented Reality (AR) to provide visual feedback of the planned outcome. We showcase the effectiveness of our framework with a simple pick-and-place task, which we implement on a real robot. Moreover, we present an early concept of expressive robot behavior and skill generation that can be used to communicate with the user and learn new skills (e.g., object grasping)},
	language = {en},
	urldate = {2025-02-03},
	author = {Fang, Cathy Mengying and Zielinski, Krzysztof and Maes, Patricia and Paradiso, Joe and Blumberg, Bruce and Kj√¶rgaard, Mikkel Baun},
	month = apr,
	year = {2024},
}

@inproceedings{fischer_adaptive_2023,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Adaptive and {Adaptable} {Systems}: {Differentiating} and {Integrating} {AI} and {EUD}},
	isbn = {978-3-031-34433-6},
	shorttitle = {Adaptive and {Adaptable} {Systems}},
	doi = {10.1007/978-3-031-34433-6_1},
	abstract = {The framework presented in the paper identifies the promises and pitfalls of Artificial Intelligence (AI) and End-User Development (EUD) approaches by focusing on two basic system components: (1) adaptive systems (grounded in AI) that change their behavior automatically driven by context-aware mechanisms including models of their users and specific task contexts, and (2) adaptable systems (grounded in EUD) that can be adjusted, modified, and extended by their users in order to capture unforeseen and important emergent user needs and aspects of problems. Grounded in an analysis of design trade-offs between the two approaches, arguments, and examples for creating a desirable symbiosis between adaptive and adaptable systems are described and design guidelines for future socio-technical environments are explored contributing to the development of theoretical concepts for the future of EUD.},
	language = {en},
	booktitle = {End-{User} {Development}},
	publisher = {Springer Nature Switzerland},
	author = {Fischer, Gerhard},
	editor = {Spano, Lucio Davide and Schmidt, Albrecht and Santoro, Carmen and Stumpf, Simone},
	year = {2023},
	keywords = {Auto-Correct, ChatGPT, Personalization, adaptable systems meta-design, adaptive systems, artificial intelligence, ccfInfo: CCF-None ISEUD, citationNumber: 0, context-aware interactions, creativity, design guidelines, design trade-offs, end-user development, user and task modeling, ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê},
	pages = {3--18},
}

@inproceedings{gargioni_integrating_2024,
	address = {New York, NY, USA},
	series = {{HRI} '24},
	title = {Integrating {ChatGPT} with {Blockly} for {End}-{User} {Development} of {Robot} {Tasks}},
	isbn = {979-8-4007-0323-2},
	url = {https://dl.acm.org/doi/10.1145/3610978.3640653},
	doi = {10.1145/3610978.3640653},
	abstract = {This paper presents an End-User Development environment for collaborative robot programming, which integrates Open AI ChatGPT with Google Blockly. Within this environment, a user, who is neither expert in robotics nor in computer programming, can define the items characterizing the application domain (e.g., objects, actions, and locations) and define pick-and-place tasks involving these items. Task definition can be achieved with a combination of natural language and block-based interaction, which exploits the computational capabilities of ChatGPT and the graphical interaction features offered by Blockly, to check the correctness of generated robot programs and modify them through direct manipulation.},
	urldate = {2024-05-13},
	booktitle = {Companion of the 2024 {ACM}/{IEEE} {International} {Conference} on {Human}-{Robot} {Interaction}},
	publisher = {Association for Computing Machinery},
	author = {Gargioni, Luigi and Fogli, Daniela},
	month = mar,
	year = {2024},
	keywords = {ccfInfo: CCF-None HRI, citationNumber: 0, collaborative robots, end-user development, human-machine interaction, human-robot collaboration},
	pages = {478--482},
}

@inproceedings{ge_cocobo_2024,
	title = {Cocobo: {Exploring} {Large} {Language} {Models} as the {Engine} for {End}-{User} {Robot} {Programming}},
	shorttitle = {Cocobo},
	url = {https://ieeexplore.ieee.org/document/10714576},
	doi = {10.1109/VL/HCC60511.2024.00020},
	abstract = {End-user development allows everyday users to tailor service robots or applications to their needs. One user-friendly approach is natural language programming. However, it encounters challenges such as an expansive user expression space and limited support for debugging and editing, which restrict its application in end-user programming. The emergence of large language models (LLMs) offers promising avenues for the translation and interpretation between human language instructions and the code executed by robots, but their application in end-user programming systems requires further study. We introduce Cocobo, a natural language programming system with interactive diagrams powered by LLMs. Cocobo employs LLMs to understand users‚Äô authoring intentions, generate and explain robot programs, and facilitate the conversion between executable code and flowchart representations. Our user study shows that Cocobo has a low learning curve, enabling even users with zero coding experience to customize robot programs successfully.},
	urldate = {2025-02-03},
	booktitle = {2024 {IEEE} {Symposium} on {Visual} {Languages} and {Human}-{Centric} {Computing} ({VL}/{HCC})},
	author = {Ge, Yate and Dai, Yi and Shan, Run and Li, Kechun and Hu, Yuanda and Sun, Xiaohua},
	month = sep,
	year = {2024},
	note = {ISSN: 1943-6106},
	keywords = {Codes, Computational modeling, Debugging, Encoding, End-User Development, Engines, Flowcharts, Large Language Model, Large language models, Robot Programming, Robot programming, Service robots, Visualization},
	pages = {89--95},
}

@misc{glassman_designing_2023,
	title = {Designing {Interfaces} for {Human}-{Computer} {Communication}: {An} {On}-{Going} {Collection} of {Considerations}},
	shorttitle = {Designing {Interfaces} for {Human}-{Computer} {Communication}},
	url = {http://arxiv.org/abs/2309.02257},
	doi = {10.48550/arXiv.2309.02257},
	abstract = {While we do not always use words, communicating what we want to an AI is a conversation -- with ourselves as well as with it, a recurring loop with optional steps depending on the complexity of the situation and our request. Any given conversation of this type may include: (a) the human forming an intent, (b) the human expressing that intent as a command or utterance, (c) the AI performing one or more rounds of inference on that command to resolve ambiguities and/or requesting clarifications from the human, (d) the AI showing the inferred meaning of the command and/or its execution on current and future situations or data, (e) the human hopefully correctly recognizing whether the AI's interpretation actually aligns with their intent. In the process, they may (f) update their model of the AI's capabilities and characteristics, (g) update their model of the situations in which the AI is executing its interpretation of their intent, (h) confirm or refine their intent, and (i) revise their expression of their intent to the AI, where the loop repeats until the human is satisfied. With these critical cognitive and computational steps within this back-and-forth laid out as a framework, it is easier to anticipate where communication can fail, and design algorithms and interfaces that ameliorate those failure points.},
	urldate = {2025-02-03},
	publisher = {arXiv},
	author = {Glassman, Elena L.},
	month = sep,
	year = {2023},
	note = {arXiv:2309.02257 [cs]},
	keywords = {Computer Science - Human-Computer Interaction},
}

@article{glauser_how_2023,
	title = {How can social robot use cases in healthcare be pushed - with an interoperable programming interface},
	volume = {23},
	issn = {1472-6947},
	url = {https://bmcmedinformdecismak.biomedcentral.com/articles/10.1186/s12911-023-02210-7},
	doi = {10.1186/s12911-023-02210-7},
	abstract = {Introduction‚ÄÇ Research into current robot middleware has revealed that most of them are either too complicated or outdated. These facts have motivated the development of a new middleware to meet the requirements of usability by non-experts. The proposed middleware is based on Android and is intended to be placed over existing robot SDKs and middleware. It runs on the android tablet of the Cruzr robot. Various toolings have been developed, such as a web component to control the robot via a webinterface, which facilitates its use.
Methods‚ÄÇ The middleware was developed using Android Java and runs on the Cruzr tablet as an app. It features a WebSocket server that interfaces with the robot and allows control via Python or other WebSocket-compatible languages. The speech interface utilizes Google Cloud Voice text-to-speech and speech-to-text services. The interface was implemented in Python, allowing for easy integration with existing robotics development workflows, and a web interface was developed for direct control of the robot via the web.
Results‚ÄÇ The new robot middleware was created and deployed on a Cruzr robot, relying on the WebSocket API and featuring a Python implementation. It supports various robot functions, such as text-to-speech, speech-to-text, navigation, displaying content and scanning bar codes. The system‚Äôs architecture allows for porting the interface to other robots and platforms, showcasing its adaptability. It has been demonstrated that the middleware can be run on a Pepper robot, although not all functions have been implemented yet. The middleware was utilized to implement healthcare use cases and received good feedback.
Conclusion‚ÄÇ Cloud and local speech services were discussed in regard to the middleware‚Äôs needs, to run without having to change any code on other robots. An outlook on how the programming interface can further be simplified by using natural text to code generators has been/is given. For other researchers using the aforementioned platforms (Cruzr, Pepper), the new middleware can be utilized for testing human-robot interaction. It can be used in a teaching setting, as well as be adapted to other robots using the same interface and philosophy regarding simple methods.},
	language = {en},
	number = {1},
	urldate = {2024-09-11},
	journal = {BMC Medical Informatics and Decision Making},
	author = {Glauser, Robin and Holm, J√ºrgen and Bender, Matthias and B√ºrkle, Thomas},
	month = jul,
	year = {2023},
	keywords = {\#EX: ‰∏çÂåÖÂê´HRI, \#EX:‰∏çÂåÖÂê´Â§ßÊ®°Âûã, \#EX:Â∑•‰∏öÊú∫Âô®‰∫∫, \#‰∏çÁ°ÆÂÆö, \#Áü≠Êñá, \#ÁªºËø∞, Design, Users, ccfInfo: CCF-None MIDM, citationNumber: 0},
	pages = {118},
}

@article{gorostiza_end-user_2011,
	title = {End-user programming of a social robot by dialog},
	volume = {59},
	issn = {0921-8890},
	url = {https://www.sciencedirect.com/science/article/pii/S092188901100131X},
	doi = {10.1016/j.robot.2011.07.009},
	abstract = {One of the main challenges faced by social robots is how to provide intuitive, natural and enjoyable usability for the end-user. In our ordinary environment, social robots could be important tools for education and entertainment (edutainment) in a variety of ways. This paper presents a Natural Programming System (NPS) that is geared to non-expert users. The main goal of such a system is to provide an enjoyable interactive platform for the users to build different programs within their social robot platform. The end-user can build a complex net of actions and conditions (a sequence) in a social robot via mixed-initiative dialogs and multimodal interaction. The system has been implemented and tested in Maggie, a real social robot with multiple skills, conceived as a general HRI researching platform. The robot‚Äôs internal features (skills) have been implemented to be verbally accessible to the end-user, who can combine them into others that are more complex following a bottom-up model. The built sequence is internally implemented as a Sequence Function Chart (SFC), which allows parallel execution, modularity and re-use. A multimodal Dialog Manager System (DMS) takes charge of keeping the coherence of the interaction. This work is thought for bringing social robots closer to non-expert users, who can play the game of ‚Äúteaching how to do things‚Äù with the robot.},
	number = {12},
	urldate = {2024-12-11},
	journal = {Robotics and Autonomous Systems},
	author = {Gorostiza, Javi F. and Salichs, Miguel A.},
	month = dec,
	year = {2011},
	keywords = {/unread, Dialog manager system, Human‚Äìrobot dialogs, Instruction-based learning, Natural programming, Petri nets, Semantic grammars, Sequence function charts, Social robotics},
	pages = {1102--1114},
}

@inproceedings{higger_toward_2023,
	title = {Toward {Open}-{World} {Human}-{Robot} {Interaction}: {What} {Types} of {Gestures} {Are} {Used} in {Task}-{Based} {Open}-{World} {Referential} {Communication}?},
	shorttitle = {Toward {Open}-{World} {Human}-{Robot} {Interaction}},
	url = {https://www.semdial.org/anthology/Z23-Higger_semdial_0015.pdf},
	urldate = {2025-02-03},
	booktitle = {Proceedings of the 27th {Workshop} on the {Semantics} and {Pragmatics} of {Dialogue}},
	author = {Higger, Mark and Rygina, Polina and Daigler, Logan and Bezerra, Lara Ferreira and Han, Zhao and Williams, Tom},
	year = {2023},
}

@inproceedings{huang_gestures_2024,
	address = {Boulder CO USA},
	title = {({Gestures} {Vaguely}): {The} {Effects} of {Robots}' {Use} of {Abstract} {Pointing} {Gestures} in {Large}-{Scale} {Environments}},
	isbn = {979-8-4007-0322-5},
	shorttitle = {({Gestures} {Vaguely})},
	url = {https://dl.acm.org/doi/10.1145/3610977.3634924},
	doi = {10.1145/3610977.3634924},
	abstract = {As robots are deployed into large-scale human environments, they will need to engage in task-oriented dialogues about objects and locations beyond those that can currently be seen. In these contexts, speakers use a wide range of referring gestures beyond those used in the small-scale interaction contexts that HRI research typically investigates. In this work, we thus seek to understand how robots can better generate gestures to accompany their referring language in large-scale interaction contexts. In service of this goal, we present the results of two human-subject studies: (1) a humanhuman study exploring how human gestures change in large-scale interaction contexts, and to identify human-like gestures suitable to such contexts yet readily implemented on robot hardware; and (2) a human-robot study conducted in a tightly controlled Virtual Reality environment, to evaluate robots‚Äô use of those identified gestures. Our results show that robot use of Precise Deictic and Abstract Pointing gestures afford different types of benefits when used to refer to visible vs. non-visible referents, leading us to formulate three concrete design guidelines. These results highlight both the opportunities for robot use of more humanlike gestures in large-scale interaction contexts, as well as the need for future work exploring their use as part of multi-modal communication.},
	language = {en},
	urldate = {2024-09-11},
	booktitle = {Proceedings of the 2024 {ACM}/{IEEE} {International} {Conference} on {Human}-{Robot} {Interaction}},
	publisher = {ACM},
	author = {Huang, Annie and Ranucci, Alyson and Stogsdill, Adam and Clark, Grace and Schott, Keenan and Higger, Mark and Han, Zhao and Williams, Tom},
	month = mar,
	year = {2024},
	keywords = {\#EX: ‰∏çÂåÖÂê´HRI, \#EX:‰∏çÂåÖÂê´Â§ßÊ®°Âûã, \#EX:Â∑•‰∏öÊú∫Âô®‰∫∫, \#‰∏çÁ°ÆÂÆö, \#Áü≠Êñá, \#ÁªºËø∞, /unread, Design, Users, ccfInfo: CCF-None HRI, citationNumber: 0},
	pages = {293--302},
}

@inproceedings{huang_vipo_2020,
	address = {Honolulu HI USA},
	title = {Vipo: {Spatial}-{Visual} {Programming} with {Functions} for {Robot}-{IoT} {Workflows}},
	isbn = {978-1-4503-6708-0},
	shorttitle = {Vipo},
	url = {https://dl.acm.org/doi/10.1145/3313831.3376670},
	doi = {10.1145/3313831.3376670},
	language = {en},
	urldate = {2024-08-14},
	booktitle = {Proceedings of the 2020 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Huang, Gaoping and Rao, Pawan S. and Wu, Meng-Han and Qian, Xun and Nof, Shimon Y. and Ramani, Karthik and Quinn, Alexander J.},
	month = apr,
	year = {2020},
	keywords = {\#EX: ‰∏çÂåÖÂê´HRI, \#EX:‰∏çÂåÖÂê´Â§ßÊ®°Âûã, \#EX:Â∑•‰∏öÊú∫Âô®‰∫∫, \#‰∏çÁ°ÆÂÆö, \#Áü≠Êñá, \#ÁªºËø∞, /unread, Design, Users, ccfInfo: CCF-A CHI, citationNumber: 27},
	pages = {1--13},
}

@article{ionescu_programming_2021,
	series = {8th {CIRP} {Conference} of {Assembly} {Technology} and {Systems}},
	title = {Programming cobots by voice: {A} human-centered, web-based approach},
	volume = {97},
	issn = {2212-8271},
	shorttitle = {Programming cobots by voice},
	url = {https://www.sciencedirect.com/science/article/pii/S2212827120314347},
	doi = {10.1016/j.procir.2020.05.213},
	abstract = {We present a new voice-based programming approach and software framework for collaborative robots (cobots) based on the Web Speech API, which is now supported by most modern browsers. The framework targets human programmable interfaces (HPIs), which can be used by people with little or no programming experience. The framework follows a meta-programming approach by enabling users to program cobots by voice in addition to using a mouse, tablet or keyboard. Upon a voice instruction (such as move, pick, release, etc.), the framework automates the manual tasks required to manipulate the vendor-provided HPI. The main advantages of this approach are simplified, guided programming, which only requires the knowledge of 5‚Äì10 voice instructions; increased programming speed by up to 46\% compared to the manual approach; and the possibility of sharing programs as videos. The open-source framework was evaluated using two application scenarios.},
	language = {en-US},
	urldate = {2024-05-16},
	journal = {Procedia CIRP},
	author = {Ionescu, Tudor B. and Schlund, Sebastian},
	month = jan,
	year = {2021},
	keywords = {Assembly, Cobots, Collaborative robots, GUI automation, Robot programming, Speech API, Speech-based programming, Voice-based programming, ccfInfo: Not Found, citationNumber: Not Found},
	pages = {123--129},
}

@misc{kargaran_menucraft_2023,
	title = {{MenuCraft}: {Interactive} {Menu} {System} {Design} with {Large} {Language} {Models}},
	shorttitle = {{MenuCraft}},
	url = {http://arxiv.org/abs/2303.04496},
	doi = {10.48550/arXiv.2303.04496},
	abstract = {Menu system design is a challenging task involving many design options and various human factors. For example, one crucial factor that designers need to consider is the semantic and systematic relation of menu commands. However, capturing these relations can be challenging due to limited available resources. With the advancement of neural language models, large language models can utilize their vast pre-existing knowledge in designing and refining menu systems. In this paper, we propose MenuCraft, an AI-assisted designer for menu design that enables collaboration between the designer and a dialogue system to design menus. MenuCraft offers an interactive language-based menu design tool that simplifies the menu design process and enables easy customization of design options. MenuCraft supports a variety of interactions through dialog that allows performing zero/few-shot learning.},
	language = {en-US},
	urldate = {2024-05-20},
	publisher = {arXiv},
	author = {Kargaran, Amir Hossein and Nikeghbal, Nafiseh and Heydarnoori, Abbas and Sch√ºtze, Hinrich},
	month = jul,
	year = {2023},
	note = {arXiv:2303.04496 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Human-Computer Interaction, ccfInfo: CCF-None CORR, citationNumber: 2},
}

@inproceedings{karli_alchemist_2024,
	address = {Boulder CO USA},
	title = {Alchemist: {LLM}-{Aided} {End}-{User} {Development} of {Robot} {Applications}},
	isbn = {979-8-4007-0322-5},
	shorttitle = {Alchemist},
	url = {https://dl.acm.org/doi/10.1145/3610977.3634969},
	doi = {10.1145/3610977.3634969},
	language = {en},
	urldate = {2024-11-26},
	booktitle = {Proceedings of the 2024 {ACM}/{IEEE} {International} {Conference} on {Human}-{Robot} {Interaction}},
	publisher = {ACM},
	author = {Karli, Ulas Berk and Chen, Juo-Tung and Antony, Victor Nikhil and Huang, Chien-Ming},
	month = mar,
	year = {2024},
	keywords = {/unread, ccfInfo: CCF-None HRI, citationNumber: 0, ü§ñ},
	pages = {361--370},
}

@article{lauria_mobile_2002,
	title = {Mobile robot programming using natural language},
	volume = {38},
	copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
	issn = {09218890},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0921889002001665},
	doi = {10.1016/S0921-8890(02)00166-5},
	language = {en},
	number = {3-4},
	urldate = {2024-09-04},
	journal = {Robotics and Autonomous Systems},
	author = {Lauria, Stanislao and Bugmann, Guido and Kyriacou, Theocharis and Klein, Ewan},
	month = mar,
	year = {2002},
	keywords = {\#EX: ‰∏çÂåÖÂê´HRI, \#EX:‰∏çÂåÖÂê´Â§ßÊ®°Âûã, \#EX:Â∑•‰∏öÊú∫Âô®‰∫∫, \#‰∏çÁ°ÆÂÆö, \#Áü≠Êñá, \#ÁªºËø∞, /unread, Design, Users, ccfInfo: CCF-None RAS, citationNumber: 187},
	pages = {171--181},
}

@incollection{lieberman_end-user_2006,
	address = {Dordrecht},
	series = {Human-{Computer} {Interaction} {Series}},
	title = {End-{User} {Development}: {An} {Emerging} {Paradigm}},
	isbn = {978-1-4020-5386-3},
	shorttitle = {End-{User} {Development}},
	url = {https://doi.org/10.1007/1-4020-5386-X_1},
	abstract = {We think that over the next few years, the goal of interactive systems and services will evolve from just making systems easy to use (even though that goal has not yet been completely achieved) to making systems that are easy to develop by end users. By now, most people have become familiar with the basic functionality and interfaces of computers, but they are not able to manage any programming language. Therefore, they cannot develop new applications or modify current ones according to their needs.},
	language = {en},
	urldate = {2023-09-25},
	booktitle = {End {User} {Development}},
	publisher = {Springer Netherlands},
	author = {Lieberman, Henry and Patern√≤, Fabio and Klann, Markus and Wulf, Volker},
	editor = {Lieberman, Henry and Patern√≤, Fabio and Wulf, Volker},
	year = {2006},
	doi = {10.1007/1-4020-5386-X_1},
	keywords = {/unread, Agile Software Development, Computer Support Cooperative Work, Human Centric Computing, Software Cost Estimation, Software Professional, ccfInfo: CCF-None HCI, citationNumber: 1202},
	pages = {1--8},
}

@inproceedings{liu_what_2023,
	address = {New York, NY, USA},
	series = {{CHI} '23},
	title = {‚Äú{What} {It} {Wants} {Me} {To} {Say}‚Äù: {Bridging} the {Abstraction} {Gap} {Between} {End}-{User} {Programmers} and {Code}-{Generating} {Large} {Language} {Models}},
	isbn = {978-1-4503-9421-5},
	shorttitle = {‚Äú{What} {It} {Wants} {Me} {To} {Say}‚Äù},
	url = {https://dl.acm.org/doi/10.1145/3544548.3580817},
	doi = {10.1145/3544548.3580817},
	abstract = {Code-generating large language models map natural language to code. However, only a small portion of the infinite space of naturalistic utterances is effective at guiding code generation. For non-expert end-user programmers, learning this is the challenge of abstraction matching. We examine this challenge in the specific context of data analysis in spreadsheets, in a system that maps the user‚Äôs natural language query to Python code using the Codex generator, executes the code, and shows the result. We propose grounded abstraction matching, which bridges the abstraction gap by translating the code back into a systematic and predictable naturalistic utterance. In a between-subjects, think-aloud study (n=24), we compare grounded abstraction matching to an ungrounded alternative based on previously established query framing principles. We find that the grounded approach improves end-users‚Äô understanding of the scope and capabilities of the code-generating model, and the kind of language needed to use it effectively.},
	language = {en-US},
	urldate = {2024-11-25},
	booktitle = {Proceedings of the 2023 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Liu, Michael Xieyang and Sarkar, Advait and Negreanu, Carina and Zorn, Benjamin and Williams, Jack and Toronto, Neil and Gordon, Andrew D.},
	month = apr,
	year = {2023},
	keywords = {/unread, ccfInfo: CCF-A CHI, citationNumber: 1},
	pages = {1--31},
}

@misc{lu_ui_2023,
	title = {{UI} {Layout} {Generation} with {LLMs} {Guided} by {UI} {Grammar}},
	url = {http://arxiv.org/abs/2310.15455},
	doi = {10.48550/arXiv.2310.15455},
	abstract = {The recent advances in Large Language Models (LLMs) have stimulated interest among researchers and industry professionals, particularly in their application to tasks concerning mobile user interfaces (UIs). This position paper investigates the use of LLMs for UI layout generation. Central to our exploration is the introduction of UI grammar -- a novel approach we proposed to represent the hierarchical structure inherent in UI screens. The aim of this approach is to guide the generative capacities of LLMs more effectively and improve the explainability and controllability of the process. Initial experiments conducted with GPT-4 showed the promising capability of LLMs to produce high-quality user interfaces via in-context learning. Furthermore, our preliminary comparative study suggested the potential of the grammar-based approach in improving the quality of generative results in specific aspects.},
	language = {en-US},
	urldate = {2024-05-20},
	publisher = {arXiv},
	author = {Lu, Yuwen and Tong, Ziang and Zhao, Qinyi and Zhang, Chengzhi and Li, Toby Jia-Jun},
	month = oct,
	year = {2023},
	note = {arXiv:2310.15455 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Human-Computer Interaction, ccfInfo: CCF-None CORR, citationNumber: 0},
}

@inproceedings{maccio_mixed_2022,
	address = {Philadelphia, PA, USA},
	title = {Mixed {Reality} as {Communication} {Medium} for {Human}-{Robot} {Collaboration}},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {978-1-7281-9681-7},
	url = {https://ieeexplore.ieee.org/document/9812233/},
	doi = {10.1109/ICRA46639.2022.9812233},
	language = {en-US},
	urldate = {2024-09-06},
	booktitle = {2022 {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	publisher = {IEEE},
	author = {Maccio, Simone and Carfi, Alessandro and Mastrogiovanni, Fulvio},
	month = may,
	year = {2022},
	keywords = {\#EX: ‰∏çÂåÖÂê´HRI, \#EX:‰∏çÂåÖÂê´Â§ßÊ®°Âûã, \#EX:Â∑•‰∏öÊú∫Âô®‰∫∫, \#‰∏çÁ°ÆÂÆö, \#Áü≠Êñá, \#ÁªºËø∞, /unread, Design, Users, ccfInfo: CCF-B ICRA, citationNumber: 6},
	pages = {2796--2802},
}

@inproceedings{mahadevan_generative_2024,
	title = {Generative {Expressive} {Robot} {Behaviors} using {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2401.14673},
	doi = {10.1145/3610977.3634999},
	abstract = {People employ expressive behaviors to effectively communicate and coordinate their actions with others, such as nodding to acknowledge a person glancing at them or saying "excuse me" to pass people in a busy corridor. We would like robots to also demonstrate expressive behaviors in human-robot interaction. Prior work proposes rule-based methods that struggle to scale to new communication modalities or social situations, while data-driven methods require specialized datasets for each social situation the robot is used in. We propose to leverage the rich social context available from large language models (LLMs) and their ability to generate motion based on instructions or user preferences, to generate expressive robot motion that is adaptable and composable, building upon each other. Our approach utilizes few-shot chain-of-thought prompting to translate human language instructions into parametrized control code using the robot's available and learned skills. Through user studies and simulation experiments, we demonstrate that our approach produces behaviors that users found to be competent and easy to understand. Supplementary material can be found at https://generative-expressive-motion.github.io/.},
	language = {en-US},
	urldate = {2024-03-18},
	booktitle = {Proceedings of the 2024 {ACM}/{IEEE} {International} {Conference} on {Human}-{Robot} {Interaction}},
	author = {Mahadevan, Karthik and Chien, Jonathan and Brown, Noah and Xu, Zhuo and Parada, Carolina and Xia, Fei and Zeng, Andy and Takayama, Leila and Sadigh, Dorsa},
	month = mar,
	year = {2024},
	keywords = {\#EX: ‰∏çÂåÖÂê´HRI, \#EX:‰∏çÂåÖÂê´Â§ßÊ®°Âûã, \#EX:Â∑•‰∏öÊú∫Âô®‰∫∫, \#‰∏çÁ°ÆÂÆö, \#Áü≠Êñá, \#ÁªºËø∞, Computer Science - Robotics, Design, ReadList, Users, ccfInfo: CCF-None HRI, citationNumber: 20, ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê},
	pages = {482--491},
}

@incollection{matuszek_learning_2013,
	address = {Heidelberg},
	title = {Learning to {Parse} {Natural} {Language} {Commands} to a {Robot} {Control} {System}},
	isbn = {978-3-319-00065-7},
	url = {https://doi.org/10.1007/978-3-319-00065-7_28},
	abstract = {As robots become more ubiquitous and capable of performing complex tasks, the importance of enabling untrained users to interact with them has increased. In response, unconstrained natural-language interaction with robots has emerged as a significant research area. We discuss the problem of parsing natural language commands to actions and control structures that can be readily implemented in a robot execution system. Our approach learns a parser based on example pairs of English commands and corresponding control language expressions. We evaluate this approach in the context of following route instructions through an indoor environment, and demonstrate that our system can learn to translate English commands into sequences of desired actions, while correctly capturing the semantic intent of statements involving complex control structures. The procedural nature of our formal representation allows a robot to interpret route instructions online while moving through a previously unknown environment.},
	language = {en},
	urldate = {2024-09-13},
	booktitle = {Experimental {Robotics}: {The} 13th {International} {Symposium} on {Experimental} {Robotics}},
	publisher = {Springer International Publishing},
	author = {Matuszek, Cynthia and Herbst, Evan and Zettlemoyer, Luke and Fox, Dieter},
	editor = {Desai, Jaydev P. and Dudek, Gregory and Khatib, Oussama and Kumar, Vijay},
	year = {2013},
	doi = {10.1007/978-3-319-00065-7_28},
	keywords = {\#EX: ‰∏çÂåÖÂê´HRI, \#EX:‰∏çÂåÖÂê´Â§ßÊ®°Âûã, \#EX:Â∑•‰∏öÊú∫Âô®‰∫∫, \#‰∏çÁ°ÆÂÆö, \#Áü≠Êñá, \#ÁªºËø∞, /unread, Categorial Grammar, Design, Lexical Item, Natural Language, Robot Control, Statistical Machine Translation, Users, ccfInfo: CCF-None ISER, citationNumber: 434},
	pages = {403--415},
}

@inproceedings{okamoto_usability_2009,
	title = {Usability study of {VUI} consistent with {GUI} focusing on age-groups},
	url = {https://www.isca-archive.org/interspeech_2009/okamoto09b_interspeech.html},
	doi = {10.21437/Interspeech.2009-535},
	abstract = {We studied the usability of a Voice User Interface (VUI) that is consistent with a Graphical User Interface (GUI), and focused on its dependency with user age-groups. Usability tests were iteratively conducted on 245 Japanese subjects with age-groups from 20s to 60s using a prototype of an in-vehicle information application. Next we calculated and analyzed statistics of the usability tests. We discuss the differences in usability with respect to age-groups and how to handle them. We propose that it is necessary to make voice guidance straightforward and to devise a VUI consistent with a GUI (VGUI) in order to let users understand the system structure. Also we found that the default design of a VGUI should be as simple as possible so that elderly users, who may be slow to learn the new system structure, are able to easily learn it.},
	language = {en},
	urldate = {2024-09-11},
	booktitle = {Interspeech 2009},
	publisher = {ISCA},
	author = {Okamoto, Jun and Kato, Tomoyuki and Shozakai, Makoto},
	month = sep,
	year = {2009},
	keywords = {\#EX: ‰∏çÂåÖÂê´HRI, \#EX:‰∏çÂåÖÂê´Â§ßÊ®°Âûã, \#EX:Â∑•‰∏öÊú∫Âô®‰∫∫, \#‰∏çÁ°ÆÂÆö, \#Áü≠Êñá, \#ÁªºËø∞, /unread, Design, Users, ccfInfo: CCF-C INTERSPEECH, citationNumber: 24},
	pages = {1839--1842},
}

@inproceedings{peng_understanding_2020,
	address = {Honolulu HI USA},
	title = {Understanding {User} {Perceptions} of {Robot}'s {Delay}, {Voice} {Quality}-{Speed} {Trade}-off and {GUI} during {Conversation}},
	isbn = {978-1-4503-6819-3},
	url = {https://dl.acm.org/doi/10.1145/3334480.3382792},
	doi = {10.1145/3334480.3382792},
	abstract = {Conversational robots face the practical challenge of providing timely responses to ensure smooth interactions with users. Thus, those who design and implement robots will need to understand how different levels of delay in response may affect users‚Äô satisfaction with the conversation, how to balance the trade-off between a robot‚Äôs quality of voice and response time, and how to design strategies to mitigate possible negative effects of a long delay. Via an online video-prototype study on a service robot with 94 Chinese participants, we Ô¨Ånd that users could tolerate up to 4s delay but their satisfaction drops at the 8s delay during both information-retrieval conversations and chitchats. We gain an in-depth understanding of users‚Äô preference for the trade-off between the voice quality and the response speed, as well as their opinions on possible robot graphic user interface (GUI) design to alleviate negative user experience with response latency.},
	language = {en},
	urldate = {2024-09-11},
	booktitle = {Extended {Abstracts} of the 2020 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Peng, Zhenhui and Mo, Kaixiang and Zhu, Xiaogang and Chen, Junlin and Chen, Zhijun and Xu, Qian and Ma, Xiaojuan},
	month = apr,
	year = {2020},
	keywords = {\#EX: ‰∏çÂåÖÂê´HRI, \#EX:‰∏çÂåÖÂê´Â§ßÊ®°Âûã, \#EX:Â∑•‰∏öÊú∫Âô®‰∫∫, \#‰∏çÁ°ÆÂÆö, \#Áü≠Êñá, \#ÁªºËø∞, Design, Users, ccfInfo: CCF-A CHI, citationNumber: 9},
	pages = {1--8},
}

@article{porfirio_sketching_2023,
	title = {Sketching {Robot} {Programs} {On} the {Fly}},
	abstract = {Service robots for personal use in the home and the workplace require end-user development solutions for swiftly scripting robot tasks as the need arises. Many existing solutions preserve ease, efficiency, and convenience through simple programming interfaces or by restricting task complexity. Others facilitate meticulous task design but often do so at the expense of simplicity and efficiency. There is a need for robot programming solutions that reconcile the complexity of robotics with the on-the-fly goals of end-user development. In response to this need, we present a novel, multimodal, and on-the-fly development system, Tabula. Inspired by a formative design study with a prototype, Tabula leverages a combination of spoken language for specifying the core of a robot task and sketching for contextualizing the core. The result is that developers can script partial, sloppy versions of robot programs to be completed and refined by a program synthesizer. Lastly, we demonstrate our anticipated use cases of Tabula via a set of application scenarios.},
	language = {en},
	author = {Porfirio, David and Stegner, Laura and Cakmak, Maya and Saupp√©, Allison and Albarghouthi, Aws and Mutlu, Bilge},
	year = {2023},
	keywords = {ccfInfo: CCF-None HRI, citationNumber: 0, ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê},
}

@inproceedings{sonawani_sisco_2024,
	title = {{SiSCo}: {Signal} {Synthesis} for {Effective} {Human}-{Robot} {Communication} {Via} {Large} {Language} {Models}},
	shorttitle = {{SiSCo}},
	url = {https://ieeexplore.ieee.org/document/10802561},
	doi = {10.1109/IROS58592.2024.10802561},
	abstract = {Effective human-robot collaboration hinges on robust communication channels, with visual signaling playing a pivotal role due to its intuitive appeal. Yet, the creation of visually intuitive cues often demands extensive resources and specialized knowledge. The emergence of Large Language Models (LLMs) offers promising avenues for enhancing human-robot interactions and revolutionizing the way we generate context-aware visual cues. To this end, we introduce SiSCo‚Äìa novel framework that combines the computational power of LLMs with mixed-reality technologies to streamline the creation of visual cues for human-robot collaboration. Our results show that SiSCo improves the efficiency of communication in human-robot teaming tasks, reducing task completion time by approximately 73\% and increasing task success rates by 18\% compared to baseline natural language signals. Additionally, SiSCo reduces cognitive load for participants by 46\%, as measured by the NASA-TLX subscale, and receives above-average user ratings for on-the-fly signals generated for unseen objects. To encourage further development and broader community engagement, we provide full access to SiSCo‚Äôs implementation and related materials on our GitHub repository.1},
	urldate = {2025-02-03},
	booktitle = {2024 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems} ({IROS})},
	author = {Sonawani, Shubham and Weigend, Fabian and Amor, Heni Ben},
	month = oct,
	year = {2024},
	note = {ISSN: 2153-0866},
	keywords = {Fasteners, Human-robot interaction, Intelligent robots, Large language models, Natural languages, Particle measurements, Signal synthesis, Software development management, Virtual reality, Visualization},
	pages = {7107--7114},
}

@inproceedings{stenmark_natural_2013,
	address = {Seoul, Korea (South)},
	title = {Natural language programming of industrial robots},
	isbn = {978-1-4799-1173-8},
	url = {http://ieeexplore.ieee.org/document/6695630/},
	doi = {10.1109/ISR.2013.6695630},
	urldate = {2024-09-13},
	booktitle = {{IEEE} {ISR} 2013},
	publisher = {IEEE},
	author = {Stenmark, Maj and Nugues, Pierre},
	month = oct,
	year = {2013},
	keywords = {\#EX: ‰∏çÂåÖÂê´HRI, \#EX:‰∏çÂåÖÂê´Â§ßÊ®°Âûã, \#EX:Â∑•‰∏öÊú∫Âô®‰∫∫, \#‰∏çÁ°ÆÂÆö, \#Áü≠Êñá, \#ÁªºËø∞, /unread, Design, Users, ccfInfo: CCF-None ISR, citationNumber: 49},
	pages = {1--5},
}

@inproceedings{thomason_improving_2019,
	address = {Montreal, QC, Canada},
	title = {Improving {Grounded} {Natural} {Language} {Understanding} through {Human}-{Robot} {Dialog}},
	url = {https://doi.org/10.1109/ICRA.2019.8794287},
	doi = {10.1109/ICRA.2019.8794287},
	abstract = {Natural language understanding for robotics can require substantial domain- and platform-specific engineering. For example, for mobile robots to pick-and-place objects in an environment to satisfy human commands, we can specify the language humans use to issue such commands, and connect concept words like red can to physical object properties. One way to alleviate this engineering for a new domain is to enable robots in human environments to adapt dynamically\&amp;\#x2014;continually learning new language constructions and perceptual concepts. In this work, we present an end-to-end pipeline for translating natural language commands to discrete robot actions, and use clarification dialogs to jointly improve language parsing and concept grounding. We train and evaluate this agent in a virtual setting on Amazon Mechanical Turk, and we transfer the learned agent to a physical robot platform to demonstrate it in the real world.},
	urldate = {2024-12-10},
	booktitle = {2019 {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	publisher = {IEEE Press},
	author = {Thomason, Jesse and Padmakumar, Aishwarya and Sinapov, Jivko and Walker, Nick and Jiang, Yuqian and Yedidsion, Harel and Hart, Justin and Stone, Peter and Mooney, Raymond J.},
	month = may,
	year = {2019},
	keywords = {/unread, ccfInfo: CCF-B ICRA, citationNumber: 68},
	pages = {6934--6941},
}

@misc{wei_emergent_2022,
	title = {Emergent {Abilities} of {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2206.07682},
	doi = {10.48550/arXiv.2206.07682},
	abstract = {Scaling up language models has been shown to predictably improve performance and sample efficiency on a wide range of downstream tasks. This paper instead discusses an unpredictable phenomenon that we refer to as emergent abilities of large language models. We consider an ability to be emergent if it is not present in smaller models but is present in larger models. Thus, emergent abilities cannot be predicted simply by extrapolating the performance of smaller models. The existence of such emergence implies that additional scaling could further expand the range of capabilities of language models.},
	language = {en},
	urldate = {2023-02-13},
	publisher = {arXiv},
	author = {Wei, Jason and Tay, Yi and Bommasani, Rishi and Raffel, Colin and Zoph, Barret and Borgeaud, Sebastian and Yogatama, Dani and Bosma, Maarten and Zhou, Denny and Metzler, Donald and Chi, Ed H. and Hashimoto, Tatsunori and Vinyals, Oriol and Liang, Percy and Dean, Jeff and Fedus, William},
	month = oct,
	year = {2022},
	note = {arXiv:2206.07682 [cs]},
	keywords = {/unread, Computer Science - Computation and Language, ccfInfo: CCF-None TMLR, citationNumber: 2342},
}

@inproceedings{you_emi_2020,
	address = {Honolulu HI USA},
	title = {{EMI}: {An} {Expressive} {Mobile} {Interactive} {Robot}},
	isbn = {978-1-4503-6819-3},
	shorttitle = {{EMI}},
	url = {https://dl.acm.org/doi/10.1145/3334480.3382852},
	doi = {10.1145/3334480.3382852},
	abstract = {In this paper, we explore how the emotional behavior of a robot affects interactions with humans. We introduce the EMI platform ‚Äì an expressive, mobile and interactive robot ‚Äì consisting of a circular diff-drive robot base equipped with a rear-projected expressive face, and omni-directional microphone for voiceinteraction. We exhibited the EMI robot at a public event, in which attendees were given the option to interact with the robot and participate in a survey and observational study. The survey and observations focused on the effects of the robot‚Äôs expressiveness in interactions with users of different ages and cultural backgrounds. From the survey responses, video observations and informal interviews we highlight key design decisions in EMI that resulted in positive user reactions.},
	language = {en},
	urldate = {2024-09-11},
	booktitle = {Extended {Abstracts} of the 2020 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {You, Yuhui and Fogelson, Mitchell and Cheng, Kelvin and Stenger, Bjorn},
	month = apr,
	year = {2020},
	keywords = {\#EX: ‰∏çÂåÖÂê´HRI, \#EX:‰∏çÂåÖÂê´Â§ßÊ®°Âûã, \#EX:Â∑•‰∏öÊú∫Âô®‰∫∫, \#‰∏çÁ°ÆÂÆö, \#Áü≠Êñá, \#ÁªºËø∞, Design, Users, ccfInfo: CCF-A CHI, citationNumber: 2},
	pages = {1--8},
}

@inproceedings{you_ferret-ui_2024,
	address = {Berlin, Heidelberg},
	title = {Ferret-{UI}: {Grounded} {Mobile} {UI} {Understanding} with¬†{Multimodal} {LLMs}},
	isbn = {978-3-031-73038-2},
	shorttitle = {Ferret-{UI}},
	url = {https://doi.org/10.1007/978-3-031-73039-9_14},
	doi = {10.1007/978-3-031-73039-9_14},
	abstract = {Recent advancements in multimodal large language models (MLLMs) have been noteworthy, yet, these general-domain MLLMs often fall short in their ability to comprehend and interact effectively with user interface (UI) screens. In this paper, we present Ferret-UI, a new MLLM tailored for enhanced understanding of mobile UI screens, equipped with referring, grounding, and reasoning capabilities. Given that UI screens typically exhibit a more elongated aspect ratio and contain smaller objects of interest (e.g., icons, texts) than natural images, we incorporate ‚Äúany resolution‚Äù on top of Ferret to magnify details and leverage enhanced visual features. Specifically, each screen is divided into 2 sub-images based on the original aspect ratio and sub-images are encoded separately as additional features. We meticulously gather training samples from an extensive range of elementary UI tasks, such as icon recognition, find text, and widget listing. These samples are formatted for instruction-following with region annotations to facilitate precise referring and grounding. To augment the model‚Äôs reasoning ability, we further compile a dataset for advanced tasks, including detailed description, conversations, and function inference. After training on the curated datasets, Ferret-UI exhibits outstanding comprehension of UI screens and the capability to execute open-ended instructions. For model evaluation, we establish a comprehensive benchmark encompassing all the aforementioned tasks. Ferret-UI excels not only beyond most open-source UI MLLMs, but also surpasses GPT-4V on all the elementary UI tasks.},
	urldate = {2025-02-03},
	booktitle = {Computer {Vision} ‚Äì {ECCV} 2024: 18th {European} {Conference}, {Milan}, {Italy}, {September} 29‚Äì{October} 4, 2024, {Proceedings}, {Part} {LXIV}},
	publisher = {Springer-Verlag},
	author = {You, Keen and Zhang, Haotian and Schoop, Eldon and Weers, Floris and Swearngin, Amanda and Nichols, Jeffrey and Yang, Yinfei and Gan, Zhe},
	month = oct,
	year = {2024},
	pages = {240--255},
}

@inproceedings{zhang_patterns_2021,
	address = {Yokohama Japan},
	title = {Patterns for {Representing} {Knowledge} {Graphs} to {Communicate} {Situational} {Knowledge} of {Service} {Robots}},
	isbn = {978-1-4503-8096-6},
	url = {https://dl.acm.org/doi/10.1145/3411764.3445767},
	doi = {10.1145/3411764.3445767},
	language = {en},
	urldate = {2024-09-13},
	booktitle = {Proceedings of the 2021 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Zhang, Shengchen and Wang, Zixuan and Chen, Chaoran and Dai, Yi and Ye, Lyumanshan and Sun, Xiaohua},
	month = may,
	year = {2021},
	keywords = {\#EX: ‰∏çÂåÖÂê´HRI, \#EX:‰∏çÂåÖÂê´Â§ßÊ®°Âûã, \#EX:Â∑•‰∏öÊú∫Âô®‰∫∫, \#‰∏çÁ°ÆÂÆö, \#Áü≠Êñá, \#ÁªºËø∞, /unread, Design, Users, ccfInfo: CCF-A CHI, citationNumber: 6},
	pages = {1--12},
}

@misc{zhao_survey_2024,
	title = {A {Survey} of {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2303.18223},
	doi = {10.48550/arXiv.2303.18223},
	abstract = {Language is essentially a complex, intricate system of human expressions governed by grammatical rules. It poses a significant challenge to develop capable AI algorithms for comprehending and grasping a language. As a major approach, language modeling has been widely studied for language understanding and generation in the past two decades, evolving from statistical language models to neural language models. Recently, pre-trained language models (PLMs) have been proposed by pre-training Transformer models over large-scale corpora, showing strong capabilities in solving various NLP tasks. Since researchers have found that model scaling can lead to performance improvement, they further study the scaling effect by increasing the model size to an even larger size. Interestingly, when the parameter scale exceeds a certain level, these enlarged language models not only achieve a significant performance improvement but also show some special abilities that are not present in small-scale language models. To discriminate the difference in parameter scale, the research community has coined the term large language models (LLM) for the PLMs of significant size. Recently, the research on LLMs has been largely advanced by both academia and industry, and a remarkable progress is the launch of ChatGPT, which has attracted widespread attention from society. The technical evolution of LLMs has been making an important impact on the entire AI community, which would revolutionize the way how we develop and use AI algorithms. In this survey, we review the recent advances of LLMs by introducing the background, key findings, and mainstream techniques. In particular, we focus on four major aspects of LLMs, namely pre-training, adaptation tuning, utilization, and capacity evaluation. Besides, we also summarize the available resources for developing LLMs and discuss the remaining issues for future directions.},
	urldate = {2024-12-05},
	publisher = {arXiv},
	author = {Zhao, Wayne Xin and Zhou, Kun and Li, Junyi and Tang, Tianyi and Wang, Xiaolei and Hou, Yupeng and Min, Yingqian and Zhang, Beichen and Zhang, Junjie and Dong, Zican and Du, Yifan and Yang, Chen and Chen, Yushuo and Chen, Zhipeng and Jiang, Jinhao and Ren, Ruiyang and Li, Yifan and Tang, Xinyu and Liu, Zikang and Liu, Peiyu and Nie, Jian-Yun and Wen, Ji-Rong},
	month = oct,
	year = {2024},
	note = {arXiv:2303.18223 [cs]},
	keywords = {/unread, Computer Science - Artificial Intelligence, Computer Science - Computation and Language, ccfInfo: CCF-None CORR},
}

