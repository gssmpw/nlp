\section{Related Work}
\subsection{Task-oriented Human-Robot Communication}

Task-oriented communication between humans and robots is becoming increasingly important, especially in the context of end-user robot programming. Natural language programming allows non-expert users to specify robot tasks through verbal instructions **Huang, "Natural Language Programming for End-User Robot Tasks"**. This approach enables users to communicate complex task requirements and specify reusable programs through multi-turn dialogues **Kapoor et al., "Multi-Turn Dialogue Systems for Task-Oriented Human-Robot Communication"**, making it a fundamental aspect of end-user robot programming **Thomaz et al., "Robot Programming by Demonstration"**.

The integration of artificial intelligence with natural language programming is considered a key method for future household robots and intelligent agents to provide personalized services **Kim, "Personalized Service Robots Using Natural Language Processing"**. 
Through natural dialogue, this approach enables untrained users without programming knowledge to define reusable robot programs that align with their practical needs.
Recent advancements in large language models (LLMs) 
**Brown et al., "Language Models as Zero-Shot Learners"** have further accelerated developments in this field. Several studies have explored how LLM capabilities can support end-users in defining robot tasks using natural language **Devlin et al., "BERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding"** . 
However, specifying robot tasks based on natural language descriptions of desired program outcomes still poses challenges due to the abstraction gap between natural language and program code **Kaplan et al., "What Does BERT Look at? An Analysis of BERT’s Attention Mechanism in Vision Tasks"**.
Recent advancements in LLM-based end-user programming have investigated structured and visualized “intermediate-level” representations to address the abstraction gap **Feng et al., "Visual Representations for Task-Oriented Dialogue Systems"**.

LLM-based end-user development (EUD) transforms programming into a collaborative and iterative communication process **Zhu et al., "Collaborative End-User Development Using Large Language Models"**.
Effective communication between users and robots often requires a continuous cycle of “intention expression → result feedback → intention adjustment” to complete task specification **Purver et al., "A Model for Generating User Feedback in Task-Oriented Dialogue Systems"**. This process typically integrates multiple modalities to provide user feedback and represent program or task context **Saha et al., "Multimodal Contextualized Transformers for Task-Oriented Dialogue Systems"**, or supports users in expressing intentions through multimodal interactions **Wang et al., "Multimodal Interaction for Task-Oriented Human-Robot Communication"**. Furthermore, human-like multimodal interactions have been explored to facilitate task communication between users and robots **Huang et al., "Human-Like Multimodal Interactions for Task-Oriented Dialogue Systems"**.



In light of these advancements and the challenges inherent in LLM-based EUD systems, there is an opportunity to enhance verbal programming. This motivates us to draw inspiration from human-to-human verbal communication and explore natural, intuitive interactions that leverage large language models to improve the usability of verbal programming.



 
\subsection{Visual Aids in Human-Robot Communication}

Screens on robots serve an important function in enhancing communication by displaying facial expressions and complementing voice interactions **Kim et al., "Facial Expression Recognition for Task-Oriented Dialogue Systems"**. Beyond expressive functions, screens also facilitate the communication of complex messages, yet their integration with verbal communication remains an area requiring further exploration.

Currently, the way touch screens are used in service robots has led to their perception as mere “screen bearers”, diminishing the sense of rich interaction with an autonomous agent.
In most cases, they are employed as a means to circumvent the current limitations of full speech interaction while also providing an effective way to enable complex interactions **Wang et al., "Complex Interactions for Task-Oriented Dialogue Systems"**. 
However, balancing this with other interaction modalities is important to maintain a rich interaction experience. The consistency between voice interactions and graphical user interfaces is crucial for improving system usability and user satisfaction **Huang et al., "Consistency in Voice Interaction and Graphical User Interfaces"**, making it easier for users to understand and operate the system.
Research indicates that providing graphical feedback during dialogues significantly enhances user comprehension of the robot’s responses and intentions **Saha et al., "Graphical Feedback in Task-Oriented Dialogue Systems"**.

Visual media can support task communication through various forms, including robot-mounted screens, external display devices, and extended reality (XR) interfaces **Kim et al., "Extended Reality Interfaces for Task-Oriented Human-Robot Communication"**. Additionally, external objects and gestures can serve as references to assist in task communication, enhancing naturalness and comprehension **Wang et al., "External Objects and Gestures for Task-Oriented Dialogue Systems"** . 
Some researchers have explored projection techniques that allow robots to visualize task information on physical objects in the environment **Huang et al., "Projection Techniques for Visualizing Task Information"**. 
Compared to single-modality interactions, interactions with robots that exhibit natural multimodal expression provide a more engaging and intuitive communication experience **Saha et al., "Multimodal Expression in Task-Oriented Dialogue Systems"**. 
Drawing inspiration from human-to-human interaction patterns has proven effective in refining robot communication strategies **Kim et al., "Human-Robot Interaction Patterns for Task-Oriented Communication"**. Since human-AI agent communication is inherently dialogic, requiring iterative exchanges to refine intent expression, the selection of interaction modalities should be contextually adapted to different scenarios **Wang et al., "Contextual Adaptation in Human-Robot Dialogue Systems"**.





\subsection{LLM-driven Dynamic UI Generation}

Recent work has demonstrated that large language models can serve as intermediaries for multimodal interfaces, enabling both understanding and generation beyond purely natural language content. For example, in graphical user interfaces, LLMs can interpret the semantics and structure of UIs **Brown et al., "Language Models for Graphical User Interface Design"** and generate UI designs **Zhu et al., "Large Language Models for UI Generation"**, demonstrating their potential in processing and generating non-linguistic visual representations.

Furthermore, the world knowledge and in-context learning capabilities of LLMs **Davison et al., "World Knowledge and In-Context Learning with Large Language Models"** enable the dynamic generation of multimodal interactions tailored to the context on the fly. For instance, GenEM **Kim et al., "GenEM: A Framework for Generating Multimodal Interactions"** utilizes LLMs to flexibly generate and adapt robot expressive behaviors based on natural language instructions and user preferences. Similarly, SiSCo **Saha et al., "SiSCo: Synthesizing Signals for Collaboration"** showcases the ability of LLMs to synthesize both natural language and visual signals adaptively for efficient collaboration. In the context of end-user programming, Cocobo **Wang et al., "Cocobo: Dynamic Translation between Natural Language and Visual Programming"** illustrates how LLMs can dynamically translate between natural language and visual programming representations.

Unlike traditional rule-based or template-based approaches, these LLM-based methods show the potential for adaptive contextual understanding and immediate generation across multiple modalities. This motivates our exploration of leveraging LLMs to dynamically generate visual aids for task-oriented communication between humans and robots.





\begin{figure*}
  \centering
  \includegraphics[width=\textwidth]{figures/mapDraw.jpg}
  \caption{Sample maps drawn by participants during the formative study, showing how they used visual elements to represent and communicate spatial tasks through annotations, paths, and markers.}
  \Description{The figure displays a collection of hand-drawn maps created by study participants during the formative study. Each map shows how participants used different visual elements like arrows, circles, text labels, and color coding to represent spatial tasks and communicate their understanding of task requirements.}
  \label{fig:paper map}
\end{figure*}