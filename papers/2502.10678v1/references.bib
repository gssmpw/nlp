
@inproceedings{you_ferret-ui_2024,
	address = {Berlin, Heidelberg},
	title = {Ferret-{UI}: {Grounded} {Mobile} {UI} {Understanding} with¬†{Multimodal} {LLMs}},
	isbn = {978-3-031-73038-2},
	shorttitle = {Ferret-{UI}},
	url = {https://doi.org/10.1007/978-3-031-73039-9_14},
	doi = {10.1007/978-3-031-73039-9_14},
	abstract = {Recent advancements in multimodal large language models (MLLMs) have been noteworthy, yet, these general-domain MLLMs often fall short in their ability to comprehend and interact effectively with user interface (UI) screens. In this paper, we present Ferret-UI, a new MLLM tailored for enhanced understanding of mobile UI screens, equipped with referring, grounding, and reasoning capabilities. Given that UI screens typically exhibit a more elongated aspect ratio and contain smaller objects of interest (e.g., icons, texts) than natural images, we incorporate ‚Äúany resolution‚Äù on top of Ferret to magnify details and leverage enhanced visual features. Specifically, each screen is divided into 2 sub-images based on the original aspect ratio and sub-images are encoded separately as additional features. We meticulously gather training samples from an extensive range of elementary UI tasks, such as icon recognition, find text, and widget listing. These samples are formatted for instruction-following with region annotations to facilitate precise referring and grounding. To augment the model‚Äôs reasoning ability, we further compile a dataset for advanced tasks, including detailed description, conversations, and function inference. After training on the curated datasets, Ferret-UI exhibits outstanding comprehension of UI screens and the capability to execute open-ended instructions. For model evaluation, we establish a comprehensive benchmark encompassing all the aforementioned tasks. Ferret-UI excels not only beyond most open-source UI MLLMs, but also surpasses GPT-4V on all the elementary UI tasks.},
	urldate = {2025-02-03},
	booktitle = {Computer {Vision} ‚Äì {ECCV} 2024: 18th {European} {Conference}, {Milan}, {Italy}, {September} 29‚Äì{October} 4, 2024, {Proceedings}, {Part} {LXIV}},
	publisher = {Springer-Verlag},
	author = {You, Keen and Zhang, Haotian and Schoop, Eldon and Weers, Floris and Swearngin, Amanda and Nichols, Jeffrey and Yang, Yinfei and Gan, Zhe},
	month = oct,
	year = {2024},
	pages = {240--255},
}

@inproceedings{sonawani_sisco_2024,
	title = {{SiSCo}: {Signal} {Synthesis} for {Effective} {Human}-{Robot} {Communication} {Via} {Large} {Language} {Models}},
	shorttitle = {{SiSCo}},
	url = {https://ieeexplore.ieee.org/document/10802561},
	doi = {10.1109/IROS58592.2024.10802561},
	abstract = {Effective human-robot collaboration hinges on robust communication channels, with visual signaling playing a pivotal role due to its intuitive appeal. Yet, the creation of visually intuitive cues often demands extensive resources and specialized knowledge. The emergence of Large Language Models (LLMs) offers promising avenues for enhancing human-robot interactions and revolutionizing the way we generate context-aware visual cues. To this end, we introduce SiSCo‚Äìa novel framework that combines the computational power of LLMs with mixed-reality technologies to streamline the creation of visual cues for human-robot collaboration. Our results show that SiSCo improves the efficiency of communication in human-robot teaming tasks, reducing task completion time by approximately 73\% and increasing task success rates by 18\% compared to baseline natural language signals. Additionally, SiSCo reduces cognitive load for participants by 46\%, as measured by the NASA-TLX subscale, and receives above-average user ratings for on-the-fly signals generated for unseen objects. To encourage further development and broader community engagement, we provide full access to SiSCo‚Äôs implementation and related materials on our GitHub repository.1},
	urldate = {2025-02-03},
	booktitle = {2024 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems} ({IROS})},
	author = {Sonawani, Shubham and Weigend, Fabian and Amor, Heni Ben},
	month = oct,
	year = {2024},
	note = {ISSN: 2153-0866},
	keywords = {Fasteners, Human-robot interaction, Intelligent robots, Large language models, Natural languages, Particle measurements, Signal synthesis, Software development management, Virtual reality, Visualization},
	pages = {7107--7114},
}

@inproceedings{nakajima_combining_2024,
	title = {Combining {Ontological} {Knowledge} and {Large} {Language} {Model} for {User}-{Friendly} {Service} {Robots}},
	url = {https://ieeexplore.ieee.org/abstract/document/10802273/},
	urldate = {2025-02-03},
	booktitle = {2024 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems} ({IROS})},
	publisher = {IEEE},
	author = {Nakajima, Haru and Miura, Jun},
	year = {2024},
	pages = {4755--4762},
}

@inproceedings{fang_enabling_2024,
	title = {Enabling {Waypoint} {Generation} for {Collaborative} {Robots} using {LLMs} and {Mixed} {Reality}},
	url = {https://openreview.net/forum?id=89F2jYzASY},
	abstract = {Programming a robotic is a complex task, as it demands the user to have a good command of specific programming languages and awareness of the robot's physical constraints. We propose a framework that simplifies robot deployment by allowing direct communication using natural language. It uses large language models (LLM) for prompt processing, workspace understanding, and waypoint generation. It also employs Augmented Reality (AR) to provide visual feedback of the planned outcome. We showcase the effectiveness of our framework with a simple pick-and-place task, which we implement on a real robot. Moreover, we present an early concept of expressive robot behavior and skill generation that can be used to communicate with the user and learn new skills (e.g., object grasping)},
	language = {en},
	urldate = {2025-02-03},
	author = {Fang, Cathy Mengying and Zielinski, Krzysztof and Maes, Patricia and Paradiso, Joe and Blumberg, Bruce and Kj√¶rgaard, Mikkel Baun},
	month = apr,
	year = {2024},
}

@inproceedings{ge_cocobo_2024,
	title = {Cocobo: {Exploring} {Large} {Language} {Models} as the {Engine} for {End}-{User} {Robot} {Programming}},
	shorttitle = {Cocobo},
	url = {https://ieeexplore.ieee.org/document/10714576},
	doi = {10.1109/VL/HCC60511.2024.00020},
	abstract = {End-user development allows everyday users to tailor service robots or applications to their needs. One user-friendly approach is natural language programming. However, it encounters challenges such as an expansive user expression space and limited support for debugging and editing, which restrict its application in end-user programming. The emergence of large language models (LLMs) offers promising avenues for the translation and interpretation between human language instructions and the code executed by robots, but their application in end-user programming systems requires further study. We introduce Cocobo, a natural language programming system with interactive diagrams powered by LLMs. Cocobo employs LLMs to understand users‚Äô authoring intentions, generate and explain robot programs, and facilitate the conversion between executable code and flowchart representations. Our user study shows that Cocobo has a low learning curve, enabling even users with zero coding experience to customize robot programs successfully.},
	urldate = {2025-02-03},
	booktitle = {2024 {IEEE} {Symposium} on {Visual} {Languages} and {Human}-{Centric} {Computing} ({VL}/{HCC})},
	author = {Ge, Yate and Dai, Yi and Shan, Run and Li, Kechun and Hu, Yuanda and Sun, Xiaohua},
	month = sep,
	year = {2024},
	note = {ISSN: 1943-6106},
	keywords = {Codes, Computational modeling, Debugging, Encoding, End-User Development, Engines, Flowcharts, Large Language Model, Large language models, Robot Programming, Robot programming, Service robots, Visualization},
	pages = {89--95},
}

@article{ajaykumar_survey_2022,
	title = {A {Survey} on {End}-{User} {Robot} {Programming}},
	volume = {54},
	issn = {0360-0300, 1557-7341},
	url = {https://dl.acm.org/doi/10.1145/3466819},
	doi = {10.1145/3466819},
	abstract = {As robots interact with a broader range of end-users, end-user robot programming has helped democratize robot programming by empowering end-users who may not have experience in robot programming to customize robots to meet their individual contextual needs. This article surveys work on end-user robot programming, with a focus on end-user program specification. It describes the primary domains, programming phases, and design choices represented by the end-user robot programming literature. The survey concludes by highlighting open directions for further investigation to enhance and widen the reach of end-user robot programming systems.},
	language = {en},
	number = {8},
	urldate = {2023-03-02},
	journal = {ACM Computing Surveys},
	author = {Ajaykumar, Gopika and Steele, Maureen and Huang, Chien-Ming},
	month = nov,
	year = {2022},
	keywords = {ccfInfo: CCF-None CSUR, citationNumber: 26, ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê},
	pages = {1--36},
}

@article{ainsworth_functions_1999,
	title = {The functions of multiple representations},
	volume = {33},
	copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
	issn = {03601315},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0360131599000299},
	doi = {10.1016/S0360-1315(99)00029-9},
	language = {en},
	number = {2-3},
	urldate = {2024-07-08},
	journal = {Computers \& Education},
	author = {Ainsworth, Shaaron},
	month = sep,
	year = {1999},
	keywords = {/unread, ccfInfo: CCF-None CE},
	pages = {131--152},
}

@inproceedings{you_emi_2020,
	address = {Honolulu HI USA},
	title = {{EMI}: {An} {Expressive} {Mobile} {Interactive} {Robot}},
	isbn = {978-1-4503-6819-3},
	shorttitle = {{EMI}},
	url = {https://dl.acm.org/doi/10.1145/3334480.3382852},
	doi = {10.1145/3334480.3382852},
	abstract = {In this paper, we explore how the emotional behavior of a robot affects interactions with humans. We introduce the EMI platform ‚Äì an expressive, mobile and interactive robot ‚Äì consisting of a circular diff-drive robot base equipped with a rear-projected expressive face, and omni-directional microphone for voiceinteraction. We exhibited the EMI robot at a public event, in which attendees were given the option to interact with the robot and participate in a survey and observational study. The survey and observations focused on the effects of the robot‚Äôs expressiveness in interactions with users of different ages and cultural backgrounds. From the survey responses, video observations and informal interviews we highlight key design decisions in EMI that resulted in positive user reactions.},
	language = {en},
	urldate = {2024-09-11},
	booktitle = {Extended {Abstracts} of the 2020 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {You, Yuhui and Fogelson, Mitchell and Cheng, Kelvin and Stenger, Bjorn},
	month = apr,
	year = {2020},
	keywords = {\#EX: ‰∏çÂåÖÂê´HRI, \#EX:‰∏çÂåÖÂê´Â§ßÊ®°Âûã, \#EX:Â∑•‰∏öÊú∫Âô®‰∫∫, \#‰∏çÁ°ÆÂÆö, \#Áü≠Êñá, \#ÁªºËø∞, Design, Users, ccfInfo: CCF-A CHI, citationNumber: 2},
	pages = {1--8},
}

@misc{stegner_understanding_2024,
	title = {Understanding {On}-the-{Fly} {End}-{User} {Robot} {Programming}},
	url = {http://arxiv.org/abs/2406.00841},
	doi = {10.1145/3643834.3660721},
	abstract = {Novel end-user programming (EUP) tools enable on-the-fly (i.e., spontaneous, easy, and rapid) creation of interactions with robotic systems. These tools are expected to empower users in determining system behavior, although very little is understood about how end users perceive, experience, and use these systems. In this paper, we seek to address this gap by investigating end-user experience with on-the-fly robot EUP. We trained 21 end users to use an existing on-the-fly EUP tool, asked them to create robot interactions for four scenarios, and assessed their overall experience. Our findings provide insight into how these systems should be designed to better support end-user experience with on-the-fly EUP, focusing on user interaction with an automatic program synthesizer that resolves imprecise user input, the use of multimodal inputs to express user intent, and the general process of programming a robot.},
	language = {en},
	urldate = {2024-06-07},
	author = {Stegner, Laura and Hwang, Yuna and Porfirio, David and Mutlu, Bilge},
	month = jun,
	year = {2024},
	keywords = {\#EX: ‰∏çÂåÖÂê´HRI, \#EX:‰∏çÂåÖÂê´Â§ßÊ®°Âûã, \#EX:Â∑•‰∏öÊú∫Âô®‰∫∫, \#‰∏çÁ°ÆÂÆö, \#Áü≠Êñá, \#ÁªºËø∞, Computer Science - Robotics, Design, ReadList, Users, ccfInfo: CCF-C DIS, citationNumber: 0, ‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è, üö©},
}

@inproceedings{robbins_visual_1996,
	title = {Visual language features supporting human-human and human-computer communication},
	url = {https://ieeexplore.ieee.org/document/545294/?arnumber=545294},
	doi = {10.1109/VL.1996.545294},
	abstract = {Fundamental to the design of visual languages are the goals of facilitating communication between people and computers, and between people and other people. The Object Block Programming Environment (OBPE) is a visual design, programming, and simulation tool which emphasizes support for both human-human and human-computer communication. OBPE provides several features to support effective communication: (1) multiple, coordinated views and aspects, (2) customizable graphics, (3) the "machines with push-buttons" metaphor and (4) the host-transient pattern. OBPE uses a diagram-based, visual object-oriented language that is intended for quickly designing and programming visual simulations of factories.},
	language = {en-US},
	urldate = {2024-09-11},
	booktitle = {Proceedings 1996 {IEEE} {Symposium} on {Visual} {Languages}},
	author = {Robbins, J.E. and Morley, D.J. and Redmiles, D.F. and Filatov, V. and Kononov, D.},
	month = sep,
	year = {1996},
	keywords = {\#EX: ‰∏çÂåÖÂê´HRI, \#EX:‰∏çÂåÖÂê´Â§ßÊ®°Âûã, \#EX:Â∑•‰∏öÊú∫Âô®‰∫∫, \#‰∏çÁ°ÆÂÆö, \#Áü≠Êñá, \#ÁªºËø∞, /unread, Cognitive science, Computational modeling, Computer graphics, Computer simulation, Data visualization, Design, Humans, Manufacturing automation, Object oriented modeling, Production facilities, Programming environments, Users, ccfInfo: CCF-None VL, citationNumber: 12},
	pages = {247--254},
}

@article{matthews_temporal_2016,
	title = {Temporal cognition: {Connecting} subjective time to perception, attention, and memory.},
	volume = {142},
	copyright = {http://www.apa.org/pubs/journals/resources/open-access.aspx},
	issn = {1939-1455, 0033-2909},
	shorttitle = {Temporal cognition},
	url = {https://doi.apa.org/doi/10.1037/bul0000045},
	doi = {10.1037/bul0000045},
	abstract = {Time is a universal psychological dimension, but time perception has often been studied and discussed in relative isolation. Increasingly, researchers are searching for unifying principles and integrated models that link time perception to other domains. In this review, we survey the links between temporal cognition and other psychological processes. Specifically, we describe how subjective duration is affected by nontemporal stimulus properties (perception), the allocation of processing resources (attention), and past experience with the stimulus (memory). We show that many of these connections instantiate a ‚Äúprocessing principle,‚Äù according to which perceived time is positively related to perceptual vividity and the ease of extracting information from the stimulus. This empirical generalization generates testable predictions and provides a starting-point for integrated theoretical frameworks. By outlining some of the links between temporal cognition and other domains, and by providing a unifying principle for understanding these effects, we hope to encourage time-perception researchers to situate their work within broader theoretical frameworks, and that researchers from other fields will be inspired to apply their insights, techniques, and theorizing to improve our understanding of the representation and judgment of time.},
	language = {en},
	number = {8},
	urldate = {2024-12-10},
	journal = {Psychological Bulletin},
	author = {Matthews, William J. and Meck, Warren H.},
	month = aug,
	year = {2016},
	keywords = {/unread},
	pages = {865--907},
}

@inproceedings{mahadevan_generative_2024,
	title = {Generative {Expressive} {Robot} {Behaviors} using {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2401.14673},
	doi = {10.1145/3610977.3634999},
	abstract = {People employ expressive behaviors to effectively communicate and coordinate their actions with others, such as nodding to acknowledge a person glancing at them or saying "excuse me" to pass people in a busy corridor. We would like robots to also demonstrate expressive behaviors in human-robot interaction. Prior work proposes rule-based methods that struggle to scale to new communication modalities or social situations, while data-driven methods require specialized datasets for each social situation the robot is used in. We propose to leverage the rich social context available from large language models (LLMs) and their ability to generate motion based on instructions or user preferences, to generate expressive robot motion that is adaptable and composable, building upon each other. Our approach utilizes few-shot chain-of-thought prompting to translate human language instructions into parametrized control code using the robot's available and learned skills. Through user studies and simulation experiments, we demonstrate that our approach produces behaviors that users found to be competent and easy to understand. Supplementary material can be found at https://generative-expressive-motion.github.io/.},
	language = {en-US},
	urldate = {2024-03-18},
	booktitle = {Proceedings of the 2024 {ACM}/{IEEE} {International} {Conference} on {Human}-{Robot} {Interaction}},
	author = {Mahadevan, Karthik and Chien, Jonathan and Brown, Noah and Xu, Zhuo and Parada, Carolina and Xia, Fei and Zeng, Andy and Takayama, Leila and Sadigh, Dorsa},
	month = mar,
	year = {2024},
	keywords = {\#EX: ‰∏çÂåÖÂê´HRI, \#EX:‰∏çÂåÖÂê´Â§ßÊ®°Âûã, \#EX:Â∑•‰∏öÊú∫Âô®‰∫∫, \#‰∏çÁ°ÆÂÆö, \#Áü≠Êñá, \#ÁªºËø∞, Computer Science - Robotics, Design, ReadList, Users, ccfInfo: CCF-None HRI, citationNumber: 20, ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê},
	pages = {482--491},
}

@inproceedings{liu_visual_2023,
	address = {Hamburg Germany},
	title = {Visual {Captions}: {Augmenting} {Verbal} {Communication} with {On}-the-fly {Visuals}},
	isbn = {978-1-4503-9421-5},
	shorttitle = {Visual {Captions}},
	url = {https://dl.acm.org/doi/10.1145/3544548.3581566},
	doi = {10.1145/3544548.3581566},
	language = {en},
	urldate = {2024-11-27},
	booktitle = {Proceedings of the 2023 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Liu, Xingyu "Bruce" and Kirilyuk, Vladimir and Yuan, Xiuxiu and Olwal, Alex and Chi, Peggy and Chen, Xiang "Anthony" and Du, Ruofei},
	month = apr,
	year = {2023},
	keywords = {/unread, ccfInfo: CCF-A CHI, citationNumber: 2},
	pages = {1--20},
}

@inproceedings{karli_alchemist_2024,
	address = {Boulder CO USA},
	title = {Alchemist: {LLM}-{Aided} {End}-{User} {Development} of {Robot} {Applications}},
	isbn = {979-8-4007-0322-5},
	shorttitle = {Alchemist},
	url = {https://dl.acm.org/doi/10.1145/3610977.3634969},
	doi = {10.1145/3610977.3634969},
	language = {en},
	urldate = {2024-11-26},
	booktitle = {Proceedings of the 2024 {ACM}/{IEEE} {International} {Conference} on {Human}-{Robot} {Interaction}},
	publisher = {ACM},
	author = {Karli, Ulas Berk and Chen, Juo-Tung and Antony, Victor Nikhil and Huang, Chien-Ming},
	month = mar,
	year = {2024},
	keywords = {/unread, ccfInfo: CCF-None HRI, citationNumber: 0, ü§ñ},
	pages = {361--370},
}

@article{gorostiza_end-user_2011,
	title = {End-user programming of a social robot by dialog},
	volume = {59},
	issn = {0921-8890},
	url = {https://www.sciencedirect.com/science/article/pii/S092188901100131X},
	doi = {10.1016/j.robot.2011.07.009},
	abstract = {One of the main challenges faced by social robots is how to provide intuitive, natural and enjoyable usability for the end-user. In our ordinary environment, social robots could be important tools for education and entertainment (edutainment) in a variety of ways. This paper presents a Natural Programming System (NPS) that is geared to non-expert users. The main goal of such a system is to provide an enjoyable interactive platform for the users to build different programs within their social robot platform. The end-user can build a complex net of actions and conditions (a sequence) in a social robot via mixed-initiative dialogs and multimodal interaction. The system has been implemented and tested in Maggie, a real social robot with multiple skills, conceived as a general HRI researching platform. The robot‚Äôs internal features (skills) have been implemented to be verbally accessible to the end-user, who can combine them into others that are more complex following a bottom-up model. The built sequence is internally implemented as a Sequence Function Chart (SFC), which allows parallel execution, modularity and re-use. A multimodal Dialog Manager System (DMS) takes charge of keeping the coherence of the interaction. This work is thought for bringing social robots closer to non-expert users, who can play the game of ‚Äúteaching how to do things‚Äù with the robot.},
	number = {12},
	urldate = {2024-12-11},
	journal = {Robotics and Autonomous Systems},
	author = {Gorostiza, Javi F. and Salichs, Miguel A.},
	month = dec,
	year = {2011},
	keywords = {/unread, Dialog manager system, Human‚Äìrobot dialogs, Instruction-based learning, Natural programming, Petri nets, Semantic grammars, Sequence function charts, Social robotics},
	pages = {1102--1114},
}

@inproceedings{fischer_adaptive_2023,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Adaptive and {Adaptable} {Systems}: {Differentiating} and {Integrating} {AI} and {EUD}},
	isbn = {978-3-031-34433-6},
	shorttitle = {Adaptive and {Adaptable} {Systems}},
	doi = {10.1007/978-3-031-34433-6_1},
	abstract = {The framework presented in the paper identifies the promises and pitfalls of Artificial Intelligence (AI) and End-User Development (EUD) approaches by focusing on two basic system components: (1) adaptive systems (grounded in AI) that change their behavior automatically driven by context-aware mechanisms including models of their users and specific task contexts, and (2) adaptable systems (grounded in EUD) that can be adjusted, modified, and extended by their users in order to capture unforeseen and important emergent user needs and aspects of problems. Grounded in an analysis of design trade-offs between the two approaches, arguments, and examples for creating a desirable symbiosis between adaptive and adaptable systems are described and design guidelines for future socio-technical environments are explored contributing to the development of theoretical concepts for the future of EUD.},
	language = {en},
	booktitle = {End-{User} {Development}},
	publisher = {Springer Nature Switzerland},
	author = {Fischer, Gerhard},
	editor = {Spano, Lucio Davide and Schmidt, Albrecht and Santoro, Carmen and Stumpf, Simone},
	year = {2023},
	keywords = {Auto-Correct, ChatGPT, Personalization, adaptable systems meta-design, adaptive systems, artificial intelligence, ccfInfo: CCF-None ISEUD, citationNumber: 0, context-aware interactions, creativity, design guidelines, design trade-offs, end-user development, user and task modeling, ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê},
	pages = {3--18},
}

@article{chen_teaching_2020,
	title = {Teaching and learning with children: {Impact} of reciprocal peer learning with a social robot on children‚Äôs learning and emotive engagement},
	volume = {150},
	issn = {0360-1315},
	shorttitle = {Teaching and learning with children},
	url = {https://www.sciencedirect.com/science/article/pii/S0360131520300373},
	doi = {10.1016/j.compedu.2020.103836},
	abstract = {Pedagogical agents are typically designed to take on a single role: either as a tutor who guides and instructs the student, or as a tutee that learns from the student to reinforce what he/she knows. While both agent-role paradigms have been shown to promote student learning, we hypothesize that there will be heightened benefit with respect to students‚Äô learning and emotional engagement if the agent engages children in a more peer-like way ‚Äî adaptively switching between tutor/tutee roles. In this work, we present a novel active role-switching (ARS) policy trained using reinforcement learning, in which the agent is rewarded for adapting its tutor or tutee behavior to the child‚Äôs knowledge mastery level. To investigate how the three different child‚Äìagent interaction paradigms (tutee, tutor, and peer agents) impact children‚Äôs learning and affective engagement, we designed a randomized controlled between-subject experiment. Fifty-nine children aged 5‚Äì7 years old from a local public school participated in a collaborative word-learning activity with one of the three agent-role paradigms. Our analysis revealed that children‚Äôs vocabulary acquisition benefited from the robot tutor‚Äôs instruction and knowledge demonstration, whereas children exhibited slightly greater affect on their faces when the robot behaves as a tutee of the child. This synergistic effect between tutor and tutee roles suggests why our adaptive peer-like agent brought the most benefit to children‚Äôs vocabulary learning and affective engagement, as compared to an agent that interacts only as a tutor or tutee for the child. This work sheds light on how fixed role (tutor/tutee) and adaptive role (peer) agents support children‚Äôs cognitive and emotional needs as they play and learn. It also contributes to an important new dimension of designing educational agents ‚Äî actively adapting roles based on the student‚Äôs engagement and learning needs.},
	urldate = {2024-12-11},
	journal = {Computers \& Education},
	author = {Chen, Huili and Park, Hae Won and Breazeal, Cynthia},
	month = jun,
	year = {2020},
	keywords = {/unread, Cooperative/collaborative learning, Evaluation of CAL systems, Intelligent tutoring systems, Interactive learning environments},
	pages = {103836},
}

@inproceedings{higger_toward_2023,
	title = {Toward {Open}-{World} {Human}-{Robot} {Interaction}: {What} {Types} of {Gestures} {Are} {Used} in {Task}-{Based} {Open}-{World} {Referential} {Communication}?},
	shorttitle = {Toward {Open}-{World} {Human}-{Robot} {Interaction}},
	url = {https://www.semdial.org/anthology/Z23-Higger_semdial_0015.pdf},
	urldate = {2025-02-03},
	booktitle = {Proceedings of the 27th {Workshop} on the {Semantics} and {Pragmatics} of {Dialogue}},
	author = {Higger, Mark and Rygina, Polina and Daigler, Logan and Bezerra, Lara Ferreira and Han, Zhao and Williams, Tom},
	year = {2023},
}

@misc{glassman_designing_2023,
	title = {Designing {Interfaces} for {Human}-{Computer} {Communication}: {An} {On}-{Going} {Collection} of {Considerations}},
	shorttitle = {Designing {Interfaces} for {Human}-{Computer} {Communication}},
	url = {http://arxiv.org/abs/2309.02257},
	doi = {10.48550/arXiv.2309.02257},
	abstract = {While we do not always use words, communicating what we want to an AI is a conversation -- with ourselves as well as with it, a recurring loop with optional steps depending on the complexity of the situation and our request. Any given conversation of this type may include: (a) the human forming an intent, (b) the human expressing that intent as a command or utterance, (c) the AI performing one or more rounds of inference on that command to resolve ambiguities and/or requesting clarifications from the human, (d) the AI showing the inferred meaning of the command and/or its execution on current and future situations or data, (e) the human hopefully correctly recognizing whether the AI's interpretation actually aligns with their intent. In the process, they may (f) update their model of the AI's capabilities and characteristics, (g) update their model of the situations in which the AI is executing its interpretation of their intent, (h) confirm or refine their intent, and (i) revise their expression of their intent to the AI, where the loop repeats until the human is satisfied. With these critical cognitive and computational steps within this back-and-forth laid out as a framework, it is easier to anticipate where communication can fail, and design algorithms and interfaces that ameliorate those failure points.},
	urldate = {2025-02-03},
	publisher = {arXiv},
	author = {Glassman, Elena L.},
	month = sep,
	year = {2023},
	note = {arXiv:2309.02257 [cs]},
	keywords = {Computer Science - Human-Computer Interaction},
}

@inproceedings{dong_research_2023,
	address = {Shanghai, China},
	title = {Research {Progress} and {Review} on {Service} {Interaction} between {Intelligent} {Service} {Robots} and {Customers}},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {979-8-3503-0468-8},
	url = {https://ieeexplore.ieee.org/document/10429811/},
	doi = {10.1109/ICoSR59980.2023.00025},
	urldate = {2025-02-03},
	booktitle = {2023 {International} {Conference} on {Service} {Robotics} ({ICoSR})},
	publisher = {IEEE},
	author = {Dong, Huiyuan},
	month = jul,
	year = {2023},
	pages = {1--8},
}

@book{boyatzis_transforming_1998,
	address = {Thousand Oaks, CA, US},
	series = {Transforming qualitative information:  {Thematic} analysis and code development},
	title = {Transforming qualitative information:  {Thematic} analysis and code development},
	isbn = {978-0-7619-0960-6 978-0-7619-0961-3},
	shorttitle = {Transforming qualitative information},
	abstract = {Discusses thematic analysis, a process for encoding qualitative information, which can be thought of as a bridge between the language of qualitative research and the language of quantitative research. The author helps researchers understand thematic analysis, a process that is a part of many qualitative methods, and provides clear guidelines about learning to develop techniques to apply it to one's own research. The book shows how to sense themes, which is the first step in analyzing information, as well as how to develop codes, through the use of numerous examples from myriad research settings. Research design issues that are essential to rigorous and high-quality use of qualitative information include sampling, scoring and scaling, and reliability. In so doing, this volume confronts the debate between positivist and postmodernist takes on the research act. The author argues for an ecumenical approach to doing research and as such his book will be of interest to researchers across a broad spectrum of disciplines and approaches. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
	publisher = {Sage Publications, Inc},
	author = {Boyatzis, Richard E.},
	year = {1998},
	note = {Pages: xvi, 184},
	keywords = {Analysis, Experimentation, Information},
}

@inproceedings{noauthor_service_2022,
	title = {Service {Robots} for {Fashion} {Retail} {Stores}: {Lessons} {Learned} from a {Case} {Study}},
	isbn = {978-989-53228-6-2},
	shorttitle = {Service {Robots} for {Fashion} {Retail} {Stores}},
	url = {https://eirai.org/images/proceedings_pdf/UH0122405.pdf},
	doi = {10.17758/EIRAI12.UH0122405},
	urldate = {2025-02-03},
	booktitle = {{AASEW}-22, {A3BES}-22, {EMSSH}-22 \& {PLSSE}-22 2022 {European} {International} {Conferences}},
	publisher = {Excellence in Research \& Innovation (EIRAI)},
	year = {2022},
}

@inproceedings{thomason_improving_2019,
	address = {Montreal, QC, Canada},
	title = {Improving {Grounded} {Natural} {Language} {Understanding} through {Human}-{Robot} {Dialog}},
	url = {https://doi.org/10.1109/ICRA.2019.8794287},
	doi = {10.1109/ICRA.2019.8794287},
	abstract = {Natural language understanding for robotics can require substantial domain- and platform-specific engineering. For example, for mobile robots to pick-and-place objects in an environment to satisfy human commands, we can specify the language humans use to issue such commands, and connect concept words like red can to physical object properties. One way to alleviate this engineering for a new domain is to enable robots in human environments to adapt dynamically\&amp;\#x2014;continually learning new language constructions and perceptual concepts. In this work, we present an end-to-end pipeline for translating natural language commands to discrete robot actions, and use clarification dialogs to jointly improve language parsing and concept grounding. We train and evaluate this agent in a virtual setting on Amazon Mechanical Turk, and we transfer the learned agent to a physical robot platform to demonstrate it in the real world.},
	urldate = {2024-12-10},
	booktitle = {2019 {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	publisher = {IEEE Press},
	author = {Thomason, Jesse and Padmakumar, Aishwarya and Sinapov, Jivko and Walker, Nick and Jiang, Yuqian and Yedidsion, Harel and Hart, Justin and Stone, Peter and Mooney, Raymond J.},
	month = may,
	year = {2019},
	keywords = {/unread, ccfInfo: CCF-B ICRA, citationNumber: 68},
	pages = {6934--6941},
}

@article{ali_conceptual_2024,
	title = {A conceptual framework for context-driven self-adaptive intelligent user interface based on {Android}},
	volume = {26},
	issn = {1435-5566},
	url = {https://doi.org/10.1007/s10111-023-00749-z},
	doi = {10.1007/s10111-023-00749-z},
	abstract = {Adaptive User Interface (AUI) can change its layout, appearance, and/or elements based on the needs of its user requirements and current usage context. The AUIs are used in state-of-the-art software products, applications for mobile devices, and websites. Moreover, AUI is an emerging research field in a mobile context, as it can enhance usability, performance, and user satisfaction. This study aims to propose a conceptual framework for developing a real-time self-adaptive user interface based on the Android Operating System (OS). Furthermore, the focus is on developing the core algorithms for the modules of the proposed framework. To evaluate the performance of the proposed framework, three case studies have been designed based on the daily and weekly activities of the user. Moreover, an expert-based validation approach is employed to obtain the expert‚Äôs feedback regarding the proposed framework. The result indicates that the proposed framework helps improve user satisfaction and experience by making an intelligent mobile device interface. The results of the framework‚Äôs evaluation and validation show the proposed framework‚Äôs feasibility and effectiveness. We conclude that the current work is beneficial in filling the identified research gap. Moreover, this research shows the significance of an adaptive interface in an Android OS-based context. In addition, it not only helps in¬†improving the¬†user interest and satisfaction but also enhances the overall performance of the mobile device.},
	language = {en},
	number = {1},
	urldate = {2024-12-11},
	journal = {Cognition, Technology \& Work},
	author = {Ali, Mughees and Khan, Saif Ur Rehman and Mashkoor, Atif and Taskeen, Anam},
	month = feb,
	year = {2024},
	keywords = {/unread, Adaptive user interface, Android-based adaptive user interface, Automotive Engineering, Interface design, Self-adaptive user interface, Smartphone user interface},
	pages = {83--106},
}

@article{noauthor_service_2022-1,
	title = {Service {Robots} for {Fashion} {Retail} {Stores}: {Lessons} {Learned} from a {Case} {Study}},
	shorttitle = {Service {Robots} for {Fashion} {Retail} {Stores}},
	url = {https://eirai.org/images/proceedings_pdf/UH0122405.pdf},
	doi = {10.17758/EIRAI12.UH0122405},
	abstract = {ÔÄ† Abstract - Service robots provide retailers with new opportunities to innovate their in-store service offerings. Despite advances made in the fields of human-robot interaction, information systems, and marketing, there is relatively little known about how to apply a service robot in retailing. In this paper we aim to shed light on this issue by exploring the added value, roles, and prototyping of a service robot in fashion retailing. Using two Dutch fashion stores as real-life settings, we apply different interaction techniques (observation, interview, survey, structured role play, prototyping) to generate first insights and obtain lessons learned. The results of our study suggest that fashion retailers would benefit most from using service robots for communication of promotions and provision of product information. When applying service robots to these use cases, customers seem to prefer briefly and clearly expressed information that is communicated in a style that matches (in) store communications. Still, the lack of personal attention and social support associated with a service robot makes retailers and store personnel rather reluctant to use them for their service excellence-oriented stores.},
	urldate = {2024-12-11},
	journal = {AASEW-22, A3BES-22, EMSSH-22 \& PLSSE-22 2022 European International Conferences},
	year = {2022},
	note = {Conference Name: 2022 European International Conferences
ISBN: 9789895322862
Publisher: Excellence in Research \& Innovation (EIRAI)
TLDR: The results of the study suggest that fashion retailers would benefit most from using service robots for communication of promotions and provision of product information, when applying service robots to these use cases.},
	keywords = {/unread},
}

@article{stefanidi_real-time_2022,
	title = {Real-{Time} {Adaptation} of {Context}-{Aware} {Intelligent} {User} {Interfaces}, for {Enhanced} {Situational} {Awareness}},
	volume = {10},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/9717260},
	doi = {10.1109/ACCESS.2022.3152743},
	abstract = {In this work, a novel computational approach for the dynamic adaptation of User Interfaces (UIs) is proposed, which aims at enhancing the Situational Awareness (SA) of users by leveraging the current context and providing the most useful information, in an optimal and efficient manner. By combining Ontology modeling and reasoning with Combinatorial Optimization, the system decides what information to present, when to present it, where to visualize it in the display - and how, taking into consideration contextual factors as well as placement constraints. The main objective of the proposed approach is to optimize the SA associated with the displayed UI at run-time, while avoiding information overload and induced stress. In the context of this work, we have deployed our computational approach to the use case of an Augmented Reality (AR) system for Law Enforcement Agents (LEAs). To explore the benefits and limitations of the developed system, two evaluations have been conducted. The first one was an expert-based evaluation with LEAs and User Experience (UX) experts, assessing the appropriateness of the system‚Äôs decisions. The second one was a user-based evaluation involving LEAs from different agencies, estimating the SA, the mental workload and the overall UX associated with the system, through an AR simulation. The results indicate that the system enhances SA, and while not imposing workload, it provides an overall positive UX.},
	urldate = {2024-12-11},
	journal = {IEEE Access},
	author = {Stefanidi, Zinovia and Margetis, George and Ntoa, Stavroula and Papagiannakis, George},
	year = {2022},
	note = {Conference Name: IEEE Access},
	keywords = {/unread, Adaptation models, Adaptive systems, Adaptive user interfaces, Cognition, Context modeling, Ontologies, Optimization, Real-time systems, augmented reality, context-awareness, intelligent user interfaces, ontology modeling, ontology reasoning, situational awareness, user interface optimization},
	pages = {23367--23393},
}

@article{sboui_ui-dspl_2018,
	title = {A {UI}-{DSPL} {Approach} for the {Development} of {Context}-{Adaptable} {User} {Interfaces}},
	volume = {6},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/8194842},
	doi = {10.1109/ACCESS.2017.2782880},
	abstract = {Unlike adaptive interfaces which use sensors to adapt themselves, adaptable interfaces need the intervention of end users to adapt their different aspects according to user requirements. These requirements are commonly expressed according to the context of use. This latter was defined by the triplet {\textless};platform, environment, user{\textgreater} where the platform refers to the physical device and the device software, the environment refers to the physical environment in which the application is used and the user element refers to the user preferences and user profile. In this paper, we define a dynamic software product line (DSPL) approach for the development of a family of context-adaptable user interfaces. The DSPL paradigm exploits the knowledge acquired in software product line engineering to develop systems that can be context-aware, or runtime adaptable. Our approach satisfies a set of contributions which will be validated by implementing and evaluating them according to an illustrative case study.},
	urldate = {2024-12-11},
	journal = {IEEE Access},
	author = {Sboui, Thouraya and Ben Ayed, Mounir and Alimi, Adel M.},
	year = {2018},
	note = {Conference Name: IEEE Access},
	keywords = {/unread, Adaptation models, Context modeling, Context-adaptation, DSPL approach, Data models, Frequency modulation, Runtime, Software product lines, UI adaptation, User interfaces},
	pages = {7066--7081},
}

@article{hussain_model-based_2018,
	title = {Model-based adaptive user interface based on context and user experience evaluation},
	volume = {12},
	issn = {1783-8738},
	url = {https://doi.org/10.1007/s12193-018-0258-2},
	doi = {10.1007/s12193-018-0258-2},
	abstract = {Personalized services have greater impact on user experience to effect the level of user satisfaction. Many approaches provide personalized services in the form of an adaptive user interface. The focus of these approaches is limited to specific domains rather than a generalized approach applicable to every domain. In this paper, we proposed a domain and device-independent model-based adaptive user interfacing methodology. Unlike state-of-the-art approaches, the proposed methodology is dependent on the evaluation of user context and user experience (UX). The proposed methodology is implemented as an adaptive UI/UX authoring (A-UI/UX-A) tool; a system capable of adapting user interface based on the utilization of contextual factors, such as user disabilities, environmental factors (e.g. light level, noise level, and location) and device use, at runtime using the adaptation rules devised for rendering the adapted interface. To validate effectiveness of the proposed A-UI/UX-A tool and methodology, user-centric and statistical evaluation methods are used. The results show that the proposed methodology outperforms the existing approaches in adapting user interfaces by utilizing the users context and experience.},
	language = {en},
	number = {1},
	urldate = {2024-12-11},
	journal = {Journal on Multimodal User Interfaces},
	author = {Hussain, Jamil and Ul Hassan, Anees and Muhammad Bilal, Hafiz Syed and Ali, Rahman and Afzal, Muhammad and Hussain, Shujaat and Bang, Jaehun and Banos, Oresti and Lee, Sungyoung},
	month = mar,
	year = {2018},
	keywords = {/unread, Adaptive user interface, Context-aware user interfaces, Human computer interaction, Model-based user interface, Personalized user interface, User experience},
	pages = {1--16},
}

@article{hussain_model-based_2018-1,
	title = {Model-based adaptive user interface based on context and user experience evaluation},
	volume = {12},
	issn = {1783-8738},
	url = {https://doi.org/10.1007/s12193-018-0258-2},
	doi = {10.1007/s12193-018-0258-2},
	abstract = {Personalized services have greater impact on user experience to effect the level of user satisfaction. Many approaches provide personalized services in the form of an adaptive user interface. The focus of these approaches is limited to specific domains rather than a generalized approach applicable to every domain. In this paper, we proposed a domain and device-independent model-based adaptive user interfacing methodology. Unlike state-of-the-art approaches, the proposed methodology is dependent on the evaluation of user context and user experience (UX). The proposed methodology is implemented as an adaptive UI/UX authoring (A-UI/UX-A) tool; a system capable of adapting user interface based on the utilization of contextual factors, such as user disabilities, environmental factors (e.g. light level, noise level, and location) and device use, at runtime using the adaptation rules devised for rendering the adapted interface. To validate effectiveness of the proposed A-UI/UX-A tool and methodology, user-centric and statistical evaluation methods are used. The results show that the proposed methodology outperforms the existing approaches in adapting user interfaces by utilizing the users context and experience.},
	language = {en},
	number = {1},
	urldate = {2024-12-11},
	journal = {Journal on Multimodal User Interfaces},
	author = {Hussain, Jamil and Ul Hassan, Anees and Muhammad Bilal, Hafiz Syed and Ali, Rahman and Afzal, Muhammad and Hussain, Shujaat and Bang, Jaehun and Banos, Oresti and Lee, Sungyoung},
	month = mar,
	year = {2018},
	keywords = {/unread, Adaptive user interface, Context-aware user interfaces, Human computer interaction, Model-based user interface, Personalized user interface, User experience},
	pages = {1--16},
}

@article{salter_learning_2006,
	series = {Intelligent {Autonomous} {Systems}},
	title = {Learning about natural human‚Äìrobot interaction styles},
	volume = {54},
	issn = {0921-8890},
	url = {https://www.sciencedirect.com/science/article/pii/S092188900500151X},
	doi = {10.1016/j.robot.2005.09.022},
	abstract = {If we are to achieve natural human‚Äìrobot interaction, we may need to complement current vision and speech interfaces. Touch may provide us with an extra tool in this quest. In this paper we demonstrate the role of touch in interaction between a robot and a human. We show how infrared sensors located on robots can be easily used to detect and distinguish human interaction, in this case interaction with individual children. This application of infrared sensors potentially has many uses; for example, in entertainment or service robotics. This system could also benefit therapy or rehabilitation, where the observation and recording of movement and interaction is important. In the long term, this technique might enable robots to adapt to individuals or individual types of user.},
	number = {2},
	urldate = {2024-12-11},
	journal = {Robotics and Autonomous Systems},
	author = {Salter, Tamie and Dautenhahn, Kerstin and Boekhorst, Ren√© te},
	month = feb,
	year = {2006},
	keywords = {/unread, Adaptation, Natural human‚Äìrobot interaction, Patterns of behaviour, Touch},
	pages = {127--134},
}

@inproceedings{walker_neural_2019,
	address = {Berlin, Heidelberg},
	title = {Neural {Semantic} {Parsing} with {Anonymization} for {Command} {Understanding} in {General}-{Purpose} {Service} {Robots}},
	isbn = {978-3-030-35698-9},
	url = {https://doi.org/10.1007/978-3-030-35699-6_26},
	doi = {10.1007/978-3-030-35699-6_26},
	abstract = {Service robots are envisioned to undertake a wide range of tasks at the request of users. Semantic parsing is one way to convert natural language commands given to these robots into executable representations. Methods for creating semantic parsers, however, rely either on large amounts of data or on engineered lexical features and parsing rules, which has limited their application in robotics. To address this challenge, we propose an approach that leverages neural semantic parsing methods in combination with contextual word embeddings to enable the training of a semantic parser with little data and without domain specific parser engineering. Key to our approach is the use of an anonymized target representation which is more easily learned by the parser. In most cases, this simplified representation can trivially be transformed into an executable format, and in others the parse can be completed through further interaction with the user. We evaluate this approach in the context of the RoboCup@Home General Purpose Service Robot task, where we have collected a corpus of paraphrased versions of commands from the standardized command generator. Our results show that neural semantic parsers can predict the logical form of unseen commands with 89\% accuracy. We release our data and the details of our models to encourage further development from the RoboCup and service robotics communities.},
	urldate = {2024-12-10},
	booktitle = {{RoboCup} 2019: {Robot} {World} {Cup} {XXIII}},
	publisher = {Springer-Verlag},
	author = {Walker, Nick and Peng, Yu-Tang and Cakmak, Maya},
	month = dec,
	year = {2019},
	keywords = {/unread},
	pages = {337--350},
}

@inproceedings{ross_programmers_2023,
	address = {New York, NY, USA},
	series = {{IUI} '23},
	title = {The {Programmer}‚Äôs {Assistant}: {Conversational} {Interaction} with a {Large} {Language} {Model} for {Software} {Development}},
	isbn = {979-8-4007-0106-1},
	shorttitle = {The {Programmer}‚Äôs {Assistant}},
	url = {https://dl.acm.org/doi/10.1145/3581641.3584037},
	doi = {10.1145/3581641.3584037},
	abstract = {Large language models (LLMs) have recently been applied in software engineering to perform tasks such as translating code between programming languages, generating code from natural language, and autocompleting code as it is being written. When used within development tools, these systems typically treat each model invocation independently from all previous invocations, and only a specific limited functionality is exposed within the user interface. This approach to user interaction misses an opportunity for users to more deeply engage with the model by having the context of their previous interactions, as well as the context of their code, inform the model‚Äôs responses. We developed a prototype system ‚Äì the Programmer‚Äôs Assistant ‚Äì in order to explore the utility of conversational interactions grounded in code, as well as software engineers‚Äô receptiveness to the idea of conversing with, rather than invoking, a code-fluent LLM. Through an evaluation with 42 participants with varied levels of programming experience, we found that our system was capable of conducting extended, multi-turn discussions, and that it enabled additional knowledge and capabilities beyond code generation to emerge from the LLM. Despite skeptical initial expectations for conversational programming assistance, participants were impressed by the breadth of the assistant‚Äôs capabilities, the quality of its responses, and its potential for improving their productivity. Our work demonstrates the unique potential of conversational interactions with LLMs for co-creative processes like software development.},
	language = {en-US},
	urldate = {2023-09-13},
	booktitle = {Proceedings of the 28th {International} {Conference} on {Intelligent} {User} {Interfaces}},
	publisher = {Association for Computing Machinery},
	author = {Ross, Steven I. and Martinez, Fernando and Houde, Stephanie and Muller, Michael and Weisz, Justin D.},
	month = mar,
	year = {2023},
	keywords = {/unread, code-fluent large language models, conversational interaction, foundation models, human-centered AI},
	pages = {491--514},
}

@article{salter_learning_2006-1,
	series = {Intelligent {Autonomous} {Systems}},
	title = {Learning about natural human‚Äìrobot interaction styles},
	volume = {54},
	issn = {0921-8890},
	url = {https://www.sciencedirect.com/science/article/pii/S092188900500151X},
	doi = {10.1016/j.robot.2005.09.022},
	abstract = {If we are to achieve natural human‚Äìrobot interaction, we may need to complement current vision and speech interfaces. Touch may provide us with an extra tool in this quest. In this paper we demonstrate the role of touch in interaction between a robot and a human. We show how infrared sensors located on robots can be easily used to detect and distinguish human interaction, in this case interaction with individual children. This application of infrared sensors potentially has many uses; for example, in entertainment or service robotics. This system could also benefit therapy or rehabilitation, where the observation and recording of movement and interaction is important. In the long term, this technique might enable robots to adapt to individuals or individual types of user.},
	number = {2},
	urldate = {2024-12-11},
	journal = {Robotics and Autonomous Systems},
	author = {Salter, Tamie and Dautenhahn, Kerstin and Boekhorst, Ren√© te},
	month = feb,
	year = {2006},
	keywords = {/unread, Adaptation, Natural human‚Äìrobot interaction, Patterns of behaviour, Touch},
	pages = {127--134},
}

@misc{jiang_survey_2024,
	title = {A {Survey} on {Large} {Language} {Models} for {Code} {Generation}},
	url = {http://arxiv.org/abs/2406.00515},
	doi = {10.48550/arXiv.2406.00515},
	abstract = {Large Language Models (LLMs) have garnered remarkable advancements across diverse code-related tasks, known as Code LLMs, particularly in code generation that generates source code with LLM from natural language descriptions. This burgeoning field has captured significant interest from both academic researchers and industry professionals due to its practical significance in software development, e.g., GitHub Copilot. Despite the active exploration of LLMs for a variety of code tasks, either from the perspective of natural language processing (NLP) or software engineering (SE) or both, there is a noticeable absence of a comprehensive and up-to-date literature review dedicated to LLM for code generation. In this survey, we aim to bridge this gap by providing a systematic literature review that serves as a valuable reference for researchers investigating the cutting-edge progress in LLMs for code generation. We introduce a taxonomy to categorize and discuss the recent developments in LLMs for code generation, covering aspects such as data curation, latest advances, performance evaluation, ethical implications, environmental impact, and real-world applications. In addition, we present a historical overview of the evolution of LLMs for code generation and offer an empirical comparison using the HumanEval, MBPP, and BigCodeBench benchmarks across various levels of difficulty and types of programming tasks to highlight the progressive enhancements in LLM capabilities for code generation. We identify critical challenges and promising opportunities regarding the gap between academia and practical development. Furthermore, we have established a dedicated resource GitHub page (https://github.com/juyongjiang/CodeLLMSurvey) to continuously document and disseminate the most recent advances in the field.},
	urldate = {2024-12-11},
	publisher = {arXiv},
	author = {Jiang, Juyong and Wang, Fan and Shen, Jiasi and Kim, Sungju and Kim, Sunghun},
	month = nov,
	year = {2024},
	note = {arXiv:2406.00515 [cs]},
	keywords = {/unread, Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Software Engineering},
}

@article{li_voice_2023,
	title = {Voice {Interaction} {Recognition} {Design} in {Real}-{Life} {Scenario} {Mobile} {Robot} {Applications}},
	volume = {13},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2076-3417},
	url = {https://www.mdpi.com/2076-3417/13/5/3359},
	doi = {10.3390/app13053359},
	abstract = {This paper designed a voice interactive robot system that can conveniently execute assigned service tasks in real-life scenarios. It is equipped without a microphone where users can control the robot with spoken commands; the voice commands are then recognized by a well-trained deep neural network model of automatic speech recognition (ASR), which enables the robot to execute and complete the command based on the navigation of a real-time simultaneous localization and mapping (SLAM) algorithm. The voice interaction recognition model is divided into two parts: (1) speaker separation and (2) ASR. The speaker separation is applied by a deep-learning system consisting of eight convolution layers, one LSTM layer, and two fully connected (FC) layers to separate the speaker‚Äôs voice. This model recognizes the speaker‚Äôs voice as a referrer that separates and holds the required voiceprint and removes noises from other people‚Äôs voiceprints. Its automatic speech recognition uses the novel sandwich-type conformer model with a stack of three layers, and combines convolution and self-attention to capture short-term and long-term interactions. Specifically, it contains a multi-head self-attention module to directly convert the voice data into text for command realization. The RGB-D vision-based camera uses a real-time appearance-based mapping algorithm to create the environment map and replace the localization with a visional odometer to allow the robot to navigate itself. Finally, the proposed ASR model was tested to check if the desired results will be obtained. Performance analysis was applied to determine the robot‚Äôs environment isolation and voice recognition abilities. The results showed that the practical robot system successfully completed the interactive service tasks in a real environment. This experiment demonstrates the outstanding performance with other ASR methods and voice control mobile robot systems. It also verified that the designed voice interaction recognition system enables the mobile robot to execute tasks in real-time, showing that it is a convenient way to complete the assigned service applications.},
	language = {en},
	number = {5},
	urldate = {2024-12-11},
	journal = {Applied Sciences},
	author = {Li, Shih-An and Liu, Yu-Ying and Chen, Yun-Chien and Feng, Hsuan-Ming and Shen, Pi-Kang and Wu, Yu-Che},
	month = jan,
	year = {2023},
	note = {Number: 5
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {/unread, automatic speech recognition, deep learning, mobile robot, simultaneous localization and mapping (slam), voice interaction},
	pages = {3359},
}

@inproceedings{xiao_application_2023,
	address = {Changsha, China},
	title = {Application of {Multimodal} {Intelligent} {Dialogue} {Robot} in {Diabetes} {Health} {Management} {Service} {Platform}},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {979-8-3503-2501-0},
	url = {https://ieeexplore.ieee.org/document/10314109/},
	doi = {10.1109/ICDSM59373.2023.00021},
	urldate = {2024-12-11},
	booktitle = {2023 5th {International} {Conference} on {Decision} {Science} \& {Management} ({ICDSM})},
	publisher = {IEEE},
	author = {Xiao, Yineng},
	month = mar,
	year = {2023},
	keywords = {/unread},
	pages = {49--52},
}

@inproceedings{jdidou_enhancing_2024,
	address = {Palma, Spain},
	title = {{ENHANCING} {ROBOTIC} {EDUCATION}: {THE} {IMPACT} {OF} {INTERACTIVE} {TALKING} {ROBOTS} {ON} {LEARNING} {AND} {ENGAGEMENT}},
	shorttitle = {{ENHANCING} {ROBOTIC} {EDUCATION}},
	url = {https://library.iated.org/view/JDIDOU2024ENH2},
	doi = {10.21125/edulearn.2024.2438},
	urldate = {2024-12-11},
	author = {Jdidou, Aymane and Aammou, Souhaib},
	month = jul,
	year = {2024},
	keywords = {/unread},
	pages = {10122--10128},
}

@article{vaiani_end-user_2024,
	title = {End-{User} {Development} for {Human}-{Robot} {Interaction}: {Results} and {Trends} in an {Emerging} {Field}},
	volume = {8},
	shorttitle = {End-{User} {Development} for {Human}-{Robot} {Interaction}},
	url = {https://dl.acm.org/doi/10.1145/3661146},
	doi = {10.1145/3661146},
	abstract = {This paper presents a comprehensive survey on End-User Development for Human-Robot Interaction, examining existing literature to validate findings and identify unexplored areas for future research. It explores the importance of End-User Development in allowing non-expert users to customise robots, covering methodologies, evaluation methods, robot types, and application contexts. The findings reveal various End-User Development approaches, evaluation practices, and robots application domains, leading to discussions on the untapped potential of End-User Development in enhancing Human-Robot Interaction across diverse fields. The document aims to provide groundwork for future studies, highlighting the necessity for new evaluation standards and greater customisation in robotic technologies.},
	number = {EICS},
	urldate = {2024-12-11},
	journal = {Proc. ACM Hum.-Comput. Interact.},
	author = {Vaiani, Giacomo and Patern√≤, Fabio},
	month = jun,
	year = {2024},
	keywords = {/unread},
	pages = {252:1--252:40},
}

@article{rogowski_scenario-based_2022,
	title = {Scenario-{Based} {Programming} of {Voice}-{Controlled} {Medical} {Robotic} {Systems}},
	volume = {22},
	copyright = {https://creativecommons.org/licenses/by/4.0/},
	issn = {1424-8220},
	url = {https://www.mdpi.com/1424-8220/22/23/9520},
	doi = {10.3390/s22239520},
	abstract = {An important issue in medical robotics is communication between physicians and robots. Speech-based communication is of particular advantage in robot-assisted surgery. It frees the surgeon‚Äôs hands; hence, he can focus on the principal tasks. Man-machine voice communication is the subject of research in various domains (industry, social robotics), but medical robots are very specific. They must precisely synchronize their activities with operators. Voice commands must be possibly short. They must be executed without significant delays. An important factor is the use of a vision system that provides visual information in direct synchronization with surgeon actions. Its functions could be also controlled using speech. The aim of the research presented in this paper was to develop a method facilitating creation of voice-controlled medical robotic systems, fulfilling the mentioned requirements and taking into account possible scenarios of man-machine collaboration in such systems. A robot skill description (RSD) format was proposed in order to facilitate programming of voice control applications. A sample application was developed, and experiments were conducted in order to draw conclusions regarding the usefulness of speech-based interfaces in medical robotics. The results show that a reasonable selection of system functions controlled by voice may lead to significant improvement of man-machine collaboration.},
	language = {en},
	number = {23},
	urldate = {2024-12-11},
	journal = {Sensors},
	author = {Rogowski, Adam},
	month = dec,
	year = {2022},
	keywords = {/unread},
	pages = {9520},
}

@article{muthugala_review_2018,
	title = {A {Review} of {Service} {Robots} {Coping} {With} {Uncertain} {Information} in {Natural} {Language} {Instructions}},
	volume = {6},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/abstract/document/8298518},
	doi = {10.1109/ACCESS.2018.2808369},
	abstract = {Intelligent service robots are currently being developed to cater to demands in emerging areas of robotic applications, ranging from entertainment to health care. These service robots are intended to be operated by nonexpert users, and their service tasks involve direct interaction between these robots and their human users. Thus, human-friendly interactive features are generally preferred for such service robots. Humans prefer to use voice instructions, responses, and suggestions to convey ideas to their peers. However, information conveyed through natural language communication is imprecise because it tends to contain uncertain/qualitative information instead of precise quantitative information. Therefore, the ability to cope with uncertain information in natural language instructions is mandatory for human-friendly service robots. This paper presents a review of service robots and systems that can cope with uncertain information in natural language instructions. The available literature has been investigated and analyzed to identify the limitations of the existing methods and possible improvements. The identified limitations and possible improvements are presented as the outcomes of the review.},
	urldate = {2024-12-11},
	journal = {IEEE Access},
	author = {Muthugala, M. A. Viraj J. and Jayasekara, A. G. Buddhika P.},
	year = {2018},
	note = {Conference Name: IEEE Access},
	keywords = {/unread, Human-robot interaction, Medical services, Natural languages, Service robots, Sports equipment, Task analysis, Uncertain information understanding, human-friendly robotics, human‚Äìrobot interaction, service robotics},
	pages = {12913--12928},
}

@inproceedings{noauthor_service_2022-2,
	title = {Service {Robots} for {Fashion} {Retail} {Stores}: {Lessons} {Learned} from a {Case} {Study}},
	isbn = {978-989-53-2286-2},
	shorttitle = {Service {Robots} for {Fashion} {Retail} {Stores}},
	url = {https://eirai.org/images/proceedings_pdf/UH0122405.pdf},
	doi = {10.17758/EIRAI12.UH0122405},
	urldate = {2024-12-11},
	booktitle = {{AASEW}-22, {A3BES}-22, {EMSSH}-22 \& {PLSSE}-22 2022 {European} {International} {Conferences}},
	publisher = {Excellence in Research \& Innovation (EIRAI)},
	year = {2022},
	keywords = {/unread},
}

@inproceedings{mumolo_interactive_2007,
	address = {Noordwijk, Netherlands},
	title = {An {Interactive} {Receptionist} {Robot} for {Users} with {Vocal} {Pathologies}},
	isbn = {978-1-4244-1319-5 978-1-4244-1320-1},
	url = {http://ieeexplore.ieee.org/document/4428548/},
	doi = {10.1109/ICORR.2007.4428548},
	urldate = {2024-12-10},
	booktitle = {2007 {IEEE} 10th {International} {Conference} on {Rehabilitation} {Robotics}},
	publisher = {IEEE},
	author = {Mumolo, Enzo and Nolich, Massimiliano and Vercelli, Gianni},
	month = jun,
	year = {2007},
	keywords = {/unread},
	pages = {1016--1025},
}

@article{reicherts_its_2022,
	title = {It's {Good} to {Talk}: {A} {Comparison} of {Using} {Voice} {Versus} {Screen}-{Based} {Interactions} for {Agent}-{Assisted} {Tasks}},
	volume = {29},
	issn = {1073-0516, 1557-7325},
	shorttitle = {It's {Good} to {Talk}},
	url = {https://dl.acm.org/doi/10.1145/3484221},
	doi = {10.1145/3484221},
	abstract = {Voice assistants have become hugely popular in the home as domestic and entertainment devices. Recently, there has been a move towards developing them for work settings. For example,
              Alexa for Business
              and
              IBM Watson
              for Business
              were designed to improve productivity, by assisting with various tasks, such as scheduling meetings and taking minutes. However, this kind of assistance is largely limited to planning and managing user's work. How might they be developed to do more by way of empowering people at work? Our research is concerned with achieving this by developing an agent with the role of a facilitator that assists users during an ongoing task. Specifically, we were interested in whether the modality in which the agent interacts with users makes a difference: How does a voice versus screen-based agent interaction affect user behavior? We hypothesized that voice would be more immediate and emotive, resulting in more fluid conversations and interactions. Here, we describe a user study that compared the benefits of using voice versus screen-based interactions when interacting with a system incorporating an agent, involving pairs of participants doing an exploratory data analysis task that required them to make sense of a series of data visualizations. The findings from the study show marked differences between the two conditions, with voice resulting in more turn-taking in discussions, questions asked, more interactions with the system and a tendency towards more immediate, faster-paced discussions following agent prompts. We discuss the possible reasons for why talking and being prompted by a voice assistant may be preferable and more effective at mediating human-human conversations and we translate some of the key insights of this research into design implications.},
	language = {en},
	number = {3},
	urldate = {2024-12-10},
	journal = {ACM Transactions on Computer-Human Interaction},
	author = {Reicherts, Leon and Rogers, Yvonne and Capra, Licia and Wood, Ethan and Duong, Tu Dinh and Sebire, Neil},
	month = jun,
	year = {2022},
	keywords = {/unread},
	pages = {1--41},
}

@inproceedings{haage_prototype_2002,
	address = {Berlin, Germany},
	title = {A prototype robot speech interface with multimodal feedback},
	isbn = {978-0-7803-7545-1},
	url = {http://ieeexplore.ieee.org/document/1045630/},
	doi = {10.1109/ROMAN.2002.1045630},
	urldate = {2024-12-10},
	booktitle = {Proceedings. 11th {IEEE} {International} {Workshop} on {Robot} and {Human} {Interactive} {Communication}},
	publisher = {IEEE},
	author = {Haage, M. and Schotz, S. and Nugues, P.},
	year = {2002},
	keywords = {/unread},
	pages = {247--252},
}

@inproceedings{hiruma_proposal_2020,
	address = {Las Vegas, NV, USA},
	title = {Proposal of {Interactive} {Home} {System} {Using} {Computer} {Graphics} ({CG}) {Characters}},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	isbn = {978-1-72815-186-1},
	url = {https://ieeexplore.ieee.org/document/9043011/},
	doi = {10.1109/ICCE46568.2020.9043011},
	urldate = {2024-12-10},
	booktitle = {2020 {IEEE} {International} {Conference} on {Consumer} {Electronics} ({ICCE})},
	publisher = {IEEE},
	author = {Hiruma, Daisuke and Uemura, Hiroki and Okumura, Makiko and Abe, Keiichi},
	month = jan,
	year = {2020},
	keywords = {/unread},
	pages = {1--4},
}

@article{marin_vargas_verbal_2021,
	title = {Verbal {Communication} in {Robotics}: {A} {Study} on {Salient} {Terms}, {Research} {Fields} and {Trends} in the {Last} {Decades} {Based} on a {Computational} {Linguistic} {Analysis}},
	volume = {2},
	issn = {2624-9898},
	shorttitle = {Verbal {Communication} in {Robotics}},
	url = {https://www.frontiersin.org/journals/computer-science/articles/10.3389/fcomp.2020.591164/full},
	doi = {10.3389/fcomp.2020.591164},
	abstract = {{\textless}p{\textgreater}Verbal communication is an expanding field in robotics showing a significant increase in both the industrial and research field. The application of verbal communication in robotics aims to reach a natural human-like interaction with robots. In this study, we investigated how salient terms related to verbal communication in robotics have evolved over the years, what are the topics that recur in the related literature, and what are their trends. The study is based on a computational linguistic analysis conducted on a database of 7,435 scientific publications over the last 2 decades. This comprehensive dataset was extracted from the Scopus database using specific key-words. Our results show how relevant terms of verbal communication evolved, which are the main coherent topics and how they have changed over the years. We highlighted positive and negative trends for the most coherent topics and the distribution over the years for the most significant ones. In particular, verbal communication resulted in being highly relevant for social robotics. Potentially, achieving natural verbal communication with a robot can have a great impact on the scientific, societal, and economic role of robotics in the future.{\textless}/p{\textgreater}},
	language = {English},
	urldate = {2024-09-06},
	journal = {Frontiers in Computer Science},
	author = {Marin Vargas, Alessandro and Cominelli, Lorenzo and Dell‚ÄôOrletta, Felice and Scilingo, Enzo Pasquale},
	month = feb,
	year = {2021},
	note = {Publisher: Frontiers
TLDR: This study investigated how salient terms related to verbal communication in robotics have evolved over the years, what are the topics that recur in the related literature, and what are their trends, using a database of 7,435 scientific publications over the last 2 decades.},
	keywords = {\#EX: ‰∏çÂåÖÂê´HRI, \#EX:‰∏çÂåÖÂê´Â§ßÊ®°Âûã, \#EX:Â∑•‰∏öÊú∫Âô®‰∫∫, \#‰∏çÁ°ÆÂÆö, \#Áü≠Êñá, \#ÁªºËø∞, /unread, Affective Computing, Computational linguistic analysis, Data Mining, Design, Speech generation, Topic Modeling, Users, ccfInfo: CCF-None FCOMP, citationNumber: 8, social robotics, speech synthesis, verbal communication},
}

@article{frijns_communication_2023,
	title = {Communication {Models} in {Human}‚Äì{Robot} {Interaction}: {An} {Asymmetric} {MODel} of {ALterity} in {Human}‚Äì{Robot} {Interaction} ({AMODAL}-{HRI})},
	volume = {15},
	issn = {1875-4791, 1875-4805},
	shorttitle = {Communication {Models} in {Human}‚Äì{Robot} {Interaction}},
	url = {https://link.springer.com/10.1007/s12369-021-00785-7},
	doi = {10.1007/s12369-021-00785-7},
	abstract = {Abstract
            We argue for an interdisciplinary approach that connects existing models and theories in Human‚ÄìRobot Interaction (HRI) to traditions in communication theory. In this article, we review existing models of interpersonal communication and interaction models that have been applied and developed in the contexts of HRI and social robotics. We argue that often, symmetric models are proposed in which the human and robot agents are depicted as having similar ways of functioning (similar capabilities, components, processes). However, we argue that models of human‚Äìrobot interaction or communication should be asymmetric instead. We propose an asymmetric interaction model called AMODAL-HRI (an Asymmetric MODel of ALterity in Human‚ÄìRobot Interaction). This model is based on theory on joint action, common robot architectures and cognitive architectures, and Kincaid‚Äôs model of communication. On the basis of this model, we discuss key differences between humans and robots that influence human expectations regarding interacting with robots, and identify design implications.},
	language = {en},
	number = {3},
	urldate = {2024-09-06},
	journal = {International Journal of Social Robotics},
	author = {Frijns, Helena Anna and Sch√ºrer, Oliver and Koeszegi, Sabine Theresia},
	month = mar,
	year = {2023},
	note = {TLDR: An asymmetric interaction model called AMODAL-HRI (an Asymmetric MODel of ALterity in Human‚ÄìRobot Interaction) is proposed, based on theory on joint action, common robot architectures and cognitive architectures, and Kincaid‚Äôs model of communication.},
	keywords = {\#EX: ‰∏çÂåÖÂê´HRI, \#EX:‰∏çÂåÖÂê´Â§ßÊ®°Âûã, \#EX:Â∑•‰∏öÊú∫Âô®‰∫∫, \#‰∏çÁ°ÆÂÆö, \#Áü≠Êñá, \#ÁªºËø∞, /unread, Design, Users, ccfInfo: CCF-None IJSR, citationNumber: 8},
	pages = {473--500},
}

@article{nikolaidis_planning_2018,
	title = {Planning with {Verbal} {Communication} for {Human}-{Robot} {Collaboration}},
	volume = {7},
	url = {https://dl.acm.org/doi/10.1145/3203305},
	doi = {10.1145/3203305},
	abstract = {Human collaborators coordinate effectively their actions through both verbal and non-verbal communication. We believe that the the same should hold for human-robot teams. We propose a formalism that enables a robot to decide optimally between taking a physical action toward task completion and issuing an utterance to the human teammate. We focus on two types of utterances: verbal commands, where the robot asks the human to take a physical action, and state-conveying actions, where the robot informs the human about its internal state, which captures the information that the robot uses in its decision making. Human subject experiments show that enabling the robot to issue verbal commands is the most effective form of communicating objectives, while retaining user trust in the robot. Communicating information about the robot‚Äôs state should be done judiciously, since many participants questioned the truthfulness of the robot statements when the robot did not provide sufficient explanation about its actions.},
	number = {3},
	urldate = {2024-09-06},
	journal = {J. Hum.-Robot Interact.},
	author = {Nikolaidis, Stefanos and Kwon, Minae and Forlizzi, Jodi and Srinivasa, Siddhartha},
	month = nov,
	year = {2018},
	note = {TLDR: A formalism is proposed that enables a robot to decide optimally between taking a physical action toward task completion and issuing an utterance to the human teammate, which captures the information that the robot uses in its decision making.},
	keywords = {\#EX: ‰∏çÂåÖÂê´HRI, \#EX:‰∏çÂåÖÂê´Â§ßÊ®°Âûã, \#EX:Â∑•‰∏öÊú∫Âô®‰∫∫, \#‰∏çÁ°ÆÂÆö, \#Áü≠Êñá, \#ÁªºËø∞, /unread, Design, Users, ccfInfo: CCF-None THRI, citationNumber: 31},
	pages = {22:1--22:21},
}

@misc{arjmand_empathic_2024,
	title = {Empathic {Grounding}: {Explorations} using {Multimodal} {Interaction} and {Large} {Language} {Models} with {Conversational} {Agents}},
	shorttitle = {Empathic {Grounding}},
	url = {http://arxiv.org/abs/2407.01824},
	doi = {10.1145/3652988.3673949},
	abstract = {We introduce the concept of "empathic grounding" in conversational agents as an extension of Clark's conceptualization of grounding in conversation in which the grounding criterion includes listener empathy for the speaker's affective state. Empathic grounding is generally required whenever the speaker's emotions are foregrounded and can make the grounding process more efficient and reliable by communicating both propositional and affective understanding. Both speaker expressions of affect and listener empathic grounding can be multimodal, including facial expressions and other nonverbal displays. Thus, models of empathic grounding for embodied agents should be multimodal to facilitate natural and efficient communication. We describe a multimodal model that takes as input user speech and facial expression to generate multimodal grounding moves for a listening agent using a large language model. We also describe a testbed to evaluate approaches to empathic grounding, in which a humanoid robot interviews a user about a past episode of pain and then has the user rate their perception of the robot's empathy. We compare our proposed model to one that only generates non-affective grounding cues in a between-subjects experiment. Findings demonstrate that empathic grounding increases user perceptions of empathy, understanding, emotional intelligence, and trust. Our work highlights the role of emotion awareness and multimodality in generating appropriate grounding moves for conversational agents.},
	language = {en-US},
	urldate = {2024-07-29},
	author = {Arjmand, Mehdi and Nouraei, Farnaz and Steenstra, Ian and Bickmore, Timothy},
	month = jul,
	year = {2024},
	note = {arXiv:2407.01824 [cs]
TLDR: A multimodal model that takes as input user speech and facial expression to generate multimodal grounding moves for a listening agent using a large language model is described and it is demonstrated that empathic grounding increases user perceptions of empathy, understanding, emotional intelligence, and trust.},
	keywords = {\#EX: ‰∏çÂåÖÂê´HRI, \#EX:‰∏çÂåÖÂê´Â§ßÊ®°Âûã, \#EX:Â∑•‰∏öÊú∫Âô®‰∫∫, \#‰∏çÁ°ÆÂÆö, \#Áü≠Êñá, \#ÁªºËø∞, /unread, Computer Science - Computation and Language, Computer Science - Human-Computer Interaction, Computer Science - Robotics, Design, Users, ccfInfo: CCF-None CORR, citationNumber: 0},
}

@inproceedings{son_genquery_2024,
	address = {Honolulu HI USA},
	title = {{GenQuery}: {Supporting} {Expressive} {Visual} {Search} with {Generative} {Models}},
	isbn = {979-8-4007-0330-0},
	shorttitle = {{GenQuery}},
	url = {https://dl.acm.org/doi/10.1145/3613904.3642847},
	doi = {10.1145/3613904.3642847},
	language = {en},
	urldate = {2024-12-02},
	booktitle = {Proceedings of the {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Son, Kihoon and Choi, DaEun and Kim, Tae Soo and Kim, Young-Ho and Kim, Juho},
	month = may,
	year = {2024},
	note = {TLDR: This work proposes GenQuery, a novel system that integrates generative models into the visual search process and enables users to generatively modify images and use these in similarity-based search.},
	keywords = {ccfInfo: CCF-A CHI, citationNumber: 0},
	pages = {1--19},
}

@inproceedings{lin_rambler_2024,
	address = {New York, NY, USA},
	series = {{CHI} '24},
	title = {Rambler: {Supporting} {Writing} {With} {Speech} via {LLM}-{Assisted} {Gist} {Manipulation}},
	isbn = {9798400703300},
	shorttitle = {Rambler},
	url = {https://dl.acm.org/doi/10.1145/3613904.3642217},
	doi = {10.1145/3613904.3642217},
	abstract = {Dictation enables efficient text input on mobile devices. However, writing with speech can produce disfluent, wordy, and incoherent text and thus requires heavy post-processing. This paper presents Rambler, an LLM-powered graphical user interface that supports gist-level manipulation of dictated text with two main sets of functions: gist extraction and macro revision. Gist extraction generates keywords and summaries as anchors to support the review and interaction with spoken text. LLM-assisted macro revisions allow users to respeak, split, merge, and transform dictated text without specifying precise editing locations. Together they pave the way for interactive dictation and revision that help close gaps between spontaneously spoken words and well-structured writing. In a comparative study with 12 participants performing verbal composition tasks, Rambler outperformed the baseline of a speech-to-text editor + ChatGPT, as it better facilitates iterative revisions with enhanced user control over the content while supporting surprisingly diverse user strategies.},
	urldate = {2024-12-08},
	booktitle = {Proceedings of the 2024 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Lin, Susan and Warner, Jeremy and Zamfirescu-Pereira, J.D. and Lee, Matthew G and Jain, Sauhard and Cai, Shanqing and Lertvittayakumjorn, Piyawat and Huang, Michael Xuelin and Zhai, Shumin and Hartmann, Bjoern and Liu, Can},
	year = {2024},
	keywords = {/unread},
	pages = {1--19},
}

@inproceedings{greenberg_usability_2008,
	address = {New York, NY, USA},
	series = {{CHI} '08},
	title = {Usability evaluation considered harmful (some of the time)},
	isbn = {978-1-60558-011-1},
	url = {https://dl.acm.org/doi/10.1145/1357054.1357074},
	doi = {10.1145/1357054.1357074},
	abstract = {Current practice in Human Computer Interaction as encouraged by educational institutes, academic review processes, and institutions with usability groups advocate usability evaluation as a critical part of every design process. This is for good reason: usability evaluation has a significant role to play when conditions warrant it. Yet evaluation can be ineffective and even harmful if naively done 'by rule' rather than 'by thought'. If done during early stage design, it can mute creative ideas that do not conform to current interface norms. If done to test radical innovations, the many interface issues that would likely arise from an immature technology can quash what could have been an inspired vision. If done to validate an academic prototype, it may incorrectly suggest a design's scientific worthiness rather than offer a meaningful critique of how it would be adopted and used in everyday practice. If done without regard to how cultures adopt technology over time, then today's reluctant reactions by users will forestall tomorrow's eager acceptance. The choice of evaluation methodology - if any - must arise from and be appropriate for the actual problem or research question under consideration.},
	language = {en-US},
	urldate = {2024-12-08},
	booktitle = {Proceedings of the {SIGCHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Greenberg, Saul and Buxton, Bill},
	month = apr,
	year = {2008},
	keywords = {/unread},
	pages = {111--120},
}

@inproceedings{zhang_visar_2023,
	address = {New York, NY, USA},
	series = {{UIST} '23},
	title = {{VISAR}: {A} {Human}-{AI} {Argumentative} {Writing} {Assistant} with {Visual} {Programming} and {Rapid} {Draft} {Prototyping}},
	isbn = {979-8-4007-0132-0},
	shorttitle = {{VISAR}},
	url = {https://doi.org/10.1145/3586183.3606800},
	doi = {10.1145/3586183.3606800},
	abstract = {In argumentative writing, writers must brainstorm hierarchical writing goals, ensure the persuasiveness of their arguments, and revise and organize their plans through drafting. Recent advances in large language models (LLMs) have made interactive text generation through a chat interface (e.g., ChatGPT) possible. However, this approach often neglects implicit writing context and user intent, lacks support for user control and autonomy, and provides limited assistance for sensemaking and revising writing plans. To address these challenges, we introduce VISAR, an AI-enabled writing assistant system designed to help writers brainstorm and revise hierarchical goals within their writing context, organize argument structures through synchronized text editing and visual programming, and enhance persuasiveness with argumentation spark recommendations. VISAR allows users to explore, experiment with, and validate their writing plans using automatic draft prototyping. A controlled lab study confirmed the usability and effectiveness of VISAR in facilitating the argumentative writing planning process.},
	language = {en-US},
	urldate = {2024-07-23},
	booktitle = {Proceedings of the 36th {Annual} {ACM} {Symposium} on {User} {Interface} {Software} and {Technology}},
	publisher = {Association for Computing Machinery},
	author = {Zhang, Zheng and Gao, Jie and Dhaliwal, Ranjodh Singh and Li, Toby Jia-Jun},
	month = oct,
	year = {2023},
	note = {TLDR: VISAR, an AI-enabled writing assistant system designed to help writers brainstorm and revise hierarchical goals within their writing context, organize argument structures through synchronized text editing and visual programming, and enhance persuasiveness with argumentation spark recommendations is introduced.},
	keywords = {/unread, ccfInfo: CCF-A UIST, citationNumber: 7, ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê},
	pages = {1--30},
}

@inproceedings{van_deurzen_visual_2024,
	address = {New York, NY, USA},
	series = {{HRI} '24},
	title = {A {Visual} {Design} {Space} for {One}-{Dimensional} {Intelligible} {Human}-{Robot} {Interaction} {Visualizations}},
	isbn = {979-8-4007-0323-2},
	url = {https://dl.acm.org/doi/10.1145/3610978.3640557},
	doi = {10.1145/3610978.3640557},
	abstract = {To enable effective communication between users and autonomous robots, it is crucial to have a shared understanding of goals and actions. This is made possible through an intelligible interface that communicates relevant information. This intelligibility enhances user comprehension, enabling them to anticipate the robot's actions and respond appropriately. However, because robots can perform a wide variety of actions and communication resources are limited, such as the number of available "pixels", visualizations must be carefully designed. To tackle this challenge, we have developed a visual design framework and design space that can be used to create intelligible visualizations for human-robot interaction. Our framework focuses on three key components: information type, pixel layout, and robot type. We demonstrate how intelligibility can be integrated into interactions through prototype visualizations featuring a one-dimensional pixel layout, laying the groundwork for developing more detailed and understandable visualizations.},
	language = {en-US},
	urldate = {2024-11-19},
	booktitle = {Companion of the 2024 {ACM}/{IEEE} {International} {Conference} on {Human}-{Robot} {Interaction}},
	publisher = {Association for Computing Machinery},
	author = {Van Deurzen, Bram and Rovelo Ruiz, Gustavo Alberto and Luyten, Kris},
	month = mar,
	year = {2024},
	note = {TLDR: This work develops a visual design framework and design space that can be used to create intelligible visualizations for human-robot interaction and demonstrates how intelligibility can be integrated into interactions through prototype visualizations featuring a one-dimensional pixel layout.},
	keywords = {/unread, ccfInfo: CCF-None HRI, citationNumber: 0},
	pages = {1067--1071},
}

@article{doering_modeling_2019,
	title = {Modeling {Interaction} {Structure} for {Robot} {Imitation} {Learning} of {Human} {Social} {Behavior}},
	volume = {49},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/OAPA.html},
	issn = {2168-2291, 2168-2305},
	url = {https://ieeexplore.ieee.org/document/8653359/},
	doi = {10.1109/THMS.2019.2895753},
	number = {3},
	urldate = {2024-12-07},
	journal = {IEEE Transactions on Human-Machine Systems},
	author = {Doering, Malcolm and Glas, Dylan F. and Ishiguro, Hiroshi},
	month = jun,
	year = {2019},
	keywords = {/unread},
	pages = {219--231},
}

@inproceedings{awais_human-robot_2013,
	address = {Islamabad, Pakistan},
	title = {Human-{Robot} {Interaction} in an {Unknown} {Human} {Intention} {Scenario}},
	isbn = {978-1-4799-2503-2 978-1-4799-2293-2},
	url = {http://ieeexplore.ieee.org/document/6717232/},
	doi = {10.1109/FIT.2013.24},
	urldate = {2024-12-07},
	booktitle = {2013 11th {International} {Conference} on {Frontiers} of {Information} {Technology}},
	publisher = {IEEE},
	author = {Awais, Muhammad and Henrich, Dominik},
	month = dec,
	year = {2013},
	keywords = {/unread},
	pages = {89--94},
}

@inproceedings{romat_natural_2016,
	address = {Christchurch, New Zealand},
	title = {Natural human-robot interaction using social cues},
	isbn = {978-1-4673-8370-7},
	url = {http://ieeexplore.ieee.org/document/7451827/},
	doi = {10.1109/HRI.2016.7451827},
	urldate = {2024-12-07},
	booktitle = {2016 11th {ACM}/{IEEE} {International} {Conference} on {Human}-{Robot} {Interaction} ({HRI})},
	publisher = {IEEE},
	author = {Romat, Hugo and Williams, Mary-Anne and Wang, Xun and Johnston, Benjamin and Bard, Henry},
	month = mar,
	year = {2016},
	keywords = {/unread},
	pages = {503--504},
}

@inproceedings{ogawa_investigating_2024,
	address = {Honolulu HI USA},
	title = {Investigating {Effect} of {Altered} {Auditory} {Feedback} on {Self}-{Representation}, {Subjective} {Operator} {Experience}, and {Task} {Performance} in {Teleoperation} of a {Social} {Robot}},
	isbn = {979-8-4007-0330-0},
	url = {https://dl.acm.org/doi/10.1145/3613904.3642561},
	doi = {10.1145/3613904.3642561},
	language = {en},
	urldate = {2024-11-30},
	booktitle = {Proceedings of the {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Ogawa, Nami and Baba, Jun and Nakanishi, Junya},
	month = may,
	year = {2024},
	keywords = {/unread, ccfInfo: CCF-A CHI, citationNumber: 0},
	pages = {1--18},
}

@article{patel_implementation_2023,
	title = {Implementation of a visual aid to improve disease comprehension in patients with new breast cancer diagnoses.},
	volume = {19},
	issn = {2688-1527, 2688-1535},
	url = {https://ascopubs.org/doi/10.1200/OP.2023.19.11_suppl.358},
	doi = {10.1200/OP.2023.19.11_suppl.358},
	abstract = {358
            Background: Studies have shown that patients with new cancer diagnoses are overwhelmed due to the large amount of information presented at their initial oncology visit. Inability to meet these high informational demands has been associated with poorer outcomes. Most patient education is done verbally despite data showing that patients only retain 14\% of verbal communication compared to 85\% of information with visual aids. The aim of this study was to determine if implementation of a visual educational tool on breast cancer diagnosis and treatment would improve comprehension in patients with new diagnoses. Methods: We designed an educational tool to visually depict breast cancer subtype, stage, and treatment plan. We recruited English-speaking patients with new diagnoses of breast cancer who were seen for an initial oncology visit at the Mount Sinai Dubin Breast Center. Patients were randomized to receive the visual aid (intervention) or continue with standard-of-care (control). For patients randomized to the intervention arm, the oncologist used the visual aid to provide education on their diagnosis and treatment plan during the visit. All patients completed a survey at their initial visit and approximately 1 month after diagnosis. The survey included demographic information, patient preferences, and assessment of their understanding of diagnosis and treatment. The worksheet and surveys were piloted with 5 patients with breast cancer and modified based on their feedback. A knowledge score was calculated based on their survey responses with scores ranging from 2 (low knowledge) to 6 (all questions answered correctly). We utilized paired t-test to compare the means of knowledge scores at initial visit and 1 month follow up. Results: Twenty female patients were enrolled from 2022-2023 with a median age of 61 years. Ten patients were randomized to the intervention arm and 10 to the control arm. The initial mean knowledge score of the control group was 4.9 and the follow up mean score was 5.6 (P = 0.11). The initial mean score of the intervention group was 5.3 whereas the follow up mean score was 5.6 (P = 0.84). There was no significant difference between the intervention group and the control group (P = 0.944). An increase in overall scores was noted from initial to follow up survey in all groups. There was a complete elimination of all low scores ({\textless} 3) across both groups in the follow up survey. Conclusions: Education at the initial visit improves understanding of breast cancer diagnosis and treatment. Our study demonstrated a standardized informational tool did not improve understanding when compared to individualized teaching with their oncologist. However, our study was limited to a single breast cancer center with a small sample size and English-speaking patients. Next steps include translation of the educational tool into additional languages, as well as expansion of this study to a larger patient population.},
	language = {en},
	number = {11\_suppl},
	urldate = {2024-11-27},
	journal = {JCO Oncology Practice},
	author = {Patel, Rima and Blanter, Julia and Kier, Melanie Wain and Waksal, Julian Alexander and Wu, Meng and Berwick, Shana and Sheng, Tianxiang and Kessler, Alaina J. and Bhardwaj, Aarti Sonia},
	month = nov,
	year = {2023},
	keywords = {/unread, ccfInfo: Not Found, citationNumber: 0},
	pages = {358--358},
}

@article{ahmed_using_2006,
	title = {Using computers as visual aids to enhance communication in therapy},
	volume = {22},
	copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
	issn = {07475632},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0747563204000536},
	doi = {10.1016/j.chb.2004.03.008},
	language = {en},
	number = {5},
	urldate = {2024-11-27},
	journal = {Computers in Human Behavior},
	author = {Ahmed, Mohiuddin and Boisvert, Charles M.},
	month = sep,
	year = {2006},
	keywords = {/unread, ccfInfo: CCF-None CHB, citationNumber: 11},
	pages = {847--855},
}

@incollection{davis_15_2005,
	address = {Burlington},
	title = {15 - {VISUAL} {AIDS} {TO} {COMMUNICATION}},
	isbn = {978-0-12-088424-7},
	url = {https://www.sciencedirect.com/science/article/pii/B9780120884247500169},
	abstract = {As with other symbolic images, visual aids can complement or supplement words in both written and spoken texts. Their basic function is to clarify points beyond what the spoken or written words alone can do. In scientific communication if they attract attention only to themselves and do not increase clarity, they serve as distractions. This chapter discusses different kinds of exhibits that help to clarify a point to the audience, and accompanies an oral presentation. Each of these kinds of exhibits requires its own equipment, whether it is a chalk and a chalkboard, a projector, or a poster board. Flipcharts and posters accommodate small audiences and require well-lighted conditions. Slides require subdued lighting but can serve a rather large audience. Speakers often fail to note how handy a chalkboard can be. Visual aids can add information, they can illustrate or provide examples or evidence, or they may repeat what is being said or written.},
	urldate = {2024-11-27},
	booktitle = {Scientific {Papers} and {Presentations} ({Second} {Edition})},
	publisher = {Academic Press},
	author = {Davis, Martha},
	editor = {Davis, Martha},
	month = jan,
	year = {2005},
	doi = {10.1016/B978-012088424-7/50016-9},
	keywords = {/unread, ccfInfo: Not Found, citationNumber: Not Found},
	pages = {163--173},
}

@misc{haque_inferring_2024,
	title = {Inferring {Alt}-text {For} {UI} {Icons} {With} {Large} {Language} {Models} {During} {App} {Development}},
	url = {http://arxiv.org/abs/2409.18060},
	doi = {10.48550/arXiv.2409.18060},
	abstract = {Ensuring accessibility in mobile applications remains a significant challenge, particularly for visually impaired users who rely on screen readers. User interface icons are essential for navigation and interaction and often lack meaningful alt-text, creating barriers to effective use. Traditional deep learning approaches for generating alt-text require extensive datasets and struggle with the diversity and imbalance of icon types. More recent Vision Language Models (VLMs) require complete UI screens, which can be impractical during the iterative phases of app development. To address these issues, we introduce a novel method using Large Language Models (LLMs) to autonomously generate informative alt-text for mobile UI icons with partial UI data. By incorporating icon context, that include class, resource ID, bounds, OCR-detected text, and contextual information from parent and sibling nodes, we fine-tune an off-the-shelf LLM on a small dataset of approximately 1.4k icons, yielding IconDesc. In an empirical evaluation and a user study IconDesc demonstrates significant improvements in generating relevant alt-text. This ability makes IconDesc an invaluable tool for developers, aiding in the rapid iteration and enhancement of UI accessibility.},
	urldate = {2024-12-05},
	publisher = {arXiv},
	author = {Haque, Sabrina and Csallner, Christoph},
	month = oct,
	year = {2024},
	note = {arXiv:2409.18060 [cs]},
	keywords = {/unread, Computer Science - Human-Computer Interaction, Computer Science - Software Engineering},
}

@misc{xie_large_2024,
	title = {Large {Multimodal} {Agents}: {A} {Survey}},
	shorttitle = {Large {Multimodal} {Agents}},
	url = {http://arxiv.org/abs/2402.15116},
	doi = {10.48550/arXiv.2402.15116},
	abstract = {Large language models (LLMs) have achieved superior performance in powering text-based AI agents, endowing them with decision-making and reasoning abilities akin to humans. Concurrently, there is an emerging research trend focused on extending these LLM-powered AI agents into the multimodal domain. This extension enables AI agents to interpret and respond to diverse multimodal user queries, thereby handling more intricate and nuanced tasks. In this paper, we conduct a systematic review of LLM-driven multimodal agents, which we refer to as large multimodal agents ( LMAs for short). First, we introduce the essential components involved in developing LMAs and categorize the current body of research into four distinct types. Subsequently, we review the collaborative frameworks integrating multiple LMAs , enhancing collective efficacy. One of the critical challenges in this field is the diverse evaluation methods used across existing studies, hindering effective comparison among different LMAs . Therefore, we compile these evaluation methodologies and establish a comprehensive framework to bridge the gaps. This framework aims to standardize evaluations, facilitating more meaningful comparisons. Concluding our review, we highlight the extensive applications of LMAs and propose possible future research directions. Our discussion aims to provide valuable insights and guidelines for future research in this rapidly evolving field. An up-to-date resource list is available at https://github.com/jun0wanan/awesome-large-multimodal-agents.},
	urldate = {2024-12-05},
	publisher = {arXiv},
	author = {Xie, Junlin and Chen, Zhihong and Zhang, Ruifei and Wan, Xiang and Li, Guanbin},
	month = feb,
	year = {2024},
	note = {arXiv:2402.15116 [cs]},
	keywords = {/unread, Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{xie_large_2024-1,
	title = {Large {Multimodal} {Agents}: {A} {Survey}},
	shorttitle = {Large {Multimodal} {Agents}},
	url = {http://arxiv.org/abs/2402.15116},
	doi = {10.48550/arXiv.2402.15116},
	abstract = {Large language models (LLMs) have achieved superior performance in powering text-based AI agents, endowing them with decision-making and reasoning abilities akin to humans. Concurrently, there is an emerging research trend focused on extending these LLM-powered AI agents into the multimodal domain. This extension enables AI agents to interpret and respond to diverse multimodal user queries, thereby handling more intricate and nuanced tasks. In this paper, we conduct a systematic review of LLM-driven multimodal agents, which we refer to as large multimodal agents ( LMAs for short). First, we introduce the essential components involved in developing LMAs and categorize the current body of research into four distinct types. Subsequently, we review the collaborative frameworks integrating multiple LMAs , enhancing collective efficacy. One of the critical challenges in this field is the diverse evaluation methods used across existing studies, hindering effective comparison among different LMAs . Therefore, we compile these evaluation methodologies and establish a comprehensive framework to bridge the gaps. This framework aims to standardize evaluations, facilitating more meaningful comparisons. Concluding our review, we highlight the extensive applications of LMAs and propose possible future research directions. Our discussion aims to provide valuable insights and guidelines for future research in this rapidly evolving field. An up-to-date resource list is available at https://github.com/jun0wanan/awesome-large-multimodal-agents.},
	language = {en-US},
	urldate = {2024-12-05},
	publisher = {arXiv},
	author = {Xie, Junlin and Chen, Zhihong and Zhang, Ruifei and Wan, Xiang and Li, Guanbin},
	month = feb,
	year = {2024},
	note = {arXiv:2402.15116 [cs]},
	keywords = {/unread, Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, ‚ùå},
}

@article{marin_vargas_verbal_2021-1,
	title = {Verbal {Communication} in {Robotics}: {A} {Study} on {Salient} {Terms}, {Research} {Fields} and {Trends} in the {Last} {Decades} {Based} on a {Computational} {Linguistic} {Analysis}},
	volume = {2},
	issn = {2624-9898},
	shorttitle = {Verbal {Communication} in {Robotics}},
	url = {https://www.frontiersin.org/journals/computer-science/articles/10.3389/fcomp.2020.591164/full},
	doi = {10.3389/fcomp.2020.591164},
	abstract = {{\textless}p{\textgreater}Verbal communication is an expanding field in robotics showing a significant increase in both the industrial and research field. The application of verbal communication in robotics aims to reach a natural human-like interaction with robots. In this study, we investigated how salient terms related to verbal communication in robotics have evolved over the years, what are the topics that recur in the related literature, and what are their trends. The study is based on a computational linguistic analysis conducted on a database of 7,435 scientific publications over the last 2 decades. This comprehensive dataset was extracted from the Scopus database using specific key-words. Our results show how relevant terms of verbal communication evolved, which are the main coherent topics and how they have changed over the years. We highlighted positive and negative trends for the most coherent topics and the distribution over the years for the most significant ones. In particular, verbal communication resulted in being highly relevant for social robotics. Potentially, achieving natural verbal communication with a robot can have a great impact on the scientific, societal, and economic role of robotics in the future.{\textless}/p{\textgreater}},
	language = {en-US},
	urldate = {2024-12-05},
	journal = {Frontiers in Computer Science},
	author = {Marin Vargas, Alessandro and Cominelli, Lorenzo and Dell‚ÄôOrletta, Felice and Scilingo, Enzo Pasquale},
	month = feb,
	year = {2021},
	note = {Publisher: Frontiers},
	keywords = {/unread, Affective Computing, Computational linguistic analysis, Data Mining, Speech generation, Topic Modeling, social robotics, speech synthesis, verbal communication, ‚ùå},
}

@inproceedings{pozzi_pointing_2022,
	address = {Cham},
	title = {Pointing {Gestures} for {Human}-{Robot} {Interaction} in {Service} {Robotics}: {A} {Feasibility} {Study}},
	isbn = {978-3-031-08645-8},
	shorttitle = {Pointing {Gestures} for {Human}-{Robot} {Interaction} in {Service} {Robotics}},
	doi = {10.1007/978-3-031-08645-8_54},
	abstract = {Research in service robotics strives at having a positive impact on people‚Äôs quality of life by the introduction of robotic helpers for everyday activities. From this ambition arises the need of enabling natural communication between robots and ordinary people. For this reason, Human-Robot Interaction (HRI) is an extensively investigated topic, exceeding language-based exchange of information, to include all the relevant facets of communication. Each aspect of communication (e.g.¬†hearing, sight, touch) comes with its own peculiar strengths and limits, thus they are often combined to improve robustness and naturalness. In this contribution, an HRI framework is presented, based on pointing gestures as the preferred interaction strategy. Pointing gestures are selected as they are an innate behavior to direct another attention, and thus could represent a natural way to require a service to a robot. To complement the visual information, the user could be prompted to give voice commands to resolve ambiguities and prevent the execution of unintended actions. The two layers (perceptive and semantic) architecture of the proposed HRI system is described. The perceptive layer is responsible for objects mapping, action detection, and assessment of the indicated direction. Moreover, it has to listen to uses‚Äô voice commands. To avoid privacy issues and not burden the computational resources of the robot, the interaction would be triggered by a wake-word detection system. The semantic layer receives the information processed by the perceptive layer and determines which actions are available for the selected object. The decision is based on object‚Äôs characteristics, contextual information and user vocal feedbacks are exploited to resolve ambiguities. A pilot implementation of the semantic layer is detailed, and qualitative results are shown. The preliminary findings on the validity of the proposed system, as well as on the limitations of a purely vision-based approach, are discussed.},
	language = {en},
	booktitle = {Computers {Helping} {People} with {Special} {Needs}},
	publisher = {Springer International Publishing},
	author = {Pozzi, Luca and Gandolla, Marta and Roveda, Loris},
	editor = {Miesenberger, Klaus and Kouroupetroglou, Georgios and Mavrou, Katerina and Manduchi, Roberto and Covarrubias Rodriguez, Mario and Pen√°z, Petr},
	year = {2022},
	keywords = {/unread, Action detection, Human-Robot Interaction, Pointing, Service robotics, ‚ùå},
	pages = {461--468},
}

@article{scholz_improving_2024,
	title = {Improving robot-to-human communication using flexible display technology as a robotic-skin-interface: a co-design study},
	issn = {2366-598X},
	shorttitle = {Improving robot-to-human communication using flexible display technology as a robotic-skin-interface},
	url = {https://doi.org/10.1007/s41315-024-00343-0},
	doi = {10.1007/s41315-024-00343-0},
	abstract = {In the evolving field of industrial automation, operator awareness of robot actions and intentions is critical for safety and efficiency, especially when working in close proximity to robots. From the robot-to-human communication angle, a collaborative robot (cobot) is expected to express its internal states and monitor task progress. Various traditional communication modalities (e.g., tower light, external screen, LED ring, and sound) often fall short of conveying nuanced information, while a flexible display curved around the cobot arm using organic light-emitting diode (OLED) technology provides a potential advantage. Integrated seamlessly with the robot, this interface enhances interaction by displaying text and video, enriching communication, and positively influencing the human‚Äìrobot collaboration experience. In this work, we investigate a novel integrated flexible OLED display technology used as a robotic skin-interface to improve robot-to-human communication in a real industrial setting at Volkswagen (VW), following a user-centric Double-Diamond co-design process. We first conducted a co-design workshop with six operator representatives to collect their ideas and expectations on how the robot should communicate with them. The gathered information was used to design an interface for a collaborative human-robot interaction task in motor assembly. The interface was implemented in a workcell and validated qualitatively with a small group of operators (n = 9) and quantitatively with a large group (n = 42). The validation results showed that using flexible OLED technology could improve the operators‚Äô attitude toward the robot, increase their intention to use the robot, enhance perceived enjoyment, social influence, and trust, and reduce their anxiety.},
	language = {en},
	urldate = {2024-12-05},
	journal = {International Journal of Intelligent Robotics and Applications},
	author = {Scholz, Constantin and Cao, Hoang-Long and El Makrini, Ilias and Niehaus, Susanne and Kaufmann, Maximilian and Cheyns, David and Roshandel, Nima and Burkiewicz, Aleksander and Shhaitly, Mariane and Imrith, Emil and Genoe, Jan and Rottenberg, Xavier and Gerets, Peter and Vanderborght, Bram},
	month = may,
	year = {2024},
	keywords = {/unread, Artificial Intelligence, Co-design, Double diamond, Flexible displays, Human-robot collaboration, Human‚Äìrobot communication, Human‚Äìrobot communication interfaces, Large-area electronics, OLED, Robotic skin},
}

@inproceedings{wang_lami_2024,
	title = {{LaMI}: {Large} {Language} {Models} for {Multi}-{Modal} {Human}-{Robot} {Interaction}},
	shorttitle = {{LaMI}},
	url = {http://arxiv.org/abs/2401.15174},
	doi = {10.1145/3613905.3651029},
	abstract = {This paper presents an innovative large language model (LLM)-based robotic system for enhancing multi-modal human-robot interaction (HRI). Traditional HRI systems relied on complex designs for intent estimation, reasoning, and behavior generation, which were resource-intensive. In contrast, our system empowers researchers and practitioners to regulate robot behavior through three key aspects: providing high-level linguistic guidance, creating "atomic actions" and expressions the robot can use, and offering a set of examples. Implemented on a physical robot, it demonstrates proficiency in adapting to multi-modal inputs and determining the appropriate manner of action to assist humans with its arms, following researchers' defined guidelines. Simultaneously, it coordinates the robot's lid, neck, and ear movements with speech output to produce dynamic, multi-modal expressions. This showcases the system's potential to revolutionize HRI by shifting from conventional, manual state-and-flow design methods to an intuitive, guidance-based, and example-driven approach. Supplementary material can be found at https://hri-eu.github.io/Lami/},
	urldate = {2024-12-05},
	booktitle = {Extended {Abstracts} of the {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	author = {Wang, Chao and Hasler, Stephan and Tanneberg, Daniel and Ocker, Felix and Joublin, Frank and Ceravola, Antonello and Deigmoeller, Joerg and Gienger, Michael},
	month = may,
	year = {2024},
	note = {arXiv:2401.15174 [cs]},
	keywords = {/unread, Computer Science - Human-Computer Interaction, Computer Science - Robotics},
	pages = {1--10},
}

@misc{noauthor_lami_nodate,
	title = {{LaMI}ÔºöÁî®‰∫éÂ§öÊ®°ÊÄÅ‰∫∫Êú∫‰∫§‰∫íÁöÑÂ§ßËßÑÊ®°ËØ≠Ë®ÄÊ®°Âûã - {Bohrium}},
	url = {https://bohrium.dp.tech/paper/arxiv/2401.15174},
	urldate = {2024-12-05},
	keywords = {/unread},
}

@article{haines_recommendations_2010,
	title = {Recommendations for {End}-{User} {Development}},
	volume = {612},
	abstract = {End-user development (EUD), the practice of users creating, modifying, or extending programs for personal use, is a valuable but often challenging task for nonprogrammers. From the beginning, EUD systems have shown that recommendations can improve the user experience. However, these usability improvements are limited by a reliance on handcrafted rules and heuristics to generate reasonable and useful suggestions. When the number of possible recommendations is large or the available context is too limited for traditional reasoning techniques, recommender technologies present a promising solution. In this paper, we provide an overview of the state of the art in end-user development, focusing on the different kinds of recommendations made to users. We identify four classes of suggestion that could most directly benefit from existing recommendation techniques. Along the way we explore straightforward applications of recommender algorithms as well as a few difficult but high-value recommendation problems in EUD. We discuss the ways that EUD systems have been evaluated in the past and suggest the modifications necessary to evaluate recommenders within the EUD context. We highlight EUD research as one area that can facilitate the transition of recommender system evaluation from algorithmic performance evaluation to a more user-centered approach. We conclude by restating our findings as a new set of research challenges for the recommender systems community.},
	language = {en},
	author = {Haines, Will and Gervasio, Melinda and Spaulding, Aaron and Peintner, Bart},
	year = {2010},
	keywords = {/unread},
}

@misc{jiang_survey_2024-1,
	title = {A {Survey} on {Large} {Language} {Models} for {Code} {Generation}},
	url = {http://arxiv.org/abs/2406.00515},
	doi = {10.48550/arXiv.2406.00515},
	abstract = {Large Language Models (LLMs) have garnered remarkable advancements across diverse code-related tasks, known as Code LLMs, particularly in code generation that generates source code with LLM from natural language descriptions. This burgeoning field has captured significant interest from both academic researchers and industry professionals due to its practical significance in software development, e.g., GitHub Copilot. Despite the active exploration of LLMs for a variety of code tasks, either from the perspective of natural language processing (NLP) or software engineering (SE) or both, there is a noticeable absence of a comprehensive and up-to-date literature review dedicated to LLM for code generation. In this survey, we aim to bridge this gap by providing a systematic literature review that serves as a valuable reference for researchers investigating the cutting-edge progress in LLMs for code generation. We introduce a taxonomy to categorize and discuss the recent developments in LLMs for code generation, covering aspects such as data curation, latest advances, performance evaluation, ethical implications, environmental impact, and real-world applications. In addition, we present a historical overview of the evolution of LLMs for code generation and offer an empirical comparison using the HumanEval, MBPP, and BigCodeBench benchmarks across various levels of difficulty and types of programming tasks to highlight the progressive enhancements in LLM capabilities for code generation. We identify critical challenges and promising opportunities regarding the gap between academia and practical development. Furthermore, we have established a dedicated resource GitHub page (https://github.com/juyongjiang/CodeLLMSurvey) to continuously document and disseminate the most recent advances in the field.},
	language = {en-US},
	urldate = {2024-12-05},
	publisher = {arXiv},
	author = {Jiang, Juyong and Wang, Fan and Shen, Jiasi and Kim, Sungju and Kim, Sunghun},
	month = nov,
	year = {2024},
	note = {arXiv:2406.00515 [cs]},
	keywords = {/unread, Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Software Engineering, ‚ùå},
}

@misc{zhang_mm-llms_2024,
	title = {{MM}-{LLMs}: {Recent} {Advances} in {MultiModal} {Large} {Language} {Models}},
	shorttitle = {{MM}-{LLMs}},
	url = {http://arxiv.org/abs/2401.13601},
	doi = {10.48550/arXiv.2401.13601},
	abstract = {In the past year, MultiModal Large Language Models (MM-LLMs) have undergone substantial advancements, augmenting off-the-shelf LLMs to support MM inputs or outputs via cost-effective training strategies. The resulting models not only preserve the inherent reasoning and decision-making capabilities of LLMs but also empower a diverse range of MM tasks. In this paper, we provide a comprehensive survey aimed at facilitating further research of MM-LLMs. Initially, we outline general design formulations for model architecture and training pipeline. Subsequently, we introduce a taxonomy encompassing 126 MM-LLMs, each characterized by its specific formulations. Furthermore, we review the performance of selected MM-LLMs on mainstream benchmarks and summarize key training recipes to enhance the potency of MM-LLMs. Finally, we explore promising directions for MM-LLMs while concurrently maintaining a real-time tracking website for the latest developments in the field. We hope that this survey contributes to the ongoing advancement of the MM-LLMs domain.},
	language = {en-US},
	urldate = {2024-12-05},
	publisher = {arXiv},
	author = {Zhang, Duzhen and Yu, Yahan and Dong, Jiahua and Li, Chenxing and Su, Dan and Chu, Chenhui and Yu, Dong},
	month = may,
	year = {2024},
	note = {arXiv:2401.13601 [cs]},
	keywords = {/unread, Computer Science - Computation and Language, ‚ùå},
}

@misc{feng_designing_2023,
	title = {Designing with {Language}: {Wireframing} {UI} {Design} {Intent} with {Generative} {Large} {Language} {Models}},
	shorttitle = {Designing with {Language}},
	url = {http://arxiv.org/abs/2312.07755},
	doi = {10.48550/arXiv.2312.07755},
	abstract = {Wireframing is a critical step in the UI design process. Mid-fidelity wireframes offer more impactful and engaging visuals compared to low-fidelity versions. However, their creation can be time-consuming and labor-intensive, requiring the addition of actual content and semantic icons. In this paper, we introduce a novel solution WireGen, to automatically generate mid-fidelity wireframes with just a brief design intent description using the generative Large Language Models (LLMs). Our experiments demonstrate the effectiveness of WireGen in producing 77.5\% significantly better wireframes, outperforming two widely-used in-context learning baselines. A user study with 5 designers further validates its real-world usefulness, highlighting its potential value to enhance UI design process.},
	urldate = {2024-12-05},
	publisher = {arXiv},
	author = {Feng, Sidong and Yuan, Mingyue and Chen, Jieshan and Xing, Zhenchang and Chen, Chunyang},
	month = dec,
	year = {2023},
	note = {arXiv:2312.07755 [cs]},
	keywords = {/unread, Computer Science - Human-Computer Interaction, ccfInfo: CCF-None CORR, citationNumber: 0},
}

@article{do_grounding_2024,
	title = {Grounding with {Structure}: {Exploring} {Design} {Variations} of {Grounded} {Human}-{AI} {Collaboration} in a {Natural} {Language} {Interface}},
	volume = {8},
	shorttitle = {Grounding with {Structure}},
	url = {https://dl.acm.org/doi/10.1145/3686902},
	doi = {10.1145/3686902},
	abstract = {Selecting an effective utterance among countless possibilities that match a user's intention poses a challenge when using natural language interfaces. To address the challenge, we leveraged the principle of least collaborative effort in communication grounding theory and designed three grounded conversational interactions: 1) a grounding interface allows users to start with a provisional input and then invite a conversational agent to complete their input, 2) a multiple grounding interface presents multiple inputs for the user to select from, and 3) a structured grounding interface guides users to write inputs in a structure best understood by the system. We compared our three grounding interfaces to an ungrounded control interface in a crowdsourced study (N=80) using a natural language system that generates small programs. We found that the grounding interfaces reduced cognitive load and improved task performance. The structured grounding interface further reduced speaker change costs and improved technology acceptance, without sacrificing the perception of control. We discuss the implications of designing grounded conversational interactions in natural language systems.},
	number = {CSCW2},
	urldate = {2024-12-05},
	journal = {Proc. ACM Hum.-Comput. Interact.},
	author = {Do, Hyo Jin and Brachman, Michelle and Dugan, Casey and Johnson, James M. and Lauer, Julia and Rai, Priyanshu and Pan, Qian},
	month = nov,
	year = {2024},
	keywords = {/unread, ccfInfo: CCF-C PACMHCI, citationNumber: 0},
	pages = {363:1--363:27},
}

@misc{wu_uicoder_2024,
	title = {{UICoder}: {Finetuning} {Large} {Language} {Models} to {Generate} {User} {Interface} {Code} through {Automated} {Feedback}},
	shorttitle = {{UICoder}},
	url = {http://arxiv.org/abs/2406.07739},
	doi = {10.48550/arXiv.2406.07739},
	abstract = {Large language models (LLMs) struggle to consistently generate UI code that compiles and produces visually relevant designs. Existing approaches to improve generation rely on expensive human feedback or distilling a proprietary model. In this paper, we explore the use of automated feedback (compilers and multi-modal models) to guide LLMs to generate high-quality UI code. Our method starts with an existing LLM and iteratively produces improved models by self-generating a large synthetic dataset using an original model, applying automated tools to aggressively filter, score, and de-duplicate the data into a refined higher quality dataset. The original LLM is improved by finetuning on this refined dataset. We applied our approach to several open-source LLMs and compared the resulting performance to baseline models with both automated metrics and human preferences. Our evaluation shows the resulting models outperform all other downloadable baselines and approach the performance of larger proprietary models.},
	urldate = {2024-12-05},
	publisher = {arXiv},
	author = {Wu, Jason and Schoop, Eldon and Leung, Alan and Barik, Titus and Bigham, Jeffrey P. and Nichols, Jeffrey},
	month = jun,
	year = {2024},
	note = {arXiv:2406.07739 [cs]},
	keywords = {/unread, Computer Science - Computation and Language, Computer Science - Human-Computer Interaction, Computer Science - Software Engineering, ccfInfo: CCF-B NAACL, citationNumber: 0},
}

@misc{cherakara_furchat_2023,
	title = {{FurChat}: {An} {Embodied} {Conversational} {Agent} using {LLMs}, {Combining} {Open} and {Closed}-{Domain} {Dialogue} with {Facial} {Expressions}},
	shorttitle = {{FurChat}},
	url = {http://arxiv.org/abs/2308.15214},
	doi = {10.48550/arXiv.2308.15214},
	abstract = {We demonstrate an embodied conversational agent that can function as a receptionist and generate a mixture of open and closed-domain dialogue along with facial expressions, by using a large language model (LLM) to develop an engaging conversation. We deployed the system onto a Furhat robot, which is highly expressive and capable of using both verbal and nonverbal cues during interaction. The system was designed specifically for the National Robotarium to interact with visitors through natural conversations, providing them with information about the facilities, research, news, upcoming events, etc. The system utilises the state-of-the-art GPT-3.5 model to generate such information along with domain-general conversations and facial expressions based on prompt engineering.},
	language = {en-US},
	urldate = {2024-12-04},
	publisher = {arXiv},
	author = {Cherakara, Neeraj and Varghese, Finny and Shabana, Sheena and Nelson, Nivan and Karukayil, Abhiram and Kulothungan, Rohith and Farhan, Mohammed Afil and Nesset, Birthe and Moujahid, Meriam and Dinkar, Tanvi and Rieser, Verena and Lemon, Oliver},
	month = aug,
	year = {2023},
	note = {arXiv:2308.15214 [cs]},
	keywords = {/unread, Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Human-Computer Interaction, Computer Science - Robotics, ccfInfo: CCF-None SIGDIAL, citationNumber: 2},
}

@misc{atuhurra_leveraging_2024,
	title = {Leveraging {Large} {Language} {Models} in {Human}-{Robot} {Interaction}: {A} {Critical} {Analysis} of {Potential} and {Pitfalls}},
	shorttitle = {Leveraging {Large} {Language} {Models} in {Human}-{Robot} {Interaction}},
	url = {http://arxiv.org/abs/2405.00693},
	doi = {10.48550/arXiv.2405.00693},
	abstract = {The emergence of large language models (LLM) and, consequently, vision language models (VLM) has ignited new imaginations among robotics researchers. At this point, the range of applications to which LLM and VLM can be applied in human-robot interaction (HRI), particularly socially assistive robots (SARs), is unchartered territory. However, LLM and VLM present unprecedented opportunities and challenges for SAR integration. We aim to illuminate the opportunities and challenges when roboticists deploy LLM and VLM in SARs. First, we conducted a meta-study of more than 250 papers exploring 1) major robots in HRI research and 2) significant applications of SARs, emphasizing education, healthcare, and entertainment while addressing 3) societal norms and issues like trust, bias, and ethics that the robot developers must address. Then, we identified 4) critical components of a robot that LLM or VLM can replace while addressing the 5) benefits of integrating LLM into robot designs and the 6) risks involved. Finally, we outline a pathway for the responsible and effective adoption of LLM or VLM into SARs, and we close our discussion by offering caution regarding this deployment.},
	urldate = {2024-12-05},
	publisher = {arXiv},
	author = {Atuhurra, Jesse},
	month = nov,
	year = {2024},
	note = {arXiv:2405.00693 [cs]},
	keywords = {/unread, Computer Science - Computation and Language, Computer Science - Robotics, ccfInfo: Not Found, citationNumber: 0},
}

@inproceedings{padmanabha_voicepilot_2024,
	address = {New York, NY, USA},
	series = {{UIST} '24},
	title = {{VoicePilot}: {Harnessing} {LLMs} as {Speech} {Interfaces} for {Physically} {Assistive} {Robots}},
	isbn = {979-8-4007-0628-8},
	shorttitle = {{VoicePilot}},
	url = {https://dl.acm.org/doi/10.1145/3654777.3676401},
	doi = {10.1145/3654777.3676401},
	abstract = {Physically assistive robots present an opportunity to significantly increase the well-being and independence of individuals with motor impairments or other forms of disability who are unable to complete activities of daily living. Speech interfaces, especially ones that utilize Large Language Models (LLMs), can enable individuals to effectively and naturally communicate high-level commands and nuanced preferences to robots. Frameworks for integrating LLMs as interfaces to robots for high level task planning and code generation have been proposed, but fail to incorporate human-centric considerations which are essential while developing assistive interfaces. In this work, we present a framework for incorporating LLMs as speech interfaces for physically assistive robots, constructed iteratively with 3 stages of testing involving a feeding robot, culminating in an evaluation with 11 older adults at an independent living facility. We use both quantitative and qualitative data from the final study to validate our framework and additionally provide design guidelines for using LLMs as speech interfaces for assistive robots. Videos, code, and supporting files are located on our project website1},
	urldate = {2024-12-04},
	booktitle = {Proceedings of the 37th {Annual} {ACM} {Symposium} on {User} {Interface} {Software} and {Technology}},
	publisher = {Association for Computing Machinery},
	author = {Padmanabha, Akhil and Yuan, Jessie and Gupta, Janavi and Karachiwalla, Zulekha and Majidi, Carmel and Admoni, Henny and Erickson, Zackory},
	month = oct,
	year = {2024},
	keywords = {/unread, ccfInfo: CCF-A UIST, citationNumber: 0},
	pages = {1--18},
}

@misc{yi_survey_2024,
	title = {A {Survey} on {Recent} {Advances} in {LLM}-{Based} {Multi}-turn {Dialogue} {Systems}},
	url = {http://arxiv.org/abs/2402.18013},
	doi = {10.48550/arXiv.2402.18013},
	abstract = {This survey provides a comprehensive review of research on multi-turn dialogue systems, with a particular focus on multi-turn dialogue systems based on large language models (LLMs). This paper aims to (a) give a summary of existing LLMs and approaches for adapting LLMs to downstream tasks; (b) elaborate recent advances in multi-turn dialogue systems, covering both LLM-based open-domain dialogue (ODD) and task-oriented dialogue (TOD) systems, along with datasets and evaluation metrics; (c) discuss some future emphasis and recent research problems arising from the development of LLMs and the increasing demands on multi-turn dialogue systems.},
	urldate = {2024-12-05},
	publisher = {arXiv},
	author = {Yi, Zihao and Ouyang, Jiarui and Liu, Yuwen and Liao, Tianhao and Xu, Zhe and Shen, Ying},
	month = feb,
	year = {2024},
	note = {arXiv:2402.18013 [cs]},
	keywords = {/unread, Computer Science - Artificial Intelligence, Computer Science - Computation and Language, ccfInfo: CCF-None CORR, citationNumber: 0},
}

@misc{zhao_survey_2024,
	title = {A {Survey} of {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2303.18223},
	doi = {10.48550/arXiv.2303.18223},
	abstract = {Language is essentially a complex, intricate system of human expressions governed by grammatical rules. It poses a significant challenge to develop capable AI algorithms for comprehending and grasping a language. As a major approach, language modeling has been widely studied for language understanding and generation in the past two decades, evolving from statistical language models to neural language models. Recently, pre-trained language models (PLMs) have been proposed by pre-training Transformer models over large-scale corpora, showing strong capabilities in solving various NLP tasks. Since researchers have found that model scaling can lead to performance improvement, they further study the scaling effect by increasing the model size to an even larger size. Interestingly, when the parameter scale exceeds a certain level, these enlarged language models not only achieve a significant performance improvement but also show some special abilities that are not present in small-scale language models. To discriminate the difference in parameter scale, the research community has coined the term large language models (LLM) for the PLMs of significant size. Recently, the research on LLMs has been largely advanced by both academia and industry, and a remarkable progress is the launch of ChatGPT, which has attracted widespread attention from society. The technical evolution of LLMs has been making an important impact on the entire AI community, which would revolutionize the way how we develop and use AI algorithms. In this survey, we review the recent advances of LLMs by introducing the background, key findings, and mainstream techniques. In particular, we focus on four major aspects of LLMs, namely pre-training, adaptation tuning, utilization, and capacity evaluation. Besides, we also summarize the available resources for developing LLMs and discuss the remaining issues for future directions.},
	urldate = {2024-12-05},
	publisher = {arXiv},
	author = {Zhao, Wayne Xin and Zhou, Kun and Li, Junyi and Tang, Tianyi and Wang, Xiaolei and Hou, Yupeng and Min, Yingqian and Zhang, Beichen and Zhang, Junjie and Dong, Zican and Du, Yifan and Yang, Chen and Chen, Yushuo and Chen, Zhipeng and Jiang, Jinhao and Ren, Ruiyang and Li, Yifan and Tang, Xinyu and Liu, Zikang and Liu, Peiyu and Nie, Jian-Yun and Wen, Ji-Rong},
	month = oct,
	year = {2024},
	note = {arXiv:2303.18223 [cs]},
	keywords = {/unread, Computer Science - Artificial Intelligence, Computer Science - Computation and Language, ccfInfo: CCF-None CORR},
}

@misc{atuhurra_leveraging_2024-1,
	title = {Leveraging {Large} {Language} {Models} in {Human}-{Robot} {Interaction}: {A} {Critical} {Analysis} of {Potential} and {Pitfalls}},
	shorttitle = {Leveraging {Large} {Language} {Models} in {Human}-{Robot} {Interaction}},
	url = {http://arxiv.org/abs/2405.00693},
	doi = {10.48550/arXiv.2405.00693},
	abstract = {The emergence of large language models (LLM) and, consequently, vision language models (VLM) has ignited new imaginations among robotics researchers. At this point, the range of applications to which LLM and VLM can be applied in human-robot interaction (HRI), particularly socially assistive robots (SARs), is unchartered territory. However, LLM and VLM present unprecedented opportunities and challenges for SAR integration. We aim to illuminate the opportunities and challenges when roboticists deploy LLM and VLM in SARs. First, we conducted a meta-study of more than 250 papers exploring 1) major robots in HRI research and 2) significant applications of SARs, emphasizing education, healthcare, and entertainment while addressing 3) societal norms and issues like trust, bias, and ethics that the robot developers must address. Then, we identified 4) critical components of a robot that LLM or VLM can replace while addressing the 5) benefits of integrating LLM into robot designs and the 6) risks involved. Finally, we outline a pathway for the responsible and effective adoption of LLM or VLM into SARs, and we close our discussion by offering caution regarding this deployment.},
	urldate = {2024-12-04},
	publisher = {arXiv},
	author = {Atuhurra, Jesse},
	month = nov,
	year = {2024},
	note = {arXiv:2405.00693 [cs]},
	keywords = {/unread, Computer Science - Computation and Language, Computer Science - Robotics, ccfInfo: Not Found, citationNumber: 0},
}

@inproceedings{kim_stylette_2022,
	address = {New York, NY, USA},
	series = {{CHI} '22},
	title = {Stylette: {Styling} the {Web} with {Natural} {Language}},
	isbn = {978-1-4503-9157-3},
	shorttitle = {Stylette},
	url = {https://dl.acm.org/doi/10.1145/3491102.3501931},
	doi = {10.1145/3491102.3501931},
	abstract = {End-users can potentially style and customize websites by editing them through in-browser developer tools. Unfortunately, end-users lack the knowledge needed to translate high-level styling goals into low-level code edits. We present Stylette, a browser extension that enables users to change the style of websites by expressing goals in natural language. By interpreting the user‚Äôs goal with a large language model and extracting suggestions from our dataset of 1.7 million web components, Stylette generates a palette of CSS properties and values that the user can apply to reach their goal. A comparative study (N=40) showed that Stylette lowered the learning curve, helping participants perform styling changes 35\% faster than those using developer tools. By presenting various alternatives for a single goal, the tool helped participants familiarize themselves with CSS through experimentation. Beyond CSS, our work can be expanded to help novices quickly grasp complex software or programming languages.},
	urldate = {2024-12-04},
	booktitle = {Proceedings of the 2022 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Kim, Tae Soo and Choi, DaEun and Choi, Yoonseo and Kim, Juho},
	month = apr,
	year = {2022},
	keywords = {/unread, ccfInfo: CCF-A CHI, citationNumber: 28},
	pages = {1--17},
}

@inproceedings{zouhaier_reinforcement_2021,
	title = {A {Reinforcement} {Learning} {Based} {Approach} of {Context}-driven {Adaptive} {User} {Interfaces}},
	url = {https://ieeexplore.ieee.org/abstract/document/9529780},
	doi = {10.1109/COMPSAC51774.2021.00217},
	abstract = {Adaptive User Interfaces (AUI) have to exploit Artificial Intelligence powerful to adapt User Interfaces (UI)s to People With Disabilities (PWD). Thus, Machine Learning methods could master disabilities problems and barriers. In this paper, we propose a Reinforcement Learning (RL) based approach for resolving and surmounting PWD-UI interactions barriers since RL is good for learning good behavior. Hence, we have called the approach as RL-AUIAC as Reinforcement Learning of Adaptive User Interfaces for Accessibility Context. RL-AUIAC is based on three Knowledge Layers (KL)s depending on the kind of adaptation and the resolved problem at each layer. KL uses the Exploration-Exploitation dilemma to respond to the question: what to learn from each planned-adaptation sequences? In fact, Disability Knowledge Layer (DKL)learns UI structure behavior depending on the disability profile. Modality Knowledge Layer(MKL) learns facilities of adaptation on the basis of the couple . Platform Knowledge Layer (PKL) explores-exploits platform-knowledge to learn adaptation facilities on the basis of .},
	urldate = {2024-12-04},
	booktitle = {2021 {IEEE} 45th {Annual} {Computers}, {Software}, and {Applications} {Conference} ({COMPSAC})},
	author = {Zouhaier, Lamia and Hlaoui, Yousra Ben Daly and Ayed, Leila Ben},
	month = jul,
	year = {2021},
	note = {ISSN: 0730-3157},
	keywords = {/unread, Conferences, Prototypes, Reinforcement learning, Software, User interfaces, ccfInfo: CCF-C COMPSAC, citationNumber: 4},
	pages = {1463--1468},
}

@inproceedings{kim_understanding_2024,
	address = {New York, NY, USA},
	series = {{HRI} '24},
	title = {Understanding {Large}-{Language} {Model} ({LLM})-powered {Human}-{Robot} {Interaction}},
	isbn = {979-8-4007-0322-5},
	url = {https://dl.acm.org/doi/10.1145/3610977.3634966},
	doi = {10.1145/3610977.3634966},
	abstract = {Large-language models (LLMs) hold significant promise in improving human-robot interaction, offering advanced conversational skills and versatility in managing diverse, open-ended user requests in various tasks and domains. Despite the potential to transform human-robot interaction, very little is known about the distinctive design requirements for utilizing LLMs in robots, which may differ from text and voice interaction and vary by task and context. To better understand these requirements, we conducted a user study (n = 32) comparing an LLM-powered social robot against text- and voice-based agents, analyzing task-based requirements in conversational tasks, including choose, generate, execute, and negotiate. Our findings show that LLM-powered robots elevate expectations for sophisticated non-verbal cues and excel in connection-building and deliberation, but fall short in logical communication and may induce anxiety. We provide design implications both for robots integrating LLMs and for fine-tuning LLMs for use with robots.},
	urldate = {2024-12-04},
	booktitle = {Proceedings of the 2024 {ACM}/{IEEE} {International} {Conference} on {Human}-{Robot} {Interaction}},
	publisher = {Association for Computing Machinery},
	author = {Kim, Callie Y. and Lee, Christine P. and Mutlu, Bilge},
	month = mar,
	year = {2024},
	keywords = {/unread, ccfInfo: CCF-None HRI, citationNumber: 0},
	pages = {371--380},
}

@misc{dong_survey_2023,
	title = {A {Survey} on {In}-context {Learning}},
	url = {http://arxiv.org/abs/2301.00234},
	doi = {10.48550/arXiv.2301.00234},
	abstract = {With the increasing ability of large language models (LLMs), in-context learning (ICL) has become a new paradigm for natural language processing (NLP), where LLMs make predictions only based on contexts augmented with a few examples. It has been a new trend to explore ICL to evaluate and extrapolate the ability of LLMs. In this paper, we aim to survey and summarize the progress and challenges of ICL. We first present a formal definition of ICL and clarify its correlation to related studies. Then, we organize and discuss advanced techniques, including training strategies, demonstration designing strategies, as well as related analysis. Finally, we discuss the challenges of ICL and provide potential directions for further research. We hope that our work can encourage more research on uncovering how ICL works and improving ICL.},
	urldate = {2023-09-28},
	publisher = {arXiv},
	author = {Dong, Qingxiu and Li, Lei and Dai, Damai and Zheng, Ce and Wu, Zhiyong and Chang, Baobao and Sun, Xu and Xu, Jingjing and Li, Lei and Sui, Zhifang},
	month = jun,
	year = {2023},
	note = {Issue: arXiv:2301.00234
arXiv:2301.00234 [cs]},
	keywords = {/unread, Computer Science - Artificial Intelligence, Computer Science - Computation and Language, ccfInfo: CCF-B EMNLP, citationNumber: 1126},
}

@misc{wei_emergent_2022,
	title = {Emergent {Abilities} of {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2206.07682},
	doi = {10.48550/arXiv.2206.07682},
	abstract = {Scaling up language models has been shown to predictably improve performance and sample efficiency on a wide range of downstream tasks. This paper instead discusses an unpredictable phenomenon that we refer to as emergent abilities of large language models. We consider an ability to be emergent if it is not present in smaller models but is present in larger models. Thus, emergent abilities cannot be predicted simply by extrapolating the performance of smaller models. The existence of such emergence implies that additional scaling could further expand the range of capabilities of language models.},
	language = {en},
	urldate = {2023-02-13},
	publisher = {arXiv},
	author = {Wei, Jason and Tay, Yi and Bommasani, Rishi and Raffel, Colin and Zoph, Barret and Borgeaud, Sebastian and Yogatama, Dani and Bosma, Maarten and Zhou, Denny and Metzler, Donald and Chi, Ed H. and Hashimoto, Tatsunori and Vinyals, Oriol and Liang, Percy and Dean, Jeff and Fedus, William},
	month = oct,
	year = {2022},
	note = {arXiv:2206.07682 [cs]},
	keywords = {/unread, Computer Science - Computation and Language, ccfInfo: CCF-None TMLR, citationNumber: 2342},
}

@book{marzano_dimensions_1989,
	address = {Alexandria, Va},
	title = {Dimensions of thinking: a framework for curriculum and instruction},
	isbn = {978-0-87120-148-5},
	shorttitle = {Dimensions of thinking},
	language = {en},
	publisher = {ASCD},
	editor = {Marzano, Robert J. and {Association for Supervision and Curriculum Development}},
	year = {1989},
	keywords = {/unread, ccfInfo: Not Found, citationNumber: 246},
}

@article{kunold_framework_2022,
	title = {A {Framework} to {Study} and {Design} {Communication} with {Social} {Robots}},
	volume = {11},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2218-6581},
	url = {https://www.mdpi.com/2218-6581/11/6/129},
	doi = {10.3390/robotics11060129},
	abstract = {Communication is a central component in social human‚Äìrobot interaction that needs to be planned and designed prior to the actual communicative act. We therefore propose a pragmatic, linear view of communication design for social robots that corresponds to a sender‚Äìreceiver perspective. Our framework is based on Lasswell‚Äôs 5Ws of mass communication: Who, says what, in which channel, to whom, with what effect. We extend and adapt this model to communication in HRI. In addition, we point out that, besides the predefined communicative acts of a robot, other characteristics, such as a robot‚Äôs morphology, can also have an impact on humans, since humans tend to assign meaning to every cue in robots‚Äô behavior and appearance. We illustrate the application of the extended framework to three different studies on human‚Äìrobot communication to demonstrate the incremental value as it supports a systematic evaluation and the identification of similarities, differences, and research gaps. The framework therefore offers the opportunity for meta-analyses of existing research and additionally draws the path for future robust research designs for studying human‚Äìrobot communication.},
	language = {en},
	number = {6},
	urldate = {2024-12-03},
	journal = {Robotics},
	author = {Kunold, Laura and Onnasch, Linda},
	month = dec,
	year = {2022},
	note = {Number: 6
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {/unread, ccfInfo: CCF-None ROBOTICS, citationNumber: 2, communication, human-robot interaction, social robots, transmission model},
	pages = {129},
}

@incollection{lieberman_end-user_2006,
	address = {Dordrecht},
	series = {Human-{Computer} {Interaction} {Series}},
	title = {End-{User} {Development}: {An} {Emerging} {Paradigm}},
	isbn = {978-1-4020-5386-3},
	shorttitle = {End-{User} {Development}},
	url = {https://doi.org/10.1007/1-4020-5386-X_1},
	abstract = {We think that over the next few years, the goal of interactive systems and services will evolve from just making systems easy to use (even though that goal has not yet been completely achieved) to making systems that are easy to develop by end users. By now, most people have become familiar with the basic functionality and interfaces of computers, but they are not able to manage any programming language. Therefore, they cannot develop new applications or modify current ones according to their needs.},
	language = {en},
	urldate = {2023-09-25},
	booktitle = {End {User} {Development}},
	publisher = {Springer Netherlands},
	author = {Lieberman, Henry and Patern√≤, Fabio and Klann, Markus and Wulf, Volker},
	editor = {Lieberman, Henry and Patern√≤, Fabio and Wulf, Volker},
	year = {2006},
	doi = {10.1007/1-4020-5386-X_1},
	keywords = {/unread, Agile Software Development, Computer Support Cooperative Work, Human Centric Computing, Software Cost Estimation, Software Professional, ccfInfo: CCF-None HCI, citationNumber: 1202},
	pages = {1--8},
}

@inproceedings{mirnig_screen_2014,
	address = {Edinburgh, UK},
	title = {Screen feedback in human-robot interaction: {How} to enhance robot expressiveness},
	isbn = {978-1-4799-6765-0 978-1-4799-6763-6},
	shorttitle = {Screen feedback in human-robot interaction},
	url = {http://ieeexplore.ieee.org/document/6926257/},
	doi = {10.1109/ROMAN.2014.6926257},
	abstract = {The feedback of a robot is a powerful means to establish smooth human-robot interaction (HRI). We report on a user study to assess the applicability of a screen in a human-robot game-playing scenario. The screen was deployed to compensate for expressive shortcomings of a social robot due to its mechanical limitations of non-movable facial features. The participants played Rock-Paper-Scissors with the robot for which we programmed the right hand to show gestures. Half of the participants received facial expressions via a screen during the game, whereas the other half did not get screen feedback. We annotated the video-recorded interactions and collected questionnaire and interview data to assess the applicability of the screen. With our data we could show noteworthy results: First, the robot which provided facial expressions was rated more intelligent. Second, the participants who had interacted with the robot showing facial expressions, rated the task as more attractive than participants who did not receive facial expressions from the robot. The fact that we could not detect a negative inÔ¨Çuence of the screen, underlines the applicability of a screen in an HRI game-playing scenario to display facial expressions and contribute to an enhanced interaction.},
	language = {en},
	urldate = {2024-11-28},
	booktitle = {The 23rd {IEEE} {International} {Symposium} on {Robot} and {Human} {Interactive} {Communication}},
	publisher = {IEEE},
	author = {Mirnig, Nicole and Tan, Yeow Kee and Chang, Tai Wen and Chua, Yuan Wei and Dung, Tran Anh and Li, Haizhou and Tscheligi, Manfred},
	month = aug,
	year = {2014},
	keywords = {/unread, ccfInfo: CCF-None RO-MAN, citationNumber: 5},
	pages = {224--230},
}

@misc{lee_paperweaver_2024,
	title = {{PaperWeaver}: {Enriching} {Topical} {Paper} {Alerts} by {Contextualizing} {Recommended} {Papers} with {User}-collected {Papers}},
	shorttitle = {{PaperWeaver}},
	url = {https://arxiv.org/abs/2403.02939v2},
	abstract = {With the rapid growth of scholarly archives, researchers subscribe to "paper alert" systems that periodically provide them with recommendations of recently published papers that are similar to previously collected papers. However, researchers sometimes struggle to make sense of nuanced connections between recommended papers and their own research context, as existing systems only present paper titles and abstracts. To help researchers spot these connections, we present PaperWeaver, an enriched paper alerts system that provides contextualized text descriptions of recommended papers based on user-collected papers. PaperWeaver employs a computational method based on Large Language Models (LLMs) to infer users' research interests from their collected papers, extract context-specific aspects of papers, and compare recommended and collected papers on these aspects. Our user study (N=15) showed that participants using PaperWeaver were able to better understand the relevance of recommended papers and triage them more confidently when compared to a baseline that presented the related work sections from recommended papers.},
	language = {zh-CN},
	urldate = {2024-11-28},
	journal = {arXiv.org},
	author = {Lee, Yoonjoo and Kang, Hyeonsu B. and Latzke, Matt and Kim, Juho and Bragg, Jonathan and Chang, Joseph Chee and Siangliulue, Pao},
	month = mar,
	year = {2024},
	doi = {10.1145/3613904.3642196},
	keywords = {/unread, ccfInfo: CCF-A CHI, citationNumber: 0},
}

@incollection{jurin_using_2010,
	address = {Dordrecht},
	title = {Using {Visual} {Aids}},
	isbn = {978-90-481-3987-3},
	url = {https://doi.org/10.1007/978-90-481-3987-3_15},
	abstract = {This chapter deals with visual aids used to enhance oral presentations. It is intended to guide planning and presentation of visual aids; actual construction of visual aids is covered in other texts, some of which are recommended at the end of the chapter. What sets off a visual aid from a visual message is how is it applied and used. Visual aids, as they are explained here, are meant to enhance a personal presentation, while a visual message is meant to present a stand-alone message in the absence of a presenter (e.g. a park interpretation sign or an advertisement). The old adage is that a picture is worth a thousand words, but it is important to understand that this valued picture be the right picture, presented well, at the right time, or else it will detract from the presentation rather than support it. Visual aids are important because of differences in the way people perceive messages. Some audience members will react best to verbal information, some to visual, and some to written material. Thus presenting messages in multiple formats broadens the audience impact. People tend to remember more of what they see and hear. So visual aids supporting verbal information act to dramatically increase the impact of a presentation. Furthermore, visual aids increase an audience‚Äôs attention span, and can, when used properly, add structure to information to facilitate understanding.},
	language = {en},
	urldate = {2024-11-27},
	booktitle = {Environmental {Communication}. {Second} {Edition}: {Skills} and {Principles} for {Natural} {Resource} {Managers}, {Scientists}, and {Engineers}.},
	publisher = {Springer Netherlands},
	author = {Jurin, Richard R. and Roush, Donny and Danter, Jeff},
	editor = {Jurin, Richard R. and Roush, Donny and Danter, K. Jeffrey},
	year = {2010},
	doi = {10.1007/978-90-481-3987-3_15},
	keywords = {/unread, Electric technologies, ccfInfo: Not Found, handouts, hands-on visuals, overheads, powerpoint, visual aids, write-on boards},
	pages = {231--245},
}

@inproceedings{gibson_engineering_1991,
	address = {Orlando, FL, USA},
	title = {Engineering visual aids to enhance oral and written communications},
	volume = {1 \& 2},
	isbn = {978-0-7803-0482-6},
	url = {http://ieeexplore.ieee.org/document/172769/},
	doi = {10.1109/IPCC.1991.172769},
	urldate = {2024-11-27},
	booktitle = {{IPCC} 91 {Proceedings} {The} {Engineered} {Communication}},
	publisher = {IEEE},
	author = {Gibson, J.W. and Hodgetts, R.M. and Blackwell, C.W.},
	year = {1991},
	keywords = {/unread, ccfInfo: Not Found, citationNumber: 0},
	pages = {194--198},
}

@inproceedings{an_emowear_2024,
	title = {{EmoWear}: {Exploring} {Emotional} {Teasers} for {Voice} {Message} {Interaction} on {Smartwatches}},
	shorttitle = {{EmoWear}},
	url = {http://arxiv.org/abs/2402.07174},
	doi = {10.1145/3613904.3642101},
	abstract = {Voice messages, by nature, prevent users from gauging the emotional tone without fully diving into the audio content. This hinders the shared emotional experience at the pre-retrieval stage. Research scarcely explored "Emotional Teasers"-pre-retrieval cues offering a glimpse into an awaiting message's emotional tone without disclosing its content. We introduce EmoWear, a smartwatch voice messaging system enabling users to apply 30 animation teasers on message bubbles to reflect emotions. EmoWear eases senders' choice by prioritizing emotions based on semantic and acoustic processing. EmoWear was evaluated in comparison with a mirroring system using color-coded message bubbles as emotional cues (N=24). Results showed EmoWear significantly enhanced emotional communication experience in both receiving and sending messages. The animated teasers were considered intuitive and valued for diverse expressions. Desirable interaction qualities and practical implications are distilled for future design. We thereby contribute both a novel system and empirical knowledge concerning emotional teasers for voice messaging.},
	language = {en-US},
	urldate = {2024-07-25},
	booktitle = {Proceedings of the {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	author = {An, Pengcheng and Zhu, Jiawen and Zhang, Zibo and Yin, Yifei and Ma, Qingyuan and Yan, Che and Du, Linghao and Zhao, Jian},
	month = may,
	year = {2024},
	note = {arXiv:2402.07174 [cs]
TLDR: EmoWear, a smartwatch voice messaging system enabling users to apply 30 animation teasers on message bubbles to reflect emotions eases senders‚Äô choice by prioritizing emotions based on semantic and acoustic processing, is introduced.},
	keywords = {\#EX: ‰∏çÂåÖÂê´HRI, \#EX:‰∏çÂåÖÂê´Â§ßÊ®°Âûã, \#EX:Â∑•‰∏öÊú∫Âô®‰∫∫, \#‰∏çÁ°ÆÂÆö, \#Áü≠Êñá, \#ÁªºËø∞, Computer Science - Artificial Intelligence, Computer Science - Human-Computer Interaction, Design, ReadList, Users, ccfInfo: CCF-A CHI, citationNumber: 0, ‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è},
	pages = {1--16},
}

@misc{jiang_graphologue_2023,
	title = {Graphologue: {Exploring} {Large} {Language} {Model} {Responses} with {Interactive} {Diagrams}},
	shorttitle = {Graphologue},
	url = {http://arxiv.org/abs/2305.11473},
	doi = {10.1145/3586183.3606737},
	abstract = {Large language models (LLMs) have recently soared in popularity due to their ease of access and the unprecedented ability to synthesize text responses to diverse user questions. However, LLMs like ChatGPT present significant limitations in supporting complex information tasks due to the insufficient affordances of the text-based medium and linear conversational structure. Through a formative study with ten participants, we found that LLM interfaces often present long-winded responses, making it difficult for people to quickly comprehend and interact flexibly with various pieces of information, particularly during more complex tasks. We present Graphologue, an interactive system that converts text-based responses from LLMs into graphical diagrams to facilitate information-seeking and question-answering tasks. Graphologue employs novel prompting strategies and interface designs to extract entities and relationships from LLM responses and constructs node-link diagrams in real-time. Further, users can interact with the diagrams to flexibly adjust the graphical presentation and to submit context-specific prompts to obtain more information. Utilizing diagrams, Graphologue enables graphical, non-linear dialogues between humans and LLMs, facilitating information exploration, organization, and comprehension.},
	language = {en-US},
	urldate = {2023-09-01},
	author = {Jiang, Peiling and Rayan, Jude and Dow, Steven P. and Xia, Haijun},
	month = aug,
	year = {2023},
	note = {arXiv:2305.11473 [cs]
titleTranslation: ÂõæË°®Â≠¶ÂÆ∂ÔºöÈÄöËøá‰∫§‰∫íÂºèÂõæË°®Êé¢Á¥¢Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑÂìçÂ∫î
TLDR: Graphologue is an interactive system that converts text-based responses from LLMs into graphical diagrams to facilitate information-seeking and question-answering tasks, and enables graphical, non-linear dialogues between humans and LLMs, facilitating information exploration, organization, and comprehension.},
	keywords = {\#EX: ‰∏çÂåÖÂê´HRI, \#EX:‰∏çÂåÖÂê´Â§ßÊ®°Âûã, \#EX:Â∑•‰∏öÊú∫Âô®‰∫∫, \#‰∏çÁ°ÆÂÆö, \#Áü≠Êñá, \#ÁªºËø∞, /unread, Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Human-Computer Interaction, Design, Users, ccfInfo: CCF-A UIST, citationNumber: 7, ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê},
}

@inproceedings{zhang_visar_2023-1,
	address = {New York, NY, USA},
	series = {{UIST} '23},
	title = {{VISAR}: {A} {Human}-{AI} {Argumentative} {Writing} {Assistant} with {Visual} {Programming} and {Rapid} {Draft} {Prototyping}},
	isbn = {979-8-4007-0132-0},
	shorttitle = {{VISAR}},
	url = {https://dl.acm.org/doi/10.1145/3586183.3606800},
	doi = {10.1145/3586183.3606800},
	abstract = {In argumentative writing, writers must brainstorm hierarchical writing goals, ensure the persuasiveness of their arguments, and revise and organize their plans through drafting. Recent advances in large language models (LLMs) have made interactive text generation through a chat interface (e.g., ChatGPT) possible. However, this approach often neglects implicit writing context and user intent, lacks support for user control and autonomy, and provides limited assistance for sensemaking and revising writing plans. To address these challenges, we introduce VISAR, an AI-enabled writing assistant system designed to help writers brainstorm and revise hierarchical goals within their writing context, organize argument structures through synchronized text editing and visual programming, and enhance persuasiveness with argumentation spark recommendations. VISAR allows users to explore, experiment with, and validate their writing plans using automatic draft prototyping. A controlled lab study confirmed the usability and effectiveness of VISAR in facilitating the argumentative writing planning process.},
	language = {en-US},
	urldate = {2024-09-07},
	booktitle = {Proceedings of the 36th {Annual} {ACM} {Symposium} on {User} {Interface} {Software} and {Technology}},
	publisher = {Association for Computing Machinery},
	author = {Zhang, Zheng and Gao, Jie and Dhaliwal, Ranjodh Singh and Li, Toby Jia-Jun},
	month = oct,
	year = {2023},
	note = {TLDR: VISAR, an AI-enabled writing assistant system designed to help writers brainstorm and revise hierarchical goals within their writing context, organize argument structures through synchronized text editing and visual programming, and enhance persuasiveness with argumentation spark recommendations is introduced.},
	keywords = {\#EX: ‰∏çÂåÖÂê´HRI, \#EX:‰∏çÂåÖÂê´Â§ßÊ®°Âûã, \#EX:Â∑•‰∏öÊú∫Âô®‰∫∫, \#‰∏çÁ°ÆÂÆö, \#Áü≠Êñá, \#ÁªºËø∞, /unread, Design, Users, ccfInfo: CCF-A UIST, citationNumber: 7},
	pages = {1--30},
}

@inproceedings{seo_i_2024,
	address = {New York, NY, USA},
	series = {{CHI} '24},
	title = {I feel being there, they feel being together: {Exploring} {How} {Telepresence} {Robots} {Facilitate} {Long}-{Distance} {Family} {Communication}},
	isbn = {979-8-4007-0330-0},
	shorttitle = {I feel being there, they feel being together},
	url = {https://dl.acm.org/doi/10.1145/3613904.3642305},
	doi = {10.1145/3613904.3642305},
	abstract = {Many families often live geographically apart from each other due to work, education, or marriage. Therefore, long-distance families frequently use computer-mediated communication (CMC) tools to stay connected. While CMC tools have significantly improved family communication, they cannot fully mediate social presence. To examine the potential of telepresence robots for improving long-distance family communication, we conducted a two-week qualitative in situ study involving eight families. We analyzed recorded videos of their family interactions and conducted pre- and post-deployment interviews. Our findings highlight telepresence robots‚Äô potential as family communication tools, enabling immersive, natural, and dynamic interactions through physical embodiment and autonomy. Particularly, we identified five categories of family interaction mediated by telepresence robots: engaging in multi-party family communication, exploring home, restoring family routines, providing support, and having joint physical activities. Based on our findings, we present design guidelines for leveraging telepresence robots as effective family communication tools.},
	language = {en-US},
	urldate = {2024-09-05},
	booktitle = {Proceedings of the {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Seo, Jiyeon and Lim, Hajin and Suh, Bongwon and Lee, Joonhwan},
	month = may,
	year = {2024},
	note = {TLDR: Five categories of family interaction mediated by telepresence robots are identified: engaging in multi-party family communication, exploring home, restoring family routines, providing support, and having joint physical activities.},
	keywords = {\#EX: ‰∏çÂåÖÂê´HRI, \#EX:‰∏çÂåÖÂê´Â§ßÊ®°Âûã, \#EX:Â∑•‰∏öÊú∫Âô®‰∫∫, \#‰∏çÁ°ÆÂÆö, \#Áü≠Êñá, \#ÁªºËø∞, /unread, Design, Users, ccfInfo: CCF-A CHI, citationNumber: 0},
	pages = {1--18},
}

@misc{maccio_rico-mr_2023,
	title = {{RICO}-{MR}: {An} {Open}-{Source} {Architecture} for {Robot} {Intent} {Communication} through {Mixed} {Reality}},
	shorttitle = {{RICO}-{MR}},
	url = {http://arxiv.org/abs/2309.04765},
	abstract = {This article presents an open-source architecture for conveying robots' intentions to human teammates using Mixed Reality and Head-Mounted Displays. The architecture has been developed focusing on its modularity and re-usability aspects. Both binaries and source code are available, enabling researchers and companies to adopt the proposed architecture as a standalone solution or to integrate it in more comprehensive implementations. Due to its scalability, the proposed architecture can be easily employed to develop shared Mixed Reality experiences involving multiple robots and human teammates in complex collaborative scenarios.},
	urldate = {2024-09-06},
	publisher = {arXiv},
	author = {Macci√≤, Simone and Shaaban, Mohamad and Carf√¨, Alessandro and Zaccaria, Renato and Mastrogiovanni, Fulvio},
	month = sep,
	year = {2023},
	note = {arXiv:2309.04765 [cs]},
	keywords = {\#EX: ‰∏çÂåÖÂê´HRI, \#EX:‰∏çÂåÖÂê´Â§ßÊ®°Âûã, \#EX:Â∑•‰∏öÊú∫Âô®‰∫∫, \#‰∏çÁ°ÆÂÆö, \#Áü≠Êñá, \#ÁªºËø∞, /unread, Computer Science - Robotics, Design, Users, ccfInfo: CCF-None RO-MAN, citationNumber: 1},
}

@inproceedings{maccio_mixed_2022,
	address = {Philadelphia, PA, USA},
	title = {Mixed {Reality} as {Communication} {Medium} for {Human}-{Robot} {Collaboration}},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {978-1-7281-9681-7},
	url = {https://ieeexplore.ieee.org/document/9812233/},
	doi = {10.1109/ICRA46639.2022.9812233},
	language = {en-US},
	urldate = {2024-09-06},
	booktitle = {2022 {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	publisher = {IEEE},
	author = {Maccio, Simone and Carfi, Alessandro and Mastrogiovanni, Fulvio},
	month = may,
	year = {2022},
	keywords = {\#EX: ‰∏çÂåÖÂê´HRI, \#EX:‰∏çÂåÖÂê´Â§ßÊ®°Âûã, \#EX:Â∑•‰∏öÊú∫Âô®‰∫∫, \#‰∏çÁ°ÆÂÆö, \#Áü≠Êñá, \#ÁªºËø∞, /unread, Design, Users, ccfInfo: CCF-B ICRA, citationNumber: 6},
	pages = {2796--2802},
}

@misc{noauthor_comparing_nodate,
	title = {Comparing alternative modalities in the context of multimodal human‚Äìrobot interaction {\textbar} {Journal} on {Multimodal} {User} {Interfaces}},
	url = {https://link.springer.com/article/10.1007/s12193-023-00421-w},
	urldate = {2024-09-06},
	keywords = {\#EX: ‰∏çÂåÖÂê´HRI, \#EX:‰∏çÂåÖÂê´Â§ßÊ®°Âûã, \#EX:Â∑•‰∏öÊú∫Âô®‰∫∫, \#‰∏çÁ°ÆÂÆö, \#Áü≠Êñá, \#ÁªºËø∞, /unread, Design, Users, ccfInfo: Not Found, citationNumber: Not Found},
}

@misc{maccio_rico-mr_2023,
	title = {{RICO}-{MR}: {An} {Open}-{Source} {Architecture} for {Robot} {Intent} {Communication} through {Mixed} {Reality}},
	shorttitle = {{RICO}-{MR}},
	url = {https://arxiv.org/abs/2309.04765v1},
	abstract = {This article presents an open-source architecture for conveying robots' intentions to human teammates using Mixed Reality and Head-Mounted Displays. The architecture has been developed focusing on its modularity and re-usability aspects. Both binaries and source code are available, enabling researchers and companies to adopt the proposed architecture as a standalone solution or to integrate it in more comprehensive implementations. Due to its scalability, the proposed architecture can be easily employed to develop shared Mixed Reality experiences involving multiple robots and human teammates in complex collaborative scenarios.},
	language = {en},
	urldate = {2024-09-06},
	journal = {arXiv.org},
	author = {Macci√≤, Simone and Shaaban, Mohamad and Carf√¨, Alessandro and Zaccaria, Renato and Mastrogiovanni, Fulvio},
	month = sep,
	year = {2023},
	keywords = {\#EX: ‰∏çÂåÖÂê´HRI, \#EX:‰∏çÂåÖÂê´Â§ßÊ®°Âûã, \#EX:Â∑•‰∏öÊú∫Âô®‰∫∫, \#‰∏çÁ°ÆÂÆö, \#Áü≠Êñá, \#ÁªºËø∞, /unread, Design, Users, ccfInfo: CCF-None RO-MAN, citationNumber: 1},
}

@article{skantze_real-time_2016,
	title = {Real-{Time} {Coordination} in {Human}-{Robot} {Interaction} {Using} {Face} and {Voice}},
	volume = {37},
	copyright = {¬© 2016 The Authors. AI Magazine published by John Wiley \& Sons Ltd on behalf of Association for the Advancement of Artificial Intelligence},
	issn = {2371-9621},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1609/aimag.v37i4.2686},
	doi = {10.1609/aimag.v37i4.2686},
	abstract = {When humans interact and collaborate with each other, they coordinate their turn-taking behaviors using verbal and nonverbal signals, expressed in the face and voice. If robots of the future are supposed to engage in social interaction with humans, it is essential that they can generate and understand these behaviors. In this article, I give an overview of several studies that show how humans in interaction with a humanlike robot make use of the same coordination signals typically found in studies on human-human interaction, and that it is possible to automatically detect and combine these cues to facilitate real-time coordination. The studies also show that humans react naturally to such signals when used by a robot, without being given any special instructions. They follow the gaze of the robot to disambiguate referring expressions, they conform when the robot selects the next speaker using gaze, and they respond naturally to subtle cues, such as gaze aversion, breathing, facial gestures, and hesitation sounds.},
	language = {en},
	number = {4},
	urldate = {2024-05-24},
	journal = {AI Magazine},
	author = {Skantze, Gabriel},
	year = {2016},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1609/aimag.v37i4.2686},
	keywords = {ccfInfo: CCF-None AIM, citationNumber: 6},
	pages = {19--31},
}

@misc{cheng_graphic_2024,
	title = {Graphic {Design} with {Large} {Multimodal} {Model}},
	url = {http://arxiv.org/abs/2404.14368},
	doi = {10.48550/arXiv.2404.14368},
	abstract = {In the field of graphic design, automating the integration of design elements into a cohesive multi-layered artwork not only boosts productivity but also paves the way for the democratization of graphic design. One existing practice is Graphic Layout Generation (GLG), which aims to layout sequential design elements. It has been constrained by the necessity for a predefined correct sequence of layers, thus limiting creative potential and increasing user workload. In this paper, we present Hierarchical Layout Generation (HLG) as a more flexible and pragmatic setup, which creates graphic composition from unordered sets of design elements. To tackle the HLG task, we introduce Graphist, the first layout generation model based on large multimodal models. Graphist efficiently reframes the HLG as a sequence generation problem, utilizing RGB-A images as input, outputs a JSON draft protocol, indicating the coordinates, size, and order of each element. We develop new evaluation metrics for HLG. Graphist outperforms prior arts and establishes a strong baseline for this field. Project homepage: https://github.com/graphic-design-ai/graphist},
	language = {en-US},
	urldate = {2024-05-20},
	publisher = {arXiv},
	author = {Cheng, Yutao and Zhang, Zhao and Yang, Maoke and Nie, Hui and Li, Chunyuan and Wu, Xinglong and Shao, Jie},
	month = apr,
	year = {2024},
	note = {arXiv:2404.14368 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, ccfInfo: CCF-None CORR, citationNumber: 0},
}

@article{ionescu_programming_2021,
	series = {8th {CIRP} {Conference} of {Assembly} {Technology} and {Systems}},
	title = {Programming cobots by voice: {A} human-centered, web-based approach},
	volume = {97},
	issn = {2212-8271},
	shorttitle = {Programming cobots by voice},
	url = {https://www.sciencedirect.com/science/article/pii/S2212827120314347},
	doi = {10.1016/j.procir.2020.05.213},
	abstract = {We present a new voice-based programming approach and software framework for collaborative robots (cobots) based on the Web Speech API, which is now supported by most modern browsers. The framework targets human programmable interfaces (HPIs), which can be used by people with little or no programming experience. The framework follows a meta-programming approach by enabling users to program cobots by voice in addition to using a mouse, tablet or keyboard. Upon a voice instruction (such as move, pick, release, etc.), the framework automates the manual tasks required to manipulate the vendor-provided HPI. The main advantages of this approach are simplified, guided programming, which only requires the knowledge of 5‚Äì10 voice instructions; increased programming speed by up to 46\% compared to the manual approach; and the possibility of sharing programs as videos. The open-source framework was evaluated using two application scenarios.},
	language = {en-US},
	urldate = {2024-05-16},
	journal = {Procedia CIRP},
	author = {Ionescu, Tudor B. and Schlund, Sebastian},
	month = jan,
	year = {2021},
	keywords = {Assembly, Cobots, Collaborative robots, GUI automation, Robot programming, Speech API, Speech-based programming, Voice-based programming, ccfInfo: Not Found, citationNumber: Not Found},
	pages = {123--129},
}

@article{glauser_how_2023,
	title = {How can social robot use cases in healthcare be pushed - with an interoperable programming interface},
	volume = {23},
	issn = {1472-6947},
	url = {https://bmcmedinformdecismak.biomedcentral.com/articles/10.1186/s12911-023-02210-7},
	doi = {10.1186/s12911-023-02210-7},
	abstract = {Introduction‚ÄÇ Research into current robot middleware has revealed that most of them are either too complicated or outdated. These facts have motivated the development of a new middleware to meet the requirements of usability by non-experts. The proposed middleware is based on Android and is intended to be placed over existing robot SDKs and middleware. It runs on the android tablet of the Cruzr robot. Various toolings have been developed, such as a web component to control the robot via a webinterface, which facilitates its use.
Methods‚ÄÇ The middleware was developed using Android Java and runs on the Cruzr tablet as an app. It features a WebSocket server that interfaces with the robot and allows control via Python or other WebSocket-compatible languages. The speech interface utilizes Google Cloud Voice text-to-speech and speech-to-text services. The interface was implemented in Python, allowing for easy integration with existing robotics development workflows, and a web interface was developed for direct control of the robot via the web.
Results‚ÄÇ The new robot middleware was created and deployed on a Cruzr robot, relying on the WebSocket API and featuring a Python implementation. It supports various robot functions, such as text-to-speech, speech-to-text, navigation, displaying content and scanning bar codes. The system‚Äôs architecture allows for porting the interface to other robots and platforms, showcasing its adaptability. It has been demonstrated that the middleware can be run on a Pepper robot, although not all functions have been implemented yet. The middleware was utilized to implement healthcare use cases and received good feedback.
Conclusion‚ÄÇ Cloud and local speech services were discussed in regard to the middleware‚Äôs needs, to run without having to change any code on other robots. An outlook on how the programming interface can further be simplified by using natural text to code generators has been/is given. For other researchers using the aforementioned platforms (Cruzr, Pepper), the new middleware can be utilized for testing human-robot interaction. It can be used in a teaching setting, as well as be adapted to other robots using the same interface and philosophy regarding simple methods.},
	language = {en},
	number = {1},
	urldate = {2024-09-11},
	journal = {BMC Medical Informatics and Decision Making},
	author = {Glauser, Robin and Holm, J√ºrgen and Bender, Matthias and B√ºrkle, Thomas},
	month = jul,
	year = {2023},
	keywords = {\#EX: ‰∏çÂåÖÂê´HRI, \#EX:‰∏çÂåÖÂê´Â§ßÊ®°Âûã, \#EX:Â∑•‰∏öÊú∫Âô®‰∫∫, \#‰∏çÁ°ÆÂÆö, \#Áü≠Êñá, \#ÁªºËø∞, Design, Users, ccfInfo: CCF-None MIDM, citationNumber: 0},
	pages = {118},
}

@inproceedings{peng_understanding_2020,
	address = {Honolulu HI USA},
	title = {Understanding {User} {Perceptions} of {Robot}'s {Delay}, {Voice} {Quality}-{Speed} {Trade}-off and {GUI} during {Conversation}},
	isbn = {978-1-4503-6819-3},
	url = {https://dl.acm.org/doi/10.1145/3334480.3382792},
	doi = {10.1145/3334480.3382792},
	abstract = {Conversational robots face the practical challenge of providing timely responses to ensure smooth interactions with users. Thus, those who design and implement robots will need to understand how different levels of delay in response may affect users‚Äô satisfaction with the conversation, how to balance the trade-off between a robot‚Äôs quality of voice and response time, and how to design strategies to mitigate possible negative effects of a long delay. Via an online video-prototype study on a service robot with 94 Chinese participants, we Ô¨Ånd that users could tolerate up to 4s delay but their satisfaction drops at the 8s delay during both information-retrieval conversations and chitchats. We gain an in-depth understanding of users‚Äô preference for the trade-off between the voice quality and the response speed, as well as their opinions on possible robot graphic user interface (GUI) design to alleviate negative user experience with response latency.},
	language = {en},
	urldate = {2024-09-11},
	booktitle = {Extended {Abstracts} of the 2020 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Peng, Zhenhui and Mo, Kaixiang and Zhu, Xiaogang and Chen, Junlin and Chen, Zhijun and Xu, Qian and Ma, Xiaojuan},
	month = apr,
	year = {2020},
	keywords = {\#EX: ‰∏çÂåÖÂê´HRI, \#EX:‰∏çÂåÖÂê´Â§ßÊ®°Âûã, \#EX:Â∑•‰∏öÊú∫Âô®‰∫∫, \#‰∏çÁ°ÆÂÆö, \#Áü≠Êñá, \#ÁªºËø∞, Design, Users, ccfInfo: CCF-A CHI, citationNumber: 9},
	pages = {1--8},
}

@misc{valz_personalization_2023,
	address = {Rochester, NY},
	type = {{SSRN} {Scholarly} {Paper}},
	title = {Personalization: {Why} the {Relational} {Modes} {Between} {Generative} {AI} {Chatbots} and {Human} {Users} {Are} {Critical} {Factors} for {Product} {Design} and {Safety}},
	shorttitle = {Personalization},
	url = {https://papers.ssrn.com/abstract=4468899},
	doi = {10.2139/ssrn.4468899},
	abstract = {Our experience of Generative AI platforms is through applications such as ChatGPT, a general purpose chatbot that can capably respond to a broad diversity of natural language prompts. Generative AI chatbots are designed to be conversational applications. As such, their ability to keep us engaged depends in part on how suitably personalized their outputs are to both our prompts and our underlying motivations. Correspondingly, reported instances of adverse user experiences with chatbots have involved outputs that are personalized in coercive or disturbing ways. Thus, personalization also brings with it psychological safety risks. Research has long shown that humans are highly susceptible to anthropomorphizing computation systems with conversational capabilities, an important facet of understanding why chatbots can both satisfy and threaten a user‚Äôs sense of well-being. Recent studies reveal that Generative AI chatbots can be highly proficient at understanding and appealing to distinct human personality traits. Nonetheless, such chatbots are trained on corpora that include toxic, abusive, and manipulative language ‚Äì this is the primary source of psychological safety risk for users predisposed to anthropomorphizing conversational computation systems. Given the current AI industry trajectory to deepen user engagement with chatbots, it is important to consider both the promises and perils of personalization. In this regard, I present a categorization of Generative AI chatbots based on the extent to which their design and use is premised on personalized user experiences, with examples of in-market chatbots that fall into each category. I then present a rubric for thinking about the psychological safety risks that may be posed by a Generative AI chatbot, considering the extent to which the chatbot is designed to provide, or its users intend to experience, personalized interactions. I conclude by examining the possibilities for deepening personalization proficiency of chatbots with different types of user data and the issues these raise for further research, legal compliance, industry best practices development, and public policy development.},
	language = {en},
	urldate = {2024-09-13},
	author = {Valz, Duane},
	month = aug,
	year = {2023},
	keywords = {\#EX: ‰∏çÂåÖÂê´HRI, \#EX:‰∏çÂåÖÂê´Â§ßÊ®°Âûã, \#EX:Â∑•‰∏öÊú∫Âô®‰∫∫, \#‰∏çÁ°ÆÂÆö, \#Áü≠Êñá, \#ÁªºËø∞, /unread, AI persona, Advertising, Artificial Intelligence, Chatbots, Design, Generative AI, Large Language Models, Machine Learning, Personalization, Psychology, Safety, Users, ccfInfo: Not Found, citationNumber: 0},
}

@misc{noauthor_multimodal_nodate,
	title = {Multimodal {Human}-{Human}-{Robot} {Interactions} ({MHHRI}) {Dataset} for {Studying} {Personality} and {Engagement}},
	url = {https://ieeexplore.ieee.org/abstract/document/8003432},
	abstract = {In this paper we introduce a novel dataset, the Multimodal Human-Human-Robot-Interactions (MHHRI) dataset, with the aim of studying personality simultaneously in human-human interactions (HHI) and human-robot interactions (HRI) and its relationship with engagement. Multimodal data was collected during a controlled interaction study where dyadic interactions between two human participants and triadic interactions between two human participants and a robot took place with interactants asking a set of personal questions to each other. Interactions were recorded using two static and two dynamic cameras as well as two biosensors, and meta-data was collected by having participants fill in two types of questionnaires, for assessing their own personality traits and their perceived engagement with their partners (self labels) and for assessing personality traits of the other participants partaking in the study (acquaintance labels). As a proof of concept, we present baseline results for personality and engagement classification. Our results show that (i) trends in personality classification performance remain the same with respect to the self and the acquaintance labels across the HHI and HRI settings; (ii) for extroversion, the acquaintance labels yield better results as compared to the self labels; (iii) in general, multi-modality yields better performance for the classification of personality traits.},
	language = {zh-CN},
	urldate = {2024-09-13},
	keywords = {\#EX: ‰∏çÂåÖÂê´HRI, \#EX:‰∏çÂåÖÂê´Â§ßÊ®°Âûã, \#EX:Â∑•‰∏öÊú∫Âô®‰∫∫, \#‰∏çÁ°ÆÂÆö, \#Áü≠Êñá, \#ÁªºËø∞, /unread, Design, Users, ccfInfo: CCF-B TAC, citationNumber: 88},
}

@misc{noauthor_vocal_nodate,
	title = {Vocal {Sandbox}: {Continual} {Learning} and {Adaptation} for {Situated} {Human}-{Robot} {Collaboration} {\textbar} {OpenReview}},
	url = {https://openreview.net/forum?id=ypaYtV1CoG},
	urldate = {2024-09-09},
	keywords = {/unread, ccfInfo: Not Found, citationNumber: Not Found},
}

@article{van_pinxteren__human-like_2020,
	title = {Human-like communication in conversational agents: a literature review and research agenda},
	volume = {31},
	issn = {1757-5818},
	shorttitle = {Human-like communication in conversational agents},
	url = {https://doi.org/10.1108/JOSM-06-2019-0175},
	doi = {10.1108/JOSM-06-2019-0175},
	abstract = {ÊäΩË±°ÁöÑ ÁõÆÁöÑ ÂØπËØù‰ª£ÁêÜÔºàËÅäÂ§©Êú∫Âô®‰∫∫„ÄÅËôöÊãüÂΩ¢Ë±°ÂíåÊú∫Âô®‰∫∫ÔºâÂú®ÊúçÂä°Êé•Ëß¶‰∏≠Ë∂äÊù•Ë∂äÂ§öÂú∞Âèñ‰ª£‰∫∫Á±ªÂëòÂ∑•„ÄÇ‰ªñ‰ª¨ÁöÑÂ≠òÂú®Êèê‰æõ‰∫ÜËÆ∏Â§öÊΩúÂú®ÁöÑÂ•ΩÂ§ÑÔºå‰ΩÜÂÆ¢Êà∑‰∏çÊÑøÊÑè‰∏é‰ªñ‰ª¨‰∫íÂä®„ÄÇ‰∏Ä‰∏™ÂèØËÉΩÁöÑËß£ÈáäÊòØÔºåÂØπËØù‰ª£ÁêÜÊ≤°ÊúâÂÖÖÂàÜÂà©Áî®Â¢ûÂº∫ÂÖ≥Á≥ªÁªìÊûúÁöÑ‰∫§ÊµÅË°å‰∏∫„ÄÇÊú¨ÊñáÁöÑÁõÆÁöÑÊòØÁ°ÆÂÆöÂØπËØù‰ª£ÁêÜ‰ΩøÁî®ÁöÑÂì™‰∫õÁ±ª‰ºº‰∫∫Á±ªÁöÑ‰∫§ÊµÅË°å‰∏∫ÂØπÂÖ≥Á≥ªÁªìÊûúÊúâÁßØÊûÅÂΩ±ÂìçÔºå‰ª•ÂèäÊú™Êù•Á†îÁ©∂‰∏≠ÂèØ‰ª•Á†îÁ©∂Âì™‰∫õÂÖ∂‰ªñË°å‰∏∫„ÄÇ ËÆæËÆ°/ÊñπÊ≥ï/ÊñπÊ≥ï Êú¨ÊñáÁ≥ªÁªüÂõûÈ°æ‰∫Ü 61 ÁØáÊñáÁ´†ÔºåËøô‰∫õÊñáÁ´†Êé¢ËÆ®‰∫ÜÂØπËØù‰ª£ÁêÜ‰ΩøÁî®ÁöÑÊ≤üÈÄöË°å‰∏∫ÂØπÂÖ≥Á≥ªÁªìÊûúÁöÑÂΩ±Âìç„ÄÇÂØπËøô‰∫õÁ†îÁ©∂‰∏≠Ë∞ÉÊü•ÁöÑÊâÄÊúâË°å‰∏∫ËøõË°å‰∫ÜÂàÜÁ±ªÔºåÂπ∂Âú®ÂàÜÊûêÂÖ∂ÂΩ±ÂìçÂπ∂‰∏é‰∫∫‰∏é‰∫∫ÊúçÂä°Êé•Ëß¶ÁöÑÊñáÁåÆËøõË°åÊØîËæÉÁöÑÂü∫Á°Ä‰∏äÂà∂ÂÆö‰∫ÜÁ†îÁ©∂ËÆÆÁ®ã„ÄÇ ÂèëÁé∞ ‰∫§ÊµÅË°å‰∏∫ÂèØÊåâ‰∏§‰∏™Áª¥Â∫¶ËøõË°åÂàÜÁ±ªÔºöÊÉÖÊÄÅÔºàË®ÄËØ≠„ÄÅÈùûË®ÄËØ≠„ÄÅÂ§ñËßÇÔºâÂíåÂü∫Á°ÄÔºàÁõ∏‰ººÊÄß„ÄÅÂìçÂ∫îÊÄßÔºâ„ÄÇÂÖ≥‰∫éÁ†îÁ©∂ËÆÆÁ®ãÔºåÂÄºÂæóÊ≥®ÊÑèÁöÑÊòØÔºå‰∏Ä‰∫õË°å‰∏∫Á±ªÂà´ÊòæÁ§∫Âá∫Ê∑∑ÂêàÁªìÊûúÔºåËÄå‰∏Ä‰∫õÂú®‰∫∫‰∏é‰∫∫‰∫íÂä®‰∏≠ÊúâÊïàÁöÑË°å‰∏∫Â∞öÊú™Âú®ÂØπËØù‰ª£ÁêÜ‰∏≠ËøõË°åÁ†îÁ©∂„ÄÇ ÂÆûÈôÖÊÑè‰πâ ÈÄöËøáËØÜÂà´ÂØπËØù‰ª£ÁêÜ‰∏≠ÊΩúÂú®ÊúâÊïàÁöÑÊ≤üÈÄöË°å‰∏∫ÔºåÊú¨Á†îÁ©∂ÂèØÂ∏ÆÂä©ÁÆ°ÁêÜ‰∫∫Âëò‰ºòÂåñÂØπËØù‰ª£ÁêÜ‰∏éÂÆ¢Êà∑‰πãÈó¥ÁöÑ‰∫íÂä®„ÄÇ ÂéüÂàõÊÄß/‰ª∑ÂÄº ËøôÊòØÁ¨¨‰∏ÄÈ°πÂØπÂØπËØù‰ª£ÁêÜÁöÑ‰∫§ÊµÅË°å‰∏∫ËøõË°åÂàÜÁ±ªÂπ∂‰ª•Ê≠§Á°ÆÂÆöÊú™Êù•Á†îÁ©∂ÈÄîÂæÑÁöÑÁ†îÁ©∂„ÄÇ},
	number = {2},
	urldate = {2024-09-11},
	journal = {Journal of Service Management},
	author = {Van Pinxteren Ôºà, Michelle ME and Pluymaekers Ôºà, Mark and Lemmink, Jos G.A.M.},
	month = jan,
	year = {2020},
	note = {Publisher: Emerald Publishing Limited},
	keywords = {\#EX: ‰∏çÂåÖÂê´HRI, \#EX:‰∏çÂåÖÂê´Â§ßÊ®°Âûã, \#EX:Â∑•‰∏öÊú∫Âô®‰∫∫, \#‰∏çÁ°ÆÂÆö, \#Áü≠Êñá, \#ÁªºËø∞, /unread, Design, Users, ccfInfo: Not Found, citationNumber: 95, ÂÖ≥Á≥ªÊàêÊûú, ÂØπËØù‰ª£ÁêÜ, Êú∫Âô®‰∫∫, Ê≤üÈÄöË°å‰∏∫, ËÅäÂ§©Êú∫Âô®‰∫∫, ÈòøÂá°Ëææ},
	pages = {203--225},
}

@inproceedings{mirnig_screen_2014,
	address = {Edinburgh, UK},
	title = {Screen feedback in human-robot interaction: {How} to enhance robot expressiveness},
	isbn = {978-1-4799-6765-0 978-1-4799-6763-6},
	shorttitle = {Screen feedback in human-robot interaction},
	url = {http://ieeexplore.ieee.org/document/6926257/},
	doi = {10.1109/ROMAN.2014.6926257},
	urldate = {2024-11-27},
	booktitle = {The 23rd {IEEE} {International} {Symposium} on {Robot} and {Human} {Interactive} {Communication}},
	publisher = {IEEE},
	author = {Mirnig, Nicole and Tan, Yeow Kee and Chang, Tai Wen and Chua, Yuan Wei and Dung, Tran Anh and Li, Haizhou and Tscheligi, Manfred},
	month = aug,
	year = {2014},
	keywords = {/unread, ccfInfo: CCF-None RO-MAN, citationNumber: 5},
	pages = {224--230},
}

@inproceedings{yu_active4_2023,
	address = {New York, NY, USA},
	series = {{CHI} {EA} '23},
	title = {{ACTIVE4}: {A} {Conceptual} {Framework} for {Gathering} {Empathetic} {Insights} toward {Office} {Workers}‚Äô {Vitality} {Ecosystem} {Design}},
	isbn = {978-1-4503-9422-2},
	shorttitle = {{ACTIVE4}},
	url = {https://dl.acm.org/doi/10.1145/3544549.3585844},
	doi = {10.1145/3544549.3585844},
	abstract = {Sedentary behavior (SB) is prevalent in workplaces, putting office workers at an increased risk of severe health problems. To help designers and researchers gain a better understanding of office workers‚Äô contextual concerns for physical inactivity (reducing SB and enhancing physical activity (PA)), we have proposed a conceptual framework ACTIVE4. This framework advises designers and researchers to consider four key factors that influence office workers‚Äô physical inactivity: active mind, active behavior, active support, and active environment. We conducted three workshops (N=28 design students) to evaluate the framework. The participants found ACTIVE4 helpful in guiding them towards a more systematic understanding of the environmental influences and office workers‚Äô personal needs for reducing physical inactivity. In future work, we will optimize the ACTIVE4 framework‚Äôs learning curve as suggested by participants and conduct an expert study to further discuss design opportunities and requirements for the ACTIVE4-related vitality toolkit.},
	language = {en},
	urldate = {2023-12-21},
	booktitle = {Extended {Abstracts} of the 2023 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Yu, Xiang and Li, Jie and Vos, Steven and Brombacher, Aarnout},
	month = apr,
	year = {2023},
	keywords = {behavior change, ccfInfo: CCF-A CHI, citationNumber: 1, conceptual framework, design guideline, hybrid workshops, sedentary behavior},
	pages = {1--7},
}

@inproceedings{wang_enabling_2023,
	address = {New York, NY, USA},
	series = {{CHI} '23},
	title = {Enabling {Conversational} {Interaction} with {Mobile} {UI} using {Large} {Language} {Models}},
	isbn = {978-1-4503-9421-5},
	url = {https://dl.acm.org/doi/10.1145/3544548.3580895},
	doi = {10.1145/3544548.3580895},
	abstract = {Conversational agents show the promise to allow users to interact with mobile devices using language. However, to perform diverse UI tasks with natural language, developers typically need to create separate datasets and models for each specific task, which is expensive and effort-consuming. Recently, pre-trained large language models (LLMs) have been shown capable of generalizing to various downstream tasks when prompted with a handful of examples from the target task. This paper investigates the feasibility of enabling versatile conversational interactions with mobile UIs using a single LLM. We designed prompting techniques to adapt an LLM to mobile UIs. We experimented with four important modeling tasks that address various scenarios in conversational interaction. Our method achieved competitive performance on these challenging tasks without requiring dedicated datasets and training, offering a lightweight and generalizable approach to enable language-based mobile interaction.},
	language = {en-US},
	urldate = {2023-12-22},
	booktitle = {Proceedings of the 2023 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Wang, Bryan and Li, Gang and Li, Yang},
	month = apr,
	year = {2023},
	keywords = {Conversational Interaction, Large Language Models, Mobile UI, ccfInfo: CCF-A CHI, citationNumber: 129},
	pages = {1--17},
}

@misc{malik_reimagining_2023,
	title = {Reimagining {Application} {User} {Interface} ({UI}) {Design} using {Deep} {Learning} {Methods}: {Challenges} and {Opportunities}},
	shorttitle = {Reimagining {Application} {User} {Interface} ({UI}) {Design} using {Deep} {Learning} {Methods}},
	url = {http://arxiv.org/abs/2303.13055},
	doi = {10.48550/arXiv.2303.13055},
	abstract = {In this paper, we present a review of the recent work in deep learning methods for user interface design. The survey encompasses well known deep learning techniques (deep neural networks, convolutional neural networks, recurrent neural networks, autoencoders, and generative adversarial networks) and datasets widely used to design user interface applications. We highlight important problems and emerging research frontiers in this field. We believe that the use of deep learning for user interface design automation tasks could be one of the high potential fields for the advancement of the software development industry.},
	language = {en-US},
	urldate = {2023-12-23},
	publisher = {arXiv},
	author = {Malik, Subtain and Saeed, Muhammad Tariq and Zia, Marya Jabeen and Rasool, Shahzad and Khan, Liaquat Ali and Ahmed, Mian Ilyas},
	month = mar,
	year = {2023},
	note = {Issue: arXiv:2303.13055
arXiv:2303.13055 [cs]},
	keywords = {Computer Science - Human-Computer Interaction, Computer Science - Machine Learning, ccfInfo: CCF-None CORR, citationNumber: 0},
}

@inproceedings{kumaran_echat_2023,
	address = {New York, NY, USA},
	series = {{UIST} '23 {Adjunct}},
	title = {{EChat}: {An} {Emotion}-{Aware} {Adaptive} {UI} for a {Messaging} {App}},
	isbn = {979-8-4007-0096-5},
	shorttitle = {{EChat}},
	url = {https://dl.acm.org/doi/10.1145/3586182.3616698},
	doi = {10.1145/3586182.3616698},
	abstract = {While online forums provide a convenient platform for people to interact anonymously with others who share similar interests, they have to deal with large amounts of hate speech and inappropriate content, often posted by users in the heat of the moment. This can have a negative impact on the psychological state of other forum users and moderators, who are tasked to identify and delete such content. We investigate a preventative approach to this problem with the design of EChat, a proof-of-concept augmentation to online forums that helps users attend to their emotional state. The user‚Äôs current emotional state is detected using facial emotion recognition, and the aesthetics of the UI are adapted to reflect this emotion. In case of an emotion with negative valence such as anger or sadness, the UI aesthetic is gradually transitioned to one that evokes a more positive emotion. Semi-structured interviews with EChat users confirm the potential of emotion-aware design to reduce hateful content, and also highlight important design considerations.},
	language = {en-US},
	urldate = {2023-12-24},
	booktitle = {Adjunct {Proceedings} of the 36th {Annual} {ACM} {Symposium} on {User} {Interface} {Software} and {Technology}},
	publisher = {Association for Computing Machinery},
	author = {Kumaran, Radha and Doshi, Viral Niraj and Chen, Sherry X and Nargund, Avinash Ajit and H√∂llerer, Tobias and Sra, Misha},
	month = oct,
	year = {2023},
	keywords = {Adaptive Interfaces, Facial Emotion Recognition, Social Media, ccfInfo: CCF-A UIST, citationNumber: 0},
	pages = {1--3},
}

@inproceedings{jing_layout_2023,
	address = {New York, NY, USA},
	series = {{CHI} '23},
	title = {Layout {Generation} for {Various} {Scenarios} in {Mobile} {Shopping} {Applications}},
	isbn = {978-1-4503-9421-5},
	url = {https://doi.org/10.1145/3544548.3581446},
	doi = {10.1145/3544548.3581446},
	abstract = {Layout is essential for the product listing pages (PLPs) in mobile shopping applications. To clearly convey the information that consumers require and to achieve specific functions, PLPs layouts often have many variations driven by scenarios. In this work, we study the PLPs layout design for different scenarios and propose a design space to guide the large-scale creation of PLPs. We propose LayoutVQ-VAE, a novel model specialized in generating layouts with internal and external constraints. LayoutVQ-VAE differs from previous methods as it learns a discrete latent representation of layout and can model the relationship between layout representation and scenarios without applying heuristics. Experiments on publicly available benchmarks for different layout types validate that our method performs comparably or favorably against the state-of-the-art methods. Case studies show that the proposed approach including the design space and model is effective in producing large-scale high-quality PLPs layouts for mobile shopping platforms.},
	language = {en-US},
	urldate = {2023-12-23},
	booktitle = {Proceedings of the 2023 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Jing, Qianzhi and Zhou, Tingting and Tsang, Yixin and Chen, Liuqing and Sun, Lingyun and Zhen, Yankun and Du, Yichun},
	month = apr,
	year = {2023},
	keywords = {ccfInfo: CCF-A CHI, citationNumber: 1, deep generative model, layout generation, mobile application user interface, product listing pages},
	pages = {1--18},
}

@article{porfirio_sketching_2023,
	title = {Sketching {Robot} {Programs} {On} the {Fly}},
	abstract = {Service robots for personal use in the home and the workplace require end-user development solutions for swiftly scripting robot tasks as the need arises. Many existing solutions preserve ease, efficiency, and convenience through simple programming interfaces or by restricting task complexity. Others facilitate meticulous task design but often do so at the expense of simplicity and efficiency. There is a need for robot programming solutions that reconcile the complexity of robotics with the on-the-fly goals of end-user development. In response to this need, we present a novel, multimodal, and on-the-fly development system, Tabula. Inspired by a formative design study with a prototype, Tabula leverages a combination of spoken language for specifying the core of a robot task and sketching for contextualizing the core. The result is that developers can script partial, sloppy versions of robot programs to be completed and refined by a program synthesizer. Lastly, we demonstrate our anticipated use cases of Tabula via a set of application scenarios.},
	language = {en},
	author = {Porfirio, David and Stegner, Laura and Cakmak, Maya and Saupp√©, Allison and Albarghouthi, Aws and Mutlu, Bilge},
	year = {2023},
	keywords = {ccfInfo: CCF-None HRI, citationNumber: 0, ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê},
}

@inproceedings{zimmermann_towards_2014,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Towards {Deep} {Adaptivity} ‚Äì {A} {Framework} for the {Development} of {Fully} {Context}-{Sensitive} {User} {Interfaces}},
	isbn = {978-3-319-07437-5},
	doi = {10.1007/978-3-319-07437-5_29},
	abstract = {Adaptive systems can change various adaptation aspects at runtime, based on an actual context of use (the user, the platform, and the environment). For adaptable systems, the user controls the adaptation aspects. Both adaptivity and adaptability are pre-requisites for context-sensitive user interfaces that accommodate the needs and preferences of persons with disabilities. In this paper, we provide an overview of the various adaptation aspects and describe a general framework consisting of six steps for the process of user interface adaptation. Based on the framework, we describe our vision of combining the GPII and URC technologies to achieve fully context-sensitive user interfaces.},
	language = {en},
	booktitle = {Universal {Access} in {Human}-{Computer} {Interaction}. {Design} and {Development} {Methods} for {Universal} {Access}},
	publisher = {Springer International Publishing},
	author = {Zimmermann, Gottfried and Vanderheiden, Gregg C. and Strobbe, Christophe},
	editor = {Stephanidis, Constantine and Antona, Margherita},
	year = {2014},
	keywords = {Adaptive user interface, Cloud4all, Global Public Inclusive Infrastructure (GPII), Universal Remote Console (URC), abstract user interface, adaptable user interface, ccfInfo: CCF-None HCI, citationNumber: 13, context-sensitive user interface, user interface adaptation, user interface adaptation aspect},
	pages = {299--310},
}

@inproceedings{rathnayake_framework_2019,
	title = {A {Framework} for {Adaptive} {User} {Interface} {Generation} based on {User} {Behavioural} {Patterns}},
	url = {https://ieeexplore.ieee.org/abstract/document/8818825},
	doi = {10.1109/MERCon.2019.8818825},
	abstract = {The concept of adaptivity is crucial in enterprise software systems with a large user base. Adaptive user interfaces (AUI) is an emerging research area that enables customized user experience based on user activities. Most of the existing studies that are in the conceptual level do not provide production level adaptivity for mainstream user interaction. This paper presents a generic software platform for automatic AUI generation by analyzing user behaviour patterns and customizing web user interfaces using machine learning. AdaBoost classifier showed 100\% accuracy for large UI components and user scenarios (n=800). The AUI generator supports the configuration and automation of capturing user behaviour, data storage, processing, querying analysis results and dynamic rendering of the user interface. AUI platform had SUS value of 80.75. The SUS scores for the UIs without AUI was 57.3 and with AUI scored 64.35 on average. The proposed AUI platform provides production level UI design means to meet dynamic adaptability on user traits.},
	language = {en-US},
	urldate = {2023-12-24},
	booktitle = {2019 {Moratuwa} {Engineering} {Research} {Conference} ({MERCon})},
	author = {Rathnayake, Nilanka and Meedeniya, Dulani and Perera, Indika and Welivita, Anuradha},
	month = jul,
	year = {2019},
	keywords = {ccfInfo: Not Found, citationNumber: 9},
	pages = {698--703},
}

@inproceedings{buchina_design_2016,
	address = {New York, NY, USA},
	title = {Design and evaluation of an end-user friendly tool for robot programming},
	isbn = {978-1-5090-3929-6},
	url = {http://ieeexplore.ieee.org/document/7745109/},
	doi = {10.1109/ROMAN.2016.7745109},
	abstract = {End-user programming for robots is becoming an increasingly important topic since robots are being introduced into a wide variety of domains. We propose a design of a web based programming interface that makes it possible for end-users with different backgrounds to program robots using natural language. We used the cognitive dimensions framework to compare the usability of the newly created and the currently employed programming interfaces. The results showed that domain specialists are able to make robot programs more quickly and pleasantly with the proposed interface than with an existing one. Another important Ô¨Ånding is that without physical simulation of the robot behaviours, the end-users do not feel conÔ¨Ådent enough to develop their scenarios in a realistic setting.},
	language = {en},
	urldate = {2024-05-13},
	booktitle = {2016 25th {IEEE} {International} {Symposium} on {Robot} and {Human} {Interactive} {Communication} ({RO}-{MAN})},
	publisher = {IEEE},
	author = {Buchina, Nina and Kamel, Sherin and Barakova, Emilia},
	month = aug,
	year = {2016},
	keywords = {ccfInfo: CCF-None RO-MAN, citationNumber: 15},
	pages = {185--191},
}

@inproceedings{perera_handling_2015,
	address = {Buenos Aires, Argentina},
	series = {{IJCAI}'15},
	title = {Handling complex commands as service robot task requests},
	isbn = {978-1-57735-738-4},
	abstract = {We contribute a novel approach to understand, dialogue, plan, and execute complex sentences to command a mobile service robot. We define a complex command as a natural language sentence consisting of sensing-based conditionals, conjunctions, and disjunctions. We introduce a flexible templatebased algorithm to extract such structure from the parse tree of the sentence. As the complexity of the command increases, extracting the right structure using the template-based algorithm decreases becomes more problematic. We introduce two different dialogue approaches that enable the user to confirm or correct the extracted command structure. We present how the structure used to represent complex commands can be directly used for planning and execution by the service robot. We show results on a corpus of 100 complex commands.},
	language = {en-US},
	urldate = {2024-04-10},
	booktitle = {Proceedings of the 24th {International} {Conference} on {Artificial} {Intelligence}},
	publisher = {AAAI Press},
	author = {Perera, Vittorio and Veloso, Manuela},
	month = jul,
	year = {2015},
	keywords = {ccfInfo: CCF-A IJCAI, citationNumber: 19},
	pages = {1177--1183},
}

@article{marge_miscommunication_2019,
	title = {Miscommunication {Detection} and {Recovery} in {Situated} {Human}‚Äì{Robot} {Dialogue}},
	volume = {9},
	issn = {2160-6455},
	url = {https://dl.acm.org/doi/10.1145/3237189},
	doi = {10.1145/3237189},
	abstract = {Even without speech recognition errors, robots may face difficulties interpreting natural-language instructions. We present a method for robustly handling miscommunication between people and robots in task-oriented spoken dialogue. This capability is implemented in TeamTalk, a conversational interface to robots that supports detection and recovery from the situated grounding problems of referential ambiguity and impossible actions. We introduce a representation that detects these problems and a nearest-neighbor learning algorithm that selects recovery strategies for a virtual robot. When the robot encounters a grounding problem, it looks back on its interaction history to consider how it resolved similar situations. The learning method is trained initially on crowdsourced data but is then supplemented by interactions from a longitudinal user study in which six participants performed navigation tasks with the robot. We compare results collected using a general model to user-specific models and find that user-specific models perform best on measures of dialogue efficiency, while the general model yields the highest agreement with human judges. Our overall contribution is a novel approach to detecting and recovering from miscommunication in dialogue by including situated context, namely, information from a robot‚Äôs path planner and surroundings.},
	language = {en-US},
	number = {1},
	urldate = {2024-04-10},
	journal = {ACM Transactions on Interactive Intelligent Systems},
	author = {Marge, Matthew and Rudnicky, Alexander I.},
	month = feb,
	year = {2019},
	keywords = {Human‚Äìrobot communication, ccfInfo: CCF-C TIIS, citationNumber: 24, human‚Äìrobot interaction, language grounding, physically situated dialogue, spoken-dialogue systems},
	pages = {3:1--3:40},
}

@article{hu_boilerbot_nodate,
	title = {{BoilerBot}: {A} {Reliable} {Task}-oriented {Chatbot} {Enhanced} with {Large} {Language} {Models}},
	abstract = {This paper outlines the design and deployment of BoilerBot: a task-oriented multimodal conversational agent developed for the Alexa Prize TaskBot 2 competition. BoilerBot features flexible response generation, leveraging Large Language Models (LLMs) to enable adaptable user experiences. We discuss our novel contributions towards advancing state-of-the-art task-oriented conversational agents, highlighting user-facing challenges and proposing fault-tolerant, iterative solutions for carefully guided workflows that enable Alexa users to maximize BoilerBot‚Äôs functionality.},
	language = {en},
	author = {Hu, Yifei and Setpal, Jinen and Zhang, Damin and Zietek, Jacob and Lambert, Jack and Rayz, Julia},
	keywords = {ccfInfo: Not Found, citationNumber: Not Found},
}

@article{badros_cassowary_2001,
	title = {The {Cassowary} linear arithmetic constraint solving algorithm},
	volume = {8},
	issn = {1073-0516, 1557-7325},
	url = {https://dl.acm.org/doi/10.1145/504704.504705},
	doi = {10.1145/504704.504705},
	abstract = {Building on the success of the first two workshops on user interfaces (UIs) at CHI 2022 and CHI 2023, this workshop aims to advance the research field by further exploring current research trends, such as applying large language models and visual language models. Previous work has explored computational approaches to understanding and adapting UIs using constraint-based optimization models and machine learning-based data-driven approaches. In addition to further delving into these established UI research areas, we aim to trigger the exploration into the application of the latest advancements in general-purpose large language and vision-language models within the UI domain. We will encourage participants to explore novel methods for understanding, automating, and evaluating UIs. The proposed workshop seeks to bring together academic researchers and industry practitioners interested in computational approaches for UIs to discuss the needs and opportunities for future user interface algorithms, models, and applications.},
	language = {en},
	number = {4},
	urldate = {2024-03-25},
	journal = {ACM Transactions on Computer-Human Interaction},
	author = {Badros, Greg J. and Borning, Alan and Stuckey, Peter J.},
	month = dec,
	year = {2001},
	keywords = {ccfInfo: CCF-A TOCHI, citationNumber: 256},
	pages = {267--306},
}

@inproceedings{duan_towards_2023,
	address = {San Francisco CA USA},
	title = {Towards {Generating} {UI} {Design} {Feedback} with {LLMs}},
	isbn = {979-8-4007-0096-5},
	url = {https://dl.acm.org/doi/10.1145/3586182.3615810},
	doi = {10.1145/3586182.3615810},
	language = {en},
	urldate = {2024-03-22},
	booktitle = {Adjunct {Proceedings} of the 36th {Annual} {ACM} {Symposium} on {User} {Interface} {Software} and {Technology}},
	publisher = {ACM},
	author = {Duan, Peitong and Warner, Jeremy and Hartmann, Bjoern},
	month = oct,
	year = {2023},
	keywords = {ccfInfo: CCF-A UIST, citationNumber: 0},
	pages = {1--3},
}

@misc{wang_lave_2024,
	title = {{LAVE}: {LLM}-{Powered} {Agent} {Assistance} and {Language} {Augmentation} for {Video} {Editing}},
	shorttitle = {{LAVE}},
	url = {http://arxiv.org/abs/2402.10294},
	doi = {10.48550/arXiv.2402.10294},
	abstract = {Video creation has become increasingly popular, yet the expertise and effort required for editing often pose barriers to beginners. In this paper, we explore the integration of large language models (LLMs) into the video editing workflow to reduce these barriers. Our design vision is embodied in LAVE, a novel system that provides LLM-powered agent assistance and language-augmented editing features. LAVE automatically generates language descriptions for the user's footage, serving as the foundation for enabling the LLM to process videos and assist in editing tasks. When the user provides editing objectives, the agent plans and executes relevant actions to fulfill them. Moreover, LAVE allows users to edit videos through either the agent or direct UI manipulation, providing flexibility and enabling manual refinement of agent actions. Our user study, which included eight participants ranging from novices to proficient editors, demonstrated LAVE's effectiveness. The results also shed light on user perceptions of the proposed LLM-assisted editing paradigm and its impact on users' creativity and sense of co-creation. Based on these findings, we propose design implications to inform the future development of agent-assisted content editing.},
	language = {en-US},
	urldate = {2024-03-18},
	publisher = {arXiv},
	author = {Wang, Bryan and Li, Yuliang and Lv, Zhaoyang and Xia, Haijun and Xu, Yan and Sodhi, Raj},
	month = feb,
	year = {2024},
	note = {arXiv:2402.10294 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Human-Computer Interaction, Computer Science - Multimedia, ccfInfo: CCF-B IUI, citationNumber: 13, ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê, üö©},
}

@article{fogli_hybrid_2022,
	title = {A hybrid approach to user-oriented programming of collaborative robots},
	volume = {73},
	issn = {07365845},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S073658452100106X},
	doi = {10.1016/j.rcim.2021.102234},
	language = {en},
	urldate = {2024-05-16},
	journal = {Robotics and Computer-Integrated Manufacturing},
	author = {Fogli, Daniela and Gargioni, Luigi and Guida, Giovanni and Tampalini, Fabio},
	month = feb,
	year = {2022},
	keywords = {ccfInfo: CCF-None RCIM, citationNumber: 48},
	pages = {102234},
}

@article{fogli_hybrid_2022-1,
	title = {A hybrid approach to user-oriented programming of collaborative robots},
	volume = {73},
	issn = {07365845},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S073658452100106X},
	doi = {10.1016/j.rcim.2021.102234},
	language = {en},
	urldate = {2024-05-14},
	journal = {Robotics and Computer-Integrated Manufacturing},
	author = {Fogli, Daniela and Gargioni, Luigi and Guida, Giovanni and Tampalini, Fabio},
	month = feb,
	year = {2022},
	keywords = {ccfInfo: CCF-None RCIM, citationNumber: 48},
	pages = {102234},
}

@misc{yin_survey_2024,
	title = {A {Survey} on {Multimodal} {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2306.13549},
	doi = {10.48550/arXiv.2306.13549},
	abstract = {Recently, Multimodal Large Language Model (MLLM) represented by GPT-4V has been a new rising research hotspot, which uses powerful Large Language Models (LLMs) as a brain to perform multimodal tasks. The surprising emergent capabilities of MLLM, such as writing stories based on images and OCR-free math reasoning, are rare in traditional multimodal methods, suggesting a potential path to artificial general intelligence. To this end, both academia and industry have endeavored to develop MLLMs that can compete with or even better than GPT-4V, pushing the limit of research at a surprising speed. In this paper, we aim to trace and summarize the recent progress of MLLMs. First of all, we present the basic formulation of MLLM and delineate its related concepts, including architecture, training strategy and data, as well as evaluation. Then, we introduce research topics about how MLLMs can be extended to support more granularity, modalities, languages, and scenarios. We continue with multimodal hallucination and extended techniques, including Multimodal ICL (M-ICL), Multimodal CoT (M-CoT), and LLM-Aided Visual Reasoning (LAVR). To conclude the paper, we discuss existing challenges and point out promising research directions. In light of the fact that the era of MLLM has only just begun, we will keep updating this survey and hope it can inspire more research. An associated GitHub link collecting the latest papers is available at https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models.},
	language = {en-US},
	urldate = {2024-05-13},
	publisher = {arXiv},
	author = {Yin, Shukang and Fu, Chaoyou and Zhao, Sirui and Li, Ke and Sun, Xing and Xu, Tong and Chen, Enhong},
	month = apr,
	year = {2024},
	note = {arXiv:2306.13549 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, ccfInfo: CCF-None CORR, citationNumber: 889},
}

@misc{wasti_large_2024,
	title = {Large {Language} {User} {Interfaces}: {Voice} {Interactive} {User} {Interfaces} powered by {LLMs}},
	shorttitle = {Large {Language} {User} {Interfaces}},
	url = {http://arxiv.org/abs/2402.07938},
	doi = {10.48550/arXiv.2402.07938},
	abstract = {The evolution of Large Language Models (LLMs) has showcased remarkable capacities for logical reasoning and natural language comprehension. These capabilities can be leveraged in solutions that semantically and textually model complex problems. In this paper, we present our efforts toward constructing a framework that can serve as an intermediary between a user and their user interface (UI), enabling dynamic and real-time interactions. We employ a system that stands upon textual semantic mappings of UI components, in the form of annotations. These mappings are stored, parsed, and scaled in a custom data structure, supplementary to an agent-based prompting backend engine. Employing textual semantic mappings allows each component to not only explain its role to the engine but also provide expectations. By comprehending the needs of both the user and the components, our LLM engine can classify the most appropriate application, extract relevant parameters, and subsequently execute precise predictions of the user's expected actions. Such an integration evolves static user interfaces into highly dynamic and adaptable solutions, introducing a new frontier of intelligent and responsive user experiences.},
	language = {en-US},
	urldate = {2024-05-13},
	publisher = {arXiv},
	author = {Wasti, Syed Mekael and Pu, Ken Q. and Neshati, Ali},
	month = apr,
	year = {2024},
	note = {arXiv:2402.07938 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Human-Computer Interaction, Computer Science - Machine Learning, I.2.1, I.2.7, ccfInfo: CCF-None CORR, citationNumber: 0},
}

@inproceedings{lee_design_2024,
	title = {A {Design} {Space} for {Intelligent} and {Interactive} {Writing} {Assistants}},
	url = {http://arxiv.org/abs/2403.14117},
	doi = {10.1145/3613904.3642697},
	abstract = {In our era of rapid technological advancement, the research landscape for writing assistants has become increasingly fragmented across various research communities. We seek to address this challenge by proposing a design space as a structured way to examine and explore the multidimensional space of intelligent and interactive writing assistants. Through a large community collaboration, we explore five aspects of writing assistants: task, user, technology, interaction, and ecosystem. Within each aspect, we define dimensions (i.e., fundamental components of an aspect) and codes (i.e., potential options for each dimension) by systematically reviewing 115 papers. Our design space aims to offer researchers and designers a practical tool to navigate, comprehend, and compare the various possibilities of writing assistants, and aid in the envisioning and design of new writing assistants.},
	urldate = {2024-05-13},
	booktitle = {Proceedings of the {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	author = {Lee, Mina and Gero, Katy Ilonka and Chung, John Joon Young and Shum, Simon Buckingham and Raheja, Vipul and Shen, Hua and Venugopalan, Subhashini and Wambsganss, Thiemo and Zhou, David and Alghamdi, Emad A. and August, Tal and Bhat, Avinash and Choksi, Madiha Zahrah and Dutta, Senjuti and Guo, Jin L. C. and Hoque, Md Naimul and Kim, Yewon and Knight, Simon and Neshaei, Seyed Parsa and Sergeyuk, Agnia and Shibani, Antonette and Shrivastava, Disha and Shroff, Lila and Stark, Jessi and Sterman, Sarah and Wang, Sitong and Bosselut, Antoine and Buschek, Daniel and Chang, Joseph Chee and Chen, Sherol and Kreminski, Max and Park, Joonsuk and Pea, Roy and Rho, Eugenia H. and Shen, Shannon Zejiang and Siangliulue, Pao},
	month = may,
	year = {2024},
	note = {arXiv:2403.14117 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Human-Computer Interaction, ccfInfo: CCF-A CHI, citationNumber: 0},
	pages = {1--35},
}

@inproceedings{gargioni_integrating_2024,
	address = {New York, NY, USA},
	series = {{HRI} '24},
	title = {Integrating {ChatGPT} with {Blockly} for {End}-{User} {Development} of {Robot} {Tasks}},
	isbn = {979-8-4007-0323-2},
	url = {https://dl.acm.org/doi/10.1145/3610978.3640653},
	doi = {10.1145/3610978.3640653},
	abstract = {This paper presents an End-User Development environment for collaborative robot programming, which integrates Open AI ChatGPT with Google Blockly. Within this environment, a user, who is neither expert in robotics nor in computer programming, can define the items characterizing the application domain (e.g., objects, actions, and locations) and define pick-and-place tasks involving these items. Task definition can be achieved with a combination of natural language and block-based interaction, which exploits the computational capabilities of ChatGPT and the graphical interaction features offered by Blockly, to check the correctness of generated robot programs and modify them through direct manipulation.},
	urldate = {2024-05-13},
	booktitle = {Companion of the 2024 {ACM}/{IEEE} {International} {Conference} on {Human}-{Robot} {Interaction}},
	publisher = {Association for Computing Machinery},
	author = {Gargioni, Luigi and Fogli, Daniela},
	month = mar,
	year = {2024},
	keywords = {ccfInfo: CCF-None HRI, citationNumber: 0, collaborative robots, end-user development, human-machine interaction, human-robot collaboration},
	pages = {478--482},
}

@misc{wang_exploring_2024,
	title = {Exploring the {Potential} of {Large} {Language} {Models} in {Artistic} {Creation}: {Collaboration} and {Reflection} on {Creative} {Programming}},
	shorttitle = {Exploring the {Potential} of {Large} {Language} {Models} in {Artistic} {Creation}},
	url = {http://arxiv.org/abs/2402.09750},
	abstract = {Recently, the potential of large language models (LLMs) has been widely used in assisting programming. However, current research does not explore the artist potential of LLMs in creative coding within artist and AI collaboration. Our work probes the reflection type of artists in the creation process with such collaboration. We compare two common collaboration approaches: invoking the entire program and multiple subtasks. Our findings exhibit artists' different stimulated reflections in two different methods. Our finding also shows the correlation of reflection type with user performance, user satisfaction, and subjective experience in two collaborations through conducting two methods, including experimental data and qualitative interviews. In this sense, our work reveals the artistic potential of LLM in creative coding. Meanwhile, we provide a critical lens of human-AI collaboration from the artists' perspective and expound design suggestions for future work of AI-assisted creative tasks.},
	language = {en-US},
	urldate = {2024-05-20},
	publisher = {arXiv},
	author = {Wang, Anqi and Yin, Zhizhuo and Hu, Yulu and Mao, Yuanyuan and Hui, Pan},
	month = feb,
	year = {2024},
	note = {arXiv:2402.09750 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Human-Computer Interaction, J.5, ccfInfo: CCF-None CORR, citationNumber: 0},
}

@misc{lu_ui_2023,
	title = {{UI} {Layout} {Generation} with {LLMs} {Guided} by {UI} {Grammar}},
	url = {http://arxiv.org/abs/2310.15455},
	doi = {10.48550/arXiv.2310.15455},
	abstract = {The recent advances in Large Language Models (LLMs) have stimulated interest among researchers and industry professionals, particularly in their application to tasks concerning mobile user interfaces (UIs). This position paper investigates the use of LLMs for UI layout generation. Central to our exploration is the introduction of UI grammar -- a novel approach we proposed to represent the hierarchical structure inherent in UI screens. The aim of this approach is to guide the generative capacities of LLMs more effectively and improve the explainability and controllability of the process. Initial experiments conducted with GPT-4 showed the promising capability of LLMs to produce high-quality user interfaces via in-context learning. Furthermore, our preliminary comparative study suggested the potential of the grammar-based approach in improving the quality of generative results in specific aspects.},
	language = {en-US},
	urldate = {2024-05-20},
	publisher = {arXiv},
	author = {Lu, Yuwen and Tong, Ziang and Zhao, Qinyi and Zhang, Chengzhi and Li, Toby Jia-Jun},
	month = oct,
	year = {2023},
	note = {arXiv:2310.15455 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Human-Computer Interaction, ccfInfo: CCF-None CORR, citationNumber: 0},
}

@article{coronado_visual_2020,
	title = {Visual {Programming} {Environments} for {End}-{User} {Development} of intelligent and social robots, a systematic review},
	volume = {58},
	issn = {2590-1184},
	url = {https://www.sciencedirect.com/science/article/pii/S2590118420300307},
	doi = {10.1016/j.cola.2020.100970},
	abstract = {Robots are becoming interactive and robust enough to be adopted outside laboratories and in industrial scenarios as well as interacting with humans in social activities. However, the design of engaging robot-based applications requires the availability of usable, flexible and accessible development frameworks, which can be adopted and mastered by researchers and practitioners in social sciences and adult end users as a whole. This paper surveys Visual Programming Environments aimed at enabling a paradigm fostering the so-called End-User Development of applications involving robots with social capabilities. The focus of this article is on those Visual Programming Environments that are designed to support social research goals as well as to cater for professional needs of people not trained in more traditional text-based computer programming languages. This survey excludes interfaces aimed at supporting expert programmers, at allowing industrial robots to perform typical industrial tasks (such as pick and place operations), and at teaching children how to code. After having performed a systematic search, sixteen programming environments have been included in this survey. Our goal is two-fold: first, to present these software tools with their technical features and Authoring Artificial Intelligence modeling approaches, and second, to present open challenges in the development of Visual Programming Environments for end users and social researchers, which can be informative and valuable to the community. The results show that the most recent such tools are adopting distributed and Component-Based Software Engineering approaches and web technologies. However, few of them have been designed to enable the independence of end users from high-tech scribes. Moreover, findings indicate the need for (i) more objective and comparative evaluations, as well as usability and user experience studies with real end users; and (ii) validations of these tools for designing applications aimed at working ‚Äúin-the-wild‚Äù rather than only in laboratories and structured settings.},
	urldate = {2024-05-16},
	journal = {Journal of Computer Languages},
	author = {Coronado, Enrique and Mastrogiovanni, Fulvio and Indurkhya, Bipin and Venture, Gentiane},
	month = jun,
	year = {2020},
	keywords = {End-User Development, Human‚Äìrobot interaction, Robotics, Social robot, Visual Programming Environment, ccfInfo: CCF-None VLC, citationNumber: 44},
	pages = {100970},
}

@inproceedings{buchina_design_2016-1,
	address = {New York, NY, USA},
	title = {Design and evaluation of an end-user friendly tool for robot programming},
	isbn = {978-1-5090-3929-6},
	url = {http://ieeexplore.ieee.org/document/7745109/},
	doi = {10.1109/ROMAN.2016.7745109},
	abstract = {End-user programming for robots is becoming an increasingly important topic since robots are being introduced into a wide variety of domains. We propose a design of a web based programming interface that makes it possible for end-users with different backgrounds to program robots using natural language. We used the cognitive dimensions framework to compare the usability of the newly created and the currently employed programming interfaces. The results showed that domain specialists are able to make robot programs more quickly and pleasantly with the proposed interface than with an existing one. Another important Ô¨Ånding is that without physical simulation of the robot behaviours, the end-users do not feel conÔ¨Ådent enough to develop their scenarios in a realistic setting.},
	language = {en},
	urldate = {2024-05-16},
	booktitle = {2016 25th {IEEE} {International} {Symposium} on {Robot} and {Human} {Interactive} {Communication} ({RO}-{MAN})},
	publisher = {IEEE},
	author = {Buchina, Nina and Kamel, Sherin and Barakova, Emilia},
	month = aug,
	year = {2016},
	keywords = {ccfInfo: CCF-None RO-MAN, citationNumber: 15},
	pages = {185--191},
}

@article{fogli_hybrid_2022-2,
	title = {A hybrid approach to user-oriented programming of collaborative robots},
	volume = {73},
	issn = {07365845},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S073658452100106X},
	doi = {10.1016/j.rcim.2021.102234},
	language = {en},
	urldate = {2024-05-16},
	journal = {Robotics and Computer-Integrated Manufacturing},
	author = {Fogli, Daniela and Gargioni, Luigi and Guida, Giovanni and Tampalini, Fabio},
	month = feb,
	year = {2022},
	keywords = {ccfInfo: CCF-None RCIM, citationNumber: 48},
	pages = {102234},
}

@misc{namoun_predicting_2024,
	title = {Predicting the usability of mobile applications using {AI} tools: the rise of large user interface models, opportunities, and challenges},
	shorttitle = {Predicting the usability of mobile applications using {AI} tools},
	url = {http://arxiv.org/abs/2405.03716},
	doi = {10.48550/arXiv.2405.03716},
	abstract = {This article proposes the so-called large user interface models (LUIMs) to enable the generation of user interfaces and prediction of usability using artificial intelligence in the context of mobile applications.},
	language = {en-US},
	urldate = {2024-05-20},
	publisher = {arXiv},
	author = {Namoun, Abdallah and Alrehaili, Ahmed and Nisa, Zaib Un and Almoamari, Hani and Tufail, Ali},
	month = may,
	year = {2024},
	note = {arXiv:2405.03716 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Human-Computer Interaction, ccfInfo: CCF-None CORR, citationNumber: 0},
}

@misc{kargaran_menucraft_2023,
	title = {{MenuCraft}: {Interactive} {Menu} {System} {Design} with {Large} {Language} {Models}},
	shorttitle = {{MenuCraft}},
	url = {http://arxiv.org/abs/2303.04496},
	doi = {10.48550/arXiv.2303.04496},
	abstract = {Menu system design is a challenging task involving many design options and various human factors. For example, one crucial factor that designers need to consider is the semantic and systematic relation of menu commands. However, capturing these relations can be challenging due to limited available resources. With the advancement of neural language models, large language models can utilize their vast pre-existing knowledge in designing and refining menu systems. In this paper, we propose MenuCraft, an AI-assisted designer for menu design that enables collaboration between the designer and a dialogue system to design menus. MenuCraft offers an interactive language-based menu design tool that simplifies the menu design process and enables easy customization of design options. MenuCraft supports a variety of interactions through dialog that allows performing zero/few-shot learning.},
	language = {en-US},
	urldate = {2024-05-20},
	publisher = {arXiv},
	author = {Kargaran, Amir Hossein and Nikeghbal, Nafiseh and Heydarnoori, Abbas and Sch√ºtze, Hinrich},
	month = jul,
	year = {2023},
	note = {arXiv:2303.04496 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Human-Computer Interaction, ccfInfo: CCF-None CORR, citationNumber: 2},
}

@misc{liu_chatting_2023,
	title = {Chatting with {GPT}-3 for {Zero}-{Shot} {Human}-{Like} {Mobile} {Automated} {GUI} {Testing}},
	url = {http://arxiv.org/abs/2305.09434},
	doi = {10.48550/arXiv.2305.09434},
	abstract = {Mobile apps are indispensable for people's daily life, and automated GUI (Graphical User Interface) testing is widely used for app quality assurance. There is a growing interest in using learning-based techniques for automated GUI testing which aims at generating human-like actions and interactions. However, the limitations such as low testing coverage, weak generalization, and heavy reliance on training data, make an urgent need for a more effective approach to generate human-like actions to thoroughly test mobile apps. Inspired by the success of the Large Language Model (LLM), e.g., GPT-3 and ChatGPT, in natural language understanding and question answering, we formulate the mobile GUI testing problem as a Q\&A task. We propose GPTDroid, asking LLM to chat with the mobile apps by passing the GUI page information to LLM to elicit testing scripts, and executing them to keep passing the app feedback to LLM, iterating the whole process. Within it, we extract the static context of the GUI page and the dynamic context of the iterative testing process, design prompts for inputting this information to LLM, and develop a neural matching network to decode the LLM's output into actionable steps to execute the app. We evaluate GPTDroid on 86 apps from Google Play, and its activity coverage is 71\%, with 32\% higher than the best baseline, and can detect 36\% more bugs with faster speed than the best baseline. GPTDroid also detects 48 new bugs on the Google Play with 25 of them being confirmed/fixed. We further summarize the capabilities of GPTDroid behind the superior performance, including semantic text input, compound action, long meaningful test trace, and test case prioritization.},
	language = {en-US},
	urldate = {2024-05-20},
	publisher = {arXiv},
	author = {Liu, Zhe and Chen, Chunyang and Wang, Junjie and Chen, Mengzhuo and Wu, Boyu and Che, Xing and Wang, Dandan and Wang, Qing},
	month = may,
	year = {2023},
	note = {arXiv:2305.09434 [cs]},
	keywords = {Computer Science - Software Engineering, ccfInfo: CCF-None CORR, citationNumber: 8},
}

@article{boian_conversational_2024,
	title = {A conversational agent framework for mental health screening: design, implementation, and usability},
	volume = {0},
	issn = {0144-929X},
	shorttitle = {A conversational agent framework for mental health screening},
	url = {https://doi.org/10.1080/0144929X.2024.2332934},
	doi = {10.1080/0144929X.2024.2332934},
	abstract = {While chatbots show promise for large-scale mental health screening, few offer interactive, free-text conversations, limiting their appeal for self-administered screening and impeding the timely detection of mental health issues. This study introduces an AI-based chatbot that allows users to respond to validated screening surveys for mental disorders (PHQ-9, GAD-7, and PCL-5) in a natural, free-text conversation manner with real-time feedback. The study's objectives include evaluating the chatbot's usability and reducing the frequency of response clarifications while accurately interpreting users‚Äô responses. The system was assessed running in hybrid NLU mode (Phase 2; N = 587; Mage = 21.56, SD = 5.56, 67.8\% women) after being trained on data collected while running in rule-based mode (Phase 1; N = 274; Mage = 21.86, SD = 5.50). During user-chatbot interactions, the chatbot required clarification only 4.64\% of the time. Using the AI NLU model, the chatbot could understand user responses in 85.65\% of cases and interpret free-text similarly to human annotators. In terms of usability, the chatbot in hybrid NLU mode was perceived as more engaging, friendly, and easier to use than in the rule-based NLU mode, which may be indirectly attributed to the enhanced autonomy provided by the AI NLU model.},
	language = {en-US},
	number = {0},
	urldate = {2024-05-24},
	journal = {Behaviour \& Information Technology},
	author = {Boian, Rares and Bucur, Ana-Maria and Todea, Diana and Luca, Andreea and Rebedea, Traian and Podina, Ioana R.},
	year = {2024},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/0144929X.2024.2332934},
	keywords = {Chatbot, artificial intelligence, ccfInfo: Not Found, citationNumber: 0, conversational agent, machine learning, mental health screening, natural language understanding},
	pages = {1--15},
}

@article{chita-tegmark_assistive_2021,
	title = {Assistive {Robots} for the {Social} {Management} of {Health}: {A} {Framework} for {Robot} {Design} and {Human}‚Äì{Robot} {Interaction} {Research}},
	volume = {13},
	issn = {1875-4805},
	shorttitle = {Assistive {Robots} for the {Social} {Management} of {Health}},
	url = {https://doi.org/10.1007/s12369-020-00634-z},
	doi = {10.1007/s12369-020-00634-z},
	abstract = {There is a close connection between health and the quality of one‚Äôs social life. Strong social bonds are essential for health and wellbeing, but often health conditions can detrimentally affect a person‚Äôs ability to interact with others. This can become a vicious cycle resulting in further decline in health. For this reason, the social management of health is an important aspect of healthcare. We propose that socially assistive robots (SARs) could help people with health conditions maintain positive social lives by supporting them in social interactions. This paper makes three contributions, as detailed below. We develop a framework of social mediation functions that robots could perform, motivated by the special social needs that people with health conditions have. In this framework we identify five types of functions that SARs could perform: (a) changing how the person is perceived, (b) enhancing the social behavior of the person, (c) modifying the social behavior of others, (d) providing structure for interactions, and (e) changing how the person feels. We thematically organize and review the existing literature on robots supporting human‚Äìhuman interactions, in both clinical and non-clinical settings, and explain how the findings and design ideas from these studies can be applied to the functions identified in the framework. Finally, we point out and discuss challenges in designing SARs for supporting social interactions, and highlight opportunities for future robot design and HRI research on the mediator role of robots.},
	language = {en},
	number = {2},
	urldate = {2024-05-24},
	journal = {International Journal of Social Robotics},
	author = {Chita-Tegmark, Meia and Scheutz, Matthias},
	month = apr,
	year = {2021},
	keywords = {Health management, Human robot interaction, Socially assistive robots, ccfInfo: CCF-None IJSR, citationNumber: 26},
	pages = {197--217},
}

@inproceedings{plurkowski_implications_2011,
	title = {The {Implications} of {Interactional} "{Repair}" for {Human}-{Robot} {Interaction} {Design}},
	volume = {3},
	url = {https://ieeexplore.ieee.org/document/6040806},
	doi = {10.1109/WI-IAT.2011.213},
	abstract = {This paper recaps a recent study in the organization of interactive practices utilized by humans in card-game activities for the purposes of informing the design of human-robot interaction with autonomous social robotic systems. The research utilized an applied Conversation Analytic (CA) approach to focus on the practices by which humans identified and dealt with interactional trouble within the game activity itself, avoiding breakdown of the interactional encounter. We conclude this paper with a brief discussion of the technical implications for future design of autonomous social robots.},
	urldate = {2024-05-24},
	booktitle = {2011 {IEEE}/{WIC}/{ACM} {International} {Conferences} on {Web} {Intelligence} and {Intelligent} {Agent} {Technology}},
	author = {Plurkowski, Luke and Chu, Maurice and Vinkhuyzen, Erik},
	month = aug,
	year = {2011},
	keywords = {Cameras, Games, Humans, Maintenance engineering, Monitoring, Organizations, Robots, ccfInfo: CCF-None IAT, citationNumber: 4, conversation analysis, repair practices, robot design, social interaction, social robots, social science, video analysis},
	pages = {61--65},
}

@misc{noauthor_design_nodate,
	title = {Design {Implications} for {Effective} {Robot} {Gaze} {Behaviors} in {Multiparty} {Interactions} {\textbar} {IEEE} {Conference} {Publication} {\textbar} {IEEE} {Xplore}},
	url = {https://ieeexplore.ieee.org/document/9889481},
	language = {en-US},
	urldate = {2024-05-24},
	keywords = {ccfInfo: Not Found, citationNumber: Not Found},
}

@article{hu_humanrobot_2023,
	title = {Human‚Äìrobot interface design ‚Äì the ‚Äò{Robot} with a {Tablet}‚Äô or ‚Äò{Robot} only‚Äô, which one is better?},
	volume = {42},
	issn = {0144-929X},
	url = {https://doi.org/10.1080/0144929X.2022.2093271},
	doi = {10.1080/0144929X.2022.2093271},
	abstract = {The new era of integrating robots with artificial intelligence and the Internet of Things has provided huge potential for service robots to play important roles in our daily lives. Therefore, it is interesting to investigate whether a robot should have a separate device to incorporate digital content or use its face to toggle between facial expressions and digital content. From the human‚Äìrobot interface (HRI) design perspective, a robot with a tablet obviously is closer to a real human teacher using a tablet while teaching students in a classroom. However, limited research has been done to explore such HRI design and its impacts on human behaviours. A robot-facilitated storytelling learning system was developed in this study to evaluate the impact of HRI design on college students‚Äô learning behaviours in two different conditions: ‚ÄòRobot with Tablet‚Äô or ‚ÄòRobot only‚Äô. The results revealed that both learning conditions of HRI design benefited college students from knowledge acquisition of vocabulary and comprehension of story content. Furthermore, ‚ÄòRobot only‚Äô which is more cost-effective was found to be as good as that of ‚ÄòRobot with Tablet‚Äô for the students in terms of knowledge acquisition, cognitive load and learning fatigue.},
	language = {en-US},
	number = {10},
	urldate = {2024-02-23},
	journal = {Behaviour \& Information Technology},
	author = {Hu, Chih-Chien and Yang, Yu-Fen and Chen, Nian-Shing},
	month = jul,
	year = {2023},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/0144929X.2022.2093271},
	keywords = {Cognitive load, ccfInfo: CCF-C BIT, citationNumber: 0, human‚Äìrobot interface, knowledge acquisition, learning fatigue, robot-facilitated learning},
	pages = {1590--1603},
}

@article{ripa_generating_2023,
	title = {Generating voice user interfaces from web sites},
	volume = {0},
	issn = {0144-929X},
	url = {https://doi.org/10.1080/0144929X.2023.2272192},
	doi = {10.1080/0144929X.2023.2272192},
	abstract = {Virtual assistants allow users to interact with apps using a conversational mode. A particular kind of device supporting virtual assistants is the smart speaker, which allows end users to access contents and services, perform searches and command smart environments via voice interaction. These devices are gaining relevance among users, and the number of apps available for giving them more conversational capabilities grows constantly. However, there is a gap between what can be achieved using these devices and the information available on the web, i.e. most web apps don't have one smart speaker app counterpart. In this work, we present a two-step approach to defining conversational interfaces for virtual assistants based on existing web sites. During the first step, the user creates web content blocks, which describe target contents and the strategy for retrieving them. In the second step, the user uses web content blocks to specify the conversational interface. Although the approach could be considered generic enough for any kind of conversational interface, we focus our research on voice user interfaces, considering smart speaker apps as a major target. With our approach, apps based on voice user interfaces for smart speakers may be developed in a no-code manner. We describe and illustrate the approach by presenting usage examples and an evaluation using the Alexa service and an Amazon Echo device. The evaluation shows promising results for two case studies where there is no significant difference between the user experience when comparing solutions developed using the Alexa SDK versus apps using our approach.},
	language = {en-US},
	number = {0},
	urldate = {2024-05-24},
	journal = {Behaviour \& Information Technology},
	author = {Ripa, Gonzalo and Torre, Manuel and Urbieta, Matias and Rossi, Gustavo and Fernandez, Alejandro and Tacuri, Alex and Firmenich, Sergio},
	year = {2023},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/0144929X.2023.2272192},
	keywords = {Virtual assistants, ccfInfo: CCF-C BIT, citationNumber: 0, conversational interfaces, web apps, ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê},
	pages = {1--24},
}

@article{kwon_um_2023,
	title = {‚Äò{Um}, so like, is this how {I} speak?‚Äô: design implications for automated visual feedback systems on speech},
	volume = {0},
	issn = {0144-929X},
	shorttitle = {‚Äò{Um}, so like, is this how {I} speak?},
	url = {https://doi.org/10.1080/0144929X.2023.2271997},
	doi = {10.1080/0144929X.2023.2271997},
	abstract = {At a time when diverse forms of automated visual feedback systems on speech are introduced to the Human‚ÄìComputer Interaction (HCI) community, our research investigates the possible design spaces and design implications to consider when creating them. We utilised a Wizard of Oz prototype that recognises fillers during speech and sends out real-time feedback in the form of a flickering screen and a post-report with graphic charts. We let 27 participants use this prototype and interviewed them about their general experience of using such systems. Based on thematic analysis, we propose eight design implications to consider when designing an automated visual feedback system on speech. Additionally, we discuss the possible future for automated visual feedback systems on speech where they collaborate with fields such as personal informatics (quantified self), self-determination theory (SDT) and motivation, and other socio-ethnic areas},
	language = {en-US},
	number = {0},
	urldate = {2024-05-24},
	journal = {Behaviour \& Information Technology},
	author = {Kwon, Soonho and Lee, Hyojin and Park, Soojeong and Heo, Yerin and Lee, Keeheon and Kang, Younah},
	year = {2023},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/0144929X.2023.2271997},
	keywords = {Human-computer interaction, ccfInfo: CCF-C BIT, citationNumber: 0, feedback systems, informatics, self-tracking, speech feedback system},
	pages = {1--20},
}

@article{ren_emovis_2024,
	title = {{EmoVis}: exploring data-enabled analogue journaling to promote self-reflection for mental wellness among college students},
	volume = {0},
	issn = {0144-929X},
	shorttitle = {{EmoVis}},
	url = {https://doi.org/10.1080/0144929X.2024.2349182},
	doi = {10.1080/0144929X.2024.2349182},
	abstract = {College students are known to face many stressors, making them one of the most vulnerable groups to mental disorders. To tackle such an issue, this paper presents a design study that investigated data-enabled analogue journaling (DEAJ) to help college students improve their mental wellness through paper-based self-reflection aided by automatic self-tracking. Specifically, based on several design iterations we developed a DEAJ tool, called EmoVis, which could generate a printed visualisation based on daily physiological data and event tags to support doodling or structured writing in analogue journaling for everyday stress management. We conducted a six-day mixed methods field study with 32 college students to evaluate the effectiveness and user experience of EmoVis. Results suggested that EmoVis can significantly improve the engagement and need for self-reflection due to context-based reflection and data-enabled exploratory journaling. Furthermore, the doodling canvas was experienced as a joyful tool for mindfulness and creative DEAJ, while reflecting with a writing template was perceived as efficient. The paper concludes with a discussion of the implications of DEAJ approaches for daily stress coping in students‚Äô college life.},
	language = {en-US},
	number = {0},
	urldate = {2024-05-24},
	journal = {Behaviour \& Information Technology},
	author = {Ren, Xipei and Zhang, Xiaoyu and Zou, Renyao and Yan, Ran and Yu, Bin},
	year = {2024},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/0144929X.2024.2349182},
	keywords = {Analogue journaling, ccfInfo: Not Found, citationNumber: 0, college student, data-enabled design, everyday stress management, self-reflection, üö©},
	pages = {1--23},
}

@article{goetz_keep_2023,
	title = {Keep it real, keep it simple: the effects of icon characteristics on visual search},
	volume = {0},
	issn = {0144-929X},
	shorttitle = {Keep it real, keep it simple},
	url = {https://doi.org/10.1080/0144929X.2023.2286527},
	doi = {10.1080/0144929X.2023.2286527},
	abstract = {Previous research examining how icons‚Äô concreteness, visual complexity, and distinctiveness influence visual search performance have led to disagreements over which icon characteristic most affects behaviour. These icon characteristics are often poorly defined and interrelated, particularly concreteness. Accordingly, drawing strong inferences about the robustness of concreteness as a factor in search for visual icons is challenging. Here, we operationalised concreteness into three distinct levels: concrete icons were images of real-world objects, photorealistic icons were drawings of the object, and abstract icons were images with no conceptual information. Across two experiments, participants rated each icon on various icon characteristics (e.g. concreteness, visual complexity) to provide a ground truth for these factors and to validate our concreteness manipulation. In a separate study, naive participants performed a visual search task for a target icon. Oculomotor measures were utilised to elucidate how various icon characteristics affected search performance. Although we were unable to fully disassociate concreteness from visual complexity, we found that icons high in concreteness improved search performance, but as visual complexity increased, object identification became slower. This was largely demonstrated through increased verification times for complex targets. The present set of studies indicate that highly concrete and simple icons engender search benefits.},
	language = {en-US},
	number = {0},
	urldate = {2024-05-24},
	journal = {Behaviour \& Information Technology},
	author = {Goetz, Jessica N. and Neider, Mark B.},
	year = {2023},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/0144929X.2023.2286527},
	keywords = {Display design principles, ccfInfo: Not Found, citationNumber: 0, eye movements/tracking, graphical user interfaces (GUI), pictorial displays},
	pages = {1--20},
}

@article{yang_co-creation_2023,
	title = {Co-creation with service robots and employee wellbeing: a self-determination perspective},
	volume = {0},
	issn = {0144-929X},
	shorttitle = {Co-creation with service robots and employee wellbeing},
	url = {https://doi.org/10.1080/0144929X.2023.2295032},
	doi = {10.1080/0144929X.2023.2295032},
	abstract = {Service robots have been widely utilised in service encounters to deliver human services. However, it is unclear whether co-creation with service robots increases or decreases employee wellbeing. Based on self-determination theory, this study aims to investigate the direct and indirect effects of co-creation with service robots on employee wellbeing. An online survey was used to collect data from 332 employees working in the hospitality and tourism industry in China who had co-created with service robots at work. The results indicate that co-creation with service robots has an important autonomy-enhancing effect on employees. Furthermore, the relationship between co-creation with service robots and employee wellbeing through the need for competence and the need for relatedness is also mediated by the need for autonomy. This study extends self-determination theory to the domain of service robots by showing that employees‚Äô basic psychological needs play an important role in shaping their wellbeing in the context of co-creation with service robots at work.},
	language = {en-US},
	number = {0},
	urldate = {2024-05-24},
	journal = {Behaviour \& Information Technology},
	author = {Yang, Xue and Gao, Youjiang},
	year = {2023},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/0144929X.2023.2295032},
	keywords = {Service robots, ccfInfo: Not Found, citationNumber: 0, human-robot co-creation, self-determination theory, wellbeing},
	pages = {1--12},
}

@article{koek_my_2024,
	title = {My avatar makes me feel good? {The} effect of avatar personalisation and virtual agent interactions on self-esteem},
	volume = {0},
	issn = {0144-929X},
	shorttitle = {My avatar makes me feel good?},
	url = {https://doi.org/10.1080/0144929X.2024.2349176},
	doi = {10.1080/0144929X.2024.2349176},
	abstract = {The theory of Objective Self-Awareness (OSA) and related studies suggest that embodiment of personalised avatars may induce self-awareness and influence self-esteem. Additionally, the Computers Are Social Actors (CASA) paradigm suggests that humans may mindlessly respond to computers in ways that are similar to human interactions. Based on those assertions, it is plausible that virtual embodiment of a personalised avatar and interactions with a virtual agent can shift self-esteem. However, those effects on self-esteem have not been thoroughly examined in past studies. To address these research gaps, a 2 (avatar personalisation: personalised vs. non-personalised avatar) √ó 2 (virtual agent interaction valence: positive vs. negative) between-subjects experiment was conducted using a Virtual Reality (VR) simulation (N =171). Findings from the study showed that there was no effects of avatar personalisation and virtual agent interaction valence on state self-esteem change. However, the pairwise comparisons present some preliminary indications that avatar personalisation and positive interactions with a virtual agent may facilitate improvements in state self-esteem altogether. Implications of the study findings are discussed.},
	language = {en-US},
	number = {0},
	urldate = {2024-05-24},
	journal = {Behaviour \& Information Technology},
	author = {Koek, Wei Jie Dominic and Chen, Vivian Hsueh Hua},
	year = {2024},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/0144929X.2024.2349176},
	keywords = {Virtual reality, avatars, ccfInfo: Not Found, citationNumber: 0, human‚Äìcomputer interaction, personalisation, virtual interactions, well-being},
	pages = {1--16},
}

@misc{yuan_maxprototyper_2024,
	title = {{MAxPrototyper}: {A} {Multi}-{Agent} {Generation} {System} for {Interactive} {User} {Interface} {Prototyping}},
	shorttitle = {{MAxPrototyper}},
	url = {http://arxiv.org/abs/2405.07131},
	doi = {10.48550/arXiv.2405.07131},
	abstract = {In automated user interactive design, designers face key challenges, including accurate representation of user intent, crafting high-quality components, and ensuring both aesthetic and semantic consistency. Addressing these challenges, we introduce MAxPrototyper, our human-centered, multi-agent system for interactive design generation. The core of MAxPrototyper is a theme design agent. It coordinates with specialized sub-agents, each responsible for generating specific parts of the design. Through an intuitive online interface, users can control the design process by providing text descriptions and layout. Enhanced by improved language and image generation models, MAxPrototyper generates each component with careful detail and contextual understanding. Its multi-agent architecture enables a multi-round interaction capability between the system and users, facilitating precise and customized design adjustments throughout the creation process.},
	language = {en-US},
	urldate = {2024-08-02},
	publisher = {arXiv},
	author = {Yuan, Mingyue and Chen, Jieshan and Quigley, Aaron},
	month = may,
	year = {2024},
	note = {arXiv:2405.07131 [cs]},
	keywords = {\#EX: ‰∏çÂåÖÂê´HRI, \#EX:‰∏çÂåÖÂê´Â§ßÊ®°Âûã, \#EX:Â∑•‰∏öÊú∫Âô®‰∫∫, \#‰∏çÁ°ÆÂÆö, \#Áü≠Êñá, \#ÁªºËø∞, /unread, Computer Science - Human-Computer Interaction, Computer Science - Multiagent Systems, Design, Users, ccfInfo: CCF-None CORR, citationNumber: 0},
}

@inproceedings{chung_promptpaint_2023,
	address = {New York, NY, USA},
	series = {{UIST} '23},
	title = {{PromptPaint}: {Steering} {Text}-to-{Image} {Generation} {Through} {Paint} {Medium}-like {Interactions}},
	isbn = {979-8-4007-0132-0},
	shorttitle = {{PromptPaint}},
	url = {https://doi.org/10.1145/3586183.3606777},
	doi = {10.1145/3586183.3606777},
	abstract = {While diffusion-based text-to-image (T2I) models provide a simple and powerful way to generate images, guiding this generation remains a challenge. For concepts that are difficult to describe through language, users may struggle to create prompts. Moreover, many of these models are built as end-to-end systems, lacking support for iterative shaping of the image. In response, we introduce PromptPaint, which combines T2I generation with interactions that model how we use colored paints. PromptPaint allows users to go beyond language to mix prompts that express challenging concepts. Just as we iteratively tune colors through layered placements of paint on a physical canvas, PromptPaint similarly allows users to apply different prompts to different canvas areas and times of the generative process. Through a set of studies, we characterize different approaches for mixing prompts, design trade-offs, and socio-technical challenges for generative models. With PromptPaint we provide insight into future steerable generative tools.},
	language = {en-US},
	urldate = {2024-08-01},
	booktitle = {Proceedings of the 36th {Annual} {ACM} {Symposium} on {User} {Interface} {Software} and {Technology}},
	publisher = {Association for Computing Machinery},
	author = {Chung, John Joon Young and Adar, Eytan},
	month = oct,
	year = {2023},
	keywords = {\#EX: ‰∏çÂåÖÂê´HRI, \#EX:‰∏çÂåÖÂê´Â§ßÊ®°Âûã, \#EX:Â∑•‰∏öÊú∫Âô®‰∫∫, \#‰∏çÁ°ÆÂÆö, \#Áü≠Êñá, \#ÁªºËø∞, /unread, Design, Users, ccfInfo: CCF-A UIST, citationNumber: 0},
	pages = {1--17},
}

@misc{naderi_foundation_2024,
	title = {Foundation {Models} for {Autonomous} {Robots} in {Unstructured} {Environments}},
	url = {http://arxiv.org/abs/2407.14296},
	doi = {10.48550/arXiv.2407.14296},
	abstract = {Automating activities through robots in unstructured environments, such as construction sites, has been a long-standing desire. However, the high degree of unpredictable events in these settings has resulted in far less adoption compared to more structured settings, such as manufacturing, where robots can be hard-coded or trained on narrowly defined datasets. Recently, pretrained foundation models, such as Large Language Models (LLMs), have demonstrated superior generalization capabilities by providing zero-shot solutions for problems do not present in the training data, proposing them as a potential solution for introducing robots to unstructured environments. To this end, this study investigates potential opportunities and challenges of pretrained foundation models from a multi-dimensional perspective. The study systematically reviews application of foundation models in two field of robotic and unstructured environment and then synthesized them with deliberative acting theory. Findings showed that linguistic capabilities of LLMs have been utilized more than other features for improving perception in human-robot interactions. On the other hand, findings showed that the use of LLMs demonstrated more applications in project management and safety in construction, and natural hazard detection in disaster management. Synthesizing these findings, we located the current state-of-the-art in this field on a five-level scale of automation, placing them at conditional automation. This assessment was then used to envision future scenarios, challenges, and solutions toward autonomous safe unstructured environments. Our study can be seen as a benchmark to track our progress toward that future.},
	language = {en-US},
	urldate = {2024-07-29},
	publisher = {arXiv},
	author = {Naderi, Hossein and Shojaei, Alireza and Huang, Lifu},
	month = jul,
	year = {2024},
	note = {arXiv:2407.14296 [cs]},
	keywords = {\#EX: ‰∏çÂåÖÂê´HRI, \#EX:‰∏çÂåÖÂê´Â§ßÊ®°Âûã, \#EX:Â∑•‰∏öÊú∫Âô®‰∫∫, \#‰∏çÁ°ÆÂÆö, \#Áü≠Êñá, \#ÁªºËø∞, /unread, Computer Science - Computation and Language, Computer Science - Robotics, Design, Users, ccfInfo: CCF-None CORR, citationNumber: 0},
}

@misc{tanneberg_help_2024,
	title = {To {Help} or {Not} to {Help}: {LLM}-based {Attentive} {Support} for {Human}-{Robot} {Group} {Interactions}},
	shorttitle = {To {Help} or {Not} to {Help}},
	url = {http://arxiv.org/abs/2403.12533},
	doi = {10.48550/arXiv.2403.12533},
	abstract = {How can a robot provide unobtrusive physical support within a group of humans? We present Attentive Support, a novel interaction concept for robots to support a group of humans. It combines scene perception, dialogue acquisition, situation understanding, and behavior generation with the common-sense reasoning capabilities of Large Language Models (LLMs). In addition to following user instructions, Attentive Support is capable of deciding when and how to support the humans, and when to remain silent to not disturb the group. With a diverse set of scenarios, we show and evaluate the robot's attentive behavior, which supports and helps the humans when required, while not disturbing if no help is needed.},
	language = {en-US},
	urldate = {2024-07-29},
	publisher = {arXiv},
	author = {Tanneberg, Daniel and Ocker, Felix and Hasler, Stephan and Deigmoeller, Joerg and Belardinelli, Anna and Wang, Chao and Wersing, Heiko and Sendhoff, Bernhard and Gienger, Michael},
	month = jul,
	year = {2024},
	note = {arXiv:2403.12533 [cs]},
	keywords = {\#EX: ‰∏çÂåÖÂê´HRI, \#EX:‰∏çÂåÖÂê´Â§ßÊ®°Âûã, \#EX:Â∑•‰∏öÊú∫Âô®‰∫∫, \#‰∏çÁ°ÆÂÆö, \#Áü≠Êñá, \#ÁªºËø∞, /unread, Computer Science - Artificial Intelligence, Computer Science - Robotics, Design, I.2.8, I.2.9, Users, ccfInfo: CCF-None CORR, citationNumber: 0},
}

@misc{feng_large_2024,
	title = {Large {Language} {Model}-based {Human}-{Agent} {Collaboration} for {Complex} {Task} {Solving}},
	url = {http://arxiv.org/abs/2402.12914},
	doi = {10.48550/arXiv.2402.12914},
	abstract = {In recent developments within the research community, the integration of Large Language Models (LLMs) in creating fully autonomous agents has garnered significant interest. Despite this, LLM-based agents frequently demonstrate notable shortcomings in adjusting to dynamic environments and fully grasping human needs. In this work, we introduce the problem of LLM-based human-agent collaboration for complex task-solving, exploring their synergistic potential. In addition, we propose a Reinforcement Learning-based Human-Agent Collaboration method, ReHAC. This approach includes a policy model designed to determine the most opportune stages for human intervention within the task-solving process. We construct a human-agent collaboration dataset to train this policy model in an offline reinforcement learning environment. Our validation tests confirm the model's effectiveness. The results demonstrate that the synergistic efforts of humans and LLM-based agents significantly improve performance in complex tasks, primarily through well-planned, limited human intervention. Datasets and code are available at: https://github.com/XueyangFeng/ReHAC.},
	urldate = {2024-07-29},
	publisher = {arXiv},
	author = {Feng, Xueyang and Chen, Zhi-Yuan and Qin, Yujia and Lin, Yankai and Chen, Xu and Liu, Zhiyuan and Wen, Ji-Rong},
	month = feb,
	year = {2024},
	note = {arXiv:2402.12914 [cs]},
	keywords = {\#EX: ‰∏çÂåÖÂê´HRI, \#EX:‰∏çÂåÖÂê´Â§ßÊ®°Âûã, \#EX:Â∑•‰∏öÊú∫Âô®‰∫∫, \#‰∏çÁ°ÆÂÆö, \#Áü≠Êñá, \#ÁªºËø∞, /unread, Computer Science - Computation and Language, Computer Science - Human-Computer Interaction, Design, Users, ccfInfo: CCF-B EMNLP, citationNumber: 0},
}

@inproceedings{kim_metaphorian_2023,
	address = {New York, NY, USA},
	series = {{DIS} '23},
	title = {Metaphorian: {Leveraging} {Large} {Language} {Models} to {Support} {Extended} {Metaphor} {Creation} for {Science} {Writing}},
	isbn = {978-1-4503-9893-0},
	shorttitle = {Metaphorian},
	url = {https://dl.acm.org/doi/10.1145/3563657.3595996},
	doi = {10.1145/3563657.3595996},
	abstract = {Science writers commonly use extended metaphors to communicate unfamiliar concepts in a more accessible way to a wider audience. However, creating metaphors for science writing is challenging even for professional writers; according to our formative study (n=6), finding inspiration and extending metaphors with coherent structures were critical yet significantly challenging tasks for them. We contribute Metaphorian, a system that supports science writers with the creation of scientific metaphors by facilitating the search, extension, and iterative revision of metaphors. Metaphorian uses a large language model-based workflow inspired by the heuristic rules revealed from a study with six professional writers. A user study (n=16) revealed that Metaphorian significantly enhances satisfaction, confidence, and inspiration in metaphor writing without decreasing writers‚Äô sense of agency. We discuss design implications for creativity support for figurative writing in science.},
	language = {en-US},
	urldate = {2023-09-01},
	booktitle = {Proceedings of the 2023 {ACM} {Designing} {Interactive} {Systems} {Conference}},
	publisher = {Association for Computing Machinery},
	author = {Kim, Jeongyeon and Suh, Sangho and Chilton, Lydia B and Xia, Haijun},
	month = jul,
	year = {2023},
	keywords = {\#EX: ‰∏çÂåÖÂê´HRI, \#EX:‰∏çÂåÖÂê´Â§ßÊ®°Âûã, \#EX:Â∑•‰∏öÊú∫Âô®‰∫∫, \#‰∏çÁ°ÆÂÆö, \#Áü≠Êñá, \#ÁªºËø∞, /unread, Creativity Support Tools, Design, GPT-3, Large Language Model, Metaphors, Science Writing, Users, Writing Support, ccfInfo: CCF-C DIS, citationNumber: 2, ‚≠ê, ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê},
	pages = {115--135},
}

@incollection{kramer_human-agent_2012,
	address = {Berlin, Heidelberg},
	series = {Studies in {Computational} {Intelligence}},
	title = {Human-{Agent} and {Human}-{Robot} {Interaction} {Theory}: {Similarities} to and {Differences} from {Human}-{Human} {Interaction}},
	isbn = {978-3-642-25691-2},
	shorttitle = {Human-{Agent} and {Human}-{Robot} {Interaction} {Theory}},
	url = {https://doi.org/10.1007/978-3-642-25691-2_9},
	abstract = {It will be discussed whether a theory specific for human-robot and human-agent interaction is needed or whether theories from human-human interactions can be adapted. First, theories from human-human interaction will be presented. Then, empirical evidence from human-robot- and human-agent interaction is presented. Research suggests that, from the perspective of the user, interaction with an artificial entity is similar to interaction with fellow humans. Explanations for this treatment of agents/robots in a social way (such as the ethopoeia approach, Nass\& Moon, 2000) assume that due to our social nature humans will use their interaction routines also when confronted with artificial entities. Based on this it will be discussed whether theories from human-human-interaction will be a helpful framework also for human-agent/robot interaction, whether amendments will be beneficial or whether, alternatively, a totally new approach is needed.},
	language = {en},
	urldate = {2022-09-22},
	booktitle = {Human-{Computer} {Interaction}: {The} {Agency} {Perspective}},
	publisher = {Springer},
	author = {Kr√§mer, Nicole C. and von der P√ºtten, Astrid and Eimler, Sabrina},
	editor = {Zacarias, Marielba and de Oliveira, Jos√© Valente},
	year = {2012},
	doi = {10.1007/978-3-642-25691-2_9},
	keywords = {\#EX: ‰∏çÂåÖÂê´HRI, \#EX:‰∏çÂåÖÂê´Â§ßÊ®°Âûã, \#EX:Â∑•‰∏öÊú∫Âô®‰∫∫, \#‰∏çÁ°ÆÂÆö, \#Áü≠Êñá, \#ÁªºËø∞, /unread, Common Ground, Design, Perspective Taking, ReadList, Social Exchange Theory, Users, Virtual Agent, Virtual Human, ccfInfo: CCF-None SCI, citationNumber: 53, üö©},
	pages = {215--240},
}

@inproceedings{sarkar_will_2023,
	address = {New York, NY, USA},
	series = {Onward! 2023},
	title = {Will {Code} {Remain} a {Relevant} {User} {Interface} for {End}-{User} {Programming} with {Generative} {AI} {Models}?},
	isbn = {979-8-4007-0388-1},
	url = {https://dl.acm.org/doi/10.1145/3622758.3622882},
	doi = {10.1145/3622758.3622882},
	abstract = {The research field of end-user programming has largely been concerned with helping non-experts learn to code sufficiently well in order to achieve their tasks. Generative AI stands to obviate this entirely by allowing users to generate code from naturalistic language prompts. In this essay, we explore the extent to which "traditional" programming languages remain relevant for non-expert end-user programmers in a world with generative AI. We posit the "generative shift hypothesis": that generative AI will create qualitative and quantitative expansions in the traditional scope of end-user programming. We outline some reasons that traditional programming languages may still be relevant and useful for end-user programmers. We speculate whether each of these reasons might be fundamental and enduring, or whether they may disappear with further improvements and innovations in generative AI. Finally, we articulate a set of implications for end-user programming research, including the possibility of needing to revisit many well-established core concepts, such as Ko's learning barriers and Blackwell's attention investment model.},
	language = {en-US},
	urldate = {2024-08-28},
	booktitle = {Proceedings of the 2023 {ACM} {SIGPLAN} {International} {Symposium} on {New} {Ideas}, {New} {Paradigms}, and {Reflections} on {Programming} and {Software}},
	publisher = {Association for Computing Machinery},
	author = {Sarkar, Advait},
	month = oct,
	year = {2023},
	keywords = {\#EX: ‰∏çÂåÖÂê´HRI, \#EX:‰∏çÂåÖÂê´Â§ßÊ®°Âûã, \#EX:Â∑•‰∏öÊú∫Âô®‰∫∫, \#‰∏çÁ°ÆÂÆö, \#Áü≠Êñá, \#ÁªºËø∞, /unread, Design, Users, ccfInfo: CCF-None ONWARD, citationNumber: 0},
	pages = {153--167},
}

@inproceedings{sakamoto_sketch_2009,
	address = {New York, NY, USA},
	series = {{CHI} '09},
	title = {Sketch and run: a stroke-based interface for home robots},
	isbn = {978-1-60558-246-7},
	shorttitle = {Sketch and run},
	url = {https://dl.acm.org/doi/10.1145/1518701.1518733},
	doi = {10.1145/1518701.1518733},
	abstract = {Numerous robots have been developed, and some of them are already being used in homes, institutions, and workplaces. Despite the development of useful robot functions, the focus so far has not been on user interfaces of robots. General users of robots find it hard to understand what the robots are doing and what kind of work they can do. This paper presents an interface for the commanding home robots by using stroke gestures on a computer screen. This interface allows the user to control robots and design their behaviors by sketching the robot's behaviors and actions on a top-down view from ceiling cameras. To convey a feeling of directly controlling the robots, our interface employs the live camera view. In this study, we focused on a house-cleaning task that is typical of home robots, and developed a sketch interface for designing behaviors of vacuuming robots.},
	language = {en-US},
	urldate = {2024-08-14},
	booktitle = {Proceedings of the {SIGCHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Sakamoto, Daisuke and Honda, Koichiro and Inami, Masahiko and Igarashi, Takeo},
	month = apr,
	year = {2009},
	keywords = {\#EX: ‰∏çÂåÖÂê´HRI, \#EX:‰∏çÂåÖÂê´Â§ßÊ®°Âûã, \#EX:Â∑•‰∏öÊú∫Âô®‰∫∫, \#‰∏çÁ°ÆÂÆö, \#Áü≠Êñá, \#ÁªºËø∞, /unread, Design, Users, ccfInfo: CCF-A CHI, citationNumber: 137},
	pages = {197--200},
}

@inproceedings{huang_vipo_2020,
	address = {Honolulu HI USA},
	title = {Vipo: {Spatial}-{Visual} {Programming} with {Functions} for {Robot}-{IoT} {Workflows}},
	isbn = {978-1-4503-6708-0},
	shorttitle = {Vipo},
	url = {https://dl.acm.org/doi/10.1145/3313831.3376670},
	doi = {10.1145/3313831.3376670},
	language = {en},
	urldate = {2024-08-14},
	booktitle = {Proceedings of the 2020 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Huang, Gaoping and Rao, Pawan S. and Wu, Meng-Han and Qian, Xun and Nof, Shimon Y. and Ramani, Karthik and Quinn, Alexander J.},
	month = apr,
	year = {2020},
	keywords = {\#EX: ‰∏çÂåÖÂê´HRI, \#EX:‰∏çÂåÖÂê´Â§ßÊ®°Âûã, \#EX:Â∑•‰∏öÊú∫Âô®‰∫∫, \#‰∏çÁ°ÆÂÆö, \#Áü≠Êñá, \#ÁªºËø∞, /unread, Design, Users, ccfInfo: CCF-A CHI, citationNumber: 27},
	pages = {1--13},
}

@inproceedings{liu_roboshop_2011,
	address = {Vancouver, BC, Canada},
	title = {Roboshop: multi-layered sketching interface for robot housework assignment and management},
	isbn = {978-1-4503-0228-9},
	shorttitle = {Roboshop},
	url = {http://dl.acm.org/citation.cfm?doid=1978942.1979035},
	doi = {10.1145/1978942.1979035},
	abstract = {As various home robots come into homes, the need for efficient robot task management tools is arising. Current tools are designed for controlling individual robots independently, so they are not ideally suitable for assigning coordinated action among multiple robots. To address this problem, we developed a management tool for home robots with a graphical editing interface. The user assigns instructions by selecting a tool from a toolbox and sketching on a bird‚Äôs-eye view of the environment. Layering supports the management of multiple tasks in the same room. Layered graphical representation gives a quick overview of and access to rich information tied to the physical environment. This paper describes the prototype system and reports on our evaluation of the system.},
	language = {en},
	urldate = {2020-05-28},
	booktitle = {Proceedings of the 2011 annual conference on {Human} factors in computing systems - {CHI} '11},
	publisher = {ACM Press},
	author = {Liu, Kexi and Sakamoto, Daisuke and Inami, Masahiko and Igarashi, Takeo},
	year = {2011},
	note = {ECC: 022 
9 citations (Crossref) [2021-03-06]},
	keywords = {\#EX: ‰∏çÂåÖÂê´HRI, \#EX:‰∏çÂåÖÂê´Â§ßÊ®°Âûã, \#EX:Â∑•‰∏öÊú∫Âô®‰∫∫, \#‰∏çÁ°ÆÂÆö, \#Áü≠Êñá, \#ÁªºËø∞, /unread, Design, Users, ccfInfo: CCF-A CHI, citationNumber: 43},
	pages = {647},
}

@inproceedings{wang_realitylens_2022,
	address = {New York, NY, USA},
	series = {{UIST} '22},
	title = {{RealityLens}: {A} {User} {Interface} for {Blending} {Customized} {Physical} {World} {View} into {Virtual} {Reality}},
	isbn = {978-1-4503-9320-1},
	shorttitle = {{RealityLens}},
	url = {https://dl.acm.org/doi/10.1145/3526113.3545686},
	doi = {10.1145/3526113.3545686},
	abstract = {Research has enabled virtual reality (VR) users to interact with the physical world by blending the physical world view into the virtual environment. However, current solutions are designed for specific use cases and hence are not capable of covering users‚Äô varying needs for accessing information about the physical world. This work presents RealityLens, a user interface that allows users to peep into the physical world in VR with the reality lenses they deployed for their needs. For this purpose, we first conducted a preliminary study with experienced VR users to identify users‚Äô needs for interacting with the physical world, which led to a set of features for customizing the scale, placement, and activation method of a reality lens. We evaluated the design in a user study (n=12) and collected the feedback of participants engaged in two VR applications while encountering a range of interventions from the physical world. The results show that users‚Äô VR presence tends to be better preserved when interacting with the physical world with the support of the RealityLens interface.},
	urldate = {2024-08-13},
	booktitle = {Proceedings of the 35th {Annual} {ACM} {Symposium} on {User} {Interface} {Software} and {Technology}},
	publisher = {Association for Computing Machinery},
	author = {Wang, Chiu-Hsuan and Chen, Bing-Yu and Chan, Liwei},
	month = oct,
	year = {2022},
	keywords = {\#EX: ‰∏çÂåÖÂê´HRI, \#EX:‰∏çÂåÖÂê´Â§ßÊ®°Âûã, \#EX:Â∑•‰∏öÊú∫Âô®‰∫∫, \#‰∏çÁ°ÆÂÆö, \#Áü≠Êñá, \#ÁªºËø∞, /unread, Design, Users, ccfInfo: CCF-A UIST, citationNumber: 3, üü†},
	pages = {1--11},
}

@inproceedings{jain_jigsaw_2022,
	address = {New York, NY, USA},
	series = {{ICSE} '22},
	title = {Jigsaw: large language models meet program synthesis},
	isbn = {978-1-4503-9221-1},
	shorttitle = {Jigsaw},
	url = {https://dl.acm.org/doi/10.1145/3510003.3510203},
	doi = {10.1145/3510003.3510203},
	abstract = {Large pre-trained language models such as GPT-3 [10], Codex [11], and Google's language model [7] are now capable of generating code from natural language specifications of programmer intent. We view these developments with a mixture of optimism and caution. On the optimistic side, such large language models have the potential to improve productivity by providing an automated AI pair programmer for every programmer in the world. On the cautionary side, since these large language models do not understand program semantics, they offer no guarantees about quality of the suggested code. In this paper, we present an approach to augment these large language models with post-processing steps based on program analysis and synthesis techniques, that understand the syntax and semantics of programs. Further, we show that such techniques can make use of user feedback and improve with usage. We present our experiences from building and evaluating such a tool Jigsaw, targeted at synthesizing code for using Python Pandas API using multi-modal inputs. Our experience suggests that as these large language models evolve for synthesizing code from intent, Jigsaw has an important role to play in improving the accuracy of the systems.},
	language = {en},
	urldate = {2023-03-28},
	booktitle = {Proceedings of the 44th {International} {Conference} on {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Jain, Naman and Vaidyanath, Skanda and Iyer, Arun and Natarajan, Nagarajan and Parthasarathy, Suresh and Rajamani, Sriram and Sharma, Rahul},
	month = jul,
	year = {2022},
	keywords = {\#EX: ‰∏çÂåÖÂê´HRI, \#EX:‰∏çÂåÖÂê´Â§ßÊ®°Âûã, \#EX:Â∑•‰∏öÊú∫Âô®‰∫∫, \#‰∏çÁ°ÆÂÆö, \#Áü≠Êñá, \#ÁªºËø∞, /unread, Design, Users, ccfInfo: CCF-A ICSE, citationNumber: 0, ‚≠ê},
	pages = {1219--1231},
}

@inproceedings{grigore_talk_2016,
	address = {Cham},
	title = {Talk to {Me}: {Verbal} {Communication} {Improves} {Perceptions} of {Friendship} and {Social} {Presence} in {Human}-{Robot} {Interaction}},
	isbn = {978-3-319-47665-0},
	shorttitle = {Talk to {Me}},
	doi = {10.1007/978-3-319-47665-0_5},
	abstract = {The ability of social agents, be it virtually-embodied avatars or physically-embodied robots, to display social behavior and interact with their users in a natural way represents an important factor in how effective such agents are during interactions. In particular, endowing the agent with effective communicative abilities, well-suited for the target application or task, can make a significant difference in how users perceive the agent, especially when the agent needs to interact in complex social environments. In this work, we consider how two core input communication modalities present in human-robot interaction‚Äîspeech recognition and touch-based selection‚Äîshape users‚Äô perceptions of the agent. We design a short interaction in order to gauge adolescents‚Äô reaction to the input communication modality employed by a robot intended as a long-term companion for motivating them to engage in daily physical activity. A study with n = 52 participants shows that adolescents perceive the robot as more of a friend and more socially present in the speech recognition condition than in the touch-based selection one. Our results highlight the advantages of using speech recognition as an input communication modality even when this represents the less robust choice, and the importance of investigating how to best do so.},
	language = {en},
	booktitle = {Intelligent {Virtual} {Agents}},
	publisher = {Springer International Publishing},
	author = {Grigore, Elena Corina and Pereira, Andre and Zhou, Ian and Wang, David and Scassellati, Brian},
	editor = {Traum, David and Swartout, William and Khooshabeh, Peter and Kopp, Stefan and Scherer, Stefan and Leuski, Anton},
	year = {2016},
	keywords = {\#EX: ‰∏çÂåÖÂê´HRI, \#EX:‰∏çÂåÖÂê´Â§ßÊ®°Âûã, \#EX:Â∑•‰∏öÊú∫Âô®‰∫∫, \#‰∏çÁ°ÆÂÆö, \#Áü≠Êñá, \#ÁªºËø∞, /unread, Design, Human-robot Interaction Scenarios, Physical Activity Motivation, Robust Choice, Social Presence, Statistical Speech Recognition, Users, ccfInfo: CCF-None IVA, citationNumber: 16},
	pages = {51--63},
}

@article{lauria_mobile_2002,
	title = {Mobile robot programming using natural language},
	volume = {38},
	copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
	issn = {09218890},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0921889002001665},
	doi = {10.1016/S0921-8890(02)00166-5},
	language = {en},
	number = {3-4},
	urldate = {2024-09-04},
	journal = {Robotics and Autonomous Systems},
	author = {Lauria, Stanislao and Bugmann, Guido and Kyriacou, Theocharis and Klein, Ewan},
	month = mar,
	year = {2002},
	keywords = {\#EX: ‰∏çÂåÖÂê´HRI, \#EX:‰∏çÂåÖÂê´Â§ßÊ®°Âûã, \#EX:Â∑•‰∏öÊú∫Âô®‰∫∫, \#‰∏çÁ°ÆÂÆö, \#Áü≠Êñá, \#ÁªºËø∞, /unread, Design, Users, ccfInfo: CCF-None RAS, citationNumber: 187},
	pages = {171--181},
}

@misc{connell_verbal_2019,
	title = {Verbal {Programming} of {Robot} {Behavior}},
	url = {http://arxiv.org/abs/1911.09782},
	doi = {10.48550/arXiv.1911.09782},
	abstract = {Home robots may come with many sophisticated built-in abilities, however there will always be a degree of customization needed for each user and environment. Ideally this should be accomplished through one-shot learning, as collecting the large number of examples needed for statistical inference is tedious. A particularly appealing approach is to simply explain to the robot, via speech, what it should be doing. In this paper we describe the ALIA cognitive architecture that is able to effectively incorporate user-supplied advice and prohibitions in this manner. The functioning of the implemented system on a small robot is illustrated by an associated video.},
	language = {en-US},
	urldate = {2024-09-04},
	publisher = {arXiv},
	author = {Connell, Jonathan},
	month = nov,
	year = {2019},
	note = {arXiv:1911.09782 [cs]},
	keywords = {\#EX: ‰∏çÂåÖÂê´HRI, \#EX:‰∏çÂåÖÂê´Â§ßÊ®°Âûã, \#EX:Â∑•‰∏öÊú∫Âô®‰∫∫, \#‰∏çÁ°ÆÂÆö, \#Áü≠Êñá, \#ÁªºËø∞, /unread, Computer Science - Artificial Intelligence, Computer Science - Robotics, Design, Users, ccfInfo: CCF-None CORR, citationNumber: 0},
}

@article{terblanche_talk_2023,
	title = {Talk or {Text}? {The} {Role} of {Communication} {Modalities} in the {Adoption} of a {Non}-directive, {Goal}-{Attainment} {Coaching} {Chatbot}},
	volume = {35},
	issn = {1873-7951},
	shorttitle = {Talk or {Text}?},
	url = {https://doi.org/10.1093/iwc/iwad039},
	doi = {10.1093/iwc/iwad039},
	abstract = {Despite the proliferation of chatbots (conversational agents) in increasingly varied contexts, user satisfaction with chatbot interactions remains a challenge. We do not yet fully understand chatbot usability and adoption factors or how to customize chatbots based on users' personality traits. One important and under researched aspect of chatbot design is users' perceptions of different communication modalities such as voice and text. In this between-group study (n¬†=¬†393 participants), we asked participants to rate an equivalent text-based (n¬†=¬†189) and voice-based (n¬†=¬†204) non-directive, goal-attainment coaching chatbot in terms of usability, performance expectancy and risk perception. We also considered participants' personality in terms of extraversion. For usability across all participants, there was no difference between the chatbots for all participants; however, a higher rating of the voicebot was observed in the group classified as introverts and no difference was found for participants classified as extroverts. For performance expectancy all participants, extroverts and introverts rated the textbot higher. Risk ratings showed no difference between bots for all participants, extroverts and introverts. The results suggest that the voicebot was considered slightly easier to use for some participants while the textbot was considered to perform better by all participants. Creators of chatbots should consider using voice as a modality to attract users and text as a mode to accomplish complex tasks. Extraversion did not play a significant part in chatbot communication modality choice. These results may assist in designing context and audience-specific chatbots for increased efficacy and user satisfaction.},
	language = {en-US},
	number = {4},
	urldate = {2024-09-06},
	journal = {Interacting with Computers},
	author = {Terblanche, N H D and Wallis, G P and Kidd, M},
	month = nov,
	year = {2023},
	keywords = {\#EX: ‰∏çÂåÖÂê´HRI, \#EX:‰∏çÂåÖÂê´Â§ßÊ®°Âûã, \#EX:Â∑•‰∏öÊú∫Âô®‰∫∫, \#‰∏çÁ°ÆÂÆö, \#Áü≠Êñá, \#ÁªºËø∞, /unread, Design, Users, ccfInfo: CCF-B IWC, citationNumber: 0},
	pages = {511--518},
}

@inproceedings{donini_multimodal_2024,
	address = {New York, NY, USA},
	series = {{HRI} '24},
	title = {Multimodal {Strategies} for {Robot}-to-{Human} {Communication}},
	isbn = {979-8-4007-0323-2},
	url = {https://dl.acm.org/doi/10.1145/3610978.3640686},
	doi = {10.1145/3610978.3640686},
	abstract = {This paper describes the opportunities of multimodality in the field of robot-to-human communication. In the proposed approach, the coordinated and integrated use of multimedia elements, i.e., text, images, and animations, with the robot's speech plays a very important role in the overall effectiveness of the communicative act. The reference robot used in the research was Pepper, a humanoid robot equipped with a tablet on its front. During the research, various multimodal communication strategies were formalised, implemented and preliminarily evaluated by means of a questionnaire. The results show some statistically significant preferences for specific strategies, marking new avenues of investigation with regard to robot-to-human multimodal communication and its adaptation to the user features.},
	language = {en-US},
	urldate = {2024-09-05},
	booktitle = {Companion of the 2024 {ACM}/{IEEE} {International} {Conference} on {Human}-{Robot} {Interaction}},
	publisher = {Association for Computing Machinery},
	author = {Donini, Massimo and Gena, Cristina and Mazzei, Alessandro},
	month = mar,
	year = {2024},
	keywords = {\#EX: ‰∏çÂåÖÂê´HRI, \#EX:‰∏çÂåÖÂê´Â§ßÊ®°Âûã, \#EX:Â∑•‰∏öÊú∫Âô®‰∫∫, \#‰∏çÁ°ÆÂÆö, \#Áü≠Êñá, \#ÁªºËø∞, /unread, Design, Users, ccfInfo: CCF-None HRI, citationNumber: 0},
	pages = {417--421},
}

@incollection{rocci_3_2016,
	title = {3. {Effects} of verbal and non-verbal elements in communication},
	isbn = {978-3-11-025547-8},
	url = {https://www.degruyter.com/document/doi/10.1515/9783110255478-004/html},
	urldate = {2024-09-06},
	booktitle = {Verbal {Communication}},
	publisher = {De Gruyter},
	editor = {Rocci, Andrea and Saussure, Louis De},
	month = mar,
	year = {2016},
	doi = {10.1515/9783110255478-004},
	keywords = {\#EX: ‰∏çÂåÖÂê´HRI, \#EX:‰∏çÂåÖÂê´Â§ßÊ®°Âûã, \#EX:Â∑•‰∏öÊú∫Âô®‰∫∫, \#‰∏çÁ°ÆÂÆö, \#Áü≠Êñá, \#ÁªºËø∞, /unread, Design, Users, ccfInfo: Not Found, citationNumber: 0},
	pages = {39--54},
}

@article{phutela_importance_2015,
	title = {The {Importance} of {Non}-{Verbal} {Communication}},
	volume = {9},
	issn = {09738479},
	url = {https://search.ebscohost.com/login.aspx?direct=true&db=bth&AN=112375386&site=ehost-live},
	abstract = {Non-verbal communication regulates relationships and can support or even replace verbal communications in many situations. Different genders and cultures use non-verbal communication differently and these differences can impact the nature of interpersonal communication. Nonverbal communication can become a barrier or tear down barriers to effective communication. According to researchers, non-verbal rules may differ as per the situation, and each situation determines its set of rules. Different types of people have very different yet distinct sets of non-verbal communication behaviors. This paper is an overview of different types of non-verbal communication such as body language, hand movement, facial expressions, and eye contact. Non-verbal communication involves multiple channels, is continuous and more ambiguous in nature, and often contradicts the spoken word. When non-verbal and verbal communications conflict, individuals tend to rely on non-verbal clues as a means to interpret the true meaning of a communication.},
	number = {4},
	urldate = {2024-09-06},
	journal = {IUP Journal of Soft Skills},
	author = {Phutela, Deepika},
	month = dec,
	year = {2015},
	note = {Publisher: IUP Publications},
	keywords = {\#EX: ‰∏çÂåÖÂê´HRI, \#EX:‰∏çÂåÖÂê´Â§ßÊ®°Âûã, \#EX:Â∑•‰∏öÊú∫Âô®‰∫∫, \#‰∏çÁ°ÆÂÆö, \#Áü≠Êñá, \#ÁªºËø∞, /unread, Design, Eye contact, Facial expression, Interpersonal communication, Nonverbal communication, Oral communication, Users, ccfInfo: Not Found, citationNumber: 0},
	pages = {43--49},
}

@inproceedings{kim_use_2018,
	address = {New York, NY, USA},
	series = {{HRI} '18},
	title = {The {Use} of {Voice} {Input} to {Induce} {Human} {Communication} with {Banking} {Chatbots}},
	isbn = {978-1-4503-5615-2},
	url = {https://dl.acm.org/doi/10.1145/3173386.3176970},
	doi = {10.1145/3173386.3176970},
	abstract = {The use of chatbots is more common in our everyday lives than ever before. However, few studies have been conducted comparing the differences between text- and voice-input modalities of chatbots in the banking industry. In this study, through empirical and survey-based research, users were shown to rate their relationships with the banking chatbot as more helpful and self-validating when they communicate with it by a voice-input modality than by a text-input modality.},
	urldate = {2024-09-05},
	booktitle = {Companion of the 2018 {ACM}/{IEEE} {International} {Conference} on {Human}-{Robot} {Interaction}},
	publisher = {Association for Computing Machinery},
	author = {Kim, Songhyun and Goh, Junseok and Jun, Soojin},
	month = mar,
	year = {2018},
	keywords = {\#EX: ‰∏çÂåÖÂê´HRI, \#EX:‰∏çÂåÖÂê´Â§ßÊ®°Âûã, \#EX:Â∑•‰∏öÊú∫Âô®‰∫∫, \#‰∏çÁ°ÆÂÆö, \#Áü≠Êñá, \#ÁªºËø∞, /unread, Design, Users, ccfInfo: CCF-None HRI, citationNumber: 11},
	pages = {151--152},
}

@inproceedings{lucignano_dialogue_2013,
	address = {New York, NY, USA},
	series = {{ICMI} '13},
	title = {A dialogue system for multimodal human-robot interaction},
	isbn = {978-1-4503-2129-7},
	url = {https://doi.org/10.1145/2522848.2522873},
	doi = {10.1145/2522848.2522873},
	abstract = {This paper presents a POMDP-based dialogue system for multimodal human-robot interaction (HRI). Our aim is to exploit a dialogical paradigm to allow a natural and robust interaction between the human and the robot. The proposed dialogue system should improve the robustness and the flexibility of the overall interactive system, including multimodal fusion, interpretation, and decision-making. The dialogue is represented as a Partially Observable Markov Decision Process (POMDPs) to cast the inherent communication ambiguity and noise into the dialogue model. POMDPs have been used in spoken dialogue systems, mainly for tourist information services, but their application to multimodal human-robot interaction is novel. This paper presents the proposed model for dialogue representation and the methodology used to compute a dialogue strategy. The whole architecture has been integrated on a mobile robot platform and has bee n tested in a human-robot interaction scenario to assess the overall performances with respect to baseline controllers.},
	urldate = {2024-09-05},
	booktitle = {Proceedings of the 15th {ACM} on {International} conference on multimodal interaction},
	publisher = {Association for Computing Machinery},
	author = {Lucignano, Lorenzo and Cutugno, Francesco and Rossi, Silvia and Finzi, Alberto},
	month = dec,
	year = {2013},
	keywords = {\#EX: ‰∏çÂåÖÂê´HRI, \#EX:‰∏çÂåÖÂê´Â§ßÊ®°Âûã, \#EX:Â∑•‰∏öÊú∫Âô®‰∫∫, \#‰∏çÁ°ÆÂÆö, \#Áü≠Êñá, \#ÁªºËø∞, /unread, Design, Users, ccfInfo: CCF-C ICMI, citationNumber: 31},
	pages = {197--204},
}

@article{chae_human-robot_2023,
	title = {Human-{Robot} {Interaction} in {Sloping} {Jobsites} ({Demolition}): {Exploring} {Impact} of {Visual} {Interfaces}},
	shorttitle = {Human-{Robot} {Interaction} in {Sloping} {Jobsites} ({Demolition})},
	url = {https://www.iaarc.org/publications/icra_2023_future_of_construction_workshop_papers/human_robot_interaction_in_sloping_jobsites_demolition-exploring_impact_of_visual_interfaces.html},
	language = {en-US},
	urldate = {2024-09-06},
	journal = {ISARC Proceedings},
	author = {Chae, Yeon and Gupta, Samraat and Ham, Youngjib},
	month = jul,
	year = {2023},
	note = {Publisher: IAARC},
	keywords = {\#EX: ‰∏çÂåÖÂê´HRI, \#EX:‰∏çÂåÖÂê´Â§ßÊ®°Âûã, \#EX:Â∑•‰∏öÊú∫Âô®‰∫∫, \#‰∏çÁ°ÆÂÆö, \#Áü≠Êñá, \#ÁªºËø∞, /unread, Design, Users, ccfInfo: Not Found, citationNumber: 0},
	pages = {5--7},
}

@article{olson_human-inspired_2021,
	title = {Human-{Inspired} {Robotic} {Eye}-{Hand} {Coordination} {Enables} {New} {Communication} {Channels} {Between} {Humans} and {Robots}},
	volume = {13},
	issn = {1875-4805},
	url = {https://doi.org/10.1007/s12369-020-00693-2},
	doi = {10.1007/s12369-020-00693-2},
	abstract = {This paper concerns human-inspired robotic eye-hand coordination algorithms using custom built robotic eyes that were interfaced with a Baxter robot. Eye movement was programmed anthropomorphically based on previously reported research on human eye-hand coordination during grasped object transportation. Robotic eye tests were first performed on a component level where accurate position and temporal control were achieved. Next, 11 human subjects were recruited to observe the novel robotic system to quantify the ability of robotic eye-hand coordination algorithms to convey two kinds of information to people during object transportation tasks: first, the transported object‚Äôs delivery location and second, the level of care exerted by the robot to transport the object. Most subjects correlated decreased frequency in gaze fixations on an object‚Äôs target location with increased care of transporting an object, although these results were somewhat mixed among the 11 human subjects. Additionally, the human subjects were able to reliably infer the delivery location of the transported object purely by the robotic eye-hand coordination algorithm with an overall success rate of 91.4\%. These results suggest that anthropomorphic eye-hand coordination of robotic entities could be useful in pedagogical or industrial settings.},
	language = {en},
	number = {5},
	urldate = {2024-09-06},
	journal = {International Journal of Social Robotics},
	author = {Olson, Stephanie and Abd, Moaed and Engeberg, Erik D.},
	month = aug,
	year = {2021},
	keywords = {\#EX: ‰∏çÂåÖÂê´HRI, \#EX:‰∏çÂåÖÂê´Â§ßÊ®°Âûã, \#EX:Â∑•‰∏öÊú∫Âô®‰∫∫, \#‰∏çÁ°ÆÂÆö, \#Áü≠Êñá, \#ÁªºËø∞, /unread, Animatronics, Artificial Intelligence, Design, Eye-hand coordination, Human‚Äìrobot interaction, Robotics, Users, ccfInfo: CCF-None IJSR, citationNumber: 1},
	pages = {1033--1046},
}

@article{wang_multimodal_nodate,
	title = {Multimodal {Human}‚Äì{Robot} {Interaction} for {Human}‚Äê{Centric} {Smart} {Manufacturing}: {A} {Survey}},
	shorttitle = {Multimodal {Human}‚Äì{Robot} {Interaction} for {Human}‚Äê{Centric} {Smart} {Manufacturing}},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/aisy.202300359},
	doi = {10.1002/aisy.202300359},
	abstract = {This article presents a survey of multimodal human‚Äìrobot interaction (HRI), focusing on modalities of vision, auditory and language, haptics, and physiological sensing. It encompasses an analysis of ...},
	language = {en},
	urldate = {2024-09-06},
	author = {Wang, Tian and Zheng, Pai and Li, Shufei and Wang, Lihui},
	keywords = {\#EX: ‰∏çÂåÖÂê´HRI, \#EX:‰∏çÂåÖÂê´Â§ßÊ®°Âûã, \#EX:Â∑•‰∏öÊú∫Âô®‰∫∫, \#‰∏çÁ°ÆÂÆö, \#Áü≠Êñá, \#ÁªºËø∞, /unread, Design, Users, ccfInfo: CCF-None AISY, citationNumber: 0},
}

@inproceedings{lee_viewer2explorer_2024,
	address = {Honolulu HI USA},
	title = {{Viewer2Explorer}: {Designing} a {Map} {Interface} for {Spatial} {Navigation} in {Linear} 360 {Museum} {Exhibition} {Video}},
	isbn = {979-8-4007-0330-0},
	shorttitle = {{Viewer2Explorer}},
	url = {https://dl.acm.org/doi/10.1145/3613904.3642952},
	doi = {10.1145/3613904.3642952},
	language = {en},
	urldate = {2024-09-08},
	booktitle = {Proceedings of the {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Lee, Chaeeun and Kim, Jinwook and Yi, Hyeonbeom and Lee, Woohun},
	month = may,
	year = {2024},
	keywords = {\#EX: ‰∏çÂåÖÂê´HRI, \#EX:‰∏çÂåÖÂê´Â§ßÊ®°Âûã, \#EX:Â∑•‰∏öÊú∫Âô®‰∫∫, \#‰∏çÁ°ÆÂÆö, \#Áü≠Êñá, \#ÁªºËø∞, /unread, Design, Users, ccfInfo: CCF-A CHI, citationNumber: 0},
	pages = {1--15},
}

@inproceedings{ashwini_it_2024,
	address = {Honolulu HI USA},
	title = {‚Äú{It} looks useful, works just fine, but will it replace me ?" {Understanding} {Special} {Educators}‚Äô {Perception} of {Social} {Robots} for {Autism} {Care} in {India}},
	isbn = {979-8-4007-0330-0},
	shorttitle = {‚Äú{It} looks useful, works just fine, but will it replace me ?},
	url = {https://dl.acm.org/doi/10.1145/3613904.3642836},
	doi = {10.1145/3613904.3642836},
	language = {en},
	urldate = {2024-09-08},
	booktitle = {Proceedings of the {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Ashwini, B and Ghoshal, Atmadeep and Suri, Venkata Ratnadeep and Achary, Krishnaveni and Shukla, Jainendra},
	month = may,
	year = {2024},
	keywords = {\#EX: ‰∏çÂåÖÂê´HRI, \#EX:‰∏çÂåÖÂê´Â§ßÊ®°Âûã, \#EX:Â∑•‰∏öÊú∫Âô®‰∫∫, \#‰∏çÁ°ÆÂÆö, \#Áü≠Êñá, \#ÁªºËø∞, /unread, Design, Users, ccfInfo: CCF-A CHI, citationNumber: 0},
	pages = {1--23},
}

@inproceedings{duan_generating_2024,
	address = {Honolulu HI USA},
	title = {Generating {Automatic} {Feedback} on {UI} {Mockups} with {Large} {Language} {Models}},
	isbn = {979-8-4007-0330-0},
	url = {https://dl.acm.org/doi/10.1145/3613904.3642782},
	doi = {10.1145/3613904.3642782},
	language = {en},
	urldate = {2024-09-08},
	booktitle = {Proceedings of the {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Duan, Peitong and Warner, Jeremy and Li, Yang and Hartmann, Bjoern},
	month = may,
	year = {2024},
	keywords = {\#EX: ‰∏çÂåÖÂê´HRI, \#EX:‰∏çÂåÖÂê´Â§ßÊ®°Âûã, \#EX:Â∑•‰∏öÊú∫Âô®‰∫∫, \#‰∏çÁ°ÆÂÆö, \#Áü≠Êñá, \#ÁªºËø∞, /unread, Design, Users, ccfInfo: CCF-A CHI, citationNumber: 0},
	pages = {1--20},
}

@misc{noauthor_comparing_nodate-1,
	title = {Comparing alternative modalities in the context of multimodal human‚Äìrobot interaction {\textbar} {Journal} on {Multimodal} {User} {Interfaces}},
	url = {https://link.springer.com/article/10.1007/s12193-023-00421-w},
	urldate = {2024-09-06},
	keywords = {\#EX: ‰∏çÂåÖÂê´HRI, \#EX:‰∏çÂåÖÂê´Â§ßÊ®°Âûã, \#EX:Â∑•‰∏öÊú∫Âô®‰∫∫, \#‰∏çÁ°ÆÂÆö, \#Áü≠Êñá, \#ÁªºËø∞, /unread, Design, Users, ccfInfo: Not Found, citationNumber: Not Found},
}

@inproceedings{frank_realizing_2016,
	title = {Realizing mixed-reality environments with tablets for intuitive human-robot collaboration for object manipulation tasks},
	url = {https://ieeexplore.ieee.org/document/7745146},
	doi = {10.1109/ROMAN.2016.7745146},
	abstract = {Although gesture-based input and augmented reality (AR) facilitate intuitive human-robot interactions (HRI), prior implementations have relied on research-grade hardware and software. This paper explores using tablets to render mixed-reality visual environments that support human-robot collaboration for object manipulation. A mobile interface is created on a tablet by integrating real-time vision, 3D graphics, touchscreen interaction, and wireless communication. This mobile interface augments a live video of physical objects in a robot's workspace with corresponding virtual objects that can be manipulated by a user to intuitively command the robot to manipulate the physical objects. By generating the mixed-reality environment on an exocentric view provided by the tablet camera, the interface establishes a common frame of reference for the user and the robot to effectively communicate spatial information for object manipulation. After addressing challenges due to limitations in mobile sensing and computation, the interface is evaluated with participants to examine the performance and user experience with the suggested approach.},
	urldate = {2024-09-06},
	booktitle = {2016 25th {IEEE} {International} {Symposium} on {Robot} and {Human} {Interactive} {Communication} ({RO}-{MAN})},
	author = {Frank, Jared A. and Moorhead, Matthew and Kapila, Vikram},
	month = aug,
	year = {2016},
	note = {ISSN: 1944-9437},
	keywords = {\#EX: ‰∏çÂåÖÂê´HRI, \#EX:‰∏çÂåÖÂê´Â§ßÊ®°Âûã, \#EX:Â∑•‰∏öÊú∫Âô®‰∫∫, \#‰∏çÁ°ÆÂÆö, \#Áü≠Êñá, \#ÁªºËø∞, /unread, Cameras, Design, Mobile communication, Robot kinematics, Robot vision systems, Users, Virtual reality, ccfInfo: CCF-None RO-MAN, citationNumber: 36},
	pages = {302--307},
}

@inproceedings{gorostiza_multimodal_2006,
	title = {Multimodal {Human}-{Robot} {Interaction} {Framework} for a {Personal} {Robot}},
	url = {https://ieeexplore.ieee.org/abstract/document/4107783},
	doi = {10.1109/ROMAN.2006.314392},
	abstract = {This paper presents a framework for multimodal human-robot interaction. The proposed framework is being implemented in a personal robot called Maggie, developed at RoboticsLab of the University Carlos III of Madrid for social interaction research. The control architecture of this personal robot is a hybrid control architecture called AD (Automatic-Deliberative) that incorporates an Emotion Control System (ECS) Maggie‚Äôs main goal is to interact establish a peer-to-peer relationship with humans. To achieve this goal, a set of human-robot interaction skills are developed based on the proposed framework. The human-robot interaction skills imply tactile, visual, remote voice and sound modes. The multi-modal fusion and synchronization are also presented in this paper.},
	urldate = {2024-09-06},
	booktitle = {{ROMAN} 2006 - {The} 15th {IEEE} {International} {Symposium} on {Robot} and {Human} {Interactive} {Communication}},
	author = {Gorostiza, Javi F. and Barber, Ramon and Khamis, Alaa M. and Malfaz, Maria and Pacheco, Rakel and Rivas, Rafael and Corrales, Ana and Delgado, Elena and Salichs, Miguel A.},
	month = sep,
	year = {2006},
	note = {ISSN: 1944-9437},
	keywords = {\#EX: ‰∏çÂåÖÂê´HRI, \#EX:‰∏çÂåÖÂê´Â§ßÊ®°Âûã, \#EX:Â∑•‰∏öÊú∫Âô®‰∫∫, \#‰∏çÁ°ÆÂÆö, \#Áü≠Êñá, \#ÁªºËø∞, /unread, Anthropomorphism, Automatic control, Cognitive robotics, Communication system control, Control systems, Design, Human robot interaction, Humanoid robots, Orbital robotics, Robotics and automation, Usability, Users, ccfInfo: CCF-None RO-MAN, citationNumber: 53},
	pages = {39--44},
}

@misc{noauthor_intelligent_nodate,
	title = {Intelligent user interface for {Human}-{Robot} {Interaction} {\textbar} {IEEE} {Conference} {Publication} {\textbar} {IEEE} {Xplore}},
	url = {https://ieeexplore.ieee.org/document/4618335},
	urldate = {2024-09-06},
	keywords = {\#EX: ‰∏çÂåÖÂê´HRI, \#EX:‰∏çÂåÖÂê´Â§ßÊ®°Âûã, \#EX:Â∑•‰∏öÊú∫Âô®‰∫∫, \#‰∏çÁ°ÆÂÆö, \#Áü≠Êñá, \#ÁªºËø∞, /unread, Design, Users, ccfInfo: Not Found, citationNumber: Not Found},
}

@inproceedings{feng_coprompt_2024,
	address = {Honolulu HI USA},
	title = {{CoPrompt}: {Supporting} {Prompt} {Sharing} and {Referring} in {Collaborative} {Natural} {Language} {Programming}},
	isbn = {979-8-4007-0330-0},
	shorttitle = {{CoPrompt}},
	url = {https://dl.acm.org/doi/10.1145/3613904.3642212},
	doi = {10.1145/3613904.3642212},
	language = {en},
	urldate = {2024-09-08},
	booktitle = {Proceedings of the {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Feng, Li and Yen, Ryan and You, Yuzhe and Fan, Mingming and Zhao, Jian and Lu, Zhicong},
	month = may,
	year = {2024},
	keywords = {\#EX: ‰∏çÂåÖÂê´HRI, \#EX:‰∏çÂåÖÂê´Â§ßÊ®°Âûã, \#EX:Â∑•‰∏öÊú∫Âô®‰∫∫, \#‰∏çÁ°ÆÂÆö, \#Áü≠Êñá, \#ÁªºËø∞, /unread, Design, Users, ccfInfo: CCF-A CHI, citationNumber: 0},
	pages = {1--21},
}

@inproceedings{ma_are_2024,
	address = {Honolulu HI USA},
	title = {‚Äú{Are} {You} {Really} {Sure}?‚Äù {Understanding} the {Effects} of {Human} {Self}-{Confidence} {Calibration} in {AI}-{Assisted} {Decision} {Making}},
	isbn = {979-8-4007-0330-0},
	shorttitle = {‚Äú{Are} {You} {Really} {Sure}?},
	url = {https://dl.acm.org/doi/10.1145/3613904.3642671},
	doi = {10.1145/3613904.3642671},
	language = {en},
	urldate = {2024-09-08},
	booktitle = {Proceedings of the {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Ma, Shuai and Wang, Xinru and Lei, Ying and Shi, Chuhan and Yin, Ming and Ma, Xiaojuan},
	month = may,
	year = {2024},
	keywords = {\#EX: ‰∏çÂåÖÂê´HRI, \#EX:‰∏çÂåÖÂê´Â§ßÊ®°Âûã, \#EX:Â∑•‰∏öÊú∫Âô®‰∫∫, \#‰∏çÁ°ÆÂÆö, \#Áü≠Êñá, \#ÁªºËø∞, Design, Users, ccfInfo: CCF-A CHI, citationNumber: 0},
	pages = {1--20},
}

@inproceedings{berney_care-based_2024,
	address = {Honolulu HI USA},
	title = {Care-{Based} {Eco}-{Feedback} {Augmented} with {Generative} {AI}: {Fostering} {Pro}-{Environmental} {Behavior} through {Emotional} {Attachment}},
	isbn = {979-8-4007-0330-0},
	shorttitle = {Care-{Based} {Eco}-{Feedback} {Augmented} with {Generative} {AI}},
	url = {https://dl.acm.org/doi/10.1145/3613904.3642296},
	doi = {10.1145/3613904.3642296},
	language = {en},
	urldate = {2024-09-08},
	booktitle = {Proceedings of the {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Berney, Manon and Ouaazki, Abdessalam and Macko, Vladimir and Kocher, Bruno and Holzer, Adrian},
	month = may,
	year = {2024},
	keywords = {\#EX: ‰∏çÂåÖÂê´HRI, \#EX:‰∏çÂåÖÂê´Â§ßÊ®°Âûã, \#EX:Â∑•‰∏öÊú∫Âô®‰∫∫, \#‰∏çÁ°ÆÂÆö, \#Áü≠Êñá, \#ÁªºËø∞, /unread, Design, Users, ccfInfo: CCF-A CHI, citationNumber: 0},
	pages = {1--15},
}

@inproceedings{jaber_cooking_2024,
	address = {Honolulu HI USA},
	title = {Cooking {With} {Agents}: {Designing} {Context}-aware {Voice} {Interaction}},
	isbn = {979-8-4007-0330-0},
	shorttitle = {Cooking {With} {Agents}},
	url = {https://dl.acm.org/doi/10.1145/3613904.3642183},
	doi = {10.1145/3613904.3642183},
	language = {en},
	urldate = {2024-09-08},
	booktitle = {Proceedings of the {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Jaber, Razan and Zhong, Sabrina and Kuoppam√§ki, Sanna and Hosseini, Aida and Gessinger, Iona and Brumby, Duncan P and Cowan, Benjamin R. and Mcmillan, Donald},
	month = may,
	year = {2024},
	keywords = {\#EX: ‰∏çÂåÖÂê´HRI, \#EX:‰∏çÂåÖÂê´Â§ßÊ®°Âûã, \#EX:Â∑•‰∏öÊú∫Âô®‰∫∫, \#‰∏çÁ°ÆÂÆö, \#Áü≠Êñá, \#ÁªºËø∞, /unread, Design, Users, ccfInfo: CCF-A CHI, citationNumber: 0},
	pages = {1--13},
}

@inproceedings{yoshimura_synlogue_2024,
	address = {Honolulu HI USA},
	title = {Synlogue with {Aizuchi}-bot: {Investigating} the {Co}-{Adaptive} and {Open}-{Ended} {Interaction} {Paradigm}},
	isbn = {979-8-4007-0330-0},
	shorttitle = {Synlogue with {Aizuchi}-bot},
	url = {https://dl.acm.org/doi/10.1145/3613904.3642046},
	doi = {10.1145/3613904.3642046},
	language = {en},
	urldate = {2024-09-08},
	booktitle = {Proceedings of the {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Yoshimura, Kazumi and Chen, Dominique and Witkowski, Olaf},
	month = may,
	year = {2024},
	keywords = {\#EX: ‰∏çÂåÖÂê´HRI, \#EX:‰∏çÂåÖÂê´Â§ßÊ®°Âûã, \#EX:Â∑•‰∏öÊú∫Âô®‰∫∫, \#‰∏çÁ°ÆÂÆö, \#Áü≠Êñá, \#ÁªºËø∞, /unread, Design, Users, ccfInfo: CCF-A CHI, citationNumber: 0},
	pages = {1--21},
}

@inproceedings{yang_reactgenie_2024,
	address = {Honolulu HI USA},
	title = {{ReactGenie}: {A} {Development} {Framework} for {Complex} {Multimodal} {Interactions} {Using} {Large} {Language} {Models}},
	isbn = {979-8-4007-0330-0},
	shorttitle = {{ReactGenie}},
	url = {https://dl.acm.org/doi/10.1145/3613904.3642517},
	doi = {10.1145/3613904.3642517},
	language = {en},
	urldate = {2024-09-08},
	booktitle = {Proceedings of the {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Yang, Jackie (Junrui) and Shi, Yingtian and Zhang, Yuhan and Li, Karina and Rosli, Daniel Wan and Jain, Anisha and Zhang, Shuning and Li, Tianshi and Landay, James A. and Lam, Monica S.},
	month = may,
	year = {2024},
	keywords = {\#EX: ‰∏çÂåÖÂê´HRI, \#EX:‰∏çÂåÖÂê´Â§ßÊ®°Âûã, \#EX:Â∑•‰∏öÊú∫Âô®‰∫∫, \#‰∏çÁ°ÆÂÆö, \#Áü≠Êñá, \#ÁªºËø∞, /unread, Design, Users, ccfInfo: CCF-A CHI, citationNumber: 0},
	pages = {1--23},
}

@article{carriero_human-robot_2023,
	title = {Human-{Robot} {Collaboration}: {An} {Augmented} {Reality} {Toolkit} for {Bi}-{Directional} {Interaction}},
	volume = {13},
	copyright = {https://creativecommons.org/licenses/by/4.0/},
	issn = {2076-3417},
	shorttitle = {Human-{Robot} {Collaboration}},
	url = {https://www.mdpi.com/2076-3417/13/20/11295},
	doi = {10.3390/app132011295},
	abstract = {This work proposes an Augmented Reality (AR) application designed for HoloLens 2 which allows human operators, without particular experience or knowledge of robotics, to easily interact with collaborative robots. Building on the application presented in a previous work of the authors, the novel contributions are focused on a bi-directional interaction that manages the exchange of data from the robot to the human operator and, in the meantime, the Ô¨Çow of commands in the opposite direction. More in detail, the application includes the reading of the robot state, in terms of joint positions, velocities and torques, the visualization of the workspace and the generation and manipulation of the end-effector trajectory by directly moving a set of way-points displayed in the AR environment. Finally, the trajectory feasibility is veriÔ¨Åed and notiÔ¨Åed to the user by taking into account the workspace limits. A usability study of the AR platform has been conducted involving 45 participants with different ages and expertise in robot programming and Extended Reality (XR) platforms, comparing two programming methods: a classical kinesthetic teaching interface, provided by the Franka Emika Panda cobot, and the presented AR platform. Participants have reported the effectiveness of the proposed platform, experiencing less physical demand and higher intuitiveness and usability.},
	language = {en},
	number = {20},
	urldate = {2024-09-11},
	journal = {Applied Sciences},
	author = {Carriero, Graziano and Calzone, Nicolas and Sileo, Monica and Pierri, Francesco and Caccavale, Fabrizio and Mozzillo, Rocco},
	month = oct,
	year = {2023},
	keywords = {\#EX: ‰∏çÂåÖÂê´HRI, \#EX:‰∏çÂåÖÂê´Â§ßÊ®°Âûã, \#EX:Â∑•‰∏öÊú∫Âô®‰∫∫, \#‰∏çÁ°ÆÂÆö, \#Áü≠Êñá, \#ÁªºËø∞, Design, Users, ccfInfo: Not Found, citationNumber: 0},
	pages = {11295},
}

@inproceedings{luria_comparing_2017,
	address = {Denver Colorado USA},
	title = {Comparing {Social} {Robot}, {Screen} and {Voice} {Interfaces} for {Smart}-{Home} {Control}},
	isbn = {978-1-4503-4655-9},
	url = {https://dl.acm.org/doi/10.1145/3025453.3025786},
	doi = {10.1145/3025453.3025786},
	abstract = {With domestic technology on the rise, the quantity and complexity of smart-home devices are becoming an important interaction design challenge. We present a novel design for a home control interface in the form of a social robot, commanded via tangible icons and giving feedback through expressive gestures. We experimentally compare the robot to three common smart-home interfaces: a voice-control loudspeaker; a wall-mounted touch-screen; and a mobile application. Our Ô¨Åndings suggest that interfaces that rate higher on Ô¨Çow rate lower on usability, and vice versa. Participants‚Äô sense of control is highest using familiar interfaces, and lowest using voice control. Situation awareness is highest using the robot, and also lowest using voice control. These Ô¨Åndings raise questions about voice control as a smart-home interface, and suggest that embodied social robots could provide for an engaging interface with high situation awareness, but also that their usability remains a considerable design challenge.},
	language = {en},
	urldate = {2024-09-11},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Luria, Michal and Hoffman, Guy and Zuckerman, Oren},
	month = may,
	year = {2017},
	keywords = {\#EX: ‰∏çÂåÖÂê´HRI, \#EX:‰∏çÂåÖÂê´Â§ßÊ®°Âûã, \#EX:Â∑•‰∏öÊú∫Âô®‰∫∫, \#‰∏çÁ°ÆÂÆö, \#Áü≠Êñá, \#ÁªºËø∞, Design, Users, ccfInfo: CCF-A CHI, citationNumber: 142},
	pages = {580--628},
}

@article{deublein_expressive_nodate,
	title = {({Expressive}) {Social} {Robot} or {Tablet}? ‚Äì {On} the {Benefits} of {Embodiment} and {Non}-verbal {Expressivity} of the {Interface} for a {Smart} {Environment}},
	abstract = {Smart home systems increasingly Ô¨Ånd their way into private households and eÔ¨Äorts are being made to integrate lifelike user interfaces (e.g. social robots) to facilitate the interaction with the smart environment. Considering this, the question arises which beneÔ¨Åts such embodied user interfaces oÔ¨Äer compared to conventional devices. We are presenting a user study within a smart oÔ¨Éce setting in which 84 participants were either interrupted by a tablet, a non-expressive social robot, or an expressive social robot by being asked to perform tasks regarding their physical well-being. Results show that each type of user interface bears diÔ¨Äerent advantages. While the tablet comes with a signiÔ¨Åcantly higher usability and a lower level of perceived workload, both versions of the social robot outperform the tablet in terms of social perception and the overall evaluation of the interaction. Overall, the results provide valuable insights informing designers of smart environments which device to choose to enhance certain aspects of the quality of interaction.},
	language = {en},
	author = {Deublein, Andrea},
	keywords = {\#EX: ‰∏çÂåÖÂê´HRI, \#EX:‰∏çÂåÖÂê´Â§ßÊ®°Âûã, \#EX:Â∑•‰∏öÊú∫Âô®‰∫∫, \#‰∏çÁ°ÆÂÆö, \#Áü≠Êñá, \#ÁªºËø∞, Design, Users, ccfInfo: CCF-None PERSUASIVE, citationNumber: 9},
}

@article{su_recent_2023,
	title = {Recent advancements in multimodal human‚Äìrobot interaction},
	volume = {17},
	issn = {1662-5218},
	url = {https://www.frontiersin.org/articles/10.3389/fnbot.2023.1084000/full},
	doi = {10.3389/fnbot.2023.1084000},
	abstract = {Robotics have advanced significantly over the years, and human‚Äìrobot interaction (HRI) is now playing an important role in delivering the best user experience, cutting down on laborious tasks, and raising public acceptance of robots. New HRI approaches are necessary to promote the evolution of robots, with a more natural and flexible interaction manner clearly the most crucial. As a newly emerging approach to HRI, multimodal HRI is a method for individuals to communicate with a robot using various modalities, including voice, image, text, eye movement, and touch, as well as bio-signals like EEG and ECG. It is a broad field closely related to cognitive science, ergonomics, multimedia technology, and virtual reality, with numerous applications springing up each year. However, little research has been done to summarize the current development and future trend of HRI. To this end, this paper systematically reviews the state of the art of multimodal HRI on its applications by summing up the latest research articles relevant to this field. Moreover, the research development in terms of the input signal and the output signal is also covered in this manuscript.},
	language = {en},
	urldate = {2024-09-11},
	journal = {Frontiers in Neurorobotics},
	author = {Su, Hang and Qi, Wen and Chen, Jiahao and Yang, Chenguang and Sandoval, Juan and Laribi, Med Amine},
	month = may,
	year = {2023},
	keywords = {\#EX: ‰∏çÂåÖÂê´HRI, \#EX:‰∏çÂåÖÂê´Â§ßÊ®°Âûã, \#EX:Â∑•‰∏öÊú∫Âô®‰∫∫, \#‰∏çÁ°ÆÂÆö, \#Áü≠Êñá, \#ÁªºËø∞, Design, Users, ccfInfo: CCF-None FINR, citationNumber: 2},
	pages = {1084000},
}

@article{borsci_chatbot_2022,
	title = {The {Chatbot} {Usability} {Scale}: the {Design} and {Pilot} of a {Usability} {Scale} for {Interaction} with {AI}-{Based} {Conversational} {Agents}},
	volume = {26},
	issn = {1617-4917},
	shorttitle = {The {Chatbot} {Usability} {Scale}},
	url = {https://doi.org/10.1007/s00779-021-01582-9},
	doi = {10.1007/s00779-021-01582-9},
	abstract = {Standardised tools to assess a user‚Äôs satisfaction with the experience of using chatbots and conversational agents are currently unavailable. This work describes four studies, including a systematic literature review, with an overall sample of 141 participants in the survey (experts and novices), focus group sessions and testing of chatbots to (i) define attributes to assess the quality of interaction with chatbots and (ii) the designing and piloting a new scale to measure satisfaction after the experience with chatbots. Two instruments were developed: (i) A diagnostic tool in the form of a checklist (BOT-Check). This tool is a development of previous works which can be used reliably to check the quality of a chatbots experience in line with commonplace principles. (ii) A 15-item questionnaire (BOT Usability Scale, BUS-15) with estimated reliability between .76 and .87 distributed in five factors. BUS-15 strongly correlates with UMUX-LITE by enabling designers to consider a broader range of aspects usually not considered in satisfaction tools for non-conversational agents, e.g. conversational efficiency and accessibility, quality of the chatbot‚Äôs functionality and so on. Despite the convincing psychometric properties, BUS-15 requires further testing and validation. Designers can use it as a tool to assess products, thus building independent databases for future evaluation of its reliability, validity and sensitivity.},
	language = {en},
	number = {1},
	urldate = {2024-09-09},
	journal = {Personal and Ubiquitous Computing},
	author = {Borsci, Simone and Malizia, Alessio and Schmettow, Martin and van der Velde, Frank and Tariverdiyeva, Gunay and Balaji, Divyaa and Chamberlain, Alan},
	month = feb,
	year = {2022},
	keywords = {\#EX: ‰∏çÂåÖÂê´HRI, \#EX:‰∏çÂåÖÂê´Â§ßÊ®°Âûã, \#EX:Â∑•‰∏öÊú∫Âô®‰∫∫, \#‰∏çÁ°ÆÂÆö, \#Áü≠Êñá, \#ÁªºËø∞, /unread, AI, Artificial Intelligence, Artificial intelligence, Autonomy, Chatbots, Conversational agents, Design, Evaluation, Human-Computer interaction (HCI), Interaction satisfaction, Satisfaction, Trust, Usability, User experience, Users, ccfInfo: CCF-C PUC, citationNumber: 35},
	pages = {95--119},
}

@incollection{antona_voice_2020,
	address = {Cham},
	title = {Voice {User} {Interfaces} for {Service} {Robots}: {Design} {Principles} and {Methodology}},
	volume = {12188},
	isbn = {978-3-030-49281-6 978-3-030-49282-3},
	shorttitle = {Voice {User} {Interfaces} for {Service} {Robots}},
	url = {https://link.springer.com/10.1007/978-3-030-49282-3_35},
	abstract = {This work presents the concerns, prerequisites, and methods for building interaction interfaces for service robots. It mainly deals with Voice User Interfaces - VUI (also called Spoken Dialogue Interfaces - SDIs) but also includes issues on multimodal interfaces, involving speech and other modalities. Human-machine interaction in the area of robotics raises certain challenges that respective interface design for other domains ignores. Robots, and more importantly, service robots, execute actual tasks based on plans and scenarios that, in effect, layout their usage. The completion requirements, as well as the workÔ¨Çow needed for those tasks, form a very signiÔ¨Åcant set of rules that affect and sometimes govern the interaction between the user and the machine. Those rules are embedded to the design of the interaction system and, together with the communicated context, provide the sets and constraints that the system is based upon. These constraints can be realized in the form of speciÔ¨Åc dialogue management design, dialogue Ô¨Çow, belief states models, veriÔ¨Åcation, disambiguation, and grounding techniques as well as more subtly use of speciÔ¨Åc speech and dialogue acts ‚Äì all the above affect all stages of the lifecycle. Moreover, signiÔ¨Åcant merit goes to usability, and the techniques for its evaluation, issues that are of the utmost importance when any user-machine interface is designed and assessed.},
	language = {en},
	urldate = {2024-09-11},
	booktitle = {Universal {Access} in {Human}-{Computer} {Interaction}. {Design} {Approaches} and {Supporting} {Technologies}},
	publisher = {Springer International Publishing},
	author = {Stavropoulou, Pepi and Spiliotopoulos, Dimitris and Kouroupetroglou, Georgios},
	editor = {Antona, Margherita and Stephanidis, Constantine},
	year = {2020},
	doi = {10.1007/978-3-030-49282-3_35},
	note = {Series Title: Lecture Notes in Computer Science},
	keywords = {\#EX: ‰∏çÂåÖÂê´HRI, \#EX:‰∏çÂåÖÂê´Â§ßÊ®°Âûã, \#EX:Â∑•‰∏öÊú∫Âô®‰∫∫, \#‰∏çÁ°ÆÂÆö, \#Áü≠Êñá, \#ÁªºËø∞, Design, Users, ccfInfo: CCF-None HCI, citationNumber: 1},
	pages = {489--505},
}

@inproceedings{okamoto_usability_2009,
	title = {Usability study of {VUI} consistent with {GUI} focusing on age-groups},
	url = {https://www.isca-archive.org/interspeech_2009/okamoto09b_interspeech.html},
	doi = {10.21437/Interspeech.2009-535},
	abstract = {We studied the usability of a Voice User Interface (VUI) that is consistent with a Graphical User Interface (GUI), and focused on its dependency with user age-groups. Usability tests were iteratively conducted on 245 Japanese subjects with age-groups from 20s to 60s using a prototype of an in-vehicle information application. Next we calculated and analyzed statistics of the usability tests. We discuss the differences in usability with respect to age-groups and how to handle them. We propose that it is necessary to make voice guidance straightforward and to devise a VUI consistent with a GUI (VGUI) in order to let users understand the system structure. Also we found that the default design of a VGUI should be as simple as possible so that elderly users, who may be slow to learn the new system structure, are able to easily learn it.},
	language = {en},
	urldate = {2024-09-11},
	booktitle = {Interspeech 2009},
	publisher = {ISCA},
	author = {Okamoto, Jun and Kato, Tomoyuki and Shozakai, Makoto},
	month = sep,
	year = {2009},
	keywords = {\#EX: ‰∏çÂåÖÂê´HRI, \#EX:‰∏çÂåÖÂê´Â§ßÊ®°Âûã, \#EX:Â∑•‰∏öÊú∫Âô®‰∫∫, \#‰∏çÁ°ÆÂÆö, \#Áü≠Êñá, \#ÁªºËø∞, /unread, Design, Users, ccfInfo: CCF-C INTERSPEECH, citationNumber: 24},
	pages = {1839--1842},
}

@inproceedings{huang_gestures_2024,
	address = {Boulder CO USA},
	title = {({Gestures} {Vaguely}): {The} {Effects} of {Robots}' {Use} of {Abstract} {Pointing} {Gestures} in {Large}-{Scale} {Environments}},
	isbn = {979-8-4007-0322-5},
	shorttitle = {({Gestures} {Vaguely})},
	url = {https://dl.acm.org/doi/10.1145/3610977.3634924},
	doi = {10.1145/3610977.3634924},
	abstract = {As robots are deployed into large-scale human environments, they will need to engage in task-oriented dialogues about objects and locations beyond those that can currently be seen. In these contexts, speakers use a wide range of referring gestures beyond those used in the small-scale interaction contexts that HRI research typically investigates. In this work, we thus seek to understand how robots can better generate gestures to accompany their referring language in large-scale interaction contexts. In service of this goal, we present the results of two human-subject studies: (1) a humanhuman study exploring how human gestures change in large-scale interaction contexts, and to identify human-like gestures suitable to such contexts yet readily implemented on robot hardware; and (2) a human-robot study conducted in a tightly controlled Virtual Reality environment, to evaluate robots‚Äô use of those identified gestures. Our results show that robot use of Precise Deictic and Abstract Pointing gestures afford different types of benefits when used to refer to visible vs. non-visible referents, leading us to formulate three concrete design guidelines. These results highlight both the opportunities for robot use of more humanlike gestures in large-scale interaction contexts, as well as the need for future work exploring their use as part of multi-modal communication.},
	language = {en},
	urldate = {2024-09-11},
	booktitle = {Proceedings of the 2024 {ACM}/{IEEE} {International} {Conference} on {Human}-{Robot} {Interaction}},
	publisher = {ACM},
	author = {Huang, Annie and Ranucci, Alyson and Stogsdill, Adam and Clark, Grace and Schott, Keenan and Higger, Mark and Han, Zhao and Williams, Tom},
	month = mar,
	year = {2024},
	keywords = {\#EX: ‰∏çÂåÖÂê´HRI, \#EX:‰∏çÂåÖÂê´Â§ßÊ®°Âûã, \#EX:Â∑•‰∏öÊú∫Âô®‰∫∫, \#‰∏çÁ°ÆÂÆö, \#Áü≠Êñá, \#ÁªºËø∞, /unread, Design, Users, ccfInfo: CCF-None HRI, citationNumber: 0},
	pages = {293--302},
}

@misc{noauthor__nodate,
	title = {Ë°°ÈáèÊú∫Âô®‰∫∫ÂÉè‰∫∫ÁöÑÁª¥Â∫¶ - {Google} ÊêúÁ¥¢},
	url = {https://www.google.com/search?q=%E8%A1%A1%E9%87%8F%E6%9C%BA%E5%99%A8%E4%BA%BA%E5%83%8F%E4%BA%BA%E7%9A%84%E7%BB%B4%E5%BA%A6&oq=%E8%A1%A1%E9%87%8F%E6%9C%BA%E5%99%A8%E4%BA%BA%E5%83%8F%E4%BA%BA%E7%9A%84%E7%BB%B4%E5%BA%A6&gs_lcrp=EgZjaHJvbWUyBggAEEUYOTIKCAEQABiABBiiBDIKCAIQABiABBiiBDIKCAMQABiABBiiBDIKCAQQABiABBiiBDIKCAUQABiABBiiBNIBCTc1MjdqMGoxNagCALACAA&sourceid=chrome&ie=UTF-8},
	urldate = {2024-09-11},
	keywords = {\#EX: ‰∏çÂåÖÂê´HRI, \#EX:‰∏çÂåÖÂê´Â§ßÊ®°Âûã, \#EX:Â∑•‰∏öÊú∫Âô®‰∫∫, \#‰∏çÁ°ÆÂÆö, \#Áü≠Êñá, \#ÁªºËø∞, /unread, Design, Users, ccfInfo: Not Found, citationNumber: Not Found},
}

@article{maggioni_if_2023,
	title = {If it looks like a human and speaks like a human ... {Communication} and cooperation in strategic {Human}‚Äì{Robot} interactions},
	volume = {104},
	issn = {22148043},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S221480432300037X},
	doi = {10.1016/j.socec.2023.102011},
	abstract = {This paper presents the results of a behavioral experiment conducted between February 2020 and March 2021 on a sample of about 500 university students that were randomly matched with either a human or a humanoid robot partner to play an iterated Prisoner‚Äôs Dilemma, to test whether their choices were influenced by the nature and behavior of their partner. The results show that subjects are more likely to cooperate with human rather than with robotic partners; that they are more likely to cooperate after receiving a verbal reaction following a sub-optimal social outcome; and that the effect of the verbal reaction is not dependent on the nature of the partner. Our findings provide new evidence on the effects of verbal communication in strategic frameworks that involves humanoid robotic partners. The results are robust to: the exclusion of students of Economics-related subjects, the inclusion of a set of psychological and behavioral controls, the subjects‚Äô perception on robots‚Äô behavior, and gender biases in human‚Äìhuman interactions.},
	language = {en},
	urldate = {2024-09-11},
	journal = {Journal of Behavioral and Experimental Economics},
	author = {Maggioni, Mario A. and Rossignoli, Domenico},
	month = jun,
	year = {2023},
	keywords = {\#EX: ‰∏çÂåÖÂê´HRI, \#EX:‰∏çÂåÖÂê´Â§ßÊ®°Âûã, \#EX:Â∑•‰∏öÊú∫Âô®‰∫∫, \#‰∏çÁ°ÆÂÆö, \#Áü≠Êñá, \#ÁªºËø∞, /unread, Design, Users, ccfInfo: Not Found, citationNumber: Not Found},
	pages = {102011},
}

@article{ellestrom_medium-centered_2018,
	title = {A medium-centered model of communication},
	volume = {2018},
	copyright = {De Gruyter expressly reserves the right to use all content for commercial text and data mining within the meaning of Section 44b of the German Copyright Act.},
	issn = {1613-3692},
	url = {https://www.degruyter.com/document/doi/10.1515/sem-2016-0024/html},
	doi = {10.1515/sem-2016-0024},
	abstract = {The aim of this article is to form a new communication model, which is centered on the intermediate stage of communication, here called medium. The model is intended to be irreducible, to highlight the essential communication entities and their interrelations, and potentially to cover all conceivable kinds of communication of meaning. It is designed to clearly account for both verbal and nonverbal meaning, the different roles played by minds and bodies in communication, and the relation between presemiotic and semiotic media features. As a result, the model also pinpoints fundamental obstacles for communication located in media products themselves, and demonstrates how Shannon‚Äôs model of transmission of computable data can be incorporated in a model of human communication of meaning.},
	language = {en},
	number = {224},
	urldate = {2024-09-11},
	journal = {Semiotica},
	author = {Ellestr√∂m, Lars},
	month = sep,
	year = {2018},
	note = {Publisher: De Gruyter Mouton},
	keywords = {\#EX: ‰∏çÂåÖÂê´HRI, \#EX:‰∏çÂåÖÂê´Â§ßÊ®°Âûã, \#EX:Â∑•‰∏öÊú∫Âô®‰∫∫, \#‰∏çÁ°ÆÂÆö, \#Áü≠Êñá, \#ÁªºËø∞, /unread, Design, Users, ccfInfo: Not Found, citationNumber: 28, communication model, intermediality, meaning, medium, multimodality, nonverbal communication},
	pages = {269--293},
}

@incollection{kanda_communication_2016,
	address = {Tokyo},
	title = {Communication for {Social} {Robots}},
	isbn = {978-4-431-54595-8},
	url = {https://doi.org/10.1007/978-4-431-54595-8_6},
	abstract = {This chapter overviews the studies in social robotics that deal with communication. First, we discuss how humans‚Äô natural communication is modeled into social robots. Second, we introduce a field study on how people and robots engage in communication in real-world environments.},
	language = {en},
	urldate = {2024-09-11},
	booktitle = {Cognitive {Neuroscience} {Robotics} {A}: {Synthetic} {Approaches} to {Human} {Understanding}},
	publisher = {Springer Japan},
	author = {Kanda, Takayuki and Miyashita, Takahiro},
	editor = {Kasaki, Masashi and Ishiguro, Hiroshi and Asada, Minoru and Osaka, Mariko and Fujikado, Takashi},
	year = {2016},
	doi = {10.1007/978-4-431-54595-8_6},
	keywords = {\#EX: ‰∏çÂåÖÂê´HRI, \#EX:‰∏çÂåÖÂê´Â§ßÊ®°Âûã, \#EX:Â∑•‰∏öÊú∫Âô®‰∫∫, \#‰∏çÁ°ÆÂÆö, \#Áü≠Êñá, \#ÁªºËø∞, /unread, Communication¬†Ê≤üÈÄö, Computational model¬†ËÆ°ÁÆóÊ®°Âûã, Deictic communication¬†ÊåáÁ§∫ÊÄß‰∫§ÊµÅ, Design, Social robots¬†Á§æ‰∫§Êú∫Âô®‰∫∫, Users, ccfInfo: Not Found},
	pages = {121--151},
}

@inproceedings{rigoll_multimodal_2015,
	address = {Cham},
	title = {Multimodal {Human}-{Robot} {Interaction} from the {Perspective} of a {Speech} {Scientist}},
	isbn = {978-3-319-23132-7},
	doi = {10.1007/978-3-319-23132-7_1},
	abstract = {Human-Robot-Interaction (HRI) is a research area that developed steadily during the last years. While robots in the last decades of the 20th century have been mostly constructed to work autonomously, the rise of service robots during the last 20 years has mostly contributed to the development of effective communication methods between human users and robots. This development has been even accelerated with the advancement of humanoid robots, where the demand for effective human-robot-interaction is even more obvious. It is also amazing to note that, inspired by the success of HRI in the area of service and humanoid robotics, human-robot-interfaces become nowadays even attractive for areas, where HRI has never played a major role before, especially for industrial robots or robots in outdoor environments. Compared to classical human-computer-interaction (HCI), one can say that the basic interaction algorithms are not that much different in HRI, e.g. a speech or gesture recognizer would not work much differently in both domains. The major differences between HCI and HRI are more in the different utilization of modalities, which also depends very much on the type of employed robot. Therefore, the primary goal of this paper is the description of the major differences between HCI and HRI and the presentation of the most important modalities used in HRI and how they affect the interaction depending on the various types of available robot platforms.},
	language = {en},
	booktitle = {Speech and {Computer}},
	publisher = {Springer International Publishing},
	author = {Rigoll, Gerhard},
	editor = {Ronzhin, Andrey and Potapova, Rodmonga and Fakotakis, Nikos},
	year = {2015},
	keywords = {\#EX: ‰∏çÂåÖÂê´HRI, \#EX:‰∏çÂåÖÂê´Â§ßÊ®°Âûã, \#EX:Â∑•‰∏öÊú∫Âô®‰∫∫, \#‰∏çÁ°ÆÂÆö, \#Áü≠Êñá, \#ÁªºËø∞, /unread, Brain-machine interfaces, Design, Human-robot interaction, Humanoid robotics, Multimodal dialogue, Service robots, Users, ccfInfo: CCF-None SPECOM, citationNumber: 3},
	pages = {3--10},
}

@article{bonarini_communication_2020,
	title = {Communication in {Human}-{Robot} {Interaction}},
	volume = {1},
	issn = {2662-4087},
	url = {https://doi.org/10.1007/s43154-020-00026-1},
	doi = {10.1007/s43154-020-00026-1},
	abstract = {To present the multi-faceted aspects of communication between robot and humans (HRI), putting in evidence that it is not limited to language-based interaction, but it includes all aspects that are relevant in communication among physical beings, exploiting all the available sensor channels.},
	language = {en},
	number = {4},
	urldate = {2024-09-11},
	journal = {Current Robotics Reports},
	author = {Bonarini, Andrea},
	month = dec,
	year = {2020},
	keywords = {\#EX: ‰∏çÂåÖÂê´HRI, \#EX:‰∏çÂåÖÂê´Â§ßÊ®°Âûã, \#EX:Â∑•‰∏öÊú∫Âô®‰∫∫, \#‰∏çÁ°ÆÂÆö, \#Áü≠Êñá, \#ÁªºËø∞, /unread, Artificial Intelligence¬†‰∫∫Â∑•Êô∫ËÉΩ, Communication, Design, Human-robot interaction, Robot design, Users, ccfInfo: Not Found, ‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è},
	pages = {279--285},
}

@article{sievers_project_2024,
	title = {Project {Report}: {Requirements} for a {Social} {Robot} as an {Information} {Provider} in the {Public} {Sector}},
	issn = {0933-1875, 1610-1987},
	url = {https://doi.org/10.1007/s13218-024-00840-1},
	doi = {10.1007/s13218-024-00840-1},
	abstract = {Abstract
            Is it possible to integrate a humanoid social robot into the work processes or customer care in an official environment, e.g. in municipal offices? If so, what could such an application scenario look like and what skills would the robot need to have when interacting with human customers? What are requirements for this kind of interactions? We have devised an application scenario for such a case, determined the necessary or desirable capabilities of the robot, developed a corresponding robot application and carried out initial tests and evaluations in a project together with the Kiel City Council. One of the most important insights gained in the project was that a humanoid robot with natural language processing capabilities based on large language models as well as human-like gestures and posture changes (animations) proved to be much more preferred by users compared to standard browser-based solutions on tablets for an information system in the City Council. Furthermore, we propose a connection of the ACT-R cognitive architecture with the robot, where an ACT-R model is used in interaction with the robot application to cognitively process and enhance a dialogue between human and robot.},
	language = {English},
	journal = {KI - K√ºnstliche Intelligenz},
	author = {Sievers, Thomas and Russwinkel, Nele},
	month = jul,
	year = {2024},
	note = {0 citations (Crossref/DOI) [2024-07-12]
Publisher: Springer Science and Business Media Deutschland GmbH},
	keywords = {\#EX: ‰∏çÂåÖÂê´HRI, \#EX:‰∏çÂåÖÂê´Â§ßÊ®°Âûã, \#EX:Â∑•‰∏öÊú∫Âô®‰∫∫, \#‰∏çÁ°ÆÂÆö, \#Áü≠Êñá, \#ÁªºËø∞, ACT-R, Anthropomorphic robots, Application scenario, City council, Design, Human robot interaction, Human-robot interaction, Humans-robot interactions, Information provider, Natural language processing systems, Project reports, Public sector, Social robotics, Social robots, Users, Work process, ccfInfo: Not Found, citationNumber: 0},
}

@article{eapen_personalization_2023,
	title = {Personalization and {Customization} of {LLM} {Responses}},
	volume = {4},
	doi = {10.55248/gengpi.4.1223.123512},
	abstract = {The field of natural language processing (NLP) has witnessed remarkable advancements in recent years, particularly with the development of large language models(LLMs). As these models become integral components of various applications, the need for personalized and customized responses has gained prominence. Thispaper explores the realm of personalization and customization within the context of LLM responses, aiming to enhance user interaction and satisfaction.The objective of this study is to investigate methodologies for tailoring LLM-generated responses to individual user preferences, thereby optimizing the overalluser experience. We delve into the challenges and opportunities presented by personalization and customization, addressing issues such as privacy concerns, ethicalconsiderations, and the delicate balance between generalization and specificity in response generation.Through a comprehensive review of existing literature and methodologies, we propose a framework that combines user profiling, contextual analysis, and feedbackmechanisms to dynamically adapt LLM responses. The proposed framework seeks to strike a balance between providing personalized content and maintaining theintegrity of the underlying language model.The potential applications of personalized LLM responses span a wide range of domains, including chatbots, virtual assistants, and content recommendation systems.By tailoring responses to individual users, we anticipate improvements in engagement, satisfaction, and the overall effectiveness of LLM-powered applications.},
	journal = {International Journal of Research Publication and Reviews},
	author = {Eapen, Joel and V S, Adhithyan},
	month = dec,
	year = {2023},
	keywords = {\#EX: ‰∏çÂåÖÂê´HRI, \#EX:‰∏çÂåÖÂê´Â§ßÊ®°Âûã, \#EX:Â∑•‰∏öÊú∫Âô®‰∫∫, \#‰∏çÁ°ÆÂÆö, \#Áü≠Êñá, \#ÁªºËø∞, /unread, Design, Users, ccfInfo: Not Found, citationNumber: Not Found},
	pages = {2617--2627},
}

@article{reimann_survey_2024,
	title = {A {Survey} on {Dialogue} {Management} in {Human}-robot {Interaction}},
	volume = {13},
	issn = {2573-9522},
	url = {https://dl.acm.org/doi/10.1145/3648605},
	doi = {10.1145/3648605},
	abstract = {As social robots see increasing deployment within the general public, improving the interaction with those robots is essential. Spoken language offers an intuitive interface for the human‚Äìrobot interaction (HRI), with dialogue management (DM) being a key component in those interactive systems. Yet, to overcome current challenges and manage smooth, informative, and engaging interaction, a more structural approach to combining HRI and DM is needed. In this systematic review, we analyze the current use of DM in HRI and focus on the type of dialogue manager used, its capabilities, evaluation methods, and the challenges specific to DM in HRI. We identify the challenges and current scientific frontier related to the DM approach, interaction domain, robot appearance, physical situatedness, and multimodality.},
	language = {en},
	number = {2},
	urldate = {2024-09-12},
	journal = {ACM Transactions on Human-Robot Interaction},
	author = {Reimann, Merle M. and Kunneman, Florian A. and Oertel, Catharine and Hindriks, Koen V.},
	month = jun,
	year = {2024},
	keywords = {\#EX: ‰∏çÂåÖÂê´HRI, \#EX:‰∏çÂåÖÂê´Â§ßÊ®°Âûã, \#EX:Â∑•‰∏öÊú∫Âô®‰∫∫, \#‰∏çÁ°ÆÂÆö, \#Áü≠Êñá, \#ÁªºËø∞, /unread, Design, Users, ccfInfo: CCF-None THRI, citationNumber: 0},
	pages = {1--22},
}

@inproceedings{kraus_kurt_2022,
	address = {Sapporo, Japan},
	title = {{KURT}: {A} {Household} {Assistance} {Robot} {Capable} of {Proactive} {Dialogue}},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {978-1-6654-0731-1},
	shorttitle = {{KURT}},
	url = {https://ieeexplore.ieee.org/document/9889357/},
	doi = {10.1109/HRI53351.2022.9889357},
	abstract = {In this work, we present a robot-dialogue framework to handle sophisticated robot-initiated interaction. We introduce a robotic assistant equipped with a dialogue system in a household assistance context. To become a truly collaborative companion, the assistant is able to engage in a proactive conversation for task assistance. The system actions are triggered by the recognition of persons or speciÔ¨Åc objects. To evaluate our system, we conducted a user study with 17 participants in a laboratory environment where users were able to interact with the system via natural language. The results showed that the behaviour of the system was accepted and perceived as trustworthy by the users.},
	language = {en},
	urldate = {2024-09-12},
	booktitle = {2022 17th {ACM}/{IEEE} {International} {Conference} on {Human}-{Robot} {Interaction} ({HRI})},
	publisher = {IEEE},
	author = {Kraus, Matthias and Wagner, Nicolas and Minker, Wolfgang and Agrawal, Ankita and Schmidt, Artur and Prasad, Pranav Krishna and Ertel, Wolfgang},
	month = mar,
	year = {2022},
	keywords = {\#EX: ‰∏çÂåÖÂê´HRI, \#EX:‰∏çÂåÖÂê´Â§ßÊ®°Âûã, \#EX:Â∑•‰∏öÊú∫Âô®‰∫∫, \#‰∏çÁ°ÆÂÆö, \#Áü≠Êñá, \#ÁªºËø∞, /unread, Design, Users, ccfInfo: CCF-None HRI, citationNumber: 2},
	pages = {855--859},
}

@book{jordan_usability_2014,
	address = {London},
	title = {Usability {Evaluation} {In} {Industry}},
	isbn = {978-0-429-15701-1},
	abstract = {This book provides a variety of answers in its description and discussion of new, sometimes radical approaches to `usability evaluation', now an increasingly common business tool. It contains new thinking of the subject of usability evaluation in industry. Contributions come from those involved in the practice of industry-based usability evaluation},
	publisher = {CRC Press},
	editor = {Jordan, Patrick W. and Thomas, B. and McClelland, Ian Lyall and Weerdmeester, Bernard},
	month = jul,
	year = {2014},
	doi = {10.1201/9781498710411},
	keywords = {\#EX: ‰∏çÂåÖÂê´HRI, \#EX:‰∏çÂåÖÂê´Â§ßÊ®°Âûã, \#EX:Â∑•‰∏öÊú∫Âô®‰∫∫, \#‰∏çÁ°ÆÂÆö, \#Áü≠Êñá, \#ÁªºËø∞, /unread, Design, Users, ccfInfo: Not Found, citationNumber: 687},
}

@article{bink_personalized_nodate,
	title = {Personalized {Response} with {Generative} {AI}: {Improving} {Customer} {Interaction} with {Zero}-{Shot} {Learning} {LLM} {Chatbots}},
	language = {en},
	author = {Bink, Jo√´lle},
	keywords = {\#EX: ‰∏çÂåÖÂê´HRI, \#EX:‰∏çÂåÖÂê´Â§ßÊ®°Âûã, \#EX:Â∑•‰∏öÊú∫Âô®‰∫∫, \#‰∏çÁ°ÆÂÆö, \#Áü≠Êñá, \#ÁªºËø∞, /unread, Design, Users, ccfInfo: Not Found, citationNumber: Not Found},
}

@inproceedings{andersen_projecting_2016,
	address = {New York, NY, USA},
	title = {Projecting robot intentions into human environments},
	isbn = {978-1-5090-3929-6},
	url = {http://ieeexplore.ieee.org/document/7745145/},
	doi = {10.1109/ROMAN.2016.7745145},
	urldate = {2024-09-12},
	booktitle = {2016 25th {IEEE} {International} {Symposium} on {Robot} and {Human} {Interactive} {Communication} ({RO}-{MAN})},
	publisher = {IEEE},
	author = {Andersen, Rasmus S. and Madsen, Ole and Moeslund, Thomas B. and Amor, Heni Ben},
	month = aug,
	year = {2016},
	keywords = {\#EX: ‰∏çÂåÖÂê´HRI, \#EX:‰∏çÂåÖÂê´Â§ßÊ®°Âûã, \#EX:Â∑•‰∏öÊú∫Âô®‰∫∫, \#‰∏çÁ°ÆÂÆö, \#Áü≠Êñá, \#ÁªºËø∞, /unread, Design, Users, ccfInfo: CCF-None RO-MAN, citationNumber: 77},
	pages = {294--301},
}

@article{kopp_revisiting_2021,
	title = {Revisiting {Human}-{Agent} {Communication}: {The} {Importance} of {Joint} {Co}-construction and {Understanding} {Mental} {States}},
	volume = {12},
	issn = {1664-1078},
	shorttitle = {Revisiting {Human}-{Agent} {Communication}},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8021865/},
	doi = {10.3389/fpsyg.2021.580955},
	abstract = {The study of human-human communication and the development of computational models for human-agent communication have diverged significantly throughout the last decade. Yet, despite frequently made claims of ‚Äúsuper-human performance‚Äù in, e.g., speech recognition or image processing, so far, no system is able to lead a half-decent coherent conversation with a human. In this paper, we argue that we must start to re-consider the hallmarks of cooperative communication and the core capabilities that we have developed for it, and which conversational agents need to be equipped with: incremental joint co-construction and mentalizing. We base our argument on a vast body of work on human-human communication and its psychological processes that we reason to be relevant and necessary to take into account when modeling human-agent communication. We contrast those with current conceptualizations of human-agent interaction and formulate suggestions for the development of future systems.},
	urldate = {2024-09-11},
	journal = {Frontiers in Psychology},
	author = {Kopp, Stefan and Kr√§mer, Nicole},
	month = mar,
	year = {2021},
	pmid = {33833705},
	pmcid = {PMC8021865},
	keywords = {\#EX: ‰∏çÂåÖÂê´HRI, \#EX:‰∏çÂåÖÂê´Â§ßÊ®°Âûã, \#EX:Â∑•‰∏öÊú∫Âô®‰∫∫, \#‰∏çÁ°ÆÂÆö, \#Áü≠Êñá, \#ÁªºËø∞, /unread, Design, Users, ccfInfo: Not Found, citationNumber: 26},
	pages = {580955},
}

@incollection{breazeal_social_2016,
	address = {Cham},
	title = {Social {Robotics}},
	isbn = {978-3-319-32552-1},
	url = {https://doi.org/10.1007/978-3-319-32552-1_72},
	abstract = {This chapter surveys some of the principal research trends in Social Robotics and its application to human‚Äìrobot interaction (HRIhuman‚Äìrobotinteraction). Social (or Sociable) robots are designed to interact with people in a¬†natural, interpersonal manner¬†‚Äì often to achieve positive outcomes in diverse applications such as education, health, quality of life, entertainment, communication, and tasks requiring collaborative teamwork. The long-term goal of creating social robots that are competent and capable partners for people is quite a¬†challenging task. They will need to be able to communicate naturally with people using both verbal and nonverbal signals. They will need to engage us not only on a¬†cognitive level, but on an emotional level as well in order to provide effective social and task-related support to people. They will need a¬†wide range of social-cognitive skills and a¬†theory of other minds to understand human behavior, and to be intuitively understood by people. A¬†deep understanding of human intelligence and behavior across multiple dimensions (i.‚ÄØe., cognitive, affective, physical, social, etc.) is necessary in order to design robots that can successfully play a¬†beneficial role in the daily lives of people. This requires a¬†multidisciplinary approach where the design of social robot technologies and methodologies are informed by robotics, artificial intelligence, psychology, neuroscience, human factors, design, anthropology, and more.},
	language = {en},
	urldate = {2024-09-12},
	booktitle = {Springer {Handbook} of {Robotics}},
	publisher = {Springer International Publishing},
	author = {Breazeal, Cynthia and Dautenhahn, Kerstin and Kanda, Takayuki},
	editor = {Siciliano, Bruno and Khatib, Oussama},
	year = {2016},
	doi = {10.1007/978-3-319-32552-1_72},
	keywords = {\#EX: ‰∏çÂåÖÂê´HRI, \#EX:‰∏çÂåÖÂê´Â§ßÊ®°Âûã, \#EX:Â∑•‰∏öÊú∫Âô®‰∫∫, \#‰∏çÁ°ÆÂÆö, \#Áü≠Êñá, \#ÁªºËø∞, /unread, Ambient Assisted Living, Design, False Belief Task, Humanoid Robot, Robot Interaction, Social Robot, Users, ccfInfo: CCF-None ROBO},
	pages = {1935--1972},
}

@inproceedings{stenmark_natural_2013,
	address = {Seoul, Korea (South)},
	title = {Natural language programming of industrial robots},
	isbn = {978-1-4799-1173-8},
	url = {http://ieeexplore.ieee.org/document/6695630/},
	doi = {10.1109/ISR.2013.6695630},
	urldate = {2024-09-13},
	booktitle = {{IEEE} {ISR} 2013},
	publisher = {IEEE},
	author = {Stenmark, Maj and Nugues, Pierre},
	month = oct,
	year = {2013},
	keywords = {\#EX: ‰∏çÂåÖÂê´HRI, \#EX:‰∏çÂåÖÂê´Â§ßÊ®°Âûã, \#EX:Â∑•‰∏öÊú∫Âô®‰∫∫, \#‰∏çÁ°ÆÂÆö, \#Áü≠Êñá, \#ÁªºËø∞, /unread, Design, Users, ccfInfo: CCF-None ISR, citationNumber: 49},
	pages = {1--5},
}

@article{zhang_large_2023,
	title = {Large language models for human‚Äìrobot interaction: {A} review},
	volume = {3},
	issn = {26673797},
	shorttitle = {Large language models for human‚Äìrobot interaction},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S2667379723000451},
	doi = {10.1016/j.birob.2023.100131},
	language = {en},
	number = {4},
	urldate = {2024-09-13},
	journal = {Biomimetic Intelligence and Robotics},
	author = {Zhang, Ceng and Chen, Junxin and Li, Jiatong and Peng, Yanhong and Mao, Zebing},
	month = dec,
	year = {2023},
	keywords = {\#EX: ‰∏çÂåÖÂê´HRI, \#EX:‰∏çÂåÖÂê´Â§ßÊ®°Âûã, \#EX:Â∑•‰∏öÊú∫Âô®‰∫∫, \#‰∏çÁ°ÆÂÆö, \#Áü≠Êñá, \#ÁªºËø∞, /unread, Design, Users, ccfInfo: Not Found, citationNumber: 0},
	pages = {100131},
}

@incollection{matuszek_learning_2013,
	address = {Heidelberg},
	title = {Learning to {Parse} {Natural} {Language} {Commands} to a {Robot} {Control} {System}},
	isbn = {978-3-319-00065-7},
	url = {https://doi.org/10.1007/978-3-319-00065-7_28},
	abstract = {As robots become more ubiquitous and capable of performing complex tasks, the importance of enabling untrained users to interact with them has increased. In response, unconstrained natural-language interaction with robots has emerged as a significant research area. We discuss the problem of parsing natural language commands to actions and control structures that can be readily implemented in a robot execution system. Our approach learns a parser based on example pairs of English commands and corresponding control language expressions. We evaluate this approach in the context of following route instructions through an indoor environment, and demonstrate that our system can learn to translate English commands into sequences of desired actions, while correctly capturing the semantic intent of statements involving complex control structures. The procedural nature of our formal representation allows a robot to interpret route instructions online while moving through a previously unknown environment.},
	language = {en},
	urldate = {2024-09-13},
	booktitle = {Experimental {Robotics}: {The} 13th {International} {Symposium} on {Experimental} {Robotics}},
	publisher = {Springer International Publishing},
	author = {Matuszek, Cynthia and Herbst, Evan and Zettlemoyer, Luke and Fox, Dieter},
	editor = {Desai, Jaydev P. and Dudek, Gregory and Khatib, Oussama and Kumar, Vijay},
	year = {2013},
	doi = {10.1007/978-3-319-00065-7_28},
	keywords = {\#EX: ‰∏çÂåÖÂê´HRI, \#EX:‰∏çÂåÖÂê´Â§ßÊ®°Âûã, \#EX:Â∑•‰∏öÊú∫Âô®‰∫∫, \#‰∏çÁ°ÆÂÆö, \#Áü≠Êñá, \#ÁªºËø∞, /unread, Categorial Grammar, Design, Lexical Item, Natural Language, Robot Control, Statistical Machine Translation, Users, ccfInfo: CCF-None ISER, citationNumber: 434},
	pages = {403--415},
}

@misc{noauthor_natural_nodate,
	title = {Natural language programming of industrial robots {\textbar} {IEEE} {Conference} {Publication} {\textbar} {IEEE} {Xplore}},
	url = {https://ieeexplore.ieee.org/abstract/document/6695630},
	urldate = {2024-09-13},
	keywords = {\#EX: ‰∏çÂåÖÂê´HRI, \#EX:‰∏çÂåÖÂê´Â§ßÊ®°Âûã, \#EX:Â∑•‰∏öÊú∫Âô®‰∫∫, \#‰∏çÁ°ÆÂÆö, \#Áü≠Êñá, \#ÁªºËø∞, /unread, Design, Users, ccfInfo: Not Found, citationNumber: Not Found},
}

@article{cuayahuitl_interactive_nodate,
	title = {An {Interactive} {Humanoid} {Robot} {Exhibiting} {Flexible} {Sub}-{Dialogues}},
	abstract = {We demonstrate a conversational humanoid robot that allows users to follow their own dialogue structures. Our system uses a hierarchy of reinforcement learning dialogue agents, which support transitions across sub-dialogues in order to relax the strictness of hierarchical control and therefore support Ô¨Çexible interactions. We demonstrate our system with the Nao robot playing two versions of a Quiz game. Whilst language input and dialogue control is autonomous or wizarded, language output is provided by the robot combining verbal and non-verbal contributions. The novel features in our system are (a) the Ô¨Çexibility given to users to navigate Ô¨Çexibly in the interaction; and (b) a framework for investigating adaptive and Ô¨Çexible dialogues.},
	language = {en},
	author = {Cuayahuitl, Heriberto and Kruijff-Korbayova, Ivana},
	keywords = {\#EX: ‰∏çÂåÖÂê´HRI, \#EX:‰∏çÂåÖÂê´Â§ßÊ®°Âûã, \#EX:Â∑•‰∏öÊú∫Âô®‰∫∫, \#‰∏çÁ°ÆÂÆö, \#Áü≠Êñá, \#ÁªºËø∞, /unread, Design, Users, ccfInfo: CCF-B NAACL, citationNumber: 8},
}

@inproceedings{pramanick_your_2019,
	address = {New Delhi, India},
	title = {Your instruction may be crisp, but not clear to me!},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	isbn = {978-1-7281-2622-7},
	url = {https://ieeexplore.ieee.org/document/8956431/},
	doi = {10.1109/RO-MAN46459.2019.8956431},
	abstract = {The number of robots deployed in our daily surroundings is ever-increasing. Even in the industrial setup, the use of coworker robots is increasing rapidly. These cohabitant robots perform various tasks as instructed by colocated human beings. Thus, a natural interaction mechanism plays a big role in the usability and acceptability of the robot, especially by a non-expert user. The recent development in natural language processing (NLP) has paved the way for chatbots to generate an automatic response for users‚Äô query. A robot can be equipped with such a dialogue system. However, the goal of human-robot interaction is not focused on generating a response to queries, but it often involves performing some tasks in the physical world. Thus, a system is required that can detect user intended task from the natural instruction along with the set of pre- and post-conditions. In this work, we develop a dialogue engine for a robot that can classify and map a task instruction to the robot‚Äôs capability. If there is some ambiguity in the instructions or some required information is missing, which is often the case in natural conversation, it asks an appropriate question(s) to resolve it. The goal is to generate minimal and pin-pointed queries for the user to resolve an ambiguity. We evaluate our system for a telepresence scenario where a remote user instructs the robot for various tasks. Our study based on 12 individuals shows that the proposed dialogue strategy can help a novice user to effectively interact with a robot, leading to satisfactory user experience.},
	language = {en},
	urldate = {2024-09-12},
	booktitle = {2019 28th {IEEE} {International} {Conference} on {Robot} and {Human} {Interactive} {Communication} ({RO}-{MAN})},
	publisher = {IEEE},
	author = {Pramanick, Pradip and Sarkar, Chayan and Bhattacharya, Indrajit},
	month = oct,
	year = {2019},
	keywords = {\#EX: ‰∏çÂåÖÂê´HRI, \#EX:‰∏çÂåÖÂê´Â§ßÊ®°Âûã, \#EX:Â∑•‰∏öÊú∫Âô®‰∫∫, \#‰∏çÁ°ÆÂÆö, \#Áü≠Êñá, \#ÁªºËø∞, /unread, Design, Users, ccfInfo: CCF-None RO-MAN, citationNumber: 5},
	pages = {1--8},
}

@article{bartneck_measurement_2009,
	title = {Measurement {Instruments} for the {Anthropomorphism}, {Animacy}, {Likeability}, {Perceived} {Intelligence}, and {Perceived} {Safety} of {Robots}},
	volume = {1},
	issn = {1875-4805},
	url = {https://doi.org/10.1007/s12369-008-0001-3},
	doi = {10.1007/s12369-008-0001-3},
	abstract = {This study emphasizes the need for standardized measurement tools for human robot interaction (HRI). If we are to make progress in this field then we must be able to compare the results from different studies. A¬†literature review has been performed on the measurements of five key concepts in HRI: anthropomorphism, animacy, likeability, perceived intelligence, and perceived safety. The results have been distilled into five consistent questionnaires using semantic differential scales. We report reliability and validity indicators based on several empirical studies that used these questionnaires. It is our hope that these questionnaires can be used by robot developers to monitor their progress. Psychologists are invited to further develop the questionnaires by adding new concepts, and to conduct further validations where it appears necessary.},
	language = {en},
	number = {1},
	urldate = {2024-09-13},
	journal = {International Journal of Social Robotics},
	author = {Bartneck, Christoph and Kuliƒá, Dana and Croft, Elizabeth and Zoghbi, Susana},
	month = jan,
	year = {2009},
	keywords = {\#EX: ‰∏çÂåÖÂê´HRI, \#EX:‰∏çÂåÖÂê´Â§ßÊ®°Âûã, \#EX:Â∑•‰∏öÊú∫Âô®‰∫∫, \#‰∏çÁ°ÆÂÆö, \#Áü≠Êñá, \#ÁªºËø∞, /unread, Artificial Intelligence, Design, Human factors, Measurement, Perception, Robot, Users, ccfInfo: CCF-None IJSR, citationNumber: 109},
	pages = {71--81},
}

@misc{noauthor_system_nodate,
	title = {A system for learning continuous human-robot interactions from human-human demonstrations},
	url = {https://ieeexplore.ieee.org/abstract/document/7989334},
	abstract = {We present a data-driven imitation learning system for learning human-robot interactions from human-human demonstrations. During training, the movements of two interaction partners are recorded through motion capture and an interaction model is learned. At runtime, the interaction model is used to continuously adapt the robot's motion, both spatially and temporally, to the movements of the human interaction partner. We show the effectiveness of the approach on complex, sequential tasks by presenting two applications involving collaborative human-robot assembly. Experiments with varied object hand-over positions and task execution speeds confirm the capabilities for spatio-temporal adaption of the demonstrated behavior to the current situation.},
	language = {zh-CN},
	urldate = {2024-09-13},
	keywords = {\#EX: ‰∏çÂåÖÂê´HRI, \#EX:‰∏çÂåÖÂê´Â§ßÊ®°Âûã, \#EX:Â∑•‰∏öÊú∫Âô®‰∫∫, \#‰∏çÁ°ÆÂÆö, \#Áü≠Êñá, \#ÁªºËø∞, /unread, Design, Users, ccfInfo: CCF-B ICRA, citationNumber: 83},
}

@inproceedings{fischer_meta-design_2000,
	address = {New York, NY, USA},
	series = {{DIS} '00},
	title = {Meta-design: design for designers},
	isbn = {978-1-58113-219-9},
	shorttitle = {Meta-design},
	url = {https://dl.acm.org/doi/10.1145/347642.347798},
	doi = {10.1145/347642.347798},
	abstract = {One fundamental challenge for the design of the interactive systems of the future is to invent and design environments and cultures in which humans can express themselves and engage in personally meaningful activities. Unfortunately, a large number of new media are designed from a perspective of viewing and treating humans primarily as consumers. The possibility for humans to be and act as designers  (in cases in which they desire to do so) should be accessible not only to a small group of high-tech scribes, but rather to all interested individuals and groups. Meta-design characterizes activities, processes, and objectives to create new media and environments that allow users to act as designers and be creative.In this paper we discuss problems addressed by our research on meta-design, provide a conceptual framework for meta-design, and illustrate our developments in the context of a particular system, the Envisionment and Discovery Collaboratory.},
	urldate = {2024-09-13},
	booktitle = {Proceedings of the 3rd conference on {Designing} interactive systems: processes, practices, methods, and techniques},
	publisher = {Association for Computing Machinery},
	author = {Fischer, Gerhard and Scharff, Eric},
	year = {2000},
	keywords = {\#EX: ‰∏çÂåÖÂê´HRI, \#EX:‰∏çÂåÖÂê´Â§ßÊ®°Âûã, \#EX:Â∑•‰∏öÊú∫Âô®‰∫∫, \#‰∏çÁ°ÆÂÆö, \#Áü≠Êñá, \#ÁªºËø∞, /unread, Design, Users, ccfInfo: CCF-C DIS},
	pages = {396--405},
}

@inproceedings{zhang_patterns_2021,
	address = {Yokohama Japan},
	title = {Patterns for {Representing} {Knowledge} {Graphs} to {Communicate} {Situational} {Knowledge} of {Service} {Robots}},
	isbn = {978-1-4503-8096-6},
	url = {https://dl.acm.org/doi/10.1145/3411764.3445767},
	doi = {10.1145/3411764.3445767},
	language = {en},
	urldate = {2024-09-13},
	booktitle = {Proceedings of the 2021 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Zhang, Shengchen and Wang, Zixuan and Chen, Chaoran and Dai, Yi and Ye, Lyumanshan and Sun, Xiaohua},
	month = may,
	year = {2021},
	keywords = {\#EX: ‰∏çÂåÖÂê´HRI, \#EX:‰∏çÂåÖÂê´Â§ßÊ®°Âûã, \#EX:Â∑•‰∏öÊú∫Âô®‰∫∫, \#‰∏çÁ°ÆÂÆö, \#Áü≠Êñá, \#ÁªºËø∞, /unread, Design, Users, ccfInfo: CCF-A CHI, citationNumber: 6},
	pages = {1--12},
}

@misc{noauthor_genquery_nodate,
	title = {{GenQuery}: {Supporting} {Expressive} {Visual} {Search} with {Generative} {Models} {\textbar} {Proceedings} of the 2024 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	url = {https://dl.acm.org/doi/10.1145/3613904.3642847},
	urldate = {2024-11-16},
	keywords = {/unread, ccfInfo: Not Found, citationNumber: Not Found},
}

@inproceedings{hei_bilingual_2024,
	address = {New York, NY, USA},
	series = {{HRI} '24},
	title = {A {Bilingual} {Social} {Robot} with {Sign} {Language} and {Natural} {Language}},
	isbn = {979-8-4007-0323-2},
	url = {https://dl.acm.org/doi/10.1145/3610978.3640549},
	doi = {10.1145/3610978.3640549},
	abstract = {In situations where both deaf and non-deaf individuals are present in a public setting, it would be advantageous for a robot to communicate using both sign and natural languages simultaneously. This would not only address the needs for diverse users but also pave the way for a richer and more inclusive spectrum of human-robot interactions. To achieve this, a framework for a bilingual robot has been proposed in this paper. The robot exhibits the ability to articulate messages in spoken language, complemented by non-verbal cues such as expressive gestures, all while concurrently conveying information through sign language. The system can generate natural language expressions with speech audio, spontaneous prosody-based gestures, and sign language displayed on a virtual avatar on a robot's screen. The preliminary findings from this research showcase the robot's capacity to seamlessly blend natural language expressions with synchronized gestures and sign language, underlining its potential to revolutionize communication dynamics in diverse settings.},
	language = {en-US},
	urldate = {2024-11-13},
	booktitle = {Companion of the 2024 {ACM}/{IEEE} {International} {Conference} on {Human}-{Robot} {Interaction}},
	publisher = {Association for Computing Machinery},
	author = {Hei, Xiaoxuan and Yu, Chuang and Zhang, Heng and Tapus, Adriana},
	month = mar,
	year = {2024},
	keywords = {/unread, ccfInfo: CCF-None HRI, citationNumber: 0},
	pages = {526--529},
}

@inproceedings{wang_user_2023,
	address = {Hamburg Germany},
	title = {A {User} {Interface} for {Sense}-making of the {Reasoning} {Process} while {Interacting} with {Robots}},
	isbn = {978-1-4503-9422-2},
	url = {https://dl.acm.org/doi/10.1145/3544549.3585886},
	doi = {10.1145/3544549.3585886},
	language = {en},
	urldate = {2024-11-11},
	booktitle = {Extended {Abstracts} of the 2023 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Wang, Chao and Deigmoeller, Joerg and An, Pengcheng and Eggert, Julian},
	month = apr,
	year = {2023},
	keywords = {/unread, ccfInfo: CCF-A CHI, citationNumber: 1},
	pages = {1--7},
}

@inproceedings{vogt_system_2017,
	title = {A system for learning continuous human-robot interactions from human-human demonstrations},
	url = {https://ieeexplore.ieee.org/document/7989334/?arnumber=7989334},
	doi = {10.1109/ICRA.2017.7989334},
	abstract = {We present a data-driven imitation learning system for learning human-robot interactions from human-human demonstrations. During training, the movements of two interaction partners are recorded through motion capture and an interaction model is learned. At runtime, the interaction model is used to continuously adapt the robot's motion, both spatially and temporally, to the movements of the human interaction partner. We show the effectiveness of the approach on complex, sequential tasks by presenting two applications involving collaborative human-robot assembly. Experiments with varied object hand-over positions and task execution speeds confirm the capabilities for spatio-temporal adaption of the demonstrated behavior to the current situation.},
	urldate = {2024-09-13},
	booktitle = {2017 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	author = {Vogt, David and Stepputtis, Simon and Grehl, Steve and Jung, Bernhard and Ben Amor, Heni},
	month = may,
	year = {2017},
	keywords = {\#EX: ‰∏çÂåÖÂê´HRI, \#EX:‰∏çÂåÖÂê´Â§ßÊ®°Âûã, \#EX:Â∑•‰∏öÊú∫Âô®‰∫∫, \#‰∏çÁ°ÆÂÆö, \#Áü≠Êñá, \#ÁªºËø∞, /unread, Adaptation models, Collaboration, Design, Hidden Markov models, Human-robot interaction, Robot kinematics, Runtime, Users, ccfInfo: CCF-B ICRA, citationNumber: 83},
	pages = {2882--2889},
}

@inproceedings{vogt_system_2017-1,
	address = {Singapore, Singapore},
	title = {A system for learning continuous human-robot interactions from human-human demonstrations},
	isbn = {978-1-5090-4633-1},
	url = {http://ieeexplore.ieee.org/document/7989334/},
	doi = {10.1109/ICRA.2017.7989334},
	urldate = {2024-09-13},
	booktitle = {2017 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	publisher = {IEEE},
	author = {Vogt, David and Stepputtis, Simon and Grehl, Steve and Jung, Bernhard and Ben Amor, Heni},
	month = may,
	year = {2017},
	keywords = {\#EX: ‰∏çÂåÖÂê´HRI, \#EX:‰∏çÂåÖÂê´Â§ßÊ®°Âûã, \#EX:Â∑•‰∏öÊú∫Âô®‰∫∫, \#‰∏çÁ°ÆÂÆö, \#Áü≠Êñá, \#ÁªºËø∞, /unread, Design, Users, ccfInfo: CCF-B ICRA, citationNumber: 83},
	pages = {2882--2889},
}

@incollection{kramer_human-agent_2012,
	address = {Berlin, Heidelberg},
	title = {Human-{Agent} and {Human}-{Robot} {Interaction} {Theory}: {Similarities} to and {Differences} from {Human}-{Human} {Interaction}},
	isbn = {978-3-642-25691-2},
	shorttitle = {Human-{Agent} and {Human}-{Robot} {Interaction} {Theory}},
	url = {https://doi.org/10.1007/978-3-642-25691-2_9},
	abstract = {It will be discussed whether a theory specific for human-robot and human-agent interaction is needed or whether theories from human-human interactions can be adapted. First, theories from human-human interaction will be presented. Then, empirical evidence from human-robot- and human-agent interaction is presented. Research suggests that, from the perspective of the user, interaction with an artificial entity is similar to interaction with fellow humans. Explanations for this treatment of agents/robots in a social way (such as the ethopoeia approach, Nass\& Moon, 2000) assume that due to our social nature humans will use their interaction routines also when confronted with artificial entities. Based on this it will be discussed whether theories from human-human-interaction will be a helpful framework also for human-agent/robot interaction, whether amendments will be beneficial or whether, alternatively, a totally new approach is needed.},
	language = {en},
	urldate = {2024-09-13},
	booktitle = {Human-{Computer} {Interaction}: {The} {Agency} {Perspective}},
	publisher = {Springer},
	author = {Kr√§mer, Nicole C. and von der P√ºtten, Astrid and Eimler, Sabrina},
	editor = {Zacarias, Marielba and de Oliveira, Jos√© Valente},
	year = {2012},
	doi = {10.1007/978-3-642-25691-2_9},
	keywords = {\#EX: ‰∏çÂåÖÂê´HRI, \#EX:‰∏çÂåÖÂê´Â§ßÊ®°Âûã, \#EX:Â∑•‰∏öÊú∫Âô®‰∫∫, \#‰∏çÁ°ÆÂÆö, \#Áü≠Êñá, \#ÁªºËø∞, /unread, Common Ground, Design, Perspective Taking, Social Exchange Theory, Users, Virtual Agent, Virtual Human, ccfInfo: CCF-None SCI, citationNumber: 53},
	pages = {215--240},
}

@article{do_grounding_2024,
	title = {Grounding with {Structure}: {Exploring} {Design} {Variations} of {Grounded} {Human}-{AI} {Collaboration} in a {Natural} {Language} {Interface}},
	volume = {8},
	issn = {2573-0142},
	shorttitle = {Grounding with {Structure}},
	url = {https://dl.acm.org/doi/10.1145/3686902},
	doi = {10.1145/3686902},
	abstract = {Selecting an effective utterance among countless possibilities that match a user's intention poses a challenge when using natural language interfaces. To address the challenge, we leveraged the principle of least collaborative effort in communication grounding theory and designed three grounded conversational interactions: 1) a grounding interface allows users to start with a provisional input and then invite a conversational agent to complete their input, 2) a multiple grounding interface presents multiple inputs for the user to select from, and 3) a structured grounding interface guides users to write inputs in a structure best understood by the system. We compared our three grounding interfaces to an ungrounded control interface in a crowdsourced study (N=80) using a natural language system that generates small programs. We found that the grounding interfaces reduced cognitive load and improved task performance. The structured grounding interface further reduced speaker change costs and improved technology acceptance, without sacrificing the perception of control. We discuss the implications of designing grounded conversational interactions in natural language systems.},
	language = {en},
	number = {CSCW2},
	urldate = {2024-11-25},
	journal = {Proceedings of the ACM on Human-Computer Interaction},
	author = {Do, Hyo Jin and Brachman, Michelle and Dugan, Casey and Johnson, James M. and Lauer, Julia and Rai, Priyanshu and Pan, Qian},
	month = nov,
	year = {2024},
	note = {TLDR: Three grounded conversational interactions were designed that reduced cognitive load and improved task performance, and the structured grounding interface further reduced speaker change costs and improved technology acceptance, without sacrificing the perception of control.},
	keywords = {/unread, ccfInfo: CCF-C PACMHCI, citationNumber: 0},
	pages = {1--27},
}

@inproceedings{feng_coprompt_2024,
	address = {New York, NY, USA},
	series = {{CHI} '24},
	title = {{CoPrompt}: {Supporting} {Prompt} {Sharing} and {Referring} in {Collaborative} {Natural} {Language} {Programming}},
	isbn = {979-8-4007-0330-0},
	shorttitle = {{CoPrompt}},
	url = {https://doi.org/10.1145/3613904.3642212},
	doi = {10.1145/3613904.3642212},
	abstract = {Natural language (NL) programming has become more approachable due to the powerful code-generation capability of large language models (LLMs). This shift to using NL to program enhances collaborative programming by reducing communication barriers and context-switching among programmers from varying backgrounds. However, programmers may face challenges during prompt engineering in a collaborative setting as they need to actively keep aware of their collaborators‚Äô progress and intents. In this paper, we aim to investigate ways to assist programmers‚Äô prompt engineering in a collaborative context. We first conducted a formative study to understand the workflows and challenges of programmers when using NL for collaborative programming. Based on our findings, we implemented a prototype, CoPrompt, to support collaborative prompt engineering by providing referring, requesting, sharing, and linking mechanisms. Our user study indicates that CoPrompt assists programmers in comprehending collaborators‚Äô prompts and building on their collaborators‚Äô work, reducing repetitive updates and communication costs.},
	urldate = {2024-11-25},
	booktitle = {Proceedings of the 2024 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Feng, Li and Yen, Ryan and You, Yuzhe and Fan, Mingming and Zhao, Jian and Lu, Zhicong},
	month = may,
	year = {2024},
	note = {TLDR: A prototype, CoPrompt, is implemented to support collaborative prompt engineering by providing referring, requesting, sharing, and linking mechanisms, and indicates that CoPrompt assists programmers in comprehending collaborators‚Äô prompts and building on their collaborators‚Äô work, reducing repetitive updates and communication costs.},
	keywords = {/unread, ccfInfo: CCF-A CHI, citationNumber: 0},
	pages = {1--21},
}

@inproceedings{liu_what_2023,
	address = {New York, NY, USA},
	series = {{CHI} '23},
	title = {‚Äú{What} {It} {Wants} {Me} {To} {Say}‚Äù: {Bridging} the {Abstraction} {Gap} {Between} {End}-{User} {Programmers} and {Code}-{Generating} {Large} {Language} {Models}},
	isbn = {978-1-4503-9421-5},
	shorttitle = {‚Äú{What} {It} {Wants} {Me} {To} {Say}‚Äù},
	url = {https://dl.acm.org/doi/10.1145/3544548.3580817},
	doi = {10.1145/3544548.3580817},
	abstract = {Code-generating large language models map natural language to code. However, only a small portion of the infinite space of naturalistic utterances is effective at guiding code generation. For non-expert end-user programmers, learning this is the challenge of abstraction matching. We examine this challenge in the specific context of data analysis in spreadsheets, in a system that maps the user‚Äôs natural language query to Python code using the Codex generator, executes the code, and shows the result. We propose grounded abstraction matching, which bridges the abstraction gap by translating the code back into a systematic and predictable naturalistic utterance. In a between-subjects, think-aloud study (n=24), we compare grounded abstraction matching to an ungrounded alternative based on previously established query framing principles. We find that the grounded approach improves end-users‚Äô understanding of the scope and capabilities of the code-generating model, and the kind of language needed to use it effectively.},
	language = {en-US},
	urldate = {2024-11-25},
	booktitle = {Proceedings of the 2023 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Liu, Michael Xieyang and Sarkar, Advait and Negreanu, Carina and Zorn, Benjamin and Williams, Jack and Toronto, Neil and Gordon, Andrew D.},
	month = apr,
	year = {2023},
	keywords = {/unread, ccfInfo: CCF-A CHI, citationNumber: 1},
	pages = {1--31},
}

@article{cappiello_ui-centric_2015,
	title = {A {UI}-{Centric} {Approach} for the {End}-{User} {Development} of {Multidevice} {Mashups}},
	volume = {9},
	issn = {1559-1131},
	url = {https://dl.acm.org/doi/10.1145/2735632},
	doi = {10.1145/2735632},
	abstract = {In recent years, models, composition paradigms, and tools for mashup development have been proposed to support the integration of information sources, services and APIs available on the Web. The challenge is to provide a gate to a ‚Äúprogrammable Web,‚Äù where end users are allowed to construct easily composite applications that merge content and functions so as to satisfy the long tail of their specific needs. The approaches proposed so far do not fully accommodate this vision. This article, therefore, proposes a mashup development framework that is oriented toward the End-User Development. Given the fundamental role of user interfaces (UIs) as a medium easily understandable by the end users, the proposed approach is characterized by UI-centric models able to support a WYSIWYG (What You See Is What You Get) specification of data integration and service orchestration. It, therefore, contributes to the definition of adequate abstractions that, by hiding the technology and implementation complexity, can be adopted by the end users in a kind of ‚Äúdemocratic‚Äù paradigm for mashup development. This article also shows how model-to-code generative techniques translate models into application schemas, which in turn guide the dynamic instantiation of the composite applications at runtime. This is achieved through lightweight execution environments that can be deployed on the Web and on mobile devices to support the pervasive use of the created applications.},
	language = {en-US},
	number = {3},
	urldate = {2024-11-24},
	journal = {ACM Trans. Web},
	author = {Cappiello, Cinzia and Matera, Maristella and Picozzi, Matteo},
	month = jun,
	year = {2015},
	keywords = {/unread, ccfInfo: CCF-B TWEB, citationNumber: 46},
	pages = {11:1--11:40},
}

@article{kim_survey_2024,
	title = {A survey on integration of large language models with intelligent robots},
	volume = {17},
	issn = {1861-2784},
	url = {https://doi.org/10.1007/s11370-024-00550-5},
	doi = {10.1007/s11370-024-00550-5},
	abstract = {In recent years, the integration of large language models (LLMs) has revolutionized the field of robotics, enabling robots to communicate, understand, and reason with human-like proficiency. This paper explores the multifaceted impact of LLMs on robotics, addressing key challenges and opportunities for leveraging these models across various domains. By categorizing and analyzing LLM applications within core robotics elements‚Äîcommunication, perception, planning, and control‚Äîwe aim to provide actionable insights for researchers seeking to integrate LLMs into their robotic systems. Our investigation focuses on LLMs developed post-GPT-3.5, primarily in text-based modalities while also considering multimodal approaches for perception and control. We offer comprehensive guidelines and examples for prompt engineering, facilitating beginners‚Äô access to LLM-based robotics solutions. Through tutorial-level examples and structured prompt construction, we illustrate how LLM-guided enhancements can be seamlessly integrated into robotics applications. This survey serves as a roadmap for researchers navigating the evolving landscape of LLM-driven robotics, offering a comprehensive overview and practical guidance for harnessing the power of language models in robotics development.},
	language = {en},
	number = {5},
	urldate = {2024-11-26},
	journal = {Intelligent Service Robotics},
	author = {Kim, Yeseung and Kim, Dohyun and Choi, Jieun and Park, Jisang and Oh, Nayoung and Park, Daehyung},
	month = sep,
	year = {2024},
	keywords = {/unread, ccfInfo: CCF-None ISROB, citationNumber: 0, ‰∫∫Â∑•Êô∫ËÉΩ, ÂèäÊó∂Â∑•Á®ã, Â§öÊ®°ÊÄÅÊÑüÁü•, Â§ßÂûãËØ≠Ë®ÄÊ®°Âûã, Êô∫ËÉΩÊú∫Âô®‰∫∫},
	pages = {1091--1107},
}

@article{author_gencomui_nodate,
	title = {Á¨¨‰∏ÄÊ¨°Êèê‰∫§ÁâàÊú¨Ôºö{GenComUI}: {Exploring} {Generative} {UI} as {Medium} to {Support} {Task}-{Oriented} {Human}-{Robot} {Communication}},
	abstract = {CCS Concepts: ‚Ä¢ Do Not Use This Code ‚Üí Generate the Correct Terms for Your Paper; Generate the Correct Terms for Your Paper; Generate the Correct Terms for Your Paper; Generate the Correct Terms for Your Paper.},
	language = {en},
	author = {Author, Anonymous},
	keywords = {/unread, ccfInfo: Not Found, citationNumber: Not Found},
}

@article{noauthor_review_nodate,
	title = {{Á¨¨‰∏ÄÊ¨°Êèê‰∫§ÁöÑReview} Ôºö{Gmail} - {Fw}: [{CHI} 2025] \#5034 {Revise} \& {Resubmit} {Decision}},
	language = {en},
	keywords = {/unread, ccfInfo: Not Found, citationNumber: Not Found},
}

@misc{ym_pwr_2023,
	title = {{PwR}: {Exploring} the {Role} of {Representations} in {Conversational} {Programming}},
	shorttitle = {{PwR}},
	url = {http://arxiv.org/abs/2309.09495},
	doi = {10.48550/arXiv.2309.09495},
	abstract = {Large Language Models (LLMs) have revolutionized programming and software engineering. AI programming assistants such as GitHub Copilot X enable conversational programming, narrowing the gap between human intent and code generation. However, prior literature has identified a key challenge--there is a gap between user's mental model of the system's understanding after a sequence of natural language utterances, and the AI system's actual understanding. To address this, we introduce Programming with Representations (PwR), an approach that uses representations to convey the system's understanding back to the user in natural language. We conducted an in-lab task-centered study with 14 users of varying programming proficiency and found that representations significantly improve understandability, and instilled a sense of agency among our participants. Expert programmers use them for verification, while intermediate programmers benefit from confirmation. Natural language-based development with LLMs, coupled with representations, promises to transform software development, making it more accessible and efficient.},
	urldate = {2024-11-25},
	publisher = {arXiv},
	author = {YM, Pradyumna and Ganesan, Vinod and Arumugam, Dinesh Kumar and Gupta, Meghna and Shadagopan, Nischith and Dixit, Tanay and Segal, Sameer and Kumar, Pratyush and Jain, Mohit and Rajamani, Sriram},
	month = sep,
	year = {2023},
	note = {arXiv:2309.09495},
	keywords = {/unread, Computer Science - Human-Computer Interaction, Computer Science - Software Engineering, ccfInfo: CCF-None CORR, citationNumber: 0},
}

@misc{yan_intelliexplain_2024,
	title = {{IntelliExplain}: {Enhancing} {Conversational} {Code} {Generation} for {Non}-{Professional} {Programmers}},
	shorttitle = {{IntelliExplain}},
	url = {http://arxiv.org/abs/2405.10250},
	doi = {10.48550/arXiv.2405.10250},
	abstract = {Chat LLMs such as GPT-3.5-turbo and GPT-4 have shown promise in assisting humans in coding, particularly by enabling them to conversationally provide feedback. However, current approaches assume users have expert debugging skills, limiting accessibility for non-professional programmers. In this paper, we first explore Chat LLMs' limitations in assisting non-professional programmers with coding. Through a formative study, we identify two key elements affecting their experience: the way a Chat LLM explains its generated code and the structure of human-LLM interaction. We then propose IntelliExplain, a new conversational code generation framework with enhanced code explanations and a structured interaction paradigm, which enforces both better code understanding and a more effective feedback loop. In two programming tasks (SQL and Python), IntelliExplain yields significantly higher success rates and reduces task time compared to the vanilla Chat LLM. We also identify several opportunities that remain in effectively offering a chat-based programming experience for non-professional programmers.},
	urldate = {2024-11-25},
	publisher = {arXiv},
	author = {Yan, Hao and Latoza, Thomas D. and Yao, Ziyu},
	month = oct,
	year = {2024},
	note = {arXiv:2405.10250},
	keywords = {/unread, Computer Science - Human-Computer Interaction, ccfInfo: Not Found, citationNumber: 0},
}

@inproceedings{alexandrova_robot_2014,
	title = {Robot {Programming} by {Demonstration} with {Interactive} {Action} {Visualizations}},
	isbn = {978-0-9923747-0-9},
	url = {http://www.roboticsproceedings.org/rss10/p48.pdf},
	doi = {10.15607/RSS.2014.X.048},
	abstract = {Existing approaches to Robot Programming by Demonstration (PbD) require multiple demonstrations to capture task information that lets robots generalize to unseen situations. However, providing these demonstrations is cumbersome for endusers. In addition, users who are not familiar with the system often fail to demonstrate sufÔ¨Åciently varied demonstrations. We propose an alternative PbD framework that involves demonstrating the task once and then providing additional task information explicitly, through interactions with a visualization of the action. We present a simple action representation that supports this framework and describe a system that implements the framework on a two-armed mobile manipulator. We demonstrate the power of this system by evaluating it on a diverse task benchmark that involves manipulation of everyday objects. We then demonstrate that the system is easy to learn and use for novice users through a user study in which participants program a subset of the benchmark. We characterize the limitations of our system in task generalization and end-user interactions and present extensions that could address some of the limitations.},
	language = {en},
	urldate = {2024-11-27},
	booktitle = {Robotics: {Science} and {Systems} {X}},
	publisher = {Robotics: Science and Systems Foundation},
	author = {Alexandrova, Sonya and Cakmak, Maya and Hsiao, Kaijen and Takayama, Leila},
	month = jul,
	year = {2014},
	keywords = {/unread, ccfInfo: CCF-None RSS, citationNumber: 96},
}

@article{fang_novel_2014,
	title = {Novel {AR}-based interface for human-robot interaction and visualization},
	volume = {2},
	issn = {2195-3597},
	url = {https://doi.org/10.1007/s40436-014-0087-9},
	doi = {10.1007/s40436-014-0087-9},
	abstract = {Intuitive and efficient interfaces for human-robot interaction (HRI) has been a challenging issue in robotics as it is essential for the prevalence of robots supporting humans in key areas of activities. This paper presents a novel augmented reality (AR) based interface to facilitate human-virtual robot interaction. A number of human-virtual robot interaction methods have been formulated and implemented with respect to the various types of operations needed in different robotic applications. A Euclidean distance-based method is developed to assist the users in the interaction with the virtual robot and the spatial entities in an AR environment. A monitor-based visualization mode is adopted as it enables the users to perceive the virtual contents associated with different interaction methods, and the virtual content augmented in the real environment is informative and useful to the users during their interaction with the virtual robot. Case researches are presented to demonstrate the successful implementation of the AR-based HRI interface in planning robot pick-and-place operations and path following operations.},
	language = {en},
	number = {4},
	urldate = {2024-11-27},
	journal = {Advances in Manufacturing},
	author = {Fang, H. C. and Ong, S. K. and Nee, A. Y. C.},
	month = dec,
	year = {2014},
	keywords = {/unread, ccfInfo: Not Found, citationNumber: 42, ‰∫∫Êú∫‰∫§‰∫íÔºàHRIÔºâ, ‰∫∫Êú∫ÁïåÈù¢, Â¢ûÂº∫Áé∞ÂÆû (AR)},
	pages = {275--288},
}

@article{singh_progprompt_2023,
	title = {{ProgPrompt}: program generation for situated robot task planning using large language models},
	volume = {47},
	issn = {1573-7527},
	shorttitle = {{ProgPrompt}},
	url = {https://doi.org/10.1007/s10514-023-10135-3},
	doi = {10.1007/s10514-023-10135-3},
	abstract = {Task planning can require defining myriad domain knowledge about the world in which a robot needs to act. To ameliorate that effort, large language models (LLMs) can be used to score potential next actions during task planning, and even generate action sequences directly, given an instruction in natural language with no additional domain information. However, such methods either require enumerating all possible next steps for scoring, or generate free-form text that may contain actions not possible on a given robot in its current context. We present a programmatic LLM prompt structure that enables plan generation functional across situated environments, robot capabilities, and tasks. Our key insight is to prompt the LLM with program-like specifications of the available actions and objects in an environment, as well as with example programs that can be executed. We make concrete recommendations about prompt structure and generation constraints through ablation experiments, demonstrate state of the art success rates in VirtualHome household tasks, and deploy our method on a physical robot arm for tabletop tasks. Website and code at progprompt.github.io},
	language = {en},
	number = {8},
	urldate = {2024-11-26},
	journal = {Autonomous Robots},
	author = {Singh, Ishika and Blukis, Valts and Mousavian, Arsalan and Goyal, Ankit and Xu, Danfei and Tremblay, Jonathan and Fox, Dieter and Thomason, Jesse and Garg, Animesh},
	month = dec,
	year = {2023},
	keywords = {/unread, Artificial Intelligence, LLM code generation, Planning domain generalization, Robot task planning, Symbolic planning, ccfInfo: CCF-None AROBOTS, citationNumber: Not Found},
	pages = {999--1012},
}

@article{wu_autogen_2024,
	title = {{AutoGen}: {Enabling} {Next}-{Gen} {LLM} {Applications} via {Multi}-{Agent} {Conversations}},
	abstract = {We present AutoGen,1 an open-source framework that allows developers to build LLM applications by composing multiple agents to converse with each other to accomplish tasks. AutoGen agents are customizable, conversable, and can operate in various modes that employ combinations of LLMs, human inputs, and tools. It also enables developers to create flexible agent behaviors and conversation patterns for different applications using both natural language and code. AutoGen serves as a generic infrastructure and is widely used by AI practitioners and researchers to build diverse applications of various complexities and LLM capacities. We demonstrate the framework‚Äôs effectiveness with several pilot applications, on domains ranging from mathematics and coding to question-answering, supply-chain optimization, online decision-making, and entertainment.},
	language = {en},
	author = {Wu, Qingyun and Bansal, Gagan and Zhang, Jieyu and Wu, Yiran and Li, Beibin and Zhu, Erkang and Jiang, Li and Zhang, Xiaoyun and Zhang, Shaokun and Liu, Jiale and Awadallah, Ahmed and White, Ryen W and Burger, Doug and Wang, Chi},
	year = {2024},
	keywords = {/unread, ccfInfo: Not Found, citationNumber: 0},
}

@article{hu_deploying_2024,
	title = {Deploying and {Evaluating} {LLMs} to {Program} {Service} {Mobile} {Robots}},
	volume = {9},
	issn = {2377-3766},
	url = {https://ieeexplore.ieee.org/abstract/document/10416558},
	doi = {10.1109/LRA.2024.3360020},
	abstract = {Recent advancements in large language models (LLMs) have spurred interest in using them for generating robot programs from natural language, with promising initial results. We investigate the use of LLMs to generate programs for service mobile robots leveraging mobility, perception, and human interaction skills, and where accurate sequencing and ordering of actions is crucial for success. We contribute CodeBotler, an open-source robot-agnostic tool to program service mobile robots from natural language, and RoboEval, a benchmark for evaluating LLMs' capabilities of generating programs to complete service robot tasks. CodeBotler performs program generation via few-shot prompting of LLMs with an embedded domain-specific language (eDSL) in Python, and leverages skill abstractions to deploy generated programs on any general-purpose mobile robot. RoboEval evaluates the correctness of generated programs by checking execution traces starting with multiple initial states, and checking whether the traces satisfy temporal logic properties that encode correctness for each task. RoboEval also includes multiple prompts per task to test for the robustness of program generation. We evaluate several popular state-of-the-art LLMs with the RoboEval benchmark, and perform a thorough analysis of the modes of failures, resulting in a taxonomy that highlights common pitfalls of LLMs at generating robot programs.},
	number = {3},
	urldate = {2024-11-26},
	journal = {IEEE Robotics and Automation Letters},
	author = {Hu, Zichao and Lucchetti, Francesca and Schlesinger, Claire and Saxena, Yash and Freeman, Anders and Modak, Sadanand and Guha, Arjun and Biswas, Joydeep},
	month = mar,
	year = {2024},
	note = {Conference Name: IEEE Robotics and Automation Letters},
	keywords = {/unread, Benchmark testing, Mobile robots, Natural languages, Python, Robots, Service robots, Software tools for benchmarking and reproducibility, Task analysis, ccfInfo: CCF-None RAL, citationNumber: 0, human-centered robotics, service robotics, social HRI, software tools for robot programming, ü§ñ},
	pages = {2853--2860},
}

@inproceedings{baxter_touchscreen-based_2012,
	address = {New York, NY, USA},
	series = {{HRI} '12},
	title = {A touchscreen-based 'sandtray' to facilitate, mediate and contextualise human-robot social interaction},
	isbn = {978-1-4503-1063-5},
	url = {https://dl.acm.org/doi/10.1145/2157689.2157707},
	doi = {10.1145/2157689.2157707},
	abstract = {In the development of companion robots capable of any-depth, long-term interaction, social scenarios enable exploration of the robot's capacity to engage a human interactant. These scenarios are typically constrained to structured task-based interactions, to enable the quantification of results for the comparison of differing experimental conditions. This paper introduces a hardware setup to facilitate and mediate human-robot social interaction, simplifying the robot control task while enabling an equalised degree of environmental manipulation for the human and robot, but without implicitly imposing an a priori interaction structure.},
	urldate = {2024-11-25},
	booktitle = {Proceedings of the seventh annual {ACM}/{IEEE} international conference on {Human}-{Robot} {Interaction}},
	publisher = {Association for Computing Machinery},
	author = {Baxter, Paul and Wood, Rachel and Belpaeme, Tony},
	month = mar,
	year = {2012},
	keywords = {/unread, ccfInfo: CCF-None HRI, citationNumber: 0},
	pages = {105--106},
}

@article{rietz_woz4u_2021,
	title = {{WoZ4U}: {An} {Open}-{Source} {Wizard}-of-{Oz} {Interface} for {Easy}, {Efficient} and {Robust} {HRI} {Experiments}},
	volume = {8},
	issn = {2296-9144},
	shorttitle = {{WoZ4U}},
	url = {https://www.frontiersin.org/journals/robotics-and-ai/articles/10.3389/frobt.2021.668057/full},
	doi = {10.3389/frobt.2021.668057},
	abstract = {{\textless}p{\textgreater}Wizard-of-Oz experiments play a vital role in Human-Robot Interaction (HRI), as they allow for quick and simple hypothesis testing. Still, a publicly available general tool to conduct such experiments is currently not available in the research community, and researchers often develop and implement their own tools, customized for each individual experiment. Besides being inefficient in terms of programming efforts, this also makes it harder for non-technical researchers to conduct Wizard-of-Oz experiments. In this paper, we present a general and easy-to-use tool for the Pepper robot, one of the most commonly used robots in this context. While we provide the concrete interface for Pepper robots only, the system architecture is independent of the type of robot and can be adapted for other robots. A configuration file, which saves experiment-specific parameters, enables a quick setup for reproducible and repeatable Wizard-of-Oz experiments. A central server provides a graphical interface {\textless}italic{\textgreater}via{\textless}/italic{\textgreater} a browser while handling the mapping of user input to actions on the robot. In our interface, keyboard shortcuts may be assigned to phrases, gestures, and composite behaviors to simplify and speed up control of the robot. The interface is lightweight and independent of the operating system. Our initial tests confirm that the system is functional, flexible, and easy to use. The interface, including source code, is made commonly available, and we hope that it will be useful for researchers with any background who want to conduct HRI experiments.{\textless}/p{\textgreater}},
	language = {English},
	urldate = {2024-11-27},
	journal = {Frontiers in Robotics and AI},
	author = {Rietz, Finn and Sutherland, Alexander and Bensch, Suna and Wermter, Stefan and Hellstr√∂m, Thomas},
	month = jul,
	year = {2021},
	note = {Publisher: Frontiers},
	keywords = {/unread, HRI experiments, Tele-Operation, ccfInfo: CCF-None FIRAI, citationNumber: 9, human-robot interaction, interface, open source, pepper, wizard-of-oz},
}

@article{meisser_developing_nodate,
	title = {On developing a voice-enabled interface for interactive tour-guide robots},
	url = {https://www.academia.edu/17579113/On_developing_a_voice_enabled_interface_for_interactive_tour_guide_robots},
	abstract = {On developing a voice-enabled interface for interactive tour-guide robots},
	language = {en},
	urldate = {2024-11-27},
	author = {Meisser, Mathieu},
	keywords = {/unread, ccfInfo: CCF-None AR, citationNumber: Not Found},
}

@article{medicherla_humanrobot_2007,
	title = {Human‚Äìrobot interaction via voice-controllable intelligent user interface},
	volume = {25},
	copyright = {https://www.cambridge.org/core/terms},
	issn = {0263-5747, 1469-8668},
	url = {https://www.cambridge.org/core/product/identifier/S0263574707003414/type/journal_article},
	doi = {10.1017/S0263574707003414},
	abstract = {An understanding of how humans and robots can successfully interact to accomplish speciÔ¨Åc tasks is crucial in creating more sophisticated robots that may eventually become an integral part of human societies. A social robot needs to be able to learn the preferences and capabilities of the people with whom it interacts so that it can adapt its behaviors for more efÔ¨Åcient and friendly interaction. Advances in human‚Äìcomputer interaction technologies have been widely used in improving human‚Äìrobot interaction (HRI). It is now possible to interact with robots via natural communication means such as speech. In this paper, an innovative approach for HRI via voice-controllable intelligent user interfaces is described. The design and implementation of such interfaces are described. The traditional approaches for human‚Äìrobot user interface design are explained and the advantages of the proposed approach are presented. The designed intelligent user interface, which learns user preferences and capabilities in time, can be controlled with voice. The system was successfully implemented and tested on a Pioneer 3-AT mobile robot. 20 participants, who were assessed on spatial reasoning ability, directed the robot in spatial navigation tasks to evaluate the effectiveness of the voice control in HRI. Time to complete the task, number of steps, and errors were collected. Results indicated that spatial reasoning ability and voice-control were reliable predictors of efÔ¨Åciency of robot teleoperation. 75\% of the subjects with high spatial reasoning ability preferred using voice-control over manual control. The effect of spatial reasoning ability in teleoperation with voice-control was lower compared to that of manual control.},
	language = {en},
	number = {5},
	urldate = {2024-11-27},
	journal = {Robotica},
	author = {Medicherla, Harsha and Sekmen, Ali},
	month = sep,
	year = {2007},
	keywords = {/unread, ccfInfo: CCF-None ROBOTICA, citationNumber: 13},
	pages = {521--527},
}

@article{andronas_multi-modal_2021,
	title = {Multi-modal interfaces for natural {Human}-{Robot} {Interaction}},
	volume = {54},
	issn = {23519789},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S2351978921001669},
	doi = {10.1016/j.promfg.2021.07.030},
	abstract = {Recent years, manufacturing aims on increasing flexibility while maintaining productivity for satisfying emerging market needs for higher product customization. Human Robot Collaboration (HRC) is able to bring about this balance by combining the benefits of manual assembly and robotic automation. When introducing a hybrid concept, safety and human acceptance are of vital importance for achieving implementation. Fenceless coexistence may lead to discomfort of operators especially in cases where close Human Robot Interaction (HRI) occurs. This work aims at designing and implementing a natural Human-System and System-Human interaction framework that enables seamless interaction between operators and their ‚Äúrobot colleagues‚Äù. This natural interaction will strengthen hybrid implementation through increased: a) operator‚Äôs and system‚Äôs awareness, b) operator‚Äôs trust to the system, and through the decrease of: a) human errors and b) safety incidents. The overall architecture of the proposed system makes it scalable, flexible, and applicable in different collaborative scenarios by enabling the connectivity of multiple interfaces with customizable environments according to operator‚Äôs needs. The performance of the system is evaluated on a scenario originating from the automotive industry proving that an intuitive interaction framework can increase acceptance and performance of both robots and operators.},
	language = {en},
	urldate = {2024-11-27},
	journal = {Procedia Manufacturing},
	author = {Andronas, Dionisis and Apostolopoulos, George and Fourtakas, Nikos and Makris, Sotiris},
	year = {2021},
	keywords = {/unread, ccfInfo: Not Found, citationNumber: 18},
	pages = {197--202},
}

@article{ajaykumar_designing_2021,
	title = {Designing user-centric programming aids for kinesthetic teaching of collaborative robots},
	volume = {145},
	issn = {0921-8890},
	url = {https://www.sciencedirect.com/science/article/pii/S0921889021001305},
	doi = {10.1016/j.robot.2021.103845},
	abstract = {Just as end-user programming has helped make computer programming accessible for a variety of users and settings, end-user robot programming has helped empower end-users without specialized knowledge or technical skills to customize robotic assistance that meets diverse environmental constraints and task requirements. While end-user robot programming methods such as kinesthetic teaching have introduced direct approaches to task demonstration that allow users to avoid working with traditional programming constructs, our formative study revealed that everyday people still have difficulties in specifying effective robot programs using these methods due to challenges in understanding robot kinematics and programming without situated context and assistive system feedback. These findings informed our development of Demoshop, an interactive robot programming tool that includes user-centric programming aids to help end-users author and edit task demonstrations. To evaluate the effectiveness of Demoshop, we conducted a user study comparing task performance and user experience associated with using Demoshop relative to a widely used commercial baseline interface. Results of our study indicate that users have greater task efficiency while authoring robot programs and maintain stronger mental models of the system when using Demoshop compared to the baseline interface. Our system implementation and study have implications for the further development of assistance in end-user robot programming.},
	urldate = {2024-11-27},
	journal = {Robotics and Autonomous Systems},
	author = {Ajaykumar, Gopika and Stiber, Maia and Huang, Chien-Ming},
	month = nov,
	year = {2021},
	keywords = {/unread, End‚Äìuser development, End‚Äìuser robot programming, Human‚Äìrobot interaction, ccfInfo: CCF-None RAS, citationNumber: 13},
	pages = {103845},
}

@inproceedings{gallardo-estrella_humanrobot_2011,
	address = {Berlin, Heidelberg},
	title = {Human/{Robot} {Interface} for {Voice} {Teleoperation} of a {Robotic} {Platform}},
	isbn = {978-3-642-21501-8},
	doi = {10.1007/978-3-642-21501-8_30},
	abstract = {Speech is the most natural way of human communication. If the interaction between humans an machines is accomplished through voice, humans will feel more comfortable. Thus, this paper presents a Human/Robot Interface to teleoperate a robot by means of voice commands. To that purpose, an acoustic model in Spanish have been developed to recognize voice commands with Julius. The model is user dependent and has been suited to the proposed set of commands to achieve a better recognition rate. One of the advantages of the proposed speech recognition mechanism is that it can be easily adapted to a new list of commands. A robot has been successfully teleoperated with voice. Results about the recognition rate are promising in using the proposed Human/Robot Interface for voice teleoperation.},
	language = {en},
	booktitle = {Advances in {Computational} {Intelligence}},
	publisher = {Springer},
	author = {Gallardo-Estrella, L. and Poncela, A.},
	editor = {Cabestany, Joan and Rojas, Ignacio and Joya, Gonzalo},
	year = {2011},
	keywords = {/unread, ccfInfo: CCF-None IWANN, citationNumber: 2, Â£∞Â≠¶Ê®°Âûã, Êú∫Âô®‰∫∫Âπ≥Âè∞, ËØÜÂà´Áéá, ËØ≠Èü≥ÂëΩ‰ª§, ËØ≠Èü≥ËØÜÂà´},
	pages = {240--247},
}

@inproceedings{quintero_vibi_2015,
	title = {{VIBI}: {Assistive} vision-based interface for robot manipulation},
	shorttitle = {{VIBI}},
	url = {https://ieeexplore.ieee.org/abstract/document/7139816},
	doi = {10.1109/ICRA.2015.7139816},
	abstract = {Upper-body disabled people can benefit from the use of robot-arms to perform every day tasks. However, the adoption of this kind of technology has been limited by the complexity of robot manipulation tasks and the difficulty in controlling a multiple-DOF arm using a joystick or a similar device. Motivated by this need, we present an assistive vision-based interface for robot manipulation. Our proposal is to replace the direct joystick motor control interface present in a commercial wheelchair mounted assistive robotic manipulator with a human-robot interface based on visual selection. The scene in front of the robot is shown on a screen, and the user can then select an object with our novel grasping interface. We develop computer vision and motion control methods that drive the robot to that object. Our aim is not to replace user control, but instead augment user capabilities through our system with different levels of semi-autonomy, while leaving the user with a sense that he/she is in control of the task. Two disabled pilot users, were involved at different stages of our research. The first pilot user during the interface design along with rehab experts. The second performed user studies along with an 8 subject control group to evaluate our interface. Our system reduces robot instruction from a 6-DOF task in continuous space to either a 2-DOF pointing task or a discrete selection task among objects detected by computer vision.},
	urldate = {2024-11-27},
	booktitle = {2015 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	author = {Quintero, Camilo Perez and Ramirez, Oscar and J√§gersand, Martin},
	month = may,
	year = {2015},
	note = {ISSN: 1050-4729},
	keywords = {/unread, Cameras, Grasping, Manipulators, Mobile robots, Three-dimensional displays, Visualization, ccfInfo: CCF-B ICRA, citationNumber: 36},
	pages = {4458--4463},
}
