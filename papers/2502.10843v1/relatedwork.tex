\section{Related Work}
\label{sec:related_work}
\textbf{CTMCs.} CTMCs \cite{campbell2022continuous} have been used for various applications in generative modeling ("discrete diffusion" models), including text and image generation \cite{shi2024simplified,gatdiscrete,shaul2024flow, sahoo2024simple} and molecular design \cite{gruver2023protein, campbell2024generative, lisanza2024}. While here we use a RND for CTMCs running in \emph{reverse} time, one recovers the loss functions of these generative models considering a RND of two \emph{forward} time CTMCs (in \cref{sec:proof_rnds}, we show this in detail).

\textbf{Transport and sampling.}
Over the past decade there has been continued interest in combining the statistical guarantees of MCMC and IS with learning transport maps. A non-parametric version of this is described in \cite{marzouk2016}, and a parametric version through coupling-based normalizing flows was used to study systems in molecular dynamics and statistical physics \cite{noe2019, albergo2019, gabrie2022, wang2022}. These methods were extended to weave normalizing flows with SMC moves \cite{arbel2021, matthews2022}. More recent research focuses on replacing the generative model with a continuous flow or diffusion \cite{zhang2022path, vargas2023denoising, akhound2024iterated, sun2024}. Our method is inspired by approaches combining measure transport with MCMC schemes \cite{albergo2024nets, vargas2024transport} and other samplers relying on PINN-based objectives in continuous spaces \citep{mate2023learning, tian2024, sun2024}.

\textbf{Discrete Neural samplers.} The primary alternative to what we propose is to correct using importance weights arising from the estimate of the probability density computed using an autoregressive model \cite{nicoli2020}. However, the computational cost of producing samples in this case scales naively as $O(d)$, whereas we have no such constraint \textit{a priori} in our case so long as the error in the Euler sampling scheme is kept small. Other work focuses on discrete formulations of normalizing flows, but the performant version reduces to an autoregressive model \cite{tran2019}. Recent work has considered using CTMCs for sampling by parameterizing their evolution operators directly via tensor networks \citep{causer2025discrete} as opposed to neural network representations of rate matrices here. 

% \begin{itemize}
%     \item there's that one discrete flow paper but it won't work well i imagine
    % \item then there's some autorgressive ones like from \cite{nicoli2020}. We can argue that we don't necessarily require $O(L^2)$ sampling steps, as they would. 
% \end{itemize}
% In the context of sampling, it has already been useful to consider RNDs with respect to processes running in reverse time, e.g. as used in Annealed Importance Sampling (AIS) or in Euclidean space via path measures \citep{vargas2024transport}.

% \begin{figure*}[ht]
%     \includegraphics[width=\linewidth]{figures/ising-stats.pdf}
%     \vspace{-0.8cm}
%     \caption{Performance metrics of LEAPS on the $L=15, J = 0.4, \beta = 0.7$ Ising model with the LEA and LEC architectures, using 100 annealing steps for sample generation. \textbf{Left:} Effective sample size of LEAPS samplers over training. Increasing the depth of LEC significantly improves performance. \textbf{Center:} Difference in the histograms of the magnetization $M(x)$ of configurations as compared to the ground truth set attained from a Glauber dynamics run of 25,000 steps, labeled as $M^*$. We denote by "no transport" the case of using  annealed dynamics with just the marginal preserving MCMC updates to show that the transport from $Q_t$ is essential in our construction. \textbf{Right:} Comparison of the 2-point correlation function for the LEA and LEC samplers against the Glauber dynamics ground truth.}
%     \label{fig:ising}
% \end{figure*}