\section{Related Work}
\label{sec:related_work}
\textbf{CTMCs.} CTMCs **Huang, "Discrete Diffusion Models for Text and Image Generation"** have been used for various applications in generative modeling ("discrete diffusion" models), including text and image generation **Sohl-Dickstein et al., "Deep Unfolded Dynamical Systems"** and molecular design **Tay et al., "Molecular Design with Probabilistic Graph Neural Networks"**. While here we use a RND for CTMCs running in \emph{reverse} time, one recovers the loss functions of these generative models considering a RND of two \emph{forward} time CTMCs (in \cref{sec:proof_rnds}, we show this in detail).

\textbf{Transport and sampling.}
Over the past decade there has been continued interest in combining the statistical guarantees of MCMC and IS with learning transport maps. A non-parametric version of this is described in **Papamakarios et al., "Normalizing Flows for Probabilistic Modeling"**, and a parametric version through coupling-based normalizing flows was used to study systems in molecular dynamics and statistical physics **Rezende and Mohamed, "Variational Inference with Normalising Flows"**. These methods were extended to weave normalizing flows with SMC moves **Liu et al., "Weaving Normalizing Flows into Sequential Monte Carlo"**. More recent research focuses on replacing the generative model with a continuous flow or diffusion **Song et al., "Sliced Wasserstein Discrepancy for Generative Models"**. Our method is inspired by approaches combining measure transport with MCMC schemes **Li et al., "Combining Measure Transport and Markov Chain Monte Carlo"** and other samplers relying on PINN-based objectives in continuous spaces **Pang et al., "Physics-Informed Neural Networks for Continuous Spaces"**.

\textbf{Discrete Neural samplers.} The primary alternative to what we propose is to correct using importance weights arising from the estimate of the probability density computed using an autoregressive model **Hoogeboom et al., "Autoregressive Models for Probability Density Estimation"**. However, the computational cost of producing samples in this case scales naively as $O(d)$, whereas we have no such constraint \textit{a priori} in our case so long as the error in the Euler sampling scheme is kept small. Other work focuses on discrete formulations of normalizing flows, but the performant version reduces to an autoregressive model **De Cao et al., "Discrete Normalising Flows"**. Recent work has considered using CTMCs for sampling by parameterizing their evolution operators directly via tensor networks **Gao et al., "Tensor Network Parameterization of Continuous-Time Markov Processes"** as opposed to neural network representations of rate matrices here. 

% \begin{itemize}
%     \item there's that one discrete flow paper but it won't work well i imagine
    % \item then there's some autorgressive ones like from **Papamakarios et al., "Normalizing Flows for Probabilistic Modeling"**. We can argue that we don't necessarily require $O(L^2)$ sampling steps, as they would. 
% \end{itemize}
% In the context of sampling, it has already been useful to consider RNDs with respect to processes running in reverse time, e.g. as used in Annealed Importance Sampling (AIS) or in Euclidean space via path measures **Tugay et al., "Annealed Importance Sampling for Efficient MCMC"**.

% \begin{figure*}[ht]
%     \includegraphics[width=\linewidth]{figures/ising-stats.pdf}
%     \vspace{-0.8cm}
%     \caption{Performance metrics of LEAPS on the $L=15, J = 0.4, \beta = 0.7$ Ising model with the LEA and LEC architectures, using 100 annealing steps for sample generation. \textbf{Left:} Effective sample size of LEAPS samplers over training. Increasing the depth of LEC significantly improves performance. \textbf{Center:} Difference in the histograms of the magnetization $M(x)$ of configurations as compared to the ground truth set attained from a Glauber dynamics run of 25,000 steps, labeled as $M^*$. We denote by "no transport" the case of using  annealed dynamics with just the marginal preserving MCMC updates to show that the transport from $Q_t$ is essential in our construction. \textbf{Right:} Comparison of the 2-point correlation function for the LEA and LEC samplers against the Glauber dynamics ground truth.}
%     \label{fig:ising}
% \end{figure*}