@article{campbell2022continuous,
  title={A continuous time framework for discrete denoising models},
  author={Campbell, Andrew and Benton, Joe and De Bortoli, Valentin and Rainforth, Thomas and Deligiannidis, George and Doucet, Arnaud},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={28266--28279},
  year={2022}
}
@article{causer2024discrete,
  title={Discrete generative diffusion models without stochastic differential equations: a tensor network approach},
  author={Causer, Luke and Rotskoff, Grant M and Garrahan, Juan P},
  journal={arXiv preprint arXiv:2407.11133},
  year={2024}
}
@article{zhang2021path,
  title={Path integral sampler: a stochastic control approach for sampling},
  author={Zhang, Qinsheng and Chen, Yongxin},
  journal={arXiv preprint arXiv:2111.15141},
  year={2021}
}
@article{midgley2022flow,
  title={Flow annealed importance sampling bootstrap},
  author={Midgley, Laurence Illing and Stimper, Vincent and Simm, Gregor NC and Sch{\"o}lkopf, Bernhard and Hern{\'a}ndez-Lobato, Jos{\'e} Miguel},
  journal={arXiv preprint arXiv:2208.01893},
  year={2022}
}
@book{robert1999monte,
  title={Monte Carlo statistical methods},
  author={Robert, Christian P and Casella, George and Casella, George},
  volume={2},
  year={1999},
  publisher={Springer}
}
@article{richter2023improved,
  title={Improved sampling via learned diffusions},
  author={Richter, Lorenz and Berner, Julius},
  journal={arXiv preprint arXiv:2307.01198},
  year={2023}
}
@article{shaul2024flow,
  title={Flow Matching with General Discrete Paths: A Kinetic-Optimal Perspective},
  author={Shaul, Neta and Gat, Itai and Havasi, Marton and Severo, Daniel and Sriram, Anuroop and Holderrieth, Peter and Karrer, Brian and Lipman, Yaron and Chen, Ricky TQ},
  journal={arXiv preprint arXiv:2412.03487},
  year={2024}
}
@article{campbell2024generative,
  title={Generative flows on discrete state-spaces: Enabling multimodal flows with applications to protein co-design},
  author={Campbell, Andrew and Yim, Jason and Barzilay, Regina and Rainforth, Tom and Jaakkola, Tommi},
  journal={arXiv preprint arXiv:2402.04997},
  year={2024}
}
@article{akhound2024iterated,
  title={Iterated denoising energy matching for sampling from Boltzmann densities},
  author={Akhound-Sadegh, Tara and Rector-Brooks, Jarrid and Bose, Avishek Joey and Mittal, Sarthak and Lemos, Pablo and Liu, Cheng-Hao and Sendera, Marcin and Ravanbakhsh, Siamak and Gidel, Gauthier and Bengio, Yoshua and others},
  journal={arXiv preprint arXiv:2402.06121},
  year={2024}
}
@inproceedings{vargas2024transport,
  title={Transport meets variational inference: Controlled Monte Carlo diffusions},
  author={Vargas, Francisco and Padhy, Shreyas and Blessing, Denis and Nusken, Nikolas},
  booktitle={The Twelfth International Conference on Learning Representations: ICLR 2024},
  year={2024}
}
@article{sanokowski2024diffusion,
  title={A Diffusion Model Framework for Unsupervised Neural Combinatorial Optimization},
  author={Sanokowski, Sebastian and Hochreiter, Sepp and Lehner, Sebastian},
  journal={arXiv preprint arXiv:2406.01661},
  year={2024}
}
@article{neal2001annealed,
  title={Annealed importance sampling},
  author={Neal, Radford M},
  journal={Statistics and computing},
  volume={11},
  pages={125--139},
  year={2001},
  publisher={Springer}
}
@article{holderrieth2024generator,
  title={Generator Matching: Generative modeling with arbitrary Markov processes},
  author={Holderrieth, Peter and Havasi, Marton and Yim, Jason and Shaul, Neta and Gat, Itai and Jaakkola, Tommi and Karrer, Brian and Chen, Ricky TQ and Lipman, Yaron},
  journal={arXiv preprint arXiv:2410.20587},
  year={2024}
}
@article{albergo2024nets,
  title={NETS: A Non-Equilibrium Transport Sampler},
  author={Albergo, Michael S and Vanden-Eijnden, Eric},
  journal={arXiv preprint arXiv:2410.02711},
  year={2024}
}

@article{vaikuntanathan2008,
  title = {Escorted Free Energy Simulations: Improving Convergence by Reducing Dissipation},
  author = {Vaikuntanathan, Suriyanarayanan and Jarzynski, Christopher},
  journal = {Phys. Rev. Lett.},
  volume = {100},
  issue = {19},
  pages = {190601},
  numpages = {4},
  year = {2008},
  month = {May},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevLett.100.190601},
  url = {https://link.aps.org/doi/10.1103/PhysRevLett.100.190601}
}


@article{gelfand1990,
 ISSN = {01621459, 1537274X},
 URL = {http://www.jstor.org/stable/2289776},
 abstract = {Stochastic substitution, the Gibbs sampler, and the sampling-importance-resampling algorithm can be viewed as three alternative sampling- (or Monte Carlo-) based approaches to the calculation of numerical estimates of marginal probability distributions. The three approaches will be reviewed, compared, and contrasted in relation to various joint probability structures frequently encountered in applications. In particular, the relevance of the approaches to calculating Bayesian posterior densities for a variety of structured models will be discussed and illustrated.},
 author = {Alan E. Gelfand and Adrian F. M. Smith},
 journal = {Journal of the American Statistical Association},
 number = {410},
 pages = {398--409},
 publisher = {[American Statistical Association, Taylor & Francis, Ltd.]},
 title = {Sampling-Based Approaches to Calculating Marginal Densities},
 urldate = {2025-01-30},
 volume = {85},
 year = {1990}
}


@article{berendsen1984,
    author = {Berendsen, H. J. C. and Postma, J. P. M. and van Gunsteren, W. F. and DiNola, A. and Haak, J. R.},
    title = {Molecular dynamics with coupling to an external bath},
    journal = {The Journal of Chemical Physics},
    volume = {81},
    number = {8},
    pages = {3684-3690},
    year = {1984},
    month = {10},
    abstract = {In molecular dynamics (MD) simulations the need often arises to maintain such parameters as temperature or pressure rather than energy and volume, or to impose gradients for studying transport properties in nonequilibrium MD. A method is described to realize coupling to an external bath with constant temperature or pressure with adjustable time constants for the coupling. The method is easily extendable to other variables and to gradients, and can be applied also to polyatomic molecules involving internal constraints. The influence of coupling time constants on dynamical variables is evaluated. A leap‐frog algorithm is presented for the general case involving constraints with coupling to both a constant temperature and a constant pressure bath.},
    issn = {0021-9606},
    doi = {10.1063/1.448118},
    url = {https://doi.org/10.1063/1.448118},
    eprint = {https://pubs.aip.org/aip/jcp/article-pdf/81/8/3684/18950084/3684\_1\_online.pdf},
}

@book{allen1987,
  title={Computer simulation of liquids},
  author={Allen, Michael P and Tildesley, Dominic J},
  year={2017},
  publisher={Oxford university press}
}
@article{thomas2018tensor,
  title={Tensor field networks: Rotation-and translation-equivariant neural networks for 3d point clouds},
  author={Thomas, Nathaniel and Smidt, Tess and Kearnes, Steven and Yang, Lusann and Li, Li and Kohlhoff, Kai and Riley, Patrick},
  journal={arXiv preprint arXiv:1802.08219},
  year={2018}
}
@article{causer2025discrete,
  title={Discrete generative diffusion models without stochastic differential equations: a tensor network approach},
  author={Causer, Luke and Rotskoff, Grant M and Garrahan, Juan P},
  journal={Physical Review E},
  volume={111},
  number={2},
  pages={025302},
  year={2025},
  publisher={APS}
}
@book{halmos2013measure,
  title={Measure theory},
  author={Halmos, Paul R},
  volume={18},
  year={2013},
  publisher={Springer}
}
@article{weiler2019general,
  title={General e (2)-equivariant steerable cnns},
  author={Weiler, Maurice and Cesa, Gabriele},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}
@article{bronstein2021geometric,
  title={Geometric deep learning: Grids, groups, graphs, geodesics, and gauges},
  author={Bronstein, Michael M and Bruna, Joan and Cohen, Taco and Veli{\v{c}}kovi{\'c}, Petar},
  journal={arXiv preprint arXiv:2104.13478},
  year={2021}
}
@misc{faulkner2023sampling,
      title={Sampling algorithms in statistical physics: a guide for statistics and machine learning}, 
      author={Michael F. Faulkner and Samuel Livingstone},
      year={2023},
      eprint={2208.04751},
      archivePrefix={arXiv},
      primaryClass={stat.CO},
      url={https://arxiv.org/abs/2208.04751}, 
}
@inproceedings{loudiscrete,
  title={Discrete Diffusion Modeling by Estimating the Ratios of the Data Distribution},
  author={Lou, Aaron and Meng, Chenlin and Ermon, Stefano},
  booktitle={Forty-first International Conference on Machine Learning}
}

@article{gatdiscrete,
  title={Discrete flow matching},
  author={Gat, Itai and Remez, Tal and Shaul, Neta and Kreuk, Felix and Chen, Ricky TQ and Synnaeve, Gabriel and Adi, Yossi and Lipman, Yaron},
  journal={Advances in Neural Information Processing Systems},
  volume={37},
  pages={133345--133385},
  year={2025}
}
@article{wilson1974,
  title = {Confinement of quarks},
  author = {Wilson, Kenneth G.},
  journal = {Phys. Rev. D},
  volume = {10},
  issue = {8},
  pages = {2445--2459},
  numpages = {0},
  year = {1974},
  month = {Oct},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevD.10.2445},
  url = {https://link.aps.org/doi/10.1103/PhysRevD.10.2445}
}

@article{duane1987,
title = {Hybrid Monte Carlo},
journal = {Physics Letters B},
volume = {195},
number = {2},
pages = {216-222},
year = {1987},
issn = {0370-2693},
doi = {https://doi.org/10.1016/0370-2693(87)91197-X},
url = {https://www.sciencedirect.com/science/article/pii/037026938791197X},
author = {Simon Duane and A.D. Kennedy and Brian J. Pendleton and Duncan Roweth},
abstract = {We present a new method for the numerical simulation of lattice field theory. A hybrid (molecular dynamics/Langevin) algorithm is used to guide a Monte Carlo simulation. There are no discretization errors even for large step sizes. The method is especially efficient for systems such as quantum chromodynamics which contain fermionic degrees of freedom. Detailed results are presented for four-dimensional compact quantum electrodynamics including the dynamical effects of electrons.}
}

@article{metropolis1953,
    author = {Metropolis, Nicholas and Rosenbluth, Arianna W. and Rosenbluth, Marshall N. and Teller, Augusta H. and Teller, Edward},
    title = {Equation of State Calculations by Fast Computing Machines},
    journal = {The Journal of Chemical Physics},
    volume = {21},
    number = {6},
    pages = {1087-1092},
    year = {1953},
    month = {06},
    abstract = {A general method, suitable for fast computing machines, for investigating such properties as equations of state for substances consisting of interacting individual molecules is described. The method consists of a modified Monte Carlo integration over configuration space. Results for the two‐dimensional rigid‐sphere system have been obtained on the Los Alamos MANIAC and are presented here. These results are compared to the free volume equation of state and to a four‐term virial coefficient expansion.},
    issn = {0021-9606},
    doi = {10.1063/1.1699114},
    url = {https://doi.org/10.1063/1.1699114},
    eprint = {https://pubs.aip.org/aip/jcp/article-pdf/21/6/1087/18802390/1087\_1\_online.pdf},
}


@article{kahn1951,
  added-at = {2020-01-29T18:29:49.000+0100},
  author = {Kahn, Herman and Harris, Theodore E},
  journal = {National Bureau of Standards applied mathematics series},
  keywords = {enhanced-sampling history path-sampling splitting-methods},
  pages = {27--30},
  title = {Estimation of particle transmission by random sampling},
  volume = 12,
  year = 1951
}


@inproceedings{
midgley2023flow,
title={Flow Annealed Importance Sampling Bootstrap},
author={Laurence Illing Midgley and Vincent Stimper and Gregor N. C. Simm and Bernhard Sch{\"o}lkopf and Jos{\'e} Miguel Hern{\'a}ndez-Lobato},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=XCTVFJwS9LJ}
}

@inproceedings{
zhang2022path,
title={Path Integral Sampler: A Stochastic Control Approach For Sampling},
author={Qinsheng Zhang and Yongxin Chen},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=_uCb2ynRu7Y}
}




@inproceedings{
vargas2023denoising,
title={Denoising Diffusion Samplers},
author={Francisco Vargas and Will Sussman Grathwohl and Arnaud Doucet},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=8pvnfTAbu1f}
}

@InProceedings{arbel2021,
  title={Annealed Flow Transport Monte Carlo},
  author={Michael Arbel and Alexander G. D. G. Matthews and Arnaud Doucet},
  booktitle = {Proceedings of the 38th International Conference on Machine Learning},
  series = {Proceedings of Machine Learning Research},
  year={2021},
  month = {18--24 Jul}
}



@article{caselle2022,
	abstract = {Normalizing flows are a class of deep generative models that provide a promising route to sample lattice field theories more efficiently than conventional Monte Carlo simulations. In this work we show that the theoretical framework of stochastic normalizing flows, in which neural-network layers are combined with Monte Carlo updates, is the same that underlies out-of-equilibrium simulations based on Jarzynski's equality, which have been recently deployed to compute free-energy differences in lattice gauge theories. We lay out a strategy to optimize the efficiency of this extended class of generative models and present examples of applications.},
	author = {Caselle, Michele and Cellini, Elia and Nada, Alessandro and Panero, Marco},
	date = {2022/07/04},
	date-added = {2024-10-01 17:11:19 -0400},
	date-modified = {2024-10-01 17:11:19 -0400},
	doi = {10.1007/JHEP07(2022)015},
	id = {Caselle2022},
	isbn = {1029-8479},
	journal = {Journal of High Energy Physics},
	number = {7},
	pages = {15},
	title = {Stochastic normalizing flows as non-equilibrium transformations},
	url = {https://doi.org/10.1007/JHEP07(2022)015},
	volume = {2022},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1007/JHEP07(2022)015}}


@misc{caselle2022lattice,
      title={Stochastic normalizing flows for lattice field theory}, 
      author={Michele Caselle and Elia Cellini and Alessandro Nada and Marco Panero},
      year={2022},
      eprint={2210.03139},
      archivePrefix={arXiv},
      primaryClass={hep-lat},
      url={https://arxiv.org/abs/2210.03139}, 
}

@book{doucet2001,
  editor       = {Arnaud Doucet and
                  Nando de Freitas and
                  Neil J. Gordon},
  title        = {Sequential Monte Carlo Methods in Practice},
  series       = {Statistics for Engineering and Information Science},
  publisher    = {Springer},
  year         = {2001},
  url          = {https://doi.org/10.1007/978-1-4757-3437-9},
  doi          = {10.1007/978-1-4757-3437-9},
  isbn         = {978-1-4419-2887-0},
  timestamp    = {Sat, 30 Sep 2023 09:32:43 +0200},
  biburl       = {https://dblp.org/rec/books/sp/DoucetFG01.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@article{noe2019,
author = {Frank Noé  and Simon Olsson  and Jonas Köhler  and Hao Wu },
title = {Boltzmann generators: Sampling equilibrium states of many-body systems with deep learning},
journal = {Science},
volume = {365},
number = {6457},
pages = {eaaw1147},
year = {2019},
doi = {10.1126/science.aaw1147},
URL = {https://www.science.org/doi/abs/10.1126/science.aaw1147},
eprint = {https://www.science.org/doi/pdf/10.1126/science.aaw1147},
abstract = {Molecular dynamics or Monte Carlo methods can be used to sample equilibrium states, but these methods become computationally expensive for complex systems, where the transition from one equilibrium state to another may only occur through rare events. Noé et al. used neural networks and deep learning to generate distributions of independent soft condensed-matter samples at equilibrium (see the Perspective by Tuckerman). Supervised training is used to construct invertible transformations between the coordinates of the complex system of interest and simple Gaussian coordinates of the same dimensionality. Thus, configurations can be sampled in this simpler coordinate system and then transformed back into the complex one using the correct statistical weighting. Science, this issue p. eaaw1147; see also p. 982 By combining deep learning and statistical mechanics, neural networks sample the equilibrium distribution of many-body systems. Computing equilibrium states in condensed-matter many-body systems, such as solvated proteins, is a long-standing challenge. Lacking methods for generating statistically independent equilibrium samples in “one shot,” vast computational effort is invested for simulating these systems in small steps, e.g., using molecular dynamics. Combining deep learning and statistical mechanics, we developed Boltzmann generators, which are shown to generate unbiased one-shot equilibrium samples of representative condensed-matter systems and proteins. Boltzmann generators use neural networks to learn a coordinate transformation of the complex configurational equilibrium distribution to a distribution that can be easily sampled. Accurate computation of free-energy differences and discovery of new configurations are demonstrated, providing a statistical mechanics tool that can avoid rare events during sampling without prior knowledge of reaction coordinates.}}




@inproceedings{albergo2022building,
  title={Building Normalizing Flows with Stochastic Interpolants},
  author={Albergo, Michael S and Vanden-Eijnden, Eric},
  booktitle={The Eleventh International Conference on Learning Representations},
  year={2022}
}


@article{albergo2019,
  title = {Flow-based generative models for Markov chain Monte Carlo in lattice field theory},
  author = {Albergo, M. S. and Kanwar, G. and Shanahan, P. E.},
  journal = {Phys. Rev. D},
  volume = {100},
  issue = {3},
  pages = {034515},
  numpages = {13},
  year = {2019},
  month = {Aug},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevD.100.034515},
  url = {https://link.aps.org/doi/10.1103/PhysRevD.100.034515}
}


@article{
gabrie2022,
author = {Marylou Gabrié  and Grant M. Rotskoff  and Eric Vanden-Eijnden },
title = {Adaptive Monte Carlo augmented with normalizing flows},
journal = {Proceedings of the National Academy of Sciences},
volume = {119},
number = {10},
pages = {e2109420119},
year = {2022},
doi = {10.1073/pnas.2109420119},
URL = {https://www.pnas.org/doi/abs/10.1073/pnas.2109420119},
eprint = {https://www.pnas.org/doi/pdf/10.1073/pnas.2109420119},
abstract = {Monte Carlo methods, tools for sampling data from probability distributions, are widely used in the physical sciences, applied mathematics, and Bayesian statistics. Nevertheless, there are many situations in which it is computationally prohibitive to use Monte Carlo due to slow “mixing” between modes of a distribution unless hand-tuned algorithms are used to accelerate the scheme. Machine learning techniques based on generative models offer a compelling alternative to the challenge of designing efficient schemes for a specific system. Here, we formalize Monte Carlo augmented with normalizing flows and show that, with limited prior data and a physically inspired algorithm, we can substantially accelerate sampling with generative models. Many problems in the physical sciences, machine learning, and statistical inference necessitate sampling from a high-dimensional, multimodal probability distribution. Markov Chain Monte Carlo (MCMC) algorithms, the ubiquitous tool for this task, typically rely on random local updates to propagate configurations of a given system in a way that ensures that generated configurations will be distributed according to a target probability distribution asymptotically. In high-dimensional settings with multiple relevant metastable basins, local approaches require either immense computational effort or intricately designed importance sampling strategies to capture information about, for example, the relative populations of such basins. Here, we analyze an adaptive MCMC, which augments MCMC sampling with nonlocal transition kernels parameterized with generative models known as normalizing flows. We focus on a setting where there are no preexisting data, as is commonly the case for problems in which MCMC is used. Our method uses 1) an MCMC strategy that blends local moves obtained from any standard transition kernel with those from a generative model to accelerate the sampling and 2) the data generated this way to adapt the generative model and improve its efficacy in the MCMC algorithm. We provide a theoretical analysis of the convergence properties of this algorithm and investigate numerically its efficiency, in particular in terms of its propensity to equilibrate fast between metastable modes whose rough location is known a priori but respective probability weight is not. We show that our algorithm can sample effectively across large free energy barriers, providing dramatic accelerations relative to traditional MCMC algorithms.}}


@article{nicoli2021,
  title = {Estimation of Thermodynamic Observables in Lattice Field Theories with Deep Generative Models},
  author = {Nicoli, Kim A. and Anders, Christopher J. and Funcke, Lena and Hartung, Tobias and Jansen, Karl and Kessel, Pan and Nakajima, Shinichi and Stornati, Paolo},
  journal = {Phys. Rev. Lett.},
  volume = {126},
  issue = {3},
  pages = {032001},
  numpages = {6},
  year = {2021},
  month = {Jan},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevLett.126.032001},
  url = {https://link.aps.org/doi/10.1103/PhysRevLett.126.032001}
}

@article{nicoli2020,
  title = {Asymptotically unbiased estimation of physical observables with neural samplers},
  author = {Nicoli, Kim A. and Nakajima, Shinichi and Strodthoff, Nils and Samek, Wojciech and M\"uller, Klaus-Robert and Kessel, Pan},
  journal = {Phys. Rev. E},
  volume = {101},
  issue = {2},
  pages = {023304},
  numpages = {10},
  year = {2020},
  month = {Feb},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevE.101.023304},
  url = {https://link.aps.org/doi/10.1103/PhysRevE.101.023304}
}



@InProceedings{matthews2022,
  title = 	 {Continual Repeated Annealed Flow Transport {M}onte {C}arlo},
  author =       {Matthews, Alex and Arbel, Michael and Rezende, Danilo Jimenez and Doucet, Arnaud},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {15196--15219},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/matthews22a/matthews22a.pdf},
  url = 	 {https://proceedings.mlr.press/v162/matthews22a.html},
  abstract = 	 {We propose Continual Repeated Annealed Flow Transport Monte Carlo (CRAFT), a method that combines a sequential Monte Carlo (SMC) sampler (itself a generalization of Annealed Importance Sampling) with variational inference using normalizing flows. The normalizing flows are directly trained to transport between annealing temperatures using a KL divergence for each transition. This optimization objective is itself estimated using the normalizing flow/SMC approximation. We show conceptually and using multiple empirical examples that CRAFT improves on Annealed Flow Transport Monte Carlo (Arbel et al., 2021), on which it builds and also on Markov chain Monte Carlo (MCMC) based Stochastic Normalizing Flows (Wu et al., 2020). By incorporating CRAFT within particle MCMC, we show that such learnt samplers can achieve impressively accurate results on a challenging lattice field theory example.}
}


article{BIALAS2022108502,
title = {Hierarchical autoregressive neural networks for statistical systems},
journal = {Computer Physics Communications},
volume = {281},
pages = {108502},
year = {2022},
issn = {0010-4655},
doi = {https://doi.org/10.1016/j.cpc.2022.108502},
url = {https://www.sciencedirect.com/science/article/pii/S0010465522002211},
author = {Piotr Białas and Piotr Korcyl and Tomasz Stebel},
keywords = {Variational autoregressive neural networks, Hierarchical neural networks, Spin systems, Ising model, Markov Chain Monte Carlo},
abstract = {It was recently proposed that neural networks could be used to approximate many-dimensional probability distributions that appear e.g. in lattice field theories or statistical mechanics. Subsequently they can be used as variational approximators to assess extensive properties of statistical systems, like free energy, and also as neural samplers used in Monte Carlo simulations. The practical application of this approach is unfortunately limited by its unfavourable scaling both of the numerical cost required for training, and the memory requirements with the system size. This is due to the fact that the original proposition involved a neural network of width which scaled with the total number of degrees of freedom, e.g. L2 in case of a two dimensional L×L lattice. In this work we propose a hierarchical association of physical degrees of freedom, for instance spins, to neurons which replaces it with the scaling with the linear extent L of the system. We demonstrate our approach on the two-dimensional Ising model by simulating lattices of various sizes up to 128×128 spins, with time benchmarks reaching lattices of size 512×512. We observe that our proposal improves the quality of neural network training, i.e. the approximated probability distribution is closer to the target that could be previously achieved. As a consequence, the variational free energy reaches a value closer to its theoretical expectation and, if applied in a Markov Chain Monte Carlo algorithm, the resulting autocorrelation time is smaller. Finally, the replacement of a single neural network by a hierarchy of smaller networks considerably reduces the memory requirements.}
}

@article{marzouk2016,
  title={Sampling via measure transport: An introduction},
  author={Marzouk, Youssef and Moselhy, Tarek and Parno, Matthew and Spantini, Alessio},
  journal={Handbook of uncertainty quantification},
  volume={1},
  pages={2},
  year={2016},
  publisher={Springer Cham}
}

@article{wang2022,
author = {Yihang Wang  and Lukas Herron  and Pratyush Tiwary },
title = {From data to noise to data for mixing physics across temperatures with generative artificial intelligence},
journal = {Proceedings of the National Academy of Sciences},
volume = {119},
number = {32},
pages = {e2203656119},
year = {2022},
doi = {10.1073/pnas.2203656119},
URL = {https://www.pnas.org/doi/abs/10.1073/pnas.2203656119},
eprint = {https://www.pnas.org/doi/pdf/10.1073/pnas.2203656119},
abstract = {While it is tempting to use high-temperature simulations to infer observations about low temperature, it is not always clear how to do so. Here we demonstrate how using generative artificial intelligence we can mix information from simulations conducted at a set of temperatures and generate molecular configurations at any temperature of interest including temperatures at which simulations were never performed. The configurations we generate carry correct Boltzmann weights, and our model minimizes the generation of spurious unphysical configurations. We demonstrate its use here through combining with replica exchange molecular dynamics in a postprocessing framework for sampling peptide and ribonucleic acid. We believe the framework is extensible to generic simulations and experiments for mixing control parameters other than temperature. Using simulations or experiments performed at some set of temperatures to learn about the physics or chemistry at some other arbitrary temperature is a problem of immense practical and theoretical relevance. Here we develop a framework based on statistical mechanics and generative artificial intelligence that allows solving this problem. Specifically, we work with denoising diffusion probabilistic models and show how these models in combination with replica exchange molecular dynamics achieve superior sampling of the biomolecular energy landscape at temperatures that were never simulated without assuming any particular slow degrees of freedom. The key idea is to treat the temperature as a fluctuating random variable and not a control parameter as is usually done. This allows us to directly sample from the joint probability distribution in configuration and temperature space. The results here are demonstrated for a chirally symmetric peptide and single-strand RNA undergoing conformational transitions in all-atom water. We demonstrate how we can discover transition states and metastable states that were previously unseen at the temperature of interest and even bypass the need to perform further simulations for a wide range of temperatures. At the same time, any unphysical states are easily identifiable through very low Boltzmann weights. The procedure while shown here for a class of molecular simulations should be more generally applicable to mixing information across simulations and experiments with varying control parameters.}}


@inproceedings{
shi2024simplified,
title={Simplified and Generalized Masked Diffusion for Discrete Data},
author={Jiaxin Shi and Kehang Han and Zhe Wang and Arnaud Doucet and Michalis Titsias},
booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
year={2024},
url={https://openreview.net/forum?id=xcqSOfHt4g}
}

@inproceedings{
gruver2023protein,
title={Protein Design with Guided Discrete Diffusion},
author={Nate Gruver and Samuel Don Stanton and Nathan C. Frey and Tim G. J. Rudner and Isidro Hotzel and Julien Lafrance-Vanasse and Arvind Rajpal and Kyunghyun Cho and Andrew Gordon Wilson},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
year={2023},
url={https://openreview.net/forum?id=MfiK69Ga6p}
}


@article{lisanza2024,
	abstract = {Protein denoising diffusion probabilistic models are used for the de novo generation of protein backbones but are limited in their ability to guide generation of proteins with sequence-specific attributes and functional properties. To overcome this limitation, we developed ProteinGenerator (PG), a sequence space diffusion model based on RoseTTAFold that simultaneously generates protein sequences and structures. Beginning from a noised sequence representation, PG generates sequence and structure pairs by iterative denoising, guided by desired sequence and structural protein attributes. We designed thermostable proteins with varying amino acid compositions and internal sequence repeats and cage bioactive peptides, such as melittin. By averaging sequence logits between diffusion trajectories with distinct structural constraints, we designed multistate parent--child protein triples in which the same sequence folds to different supersecondary structures when intact in the parent versus split into two child domains. PG design trajectories can be guided by experimental sequence--activity data, providing a general approach for integrated computational and experimental optimization of protein function.},
	author = {Lisanza, Sidney Lyayuga and Gershon, Jacob Merle and Tipps, Samuel W. K. and Sims, Jeremiah Nelson and Arnoldt, Lucas and Hendel, Samuel J. and Simma, Miriam K. and Liu, Ge and Yase, Muna and Wu, Hongwei and Tharp, Claire D. and Li, Xinting and Kang, Alex and Brackenbrough, Evans and Bera, Asim K. and Gerben, Stacey and Wittmann, Bruce J. and McShan, Andrew C. and Baker, David},
	date = {2024/09/25},
	date-added = {2025-01-30 20:41:37 -0500},
	date-modified = {2025-01-30 20:41:37 -0500},
	doi = {10.1038/s41587-024-02395-w},
	id = {Lisanza2024},
	isbn = {1546-1696},
	journal = {Nature Biotechnology},
	title = {Multistate and functional protein design using RoseTTAFold sequence space diffusion},
	url = {https://doi.org/10.1038/s41587-024-02395-w},
	year = {2024},
	bdsk-url-1 = {https://doi.org/10.1038/s41587-024-02395-w}}


@inproceedings{
sahoo2024simple,
title={Simple and Effective Masked Diffusion Language Models},
author={Subham Sekhar Sahoo and Marianne Arriola and Aaron Gokaslan and Edgar Mariano Marroquin and Alexander M Rush and Yair Schiff and Justin T Chiu and Volodymyr Kuleshov},
booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
year={2024},
url={https://openreview.net/forum?id=L4uaAR4ArM}
}

@inproceedings{ho2020,
 author = {Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {6840--6851},
 publisher = {Curran Associates, Inc.},
 title = {Denoising Diffusion Probabilistic Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/4c5bcfec8584af0d967f1ab10179ca4b-Paper.pdf},
 volume = {33},
 year = {2020}
}

@article{song2020score,
  title={Score-based generative modeling through stochastic differential equations},
  author={Song, Yang and Sohl-Dickstein, Jascha and Kingma, Diederik P and Kumar, Abhishek and Ermon, Stefano and Poole, Ben},
  journal={arXiv preprint arXiv:2011.13456},
  year={2020}
}


@inproceedings{
lipman2023flow,
title={Flow Matching for Generative Modeling},
author={Yaron Lipman and Ricky T. Q. Chen and Heli Ben-Hamu and Maximilian Nickel and Matthew Le},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=PqvMRDCJT9t}
}

@article{liu2022flow,
  title={Flow straight and fast: Learning to generate and transfer data with rectified flow},
  author={Liu, Xingchao and Gong, Chengyue and Liu, Qiang},
  journal={arXiv preprint arXiv:2209.03003},
  year={2022}
}


@InProceedings{tian2024,
  title = 	 {Liouville Flow Importance Sampler},
  author =       {Tian, Yifeng and Panda, Nishant and Lin, Yen Ting},
  booktitle = 	 {Proceedings of the 41st International Conference on Machine Learning},
  pages = 	 {48186--48210},
  year = 	 {2024},
  editor = 	 {Salakhutdinov, Ruslan and Kolter, Zico and Heller, Katherine and Weller, Adrian and Oliver, Nuria and Scarlett, Jonathan and Berkenkamp, Felix},
  volume = 	 {235},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {21--27 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://raw.githubusercontent.com/mlresearch/v235/main/assets/tian24c/tian24c.pdf},
  url = 	 {https://proceedings.mlr.press/v235/tian24c.html},
  abstract = 	 {We present the Liouville Flow Importance Sampler (LFIS), an innovative flow-based model for generating samples from unnormalized density functions. LFIS learns a time-dependent velocity field that deterministically transports samples from a simple initial distribution to a complex target distribution, guided by a prescribed path of annealed distributions. The training of LFIS utilizes a unique method that enforces the structure of a derived partial differential equation to neural networks modeling velocity fields. By considering the neural velocity field as an importance sampler, sample weights can be computed through accumulating errors along the sample trajectories driven by neural velocity fields, ensuring unbiased and consistent estimation of statistical quantities. We demonstrate the effectiveness of LFIS through its application to a range of benchmark problems, on many of which LFIS achieved state-of-the-art performance.}
}

@misc{mate2023learning,
      title={Learning Interpolations between Boltzmann Densities}, 
      author={Bálint Máté and François Fleuret},
      year={2023},
      eprint={2301.07388},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/2301.07388}, 
}

@inproceedings{
richter2024improved,
title={Improved sampling via learned diffusions},
author={Lorenz Richter and Julius Berner},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=h4pNROsO06}
}

@misc{nüsken2023,
      title={Solving high-dimensional Hamilton-Jacobi-Bellman PDEs using neural networks: perspectives from the theory of controlled diffusions and measures on path space}, 
      author={Nikolas Nüsken and Lorenz Richter},
      year={2023},
      eprint={2005.05409},
      archivePrefix={arXiv},
      primaryClass={math.OC},
      url={https://arxiv.org/abs/2005.05409}, 
}

@article{jarzynski1997,
  title = {Nonequilibrium Equality for Free Energy Differences},
  author = {Jarzynski, C.},
  journal = {Phys. Rev. Lett.},
  volume = {78},
  issue = {14},
  pages = {2690--2693},
  numpages = {0},
  year = {1997},
  month = {Apr},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevLett.78.2690},
  url = {https://link.aps.org/doi/10.1103/PhysRevLett.78.2690}
}



@misc{sun2024,
      title={Dynamical Measure Transport and Neural PDE Solvers for Sampling}, 
      author={Jingtong Sun and Julius Berner and Lorenz Richter and Marius Zeinhofer and Johannes Müller and Kamyar Azizzadenesheli and Anima Anandkumar},
      year={2024},
      eprint={2407.07873},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2407.07873}, 
}

@article{albergo2023stochastic,
  title={Stochastic interpolants: A unifying framework for flows and diffusions},
  author={Albergo, Michael S and Boffi, Nicholas M and Vanden-Eijnden, Eric},
  journal={arXiv preprint arXiv:2303.08797},
  year={2023}
}

@misc{tran2019,
      title={Discrete Flows: Invertible Generative Models of Discrete Data}, 
      author={Dustin Tran and Keyon Vafa and Kumar Krishna Agrawal and Laurent Dinh and Ben Poole},
      year={2019},
      eprint={1905.10347},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1905.10347}, 
}