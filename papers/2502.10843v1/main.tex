%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

\usepackage{duckuments}
\usepackage{dirtytalk}
\usepackage{algorithmic}
% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage[shortlabels]{enumitem} 
% \usepackage{cleveref} 
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}


%\usepackage{icml2025}
%\usepackage[accepted]{icml2025}
\usepackage[accepted]{arxiv}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
% \usepackage{algorithm}    
% \usepackage{algorithmic}
% \usepackage{algpseudocode}


\newcommand{\new}[1]{{\color{orange}{#1}}}


% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}
\renewcommand{\eqref}[1]{(\ref{#1})}
\crefformat{equation}{(#2#1#3)}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}

\input{math_commands}
\newcommand{\brac}[1]{[#1]}
\renewcommand{\rd}{\rho}
\newcommand{\urd}{\tilde{\rho}}
\newcommand{\nc}{Z}
\renewcommand{\rp}{U}
\newcommand{\z}{\mathbf{z}}
\newcommand{\I}{\mathbf{I}}
\newcommand{\tkernel}{k}
\newcommand{\mycomment}[1]{%
  \textcolor{red}{~#1}%
}
\newcommand{\tommi}[1]{%
  \textcolor{red}{~#1}%
}
\newcommand{\msa}[1]{%
  \textcolor{red}{~#1}%
}

\DeclareMathOperator*{\RTs}{\text{SE}(3)}
% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{LEAPS: A Discrete Neural Sampler}

\begin{document}

\twocolumn[
\icmltitle{LEAPS: A discrete neural sampler via locally equivariant networks}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Peter Holderrieth}{equal,mit}
\icmlauthor{Michael S. Albergo}{equal,harvard,iaifi}
\icmlauthor{Tommi Jaakkola}{mit}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{mit}{Massachusetts Institute of Technology}
\icmlaffiliation{harvard}{Society of Fellow, Harvard University}
\icmlaffiliation{iaifi}{Institute for Artificial Intelligence and Fundamental Interactions}

\icmlcorrespondingauthor{Peter Holderrieth}{phold@mit.edu}
\icmlcorrespondingauthor{Michael S. Albergo}{malbergo@fas.harvard.edu}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
% \begin{itemize}
    We propose \emph{LEAPS}, an algorithm to sample from discrete distributions known up to normalization by learning a rate matrix of a continuous-time Markov chain (CTMC). LEAPS can be seen as a continuous-time formulation of annealed importance sampling and sequential Monte Carlo methods, extended so that the variance of the importance weights is offset by the inclusion of the CTMC. 
    To derive these importance weights, we introduce a set of Radon-Nikodym derivatives of CTMCs over their path measures. Because the computation of these weights is intractable with standard neural network parameterizations of rate matrices, we devise a new compact representation for rate matrices via what we call \textit{locally equivariant} functions.  
    To parameterize them, we introduce a family of locally equivariant multilayer perceptrons, attention layers, and convolutional networks, and provide an approach to make deep networks that preserve the local equivariance.
    This property allows us to propose a scalable training algorithm for the rate matrix such that the variance of the importance weights associated to the CTMC are minimal. We demonstrate the efficacy of LEAPS on problems in statistical physics. 
\end{abstract}
    
    
    % The method can be seen as an extension of annealed importance sampling and sequential Monte Carlo methods via the inclusion of a parametric rate matrix of a Continuous Time Markov Chain (CTMC). that offsets the variance of the importance weights. To fulfill this, we introduce a set of Radon-Nikodym derivatives for Continuous-time Markov chains (CTMCs) over their path measures that allows us to define these importance weights for arbitrary CTMCs.
    % \item To make these algorithms scalable, we devise a new compact representation for rate matrices in discrete diffusion via a parameterization which we call local equivariance. 
% \end{itemize}
% An algorithm is proposed, which we call _____, that augments sequential monte carlo methods for sampling discrete distributions through the inclusion of dynamical measure transport. 



% An algorithm is proposed based on dynamical transport of measure to sample from distributions defined on a discrete state space. 


% To make these algorithms scalable, we devise a new compact representation for rate matrices in discrete diffusion via a parameterization which we call proximal equivariance coherence. 

% We devise 



% \begin{itemize}

% \item However, the computation of these weights is intractable with standard neural network parameterizations of rate matrices of CTMCs used in the context of “discrete diffusion models”.
% \item To make this scalable, we impose a novel condition on a neural network that we call \emph{local equivariance}. To parameterize such functions, we introduce a locally equivariant attention networks (LEANs) that make the computation of importance weights scalable. 
% \item Further, LEANs allow us to
% \emph{train} the neural sampler.
% \item To make this algorithm scalable, we devise a new compact representation for rate matrices of CTMCs via a parameterization which we call proximal equivariance. 
% \end{itemize}
% \end{abstract}

\section{Introduction}
\label{sec:introduction}

A prevailing task across statistics and the sciences is to draw samples from a probability distribution whose probability density is known up to normalization. Solutions to this problem have applications in topics ranging across Bayesian uncertainty quantification \citep{gelfand1990}, capturing the molecular dynamics of chemical compounds \citep{berendsen1984, allen1987}, and computational approaches to statistical and quantum physics \citep{wilson1974, duane1987, faulkner2023sampling}. 


\begin{figure}[t]
    \centering
    \includegraphics[width=0.75\linewidth]{figures/front_plot_wide.pdf}
    \caption{ Illustration of the LEAPS algorithm. LEAPS allows to learn a dynamical transport of discrete distributions from $t=0$ to $t=1$ (blue).
    Sample are generated via the simulation of a Continuous-time Markov chain (yellow). Further, importance sampling weights allow to correct training errors trading off sample efficiency (red).}
    \label{fig:enter-label}
\end{figure}
The most salient approach to such sampling problems is Markov chain Monte Carlo (MCMC) \cite{metropolis1953, robert1999monte}, in which a randomized process is simulated whose equilibrium is the distribution of interest. While powerful and widely applied, MCMC methods can be inefficient as they suffer from slow convergence times into equilibrium, especially for distributions exhibiting multi-modality. Therefore, MCMC is often combined with other techniques that rely on non-equilibrium dynamics, e.g. via annealing from a simpler distribution with annealed importance sampling (AIS) \cite{kahn1951, neal2001annealed} or sequential Monte Carlo methods (SMC) \cite{ doucet2001}. Even then, the variance of these importance weights may be untenably large, and making sampling algorithms more efficient remains an active area of research. Inspired by the rapid progress in deep learning,
there has been extensive interest in augmenting contemporary sampling algorithms with learning \cite{noe2019,albergo2019,gabrie2022,nicoli2020, matthews2022}, while still maintaining their statistical guarantees. 

Recently, there has been rapid progress in development of generative models using techniques from dynamical measure transport, i.e. where data from a base distribution is transformed into samples from the target distribution via flow or diffusion processes \cite{ho2020, song2020score, albergo2022building, albergo2023stochastic, lipman2023flow, liu2022flow}. More recently, these models could also be developed for discrete state spaces \citep{campbell2022continuous, gatdiscrete, shaul2024flow, campbell2024generative} and general state spaces and Markov processes \citep{holderrieth2024generator}.
% , commonly called "discrete diffusion" models in the context of generative modeling \citep{campbell2022continuous, gatdiscrete, shaul2024flow, campbell2024generative}.

While there have been various developments on adapting these non-equilibrium dynamics for sampling in \emph{continuous} state spaces \cite{zhang2022path, vargas2023denoising, mate2023learning, tian2024, albergo2024nets, richter2024improved, akhound2024iterated, sun2024}, there is a lack of existing literature on such sampling approaches for discrete distributions. However, discrete data are prevalent in various applications, such as in the study of spin models in statistical physics, protein and genomic data, and language. To this end, we provide a new solution to the discrete sampling problem via CTMCs. Our method is similar in spirit to the results in \cite{albergo2024nets, vargas2024transport} but takes the necessary theoretical and computational leaps to make these approaches possible for discrete distributions. Our \textbf{main contributions} are:

\begin{itemize}
    \item We introduce LEAPS, a non-equilibrium transport sampler for discrete distributions via CTMCs that combines annealed importance sampling and sequential Monte Carlo with learned measure transport.
    \item To define the importance weights, we derive a Radon-Nikodym derivative for reverse-time CTMCs, control of which minimizes the variance of these weights.
    \item We show that the measure transport can be learnt and the variance of the importance weights minimized by optimizing a physics-informed neural network (PINN) loss function.
    \item We make the computation of the PINN objective scalable by introducing the notion of a locally equivariant network. We show how to build locally equivariant versions of common neural network architectures, including attention and convolutions.
    %We present several constructions of locally equivariant networks allowing to train dynamic measure transport at scale.
    \item  We experimentally verify the correctness and efficacy of the resulting LEAPS algorithm in high dimensions via simulation of the Ising model. 
\end{itemize}

\section{Setup and Assumptions}
In this work, we are interested in the problem of sampling from a \textbf{target distribution} $\rd_1$ on a \textbf{finite state space} $S$. We refer to $\rd_1$ by its probability mass function (pmf) given by
\begin{align}
    \rd_1(x) = \frac{1}{Z_1}\exp(-U_1(x))\quad (x\in S),
\end{align}
where we assume that we do not know the normalization constant $Z_1$ but only the function \textbf{potential} $U_1$. Our goal is to  produce samples $X\sim \rd_1$. To achieve this goal, it is common to construct a \textbf{time-dependent probability mass function (pmf)} $(\rd_t)_{0\leq t\leq 1}$  over $S$ which fulfils that $\rd_0$ has a distribution from which we can sample easily, e.g. $\rd_0=\text{Unif}_{S}$, and $\rho_1$ is our target of interest. We write $\rd_t$ as:
%
%fulfilling two conditions: (1) The distribution at time $t=1$ is the target distribution, i.e. $\rd_1=\rd$. (2) The distribution at time $t=0$ is a distribution that we know and can easily sample from. 
%A common choice is $\rd_0=\text{Unif}_{S}$, the uniform distribution over $S$. We equivalently represent $\rd_t$ via \textbf{time-dependent potentials} $U_t$ in the form
\begin{align}
    \rd_t(x)=&\frac{1}{Z_t}\exp(-U_t(x)),\\
    Z_t=&\sum\limits_{y\in S}\exp(-U_t(y)),  \quad F_t=-\log Z_t
\end{align}
where $Z_t$ (or equivalently $F_t$) is unknown. The value $F_t$ is also called the \textbf{free energy}. Throughout, we assume that $U_t$ is continuously differentiable in $t$. 
%A standard example to construct $U_t$ (and thereby $\rd_t$) is via
%\begin{align*}
%    U_t(x) = (1-t) U_0(x) + t U_1(x)
%\end{align*}
%where $U_0$ is the potential of a chosen $\rd_0$ and $U_1=U$ is the potential of the target distribution $\rd$, though it is often advantageous to explore alternatives. 
For example, we can set $U_t(x)=tU_1(x)$ so that $\rd_0=\text{Unif}_{S}$ and we get that $\rd_t\propto \exp(-tU_1(x))$ that can be considered a form of \emph{temperature annealing}.

\section{Sampling with CTMCs}
\label{sec:ctmc}
In this work, we seek to sample from $\rd_1$ using \textbf{continuous-time Markov chains (CTMCs)}. A CTMC $(X_t)_{0\leq t\leq 1}$ is given by a set of random variables $X_t\in S$ ($0\leq t\leq 1$) whose evolution is determined by a time-dependent \textbf{rate matrix} $Q_t(y,x)\in \mathbb{R}$ ($0\leq t\leq 1,x,y\in S$) which fulfills the conditions:
\begin{subequations}
\label{eq:q_t_cond}
\begin{align}
\label{eq:q_t_cond_1}
    Q_{t}(y;x)\geq& 0 \quad &\text{(for  }y\neq x)\\
\label{eq:q_t_cond_2}
    Q_{t}(x;x)=&-\sum\limits_{y\neq x}Q_t(y,x)\quad &\text{(for }x\in S)
\end{align}
\end{subequations}
The rate matrix $Q_t$ determines the \textbf{generator equation}
\begin{align}
\mathbb{P}[X_{t+h}=y|X_t=x] = \mathbf{1}_{x=y}+hQ_t(y,x)+o(h)
\end{align}
for all $x,y\in S$ and $h>0$ where $o(h)$ describes an error function such that $\lim\limits_{h\to 0}o(h)/h=0$. Because this equation describes the infinitesimal transition probabilities of the CTMC, we can sample from $X_t$ approximately via the following iterative Euler scheme:
\begin{align}
\label{eq:gen:eq:euler}
    X_{t+h} \sim \tilde{\mathbb{P}}[\cdot|X_t]:=(\mathbf{1}_{X_t=y}+hQ_t(y,X_t))_{y\in S}
\end{align}
where $\tilde{\mathbb{P}}[\cdot|X_t]$ describes a valid probability distribution for small $h>0$ by the conditions we imposed on $Q_t$ (see \cref{eq:q_t_cond}).

% \begin{align*}
%     X_{t+h} \sim& \begin{cases}
%       \exp(hQ_t(X_t;X_t))+o(h) & \text{ if }X_{t+h}=X_t\\
%       \frac{Q_t(X_{t+h};X_t)}{|Q_t(X_t;X_t)|}\left(1-\exp(hQ_t(X_t;X_t))\right)+o(h)& \text{ if }X_{t+h}\neq X_t
%   \end{cases}
% \end{align*}
Our goal is to find a $Q_t$ that is a solution to the \textbf{Kolmogorov forward equation (KFE)}
\begin{align}
    \label{eq:kfe}
    \partial_t \rd_t(x) =& \sum\limits_{y\in S}Q_t(x,y)\rd_t(y), \quad \rho_{t=0} = \rho_0.
\end{align}
Fulfilling the KFE is a necessary and sufficient condition to ensure that the distribution of walkers initialized as $X_0 \sim \rho_0$ and evolving according to \cref{eq:gen:eq:euler} follow the prescribed path $\rho_t$, in particular such that $X_{t=1} \sim \rho_1$.

% Our goal is to learn a rate matrix $Q_t$ such that $X_t$ initialized with $\rd_0$ "follows along" the time-varying density, i.e. that the condition holds
% \begin{align}
% \label{eq:follow_density}
%     X_0\sim \rd_0\quad \Rightarrow\quad X_t\sim \rd_t \quad (0\leq t\leq 1)
% \end{align}
% As $\rd_1=\rd$ is the target distribution by construction, finding a CTMC satisfying this condition in particular implies that we faithfully sample from $\rd$ by using the endpoint $X_1$ of a simulated CTMC trajectory. 

% A sufficient and necessary condition for \cref{eq:follow_density} is that $Q_t$ satisfies the \textbf{Kolmgorov Forward Equation (KFE)} given by:
% \begin{align}
%     \label{eq:kfe}
%     \partial_t \rd_t(x) =& \sum\limits_{y\in S}Q_t(x,y)\rd_t(y)
% \end{align}
% We can therefore rephrase the problem from \cref{eq:follow_density} to finding a $Q_t$ satisfying the KFE.

\begin{remark}
While the problem we focus on this work is sampling, the contributions of this work hold more generally for the problem of finding a CTMC that follows a prescribed time-varying density $\rd_t$, i.e. that the condition in \cref{eq:kfe} holds.
\end{remark}

\section{Proactive Importance Sampling}
In general, the CTMC $(X_t)_{0\leq t\leq 1}$ with arbitrary $Q_t$ will have different marginals than $\rd_t$. To still obtain an unbiased estimator, it is common to use \textbf{importance sampling (IS)} to reweight samples obtained while simulating $X_t$ or \textbf{sequential Monte Carlo (SMC)} \citep{doucet2001} to resample the walkers along the trajectory. Here, we introduce a time-evolving set of log-weights $A_t\in \mathbb{R}$ for $0\leq t\leq 1$ to re-weight the distribution of $X_t$ to a distribution $\mu_t$ defined such that for all $h:S\to\mathbb{R}$
\begin{align*}
\mathbb{E}_{x\sim \mu_t}[h(x)]=&\frac{\mathbb{E}[\exp(A_t)h(X_t)]}{\mathbb{E}[\exp(A_t)]}\\
\Leftrightarrow \quad \mu_t(x)=&\frac{\mathbb{E}[\exp(A_t)|X_t=x]}{\sum\limits_{y\in S}\mathbb{E}[\exp(A_t)|X_t=y]},
\end{align*}
where $\mathbb E[\cdot]$ denotes expectation over the process ($X_t, A_t$). Intuitively, the distribution $\mu_t$ is obtained by re-weighting samples from the current distribution of $X_t$. This effectively means that from a finite number of samples $(X_t^1,A_t^1),\dots,(X_t^n,A_t^n)$, we can obtain a Monte Carlo estimator via
\begin{align}
\label{eq:is_weights_mc_estimator}
    \mathbb{E}_{x\sim \mu_t}[h(x)]\approx\sum\limits_{i=1}^{n}\frac{\exp(A_t^i)}{\sum\limits_{j=1}^{n}\exp(A_t^j)}h(X_t^i)
\end{align}
Our goal is to find a scheme of computing $A_t$ such that its reweighted distribution coincides with the target densities $\rd_t$:
\begin{align}
\label{eq:reweighted_equals_target}
\mu_t = \rd_t \quad (0\leq t\leq 1)
\end{align}
In particular, this would mean
that \cref{eq:is_weights_mc_estimator} is a good approximation for large $n$.

\paragraph{Proactive importance sampling.} We next propose an IS scheme of computing weights $A_t$. Before we provide a formal derivation, we  provide a \emph{heuristic} derivation of our proposed scheme in the following paragraph. Intuitively, the log-weights $A_t$ should accumulate the deviation from the true distribution of $X_t$ to the desired distribution $\rd_t$. We can rephrase this as "accumulating the error of the KFE" that one may want to write as the difference between both sides of \cref{eq:kfe}:
\begin{align*}
    \partial_t\rd_t(x) - \sum\limits_{y\in S}Q_t(x,y)\rd_t(y)
\end{align*}
As we do not know the normalization constant $Z_t$, it is intuitive to divide by $\rd_t(x)$ to get
\begin{align*}
    \frac{\partial_t\rd_t(x)}{\rd_t(x)}-\sum\limits_{y\in S}Q_t(x,y)\frac{\rd_t(y)}{\rd_t(x)}
\end{align*}
Using that $\partial_t U_t(x)=-\partial_t\log 
\rd_t(x)=-\partial_t\rd_t(x)/\rd_t(x)$, we obtain equivalently:
\begin{align}
\label{eq:proactivate_operator}
    \kop_t\rd_t(x) = -\partial_tU_t(x) - \sum\limits_{y\in S}Q_t(x,y)\frac{\rd_t(y)}{\rd_t(x)}
\end{align}
where we defined a new operator $\kop_t\rd_t$. Intuitively, the operator $\kop_t$ measures the violation from the KFE in log-space and it is intuitive to define $A_t$ as the accumulated error of that violation, i.e. as the integral
\begin{align}
\label{eq:A_t_definition}
A_t =& \int\limits_{0}^{t}\kop_s\rd_s(X_s)\dd s
\end{align}
To simulate $A_t$ alongside $X_t$, we use the approximate update role:
\begin{align*}
    A_{t+h} = A_{t}+h\kop_t\rd_t(X_t)
\end{align*}
We call this \textbf{proactive importance sampling (proactivate IS)} as the update operator $\mathcal{K}_t$ anticipates where $X_t$ is jumping to. We next provide a rigorous characterization of $A_t$ defined in this manner.

% \mycomment{Make sure that it is clear that this is heuristic.}


% \msa{Some comments: Let's start by stating the KFE so that we can say there is some target $p_t$ which evolves according to the KFE, our aims are to find the $Q_t$ under the dynamics given in your $X_{t+h}$ equation  are s.t. $t=0$ we have $p_0$ and at $t=1$ we have $p_1$. State In our setting we do not have data for a simple regression loss of $Q_t$ but instead have a specified path $p_t = e^{-U_t + F_t}$. Later we show that the minimizer of the PINN is s.t. the KFE holds.}

\section{Proactivate IS via Radon-Nikodym Derivatives}
A priori, it is not clear that the log-weights $A_t$ that we obtain via the proactive rule fulfil the desired condition in \cref{eq:reweighted_equals_target} (i.e. provide a valid IS scheme). Beyond showing this property, we show that there are many possible IS schemes but the proactive update rule is \emph{optimal} among a natural family of IS schemes. To do so, we present a set of Radon-Nikodym derivatives in path space.

Let $\tspace$ be a state space and $\P,\Q$ be two probability measures over $\tspace$. Then the \textbf{Radon-Nikodym derivative (RND)} $\frac{\dd\Q}{\dd \P}$ allows to express expected values of $\Q$ via expected values of $\P$. Specifically, the RND is a function $\frac{\dd\Q}{\dd \P}:\tspace\to\mathbb{R}$ such that for any (bounded, measurable) function $G:\tspace\to \mathbb{R}$ it holds that:
\begin{align*}
\mathbb{E}_{\X\sim \mathbb{Q}}[G(\X)] = \mathbb{E}_{\X\sim\mathbb{P}}\left[G(\X)\frac{\dd\Q}{\dd \P}(\X)\right]
\end{align*}
% A well-known result in probability theory is the Radon–Nikodym theorem \citep{halmos2013measure} that says that the RND exists if and only if $\Q$ is absolutely continuous with respect to $\P$, i.e. if $\P[A]=0$ also $\Q[A]=0$ for all measurable subsets $A\in \tspace$.


The state space that we are interested in is the space $\tspace$ of CTMC trajectories. Specifically, for a trajectory we denote with $X_{t^-}=\lim_{t'\uparrow t}X_{t'}$ the left limit and with $X_{t^+}=\lim_{t'\downarrow t}X_{t'}$ the right limit. The space $\tspace$ of CTMC trajectories is then defined as
\begin{align*}
    \tspace = \{X:[0,1]\to S| X_{t^-}\text{ exists and }X_{t^+}=X_{t}\},
\end{align*}
i.e. all trajectories that are continuous from the right with left limits. Such trajectories are commonly called  \textbf{c\`adl\`ag} trajectories. In other words, jumps (switches between states) happen if and only if $X_{t^-}\neq X_{t}$. We consider \emph{path distributions} (or \emph{path measures}), i.e. probability distributions over trajectories. For a CTMC $\X =(X_t)_{0\leq t\leq 1}$ with rate matrix $Q_t$ and initial distribution $\mu$, we denote the corresponding \textbf{path distribution} as
$\overrightarrow{\P}^{\mu, Q}$ where the arrow $\overrightarrow{\P}$ denotes that we go forward in time. Similarly, we denote with $\overleftarrow{\P}^{\nu, Q'}$ a CTMC running in reverse time initialized with $\nu$. We present the following proposition whose proof can be found in \cref{sec:proof_rnds}:
\begin{proposition}
\label{prop:rnd_forward_backward_ctmcs}
Let $\mu,\nu$ be two initial distributions over $S$. Let $Q_t,Q_t'$ be two rate matrices. Then the Radon-Nikodym derivative of the corresponding path distributions running in opposite time over the time interval $[0,t]$ is given by:
\begin{align*}
\log\frac{\dd\overleftarrow{\P}^{\nu, Q'}}{\dd\overrightarrow{\P}^{\mu, Q}}(\X) &= \log(\nu(X_t))-\log(\mu(X_0))\\
&+\int\limits_{0}^{t}Q_s'(X_s,X_s)-Q_s(X_s,X_s)\dd s\\
&+\sum\limits_{s,X_{s}^{-}\neq X_{s}}\log\left(\frac{Q_s'(X_{s}^-,X_s)}
{Q_s(X_{s},X_s^-)}\right)
\end{align*}
where we sum over all points where $X_s$ jumps in the last term.
\end{proposition}
Let us now revisit our goal of finding an IS scheme to sample from the target distribution $\rd_1$. The key idea is to construct a CTMC running in reverse-time with initial distribution $\rd_t$ and then use the RND from \cref{prop:rnd_forward_backward_ctmcs}. For a function $h:S\to\R$, we can then express its expectation under $\rd_t$ as:
\begin{equation}
\begin{aligned}
	\mathbb{E}_{x\sim \rd_t}[h(x)] &= \mathbb{E}_{\X\sim \overleftarrow{\P}^{\rd_t, Q'}}[h(X_t)]\\
	&=\mathbb{E}_{\Y\sim \overrightarrow{\P}^{\rd_0, Q}}\left[h(X_t)\frac{\dd\overleftarrow{\P}^{\rd_t, Q'}}{\dd\overrightarrow{\P}^{\rd_0, Q}}(\X)\right]
\end{aligned}
\end{equation}
i.e. the RND $\frac{\dd\overleftarrow{\P}^{\rd_1, Q'}}{\dd\overrightarrow{\P}^{\rd_0, Q}}(\X)$ gives a valid set of importance weights. Note that this holds for \emph{arbitrary} $Q_t'$.\\

However, to sample efficiently, it is crucial that the IS weights have low variance. Therefore, we will now derive the \emph{optimal} IS scheme of this form. Ideally the weights will have \emph{zero} variance - in other words the RND $\frac{\dd\overleftarrow{\P}^{\rd_1, Q'}}{\dd\overrightarrow{\P}^{\rd_0, Q}}(\X)$ will be constant $=1$. This is the case if and only if the path measures are the same, i.e. if the CTMC in reverse time is a time-reversal of the CTMC running in forward time. It is well-known that this is equivalent to:
\begin{align}
\label{eq:time_reversal}
    Q_t'(y,x) = Q_t(x,y)\frac{q_t(y)}{q_t(x)}\quad \text{for all }y\neq x
\end{align}
where $q_t$ denotes the true marginal of $X_t$, i.e. $X_t\sim q_t$. As we strive to make $q_t=\rd_t$, it is natural to set $q_t=\rd_t$ in \cref{eq:time_reversal} and define $Q_t'=\bar{Q}_t$ as
\begin{subequations}
\label{eq:Q_bar_def}
\begin{align}
\label{eq:Q_bar_de_1}
    \bar{Q}_t(y,x) = &Q_t(x,y)\frac{\rd_t(y)}{\rd_t(x)}\quad \text{for all }y\neq x\\
\label{eq:Q_bar_def_2}
    \bar{Q}_t(x,x)=&-\sum\limits_{y\in S,y\neq x}Q_t(x,y)\frac{\rd_t(y)}{\rd_t(x)}
\end{align}
\end{subequations}
Let us now return to the proactive update that we defined in \cref{eq:A_t_definition}. We can now rigorously characterize it. Plugging in the definition of $\bar{Q}$, we can use \cref{prop:rnd_forward_backward_ctmcs} to obtain the main result of this section:
\begin{theorem}
\label{thm:is_theorem}
For the proactivate updates $A_t$ as defined in \cref{eq:A_t_definition} and $\bar{Q}_t$ as defined in \cref{eq:Q_bar_def}, it holds:
\begin{align}
\label{eq:A_t_to_RND}
   A_t+F_t-F_0 = \log\frac{\dd\overleftarrow{\P}^{\rd_t, \bar{Q}}}{\dd\overrightarrow{\P}^{\rd_0, Q}}(\X)
\end{align}
This implies that we obtain a valid IS scheme fulfilling:
\begin{align}
\label{eq:is_scheme_is_theorem}
\mathbb{E}_{x\sim \rd_t}[h(x)] = \frac{\mathbb{E}[\exp(A_t)h(X_t)]}{\mathbb{E}[\exp(A_t)]}\quad (0\leq t \leq 1)
\end{align}
i.e. \cref{eq:reweighted_equals_target} holds. Further, $A_t$ will have zero variance for every $0\leq t\leq 1$ if and only if $X_t\sim \rd_t$ for all $0\leq t\leq 1$.
\end{theorem}
A proof can be bound in \cref{appendix:proof_is_theorem}. 
Note that \cref{thm:is_theorem} is useful because we can, in principle, compute $A_t$, i.e. there are no unknown variables, and that this holds for arbitrary $Q_t$. This theorem can be seen as a discrete state space equivalent of the generalized version of the Jarzynski equality \cite{jarzynski1997, vaikuntanathan2008} that has also recently been used for sampling in continuous spaces \cite{vargas2024transport, albergo2024nets}. Finally, it is important to note that the IS scheme will not have zero variance if it does not hold that $X_t\sim \rd_t$.

\section{PINN objective}

As a next step, we introduce a learning procedure for learning an optimal rate matrix of a CTMC. For this, we denote with $Q_t^\theta$ a parameterized rate matrix with parameters $\theta$ (e.g. represented in a neural network). Our goal is to learn $Q_t^\theta$ such that $X_t\sim\rd_t$ is fulfilled for all $0\leq t\leq 1$. By \cref{thm:is_theorem} this equivalent to minimizing the variance of the IS weights. To measure the variance the weights, it is common to use the log-variance divergence \citep{nüsken2023, richter2023improved} given by
\begin{align*}
\mathcal{L}^{\text{log-var}}(\theta;t)=&\mathbb{V}_{\X\sim \mathbb{Q}}[\log\frac{\dd\overleftarrow{\P}^{\rd_t, \bar{Q}^\theta}}{\dd\overrightarrow{\P}^{\rd_0, Q^\theta}}(\X)]\\
=&\mathbb{V}_{\X\sim \mathbb{Q}}[A_t+F_t-F_0]\\
=&\mathbb{V}_{\X\sim \mathbb{Q}}[A_t]
\end{align*}
where $\Q$ is a reference measuring whose support covers the support of $\overleftarrow{\P}^{\rd_t, \bar{Q}^\theta}$ and $\overrightarrow{\P}^{\rd_0, Q^\theta}$ and where we used that $F_0,F_t$ are constants. The above loss is tractable but we can bound it by a loss that is computationally more efficient. To do so, we use an auxiliary \textbf{free energy network} $F_t^\phi:\mathbb{R}\to\mathbb{R}$ with parameters $\phi$. Note that $F_t^\phi$ is a one-dimensional function and therefore induces minimal additional computational cost. As before, let the operator $\mathcal{K}_t^\theta$ be defined as:
\begin{align}
\label{eq:k_t_operator_neural_net}
\mathcal{K}_t^\theta\rd_t(x)=&-\partial_tU_t(x)-\sum\limits_{y\in S}Q_t^\theta(x,y)\frac{\rd_t(y)}{\rd_t(x)}
\end{align}
\begin{proposition}
\label{thm:pinn_theorem}
For any reference measure $\mathbb{Q}$, the \textbf{PINN-objective} defined by 
\begin{align*}
% \label{eq:pinn}
    \mathcal{L}(\theta,\phi;t) = \mathbb{E}_{s\sim \text{Unif}_{[0,t]},x_s\sim \mathbb{Q}_s}\left[|\kop_s^\theta\rd_s(x_s)-\partial_sF_s^\phi|^2\right]
\end{align*}
has a unique minimizer $(\theta^*,\phi^*)$ such that $Q_t^{\theta*}$ satisfies the KFE and $F_t^{\phi^*}=F_t$  is the free energy. Further, this objective is an upper bound to the log-variance divergence:
\begin{align*}
\mathcal{L}^{\text{log-var}}(\theta;t)\leq t^2\mathcal{L}(\theta,\phi;t)
\end{align*}
In particular, if $\mathcal{L}(\theta,\phi;t)=0$, then also $\mathcal{L}^{\text{log-var}}(\theta;t)=0$ and the variance of the IS weights is zero.
\end{proposition}
A proof can be found in \cref{appendix:proof_pinn_theorem}. Note that we can easily minimize the PINN objective via stochastic gradient descent (see \cref{alg:train-with-buffer}). It is "off-policy" as the reference distribution $\mathbb{Q}$ is arbitrary. This objective can be seen as the CTMC equivalent of that in \cite{mate2023learning,albergo2024nets, tian2024, sun2024}. 
% \mycomment{Discuss design choices of $\mathbb{Q}$ to make bound tight. Optimal one would be the reference measure $\mathbb{Q}_s$ times the error. We could separately optimize $\mathbb{Q}_s$.}
% We note that the above bounds also holds for samplers in Euclidean space $S=\mathbb{R}^d$. This connects PINN-based samplers  \citep{albergo2024nets} and minimizing the log-variance divergence \citep{richter2023improved}.


\begin{figure}[ht]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=0.7\columnwidth]{figures/local_equiv.pdf}}
\caption{Visualization of local equivariance. Two tokens $\mathcal{T}=\{-1,+1\}$ and $d=6$. Local equivariance means that the \say{flux} to transition to a neighbor is the negative of the flux of transitioning from that neighbor back.}
\label{fig:local_equivariance}
\end{center}
\vskip -0.2in
\end{figure}

\section{Adding Annealed IS and SMC}
It is possible to add an arbitrary MCMC scheme to the above dynamics. This effectively combines an \say{unlearned} MCMC scheme with a learned transport. Most of the time, MCMC schemes are formulated as \emph{discrete} time Markov chains. Hence, we first describe how they can be formulated as CTMCs. For every fixed $0\leq t\leq 1$, an MCMC scheme for the distribution $\rd_t$ is given by a stochastic matrix $\mathcal{M}_t(y,x)$, i.e. $\mathcal{M}_t(y,x)\geq 0$ with $\sum_{y\in S}\mathcal{M}_t(y,x)=1$. These are constructed to satisfy the corresponding \emph{detailed balance condition}:
\begin{align}
\label{eq:db}
\mathcal{M}_t(y,x)\rd_t(x) = \mathcal{M}_t(x,y)\rd_t(y)
\end{align}
We can convert this into an annealed CTMC scheme by only accepting updates with a probability scaled by a parameter $\epsilon_t\geq 0$:
\begin{align*}
    Q_t^\text{MCMC}(y,x) = \begin{cases} 
    \epsilon_t \mathcal{M}_t(y,x) & \text{ if }y\neq x\\
\epsilon_t(\mathcal{M}_t(x,x)-1)& \text{ if }y= x
    \end{cases}
\end{align*}
% The corresponding rate matrix $Q_t^{\text{MCMC}}$ satisfy a corresponding CTMC \emph{detailed balance condition} \msa{We now emphasize detailed balance condition twice. What if we just say "
The rate matrix $Q_t^{MCMC}$ satisfies \eqref{eq:db} so that the second term of the $\mathcal K_t$-operator (see \eqref{eq:proactivate_operator}) vanishes:
\begin{align*}
\sum\limits_{y\in S}Q_t^{\text{MCMC}}(x,y)\frac{\rd_t(y)}{\rd_t(x)}
=\sum\limits_{y\in S}Q_t^{\text{MCMC}}(y,x)=0
\end{align*}
where we used \cref{eq:q_t_cond}. This implies that the rate matrix
\begin{align*}
Q_t^\theta(y,x)+Q_t^\text{MCMC}(y,x)
\end{align*}
will have the same PINN loss and the same IS weights for the same trajectories - because the $\kop_t^\theta$ remains unchanged (Note that while the RND in \cref{eq:A_t_to_RND} is the same, the  path measures do change). Specifically, this means that we can sample and compute the weights via \cref{alg:samp}. The parameter $\epsilon_t$ controls \say{how much local MCMC mixing} we want to induce. Note that the IS weights can be used both for reweighting at the final time or in addition to resample the walkers along the trajectories, connecting it to the SMC literature \citep{doucet2001}. We specify that one may want to do this whenever the effective sample size (ESS) (see Appendix \ref{app:exp}) drops below a threshold.

\textbf{Generalization of AIS and SMC.} For $Q_t^\theta=0$, the above dynamics describe a continuous formulation of AIS that can be simulated approximately via \cref{alg:samp}. In particular, this means that the algorithm presented here is a \textbf{strict generalization of AIS and SMC} \citep{neal2001annealed, doucet2001}. Note that in the $h\rightarrow 0$, $\epsilon_t \rightarrow \infty$ limit, LEAPS would recover the exact distributions $\rd_t$ with AIS (i.e. even with $Q_t^{\theta} = 0$). Of course, this asymptotic limit is not realizable in practice with finite number of steps, and the inclusion of $Q_t^{\theta}$ allows us to sample much more efficiently while still maintaining statistical guarantees.
% \msa{We introduced AIS earlier already so no need to spell it out. What about: "Algorithm 1 suggests that in the $h\rightarrow 0$, $\epsilon_t \rightarrow \infty$ limit, LEAPS is an AIS/SMC sampler with with ideal importance weights, even with $Q_t^{\theta} = 0$. Of course, this limit is not realizable in practice, and the inclusion of $Q_t^{\theta}$ allows us to approach it much more efficiently, thereby making LEAPS a strict generalization of these methods.}

% \begin{algorithm}[tb]
%    \caption{Bubble Sort}
%    \label{alg:example}
% \begin{algorithmic}
%    \STATE {\bfseries Input:} data $x_i$, size $m$
%    \REPEAT
%    \STATE Initialize $noChange = true$.
%    \FOR{$i=1$ {\bfseries to} $m-1$}
%    \IF{$x_i > x_{i+1}$}
%    \STATE Swap $x_i$ and $x_{i+1}$
%    \STATE $noChange = false$
%    \ENDIF
%    \ENDFOR
%    \UNTIL{$noChange$ is $true$}
% \end{algorithmic}
% \end{algorithm}

\begin{algorithm}[tb]
\caption{LEAPS training with optional replay buffer}
\label{alg:train-with-buffer}
\begin{algorithmic}[1]
\REQUIRE \(B\) batch size, \(N\) time steps, model \(G^\theta_t\), free energy net $F_t^\phi$, learning rate \(\eta\), replay buffer \(\mathcal{B}\).
\WHILE{not converged}
    \IF{\(\texttt{use\_buffer}\)}
    \STATE $(X_{t_m}^m,A_{t_m}^m, t_m)_{m=1,\dots,B} \leftarrow \text{SampleBatch}(\mathcal{B})$
    \ELSE
        \STATE $(X_{t_m}^m,A_{t_m}^m, t_m)_{m=1,\dots,B}\leftarrow$  \cref{alg:samp}
    \ENDIF
    \STATE $\mathcal{L}(\theta,\phi) = \frac{1}{B}\sum_{m} |\kop_{t_m}^\theta\rd_{t_m}(X_{t_m}^m)-\partial_{t_m}F_{t_m}^\phi|^2$
    \STATE \(\theta \gets \theta - \eta \,\nabla_\theta \mathcal{L}(\theta,\phi) \)
    \STATE \(\phi \gets \phi - \eta \,\nabla_\phi \mathcal{L}(\theta,\phi) \)\
\ENDWHILE
\end{algorithmic}
\end{algorithm}
\begin{algorithm}[tb]
\caption{LEAPS Sampling}
\label{alg:samp}
\begin{algorithmic}[1]
\STATE \textbf{Require:} $N$ time steps, $M$ walkers, model $F_t^{\theta}$, replay buffer $\mathcal{B}$, MCMC kernel $\mathcal M_t$, density $\rd_t$, coeff. $\epsilon_t\geq 0$, resample thres. $ 0 \leq \delta \leq 1$
\STATE \textbf{Init: }$X_0^m\sim \rho_0,A_0^m=0$\quad ($m=1,\dots,M)$ 
\STATE \textbf{Set }$h=1/N$
\FOR{$n = 0$ to $N-1$}
    \FOR{$m = 1$ to $M$}    
        \STATE $X_t^m \sim \mathcal{M}_t(\cdot,X_t^m)$  with prob.  $h\epsilon_t$ else $X_t^m$ 
        \STATE $X^m_{t+h} \sim (\mathbf{1}_{X_{t}=y}+hQ_t^\theta(y,X_{t}))_{y\in S}$ 
        \vspace{0.05cm}
        \STATE $A^m_{t + h} =A^m_{t}+h\mathcal{K}_{t}^\theta\rd_t(X^m_{t})$
        \ENDFOR
    \STATE $t\leftarrow t+h$
    \IF{$\text{ESS}(A_t) \leq \delta$}
    \STATE $X_t = \text{resample}(X_t, A_t) \quad  (m=1,\dots,M)$
    \STATE $A_t = 0 \qquad \qquad \qquad \,\,\,\,  \quad  (m=1,\dots,M)$
    \ENDIF
    \ENDFOR
    \STATE \textbf{Optional:} Store $\{(X_t^m, A_t^m, t)\}_{t,m}$  in $\mathcal{B}$.
\end{algorithmic}
\end{algorithm}


% \begin{algorithm}[tb]
% \caption{LEAPS training with optional replay buffer}
% \label{alg:train-with-buffer}
% \begin{algorithmic}[1]
% \Require \(M\) walkers, \(N\) time steps, model \(G^\theta_t\), free energy net $F_t^\phi$, learning rate \(\eta\), replay buffer \(\mathcal{B}\).
% \While{not converged}
%     \If{\(\texttt{use\_buffer}\)}
%     \State \(\{(\tilde X, \tilde A, \tilde t)\} \leftarrow \text{SampleBatch}(\mathcal{B})\)
%     \Else
%         \State $\{(\tilde X, \tilde A, \tilde t)\}   \leftarrow$  \cref{alg:samp}
%     \EndIf
%     \State \(\theta \gets \theta - \eta \,\nabla_\theta \mathcal{L}(\theta,\phi; \tilde X, \tilde A, \tilde t \,) \)
%     \State \(\phi \gets \phi - \eta \,\nabla_\phi \mathcal{L}(\theta,\phi; \tilde X, \tilde A, \tilde t \,) \)

% \EndWhile
% \end{algorithmic}
% \end{algorithm}
\section{Efficient IS and training via local equivariance}

We now turn to the question of how to make the above training procedure efficient. Note that for small state spaces $S$ we could rely on analytical solutions to the KFE \citep{campbell2022continuous, shaul2024flow}. In many applications, though, the state space $S$ is so large 
that we cannot store $|S|$ elements efficiently in a computer. Often state spaces $S$ are of the form $S=\mathcal{T}^d$ where $\mathcal{T}=\{1,\dots,N\}$ is a set of $N$ tokens. We use the notation $\tau$ for a \textbf{token}, i.e. an element $\tau\in\mathcal{T}$. One then defines a notion of a \textbf{neighbor} $y$ of $x$, i.e. an element $y=(y_1,\dots,y_d)$ that differs from $x$ in at most one dimension (i.e. $y_i\neq x_i$ for at most one $i$). We denote as $N(x)$ the set of all neighbors of $x$. We then restrict functional form of the rate matrices to only allow for jumps to neighbors, i.e. $Q_t^\theta(y,x)=0$ if $y\notin N(x)$. One can then use a neural network $Q_t^\theta$ represented by the function
\begin{align*}
Q_t^\theta:S&\to(\mathbb{R}^{N-1})^d\\
x &\mapsto (Q_t^\theta(\tau,i|x))_{i=1,\dots,d,\tau\in\mathcal{T}\setminus\{x_i\}}
\end{align*}
i.e. the neural network is given by the function $x\mapsto Q_t^\theta(\cdot|x)$ that returns for very dimension $i$ a value for every token $\tau$ different from $x_i$. We then parameterize a rate matrix via
\begin{align}
\label{eq:q_t_neighbors}
    Q_t^\theta(y,x) = \begin{cases}
        0 & \text{if }y\notin N(x)\\
        Q_t^\theta(y^j,j|x)& \text{else if }y_j\neq x_j\\
        -\sum\limits_{i,\tau} Q_t^\theta(\tau,i|x)& \text{if }x=y\\
    \end{cases}
\end{align}
This parameterization is commonly used in the context of discrete markov models ("discrete diffusion models") \citep{campbell2022continuous,campbell2024generative}. With that, the operator $\kop_t^\theta$ in \cref{eq:proactivate_operator} becomes:
\begin{align*}
&\kop_t^\theta\rd_t(x)+\partial_tU_t(x)\\
=&\sum_{\substack{
  i = 1, \dots, d \\ 
  y \in N(x),\, y_i \neq x_i
}}\left[Q_t^\theta(y^i,i|x)-Q_t^\theta(x^i,i|y)\frac{\rd_t(y)}{\rd_t(x)}\right]
\end{align*}
The key problem with the above update is that it requires us to evaluate the neural network $|N(x)|$ times (for every neighbor $y$). This makes computing $\mathcal{K}_t^\theta$ computationally prohibitively expensive. Hence, \textbf{with the standard rate matrix parameterization, the proactive IS sampling scheme and training via the PINN-objective is very \emph{inefficient}}. 

To make the computation of $\kop_t^\theta$ efficient, we choose to induce an \textbf{inductive bias} into our neural network architecture to compute $\kop_t^\theta$ with no additional cost. Specifically, we introduce here the notion of \textbf{local equivariance}. A neural network $G^\theta_t$ represented by the function
\begin{align*}
    G_{t}^\theta: S &\to (\mathbb{R}^{N-1})^d\\
    x&\mapsto (G_{t}^\theta(\tau,i|x))_{i=1,\dots,d,\tau\in\mathcal{T}\setminus\{x_i\}}
\end{align*}
is called \textbf{locally equivariant} if the following condition holds:
\begin{align*}    G_{t}^\theta(\tau,i|x)=-G_{t}^\theta(x^i,i|\text{Swap}(x,i,\tau))\quad (i=1,\dots,d)\\
    \text{where}\quad \text{Swap}(x,i,\tau)=(x_1,\dots,x_{i-1},\tau,x_{i+1},\dots,x_d)
\end{align*}
In other words, the function $G_{t}^\theta$ gives the \say{flux of probability} going from $x$ to each neighbor. Local equivariance says that the flux from $x$ to its neighbor is negative the flux from the neighbor to $x$ (see \cref{fig:local_equivariance}).
% For simplicity, we focus on the case where $|\mathcal{T}|=2$. Specifically, we set $\mathcal{T}=\{-1,+1\}$ and $S=\{-1,+1\}^d$. In 
% \cref{appendix:local_equivariance_non_binary_case}, we explain how to extend the ideas presented here to the more general setting.
% Specifically, we introduce here the notion of \textbf{local equivariance}. A neural network $G^\theta_t=(F_{t,1}^\theta,\dots,F_{t,d}^\theta):\{+1,-1\}^d\to \mathbb{R}^{d}$ is called \textbf{locally equivariant} if
% \begin{align*}    F_{t,j}^\theta(\text{Flip}(x,j)) = -F_{t,j}^\theta(x)\quad (j=1,\dots,d)\\
%     \text{where}\quad \text{Flip}(x,j)=(x_1,\dots,x_{j-1},-x_j,x_{j+1},\dots,x_d)
% \end{align*}
% In other words, local equivariance implies that the $j$-th output flips its sign if the $j$-th input flips its sign. 

Therefore, every coordinate map $F_j$ is equivariant with respect to transformations of the $j$-th input (\say{locally} equivariant). Note that we do not specify how $F_i$ transforms for $i\neq j$ under transformations of $x_j$. This distinguishes it from "full" equivariance as, for example, used in geometric deep learning \citep{bronstein2021geometric,weiler2019general, thomas2018tensor}. We can use a locally equivariant neural network to parameterize a rate matrix via:
\begin{align}
\label{eq:local_equiv_rate_matrix}
    Q_t^\theta(\tau,j|x)=[G_{t}^\theta(\tau,j|x)]_{+}
\end{align}
where $[z]_{+}=\max(z,0)$ describes the ReLU operation. This representation is not a restriction (see \cref{appendix:proof_universal_representation_theorem} for a proof):
\begin{proposition}[Universal representation theorem]
\label{thm:universal_representation_theorem}
For any CTMC as in \cref{eq:q_t_neighbors} with marginals $\rd_t$, there is a corresponding CTMC with the same marginals $\rd_t$ and a rate matrix that can be written as in \cref{eq:local_equiv_rate_matrix} for a locally equivariant function $G_{t}^\theta$.
\end{proposition}
Crucially, this representation allows to efficiently compute $\kop_t^\theta$ in one forward pass of the neural network:
\begin{align*}
&\kop_t^\theta\rd_t(x)+\partial_tU_t(x)\\
=&\sum_{\substack{
  i = 1, \dots, d \\ 
  y \in N(x),\, y_i \neq x_i
}}\left[[G_{t}^\theta(y^i,i|x)]_{+}-[-G_{t}^\theta(y^i,i|x)]_{+}\frac{\rd_t(y)}{\rd_t(x)}\right]
\end{align*}
With this, we can efficiently compute the proactive IS update $A_t$ and evaluate the PINN-objective. Therefore, this construction allows for scalable training and efficient proactivate importance sampling. We call the resulting algorithm \textbf{LEAPS} (\textbf{L}ocally \textbf{E}quivariant discrete \textbf{A}nnealed \textbf{P}roactivate  \textbf{S}ampler). The acronym also highlights that we use a Markov \emph{jump} process to sample (i.e. that takes "leaps" through space).
% In \cref{alg:training_leaps} and \cref{alg:sampling_leaps} given in Appendix \ref{app:exp}, we summarize the training and sampling algorithm. 
\begin{remark}
It is important to note that with the above construction, we would need to naively evaluate $\rd_t(x)$ as often as $d$ times for a single computation of $\mathcal{K}_t^\theta$. However, note that the sum goes over all neighbors over $x$. Therefore, this can be a considered as computing a \emph{discrete gradient}. Such ratios can often be computed efficiently, e.g. for many scientific and physical models it is often only $2\times$ the computation compared to a single evaluation of $\rd_t(x)$.
\end{remark}
\begin{figure*}[ht]
% \vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=0.7\linewidth]{figures/LocallyEquivariantConvNet.pdf}}
\caption{Overview of locally equivariant convolutional neural network architecture.}
\label{fig:conv_net_architecture}
\end{center}
\vskip -0.4in
\end{figure*}

\section{Design of locally equivariant networks}
It remains to be stated how to construct locally equivariant neural networks - a question we turn to in this section. We will focus on three fundamental designs used throughout deep learning: Multilayer perceptrons (MLPs), attention layers, and convolutional neural networks. Usually, tokens are embedded as token vectors $e_\tau\in\mathbb{R}^{\cin}$ where $\cin$ is the embedding dimension. We therefore consider the embedded sequence of vectors: $x=(x_1,\dots,x_d)\in(\mathbb{R}^{\cin})^d$ as the input to the neural network.
% For simplicity, we assume that that $c=1$ and drop the dependency on time $t$ in this section. It is straight-forward to generalize the construction to $c>1$ and for time-dependent neural networks. Specifically, the input to the neural network is a vector $x\in \mathbb{R}^d$. 

% Local equivariance has  complexities different from "proper" equivariance in that we cannot trivially compose locally equivariant neural network layers. The first reason

% % Usually, tokens are embedded as token vectors $e_\tau\in\mathbb{R}^{c}$ where $c$ is the embedding dimension. Therefore, our goal is to construct neural networks $G_{t}^\theta:(\mathbb{R}^{c_1})^d\to(\mathbb{R}^{c_2})^d$ 


% % For simplicity, we assume that that $c=1$ and drop the dependency on time $t$ in this section. It is straight-forward to generalize the construction to $c>1$ and for time-dependent neural networks. Specifically, the input to the neural network is a vector $x\in \mathbb{R}^d$.


% Local equivariance is different from "proper" equivariance in that \textbf{one cannot trivially compose locally equivariant neural network layers}.



% Considering the full complexity of neural network architecture design goes beyond the scope of this work and we aim to provide 

% Local equivariance is different from "proper" equivariance in that \textbf{the composition of locally equivariant functions is not locally equivariant} in general. Therefore, we cannot simply compose locally equivariant neural network layers as we would do with "proper" equivariant neural networks.

% Therefore, we propose the following construction: We split the attention


% Usually, tokens are embedded as token vectors $e_\tau\in\mathbb{R}^{c}$ where $c$ is the embedding dimension. For simplicity, we assume that that $c=1$ and drop the dependency on time $t$ in this section. It is straight-forward to generalize the construction to $c>1$ and for time-dependent neural networks. Specifically, the input to the neural network is a vector $x\in \mathbb{R}^d$. We call a neural network layer $G_{t}^\theta:(\mathbb{R}^{c_1})^d\to(\mathbb{R}^{c_2})^d$ locally equivariant if 



% Local equivariance is different from "proper" equivariance in that \textbf{the composition of locally equivariant functions is not locally equivariant} in general. Therefore, we cannot simply compose locally equivariant neural network layers as we would do with "proper" equivariant neural networks. This fundamentally changes considerations about how to compose layers and how to construct \emph{deep} neural networks.


\textbf{Multilayer perceptron (MLP).} Let us set $\cin=1$ in this paragraph for readability. Let $W^1,\dots,W^k\in\mathbb{R}^{d\times d}$ be a set of weight matrices with a zero diagonal, i.e. $W_{ii}=0$ for $i=1,\dots,d$. Further, let $\sigma:\mathbb{R}\to\mathbb{R}$ be an activation function and $\omega_{\tau}\in\mathbb{R}^k$ be a learnable projection vector for every token $\tau\in\mathcal{T}$. Then define the map:
\begin{align*}
    G_{t}^\theta(\tau,j|x) =& \sum\limits_{i=1}^{k}(\omega_\tau^i-\omega_{x_j}^i)\sigma(W^ix)_j
\end{align*}
where $\sigma(W^ix)_j$ denotes the $j$-th element of the vector obtained by multiplying the vector $x$ with the matrix $W^i$ and applying the activation function $a$ componentwise. One can easily show that this is a locally equivariant neural network corresponding to a MLP with one hidden layer.

\textbf{Locally-equivariant attention (LEA) layer.} Let us consider a self-attention layer operating on keys $k_j=k_j(x_j)$, queries $q_j=q_j(x_j)$, and values $v_j=v_j(x_j)$ - each of which is a function of element $x_j$. We define the locally equivariant attention layer then as:
\begin{align*}
G_{t}^\theta(\tau,j|x)=(\omega_\tau-\omega_{x_j})^T\left[\sum\limits_{s\neq j}\frac{\exp(k_s^Tq_j)}{\sum\limits_{t\neq j}\exp(k_t^Tq_j)}v_s\right]
\end{align*}
It can be shown that this layer is locally equivariant if the queries $q_j$ are independent of the sign of $x_j$ (i.e. $q_j(x_j)=q_j(-x_j)$) which can be easily achieved. By stacking across multiple attention heads, one can create a locally equivariant MultiHeadAttention (LEA) with this.\\

\textbf{Hierchical local equivariance.} Local equivariance is different from \say{proper} equivariance in that \textbf{the composition of locally equivariant functions is not locally equivariant} in general. Therefore, we cannot simply compose locally equivariant neural network layers as we would do with \say{proper} equivariant neural networks. In particular, the above MLP and the attention layers cannot simply be composed as their composition would violate the local equivariance. This fundamentally changes considerations about how to compose layers and how to construct \emph{deep} neural networks. We will now illustrate this for the case of convolutional neural networks.

\textbf{Locally-equivariant convolutional (LEC) network.} To construct a locally equivariant convolutional neural network (LEC), we assume that our data lies on a grid. A convolutional layer is characterized by a matrix $W\in \mathbb{R}^{(2k-1)\times (2k-1)}$ and its operation is denoted via $k(W)*x$ where $k$ denotes the convolutional kernel with weights $W$. Here, we set the center of $W$ to zero: $W_{kk}=0$ (i.e. such that corresponding location is effectively ignored). To stack such layers, we can make the output of the previous layer feed into the \emph{weights} of the next layer:
\begin{align*}
    h_0 =& (1,\dots,1)^{T}\\
    W_i =& \sigma(A_ih_i+b_i)+c_j\quad &(i=1,\dots, L)\\
    h_{i+1} = & k_t(W_i) * x\quad &(i=1,\dots, L)\\
    H_t^\theta(x)=&h_{L}
\end{align*}
where $A_i\in \mathbb{R}^{d_i\times d_i}, b_i\in\mathbb{R}^{d_i}, c_i\in \mathbb{R}^{d_i}$ are learnable tensors which operate on each coordinate independently (i.e. a 1x1 convolution) and $\sigma:\mathbb{R}\to\mathbb{R}$ is an activation function to make it non-linear. We call the resulting network 
\begin{align*}
    H_t^\theta:(\mathbb{R}^{\cin})^d\to& (\mathbb{R}^{\cout})^d\\
    x\mapsto& (H_t^\theta(1|x),\dots, H_t^\theta(d|x))
\end{align*}
the \textbf{prediction head}. Combined with a small network $P_t^\theta:\mathcal{T}\to\mathbb{R}^k$ that we call \textbf{token projector}, we define the full neural network as
\begin{align*}
    G_{t}^\theta(\tau,j|x)=(P_t^\theta(e_\tau)-P_t^\theta(x_j))^TH_t^\theta(j|x)
\end{align*}
In \cref{appendix:conv_net_locally_equivariant}, we verify that $G_{t}^\theta$ defined in this way is locally equivariant. With this construction, one can stack deep highly complex convolutional neural networks. Note that this convolutional neural network has two (separate) symmetries: it is geometrically translation equivariant and locally equivariant in the sense defined in this work.
\begin{figure*}[ht]
\centering
    \vspace{-0.25cm}
    \includegraphics[width=0.9\linewidth]{figures/ising-stats.pdf}
    \vspace{-0.65cm}
    \caption{Performance metrics of LEAPS on the $L=15, J = 0.4, \beta = 0.7$ Ising model with the LEA and LEC architectures, using 100 annealing steps for sample generation. \textbf{Left:} Effective sample size of LEAPS samplers over training. Green area denotes an annealing phase during training where $t$ is increased from $0$ to $1$. Increasing the depth of LEC significantly improves performance. \textbf{Center:} Difference in the histograms of the magnetization $M(x)$ of configurations as compared to the ground truth set attained from a Glauber dynamics run of 25,000 steps, labeled as $M^*$. We denote by "no transport" the case of using  annealed dynamics with just the marginal preserving MCMC updates to show that the transport from $Q_t$ is essential in our construction. \textbf{Right:} Comparison of the 2-point correlation function for the LEA and LEC samplers against the Glauber dynamics ground truth.}
    \label{fig:ising}
\end{figure*}
% \begin{algorithm}[tb]
% \caption{LEAPS Sampling}
% \begin{algorithmic}[1]
% \State \textbf{Initialize:} $M$ CTMC walkers $\{X^m \}_{m=1}^M$, initial samples $\{X_0^m\}_{m=1^M} \sim \rho_0$, $A_0 = 0$, $N$ time steps, model $F_t^{\theta}$, replay buffer $\mathcal{B}$, local MCMC kernel $\mathcal M$.  
% \State \textbf{Make time grid}: $\mathbf{t} = \{t_0, t_1, \ldots, t_N\}$ where $t_0 = 0$ and $t_N = 1$.
% \For{$n = 0$ to $N-1$}
%  \State $h_n = t_{n+1} - t_n$, 
%     \For{$m = 1$ to $M$}    
%         % \State $X^m_{{n+1}} = X^m_{n}  + ...$ \msa{Wait for update formatting}
%         \State \textbf{Optional} local MCMC: $X^m_{t_{n}} = \mathcal M(X^m_{t_n})$
%         \vspace{0.05cm}
%         \State $X^m_{{t_{n}+h_{n}}} \sim (\mathbf{1}_{X_{t_n}=y}+h_nQ_t^\theta(y,X_{t_n}))_{y\in S}$ 
%         \vspace{0.05cm}
%         \State $A^m_{t_{n} + h_n} =A^m_{t_n}+h_n\mathcal{K}_{t_n}^\theta\rd_t(X^m_{t_n})$
%         \EndFor
%     \EndFor
%     \State \textbf{Optional:} Store sequences $\{(X_n^m, A_n^m, t_n)\}_{m,n}$  in $\mathcal{B}$.
% \end{algorithmic}
% \label{alg:samp}
% \end{algorithm}

\section{Related Work}
\label{sec:related_work}
\textbf{CTMCs.} CTMCs \cite{campbell2022continuous} have been used for various applications in generative modeling ("discrete diffusion" models), including text and image generation \cite{shi2024simplified,gatdiscrete,shaul2024flow, sahoo2024simple} and molecular design \cite{gruver2023protein, campbell2024generative, lisanza2024}. While here we use a RND for CTMCs running in \emph{reverse} time, one recovers the loss functions of these generative models considering a RND of two \emph{forward} time CTMCs (in \cref{sec:proof_rnds}, we show this in detail).

\textbf{Transport and sampling.}
Over the past decade there has been continued interest in combining the statistical guarantees of MCMC and IS with learning transport maps. A non-parametric version of this is described in \cite{marzouk2016}, and a parametric version through coupling-based normalizing flows was used to study systems in molecular dynamics and statistical physics \cite{noe2019, albergo2019, gabrie2022, wang2022}. These methods were extended to weave normalizing flows with SMC moves \cite{arbel2021, matthews2022}. More recent research focuses on replacing the generative model with a continuous flow or diffusion \cite{zhang2022path, vargas2023denoising, akhound2024iterated, sun2024}. Our method is inspired by approaches combining measure transport with MCMC schemes \cite{albergo2024nets, vargas2024transport} and other samplers relying on PINN-based objectives in continuous spaces \citep{mate2023learning, tian2024, sun2024}.

\textbf{Discrete Neural samplers.} The primary alternative to what we propose is to correct using importance weights arising from the estimate of the probability density computed using an autoregressive model \cite{nicoli2020}. However, the computational cost of producing samples in this case scales naively as $O(d)$, whereas we have no such constraint \textit{a priori} in our case so long as the error in the Euler sampling scheme is kept small. Other work focuses on discrete formulations of normalizing flows, but the performant version reduces to an autoregressive model \cite{tran2019}. Recent work has considered using CTMCs for sampling by parameterizing their evolution operators directly via tensor networks \citep{causer2025discrete} as opposed to neural network representations of rate matrices here. 

% \begin{itemize}
%     \item there's that one discrete flow paper but it won't work well i imagine
    % \item then there's some autorgressive ones like from \cite{nicoli2020}. We can argue that we don't necessarily require $O(L^2)$ sampling steps, as they would. 
% \end{itemize}
% In the context of sampling, it has already been useful to consider RNDs with respect to processes running in reverse time, e.g. as used in Annealed Importance Sampling (AIS) or in Euclidean space via path measures \citep{vargas2024transport}.

% \begin{figure*}[ht]
%     \includegraphics[width=\linewidth]{figures/ising-stats.pdf}
%     \vspace{-0.8cm}
%     \caption{Performance metrics of LEAPS on the $L=15, J = 0.4, \beta = 0.7$ Ising model with the LEA and LEC architectures, using 100 annealing steps for sample generation. \textbf{Left:} Effective sample size of LEAPS samplers over training. Increasing the depth of LEC significantly improves performance. \textbf{Center:} Difference in the histograms of the magnetization $M(x)$ of configurations as compared to the ground truth set attained from a Glauber dynamics run of 25,000 steps, labeled as $M^*$. We denote by "no transport" the case of using  annealed dynamics with just the marginal preserving MCMC updates to show that the transport from $Q_t$ is essential in our construction. \textbf{Right:} Comparison of the 2-point correlation function for the LEA and LEC samplers against the Glauber dynamics ground truth.}
%     \label{fig:ising}
% \end{figure*}
\section{Experiments}

% \subsection{Synthetic image distributions}
% \subsection{Ising Model}

As a demonstration of the validity of LEAPS in high dimensions, we sample the Gibbs distribution associated to a 2-dimensional Ising model. We choose the Ising model because it is a well-studied model. In particular, it is a solvable model, which allows us to construct a robust ground truth against which we can assess the various neural architectures underlying our algorithm. Configurations of the $L\times L$ Ising lattice follow the target distribution $\rho_1(x) = e^{-\beta H(x) + F_1}$ where $\beta$ is the inverse temperature of the system, $F_1$ the free energy, and $H(x): \{-1,1\}^{L\times L} \rightarrow \mathbb R$ is the Hamiltonian for the model defined as
\begin{align}
    H(x) = - J \sum_{\langle i,j\rangle} x_i x_j + \mu \sum_i x_i.
\end{align}
Here, $J$ is the interaction strength, $\langle i,j\rangle$ denotes summation over nearest neighbors of spins $x_i, x_j$ and $\mu$ is the magnetic moment. Neighboring spins are uncorrelated at high temperature but reach a critical correlation when the temperature drops behold a certain threshold. We use LEAPS to reproduce the statistics of the theory on a $15 \times 15$ lattice at parameters $\beta =0.7, J = 0.4$, which approach this threshold, and compare our results against a ground truth of long-run Glauber dynamics, an efficient algorithm for simulation in this parameter regime. Note that this corresponds to a $d=15\times 15 = 225$ dimensional space. To make $\rho_t$ time dependent, we make the parameters of $J_t$, $\mu_t$, $\beta_t$ linear functions of time, starting from the non-interacting case $J_0 = 0$. 

\textbf{Results.} We compare three different realizations of our method, one using LEA, and the other two using deep LEC that vary in depth. For all generated samples, we use 100 steps of integration of \cref{eq:gen:eq:euler}. We benchmark them on the effective sample size (ESS), which is a standard measure of how many effective samples our model gives according to the variance of the importance weights (see details Appendix \ref{app:exp}). In addition, we compute various observables using the Ising configurations generated by our model, such as histograms of the magnetization $M(x) = \sum_i x_i$ compared to ground truth, as well as the two point connected correlation function 
\begin{equation}
G_{\mathrm{conn}}(r)
= \mathbb{E}[x_i x_{i+r}]
- \mathbb{E}[x_i] \;\mathbb{E}[x_{i+r}].
\end{equation}
The latter is a measure of the dependency between spin values a distance $r$ in lattice separation.
In Figure \ref{fig:ising}, we show in the leftmost panel that the convolutional architecture outperforms the attention-based method, and the performance gap grows as we make the LEC network deeper. In the center panel, the difference in histograms of the magnetization of lattice configurations for our models as compared to ground truth samples is shown to be statistically zero, whereas relying on local MCMC alone for the same number of sampling steps (plotted in purple) illustrates that the dynamics have not converged. In the right plot, we see clear agreement between our learned sampler and the ground truth for the connected two point function. These results show that LEAPS can be an efficient simulator of complex, high dimensional target distributions.

\section{Discussion}
In this work, we present the LEAPS algorithm that allows to learn a non-equilibrium sampler via CTMCs parameterized by a locally equivariant neural network that is trained to minimize the variance of proactivate IS weights. A natural direction of future work will be to connect the ideas presented here with guidance or reward fine-tuning of generative CTMC models (discrete diffusion) - a problem known to be strongly tied to sampling. Further, LEAPS could easily be extended to sample across a whole family of distributions as opposed to only for a single, fixed target. Finally, we anticipate that the use of locally equivariant neural networks as well as the proactive IS scheme presented here might be useful more broadly for probabilistic models.


\newpage
\section{Impact Statement}
This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here.

\section{Acknowledgements}
We thank Eric Vanden-Eijnden for useful discussions. PH acknowledges support from the Machine Learning for Pharmaceutical Discovery and Synthesis (MLPDS) consortium, the DTRA Discovery of Medical Countermeasures Against New and Emerging (DOMANE) threats program, and the NSF Expeditions grant (award 1918839) Understanding the World Through Code. MSA is supported by a Junior Fellowship at the Harvard Society of Fellows as well as the National Science Foundation under Cooperative Agreement PHY-2019786 (The NSF AI Institute for Artificial Intelligence and Fundamental Interactions, http://iaifi.org/). 

\bibliography{main}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
\section{Proof of \cref{prop:rnd_forward_backward_ctmcs}}
\label{sec:proof_rnds}
Without loss of generality, we set the final time point to be $t=1$. We compute for a bounded continuous function $\Phi:\tspace\to \mathbb{R}$:
\begin{align*}
		&\mathbb{E}_{\X\sim \overrightarrow{\P}^{\mu, Q}}[\Phi(\X)]\\
		=&\lim\limits_{n\to\infty}\mathbb{E}_{\X\sim \overrightarrow{\P}^{\mu, Q}}
[\Phi(X_0,X_{1/n},X_{2/n},\dots, X_{\frac{n-1}{n}},X_1)]\\
=&\lim\limits_{n\to\infty}
		\E_{\X\sim \overleftarrow{\P}^{\nu, Q'}}\left[
\Phi(X_0,X_{1/n},X_{2/n},\dots, X_{\frac{n-1}{n}},X_1) \frac{\overrightarrow{\P}^{\mu, Q}(X_0,X_{1/n},\dots,X_{\frac{n-1}{n}},X_1)}{\overleftarrow{\P}^{\nu, Q'}(X_0,X_{1/n},X_{2/n},\dots, X_{\frac{n-1}{n}},X_1)}\right]\\
		=&\lim\limits_{n\to\infty}
		\mathbb{E}_{\X\sim \overleftarrow{\P}^{\nu, Q'}}\left[\Phi(X_0,X_{1/n},X_{2/n},\dots, X_{\frac{n-1}{n}},X_1)\frac{\mu(X_0)}{\nu(X_1)}\prod\limits_{s=0,1/n,2/n,\dots,\frac{n-1}{n}}\frac{\overrightarrow{\P}^{\mu, Q}(X_{s+h}|X_{s})}{\overleftarrow{\P}^{\nu, Q'}(X_{s}|X_{s+h})}\right]\\
		% =&\lim\limits_{n\to\infty}
		% \mathbb{E}_{\X\sim \overleftarrow{\P}^{\nu, Q'}}\left[\Phi(\X)\frac{\mu(X_0)}{\nu(X_1)}\prod\limits_{s,X_{s+h}=X_{s}}\frac{\exp(hQ_s(X_{s},X_s))}{\exp(hQ_{s+h}'(X_{s+h},X_{s+h}))}
		% \prod\limits_{s,X_{s+h}\neq X_{s}}\frac{\frac{Q_s(X_{s+h},X_s)}{|Q_s(X_{s},X_s)|}(1-\exp(hQ_s(X_s,X_s)))}{\frac{Q_{s+h}'(X_{s},X_{s+h})}{|Q_{s+h}'(X_{s+h},X_{s+h})|}(1-\exp(hQ_{s+h}'(X_{s+h},X_{s+h})))}
		% \right]\\
		=&\lim\limits_{n\to\infty}
		\mathbb{E}_{\X\sim \overleftarrow{\P}^{\nu, Q'}}\left[\Phi(\X)\frac{\mu(X_0)}{\nu(X_1)}
\exp\left(h\sum\limits_{s,X_{s+h}=X_{s}}Q_s(X_{s},X_s)-Q_{s+h}'(X_{s},X_{s})\right)
		\prod\limits_{s,X_{s+h}\neq X_{s}}\frac{Q_s(X_{s+h},X_s)}{Q_{s+h}'(X_{s},X_{s+h})}\right]
		\\
=&\mathbb{E}_{\X\sim \overleftarrow{\P}^{\nu, Q'}}\left[\Phi(\X)\frac{\mu(X_0)}{\nu(X_1)}
\exp\left(\int\limits_{0}^{1}Q_s(X_{s},X_s)-Q_{s}'(X_{s},X_{s})\dd s\right)
		\prod\limits_{s,X_{s^-}\neq X_{s}}\frac{Q_s(X_{s},X_{s^-})}{Q_{s}'(X_{s^-},X_{s})}\right]
		\\
	\end{align*}
where we used the definition of the rate matrix $Q_t,Q_t'$, the continuity of $Q_t'$ in $t$ and the fact that the left and right Riemann integral coincide. As $\Phi$ was arbitrary, the RND is given by:
\begin{align*}
\log\frac{\dd\overrightarrow{\P}^{\mu, Q}}{\dd\overleftarrow{\P}^{\nu, Q'}}(\X)=&\log(\mu(X_0))-\log(\nu(X_1))+\int\limits_{0}^{1}Q_s(X_s,X_s)-Q_s'(X_s,X_s)\dd s+
		\sum\limits_{s,X_{s}^{-}\neq X_{s}}
		\log\left(\frac{Q_s(X_{s},X_s^-)}{Q_s'(X_{s}^-,X_s)}\right)
\end{align*}



\section{Proof of \cref{thm:is_theorem}}
\label{appendix:proof_is_theorem}
Specifically, we use \cref{prop:rnd_forward_backward_ctmcs} to compute
	\begin{align*}
	\log\frac{\dd\overleftarrow{\P}^{\rd_t, \bar{Q}_t}}{\dd\overrightarrow{\P}^{\rd_0, Q_t}}(\X)=&\log(\rd_t(X_t))-\log(\rd_0(X_0))+\int\limits_{0}^{t}\bar{Q}_s(X_s,X_s)-Q_s(X_s,X_s)\dd s+
	\sum\limits_{s,X_{s}^{-}\neq X_{s}}
	\log\left(\frac{\bar{Q}_s(X_{s}^-,X_s)}{Q_s(X_{s},X_s^-)}\right)\\
=&F_t - F_0 - U_t(X_t)+U_0(X_0)+\int\limits_{0}^{t}\bar{Q}_s(X_s,X_s)-Q_s(X_s,X_s)\dd s+
\sum\limits_{s,X_{s}^{-}\neq X_{s}}
\log\left(\frac{\bar{Q}_s(X_{s}^-,X_s)}{Q_s(X_{s},X_s^-)}\right)
\end{align*}
Note that the function $t\mapsto U_t(X_t)$ is a piecewise differentiable function. Therefore, we can apply the fundamental theorem on every differentiable "piece" and get:
\begin{align*}
	U_t(X_t)-U_0(X_0) =& \int\limits_{0}^{t}\partial_sU_t(X_t)\dd s +\sum\limits_{s,X_{s}^{-}\neq X_{s}}U_s(X_s)-U_s(X_s^-)\\
=&\int\limits_{0}^{t}\partial_sU_s(X_s)\dd s+\sum\limits_{s,X_{s}^{-}\neq X_{s}}\log \frac{\rd_s(X_s^-)}{\rd_s(X_s)}
\end{align*}
Next, we can insert the above equation and get:
\begin{align*}
	&\log\frac{\dd\overleftarrow{\P}^{\rd_t, \bar{Q}_t}}{\dd\overrightarrow{\P}^{\rd_0, Q_t}}(\X)\\
=&F_t - F_0 - U_t(X_t)+U_0(Y_0)+\int\limits_{0}^{t}\bar{Q}_s(X_s,X_s)-Q_s(X_s,X_s)\dd s+
\sum\limits_{s,X_{s}^{-}\neq X_{s}}
\log\left(\frac{\bar{Q}_s(X_{s}^-,X_s)}{Q_s(X_{s},X_s^-)}\right)\\
=&F_t - F_0 -\int\limits_{0}^{t}\partial_sU_s(X_s)\dd s-\sum\limits_{s,X_{s}^{-}\neq X_{s}}\log \frac{\rd_s(X_s^-)}{\rd_s(X_s)}+\int\limits_{0}^{t}\bar{Q}_s(X_s,X_s)-Q_s(X_s,X_s)\dd s+
\sum\limits_{s,X_{s}^{-}\neq X_{s}}
\log\left(\frac{\bar{Q}_s(X_{s}^-,X_s)}{Q_s(X_{s},X_s^-)}\right)\\
=&F_t - F_0 -\int\limits_{0}^{t}\partial_sU_s(X_s)\dd s+\int\limits_{0}^{t}\bar{Q}_s(X_s,X_s)-Q_s(X_s,X_s)\dd s+
\sum\limits_{s,X_{s}^{-}\neq X_{s}}
\log\left(\underbrace{\frac{\bar{Q}_s(X_{s}^-,X_s)}{Q_s(X_{s},X_s^-)}\frac{\rd_s(X_s)}{\rd_s(X_s^-)}}_{=1}\right)\\
=&F_t - F_0 -\int\limits_{0}^{t}\partial_sU_s(X_s)\dd s+\int\limits_{0}^{t}-\sum\limits_{y\neq X_s}Q_s(X_s,y)\frac{\rd_t(y)}{\rd_t(X_s)}-Q_s(X_s,X_s)\dd s+0\\
=&F_t - F_0 +\left[-\int\limits_{0}^{t}\partial_sU_s(X_s)\dd s-\int\limits_{0}^{t}\sum\limits_{y\in S}Q_s(X_s,y)\frac{\rd_t(y)}{\rd_t(X_s)}\dd s\right]\\
=&F_t-F_0+A_t
\end{align*}
where we used the definition of $A_t$ in \cref{eq:A_t_definition} and the definition of $\bar{Q}_t$ in \cref{eq:Q_bar_def}. Note that for $h=1$, we get that
\begin{align*}
    1=\mathbb{E}_{x\sim \rd_t}[h(x)]=\mathbb{E}[\exp(A_t+F_t-F_0)]=\mathbb{E}[\exp(A_t)]\exp(F_t-F_0)
\end{align*}
because $F_t,F_0$ are constants. Therefore, in particular $\mathbb{E}[\exp(A_t)]=\exp(F_0-F_t)=Z_t/Z_0$. Note that we assume that $Z_0=1$ as we know $\rd_0$. Therefore, $\mathbb{E}[\exp(A_t)]=Z_t$. This proves \cref{eq:is_scheme_is_theorem}.

\section{Proof of \cref{thm:pinn_theorem}}
\label{appendix:proof_pinn_theorem}
We can use the variational formulation of the variance as the minimizer of the mean squared error to derive a computationally more efficient upper bound, i.e. we can re-express for every $0\leq t\leq 1$:
\begin{align*}
&\mathcal{L}^{\text{log-var}}(\theta;t)\\
=&\mathbb{V}_{\X\sim \mathbb{Q}}[A_t]\\
=&\min\limits_{\hat{F}_t\in \mathbb{R}}\mathbb{E}_{\X\sim\Q}[|A_t-\hat{F}_t|^2]\\
=&t^2\min\limits_{\partial_s\hat{F}_s\in \mathbb{R},0\leq s\leq t}\mathbb{E}_{\X\sim \Q}\left[|\frac{1}{t}\int\limits_{0}^{t} \kop_s^\theta \rd_s(X_s)-\partial_s\hat{F}_s\dd s|^2\right]\\
\leq &t^2\min\limits_{\partial_s\hat{F}_s\in \mathbb{R},0\leq s \leq t}\mathbb{E}_{\X\sim \mathbb{Q}}\left[\frac{1}{t}\int\limits_{0}^{t} |\kop_s^\theta \rd_s(X_s)-\partial_s\hat{F}_s|^2\dd s\right]\\
=&t^2\min\limits_{\partial_s\hat{F}_s\in \mathbb{R},0\leq s \leq t}\mathbb{E}_{s\sim\text{Unif}_{[0,1]}, X_s\sim \mathbb{Q}_s}\left[|\kop_s^\theta \rd_s(X_s)-\partial_s\hat{F}_s|^2\right]
\end{align*}
where we used Jensen's inequality and denote with $\mathbb{Q}_s$ the marginal of $\mathbb{Q}$ at time $s$. We now arrive at the result by replacing the above with the free energy network $F_t^\phi$. Further, note that the above bound is tight for $\mathbb{Q}$-almost every $\X$:
\begin{align*}
    \kop_s^\theta \rd_s(X_s)-\partial_sF_s=C(\X_{0:t})
\end{align*}
is a constant in time $s$. However, this constant may depend on $\X$. Further, note that 
\begin{align*}
    \mathbb{Q}_s(X_s) = |\kop_s^\theta\rho_s(X_s)-\partial_s F_s|^2
\end{align*}

\section{Proof of \cref{thm:universal_representation_theorem}}
\label{appendix:proof_universal_representation_theorem}

Before we prove the statement, we prove an auxiliary statement about one-way rate matrices. We call a rate matrix $Q_t$ a \textbf{one-way} rate matrix if 
\begin{align*}
	Q_t(y,x)\neq 0 \quad \Rightarrow Q_t(x,y)=0 \quad \text{ for all }x\neq y\\
	\Leftrightarrow \quad  Q_t(y,x)=0 \quad \text{or}\quad Q_t(x,y)=0 \quad \text{ for all }x\neq y
\end{align*}
Intuitively, a rate matrix $Q_t$ is a one-way rate matrix if we can always only go from $x\to y$ or from $y\to x$. The next proposition shows that there is no problem restricting ourselves to one-way rate matrices.
\begin{lemma}
	For every CTMC with rate matrix $Q_t$ and marginals $q_t$, there is a one-way rate matrix $\bar{Q}_t$ such that its corresponding CTMC $X_t$ has marginals $q_t$ if $X_0\sim q_0$ is initialized with the same initial distribution. Furhter, if $Q_t(y,x)=0$ for $y\neq x$, then also $\bar{Q}_t(y,x)=0$.
\end{lemma}
\begin{proof}
Let $Q_t$ be a rate matrix defining a CTMC with marginals $q_t$. Then
\begin{align*}
	\partial_t q_t(x) =& \sum\limits_{y\in S} Q_t(x,y) q_t(y)\\
	=&\sum\limits_{y\neq x} Q_t(x,y) q_t(y)-Q_t(y,x)q_t(x)\\
	=&\sum\limits_{y\neq x}\left[Q_t(x,y)-Q_t(y,x)\frac{q_t(x)}{q_t(y)}\right]q_t(y)\\
	=&\sum\limits_{y\neq x}\left[Q_t(x,y)-Q_t(y,x)\frac{q_t(x)}{q_t(y)}\right]_{+}q_t(y)-\left[Q_t(y,x)\frac{q_t(x)}{q_t(y)}-Q_t(x,y)\right]_{+}q_t(y)\\
	=&\sum\limits_{y\neq x}\left[Q_t(x,y)-Q_t(y,x)\frac{q_t(x)}{q_t(y)}\right]_{+}q_t(y)-\left[Q_t(y,x)-Q_t(x,y)\frac{q_t(y)}{q_t(x)}\right]_{+}q_t(x)\\
	=&\sum\limits_{y\neq x}\bar{Q}_t(x,y)q_t(y)-\bar{Q}_t(y,x)q_t(x)\\
	=&\sum\limits_{y\in S}\bar{Q}_t(x,y)q_t(y)
\end{align*}
where we defined
\begin{align*}
	\bar{Q}_t(y,x) = \begin{cases}
		\left[Q_t(y,x)-Q_t(x,y)\frac{q_t(y)}{q_t(x)}\right]_{+} & y\neq x \\
		-\sum\limits_{z\neq x} Q_t(z,x) &  y=x
	\end{cases}
\end{align*}
Note that
\begin{align*}
	\bar{Q}_t(y,x)>&0\\
	\Rightarrow \quad Q_t(y,x) > & Q_t(x,y)\frac{q_t(y)}{q_t(x)}\\
	\Rightarrow \quad Q_t(y,x) \frac{q_t(x)}{q_t(y)} > & Q_t(x,y)\\
	\Rightarrow \quad \left[Q_t(x,y)-Q_t(y,x) \frac{q_t(x)}{q_t(y)}\right]_{+} = & 0\\
	\Rightarrow \bar{Q}_t(x,y) =& 0
\end{align*}
Therefore, we learn that $\bar{Q}_t$ fulfils the desired condition and fulfils the KFE. Therefore, we have proved that we can swap out $Q_t$ for $\bar{Q}_t$ and we will have an CTMC with the same marginals. 
\end{proof}
Now, let us return to the proof of \cref{thm:universal_representation_theorem}. Given a rate matrix $Q_t$, we can now use a one-way rate matrix $\bar{Q}_t$ with the same marginals and define function:
\begin{align*}
    F_t(\tau,i|x) = \begin{cases}
        \bar{Q}_t(y,x) & \text{if }Q_t(y,x)>0\\
        -\bar{Q}_t(x,y) & \text{otherwise}
    \end{cases}\quad \text{ where }y=\text{Swap}(x,i,\tau)
\end{align*}
By construction, it holds that $F_t(\tau,i|x)$ is locally equivariant and that $[F_t(\tau,i|x)]_{+}=\bar{Q}_t(y,x)$. This finishes the proof.

\section{Local equivariance of ConvNet} 
\label{appendix:conv_net_locally_equivariant}
To verify the local equivariance, one can compute
\begin{align*}
    G_{t}^\theta(\tau,i|x)=&(P_t^\theta(e_\tau)-P_t^\theta(x_i))^TH_t^\theta(i|x)\\
    =&-(P_t^\theta(x_i)-P_t^\theta(e_\tau))^TH_t^\theta(i|x)\\
    =&-(P_t^\theta(x_i)-P_t^\theta(e_\tau))^TH_t^\theta(i|\text{Swap}(x,i,\tau))\\
    =&-F_t(x^i,i|\text{Swap}(x,i,\tau)),
\end{align*}
where we have used the invariance of the projection head $H_t^\theta(i|x)$ to changes in the $i$-th dimension. This shows the local equivariance. 

\section{Recovering loss functions for CTMC models via RNDs}
We discuss here in more detail how the Radon-Nikodym derivatives (RNDs) presented in \cref{prop:rnd_forward_backward_ctmcs} relate to the construction of loss function for CTMC generative models, also called "discrete diffusion" models. The connection lies in the fact that the loss function of these models relies on RNDs of two CTMCs running both in forward time. We can prove the following statement:
\begin{proposition}
\label{prop:rnd_ctmcs}
Let $\mu,\nu$ be two initial distributions over $S$. Let $Q_t,Q_t'$ be two rate matrices. Then the Radon-Nikodym derivative of the corresponding path distributions in forward time over the interval $[0,t]$ is given by:
\begin{align*}
&\log\frac{\dd\overrightarrow{\P}^{\mu, Q}}{\dd\overrightarrow{\P}^{\nu, Q'}}(\X)\\
=&\log\frac{\dd \mu}{\dd \nu}(X_0)+\int\limits_{0}^{t}Q_s(X_s,X_s)-Q_s'(X_s,X_s)ds\\
&+
\sum\limits_{s,X_{s}^{-}\neq X_{s}}
\log\left(\frac{Q_s(X_{s},X_s^-)}{Q_s'(X_{s},X_s^-)}\right)
\end{align*}
where we sum over all points where $X_s$ jumps in the last term.
\end{proposition}
The proof of the above formula is very similar to the proof of \cref{prop:rnd_forward_backward_ctmcs} and an analogous formula also appeared in \citep{campbell2024generative}, for example. The above proposition allows us to by compute the KL-divergence:
\begin{align*}
&D_{KL}(\overrightarrow{\P}^{\mu, Q}_1||\overrightarrow{\P}^{\nu, Q'}_1)\\
\leq &D_{KL}(\overrightarrow{\P}^{\mu, Q}||\overrightarrow{\P}^{\nu, Q'})\\
=&\mathbb{E}_{\X\sim\overrightarrow{\P}^{\mu, Q}}\left[\log\frac{\dd\overrightarrow{\P}^{\mu, Q}}{\dd\overrightarrow{\P}^{\nu, Q'}}(\X)\right]\\
=&D_{KL}(\mu||\nu)+\mathbb{E}_{\X\sim\overrightarrow{\P}^{\mu, Q}}\left[\int\limits_{0}^{1}Q_t(X_t,X_t)-Q_t'(X_t,X_t)dt+
\sum\limits_{t,X_{t}^{-}\neq X_{t}}
\log\left(\frac{Q_t(X_{t},X_t^-)}{Q_t'(X_{t},X_t^-)}\right)\right]\\
=&D_{KL}(\mu||\nu)+\mathbb{E}_{t\sim\text{Unif}_{[0,1]},x_t\sim \overrightarrow{\P}^{\mu, Q}_t}\left[Q_t(X_t,X_t)-Q_t'(X_t,X_t)\right]\\&
+\mathbb{E}_{\X\sim\overrightarrow{\P}^{\mu, Q}}\left[\sum\limits_{t,X_{t}^{-}\neq X_{t}}
\log\left(\frac{Q_t(X_{t},X_t^-)}{Q_t'(X_{t},X_t^-)}\right)\right]\\
=&D_{KL}(\mu||\nu)+\mathbb{E}_{t\sim\text{Unif}_{[0,1]},x_t\sim \overrightarrow{\P}^{\mu, Q}_t}\left[Q_t(X_t,X_t)-Q_t'(X_t,X_t)\right]\\&
+\int\limits_{0}^{1}
\mathbb{E}_{X_t\sim\overrightarrow{\P}^{\mu, Q}_t}\left[\sum\limits_{y\neq X_t}Q_t(y;X_t)\log\left(\frac{Q_t(y;X_t)}{Q_t'(y,X_t)}\right)\right]\dd t\\
=&D_{KL}(\mu||\nu)+\mathbb{E}_{t\sim\text{Unif}_{[0,1]},X_t\sim \overrightarrow{\P}^{\mu, Q}_t}\left[
\sum\limits_{y\neq X_t}Q_t'(y,X_t)-Q_t(y,X_t)+Q_t(y;X_t)\log\left(\frac{Q_t(y;X_t)}{Q_t'(y,X_t)}\right)\right]
\end{align*}
where we have used the data processing inequality in the first term. Having a parameterized model $Q_t'=Q_t^\theta$, this leads to the following loss:
\begin{align*}
L(\theta)=&D_{KL}\left(\overrightarrow{\P}^{\mu, Q}||\overrightarrow{\P}^{\mu, Q_t^\theta}\right)\\
=&D_{KL}(\mu||\nu)+\mathbb{E}_{t\sim\text{Unif}_{[0,1]},X_t\sim \overrightarrow{\P}^{\mu, Q}_t}\left[
\sum\limits_{y\neq X_t}Q_t^\theta(y,X_t)-Q_t(y,X_t)\log\left(Q_t^\theta(y,X_t)\right)\right]+C
\end{align*}
where $Q_t$ is some reference process. The above recovers loss functions in the context of CTMC and jump generative models \citep{campbell2022continuous, gatdiscrete, shaul2024flow, campbell2024generative} and Euclidean jump models \citep[Section D.1.]{holderrieth2024generator}. Note that the above loss cannot be used for the purposes of sampling in a straight-forward manner because we do not have access to samples from the marginals of the ground  reference $\overrightarrow{\P}^{\mu, Q}$.




\section{Numerical experiments}
\label{app:exp}

% \begin{algorithm}[tb]
%    \caption{LEAPS training}
%    \label{alg:training_leaps}
% \begin{algorithmic}
%    \STATE {\bfseries Input:} data $x_i$, size $m$
%    \REPEAT
%    \STATE Initialize $noChange = true$.
%    \FOR{$i=1$ {\bfseries to} $m-1$}
%    \IF{$x_i > x_{i+1}$}
%    \STATE Swap $x_i$ and $x_{i+1}$
%    \STATE $noChange = false$
%    \ENDIF
%    \ENDFOR
%    \UNTIL{$noChange$ is $true$}
% \end{algorithmic}
% \end{algorithm}

% \begin{algorithm}[tb]
%    \caption{LEAPS sampling}
%    \label{alg:sampling_leaps}
% \begin{algorithmic}
%    \STATE {\bfseries Input:} data $x_i$, size $m$
%    \REPEAT
%    \STATE Initialize $noChange = true$.
%    \FOR{$i=1$ {\bfseries to} $m-1$}
%    \IF{$x_i > x_{i+1}$}
%    \STATE Swap $x_i$ and $x_{i+1}$
%    \STATE $noChange = false$
%    \ENDIF
%    \ENDFOR
%    \UNTIL{$noChange$ is $true$}
% \end{algorithmic}
% \end{algorithm}

\paragraph{Effective Sample Size} We use the self-normalized definition of the effective sample size such that, given the log weights $A_t$ associated to $N$ CTMC instances, the ESS at time $t$ in the generation is given by:

\begin{equation}
\label{eq:ess}
    \operatorname{ESS}_t=\frac{\left(N^{-1} \sum_{t=1}^N \exp \left(A_t^i\right)\right)^2}{N^{-1} \sum_{i=1}^N \exp \left(2 A_i^i\right)}
\end{equation}


\subsection{Ising model experiments}

Here we provide details of the numerical implementation of our study of the $L=15$ Ising model. For the locally equivariant attention (LEA) mechanism, we use 40 attention heads, each with query, key, and value matrices of dimension 40x40. As such, there are about 350,000 parameters in the model. In addition, the locally equivariant convolutional net (LEC) of depth three uses kernel sizes of  [5, 7, 15], while the depth five version uses [3, 5, 7, 9, 15], amounting to around 100,000 parameters. 

\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
