@article{akhound2024iterated,
  title={Iterated denoising energy matching for sampling from Boltzmann densities},
  author={Akhound-Sadegh, Tara and Rector-Brooks, Jarrid and Bose, Avishek Joey and Mittal, Sarthak and Lemos, Pablo and Liu, Cheng-Hao and Sendera, Marcin and Ravanbakhsh, Siamak and Gidel, Gauthier and Bengio, Yoshua and others},
  journal={arXiv preprint arXiv:2402.06121},
  year={2024}
}

@article{albergo2019,
  title = {Flow-based generative models for Markov chain Monte Carlo in lattice field theory},
  author = {Albergo, M. S. and Kanwar, G. and Shanahan, P. E.},
  journal = {Phys. Rev. D},
  volume = {100},
  issue = {3},
  pages = {034515},
  numpages = {13},
  year = {2019},
  month = {Aug},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevD.100.034515},
  url = {https://link.aps.org/doi/10.1103/PhysRevD.100.034515}
}

@article{albergo2024nets,
  title={NETS: A Non-Equilibrium Transport Sampler},
  author={Albergo, Michael S and Vanden-Eijnden, Eric},
  journal={arXiv preprint arXiv:2410.02711},
  year={2024}
}

@InProceedings{arbel2021,
  title={Annealed Flow Transport Monte Carlo},
  author={Michael Arbel and Alexander G. D. G. Matthews and Arnaud Doucet},
  booktitle = {Proceedings of the 38th International Conference on Machine Learning},
  series = {Proceedings of Machine Learning Research},
  year={2021},
  month = {18--24 Jul}
}

@article{campbell2022continuous,
  title={A continuous time framework for discrete denoising models},
  author={Campbell, Andrew and Benton, Joe and De Bortoli, Valentin and Rainforth, Thomas and Deligiannidis, George and Doucet, Arnaud},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={28266--28279},
  year={2022}
}

@article{campbell2024generative,
  title={Generative flows on discrete state-spaces: Enabling multimodal flows with applications to protein co-design},
  author={Campbell, Andrew and Yim, Jason and Barzilay, Regina and Rainforth, Tom and Jaakkola, Tommi},
  journal={arXiv preprint arXiv:2402.04997},
  year={2024}
}

@article{causer2025discrete,
  title={Discrete generative diffusion models without stochastic differential equations: a tensor network approach},
  author={Causer, Luke and Rotskoff, Grant M and Garrahan, Juan P},
  journal={Physical Review E},
  volume={111},
  number={2},
  pages={025302},
  year={2025},
  publisher={APS}
}

@article{gatdiscrete,
  title={Discrete flow matching},
  author={Gat, Itai and Remez, Tal and Shaul, Neta and Kreuk, Felix and Chen, Ricky TQ and Synnaeve, Gabriel and Adi, Yossi and Lipman, Yaron},
  journal={Advances in Neural Information Processing Systems},
  volume={37},
  pages={133345--133385},
  year={2025}
}

@article{lisanza2024,
	abstract = {Protein denoising diffusion probabilistic models are used for the de novo generation of protein backbones but are limited in their ability to guide generation of proteins with sequence-specific attributes and functional properties. To overcome this limitation, we developed ProteinGenerator (PG), a sequence space diffusion model based on RoseTTAFold that simultaneously generates protein sequences and structures. Beginning from a noised sequence representation, PG generates sequence and structure pairs by iterative denoising, guided by desired sequence and structural protein attributes. We designed thermostable proteins with varying amino acid compositions and internal sequence repeats and cage bioactive peptides, such as melittin. By averaging sequence logits between diffusion trajectories with distinct structural constraints, we designed multistate parent--child protein triples in which the same sequence folds to different supersecondary structures when intact in the parent versus split into two child domains. PG design trajectories can be guided by experimental sequence--activity data, providing a general approach for integrated computational and experimental optimization of protein function.},
	author = {Lisanza, Sidney Lyayuga and Gershon, Jacob Merle and Tipps, Samuel W. K. and Sims, Jeremiah Nelson and Arnoldt, Lucas and Hendel, Samuel J. and Simma, Miriam K. and Liu, Ge and Yase, Muna and Wu, Hongwei and Tharp, Claire D. and Li, Xinting and Kang, Alex and Brackenbrough, Evans and Bera, Asim K. and Gerben, Stacey and Wittmann, Bruce J. and McShan, Andrew C. and Baker, David},
	date = {2024/09/25},
	date-added = {2025-01-30 20:41:37 -0500},
	date-modified = {2025-01-30 20:41:37 -0500},
	doi = {10.1038/s41587-024-02395-w},
	id = {Lisanza2024},
	isbn = {1546-1696},
	journal = {Nature Biotechnology},
	title = {Multistate and functional protein design using RoseTTAFold sequence space diffusion},
	url = {https://doi.org/10.1038/s41587-024-02395-w},
	year = {2024},
	bdsk-url-1 = {https://doi.org/10.1038/s41587-024-02395-w}}

@article{marzouk2016,
  title={Sampling via measure transport: An introduction},
  author={Marzouk, Youssef and Moselhy, Tarek and Parno, Matthew and Spantini, Alessio},
  journal={Handbook of uncertainty quantification},
  volume={1},
  pages={2},
  year={2016},
  publisher={Springer Cham}
}

@misc{mate2023learning,
      title={Learning Interpolations between Boltzmann Densities}, 
      author={Bálint Máté and François Fleuret},
      year={2023},
      eprint={2301.07388},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/2301.07388}, 
}

@InProceedings{matthews2022,
  title = 	 {Continual Repeated Annealed Flow Transport {M}onte {C}arlo},
  author =       {Matthews, Alex and Arbel, Michael and Rezende, Danilo Jimenez and Doucet, Arnaud},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {15196--15219},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/matthews22a/matthews22a.pdf},
  url = 	 {https://proceedings.mlr.press/v162/matthews22a.html},
  abstract = 	 {We propose Continual Repeated Annealed Flow Transport Monte Carlo (CRAFT), a method that combines a sequential Monte Carlo (SMC) sampler (itself a generalization of Annealed Importance Sampling) with variational inference using normalizing flows. The normalizing flows are directly trained to transport between annealing temperatures using a KL divergence for each transition. This optimization objective is itself estimated using the normalizing flow/SMC approximation. We show conceptually and using multiple empirical examples that CRAFT improves on Annealed Flow Transport Monte Carlo (Arbel et al., 2021), on which it builds and also on Markov chain Monte Carlo (MCMC) based Stochastic Normalizing Flows (Wu et al., 2020). By incorporating CRAFT within particle MCMC, we show that such learnt samplers can achieve impressively accurate results on a challenging lattice field theory example.}
}


article{BIALAS2022108502,
title = {Hierarchical autoregressive neural networks for statistical systems},
journal = {Computer Physics Communications},
volume = {281},
pages = {108502},
year = {2022},
issn = {0010-4655},
doi = {https://doi.org/10.1016/j.cpc.2022.108502},
url = {https://www.sciencedirect.com/science/article/pii/S0010465522002211},
author = {Piotr Białas and Piotr Korcyl and Tomasz Stebel},
keywords = {Variational autoregressive neural networks, Hierarchical neural networks, Spin systems, Ising model, Markov Chain Monte Carlo},
abstract = {It was recently proposed that neural networks could be used to approximate many-dimensional probability distributions that appear e.g. in lattice field theories or statistical mechanics. Subsequently they can be used as variational approximators to assess extensive properties of statistical systems, like free energy, and also as neural samplers used in Monte Carlo simulations. The practical application of this approach is unfortunately limited by its unfavourable scaling both of the numerical cost required for training, and the memory requirements with the system size. This is due to the fact that the original proposition involved a neural network of width which scaled with the total number of degrees of freedom, e.g. L2 in case of a two dimensional L×L lattice. In this work we propose a hierarchical association of physical degrees of freedom, for instance spins, to neurons which replaces it with the scaling with the linear extent L of the system. We demonstrate our approach on the two-dimensional Ising model by simulating lattices of various sizes up to 128×128 spins, with time benchmarks reaching lattices of size 512×512. We observe that our proposal improves the quality of neural network training, i.e. the approximated probability distribution is closer to the target that could be previously achieved. As a consequence, the variational free energy reaches a value closer to its theoretical expectation and, if applied in a Markov Chain Monte Carlo algorithm, the resulting autocorrelation time is smaller. Finally, the replacement of a single neural network by a hierarchy of smaller networks considerably reduces the memory requirements.}
}

@article{nicoli2020,
  title = {Asymptotically unbiased estimation of physical observables with neural samplers},
  author = {Nicoli, Kim A. and Nakajima, Shinichi and Strodthoff, Nils and Samek, Wojciech and M\"uller, Klaus-Robert and Kessel, Pan},
  journal = {Phys. Rev. E},
  volume = {101},
  issue = {2},
  pages = {023304},
  numpages = {10},
  year = {2020},
  month = {Feb},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevE.101.023304},
  url = {https://link.aps.org/doi/10.1103/PhysRevE.101.023304}
}

@article{noe2019,
author = {Frank Noé  and Simon Olsson  and Jonas Köhler  and Hao Wu },
title = {Boltzmann generators: Sampling equilibrium states of many-body systems with deep learning},
journal = {Science},
volume = {365},
number = {6457},
pages = {eaaw1147},
year = {2019},
doi = {10.1126/science.aaw1147},
URL = {https://www.science.org/doi/abs/10.1126/science.aaw1147},
eprint = {https://www.science.org/doi/pdf/10.1126/science.aaw1147},
abstract = {Molecular dynamics or Monte Carlo methods can be used to sample equilibrium states, but these methods become computationally expensive for complex systems, where the transition from one equilibrium state to another may only occur through rare events. Noé et al. used neural networks and deep learning to generate distributions of independent soft condensed-matter samples at equilibrium (see the Perspective by Tuckerman). Supervised training is used to construct invertible transformations between the coordinates of the complex system of interest and simple Gaussian coordinates of the same dimensionality. Thus, configurations can be sampled in this simpler coordinate system and then transformed back into the complex one using the correct statistical weighting. Science, this issue p. eaaw1147; see also p. 982 By combining deep learning and statistical mechanics, neural networks sample the equilibrium distribution of many-body systems. Computing equilibrium states in condensed-matter many-body systems, such as solvated proteins, is a long-standing challenge. Lacking methods for generating statistically independent equilibrium samples in “one shot,” vast computational effort is invested for simulating these systems in small steps, e.g., using molecular dynamics. Combining deep learning and statistical mechanics, we developed Boltzmann generators, which are shown to generate unbiased one-shot equilibrium samples of representative condensed-matter systems and proteins. Boltzmann generators use neural networks to learn a coordinate transformation of the complex configurational equilibrium distribution to a distribution that can be easily sampled. Accurate computation of free-energy differences and discovery of new configurations are demonstrated, providing a statistical mechanics tool that can avoid rare events during sampling without prior knowledge of reaction coordinates.}}

@article{shaul2024flow,
  title={Flow Matching with General Discrete Paths: A Kinetic-Optimal Perspective},
  author={Shaul, Neta and Gat, Itai and Havasi, Marton and Severo, Daniel and Sriram, Anuroop and Holderrieth, Peter and Karrer, Brian and Lipman, Yaron and Chen, Ricky TQ},
  journal={arXiv preprint arXiv:2412.03487},
  year={2024}
}

@misc{sun2024,
      title={Dynamical Measure Transport and Neural PDE Solvers for Sampling}, 
      author={Jingtong Sun and Julius Berner and Lorenz Richter and Marius Zeinhofer and Johannes Müller and Kamyar Azizzadenesheli and Anima Anandkumar},
      year={2024},
      eprint={2407.07873},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2407.07873}, 
}

@InProceedings{tian2024,
  title = 	 {Liouville Flow Importance Sampler},
  author =       {Tian, Yifeng and Panda, Nishant and Lin, Yen Ting},
  booktitle = 	 {Proceedings of the 41st International Conference on Machine Learning},
  pages = 	 {48186--48210},
  year = 	 {2024},
  editor = 	 {Salakhutdinov, Ruslan and Kolter, Zico and Heller, Katherine and Weller, Adrian and Oliver, Nuria and Scarlett, Jonathan and Berkenkamp, Felix},
  volume = 	 {235},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {21--27 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://raw.githubusercontent.com/mlresearch/v235/main/assets/tian24c/tian24c.pdf},
  url = 	 {https://proceedings.mlr.press/v235/tian24c.html},
  abstract = 	 {We present the Liouville Flow Importance Sampler (LFIS), an innovative flow-based model for generating samples from unnormalized density functions. LFIS learns a time-dependent velocity field that deterministically transports samples from a simple initial distribution to a complex target distribution, guided by a prescribed path of annealed distributions. The training of LFIS utilizes a unique method that enforces the structure of a derived partial differential equation to neural networks modeling velocity fields. By considering the neural velocity field as an importance sampler, sample weights can be computed through accumulating errors along the sample trajectories driven by neural velocity fields, ensuring unbiased and consistent estimation of statistical quantities. We demonstrate the effectiveness of LFIS through its application to a range of benchmark problems, on many of which LFIS achieved state-of-the-art performance.}
}

@misc{tran2019,
      title={Discrete Flows: Invertible Generative Models of Discrete Data}, 
      author={Dustin Tran and Keyon Vafa and Kumar Krishna Agrawal and Laurent Dinh and Ben Poole},
      year={2019},
      eprint={1905.10347},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1905.10347}, 
}

@inproceedings{vargas2024transport,
  title={Transport meets variational inference: Controlled Monte Carlo diffusions},
  author={Vargas, Francisco and Padhy, Shreyas and Blessing, Denis and Nusken, Nikolas},
  booktitle={The Twelfth International Conference on Learning Representations: ICLR 2024},
  year={2024}
}

@article{wang2022,
author = {Yihang Wang  and Lukas Herron  and Pratyush Tiwary },
title = {From data to noise to data for mixing physics across temperatures with generative artificial intelligence},
journal = {Proceedings of the National Academy of Sciences},
volume = {119},
number = {32},
pages = {e2203656119},
year = {2022},
doi = {10.1073/pnas.2203656119},
URL = {https://www.pnas.org/doi/abs/10.1073/pnas.2203656119},
eprint = {https://www.pnas.org/doi/pdf/10.1073/pnas.2203656119},
abstract = {While it is tempting to use high-temperature simulations to infer observations about low temperature, it is not always clear how to do so. Here we demonstrate how using generative artificial intelligence we can mix information from simulations conducted at a set of temperatures and generate molecular configurations at any temperature of interest including temperatures at which simulations were never performed. The configurations we generate carry correct Boltzmann weights, and our model minimizes the generation of spurious unphysical configurations. We demonstrate its use here through combining with replica exchange molecular dynamics in a postprocessing framework for sampling peptide and ribonucleic acid. We believe the framework is extensible to generic simulations and experiments for mixing control parameters other than temperature. Using simulations or experiments performed at some set of temperatures to learn about the physics or chemistry at some other arbitrary temperature is a problem of immense practical and theoretical relevance. Here we develop a framework based on statistical mechanics and generative artificial intelligence that allows solving this problem. Specifically, we work with denoising diffusion probabilistic models and show how these models in combination with replica exchange molecular dynamics achieve superior sampling of the biomolecular energy landscape at temperatures that were never simulated without assuming any particular slow degrees of freedom. The key idea is to treat the temperature as a fluctuating random variable and not a control parameter as is usually done. This allows us to directly sample from the joint probability distribution in configuration and temperature space. The results here are demonstrated for a chirally symmetric peptide and single-strand RNA undergoing conformational transitions in all-atom water. We demonstrate how we can discover transition states and metastable states that were previously unseen at the temperature of interest and even bypass the need to perform further simulations for a wide range of temperatures. At the same time, any unphysical states are easily identifiable through very low Boltzmann weights. The procedure while shown here for a class of molecular simulations should be more generally applicable to mixing information across simulations and experiments with varying control parameters.}}

