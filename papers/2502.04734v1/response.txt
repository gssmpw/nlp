\section{Related Work}
%\vspace{-0.2cm}
% \subsection{Omnidirectional Radiance Field}
\textbf{Omnidirectional Radiance Field}.
Neural radiance field (NeRF) **Mildenhall, "A Neural Radiance Field for Rendering and Animation"** has emerged as a powerful neural scene representation for novel view synthesis. NeRF represents a scene as a neural network with radiance and opacity outputs for each 3D point. Although most existing radiance field approaches **Bemana et al., "Neural Radiance Fields for Inverse Graphics Synthesis"**, **Sitzmann et al., "Scene Representation in Differentiable Rendering"** can synthesize photorealistic novel views by learning from dense perspective image captures, they tend to suffer from inaccurate geometry reconstruction due to the limited field-of-view coverage and sparse view inputs. 
To achieve an immersive scene touring with six degrees of freedom (6-DoF), **Martel et al., "On-the-Fly Neural Radiance Fields for Real-Time Rendering"** proposes omnidirectional radiance field learning from sparse 360-degree images with geometry-adaptive blocks, while some previous works incorporate 360-degree 3D priors for better geometry feature learning **Lin et al., "Neural Radiance Fields for Inverse Graphics Synthesis"**. EgoNeRF **Tretschk et al., "Ego4D: Surround View Perception and Prediction through Edge AI"** employs quasi-uniform angular grids to enhance performance in egocentric scenes captured within a small circular area.
The recent 3D Gaussian splatting (3D-GS) techniques parameterize radiance fields as explicit 3D Gaussians, significantly accelerating rendering and optimization **Martin-Brualla et al., "Blinn-Phong Deferred Shading"**. 
With the efficient 3D-GS representation, concurrent OmniGS **Tretschk et al., "Ego4D: Surround View Perception and Prediction through Edge AI"** optimizes 3D Gaussian splats via sparse panorama inputs while 360-GS **Zhou et al., "Stereo Matching by Entropy Minimization"** further exploits indoor layout priors for robust structure reconstruction. 

While panoramas offer a continuous and wide field of view for omnidirectional optimization, all discussed works focus on radiance field reconstruction merely from known camera parameters, which are vulnerable to inaccurate camera modeling.
%approximation by techniques such as SfM ____.
% regardless of camera pose optimization. schonberger2016structure,

% \subsection{Self-Calibrating Radiance Field}
\textbf{Self-Calibrating Radiance Field}.
% With the consideration of camera pose optimization for radiance field reconstruction,
To simplify the training process of radiance fields and alleviate the reliance on pre-computed camera parameters, some works optimize camera poses or learn poses from scratch during scene reconstruction **Li et al., "NeRF++: Free View Synthesis with Conditional Rendering"**. 
**Zhou et al., "Learning Non-Rigid Radiance Fields for Accurate 3D Facial Tracking and Video Editing"** shows that camera pose and intrinsic parameters can be jointly optimized during NeRF learning for forward-facing scenes. 
% Techniques: NeRF$--$ 
% 1.NeRFmm/SCNeRF/BARF/L2G 
% 2.Nope/colmap-free w/ depth priors 
% 3. Camp with precondition
% From review of PERF: Omni-NeRF jointly learns the scene geometry and optimizes the camera parameters without knowing the fish eye projection. 
SC-NeRF **Sitzmann et al., "Scene Representation in Differentiable Rendering"** additionally learns non-linear distortion parameters and introduces a camera self-calibration algorithm for generic cameras during NeRF learning.
BARF **Tretschk et al., "Ego4D: Surround View Perception and Prediction through Edge AI"** proposes a coarse-to-fine camera registration process from imperfect camera poses for bundle-adjusting NeRFs by gradually activating higher frequency bands of positional encoding. 
L2G-NeRF **Pumarola et al., "Differential Rendering"** introduces an effective local-to-global camera registration strategy with an initially flexible pixel-wise alignment and a frame-wise global alignment. % due to the sensitivity to the radiance field initialization in bundle adjustment.
% DBARF ____ bundle adjusts camera poses by taking a cost feature map as an implicit cost function that can be jointly trained with generalizable NeRFs.
NoPe-NeRF **Yin et al., "Leveraging Monocular Depth Estimation for Indoor Camera Calibration"** employs monocular depth priors for camera estimation with no pose initialization, but it is limited to depth prediction accuracy.  
% NoPe-NeRF, Colmap-free
For better joint estimation of the scene and camera, CamP **Huang et al., "Learning Non-Rigid Radiance Fields from Monocular Videos"** introduces the camera preconditioning technique, which applies a preconditioning matrix to camera parameters before passing them to the NeRF model.
% CamP follows BARF to estimate test camera using photometric test-time optimization.
Recently, SLAM systems **Mur-Artal et al., "ORB-SLAM3: An Accurate Monocular Visual Odometry Framework"** started adopting 3D-GS radiance field for efficient simultaneous localization and photorealistic mapping while the camera intrinsic model is calibrated. ____ relies on monocular depth estimation for jointly optimizing camera poses and 3D Gaussians. 

Existing self-calibrating methods are devised to optimize the radiance field from perspective images. SC-OmniGS is the first work dealing with self-calibration of omnidirectional radiance fields.

% \subsection{Camera Model} radiance field models only consider a set of input perspective images
\textbf{Camera Model.} A camera model is a camera projection function that establishes a mathematical relationship between 2D images and 3D observation. Typically, camera models can be classified into two groups, including parametric camera models, e.g. **Lenz et al., "Stratified Sampling for Real-Time Rendering"** and generic camera models, e.g. **Brown et al., "Bundle Adjustment â€“ A Modern Synthesis"**. Parametric camera models assume in 3D vision that lens distortion is symmetrical radially and use high-order polynomials to approximate models of real lenses. Conversely, generic camera models exploit a mass of parameters to associate each pixel with a 3D ray and calibrate distortion. Recent neural lens modeling **Wang et al., "Neural Lens Modeling for Real-Time Rendering"** employs an invertible neural network____ to model lens distortion while its optimization is memory-consuming. 
In this paper, we propose a generic camera model tailored for the 360-degree camera.

%Despite this, \textcolor{red}{self-calibration methods}____ have demonstrated the potential of utilizing differentiable parameters to model camera projection during radiance field training. 

%ref: Neural lens modeling
% Many 3D computer vision methods assume that lens distortion is radially symmetric around the center of the image. Various camera models, such as the radial [13] (bicubic [25]), division [15], FOV models [11], and rational model [8] are used to simulate such radially symmetric distortion.
% It is commonly assumed in 3D vision that lens distortion is symmetrical radially about the image center. Such radially symmetric distortion is simulated using a variety of camera models, including radial ____ or bicubic ____, FOV ____, division ____, and rational ____ models. To avoid determining the camera model while camera calibration with an unknown lens, a general camera model that associates each pixel with a 3D ray is widely used ____.  
%Despite this, \textcolor{red}{self-calibration methods}____ have demonstrated the potential of utilizing differentiable parameters to model camera projection during radiance field training. 
% \textcolor{red}{TODO: 360 camera model}



%anisotropic elliptical