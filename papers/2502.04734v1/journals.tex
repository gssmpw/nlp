\section{Introduction}\label{sec:intro}
%% why omnidirectional radiance field
% - implicit NeRF requires dense data.
% - traditional NeRF and variants rely on DENSE images with limited FoV
The radiance field techniques pioneered by NeRF~\citep{mildenhall2020nerf} have become an essential paradigm to facilitate scene reconstruction and novel view synthesis.
NeRF-based approaches~\citep{barron2021mip, zhang2020nerf, barron2022mip, fridovich2022plenoxels, chen2022tensorf, muller2022instant} implicitly representing the structure and appearance of captured objects generally necessitate a dense set of calibrated images for training. 
However, NeRF requires comprehensive data capture to reconstruct a scene accurately. 360-degree images can greatly facilitate such data capture.
Previous works, such as \cite{huang2022360roam} and \cite{chen2023panogrf}, have demonstrated the feasibility and efficiency of reconstructing omnidirectional radiance fields in large scenes using sparse and wide-baseline 360-degree image inputs.

% For example, \citet{huang2022360roam} and \citet{chen2023panogrf} demonstrated the feasibility of reconstructing omnidirectional radiance fields from sparse and wide-baseline 360-degree image inputs in large scenes. 

%This is because an omnidirectional camera captures a 360-degree horizontal and 180-degree vertical field of view, providing a comprehensive visual representation of the scene.

% the incorporation of is effective and efficient for learning .
% Notably, 
% The incorporation of 360-degree images is effective and efficient for comprehensive scene modeling, enabling the learning of omnidirectional radiance fields. 

%For example, 360Roam~\citep{huang2022360roam} has demonstrated the feasibility of omnidirectional radiance fields for immersive indoor roaming, requiring only 100-200 training images captured in large-scale environments, such as multiple rooms. 
%To improve the generalizability of the omnidirectional radiance field in sparse and wide-baseline inputs\yajie{are sparse and wide-baseline the same meaning?}, PanoGRF~\citep{chen2023panogrf} integrates deep features and scene priors. \yajie{How? please provide a simple explanation of how it enables sparse inputs. Also, why mentioning this work? why this relates to the self-calibration?} 
%360Roam~\citep{} is one of the representatives exploring omnidirectional radiance fields and realizing immersive indoor roaming. It only takes 100-200 training images captured in large-scale environments, such as multiple rooms. 
%Taming 360-degree field-of-view cameras to learn omnidirectional radiance field is effective and efficient for comprehensive scene modeling. 
%models \yajie{I think we don’t need model} and extrinsic poses \yajie{same here, we don’t need pose, simple camera intrinsic and extrinsic. Correct me if I'm wrong, you are not only estimate camera pose, but also want to introduce new camera models?}

% The accuracy of camera intrinsic and extrinsic parameters is crucial for these mapping-focused approaches to reconstruct high-definition models. However, 
Although 360-degree images have shown potential in reconstructing omnidirectional radiance fields, the quality of the reconstructed models is highly dependent on the accuracy of camera intrinsic and extrinsic parameters.
Existing methods for recovering 3D information from 360-degree images, including structure-from-motion (SfM) systems~\citep{moulon2013global, hhuang2022VO}, rely on an idealized spherical camera model to describe the mathematical relationship between 2D 360-degree images and 3D world projection. 
The 360-degree images are typically obtained by stitching multiple wide angle images, inheriting the distortion from each lens and resulting in a complex distortion pattern. The adverse impact of such distortion is neglected in the idealized spherical camera model. Consequently, the inaccurate camera projection modeling leads to poor SfM pose estimation, ultimately compromising the quality of 3D radiance field reconstruction when using real-world data. 
%To reduce reliance on SfM for camera calibration and increase system flexibility, 
To enhance system performance under camera perturbation and reduce reliance on SfM, some approaches~\citep{lin2021barf, jeong2021self, chen2023local, park2023camp} have explored radiance field self-calibration, where camera intrinsic and extrinsic parameters are jointly optimized with the radiance field representation. 
However, these solutions focus on traditional images, using well-established camera models for perspective cameras. A naive approach to self-calibrating the omnidirectional radiance field would consist of projecting the 360-degree images onto cube maps with perspective images. 
However, this approach undermines the integrity of 360-degree images, leading to increasing optimization complexity and instability~\citep{huang2024360loc}. %360-degree reconstruction 
% Since bundle adjustment in these systems is typically based on well-established camera models for perspective cameras, a straightforward solution to enhance omnidirectional radiance field quality would be to convert 360-degree images into cube maps with perspective images and then perform self-calibration. 
% However, such a solution undermines the integrity of 360-degree images, introducing additional optimization complexity and instability. %and advantages
%Despite this, \textcolor{red}{self-calibration methods}~\citep{?} have demonstrated the potential of utilizing differentiable parameters to model camera projection during radiance field training. \yajie{Could you please give concrete examples? in which specific work or works, cube is used for modeling? this is too vague} \yajie{why undermins the integrity? sphere to cube, all the information is there. Could you please clearly describe the problem?}
Given the lack of camera models accounting for the distortion of 360-degree images and the limitations of existing self-calibration approaches, there is an urgent need for a framework that calibrates the omnidirectional camera model and poses directly.

%Some approaches, such as 360FusionNeRF~\citep{kulkarni2023360fusionnerf} and PERF~\citep{perf2023}, attempt to bypass pose estimation by reconstructing omnidirectional radiance fields from a single omnidirectional image. However, these methods heavily rely on scene geometry priors provided by neural networks, while the issue of performance degradation in real-world situation remains. 

%\yajie{why this is a problem? do you want to say geometry estimation error will propagate to pose estimation? }
% For approaches focusing on mapping, accurate camera intrinsic models and extrinsic poses are indispensable for high-definition radiance field reconstruction. However, existing approaches recovering 3D information from 360-degree images, including structure-from-motion (SfM) systems~\citep{}, leverage an ideal spherical camera model to formulate the mathematical relationship between 2D 360-degree images and 3D world projection. The adverse impact of camera distortion is overlooked. The 360-degree images are obtained by stitching multiple images. Although a single image can seamlessly capture a 360-degree field of view, the distortion from each lens is inherited making the distortion pattern of 360-degree images complicated. The inaccurate camera projection modeling results in poor SfM pose estimation. Eventually 3D radiance field reconstruction quality suffers when using real data. Some methods such as 360FusionNeRF~\citep{} and PERF~\citep{} seek to reconstruct an omnidirectional radiance field from a single omnidirectional image avoiding the necessity of pose estimation. However, these approaches turn out to highly rely on scene geometry priors provided by neural networks. The problem of performance degradation in the real world still exists. 

%suffer deteriorative performance degradation in the real world.
%However, they rely on accurate poses estimated by Struct-from-Motion (SfM) systems. 
%decomposes and samples omnidirectional radiance fields in a geometry-aware fashion which not only accelerates the rendering speed but also improves the overall rendering quality.
%The photorealistic views are rendered through dense radiance field sampling along each ray. 



%

%To alleviate the reliance on SfM and make the system flexible, some approaches~\citep{} explore radiance field self-calibration. The camera's intrinsic and extrinsic parameters are bundle-adjusted while learning the corresponding radiance field representation. Since bundle adjustment in these systems is based on well-study camera models for conventional perspective and fisheye cameras, a straightforward solution is to convert 360-degree images into cube maps and then perform self-calibration. However, such a solution undermines the integrity and merit of 360-degree images ~\citep{}, increasing optimization complexity. Nevertheless, they showcase the potential of utilizing differentiable parameters to model the camera projection during the radiance field training.

%With the advent of 3D Gaussian splatting (3D GS) techniques, the increasing rendering speed significantly speeds up the optimization process. Therefore SLAM systems~\citep{} started adopting 3D-GS for simultaneous localization and photorealistic mapping. 

% Since these methods only support perspective cameras, a straightforward solution 
% The question is how can we 
% pose optimization approach -> cube map. 1.  , that can calibrate omnidirectional camera models and extrinsic poses along with omnidirectional radiance field reconstruction. 


\begin{figure}[t]
    \centering 
    \includegraphics[width=0.99\linewidth]{figure/teaser_v2.jpg}
    % \vspace{-0.35cm} %remove space between sub-figure's caption and caption
    % \vspace{-0.1cm}
    \caption{SC-OmniGS jointly optimizes the omnidirectional camera model, poses, and 3D Gaussians using a differentiable omnidirectional rasterizer. It can achieve rapid radiance field reconstruction with no pose prior and render high-fidelity novel views.}
    \label{fig:teaser}
    \vspace{-0.1cm}
\end{figure}

%\yajie{can add one more summary of how you achieve this. like with a novel camera model and xxx algorithm} By rendering images through splatting and rasterizing 3D Gaussians into planar screen space, 3D-GS significantly increases rendering speed. \yajie{a bit wordy about 3DGS, can reduce the introduction of 3DGS} and applying local affine approximations to rasterize 2D Gaussians~\citep{zwicker2002ewa}.
In this paper, we propose SC-OmniGS, a novel system 
that self-calibrates the omnidirectional camera model and poses along with omnidirectional radiance field reconstruction.
We leverage 3D Gaussian splatting (3D-GS) techniques~\citep{kerbl20233dgs} to represent radiance fields by a set of 3D Gaussians with explicit positions, covariances, and spherical harmonic coefficients, accelerating the optimization process.
%However, the original 3D-GS rendering algorithm is limited to rendering undistorted perspective images with constrained field-of-view (FoV). To overcome this limitation, 
In order to realize self-calibrating omnidirectional Gaussian splatting, we adopt a differentiable rasterizer that renders omnidirectional images by splatting 3D Gaussians onto a unit sphere~\citep{li2024omnigs}. Crucially, we derive omnidirectional camera pose gradients within the rendering procedure, enabling the optimization of noisy camera poses and even learning from scratch. An example is illustrated in Figure~\ref{fig:teaser}. To rectify distortion patterns in the input image, we propose a differentiable omnidirectional camera model comprising a learnable 3D spherical grid to regress the camera distortion. 
We thus obtain undistorted omnidirectional images by re-sampling input images based on the learned omnidirectional camera model. We jointly optimize 3D Gaussians, camera poses, and camera models by minimizing photometric loss between rendered and undistorted omnidirectional images. The overview of SC-OmniGS framework is demonstrated in Figure~\ref{fig:overview}.
Moreover, considering omnidirectional images in the equirectangular projection have an unbalanced spatial resolution, we introduce weighted spherical photometric loss to ensure the spatially equivalent optimization. Furthermore, we apply an anisotropy regularizer to constrain 3D Gaussian scales preventing the generation of filamentous kernels, particularly near the polar areas. To verify the efficacy of SC-OmniGS, we conducted extensive experiments using a synthetic dataset OmniBlender~\citep{choi2023balanced} and a real-world 360Roam dataset~\citep{huang2022360roam}. The results showed that our proposed system can effectively calibrate the intrinsic model and extrinsic poses of the omnidirectional camera, achieving state-of-the-art performance on the omnidirectional radiance field reconstruction.   

To summarize, the main contributions of this work include: 
% \vspace{-0.3cm}
\begin{itemize}%[leftmargin=*] %topsep=0pt, itemsep=5pt,
\item We proposed the first system for self-calibrating omnidirectional radiance fields, which jointly optimizes 3D Gaussians, omnidirectional camera poses, and camera models.
%We proposed the first self-calibrating omnidirectional Gaussian splatting system that achieves high-quality omnidirectional radiance field reconstruction from noisy camera poses or even without prior pose information.
%that jointly optimizes omnidirectional camera models, poses, and 3D Gaussian

\item We provided the derivation of omnidirectional camera pose gradients within the omnidirectional Gaussian splatting procedure, enabling the optimization of noisy camera poses and even learning from scratch. It can also facilitate other applications such as GS-based omnidirectional SLAM.

\item We introduced a novel differentiable omnidirectional camera model that effectively tackles the complex distortion pattern contained in omnidirectional cameras.
\end{itemize}


%However, omnidirectional images in the equirectangular projection have large non-linear distortion near the polar area. In other words, similar 3D spatial information would occupy more pixels when projected to the top and bottom 2D image area. 
%obtain undistorted omnidirectional images,

\section{Related Work}
%\vspace{-0.2cm}
% \subsection{Omnidirectional Radiance Field}
\textbf{Omnidirectional Radiance Field}.
Neural radiance field (NeRF)~\citep{mildenhall2020nerf} has emerged as a powerful neural scene representation for novel view synthesis. NeRF represents a scene as a neural network with radiance and opacity outputs for each 3D point. Although most existing radiance field approaches~\citep{chen2022tensorf, barron2023zipnerf, dvgo, pointnerf} can synthesize photorealistic novel views by learning from dense perspective image captures, they tend to suffer from inaccurate geometry reconstruction due to the limited field-of-view coverage and sparse view inputs. 
To achieve an immersive scene touring with six degrees of freedom (6-DoF),~\citet{huang2022360roam} proposes omnidirectional radiance field learning from sparse 360-degree images with geometry-adaptive blocks, while some previous works incorporate 360-degree 3D priors for better geometry feature learning~\citep{chen2023panogrf, kulkarni2023360fusionnerf, perf2023}. EgoNeRF~\citep{choi2023balanced} employs quasi-uniform angular grids to enhance performance in egocentric scenes captured within a small circular area.
The recent 3D Gaussian splatting (3D-GS) techniques parameterize radiance fields as explicit 3D Gaussians, significantly accelerating rendering and optimization~\citep{kerbl20233dgs}. 
With the efficient 3D-GS representation, concurrent OmniGS~\citep{li2024omnigs} optimizes 3D Gaussian splats via sparse panorama inputs while 360-GS~\citep{bai2024360} further exploits indoor layout priors for robust structure reconstruction. 

While panoramas offer a continuous and wide field of view for omnidirectional optimization, all discussed works focus on radiance field reconstruction merely from known camera parameters, which are vulnerable to inaccurate camera modeling.
%approximation by techniques such as SfM \citep{moulon2013global}.
% regardless of camera pose optimization. schonberger2016structure,

% \subsection{Self-Calibrating Radiance Field}
\textbf{Self-Calibrating Radiance Field}.
% With the consideration of camera pose optimization for radiance field reconstruction,
To simplify the training process of radiance fields and alleviate the reliance on pre-computed camera parameters, some works optimize camera poses or learn poses from scratch during scene reconstruction~\citep{wang2021nerfmm, jeong2021self, lin2021barf}. 
\citet{wang2021nerfmm} shows that camera pose and intrinsic parameters can be jointly optimized during NeRF learning for forward-facing scenes. 
% Techniques: NeRF$--$ 
% 1.NeRFmm/SCNeRF/BARF/L2G 
% 2.Nope/colmap-free w/ depth priors 
% 3. Camp with precondition
% From review of PERF: Omni-NeRF jointly learns the scene geometry and optimizes the camera parameters without knowing the fish eye projection. 
SC-NeRF~\citep{jeong2021self} additionally learns non-linear distortion parameters and introduces a camera self-calibration algorithm for generic cameras during NeRF learning.
BARF~\citep{lin2021barf} proposes a coarse-to-fine camera registration process from imperfect camera poses for bundle-adjusting NeRFs by gradually activating higher frequency bands of positional encoding. 
L2G-NeRF~\citep{chen2023local} introduces an effective local-to-global camera registration strategy with an initially flexible pixel-wise alignment and a frame-wise global alignment. % due to the sensitivity to the radiance field initialization in bundle adjustment.
% DBARF \citep{chen2023dbarf} bundle adjusts camera poses by taking a cost feature map as an implicit cost function that can be jointly trained with generalizable NeRFs.
NoPe-NeRF~\citep{bian2023nope} employs monocular depth priors for camera estimation with no pose initialization, but it is limited to depth prediction accuracy.  
% NoPe-NeRF, Colmap-free
For better joint estimation of the scene and camera, CamP~\citep{park2023camp} introduces the camera preconditioning technique, which applies a preconditioning matrix to camera parameters before passing them to the NeRF model.
% CamP follows BARF to estimate test camera using photometric test-time optimization.
Recently, SLAM systems~\citep{hhuang2024photoslam, yan2023gsslam, matsuki_and_murai2024gsslam, keetha2024splatam} started adopting 3D-GS radiance field for efficient simultaneous localization and photorealistic mapping while the camera intrinsic model is calibrated. \citet{fu2024colmap} relies on monocular depth estimation for jointly optimizing camera poses and 3D Gaussians. 

Existing self-calibrating methods are devised to optimize the radiance field from perspective images. SC-OmniGS is the first work dealing with self-calibration of omnidirectional radiance fields.

% \subsection{Camera Model} radiance field models only consider a set of input perspective images
\textbf{Camera Model.} A camera model is a camera projection function that establishes a mathematical relationship between 2D images and 3D observation. Typically, camera models can be classified into two groups, including parametric camera models, e.g.~\citep{camera_model:kd, camera_model:ds} and generic camera models, e.g.~\citep{camera_model:dist, camera_model:having}. Parametric camera models assume in 3D vision that lens distortion is symmetrical radially and use high-order polynomials to approximate models of real lenses. Conversely, generic camera models exploit a mass of parameters to associate each pixel with a 3D ray and calibrate distortion. Recent neural lens modeling~\citep{camera_model:nlm} employs an invertible neural network~\citep{FrEIA} to model lens distortion while its optimization is memory-consuming. 
In this paper, we propose a generic camera model tailored for the 360-degree camera.

%Despite this, \textcolor{red}{self-calibration methods}~\citep{?} have demonstrated the potential of utilizing differentiable parameters to model camera projection during radiance field training. 

%ref: Neural lens modeling
% Many 3D computer vision methods assume that lens distortion is radially symmetric around the center of the image. Various camera models, such as the radial [13] (bicubic [25]), division [15], FOV models [11], and rational model [8] are used to simulate such radially symmetric distortion.
% It is commonly assumed in 3D vision that lens distortion is symmetrical radially about the image center. Such radially symmetric distortion is simulated using a variety of camera models, including radial \citep{duane1971close} or bicubic \citep{kilpela1981compensation}, FOV \citep{devernay2001straight}, division \citep{fitzgibbon2001simultaneous}, and rational \citep{claus2005rational} models. To avoid determining the camera model while camera calibration with an unknown lens, a general camera model that associates each pixel with a 3D ray is widely used \citep{grossberg2001general,ramalingam2005towards,hartley2007parameter,camposeco2015non}.  
%Despite this, \textcolor{red}{self-calibration methods}~\citep{?} have demonstrated the potential of utilizing differentiable parameters to model camera projection during radiance field training. 
% \textcolor{red}{TODO: 360 camera model}



%anisotropic elliptical
\section{Preliminary: 3D Gaussian Splatting}
%\yajie{you can make this session shorter, for example, we can omit equation 4 and 5. In Neurips, people usually will skip the preliminary and only provide references, assuming reviewers are experts of this domain}
3D Gaussian splatting (3D-GS)~\citep{kerbl20233dgs} represents the scene with a set of 3D Gaussians, of which $i^{th}$ Gaussian is parameterized by 3D position $\mathbf{P}_i$, covariance $\Sigma_i$, opacity $\sigma_i$, and color $c_i$ represented by spherical harmonics (SH) coefficients. The 3D Gaussian reconstruction kernel is formulated as
\begin{equation}\label{eq:r3d}
    \mathbf{r_{3D}(P)} = \mathbf{G_{3D}(P-P}_i) = exp\{-\tfrac{1}{2}(\mathbf{P}-\mathbf{P}_i)^T\Sigma_i^{-1}(\mathbf{P}-\mathbf{P}_i)\} ,
\end{equation}
where $\mathbf{P}\in \mathbb{R}^3:= (X, Y, Z)^T$ denotes the sampling position in the world space. To render an image, 
3D Gaussians are transformed from the world space to the camera space $\{\mathbf{x}:=(x, y, z)^T|\mathbf{x}\in \mathbb{R}^3\}$ by a viewing transformation matrix $\mathbf{T}=\left[\mathbf{R|t}\right]$, and $\mathbf{x = RP + t}$. 3D Gaussians are then projected onto the image plane $\{\mathbf{u}:=(u, v)^T |\mathbf{u}\in \mathbb{R}^2\}$. The projection function $\phi$ for a perspective image is described as 
\begin{equation}
    \mathbf{u = \phi(x)}=\begin{bmatrix}
        f_x x/{z} + c_x \\ f_y {y}/{z} + c_y
    \end{bmatrix}, 
\end{equation} 
where $f_x, f_y$ are focal lengths and $c_x, c_y$ are the principle points of the pinhole camera model. Since this projection process is not affine, the 3D Gaussian reconstruction kernel $\mathbf{r_{3D}(P)}$ cannot be directly mapped to 2D. To address this problem, \citet{zwicker2002ewa} introduced the local affine approximation of the projection function: 
\begin{equation}\label{eq:u_affine}
    \mathbf{u = u}_i + \mathrm{J}_i \cdot (\mathbf{x-x}_i) = \phi(\mathbf{RP}_i+\mathbf{t}) + \mathrm{J}_i \cdot (\mathbf{x-RP}_i-\mathbf{t}) .
\end{equation} 
The Jacobian $\mathrm{J}_i$ is defined by the partial derivatives of projection function $\phi$ at point $\mathbf{x}_i$:
\begin{gather}\label{eq:J}
    \mathbf{J}_i = \pdv{\phi}{\mathbf{x}}(\mathbf{x}_i) =
    \begin{bmatrix}
        \frac{\partial u_i}{\partial x} & \frac{\partial u_i}{\partial y} & \frac{\partial u_i}{\partial z} \\
        \frac{\partial v_i}{\partial x} & \frac{\partial v_i}{\partial y} & \frac{\partial v_i}{\partial z} \\
    \end{bmatrix}
    % = 
    % \begin{bmatrix}
    %     \frac{f_x}{z_i} & 0 & -\frac{f_xx_i}{z_i^2} \\
    %     0 & \frac{f_y}{z_i} & -\frac{f_yy_i}{z_i^2} \\
    % \end{bmatrix}.
    ,\\
    \begin{matrix}
    \frac{\partial u_i}{\partial x} = \frac{f_x}{z_i} , &
    \frac{\partial u_i}{\partial y} = 0 , &
    \frac{\partial u_i}{\partial z} = -\frac{f_xx_i}{z_i^2} ,  &
    \frac{\partial v_i}{\partial x} = 0 , &
    \frac{\partial v_i}{\partial y} = \frac{f_y}{z_i}, &
    \frac{\partial v_i}{\partial z} = -\frac{f_yy_i}{z_i^2}.\\
    \end{matrix}
\end{gather}
% \begin{equation}
% where
%     \begin{matrix}
%     \frac{\partial u_i}{\partial x} = \frac{f_x}{z_i} , &
%     \frac{\partial u_i}{\partial y} = 0 , &
%     \frac{\partial u_i}{\partial z} = -\frac{f_xx_i}{z_i^2} ,  &
%     \frac{\partial v_i}{\partial x} = 0 , &
%     \frac{\partial v_i}{\partial y} = \frac{f_y}{z_i}, &
%     \frac{\partial v_i}{\partial z} = -\frac{f_yy_i}{z_i^2}.\\
%     \end{matrix}
% \end{equation}
According to Eq.~\ref{eq:r3d} and~\ref{eq:u_affine}, the 2D Gaussian reconstruction kernel is thus calculated by
\begin{equation}\label{eq:r2d}
    \mathbf{r}_{2D}(\mathbf{u}) = \mathbf{G}_{2D}(\mathbf{u-u}_i) = \exp\{-\tfrac{1}{2}(\mathbf{u}-\mathbf{u}_i)^T(\mathbf{J}_i\mathbf{R}\Sigma_i\mathbf{R}^T\mathbf{J}_i^T)^{-1}(\mathbf{u}-\mathbf{u}_i)\} .
\end{equation}

The final rendering color $\mathbf{C(u)}$ of a pixel $\mathbf{u}$ in the image can be computed by volumetric rendering: 
\begin{align}
    \mathbf{C(u)} = \sum_{i\in \mathcal{N}} c_i \alpha_i \prod^{i-1}_{j=1}(1-\alpha_j) &, \quad
    \alpha_j = \sigma_j \cdot \mathbf{r}_{2D}(\mathbf{u}) , \\
    c_i = \sum_{m=0}^\mathcal{M} \mathbf{SH}^m_i(dir_i)\label{eq:c_i}, \quad dir_i = & normalize(\left[\mathbf{P}_i - (-\mathbf{R}^T\mathbf{t})\right]) ,
\end{align}
where $\mathcal{N}$ denotes the set of ordered 3D Gaussians affecting the pixel $\mathbf{u}$ after splatting onto 2D image, while  $\mathcal{M}$ is the degree of SH coefficients. $\mathbf{SH}^m_i(\cdot)$ denotes spherical harmonics functions of the normalized viewing orientation $dir_i$. % = normalize(\left[P_i - (-\mathbf{R}^T\mathbf{t})\right])$.
%ordered points overlapping the pixel are blended to compute the pixel color.

% 3D Gaussians $\mathbf{G(p)}$ can be efficiently projected into image space as 2D Gaussians $\mathbf{G'(p')}$ for rendering. Given the world-to-camera transformation matrix $\mathrm{W}$ and the Jacobian of the projective transformation $\mathrm{J}_i$, the splatted 2D Gaussian's covariance matrix $\Sigma'$ in camera coordinates is obtained by \citep{zwicker2002ewa}:
% \begin{equation}
%     \Sigma' = \mathrm{J} \mathrm{W} \Sigma^{T} \mathrm{W}^{T} \mathrm{J}^{T} .
% \end{equation} 

% We denote the camera coordinates by a vector $\mathbf{t}=(t_x, t_y, t_z)^{T}$. The Jacobian $\mathrm{J}_i$ is given by partial derivatives of local affine approximation $\phi$ at point $\mathbf{t}_i$: % EWA-splat Eq.29
% \begin{equation}
%     \mathrm{J}_i = \pdv{\phi}{\mathbf{t}}(\mathbf{t}_i) = 
%     \begin{pmatrix}
%     1/t_{i,z} & 0 & -t_{i,x}/t^2_{i,z} \\
%     0 & 1/t_{i,z} & -t_{i,y}/t^2_{i,z} \\
%     t_{i,x}/l' & t_{i,y}/l' & t_{i,z}/l'
%     \end{pmatrix},
% \end{equation}
% where $\mathbf{t}_i$ is the center of Gausssian in camera space, and $l'= \|(t_{i,x}, t_{i,y}, t_{i,z})^{T}\|$ is the distance from point to camera center.

% Points $\mathbf{t}$ in camera space are projected to 2D screen space by dividing the depth $t_z$, and we get the Jacobian $\mathrm{J}_i$ as a $2 \times 3$ matrix:
% \begin{equation}
%     \mathrm{J}_i = \pdv{\phi}{\mathbf{t}}(\mathbf{t}_i) = 
%     \begin{pmatrix}
%     1/t_{i,z} & 0 & -t_{i,x}/t^2_{i,z} \\
%     0 & 1/t_{i,z} & -t_{i,y}/t^2_{i,z} 
%     \end{pmatrix}.
% \end{equation}

\begin{figure*}
    \centering
    \includegraphics[width=0.99\linewidth]{figure/overview.jpg}
    \caption{A schematic overview of SC-OmniGS optimization flow.} %After initializing coarse 3D Gaussians from sparse points, we optimize 
    \label{fig:overview}
    % \vspace{-0.1cm}
\end{figure*}
\section{Methodology: SC-OmniGS}

SC-OmniGS is a self-calibrating framework for omnidirectional radiance field reconstruction. It takes multiple 360-degree images without pose information or with noisy pose estimations as input to recover a fine-grained omnidirectional radiance field. We adopt 3D-GS~\citep{kerbl20233dgs} as the radiance field representation to achieve fast reconstruction and real-time novel view rendering. Similar to 3D-GS, we initialize the 3D Gaussians from coarse points input obtained from SfM estimation or an omnidirectional depth map. We then jointly optimize 3D Gaussians, the omnidirectional camera model, and poses. The overview of our framework is demonstrated in Figure~\ref{fig:overview}.

%\yajie{I got confused, you mentioned that your method skip SFM for pose estimation, but here you still need SfM to provide initial points for 3DGS. You should modify your introduction, that you are more an optimization approach rather than end-to-end or explain if you can also taking just images} 

%theoretical
In this section, we first revisit omnidirectional Gaussian splatting and introduce a differentiable rasterizer that can render omnidirectional images in the equirectangular projection. In addition, we conduct a mathematical analysis of omnidirectional camera pose derivatives within the rasterizer. Furthermore, we propose a novel omnidirectional camera model to rectify input training images. Finally, the joint optimization is performed to minimize weighted spherical photometric loss and anisotropy loss.
%$\mathcal{L}$.

% \begin{figure*}
%     \centering
%     \includegraphics[width=0.99\linewidth]{figure/overview.png}
%     \caption{A schematic overview of SC-OmniGS optimization flow.} %After initializing coarse 3D Gaussians from sparse points, we optimize 
%     \label{fig:overview}
%     \vspace{-0.1cm}
% \end{figure*}

\subsection{Omnidirectional Gaussian Splatting} %which is a unit sphere 
% An omnidirectional image covers 360-degree horizontal and 180-degree vertical field-of-views. To make the rasterizer universal, we utilize an idealized spherical camera model to describe the projection relationship of an omnidirectional camera. Instead of splatting 3D Gaussian onto an image plane, we splat 3D Gaussian onto a unit sphere and then expand the unit sphere to a 2D image in the equirectangular projection. The projection function for an 360-degree image $\phi^o$ is denoted as:
To develop a universal rasterizer, we adopt an idealized spherical camera model to describe the projection relationship of an omnidirectional camera \citep{li2024omnigs}. Rather than splatting 3D Gaussians onto an image plane, we project them onto a unit sphere and subsequently expand the unit sphere to a 2D image in the equirectangular projection. The projection function for an omnidirectional image, denoted as $\phi^o$, is defined as:
% \vspace{-0.1cm}
\begin{equation}\label{eq:u_o}
    \mathbf{u = \phi^o(x)}=\begin{bmatrix}
        f^o_x \cdot arctan2(x, z) + c^o_x \\ f^o_y \cdot arcsin({y}/{d}) + c^o_y
    \end{bmatrix} = \begin{bmatrix}
        \tfrac{W}{2\pi} \cdot arctan2(x, z) + \tfrac{W}{2} \\ \tfrac{H}{\pi} \cdot arcsin({y}/{d}) + \frac{H}{2}
    \end{bmatrix}, 
\end{equation}
where $arctan2$ is the 2-argument arctangent function and $d=\sqrt{x^2+y^2+z^2}$. $H$ and $W$ denote image height and width respectively. 
According to Eq.~\ref{eq:J}, the partial derivatives of projection function $\phi^o$ at point $\mathbf{x}_i$ is $\mathbf{J}^o_i$, and 
\begin{equation}\label{eq:J_oi}
    \mathbf{J}^o_i = \pdv{\phi^o}{\mathbf{x}}(\mathbf{x}_i) =
    \begin{bmatrix}
        f^o_x\cdot\frac{z_i}{{x_i^2 + z_i^2}} & 0 & -f^o_x\cdot\frac{x_i}{{x_i^2 + z_i^2}}\\
        f^o_y\cdot\frac{x_i y_i}{d_i^2\sqrt{x_i^2 + z_i^2}} & f^o_y\cdot\frac{\sqrt{x_i^2 + z_i^2}}{d_i^2} & -f^o_y\cdot\frac{z_i y_i}{d_i^2\sqrt{x_i^2 + z_i^2}}
    \end{bmatrix} .
\end{equation}
We substitute $\mathbf{J}_i$ in Eq.~\ref{eq:r2d} and obtain the 2D Gaussian reconstruction kernel for omnidirectional Gaussian splitting:
\begin{equation}\label{eq:ro2d}
    \mathbf{r}^o_{2D}(\mathbf{u}) = \mathbf{G}^o_{2D}(\mathbf{u-u}_i) = \exp\{-\tfrac{1}{2}(\mathbf{u}-\mathbf{u}_i)^T(\mathbf{J}^o_i\mathbf{R}\Sigma_i\mathbf{R}^T{\mathbf{J}^o_i}^T)^{-1}(\mathbf{u}-\mathbf{u}_i)\} .
\end{equation}
Eventually, the rendering color $\mathbf{C^o(u)}$ of a pixel $\mathbf{u}$ in the omnidirectional image can be computed by: 
\begin{equation}\label{eq:Cou}
    \mathbf{C}^o\mathbf{(u)} = \sum_{i\in \mathcal{N}} c_i \alpha^o_i \prod^{i-1}_{j=1}(1-\alpha^o_j), \quad \alpha^o_j = \sigma_j \cdot \mathbf{r}^o_{2D}(\mathbf{u}) .
\end{equation}
% where $\mathcal{N}$ denotes the set of ordered 3D Gaussians affecting the pixel $\mathbf{u}$ after splatting onto 2D image.

% The camera model projects a 3D camera-space point $\mathbf{t} = (t_x, t_y, t_z)^T$ to a pixel point $\mathbf{p'}=(p'_x, p'_y)^T$ on the image space, and vice versa. We employ the commonly used equirectangular projection model as our omnidirectional camera model, projecting each view into a unit sphere with longitudes $lon \in [-\pi, \pi)$ and latitudes $lat \in [-\pi/2, \pi/2]$. The conversion of a 3D point to spherical coordinate system is:
%Besides backpropagating gradients with respect to 3D Gaussian, our differentiable omnidirectional rasterizer also transmits gradients w.r.t the world-to-camera transformation matrix $\mathbf{T}=\left[\mathbf{R|t}\right]$ for camera pose optimization. To make optimization numerically stable and singularity-free, we represent and optimize the transformation matrix in the form of a 7-dimensional vector that consists of a rotation quaternion and translation: \yajie{should define this term in 4.1, can change title of 4.1 in to omnidrectional rasterizer} \yajie{is this a widely adopted representation? Or you just simply put them together? what's the difference between this and dual quaternion? }

\subsection{Gradients of Omnidirectional Camera Pose}
In addition to backpropagating gradients with respect to 3D Gaussians, our differentiable omnidirectional rasterizer also propagates gradients with respect to world-to-camera transformation metrics $\mathbf{T}=\left[\mathbf{R|t}\right]$ for camera pose optimization. To ensure numerical stability and avoid singularities during optimization, we represent and optimize the transformation matrix in a compact and singularity-free form, which is a 7-dimensional vector comprising a rotation quaternion and translation: $\mathbf{T'}=\left[\mathbf{q|t}\right]=\left[q_w \,q_x \,q_y \,q_z\,t_x\, t_y \, t_z\right]$. By applying the chain rule to the rendering function in Eq.~\ref{eq:Cou}, the gradients of camera pose can be decomposed into two primary branches: $\pdv{\mathcal{L}}{c}\cdot\pdv{c}{\mathbf{T'}}$ and $\pdv{\mathcal{L}}{\alpha^o_j}\cdot\pdv{\alpha^o_j}{\mathbf{r}^o_{2D}}\cdot\pdv{\mathbf{r}^o_{2D}}{\mathbf{T'}}$. Since $\pdv{\mathcal{L}}{c}$ and $\pdv{\mathcal{L}}{\alpha^o_j}\cdot\pdv{\alpha^o_j}{\mathbf{r}^o_{2D}}$ have been previously derivated for 3D Gaussian optimization~\citep{kerbl20233dgs, li2024omnigs}, we further elaborate unsolved parts subsequently. %naturally

\textbf{Part 1}: $\pdv{c}{\mathbf{T'}}$, the gradient of color w.r.t. pose $\left[\mathbf{q|t}\right]$.
The view-dependent color of a 3D Gaussian is obtained from spherical harmonics coefficients as depicted in Eq~\ref{eq:c_i}. It is related to its normalized viewing orientation. Hence, $\pdv{c}{\mathbf{T'}}$ is equal to 
\begin{equation}
% \small
    \pdv{c}{\mathbf{T'}} = \pdv{c}{dir} \cdot\pdv{dir}{\mathbf{T'}} = \pdv{c}{{dir}} \cdot \left[\pdv{dir}{\mathbf{R}}\cdot\pdv{\mathbf{R}}{\mathbf{q}}, \quad \pdv{dir}{\mathbf{t}} \right]  .
\end{equation}

\textbf{Part 2}: $\pdv{\mathbf{r}^o_{2D}}{\mathbf{T'}}$, the gradient of 2D Gaussian w.r.t. pose $\left[\mathbf{q|t}\right]$. Camera pose gets involved in the splatting of Gaussian onto 2D omnidirectional images. According to Eq.~\ref{eq:u_o}-\ref{eq:ro2d}, 
\begin{equation}
% \small
\begin{split}
    \pdv{\mathbf{r}^o_{2D}}{\mathbf{T'}} &= 
    \left[\pdv{\mathbf{r}^o_{2D}}{\mathbf{u}_i}\cdot\pdv{\mathbf{u}_i}{\mathbf{T'}}, \quad \pdv{\mathbf{r}^o_{2D}}{\mathbf{J}^o_i}\cdot\pdv{\mathbf{J}^o_i}{\mathbf{T'}}, \quad \pdv{\mathbf{r}^o_{2D}}{\mathbf{R}}\cdot\pdv{\mathbf{R}}{\mathbf{T}'} \right] \\
    &= 
    \left[\pdv{\mathbf{r}^o_{2D}}{\mathbf{u}_i}\cdot\pdv{\mathbf{u}_i}{\mathbf{x}_i}, \quad \pdv{\mathbf{r}^o_{2D}}{\mathbf{J}^o_i}\cdot\pdv{\mathbf{J}^o_i}{\mathbf{x}_i}\right]
    \cdot\left[\pdv{\mathbf{x}_i}{\mathbf{R}}
    \cdot\pdv{\mathbf{R}}{\mathbf{q}}, \quad \pdv{\mathbf{x}_i}{\mathbf{t}}
    \right]
    +
    \left[\pdv{\mathbf{r}^o_{2D}}{\mathbf{R}}\cdot\pdv{\mathbf{R}}{\mathbf{q}} \right] .
\end{split}
\end{equation}

\subsection{Omnidirectional Camera Model} \label{sec:camera_model}
Omnidirectional cameras, which typically consist of at least two fisheye lenses, capture 360-degree images through image stitching. However, factory calibration prioritizes seamless stitching over rectifying distortion. As such, stitched omnidirectional images inherently retain distortion from the original camera lenses and deviate from ideal spherical camera models. Unfortunately, there is a lack of well-established camera models capable of accurately representing omnidirectional camera distortion, which inevitably compromises 3D reconstruction quality. To address this limitation, we propose the first generic omnidirectional camera model that learns complex distorting patterns through differentiable optimization. Our omnidirectional camera model comprises a frozen unit sphere and trainable focal length coefficient $f_t$ and angle distortion coefficients, as illustrated in Figure~\ref{fig:camera_model}.
For model initialization, we create a spherical grid $\mathbf{\mathcal{S}}\in\mathbb{R}^{H\times W \times3}$ and set the corresponding angle distortion coefficients $\mathbf{\mathcal{D}}$ with the same dimension to zeros. The camera ray distortion is then estimated by the Hadamard product of the spherical grid and learned angle distortion coefficients. This approach is more stable than directly learning camera ray distortion.
%Instead of directly learning camera ray distortion, we found this approach is more stable. 
Consequently, the omnidirectional camera model $\Theta$ is defined as:
\begin{equation} \label{eq:camera_model}
    \Theta := \mathbf{\mathcal{S}} \cdot f_t +  \mathbf{\mathcal{S}} \odot \mathbf{\mathcal{D}}.
\end{equation}
Our differentiable camera model is decoupled from the rasterization pipeline, ensuring that it does not compromise the efficiency of the rendering process. By leveraging the learned camera model parameters $\Theta$, we can extract a distortion-free omnidirectional image $I^o$ from the input image using bicubic grid sampling. Please refer to Algorithm~\ref{alg:camera_model} for details. The extracted images $I^o$ are then utilized to compute the total loss against the rendered images $I^r$. 

%Pytorch automatic differentiation mechanism.
% The angle distortion coefficients are applied to the spherical grid by a dot product rather than directly added to camera rays. 
%The angle distortion coefficients are applied to the spherical grid by element-wise multiplication. 

\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{figure/camera_model2.jpg}
    \vspace{-0.1cm}
    \caption{Differentiable omnidirectional camera model.}
    \label{fig:camera_model}
\end{figure}

\subsection{Joint Optimization} \label{sec:joint_opt}
The optimization in terms of 3D Gaussian, camera pose $\mathbf{T'}$, and camera model $\Theta$ is performed by minimizing the photometric loss, comprising the mean absolute error (MAE) and structural similarity index measure (SSIM) loss. However, the equirectangular image projection is not conformal, as the region deformation increases along parallels towards poles. In other words, similar 3D spatial information would occupy more pixels when projected to the top and bottom areas of the 2D image. To ensure spatially equivalent optimization, we introduce a weighted spherical photometric loss, which is defined as: 
\begin{align}
    \mathcal{L}_{wsp}(I^r, I^o) &=\tfrac{1}{\mathcal{\left|I\right|}} 
    \sum_{\mathbf{u}\in \mathcal{I}}
    \left\{ (1-\lambda)\left|{\hat{I^r}- \hat{I^o}}\right|_1 + \lambda{(1-\text{SSIM}(\hat{I^r}, \hat{I^o}))} 
    \right\}, \label{eq:photoloss}\\
    \hat{I}&=wI , \quad w(\mathbf{u}) = \cos{{(v - c^o_y + 0.5)}/{f^o_y}}
\end{align} 
where $\lambda$ is a hyperparameter, $\mathcal{I}$ represents a set of image pixels, and $w(\cdot)$ is the spherical weights~\citep{sun2017weighted} used to ensure a spherically uniform sample.
In addition, we leverage an anisotropy regularizer to constrain the ratio between the major and minor axis lengths of 3D Gaussians, thereby preventing them from degenerating into filamentous kernels. The anisotropy regularizer is formulated as:
\begin{equation} \label{eq:anisotropy}
    \mathcal{L}_{aniso} =\tfrac{1}{\mathcal{\left|N\right|}}\sum_{i\in \mathcal{N}} 
    \left\{\max(\tfrac{\max(\mathbf{s}_i)}{\min(\mathbf{s}_i)}, \gamma)-\gamma \right\},
\end{equation}
where $\mathbf{s}_i$ is the scaling of 3D Gaussians~\citep{kerbl20233dgs} and $\gamma$ is the ratio threshold. 
Overall, the joint optimization objective is:
\begin{equation}
    \mathcal{L} = \mathcal{L}_{wsp} + \mathcal{L}_{aniso} .
\end{equation}

\begin{comment}
    
\subsection{Implementation Detail}\label{sec:imple_detail}
% \vspace{-0.2cm} 
% follow the default setting of 3D-GS~\citep{kerbl20233dgs}, e.g., $\lambda=2$, and the total optimization iteration is 30000. 

Our SC-OmniGS implementation is built on Pytorch and CUDA. The gradients of the omnidirectional camera model are maintained by the Pytorch automatic differentiation mechanism. We utilize Adam optimizer to update trainable parameters. The hyperparameters for 3D Gaussians optimization are set according to the default settings of 3D-GS~\citep{kerbl20233dgs}, with $\lambda=2$ and a total of 30,000 optimization iterations. We set the ratio threshold $\gamma$ to 10.
Moreover, we set the learning rate of the camera model $\Theta$ to 1e-4 and activate the angle distortion coefficients $\mathbf{\mathcal{D}}$ using the Tanh function. 
For simplicity, we fix $f_t$ to 1. The initial learning rates for each camera quaternion $\mathbf{q}$ and translation $\mathbf{t}$ are set to 0.01, with exponential decay to 1.6e-4 and 6e-3, respectively, in 100 steps per camera. When calibrating from scratch, we increase the initial learning rate of $\mathbf{t}$ to 0.1. Furthermore, we observe that noisy camera poses can lead to the generation of numerous incorrect 3D Gaussians at the beginning of optimization, making it challenging to filter them out during optimization. To address this, we re-initialize the 3D Gaussian with the input coarse points twice, at the $2000^{th}$ and $4000^{th}$ iterations.

%$f_t$ is fixed to 1 for simplification. The initial learning rates for camera quaternion $\mathbf{q}$ and translation $\mathbf{t}$ are set to 0.01. They will decay exponentially to 1.6e-4 and 6e-3 respectively within 100 steps. Additionally, we increase the initial learning rate of $\mathbf{t}$ to 0.1 when training from scratch. Furthermore, we found that noisy camera poses lead to the generation of vast incorrect 3D Gaussians at the beginning of optimization while it is hard to completely filter during optimization. Therefore, we re-initialize 3D Gaussian with the input coarse points twice, i.e., at $2000^{th}$ and $4000^{th}$ iterations.

%experimentally
%alternate optimization strategy 

\end{comment}

% \vspace{-0.2cm}
\section{Experiments}
\vspace{-0.3cm}
% \subsection{Implementation Detail}
\subsection{Experiment Setup}\label{sec:esetup}

\textbf{Implementation Detail.}\label{sec:imple_detail}
Our SC-OmniGS implementation is built on Pytorch and CUDA.  We utilize Adam optimizer to update trainable parameters. The hyperparameters for 3D Gaussians optimization are set according to the default settings of 3D-GS~\citep{kerbl20233dgs}, with $\lambda=0.2$ and a total of 30,000 optimization iterations. We set the ratio threshold $\gamma$ to 10. \edit{The omnidirectional camera model is shared across all views on individual scene.}
Moreover, we set the learning rate of the camera model $\Theta$ to 1e-4 and activate the angle distortion coefficients $\mathbf{\mathcal{D}}$ using the Tanh function. 
For simplicity, we fix $f_t$ to 1. The initial learning rates for each camera quaternion $\mathbf{q}$ and translation $\mathbf{t}$ are set to 0.01, with exponential decay to 1.6e-4 and 6e-3, respectively, in 100 steps per camera. When calibrating from scratch, we increase the initial learning rate of $\mathbf{t}$ to 0.1. 

%Furthermore, we observe that noisy camera poses can lead to the generation of numerous incorrect 3D Gaussians at the beginning of optimization, making it challenging to filter them out during optimization. To address this, we re-initialize the 3D Gaussian with the input coarse points twice, at the $2000^{th}$ and $4000^{th}$ iterations.

% \textcolor{red}{TODO: add point cloud initialization details.}

% \vspace{-0.2cm} with sparse training views

\textbf{Baselines.} For comparison, we select BARF \citep{lin2021barf}, L2G-NeRF \citep{chen2023local} and CamP \citep{park2023camp} as SOTA radiance field calibration baselines trained with training cameras initialized with preset perturbations or from scratch with no pose prior. For reference, we also run 3D-GS \citep{kerbl20233dgs} and OmniGS \citep{li2024omnigs} as non-calibration SOTA baselines. However, apart from OmniGS, other baselines devised for perspective images are not compatible with omnidirectional images as input. To accommodate baselines for fair comparisons, we adopted two practices: 1) We converted each omnidirectional image into a cube map consisting of six perspective images, and then we took the cube maps as input to run the open-source systems with default configurations. 2) Following 360Roam~\citep{huang2022360roam}, we replaced the ray sampling functions of NeRF-based methods (BARF, L2G-NeRF, CamP) with omnidirectional ray sampling to support omnidirectional image training and rendering. Additionally, since point cloud initialization is demanded by 3D-GS based methods, we conducted experiments using different initialization strategies to further verify our system's robustness and flexibility.

%initialized 3D Gaussains from point clouds provided by 360Roam, or from omnidirectional depth maps in OmniBlender. 

\textbf{Datasets.} We evaluated SG-OmniGS against several SOTA models on datasets of 360-degree images, including eight real-world multi-room scenes from 360Roam dataset~\citep{huang2022360roam} each with on average 110 training views and 37 test views, and three synthetic single-room scenes from OmniBlender dataset \citep{choi2023balanced} each with 25 training views and 25 test views. 360Roam dataset utilizes camera poses estimated by SfM as ground truth and also provides SfM sparse point cloud. OmniBlender dataset provides noise-free camera poses and depth maps.

All methods were run on a desktop computer with an RTX 3090 GPU. 
We use metrics PSNR, SSIM, and LPIPS for evaluating novel view synthesis.  Please refer to Appendix~\ref{app:exp_detail} for details on camera perturbations and experimental setup.

%

%and converted 6 novel views in a cube map back to an omnidirectional image for evaluation. At inference, ground-truth test camera parameters are given to baselines for novel view synthesis. 
% For 3D-GS based methods, we initialized 3D Gaussains from point clouds provided by 360Roam, or from omnidirectional depth maps in OmniBlender. 


% All experiments followed these default configurations unless stated otherwise.
% Please refer to Appendix for perturbation details used in experiments. 
%The gradients of the omnidirectional camera model are maintained by the Pytorch automatic differentiation mechanism.


\subsection{Evaluation on Single-Room Synthetic Dataset} \label{sec:exp_omniblender}

\begin{table}[t]
    %\vskip -0.2cm  \citep{yun2023egformer} (est. depth)
    \caption{Quantitative comparisons on synthetic dataset OmniBlender. Checked "Perturb" indicates perturbed training camera poses for training, $\dag$ indicates training from scratch. 3D-GS based methods are marked with different point cloud initializations: random sampling (random), projection from an estimated mono-depth (est. depth), or from a rendered mono-depth (render depth). Methods marked with superscript $^\circ$ are modified via omnidirectional sampling. We mark the best two results in each experiment group with \colorfirsttext{first} and \colorsecondtext{second}.}
    \label{tab:omniblender}\label{tab:pcd_compare}
    
    \centering
    % \footnotesize
    \tabcolsep=0.03cm 
    % \renewcommand{\arraystretch}{1.2}
    \resizebox{\linewidth}{!}{
    \begin{tabular}{l c | ccc ccc ccc  ccc ccc ccc}
    \toprule
    \multirow{3}{*}{\makecell{On\\ OmniBlender}}& \multirow{3}{*}{Perturb} & \multicolumn{9}{c}{train} & \multicolumn{9}{c}{test} \\ 
    \cmidrule(lr){3-11} \cmidrule(lr){12-20}
    &&\multicolumn{3}{c}{$\mathbf{Barbershop}$}&\multicolumn{3}{c}{$\mathbf{Classroom}$}&\multicolumn{3}{c}{$\mathbf{Flat}$} &\multicolumn{3}{c}{$\mathbf{Barbershop}$}&\multicolumn{3}{c}{$\mathbf{Classroom}$}&\multicolumn{3}{c}{$\mathbf{Flat}$} \\
    \cmidrule(lr){3-5} \cmidrule(lr){6-8} \cmidrule(lr){9-11} \cmidrule(lr){12-14} \cmidrule(lr){15-17} \cmidrule(lr){18-20}
    & & {PSNR$\uparrow$} &{ SSIM$\uparrow$} & { LPIPS$\downarrow$} & { PSNR$\uparrow$} &{ SSIM$\uparrow$} & { LPIPS $\downarrow$} &{ PSNR$\uparrow$} &{ SSIM$\uparrow$} & { LPIPS$\downarrow$} & { PSNR$\uparrow$} &{ SSIM$\uparrow$} & { LPIPS$\downarrow$} & { PSNR$\uparrow$} &{ SSIM$\uparrow$} & { LPIPS$\downarrow$} &{ PSNR$\uparrow$} &{ SSIM$\uparrow$} & { LPIPS$\downarrow$}\\
    \midrule
    3D-GS (render depth)& $\times$ &31.308&0.922&0.093 &26.489&0.782&0.248 &30.274&0.882&0.149 &30.526&0.912&0.101 &25.794&0.766&0.262 &28.357&0.869&0.161\\
    OmniGS (render depth) & $\times$ &\markfirst{37.270}&\markfirst{0.971}&\markfirst{0.040} &\markfirst{32.565}&\markfirst{0.857}&\markfirst{0.161} &\markfirst{34.484}&\markfirst{0.928}&\markfirst{0.081} &\markfirst{35.485}&\markfirst{0.965}&\markfirst{0.043} &\markfirst{31.552}&\markfirst{0.846}&\markfirst{0.164} &\markfirst{33.477}&\markfirst{0.922}&\markfirst{0.083}\\	
    \midrule
    % \hline    
    OmniGS (render depth) &\checkmark &24.155&0.830&0.268 &20.175&0.699&0.399 &22.904&0.813&0.285  &17.717&0.595&0.446 &16.917&0.561&0.484 &18.768&0.700&0.372 \\		
    BARF & \checkmark &28.796&0.851&0.242 &25.854&0.741&0.309 &28.072&0.823&0.252 & 23.477&0.752&0.260 &{25.705}&{0.739}&{0.309} &{22.235}&{0.759}&{0.294} \\
    BARF$^\circ$&\checkmark &30.066&0.869&0.191 &29.204&0.768&0.261 &31.003&0.868&0.143 &29.739&0.866&0.191 &28.865&0.765&0.263 &30.417&0.649&0.144\\
    L2G-NeRF& \checkmark &29.023&0.858&0.222 &25.585&0.729&0.325 &27.970&0.825&0.243 &{28.749}&{0.856}&0.224 &18.064&0.597&0.408 &18.937&0.713&0.353 \\
    L2G-NeRF$^\circ$&\checkmark &30.083&0.870&0.188 &29.140&0.765&0.267 &31.020&0.866&0.145 &29.705& 0.867&0.189 &28.823&0.762&0.268 &30.576&0.863&0.146\\
    CamP & \checkmark &29.916&0.888&{0.185} &{26.774}&{0.813}&{0.181} &{29.440}&{0.864}&{0.179}
    &17.770&0.605&0.449 &16.258&0.553&0.558 &18.383&0.699&0.380 \\
    CamP$^\circ$&\checkmark &30.865&0.905&0.162& \marksecond{30.749}&\markfirst{0.884}&\markfirst{0.108}& 29.930&0.883&0.162 &17.892&0.688&0.391 & 15.948&0.544&0.549 &17.892&0.688&0.391\\
    \edit{Ours (random)}& \checkmark &36.255&0.960&0.066 &32.764&0.848&0.185 & \marksecond{34.476}&0.918&0.094 &\marksecond{34.719}&\marksecond{0.957}&0.062 &\marksecond{30.659}&\marksecond{0.827}&0.189 &\marksecond{33.344}&\marksecond{0.912}&0.096 \\
    \edit{Ours (est. depth)} &\checkmark &\marksecond{36.578}&\marksecond{0.964}&\marksecond{0.051} &\marksecond{33.066}&0.859&0.149 &33.256&\marksecond{0.922}&\markfirst{0.023} &34.404&0.952&\marksecond{0.055} &30.122&0.816&\marksecond{0.156}& 31.472&0.901&\marksecond{0.084}\\
    Ours (render depth)& \checkmark &\markfirst{37.612}&\markfirst{0.978}&\markfirst{0.028} &\markfirst{33.075}&\marksecond{0.875}&\marksecond{0.127} &\markfirst{35.240}&\markfirst{0.941}&\marksecond{0.063} &\markfirst{35.612}&\markfirst{0.972}&\markfirst{0.030} &\markfirst{31.151}&\markfirst{0.853}&\markfirst{0.132} &\markfirst{34.129}&\markfirst{0.935}&\markfirst{0.065} \\
    \midrule
    % \hline
    OmniGS (render depth) & $\dag$ &18.507&0.689&0.542 &17.160&0.622&0.555 &18.758&0.747&0.395 &18.431&0.678&0.542 &17.120&0.611&0.556 &18.728&0.744&0.395\\
    BARF & $\dag$ &27.871&0.823&0.296 &24.752&0.700&0.360 &27.621&0.814&0.269 &18.299&0.631&0.410 &16.794&0.564&0.455 &20.645&0.735&0.329\\
    BARF$^\circ$&$\dag$ &27.598 &0.807 &0.303 &25.869 &0.706 &0.360 &28.410 &0.820 &0.231 &27.508 &0.805 &0.303 &25.710 &0.703 &0.360 &28.140 &0.818 &0.231\\
    L2G-NeRF & $\dag$ &28.300&{0.840}&0.255 &25.623&0.731&0.324 &27.911&0.820&0.258 &20.165&0.679&0.317 &19.461&0.621&0.377 &18.921&0.714&0.359 \\
    L2G-NeRF$^\circ$&$\dag$ &{28.488} &0.834 &0.256 &{26.802} &0.719 &0.341 &29.152 &0.832 &0.209 &{28.198} &{0.830} &{0.256} &{26.300} &{0.714} &{0.342} &{28.717} &0.828 &0.211 \\
    CamP & $\dag$ &27.316&0.834&0.273 &25.738&0.767&0.255 &30.202&0.868&0.163
    &17.753&0.605&0.389 &15.420&0.526&0.493 &18.342&0.711&0.306\\
    CamP$^\circ$& $\dag$ & 27.818 & 0.839 & {0.241} &26.710&0.790&0.211 &32.169&0.891&0.116 &16.807&0.585&0.413 &14.664&0.501&0.490 &27.982&{0.856}&{0.124}\\	
    Ours (random) &$\dag$ & 35.196& \marksecond{0.953}& \marksecond{0.075}& 31.082& 0.833& 0.203& 32.614& 0.903& 0.111& \marksecond{33.422}& \marksecond{0.944}& \marksecond{0.084}& 28.971& \marksecond{0.806}& 0.214& \marksecond{31.673}& 0.895& 0.114 \\
    Ours (est. depth) &$\dag$& \marksecond{35.343}& 0.952& 0.082& \marksecond{32.294}& \marksecond{0.851}& \marksecond{0.166}& \marksecond{32.924}& \marksecond{0.915}& \marksecond{0.088}& 33.401& 0.940& 0.087& \marksecond{29.385}& 0.801& \marksecond{0.195}& 31.278& \marksecond{0.897}& \marksecond{0.094}\\
    % render depth&$\dag$& 35.601& 0.961& 0.060& 30.815& 0.846& 0.173& 33.064& 0.910& 0.110& 34.368& 0.956& 0.063& 30.212& 0.837& 0.176& 32.424& 0.906& 0.112\\
    Ours (render depth) & $\dag$ &\markfirst{35.601}&\markfirst{0.961}&\markfirst{0.060} &\markfirst{30.815}&\markfirst{0.846}&\markfirst{0.173} &\markfirst{33.064}&\markfirst{0.910}&\markfirst{0.110} &\markfirst{34.368}&\markfirst{0.956}&\markfirst{0.063} &\markfirst{30.212}&\markfirst{0.837}&\markfirst{0.176} &\markfirst{32.424}&\markfirst{0.906}&\markfirst{0.112} \\
    \bottomrule
    \end{tabular}
    }
    \vspace{-0.3cm}
\end{table}

%To compare estimated cameras with ground-truth camera parameters, 
We conducted experiments on three synthetic scenes from OmniBlender~\citep{choi2023balanced}, namely $\mathbf{Barbershop}$, $\mathbf{Classroom}$, and $\mathbf{Flat}$.
As depicted in Table~\ref{tab:omniblender}, we configured four settings of radiance field calibration, 
\begin{itemize}[topsep=0pt, itemsep=5pt,leftmargin=15pt] %,leftmargin=*
    \item Camera poses with perturbation and 3D Gaussians initialized from a single rendering depth map.
    \item No camera poses prior but 3D Gaussians initialized from a single rendering depth map.
    \item No camera poses prior but 3D Gaussians initialized from a single estimated depth map.
    \item No camera poses prior and random 3D Gaussians initialization.
\end{itemize}

In the first setting, we perturbed the training camera poses using the same preset noises, indicated by "$\checkmark$" under the "Perturb" column in Table~\ref{tab:omniblender}. OmniGS is the SOTA method in non-calibration omnidirectional radiance field reconstruction. When the input camera poses contain noticeable perturbance, OmniGS suffers significant performance degradation and struggles to synthesize clear novel views. BARF and L2G-NeRF exhibit acceptable performance with perturbed training cameras. After modifying ray sampling functions, we can effectively improve NeRF-based methods' performance, proving the necessity of properly treating omnidirectional images as a whole. However, we cannot apply a similar modification to 3D-GS based methods. It is non-trivial to achieve omnidirectional radiance field bundle adjustment, while our SC-OmniGS achieves dominant performance, on par with OmniGS trained with ground-truth cameras.

Additionally, we initialized all training cameras at the origin, enabling training the models from scratch without pose priors. This is denoted by a "$\dag$" under the "Perturb" column in Table~\ref{tab:omniblender}. 
In comparison to all baselines, our SC-OmniGS demonstrates stable and excellent performance. To verify SC-OmniGS flexibility and robustness, we utilized an omnidirectional monocular depth estimation method, e.g. EGformer \citep{yun2023egformer}, to estimate a depth map of the first image for 3D Gaussians initialization without the necessity of a known camera pose. Despite a slight decrease in rendering quality, the results demonstrate that our method still exhibits significant performance improvements compared to baseline methods. Finally, rather than using the rendered or estimated geometry as the starting point, we randomly sampled 300k points with random colors and positions as the initial 3D Gaussians to run our method. Our method is able to effectively optimize the scene representation, displaying a low sensitivity to initial values.

%starting from a coarse, uninformative initialization.

%\noindent\textbf{Flexibility.} We conducted additional experiments in terms of point cloud initialization to further demonstrate our method's robustness and flexibility. We recover omnidirectional radiance field from two more situations: (1) Randomly initialized point cloud without camera poses. (2) Estimated depth map without camera poses.  The evaluation results are reported in Table \ref{tab:pcd_compare} and \ref{tab:comp_360Roam}. Despite a slight decrease in rendering quality, the results demonstrate that SC-OmniGS still exhibits significant performance improvements compared to baseline methods. In addition, SC-OmniGS showcases robust performance even when scene point clouds are randomly initialized, displaying a low sensitivity to initial values.Rather than using the rendered or estimated geometry as the starting point, we randomly sampled 300k points with random colors as the initial point cloud, following the original 3D GS random initialization strategy.Even with this randomly initialized point cloud, SC-OmniGS was able to converge to high-quality results, outperforming the baseline methods. This shows that our method is able to effectively optimize the scene representation starting from a coarse, uninformative initialization.

%When trained from scratch, they gain a dramatic degradation in test view synthesis, with an average PSNR of less than 20. 

%Similar to its performance in real-world scenes, the non-calibration method OmniGS struggles to synthesize clear novel views when trained with perturbed training views. Furthermore, it fails to accurately model the scene when trained with no pose prior, as indicated by the reported poor metrics for both training and test views.
%However, 


Figures~\ref{fig:exp_pano_barber} and ~\ref{fig:exp_pano_class} display visual comparisons among calibration methods trained from scratch. Based on the conventional pinhole camera model, inaccurate camera optimization for individual perspective views leads to disconnected faces of a cube map, such as red insets of BARF and L2G-NeRF. In contrast, our omnidirectional camera model assists in optimizing cameras with concern about the holistic field of view, achieving a continuous synthesis.

\begin{figure*}[t]
    \def\imgw{0.24}
    \def\imgh{0.11}
    \def\namew{0.105}
    \centering
    \footnotesize
    \rotatebox{90}{ \parbox{\namew\linewidth}{\centering \scriptsize Ground truth}} 
    \includegraphics[width=\imgw\linewidth, height=\imgh\linewidth]{figure/exp/baseline_comp/gt_barber_78_box.jpg}
    \includegraphics[width=\imgw\linewidth, height=\imgh\linewidth]{figure/exp/baseline_comp/gt_class_22.jpg} 
    \includegraphics[width=\imgw\linewidth, height=\imgh\linewidth]{figure/exp/baseline_comp/gt_canteen_1_0008.jpg} 
    \includegraphics[width=\imgw\linewidth, height=\imgh\linewidth]{figure/exp/baseline_comp/gt_inno_1_0036.jpg}\\
    
    \rotatebox{90}{ \parbox{\namew\linewidth}{\centering \scriptsize BARF}} %\citeyear{lin2021barf}
    \includegraphics[width=\imgw\linewidth, height=\imgh\linewidth]{figure/exp/baseline_comp/barf_barber_78_cube2sphere_box.jpg}
    \includegraphics[width=\imgw\linewidth, height=\imgh\linewidth]{figure/exp/baseline_comp/barf_class_22_cube2sphere.jpg} 
    \includegraphics[width=\imgw\linewidth, height=\imgh\linewidth]{figure/exp/baseline_comp/barf_canteen_1_0008_cube2sphere.jpg} \includegraphics[width=\imgw\linewidth, height=\imgh\linewidth]{figure/exp/baseline_comp/barf_inno_1_0036_cube2sphere.jpg}  \\
    
    \rotatebox{90}{ \parbox{\namew\linewidth}{\centering \scriptsize L2G-NeRF}} %\citeyear{chen2023local}
    \includegraphics[width=\imgw\linewidth, height=\imgh\linewidth]{figure/exp/baseline_comp/l2g_barber_78_cube2sphere_box.jpg} \includegraphics[width=\imgw\linewidth, height=\imgh\linewidth]{figure/exp/baseline_comp/l2g_class_22_cube2sphere.jpg} 
    \includegraphics[width=\imgw\linewidth, height=\imgh\linewidth]{figure/exp/baseline_comp/l2g_canteen_1_0008_cube2sphere.jpg} \includegraphics[width=\imgw\linewidth, height=\imgh\linewidth]{figure/exp/baseline_comp/l2g_inno_1_0036_cube2sphere.jpg} \\
    
    \rotatebox{90}{ \parbox{\namew\linewidth}{\centering \scriptsize CamP}} %\citeyear{park2023camp}
    \includegraphics[width=\imgw\linewidth, height=\imgh\linewidth]{figure/exp/baseline_comp/camp_barber_78_cube2sphere_box.jpg} \includegraphics[width=\imgw\linewidth, height=\imgh\linewidth]{figure/exp/baseline_comp/camp_class_22_cube2sphere.jpg} 
    \includegraphics[width=\imgw\linewidth, height=\imgh\linewidth]{figure/exp/baseline_comp/camp_canteen_1_0008_cube2sphere.jpg} \includegraphics[width=\imgw\linewidth, height=\imgh\linewidth]{figure/exp/baseline_comp/camp_inno_1_0036_cube2sphere.jpg}  \\
    
    \rotatebox{90}{ \parbox{\namew\linewidth}{\centering \scriptsize Ours}} 
    \includegraphics[width=\imgw\linewidth, height=\imgh\linewidth]{figure/exp/baseline_comp/ours_barber_iter_30000_78_box.jpg}
    \includegraphics[width=\imgw\linewidth, height=\imgh\linewidth]{figure/exp/baseline_comp/ours_class_iter_30000_22.jpg} 
    \includegraphics[width=\imgw\linewidth, height=\imgh\linewidth]{figure/exp/baseline_comp/ours_canteen_iter_30000_1_0008.jpg} \includegraphics[width=\imgw\linewidth, height=\imgh\linewidth]{figure/exp/baseline_comp/ours_inno_iter_30000_1_0036.jpg} 

    \begin{subfigure}[b]{\imgw\linewidth}
        \vspace{-0.1cm}
        \caption{$\mathbf{Barbershop}^\dag$}\label{fig:exp_pano_barber}
    \end{subfigure}
    \begin{subfigure}[b]{\imgw\linewidth}
        \vspace{-0.1cm}
        \caption{$\mathbf{Classroom}^\dag$}\label{fig:exp_pano_class}
    \end{subfigure}
    \begin{subfigure}[b]{\imgw\linewidth}
        \vspace{-0.1cm}
        \caption{$\mathbf{Canteen}$}
    \end{subfigure}
    \begin{subfigure}[b]{\imgw\linewidth}
        \vspace{-0.1cm}
        \caption{$\mathbf{Innovation}$}
    \end{subfigure}
    
    % \vspace{-0.2cm}
    \caption{Qualitative comparisons of 360-degree novel views among calibration methods. Our results outperform in both rendering quality and camera accuracy. $\dag$ indicates training from scratch. }
    % trained with camera perturbation and with no pose prior.
    \label{fig:exp_pano_comp}
    % \vspace{-0.5cm}
\end{figure*}

\subsection{Evaluation on Multi-Room Real-World Dataset} \label{sec:exp_360roam}
\begin{table*}[t]
    \caption{Quantitative comparisons on real-world dataset 360Roam.  
    ``Point Init" indicates the way of point cloud initialization for 3D-GS based methods, checked ``Perturb" indicates perturbed camera poses as inputs, ``train" and ``test" indicate training and test views, respectively. Methods marked with superscript $^\circ$ are modified via omnidirectional sampling. We mark the best two results in each experiment group with \colorfirsttext{first} and \colorsecondtext{second}.}
    \label{tab:comp_360Roam}
    \centering    
    \tabcolsep=0.07cm 
    \resizebox{0.7\linewidth}{!}{
    \begin{tabular}{lcc|ccc ccc}
    \toprule
    \multirow{2}{*}{\makecell{ On 360Roam}} &\multirow{2}{*}{Perturb} & \multirow{2}{*}{Point Init} &\multicolumn{3}{c}{train}&\multicolumn{3}{c}{test}\\
    \cmidrule(lr){4-6} \cmidrule(lr){7-9} 
    
    &&& { PSNR$\uparrow$} &{ SSIM$\uparrow$} & { LPIPS$\downarrow$} & { PSNR$\uparrow$} &{ SSIM$\uparrow$} & { LPIPS$\downarrow$} \\
    \midrule
    3D-GS \citep{kerbl20233dgs} &$\times$& SfM &23.943&0.744&0.223 &20.791&0.684&0.261\\
    %OmniGS \citep{li2024omnigs} &$\times$ &Random &26.142&0.796&0.233 &23.619&0.744&0.258 \\
    OmniGS \citep{li2024omnigs} &$\times$ &SfM &\marksecond{28.517}&\marksecond{0.861}&\markfirst{0.137}  &\marksecond{24.212}&\marksecond{0.768}&\markfirst{0.176}\\
    %SC-OmniGS (Ours) &$\times$ &Random &&& &&& \\
    SC-OmniGS (Ours) &$\times$ &SfM &\markfirst{29.495}&\markfirst{0.877}&\marksecond{0.141}& \markfirst{25.297}&\markfirst{0.803}&\marksecond{0.180} \\
    \hline\hline
    
    OmniGS \citep{li2024omnigs} &$\checkmark$ &SfM &22.111&0.705&0.334 &15.619&0.455&0.489\\
    BARF \citep{lin2021barf} &\checkmark&N/A&21.699&0.594&0.465 &20.200&0.572&0.481\\
    BARF$^\circ$ \citep{lin2021barf} &\checkmark&N/A &22.136&0.575&0.492 &20.484&0.546&0.510 \\
    L2G-NeRF \citep{chen2023local} &\checkmark&N/A&21.797&0.598&0.460 &20.507&0.576&0.473\\
    L2G-NeRF$^\circ$ \citep{chen2023local}  &\checkmark&N/A &22.581&0.590&0.462 &20.023&0.542&0.495 \\
    CamP \citep{park2023camp}  &\checkmark&N/A&24.592&0.735&0.264 &14.253&0.438&0.573\\
    CamP$^\circ$ \citep{park2023camp}  &\checkmark&N/A &26.134&0.786&0.239 &13.659&0.437&0.622 \\
    % \midrule
    
    SC-OmniGS (Ours) &$\checkmark$ & Random &\marksecond{28.562}&\marksecond{0.852}&\marksecond{0.175} &\marksecond{24.343}&\marksecond{0.770}&\marksecond{0.224} \\
    SC-OmniGS (Ours) &\checkmark&SfM&\markfirst{29.232}&\markfirst{0.872}&\markfirst{0.147}&\markfirst{24.910}&\markfirst{0.790}&\markfirst{0.188}\\ %nscmpose_noise
    
    % \hline\hline
    % SC-OmniGS (Ours) &$\dag$ & SfM &20.537&0.635&0.552 &16.141&0.550&0.632 \\ 
    \bottomrule
    \end{tabular}
    }
\end{table*}

% To measure the performance and robustness in real-world scenarios, we conducted evaluations on eight multi-room scenes in dataset 360Roam \citep{huang2022360roam}.
In real-world scenarios, we studied three situations of SC-OmniGS and reported the average metric scores across scenes in Table~\ref{tab:comp_360Roam}: 
\begin{itemize}[topsep=0pt, itemsep=5pt,leftmargin=15pt] %,leftmargin=*
    \item SfM camera poses without perturbation and 3D Gaussians initialized from SfM point clouds.
    \item SfM camera poses with perturbation and 3D Gaussians initialized from SfM point clouds.
    \item SfM camera poses with perturbation and random 3D Gaussians initialization.
\end{itemize}

%we utilized SfM non-perturbed cameras for non-calibration methods (3D-GS and OmniGS), and applied preset perturbation to training camera poses for calibration baselines (BARF, L2G-NeRF, CamP) and OmniGS for reference. 
%Table~\ref{tab:comp_360Roam} reports the average metric scores across scenes, while Figure~\ref{fig:exp_pano_comp} showcases visual comparisons.

% 0.  OmniGS w/o calibration 
%Without camera calibration, radiance field reconstruction is vulnerable to inaccurate camera parameters. For example, when trained with perturbed cameras, OmniGS exhibits a significant performance decrease compared to models trained with non-perturbed cameras, particularly in the context of test view synthesis.
% 1. we work better than non-calibration methods trained with estimated ground-truth camera parameters.
% 2. we outperform among all calibration baselines

Real-world omnidirectional images captured by 360-degree cameras inherit the distortion from each lens and result in a complex distortion pattern. However, most methods leverage an ideal spherical camera model to describe omnidirectional projection while overlooking the impact of 360-degree camera distortion.
With our proposed calibration approach, SC-OmniGS can further optimize camera parameters in particular the camera intrinsic model, eventually outperforming the non-calibration method OmniGS trained with SfM cameras,  as demonstrated in the first block of Table~\ref{tab:comp_360Roam}.
%achieves the best performance among all calibration methods and performs competitively with the non-calibration method OmniGS trained with SfM cameras. 
Under the situation of camera perturbation, SC-OmniGS demonstrates consistent performance across both training and test views, no matter how 3D Gaussians are initialized. 

As visualized in Figure~\ref{fig:exp_pano_comp}, our SC-OmniGS also dominates qualitative performance in omnidirectional scenarios.
BARF and L2G-NeRF tend to synthesize low-quality and blurry images, while CamP generates floating fuzzy artifacts, albeit with some high-frequency details. Please refer to Appendix \ref{sec:app_more_results} for more quantitative and qualitative comparison results.


%with sparse training views, we found CamP often downgrades the quality of test-view optimization and synthesis (see fourth row in Figure~\ref{fig:exp_pano_comp}), despite achieving acceptable training view quality with good photometric evaluation numbers. 



% Particularly, in seldom-seen viewpoints, BARF and L2G-NeRF struggle to produce smooth and continuous panoramas from perspective cube maps with erroneous individual camera calibration. \textcolor{red}{[TODO: show some zoom in if possible, e.g. barbershop]}


\subsection{Robustness and Analysis of SC-OmniGS}\label{sec:robustness}
\textbf{Robustness.}
To further assess the robustness of our method against varying levels of camera perturbation, we conducted experiments using the same learning rate with increasing scales of translation and rotation noise applied to the training cameras.
In Figure~\ref{fig:eval_robust}, we visualize the performance trend depicting the impact of increasing noise scales on the synthetic scene $\mathbf{Barbershop}$ and the real-world scene $\mathbf{Lab}$. 
In the left charts of Figures~\ref{fig:eval_robust_barber} and~\ref{fig:eval_robust_lab}, we fixed the default rotation noise scale and varied translation noise scales, while the right charts represent variable rotation noise scale and fixed translation noise scale. 
% while in the right charts, we fixed the default translation noise scale and varied the rotation noise scales.
Our camera calibration demonstrates greater robustness to translation errors with only minor degradation compared to rotation errors.  Furthermore, when compared to other calibration baselines (see $\mathbf{Barbershop}$ in Table~\ref{tab:omniblender}), SC-OmniGS consistently outperforms them with most increased rotation noise scales.

\begin{figure}[t]
    \centering
    \footnotesize
    \def\resw{0.248}
    % \includegraphics[width=\resw\linewidth,height=0.16\linewidth]{figure/exp/ablation/psnr_tran_v3.png} \includegraphics[width=\resw\linewidth,height=0.16\linewidth]{figure/exp/ablation/psnr_rot_v3.png}\\
    % \includegraphics[width=\resw\linewidth,height=0.16\linewidth]{figure/exp/ablation/psnr_tran_v3_lab.png} \includegraphics[width=\resw\linewidth,height=0.16\linewidth]{figure/exp/ablation/psnr_rot_v3_lab.png}
    \begin{subfigure}[b]{0.49\linewidth}
        \centering
        \includegraphics[width=0.49\linewidth]{figure/exp/ablation/psnr_tran_v3_3.pdf}
        \includegraphics[width=0.49\linewidth]{figure/exp/ablation/psnr_rot_v3_3.pdf}
        
        % \vspace{-0.2cm}
        \caption{Synthetic scene $\mathbf{Barbershop}$.}\label{fig:eval_robust_barber}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\linewidth}
        \centering
        \includegraphics[width=0.49\linewidth]{figure/exp/ablation/psnr_tran_v3_lab3.pdf}
        \includegraphics[width=0.49\linewidth]{figure/exp/ablation/psnr_rot_v3_lab3.pdf}
        
        % \vspace{-0.2cm}
        \caption{Real-world scene $\mathbf{Lab}$.} \label{fig:eval_robust_lab}
    \end{subfigure}    
    
    % \vspace{-0.1cm}
    \caption{Performance with different camera perturbations (PSNR$\uparrow$). Zoom in for details. }
    \label{fig:eval_robust}
    % \vspace{-0.3cm}
\end{figure}

\textbf{Ablation study.} 
As a novel self-calibrating omnidirectional radiance fields method, SC-OmniGS proposed two main components, i.e. a generic omnidirectional camera model and camera pose optimization.
% \textit{What are effects of our camera calibrations? }  of the camera calibration components, i.e. camera model and camera poses. 
To validate the effectiveness of our camera calibration, we conducted ablation studies on a real scene $\mathbf{Center}$, with and without perturbation to training cameras. The results are presented in Table~\ref{tab:ablation}. 
%A high-fidelity radiance field reconstruction relies on proper camera calibration.
When the input camera poses are estimated by SfM without perturbation, we can slightly increase the quality of radiance field reconstruction by camera pose refinement, although its performance gain is not higher than adding an omnidirectional camera model. When trained with pose perturbation, our full model, incorporating both camera model and pose optimization, consistently achieves improvement in both training and test view synthesis.

%In the context of an omnidirectional real scene, the camera model is not perfect and may exhibit distortion, as depicted in Sec.~\ref{sec:camera_model}. Therefore, accurate camera estimation relies on proper calibration of the camera model. 
% With inevitable distortion, camera model calibration plays an essential role in accurate camera estimation. 
%For instance, when trained without pose perturbation, the models with camera model optimization perform better than the other two variants. When trained with pose perturbation, our full model, incorporating both camera model and pose optimization, consistently gains improvement in both training and test view synthesis.

% \textit{What are effects of different point cloud initializations? } 




\begin{table}[t!]
    %\vskip -0.2cm
    \caption{Ablation study on scene $\mathbf{Center}$ of 360Roam, in terms of the optimization of camera pose, camera model, or both. "Perturb" indicates perturbed camera poses, "train" and "test" indicate training and test views, respectively. We mark the best two results with \colorfirsttext{first} and \colorsecondtext{second} .}
    \label{tab:ablation}
    
    \centering
    % \footnotesize
    \tabcolsep=0.05cm 
    \resizebox{1\linewidth}{!}{
    \begin{tabular}{l | cccccc cccccc }
    \toprule
    \multirow{3}{*}{\makecell[l]{\\Calibration}}&\multicolumn{6}{c}{w/o Perturb}&\multicolumn{6}{c}{w/ Perturb}\\
    \cmidrule(lr){2-7} \cmidrule(lr){8-13} 
    &\multicolumn{3}{c}{train}&\multicolumn{3}{c}{test} &\multicolumn{3}{c}{train}&\multicolumn{3}{c}{test}\\    
    \cmidrule(lr){2-4} \cmidrule(lr){5-7} \cmidrule(lr){8-10} \cmidrule(lr){11-13} 
    & { PSNR $\uparrow$} &{ SSIM $\uparrow$} & { LPIPS $\downarrow$} & { PSNR $\uparrow$} &{ SSIM $\uparrow$} & { LPIPS $\downarrow$} & { PSNR $\uparrow$} &{ SSIM $\uparrow$} & { LPIPS $\downarrow$} & { PSNR $\uparrow$} &{ SSIM $\uparrow$} & { LPIPS $\downarrow$} \\
    \midrule
    % \hline
    none  &28.728&0.848&0.170 &24.264&0.763&0.213 &22.740&0.717&0.372 &15.597&0.510&0.553\\
    $+$camera model &\markfirst{30.230}&\markfirst{0.877}&\markfirst{0.153} &\marksecond{25.123}&\marksecond{0.795}&\markfirst{0.195} &22.743&0.730&0.408 &15.702&0.543&0.568\\ 
    % center train: PSNR:22.7426 SSIM:0.7300 LPIPS:0.4083
    % test: PSNR:15.7020 SSIM:0.5427 LPIPS:0.5681
    $+$pose  &28.334&0.837&0.191 &24.906&0.781&0.224 &\marksecond{28.130}&\marksecond{0.834}&\marksecond{0.198} &\marksecond{24.739}&\marksecond{0.777}&\marksecond{0.233}\\
    $+$camera model$+$pose &\marksecond{30.035}&\marksecond{0.872}&\marksecond{0.169} &\markfirst{25.802}&\markfirst{0.813}&\marksecond{0.203} &\markfirst{29.706}&\markfirst{0.867}&\markfirst{0.177} &\markfirst{25.304}&\markfirst{0.799}&\markfirst{0.220}\\
    \bottomrule
    \end{tabular}
    }
    % \vspace{-0.5cm}
\end{table}

\begin{comment}
    
\begin{table}[t!]
    \caption{Comparisons among different point cloud initializations on synthetic dataset OmniBlender.  $\dag$ indicates training from scratch, "Point Init" is point cloud initialization by random sampling, projection from an estimated mono-depth \citep{yun2023egformer} or from a rendered mono-depth, "train" and "test" indicate training and test views, respectively. }
    % We mark the best two results with \colorfirsttext{first} and \colorsecondtext{second}.
    \label{tab:pcd_compare}
    \centering
    % \footnotesize
    \tabcolsep=0.03cm 
    \resizebox{\linewidth}{!}{
    \begin{tabular}{l | ccc ccc ccc  ccc ccc ccc}
    \toprule
    \multirow{3}{*}{\makecell{Point Init}} & \multicolumn{9}{c}{train} & \multicolumn{9}{c}{test} \\ 
    \cmidrule(lr){2-10} \cmidrule(lr){11-19}
    &\multicolumn{3}{c}{$\mathbf{Barbershop}$}&\multicolumn{3}{c}{$\mathbf{Classroom}$}&\multicolumn{3}{c}{$\mathbf{Flat}$} &\multicolumn{3}{c}{$\mathbf{Barbershop}$}&\multicolumn{3}{c}{$\mathbf{Classroom}$}&\multicolumn{3}{c}{$\mathbf{Flat}$} \\
    \cmidrule(lr){2-4} \cmidrule(lr){5-7} \cmidrule(lr){8-10} \cmidrule(lr){11-13} \cmidrule(lr){14-16} \cmidrule(lr){17-19}
    & { PSNR$\uparrow$} &{ SSIM$\uparrow$} & { LPIPS$\downarrow$} & { PSNR$\uparrow$} &{ SSIM$\uparrow$} & { LPIPS $\downarrow$} &{ PSNR$\uparrow$} &{ SSIM$\uparrow$} & { LPIPS$\downarrow$} & { PSNR$\uparrow$} &{ SSIM$\uparrow$} & { LPIPS$\downarrow$} & { PSNR$\uparrow$} &{ SSIM$\uparrow$} & { LPIPS$\downarrow$} &{ PSNR$\uparrow$} &{ SSIM$\uparrow$} & { LPIPS$\downarrow$}\\
    \midrule   
    random$\dag$ & 35.196& 0.953& 0.075& 31.082& 0.833& 0.203& 32.614& 0.903& 0.111& 33.422& 0.944& 0.084& 28.971& 0.806& 0.214& 31.673& 0.895& 0.114 \\
    est. depth$\dag$& 35.343& 0.952& 0.082& 32.294& 0.851& 0.166& 32.924& 0.915& 0.088& 33.401& 0.940& 0.087& 29.385& 0.801& 0.195& 31.278& 0.897& 0.094\\
    render depth$\dag$& 35.601& 0.961& 0.060& 30.815& 0.846& 0.173& 33.064& 0.910& 0.110& 34.368& 0.956& 0.063& 30.212& 0.837& 0.176& 32.424& 0.906& 0.112\\
    \bottomrule
    \end{tabular}
    }
\end{table}
\end{comment}

% \subsection{Observation and Discussion}
% \subsection{Limitation}\label{sec:limitation}

% \vspace{-0.2cm}
\section{Conclusion} \label{sec:conclusion}
% \vspace{-0.3cm}
This paper introduces SC-OmniGS, the first self-calibrating omnidirectional Gaussian splatting system that enables swift and accurate reconstruction of omnidirectional radiance fields. With the differentiable omnidirectional camera model and Gaussian splatting procedure, our approach jointly optimizes 3D Gaussians, omnidirectional camera poses and camera model, leading to robust camera optimization and enhanced reconstruction quality. Extensive experiments validate the effectiveness of SC-OmniGS in recovering high-quality omnidirectional radiance fields, either with noisy poses or without pose prior. Our work offers an efficient and precise omnidirectional radiance field reconstruction for potential applications in virtual reality, robotics, and autonomous navigation.
% \textbf{Limitation.} SC-OmniGS supports large-scale scene reconstruction such as multi-room scenes with sparse training cameras, but still cannot support training from scratch for large scenes with sparse views. 

\textbf{Limitation.} When confronted with challenging omnidirectional scenes, i.e., multi-room-level scenes with sparse \edit{discrete} views, training from scratch is a challenging task without the assistance of a typical SfM pipeline. We conducted an additional training from scratch experiment using the 360Roam dataset. All self-calibration methods fail to learn radiance fields without any pose priors while our SC-OmniGS is no exception. To address these issues, integrating SC-OmniGS into an omnidirectional SLAM framework is a promising direction, which can be a future work.
%Nevertheless, SC-OmniGS holds promise to reconstruct large scenes with no pose prior by incorporating sequential video frames.

%SC-OmniGS still cannot support training from scratch with sparse views for large scenes, such as multi-room scenes. 

% \subsubsection*{Acknowledgments}
% Use unnumbered third level headings for the acknowledgments. All
% acknowledgments, including those to funding agencies, go at the end of the paper.

% Bibliography
\bibliography{bibliography}
\bibliographystyle{iclr2025_conference}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%% Appendix %%%%
\newpage

% \appendix
% \section{Appendix}
% You may include other additional sections here.

\section*{Appendix}
\appendix

\section{Societal Impacts}\label{sec:social_impact}
This research explored the efficient and robust self-calibrating omnidirectional radiance field for large omnidirectional scenarios, experimenting with real-world data captured with the consumer-grade 360-degree camera and synthetic data. It has broad potential impacts and applications in the real world. For example, it supports real-time photorealistic rendering for virtual environments, which enhances virtual immersiveness and enables mixed-reality production. In addition, it can be incorporated into SLAM techniques to upgrade localization robustness.


\section{Experiment Details} \label{app:exp_detail}

\begin{comment}
    
\subsection{Implementation Detail}\label{sec:imple_detail}
Our SC-OmniGS implementation is built on Pytorch and CUDA. The gradients of the omnidirectional camera model are maintained by the Pytorch automatic differentiation mechanism. We utilize Adam optimizer to update trainable parameters. The hyperparameters for 3D Gaussians optimization are set according to the default settings of 3D-GS~\citep{kerbl20233dgs}, with $\lambda=0.2$ and a total of 30,000 optimization iterations. We set the ratio threshold $\gamma$ to 10.
Moreover, we set the learning rate of the camera model $\Theta$ to 1e-4 and activate the angle distortion coefficients $\mathbf{\mathcal{D}}$ using the Tanh function. 
For simplicity, we fix $f_t$ to 1. The initial learning rates for each camera quaternion $\mathbf{q}$ and translation $\mathbf{t}$ are set to 0.01, with exponential decay to 1.6e-4 and 6e-3, respectively, in 100 steps per camera. When calibrating from scratch, we increase the initial learning rate of $\mathbf{t}$ to 0.1. Furthermore, we observe that noisy camera poses can lead to the generation of numerous incorrect 3D Gaussians at the beginning of optimization, making it challenging to filter them out during optimization. To address this, we re-initialize the 3D Gaussian with the input coarse points twice, at the $2000^{th}$ and $4000^{th}$ iterations.

\end{comment}

\subsection{Pseudo-code of Differentiable Omnidirectional Camera Model}
Algorithm~\ref{alg:camera_model} illustrates the backpropagation process and the usage of the proposed generic camera model.

\begin{algorithm}[h]
\caption{Differentiable Omnidirectional Camera Model} \label{alg:camera_model}
\KwIn{ input image $I$ }
% \KwResult{ undistorted image $I^o$}
% \texttt{\\}
{\bf/* Initialization */}\\
$H, W, C \gets$ image dimension of $I$\;
$\mathbf{u} \gets $ image pixel coordinates\; 
$\mathbf{S} \gets \phi'(\mathbf{u})$;   \tcp{project UV back to camera space} 
% $\mathcal{S} \gets$ ideal spherical grid in dimension $(H, W, C)$\;
$f_t \gets 1$; \tcp{focal length coefficient}
$\mathcal{D} \gets$ initialize as zeros in in dimension $(H, W, 3)$\; 
$\mathcal{D} \gets$ enable gradients; \tcp{learnable angle distortion coefficients} 
% Omnidirectional camera model $\Theta$ consists of learnable angle distortion coefficients $\mathcal{D}$, and focal length coefficient $f_t$ \\
\texttt{\\}
{\bf/* Image Undistortion */} \\

% $\mathcal{D} \gets$ backpropagate and update via total loss $\mathcal{L}$; \\
$\mathcal{D} \gets Tanh(\mathcal{D})$ ;  \tcp{apply activation function}
$\hat{\mathbf{S}} \gets \mathbf{S} \cdot f_t + \mathbf{S} \odot \mathcal{D} $;  \tcp{ Eq.~\ref{eq:camera_model} }
$\hat{\mathbf{u}} \gets \phi(\hat{\mathbf{S}})$; \tcp{ undistorted UV coordinates }
Output undistorted image $I^o \gets grid\_sample(I, \hat{\mathbf{u}})$;     \tcp{bicubic grid sample }
\texttt{\\}

$\mathcal{D} \gets$ backpropagate and update via total loss $\mathcal{L}$\;


\end{algorithm}

\subsection{Datasets} \label{app:exp_dataset}
\paragraph{360Roam.} 360Roam \citep{huang2022360roam} provides 360-degree captured images by a consumer-grade 360-degree camera for indoor scenes with multiple rooms, and corresponding initial sparse point clouds from SfM. We selected eight scenes with relatively large scales for evaluation, including $\mathbf{Bar}$, $\mathbf{Base}$, $\mathbf{Cafe}$, $\mathbf{Canteen}$ $\mathbf{Center}$, $\mathbf{Innovation}$, $\mathbf{Lab}$, and $\mathbf{Library}$. All data are under CC BY-NC-SA 4.0 license.

\paragraph{OmniBlender.} OmniBlender~\citep{choi2023balanced} contains multi-view 360-degree images rendered from Blender synthetic single indoor scenes under MIT License. It provides ground-truth camera parameters, and we additionally rendered a ground-truth depth map of each scene to initialize a sparse point cloud for 3D-GS based methods.

The synthetic Blender scene $\mathbf{Classroom}$ is under CC0 license, $\mathbf{Barbershop}$ and $\mathbf{Flat}$ are under CC-BY 4.0 license. All original models can be downloaded in \url{https://www.blender.org/download/demo-files/}.

\subsection{Perturbation Details} \label{app:exp_perturb}
In comparison experiments in Sec.~\ref{sec:exp_360roam} and ~\ref{sec:exp_omniblender}, we add translation noise to SfM or ground-truth camera translation, and multiply rotation by rotation noise. 
Specifically, we set translation perturbation noise $T_{noise}= \alpha T_{scale} \times inv\_r$, where $\alpha$ is random samples from a uniform distribution over $[-1,1)$, default $T_{scale} = 0.5$, and $inv\_r$ is the inverse of maximum radius of camera positions for scale normalization. We set rotation perturbation noise $R_{noise} = \beta R_{scale}$, where $\beta$ is normalized rotation direction with dimensional values randomly sampled from a normal distribution over the angle range $[-1^\circ, 1^\circ)$, and default $R_{scale} = 0.5$. Finally, we get preset perturbed translation $\hat{T}$ and rotation $\hat{R}$:
\[
\hat{T} = T + T_{noise}, 
\hat{R} = R \times R_{noise}.
\]

%pos_noise_scales = [0.05, 0.06, 0.07, 0.08, 0.09, 0.11, 0.12, 0.13] # for 360Roam
% pos_noise_scales = [0.5, 0.6, 0.7, 0.8, 0.9, 1.1, 1.2, 1.3] # for blender
% rot_noise_scales = [0.5, 1, 2, 3, 4, 5, 6, 7]

In Sec.~\ref{sec:robustness} for robustness measurement, we fixed rotation noise scale $R_{scale} = 0.5$ and changed translation noise scale with $T_{scale} \in [0.5, 0.6, 0.7, 0.8, 0.9, 1.1, 1.2, 1.3]$, and also fixed translation with noise scale $T_{scale} = 0.5$ and changed rotation noise scale with $R_{scale} \in [0.5, 1, 2, 3, 4, 5, 6, 7]$.

\subsection{Baselines}
We trained experimental models by all baselines, i.e., 3D-GS~\citep{kerbl20233dgs}, OmniGS~\citep{li2024omnigs}, BARF~\citep{lin2021barf}, L2G-NeRF~\citep{chen2023local}, CamP~\citep{park2023camp}, using their official published source codes and default training configurations.
The baseline authors hold all the ownership rights on their software. 

By default, we convert each 360-degree image in Appendix~\ref{app:exp_dataset} into a cube map with six non-overlapped $480 \times 480$ perspective images and re-computed six camera parameters. BARF, L2G-NeRF and CamP trained scenes using converted perspective training and test images. In particular, we increase training iterations of 3D-GS to six fold, i.e., 180,000 iterations for each scene for a fair comparison.

In addition, we modified the calibration baselines, i.e., BARF, L2G-NeRF and CamP, by replacing original perspective ray sampling with omnidirectional ray sampling for training and rendering. These modified baselines, OmniGS and our SC-OmniGS trained scenes using resolution $760 \times 1520$ for 360Roam dataset and $1000 \times 2000$ for OmniBlender dataset.


\subsection{Runtime}
% 60 hours
Table \ref{tab:runtime} reports the quantitative comparisons of training time and inference speed among different methods.
On average, for a scene with a GeForce RTX  3090 GPU, BARF trains for over 2 days, L2G-NeRF and CamP for half a day, 3D-GS (six-fold iterations), OmniGS and our SC-OmniGS within 30 minutes.  It is noted that SC-OmniGS does not increase much training time with camera self-calibration compared to OmniGS without camera calibration, meanwhile SC-OmniGS supports real-time rendering.
% CamP ran for 15 hours with 4 3090 GPUs.
% , or 1.5 days with one 3090 GPU.
\begin{table}[h]
    \centering
    \begin{tabular}{c|c c}
        \toprule
         Method & Training time & Rendering speed for one panorama (FPS)  \\
         \midrule
         BARF&$>$ 2 days &$<$ 0.05 \\
         L2G-NeRF&$>$ 12 hours &$<$ 0.05 \\
         CamP&$>$ 12 hours &$<$ 0.2 \\
         3D-GS&30 mins &$>$ 60 \\
         OmniGS	&30 mins &	$>$ 60  \\
         SC-OmniGS &	30 mins	& $>$ 60 \\
         \bottomrule
    \end{tabular}
    \caption{Runtime comparison for methods running on one GeForce RTX 3090 GPU.}
    \label{tab:runtime}
\end{table}



\begin{table}
\centering
\caption{Ablation study. "Re-init" indicates re-initialization of 3D Gaussians; w/o $\mathcal{L}_{wsp}$ means we disable the spherical weight and calculate classical photometric loss for optimization; "Perturb" indicates perturbation; $\dag$ indicates training from scratch without pose priors. We mark the best two results with \colorfirsttext{first} and \colorsecondtext{second}.} %"Perturb" indicates perturbed camera poses;
    \label{tab:app_ablate}
\resizebox{0.6\linewidth}{!}{
    \begin{tabular}{l c| HHH ccc}
    \toprule
    % \multirow{2}{*}{$\mathbf{Classroom}$} & \multirow{2}{*}{Perturb} & \multicolumn{3}{c}{train}&\multicolumn{3}{c}{test}\\  \cmidrule(lr){3-5} \cmidrule(lr){6-8}
    % &&{ PSNR $\uparrow$} &{ SSIM $\uparrow$} & { LPIPS $\downarrow$} & { PSNR $\uparrow$} &{ SSIM $\uparrow$} & { LPIPS $\downarrow$} \\    
    $\mathbf{Classroom}$ & Perturb & { PSNR $\uparrow$} &{ SSIM $\uparrow$} & { LPIPS $\downarrow$} & { PSNR $\uparrow$} &{ SSIM $\uparrow$} & { LPIPS $\downarrow$} \\
    \midrule
    w/o Re-init &$\dag$& 30.271 &0.843 &0.180 & 29.183&\marksecond{0.823}&0.193\\
    w/o $\mathcal{L}_{wsp}$ &$\dag$&\marksecond{30.706} &\markfirst{0.862} &\markfirst{0.152} &\marksecond{28.225} &0.811&\marksecond{0.192}\\
    \midrule
    Ours&$\dag$& \markfirst{30.815}&\marksecond{0.846}&\marksecond{0.173} &\markfirst{30.212} & \markfirst{0.837} & \markfirst{0.176} \\
    \bottomrule
    \end{tabular}
}
\end{table}

\begin{figure}
    \centering
    \def\imgw{0.32}
    \def\imgh{0.16}
    \includegraphics[width=\imgw\linewidth, height=\imgh\linewidth]{figure/exp/app_ablation/18.jpg} \includegraphics[width=\imgw\linewidth, height=\imgh\linewidth]{figure/exp/app_ablation/predict_iter_30000_18_nowsp.jpg}
    \includegraphics[width=\imgw\linewidth, height=\imgh\linewidth]{figure/exp/app_ablation/predict_iter_30000_18_ours.jpg}\\
    \includegraphics[width=\imgw\linewidth, height=\imgh\linewidth]{figure/exp/app_ablation/62.jpg} \includegraphics[width=\imgw\linewidth, height=\imgh\linewidth]{figure/exp/app_ablation/predict_iter_30000_62_nowsp.jpg}
    \includegraphics[width=\imgw\linewidth, height=\imgh\linewidth]{figure/exp/app_ablation/predict_iter_30000_62_ours.jpg}\\ 
    \begin{subfigure}[b]{\imgw\linewidth}
        \centering
        \caption{Ground truth}
    \end{subfigure}
    \begin{subfigure}[b]{\imgw\linewidth}
        \centering
        \caption{w/o $\mathcal{L}_{wsp}$}
    \end{subfigure}
    \begin{subfigure}[b]{\imgw\linewidth}
        \centering
        \caption{Ours}
    \end{subfigure}
    
    \caption{Ablation study of weighted spherical photometric loss $\mathcal{L}_{wsp}$. Without using $\mathcal{L}_{wsp}$, the estimated poses of some cameras suffer obvious errors leading to performance degradation in novel view synthesis.} %w/o $\mathcal{L}_{wsp}$ means we disable the spherical weight and calculate classical photometric loss for optimization.
    \label{fig:app_ablate}
\end{figure}

\section{More Experiment Results}\label{sec:app_more_results}

\subsection{Additional Ablation Study}
Considering the characteristic of the omnidirectional image, we introduce a weighted spherical photometric loss $\mathcal{L}_{wsp}$ as defined in Eq.~\ref{eq:photoloss} for spatially equivalent optimization. 
Furthermore, we observe that noisy camera poses can lead to the generation of numerous incorrect 3D Gaussians at the beginning of optimization, making it challenging to filter them out during optimization. To address this, we re-initialize the 3D Gaussian with the input coarse points twice, at the $2000^{th}$ and $4000^{th}$ iterations. 
% In addition, we re-initialize 3D-GS twice in order to eliminate numerous incorrect 3D Gaussians due to noisy camera poses at the beginning of optimization.
To further verify the effect of the weighted spherical photometric loss and calibration strategy, we conducted additional experiments on $\mathbf{Classroom}$ as an ablation study. The test view results are reported in Table~\ref{tab:app_ablate} and Figure~\ref{fig:app_ablate}.
%with and without weighted spherical photometric loss, referred to as Eq. \ref{eq:anisotropy} in Sec. \ref{sec:joint_opt}.

% \subsection{Effect of Weighted Spherical Photometric Loss}



\subsection{More Quantitative and Qualitative Comparisons}
\edit{We report the complete image quantitative evaluation results on 360Roam dataset in Table~\ref{tab:app_full_360Roam}, and the additional camera pose optimization comparisons in individual scenes in Table~\ref{tab:app_opt_train_pose}. Under different scenes and different point cloud initializations, SC-OmniGS outperforms other calibration baselines achieving robust camera calibration capability.}

\edit{Figures \ref{fig:app_exp_pano_comp}-\ref{fig:app_exp_pano_comp_depth} supplement some qualitative rendering and depth comparisons} among adapted calibration baselines with omnidirectional sampling in the scenes same as Figure \ref{fig:exp_pano_comp} in the main manuscript. We should intuitively notice that baselines with omnidirectional sampling render continuous 360-degree views, while our SC-OmniGS still gains the best rendering fidelity and most accurately calibrated cameras.
Furthermore, Figures~\ref{fig:app_exp_comp_flat}-\ref{fig:app_exp_comp_base} visualize more comparison results of novel 360-degree and perspective views among calibration baselines.


\begin{table*}[h]
    \caption{The complete \edit{image} quantitative evaluation results on real-world dataset 360Roam. Checked ``Perturb" indicates perturbed camera poses as inputs, ``Point Init" indicates the way of point cloud initialization for 3D-GS based methods, ``train" and ``test" indicate training and test views, respectively. Methods marked with superscript $^\circ$ are modified via omnidirectional sampling.}
    \label{tab:app_full_360Roam}
    \centering
    \footnotesize
    \tabcolsep=0.07cm     
    \resizebox{\linewidth}{!}{
    \begin{tabular}{lc |cc cc cc | cc cc cc}
    \toprule
    \multicolumn{2}{c|}{\multirow{2}{*}{\makecell{On 360Roam}}} & \multicolumn{2}{c}{3D-GS} & \multicolumn{2}{c}{OmniGS} &\multicolumn{2}{c|}{SC-OmniGS} & \multicolumn{2}{c}{BARF} & \multicolumn{2}{c}{L2G-NeRF} & \multicolumn{2}{c}{CamP} \\
    \cmidrule(lr){3-4} \cmidrule(lr){5-6} \cmidrule(lr){7-8} \cmidrule(lr){9-10} \cmidrule(lr){11-12} \cmidrule(lr){13-14}
    & &train&test &train&test &train&test &train&test &train&test &train&test \\ 
    \hline
    & Perturb & \multicolumn{2}{c}{$\times$} &\multicolumn{2}{c}{$\times$} &\multicolumn{2}{c|}{$\times$}   &\multicolumn{2}{c}{$\checkmark$} &\multicolumn{2}{c}{$\checkmark$} &\multicolumn{2}{c}{$\checkmark$} \\
    & Point Init &\multicolumn{2}{c}{SfM} &\multicolumn{2}{c}{SfM} &\multicolumn{2}{c|}{SfM} &\multicolumn{2}{c}{N/A} &\multicolumn{2}{c}{N/A} &\multicolumn{2}{c}{N/A}  \\
    \hline		
    			
    \multirow{3}{*}
    {\scriptsize$\mathbf{Bar}$}&PSNR$\uparrow$ &20.983&18.764 &24.511&21.567 &25.653&22.556  &19.047&18.020 &19.089&18.333 &22.181&13.534 \\
    &SSIM$\uparrow$ &0.734&0.673 &0.849&0.760 &0.862&0.783  &0.543&0.523 &0.547&0.533 &0.736&0.388 \\
    &LPIPS$\downarrow$ &0.235&0.268 &0.155&0.191 &0.158&0.200 &0.528&0.538 &0.518&0.527 &0.283&0.556 \\
    \hline
    \multirow{3}{*}{\scriptsize$\mathbf{Base}$}&PSNR$\uparrow$ &23.677&20.764 &28.914&24.254 &30.070&25.504  &20.409&19.638 &20.582&19.991 &23.874&13.402 \\
    &SSIM$\uparrow$ &0.733&0.681 &0.876&0.768 &0.897&0.816  &0.506&0.499 &0.511&0.505 &0.674&0.372  \\
    &LPIPS$\downarrow$ &0.206&0.233 &0.101&0.135 &0.098&0.133  &0.555&0.562 &0.544&0.549 &0.319&0.632 \\
    \hline			
    \multirow{3}{*}{\scriptsize$\mathbf{Cafe}$}&PSNR$\uparrow$ &24.715&19.428 &28.846&24.315 &29.283&24.838  &22.020&19.440 &22.198&20.506 &25.086&14.251\\
    &SSIM$\uparrow$ &0.788&0.712 &0.902&0.803 &0.905&0.813  &0.627&0.590 &0.637&0.608 &0.780&0.448  \\
    &LPIPS$\downarrow$ &0.171&0.214 &0.087&0.128 &0.108&0.161  &0.452&0.474 &0.440&0.452 &0.229&0.579 \\
    \hline 							
    \multirow{3}{*}{\scriptsize$\mathbf{Canteen}$}&PSNR$\uparrow$ &23.211&19.077 &27.318&21.632 &27.335&22.159  &21.103&18.558 &21.116&18.476 &24.360&12.861 \\
    &SSIM$\uparrow$ &0.733&0.631 &0.849&0.712 &0.838&0.734 &0.591&0.546 &0.592&0.540 &0.761&0.426 \\
    &LPIPS$\downarrow$ &0.253&0.330 &0.168&0.236 &0.204&0.263  &0.483&0.507 &0.480&0.505 &0.225&0.595 \\
    \hline			
    \multirow{3}{*}{\scriptsize$\mathbf{Center}$}&PSNR$\uparrow$ &24.677&21.801 &28.728&24.264 &30.035&25.802  &21.641&18.870 &21.953&19.468 &25.098&14.574 \\
    &SSIM$\uparrow$ &0.754&0.696 &0.848&0.763 &0.872&0.813  &0.598&0.559 &0.609&0.564 &0.737&0.486 \\
    &LPIPS$\downarrow$ &0.239&0.282 &0.170&0.213 &0.169&0.203  &0.489&0.524 &0.475&0.507 &0.288&0.607 \\
    \hline						
    \multirow{3}{*}{\scriptsize$\mathbf{Innovation}$}&PSNR$\uparrow$&24.258&22.062 &28.980&25.201 &30.554&26.390  &21.964&21.357 &22.021&21.525 &24.518&14.389 \\
    &SSIM$\uparrow$ &0.712&0.677 &0.858&0.771 &0.898&0.819 &0.573&0.568 &0.574&0.570 &0.687&0.424  \\
    &LPIPS$\downarrow$ &0.250&0.269 &0.137&0.164 &0.120&0.148  &0.440&0.445 &0.438&0.440 &0.308&0.558  \\
    \hline						
    \multirow{3}{*}{\scriptsize$\mathbf{Lab}$}&PSNR$\uparrow$ &24.924&22.003 &31.651&27.325 &32.890&28.875  &23.614&22.889 &23.624&22.873 &25.840&15.565 \\
    &SSIM$\uparrow$ &0.824&0.785 &0.926&0.869 &0.939&0.898  &0.725&0.715 &0.725&0.716 &0.812&0.544  \\
    &LPIPS$\downarrow$ &0.145&0.167 &0.069&0.093 &0.066&0.087  &0.351&0.361 &0.360&0.371 &0.198&0.468 \\
    \hline
    \multirow{3}{*}{\scriptsize$\mathbf{Library}$}&PSNR$\uparrow$ &25.103&22.427 &29.192&25.137 &30.137&26.250  &23.796&22.830 &23.794&22.883 &25.779&15.446 \\
    &SSIM$\uparrow$ &0.671&0.620 &0.782&0.699 &0.806&0.746  &0.589&0.574 &0.589&0.574 &0.692&0.417  \\
    &LPIPS$\downarrow$ &0.286&0.324 &0.209&0.249 &0.206&0.243   &0.423&0.435 &0.423&0.435 &0.260&0.585 \\
    \bottomrule
    \end{tabular}			
    }
    
    \resizebox{\linewidth}{!}{
    \begin{tabular}{lc |cc cc cc cc cc cc}
    \toprule
    \multicolumn{2}{c|}{\multirow{2}{*}{\makecell{On 360Roam}}} & \multicolumn{2}{c}{OmniGS} & \multicolumn{2}{c}{SC-OmniGS} & \multicolumn{2}{c}{SC-OmniGS}  & \multicolumn{2}{c}{BARF$^\circ$} & \multicolumn{2}{c}{L2G-NeRF$^\circ$} & \multicolumn{2}{c}{CamP$^\circ$}  \\
    \cmidrule(lr){3-4} \cmidrule(lr){5-6} \cmidrule(lr){7-8} \cmidrule(lr){9-10} \cmidrule(lr){11-12} \cmidrule(lr){13-14} 
    & &train&test &train&test &train&test &train&test &train&test &train&test \\ 
    \hline
    & Perturb &\multicolumn{2}{c}{$\checkmark$} &\multicolumn{2}{c}{$\checkmark$} &\multicolumn{2}{c}{$\checkmark$} &\multicolumn{2}{c}{$\checkmark$} &\multicolumn{2}{c}{$\checkmark$} &\multicolumn{2}{c}{$\checkmark$}\\
    & Point Init &\multicolumn{2}{c}{SfM} &\multicolumn{2}{c}{Random} &\multicolumn{2}{c}{SfM} &\multicolumn{2}{c}{N/A} &\multicolumn{2}{c}{N/A} &\multicolumn{2}{c}{N/A}  \\
    \hline		
    \multirow{3}{*} 		{\scriptsize$\mathbf{Bar}$}&PSNR$\uparrow$ &18.915&14.718 &24.876&22.090 &25.410&22.556 &19.457&18.499 &19.803&18.794 &22.946&12.600 \\
    &SSIM$\uparrow$ &0.640&0.431 &0.840&0.763 &0.854&0.785 &0.519&0.498 &0.533&0.510 &0.765&0.380   \\
    &LPIPS$\downarrow$  &0.404&0.504 &0.192&0.235 &0.166&0.205 &0.567&0.580 &0.542&0.557 &0.273&0.636\\

    \hline
    \multirow{3}{*}		{\scriptsize$\mathbf{Base}$}&PSNR$\uparrow$ &21.449&14.559 &28.322&24.780 &29.226&24.308 &20.986&20.024 &21.382&20.122 &25.179&13.251\\
    &SSIM$\uparrow$ &0.674&0.351 &0.842&0.777 &0.880&0.777 &0.488&0.472 &0.501&0.481 &0.728&0.381 \\
    &LPIPS$\downarrow$ &0.328&0.498 &0.172&0.198 &0.114&0.157 &0.590&0.601 &0.557&0.572 &0.282&0.653\\	

    \hline
    \multirow{3}{*}{\scriptsize$\mathbf{Cafe}$}&PSNR$\uparrow$ &22.313&15.680 &28.156&23.917 &29.278&25.171 &22.169&19.895 &22.518&20.146 &26.908&13.689\\
    &SSIM$\uparrow$ &0.734&0.441 &0.894&0.789 &0.904&0.827 &0.607&0.563 &0.617&0.571 &0.829&0.429\\
    &LPIPS$\downarrow$ &0.294&0.462 &0.123&0.178 &0.108&0.145 &0.478&0.497 &0.454&0.479 &0.196&0.620\\	

    \hline
    \multirow{3}{*}{\scriptsize$\mathbf{Canteen}$}&PSNR$\uparrow$&22.814&14.273 &27.494&21.251 &27.259&22.139 &21.395&18.887 &21.761&17.027 &26.388&12.691\\
    &SSIM$\uparrow$  &0.732&0.458 &0.844&0.692 &0.837&0.732 &0.564&0.521 &0.575&0.476 &0.817&0.445\\
    &LPIPS$\downarrow$ &0.331&0.536 &0.198&0.289  &0.206&0.265 &0.526&0.558 &0.503&0.571 &0.196&0.627\\	

    \hline			
    \multirow{3}{*}{\scriptsize$\mathbf{Center}$}&PSNR$\uparrow$ &22.740&15.597 &28.972&24.482 &29.706&25.304 &22.275&19.689 &22.859&16.855 &26.616&14.471\\
    &SSIM$\uparrow$ &0.717&0.510 &0.847&0.779 &0.867&0.799 &0.584&0.524 &0.604&0.478 &0.780&0.487\\
    &LPIPS$\downarrow$ &0.372&0.553 &0.205&0.265 &0.177&0.220 &0.505&0.540 &0.474&0.559 &0.264&0.608\\

    \hline
    \multirow{3}{*}{\scriptsize$\mathbf{Innovation}$}&PSNR$\uparrow$ &21.880&16.047 &28.916&25.943 &30.079&24.788 &22.291&21.242 &22.761&21.450 &25.890&13.361\\
    &SSIM$\uparrow$ &0.697&0.440  &0.828&0.785 &0.887&0.762 &0.545&0.535 &0.558&0.545 &0.738&0.421\\
    &LPIPS$\downarrow$ &0.325&0.447 &0.199&0.219 &0.129&0.177 &0.475&0.482 &0.449&0.460 &0.287&0.616\\

    \hline
    \multirow{3}{*}{\scriptsize$\mathbf{Lab}$}&PSNR$\uparrow$ &22.049&16.642 &32.175&27.568 &32.801&28.812 &23.997&22.838 &24.622&22.951 &27.002&14.315\\
    &SSIM$\uparrow$ &0.762&0.563 &0.930&0.874 &0.938&0.895 &0.709&0.692 &0.729&0.707 &0.837&0.530 \\
    &LPIPS$\downarrow$ &0.299&0.421 &0.089&0.122 &0.068&0.090 &0.361&0.372 &0.314&0.330 &0.209&0.594\\

    \hline
    \multirow{3}{*}{\scriptsize$\mathbf{Library}$}&PSNR$\uparrow$&24.725&17.437 &29.588&24.710 &30.095&26.202 &24.514&22.796 &24.944&22.838 &28.141&14.891 \\
    &SSIM$\uparrow$ &0.684&0.445 &0.791&0.703 &0.805&0.743 &0.584&0.558 &0.600&0.568 &0.798&0.422 \\
    &LPIPS$\downarrow$ &0.323&0.495 &0.225&0.289 &0.207&0.244  &0.430&0.451 &0.402&0.430 &0.205&0.623 \\
    \bottomrule
    \end{tabular}			
    }
\end{table*}

\begin{table*}[h]
    \caption{\edit{The training camera pose quantitative evaluation among calibration methods. Checked ``Perturb" indicates perturbed camera poses as inputs, $\dag$ indicates training from scratch, ``Point Init" indicates the way of point cloud initialization for 3D-GS based methods, ``$\mathbf{p}$" and ``$R$" indicate Root Mean Squared Error (RMSE) of camera position (in world units) and rotation (in degrees), respectively. Methods marked with superscript $^\circ$ are modified via omnidirectional sampling. SC-OmniGS performs robust camera calibration capability in different scenarios and point initialization.}}
    \label{tab:app_opt_train_pose}
    \centering
    \footnotesize
    \tabcolsep=0.07cm     
    \resizebox{\linewidth}{!}{
    \begin{tabular}{lc | c c c c c c | cc}
    \toprule
    \multicolumn{2}{c|}{\makecell{On 360Roam}} & BARF  & BARF$^\circ$ & L2G-NeRF & L2G-NeRF$^\circ$ & CamP & CamP$^\circ$ & SC-OmniGS & SC-OmniGS\\
    \hline
    & Perturb & $\checkmark$ &$\checkmark$ & $\checkmark$  &$\checkmark$ &$\checkmark$ & $\checkmark$ &$\checkmark$ & $\checkmark$\\
    & Point Init & N/A & N/A & N/A & N/A & N/A & N/A & random & SfM \\
    \hline		
	
    \multirow{2}{*}    {\scriptsize$\mathbf{Bar}$}&$\mathbf{p}\downarrow$ & 0.31873 & 0.11240 &0.23656 &0.05947 &0.16559 & 0.16692 & 0.03811 & 0.03401\\
    &$R\downarrow$ &0.12260 &0.03499 &0.08151 &0.06093 &0.02700 &0.02568 & 0.01880 &0.01402\\
    \hline
    \multirow{2}{*}{\scriptsize$\mathbf{Base}$}&$\mathbf{p}\downarrow$ & 0.38139 &0.03944 &0.22836 &0.18336 &0.19603 & 0.19792 &0.08044  &0.02074 \\
    &$R\downarrow$ & 0.10911 &0.00561 &0.05018 &0.02207 &0.02758 &0.02575 &0.02459 &0.00318\\
    \hline			
    \multirow{2}{*}{\scriptsize$\mathbf{Cafe}$}& $\mathbf{p}\downarrow$ & 0.34125 & 0.32115& 0.14891 & 0.18808 & 0.14154 &0.14064& 0.00651 &0.00627 \\
    &$R\downarrow$ & 0.12002 &0.08143 &0.03887 &0.07296 &0.02560&0.02694 &0.00236 &0.00212 \\
    \hline 							
    \multirow{2}{*}{\scriptsize$\mathbf{Canteen}$}&$\mathbf{p}\downarrow$ & 0.47954&0.24846&0.55104 &0.58446&0.16421&0.16661 &0.04292 &0.03002\\
    &$R\downarrow$ & 0.18377& 0.09021& 0.23060& 0.18187& 0.02624&0.02444 &0.00592 & 0.00253 \\
    \hline			
    \multirow{2}{*}{\scriptsize$\mathbf{Center}$}&$\mathbf{p}\downarrow$ & 0.72546& 0.53148& 0.72888& 0.81537& 0.19709&0.19951 &0.17692 &0.10194 \\
    &$R\downarrow$ & 0.26783 &0.19900 &0.22620 &0.38847 &0.02768&0.02532 &0.06964 &0.00746 \\
    \hline						
    \multirow{2}{*}{\scriptsize$\mathbf{Innovation}$}&$\mathbf{p}\downarrow$ &0.23938& 0.20665& 0.11435& 0.30508& 0.20174&0.20299 & 0.00565 &0.02205\\
    &$R\downarrow$ &0.08755 &0.06569 &0.03044 &0.06525 &0.02823&0.025232 &0.00190 & 0.00598 \\
    \hline						
    \multirow{2}{*}{\scriptsize$\mathbf{Lab}$}&$\mathbf{p}\downarrow$ & 0.07353&0.02230&0.03886&0.01235&0.23800&0.23774 &0.01353 &0.01432 \\
    &$R\downarrow$ & 0.02864&0.00385 &0.01433&0.00301&0.03342&0.02524 &0.00248 &0.00191 \\
    \hline
    \multirow{2}{*}{\scriptsize$\mathbf{Library}$}&$\mathbf{p}\downarrow$ & 0.27276 &0.02723&0.26827&0.02759&0.21650&0.21446 &0.11948 &0.00632 \\
    &$R\downarrow$ & 0.07719 &0.00248&0.07728&0.00283&0.02787& 
0.02771 &0.01251 &0.00162 \\
    \bottomrule
    \end{tabular}			
    }
    
    \resizebox{\linewidth}{!}{
    \begin{tabular}{lc | c c c c c c | ccc}
    \toprule
    \multicolumn{2}{c|}{\makecell{On OmniBlender}} & BARF  & BARF$^\circ$ & L2G-NeRF & L2G-NeRF$^\circ$ & CamP & CamP$^\circ$ & SC-OmniGS & SC-OmniGS & SC-OmniGS\\
    \hline
    & Perturb & $\checkmark$ &$\checkmark$ & $\checkmark$  &$\checkmark$ &$\checkmark$ & $\checkmark$ &$\checkmark$ & $\checkmark$ & $\checkmark$\\
    & Point Init & N/A & N/A & N/A & N/A & N/A & N/A & Random & Est. depth & Render depth \\
    \hline					
    \multirow{2}{*}    {\scriptsize$\mathbf{Barbershop}$}&$\mathbf{p}\downarrow$ & 0.14411 &0.00053&0.00560&0.00048& 0.18435&0.17873 &0.11106 & 0.00032 &0.00025 \\
    &$R\downarrow$ &0.09418&0.00040&0.00529&0.00047&0.08132&0.07486 &0.04919 &0.00034 & 0.00024\\
    \hline
    \multirow{2}{*}{\scriptsize$\mathbf{Classroom}$}&$\mathbf{p}\downarrow$ & 0.00882&0.00059&0.36072&0.00062& 0.21609&0.21072 &0.00015 &0.00023 &0.00014\\
    &$R\downarrow$ & 0.00995&0.00094&0.28451&0.00095& 0.18112&0.16902 &0.00028 &0.00040 &0.00021\\
    \hline
    \multirow{2}{*}{\scriptsize$\mathbf{Flat}$}&$\mathbf{p}\downarrow$ & 0.21386 &0.00053&0.40058&0.00048& 0.25824&0.25266 &0.00051 &0.00108 &0.00032\\
    &$R\downarrow$ & 0.15046&0.00109&0.19573&0.00113& 0.07878&0.06339 &0.00077 &0.00351 &0.00035\\
    \bottomrule
    \end{tabular}			
    }
    
    \resizebox{\linewidth}{!}{
    \begin{tabular}{lc | c c c c c c | ccc}
    \toprule
    \multicolumn{2}{c|}{\makecell{On OmniBlender}} & BARF  & BARF$^\circ$ & L2G-NeRF & L2G-NeRF$^\circ$ & CamP & CamP$^\circ$ & SC-OmniGS & SC-OmniGS & SC-OmniGS\\
    \hline
    & Perturb & $\dag$ &$\dag$ & $\dag$  &$\dag$ &$\dag$ & $\dag$ &$\dag$ & $\dag$  &$\dag$\\
    & Point Init & N/A & N/A & N/A & N/A & N/A & N/A & Random & Est. depth & Render depth \\
    \hline					
    \multirow{2}{*}    {\scriptsize$\mathbf{Barbershop}$}&$\mathbf{p}\downarrow$ & 0.34757&0.00065&0.37682&0.00050&0.41992& 0.11743 &0.00126 &0.00061 &0.00037 \\
    &$R\downarrow$ &0.30309&0.00058&0.24394&0.00047&0.07589&0.25327 &0.00202 &0.00065 &0.00059\\
    \hline
    \multirow{2}{*}{\scriptsize$\mathbf{Classroom}$}&$\mathbf{p}\downarrow$ & 0.45917&0.00041&0.41830&0.00055&0.49153&0.33876 &0.00071 & 0.00064 &0.00018\\
    &$R\downarrow$ & 0.34051&0.00061&0.30008&0.00096&0.25800&0.51458 & 0.00093 &0.00111 &0.00018\\
    \hline
    \multirow{2}{*}{\scriptsize$\mathbf{Flat}$}&$\mathbf{p}\downarrow$ & 0.31282&0.00050&0.39268&0.00034&0.27143& 0.00096 &0.00308 &0.00093 &0.00060\\
    &$R\downarrow$ &0.21171&0.00045&0.23691&0.00044&0.15632&0.01593 &0.00883 &0.00171 &0.00088\\
    \bottomrule
    \end{tabular}			
    }
\end{table*}


\begin{figure*}[t]
    \def\imgw{0.24}
    \def\imgh{0.11}
    \def\namew{0.105}
    \centering
    \footnotesize
    \rotatebox{90}{ \parbox{\namew\linewidth}{\centering \scriptsize Ground truth}} 
    \includegraphics[width=\imgw\linewidth, height=\imgh\linewidth]{figure/exp/baseline_comp/gt_barber_78_box.jpg}
    \includegraphics[width=\imgw\linewidth, height=\imgh\linewidth]{figure/exp/baseline_comp/gt_class_22.jpg} 
    \includegraphics[width=\imgw\linewidth, height=\imgh\linewidth]{figure/exp/baseline_comp/gt_canteen_1_0008.jpg} 
    \includegraphics[width=\imgw\linewidth, height=\imgh\linewidth]{figure/exp/baseline_comp/gt_inno_1_0036.jpg}\\
    
    \rotatebox{90}{ \parbox{\namew\linewidth}{\centering \scriptsize BARF$^\circ$}} %\citeyear{lin2021barf}
    \includegraphics[width=\imgw\linewidth, height=\imgh\linewidth]{figure/exp/baseline_comp/barfpano_barber_78_box.jpg}
    \includegraphics[width=\imgw\linewidth, height=\imgh\linewidth]{figure/exp/baseline_comp/barfpano_class_22.jpg} 
    \includegraphics[width=\imgw\linewidth, height=\imgh\linewidth]{figure/exp/baseline_comp/barfpano_canteen_1_0008.jpg} \includegraphics[width=\imgw\linewidth, height=\imgh\linewidth]{figure/exp/baseline_comp/barfpano_inno_1_0036.jpg}  \\
    
    \rotatebox{90}{ \parbox{\namew\linewidth}{\centering \scriptsize L2G-NeRF$^\circ$}} %\citeyear{chen2023local}
    \includegraphics[width=\imgw\linewidth, height=\imgh\linewidth]{figure/exp/baseline_comp/l2gpano_barber_78_box.jpg} \includegraphics[width=\imgw\linewidth, height=\imgh\linewidth]{figure/exp/baseline_comp/l2gpano_class_22.jpg} 
    \includegraphics[width=\imgw\linewidth, height=\imgh\linewidth]{figure/exp/baseline_comp/l2gpano_canteen_1_0008.jpg} \includegraphics[width=\imgw\linewidth, height=\imgh\linewidth]{figure/exp/baseline_comp/l2gpano_inno_1_0036.jpg} \\
    
    \rotatebox{90}{ \parbox{\namew\linewidth}{\centering \scriptsize CamP$^\circ$}} %\citeyear{park2023camp}
    \includegraphics[width=\imgw\linewidth, height=\imgh\linewidth]{figure/exp/baseline_comp/camppano_barber_78_box.jpg} \includegraphics[width=\imgw\linewidth, height=\imgh\linewidth]{figure/exp/baseline_comp/camppano_class_22.jpg} 
    \includegraphics[width=\imgw\linewidth, height=\imgh\linewidth]{figure/exp/baseline_comp/camppano_canteen_1_0008.jpg} \includegraphics[width=\imgw\linewidth, height=\imgh\linewidth]{figure/exp/baseline_comp/camppano_inno_1_0036.jpg}  \\
    
    \rotatebox{90}{ \parbox{\namew\linewidth}{\centering \scriptsize Ours}} 
    \includegraphics[width=\imgw\linewidth, height=\imgh\linewidth]{figure/exp/baseline_comp/ours_barber_iter_30000_78_box.jpg}
    \includegraphics[width=\imgw\linewidth, height=\imgh\linewidth]{figure/exp/baseline_comp/ours_class_iter_30000_22.jpg} 
    \includegraphics[width=\imgw\linewidth, height=\imgh\linewidth]{figure/exp/baseline_comp/ours_canteen_iter_30000_1_0008.jpg} \includegraphics[width=\imgw\linewidth, height=\imgh\linewidth]{figure/exp/baseline_comp/ours_inno_iter_30000_1_0036.jpg} 

    \begin{subfigure}[b]{\imgw\linewidth}
        \vspace{-0.1cm}
        \caption{$\mathbf{Barbershop}^\dag$}
    \end{subfigure}
    \begin{subfigure}[b]{\imgw\linewidth}
        \vspace{-0.1cm}
        \caption{$\mathbf{Classroom}^\dag$}
    \end{subfigure}
    \begin{subfigure}[b]{\imgw\linewidth}
        \vspace{-0.1cm}
        \caption{$\mathbf{Canteen}$}
    \end{subfigure}
    \begin{subfigure}[b]{\imgw\linewidth}
        \vspace{-0.1cm}
        \caption{$\mathbf{Innovation}$}
    \end{subfigure}
    
    \caption{Qualitative comparisons of 360-degree novel views among calibration methods equipped with omnidirectional sampling. Our results outperform in both rendering quality and camera accuracy. $\dag$ indicates training from scratch, $^\circ$ indicates baselines modified via omnidirectional sampling. }
    \label{fig:app_exp_pano_comp}
\end{figure*}


\begin{figure*}[t]
    \def\imgw{0.24}
    \def\imgh{0.11}
    \def\namew{0.105}
    \centering
    \footnotesize
    \rotatebox{90}{ \parbox{\namew\linewidth}{\centering \scriptsize GT view}} 
    \includegraphics[width=\imgw\linewidth, height=\imgh\linewidth]{figure/exp/baseline_comp_depth/gt_barber_76.png}
    \includegraphics[width=\imgw\linewidth, height=\imgh\linewidth]{figure/exp/baseline_comp_depth/gt_classroom_20.png} 
    \includegraphics[width=\imgw\linewidth, height=\imgh\linewidth]{figure/exp/baseline_comp_depth/gt_canteen_0_0024.png} 
    \includegraphics[width=\imgw\linewidth, height=\imgh\linewidth]{figure/exp/baseline_comp_depth/gt_inno_0_0109.png}\\
    
    \rotatebox{90}{ \parbox{\namew\linewidth}{\centering \scriptsize BARF$^\circ$}} 
    \includegraphics[width=\imgw\linewidth, height=\imgh\linewidth]{figure/exp/baseline_comp_depth/barfpano_barber_76.png}
    \includegraphics[width=\imgw\linewidth, height=\imgh\linewidth]{figure/exp/baseline_comp_depth/barfpano_classroom_20.png} 
    \includegraphics[width=\imgw\linewidth, height=\imgh\linewidth]{figure/exp/baseline_comp_depth/barfpano_canteen_0_0024.png} \includegraphics[width=\imgw\linewidth, height=\imgh\linewidth]{figure/exp/baseline_comp_depth/barfpano_inno_0_0109.png}  \\
    
    \rotatebox{90}{ \parbox{\namew\linewidth}{\centering \scriptsize L2G-NeRF$^\circ$}} 
    \includegraphics[width=\imgw\linewidth, height=\imgh\linewidth]{figure/exp/baseline_comp_depth/l2gpano_barber_76.png} \includegraphics[width=\imgw\linewidth, height=\imgh\linewidth]{figure/exp/baseline_comp_depth/l2gpano_classroom_20.png} 
    \includegraphics[width=\imgw\linewidth, height=\imgh\linewidth]{figure/exp/baseline_comp_depth/l2gpano_canteen_0_0024.png} \includegraphics[width=\imgw\linewidth, height=\imgh\linewidth]{figure/exp/baseline_comp_depth/l2gpano_inno_0_0109.png} \\
    
    \rotatebox{90}{ \parbox{\namew\linewidth}{\centering \scriptsize CamP$^\circ$}} %\citeyear{park2023camp}
    \includegraphics[width=\imgw\linewidth, height=\imgh\linewidth]{figure/exp/baseline_comp_depth/camppano_barber_76.png} \includegraphics[width=\imgw\linewidth, height=\imgh\linewidth]{figure/exp/baseline_comp_depth/camppano_classroom_20.png} 
    \includegraphics[width=\imgw\linewidth, height=\imgh\linewidth]{figure/exp/baseline_comp_depth/camppano_canteen_0_0024.png} \includegraphics[width=\imgw\linewidth, height=\imgh\linewidth]{figure/exp/baseline_comp_depth/camppano_inno_0_0109.png}  \\
    
    \rotatebox{90}{ \parbox{\namew\linewidth}{\centering \scriptsize Ours}} 
    \includegraphics[width=\imgw\linewidth, height=\imgh\linewidth]{figure/exp/baseline_comp_depth/ours_barber_76.png}
    \includegraphics[width=\imgw\linewidth, height=\imgh\linewidth]{figure/exp/baseline_comp_depth/ours_classroom_20.png} 
    \includegraphics[width=\imgw\linewidth, height=\imgh\linewidth]{figure/exp/baseline_comp_depth/ours_canteen_0_0024.png} \includegraphics[width=\imgw\linewidth, height=\imgh\linewidth]{figure/exp/baseline_comp_depth/ours_inno_0_0109.png} 

    \begin{subfigure}[b]{\imgw\linewidth}
        \vspace{-0.1cm}
        \caption{$\mathbf{Barbershop}^\dag$}
    \end{subfigure}
    \begin{subfigure}[b]{\imgw\linewidth}
        \vspace{-0.1cm}
        \caption{$\mathbf{Classroom}^\dag$}
    \end{subfigure}
    \begin{subfigure}[b]{\imgw\linewidth}
        \vspace{-0.1cm}
        \caption{$\mathbf{Canteen}$}
    \end{subfigure}
    \begin{subfigure}[b]{\imgw\linewidth}
        \vspace{-0.1cm}
        \caption{$\mathbf{Innovation}$}
    \end{subfigure}
    
    \caption{\edit{Depth visualization of 360-degree views rendered by calibration methods equipped with omnidirectional sampling. Our results outperform in geometry accuracy and details. $\dag$ indicates training from scratch, $^\circ$ indicates baselines modified via omnidirectional sampling.} }
    \label{fig:app_exp_pano_comp_depth}
\end{figure*}

\begin{figure}[h]
    \def\imgw{0.32}
    \def\imgh{0.146}
    \def\namew{0.14}
    \centering
    \rotatebox{90}{ \parbox{\namew\linewidth}{\centering \scriptsize Ground-truth}} 
    \includegraphics[width=\imgw\linewidth, height=\imgh\linewidth]{figure/exp/app_baseline_comp/gt_flat_2.jpg} \includegraphics[width=\imgw\linewidth, height=\imgh\linewidth]{figure/exp/app_baseline_comp/gt_flat_46.jpg} 
    % \includegraphics[width=\imgw\linewidth, height=\imgh\linewidth]{figure/exp/app_baseline_comp/gt_flat_98.jpg} 
    \includegraphics[height=\imgh\linewidth]{figure/exp/app_baseline_comp/gt_flat_98_0.png}
    \includegraphics[height=\imgh\linewidth]{figure/exp/app_baseline_comp/gt_flat_98_4.png}\\
    
    \rotatebox{90}{ \parbox{\namew\linewidth}{\centering \scriptsize BARF}} 
    \includegraphics[width=\imgw\linewidth, height=\imgh\linewidth]{figure/exp/app_baseline_comp/barf_flat_2_cube2sphere.jpg} \includegraphics[width=\imgw\linewidth, height=\imgh\linewidth]{figure/exp/app_baseline_comp/barf_flat_46_cube2sphere.jpg} 
    % \includegraphics[width=\imgw\linewidth, height=\imgh\linewidth]{figure/exp/app_baseline_comp/barf_flat_98_cube2sphere.jpg} 
    \includegraphics[height=\imgh\linewidth]{figure/exp/app_baseline_comp/barf_flat_98_0.png}
    \includegraphics[height=\imgh\linewidth]{figure/exp/app_baseline_comp/barf_flat_98_4.png} \\
    
    \rotatebox{90}{ \parbox{\namew\linewidth}{\centering \scriptsize L2G-NeRF}} 
    \includegraphics[width=\imgw\linewidth, height=\imgh\linewidth]{figure/exp/app_baseline_comp/l2g_flat_2_cube2sphere.jpg} \includegraphics[width=\imgw\linewidth, height=\imgh\linewidth]{figure/exp/app_baseline_comp/l2g_flat_46_cube2sphere.jpg}
    % \includegraphics[width=\imgw\linewidth, height=\imgh\linewidth]{figure/exp/app_baseline_comp/l2g_flat_98_cube2sphere.jpg}
    \includegraphics[height=\imgh\linewidth]{figure/exp/app_baseline_comp/l2g_flat_98_0.png}
    \includegraphics[height=\imgh\linewidth]{figure/exp/app_baseline_comp/l2g_flat_98_4.png} \\
    
    \rotatebox{90}{ \parbox{\namew\linewidth}{\centering \scriptsize CamP}} 
    \includegraphics[width=\imgw\linewidth, height=\imgh\linewidth]{figure/exp/app_baseline_comp/camp_flat_2_cube2sphere.jpg} \includegraphics[width=\imgw\linewidth, height=\imgh\linewidth]{figure/exp/app_baseline_comp/camp_flat_46_cube2sphere.jpg} 
    % \includegraphics[width=\imgw\linewidth, height=\imgh\linewidth]{figure/exp/app_baseline_comp/camp_flat_98_cube2sphere.jpg} 
    \includegraphics[height=\imgh\linewidth]{figure/exp/app_baseline_comp/camp_flat_98_0.png}
    \includegraphics[height=\imgh\linewidth]{figure/exp/app_baseline_comp/camp_flat_98_4.png} \\
    
    \rotatebox{90}{ \parbox{\namew\linewidth}{\centering \scriptsize Ours}} 
    \includegraphics[width=\imgw\linewidth, height=\imgh\linewidth]{figure/exp/app_baseline_comp/ours_flat_iter_30000_2.jpg} \includegraphics[width=\imgw\linewidth, height=\imgh\linewidth]{figure/exp/app_baseline_comp/ours_flat_iter_30000_46.jpg}
    % \includegraphics[width=\imgw\linewidth, height=\imgh\linewidth]{figure/exp/app_baseline_comp/ours_flat_iter_30000_98.jpg}
    \includegraphics[height=\imgh\linewidth]{figure/exp/app_baseline_comp/ours_flat_iter_30000_98_0.jpg} \includegraphics[height=\imgh\linewidth]{figure/exp/app_baseline_comp/ours_flat_iter_30000_98_4.jpg}
    
    \caption{Novel views on synthetic scene $\mathbf{Flat}$ among baselines trained from scratch.}
    \label{fig:app_exp_comp_flat}
\end{figure}

\begin{figure}[h]
    \def\imgw{0.32}
    \def\imgh{0.146}
    \def\namew{0.14}
    \centering
    \rotatebox{90}{ \parbox{\namew\linewidth}{\centering \scriptsize Ground-truth}} 
    \includegraphics[width=\imgw\linewidth, height=\imgh\linewidth]{figure/exp/app_baseline_comp/gt_cafe_1_0013.jpg} \includegraphics[width=\imgw\linewidth, height=\imgh\linewidth]{figure/exp/app_baseline_comp/gt_cafe_1_0020.jpg} 
    %\includegraphics[width=\imgw\linewidth, height=\imgh\linewidth]{figure/exp/app_baseline_comp/gt_cafe_1_0024.jpg} 
    \includegraphics[height=\imgh\linewidth]{figure/exp/app_baseline_comp/gt_cafe_1_0024_0.jpg} 
    \includegraphics[height=\imgh\linewidth]{figure/exp/app_baseline_comp/gt_cafe_1_0024_1.jpg} \\
    
    \rotatebox{90}{ \parbox{\namew\linewidth}{\centering \scriptsize BARF}} 
    \includegraphics[width=\imgw\linewidth, height=\imgh\linewidth]{figure/exp/app_baseline_comp/barf_cafe_1_0013_cube2sphere.jpg} \includegraphics[width=\imgw\linewidth, height=\imgh\linewidth]{figure/exp/app_baseline_comp/barf_cafe_1_0020_cube2sphere.jpg} 
    % \includegraphics[width=\imgw\linewidth, height=\imgh\linewidth]{figure/exp/app_baseline_comp/barf_cafe_1_0024_cube2sphere.jpg}
    \includegraphics[height=\imgh\linewidth]{figure/exp/app_baseline_comp/barf_cafe_1_0024_0.png}
    \includegraphics[height=\imgh\linewidth]{figure/exp/app_baseline_comp/barf_cafe_1_0024_1.png}\\
    
    \rotatebox{90}{ \parbox{\namew\linewidth}{\centering \scriptsize L2G-NeRF}} 
    \includegraphics[width=\imgw\linewidth, height=\imgh\linewidth]{figure/exp/app_baseline_comp/l2g_cafe_1_0013_cube2sphere.jpg} \includegraphics[width=\imgw\linewidth, height=\imgh\linewidth]{figure/exp/app_baseline_comp/l2g_cafe_1_0020_cube2sphere.jpg} 
    % \includegraphics[width=\imgw\linewidth, height=\imgh\linewidth]{figure/exp/app_baseline_comp/l2g_cafe_1_0024_cube2sphere.jpg} 
    \includegraphics[height=\imgh\linewidth]{figure/exp/app_baseline_comp/l2g_cafe_1_0024_0.png}
    \includegraphics[height=\imgh\linewidth]{figure/exp/app_baseline_comp/l2g_cafe_1_0024_1.png}\\
    
    \rotatebox{90}{ \parbox{\namew\linewidth}{\centering \scriptsize CamP}} 
    \includegraphics[width=\imgw\linewidth, height=\imgh\linewidth]{figure/exp/app_baseline_comp/camp_cafe_1_0013_cube2sphere.jpg} \includegraphics[width=\imgw\linewidth, height=\imgh\linewidth]{figure/exp/app_baseline_comp/camp_cafe_1_0020_cube2sphere.jpg} 
    % \includegraphics[width=\imgw\linewidth, height=\imgh\linewidth]{figure/exp/app_baseline_comp/camp_cafe_1_0024_cube2sphere.jpg} 
    \includegraphics[height=\imgh\linewidth]{figure/exp/app_baseline_comp/camp_cafe_1_0024_0.png}
    \includegraphics[height=\imgh\linewidth]{figure/exp/app_baseline_comp/camp_cafe_1_0024_1.png}
   \\
    
    \rotatebox{90}{ \parbox{\namew\linewidth}{\centering \scriptsize Ours}} 
    \includegraphics[width=\imgw\linewidth, height=\imgh\linewidth]{figure/exp/app_baseline_comp/ours_cafe_iter_30000_1_0013.jpg} \includegraphics[width=\imgw\linewidth, height=\imgh\linewidth]{figure/exp/app_baseline_comp/ours_cafe_iter_30000_1_0020.jpg} 
    % \includegraphics[width=\imgw\linewidth, height=\imgh\linewidth]{figure/exp/app_baseline_comp/ours_cafe_iter_30000_1_0024.jpg}
    \includegraphics[height=\imgh\linewidth]{figure/exp/app_baseline_comp/ours_cafe_iter_30000_1_0024_0.jpg} 
    \includegraphics[height=\imgh\linewidth]{figure/exp/app_baseline_comp/ours_cafe_iter_30000_1_0024_1.jpg}
    
    \caption{Novel views on real scene $\mathbf{Cafe}$ among baselines trained with camera perturbation.}
    \label{fig:app_exp_comp_cafe}
\end{figure}



\begin{figure}[h]
    \def\imgw{0.32}
    \def\imgh{0.146}
    \def\namew{0.14}
    \centering
    \rotatebox{90}{ \parbox{\namew\linewidth}{\centering \scriptsize Ground-truth}} 
    \includegraphics[width=\imgw\linewidth, height=\imgh\linewidth]{figure/exp/app_baseline_comp/gt_bar_1_0005.jpg} \includegraphics[width=\imgw\linewidth, height=\imgh\linewidth]{figure/exp/app_baseline_comp/gt_bar_1_0016.jpg} 
    % \includegraphics[width=\imgw\linewidth, height=\imgh\linewidth]{figure/exp/app_baseline_comp/gt_bar_1_0031.jpg} 
    \includegraphics[height=\imgh\linewidth]{figure/exp/app_baseline_comp/gt_bar_1_0031_0.jpg}
    \includegraphics[height=\imgh\linewidth]{figure/exp/app_baseline_comp/gt_bar_1_0031_3.jpg} 
    \\
    
    \rotatebox{90}{ \parbox{\namew\linewidth}{\centering \scriptsize BARF}} %\citeyear{lin2021barf}
    \includegraphics[width=\imgw\linewidth, height=\imgh\linewidth]{figure/exp/app_baseline_comp/barf_bar_1_0005_cube2sphere.jpg} \includegraphics[width=\imgw\linewidth, height=\imgh\linewidth]{figure/exp/app_baseline_comp/barf_bar_1_0016_cube2sphere.jpg} 
    % \includegraphics[width=\imgw\linewidth, height=\imgh\linewidth]{figure/exp/app_baseline_comp/barf_bar_1_0031_cube2sphere.jpg}
    \includegraphics[height=\imgh\linewidth]{figure/exp/app_baseline_comp/barf_bar_1_0031_0.png} 
    \includegraphics[height=\imgh\linewidth]{figure/exp/app_baseline_comp/barf_bar_1_0031_3.png} 
    \\
    
    \rotatebox{90}{ \parbox{\namew\linewidth}{\centering \scriptsize L2G-NeRF}} %\citeyear{chen2023local}
    \includegraphics[width=\imgw\linewidth, height=\imgh\linewidth]{figure/exp/app_baseline_comp/l2g_bar_1_0005_cube2sphere.jpg} \includegraphics[width=\imgw\linewidth, height=\imgh\linewidth]{figure/exp/app_baseline_comp/l2g_bar_1_0016_cube2sphere.jpg} 
    % \includegraphics[width=\imgw\linewidth, height=\imgh\linewidth]{figure/exp/app_baseline_comp/l2g_bar_1_0031_cube2sphere.jpg} 
    \includegraphics[height=\imgh\linewidth]{figure/exp/app_baseline_comp/l2g_bar_1_0031_0.png} 
    \includegraphics[height=\imgh\linewidth]{figure/exp/app_baseline_comp/l2g_bar_1_0031_3.png} \\
    
    \rotatebox{90}{ \parbox{\namew\linewidth}{\centering \scriptsize CamP}} %\citeyear{park2023camp}
    \includegraphics[width=\imgw\linewidth, height=\imgh\linewidth]{figure/exp/app_baseline_comp/camp_bar_1_0005_cube2sphere.jpg} \includegraphics[width=\imgw\linewidth, height=\imgh\linewidth]{figure/exp/app_baseline_comp/camp_bar_1_0016_cube2sphere.jpg} 
    % \includegraphics[width=\imgw\linewidth, height=\imgh\linewidth]{figure/exp/app_baseline_comp/camp_bar_1_0031_cube2sphere.jpg}  
    \includegraphics[height=\imgh\linewidth]{figure/exp/app_baseline_comp/camp_bar_1_0031_0.png} 
    \includegraphics[height=\imgh\linewidth]{figure/exp/app_baseline_comp/camp_bar_1_0031_3.png} 
    \\
    
    \rotatebox{90}{ \parbox{\namew\linewidth}{\centering \scriptsize Ours}} 
    \includegraphics[width=\imgw\linewidth, height=\imgh\linewidth]{figure/exp/app_baseline_comp/ours_bar_iter_30000_1_0005.jpg} \includegraphics[width=\imgw\linewidth, height=\imgh\linewidth]{figure/exp/app_baseline_comp/ours_bar_iter_30000_1_0016.jpg} 
    % \includegraphics[width=\imgw\linewidth, height=\imgh\linewidth]{figure/exp/app_baseline_comp/ours_bar_iter_30000_1_0031.jpg}
    \includegraphics[height=\imgh\linewidth]{figure/exp/app_baseline_comp/ours_bar_iter_30000_1_0031_0.jpg} \includegraphics[height=\imgh\linewidth]{figure/exp/app_baseline_comp/ours_bar_iter_30000_1_0031_3.jpg}
    
    \caption{Novel views on real scene $\mathbf{Bar}$ among baselines trained with camera perturbation.}
    \label{fig:app_exp_comp_bar}
\end{figure}


\begin{figure}[h]
    \def\imgw{0.32}
    \def\imgh{0.146}
    \def\namew{0.14}
    \centering
    \rotatebox{90}{ \parbox{\namew\linewidth}{\centering \scriptsize Ground-truth}} 
    \includegraphics[width=\imgw\linewidth, height=\imgh\linewidth]{figure/exp/app_baseline_comp/gt_base_1_0014.jpg} \includegraphics[width=\imgw\linewidth, height=\imgh\linewidth]{figure/exp/app_baseline_comp/gt_base_1_0020.jpg} 
    % \includegraphics[width=\imgw\linewidth, height=\imgh\linewidth]{figure/exp/app_baseline_comp/gt_base_1_0034.jpg} 
    \includegraphics[height=\imgh\linewidth]{figure/exp/app_baseline_comp/gt_base_1_0034_0.jpg}
    \includegraphics[height=\imgh\linewidth]{figure/exp/app_baseline_comp/gt_base_1_0034_3.jpg}\\
    
    \rotatebox{90}{ \parbox{\namew\linewidth}{\centering \scriptsize BARF}} 
    \includegraphics[width=\imgw\linewidth, height=\imgh\linewidth]{figure/exp/app_baseline_comp/barf_base_1_0014_cube2sphere.jpg} \includegraphics[width=\imgw\linewidth, height=\imgh\linewidth]{figure/exp/app_baseline_comp/barf_base_1_0020_cube2sphere.jpg} 
    % \includegraphics[width=\imgw\linewidth, height=\imgh\linewidth]{figure/exp/app_baseline_comp/barf_base_1_0034_cube2sphere.jpg} 
    \includegraphics[height=\imgh\linewidth]{figure/exp/app_baseline_comp/barf_base_1_0034_0.png}
    \includegraphics[height=\imgh\linewidth]{figure/exp/app_baseline_comp/barf_base_1_0034_3.png}\\
    
    \rotatebox{90}{ \parbox{\namew\linewidth}{\centering \scriptsize L2G-NeRF}} 
    \includegraphics[width=\imgw\linewidth, height=\imgh\linewidth]{figure/exp/app_baseline_comp/l2g_base_1_0014_cube2sphere.jpg} \includegraphics[width=\imgw\linewidth, height=\imgh\linewidth]{figure/exp/app_baseline_comp/l2g_base_1_0020_cube2sphere.jpg} 
    % \includegraphics[width=\imgw\linewidth, height=\imgh\linewidth]{figure/exp/app_baseline_comp/l2g_base_1_0034_cube2sphere.jpg}     
    \includegraphics[height=\imgh\linewidth]{figure/exp/app_baseline_comp/l2g_base_1_0034_0.png}
    \includegraphics[height=\imgh\linewidth]{figure/exp/app_baseline_comp/l2g_base_1_0034_3.png}\\
    
    \rotatebox{90}{ \parbox{\namew\linewidth}{\centering \scriptsize CamP}} 
    \includegraphics[width=\imgw\linewidth, height=\imgh\linewidth]{figure/exp/app_baseline_comp/camp_base_1_0014_cube2sphere.jpg} \includegraphics[width=\imgw\linewidth, height=\imgh\linewidth]{figure/exp/app_baseline_comp/camp_base_1_0020_cube2sphere.jpg} 
    % \includegraphics[width=\imgw\linewidth, height=\imgh\linewidth]{figure/exp/app_baseline_comp/camp_base_1_0034_cube2sphere.jpg}  
    \includegraphics[height=\imgh\linewidth]{figure/exp/app_baseline_comp/camp_base_1_0034_0.png}
    \includegraphics[height=\imgh\linewidth]{figure/exp/app_baseline_comp/camp_base_1_0034_3.png}
    \\
    
    \rotatebox{90}{ \parbox{\namew\linewidth}{\centering \scriptsize Ours}} 
    \includegraphics[width=\imgw\linewidth, height=\imgh\linewidth]{figure/exp/app_baseline_comp/ours_base_iter_30000_1_0014.jpg} \includegraphics[width=\imgw\linewidth, height=\imgh\linewidth]{figure/exp/app_baseline_comp/ours_base_iter_30000_1_0020.jpg} 
    % \includegraphics[width=\imgw\linewidth, height=\imgh\linewidth]{figure/exp/app_baseline_comp/ours_base_iter_30000_1_0034.jpg}
    \includegraphics[height=\imgh\linewidth]{figure/exp/app_baseline_comp/ours_base_iter_30000_1_0034_0.jpg} \includegraphics[height=\imgh\linewidth]{figure/exp/app_baseline_comp/ours_base_iter_30000_1_0034_3.jpg}
    
    \caption{Novel views on real scene $\mathbf{Base}$ among baselines trained with camera perturbation.}
    \label{fig:app_exp_comp_base}
\end{figure}

