\section{Additional Experimental Settings}

\subsection{Datasets}
\label{appendix-sec:dataset}
Our antibody dataset preprocessing pipeline leverages the Structural Antibody Database (SAbDab) snapshot from November 2022, with structures pre-numbered using the IMGT numbering scheme \cite{lefranc2003imgt}. The IMGT system provides precise residue numbering for antibody structures, defining specific ranges for framework regions (FR) and complementarity determining regions (CDRs) in both heavy and light chains. For the heavy chain, these ranges encompass FR-H1 (1-26), CDR-H1 (27-38), FR-H2 (39-55), CDR-H2 (56-65), FR-H3 (66-104), CDR-H3 (105-117), and FR-H4 (118-129), with parallel definitions for the light chain regions.
This standardized numbering scheme is crucial for consistent processing and analysis across all antibody structures. 


We implement stringent filtering criteria, focusing exclusively on protein and peptide antigens while ensuring no chain overlap between antibody and antigen components. Other potential antigen types like small molecules, nucleic acids, or carbohydrates are excluded from our dataset. The validation process enforces three critical requirements: structural completeness, with verification of conserved residues (CYS23, CYS104, TRP41) in both chains; correct IMGT numbering across all regions; and comprehensive atomic coordinate validation, including backbone and sidechain atoms, with a 6.6 \AA\ contact distance threshold for residue interactions.

The final dataset preprocessing stage involves sequence-based clustering using MMseqs2 with a 40\% identity threshold, particularly focusing on CDR-H3 sequence similarity. This process yields 3,246 training and 365 validation antibodies. The curated SAbDab dataset serves as the foundation for both the RAbD \cite{dunbar2013sabdab} (60 PDB structures) and the IgFold \cite{ruffolo2023IgFold} benchmark dataset (51 structures), ensuring consistent quality and standardization across all experimental evaluations.




\subsection{Implementation Details}
\label{appendix-sec:exp-setting}
We implement Igformer using PyTorch \cite{paszke2019pytorch} and train the model on a single GeForce RTX 3090 Ti GPU using the Adam optimizer \cite{kingma2015adam}. The model training process utilizes a batch size of 16, running for 150 epochs in task 1 and 300 epochs for tasks 2-4, while preserving the 10 best-performing model checkpoints throughout the training phase of each individual task. The model architecture employs a 64-dimensional embedding space for antibody feature representation, 14 channels (i.e., 14 atoms) for each residue, and 3 iterations. The graph structure connects each residue to its 9 nearest neighbors (k = 9). The model incorporates a dropout rate of 0.1 (dropout = 0.1) to enhance generalization. 
Training proceeds through 3 iterations per batch, with each iteration updating epitope coordinates and embeddings, followed by reconstruction of intra-graph and inter-graph connections. The final iteration is used for loss calculation.
For structure analysis, the model defines interacting residues in the antibody-antigen interface using a binding distance threshold of 6.6 \AA. 

The optimization process employs gradient clipping at 1.0 and implements an adaptive learning rate schedule. The learning rate decays exponentially from 0.0013 to 0.0001 across the training epochs, with the decay factor determined as $ln(0.0001/0.0013)$, divided by the total number of training steps, ensuring a smooth convergence to the target learning rate.



\textbf{Feature Initiation.}
Igformer employs two parallel embedding components to construct the initial residue representation. 
First, the residue type embedding transforms each amino acid into a 64-dimensional vector using a learned embedding matrix of size $25\times 64$, where 25 accounts for the 20 types of amino acids and special tokens. Each amino acid is mapped to a unique embedding vector $\vect{H}_i^{\textit{res}}$ that captures its chemical and physical properties.
The positional information is encoded through a separate learned embedding matrix of size $192 \times 64$, which maps each position in the sequence to a 64-dimensional vector $\vect{H}_i^{\textit{pos}}$. This enables the model to maintain sequential context for sequences up to 192 residues in length, allowing differentiation between identical amino acids at different positions.
The final feature representation $\vect{H}_i$ is computed through element-wise addition of these two embeddings. This additive combination maintains the 64-dimensional feature space while integrating both residue identity and positional context, enabling the model to learn position-aware representations with comprehensive chemical information.

\textbf{EMP Module.}
The EMP module iterates three times (3 blocks) and consists of several key components: feature initialization, feature similarity network, EdgeMLP, CoordMLP and NodeMLP with residual connections, as detailed in Table \ref{tab:igformer_layers}. 
Each layer incorporates dropout \cite{srivastava2014dropout} mechanisms after activation functions. The similarity features are normalized after being mapped from 196 to 128 dimensions, including $sim_{ij}^{res}$ and $sim_{ij}^{atom}$. 
The iterative application of these components ensures efficient updates of both node features and coordinate transformations while maintaining stable training dynamics through the dropout mechanism.
The EdgeMLP processes concatenated input features of dimension 384, comprising source\_node features (128), target\_node features (128), and similarity features (128). This design enables the model to effectively capture interactions between residues while preserving both structural and relational information crucial for antibody representation.

\input{algorithm/EMP.tex}
\input{algorithm/transformer.tex}
\input{algorithm/triangle}


\textbf{APP and SGFormer.}
As shown in Table \ref{tab:additional_layers}, the inter-grpah refinment network consists of two main components. First, the APP module conducts 16 steps of propagation with $\alpha=0.1$, , allocating 90\% weight to neighbor-propagated information while retaining 10\% of original residue features. 
This enables effective information spread across the graph while preserving essential original residue characteristics.
The SGFormer component employs 5 single-head attention layers, computing attention once per layer rather than utilizing multiple attention heads. In its residual connections after each transformer layer, it uses $\alpha=0.9$, emphasizing transformed features (90\%) while maintaining minimal previous layer information (10\%).
Layer normalization is applied to the 64-dimensional feature space after each attention computation, working in conjunction with dropout (0.5) to regulate feature magnitudes and ensure training stability.


\textbf{Triangle Module.}
The triangle network consists of two main components, as detailed in Table \ref{tab:feature_pair_layers}. First, the triangle multiplication module performs feature pair-wise propagation with directed connections.
For each sequence of $n$ residues, it constructs an $n\times n$ matrix by concatenating paired residue features (128-dim) and projecting them to 256-dimensional space via an MLP, establishing directed connections where $\vect{Z}_{ij}$ $\neq$ $\vect{Z}_{ji}$.
The triangle attention component uses four sequential operations: multiply outgoing, multiply incoming, attention outgoing, and attention incoming. Each operation maintains the $n \times n \times 256$ dimensionality while enabling distinct information flows. The multiply operations use MLPs for feature transformation while preserving the triangle structure, whereas the attention operations employ single-head self-attention with query/key/value projections to capture long-range dependencies. The final step extracts diagonal elements and reduces them to 128 dimensions for network compatibility.
The implementation incorporates a dropout rate of 0.1 and SiLU activation functions across all linear projections. Layer normalization is applied after attention operations, and an additional normalization step in the final dimension reduction. The entire module operates on a per-sequence basis, processing each unique sequence independently before the features are reassembled.



\textbf{Dual EMP Module.}
The Dual EMP architecture extends the base EMP module by operating on two distinct graph structures simultaneously. It processes the intra-graph and inter-graph representations, along with their respective coordinates ($\vect{X}_{full}$ and $\vect{X}_{inter}$), while maintaining shared embeddings between them. A detailed description of this architecture is provided in Section~\ref{sec:subsec-update}.


\textbf{Embedding Distance.}
As detailed in Table \ref{tab:edge_distance_prediction_mlp}, The edge distance MLP computes pairwise distances between residue embeddings. First, it concatenates
pairs of 128-dimensional residue embeddings into 256-dimensional vectors. These concatenated features are processed through a two-layer MLP with SiLU activations. The final linear projection produces a scalar value representing the predicted pairwise distance or interaction strength.

\input{algorithm/distance.tex}

\textbf{Residue Type Prediction.}
As shown in Table \ref{tab:edge_distance_prediction_mlp}, the residue type prediction MLP converts the learned 128-dimensional residue representations into probability distributions over 20 amino acid classes through a two-layer MLP with skip connections. The architecture processes features sequentially: an initial SiLU activation and linear projection preserve the 128-dimensional space, followed by a second SiLU activation and final linear transformation that maps to 20-dimensional amino acid class probabilities.


\subsection{Baselines}
\label{appendix-sec:baselines}
For the baselines, we adopt the hyperparameters and training procedures from their official releases since all methods utilize SAbDab to construct training sets of similar scale and distribution. We save model parameters from the top 10 validation rounds, compute metrics for each model independently, and report the mean values as final results. State-of-the-art models on each task are included as competitors. 

In Task 1, we evaluate against RosettaAb, an energy-based method using statistical functions \cite{adolf2018rabd}; DiffAb, a diffusion-based generative model \cite{luo2022diffab}; MEAN, an equivariant attention network \cite{kong2023mean}; HERN, an end-to-end framework limited to CDR-H3 \cite{jin2022hern}; and dyMEAN, an adaptive multi-channel equivariant network \cite{kong2023dymean}.

For Task 4, we compare Igformer against IgFold$\Rightarrow$HDock, IgFold$\Rightarrow$HERN, GT$\Rightarrow$HERN, and dyMEAN, each employing different docking strategies. IgFold$\Rightarrow$HDock follows a two-stage pipeline where IgFold first predicts the antibody structure \cite{ruffolo2023IgFold}, and HDock \cite{yan2020hdock}, using knowledge-based scoring functions, performs docking. IgFold is a specialized version of AlphaFold tailored for antibody structure prediction, while HDock applies traditional docking approaches. In contrast, IgFold$\Rightarrow$HERN adopts a similar two-stage approach but replaces HDock with HERN \cite{jin2022hern}, which takes the IgFold-predicted backbone structure as input and generates docked backbones, followed by Rosetta for side-chain packing \cite{adolf2018rabd}. GT$\Rightarrow$HERN further extends this pipeline by using ground truth antibody structures instead of predicted ones, allowing HERN to dock CDR-H3 along with other regions toward the epitope, thus providing an upper bound on performance by leveraging perfect structural information. Unlike these multi-stage pipelines, dyMEAN distinguishes itself as a fully end-to-end approach that directly models antigen-antibody interactions without requiring separate structure prediction and docking stages \cite{kong2023dymean}.

We adopt hyperparameters and training procedures from their official releases, as all methods utilize SAbDab for training. Specifically, HERN by Jin et al. \cite{jin2022hern} uses a larger hidden size of 256 with four layers and 16 RBF kernels for distance embedding. DiffAb by Luo et al. \cite{luo2022diffab} employs a hidden size of 128, a pair feature size of 64, six layers, and 100 diffusion steps. MEAN and dyMEAN share similar parameters, including an embedding size of 64, a hidden size of 128, three layers, three iterations, and nine nearest neighbors. dyMEAN introduces an additional parameter, d = 16, for atom type and position embedding. All baseline models use nine neighbors for KNN graph construction. 


\subsection{Metrics}
\label{appendix-sec:metrics}
We employ a comprehensive set of metrics to evaluate model performance in both antibody 1D sequence and 3D structure prediction tasks, encompassing both structural accuracy and sequence recovery:
\begin{itemize}[topsep=0.5mm, partopsep=0pt, itemsep=0pt, leftmargin=10pt]
    \item DockQ \cite{basu2016dockq}: This metric evaluates antibody-antigen interaction quality, especially within the paratope-epitope binding region. While the model optimizes paratope-epitope interactions within a specific cutoff distance during training, its evaluation considers interactions with the complete antigen structure, ensuring a comprehensive assessment of binding interface prediction.
    \item Root Mean Square Deviation (RMSD): This metric measures structural accuracy by calculating unaligned distances between predicted and actual CA atoms in CDR regions, providing direct assessment of local structural precision.
    \item TM-score \cite{zhang2005tmalign}: This metric assesses global structural similarity between predicted and reference antibody structures, evaluating the overall quality of structure prediction across the entire antibody.
    \item Local Distance Difference Test (lDDT) \cite{mariani2013lddt}: This metric provides an atomic-level assessment of local structural accuracy by comparing predicted and actual atomic positions across the entire antibody, offering detailed insights into structural fidelity.
    \item Amino Acid Recovery (AAR): This metric calculates the fraction of correctly predicted amino acids across the entire sequence, measuring overall sequence prediction accuracy.
    \item Contact AAR \cite{ramaraj2012caar}: This metric specifically evaluates prediction accuracy for residues in direct contact with the epitope, defined by a 6.6 \AA\ distance threshold, focusing on binding interface accuracy.
\end{itemize}


\section{Additional Experimental Results}
\label{appendix-sec:exp-results}

\input{table/table8-appendix-paratope}


\subsection{Additional Ablation Study}
\label{appendix-sec:subsec-ablation}
{\bf Choice of Paratope.}
We investigate the impact of paratope selection by comparing models using CDR-H3 alone versus both CDR-H3 and CDR-L3 regions. As shown in Table \ref{tab:additional-paratope}, the choice of paratope influences model performance across different tasks.
For Task 1, using only CDR-H3 achieves superior performance with a TMscore of 0.9757, lDDT of 0.8650, RMSD of 7.15, and DockQ score of 0.45, compared to 0.9734, 0.8512, 8.05, and 0.41 respectively when including both CDR-H3 and L3. This trend continues in Task 3, where CDR-H3 alone yields better TMscore (0.9681 vs 0.9667), lDDT (0.7580 vs 0.7490), and DockQ (0.4600 vs 0.4367). Task 4 follows a similar pattern with improvements across all metrics when using only CDR-H3. 
Interestingly, Task 2 exhibits different characteristics, where including both CDR-H3 and CDR-L3 leads to improved performance, with TMscore increasing from 0.9706 to 0.9750, lDDT from 0.8195 to 0.8311, and DockQ from 0.4255 to 0.4817. These results underscore the importance of task-specific paratope selection for optimal performance.

{\bf Ablation Study on Task 4.}
We conduct additional experiments to evaluate the individual contributions of key components in Igformer for complex structure prediction. Table~\ref{tab:ablation-appendix} presents the performance impact of removing each architectural component. Our observations are as follows. Both the triangle multiplicative module and axial attention prove critical to model performance. The removal of TM leads to the most significant degradation, with RMSD increasing to 8.12 and DockQ dropping to 0.447, while removing AA results in an RMSD of 8.06 and DockQ of 0.453. Additionally, replacing the dual EMP architecture with a single message passing framework diminishes performance, with RMSD rising to 7.99 and DockQ decreasing to 0.452. These systematic evaluations further validate the essential contribution of each component in Igformer.


\input{table/table7-appendix-ablation}

\subsection{Hyperparameter analysis}
\label{appendix-sec:subsec-hyperparameter}

As illustrated in Figure \ref{fig:parameters}, we analyze the impact of learnable weight factor $w$ (refer to Section~\ref{sec:subsec-emp}) on similarity computation across Tasks 1-4. 
The DockQ, as a primary performance metric, consistently achieves its peak at $w = 0.2$ across all tasks. This optimal value indicates that atom-level similarity plays a predominant role in model performance.
While both residue-level similarity (parameterized by $w$) and atom-level similarity contribute to modeling structural interactions, the optimal performance at $w = 0.2$ suggests that emphasizing atomic information (0.8) over residue-level features (0.2) most effectively captures antibody-antigen interactions. This finding provides crucial guidance for optimizing similarity matrix parameters in antibody sequence design and structure prediction tasks.


\begin{figure}[t]
    \centering
    \includegraphics[height=55mm]{figure/w1_w2.pdf}
    \vspace{-6mm}
    \caption{DockQ scores for varying $w$ values across four tasks, with the peak observed at $w=0.2$, emphasizing the importance of geometric distance in similairty matrix computation.}
    \vspace{-4mm}
    \label{fig:parameters}
\end{figure}


\subsection{Additional Case Study of Task 1}
\label{appendix-sec:subsec-case-study}
Figure~\ref{fig:fig6-appendix-case} illustrates more antibody structures generated by Igformer in Task 1.

\begin{figure}[t]
\centering
%\vspace{2mm}
  \begin{small}
    \begin{tabular}{ccc}
        \includegraphics[height=50mm]{figure/case-study/1a14.pdf} &
        \hspace{-3mm}
        \includegraphics[height=50mm]{figure/case-study/1fe8.pdf} &
        \hspace{-3mm}
        \includegraphics[height=50mm]{figure/case-study/1ic7.pdf} \\
        \includegraphics[height=50mm]{figure/case-study/1uj3.pdf} &
        \hspace{-3mm}
        \includegraphics[height=50mm]{figure/case-study/4ffv.pdf} &
        \hspace{-3mm}
        \includegraphics[height=50mm]{figure/case-study/5d93.pdf} \\
    \end{tabular}
    \vspace{-2mm}
    \caption{Additional antibody structures generated by Igformer.}
    \label{fig:fig6-appendix-case}
    \vspace{-4mm}
  \end{small}
\end{figure}

