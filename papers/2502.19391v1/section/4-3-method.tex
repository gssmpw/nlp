\section{Method}
\label{sec:framework}

In this section, we present Igformer, an end-to-end framework for antibody co-design that introduces a novel approach to modeling antibody-antigen interactions. At its core, Igformer advances previous attempts through a sophisticated inter-graph refinement mechanism that combines personalized propagation with global attention, enabling more accurate modeling of complex interactions within antibody-antigen binding interfaces.
As illustrated in Figure~\ref{fig:fig3-framework}, Igformer consists of three key components: (i) Equivariant Message Passing module, (ii) Inter-graph Refinement module, and (iii) Iterative Update module. In the following, we will detail the initialization process in Section~\ref{sec:subsec-init}, elaborate on the Equivariant Message Passing module in Section~\ref{sec:subsec-emp}, followed by the Inter-Graph Refinement module in Section~\ref{sec:subsec-inter-graph}, and the Iterative Update module in Section~\ref{sec:subsec-update}. We then present the Igformer pipeline in Section~\ref{sec:subsec-pipeline} and learning objectives in Section~\ref{sec:subsec-loss}.



\subsection{Initialization}
\label{sec:subsec-init}
The foundation of Igformer lies in its comprehensive representation scheme that integrates both biochemical and structural information through embedding and coordinate initialization steps.

{\bf Feature initialization.}
For each residue $v_i$, we construct a dual-component embedding: a residue embedding $\vect{H}_i^{res} \in \mathbb{R}^{d}$ that encodes amino acid properties, and a position embedding $\vect{H}_i^{pos} \in \mathbb{R}^{d}$ that captures sequential context through distinct indices for antigen, heavy chain, and light chain regions. These components are combined additively to form the final residue representation:
$
    \vect{H}_i = \vect{H}_i^{res} + \vect{H}_i^{pos},
$
This initialization approach enables the simultaneous processing of amino acid properties and their structural context while differentiating between functionally distinct antibody-antigen complex regions. More details of embedding initialization are provided in Appendix~\ref{appendix-sec:embeds}.


{\bf Coordinate initialization.}
Igformer employs a structured coordinate initialization strategy that combines template-based modeling with precise alignment and normalization procedures. For the epitope region $V_{ae}$, we maintain the actual experimental coordinates, while other regions are initialized using templates (refer to Appendix~\ref{appendix-sec:subsec-template-coord}) that provide positions of backbone atoms (N, CA, C, O). Missing template coordinates are estimated through linear interpolation between the nearest known template positions along the sequence.
We then optimize these initial structures through rigid-body alignment using the Kabsch algorithm \cite{kabsch1976rotation}, which minimizes structural discrepancies between the template and known coordinates.
Following initialization, we apply a chain-specific processing pipeline to establish a consistent reference frame. Coordinates of each chain are centered relative to their respective virtual center of mass, and subsequently normalized to ensure uniform scale across the complex.
This carefully calibrated initialization process provides a robust foundation for subsequent equivariant message passing and inter-graph refinement stages. For a comprehensive description of the coordinate initialization and processing procedures, we refer readers to Appendix~\ref{appendix-sec:coord}.


\subsection{Equivariant Message Passing}
\label{sec:subsec-emp}

Given input coordinates $\vect{X}$ and embeddings $\vect{H}$ of residues in a graph $G$, at the $l$-th layer, the {\em equivariant message passing (EMP)} module updates residue representations and coordinates 
$\vect{H}_i^{(l+1)}, \vect{X}_i^{(l+1)} = \textit{EMP}\left(\vect{H}_i^{(l)}, \vect{X}_i^{(l)}\right)$ through four sequential steps.

First, we compute multi-level pairwise residue similarities by combining residue and atomic information:
\begin{equation*}
\begin{aligned}
    \Delta \vect{X}_{ij}^{(l)} &= \vect{X}_{i}^{(l)} - \vect{X}_{j}^{(l)}, \ sim_{ij}^{res} = \Delta \vect{X}_{ij}^{(l)}(\Delta \vect{X}_{ij}^{(l)})^{\top}, \\
    sim_{ij(m,n)}^{atom} &= \sum_{c=1}^3 (\vect{X}_{i,m,c} - \vect{X}_{j,n,c})(\vect{X}_{i,m,c} - \vect{X}_{j,n,c})^{\top},
\end{aligned}
\end{equation*}
where $m, n \in \{1,\cdots,14\}$\footnote{Each residue is represented using 14 atoms in 3D space.} denote different atoms in the residue. The multi-level positional information is processed through MLPs and combined into a similarity matrix:
\begin{equation*}
    \vect{S}_{i,j}^{(l)} = w \cdot \textit{MLP}_1(sim_{ij}^{res}) + (1-w) \cdot \textit{MLP}_2(sim_{ij}^{atom}).
\end{equation*}
Next, we update edge features by incorporating node information and similarity scores:
\begin{equation*}
    \vect{H}_{e_{ij}}^{(l+1)} = \textit{EdgeMLP} \left(  \vect{H}_i^{(l)} \oplus \vect{H}_j^{(l)} \oplus \vect{S}_{i,j}^{(l)} \oplus \vect{H}_{e_{ij}}^{(l)}  \right),
\end{equation*}
where $\oplus$ denotes concatenation operation. The coordinates are subsequently updated through weighted aggregation of neighbor differences:
\begin{equation*}
    \vect{X}_i^{(l+1)} = \vect{X}_i^{(l)} + \sum_{j \in \mathcal{N}(i)} \Delta \vect{X}_{ij}^{(l)} \cdot \textit{CoordMLP}(\vect{H}_{e_{ij}}^{(l+1)}),
\end{equation*}
where $\mathcal{N}_i$ denotes the neighboring nodes of $v_i$ in graph $G$.
Finally, node features are updated by aggregating neighboring edge information with residual connections:
\begin{equation*}
    \vect{H}_i^{(l+1)} = \vect{H}_i^{(l)} + \textit{NodeMLP} \left(\vect{H}_i^{(l)} \oplus \sum_{j \in \mathcal{N}(i)} \vect{H}_{e_{ij}}^{(l+1)} \right)
\end{equation*}
This E(3)-equivariant (refer to Definition~\ref{def:e3-equiv-inv} in Appendix~\ref{appendix-sec:subsec-e3-def}) message passing scheme ensures that both spatial and biochemical properties are properly captured and updated during the message passing process.
\begin{theorem}
\label{thm:thm1-emp}
    For any transformation $T \in E(3)$, we have $\vect{H}_i^{(l+1)}, T(\vect{X}_i^{(l+1)}) = \textit{EMP}\left(\vect{H}_i^{(l)}, T(\vect{X}_i^{(l)})\right)$, where $T(X) := \vect{Q}\vect{X} + \vect{b}$ denotes the E(3) transformation of $\vect{X}$.
\end{theorem}
All proofs of theorems are provided in Appendix~\ref{appendix-sec:proof}.

\subsection{Inter-Graph Refinement}
\label{sec:subsec-inter-graph}
The inter-graph $G_{inter} = (V_p,V_{ae},E_{inter})$ represents interactions in the antibody-antigen binding interface between antibody paratope residues $V_{p}$ and antigen epitope residues $V_{ae}$. 
The key innovation of Igformer lies in its sophisticated inter-graph refinement strategy, which combines personalized propagation with global attention to capture complex antibody-antigen interactions. 
Our approach integrates two complementary components: {\em approximate personalized propagation (APP)} for preserving local structural information and {\em simplified graph transformer (SGFormer)} for capturing global dependencies.
Next, we elaborate on the graph generation and refinement process.

{\bf Approximate Personalized Propagation.}
We construct intra-graphs $G_{ab}$ and $G_{ae}$ by establishing connectivity between $k$-nearest residues based on atomic-level coordinate comparisons, ensuring that graph connectivity reflects spatial relationships within both antibody and antigen components. A detailed description of the intra-graph construction process is provided in Appendix~\ref{appendix-sec:graph-construct}.

Given the residue embeddings $\vect{H}^{(0)}$ updated by EMP in the intra-graph, APP performs iterative message passing while maintaining a balance between local and global information:
\begin{equation}
\label{eq:eq-app}
    \vect{H}^{(j+1)} = (1 - \alpha) \cdot \hat{\vect{P}} \vect{H}^{(j)} + \alpha \cdot \vect{H}^{(0)},
\end{equation}
where $\hat{\vect{P}} = \hat{\mathbf{D}}^{-1/2} \hat{\mathbf{A}} \hat{\mathbf{D}}^{-1/2}$ denotes the normalized transition matrix in the intra-graph with self-loops, and $\alpha$ controls the retention of initial node features. This propagation scheme enables the updated embeddings to capture neighborhood context while preserving crucial local structural information \cite{klicpera2019appnp}.

{\bf Simplified Graph Transformer.}
Subsequently, we introduce a Simplified Graph Transformer (SGFormer) component that implements a global attention mechanism to dynamically model complex interactions between residues across the entire binding interface $V_{\textit{inter}} = V_{p} \cup V_{ae}$.
Specifically, for a pair of residues $(v_i,v_j)$ in the given antibody-antigen binding region, we compute the attention weight:
\begin{equation*}
    a_{ij} = \frac{\exp\left(\vect{H}_i^{\top} \vect{W} \vect{H}_j\right)}{\sum_{k \in V} \exp\left(\vect{H}_i^{\top} \vect{W} \vect{H}_k\right)},
\end{equation*}
where $\vect{W}$ is a learnable weight matrix. Residue embeddings are updated through attention-weighted aggregation:
\begin{equation*}
    \vect{H}_i = \sum_{j \in V} a_{ij} \cdot \vect{H}_j.
\end{equation*}
This self-attention mechanism allows SGFormer to globally capture interactions between all residues in the binding region, thereby capturing both local information and long-range dependencies. 

{\bf Inter-Graph Refinement.}
To model the binding interface dynamics, we encode bi-directional interactions between antibody paratope and antigen epitope residues into edge embeddings.
For each residue pair $(v_i,v_j)$, where $v_i$ belongs to the antibody paratope and $v_j$ to the antigen epitope. The updated pair-wise distance $dist_{ij}$ is:
\begin{equation*}
    dist_{ij} = \textit{EdgeMLP}\left(\vect{H}_i \oplus \vect{H}_j, \vect{H}_j \oplus \vect{H}_i \right).
\end{equation*}
These learned distances capture the complex chemical and spatial relationships between residues, enabling dynamic refinement of the inter-graph structure through $k$-nearest neighbor selection. The complete inter-graph refinement process is detailed in Appendix~\ref{appendix-sec:graph-construct}.
\begin{remark}
    The integration of APP and SGFormer enables comprehensive refinement of both residue and edge representations. APP preserving crucial local structural properties while facilitating efficient information propagation, and the attention mechanism in SGFormer captures dynamic residue interactions across the entire binding interface. This dual-approach enables precise modeling of both local chemical interactions and long-range structural dependencies, which is crucial for accurate antibody structure prediction, as we will show in our experiments.
\end{remark}




\subsection{Iterative Update} 
\label{sec:subsec-update}
Our framework implements a sophisticated iterative learning process that incorporates triangle attention mechanism \cite{jumper2021alphafold} with a dual-EMP module to capture complex interactions within antibody-antigen binding interfaces. The process begins with the initial embeddings $\vect{H}_{full} = \vect{H}_{ae} \cup \vect{H}_{ab}$ and extracts the interface-specific representations $\vect{H}_{inter} = \vect{H}_{ae} \cup \vect{H}_{p}$ from $\vect{H}_{full}$. The corresponding coordinates $\vect{X}_{full} = \vect{X}_{ae} \cup \vect{X}_{ab}$ and $\vect{X}_{inter} = \vect{X}_{ae} \cup \vect{X}_{p}$ are initialized according to Appendix~\ref{appendix-sec:coord}. 
For clarity, we denote $\vect{H}_{full}$ as $\vect{H}_{intra}$ and $\vect{X}_{full}$ as $\vect{X}_{intra}$ in subsequent discussions.

{\bf Triangle Multiplicative Module.}
Within the paratope region $V_p$ containing $n$ residues, we construct a normalized interaction matrix $\vect{Z} \in \mathbb{R}^{n \times n}$ to encode pairwise residue relationships for each residue pair $(v_i,v_j)$:
\begin{equation*}
    \vect{Z}_{ij} = \textit{LayerNorm} \left(\textit{MLP} \left([\vect{H}_i \oplus \vect{H}_j]\right) \right).
\end{equation*}
The concatenation operation $\oplus$ ensures directional sensitivity through its non-commutative nature. This interaction matrix undergoes iterative refinement through two complementary mechanisms: triangle multiplicative module and axial attention.

The triangle multiplicative module processes embeddings through learnable normalized projections to generate pair-wise residue interactions in the paratope:
\begin{equation*}
    \vect{H}_l = \vect{W}_l \vect{Z}, \quad \vect{H}_r = \vect{W}_r \vect{Z}, \quad \vect{I}_{ij} = (\vect{H}_l)_i^{\top} (\vect{H}_r)_j,
\end{equation*}
where $\vect{W}_l$ and $\vect{W}_r$ are learnable weight matrices.
The resulting pair-wise interactions are then modulated by a gating mechanism and projected into the embedding space:
\begin{equation*}
    f_{triangle}^{out}(\vect{H}) = \vect{W}_o \cdot \sigma(\vect{W}_g \vect{Z}) \cdot \vect{I},
\end{equation*}
where $ \sigma(\cdot) $ is the sigmoid function.

{\bf Axial Attention.}
The row-wise axial attention mechanism computes dynamic residue relationships through a scaled dot-product attention:
\begin{equation*}
    f_{att}^{out}(\vect{H}) = \text{softmax}\left( \frac{\vect{Q}^{\top} \vect{K}}{\sqrt{d_k}} \right) \cdot \vect{V}, 
\end{equation*}
where residue embeddings in $\vect{Q} = \vect{W}_q \vect{H}$, $\vect{K} = \vect{W}_k \vect{H}$, and $\vect{V} = \vect{W}_v \vect{H}$ are generated by $f_{triangle}^{out}(\vect{H})$, and $\vect{W}_q$, $\vect{W}_k$, and $\vect{W}_v$ are learnable parameters. The final paratope outgoing representation integrates both attention and triangle multiplicative mechanism:
\begin{equation*}
    f_{out}(\vect{H}) = f_{triangle}^{out}(\vect{H}) + f_{att}^{out}(\vect{H}).
\end{equation*}
The representations of the paratope region are updated iteratively integrating both outgoing and incoming representations:
\begin{equation}
\label{eq:eq-paratope}
    \vect{H}^{(k+1)} = \vect{H}^{(k)} + f_{out}(\vect{H}^{(k)}) + f_{in}(\vect{H}^{(k)}),
\end{equation}
where $f_{in}(\vect{H})$ represents the ingoing update analogous to the outgoing formulation but adopts ingoing (column-wise) multiplicative interactions and column-wise attention.
This bidirectional updating process ensures that paratope embedding $\vect{H}$ captures both pairwise geometric interactions via $ f_{triangle} $ and dynamic global dependencies via $f_{att}$.

After $Ks$ iterations, the diagonal elements of paratope embeddings calculated by Equation~\ref{eq:eq-paratope} are extracted to generate the final embeddings of paratope:  
\begin{equation}
\label{eq:eq-paratope-final}
    \vect{H}_{p} = \textit{MLP} \left(\textit{diag} \left( \vect{H}^{(K)} \right) \right),
\end{equation}


{\bf Dual EMP Module.} Next, we employ a dual-scale message-passing framework using two separate EMP modules to model intra-graph $G_{intra}$ and inter-graph $G_{inter}$ interactions:
\begin{equation}
\label{eq:eq-dual-emp}
\begin{aligned}
    \left( \vect{H}_{\textit{intra}}^{(t+1)}, \vect{X}_{\textit{intra}}^{(t+1)}\right) &= \textit{EMP}_{\textit{intra}} \left(\vect{H}_{\textit{intra}}^{(t)}, \vect{X}_{\textit{intra}}^{(t)} \right), \\
    \left( \vect{H}_{\textit{inter}}^{(t+1)}, \vect{X}_{\textit{inter}}^{(t+1)}\right) &= \textit{EMP}_{\textit{inter}} \left( \vect{H}_{\textit{inter}}^{(t)}, \vect{X}_{\textit{inter}}^{(t)} \right).
\end{aligned}
\end{equation}
At each iteration, we update our graph representations according to Equation~\ref{eq:eq-dual-emp}, and substitute the paratope embeddings in the intra-graph $\vect{H}_{\textit{intra}}^{(t+1)}$ using $\vect{H}_{\textit{inter}}^{(t+1)}$. On the other hand, coordinates for intra- and inter-graph are maintained separately. This dual-scale message-passing framework enables comprehensive modeling of both local residue relationships and global antigen-antibody interactions, with each scale optimized for its specific context.
Finally, residue connections are incorporated in the updated embeddings for stable learning across different training iterations.

\subsection{Igformer Pipeline}
\label{sec:subsec-pipeline}
We present the key stages of the Igformer learning pipeline, with detailed training algorithms provided in Appendix~\ref{appendix-sec:pipeline}. 
The process begins with the initialization of residue representations and coordinates. Following this, the intra-graph is constructed for personalized information propagation, which is then processed by the SGFormer for inter-graph refinement. The embeddings generated in this process serve exclusively for inter-graph refinement and are not involved in the representation learning process. Concurrently, a triangle attention module updates the representation of each residue in the paratope. Finally, a dual EMP module processes residue representations to generate the final coordinates and representations, which are then used for downstream structure and sequence prediction tasks.

The following theorem indicates that the coordinates generated by Igformer are E(3)-equivariant and the residue embeddings are invariant.
\begin{theorem}
\label{thm:thm-igformer}
    Let $\hat{\vect{H}}_i, \hat{\vect{X}}_i = \textit{Igformer} \left( \vect{H}_i^{(0)}, \vect{X_i} \right)$ denote the embedding and coordinate of $v_i$ generated by Igformer.
    For any transformation $T \in E(3)$, we have $\hat{\vect{H}}_i, T(\hat{\vect{X}}_i) = \textit{Igformer}\left(\vect{H}_i^{(0)}, T(\vect{X_i})\right)$, where $T(X) := \vect{Q}\vect{X} + \vect{b}$ denotes the E(3) transformation of $\vect{X}$.
\end{theorem}

\subsection{Prediction and Loss Function}
\label{sec:subsec-loss}

{\bf Sequence Prediction.}
The $i$-th position in the amino acid sequence of the paratope is predicted using its embedding:
$$
    s_i = \textit{softmax}\left( \textit{MLP}\left(\vect{H}_i\right) \right)
$$
{\bf Structure Prediction.}
The 3D structures of paratope are generated by the coordinates $\vect{X}$ computed in the last round of Dual EMP Module via Equation~\ref{eq:eq-dual-emp}.

{\bf Loss Function.}
The loss function of Igformer consists of three components:
\begin{equation}
\label{eq:eq-loss}
  \mathcal{L} =  \mathcal{L}_{\textit{seq}} + \mathcal{L}_{\textit{struct}} +  \mathcal{L}_{\textit{interface}}
\end{equation}
Here, $\mathcal{L}_{\textit{seq}}$ is the cross entropy loss that minimizes the dissimilarities between predicted and original 1D sequences. $\mathcal{L}_{\textit{struct}}$ minimizes the difference between reconstructed and ground-truth 3D structures. $\mathcal{L}_{\textit{interface}}$ optimizes the reconstructed inter-graph. Detailed descriptions of each component are provided in Appendix~\ref{appendix-sec:loss}.

