\documentclass{article}

\usepackage{microtype}
\usepackage{graphicx}
\usepackage{booktabs}

\usepackage{hyperref}

\newcommand{\theHalgorithm}{\arabic{algorithm}}
\usepackage[accepted]{arxiv/style_folder/style}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{dsfont}

\usepackage[capitalize,noabbrev]{cleveref}

\usepackage{balance}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

\usepackage[textsize=tiny]{todonotes}

\input{utils/my_pkgs}
\input{utils/my_cmds}
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}


\icmltitlerunning{Model-Based Exploration in Mon-MDPs}

\usepackage{xcolor}
\newcommand{\MET}[1]{\textcolor{blue}{MET: #1}}
\newcommand{\SIM}[1]{\textcolor{purple}{SIM: #1}}
\newcommand{\MHB}[1]{\textcolor{brown}{MHB: #1}}

\newenvironment{packed_enum}{
\begin{enumerate}
  \setlength{\itemsep}{1pt}
  \setlength{\parskip}{0pt}
  \setlength{\parsep}{0pt}
}{\end{enumerate}}

 
\begin{document}

\twocolumn[
\icmltitle{Model-Based Exploration in Monitored Markov Decision Processes}

\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Alireza Kazemipour}{yyy,zzz}
\icmlauthor{Simone Parisi}{yyy,zzz}
\icmlauthor{Matthew E. Taylor}{yyy,zzz,ccc}
\icmlauthor{Michael Bowling}{yyy,zzz,ccc}
\end{icmlauthorlist}

\icmlaffiliation{yyy}{Department of Computing Science, University of Alberta, Edmonton, AB T6G 2R3, Canada}
\icmlaffiliation{zzz}{Alberta Machine Intelligence Institute (Amii)}
\icmlaffiliation{ccc}{Canada CIFAR AI Chair}


\icmlcorrespondingauthor{Alireza Kazemipour}{kazemipo@ualberta.ca}

\icmlkeywords{Reinforcement Learning, Exploration-Exploitation, Model-Based Interval Estimation, Monitored Markov Decision Processes}

\vskip 0.3in
]

\printAffiliationsAndNotice{} 

\begin{abstract}
A tenet of reinforcement learning is that rewards are always observed by the agent. However, this is not true in many realistic settings, e.g., a human observer may not always be able to provide rewards, a sensor to observe rewards may be limited or broken, or rewards may be unavailable during deployment.  Monitored Markov decision processes (Mon-MDPs) have recently been proposed as a model of such settings. Yet, Mon-MDP algorithms developed thus far do not fully exploit the problem structure, cannot take advantage of a known monitor, have no worst-case guarantees for ``unsolvable'' Mon-MDPs without specific initialization, and only have asymptotic proofs of convergence. This paper makes three contributions. 
First, we introduce a model-based algorithm for Mon-MDPs that addresses all of these shortcomings. The algorithm uses two instances of model-based interval estimation, one to guarantee that observable rewards are indeed observed, and another to learn the optimal policy.  
Second, empirical results demonstrate these advantages, showing faster convergence than prior algorithms in over two dozen benchmark settings, and even more dramatic improvements when the monitor process is known.
Third, we present the first finite-sample bound on performance and show convergence to an optimal worst-case policy when some rewards are never observable. 
\end{abstract}
%
\section{Introduction}
Reinforcement learning (RL) is founded on trial-and-error: instead of being directly shown what to do, an agent receives consistent numerical feedback for its decisions in reward. However, this assumption is not always realistic as the feedback often comes from an exogenous entity such as humans~\citep{Shao2020Concept2RobotLM, hejna2024inverse} or monitoring instrumentation~\citep{thanhlong2021barrier}. Assuming the reward is available at all times is not reasonable in such settings, e.g., due to time constraints of humans~\citep{pilarski2011online}, hardware failure~\citep{bossev2016radiation, dixit2021silent}, or inaccessible rewards during deployment~\citep{andrychowicz2020learning}.  Hence, relaxing the assumption that rewards are always observable would mark a significant step towards agents continually and autonomously operating in the real world.  Monitored Markov decision processes (Mon-MDPs)~\citep{parisi2024monitored} have been proposed as an extension of MDPs to model such situations, with algorithms for Mon-MDPs still in their infancy~\cite{parisi2024beyond}.
Existing algorithms do not take advantage of the structure of Mon-MDPs, typically focusing exploration on the state-action space, and only have asymptotic guarantees rather than finite sample complexity bounds. Furthermore, they have focused on \emph{solvable} Mon-MDPs where it is possible to observe every reward under some circumstances. The original introduction of Mon-MDPs also considered \emph{unsolvable} Mon-MDPs, proposing a minimax formulation as optimal behavior, but no algorithms have explored this setting.

In this paper we introduce \textit{Monitored Model-Based Interval Estimation with Exploration Bonus (\thealgo)}, a model-based algorithm for Mon-MDPs with a number of advantages over previous algorithms. \thealgo exploits the Mon-MDP structure to consider its uncertainty on each of the unknown components separately.  This approach also makes it the first algorithm that can take advantage of situations where the monitoring process is known by the agent in advance.  Furthermore, \thealgo balances optimism in uncertain quantities with pessimism for rewards that have never been observed, reaching minimax-optimal behavior in unsolvable Mon-MDPs.  This is challenging as pessimism may dissuade agents from exploring sufficiently in solvable Mon-MDPs. We address this by having a \emph{second} instance of MBIE to force the agent to efficiently observe all rewards that can be observed.  Building off of MBIE~\cite{strehl2008analysis}, we prove the first polynomial sample complexity bounds, which applies equally to solvable and unsolvable Mon-MDPs.  We then explore the efficacy of \thealgo empirically. We show its efficient exploration in practice, outperforming the recent Directed-E$^2$~\cite{parisi2024beyond} on two dozen benchmark environments, including all of the environments from~\citet{parisi2024monitored}.  We show that it indeed converges to optimal policies in solvable Mon-MDPs and minimax-optimal policies unsolvable Mon-MDPs, and is able to separate the two.  Finally, we show that it can exploit knowledge of the monitoring process to learn even faster.

%
\section{Preliminaries}
\label{sec:preliminaries}
Traditionally the RL agent-environment interaction is modeled as a Markov decision process (MDPs)~\citep{puterman1994discounted, sutton2018reinforcement}: at every timestep $t$ the agent performs an action $A_t$\footnote{We denote random variables with capital letters.} according to the environment state $S_t$; in turn, the environment transitions to a new state $S_{t+1}$ and generates a bounded numerical reward $R_t$. Rewards are assumed to be observable by the agent at all times, and any partial observability is only considered for states, resulting in partially observable MDPs (POMDPs)~\citep{KAELBLING199899, chadÃ¨s2021}. 
Until recently, prior work on partially observable rewards was limited to active RL~\citep{schulze2018active, krueger2020active} and RL from human feedback (RLHF)~\citep{kausik2024framework}.
However, these frameworks lack the complexity to formalize situations where the reward observability stems from stochastic processes --- in active RL, the reward can always be observed by simply paying a cost; in RLHF the reward is either never observable (and all guidance comes from the human) or always observable (and the human provides additional guidance). In neither of these settings are there rewards that the agent can only \textit{sometimes} see and whose observability can be predicted and possibly controlled. 

\begin{figure}[t]
    \centering
    \begin{subfigure}[b]{0.49\columnwidth}
        \centering
        \includegraphics[width=\columnwidth]{imgs/diagrams/MDP.pdf}
        \caption{MDP framework} 
        \label{fig:mdp_framework}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\columnwidth}
        \centering
        \includegraphics[width=\columnwidth]{imgs/diagrams/Mon_MDP.pdf}
        \caption{Mon-MDP framework}  
        \label{fig:mon-mdp_framework}
    \end{subfigure}
    \caption{In MDPs (left) the agent interacts only with the environment and observes rewards at all times. In Mon-MDPs (right), the agent also interacts with the monitor, which dictates what rewards the agent observes.}
    \label{fig:mdp_vs_monmdp}
    
\end{figure}
%
Recently, inspired by partial monitoring~\citep{bartok2014partial}, \citet{parisi2024monitored} extended the MDP framework to also consider partially observable rewards by proposing the Monitored MDP (Mon-MDP) framework. In Mon-MDPs, the observability of the reward is dictated by a ``Markovian entity'' (the monitor), thus actions can have either immediate or long-term effects on the reward observability. For example, rewards may become observable only after pressing a button in the environment, or as long as the agent carries some special item, or only in areas where instrumentation is present. 
This opens avenues for model-based methods that try to model the process governing the reward observability, in order to \textit{plan what rewards to observe or to plan visits to states where rewards are more likely to be observed}.
In the next sections, we 1) define Mon-MDPs as an extension of MDPs, 2) define optimality when some rewards may \textit{never be observable}, and 3) highlight how our algorithm addresses current limitations and open challenges.
%
\begin{figure*}[t]
    \begin{subfigure}[t]{0.3\linewidth}
        \centering
        \includegraphics[width=\linewidth]{imgs/scenario/Tresasure_Hunt_Uncertainty.pdf}
        \caption{\texttt{LEFT}'s outcome cannot be observed and will always be unknown.}  
        \label{fig:dilemma}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.3\linewidth}
        \centering
        \includegraphics[width=\linewidth]{imgs/scenario/Tresasure_Hunt_Cautious.pdf}
        \caption{A pessimistic agent assumes the worst (snake) for \texttt{LEFT}.}
        \label{fig:cautious}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.3\linewidth}
        \centering
        \includegraphics[width=\linewidth]{imgs/scenario/Tresasure_Hunt_Conterfactual.pdf}
        \caption{The agent would be pessimistic even if \texttt{LEFT}'s true outcome is not the snake.}  
        \label{fig:confident-cautious}
    \end{subfigure}
    \caption{\textbf{Example of a pessimistic agent in Mon-MDPs.} (a) The agent has to choose between \texttt{LEFT}, \texttt{UP}, and \texttt{RIGHT}. \texttt{RIGHT} leads to a snake, \texttt{UP} to gold bars, and \texttt{LEFT} to either a snake or a treasure chest (more valuable than gold bars), but the agent \textit{can never observe the result} of executing \texttt{LEFT}. (b) After sufficient attempts\footnotemark, the agent excludes \texttt{LEFT} because its outcome is unknown and the agent assumes the worst. (c) \texttt{LEFT} is ruled out even though it could actually yield the treasure chest. However, since this cannot be known, acting pessimistically complies with minimax-optimality in \cref{eq:minimax}. In the end, the agent explores new actions but it is also wary because some actions may never yield a reward. Thus, after enough exploration, it assumes the worst if the action outcome is still unknown.}
    \label{fig:treasure_hunter_robot}
\end{figure*}
%
\subsection{Monitored Markov Decision Processes}
\label{subsec:mon-mdps}
MDPs are represented by the tuple $\angle{\c{S}, \c{A}, r, P, \gamma}$, where $\c{S}$ is the finite state space, $\c{A}$ is the finite action space, $r: \c{S} \times \c{A} \to [\env{\rmin}, \env{\rmax}]$ is the mean of the reward function, $P: \c{S} \times \c{A} \to \Delta(\c{S})$ is the Markovian transition dynamics\footnote{$\Delta(\c{X})$ denotes the set of distributions over the finite set $\c{X}$.}, and $\gamma \in [0, 1)$ is the discount factor describing the trade-off between immediate and future rewards.
Mon-MDPs extend MDPs by introducing the \textit{monitor}, another entity that the agent interacts with, and is also governed by Markovian transition dynamics. 
Intuitively, Mon-MDPs incorporate two MDPs --- one for the environment and one for the monitor --- and we differentiate quantities associated with each of them using superscripts ``e'' and ``m'', respectively.


In Mon-MDPs, the state and the action spaces are composed of the environment and the monitor spaces, i.e., $\c{S} \coloneq \env{\c{S}} \times \mon{\c{S}}$ and $\c{A} \coloneq \env{\c{A}} \times \mon{\c{A}}$. At every timestep, the agent observes the state of both the environment and the monitor, and performs two actions, one for the environment and one for the monitor. 
The monitor also has Markovian dynamics, i.e., $\mon{P}: \mon{\c{S}} \times \mon{\c{A}} \times \env{\c{S}} \times \env{\c{A}} \to \Delta(\mon{\c{S}})$, and the joint transition dynamics is denoted by $P \coloneq \env{P} \otimes \mon{P}: \c{S} \times \c{A} \to \Delta(\c{S})$. Note the \emph{monitor transition} depends on the \emph{environment state and action}, highlighting the interplay between the monitor and the environment.

Mon-MDPs have two rewards as well, $r \coloneq \left(\env{r}, \mon{r}\right)$, where $\mon{r}: \mon{\c{S}} \times \mon{\c{A}} \to [\mon{\rmin}, \mon{\rmax}]$ is also bounded.
However, unlike MDPs, \textit{the environment rewards are not directly observable}. Instead, the agent observes \textit{proxy rewards} $\env{\widehat{R}} \sim \mon{f}$, where $\mon{f}: \Reals \times \mon{\c{S}} \times \mon{\c{A}} \to \Reals \cup \{\bot\}$ is the monitor function and $\bot$ denotes an ``unobserved reward'', i.e., the agent does not receive any numerical reward.\footnote{Note that $\mon{f}$ could return any arbitrary real number, unrelated to the environment reward. To rule out pathological cases (e.g., the monitor function always returns 0), $\mon{f}$ is assumed to be truthful~\citep{parisi2024monitored}, i.e., the monitor either reveals the true environment reward ($\env{\widehat{R}}_t = \env{R}_t$) or hides it ($\env{\widehat{R}}_t = \bot$).}
Using the above notation, Mon-MDPs can be compactly denoted by the tuple $M = \angle{\c{S}, \c{A}, P, r, \mon{f}, \gamma}$.  \cref{fig:mdp_vs_monmdp} illustrates the agent-environment-monitor interaction.

%Footnote for Figure
\footnotetext{We formally define ``sufficient'' in \cref{thm:sample_cmplx}. Intuitively, the more confident about its guess the agent wants to be, the more it should try the action.}

In Mon-MDPs, the agent executes joint actions $A_t \coloneqq (\env{A}_t, \mon{A}_t)$ at the joint state $S_t \coloneqq (\env{S}_t, \mon{S}_t)$. In turn, the environment and monitor states change and produce a joint reward $(\env{R}_t, \mon{R}_t)$, but the agent observes $(\env{\widehat{R}}, \mon{R}_t)$. 
The agent's goal is to learn a policy $\pi: \c{S} \to \Delta(\c{A)}$ selecting joint actions to maximize the discounted return ${\footnotesize\sum}_{t=1}^\infty \gamma^{t-1} \left(\env{R}_t + \mon{R}_t\right)$ \emph{even though the agent observes $\env{\widehat{R}}_t$ instead of} $\env{R}_t$.
This is the crucial difference between MDPs and Mon-MDPs: the immediate environment reward $\env{R}_t$ \textit{is always generated by the environment}, i.e., desired behavior is well-defined as the reward is sufficient to fully describe the agent's task~\citep{bowling2023settling}. However, the monitor may ``hide it'' from the agent, possibly even always yielding ``unobservable reward'' $\env{\widehat{R}}_t = \bot$ at all times for some states. 
For example, consider a task where the reward is given by a human supervisor (the monitor): if the supervisor must leave, the agent will not observe any reward; yet, the task has not changed, i.e., the human --- \textit{if present} --- would still give the same rewards.
 
\subsection{Learning Objective in Mon-MDPs}
Following MDP notation, we define the V-function $V^M_\pi$ and the Q-function $Q^M_\pi$ as the expected sum of the discounted rewards, and an optimal policy $\pi^*$ as their maximizer: 
\begin{align}
    V^M_\pi(s) &\coloneqq \EV{\sum_{k=t}^{\infty}\gamma^{k - t} (\env{R}_k + \mon{R}_k) \mid S_t=s}{\pi}, \nonumber    
    \\
    Q^M_\pi(s, a) & \coloneqq\EV{\sum_{k=t}^{\infty}\gamma^{k - t} (\env{R}_k + \mon{R}_k) \mid S_t=s, A_t=a}{\pi}, \nonumber 
    \\
    \pi^* &\in \argmax_{\pi \in \Pi} V_\pi^M(s)  \quad\quad \forall s\in \c{S}, \label{eq:pi_opt}
\end{align}
where $\env{R}_k$ and $\mon{R}_k$ are the immediate environment and monitor rewards at timestep $k$. We stress once more that the agent cannot observe $\env{R}$ directly and observes $\env{\widehat{R}}$, even though the environment still assigns rewards to the agent's actions.

\citet{parisi2024monitored} showed it is possible to asymptotically converge to an optimal policy if the monitor function $\mon{f}$ is \textit{ergodic}, i.e., if for all environment pairs $(\env{s}, \env{a})$ the proxy reward will be observable ($\env{\widehat{R}} \neq \bot$) given infinite exploration. Intuitively, this means the agent will be always able to observe every environment reward (infinitely many times, given infinite exploration). 
However, if even one environment reward is \textit{never} observable, the Mon-MDP is \textit{unsolvable} and  we cannot guarantee convergence to an optimal policy. Intuitively, if the agent can never know that a certain state yields the highest (or lowest) environment reward, then it can never learn to visit (or avoid) it.
Nonetheless, we argue that assuming every environment reward is observable (sooner or later) is a very stringent condition, not suitable for real-world tasks --- reward instrumentation may have limited coverage, human supervisors may never be available in the evening, or training before deployment may not guarantee full state coverage.

We follow~\citet[Appendix B.3]{parisi2024monitored} in defining reasonable behavior in an unsolvable Mon-MDP.
First, let $[M]_{\mathbb{I}}$ be the set of all Mon-MDPs the agent cannot distinguish based on the reward observability in $M$. If $M$ is solvable, all rewards can be observed and $[M]_{\mathbb{I}} = \{M\}$. Otherwise, from the agent's perspective, there are possibly infinitely many Mon-MDPs in $[M]_{\mathbb{I}}$ because never-observable rewards could be any real number withing their bounded range. Second, let $M_\downarrow$ be the worst-case Mon-MDP, i.e., the one where all never-observable rewards are $-\env{\rmax}$: 
%
\begin{equation}
\vspace{-3pt}
M_\downarrow \in \argmin_{M' \in [M]_{\mathbb{I}}} V_{\pi^*}^{M'}(s) \quad\quad \forall s\in \c{S}\label{eq:worst_monmdp}
\end{equation}
i.e., $M_\downarrow$ is a Mon-MDP whose optimal value function is minimized over all Mon-MDPs indistinguishable from $M$. 
Then, we define the \textit{minimax-optimal policy} of $M$ as the optimal policy of the worst-case Mon-MDP, i.e., 
\begin{equation}
\pi^*_{\downarrow} \coloneq \argmax_{\pi} \, V_{\pi}^{M_\downarrow}(s). \label{eq:minimax}
\end{equation}
If $M$ is solvable then $[M]_{\mathbb{I}} = \{M\}$, and \cref{eq:minimax} is equivalent to \cref{eq:pi_opt}. So the minimax-optimal policy is simply the optimal policy. 
\cref{fig:treasure_hunter_robot} shows an example of an unsolvable Mon-MDP and the minimax-optimal policy.
%
\section{Monitored Model-Based Interval Estimation}
\def\estimate#1{\tilde{#1}}
\def\model#1{\bar{#1}}
%
We propose a novel model-based algorithm to exploit the structure of Mon-MDPs, show how to apply it on solvable and unsolvable Mon-MDPs, and provide sample complexity bounds.
As our algorithm builds upon MBIE and MBIE-EB~\cite{strehl2008analysis}, we first briefly review both.

\subsection{MBIE and MBIE-EB}
\label{sec:mbie_mbieeb}
MBIE is an algorithm for learning an optimal policy in MDPs with polynomial sample complexity bounds.  MBIE maintains confidence intervals on all unknown quantities (e.g., rewards and transition dynamics) and then solves the set of corresponding MDPs to produce an optimistic value function.  Greedy actions with respect to this value function direct the agent toward state-action pairs that have not been sufficiently visited to be certain whether they are part of the optimal policy or not.  MBIE-EB is a simpler variant that constructs one model of the MDP with exploration bonuses to be optimistic with respect to the uncertain quantities.

Let $\estimate{R}$ and $\estimate{P}$ be maximum likelihood estimates of the MDP's unknown reward and transition dynamics based on the agent's experience, and let $N(s,a)$ count the number of times action $a$ has been taken in state $s$.  MBIE-EB constructs an optimistic MDP,
\begin{equation}
\model R(s,a) = \estimate R(s,a) + 
\underbrace{\frac{\beta}{\sqrt{N(s,a)}}}_{\text{bonus for $R, P$}},
\quad\quad
\model P = \estimate P,
\end{equation}
where $\beta$ is a parameter chosen to be sufficiently large.  It then solves this MDP to find $\model Q_*$, the optimal Q-function under the model, and acts greedily with respect to this value function to gather more data to update its MDP model.
%
\subsection{\thealgo}
\thealgo can be considered an extension of MBIE-EB to the Mon-MDP setting with three important innovations. First, we adapt MBIE-EB to model each of the vital unknown components of the Mon-MDP (reward and transition dynamics of both the environment and the monitor), each with their own exploration bonuses.  Second, observing that the optimism for unobservable environment state-action pairs in an unsolvable Mon-MDP will never vanish --- the agent will try forever to observe rewards that are actually unobservable --- we further adapt the algorithm to make worst-case assumptions on all unobserved environment rewards.  Unfortunately, this creates an additional problem: environment state-action pairs whose rewards are hard to observe may never be sufficiently tried because they are dissuaded by the pessimistic model.  Third, \thealgo balances this minimax objective by interleaving a second MBIE-EB instance that ensures sufficient exploration of unobserved environment state-actions.  We describe these innovations in order.

\textbf{First Innovation: Extend MBIE-EB to Mon-MDPs.}
Let $\env{\estimate{R}}, \mon{\estimate{R}}, \estimate{P}$ be maximum likelihood estimates of the environment reward, monitor reward, and the joint transition dynamics, respectively, all based on the agent's experience.  Let $N(\mon{s}, \mon{a})$ count the number of times action $\mon{a}$ has been taken in $\mon{s}$, $N(s,a)$ count the same joint state-action pairs, and $N(\env{s}, \env{a})$ count environment state-action pairs, \emph{but only if the environment reward was observed.}
We can then construct the following optimistic MDP using reward bonuses for the unknown estimated quantities,
\begin{align}
\model R_{\text{basic}}(s,a) =\,& 
\env{\estimate{R}}(\env{s},\env{a}) + 
\overbrace{\frac{\env{\beta}}{\sqrt{N(\env{s},\env{a})}}}^{\text{bonus for $\env{R}$}} + \nonumber \\
& \,\mon{\estimate{R}}(\mon{s},\mon{a}) + 
\underbrace{\frac{\mon{\beta}}{\sqrt{N(\mon{s},\mon{a})}}}_{\text{bonus for $\mon{R}$}} +
\underbrace{\frac{\beta}{\sqrt{N(s, a)}}}_{\text{bonus for $P$}}, \nonumber
\\
\model P =\,& \estimate P, \label{eq:RBasic}
\end{align}
where $\beta$, $\env{\beta}$, $\mon{\beta}$ are hyperparameters for the confidence level of our optimistic MDP.  As with MBIE-EB, this optimistic model is solved to find $\model Q_*$ and actions are selected greedily. For solvable Mon-MDPs, we can apply MBIE-EB's theoretical result directly to the joint MDP to get a sample complexity bound for this approach. But, this algorithm fails to make any guarantees for unsolvable Mon-MDPs, where some environment rewards are never-observable.  In such situations, $N(s^e, a^e)$ never grows for some state-action pairs, thus optimism will direct the agent to seek out these state-actions, for which it can never reduce its uncertainty.  

\textbf{Second Innovation: Pessimism Instead of Optimism.}
We fix this excessive optimism in \cref{eq:RBasic} by creating a new reward model that is pessimistic, rathter than optimistic, about unobserved environment state-action rewards:
\begin{align}
\model R_{\text{optimize}}(s,a) =& \, \begin{cases}
\model R_{\text{basic}}(s,a) & \text{if $N(\env{s},\env{a}) > 0$} \\
\model R_{\text{min}}(s,a) & \text{otherwise} ,    
\end{cases}
\intertext{where,}
\model R_{\text{min}}(s,a) =& \, 
\env{r}_{\min} + \mon{\estimate{R}}(\mon{s},\mon{a}) \, + \nonumber \\ 
& \frac{\mon{\beta}}{\sqrt{N(\mon{s},\mon{a})}} +
\frac{\beta}{\sqrt{N(s, a)}}.
\end{align}
We call an episode where we take greedy actions according to $\model Q_{*, \text{optimize}}$ an \emph{optimize} episode, as this ideally produces a minimax-optimal policy for all Mon-MDPs. The reader may have already realized this pessimism will introduce a new problem --- dissuading the agent from exploring to observe previously unobserved environment rewards. Instead, we aim to observe all rewards but not too frequently that we prevent the agent from following the minimax-optimal policy when the Mon-MDP is actually unsolvable.

\textbf{Third Innovation: Explore to Observe Rewards.}
We fix this now excessive pessimism by introducing a separate MBIE-guided exploration aimed at discovering previously unobserved environment rewards. The following reward model does exactly that.
\begin{align}
\model R_{\text{observe}}(s,a) &= 
\begin{array}[t]{l}
\frac{\beta^{\text{observe}}}{\sqrt{N(s,a)}} + \\[10pt]
\text{KL-UCB}(N(s, a)) \mathds{1}\scriptstyle{\{N(\env{s}, \env{a}) = 0\}},
\end{array}
\end{align}
where $\mathds{1}\scriptstyle{\{N(\env{s}, \env{a}) = 0\}}$ is an indicator function returning 1 if $N(\env{s}, \env{a}) = 0$ and 0 otherwise. Therefore, the KL-UCB~\cite{garivier2011kl} term is only included for environment state-action pairs whose rewards have not been observed.  
We are using KL-UCB to estimate an upper-confidence bound (with confidence $\beta^{\text{KL-UCB}}$) on the probability of observing the environment reward from joint state-action $(s, a)$ given that we tried $N(s, a)$ times already and have not succeeded.  As we are constructing a bound on a Bernoulli random variable (whether the reward is observed or not), KL-UCB is ideally suited and provides tight bounds.  The result is an optimistic model for an MDP that rewards the agent for observing previously unobserved environment rewards (if they can be observed).  An episode where we take greedy actions with respect to $\model Q_{*, \text{observe}}$ we call an \emph{observe} episode.
If we have enough \emph{observe} episodes, we can guarantee that all observable environment rewards are observed with high probability.  If we have enough \emph{optimize} episodes we can learn and follow the minimax-optimal policy.  We balance between the two by switching the model we optimize according to the following condition 
\begin{align}
\model R(s,a) & =
\left\{ \begin{array}{ll}
\model R_{\text{observe}}(s,a) & \text{if $\kappa \le \kappa^*(k)$} \\
\model R_{\text{optimize}}(s,a) & \text{otherwise} ,
\end{array} \right. 
\end{align}
where $\kappa^*$ is a sublinear function returning how many episodes of the $k$ total episodes should have been used to \emph{observe} and $\kappa$ is the number of episodes that have been used to \emph{observe}.  The choice of $\kappa^*$ is a hyperparameter. 
\thealgo then constructs the $\{\model R(s,a), \model P(s,a)\}$ MDP model and selects greedy actions with the respect to its optimal Q-function $\model Q_*$.  
The choice to hold the policy fixed throughout the course of an episode is a matter of simplicity, giving easier analysis that \emph{observe} episodes will observe environment rewards, as well as computational convenience.

\subsection{Theoretical Guarantees}

\thealgo has a polynomial sample complexity even in unsolvable Mon-MDPs. There exists parameters where the algorithm guarantees with high probability ($1-\delta$) of being arbitrarily close to minimax-optimal ($\varepsilon$) for all but a finite number of time steps, which is a polynomial of $\nicefrac{1}{\varepsilon}$, $\nicefrac{1}{\delta}$, and other quantities characterizing the Mon-MDP.  This gives the first sample complexity bound for Mon-MDPs.
%
\begin{theorem}
\label{thm:sample_cmplx}
For any $\varepsilon, \delta > 0$, and Mon-MDP $M$ where $\rho$ is the minimum non-zero probability of observing the environment reward in $M$ and $H$ is the maximum episode length, there exists constants $m_1$, $m_2$, and $m_3$ where
\begin{align*}
        m_1 & = \tilde{\bigO} \left(\frac{|\c{S}|}{\varepsilon^2(1 - \gamma)^4}\right) \\
        m_2 & = \tilde{\bigO}\left(\frac{\env{\rmax} (\env{\rmax} + \mon{\rmax})^2}{\rho \varepsilon^2(1 - \gamma)^4} \right) \\
        m_3 &= \Tilde{\bigO}\left(\frac{\abs{\c{S}}^2\abs{\c{A}}}{\varepsilon^3(1 - \gamma)^5}\right)
\end{align*}
%
such that \thealgo with parameters,
%
\begin{align*}
    \beta &= \gamma(\env{\rmax} + \mon{\rmax})\sqrt{2\ln{(12\abs{\c{S}}\abs{\c{A}}m_2/\delta)}} \big/ (1 - \gamma) \\
    \mon{\beta} &= \mon{\rmax}\sqrt{2\ln{(12\abs{\c{S}}\abs{\c{A}}m_2 / \delta)}} \\
    \env{\beta} &= \env{\rmax}\sqrt{2\ln{(12\abs{\c{S}}\abs{\c{A}}m_2/\delta)}} \\
    \beta^{\text{observe}} &= \left({1}\big/{(1 - \gamma)} \sqrt{\ln{\left( \nicefrac{8\abs{\c{S}}\abs{\c{A}}m_1}{\delta}\right)}\big/{2}}\right) \\    \beta^{\text{KL-UCB}} &= \ln(\nicefrac{8\abs{\c{S}}\abs{\c{A}}m_1}{\delta}) \\
    \kappa^*(k) &= m_3 \quad\quad \text{(constant function)}
\end{align*}
%
provides the following bounds for $M$.
%the following holds. 
Let $\pi_t$ denote \thealgo's policy and $s_t$ denote the state at time $t$.  With probability at least $1 - \delta$, $V^{M_{\downarrow}}_{\pi_t}(s_t) \geq V^{M_{\downarrow}}_{\pi^*} - \varepsilon$ for all but $\Tilde{\bigO}\left(\dfrac{\env{\rmax}(\env{\rmax} + \mon{\rmax})\abs{\c{S}}\abs{\c{A}}H}{\varepsilon^3 (1 - \gamma)^5\rho}\right)$ timesteps.
\end{theorem}
The reader may refer to \cref{appendix:proof_sample_cmplx} for the proof.  

An interesting addition to the bound over MBIE bounds for traditional MDPs is the dependence on $\rho$, which bounds how difficult it is to observe the observable environment rewards.  If a Mon-MDP does reveal all rewards (it is solvable) but only does so with infinitesimal probability, an algorithm must be suboptimal for many more time steps.

\subsection{Practical Implementation}
The theoretically justified parameters for \thealgo present a couple of challenges in practice.  First off, we rarely have a particular $\epsilon$ and $\delta$ in mind, preferring algorithms that produce ever-improving approximations with ever-improving probability.  Second, the bound, while polynomial in the relevant values, does not suggest practical parameters.  The most problematic in this regard is the constant $\kappa^*$, which places all \emph{observe} episodes at the start of training.  Third, solving an MDP model exactly and from scratch each episode to compute $\model Q_*$ is computationally wasteful.

In practice, we slowly increase the confidence levels used in the exploration bonuses over time.  We follow the pattern of~\citet{lattimore2020bandit}, with the confidence growing slightly faster than logarithmically.\footnote{
We replace $\beta$ with $\beta \sqrt{g(\ln N(s))}$, where $g(x)= 1 + x\ln^2(x)$ and $N(s)$ counts the number of visits to state $s$.  This choice of $g$ is required as rewards and states are unlikely to follow a Gaussian distribution, but being bounded allows us to assume they are sub-Gaussian. We similarly grow $\env\beta$, $\mon\beta$, and $\beta^{\text{observe}}$, and replace $\beta^{\text{KL-UCB}}$ with $\beta^{\text{KL-UCB}} g(\ln N(s))$.}
The scale parameters $\beta$, $\mon\beta$, $\env\beta$, $\beta^{\text{observe}}$, $\beta^{\text{KL-UCB}}$ were then tuned manually for each environment.  
For $\kappa^*$ we also grow it slowly over time allowing the agent to interleave \emph{observe} and \emph{optimize} episodes: $\kappa^*(k) = \log k$, where the log base was also tuned manually for each environment. 
Finally, rather than exactly solving the models every episode, we maintain two value functions: $\model Q_{\text{observe}}$ and $\model Q_{\text{optimize}}$, both initialized optimistically. we do 50 steps of synchronous value iteration before every episode to improve $\model Q_{\text{optimize}}$, and before \emph{observe} episodes to improve $\model Q_{\text{observe}}$. 
%
%
%
\begin{figure*}[t]
\begin{minipage}[b]{0.2\textwidth}
\centering
    \begin{subfigure}[b]{\columnwidth}
        \centering
        \includegraphics[width=\columnwidth]{imgs/envs/River_Swim_without_Button.pdf}
        \caption{\label{fig:main_river_swim}River Swim}
    \end{subfigure}
    \\[1em]
    \begin{subfigure}[b]{\columnwidth}
        \centering
        \includegraphics[width=\columnwidth]{imgs/envs/Bottleneck.pdf}
        \caption{\label{fig:main_Bottleneck}Bottleneck}
    \end{subfigure}
\caption{\textbf{Environments}}
\label{fig:three}
\end{minipage}
\hfill
\begin{minipage}[b]{0.767\textwidth}
    \centering
    \includegraphics[width=0.84\linewidth]{imgs/results/all_legend.pdf}
    \\[4pt]
    \raisebox{55pt}{\rotatebox[origin=t]{90}{\fontfamily{cmss}\scriptsize{Discounted Test Return}}}
    % \hfill
    \begin{subfigure}[b]{0.24\linewidth}
        \centering
        \includegraphics[width=\linewidth]{imgs/results/comp_neurips/Full_RiverSwim-6-v0.pdf}
        \\[-4pt]
        {\hspace*{1em}\fontfamily{cmss}\scriptsize{Training Steps ($\times 10^3$)}}
        \caption{\label{fig:river_return}River Swim}
    \end{subfigure} 
    \hfill
    \begin{subfigure}[b]{0.24\linewidth}
        \centering
        \includegraphics[width=\linewidth]{imgs/results/understanding/Button_Gridworld-Bottleneck_1.pdf}
        \\[-4pt]
        {\hspace*{1em}\fontfamily{cmss}\scriptsize{Training Steps ($\times 10^3$)}}
        \caption{\label{fig:bottleneck_100_return}Bottleneck (100\%)}
    \end{subfigure} 
    \hfill
        \begin{subfigure}[b]{0.24\textwidth}
        \centering
        \includegraphics[width=\linewidth]{imgs/results/understanding/Button_Gridworld-Bottleneck_0.05.pdf}
        \\[-4pt]
        {\hspace*{1em}\fontfamily{cmss}\scriptsize{Training Steps ($\times 10^3$)}}
        \caption{\label{fig:bottleneck_5_return}Bottleneck (5\%)}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.24\textwidth}
        \centering
        \includegraphics[width=\linewidth]{imgs/results/understanding/Button_Gridworld-Bottleneck_0.05_known.pdf}
        \\[-4pt]
        {\hspace*{1em}\fontfamily{cmss}\scriptsize{Training Steps ($\times 10^3$)}}
        \caption{\label{fig:bottleneck_5_return_zoom}Bottleneck (5\%)}
    \end{subfigure}

    \caption{\label{fig:four}\textbf{Discounted return at test time}, averaged over 30 seeds (shaded areas denote 95\% confidence intervals). 
    \thealgo (in green) outperforms Directed-E$^2$ (in orange) and always converges to the minimax-optimal policy (the dashed black line).
    (c) and (d) both show results in the Bottleneck with the 5\% Button Monitor, but with different axis ranges to highlight the improvement if \thealgo already knows details of the monitor (in purple).
    }
\end{minipage}
\end{figure*}

\section{Empirical Evaluation}

This paper began by detailing limitations in prior work not taking advantage of the Mon-MDP structure, the possibility of a known monitor, nor dealing with unsolvable Mon-MDPs with unobservable rewards. This section breaks down this claim into four research questions (RQ) to investigate if \thealgo can:
\begin{enumerate}[leftmargin=28pt, itemsep=-4pt, topsep=0pt]
    \item [RQ1)] Explore efficiently in hard-exploration tasks?
    \item [RQ2)] Act pessimistically when rewards are unobservable?
    \item [RQ3)] Identify and learn about difficult to observe rewards?
    \item [RQ4)] Take advantage of a known model of the monitor?
\end{enumerate}
To directly address these questions, we first show results on two tasks with two monitors. Then, we show results on 24 benchmarks to strengthen our claims.\footnote{Source code will be released upon publication.} 

\subsection{Environment and Monitor Description}
%
\emph{River Swim} (Figure~\ref{fig:main_river_swim}) is a well-known difficult exploration task with two actions. Moving \aleft always succeeds, but moving \aright may not --- the river current may cause the agent to stay at the same location or even be pushed to the left. There is a goal state on the far right, where $\env{R} = 1$. However, the leftmost tile yields $\env{R} = 0.1$ and it is much easier to reach. Other states have zero rewards. Agents often struggle to find the optimal policy (always move \aright), and instead converge to always move \aleft.
In our experiments, we pair River Swim with the \emph{Full Monitor} where environment rewards are always freely observable, allowing us to focus on an algorithm's exploration ability.

\emph{Bottleneck} (Figure~\ref{fig:main_Bottleneck}) has five deterministic actions: \aleft, \aup, \aright, \adown, \astay, which move the agent around the grid.  Episodes end when the agent executes \astay in either the gold bars state ($\env{R} = 0.1$) or in the treasure chest state ($\env{R} = 1$). Reaching the snake state yields $\env{R} = -10$, and other states yield $\env{R} = 0$. However, states denoted by $\bot$ have \textit{never-observable} rewards of -10, i.e., $\env{R} = -10$ but $\env{\widehat{R}} = \bot$ \textit{at all times}. 
In our experiments, we pair Bottleneck with the \emph{Button Monitor}, where the monitor state can be \son or \soff (initialized at random) and can be switched if the agent executes \adown in the button state. 
When the monitor is \son, the agent receives $\mon{R} = -0.2$ at every timestep, but also observes the current environment reward (unless the agent is in a $\bot$ state). 
The optimal policy follows the shortest path to the treasure chest, while avoiding the snake and $\bot$ states, and turning the monitor \soff if it was \son at the beginning of the episode. 
To evaluate how \thealgo performs when observability is stochastic, we consider two versions of the Button Monitor: one where the monitor works as intended and rewards are observable with 100\% probability if \son, and a second where the  rewards are observable only with 5\% probability if \son.


\begin{figure*}[t]
    \begin{minipage}[c]{0.41\textwidth}

    \raisebox{60pt}{\rotatebox[origin=t]{90}{\fontfamily{cmss}\scriptsize{Visitation Count}}}
    % \hfill
    \begin{subfigure}[b]{0.47\textwidth}
        \centering
        \includegraphics[width=\linewidth]{imgs/results/understanding/Goal_Visited_0.05.pdf}
        \\[-4pt]
        {\hspace*{1em}\fontfamily{cmss}\scriptsize{Training Steps ($\times 10^3$)}}
        \caption{\label{fig:goal_visits}Visits to the goal}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.47\textwidth}
        \centering
        \includegraphics[width=\linewidth]{imgs/results/understanding/Unobserved_Visited_0.05.pdf}
        \\[-4pt]
        {\hspace*{1em}\fontfamily{cmss}\scriptsize{Training Steps ($\times 10^3$)}}
        \caption{\label{fig:bot_visits}Visits to $\bot$}
    \end{subfigure}
    \end{minipage}
    \hfill
    \begin{minipage}[c]{0.55\textwidth}
    {\centering
    \includegraphics[width=0.6\linewidth]{imgs/results/two_legend.pdf}}
    \caption{\label{fig:visits}\textbf{Visits to important states at test time in the Bottleneck with 5\% Button Monitor}.  Results are averaged over 30 trials, and shaded areas denote 95\% confidence interval.  Directed-E$^2$ fails to focus on the goal and instead keeps visiting $\bot$ states, whereas \thealgo reduces its visitation frequency instead, ultimately visiting only the goal.
    }
    \end{minipage}
\end{figure*}
%
\subsection{Results}
We compare \thealgo against Directed Exploration-Exploitation (Directed-E$^2$)~\citep{parisi2024beyond}, which is, to the best of our knowledge, currently the most performant algorithm in Mon-MDPs.
In all benchmarks, the discount factor is $\gamma = 0.99$. The full set of hyperparameters are in \cref{appendix:hyperparameters} are full evaluation details (e.g., episodes lengths, evaluation frequencies) are in \cref{appendix:empirical _details}. Results shown in Figures \ref{fig:four} and \ref{fig:visits} are at test time, i.e., when the agent follows the current greedy policy without exploring.

To answer RQ1, consider the results in Figure~\ref{fig:river_return}. In this case, the performance of \thealgo significantly outperforms that of Directed-E$^2$. This task is difficult for any $\epsilon$-greedy exploration strategy (such as the one of Directed-E$^2$) and highlights the first innovation: taking a model-based approach in Mon-MDPs (i.e., extending MBIE-EB) allows \thealgo to have more efficient exploration.

To answer RQ2, consider Figure~\ref{fig:bottleneck_100_return}. In this case, states marked with $\bot$ are never observable by the agent, regardless of the monitor state. Because the minimum reward in this task is $\rmin = -10$, the minimax-optimal policy is to avoid states marked by $\bot$ while reaching the goal state ($\env{R} = 1$).  \thealgo is able to find this optimal policy, whereas Directed-E$^2$ does not because it does not learn to avoid unobservable rewards.\footnote{\label{fn:DE2-initialize}Directed-E$^2$ describes initializing its reward model randomly, relying on the Mon-MDP being solvable to not depend on the initialization.  For unsolvable Mon-MDPs, however,  this is not true and Directed-E$^2$ depends significantly on initialization.  In fact, while not noted by~\citet{parisi2024beyond}, pessimistic initialization with Directed-E$^2$ is sufficient to give an asymptotic convergence result for unsolvable Mon-MDPs.} This result highlights the impact of the second innovation: unsolvable Mon-MDPS require pessimism when the reward cannot be observed.

To answer RQ3, consider Figure~\ref{fig:bottleneck_5_return}, where the Button Monitor provides a reward only 5\% of the time when \son (and 0\% of the time when \soff). Despite how difficult it is to observe rewards, \thealgo is able to learn the minimax-optimal policy. This shows that \thealgo is still appropriately pessimistic, successfully avoiding $\bot$ states and the snake, and reaching the better goal state. Since rewards are only visible one out of twenty times (when the monitor is \son), learning is much slower than in Figure~\ref{fig:bottleneck_100_return}, matching the appearance of $\rho$ in Theorem~\ref{thm:sample_cmplx}'s bound. This result also shows the impact of the third innovation: it is important to explore just enough to guarantee that the agent will learn about observable rewards, but no more.
This result highlights the impact of the third innovation.

To answer RQ4, now consider the performance of Known Monitor in Figure~\ref{fig:bottleneck_5_return_zoom}, showing the performance of \thealgo when provided the model of the Button Monitor 5\%. Results show that its convergence speed increases significantly, as \thealgo takes (on average) 30\% fewer steps to find the optimal policy.
This feature of \thealgo is particularly important in settings where the agent has already learned about the monitor previously or the practitioner can provide the agent with an accurate model of the monitor.  The agent, then, needs only to learn about the environment, and does not need to explore the monitor component of the Mon-MDP. 
%
%
\begin{figure}[!htb]
\setlength{\tabcolsep}{0pt}
\centering
\raisebox{5pt}{\rotatebox[origin=t]{90}{\fontfamily{cmss}\scriptsize{Discounted Test Return}}}
\begin{tabular}{p{0.32\hsize}p{0.32\hsize}p{0.32\hsize}}
% \includegraphics[width=0.9\linewidth, trim={2cm 0 0 0}, clip]{imgs/results/comp_neurips/Full_Gridworld-Empty.pdf} &
\includegraphics[width=\linewidth]{imgs/results/thumbnail/Full_Gridworld-Empty.pdf} &
\includegraphics[width=\linewidth]{imgs/results/thumbnail/RandomNonZero_Gridworld-Empty.pdf} &
\includegraphics[width=\linewidth]{imgs/results/thumbnail/Ask_Gridworld-Empty.pdf} \\
\includegraphics[width=\linewidth]{imgs/results/thumbnail/Button_Gridworld-Empty.pdf} &
\includegraphics[width=\linewidth]{imgs/results/thumbnail/N_Gridworld-Empty.pdf} &
\includegraphics[width=\linewidth]{imgs/results/thumbnail/Level_Gridworld-Empty.pdf} \\
\includegraphics[width=\linewidth]{imgs/results/thumbnail/Full_Gridworld-Hazard.pdf} &
\includegraphics[width=\linewidth]{imgs/results/thumbnail/RandomNonZero_Gridworld-Hazard.pdf} &
\includegraphics[width=\linewidth]{imgs/results/thumbnail/Ask_Gridworld-Hazard.pdf} \\
\includegraphics[width=\linewidth]{imgs/results/thumbnail/Button_Gridworld-Hazard.pdf}  & 
\includegraphics[width=\linewidth]{imgs/results/thumbnail/N_Gridworld-Hazard.pdf} & 
\includegraphics[width=\linewidth]{imgs/results/thumbnail/Level_Gridworld-Hazard.pdf} \\
\includegraphics[width=\linewidth]{imgs/results/thumbnail/Full_Gridworld-OneWay.pdf} & 
\includegraphics[width=\linewidth]{imgs/results/thumbnail/RandomNonZero_Gridworld-OneWay.pdf} & 
\includegraphics[width=\linewidth]{imgs/results/thumbnail/Ask_Gridworld-OneWay.pdf} \\
\includegraphics[width=\linewidth]{imgs/results/thumbnail/Button_Gridworld-OneWay.pdf} &
\includegraphics[width=\linewidth]{imgs/results/thumbnail/N_Gridworld-OneWay.pdf} & 
\includegraphics[width=\linewidth]{imgs/results/thumbnail/Level_Gridworld-OneWay.pdf} \\
\includegraphics[width=\linewidth]{imgs/results/thumbnail/Full_RiverSwim-6-v0.pdf} &
\includegraphics[width=\linewidth]{imgs/results/thumbnail/RandomNonZero_RiverSwim-6-v0.pdf} & 
\includegraphics[width=\linewidth]{imgs/results/thumbnail/Ask_RiverSwim-6-v0.pdf} \\ 
\includegraphics[width=\linewidth]{imgs/results/thumbnail/Button_RiverSwim-6-v0.pdf}  & 
\includegraphics[width=\linewidth]{imgs/results/thumbnail/N_RiverSwim-6-v0.pdf} & 
\includegraphics[width=\linewidth]{imgs/results/thumbnail/Level_RiverSwim-6-v0.pdf}
\end{tabular}
\\[-4pt]
{\hspace*{1em}\fontfamily{cmss}\scriptsize{Training Steps}}
\caption{\textbf{Performance on 24 benchmark environments} from \citet{parisi2024beyond}. \thealgo clearly outperforms Directed-E$^2$ in 22 of them and performs on par in the remainder two. 
Full-size graphs with details of all 24 benchmarks are in \cref{appendix:compared2dee}.}
\vspace{-5pt}
\label{fig:24-benchmarks}
\end{figure}

To better understand the above results, Figure~\ref{fig:visits} shows how many times the agent visits the goal state and $\bot$ states per testing episode. 
Both algorithms initially visit the goal state (Figure~\ref{fig:goal_visits}) during random exploration (i.e., when executing the policy after 0 timesteps of training). \thealgo appropriately explores for some training episodes (recall that rewards are only observed in \son and even then only 5\% of the time), and then learns to always go to the goal. Both also initially visit $\bot$ states (Figure~\ref{fig:bot_visits}). However, while \thealgo learns to be appropriately pessimistic over time and avoids them, Directed-E$^2$ never updates its (random) initial estimate of the value of $\bot$ states and incorrectly believes they should continue to be visited. This also explains why Directed-E$^2$ performs even worse in Figure~\ref{fig:bottleneck_5_return}.
% throughout the course of an episode.

Finally, Figure~\ref{fig:24-benchmarks} presents results comparing \thealgo across all of the domains and monitor benchmarks from~\citet{parisi2024beyond}.  In these 24 benchmarks, \thealgo significantly outperforms Directed-E$^2$ in all but 2 of them, where they perform similarly.
%
\section{Discussion}
There are a number of limitations to our approach that suggest directions for future improvements. 
First, Mon-MDPs contain an exploration-exploitation dilemma, but with an added twist --- the agent needs to treat never observed rewards pessimistically in order to achieve a minimax-optimal solution; however, it should continue exploring those states to get more confident about their unobservability.  Much like early algorithms for the exploration-exploitation dilemma in MDPs~\citep{kearns2002near}, our approach separately optimizes a model for exploring and one for seeking a minimax-optimal solution.  A more elegant approach would be to simultaneously optimize for both.  Second, our approach uses explicit counts to drive its exploration, which limits it to enumerable Mon-MDPs.  Adapting psuedocount-based methods~\citep{bellemare2016unifying, martin2017count, tang2017exploration, machado2020count} can help making \thealgo more applicable to large or continuous state spaces.  Finally, the decision of when to stop trying to observe rewards and instead optimize is essentially an optimal stopping time problem~\citep{lattimore2020bandit}, and there may be considerable innovations that could improve the bounds along with empirical performance.
%
\section{Conclusion}
We introduced a new algorithm, \thealgo, for Mon-MDPs that addresses many of the shortcomings of previous algorithms.  It gives the first finite-time sample complexity bounds for Mon-MDPs, while being applicable to both solvable and unsolvable Mon-MDPs, for which it is also the first.  Furthermore, it both exploits the structure of the Mon-MDP and can take advantage of knowledge of the monitor process, if available.  These features are not just theoretical, as we see these innovations resulting in empirical improvements in Mon-MDP benchmarks, comprehensively outperforming the previous best learning algorithms.
%
\clearpage
\section*{Acknowledgments}
The authors are grateful to the anonymous reviewers that provided valuable feedback on the paper. Part of this work has taken place in the Intelligent Robot Learning (IRL) Lab at the University of Alberta, which is supported in part by research grants from Alberta Innovates; Alberta Machine Intelligence Institute (Amii); a Canada CIFAR AI Chair, Amii; Digital Research Alliance of Canada; Mitacs; and the National Science and Engineering Research Council (NSERC).
%
\section*{Impact Statement}
This paper presents work whose goal is to advance the fundamental understanding of reinforcement learning. 
Our work is mostly theoretical and experiments are conducted on simple environments that do not involve human participants or concerning datasets, and it is not tied to specific real-world applications.  We believe that our contribution has little to no potential for harmful impact. 
%
\balance 
\bibliographystyle{abbrvnat}
\bibliography{arxiv_paper}
%
\clearpage
\appendix
\input{utils/appendix}
\end{document}



