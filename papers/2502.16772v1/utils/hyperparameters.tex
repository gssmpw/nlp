   \section{Hyperparameters} 
   \label{appendix:hyperparameters}
   Let $\c{U}$ denote the uniform distribution and $x \mapsto y$ denote the linear annealing of a quantity taking initially the value of $x$ and ends with $y$.
    %
    %
    %
    \begin{table}[tbh]
    \caption{Hyperparameters of \thealgo across experiments.}
    \scriptsize
    \hskip0.25in
    \begin{tabular}{ |m{6em} m{6em} c c c c c c c c| }     

    %TODO(MHB): The column names should be lining up, but they're in different orders.
    \hline
    \multicolumn{10}{|c|}{Unknown monitor} \\
    \hline
    Experiment & Environment & $\model Q_{\text{optimize-init}}$ & $\model Q_{\text{observe-init}}$ & $\kappa^{*}(k)$ & $\beta^{\text{obs}}$ & $\beta^{\text{KL-UCB}}$ & $\beta$ & $\mon \beta$ & $\env \beta$\\
    \hline
    \multirow{4}{6em}{\cref{appendix:compared2dee}} & \textbf{Empty} & 1 & 100 & $\log_{1.005}k$ & $5 \times 10^{-4}$ & $5 \times 10^{-2}$ & $5 \times 10^{-4}$ & $5 \times 10^{-4}$ & $5 \times 10^{-4}$\\ 
    & \textbf{Hazard} & 1 & 100 & $\log_{1.005}k$ & $5 \times 10^{-4}$ & $5 \times 10^{-2}$ & $5 \times 10^{-4}$ & $5 \times 10^{-4}$ & $5 \times 10^{-4}$\\ 
    & \textbf{One-Way} & 1 & 100 & $\log_{1.005}k$ & $5 \times 10^{-4}$ & $5 \times 10^{-2}$ & $5 \times 10^{-4}$ & $5 \times 10^{-4}$ & $5 \times 10^{-4}$\\
    & \textbf{River-Swim} & 30 & 100 & $\log_{1.005}k$ & $5 \times 10^{-4}$ & $5 \times 10^{-2}$ & $5 \times 10^{-4}$ & $5 \times 10^{-4}$ & $5 \times 10^{-4}$\\
    \hline
    \multirow{1}{6em}{\cref{appendix:never_obsrv}} & \textbf{Bottleneck} & 1 & 100 & $\log_{1.005}k$ & $5 \times 10^{-4}$ & $5 \times 10^{-2}$ & $5 \times 10^{-4}$ & $5 \times 10^{-4}$ & $5 \times 10^{-4}$\\ 
    \hline
    \multirow{1}{6em}{\cref{appendix:stochas_obsrv}} & \textbf{Bottleneck} & 1 & 100 & $\log_{1.005}k$ & $5 \times 10^{-4}$ & $5 \times 10^{-2}$ & $5 \times 10^{-4}$ & $5 \times 10^{-4}$ & $5 \times 10^{-4}$\\ 
    \hline
    \multirow{1}{6em}{\cref{appendix:ablation}} & \textbf{Bottleneck} & 50 & 100 & $\log_{1.005}k$ & $5 \times 10^{-4}$ & $5 \times 10^{-2}$ & $5 \times 10^{-4}$ & $5 \times 10^{-4}$ & $5 \times 10^{-4}$\\ 
    \hline
    \end{tabular}
    %
    \\[3 pt]
    %
    \strut\hskip0.25in
    \begin{tabular}{ |m{6em} m{6em} c c c c c| }     
    \hline
    \multicolumn{7}{|c|}{Known monitor} \\
    \hline
    Experiment & Environment  & $\model Q_{\text{optimize-init}}$ & $\model Q_{\text{observe-init}}$ & $\kappa^*(k)$ & $\env\beta$ & $\beta$\\
    \hline
    \multirow{1}{6em}{\cref{appendix:known_monitor}} & \textbf{Bottleneck} & 1 & 100 & $\log_{1.005}k$ & $5 \times 10^{-4}$ & $5 \times 10^{-4}$\\ 
    \hline
    \end{tabular}
\end{table}
%
%
%
 \begin{table}[tbh]
    \caption{Hyperparameters of Directed-E$^2$ across experiments.}
    \scriptsize\centering
    \begin{tabular}{ |m{6em} m{6em} c c c c c| }     
    \hline
    \multicolumn{7}{|c|}{(Annealing for $N$-Supporters and $N$-Experts)} \\
    \hline
    Experiment & Environment & $Q_{\text{int}}$ & $S_0$ & $r_0$ & $\bar{\beta}$ & $\upalpha$  \\
    \hline
    \multirow{4}{6em}{\cref{appendix:compared2dee}} & \textbf{Empty} & -10 & 0 & $\c{U}[-0.1, 0.1]$ & $10^{-2}$ & $1 (1 \mapsto 0.1)$  \\
    & \textbf{One-Way} & -10 & 0 & $\c{U}[-0.1, 0.1]$ & $10^{-2}$ & $1 (1 \mapsto 0.1)$  \\
    & \textbf{Hazard} & -10 & 0 & $\c{U}[-0.1, 0.1]$ & $10^{-2}$ & $0.5 (0.5 \mapsto 0.1)$  \\ 
    & \textbf{River-Swim} & -10 & 0 & $\c{U}[-0.1, 0.1]$ & $10^{-2}$ & $0.5 \mapsto 0.05$  \\
    \hline
    \multirow{1}{6em}{\cref{appendix:stochas_obsrv}} & \textbf{Bottleneck} & -10 & 0 & $-10$ & $10^{-2}$ & $1 (1 \mapsto 0.1)$  \\
    \hline
    \multirow{1}{6em}{\cref{appendix:stochas_obsrv}} & \textbf{Bottleneck} & -10 & 0 & $-10$ & $10^{-2}$ & $1 (1 \mapsto 0.1)$  \\
    \hline
    \end{tabular}
\end{table}
%
%
%
\subsection{Considerations}
%
\begin{itemize}
    \item Directed-E$^2$ uses two action-values, one as the ordinary action-values that uses a reward model in place of the environment reward to update action-values (this sets the agent free from the partial observability of the environment reward once the reward model is learned). Second action value function denoted by $S$ is dubbed as visitation-values that tries to maximized the successor representations. Directed-E$^2$ uses visitation-values to visit every joint state-action pairs but in the limit of infinite exploration it becomes greedy with respect to the main action-values for maximizing the expected sum of discounted rewards.
    \item Hyperparameters of Directed-E$^2$ consist of: $Q_{\text{int}}$ the initial action-values, $S_0$ the initial visitation-values, $r_0$ the initial values of the environment reward model, $\bar{\beta}$ goal-conditioned threshold specifying when a joint state-action pair should be visited through the use of visitation-values, $\upalpha$ the learning rate to update each $Q$ or $S$ incrementally and discount factor $\gamma$ that is held fixed $0.99$.  These values are directly reported from \citet{parisi2024beyond}.
    \item \thealgo's hyperparameters are set per environment and do not change across monitors. The same applies to Directed-E$^2$, but \citet{parisi2024beyond} recommend to tune an ad-hoc learning rate once N-Supporters or N-Experts are used as monitors.
    \item In experiments of\cref{appendix:stochas_obsrv}, observability is stochastic thus we make $\kappa^*(k)$ grow slowly. This makes the agent spend time more frequently at the observe episodes to deal with low probability of 5\% for observing the reward more aggressively. Even if in higher probabilities of 20\% and 80\% faster rates for $\kappa^{*}(k)$ are also applicable we kept $\kappa^{*}(k)$ universal for those experiments.
    \item KL-UCB does not have a closed form solution, we compute it using the Newton's method. The stopping condition for the Newton's method is chosen 50 iterations or the accuracy of at least $10^{-5}$ between successive iterative solutions, which one happens first.
    \item Due to knowledge of the monitor, agent should would not need to spend a lot of time in the observe episodes, although probability of observing the reward could go as low as $5\%$, we set the $\kappa^*(k)$ with faster rate than when the monitor is unknown. Due to this agent wastes less of its efforts in the observe episodes.
\end{itemize}
%
\section{Compute}
We ran all experiments on a SLURM-based cluster, using 32 Intel E5-2683 v4 Broadwell @ 2.1GHz CPUs. 30 runs took about an hour on a 32 core CPU. Runs were parallelized whenever possible across cores.
