\section{Proof of \cref{thm:sample_cmplx}}
\label{appendix:proof_sample_cmplx}
We present the proof in 5 stages, each holds with high probability.
\begin{enumerate}
    \item Specifying the number of samples required to estimate the observability of the environment reward in observe episodes
    \item Optimism of $\model{Q}_{*, \text{observe}}$
    \item Specifying the number of samples required to find a near minimax optimal policy in optimize episodes
    \item Optimism of $\model{Q}_{*, \text{optimize}}$
    \item Determining the sample complexity
\end{enumerate}
%
For the first 2 stage of the proof let $b(s, a) = \EV{\mathds{1}\scriptstyle{\{\env{\widehat{R}}(s, a) \neq 0\}}}{}$ denote the expected observability of the environment reward for the joint state-action $(s, a)$. And define $\estimate{B}$ to be the maximum likelihood estimation of $b$.
%
\paragraph{1. Specifying the number of samples required to estimate the observability of reward in observe episodes.}

According to Lemma 2 of \citet{strehl2008analysis}, to find a policy in observation episodes that its action-values, computed using $\estimate{B}$ and $\estimate{P}$, are $\varepsilon_1$-close to their true values, $\estimate{B}$ and $\estimate{P}$ should be $\tau$-close to their true mean for all states $s$ and actions $a$ where $\tau=\varepsilon_1(1 - \gamma)^2 \big/ 2$. So in order to specify the least number of visits $m_1$ to $(s, a)$ to fulfill the $\tau$-closeness:
%
\begin{align}
    \begin{split}
    \label{eq:tau_p}
    \oneNorm{\estimate{P}(\cdot \mid s, a) - P(\cdot \mid s, a)} 
    & \leq \tau 
    \end{split}
    \\
    \begin{split}
    \label{eq:tau_b}
    \abs{\estimate{B}(s, a) - b(s, a)} & \leq \tau
    \end{split}
\end{align}
%
On the other hand we know if $(s, a)$ has been visited $m_1$ times, with probabilities at least $1  - \delta_P$, and $1 - \delta_B$:
%
\begin{align*}
    \oneNorm{\estimate{P}(\cdot \mid s, a) - P(\cdot \mid s, a)} 
    & \leq \sqrt{\frac{2\left[\ln{(2^{|\c{S}|} - 2) - \ln{\delta_P}}\right]}{m_1}} \\
    %
    \abs{\estimate{B}(s, a) - b(s, a)} & \leq \sqrt{\frac{\ln{\nicefrac 2 {\delta_{B}}}}{2m_1}}
\end{align*}
%
So for \cref{eq:tau_p} and \cref{eq:tau_b} to hold simultaneously with probability at $1 - \delta_1$ until $(s, a)$ is visited $m_1$ times, by setting $\delta_p = \delta_B = \delta_1 \big/ 2\abs{\c{S}}\abs{\c{A}}m_1$ to split the failure probability equally for all state-action pairs until each of them have been visited $m_1$ times, it is enough ensure $\tau$ is bigger than the length of the confidence intervals, so:
%
\begin{align*}
        m_1 & \geq \max \left\{\frac{8\left[\ln{(2^{\abs{\c{S}}} - 2) - \ln{\delta_P}}\right]}{\tau^2}, \frac{2\ln{\nicefrac 2 {\delta_{B}}}}{\tau^2} \right\}
        \\
        & \geq \max \left\{\frac{8\left[\ln{(2^{\abs{\c{S}}} - 2) - \ln{\nicefrac{\delta_1}{2\abs{\c{S}}\abs{\c{A}}m_1}}}\right]}{\tau^2}, \frac{2\ln{\nicefrac {4\abs{\c{S}}\abs{\c{A}}m_1}{\delta_1}}}{\tau^2} \right\} 
        \\
        & = \frac{8\left[\ln{(2^{|\c{S}|} - 2) + \ln{\nicefrac{2\abs{\c{S}}\abs{\c{A}}m_1}{\delta_1}}}\right]}{\tau^2}
    \end{align*}
\cref{lemma:x-lnx} helps us to bring $m_1$ out of the right-hand side of the above inequality. Hence:
%
\begin{equation*}
    m_1 = \bigO \left(\frac{\abs{\c{S}}}{\tau^2} + \frac{1}{\tau^2}\ln{\frac{\abs{\c{S}}\abs{\c{A}}}{\tau\delta_1}} \right) = \bigO \left(\frac{|\c{S}|}{\varepsilon_1^2(1 - \gamma)^4} + \frac{1}{\varepsilon_1^2(1 - \gamma)^4} \ln{\frac{\abs{\c{S}}\abs{\c{A}}}{\varepsilon_1(1 - \gamma)^2\delta_1}}\right)
\end{equation*}
The bound shows the fact that regardless of how infinitesimal the probability of observing the reward is, the difficulty of learning good estimates lies in learning the transition dynamics~\citep{kakade2003sample, szitamormax}; by the time that the transition dynamics are approximately learned, the agent has approximately figured out the what the probability of observing the environment reward for taking any joint action is.
%
\paragraph{2. Optimism of $\model{Q}_{*, \text{observe}}$.}

Optimism is needed to make sure the agent has enough incentives to visit state-action pairs that their statistics are not close enough to their true values. Suppose $m_1$ is least number of samples required to ensure $\estimate{B}$ and $\estimate{P}$ are close to their true mean. Then the agent should be optimistic about the action-values of state-action pairs that have not been visited $m_1$ times so far.

Let:
%
\begin{equation*}
    Q_{*, \text{observe}}(s, a) = b(s, a)\mathds{1}\{N(\env{s}, \env{a}) = 0\} + \gamma \sum_{s'}P(s' \mid s, a)\max_{a'}Q_{\text{*, \text{observe}}}(s', a')
\end{equation*}
%
Now consider the first $v$ visits to $(s, a)$ during observation episodes, where $v < m_1$ and the corresponding action values:
%
\begin{equation}
    \label{eq:q_observe_base}
    \model{Q}_{*, \text{observe}}(s, a) = \estimate{B}(s, a)\mathds{1}\{N(\env{s}, \env{a}) = 0\} + \gamma \sum_{s'}\estimate{P}(s' \mid s, a)\max_{a'}\model{Q}_{*, \text{observe}}(s', a') + \frac{\beta^{\text{obs}}}{\sqrt{v}}
\end{equation}
%
$\estimate{B}(s, a)\mathds{1}\{N(\env{s}, \env{a}) = 0\} = 0$ for all state-action pairs. Because if $\mathds{1}\{N(\env{s}, \env{a}) = 0\}$ returns 1, then it means environment reward of $(s, a)$ has not been observed so far and $\estimate{B}(s, a) = 0$. So \cref{eq:q_observe_base} turns into:
%
\begin{equation}
\model{Q}_{*, \text{observe}}(s, a) = \gamma \sum_{s'}\estimate{P}(s' \mid s, a)\max_{a'}\model{Q}_{*, \text{observe}}(s', a') + \frac{\beta^{\text{obs}}}{\sqrt{v}}\end{equation}
%
According to Lemma 7 of \citet{strehl2008analysis}, by choosing $\beta^{\text{obs}} = \left({1}\big/{(1 - \gamma)} \sqrt{\ln{\left( \nicefrac{4\abs{\c{S}}\abs{\c{A}}m_1}{\delta_1}\right)}\big/{2}}\right)$:
%
\begin{align*}
     \model{Q}_{*, \text{observe}}(s, a) = \gamma \sum_{s'}\estimate{P}(s' \mid s, a)\max_{a'}\model{Q}_{*, \text{observe}}(s', a') + \frac{\beta^{\text{obs}}}{\sqrt{v}} \geq Q_{*, \text{observe}}(s, a)
\end{align*}
% 
with probability at least $1 - {\delta_1} \big/ {4\abs{\c{S}}\abs{\c{A}}m_1}$. On the other hand by choosing $\beta^{\text{KL-UCB}} = \ln(4\abs{\c{S}}\abs{\c{A}}m_1 \big/ \delta_1)$:
%
\begin{equation*}
    \text{KL-UCB}(v) =  \max \{\mu \in [0, 1]: d(0, \mu) \leq \frac{\beta^{\text{KL-UCB}}}{v}\} \geq b(s, a)\mathds{1}\{N(\env{s}, \env{a}) = 0\}
\end{equation*}
%
holds with probability at least $1 - {\delta_1} \big/ {4\abs{\c{S}}\abs{\c{A}}m_1}$, where $d$ is the relative entropy distance appropriate for building the confidence interval of Bernoulli random variables~\citep{lattimore2020bandit, garivier2011kl, maillard2011finite}.

Now consider the following random variables:
%
\begin{align*}
    X =  Q_{*, \text{observe}}(s, a) - \gamma \sum_{s'}\estimate{P}(s' \mid s, a)\max_{a'}\model{Q}_{*, \text{observe}}(s', a') , && Y = \estimate{B}(s, a)\mathds{1}\{N(\env{s}, \env{a}) = 0\} = 0
\end{align*}
%
We have that:
%
\begin{align*}
    \prob{X \geq \underbrace{\frac{\beta^{\text{obs}}}{\sqrt{v}}}_{B_1}} \leq \frac{\delta_1}{4\abs{\c{S}}\abs{\c{A}}m_1}, && \prob{Y \geq \underbrace{\max \left\{\mu \in [0, 1]: d(0, \mu) \leq \frac{\beta^{\text{KL-UCB}}}{v}\right\}}_{B_2}} \leq \frac{\delta_1}{4\abs{\c{S}}\abs{\c{A}}m_1}
\end{align*}
%
Using Corollary~\ref{corollary:union_bound} we have:
%
\begin{equation*}
    \prob{X + Y \geq B_1 + B_2} \leq \frac{\delta_1}{2\abs{\c{S}}\abs{\c{A}}m_1}
\end{equation*}
%
Thus with probability at least $1 - {\delta_1} \big/ {2\abs{\c{S}}\abs{\c{A}}m_1}$ we must have that $X + Y \leq B_1 + B_2$. By replacing the explicit values of $X$ and $Y$, we have:
%
\begin{align*}
    & Q_{*, \text{observe}}(s, a) - \gamma \sum_{s'}\estimate{P}(s' \mid s, a)\max_{a'}\model{Q}_{*, \text{observe}}(s', a')  \leq B_1 + B_2 \\
    & Q_{*, \text{observe}}(s, a) \leq \gamma \sum_{s'}\estimate{P}(s' \mid s, a)\max_{a'}\model{Q}_{*, \text{observe}}(s', a')  + B_1 + B_2 \\
    & Q_{*, \text{observe}}(s, a) \leq \model{Q}_{*, \text{observe}}(s, a)  + \max \left\{\mu \in [0, 1]: d(0, \mu) \leq \frac{\beta^{\text{KL-UCB}}}{v}\right\}
\end{align*}
%
And by abusing the notation for $\model{Q}_{*, \text{observe}}$ to incorporate the \text{KL-UCB} term, we have:
%
\begin{equation*}
    Q_{*, \text{observe}}(s, a) \leq \model{Q}_{*, \text{observe}}(s, a)
\end{equation*}
%
By using the union bound over $\c{S}, \c{A}$ and $m_1$ the above inequality holds for all state-action pairs until they are visited $m_1$ times with probability at least $1 - \frac{\delta_1}{2}$.
%
\paragraph{3. Specifying the number of samples required to find a near minimax optimal policy in optimize episodes.}

During optimize episodes agent can face two kinds of joint state-action pairs: 1) state-action pairs that lead to observing the environment reward e.g., moving and asking for reward 2) state -action pairs that do not lead to observing the environment reward e.g., moving and not asking for reward. Let us denote these sets as the \textbf{observable} and the \textbf{unobservable} respectively.

\subsubsection*{Number of samples for the observable set}
In contrast to what was explained for the stage 1 of the proof, which was an off-the-shelf application of the analysis done by \citet{strehl2008analysis}, since in optimize episodes we have three unknown quantities $\env{r}, \mon{r}$, and $P$ --that are mappings from different input spaces-- we need Lemmas~\ref{lemma:general_closeness} and~\ref{lemma:tau_closeness} that are straight adaptations of Lemmas 1 and 2 in \citet{strehl2008analysis}.

Using Lemma~\ref{lemma:tau_closeness} if we want to find an $\varepsilon_2-$minimax-optimal policy for the state-action pairs that are in the observable set, by choosing $\tau = \frac{\varepsilon_2(1 - \gamma)^2}{6(\env{\rmax} + \mon{\rmax})}$ we must have:
\begin{equation*}
    \abs{\env{\estimate{R}}(\env{s}, \env{a}) - \env{r}(\env{s}, \env{a})} \leq \tau
\end{equation*}
\begin{equation*}
    \abs{\mon{\estimate{R}}(\mon{s}, \mon{a}) - \mon{r}(\mon{s}, \mon{a})} \leq \tau
\end{equation*}
\begin{equation*}
    \oneNorm{\estimate{P}(\cdot \mid s, a) - P(\cdot \mid s, a)} \leq \tau
\end{equation*}

On the other hand, we know that if $(s, a) \equiv (\env{s}, \mon{s}, \env{a}, \mon{a})$ has been visited $v_1$ times, its monitor reward has been observed $v_2$ times, and its environment reward has been observed $v_3$ times, with probabilities at least $1 - \delta_{\env{R}}, 1 - \delta_{\mon{R}}$, and $1 - \delta_P$:
%
\begin{equation}
\label{eq:exploit_tau_p}
    \oneNorm{\estimate{P}(\cdot \mid s, a) - P(\cdot \mid s, a)} 
    \leq \sqrt{\frac{2\left[\ln{(2^{|\c{S}|} - 2) - \ln{\delta_P}}\right]}{v_1}}
\end{equation}
%
\begin{equation}
\label{eq:exploit_tau_rmon}
    \abs{\mon{\estimate{R}}(\mon{s}, \mon{a}) - \mon{r}(\mon{s}, \mon{a})} \leq \sqrt{\frac{2\mon{\rmax}\ln(2/\delta_{\mon{R}})}{v_2}}
\end{equation}
%
\begin{equation}
\label{eq:exploit_tau_renv}
    \abs{\env{\estimate{R}}(\env{s}, \env{a}) - \env{r}(\env{s}, \env{a})} \leq \sqrt{\frac{2\env{\rmax}\ln(2/\delta_{\env{R}})}{v_3}}
\end{equation}
%
So in order to find $m_2$, the least number of visits to $(s, a)$, we make connections between $m_2, v_1, v_2$, and $v_3$. If a joint state-action pair is visited $m_2$ times, then we have:
%
\begin{align*}
    m_2 & = v_1 \\
    m_2 & \leq v_2 \leq \sum_{\env{s} \in \env{\c{S}}}\sum_{\env{a} \in \env{\c{A}}} m_2 = |\env{\c{S}}||\env{\c{A}}|m_2 \\
    m_2\cdot\rho &\leq v_3
\end{align*}
%
where the last inequality follows from the fact that the environment reward is observed with probability  $\rho$ upon occurrence of the joint state-action pair. 

So if we want \cref{eq:exploit_tau_renv}, \cref{eq:exploit_tau_rmon} and \cref{eq:exploit_tau_p} to hold simultaneously with probability at $1 - \delta_2$ until $(s, a)$ is visited $m_2$ times, by setting $\delta_p = \delta_{\mon{R}} = \delta_2 \big/ 3\abs{\c{S}}\abs{\c{A}}m_2$ and $\delta_{\env{R}} = \delta_2 \big/ 3\abs{\c{S}}\abs{\c{A}}\rho m_2$ to split the failure probability equally for rewards and transitions of all state-action pairs until each of them have been visited $m_2$, it is enough ensure $\tau$ is bigger than the length of the confidence intervals:
\begin{align*}
     m_2 & \geq \max \left\{\frac{8\left[\ln{(2^{|\c{S}|} - 2)} - \ln{\delta_P}\right]}{\tau^2}, \frac{8\mon{\rmax}\ln{(2/\delta_{\mon{R}})}}{\tau^2},
    \frac{8\env{\rmax}\ln{(2/\delta_{\env{R}})}}{\tau^2} \right\} \\
    %
    & \geq \max \left\{\frac{8\left[\ln{(2^{|\c{S}|} - 2)} - \ln{\delta_P}\right]}{\tau^2},
    \frac{8\env{\rmax}\ln{(2/\delta_{\env{R}})}}{\rho \tau^2} \right\} \\
    %
    & \geq \max \left\{\frac{8\left[\ln{(2^{|\c{S}|} - 2)} + \ln{\frac{3\abs{\c{S}}\abs{\c{A}}m_2}{\delta_2}}\right]}{\tau^2},
    \frac{8\env{\rmax}\ln{\frac{6\abs{\c{S}}\abs{\c{A}}\rho m_2}{\delta_2}}}{\rho \tau^2} \right\}
\end{align*}
%
If $\frac{1}{\rho} \geq \bigO(\abs{\c{S}})$, then:
%
\begin{equation*}
    m_2 \geq \frac{8\env{\rmax}\ln{\frac{6\abs{\c{S}}\abs{\c{A}}\rho m_2}{\delta_2}}}{\rho \tau^2}
\end{equation*}
%
which by Lemma~\ref{lemma:x-lnx}:
\begin{equation}
    m_2 = \bigO\left(\frac{\env{\rmax}}{\rho \tau^2} \ln{\frac{\env{\rmax}\abs{\c{S}}\abs{\c{A}}}{\tau\delta_2}}\right) = \bigO\left(\frac{\env{\rmax} (\env{\rmax} + \mon{\rmax})^2}{\rho \varepsilon_2^2(1 - \gamma)^4} \ln{\frac{\env{\rmax}(\env{\rmax} + \mon{\rmax})\abs{\c{S}}\abs{\c{A}}}{\varepsilon_2(1 - \gamma)^2\delta_2}}\right)
\end{equation}
%
If $\frac{1}{\rho} \leq \bigO(\abs{\c{S}})$:
\begin{equation*}
    m_2 \geq \frac{8\left[\ln{(2^{|\c{S}|} - 2)} + \ln{\frac{3\abs{\c{S}}\abs{\c{A}}m_2}{\delta_2}}\right]}{\tau^2}
\end{equation*}
which by Lemma~\ref{lemma:x-lnx} implies:
\begin{equation}
    m_2 = \bigO\left(\frac{\abs{\c{S}}}{\tau^2} + \frac{1}{\tau^2}\ln{\frac{\abs{\c{S}}\abs{\c{A}}}{\tau\delta_2}}\right) = \bigO\left(\frac{(\env{\rmax} + \mon{\rmax})^2\abs{\c{S}}}{\varepsilon_2^2(1 - \gamma)^4} + \frac{(\env{\rmax} + \mon{\rmax})^2}{\varepsilon_2^2(1 - \gamma)^4}\ln{\frac{(\env{\rmax} + \mon{\rmax})\abs{\c{S}}\abs{\c{A}}}{\varepsilon_2(1 - \gamma)^2\delta_2}}\right)
\end{equation}
%
\subsubsection*{Number of samples for the unobservable set}
These state-action pairs cannot change the sample estimate of the environment reward and the only quantities updated upon visitation are the transition dynamics and the monitor reward. It is enough to have:
%
\begin{equation*}
    m_2 \geq \max \left\{\frac{8\left[\ln{(2^{|\c{S}|} - 2)} - \ln{\delta_P}\right]}{\tau^2}, \frac{8\mon{\rmax}\ln{(2/\delta_{\mon{R}})}}{\tau^2}\right\}
\end{equation*}
%
Hence similar to the above case when $\frac{1}{\rho} \leq \bigO(\abs{\c{S}})$, the dominant factor around learning the sample estimates would be the transitions and the required sample size would be :
\begin{equation}
    m_2 = \bigO\left(\frac{(\env{\rmax} + \mon{\rmax})^2\abs{\c{S}}}{\varepsilon_2^2(1 - \gamma)^4} + \frac{(\env{\rmax} + \mon{\rmax})^2}{\varepsilon_2^2(1 - \gamma)^4}\ln{\frac{(\env{\rmax} + \mon{\rmax})\abs{\c{S}} \abs{\c{A}}}{\varepsilon_2(1 - \gamma)^2\delta_2}}\right)
\end{equation}

So overall the interplay between $\nicefrac 1\rho$ and $\bigO(\abs{\c{S}})$ determines the value of $m_2$. In the worst case $\frac{1}{\rho} \geq \bigO(\abs{\c{S}})$ and:
%
\begin{equation}
m_2 = \bigO\left(\frac{\env{\rmax} (\env{\rmax} + \mon{\rmax})^2}{\rho \varepsilon_2^2(1 - \gamma)^4} \ln{\frac{\env{\rmax}(\env{\rmax} + \mon{\rmax})\abs{\c{S}}\abs{\c{A}}}{\varepsilon_2(1 - \gamma)^2\delta_2}}\right)
\end{equation}
%
\paragraph{4. Optimism of $\model{Q}_{*, \text{optimize}}$.} The same as it is the case in observe episodes, optimism is needed to make sure that the agent has enough incentives to visit state-action pairs that their statistics are not close enough to their true values.  Suppose $m_2$ is least number of samples required to ensure $\mon{\estimate{R}}$, $\estimate{P}$, and, if possible, $\env{\estimate{R}}$ are close to their true mean.

To be pessimistic in cases that $\env{\estimate{R}}$ cannot be computed due to its ever-lasting unobservability, we need to investigate the optimism in two cases where $\env{\estimate{R}}$ can be computed and when it cannot. Consider $v$ experiences of a joint state-action $(s, a) \equiv (\env{s}, \mon{s}, \env{a}, \mon{a})$ and the first $\env{v}$ experiences of the environment state-action $(\env{s}, \env{a})$ where the environment reward has been observed. Also let $\estimate{V}_{*, \text{optimize}}$ denote the value function computed with $\env{\estimate{R}}, \mon{\estimate{R}}$ and ${\estimate{P}}$.
%

\textbf{Case 1.} $\env{v} > 0$.

Let $Y_i, Z_i$, and $W_i$ be random variables defined at the $i$th visit as below, where $s'_i$ is the next state visited after the the $i$th visit:
%
\begin{equation*}
        Y_i = \env{R}_i(\env{s}, \env{a}), Z_i = \mon{R}_i(\mon{s}, \mon{a}), W_i = \gamma \estimate{V}^{{M}_{\downarrow}}_{*, \text{optimize}}(s'_i)
\end{equation*}
%
If $(s, a)$ has been visited $v$ times and $R^e(s^e, a^e)$ has been observed $\env{v}$ times, then:
    \begin{itemize}
        \item The set $(Y_i)_{i=1}^{\env{v}}$ is available.
        \item The set $(W_i)_{i=1}^v$ is available.
        \item At least the set $(Z_i)_{i=1}^v$ is available. (At most $(Z_i)_{i=1}^{|\env{\c{S}}||\env{\c{A}}|v}$)
    \end{itemize}
    Let $(Y_i)_{i = 1}^{\env{v}}, (Z_i)_{i = 1}^{v}$, and $(W_i)_{i = 1}^{v}$ be random variables on joint probability space $(\Omega, \mathcal{F}, \mathbb{P})$. By applying the Chernoff-Hoeffding's inequality separately on them:
    
    For $W_i$ we have that $\gamma\frac{-\env{\rmax} - \mon{\rmax}}{1 - \gamma} \leq W_i \leq \gamma\frac{\env{\rmax} + \mon{\rmax}}{1 - \gamma}$ hence:
    %
    \begin{equation*}
        \prob{\EV{W_1}{} - \frac 1v \sum_{i=1}^{v} W_i \geq B_1} \leq \exp{\left(- \frac{vB_1^2(1 - \gamma)^2}{2\gamma^2(\env{\rmax} + \mon{\rmax})^2}\right)}
    \end{equation*}
    %
    For $Z_i$ we have:
    %
    \begin{equation*}
        \prob{\EV{Z_1}{} - \frac 1v \sum_{i=1}^{v} Z_i \geq B_2} \leq \exp{\left(- \frac{vB_2^2}{2(\mon{\rmax})^2}\right)}
    \end{equation*}
    %
    Similarly for $Y_i$ we have:
    %
    \begin{equation*}
        \prob{\EV{Y_1}{} - \frac{1}{\env{v}} \sum_{i=1}^{\env{v}} Y_i \geq B_3} \leq \exp{\left(- \frac{\env{v}B_3^2}{2(\env{\rmax})^2}\right)}
    \end{equation*}
    %
Define the following random variables on $(\Omega, \mathcal{F}, \mathbb{P})$:
    \begin{align*}
        W &= \EV{W_1}{} - \frac 1v \sum_{i = 1}^{v} W_i \\
        Z &= \EV{Z_1}{} - \frac 1v \sum_{i = 1}^{v} Z_i \\
        Y &= \EV{Y_1}{} - \frac{1}{\env{v}} \sum_{i = 1}^{\env{v}} W_i
    \end{align*}
By choosing $B_1 = \frac{\beta}{\sqrt v}, B_2 = \frac{\mon{\beta}}{\sqrt v}$, and $B_3 = \frac{\env{\beta}}{\sqrt{\env{v}}}$ where:
    %
    \begin{align*}
        &\beta = \frac{\gamma(\env{\rmax} + \mon{\rmax})\sqrt{2\ln{(6\abs{\c{S}}\abs{\c{A}}m_2 / \delta_2)}}}{1 - \gamma} \\
        & \mon{\beta} = \mon{\rmax}\sqrt{2\ln{(6\abs{\c{S}}\abs{\c{A}}m_2/\delta_2)}} \\
         & \env{\beta} = \env{\rmax}\sqrt{2\ln{(6\abs{\c{S}}\abs{\c{A}}m_2/\delta_2)}}
\end{align*}
%
And using Corollary~\ref{corollary:union_bound}, we have:
%
\begin{equation*}
        \prob{Y + Z + W \geq B_1 + B_2 + B_3} \leq \exp{\left(- \frac{vB_1^2(1 - \gamma)^2}{2\gamma^2(\env{\rmax} + \mon{\rmax})^2}\right)} + \exp{\left(- \frac{vB_2^2}{2(\mon{\rmax})^2}\right)} + \exp{\left(- \frac{\env{v}B_3^2}{2(\env{\rmax})^2}\right)}
\end{equation*}
%
Thus:
%
\begin{equation}
        \label{eq:y_z_w}
        \prob{Y + Z + W \geq \frac{\beta}{\sqrt v} + \frac{\mon{\beta}}{\sqrt v} + \frac{\env{\beta}}{\sqrt{\env{v}}}} \leq \frac{\delta_2}{2\abs{\c{S}}\abs{\c{A}}m_2}
\end{equation}
%
So with probability $1 - \nicefrac{\delta_2}{2\abs{\c{S}}\abs{\c{A}}m_2}$  it must hold that:
    \begin{equation*}
        Y + Z +W \leq \left(\frac{\beta}{\sqrt v} + \frac{\mon{\beta}}{\sqrt v} + \frac{\env{\beta}}{\sqrt{\env{v}}} \right)
    \end{equation*}
    which is equal to:
    \begin{align*}
        & \frac{1}{\env{v}} \sum_{i = 1}^{k}\env{R}_i(\env{s}, \env{a}) + \frac 1v\sum_{j = 1}^{v}\left(\mon{R}_j(\mon{s}, \mon{a}) + \estimate{V}^{{M}_{\downarrow}}_{*, \text{optimize}}(s_{j}')\right) + \left(\frac{\beta}{\sqrt v} + \frac{\mon{\beta}}{\sqrt v} + \frac{\env{\beta}}{\sqrt{\env{v}}} \right) \geq \\
        %
        & \EV{\env{R}_1(\env{s}, \env{a}) + \mon{R}_1(\mon{s}, \mon{a}) + \gamma V^{M_{\downarrow}}_{*, \text{optimize}}(s_1')}{} = Q^{M_{\downarrow}}_{*, \text{optimize}}(s, a)
    \end{align*}
    %
    So:
    \begin{equation}
        \label{eq:end_hoeffding}
        \env{\estimate{R}}(\env{s}, \env{a}) + \mon{\estimate{R}}(\mon{s}, \mon{a}) + \gamma \sum_{s'}\estimate{P}(s' \mid s, a)\estimate{V}^{{M}_{\downarrow}}_{*, \text{optimize}}(s') + \left(\frac{\beta}{\sqrt v} + \frac{\mon{\beta}}{\sqrt v} + \frac{\env{\beta}}{\sqrt{\env{v}}} \right) \geq Q^{M_{\downarrow}}_{*}(s, a)
    \end{equation}
    %
    By using the union bound over $\c{S}, \c{A}$ and $m_2$ the above inequality holds for all state-action pairs until they are visited $m_2$ times with probability at least $1 - \frac{\delta_2}{2}$. But Instead of left-hand side of~\cref{eq:end_hoeffding}, \thealgo the following action values:
    %
    \begin{equation}
        \model{Q}^{M_{\downarrow}}_{*, \text{optimize}}(s, a) = \env{\estimate{R}}(\env{s}, \env{a}) + \mon{\estimate{R}}(\mon{s}, \mon{a}) + \gamma \sum_{s'}\estimate{P}(s' \mid s, a)\bar{V}^{{M}_{\downarrow}}_{*, \text{optimize}}(s') + \left(\frac{\beta}{\sqrt v} + \frac{\mon{\beta}}{\sqrt v} + \frac{\env{\beta}}{\sqrt{\env{v}}} \right)
    \end{equation}
    %
    So by following the exact induction of Lemma 7 of \citet{strehl2008analysis}, we prove that $\model{Q}_{*, \text{optimize}}^{M_{\downarrow}}(s, a) \geq Q^{M_{\downarrow}}_{*, \text{optimize}}(s, a)$. Let:
    %
    \begin{equation*}
        C = \left(\frac{\beta}{\sqrt v} + \frac{\mon{\beta}}{\sqrt v} + \frac{\env{\beta}}{\sqrt{\env{v}}} \right)
    \end{equation*} 
    %
    Proof by induction is on the number of value iteration steps. Let $\model{Q}^{{M}_{\downarrow}}_{i, \text{optimize}}$ be the $i$th iterate of the value iteration for $(s, a)$. By the optimistic initialization we have that $\model{Q}^{{M}_{\downarrow}}_{0, \text{optimize}} \geq Q^{M_{\downarrow}}_{*, \text{optimize}}(s, a)$ for all state-action pairs. Now suppose the claim holds for $\model{Q}^{{M}_{\downarrow}}_{i, \text{optimize}}$, we have:
    %
\begin{align*}
        \model{Q}^{{M}_{\downarrow}}_{i + 1, \text{optimize}}(s, a) & = \env{\estimate{R}}(\env{s}, \env{a}) + \mon{\estimate{R}}(\mon{s}, \mon{a}) + \gamma\sum_{s'}\estimate{P}(s' \mid s, a)\max_{a'}\model{Q}^{{M}_{\downarrow}}_{i, \text{optimize}}(s', a') + C \\
        %
        & = \env{\estimate{R}}(\env{s}, \env{a}) + \mon{\estimate{R}}(\mon{s}, \mon{a}) + \gamma\sum_{s'}\estimate{P}(s' \mid s, a)\model{V}^{{M}_{\downarrow}}_{i, \text{optimize}}(s') + C \\
        %
        & \geq \env{\estimate{R}}(\env{s}, \env{a}) + \mon{\estimate{R}}(\mon{s}, \mon{a}) + \gamma\sum_{s'}\estimate{P}(s' \mid s, a)Q^{{M}_{\downarrow}}_{*, \text{optimize}}(s') + C & \text{(Using induction)} \\
        & \geq Q^{M_{\downarrow}}_{*}(s, a) & \text{(Using \cref{eq:end_hoeffding})}
\end{align*} 
%
%
%

\textbf{Case 2.} $\env{v} = 0$.

If $\env{v} = 0 = 0$ for $(s, a)$, then \thealgo assigns $-\env{\rmax}$ to $\estimate{R}(\env{s}, \env{a})$ deterministically. So the previously random variable $Y$ in Case1 is deterministically 0 and there would be no randomness around it. Consequently \cref{eq:y_z_w} is turned into:
%
\begin{equation*}
    \prob{Z + W \geq \frac{\beta}{\sqrt{v}} + \frac{\mon{\beta}}{\sqrt{v}}} \leq \frac{\delta_2}{3\abs{\c{S}}\abs{\c{A}}m_2}
\end{equation*}
%
Where $\beta$ and $\mon{\beta}$ are as before. Then with probability $1 - \nicefrac{\delta_2}{3\abs{\c{S}}\abs{\c{A}}m_2}$  it must hold that:
    \begin{equation*}
        Z +W \leq \left(\frac{\beta}{\sqrt v} + \frac{\mon{\beta}}{\sqrt v} \right)
    \end{equation*}
    which is equal to:
    \begin{align*}
        \frac 1v\sum_{j = 1}^{v}\left(\mon{R}_j(\mon{s}, \mon{a}) + \gamma \estimate{V}^{{M}_{\downarrow}}_{*, \text{optimize}}(s_{j}')\right) + \left(\frac{\beta}{\sqrt v} + \frac{\mon{\beta}}{\sqrt v}\right) & \geq \EV{\mon{R}_1(\mon{s}, \mon{a}) + \gamma V^{{M}_{\downarrow}}_{*, \text{optimize}}(s_1')}{} \\
        %
        & = Q^{{M}_{\downarrow}}_{*, \text{optimize}}(s, a) - (-\env{\rmax})
    \end{align*}
    %
    So:
    \begin{equation}
        -\env{\rmax} + \mon{\estimate{R}}(\mon{s}, \mon{a}) + \gamma \sum_{s'}\estimate{P}(s' \mid s, a)\estimate{V}^{{M}_{\downarrow}}_{*, \text{optimize}}(s') + \left(\frac{\beta}{\sqrt v} + \frac{\mon{\beta}}{\sqrt v} \right) \geq Q^{M_{\downarrow}}_{*}(s, a)
    \end{equation}
    %
And the rest of the proof is identical to the induction steps as in case 1, but with probability at least $1 - \frac{\delta_2}{3}$.

Hence considering both cases, with probability at least $1 - \frac{\delta_2}{2} - \frac{\delta_2}{3} = 1 - \frac{5\delta_2}{6}$:
\begin{equation*}
    \model{Q}_{*, \text{optimize}}^{M_{\downarrow}}(s, a) \geq Q^{M_{\downarrow}}_{*, \text{optimize}}(s, a)
\end{equation*}
%
%
%
\paragraph{5. Sample Complexity.}
An algorithm is PAC-MDP if for any MDP $M=\langle \c{S}, \c{A}, \c{R}, P, \gamma \rangle$ and any $\delta, \varepsilon > 0$ it finds an $\varepsilon$-optimal policy in $M$, in time polynomial to $(|\c{S}|, |\c{A}|, \frac{1}{\varepsilon}, \log\frac{1}{\delta}, \frac{1}{1 - \gamma}, \env{\rmax})$. In Mon-MDPs partial observability of the environment reward naturally makes any algorithm take more time, as the lower the probability is, the more samples are required to confidently approximate the statistics of the environment reward. As a result we extent the definition of PAC in MDPs to Mon-MDPs such that an algorithm is PAC-Mon-MDP minimax-optimal if for any Mon-MDP $M = \angle{\c{S}, \c{A}, P, \c{R}, \mon{f}, \gamma}$ and any $\delta, \varepsilon > 0$ it finds an $\varepsilon$-optimal policy in $M_{\downarrow}$, in time polynomial to $(|\c{S}|, |\c{A}|, \frac{1}{\varepsilon}, \log\frac{1}{\delta}, \frac{1}{1 - \gamma}, \env{\rmax} + \mon{\rmax}, \frac{1}{\rho})$, where $\rho$ is the minimum non-zero probability of observing the environment reward baked into $\mon{f}$.

Monitored MBIE operates in slices of episodes of maximum length of $H$, and computes its action-values at the beginning of each episode. To derive the sample complexity we make use of the result of Theorem 2 by \citet{strehl2008analysis}. Let $\delta_1 = \delta_2 = \delta' / 2, \varepsilon_1 = \varepsilon_2 = \varepsilon / 2$:

During observe episodes, as was discussed in the stage 1 of the proof learning transition dynamics is the challenge. Reward are also between in $[0, 1]$ hence it identically follows the procedure of MBIE-EB and its sample complexity during these episodes with probability at least $1 - \nicefrac{\delta'}{4}$ is:
%
\begin{equation*}
    \Tilde{\bigO}\left(\frac{m_1\abs{\c{S}}\abs{\c{A}}H}{\varepsilon_1(1 - \gamma)}\right) = \Tilde{\bigO}\left(\frac{\abs{\c{S}}^2\abs{\c{A}}H}{\varepsilon^3(1 - \gamma)^5}\right)
\end{equation*}
%
That is in the worst-case scenario in each episode only one unknown state-action is visited with probability $\varepsilon_1(1 - \gamma)$ and this visitation should be repeated $m_1$ times. This specifies the order of $m_3$ and $\kappa^*(k)$ in the theorem statement:
%
%
\begin{equation*}
    \kappa^*(k) = m_3 = \Tilde{\bigO}\left(\frac{m_1\abs{\c{S}}\abs{\c{A}}}{\varepsilon_1(1 - \gamma)}\right) = \Tilde{\bigO}\left(\frac{\abs{\c{S}}^2\abs{\c{A}}}{\varepsilon^3(1 - \gamma)^5}\right)
\end{equation*}
%
%
During optimize episodes, as is discussed in stage 3, the bottleneck of learning models depends on $\rho$. If $\nicefrac{1}{\rho} < \bigO(\abs{\c{S}})$ then the bottleneck is transition dynamics. With probability at least $1 - \nicefrac{5\delta'}{12}$, the sample complexity would be:
%
\begin{equation*}
    \Tilde{\bigO}\left(\frac{m_2\abs{\c{S}}\abs{\c{A}}H}{\varepsilon_2(1 - \gamma)(\env{\rmax} + \mon{\rmax})}\right)
\end{equation*}
%
where $(\env{\rmax} + \mon{\rmax})$ in the denominator is due to Lemma 3 of \citet{strehl2008analysis} that requires normalized rewards with maximum value of one to compute the probability of visiting unknown state-action pairs. 

If that $m_2 \big/ (\env{\rmax} + \mon{\rmax}) \geq m_1$, the overall sample complexity is:
%
\begin{equation*}
    \Tilde{\bigO}\left(\frac{m_2\abs{\c{S}}\abs{\c{A}}H}{\varepsilon_2(1 - \gamma)(\env{\rmax} + \mon{\rmax})}\right) =     \Tilde{\bigO}\left(\frac{(\env{\rmax} + \mon{\rmax})\abs{\c{S}}^2\abs{\c{A}}H}{\varepsilon^3(1 - \gamma)^5}\right)
\end{equation*}
%
Otherwise:
\begin{equation*}
    \Tilde{\bigO}\left(\frac{m_1\abs{\c{S}}\abs{\c{A}}H}{\varepsilon_1(1 - \gamma)}\right) =     \Tilde{\bigO}\left(\frac{\abs{\c{S}}^2\abs{\c{A}}H}{\varepsilon^3(1 - \gamma)^5}\right)
\end{equation*}
%

But if $\nicefrac{1}{\rho} > \bigO(\abs{\c{S}})$, then the bottleneck is learning the environment reward model. The sample complexity in this stage with probability at least $1 - \nicefrac{5\delta'}{12}$ is:
%
\begin{equation*}
   \Tilde{\bigO}\left(\frac{m_2\abs{\c{S}}\abs{\c{A}}H}{\varepsilon_2 (1 - \gamma)(\env{\rmax} + \mon{\rmax})}\right) = 
   \Tilde{\bigO}\left(\frac{\env{\rmax}(\env{\rmax} + \mon{\rmax})\abs{\c{S}}\abs{\c{A}}H}{\varepsilon^3 (1 - \gamma)^5\rho}\right)
\end{equation*}
%
Thus we conclude that the worst-case scenario is when $\nicefrac{1}{\rho} > \bigO(\abs{\c{S}})$ and the eventual sample complexity is dominated by the optimize episodes which holds with probability at least $1 - \nicefrac{5\delta'}{12}$:
\begin{equation*}
       \Tilde{\bigO}\left(\frac{\env{\rmax}(\env{\rmax} + \mon{\rmax})\abs{\c{S}}\abs{\c{A}}H}{\varepsilon^3 (1 - \gamma)^5\rho}\right)
\end{equation*}
%
Defining $\delta = \frac{5\delta'}{12}$ completes the proof. \qedsymbol
