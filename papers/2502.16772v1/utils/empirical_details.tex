\section{Empirical Evaluation}
\label{appendix:empirical _details}
Our baseline is Directed-E$^2$ as the precursor of our work that \citet{parisi2024beyond} showed its superior performance on Mon-MDPs against many well-established algorithms. Directed-E$^2$ uses two action-values, one as the ordinary action-values that uses a reward model in place of the environment reward to update action-values (this sets the agent free from the partial observability of the environment reward once the reward model is learned). Second action value function denoted by $S$ is dubbed as visitation-values that tries to maximized the successor representations. Directed-E$^2$ uses visitation-values to visit every joint state-action pairs but in the limit of infinite exploration it becomes greedy with respect to the main action-values for maximizing the expected sum of discounted rewards. 

Evaluations are based on the \textbf{discounted test return} and the results are averaged across 30 independent random seeds with their corresponding $95\%$ confidence intervals. To measure the discounted test return, after every 100 steps of the interaction, the learning is paused and the agent is tested for 100 episodes and the mean over the obtained return is used as a data point, then the learning carries on.
%
%
%
\begin{figure}[tbh]
        \centering
        \begin{subfigure}[b]{0.32\linewidth}
            \centering
            \caption*{\textbf{Empty}}
            \includegraphics[width=\linewidth]{imgs/envs/Empty.pdf}
        \end{subfigure}
        \hfill
        \begin{subfigure}[b]{0.32\linewidth}
            \centering
            \caption*{\textbf{Hazard}}
            \includegraphics[width=\linewidth]{imgs/envs/Hazard.pdf}
        \end{subfigure}
        \hfill
        \begin{subfigure}[b]{0.32\linewidth}
            \centering
            \caption*{\textbf{Bottleneck}}
            \includegraphics[width=\linewidth]{imgs/envs/Bottleneck.pdf}
        \end{subfigure}
        \\[5pt]
        \begin{subfigure}[b]{0.32\linewidth}
            \centering
            \caption*{\textbf{Loop}}
            \includegraphics[width=\linewidth]{imgs/envs/Loop.pdf}
        \end{subfigure}
        \hfill
        \begin{minipage}[b]{0.32\linewidth}
        \begin{subfigure}[b]{\linewidth}
                \centering
                \caption*{\textbf{River Swim}}
                \includegraphics[width=\linewidth]{imgs/envs/River_Swim.pdf}
            \end{subfigure}
        \\
        \begin{subfigure}[b]{\linewidth}
            \centering
            \caption*{\textbf{One-Way}}
            \includegraphics[width=\linewidth]{imgs/envs/Oneway.pdf}
        \end{subfigure}
        \end{minipage}
        \hfill
        \begin{minipage}[b]{0.32\linewidth}
        \begin{subfigure}[b]{\linewidth}
            \centering
            \caption*{\textbf{Two-Room-2x11}}
            \includegraphics[width=\linewidth]{imgs/envs/Two-Room-2x11.pdf}
         \end{subfigure}
        \\
        \begin{subfigure}[b]{\linewidth}
            \centering
            \caption*{\textbf{Corridor}}
            \includegraphics[width=\linewidth]{imgs/envs/Corridor.pdf}
         \end{subfigure}
        \\
         \begin{subfigure}[b]{\linewidth}
            \centering
            \caption*{\textbf{Two-Room-3x5}}
            \includegraphics[width=\linewidth]{imgs/envs/Two-Room-3x5.pdf}
        \end{subfigure}
        \end{minipage}
    \caption{\textbf{Full set of environments}}
    \label{fig:envs}
    \end{figure}
%
%
%
\begin{figure}[tbh]
    \centering
    \includegraphics[width=\linewidth]{imgs/envs/River_Swim_Transitions.pdf}
    \caption{\textbf{Dynamics of River Swim}. Each tuple represents (action, transition probability, reward).}
    \label{fig:river_swim_dynam}
\end{figure}
%
%
%
\subsection{Full Environments' Details}
\label{appendix:env_details}
Environments that comprise the experiments are: \textbf{Empty}, \textbf{Hazard}, \textbf{Bottleneck}, \textbf{Loop}, \textbf{River Swim}, \textbf{One-Way}, \textbf{Corridor}, \textbf{Two-Room-3x5} and \textbf{Two-Room-2x11} shown in Figure~\ref{fig:envs}. In all of them (except River Swim) the agent, represented by the robot, has 5 actions including 4 cardinal movement \{\texttt{LEFT, DOWN, RIGHT, UP}\} and an extra \texttt{STAY} action. its goal is get to the treasure chest as fast as possible it should \texttt{STAY} there to get a reward of +1 and this terminates the episode. it should not \texttt{STAY} at gold bars as they are distractors because it would get a reward of 0.1 and the episode gets terminated too. Snakes should be avoided as any action leading to their states results in rewards of -10. Cells with a one-way sign transition the agent only to their unique direction and if the agent stumbles on a hole, it would spend the whole episode in it, unless with probability 10\% its action is effective and it gets transitioned. When a button monitor is configured on top of the environment, the location of the button is metaphorically is indicated by a push button placed on a cell's border. It shows the button is pushed if agent bump herself into that cell's wall. The episode's time limit in River Swim, corridor and Two-Room-2x11 is 200 steps, and other environment it is 50 steps. In River Swim the agent has two actions \texttt{L} $\equiv$ \texttt{LEFT} and \texttt{R} $\equiv$ \texttt{RIGHT}. There is no termination except the episode's time limit. In this environment gold bars have a reward of 0.01. And It is the only environment in our experiment suite that has stochastic transitions as in Figure~\ref{fig:river_swim_dynam}.
%
\subsection{Full Monitors' Details}
\label{appendix:monitor_details}
Monitors that comprise the experiments are: \textbf{Full-Monitor (MDP)}, \textbf{Semi-Random}, \textbf{Full-Random}, \textbf{Ask}, \textbf{Button}, \textbf{N-Supporters}, \textbf{N-Experts} and \textbf{Level-Up}. For any of the monitors, except \textbf{Full-Monitor}, if a cell in the environment is marked with $\bot$, then under no circumstances and at no time step, the monitor would reveal the environment reward to agent for the action that led agent to that cell. For \emph{the rest} of the environment state-action pairs the behavior of monitors, by letting $X_t \sim \c{U}[0, 1]$, where $\c{U}$ is the uniform distribution and $\rho \in [0, 1]$, is as follows :
%
%
%
\begin{itemize}[leftmargin=*]
    \item \textbf{Full-Monitor}. This corresponds to MDP setting. Monitor shows the environment reward for all environment state-action pairs. State space and action state space of the monitor are singletons and the monitor reward is zero:
    \begin{align*}
        &\mon{\c{S}} \coloneq \{\texttt{ON}\}, && \mon{\c{A}} \coloneq \{\texttt{NO-OP}\}, && \mon{S}_{t + 1} \coloneq \texttt{ON} \\
        & \env{\widehat{R}}_t \coloneq \env{R}_t, \forall t \geq 0, && \mon{R}_t \coloneq 0
    \end{align*}
    %
    \item \textbf{Semi-Random}. It is similar to Full-Monitor except that non-zero environment rewards are stashed with 50\% chance:
    %
    \begin{align*}
        &\mon{\c{S}} \coloneq \{\texttt{ON}\}, && \mon{\c{A}} \coloneq \{\texttt{NO-OP}\}, && \mon{S}_{t + 1} \coloneq \texttt{ON} \\
        & \env{\widehat{R}}_t \coloneq 
        \begin{cases}
        \env{R}_t & \textbf{if } \env{R}_t = 0 \\
        \env{R}_t & \textbf{if } X_t \leq 0.5 \\
        \bot & \text{Otherwise}
        \end{cases},
        && \mon{R}_t \coloneq 0
    \end{align*}
    %
    \item \textbf{Full-Random}. It is similar to Semi-Random except \emph{any} environment reward is stashed with a predefined probability $\rho$:
    %
    \begin{align*}
        &\mon{\c{S}} \coloneq \{\texttt{ON}\}, && \mon{\c{A}} \coloneq \{\texttt{NO-OP}\}, && \mon{S}_{t + 1} \coloneq \texttt{ON} \\
        & \env{\widehat{R}}_t \coloneq 
        \begin{cases}
        \env{R}_t & \textbf{if }  X_t \leq \rho \\
        \bot & \text{Otherwise}
        \end{cases}
        && \mon{R}_t \coloneq 0
    \end{align*}
    %
    \item \textbf{Ask}. The state space is a singleton but the action space has two actions: \{\texttt{ASK}, \texttt{NO-OP}\}. Agent gets to see the environment reward with probability $\rho$ if it \texttt{ASK}s. Upon asking agent pays -0.2 as the monitor reward:
    \begin{align*}
        &\mon{\c{S}} \coloneq \{\texttt{ON}\}, && \mon{\c{A}} \coloneq \{\texttt{ASK, NO-OP}\}, && \mon{S}_{t + 1} \coloneqq \texttt{ON} \\
        & \env{\widehat{R}}_t \coloneq 
        \begin{cases}
        \env{R}_t & \textbf{if } X_t \leq \rho \textbf{ and } \mon{A}_t = \texttt{ASK}\\
        \bot & \text{Otherwise}
        \end{cases},
        && \mon{R}_t \coloneq 
        \begin{cases}
        -0.2 & \textbf{if } \mon{A}_t = \texttt{ASK}\\
        0 & \text{Otherwise}
        \end{cases},
    \end{align*}
    %
    \item \textbf{Button}. The state space is binary $\{\texttt{ON, OFF}\}$. The action space is a singleton. Agent gets to see the environment reward with probability $\rho$ if it manages to have the monitor be \texttt{ON}. The state is flipped if agent bumps herself to the wall that the button is placed on. As long as the button is \texttt{ON} agent pays -0.2:
    \begin{align*}
        &\mon{\c{S}} \coloneq \{\texttt{OFF, ON}\}, \\
        & \mon{\c{A}} \coloneq \{\texttt{NO-OP}\}, \\
        & \env{\widehat{R}}_t \coloneq 
        \begin{cases}
        \env{R}_t & \textbf{if } X_t \leq \rho \textbf{ and } \mon{S}_t = \texttt{ON} \\
        \bot & \text{Otherwise}
        \end{cases},
        \\
        & \mon{S}_{t + 1} \coloneq 
        \begin{cases}
            \texttt{ON} & \textbf{if } \mon{S}_t = \texttt{OFF} \textbf{ and } \env{S}_t = \texttt{"BUTTON-CELL"} \textbf{ and } \env{A}_t = \texttt{"BUMP-INTO-BUTTON"}\\
            \texttt{OFF} & \textbf{if } \mon{S}_t = \texttt{ON} \textbf{ and } \env{S}_t = \texttt{"BUTTON-CELL"} \textbf{ and } \env{A}_t = \texttt{"BUMP-INTO-BUTTON"}\\
            \mon{S}_t & \text{Otherwise}
        \end{cases} \\
        & \mon{S}_{1} \coloneqq  \text{Random uniform from } \mon{\c{S}} \\
        & \mon{R}_t \coloneq 
        \begin{cases}
        -0.2 & \textbf{if } \mon{S}_t = \texttt{ON}\\
        0 & \text{Otherwise}
        \end{cases},
    \end{align*}
    %
    \item \textbf{N-Supporters}. The state space is consisted of $N$ states that each represents the presence of a supporter. The action space is also consisted of $N$ actions. At each time step one of the supporters is randomly present and if agent could choose the action that matches the index of the supporter, then it gets to see the environment reward with probability $\rho$. Upon observing the environment reward agent pays a penalty of $-0.2$ as the monitor reward. However if it chooses a wrong supporter, then it will be rewarded $0.001$ as the monitor reward:
    %
    \begin{align*}
        &\mon{\c{S}} \coloneq \{0, \cdots, N - 1\}, 
        && \mon{\c{A}} \coloneq \{0, \cdots, N - 1\} \\
        & \env{\widehat{R}}_t \coloneq 
        \begin{cases}
        \env{R}_t & \textbf{if } X_t \leq \rho \textbf{ and } \mon{S}_t = \mon{A}_t\\
        \bot & \text{Otherwise}
        \end{cases},
        && \mon{R}_t \coloneq 
        \begin{cases}
        -0.2 &  \mon{S}_t = \mon{A}_t\\
        0.001 & \text{Otherwise}
        \end{cases} \\
        & \mon{S}_{t + 1} \coloneq \text{Random uniform from } \mon{\c{S}}
    \end{align*}
    %
    \citet{parisi2024beyond} introduced this monitor as being difficult due to its bigger spaces; however, the encouraging nature of the monitor regarding the agent's mistakes makes the monitor easy for reward-respecting algorithms e.g. \thealgo.
    %
    \item \textbf{N-Experts}. Similar to N-Supporter the state space has $N$ states, each corresponding the presence of $N$ experts. However getting experts' advice is costly, hence the action space has $N + 1$ action where the last action corresponds to not pinging any experts. At each time step one of the experts is randomly present and if the agent selects the action that matches the present expert's id, it gets to see the environment reward with probability $\rho$. Upon observing the environment reward agent pays a penalty of $-0.2$ as the monitor reward. However if it chooses a wrong expert it will be penalized by $-0.001$ as the monitor reward. Since the last action does not inquire any of the experts its monitor reward is $0$:
    %
    \begin{align*}
        &\mon{\c{S}} \coloneq \{0, \cdots, N - 1\}, 
        && \mon{\c{A}} \coloneq \{0, \cdots, N\} \\
        & \env{\widehat{R}}_t \coloneq 
        \begin{cases}
        \env{R}_t & \textbf{if } X_t \leq \rho \textbf{ and } \mon{S}_t = \mon{A}_t\\
        \bot & \text{Otherwise}
        \end{cases},
        && \mon{R}_t \coloneq 
        \begin{cases}
        -0.2 &  \textbf{if } \mon{S}_t = \mon{A}_t\\
        0 &  \textbf{if } \mon{A}_t = N\\
        -0.001 & \text{Otherwise}
        \end{cases} \\
        & \mon{S}_{t + 1} \coloneq \text{Random uniform from } \mon{\c{S}}
    \end{align*}
    %
    \item \textbf{Level-Up}. This monitor tries to test the agent's capabilities of performing deep exploration~\citep{osband2019deep}. The state space has $N$ states corresponding to $N$ levels. The action space has $N + 1$ actions. The initial state of the monitor is 0 and if at each time step agent selects the action that matches the state of the monitor, the state increases by one. If agent selects the wrong action the state is reset back to 0. Agent only gets to observe the environment reward with probability $\rho$ if it manages to get the state of the monitor to the max level. Agent is penalized with $-0.2$ as the monitor reward every time it does not select the last action which does nothing and keeps the state as it is:
    \begin{align*}
        &\mon{\c{S}} \coloneq \{0, \cdots, N - 1\}, 
        && \mon{\c{A}} \coloneq \{0, \cdots, N\} \\
        & \env{\widehat{R}}_t \coloneq 
        \begin{cases}
        \env{R}_t & \textbf{if } X_t \leq \rho \textbf{ and } \mon{S}_t = N - 1 \\
        \bot & \text{Otherwise}
        \end{cases},
        && \mon{R}_t \coloneq 
        \begin{cases}
        0 &  \textbf{if } \mon{A}_t = N \\
        -0.2 & \text{Otherwise}
        \end{cases} \\
        & \mon{S}_{t + 1} \coloneq
        \begin{cases}
            \mon{S}_t & \textbf{if } \mon{A}_t = N \\
            \max\left\{ \mon{S}_t + 1, N - 1 \right\} & \textbf{if } \mon{S}_t = \mon{A}_t \\
            0 & \text{Otherwise}
        \end{cases}
    \end{align*}
\end{itemize}
In the experiments $\rho = 1$ unless otherwise states. Experiments that include N-Supporters or N-Experts, $N = 4$ and the number of levels for Level-Up is 3.

