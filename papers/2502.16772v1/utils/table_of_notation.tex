\section{Table of Notation}
% \centering
\begin{tabular}{ l l } 
 \hline
 \textbf{Symbol} & \textbf{Explanation} \\
 \hline
 $\Delta(\c{X})$ & $\Delta(\c{X})$ denotes the set of distributions over the finite set $\c{X}$  \\ 
 $M$ & An instance of a Mon-MDP  \\ 
 % $M_{\downarrow}$ & Worst-case Mon-MDP in the equivalence class of $M$ \SIM{equiv class not defined} \\ 
 $\c{S}$ & State space (environment-only in MDPs, joint environment-monitor in Mon-MDPs)  \\ 
 $\c{A}$ & Action space (environment-only in MDPs, joint environment-monitor in Mon-MDPs)  \\ 
 $r(s, a)$ & Expected reward from taking action $a$ from state $s$\\ 
 $P$ & Transition probabilities in MDPs / Joint transition probabilities in Mon-MDPs  \\ 
 $\estimate{P}$ & maximum likelihood estimate of $P$  \\
 $\gamma$ & Discount factor \\
 $\env{\rmax}$ & Maximum expected environment reward; $-\env{\rmax}$ is the minimum \\
 $\env{\c{S}}$ & Environment state space  \\ 
 $\env{\c{A}}$ & Environment action space  \\ 
 $\env{R}$ & Immediate environment reward  \\
 $\env{\widehat{R}}$ & Immediate environment proxy reward  \\
$\env{\estimate{R}}$ & Maximum likelihood estimate of $\env{r}$  \\
$\env{P}$ & Environment transition probabilities  \\ 
 $\mon{\c{S}}$ & Monitor state space  \\ 
  $\mon{P}$ & Monitor transition probabilities  \\ 
 $\mon{f}$ & Monitor function  \\
 $\mon{\rmax}$ & Maximum expected monitor reward; $-\mon{\rmax}$ is the minimum\\
 $N(s, a)$ & Number of times  $a$ has been taken from $s$ \\
 $N(\mon{s}, \mon{a})$ &  Number of times $\mon{a}$ has been taken from $\mon{s}$ \\
$N(\env{s}, \env{a})$ & Number of times $\env{s}$ has been taken from $\env{s}$ and the reward observed \\
 $\kappa$& Number of explore episodes \\
 $\kappa^*(k)$ & Desired number of explore episodes through episode $k$ \\
 $\rho$ & The minimum non-zero probability of observing the environment reward\\
 \hline
\end{tabular}