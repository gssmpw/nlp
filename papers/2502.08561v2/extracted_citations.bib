@inproceedings{blain2023findings,
  title={Findings of the WMT 2023 shared task on quality estimation},
  author={Blain, Frederic and Zerva, Chrysoula and Rei, Ricardo and Guerreiro, Nuno M and Kanojia, Diptesh and de Souza, Jos{\'e} GC and Silva, Beatriz and Vaz, T{\^a}nia and Jingxuan, Yan and Azadi, Fatemeh and others},
  booktitle={Proceedings of the Eighth Conference on Machine Translation},
  pages={629--653},
  year={2023}
}

@inproceedings{deng-raffel-2023-reward,
    title = "Reward-Augmented Decoding: Efficient Controlled Text Generation With a Unidirectional Reward Model",
    author = "Deng, Haikang  and
      Raffel, Colin",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.721/",
    doi = "10.18653/v1/2023.emnlp-main.721",
    pages = "11781--11791",
    abstract = "While large language models have proven effective in a huge range of downstream applications, they often generate text that is problematic or lacks a desired attribute. In this paper, we introduce Reward-Augmented Decoding (RAD), a text generation procedure that uses a small unidirectional reward model to encourage a language model to generate text that has certain properties. Specifically, RAD uses the reward model to score generations as they are produced and rescales sampling probabilities to favor high-reward tokens. By using a unidirectional reward model, RAD can cache activations from prior generation steps to decrease computational overhead. Through experiments on generating non-toxic and sentiment-controlled text, we demonstrate that RAD performs best among methods that change only the generation procedure and matches the performance of state-of-the-art methods that involve re-training the language model. We further validate that RAD is effective on very large language models while incurring a minimal computational overhead."
}

@article{faria2024quest,
  title={QUEST: Quality-Aware Metropolis-Hastings Sampling for Machine Translation},
  author={Faria, Gon{\c{c}}alo RA and Agrawal, Sweta and Farinhas, Ant{\'o}nio and Rei, Ricardo and de Souza, Jos{\'e} GC and Martins, Andr{\'e} FT},
  journal={arXiv preprint arXiv:2406.00049},
  year={2024}
}

@inproceedings{fernandes2022quality,
  title={Quality-Aware Decoding for Neural Machine Translation},
  author={Fernandes, Patrick and Farinhas, Ant{\'o}nio and Rei, Ricardo and de Souza, Jos{\'e} GC and Ogayo, Perez and Neubig, Graham and Martins, Andr{\'e} FT},
  booktitle={Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  pages={1396--1412},
  year={2022}
}

@inproceedings{finkelstein2024introducing,
  title={Introducing the NewsPaLM MBR and QE Dataset: LLM-Generated High-Quality Parallel Data Outperforms Traditional Web-Crawled Data},
  author={Finkelstein, Mara and Vilar, David and Freitag, Markus},
  booktitle={Proceedings of the Ninth Conference on Machine Translation},
  pages={1355--1372},
  year={2024}
}

@article{guerreiro2024xcomet,
  title={xcomet: Transparent machine translation evaluation through fine-grained error detection},
  author={Guerreiro, Nuno M and Rei, Ricardo and Stigt, Daan van and Coheur, Luisa and Colombo, Pierre and Martins, Andr{\'e} FT},
  journal={Transactions of the Association for Computational Linguistics},
  volume={12},
  pages={979--995},
  year={2024},
  publisher={MIT Press 255 Main Street, 9th Floor, Cambridge, Massachusetts 02142, USA~â€¦}
}

@inproceedings{li-etal-2024-reinforcement,
    title = "Reinforcement Learning with Token-level Feedback for Controllable Text Generation",
    author = "Li, Wendi  and
      Wei, Wei  and
      Xu, Kaihe  and
      Xie, Wenfeng  and
      Chen, Dangyang  and
      Cheng, Yu",
    editor = "Duh, Kevin  and
      Gomez, Helena  and
      Bethard, Steven",
    booktitle = "Findings of the Association for Computational Linguistics: NAACL 2024",
    month = jun,
    year = "2024",
    address = "Mexico City, Mexico",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-naacl.111/",
    doi = "10.18653/v1/2024.findings-naacl.111",
    pages = "1704--1719",
    abstract = "To meet the requirements of real-world applications, it is essential to control generations of large language models (LLMs). Prior research has tried to introduce reinforcement learning (RL) into controllable text generation while most existing methods suffer from overfitting issues (finetuning-based methods) or semantic collapse (post-processing methods). However, current RL methods are generally guided by coarse-grained (sentence/paragraph-level) feedback, which may lead to suboptimal performance owing to semantic twists or progressions within sentences. To tackle that, we propose a novel reinforcement learning algorithm named TOLE which formulates TOken-LEvel rewards for controllable text generation, and employs a {\textquotedblleft}first-quantize-then-noise{\textquotedblright} paradigm to enhance the robustness of the RL algorithm. Furthermore, TOLE can be flexibly extended to multiple constraints with little computational expense. Experimental results show that our algorithm can achieve superior performance on both single-attribute and multi-attribute control tasks. We have released our codes at https://github.com/WindyLee0822/CTG."
}

@inproceedings{rei2021references,
  title={Are references really needed? unbabel-IST 2021 submission for the metrics shared task},
  author={Rei, Ricardo and Farinha, Ana C and Zerva, Chrysoula and van Stigt, Daan and Stewart, Craig and Ramos, Pedro and Glushkova, Taisiya and Martins, Andr{\'e} FT and Lavie, Alon},
  booktitle={Proceedings of the Sixth Conference on Machine Translation},
  pages={1030--1040},
  year={2021}
}

@inproceedings{rei2022cometkiwi,
  title={CometKiwi: IST-Unbabel 2022 Submission for the Quality Estimation Shared Task},
  author={Rei, Ricardo and Treviso, Marcos and Guerreiro, Nuno M and Zerva, Chrysoula and Farinha, Ana C and Maroti, Christine and de Souza, Jos{\'e} GC and Glushkova, Taisiya and Alves, Duarte and Coheur, Lu{\'\i}sa and others},
  booktitle={Proceedings of the Seventh Conference on Machine Translation (WMT)},
  pages={634--645},
  year={2022}
}

@inproceedings{tomani2024quality,
  title={Quality-aware translation models: Efficient generation and quality estimation in a single model},
  author={Tomani, Christian and Vilar, David and Freitag, Markus and Cherry, Colin and Naskar, Subhajit and Finkelstein, Mara and Garcia, Xavier and Cremers, Daniel},
  booktitle={Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={15660--15679},
  year={2024}
}

@article{vernikos2024don,
  title={Don't Rank, Combine! Combining Machine Translation Hypotheses Using Quality Estimation},
  author={Vernikos, Giorgos and Popescu-Belis, Andrei},
  journal={arXiv preprint arXiv:2401.06688},
  year={2024}
}

@article{xu2024contrastive,
  title={Contrastive preference optimization: Pushing the boundaries of llm performance in machine translation},
  author={Xu, Haoran and Sharaf, Amr and Chen, Yunmo and Tan, Weiting and Shen, Lingfeng and Van Durme, Benjamin and Murray, Kenton and Kim, Young Jin},
  journal={arXiv preprint arXiv:2401.08417},
  year={2024}
}

@inproceedings{yang2021fudge,
  title={FUDGE: Controlled Text Generation With Future Discriminators},
  author={Yang, Kevin and Klein, Dan},
  booktitle={Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  pages={3511--3535},
  year={2021}
}

@inproceedings{zerva2024findings,
  title={Findings of the Quality Estimation Shared Task at WMT 2024 Are LLMs Closing the Gap in QE?},
  author={Zerva, Chrysoula and Blain, Fr{\'e}d{\'e}ric and De Souza, Jos{\'e} GC and Kanojia, Diptesh and Deoghare, Sourabh and Guerreiro, Nuno M and Attanasio, Giuseppe and Rei, Ricardo and Orasan, Constantin and Negri, Matteo and others},
  booktitle={Proceedings of the Ninth Conference on Machine Translation,},
  pages={82--109},
  year={2024},
  organization={Association for Computational Linguistics}
}

@article{zhang2024learning,
  title={Learning from others' mistakes: Finetuning machine translation models with span-level error annotations},
  author={Zhang, Lily H and Dadkhahi, Hamid and Finkelstein, Mara and Trabelsi, Firas and Luo, Jiaming and Freitag, Markus},
  journal={arXiv preprint arXiv:2410.16509},
  year={2024}
}

