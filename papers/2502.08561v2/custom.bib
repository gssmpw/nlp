% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").

% Log probs are not good

@inproceedings{tomani2024quality,
  title={Quality-aware translation models: Efficient generation and quality estimation in a single model},
  author={Tomani, Christian and Vilar, David and Freitag, Markus and Cherry, Colin and Naskar, Subhajit and Finkelstein, Mara and Garcia, Xavier and Cremers, Daniel},
  booktitle={Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={15660--15679},
  year={2024}
}

@inproceedings{eikema2020map,
  title={Is MAP Decoding All You Need? The Inadequacy of the Mode in Neural Machine Translation},
  author={Eikema, Bryan and Aziz, Wilker},
  booktitle={Proceedings of the 28th International Conference on Computational Linguistics},
  pages={4506--4520},
  year={2020}
}

@inproceedings{freitag2020bleu,
  title={BLEU might be Guilty but References are not Innocent},
  author={Freitag, Markus and Grangier, David and Caswell, Isaac},
  booktitle={Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  pages={61--71},
  year={2020}
}

@inproceedings{fernandes2022quality,
  title={Quality-Aware Decoding for Neural Machine Translation},
  author={Fernandes, Patrick and Farinhas, Ant{\'o}nio and Rei, Ricardo and de Souza, Jos{\'e} GC and Ogayo, Perez and Neubig, Graham and Martins, Andr{\'e} FT},
  booktitle={Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  pages={1396--1412},
  year={2022}
}

% Translation models

@article{alves2024tower,
  title={Tower: An open multilingual large language model for translation-related tasks},
  author={Alves, Duarte M and Pombal, Jos{\'e} and Guerreiro, Nuno M and Martins, Pedro H and Alves, Jo{\~a}o and Farajian, Amin and Peters, Ben and Rei, Ricardo and Fernandes, Patrick and Agrawal, Sweta and others},
  journal={arXiv preprint arXiv:2402.17733},
  year={2024}
}

@article{xu2024contrastive,
  title={Contrastive preference optimization: Pushing the boundaries of llm performance in machine translation},
  author={Xu, Haoran and Sharaf, Amr and Chen, Yunmo and Tan, Weiting and Shen, Lingfeng and Van Durme, Benjamin and Murray, Kenton and Kim, Young Jin},
  journal={arXiv preprint arXiv:2401.08417},
  year={2024}
}

@article{kudugunta2024madlad,
  title={Madlad-400: A multilingual and document-level large audited dataset},
  author={Kudugunta, Sneha and Caswell, Isaac and Zhang, Biao and Garcia, Xavier and Xin, Derrick and Kusupati, Aditya and Stella, Romi and Bapna, Ankur and Firat, Orhan},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, A},
  journal={Advances in Neural Information Processing Systems},
  year={2017}
}

@article{dubey2024llama,
  title={The llama 3 herd of models},
  author={Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024}
}

@article{jiang2023mistral,
  title={Mistral 7B},
  author={Jiang, Albert Q and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lucile and others},
  journal={arXiv preprint arXiv:2310.06825},
  year={2023}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@inproceedings{kocmi2024findings,
  title={Findings of the WMT24 General Machine Translation Shared Task: The LLM Era Is Here but MT Is Not Solved Yet},
  author={Kocmi, Tom and Avramidis, Eleftherios and Bawden, Rachel and Bojar, Ond{\v{r}}ej and Dvorkovich, Anton and Federmann, Christian and Fishel, Mark and Freitag, Markus and Gowda, Thamme and Grundkiewicz, Roman and others},
  booktitle={Proceedings of the Ninth Conference on Machine Translation},
  pages={1--46},
  year={2024}
}


% QE

@misc{freitag2021experts,
      title={Experts, Errors, and Context: A Large-Scale Study of Human Evaluation for Machine Translation}, 
      author={Markus Freitag and George Foster and David Grangier and Viresh Ratnakar and Qijun Tan and Wolfgang Macherey},
      year={2021},
      eprint={2104.14478},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{rei2021references,
  title={Are references really needed? unbabel-IST 2021 submission for the metrics shared task},
  author={Rei, Ricardo and Farinha, Ana C and Zerva, Chrysoula and van Stigt, Daan and Stewart, Craig and Ramos, Pedro and Glushkova, Taisiya and Martins, Andr{\'e} FT and Lavie, Alon},
  booktitle={Proceedings of the Sixth Conference on Machine Translation},
  pages={1030--1040},
  year={2021}
}

@inproceedings{zerva2024findings,
  title={Findings of the Quality Estimation Shared Task at WMT 2024 Are LLMs Closing the Gap in QE?},
  author={Zerva, Chrysoula and Blain, Fr{\'e}d{\'e}ric and De Souza, Jos{\'e} GC and Kanojia, Diptesh and Deoghare, Sourabh and Guerreiro, Nuno M and Attanasio, Giuseppe and Rei, Ricardo and Orasan, Constantin and Negri, Matteo and others},
  booktitle={Proceedings of the Ninth Conference on Machine Translation,},
  pages={82--109},
  year={2024},
  organization={Association for Computational Linguistics}
}

@inproceedings{treviso2024xtower,
  title={xTower: A Multilingual LLM for Explaining and Correcting Translation Errors},
  author={Treviso, Marcos and Guerreiro, Nuno and Agrawal, Sweta and Rei, Ricardo and Pombal, Jos{\'e} and Vaz, T{\^a}nia and Wu, Helena and Silva, Beatriz and Stigt, Daan and Martins, Andr{\'e} FT},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2024},
  pages={15222--15239},
  year={2024}
}

@inproceedings{burchardt2013multidimensional,
  title={Multidimensional quality metrics: a flexible system for assessing translation quality},
  author={Burchardt, Aljoscha},
  booktitle={Proceedings of Translating and the Computer 35},
  year={2013}
}

@inproceedings{freitag2024llms,
  title={Are LLMs breaking MT metrics? results of the WMT24 metrics shared task},
  author={Freitag, Markus and Mathur, Nitika and Deutsch, Daniel and Lo, Chi-Kiu and Avramidis, Eleftherios and Rei, Ricardo and Thompson, Brian and Blain, Frederic and Kocmi, Tom and Wang, Jiayi and others},
  booktitle={Proceedings of the Ninth Conference on Machine Translation},
  pages={47--81},
  year={2024}
}

@article{nllb2024scaling,
  title={Scaling neural machine translation to 200 languages},
  author={NLLB Team and others},
  journal={Nature},
  volume={630},
  number={8018},
  pages={841},
  year={2024}
}

@inproceedings{koneru2024plug,
  title={Plug, Play, and Fuse: Zero-Shot Joint Decoding via Word-Level Re-ranking across Diverse Vocabularies},
  author={Koneru, Sai and Huck, Matthias and Exel, Miriam and Niehues, Jan},
  booktitle={Proceedings of the Ninth Conference on Machine Translation},
  pages={1467--1481},
  year={2024}
}

@article{guerreiro2024xcomet,
  title={xcomet: Transparent machine translation evaluation through fine-grained error detection},
  author={Guerreiro, Nuno M and Rei, Ricardo and Stigt, Daan van and Coheur, Luisa and Colombo, Pierre and Martins, Andr{\'e} FT},
  journal={Transactions of the Association for Computational Linguistics},
  volume={12},
  pages={979--995},
  year={2024},
  publisher={MIT Press 255 Main Street, 9th Floor, Cambridge, Massachusetts 02142, USA~â€¦}
}

@article{juraska2024metricx,
  title={Metricx-24: The google submission to the WMT 2024 metrics shared task},
  author={Juraska, Juraj and Deutsch, Daniel and Finkelstein, Mara and Freitag, Markus},
  journal={arXiv preprint arXiv:2410.03983},
  year={2024}
}

@article{zhang2024learning,
  title={Learning from others' mistakes: Finetuning machine translation models with span-level error annotations},
  author={Zhang, Lily H and Dadkhahi, Hamid and Finkelstein, Mara and Trabelsi, Firas and Luo, Jiaming and Freitag, Markus},
  journal={arXiv preprint arXiv:2410.16509},
  year={2024}
}

@inproceedings{yang2021fudge,
  title={FUDGE: Controlled Text Generation With Future Discriminators},
  author={Yang, Kevin and Klein, Dan},
  booktitle={Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  pages={3511--3535},
  year={2021}
}

@inproceedings{finkelstein2024introducing,
  title={Introducing the NewsPaLM MBR and QE Dataset: LLM-Generated High-Quality Parallel Data Outperforms Traditional Web-Crawled Data},
  author={Finkelstein, Mara and Vilar, David and Freitag, Markus},
  booktitle={Proceedings of the Ninth Conference on Machine Translation},
  pages={1355--1372},
  year={2024}
}


@inproceedings{blain2023findings,
  title={Findings of the WMT 2023 shared task on quality estimation},
  author={Blain, Frederic and Zerva, Chrysoula and Rei, Ricardo and Guerreiro, Nuno M and Kanojia, Diptesh and de Souza, Jos{\'e} GC and Silva, Beatriz and Vaz, T{\^a}nia and Jingxuan, Yan and Azadi, Fatemeh and others},
  booktitle={Proceedings of the Eighth Conference on Machine Translation},
  pages={629--653},
  year={2023}
}

@inproceedings{rei2022cometkiwi,
  title={CometKiwi: IST-Unbabel 2022 Submission for the Quality Estimation Shared Task},
  author={Rei, Ricardo and Treviso, Marcos and Guerreiro, Nuno M and Zerva, Chrysoula and Farinha, Ana C and Maroti, Christine and de Souza, Jos{\'e} GC and Glushkova, Taisiya and Alves, Duarte and Coheur, Lu{\'\i}sa and others},
  booktitle={Proceedings of the Seventh Conference on Machine Translation (WMT)},
  pages={634--645},
  year={2022}
}

@inproceedings{deng-raffel-2023-reward,
    title = "Reward-Augmented Decoding: Efficient Controlled Text Generation With a Unidirectional Reward Model",
    author = "Deng, Haikang  and
      Raffel, Colin",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.721/",
    doi = "10.18653/v1/2023.emnlp-main.721",
    pages = "11781--11791",
    abstract = "While large language models have proven effective in a huge range of downstream applications, they often generate text that is problematic or lacks a desired attribute. In this paper, we introduce Reward-Augmented Decoding (RAD), a text generation procedure that uses a small unidirectional reward model to encourage a language model to generate text that has certain properties. Specifically, RAD uses the reward model to score generations as they are produced and rescales sampling probabilities to favor high-reward tokens. By using a unidirectional reward model, RAD can cache activations from prior generation steps to decrease computational overhead. Through experiments on generating non-toxic and sentiment-controlled text, we demonstrate that RAD performs best among methods that change only the generation procedure and matches the performance of state-of-the-art methods that involve re-training the language model. We further validate that RAD is effective on very large language models while incurring a minimal computational overhead."
}

@inproceedings{li-etal-2024-reinforcement,
    title = "Reinforcement Learning with Token-level Feedback for Controllable Text Generation",
    author = "Li, Wendi  and
      Wei, Wei  and
      Xu, Kaihe  and
      Xie, Wenfeng  and
      Chen, Dangyang  and
      Cheng, Yu",
    editor = "Duh, Kevin  and
      Gomez, Helena  and
      Bethard, Steven",
    booktitle = "Findings of the Association for Computational Linguistics: NAACL 2024",
    month = jun,
    year = "2024",
    address = "Mexico City, Mexico",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-naacl.111/",
    doi = "10.18653/v1/2024.findings-naacl.111",
    pages = "1704--1719",
    abstract = "To meet the requirements of real-world applications, it is essential to control generations of large language models (LLMs). Prior research has tried to introduce reinforcement learning (RL) into controllable text generation while most existing methods suffer from overfitting issues (finetuning-based methods) or semantic collapse (post-processing methods). However, current RL methods are generally guided by coarse-grained (sentence/paragraph-level) feedback, which may lead to suboptimal performance owing to semantic twists or progressions within sentences. To tackle that, we propose a novel reinforcement learning algorithm named TOLE which formulates TOken-LEvel rewards for controllable text generation, and employs a {\textquotedblleft}first-quantize-then-noise{\textquotedblright} paradigm to enhance the robustness of the RL algorithm. Furthermore, TOLE can be flexibly extended to multiple constraints with little computational expense. Experimental results show that our algorithm can achieve superior performance on both single-attribute and multi-attribute control tasks. We have released our codes at https://github.com/WindyLee0822/CTG."
}

@inproceedings{wolf-etal-2020-transformers,
    title = "Transformers: State-of-the-Art Natural Language Processing",
    author = "Wolf, Thomas  and
      Debut, Lysandre  and
      Sanh, Victor  and
      Chaumond, Julien  and
      Delangue, Clement  and
      Moi, Anthony  and
      Cistac, Pierric  and
      Rault, Tim  and
      Louf, Remi  and
      Funtowicz, Morgan  and
      Davison, Joe  and
      Shleifer, Sam  and
      von Platen, Patrick  and
      Ma, Clara  and
      Jernite, Yacine  and
      Plu, Julien  and
      Xu, Canwen  and
      Le Scao, Teven  and
      Gugger, Sylvain  and
      Drame, Mariama  and
      Lhoest, Quentin  and
      Rush, Alexander",
    editor = "Liu, Qun  and
      Schlangen, David",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
    month = oct,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-demos.6/",
    doi = "10.18653/v1/2020.emnlp-demos.6",
    pages = "38--45",
    abstract = "Recent progress in natural language processing has been driven by advances in both model architecture and model pretraining. Transformer architectures have facilitated building higher-capacity models and pretraining has made it possible to effectively utilize this capacity for a wide variety of tasks. Transformers is an open-source library with the goal of opening up these advances to the wider machine learning community. The library consists of carefully engineered state-of-the art Transformer architectures under a unified API. Backing this library is a curated collection of pretrained models made by and available for the community. Transformers is designed to be extensible by researchers, simple for practitioners, and fast and robust in industrial deployments. The library is available at \url{https://github.com/huggingface/transformers}."
}

@inproceedings{hulora,
  title={LoRA: Low-Rank Adaptation of Large Language Models},
  author={Hu, Edward J and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu and others},
  booktitle={International Conference on Learning Representations},
  year={2021}
}


@article{faria2024quest,
  title={QUEST: Quality-Aware Metropolis-Hastings Sampling for Machine Translation},
  author={Faria, Gon{\c{c}}alo RA and Agrawal, Sweta and Farinhas, Ant{\'o}nio and Rei, Ricardo and de Souza, Jos{\'e} GC and Martins, Andr{\'e} FT},
  journal={arXiv preprint arXiv:2406.00049},
  year={2024}
}

@article{vernikos2024don,
  title={Don't Rank, Combine! Combining Machine Translation Hypotheses Using Quality Estimation},
  author={Vernikos, Giorgos and Popescu-Belis, Andrei},
  journal={arXiv preprint arXiv:2401.06688},
  year={2024}
}