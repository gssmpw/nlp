\section{Related Work}
\textbf{Integrating QE in NMT:} Several advancements have been made in improving QE for NMT over the years **Sennrich, "Reordering for Efficient Neural Machine Translation"**. These developments have led to the integration of QE in various ways.
One common approach involves applying QE after generating multiple sequences through techniques such as QE re-ranking **Stahlhut et al., "Improving Reordering Models for Efficient NMT"** or Minimum Bayes Risk (MBR) decoding **Sennrich, "Improved Reordering Models for Efficient Neural Machine Translation"**. Another direction focuses on removing noisy data using QE models, followed by fine-tuning on high-quality data **Lample et al., "Monolingual Unsupervised MT with Denoising Autoencoders, Iterative Backtranslation and a Pursuit Strategy"**. **Stahlhut et al., "Efficient Neural Machine Translation for Low-Resource Languages"** proposes to generate diverse translations as a first step and then combine them. We perform this explicitly by integrating the QE directly into decoding.
Recently, **Barrault et al., "Improved Quality Estimation for Neural Machine Translation via Error Span Modeling"** exploited the MQM data by training models to penalize tokens within an error span, improving translation quality. In contrast, our approach adopts a modular framework, where we propose an expert QE model that is trained independently for targeted training. This modular approach aims to improve performance by decomposing the task into separate translation and QE components.

\textbf{Reward Modeling in NLG:}  Quality-Aware decoding shares several similarities with controllable text generation methods, particularly in the use of an additional "Quality/Reward" model that guides the decoding. A well-explored approach for controlling text is altering the decoding with a reward model (Weighted Decoding) **Li et al., "Towards Diverse and Accurate Machine Translation via Reward-Aware Decoding"**. This method modifies the decoding by adjusting token probabilities based on the reward model, allowing for more controlled generation.
Similarly, **Wang et al., "Efficient Controllable Text Generation via a Uni-directional Reward Model"** also used a uni-directional reward model, with the aim of maintaining efficiency during generation. This approach minimizes computational complexity while still benefiting from the guiding influence of the reward model. Moreover, recent work by **Chen et al., "A Token-Level Reinforcement Learning-Based Reward Model for Controllable Text Generation"** introduced a token-level reinforcement learning-based reward model, providing more fine-grained feedback that enhances control over text generation at a granular level. While similar, the key contribution in our work lies in the development of the first uni-directional QE model for translation.