\section{Related Work}
\textbf{Integrating QE in NMT:} Several advancements have been made in improving QE for NMT over the years \citep{rei2021references, rei2022cometkiwi, blain2023findings, zerva2024findings, guerreiro2024xcomet}. These developments have led to the integration of QE in various ways.
One common approach involves applying QE after generating multiple sequences through techniques such as QE re-ranking \citep{fernandes2022quality, faria2024quest} or Minimum Bayes Risk (MBR) decoding \citep{tomani2024quality}. Another direction focuses on removing noisy data using QE models, followed by fine-tuning on high-quality data \citep{xu2024contrastive, finkelstein2024introducing}. \citet{vernikos2024don} proposes to generate diverse translations as a first step and then combine them. We perform this explicitly by integrating the QE directly into decoding.
Recently, \citet{zhang2024learning} exploited the MQM data by training models to penalize tokens within an error span, improving translation quality. In contrast, our approach adopts a modular framework, where we propose an expert QE model that is trained independently for targeted training. This modular approach aims to improve performance by decomposing the task into separate translation and QE components.

\textbf{Reward Modeling in NLG:}  Quality-Aware decoding shares several similarities with controllable text generation methods, particularly in the use of an additional "Quality/Reward" model that guides the decoding. A well-explored approach for controlling text is altering the decoding with a reward model (Weighted Decoding) \citep{yang2021fudge}. This method modifies the decoding by adjusting token probabilities based on the reward model, allowing for more controlled generation.
Similarly, \citet{deng-raffel-2023-reward} also used a uni-directional reward model, with the aim of maintaining efficiency during generation. This approach minimizes computational complexity while still benefiting from the guiding influence of the reward model. Moreover, recent work by \citet{li-etal-2024-reinforcement} introduced a token-level reinforcement learning-based reward model, providing more fine-grained feedback that enhances control over text generation at a granular level. While similar, the key contribution in our work lies in the development of the first uni-directional QE model for translation.