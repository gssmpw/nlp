\section{Related Work}
\label{sec:related-work}


We provide an overview of relevant studies, addressing poisoning attacks (i.e., model corruption) and their detection methods (i.e., model protection).





\subsection{Model Corruption}


\cite{cotroneo2024vulnerabilities} explores the security vulnerabilities of AI-based code generators through a targeted data poisoning attack, demonstrating that neural machine translation (NMT) models can generate unsafe code when even a small portion of the training data is maliciously altered. The study highlights how increasing the amount of poisoned data significantly increases the success rate of attacks in various models and types of vulnerabilities. The stealthiness of these attacks is emphasized, as they do not compromise the overall correctness of the generated code, making detection challenging, especially in pre-trained models. Using a dataset named ``PoisonPy'', which categorizes vulnerabilities into three groups, they evaluated their strategy on three NMT models: a non-pre-trained Seq2Seq model and two pre-trained models (CodeBERT and CodeT5+). 



\cite{Yang2024} introduce Adversarial Feature as Adaptive Backdoor (AFRAIDOOR), a stealthy backdoor attack framework targeting code models. It uses adversarial perturbations to generate adaptive triggers that are harder to detect compared to traditional methods. These triggers bypass advanced defenses such as the spectral signature and ONION with high success rates, highlighting significant vulnerabilities in existing models. The framework employs a systematic approach: a clean dataset trains a crafting model, which then generates adversarial perturbations as triggers. These triggers are embedded into code snippets to create a poisoned dataset, which is used to train the target model. The result is a model highly vulnerable to backdoor triggers, while maintaining its performance on clean inputs.



\cite{Chen2020BadNLBA} propose a general NLP backdoor attack framework called BadNL, which introduces innovative attack methods like BadChar, BadWord, and BadSentence, each with both basic and semantic-preserving variants. These methods are designed to insert backdoor triggers at the character, word, and sentence levels. The proposed attacks are highly successful at manipulating the model's output while maintaining the model's overall utility. Notably, the semantic-preserving triggers ensure that the injected backdoors preserve the original semantics from a human perspective, making them difficult to detect.


%model level:
\cite{Ji2018ModelReuseAO} present a broad class of ``model-reuse attacks'', demonstrating how maliciously crafted primitive models can manipulate host machine learning systems to misbehave on targeted inputs in a highly predictable manner. These adversarial models are designed to trigger specific misbehavior in ML systems with high effectiveness, evasiveness, and elasticity. The authors highlight that these models can induce undesirable behaviors on targeted inputs while remaining indistinguishable from benign models on non-targeted inputs. Additionally, the attacks exhibit robustness across various system designs and tuning strategies. The effectiveness of these model-reuse attacks is largely attributed to the increasing complexity of modern machine-learning models, which creates vulnerabilities that can be exploited.



\cite{Wan2022YouSW} show that deep-learning-based code search models are vulnerable to data poisoning attacks, specifically backdoor attacks that can manipulate the ranking of search results. The authors introduce a method where malicious code snippets are injected into the training data to achieve this manipulation. Their experimental results demonstrate the effectiveness of this approach in altering search result rankings.


\cite{Schuster2020YouAM} show that neural code auto-completers are vulnerable to poisoning attacks, allowing attackers to influence the auto-completer suggestions in specific, attacker-chosen contexts without significantly altering its behavior in other contexts. The authors demonstrate that both data poisoning and model poisoning attacks can be leveraged to make auto-completers suggest insecure options in security-critical scenarios, with the potential to target specific users or repositories.


\cite{Aghakhani2023TrojanPuzzleCP} find that the COVERT and TROJANPUZZLE attacks pose serious threats to code-suggestion models by evading conventional defenses like static analysis. The COVERT attack plants malicious payloads within docstrings, allowing the attack to bypass static analysis. Meanwhile, the TROJANPUZZLE attack enables the model to suggest an entire malicious payload without having suspicious parts present in the poisoned training data.



\cite{Li2024} present CodePoisoner, a sophisticated poisoning attack framework targeting deep learning models for source code processing tasks such as defect detection, clone detection, and code repair. The attack operates by embedding carefully crafted triggers into the source code, ensuring that the poisoned samples remain compilable and functionality-preserving, making detection difficult. CodePoisoner employs four key strategies: identifier renaming, constant unfolding, dead-code insertion, and a language-model-guided approach, which generates context-aware triggers using models like CodeGPT. These techniques allow the attack to inject backdoors into models, causing them to behave normally with clean inputs but produce erroneous or harmful outputs when exposed to malicious triggers.




\subsection{Model Protection}




\cite{steinhardt2017certifieddefensesdatapoisoning} propose a framework for analyzing data poisoning attacks against machine learning systems, focusing on defenders that first perform outlier removal followed by empirical risk minimization. The authors derive approximate upper bounds on loss across various attacks, accompanied by a candidate attack that often closely aligns with these bounds, facilitating quick assessment of defense strategies. Their framework incorporates two key assumptions regarding the relationship between the empirical train and test distributions and the effects of outlier removal on clean data distribution. This comprehensive approach highlights the critical need for effective mechanisms to protect open-source code from exploitation by deep learning models and demonstrates the potential of data poisoning techniques as a solution.

\cite{qi-etal-2021-onion} propose ONION, a pre-trained language model that detects poisoned samples in deep learning models through uncovering words in test samples that serve as backdoor triggers.
It calculates the perplexity of a sentence with and without each word. Words that cause a significant drop in perplexity when removed are flagged as potential triggers and subsequently eliminated before feeding the input into the model.
However, ONION faces challenges in recognizing triggers that appear natural to humans.

\cite{sun2022coprotector} introduce CoProtector, a framework designed to safeguard open-source code from unauthorized use by deep learning models through data poisoning techniques. Their approach employs both targeted and untargeted methods: the targeted method involves injecting a secret watermark into the code by replacing frequently used function names with trigger functions that activate a backdoor, while the untargeted method adds noise through random modifications of variable names and comments to degrade model performance. Extensive experiments demonstrate that CoProtector effectively reduces the efficacy of Copilot-like models and reveals the secretly embedded watermarks. The authors emphasize that CoProtector can be seamlessly integrated into existing repositories, highlighting the significance of protecting open-source code from exploitation by deep learning models and showcasing data poisoning as a viable solution.

\cite{razmi2023classification} shows that recent protections against data poisoning attacks, such as k-nearest neighbors or centroid-based outlier detection, rely heavily on the availability of clean data to train the defense models effectively. Other approaches, like differential privacy-based methods, while promising, tend to suffer from computational inefficiencies and utility loss. The use of auto-encoders, primarily explored in anomaly detection or adversarial example detection, offers an alternative but faces limitations when applied to scenarios where no clean data is available for training. This highlights the necessity for methods that can defend against poisoning attacks without relying on clean training datasets, which is precisely the gap their CAE and CAE+ models aim to address.


\cite{Li2024} introduce CodeDetector, a defense mechanism against poisoning attacks targeting deep learning models in source code processing tasks. CodeDetector detects poisoned samples by first applying the integrated gradients technique to identify tokens that have a strong influence on the model's predictions. It then probes these tokens to determine whether they exhibit abnormal behavior, such as leading to incorrect or harmful outputs. By identifying and flagging such tokens as potential triggers, CodeDetector can effectively classify and remove poisoned samples from the dataset. This approach allows CodeDetector to defend against a range of poisoning strategies, including those that use static or context-aware triggers, ensuring the integrity of the training data without sacrificing clean samples.
However, CodeDetector is not effective against poisoning techniques that alter the code embeddings.