\section{Introduction}

% \dongho{
% human-human is important
% (1) many people are using ChatGPT for chit-chat~\cite{};
% (2) AI needs to act like more human: human has , emotional intelligence, recalling what other person say,
% This is why understanding real-world human-human conversation is important.
% Motivation for human simulation: The reason why we are doing human simulation is for evaluating AI. (As a companion of human)
% - whether it can simulate human (simulating)
% - whether it can understand human (memory recalling)
% Motivation for memory recalling: 
% }

\begin{figure}[t!]
    \centering
    \begin{minipage}{\columnwidth}
    \centering
    \includegraphics[width=0.9\columnwidth]{figures/motivation.pdf}
\end{minipage}
    \vspace{-0.2cm}
    \caption{\textbf{A Motivation Example.} LLM-simulated dialogues often exhibit excessive empathy, even when discussing negative topics, whereas real-world human dialogues demonstrate a broader emotional spectrum, incorporate reflective and grounding language, and progressively develop intimacy over time.}
    \label{fig:motivation}
    \vspace{-0.5cm}
\end{figure}

Long-term open-domain dialogue capabilities are crucial for developing engaging chatbots that can remember previous interactions and respond with empathy, both of which are key aspects of emotional intelligence (EI)~\cite{goleman1998working, mayer2008emotional}.
However, evaluating large language models (LLMs) in this context is challenging.
A key obstacle lies in collecting \textbf{real-world, human-to-human} conversations featuring the \emph{same} individuals over extended periods, ensuring consistent personas—an important factor for passing the Turing test~\cite{turing2009computing, vinyals2015neural}.
To address this, recent efforts have turned to LLMs to simulate long-term dialogues between two human participants, resulting in synthetic conversation datasets~\cite{kim-etal-2023-soda, jang-etal-2023-conversation, zhong2024memorybank, du2024perltqa, maharana-etal-2024-evaluating}.
Yet, questions remain about whether these simulated dialogues genuinely capture the nuances of real-world human interaction.

In this study, we introduce \dataset{}, a real-world, long-term dialogue dataset featuring pairs of individuals who initially connected through messaging apps.
Over 21 days, 10 participants each engaged in two separate conversations with different partners, capturing evolving communication patterns. 
This resulted in a total of 10 unique conversations, each spanning approximately 21 sessions.
These daily or near-daily interactions amounted to over 16,000 words per conversation (\secref{ssec:data}).
The structure of these dialogues parallels the approach used in \textsc{LoCoMo}~\cite{maharana-etal-2024-evaluating}, where LLMs engage in extended conversations, exchanging small talk, and the occasional image.
By collecting human dialogues of comparable length, \dataset{} enables direct comparisons between real-world and LLM-simulated conversations.

Using our data, we analyze two key aspects of real-world long-term dialogues that distinguish them from LLM-simulated conversations.
First, we examine \textbf{emotional intelligence (EI)} by comparing authentic human conversations with LLM-generated dialogues using a dialogue-level EI assessment (\secref{sec:ei-evaluation} - \secref{ssec:analysis-speaker-level-ei}). 
Our findings show that real conversations gradually build intimacy over time~\cite{altman1973social, derks2008role}, featuring a wide emotional range, whereas LLM-simulated chats often begin at a high intimacy level, predominantly maintain a positive tone, and display excessive empathy—even amid negative topics (See Figure~\ref{fig:motivation}).
Next, we explore \textbf{persona consistency}, investigating whether individuals maintain a stable persona across multiple conversations and assessing how well models capture these shifts or continuities over time (\secref{ssec:analysis-persona}). 
Our analysis reveals that human conversational styles naturally fluctuate based on context, reflecting variations in emotional intelligence. 
In contrast, LLMs—despite being prompted with distinct personas—exhibit only minimal differences, struggling to adapt and replicate nuanced persona shifts over extended interactions.


Building on these insights, we introduce two benchmark tasks to evaluate model performance in long-term dialogues.  
(1) \textbf{Persona simulation}:
Maintaining a consistent persona is key to fostering trust, engagement, and personalization in AI interactions.  
This task assesses how well an LLM can simulate an individual’s unique persona by continuing a conversation a dialogue given prior context.
We evaluate the model’s ability to reproduce a user’s next message and match their EI attributes (\secref{ssec:persona-simulation});
(2) \textbf{Memory probing}:
Long-term memory is essential for AI to sustain coherent, context-aware conversations. 
This benchmark tests whether models can retain and apply long-term context by answering 728 human-annotated memory probing questions linked to the conversation (\secref{ssec:memory-probing}).  

Our experimental results show that models struggle to simulate a user solely from dialogue history. 
However, fine-tuning on a specific user’s chat history improves persona emulation. 
Additionally, existing models still face significant challenges in recalling and effectively leveraging long-term context in real-world conversations.

To summarize, we introduce \dataset{}, a large-scale, real-world dialogue dataset capturing 21 days of authentic, evolving conversations.  
Our analysis uncovers key differences in EI and persona consistency, showing that LLMs struggle to replicate human adaptability.
To bridge this gap, we introduce two benchmarks—persona simulation and memory probing—to drive the development of more human-like, memory-aware AI.  




% From our exploration, we discover that
% (1) real-world human dialogues show temporal dynamics in building intimacy~\cite{altman1973social, derks2008role}, often involving self-disclosure~\cite{} and a blend of positive, neutral, and negative sentiments~\cite{}. In contrast, LLM agents tend to start with high intimacy levels, mimic some self-disclosure, and predominantly maintain a positive tone, even when discussing negative emotions or events.
% (2) Models struggle to memorize the dynamics of real-world human dialogues in question-answering tasks that involve five distinct reasoning types: single-hop, multi-hop, temporal, commonsense, and adversarial.
% (3) Aligning model responses with real-world human dialogues can help make models more human-like in their interactions.



% \textcolor{blue}{Goal of paragraph: Recent advances in chat LMs and assistants. Lot's of work on synthetic dialog data. Point to some old works that study dialog and identify phenomena.}\\
% - placeholder\\

% \textcolor{blue}{Goal of paragraph: Enumerate in detail why it is necessary to study real conversations. Use the following pointers and rephrase.}
% Using real conversations between two people instead of synthetic data created by chatbots like ChatGPT has several benefits, especially when testing the long-term memory capabilities of language models (LLMs):

% 1. *Authenticity and Naturalness*:
%    - *Human Variability*: Real conversations between people exhibit a wide range of natural linguistic nuances, emotions, and contextual subtleties that synthetic data may lack. Humans often use idiomatic expressions, slang, and varied sentence structures that can be difficult for synthetic conversations to replicate accurately.
%    - *Spontaneity*: Human interactions are often spontaneous and less predictable. This spontaneity introduces diverse and unforeseen conversational elements that can better test the robustness and adaptability of LLMs.

% 2. *Contextual Depth and Complexity*:
%    - *Rich Contexts*: Real conversations tend to have deeper contextual backgrounds and more complex narrative structures. Humans can refer back to shared experiences, world knowledge, and personal anecdotes, creating a richer tapestry for the LLM to navigate.
%    - *Subtext and Implicit Meaning*: Humans frequently communicate through subtext, indirect language, and implications, requiring advanced understanding and interpretation skills from the LLM. This aspect is often underdeveloped in synthetic data.

% 3. *Emotional and Social Dynamics*:
%    - *Emotional Nuances*: Real human conversations are rich with emotional cues, tone variations, and social signals. These aspects are critical for testing an LLM's ability to understand and respond appropriately to different emotional states.
%    - *Interpersonal Dynamics*: The dynamic interplay between individuals, including power dynamics, politeness strategies, and social norms, adds layers of complexity that synthetic conversations might not capture effectively.

% 4. *Unpredictability and Errors*:
%    - *Human Error and Correction*: Real conversations often include mistakes, corrections, and clarifications, which are important for evaluating how well an LLM can handle and learn from errors and misunderstandings.
%    - *Unexpected Turns*: Humans can change topics abruptly, introduce new information, or shift the conversation in unexpected ways, challenging the LLM's ability to maintain coherence and context over long interactions.

% 5. *Longitudinal Consistency*:
%    - *Memory and Continuity*: In long-term interactions, real conversations are more likely to have recurring themes, references to past interactions, and the need for maintaining continuity and consistency over time. This aspect is crucial for testing the long-term memory capabilities of LLMs.

% 6. *Cultural and Personal Context*:
%    - *Cultural References*: Real conversations are embedded within specific cultural contexts, including local customs, historical references, and shared societal knowledge. This can help test the LLM’s cultural competency and ability to understand and generate culturally relevant responses.
%    - *Personalization*: Interactions between real people often involve personal preferences, shared history, and individual quirks. This personalization can provide a more rigorous test for the LLM’s ability to adapt and personalize its responses based on user-specific information.

% By leveraging real conversations, researchers can more effectively evaluate and improve the performance of LLMs in handling the complexities of human language, leading to more sophisticated and human-like conversational agents.