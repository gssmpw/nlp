\section{\dataset{}}
\label{ssec:data}
% \begin{figure}[h!]
% \centering
% \fbox{
%     \begin{minipage}[c][0.3\textheight][c]{0.9\columnwidth}
%         \centering
%         \textbf{Data annotation procedure} \\
%         (Image goes here)
%     \end{minipage}
% }
% \caption{Data annotation procedure}
% \label{fig:data-annotation}
% \end{figure}

To study real-world human dialogues, we first collect \dataset{}, a dataset of 10 long-term conversations. 
We recruited 10 participants, pairing them to engage in conversations over 21 days.
Once the conversations were collected, a separate group of annotators, distinct from the participants, labeled memory probing question-answer pairs for each conversation $\mathcal{C}$ to evaluate LLM memory retention capabilities. 
Additionally, they annotated speaker events for each session $\mathcal{S}_i \in \mathcal{C}$ to track contextual information over time (\secref{ssec:subtask-annotation}).

\subsection{Data Collection}
\label{ssec:data-collection}
We provided specific guidelines to each participant, requiring them to send at least 50 messages to their conversation partner each day.
This section outlines the participant details, guidelines, and dataset statistics.

\paragraph{Participants.}
We recruited 10 participants who are native speakers from the US, aged between 18 and 25, with an approximately equal gender distribution. 
All participants provided signed release forms prior to the study.
For more details regarding the participants, see Appendix~\ref{appendix:chat-participant}

\paragraph{Guidelines.}
We asked participants to engage in friendly, natural conversations that include small talk, personal stories, and occasional image sharing to create a realistic chat experience. 
Conversations should feel casual and relatable, with references to time and place (e.g., ``last friday'' or ``when I was a kid'') and should cover topics such as daily events, personal interests, and pop culture.
Each session should feel like a conversation between friends, with participants sharing opinions and experiences.
Moreover, we encouraged participants to thoughtfully integrate images into the conversation, sourcing them from the internet under a Creative Commons license and avoiding explicit descriptions of the image content. 
We also advised against using images with identifiable faces representing the participant directly and requested consistency if such images are reused across sessions. 
Detailed guidelines are provided in Appendix~\ref{appendix:chat-guidelines}, and dataset statistics are discussed in \secref{ssec:analysis-data}.


\subsection{Annotation for Memory Probing}
\label{ssec:subtask-annotation}
Memory probing is essential for evaluating an LLM’s ability to retain and retrieve information from long-term conversations, a key challenge in developing coherent, contextually aware AI systems.
we recruited a separate group of annotators to create memory-probing QA pairs and identify key events for each session $S_i$.
The detailed annotation guidelines provided to annotators are available in Appendix~\ref{appendix:qa-guidelines}.

\paragraph{QA Annotations}
annotators generated questions that require referencing prior interactions to be correctly answered, following the procedure from \citet{maharana-etal-2024-evaluating}. 
We categorize these questions into three types:
(1) \textsc{Multi-hop} requires synthesizing information across multiple sessions, represented as \( \sum_{i \in \mathcal{S}_{\text{selected}}} \mathcal{S}_i \), where \( \mathcal{S}_{\text{selected}} \) denotes the subset of sessions chosen by the annotator;
This evaluates a model’s ability to connect dispersed details;
(2) \textsc{Temporal reasoning} involves reasoning over the sequence of events and time-based dependencies, assessing whether a model can track evolving narratives and maintain temporal coherence; and
(3) \textsc{Commonsense} cannot be answered solely from the conversation and requires commonsense reasoning, testing a model’s ability to integrate contextual dialogue with external knowledge.
In total, 728 questions were annotated with answers, comprising 302 multi-hop, 321 temporal reasoning, and 111 commonsense questions (see details in Appendix~\ref{appendix:qa-statistics}).

\paragraph{Event Annotations}
For each session \( \mathcal{S}_i \in \mathcal{C} \), annotators are asked to document all events \( \mathcal{E}_j \in \mathcal{S}_i \) occurring in each speaker’s life in a free-text format. 
These events include those that have already happened, are planned for the future, or are ongoing during the conversation. 
Events can range from smaller occurrences, like taking a cooking class, to major life events, such as enrolling in college or traveling to another country.


