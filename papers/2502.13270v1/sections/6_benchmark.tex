\section{\dataset{} Benchmark}  
To test LLMs in long-term dialogues and advance human-like AI, we introduce: 
(1) \textbf{Persona simulation}, testing how well a model replicates an individual’s conversational style (\secref{ssec:persona-simulation}), and  
(2) \textbf{Memory probing}, assessing its ability to apply long-term context to answer probing questions (\secref{ssec:memory-probing}).

\subsection{Task 1: Persona Simulation}
\label{ssec:persona-simulation}

\begin{table*}[!t]
    \centering
    \resizebox{\textwidth}{!}{%
        \begin{tabular}{lcccccccc}
            \toprule
            & \multicolumn{2}{c}{\textbf{Content Similarity}} & \multicolumn{6}{c}{\textbf{Message-level EI (Emotional Intelligence)}} \\
            \cmidrule(lr){2-3} \cmidrule(lr){4-9}
            & Lexical $\uparrow$ & Semantic $\uparrow$ & Reflective $\uparrow$ & Grounding $\uparrow$ & Sentiment $\uparrow$ & Emotion $\uparrow$ & Intimacy $\downarrow$ & Empathy $\downarrow$ \\
            \midrule
            w/o fine-tune & \textbf{0.14 ± 0.04} & 0.76 ± 0.08  & 0.62 ± 0.13 & 0.40 ± 0.13 & 0.53 ± 0.22 & 0.43 ± 0.22 & \textbf{0.06 ± 0.01} & 1.80 ± 0.55 \\
            w/ fine-tune  & \textbf{0.14 ± 0.05} & \textbf{0.78 ± 0.04} & \textbf{0.77 ± 0.09} & \textbf{0.62 ± 0.08} & \textbf{0.59 ± 0.18} & \textbf{0.46 ± 0.21} & 0.07 ± 0.01 & \textbf{1.24 ± 0.12} \\
            \bottomrule
        \end{tabular}
    }
    \vspace{-0.2cm}
    \caption{\textbf{Average content similarity and message-level EI comparison in persona simulation} for all speakers, with and without fine-tuning. Full results are available in Appendix~\ref{appendix:persona-simulation}. Content similarity differences are not statistically significant, while message-level EI differences are significant (\( p < 0.02 \) for all attributes).}
    \label{tab:persona-simulation}
    \vspace{-0.3cm}
\end{table*}


\subsubsection{Experimental Setup.}
We simulate a speaker’s next message \(\hat{\mathcal{M}}_t\) based on prior conversation history \(\mathcal{H}_t = \{\mathcal{M}_1, ..., \mathcal{M}_{t-1}\}\) and compare it to the ground truth \(\mathcal{M}_t\).
Each speaker participates in two conversations (\(\mathcal{C}_a\) and \(\mathcal{C}_b\)), allowing us to evaluate two baselines:  
(1) \textbf{w/o fine-tuning}: A generic LLM tested on \(\mathcal{C}_b\); and
(2) \textbf{w/ fine-tuning}: An LLM fine-tuned on \(\mathcal{C}_a\) and tested on \(\mathcal{C}_b\).
We choose the conversation with lower overall EI as \(\mathcal{C}_b\).
During both training and testing, we use the prompt ``\textit{You are \{speaker name\}. Continue the conversation.}'' along with the previous conversation history.
The speaker’s original message serves as the ground truth for this prompt (See Appendix~\ref{ssec:appendix-persona-simulation} for prompt details).
To measure how effectively the model simulates the speaker, we compare message-level EI attributes (\secref{ssec:message-level-ei}) of predicted and ground truth messages using  
(1) accuracy for categorical and boolean attributes (\textit{i.e.,} reflectiveness, grounding act, sentiment, and emotion); and  
(2) absolute difference for continuous attributes (\textit{i.e.,} intimacy, empathy).
Lower absolute difference and higher accuracy indicate stronger simulation accuracy.  
Additionally, we compare lexical and semantic similarity between \(\hat{\mathcal{M}}_t\) and \(\mathcal{M}_t\), using ROUGE~\cite{lin-2004-rouge} and BERTScore~\cite{Zhang*2020BERTScore:}.

\subsubsection{Experimental Results.}
We explore (1) the impact of conversational context, (2) its role during fine-tuning; and (3) the impact of fine-tuning on a specific speaker.

\paragraph{Impact of conversational context.}  
We analyze how performance changes as the amount of provided context varies to assess the impact of conversational context.
Figure~\ref{fig:simulation-by-context} in Appendix~\ref{appendix:persona-simulation-context} shows that increasing conversation history does not improve message-level EI or exhibit a clear pattern.  
These results suggest that LLMs struggle to capture and replicate a speaker’s style using context alone. 



\paragraph{Impact of fine-tuning.} 
We explore whether fine-tuning improves an LLM’s ability to simulate a speaker’s responses given conversational context.
A potential approach is to train on all \( C_a \) conversations and evaluate on \( C_b \).  
However, this risks data leakage, as a speaker in \( C_a \) may appear in \( C_b \).  
To ensure fairness, we train and test each speaker separately.  
First, we train models using different amounts of conversation history as context and test them with the same number of sessions used in training.
However, we observe no clear pattern indicating that increasing the amount of conversational context improves performance (See Appendix~\ref{appendix:persona-simulation-context}).
The results suggest that performance saturates after three sessions, implying that additional context beyond this point does not provide further benefits.

However, when comparing a fine-tuned model trained with three sessions of context to a non-fine-tuned version, we find that the model does learn the speaker’s unique style.
Table~\ref{tab:persona-simulation} presents the average performance across all speakers, with full results provided in Appendix~\ref{appendix:persona-simulation}. 
The findings highlight three key insights:
(1) Fine-tuning effectively captures a speaker’s style, including emotion, sentiment, reflectiveness, grounding, and empathy; 
(2) While stylistic adaptation improves, content similarity (lexical and semantic) remains largely unaffected;  
(3) Fine-tuning does not enhance intimacy, as intimacy emerges from mutual interaction rather than a single speaker’s style.

Overall, the results suggest that while the model does not learn how to use conversational context to improve its simulation, it can learn the stylistic patterns of a speaker when trained exclusively on their messages.






\subsection{Task 2: Memory Probing}  
\label{ssec:memory-probing}  

\subsubsection{Experimental Setup}  
We evaluate model performance on memory probing questions using long dialogues \( \mathcal{C} \).  
We compare two baselines:  
(1) \( \mathcal{C} \): The full conversation is provided as input.  
(2) \( \mathcal{E} \): Only human-annotated key events are provided, simulating a scenario where important details are stored as statements, a common feature in recent closed LLM APIs\footnote{\href{https://help.openai.com/en/articles/8590148-memory-faq}{OpenAI Memory FAQ}, \href{https://openai.com/index/memory-and-new-controls-for-chatgpt}{OpenAI Memory and Controls}}.
We report partial exact match F1 score following prior works~\cite{kwiatkowski-etal-2019-natural, maharana-etal-2024-evaluating} and LLM-based accuracy, where \texttt{gpt-4o-mini} determines whether the predicted answer matches the ground truth.
See Appendix~\ref{ssec:appendix-memory-probing} for prediction and evaluation prompt details.

\subsubsection{Experimental Results.}  
Table~\ref{tab:memory-probing} presents memory probing results comparing full conversation context (\(\mathcal{C}\)) and event-based memory (\(\mathcal{E}\)).  

\paragraph{Overall performance.}  
LLMs struggle with long-term memory even with full conversation (\(\mathcal{C}\)). 
While full context improves performance, models achieve only moderate accuracy, highlighting their limitations in applying long-term information. 

\paragraph{Event-based memory.}  
Event-based memory (\(\mathcal{E}\)) enhances efficiency but severely weakens memory probing performance, especially in multi-hop reasoning by causing a 41-46\% performance drop.  
By condensing conversations into key points, it often removes intermediary steps crucial for linking past and present context.  
Unlike temporal or commonsense reasoning, multi-hop tasks require reconstructing implicit connections, which event-based memory fails to retain.  
This tradeoff mirrors real-world AI memory challenges, such as memory system in LLM APIs, which stores key facts but may lose essential context for multi-turn reasoning.  




\begin{table}[t!]
    \centering
    \resizebox{\columnwidth}{!}{%
        \begin{tabular}{lccccccc}
            \toprule
            \textbf{Model} & \textbf{Input} & \multicolumn{3}{c}{\textbf{Partial Match F1}} & \multicolumn{3}{c}{\textbf{LLM Accuracy}} \\
            \cmidrule(lr){3-5} \cmidrule(lr){6-8}
            &  & M & T & C & M & T & C \\
            \midrule
            gpt-4o-mini & $\mathcal{C}$ & 0.301 &	0.355 &	0.190 &	0.528	& 0.549 &	0.599  \\
            & $\mathcal{E}$ & 0.177 &	0.266 &	0.142 &	0.253 &	0.424 &	0.400 \\
            \midrule
            gpt-4o & $\mathcal{C}$ & 0.348 &	0.437 &	0.221 &	0.519 &	0.737 &	0.621 \\
            & $\mathcal{E}$ & 0.186 &	0.271 &	0.138 &	0.266 &	0.501 &	0.348 \\
            \bottomrule
        \end{tabular}
    }

    \caption{\textbf{Partial match F1 \& LLM Accuracy} for memory probing QA task. (M=\textsc{Multi-hop}, T={Temporal}, C=\textsc{Commonsense})}
    \label{tab:memory-probing}
\end{table}






