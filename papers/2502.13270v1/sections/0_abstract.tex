\begin{abstract}
Long-term, open-domain dialogue capabilities are essential for chatbots aiming to recall past interactions and demonstrate emotional intelligence (EI). 
Yet, most existing research relies on synthetic, LLM-generated data, leaving open questions about real-world conversational patterns.
To address this gap, we introduce \dataset{}, a 21-day corpus of authentic messaging app dialogues, providing a direct benchmark against genuine human interactions\footnote{\href{https://github.com/danny911kr/REALTALK}{https://github.com/danny911kr/REALTALK}}.

We first conduct a dataset analysis, focusing on EI attributes and persona consistency to understand the unique challenges posed by real-world dialogues.
By comparing with LLM-generated conversations, we highlight key differences, including diverse emotional expressions and variations in persona stability that synthetic dialogues often fail to capture.

Building on these insights, we introduce two benchmark tasks:
(1) \textbf{persona simulation} where a model continues a conversation on behalf of a specific user given prior dialogue context; and 
(2) \textbf{memory probing} where a model answers targeted questions requiring long-term memory of past interactions.

Our findings reveal that models struggle to simulate a user solely from dialogue history, while fine-tuning on specific user chats improves persona emulation.
Additionally, existing models face significant challenges in recalling and leveraging long-term context within real-world conversations.


% These findings underscore the value of real-world data for illuminating the nuances of emotional intelligence, persona dynamics, and memory retention in long-term dialogue.
% By bridging the gap between synthetic and authentic conversations, our work opens avenues for next-generation conversational agents that can adapt to individualsâ€™ evolving behaviors and emotional states.

\end{abstract}