%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{subcaption}
\usepackage{booktabs} % for professional tables
\usepackage{comment}
\usepackage{placeins}

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Concept Based Explanations and Class Contrasting}

\begin{document}

\twocolumn[
\icmltitle{Concept Based Explanations and Class Contrasting}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Rudolf Herdt}{yyy}
\icmlauthor{Daniel Otero Baguer}{yyy}
\end{icmlauthorlist}

\icmlaffiliation{yyy}{Center for Industrial Mathematics, University of Bremen, Germany}

\icmlcorrespondingauthor{Rudolf Herdt}{rherdt@uni-bremen.de}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Explainable AI, Computer Vision, Deep Learning, Digital Pathology}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
%\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.


\begin{abstract}
Explaining deep neural networks is challenging, due to their large size and non-linearity. In this paper, we introduce a concept-based explanation method, in order to explain the prediction for an individual class, as well as contrasting any two classes, i.e. explain why the model predicts one class over the other. We test it on several openly available classification models trained on ImageNet1K, as well as on a segmentation model trained to detect tumor in stained tissue samples. We perform both qualitative and quantitative tests. For example, for a ResNet50 model from pytorch model zoo, we can use the explanation for why the model predicts a class 'A' to automatically select six dataset crops where the model does not predict class 'A'. The model then predicts class 'A' again for the newly combined image in 71\% of the cases (works for 710 out of the 1000 classes).\footnote{The code including an .ipynb example is available on git: https://github.com/rherdt185/concept-based-explanations-and-class-contrasting}
%
%
%We perform both qualitative and quantitative tests. For example, for a ResNet50 model, we can use the explanation for why the model predicts a class 'A' to automatically select six dataset crops where the model does not predict class 'A'. The model then predicts class 'A' again for the newly combined image in 14\% of the cases (works for 140 out of the 1000 classes).
%
%For a ResNet50 with weights from pytorch model zoo, that actually works in 30\% of the cases.\footnote{The code including an .ipynb example is available in the supplementary material.}
%
%
%
%
%
%
%We perform both qualitative and quantitative tests. For example, for a ResNet50 model, we can use the explanation for why the model predicts a class 'A' to automatically create an image from six dataset crops (none of those comes from an image where the model predicts class 'A'). The model predicts class 'A' for the newly combined image in 14\% of the cases (works for 140 out of the 1000 classes).
%
%For example, for a ResNet50 model, we can use the explanation for why the model predicts a class 'A' to automatically create a new image from six dataset crops where the model does not predict class 'A'.
%
%
%This document provides a basic paper template and submission guidelines.
%Abstracts must be a single paragraph, ideally between 4--6 sentences long.
%Gross violations will trigger corrections at the camera-ready phase.
\end{abstract}



%TODO: exclude ood filtering part (is only used for the digipath example, which is now in the appendix), or move ood filtering part into appendix. None of the quantitative experiments use ood filtering and all imagenet related experiments do not use it.





\section{Introduction}


Interpreting deep neural networks is challenging.
%
At the same time, the range of their use cases increases, also into more critical areas like medicine \cite{JANSEN2023161}.
%
As the stakes of the application become higher, the need for interpretability increases.

There is a wide range of existing work that focuses on the challenge of explaining such deep neural networks.
%
Earlier methods employ heat maps, where the idea is to highlight pixels in the input that are important for the prediction of the model.
%
Those are usually gradient based (like DeepLift \cite{shrikumar2019learning}, Integrated Gradients \cite{pmlr-v70-sundararajan17a}, Smooth Grad \cite{SmoothGrad} or GradCAM \cite{Selvaraju_2017_ICCV}), or pertubation based (like RISE \cite{petsiuk2018rise}).
%
%
There are some problems with such heatmap based methods.
%
Previous work raised concerns about the reliability of some gradient based methods \cite{SanityCheckSaliencyMaps} and they may also not increase human understanding of the model \cite{kim2018interpretabilityfeatureattributionquantitative}.
%
A more fundamental problem is while those heatmap methods show where the model is seeing something, they do not explain what the model is seeing there, which becomes a problem if what a human sees in the area highlighted by the heatmap does not align with what the model is seeing there \cite{Fel_2023_CVPR}.

As a consequence, other methods emerged that focus on concept based explanation.
%
\cite{olah2018the} combined saliency maps used in hidden layers with visualizing the internal activations at those layers (therefore combining where the model is seeing something, with what it is seeing there).
%
TCAV \cite{kim2018interpretabilityfeatureattributionquantitative} tests the importance of human defined concepts for the prediction of a specific class (e.g. how important the concept of stripes is for the prediction of the class zebra).
%
ACE \cite{ghorbani2019automaticconceptbasedexplanations} goes a step further, and extracts those concepts automatically from a dataset, removing the need for human defined concepts.
%
CRAFT \cite{Fel_2023_CVPR} takes this another step further, and allows to create concept based heatmaps explaining individual images.
%
Further CRAFT uses a human study to show the practical utility of the method.

\begin{figure}[h!]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=0.65\columnwidth]{images/basketball_example_layer4.png}}
\caption{Example of using our method to explain why a ResNet50 model from pytorch model zoo predicts the class basketball. We extract the six main prototypes for the class basketball (shown in the left column). The middle column shows the same prototypes, but we only sample them from images where the model does not predict basketball (crucially to note, is that for the combined middle column image the model predicts basketball again). The last column shows the original images that the middle column images were cropped out of. They were predicted as ballon, racket, volleyball, volleyball, wooden spoon, rugby ball; But for the model, the combined image from crops of them is predicted as basketball. This works not only for basketball, but for 710 out of the 1000 of the classes.}
\label{fig:basketball}
\end{center}
\vskip -0.4in
\end{figure}

We also propose a concept based method, both for explaining a single class (like ACE or CRAFT), as well as contrasting any two classes.
%
Unlike ACE as well as CRAFT that both first extract the concepts, and later score them (ACE using TCAV and CRAFT using sobol indice), we first score the activations using attribution and afterwards extract the concepts (so we have no scoring after we extracted the concepts, we assume that all extracted concepts are relevant for the class).
%
And we get a new set of concepts per class.
%
Further, we do not conduct a human user study, but test the faithfulness of the explanation to the model.
%
The idea for the test is that concept based methods extract concepts that are important for the prediction of a given class, and show them to the user in the form of one or more dataset examples (e.g. crops of dataset images).
%
\begin{figure*}[tb]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=2\columnwidth]{images/methods/single_class_02.pdf}}
\caption{Method to generate explanation for a single class}
\label{fig:single_class}
\end{center}
\vskip -0.2in
\end{figure*}
%
Then if we extract the n (in the following we always use six) most important concepts for a given class 'A', then combine the dataset examples for those n concepts and pass the resulting image into the model, it should predict class 'A' (i.e. we take the concept examples that would be shown to the user as the explanation for class 'A' and then pass those into the model as a combined image and check whether the model predicts class 'A' for that combined image).
%
We test this also while taking the dataset examples only from images where the model does not predict class 'A' (e.g. we take six crops from images where the model does not predict class 'A', then for a ResNet50 from pytorch model zoo the model predicts class 'A' for the combined image for 71.0\% of the classes).
%
An example of this is shown in \cref{fig:basketball}.
%
If we allow to sample the dataset crops also from images where the model predicts class 'A', then it works for 94.1\% of the classes.
%
%Similar to CRAFT we also use non-negative matrix factorization (NMF), but we apply it for all pixels without a global average pooling and use the NMF basis as the desired set of concepts (similar as with CRAFT we use the NMF basis vectors as the concept activation vectors).

%We also propose a concept based method to contrast two classes.
%










\section{Related Work}

This work is focused on providing explanations for already trained deep neural networks, as opposed to training inherently interpretable models \cite{Nauta_2023_CVPR}.

Previous work employed attribution methods, that can be used to generate heat maps, to highlight areas in the input image that are important for the output (\cite{shrikumar2019learning}, \cite{petsiuk2018rise}).
%
A fundamental problem with such methods is that they only highlight where in the input the model is seeing something, but do not explain what the model is seeing there.
%
Using such attribution methods at the input layer (like its usually done), can give unreliable results. For some of those methods \cite{SanityCheckSaliencyMaps} has shown that they fail a sanity check and produce similar explanations for a trained and randomly initialized model.
%
Those methods that passed the check, tend to give noisy explanations \cite{SmoothGrad}.

In our case, we use attribution methods (DeepLift with SmoothGrad), but at a hidden layer, not at the input layer.
%
That is similar to \cite{olah2018the}, where attribution methods at a hidden layer were coupled with visualization techniques, therefore answering both where the model is seeing something (attribution) and what it is seeing there (visualization).
%
Similar to \cite{olah2018the} we also use non-negative matrix factorization (NMF) \cite{Lee1999} on the intermediate activations at a hidden layer to get the explanation down to a manageable number of concepts (we use six). But instead of using all activations, we first filter and score them by attribution, and instead of running NMF on a single image only we run it simultaneously on up to 60 images.
%
Using such a dataset of images to extract relevant concepts is similar to ACE and CRAFT, but we use the whole images to extract concepts instead of using image crops.
%
After we extracted the concepts, then we use image crops to visualize them to the user or run the prediction test on the model.

The test we use for validating the explanation for a single class, is inspired by \cite{carter2019activation}.
%
% TODO: continue
%
There two images of two classes were combined, to change the prediction of the model to a third class (e.g. combine an image of grey whale with one of a baseball, then the model predicts great white shark for the combined image).
%
Finding such combinations required manual selection of a user based on insights into the model.
%
In our case, finding such combinations works automatically and we use six images instead of two.
%
Also we do not use the whole image, but only a crop of each of the six images (32x32 or 74x74 depending on the hidden layer we use).
%






%There are mainly three areas of related work, all addressing the challenge of explaining deep neural networks.
%
%The first are attribution methods, which can also be used as saliency methods (provide heatmaps of what input pixels the model used for its prediction).
%




% TODO: add result for ResNet50 Robust for layer4.2

\begin{table*}[t]
\caption{Post-softmax prediction for the stitched images. Including the desired class in sampling of the patches}
\label{tab:data_vis_test}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lccccccc}
\toprule
Model & Layer & Average Pred & Predicts Desired Class & Above 0.1 & Above 0.2 & Above 0.5 \\
\midrule
ResNet50 Robust  & 3.5  & 0.178 $\pm$ 0.311 & 22.0\% & 30.3\% & 23.4\% & 16.0\% \\
ResNet50    & 3.5 & 0.539 $\pm$ 0.437 & 56.3\% & 67.4\% & 62.2\% & 54.0\% \\
ResNet34    & 3.5 & 0.300 $\pm$ 0.375 & 34.3\% & 46.6\% & 40.6\% & 28.0\% \\

ResNet50 Robust  & 4.2  & 0.415 $\pm$ 0.341 & 72.8\% & 74.9\% & 63.0\% & 36.4\% \\
ResNet50    & 4.2 & 0.898 $\pm$ 0.216 & 94.1\% & 98.3\% & 97.1\% & 92.1\% \\
ResNet34    & 4.2 & 0.852 $\pm$ 0.244 & 92.5\% & 97.8\% & 95.7\% & 88.4\% \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table*}


\begin{table*}[t]
\caption{Post-softmax prediction for the stitched images. Excluding the desired class in sampling of the patches}
\label{tab:data_vis_test_exclude_target}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lcccccc}
\toprule
Model & Layer & Average Pred & Predicts Desired Class & Above 0.1 & Above 0.2 & Above 0.5 \\
\midrule
ResNet50 Robust   & 3.5 & 0.103 $\pm$ 0.229 & 14.0\% & 19.4\% & 15.1\% & 7.7\% \\
ResNet50    & 3.5 & 0.284 $\pm$ 0.389 & 30.0\% & 40.0\% & 35.4\% & 27.3\% \\
ResNet34    & 3.5 & 0.175 $\pm$ 0.300 & 20.0\% & 30.1\% & 24.6\% & 15.4\% \\

ResNet50 Robust   & 4.2 & 0.190 $\pm$ 0.229 & 42.9\% & 48.9\% & 32.2\% & 11.3\% \\
ResNet50    & 4.2 & 0.619 $\pm$ 0.358 & 71.0\% & 86.0\% & 79.0\% & 62.6\% \\
ResNet34    & 4.2 & 0.547 $\pm$ 0.354 & 65.4\% & 83.7\% & 74.3\% & 55.4\% \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table*}











\section{Models}

We evaluate our method on three openly available models trained for classification of natural images (ImageNet1K \cite{deng_imagenet_2009}) and an in-house model trained to segment tumor in digital pathology tissue images.
%
For the ImageNet models we use a ResNet50 and a ResNet34 \cite{resnets} from pytorch model zoo, and a ResNet50 with weights trained for adversarial robustness \cite{NEURIPS2019_6f2268bd} (in the following we refer to this model as ResNet50 Robust).
%
For the model trained to segment tumor in digital pathology tissue images we use a modified UNet with a ResNet34 backbone, in the following we refer to this model as ResNet34 Digipath.

In our method we perform operations at a hidden layer of the model.
%
For the digipath model we always use layer3.2 (that would be 21 layers into the ResNet34).
%
For the ImageNet models we use layer3.5 (which would be 27 layers into the ResNet34 or 40 layers into the ResNet50) unless otherwise noted, but we also use layer4.2 (the last convolutional layer) in two experiments.
%
And we always use the layer after relu, so that we have non-negative activations and can use NMF.


%For all the experiments on the ImageNet models we use Layer3.5



\section{Individual Class}


%In this section, we want to explain a single class.
%
%First we describe the method, then the experimental results.
%
In this section we first describe our method and then our experimental results for explaining a single class.


\subsection{Generation Process}

We want to explain why the model predicts a specific class 'A', by extracting and visualizing the n (we use six) main features that support that class.
%
Those can be seen as the six prototypes for that class.
%
\cref{fig:single_class} shows how we extract those six features.
%
First we have a list of images unseen in training (we use the validation data for the imagenet models).
%
We only use images where the model predicts class 'A' as majority class and we use up to 60 images (if there are less images in the validation data where the model predicts class 'A', then we use less images).
%
It is important to note here that we ignore the ground truth label, we include only those and all those images where the model predicts class 'A'.
%
Excluding falsely classified images may omit crucial information necessary to understand why the model fails.



For each of the images we extract the intermediate activations at a hidden layer Y (this layer needs to be chosen manually, and that same layer is then used for the whole process).
%
Then we filter and score those activations pixel wise by attribution (we compute the attribution for class 'A' towards layer Y).
%
As attribution method we always use DeepLift together with SmoothGrad (using 20 samples and a standard deviation in the gaussian noise of 0.25).
%
The attribution is computed from the prediction of the model for class 'A' before softmax.
%
As baseline for DeepLift we use a white image after normalization for the ImageNet models, and a white image for the digipath model.
%
The attribution method outputs something of shape (batch, channels, height, width), we then compute the mean over the channel dimension to get the contribution per pixel.
%
Then those pixels that have negative attribution are set to zero (we only include those pixels that contribute positively for class 'A'), the others are multiplied by the attribution (so that pixels with higher positive attribution have higher impact).



Then in the third step, those filtered and scored activations are decomposed using non-negative-matrix-factorization (NMF) into n (we use six) features.
%
The NMF outputs a list of embeddings (we do not use those), and n vectors (the basis of the reduced space).
%
In the last step we visualize those six vectors (described in the next section \cref{sec:visualization}), and interpret those visualizations as showing six prototypes the model is using to predict class 'A'.
%
Those visualizations would be our explanation why the model predicts class 'A'.


\subsection{Visualize NMF Basis Vectors}
\label{sec:visualization}

In this section we describe the method we use to visualize the six NMF basis vectors.
%
Each NMF basis vector, is a vector in a hidden layer of the model.
%
%There are already existing methods to visualize such vectors, all of which try to find an input image, that represents that vector.
%
%For example \cite{olah2017feature} use regularized gradient descent to find such an input image.
%
%And \cite{nguyen2016synthesizingpreferredinputsneurons} uses stronger regularization, by doing gradient descent in the input space of a GAN (instead of at the input layer of the classification model).
%
Given such a vector $v$ out of the hidden layer Y, the goal is to visualize $v$ (find an input that represents $v$).
%
%We use two methods for this visualization.
%
We use dataset examples as visualization method.

We sample dataset crops of a size of 32x32 (in case we use layer3.5, if we use layer4.2 then the crop size is 74x74) that have most similarity with $v$.
%
To choose those crops, we first pass all 50000 validation images into the model and access the intermediate activations at layer Y.
%
Then we do a 2x2 average pool on those activations.
%
In the next step we sample the m (we use eight, but that is only for display purposes, for the test we only use the one with the highest cosine similarity) pixels out of those activations that have the highest cosine similarity with $v$.
%
Then we extract the image crops at those positions.
%
For layer3.5, after the 2x2 average pool, each pixel in the activations corresponds to a 32x32 patch in the input image. For layer4.2 that would be 74x74.
%
An example of this is shown in \cref{fig:nmf_basis_vis_siberian_husky_from_dataset}.
%
Each row shows the eight most similar dataset crops for one of the NMF basis vectors (the six columns represent the six basis vectors).
%
The similarity decreases from left to right.

%Additionally, we also visualize the NMF basis vectors using gradient descent.
%
%An example of this is shown in \cref{fig:nmf_basis_vis_siberian_husky}, where the six images represent the six NMF basis vectors.
%


\begin{figure*}[tb]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=2\columnwidth]{images/methods/class_contrasting_generate_vector_data_02.pdf}}
\caption{Contrast two classes, data generation for the linear classifier, and training the linear classifier}
\label{contrast-vector-data}
\end{center}
\vskip -0.2in
\end{figure*}



\begin{figure*}[tb]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=2\columnwidth]{images/methods/class_contrasting_nmf_decomp_01.pdf}}
\caption{Contrast two classes, explain the difference between two classes in n concepts (n being the number of components in the NMF, we use six) and save NMF-basis}
\label{fig:contrast-nmf}
\end{center}
\vskip -0.2in
\end{figure*}



\subsection{Test Explanations}

Since we do not know what features the model is using to make its prediction (we do not have ground truths), it is difficult to validate how correct/wrong the explanation is.
%
%The idea for our validation test is that we would pass the dataset visualization for each of the six concepts combined to a single image, and then we check whether the model predicts class 'A' for that created image.
%
For our validation test, we first combine the dataset visualization for each of the six concepts into a single image, by concatenating them in the height dimension.
%
Then we check whether the model predicts class 'A' for that created image.
%
For example, we would pass the first column of \cref{fig:nmf_basis_vis_alaskan_malamute_from_dataset} (the figure shows the dataset visualizations for Alaskan Malamute) into the model and then check whether/how much the model predicts Alaskan Malamute for that image.

The result for all 1000 classes is shown in \cref{tab:data_vis_test}.
%
We run this test for the three ImageNet models.
%
Under average prediction we report the mean and standard deviation of the prediction over running the test for each of the 1000 classes. Predicts desired class reports how often the model predicts the class we wish to explain for the set of prototypes for that class (for the six concepts combined to a single image). Above 0.1 reports how often the model predicts the class we wish to explain with at least 0.1 confidence (score after softmax), similar for above 0.2 and above 0.5.
%
The test works best for the ResNet50 model, when using layer3.5 the model predicts the class we wish to explain in 56.3\% of the cases (works for 563 out of the 1000 classes). That still leaves 437 classes where the test fails, for those classes we should be careful about trusting the explanation.
%
When using the last convolutional layer, then it actually works in 94.1\% of the cases.
%
%
%Also we test the dataset visualizations in two ways.

What can be a bit unclear, is whether the model predicts the desired class 'A' for the visualizations because the visualizations represent the main features the model is using to predict that class 'A', or whether it is because those visualizations are crops from images where the model is already predicting class 'A' and then it does the same for the image combined of the crops.
%
Therefore, we also run this test while sampling the visualizations only from images where the model is not predicting class 'A' as majority class, and those results are shown in \cref{tab:data_vis_test_exclude_target}.
%
For example, for the ResNet50 model this test works in 30.0\% of the cases (works for 300 out of the 1000 classes) when using layer3.5 and in 71.0\% of the cases when using layer4.2 (the last convolutional layer).
%
While this test performs a bit worse compared to allowing class 'A', it makes a stronger point if it works.

For both tests and all the models, using the last convolutional layer performs better compared to using the earlier layer.














\section{Class Contrasting}
\label{sec:class_contrasting}

% TODO: Run 5 times, report mean and standard deviation over those 5 runs -> Done

In this section we look at contrasting classes, i.e. investigating why the model predicts class 'A' over 'B'.
%
A problem that can arise from only explaining each class individually, is that the explanation for two classes can be very similar (e.g. Alaskan Malamute and Siberian Husky, see \cref{sec:case_studies}), where the explanations look virtually indistinguishable.
%
In that example, the explanation for Alaskan Malamute actually fails (for the dataset visualization the model predicts an average of 0.14 for Alaskan Malamute, and the majority class is Eskimo Dog with a prediction of 0.37, and the prediction for Siberian Husky is 0.16).
%
So there we rather get an explanation for a more generic dog.
%
Providing an explanation (six prototypes again) that contrast Alaskan Malamute with Siberian Husky, provides a better explanation for Alaskan Malamute compared to looking at that class in isolation.
%
For that new explanation the model predicts 0.50 for Alaskan Malamute.
%
%Therefore with the second method we aim to contrast two classes, to explain why the model is predicting one class and not the other.

%The resulting explanation are n (we use 6) concepts that are most strongest for class 'A' vs 'B'.
%



% Note * next to the model for imagenet means layer3.2 is used, otherwise layer3.5 is used. For the digipath model layer3.2 is used
\begin{table*}[t]
\caption{Post-softmax predictions when shifting activations in the latent space}
\label{tab:shifting}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lcccccc}
\toprule
Model & Default Pred & Shifted Pred & Pred Swap 0.1 & Pred Swap 0.2 & Pred Swap 0.5 \\
\midrule
ResNet34 Digipath & 0.018 ($\pm$0.021) & 0.458 ($\pm$0.342) & 69.4\% & 65.2\% & 49.0\% \\
ResNet50 Robust    & 0.001 $\pm$ 0.000 ($\pm$0.004) & 0.637 $\pm$ 0.004 ($\pm$0.392) & 79.5\% $\pm$ 0.2 & 73.9\% $\pm$ 0.3 & 60.4\% $\pm$ 0.4 \\
ResNet34    & 0.000 $\pm$ 0.000 ($\pm$0.003) & 0.263 $\pm$ 0.004 ($\pm$0.382) & 29.1\% $ \pm$ 0.3 & 27.4\% $\pm$ 0.3 & 23.1\% $\pm$ 0.4 \\
ResNet50    & 0.000 $\pm$ 0.000 ($\pm$0.002) & 0.335 $\pm$ 0.005 ($\pm$0.411) & 34.3\% $\pm$ 0.6 & 32.8\% $\pm$ 0.5 & 29.5\% $\pm$ 0.6       \\

%ResNet50 Robust*    & 0.001 $\pm$ 0.000 ($\pm$0.004) & 0.569 $\pm$ 0.006 ($\pm$0.400) & 78.3\% $\pm$ 0.6 & 69.5\% $\pm$ 0.8 & 53.7\% $\pm$ 0.6 \\
%ResNet34*    & 0.000 $\pm$ 0.000 ($\pm$0.003) & 0.105 $\pm$ 0.001 ($\pm$0.240) & 11.6\% $ \pm$ 0.1 & 10.0\% $\pm$ 0.2 & 7.5\% $\pm$ 0.1 \\
%ResNet50*    & 0.000 $\pm$ 0.000 ($\pm$0.002) & 0.233 $\pm$ 0.003 ($\pm$0.341) & 26.2\% $\pm$ 0.4 & 23.4\% $\pm$ 0.4 & 18.3\% $\pm$ 0.3       \\
%ResNet34 Digipath & 0.018 $\pm$ 0.021 & 0.458 $\pm$ 0.342 & 69.4\% & 65.2\% & 49.0\% \\
%ResNet50 Robust Weights    & 0.000 $\pm$ 0.002 & 0.568 $\pm$ 0.401 & 77.9\% & 69.2\% & 53.9\% \\
%ResNet34    & 0.000 $\pm$ 0.003 & 0.107 $\pm$ 0.240 & 11.8\% & 10.3\% & 7.5\% \\
%ResNet50    & 0.000 $\pm$ 0.002 & 0.233 $\pm$ 0.341 & 26.8\% & 23.4\% & 18.2\%       \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table*}



\iffalse
\begin{comment}
\begin{table*}[t]
\caption{Post-softmax scores when masking the input according to the hyperplane normal.}
\label{tab:masking}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lccccc}
\toprule
Model & Default Pred & Masked Pred & Pred Swap 0.1 & Pred Swap 0.2 & Pred Swap 0.5 \\
\midrule
ResNet34 Digipath & 0.018 $\pm$ 0.021 & 0.064 $\pm$ 0.082 & 12.0\% & 4.40\% & 0.386\% \\
ResNet34* & 0.000 $\pm$ 0.000 ($\pm$0.002) & 0.004 $\pm$ 0.000 ($\pm$0.013) & 0.33\% $\pm$ 0.05 & 0.07\% $\pm$ 0.01 & 0.00\% $\pm$ 0.00 \\
ResNet50* & 0.000 $\pm$ 0.000 ($\pm$0.003) & 0.003 $\pm$ 0.000 ($\pm$0.013) & 0.23\% $\pm$ 0.05 & 0.10\% $\pm$ 0.04 & 0.00\% $\pm$ 0.00 \\
ResNet50 Robust* & 0.001 $\pm$ 0.000 ($\pm$0.004) & 0.003 $\pm$ 0.000 ($\pm$0.006) & 0.03\% $\pm$ 0.02 & 0.00\% $\pm$ 0.00 & 0.00\% $\pm$ 0.00 \\
%ResNet34 Digipath & 0.018 $\pm$ 0.021 & 0.064 $\pm$ 0.082 & 12.0\% & 4.40\% & 0.386\% \\
%ResNet34 & 0.000 $\pm$ 0.002 & 0.004 $\pm$ 0.013 & 0.3\% & 0.06\% & 0.00\% \\
%ResNet50 & 0.000 $\pm$ 0.003 & 0.003 $\pm$ 0.012 & 0.21\% & 0.08\% & 0.00\% \\
%ResNet50 Robust & 0.001 $\pm$ 0.004 & 0.003 $\pm$ 0.006 & 0.01 \% & 0.00\% & 0.00\% \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table*}
\end{comment}
\fi



\subsection{Method}

The idea is to train a linear classifier (single linear layer with a scalar bias and a sigmoid in the end) for binary classification between activations that are predictive for class 'A' and those that predictive for class 'B'.
%
%The linear classifier is simply a weight vector $w$ with the same dimensionality as the number of channels in layer Y and a scalar bias $b$.
%
Then the normal of the hyperplane spanned by the linear classifier would ideally point from class 'A' to 'B', and those activations that are far along the hyperplane normal in the direction of class 'A' would be predictive for class 'A' vs 'B'.
%
Its a similar approach as before for explaining a single class, the only difference is that instead of using attribution to filter and score the activations, we use the linear classifier to filter and score the activations. The rest is the exact same.
%
\cref{contrast-vector-data} shows our approach to generate the hyperplane separating class 'A' and 'B'.
%
This hyperplane is used in the next step shown in \cref{fig:contrast-nmf} to extract the n most important concepts separating class 'A' from 'B'.
%




\subsubsection{Hyperplane}

First we describe \cref{contrast-vector-data}, which shows the generation process of the linear classifier.
%
Step 1 to 3 are done individually per class, step 4 is run for all combinations of any two classes. In the case of ImageNet with 1000 classes we would get roughly one million linear classifier (we exclude the combination of a class with itself).
%
Step 1 and 2 are similar to \cref{fig:single_class}, the only difference is that we do not scale the activations by the attribution, but filter only.
%
And the filtering is a bit more aggressive. Instead of filtering out only those pixels that have negative attribution we also filter out all those pixels that have less than 0.25 of the maximum attribution over all pixels of that image.
%
The reason why we choose a higher cutoff, is that previously the activations would be scaled by the attribution, so that those with lower attribution would have lower impact on the final prototypes.
%
But now all the activations that pass the filtering step have the same impact on the linear classifier, therefore we use a higher cutoff.

In the next step the activations of the pixels remaining after the filtering are saved (i.e. we save a list of vectors per class, each vector representing a pixel in the activations at layer Y).
%
Then in the last step we train a linear classifier to do binary classification between two lists of vectors, that were created in step three (one list for class 'A' and one for class 'B').
%
The linear classifier is always trained for 100 epochs, running the whole dataset at once (no mini-batching).
%
As optimizer we use stochastic gradient descent \cite{10.1214/aoms/1177729586}, \cite{10.1007/978-3-7908-2604-3_16} with a learning rate of 0.01.
%



%Step 1 to 4 in \cref{contrast-vector-data} is done individually per class.
%
%We start from a set of images coming from the validation data (images not seen in training).
%
%We use up to 60 images (if we have less images for that specific class, then we use all the images available, but we do not use more than 60).
%
%For imagenet we only use those images where the model predicts the desired class as majority class (regardless of the actual ground truth label), for the digital pathology model we use those images where the model predicts the class with more than 0.5 post-softmax score anywhere, and that contain the ground truth label.

%Then in the second step, for each of the images we extract the intermediate activations at a predefined hidden layer (in the following referred to as Layer Y).
%
%The hyperplane will be trained in the latent space of this Layer Y.
%
%We do not use all the intermediate activations, but only those that are relevant to the prediction of the desired class.
%
%In order to extract the relevant activations, we compute attribution of the prediction towards those activations using DeepLift and filter the activations by that.
%
%Computing the attribution gives us something of shape (batch, channels, height, width), then we compute the mean over the channels and exclude all negative pixels (those contribute negatively towards the desired class).
%
%Also we use a cutoff to remove those pixels with lower impact, as cutoff we use 0.25 of the maximum attribution per image (the cutoff is done individually per image).
%
%The activation of the remaining pixels is saved (i.e. we save a list of vectors per class, each vector representing a pixel in the activations at Layer Y).

%Then in the fifth step we train a linear classifier to do binary classification between two lists of vectors (one list for class 'A' and one for class 'B').





\subsubsection{Extract Separating Concepts}

Now we describe how we extract the six prototypes for the contrasting.
%
\cref{fig:contrast-nmf} shows our method to find those six concepts separating class 'A' from 'B'.
%
The method is similar to the method explaining a single class, shown in \cref{fig:single_class}.
%
The only difference is in step 2, where we now scale and filter the activations by the linear classifier, instead of using attribution.

We run the linear classifier (without the sigmoid, so only the dot product between the weight vector and the pixel and adding the scalar bias) pixel wise on all the intermediate activations we get at layer Y.
%
If $w$ is the weight vector, $b$ is the scalar bias and $x$ is a pixel of the intermediate activations at layer Y, then we do: $z = w \cdot x + b$, where the output $z$ is a scalar value.
%
Then for each pixel we do: $x' = \text{max}(z, 0) \cdot x$, that is we scale the activations by $z$ if $z$ is larger than zero, otherwise we set those activations to zero.
%
We only keep those activations that are on the side of the hyperplane that is predictive for class 'A', and scale those remaining activations by how far along the hyperplane normal they are.
%
The remaining two steps are the exact same as for explaining a single class.
%
The modified activations are decomposed via NMF into six concepts, and those six concepts are then visualized.
%



%
%We use the same input images and hidden layer as in the previous step \ref{contrast-vector-data}.
%
%For each input image we extract the intermediate activations at the hidden layer.
%
%If we would only score the activations by how far along the hyperplane normal they are, then we would retain irrelevant activations, as shown in \cref{fig:ood_example} where irrelevant activations in the white background are far along the hyperplane.
%
%That is likely because the linear classifier was only trained on activations relevant to the prediction of the class, but later used on all activations (many of which coming from tissue unseen in training).
%
%Therefore we also filter the activations pixel wise using out-of-distribution (OOD) detection using a gaussian mixture model (GMM).
%
%The GMM is trained on a subset of the data the linear classifier is trained on (we only use data from a single class to train the GMM, meaning we get one GMM per class whereas the linear classifier is trained for binary classification between two classes).
%
%After filtering the activations by OOD, we also filter out all those that are on the wrong side of the hyperplane.
%
%The remaining ones are scaled by how far along the hyperplane they are.
%
%Those filtered and scaled activations are then decomposed through NMF, into n (we use 6) dimensions.
%
%And in the last step, we visualize those 6 NMF basis vectors, and interpret those 6 visualizations as the 6 concepts that make the model predict class 'A' instead of 'B'.
%


%-------------- Alaskan Malamute and Siberian Husky single class --------------------------------------

\iffalse
\begin{figure}[tb]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{images/nmf/imagenet/resnet50_robust_layer3_5/249/basis_vis_1.jpg}}
\caption{Visualization for the 6 NMF components for Alaskan Malamute}
\label{fig:nmf_basis_vis_alaskan_malamute}
\end{center}
\vskip -0.2in
\end{figure}



\begin{figure}[tb]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{images/nmf/imagenet/resnet50_robust_layer3_5/250/basis_vis_1.jpg}}
\caption{Visualization for the 6 NMF components for Siberian Husky}
\label{fig:nmf_basis_vis_siberian_husky}
\end{center}
\vskip -0.2in
\end{figure}
\fi


\begin{figure}[tb]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{images/nmf/imagenet/resnet50_robust_layer3_5/249/data_vis_basis.jpg}}
\caption{Visualization for the 6 NMF components for Alaskan Malamute from dataset crops}
\label{fig:nmf_basis_vis_alaskan_malamute_from_dataset}
\end{center}
\vskip -0.2in
\end{figure}



\begin{figure}[tb]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{images/nmf/imagenet/resnet50_robust_layer3_5/250/data_vis_basis.jpg}}
\caption{Visualization for the 6 NMF components for Siberian Husky from dataset crops}
\label{fig:nmf_basis_vis_siberian_husky_from_dataset}
\end{center}
\vskip -0.2in
\end{figure}


%-------------- END Alaskan Malamute and Siberian Husky single class --------------------------------------



%-------------- Alaskan Malamute vs Siberian Husky --------------------------------------


\iffalse

\begin{figure}[tb]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{images/nmf/imagenet/resnet50_robust_layer3_5/249_250/basis_vis_1.jpg}}
\caption{Visualization for the 6 NMF components pro Alaskan Malamute vs Siberian Husky}
\label{fig:nmf_basis_vis_alaskan_malamute_vs_husky}
\end{center}
\vskip -0.2in
\end{figure}



\begin{figure}[tb]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{images/nmf/imagenet/resnet50_robust_layer3_5/250_249/basis_vis_1.jpg}}
\caption{Visualization for the 6 NMF components pro Siberian Husky vs Alaskan Malamute}
\label{fig:nmf_basis_vis_siberian_husky_vs_malamute}
\end{center}
\vskip -0.2in
\end{figure}

\fi



\begin{figure}[tb]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{images/nmf/imagenet/resnet50_robust_layer3_5/249_250/data_vis_basis.jpg}}
\caption{Visualization for the 6 NMF components pro Alaskan Malamute vs Siberian Husky from dataset crops}
\label{fig:nmf_basis_vis_alaskan_malamute_vs_husky_from_dataset}
\end{center}
\vskip -0.2in
\end{figure}



\begin{figure}[tb]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{images/nmf/imagenet/resnet50_robust_layer3_5/250_249/data_vis_basis.jpg}}
\caption{Visualization for the 6 NMF components pro Siberian Husky vs Alaskan Malamute from dataset crops}
\label{fig:nmf_basis_vis_siberian_husky_vs_malamute_from_dataset}
\end{center}
\vskip -0.2in
\end{figure}



%-------------- END Alaskan Malamute vs Siberian Husky --------------------------------------




\subsection{Results}

\subsubsection{Shifting Test}
\label{sec:shifting_test}

%We run two tests, in order of checking the faithfulness of our method to the model.
%
%Both tests investigate the hyperplane normal.
%
The test we propose for the contrasting, tests the direction of the hyperplane.
%
Ideally, the hyperplane normal should point from class 'B' towards class 'A'.
%
This means that if we would start from an image of class 'B', and then translate the activations in the hidden layer along the hyperplane normal, we would like the model to swap prediction to class 'A'.
%
We do the translation using ten offset values (those are fixed and the same for all classes and models) and report the result of the offset value where the prediction for class 'A' is highest.

The results from this translation test are shown in \cref{tab:shifting}.
%
In the following we describe how to read that table.
%
Default pred as well as shifted pred both show the results for class 'A'.
%
Default pred shows the mean prediction for class 'A' for the original activations (when not adjusting anything).
%
Shifted pred shows the mean prediction for class 'A' when translating the activations along the hyperplane normal.
%
For the digipath model we run all combination of classes (there we only have 37 classes), and report the mean ($\pm$ standard deviation over all samples).
%
For the imagenet models we have too many classes to run all combinations time wise, instead there we use each of the 1000 classes as starting class ('B'), and for each of them we choose only 10 random classes ('A') to shift towards.
%
Then we repeat this 5 times and report mean $\pm$ standard deviation over the 5 runs as well as ($\pm$ standard deviation over all samples).

The next three columns show in how many of the class combinations the model swaps prediction (i.e. for those shifted activations the model now predicts class 'A' as majority class instead of class 'B').
%
Also the model needs to predict class 'A' with at least a certain average confidence.
%
For example, for pred swap 0.5 it would need to predict at least 0.5 for class 'A', on average over all samples.

We can see that for all the models, the original prediction for class 'A' is very low (default pred column).
%
Which makes sense, since we choose images where the model is predicting another class 'B' as majority class.
%
But the shifted prediction becomes much larger.
%
When comparing the shifted prediction with the default prediction, we have a minimum increase factor of 25x for the ResNet34 Digipath model, and a maximum increase factor of 1494x for the ResNet50 model.
%
We get a minimum average shifted prediction of 0.263 for the ResNet34 model, and a maximum of 0.637 for the ResNet50 Robust model.


\iffalse
\begin{comment}
\subsubsection{Masking Test}

For this test, we score each pixel in the hidden layer by how far it is away from the hyperplane.
%
Then we iteratively 'remove' pixels that are furthest on the side of class 'A'.
%
We remove them, by setting the corresponding pixels in the input image to a baseline (as baseline we use a solid color, white for the digital pathology images, black after normalization for the imagenet images).
%
We do this removal over 11 equidistant steps (from the full image to the completely erased image, each step removing 10\% of the pixels).
%
At each step we check the prediction of the model, and in \cref{tab:masking} we report the results for the step that has highest prediction for class 'B'.
%
The table can be read in a similar way as the table \cref{tab:shifting}, described in the previous section \cref{sec:shifting_test}.

The increase in the prediction is smaller now.
%
We get a minimum increase by a factor of 3.5x for the ResNet34 Digipath model, and a maximum increase of 16.5x for the ResNet34 model.
%
For the imagenet models, the prediction for the masked image (0.003 to 0.004) is not much higher than the average prediction for 1000 classes (which would be 0.001).
%
But there are still some cases, where the model swaps prediction, especially for the ResNet34 Digipath model.
%
\end{comment}
\fi









\begin{table*}[t]
\caption{Prediction for the original and stitched images. Both when inserting the person, as well as inserting zeros instead of the person.}
\label{tab:example_preds_malamute_husky}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lcccccc}
\toprule
Class & Original Pred & Person Inserted Pred & Black Inserted Pred \\
\midrule
Siberian Husky    & 0.377 & 0.265 & 0.342 \\
Alaskan Malamute   & 0.203 & 0.265 & 0.189 \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table*}




\section{Case Studies}
\label{sec:case_studies}

\subsection{Alaskan Malamute vs Siberian Husky}

In this section we investigate why the ResNet50 Robust model classifies an image as Alaskan Malamute vs Siberian Husky and vice versa (in the appendix we show more examples also from the other models).
%
We apply the method at layer3.5.
%
Both classes are dog breeds that look very similar.
%
%\cref{fig:nmf_basis_vis_alaskan_malamute} shows synthesized visualizations (via gradient descent) for the six most important concepts supporting the class Alaskan Malamute and \cref{fig:nmf_basis_vis_siberian_husky} shows synthesized visualizations for the 6 most important concepts supporting Siberian Husky.
%
\cref{fig:nmf_basis_vis_alaskan_malamute_from_dataset} shows visualizations from dataset crops for those six most important concepts supporting Alaskan Malamute, whereas \cref{fig:nmf_basis_vis_siberian_husky_from_dataset} shows the same for Siberian Husky.
%
We can see that the model mostly uses the dog nose, eyes and ears to predict either class.
%
For example, the first concept for Siberian Husky shows dog noses, the second looks unclear (from the dataset visualization it looks like fur), the third shows eyes, the fourth looks like it shows fur at the head, the fifth shows the top of head without ears and the last shows pointy ears.
%
But both sets of images (Alaskan Malamute vs Siberian Husky) look very similar, just based on this it is not possible to understand why the model predicts one class versus the other.

Next we can contrast both classes (using our method described in \cref{sec:class_contrasting}), and look at the six most important concepts separating one class from the other.
%
Now we can see differences between the visualizations of both classes.
%
For Alaskan Malamute, there is now only one component showing part of the dog head (the full head without ears in this case), as shown in the second row of \cref{fig:nmf_basis_vis_alaskan_malamute_vs_husky_from_dataset}.
%
For Siberian Husky, most of the components still show parts of the dog head, the bottom most component looks a bit different though.
%
Now it shows larger eyes.
%
For the Alaskan Malamute, the three bottom most components are interesting (since they were also not present when looking only at the Alaskan Malamute class in isolation), shown in \cref{fig:nmf_basis_vis_alaskan_malamute_vs_husky_from_dataset}.
%
The top of those bottom three components shows dog feet on grass, the next one shows grass and the last one looks like the top of human heads.
%
The last two components (grass and top of human heads), point to a bias in the model, cause whether the dog is standing on grass or snow, or whether a human is next to the dog or not, does not change the class the dog belongs to.
%
So the model should not rely on those features to separate between the two dog classes.

To test this bias, we insert an image of a person into each of the sixty images predicted as Siberian Husky (always at the bottom right), and then check whether the model changes its prediction to Alaskan Malamute.
%
Something to note is that the person is cropped out of a (validation) image where the model predicts Alaskan Malamute.
%
One of those sixty modified images is shown in \cref{fig:husky_person_inserted}, once with the person inserted and once with zeros inserted. The image is shown after normalization.
%
\begin{figure}[tb]
    \centering
    \begin{minipage}{0.45\columnwidth}
        \centering
        \includegraphics[width=\textwidth]{images/husky_images_person_inserted.jpg}
    \end{minipage}
    \hfill
    \begin{minipage}{0.45\columnwidth}
        \centering
        \includegraphics[width=\textwidth]{images/husky_images_black_inserted.jpg}
    \end{minipage}
    \caption{Example of inserting the person (or zeros) into an originally as husky predicted image}
    \label{fig:husky_person_inserted}
\end{figure}
%
%
\cref{tab:example_preds_malamute_husky} shows the average prediction for Siberian Husky and Alaskan Malamute for those sixty images.
%
Originally the model predicts more Siberian Husky (0.377). After inserting the person into the lower right of each of the images, the model predicts the same amount of Siberian Husky and Alaskan Malamute (both 0.265).
%
Since we occlude part of the image by inserting the person, we also test the prediction if we insert zeros (black) instead of the person. In that case the prediction does not change much, and slightly decreases for both Siberian Husky as well as Alaskan Malamute.
%




\section{Additional Discussion}

The method to explain a single class could be seen as an interpretability method (which was our goal), but can also been seen as a method to generate high level adversarial examples.
%
That is, because it provides an automated method to extract crops from six images where the model is not predicting the targeted class, then for the combined image the model is often predicting that targeted class. %(for 71.0\% of the classes for the ResNet50 model).
%
This test (or attack) works worse for the ResNet50 Robust model which has adversarially robust weights (it was trained for robustness against gradient descent attacks).
%
While it still works for 42.9\% of the classes, the confidence is lower (the average prediction for the targeted class is only 0.190).
%
Also, when it predicts the targeted class, then only for 11.3\% of the classes does it do so with a confidence of larger than 0.5.
%
As comparison, the ResNet50 and ResNet34 model with weights from pytorch model zoo do so for 62.6\% respectively 55.4\% of the classes (much more often).
%Also it predicted the targeted class with a confidence of larger than 0.5 for only 11.3\% of the classes, whereas the ResNet50 and ResNet34 model with weights from pytorch model zoo do that in 62.6\% respectively 55.4\% of the classes (much more often). % TODO rewrite this sentence
%
For the ResNet50 model with weights from pytorch model zoo the test (or attack) works for 71.0\% of the classes and the confidence for the targeted class is higher, there we have an average prediction of 0.619.
%
Similar for the ResNet34 model with weights from pytorch model zoo, there the test works for 65.4\% of the classes with an average prediction of 0.547.

While using the last convolutional layer (layer4.2) provides best results in the test, the explanations may be less interpretable compared to using layer3.5.
%
%We only tested our method extracting six prototypes, but there was no specific reason to choose six.





\section{Limitations}

%At this point the method does not work with ViTs, we tested it on a ViT-B16, but the prediction does not change when shifting the activations in the latent space.
%
%The shifting test does not work for all combination of classes, it works best for the ResNet50 with adversarially robust weights in 69.2\% of the combinations, and worst for the ResNet34 where it only works for 10.3\% of the combinations.
%
%In a class combination where the test fails, the method should likely not be trusted.
%
%Also we need to manually choose a hidden layer in the model where the method should be applied at.
%
%And the OOD-Detection with the GMM also needs manual choosing of the cutoff value between what is in and out of distribution (but we use the same cutoff value for all classes of a model).
%
%Also we use attribution (DeepLift) in order to filter out activations that are irrelevant for the prediction of the class.
%
We only tested the method on convolutional neural networks.
%
Further, for a class where the test fails, the method should likely not be trusted.
%
Also we need to manually choose a hidden layer in the model where the method should be applied at (we could potentially run the prediction test for the prototypes for different layers of the model, and then choose the layer where the test performs best).
%
%But the layer where the test performs best, may not be the layer where the explanation is easiest to understand
%


%\section{Additional Discussion}

%Something to note is, that while it is desired that the model swaps prediction in the shifting test, that alone is not enough.
%
%We also need to be able to interprete the direction of the hyperplane normal.
%

\section{Acknowledgments}

R.H. is funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) - Projectnumber 459360854 (DFG FOR 5347 Lifespan AI). D.O.B. acknowledges the financial support by the Federal Ministry of Education and Research (BMBF) within the T!Raum-Initiative "\#MOIN! Modellregion Industriemathematik" in the sub-project "MUKIDerm".



\bibliography{example_paper}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\FloatBarrier
%\onecolumn


\section{Hardware and Software}

On the software side we use pytorch \cite{paszke2019} version 1.13.1 with cuda version 11.6 and torchvision \cite{torchvision2016} version 0.14.1.
%
For the attribution methods (for DeepLift and SmoothGrad) we use the captum library \cite{kokhlikyan2020captum} version 0.6.0.
%
For NMF we use scipy \cite{2020SciPy-NMeth} version 1.10.1.

For the hardware, we ran the experiments on a Linux server with 8 Nvidia RTX 2080Ti GPUs and we also used another Linux server with 4 Nvidia RTX A6000 GPUs. But the experiments are all doable on a single RTX 2080Ti GPU.
%


\section{Generate Visualizations via Gradient Descent}

In the main paper we only showed examples where we generate the visualizations by sampling crops from validation images.
%
We can also synthesize them via gradient descent though, similar to \cite{olah2017feature} or \cite{nguyen2016synthesizingpreferredinputsneurons}.
%
For the following digipath example, we also generate the visualizations for each of the six prototypes via gradient descent in the latent space of a GAN (generative adversarial network).
%
Such visualizations might be more faithful compared to sampling dataset examples.
%
For example in \cref{fig:nmf_basis_vis_3} the last third and the last two images show palisading cell edges with different orientations (the first one shows an upper right edge, the next one a lower right edge and the last one a left edge).
%
But in the corresponding dataset visualizations shown in \cref{fig:nmf_basis_vis_from_data} (third row and last two rows), there is no edge orientation visible, but they simply show BCC islands.
%
Meaning, from the dataset visualizations alone, we could not see that the model is actually looking at the edge of the BCC island that has a specific orientation (from only the dataset example we could think that its looking for the islands themselves, or for a right edge instead of a left edge).
%





\section{Basal Cell Carcinoma vs Trichoblastoma}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%





\begin{figure*}[tb]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=2\columnwidth]{images/nmf/trichoblastoma_basis_3_1.jpg}}
\caption{Visualization for the 6 NMF components for Trichoblastoma}
\label{fig:nmf_basis_vis_3_trichoblastoma}
\end{center}
\vskip -0.2in
\end{figure*}


\begin{figure*}[tb]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=2\columnwidth]{images/nmf/basis_3_1.jpg}}
\caption{Visualization for the 6 NMF components for BCC}
\label{fig:nmf_basis_vis_3}
\end{center}
\vskip -0.2in
\end{figure*}

\iffalse
\begin{comment}
\begin{figure*}[tb]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=2\columnwidth]{images/nmf/basis_0_1.jpg}}
\caption{Visualization for the 6 NMF components for BCC}
\label{fig:nmf_basis_vis}
\end{center}
\vskip -0.2in
\end{figure*}
\end{comment}
\fi

\begin{figure*}[tb]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=2\columnwidth]{images/nmf/1_vs_32_basis_3_1.jpg}}
\caption{Visualization for the 6 NMF components pro BCC vs Trichoblastoma}
\label{fig:nmf_basis_vis_3_bcc_vs_trichoblastoma}
\end{center}
\vskip -0.2in
\end{figure*}




\begin{figure}[tb]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{images/nmf/data_vis_basis.jpg}}
\caption{Dataset visualization for the 6 NMF components for BCC (best viewed zoomed in)}
\label{fig:nmf_basis_vis_from_data}
\end{center}
\vskip -0.2in
\end{figure}

\begin{figure}[tb]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{images/nmf/1_vs_32_data_vis_basis.jpg}}
\caption{Dataset visualization for the 6 NMF components for BCC vs Trichoblastoma (best viewed zoomed in)}
\label{fig:1_vs_32_nmf_basis_vis_from_data}
\end{center}
\vskip -0.2in
\end{figure}







Basal Cell Carcinoma (BCC) as well as Trichoblastoma look clinically very similar to each other \cite{Patel2020}. % TODO: Some citation for the similarity claim -> done
%
Human experts as well as the AI model can separate them, and in this section we investigate why the model predicts Basal Cell Carcinoma over Trichoblastoma.
%
First we can look at the main concepts the model uses to predict BCC shown in \cref{fig:nmf_basis_vis_3} and compare them to the main concepts the model uses to predict Trichoblastoma shown in \cref{fig:nmf_basis_vis_3_trichoblastoma}.
%
The problem is that the concepts look very similar (which makes sense since the two diseases look clinically similar), there is no single concept that is present for one class but not the other.
%
For example the first concept from the left for Trichoblastoma looks similar compared to the first from the left for BCC.
%
The second one for Trichoblastoma looks similar to the fourth for BCC and the third looks similar to the second.
%
And the last three concepts for Trichoblastoma show palisading cell edges with different orientations, similar to the third and last two concepts for BCC.
%
So those visualizations are not enough to spot a difference between Trichoblastoma and BCC.

In the next step we look at the visualizations pro BCC we get from contrasting the two classes.
%
\cref{fig:nmf_basis_vis_3_bcc_vs_trichoblastoma} shows the 6 main concepts that are pro BCC vs Trichoblastoma.
%
The second, third and fourth concept look differently from the 6 main concepts pro Trichoblastoma (so here we can now see a difference).
%
%Next we can look at the dataset examples instead of synthesized visualizations, since the dataset examples are easier to interpret. \cref{fig:1_vs_32_nmf_basis_vis_from_data} shows the dataset crop examples for those 6 concepts that are pro BCC vs Trichoblastoma.
%
For interpreting those 3 concepts we can look at dataset examples, shown in \cref{fig:1_vs_32_nmf_basis_vis_from_data}.
%
The fourth concept shows white areas inside tumor, which could resemble cleavage (in BCC the tumor is often separated from the surrounding tissue by white area).
%
The third concept shows epidermis, which is also interesting since according to \cite{Patel2020} BCC is sometimes attached to the epidermis, whereas Trichoblastoma never is.

For the quantitative check, the average maximum prediction for BCC in Trichoblastoma images is 0.013 and the average maximum prediction for Trichoblastoma is 0.889.
%
So at this point the model is very sure that the images are Trichoblastoma and not BCC.
%
When masking 20\% of the input pixels that lie furthest on the Trichoblastoma side of the hyperplane between Trichoblastoma and BCC, then the maximum average prediction for BCC goes up to 0.408, which is an increase by a factor of 31.
%
And the average maximum prediction for Trichoblastoma goes down to 0.467, so now the model is no longer sure whether the images belong to BCC or Trichoblastoma.
%
We mask the input pixels in steps of 10\%, and masking more or less than those 20\% reduces the prediction for BCC.
%
Something to note, is that between BCC and Trichoblastoma this masking only works one way, from Trichoblastoma to BCC.
%
Doing it the other way around (using BCC images and mask the most important features for BCC vs Trichoblastoma) reduces the predictions for both BCC as well as Trichoblastoma.

Shifting the activations in the latent space in the direction of the hyperplane normal works both ways.
%
Here we reach an average maximum prediction for BCC of 0.721 (at that point the prediction for Trichoblastoma is 0.481).
%
And the other way around, we reach an average maximum prediction for Trichoblastoma of 0.673 and at that point the prediction for BCC is 0.017.






\section{Examples for Single Class for the ResNet50}

\begin{figure}[tb]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{images/appendix_examples/fireboat_554_layer4.jpg}}
\caption{Visualization for the 6 NMF components for fireboat, using layer4.2. The model predicts 1.0 for fireboat for the six prototypes (passing the first column into the model)}
\label{fig:goldfish_layer3}
\end{center}
\vskip -0.2in
\end{figure}

\begin{figure}[tb]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{images/appendix_examples/fireboat_554_layer4_exclude_target.jpg}}
\caption{Visualization for the 6 NMF components for fireboat, using layer4.2 and excluding images where the model predicts fireboat. The model predicts 0.98 for fireboat for the six prototypes}
\label{fig:goldfish_layer3}
\end{center}
\vskip -0.2in
\end{figure}



\iffalse
\begin{comment}
\begin{figure}[tb]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{images/appendix_examples/goldfish_1_layer3.jpg}}
\caption{Visualization for the 6 NMF components goldfish, using layer3.5. The model predicts 0.98 for goldfish for the six prototypes}
\label{fig:goldfish_layer3}
\end{center}
\vskip -0.2in
\end{figure}


\begin{figure}[tb]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{images/appendix_examples/goldfish_1_layer4.jpg}}
\caption{Visualization for the 6 NMF components goldfish, using layer4.2. The model predicts 1.0 for goldfish for the six prototypes}
\label{fig:goldfish_layer4}
\end{center}
\vskip -0.2in
\end{figure}


\begin{figure}[tb]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{images/appendix_examples/goldfish_1_layer4_non_target.jpg}}
\caption{Visualization for the 6 NMF components goldfish, using layer4.2 and excluding images where the model predicts goldfish. The model still predicts 1.0 for goldfish for the six prototypes. The prototype at the bottom looks like the hind fin of the fish \cref{fig:goldfish_layer4}, which has been replaced by the comb of a rooster which looks kind of similar.}
\label{fig:goldfish_layer4_exclude_target}
\end{center}
\vskip -0.2in
\end{figure}
\end{comment}
\fi



\begin{figure}[tb]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{images/appendix_examples/gazelle_353_layer3.jpg}}
\caption{Visualization for the 6 NMF components for gazelle, using layer3.5. The model predicts 1.0 for gazelle for the six prototypes}
\label{fig:goldfish_layer3}
\end{center}
\vskip -0.2in
\end{figure}


\begin{figure}[tb]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{images/appendix_examples/gazelle_353_layer3_non_target.jpg}}
\caption{Visualization for the 6 NMF components for gazelle, using layer3.5 and excludig images where the model predicts gazelle. The model predicts 0.82 for gazelle for the six prototypes}
\label{fig:goldfish_layer3}
\end{center}
\vskip -0.2in
\end{figure}

\begin{figure}[tb]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{images/appendix_examples/gazelle_353_layer4.jpg}}
\caption{Visualization for the 6 NMF components for gazelle, using layer4.2. The model predicts 0.99 for gazelle for the six prototypes}
\label{fig:goldfish_layer4}
\end{center}
\vskip -0.2in
\end{figure}


\begin{figure}[tb]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{images/appendix_examples/gazelle_353_layer4_non_target.jpg}}
\caption{Visualization for the 6 NMF components for gazelle, using layer4.2 and excluding images where the model predicts gazelle. Now the model predicts only 0.33 for gazelle, and 0.67 for impala. This would be a failure case, because gazelle is not the majority class}
\label{fig:goldfish_layer4_exclude_target}
\end{center}
\vskip -0.2in
\end{figure}







\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.





% 




