\section{Related Work}
This work is focused on providing explanations for already trained deep neural networks, as opposed to training inherently interpretable models \cite{Nauta_2023_CVPR}.

Previous work employed attribution methods, that can be used to generate heat maps, to highlight areas in the input image that are important for the output (\cite{shrikumar2019learning}, \cite{petsiuk2018rise}).
%
A fundamental problem with such methods is that they only highlight where in the input the model is seeing something, but do not explain what the model is seeing there.
%
Using such attribution methods at the input layer (like its usually done), can give unreliable results. For some of those methods \cite{SanityCheckSaliencyMaps} has shown that they fail a sanity check and produce similar explanations for a trained and randomly initialized model.
%
Those methods that passed the check, tend to give noisy explanations \cite{SmoothGrad}.

In our case, we use attribution methods (DeepLift with SmoothGrad), but at a hidden layer, not at the input layer.
%
That is similar to \cite{olah2018the}, where attribution methods at a hidden layer were coupled with visualization techniques, therefore answering both where the model is seeing something (attribution) and what it is seeing there (visualization).
%
Similar to \cite{olah2018the} we also use non-negative matrix factorization (NMF) \cite{Lee1999} on the intermediate activations at a hidden layer to get the explanation down to a manageable number of concepts (we use six). But instead of using all activations, we first filter and score them by attribution, and instead of running NMF on a single image only we run it simultaneously on up to 60 images.
%
Using such a dataset of images to extract relevant concepts is similar to ACE and CRAFT, but we use the whole images to extract concepts instead of using image crops.
%
After we extracted the concepts, then we use image crops to visualize them to the user or run the prediction test on the model.

The test we use for validating the explanation for a single class, is inspired by \cite{carter2019activation}.
%
% TODO: continue
%
There two images of two classes were combined, to change the prediction of the model to a third class (e.g. combine an image of grey whale with one of a baseball, then the model predicts great white shark for the combined image).
%
Finding such combinations required manual selection of a user based on insights into the model.
%
In our case, finding such combinations works automatically and we use six images instead of two.
%
Also we do not use the whole image, but only a crop of each of the six images (32x32 or 74x74 depending on the hidden layer we use).
%






%There are mainly three areas of related work, all addressing the challenge of explaining deep neural networks.
%
%The first are attribution methods, which can also be used as saliency methods (provide heatmaps of what input pixels the model used for its prediction).
%




% TODO: add result for ResNet50 Robust for layer4.2

\begin{table*}[t]
\caption{Post-softmax prediction for the stitched images. Including the desired class in sampling of the patches}
\label{tab:data_vis_test}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lccccccc}
\toprule
Model & Layer & Average Pred & Predicts Desired Class & Above 0.1 & Above 0.2 & Above 0.5 \\
\midrule
ResNet50 Robust  & 3.5  & 0.178 $\pm$ 0.311 & 22.0\% & 30.3\% & 23.4\% & 16.0\% \\
ResNet50    & 3.5 & 0.539 $\pm$ 0.437 & 56.3\% & 67.4\% & 62.2\% & 54.0\% \\
ResNet34    & 3.5 & 0.300 $\pm$ 0.375 & 34.3\% & 46.6\% & 40.6\% & 28.0\% \\

ResNet50 Robust  & 4.2  & 0.415 $\pm$ 0.341 & 72.8\% & 74.9\% & 63.0\% & 36.4\% \\
ResNet50    & 4.2 & 0.898 $\pm$ 0.216 & 94.1\% & 98.3\% & 97.1\% & 92.1\% \\
ResNet34    & 4.2 & 0.852 $\pm$ 0.244 & 92.5\% & 97.8\% & 95.7\% & 88.4\% \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table*}


\begin{table*}[t]
\caption{Post-softmax prediction for the stitched images. Excluding the desired class in sampling of the patches}
\label{tab:data_vis_test_exclude_target}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lcccccc}
\toprule
Model & Layer & Average Pred & Predicts Desired Class & Above 0.1 & Above 0.2 & Above 0.5 \\
\midrule
ResNet50 Robust   & 3.5 & 0.103 $\pm$ 0.229 & 14.0\% & 19.4\% & 15.1\% & 7.7\% \\
ResNet50    & 3.5 & 0.284 $\pm$ 0.389 & 30.0\% & 40.0\% & 35.4\% & 27.3\% \\
ResNet34    & 3.5 & 0.175 $\pm$ 0.300 & 20.0\% & 30.1\% & 24.6\% & 15.4\% \\

ResNet50 Robust   & 4.2 & 0.190 $\pm$ 0.229 & 42.9\% & 48.9\% & 32.2\% & 11.3\% \\
ResNet50    & 4.2 & 0.619 $\pm$ 0.358 & 71.0\% & 86.0\% & 79.0\% & 62.6\% \\
ResNet34    & 4.2 & 0.547 $\pm$ 0.354 & 65.4\% & 83.7\% & 74.3\% & 55.4\% \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table*}