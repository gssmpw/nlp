@inproceedings{langley00,
 author    = {P. Langley},
 title     = {Crafting Papers on Machine Learning},
 year      = {2000},
 pages     = {1207--1216},
 editor    = {Pat Langley},
 booktitle     = {Proceedings of the 17th International Conference
              on Machine Learning (ICML 2000)},
 address   = {Stanford, CA},
 publisher = {Morgan Kaufmann}
}

@TechReport{mitchell80,
  author = 	 "T. M. Mitchell",
  title = 	 "The Need for Biases in Learning Generalizations",
  institution =  "Computer Science Department, Rutgers University",
  year = 	 "1980",
  address =	 "New Brunswick, MA",
}

@phdthesis{kearns89,
  author = {M. J. Kearns},
  title =  {Computational Complexity of Machine Learning},
  school = {Department of Computer Science, Harvard University},
  year =   {1989}
}

@Book{MachineLearningI,
  editor = 	 "R. S. Michalski and J. G. Carbonell and T.
		  M. Mitchell",
  title = 	 "Machine Learning: An Artificial Intelligence
		  Approach, Vol. I",
  publisher = 	 "Tioga",
  year = 	 "1983",
  address =	 "Palo Alto, CA"
}

@Book{DudaHart2nd,
  author =       "R. O. Duda and P. E. Hart and D. G. Stork",
  title =        "Pattern Classification",
  publisher =    "John Wiley and Sons",
  edition =      "2nd",
  year =         "2000"
}

@misc{anonymous,
  title= {Suppressed for Anonymity},
  author= {Author, N. N.},
  year= {2021}
}

@InCollection{Newell81,
  author =       "A. Newell and P. S. Rosenbloom",
  title =        "Mechanisms of Skill Acquisition and the Law of
                  Practice", 
  booktitle =    "Cognitive Skills and Their Acquisition",
  pages =        "1--51",
  publisher =    "Lawrence Erlbaum Associates, Inc.",
  year =         "1981",
  editor =       "J. R. Anderson",
  chapter =      "1",
  address =      "Hillsdale, NJ"
}


@Article{Samuel59,
  author = 	 "A. L. Samuel",
  title = 	 "Some Studies in Machine Learning Using the Game of
		  Checkers",
  journal =	 "IBM Journal of Research and Development",
  year =	 "1959",
  volume =	 "3",
  number =	 "3",
  pages =	 "211--229"
}


% feature vis distill, for fft dec gradient descent
@article{olah2017feature,
  author = {Olah, Chris and Mordvintsev, Alexander and Schubert, Ludwig},
  title = {Feature Visualization},
  journal = {Distill},
  year = {2017},
  note = {https://distill.pub/2017/feature-visualization},
  doi = {10.23915/distill.00007}
}


% feature vis, precondition with GAN
@misc{nguyen2016synthesizingpreferredinputsneurons,
      title={Synthesizing the preferred inputs for neurons in neural networks via deep generator networks}, 
      author={Anh Nguyen and Alexey Dosovitskiy and Jason Yosinski and Thomas Brox and Jeff Clune},
      year={2016},
      eprint={1605.09304},
      archivePrefix={arXiv},
      primaryClass={cs.NE},
      url={https://arxiv.org/abs/1605.09304}, 
}


@article{JANSEN2023161,
title = {Deep learning detection of melanoma metastases in lymph nodes},
journal = {European Journal of Cancer},
volume = {188},
pages = {161-170},
year = {2023},
issn = {0959-8049},
doi = {https://doi.org/10.1016/j.ejca.2023.04.023},
url = {https://www.sciencedirect.com/science/article/pii/S0959804923002241},
author = {Philipp Jansen and Daniel Otero Baguer and Nicole Duschner and Jean Le’Clerc Arrastia and Maximilian Schmidt and Jennifer Landsberg and Jörg Wenzel and Dirk Schadendorf and Eva Hadaschik and Peter Maass and Jörg Schaller and Klaus Georg Griewank},
keywords = {Melanoma metastasis, Lymph nodes, Artificial intelligence, U-Net, Digital pathology, Whole-slide image (WSI), Computer-aided diagnosis (CAD)},
abstract = {
}}


% Deeplift

@misc{shrikumar2019learning,
      title={Learning Important Features Through Propagating Activation Differences}, 
      author={Avanti Shrikumar and Peyton Greenside and Anshul Kundaje},
      year={2019},
      eprint={1704.02685},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

% Integrated gradients
@InProceedings{pmlr-v70-sundararajan17a,
  title = 	 {Axiomatic Attribution for Deep Networks},
  author =       {Mukund Sundararajan and Ankur Taly and Qiqi Yan},
  booktitle = 	 {Proceedings of the 34th International Conference on Machine Learning},
  pages = 	 {3319--3328},
  year = 	 {2017},
  editor = 	 {Precup, Doina and Teh, Yee Whye},
  volume = 	 {70},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {06--11 Aug},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v70/sundararajan17a/sundararajan17a.pdf},
  url = 	 {https://proceedings.mlr.press/v70/sundararajan17a.html},
  abstract = 	 {We study the problem of attributing the prediction of a deep network to its input features, a problem previously studied by several other works. We identify two fundamental axioms—Sensitivity and Implementation Invariance that attribution methods ought to satisfy. We show that they are not satisfied by most known attribution methods, which we consider to be a fundamental weakness of those methods. We use the axioms to guide the design of a new attribution method called Integrated Gradients. Our method requires no modification to the original network and is extremely simple to implement; it just needs a few calls to the standard gradient operator. We apply this method to a couple of image models, a couple of text models and a chemistry model, demonstrating its ability to debug networks, to extract rules from a network, and to enable users to engage with models better.}
}

%Make gradient based attribution smoother, by computing attribution n times (e.g. 50 times) and each time adding gaussian noise
%to the input. Then average over the n results. Intuetively, the networks gradient may be not very continuos and may jitter strongly with
%small pertubations to the input. Therefore, it might make more sense to compute the attribution by average of e.g. 50 pertubed input images,
%instead of just a single run with the input image.

@misc{SmoothGrad,
  doi = {10.48550/ARXIV.1706.03825},
  
  url = {https://arxiv.org/abs/1706.03825},
  
  author = {Smilkov, Daniel and Thorat, Nikhil and Kim, Been and Viégas, Fernanda and Wattenberg, Martin},
  
  keywords = {Machine Learning (cs.LG), Computer Vision and Pattern Recognition (cs.CV), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {SmoothGrad: removing noise by adding noise},
  
  publisher = {arXiv},
  
  year = {2017},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}



% grad cam
@InProceedings{Selvaraju_2017_ICCV,
author = {Selvaraju, Ramprasaath R. and Cogswell, Michael and Das, Abhishek and Vedantam, Ramakrishna and Parikh, Devi and Batra, Dhruv},
title = {Grad-CAM: Visual Explanations From Deep Networks via Gradient-Based Localization},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
month = {Oct},
year = {2017}
} 



@misc{petsiuk2018rise,
      title={RISE: Randomized Input Sampling for Explanation of Black-box Models}, 
      author={Vitali Petsiuk and Abir Das and Kate Saenko},
      year={2018},
      eprint={1806.07421},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}


@inproceedings{SanityCheckSaliencyMaps,
  author       = {Julius Adebayo and
                  Justin Gilmer and
                  Michael Muelly and
                  Ian J. Goodfellow and
                  Moritz Hardt and
                  Been Kim},
  editor       = {Samy Bengio and
                  Hanna M. Wallach and
                  Hugo Larochelle and
                  Kristen Grauman and
                  Nicol{\`{o}} Cesa{-}Bianchi and
                  Roman Garnett},
  title        = {Sanity Checks for Saliency Maps},
  booktitle    = {Advances in Neural Information Processing Systems 31: Annual Conference
                  on Neural Information Processing Systems 2018, NeurIPS 2018, December
                  3-8, 2018, Montr{\'{e}}al, Canada},
  pages        = {9525--9536},
  year         = {2018},
  url          = {https://proceedings.neurips.cc/paper/2018/hash/294a8ed24b1ad22ec2e7efea049b8737-Abstract.html},
  timestamp    = {Mon, 16 May 2022 15:41:51 +0200},
  biburl       = {https://dblp.org/rec/conf/nips/AdebayoGMGHK18.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

Interpretability Beyond Feature Attribution:
Quantitative Testing with Concept Activation Vectors (TCAV)
@misc{kim2018interpretabilityfeatureattributionquantitative,
      title={Interpretability Beyond Feature Attribution: Quantitative Testing with Concept Activation Vectors (TCAV)}, 
      author={Been Kim and Martin Wattenberg and Justin Gilmer and Carrie Cai and James Wexler and Fernanda Viegas and Rory Sayres},
      year={2018},
      eprint={1711.11279},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/1711.11279}, 
}


CRAFT: Concept Recursive Activation FacTorization for Explainability
@InProceedings{Fel_2023_CVPR,
    author    = {Fel, Thomas and Picard, Agustin and B\'ethune, Louis and Boissin, Thibaut and Vigouroux, David and Colin, Julien and Cad\`ene, R\'emi and Serre, Thomas},
    title     = {CRAFT: Concept Recursive Activation FacTorization for Explainability},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2023},
    pages     = {2711-2721}
}


% building blocks of interpretability
@article{olah2018the,
  author = {Olah, Chris and Satyanarayan, Arvind and Johnson, Ian and Carter, Shan and Schubert, Ludwig and Ye, Katherine and Mordvintsev, Alexander},
  title = {The Building Blocks of Interpretability},
  journal = {Distill},
  year = {2018},
  note = {https://distill.pub/2018/building-blocks},
  doi = {10.23915/distill.00010}
}



Towards Automatic Concept-based Explanations % ACE
@misc{ghorbani2019automaticconceptbasedexplanations,
      title={Towards Automatic Concept-based Explanations}, 
      author={Amirata Ghorbani and James Wexler and James Zou and Been Kim},
      year={2019},
      eprint={1902.03129},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/1902.03129}, 
}


% Use "prototypes" for image classificiation, try to make model architecture inherently interpretable.
% First use self supervision for training, image gets passed to hidden layer, pixels should align.
% In each pixel, each channel is a prototype. Those channels seem to align with interpretable features.
@InProceedings{Nauta_2023_CVPR,
    author    = {Nauta, Meike and Schl\"otterer, J\"org and van Keulen, Maurice and Seifert, Christin},
    title     = {PIP-Net: Patch-Based Intuitive Prototypes for Interpretable Image Classification},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2023},
    pages     = {2744-2753}
}

@inproceedings{deng_imagenet_2009, title = {{ImageNet}: {A} large-scale hierarchical image database}, doi = {10.1109/CVPR.2009.5206848}, booktitle = {2009 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}}, author = {Deng, J. and Dong, W. and Socher, R. and Li, L. and {Kai Li} and {Li Fei-Fei}}, year = {2009}, keywords = {Datasets}, pages = {248--255}, }


% Use a adversarially robust network for feature visualization and inversion, without using any regularization.
% Model checkpoints and code available.
% Also done on ImageNet
@inproceedings{NEURIPS2019_6f2268bd,
 author = {Santurkar, Shibani and Ilyas, Andrew and Tsipras, Dimitris and Engstrom, Logan and Tran, Brandon and Madry, Aleksander},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Image Synthesis with a Single (Robust) Classifier},
 url = {https://proceedings.neurips.cc/paper_files/paper/2019/file/6f2268bd1d3d3ebaabb04d6b5d099425-Paper.pdf},
 volume = {32},
 year = {2019}
}


% resnets
@INPROCEEDINGS{resnets,

  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},

  booktitle={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 

  title={Deep Residual Learning for Image Recognition}, 

  year={2016},

  volume={},

  number={},

  pages={770-778},

  doi={10.1109/CVPR.2016.90}}



% SGD optimization
@article{10.1214/aoms/1177729586,
author = {Herbert Robbins and Sutton Monro},
title = {{A Stochastic Approximation Method}},
volume = {22},
journal = {The Annals of Mathematical Statistics},
number = {3},
publisher = {Institute of Mathematical Statistics},
pages = {400 -- 407},
year = {1951},
doi = {10.1214/aoms/1177729586},
URL = {https://doi.org/10.1214/aoms/1177729586}
}

@InProceedings{10.1007/978-3-7908-2604-3_16,
author="Bottou, L{\'e}on",
editor="Lechevallier, Yves
and Saporta, Gilbert",
title="Large-Scale Machine Learning with Stochastic Gradient Descent",
booktitle="Proceedings of COMPSTAT'2010",
year="2010",
publisher="Physica-Verlag HD",
address="Heidelberg",
pages="177--186",
abstract="During the last decade, the data sizes have grown faster than the speed of processors. In this context, the capabilities of statistical machine learning methods is limited by the computing time rather than the sample size. A more precise analysis uncovers qualitatively different tradeoffs for the case of small-scale and large-scale learning problems. The large-scale case involves the computational complexity of the underlying optimization algorithm in non-trivial ways. Unlikely optimization algorithms such as stochastic gradient descent show amazing performance for large-scale problems. In particular, second order stochastic gradient and averaged stochastic gradient are asymptotically efficient after a single pass on the training set.",
isbn="978-3-7908-2604-3"
}

@inproceedings{paszke2019,
  author    = {Adam Paszke and
               Sam Gross and
               Francisco Massa and
               Adam Lerer and
               James Bradbury and
               Gregory Chanan and
               Trevor Killeen and
               Zeming Lin and
               Natalia Gimelshein and
               Luca Antiga and
               Alban Desmaison and
               Andreas K{\"{o}}pf and
               Edward Z. Yang and
               Zachary DeVito and
               Martin Raison and
               Alykhan Tejani and
               Sasank Chilamkurthy and
               Benoit Steiner and
               Lu Fang and
               Junjie Bai and
               Soumith Chintala},
  editor    = {Hanna M. Wallach and
               Hugo Larochelle and
               Alina Beygelzimer and
               Florence d'Alch{\'{e}}{-}Buc and
               Emily B. Fox and
               Roman Garnett},
  title     = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
  booktitle = {Advances in Neural Information Processing Systems 32: Annual Conference
               on Neural Information Processing Systems 2019, NeurIPS 2019, December
               8-14, 2019, Vancouver, BC, Canada},
  pages     = {8024--8035},
  year      = {2019},
  url       = {https://proceedings.neurips.cc/paper/2019/hash/bdbca288fee7f92f2bfa9f7012727740-Abstract.html}
}

% captum
@misc{kokhlikyan2020captum,
    title={Captum: A unified and generic model interpretability library for PyTorch},
    author={Narine Kokhlikyan and Vivek Miglani and Miguel Martin and Edward Wang and Bilal Alsallakh and Jonathan Reynolds and Alexander Melnikov and Natalia Kliushkina and Carlos Araya and Siqi Yan and Orion Reblitz-Richardson},
    year={2020},
    eprint={2009.07896},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}


@misc{torchvision2016,
  author = "{TorchVision maintainers and contributors}",
  title = "{TorchVision: PyTorch's Computer Vision library}",
  year = {2016},
  url = {https://github.com/pytorch/vision},
  note = {GitHub repository},
  publisher = {GitHub},
  howpublished = {Software},
  license = {BSD-3-Clause}
}

% activation atlas
@article{carter2019activation,
  author = {Carter, Shan and Armstrong, Zan and Schubert, Ludwig and Johnson, Ian and Olah, Chris},
  title = {Activation Atlas},
  journal = {Distill},
  year = {2019},
  note = {https://distill.pub/2019/activation-atlas},
  doi = {10.23915/distill.00015}
}


% scipy
@ARTICLE{2020SciPy-NMeth,
  author  = {Virtanen, Pauli and Gommers, Ralf and Oliphant, Travis E. and
            Haberland, Matt and Reddy, Tyler and Cournapeau, David and
            Burovski, Evgeni and Peterson, Pearu and Weckesser, Warren and
            Bright, Jonathan and {van der Walt}, St{\'e}fan J. and
            Brett, Matthew and Wilson, Joshua and Millman, K. Jarrod and
            Mayorov, Nikolay and Nelson, Andrew R. J. and Jones, Eric and
            Kern, Robert and Larson, Eric and Carey, C J and
            Polat, {\.I}lhan and Feng, Yu and Moore, Eric W. and
            {VanderPlas}, Jake and Laxalde, Denis and Perktold, Josef and
            Cimrman, Robert and Henriksen, Ian and Quintero, E. A. and
            Harris, Charles R. and Archibald, Anne M. and
            Ribeiro, Ant{\^o}nio H. and Pedregosa, Fabian and
            {van Mulbregt}, Paul and {SciPy 1.0 Contributors}},
  title   = {{{SciPy} 1.0: Fundamental Algorithms for Scientific
            Computing in Python}},
  journal = {Nature Methods},
  year    = {2020},
  volume  = {17},
  pages   = {261--272},
  adsurl  = {https://rdcu.be/b08Wh},
  doi     = {10.1038/s41592-019-0686-2},
}


% NMF
@article{Lee1999,
  author = {Daniel D. Lee and H. Sebastian Seung},
  title = {Learning the parts of objects by non-negative matrix factorization},
  journal = {Nature},
  year = {1999},
  volume = {401},
  number = {6755},
  pages = {788--791},
  doi = {10.1038/44565},
  url = {https://doi.org/10.1038/44565},
  issn = {1476-4687},
  abstract = {Is perception of the whole based on perception of its parts? There is psychological and physiological evidence for parts-based representations in the brain, and certain computational theories of object recognition rely on such representations. But little is known about how brains or computers might learn the parts of objects. Here we demonstrate an algorithm for non-negative matrix factorization that is able to learn parts of faces and semantic features of text. This is in contrast to other methods, such as principal components analysis and vector quantization, that learn holistic, not parts-based, representations. Non-negative matrix factorization is distinguished from the other methods by its use of non-negativity constraints. These constraints lead to a parts-based representation because they allow only additive, not subtractive, combinations. When non-negative matrix factorization is implemented as a neural network, parts-based representations emerge by virtue of two properties: the firing rates of neurons are never negative and synaptic strengths do not change sign.}
}



% bcc is similar to trichoblastoma

@article{Patel2020,
  author = {Parth Patel and Shiri Nawrocki and Kelsey Hinther and Amor Khachemoune},
  title = {Trichoblastomas Mimicking Basal Cell Carcinoma: The Importance of Identification and Differentiation},
  journal = {Cureus},
  year = {2020},
  volume = {12},
  number = {5},
  pages = {e8272},
  doi = {10.7759/cureus.8272},
  url = {https://doi.org/10.7759/cureus.8272},
  publisher = {Cureus},
  abstract = {Trichoblastoma is a rare, slow-growing, benign cutaneous tumor derived from follicular germinative cells. Trichoblastoma commonly appears as an asymptomatic, symmetrical, well-circumscribed, skin-colored to brown or blue-black papule or nodule. It may appear clinically and histologically similar to basal cell carcinoma, making its diagnosis challenging. Even on dermoscopy, it is challenging to differentiate trichoblastoma from basal cell carcinoma. In practice, it is important to differentiate the two, because the choice of treatment and resulting prognosis differ between the lesions. Surgical biopsy to analyze histopathological and immunohistochemical differences is the gold standard for diagnosing and differentiating trichoblastoma from basal cell carcinoma. Trichoblastoma typically has a favorable prognosis, with a low incidence of recurrence, progression or association with malignancy. This paper provides a review of the epidemiology, clinical presentation, dermoscopy, histology, immunochemistry, treatment, and prognosis of trichoblastoma.},
  pmid = {32596088},
  pmc = {PMC7314372}
}


