% Template for ICME 2022 paper; to be used with:
%          spconf.sty  - ICASSP/ICIP/ICME LaTeX style file, and
%          IEEEbib.bst - IEEE bibliography style file.
% --------------------------------------------------------------------------
\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
	
\usepackage{amsmath}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\usepackage{siunitx}
\usepackage{amsfonts}
\usepackage[nocompress]{cite}
\usepackage{booktabs,multirow}
\usepackage{color}
\usepackage{tabularx}
\usepackage{orcidlink}
\usepackage{hyperref}
\usepackage{amssymb}
\hypersetup{
	colorlinks=true,
	linkcolor=blue, % 链接颜色
	citecolor=green,  % 引用颜色
	urlcolor=blue   % URL 颜色
}

% Example definitions.
% --------------------
\def\x{{\mathbf x}}
\def\L{{\cal L}}


\pagestyle{empty}


\title{DeltaDiff: A Residual-Guided Diffusion Model for Enhanced Image Super-Resolution}

%
% Single address.
% ---------------
% \author{\IEEEauthorblockN{Chao Yang\orcidlink{0009-0009-1378-0644}}
% % \IEEEauthorblockA{Southwest University \\of Science and Technology, China\\
% % Email: yangchao@mails.swust.edu.cn}
% % \and
% \IEEEauthorblockN{Yong Fan}
% % \IEEEauthorblockA{Southwest University \\of Science and Technology, China\\
% % Email: fanyong@mails.swust.edu.cn}
% \and
% \IEEEauthorblockN{ ZhiJing Yang\orcidlink{0009-0000-9865-0666}}
% % \IEEEauthorblockA{Southwest University \\of Science and Technology, China\\
% % Email: yangczhijing@mails.swust.edu.cn}
% % % \and
% \IEEEauthorblockN{ Cheng Lu\orcidlink{0009-0006-5324-5361}}
% % \IEEEauthorblockA{Southwest University \\of Science and Technology, China\\
% % Email: lucheng@mails.swust.edu.cn}}
% 
%
%
\author{
	\IEEEauthorblockN{Chao Yang\orcidlink{0009-0009-1378-0644},
		Yong Fan$^*$,
		Cheng Lu\orcidlink{0009-0006-5324-5361},
		Zhijing Yang\orcidlink{0009-0000-9865-0666}}
	\IEEEauthorblockA{Southwest University of Science and Technology, China}
}


% }
% \author{{Anonymous ICME Submission}

% \\
%     \address{Anonymous ICME Submission}}
% % For example:
% ------------
%\address{School\\
%	Department\\
%	Address}
%
% Two addresses (uncomment and modify for two-address case).
% ----------------------------------------------------------
%\twoauthors
%  {A. Author-one, B. Author-two\sthanks{Thanks to XYZ agency for funding.}}
%	{School A-B\\
%	Department A-B\\
%	Address A-B}
%  {C. Author-three, D. Author-four\sthanks{The fourth author performed the work
%	while at ...}}
%	{School C-D\\
%	Department C-D\\
%	Address C-D}
%

\begin{document}
%\ninept
%y utlize
%\maketitle
\maketitle

\begin{abstract}
Recently, the application of diffusion models in super-resolution tasks has become a popular research direction. Existing work is focused on fully migrating diffusion models to SR tasks. The diffusion model is proposed in the field of image generation, so in order to make the generated results diverse, the diffusion model combines random Gaussian noise and distributed sampling to increase the randomness of the model. 
 However, the essence of super-resolution tasks requires the model to generate high-resolution images with fidelity. Excessive addition of random factors can result in the model generating detailed information that does not belong to the HR image. To address this issue, we propose a new diffusion model called Deltadiff, which uses only residuals between images for diffusion, making the entire diffusion process more stable. The experimental results show that our method surpasses state-of-the-art models and generates results with better fidelity. Our code and model are publicly available at \href{https://github.com/continueyang/DeltaDiff} {https://github.com/continueyang/DeltaDiff}

\end{abstract}
\begin{IEEEkeywords}
Residual Diffusion, Super-Resolution, Diffusion Model.
\end{IEEEkeywords}


\section{Introduction}
\label{sec:intro}

Image super-resolution (SR) is a fundamental problem in computer vision that aims to recover high-resolution (HR) images from low-resolution (LR) counterparts. SR tasks require not only the generation of visually pleasing images, but also the reconstruction of accurate HR images that correspond to the given LR inputs. However, because of the irreversibility of the degradation process and the complexity and unknown properties of the degradation kernels in real-world scenarios. SR tasks are an ill-posed problem, and an LR image can correspond to many HR images. These issues make it a long-standing and challenging research field in low-level visual tasks.\cite{zs1,dropyang}.

Currently, diffusion models have gained widespread attention in the field of computer vision due to their exceptional image generation capabilities. Consequently,
\begin{figure}[htb]
  \centering
  \centerline{\includegraphics[width=9.5cm,height=6.5cm]{img1.pdf}}
  %\vspace{-20pt}
  \caption{introduce the relationships between different models and tasks. (a) The image generation task requires the model to generate an image that conforms to human perception from pure Gaussian noise, and does not require the image to actually exist\cite{transgan}. (b) The forward process of the diffusion model, in which the image gradually degrades over multiple time steps by adding random Gaussian noise, ultimately becoming pure Gaussian noise. After learning the distribution of a certain image class, the reverse process gradually generates an image from a Gaussian noise.\cite{ddpm}. (c) The SR task aims to generate a HR image from a LR input\cite{ica1}. (d) We designed a forward diffusion process that starts with a HR image, diffuses through residual, and eventually results in a blurred, LR image.}
  \label{fig:1}
\end{figure}
they have been increasingly applied to SR tasks. Current approaches for applying diffusion models to SR can be divided into two main paradigms. The first involves upsampling the LR images by interpolation before feeding them into the diffusion model\cite{resshift,sinsr,idm}. During the forward diffusion process, random noise is progressively added to the upsampled images, and in the reverse diffusion process, the model removes the noise to recover sharp images. The second paradigm uses LR images as conditional inputs to guide the diffusion process\cite{ldm,diffsr}. 

However, the methods mentioned above all preserve the diffusion process of the diffusion model intact. However, since the diffusion model itself is designed for image generation tasks, many mechanisms are designed to increase the diversity of the model's generated results. Directly transferring the entire model will result in too many unrealistic details in the output results that do not belong to the HR image. This does not achieve the goal of the SR task.

In order to better apply the diffusion model to SR tasks, many works have begun to introduce the residual between LR and HR images into the diffusion model\cite{res1,res2,resshift}. These works incorporate residuals and random noise as guidance while maintaining the original diffusion model framework. This modification of the "SRifying" process achieved better SR results. This phenomenon has prompted us to consider how to develop a diffusion model framework that is more suitable for SR tasks.
\begin{figure*}[htb]
  \centering
  \includegraphics[width=0.95\textwidth,height=0.5\textwidth]{img_A.pdf}
  \caption{The forward and reverse processes of Deltadiff proposed. The forward process subtracts a portion of the residual from the HR image at each step, while the backward process is the inverse of this process. The denoising module uses Unet\cite{unet}.}
  \label{fig:2}
\end{figure*}
 The traditional diffusion model's forward process involves gradually introducing random noise into the target image distribution, ultimately transitioning to a pure Gaussian noise distribution. The reverse process starts with Gaussian noise and progressively reverses the forward steps, reducing the noise and restoring the original image. Although this process is mathematically rigorous and well suited for image generation tasks, but at the same time, it inherently aims to increase diversity and randomness. As shown in (a) and (b) of {\bf Fig.}\ref{fig:1}. However, in SR tasks, the objective is to generate HR images that accurately correspond to the LR inputs. The diversity of output results is not required in SR tasks. Therefore, we have designed a new diffusion process specifically for SR tasks. Only using image residuals to diffuse from LR to HR images. Building on this idea and the baseline models from previous work, we propose DeltaDiff, a novel diffusion model for SR tasks. Unlike previous reverse processes, the Deltadiff diffusion process is more in line with the objectives of SR tasks. The starting point of its forward process is the HR image, and the ending point is the LR image. The reverse process is the inverse process of the forward process, generating HR images by inputting LR images. This is completely consistent with the SR process. The diffusion process designed in this way is more stable and the SR details will not deviate from the HR image, as shown in (c) and (d) of {\bf Fig.}\ref{fig:1}. Through quantitative and visual comparison experiments with state-of-the-art models, we demonstrate that DeltaDiff enhances the model SR capability while preserving the authenticity of the SR images. 

In summary, the contributions of this paper are as follows.

1)This paper developed a diffusion model for SR, DeltaDiff, whose diffusion process requires only four steps. 

2)This paper introduces an innovative diffusion process through image residuals, eliminating random noise and sampling mechanisms.

3)Through  quantitative and visual experiments, this paper validates the effectiveness of the DeltaDiff model for SR.





\section{METHODOLOGY}


\label{sec:pagestyle}

 \noindent This section presents our proposed DeltaDiff method. This approach is built upon the Resshift model and involves a diffusion operation based on the residual between the LR and HR images, ensuring that the generated images maintain high fidelity. {\bf Fig.}\ref{fig:2} illustrates the overall framework of the method.
\begin{table*}[htb] 

	 \caption{Quantitative comparison with state-of-the-art methods on benchmark
datasets. The top two results are marked in \textcolor{red}{red} and \textcolor{blue}{blue}}
        \vspace{1em}

	\centering


	\begin{tabular}{r|c|c|c|c|c|c|c|c|c}
		\hline
		\multirow{2}*{Method}  & \multicolumn{3}{c|}{Set14\cite{set14}} & \multicolumn{3}{c|}{BSDS100\cite{bsds}} & \multicolumn{3}{c}{Urban100\cite{urban}} \\

		  & PSNR & SSIM & LPIPS & SSIM & PSNR &  LPIPS & PSNR & SSIM & LPIPS \\
		\hline
		    %RCAN(2018)[]  & X2  & 38.27& 0.9614 &  34.12& 0.9216 & 32.41 & 0.9027 & 33.34 & 0.9384 & 39.44 & 0.9786  \\

            BSRGAN(2021)\cite{bsrgan}   & 19.82 & 0.5165 & 0.3130 & 21.22 & 0.5179 & 0.3219 & 18.28 & 0.5229 & 0.3159  \\

            SwinIR(2021)\cite{ir}  & 19.33 & 0.5176 & 0.3062 & 20.70 & 0.5118 & 0.3146 & 17.73 & 0.5278 & 0.2936  \\

            
            DAT(2023)\cite{dual}  & 18.59 & 0.5450 & 0.4028 & 19.75 & 0.5445&0.3965 & 19.75 & 0.5445 & 0.3190   \\
            
            ResShift(2023)\cite{resshift}& \textcolor{blue}{24.19} & \textcolor{blue}{0.6660} & \textcolor{red}{0.2701} & \textcolor{blue}{24.58} & \textcolor{blue}{0.6228}& \textcolor{blue}{0.2990} & \textcolor{blue}{22.54} & \textcolor{blue}{0.6676} & \textcolor{blue}{0.2710}   \\            
            
            DeltaDiff(ours)&\textcolor{red}{24.58} & \textcolor{red}{0.6766}  & \textcolor{blue}{0.2709}  & \textcolor{red}{25.08}  & \textcolor{red}{0.6382}  & \textcolor{red}{0.2927}  & \textcolor{red}{22.88}  & \textcolor{red}{0.6757}  & \textcolor{red}{0.2706}   \\

            
		\hline
	\end{tabular}
\label{table:1}
\end{table*}


\begin{figure*}[htb]
  \centering
  \includegraphics[width=0.95\textwidth,height=0.3\textwidth]{img3.pdf}
  \caption{Compared with the state-of-the-art 4x magnification SR visualization algorithm, please zoom in on the image for details.}
  \label{fig:3}
\end{figure*}
\noindent{\bf Problem Statement}.
Firstly, to introduce the changes made by Deltadiff,  let us review the traditional diffusion model. In previous methods, the diffusion model applies Gaussian noise to the image through a Markov process, the forward Markovian diffusion process $q$ defined as follows:

\begin{equation}
 q\left(\mathbf{y}_{1: T} \mid \mathbf{y}_0\right)=\prod_{t=1}^T q\left(\mathbf{y}_t \mid \mathbf{y}_{t-1}\right) \end{equation}
 Here, $\mathbf {y}_0$ is the input image,where the transition probability at each step of the Markov chain is:
 \begin{equation}
  q\left(\mathbf{y}_t \mid \mathbf{y}_{t-1}\right)=\mathcal{N}\left(\mathbf{y}_t \mid \sqrt{1-\beta_t} \mathbf{y}_{t-1}, \beta_t \mathbf{I}\right)\end{equation}

Here, $\beta_t \in (0,1)$represents the variance of the Gaussian noise across $T$ iterations. $\mathbf{y}$ is the given input image.
During the reverse diffusion process, the model learns the conditional distribution $p_{\theta}(\mathbf{y}_{t-1} \mid \mathbf{y}_t, x)$ and samples it, gradually denoising the latent features. The inference process can be viewed as a reverse Markov process that progressively reconstructs $y_0$ from Gaussian noise $\mathbf{y}_T$:
\begin{equation}
\begin{aligned}
p_\theta\left(\mathbf{y}_{0: T} \mid \mathbf{x}\right) & =p\left(\mathbf{y}_T\right) \prod_{t=1}^T p_\theta\left(\mathbf{y}_{t-1} \mid \mathbf{y}_t, \mathbf{x}\right) \\
p\left(\mathbf{y}_T\right) & =\mathcal{N}\left(\mathbf{y}_T \mid \mathbf{0}, \mathbf{I}\right) \\
p_\theta\left(\mathbf{y}_{t-1} \mid \mathbf{y}_t, \mathbf{x}\right) & =\mathcal{N}\left(\mathbf{y}_{t-1} \mid \mu_\theta\left(\mathbf{x}, \mathbf{y}_t, t\right), \sigma_t^2 \mathbf{I}\right)
\end{aligned}
\end{equation}
The above process describes how to generate a HR image starting from pure Gaussian noise. 

\vspace{10pt}
\noindent {\bf Model design.} Initially, the diffusion model was designed for image generation tasks, where the goal is to learn the distribution of a specific type of image. Once the model learns this distribution, it can generate images by sampling from it during inference. However, in SR tasks, each LR image corresponds to a unique HR counterpart. The use of sampling and random noise can cause details in the SR images that do not exist in the HR image. This challenge motivated us to develop a more stable diffusion process.
\\

\noindent {\bf Forward process}. Inspired by \cite{resshift,sinsr}, we only use the residual between HR and LR images as the core of our diffusion process to improve image resolution. Unlike previous methods, due to the absence of Gaussian noise in the diffusion process, each step in our forward and reverse processes does not follow a Gaussian distribution. Our diffusion process does not target a certain Gaussian distribution, but rather a specific image. Specifically, given an LR-HR image pair, our forward process begins with the HR image $\mathbf{y}_T$ and ends at the LR image $\mathbf{y}_0$
. The forward process is defined as follows:
\begin{equation}
\mathbf{y}_{t-1} =  \mathbf{y}_0 + \eta_t \cdot (\mathbf{y}_t - \mathbf{y}_0)
\label{eq:4}
\end{equation}
Here, $\eta$ is a time-dependent factor, such that $\eta_0 \rightarrow 0$ (a hyperparameter) and $\eta_t \rightarrow 1$. When $\eta_t \rightarrow 1$, the output is equivalent to the HR image.

\vspace{10pt}

\noindent {\bf Reverse process.} The reverse process fully conforms to the SR process of input LR and output HR. Each step of our reverse process targets images containing different residual information in each step of the forward process. Reconstruct partial residuals at each step of the reverse process. When inputting a blurry image, the reverse process can gradually transition to the final clear image. This process without introducing random factors and distributed sampling greatly ensures the authenticity of the SR image. The definition of the entire reverse process is as follows.



\begin{equation}
\mathbf{y}_{t-1} = \frac{\eta_{t-1}}{\eta_{t}} \cdot \mathbf{x}_{t} + \frac{\alpha_{t}}{\eta_{t}} \cdot \mathbf{y}_{t-1}
\end{equation}

\begin{figure*}[htb]
  \centering
  
  \includegraphics[width=0.9\textwidth,height=0.25\textwidth]{img7.pdf}
  \caption{Comparison of visualization experiments on forward and backward diffusion processes. Compared to previous versions, our diffusion process is more stable and the result is closer to the HR image.}
  \label{fig:4}
\end{figure*}




Here, $\mathbf{x}_0$ is the input of the LR image during inference. The sequence
$\{\eta_t\}_{t=1}^{T}$ is derived from a moving sequence used in Reshift\cite{resshift}, which we have adopted. This sequence is increasing monotonically, $ \alpha_{t}$ is the difference between the sequence moving at $ \eta_t$ and the previous $ \eta_{t-1}$, $ \alpha_{t}= \eta_t- \eta_{t-1}$.
From these equations, we can derive the final model output $\mathbf{y}_0$ as follows:
\begin{equation}
\mathbf{y}_0 = \frac{\eta_0}{\eta_1} \cdot \mathbf{x}_0 + \frac{\alpha_1}{\eta_1} \cdot \mathbf{y}_1
\end{equation}
This diffusion process only requires four steps to obtain the SR output $\mathbf{y}_1$. 

Due to the fact that the input and output sizes of the diffusion model are the same, HR input can result in extremely high computational resource consumption. To solve this problem, our model adopts the commonly used dimensionality reduction method in diffusion models. Map the input to the hidden space of VQGAN\cite{vqgan} using its encoder, then obtain the output, and restore the image using a decoder. Due to the downsampling process during the mapping process, this greatly alleviates the problem of computational resource consumption. It should be noted that the weights of VQGAN are pre-trained and frozen throughout the training process.

\vspace{10pt}

\noindent {\bf Scheduling Strategy.} We utilized a noise scheduling strategy in diffusion models that allows SR in only four diffusion steps. The adopted schedule is as follows:
\begin{align}
\eta_t &= \sqrt{\eta_1} \times b_0^{\alpha_t}, \quad t = 2, \dots, T-1 \notag \\
\alpha_t &= \left( \frac{t - 1}{T - 1} \right)^p \times (T - 1) \notag \\
b_0 &= \exp \left( \frac{1}{2(T - 1)} \log \frac{\eta_T}{\eta_1} \right)
\end{align}

This design ensures that the degradation process within our diffusion framework increases monotonically with time step $t$ . Meanwhile , the control of the parameter $\eta$ provides the ability to control the starting and ending points of diffusion expansion, which makes it possible to balance the fidelity realism of the SR results. We have demonstrated this in the ablation experiment.
\section{Experiments}
\subsection{Experimental Setup}
\label{ssec:subhead}

In this section, we validate the effectiveness of DeltaDiff by designing quantitative and visual experiments compared to current state-of-the-art algorithms. We used the DIV2K dataset \cite{div} for training, randomly cropping it into 256x256 image patches as input.  Adam\cite{adam} optimizer was used and the code was implemented in the Pytorch\cite{pytorch} framework. We set the batch size to 64 and the learning rate to 5e-5. All experiments were performed on a single NVIDIA RTX A6000 GPU (48GB). Quantitative performance evaluation was performed using PSNR\cite{psnr}, SSIM\cite{psnr} and LPIPS\cite{lpips} metrics (calculated based on the Y channel). For more details on the experimental setup, please refer to our open-source code.

\subsection{Ablation Study}



We conducted ablation experiments to investigate the impact of two hyperparameters in the diffusion process, $\eta_t$ and $\eta_0 $. $\eta_0$ determines the end point of the forward diffusion process. According to {\bf Eq.}\ref{eq:4}, when $\eta_0 \rightarrow 0$, the forward process starts with the LR image. $\eta_t$ determines the starting point of the forward process. When $\eta_t\rightarrow 1$ ,the result of the forward diffusion process approaches the HR image. To test the effects of different starting and ending points, we performed experiments on the three metrics using four different settings for  $\eta_t$ and  $\eta_0$, as shown in {\bf Table.}\ref{table:2}. The results were optimal when  $\eta_t=0.01$ and  $\eta_0=0.99$. Adjusting the endpoint $\eta_t$ led to significant image degradation, indicating that the model was unable to restore true high definition images. Adjusting the starting point $\eta_t$ increased the amount of information received by the model, which reduced the learning difficulty, but resulted in slightly lower generation quality.
And when we adjust $\eta_0=0.999$, it is closer to the HR image. Although $\eta_t=0.5$, the PSNR and SSIM metrics still improved, indicating that the generated images are closer to HR images. But at the same time, the perceptual indicator LPIPS, which represents generalization ability, showed a slight decrease.

\begin{table}[h]
\caption{Experimental ablation of hyperparameters $\eta_t$ and $\eta_1$ proposed during the forward process.}
    \centering
    

    \begin{tabular}{c|c|c|c|c}
   
    \hline
    $\eta_t$  & $\eta_0$ & PSNR  & SSIM &LPIPS \\ \hline
    0.01            & 0.8 & 23.73             & 0.6379 &0.3897           \\ \hline
    0.01             & 0.99       & 24.58             & 0.6782&0.2709           \\ \hline
    0.2             & 0.99       & 24.34             & 0.6799&0.2784           \\ \hline
    0.5             & 0.999       & 24.68             & 0.6950&0.2767           \\ \hline
    
    \end{tabular}
    
    \label{table:2}
    
\end{table}
We provide visual ablation experiments comparing the forward and reverse diffusion processes between DeltaDiff and previous diffusion processes. The results are shown in {\bf Fig.}\ref{fig:4}. From the experimental results, it can easily be observed from the experimental results that the starting point of the forward diffusion process is the HR image, while the end point is Gaussian noise. It is different from the previous diffusion process. The starting point of deltadiff is the HR image, and the ending point is the LR image. The starting point of the reverse process was noise, and the end point was the HR image. The starting point of deltadiff is the LR image. Meanwhile, we observe that Resshift diffusion processes, which rely on noise and sampling mechanisms, diffuse images into noisy representations. In the reverse diffusion process, this resembles an image generation process, but it fails to fully utilize the information from the LR images, potentially leading to the generation of fictitious details. DeltaDiff’s diffusion process is more stable, without the addition of noise or the use of sampling mechanisms, allowing for the SR result of more authentic information based on the LR images.



\subsection{Comparison with State-of-the-Art Methods}
\label{ssec:subhead3}
To validate the SR capabilities of our method in various types of images, we selected three datasets, Set14\cite{set14}, BSDS\cite{bsds}, and Urban\cite{urban} as our test sets. These datasets encompass a variety of real-world restoration targets. For consistency, we unified the sizes of the test sets to either 256x256 or 512x512. We compared our model with the current state-of-the-art SR algorithms, including BSRGAN\cite{bsrgan}, SwinIR\cite{ir}, DAT\cite{dual}, and Resshift\cite{resshift}, with quantitative results reported in {\bf Table.}\ref{table:1}. Compared to these advanced algorithms, DeltaDiff achieved the best overall results in all three evaluation metrics. This performance improvement did not come at the cost of increased computational load or model parameters relative to baseline. However, even so, in terms of parameter count, compared to non-diffusion models, Deltadiff still has a parameter count of over 100M. Compared to the previous commonly used SR algorithms of 10M+, this performance consumption is still significant.
	\begin{figure}[htb]
		\centering
		\centerline{\includegraphics[width=8.5cm,height=6.5cm]{img8.pdf}}
		%\vspace{-20pt}
		\caption{The visualization experiment focuses on the SR capability of details, and zooming in can reveal the degree of difference in details between HR images. The previous diffusion model generates details that do not belong to the HR image, such as (c).}
		\label{fig:8}
	\end{figure}


To demonstrate the SR capability of our algorithm. We also performed visual comparison experiments on the SR quality of our model against the state-of-the-art algorithms, with the results shown in {\bf Fig.}\ref{fig:4}. Compared to non-diffusion-based methods, our approach produced the best SR results. Compared to other diffusion-based models, our method generated results that were closer to the true HR images. Zooming in to examine the details of the images shows that the images generated by our model contain less fictional information than the images generated by the previous diffusion model. For example, the patterns on the butterfly wings and the body details of the fish in our images. 

To verify whether our algorithm improves the authenticity of SR results, we compared our algorithm with other algorithms in detail, and the comparison results are provided in {\bf Fig.}\ref{fig:8}. By comparing the details, we can find that the previous GAN based network and Transformer based network (such as BSRGAN\cite{bsrgan}, SwinIR\cite{ir}) lack image details in the SR results. However, algorithms that use diffusion models (such as the Resshift of the baseline model\cite{resshift}) may have details, but these details do not come from HR images, causing the SR results to lose authenticity. Compared with the above methods, our algorithm has achieved good preservation of details and authenticity. We provide more results of our generated images on our open-source website.



        





% Below is an example of how to insert images. Delete the ``\vspace'' line,
% uncomment the preceding line ``\centerline...'' and replace ``imageX.ps''
% with a suitable PostScript file name.
% -------------------------------------------------------------------------



% To start a new column (but not a new page) and help balance the last-page
% column length use \vfill\pagebreak.
% -------------------------------------------------------------------------
%\vfill
%\pagebreak

\section{Conclusion}
\label{sec:copyright}

Existing work that applies diffusion models to SR tasks suffers from retaining random Gaussian noise and sampling mechanisms, resulting in generated images that deviate from true HR information. Our analysis shows that directly applying diffusion models for image generation tasks to the SR domain is suboptimal. We designed a new diffusion process that transitions from LR images to HR images using image residuals. Our diffusion process targets the combination of images and residuals at each step, without introducing noise factors that would increase the randomness of the reconstruction results. Through quantitative and visual comparisons with state-of-the-art models, we demonstrated that our method can effectively enhance reconstruction capabilities while ensuring authenticity.

  Our study not only shares the work we have done, but also aims to explore and reflect on a broader question: when combining diffusion models with downstream tasks, is it necessary for the diffusion process to involve Gaussian noise? For example, in real-world SR tasks, could different types of degradation be incorporated into the diffusion process? Due to our limited resources, the designed process in this work may not be the optimal solution, but we hope to see more rational diffusion processes proposed and explored by researchers in the future.


\vfill\clearpage






% References should be produced using the bibtex program from suitable
% BiBTeX files (here: strings, refs, manuals). The IEEEbib.bst bibliography
% style file from IEEE produces unsorted bibliography list.
% -------------------------------------------------------------------------

\bibliographystyle{IEEEbib}
\bibliography{references}

\end{document}
