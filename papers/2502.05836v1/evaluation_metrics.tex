\section{Evaluation Metrics}
To evaluate the performance of models, we adopt a set of standard metrics commonly used in classification tasks. For each sentence in the dataset, the predicted label (rhetorical role) is considered correct if it matches the label assigned by the human expert annotator.

We utilize macro-averaged Precision, Recall, F-score, Accuracy, and Matthew Correlation Coefficient (MCC) \citet{chicco2020advantages} as our primary evaluation metrics. Macro-averaging involves calculating these metrics for each class separately and then taking their average. This method is particularly beneficial as it prevents bias towards high-frequency classes, ensuring that all rhetorical roles are treated equally in the evaluation process.