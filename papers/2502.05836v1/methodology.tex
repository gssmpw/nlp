\section{Methodology}
This section outlines the methodology employed for the task of semantic segmentation of legal documents via rhetorical roles. We implemented several SoTA methods while also exploring new techniques. These methodologies collectively aim to enhance the model's ability to understand and classify rhetorical roles in legal texts by incorporating structural, contextual, and sequential information. Each technique addresses different aspects of the complex relationships between sentences in legal documents, contributing to more accurate and context-aware classification outcomes.

\subsection{TransformerOverInLegalBERT (ToInLegalBERT)}
This pipeline is inspired by \citet{marino2023automatic}. While they employed a general-purpose BERT model, we utilized InLegalBERT, a transformer pre-trained specifically on the Indian legal domain. This substitution enhances the model’s ability to capture domain-specific nuances, resulting in improved performance. The TransformerOverInLegalBERT (ToInLegalBERT) model follows a hierarchical architecture, consisting of four main components: (i) an InLegalBERT token-level encoder, (ii) a sentence-level positional encoder, (iii) a sentence-level encoder, and (iv) a prediction layer.

The process begins by splitting the document into sentences and tokenizing them.  Each sentence is then input into the ToInLegalBERT token-level encoder, where the pooled output—specifically, the hidden representation of the [CLS] token—is extracted. These pooled outputs are subsequently fed into the positional layer to create a position-dependent encoding for each sentence within the document. The encoded representations are then passed to the sentence-level encoder, which captures the relationships between sentences in the document, and finally, these outputs are directed to the prediction layer for rhetorical role classification. This method incorporates both the local context of sentences and their position in the document, enabling better rhetorical role classification. By incorporating both the local context of sentences and their position in the document, this method enables improved rhetorical role classification by effectively modeling the hierarchical structure of legal texts.

\subsection{Hierarchical BiLSTM CRF Classifier}
We also implemented the BiLSTM-CRF model proposed by \citet{bhattacharya2019identification}, which combines Bidirectional Long Short-Term Memory (BiLSTM) with a Conditional Random Field (CRF) layer. The input to this model is sentence embeddings generated using a sent2vec model trained on Indian Supreme Court judgments. These sentence embeddings are passed through a BiLSTM model, which captures sequential dependencies between sentences. The CRF layer on top of the BiLSTM ensures that the predicted rhetorical role labels adhere to the structured nature of legal documents. This model predicts the rhetorical role for each sentence by considering the context provided by neighboring sentences.

\subsection{Multi-Task Learning (MTL)}
Inspired by the Multi-Task Learning framework proposed by \citet{malik-etal-2022-semantic}, we adopt an MTL approach where rhetorical role prediction is the main task, and label shift prediction serves as the auxiliary task. The model consists of two components: a label shift detection component and a rhetorical role prediction component. The intuition is that the label shift between sentences (indicating a change in rhetorical role) helps improve role classification. The label shift detection component predicts whether a shift in rhetorical role occurs at the \(i^{th}\) sentence, while the rhetorical role classification component predicts the rhetorical role for that sentence. The output from both components is concatenated and passed to the CRF layer for final role predictions. The overall loss function for the MTL model is:
$L = \lambda L_{\text{shift}} + (1-\lambda) L_{\text{RR}},$
where \(L_{\text{shift}}\) corresponds to label shift prediction, \(L_{\text{RR}}\) corresponds to rhetorical role classification, and \(\lambda\) is a hyperparameter balancing the two tasks. This method allows the model to learn dependencies between sentences more effectively.

\subsection{InLegalBERT Variants}
We experimented with different configurations of the InLegalBERT~\citet{paul-2022-pretraining} model to improve performance. These configurations vary in terms of the number of sentences provided as input during training and inference:
\begin{itemize}
    \item InLegalBERT(i): The model is trained and tested using only the current sentence \( i \).
    \item InLegalBERT(i-1, i): The model is trained with the previous sentence \( i-1 \) and the current sentence \( i \).
    \item InLegalBERT(i-2, i-1, i): The model is trained using the previous two sentences \( i-2, i-1 \) and the current sentence \( i \).
    \item InLegalBERT(i-1, i, i+1): The model is trained with the previous sentence, current sentence, and the next sentence.
\end{itemize}

\subsection{Incorporate Previous Sentence and Label}
We further explored methods where we provide the model with additional contextual information. In one variant, we concatenate the current sentence with the previous sentence and the true label of the previous sentence during training. This approach allows the model to leverage contextual information from preceding sentences to make better predictions. Another variant replaces the true label with the predicted label of the previous sentence during inference, simulating real-world conditions where true labels are unavailable. This method helps the model handle prediction errors and learn sequential dependencies between rhetorical roles.

\subsection{Self-Supervised Pre-Training with Role-Aware Transformers}

We propose a novel Role-Aware Transformer, which extends the standard transformer architecture by integrating role embeddings to represent rhetorical roles such as Facts, Issues, Arguments, and Reasoning. The model is pre-trained in a self-supervised manner on a large corpus of legal documents, allowing it to learn structural and contextual dependencies in legal discourse.

During pre-training, the model predicts masked tokens while leveraging sentence-level role embeddings. Unlike standard transformers, which process sentences without explicit role awareness, our approach incorporates additional role-specific information into the input embeddings. Specifically, each token embedding is enriched with a learned role embedding that represents its rhetorical role, allowing the model to develop a deeper understanding of legal text organization. This enhances the ability to distinguish between similar rhetorical roles and improves overall classification performance.

For pre-training, we initialize the model with InLegalBERT, a transformer specifically pre-trained on Indian legal documents. By incorporating rhetorical role awareness, this method enables the model to better capture the discourse structure of legal texts, leading to more accurate and context-aware classification outcomes.

% \subsection{Self-Supervised Pre-Training with Role-Aware Transformers}
% We propose a novel method, Role-Aware Transformer model, a variant of the standard transformer architecture that incorporates role embeddings to represent different rhetorical roles (e.g., Facts, Issues, Arguments). The model is pre-trained in a self-supervised manner on a large corpus of legal documents. During pre-training, it predicts masked tokens while considering the rhetorical role of each sentence, thereby learning how these roles influence language within legal contexts.

% We experimented with different variants, including BERT, InLegalBERT, and InLegalBERT, with label definitions. Additionally, we handled class imbalance by using class weights in the loss function to ensure that minority classes are given higher importance during training.

\subsection{GNN with Document Context}
To capture the structural relationships between sentences, we propose a method that leverages Graph Neural Networks (GNNs). In this approach, each sentence in a document is represented as a node in a graph, and the edges between nodes are based on sentence order or semantic similarity. Sentence embedding generated via InLegalBERT, a pre-trained language model on the Indian legal domain, serves as a node feature. The GNN processes the graph by propagating information between connected sentences, allowing the model to capture both local and global contextual dependencies. The GNN processes this graph, allowing for information propagation and aggregation across connected sentences, which enhances understanding of interdependencies between sentences.

\subsection{\texttt{RhetoricLLaMA}}
To leverage the power of LLMs for rhetorical role prediction, we implemented \texttt{RhetoricLLaMA}, an instruction-tuned model based on LLaMA-2-7B \citet{touvron2023llama}. For this specific task, we fine-tuned the LLaMA-2-7B model on our \texttt{LegalSeg} dataset using instruction-tuning, a method designed to guide the model’s understanding of specific tasks through a set of structured instructions.

To enhance the model's ability to segment legal documents accurately, we developed a set of 16 instruction sets tailored to the nature of rhetorical role classification in legal texts. These instructions provided the model with explicit guidance on how to handle the different rhetorical roles in a legal document. A complete list of these instruction sets can be found in Table~\ref{tab:instruction_sets} in the Appendix.

% Instruction-tuning allows the model to comprehend the structured nature of legal texts better, improving its contextual understanding of complex relationships between sentences. The key idea behind instruction-tuning is to shape the model’s behavior by providing explicit directions, making it more adept at handling the intricacies of rhetorical roles in legal documents. Through this process, \texttt{RhetoricLLaMA} is trained not only to process large volumes of legal data but also to maintain a high degree of accuracy in identifying and labeling the rhetorical roles in each sentence.

% This approach showcases the potential of instruction-tuned LLMs to excel in specialized tasks within legal NLP, such as rhetorical role classification. By combining the capabilities of LLaMA-2-7B with domain-specific instruction sets, \texttt{RhetoricLLaMA} stands out as a powerful tool for improving performance in complex legal document processing. The use of large pre-trained models with instruction-tuning demonstrates how LLMs can be fine-tuned for highly specialized applications, delivering state-of-the-art results in the segmentation and classification of legal texts.

% To evaluate the performance of the fine-tuned model, we tested it not only on our own test data but also on publicly available datasets from \citet{malik-etal-2022-semantic}  and \citet{marino2023automatic}. This comparative analysis allows us to assess how well our LLM-based approach generalizes across different legal contexts and datasets, ultimately contributing to improved accuracy in classifying rhetorical roles within legal texts. By leveraging the capabilities of LLaMA-2-7B and fine-tuning it with relevant instructions, we aim to enhance the model's performance in understanding and segmenting legal documents effectively.
