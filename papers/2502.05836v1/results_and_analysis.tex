\section{Results and Analysis}
\begin{table}[t]
\centering
\resizebox{\linewidth}{!}{%
\begin{tabular}{lccccc}
\toprule
\textbf{Model}                  & \textbf{Precision} & \textbf{Recall} & \textbf{F1-Score} & \textbf{Accuracy} & \textbf{MCC} \\ 
\midrule
MTL                             & 0.59               & 0.40            & 0.37              & 0.41              & \textbf{0.78}            \\ 
GNN                             & 0.64               & 0.50            & 0.54              & \textbf{0.64}              & 0.40         \\ 
Role-Aware                      & 0.21               & 0.20            & 0.14              & 0.50              & 0.04            \\ 
ToInLegalBERT                   & 0.67               & 0.60            & 0.62              & \textbf{0.64}              & 0.52         \\ 
LLaMA-2 (Quantized)   & 0.17                  & 0.16               & 0.09                 & 0.20                 & 0.3            \\ 
LLaMA-2 (Unquantized)   & 0.19                  & 0.15               & 0.08                 & 0.25                 & 0.05            \\
\texttt{RhetoricLLaMA}                   & 0.19               & 0.15            & 0.09              & 0.39              & 0.02         \\ 
InLegalBERT(i)                  & 0.57               & 0.45            & 0.49              & 0.53              & 0.45         \\ 
InLegalBERT(i-1, i)             & 0.60               & 0.53            & 0.55              & 0.57              & 0.50         \\ 
InLegalBERT(i-2, i-1, i)        & 0.62               & 0.56            & 0.58              & 0.59              & 0.52         \\ 
InLegalBERT(i-1, i, i+1)        & 0.61               & 0.56            & 0.58              & 0.59              & 0.52         \\ 
InLegalBERT(i-1, label\_t, i)   & 0.63                  & 0.32               & 0.34                 & 0.45                 & 0.22            \\ 
InLegalBERT(i-1, label\_p, i)   & 0.54               & 0.46            & 0.48              & 0.52              & 0.35         \\ 
Hier\_BiLSTM CRF                & \textbf{0.78}               & \textbf{0.77}            & \textbf{0.77}              & 0.62              & 0.68         \\ 

\bottomrule
\end{tabular}
}
\caption{Performance Comparison of Models on Rhetorical Role Classification. In the Model column, \(i\) indicates the current sentence, \(i-1\) means the previous sentence, and \(i+1\) means the next sentence. \texttt{label\_t} and \texttt{label\_p} refer to the true and predicted labels of the previous sentences. The best results are in bold.}
\label{tab:model_performance}
\end{table}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In this section, we present the results of our experiments on rhetorical role classification and analyze the performance of different models. Table \ref{tab:model_performance} summarizes the evaluation metrics for each model.

\subsection{Model Performance}
Among the evaluated models, the hierarchical BiLSTM-CRF achieves the highest overall performance. The sequential nature of BiLSTM allows the model to capture dependencies between sentences, while the CRF layer explicitly models label transitions, refining predictions by enforcing structural coherence. This ability to learn the transition relationships between rhetorical roles plays a crucial role in classification, as labels in legal documents follow a structured sequence. For example, an issue is likely to be followed by supporting arguments and eventually a decision. The ability to maintain coherence in predictions by capturing dependencies between consecutive sentences makes the BiLSTM-CRF model more effective in comparison to models that classify each sentence independently. Prior studies in structured text classification have similarly observed the benefits of explicit modeling of transition relationships between labels, as seen in \citet{bhattacharya2019identification, modi-etal-2023-semeval, santosh2024hiculr}.

In contrast, transformer-based models such as ToInLegalBERT, InLegalBERT, and Role-Aware Transformers process sentences independently, limiting their ability to model long-range dependencies within legal documents. These models rely primarily on self-attention mechanisms, which work well for general NLP tasks but struggle to capture structured rhetorical transitions without explicit sequential modeling. ToInLegalBERT, which integrates sentence-level positional encodings and hierarchical structuring, performs better than standard BERT-based models, highlighting the benefit of incorporating document structure into transformers.

The Graph Neural Network model performs competitively by effectively propagating contextual information across sentence nodes, capturing both local and global dependencies within legal documents. Among the InLegalBERT variants, the model trained using the current sentence along with two preceding sentences achieves the best performance, reinforcing the importance of sentence context in improving classification accuracy.

The Multi-Task Learning model, which incorporates label shift prediction as an auxiliary task, achieves moderate performance. While this method aims to capture role transitions, the additional complexity may have introduced challenges in optimization. Despite this, multitask learning remains a promising approach, particularly when combined with stronger baseline models.

The \texttt{RhetoricLLaMA} model, despite being instruction-tuned, did not perform as strongly as expected. While large language models like LLaMA-2-7B have achieved success in NLP, their effectiveness in specialized tasks such as rhetorical role classification remains limited without extensive domain-specific fine-tuning. Further research is needed to optimize large language models for structured legal NLP tasks.

\subsection{Impact of Transition Relationships in Classification}
Our experiments highlight the critical role of transition relationships between rhetorical roles in improving classification performance. Models such as the BiLSTM-CRF explicitly model these transitions, allowing them to maintain coherence in predictions by capturing dependencies between consecutive sentences. This is particularly advantageous because legal documents are highly structured, with rhetorical roles appearing in predictable sequences. In contrast, models that classify each sentence in isolation struggle to maintain contextual consistency, leading to higher misclassification rates.

For instance, when a sentence is labeled as an issue, the subsequent sentences are highly likely to be arguments or facts rather than a decision. CRF layers enforce these structural constraints, making BiLSTM-CRF more effective than independent sentence classifiers. This aligns with previous findings in rhetorical role classification, where modeling dependencies between sequential labels significantly improved performance in structured text classification tasks.

\subsection{Justification for Predicted Labels Showing Higher Performance}
An interesting observation from Table \ref{tab:model_performance} is that models using predicted labels for previous sentences sometimes outperform those using true labels. This initially appears counterintuitive, but a plausible explanation is that during training, both true labels and predicted labels were provided to the model, allowing it to learn effective dependencies. However, during testing, true labels are not available, meaning models trained exclusively with true labels may not learn to handle missing labels during inference. In contrast, models using predicted labels during training are already exposed to prediction noise, making them better adapted to real-world inference conditions where true labels are not available.

This suggests that training models to rely on predicted labels during both training and inference improves robustness, as the model learns to correct potential errors in label predictions over multiple steps. However, further research is needed to analyze whether explicitly modeling label uncertainty could further enhance performance.

\subsection{Impact of Instruction-Tuning in \texttt{RhetoricLLaMA}}
We conducted extensive experiments to analyze the impact of instruction-tuning in \texttt{RhetoricLLaMA} by comparing it against Vanilla LLaMA models in both quantized (4-bits) and unquantized forms. Despite leveraging large-scale pre-trained models, the instruction-tuned \texttt{RhetoricLLaMA} did not achieve the expected performance, suggesting that rhetorical role classification in legal texts requires more specialized adaptations.

The comparison revealed that the instruction-tuned model performed slightly better than the Vanilla LLaMA model but still lagged behind traditional transformer-based models like \texttt{ToInLegalBERT} and BiLSTM-CRF. While instruction-tuning provides explicit task-specific guidance, our results indicate that for highly specialized domains such as legal NLP, additional domain-specific pre-training and refined instruction sets are necessary to enhance model performance.

\subsection{Error Analysis}
Our error analysis revealed that the models struggled primarily with distinguishing between closely related rhetorical roles, such as Facts and Reasoning, due to the overlap in their language and structure within legal documents. This challenge is clearly illustrated in the confusion matrix of the Hierarchical BiLSTM-CRF model Figure~\ref{fig:hier_bilstm_crf}, which shows frequent misclassifications between these roles. Similarly, confusion between Arguments of Petitioner and Arguments of Respondent was prevalent, as both often exhibit similar language patterns, further complicating accurate classification. Models that incorporated contextual information from preceding or following sentences demonstrated some improvement in reducing these errors, particularly for roles requiring a clear transition, such as Issue and Decision. However, despite this improvement, the context-aware models still encountered difficulties, suggesting that the rhetorical role boundaries within these transitions are not always well-defined. Another critical issue identified was class imbalance. More frequent labels like None and Facts were consistently overpredicted, leading to lower precision for less frequent labels such as Issue and Decision. This imbalance skewed the performance, resulting in models favoring high-frequency roles at the expense of accuracy for underrepresented roles.
% Furthermore, models utilizing predicted labels from previous sentences experienced error propagation. When earlier sentences were misclassified, these errors compounded, negatively affecting the subsequent predictions, particularly in sequential models that rely heavily on the accuracy of previous classifications. 
Figures~\ref{fig:mtl},~\ref{fig:gnn},~\ref{fig:toinlegalbert},~\ref{fig:rhetoricllama},~\ref{fig:inlegalbert_i},~\ref{fig:inlegalbert_i-1_i},~\ref{fig:inlegalbert_i-2_i-1_i},~\ref{fig:inlegalbert_i-1_i_i+1},~\ref{fig:inlegalbert_label_t},~\ref{fig:inlegalbert_label_p} illustrating the confusion matrices for other models, are provided in the Appendix due to space constraints. These figures further highlight the patterns of misclassification and the impact of various model architectures on error distribution. Addressing these issues, particularly through improved handling of context, mitigating class imbalance, and minimizing the propagation of sequential errors, remains a critical area for future research and model refinement.
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/Hier_BiLSTM_CRF.pdf}
    \caption{Confusion matrix for rhetorical role classification using Hierarchical BiLSTM-CRF model.}
    \label{fig:hier_bilstm_crf}
\end{figure}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%