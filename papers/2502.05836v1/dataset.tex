\section{Dataset}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[t] % [h] 
    \centering 
    \includegraphics[width=\linewidth]{figures/rhetorical_roles_pie_chart.pdf} 
    \caption{Distribution of Rhetorical Roles within the Dataset.}
    \label{pie-chart}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
In this research, we present the \texttt{LegalSeg} Judgment Dataset, the largest annotated dataset of legal judgments in the English language, specifically focused on rhetorical role segmentation. This dataset represents a significant advancement in the field of legal Natural Language Processing (L-NLP), especially in the context of the Indian judiciary. It aims to address existing gaps in annotation comprehensiveness by offering a rich resource of annotated legal judgments designed to facilitate semantic labeling task.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Dataset Compilation}
The dataset comprises 16,000 legal judgments sourced from the IndianKanoon database, a widely used legal search engine for Indian legal documents. These judgments were collected from the Supreme Court of India and various High Courts, ensuring a diverse selection of cases across multiple domains of law, such as criminal, civil, and constitutional matters.  

During the data curation process, several documents were excluded for reasons such as corruption (e.g., containing unrecognized characters or missing segments) or being extremely short, often comprising procedural orders rather than substantive judgments. Additionally, after annotation, the final dataset was refined to 7,120 judgments by removing documents with incomplete or ambiguous annotations, ensuring a high-quality corpus that is also the largest of its kind by a significant margin.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Dataset Preparation and Preprocessing}
To train and evaluate models for this task, the dataset was divided into training, validation, and test sets using a 70-20-10 split, which comprises 4,984, 1,424, and 712 documents correspondingly. This split ensures a robust set of data for both training and evaluating models. Additionally, we computed various statistics regarding the documents and sentences within the dataset, including the average number of sentences per document and token counts presented in Table~\ref{tab:merged_dataset_stats}. Furthermore, the distribution of rhetorical roles within the dataset is visualized in a pie chart, Figure~\ref{pie-chart}.

To improve the performance of our models, we modified the dataset by breaking the documents into individual sentences and assigning each sentence its respective label. For sentence segmentation, we utilized SpaCy\footnote{\href{https://spacy.io/api/sentencizer}{https://spacy.io/api/sentencizer}}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table}[t]
\centering
\resizebox{\linewidth}{!}{%
\begin{tabular}{lccc}
\toprule
\textbf{Statistic}                        & \textbf{Train Set} & \textbf{Validation Set} & \textbf{Test Set} \\ 
\midrule
% \hline
{\# Documents}              & 4,984             & 1,424                   & 712             \\ 
% \hline
{Total \# Sentences}        & 11,22,507         & 2,93,370                 & 1,49,881           \\ 
% \hline
{Avg. \# Sentences per Doc} & 225             & 206                  & 210            \\ 
% \hline

{Avg. \# Tokens per Sentence} & 34             & 30                   & 32             \\ 
% \hline
\midrule
\multicolumn{4}{c}{\textbf{Sentence Count per Label}} \\ 
% \hline
\midrule
Facts                                     & 1,69,653            & 51,924                  & 24,909            \\ 
% \hline
Issue                                     & 12,791             & 4,259                   & 1,843             \\ 
% \hline
AoP             & 64,987             & 24,707                  & 14,520            \\ 
% \hline
AoR             & 50,097             & 16,021                  & 9,579             \\ 
% \hline
Reasoning                                 & 2,02,346            & 67,113                  & 36,689            \\ 
% \hline
Decision                                  & 19,574             & 7,634                   & 3,841             \\ 
% \hline
None                                      & 6,03,059            & 1,21,712                  & 58,500            \\ 
\midrule
% \hline

\multicolumn{4}{c}{\textbf{Average Number of Tokens per Label}} \\ 
\midrule
% \hline
Facts                                     & 34              & 33                   & 32             \\ 
% \hline
Issue                                     & 41              & 42                   & 46             \\ 
% \hline
AoP             & 37              & 31                   & 33             \\ 
% \hline
AoR             & 38              & 35                   & 35             \\ 
% \hline
Reasoning                                 & 34              & 34                   & 33             \\ 
% \hline
Decision                                  & 26              & 25                   & 25             \\ 
\bottomrule
% \hline
\end{tabular}
}
\caption{Dataset Statistics for \texttt{LegalSeg} Dataset}
\label{tab:merged_dataset_stats}
\end{table}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \subsection{Test Data}

% Our primary test data consists of 712 documents and 140,760 sentences, annotated with 7 rhetorical role (RR) labels from our \texttt{LegalSeg} dataset. This robust dataset was used to evaluate the performance of our fine-tuned model on unseen data, ensuring the accuracy of our segmentation task.

% Additionally, to further evaluate the generalizability and accuracy of our fine-tuned model, we tested it on publicly available Indian legal datasets~\ref{table:test_datasets}. These external datasets come from different sources and vary in the number of documents, sentences, and RR labels, providing a diverse test environment for our model:
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{table}[h]
% \centering
% \resizebox{0.95\columnwidth}{!}{%
% \begin{tabular}{lccc}
% \toprule
% \textbf{Dataset Source} & \textbf{\# Documents} & \textbf{\# Sentences} & \textbf{RR Labels} \\ 
% \midrule
% \texttt{LegalSeg} (Our Dataset) & 712                         & 140,760                      & 7                  \\ 
% \citet{bhattacharya2019identification} & 50                          & 9,380                        & 7                  \\ 
% \citet{malik-etal-2022-semantic}           & 100                         & 21,184                       & 13                 \\ 
% \citet{kalamkar-etal-2022-corpus}                     & 50                          & 4,158                        & 13                 \\ 
% \citet{marino2023automatic} & 30 & 2,879                        & 13                 \\ 
% \bottomrule
% % \hline
% \end{tabular}
% }
% \caption{Test Datasets Overview}
% \label{table:test_datasets}
% \end{table}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% For fair testing across these external datasets, we retained only the common rhetorical role labels between our \texttt{LegalSeg} dataset and the other datasets. The uncommon labels that did not match across the datasets were skipped to ensure consistency in the evaluation process. This standardization allowed us to benchmark the performance of our model in a unified and comparable manner across different datasets.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Annotation Process}
The annotation process was performed by a group of 10 legal experts, consisting of third and fourth year law students selected for their strong academic backgrounds and familiarity with legal processes. The annotation process spanned from April 2022 to October 2023. Each annotator was assigned 10 judgments per week, ensuring detailed attention to every document.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Quality Control}
To ensure the annotation accuracy and consistency, we implemented multiple levels of quality control:
\begin{itemize}
    \item Senior Expert Review: All disagreements in annotations were escalated to them for resolution.
    
    \item Regular Training: Annotators participated in bi-weekly training sessions, ensuring consistency in understanding and interpreting legal content. Ambiguous or difficult segments were regularly discussed and standardized.
\end{itemize}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Annotation Roles}
Legal experts annotated each sentence with one of the following rhetorical roles:

\begin{itemize}
    \item \textbf{Facts:} Sentences that describe the sequence of events that led to the case. These typically involve details of the circumstances and actions related to the case, providing a factual narrative of the caseâ€™s background, and details about the parties involved, including key dates, events, and parties involved.

    \item \textbf{Issue:} Sentences that outline the legal issues or questions being addressed in the case. These often identify the core legal disputes or points of law that the court must resolve to make a ruling.
    % This role frames the central questions at stake.
 
    \item \textbf{Arguments of Petitioner (AoP):} Sentences representing the arguments made by the petitioner (the party bringing the case to court). These include claims, reasoning, and justifications presented by the petitioner to support their position and persuade the court to rule in their favor.   
    
    \item \textbf{Arguments of Respondent (AoR):} Sentences that summarize the arguments made by the respondent (the party defending against the case). Like the petitioner's arguments, these statements offer counterpoints, legal interpretations, and rebuttals designed to challenge the petitionerâ€™s claims and persuade the court to rule in the respondentâ€™s favor.
    
    \item \textbf{Reasoning:} Sentences that provide the rationale or reasoning behind the court's decision. This includes the application of legal principles and precedents, as well as the logic that connects the facts and arguments to the final ruling. This label captures how the court justifies its decision in light of the legal issues presented.

    \item \textbf{Decision:} Sentences that reflect the final ruling or judgment of the court. This label marks the conclusion of the case, where the court issues its verdict or order, stating the outcome of the case based on its reasoning, such as granting relief, compensation, or dismissing the case.
    
    \item \textbf{None:} Sentences that do not fall under any of the defined rhetorical roles. These sentences may include procedural information, non-substantive remarks, legal jargon, or content that is not directly relevant to the legal analysis or judgment.
\end{itemize}


This annotation schema follows closely with prior works in rhetorical role segmentation, as demonstrated by the datasets used in similar research efforts such as those by \citet{bhattacharya2019identification, kalamkar-etal-2022-corpus, malik-etal-2022-semantic}.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
