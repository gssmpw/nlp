\section{Conclusion and Future Work}
In this work, we addressed the challenging task of rhetorical role classification in legal documents by introducing the \texttt{LegalSeg} dataset, the largest annotated dataset for this task. \texttt{LegalSeg}, provides a significant resource for advancing research in this domain. We evaluated multiple models, including \texttt{RhetoricLLaMA}, ToInLegalBERT, Role-aware, and GNNs. Our results show that models incorporating both sequential and contextual information, such as Hierarchical BiLSTM-CRF and ToInLegalBERT, perform best in identifying and classifying rhetorical roles in legal texts. We also demonstrated that adding sentence-level context improves the model's ability to capture transitions between rhetorical roles, reducing errors caused by the inherent similarity between roles like Facts and Reasoning.

Despite these advancements, our error analysis revealed several challenges, such as misclassification between similar roles and the cascading effect of label prediction errors. Furthermore, class imbalance remains a significant issue, with frequent misclassifications of minority labels.

For future work, we aim to explore more sophisticated techniques to handle class imbalance, such as advanced sampling strategies and loss function adjustments. Additionally, refining models' ability to capture long-range dependencies and leveraging more robust pre-training strategies could further enhance the performance of LLMs. We also plan to incorporate more domain-specific knowledge into the models and experiment with cross-domain transfer learning to improve their adaptability across different legal contexts. 
% Lastly, reducing the cascading effect of errors in real-time inference scenarios is another key area we plan to investigate.
