
\appendix

% \section{Example Appendix}
\section{Appendix}
\label{sec:appendix}

\subsection{Example of Coding Problem}
\label{sec:appendix_example_code}
In the method section (\cref{sec:method}), we introduce our problem setup, which includes unambiguous configuration with a small amount of demonstrations. In this section, we provide one such example to better illustrate how typically dataset provide questions. In particular, we show one sample from the hard subset of LiveCodeBench~\citep{jain2024livecodebench}.

\begin{tcolorbox}
[colback=white,colframe=gray,title=Question]
You are given a string word and an array of strings forbidden.
A string is called valid if none of its substrings are present in forbidden.
Return the length of the longest valid substring of the string word.
A substring is a contiguous sequence of characters in a string, possibly empty.
\newline

Example 1:

Input: word = "cbaaaabc", forbidden = ["aaa","cb"]

Output: 4

Explanation: There are 11 valid substrings in word: "c", "b", "a", "ba", "aa", "bc", "baa", "aab", "ab", "abc" and "aabc". The length of the longest valid substring is 4. 
It can be shown that all other substrings contain either "aaa" or "cb" as a substring. 
\newline

Example 2:

Input: word = "leetcode", forbidden = ["de","le","e"]

Output: 4

Explanation: There are 11 valid substrings in word: "l", "t", "c", "o", "d", "tc", "co", "od", "tco", "cod", and "tcod". The length of the longest valid substring is 4.
It can be shown that all other substrings contain either "de", "le", or "e" as a substring. 
\newline
 
Constraints:

1 $\leq$ word. length $\leq 10^5$ word consists only of lowercase English letters. 1 $\leq$ forbidden. length $\leq 10^5$. 1 $\leq$ forbidden$[i]$. length $\leq$ 10. forbidden[i] consists only of lowercase English letters.

\label{box:example_code}
\end{tcolorbox}

\iffalse
\subsection{Consensus Grouping of Public Correct Samples}
\label{sec:consensus_grouping}

To evaluate and rank samples effectively, we first identify samples with a public test accuracy of 1.0. These samples are stored in a list called \textit{public correct samples}. If no such samples exist, we return the sample with the highest public test accuracy.

Next, we categorize these \textit{public correct samples} into consensus groups based on their execution results on precomputed generated tests. A consensus group record is structured as a list of dictionaries, where:

- Each dictionary represents the execution outcomes of all public correct samples on a specific test.
- The dictionary keys are unique execution results (formatted as strings).
- The dictionary values are arrays of sample indices corresponding to samples that share the same execution result for that test.

\subsection{Scoring Samples Using LLM Comparison}
\label{sec:scoring_samples}

Once consensus groups are determined, we score each sample to identify the best-performing one. For each test (each dictionary entry in the consensus group record), we perform pairwise comparisons between different consensus groups.

For each test, given two consensus groups, \( A \) and \( B \), we compare every sample from group \( A \) with every sample from group \( B \). We use a Large Language Model (LLM) to judge which sample is better. The LLM assigns either 0, 1, or 0.5 points to each sample, where 0 indicates the sample is worse, 1 indicates it is better, and 0.5 represents a tie. These scores accumulate for the respective clusters. The scoring follows this process:

1. If a sample from group \( A \) receives a point from the LLM, group \( A \) gains one additional point.
2. After all pairwise comparisons for a given test, if a single group emerges as the unique winner, every sample in that group receives an additional point.
3. If there is a tie among groups, we proceed to the next test without awarding extra points.

\subsection{LLM Comparison Function}
\label{sec:llm_comparison}

The LLM comparison function is designed to evaluate two given samples fairly. It follows these steps:

1. The LLM generates new test cases tailored to distinguish between the two samples based on execution results.
2. Both samples are executed on the generated tests.
3. The LLM evaluates their outputs and determines whether \textit{sample A is better}, \textit{sample B is better}, or if the result is a \textit{tie}.
4. To mitigate order bias, the process is performed twice:
   - The first time, sample \( A \) is presented first.
   - The second time, sample \( B \) is presented first.
5. A score is counted only if both evaluations produce a consistent result.
6. The final scores are assigned accordingly, ensuring a reliable comparison between samples.
\fi

\subsection{Prompt templates}
\label{sec:appendix_prompts}
We also provide detailed prompts used in our experiments in \cref{fig:self-refine} to \cref{fig:generation-prompt}. These prompts are generated automatically by DSPy~\citep{khattab2023dspy}.
% Please refer to our github repository~\footnote{\url{} for full details.

\begin{figure*}[t]
\centering
\includegraphics[width=\textwidth]{figures/self_refine.pdf}
\caption{The prompt for iterative debugging.}
\label{fig:self-refine}
\end{figure*}

\begin{figure*}[t]
\centering
\includegraphics[width=\textwidth]{figures/precomputed_tests.pdf}
\caption{The prompt for generating test cases.}
\end{figure*}

\begin{figure*}[t]
\centering
\includegraphics[width=\textwidth]{figures/generation_prompt.pdf}
\caption{The prompt for code generation.}
\label{fig:generation-prompt}
\end{figure*}