\section{Introduction}
% Scaling test time is important. There has been quite some work in integrating parallel, sequential, prm, in the math domain. But coding domain has less been explored. This may be due to that . Thus we ask. how can we build the simplest and effective framework to scale coding domain compute effectively?
Scaling test time compute is highly effective in improving the performance of Large Language Models (LLMs)~\citep{openai_learning_to_reason_2024, guo2025deepseek,qwen_qwq_2024,muennighoff2025s1,sky_t1_2025,brown2024large,snell2024scaling}. 
In math reasoning tasks, in particular, extensive research has explored various test-time scaling strategies, including repeated sampling to improve solution coverage~\citep{ehrlich2025codemonkeys}, iterative refinement through rethinking and revising previous solutions, and the use of reward models to provide more effective guidance in the search process~\citep{snell2024scaling, li2024rethinkmcts}. 

While significant progress has been made in math reasoning tasks, test-time scaling for code generation---an essential domain with wide-ranging applications---remains largely underexplored.
Unlike math reasoning tasks, where correctness can often be verified by exact string matching~\cite{}, \jx{Need a citation}
checking correctness in coding tasks requires executing a large scale of test cases to increase accuracy~\citep{liu2023your}, adding complexity to the test time framework and the potential reward model feedback~\citep{AceCoder}. 
On the other hand, coding tasks offer a unique advantage: they can leverage existing language interpreters to generate precise outputs for the given program and inputs. This capability provides a reliable grounding for revising or selecting solutions~\citep{chen2023teaching}.

\iffalse
Scaling test time compute for Large Language Models (LLMs) has increasingly improved model performance~\citep{openai_learning_to_reason_2024, guo2025deepseek,qwen_qwq_2024,muennighoff2025s1,sky_t1_2025,brown2024large,snell2024scaling}. In math reasoning tasks, a rich line of literature has studied how to use repeated sampling to improve coverage~\citep{ehrlich2025codemonkeys}, rethinking and revising previous solutions, and reward models to provide accurate guidance in a searching process~\citep{snell2024scaling, li2024rethinkmcts}. While there has been great success in the math reasoning domain, the code generation domain has been largely under-explored. Unlike the math domain where correctness can be easily checked by exact string matching, correctness in coding tasks requires executing a large scale of test cases to avoid false positives~\citep{liu2023your}, complicating test time framework, and potential reward model feedback~\citep{AceCoder}. On the other hand, coding tasks can leverage interpreter, which provides accurate output given a coding program and an input. This can potentially provide grounding for revising or selecting solutions~\citep{chen2023teaching}.
% test cases with execution, which can potentially provide some grounding as reward models.
\fi

% Thus, in this paper, we ask: \textit{how can we build the simplest framework to scale test time compute in for the coding domain effectively?}
\begin{figure}[!t]
    \centering
    % First subfigure
    \includegraphics[width=0.49\textwidth]{figures/diff_model_performance.pdf}      \caption{\textbf{Performance improvement with~\frameworkname~in LiveCodeBench}. "Qwen-Coder" denotes "Qwen2.5-Coder-Instruct", "R1-Distill" denotes "DeepSeek-R1-Distill-Qwen".~\frameworkname consistently improves models across different sizes, allowing non-reasoning models to surpass reasoning models, and open models to be competitive to o1 (high reasoning effort).}%.\dacheng{run o1-high}}
    \label{fig:all_models_performance}
\end{figure}
%\begin{figure}[!t]
%    \centering
    % First subfigure
    %\includegraphics[width=0.45\textwidth]{figures/performance.pdf}      \caption{\textbf{Performance comparison in LiveCodeBench}. ~\frameworkname consistently improves models across different sizes, and both instruction-tuned and newer reasoning models. In particular, ~\frameworkname allows (1) smaller models to outperform 10x larger model: Qwen2.5-3B-Coder-Instruct, integrated with~\frameworkname, outperforms its 32B variant. (2) Older models to outperform newer models: GPT-4o-mini, integrated with~\frameworkname, outperforms O1-Preview. (3) Reasoning models achieve state-of-the-art performance: DeepSeek-R1-Distill-Qwen-32B, integrated with~\frameworkname. outperforms O1-High.}
       % \label{fig:performance}
%\end{figure}

% In this paper, we propose \frameworkname, the first hybrid test-time scaling framework that substantially improves the coverage and selection accuracy of the generated code. \frameworkname extends the existing parallel scaling paradigm with sequential scaling to further push the upper bound and leverages a novel selection mechanism that combines LLM-based evaluation with execution-grounded verification to robustly identify correct solutions.

\begin{figure*}[!t]
    \centering
    \includegraphics[width=0.95\linewidth]{latex/figures/overview_new.pdf}
\caption{\frameworkname builds on the paradigm of a combination of revision and parallel samples. 
It first performs a self-refinement on the initial solution to improve algorithm efficiency, and subsequent self-debug round to correct solutions based on public test execution feedback. At the end, it undergoes a selection policy module to select the best solution to output.
% \joey{I was confused by this figure as it looks likes Figure 2 from the Snell paper (probably Fig2a).  Does each square reflect a new decode call? It might help to show where teacher-forcing or prompting is done.  Like at the beginning show multiple different prompts to get k different generations. Then show each long context as a tall rectangle and then illustrate the selection process more. }
}
\label{fig:Design}
\end{figure*}

In this paper, we introduce \frameworkname, the first hybrid test-time scaling framework for coding tasks, which significantly enhances the coverage and quality of generated code.
\frameworkname has two stages, as illustrated in Figure~\ref{fig:Design}.
In the generation stage, \frameworkname extends parallel scaling (i.e., repeated sampling)~\citep{brown2024large, li2022competition} by incorporating sequential scaling with \textit{iterative debugging}, enhancing both code coverage and quality.
Specifically, for each parallel sample, \frameworkname executes the generated code on public test cases using the corresponding language interpreter to obtain accurate outputs and/or error messages. These outputs and error messages are then fed back to the model, enabling it to iteratively revise the code and generate improved samples.
Interestingly, we find that even for reasoning models that already perform hypothetical debugging during the thinking process, iterative debugging with actual code execution, along with output and error message feedback, can further significantly enhance their performance.~\dacheng{add a small section in experiments to cite}


\iffalse
In this paper, we develop a two-stage approach,  which we call \frameworkname, that leverages test cases and execution to provide grounding information to scale the test time compute for LLMs effectively.
%\textbf{In this paper, we propose the first simple hybrid framework~\frameworkname}, to study parallel scaling and sequential scaling strategies of code generation.
\shiyi{}
In the generation stage, \frameworkname leverages repeated sampling~\citep{brown2024large, li2022competition} to generate solutions in parallel, enabling models to generate multiple coding candidates.
% \joey{finish this sentence}
In contrast to prior work that based on repeated sampling~\citep{brown2024large, li2022competition}, we find that performing further revision on public test cases significantly improves the quality of these candidates. 
Interestingly, we find that even reasoning models, which already perform hypothetical debugging during the thinking process, also benefit significantly from additional debugging iterations~\dacheng{add a small section in experiments to cite}. 
\fi

For sample selection, we find that existing approaches, which blindly generate test inputs for all samples, fail to accurately distinguish between pairwise samples. To address this limitation, we propose \textit{adaptive input synthesis}. Specifically, for each pair of samples, we prompt an LLM to generate test inputs that can effectively differentiate them. Then, by executing the samples with these inputs, we select the best-performing sample based on the actual execution results.


\iffalse
We also explore several approaches to the selection stage. 
We find that the commonly used majority voting  and LLM-as-a-judge based approaches to select the correct solution perform poorly in the code generation setting.
Thus, we develop a novel test-assisted LLM-as-a-judge approach, where an LLM is prompted to generate distinguishing inputs on which the code is run.  The LLM-as-a-judge is then given the input-output pairs along with the code to make a more informed decision about correctness.
\fi


% three phrases that have been proven to be powerful in the context of math problems or partially explored in the coding domain: (1) repeated sampling, which improves overage by generating diverse attempts, enabling models to solve a higher fraction of problems~\citep{brown2024large}, (2) self-debugging,~\citep{chen2023teaching, madaan2024self} on test demonstration, and (3) selection. %\jx{We explained the first phases but not the last two. Should we also explain them briefly or removing the explanation of the 1st point to make them aligned?}
% We carefully study basic design choice in each axis. Here are the takeaways that enable our final default setting: (1) Parallel: Medium-to-high temperature is sufficient to balance diversity while preserving code quality. We also experiment with several in-context variants but do not observe more significant improvement. (2) Sequential: Self-debug flattens out at usually 2-3 rounds. Interestingly, even strong reasoning models such as R1-7B benefits from real execution. (3) Selection: Majority does not work well; LLM-as-a-judge does not work well by looking at the code; we make a hybrid selection with test-case execution + LLM-as-a-judge with tool-use (i.e., test-case execution) work well. % \shangyin{(1) is super clear. (2) and (3) are fuzzy. I think this paragraph could be improved by saying: \frameworkname does (1) (2) and (3), and we observed that they are the best design choices among all.}



% We found xxx works

% We conduct xxx

% Our contribution is xxx

We evaluate \frameworkname on 12 models across model sizes, open and closed, and both instruction-based or reasoning models. \frameworkname has shown consistent improvement over zero-shot generation or existing popular test-time scaling methods. In particular, \frameworkname~allows (1) small models to outperform large models in the same family: Qwen2.5-7B-Instruct + \frameworkname outperforms Qwen2.5-32B-Instruct on LiveCodeBench by 10.7\%; (2) instruction-based model to outperform reasoning models: GPT-4o-mini + \frameworkname outperforms o1-preview by 3.7\%; 
and (3) open reasoning models to perform competitively to the state-of-the-art: DeepSeek-R1-Distill-Qwen-32B + \frameworkname~ achieves 85.7\% on LiveCodeBench, where the state-of-the-art (o1-high) is 88.7\%.
Figure~\ref{fig:breakdown} shows an example overview of the benefits of our proposed techniques.
We further include comprehensive ablation studies on 
% different selection methods, 
% the effect of temperature tuning during the generation phase, 
% the impact of using in-context retrieval methods, 
variants of iterative debugging and selection methods in the experiment section~\ref{sec:exp}.

In conclusion, our contributions are:
\begin{enumerate}
\item The first hybrid test-time scaling framework that extends parallel scaling by incorporating sequential scaling with iterative debugging for high code coverage and uses LLMs to adaptively synthesize test inputs for accurate best sample selection. 
%    \item The first hybrid test-time-scaling framework for code generation.
\item We perform extensive evaluation on LiveCodeBench and CodeContests, and show that \frameworkname consistently improve performance across model families and sizes.
%with ablations on detailed design choice. Our framework improves xxx...
\item We will open-source all software artifacts, as well as model generations and intermediates results to facilitate the research community.
\end{enumerate}

\iffalse
In conclusion, our contributions are:
\begin{enumerate}
\item A framework that incorporates repeated sampling with revision from test execution result, LLM selection with code execution result to effectively leverage test-time compute to improve code generation.
%    \item The first hybrid test-time-scaling framework for code generation.
\item We perform extensive evaluation on LiveCodeBench and CodeContests, and show that \frameworkname consistently improve performance across model sizes.
%with ablations on detailed design choice. Our framework improves xxx...
\item We will open-source all software artifacts, as well as model generations and intermediates results to facilitate the research community.
\end{enumerate}
\fi

\begin{figure}[!t]
    \centering
    % First subfigure
    \includegraphics[width=1.0\linewidth]{figures/breakdown_new.pdf}      \caption{\textbf{Example overview of performance.} 
    % \jx{We can potentially replace the non-reasoning model to beat o1-preview.}
}
        \label{fig:breakdown}
\end{figure}
