@article{AceCoder,
    title={AceCoder: Acing Coder RL via Automated Test-Case Synthesis},
    author={Zeng, Huaye and Jiang, Dongfu and Wang, Haozhe and Nie, Ping and Chen, Xiaotong and Chen, Wenhu},
    journal={ArXiv},
    year={2025},
    volume={2502.01718}
}

@misc{beeching2024dvts,
      title={Scaling test-time compute with open models},
      author={Edward Beeching and Lewis Tunstall and Sasha Rush},
      url={https://huggingface.co/spaces/HuggingFaceH4/blogpost-scaling-test-time-compute},
}

@article{brown2024large,
  title={Large language monkeys: Scaling inference compute with repeated sampling},
  author={Brown, Bradley and Juravsky, Jordan and Ehrlich, Ryan and Clark, Ronald and Le, Quoc V and R{\'e}, Christopher and Mirhoseini, Azalia},
  journal={arXiv preprint arXiv:2407.21787},
  year={2024}
}

@article{chen2022codet,
  title={Codet: Code generation with generated tests},
  author={Chen, Bei and Zhang, Fengji and Nguyen, Anh and Zan, Daoguang and Lin, Zeqi and Lou, Jian-Guang and Chen, Weizhu},
  journal={arXiv preprint arXiv:2207.10397},
  year={2022}
}

@article{chen2023teaching,
  title={Teaching large language models to self-debug},
  author={Chen, Xinyun and Lin, Maxwell and Sch{\"a}rli, Nathanael and Zhou, Denny},
  journal={arXiv preprint arXiv:2304.05128},
  year={2023}
}

@article{chollet2019measure,
  title={On the measure of intelligence},
  author={Chollet, Fran{\c{c}}ois},
  journal={arXiv preprint arXiv:1911.01547},
  year={2019}
}

@article{christiano2017deep,
  title={Deep reinforcement learning from human preferences},
  author={Christiano, Paul F and Leike, Jan and Brown, Tom and Martic, Miljan and Legg, Shane and Amodei, Dario},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@misc{deepseek_r1_lite_2024,
  author       = {DeepSeek},
  title        = {DeepSeek-R1-Lite-Preview Release},
  year         = {2024},
  howpublished = {\url{https://api-docs.deepseek.com/news/news1120}},
  note         = {Accessed: 2024-11-20}
}

@article{ehrlich2025codemonkeys,
  title={CodeMonkeys: Scaling Test-Time Compute for Software Engineering},
  author={Ehrlich, Ryan and Brown, Bradley and Juravsky, Jordan and Clark, Ronald and R{\'e}, Christopher and Mirhoseini, Azalia},
  journal={arXiv preprint arXiv:2501.14723},
  year={2025}
}

@article{gao2024interpretable,
  title={Interpretable contrastive monte carlo tree search reasoning},
  author={Gao, Zitian and Niu, Boye and He, Xuzheng and Xu, Haotian and Liu, Hongzhang and Liu, Aiwei and Hu, Xuming and Wen, Lijie},
  journal={arXiv preprint arXiv:2410.01707},
  year={2024}
}

@article{gu2024cruxeval,
  title={Cruxeval: A benchmark for code reasoning, understanding and execution},
  author={Gu, Alex and Rozi{\`e}re, Baptiste and Leather, Hugh and Solar-Lezama, Armando and Synnaeve, Gabriel and Wang, Sida I},
  journal={arXiv preprint arXiv:2401.03065},
  year={2024}
}

@article{guo2025deepseek,
  title={Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning},
  author={Guo, Daya and Yang, Dejian and Zhang, Haowei and Song, Junxiao and Zhang, Ruoyu and Xu, Runxin and Zhu, Qihao and Ma, Shirong and Wang, Peiyi and Bi, Xiao and others},
  journal={arXiv preprint arXiv:2501.12948},
  year={2025}
}

@article{hassid2024larger,
  title={The Larger the Better? Improved LLM Code-Generation via Budget Reallocation},
  author={Hassid, Michael and Remez, Tal and Gehring, Jonas and Schwartz, Roy and Adi, Yossi},
  journal={arXiv preprint arXiv:2404.00725},
  year={2024}
}

@article{hendrycks2021measuring,
  title={Measuring coding challenge competence with apps},
  author={Hendrycks, Dan and Basart, Steven and Kadavath, Saurav and Mazeika, Mantas and Arora, Akul and Guo, Ethan and Burns, Collin and Puranik, Samir and He, Horace and Song, Dawn and others},
  journal={arXiv preprint arXiv:2105.09938},
  year={2021}
}

@article{hou2025advancing,
  title={Advancing Language Model Reasoning through Reinforcement Learning and Inference Scaling},
  author={Hou, Zhenyu and Lv, Xin and Lu, Rui and Zhang, Jiajie and Li, Yujiang and Yao, Zijun and Li, Juanzi and Tang, Jie and Dong, Yuxiao},
  journal={arXiv preprint arXiv:2501.11651},
  year={2025}
}

@article{huang2022large,
  title={Large language models can self-improve},
  author={Huang, Jiaxin and Gu, Shixiang Shane and Hou, Le and Wu, Yuexin and Wang, Xuezhi and Yu, Hongkun and Han, Jiawei},
  journal={arXiv preprint arXiv:2210.11610},
  year={2022}
}

@article{huang2023enhancing,
  title={Enhancing large language models in coding through multi-perspective self-consistency},
  author={Huang, Baizhou and Lu, Shuai and Chen, Weizhu and Wan, Xiaojun and Duan, Nan},
  journal={arXiv preprint arXiv:2309.17272},
  year={2023}
}

@article{irvine2023rewarding,
  title={Rewarding chatbots for real-world engagement with millions of users},
  author={Irvine, Robert and Boubert, Douglas and Raina, Vyas and Liusie, Adian and Zhu, Ziyi and Mudupalli, Vineet and Korshuk, Aliaksei and Liu, Zongyi and Cremer, Fritz and Assassi, Valentin and others},
  journal={arXiv preprint arXiv:2303.06135},
  year={2023}
}

@article{jain2024livecodebench,
  title={Livecodebench: Holistic and contamination free evaluation of large language models for code},
  author={Jain, Naman and Han, King and Gu, Alex and Li, Wen-Ding and Yan, Fanjia and Zhang, Tianjun and Wang, Sida and Solar-Lezama, Armando and Sen, Koushik and Stoica, Ion},
  journal={arXiv preprint arXiv:2403.07974},
  year={2024}
}

@article{jiao2024preference,
  title={Preference Optimization for Reasoning with Pseudo Feedback},
  author={Jiao, Fangkai and Guo, Geyang and Zhang, Xingxing and Chen, Nancy F and Joty, Shafiq and Wei, Furu},
  journal={arXiv preprint arXiv:2411.16345},
  year={2024}
}

@article{lee2025evolving,
  title={Evolving Deeper LLM Thinking},
  author={Lee, Kuang-Huei and Fischer, Ian and Wu, Yueh-Hua and Marwood, Dave and Baluja, Shumeet and Schuurmans, Dale and Chen, Xinyun},
  journal={arXiv preprint arXiv:2501.09891},
  year={2025}
}

@article{li2022competition,
  title={Competition-level code generation with alphacode},
  author={Li, Yujia and Choi, David and Chung, Junyoung and Kushman, Nate and Schrittwieser, Julian and Leblond, R{\'e}mi and Eccles, Tom and Keeling, James and Gimeno, Felix and Dal Lago, Agustin and others},
  journal={Science},
  volume={378},
  number={6624},
  pages={1092--1097},
  year={2022},
  publisher={American Association for the Advancement of Science}
}

@article{li2024rethinkmcts,
  title={RethinkMCTS: Refining Erroneous Thoughts in Monte Carlo Tree Search for Code Generation},
  author={Li, Qingyao and Xia, Wei and Du, Kounianhua and Dai, Xinyi and Tang, Ruiming and Wang, Yasheng and Yu, Yong and Zhang, Weinan},
  journal={arXiv preprint arXiv:2409.09584},
  year={2024}
}

@article{li2025llms,
  title={LLMs Can Easily Learn to Reason from Demonstrations Structure, not content, is what matters!},
  author={Li, Dacheng and Cao, Shiyi and Griggs, Tyler and Liu, Shu and Mo, Xiangxi and Patil, Shishir G and Zaharia, Matei and Gonzalez, Joseph E and Stoica, Ion},
  journal={arXiv preprint arXiv:2502.07374},
  year={2025}
}

@article{lightman2023let,
  title={Let's verify step by step},
  author={Lightman, Hunter and Kosaraju, Vineet and Burda, Yura and Edwards, Harri and Baker, Bowen and Lee, Teddy and Leike, Jan and Schulman, John and Sutskever, Ilya and Cobbe, Karl},
  journal={arXiv preprint arXiv:2305.20050},
  year={2023}
}

@article{madaan2024self,
  title={Self-refine: Iterative refinement with self-feedback},
  author={Madaan, Aman and Tandon, Niket and Gupta, Prakhar and Hallinan, Skyler and Gao, Luyu and Wiegreffe, Sarah and Alon, Uri and Dziri, Nouha and Prabhumoye, Shrimai and Yang, Yiming and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{min2024imitate,
  title={Imitate, explore, and self-improve: A reproduction report on slow-thinking reasoning systems},
  author={Min, Yingqian and Chen, Zhipeng and Jiang, Jinhao and Chen, Jie and Deng, Jia and Hu, Yiwen and Tang, Yiru and Wang, Jiapeng and Cheng, Xiaoxue and Song, Huatong and others},
  journal={arXiv preprint arXiv:2412.09413},
  year={2024}
}

@article{muennighoff2025s1,
  title={s1: Simple test-time scaling},
  author={Muennighoff, Niklas and Yang, Zitong and Shi, Weijia and Li, Xiang Lisa and Fei-Fei, Li and Hajishirzi, Hannaneh and Zettlemoyer, Luke and Liang, Percy and Cand{\`e}s, Emmanuel and Hashimoto, Tatsunori},
  journal={arXiv preprint arXiv:2501.19393},
  year={2025}
}

@article{nye2021show,
  title={Show your work: Scratchpads for intermediate computation with language models},
  author={Nye, Maxwell and Andreassen, Anders Johan and Gur-Ari, Guy and Michalewski, Henryk and Austin, Jacob and Bieber, David and Dohan, David and Lewkowycz, Aitor and Bosma, Maarten and Luan, David and others},
  journal={arXiv preprint arXiv:2112.00114},
  year={2021}
}

@misc{openai_learning_to_reason_2024,
  author       = {OpenAI},
  title        = {Learning to Reason with LLMs},
  year         = {2024},
  howpublished = {\url{https://openai.com/index/learning-to-reason-with-llms/}},
  note         = {Accessed: 2024-11-20}
}

@article{pan2024swegym,
  title   = {Training Software Engineering Agents and Verifiers with SWE-Gym},
  author  = {Jiayi Pan and Xingyao Wang and Graham Neubig and Navdeep Jaitly and Heng Ji and Alane Suhr and Yizhe Zhang},
  year    = {2024},
  journal = {arXiv preprint arXiv: 2412.21139}
}

@misc{qwen_qwq_2024,
  author       = {Qwen},
  title        = {QwQ: Reflect Deeply on the Boundaries of the Unknown},
  year         = {2024},
  howpublished = {\url{https://qwenlm.github.io/blog/qwq-32b-preview/}}
}

@article{ridnik2024code,
  title={Code generation with alphacodium: From prompt engineering to flow engineering},
  author={Ridnik, Tal and Kredo, Dedy and Friedman, Itamar},
  journal={arXiv preprint arXiv:2401.08500},
  year={2024}
}

@article{saad2024archon,
  title={Archon: An architecture search framework for inference-time techniques},
  author={Saad-Falcon, Jon and Lafuente, Adrian Gamarra and Natarajan, Shlok and Maru, Nahum and Todorov, Hristo and Guha, Etash and Buchanan, E Kelly and Chen, Mayee and Guha, Neel and R{\'e}, Christopher and others},
  journal={arXiv preprint arXiv:2409.15254},
  year={2024}
}

@article{silver2016mastering,
  title={Mastering the game of Go with deep neural networks and tree search},
  author={Silver, David and Huang, Aja and Maddison, Chris J and Guez, Arthur and Sifre, Laurent and Van Den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and others},
  journal={nature},
  volume={529},
  number={7587},
  pages={484--489},
  year={2016},
  publisher={Nature Publishing Group}
}

@misc{sky_t1_2025,
  author       = {NovaSky Team},
  title        = {Sky-T1: Train your own O1 preview model within 450 dollars},
  howpublished = {https://novasky-ai.github.io/posts/sky-t1},
  note         = {Accessed: 2025-01-09},
  year         = {2025}
}

@article{snell2024scaling,
  title={Scaling llm test-time compute optimally can be more effective than scaling model parameters},
  author={Snell, Charlie and Lee, Jaehoon and Xu, Kelvin and Kumar, Aviral},
  journal={arXiv preprint arXiv:2408.03314},
  year={2024}
}

@article{stroebl2024inference,
  title={Inference Scaling fLaws: The Limits of LLM Resampling with Imperfect Verifiers},
  author={Stroebl, Benedikt and Kapoor, Sayash and Narayanan, Arvind},
  journal={arXiv preprint arXiv:2411.17501},
  year={2024}
}

@article{team2025kimi,
  title={Kimi k1. 5: Scaling Reinforcement Learning with LLMs},
  author={Team, Kimi and Du, Angang and Gao, Bofei and Xing, Bowei and Jiang, Changjiu and Chen, Cheng and Li, Cheng and Xiao, Chenjun and Du, Chenzhuang and Liao, Chonghua and others},
  journal={arXiv preprint arXiv:2501.12599},
  year={2025}
}

@article{wang2022self,
  title={Self-consistency improves chain of thought reasoning in language models},
  author={Wang, Xuezhi and Wei, Jason and Schuurmans, Dale and Le, Quoc and Chi, Ed and Narang, Sharan and Chowdhery, Aakanksha and Zhou, Denny},
  journal={arXiv preprint arXiv:2203.11171},
  year={2022}
}

@inproceedings{wang2024math,
  title={Math-shepherd: Verify and reinforce llms step-by-step without human annotations},
  author={Wang, Peiyi and Li, Lei and Shao, Zhihong and Xu, Runxin and Dai, Damai and Li, Yifei and Chen, Deli and Wu, Yu and Sui, Zhifang},
  booktitle={Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={9426--9439},
  year={2024}
}

@article{wang2024theoretical,
  title={A Theoretical Understanding of Self-Correction through In-context Alignment},
  author={Wang, Yifei and Wu, Yuyang and Wei, Zeming and Jegelka, Stefanie and Wang, Yisen},
  journal={arXiv preprint arXiv:2405.18634},
  year={2024}
}

@article{wei2022chain,
  title={Chain-of-thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={24824--24837},
  year={2022}
}

@article{wu2024inference,
  title={Inference scaling laws: An empirical analysis of compute-optimal inference for problem-solving with language models},
  author={Wu, Yangzhen and Sun, Zhiqing and Li, Shanda and Welleck, Sean and Yang, Yiming},
  journal={arXiv preprint arXiv:2408.00724},
  year={2024}
}

