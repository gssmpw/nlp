\section{Introduction}
Increasing test-time compute has emerged as a powerful approach for improving the performance of large language models (LLMs) across diverse tasks~\citep{openai_learning_to_reason_2024, guo2025deepseek,qwen_qwq_2024,muennighoff2025s1,sky_t1_2025,brown2024large,snell2024scaling}. In particular, test-time scaling has been extensively explored in mathematical reasoning, where parallel sampling increases solution coverage, sequential refinement improves individual samples through rethinking and revising, and reward models guide the search process more effectively~\citep{ehrlich2025codemonkeys, snell2024scaling, li2024rethinkmcts}. These methods collectively push the performance boundaries of LLMs by leveraging additional compute during inference.
%\dacheng{@Matei: cost comparison: small + test > large pass@1}
\begin{figure}[!t]
    \centering
    % First subfigure
    \includegraphics[width=0.48\textwidth]{figures/diff_model_performance-2.pdf}      \caption{\textbf{Performance improvement with~\frameworkname in LiveCodeBench (v2)}~\citep{jain2024livecodebench}. \frameworkname consistently improves models across different sizes, allowing non-reasoning models to surpass reasoning models and open models to be competitive with o1 (high reasoning effort). "Qwen-Coder" denotes "Qwen2.5-Coder-Instruct,"~\citep{hui2024qwen2} and "R1-Distill" denotes "DeepSeek-R1-Distill-Qwen." ~\citep{guo2025deepseek}. }%.\dacheng{run o1-high}}
    \label{fig:all_models_performance}
    \vspace{-5mm}
\end{figure}

\begin{figure*}[!t]
    \centering
    \includegraphics[width=1\linewidth]{figures/overview_new_v2.pdf}
\caption{
\textbf{Overview of \frameworkname}. \textbf{Stage 1: Generation}---\frameworkname enhances parallel samples through iterative debugging. Each sample is tested using public test cases executed via an interpreter, with outputs and/or error messages used to guide the next round of sample generation.
\textbf{Stage 2: Selection}---\frameworkname selects the best sample by prompting an LLM to generate inputs that differentiate between paired samples, then leveraging actual execution results to inform the LLM to determine the optimal choice.
% \frameworkname builds on the paradigm of a combination of revision and parallel samples. It first performs a self-refinement on the initial solution to improve algorithm efficiency, and subsequent self-debug round to correct solutions based on public test execution feedback. At the end, it undergoes a selection policy module to select the best solution to output.
}
\vspace{-5mm}
\label{fig:Design}
\end{figure*}



Despite these advancements in the math domain, the potential of test-time scaling for code generation---a domain with both fundamental importance and widespread practical applications---remains under-explored. Code generation introduces unique challenges compared to math reasoning. Correctness in math can often be verified through rule-based string matching with reference answers~\citep{guo2025deepseek, AceCoder}, whereas validating code requires executing a large set of test cases to accurately check functional correctness~\citep{liu2023your}. This dependence on execution increases the complexity of test-time scaling and complicates the design of reward models~\citep{AceCoder}. However, code generation also offers a distinct advantage: The availability of programmatic interpreters enables the execution of programs to obtain precise outputs and error messages, providing a reliable grounding mechanism for improving generation and selection~\citep{chen2023teaching, li2022competition}.



In this paper, we propose \frameworkname, the first hybrid test-time scaling framework for code generation, which substantially improves both coverage \footnote{The fraction of problems that are solved by any generated
sample~\citep{brown2024large}.} and selection accuracy. \frameworkname pushes the limits of existing parallel scaling strategies by integrating sequential scaling through \emph{iterative debugging}, while introducing a novel adaptive selection mechanism grounded in execution. The framework operates in two key stages, as shown in \cref{fig:Design}.

\textbf{First}, in the generation stage, \frameworkname augments parallel sampling~\citep{brown2024large, li2022competition} with sequential scaling via iterative debugging. Each generated sample is executed on public test cases to obtain outputs and/or error messages, which are fed back into the model to iteratively refine the code. 
% Notably, we find that even models capable of hypothetical debugging during reasoning benefit significantly from this execution-grounded feedback loop, leading to substantial performance gains.
\textbf{Second}, in the selection stage, existing methods often rely on generating test inputs indiscriminately, which can fail to effectively differentiate between candidate solutions~\citep{chen2022codet, AceCoder}. To overcome this limitation, \frameworkname introduces \textit{adaptive input synthesis}: for each pair of samples, an LLM is prompted to generate distinguishing test inputs. These inputs are executed, where the outputs are further provided to ground the LLM to select the best sample.
%and the sample that performs best on the synthesized inputs is selected as the final output. 
This adaptive, execution-grounded approach ensures robust identification of correct solutions (\cref{sec:ablation_selection}).


\frameworkname is a general approach that outperforms zero-shot generation and existing test-time scaling methods. We evaluate \frameworkname on 12 models, spanning a wide range of sizes, both open and closed, instruction-based and reasoning models. \frameworkname consistently enhances performance across these diverse settings. Notably, \frameworkname enables: (1) Small models to surpass larger models within the same family: Qwen2.5-7B-Instruct + \frameworkname outperforms Qwen2.5-32B-Instruct on LiveCodeBench by 10.7\%; (2) Instruction-based models to outperform reasoning models: GPT-4o-mini + \frameworkname surpasses o1-preview by 3.7\%; and (3) Open reasoning models to achieve performance competitive with state-of-the-art closed models: DeepSeek-R1-Distill-Qwen-32B + \frameworkname achieves 85.7\% on LiveCodeBench, approaching the state-of-the-art performance of o1-high at 88.7\%.
\cref{fig:breakdown} provides an overview of the performance improvements enabled by our techniques. %We further conduct comprehensive ablation studies on variants of iterative debugging and selection methods, as detailed in~\cref{sec:exp}. 
In summary, our contributions are:

% We evaluate\shiyi{I am here} \frameworkname on 12 models across model sizes, open and closed, and both instruction-based or reasoning models. \frameworkname has shown consistent improvement over zero-shot generation or existing popular test-time scaling methods. In particular, \frameworkname~allows (1) small models to outperform large models in the same family: Qwen2.5-7B-Instruct + \frameworkname outperforms Qwen2.5-32B-Instruct on LiveCodeBench by 10.7\%; (2) instruction-based model to outperform reasoning models: GPT-4o-mini + \frameworkname outperforms o1-preview by 3.7\%; 
% and (3) open reasoning models to perform competitively to the state-of-the-art: DeepSeek-R1-Distill-Qwen-32B + \frameworkname~ achieves 85.7\% on LiveCodeBench, where the state-of-the-art (o1-high) is 88.7\%.
% Figure~\ref{fig:breakdown} shows an example overview of the benefits of our proposed techniques.
% We further include comprehensive ablation studies on variants of iterative debugging and selection methods in the experiment section~\ref{sec:exp}.
\begin{enumerate}
\item We propose \frameworkname, the first hybrid test-time scaling framework for code generation, which augments parallel scaling with sequential scaling via iterative debugging and introduces adaptive test input synthesis using LLMs for robust sample selection.
\item We conduct extensive evaluations on LiveCodeBench and CodeContests, demonstrating that \frameworkname consistently improves performance across diverse model families and sizes.
\item We will release all software artifacts, model generations, and intermediate results to support and accelerate future research in this area.
\end{enumerate}


\begin{figure}[!t]
    \centering
    % First subfigure
    \includegraphics[width=1.0\linewidth]{figures/breakdown_new_v2.pdf}      
    \caption{
    % \textbf{Example overview of performance.}
    \textbf{Ablation of \frameworkname performance benefits}: Qwen2.5-Coder-14B-Instruct (denoted as Qwen-Coder-14B)~\citep{hui2024qwen2} with \frameworkname can surpass o1-preview without \frameworkname. DeepSeek-R1-Distill-Qwen-14B (denoted as R1-Distill-14B)~\citep{guo2025deepseek} with \frameworkname outperforms o1-mini without \frameworkname.
    % \xiuyu{should we use +xx\% instead than +xx $\times$}}
    }
    % \jx{We can potentially replace the non-reasoning model to beat o1-preview.}
    \label{fig:breakdown}
\end{figure}