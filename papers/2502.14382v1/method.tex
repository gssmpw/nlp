\begin{figure}[!t]
    \centering
    % First subfigure
    \includegraphics[width=0.45\textwidth]{figures/breakdown.pdf}      \caption{\textbf{Example overview of performance.}}
        \label{fig:breakdown}
\end{figure}

\begin{figure*}
    \centering
    \includegraphics[width=1.0\linewidth]{icml2024/figures/scaling_preview.pdf}
\caption{Scaling curve of~\frameworkname. With the best configuration, we match the performance of o1-preview model. \shangyin{what model is this?}}
\label{fig:scaling_preview}
\end{figure*}

% unique challenges for coding
% todo: make the transition smoother
\begin{figure}[th!]
    \centering
    \includegraphics[width=1.0\linewidth]{latex/figures/overview.pdf}
\caption{~\frameworkname~builds on the paradigm of a combination of revision and parallel samples. \jx{Be careful of this because we say we are a three-stage approach.} It first performs a self-refinement on the initial solution to improve algorithm efficiency, and subsequent self-debug round to correct solutions based on public test execution feedback. At the end, it undergoes a selection policy module to select the best solution to output.
\joey{I was confused by this figure as it looks likes Figure 2 from the Snell paper (probably Fig2a).  Does each square reflect a new decode call? It might help to show where teacher-forcing or prompting is done.  Like at the beginning show multiple different prompts to get k different generations. Then show each long context as a tall rectangle and then illustrate the selection process more. }
}
\label{fig:Design}
\end{figure}

\section{Method}
\label{sec:method}
In this section, we formally introduce our framework~\frameworkname. ~\frameworkname~takes in a coding problem $\mathbf{P}$, and a model $\mathbf{M}$. The model $\mathbf{M}$ attempts to a program solution $\mathbf{S(\cdot)}$ to maps an input to an output following the problem specification.

In our setup, a coding problem $\mathbf{P}$ contains a problem description in natural language. It contains two sets of test cases in the format of input and expected output: public test $T_{public} = \{(I_i, O_i)\}_{i=1}^{k_{public}}$, and private test $T_{private} = \{(I_i, O_i)\}_{i=1}^{k_{private}}$. Public tests are available to $\mathbf{M}$ to clarify the intent of the natural language description. Private tests are not available to $\mathbf{M}$. The solution is correct if $\mathbf{S}$ passes all the private tests, i.e. if $S(I_i) = O_i, \forall i = 1, ..., k_{private}$. This formulation has been proposed in benchmarks such as~\citet{chen2021evaluating, li2022competition, li2023taco, jain2024livecodebench, hendrycksapps2021, gulwani4foundations}. This is in contrast to math problems, where no additional information is given beyond the problem specification, and evaluation is done by exact matching on the final solution.


% In this section, we formally introduce the problem setup and the ~\frameworkname~framework. %an experiment setup to configure the default setting that we will use in our final experiment.
% \joey{I wouldn't add the experiment setup in the methods section.}


%\noindent\textbf{Problem Setup} We consider coding problems that are clearly specified and unambiguous, as pointed out to be important in~\citet{chowdhury2024introducing}. In particular, we assume coding problems that include a small amount of input and output demonstrations, a common setup as~\citep{chen2021evaluating, li2022competition, li2023taco, jain2024livecodebench, hendrycksapps2021, gulwani4foundations}. We provide one such example in~\autoref{sec:appendix_example_code}. \joey{This paragraph doesn't actually say much. Instead, you might say something that is more detailed and provides greater contrast to the math setting.  In the programming setting we are given a detailed description of a function to implement along with a set of $k$? test input-output pairs and asked to produce a program that produces the same output for each test input.  The model then produces an implementation that is evaluated on a hidden set of unit-tests.  This is in contrast to the math setting where no additional information is given beyond the problem statement and evaluation is done by exact matching on the final solution.}

\subsection{\frameworkname}
\frameworkname~is a two-stage framework: Generation stage and Selection stage.
The generation stage leverages parallel sampling to improve coverage, the fraction of problems that could be solved, jointly with self debugging from public tests to control the quality of such diverse samples~\autoref{sec:scale_parallel_samples}. The selection stage features a test-assisted LLM-as-a-judge approach, which leverages code execution to provide further information to enable accurate LLM evaluation (~\autoref{sec:scale_selection}).

% \frameworkname~is a simple and effective three-stage approach. 
%\joey{I don't think you need the words simple and effective.  They are okay in the intro but here just say it is a three stage approach that combines sequential and parallel scaling with a verification process.  Then provide a high-level description of each of the three phases and how they work together.  It should be clear at the end that this is simple and in the eval you will prove it is effective.}
% It incorporates the advantage of parallel scaling by allowing the model to generate multiple solutions to improve coverage~\citep{brown2024large}, sequential scaling by allowing the model to revise a previous code solution by providing execution feedback on available public demonstration, and that of a verifier by choosing the best one. A high-level illustration can be found at~\autoref{fig:Design}. % In this section, we will introduce each stage, and our findings based on a development set to enable the default setting of~\frameworkname.

\subsubsection{Stage 1: Generation}
% \section{Scaling Parallel Sampling}
\label{sec:scale_parallel_samples}
In the generation stage,~\frameworkname~integrates repeated sampling with the SELF-DEBUGGING approach. The model $\mathbf{M}$ first generates $\mathbf{N}$ coding solutions in parallel. Then perform SELF-DEBUGGING of these $\mathbf{N}$ samples independently. For each sample, if it passes $T_{publlic}$, then it exits for the selection phrase.








Recent work suggests that drawing multiple candidate solutions from language models can substantially improve the final result by increasing coverage of the solution space~\citep{brown2024large, li2022competition}. In this section, we evaluate how sampling many times in parallel scales performance across tasks of varying difficulty (Easy, Medium, Hard), under different temperatures, and with or without in-context examples. 
We adopt an \textit{oracle verifier} (i.e., the checker is assumed perfect) to isolate the effect of sampling itself, following the recommendations of \citet{stroebl2024inference}.
\joey{We only adopt the oracle in the evaluation? Does the method also assume an oracle?}

\paragraph{Repeated Sampling Improves Accuracy, even for reasoning models.}
Table~\ref{tab:sec4_vanilla-temp07} shows that, for \textbf{easy} problems, all models eventually reach nearly 100\% accuracy given sufficient samples (often by \(n=64\)). For instance, \textit{gpt-4o-mini} jumps from 80.7\% at \(n=1\) to 100\% by \(n=8\), and QwQ-32B-Preview achieves 100\% by \(n=2\). Meanwhile, on \textbf{medium} problems, stronger models exhibit larger gains, e.g., \textit{gpt-4o-mini} improves from 32.4\% at \(n=1\) to 70.3\% at \(n=128\), whereas a weaker model like \textit{Qwen2.5-Coder-7B-Instruct} moves from 8.1\% to 51.4\%. \textbf{Hard} problems benefit as well, albeit starting from low baselines; for example, \textit{Qwen2.5-Coder-32B-Instruct} rises from 2.04\% to 14.3\% by sampling 128 times. Interestingly, our preliminary experiments on a more recent reasoning-oriented model (\textit{QwQ-32B-Preview}) reveal a similar pattern: it scales from 4.08\% to 32.65\% on hard problems once sampled 64 times.

\paragraph{Medium temperature (0.7) improves performance, but higher temperature hurts (0.95).} To examine the influence of temperature on parallel sampling, we vary this parameter and present the results in Table~\ref{tab:temp-various}. Moderate temperatures (e.g., 0.2 to 0.7) typically yield performance improvements by promoting greater diversity among the generated samples. However, once temperature surpasses approximately 0.7, performance tends to deteriorateâ€”likely due to the introduction of excessive noise. For example, while \textit{gpt-4o-mini} attains a notable increase in \textbf{hard} task accuracy (from 2.04\% at 0.2 to 8.16\% at 0.95), the gains observed in medium-difficulty tasks do not follow an equally consistent upward trend. Indeed, some models experience performance regressions at higher temperatures, underscoring the trade-off between sampling diversity and solution consistency.

\paragraph{Using relevant in-context examples enhances generation diversity and performance.} 

Furthermore, we investigate whether augmenting prompts with various in-context examples improves parallel sampling. Our example set consists of 362 coding problems with correct solutions and reasoning generated by GPT-4o mini. We design two retrieval approaches for selecting examples. The first, \emph{ICL (BM25)}, uses a BM25 retriever to fetch the top-$k$ similar prompts and prepend each distinct one to different samples when $n = k$. While straightforward, this method may overlook semantics and fail to capture solution-level similarities. To address this, our second approach, \emph{ICL (Pattern)}, categorizes problems based on recurring solution techniques (e.g., dynamic programming) and retrieves examples of the same type. This pattern-based method aims to provide more relevant and structurally similar examples.

\begin{figure*}[ht]
    \centering
    % Placeholder figure
    \includegraphics[width=.8\linewidth]{latex/figures/icl_results.pdf}
    \caption{Performance on medium coding problems with varying numbers of parallel samples (\(n\)) across different methods: Zero-Shot, ICL (BM25), and ICL (Pattern). The results are shown for three models: GPT-4o-mini, Qwen2.5-Coder-7B-Instruct, and Qwen2.5-Coder-32B-Instruct.}
    \label{fig:icl_performance}
\end{figure*}

% Our preliminary findings suggest negligible or inconsistent gains, potentially due to limited marginal improvements in solution diversity once the model already samples multiple times. Further analysis of \emph{token usage} and \emph{response diversity} is underway to clarify whether reasoning-style prompts might yield greater benefit from parallel sampling compared to simpler, non-reasoning tasks.

\ref{tab:icl} and \ref{tab:diversity} demonstrate that selecting in-context examples increases response diversity, and 

\xiuyu{Will write results analysis -- takeaways beyond inconsistent performance improvements (difficulty to construct and retrieve useful examples, model dependent, relation to diversity and number of samples etc.). Future directions could be using a larger, more diverse, more relevant ICL examples database, or design more sophisticated retrieval pipeline.}

Overall, these results reinforce the value of parallel sampling in improving coverage and final accuracy, highlighting (i) the substantial gains on harder tasks achieved by stronger models, (ii) the beneficial but nuanced effect of temperature, and (iii) the complexity and potential limits of simple in-context approaches in further boosting parallel sampling.

\begin{table}[ht]
\centering
\small
\resizebox{0.5\textwidth}{!}{%
\begin{tabular}{l l r r r r r r r r}
\toprule
\textbf{Model} & \textbf{Difficulty} 
   & \textbf{n=1} & \textbf{n=2} & \textbf{n=4} & \textbf{n=8} 
   & \textbf{n=16} & \textbf{n=32} & \textbf{n=64} & \textbf{n=128} \\
\midrule
\multirow{3}{*}{gpt-4o-mini} 
 & Easy   & 80.7 & 88.5 & 92.3 & 100  & 100  & 100  & 100  & 100  \\
 & Medium & 32.4 & 37.8 & 46.0 & 48.9 & 56.8 & 64.9 & 67.5 & 70.3 \\
 & Hard   & 0.0  & 0.0  & 2.04 & 2.04 & 2.04 & 2.04 & 4.08 & 6.12 \\
\midrule
\multirow{3}{*}{Qwen2.5-Coder-7B-Instruct} 
 & Easy   & 46.2 & 65.4 & 76.9 & 84.6 & 88.5 & 96.2 & 100  & 100  \\
 & Medium & 8.1  & 18.9 & 21.6 & 32.4 & 40.5 & 46.0 & 51.4 & 51.4 \\
 & Hard   & 2.04 & 2.04 & 2.04 & 2.04 & 2.04 & 2.04 & 2.04 & 4.08 \\
\midrule
\multirow{3}{*}{Qwen2.5-Coder-32B-Instruct} 
 & Easy   & 75.9 & 84.6 & 96.2 & 100  & 100  & 100  & 100  & 100  \\
 & Medium & 35.1 & 37.8 & 43.2 & 48.9 & 51.4 & 54.1 & 59.5 & 62.1 \\
 & Hard   & 2.04 & 6.12 & 6.12 & 8.16 & 10.2 & 10.2 & 10.2 & 14.3 \\
\midrule
\multirow{3}{*}{QwQ-32B-Preview} 
 & Easy   & 84.6 & 100  & 100  & 100  & 100  & 100  & 100  & 100  \\
 & Medium & 43.2 & 54.5 & 75.7 & 86.5 & 86.5 & 91.9 & 91.9 & R    \\
 & Hard   & 4.08 & 20.4 & 16.3 & 22.45 & 24.5 & 32.65 & R   & R    \\
\bottomrule
\end{tabular}
}
\caption{Vanilla --- temp=0.7. The effect of repeated sampling on Easy, Medium, and Hard tasks across different models.
\joey{Can we make this three line plots (Easy, Medium, Hard) with $n$ as the x axis.  Show markers (with numbers) and use log-scale on the x-axis. Each model should be a different line.}
}
\label{tab:sec4_vanilla-temp07}
\end{table}

\begin{table}[ht]
\centering
\small
\resizebox{0.5\textwidth}{!}{%
\begin{tabular}{l l r r r r}
\toprule
\textbf{Model} & \textbf{Difficulty} & 
\textbf{temp=0.2} & \textbf{temp=0.5} & \textbf{temp=0.7} & \textbf{temp=0.95} \\
\midrule

%============= GPT-4o-mini
\multirow{3}{*}{\begin{tabular}{c}
gpt-4o-\\
mini
\end{tabular}}
 & Easy   & 100   & 100   & 100   & 100   \\
 & Medium & 59.5  & 64.8  & 70.3  & 70.3  \\
 & Hard   & 2.04  & 4.10  & 6.12  & 8.16  \\
\midrule

%============= Qwen2.5-Coder-7B-Instruct
\multirow{3}{*}{\begin{tabular}{c}
Qwen2.5-Coder-\\
7B-Instruct
\end{tabular}}
 & Easy   & 80.8  & 96.2  & 100   & 100   \\
 & Medium & 37.9  & 51.4  & 51.4  & 48.9  \\
 & Hard   & 2.04  & 2.04  & 4.08  & 2.04  \\
\midrule

%============= Qwen2.5-Coder-32B-Instruct
\multirow{3}{*}{\begin{tabular}{c}
Qwen2.5-Coder-\\
32B-Instruct
\end{tabular}}
 & Easy   & 100   & 100   & 100   & 100   \\
 & Medium & 54.1  & 62.2  & 62.1  & 65.0  \\
 & Hard   & 10.2  & 12.24 & 14.3  & 12.2  \\
\midrule
\end{tabular}
} 
\caption{Effect of temperature: benefits are observed up to around 0.7, but performance may drop with higher temperature (0.95).
\joey{Can we make this a line plot following the same setup in as proposed for Table 1.}
}
\label{tab:temp-various}
\end{table}

\textbf{Code Diversity} Use execution to cluster responses, measure corresponding final results. % \DL{@xiuyu will help}

%\textbf{Token Efficiency} \textbf{@Shiyi:} Whether QwQ is worth it.
%\DL{Soon}

\subsubsection{Stage 2: Sequential Scaling}
\label{sec:scale_revision}

\paragraph{Setting Description.}
To explore the potential of revision, we leverage the self-debug paradigm~\citep{chen2023teaching}, which executes the generated code on the public tests, and ask the LLMs to further debug based on the error messages. We study three self-debug paradigms in this section - the vanilla one which simply appends the context with new error messages. Last-Round: which only uses the code and error fround the last round of self-debug, to avoid problems such as lost-in-the-middle. Generated tests - public tests can only cover a small portion of tests, we further generate AI-tests to debug according to AlphaCodium. We use 8 parallel samples and assume an oracle verifier in this section.

\DL{Messages:
1. Similar effect of self-debug and scaling samples. e.g. spending 4x more compute on self-debugging gpt-4o-mini improves 13\% while 16\% in parallel samples.
2. Self-debug techniques quickly saturated in 3-4 rounds. Easier problems require fewer rounds to reach upper bound.
3. Further debugging on generated tests or managing context do not improve result.
}
\paragraph{Usefulness of Self-Debugging.}
We first examine how well self-debugging improves performance and how it compares to repeated sampling across different scenarios. As shown in Table~\ref{tab:revision_comparison}, allowing the model to iteratively refine its output (\emph{Vanilla} approach) can yield appreciable gains on \textbf{easy} and, to a lesser extent, \textbf{medium} tasks. For example, \textit{gpt-4o-mini} improves from 96.2\% at \(r=0\) to 100\% by \(r=1\) on easy problems, while \textit{Qwen2.5-Coder-32B-Instruct} moves from 45.9\% to 59.5\% on medium tasks. However, all models struggle with \textbf{hard} tasks: even multiple rounds of refinement tend to yield only marginal gains (e.g., 2.04\% to 4.08\% or 6.12\%, see also \S\ref{sec:scale_parallel_samples} for parallel sampling results). In practice, repeated sampling often provides broader coverage of potential solutions, whereas self-debugging helps refine an existing candidate. The two methods can complement each other, though our findings indicate that repeated sampling alone is frequently more impactful on harder items.

\subsubsection{Selection}
\label{sec:scale_selection}
After Stage 2, there are N samples that needed to be selected. In this section, we consider several ways to do this selection, and propose our simple solution to better select candidates. We consider the following algorithms.

\paragraph{Generated tests based} The most straightforward way to do selection is to do majority voting. Unlike math domains, where results are matched by exact match, in coding, we need to generate test inputs -> Generated test inputs + majority voting on the output. The other way is to do generated inputs and generated test outputs, also a popular choice~\citep{ehrlich2025codemonkeys}.

\paragraph{LLM-as-a-judge based} This is often used in natural language as the way to select the better samples.


Some ablation here - we find that generated tests + majority voting is not reliable because majority is wrong for hard problems. Using test outputs are also wrong because usually that assumes a strong model.

On the other hand, LLM-as-a-judge can not found the correct one, because llm can hardly reason on code.

Based on this, we develope a hybrid approach, shown in some figure, that first uses test inputs to cluster into several groups. then we use llm-as-a-judge. However, we do not simply use llm-as-a-judge, as we know it does not work. We let the llm to propose a test input that can potentially distinguish samples, execute it back, and then based on the results, do the selection, We found this consistently improve the result.\DL{Add figure, and a table here.}
