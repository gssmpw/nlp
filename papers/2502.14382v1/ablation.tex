\section{Ablation Studies}
\label{sec:ablations}
% In this section, we conduct ablation studies to analyze the key components of \frameworkname. We examine the impact of different variations of iterative debugging (\cref{ssect:ablation_selfdebug}) and selection policies (\cref{sec:ablation_selection}). Additionally, we investigate how different hyper-parameter choices, such as the temperature setting and number of samples, affect parallel sampling performance (\cref{ssect:ablation_parallel}). We also assess the impact of further incorporating in-context example retrieval into the parallel sampling (\cref{sec:ablation_ICL}). All experiments in this study are conducted on LiveCodeBench (v4).
In this section, we conduct ablation studies to analyze the key components of \frameworkname, focusing on the effectiveness and variations within each stage of the framework. We evaluate the following aspects:
\begin{enumerate}
\item \textbf{Parallel Scaling:} We analyze the impact of different hyper-parameter choices, such as the temperature setting and the number of samples, on parallel sampling performance (\cref{ssect:ablation_parallel}). Additionally, we investigate the effect of incorporating in-context example retrieval into the parallel sampling process (\cref{sec:ablation_ICL}). We find that moderate temperatures improve performance, and adding ICL example can potential further improve performance.
\item \textbf{Sequential Scaling:} We explore variations of the iterative debugging process, including self-debugging with model-generated test cases (\cref{ssect:ablation_selfdebug}). We find that iteratively debugging from test execution feedback improve performance, even for reasoning models. We find that simply appending execution results from public tests for every iteration works the best.
\item \textbf{Selection Policy:} We assess the performance of different selection policies, comparing our adaptive input synthesis approach with alternative selection strategies (\cref{sec:ablation_selection}). We find that our adaptive input synthesis selection method is consistently more reliable than the generated tests and the LLM judge selection method.
\end{enumerate}
All ablation experiments are conducted on LiveCodeBench (v4).

\subsection{Parallel Sampling Hyper-Parameters}
\begin{figure}[h]
    \centering
    % Placeholder figure
    \includegraphics[width=.99\linewidth]{figures/temperature_num_samples.pdf}
    \caption{\textbf{The effect of hyper-parameters}. Left: The impact of temperature. A moderate temperature (0.7) balances diversity and quality, leading to higher Pass@N. In contrast, a higher temperature (0.95) does not further improve Pass@N, potentially degrading code quality. Right: The effect of increasing the number of samples. Performance improves log-linearly.}
    \label{fig:exp_hyper_parameters}
    % \vspace{-4mm}
\end{figure}
\label{ssect:ablation_parallel}

We examine the impact of two key factors in parallel sampling: temperature and the number of parallel samples. Understanding their influence is essential for optimizing test-time scaling strategies.

\myparagraph{Moderate temperatures improve performance, but high temperatures degrade it.}
%To analyze temperature effects, we vary  and report results in \cref{fig:exp_hyper_parameters} (left). 
\cref{fig:exp_hyper_parameters} (left) shows that moderate temperatures (0.2–0.7) enhance performance by balancing exploration and sample diversity. However, beyond 0.7, performance plateaus or declines, likely due to excessive randomness introducing noise. Some models, such as \qwensevenb, exhibit performance regression at higher temperatures, emphasizing the trade-off between diversity and solution consistency. These findings suggest that while moderate temperatures improve generation quality, excessively high values reduce code quality.


% For example, while \textit{\fouromini} attains a notable increase in \textbf{hard} task accuracy (from 2.04\% at 0.2 to 8.16\% at 0.95), the gains observed in medium-difficulty tasks do not follow an equally consistent upward trend. Indeed, some models experience performance regressions at higher temperatures, underscoring the trade-off between sampling diversity and solution consistency.

% Overall, these results reinforce the value of parallel sampling in improving coverage and final accuracy, highlighting (i) the substantial gains on harder tasks achieved by stronger models, (ii) the beneficial but nuanced effect of temperature.

\myparagraph{Repeated sampling improves performance, even for reasoning models.} 
% \myparagraph{Repeated sampling improves performance, even for reasoning models.} \autoref{fig:temp-various} (right) shows that, for \textbf{easy} problems, all models eventually reach nearly 100\% accuracy given sufficient samples (often by \(n=64\)). For instance, \textit{\fouromini} jumps from 80.7\% at \(n=1\) to 100\% by \(n=8\), and QwQ-32B-Preview achieves 100\% by \(n=2\). Meanwhile, on \textbf{medium} problems, stronger models exhibit larger gains, e.g., \textit{\fouromini} improves from 32.4\% at \(n=1\) to 70.3\% at \(n=128\), whereas a weaker model like \textit{Qwen2.5-Coder-7B-Instruct} moves from 8.1\% to 51.4\%. \textbf{Hard} problems benefit as well, albeit starting from low baselines; for example, \textit{Qwen2.5-Coder-32B-Instruct} rises from 2.04\% to 14.3\% by sampling 128 times. Interestingly, our preliminary experiments on a more recent reasoning-oriented model (\textit{QwQ-32B-Preview}) reveal a similar pattern: it scales from 4.08\% to 32.65\% on hard problems once sampled 64 times.
As shown in \cref{fig:exp_hyper_parameters} (right), increasing the number of parallel samples significantly improves performance across all models. Notably, \qwensevenb, the weakest performer at \( N=1 \), shows the largest gain, exceeding 35\% at \( N=64 \). Similarly, the more capable reasoning-model (QwQ-32B-Preview) follows the same trend, though its gains plateau beyond \( N=32 \). Nevertheless, it improves substantially, rising from 50\% at \( N=1 \) to 80\% at \( N=32 \). These results confirm that increasing the number of parallel samples is a simple yet effective strategy for enhancing performance in both instruction-following and reasoning-based models.

\subsection{Impact of In-Context Examples} 
\label{sec:ablation_ICL}

\begin{figure}[h]
    \centering
    % Placeholder figure
    \includegraphics[width=.95\linewidth]{figures/icl_results.pdf}
    \caption{\textbf{Performance with in-context examples across different numbers of parallel samples (\(N\))}, for \fouromini, Qwen2.5-Coder-7B-Instruct, and Qwen2.5-Coder-32B-Instruct.}
    % \vspace{-5mm}
    \label{fig:icl_performance}
\end{figure}
% Using relevant in-context examples enhances generation diversity and performance.

While \frameworkname primarily focuses on repeated sampling for parallel scaling, it can be integrated with more advanced parallel scaling techniques. For instance, varying input prompts can create more diverse responses~\citep{lambert2024t}, which in turn may lead to better coverage.
In this ablation study, we investigate whether augmenting prompts with in-context examples can further improve parallel scaling performance.

% We investigate whether augmenting prompts with in-context examples enhances parallel scaling performance%\shiyi{add citation}
% , as existing works show varying input prompts can create more diverse responses~\citep{lambert2024t}.

We construct an example set from LiveCodeBench (v2) containing correct solutions and reasoning traces generated by GPT-4o mini. We explore two retrieval approaches for selecting in-context examples. \emph{ICL (BM25)} retrieves the top-$k$ similar prompts using a BM25 retriever and prepends each to a different sample when $n = k$~\citep{robertson2009probabilistic}. This approach is simple but may overlook solution-level similarities. \emph{ICL (Pattern)} groups problems by techniques (e.g., dynamic programming) and retrieves examples from the same technique, aiming to provide more relevant and structurally similar examples.


% The first, \emph{ICL (BM25)}, uses a BM25 retriever to fetch the top-$k$ similar prompts when $n = k$, and prepend each distinct one to a different sample. While straightforward, this method may overlook semantics and fail to capture solution-level similarities. To address this, our second approach, \emph{ICL (Pattern)}, categorizes problems based on recurring solution techniques (e.g., dynamic programming) and retrieves examples of the same type. This heuristic-driven, pattern-based method aims to provide more relevant and structurally similar examples. 

We evaluate medium-difficulty problems from LiveCodeBench (v4) with oracle selection. As shown in \cref{fig:icl_performance}, performance is highly sensitive to in-context example quality. ICL (BM25) performs similarly to or worse than the zero-shot baseline in most cases, except for $n=64$ with \qwenthirtytwob. In contrast, ICL (Pattern) outperforms the baseline when $n \geq 8$ for \qwensevenb and $n \geq 4$ for \qwenthirtytwob, while showing comparable performance with \fouromini.

These results highlight that selecting high-quality examples is crucial, and naive retrieval methods often fall short. Although ICL itself is promising, its performance is sensitive to example quality and retrieval effectiveness. We regard it as future work to develop robust ICL techniques that can be integrated into \frameworkname to further enhance parallel scaling performance.

% We conduct evaluation on medium-difficulty problems in LiveCodeBench (v4) with oracle selection. As shown in \cref{fig:icl_performance}, the choice of in-context examples significantly impacts performance. ICL (BM25) performs similarly to or worse than the zero-shot baseline in most settings, except for $n=64$ with \qwenthirtytwob. In contrast, ICL (Pattern) consistently outperforms the baseline when $n \geq 8$ for \qwensevenb and $n \geq 4$ for \qwenthirtytwob, while performing comparably with \fouromini across all $n$ samples. 
% This underscores that retrieving the right in-context examples is both challenging and crucial—simple methods often fall short. Notably, the right examples could improve parallel sampling performance, especially when $n$ is large. Although we choose not to incorporate them into our final iterative debugging solution to avoid additional complexity, the results point toward future directions. In particular, building a larger, more diverse, and more relevant ICL example database and developing better retrieval approaches show promise to further enhance performance.

% Overall, these results reinforce the value of parallel sampling in improving coverage and final accuracy, highlighting (i) the substantial gains on harder tasks achieved by stronger models, (ii) the beneficial but nuanced effect of temperature, and (iii) the complexity and potential limits of simple in-context approaches in further boosting parallel sampling.



\subsection{Impact of Iterative Debugging Variants}
\label{ssect:ablation_selfdebug}
\begin{figure}[h]
    \centering
    % Placeholder figure
    \includegraphics[width=.99\linewidth]{figures/iterative_debug_variants.pdf}
    \caption{\textbf{Comparison of three iterative debugging approaches}: \textit{Public Tests},  \textit{+ Generated Tests}  and \textit{Last Round Context}. Results are obtained with \(N=8\), \(\text{temperature}=0.7\) and up to four rounds of debugging.}
    \label{fig:iterative_debug_performance}
    \vspace{-5mm}
\end{figure}
We examine the effectiveness of three variants of iterative debugging: (1) \textbf{Public Tests}: The model iteratively debugs using public tests and stops once the sample passes all of them. (2) \textbf{+Generated Tests}: In addition to public tests, the model continues debugging on model-generated tests following the algorithm in~\citep{ridnik2024code}. (3) \textbf{Last Round Context}: The model iteratively debugs using only public tests, but instead of using code samples from all previous rounds for debugging, it uses only the last round of code sample as context. This is motivated by observations that LLMs may perform sub-optimally when handling large context windows~\citep{liu2024lost}.

\cref{fig:iterative_debug_performance} summarizes the result. We find that: (1) \textit{Even though reasoning models implicitly perform self-reflection and revising, they benefit from explicit debugging through test execution feedback}: the performance of QwQ-32B-Preview model improves from 72.6 to 74.2 with 2 rounds of debugging. 
(2) \textit{Reducing the context window or considering more model-generated tests does not show consistent improvement}: while using only the last round of context improves performance for the Qwen2.5-Coder-7B-Instruct model, it results in worse performance for the other two models. Similarly, incorporating additional model-generated tests does not enhance performance for \fouromini.
(3) \textit{The benefits of iterative debugging tend to plateau, typically after 2–3 rounds}: this finding aligns with the observation that the benefit of sequential scaling flattens out~\citep{muennighoff2025s1}. Motivated by these findings, we choose to use 2 round of debugging, only on public tests for simplicity, and apply iterative debugging even for reasoning models in~\cref{sec:exp_main}.

% As shown in Table~\ref{tab:revision_comparison}, allowing the model to iteratively refine its output (\emph{Vanilla} approach) can yield appreciable gains on \textbf{easy} and, to a lesser extent, \textbf{medium} tasks. For example, \textit{\fouromini} improves from 96.2\% at \(r=0\) to 100\% by \(r=1\) on easy problems, while \textit{Qwen2.5-Coder-32B-Instruct} moves from 45.9\% to 59.5\% on medium tasks. However, all models struggle with \textbf{hard} tasks: even multiple rounds of refinement tend to yield only marginal gains (e.g., 2.04\% to 4.08\% or 6.12\%, see also \S\ref{sec:scale_parallel_samples} for parallel sampling results). In practice, repeated sampling often provides broader coverage of potential solutions, whereas self-debugging helps refine an existing candidate. The two methods can complement each other, though our findings indicate that repeated sampling alone is frequently more impactful on harder items.

% We also investigated whether context management (e.g., hiding or exposing chain-of-thought) or iterative “further debugging” steps (e.g., \emph{Last-Round Self-Refine} or \emph{Generated Tests}) add additional value. As seen in Table~\ref{tab:revision_comparison}, these more elaborate debugging strategies rarely surpass the simpler \emph{Vanilla} self-debug. For instance, \textit{gpt-4o-mini} remains at around 2.04\% on hard tasks despite multiple last-round refinements or generated-test prompts, and \textit{Qwen2.5-Coder-7B-Instruct} plateaus near 37--40\% on medium tasks. Preliminary exploration of “reasoning” models (e.g., \textit{QwQ-32B-Preview}) suggests a similarly limited impact, but further investigation is ongoing. Hence, neither specialized context management nor extra debugging steps appears to deliver robust improvements, particularly for challenging items.



\subsection{Impact of Different Selection Policies}
\label{sec:ablation_selection}
\begin{table}[h]
\centering
\resizebox{0.48\textwidth}{!}{%
% \begin{tabular}{l|cc>{\columncolor{lightblue}}c}
\begin{tabular}{l|cccc}
\toprule
\textbf{Model} & Public & Generated & LLM & Adaptive Input\\ 
& Only & Tests & Judge & Synthesis (Ours)\\ 
\hline
Qwen-Coder-7B     &  42.3 & 42.3  & 42.3 &  \textbf{44.5} \\
Qwen-Coder-32B    &  54.6 & 57.8  & 55.5 &  \textbf{58.3} \\
\fouromini       &  54.1 &  55.2 & 56.3 & \textbf{57.3} \\
QwQ-32B-Preview & 64.3 & 65.9 & 68.6 &  \textbf{69.7}\\
\rowcolor{black!10} Avg. & 53.8 & 53.1 & 55.6 & \textbf{57.5}\\
\bottomrule
\end{tabular}%
}
\caption{\textbf{Pass@1 Performance comparison between different selection methods on LiveCodeBench(v4).} 
Bold text denotes the best performance of the same model. 
"Qwen-Coder", "R1-Distill" is short for "Qwen2.5-Coder-Instruct" and "DeepSeek-R1-Distill-Qwen". Results are obtained with N=8 and temperature=0.7. \textit{Our Adaptive Input Synthesis method achieves better accuracy over other methods.}}
\label{tab:diff_selection}
\vspace{-2mm}
\end{table}
We compare different policies for selecting the best sample after iterative debugging. We evaluate four approaches: 
% (1) \textbf{Random}: randomly selecting one sample after debugging; \xiuyu{"after debugging" sounds misleading since random doesn't benefit from debugging? Also table 2 does not contain random -- should be removed here if we don't want to include it.} 
(1) \textbf{Public Only}: using only public test cases for selection and randomly selecting a sample if it passes all tests; (2) \textbf{Generated Tests}: applying public test filtering followed by additional test case generation using \fouromini, selecting the sample that passes the most test cases; (3) \textbf{LLM Judge}: applying public test filtering and then using LLMs for pairwise selection among code samples; and (4) \textbf{Adaptive Input Synthesis} —applying the selection algorithm described in~\ssect{sec:scale_selection} with \fouromini after public test filtering. % Our experiments are conducted using 8 parallel samples with a temperature of 0.7 in LiveCodeBench v4.

% ~\autoref{tab:exp_diff_strategies} summarizes the performance. We finds that firstly, selection is important, even for strong model after iterating on several rounds of self-debugging: For instance, o1-mini benefits from simple public test filtering by 8\%, even though these tests are available during its self-debugging procedure. Secondly, neither LLM-as-a-judge or generated tests can relaibly generate better selection than the available public tests. In contrast, the test-assisted LLM-as-a-judge can provide stable improvement. 
\cref{tab:diff_selection} summarizes the results. Notably, the Generated Tests approach does not yield improvements over the Public Only baseline. 
This is due to errors in model-generated outputs, which, when applied to poorly chosen inputs, introduce significant noise in the selection process, ultimately degrading performance. 
In contrast, our Adaptive Selection method enables the LLM to strategically select an input that best differentiates samples while avoiding the need to predict outputs. By leveraging real execution outputs rather than model predicttions, the LLM makes more reliable decisions, leading to improved selection accuracy. 
%  Our findings indicate that selection plays a crucial role, even for strong models after multiple rounds of self-debugging. For example, o1-mini benefits from simple public test filtering with an 8\% performance improvement, despite these tests being available during its self-debugging process. Furthermore, neither LLM-as-a-Judge nor generated tests consistently outperform selection based on public tests. In contrast, our test-assisted LLM-as-a-Judge approach provides significantly more stable and reliable improvements.  



