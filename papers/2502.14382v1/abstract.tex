\begin{abstract}
%This document provides a basic paper template and submission guidelines.
% Abstracts must be a single paragraph, ideally between 4--6 sentences long.
% Gross violations will trigger corrections at the camera-ready phase.
Increasing test-time compute for LLMs shows promise across domains but remains underexplored in code generation, despite extensive study in math. In this paper, we propose \frameworkname, the first hybrid test-time scaling framework that substantially improves the coverage and selection accuracy of generated code. \frameworkname extends the existing parallel scaling paradigm with sequential scaling to push performance boundaries. It further leverages a novel selection mechanism that adaptively generates distinguishing inputs for pairwise comparison, combined with execution-grounded information to robustly identify correct solutions.
% combining LLM-based evaluation with execution-grounded verification to robustly identify correct solutions.
% Increasing the test-time compute of large language models (LLMs) has shown promising results. While extensive studies have been conducted in the math domain, the counterpart in code generation has been less explored. In this paper, we propose a simple and effective two-stage approach we call ~\frameworkname that improves (1) current repeated sampling paradigm by integrating revision from code execution feedback to improve the quality of each code samples, and (2) current majority-voting selection based method by grounding LLM-based selection method with code execution results.
% LLM-as-a-judge paradigm with test case execution grounding. 

%\kurt{as I'll bring up later, I think the consistent/uniform improvements across models is more impressive than the smaller models surpass larger ones 
%- unless you point to a new application that the smaller models can now handle that they could not before.}
We evaluate across 12 Large Language Models and Large Reasoning Model and show: (1) \frameworkname consistently improves performance across model families and sizes, enabling a 3B model to outperform GPT-4o-mini; (2) \frameworkname  enables non-reasoning models to surpass reasoning models—GPT-4o-mini with \frameworkname outperforms o1-preview by 3.7\% on LiveCodeBench; (3) \frameworkname 
further boosts state-of-the-art reasoning models—DeepSeek-R1-Distill-Qwen-32B with \frameworkname achieves 85.7\% on LiveCodeBench, approaching o1 (high) at 88.5\%.  
Code will be available under \url{https://github.com/NovaSky-AI/SkyThought}.
%Anonymous code is available at \url{https://anonymous.4open.science/r/TestTimeCodeGen-1BB1}.

% Evaluated on 12 Large Language Models or Large Reasoning Models across model sizes,\frameworkname consistently improves the model capability. In particular, it allows non-reasoning models to outperform reasoning models: gpt-4o-mini with \frameworkname outperforms O1-Preview by 3.7\% on LiveCodeBench. It allows open reasoning models to match the state-of-the-art performance: DeepSeek-R1-Distill-Qwen-32B with \frameworkname achieves 85.7 on LiveCodeBench, competitive to O1 (high) with 88.5.
%Evaluated on 12 Large Language Models or Large Reasoning Models across model sizes, \frameworkname consistently improves the model capability: (1) allows smaller models to surpass larger models: Qwen-2.5-Coder-7B-Instruct model outperforms its 32B variant by 10.7\% on LiveCodeBench, (2) allows non-reasoning models to surpass reasoning models: gpt-4o-mini model outperforms O1-Preview by 3.7\% on LiveCodeBench, (3) allows open reasoning models to match the state-of-the-art proprietary models: DeepSeek-R1-Distill-Qwen-32B achieves 85.7 on LiveCodeBench, competitive to O1 (high) with 88.5. 
% Evaluations on LiveCodeBench and CodeContest show that ~\frameworkname~is consistently better than a majority-voting based approach and SELF-DEBUGGING approach by \textcolor{red}{XX}.

% In this paper, we propose our framework~\frameworkname~that addresses several key and basic problems of the direction.~\frameworkname~has three stages: (1) repeated sampling with medium temperature to balance diversity and code quality, (2) self-debug that utilizes test demonstrations in the problem, and (3) test-assisted verifier that selects the best sample. We evaluate ten models across model sizes, open and closed, and both instruction-based or reasoning models.~\frameworkname~has shown consistent improvement, allowing smaller models to outperform 10x larger models \shangyin{by how much? reviewers would be interested}, older models to outperform newer models, and reasoning models to set the new state-of-the-art. Code is available at~\url{https://github.com/NovaSky-AI/S*}. \jx{Will we replace this with an anonymous link?}
\end{abstract}