
\section{Method}
\label{sec:method}

% In this section, we introduce the \frameworkname framework. 
\frameworkname takes as input a coding problem $\mathbf{P}$ and a code generation model $\mathbf{M}$. The model $\mathbf{M}$ aims to generate a program solution $\mathbf{X(\cdot)}$ that maps inputs to outputs according to the problem specification.

We adopt the standard setup widely used in existing coding benchmarks~\citep{chen2021evaluating, li2022competition, li2023taco, jain2024livecodebench, hendrycksapps2021, gulwani4foundations}. Each coding problem $\mathbf{P}$ consists of a natural language description and a set of public and private test cases, each represented as input-output pairs. 

Private tests evaluate the correctness of $\mathbf{X}$ but remain inaccessible to $\mathbf{M}$ during code generation. A solution is considered correct if it passes all private tests. In contrast, public tests are provided to clarify the problem's intent and are typically included in the prompt. Public tests are usually far fewer than private tests; for instance, in CodeContests~\citep{li2022competition}, there are, on average, 2.0 public tests and 202.1 private tests per problem. This contrasts with mathematical reasoning tasks, where evaluation typically relies on exact string matching of the final solution without additional test information~\citep{li2024numinamath}.

\subsection{The \frameworkname Framework}
\frameworkname is a two-stage hybrid test-time scaling framework consisting of \emph{Generation} and \emph{Selection} stages, as demonstrated in~\cref{fig:Design}. It extends parallel sampling with sequential sampling via iterative debugging to \emph{improve coverage} and employs adaptive input synthesis during selection to \emph{enhance selection accuracy}, leveraging execution results throughout the process. An example of effect for different stages can be found in~\cref{fig:breakdown}.

\paragraph{Stage 1: Generation.}
\label{sec:scale_parallel_samples}
In the generation stage, \frameworkname improves coverage by extending parallel scaling with sequential scaling through \textit{iterative debugging grounded with execution feedback}. Specifically, \frameworkname first generates $\mathbf{N}$ initial samples independently, leveraging parallel sampling techniques~\citep{chen2023teaching}. Each sample is then refined through up to $\mathbf{R}$ rounds of sequential revision, informed by execution results on public test cases. The revision process halts once a sample passes all public tests or reaches the maximum number of revision attempts.
% \shiyi{We also experimented with self-generated tests for additional debugging rounds but observed no further performance gains (see~\cref{ssect:ablation_selfdebug}) is removed from here for cleaner message.}
% We also experimented with self-generated tests for additional debugging rounds but observed no further performance gains (see~\cref{ssect:ablation_selfdebug}).



\paragraph{Stage 2: Selection.}
\label{sec:scale_selection}
After generating $\mathbf{N}$ candidate solutions, the next step is to identify the best one. Since the public tests are already used during the generation stage, additional evaluation is needed for reliable selection. We investigate two existing approaches: (1) LLM-as-a-judge~\citep{zheng2023judging}, which relies on pre-trained knowledge to compare candidate solutions, and (2) generated test cases~\citep{li2022competition, chen2022codet}
%\shiyi{add citation to codeT}, 
which uses synthesized test cases to guide selection.

Unfortunately, we find that LLM-based judging alone often struggles to predict program behavior accurately, while generated tests frequently fail to provide reliable outputs for grounding the selection or to produce high-quality inputs that effectively distinguish samples (see \cref{tab:diff_selection}).

% Unfortunately, we find that LLM-based judging alone often struggles to predict program behavior accurately, while generated tests frequently fail to produce high-quality inputs that effectively distinguish samples (see \cref{tab:diff_selection}).

\input{algorithm}

To overcome these limitations, \frameworkname introduces \emph{adaptive input synthesis}, a hybrid selection approach that integrates LLM evaluation with execution-grounded verification, as illustrated in Algorithm~\ref{alg:selection1}. First, we prompt an LLM to synthesize a set of test inputs. We execute these inputs and cluster the $\mathbf{N}$ samples based on their execution outputs (Line~\ref{line:execute} to Line~\ref{line:cluster})~\citep{li2022competition}. We then perform pairwise comparisons across clusters: for each comparison, we prompt the LLM to generate distinguishing inputs, execute both samples using these inputs, and select the superior one based on the execution results (Line~\ref{line:adaptive} to Line~\ref{line:increase}). This adaptive selection process grounds LLM evaluations in concrete execution feedback, resulting in more reliable and accurate sample selection compared to naive LLM judging or generated tests-based methods (see~\cref{sec:exp}).





\begin{table*}[!t]
\centering
\renewcommand{\arraystretch}{1.2}
\setlength{\tabcolsep}{6pt}
\resizebox{0.9\textwidth}{!}{%
\begin{tabular}{l cccccc c ccc c c}
\toprule
\textbf{Method} 
& \multicolumn{6}{c}{\textbf{Qwen2.5-Coder-Instruct}} 
& \textbf{4o-mini} 
& \multicolumn{3}{c}{\textbf{R1-Distill}} 
& \textbf{QwQ} 
& \textbf{o1-mini} \\
\cline{2-7} \cline{9-11}
& \textbf{0.5B} & \textbf{1.5B} & \textbf{3B} & \textbf{7B} & \textbf{14B} & \textbf{32B}
& 
& \textbf{7B} & \textbf{14B} & \textbf{32B}
& 
& \\
\midrule
\textbf{Zero-Shot}      
& 1.2  & 7.0  & 18.4 & 29.4 & 44.8 & 47.4 
& 40.9 
& 48.4 & 63.2 & 69.1 
& 62.1 
& 76.5 \\
\textbf{Majority Vote}  
& 2.5  & 11.0 & 25.2 & 40.5 & 50.8 & 55.9 
& 46.6 
& 58.7 & 68.1 & 75.8 
& 67.3 
& 81.6 \\
\textbf{Self-Debugging} 
& 2.4  & 9.4  & 27.8 & 39.9 & 51.5 & 59.5 
& 51.7 
& 58.4 & 66.2 & 70.1 
& 59.3 
& 79.9 \\
\textbf{S*}             
& \textbf{10.9} & \textbf{27.6} & \textbf{42.7} & \textbf{54.4} & \textbf{64.6} & \textbf{70.1} 
& \textbf{61.3} 
& \textbf{73.2} & \textbf{82.8} & \textbf{85.7} 
& \textbf{76.3}
& \textbf{85.3} \\
\bottomrule
\end{tabular}%
}
\caption{\textbf{Pass@1 of zero-shot, majority voting~\citep{wang2022self, li2022competition}, self-debugging~\citep{chen2023teaching}, and \frameworkname on LiveCodeBench (v2)}. Bold text denotes the best performance. "R1-Distill", "QwQ", "4o-mini" is short for "DeepSeek-R1-Distill-Qwen"~\citep{guo2025deepseek}, "QwQ-32B-Preview"~\citep{qwen_qwq_2024}, and "GPT-4o-mini"~\citep{achiam2023gpt} respectively. \textit{\frameworkname consistently outperforms other baselines.}}\label{tab:exp_diff_strategies}
%\vspace{-5mm}
\end{table*}