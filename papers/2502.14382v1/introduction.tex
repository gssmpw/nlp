\section{Introduction}
% Scaling test time is important. There has been quite some work in integrating parallel, sequential, prm, in the math domain. But coding domain has less been explored. This may be due to that . Thus we ask. how can we build the simplest and effective framework to scale coding domain compute effectively?
Scaling test-time compute for Large Language Models (LLMs) has increasingly improved their performance. ~\citet{muennighoff2025s1} introduces 

\joey{
I think the intro is struggling to reverse engineer a problem from the solution. 
Instead, why don't you start by saying that there is a lot of excitement around test-time compute and reasoning capabilities in models.  Then cite \cite{muennighoff2025s1}   as introducing a simple  taxonomy of these techniques and exploring their how to combine these techniques in the context of mathematical reasoning capabilities. (all in the first paragraph)  You then briefly describe their parallel and sequential framing.
Then in the second paragraph you can say in this paper we explore how to combine sequential and parallel scaling in the context of coding tasks. You should then say what is different about coding tasks.  
Then you can jump straight to in this paper we propose frameworkname
}

%\shiyi{Intro outline:
%\begin{enumerate}
%\begin{enumerate}
%    \item Scaling test-time computing is promising.
%    \item What does existing work do? Introduce the commonly used three-stage pipeline.
%    \item However, unlike math, coding presents unique challenges and opportunities within each of these three stages, which remains largely underexplored. Describe the challenges in these three stages.
%    \item To address these challenges, we propose \frameworkname, with optimizations 1, 2, 3, ..., which together boost the performance to xxx.
%    \item Additionally, we conduct a comprehensive ablation study over the large design space to justify our design choices.
%\end{enumerate}
%}
% Increasing the test time compute has been a new trend in increasing LLM performance. As evidenced by the insights from large language monkeys, and the promising results on Math problems in the Deepminds paper.
% v2 --
Scaling test-time compute for Large Language Models (LLMs) has increasingly improved their performance. Mainstream methods often fall into two categories. 
Firstly, \textbf{Sequential} Scaling allows the model to reflect and revise a single Chain-of-thought like solution~\citep{openai_learning_to_reason_2024, deepseek_r1_lite_2024, sky_t1_2025, guo2025deepseek,qwen_qwq_2024}.
\joey{I don't think Sequential Scaling is an established term.  What you are calling sequential scaling is really long chain-of-thought reasoning. None of the citations you listed use this terminology. 
The test-time compute scaling paper refers to sequential sampling but in that case it is re-prompting the model to produce another solution based on the best solution (another way to get Best-of-N) and then picks from the best sequentially generated solutions.  I think actually many people mistakenly attributed reasoning behavior to the test-time compute scaling paper.  This might be because for a while people thought reasoning APIs were running MCTS at inference time.  You could write -- "There has been considerable recent interest in long chain-of-thought reasoning models which leverage significantly increased token generation to improve performance on many challenging benchmarks.  We refer to this form of scaling as Sequential Scaling."  You might want to introduce sequential scaling after parallel scaling? 
Actually, you later cite \cite{muennighoff2025s1} which claims to introduce this terminology on Feb 3rd and does what I proposed here -- renames reasoning behavior as sequential.
}
Secondly, \textbf{Parallel} Scaling repeatedly samples and selects the best solution, often with methods such as Best-of-N~\citep{brown2024large}. 
\begin{figure}[!t]
    \centering
    % First subfigure
    \includegraphics[width=0.45\textwidth]{figures/performance.pdf}      \caption{\textbf{Performance comparison in LiveCodeBench}. ~\frameworkname consistently improves models across different sizes, and both instruction-tuned and newer reasoning models. In particular, ~\frameworkname allows (1) smaller models to outperform 10x larger model: Qwen2.5-3B-Coder-Instruct, integrated with~\frameworkname, outperforms its 32B variant. (2) Older models to outperform newer models: GPT-4o-mini, integrated with~\frameworkname, outperforms O1-Preview. (3) Reasoning models achieve state-of-the-art performance: DeepSeek-R1-Distill-Qwen-32B, integrated with~\frameworkname. outperforms O1-High.}
        \label{fig:performance}
\end{figure}


While both directions have their advantages, they present limitations when we would like to further scale up test-time compute. Firstly, the advantage of sequential scaling eventually flattens out and is limited by the model context window~\citep{muennighoff2025s1}. 
\kurt{awkward; rewrite}
Secondly, even when repeated sampling improves the coverage, the fraction of problem being solved if any of the N samples are correct, the selection method can not always find the correct sample~\citep{brown2024large, stroebl2024inference}. \jx{The second point is a bit hard to get. AND the fraction of problem being solved ...?}
To bridge this gap, hybrid approaches that investigate how to combine parallel and scaling methods have been proposed~\citep{muennighoff2025s1, snell2024scaling}.

\textbf{While hybrid approaches are promising, existing works focus on math domains, due to a wide range of established reward models~\citep{snell2024scaling, muennighoff2025s1}}. 
\joey{Are you saying there is no work on reasoning models for coding tasks?}
The most effective strategies for scaling test-time compute in code generation remain largely unexplored. \jx{Should the 2nd sentence be merged with the 1st sentence and highlighted together?}
Unlike math reasoning tasks, where reward 
signal and selection can be easily derived by exact matching \shangyin{I am not quite convinced by this. What exact-matching signal can we get from an unsolved math problem?}, evaluating coding tasks require large-scale of tests, 
\joey{Do they require large-scale tests?  Couldn't you use a few unit tests to get some signal? Or if you have the solution, just use fuzz testing to verify that the generated solution produced similar results to the ground-truth solution?}
which is scarce in existing datasets~\citep{AceCoder}.
\joey{Several of us are having trouble with the claim that it would be difficult to obtain a reward model for coding tasks.  Do we need to argue this point in the first place? Does there need to be a reason that reasoning models aren't yet applied to coding tasks? Or if there is one then you probably should directly address that in this paper (e.g., propose a reward model).}
This limitation makes the selection phase significantly more complex for code generation compared to math reasoning. On the other hand, coding tasks offer a distinct advantage through program execution, which provides grounded feedback. This characteristics enable self-debug, or potentially reliable selection~\citep{chen2023teaching}.
\jx{Feel that there is a small gap between this para and the next. It is a bit unclear whether we want to address the problem by complementing the dataset or proposing new methods to leverage existing dataset.}

\textbf{In this paper, we propose the first simple hybrid framework~\frameworkname}, to study parallel scaling and sequential scaling strategies of code generation. ~\frameworkname~consists of three phrases that have been proven to be powerful in the context of math problems or partially explored in the coding domain: (1) repeated sampling, which improves overage by generating diverse attempts, enabling models to solve a higher fraction of problems~\citep{brown2024large}, (2) self-debugging,~\citep{chen2023teaching, madaan2024self} on test demonstration, and (3) selection. \jx{We explained the first phases but not the last two. Should we also explain them briefly or removing the explanation of the 1st point to make them aligned?}
We carefully study basic design choice in each axis. Here are the takeaways that enable our final default setting: (1) Parallel: Medium-to-high temperature is sufficient to balance diversity while preserving code quality. We also experiment with several in-context variants but do not observe more significant improvement. (2) Sequential: Self-debug flattens out at usually 2-3 rounds. Interestingly, even strong reasoning models such as R1-7B benefits from real execution. (3) Selection: Majority does not work well; LLM-as-a-judge does not work well by looking at the code; we make a hybrid selection with test-case execution + LLM-as-a-judge with tool-use (i.e., test-case execution) work well. \shangyin{(1) is super clear. (2) and (3) are fuzzy. I think this paragraph could be improved by saying: \frameworkname does (1) (2) and (3), and we observed that they are the best design choices among all.}

% unique challenges for coding
% todo: make the transition smoother
\begin{figure}[th!]
    \centering
    \includegraphics[width=1.0\linewidth]{latex/figures/overview.pdf}
\caption{~\frameworkname~builds on the paradigm of a combination of revision and parallel samples. \jx{Be careful of this because we say we are a three-stage approach.} It first performs a self-refinement on the initial solution to improve algorithm efficiency, and subsequent self-debug round to correct solutions based on public test execution feedback. At the end, it undergoes a selection policy module to select the best solution to output.
\joey{I was confused by this figure as it looks likes Figure 2 from the Snell paper (probably Fig2a).  Does each square reflect a new decode call? It might help to show where teacher-forcing or prompting is done.  Like at the beginning show multiple different prompts to get k different generations. Then show each long context as a tall rectangle and then illustrate the selection process more. }
}
\label{fig:Design}
\end{figure}

We evaluate the effectiveness of~\frameworkname~on ten models across model sizes, open and closed, and both instruction-based or reasoning models.~\frameworkname~has shown consistent improvement, allowing smaller models to outperform 10x larger models, older models to outperform newer models, and reasoning models to set the new state-of-the-art.

In conclusion, our contribution are:
\begin{enumerate}
    \item The first hybrid test-time-scaling framework for code generation.
    \item We study the basic component of each axis with takeaways, provide simple yet strong foundation to be built upon.
    \item We extensively evaluate the effectivenes of our framework, with ablations on detailed design choice. Our framework improves xxx...
    \item We will open-source all softward artifacts, as well as all model generation, intermediates results to facilitate the community research.
\end{enumerate}

\dacheng{(1) simulate cases without unite tests}
~\dacheng{Seems like missing : coding community has not really look at how to scale these little components, like tuning temperature versus ICL, diversity favor, scaling self-debug round. They mostly do look at how to do selection (but not our test+LLM hybrid selection).}

