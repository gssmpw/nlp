\section{Related work}
\myparagraph{Test Time Scaling for LLMs.} 
Existing approaches to increase test-time compute can be broadly categorized into two paradigms: parallel scaling and sequential scaling~\citep{muennighoff2025s1}. 
% Existing approaches for increasing test-time compute can be generally classified into two basic directions: parallel and sequential. 
Parallel scaling (i.e., repeated sampling) involves generating multiple solutions simultaneously and selecting the best one, a strategy commonly known as Best-of-N.
Coverage---the fraction of problems solved by any of these N samples---continues to improve as $N$ increases~\citep{chollet2019measure, irvine2023rewarding}, even at the scale of $10^4$ to $10^6$~\citep{brown2024large, li2022competition}. 
However, common selection strategies, such as (weighted) majority voting~\citep{wang2022self} and reward model scoring~\citep{christiano2017deep, lightman2023let, wang2024math, wu2024inference, beeching2024dvts, pan2024swegym}, often struggle to select the correct best sample in parallel scaling~\citep{brown2024large, hassid2024larger, stroebl2024inference}. In this paper, we propose a novel method that improves selection for coding tasks. 
% Although increases the coverage, it is often difficult to select the correct best sample in parallel scaling~\citep{brown2024large, hassid2024larger, stroebl2024inference}.

Sequential scaling, on the other hand, encourages the model to refine its reasoning over multiple steps. This includes methods like chain-of-thought (CoT) prompting~\citep{wei2022chain, nye2021show}, and iterative rethinking and revision~\citep{madaan2024self, lee2025evolving, hou2025advancing, huang2022large, min2024imitate, sky_t1_2025, muennighoff2025s1, wang2024theoretical, li2025llms}. Noticeably, OpenAI o1, DeepSeek R1, Qwen QwQ, and Kimi employ in-context long CoT with revision and backtracking to find the best solution~\citep{openai_learning_to_reason_2024, guo2025deepseek, qwen_qwq_2024, team2025kimi}. In this paper, we leverage iterative debugging from test execution feedback for sequential scaling code generation performance~\citep{chen2023teaching}.


% On the other hand, sequential scaling encourages the model to produce intermediate steps, e.g., chain-of-thoughts (CoT) and scratchpad~\citep{wei2022chain, nye2021show}, or to rethink and revise a previous solution~\citep{madaan2024self,lee2025evolving,hou2025advancing, huang2022large, min2024imitate, sky_t1_2025, muennighoff2025s1, wang2024theoretical}. Noticeably, OpenAI o1, DeepSeek R1, Qwen QwQ, and Kimi employ in-context long CoT with revision and backtracking to find the best solution~\citep{openai_learning_to_reason_2024, deepseek_r1_lite_2024, guo2025deepseek, qwen_qwq_2024, team2025kimi}. 

% Our work is unique in that it adopts a hybrid approach, integrating both parallel and sequential scaling while enhancing them with iterative debugging and adaptive input selection.
% Our work adopts a hybrid approach, integrating both parallel and sequential scaling.

\myparagraph{Test Time Scaling for Code Generation.}
~\citet{chen2022codet, huang2023enhancing, jiao2024preference} use models to generate code samples and test cases, selecting the final sample in a self-consistency manner~\citep{wang2022self, AceCoder}. However, these approaches often suffer from model hallucination, where the model fails to accurately predict the output of a test input~\citep{jain2024livecodebench, AceCoder,gu2024cruxeval}.
%CodeT uses RANSAC algorithm to select the correct sample using model-generated test cases to filter model-generated solutions~\citep{chen2022codet}. 
AlphaCode explores large-scale parallel sampling with a trained model to generate test cases for filtering and selection~\citep{li2022competition}. AlphaCodium uses a series of self-revision on both public demonstration and model-generated tests to improve solutions~\citep{ridnik2024code}. ~\citet{saad2024archon} searches over various inference techniques and finds that parallel sampling with model-generated tests works well for CodeContests problems~\citep{li2022competition}. Unlike methods relying solely on parallel sampling or sequential scaling, we use a hybrid approach that combines their advantages.
% Our framework is different than existing approach in that it leverages parallel scaling, sequential scaling, and a hybrid method to leverage both model-generated test inputs and LLM-as-a-judge approach to do selection.

% \frameworkname adopts a hybrid test-time scaling approach for code generation by extending parallel scaling with sequential scaling through iterative debugging for increased coverage and employing adaptive input synthesis to improve selection accuracy.

% Our framework differs by enhancing sequential scaling with execution-grounded iterative debugging and leveraging adaptive input synthesis to distinguish samples and select the best one.

\myparagraph{Hybrid Test-Time Scaling.}
Many works in the math domain study hybrid approaches that combine parallel and sequential scaling, often leveraging reward-model-guided tree search algorithms, such as Monte-Carlo Tree Search (MCTS), to effectively navigate the solution space~\citep{gao2024interpretable, li2024rethinkmcts, silver2016mastering, snell2024scaling, hendrycks2021measuring}. S1~\citep{muennighoff2025s1} primarily focuses on sequential scaling but observes diminishing returns and thus incorporates parallel-based approaches like majority voting and tree search to further enhance performance.

In contrast, our work applies hybrid scaling to code generation tasks without relying on tree search methods, as developing a general and effective reward model for the code generation domain remains challenging~\citep{AceCoder}. Instead, \frameworkname augments parallel scaling with sequential scaling via execution-grounded iterative debugging to improve coverage and introduces adaptive input synthesis to enhance selection accuracy.

% \myparagraph{Hybrid-based Test Time Scaling.}
% S1~\citep{muennighoff2025s1} primarily studies sequential scaling but finds that its effectiveness eventually plateaus. To address this, it explores parallel-based approaches, including majority voting and tree searching. 
% There are also hybrid approaches leveraging tree search-like algorithms, such as using Monte-Carlo Tree search~\citep{gao2024interpretable,li2024rethinkmcts, silver2016mastering}. 
% ~\citet{snell2024scaling} studies the combination of parallel scaling and sequential scaling on a math benchmark~\citep{hendrycks2021measuring}. 
% Our work combines parallel scaling and sequential scaling for code generation without relying on tree search methods, as developing a general and effective reward model for code generation domain remains challenging~\citep{AceCoder}.

% reward models for coding remain underdeveloped~\citep{AceCoder}.


\myparagraph{Concurrent Work.} 
CodeMonkeys is a noticeable concurrent work to this paper, released on Arxiv in Jan 2025~\citep{ehrlich2025codemonkeys}. It also generates multiple samples in parallel and revises samples. However, CodeMonkeys focuses on the software engineering domain, optimizing performance on SWE-Bench (Chowdhury et al., 2024),
which addresses challenges such as identifying files
that need to be edited. In contrast, our work focuses on competition-level code generation, where domain differences influence our algorithm choice. For instance, during sequential scaling, CodeMonkeys requires a model to generate tests over multiple iterations, while we instead incorporate feedback from public tests (ablated variants in~\cref{ssect:ablation_selfdebug}).