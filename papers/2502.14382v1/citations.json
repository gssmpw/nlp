[
  {
    "index": 0,
    "papers": [
      {
        "key": "muennighoff2025s1",
        "author": "Muennighoff, Niklas and Yang, Zitong and Shi, Weijia and Li, Xiang Lisa and Fei-Fei, Li and Hajishirzi, Hannaneh and Zettlemoyer, Luke and Liang, Percy and Cand{\\`e}s, Emmanuel and Hashimoto, Tatsunori",
        "title": "s1: Simple test-time scaling"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "chollet2019measure",
        "author": "Chollet, Fran{\\c{c}}ois",
        "title": "On the measure of intelligence"
      },
      {
        "key": "irvine2023rewarding",
        "author": "Irvine, Robert and Boubert, Douglas and Raina, Vyas and Liusie, Adian and Zhu, Ziyi and Mudupalli, Vineet and Korshuk, Aliaksei and Liu, Zongyi and Cremer, Fritz and Assassi, Valentin and others",
        "title": "Rewarding chatbots for real-world engagement with millions of users"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "brown2024large",
        "author": "Brown, Bradley and Juravsky, Jordan and Ehrlich, Ryan and Clark, Ronald and Le, Quoc V and R{\\'e}, Christopher and Mirhoseini, Azalia",
        "title": "Large language monkeys: Scaling inference compute with repeated sampling"
      },
      {
        "key": "li2022competition",
        "author": "Li, Yujia and Choi, David and Chung, Junyoung and Kushman, Nate and Schrittwieser, Julian and Leblond, R{\\'e}mi and Eccles, Tom and Keeling, James and Gimeno, Felix and Dal Lago, Agustin and others",
        "title": "Competition-level code generation with alphacode"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "wang2022self",
        "author": "Wang, Xuezhi and Wei, Jason and Schuurmans, Dale and Le, Quoc and Chi, Ed and Narang, Sharan and Chowdhery, Aakanksha and Zhou, Denny",
        "title": "Self-consistency improves chain of thought reasoning in language models"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "christiano2017deep",
        "author": "Christiano, Paul F and Leike, Jan and Brown, Tom and Martic, Miljan and Legg, Shane and Amodei, Dario",
        "title": "Deep reinforcement learning from human preferences"
      },
      {
        "key": "lightman2023let",
        "author": "Lightman, Hunter and Kosaraju, Vineet and Burda, Yura and Edwards, Harri and Baker, Bowen and Lee, Teddy and Leike, Jan and Schulman, John and Sutskever, Ilya and Cobbe, Karl",
        "title": "Let's verify step by step"
      },
      {
        "key": "wang2024math",
        "author": "Wang, Peiyi and Li, Lei and Shao, Zhihong and Xu, Runxin and Dai, Damai and Li, Yifei and Chen, Deli and Wu, Yu and Sui, Zhifang",
        "title": "Math-shepherd: Verify and reinforce llms step-by-step without human annotations"
      },
      {
        "key": "wu2024inference",
        "author": "Wu, Yangzhen and Sun, Zhiqing and Li, Shanda and Welleck, Sean and Yang, Yiming",
        "title": "Inference scaling laws: An empirical analysis of compute-optimal inference for problem-solving with language models"
      },
      {
        "key": "beeching2024dvts",
        "author": "Edward Beeching and Lewis Tunstall and Sasha Rush",
        "title": "Scaling test-time compute with open models"
      },
      {
        "key": "pan2024swegym",
        "author": "Jiayi Pan and Xingyao Wang and Graham Neubig and Navdeep Jaitly and Heng Ji and Alane Suhr and Yizhe Zhang",
        "title": "Training Software Engineering Agents and Verifiers with SWE-Gym"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "brown2024large",
        "author": "Brown, Bradley and Juravsky, Jordan and Ehrlich, Ryan and Clark, Ronald and Le, Quoc V and R{\\'e}, Christopher and Mirhoseini, Azalia",
        "title": "Large language monkeys: Scaling inference compute with repeated sampling"
      },
      {
        "key": "hassid2024larger",
        "author": "Hassid, Michael and Remez, Tal and Gehring, Jonas and Schwartz, Roy and Adi, Yossi",
        "title": "The Larger the Better? Improved LLM Code-Generation via Budget Reallocation"
      },
      {
        "key": "stroebl2024inference",
        "author": "Stroebl, Benedikt and Kapoor, Sayash and Narayanan, Arvind",
        "title": "Inference Scaling fLaws: The Limits of LLM Resampling with Imperfect Verifiers"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "brown2024large",
        "author": "Brown, Bradley and Juravsky, Jordan and Ehrlich, Ryan and Clark, Ronald and Le, Quoc V and R{\\'e}, Christopher and Mirhoseini, Azalia",
        "title": "Large language monkeys: Scaling inference compute with repeated sampling"
      },
      {
        "key": "hassid2024larger",
        "author": "Hassid, Michael and Remez, Tal and Gehring, Jonas and Schwartz, Roy and Adi, Yossi",
        "title": "The Larger the Better? Improved LLM Code-Generation via Budget Reallocation"
      },
      {
        "key": "stroebl2024inference",
        "author": "Stroebl, Benedikt and Kapoor, Sayash and Narayanan, Arvind",
        "title": "Inference Scaling fLaws: The Limits of LLM Resampling with Imperfect Verifiers"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "wei2022chain",
        "author": "Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others",
        "title": "Chain-of-thought prompting elicits reasoning in large language models"
      },
      {
        "key": "nye2021show",
        "author": "Nye, Maxwell and Andreassen, Anders Johan and Gur-Ari, Guy and Michalewski, Henryk and Austin, Jacob and Bieber, David and Dohan, David and Lewkowycz, Aitor and Bosma, Maarten and Luan, David and others",
        "title": "Show your work: Scratchpads for intermediate computation with language models"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "madaan2024self",
        "author": "Madaan, Aman and Tandon, Niket and Gupta, Prakhar and Hallinan, Skyler and Gao, Luyu and Wiegreffe, Sarah and Alon, Uri and Dziri, Nouha and Prabhumoye, Shrimai and Yang, Yiming and others",
        "title": "Self-refine: Iterative refinement with self-feedback"
      },
      {
        "key": "lee2025evolving",
        "author": "Lee, Kuang-Huei and Fischer, Ian and Wu, Yueh-Hua and Marwood, Dave and Baluja, Shumeet and Schuurmans, Dale and Chen, Xinyun",
        "title": "Evolving Deeper LLM Thinking"
      },
      {
        "key": "hou2025advancing",
        "author": "Hou, Zhenyu and Lv, Xin and Lu, Rui and Zhang, Jiajie and Li, Yujiang and Yao, Zijun and Li, Juanzi and Tang, Jie and Dong, Yuxiao",
        "title": "Advancing Language Model Reasoning through Reinforcement Learning and Inference Scaling"
      },
      {
        "key": "huang2022large",
        "author": "Huang, Jiaxin and Gu, Shixiang Shane and Hou, Le and Wu, Yuexin and Wang, Xuezhi and Yu, Hongkun and Han, Jiawei",
        "title": "Large language models can self-improve"
      },
      {
        "key": "min2024imitate",
        "author": "Min, Yingqian and Chen, Zhipeng and Jiang, Jinhao and Chen, Jie and Deng, Jia and Hu, Yiwen and Tang, Yiru and Wang, Jiapeng and Cheng, Xiaoxue and Song, Huatong and others",
        "title": "Imitate, explore, and self-improve: A reproduction report on slow-thinking reasoning systems"
      },
      {
        "key": "sky_t1_2025",
        "author": "NovaSky Team",
        "title": "Sky-T1: Train your own O1 preview model within 450 dollars"
      },
      {
        "key": "muennighoff2025s1",
        "author": "Muennighoff, Niklas and Yang, Zitong and Shi, Weijia and Li, Xiang Lisa and Fei-Fei, Li and Hajishirzi, Hannaneh and Zettlemoyer, Luke and Liang, Percy and Cand{\\`e}s, Emmanuel and Hashimoto, Tatsunori",
        "title": "s1: Simple test-time scaling"
      },
      {
        "key": "wang2024theoretical",
        "author": "Wang, Yifei and Wu, Yuyang and Wei, Zeming and Jegelka, Stefanie and Wang, Yisen",
        "title": "A Theoretical Understanding of Self-Correction through In-context Alignment"
      },
      {
        "key": "li2025llms",
        "author": "Li, Dacheng and Cao, Shiyi and Griggs, Tyler and Liu, Shu and Mo, Xiangxi and Patil, Shishir G and Zaharia, Matei and Gonzalez, Joseph E and Stoica, Ion",
        "title": "LLMs Can Easily Learn to Reason from Demonstrations Structure, not content, is what matters!"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "openai_learning_to_reason_2024",
        "author": "OpenAI",
        "title": "Learning to Reason with LLMs"
      },
      {
        "key": "guo2025deepseek",
        "author": "Guo, Daya and Yang, Dejian and Zhang, Haowei and Song, Junxiao and Zhang, Ruoyu and Xu, Runxin and Zhu, Qihao and Ma, Shirong and Wang, Peiyi and Bi, Xiao and others",
        "title": "Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning"
      },
      {
        "key": "qwen_qwq_2024",
        "author": "Qwen",
        "title": "QwQ: Reflect Deeply on the Boundaries of the Unknown"
      },
      {
        "key": "team2025kimi",
        "author": "Team, Kimi and Du, Angang and Gao, Bofei and Xing, Bowei and Jiang, Changjiu and Chen, Cheng and Li, Cheng and Xiao, Chenjun and Du, Chenzhuang and Liao, Chonghua and others",
        "title": "Kimi k1. 5: Scaling Reinforcement Learning with LLMs"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "chen2023teaching",
        "author": "Chen, Xinyun and Lin, Maxwell and Sch{\\\"a}rli, Nathanael and Zhou, Denny",
        "title": "Teaching large language models to self-debug"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "wei2022chain",
        "author": "Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others",
        "title": "Chain-of-thought prompting elicits reasoning in large language models"
      },
      {
        "key": "nye2021show",
        "author": "Nye, Maxwell and Andreassen, Anders Johan and Gur-Ari, Guy and Michalewski, Henryk and Austin, Jacob and Bieber, David and Dohan, David and Lewkowycz, Aitor and Bosma, Maarten and Luan, David and others",
        "title": "Show your work: Scratchpads for intermediate computation with language models"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "madaan2024self",
        "author": "Madaan, Aman and Tandon, Niket and Gupta, Prakhar and Hallinan, Skyler and Gao, Luyu and Wiegreffe, Sarah and Alon, Uri and Dziri, Nouha and Prabhumoye, Shrimai and Yang, Yiming and others",
        "title": "Self-refine: Iterative refinement with self-feedback"
      },
      {
        "key": "lee2025evolving",
        "author": "Lee, Kuang-Huei and Fischer, Ian and Wu, Yueh-Hua and Marwood, Dave and Baluja, Shumeet and Schuurmans, Dale and Chen, Xinyun",
        "title": "Evolving Deeper LLM Thinking"
      },
      {
        "key": "hou2025advancing",
        "author": "Hou, Zhenyu and Lv, Xin and Lu, Rui and Zhang, Jiajie and Li, Yujiang and Yao, Zijun and Li, Juanzi and Tang, Jie and Dong, Yuxiao",
        "title": "Advancing Language Model Reasoning through Reinforcement Learning and Inference Scaling"
      },
      {
        "key": "huang2022large",
        "author": "Huang, Jiaxin and Gu, Shixiang Shane and Hou, Le and Wu, Yuexin and Wang, Xuezhi and Yu, Hongkun and Han, Jiawei",
        "title": "Large language models can self-improve"
      },
      {
        "key": "min2024imitate",
        "author": "Min, Yingqian and Chen, Zhipeng and Jiang, Jinhao and Chen, Jie and Deng, Jia and Hu, Yiwen and Tang, Yiru and Wang, Jiapeng and Cheng, Xiaoxue and Song, Huatong and others",
        "title": "Imitate, explore, and self-improve: A reproduction report on slow-thinking reasoning systems"
      },
      {
        "key": "sky_t1_2025",
        "author": "NovaSky Team",
        "title": "Sky-T1: Train your own O1 preview model within 450 dollars"
      },
      {
        "key": "muennighoff2025s1",
        "author": "Muennighoff, Niklas and Yang, Zitong and Shi, Weijia and Li, Xiang Lisa and Fei-Fei, Li and Hajishirzi, Hannaneh and Zettlemoyer, Luke and Liang, Percy and Cand{\\`e}s, Emmanuel and Hashimoto, Tatsunori",
        "title": "s1: Simple test-time scaling"
      },
      {
        "key": "wang2024theoretical",
        "author": "Wang, Yifei and Wu, Yuyang and Wei, Zeming and Jegelka, Stefanie and Wang, Yisen",
        "title": "A Theoretical Understanding of Self-Correction through In-context Alignment"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "openai_learning_to_reason_2024",
        "author": "OpenAI",
        "title": "Learning to Reason with LLMs"
      },
      {
        "key": "deepseek_r1_lite_2024",
        "author": "DeepSeek",
        "title": "DeepSeek-R1-Lite-Preview Release"
      },
      {
        "key": "guo2025deepseek",
        "author": "Guo, Daya and Yang, Dejian and Zhang, Haowei and Song, Junxiao and Zhang, Ruoyu and Xu, Runxin and Zhu, Qihao and Ma, Shirong and Wang, Peiyi and Bi, Xiao and others",
        "title": "Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning"
      },
      {
        "key": "qwen_qwq_2024",
        "author": "Qwen",
        "title": "QwQ: Reflect Deeply on the Boundaries of the Unknown"
      },
      {
        "key": "team2025kimi",
        "author": "Team, Kimi and Du, Angang and Gao, Bofei and Xing, Bowei and Jiang, Changjiu and Chen, Cheng and Li, Cheng and Xiao, Chenjun and Du, Chenzhuang and Liao, Chonghua and others",
        "title": "Kimi k1. 5: Scaling Reinforcement Learning with LLMs"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "chen2022codet",
        "author": "Chen, Bei and Zhang, Fengji and Nguyen, Anh and Zan, Daoguang and Lin, Zeqi and Lou, Jian-Guang and Chen, Weizhu",
        "title": "Codet: Code generation with generated tests"
      },
      {
        "key": "huang2023enhancing",
        "author": "Huang, Baizhou and Lu, Shuai and Chen, Weizhu and Wan, Xiaojun and Duan, Nan",
        "title": "Enhancing large language models in coding through multi-perspective self-consistency"
      },
      {
        "key": "jiao2024preference",
        "author": "Jiao, Fangkai and Guo, Geyang and Zhang, Xingxing and Chen, Nancy F and Joty, Shafiq and Wei, Furu",
        "title": "Preference Optimization for Reasoning with Pseudo Feedback"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "wang2022self",
        "author": "Wang, Xuezhi and Wei, Jason and Schuurmans, Dale and Le, Quoc and Chi, Ed and Narang, Sharan and Chowdhery, Aakanksha and Zhou, Denny",
        "title": "Self-consistency improves chain of thought reasoning in language models"
      },
      {
        "key": "AceCoder",
        "author": "Zeng, Huaye and Jiang, Dongfu and Wang, Haozhe and Nie, Ping and Chen, Xiaotong and Chen, Wenhu",
        "title": "AceCoder: Acing Coder RL via Automated Test-Case Synthesis"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "jain2024livecodebench",
        "author": "Jain, Naman and Han, King and Gu, Alex and Li, Wen-Ding and Yan, Fanjia and Zhang, Tianjun and Wang, Sida and Solar-Lezama, Armando and Sen, Koushik and Stoica, Ion",
        "title": "Livecodebench: Holistic and contamination free evaluation of large language models for code"
      },
      {
        "key": "AceCoder",
        "author": "Zeng, Huaye and Jiang, Dongfu and Wang, Haozhe and Nie, Ping and Chen, Xiaotong and Chen, Wenhu",
        "title": "AceCoder: Acing Coder RL via Automated Test-Case Synthesis"
      },
      {
        "key": "gu2024cruxeval",
        "author": "Gu, Alex and Rozi{\\`e}re, Baptiste and Leather, Hugh and Solar-Lezama, Armando and Synnaeve, Gabriel and Wang, Sida I",
        "title": "Cruxeval: A benchmark for code reasoning, understanding and execution"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "chen2022codet",
        "author": "Chen, Bei and Zhang, Fengji and Nguyen, Anh and Zan, Daoguang and Lin, Zeqi and Lou, Jian-Guang and Chen, Weizhu",
        "title": "Codet: Code generation with generated tests"
      }
    ]
  },
  {
    "index": 18,
    "papers": [
      {
        "key": "li2022competition",
        "author": "Li, Yujia and Choi, David and Chung, Junyoung and Kushman, Nate and Schrittwieser, Julian and Leblond, R{\\'e}mi and Eccles, Tom and Keeling, James and Gimeno, Felix and Dal Lago, Agustin and others",
        "title": "Competition-level code generation with alphacode"
      }
    ]
  },
  {
    "index": 19,
    "papers": [
      {
        "key": "ridnik2024code",
        "author": "Ridnik, Tal and Kredo, Dedy and Friedman, Itamar",
        "title": "Code generation with alphacodium: From prompt engineering to flow engineering"
      }
    ]
  },
  {
    "index": 20,
    "papers": [
      {
        "key": "saad2024archon",
        "author": "Saad-Falcon, Jon and Lafuente, Adrian Gamarra and Natarajan, Shlok and Maru, Nahum and Todorov, Hristo and Guha, Etash and Buchanan, E Kelly and Chen, Mayee and Guha, Neel and R{\\'e}, Christopher and others",
        "title": "Archon: An architecture search framework for inference-time techniques"
      }
    ]
  },
  {
    "index": 21,
    "papers": [
      {
        "key": "li2022competition",
        "author": "Li, Yujia and Choi, David and Chung, Junyoung and Kushman, Nate and Schrittwieser, Julian and Leblond, R{\\'e}mi and Eccles, Tom and Keeling, James and Gimeno, Felix and Dal Lago, Agustin and others",
        "title": "Competition-level code generation with alphacode"
      }
    ]
  },
  {
    "index": 22,
    "papers": [
      {
        "key": "gao2024interpretable",
        "author": "Gao, Zitian and Niu, Boye and He, Xuzheng and Xu, Haotian and Liu, Hongzhang and Liu, Aiwei and Hu, Xuming and Wen, Lijie",
        "title": "Interpretable contrastive monte carlo tree search reasoning"
      },
      {
        "key": "li2024rethinkmcts",
        "author": "Li, Qingyao and Xia, Wei and Du, Kounianhua and Dai, Xinyi and Tang, Ruiming and Wang, Yasheng and Yu, Yong and Zhang, Weinan",
        "title": "RethinkMCTS: Refining Erroneous Thoughts in Monte Carlo Tree Search for Code Generation"
      },
      {
        "key": "silver2016mastering",
        "author": "Silver, David and Huang, Aja and Maddison, Chris J and Guez, Arthur and Sifre, Laurent and Van Den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and others",
        "title": "Mastering the game of Go with deep neural networks and tree search"
      },
      {
        "key": "snell2024scaling",
        "author": "Snell, Charlie and Lee, Jaehoon and Xu, Kelvin and Kumar, Aviral",
        "title": "Scaling llm test-time compute optimally can be more effective than scaling model parameters"
      },
      {
        "key": "hendrycks2021measuring",
        "author": "Hendrycks, Dan and Basart, Steven and Kadavath, Saurav and Mazeika, Mantas and Arora, Akul and Guo, Ethan and Burns, Collin and Puranik, Samir and He, Horace and Song, Dawn and others",
        "title": "Measuring coding challenge competence with apps"
      }
    ]
  },
  {
    "index": 23,
    "papers": [
      {
        "key": "muennighoff2025s1",
        "author": "Muennighoff, Niklas and Yang, Zitong and Shi, Weijia and Li, Xiang Lisa and Fei-Fei, Li and Hajishirzi, Hannaneh and Zettlemoyer, Luke and Liang, Percy and Cand{\\`e}s, Emmanuel and Hashimoto, Tatsunori",
        "title": "s1: Simple test-time scaling"
      }
    ]
  },
  {
    "index": 24,
    "papers": [
      {
        "key": "AceCoder",
        "author": "Zeng, Huaye and Jiang, Dongfu and Wang, Haozhe and Nie, Ping and Chen, Xiaotong and Chen, Wenhu",
        "title": "AceCoder: Acing Coder RL via Automated Test-Case Synthesis"
      }
    ]
  },
  {
    "index": 25,
    "papers": [
      {
        "key": "muennighoff2025s1",
        "author": "Muennighoff, Niklas and Yang, Zitong and Shi, Weijia and Li, Xiang Lisa and Fei-Fei, Li and Hajishirzi, Hannaneh and Zettlemoyer, Luke and Liang, Percy and Cand{\\`e}s, Emmanuel and Hashimoto, Tatsunori",
        "title": "s1: Simple test-time scaling"
      }
    ]
  },
  {
    "index": 26,
    "papers": [
      {
        "key": "gao2024interpretable",
        "author": "Gao, Zitian and Niu, Boye and He, Xuzheng and Xu, Haotian and Liu, Hongzhang and Liu, Aiwei and Hu, Xuming and Wen, Lijie",
        "title": "Interpretable contrastive monte carlo tree search reasoning"
      },
      {
        "key": "li2024rethinkmcts",
        "author": "Li, Qingyao and Xia, Wei and Du, Kounianhua and Dai, Xinyi and Tang, Ruiming and Wang, Yasheng and Yu, Yong and Zhang, Weinan",
        "title": "RethinkMCTS: Refining Erroneous Thoughts in Monte Carlo Tree Search for Code Generation"
      },
      {
        "key": "silver2016mastering",
        "author": "Silver, David and Huang, Aja and Maddison, Chris J and Guez, Arthur and Sifre, Laurent and Van Den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and others",
        "title": "Mastering the game of Go with deep neural networks and tree search"
      }
    ]
  },
  {
    "index": 27,
    "papers": [
      {
        "key": "snell2024scaling",
        "author": "Snell, Charlie and Lee, Jaehoon and Xu, Kelvin and Kumar, Aviral",
        "title": "Scaling llm test-time compute optimally can be more effective than scaling model parameters"
      }
    ]
  },
  {
    "index": 28,
    "papers": [
      {
        "key": "hendrycks2021measuring",
        "author": "Hendrycks, Dan and Basart, Steven and Kadavath, Saurav and Mazeika, Mantas and Arora, Akul and Guo, Ethan and Burns, Collin and Puranik, Samir and He, Horace and Song, Dawn and others",
        "title": "Measuring coding challenge competence with apps"
      }
    ]
  },
  {
    "index": 29,
    "papers": [
      {
        "key": "AceCoder",
        "author": "Zeng, Huaye and Jiang, Dongfu and Wang, Haozhe and Nie, Ping and Chen, Xiaotong and Chen, Wenhu",
        "title": "AceCoder: Acing Coder RL via Automated Test-Case Synthesis"
      }
    ]
  },
  {
    "index": 30,
    "papers": [
      {
        "key": "AceCoder",
        "author": "Zeng, Huaye and Jiang, Dongfu and Wang, Haozhe and Nie, Ping and Chen, Xiaotong and Chen, Wenhu",
        "title": "AceCoder: Acing Coder RL via Automated Test-Case Synthesis"
      }
    ]
  },
  {
    "index": 31,
    "papers": [
      {
        "key": "ehrlich2025codemonkeys",
        "author": "Ehrlich, Ryan and Brown, Bradley and Juravsky, Jordan and Clark, Ronald and R{\\'e}, Christopher and Mirhoseini, Azalia",
        "title": "CodeMonkeys: Scaling Test-Time Compute for Software Engineering"
      }
    ]
  }
]