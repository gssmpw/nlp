\section{Related work}
\myparagraph{Test Time Scaling for LLMs.} 
Existing approaches to increase test-time compute can be broadly categorized into two paradigms: parallel scaling and sequential scaling____. 
% Existing approaches for increasing test-time compute can be generally classified into two basic directions: parallel and sequential. 
Parallel scaling (i.e., repeated sampling) involves generating multiple solutions simultaneously and selecting the best one, a strategy commonly known as Best-of-N.
Coverage---the fraction of problems solved by any of these N samples---continues to improve as $N$ increases____, even at the scale of $10^4$ to $10^6$____. 
However, common selection strategies, such as (weighted) majority voting____ and reward model scoring____, often struggle to select the correct best sample in parallel scaling____. In this paper, we propose a novel method that improves selection for coding tasks. 
% Although increases the coverage, it is often difficult to select the correct best sample in parallel scaling____.

Sequential scaling, on the other hand, encourages the model to refine its reasoning over multiple steps. This includes methods like chain-of-thought (CoT) prompting____, and iterative rethinking and revision____. Noticeably, OpenAI o1, DeepSeek R1, Qwen QwQ, and Kimi employ in-context long CoT with revision and backtracking to find the best solution____. In this paper, we leverage iterative debugging from test execution feedback for sequential scaling code generation performance____.


% On the other hand, sequential scaling encourages the model to produce intermediate steps, e.g., chain-of-thoughts (CoT) and scratchpad____, or to rethink and revise a previous solution____. Noticeably, OpenAI o1, DeepSeek R1, Qwen QwQ, and Kimi employ in-context long CoT with revision and backtracking to find the best solution____. 

% Our work is unique in that it adopts a hybrid approach, integrating both parallel and sequential scaling while enhancing them with iterative debugging and adaptive input selection.
% Our work adopts a hybrid approach, integrating both parallel and sequential scaling.

\myparagraph{Test Time Scaling for Code Generation.}
____ use models to generate code samples and test cases, selecting the final sample in a self-consistency manner____. However, these approaches often suffer from model hallucination, where the model fails to accurately predict the output of a test input____.
%CodeT uses RANSAC algorithm to select the correct sample using model-generated test cases to filter model-generated solutions____. 
AlphaCode explores large-scale parallel sampling with a trained model to generate test cases for filtering and selection____. AlphaCodium uses a series of self-revision on both public demonstration and model-generated tests to improve solutions____. ____ searches over various inference techniques and finds that parallel sampling with model-generated tests works well for CodeContests problems____. Unlike methods relying solely on parallel sampling or sequential scaling, we use a hybrid approach that combines their advantages.
% Our framework is different than existing approach in that it leverages parallel scaling, sequential scaling, and a hybrid method to leverage both model-generated test inputs and LLM-as-a-judge approach to do selection.

% \frameworkname adopts a hybrid test-time scaling approach for code generation by extending parallel scaling with sequential scaling through iterative debugging for increased coverage and employing adaptive input synthesis to improve selection accuracy.

% Our framework differs by enhancing sequential scaling with execution-grounded iterative debugging and leveraging adaptive input synthesis to distinguish samples and select the best one.

\myparagraph{Hybrid Test-Time Scaling.}
Many works in the math domain study hybrid approaches that combine parallel and sequential scaling, often leveraging reward-model-guided tree search algorithms, such as Monte-Carlo Tree Search (MCTS), to effectively navigate the solution space____. S1____ primarily focuses on sequential scaling but observes diminishing returns and thus incorporates parallel-based approaches like majority voting and tree search to further enhance performance.

In contrast, our work applies hybrid scaling to code generation tasks without relying on tree search methods, as developing a general and effective reward model for the code generation domain remains challenging____. Instead, \frameworkname augments parallel scaling with sequential scaling via execution-grounded iterative debugging to improve coverage and introduces adaptive input synthesis to enhance selection accuracy.

% \myparagraph{Hybrid-based Test Time Scaling.}
% S1____ primarily studies sequential scaling but finds that its effectiveness eventually plateaus. To address this, it explores parallel-based approaches, including majority voting and tree searching. 
% There are also hybrid approaches leveraging tree search-like algorithms, such as using Monte-Carlo Tree search____. 
% ____ studies the combination of parallel scaling and sequential scaling on a math benchmark____. 
% Our work combines parallel scaling and sequential scaling for code generation without relying on tree search methods, as developing a general and effective reward model for code generation domain remains challenging____.

% reward models for coding remain underdeveloped____.


\myparagraph{Concurrent Work.} 
CodeMonkeys is a noticeable concurrent work to this paper, released on Arxiv in Jan 2025____. It also generates multiple samples in parallel and revises samples. However, CodeMonkeys focuses on the software engineering domain, optimizing performance on SWE-Bench (Chowdhury et al., 2024),
which addresses challenges such as identifying files
that need to be edited. In contrast, our work focuses on competition-level code generation, where domain differences influence our algorithm choice. For instance, during sequential scaling, CodeMonkeys requires a model to generate tests over multiple iterations, while we instead incorporate feedback from public tests (ablated variants in~\cref{ssect:ablation_selfdebug}).