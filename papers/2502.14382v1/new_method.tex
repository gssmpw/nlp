% unique challenges for coding
% todo: make the transition smoother
\section{Method}
\label{sec:method}
In this section, we introduce our framework. ~\frameworkname~takes in a coding problem $\mathbf{P}$, and a code generation model $\mathbf{M}$. 
The model $\mathbf{M}$ attempts to generate a program solution $\mathbf{S(\cdot)}$ to maps an input to an output following the problem specification.%\joey{This sentence seems broken.}

In this work, we follow the standard coding framework adopted by most benchmarks~\citep{chen2021evaluating, li2022competition, li2023taco, jain2024livecodebench, hendrycksapps2021, gulwani4foundations}.
Each coding problem $\mathbf{P}$ consists of a problem description in natural language along with set of public and private test cases. 
The test cases are in the form of input-output pairs. 
%There are typically \joey{add number} public test cases and \joey{add number} private test cases.
% and expected output pair: public test $T_\text{public} = \{(I_i, O_i)\}_{i=1}^{k_\text{public}}$, and private test $T_\text{private} = \{(I_i, O_i)\}_{i=1}^{k_\text{private}}$. 
Private tests are not available to $\mathbf{M}$ and are instead used to score the mode on the benchmark. 
The solution is correct if $\mathbf{S}$ passes all the private tests. In contrast, the public tests are available to $\mathbf{M}$ to clarify the intent of the natural language description and are typically included in the prompt. The number of public tests are usually much less than that of private tests. For instance, in CodeContests~\citep{li2022competition}, there are on average 2.0 public tests and 202.1 private tests per problem.
% i.e., if $S(I_i) = O_i, \forall i = 1, ..., k_{private}$. 
% This formulation has been proposed in benchmarks such as~\citet{chen2021evaluating, li2022competition, li2023taco, jain2024livecodebench, hendrycksapps2021, gulwani4foundations}. 
This setup is in contrast to math problems, where no additional information is given beyond the problem specification, and evaluation is done by exact matching on the final solution~\citep{li2024numinamath}.


% In this section, we formally introduce the problem setup and the ~\frameworkname~framework. %an experiment setup to configure the default setting that we will use in our final experiment.
% \joey{I wouldn't add the experiment setup in the methods section.}


%\noindent\textbf{Problem Setup} We consider coding problems that are clearly specified and unambiguous, as pointed out to be important in~\citet{chowdhury2024introducing}. In particular, we assume coding problems that include a small amount of input and output demonstrations, a common setup as~\citep{chen2021evaluating, li2022competition, li2023taco, jain2024livecodebench, hendrycksapps2021, gulwani4foundations}. We provide one such example in~\autoref{sec:appendix_example_code}. \joey{This paragraph doesn't actually say much. Instead, you might say something that is more detailed and provides greater contrast to the math setting.  In the programming setting we are given a detailed description of a function to implement along with a set of $k$? test input-output pairs and asked to produce a program that produces the same output for each test input.  The model then produces an implementation that is evaluated on a hidden set of unit-tests.  This is in contrast to the math setting where no additional information is given beyond the problem statement and evaluation is done by exact matching on the final solution.}

\subsection{The \frameworkname Framework}
The \frameworkname consists of two-stages: \emph{Generation}  and \emph{Selection}.
The generation stage combines repeated sampling to improve coverage%, the fraction of problems that could be solved, 
, with revision from public tests execution output to
improve the quality of %such diverse 
the samples. The selection stage features a test-assisted LLM-as-a-judge approach, which grounds LLM selection with code execution results.
%to provide further information 
%to enable more accurate LLM guided selection (~\autoref{sec:scale_selection}).

% \frameworkname~is a simple and effective three-stage approach. 
%\joey{I don't think you need the words simple and effective.  They are okay in the intro but here just say it is a three stage approach that combines sequential and parallel scaling with a verification process.  Then provide a high-level description of each of the three phases and how they work together.  It should be clear at the end that this is simple and in the eval you will prove it is effective.}
% It incorporates the advantage of parallel scaling by allowing the model to generate multiple solutions to improve coverage~\citep{brown2024large}, sequential scaling by allowing the model to revise a previous code solution by providing execution feedback on available public demonstration, and that of a verifier by choosing the best one. A high-level illustration can be found at~\autoref{fig:Design}. % In this section, we will introduce each stage, and our findings based on a development set to enable the default setting of~\frameworkname.

\subsubsection{Stage 1: Generation}
% \section{Scaling Parallel Sampling}
\label{sec:scale_parallel_samples}
In the generation stage, \frameworkname leverages the repeated sampling paradigm and leverages public tests execution result to ground revision. % For each sample, the LLM further revise its previous solution based on the execution result on public tests. Following ~\citet{chen2023teaching}, we implement this by executing
% using the SELF-DEBUGGING approach proposed by % \citet{chen2023teaching}. 
% \joey{Can you give a short description of how they defined self-debugging in the original paper. Then if there are any differences say how we implement self debugging here.  If they are identical then say "As in ..., we implement self-debugging by ..."}
Following similar implementation as in~\citet{chen2023teaching}, \frameworkname  generates $\mathbf{N}$ samples independently, and performs at most $\mathbf{R}$ rounds of revision with execution feedback from the set of public tests. For each sample, the revision procedure terminates if it passes all public tests or reaches the maximal debug round. We further experiments with additional revision from model generated tests, but do not found additional benefit in~\autoref{ssect:ablation_selfdebug}.
% We denote the $n^{th} (\in {1, ..., N})$ solution produced at the $r^{th} (\in {1, ... R})$ round as $S_{r}^{n}$. 
% For each sample, the SELF-DEBUGGING procedure terminates if the sample passes all public tests, or reaches the maximal debug round $\mathbf{R}$.


\subsubsection{Stage 2: Selection}
\label{sec:scale_selection}
In the selection stage, we need to select the best program from the $N$ candidate programs produced in the generation stage. However, since we have exhausted public tests in the Generation stage, we need to generate additional filtering step. We consider two prior paradigms, (1) LLM-as-a-judge based, which harnesses general pre-trained knowledge~\citep{zheng2023judging}, and (2) generated test based~\citep{li2022competition}, which leverages interpreter to ground selection. During exploration, we find that a naive LLM-as-a-judge approach that compares two code samples could hardly provide accurate selection because the LLM could hardly simulate the outcomes of such codes. On the other hand, a pure generated test based approach could hardly accurately select a sample because they can hardly predicts the test case accurately (\autoref{tab:diff_selection}).

%However, because all of the $N$ programs now pass the public tests, we need an additional filtering step to select the best program. 
% In addition, prior work has show that the public tests often have very little coverage of the problem. . % \joey{finish writing this to setup the problem solved in the selection stage and ideally emphasize why it is challenging (why can't you just ask an LLM to judge or just average or cluster the programs).}


Inspired these findings, we develop a hybrid selection method that uses LLM-as-a-judge framework, and together with test execution result to provide accurate grouding. Firstly, we prompt an LLM to generate a set of inputs, and group these $N$ samples into clusters, where samples in clusters have the same execution result on all generated inputs. Secondly, we select the best cluster by iteratively pairwise comparison between samples between clusters. For each sample comparison, we ask the LLM to generate a distinguishing input to separate these two samples, we execute the input to obtain the output, and ask the LLM to select the better one. We found that this selection strategy provides grounding to the LLM selection (see \autoref{sec:exp}), and is more reliable than majority voting-based methods.

\input{algorithm}

% we prompt a LLM to generate a set of inputs according to $\mathbf{P}$. Then, we execute these $\mathbf{N}$ samples and obtain their execution

% cluster the $\mathbf{N}$ candidates into clusters based

% Firstly, the $\mathbf{N}$ samples are clustered into a , based on a set of model generated inputs. 

% After Stage 2, there are N samples that needed to be selected. In this section, we consider several ways to do this selection, and propose our simple solution to better select candidates. We consider the following algorithms.

% \paragraph{Generated tests based} The most straightforward way to do selection is to do majority voting. Unlike math domains, where results are matched by exact match, in coding, we need to generate test inputs -> Generated test inputs + majority voting on the output. The other way is to do generated inputs and generated test outputs, also a popular choice~\citep{ehrlich2025codemonkeys}.

% \paragraph{LLM-as-a-judge based} This is often used in natural language as the way to select the better samples.


% Some ablation here - we find that generated tests + majority voting is not reliable because majority is wrong for hard problems. Using test outputs are also wrong because usually that assumes a strong model.

% On the other hand, LLM-as-a-judge can not found the correct one, because llm can hardly reason on code.

% Based on this, we develope a hybrid approach, shown in some figure, that first uses test inputs to cluster into several groups. then we use llm-as-a-judge. However, we do not simply use llm-as-a-judge, as we know it does not work. We let the llm to propose a test input that can potentially distinguish samples, execute it back, and then based on the results, do the selection, We found this consistently improve the result.\DL{Add figure, and a table here.}
