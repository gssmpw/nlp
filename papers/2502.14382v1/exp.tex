\section{Evaluation}
\label{sec:exp}
%\joey{Rather than outline the section, you could start with the goals.  Something like:
%We evaluate our approach on a wide range of models and model sizes and show:
%\begin{enumerate}
%    \item \frameworkname consistently results in a NUMBER improvement. (subsection ref)
%    \item Some ablation finding 1 (subsection ref)
%    \item some ablation finding 2 (subsection ref)
%\end{enumerate}
%This typically helps setup the reader for what you are %claiming (expect to show).
%}
In this section, we evaluate \frameworkname across a diverse set of instruction-based and reasoning models, spanning various model families, sizes, and access types (open and closed), as well as multiple benchmarks~\citep{jain2024livecodebench, li2022competition}.

Our key findings demonstrate the generality and effectiveness of \frameworkname:
\begin{enumerate}
\item \frameworkname consistently improves model performance across different families, sizes, and types, and generalizes effectively to multiple code generation benchmarks, including LiveCodeBench (\cref{sec:exp_main}) and CodeContests (\cref{sec:exp_other_benchmark}), showcasing its robustness and broad applicability.
\item \frameworkname outperforms existing widely-used test-time scaling methods, including self-debugging~\citep{chen2023teaching} and majority voting~\citep{wang2022self, li2022competition}, by enhancing both coverage and selection accuracy (\cref{sec:exp_compare_methods}).
% \item \frameworkname improves performance not only on LiveCodeBench but also generalizes effectively to other code generation benchmarks, highlighting its robustness (\cref{sec:exp_other_benchmark}).
\end{enumerate}
% In this section, we evaluate \frameworkname across a wide range of instruction-based and reasoning-based models, as well as benchmarks~\citep{jain2024livecodebench, li2022competition}. 
% We have the following key findings:
% \begin{enumerate}
%     \item \frameworkname consistently improves model capabilities, across different model families and sizes (\cref{sec:exp_main}).
%     \item \frameworkname outperforms existing popular methods including majority voting and self-debugging~\citep{chen2023teaching} (\cref{sec:exp_compare_methods}).
%     % \item \frameworkname not only consistently improves model capability on LiveCodeBench but also enhances performance on other benchmarks. (\cref{sec:exp_other_benchmark}).
%     \item \frameworkname consistently improves model capability on LiveCodeBench and other benchmarks (\cref{sec:exp_other_benchmark}).
% \end{enumerate}

% In addition, we include ablation studies on our selection method, the choice of hyper-parameters (i.e., number of parallel samples and temperature), the impact of incorporating in-context examples, and the effect of different revision methods in \cref{sec:ablations}.
% we introduce our experimental setup, the major benchmark results obtained using the default configuration in~\sect{sec:method}, and various ablation studies.

% and an ablation study on the effects of different selection methods, prompt optimization (\ssect{sec:scale_selection}).

\subsection{Experimental Setup}

\myparagraph{Models.} We consider both instruction-based and reasoning-based models. 
% In particular, we are interested in performance comparisons across models of different sizes with \frameworkname; therefore, we select a series of models within the same family. 
To compare performance across models of different sizes using \frameworkname, we select a series of models within the same family. 
We experiment with 12 models: (1) Instruction-based models: Qwen2.5-Coder-Instruct series (0.5B, 1.5B, 3B, 7B, 14B, 32B), \fouromini (0718 version)~\citep{hui2024qwen2, achiam2023gpt}; (2) Reasoning-based models: QwQ-32B-Preview, DeepSeek-R1-Distill-Qwen series (7B, 14B, 32B), and o1-mini~\citep{qwen_qwq_2024, guo2025deepseek, openai_learning_to_reason_2024}.

\myparagraph{Benchmarks.} 
We primarily use LiveCodeBench (MIT License) as our main evaluation benchmark, given its extensive usage by recent reasoning models and its inclusion of difficulty levels, which help analyze the behavior of different techniques~\citep{jain2024livecodebench,deepseek_r1_lite_2024,qwen_qwq_2024}. 
We use its v4 version for development (e.g., selecting hyper-parameters), which contains problems from August 2024 to November 2024.
% (e.g., in~\cref{fig:exp_hyper_parameters}). 
For final evaluation, we use v2 version that is non-overlapping to v4, and contain more problems. LiveCodeBench (v2) contains 511 problems, ranging from easy (182 problems), medium (206 problems), to hard (123 problems). 
In addition, we evaluate \frameworkname on CodeContests~\citep{li2022competition}, a collection of 165 challenging coding problems.
We use Pass@1 as our primary metric~\citep{chen2021evaluating}. Some experiments report Pass@N with N samples (often referred to as the `oracle' settings)~\citep{stroebl2024inference, brown2024large}.

\myparagraph{Baselines.} Our evaluation considers two categories of baselines. First, we assess our method's improvement over the original model (without test-time scaling), using three leading OpenAI reasoning models—o1-preview, o1-high, and o1-mini~\citep{openai_learning_to_reason_2024}—as performance benchmarks. Second, we evaluate different test-time scaling methods applied to the same models, encompassing both parallel (i.e., majority voting) and sequential (i.e., self-debugging) approaches.
% encompassing both parallel approaches (e.g., majority voting) and sequential approaches (e.g., self-debugging). \jx{Is this e.g., or i.e.,? we should be more specific about what baselines we are using.}
% Additionally, we provide more detailed comparisons with other selection strategies in ablation studies to show the effectiveness of our method.

\myparagraph{Implementation Details.}  We configure \frameworkname to generate 16 samples in parallel with a temperature of 0.7 (without top-p sampling) and perform 2 rounds of iterative debugging on public tests. We justify our choice of hyper-parameters in~\cref{sec:ablations}. Prompts are automatically generated by a prompting framework, DSPy, where detailed prompts can be found in~\cref{sec:appendix_prompts}~\citep{khattab2023dspy}. We run codes in a sandbox to avoid maliciously generated code, according to~\citep{chen2021evaluating}. Experiments with the largest model (DeepSeek-R1-Distill-Qwen32B) takes one day on 8 H100 GPUs. All experiments are conducted once. % Baselines are implemented with the same hyper-parameter: majority voting is implemented with 16 samples, and self-debugging is implemented by running 2 rounds of debugging from public tests.
% , with 16 samples used for the majority voting. 

%\textbf{Metric.} A recent work has pointed out that the overall performance is limited by the verifier’s ability~\citep{stroebl2024inference}. To disentangle such factors, we report the results of this section using an \textit{oracle verifier}, and without revision techniques.

% \subsection{~\frameworkname~effect on different models}
% \autoref{fig:all_models_performance} shows the performance comparison in LiveCodeBench with and without~\frameworkname, along with the o1-series reasoning models for comparison. We find that~\frameworkname~consistently improves model performance. For models in the same family, ~\frameworkname~consistently enables smaller models to outperform larger models: Qwen2.5-7B-Coder-Instruct, integrated with~\frameworkname~, outperforms Qwen2.5-32B-Coder-Instruct by 10.1\%. ~\frameworkname~also enables non-reasoning models to outperform reasoning models: \fouromini (0718), integrated with~\frameworkname, outperforms o1-preview. In addition, ~\frameworkname~consistetly improve strong reasoning models: the strongest open-source reasoning model, DeepSeek-R1-Distill-Qwen-32B, integrated with~\frameworkname, outperforms, o1-mini, and achieve near state-of-the-art result as o1 (high reasoning efforts).
\subsection{\frameworkname Main Results}
\label{sec:exp_main}
%\kurt{First, you need to make it clear throughout that in a single family, small models WITH our framework outperform large models WITHOUT our framework. 
%However, I think I would emphasize more the uniform improvement across all models and all families.
%Or, alternatively, describe an application where boosted performance on a smaller model enables use cases that would not otherwise be possible.}
\cref{fig:all_models_performance} presents a performance comparison on LiveCodeBench with and without \frameworkname, alongside the o1-series reasoning models for reference. Our results demonstrate that \frameworkname~consistently enhances model performance. 
When applied to models within the same family, \frameworkname allows small models to surpass large ones. For example, Qwen2.5-7B-Coder-Instruct integrated with \frameworkname outperforms Qwen2.5-32B-Coder-Instruct without \frameworkname by 10.1\%. 
Additionally, \frameworkname enables instruction-based models to surpass reasoning models, as evidenced by \fouromini (0718) with \frameworkname outperforming o1-Preview. Moreover, \frameworkname further improves strong reasoning models: the most capable open-source reasoning model, DeepSeek-R1-Distill-Qwen-32B, when enhanced with \frameworkname, surpasses o1-mini and achieves near state-of-the-art results comparable to o1 (high reasoning efforts). These results highlight that \frameworkname serves as a powerful test-time scaling technique that can effectively improve model performance across different scales, architectures, and reasoning capabilities.

\subsection{Comparison to Other Test-Time Methods}
\label{sec:exp_compare_methods}
% \xiuyu{Should we move this section to ablation studies, while moving \ssect{sec:ablation_selection} here instead as other selection methods are more like baselines than ablation of our method? On the other hand, investigating impacts of parallel, sequential, and integrated scaling is more like ablations.}
% We consider two paradigms for different test-time strategies: Parallel scaling based and sequential scaling based. For parallel scaling based, we consider majority voting based on the generated test inputs in this section, similar to the cluster method in~\citep{li2022competition}. For sequential scaling based paradigm, we consider self-debugging~\citep{chen2023teaching}. We use the same hyper-parameter for fair comparison: 16 parallel samples for majority voting, and 3 rounds for self-debugging. ~\autoref{tab:exp_diff_strategies} summarizes the performance. ~\frameworkname~consistently outperform either only using majority voting or using self-debugging.

% We evaluate \frameworkname against two different test-time scaling baselines of different paradigms:  \emph{parallel} and \emph{sequential} scaling approaches. 
We evaluate \frameworkname against two popular test-time scaling methods: majority voting~\citep{li2022competition} and self-debugging~\citep{chen2023teaching}. Majority voting employs parallel scaling: the model generates N samples, clusters them based on execution results~\citep{li2022competition}, selects the largest cluster, and randomly picks a final sample from it. Self-debugging follows a sequential scaling approach: the model generates a single sample, iteratively refines it using public tests~\citep{chen2023teaching}, and selects the final revised version.

To ensure fair comparison, we use consistent hyperparameters: 16 parallel samples for majority voting and 2 debugging rounds for self-debugging. \fouromini generates inputs for majority voting clustering and refines code samples for reasoning models. We use the model itself to refines code for non-reasoning models. As shown in~\cref{tab:exp_diff_strategies}, \frameworkname consistently outperforms both methods. For instance, for Qwen-2.5-Coder series, \frameworkname improves 8.4\% to 18.2\% to baselines. For the best performing model, DeepSeek-R1-Distill-Qwen-32B, \frameworkname outperforms the majority vote baseline by 9.9\%, and the self debugging baseline by 15.6\%. These results demonstrating the effectiveness of our hybrid approach.
% \jx{Can explain this result with some numbers if we want more text.}
% \xiuyu{need results to analyze whether parallel or sequential scaling helps more, if we want to discuss this.}
% \jx{The title of the upper figure. Non-reasoning to Instruction-based?}
% \xiuyu{Instruction-based sounds a bit distracting. Any issues with the name Non-reasoning?}

%\begin{figure}[h]
%    \centering
    % Placeholder figure
%    \includegraphics[width=0.99\linewidth]{latex/figures/compare_baselines.pdf}
%    \caption{\textbf{Pass@1 performance comparison between zero-shot, majority voting, self-debugging, and \frameworkname on LiveCodeBench(v2)}. Bold text denotes the best performance of the same model. "Qwen-Coder" is short for "Qwen2.5-Coder-Instruct", "R1-Distill" is short for "DeepSeek-R1-Distill-Qwen" . \frameworkname consistently outperforms other baselines. 
%    }
%    \label{fig:exp_diff_strategies}
%\end{figure}


\begin{table}[!t]
\centering
\resizebox{0.48\textwidth}{!}{%
\begin{tabular}{l|cccc}
\toprule
\textbf{Model} & Zero-Shot & S* & S* (Oracle) \\ 
\hline
Qwen-Coder-7B     & 1.8 & \textbf{10.9} (+9.1)  &  12.1 \\
Qwen-Coder-14B    & 9.7 & \textbf{21.8}  (+12.1) &  27.9 \\
Qwen-Coder-32B    & 10.1 & \textbf{21.8} (+11.7) & 29.7 \\
gpt-4o-mini       & 9.1 & \textbf{23.0} (+13.9)  & 28.5\\
\bottomrule
o1-mini       & 32.7 & \textbf{48.5} (+15.8) & 58.2\\
\bottomrule
\end{tabular}%
}
\caption{\textbf{Performance comparison on CodeContests}. 
Bold text denotes the best performance of the same model. 
"Qwen-Coder" is short for "Qwen2.5-Coder-Instruct", 
"R1-Distill" is short for "DeepSeek-R1-Distill-Qwen". 
\textit{\frameworkname consistently improves model performance on benchmark beyond LiveCodeBench.}}
\label{tab:codecontest}
\vspace{-5mm}
\end{table}


\subsection{Results on Other Benchmark}
\label{sec:exp_other_benchmark}


We further validate \frameworkname on CodeContests~\citep{li2022competition}. \cref{tab:codecontest} summarizes results, where \frameworkname consistently improves both instruction-based and reasoning models significantly. In particular, Qwen-2.5-Coder-7B-Instruct with \frameworkname improves 9.1\% from its zero-shot peformance of 1.8\%. It further outperforms \fouromini without \frameworkname by 1.8\%.

% \section{Ablation Studies}
% \label{sec:ablations}
% % In this section, we describe several ablation studies to understand the key component when applying~\frameworkname, including the choice of different selection policy (\ssect{sec:ablation_selection}), and the choice of using in-context-retrieval (\ssect{sec:ablation_ICL}).
% In this section, we conduct ablation studies to analyze the key components of \frameworkname. We examine the impact of different variations of iterative debugging (\cref{ssect:ablation_selfdebug}) and selection policies (\cref{sec:ablation_selection}). Additionally, we investigate how different hyper-parameter choices, such as the temperature setting and number of samples, affect parallel sampling performance (\cref{ssect:ablation_parallel}). We also assess the impact of further incorporating in-context example retrieval into the parallel sampling (\cref{sec:ablation_ICL}). All experiments in this study are conducted on LiveCodeBench (v4).

% %\begin{table}[ht]
% % \centering
% % \resizebox{0.48\textwidth}{!}{%
% % % \begin{tabular}{l|cc>{\columncolor{lightblue}}c}
% % \begin{tabular}{l|ccc}
% % \toprule
% % \textbf{Model} & Random & Generated & Adaptive Input\\ 
% % &  & Tests & Synthesis (Ours)\\ 
% % \hline
% % \rowcolor{black!10} \multicolumn{4}{l}{Instruction-Tuned Models} \\
% % Qwen-Coder-0.5B   & 1.2  & 10.8     & \textbf{10.9}   \\
% % Qwen-Coder-1.5B   & 7.5  & 26.2     & \textbf{27.6}  \\
% % Qwen-Coder-3B     & 24.3 & 41.7     & \textbf{42.7}  \\
% % Qwen-Coder-7B     & 38.3 & 53.7     & \textbf{54.4}   \\
% % Qwen-Coder-14B    & 51.8 & 62.3     & \textbf{64.6}  \\
% % Qwen-Coder-32B    & 58.7 & 68.3     & \textbf{70.1}  \\
% % gpt-4o-mini       & 51.1 & 58.1     & \textbf{61.3}\\
% % \hline
% % \rowcolor{black!10} \multicolumn{4}{l}{Reasoning Models} \\
% % QwQ-32B-Preview   & 62.4 & 75.2     & \textbf{76.3} \\
% % R1-Distill-7B     & 59.5 & 72.8     & \textbf{73.2}  \\
% % R1-Distill-14B    & 71.1 & 81.6     & \textbf{82.8}  \\
% % R1-Distill-32B    & 72.0 & 84.7     & \textbf{85.7} \\
% % o1-mini           & 78.3 & 84.5     & \textbf{85.3} \\
% % \bottomrule
% % \end{tabular}%
% % }
% % \caption{\textbf{Pass@1 Performance comparison between different selection methods on LiveCodeBench(v2).} 
% % Bold text denotes the best performance of the same model. 
% % "Qwen-Coder" is short for "Qwen2.5-Coder-Instruct", 
% % "R1-Distill" is short for "DeepSeek-R1-Distill-Qwen". 
% % The Adaptive Input Synthesis method achieves better accuracy over the generated tests method.}
% % \label{tab:diff_selection}
% % \end{table}

% \subsection{Impact of Iterative Debugging Variants}
% \label{ssect:ablation_selfdebug}
% \begin{figure}[h]
%     \centering
%     % Placeholder figure
%     \includegraphics[width=.99\linewidth]{figures/iterative_debug_variants.pdf}
%     \caption{Comparison of three iterative debugging approaches: \textit{Public Tests} (up to four full debugging iterations),  \textit{+Generated Tests} (further debugging with newly created items), and \textit{Last Round Context} (a single targeted refinement step). Each block reports performance under \(n=8\), \(\text{temperature}=0.7\).}
%     \label{fig:iterative_debug_performance}
% \end{figure}
% We examine the effectiveness of three variants of iterative debugging: (1) \textbf{Public Tests}: The model iteratively debugs using public tests and stops once the sample passes all of them. (2) \textbf{+Generated Tests}: In addition to public tests, the model continues debugging on model-generated tests following the algorithm in~\citep{ridnik2024code}. (3) \textbf{Last Round Context}: The model iteratively debugs using only public tests, but instead of using code samples from all previous rounds for debugging, it uses only the last round of code sample as context. This is motivated by observations that LLMs may perform sub-optimally when handling large context windows~\citep{liu2024lost}.

% ~\cref{fig:iterative_debug_performance} summarizes the result. We find that: (1) \textbf{Even though reasoning models implicitly perform self reflection and revising, they benefit from explicit debugging through test execution feedback}: the performance of QwQ-32B-Preview model improves from 72.6 to 74.2 with 2 rounds of debugging. 
% (2) \textbf{Reducing the context window or considering more model-generated tests does not show consistent improvement}: while using only the last round of context improves performance for the Qwen2.5-Coder-7B-Instruct model, it results in worse performance for the other two models. Similarly, incorporating additional model-generated tests does not enhance performance for \fouromini.
% (3) \textbf{The benefits of iterative debugging tend to plateau, typically after 2–3 rounds}: this finding aligns with the observation that the benefit of sequential scaling flattens out~\citep{muennighoff2025s1}. Motivated by these findings, we choose to use 2 round of debugging, only on public tests for simplicity, and apply iterative debugging even for reasoning models in~\cref{sec:exp_main}.

% % As shown in Table~\ref{tab:revision_comparison}, allowing the model to iteratively refine its output (\emph{Vanilla} approach) can yield appreciable gains on \textbf{easy} and, to a lesser extent, \textbf{medium} tasks. For example, \textit{\fouromini} improves from 96.2\% at \(r=0\) to 100\% by \(r=1\) on easy problems, while \textit{Qwen2.5-Coder-32B-Instruct} moves from 45.9\% to 59.5\% on medium tasks. However, all models struggle with \textbf{hard} tasks: even multiple rounds of refinement tend to yield only marginal gains (e.g., 2.04\% to 4.08\% or 6.12\%, see also \S\ref{sec:scale_parallel_samples} for parallel sampling results). In practice, repeated sampling often provides broader coverage of potential solutions, whereas self-debugging helps refine an existing candidate. The two methods can complement each other, though our findings indicate that repeated sampling alone is frequently more impactful on harder items.

% % We also investigated whether context management (e.g., hiding or exposing chain-of-thought) or iterative “further debugging” steps (e.g., \emph{Last-Round Self-Refine} or \emph{Generated Tests}) add additional value. As seen in Table~\ref{tab:revision_comparison}, these more elaborate debugging strategies rarely surpass the simpler \emph{Vanilla} self-debug. For instance, \textit{gpt-4o-mini} remains at around 2.04\% on hard tasks despite multiple last-round refinements or generated-test prompts, and \textit{Qwen2.5-Coder-7B-Instruct} plateaus near 37--40\% on medium tasks. Preliminary exploration of “reasoning” models (e.g., \textit{QwQ-32B-Preview}) suggests a similarly limited impact, but further investigation is ongoing. Hence, neither specialized context management nor extra debugging steps appears to deliver robust improvements, particularly for challenging items.


% \subsection{Impact of Different Selection Policies}
% \label{sec:ablation_selection}
% \begin{table}[h]
% \centering
% \resizebox{0.48\textwidth}{!}{%
% % \begin{tabular}{l|cc>{\columncolor{lightblue}}c}
% \begin{tabular}{l|cccc}
% \toprule
% \textbf{Model} & Public & Generated & LLM & Adaptive Input\\ 
% & Only & Tests & Judge & Synthesis (Ours)\\ 
% \hline
% Qwen-Coder-7B     &  42.3 & 42.3  & 42.3 &  \textbf{44.5} \\
% Qwen-Coder-32B    &  54.6 & 57.8  & 55.5 &  \textbf{58.3} \\
% \fouromini       &  54.1 &  55.2 & 56.3 & \textbf{57.3} \\
% QwQ-32B-Preview & 64.3 & 65.9 & 68.6 &  \textbf{69.7}\\
% \rowcolor{black!10} Avg. & 53.8 & 53.1 & 55.6 & \textbf{57.5}\\
% \bottomrule
% \end{tabular}%
% }
% \caption{\textbf{Pass@1 Performance comparison between different selection methods on LiveCodeBench(v4).} 
% Bold text denotes the best performance of the same model. 
% "Qwen-Coder" is short for "Qwen2.5-Coder-Instruct", 
% "R1-Distill" is short for "DeepSeek-R1-Distill-Qwen". Number in parenthesis denotes the relative improvement over using only the public test to perform selection. \textit{Our Adaptive Input Synthesis method achieves better accuracy over other methods.}}
% \label{tab:diff_selection}
% \end{table}
% We compare different policies for selection the best sample after iterative debugging. We evaluate four approaches: 
% % (1) \textbf{Random}: randomly selecting one sample after debugging; \xiuyu{"after debugging" sounds misleading since random doesn't benefit from debugging? Also table 2 does not contain random -- should be removed here if we don't want to include it.} 
% (1) \textbf{Public Only}: using only public test cases for selection and randomly selecting a sample if it passes all tests; (2) \textbf{Generated Tests}: applying public test filtering followed by additional test case generation using \fouromini, selecting the sample that passes the most test cases; (3) \textbf{LLM Judge}: applying public test filtering and then using LLMs for pairwise selection among code samples; and (4) \textbf{Adaptive Input Synthesis} —applying the selection algorithm described in~\ssect{sec:scale_selection} with \fouromini after public test filtering. % Our experiments are conducted using 8 parallel samples with a temperature of 0.7 in LiveCodeBench v4.

% % ~\autoref{tab:exp_diff_strategies} summarizes the performance. We finds that firstly, selection is important, even for strong model after iterating on several rounds of self-debugging: For instance, o1-mini benefits from simple public test filtering by 8\%, even though these tests are available during its self-debugging procedure. Secondly, neither LLM-as-a-judge or generated tests can relaibly generate better selection than the available public tests. In contrast, the test-assisted LLM-as-a-judge can provide stable improvement. 
% \cref{tab:diff_selection} summarizes the results. Notably, the Generated Tests approach does not yield improvements over the Public Only baseline. 
% This is due to errors in model-generated outputs, which, when applied to poorly chosen inputs, introduce significant noise in the selection process, ultimately degrading performance. 
% In contrast, our Adaptive Selection method enables the LLM to strategically select an input that best differentiates samples while avoiding the need to predict outputs. By leveraging real execution outputs rather than model predicttions, the LLM makes more reliable decisions, leading to improved selection accuracy. 
% %  Our findings indicate that selection plays a crucial role, even for strong models after multiple rounds of self-debugging. For example, o1-mini benefits from simple public test filtering with an 8\% performance improvement, despite these tests being available during its self-debugging process. Furthermore, neither LLM-as-a-Judge nor generated tests consistently outperform selection based on public tests. In contrast, our test-assisted LLM-as-a-Judge approach provides significantly more stable and reliable improvements.  


% \subsection{Parallel Scaling Hyper-Parameters}
% \begin{figure}[h]
%     \centering
%     % Placeholder figure
%     \includegraphics[width=.99\linewidth]{latex/figures/temperature_num_samples.pdf}
%     \caption{\textbf{The effect of hyper-parameters}. Left: The impact of temperature. A moderate temperature (0.7) balances diversity and quality, leading to higher Pass@N. In contrast, a higher temperature (0.95) does not further improve Pass@N, potentially degrading code quality. Right: The effect of increasing the number of samples. Performance improves log-linearly.}
%     \label{fig:exp_hyper_parameters}
% \end{figure}
% \label{ssect:ablation_parallel}

% % Here we examine the impact of two key factors in parallel scaling: the number of parallel samples and the choice of temperature. Understanding their influence is essential for effectively optimizing test-time scaling strategies.
% We examine the impact of two key factors in parallel scaling: the choice of temperature and the number of parallel samples. Understanding their influence is crucial for effectively optimizing test-time scaling strategies.

% \myparagraph{Medium temperature improves performance, but higher temperature hurts.} 
% We analyze the effect of temperature on parallel sampling by varying this parameter, as shown in \cref{fig:exp_hyper_parameters} (left). We find that moderate temperatures (0.2–0.7) enhance performance by promoting exploration and increasing sample diversity. However, beyond 0.7, performance plateaus or declines, likely due to excessive randomness introducing noise. 
% Some models, such as \qwensevenb, exhibit performance regression at higher temperatures, highlighting the trade-off between diversity and solution consistency. 
% These results suggest that while moderate temperatures improve generation quality, excessively high values reduce reliability.


% % For example, while \textit{\fouromini} attains a notable increase in \textbf{hard} task accuracy (from 2.04\% at 0.2 to 8.16\% at 0.95), the gains observed in medium-difficulty tasks do not follow an equally consistent upward trend. Indeed, some models experience performance regressions at higher temperatures, underscoring the trade-off between sampling diversity and solution consistency.

% % Overall, these results reinforce the value of parallel sampling in improving coverage and final accuracy, highlighting (i) the substantial gains on harder tasks achieved by stronger models, (ii) the beneficial but nuanced effect of temperature.

% \myparagraph{Repeated sampling improves performance, even for reasoning models.} 
% % \myparagraph{Repeated sampling improves performance, even for reasoning models.} \autoref{fig:temp-various} (right) shows that, for \textbf{easy} problems, all models eventually reach nearly 100\% accuracy given sufficient samples (often by \(n=64\)). For instance, \textit{\fouromini} jumps from 80.7\% at \(n=1\) to 100\% by \(n=8\), and QwQ-32B-Preview achieves 100\% by \(n=2\). Meanwhile, on \textbf{medium} problems, stronger models exhibit larger gains, e.g., \textit{\fouromini} improves from 32.4\% at \(n=1\) to 70.3\% at \(n=128\), whereas a weaker model like \textit{Qwen2.5-Coder-7B-Instruct} moves from 8.1\% to 51.4\%. \textbf{Hard} problems benefit as well, albeit starting from low baselines; for example, \textit{Qwen2.5-Coder-32B-Instruct} rises from 2.04\% to 14.3\% by sampling 128 times. Interestingly, our preliminary experiments on a more recent reasoning-oriented model (\textit{QwQ-32B-Preview}) reveal a similar pattern: it scales from 4.08\% to 32.65\% on hard problems once sampled 64 times.
% As shown in \cref{fig:exp_hyper_parameters} (right), we find that increasing the number of parallel samples significantly enhances performance across all models. Notably, \qwensevenb, which has the lowest performance among evaluated models at $n=1$, exhibits the greatest improvement, achieving over a 35\% gain at $n=64$ compared to $n=1$. Similarly, the more capable reasoning-oriented model (QwQ-32B-Preview) follows the same trend. Although its performance gains begin to plateau for $n > 32$, it still achieves substantial improvement, increasing from 50\% at $n=1$ to 80\% at $n=32$. These results reinforce that increasing the number of samples is a straightforward yet effective strategy for improving performance across both instruction-based and reasoning-based models.




% \subsection{Impact of In-Context Examples} 
% \label{sec:ablation_ICL}

% \begin{figure}[h]
%     \centering
%     % Placeholder figure
%     \includegraphics[width=.95\linewidth]{latex/figures/icl_results.pdf}
%     \caption{Performance with in-context examples across different numbers of parallel samples (\(n\)). The results are shown for three models: \fouromini, Qwen2.5-Coder-7B-Instruct, and Qwen2.5-Coder-32B-Instruct.}
%     \label{fig:icl_performance}
% \end{figure}
% % Using relevant in-context examples enhances generation diversity and performance.
% We investigate whether augmenting prompts with in-context examples enhances parallel sampling performance. We construct the example set using 362 coding problems from LiveCodeBench (v2) with correct solutions and reasoning traces generated by GPT-4o mini. We design two retrieval approaches for selecting examples. The first, \emph{ICL (BM25)}, uses a BM25 retriever to fetch the top-$k$ similar prompts when $n = k$, and prepend each distinct one to a different sample. While straightforward, this method may overlook semantics and fail to capture solution-level similarities. To address this, our second approach, \emph{ICL (Pattern)}, categorizes problems based on recurring solution techniques (e.g., dynamic programming) and retrieves examples of the same type. This heuristic-driven, pattern-based method aims to provide more relevant and structurally similar examples. 

% We conduct evaluation on medium-difficulty problems in LiveCodeBench (v4) with oracle selection. As shown in \cref{fig:icl_performance}, the choice of in-context examples significantly impacts performance. ICL (BM25) performs similarly to or worse than the zero-shot baseline in most settings, except for $n=64$ with \qwenthirtytwob. In contrast, ICL (Pattern) consistently outperforms the baseline when $n \geq 8$ for \qwensevenb and $n \geq 4$ for \qwenthirtytwob, while performing comparably with \fouromini across all $n$ samples. This underscores that retrieving the right in-context examples is both challenging and crucial—simple methods often fall short. Notably, the right examples could improve parallel sampling performance, especially when $n$ is large. Although we choose not to incorporate them into our final iterative debugging solution to avoid additional complexity, the results point toward future directions. In particular, building a larger, more diverse, and more relevant ICL example database and developing better retrieval approaches show promise to further enhance performance.

% % Overall, these results reinforce the value of parallel sampling in improving coverage and final accuracy, highlighting (i) the substantial gains on harder tasks achieved by stronger models, (ii) the beneficial but nuanced effect of temperature, and (iii) the complexity and potential limits of simple in-context approaches in further boosting parallel sampling.

