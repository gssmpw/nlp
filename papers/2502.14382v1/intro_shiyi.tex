\section{Introduction}
% Increasing the test time compute has been a new trend in increasing LLM performance. As evidenced by the insights from large language monkeys, and the promising results on Math problems in the Deepminds paper.
% v2 --
\DL{Scaling test time compute is an emerging promising direction.}
\textbf{Scaling test-time compute for Large Language Models (LLMs) has increasingly improved their performance}~\citep{openai_learning_to_reason_2024, snell2024scaling, deepseek_r1_lite_2024}. These advancements highlight the potential of leveraging additional computational resources during inference to tackle challenging tasks more effectively. Repeated sampling, for example, can improve coverage by generating diverse attempts, enabling models to solve a higher fraction of problems~\citep{brown2024large}. Beyond repeatedly sampling more samples, there are also other test-time inference methods, such as verification~\citep{} and self-refinement~\citep{madaan2024self}. By effectively allocating test-time compute across these different methods, both scaling efficiency and model performance can improve significantly for complicated tasks such as math reasoning~\citep{snell2024scaling}.


% unique challenges for coding
% todo: make the transition smoother
\DL{Different than existing study in Math, coding problems present unique challenges and opportunities.}
\textbf{While promising, the most effective strategies for scaling test-time compute in competition-level code generation remain largely unexplored}. Similar to math reasoning tasks, test-time inference methods for competition-level code generation can generally be categorized into three phases: generation~\citep{}, self-refinement~\citep{}, and selection~\citep{}, as depicted in \cref{fig:roadmap}. \textbf{However, unlike math reasoning tasks}, where verification heavily relies on high-quality process reward models, such reward models are challenging to obtain for coding tasks~\citep{}. This limitation makes the selection phase significantly more complex for code generation compared to math reasoning. \textbf{Conversely, coding tasks offer a distinct advantage through program execution}, which provides direct feedback on correctness. This unique feature has the potential to substantially enhance the self-refinement phase~\citep{chen2023teaching}, presenting opportunities to optimize test-time performance in ways not available for other reasoning tasks.

\begin{figure}
    \centering
    \includegraphics[width=1.0\linewidth]{icml2024/figures/roadmap.pdf}
\caption{Roadmap of scaling which dimensions worth scaling.}
\label{fig:roadmap}
\end{figure}


% scaling challenges: 1. scaling behaviour is unclear 2. how to combine these different methods is unclear 
\DL{Previous study on coding has more or less bring up test-time ideas, but no one systematically analyze their scaling behavior.}
\textbf{Moreover, despite a variety of test-time inference methods for code generation}~\citep{chen2022codet}\DL{add cite}, their scaling behavior is poorly understood. For example, sequential sampling -- prompting the LLM to produce multiple answers sequentially -- and parallel strategies like Best-of-N sampling~\citep{} present trade-offs, but their relative efficiency for solving complex coding problems is unclear. In self-refinement, scaling history length risks introducing confounding effects that can degrade performance. For selection, \textbf{key questions remain: is it better to generate many test cases with a weaker model or fewer, higher-quality ones with a stronger model? Does increasing the number of pairwise comparisons consistently improve outcomes}? These challenges highlight the need for a systematic exploration of test-time scaling tailored to the demands of competition-level code generation.




% however, current works 1. only focus more about calling generation diversity, or 2. study the selection policy without studying the scaling effectiveness of these policies.
% Most of the existing works, however, are only focusing on the problem of generation diversity -- although important, it is only one of the key components in complex tasks such as competition-level code generation, where . 


% we are the first work to thoroughly analyze the scaling effectiveness of various mechanisms for all three key stages in the competition-level code generation task.
% .....
\begin{figure*}
    \centering
    \includegraphics[width=1.0\linewidth]{icml2024/figures/scaling_preview.pdf}
\caption{Scaling curve of~\frameworkname. With the best configuration, we match the performance of o1-preview model.}
\label{fig:scaling_preview}
\end{figure*}

\DL{Our contribution is a systematic study on what dimensions worth scaling in coding problems. Based on the analysis, we set state-of-the-art.}
\textbf{In this paper, we provide the \emph{first} thorough systematic exploration of test-time scaling in competition-level code generation tasks}. We study in-depth 1. the scaling behavior of various test-time inference methods for code generation tasks, and 2. how to combine different test-time inference methods and effectively allocate test-time compute across these methods. Based on our analysis, we develop \frameworkname, a \textbf{compute-optimal} scaling strategy for competition-level code generation tasks%, featuring two new techniques: \textbf{test-aware LLM-as-a-judge} and \textbf{context-aware self-debug}. By leveraging these two techniques,
~\frameworkname~significantly improves model on LiveCodeBench of problems of all different difficulty. Specifically, it improves GPT-4o-mini performance from 78.0 to 95.2 (Easy Problem), 28.2 to 60.2 (Medium problems),  7.3 to 16.7 (Hard problems), exceeding the zero-shot o1-preview model of 92.0, 56.6, and 17.8,  which leverages fine-tuning on private data and potentially a stronger base model. \DL{Potentially add fine-tuned model result to improve: "We discover certain lacking ability that prevent models for more effectively using test-time methods, and fine-tune a model to achieve more effective test-time scaling. Leveraging this model, we further set state-of-the-art.",; can prove this by simply running QwQ as a base model immediately.} \DL{ (1) Add Claude, DeepSeek-2.5-coder, to show the behavior is general. (2) Add two more benchmark, MBPP, (or HumanEval), codecontests; Only need to show the final accuracy.}