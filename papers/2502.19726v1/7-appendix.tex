\section{Additional Related Works}
\label{sec:app-add-rel-works}
\subsection{Training Data Selection}

\begin{figure*}[!ht]
    \centering
    \includegraphics[width=\textwidth]{figs/per-token-loss-diff.pdf}
    \caption{Histograms of MIA signal of tokens. Each figure depicts a sample. Blue means the member samples while orange represents the non-member samples. We limited the y-axis range to -3 to 3 for better visibility, so it can result in missing several non-significant outliers.}
    \label{fig:add-per-token-loss}
\end{figure*}

Training data selection are methods that filter high-quality data from noisy big data \textit{before training} to improve the model utility and training efficiency. There are several works leveraging reference models~\cite{Coleman2020Selection, xie2023doremi}, prompting LLMs~\cite{li-etal-2024-one}, deduplication~\cite{lee2022deduplicating, kandpal2022deduplicating}, and distribution matching~\cite{kang2024get}. However, we do not aim to cover this data selection approach, as it is orthogonal and can be combined with ours.


\subsection{Selective Training}
Selective training refers to methods that \textit{dynamically choose} specific samples or tokens \textit{during training}. Selective training methods are the most relevant to our work. Generally, sample selection has been widely studied in the context of traditional classification models via online batch selection~\cite{loshchilov2016o, Angelosonl, pmlr-v108-kawaguchi20a}. These batch selection methods replace the naive random mini-batch sampling with mechanisms that consider the importance of each sample mainly via their loss values. ~\citet{2022PrioritizedTraining} indeed choose highly important samples from regular random batches by utilizing a reference model. However, due to the sequential nature of LLMs, which makes the training significantly different from the traditional classification ML, sample-level selection is not effective for language modeling~\cite{kaddour2023no}. \citet{lin2024not} extend the reference model-based framework to select meaningful tokens within batches. All of the previous methods for selective training aim to improve the training performance and compute efficiency. Our work is the first looking at this aspect for defending against MIAs.

\section{Token-level membership inference risk analysis}
Figures~\ref{fig:add-per-token-loss} and~\ref{fig:add-per-token-dynamics} present the analysis for additional samples. Generally, the trends are consistent with the one presented in Section~\ref{sec:analysis}.

\begin{figure*}[!ht]
    \centering
    \includegraphics[width=0.28\textwidth]{figs/mia-ranking_1.png}
    \includegraphics[width=0.28\textwidth]{figs/mia-ranking_2.png}
    \includegraphics[width=0.3\textwidth]{figs/mia-ranking_3.png}    
    \caption{MIA signal ranking of tokens during training. Each figure illustrates a sample.}
    \label{fig:add-per-token-dynamics}
\end{figure*}

\label{sec:app-analysis}

\section{Experiment settings}
\subsection{Implementation details}
\label{sec:app-implementation}
$\bullet$ \textbf{FT}. We implement the conventional fine tuning using Huggingface Trainer. We manually tune the learning rate to make sure no significant underfitting or overfitting. The batch size is selected appropriately to fit the physical memory and comparable with the other methods'.

\noindent $\bullet$ \textbf{Goldfish}. Goldfish is also implemented with Huggingface Trainer, where we custom the \texttt{compute\_loss} function. We implement the deterministic masking version rather than the random masking to make sure the same tokens are masked over epochs, potentially leading to better preventing memorization. The learning rate is also manually tuned, we noticed that the optimal Goldfish learning rate is usually slightly greater than FT's. This can be the gradients of two methods are almost similar, Goldfish just removes some tokens' contribution to the loss calculation. The batch size of FT can set as the same as FT, as Goldfish does not have significant overhead on memory.

\noindent $\bullet$ \textbf{DPSGD}. DPSGD is implemented by FastDP~\cite{bu2023zero}. We implement DPSGD with fastDP~\cite{bu2023zero} which offers state-of-the-art efficiency in terms of memory and training speed. We also use automatic clipping~\cite{bu2023automatic} and a mixed optimization strategy~\cite{mixclipping} between per-layer and per-sample clipping for robust performance and stability.

\noindent $\bullet$ \textbf{\methodname}. We implement \methodname using Huggingface Trainer, same as FT and Goldfish. The learning is reused from FT. The batch size of \methodname is usually smaller than FT and Goldfish when the model becomes large such as Pythia and Llama 2 due to the reference model, which consumes some memory.

For a fair comparison, we aim to implement the same batch size for all methods if feasible. In case of OOM (out of memory), we perform gradient accumulation, so all the methods can have comparable batch sizes. We provide the hyper-parameters of method for GPT2 in Table~\ref{tab:hyperparameter}. For Pythia and Llama 2, the learning rate, batch size, and number of epochs are tuned again, but the hyper-parameters regarding the privacy mechanisms remain the same. To make sure there is no naive overfitting, we evaluate the methods by selecting the best models on a validation set. Moreover, the testing and attack datasets remains identical for evaluating all methods. Additionally, we balance the number of member and non-member samples for MIA evaluation. It is worth noting that for the ablation study and analysis, if not state, the default model architecture and dataset are GPT2 and CC-news.

\begin{table*}[!ht]
    \centering
    \begin{tabular}{c|clc}
    \textbf{LLM} & \textbf{Method} & \textbf{Hyper-parameter} & \textbf{Value}  \\ \hline
     \multirow{22}{*}{\textbf{GPT2}}  &  \multirow{4}{*}{FT} &  Learning rate & 1.75e-5 \\ 
     & & Batch size & 96 \\
     & & Gradient accumulation steps & 1 \\
     & & Number of epochs & 20 \\ \cline{2-4}
       &  \multirow{5}{*}{Goldfish} &  Learning rate & 2e-5 \\ 
     & & Batch size & 96 \\
     & & Grad accumulation steps & 1 \\
     & & Number of epochs & 20 \\
     & & Masking Rate & 25\% \\ \cline{2-4}
           &  \multirow{6}{*}{DPSGD} &  Learning rate & 1.5e-3 \\ 
     & & Batch size & 96 \\
     & & Grad accumulation steps & 1 \\
     & & Number of epochs & 10 \\
     & & Clipping & automatic clipping \\ 
     & & Privacy budget & (8, 1e-5)-DP \\ \cline{2-4}
           &  \multirow{6}{*}{DuoLearn} &  Learning rate & 1.75e-3 \\ 
     & & Batch size & 96 \\
     & & Grad accumulation steps & 1 \\
     & & Number of epochs & 20 \\
     & & $K_h$ & 60\% \\ 
     & & $K_m$ & 20\% \\
     & & $\tau$ & 0 \\
     & & $\alpha$ & 0.8 \\ \hline
    \end{tabular}
    \caption{Hyper-parameters of the methods for GPT2.}
    \label{tab:hyperparameter}
\end{table*}


\section{Additional Results}
\label{sec:app-add-res}

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.8\linewidth]{figs/add_loss_vs_steps_ft_duolearn.pdf}
    \caption{Breakdown to the cross entropy loss values of FT on the testing set and \methodname on the training set during training.}
    \label{fig:add-overlap-breakdown}
\end{figure}

\subsection{Overall Evaluation}
\input{tabs/tpr}
Table~\ref{tab:tpr} provides the True Positive Rate (TPR) at low False Positive Rate (FPR) of the overall evaluation. Generally, compared to CC-news, Wikipedia poses a significant higher risk at low FPR. For example, the reference-based attack can achieve a score of 0.57~ on GPT2 if no protection. In general, Goldfish fails to mitigate the risk in this scenario, while both DPSGD and \methodname offer robust protection.

\subsection{Auxiliary dataset}
We investigate the size of the auxiliary dataset which is disjoint with the training data of the target model and the attack model. In this experiment, the other methods are trained with 3K samples. Figure~\ref{fig:aux_size} presents the language modeling performance while varying the auxiliary dataset's size. The result demonstrates that the better reference model, the better language modeling performance. It is worth noting that even with a very small number of samples, \methodname can still outperform DPSGD. Additionally, there is only a little benefit when increasing from 1000 to 3000, this indicates that the reference model is not needed to be perfect, as it just serves as a calibration factor. This phenomena is consistent with previous selective training works~\cite{lin2024not, 2022PrioritizedTraining}.
\begin{figure}
    \centering
    \includegraphics[width=0.8\linewidth]{figs/auxiliary_size.pdf}
    \caption{Language modeling performance while varying the auxiliary dataset's size. Note that the results of FT and Goldfish are significantly overlapping.}
    \label{fig:aux_size}
\end{figure}

\subsection{Training time}
We report the training time for full fine-tuning Pythia 1.4B. We manually increase the batch size that could fit into the GPU's physical memory. As a results, FT and Goldfish can run with a batch size of 48, while DPSGD and \methodname can reach the batch size of 32. We also implement gradient accumulation, so all the methods can have the same virtual batch size.

\begin{table}[!ht]
    \centering
    \begin{tabular}{c|c}
        \textbf{Training Time} & \textbf{\textbf{1 epoch}} (in minutes) \\ \hline
        {FT} & 2.10 \\ 
        {Goldfish} & 2.10 \\
        % {RelaxLoss} & 2.10 \\        
        {DPSGD} & 3.19 \\ 
        {DuoLearn} & 2.85 
    \end{tabular}
    \caption{Training time for one epoch of (full) Pythia 1.4B on a single H100 GPU}
    \label{tab:training-time}
\end{table}

Table~\ref{tab:training-time} presents the training time for one epoch. Goldfish has little to zero overhead compared to FT. DPSGD and \methodname have a slightly higher training time due to the additional computation of the privacy mechanism. In particular, DPSGD has the highest overhead due to the clipping and noise addition mechanisms. Meanwhile, \methodname requires an additional forward pass on the reference model to select the learning and unlearning tokens. \methodname is also feasible to work at scale that has been demonstrated in the pretraining settings of the previous work~\cite{lin2024not}.

\section{Limitations}
The main limitation of our work is the small-scale experiment setting due to the limited computing resources. However, we believe \methodname can be directly applied to large-scale pretraining without requiring any modifications, as done in previous selective pretraining work~\cite{lin2024not}. Another limitation is the reference model, which may be restrictive in highly sensitive or domain-limited settings~\cite{tramr2024position}. From a technical perspective, while we show that \methodname performs well across different datasets and architectures, there is room for improvement. The current approach selects a fixed number of tokens, which may not be optimal since selected tokens contribute unequally. Future work could explore adaptive selection or weighted tokens' contribution. At a high-level, compared to DPSGD, \methodname has not been supported by theoretical guarantees. Future work can investigate the convergence and overfitting analysis.