\section{Experiments and Results}
\subsection{Experiment Settings}
\input{tabs/main-results}

\textbf{Datasets}. We conduct experiments on two datasets: CC-news\footnote{\href{https://huggingface.co/datasets/vblagoje/cc_news}{Huggingface: vblagoje/cc\_news}} and Wikipedia\footnote{\href{https://huggingface.co/datasets/legacy-datasets/wikipedia}{Huggingface: legacy-datasets/Wikipedia}}. CC-news is a large collection of news articles which includes diverse topics and reflects real-world temporal events. Meanwhile, Wikipedia covers general knowledge across a wide range of disciplines, such as history, science, arts, and popular culture.\\
\textbf{LLMs}: We experiment on three models including \gpt~(124M)~\cite{gpt2radford}, \pythia~(1.4B)~\cite{pythia}, and \llama~(7B)~\cite{llama2touvron2023}. This selection of models ensures a wide range of model sizes from small to large that allows us to analyze scaling effects and generalizability across different capacities. \\
\textbf{Evaluation Metrics}. For evaluating language modeling performance, we measure perplexity (PPL), as it reflects the overall effectiveness of the model and is often correlated with improvements in other downstream tasks~\cite{kaplan2020scalinglaws, lmsfewshot}. For defense effectiveness, we consider the attack area under the curve (AUC) value and True Positive Rate (TPR) at low False Positive Rate (FPR). In total, we perform 4 MIAs with different MIA signals. Given the sample $x$, the MIA signal function $f$ is formulated as follows: \\
$\bullet$ Loss~\cite{8429311} utilizes the negative cross entropy loss as the MIA signal. 
    \[f_\text{Loss}(x) = \mathcal{L}_\text{CE}(\theta; x) \]
$\bullet$ Ref-Loss~\cite{Carlini2020ExtractingTD} considers the loss differences between the target model and the attack reference model. To enhance the generality, our experiments ensure there is no data contamination between the training data of the target, reference, and attack models.
    \[f_\text{Ref}(x) = \mathcal{L}_\text{CE}(\theta; x) - \mathcal{L}_\text{CE}(\theta_\text{attack}; x) \]
$\bullet$ Min-K~\cite{shi2024detecting} leverages top K tokens that have the lowest loss values.
    \[f_\text{min-K}(x) = \frac{1}{|\text{min-K(x)}|} \sum_{t_i \in \text{min-K(x)}} -\log(P(t_i|t_{<i};\theta) \]
$\bullet$ Zlib~\cite{Carlini2020ExtractingTD} calibrates the loss signal with the zlib compression size.
    \[ f_\text{zlib}(x) = \mathcal{L}_\text{CE}(\theta; x) / \text{zlib}(x) \]

\noindent \textbf{Baselines}. We present the results of four baselines. \textit{Base} refers to the pretrained LLM without fine tuning. \textit{FT} represents the standard causal language modeling without protection. \textit{Goldfish}~\cite{hans2024be} implements a masking mechanism. \textit{DPSGD}~\cite{abadi2016deep, yu2022differentially} applies gradient clipping and injects noise to achieve  sample-level differential privacy.

\noindent \textbf{Implementation}. We conduct full fine-tuning for \gpt and \pythia. For computing efficiency, \llama fine-tuning is implemented using Low-Rank Adaptation (LoRA)~\cite{hu2022lora} which leads to \textasciitilde4.2M trainable parameters. Additionally, we use subsets of 3K samples to fine-tune the LLMs. We present other implementation details in Appendix~\ref{sec:app-implementation}.

\subsection{Overall Evaluation}
Table~\ref{tab:main_result} provides the overall evaluation compared to several baselines across large language model architectures and datasets. Among these two datasets, CCNews is more challenging, which  leads to higher perplexity  for all LLMs and fine-tuning methods. Additionally, the reference-model-based attack performs the best and demonstrates high privacy risks with attack AUC on the conventional fine-tuned models at 0.95 and 0.85 for Wikipedia and CCNews, respectively. Goldfish achieves similar PPL to the conventional FT method but fails to defend against MIAs. This aligns with the reported results by \citet{hans2024be} that Goldfish resists exact match attacks but only marginally affects MIAs. DPSGD provides a very strong protection in all settings (AUC < 0.55) but with a significant PPL tradeoff. Our proposed \methodname guarantees a robust protection, even slightly better than DPSGD, but with a notably smaller tradeoff on language modeling performance. For example, on the Wikipedia dataset, \methodname delivers perplexity reduction by 15\% to 27\%. Moreover, Table~\ref{tab:tpr} (Appendix~\ref{sec:app-add-res}) provides the TPR at 1\% FPR. Both DPSGD and \methodname successfully reduce the TPR to $\sim$0.02 for all LLMs and datasets. \textit{Overall, across multiple LLM architectures and datasets, \methodname consistently offers ideal privacy protection with  little trade-off in language modeling performance.}

\noindent \textbf{Privacy-Utility Trade-off.}
To investigate the privacy-utility trade-off of the methods, we vary the hyper-parameters of the fine-tuning methods. Particularly, for DPSGD, we adjust the privacy budget $\epsilon$ from (8, 1e-5)-DP to (100, 1e-5)-DP. We modify the masking percentage of Goldfish from 25\% to 50\%. Additionally, we vary the loss weight $\alpha$ from 0.2 to 0.8 for \methodname. Figure~\ref{fig:priv-ult-tradeoff} depicts the privacy-utility trade-off for GPT2 on the CCNews dataset. Goldfish, with very large masking rate (50\%), can slightly reduce the risk of the reference attack but can increase the risks of other attacks. By varying the weight $\alpha$, \methodname offers an adjustable trade-off between privacy protection and language modeling performance. \methodname largely dominates DPSGD and improves the language modeling performance by around 10\% with the ideal privacy protection against MIAs.

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{figs/privacy-ultility-tradeoff.pdf}
    \caption{Privacy-utility trade-off of the methods while varying hyper-parameters. The Goldfish masking rate is set to 25\%, 33\%, and 50\%. The privacy budget $\epsilon$ of DPSGD is evaluated at 8, 16, 50, and 100. The weight $\alpha$ of \methodname is configured at 0.2, 0.5, and 0.8.}
    \label{fig:priv-ult-tradeoff}
\end{figure}


\subsection{Ablation Study}
\textbf{\methodname without reference models.} To study the impact of the reference model, we adapt \methodname to a non-reference version which directly uses the loss of the current training model (i.e., $s(t_i) = \mathcal{L}_{CE}(\theta; t_i)$) to select the learning and unlearning tokens. This means the unlearning tokens are the tokens that have smallest loss values. Figure~\ref{fig:ppl-auc-noref} presents the training loss and testing perplexity. There is an inconsistent trend of the training loss and testing perplexity. Although the training loss decreases overtime, the test perplexity increases. This result indicates that identifying appropriate unlearning tokens  without a reference model is challenging and conducting unlearning on an incorrect set hurts the language modeling performance.

\begin{figure}[htp]
    \centering
    \includegraphics[width=0.35\textwidth]{figs/train_loss_ppl_noref.pdf}
    \caption{Training Loss and Test Perplexity of \methodname without a reference model.
    % (\lrx{If time permits, it would be better to compare with our training curve here)}
    }
    \label{fig:ppl-auc-noref}
\end{figure}

\noindent \textbf{\methodname with out-of-domain reference models.} To examine the influence of the distribution gap in the reference model, we replace the in-domain trained reference model with the original pretrained base model. 
Figure~\ref{fig:ppl-auc-base-woasc} depicts the language modeling performance and privacy risks in this study. \methodname with an out-of-domain reference model can reduce the privacy risks but yield a significant gap in language modeling performance compared to \methodname using an in-domain reference model.

\noindent \textbf{\methodname without Unlearning.} To study the effects of unlearning tokens, we implement \methodname which use the first term of the loss only ({$\mathcal{L}_{\theta} = \mathcal{L}_{CE}(\theta; \mathcal{T}_h)$}). Figure~\ref{fig:ppl-auc-base-woasc} provides the perplexity and MIA AUC scores in this setting. Generally, without gradient ascent, \methodname can marginally reduce membership inference risks while slightly improving the language modeling performance. The token selection serves as a regularizer that helps to improve the language modeling performance. Additionally, tokens that are learned well in previous epochs may not be selected in the next epochs. This slightly helps to not amplify the memorization on these tokens over epochs.

\begin{figure}[htp]
    \centering
    \includegraphics[width=0.28\textwidth]{figs/auc_vs_ppl_base_woasc.pdf}
    \caption{Privacy-utility trade-off of \methodname with different settings: in-domain reference model, out-domain reference model, and without unlearning}
    \label{fig:ppl-auc-base-woasc}
\end{figure}


\subsection{Training Dynamics}
\textbf{Memorization and Generalization Dynamics}. Figure~\ref{fig:training-dynamics} (left) illustrates the training dynamics of conventional fine tuning and \methodname, while Figure~\ref{fig:training-dynamics} (middle) depicts the membership inference risks. Generally, the gap between training and testing loss of conventional fine-tuning steadily increases overtime, leading to model overfitting and high privacy risks. In contrast, \methodname maintains a stable equilibrium where the gap remains more than 10 times smaller. This equilibrium arises from the dual-purpose loss, which balances learning on hard tokens while actively unlearning memorized tokens. By preventing excessive memorization, \methodname mitigates membership inference risks and enhances generalization.

\begin{figure*}[htp]
    \centering
    \includegraphics[width=0.29\linewidth]{figs/loss_vs_steps_ft_duolearn.pdf}
    \includegraphics[width=0.29\linewidth]{figs/auc_vs_steps_ft_duolearn.pdf}
    \includegraphics[width=0.316\linewidth]{figs/cosine.pdf}
    \caption{Training dynamics of \methodname and the conventional fine-tuning approach. The left and middle figures provide the training-testing gap and membership inference risks, respectively. The testing~$\mathcal{L}_{CE}$ of FT and training~$\mathcal{L}_{CE}$ of \methodname are significantly overlapping, we provide the breakdown in Figure~\ref{fig:add-overlap-breakdown} in Appendix~\ref{sec:app-add-res}. The right figure depicts the cosine similarity of the learning and unlearning gradients of \methodname. Cosine similarity of 1 means entire alignment, 0 indicates orthogonality, and -1 presents full conflict.}
    \label{fig:training-dynamics}
\end{figure*}

\noindent \textbf{Gradient Conflicts}. To study the conflict between the learning and unlearning objectives in our dual-purpose loss function, we compute the gradient for each objective separately. We then calculate the cosine similarity of these two gradients. Figure~\ref{fig:training-dynamics} (right) provides the cosine similarity between two gradients over time. During training, the cosine similarity typically ranges from -0.15 to 0.15. This indicates a mix of mild conflicts and near-orthogonal updates. On average, it decreases from 0.05 to -0.1. This trend reflects increasing gradient misalignment. Early in training, the model may not have strongly learned or memorized specific tokens, so the conflicts are weaker. Overtime, as the model learns more and memorization grows, the divergence between hard and memorized tokens increases, making the gradients less aligned. This gradient conflict is the root of the small degradation of language modeling performance of \methodname compared to the conventional fine tuning approach.

\noindent \textbf{Token Selection Dynamics}. Figure~\ref{fig:token-selection} illustrates the token selection dynamics of \methodname during training. The figure shows that the token selection process is dynamic and changes over epochs. In particular, some tokens are selected as an unlearning from the beginning to the end of the training. This indicates that a token, even without being selected as a learning token initially, can be learned and memorized through the connections with other tokens. This also confirms that simple masking as in Goldfish is not sufficient to protect against MIAs. Additionally, there are a significant number of tokens that are selected for learning in the early epochs but unlearned in the later epochs. This indicates that the model learned tokens and then memorized them over epochs, and the during-training unlearning process is essential to mitigate the memorization risks.

\begin{figure}[htp]
    \centering
    \includegraphics[width=0.7\linewidth]{figs/token-selection-dynamics.pdf}
    \caption{Token Selection Dynamics of \methodname}
    \label{fig:token-selection}
    \vspace{-4mm}
\end{figure}

\subsection{Privacy Backdoor}
To study the worst case of privacy attacks and defense effectiveness under the state-of-the-art MIA, we perform a privacy backdoor -- Precurious~\cite{precurious}. In this setup, the target model undergoes continual fine-tuning from a warm-up model. The attacker then applies a reference-based MIA that leverages the warm-up model as the attack's reference. Table~\ref{tab:backdoor} shows the language modeling and MIA performance on CCNews with GPT-2. Precurious increases the MIA AUC score by 5\%. Goldfish achieves the lowest PPL, aligning with~\citet{hans2024be}, where the Goldfish masking mechanism acts as a regularizer that potentially enhances generalization. Both DPSGD and \methodname provide strong privacy protection, with \methodname offering slightly better defense while maintaining lower perplexity than DPSGD.

% \begin{table}[h]
%     \centering
%     \begin{tabular}{c|cc|cc}
%        \multirow{2}{*}{\textbf{Method}}  & \multicolumn{2}{c}{\textbf{CCNews}} & \multicolumn{2}{c}{\textbf{Wikipedia}} \\ 
%        & \textbf{PPL} & \textbf{AUC} & \textbf{PPL} & \textbf{AUC} \\ \hline
%        \textbf{FT}        & 21.593 & 0.911 \\
%        \textbf{Goldfish}  & \textbf{21.074} & 0.886 \\
%        \textbf{DPSGD}     & 23.279 & 0.533 \\
%        \textbf{DuoLearn}  & 22.296 & \textbf{0.499} \\
%     \end{tabular}
%     \caption{Caption}
%     \label{tab:my_label}
% \end{table}

\begin{table}[h]
    \centering
    \resizebox{\columnwidth}{!}{\begin{tabular}{c|cccccc}
        \textbf{Metric} & \textbf{WU} & \textbf{FT} & \textbf{GF} & \textbf{DP} & \textbf{DuoL} \\ \hline
        \textbf{PPL} & \textit{23.318} & 21.593 & \textbf{21.074} & 23.279 & 22.296  \\
        \textbf{AUC} & \textit{0.500} & 0.911 & 0.886 & 0.533 & \textbf{0.499} \\
    \end{tabular}}
    \caption{Experimental results of privacy backdoor for GPT2 on the CC-news dataset. WU stands for the warm-up model leveraged by Precurious. GF, DP, and DuoL are abbreviations of Goldfish, DPSGD, and \methodname}
    \label{tab:backdoor}
\end{table}

% \subsubsection{Hyperparameter Study}

% \subsubsection{Full fine-tuning versus Parameter efficent fine tuning}

% \subsubsection{Extending to Vision Language Models}


