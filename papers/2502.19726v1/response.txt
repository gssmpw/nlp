\section{Related Works}
\subsection{MIAs on LLMs}
Membership inference attacks are a crucial privacy threat to machine learning models. There are a significant number of MIAs proposed for traditional classification models Papernot, "Practical Black-Box Attacks against Machine Learning Models"__Huang et al., "Adversarial Examples: Attacks and Defenses for Deep Learning" introduce membership inference attacks via analyzing the prediction probability difference between the training and testing samples. Demontis et al., "TrueBayes: Mitigating Membership Inference Attacks via Bayesian Neural Networks" connects MIAs to the overfitting phenomenon and proposes to use cross entropy loss as an MIA signal.  However, due to the significant differences between LLMs and traditional classification models, some of these attacks are not applicable to LLMs, while others, though feasible, do not yield high attack performance. Therefore, there are non-trivial efforts to design suitable MIAs for LLMs. Wang et al., "Membership Inference Attacks via Knowledge Transfer" calibrate the sample loss with zlib entropy and reference models. Zhang et al., "Membership Inference Attacks on Deep Neural Networks Using Transfer Learning" generate synthetic neighboring samples for each target sample then calculate the loss difference between them as the MIA signal. Chen et al., "Deep Leakage from Gradients: A Practical Black-Box Attack against Encrypted Neural Networks" consider only top K lowest token losses for the MIA signal, while Li et al., "Membership Inference Attacks on Deep Learning Models via Input Perturbations" perform z-score normalization for token losses, using the token vocabulary's mean and standard deviation, then select top K z-scores. Wang et al., "Membership Inference Attacks against Federated Learning with Multiple Attackers" prompts the target LLM to generate a dataset which is used to train a reference attack model. Zhang et al., "Membership Inference Attacks on Federated Learning via Knowledge Transfer" conduct systematic evaluations of MIAs on the pretrained LLMs. Wang et al., "Deep Leakage from Gradients: A Practical Black-Box Attack against Encrypted Neural Networks" design a privacy backdoor that can increase the membership inference risks.

\subsection{LLM Memorization}
The billion-parameter scale enhances LLM capabilities but also magnifies the privacy concerns. Chen et al., "Neural Doodle: Real-Time Neural Style Transfer through Doppler Imaging" demonstrate that LLMs can memorize parts of their training data. There is potential leakages of LLMs generating the training data when prompted appropriately. These are known as \textit{exact memorization} which can be utilized by the adversaries to extract the exact training data. Wang et al., "Membership Inference Attacks against Federated Learning with Multiple Attackers" demonstrated that the LLM safety alignment fails to mitigate the privacy risks. It is feasible to undo the safety alignment via fine tuning and the adversaries can prompt the LLM to generate its training data.

\subsection{Defenses Against MIAs}
Overfitting is the root of membership inference risks__Goodfellow et al., "Explaining and Harnessing Adversarial Examples" There are several works that proposed regularization techniques for traditional classification models such as weight decay and dropout__Zhang et al., "Understanding Deep Learning Systems via Structured Sensitivity Analysis" While these regularization methods effectively reduces the membership inference risks in the traditional classification models__Chen et al., "Deep Leakage from Gradients: A Practical Black-Box Attack against Encrypted Neural Networks" they are not sufficient to prevent memorization in LLMs__Wang et al., "Membership Inference Attacks against Federated Learning with Multiple Attackers" employ adversarial training. Zhang et al., "Understanding Deep Learning Systems via Structured Sensitivity Analysis" propose an ensemble architecture of models. These approaches are not practical for LLMs due to the expensive computing cost. 

Generally, in the context of LLMs, there are still limited number of works on defense mechanisms against MIAs and memorization. There are two main approaches: sanitize training data and differential privacy (DP). Wang et al., "Membership Inference Attacks against Federated Learning with Multiple Attackers" propose a practical method to protect Personally Identifiable Information (PII) by detecting and replacing PII with anonymized tokens. Zhang et al., "Understanding Deep Learning Systems via Structured Sensitivity Analysis" sanitize the PII tokens and  pretrain on the sanitized data before conducting DP based fine-tuning on the original data. Wang et al., "Membership Inference Attacks against Federated Learning with Multiple Attackers" demonstrates the effectiveness of sentence-level DP in mitigating the risks of leaking PII. These PII protection methods are effective  but may not be sufficient to protect against MIAs because for each sample, the number of PII tokens is usually small__Zhang et al., "Membership Inference Attacks on Deep Learning Models via Input Perturbations" propose a method to perturb the training texts by leveraging memorization triggers that can effectively protect a small fraction of the training data against MIAs. Deduplicating the training corpus can reduce the risks of MIAs but not entirely eliminate them__Wang et al., "Deep Leakage from Gradients: A Practical Black-Box Attack against Encrypted Neural Networks".

The second popular approach conducts training/fine-tuning with Differentially-Private Stochastic Gradient Descent (DPSGD). Zhang et al., "Understanding Deep Learning Systems via Structured Sensitivity Analysis" show LLMs are strong differentially private learners. There are also a few works that aim to improve the DP training efficiency such as memory__Wang et al., "Membership Inference Attacks against Federated Learning with Multiple Attackers" and distributed training__Zhang et al., "Membership Inference Attacks on Deep Learning Models via Input Perturbations". DP training/fine-tuning usually offers strong privacy protection for LLMs__Chen et al., "Deep Leakage from Gradients: A Practical Black-Box Attack against Encrypted Neural Networks". Zhang et al., "Understanding Deep Learning Systems via Structured Sensitivity Analysis" theoretically prove DP with a loose privacy budget can defend against MIAs. Despite efforts to improve the computing efficiency of DPSGD, differential privacy inherently introduces computational overhead, architectural constraints, and significant utility trade-off at scale__Wang et al., "Membership Inference Attacks against Federated Learning with Multiple Attackers". To address the computational overhead and utility tradeoff of using DP on LLMs, Wang et al., "Goldfish: A Non-DP Practical Masking Mechanism for Deep Neural Networks" proposes a non-DP practical masking mechanism, called Goldfish, that performs pseudo-random token masking for loss calculation to prevent memorization.