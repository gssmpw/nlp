\section{Conclusion}
We introduced \methodname, an effective training framework defending against MIAs for LLMs. The extensive experiments demonstrate its robustness in protecting privacy while maintaining strong language modeling performance across various datasets and architectures. Although our study focuses on fine-tuning due to computational constraints, \methodname can be seamlessly applied to large-scale pretraining, as done in prior selective pretraining work~\cite{lin2024not}. By categorizing tokens and treating them appropriately, \methodname opens a novel pathway for MIA defense. Future work can explore improved token selection strategies and multi-objective training approaches.