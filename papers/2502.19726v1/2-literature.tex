\section{Related Works}
\subsection{MIAs on LLMs}
Membership inference attacks are a crucial privacy threat to machine learning models. There are a significant number of MIAs proposed for traditional classification models~\cite{miasurvey}. \citet{MIAShokri2017} introduce membership inference attacks via analyzing the prediction probability difference between the training and testing samples. \citet{8429311} connects MIAs to the overfitting phenomenon and proposes to use cross entropy loss as an MIA signal.  However, due to the significant differences between LLMs and traditional classification models, some of these attacks are not applicable to LLMs, while others, though feasible, do not yield high attack performance. Therefore, there are non-trivial efforts to design suitable MIAs for LLMs. \citet{Carlini2020ExtractingTD} calibrate the sample loss with zlib entropy and reference models. \citet{neighbourattack} generate synthetic neighboring samples for each target sample then calculate the loss difference between them as the MIA signal. \citet{shi2024detecting} consider only top K lowest token losses for the MIA signal, while \citet{zhang2025mink} perform z-score normalization for token losses, using the token vocabulary's mean and standard deviation, then select top K z-scores. \citet{fu2024membership} prompts the target LLM to generate a dataset which is used to train a reference attack model. \citet{duan2024membership, puerto2025smia} conduct systematic evaluations of MIAs on the pretrained LLMs. \citet{precurious} design a privacy backdoor that can increase the membership inference risks.

\subsection{LLM Memorization}
The billion-parameter scale enhances LLM capabilities but also magnifies the privacy concerns. \citet{Carlini2020ExtractingTD, carlini2023quantifying} demonstrate that LLMs can memorize parts of their training data. There is potential leakages of LLMs generating the training data when prompted appropriately. These are known as \textit{exact memorization} which can be utilized by the adversaries to extract the exact training data. \citet{nasr2025scalable} demonstrated that the LLM safety alignment fails to mitigate the privacy risks. It is feasible to undo the safety alignment via fine tuning and the adversaries can prompt the LLM to generate its training data.

\subsection{Defenses Against MIAs}
Overfitting is the root of membership inference risks~\cite{MIAShokri2017}. There are several works that proposed regularization techniques for traditional classification models such as weight decay and dropout~\cite{dropout}. While these regularization methods effectively reduces the membership inference risks in the traditional classification models~\cite{272134}, they are not sufficient to prevent memorization in LLMs~\cite{tirumala2022memorization, lee2022deduplicating}. \citet{Miladadvereg} employ adversarial training. \citet{280000} propose an ensemble architecture of models. These approaches are not practical for LLMs due to the expensive computing cost. 

Generally, in the context of LLMs, there are still limited number of works on defense mechanisms against MIAs and memorization. There are two main approaches: sanitize training data and differential privacy (DP). \citet{p2022textanonymbench} propose a practical method to protect Personally Identifiable Information (PII) by detecting and replacing PII with anonymized tokens. \citet{shi2022just} sanitize the PII tokens and  pretrain on the sanitized data before conducting DP based fine-tuning on the original data. \citet{10179300} demonstrates the effectiveness of sentence-level DP in mitigating the risks of leaking PII. These PII protection methods are effective  but may not be sufficient to protect against MIAs because for each sample, the number of PII tokens is usually small~\cite{llmpbe}. \citet{liu2024exp} propose a method to perturb the training texts by leveraging memorization triggers that can effectively protect a small fraction of the training data against MIAs. Deduplicating the training corpus can reduce the risks of MIAs but not entirely eliminate them~\cite{kandpal2022deduplicating}.

The second popular approach conducts training/fine-tuning with Differentially-Private Stochastic Gradient Descent (DPSGD). \citet{li2022large,yu2022differentially} show LLMs are strong differentially private learners. There are also a few works that aim to improve the DP training efficiency such as memory~\cite{bu2023groupwise} and distributed training~\cite{bu2023zero}. DP training/fine-tuning usually offers strong privacy protection for LLMs~\cite{llmpbe, amit2024sok}. \citet{lowy2024dptheo} theoretically prove DP with a loose privacy budget can defend against MIAs. Despite efforts to improve the computing efficiency of DPSGD, differential privacy inherently introduces computational overhead, architectural constraints, and significant utility trade-off at scale~\cite{bu2024pretraining}. To address the computational overhead and utility tradeoff of using DP on LLMs, \citet{hans2024be} proposes a non-DP practical masking mechanism, called Goldfish, that performs pseudo-random token masking for loss calculation to prevent memorization.