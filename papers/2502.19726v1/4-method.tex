\section{Proposed Methodology -- \methodname}
\label{sec:method}
\begin{figure*}[htp]
    \centering
    \includegraphics[width=0.9\textwidth]{figs/duolearn.pdf}
    \caption{\methodname overview. First, the tokens are passed through the training LLM and reference LLM. They are then categorized into hard tokens (in green) and memorized tokens (in red). At the end, a dual-purpose loss is applied which achieves two targets: learning on the hard tokens while unlearning for the memorized tokens.}
    \label{fig:duolearn}
\end{figure*}

Motivated by the analysis, we propose \methodname -- a training framework that dynamically identifies hard tokens (those with higher calibrated losses) for learning and memorized tokens (those with strong MIA signals) for unlearning simultaneously. This way, the model learns useful information without memorizing specific training samples.

\noindent 
\textbf{Overview}. 
We assume the model trainer is the defender and the  goal is to mitigate the privacy risk of the training data in the trained model.
We further assume the trainer can get access to an auxiliary dataset for better calibrating the MIA signals, which can be a disjoint subset drawn from the same distribution of the training data. The general training process is illustrated in Figure~\ref{fig:duolearn}.
First, we train a reference model with the auxiliary dataset, which is feasible for the trainer based on previous works~\cite{lin2024not, 2022PrioritizedTraining, xie2023doremi}.
% Inspired by previous work on using reference models to speed up and improve the general performance of model training~\citet{lin2024not, 2022PrioritizedTraining, xie2023doremi}, we first train a reference model on auxiliary data that can reflect the desired data distribution. 
Then, during training of the target model, we use the token losses of the current training model calibrated by the reference model to dynamically identify hard tokens and memorized tokens in each training iteration. 
A dual-purpose loss function is used to keep the model simultaneously learning on hard and necessary tokens to enhance model utiilty while unlearning on memorized tokens to mitigate MIA risks.

\noindent \textbf{Reference Modeling}. Reference model ($\theta_{ref}$) shares an identical architecture with the training model ($\theta$). We fine-tune a reference model on a small portion of the original dataset (denoted as $\mathcal{T}_\text{aux}$) that can reflect the desired data distribution by standard causal language modeling (CLM), i.e., implementing next-token-prediction cross entropy loss ($\mathcal{L}_{CE}$):
\begin{equation*}
  \resizebox{\linewidth}{!}{$\mathcal{L}_{CE}(\theta_{ref}; \mathcal{T}_\text{aux}) = - \frac{1}{|\mathcal{T}_\text{aux}|} \sum_{t_i \in \mathcal{T}_\text{aux}} \log P(t_i| t_{<i}; \theta_{ref})$}.
\end{equation*}
% \[ \mathcal{L}_{CE}(\theta_{ref}; \mathcal{T}_\text{aux}) = - \frac{1}{|\mathcal{T}_\text{aux}|} \sum_{t_i \in \mathcal{T}_\text{aux}} \log P(t_i| t_{<i}; \theta_{ref}) \]
\noindent \textbf{Token Selection}. As shown in the previous analysis on LLM generalization by~\citet{lin2024not} and ours on membership inference risks, tokens contribute differently. Considering all tokens equally as the standard causal language modeling is not optimal since it can lead to memorization on some tokens and amplify the memorization over training epochs. \methodname defines two sets of tokens: hard tokens ($\mathcal{T}_{h}$) and memorized tokens ($\mathcal{T}_m$). Hard tokens are the tokens that the current training models ($\theta$) have difficulty predicting, while memorized tokens are the tokens that the model has already memorized. To identify these two sets of tokens, we propose a token selection mechanism based on the prediction loss of each token calibrated by the reference model. We implement the score $s(t_i)$ for each token $t_i$ which is the difference between the cross-entropy loss of the training model and the reference model, as used in previous works~\cite{lin2024not, 2022PrioritizedTraining}:
% \[ s(t_i) = \mathcal{L}_{CE}(\theta; t_i) - \mathcal{L}_{CE}(\theta_{ref}; t_i) \]
\[ s(t_i) = \log P(t_i| t_{<i}; \theta_{ref}) - \log P(t_i| t_{<i}; \theta). \]

The tokens with the highest scores are considered hard tokens $\mathcal{T}_{h}$ (highest calibrated loss), while the tokens with the lowest scores are considered memorized tokens $\mathcal{T}_m$ (lowest calibrated loss and strongest MIA signals). Let $\mathcal{T}$ be the set of all tokens in a batch. We select top $K_h$  hard tokens and bottom $K_m$ memorized tokens to form $\mathcal{T}_{h}$ and $\mathcal{T}_m$, respectively. Additionally, we introduce a threshold $\tau$ to filter out neutral tokens from $\mathcal{T}_m$ which have scores close to zero or greater than zero, as these are not considered memorized. The token selection process is formulated as follows:
\[ \mathcal{T}_{h} = \argmax_{S, |S|=K_h}\{{s(t_i) | t_i \in \mathcal{T}}\}  \]
\[ \mathcal{T}_{m} = \argmin_{S, |S| \leq K_m}\{{s(t_i) | t_i \in \mathcal{T}, s(t_i) \leq \tau}\} \]


\noindent \textbf{Dual-Purpose Loss}. We introduce a dual-purpose loss function designed to improve model performance on hard tokens while mitigating overfitting on memorized tokens. This loss function combines two components: the learning loss and the unlearning loss. The learning loss is the standard causal language modeling (CLM) loss applied to the hard tokens $\mathcal{T}_{h}$. The unlearning loss, in contrast, is the negative CLM loss applied to the memorized tokens $\mathcal{T}_{m}$, effectively performing gradient ascent. The dual-purpose loss is defined as follows, where $\alpha > 0$ is a hyper-parameter that balances the learning and unlearning losses:
\[ \mathcal{L}_{dual}(\theta) = \mathcal{L}_{CE}(\theta; \mathcal{T}_{h}) - \alpha \cdot \mathcal{L}_{CE}(\theta; \mathcal{T}_{m}). \]

% \begin{figure*}[t]
% \begin{lstlisting}[language=Python, caption=Python example, style=mystyle]
% def DuoLearn_training_step(batch, model, ref_model):
%     train_token_losses = compute_losses(batch, model)
%     ref_token_losses = compute_losses(batch, ref_model)
%     token_scores = train_token_losses - ref_token_losses
%     hard_tokens, memorized_tokens = select_tokens(token_scores)

%     learning_loss = compute_losses(hard_tokens, model)
%     unlearning_loss = -compute_losses(memorized_tokens, model)
%     loss = learning_loss + alpha * unlearning_loss
%     model.zero_grad()
%     loss.backward()
%     optimizer.step()
    
%     return loss
% \end{lstlisting}
% \end{figure*}   