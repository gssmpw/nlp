\section{Related Works}
\subsection{MIAs on LLMs}
Membership inference attacks are a crucial privacy threat to machine learning models. There are a significant number of MIAs proposed for traditional classification models____. ____ introduce membership inference attacks via analyzing the prediction probability difference between the training and testing samples. ____ connects MIAs to the overfitting phenomenon and proposes to use cross entropy loss as an MIA signal.  However, due to the significant differences between LLMs and traditional classification models, some of these attacks are not applicable to LLMs, while others, though feasible, do not yield high attack performance. Therefore, there are non-trivial efforts to design suitable MIAs for LLMs. ____ calibrate the sample loss with zlib entropy and reference models. ____ generate synthetic neighboring samples for each target sample then calculate the loss difference between them as the MIA signal. ____ consider only top K lowest token losses for the MIA signal, while ____ perform z-score normalization for token losses, using the token vocabulary's mean and standard deviation, then select top K z-scores. ____ prompts the target LLM to generate a dataset which is used to train a reference attack model. ____ conduct systematic evaluations of MIAs on the pretrained LLMs. ____ design a privacy backdoor that can increase the membership inference risks.

\subsection{LLM Memorization}
The billion-parameter scale enhances LLM capabilities but also magnifies the privacy concerns. ____ demonstrate that LLMs can memorize parts of their training data. There is potential leakages of LLMs generating the training data when prompted appropriately. These are known as \textit{exact memorization} which can be utilized by the adversaries to extract the exact training data. ____ demonstrated that the LLM safety alignment fails to mitigate the privacy risks. It is feasible to undo the safety alignment via fine tuning and the adversaries can prompt the LLM to generate its training data.

\subsection{Defenses Against MIAs}
Overfitting is the root of membership inference risks____. There are several works that proposed regularization techniques for traditional classification models such as weight decay and dropout____. While these regularization methods effectively reduces the membership inference risks in the traditional classification models____, they are not sufficient to prevent memorization in LLMs____. ____ employ adversarial training. ____ propose an ensemble architecture of models. These approaches are not practical for LLMs due to the expensive computing cost. 

Generally, in the context of LLMs, there are still limited number of works on defense mechanisms against MIAs and memorization. There are two main approaches: sanitize training data and differential privacy (DP). ____ propose a practical method to protect Personally Identifiable Information (PII) by detecting and replacing PII with anonymized tokens. ____ sanitize the PII tokens and  pretrain on the sanitized data before conducting DP based fine-tuning on the original data. ____ demonstrates the effectiveness of sentence-level DP in mitigating the risks of leaking PII. These PII protection methods are effective  but may not be sufficient to protect against MIAs because for each sample, the number of PII tokens is usually small____. ____ propose a method to perturb the training texts by leveraging memorization triggers that can effectively protect a small fraction of the training data against MIAs. Deduplicating the training corpus can reduce the risks of MIAs but not entirely eliminate them____.

The second popular approach conducts training/fine-tuning with Differentially-Private Stochastic Gradient Descent (DPSGD). ____ show LLMs are strong differentially private learners. There are also a few works that aim to improve the DP training efficiency such as memory____ and distributed training____. DP training/fine-tuning usually offers strong privacy protection for LLMs____. ____ theoretically prove DP with a loose privacy budget can defend against MIAs. Despite efforts to improve the computing efficiency of DPSGD, differential privacy inherently introduces computational overhead, architectural constraints, and significant utility trade-off at scale____. To address the computational overhead and utility tradeoff of using DP on LLMs, ____ proposes a non-DP practical masking mechanism, called Goldfish, that performs pseudo-random token masking for loss calculation to prevent memorization.