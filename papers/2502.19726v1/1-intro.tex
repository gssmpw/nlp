\section{Introduction}
% P1: introduce problem: membership inference attack on LLMs

% TODO: write about MIAs for traditional models as well, to connect with the second paragraph.
Large language models (LLMs) have become the foundation of modern natural language processing with a wide range of applications in various domains~\cite{llmevalsurvey}. The rapidly increasing deployment of LLMs raises serious concerns about the data privacy~\cite{YAO2024100211}. LLMs have been shown to memorize the training data which can be later extracted by adversaries~\cite{carlini2023quantifying}. 
Membership inference attacks (MIAs)~\cite{MIAShokri2017, li2024privacyllms} aim to infer whether a sample is included in a model's training data, serving as the foundation of broader privacy threats~\cite{carlini2021extracting}.

% P2: Start from MIAs for traditional classification problem, => clarify the difference of LLMs, => that's why new MIAs proposed => justify why new defense mechanisms for LLMs are needed regarding the defense mechanisms for traditional models.
Due to the importance of understanding and mitigating MIAs, a significant amount of research has been conducted to design MIA defenses~\cite{miasurvey}. 
% One the one hand, most defenses focus on general machine learning model with classification task and do not account for the sequential nature of text data.
However, most defenses focus on general machine learning models for classification tasks and do not account for the sequential nature of text data, while advanced MIAs for LLMs have leveraged such property.
For example, the series of Min-K works~\cite{zhang2025mink, shi2024detecting} use the token-level loss on outlier tokens and significantly enhance MIAs for LLMs.
Thus, conventional data sanitization or regularization techniques have limited defense effectiveness~\cite{kandpal2022deduplicating, precurious}.
And even the classic differentially private (DP) training algorithm~\cite{abadi2016deep} provides a strong defense, this approach comes at the inevitable cost of increased computation and reduced utility~\cite{li2022does, bu2023groupwise}, which may not be desirable when the model trainer serves as the defender.

% However, LLMs differ significantly from these models, leading to fundamental differences in how MIAs operate. In traditional classification tasks,  each sample corresponds to a single prediction. In contrast, LLMs generate a sequence of tokens that requires multiple predictions per input. This sequential nature introduces new challenges and opportunities for membership inference attacks in LLMs~\cite{llmpbe}. Therefore, previous MIAs designed for traditional models~\cite{MIAShokri2017, 8429311} may not be directly applicable to LLMs. This leads to non-trivial efforts of designing new MIAs for LLMs~\cite{Carlini2020ExtractingTD, neighbourattack, shi2024detecting, zhang2025mink, fu2024membership}. Consequently, the previous defense mechanisms for traditional models may not be effective for LLMs due to the differences in the underlying models~\cite{llmpbe, hans2024be}. Therefore, new defense mechanisms are needed to mitigate the risks of MIAs for LLMs. However, there are still very limited works on defending MIAs in the context of LLMs. The main approaches include data sanitization and differential privacy (DP). Data sanitization can reduce but is not sufficient to entirely eliminate MIAs~\cite{kandpal2022deduplicating}. DP training offers stronger theoretical guarantees against MIAs but comes at a significant computational and utility cost, particularly at the large scale~\cite{bu2024pretraining}.

% P3: our analysis => approach: a defense mechanism for membership inference attack on LLMs
In this paper, we propose a defense mechanism for membership inference attacks on LLMs -- \methodname. 
A recent study~\cite{lin2024not} reveals that using a carefully selected subset of tokens during training can match or even surpass the performance of using all tokens in language modeling. In the meantime, MIAs mainly exploit loss-based signals associated with a sample~\cite{neighbourattack, Carlini2020ExtractingTD}. We observe that during training, some tokens carry stronger MIA signals and make the sample more vulnerable to MIAs. Thus, we leverage such token sequence nature of LLMs and propose a dynamic token selection strategy during training to proactively identify and categorize tokens into hard tokens (those with high losses)  and memorized tokens (those with strong signals for MIA risks). Accordingly, we design a dual-objective loss function that performs learning via gradient descent on the hard tokens and unlearning via gradient ascent on the memorized tokens simultaneously in one backward pass, which makes the model learn useful information but not memorize specific training samples.
% Our defense leverages a novel aspect of that is unique to LLM training regarding token selection. We show that with the proposed token selection strategy and a suitable treatment for token types, we can effectively mitigate the risks of MIAs on LLMs. We evaluate our defense mechanism on various LLM architectures and datasets to demonstrate its effectiveness. Due to the limited compute resources, we focus on fine tuning the pretrained models. Additionally, we narrow down our experiments on decoder-only architectures and autoregressive language modelling, which have been the dominant approach for LLMs~\cite{zhao2024sllms}. 
Our contributions can be summarized as follows:
\begin{itemize}
    \item We propose a dynamic token selection strategy that identifies hard tokens and memorized tokens during training, which provides insights for investigating language modeling and memorization.
    \item We propose a simple but effective dual-objective training to perform learning over hard tokens and unlearning over memorized tokens, for mitigating privacy risk while maintaining model utility with small computing cost.
    % \item We propose a novel paradigm for mitigating MIAs on LLMs via token selection strategies. Our method categorizes tokens and treats them in a manner that effectively mitigates the risks of MIAs. 
    \item We empirically demonstrate the effectiveness of the proposed defense mechanism across various LLM architectures and datasets. Our results show that our defense mechanism can provide robust privacy protection against MIAs with minimal degradation on language modeling performance.
\end{itemize}
