\section{Related Work}
\label{sec:related_work}

\begin{figure*}
    \centering
    \includegraphics[width=\linewidth,trim=50 150 50 150, clip]{framework.png}
    \vspace{-1.3cm}
    \caption{Overview of our SATA Framework. Dotted lines indicate parts used only during training, while solid lines those used during both training and deployment. Our framework is mainly composed of 1) a Biomechanical Model (Orange) to ensure the generation of smooth, practical actuator commands \(\tau\) while informing the policy of the current actuator state, and 2) a Growth Model (Green) to help the neural network train a more robust and generalizable policy by gradually adapting rewards \(r_\textit{growth}\), control frequency \(f_\textit{policy}\), and torque limits \(\tau_\textit{limit}\) during training. Finally, we train a state estimator for real world deployment using simulated IMU data and temporal proprioception observations (Grey), to help condition our policy on the (estimated) current robot velocity.}
    \label{fig:framework}
\end{figure*}

Quadruped controllers have greatly benefited from the development of deep reinforcement learning (DRL) in recent years, allowing agents to learn impressive controllers____ that would otherwise require the design and solving of non-linear optimization problems, which often involves approximations that cannot be neglected in real-world settings____.
However, the problem of sampling efficiency still hinders its application.
Early approaches employed by ____ combined imitation learning with DRL to solve this problem, demonstrating gaits similar to the training set while adapting to different terrain or disturbances.
This approach allows for the training of adaptive quadruped controllers in highly dynamic acrobatic tasks, but its performance is limited by the dataset quality and robot deployment remains nontrivial.
Addressing these problems, Hwangbo et al. ____ proposed a method to train deployable policies in simulation by introducing an \textit{actuator network} and domain randomization.
Their learned policies can yield different gaits or recover from fallen positions.
To help the robot travel in challenging terrains, ____ introduced a teacher-student framework for quadrupedal robots, allowing them to traverse complex terrains without any visual feedback.
To aid locomotion with environmental information,____ built upon____ by integrating additional sensors and employing an attention-based recurrent encoder to fuse proprioceptive and exteroceptive inputs. This resulted in a robust and fast legged motion controller for navigating challenging terrains.
While exteroceptive inputs can enable more informed decision-making, a highly compliant and robust base policy—capable of operating effectively without vision—remains crucial for reliable real-world deployment. Moreover, reliance on exteroception introduces additional challenges, such as the sim-to-real gap, where sensor noise, latency, and real-world variations degrade performance. 

Learning-based controllers typically use position-based action spaces, where the policy directly outputs position commands for the actuators.
These commands are subsequently converted to torque using a low-level (e.g., PD)  controller during training____.
While such low-level controllers facilitate early-stage exploration for reinforcement learning policies, extensive parameter tuning is often required to ensure successful deployment____.
Moreover, position control tends to treat the robot as a rigid system, which can result in significant joint or structural stress when operating in uncertain environments (e.g. slipping, collisions), thereby increasing the risk of system damage____. 

In contrast, torque-based policies, where the policy directly outputs motor torques, eliminate the need for tuning low-level controller parameters.
They also take advantage of the inherent compliance of torque control to reduce structural stress and impact forces, enhancing the safety of human-robot interactions____.
There, Chen et al.____ achieved the first successful sim-to-real transfer of end-to-end torque control for quadrupedal locomotion, enabling RL policies to directly predict joint torques at high frequencies.
However, Kim et al.____ highlighted that torque-based state spaces exhibit significant non-linearity and the controllers need to operate at much higher frequencies, which impairs exploration efficiency during early-stage training.

With the advancement of parallel simulation techniques and high-performance computing____, massively parallel RL environments have significantly improved sampling efficiency, reducing the training duration to mere minutes____.
This allows training of more complicated frameworks that can be deployed on-robot easily____.
To enhance performance, researchers have explored the integration of learning-based approaches with traditional control techniques. For instance, ____ proposed RL2AC, which combines reinforcement learning policy with an adaptive torque compensator to mitigate external disturbances and model mismatches caused by the sim-to-real gap.
Similar studies have also demonstrated that integrating the adaptability of learning-based methods with the robustness of traditional models can significantly improve the locomotion performance of quadruped robots in complex environments ____.

The combination of bio-inspired control and reinforcement learning is another major direction to address the adaptability challenges of legged robots in dynamic and complex environments____.
This hybrid approach integrates structured prior knowledge from bio-inspired control within reinforcement learning to optimize high-level strategies and key parameters, enabling efficient robot control____. For instance, Margolis et al.____ achieved agile locomotion, including walking, trotting, and pronking, by mimicking natural gait patterns through carefully designed rewards.
Similarly, ____ employed imitation learning to teach legged robots agile skills by mimicking real-world animals, while ____ leveraged deep reinforcement learning to minimize energy consumption and achieve emergent gaits.
Inspired by Central Pattern Generators (CPGs) observed in animals, Bellegarda et al.____ combined CPGs with a DRL framework to generate robust and omnidirectional locomotion.
Building on this foundation, ____ introduced the learning-based hierarchical control framework, utilizing a spinal policy to adjust CPG frequency and amplitude for rhythmic movement, and a descending modulation policy to adaptively modify rhythmic outputs for precise control on challenging terrains.
Similar to this idea, various hierarchical methods have also been proposed for legged controllers____. 
Although these studies have achieved promising results, most of them focus on mimicking animal behaviors or neural structures. Relatively limited research has explored the effects of biomechanical properties and growth processes on the development of locomotion skills in animals, let alone incorporating them into the learning framework for legged robots.