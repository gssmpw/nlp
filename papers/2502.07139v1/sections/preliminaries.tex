\input{sections/fig-teaser}
\section{Preliminaries}

This section introduces key concepts and background knowledge for the proposed model, including LLMs and TPPs.
\subsection{Autoregressive Language Model}
State-of-the-art LLMs, such as GPT-4~\citep{achiam2023gpt}, Qwen2.5~\citep{yang2024qwen25} and Gemini 1.5~\citep{team2024gemini}, are built on causal Transformer architectures that process and generate content as discrete tokens. While these models are designed to process textual data via tokenization, handling continuous multi-modal inputs~\citep{hurst2024gpt,yao2024minicpm,fu2025vita}, e.g., images, audio, video, presents unique challenges. To address this, modern architectures typically employ specialized encoders that discretize continuous signals into tokens~\citep{liu2024visual,wang2024qwen2}. For generation in these continuous domains, diffusion models may be used to decode predicted token embeddings back to their original modal representations~\citep{bao2023one,tang2024any}. This encoder-decoder paradigm has become instrumental in bridging the gap between discrete token-based processing and continuous multi-modal data. In this work, we focus on unifying TPPs modality with LLMs, where the primary challenges lie in processing and generating continuous event timestamps. 


\subsection{Temporal Point Processes}
TPPs~\citep{Daley2008} are widely used to model the occurrence of events in continuous time. A marked TPP typically associates each event with both an event timestamp $t$ and an event type $e$. 
Mathematically, a realization of a marked TPP is a sequence of events $\{(t_i,e_i)\}_{i=1}^N$ over an observation window $[0,T]$, where $N$ is random and $e_i$ belongs to a discrete set of event type $\mathcal{E}=\{1,2,\cdots,E\}$. The conditional intensity function is written as: 
\begin{equation*}
    \lambda^*(t,e) = \underset{\delta_t \rightarrow 0}{\lim}\frac{P(\text{event with type $e$ in}[t,t+\delta_t)\mid \mathcal{H}_{t}^-)}{\delta_t}.
\end{equation*}
The conditional intensity function specifies the expected number of events occurring within an infinitesimal interval $[t,t+\delta_t)$, given the past history $\mathcal{H}_{t}^-$ up to but not including $t$. 
$*$ indicates the conditioning on history. 
To fit a TPP model to observed data, the log-likelihood function is: 
\begin{equation}~\label{eq:loglikelihood}
    \mathcal{L} = \sum^N_{i=1}\log \lambda^*(t_i,e_i)-\int^T_0 \sum_{e\in \mathcal{E}} \lambda^*(t, e)dt. 
\end{equation}

To capture complex event dynamics, neural networks have been introduced to parametrize $\lambda^*(t,e)$ (\Cref{sec:related_work}). This allows for the direct learning of temporal dependencies and event type distributions from the data. In deep TPPs, an embedding layer is introduced to map each event $(t_i, e_i)$ to a dense vector $\mathbf{z}_i \in \mathbb{R}^D$. This embedding encodes both temporal and event type information, serving as the foundation for modeling sequential dependencies. 
By employing an RNN or Transformer, we can model the dependency by summarizing the embeddings of observed events into a history representation \(\mathbf{h}_i \in \mathbb{R}^M\) in a recurrent or autoregressive manner. 
Subsequently, the next event time $t_{i+1}$ and type $e_{i+1}$ are sampled from the conditional distribution $P(t_{i+1},e_{i+1}\mid \mathbf{h}_i)$. 




