\begin{table*}[!tbp]
    \centering
    \caption{Ablation study of the impact of tokenization approaches, training strategies, and LLM sizes. Results reported in terms of RMSE and ACC.} 
    \label{tab:ablation-training}
    \begin{tabular}{ccccc}
    \toprule
    \multirow{2}{*}{Models} & \multicolumn{4}{c}{RMSE$(\downarrow)$ / ACC$(\uparrow)$} \\
    \cmidrule{2-5}
    & \textit{Retweet} & \textit{Stackoverflow} & \textit{Taobao} & \textit{Taxi} \\
    \midrule
    Language-TPP-1.5B & 18.3 / 56.4 & \textbf{1.10} / 44.1 & 0.26 / 58.2 & 0.33 / 90.4 \\
    Language-TPP-0.5B & \textbf{18.1} / \textbf{59.7} & 1.12 / \textbf{45.5} & \textbf{0.21} / \textbf{59.7} & \textbf{0.32} / \textbf{90.5} \\
    w/o Stage 1 training & 19.2 / 58.5 & 1.20 / 44.3 & 0.28 / 57.8 & 0.34 / 89.8 \\
    w/o byte tokens & 21.8 / 57.4 & 1.34 / 44.1 & 0.35 / 59.4 & 0.34 / 90.4 \\
    \bottomrule
    \end{tabular}
    \end{table*}
