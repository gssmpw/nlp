\section{Experiments}
To evaluate the effectiveness of Language-TPP, we conduct extensive experiments comparing against several strong baselines on real-world TPP datasets. 
We also examine the impact of different components of the proposed model and training protocol through an ablation study. 
\input{sections/tab-dataset}


\subsection{Baselines}
(1) \textbf{Neural Hawkes Process (NHP)}~\citep{mei2017neural}:
NHP employs a continuous-time LSTM to encode the temporal and type information of historical events. The resulting history embedding is used to model the conditional intensity function. This model cannot handle event descriptions or generate event descriptions. 

(2) \textbf{Self-Attentive Hawkes Process (SAHP)}~\citep{zhang2020self}:
SAHP encodes the temporal and type information of historical events, models long-range dependencies through self-attention mechanism, and uses the history embedding to model the conditional intensity function. This model cannot handle event descriptions or generate event descriptions. 

(3) \textbf{Transformer Hawkes Process (THP)}~\citep{zuo2020transformer}: THP is a contemporaneous work of SAHP, and their fundamental ideas are similar. Both use self-attention to derive the history embedding, which is then used to model the conditional intensity function. The two models differ only in some implementation details. This model also cannot handle event descriptions or generate event descriptions. 

(4) \textbf{GPT-3.5-turbo enhanced Attentive Neural Hawkes Process (ANHP-G3.5)}~\citep{shi2024language}: the best performing baseline in their proposed LAMP, a method that leverages pre-trained LLMs for abductive reasoning to improve event prediction. 
This model can handle historical event descriptions but cannot generate future event descriptions. 

(5) \textbf{Qwen2.5-0.5B}~\citep{yang2024qwen2}:
To the best of our knowledge, no existing TPPs baseline can generate future event descriptions.
To evaluate the quality of event description generation, we compare our method with the vanilla language model Qwen2.5-0.5B, fine-tuned on the same dataset without temporal information. 

\input{sections/fig-nll}
\input{sections/tab-results}

\subsection{Datasets}
\label{ssec:datasets}
(1) \textbf{Retweet}~\citep{zhou2013learning}: A dataset of time-stamped user retweet event sequences. Events are categorized into three types based on user follower counts: \textit{small} ($< 120$ followers), \textit{medium} ($120-1363$ followers), and \textit{large} ($> 1363$ followers).

(2) \textbf{Stackoverflow}~\citep{jure2014snap}: Two years of user award records from a question-answering website, where each sequence represents events of granting badges to a user. There are in total $22$ different badges. 

(3) \textbf{Taobao}~\citep{xue2022hypro}: Time-stamped user click behaviors on Taobao e-commerce platform, collected over a nine-day period. Events represent item clicks, with types corresponding to item categories.

(4) \textbf{Taxi}~\citep{whong2014foiling}: Time-stamped taxi pick-up and drop-off events across the five boroughs of New York City. Each combination (borough, pick-up or drop-off) defines an event type, resulting in 10 distinct event types. 

(5) \textbf{Amazon Review}~\citep{ni2019justifying}: A dataset of product reviews from Amazon, containing sequences from the 2,500 most active users. Events represent product reviews, with types corresponding to product categories. The $23$ most frequent categories were preserved as distinct types, with remaining categories merged into one. 
Each event includes a text description containing the review content. 

It is worth noting that among the five datasets, the first four are traditional TPP datasets that include only event times and types, while \textit{Amazon Review} is the only dataset containing textual event descriptions. Therefore, we use this dataset for event description evaluations.
The statistics for all datasets are summarized in \cref{tab:dataset_stats}, with detailed data splits described in \cref{ap:datasets}. 
 

\subsection{Metrics}
We employ the following metrics across different TPP tasks: 

(1) For event time prediction, we use root-mean squared error (\textbf{RMSE}) to measure the temporal prediction accuracy, i.e., $\sqrt{\sum_{i=1}^N(\tau_i - \hat{\tau_i})^2/N}$, $\hat{\tau}_i$ is the predicted time interval. 

(2) For event type prediction, we report the prediction accuracy (\textbf{ACC}), i.e., $\sum_{i=1}^N \mathbbm{1}_{e_i=\hat{e}_i}/N$, where $\hat{e}_i$ is the predicted event type. 

(3) We also evaluate the log-likelihood (\textbf{TLL}) on test sets to characterize their goodness-of-fit performance. 

(4) We evaluate the quality of generated event descriptions as a standard NLP task, employing \textbf{ROUGE-L} scores~\citep{lin2004rouge}. These scores assess the overlap between model-generated and ground-truth text based on $n$-grams, providing a comprehensive measure of both precision and recall. 

(5) We analyze the sentiment of generated descriptions using \textbf{VADER} sentiment scores~\citep{hutto2014vader}, which provide compound polarity scores ranging from $-1$ (most negative) to $+1$ (most positive).


\input{sections/fig-rouge}



\subsection{Experimental Setup}
Our proposed model is built on top of Qwen2.5-0.5B~\citep{yang2024qwen25}.
We utilize the base model variant rather than its instruction-tuned version, as our implementation employs a different prompting template from the standard chat template. We use a single NVIDIA A40 for model training and experiments. At each stage, we train the model for $5$ epochs with learning rate $1e^{-4}$ and select checkpoints achieving best validation losses. Details about hyperparameters used in training can be found in~\cref{sssec:hyper}. We sample $50$k prompt-response pairs for next-event fine-tuning (stage 2). For intensity prediction training (stage 3), the non-event integral term in the log-ikelihood is handled using Monte Carlo integration with $10$ samples per time interval, applied consistently across all models. 
The baseline models are implemented using the open source frameworks, including EasyTPP~\citep{xue2023easytpp} and LAMP~\citep{shi2024language} with default configurations. We reuse partial experimental results from~\citep{xue2023easytpp,shi2024language}. 


\subsection{Results and Analysis}

\textbf{Goodness-of-fit comparison:} We first examine the TLL of Language-TPP compared to baselines in~\cref{fig:nll}. The experimental results demonstrate the consistent superior performance of Language-TPP across four datasets. Language-TPP achieves the highest TLL in all cases, marked with stars as the best performer, with particularly notable improvements on the Taxi dataset where it reaches positive TLL values compared to baseline models. Even in scenarios with tighter competition, such as the Retweet dataset, Language-TPP maintains its edge with a TLL of $-3.51$ while other models show lower values. The consistent improvements across different domains suggest that the proposed model offers robust and generalizable capabilities in modeling TPPs, effectively capturing the underlying dynamics of various real-world temporal phenomena. 
\input{sections/tab-ablation.tex}

\textbf{Performance on type-marked TPP:}
In this experiment, we study the conventional TPP tasks on type-marked TPPs in terms of event time prediction and type prediction. 
\Cref{tab:main_results} presents the results for event prediction tasks, including time and type predictions. Language-TPP demonstrates superior or competitive performance on both RMSE and ACC metrics. 
For event time prediction, Language-TPP shows notable improvements on RMSE across most datasets, with particularly significant reductions in \textit{Retweet} ($18.1$ versus baseline ranges of $21.8$--$25.3$), \textit{Stackoverflow} ($1.12$ versus $1.37$--$1.38$), and \textit{Taobao} ($0.21$ versus $0.34$--$0.55$). 
For event type prediction, Language-TPP achieves the highest ACC on \textit{Retweet} ($59.7\%$), \textit{Stackoverflow} ($45.5\%$) and \textit{Taobao} ($59.7\%$), with a tight yet consistent improvements. While Language-TPP shows slightly lower ACC on the \textit{Taxi} dataset ($90.5\%$) compared to NHP's $91.5\%$, it maintains competitive performance. 
These results suggest that Language-TPP provides robust improvements in temporal prediction while maintaining or enhancing event type prediction capabilities across diverse real-world datasets. 


\textbf{Performance on description-marked TPP:}
In this section, we design a new type of experiment for description-marked TPPs: we use Language-TPP to process the time, type, and description of historical events, and attempt to predict the time, type, and description of future events. 
We present results on the \textit{Amazon Review} dataset in~\cref{fig:rouge} where each event is marked with a product review. Specifically, we use the summary of the review as the textual description in the experiments. 
We first investigate the prediction performance in~\cref{fig:rouge_a} where we compare Language-TPP against the recent LLM-augmented model ANHP-G3.5~\citep{shi2024language}. The results show that Language-TPP performs significantly better than the baseline model, achieving an RMSE of $106.03$ compared to $130.00$ for ANHP-G3.5. 
In particular, we also present RMSE for Language-TPP trained without event description, which achieves $118.34$, demonstrating that incorporating event description enhances the model's temporal prediction capability. 

In~\cref{fig:rouge_b}, we study a novel event description generation task, where we evaluate Language-TPP's ability to generate review summaries using the ROUGE-L metric. 
Language-TPP achieves a ROUGE-L score of $24.78$, surpassing the baseline Qwen2.5-0.5B that is fine-tuned on the same dataset without temporal information, which scores $22.60$. 
This improvement suggests that jointly learning temporal dynamics and textual information leads to better quality in generated review summaries.

To further evaluate the quality of generated event descriptions, we analyze their sentiment polarity distributions in \cref{fig:rouge_c} using a rule-based sentiment analyzer~\citep{hutto2014vader}. The results reveal a bimodal distribution in all three cases, with prominent peaks at neutral and positive sentiments. While Qwen2.5-0.5B shows a notably lower peak at neutral sentiment and overestimates positive sentiments, Language-TPP demonstrates better alignment with the ground truth distribution. This suggests that the natural sentiment patterns present in the review data is preserved better with temporal dynamics.











\subsection{Ablation Study}
\label{ssec:ablation}
In this section, we present an ablation study on Language-TPP by removing or altering key components and settings, as shown in~\Cref{tab:ablation-training}. 
We analyze the impact of tokenization approaches, training strategies, and LLM sizes. 

\textbf{Tokenization approaches:} We first study the contribution of byte-tokens to model performance. 
In this experiment, we replace temporal byte tokens with standard tokenization, where time intervals are rounded to three decimal places and represented as strings. 
All other configurations, including prompting templates and hyperparameters, remain constant. When using standard string tokenization, we observe consistent performance degradation across all datasets. Most notably, on the Retweet dataset, RMSE increased significantly from $18.1$ to $21.8$, while ACC dropped by $2.26\%$. This demonstrates that byte-token representation enables more precise temporal pattern learning compared to standard number string tokenization. 

\textbf{Training strategies:} 
Given that our goal is to predict future events based on observed event sequences --- a task directly addressed in next-event fine-tuning (stage 2) --- a natural question arises: is stage 1 pre-training necessary? To answer this, we conduct an ablation study where we maintain identical model architectures and hyperparameters while varying the training procedure. The base model (w/o Stage 1 training) is directly fine-tuned on downstream tasks without continued pre-training on temporal sequences.
Comparing this to the full model with all training stages, we observe consistent improvements across all datasets. The addition of Stage 1 training leads to substantial improvements, particularly in the Retweet dataset (RMSE decrease from $19.2$ to $18.1$) and Taobao dataset (RMSE decrease from $0.28$ to $0.21$). These results validate the importance of our staged training approach in enhancing the model's temporal understanding. 

\textbf{LLM sizes:} We experiment with two model sizes: 0.5B and 1.5B parameters. Interestingly, the 0.5B model achieves the best performance across most metrics and datasets, outperforming its larger counterpart. For instance, on the Taobao dataset, the 0.5B model achieves an RMSE of $0.21$ compared to $0.26$ for the 1.5B model.
This counter-intuitive result might be attributed to the mismatch between model capacity and dataset size. While larger language models excel at general language tasks, the relatively limited size of TPP datasets may not provide sufficient supervision for effectively adapting larger models to this specialized temporal modeling task. We hypothesize that smaller models might achieve better parameter efficiency in learning task-specific patterns from limited data while maintaining good generalization capability.

These ablation studies demonstrate that our design choices, particularly the byte-token representation and staged training strategy, are crucial for the model's performance.
We also show that relatively small model architectures are sufficiently effective for TPP modeling tasks.



