\section{Language Modeling with TPP}
In this section, we first introduce our model, Language-TPP, which bridges TPPs with natural language, along with the data preprocessing process. Then, we elaborate on the methodology for model inference and training. 

\subsection{Language-TPP}
We consider a more general event sequence dataset that includes descriptive information about events, represented as \(\{(t_i, e_i, m_i)\}_{i=1}^N\), where \(t_i\) denotes the event timestamp, \(e_i\) represents the event type, and \(m_i\) is the textual description of the event. 
Our goal is to model the dependencies in such an event sequence and predict the times, types, and descriptions of future events. 


In this work, we adopt a sequence-to-sequence model. Specifically, as depicted in~\cref{fig:architecture}, the model backbone is a causal decoder-only Transformer based on Qwen2.5~\citep{yang2024qwen2}. 
We convert each event $(t_i, e_i, m_i)$ into a sequence of tokens. 
In simple terms, for the event type \(e_i\) and description \(m_i\), we employ a built-in language tokenizer; for the event timestamp \(t_i\), we design a specialized byte-level tokenization strategy. 
We explain the tokenization in detail as follows. 

\textbf{Event description tokenization.}
The event description $m_i$ is in natural language, so we can directly use a built-in language tokenizer for tokenization. 

\textbf{Event type tokenization.}
If the event type in the dataset is represented in natural language, we can directly use a built-in tokenizer.  
If the event type is represented as an index, we attempt to recover its text label from the index. For example, \(e_i = 0\) represents \textit{Women Shoes} in the \textit{Amazon Review} dataset~\citep{ni2019justifying}.  
If the event type is represented as an index and no corresponding text label is provided, we directly use the index as the text label. 

\input{sections/fig-template.tex}

\textbf{Event timestamp tokenization.}
For event timestamps, a naive approach is to directly treat the timestamp \(t_i\) as text. However, this method requires a large number of tokens to represent a single timestamp, significantly reducing the model's processing efficiency.  
Inspired by prior works in byte-to-byte models~\citep{xue2022byt5,pagnoni2024byte}, we propose a specialized byte-level tokenization strategy. 
Specifically, we augment the vocabulary of the Qwen2.5 model with $256$ new special byte tokens, ``$<|byte\_x|>$'' $\forall x \in \{0,1,\cdots,255\}$, representing all unique values of a single byte. 
We can then parse a 32-bit floating point precision number (timestamp) into $4$ byte tokens\footnote{In implementation, we do not directly perform byte tokenization on timestamps but instead on event intervals, which facilitates subsequent time prediction tasks.}. 
This is more token-efficient compared to directly tokenizing the timestamp as text. 
For example, as shown in~\cref{fig:temporal_tokenization}, it takes $11$ tokens to represent ``0.075999237" using the default Qwen2.5 tokenizer whereas it only takes $4$ byte tokens using our approach, i.e., ``$<|byte\_125|><|byte\_165|><|byte\_155|><|byte\_61|>$''. 

\textbf{Event template.}
Eventually, we concatenate all encoded components within a predefined textual template as demonstrated in~\cref{fig:template} where each event is surrounded by special tokens. Specifically, we use ``$<|start\_of\_event|>$'' and ``$<|end\_of\_event|>$'' to denote the start and end of an event, respectively. We use ``$<|description\_prefix|>$'', ``$<|type\_prefix|>$'' and ``$<|time\_prefix|>$'' to prepend the event description, type and temporal byte tokens. 
Please refer to~\cref{sec:prompt}
for details about the templates and samples of generated prompts from datasets. 

\subsection{Inference}
Language-TPP can be applied to various downstream tasks, including event time prediction, type prediction, intensity prediction, and description generation.
We first define a set of task tokens, ``$<|description\_prediction|>$'', ``$<|type\_prediction|>$'' and ``$<|time\_prediction|>$''.
When conducting a specific downstream task, we concatenate the corresponding task token to the end of the token sequence. 
We then autoregressively predict the next tokens until the end-of-sequence token is reached. 
We apply a task-specific decoding strategy to obtain the desired output. 

\textbf{Event time prediction}: We obtain the predicted next event interval $\tau_{i+1}$ by decoding the temporal byte tokens back to a float pointing number. We then obtain the next event time, $t_{i+1} = t_i + \tau_{i+1}$. 

\textbf{Event type prediction} and \textbf{description prediction}: 
    We obtain the desired textual information by decoding the generated tokens back into natural language, as done in LLMs. 

\textbf{Event intensity prediction}: We take the hidden state output by LLM corresponding to the last token as \(\mathbf{h}(t_i)\). Similar to prior works, we define the event intensity function as: 
    \begin{equation}
        \lambda^*(t,e) = f_e(\alpha_e \frac{t-t_i}{t_i} + \mathbf{w}_e^\top \mathbf{h}(t_i) + b_e), 
    \end{equation}
    where $t \in (t_i, t_{i+1}]$ and $f_e(x)=\beta_e \log (1 + e^{x/\beta_e})$ is the softplus function with parameter $\beta_e$.


\subsection{Training}
We collect several widely adopted public TPP datasets for training the model. We follow the standard training precedure of GPT-like LLMs~\citep{openai2022}, and design our training protocol as three stages. 

\textbf{Stage 1: Event sequence continued pre-training.} We use the fixed template in~\cref{fig:template} to convert the event sequence to the token sequence $(x_1,x_2,\ldots,x_L)$.  
At this stage, we conduct a continued pre-training of the LLM backbone on the token sequence with the next-token prediction loss: 
\begin{equation}
    \mathcal{L}_{\text{stage1}}(\theta) = - \frac{1}{L} \sum_{l=1}^{L} \log P_\theta(x_l | x_{<l}). 
\end{equation}

\textbf{Stage 2: Next-event fine-tuning.} We augment the model with capabilities to conduct downstream tasks on TPPs. 
We generate training samples by randomly sampling a segment of the event sequence as the prompt \((p_1, p_2, \ldots)\), with the corresponding next event as the response \((r_1, \ldots, r_R)\). This approach constructs prompt-response pairs for event time prediction, type prediction, and description generation. 
During training, we only compute the next-token prediction loss on the response: 
\begin{equation}
    \mathcal{L}_{\text{stage2}}(\theta) = - \frac{1}{R} \sum_{l=1}^{R} \log P_\theta(r_l | p_1,p_2,\ldots, r_{<l}). 
\end{equation} 

\textbf{Stage 3: Event intensity alignment.} At this stage, we freeze the LLM and only fine-tune the intensity projection layer using the loss of log-likelihood in~\cref{eq:loglikelihood}. 

It is worth noting that, unlike common TPP training approaches, which involve training on a specific dataset and then testing on the test set of the same dataset, we merge all datasets into a single combined dataset for training and then evaluate the model separately on each individual test set. 



