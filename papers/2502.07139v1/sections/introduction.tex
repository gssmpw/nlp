\section{Introduction}

Temporal Point Processes (TPPs) provide a statistical framework for modeling sequences of events occurring in continuous time. Traditional TPP models have been proven effective in capturing temporal dynamics and event types across diverse applications, including online information diffusion~\citep{farajtabar2015coevolve,Mishra2016FeaturePrediction}, seismology~\citep{hawkes1971spectra}, neural science~\citep{johnson1996point} and finance~\citep{bacry2015hawkes}. However, these models have primarily focused on temporal and categorical aspects while the rich multi-modal information inherent in real-world events have been under-explored. 
Many events are accompanied by textual information that provides essential context beyond timestamps and event types. For example, product reviews in the Amazon user review dataset~\citep{ni2019justifying}, or accident descriptions in traffic incident datasets~\citep{zhou2020fast}. The ability to model and, more importantly, generate such multi-modal event description represents a significant gap in previous research within the TPP domain. 

Meanwhile, Large Language Models (LLMs) have achieved remarkable success in understanding and generating textual modality across numerous domains~\citep{jin2023time,m2024augmenting,xin2024bioinformatics}. This advancement allows to develop a unified framework between TPPs and LLMs that jointly models both temporal dynamics and textual information in event sequences. 
In this paper, we aim to address the following two open questions. 

While prior works have successfully incorporated self-attention mechanism into TPP models following modern LLM practices~\citep{zuo2020transformer,zhang2020self,mei2021transformer}, they rely on TPP-specific encoding strategies, including temporal positional encoding for event times and randomly initialized embeddings for event types. This raises the first question: \textbf{how can we coherently integrate standard LLM architectures with TPPs?} We address this by modeling event types and descriptions as textual information and introducing a novel temporal encoding approach using specialized byte-tokens for event time intervals as shown in~\Cref{fig:temporal_tokenization}. By converting continuous time intervals to discrete byte-tokens, we can utilize a text template to combine all event information for prompting the LLM. We adapt Qwen2.5~\citep{yang2024qwen25}, a widely used open-source LLM, for this framework. This approach enables straightforward encoding and decoding of event times, types, and descriptions through a language tokenizer. 
    
Recent work, LAMP~\citep{shi2024language}, has demonstrated performance improvements through LLM-based event information reasoning, highlighting the value of textual description information. This leads to the second question: \textbf{what are the benefits of unifying temporal and textual modalities in a unified framework?} We investigate this through two categories of tasks: conventional TPP tasks and LLM-oriented tasks, specifically event description generation. For TPP tasks --- including next event time prediction, type prediction, and intensity prediction --- we demonstrate state-of-the-art performance through extensive experiments on real-world datasets, comparing against strong baseline models. Notably, on a dataset containing textual event description, our unified framework outperforms LAMP. Furthermore, we find that augmenting temporal information improves event description generation compared to a fine-tuned LLM without temporal information, indicating the benefits of incorporating temporal dynamics into LLMs. 

In summary, our key contributions are as follows:
\begin{itemize}
    \item We introduce a multi-modal framework, \textbf{Language-TPP}, that unifies TPPs and LLMs, enabling various downstream tasks including event time prediction, type prediction, intensity prediction and description generation.
    \item We propose a novel temporal encoding approach using specialized byte-tokens for event time, providing seamless integration with LLM tokenizers.
    \item Through comprehensive experiments, we demonstrate state-of-the-art performance on multiple TPP datasets, with emphasis on event description generation --- a previously unexplored capability in TPP literature.
\end{itemize}

\section{Related Work}\label{sec:related_work}
\textbf{Deep TPPs.} Recent advances in TPP modeling have mainly been driven by deep learning-based methods~\citep{du2016recurrent,mei2017neural,Mishra2018ModelingPopularity,shchur2019intensity,mei2021transformer,kong2023interval}. 
Deep learning-based TPP models have seen significant evolution since~\citep{du2016recurrent}, which first introduced recurrent neural networks (RNNs) for TPP modeling. Early approaches primarily relied on such recurrent architectures, with notable improvements including long short-term memory (LSTM) based models~\citep{mei2017neural} and dual-LSTM frameworks~\citep{xiao2017modeling}. Since the rising popularity of the Transformer architectures~\citep{vaswani2017attention}, attention-based TPPs~\citep{zuo2020transformer,zhang2020self} were introduced, resulting in better modeling of long-range dependencies. Subsequent improvements include non-linear attention score~\citep{zhu2021deep}, sparse attention mechanism~\citep{li2023sparse}, and enhanced interpretability through alignment with statistical nonlinear Hawkes processes~\citep{meng2024interpretable}. 

\textbf{LLM-augmented TPPs.} The integration of LLMs with TPPs represents an emerging frontier in TPP domain. While this direction is still in its early stages, several works demonstrated promising results. \citet{xue2023prompt} introduced PromptTPP, which leverages continual learning principles to enable TPPs to efficiently adapt to streaming event sequences. The fusion of LLMs with TPPs was further advanced by \citet{shi2024language}, which developed a framework that harnesses LLMs' abductive reasoning capabilities to enhance future event prediction. Concurrent to our work, \citet{liu2024tpp} also explored combining LLMs with TPPs through incorporating textual event description and temporal information into pre-trained LLMs. While we share similar high-level goals, our work differentiates itself through a novel temporal encoding strategy using specialized byte-tokens and extends the capability of TPP models to textual event description generation, a previously unexplored direction in TPPs literature. 

\input{sections/fig-tokenization}