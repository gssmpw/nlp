\section{Experimental Setup}

\subsection{Objectives}
The main objectives of our experiments are to:
\begin{compactenum}[1)]
\item  demonstrate that \ourmethodshort\ successfully injects new knowledge in LLM while preserving its generic capabilities and show that it performs better than the other fine-tuning methods in both dimensions -- injecting new knowledge and preserving generic capabilities. 
\item demonstrate the importance of each of the components of \ourmethodshort, \viz, data identifier, replay-buffer, and answer multiplicity.
\item systematically analyze the issues with RAFT \citep{zhang2024raft}, \viz, contextual overfitting and canonical answer overfitting.
\end{compactenum}

All the code and test datasets used for our experiments are available on \href{https://github.com/kushagrabhushan/Systematic-Knowledge-Injection}{GitHub}. \footnote{\href{https://github.com/kushagrabhushan/Systematic-Knowledge-Injection}{https://github.com/kushagrabhushan/Systematic-Knowledge-Injection}} 

\subsection{Base model and Datasets}
\label{subsec:base_model_and_dataset}
We use Mistral-7B-Instruct-v0.2\footnote{\href{https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2}{mistralai/Mistral-7B-Instruct-v0.2}} as our base LLM and inject new knowledge in it using LoRA\cite{hu2022lora} tuning. 
To demonstrate the capability of \ourmethodshort\ to inject new knowledge, we would need information published after the cutoff date of the base model. Accordingly, we chose two Redbooks\footnote{Book 1: \href{https://www.redbooks.ibm.com/redpieces/pdfs/redp5736.pdf}{Do More with Less: Automating IBM Storage FlashSystem Tasks with REST APIs, Scripting, and Ansible}\\
Book 2: \href{https://www.redbooks.ibm.com/redpieces/pdfs/redp5711.pdf}{Red Hat OpenShift Container Platform on IBM Z and LinuxONE}
}
published on 23rd July and 16th May 2024 as our two different corpora. 
We use LlamaIndex\footnote{\href{https://www.llamaindex.ai/}{LlamaIndex}} to parse the PDFs into markdown and extract the text from the $5$ chapters of the first book and the $6$ chapters of the second book, respectively.


\noindent \textbf{Base data:}
We consider each chapter as a document and arbitrarily set $\maxtokens= 8000$ to split the chapters into chunks for creating synthetic QAs. 
Using base--data prompt $P_1$, we get a total of $5122$ and $33570$ QA pairs for book1 and book2, respectively, and we randomly split them into train/val/test splits. As a post-facto analysis, we compute the coverage of each chapter by the generated QA pairs and obtain $84.4\%$ and $82.5\%$ average coverage at the token level by the train data for book1 and book2, respectively. 

\noindent \textbf{Answer multiplicity in training data:}
We use multiplicity prompt $P_2$ to generate multiple answers for each question in the base dataset. We restrict the number of answers per question to $5$ and get $~4.6$ answers per question on average in the training data of both books. Generating multiple answers also results in an increase in the token-level coverage to 93.6\% and 92.4\%, respectively, for the two books. 

\noindent \textbf{Test Sets:} Along with the base data test set, we also created an additional factoid test set consisting of questions with up to eight words as answers. In the base data test set, answers often consist of multiple sentences and can be expressed in various paraphrased forms, making evaluation challenging. In contrast, factoid answers have a limited set of acceptable responses, making it easier to assess model performance. Evaluating models on this factoid test set provides a more direct measure of how many facts from the corpus have been injected into the model.
Additionally, we observed that several questions in the test sets contained phrases such as "in the above passage" and "in the given context." To address this, we removed such contextual questions by prompting a large language model (LLM) to filter them out. Table \ref{tab:filtering_prompt} provides the exact prompt for filtering the test set. All results in the paper are on the filtered base and factoid test set, with results on the factoid dataset presented in Appendix \ref{subsec:appendix_factoid}. Please see Table~\ref{tab:data_stats} in the Appendix~\ref{subsec:appendix_data_stats} for detailed statistics of all the training and test datasets.

\subsection{Training Details:}
Please see Appendix~\ref{subsec:appendix_training_details} for training details.

\subsection{Evaluation criteria}

We evaluate all the methods under the RAG setup, \ie, we fetch the top 5 passages that are similar to the question and provide them as context along with the question. 
To test under a realistic scenario, instead of setting up the index on passages from only book1 and book2, we  downloaded 34 red papers~\footnote{\url{https://www.redbooks.ibm.com/}} and indexed all of them. This results in the indexing of a total of 4,765 passages,
each with 512 tokens. We use LlamaIndex with BGE Embedding \cite{xiao2023c} to create the index. 
See Table~\ref{tab:ft_prompt} for the exact prompt used while fine-tuning. 

Following \cite{adlakha2023evaluating}, we use token level Recall \wrt\ the gold response as an automated metric to evaluate the generated response. 
In addition, we prompt Mixtral-8x22B-Instruct-v0.1\footnote{\href{https://huggingface.co/mistralai/Mixtral-8x22B-Instruct-v0.1}{mistralai/Mixtral-8x22B-Instruct-v0.1}} and LLaMA-3.3-70B-Instruct\footnote{\href{https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct}{meta-llama/Llama-3.3-70B-Instruct}} to identify if the predicted answer conveys the same message as gold while answering the question or not. See Table~\ref{tab:llm_as_judge_prompt} for the prompt used. We report these metrics for the test split of base data.
To quantify catastrophic forgetting and model's generic reasoning capabilities, we use a diverse set of standard benchmarks, \viz, MMLU \cite{hendrycks2020measuring}, GSM8k \cite{cobbe2021gsm8k}, Hellaswag \cite{zellers2019hellaswag}, and, TruthfulQA \cite{lin2022truthfulqa}.



\subsection{Baselines}
\label{sec:baselines}
We compare \ourmethodshort\ against the following baselines. The first three are inspired by \citeauthor{zhang2024raft}, and the last one is an augmented version of RAFT.
\begin{compactenum}[1)]
\item Domain-specific fine-tuning (DSF): We train the base LLM using base data to generate a response to a question without accessing any retrieved passages. 
Since the QAs in the training data cover most of the corpus content, the LLM should be able to answer test queries, provided it has injected the knowledge shown during fine-tuning. 
See Table~\ref{tab:qaft_prompt} for the prompt used.
\item DSF+RAG: We prompt the DSF model with the top 5 retrieved passages along with the question. 
The prompt used here is the same as in the other baselines and \ourmethodshort\ (Table~\ref{tab:ft_prompt}).
\item RAFT($\corruptionprob*$): method proposed in \cite{zhang2024raft}, trained using base data. We treat corruption probability $\corruptionprob$ in RAFT as a hyper-parameter and select amongst $\{0.0, 0.2, 0.4, 0.6, 0.8, 1.0\}$ based on the validation loss. 
\item $\raftmixfn{\corruptionprob*}$: Here we augment the training data of RAFT($\corruptionprob$) with RAFT(0) and RAFT(1). As earlier, we treat corruption probability $\corruptionprob$ as a hyper-parameter and select the optimum $\corruptionprob*$ based on validation loss. 
\end{compactenum}



