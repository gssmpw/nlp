\section{Introduction} \label{sec:intro}
Large Language Models (LLMs) have transformed natural language processing, excelling across various tasks~\citep{NEURIPS2020_1457c0d6}. To tailor LLMs for domain-specific applications, such as question answering over specialized corpora, Retrieval-Augmented Generation (RAG) has emerged as a popular approach~\citep{lewis2020retrieval, karpukhin2020dense}. Although RAG enhances the relevance of generated answers, it is prone to hallucinations \citep{ji-etal-2023-survey, nandwani-etal-2023-pointwise}, especially when the retriever fails to fetch relevant documents from the corpus.


To address this issue, \textit{knowledge injection} techniques~\citep{zhang2024raft,yoran2023making} have been proposed. 
Knowledge injection has two main objectives: (1) when the retriever succeeds in fetching correct documents, then the LLM should be able to leverage that information to generate an appropriate response, and (2) when the retriever fails, the LLM should recall the domain specific information from the infused parametric knowledge to generate the response.
To achieve them, existing techniques fine-tune LLMs with domain-specific data, embedding the knowledge directly into the LLM's parameters. Specifically, these techniques fine-tune LLMs to generate the correct response to a question irrespective of the relevance of the retrieved documents.
However, existing knowledge injection techniques suffer from two main issues:
\begin{compactenum}[1)]
   \item \textit{Conditional Memorization Bias:} In the training data, each question is assigned to either a retrieval success or a retrieval failure scenario. 
   \Ie, the relevant information to answer the question is either present or absent in the given context for a question.
   This static assignment determines how the LLM learns the knowledge.
   For example, consider a scenario where all training questions from a particular document are assigned to the `retrieval success' bucket, \ie, information required to answer is present in the accompanying context.
   In this case, 
   the LLM is encouraged to rely on the external retrieved context, and it may not memorize information from such documents.
   Conversely, if all training questions from a particular document are assigned to `retrieval failure' bucket, the LLM will be forced to memorize its content during fine-tuning and may learn to ignore the provided information for questions from that document.
   As a result, the LLM learns different sections of the domain data in different ways. This inconsistency can be problematic, as the LLM might struggle when faced with opposite scenarios during inference. We confirm this behaviour in our experiments.

	\item \textit{Canonical Answer Overfitting:} Each question in the fine-tuning dataset is associated with only one canonical answer. This singular association leads the LLM to learn and replicate spurious patterns~\cite{allen2024physics}, treating the answer as a fixed representation for that specific question. As a result, the LLM's ability to generate nuanced or diverse responses based on varying contextual factors is constrained.
\end{compactenum}

To mitigate these issues, we propose \ourmethodshort: \ourmethodlong, a novel fine-tuning framework that improves knowledge injection into LLMs for domain-specific RAG tasks. 
\ourmethodshort\ introduces two different ways of training data augmentation to mitigate conditional memorization bias and canonical answer overfitting.

First,
\ourmethodshort\
uses context augmentation to simulate both retrieval success and retrieval failure scenarios for all the training questions. 
This prevents conditional memorization bias by teaching the model to identify whether the given retrieved information is relevant or not and then accordingly decide to ignore or utilize it.

Second, \ourmethodshort\ synthetically generates multiple answers for each training question to mitigate canonical answer overfitting.
It is inspired by a recent study~\citep{allen2024physics} demonstrating that paraphrasing knowledge during pre-training significantly enhances LLMs' ability to inject and extract that information in the downstream tasks. 
However, they focus on paraphrasing only during pre-training and not during fine-tuning.
Building on this, we propose to systematically augment the fine-tuning data by synthetically generating multiple answers for each question. This encourages LLMs to memorize and extract the domain knowledge effectively while minimizing the overfitting on stylistic features.

Furthermore, to address the challenge of \emph{catastrophic forgetting}~\citep{zhang2024dissecting, ke2023continual, jang2021towards}—where an LLM’s general language understanding deteriorates as domain-specific fine-tuning overwrites prior knowledge—we introduce a \emph{self-selective rehearsal replay buffer}~\citep{gupta2024selective, huang2024mitigating}.
This buffer contains samples from an instruction tuning dataset. But unlike traditional replay buffers, the self-selective approach uses LLM’s own predictions rather than the gold response to retain the LLM’s versatility across tasks when injecting knowledge.

In addition, we also introduce the novel use of \emph{Domain Identifiers}—phrases pre-pended to questions—to help the LLM distinguish
new knowledge from its existing skills.
This improves domain-specific accuracy and also mitigates catastrophic forgetting, preserving LLM's general competence during fine-tuning. 

To validate \ourmethodshort's ability to inject new domain-specific knowledge, we require a corpus that the LLM hasn't seen during pre-training or instruction tuning. 
In the absence of such a corpus, we create two datasets using domain-specific books that were published in 2024\footnote{after the cutoff date of the fine-tuned LLM}. 
In our experiments, \ourmethodshort\ achieves a significant increase in performance against other methods on this domain-specific benchmark while maintaining general reasoning capabilities that we measure using various benchmarks such as MMLU \cite{hendrycks2020measuring}, TruthfulQA \cite{lin2022truthfulqa}, Hellaswag \cite{zellers2019hellaswag}, and GSM8k \cite{cobbe2021gsm8k}.
