\section{Related Work}\label{sec:related}

\noindent\textbf{Retrieval Augmented Generation:} RAG enhances Large Language Models (LLMs) by integrating external data sources, such as knowledge bases, to improve relevance and accuracy~\citep{lewis2020retrieval, guu2020retrieval, karpukhin2020dense}. Recent advancements have extended its applicability across domains~\citep{asai2024self, kimsure, yan2024corrective, liu-etal-2024-ra}, but RAG systems still face key challenges: hallucinations due to mismatches between retrieved data and the LLM’s pre-existing knowledge~\citep{setty2024improvingretrievalragbased, jin2024tug}, difficulty with complex multi-document reasoning~\citep{setty2024improvingretrievalragbased}, and an inability to fully leverage fixed-domain settings where all domain-specific documents are available beforehand because typically neither the retriever nor the generator LLM are trained on the domain data.

\noindent\textbf{Domain-Aware Fine-Tuning for RAG:} Joint training of the retriever and LLM has been proposed as a way to improve RAG’s domain-specific performance~\citep{guu2020retrieval, singh2021end, siriwardhana-etal-2023-improving, shi2024replug}. By jointly training the retriever and LLM, the system can better adapt to domain-specific contexts. However, this approach introduces complexities, including the need for specialized loss functions and frequent retriever updates. 

Another line of work~\citep{mecklenburg2024injecting, zhang2024self} focuses solely on adding domain knowledge to LLMs as an alternative to RAG.
These approaches fine-tune LLMs using question-answer (QA) pairs derived from domain data and aim to answer any new test query without retrieving any document. As a result, they fail to leverage access to the domain documents during inference.

Recently,~\citeauthor{zhang2024raft} introduced~\textit{Retrieval-Augmented Fine-Tuning (RAFT)}, a fine-tuning method for LLMs to incorporate domain knowledge and enhance in-domain RAG performance. 
RAFT combines RAG and fine-tuning by training LLMs on domain data using a mixture of oracle and distractor document contexts.
However, it suffers from conditional memorization bias and canonical answer overfitting. 
On the other hand, \ourmethodshort\ uses context augmentation and answer paraphrasing to address these issues.

\noindent\textbf{Catastrophic forgetting:}
Catastrophic Forgetting~\citep{french1999catastrophic, zheng-etal-2024-learn} occurs when new domain-specific fine-tuning overwrites previously learned general knowledge, reducing performance on earlier tasks. Replay-based methods~\citep{de2019episodic,rolnick2019experience}, help mitigate this by rehearsing prior task data during training. Recent advances in replay-based approaches for language models~\citep{scialom2022fine, mok2023large} have shown promise in reducing catastrophic forgetting. The Self-Synthesized Rehearsal (SSR)~\cite{gupta2024selective, huang2024mitigating} framework uses the LLM to generate synthetic rehearsal data, reducing reliance on stored instances.