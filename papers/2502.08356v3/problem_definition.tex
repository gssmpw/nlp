\section{Problem Definition}
\label{sec:prob_def}

Given a domain-specific corpus $D = \{d_i\}_{i=1}^{n}$, where each document contains domain-relevant knowledge, the goal is to fine-tune Large Language Models (LLMs) for enhanced performance in domain-specific Retrieval-Augmented Generation (RAG) systems. Unlike previous works~\citep{linra, wanginstructretro, mecklenburg2024injecting}, which handle changing test-time domains or documents, our approach, like RAFT, assumes a fixed target domain with known access to domain-specific documents.

Our proposed method, \ourmethodshort\ described in Section~\ref{sec:approach}, addresses the following key challenges:

\begin{compactenum}[1)]

    \item Canonical answer overfitting: In Section~\ref{subsec:synthetic_qa_generation}, we discuss how paraphrased answer augmentation is used to generate multiple variations of answers per train question, ensuring the LLM learns the underlying knowledge and avoids overfitting to fixed answers.
    
	\item Conditional memorization bias: In Section~\ref{subsec:fine_tuning_strategy}, we introduce a method that simulates both successful and failed retrieval scenarios for each question. This helps the LLM learn how to handle various retrieval conditions, preventing it from memorizing answers in static settings and improving its generalization.

	
	\item Catastrophic forgetting: In Sections~\ref{subsec:domain_indentifiers} and ~\ref{subsec:self_selective_replay}, we outline how a self-selective rehearsal replay buffer and domain-specific identifiers help balance domain-specific knowledge acquisition while retaining the LLM's general knowledge and capabilities.
\end{compactenum}

Through these strategies, \ourmethodshort\ enables more effective knowledge integration, improving domain-specific performance while maintaining generalization capabilities in RAG systems.