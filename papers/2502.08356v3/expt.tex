
\section{Experimental Results}
\input{main_table_with_breakup}
\subsection{Comparison with baselines}
Table \ref{tab:main_table_breakup} compares the performance of our method with various baselines. 
\ourmethodshort\ performs the best in both dimensions: knowledge injection (measured by recall and LLM Judge), as well as maintaining the base model's general capabilities, as measured by the drop in average regression score. 
We observe that the base LLM performs better than most of the baselines when the retrieved passage overlaps with the gold document (we call it "some overlap" subset), demonstrating that the instruction-tuned Mistral has decent reading comprehension capabilities. 
However, when the retrieved passages do not overlap with the gold document ("No overlap" subset), the model has to recognize this and then answer from its memory.
We observe that all the baselines perform poorly in such a scenario.
On the other hand, \ourmethodshort\ beats all the baselines in both cases, demonstrating that:
1. When the retriever succeeds, \ourmethodshort\ can identify this and leverage the retrieved information to respond.
2. When the retriever fails, \ourmethodshort\ can ignore the retrieved information and answer correctly from its own parametric memory.

Next, we observe that \raftmix\ performs better than RAFT, confirming the presence of conditional memorization bias. Recall that such a bias occurs due to the static assignment of each question to retriever success or failure case during training. In \raftmix, when we augment RAFT($\corruptionprob$) with RAFT(0) and RAFT(1), we ensure that each question is seen with and without correct passages during training. This forces the model to learn that irrespective of the question, it has to leverage the information present in the context only when it is correct and rely on its own parametric knowledge when the retriever fails. 

Next, we observe that using a replay buffer, data identifier, and multiple answers for the same question significantly improves the model's performance without impacting its generic capabilities.
We note that \ourmethodshort\ implicitly takes care of conditional memorization bias. 
In Section~\ref{subsec:analysis_of_biases}, we empirically validate that this is indeed the case.

\input{ablations}
\subsection{Ablations} 
To understand the value addition by different components of \ourmethodshort, we run ablations by removing each of the components and retraining the model. 
Specifically, we ablate on replay buffer, Data Identifier (DI), and multiple answers (Multi Ans.).
Table \ref{tab:ablations} shows the results. For each ablation, we show the change in the metric \wrt\ \ourmethodshort.

We first observe that removing the replay buffer has a significant impact on the average regression score that measures the model's generic capabilities (a drop of 1.9 from \ourmethodshort\ and an overall drop of 3.1 from the base model).
Interestingly, removing the replay buffer negatively impacts the model's performance on the "some overlap" subset. 
This demonstrates that the replay buffer helps in bolstering the reading comprehension capabilities of the model as well.
On the other hand, performance on the "no overlap" subset improves slightly, demonstrating that the reduced burden of remembering existing skills helps in memorizing and recalling new information.

Next, we observe that removing the Data Identifier (DI) results in an overall drop in the model's performance.
The drop is more prominent over "no overlap" subset of the test data, demonstrating that the DI helps in injecting and recalling the domain knowledge when the model has to answer from its own parametric memory.

Finally, we observe that using multiple paraphrases of the same answer significantly impacts the performance -- recall drops by 5.65 pts from 77.0. As expected, dropping multiple answers has no negative impact on the model's generic capabilities. 


\subsection{Analysis of conditional memorization bias}
\label{subsec:analysis_of_biases}

\input{ch_corruption_type}

In this section, we empirically demonstrate:
1). Conditional memorization bias exists, and it can significantly hamper the learning process, and
2). Answer multiplicity during training implicitly results in context augmentation when we randomly assign each augmented QA pair to either of the two buckets (`retriever success' or `retriever failure').

To do so, we systematically assign each QA pair to the `retriever success' and `retriever failure' buckets while training. 
In the first analysis, all QA pairs from chapters 1 to 3 from book 1 are assigned to the `retriever failure' bucket, and all QA pairs from chapters 4 and 5 are assigned to the `retriever success' bucket. 
Table \ref{tab:chapterwise_corruption} separately compares the token level recall for the test queries from chapters 1-3 and chapters 4-5. 
We observe that when there is no overlap between the retrieved passages and the gold documents, the performance over questions from chapters 4-5 is significantly poor for the chapter wise bucketing method.  
This happens because during training, all questions from chapters 4-5 had relevant context, and thus the model relied only on the given context to answer questions from chapters 4-5. Hence, when provided with irrelevant passages during testing, the model fails to recall that information and thus performs poorly.
We do not see any such pattern for questions from chapters 1-3.
Interestingly, when relevant information is provided during testing for questions from chapters 1-3, the model fails to leverage that, demonstrating that it learnt to always ignore the context when the question is from chapters 1-3.


%Recall that we have multiple QA pairs for the same question. 
Recall that our \ourmethodshort\ finetunes LLM using $\mathcal{D}_{aug.}$ that has multiple QA pairs for the same question. 
We show that in such a scenario, random assignment of train QA pairs to the `retriever success' or `retriever failure' bucket alleviates the need for explicit context augmentation in \ourmethodshort.   
To do so, we systematically assign all QA pairs with the same question to the same bucket. For a question, the `retriever failure' bucket is chosen with probability $\corruptionprob=0.4$.
In such a setup, we have systematically removed the context augmentation but have kept multiple answers.
Table \ref{tab:querwise_corruption} compares the token level recall of question-wise bucketing with \ourmethodshort\ that has QA-wise bucketing.
We observe that the overall token level recall worsens with question-wise bucketing. 
Notice that the gap is wider (2.5 pt) when there is no overlap between retrieved passages and the gold document for a test question. This demonstrates that the model struggles to either inject all the knowledge or recall it when required.
We attribute this to the lack of context augmentation in question-wise bucketing.

\input{q_corruption_type}

\subsection{Effect of model size and model family}
All the results presented above are obtained by fine-tuning a Mistral-7B model.
To evaluate the robustness of our method to variations in model size and architecture, we train two additional models from the LLaMA family, LLaMA-2-7B-Chat\footnote{\href{https://huggingface.co/meta-llama/Llama-2-7b-chat-hf}{meta-llama/Llama-2-7b-chat-hf}} and LLaMA-2-13B-Chat\footnote{\href{https://huggingface.co/meta-llama/Llama-2-13b-chat-hf}{meta-llama/Llama-2-13b-chat-hf}}.
Table~\ref{tab:main_table_llama7b} reports our findings.
In both cases, we observe trends similar to Mistral-7B, demonstrating that \ourmethodshort\ is robust to varying architectures and model sizes.

\subsection{Effect of ingesting already seen knowledge}

\label{subsec:clapnq}
We conduct additional experiments using a small subset (50 documents and 50 queries) of the recently proposed CLAPNQ dataset \citep{rosenthal2025clapnq}.
It is derived from NQ dataset \citep{kwiatkowski2019natural} and consists of a human-annotated corpus of actual user queries and corresponding answers from Wikipedia articles. 
Mistral may not have seen the specific QA pairs during training, but it likely encountered the underlying information from Wikipedia pages in various paraphrased forms.
%Mistral uses the Natural Questions dataset as a standard benchmark for performance evaluation; however, we believe that Mistral did not encounter the specific QA pairs in NQ during training. Nevertheless, during pretraining, it likely encountered the underlying information from Wikipedia pages in various paraphrased forms. 
As a result, there is no new knowledge to be ingested, and we do not expect our method to yield significant performance gains over the base model.
See table~\ref{tab:clapnq} for the results. 

Surprisingly, RAFT performs worse than the base model, while \ourmethodshort's performance remains at par with the base model. 
We hypothesize two causes for this: 
First, RAFT is trained using only a single answer per question, which may result in overfitting to canonical answers and losing its ability to recall knowledge already seen during pretraining. 
In contrast, \ourmethodshort\ trains on multiple answers and varied contexts, preserving and leveraging this ability.
Second, our synthetically generated QAs cover only 49\% of a document on average. However, it is plausible that the base model would have seen the remaining 51\% during pretraining, giving it an edge. RAFT results in overfitting to 49\% of the information, whereas \ourmethodshort\ generalises better by retaining the pre-existing knowledge and skills due to the replay buffer.




