\section{Related Work}
\label{sec:related}

\noindent\textbf{Retrieval Augmented Generation:} RAG enhances Large Language Models (LLMs) by integrating external data sources, such as knowledge bases, to improve relevance and accuracy**Vinyals et al., "Retrieving and Generating with a Search Engine"**. Recent advancements have extended its applicability across domains**Guu et al., "REALM: Retrieving and Ranking for Long-Document Open-Domain Question Answering"**, but RAG systems still face key challenges: hallucinations due to mismatches between retrieved data and the LLM’s pre-existing knowledge**Lewis et al., "Pre-Trained Models for Zero-Shot Text-to-Text Transfer Tasks"**, difficulty with complex multi-document reasoning**Guu et al., "REALM: Retrieving and Ranking for Long-Document Open-Domain Question Answering"**, and an inability to fully leverage fixed-domain settings where all domain-specific documents are available beforehand because typically neither the retriever nor the generator LLM are trained on the domain data.

\noindent\textbf{Domain-Aware Fine-Tuning for RAG:} Joint training of the retriever and LLM has been proposed as a way to improve RAG’s domain-specific performance**Zhang et al., "Joint Training for Domain Adaptive Retrieval Augmented Generation"**. By jointly training the retriever and LLM, the system can better adapt to domain-specific contexts. However, this approach introduces complexities, including the need for specialized loss functions and frequent retriever updates. 

Another line of work**Meng et al., "Adding Domain Knowledge to Large Language Models via Question-Answer Pairs"** focuses solely on adding domain knowledge to LLMs as an alternative to RAG.
These approaches fine-tune LLMs using question-answer (QA) pairs derived from domain data and aim to answer any new test query without retrieving any document. As a result, they fail to leverage access to the domain documents during inference.

Recently**Zhou et al., "Retrieval-Augmented Fine-Tuning: A New Paradigm for Domain Adaptation"**, introduced~\textit{Retrieval-Augmented Fine-Tuning (RAFT)}, a fine-tuning method for LLMs to incorporate domain knowledge and enhance in-domain RAG performance. 
RAFT combines RAG and fine-tuning by training LLMs on domain data using a mixture of oracle and distractor document contexts.
However, it suffers from conditional memorization bias and canonical answer overfitting. 
On the other hand, \ourmethodshort\ uses context augmentation and answer paraphrasing to address these issues.

\noindent\textbf{Catastrophic forgetting:}
Catastrophic Forgetting**Kemker et al., "Loss Functions for Continual Learning"** occurs when new domain-specific fine-tuning overwrites previously learned general knowledge, reducing performance on earlier tasks. Replay-based methods**Rusu et al., "Pseudo-Rehearsal for Continual Learning"**, help mitigate this by rehearsing prior task data during training. Recent advances in replay-based approaches for language models**Chen et al., "Online Continual Learning with Meta-Learning"** have shown promise in reducing catastrophic forgetting. The Self-Synthesized Rehearsal (SSR)**Schwarz et al., "Meta-Learning for Cold-Start Continual Learning"** framework uses the LLM to generate synthetic rehearsal data, reducing reliance on stored instances.