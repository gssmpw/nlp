\section{Related Work}
\label{sec:related}

\noindent\textbf{Retrieval Augmented Generation:} RAG enhances Large Language Models (LLMs) by integrating external data sources, such as knowledge bases, to improve relevance and accuracy____. Recent advancements have extended its applicability across domains____, but RAG systems still face key challenges: hallucinations due to mismatches between retrieved data and the LLM’s pre-existing knowledge____, difficulty with complex multi-document reasoning____, and an inability to fully leverage fixed-domain settings where all domain-specific documents are available beforehand because typically neither the retriever nor the generator LLM are trained on the domain data.

\noindent\textbf{Domain-Aware Fine-Tuning for RAG:} Joint training of the retriever and LLM has been proposed as a way to improve RAG’s domain-specific performance____. By jointly training the retriever and LLM, the system can better adapt to domain-specific contexts. However, this approach introduces complexities, including the need for specialized loss functions and frequent retriever updates. 

Another line of work____ focuses solely on adding domain knowledge to LLMs as an alternative to RAG.
These approaches fine-tune LLMs using question-answer (QA) pairs derived from domain data and aim to answer any new test query without retrieving any document. As a result, they fail to leverage access to the domain documents during inference.

Recently,____ introduced~\textit{Retrieval-Augmented Fine-Tuning (RAFT)}, a fine-tuning method for LLMs to incorporate domain knowledge and enhance in-domain RAG performance. 
RAFT combines RAG and fine-tuning by training LLMs on domain data using a mixture of oracle and distractor document contexts.
However, it suffers from conditional memorization bias and canonical answer overfitting. 
On the other hand, \ourmethodshort\ uses context augmentation and answer paraphrasing to address these issues.

\noindent\textbf{Catastrophic forgetting:}
Catastrophic Forgetting____ occurs when new domain-specific fine-tuning overwrites previously learned general knowledge, reducing performance on earlier tasks. Replay-based methods____, help mitigate this by rehearsing prior task data during training. Recent advances in replay-based approaches for language models____ have shown promise in reducing catastrophic forgetting. The Self-Synthesized Rehearsal (SSR)____ framework uses the LLM to generate synthetic rehearsal data, reducing reliance on stored instances.