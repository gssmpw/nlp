\section{Approach}
\label{sec:approach}

In this section, we outline our approach, which involves --
1). generating a synthetic QA dataset from the documents of the given domain;
2). augmenting it to improve knowledge extraction and;
3). incorporating domain identifiers to clearly define boundaries between distinct knowledge areas.

\subsection{Synthetic QA Generation}
\label{subsec:synthetic_qa_generation}
Given a domain-specific corpus $D= \{d_i\}_{i=1}^{n}$ of $n$ documents, our objective is to generate diverse question-answer (QA) pairs that maximize information coverage across the documents using an LLM. We generate these QA pairs using Mixtral-8x22B-Instruct-v0.1\footnote{\href{https://huggingface.co/mistralai/Mixtral-8x22B-Instruct-v0.1}{mistralai/Mixtral-8x22B-Instruct-v0.1}} as the LLM.

\noindent\textbf{Document Chunking:} 
If a document $d_i$ has more than $\maxtokens$ tokens, we split it into chunks of size $\maxtokens//2$. Otherwise, it is processed as a single chunk.
Here $\maxtokens$ is a hyperparameter.
We denote the set of chunks for document $d_i$ as $C^{(i)} = \{c^{(i)}_1, c^{(i)}_2, \dots, c^{(i)}_{m_i}\}$, where $m_i$ is the total number of chunks for document $d_i$. Each $c^{(i)}_s$ represents the $s$-th chunk of document $d_i$, where $1 \leq s \leq m_i$.

\noindent\textbf{QA Pair Generation:} 
For each chunk $c^{(i)}_s \in C^{(i)}$, we prompt the LLM $\numqacalls$ times using an instruction $P_1$ along with the chunk $c^{(i)}_s$. 
Each call to the LLM using prompt $P_1$ generates multiple QA pairs, ensuring that most of the tokens in $c^{(i)}_s$ are covered. Multiple calls with nucleus sampling~\cite{holtzmancurious} ensure adequate token coverage. 
Let $\{(q_s^j, a_s^j)\}_{j=1}^{k}$ be the set of QA pairs generated for chunk $c^{(i)}_s$.
We call this dataset of QA pairs as $\mathcal{D}_{\text{base}} = 
\{ \{(q_s^j, a_s^j)\}_{j=1}^{k}, \forall c^{(i)}_s \in C^{(i)}, \forall d_i \in D\}.$
This approach ensures broad content coverage, but the lack of variability in the generated answers can lead to Canonical Answer Overfitting, as described in Section~\ref{sec:intro}.

\noindent\textbf{Adding Answer Multiplicity:} 
To address canonical answer overfitting, we introduce answer multiplicity by re-prompting the LLM to generate multiple answers for each generated question $q_s^j$ using a separate prompt $P_2$. For each chunk-question pair $(c^{(i)}_s, q_s^j)$, we generate a set of paraphrased answers $\{a_s^{j1}, a_s^{j2}, \dots, a_s^{jp}\}$, where $p$ is the number of paraphrases:\\
$
\mathrm{LLM}(P_2, c^{(i)}_s, q_s^j) \rightarrow \{a_s^{j1}, a_s^{j2}, \dots, a_s^{jp}\}
$ \\
This results in the augmented dataset $\mathcal{D}_{\text{aug.}}$, where each question $q$ is linked to diverse paraphrased answers. The added variability mitigates canonical answer overfitting and improves generalization. As shown in ablation studies, the model fine-tuned on $\mathcal{D}_{\text{aug.}}$ outperforms the one fine-tuned on the base dataset $\mathcal{D}_{\text{base}}$.
All prompts used for dataset creation, including $P_1$ and $P_2$, are provided in Section~\ref{subsec: appendix_prompts}.

\subsection{Fine-tuning Strategy} \label{subsec:fine_tuning_strategy}

In the original RAFT approach~\citep{zhang2024raft}, each data point used for fine-tuning consists of a question ($q$), a context (a collection of documents), and an answer ($a$). The context is classified as either \emph{relevant}, \ie, it contains at least one document (alongside distractor documents) that provides the information needed to deduce the answer, or \emph{irrelevant} where the entire context consists of distractor documents.
Baseline RAFT, referred to as RAFT($\mathrm{p}$), presents a fraction $(1-\corruptionprob)$ of questions with a relevant context. We call this training subset as the `retriever success' bucket.
For the remaining $\corruptionprob$ fraction of questions, only an irrelevant context (composed entirely of distractor documents) is provided. We call such a training subset as the `retriever failure' bucket.
However, this fine-tuning setup leads to conditional memorization bias, where the LLM either relies on relevant contexts or stores the necessary information in its parametric memory when presented with irrelevant contexts. As a result, the model may excel at handling specific retrieval scenarios but struggle to generalize effectively across varied contexts during inference. For some parts of the document, the LLM memorizes the content, while for others, it relies on the retrieved documents, resulting in inconsistent performance.

To address conditional memorization bias, we introduce \raftmix\ (Context-Augmented RAFT), which combines RAFT($\mathrm{0}$) and RAFT($\mathrm{1}$) with RAFT($\corruptionprob$) for more granular control over relevant and irrelevant contexts. In RAFT($\mathrm{0}$), all questions are paired with irrelevant contexts, teaching the model to rely solely on its internal memory. In RAFT($\mathrm{1}$), only relevant contexts are provided. By mixing RAFT($\mathrm{0}$), RAFT($\mathrm{1}$), and RAFT($\mathrm{p}$), \raftmix($\corruptionprob$) effectively mitigates Conditional Memorization Bias.

Our complete approach, \ourmethodshort\, significantly enhances \raftmix\ by incorporating paraphrased answer augmentation to address Canonical Answer Overfitting. 
While \raftmix\ ensures the model learns how to handle both relevant and irrelevant contexts without over-relying on one or the other, \ourmethodshort\ goes further by exploiting diverse paraphrased answers during training to effectively inject new knowledge. This augmentation ensures that the model does not overfit to a single canonical answer for each question, promoting deeper learning of underlying knowledge. 

Interestingly, answer augmentation implicitly takes care of conditional memorization bias as well.
When we randomly assign each training QA pair in $\mathcal{D}_{\text{aug.}}$ to `retriever success' or `retriever failure' bucket, it automatically distributes different QA pairs for the same question into different buckets.
Thus, when training with $\mathcal{D}_{\text{aug.}}$, we do not need context augmentation separately.

\subsection{Domain Identifiers}
\label{subsec:domain_indentifiers}
LLMs store vast amounts of information across different domains, which can lead to confusion when handling specialized questions. To ensure accurate and relevant responses, we introduce \emph{domain identifiers} that establish boundaries within the modelâ€™s parametric memory by specifying the context of each question. A domain identifier is a simple token or phrase prepended to each question during training and evaluation. For example:

\vspace{5pt}
\noindent\begin{minipage}{\textwidth}
	\noindent\texttt{"This question is from \{domain\_name\}.\\\{Question\}"\\}
\end{minipage} 
Here, the placeholder \texttt{\{domain\_name\}} is replaced with the specific domain (e.g., Healthcare, Legal), and \texttt{\{Question\}} is replaced with the actual question. This template ensures that each question is clearly tied to its domain, reducing ambiguity and helping the model differentiate between similar questions from different fields. Additionally, domain identifiers have minimal impact on general performance, as removing them may allow the model to return to its original state, preserving its generalization capabilities.

\subsection{Self-Selective Replay Buffer}
\label{subsec:self_selective_replay}
Typically, a replay buffer contains old samples from previous tasks on which the LLM has been trained. This helps the model retain general knowledge and mitigate catastrophic forgetting~\citep{zhang2024dissecting, ke2023continual, jang2021towards} while fine-tuning on new domain-specific data. However, since we do not have access to the original instruction-tuning data of the target LLM, 
we introduce a self-selective rehearsal replay buffer~\citep{gupta2024selective, huang2024mitigating}.

This approach differs from traditional replay buffers in that it generates new outputs for old inputs. 
Using the technique introduced in ~\citep{sudalairaj2024lab}, we generate diverse inputs that belong to different categories, such as code, math, reasoning, extraction, safety, writing \etc.  
We then pass each input (\textit{x}) through the LLM and use (\textit{x},LLM(\textit{x})) as an auxiliary dataset during the fine-tuning process.

By combining this self-selective replay buffer with the current synthetic QA data, we ensure that the model retains general knowledge while fine-tuning on domain-specific tasks, thereby mitigating catastrophic forgetting. 