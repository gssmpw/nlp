\section{Conclusion}\label{sec:conclusion}
We introduced \ourmethodshort, a novel framework designed to enhance the fine-tuning process for domain-specific RAG tasks. By incorporating context augmentation and answer multiplicity through paraphrasing,
our approach effectively mitigates both conditional memorization bias and canonical answer overfitting. 
It results in a more adaptable LLM that is robust to retriever errors on domain-specific questions.
We use a novel replay buffer technique along with a data identifier that mitigates catastrophic forgetting.
Our experimental results demonstrate that \ourmethodshort\ outperforms baseline methods, offering a promising direction for improving domain-specific knowledge injection in LLMs.