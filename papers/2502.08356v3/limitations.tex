\section{Limitations} While \ourmethodshort\ demonstrates promising results in reducing catastrophic forgetting, it does not completely eliminate the issue. The retained parametric knowledge may still degrade when new domain-specific information is introduced due to domain-specific fine-tuning. Additionally, the effectiveness of \ourmethodshort\ depends on a strong LLM to generate high-quality QA pairs from domain documents. If the LLM struggles to generate accurate or contextually rich pairs, the overall performance may be affected. Moreover, the enhancements introduced by \ourmethodshort\, such as paraphrased answer augmentation and dynamic retrieval simulation, require additional computational resources, which can be a concern when injecting large-scale domain data. Despite these limitations, \ourmethodshort\ shows considerable promise, though further work is needed to address these challenges and make it more robust and widely applicable.