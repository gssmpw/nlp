\onecolumn 
\section{Experimental Details}
\label{sec_appendix_expt}

\subsection{Data statistics}
\label{subsec:appendix_data_stats}

See Table \ref{tab:data_stats} for detailed statistics of all the training and test datasets used in our experiments.

\input{data_stats}

\subsection{\ourmethodshort\ training details}
\label{subsec:appendix_training_details}
We use Huggingface's SFTTrainer\footnote{\href{https://huggingface.co/docs/trl/sft_trainer}{docs/trl/sft\_trainer}}
to finetune the base model separately for book1 and book2 using LoRA adapters with rank 16 and 32, respectively.
We experimented with the learning rates of $1e-5$ and $1e-4$ and selected $1e-4$ based on validation set performance. 
We train for 400 and 1200 steps for book1 and book2, respectively, with an effective batch size of 256 (16 gradient accumulation steps on 4 A100 80GB GPUs with 4 batch-size on each GPU) and select the best checkpoint based on validation loss. Training time for the two models is less than 5 and 15 hours, respectively. 


\subsection{Effect of domain identifiers and replay buffer on catastrophic forgetting}
\label{subsec:appendix_regression_scores}
\input{regression_score_book1}
\input{regression_score_book2}

Here, we demonstrate the effectiveness of both Domain Identifiers and Replay Buffer to mitigate catastrophic forgetting. 
Tables~\ref{tab:reg-book-1} and \ref{tab:reg-book-2} report the individual regression scores for each task. 
We use github repo\cite{eval-harness} to compute all the scores.
For GSM8k, we take the average of Flexible and Strict Match.
For TruthfulQA (TQA), we take the average accuracy over MC1 and MC2.
We consider TruthfulQA\_Gen separately from MC1 and MC2 and use RougeL to quantify the performance.
Reported average score is the average of these five scores.

\section{Experimental Analysis of LLaMA Models}

Tables \ref{tab:main_table_llama7b} and \ref{tab:main_table_llama_13b} show the results of , LLaMA-2-7B-Chat and , LLaMA-2-13B-Chat respectively on both book 1 and book 2. We see in both the cases that \ourmethodshort  \hspace{0.1cm} outperforms all baselines, proving its robustness to model architecture and size. 

\input{main_table_llama_7b}

\input{main_table_llama_13b}
\newpage
\section{Results on Factoid Dataset}
\label{subsec:appendix_factoid}

As mentioned in \ref{subsec:base_model_and_dataset}, we curated a factoid dataset consisting of QA pairs containing only factoid answers, i.e., short, factual questions. Tables \ref{tab:factoid_mistral}, \ref{tab:factoid_llama7b} \& \ref{tab:factoid_llama13b} show the comparisons between \ourmethodshort  \hspace{0.1cm} and various other baselines, on all three models, Mistral-7B-v0.1 , LLaMA-2-7b-chat-hf  and LLaMA-2-13b-chat-hf. 

\input{factoid_mistral}

\input{factoid_llama7b}

\input{factoid_llama13b}

\section{Results on CLAPNQ Dataset}
As mentioned in \ref{subsec:clapnq}, here we show the result of \ourmethodshort \hspace{0.1pt} along with Base+RAG and RAFT on the CLAPNQ dataset which is a subset from the Natural Questions dataset.
We see in table \ref{tab:clapnq} that token level recall for RAFT is lower than both the baseline and \ourmethodshort. 

\input{clapnq}

\section{Prompts}
\label{subsec: appendix_prompts}

This section enumerates all the prompts that are used in our experiments.
We use LLaMA-3.3-70B-Instruct\footnote{\url{https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct}} as judge and employ Mixtral-8x22B-Instruct-v0.1\footnote{\url{https://huggingface.co/mistralai/Mixtral-8x22B-Instruct-v0.1}} both for data generation and as a judge to evaluate the fine-tuned model's predicted answers against the ground truth.

\subsection{Prompt for generating question-answer pairs}
The prompt in Table~\ref{tab:qa_prompt} instructs the model to create question-answer pairs from a given document, ensuring that the questions do not use co-referencing or pronouns and that they are fully contextualized.

\begin{table*}[h]
	\centering
     \resizebox{\textwidth}{!}{%
	\begin{tikzpicture}
		\node[draw=black, rounded corners=10pt, fill=gray!20, inner sep=10pt] (box) { % Create a rounded box
			\begin{tabular}{p{\linewidth}}
				<s> [INST] Create question answer pairs from the document given below within <document> tags. Title of the document is given in the first line of the document. Do not use co-referencing and pronouns at all in the questions. Do not refer to the document in the question like "according to the document ..." or any similar paraphrasing. When needed, contextualize the question by using the topic that the question is about. You can use the title of the document as well for contextualizing. There are several figures in the document, while referring to the figure in any question, contextualize it by mentioning the title of the passage it was present in. Put questions within <question> and </question> tags and answers within <answer> and </answer> tags. Ensure that the question and answers cover the entire document. When you are done generating QA pairs, generate </done> token. [/INST]\\
			\end{tabular}
		};
	\end{tikzpicture}
 }
	\caption{Prompt for generating question-answer pairs from a document.} 
	\label{tab:qa_prompt}
\end{table*}

\subsection{Prompt for generating multiple answers}
The prompt in Table~\ref{tab:multiple_answers_prompt} enables the generation of multiple distinct answers for a question based on the provided document, encouraging diverse answer styles and formats.

\begin{table*}[h]
	\centering
     \resizebox{\textwidth}{!}{%
	\begin{tikzpicture}
		\node[draw=black, rounded corners=10pt, fill=gray!20, inner sep=10pt] (box) { % Create a rounded box
			\begin{tabular}{p{\linewidth}}
				<s> [INST] You are provided with a document within <document> and </document> tags. Followed by the document, you are provided with a question within <question> and </question> tags. Using only the information provided in the document, generate multiple correct, complete and comprehensive answers for the given question, varying the style and format. Each of your answers should be within separate <answer> and </answer> tags so that I can parse and extract them using a code. Each answer must be complete, correct, comprehensive and only from the provided document.  \\
				Start by first generating <answer> token. Then generate one valid answer following any one style or format. Then generate </answer> token. Then again generate <answer> token, followed by another valid answer. Then generate </answer> token. Keep on repeating this till you generate as many correct answers as possible. In the end, generate </done> token once you are done generating all the answers in all possible styles and formats.  \\
				Some example styles and format that you must use:  \\
				1. List Answer: Answer can be in the form of a list.  \\
				2. Extractive answer: Answer with extracted sentences from the document.  \\
				3. Inferential answer: Answer that summarizes the information in the document but is not directly quoted.  \\
				4. Definitions: Answer that gives definitions related to the question.  \\
				5. Examples: Providing examples that can be valid responses to the question.  \\
				Ensure that the answers are not just paraphrases but differ in their content and are factually correct based on the document. [/INST] \\
			\end{tabular}
		};
	\end{tikzpicture}
 }
	\caption{Prompt for generating multiple answers to a question from a document.} 
	\label{tab:multiple_answers_prompt}
\end{table*}

\subsection{Prompt used while fine-tuning} 
Table~\ref{tab:ft_prompt} shows the prompt used while fine-tuning \ourmethodshort\ using RAFT and \ourmethodshort.
Table~\ref{tab:qaft_prompt} shows the prompt for the DSF baseline that trains the model to generate a response to a given question without any retrieved passages in the context. 

\begin{table*}[h]
	\centering
     \resizebox{\textwidth}{!}{%
	\begin{tikzpicture}
		\node[draw=black, rounded corners=10pt, fill=gray!20, inner sep=10pt] (box) { % Create a rounded box
			\begin{tabular}{p{\linewidth}}
                    <s> [INST] You are an AI assistant who is provided with a conversation between a user and an agent. User utterances start with "User:" and agent utterances start with "Agent:".  Your task is to generate agent's response to the last user utterance. Enclosed within <passage> tags, you will find various excerpts. These passages may or may not contain the answer. You may or may not use the information in them to generate your response. Present your response within <response> and </response> tokens.
                    
                    <passage\_0>
                    \{document\_0\}
                    </passage\_0>
                    
                    <passage\_1> 
                    \{document\_1\}  
                    </passage\_1>  
                    
                    <passage\_2>
                    \{document\_2\}
                    </passage\_2>
                    
                    <passage\_3>
                    \{document\_3\}
                    </passage\_3>
                    
                    <passage\_4>
                    \{document\_4\}
                    </passage\_4>

                    User: \{data\_identifier\} \{question\}
                    
                    [/INST]
                    
                    \#\#\# Answer:
			\end{tabular}
		};
	\end{tikzpicture}
 }
	\caption{Prompt used while fine-tuning the base model. \{document\_i\} $\forall i = 1\cdots5$ are replaced by the retrieved passages; 
    \{question\} is replaced by the user question. For both book1 and book2, \{data\_identifier\} is replaced by "This query is with reference to IBM Redbooks".
    }
	\label{tab:ft_prompt}
\end{table*}


\begin{table*}[h]
	\centering
     \resizebox{\textwidth}{!}{%
	\begin{tikzpicture}
		\node[draw=black, rounded corners=10pt, fill=gray!20, inner sep=10pt] (box) { % Create a rounded box
			\begin{tabular}{p{\linewidth}}
                    <s> [INST] You are an AI assistant who is provided with a conversation between a user and an agent. User utterances start with "User:" and agent utterances start with "Agent:".  Your task is to generate agent's response to the last user utterance. Present your response within <response> and </response> tokens.

                    User: \{question\}
                    
                    [/INST]
                    
                    \#\#\# Answer:
			\end{tabular}
		};
	\end{tikzpicture}
 }
	\caption{Prompt used during Domain-specific fine-tuning (DSF) using only QA pairs without any passages; 
    \{question\} is replaced by the user question..
    }
	\label{tab:qaft_prompt}
\end{table*}

\subsection{Prompt for LLM-as-Judge evaluation}
The prompt in Table~\ref{tab:llm_as_judge_prompt} is used to evaluate the fine-tuned model's predicted answers against the ground truth answers, with Mixtral-8x22B-Instruct-v0.1 and LLaMA-3 70b serving as the evaluation judge.

\begin{table*}[h]
	\centering
    \resizebox{\textwidth}{!}{%
        \begin{tikzpicture}
		\node[draw=black, rounded corners=10pt, fill=gray!20, inner sep=10pt] (box) { % Create a rounded box
			\begin{tabular}{p{\linewidth}}
				<s> [INST] You are given a question, the corresponding ground-truth answer and a prediction from a model. Compare the "Ground-truth answer" and the "Prediction" to determine whether the prediction correctly answers the question. \\
                There should be no contradicting statements in a good prediction. The prediction may contain extra information. If the prediction states something as a possibility, treat it as a definitive answer. \\
                A good prediction must contain all the important information presented in the ground truths, but doesn't have to fully match it word by word. \\
                To make your decision, first read the question and Ground-truth answer carefully. Then compare the given Prediction with the Ground-truth answer in the light of the question. \\
                Start with an explanation and reasoning for your evaluation within <explanation> and </explanation> tags. \\
                Then, within <score> and </score> tokens, generate "1" if the Prediction is correct in the light of Ground-truth and question. Otherwise generate "0" if it is incomplete or incorrect.  \\
                Question: \{question\} \\
                Ground-truth answer: \{gold\_response\} \\
                Prediction: \{predicted\_response\} \\
                
                [\slash INST] <\slash s> \\
			\end{tabular}
		};
	\end{tikzpicture}
    };
	\caption{Prompts for Mixtral-8x22B-Instruct-v0.1-based evaluation. Here, \{question\}, \{gold\_response\}, and \{predicted\_response\} are placeholders that are replaced by the actual question, ground truth answer, and predicted answer, respectively.} 
	\label{tab:llm_as_judge_prompt}
\end{table*}

\subsection{Prompt for filtering test dataset}
The prompt in Table~\ref{tab:filtering_prompt} is used to filter dependent questions from the test set of the synthetic datasets. We used Mixtral-8x22B-Instruct-v0.1 to do the filtering.


\begin{table*}[h]
    \centering
    \resizebox{\textwidth}{!}{%
        \begin{tikzpicture}
            \node[draw=black, rounded corners=10pt, fill=gray!20, inner sep=10pt] (box) { % Create a rounded box
                \begin{tabular}{p{\linewidth}}
                    <s> [INST] You are given a question, and your task is to determine whether the question is complete and makes sense in isolation. A complete question should not contain text like "based on given example" or "mentioned in the chapter" because only question text is there to answer it, not the chapter or examples.  \\[2pt]
                    
                    Based on this, assign a final score of **Complete** or **Incomplete** for the question. Only assign **Incomplete** if the question uses phrases like these: "based on given example" or "mentioned in the chapter".  \\[2pt]
                    
                    **Scoring:** \\  
                    - **Complete:** The question contains all necessary details. \\  
                    - **Incomplete:** The question uses phrases like "based on given example", "mentioned in the chapter", "mentioned in the passage", "given in the document", or something similar.  \\[2pt]
                    
                    Provide your feedback in the following format: \\  
                    ``` \\ 
                    Feedback:::\\ 
                    Evaluation: (Provide your rationale for the scoring in detail)\\ 
                    Scoring: (State "Complete" or "Incomplete")\\  
                    ```  \\[2pt]
                    
                    **Important:** You must provide values for both 'Evaluation:' and 'Scoring:' in your answer.  \\[2pt]
                    
                    Now, here is the question:  \\  
                    **Question:** {\{question\}}  \\[2pt]
                    
                    Provide your feedback.  \\[2pt]
                    Feedback:::  \\  
                    Evaluation: [/INST] \\
                \end{tabular}
            };
        \end{tikzpicture}
    }
    \caption{Prompt for filtering test data using Mixtral-8x22B-Instruct-v0.1. Here, \{question\} is a placeholder that will be replaced by the actual question.}
    \label{tab:filtering_prompt}
\end{table*}


\section{Example Predictions}

\subsection{Example where the Retriever Fails}

As seen in Table \ref{tab:bad-ret}, \ourmethodshort\ is able to correctly recall the information necessary for answering the question even when the retrieved passages did not contain any relevant information. The RAFT method is expected to perform poorly conditional memorization bias, where as DSF is unable to recall the relevant information.

\begin{table}
	\centering
	\renewcommand{\arraystretch}{1.5}
	\begin{tabular}{l|p{13cm}}
	\toprule
 \textit{Question}&How can extended data tables be retrieved in IBM Storage Virtualize?\\ \midrule
 \textit{Gold Answer}& \colorbox{green}{Many ls commands} in IBM Storage Virtualize can leverage \colorbox{yellow}{the -gui parameter} to retrieve an extended data table similar to the output displayed in the IBM Storage Virtualize GUI. This provides more detailed information for all objects of the requested type and can be helpful for automation and scripting. \\ \midrule
 \textit{Base Model} & IBM Storage Virtualize does not directly support the concept of extended data tables as described in the provided passages, which involve creating external tables in Presto using data in IBM Storage Scale directories. Instead, IBM Storage Virtualize is a software-defined storage solution that provides virtualization of physical storage resources, enabling the consolidation and management of multiple storage systems into a single, unified storage pool.\\
  & To retrieve data from IBM Storage Virtualize, you can use various access methods such as block I/O (iSCSI), file I/O (POSIX compliant), or object storage (S3). The data can then be accessed using appropriate tools or applications based on the chosen access method.\\ 
 & If you need to work with data in a distributed file system like HDFS or use Spark for data processing, you may consider using IBM Watson for Data and AI solutions, such as IBM Watson Studio or IBM Watson Machine Learning, which can interact with IBM Storage Scale and provide the required functionality for managing and processing extended data tables.\\ \midrule
\textit{DSF+RAG} & Extended data tables can be retrieved in IBM Storage Virtualize by using the lsdeltas command with the -extended parameter. This command returns more detailed information about the changed objects, including the old and new values of the object attributes. \\ \midrule
\textit{RAFT} & Extended data tables can be retrieved in IBM Storage Virtualize by using the \colorbox{yellow}{-gui parameter} with the lsdrive command. This will retrieve more detailed information about each drive, including its capacity, status, and type. \\ \midrule
\textbf{\ourmethodshort} & To retrieve extended data tables in IBM Storage Virtualize, you can use the \colorbox{yellow}{-gui parameter} in \colorbox{green}{many ls commands}. This parameter allows you to retrieve an extended data table similar to the output displayed in the IBM Storage Virtualize GUI. For example, the lsdrive command with the -gui parameter will provide more detailed information about each drive, such as UID, block\_size, vendor\_id, product\_id, FRU\_part\_number, FRU\_identity, RPM, firmware\_level, FPGA\_level, quorum\_id, port\_1\_status, port\_2\_status, was\_spare, interface\_speed, write\_endurance\_used, transport\_protocol, compressed, physical\_capacity, physical\_used\_capacity, effective\_used\_capacity, date\_of\_manufacture, protection\_enabled, write\_endurance\_usage\_rate, replacement\_date, anomaly\_detection\_active, and so on. \\
\bottomrule
 \end{tabular}
 \caption{An example where the retriever fails and the models have to recall domain-specific information from their parametric knowledge.}
 \label{tab:bad-ret}
\end{table}


\subsection{Example where the Retriever is Successful}

Refer to Table \ref{tab:good-ret} for responses to a question where the retrieved passages contained the gold answer. We see that although all the methods are able to extract some relevant information from the provided context, they are unable to extract all the necessary information. However, \ourmethodshort\ captures all necessary information from the retrieved passages demonstrating its effectiveness in utilizing the retrieved data for accurate responses.

\begin{table}
	\centering
	\renewcommand{\arraystretch}{1.5}
	\begin{tabular}{l|p{13cm}}
	\toprule
 \textit{Question}&What are the options for configuring password policy in IBM Storage Virtualize?\\ \midrule
 \textit{Gold Answer}& The following attributes can be defined in password policy in IBM Storage Virtualize:
    - \colorbox{green}{Minimum password length}: \colorbox{yellow}{6-64 characters}.
    - Minimum number of:
    - \colorbox{green}{Uppercase characters}: \colorbox{yellow}{0-3}.
    - \colorbox{green}{Lowercase characters}: \colorbox{yellow}{0-3}.
    - \colorbox{green}{Special characters}: \colorbox{yellow}{0-3}.
    - \colorbox{green}{Digits}: \colorbox{yellow}{0-3}.
    - \colorbox{green}{History check} \colorbox{yellow}{(0-10)} before password reuse.
    - \colorbox{green}{Password expiry}: \colorbox{yellow}{0-365 days}.
    - \colorbox{green}{Password expiry warning} \colorbox{yellow}{(0-30 days)} which is displayed on CLI at login only.
    - \colorbox{green}{Password age} \colorbox{yellow}{(1-365 days)} which is the minimum age before a password can change. \\ \midrule
 \textit{Base Model} & The password policy in IBM Storage Virtualize can be configured through the GUI or CLI. Here are the steps to configure it through the GUI:
    1. Select `Settings` > `Security`. 2. Select the `Password Policies` tab. 3. Set the required password parameters in the `Password creation` and `Password expiration and account lockout` sections. 4. Click `Save`. Alternatively, you can use the `chsecurity` command in the CLI to change the password policy. For example, to set the minimum password length to 8 characters, you can use the following command: 
    ```bash IBM\_FlashSystem:ITSO:securityadmin $>$ chsecurity -minpasswordlength 8```. The `chsecurity` command also allows you to configure other password policy attributes such as minimum number of \colorbox{green}{uppercase}, \colorbox{green}{lowercase}, \colorbox{green}{special characters}, \colorbox{green}{digits}, \colorbox{green}{history check}, \colorbox{green}{password expiry}, and \colorbox{green}{password expiry warning}. 
    \dots
    Note that if you disable password reset and you do not have local or remote Security Administrator users except superuser, there is no way to restore management access if superuser password is lost.\\
\midrule
\textit{DSF+RAG} & The options for configuring password policy in IBM Storage Virtualize include setting \colorbox{green}{password expiration}, \colorbox{green}{password expiry warning}, \colorbox{green}{password history check}, \colorbox{green}{minimum password length}, \colorbox{green}{minimum number of uppercase characters}, \colorbox{green}{lowercase characters}, \colorbox{green}{special characters}, \colorbox{green}{digits}, and \colorbox{green}{minimum password age.} \\ \midrule
\textit{RAFT} & The options for configuring password policy in IBM Storage Virtualize include setting \colorbox{green}{minimum password length}, \colorbox{green}{minimum number of uppercase characters}, \colorbox{green}{lowercase characters}, \colorbox{green}{special characters}, and \colorbox{green}{digits, history check before password reuse}, \colorbox{green}{password expiry}, \colorbox{green}{password expiry warning}, and \colorbox{green}{password age}. \\ \midrule
\textbf{\ourmethodshort} & The options for configuring password policy in IBM Storage Virtualize are as follows:

    - \colorbox{green}{Minimum password length}: \colorbox{yellow}{6-64 characters}.
    - Minimum number of:
      - \colorbox{green}{Uppercase characters}: \colorbox{yellow}{0-3}.
      - \colorbox{green}{Lowercase characters}: \colorbox{yellow}{0-3}.
      - \colorbox{green}{Special character}s: \colorbox{yellow}{0-3}.
      - \colorbox{green}{Digits}: \colorbox{yellow}{0-3}.
    - \colorbox{green}{History check} \colorbox{yellow}{(0-10)} before password reuse.
    - \colorbox{green}{Password expiry}: \colorbox{yellow}{0-365} days.
    - \colorbox{green}{Password expiry warning} \colorbox{yellow}{(0-30 days)} which is displayed on CLI at login only.
    - \colorbox{green}{Password age} \colorbox{yellow}{(1-365 days)} which is the minimum age before a password can change.

    These attributes can be set in the GUI by selecting Settings $>$ Security, then selecting the Password Policies tab. The chsecurity command can also be used in the CLI to set the password policy. \\
\bottomrule
 \end{tabular}
 \caption{An example where the retriever is successful and the models must leverage the retrieved information efficiently to answer the questions.}
 \label{tab:good-ret}
\end{table}


\newpage

\section{Licenses}

\input{licenses}
