\appendix
\onecolumn

% \begin{center}{\bf {\LARGE Supplementary Materials}}\end{center}
% \begin{center}{\bf {\Large PARM: Poisoning Attacks on RAG-Equipped MLLMs}}

\section{Experimental Setup}
% Did you report the number of parameters in the models used, the total computational budget (e.g., GPU hours), and computing infrastructure used?

% If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation, such as NLTK, SpaCy, ROUGE, etc.), did you report the implementation, model, and parameter settings used?
\subsection{Implementation Details}
\label{appendix:implementation_details}
We evaluated the MLLM RAG system on an NVIDIA H100 GPU, allocating no more than 20 minutes per setting on the WebQA dataset (1,261 test cases). When training adversarial images against the retriever, reranker, and generator, we used a single NVIDIA H100 GPU for each model, and up to three GPUs when training against all three components in GPA-RtRrGen.

For the retriever, we used the average embedding of all queries and optimized the image to maximize similarity. Due to memory constraints, we adopted a batch size of 1 for both the reranker and generator. 
% We performed a grid search for hyperparameter selection with learning rate $\alpha \in \{0.005, 0.01\}$, retriever weight  $\lambda_1 \in \{0.1, 0.2, 0.3\}$, and reranker weight $\lambda_2 \in {}$.
The hyperparameters used in each setting are listed in Table~\ref{tab:hyper_parameters}. Each setting requires up to an hour of training.

We list the exact models used in our experiments in Table~\ref{tab:model_details}.









\input{table/hyper_parameters}
\input{table/model}

\subsection{Query Selection}
\label{appendix:query_selection}
The first step of our attack consists of selecting appropriate queries to attack. 
In order to evaluate our attack fairly, we filter out queries (i.e., text questions) that can be answered correctly without providing their associated multimodal context, since even if our attack against the retriever is successful, the MLLM could ignore the retrieved context.
To do this filtering, we employ LLaVA~\citep{liu2024llavanext} and Qwen-VL-Chat~\citep{bai2023qwen} asking them to provide an answer for each question in the datasets without providing the relevant context. Once this is done, we keep the queries where both models failed to provide the correct answer. This process results in 125 queries out of 229 being selected for MMQA and 1261 out of 2511 for WebQA.

\subsection{LPA-BB Data Generation}
\paragraph{Prompts}
\label{appendix:prompt}
In our poisoned data generation pipeline, we generate a wrong target answer and corresponding poisoned image caption for a given query. We specifically employ GPT-4o prompting it to generate both a wrong target answer and a poisoned image caption, using the following prompt:

\begin{tcolorbox}[colback=gray!10, colframe=black, title=Prompt for Poisoned Image Caption Generation]
Our objective is to create poisoned data for a multi-modal QA system to evaluate its robustness. For each question and its correct answer, please complete the following tasks:
\begin{enumerate}
    \item Create an incorrect answer that differs from the correct one.
    \item Craft a misleading image caption, which will be used to generate a poison image further. This poison image, when used as context for the question, will lead the system to generate the incorrect answer. Additionally, ensure the image will be retrieved based on the question's context. For example, if the question pertains to a movie cover, the poisoned image should also represent a movie cover, including essential details like the title.
\end{enumerate}
The provided question and correct answer are as follows:\\
\textbf{Question:} \{\{ question \}\} \\
\textbf{Correct answer:} \{\{ correct\_answer \}\}

Please format your response as a JSON object, structured as follows:

\begin{verbatim}
{
  "wrong_answer": "...",
  "poison_image_caption": "..."
}
\end{verbatim}
\end{tcolorbox}

Then, to generate the poisoned images, we use Stable Diffusion~\citep{rombach2022high} conditioned on the poisoned image captions generated by GPT-4o. Specifically, we employ the \texttt{stabilityai/stable-diffusion-3.5-large} model from Hugging Face, with the classifier free guidance parameter set to $3.5$ and the number of denoising steps set to $28$.

\section{Additional Experimental Results}
\subsection{Localized and Globalized Poisoning Attack Results on other MLLMs.}
\label{sec:other_mllm}
In addition to the results in the main paper, which use the same MLLMs for the reranker and generator, we further evaluate our attacks when different LLMs are used. Specifically, we consider a heterogeneous setting where Llava is used for the reranker and Qwen for the generator, with results shown in Table~\ref{tab:mmqa_lpa_hetero}. We observe that our attack is less effective in this setting, likely because the differing embedding spaces of the reranker and generator increase the optimization challenge.
\input{table/table_appendix}


\subsection{Transferability of \textsc{MM-PoisonRAG}}
\label{sec:transfer_appendix}
\input{figure/app_transferability}
As described in Sec~\ref{sec:transfer}, LPA-BB and LPA-Rt readily transfer across retriever variants, enabling poisoned knowledge generated from one retriever to manipulate the generation of RAG with other types of retriever towards the poisoned answer, while reducing retrieval recall and accuracy of the original context. This occurs because LPA-Rt produces poisoned images that remain close to the query embedding, even when transferred to another retriever (e.g., OpenCLIP), maintaining their position in the image embedding space (Fig~\ref{fig:app_transfer}). In contrast, GPA-RtRrGen demonstrates lower transferability, as its poisoned image embedding is positioned in the text embedding space within the CLIP model, but their distribution shifts significantly when applied to OpenCLIP models with placed on the image embedding space, reducing effectiveness. However, despite this limitation, GPA-RtRrGen remains highly effective in controlling the entire RAG pipeline, including retrieval and generation, even with a single adversarial knowledge injection.

\section{Examples of Generated Poisoned Knowledge}
\label{appendix:examples}

\input{figure/original_examples}

\input{figure/lpa_bb_examples}

\input{figure/lpa_rt_examples}

\input{figure/gpa_examples}


