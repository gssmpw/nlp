\section{Conclusions and Future Work}

In this work, we identify critical safety risks in multimodal RAG frameworks, demonstrating how knowledge poisoning attacks can exploit external multimodal KBs. Our localized and globalized poisoning attacks reveal that a single adversarial knowledge injection can misalign retrieval and manipulate model generation towards attacker-desired responses, even without direct access to the RAG pipeline or KB content. These findings highlight the vulnerabilities of multimodal RAG systems and emphasize the need for robust defense mechanisms. 
Advancing automatic poisoning detection and strengthening the robustness of cross-modal retrieval is a necessary and promising direction for research in the era of MLLMs-based systems relying heavily on retrieving from external KBs.

% arising from the integration of external multimodal knowledge bases. While these frameworks enhance generation via cross-modal retrieval, we demonstrate that injecting imperceptible poisoned knowledge misaligns cross-modal representations, leading to cascading failures from retrieval to generation. 
% To expose these vulnerabilities, we propose two novel knowledge poisoning attacks: 1) localized poisoning attack, which injects query-specific poisoned knowledge, forcing the generator to produce an attacker-defined wrong answer, and 2) globalized poisoning attack, which injects a single poisoned sample, causing widespread degradation by generating random incorrect answers across all queries. 

% Notably, even a single poisoned knowledge from our attacks significantly degrades multimodal RAG performance--all without requiring access to model parameters or original KB content. These findings underscore the urgent need for robust defense mechanisms to safeguard multimodal RAG frameworks, marking this as a pressing direction for future research.

% \heng{the conclusion here is too detailed. you can shorten it, and only highlights the main findings and insights}

% \heng{add future work items. add ethical considerations and limitations. in ethical statement, try to emphasize that the goal is not to introduce new techniques to bad actors, but rather point out the potential risks of MLLM-RAG methods because they are very vulnerable, so it can attract more research on how to make them more robust to these attacks}
% \ember{future work: focus on adaptive and stronger retrieval defenses, cross modal alignment safeguards to mitigate the risks posed by knowledge poisoning attacks on multimodal RAG.}
% \heng{Add future work: it will be very interesting to focus on developing automatic methods to detect poison in multimodal RAG.}