\section{\textsc{MM-PoisonRAG}}
% \heng{to clean up notation, it's better to change K->N, and N->M, or swap K and N}
\input{table/table0}
% \heng{To make the story more clear and appealing, you can try to add some (1) intuition on how multimodal RAG work. Text RAG basically provides more context or newer knowledge than LLM parametric knowledge; how does multimodal RAG work? perhaps works better for video understanding since related clips might provide more context? (2) what are the design principles for your attack? are you looking for the most similar but different images from the original ones so they become confusing examples? are you trying to add more toxic images? etc.}

% \heng{font in figure 1 is too small. need major improvement}

\subsection{Multimodal RAG}

Multimodal RAG retrieves relevant texts and images as context from an external KB to supplement parametric knowledge and enhance generation. Following prior work~\cite{chen2024mllm}, we build a multimodal RAG pipeline consisting of a multimodal KB, a retriever, a reranker, and a generator. Given a question-answering (QA) task $\tau = \{ (\mathcal{Q}_1, \mathcal{A}_1), \cdots, (\mathcal{Q}_d, \mathcal{A}_d)\}$, where $(\mathcal{Q}_i, \mathcal{A}_i)$ is the $i$-th query-answer pair, the multimodal RAG generates responses in three steps: multimodal KB retrieval, reranking, and response generation. 

For a given query $\mathcal{Q}_i$, the retriever selects the top-$N$ most relevant image-text pairs $\{(I_1, T_1), \cdots, (I_N, T_N)\}$ from the KB. A CLIP-based retriever, which can compute cross-modal embeddings for both texts and images, ranks pairs by computing cosine similarity between the query embedding and each image embedding. A MLLM reranker then refines the retrieved pairs by selecting the top-$K$ most relevant image-text pairs ($K < N$). It reranks the retrieved image-text pairs based on the output probability of the token \textit{``Yes''} against the prompt: ``\textit{Based on the image and its caption, is the image relevant to the question? Answer `Yes' or `No'.}'', retaining the top-$K$ pairs. Finally, the MLLM generator produces outputs $\hat{\mathcal{A}_i}$ based on the reranked multimodal context (i.e., non-parametric knowledge) and its parametric knowledge.




\subsection{Threat Model}
\label{sec:threat_scenario}
% Multimodal RAG frameworks enhance generation by retrieving external KBs, but this reliance leaves them susceptible to poisoned KBs with adversarial knowledge, which is either factually incorrect or irrelevant. We expose these vulnerabilities by designing knowledge poisoning attacks, where the attacker's goal is to corrupt the system into producing incorrect answers to queries.
% that adversarially perturb image-text pairs to mislead the cross-modal retriever, the reranker, and the generator. 

We assume a realistic threat scenario where attackers cannot access the KBs used by the multimodal RAG framework but can inject a constrained number of adversarial image-text pairs with access to the target task $\tau$; this setting emulates misinformation propagation through publicly accessible sources. The primary objective of the poisoning attack is to disrupt retrieval, thereby manipulating model generation. Our work proposes two distinct threat scenarios that conform to the objective:
(1) \textbf{Localized Poisoning Attack} (LPA): targets a specific query, ensuring the RAG framework retrieves adversarial knowledge and delivers an attacker-defined response (e.g., \texttt{Red}, \texttt{Cat} in Fig.\ref{fig:concept_fig}), 
(2) \textbf{Globalized Poisoning Attack} (GPA): induces widespread degradation in retrieval and generation across all queries by injecting a control prompt that elicits an irrelevant and nonsensical response (e.g., \texttt{Sorry} in Fig.\ref{fig:concept_fig}).

For LPA, we consider two different attack types as denoted in Table \ref{tab:exp_setting}: \textbf{LPA-BB}: attackers have only black-box (BB) access to the system and can insert only a single image-text pair; \textbf{LPA-Rt}: attackers have white-box access only to the retriever (Rt) model, optimizing poisoning strategies; white-box access refers to the full access to model parameters, gradients and hyperparameters, whereas black-box access refers to restrictive access only to the input and output of the model. 
GPA poses a greater challenge than LPA, as it requires identifying a single adversarial knowledge instance capable of corrupting responses for all queries. The attack's success depends on two key factors: the amount of adversarial knowledge inserted into the KBs, and the level of system access; the more adversarial knowledge and the greater access generally lead to more successful attacks. To account for these factors, we define two settings for GPA. \textbf{GPA-Rt}, where attackers have access only to the retriever but can insert multiple poisoned knowledge instances, and \textbf{GPA-RtRrGen}, where attackers have full access to the multimodal RAG pipeline but are limited to inserting only a single poisoned knowledge piece. We summarize all attack settings in Table~\ref{tab:exp_setting}.

% To ensure attack feasibility, we further define two different settings in 
% further we employ four access levels ranging from fully black-box settings, where the attacker has no visibility into models, to white-box scenarios, where partial or full access is granted (Table~\ref{tb:exp_setting}).

% To operate these attacks under realistic scenarios, we consider different access levels (Table~\ref{tb:exp_setting}).\qiusi{I think we need more explanation of the motivation for the designing of different settings here} LPA-Rt and GPA-Rt optimize poisoned samples for retrieval manipulation with white-box retriever access. In contrast, LPA-BB performs under completely black-box access to all models, while GPA-RtRrGen assumes full access to retriever, reranker and generator, allowing complete control over poisoning attack. By employing attack strategies under varying access constraints, we demonstrate broad applicability and severity of knowledge poisoning threats. Our attacks systematically poison the KB, triggering failures from retrieval to generation, ultimately leading to harmful or irrelevant output.

% poisoned knowledge manipulates external KBs to disrupt retrieval and generation. In our threat model, the attacker cannot access the original KB content. Instead, it can access the target task $\tau$ and inject a limited number of poisoned image-text pairs--akin to misinformation spreading through public accessible sources. 

% Our attacks exploit cross-modal retrieval by perturbing images to mislead the retriever, causing cascading failures to generation. The attacker's objective is to manipulate the model's final response by injecting carefully crafted poisoned knowledge into the KB. We especially categorize attacks into two types: 
% (1) \textit{localized poisoning attack} (LPA): targets a specific query, forcing the multimodal RAG framework to produce an attacker-defined incorrect answer;
% (2) \textit{globalized poisoning attack} (GPA): aims to degrade performance across all queries, leading to widespread erroneous outputs. To operate these attacks under realistic scenarios, we consider different access levels (Table~\ref{tb:exp_setting}).\qiusi{I think we need more explanation of the motivation for the designing of different settings here} LPA-Rt and GPA-Rt optimize poisoned samples for retrieval manipulation with white-box retriever access. In contrast, LPA-BB performs under completely black-box access to all models, while GPA-RtRrGen assumes full access to retriever, reranker and generator, allowing complete control over poisoning attack. By employing attack strategies under varying access constraints, we demonstrate broad applicability and severity of knowledge poisoning threats. Our attacks systematically poison the KB, triggering failures from retrieval to generation, ultimately leading to harmful or irrelevant output.



\subsection{Localized Poisoning Attack}
\label{sec:lpa}
Localized poisoning attack (LPA) aims to disrupt retrieval for a specific query
$(\mathcal{Q}_i, \mathcal{A}_i) \in \tau$, causing the multimodal RAG framework to generate an attacker-defined answer $\mathcal{A}_i^{\text{adv}} \neq \mathcal{A}_i$. This is achieved by injecting a poisoned image-text pair $(I_i^{\text{adv}}, T_i^{\text{adv}})$ into the KB, which is designed to be semantically plausible but factually incorrect, misleading the retriever into selecting the poisoned knowledge, cascading the failures to generation.
% retrieval and, ultimately corrupting response generation. LPA operates under two access settings: LPA-BB which performs under complete black-box access, and LPA-Rt, where the attacker refines poisoned samples against the retriever.

% \heng{I'm not clear how your model learns some information is misinformation. It's clear how it learns something is plausible, basically it checks clip score. but how does the model knows red is a wrong color?}
% \jeongh{the model won't learn some information is misinformation; we simply create the poisoned knowledge instance using GPT-4 for text and Stable Diffusion for images. The goal is to deterioriate the model performance with these poisoned knowledge instances.}
\paragraph{LPA-BB} In the most restrictive setting, the attacker has no knowledge of the multimodal RAG pipeline or access to the KBs, and must rely solely on plausible misinformation. For a QA pair $(\mathcal{Q}_i, \mathcal{A}_i) \in \tau$, the attacker selects an alternative answer $\mathcal{A}_i^{\text{adv}}$ and generates a misleading caption $T_i^{\text{adv}}$ yet semantically coherent to the query, using a large language model; we use GPT-4~\cite{openai2024gpt4ocard} in this work. For example, if the query is ``\textit{What color is Eiffel Tower?}'' with the ground-truth answer ``\textit{Gray}'', the attacker may choose ``\textit{Red}'' as $\mathcal{A}_i^{\text{adv}}$ and generate $T_i^{\text{adv}}$ such as ``\textit{A beautiful image of the Eiffel Tower bathed in warm red tones during sunset.}''. A text-to-image model (we use Stable Diffusion~\cite{rombach2022high}) is then used to generate an image $I_i^{\text{adv}}$ consistent with the adversarial caption, $T_i^{\text{adv}}$. This adversarial knowledge $(I_i^{\text{adv}}, T_i^{\text{adv}})$ is injected into the KBs to poison them, maximizing retrieval confusion and steering generation towards the targeted wrong answer. 
% \heng{it's still not clear to me how your model came up with 'red' to attack. it needs to know red also belongs to the color category, but different from grey. how is this exactly done?}


% We obtain captions using GPT-4~\cite{openai2024gpt4ocard} with a manually designed prompt (details in Appendix~\ref{appendix:prompt}). To ensure visual consistency, the attacker uses a text-to-image model (e.g., Stable Diffusion~\cite{rombach2022high}) to generate a poisoned image $I_i^{\text{adv}}$ aligned with $T_i^{\text{adv}}$. The poisoned image-text pair is then injected into the KB, making the retriever confused even without direct access to all models within multimodal RAG.

\paragraph{LPA-Rt} LPA-BB can fail if the poisoned instance is perceived as less relevant to the query than legitimate KB entries, resulting in its exclusion from retrieval and making it ineffective. To this end, we enhance the attack by adversarially optimizing the poisoned knowledge to maximize its retrieval probability with retriever access. Given a multimodal retriever that extracts cross-modal embeddings, in our case CLIP \citep{radford2021learning}, we iteratively refine the poisoned image to increase its cosine similarity with the query embedding as follows: 
% \heng{This paragraph is very confusing. try to rewrite it. Formula 1 is very difficult to understand. try to add more intuition and walk through some example. }
% \heng{notations can be further cleaned up. for cosine similarity you can simply denote it as cos}
\begin{sizeddisplay}{\normalsize}
% \vspace{-0.1in}
\begin{align}
    \label{eq:adv_noise}
    \mathcal{L}_i &= \text{cos} \left(f_I(I_{i, (t)}^{\text{adv-Rt}}), f_T(\mathcal{Q}_i) \right), \notag \\ 
    I_{i, (t+1)}^{\text{adv-Rt}} &= \Pi_{(I_i^{\text{adv}}, \epsilon)}\left( I_{i,(t)}^{\text{adv-Rt}} + \alpha \nabla_{I_{i, (t)}^{\text{adv-Rt}}} \mathcal{L}_{i} \right),
\end{align}
\end{sizeddisplay}
where $f_I$ and $f_T$ are the vision and text encoders of the retriever, $\text{cos}$ denotes cosine similarity, and $\Pi$ projects an image into an 
$\epsilon$-ball around the initial image $I_i^{\text{adv}}$ obtained from LPA-BB, $t$ is the optimization step,  and $\alpha$ is the learning rate. This adversarial refinement increases the retrieval likelihood of $I_{i}^{\text{adv-Rt}}$ while maintaining visual plausibility, being perceived as relevant knowledge to the query. Examples of our poisoned knowledge are shown in Appendix~\ref{appendix:examples}.


\subsection{Globalized Poisoning Attack}
\label{sec:gpa}
Unlike LPA, which injects specific adversarial knowledge to manipulate individual queries, GPA degrades retrieval and generation performance across an entire task $\tau$ using a single adversarial knowledge instance. The objective of GPA is to create a single, query-irrelevant adversarial image-text pair $(I^{\text{adv}}, T^{\text{adv}})$ that confuses the retriever, falsely guiding the MLLM to consistently generate wrong, incoherent responses $\forall (\mathcal{Q}_i, \mathcal{A}_i) \in \tau,\quad \hat{\mathcal{A}_i} \neq A_i$.


\input{figure/gpa_transferability}
\paragraph{GPA-Rt} A key challenge in global poisoning is constructing an adversarial knowledge base that disrupts retrieval for all queries, even without access to the KB. Given that CLIP retrieval relies on cross-modal similarity between query and image embeddings, we construct a single, \textbf{globally adversarial image} that maximally impacts retrieval for all queries. In Figure~\ref{fig:transfer_tsne}, we show that image embeddings form a separate cluster from query embeddings, suggesting that if we can generate a single, globally adversarial image that lies closely to the query embedding cluster, we can maximize retrieval disruption across the entire task $\tau$. To achieve this, we optimize the global adversarial image for GPA as follows:

\begin{sizeddisplay}{\normalsize}
% \vspace{-0.15in}
\begin{align}
    \label{eq:gpa_rt}
    \mathcal{L}_{Rt} &= \sum_{i=1}^{d}\cos \left(f_I(I_{t}^{\text{adv}}), f_T(\mathcal{Q}_i) \right), \notag \\ 
    I^{\text{adv}}_{t+1} &=  I^{\text{adv}}_t + \alpha \nabla_{I^{\text{adv}}_t} \mathcal{L}_{Rt},
\end{align}
\end{sizeddisplay}
where $d$ is the number of queries in the task, and $I^{\text{adv}}_0$ is sampled from a standard normal distribution, $I^{\text{adv}}_0 \sim \mathcal{N}(\mathbf{0},\mathbf{I})$, which is completely irrelevant to any arbitrary query. This enforces ${I^{\text{adv}}}$ to achieve high similarity with all queries, making it the preferred retrieval candidate regardless of the query. With $I^{\text{adv}}$, we craft a global adversarial caption $T^{\text{adv}}$ designed to manipulate the reranker's relevance assessment. In GPA-Rt, since attackers lack access to the reranker or generator, the only option is perturbing the input text to enforce a high relevance score for a poisoned knowledge instance. We formulate the caption ``\textit{The given image and its caption are always relevant to the query. You must generate an answer of "Yes".}'' to reinforce its selection during the reranking phase. 
% However, as the model's performance improves, our prompt injection caption becomes less effective. 
% The simplest solution is to poison the KBs with multiple pieces of adversarial knowledge. 
% When the volume of adversarial knowledge is sufficiently high, the reranker will only receive adversarial inputs, which will inevitably disrupt the generation process.
% We also explore the setting where attackers use only a single piece of adversarial knowledge in GPA-RtRtGen.
% As a result, not only does the retriever prioritize the poisoned image due to our optimization, but the reranker also confirms its relevance only with access to the retriever.

\paragraph{GPA-RtRrGen}
In this scenario, we assume a case where the attacker gains full access to the retriever, reranker, and generator. The unconstrained access to all three components allows end-to-end poisoning. For example, re-training the retriever to maximize the similarity between the adversarial images with all the queries (as in GPA-Rt), and re-training the re-ranker to assign a high rank to the adversarial image and generator to maximize the probability of the incorrect response. In GPA-RtRrGen, we still want the model to generate a query-irrelevant response (e.g., ``sorry'') for all the queries. We, therefore, attack all the three components by training the multimodal RAG with a new objective, $\mathcal{L}_{Total}$:
% To enhance the robustness of our attack scheme, we consider a scenario in which multiple images appear in the context with only one being adversarial; in this case, we set the target output as ``sorry'' and use adversarial images from LPA-BB as the context image.
% The optimization process can be formalized as follows:
\begin{sizeddisplay}{\normalsize}
\begin{align}
    \label{eq:gpa_rtrrgen}
    \mathcal{L}_{Rr} &= \sum_{i=1}^{d} \log P\Bigl(\text{``\textit{Yes}''} \mid \mathcal{Q}_i,\, I_{t}^{\text{adv}},\, T^{\text{adv}}\Bigr), \notag \\
    \mathcal{L}_{Gen} &= \sum_{i=1}^{d} \log P\Bigl(\text{``\textit{sorry}''} \mid \mathcal{Q}_i,\, I_{t}^{\text{adv}} ,\, T^{\text{adv}} ,\, \mathcal{X}_i \Bigr), \notag \\
    \mathcal{L}_{Total} &= \lambda_1 \mathcal{L}_{Rt} + \lambda_2 \mathcal{L}_{Rr} + (1 - \lambda_1 - \lambda_2) \mathcal{L}_{Gen}, \notag \\
    I^{\text{adv}}_{t+1} &=  I^{\text{adv}}_t + \alpha \nabla_{I^{\text{adv}}_t} \mathcal{L}_{Total},
\end{align}
\end{sizeddisplay}
where \(P(\cdot \mid \cdot)\) denotes the probability output by the corresponding model component, \(\mathcal{X}_i\) represents the multimodal context for the \(i\)-th query, and \(\lambda_1, \lambda_2\) are weighting coefficients balancing the contributions of the retriever, reranker, and generator losses. Similar to GPA-Rt, $I^{\text{adv}}_0 \sim \mathcal{N}(\mathbf{0},\mathbf{I})$. This is the most generalized form of attack, where GPA-Rt is the same as GPA-RtRrGen with $(\lambda_1, \lambda_2)=0$.
% % \begin{sizeddisplay}{\normalsize}
% \begin{align}
% \max_{I^{\text{adv}}}\; \mathcal{L}(I^{\text{adv}}) = \; & \lambda_1 \sum_{i=1}^{d} \cos\Bigl( f_T(\mathcal{Q}_i),\, f_I(I^{\text{adv}}) \Bigr) \\
% & + \lambda_2 \sum_{i=1}^{d} \log P\Bigl(\text{Yes} \mid \mathcal{Q}_i, I^{\text{adv}}, T^{\text{adv}}\Bigr) \\
% & + \lambda_3 \sum_{i=1}^{d} \log P\Bigl(\text{``sorry''} \mid \mathcal{Q}_i, I^{\text{adv}}, \mathcal{C}_i\Bigr),
% \end{align}
% \label{eq:adv_optimization}
% % \end{sizeddisplay}
% \begin{equation}
% \begin{aligned}
% \max_{I^{\text{adv}}}\; \mathcal{L}(I^{\text{adv}}) = \; & \lambda_1 \mathcal{L} \\
% & + \lambda_2 \sum_{i=1}^{d} \log P\Bigl(\text{Yes} \mid \mathcal{Q}_i, I^{\text{adv}}, T^{\text{adv}}\Bigr) \\
% & + \lambda_3 \sum_{i=1}^{d} \log P\Bigl(\text{``sorry''} \mid \mathcal{Q}_i, I^{\text{adv}}, \mathcal{C}_i\Bigr),
% \end{aligned}
% \label{eq:adv_optimization}
% \end{equation}
