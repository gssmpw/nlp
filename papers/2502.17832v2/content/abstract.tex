\begin{abstract}


Multimodal large language models (MLLMs) with Retrieval Augmented Generation (RAG) combine parametric and external knowledge to excel in many tasks such as Question Answering. While RAG enhances MLLMs by grounding responses in query-relevant external knowledge, this reliance poses a critical yet underexplored safety risk: \textit{knowledge poisoning attacks}, where misinformation or irrelevant knowledge is intentionally injected into external knowledge bases to manipulate model outputs to be incorrect and even harmful.
To expose such vulnerabilities in multimodal RAG, we propose \textsc{MM-PoisonRAG}, a novel knowledge poisoning attack framework with two attack strategies: \textit{Localized Poisoning Attack} (LPA), which injects query-specific misinformation in both text and images for targeted manipulation, and \textit{Globalized Poisoning Attack} (GPA) to provide false guidance during MLLM generation to elicit nonsensical responses across all queries. 
We evaluate our attacks across multiple tasks, models, and access settings, demonstrating that LPA successfully manipulates the MLLM to generate attacker-controlled answers, with a success rate of up to 56\% on MultiModalQA. Moreover, GPA completely disrupts model generation to 0\% accuracy with just a single irrelevant knowledge injection. Our results highlight the urgent need for robust defenses against knowledge poisoning to safeguard multimodal RAG frameworks. Code is available at \href{https://github.com/HyeonjeongHa/MM-PoisonRAG}
{https://github.com/HyeonjeongHa/MM-PoisonRAG}.

% \heng{later in the experiments you achieve near zero performance on some tasks? can you add that in summary? something like "For XXX tasks our attack completely destroys the model and makes its performance near to zero"}



% LPA reduces generation accuracy by over 30\% even with black-box access, 

% Without full model access, our attack significantly degrades the retrieval performance by up to 74.4\% and generation quality by 45.6\% on MMQA tasks,
% \heng{list of all tasks you used, maybe also spell them out. only if you get time you can also look into experiments with MuMuQA which is about multimodal news QA: https://blender.cs.illinois.edu/paper/mumuqa2022.pdf https://github.com/blender-nlp/MuMuQA}
% even when injecting only a single poisoned knowledge per query. 

% \heng{need to define what is knowledge poisoning}
%\heng{Motivation needs to be sharper - is this a major problem for current RAG for MLLMs? how serious is the problem? give some numbers/stats}
% Multimodal Large Language Models (MLLMs) have demonstrated remarkable performance in handling complex reasoning tasks by combining multiple data modalities, such as images and text. The integration of Retrieval-Augmented Generation (RAG) further enhances the knowledge capacity of MLLMs by incorporating an external multimodal knowledge base into language generation, improving the factual accuracy and coverage. However, this reliance on external knowledge retrieval introduces a crucial yet underexplored vulnerability: \textit{multimodal knowledge poisoning}, wherein attackers manipulate the model output by injecting adversarial instances into the knowledge base.

% In this work, we present PAMRAG, the first knowledge poisoning attack designed for multimodal RAG frameworks. PAMRAG targets cross-modal vulnerabilities to inject a small set of adversarial image-text pairs into the knowledge base, causing significant disruptions in the model's performance. 
% Notably, PAMRAG achieves this without access to RAG components (i.e., black-box setting), making it practical. \qiusi{more aspects should be related to practical in addition to black-box to RAG}
% Our experimental results across different models and multimodal question-answering tasks demonstrate the vulnerabilities in the multimodal RAG framework by achieving a 43.4\% and 58.2\% attack success rate in black-box and white-box access to the retriever, respectively. These findings highlight the pressing need for robust and adaptive defense mechanisms to secure these systems against adversarial manipulation.
% Code is available at \href{https://github.com/HyeonjeongHa/poison\_attack\_multirag}{https://github.com/HyeonjeongHa/poison\_attack\_multirag}.
\end{abstract}
