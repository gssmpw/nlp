\section{Introduction}
\input{figure/concept_fig}
% \heng{font in figure 1 is still too small}
% Consider an AI-driven e-commerce assistant powered by multimodal large language models (MLLMs) to answer product-related queries by integrating visual and textual data. However, without real-time updates, it may misinterpret visual cues and recommend outdated or harmful recommendations, such as counterfeit or recalled replacement parts, posing severe safety risks.
% \violet{not sure why we start with this motivating example -- do we tested on some data related to e-commerce application? If not, suggest remove or move it to later sections.}
The rapid adoption of Multimodal large language models (MLLMs) has highlighted their unprecedented generative and reasoning capabilities across diverse tasks, from visual question answering to chart understanding \citep{tsimpoukelli2021multimodal, lu2022dynamic, chart2023}. MLLMs, however, heavily rely on parametric knowledge, making them prone to long-tail knowledge gaps \citep{asai2024reliable} and hallucinations \citep{ye2022unreliability}. Multimodal RAG frameworks~\cite{chen2022murag, yasunaga2022retrieval, chen2024mllm} mitigate these limitations by retrieving query-relevant textual and visual contexts from external knowledge bases (KBs), improving response reliability. \looseness=-1
% \looseness=-1

% \heng{to further improve the motivation, you can show some accuracy number of state-of-the-art cross-modal IR using CLIP and cite. Then you can say usually RAG is usually more effective than LLM because text-to-text retrieval usually has much higher accuracy} 


% \heng{I still think we need a better definition of poison. is it about irrelevant information? factually false information? harmful information? talk about the types your paper is targeting at}
However, incorporating KBs into multimodal RAG introduces new safety risks: retrieved knowledge may not always be trustworthy~\cite{hong-etal-2024-gullible, tamber2025illusions}, as false or irrelevant knowledge can be easily injected. Unlike text-only RAG, multimodal RAG presents unique vulnerabilities due to its reliance on cross-modal representations during retrieval. Prior works \cite{yin2024vlattack, wu2024adversarial, schlarmann2023adversarial} have shown that even pixel-level noise can disrupt cross-modal alignment and propagate errors from retrieval to generation, leading to incorrect or harmful outputs.
% This failure may propagate from retrieval to generation, causing misinformation or harmful outputs. 
For example, a document containing counterfactual information injected among the top-N retrieved documents can easily mislead LLMs to generate false information~\cite{hong-etal-2024-gullible}.
% injecting counterfactual information among the top-N retrieved documents can easily mislead an MLLM into generating false information~\cite{hong-etal-2024-gullible}.
% \kw{I would suggest slightly reorganizing the first two paragraphs and shortening them a bit. The first paragraph talks about MLLM and RAG (you don't need to spend too much space on arguing MLLM is useful as people already know it). The second paragraph highlights the safety risk of RAG. }

In this work, we propose \textbf{\textsc{MM-PoisonRAG}}, the first knowledge poisoning attack on multimodal RAG frameworks, revealing vulnerabilities posed by poisoned external KBs. 
In \textsc{MM-PoisonRAG}, the attacker's goal is to corrupt the system into producing incorrect answers. The attacker accomplishes this by injecting adversarial knowledge—factually incorrect or irrelevant—into the KBs, thereby compromising the system’s retrieval and generation.
% Our attack injects adversarial knowledge containing either factually incorrect or irrelevant information. The injected knowledge is systematically designed by exploiting the retriever representations to impose adversarial perturbations on images (\S \ref{sec:lpa}, \ref{sec:gpa}), which effectively propagates failures from retrieval to generation. 
\textsc{MM-PoisonRAG} employs two attack strategies tailored to distinct attack scenarios: (1) \textbf{Localized Poisoning Attack (LPA)} injects query-specific \textit{factually incorrect} knowledge that appears relevant to the query, steering MLLMs to generate targeted, attacker-controlled misinformation. For instance, in an AI-driven e-commerce assistant, a malicious seller could subtly modify product images, leading to false recommendations or inflated ratings for low-quality items. (2) \textbf{Globalized Poisoning Attack (GPA)} introduces a single \textit{irrelevant} knowledge instance that is perceived as relevant for all queries, disrupting the entire RAG pipeline and leading to the generation of irrelevant or nonsensical outputs. For example, generating ``Sorry'' to a question ``What color is the Eiffel Tower?'' (Fig.\ref{fig:concept_fig}).
% illustrates an instance of a single point of failure cascading throughout the multimodal RAG framework. 
For both LPA and GPA, we use a realistic threat model (\S \ref{sec:threat_scenario}) where attackers do not have direct access to the KBs but can inject adversarial knowledge instances.
% We also define a realistic threat model, where attackers cannot directly access the original multimodal KBs but can inject a limited number of poisoned knowledge. 
 % \heng{not sure what poisoned context means. maybe show the framework figure at the beginning, and show examples of a use case, what is external knowledge, and how it could be manipulated.}
 % \heng{does the previous work have limitations on not being able to transfer across retrievers? if so emphasize your novelty}
% \qiusi{After finalizing our settings, we should provide a more comprehensive description of our methods here}

We evaluate \textsc{MM-PoisonRAG} on MultimodalQA (MMQA)~\cite{talmor2021multimodalqa} and WebQA tasks~\cite{chang2022webqa} under various attack settings. 
Our results show that LPA successfully manipulates generation, achieving a 56\% success rate in producing the attacker’s predefined answer—five times the model’s 11\% accuracy for the ground-truth answer under attack.
% Our results show that LPA successfully manipulates generation, achieving a fivefold increase in response accuracy for attacker-controlled answers compared to the ground-truth ones. 
This demonstrates how a single misinformation instance can disrupt retrieval and propagate errors through generation. Moreover, GPA completely nullifies generation, leading to the final accuracy of 0\% (Table \ref{tab:gpa_results}). Notably, despite the lack of access to the retriever (e.g., CLIP~\cite{radford2021learning}), LPA exhibits strong transferability across retriever variants (\S\ref{sec:transfer}), emphasizing the need for developing robust defenses against knowledge poisoning attacks to safeguard multimodal RAG frameworks. 

% our attacks lead to consistent performance drops in retrieval and generation, even reducing accuracy to zero. \kw{Like I mentioned in email, it might be better to highlight the attack success rate that how adversarial can control the output than the accuracy drop. }
% Notably, despite lacking direct access to the multimodal retriever (e.g., CLIP~\cite{radford2021learning}), our attacks exhibit strong transferability across retriever variants, significantly degrading their performance (\S\ref{sec:transfer}), emphasizing the need for developing robust defenses against knowledge poisoning attacks to safeguard multimodal RAG frameworks. 

% \heng{did you try to combine LPA + GPA? seems to be more powerful}

% By exposing these vulnerabilities, we highlight the pressing challenge of securing retrieval-augmented multimodal systems and call for the development of adversarially robust retrieval mechanisms to safeguard real-world AI applications.
% \qiusi{we also include multiple data points for this setting?} 
% 1st version of introduction
% Multimodal large language models (MLLMs) have recently demonstrated remarkable capabilities in integrating and reasoning over data from multiple sources, such as images and texts~\cite{tsimpoukelli2021multimodal, llava, liu2024llavanext}. This ability has driven their success in tasks ranging from Visual Question Answering~\cite{goyal2017making,hudson2019gqa,marino2019ok}, Table Question Answering~\cite{lu2022dynamic}, Chart Understanding~\cite{chart2023,lvlmchart2024}, and Text-to-image Generation~\cite{ramesh2021zero,yu2022scaling,alayrac2022flamingo, aghajanyan2022cm3}. However, typical MLLMs often rely heavily on their parametric knowledge, making them prone to long-tail knowledge gaps~\cite{asai2024reliable} and hallucinations~\cite{ye2022unreliability}, limiting their reliability and robustness in real-world applications. 

% Retrieval-Augmented Generation (RAG) frameworks have emerged as a promising paradigm by allowing MLLMs to incorporate external multimodal knowledge bases (KBs)~\cite{chen2022murag, yasunaga2022retrieval, chen2024mllm}. While multimodal RAG frameworks enhance generations by retrieving relevant images and texts from external KBs, they introduce new safety risks, as the trustworthiness of the external multimodal KBs cannot be guaranteed. Furthermore, compared to text-only RAG approaches, the multimodal RAG frameworks are uniquely vulnerable as imperceptible visual perturbations or subtle text manipulations in the external KB can misalign cross-modal representations~\cite{yin2024vlattack, wu2024adversarial}, triggering cascading failures across multimodal RAG frameworks.

% Consider an AI-driven e-commerce assistant powered by multimodal RAG to answer product-related queries. An attacker, such as a malicious seller, might inject a small set of poisoned image-text pairs by subtly altering product images paired with misleading descriptions. Such manipulations could lead to unsafe recommendations or falsely attributing higher ratings to low-quality products. Similarly, a single manipulated product image paired with misleading text could confuse the assistant, leading to nonsensical or irrelevant recommendations across a wide range of queries. These vulnerabilities are exacerbated in multimodal methods, as visual perturbations are often harder to detect and can cause cascading misinterpretations of the retrieved information.

% In this work, we propose \textbf{\textsc{MM-PoisonRAG}}, the first knowledge poisoning attack on multimodal RAG, exposing the vulnerabilities introduced by incorporating external multimodal KBs into multimodal RAG. \textsc{MM-PoisonRAG} employs two attack strategies tailored to distinct scenarios: (1) \textit{localized poisoning attack} (LPA) and (2) \textit{globalized poisoning attack} (GPA). In context of QA tasks, LPA aims to generate a query-specific poisoned data, leading the generator to produce \textit{targeted, attacker-defined} wrong answers for each query. In contrast, GPA introduces a single poisoned data point\qiusi{we also include multiple data points for this setting?} that disrupts responses to all queries, causing the generator to produce \textit{random}, incorrect outputs. 

% To achieve these attacks, we define a threat model under realistic constraints: the attacker has niether direct access to the original data in the multimodal KBs nor the MLLM's parameters, but can only inject a small number of poisoned data. Our attack leverages adversarial images to guide gradient-based optimization against the retriever model. Importantly, although we cannot assume the retriever uses a specific architecture (e.g., CLIP~\cite{radford2021learning}), we demonstrate that the poisoned data generated against one model with our attack can transfer to the other retriever variants, significantly degrading performance across different retriever model (\S\ref{sec:transfer}).
% \qiusi{After finalizing our settings, we should provide a more comprehensive description of our methods here}


% We evaluate the effectiveness of \textsc{MM-PoisonRAG} on MultimodalQA (MMQA)~\cite{talmor2021multimodalqa} and WebQA datasets~\cite{chang2022webqa} under various attack scenarios. Our experiments show that both LPA and GPA achieve significant accuracy drops of up to \textbf{42.6\%}, demonstrating the potency of our method. Notably, our experiments highlight the transferability of our attacks across diverse models, emphasizing the need for robust defenses to safeguard multimodal RAG frameworks against knowledge poisoning attacks. 

