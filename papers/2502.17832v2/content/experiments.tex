\section{Experiments}

% \heng{in table captions add (\%) after results}


\input{table/table1}

% \input{table/table5}
\subsection{Experimental Setup}
\paragraph{Datasets}
We evaluate our poisoning attacks on two widely-used multimodal QA benchmarks: MultimodalQA (MMQA)~\cite{talmor2021multimodalqa} and WebQA~\cite{chang2022webqa} following RagVL~\cite{chen2024mllm}.
Both benchmarks consist of multimodal, knowledge-seeking query-answer pairs. To focus on queries that require external context for accurate answers (details in Appendix~\ref{appendix:query_selection}), we select a subset of validation sets, yielding 125 QA pairs for MMQA and 1,261 QA pairs for WebQA. In MMQA, each QA pair is linked with one context of image-text pair, whereas in WebQA, some pairs require two contexts. 
The multimodal knowledge base $\mathcal{D}$ aggregates all contexts from the validation sets, resulting in $|\mathcal{D}| = 229$ for MMQA and $|\mathcal{D}| = 2,115$ for WebQA. 
% Each dataset contains questions with paired visual and textual context, where MMQA has one image-text pair as context, while WebQA has one or two image-text pairs as context. We use the test sets of these benchmarks as our dataset. 

\paragraph{Baselines}
Within the multimodal RAG framework, we use CLIP~\cite{radford2021learning} and OpenCLIP~\cite{openclip} as retrievers, while Qwen-VL-Chat~\cite{bai2023qwen} and LLaVA~\cite{liu2024llavanext} serve as reranker and generator. Given $\mathcal{D}$, the retriever selects the top-$N$ most relevant image-text pairs and refined by the reranker to the top-$K$ pairs, which are then passed to the generator. We evaluate our poisoning attacks on three retrieval and reranking settings: (1) no reranking ($N=m$), (2) reranking using images only ($N=5, K=m$), and (3) reranking using images and captions ($N=5, K=m$), where $m$ is the number of contexts passed to the generator ($m=1$ for MMQA and $m=2$ for WebQA). These settings allow us to assess our attack's effectiveness across different retrieval and reranking conditions.

\paragraph{Evaluation Metrics}
To assess both retrieval performance and end-to-end QA accuracy, we report two metrics: \text{R} and \text{Accuracy}. Since multimodal RAG frameworks follow a two-stage retrieval process (retriever $\rightarrow$ reranker), recall is computed based on the final set of retrieved image-text pairs $\mathcal{R}_i$ that the generator uses. Let $\mathcal{Q}_i$ be the $i$-th query, $\mathcal{C}_i$ be the ground-truth multimodal context ($|\mathcal{C}_i|$=1 for MMQA, $|\mathcal{C}_i|$=2 for WebQA), and 
$\mathcal{P}_i = \{(I^{\text{adv}}_{i,j}, T^{\text{adv}}_{i,j})\}$ be the adversarial image-text pair set ($|\mathcal{P}_i|$=5 for GPA-RtRrGen, $|\mathcal{P}_i|$=1 for the other settings). We define recall as follows:
\begin{equation}
\label{eq:recall}
\begin{split}
    \text{R}_\text{Original} &=  \frac{\sum_{i=1}^d|\mathcal{R}_i \cap \mathcal{C}_i|}{\sum_{i=1}^d| \mathcal{C}_i|}, \\
    \text{R}_\text{Poisoned} &=  \frac{\sum_{i=1}^d|\mathcal{R}_i \cap \mathcal{P}_i|}{\sum_{i=1}^d| \mathcal{P}_i|}.
\end{split}
\end{equation}
$\text{R}_\text{Original}$ measures the retrieval accuracy of the ground-truth context, while $\text{R}_\text{Poisoned}$ quantifies the frequency at which the poisoned image-text pairs are retrieved. A higher $\text{R}_\text{Poisoned}$ denotes greater success in hijacking the retrieval process. 


Following~\citet{chen2024mllm}, we define $\text{Eval}(\mathcal{A}_i, \hat{\mathcal{A}}_i)$ as the dataset-specific evaluation metric---Exact Match (EM) for MMQA and key-entity overlap for WebQA.
Given a QA pair $(\mathcal{Q}_i, \mathcal{A}_i)$, and a generated answer $\hat{\mathcal{A}_i}$, we define:
% \vspace{-0.05in}
\begin{equation}
\label{eq:acc}
\begin{split}
    \text{ACC}_\text{Original} &= \frac{1}{d}\sum_{i=1}^d \text{Eval}(\mathcal{A}_i, \hat{\mathcal{A}}_i), \\
    \text{ACC}_\text{Poisoned} &= \frac{1}{d}\sum_{i=1}^d \text{Eval}(\mathcal{A}_i^{\text{adv}}, \hat{\mathcal{A}}_i).
\end{split}
\end{equation}
$\text{ACC}_\text{Original}$ evaluates the system's ability to generate the correct answer, while $\text{ACC}_\text{Poisoned}$, specific to LPA, measures how often the model outputs the attacker-defined answer $\mathcal{A}_i^{\text{adv}}$, reflecting the LPA's success rate in manipulating the generation. 

\input{table/table2}

\subsection{Results of Localized Poisoning Attack}
LPA successfully manipulates generated outputs toward attacker-controlled answers across different retrieval and reranking settings in MMQA and WebQA tasks (Table~\ref{tab:mmqa_lpa}). Even in a complete black-box setting, LPA-BB achieves a high success rate $\text{ACC}_\text{Poisoned}$---up to \textbf{46\%}---in controlling multimodal RAG system to generate the adversarial answers. When refining poisoned knowledge with retriever access (LPA-Rt), attack success increases to \textbf{56.8\%} and \textbf{88.8\%} in $\text{ACC}_{\text{Poisoned}}$ and $\text{R}_{\text{Poisoned}}$, respectively, highlighting the impact of having access to the retriever in knowledge poisoning attacks.

Moreover, LPA generalizes well across different MLLMs used for reranking and generation, despite lacking access to these models. Consistent trends hold even when varying the reranker and generator (more results in Apendix~\ref{sec:other_mllm}), underscoring that injecting a single adversarial knowledge is sufficient to poison KB for a specific query, easily manipulating multimodal RAG outputs. LPA, however, is less effective on WebQA than on MMQA, especially in terms of accuracy drop, likely because WebQA incorporates two knowledge elements ($m=2$) as the input context to the generator, while only one adversarial entry is inserted into the KBs. This allows retrieval of both adversarial and ground-truth knowledge, leaving room for the generator to select the correct information.

% LPA-Rt further improves the attack, achieving a \textbf{72.3\%} drop in retrieval recall ($+48.2\%$ over LPA-BB) and \textbf{43.3\%} drop in final accuracy ($+31.1\%$ over LPA-BB) in MMQA, while WebQA shows 20.1\% and 10.6\% drops, respectively. These results highlight the effectiveness of leveraging the retriever access to strengthen our attack, further reducing the retrieval of ground-truth knowledge and increasing the model's reliance on poisoned context. 


% Beyond retrieval, we assess the model's susceptibility to generating attacker-defined wrong answers (Eq~\ref{eq:acc}). LPA successfully induces target outputs, achieving $\text{ACC}_{\text{Poisoned}}$ of 55.3\% and 29.2\%, surpassing the $\text{ACC}_{\text{Original}}$ of 17.7\% and 22.5\% on MMQA and WebQA, respectively. These results show that \textsc{MM-PoisonRAG} effectively disrupts both retrieval and generation, demonstrating how injecting a query-specific knowledge poisoning can alter cross-modal representations, degrade retrieval recall, and ultimately propagate misinformation into generation.

% Table~\ref{tab:mmqa_adv} summarizes the results, showing a significant improvement in the retrieval rate of the poisoned images compared to the naive approach. The fact that the recall metric of the poisoned images is higher than the recall of the original ones demonstrates the effectiveness of adversarial noise in amplifying the retrieval rate of the poisoned content. Consequently, we notice a sharp drop in the accuracy of the original answers while at the same time, we observe an increase in the accuracy of generating the wrong target answers. As expected, the poisoned image recall is higher when we just use the CLIP retriever instead of both the retriever and the re-ranker, as the adversarial noise is tailored for CLIP. Despite that, all attack metrics improve. 


% \input{figure/gpa_transferability}
\subsection{Results of Globalized Poisoning Attack}
% We evaluate the impact of GPA on RAG framework, demonstrating its ability to degrade retrieval and generation performance across all queries using a universally poisoned knowledge. 
% Table~\ref{tab:gpa_results} shows the GPA results.
% GPA-Rt injects multiple instances of the same adversarial knowledge with limited access to the retriever, while GPA-RtRrGen introduces a single poisoned knowledge but with full access to the RAG pipeline. 
As shown in Table~\ref{tab:gpa_results}, despite lacking access to the reranker and generator, GPA-Rt successfully disrupts all queries, reducing retrieval recall to a drastic \textbf{1.6\%} on MMQA and even \textbf{0.0 \%} on WebQA. GPA-RtRrGen causes consistent performance drops in both retrieval and generation, even with just one adversarial knowledge instance injected into the KBs. This demonstrates that even a single adversarial knowledge can be highly effective in corrupting the multimodal RAG framework. 
% Interestingly, accuracy remains relatively high in MMQA and WebQA despite retrieval failures. This is because MMQA includes 10 ``\textit{Yes/No}'' questions (8\%), while WebQA contains 174 queries (13.8\%), making random guesses more likely to be correct. 
% 1) # of poisoned knowledge - Rt only + 2) the input poisoned knowledge fools, MLLM to ignore its own parametric knowledge and generate our controlled response (i.e., ``Sorry'')
Our results on GPA reveal two major findings: (1) when the attacker only has access to the retriever (GPA-Rt), the number of adversarial knowledge has more impact on degrading model performance than having full access to the RAG pipeline (GPA-RtRrGen). (2) The poisoned context passed from the retriever and reranker to the MLLM tricks the model into disregarding its own parametric knowledge and generates an attacker-intended, poisoned response (e.g., ``Sorry''). These findings expose a fundamental vulnerability in the multimodal RAG framework, where poisoning the retrieval step amplifies errors in a generation, underscoring the need for robust retrieval mechanisms to improve the reliability and robustness of multimodal RAG.
\input{figure/recall_acc_comparison}
\input{figure/gpa_recall_acc}
\subsection{Qualitative Analysis } 
To understand how poisoned knowledge misleads retrieval and generation, we compare its retrieval recall against that of the original context. Across MMQA and WebQA, poisoned knowledge from LPA and GPA is retrieved more frequently, consistently achieving higher retrieval recall $\text{R}_{\text{Poisoned}}$ than $\text{R}_{\text{Original}}$.  Notably, GPA-RtRrGen reaches $90+\%$ recall, while the original context achieves only 0.4\% in top-1 retrieval on MMQA (Fig.~\ref{fig:gpa_analysis}). The generator produces poisoned responses (e.g., ``Sorry'') with 100\% accuracy while reducing original accuracy to 0\%, demonstrating the attack's control over generation even with both ground-truth and adversarial knowledge. LPA-Rt attains 88.8\% recall in top-1 retrieval, whereas the original context is retrieved only 8.8\% of the time on MMQA (Table \ref{tab:mmqa_lpa}). Query-image embedding similarity further supports this, with LPA showing 31.2\% higher similarity on MMQA and 40.7\% higher similarity on WebQA (Fig.~\ref{fig:qual_sim}), indicating poisoned knowledge is perceived as more relevant. These results highlight how our attack exploits cross-modal retrieval, misleading the retriever into prioritizing poisoned knowledge over real context, ultimately allowing adversarial knowledge to dominate generation. 


\input{table/table4}
\subsection{Transferability of MM-PoisonRAG}
\label{sec:transfer}

Both LPA-Rt and GPA-Rt optimize the adversarial image against the retriever, but in reality, direct access is often restricted. To address this limitation, we explore the transferability of our attacks, investigating whether an attack crafted using one retriever remains effective when applied to other retrievers. We generate adversarial samples using the CLIP retriever and examine them on the RAG framework with the OpenCLIP retriever. 

Our results show that adversarial knowledge generated by LPA-Rt is highly transferable across retrievers, achieving comparable performance degradation across retrieval recall and accuracy. For OpenCLIP, it leads to two times higher accuracy on the poisoned answer than that of the original answer, while the recall drops \textbf{56.0\%} and accuracy \textbf{32.8\%} on MMQA when $N=5, K=1$ and reranking with caption (Table~\ref{tab:mmqa_transfer}). Moreover, in Table~\ref{tab:mmqa_transfer}, even when the adversarial knowledge instance is generated under black-box access (LPA-BB), it still leads to \textbf{45.6\%} and \textbf{22.4\%} drops in retrieval and accuracy, respectively. This result implies another pathway, i.e., using an open model, for attackers to poison the multimodal RAG. In contrast, while GPA-Rt severely degrades retrieval and generation for all queries with a single adversarial image-text pair, it is less transferable between retrievers (Appendix~\ref{sec:transfer_appendix}). Nonetheless, despite lower transferability, GPA-Rt requires only one poisoned knowledge to corrupt the entire multimodal RAG pipeline exposing a severe vulnerability.

% This is supported by visualiation of how our adversarial knowledge is located in the embedding space. While LPA-Rt generated knowledge is distributed similar to the ground-truth image context in both CLIP and OpenCLIP representation space, adversarial knowledge generated from GPA-Rt is placed on text-embedding space in the target retriever, CLIP's embedding space while it is located in the image-embedding space in the OpenCLIP (Fig~\ref{}). The can be attributed to difference in the cross-modal alignment between CLIP and OpenCLIP model, where CLIP aligns text and image embedding well but are still somewhat entangled~\cite{}, while OpenCLIP model has improved text-image disentanglement~\cite{}. Despite lower transferability, GPA-Rt requires only one poisoned knowledge to corrupt the entire retrieval and generation process, exposing a sever vulnerability in multimodal RAG framework. 

