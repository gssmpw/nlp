\section{Related Work}
% \heng{At the end of each paragraph, try to add comparison with your work and highlight the novelty of your methods. e.g., is your work the first to generate attacks on multimodal RAG?}

\paragraph{Retrieval-Augmented Generation}
Retrieval-Augmented Generation (RAG)~\citep{lewis2020retrieval, guu2020realm, borgeaud2022retro, izacard2020leveraging} enhances language models by retrieving knowledge snippets from external KBs. A RAG framework consists of a KB, a retriever, and a generator (typically LLMs). Unlike traditional LLMs that solely rely on parametric knowledge, RAG dynamically retrieves relevant external knowledge during inference to ground its response on, improving the accuracy of tasks like fact-checking, information retrieval, and open-domain QA \citep{izacard2023atlas, borgeaud2022retro}. Multimodal RAG~\cite{chen2022murag, yang2023enhancing, xia2024rule,  sun2024fact}, which retrieves from a KB of image-text pairs, leverages cross-modal representations to examine the relevance between a query and the image-text pairs during retrieval. Despite their wide adoption, current works on multimodal RAG neglect the potential vulnerabilities that could be exploited by external attackers through knowledge poisoning.\looseness=-1

% Inspired by text-only RAG, prior studies~\cite{chen2022murag, yang2023enhancing, xia2024rule,  sun2024fact} introduce a multimodal RAG, incorporating an external multimodal KB, where image-text pairs are retrieved and fed into a multimodal LLM (MLLM). This framework leverages cross-modal representations to examine the relevance between a query and multimodal context during retrieval, which enhances generation for knowledge-intensive tasks that require up-to-date information. However, integrating an external multimodal KB introduces unique safety risks. Poisoned images or text can manipulate cross-modal representations, leading to retrieval failures and misleading outputs. Despite the importance of safety in multimodal RAG, vulnerabilities arising from external knowledge manipulation remain largely underexplored.

\paragraph{Adversarial Attacks}
Adversarial attacks have been extensively studied in the computer vision domain, beginning with pioneering work on imperceptible perturbations that can mislead neural networks~\cite{szegedy2013intriguing,DBLP:journals/corr/GoodfellowSS14}.
Subsequent research has explored various attack methods for diverse tasks, including object detection~\cite{evtimov2017robust,xie2017adversarial,DBLP:conf/cvpr/EykholtEF0RXPKS18}, visual classification~\cite{kim2023effective, kim2022few, bansal2023cleanclip}, and visual question answering~\cite{huang2023improving}. 
These efforts have revealed the vulnerability of deep vision models to seemingly minor input modifications.
In contrast, designing poisoning attacks on RAG is more challenging as they must manipulate both retrieval and generation processes. To be effective, poisoned examples should not only be retrieved by the retriever but also influence the generator to produce incorrect outputs. While prior works~\cite{zou2024poisonedrag, tamber2025illusions} explore text-only RAG poisoning, multimodal RAG poisoning remains unexplored. The key difficulty lies in manipulating cross-modal representations while distorting the generated response. To the best of our knowledge, we present the first knowledge-poisoning attack framework on multimodal RAG, exposing the vulnerabilities posed by external, multimodal KBs. \looseness=-1


% Our task differs from these previous tasks in that the image must satisfy multiple attack objectives to successfully cause the retrieval-augmented generation (RAG) system to produce incorrect outputs, which is inherently complex due to the nature of the RAG system. The most similar work to ours is~\cite{zou2024poisonedrag}, which attempts to poison the RAG system but in the purely text-domain. In that study, two conditions were established for the adversarial examples: the retrieval condition, which ensures that the adversarial text is included in the set of top-k retrieved texts for the target question, and the generation condition, which ensures that the language model produces incorrect output when the adversarial text is retrieved. However, that work is limited to the text domain, whereas our goal is to adapt these principles to the image domain.

