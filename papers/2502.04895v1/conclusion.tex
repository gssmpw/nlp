\chapter{Conclusion}
%\thispagestyle{empty}
\label{sec:conclusion}
This dissertation proposes novel ML and DL techniques to advance the field of physical layer communications. The research contributions encompass generative models, coding and decoding neural architectures, mutual information estimation and capacity learning frameworks.
In the following we examine the key conclusions and future implications of this work.

A fundamental theme centers on the segmentation of complex statistical problems into more tractable, understandable components using mathematical and information theoretical tools.
Ch.~\ref{sec:copulas} proposed a solution with a segmented GAN architecture, leveraging copula theory to separate dependence modeling from marginal distribution estimation. This fosters explainable ML and effective generation of data with complex statistical relationships. It also offers a simple framework to estimate any data distribution with NNs.

Ch.~\ref{sec:medium} presented a powerful DL methodology to model and produce synthetic channel and noise traces for any communication medium. This is achieved via pre-processing steps (with latent space representations or spectrograms), GAN-based generation, and suitable post-processing.

To effectively rethink building blocks of a communication chain as DL components, MI emerges as a cornerstone for several contributions in this thesis:
\begin{itemize}

\item Ch.~\ref{sec:decoder} introduced MIND, a MI-based decoder that attains performance of the genie MAP decoder, even without precise knowledge of the channel. The envisioned neural decoder also returns precise estimates of the a-posteriori PDFs;

\item Ch.~\ref{sec:autoencoders} discussed the limitations in traditional cross-entropy loss functions used with AE-based communication systems. We leveraged MI as a fundamental regularization term for training AEs, leading to optimal coding schemes and to the introduction of algorithms and architectures for capacity-achieving codes;

\item Ch.~\ref{sec:mi_estimators} introduced $f$-DIME, a novel class of discriminative MI estimators based on the variational representation of $f$-divergence. We proved that the variance of the estimators is the lowest in literature, and we proposed a derangement training strategy for efficient sampling. We finally evaluated $f$-DIME performance and demonstrated its excellent bias/variance trade-off. 

\item Ch.~\ref{sec:cortical} unveiled CORTICAL, a novel framework for learning capacity-achieving distributions in arbitrary discrete-time continuous memoryless channels. It employs a two-network cooperative game, validating its efficacy in non-Shannon channel settings. CORTICAL represents a powerful new DL tool for intricate channel capacity analysis. 
\end{itemize}

Ch.~\ref{sec:plc} demonstrated the value of DL in PLC systems:
Sec.~\ref{sec:plc_entanglement} and ~\ref{sec:plc_nakagami} showcase the potential for intelligent impedance detection and capacity estimation in PLC channels with non-AWGN noise (leveraging the CORTICAL framework).
Sec.~\ref{sec:plc_detection} highlights the successful application of NNs for accurate anomaly detection within PLNs.

Ch.~\ref{sec:rst} presented data interpolation techniques under given constraints exploiting polynomials and rational functions as basis. It reinforces the notion that a robust understanding of both theoretical principles and practical tools is essential for driving innovation.


\section*{Limitations}
DL contributions can be commonly grouped into three macro-areas such as: a) data collection and preparation; b) loss function design; c) neural architecture design.
Although the contribution of this thesis mostly centered around b), we discussed and provided insights for the other two areas as well. For instance, in Sec.~\ref{sec:medium_channelsynthesis} we discussed pre-processing and data transformation techniques to prepare generative models and in Sec.~\ref{subsec:mi_derangements} we showed how  training improves if we use a derangement sampling strategy. We also designed the deployed NN architectures and made several observations on the type of activation functions to be used based on the desired output. However, several limitations remain:
\begin{itemize}
    \item availability and quality of data, as our generative models are built upon datasets collected during measurement campaigns where we assumed time-invariance and stationarity as hypothesis; 
    \item during our research, we neither focused on studying architectural improvements nor on adapting state-of-the-art models for our purpose. Thus, we expect that many of our results can be improved over the next couple of years just by utilizing more advances neural architectures;
    \item for tasks such as MI estimation and capacity learning, it is difficult to investigate training converge properties, especially at high SNRs. Therefore, our theoretical derivations always assume the existence of an optimal solution, which may be different from the one found during training;
    \item we often neglected time-dependence on the input-output channel pairs. Nevertheless, the proposed formulation could be adapted and extended to such case with different architectures;
    \item as a more general limitation, while DL holds promise for the future of communications engineering, its widespread integration is not imminent. To achieve implementation in industry standards, DL models must consistently outperform existing solutions in terms of accuracy, efficiency, and robustness.
\end{itemize}

\section*{Future directions}
This thesis opens multiple avenues for future research, such as: 
\begin{itemize}
    \item an holistic system design, since the developed techniques could lead to a completely data-driven approach for communication system design, where channel modeling, coding/decoding, and capacity learning all synergistically utilize novel DL methods;
    \item data modeling and generation pipelines where probability estimation and distribution sampling tools are integrated with theoretical information theory principles to derive new bounds, performance limits, and insights into the fundamental nature of communication systems;
    \item exploration of model-based strategies to mitigate the computational expense of training, particularly focusing on methods for efficient offline training and transfer learning. Additionally, investigations into model compression and quantization techniques could enable the deployment of complex NNs on resource-constrained online devices;
    \item apply CORTICAL to tackle open problems such as the two-user Gaussian interference channel in order to get the achievable rates and the achievable distribution, which could inspire and offer theoretical insights on new coding schemes;
    \item beyond physical layer communications, since, for instance, the proposed capacity learning framework can be applied in biology, to analyze the genesâ€™ activation, in genetics, to study the capacity of DNA storage systems, for privacy-preserving data release mechanism, secrecy capacity, or more in general for representation learning. 
\end{itemize}


In summary, this dissertation establishes innovative DL methodologies that significantly contribute to the understanding and optimization of communication systems. It lays a foundation for a new paradigm where data-driven approaches, explainability and the ability to bridge theoretical insights with practical applications pave the way towards the next generation of intelligent and efficient communication infrastructure.
