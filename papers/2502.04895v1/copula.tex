\chapter{Copula for Statistical Analysis and Data Generation} % 
\chaptermark{Copula}
%\thispagestyle{empty}
\label{sec:copulas}

To comprehend how generative models operate and why they are so successful in estimating and reproducing data relationships, it is crucial to grasp the fundamental concept of data dependence. Understanding how variables in a dataset are interrelated is paramount to appreciating the power of generative models. 

In this context, the concept of copula emerges as a key foundation. Copulas provide a mathematical framework to characterize the nuanced dependencies between variables, enabling us to model and generate complex data distributions with precision and insight. 

This chapter is divided in two parts. The first one formulates  the data generation problem via copulas and describes how to implicitly sample from them. We achieve our objective in two well-defined separate steps.
In the second part, we finally exploit the envisioned procedure to explicitly estimate the copula density function, and consequently, the PDF of the collected data.

The results presented in this chapter are documented in \cite{Letizia2020,letizia2022copula}.

\subsection*{\textbf{Definitions and notation}}
\label{subsec:notation}
$\mathbf{X}$ denotes a multivariate random variable of dimension $d$ whose components are $X_{i}$ with $i=1,\dots, d$, while $\mathbf{x}^{(j)}$ for $j=1,\dots, n$ denotes the $j$-th realization of $\mathbf{X}$ among $n$ independent observations. $\mathbf{x}_i$ denotes a column vector of $n$ realizations of $X_{i}$. Furthermore, $x_{i}^{(j)}$ denotes the $i$-th entry (out of $d$) of the $j$-th collected sample (out of $n$ observations). In a compact notation, $\mathbf{x}=[\mathbf{x}_1,\mathbf{x}_2,\dots,\mathbf{x}_d]$ denotes a $n\times d$ matrix, sometimes referred to as the training data. 
$\Sigma_x$ and $p_{\mathbf{X}}(\mathbf{x})$ denote the sample covariance matrix and probability density function of $\mathbf{X}$, respectively. $F_{\mathbf{X}}(\mathbf{x}) = P(X_1\leq x_1, \dots, X_d \leq x_d)$ and $F^{-1}_{\mathbf{X}}(\mathbf{x})$ denote the cumulative distribution function and quantile function, respectively. 
The expected value of $\mathbf{X}$ is denoted with the expectation operator $\mathbb{E}_{\mathbf{x}\sim p_{\mathbf{X}}(\mathbf{x})}[\mathbf{X}]$. 

\section{Segmented generative networks}
\sectionmark{SGN}
\label{sec:sgn}
Recent advancements in generative networks (see Ch. \ref{sec:generative_networks}) have shown that it is possible to produce real, world-like data using deep neural networks. 

Some implicit probabilistic models that follow a stochastic procedure to directly generate data, such as GANs, have been introduced to overcome the intractability of the posterior distribution. However, the ability to model data requires a deep knowledge and understanding of its statistical dependence — which can be preserved and studied in appropriate latent spaces. 

In this section, we present a segmented generation process through linear and non-linear manipulations in a same-dimension latent space where data is projected to. Inspired by the known stochastic method to generate correlated data, we develop a segmented approach for the generation of dependent data, exploiting the concept of copula. The generation process is split into two frames, one embedding the covariance or copula information in the uniform probability space, and
the other embedding the marginal distribution information in the
sample domain. 
The proposed network structure, referred to as a
segmented generative network (SGN), also provides an empirical method to sample directly from implicit copulas.
To show its generality, we evaluate the presented approach in three application scenarios: a toy example, handwritten digits and face image generation.

\subsection{Proposed approach}
\label{subsec:sgn_proposal_approach}
The statistical dependence between the components of a multivariate random variable $\mathbf{X}$ is described by the joint probability distribution $p_{\mathbf{X}}(x_1, x_2, \dots, x_d)$. The correlation, instead, measures how the components are related on average and it is expressed in terms of the expectation $\mathbb{E}[\mathbf{X}\cdot \mathbf{X}^T]$ or the covariance $\mathbb{E}[(\mathbf{X}-m_X)\cdot (\mathbf{X}-m_X)^T]$, where $m_X$ denotes the expected value of $\mathbf{X}$. The correlation is often associated with the idea of \textit{linear} dependence.

Let $\mathbf{x} = [\mathbf{x}_1, \mathbf{x}_2, \dots, \mathbf{x}_d]$ be a set of realizations of $\mathbf{X}$, also referred to as the collected multivariate data. We would like to generate new unseen samples $\mathbf{\hat{x}} = [\mathbf{\hat{x}}_1, \mathbf{\hat{x}}_2, \dots, \mathbf{\hat{x}}_d]$ similar, in some way as we will discuss, to $\mathbf{x}$. In the following, we propose two different approaches: 
\begin{itemize}
\item The first one revisits the known stochastic generation process of correlated data (data that exhibits the same correlation as the collected one) and highlights the need for a segmentation and domain adaptation step. Such method will be referred to as segmented generative network targeting and modeling the correlation of data (SGN-C). 
\item The second one, instead, studies the stochastic generation process of dependent data (data that exhibits the same joint distribution as the collected one) by applying the same domain adaptation of SGN-C but integrating the concept of copula in order to segment the generation process. Moreover, it provides a direct method to sample from copulas. Such approach will be referred to as segmented generative network targeting and modeling the statistical dependence of data (SGN-D). 
\end{itemize}

\subsubsection{Transform sampling}
\label{subsec:sgn_transform_sampling}
The training data has been generated by some fixed unknown or difficult to construct probability distribution $p_{\mathbf{X}}(\mathbf{x}) = p_{\mathbf{X}}(x_1, x_2, \dots, x_d)$ with cumulative distribution function $F_{\mathbf{X}}(\mathbf{x}) = P(X_1\leq x_1, \dots, X_d \leq x_d)$. However, it is plausible to assume that the marginal density of each $X_i$ is known or can be easily derived as $p_{X_i}(x_i)$ with cumulative $F_{X_i}(x_i)$. Hence, the data can be mapped into a latent space with the same dimension using the inverse transform sampling method. In particular, let $U_i$ be a uniform random variable, then
\begin{equation}
X_i = F^{-1}_{X_i}(U_i)
\label{eq:sgn_InverseTransform}
\end{equation}
is a random variable with cumulative distribution $F_{X_i}$.
To project the data $\mathbf{x}$ into the latent space, it is enough to compute the transformation $u_i = F_{X_i}(x_i) \text{  }\forall i=1,\dots,d$.
This first step can be interpreted as a simple encoder which tries to represent the data in a domain space where linear manipulations are easier to be implemented.
One way to go back is to train a neural network which, given $\mathbf{u}$ as input and $\mathbf{x}$ as output, finds the inverse mapping between the two spaces, the latent and the sample ones. This inverse transformation back to the sample space can be easily interpreted as a decoder. Fig. \ref{fig:sgn_OurFramework} describes the SGN framework.

The autoencoder described so far has no generative properties since it only replicates the dataset $\mathbf{x}$. The way to generate new samples is to build a new encoded set $\mathbf{\hat{u}}$ and feed it into the already trained inverse network. 

In general, $X_i$ and $X_j$ with $i\neq j \leq d$ are dependent random variables (let us consider for example the intensity of two consecutive pixels in an image or the amplitude of a waveform like the sound). A first order of approximation is the linear dependence: the idea is to choose the new encoded set $\mathbf{\hat{u}}$ as a set of $d$ correlated uniform variables — correlation quantified by the sample covariance matrix $\Sigma_u$ of the encoded initial set $\mathbf{u}$.


\subsubsection{Correlated uniforms (SGN-C)}
\label{subsec:sgn_correlated_uniforms}

Let $\Sigma_x$ and $\Sigma_u$ be the sample covariance matrices of the dataset $\mathbf{x}$ and the latent uniform code $\mathbf{u}$, respectively. If $\mathbf{\hat{x}}$ has covariance matrix $\Sigma_{\hat{x}}$ equal to $\Sigma_x$, then we define $\mathbf{\hat{x}}$ \textit{similar} to $\mathbf{x}$. 
Denoting with $\mathbf{F^{-1}}$ the non-linear inverse cumulative distribution function, then if $\mathbf{\hat{u}}$ is similar to $\mathbf{u}$, it follows that $\mathbf{\hat{x}} = \mathbf{F^{-1}(\hat{u})}$ is similar to $\mathbf{x}$. 

To generate $d$ correlated uniform distributed random variables, we use the NORTA (Normal to anything) method \cite{NORTA}, in the following denoted as covariance method. 
The first step requires the generation of $d$ correlated Gaussian distributed random variables. In particular, given a mean vector $\mu$ and a covariance matrix $\Sigma$, to generate a sample $Y\sim \mathcal{N}(\mu,\Sigma)$ from the multivariate normal distribution, we need to consider first a vector $\mathbf{z}$ of uncorrelated Gaussian random variables, then find a matrix $\mathbf{C}$, square root of $\Sigma$ such as $\mathbf{C}\cdot \mathbf{C}^T = \Sigma$ (for example using the Cholesky decomposition). It follows that $\mathbf{y}=\mu + \mathbf{C}\cdot \mathbf{z}$ is a vector of $d$ Gaussian random variables $\{Y_i\}_{i=1}^d$ with the desired properties. Applying the probability integral transform to each entry, leads to $d$ correlated uniform random variables 
\begin{equation}
\hat{U}_i = \Phi(Y_i)
\label{eq:sgn_ProbabilityIntegralTransform}
\end{equation}
where $\Phi$ is the cumulative distribution function of the standard normal distribution.
If we wanted to impose $\Sigma_u$ as the covariance matrix of $\mathbf{\hat{u}}$, we could transform the latent code $\mathbf{u}$ into a new latent code $\mathbf{n}$ in the Gaussian space, compute the covariance matrix $\Sigma_n$ and use it to sample from the multivariate normal distribution $\mathcal{N}(\mu,\Sigma_n)$. Instead, if we only wanted to impose the correlation matrix $\mathbf{R_u}$, we would rather just sample from the multivariate normal distribution $\mathcal{N}(\mu,\mathbf{R_u})$ to get a good practical approximation as described in \cite{Falk1999}.

The generation of the encoded set can be easily implemented in parallel, which means that we do not need to wait for any information from previous samples. Once the encoded set $\mathbf{\hat{u}}$ is built, the transformation $\mathbf{F^{-1}}(\mathbf{\hat{u}})$ gives $\mathbf{\hat{x}}$ as a new generated sample, exploiting only the linear information inside the dataset. Notice that exploiting an artificial neural network for the inverse transform is not mandatory: one could use the inverse cumulative distribution function (quantile function, see \eqref{eq:sgn_InverseTransform}) implemented by step functions or kernel smoothing functions.
Sec. \ref{subsec:sgn_evaluation_of_results} will show some visual results.

\begin{figure}
% strip if you want 2 columns
\centering
\includegraphics[scale = 0.4]{images/copula/UnifGen.pdf}
\caption{Transform sampling and SGN approach where original data are projected into a latent space, the new codes are generated and back-projected again.}
\label{fig:sgn_OurFramework}
\end{figure}


\subsubsection{Dependent uniforms (SGN-D)}
\label{subsec:sgn_dependent_uniforms}

Working only with linear dependence (correlation) is not sufficient to fully reproduce the relationship between data. Thus, it is necessary to build a new encoded set $\mathbf{\hat{u}}$ of $d$ dependent uniform variables.  

When we discussed the procedure to build correlated uniforms, we were looking for a vector $\mathbf{\hat{u}}$ whose covariance matrix $\Sigma_{\hat{u}}$ was equal to the prescribed one $\Sigma_u$ and we built it by using the SGN-C algorithm described in Sec. \ref{subsec:sgn_correlated_uniforms}. Another approach could have been the following: given a family of functions $S_{\theta}$ which takes uncorrelated uniform variables $\mathbf{u_0}$ as input and transforms them into correlated ones $\mathbf{\hat{u}}$, one could solve the optimization problem
\begin{equation}
\theta_{\min} = \argmin_{\theta} \delta(\Sigma_u, \Sigma_{S_{\theta}(\mathbf{u_0})}),
\label{eq:sgn_DeltaSigma}
\end{equation}
where $\delta$ is a measure of distance between the sample covariance matrices of $\mathbf{u}$ and $\mathbf{\hat{u}}$, respectively.

In the same way, given the latent codes $\mathbf{u}$ with probability density function $p_{\mathbf{u}}(\mathbf{u})$, the main idea to generate dependent uniform random variables is to train a neural network $G_{\theta}$ which takes independent uniform variables $\mathbf{u_0}$ as input and maps them into new dependent ones, $\mathbf{\hat{u}}$, with distribution $q_{\mathbf{\hat{u}}}(\mathbf{\hat{u}})$. This is again a generative model whose objective function is
\begin{equation}
\theta_{\min} = \argmin_{\theta} \delta(p_{\mathbf{u}}(\mathbf{u}), q_{\mathbf{\hat{u}}}(G_{\theta}(\mathbf{u_0}))
\label{eq:sgn_DeltaP}
\end{equation}
where now $\delta$ is a measure of \textit{discrepancy} between the real distribution $p_{\mathbf{u}}$ and the generated one $q_{\mathbf{\hat{u}}}$.

Before introducing the approach chosen for solving problem \eqref{eq:sgn_DeltaP}, we focus on the particular properties that $p_{\mathbf{u}}$ and $q_{\mathbf{\hat{u}}}$ have, recalling the concept of \textit{copula}.

Let $(U_1,U_2,\dots,U_d)$ be uniform random variables, then their joint cumulative distribution function $F_{\mathbf{U}}(\mathbf{u}) = P(U_1\leq u_1, \dots, U_d \leq u_d)$ is a copula $C:[0,1]^d \rightarrow [0,1]$ (see \cite{Nelsen2006} for an analytic description). Copulas are a useful tool to construct multivariate distributions and analyze data dependence. Indeed, Sklar's theorem \cite{Sklar} states that if $F_{\mathbf{X}}$ is a $d$-dimensional cumulative distribution function with continuous marginals $F_{X_1},\dots,F_{X_d}$, then $F_{\mathbf{X}}$ has a unique copula representation
\begin{equation}
F_{\mathbf{X}}(x_1,\dots,x_d) = C(F_{X_1}(x_1),\dots,F_{X_d}(x_d)).
\label{eq:sgn_SklarF}
\end{equation}
Moreover, when the multivariate distribution has a probability density function $f_{\mathbf{X}}$, it holds that
\begin{equation}
f_{\mathbf{X}}(x_1,\dots,x_d) = c(F_{X_1}(x_1),\dots,F_{X_d}(x_d))\cdot \prod_{i=1}^{d}{f_{X_i}(x_i)},
\label{eq:sgn_Sklarf}
\end{equation}
where $c$ is the density of the copula. This last relationship is rather interesting because it affirms that the dependence internal structure of $f_{\mathbf{X}}$ can be recovered using the density of the marginals $f_{X_i}$ and the density $c$ of the copula. Under this perspective, problem \eqref{eq:sgn_DeltaP} can be reformulated as
\begin{equation}
\theta_{\min} = \argmin_{\theta} \delta(c_{\mathbf{u}}(\mathbf{u}), c_{\mathbf{\hat{u}}}(G_{\theta}(\mathbf{u_0})),
\label{eq:sgn_DeltaC}
\end{equation}
where $c_{\mathbf{u}}$ and $c_{\mathbf{\hat{u}}}$ are the densities of the copulas related to $\mathbf{u}$ and $\mathbf{\hat{u}}$, respectively. The objective is to reach the equality $c_{\mathbf{u}}=c_{\mathbf{\hat{u}}}$ and to sample a new encoded set $\mathbf{\hat{u}}$ from $c_{\mathbf{u}}$, preserving the entire hidden dependence in $\mathbf{u}$ by construction.
Due to the fact that we are working with a finite set of samples, we should build the empirical copula of the encoded given set $\mathbf{u}$, with expression
\begin{equation}
C_n(\mathbf{u}) = \frac{1}{n}\sum_{j=1}^{n}{\mathbbm{1}_{\{U_{1}^{(j)} < u_1, \dots, U_{d}^{(j)} < u_d\}}}
\end{equation}
where $n$ is the number of observations, $U_{i}^{(j)}$ denotes the $j$-th realization of the $i$-th random variable, with $i=1,\dots,d$, and $\mathbbm{1}_A$ is the indicator function. When its dimensionality increases, this is not feasible anymore and one way to proceed is to choose a parametric family of multivariate copulas, like the multivariate Gaussian copula with correlation matrix $\Sigma$ (equivalent to NORTA \cite{Bedford2016}) or the multivariate Student's t-copula with $\nu$ degrees of freedom and correlation matrix $\Sigma$, which is more suitable for data containing phenomena of extreme value dependence \cite{t-copula}. 
Archimedean copulas are a particular class of copulas that admit a closed formula and allow modeling dependence varying one parameter, they have the following representation
\begin{equation}
C(u_1,\dots, u_d; \theta) = \psi^{[-1]}(\psi(u_1,\theta)+\dots+\psi(u_d,\theta); \theta)
\end{equation}
where $\psi$ is a continuous, strictly decreasing and convex generator function with pseudo-inverse $\psi^{[-1]}$ \cite{Nelsen2006} and $\theta$ is a parameter. 
Sec. \ref{subsec:sgn_evaluation_of_results} compares some results using different copulas in specific applications.
An interesting way to overcome the lack of parametric multivariate copulas is to take advantage of the huge number of parametric families of bivariate copulas through the concept of vine copulas \cite{VineCopula, Bedford2016}. The idea is to model the copula density as the product of pairs of conditional copula bivariate densities, under a tree or vine decomposition which leads to tractable and flexible probabilistic models. A recent attempt to generate data using vine copulas \cite{VCAE} exploited autoencoders and their ability to find lower dimensional representation.

We have argued about the parameters of the function $\delta$ that we are trying to minimize, but we never mentioned so far the type of distance/discrepancy that $\delta$ has to mime. Since we are elevating our approach to a general one which does not focalize on low dimension of data or specific type of distributions, we propose two different approaches. The first one is the maximum mean discrepancy (MMD) metric.

Let $\mathbf{x}=\{\mathbf{x}^{(1)},\dots,\mathbf{x}^{(l)}\}$ and $\mathbf{y}=\{\mathbf{y}^{(1)},\dots,\mathbf{y}^{(m)}\}$ be observations taken independently from $p = c_{\mathbf{u}}$ and $q = c_{\mathbf{\hat{u}}}$, respectively. Let $(\chi,d)$ be a nonempty compact metric space in which $p$ and $q$ are defined. Then, the maximum mean discrepancy is defined as
\begin{equation}
\text{MMD}(\mathcal{F},p,q) := \sup_{f\in \mathcal{F}}(\mathbb{E}_{\mathbf{x}\sim p}[f(\mathbf{x})]-\mathbb{E}_{\mathbf{y}\sim q}[f(\mathbf{y})]),
\label{eq:sgn_MMD}
\end{equation}
where $\mathcal{F}$ is a class of functions $f:\chi \rightarrow \mathbb{R}$. 
Since $p=q$ if and only if $\mathbb{E}_{\mathbf{x}\sim p}[f(\mathbf{x})]=\mathbb{E}_{\mathbf{y}\sim q}[f(\mathbf{y})]$ $\forall f\in \mathcal{F}$, MMD is a metric that measures the disparity between $p$ and $q$ (see \cite{FortetMMD}).

When $\mathcal{F}$ is a \textit{reproducing kernel Hilbert space} (RKHS), $f$ can be replaced by a kernel $k\in \mathcal{H}$ (i.e. Gaussian or Laplace kernels). In this case, Gretton et al. \cite{Gretton2012} showed that 
\begin{equation}
\text{MMD}^2(\mathcal{H},p,q) = \mathbb{E}_{\mathbf{x,x'}\sim p}[k(\mathbf{x,x'})]-2\mathbb{E}_{\mathbf{x}\sim p,\mathbf{y}\sim q}[k(\mathbf{x,y})]  +\mathbb{E}_{\mathbf{y,y'}\sim q}[k(\mathbf{y,y'})],
\label{eq:sgn_MMDK}
\end{equation}
where $\mathbf{x'}$ is an independent copy of $\mathbf{x}$ with the same distribution, and $\mathbf{y'}$ is an independent copy of $\mathbf{y}$.
For practical implementation, an unbiased empirical estimate is given by
\begin{align}
\text{MMD}_u^2(\mathcal{H},\mathbf{x},\mathbf{y}) = & \frac{1}{l(l-1)}\sum_{i\neq i'}{k(\mathbf{x}^{(i)},\mathbf{x}^{(i')})} \nonumber \\ 
& + \frac{1}{m(m-1)}\sum_{j\neq j'}{k(\mathbf{y}^{(j)},\mathbf{y}^{(j')})} \nonumber \\ 
& - \frac{2}{lm}\sum_{i=1}^{l}{\sum_{j=1}^{m}{k(\mathbf{x}^{(i)},\mathbf{y}^{(j)})}}.
\label{eq:sgn_MMDKe}
\end{align}

Finally, we can define the type of discrepancy $\delta$ as the $\text{MMD}_u^2$ estimator, in particular
\begin{equation}
\delta(c_{\mathbf{u}}(\mathbf{u}), c_{\mathbf{\hat{u}}}(G_{\theta}(\mathbf{u_0})) = \text{MMD}_u^2(\mathcal{H},\mathbf{u},G_{\theta}(\mathbf{u_0}))
\label{eq:sgn_DeltaMMD}
\end{equation}
and proceed with its minimization by the exploitation of the chain rule and the gradient descent method as described in \cite{MMDnets}. We found this methodology not effective for embedding the copula dependence structure. Therefore, we decided to use it during the evaluation phase (see. Sec. \ref{subsec:sgn_evaluation_of_results}) and, instead, adopt a GAN framework to reproduce the copula dependence.  


The core idea is to identify the copula with a first generator $G_u$. The generator $G_u$ receives an equal-dimensional independent uniform noise source $\mathbf{z}$ as input and internally creates the copula dependence structure by opposing a discriminator $D_u$ which tries to distinguish between real and fake dependent uniform samples, $\mathbf{u}$ and $\mathbf{\hat{u}}$, respectively. 
The corresponding value function reads as follows
\begin{equation}
V(G_u,D_u) =  \mathbb{E}_{\mathbf{u} \sim c_{\mathbf{u}}(\mathbf{u})}[\log D_u(\mathbf{u})]  + \mathbb{E}_{\mathbf{z} \sim \mathcal{U}(0,1)}[\log(1-D_u(G_u(\mathbf{z})))].
\label{eq:sgn_V1}
\end{equation}

At the same time, in order to strengthen the copula generation process,  another GAN $(G_x, D_x)$ mimes the relationship (the quantile function $F_{X_i}^{-1}$ for $i=1,\dots,d$) between the latent code $\mathbf{u}$ and the sample space $\mathbf{x}$. To do so, it takes the generated uniforms $\mathbf{\hat{u}}$ and statistically transforms them into new samples $\mathbf{\hat{x}}$, checking again the statistical significance with another discriminator $D_x$. The second value function is defined as
\begin{equation}
V(G_x,D_x) = \mathbb{E}_{\mathbf{x} \sim p_{\mathbf{x}}(\mathbf{x})}[\log D_x(\mathbf{x})]
 + \mathbb{E}_{\mathbf{\hat{u}} \sim c_{\mathbf{\hat{u}}}}[\log(1-D_x(G_x(\mathbf{\hat{u}})))].
\label{eq:sgn_V2}
\end{equation}
We denote the full generation process as segmented generative networks modeling the dependence (SGN-D). Succinctly, the first generator $G_u$ embeds the copula density structure $c_{\mathbf{u}}(\mathbf{u})$ and produces dependent uniform samples $\mathbf{\hat{u}}$. The second generator $G_x$ embeds the marginals structure (the quantile $F_{X_i}^{-1}$ for $i=1,\dots,d$) and statistically maps $\mathbf{\hat{u}}$ into new samples $\mathbf{\hat{x}}$.
Fig. \ref{fig:sgn_UXGAN} graphically summarizes the entire followed methodology. 

\begin{figure}
% strip if you want 2 columns
\centering
\includegraphics[scale = 0.5]{images/copula/UXGAN.pdf}
\caption{Segmented generative network modeling the dependence: a first network builds the dependent uniforms and a second network converts them into new data.}
\label{fig:sgn_UXGAN}
\end{figure}

To identify the marginal cumulative distribution function $F_{X_i}$, for $i=1,\dots, d$, it is sufficient to \textit{cyclically} repeat the aforementioned segmentation process by concatenating another GAN which takes as input the last generated samples $\mathbf{\hat{x}}$ and projects them into a new encoded set $\mathbf{\tilde{u}}$.  

Next section presents some graphical and numerical results, comparing the different methodologies.

\subsection{Evaluation of results}
\label{subsec:sgn_evaluation_of_results}
This section discusses and compares the SGN-C and SGN-D approaches to generate new samples in three different case studies. We consider a $2D$ toy dataset and two higher dimensional datasets, MNIST \cite{MNIST} and CelebA \cite{CelebA}. For each of them we qualitatively evaluate the generation performance of the SGN-C approach (e.g. covariance, Gaussian and $t$-copula) and of the SGN-D approach (with GANs). Finally, for the last dataset scenario, we compare some quantitative results using three different metrics. 

\subsubsection{Qualitative evaluation}
We used Keras with TensorFlow \cite{TensorFlow} as backend to implement the proposed model. The code has been tested on a Windows-based operating system provided with Python 3.6, TensorFlow 1.13.1, Intel core i7-3820 CPU and one GPU GTX1080. 
Two different types of neural networks architectures have been deployed according to the specific dataset under analysis. To allow the reproducibility of the work presented, Tab. \ref{tab:combined_toy} and \ref{tab:combined_images} report all the implementation aspects. The details of the network and the chosen parameters for the first toy example are reported in Tab. \ref{tab:combined_toy}. For the two dataset containing images, the established DCGAN \cite{Radford2016} architecture has been used as foundation for the proposed SGN-D structure with all details reported in Tab. \ref{tab:combined_images}. To overcome numerical issues in the cases of the images, the definition interval of the uniform distribution is transformed from $[0,1]$ to $[-1,1]$.

\begin{table}
	\scriptsize % text dimension
	\centering
	\caption{SGN-D based on GAN architecture for synthetic toy model.}
	\begin{tabular}{ p{5cm}|p{2cm}|p{1.5cm}} 
		\toprule
		\textbf{Operation} & \textbf{Feature maps}  		& \textbf{Activation}  \\
		\midrule
		\textbf{Generator $G_u$} & &  \\ 
		$G_u(\mathbf{z}):\mathbf{z} \sim \mathcal{U}(0,1)$ & 2\\ 
		Fully connected & $500$ & ReLU \\
		Dropout & $0.5$ &  \\
		Fully connected & $2$ & Sigmoid \\  \hline
		\textbf{Generator $G_x$} &&   \\ 
		$G_x(\mathbf{u}):\mathbf{u} \sim c_u$ & 2\\ 
		Fully connected & $500$ & ReLU \\
		Dropout & $0.5$ &  \\
		Fully connected & $2$ & Sigmoid \\  \hline
		
		\textbf{Discriminator $D_u$} & &  \\ 
		$D_u(\mathbf{u}):\mathbf{u} \sim c_u$ & $2$\\ 
		Fully connected & $500$ & LeakyReLU \\ 
		Dropout & $0.4$ &  \\ 
		Fully connected & $10$ & LeakyReLU \\ 
		Fully connected & $1$ & Sigmoid \\  \hline
		\textbf{Discriminator $D_x$} & &  \\ 
		$D_x(\mathbf{x}):\mathbf{x} \sim p_x(\mathbf{x})$ & $2$ \\ 
		Fully connected & $500$ & LeakyReLU \\ 
		Dropout & $0.4$ &  \\ 
		Fully connected & $10$ & LeakyReLU \\ 
		Fully connected & $1$ & Sigmoid \\  \hline   \hline
		
		
		Number of generators & \multicolumn{2}{c}{2} \\
		Batch size & \multicolumn{2}{c}{64} \\
		Number of iterations & \multicolumn{2}{c}{50000} \\ 
		Leaky ReLU slope &  \multicolumn{2}{c}{0.2} \\ 
		Learning rate &  \multicolumn{2}{c}{0.0002}  \\ 
		Optimizer &  \multicolumn{2}{c}{Adam ($\beta_1$ = 0.5, $\beta_2$ = 0.9999)}  \\ \hline
		%\bottomrule	
	\end{tabular}
	
	\label{tab:combined_toy}
	%\noindent\makebox[\linewidth]{\rule{0.85\paperwidth}{0.4pt}} %linea
\end{table}

\begin{table}
	\scriptsize % text dimension
	\centering
	\caption{SGN-D based on DCGAN architecture for images.}
	\begin{tabular}{ p{5cm}|p{2cm}|p{1.5cm}} 
		\toprule
		\textbf{Operation} & \textbf{Feature maps}  		& \textbf{Activation}  \\
		\midrule
		\textbf{Generator $G_u$} & &  \\ 
		$G_u(\mathbf{z}):\mathbf{z} \sim \mathcal{U}(-1,1)$ & 1024\\ 
		Fully connected & $4096$ & ReLU \\ 
		Reshape and upsampling & $(8,8,64)$ &  \\ 
		Convolution and BatchNorm & $64$ & ReLU \\ 
		Upsampling & &  \\ 
		Convolution and BatchNorm & $32$ & ReLU \\ 
		Convolution & $3$ & Tanh \\  \hline
		\textbf{Generator $G_x$} &&   \\ 
		$G_x(\mathbf{u}):\mathbf{u} \sim c_u $ & $(32,32,3)$ \\ 
		Convolution and BatchNorm & $128$ & ReLU \\ 
		Upsampling & &  \\ 
		Convolution and BatchNorm & $64$ & ReLU \\ 
		Convolution & $3$ & Tanh \\  \hline
		
		\textbf{Discriminator $D_u$} & &  \\ 
		$D_u(\mathbf{u}):\mathbf{u} \sim c_u$ & $(32,32,3)$\\ 
		Convolution & $32$ & LeakyReLU \\ 
		Dropout & $0.25$ &  \\ 
		Convolution and BatchNorm & $64$ & LeakyReLU \\ 
		Dropout & $0.25$ &  \\ 
		Convolution and BatchNorm & $128$ & LeakyReLU \\ 
		Dropout & $0.25$ &  \\ 
		Convolution and BatchNorm & $256$ & LeakyReLU \\ 
		Dropout & $0.25$ &  \\ 
		Flatten and dense & $1$ & Sigmoid \\   \hline
		\textbf{Discriminator $D_x$} & &  \\ 
		$D_x(\mathbf{x}):\mathbf{x} \sim p_x(\mathbf{x})$ & $(32,32,3)$ \\ 
		Convolution & $32$ & LeakyReLU \\ 
		Dropout & $0.25$ &  \\ 
		Convolution and BatchNorm & $64$ & LeakyReLU \\ 
		Dropout & $0.25$ &  \\ 
		Convolution and BatchNorm & $128$ & LeakyReLU \\ 
		Dropout & $0.25$ &  \\ 
		Convolution and BatchNorm & $256$ & LeakyReLU \\ 
		Dropout & $0.25$ &  \\ 
		Flatten and dense & $1$ & Sigmoid \\   \hline  \hline
		
		
		Number of generators & \multicolumn{2}{c}{2} \\
		Batch size & \multicolumn{2}{c}{64} \\
		Number of iterations & \multicolumn{2}{c}{50000} \\ 
		Leaky ReLU slope &  \multicolumn{2}{c}{0.2} \\ 
		Learning rate &  \multicolumn{2}{c}{0.0002}  \\ 
		Optimizer &  \multicolumn{2}{c}{Adam ($\beta_1$ = 0.5, $\beta_2$ = 0.9999)}  \\ \hline
		%\bottomrule	
	\end{tabular}
	
	\label{tab:combined_images}
	%\noindent\makebox[\linewidth]{\rule{0.85\paperwidth}{0.4pt}} %linea
\end{table}

\subsubsection{1) 2D toy database}
Consider a set of $2$ random variables whose statistics is computed from a collection of $2000$ observations. Thus, given $\mathbf{x} = [\mathbf{x}_1, \mathbf{x}_2]$, we wish to generate a new sample $\mathbf{\hat{x}} = [\mathbf{\hat{x}}_1, \mathbf{\hat{x}}_2]$. In order to impose a non-linear statistical dependence structure, we build $\mathbf{x}$ as follows
\begin{equation}
\label{eq:sgn_toy2D}
\mathbf{x} = [\sin(t), t\cos(t)] + \mathbf{n},
\end{equation}
where $t\sim \mathcal{N}(0,1)$ and $\mathbf{n}\sim \mathcal{N}(\mathbf{0},\sigma^2 \mathbb{I})$ with $\sigma = 0.01$. 

Proceeding as explained in Sec. \ref{subsec:sgn_correlated_uniforms} leads to a set of correlated uniforms and correlated samples, $\mathbf{\hat{u}}$ and $ \mathbf{\hat{x}}$, respectively. Despite having the same covariance matrix and the same marginals, the linear transformation is not capable to account for the full dependence structure as shown in the top row of Fig. \ref{fig:sgn_Case1}.
What is missing is indeed the copula density component in \eqref{eq:sgn_Sklarf}. Since in general there is no closed-form expression for the copula density  $c(\mathbf{u})$, the multivariate Gaussian copula and the Student's $t$-copula are possible choices that contain information regarding correlation coefficients, thus linear properties. Nevertheless, both of them are not enough to cover or at least approximate the dependence in $\mathbf{x}$.
Therefore, we considered both Clayton \cite{ClaytonCop} and Frank Archimedean copulas. From the central row of Fig. \ref{fig:sgn_Case1}, we can immediately understand that there exists a set of parameters $\theta$ which approximates the dependence around the mean values but is not able to do the same around the tails.

In the low-dimensional space, i.d. $2D$, it is still feasible to calculate the empirical copula for a certain bins resolution. In such a case, sampling from copula results in a trivial task and provides good quality of the generated samples (bottom left corner of Fig. \ref{fig:sgn_Case1}). Whenever the space dimension increases, such approach cannot be easily followed anymore due to numerical problems, reason why a SGN-D based methodology, which implicitly estimates the distribution, can be exploited.
Bottom sub-plots with orange samples of Fig. \ref{fig:sgn_Case1} show the generated samples under the SGN-D framework for both $G_u$ (back-projected with quantiles in sample domain) and $G_x$ output samples. 

\begin{figure}
% strip if you want 2 columns
\centering
\includegraphics[scale = 0.6]{images/copula/Case1.pdf}
\caption{Comparison of $2D$ samples generated using SGN-C, Archimedean and empirical copulas (blue samples), and SGN-D (orange samples) approaches.}
\label{fig:sgn_Case1}
\end{figure}

\subsubsection{2) Handwritten digit database}
Consider now a set $28\times 28$ pixels representing images of the digit $4$. This results in $784$ dependent random variables whose statistics is computed from a collection of $2000$ observations. Thus, given $\mathbf{x} = [\mathbf{x}_1, \mathbf{x}_2, \dots, \mathbf{x}_{784}]$, we wish to generate a new sample $\mathbf{\hat{x}} = [\mathbf{\hat{x}}_1, \mathbf{\hat{x}}_2,\dots, \mathbf{\hat{x}}_{784}]$. The non-linear dependence structure is an intrinsic property of images so this example fits the purpose.

Again, following the steps presented in Sec. \ref{subsec:sgn_dependent_uniforms} for both SGN-C and SGN-D methods leads to a set of correlated uniforms and correlated samples, $\mathbf{\hat{u}}$ and $ \mathbf{\hat{x}}$, respectively. The covariance method is not enough to obtain a smooth detailed picture (see the generated digits in Fig. \ref{fig:sgn_Case2}b), nevertheless it is able to capture and reproduce in most of the cases the essence of the picture, i.e., the digit $4$. 
On the other hand, it is interesting to notice that the digits generated by the generator $G_x$ (Fig. \ref{fig:sgn_Case2}f) cannot be distinguished from the real, while the digits marginally back-projected from dependent uniforms coming from generator $G_u$ (Fig. \ref{fig:sgn_Case2}e) are rather blurry. Such effect is due to the discrete nature of the handwritten digit dataset distribution. Indeed, the joint distribution is mostly non zero around small spheres centered in $0$ and $1$, therefore the approximation of the marginals $F_{X_i}(x_i)$ results poor in the intermediate values (due to the steepness of the cumulative function). At the same time, the output of $G_u$ is a set of dependent uniforms with values uniformly distributed from $0$ to $1$ which have to be transformed into the sample space, using the poorly estimated $1D$ inverse cumulative distribution function. The second GAN $G_x$ solves this numerical issue by mapping the data from the uniform space into the sample one through highly non-linear smooth transformations. 

\begin{figure}
% strip if you want 2 columns
\centering
\includegraphics[scale = 0.6]{images/copula/Case2.pdf}
\caption{Comparison of high-dimensional samples (digits) generated using SGN-C (b)-(d) and SGN-D (e)-(f) approaches.}
\label{fig:sgn_Case2}
\end{figure}

\subsubsection{3) Celebrity faces database}
As last example, we propose a set of (cropped) $32\times 32$ color images representing faces of celebrities covering some pose variations and including different backgrounds. The motivation resides in the intrinsic continuous property of the distribution since all the colors are feasible, yielding to robust cumulative marginals. 
The CelebA dataset \cite{CelebA} contains more than $200K$ faces. To be coherent with previous examples and to focus more on the correct identification of the block components charged to generate both correlated and dependent uniforms rather than to their quality, we considered only the first $20000$ samples of the dataset.
Fig. \ref{fig:sgn_Case3} illustrates the results.

The methods involving covariance and parametric copulas perform poorly in terms of quality of the details but capture the relevant information of the image and replicate it in new blurry faces. On the contrary, GANs introduce higher non-linear dependence, thus harmonic details, the more the network trains itself. 

The most interesting part is that, conversely to the MNIST case where the dependent uniforms were correctly generated but erroneously back-projected to sample domain due to poor quantiles, this time the estimated marginals do not have steep gradients. Therefore, the projected generated samples are smooth as depicted in Fig. \ref{fig:sgn_Case3}e. Moreover, these samples are extremely similar to the output of the generator $G_x$ (Fig. \ref{fig:sgn_Case3}f) accordingly to the interpretation that $G_x$ mimes the quantile functions $F_{X_i}^{-1}(x_i)\; \forall i \in \{1,\dots,1024\}$. Fig. \ref{fig:sgn_Case3_u} illustrates an example of data representation (a) and data generation (b) in the uniform space.

\begin{figure}
% strip if you want 2 columns
\centering
\includegraphics[scale = 0.6]{images/copula/Case3.pdf}
\caption{Comparison of high-dimensional samples (faces) generated using SGN-C (b)-(d) and SGN-D (e)-(f) approaches.}
\label{fig:sgn_Case3}
\end{figure}

\begin{figure}
% strip if you want 2 columns
\centering
\includegraphics[scale = 0.8]{images/copula/Case3_u.pdf}
\caption{Dependent uniforms obtained from transform sampling of original data (a) and obtained as output of $G_u$ (b).}
\label{fig:sgn_Case3_u}
\end{figure}

\subsubsection{Quantitative evaluation}
While several measures have been introduced so far to assess the performance of a specific generative model, there is no consensus as to which measure is the most appropriate \cite{ProsCons}. Nevertheless, lately, measures that deal with embedding layers and feature space have found extensive use. For the purpose of this section, we decided to adopt three of them, in particular, the Inception Score (IS) \cite{InceptionScore}, the Frèchet Inception Distance (FID) \cite{FID}, and the Kernel Inception Distance (KID) \cite{KID} metrics.

The IS computes the average KL divergence between the conditional distribution of the images label $p(y|\mathbf{x})$ and the marginal distribution $p(y)=\mathbb{E}_{\mathbf{x}}[p(y|\mathbf{x})]$, on the pre-trained Inception Net \cite{InceptionV3}. It is defined as
$\exp(\mathbb{E}_{\mathbf{x}}[D_{\text{KL}}(p(y|\mathbf{x})||p(y))])$ and assumes high values for a low entropy of $p(y|\mathbf{x})$, achieved when samples are easily classifiable, and for a high entropy of $p(y)$, to favor diversity.

FID compares the statistics of generated samples to real ones by computing the Frèchet distance between two multivariate Gaussian distributions. Indeed, both data are projected into a feature space (Inception representations) wherein a Gaussian distribution fits them.  

\begin{equation}
\text{FID}(r,g) = ||\mu_r - \mu_g||_2^2 + \text{Tr}\biggl(\Sigma_r + \Sigma_g - 2(\Sigma_r \Sigma_g)^{\frac{1}{2}}\biggr),
\end{equation}
where $X_r \sim \mathcal{N}(\mu_r, \Sigma_r)$ and $X_g \sim \mathcal{N}(\mu_g, \Sigma_g)$ are outputs of a pool layer in the Inception Net \cite{InceptionV3} for real and generated samples, respectively. Low FID values correspond to better similarity in distribution.

However, the Gaussianity of the Inception representations is often not guaranteed. As a result of the ReLU activations, the representations are not negative with some components equal to zero \cite{KID}. To overcome such limitation, we decided to use the KID metric \cite{KID}. 
KID is the squared MMD (Sec.\ref{subsec:sgn_dependent_uniforms}) between the Inception representations. In particular, let $\phi(\cdot)$ be the function mapping the real ($\mathbf{x}_r$) and generated ($\mathbf{x}_g$) samples into the Inception representation, then
\begin{equation}
\text{KID}(r,g) = \text{MMD}^2(\mathcal{H},\phi(\mathbf{x}_r),\phi(\mathbf{x}_g)).
\end{equation}
We used the polynomial kernel $k(\mathbf{x},\mathbf{y})=(\frac{1}{d} \mathbf{x}\cdot \mathbf{y}^T +1)^3$, where $d$ is the representation dimension. Compared to FID, KID has the advantage of being independent from the distribution of the latent representation.

We evaluated the scores only for the generated samples in the CelebA dataset scenario. The Inception network is a deep CNN  pre-trained on the ImageNet dataset. Hence, datasets that are too semantically different from ImageNet would lead to poor inception scores.
Since we are not interested in getting the best performance out of our architecture, but rather in a fair comparison between the different methodologies, we look at relative results. In particular, Tab. \ref{tab:InceptionScores} reports the scores for the different methodologies adopted. 

\begin{table}[]
\centering
\begin{tabular}{l|c|c|c|c|c|c|}
\cline{2-7}
                                          & \multicolumn{6}{c|}{\textbf{CelebA Dataset ($32\times 32$)}} \\ \hline

\multicolumn{1}{|l|}{\textbf{Space}}     & \multicolumn{3}{c|}{\textbf{Uniform}}       & \multicolumn{3}{c|}{\textbf{Sample}}      \\ \hline \hline                                      
                                          
\multicolumn{1}{|l|}{\textbf{Method}}     & \textbf{IS}  & \textbf{FID} & \textbf{KID} & \textbf{IS} & \textbf{FID} & \textbf{KID}     \\ \hline \hline

\multicolumn{1}{|l|}{Real synthetic data} & $3.71$ & $25.6$     & $0.00$ &  $2.77$                    & $18.0$ & $0.00$ \\ \hline \hline
\multicolumn{1}{|l|}{Covariance}         & $3.43$ & $210$     & $0.19$ & $2.13$                    & $136$ & $0.11$                        \\ \hline
\multicolumn{1}{|l|}{Gaussian copula}    & $3.30$ & $208$     & $0.19$ & $2.16$                    & $136$ & $0.11$                        \\ \hline
\multicolumn{1}{|l|}{t-copula}            & $3.50$ & $195$     & $0.16$ & $2.13$                    & $125$ & $0.10$ \\ \hline \hline
\multicolumn{1}{|l|}{SGN-D ($G_u$)}         & $3.27$ & $76.6$     & $0.03$ &  &  &  \\ \hline
\multicolumn{1}{|l|}{SGN-D ($G_x$)}         &  &      &  & $2.61$                    & $70.9$ & $0.03$ \\ \hline 
\end{tabular}
\caption{Performance of SGN-C and SGN-D using the, IS, FID and KID measures on the CelebA dataset.}
\label{tab:InceptionScores}
\end{table}

The IS, FID and KID scores are consistent with human visual perceptions. Indeed, as depicted from visual intuition, there is almost no difference between the scores obtained from covariance and parametric copulas methods, while there is a significant gap between the achieved scores with the linear (SGN-C) and non-linear dependence (SGN-D) approaches.

\subsection{Summary}
This section of the chapter has firstly discussed some known procedures to generate correlated variables from a sample set, highlighting the need of a domain transformation (from the sample to the uniform variables domain). The same domain adaptation has been exploited for the generation of statistically dependent variables, recalling the concept of copula. This mathematical tool enables the partitioning and segmentation of the dependence structure generation into two well-defined steps, an initial step which creates the data dependence between uniform random variables (copula) followed by a second step which projects the uniform random variables back into the sample domain (inverse transform sampling), leading to new data. 
The former case has been analyzed through the aid of different copula structures while the latter case through an estimation of the marginal cumulative distribution (and its inverse), where the more samples are available, the better the approximation is. Such segmentation totally disregards the semantics of the input data and, in principle, can be applied to any type of data/signal. 

This procedure has led to the design of a segmented generative network architecture, based on GANs, successfully implemented as proved by several qualitative and quantitative results. This goes in the direction of explainable machine learning, i.e., a full comprehension of the design of a neural network through a mathematical segmentation of the problem.

In the following section, we describe how to apply such segmentation steps to explicitly estimate the PDF of the collected data.

\section{Copula density neural estimation}
\sectionmark{CODINE}
\label{sec:codine}
A natural way to discover data properties is to study the underlying PDF. Parametric and non-parametric models \cite{Silverman86} are viable solutions for density estimation problems that deal with low-dimensional data. The former are typically used when a prior knowledge on the data structure (e.g. distribution family) is available. The latter, instead, are more flexible since they do not require any specification of the distribution's parameters. Practically, the majority of methods from both classes fail in estimating high-dimensional densities. Hence, some recent works leveraged deep NNs as density estimators \cite{PixelRNN,dinh2017density}.
Although significant efforts have been made to scale NN architectures in order to improve their modeling capabilities, most of tasks translate into conditional distribution estimations. 
Instead, generative models attempt to learn the a-priori distribution to synthesize new data out of it. Deep generative models such as GANs \cite{Goodfellow2014}, VAEs \cite{Kingma2013} and diffusion models \cite{Ho2020}, tend to either implicitly estimate the underlying PDF or explicitly estimate a variational lower bound, providing the designer with no simple access to the investigated PDF. 

In this section, we propose to work with pseudo-observations, a projection of the collected observations into the uniform probability space via transform sampling, as described in Sec. \ref{subsec:sgn_proposal_approach}. The probability density estimation becomes a copula density estimation problem, which is formulated and solved using DL techniques. The envisioned copula density neural estimation method is referred to as CODINE. We further present self-consistency tests and metrics that can be used to assess the quality of the estimator. We prove and exploit the fact that the MI can be rewritten in terms of copula PDFs. Finally, we apply CODINE in the context of data generation.

\subsection{Variational approach for the copula density estimation}
\label{subsec:codine}
Let us assume that the collected $n$ data observations $\{\mathbf{x}^{(1)},\dots,\mathbf{x}^{(n)}\}$ are sampled from $p_{X}(\mathbf{x}) = p_{X}(x_1, x_2, \dots, x_d)$ with CDF $F_{X}(\mathbf{x}) = P(X_1\leq x_1, \dots, X_d \leq x_d)$.
Under similar assumptions as in Sec. \ref{subsec:sgn_transform_sampling}, 
the inverse transform sampling method can be used to map the data into the uniform probability space. In fact, if $U_i$ is a uniform random variable, then
$X_i = F^{-1}_{X_i}(U_i)$ is a random variable with CDF $F_{X_i}$.
Therefore, if the CDF is invertible, the transformation $u_i = F_{X_i}(x_i) \; \forall i=1,\dots,d$ projects the data $\mathbf{x}$ into the uniform probability space with finite distribution's support $u_i \in [0,1]$. The obtained transformed observations are typically called pseudo-observations.
In principle, the transform sampling method is extremely beneficial: it offers a statistical normalization, thus a pre-processing operation that constitutes the first step of any DL pipeline.

To characterize the nature of the transformed data in the uniform probability space, we use the concept of copula introduced in Sec. \ref{subsec:sgn_dependent_uniforms}.
We here recall that, when the multivariate distribution is described in terms of the PDF $p_{X}$, it holds that
\begin{equation}
\label{eq:Sklarf2}
p_{X}(x_1,\dots,x_d) = c_{U}(F_{X_1}(x_1),\dots,F_{X_d}(x_d))\cdot \prod_{i=1}^{d}{p_{X_i}(x_i)},
\end{equation}
where $c_{U}$ is the density of the copula.

The relation in \eqref{eq:Sklarf2} is the fundamental building block of this section. It separates the dependence internal structure of $p_{X}$ into two distinct components: the product of all the marginals $p_{X_i}$ and the density of the copula $c_{U}$. By nature, the former accounts only for the marginal information, thus, the statistics of each univariate variable. The latter, instead, accounts only for the joint dependence of data.

Considering the fact that building the marginals is usually a straightforward task, the estimation of the empirical joint density $\hat{p}_{X}(\mathbf{x})$ of the observations $\{\mathbf{x}^{(1)},\dots,\mathbf{x}^{(n)}\}$ passes through the estimation of the empirical copula density $\hat{c}_{U}(\mathbf{u})$ of the pseudo-observations $\{\mathbf{u}^{(1)},\dots,\mathbf{u}^{(n)}\}$. 

We propose to use deep NNs to model dependencies in high-dimensional data, and in particular to estimate the copula PDF. The proposed framework relies on the following simple idea: we can measure the statistical distance  between the pseudo-observations and uniform i.i.d. realizations using NN  parameterization. Surprisingly, by maximizing a variational lower bound on a divergence measure, we get for free the copula density neural estimator.

\subsubsection{Variational formulation}
\label{subsec:codine_f-div}
Given the definition of $f$-divergence introduced in \eqref{eq:f-divergence}, one might be interested in studying the particular case when the two densities $p$ and $q$ correspond to $c_U$ and $\pi_U$, respectively, where $\pi_U$ describes a multivariate uniform distribution on $[0,1]^d$. In such situation, it is possible to express the copula density function via the variational representation of the $f$-divergence. The following Theorem formulates an optimization problem whose solution yields to the desired copula density.
 
\begin{theorem}
\label{theorem:codine_theorem1}
Let $\mathbf{u} \sim c_U(\mathbf{u})$ be $d$-dimensional samples drawn from the copula density $c_U$. Let $f^*$ be the Fenchel conjugate of $f:\mathbb{R}_+ \to \mathbb{R}$, a convex lower semicontinuous function that satisfies $f(1)=0$ and has derivative $f^{\prime}$. If $\pi_U(\mathbf{u})$ is a multivariate uniform distribution on the unit cube $[0,1]^d$ and $\mathcal{J}_{f}(T)$ is a value function defined as 
\begin{equation}
\mathcal{J}_{f}(T) = \mathbb{E}_{\mathbf{u} \sim c_{U}(\mathbf{u})}\biggl[T\bigl(\mathbf{u}\bigr)\biggr] -\mathbb{E}_{\mathbf{u} \sim \pi_{U}(\mathbf{u})}\biggl[f^*\biggl(T\bigl(\mathbf{u}\bigr)\biggr)\biggr],
\label{eq:codine_discriminator_function_f}
\end{equation}
then
\begin{equation}
\label{eq:codine_optimal_ratio_T}
c_U(\mathbf{u}) = \bigl(f^{*}\bigr)^{\prime} \bigl(\hat{T}(\mathbf{u})\bigr), 
\end{equation}
where
\begin{equation}
\hat{T}(\mathbf{u}) = \arg \max_T \mathcal{J}_f(T),
\end{equation}
\end{theorem}
 
\begin{proof}
From the hypothesis, the $f$-divergence between $c_U$ and $\pi_U$ reads as follows
\begin{equation}
\small
D_f(c_U||\pi_U) = \int_{\mathbb{R}^d}{\pi_U(\mathbf{u})f\biggl(\frac{c_U(\mathbf{u})}{\pi_U(\mathbf{u})}\biggr)\diff \mathbf{u}} = \int_{[0,1]^d}{f\bigl(c_U(\mathbf{u})\bigr)\diff \mathbf{u}}.
\end{equation}
Moreover, from Lemma 1 of \cite{Nguyen2010}, $D_f$ can be expressed in terms of its lower bound via Fenchel convex duality
\begin{equation}
\small
\label{eq:codine_f_bound}
D_f(c_U||\pi_U) \geq \sup_{T\in \mathbb{R}} \biggl\{ \mathbb{E}_{\mathbf{u} \sim c_U(\mathbf{u})} \bigl[T(\mathbf{u})\bigr]-\mathbb{E}_{\mathbf{u}\sim \pi_U(\mathbf{u})}\bigl[f^*\bigl(T(\mathbf{u})\bigr)\bigr]\biggr\},
\end{equation}
where $T: [0,1]^d \to \mathbb{R}$ and $f^*$ is the Fenchel conjugate of $f$.
Since the equality in \eqref{eq:codine_f_bound} is attained for $T(\mathbf{u})$ as
\begin{equation}
\hat{T}(\mathbf{u}) = f^{\prime} \bigl(c_U(\mathbf{u})\bigr),
\end{equation}
it is sufficient to find the function $\hat{T}(\mathbf{u})$ that maximizes the variational lower bound $\mathcal{J}_{f}(T)$.
Finally, by Fenchel duality it is also true that
$c_U(\mathbf{u}) = \bigl(f^{*}\bigr)^{\prime} \bigl(\hat{T}(\mathbf{u})\bigr)$.
\end{proof}

Notice that the density of the copula can be derived with the same approach also by working in the sample domain. Indeed, when $p$ and $q$ correspond to the joint and the product of the marginals, respectively, the following corollary holds.

\begin{corollary}
\label{cor:codine_cor1}
Let $\mathbf{x} \sim p_X(\mathbf{x})$ be $d$-dimensional samples drawn from the joint density $p_X$. Let $f^*$ be the Fenchel conjugate of $f:\mathbb{R}_+ \to \mathbb{R}$, a convex lower semicontinuous function that satisfies $f(1)=0$ and has derivative $f^{\prime}$. If $\pi_X(\mathbf{x})$ is the product of the marginals $p_{X_i}(x_i)$ and $\mathcal{J}_{f}(T)$ is a value function defined as 
\begin{equation}
\label{eq:codine_discriminative_joint_marg}
\mathcal{J}_{f,x}(T) = \mathbb{E}_{\mathbf{x} \sim p_{X}(\mathbf{x})}\biggl[T\bigl(\mathbf{x}\bigr)\biggr] -\mathbb{E}_{\mathbf{x} \sim \pi_{X}(\mathbf{x})}\biggl[f^*\biggl(T\bigl(\mathbf{x})\bigr)\biggr)\biggr],
\end{equation}
then
\begin{equation}
c_U(\mathbf{u}) = \bigl(f^{*}\bigr)^{\prime} \bigl(\hat{T}(\mathbf{F}_X^{-1}(\mathbf{u}))\bigr)
\end{equation}
is the copula density, where
\begin{equation}
\mathbf{F}_X^{-1}(\mathbf{u}) := [F_{X_i}^{-1}(u_i),\dots,F_{X_d}^{-1}(u_d)]
\end{equation}
and
\begin{equation}
\hat{T}(\mathbf{x}) = \arg \max_T \mathcal{J}_{f,x}(T),
\end{equation}
\end{corollary}
 
\begin{proof}
From the hypothesis, the $f$-divergence between $p_X$ and $\pi_X$ reads as follows
\begin{align}
D_f(p_X||\pi_X) & = \int_{\mathbb{R}^d}{\prod_{i}{p_{X_i}(x_i)}f\biggl(\frac{p_X(\mathbf{x})}{\prod_{i}{p_{X_i}(x_i)}}\biggr)\diff \mathbf{x}} \nonumber \\
& = \int_{[0,1]^d}{f\bigl(c_U(\mathbf{u})\bigr)\diff \mathbf{u}},
\end{align}
where $c$ is the density of the copula obtained as in \eqref{eq:Sklarf2}. The thesis then follows immediately from Theorem \ref{theorem:codine_theorem1} 1.
\end{proof}

A great advantage of the formulation in \eqref{eq:codine_discriminator_function_f} comes from the second expectation term. Conversely to the variational discriminative formulation in \eqref{eq:codine_discriminative_joint_marg} that tests jointly with marginals samples, the comparison in \eqref{eq:codine_discriminator_function_f} is made between samples from the joint copula structure and independent uniforms. The latter can be easily generated without the need of any scrambler that factorizes $p_X$ into the product of the marginal PDFs. On the other hand, \eqref{eq:codine_discriminator_function_f} needs samples from the copula, thus, needs an estimate of the marginals of $X$ to apply transform sampling. In the following, we use the formulation in \eqref{eq:codine_discriminator_function_f} which inherently possesses another desired property when using NNs.

\subsubsection{Parametric implementation}
\label{subsec:codine_implementation}
To proceed, we propose to parametrize $T(\mathbf{u})$ with a deep NN $T_{\theta}$ of parameters $\theta$ and maximize $\mathcal{J}_{f}(T)$ with gradient ascent and back-propagation $\hat{\theta} = \arg \max_{\theta} \mathcal{J}_f(T_{\theta})$.
Since at convergence the network outputs a transformation of the copula density evaluated at the input $\mathbf{u}$, the final layer possesses a unique neuron with activation function that depends on the generator $f$ (see the code \cite{CODINE_github} for more details).
The resulting estimator of the copula density reads as follows
\begin{equation}
\hat{c}_U(\mathbf{u}) = \bigl(f^{*}\bigr)^{\prime} \bigl(T_{\hat{\theta}}(\mathbf{u})\bigr),
\end{equation}
and its training procedure enjoys two normalization properties. 

The former consists in a natural normalization of the input data in the interval $[0,1)$ via transform sampling that facilitates the training convergence and helps producing improved dependence measures \cite{Poczos2012}. The latter normalization property is perhaps at the core of the proposed methodology. The typical problem in creating neural density estimators is to enforce the network to return densities that integrate to one
\begin{equation}
\label{eq:codine_density_test}
\int_{\mathbb{R}^d}{p_X(\mathbf{x};\theta)\diff \mathbf{x}} = 1
\end{equation}
Energy-based models have been proposed to tackle such constraint, but they often produce intractable densities (due to the normalization factor, see \cite{Papamakarios2015}). Normalizing flows \cite{Rezende2015} provide exact likelihoods but they are limited in representation. In contrast, the discriminative formulation of \eqref{eq:codine_discriminator_function_f} produces a copula density neural estimator that naturally favors a solution of \eqref{eq:codine_density_test}, without any architectural modification or regularization term.

\subsubsection{Evaluation measures}
\label{subsec:codine_metrics}
\begin{figure}[t]
	\centering
	\includegraphics[scale=0.3]{images/copula/q_c_paper_fig_1b.pdf}
	\caption{Approximation quality of $c_V$. Comparison between CODINE method $\hat{c}_V$ and the flat copula density $\pi_V$ for different values of the signal-to-noise ratio (SNR) and for different dimensionality $d$ of the input.}
	\label{fig:codine_q_c}
\end{figure}
To assess the quality of the copula density estimator $\hat{c}_U(\mathbf{u})$, we propose the following set of self-consistency tests over the basic property illustrated in \eqref{eq:codine_density_test}. In particular,
\begin{enumerate}
\item if $\hat{c}_U(\mathbf{u})$ is a well-defined density and $\hat{c}_U(\mathbf{u})=c_U(\mathbf{u})$, then the following relation must hold
\begin{equation}
\mathbb{E}_{\mathbf{u} \sim \pi_{U}(\mathbf{u})}\bigl[\hat{c}_U(\mathbf{u})\bigr]=1,
\end{equation}
\item in general, for any $n$-th order moment, if $\hat{c}_U(\mathbf{u})$ is a well-defined density and $\hat{c}_U(\mathbf{u})=c_U(\mathbf{u})$, then
\begin{equation}
\mathbb{E}_{\mathbf{u} \sim \pi_{U}(\mathbf{u})}\bigl[\mathbf{u}^n\cdot \hat{c}_U(\mathbf{u})\bigr]=\mathbb{E}_{\mathbf{u} \sim c_{U}(\mathbf{u})}\bigl[\mathbf{u}^n\bigr].
\end{equation}
\end{enumerate}
The first test verifies that the copula density integrates to one while the second set of tests extends the first test to the moments of any order. Similarly, joint consistency tests can be defined, e.g., the Spearman rank correlation $\rho_{X,Y}$ between pairs of variables can be rewritten in terms of their joint copula density $\hat{c}_{UV}$ and it reads as follows
\begin{equation}
\rho_{X,Y} = 12 \cdot\mathbb{E}_{(\mathbf{u},\mathbf{v}) \sim \pi_{U}(\mathbf{u})\pi_{V}(\mathbf{v})}\bigl[\mathbf{u} \mathbf{v}\cdot \hat{c}_{UV}(\mathbf{u},\mathbf{v})\bigr]-3.
\end{equation}
\begin{figure}[t]
	\centering
	\includegraphics[scale=0.37]{images/copula/copula_approx_fig1a.pdf}
	\caption{Ground-truth and estimated copula density (SNR=$0$ dB) at the channel output ($d=2$) using the GAN $f$ generator for: a) uncorrelated noise $\rho=0$. b) correlated noise with coefficient $\rho=0.5$.}
	\label{fig:codine_copula_approx}
\end{figure}
When the copula density is known, it is possible to assess the quality of the copula density neural estimator $\hat{c}_{U}(\mathbf{u})$ by computing the KL divergence between the true and the estimated copulas
\begin{equation}
Q_c = D_{\text{KL}}(c_{U}||\hat{c}_{U}) = \mathbb{E}_{\mathbf{u}\sim c_{U}(\mathbf{u})}\biggl[\log\frac{c_{U}(\mathbf{u})}{\hat{c}_{U}(\mathbf{u})}\biggr].
\end{equation}

Once the dependence structure is characterized via a valid copula density $\hat{c}_{U}(\mathbf{u})$, a multiplication with the estimated marginal components $\hat{p}_{X_i}(x_i)$, $\forall i=\{1,\dots,d\}$ yields the estimate of the joint PDF $\hat{p}_{X}(\mathbf{x})$. In general, it is rather simple to build one-dimensional marginal density estimates $\hat{p}_{X_i}(x_i)$, e.g., using histograms or kernel functions. 

Now, as a first example to validate the density estimator, we consider the transmission of $d$-dimensional Gaussian samples over an additive white Gaussian channel (AWGN). Given the  AWGN model $Y=X+N$, where $X \sim \mathcal{N}(0,\mathbb{I})$ and $N \sim \mathcal{N}(0,\Sigma_N)$, it is simple to obtain closed-form expressions for the probability densities involved. 
Indeed, the copula joint density function reads as follows
\begin{equation}
\small
c_{UV}(\mathbf{u},\mathbf{v}) = \sqrt{\frac{\det(\tilde{\Sigma}_N+\mathbb{I})}{\det(\Sigma_N)}}\frac{\exp\bigl(-\frac{1}{2}\bigl(\mathbf{F}_Y^{-1}(\mathbf{v})-\mathbf{F}_X^{-1}(\mathbf{u})\bigr)^T\Sigma_N^{-1}\bigl(\mathbf{F}_Y^{-1}(\mathbf{v})-\mathbf{F}_X^{-1}(\mathbf{u})\bigr)\bigr)}{\exp\bigl(-\frac{1}{2}\bigl(\mathbf{F}_Y^{-1}(\mathbf{v})\bigr)^T\bigl(\tilde{\Sigma}_N+\mathbb{I}\bigr)^{-1}\bigl(\mathbf{F}_Y^{-1}(\mathbf{v})\bigr)\bigr)},
\end{equation}
whereas, the copula density of the output $Y$ assumes form as
\begin{equation}
\small
\label{eq:codine_copula_gaussian_y}
c_{V}(\mathbf{v}) = \sqrt{\frac{\det(\tilde{\Sigma}_N+\mathbb{I})}{\det(\Sigma_N+\mathbb{I})}}{\exp\biggl(-\frac{1}{2}\bigl(\mathbf{F}_Y^{-1}(\mathbf{v})\bigr)^T\bigl(\bigl(\Sigma_N+\mathbb{I}\bigr)^{-1}-\bigl(\tilde{\Sigma}_N+\mathbb{I}\bigr)^{-1}\bigr)\bigl(\mathbf{F}_Y^{-1}(\mathbf{v})\bigr)\biggr)},
\end{equation}
 where $\mathbf{F}_X(\mathbf{x})$ is an operator that element-wise applies transform sampling (via Gaussian cumulative distributions) to the components of $\mathbf{x}$ such that $(\mathbf{u},\mathbf{v}) = (\mathbf{F}_X(\mathbf{x}),\mathbf{F}_Y(\mathbf{y}))$ and $\tilde{\Sigma}_N = \Sigma_N \odot \mathbb{I}$, where $\mathbf{A} \odot \mathbf{B}$ denotes the Hadamard product between matrices $\mathbf{A}$ and $\mathbf{B}$. 

In Fig. \ref{fig:codine_q_c} we illustrate the KL divergence (in bits) between the ground-truth and the neural estimator obtained using the GAN generator $f(u)$ reported in Tab. \ref{tab:codine_generators}. To work with non-uniform copula structures, we study the case of non-diagonal noise covariance matrix $\Sigma_N$. In particular, we impose a tridiagonal covariance matrix such that $\Sigma_N=\sigma_N^2 R$ where $R_{i,i} = 1$ with $i=1,\dots,d$, and $R_{i,i+1}=\rho$, with $i=1,\dots,d-1$ and $\rho=0.5$. Moreover, Fig. \ref{fig:codine_q_c} depicts the quality of the approximation for different values of the signal-to-noise ratio (SNR), defined as the reciprocal of the noise power $\sigma_N^2$, and for different dimensions $d$. To provide a numerical comparison, we also report the KL divergence $D_{\text{KL}}(c_{V}||\pi_{V})$ between the ground-truth and the flat copula density $\pi_V = 1$. It can be shown that when $c_V$ is Gaussian, we obtain
\begin{equation}
D_{\text{KL}}(c_{V}||\pi_{V}) = \frac{1}{2}\log\biggl(\frac{\det(\tilde{\Sigma}_N+\mathbb{I})}{\det(\Sigma_N+\mathbb{I})}\biggr),
\end{equation}
where the proof uses the theorem on the expectation of quadratic forms, which states that
\begin{equation}
    \mathbb{E}_{\mathbf{x} \sim p_X(\mathbf{x})}[X^T\mathbf{A}X] = \mu^T\mathbf{A}\mu + \Tr(\mathbf{A}\Sigma_X).
\end{equation}

Notice that in Fig. \ref{fig:codine_q_c} we use the same simple NN architecture for both $d=2$ and $d=10$. Nonetheless, CODINE can approximate multidimensional densities even without any further hyper-parameter search. 
Fig. \ref{fig:codine_copula_approx}a reports a comparison between ground-truth and estimated copula densities at $0$ dB in the case of independent components ($\rho=0$) and correlated components ($\rho=0.5$). It is worth mentioning that when there is independence between components, the copula density is everywhere unitary $c_V(\mathbf{v})$$=$$1$. Hence, independence tests can be derived based on the structure of the estimated copula via CODINE, but we leave it for future discussions.


\begin{table}[t!]
\centering
\begin{tabular}{l|l|l}
Name & Generator $f(u)$                   & Conjugate $f^*(t)$  \\ \hline
GAN  & $u\log u -(u+1)\log (u+1)+\log(4)$ & $-\log (1-\exp(t))$ \\ \hline
KL   & $u\log u$                          & $\exp(t-1)$         \\ \hline
HD   & $(\sqrt{u}-1)^2$                   & $t/(1-t)$     \\ \hline
\end{tabular}
\caption{List of generator and conjugate functions used in the experiments.}
\label{tab:codine_generators}
\end{table} 

\subsection{Applications}
\label{subsec:codine_applications}

\subsubsection{Mutual information estimation}
Given two random variables, $X$ and $Y$, the MI $I(X;Y)$ quantifies the statistical dependence between $X$ and $Y$. It measures the amount of information obtained about one variable via the observation of the other and it can be rewritten also in terms of KL divergence as
${I}(X;Y) = D_{\text{KL}}(p_{XY}||p_Xp_Y)$.
From Sklar's theorem, it is simple to show that the MI can be computed using only copula densities as follows
\begin{equation}
\label{eq:codine_mutual_information_copula}
{I}(X;Y) = \mathbb{E}_{(\mathbf{u},\mathbf{v})\sim c_{UV}(\mathbf{u},\mathbf{v})}\biggl[\log\frac{c_{UV}(\mathbf{u},\mathbf{v})}{c_U(\mathbf{u}) c_V(\mathbf{v})}\biggr].
\end{equation}
Therefore, \eqref{eq:codine_mutual_information_copula} requires three separate copula densities estimators, each of which obtained as explained in Sec. \ref{subsec:codine}. Alternatively, one could learn the copulas density ratio via maximization of the variational lower bound on the MI (see Ch. \ref{sec:mi_estimators}). Using again Fenchel duality, the KL divergence
\begin{equation}
D_{\text{KL}}(c_{UV}||c_Uc_V) = \int_{[0,1]^{2d}}{c_{UV}(\mathbf{u},\mathbf{v})\log\biggl(\frac{c_{UV}(\mathbf{u},\mathbf{v})}{c_U(\mathbf{u})c_V(\mathbf{v})}\biggr)\diff \mathbf{u} \diff \mathbf{v}}
\end{equation}
corresponds to the supremum over $T$ of
\begin{equation}
\label{eq:codine_DIME_copula}
\mathcal{J}_{\text{KL}}(T) = \mathbb{E}_{c_{UV}}\biggl[T\bigl(\mathbf{u},\mathbf{v}\bigr)\biggr] - \mathbb{E}_{c_{U}c_{V}}\biggl[\exp{\bigl(T \bigl(\mathbf{u},\mathbf{v }\bigr)-1 \bigr)}\biggr].
\end{equation}
When $X$ is a univariate random variable, its copula density $c_U$ is unitary. Notice that \eqref{eq:codine_DIME_copula} can be seen as a special case of the more general \eqref{eq:codine_discriminator_function_f} when $f$ is the generator of the KL divergence and the second expectation is not computed over independent uniforms with distribution $\pi_U$ but over samples from the product of copula densities $c_U\cdot c_V$. 
We estimate the MI between $X$ and $Y$ in the AWGN model using \eqref{eq:codine_DIME_copula} and the generators described in Tab. \ref{tab:codine_generators}. Fig. \ref{fig:codine_i_x_y}a and Fig. \ref{fig:codine_i_x_y}b show the estimated MI for $d=1$ and $d=5$, respectively, and compare it with the closed-form capacity formula $I(X;Y) = d/2\log_2(1+\text{SNR})$. 

\begin{figure}
	\centering
	\includegraphics[scale=0.35]{images/copula/i_x_y_paper_fig_2.pdf}
	\caption{Estimated mutual information $I(X;Y)$ via joint copula $c_{UV}$ with different generators $f$ for: a) $d=1$. b) $d=5$.}
	\label{fig:codine_i_x_y}
\end{figure}

\subsubsection{Data generation}
\begin{figure}[b]
	\centering
	\includegraphics[scale=0.38]{images/copula/codine_generation_method.pdf}
	\caption{CODINE generation strategy.}
	\label{fig:codine_generation}
\end{figure}
As a second application example, we generate $n$ new pseudo-observations $\{\hat{\mathbf{u}}^{(1)},\dots,\hat{\mathbf{u}}^{(n)}\}$ from $\hat{c}_U$ by deploying a Markov chain Monte Carlo (MCMC) algorithm. Validating the quality of the generated data provides an alternative path for assessing the copula estimate itself.
We propose to use Gibbs sampling to extract valid uniform realizations of the copula estimate. In particular, we start with an initial guess $\hat{\mathbf{u}}^{(0)}$ and produce next samples $\hat{\mathbf{u}}^{(i+1)}$ by sampling each component from univariate conditional densities $\hat{c}_U(u_j^{(i+1)}|u_1^{(i+1)},\dots,u_{j-1}^{(i+1)},u_{j+1}^{(i)},\dots,u_{d}^{(i)})$ for $j=1,\dots,d$.
It is clear that the generated data in the sample domain is obtained via inverse transform sampling through the estimated quantile functions $\hat{F}_{X_i}^{-1}$.
The proposed generation scheme is illustrated in Fig. \ref{fig:codine_generation}.

Consider a bi-dimensional random variable whose realizations have form $\mathbf{x} = [\mathbf{x}_1, \mathbf{x}_2]$ and for which we want to generate new samples $\mathbf{\hat{x}} = [\mathbf{\hat{x}}_1, \mathbf{\hat{x}}_2]$. To force a non-linear statistical dependence structure, we define the same toy example $\mathbf{x}$ as in \eqref{eq:sgn_toy2D}.
We use CODINE to estimate its copula density and sample from it via Gibbs sampling. Fig. \ref{fig:codine_toy} compares the copula density estimate obtained via kernel density estimation (Fig. \ref{fig:codine_toy}a) with the estimate obtained using CODINE (Fig. \ref{fig:codine_toy}b). It also shows the generated samples in the uniform (Fig. \ref{fig:codine_toy}d) and in the sample domain (Fig. \ref{fig:codine_toy}f). 
It is plausible that the Gibbs sampling mechanism produced some discrepancies between $\mathbf{x}$ and $\hat{\mathbf{x}}$.

\begin{figure}[t]
	\centering
	\includegraphics[scale=0.35]{images/copula/subplot_toy_copula.pdf}
	\caption{Toy example contour plot and marginal densities of a) ground-truth copula density $c_u$ obtained using kernel density estimation and b) copula density neural estimate $\hat{c}_u$. c) Pseudo-observations. d) Data generated in the uniform probability space via Gibbs sampling. e) Observations. f) Data generated in the sample domain via inverse transform sampling.}
	\label{fig:codine_toy}
\end{figure}

As a more complex example, we report the generation of the MNIST handwritten digits of size $28 \times 28$. In particular, we study the copula density of the latent space obtained from an AE. 
With such experiment, we firstly prove that the approach works even for data of dimension $d = 25$ and we secondly want to emphasize the fact that the approach is potentially scalable to higher dimensions. 
The idea is to train a CNN AE (see Fig. \ref{fig:codine_architecture}) to reconstruct digits. During training, the AE learns latent representations (via an encoder mapping) that can be analyzed and synthesized with CODINE. 

\begin{figure}[t]
	\centering
	\includegraphics[scale=0.4]{images/copula/autoencoder.pdf}
	\caption{Autoencoder architecture used to learn latent vectors ($d=25$). The autoencoder is trained with binary cross-entropy}
	\label{fig:codine_architecture}
\end{figure}

The analysis block (Fig. \ref{fig:codine_generation} comprises a first marginal block to project the latent representations into the uniform probability space. Then, a CODINE block learns the copula density which is used by Gibbs sampling to generate new uniform latent representations. An inverse transform sampling block maps data from the uniform to the sample space.
Once new latent samples are generated, it is possible to feed them into the pre-trained decoder and obtain new digits, as illustrated in Fig. \ref{fig:codine_digits}.

\begin{figure}[b]
	\centering
	\includegraphics[scale=0.15]{images/copula/codine_digits_review.pdf}
	\caption{100 randomly selected digits obtained using CODINE to generate the latent vector fed in the decoder. Comparison with test data.}
	\label{fig:codine_digits}
\end{figure}

Although the CODINE block $T(\mathbf{u})$ is a simple shallow NN, the generated digits visually resemble the original ones, meaning that our approach managed to, at least partially, estimate the density of the uniform latent representations.

\subsubsection{An idea to drive the generation with the copula estimate}
MCMC methods require an increasing amount of time to sample from high-dimensional distributions. Thus, we study if it is possible to devise a neural sampling mechanism which exploit the copula density guidance. In particular, we exploit the MMD measure defined in \eqref{eq:sgn_MMD}. Indeed, let $(\chi,d)$ be a nonempty compact metric space in which two copula densities, $c_U(\mathbf{u})$ and $c_V(\mathbf{v})$, are defined. Then, the MMD reads as
\begin{equation}
\text{MMD}(\mathcal{G},c_U,c_V) := \sup_{g\in \mathcal{G}}\bigl\{\mathbb{E}_{\mathbf{u}\sim c_U(\mathbf{u})}[g(\mathbf{u})]-\mathbb{E}_{\mathbf{v}\sim c_V(\mathbf{v})}[g(\mathbf{v})]\bigr\},
\label{eq:codine_MMD}
\end{equation}
where $\mathcal{G}$ is a class of functions $g:\chi \rightarrow \mathbb{R}$. 
Since $c_U=c_V$ if and only if $\mathbb{E}_{\mathbf{u}\sim c_U(\mathbf{u})}[g(\mathbf{u})]=\mathbb{E}_{\mathbf{v}\sim c_V(\mathbf{v})}[g(\mathbf{v})]$ $\forall g\in \mathcal{G}$, the MMD measures the disparity between $c_U$ and $c_V$. If $g(\mathbf{u})=c_U(\mathbf{u})$ is a valid function, we can define a plausible loss function based on the MMD metric as follows
\begin{equation}
\min_{\theta_G} \mathbb{E}_{\mathbf{u}\sim c_U(\mathbf{u})}[c_U(\mathbf{u})]-\mathbb{E}_{\mathbf{v}\sim \pi_V(\mathbf{v})}[c_U(G(\mathbf{v};\theta_G))].
\label{eq:codine_MMD-metric}
\end{equation}
Thus, given $n$ pseudo-observations $\{\mathbf{u}^{(1)},\dots,\mathbf{u}^{(n)}\}$ for which we have built and estimated the underlined $c_U(\mathbf{u})$, it is possible to design a NN architecture, the generator $G$, which maps independent uniforms with distribution $\pi_V$ into uniforms with distribution $c_{V,\theta_G}$. The guidance provided by $c_U(\mathbf{u})$ helps minimizing the discrepancy between the two copulas when the optimization is performed over $\theta_G$. The optimal generator resulting from the solution of \eqref{eq:codine_MMD-metric} synthesizes new pseudo-observations $\hat{\mathbf{u}} = G(\mathbf{v};\theta_G^*)$.

To verify if $c_U$ is a properly defined function, it is useful to notice that the Wasserstein metric, and in particular the Kantorovich Rubinstein duality, links with the MMD in \eqref{eq:codine_MMD} for a class of functions $\mathcal{G}$ that are $K$-Lipschitz continuous
\begin{equation}
\label{eq:codine_WGAN}
W(c_U,c_V) = \frac{1}{K}\sup_{||h||_L\leq K}\biggl[ \mathbb{E}_{\mathbf{u}\sim c_U(\mathbf{u})}[h(\mathbf{u})]-\mathbb{E}_{\mathbf{v}\sim c_V(\mathbf{v})}[h(\mathbf{v}))\biggr].
\end{equation}
Under such conditions, \eqref{eq:codine_MMD-metric} can be interpreted as the generator loss function of a Wasserstein-GAN \cite{WGAN} where the optimum discriminator $h$ is supposed to be known and corresponds to the learnt copula density $c_U$. The proposed idea lies in between two established approaches. The first one, from generative moment matching networks \cite{GMMs}, assumes $\mathcal{G}$ as the reproducing kernel Hilbert space where $g$ is a kernel $k \in \mathcal{H}$ and the supremum in \eqref{eq:codine_MMD} is thus attained. Such MMD-based approach only optimizes over the generator's parameters but does not produce expressive generators, mainly because of the restriction imposed by the kernel structure. The second, instead, requires to learn both the generator and the discriminator, the latter in order to reach the supremum in \eqref{eq:codine_WGAN}. However, enforcing the Lipschitz constraint is not trivial and the alternation between generator and discriminator training suffers from the usual instability and slow convergence problems of GANs. Even if the copula-based approach does not claim optimality, it possesses two desirable properties: compared to kernel-based methods, it uses a more powerful and appropriate discriminator, the copula density itself. Moreover, the fact that $c_U$ is obtained from a prior analysis renders the generator learning process uncoupled from the discriminator's one.

The last concept serves as a foundational seed idea, inviting future exploration and offering ample room for subsequent considerations and applications.

\subsection{Summary}
\label{subsec:codine_conclusions}
This section presented CODINE, a copula density neural estimator. It works by maximizing a variational lower bound on the $f$-divergence between two distributions defined on the uniform probability space, namely, the distribution of the pseudo-observations, i.e., the copula, and the distribution of independent uniforms. CODINE also provides alternative approaches for measuring statistical dependence such as the MI, and for data generation.