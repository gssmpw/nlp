\chapter{Deep Learning Fundamentals} % 
\chaptermark{Deep Learning Fundamentals}
%\thispagestyle{empty}
\label{sec:fundamentals}
Tom Mitchell's ML preface \cite{Mitchell1997} defines ML as:
\quotes{A computer program is said to learn from experience $E$ with respect to some class of tasks $T$ and performance measure $P$, if its performance at tasks in $T$, as measured by $P$, improves with experience $E$}.
The experience is primarily described by the amount and quality of data used for the learning process. 
According to different interpretations
of the experience, it is possible to divide the learning approach into supervised, unsupervised and reinforcement learning. 

In the following, we recall different learning strategies and statistical tools we use throughout the thesis, keeping in mind that, in communication theory, signals are studied as stochastic processes. 

\section{Learning theory concepts}
\sectionmark{Learning theory concepts}
\label{sec:distances}
This section introduces key learning concepts that are essential for understanding subsequent chapters.
\subsection{Supervised learning}
Let $(\mathbf{x}_i,\mathbf{y}_i)\sim p_{XY}(\mathbf{x},\mathbf{y})$, $i=1,\dots, N$, be samples collected into a training set $\mathcal{D}$ belonging to the joint probability density function  (PDF) $p_{XY}(\mathbf{x},\mathbf{y})$. Probabilistic supervised learning predicts $\mathbf{y}$ from $\mathbf{x}$ by estimating $p_{Y|X}(\mathbf{y}|\mathbf{x})$ under a \textit{discriminative model} or by estimating the joint distribution $p_{XY}(\mathbf{x},\mathbf{y})$ under a \textit{generative model}. 

A \textit{regression} problem comprises a continuous output $\mathbf{y}$, meanwhile a discrete target is associated to a \textit{classification} problem.
The standard way to formulate the learning process is to define a cost function $C$, namely a performance measure that evaluates the quality of the prediction $\mathbf{\hat{y}}$. In most applications, we can rely only on the observed dataset $\mathcal{D}$ and derive an empirical sample distribution since we do not have knowledge of the true joint distribution $p_{XY}(\mathbf{x},\mathbf{y})$. In particular, the training objective minimizes
\begin{equation}
\label{eq:general_cost}
C(\mathbf{\hat{y}}) = \mathbb{E}_{(\mathbf{x},\mathbf{y}) \sim \mathcal{D}}[\delta(\mathbf{y},\mathbf{\hat{y}})]
\end{equation}
where $\delta$ is a measure of distance between the desired target $\mathbf{y}$ and the prediction $\mathbf{\hat{y}}$.

\subsection{Unsupervised learning}
Let $\mathbf{x}_i\sim p_X(\mathbf{x})$, $i=1,\dots, N$, be samples collected into a training set $\mathcal{D}$ belonging to the PDF $p_X(\mathbf{x})$.
Unsupervised learning aims at finding useful properties of the structure of a dataset $\mathcal{D}$, ideally inferring the true unknown distribution $p_X(\mathbf{x})$.

Several different tasks are solved using unsupervised learning, for instance: clustering, which divides the data into cluster of similar samples; feature extraction, which transforms data in a different latent space easier to handle and interpret; density estimation and generation/synthesis of new samples. The latter objective consists of learning, from data in $\mathcal{D}$, the distribution $p_X(\mathbf{x})$ and producing new unseen samples from it.

Unsupervised learning tasks require the introduction of an hidden variable $\mathbf{z}_i$ for each sample $\mathbf{x}_i$, leading to the selection of different models under a probabilistic approach. In the \textit{discriminative models}, the latent code $\mathbf{z}_i$ is extracted from $\mathbf{x}_i$ by defining a probabilistic mapping $p_{Z|X}(\mathbf{z|x};\theta)$ parameterized by $\theta$. \textit{Autoencoders} encode $\mathbf{x}_i$ into a latent variable $\mathbf{z}_i$ so that recovering $\mathbf{x}_i$ from $\mathbf{z}_i$ is possible through a decoder. The encoder models the posterior distribution $p_{Z|X}(\mathbf{z|x};\theta)$, while the decoder models the likelihood $p_{X|Z}(\mathbf{x|z};\theta)$.
Lastly, in the \textit{generative models}, an hidden variable $\mathbf{z}_i$ generates the observation $\mathbf{x}_i$. After a specification of a parameterized family $p_Z(\mathbf{z}|\theta)$, the distribution of the observation can be rewritten as $p_X(\mathbf{x|\theta})=\sum_z p_Z(\mathbf{z|\theta})p_{X|Z}(\mathbf{x|z;\theta})$.
Fig. \ref{img:taxonomy} schematically
summarizes the discussed learning models.

\begin{figure}
\centering
	\includegraphics[width=\textwidth]{images/fundamentals/LearningModels.png}
	\caption{Taxonomy of learning models. Deterministic models extract either fixed relationships between input and output (supervised) or patterns of the input (unsupervised). Probabilistic discriminative models use the input to predict either the output (supervised) or the hidden variable causing the input (unsupervised). Probabilistic generative models learn the statistical relationship between either input and output (unsupervised) or input and the hidden variable (supervised). Autoencoders model how to encode the input into the hidden variable, as well as how to decode from the hidden variable to the input.}
	\label{img:taxonomy}
\end{figure}

\subsection{Reinforcement learning}
Reinforcement learning (RL) addresses the problem of an \textit{agent} learning to act in a dynamic \textit{environment} by finding the best sequence of actions that maximizes a \textit{reward} function. The basic idea is that the agent explores the interactive environment. According to the observation experience it gets, it changes his actions in order to receive higher rewards. 

Basic RL can be modeled as a Markov decision process (MDP). Let $S_t$ be the observation (or state) provided to the agent at time $t$. The agent reacts by selecting an action $A_t$ to obtain from the environment the updated reward $R_{t+1}$, the discount $\gamma_{t+1}$, and the next state $S_{t+1}$.
In particular, the agent-environment interaction is formalized by a tuple $\langle \mathcal{S}, \mathcal{A}, T,r,\gamma\rangle$ where $\mathcal{S}$ is a finite set of states, $\mathcal{A}$ is a finite set of actions, $T(s,a,s')=P[S_{t+1}=s'|S_t = s, A_t = a]$ is the transition probability from state $s$ to state $s'$ under the action $a$, $r(s,a) = \mathbb{E}[R_{t+1}|S_t = s, A_t = a]$ is the reward function, and $\gamma \in [0,1]$ is a discount factor.
To find out which actions are good, the agent builds a \textit{policy}, i.e, a map $\pi : \mathcal{S} \times \mathcal{A} \to [0,1]$ that defines the probability of taking an action $a$ when the state is $s$. If we denote with $G_t = \sum_{k=0}^{\infty}{\biggl(\prod_{i=1}^{k}{\gamma_{t+i}}\biggr) R_{t+k+1}}$ the discount return, then the goal of the agent is to maximize the expected discount return, i.e., \textit{value} $q^{\pi}(s,a) = \mathbb{E}_{\pi}[G_t|S_t = s, A_t = a]$, by finding a good policy $\pi(s,a)$. 

RL algorithms can be categorized as \cite{SuttonRL,ArulkumaranDRL}: a) policy based methods, when the agent, given the observation as input, optimizes the policy $\pi$ without using a value function $q$; b) value based methods, when the agent, given the observation and the action as inputs, learns a value function $q$; c) actor critic methods, where a \textit{critic} measures how good the action taken is (value-based), and an \textit{actor} controls the behaviour of the agent (policy-based).
Despite several applications of reinforcement learning for physical layer communications (see Ch. 9 of \cite{Eldar2022}), the thesis mostly focuses on the first two learning approaches.

\subsection{Maximum likelihood estimation}
Maximum likelihood (MaxL) estimation is a statistical method commonly used in DL for estimating the parameters of a PDF $p_{\text{model}}(\mathbf{x};\theta)$ that best explains the observed data $p_{X}(\mathbf{x})$, assuming the parameters are fixed but unknown.
Most of generative models work with the MaxL principle \cite{Goodfellow2016}; given a probability distribution parameterized by $\theta$, the estimator for $p_X(\mathbf{x})$ is defined as
\begin{equation}
\label{eq:MaxL}
    \theta_{\text{MaxL}} = \argmax_{\theta}p_{\text{model}}(\mathbf{x};\theta), \; \mathbf{x}\sim p_{X}(\mathbf{x}).
\end{equation}
Given $N$ data points $\mathbf{x}_i\sim p_X(\mathbf{x})$, $i=1,\dots, N$, we seek to maximize
\begin{equation}
    \theta_{\text{MaxL}} = \argmax_{\theta}\prod_{i=1}^{N}{p_{\text{model}}(\mathbf{x}_i;\theta)},
\end{equation}
which can be conveniently rewritten as
\begin{equation}
\label{eq:LogL}
    \theta_{\text{MaxL}} = \argmax_{\theta}\sum_{i=1}^{N}{\log p_{\text{model}}(\mathbf{x}_i;\theta)},
\end{equation}
or alternatively
\begin{equation}
\label{eq:CE_MaxL}
    \theta_{\text{MaxL}} = \argmin_{\theta} \mathbb{E}_{\mathbf{x}\sim p_X(\mathbf{x})}\bigl[-\log p_{\text{model}}(\mathbf{x};\theta)\bigr].
\end{equation}
The last expression is equivalent to a cross-entropy minimization over the parameter $\theta$. Notice that \eqref{eq:CE_MaxL} often appears in terms of Kullback-Leibler (KL) divergence
\begin{equation}
\theta_{\text{MaxL}} = \argmin_{\theta} D_{\text{KL}}(p_{X}(\mathbf{x})||p_{\text{model}}(\mathbf{x};\theta)) = \argmin_{\theta} \mathbb{E}_{\mathbf{x}\sim p_X(\mathbf{x})}\biggl[\log \biggl(\frac{p_{X}(\mathbf{x})}{p_{\text{model}}(\mathbf{x};\theta)}\biggr)\biggr].
\label{MaxLKL}
\end{equation}
Practically, we are interested in a parameterization of $p_{\text{model}}$ which is expressive enough to fully capture data patterns and which allows iterative optimization. Artificial neural networks (NNs) represent a viable solution as explain in Sec. \ref{sec:tips-tricks}.

Similarly, MaxL can be applied to estimate the conditional probability $p_{Y|X}(\mathbf{y|x})$
\begin{equation}
\label{eq:CE_CMaxL}
    \theta_{\text{MaxL}} = \argmin_{\theta} \mathbb{E}_{(\mathbf{x},\mathbf{y})\sim p_{Y|X}(\mathbf{y|x})}\bigl[-\log p_{\text{model}}(\mathbf{y|x};\theta)\bigr].
\end{equation}
In communications, the estimation of the transition probability or likelihood $p_{Y|X}$ is extremely relevant as it is used in the maximum a-posteriori (MAP) decoding strategy. Indeed, we are typically interested in estimating the transmitted input $\mathbf{x}$ given the observation of the output $\mathbf{y}$. Formally, we need to solve
\begin{equation}
    \mathbf{\hat{x}} = \argmax_{\mathbf{x}} p_{X|Y}(\mathbf{x|y}),
\end{equation}
which using Bayes' rule and the logarithmic trick reads as
\begin{equation}
    \mathbf{\hat{x}} = \argmin_{\mathbf{x}} -\log p_{Y|X}(\mathbf{y|x}) - \log p_{X}(\mathbf{x}).
\end{equation}
A famous example is described by the scalar additive white Gaussian noise (AWGN) channel of variance $\sigma_N^2$
\begin{equation}
    p_{Y|X}(y|x)= \frac{1}{\sqrt{2\pi}\sigma_N}\exp\biggl[{-\frac{1}{2}\biggl(\frac{y-x}{\sigma_N}\biggr)^2\biggr]}
\end{equation}
and uniform discrete input source with alphabet dimension $M$, for which it is known that the minimal Euclidean distance represents the optimal decoding criterion \cite{Proakis2001}
\begin{equation}
    \hat{x} = \argmin_{x_i \in \{x_1, \dots, x_{M}\}} |y-x_i|^2.
\end{equation}
It is thus prerogative of any decoding algorithm to estimate the channel model. We leave further details to Ch.~\ref{sec:medium} and Ch.~\ref{sec:decoder}.

\subsection{Statistical distances}
Statistical distances quantify the difference or similarity between two probability distributions. 
They are widely used in various fields such as statistics, machine learning, data analysis, and information theory. Different statistical distances capture different aspects of the distributions and are suitable for different tasks. Arguably the most common statistical distance is the KL divergence.

Let $P$ and $Q$ be absolutely continuous measures w.r.t. $\diff x $ and assume they possess
densities $p$ and $q$, then the KL-divergence is defined as
\begin{equation}
D_{\text{KL}}(P||Q) = \int_{\mathcal{X}}{p(x)\log\biggl(\frac{p(x)}{q(x)}\biggr)\diff x},
\end{equation}
where $\mathcal{X}$ is a compact domain.
The KL divergence measures how much the distribution $P$ differs from a reference distribution $Q$ \cite{KL1951}. The divergence equals zero if and only if $P=Q$ as measures, it is not symmetric and it is generally not upper bounded.
A fundamental particular case of KL divergence is represented by the mutual information (MI) between two random variables $X$ and $Y$. It quantifies the statistical dependence between $X$ and $Y$ by measuring the amount of information obtained about one variable via the observation of the other. The MI is symmetric and it is defined as
\begin{equation}
\label{eq:fundamentals_MI}
    I(X;Y) = \mathbb{E}_{(\mathbf{x},\mathbf{y})\sim p_{XY}(\mathbf{x},\mathbf{y})}\biggl[\log \frac{p_{XY}(\mathbf{x},\mathbf{y})}{p_{X}(\mathbf{x})p_{Y}(\mathbf{y})}\biggr],
\end{equation}
which rewrites in terms of KL divergence as
\begin{equation}
    I(X;Y) = D_{\text{KL}}(p_{XY}(\mathbf{x},\mathbf{y})||p_{X}(\mathbf{x})p_{Y}(\mathbf{y})).
\end{equation}

The KL divergence can be extended to a more general class of divergences referred to as $f$-divergence, where $f$ is a convex lower semicontinuous function $f:\mathbb{R}_+ \to \mathbb{R}$ satisfying $f(1)=0$. The $f$-divergence between $P$ and $Q$ is defined as
\begin{equation}
\label{eq:f-divergence}
D_f(P||Q) = \int_{\mathcal{X}}{q(\mathbf{x})f\biggl(\frac{p(\mathbf{x})}{q(\mathbf{x})}\biggr)\diff \mathbf{x}}.
\end{equation}
It is immediate to notice that the KL divergence directly follows from the $f$-divergence using the generator function $f(u)=u\log u$. Other popular statistical measures based on \eqref{eq:f-divergence} are:
\begin{itemize}
    \item the Jensen-Shannon (JS) divergence, defined in terms of KL divergence as
\begin{equation}
    D_{\text{JS}}(P||Q) = \frac{1}{2}D_{\text{KL}}\biggl(P\biggl|\biggl|\frac{P+Q}{2}\biggr)+\frac{1}{2}D_{\text{KL}}\biggl(Q\biggl|\biggl|\frac{P+Q}{2}\biggr);
\end{equation}
\item the squared Hellinger distance (HD), defined as
\begin{equation}
\label{eq:HD-distance}
H^2(P,Q) :=  D_{\text{HD}}(P||Q) = \frac{1}{2} \int_{\mathcal{X}}{\biggl(\sqrt{p(\mathbf{x})}-\sqrt{q(\mathbf{x})}\biggr)^2\diff \mathbf{x}},
\end{equation}
with $0\leq H(P,Q) \leq 1$, and generator $f(u) = \frac{1}{2}(\sqrt{u}-1)^2$;
\item the total variation (TV) distance, which can be rewritten in terms of $f$-divergence as
\begin{equation}
\label{eq:TV-distance}
V(P,Q) =  D_{\text{TV}}(P||Q) = \frac{1}{2} \int_{\mathcal{X}}{|p(\mathbf{x})-q(\mathbf{x})|\diff \mathbf{x}},
\end{equation}
when $f(u)=\frac{1}{2}|u-1|$. 
\end{itemize}
It is also known that $V(P,Q)\leq \sqrt{1-\exp(-D_{\text{KL}}(P||Q))}$ and $H^2(P,Q)\leq V(P,Q) \leq \sqrt{2}H(P,Q)$.
The challenge that unites statistical distances consists in their explicit calculation. One of the objective of Ch.~\ref{sec:mi_estimators} is to shed some light on how to exploit NNs to estimate such distances.

\section{Machine learning tools}
\label{sec:tips-tricks}
In this section, we briefly introduce ML and DL tools utilized to demonstrate the main results of the thesis. In particular, we focus our attention to NNs since they can handle diverse type of data, including numerical data, text, images, and sequences. NNs are designed and trained for specific tasks by adjusting their architecture, activation functions, and other parameters, making them versatile for solving various problems.

\subsection{Neural networks}
\label{subsec:NN}
Neural networks are among the most popular tools in the ML community as they are known being universal function approximators \cite{Hornik1989}, they can be implemented in parallel on concurrent architectures and most importantly, they can be trained by backpropagation \cite{Rumelhart1985}.

A feedforward NN with $L$ layers maps a given input $\mathbf{x}_0 \in \mathbb{R}^{D_0}$ to an output $\mathbf{x}_L \in \mathbb{R}^{D_L}$ by implementing a function $F(\mathbf{x}_0;\mathbf{\theta})$ where $\mathbf{\theta}$ represents the parameters of the NN. To do so, the input is processed through $L$ iterative steps
\begin{equation}
\mathbf{x}_l = f_l(\mathbf{x}_{l-1};\theta_l), \; l=1,\dots, L
\end{equation}
where $f_l(\mathbf{x}_{l-1};\theta_l)$ maps the input of the $l$-th layer to its output. The most used layer is the fully-connected one, whose mapping is expressed as
\begin{equation}
f_l(\mathbf{x}_{l-1};\theta_l) = \sigma(\mathbf{W}_l\cdot \mathbf{x}_{l-1}+\mathbf{b}_l)
\end{equation}
where $\sigma(\cdot)$ is the activation function while $\mathbf{W}_l$ and $\mathbf{b}_l$ are the parameters, weights and the biases, respectively.
According to the specific application, several different types of layers and activation functions can be defined. Fig. \ref{img:fundamentals_NN} shows a general fully-connected architecture.

\begin{figure}
%strip if you want 2 columns
\centering
\includegraphics[scale = 0.33]{images/fundamentals/NN.pdf}
\caption{Architecture of a fully connected neural network with two hidden layers.}
\label{img:fundamentals_NN}
\end{figure}

Defined a metric $\delta$ and a cost function $C$, the easiest and most classical algorithm to find the feasible set of parameters $\mathbf{\theta}$ is the gradient descent method which iteratively updates $\mathbf{\theta}$ as $\mathbf{\theta}_t = \mathbf{\theta}_{t-1}-\eta\nabla C(\mathbf{\theta}_{t-1})$ where $\eta$ is the learning rate. Its popular variants are stochastic gradient descent (SGD) and adaptive learning rates (Adam) \cite{AdamKingma}.
Common choices for the cost function are the mean squared error and categorical cross-entropy, for which $\delta$ in \eqref{eq:general_cost} takes the form $\delta(\mathbf{y},\mathbf{\hat{y}}) = ||\mathbf{y}-\mathbf{\hat{y}}||^2$ and $\delta(\mathbf{y},\mathbf{\hat{y}}) = -\mathbf{y}\cdot \log{\mathbf{\hat{y}}}$, respectively.

\subsection{Convolutional neural networks}
Convolutional neural networks (CNNs) (Fig. \ref{img:fundamentals_CNN}) are able to capture the spatial and temporal dependencies of data, and for this reason they find application in image and document recognition \cite{LeCun1998}, medical image analysis \cite{Li2014}, natural language processing \cite{Collobert2008}, and more in general pattern recognition. CNNs are multi layer perceptrons (MLP)  with a regularization approach since they consist of multiple convolutional layers to ensure the translation invariance characteristics. In particular, given an input data matrix $\mathbf{I}_i$, the feature map $\mathbf{F}_j$ is obtained as
\begin{equation}
\mathbf{F}_j = \sigma\biggl(\sum_{i=1}^{C}{\mathbf{I}_i \ast \mathbf{K}_{i,j}+\mathbf{B}_{j}}\biggr)
\end{equation}
namely, through the superposition of $C$ layers, e.g., $C=3$ for RGB images, each comprising a convolution between the input matrix $\mathbf{I}_{i}$ and a kernel matrix $\mathbf{K}_{i,j}$, plus an additive bias term $\mathbf{B}_j$, and a final application of non-linear activation function $\sigma(\cdot)$, typically, a \textit{sigmoid}, \textit{tanh}, or \textit{ReLU}. Each set of kernel matrices represents a filter that extracts local features. To control the problem of overfitting, the dimension of data and features to be extracted is reduced by pooling layers. Finally, fully-connected layers are used to extract semantic information from features.

\begin{figure}
\centering
\includegraphics[scale = 0.40]{images/fundamentals/CNN.pdf}
\caption{Structure of a convolutional neural network with one convolutional layer.}
\label{img:fundamentals_CNN}
\end{figure}


\subsection{Autoencoders}
An autoencoder (AE) is a particular type of NN consisting of an encoding block which tries to learn a latent representation $\mathbf{z}$, typically in a lower-dimensional space, of the input variable $\mathbf{x}$ and a decoding block which reconstructs $\mathbf{x}$ at the output using the information inside the code $\mathbf{z}$ (Fig. \ref{img:fundamentals_autoencoder-architecture}).

A classical learning formulation for \textit{deterministic} AEs requires to solve the following optimization problem
\begin{equation}
\label{eq:AutoCost}
\theta_{\text{opt}} = \argmin_{\theta} \delta(\mathbf{x},G(F(\mathbf{x};\theta_1);\theta_2)),
\end{equation}
where $\theta = (\theta_1,\theta_2)$ are the parameters of the NN, $\delta$ is a measure of distance, while $F$ and $G$ stand for the encoder and decoder function, respectively.
When $F$ and $G$ are linear functions, \eqref{eq:AutoCost} collapse to the principal component analysis (PCA) algorithm. Given a $N \times D$ matrix $\mathbf{x}$ where each row is a sample $\mathbf{x}_i \in \mathbb{R}^D$, PCA represents the encoder as $F(\mathbf{x};\theta) = \mathbf{W}^T\mathbf{x}$ and the decoder as $G(\mathbf{z};\theta) = \mathbf{W}\mathbf{z}$ where $\mathbf{W}$ is the unknown parameter, a $D\times M$ matrix where $M$ is the dimension of the latent space. If $\delta$ is the classic quadratic loss function, then \eqref{eq:AutoCost} can be rewritten as
\begin{equation}
\label{eq:PCA}
\mathbf{W}_{\text{opt}} = \argmin_{\mathbf{W}} \sum_{i=1}^{N}{||\mathbf{x}_i-\mathbf{WW}^T \mathbf{x}_i||_2^2}.
\end{equation}
Let $\Sigma$ be the sample covariance matrix of $\mathbf{x}$, then $\mathbf{W}$ is given by the $M$ principal eigenvectors of $\Sigma$. The resulting transformation $\mathbf{z}=\mathbf{W}^T\mathbf{x}$ is a matrix whose row elements are mutually uncorrelated. PCA is broadly used as a dimensionality reduction and feature extractor tool.

\begin{figure}[t]
\centering
	\includegraphics[scale=0.3]{images/fundamentals/autoencoder_NN.pdf}
	\caption{Architecture of an AE.}
	\label{img:fundamentals_autoencoder-architecture}
\end{figure}

Correlation is an indicator of linear dependence, but in most of cases we are interested in representations where features have a different form of dependence and the ability to identify and extract non-linear dependencies is the main reason for adopting AEs. 
An interesting type of AE for communications purposes is the denoising autoencoder (DAE) \cite{Vincent2008}. The idea is to train a network in order to minimize the following denoising criterion:
\begin{equation}
\label{DAE}
\mathcal{L}_{\text{DAE}} = \mathbb{E}_{\mathbf{x}\sim \mathcal{D}}[\delta(\mathbf{x},G(F(\mathbf{\tilde{x}})))],
\end{equation}
where $\mathbf{\tilde{x}}$ is a stochastic corruption of $\mathbf{x}$. When we train a DAE using the quadratic loss and a corruption Gaussian noise $\mathbf{\tilde{x}} = \mathbf{x}+\epsilon$ with $\epsilon \sim \mathcal{N}(0,\sigma I)$, the work in \cite{Alain2014} proved that the AE recovers properties of the training density $p_X(\mathbf{x})$.
This is somehow remarkable in a communication framework because it asserts that corrupting the transmitted signal with some form of noise can be beneficial in the reconstruction's phase. 

They also showed that the DAE with a small corruption of variance $\sigma^2$ is similar to a contractive autoencoder (CAE) with penalty coefficient $\lambda=\sigma^2$. The CAE \cite{Rifai2011} is a particular form of regularized AE which is trained in order to minimize the following reconstruction criterion:
\begin{equation}
\label{eq:CAE}
\mathcal{L}_{\text{CAE}} = \mathbb{E}_{\mathbf{x}\sim \mathcal{D}}\biggl[\delta(\mathbf{x},G(F(\mathbf{x})))+\lambda \biggl| \frac{\partial F(\mathbf{x})}{\partial \mathbf{x}} \biggr|_F^2 \biggr],
\end{equation}
where $|\mathbf{A}|_F$ is the Frobenius norm. The idea behind CAE is that the regularization term attempts to make $F(\cdot)$ or $G(F(\cdot))$ as simple as possible, but at the same time the reconstruction error must be small.

Despite the fact that typically AEs find a low-dimensional representation of the input vector $\mathbf{x}$ at some intermediate level, when redundancy is desired or a design property, sparse AEs learn an hidden representation of the data in a way such data $\mathbf{z}$ is a $k$-sparse vector. Sparsity is mathematically described in terms of pseudo-norm $l_0$ which, due to its non-differentiability and non-convexity, results intractable. For this reason sparsity is often relaxed and described in terms of norm $l_1$. In this way, sparse AEs are trained in order to minimize the following reconstruction criterion:
\begin{equation}
\label{eq:SAE}
\mathcal{L}_{\text{SAE}} = \mathbb{E}_{\mathbf{x}\sim \mathcal{D}}[\delta(\mathbf{x},G(F(\mathbf{x})))+\lambda | F(\mathbf{x})|_1].
\end{equation}
AEs can be used inside a \textit{probabilistic} framework that aims at modeling the underlying distributions. In this context, variational autoencoders (VAEs) have been introduced in \cite{Kingma2013} as generative probabilistic models based on variational inference.

\subsection{Generative models}
\sectionmark{Generative models}
\label{sec:generative_networks}
Variational autoencoders \cite{Kingma2013} are a particular class of generative models based on variational inference. Let $\mathbf{z}$ be the latent variable of the observed value $\mathbf{x}$ for a parameter $\theta$, then $p_{\theta}(\mathbf{z|x})$ represents the intractable true posterior which can be approximated by a tractable one, $q_{\phi}(\mathbf{z|x})$, for a parameter $\phi$. A probabilistic encoder produces $q_{\phi}(\mathbf{z|x})$ while a probabilistic decoder produces $p_{\theta}(\mathbf{x|z})$. The idea is to maximize a variational lower bound $\mathcal{L}$, often referred to as evidence lower bound (ELBO), on the marginal log-likelihood (evidence)
\begin{equation}
\log p_{\theta}(\mathbf{x}_i) = D_{\text{KL}}(q_{\phi}(\mathbf{z|x}_i)||p_{\theta}(\mathbf{z|x}_i))+\mathcal{L}(\theta,\phi;\mathbf{x}_i)
\label{eq:VALower1}
\end{equation}
where
\begin{equation}
\mathcal{L}(\theta,\phi;\mathbf{x}_i) =  -D_{\text{KL}}(q_{\phi}(\mathbf{z|x}_i)||p_{\theta}(\mathbf{z}))  +\mathbb{E}_{q_{\phi}(\mathbf{z|x}_i)}\biggl[\log p_{\theta}(\mathbf{x}_i|\mathbf{z})\biggr].
\label{eq:VALower2}
\end{equation}
Rather than outputting the code $\mathbf{z}$, the encoder outputs parameters describing a distribution for each dimension of the latent space. In the case where the prior is assumed to be Gaussian, $\mathbf{z}$ will consist of mean and variance. Tuning in the latent space and processing the new latent samples through the decoder is a way to generate new data. 
An hierarchical variational autoencoder (HVAE) is an extended version of a VAE, encompassing multiple hierarchies of latent variables. Therein, latent variables are considered to be generated from higher-level, more abstract latent variables themselves, forming a hierarchical structure. In a Markovian HVAE (MHVAE) with $T$ hierarchical latents, the generative process follows a Markov chain, thus it can be interpreted as stacking VAEs on top of each other (see Fig. \ref{img:fundamentals_MHVAE}). The joint and posterior distributions rewrite as
\begin{align}
    \label{eq:MHVAE}
    p_{\theta}(\mathbf{x},\mathbf{z}_{1:T}) & = p_{Z_T}(\mathbf{z}_T)p_{\theta}(\mathbf{x|z}_{1})\prod_{t=2}^{T}{p_{\theta}(\mathbf{z}_{t-1}|\mathbf{z}_{t})} \\ 
    q_{\phi}(\mathbf{z}_{1:T}|\mathbf{x}) & = q_{\phi}(\mathbf{z}_1|\mathbf{x})\prod_{t=2}^{T}{q_{\phi}(\mathbf{z}_{t}|\mathbf{z}_{t-1})}.
\end{align}

\begin{figure}
\centering
	\includegraphics[scale=0.4]{images/fundamentals/MHVAE.pdf}
	\caption{Graph of a Markovian hierarchical variational autoencoder with $T$ hierarchical latents. Each latent $\mathbf{z}_t$ is generated only from the previous latent $\mathbf{z}_{t+1}$.}
	\label{img:fundamentals_MHVAE}
\end{figure}

Probabilistic diffusion models \cite{Ho2020} have been gaining increasing attention and importance in the field of generative modeling. They can be thought as a particular case of a MHVAE when:
\begin{itemize}
    \item the dimension of the latent $\mathbf{z}_t$, $\forall t=1,\dots,T$, equals the dimension of the data $\mathbf{x}$;
    \item the encoder transition is a simple Gaussian model $q(\mathbf{z}_{t}|\mathbf{z}_{t-1}) = \mathcal{N}(\sqrt{\alpha_t}\mathbf{z}_{t-1}, (1-\alpha_t)\mathbf{I})$, where $\alpha_t$ is a learnable coefficient; 
    \item the distribution of the latent $p_{Z_T}(\mathbf{z}_{T})$ at the timestep $T$ is a multivariate normal distribution. 
\end{itemize}
Under such restrictions, it is possible to show that the task of diffusion models consists of learning how to predict the original ground truth sample from an arbitrarily noisy version of it \cite{CalvinLuo2022}.

Flow-based generative models map the data via a non-linear deterministic transformation into a latent space of independent variables where the probability density results tractable \cite{Dinh2014}. The framework lies behind the change of variable rule
\begin{equation}
p_{X}(\mathbf{x}) = p_{Z}(g^{-1}(\mathbf{x}))\cdot \biggl| \det \frac{\partial g^{-1}(\mathbf{x})}{\partial \mathbf{x}} \biggr|
\label{NICE}
\end{equation}
when both the determinant of the Jacobian and $g^{-1}$ are easy to compute; in that case it is straightforward to directly sample from $p_{X}(\mathbf{x})$ since $\mathbf{x} = g(\mathbf{z})$.
NICE, Real NVP and GLOW \cite{Kingma2018} belong to flow-based generative models which are able to provide a good latent-variable inference and excellent log-likelihood evaluation.

The Gaussian Process Latent Variable Model (GP-LVM) \cite{GPLVM2005} aims at inferring both the latent code $\mathbf{z}$ and the mapping function $\mathbf{f}$ that lead to the dataset $\mathbf{x}$. The prior distribution over $\mathbf{z}$ is set as Gaussian, while $\mathbf{f}(\cdot)$ is described as a Gaussian Process (GP) $\mathbf{f} \sim \mathcal{GP}(\mathbf{0},\mathbf{K})$ where $K(\cdot,\cdot)$ is the covariance function, commonly referred to as squared exponential kernel.
This approach ensures a smooth mapping from the latent to the sample space while providing a closed form expression to approximate the true posterior distribution $p_{Z|X}(\mathbf{z}|\mathbf{x})$ and to find a variational lower bound for a robust training procedure \cite{GPLVM2010}.

The methods presented so far have been mostly and successfully applied to images and they all share the ability to generate new samples in parallel. 
The synthesis of fully visible belief networks (FVBNs) \cite{Frey1995,Frey1998} and autoregressive models \cite{Gregor2014} is difficult to parallelize, therefore, due to their sequential nature, they are relatively slow. Indeed, their core idea is to factorize the joint probability distribution of $D$ dimensional inputs $\mathbf{x}$ into products of one-dimensional conditional distributions:
\begin{equation}
 p_{\text{model}}(\mathbf{x}) = \prod_{j=1}^{D}{p_{\text{model}}(x_{j}|x_{1},\dots, x_{j-1}}).
\label{eq:ChainRule}
\end{equation}
Generation is done by generating one dimension at a time leading to good quality of the samples (like WaveNet for human speech \cite{Oord2016}) since they directly optimize the likelihood. 
However, the recent success of the \textit{transformer} architecture \cite{Vaswani2017} shows that parallelization is possible, and together with a self-attention mechanism to effectively model long-range dependencies, transformers have demonstrated remarkable performance in generating human-like text. The pre-training and fine-tuning paradigm used in transformer-based generative models, often referred to as generative pre-trained transformer (GPT), enables them to be highly adaptable to a variety of tasks. 

Boltzmann machines \cite{Hinton06afast} and generative stochastic network \cite{AlainB2015} rely on estimating the transition operator of a Markov Chain $p(x_{j}|x_{j-1})$ but are now less used since Markov chains fail to scale in high dimensional spaces.

One of the most successful data generation techniques is represented by generative adversarial networks (GANs) \cite{Goodfellow2014}.
Considering the importance of GANs to this thesis, we have dedicated a separate section to delve into their functioning and principles.

\section{Generative adversarial networks}
\label{sec:gans}
The main idea behind GANs is to train a pair of networks in competition with each other: a generator network $G$ that captures the observed data distribution $p_{X}(\mathbf{x})$ and a discriminator network $D$ that distinguishes if a sample is an original coming from real data rather than a fake coming from data generated by the generator $G$ (see Fig. \ref{img:fundamentals_GANs}). 
The training procedure for $G$ is to maximize the probability of $D$ making a mistake. GANs can be thought as a minimax two-player game which will end when a Nash equilibrium point is reached. 

In detail, the generator takes as input a noise vector $\mathbf{z}$ with distribution $p_{\text{noise}}(\mathbf{z})$ and maps it to the data space via a non-linear transformation $\hat{\mathbf{x}} = G(\mathbf{z})$. 
The vanilla value function $V(G,D)$ used to train GANs reads as follows
\begin{equation}
V(G,D) = \mathbb{E}_{\mathbf{x} \sim p_{X}(\mathbf{x})}[\log D(\mathbf{x})] + \mathbb{E}_{\mathbf{z} \sim p_{\text{noise}}(\mathbf{z})}[\log(1-D(G(\mathbf{z})))],
\label{eq:fundamentals_GANs}
\end{equation}
where the objective for the generator is to minimize $V$ while the discriminator maximizes $V$.
It can be proved that such optimization strategy, i.e.,
\begin{equation}
\label{eq:min_max_GAN}
    G^* = \argmin_G \max_D V(G,D),
\end{equation}
enables the generator to implicitly learn the true data distribution $p_{X}(\mathbf{x})$. Indeed, if $p_{\hat{X}}(\mathbf{x})$ is the distribution of the output $\hat{\mathbf{x}}$, it is true that $p_{\hat{X}} = p_{X}$ at the equilibrium.

\begin{figure}
\centering
\includegraphics[scale = 0.4]{images/fundamentals/GAN.pdf}
\caption{GAN framework in which generator and discriminator are learned during the training process.}
\label{img:fundamentals_GANs}
\end{figure}

Several architectures such as the deep convolutional generative adversarial network (DCGAN) \cite{Radford2016} and the StyleGAN \cite{Karras2019} have been proposed in the literature to stabilize the learning process as it is well-known that GANs suffer from problems such as mode collapse and training instability. The latter architecture is amongst the most recent evolution of GANs as it produces state-of-the-art results in synthesizing high-resolution images. The authors achieve improved performance by modifying the generator's architecture (a mapping network for the latent representation and a synthesis one with different resolution levels) for a better understanding of the output. 
However, it is also true that minor efforts have been carried out in generating structured non-images data with GANs.  In Sec.~\ref{sec:medium}, we attempted to study the GAN-based synthesis of time-domain signals.

An alternative method for enhancing GANs involves gaining a deeper comprehension of their training process. Indeed, GANs represent a unique paradigm in ML, where the traditional notion of a cost function is replaced by a value function and a game-theoretic framework. This distinctive characteristic sets GANs apart from many other optimization problems in the field. An extremely useful interpretation of GANs is offered in \cite{Nowozin2016}, where it was shown that the discriminator in GANs acts as an auxiliary NN responsible for the estimation of the variational $f$-divergence, essentially the $\max$ operator in \eqref{eq:min_max_GAN}. The generator, instead, is a neural sampler trained to minimize such divergence, thus, any $f$-divergence can be used for training generative neural samplers, allowing for flexible divergence choices to better match the desired distribution. 

The upcoming chapters will revisit and build upon the latest concept along with the explanations provided thus far.