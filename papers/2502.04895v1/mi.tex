\chapter{Variational Approaches for Mutual Information Estimation} % Neural decoder
\chaptermark{Mutual information neural estimation}
%\thispagestyle{empty}
\label{sec:mi_estimators}

Estimating MI accurately is pivotal across diverse applications, from ML to communications and biology, enabling us to gain insights into the inner mechanisms of complex systems. Yet, dealing with high-dimensional data presents a formidable challenge, due to its size and the presence of intricate relationships. 

Recently proposed neural methods employing VLBs on the MI have gained prominence. However, these approaches suffer from either high bias or high variance, as the sample size and the structure of the loss function directly influence the training process. In this chapter, we present a novel class of discriminative mutual information estimators (DIME) based on the variational representation of the $f$-divergence. We investigate the impact of the permutation function used to obtain the marginal training samples and present a novel architectural solution based on derangements. The proposed estimator is flexible since it exhibits an excellent bias/variance trade-off. The comparison with state-of-the-art neural estimators, through extensive experimentation within established reference scenarios, shows that our approach offers higher accuracy and lower complexity.

The results presented in this chapter are documented in \cite{LetiziaNIPS, f-DIME, letizia2022Balkan}.

\section{Introduction}
\sectionmark{Introduction}
\label{sec:mi_related}
The MI between two multivariate random variables, $X$ and $Y$, is a fundamental quantity in statistics, representation learning, information theory, communication engineering and biology \cite{goldfeld2021sliced, tschannen2019mutual, guo2005mutual, pluim2003mutual}. 

Unfortunately, computing $I(X;Y)$ is challenging since the joint PDF $p_{XY}(\mathbf{x},\mathbf{y})$ and the marginals $p_X(\mathbf{x}),p_Y(\mathbf{y})$ are usually unknown, especially when dealing with high-dimensional data.

Traditional approaches for the MI estimation rely on binning, density and kernel estimation \cite{Moon1995} and $k$-nearest neighbors \cite{Kraskov2004}. Nevertheless, they do not scale to problems involving high-dimensional data as it is the case in modern ML applications. Hence, deep NNs have recently been leveraged to maximize VLBs on the MI \cite{Poole2019,Nguyen2010, Mine2018}. The expressive power of NNs has shown promising results in this direction although less is known about the effectiveness of such estimators \cite{Song2020}, especially since they suffer from either high bias or high variance. 

The variational approach usually exploits an energy-based variational family of functions to provide a lower bound on the KL divergence. As an example, the Donsker-Varadhan dual representation of the KL divergence \cite{Poole2019,Donsker1983} produces an estimate of the MI using the bound optimized by the mutual neural information estimator (MINE) \cite{Mine2018}
\begin{equation}
\label{eq:MI_MINE}
I_{MINE}(X;Y) = \sup_{\theta \in \Theta} \mathbb{E}_{(\mathbf{x},\mathbf{y})\sim p_{XY}(\mathbf{x},\mathbf{y})}[T_{\theta}(\mathbf{x},\mathbf{y})]  -\log(\mathbb{E}_{(\mathbf{x},\mathbf{y})\sim p_X(\mathbf{x}) p_Y(\mathbf{y})}[e^{T_{\theta}(\mathbf{x},\mathbf{y})}]),
\end{equation}
where $\theta \in \Theta$ parameterizes a family of functions $T_{\theta} : \mathcal{X}\times \mathcal{Y} \to \mathbb{R}$ through the use of a deep NN. However, the logarithm before the expectation in the second term renders MINE a biased estimator. To avoid biased gradients, the authors in \cite{Mine2018} suggested to replace the partition function $\mathbb{E}_{p_X p_Y}[e^{T_{\theta}}]$ with an exponential moving average over mini-data-batches.


Another VLB is based on the KL divergence dual representation introduced in \cite{Nguyen2010} (also referred to as $f$-MINE in \cite{Mine2018})
\begin{equation}
\label{eq:MI_NWJ}
I_{NWJ}(X;Y) = \sup_{\theta \in \Theta} \mathbb{E}_{(\mathbf{x},\mathbf{y})\sim p_{XY}(\mathbf{x},\mathbf{y})}[T_{\theta}(\mathbf{x},\mathbf{y})]  -\mathbb{E}_{(\mathbf{x},\mathbf{y})\sim p_X(\mathbf{x}) p_Y(\mathbf{y})}[e^{T_{\theta}(\mathbf{x},\mathbf{y})-1}].
\end{equation}
Although MINE provides a tighter bound $I_{MINE}\geq I_{NWJ}$, the NWJ estimator is unbiased. 

Both MINE and NWJ suffer from high-variance estimations and to combat such a limitation, the SMILE estimator was introduced in \cite{Song2020}. It is defined as
\begin{equation}
\small
\label{eq:MI_SMILE}
I_{SMILE}(X;Y) = \sup_{\theta \in \Theta} \mathbb{E}_{(\mathbf{x},\mathbf{y})\sim p_{XY}(\mathbf{x},\mathbf{y})}[T_{\theta}(\mathbf{x},\mathbf{y})] 
 -\log(\mathbb{E}_{(\mathbf{x},\mathbf{y})\sim p_X(\mathbf{x}) p_Y(\mathbf{y})}[\text{clip}(e^{T_{\theta}(\mathbf{x},\mathbf{y})},e^{-\tau},e^{\tau})]),
\end{equation}
where $\text{clip}(v,l,u) = \max(\min(v,u),l)$ and it helps to obtain smoother partition functions estimates. SMILE is equivalent to MINE in the limit $\tau \to +\infty$. 

The MI estimator based on contrastive predictive coding (CPC) \cite{NCE2018} is defined as 
\begin{equation}
\label{eq:MI_NCE}
I_{CPC}(X;Y) = \mathbb{E}_{(\mathbf{x},\mathbf{y})\sim p_{XY,N}(\mathbf{x},\mathbf{y})}\biggl[ \frac{1}{N} \sum_{i=1}^{N}{ \log\biggl( \frac{e^{T_{\theta}(\mathbf{x}_i,\mathbf{y}_i)}}{\frac{1}{N} \sum_{j=1}^{N}{e^{T_{\theta}(\mathbf{x}_i,\mathbf{y}_j)}}}\biggr)}  \biggr],
\end{equation}
where $N$ is the batch size and $p_{XY,N}$ denotes the joint distribution of $N$ i.i.d. random variables sampled from $p_{XY}$. CPC provides low variance estimates but it is upper bounded by $\log N$, resulting in a biased estimator. Such upper bound, typical of contrastive learning objectives, has been recently analyzed in the context of skew-divergence estimators \cite{RenyiCL}.

Another estimator based on a classification task is the neural joint entropy estimator (NJEE) proposed in \cite{shalev2022neural}.
Let $X_m$ be the $m$-th component of $X$, with $m\leq d$ and $N$ the batch size. $X^k$ is the vector containing the first $k$ components of $X$. Let $\hat{H}_N(X_1)$ be the estimated marginal entropy of the first components in $X$ and let $G_{\theta_m}(X_m|X^{m-1})$ be a neural network classifier, where the input is $X^{m-1}$ and the label used is $X_m$. If $\text{CE}(\cdot)$ is the cross-entropy function, then the MI estimator based on NJEE is defined as
\begin{equation}
    I_{NJEE}(X;Y) = \hat{H}_N(X_1) + \sum_{m=2}^{d} \text{CE}(G_{\theta_m}(X_m|X^{m-1})) - \sum_{m=1}^{d} \text{CE}(G_{\theta_m}(X_m|Y, X^{m-1})),
\end{equation}
where the first two terms of the RHS constitutes the NJEE estimator.

\section{Variational $f$-divergence and data derangements}
\sectionmark{f-DIME}
\label{sec:mi_f-DIME}

Inspired by the $f$-GAN training objective \cite{Nowozin2016}, in the following, we present a class of discriminative MI estimators based on the $f$-divergence measure. Conversely to what has been proposed so far in the literature, where $f$ is always constrained to be the generator of the KL divergence, we allow for any choice of $f$. Different $f$ functions will have different impact on the training and optimization sides, while on the estimation side, the partition function does not need to be computed, leading to low variance estimators.

\subsection{Contribution}
\label{subsec:mi_contribution}
Discriminative approaches \cite{raina2003classification, tonello2022mind} compare samples from both the joint and marginal distributions to directly compute the density ratio (or the log-density ratio) 
\begin{equation}
R(\mathbf{x},\mathbf{y}) = \frac{p_{XY}(\mathbf{x},\mathbf{y})}{p_X(\mathbf{x}) p_Y(\mathbf{y})}
\label{eq:MI_density_ratio_1}.
\end{equation}
Generative approaches \cite{Barber2003, song2019generative}, instead, aim at modeling the individual densities. 

We focus on discriminative MI estimation since it can in principle enjoy some of the properties of implicit generative models, which are able of directly generating data that belongs to the same distribution of the input data without any explicit density estimate. In fact, we recall that the GAN adversarial training pushes the discriminator $D(\mathbf{x})$ towards the optimum value
\begin{equation}
\label{eq:MI_gan_density_ratio}
\hat{D}(\mathbf{x}) = \frac{p_{data}(\mathbf{x})}{p_{data}(\mathbf{x})+p_{gen}(\mathbf{x})} = \frac{1}{1+\frac{p_{gen}(\mathbf{x})}{p_{data}(\mathbf{x})}}.
\end{equation}
Therefore, the output of the optimum discriminator is itself a function of the density ratio $p_{gen}/p_{data}$, where $p_{gen}$ and $p_{data}$ are the distributions of the generated and the collected data, respectively.
We generalize the observation of \eqref{eq:MI_gan_density_ratio} and we propose a family of MI estimators based on the VLB of the $f$-divergence \cite{Poole2019, sason2016f}. In particular, we argue that the maximization of any $f$-divergence VLB can lead to a MI estimator with excellent bias/variance trade-off.

Since we typically have access only to joint data points $(\mathbf{x},\mathbf{y}) \sim p_{XY}(\mathbf{x},\mathbf{y})$, another relevant practical aspect is the sampling strategy to obtain data from the product of marginals $p_{X}(\mathbf{x})p_Y(\mathbf{y})$, for instance via a shuffling mechanism along $N$ realizations of $Y$. We analyze the impact that the permutation has on the learning and training process and we propose a derangement training strategy that achieves high performance requiring $\Omega(N)$ operations.
Simulation results demonstrate that the proposed approach exhibits improved estimations in a multitude of scenarios.

In brief, we can summarize our contributions over the state-of-the-art as follows:
\begin{itemize}
    \item For any $f$-divergence, we derive a training value function whose maximization leads to a given MI estimator.
    \item We compare different $f$-divergences and comment on the resulting estimator properties and performance.
    \item We study the impact of data derangement for the learning model and propose a novel derangement training strategy that overcomes the upper bound on the MI estimation \cite{McAllester2019}, contrarily to what happens when using a random permutation strategy.
    \item We unify the main discriminative estimators into a publicly available code which can be used to reproduce all the results of this chapter.
\end{itemize}

\subsection{$f$-divergence mutual information estimation}
\label{subsec:mi_f-DIME}
The calculation of the MI via a discriminative approach requires the density ratio \eqref{eq:MI_density_ratio_1}.
From \eqref{eq:MI_gan_density_ratio}, we observe that $I(X;Y)$ can be estimated using the optimum GAN discriminator $\hat{D}$ when $p_{data}\equiv  p_{X}p_Y$ and $p_{gen} \equiv p_{XY}$.

More in general, the authors in \cite{Nowozin2016} extended the variational divergence estimation framework presented in \cite{Nguyen2010} and showed that any $f$-divergence can be used to train GANs. Inspired by such idea, we now argue that also discriminative MI estimators enjoy similar properties if the variational representation of $f$-divergence functionals $D_f(P||Q)$ is adopted.
The following theorem introduces $f$-DIME, a class of discriminative mutual information estimators (DIME) based on the variational representation of the $f$-divergence.
 
\begin{theorem}
\label{theorem:MI_theorem1}
Let $(X,Y) \sim p_{XY}(\mathbf{x},\mathbf{y})$ be a pair of multivariate random variables. Let $\sigma(\cdot)$ be a permutation function such that  $p_{\sigma(Y)}(\sigma(\mathbf{y})|\mathbf{x}) = p_{Y}(\mathbf{y})$. Let $f^*$ be the Fenchel conjugate of $f:\mathbb{R}_+ \to \mathbb{R}$, a convex lower semicontinuous function that satisfies $f(1)=0$ with derivative $f^{\prime}$.
If $\mathcal{J}_{f}(T)$ is a value function defined as 
\begin{equation}
\mathcal{J}_{f}(T) =  \mathbb{E}_{(\mathbf{x},\mathbf{y}) \sim p_{XY}(\mathbf{x},\mathbf{y})}\biggl[T\bigl(\mathbf{x},\mathbf{y}\bigr)-f^*\biggl(T\bigl(\mathbf{x},\sigma(\mathbf{y})\bigr)\biggr)\biggr],
\label{eq:MI_discriminator_function_f}
\end{equation}
then
\begin{equation}
\label{eq:MI_optimal_ratio_T}
\hat{T}(\mathbf{x},\mathbf{y}) =\arg \max_T \mathcal{J}_f(T) = f^{\prime} \biggl(\frac{p_{XY}(\mathbf{x},\mathbf{y})}{p_X(\mathbf{x})p_Y(\mathbf{y})}\biggr),
\end{equation}
and
\begin{equation}
\label{eq:MI_f-DIME}
I(X;Y) = I_{fDIME}(X;Y) =  \mathbb{E}_{(\mathbf{x},\mathbf{y}) \sim p_{XY}(\mathbf{x},\mathbf{y})}\biggl[ \log \biggl(\bigl(f^{*}\bigr)^{\prime}\bigl(\hat{T}(\mathbf{x},\mathbf{y})\bigr) \biggr) \biggr].
\end{equation}
\end{theorem}

Theorem \ref{theorem:MI_theorem1} shows that any value function $\mathcal{J}_f$ of the form in \eqref{eq:MI_discriminator_function_f}, seen as the dual representation of a given $f$-divergence $D_f$, can be maximized to estimate the MI via \eqref{eq:MI_f-DIME}. It is interesting to notice that the proposed class of estimators does not need any evaluation of the partition term.
 
We propose to parametrize $T(\mathbf{x},\mathbf{y})$ with a deep NN $T_{\theta}$ of parameters $\theta$ and solve with gradient ascent and back-propagation to obtain
\begin{equation}
\hat{\theta} = \arg \max_{\theta} \mathcal{J}_f(T_{\theta}).
\end{equation}
By doing so, it is possible to guarantee that, at every training iteration $n$, the convergence of the $f$-DIME estimator $\hat{I}_{n,fDIME}(X;Y)$ is controlled by the convergence of $T$ towards the tight bound $\hat{T}$ while maximizing $\mathcal{J}_f(T)$, as stated in the following lemma.
\begin{lemma}
\label{lemma:MI_convergence}
Let the discriminator $T(\cdot)$ be with enough capacity, i.e., in the non parametric limit. Consider the problem
\begin{equation}
\hat{T} =  \; \arg \max_T \mathcal{J}_{f}(T)
\label{eq:MI_Lemma4_problem}
\end{equation}
where $\mathcal{J}_{f}(T)$ is defined as in \eqref{eq:MI_discriminator_function_f},
and the update rule based on the gradient descent method
\begin{equation}
T^{(n+1)} = T^{(n)} + \mu \nabla \mathcal{J}_{f}(T^{(n)}).
\end{equation}
If the gradient descent method converges to the global optimum $\hat{T}$, the mutual information estimator defined in \eqref{eq:MI_f-DIME}
converges to the real value of the mutual information $I(X;Y)$.
\end{lemma}
The proof of Lemma \ref{lemma:MI_convergence}, which is described in Sec. \ref{sec:mi_appendix}, provides some theoretical grounding for the behaviour of MI estimators when the training does not converge to the optimal density ratio. Moreover, it also offers insights about the impact of different functions $f$ on the numerical bias.

It is important to remark the difference between the classical VLBs estimators that follow a discriminative approach and the DIME-like estimators. They both achieve the goal through a discriminator network that outputs a function of the density ratio. However, the former models exploit the variational representation of the MI (or the KL) and, at the equilibrium, use the discriminator output directly in one of the value functions reported in Sec. \ref{sec:mi_related}. The latter, instead, use the variational representation of \textit{any} $f$-divergence to extract the density ratio estimate directly from the discriminator output.

In the upcoming subsections, we analyze the variance of $f$-DIME and we propose a training strategy for the implementation of Theorem \ref{theorem:MI_theorem1}.
In our experiments, we consider the cases when $f$ is the generator of: a) the KL divergence; b) the GAN divergence; c) the Hellinger distance squared. We report in Sec.~\ref{subsec:mi_appendix_DIME_examples} the value functions used for training and the mathematical expressions of the resulting DIME estimators.

\subsection{Variance analysis}
\label{subsec:mi_variance}
In this subsection, we assume that the ground truth density ratio $\hat{R}(\mathbf{x},\mathbf{y})$ exists and corresponds to the density ratio in \eqref{eq:MI_density_ratio_1}. We also assume that the optimum discriminator $\hat{T}(\mathbf{x},\mathbf{y})$ is known and already obtained (e.g. via a NN parametrization). 

We define $p^M_{XY}(\mathbf{x},\mathbf{y})$ and $p^N_{X}(\mathbf{x})p^N_Y(\mathbf{y})$ as the empirical distributions corresponding to $M$ i.i.d. samples from the true joint distribution $p_{XY}$ and to $N$ i.i.d. samples from the product of marginals $p_Xp_Y$, respectively. The randomness of the sampling procedure and the batch sizes $M,N$ influence the variance of variational MI estimators. In the following, we prove that under the previous assumptions, $f$-DIME exhibits better performance in terms of variance w.r.t. some variational estimators with a discriminative approach, e.g., MINE and NWJ.

The partition function estimation $\mathbb{E}_{p^N_X p^N_Y}[\hat{R}]$ represents the major issue when dealing with variational MI estimators. Indeed, they comprise the evaluation of two terms (using the given density ratio), and the partition function is the one responsible for the variance growth. The authors in \cite{Song2020} characterized the variance of both MINE and NWJ estimators, in particular, they proved that the variance scales exponentially with the ground-truth MI $\forall M \in \mathbb{N}$
\begin{align}
\label{eq:MI_exponentially_increasing_variance}
\text{Var}_{p_{XY},p_Xp_Y}\bigl[ I^{M,N}_{NWJ}\bigr] \geq & \frac{e^{I(X;Y)}-1}{N} \nonumber \\
\lim_{N \to \infty} N\text{Var}_{p_{XY},p_Xp_Y}\bigl[ I^{M,N}_{MINE}\bigr] \geq & e^{I(X;Y)}-1,
\end{align}
where
\begin{align}
I^{M,N}_{NWJ}:= \mathbb{E}_{p^M_{XY}}[\log \hat{R} +1] - \mathbb{E}_{p^N_Xp^N_Y}[\hat{R}]   \nonumber \\
 I^{M,N}_{MINE}:= \mathbb{E}_{p^M_{XY}}[\log \hat{R}] - \log \mathbb{E}_{p^N_Xp^N_Y}[\hat{R}].   
\end{align}
To reduce the impact of the partition function on the variance, the authors of \cite{Song2020} also proposed to clip the density ratio between $e^{-\tau}$ and $e^{\tau}$ leading to an estimator (SMILE) with bounded partition variance. However, also the variance of the log-density ratio $\mathbb{E}_{p^M_{XY}}[\log \hat{R}]$ influences the variance of the variational estimators, since it is clear that
\begin{equation}
\text{Var}_{p_{XY},p_Xp_Y}\bigl[ I^{M,N}_{VLB}\bigr] \geq \text{Var}_{p_{XY}}\bigl[ \mathbb{E}_{p^M_{XY}}[\log \hat{R}] \bigr],
\label{eq:MI_lower_bound_variance}
\end{equation}
a result that holds for any type of MI estimator based on a VLB.

The great advantage of $f$-DIME is to avoid the partition function estimation step, significantly reducing the variance of the estimator. Under the same initial assumptions, from \eqref{eq:MI_lower_bound_variance} we can immediately conclude that
\begin{equation}
\text{Var}_{p_{XY}}\bigl[I^{M}_{fDIME}\bigr] \leq \text{Var}_{p_{XY},p_Xp_Y}\bigl[ I^{M,N}_{VLB}\bigr],
\end{equation}
where
\begin{equation}
\label{eq:MI_f_dime_estimator}
I^{M}_{fDIME}:= \mathbb{E}_{p^M_{XY}}[\log \hat{R}]    
\end{equation}
is the Monte Carlo implementation of $f$-DIME. Hence, the $f$-DIME class of models has lower variance than any VLB based estimator (MINE, NWJ, SMILE, etc.). 

The following Lemma provides an upper bound on the variance of the $f$-DIME estimator. Notice that such result holds for any type of value function $\mathcal{J}_{f}$, so it is not restrictive to the KL divergence.

\begin{lemma}
\label{lemma:MI_Lemma2}
Let $\hat{R} = p_{XY}(\mathbf{x},\mathbf{y})/ (p_{X}(\mathbf{x}) p_Y(\mathbf{y}))$ be the density ratio and assume $\text{Var}_{p_{XY}}[\log \hat{R}]$ exists. Let $p^M_{XY}$ be the empirical distribution of $M$ i.i.d. samples from $p_{XY}$ and let $\mathbb{E}_{p^M_{XY}}$ denote the sample average over $p^M_{XY}$. Then, it follows that

\begin{equation}
\text{Var}_{p_{XY}}\bigl[\mathbb{E}_{p^M_{XY}} [\log \hat{R}] \bigr] \leq \frac{ 4H^2(p_{XY},p_Xp_Y)\biggl|\biggl|\frac{p_{XY}}{p_Xp_Y}\biggr|\biggr|_{\infty}-I^2(X;Y)}{M}
\end{equation}
where $H^2$ is the Hellinger distance squared.
\end{lemma}

Lemma \ref{lemma:MI_finite_variance_gaus} characterizes the variance of the estimator in \eqref{eq:MI_f_dime_estimator} when $X$ and $Y$ are correlated Gaussian random variables. We found out that the variance is finite and we use this result to verify in the experiments that the variance of $f$-DIME does not diverge for high values of MI.

\begin{lemma}
\label{lemma:MI_finite_variance_gaus}
Let $\hat{R}$ be the optimal density ratio and let  $X\sim \mathcal{N}(0,\sigma_X^2)$ and $N\sim \mathcal{N}(0,\sigma_N^2)$ be uncorrelated scalar Gaussian random variables such that $Y=X+N$. Assume $\text{Var}_{p_{XY}}[\log \hat{R}]$ exists. Let $p^M_{XY}$ be the empirical distribution of $M$ i.i.d. samples from $p_{XY}$ and let $\mathbb{E}_{p^M_{XY}}$ denote the sample average over $p^M_{XY}$. Then, it holds that
\begin{equation}
\label{eq:MI_variance_f_dime}
\text{Var}_{p_{XY}}\bigl[\mathbb{E}_{p^M_{XY}} [\log \hat{R}] \bigr] = \frac{ 1-e^{-2I(X;Y)}}{M}.
\end{equation}
\end{lemma}

\subsection{Derangement strategy}
\label{subsec:mi_derangements}
The discriminative approach essentially compares expectations over both joint $(\mathbf{x},\mathbf{y}) \sim p_{XY}$ and marginal $(\mathbf{x},\mathbf{y}) \sim p_{X}p_Y$ data points. 
Practically, we have access only to $N$ realizations of the joint distribution $p_{XY}$ and to obtain $N$ marginal samples of $p_Xp_Y$ from $p_{XY}$ a shuffling mechanism for the realizations of $Y$ is typically deployed. A general result in \cite{McAllester2019} shows that failing to sample from the correct marginal distribution would lead to an upper bounded MI estimator.

We study the structure that the permutation law $\sigma(\cdot)$ in Theorem \ref{theorem:MI_theorem1} needs to have when numerically implemented. In particular, we now prove that a naive permutation over the realizations of $Y$ results in an incorrect VLB of the $f$-divergence, causing the MI estimator to be bounded by $\log(N)$, where $N$ is the batch size. To solve this issue, we propose a derangement strategy.  

Let the data points $(\mathbf{x},\mathbf{y}) \sim p_{XY}$ be $N$ pairs $(\mathbf{x}_i, \mathbf{y}_i)$, $\forall i\in \{1,\dots,N\}$. The naive permutation of $\mathbf{y}$, denoted as $\pi(\mathbf{y})$, leads to $N$ new random pairs $(\mathbf{x}_i, \mathbf{y}_j)$, $\forall i$ and $j \in \{ 1, \cdots , N\}$. The idea is that a random naive permutation may lead to at least one pair $(\mathbf{x}_k, \mathbf{y}_k)$, with $k \in \{1,\dots,N\}$, which is actually a sample from the joint distribution. 
Viceversa, the derangement of $\mathbf{y}$, denoted as $\sigma(\mathbf{y})$, leads to $N$ new random pairs $(\mathbf{x}_i, \mathbf{y}_j)$ such that $i \neq j, \forall i$ and $j \in \{ 1, \cdots , N\}$. Such pairs $(\mathbf{x}_i, \mathbf{y}_j), i \neq j$ can effectively be considered samples from $p_X(\mathbf{x})p_Y(\mathbf{y})$.
An example using these definitions is provided in Sec. \ref{subsec:mi_derangement_considerations}.

The following lemma analyzes the relationship between the Monte Carlo approximations of the VLBs of the $f$-divergence $\mathcal{J}_{f}$ in Theorem \ref{theorem:MI_theorem1} using $\pi(\cdot)$ and $\sigma(\cdot)$ as permutation laws.

\begin{lemma}
\label{lemma:MI_lemma4}
Let $(\mathbf{x}_i,\mathbf{y}_i)$, $\forall i\in \{1,\dots,N\}$, be $N$ data points. Let $\mathcal{J}_{f}(T)$ be the value function in \eqref{eq:MI_discriminator_function_f}. Let $\mathcal{J}_{f}^{\pi}(T)$ and $\mathcal{J}_{f}^{\sigma}(T)$ be numerical implementations of $\mathcal{J}_{f}(T)$ using a random permutation and a random derangement of $\mathbf{y}$, respectively. Denote with $K$ the number of points $\mathbf{y}_k$, with $k \in \{1,\dots, N\}$, in the same position after the permutation (i.e., the fixed points). Then
\begin{equation}
\mathcal{J}_{f}^{\pi}(T) \leq \frac{N-K}{N} \mathcal{J}_{f}^{\sigma}(T).
\label{eq:MI_perm_vs_derang}
\end{equation}
\end{lemma}
Lemma \ref{lemma:MI_lemma4} practically asserts that the value function $\mathcal{J}_{f}^{\pi}(T)$ evaluated via a naive permutation of the data is not a valid VLB of the $f$-divergence, and thus, there is no guarantee on the optimality of the discriminator's output. 
An interesting mathematical connection can be obtained when studying $\mathcal{J}_{f}^{\pi}(T)$ as a sort of variational skew-divergence estimator \cite{RenyiCL}.

The following theorem states that in the case of the KL divergence, the maximum of $\mathcal{J}_{f}^{\pi}(D)$ is attained for a  value of the discriminator that is not exactly the density ratio (as it should be from \eqref{eq:MI_optimal_ratio_KL}, see Sec. \ref{subsec:mi_appendix_DIME_examples}).

\begin{theorem}
\label{theorem:MI_permutationsBound}
Let the discriminator $D(\cdot)$ be with enough capacity. Let $N$ be the batch size and $f$ be the generator of the KL divergence. Let $\mathcal{J}_{KL}^{\pi}(D)$ be defined as
\begin{equation}
\mathcal{J}_{KL}^{\pi}(D) =  \mathbb{E}_{(\mathbf{x},\mathbf{y}) \sim p_{XY}(\mathbf{x},\mathbf{y})}\biggl[\log\biggl(D\bigl(\mathbf{x},\mathbf{y}\bigr)\biggr)-f^*\biggl(\log\biggl(D\bigl(\mathbf{x},\pi(\mathbf{y})\bigr)\biggr)\biggr)\biggr].
\label{eq:MI_discriminator_function_perm_KL}
\end{equation}

Denote with $K$ the number of indices in the same position after the permutation (i.e., the fixed points), and with $R(\mathbf{x},\mathbf{y})$ the density ratio in \eqref{eq:MI_density_ratio_1}.
Then,
\begin{equation}
\label{eq:MI_optimal_ratio_perm_KL}
\hat{D}(\mathbf{x},\mathbf{y}) =\arg \max_D \mathcal{J}_{KL}^{\pi}(D) = \frac{NR(\mathbf{x},\mathbf{y})}{KR(\mathbf{x},\mathbf{y})+N-K}.
\end{equation}
\end{theorem}
Although Theorem \ref{theorem:MI_permutationsBound} is stated for the KL divergence, it can be easily extended to any $f$-divergence using Theorem \ref{theorem:MI_theorem1}. Notice that if the number of indices in the same position $K$ is equal to $0$, we fall back into the derangement strategy and we retrieve the density ratio as output. 

When we parametrize $D$ with a NN, we perform multiple training iterations and so we have multiple batches of dimension $N$. This turns into an average analysis on $K$. We report in the Supplementary material section the proof that, on average, $K$ is equal to $1$.
\begin{lemma}[see \cite{alon2016probabilistic}]
\label{lemma:MI_M=1}
The average number of fixed points in a random permutation $\pi(\cdot)$ is equal to 1.
\end{lemma}

From the previous results, it follows immediately that the estimator obtained using a naive permutation strategy is biased and upper bounded by a function of the batch size $N$.  
\begin{corollary}[Permutation bound]
\label{corollary:MI_permutationsUpperBound}
Let KL-DIME be the estimator obtained via iterative optimization of $\mathcal{J}_{KL}^{\pi}(D)$, using a batch of size $N$ every training step. Then,
\begin{equation}
    I_{KL-DIME}^{\pi} := \mathbb{E}_{(\mathbf{x},\mathbf{y}) \sim p_{XY}(\mathbf{x},\mathbf{y})}\biggl[ \log \biggl(\hat{D}(\mathbf{x},\mathbf{y})\biggr) \biggr] < \log(N).
\end{equation}
\end{corollary}

We report in Fig. \ref{fig:MI_derangementVsPermutation} an example of the difference between the derangement and permutation strategies. The estimate attained by using the permutation mechanism, showed in Fig. \ref{fig:MI_permutations}, demonstrates Theorem \ref{theorem:MI_permutationsBound} and Corollary \ref{corollary:MI_permutationsUpperBound}, as the upper bound corresponding to $\log(N)$ (with $N=128$) is clearly visible. 

\begin{figure}
\centering
\begin{subfigure}{0.5\textwidth}
  \centering
  \includegraphics[scale=0.4]{images/mi/Staircase_GAN_deranged_d20_bs128_cubicFalse_derangement.png}
  \caption{Derangement strategy.}
  \label{fig:MI_derangement}
\end{subfigure}%
\begin{subfigure}{0.5\textwidth}
  \centering
  \includegraphics[scale=0.4]{images/mi/Staircase_GAN_deranged_d20_bs128_cubicFalse_permutations.png}
  \caption{Permutation strategy.}
  \label{fig:MI_permutations}
\end{subfigure}
\caption{MI estimate obtained with derangement and permutation training procedures, for data dimension $d=20$ and batch size $N=128$.}
\label{fig:MI_derangementVsPermutation}
\end{figure}


\section{Experimental results}
\sectionmark{Results}
\label{sec:mi_results}
In this section, we first describe the architectures of the proposed estimators. Then, we outline the data used to estimate the MI, comment on the performance of the discussed estimators in different scenarios, also analyzing their computational complexity. Finally, we report in Sec. \ref{subsec:mi_appendix_consistency_tests} the self-consistency tests \cite{Song2020} over image datasets.

\subsection{Architectures}
\label{subsec:mi_architectures}
To demonstrate the behavior of the state-of-the-art MI estimators, we consider multiple NN \textit{architectures}. The word architecture needs to be intended in a wide-sense, meaning that it represents the NN architecture and its training strategy.
In particular, additionally to the architectures \textbf{joint} \cite{Mine2018} and \textbf{separable} \cite{oord2018representation}, we propose the architecture \textbf{deranged}.
The \textbf{joint} architecture concatenates the samples $\mathbf{x}$ and $\mathbf{y}$ as input of a single NN. Each training step requires $N$ realizations $(\mathbf{x}_i, \mathbf{y}_i)$ drawn from $p_{XY}(\mathbf{x},\mathbf{y})$, for $i \in \left\{ 1, \dotsc, N \right\}$ and $N(N-1)$ samples $(\mathbf{x}_i, \mathbf{y}_j), \forall i,j \in \left\{ 1, \dotsc, N \right\}$, with $ i \neq j$.\\
The \textbf{separable} architecture comprises two neural networks, the former fed in with $N$ realizations of $X$, the latter with $N$ realizations of $Y$. The inner product between the outputs of the two networks is exploited to obtain the MI estimate.

The proposed \textbf{deranged} architecture feeds a neural network with the concatenation of the samples $\mathbf{x}$ and $\mathbf{y}$, similarly to the \textit{joint} architecture. However, the \textit{deranged} one obtains the samples of $p_X(\mathbf{x})p_Y(\mathbf{y})$ by performing a derangement of the realizations $\mathbf{y}$ in the batch sampled from $p_{XY}(\mathbf{x},\mathbf{y})$.
Such diverse training strategy solves the main problem of the \textit{joint} architecture: the difficult scalability to large batch sizes. For large values of $N$, the complexity of the \textit{joint} architecture is $\Omega(N^2)$, while the complexity of the \textit{deranged} one is $\Omega(N)$.

NJEE utilizes a specific architecture, in the following referred to as \textbf{ad hoc}, comprising $2d-1$ neural networks, where $d$ is the dimension of $X$. $I_{NJEE}$ training procedure is supervised: the input of each neural network does not include the $\mathbf{y}$ samples.
All the implementation details are reported in Sec.  \ref{subsec:mi_appendix_experiment_details}.

\subsection{Multivariate linear and non-linear Gaussians}
\label{subsec:mi_stairs}
We benchmark the proposed class of MI estimators on two settings utilized in previous papers \cite{Poole2019, Song2020}.
In the first setting (called \textbf{Gaussian}), a $20$-dimensional Gaussian distribution is sampled to obtain $\mathbf{x}$ and $\mathbf{n}$ samples, independently. Then, $\mathbf{y}$ is obtained as linear combination of $\mathbf{x}$ and $\mathbf{n}$: $\mathbf{y} = \rho \, \mathbf{x} + \sqrt{1-\rho^2} \, \mathbf{n}$, where $\rho$ is the correlation coefficient.
In the second setting (referred to as \textbf{cubic}), the non-linear transformation $\mathbf{y} \mapsto \mathbf{y}^3$ is applied to the Gaussian samples. The true MI follows a staircase shape, where each step is a multiple of $2$ $nats$. Each NN is trained for 4k iterations for each stair step, with a batch size of $64$ samples ($N=64$). The values $d=20$ and $N=64$ are used in the literature to compare MI neural estimators. 
\begin{figure}
	\centering
	\includegraphics[scale=0.24]{images/mi/allStaircases_d20_bs64.png}
	\caption{Staircase MI estimation comparison for $d=20$ and $N=64$. The \textit{Gaussian} case is reported in the top row, while the \textit{cubic} case is shown in the bottom row.}
	\label{fig:MI_stairs}
\end{figure} 
The tested estimators are: $I_{NJEE}$, $I_{SMILE}$ ($\tau=1$), $I_{GAN-DIME}$, $I_{HD-DIME}$, $I_{KL-DIME}$, and $I_{CPC}$, as illustrated in Fig. \ref{fig:MI_stairs}. The performance of $I_{MINE}$, $I_{NWJ}$, and $I_{SMILE} (\tau=\infty)$ is reported in Sec. \ref{subsec:mi_appendix_experiment_details}, since these algorithms exhibit lower performance compared to both SMILE and $f$-DIME.
In fact, all the $f$-DIME estimators have lower variance compared to $I_{MINE}$, $I_{NWJ}$, and $I_{SMILE} (\tau = \infty)$, which are characterized by an exponentially increasing variance (see (\ref{eq:MI_exponentially_increasing_variance})).
In particular, all the analyzed estimators belonging to the $f$-DIME class achieve significantly low bias and variance when the true MI is small. Interestingly, for high target MI, different $f$-divergences lead to dissimilar estimation properties.
For large MI, $I_{KL-DIME}$ is characterized by a low variance, at the expense of a high bias and a slow rise time.
Contrarily, $I_{HD-DIME}$ attains a lower bias at the cost of slightly higher variance w.r.t. $I_{KL-DIME}$. 
Diversely, $I_{GAN-DIME}$ achieves the lowest bias, and a variance comparable to $I_{HD-DIME}$.

The MI estimates obtained with $I_{SMILE}$ and $I_{GAN-DIME}$ appear to possess similar behavior, although the value functions of SMILE and GAN-DIME are structurally different.
The reason why $I_{SMILE}$ is almost equivalent to $I_{GAN-DIME}$ resides in their training strategy, since they both minimize the same $f$-divergence.
Looking at the implementation code of SMILE \cite{SMILE_github}, in fact, the network's training is guided by the gradient computed using the Jensen-Shannon (JS) divergence (a linear transformation of the GAN divergence). 

Given the trained network, the clipped objective function proposed in \cite{Song2020} is only used to compute the MI estimate, since when (\ref{eq:MI_SMILE}) is used to train the network, the MI estimate diverges (see Fig. \ref{fig:MI_SMILE_no_trick} in Sec. \ref{subsec:mi_appendix_experiment_details}). 
However, with the proposed class of $f$-DIME estimators we show that during the estimation phase the partition function (clipped in \cite{Song2020}) is not necessary to obtain the MI estimate.

$I_{NJEE}$ obtains an estimate for $d=20$ and $N=64$ that has slightly higher bias than $I_{GAN-DIME}$ for large MI values and slightly higher variance than $I_{KL-DIME}$.
$I_{CPC}$ is characterized by high bias and low variance. A schematic comparison between all the MI estimators is reported in Tab. \ref{tab:MI_summary_estimators} in Sec. \ref{subsec:mi_appendix_experiment_details}.

When $N$ and $d$ vary, the class of $f$-DIME estimators proves its robustness (i.e., maintains low bias and variance), as represented in Fig. \ref{fig:MI_stairs_d5_bs64} and \ref{fig:MI_stairs_d20_bs1024}. Differently, the behavior of $I_{CPC}$ strongly depends on $N$. At the same time, $I_{NJEE}$ achieves higher bias when $N$ increases and, even more severely, when $d$ decreases (see Fig. \ref{fig:MI_stairs_d5_bs64}). 

Additional results describing all estimators' behavior when $d$ and $N$ vary are reported and described in Sec.  \ref{subsec:mi_appendix_experiment_details}.

\begin{figure}
	\centering
	\includegraphics[scale=0.24]{images/mi/allStaircases_d5_bs64.png}
	\caption{Staircase MI estimation comparison for $d=5$ and $N=64$. The \textit{Gaussian} case is reported in the top row, while the \textit{cubic} case is shown in the bottom row.}
	\label{fig:MI_stairs_d5_bs64}
\end{figure} 
 \begin{figure}
	\centering
	\includegraphics[scale=0.24]{images/mi/allStaircases_d20_bs1024.png}
	\caption{Staircase MI estimation comparison for $d=20$ and $N=1024$. The \textit{Gaussian} case is reported in the top row, while the \textit{cubic} case is shown in the bottom row.}
	\label{fig:MI_stairs_d20_bs1024}
\end{figure} 

\subsection{Complex Gaussian and non-Gaussian distributions}
 We test our estimators over additional complex Gaussian data transformations (half-cube, asinh, and swiss roll mappings) and non-Gaussian distributions (uniform and student distributions) as suggested in \cite{czyz2023beyond}. The half-cube mapping is used to lengthen the tails of the Gaussian distributions. The inverse hyperbolic sine (asinh) mapping shortens the tails of the Gaussian distributions. These two transformations are applied to the same scenario of the Gaussian and cubic already present in our paper. The swiss roll mapping increases the dimensionality of the data distribution (from two to three dimensions) and it is usually used to test dimensionality reduction techniques. It considers two Gaussian random variables that are transformed into uniform random variables via the probability integral transform. The swiss roll mapping is applied to the $X$ uniform random variable. The stairs plots are obtained by varying the correlation between the initial Gaussian distributions. The uniform case estimates the MI of the summation of two uniform random variables $U(0,1)$ and $U(-\epsilon, \epsilon)$, where we vary the parameter $\epsilon$, modifying the true MI. The student scenario analyzes the case of a multivariate student distribution with dispersion matrix chosen to be the identity matrix and degrees of freedom $df$. In this scenario, we vary $df$, implying a variation of the target MI. 

\begin{figure}
	\centering
	\includegraphics[scale=0.2]{images/mi/FigureB_allStaircases_d5_bs64_arcseparable.pdf}
	\caption{Staircase MI estimation comparison for $d=5$ and $N=64$. Top: Half-cube scenario. Middle: Asinh scenario. Bottom: Swiss roll scenario.}
	\label{fig:MI_stairs_d5_bs64_beyond}
\end{figure} 
 \begin{figure}
	\centering
	\includegraphics[scale=0.2]{images/mi/FigureC_allStaircases_d1_bs64_arcseparable_scenuniform.pdf}
	\caption{Staircase MI estimation comparison for $d=1$ and $N=64$. Top row: Uniform scenario. Bottom row: Student scenario}
	\label{fig:MI_stairs_d5_bs64_beyond2}
\end{figure} 

Figs. \ref{fig:MI_stairs_d5_bs64_beyond} and \ref{fig:MI_stairs_d5_bs64_beyond2} demonstrate the efficacy of $f$-DIME in all the analyzed scenarios.


\subsubsection{Computational Time Analysis}
\label{subsubsec:mi_time_analysis}
\begin{figure}
\centering
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[scale=0.4]{images/mi/fixed_d20_varying_bs.png}
  \caption{Multivariate Gaussian distribution size \\ fixed to $20$. Batch size varying from $64$ to $1024$.}
  \label{fig:MI_timeVaryingBS}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[scale=0.4]{images/mi/varying_d_fixed_bs64.png}
  \caption{Multivariate Gaussian distribution size \\ varying from $5$ to $25$. Batch size fixed to $64$.}
  \label{fig:MI_timeVaryingD}
\end{subfigure}
\caption{Time requirements comparison to complete the 5-step staircase MI.}
\label{fig:MI_computationalTimeAnalysisMain}
\end{figure}

A fundamental characteristic of each algorithm is the computational time. 
The time requirements to complete the 5-step staircase MI when varying the multivariate Gaussian distribution dimension $d$ and the batch size $N$ are reported in Fig. \ref{fig:MI_computationalTimeAnalysisMain}.
The difference among the estimators computational time is not significant, when comparing the same architectures.
However, as discussed in Sec. \ref{subsec:mi_architectures}, the \textit{deranged} strategy is significantly faster than the \textit{joint} one as $N$ increases. 
The fact that the \textit{separable} architecture uses two neural networks implies that when $N$ is significantly large, the \textit{deranged} implementation is much faster than the \textit{separable} one, as well as more stable to the training and distribution parameters, as shown in Sec. \ref{subsec:mi_appendix_experiment_details}. 

$I_{NJEE}$ is evaluated with its own architecture, which is the most computationally demanding, because it trains a number of neural networks equal to $2d-1$. Thus, $I_{NJEE}$ can be utilized only in cases where the time availability is orders of magnitudes higher than the other approaches considered.
When $d$ is large, the training of $I_{NJEE}$ fails due to memory requirement problems. For example, our hardware platform (described Sec. \ref{subsec:mi_appendix_experiment_details}) does not allow the usage of $d>30$. 


\section{Improved autoencoding schemes}
\sectionmark{f-DIME autoencoders}
\label{sec:fDIME_autoencoders}
In this section, we quickly discuss how to adapt the findings of $f$-DIME into the rate-driven AE architecture proposed in Sec. \ref{sec:cacao}. In particular, we show how the choice of the MI estimator impacts the training of the AE.  

\subsection{$\gamma$-DIME}
\label{subsec:mi_gammaDIME}
Only a subset of $\mathcal{J}_f$ in \eqref{eq:MI_discriminator_function_f} can be used inside the rate-driven autoencoder loss function described in Sec. \ref{sec:cacao}. Indeed, a maximization of the MI is required and only the generator $f$ of the KL divergence is suitable for the MI maximization, since in such case
\begin{equation}
\max_{p_X(x)} D_f = \max_{p_X(x)} D_{KL} = \max_{p_X(x)} I(X;Y).
\end{equation}
The generator function of the KL divergence is $f(u) = u\log u$ with conjugate $f^*(t) = e^{t-1}$, as shown when described KL-DIME estimator $I_{KL-DIME}$. Nevertheless, as shown in \cite{LSGAN}, the value function plays a fundamental role during the training process of a discriminator. Consequently, we used the approach presented for the derivation of $f$-DIME to obtain a family of lower bounds specifically on the MI that may result in more robust training processes and estimators according to specific case studies.

\begin{lemma}
\label{lemma:MI_LemmaKL}
Let $X\sim p_X(x)$ and $Y\sim p_{Y}(y|x)$ be the channel input and output, respectively. Let $D(\cdot)$ be a positive function. If $\mathcal{J}_{\gamma}(D)$, $\gamma>0$, is a value function defined as 
\begin{equation}
\label{eq:MI_value_function_f}
\mathcal{J}_{\gamma}(D) = \gamma \cdot \mathbb{E}_{(x,y) \sim p_{XY}(x,y)}\biggl[\log \biggl(D\bigl(x,y\bigr)\biggr)\biggr] + \mathbb{E}_{(x,y) \sim p_{X}(x)p_{Y}(y)}\biggl[- D^{\gamma}\bigl(x,y\bigr)\biggr],
\end{equation}
then
\begin{equation}
I(X;Y) \geq  \mathcal{J}_{\gamma}(D^*)+1,
\end{equation}
\begin{equation}
\label{eq:MI_gammaDIME}
I_{\gamma DIME} = \gamma \cdot \mathbb{E}_{(x,y) \sim p_{XY}(x,y)}\bigl[\log D^*(x,y) \bigr],
\end{equation}
where
\begin{equation}
\label{eq:MI_optimal_discriminator_gamma}
D^*(x,y) = \biggl(\frac{p_{XY}(x,y)}{p_{X}(x)\cdot p_Y(y)}\biggr)^{1/\gamma} = \arg \max_D \mathcal{J}_{\gamma}(D).
\end{equation}
\end{lemma}

\begin{proof}
Consider a scaled generator $f(u) = \frac{u}{\gamma}\log u$ and for simplicity of notation, denote $p_{XY}$ and $p_Xp_Y$ with $p$ and $q$, respectively. Then
\begin{equation}
D_{KL}(p||q) = \gamma \int_{x}{q(x) \frac{p(x)}{\gamma q(x)}\log\biggl(\frac{p(x)}{q(x)}\biggr) \diff x},
\end{equation}
with conjugate $f^*(t)$, with $t \in \mathbb{R}$ given by
\begin{equation}
f^*(t) = \frac{e^{\gamma t -1}}{\gamma}.
\end{equation}
Substituting in \eqref{eq:MI_f_bound} yields to
\begin{equation}
D_{KL}(p||q) \geq \gamma \sup_{T\in \mathbb{R}} \biggl\{ \mathbb{E}_{x \sim p(x)} \bigl[T(x)\bigr]-\frac{1}{\gamma}\mathbb{E}_{x\sim q(x)}\bigl[e^{\gamma T(x)-1} \bigr]\biggr\}.
\end{equation}
Using \eqref{eq:MI_optimal_ratio_T} it is easy to verify that the optimal value of $T$ is the log-likelihood ratio rather than the density ratio itself. Indeed,
\begin{equation}
T^*(x) = \frac{1}{\gamma}\log\biggl(\frac{p(x)}{q(x)}\biggr) + \frac{1}{\gamma}.
\end{equation}
Finally, with the change of variable $T(x)=\log(D(x))+1/{\gamma}$, the optimal discriminator has form
\begin{equation}
D^*(x) = \biggl(\frac{p(x)}{q(x)}\biggr)^{1/\gamma}
\end{equation}
and
\begin{equation}
D_{KL}(p||q) \geq \sup_{D\in \mathbb{R}_+} \biggl\{ \gamma \mathbb{E}_{x \sim p(x)} \bigl[\log \bigl( D(x) \bigr) \bigr] -\mathbb{E}_{x\sim q(x)}\bigl[D^{\gamma}(x) \bigr]\biggr\} + 1.
\end{equation}
Denoting the two terms inside the supremum with $J_{\gamma}(D)$ and replacing $p$ and $q$ with the joint and marginal distributions, concludes the proof.
\end{proof}

\begin{figure}
\centering
  	\includegraphics[scale = 0.3]{images/mi/gammaDIME_combined_text}
  	\caption{a) Behaviour of the functional $J_{\gamma}$ varying the parameter $\gamma$. Concavity of the neighbourhood of the maximal value depends on $\gamma$. b) Comparison of the BLER obtained by the AE($3,9$) and AE($6,3$) with an AWGN intermediate layer, using different estimators with $\beta = 0.2$, $\epsilon = 0.2$.}
  	\label{fig:MI_gammaDIME}
\end{figure}

Fig.~\ref{fig:MI_gammaDIME}a illustrates how the value function in \eqref{eq:MI_value_function_f} (divided by $\gamma$) behaves according to the choice of the positive parameter $\gamma$. The position of the maximum depends on the density ratio as described in \eqref{eq:MI_optimal_discriminator_gamma} ($R = 1$ in the figure). The role of $\gamma$ is to modify the concavity of the value function to improve the convergence rate of the gradient ascent algorithm. Moreover, when $D$ is parameterized by a neural network, the choice of the learning rate and activation functions may be related with $\gamma$. Indeed, notice that $\gamma$-DIME can also be derived from d-DIME using the substitution $D(x,y) \rightarrow D^{\gamma}(x,y)$.

\subsection{Results}
\label{subsec:mi_gammaDIME_results}
In the experimental results, we consider the transmission of $M$ messages at rate $R$ over an AWGN channel, for which we know the channel capacity under an
average power constraint. The choice of AWGN channel shall not be seen as a limitation since the formulation transcends the channel characteristics. Moreover, it provides a reference curve for the validation of the estimators and it allows comparisons across several application domains.

We describe two scenarios: a) the transmission at a rate $R>1$ and short code-length $n$ and b) the transmission at a low rate with larger code-length. For both scenarios, we train a rate-driven AE and compare its performance for different estimators in terms of BLER and accuracy of the mutual information estimation. 

For all the experiments and as a proof of concept, we use simple encoder, decoder and neural MI estimator architectures. In particular, the encoder is a shallow neural network with $M$ neurons in the input layer, $M$ in the hidden one and $2n$ neurons at the output while the decoder is its complementary.  
The discriminative estimator possesses instead a fixed architecture, a
two layer multilayer perceptron NN of $200$
neurons in each layer and LeakyReLU with a negative slope coefficient of $0.2$ as activation function. The only
difference in the estimators resides in the final layer where we use a linear dense layer for MINE, while a softplus layer for the DIME based estimators.
The training of the AE was conducted at a fixed $E_b/N_0$ ratio of $7$ dB. We minimized the loss function alternating encoder and decoder weights update with the discriminator update. The number of iterations was set to $10$k with a learning rate of $0.01$ for both the components. For the implementation details, we refer to the GitHub code \cite{CACAO_github}.

The first scenario describes a digital transmission scheme with input alphabet size $M=64$ over $3$ channel uses. We denote such system as AE($6,3$) since $k = \log_2(M)$. The communication rate is $R=2$. Fig.~\ref{fig:MI_gammaDIME}b shows the AE performance in terms of BLER for different estimators. From the curves we can infer that the $\gamma$-DIME estimator with $\gamma=1$ provides improved performance in terms of BLER compared to both MINE and other values of $\gamma$. The result of Fig. \ref{fig:MI_MI_6_6}a is insightful: the $\gamma$-DIME estimators provide stable estimations also at high SNRs where it is expected that the MI saturates to the information rate $R=2$. Thus, the DIME based estimators are more accurate compared to the divergent MINE (see Theorem 2 of \cite{Song2020}). It is also interesting to notice that the concavity of $J_{\gamma}$ (shown in Fig.~\ref{fig:MI_gammaDIME}a) influences the MI estimation as it appears in the results. Indeed, for low values of $\gamma$, fixed learning rate and training iterations, the estimators are trained by moving on the flat curves of Fig.~\ref{fig:MI_gammaDIME}a, which is related to using a small learning rate to upgrade the gradients. For completeness and to show the advantage of the proposed estimators under fading, we report in Fig. \ref{fig:MI_MI_6_6}b the achieved estimated information rate with Rayleigh fading. Also in this case, MINE diverges for high SNR values while the DIME based estimators are much more accurate and tend to saturate to the rate $R=2$.  In addition, $\gamma =2$ seems to perform better for low and middle SNR values, which is consistent with the idea that different scenarios may require different parameters and therefore $\gamma$-DIME provides more flexibility for model learning.

\begin{figure}
\centering
  	\includegraphics[scale=0.2]{images/mi/MI_6_6_2_three_text}
  	\caption{Comparison of different MI estimators for the AE($6,3$) with $\beta = 0.2$, $\epsilon = 0.2$ on a) the AWGN channel; b) on the Rayleigh fading channel. c) Comparison of the achieved information rate using different MI estimators for the AE($3,9$) with $\beta = 0.2$, $\epsilon = 0.2$ on the AWGN channel.}
  	\label{fig:MI_MI_6_6}
\end{figure}


The second scenario describes a digital transmission scheme with input alphabet size $M=8$ over $9$ channel uses. We denote such system as AE($3,9$). The communication rate is $R=1/3$. Fig.~\ref{fig:MI_gammaDIME}b shows that MINE and $\gamma$-DIME with $\gamma=1$ have similar performance in terms of BLER under the same training iterations. However, Fig.~\ref{fig:MI_MI_6_6}c highlights the fact that even for a low alphabet dimension and for relatively small $n$, the DIME-based estimators are more accurate than MINE since they tend to saturate to $R=1/3$ while MINE overestimates the MI. We can conclude that despite MINE being a tighter bound on the MI compared to $\gamma$-DIME, the latter and in particular the estimator with $\gamma=1$ performs much better than MINE in terms of MI estimation. We leave comments about the architecture to use for future studies, although we remark that it is one of the main aspects in modern ML applications and therefore we expect significant improvements. 

\section{Summary}
\label{sec:mi_conclusions}
In this chapter, we presented $f$-DIME, a class of discriminative MI estimators based on the variational representation of the $f$-divergence. We proved that any valid choice of the function $f$ leads to a low-variance MI estimator which can be parametrized by a NN. We also proposed a derangement training strategy that efficiently samples from the product of marginal distributions. 
The performance of $f$-DIME is evaluated using three functions $f$, and it is compared with state-of-the-art estimators. Results demonstrate excellent bias/variance trade-off for different data dimensions and different training parameters.

When $f$ is the function generator of the KL divergence, we obtained as a special case $\gamma$-DIME, a parametric estimator that can be used both for MI estimation and design of optimal channel coding. We integrated it as the MI regularization term inside the AE loss
function. A comparison in terms of estimation accuracy and
BLER was done with MINE. Results show the advantage of adopting $\gamma$-DIME over MINE for the AE end-to-end training, especially when the aim is to estimate the achieved information rate. 

In the next chapter, we extend this idea to create a framework that builds capacity-achieving distributions and that estimates the channel capacity for any discrete-time continuous memoryless channel.

\newpage
\section{Supplementary material}
\sectionmark{Proofs}
\label{sec:mi_appendix}

\subsection{DIME estimators}
\label{subsec:mi_appendix_DIME_examples}
In this section, we provide a concrete list of DIME estimators obtained using three different $f$-divergences. In particular, we maximize the value function defined in \eqref{eq:MI_discriminator_function_f}
\begin{equation}
    \mathcal{J}_{f}(T) =  \mathbb{E}_{(\mathbf{x},\mathbf{y}) \sim p_{XY}(\mathbf{x},\mathbf{y})}\biggl[T\bigl(\mathbf{x},\mathbf{y}\bigr)-f^*\biggl(T\bigl(\mathbf{x},\sigma(\mathbf{y})\bigr)\biggr)\biggr],
\end{equation}
over $T$ or its transformation. By doing that, and using \eqref{eq:MI_f-DIME}, 
\begin{equation}
    I(X;Y) = I_{fDIME}(X;Y) =  \mathbb{E}_{(\mathbf{x},\mathbf{y}) \sim p_{XY}(\mathbf{x},\mathbf{y})}\biggl[ \log \biggl(\bigl(f^{*}\bigr)^{\prime}\bigl(\hat{T}(\mathbf{x},\mathbf{y})\bigr) \biggr) \biggr],
\end{equation}
we obtain a list of three different MI estimators.
The list is used for both commenting on the impact of the $f$ function, referred to as the generator function, and for comparing the estimators discussed in Sec. \ref{sec:mi_related}. 

We consider the cases when $f$ is the generator of: 
\begin{itemize}
    \item[{a)}] the KL divergence;
    \item[{b)}] the GAN divergence; 
    \item[{c)}] the Hellinger distance squared.
\end{itemize}
We report below the derived value functions and the mathematical expressions of the proposed estimators. 

\subsubsection{KL divergence}
The variational representation of the KL divergence \cite{Nguyen2010} leads to the NWJ estimator in \eqref{eq:MI_NWJ} when $f(u) = u\log(u)$. However, since we are interested in extracting the density ratio, we apply the transformation $T(\mathbf{x})=\log(D(\mathbf{x}))$. In this way, the lower bound introduced in \eqref{eq:MI_discriminator_function_f} reads as follows
\begin{equation}
\mathcal{J}_{KL}(D) = \mathbb{E}_{(\mathbf{x},\mathbf{y}) \sim p_{XY}(\mathbf{x},\mathbf{y})}\biggl[\log\bigl(D\bigl(\mathbf{x},\mathbf{y}\bigr)\bigr)\biggr] -\mathbb{E}_{(\mathbf{x},\mathbf{y}) \sim p_{X}(\mathbf{x})p_{Y}(\mathbf{y})}\biggl[D\bigl(\mathbf{x},\mathbf{y}\bigr)\biggr]+1,
\end{equation}
which has to be maximized over positive discriminators $D(\cdot)$.
As remarked before, we do not use $\mathcal{J}_{KL}$ during the estimation, rather we define the KL-DIME estimator as
\begin{equation}
\label{eq:MI_KL-DIME}
I_{KL-DIME}(X;Y) :=  \mathbb{E}_{(\mathbf{x},\mathbf{y}) \sim p_{XY}(\mathbf{x},\mathbf{y})}\biggl[ \log \biggl(\hat{D}(\mathbf{x},\mathbf{y})\biggr) \biggr],
\end{equation}
due to the fact that
\begin{equation}
\label{eq:MI_optimal_ratio_KL}
\hat{D}(\mathbf{x},\mathbf{y}) =\arg \max_D \mathcal{J}_{KL}(D) = \frac{p_{XY}(\mathbf{x},\mathbf{y})}{p_X(\mathbf{x})p_Y(\mathbf{y})}.
\end{equation}

\subsubsection{GAN divergence}
Following a similar approach, it is possible to define $f(u) = u\log u-(u+1)\log(u+1)+\log4$ and $T(\mathbf{x})=\log(1-D(\mathbf{x}))$. We derive from Theorem \ref{theorem:MI_theorem1} the GAN-DIME estimator obtained via maximization of
\begin{equation}
\mathcal{J}_{GAN}(D) = \mathbb{E}_{(\mathbf{x},\mathbf{y}) \sim p_{XY}(\mathbf{x},\mathbf{y})}\biggl[\log\bigl(1-D\bigl(\mathbf{x},\mathbf{y}\bigr)\bigr)\biggr] +\mathbb{E}_{(\mathbf{x},\mathbf{y}) \sim p_{X}(\mathbf{x})p_{Y}(\mathbf{y})}\biggl[\log\bigl(D\bigl(\mathbf{x},\mathbf{y}\bigr)\bigr)\biggr]+\log(4).
\end{equation}
In fact, at the equilibrium we recover \eqref{eq:MI_gan_density_ratio}, hence
\begin{equation}
\label{eq:MI_GAN-DIME}
I_{GAN-DIME}(X;Y) :=  \mathbb{E}_{(\mathbf{x},\mathbf{y}) \sim p_{XY}(\mathbf{x},\mathbf{y})}\biggl[ \log \biggl(\frac{1-\hat{D}(\mathbf{x},\mathbf{y})}{\hat{D}(\mathbf{x},\mathbf{y})}\biggr) \biggr].
\end{equation}

\subsubsection{Hellinger distance}
The last example we consider is the generator of the Hellinger distance squared $f(u)=(\sqrt{u}-1)^2$ with the change of variable $T(\mathbf{x})=1-D(\mathbf{x})$. After simple manipulations, we obtain the associated value function as
\begin{equation}
\mathcal{J}_{HD}(D) = 2-\mathbb{E}_{(\mathbf{x},\mathbf{y}) \sim p_{XY}(\mathbf{x},\mathbf{y})}\biggl[D\bigl(\mathbf{x},\mathbf{y}\bigr)\biggr] -\mathbb{E}_{(\mathbf{x},\mathbf{y}) \sim p_{X}(\mathbf{x})p_{Y}(\mathbf{y})}\biggl[\frac{1}{D(\mathbf{x},\mathbf{y})}\biggr],
\end{equation}
which is maximized for
\begin{equation}
\label{eq:MI_optimal_ratio_HD}
\hat{D}(\mathbf{x},\mathbf{y}) =\arg \max_D \mathcal{J}_{HD}(D) = \sqrt{\frac{p_X(\mathbf{x})p_Y(\mathbf{y})}{p_{XY}(\mathbf{x},\mathbf{y})}},
\end{equation}
leading to the HD-DIME estimator
\begin{equation}
\label{eq:MI_HD-DIME}
I_{HD-DIME}(X;Y) :=  \mathbb{E}_{(\mathbf{x},\mathbf{y}) \sim p_{XY}(\mathbf{x},\mathbf{y})}\biggl[ \log \biggl(\frac{1}{\hat{D}^2(\mathbf{x},\mathbf{y})}\biggr) \biggr].
\end{equation}

Given that these estimators comprise only one expectation over the joint samples, their variance has different properties compared to the variational ones requiring the partition term such as MINE and NWJ. 


\subsection{Proofs of lemmas and theorems}
\label{subsec:mi_omitted_proofs}
\begin{theorem}[Related to Theorem~\ref{theorem:MI_theorem1}]
Let $(X,Y) \sim p_{XY}(\mathbf{x},\mathbf{y})$ be a pair of random variables. Let $\sigma(\cdot)$ be a permutation function such that  $p_{\sigma(Y)}(\sigma(\mathbf{y})|\mathbf{x}) = p_{Y}(\mathbf{y})$. Let $f^*$ be the Fenchel conjugate of $f:\mathbb{R}_+ \to \mathbb{R}$, a convex lower semicontinuous function that satisfies $f(1)=0$ with derivative $f^{\prime}$.
If $\mathcal{J}_{f}(T)$ is a value function defined as 
\begin{equation}
\mathcal{J}_{f}(T) =  \mathbb{E}_{(\mathbf{x},\mathbf{y}) \sim p_{XY}(\mathbf{x},\mathbf{y})}\biggl[T\bigl(\mathbf{x},\mathbf{y}\bigr)-f^*\biggl(T\bigl(\mathbf{x},\sigma(\mathbf{y})\bigr)\biggr)\biggr],
\end{equation}
then
\begin{equation}
\hat{T}(\mathbf{x},\mathbf{y}) =\arg \max_T \mathcal{J}_f(T) = f^{\prime} \biggl(\frac{p_{XY}(\mathbf{x},\mathbf{y})}{p_X(\mathbf{x})p_Y(\mathbf{y})}\biggr),
\end{equation}
and
\begin{equation}
I(X;Y) = I_{fDIME}(X;Y) =  \mathbb{E}_{(\mathbf{x},\mathbf{y}) \sim p_{XY}(\mathbf{x},\mathbf{y})}\biggl[ \log \biggl(\bigl(f^{*}\bigr)^{\prime}\bigl(\hat{T}(\mathbf{x},\mathbf{y})\bigr) \biggr) \biggr].
\end{equation}
\end{theorem}
 

\begin{proof} 
From the hypothesis, the value function can be rewritten as
\begin{equation}
\mathcal{J}_{f}(T) =  \mathbb{E}_{(\mathbf{x},\mathbf{y}) \sim p_{XY}(\mathbf{x},\mathbf{y})}\biggl[T\bigl(\mathbf{x},\mathbf{y}\bigr)\biggr]  -\mathbb{E}_{(\mathbf{x},\mathbf{y}) \sim p_{X}(\mathbf{x})p_{Y}(\mathbf{y})}\biggl[f^*\biggl(T\bigl(\mathbf{x},\mathbf{y}\bigr)\biggr)\biggr].
\end{equation}
The thesis follows immediately from Lemma 1 of \cite{Nguyen2010}. Indeed, the $f$-divergence $D_f$ can be expressed in terms of its lower bound via Fenchel convex duality
\begin{equation}
\label{eq:MI_f_bound}
D_f(P||Q) \geq \sup_{T\in \mathbb{R}} \biggl\{ \mathbb{E}_{x \sim p(x)} \bigl[T(x)\bigr]-\mathbb{E}_{x\sim q(x)}\bigl[f^*\bigl(T(x)\bigr)\bigr]\biggr\},
\end{equation}
where $T: \mathcal{X} \to \mathbb{R}$ and $f^*$ is the Fenchel conjugate of $f$ defined as
\begin{equation}
f^*(t) := \sup_{u\in \mathbb{R}} \{ ut -f(u)\}.
\end{equation}
Therein, it was shown that the bound in \eqref{eq:MI_f_bound} is tight for optimal values of $T(x)$ and it takes the following form
\begin{equation}
\hat{T}(x) = f^{\prime} \biggl(\frac{p(x)}{q(x)}\biggr),
\end{equation}
where $f^{\prime}$ is the derivative of $f$.

The mutual information $I(X;Y)$ admits the KL divergence representation 
\begin{equation}
I(X;Y) = D_{KL}(p_{XY}||p_X p_Y),
\end{equation}
and since the inverse of the derivative of $f$ is the derivative of the conjugate $f^*$, the density ratio can be rewritten in terms of the optimum discriminator $\hat{T}$
\begin{equation}
\bigl(f^{\prime}\bigr)^{-1}\bigl(\hat{T}(\mathbf{x},\mathbf{y})\bigr) = \bigl(f^*\bigr)^{\prime}\bigl(\hat{T}(\mathbf{x},\mathbf{y})\bigr) = \frac{p_{XY}(\mathbf{x},\mathbf{y})}{p_X(\mathbf{x})p_Y(\mathbf{y})}.
\end{equation}
$f$-DIME finally reads as follows
\begin{equation}
I_{fDIME}(X;Y) = \mathbb{E}_{(\mathbf{x},\mathbf{y}) \sim p_{XY}(\mathbf{x},\mathbf{y})}\biggl[ \log \biggl(\bigl(f^*\bigr)^{\prime}\bigl(\hat{T}(\mathbf{x},\mathbf{y})\bigr) \biggr) \biggr].
\end{equation}
\end{proof}


\begin{lemma}[Related to Lemma~\ref{lemma:MI_convergence}]
Let the discriminator $T(\cdot)$ be with enough capacity, i.e., in the non parametric limit. Consider the problem
\begin{equation}
\hat{T} =  \; \arg \max_T \mathcal{J}_{f}(T)
%\label{eq:MI_Lemma4_problem}
\end{equation}
where
\begin{equation}
\mathcal{J}_{f}(T) = \mathbb{E}_{(\mathbf{x},\mathbf{y}) \sim p_{XY}(\mathbf{x},\mathbf{y})}\biggl[T\bigl(\mathbf{x},\mathbf{y}\bigr)\biggr] -\mathbb{E}_{(\mathbf{x},\mathbf{y}) \sim p_{X}(\mathbf{x})p_{Y}(\mathbf{y})}\biggl[f^*\biggl(T\bigl(\mathbf{x},\mathbf{y}\bigr)\biggr)\biggr],
\end{equation}
and the update rule based on the gradient descent method
\begin{equation}
T^{(n+1)} = T^{(n)} + \mu \nabla \mathcal{J}_{f}(T^{(n)}).
\end{equation}
If the gradient descent method converges to the global optimum $\hat{T}$, the mutual information estimator 
\begin{equation}
I(X;Y) = I_{fDIME}(X;Y) =  \mathbb{E}_{(\mathbf{x},\mathbf{y}) \sim p_{XY}(\mathbf{x},\mathbf{y})}\biggl[ \log \biggl(\bigl(f^{*}\bigr)^{\prime}\bigl(\hat{T}(\mathbf{x},\mathbf{y})\bigr) \biggr) \biggr].
\end{equation}
converges to the real value of the mutual information $I(X;Y)$.
\end{lemma}

\begin{proof}
For convenience of notation, let the instantaneous mutual information be the random variable defined as
\begin{equation}
i(X;Y) := \log\biggl(\frac{p_{XY}(\mathbf{x},\mathbf{y})}{p_X(\mathbf{x})p_Y(\mathbf{y})}\biggr).
\end{equation}
It is straightforward to notice that the MI corresponds to the expected value of $i(X;Y)$ over the joint distribution $p_{XY}$.
The solution to \eqref{eq:MI_Lemma4_problem} is given by \eqref{eq:MI_optimal_ratio_T} of Theorem \ref{theorem:MI_theorem1}. Let $\delta^{(n)}=\hat{T}-T^{(n)}$ be the displacement between the optimum discriminator $\hat{T}$ and the obtained one $T^{(n)}$ at the iteration $n$, then
\begin{equation}
\hat{i}_{n,fDIME}(X;Y)  = \log \biggl(\bigl(f^{*}\bigr)^{\prime}\bigl(T^{(n)}(\mathbf{x},\mathbf{y}) \bigr) \biggr) = \log \biggl(R^{(n)}(\mathbf{x},\mathbf{y}) \biggr),
\end{equation}
where $R^{(n)}(\mathbf{x},\mathbf{y})$ represents the estimated density ratio at the $n$-th iteration and it is related with the optimum ratio $\hat{R}(\mathbf{x},\mathbf{y})$ as follows
\begin{align}
\hat{R} - R^{(n)} & = \bigl(f^{*}\bigr)^{\prime}\bigl(\hat{T}\bigr) - \bigl(f^{*}\bigr)^{\prime}\bigl(T^{(n)}\bigr) \nonumber \\  
& = \bigl(f^{*}\bigr)^{\prime}\bigl(\hat{T}\bigr) - \bigl(f^{*}\bigr)^{\prime}\bigl(\hat{T}-\delta^{(n)}\bigr)  \nonumber \\
& \simeq  \delta^{(n)}\cdot\biggl[\bigl(f^{*}\bigr)^{\prime \prime}\bigl(\hat{T}-\delta^{(n)}\bigr)\biggr],
\end{align}
where the last step follows from a first order Taylor expansion in $\hat{T}-\delta^{(n)}$.
Therefore,
\begin{align}
& \hat{i}_{n,fDIME}(X;Y) = \log \bigl(R^{(n)}\bigr) \nonumber \\
 & =  \log \Biggl(\bigl(\hat{R}\bigr)\biggl(1-\delta^{(n)}\cdot \frac{\bigl(f^{*}\bigr)^{\prime \prime}\bigl(\hat{T}-\delta^{(n)}\bigr)}{\bigl(f^{*}\bigr)^{\prime}\bigl(\hat{T}\bigr)}  \biggr)\Biggr) \nonumber \\ 
& = i(X;Y) + \log \Biggl(1-\delta^{(n)}\cdot \frac{\bigl(f^{*}\bigr)^{\prime \prime}\bigl(\hat{T}-\delta^{(n)}\bigr)}{\bigl(f^{*}\bigr)^{\prime}\bigl(\hat{T}\bigr)}\Biggr).
\end{align}

If the gradient descent method converges towards the optimum solution $\hat{T}$, $\delta^{(n)} \rightarrow 0$ and 
\begin{align}
& \hat{i}_{n,fDIME}(X;Y) \simeq i(X;Y) - \delta^{(n)} \cdot \Biggl[\frac{\bigl(f^{*}\bigr)^{\prime \prime}\bigl(\hat{T}-\delta^{(n)}\bigr)}{\bigl(f^{*}\bigr)^{\prime}\bigl(\hat{T}\bigr)} \Biggr] \nonumber \\ 
& \simeq i(X;Y) - \delta^{(n)} \cdot \Biggl[\frac{\bigl(f^{*}\bigr)^{\prime \prime}\bigl(\hat{T}\bigr)}{\bigl(f^{*}\bigr)^{\prime}\bigl(\hat{T}\bigr)} \Biggr] \nonumber \\
& = i(X;Y) - \delta^{(n)} \cdot \Biggl[\frac{\mathrm{d}}{\mathrm{d}T} \log \bigl( \bigl(f^{*}\bigr)^{\prime}(T)\bigr) \biggr|_{T=\hat{T}} \Biggr],
\end{align}
where the RHS is itself a first order Taylor expansion of the instantaneous mutual information in $\hat{T}$. 
In the asymptotic limit ($n\rightarrow +\infty$), it holds also for the expected values that 
\begin{equation}
|I(X;Y)-\hat{I}_{n,fDIME}(X;Y)|\rightarrow 0.
\end{equation}
\end{proof}

\begin{lemma}[Related to Lemma~\ref{lemma:MI_Lemma2}]
Let $\hat{R} = p_{XY}(\mathbf{x},\mathbf{y})/ (p_{X}(\mathbf{x}) p_Y(\mathbf{y}))$ and assume $\text{Var}_{p_{XY}}[\log \hat{R}]$ exists. Let $p^M_{XY}$ be the empirical distribution of $M$ i.i.d. samples from $p_{XY}$ and let $\mathbb{E}_{p^M_{XY}}$ denote the sample average over $p^M_{XY}$. Then, it holds that

\begin{equation}
\text{Var}_{p_{XY}}\bigl[\mathbb{E}_{p^M_{XY}} [\log \hat{R}] \bigr] \leq \frac{ 4H^2(p_{XY},p_Xp_Y)\biggl|\biggl|\frac{p_{XY}}{p_Xp_Y}\biggr|\biggr|_{\infty}-I^2(X;Y)}{M}
\end{equation}
where $H^2$ is the Hellinger distance squared.
\end{lemma}

\begin{proof}
Consider the variance of $\hat{R}(\mathbf{x},\mathbf{y})$ when $(\mathbf{x},\mathbf{y})\sim p_{XY}(\mathbf{x},\mathbf{y})$, then
\begin{equation}
\text{Var}_{p_{XY}}[\log \hat{R}] = \mathbb{E}_{p_{XY}}\biggl[\biggl(\log \frac{p_{XY}}{p_Xp_Y}\biggr)^2\biggr] - \biggl(\mathbb{E}_{p_{XY}}\biggl[\log \frac{p_{XY}}{p_Xp_Y}\biggr]\biggr)^2.
\end{equation}
The power of the log-density ratio is upper bounded as follows (see the approach of Lemma 8.3 in \cite{Ghosal2000})
\begin{equation}
\mathbb{E}_{p_{XY}}\biggl[\biggl(\log \frac{p_{XY}}{p_Xp_Y}\biggr)^2\biggr] \leq 4H^2(p_{XY},p_Xp_Y)\biggl|\biggl|\frac{p_{XY}}{p_Xp_Y}\biggr|\biggr|_{\infty},
\end{equation}
while the mean squared is the ground-truth mutual information squared, thus
\begin{equation}
\text{Var}_{p_{XY}}[\log \hat{R}] \leq 4H^2(p_{XY},p_Xp_Y)\biggl|\biggl|\frac{p_{XY}}{p_Xp_Y}\biggr|\biggr|_{\infty} - I^2(X;Y).
\end{equation}
Finally, the variance of the mean of $M$ i.i.d. random variables yields the thesis
\begin{align}
\text{Var}_{p_{XY}}\bigl[\mathbb{E}_{p^M_{XY}} [\log \hat{R}] \bigr]  = \frac{\text{Var}_{p_{XY}}[\log \hat{R}]}{M}  \leq \frac{ 4H^2(p_{XY},p_Xp_Y)\biggl|\biggl|\frac{p_{XY}}{p_Xp_Y}\biggr|\biggr|_{\infty}-I^2(X;Y)}{M}.
\end{align}
\end{proof}

\begin{lemma}[Related to Lemma~\ref{lemma:MI_lemma4}]
Let $(\mathbf{x}_i,\mathbf{y}_i)$, $\forall i\in \{1,\dots,N\}$, be $N$ data points. Let $\mathcal{J}_{f}(T)$ be the value function in \eqref{eq:MI_discriminator_function_f}. Let $\mathcal{J}_{f}^{\pi}(T)$ and $\mathcal{J}_{f}^{\sigma}(T)$ be numerical implementations of $\mathcal{J}_{f}(T)$ using a random permutation and a random derangement of $\mathbf{y}$, respectively. Denote with $K$ the number of points $\mathbf{y}_k$, with $k \in \{1,\dots, N\}$, in the same position after the permutation (i.e., the fixed points). Then
\begin{equation}
\mathcal{J}_{f}^{\pi}(T) \leq \frac{N-K}{N} \mathcal{J}_{f}^{\sigma}(T).
%\label{eq:MI_perm_vs_derang}
\end{equation}
\end{lemma}

\begin{proof}
Define $\mathcal{J}_{f}^{\pi}(T)$ as the Monte Carlo implementation of $\mathcal{J}_{f}(T)$ when using the permutation function $\pi(\cdot)$
\begin{equation}
    \mathcal{J}_{f}^{\pi}(T) = \frac{1}{N}\sum_{i=1}^{N}{T(\mathbf{x}_i,\mathbf{y}_i}) -  \frac{1}{N}\sum_{i=1}^{N}{f^{*}\bigl(T(\mathbf{x}_i,\mathbf{y}_j})\bigr),
\end{equation}
where the pair $(\mathbf{x}_i,\mathbf{y}_j)$ is obtained via a random permutation of the elements of $\mathbf{y}$ as $j=\pi(i)$, $\forall i \in \{1,\dots,N\}$.
Since $K$ is a non-negative integer representing the number of fixed points $i=\pi(i)$, the value function can be rewritten as
\begin{equation}
    \label{eq:MI_monte-carlo-1}
    \mathcal{J}_{f}^{\pi}(T) = \frac{1}{N}\sum_{i=1}^{N}{T(\mathbf{x}_i,\mathbf{y}_i)} -  \frac{1}{N}\sum_{i=1}^{K}{f^{*}\bigl(T(\mathbf{x}_i,\mathbf{y}_i)\bigr)} -
    \frac{1}{N}\sum_{i=1}^{N-K}{f^{*}\bigl(T(\mathbf{x}_i,\mathbf{y}_{j\neq i})\bigr)},
\end{equation}
which can also be expressed as
\begin{equation}
\label{eq:MI_51}
    \mathcal{J}_{f}^{\pi}(T) = \frac{1}{N}\sum_{i=1}^{K}{T(\mathbf{x}_i,\mathbf{y}_i)} + \frac{1}{N}\sum_{i=1}^{N-K}{T(\mathbf{x}_i,\mathbf{y}_i)} -  \frac{1}{N}\sum_{i=1}^{K}{f^{*}\bigl(T(\mathbf{x}_i,\mathbf{y}_i)\bigr)} -
    \frac{1}{N}\sum_{i=1}^{N-K}{f^{*}\bigl(T(\mathbf{x}_i,\mathbf{y}_{j\neq i})\bigr)}.
\end{equation}
In \eqref{eq:MI_51} it is possible to recognize that the second and last term of the RHS constitutes the numerical implementation of $\mathcal{J}_{f}(T)$ using a derangement strategy on $N-K$ elements, so that
\begin{equation}
    \mathcal{J}_{f}^{\pi}(T) = \frac{1}{N}\sum_{i=1}^{K}{T(\mathbf{x}_i,\mathbf{y}_i)} -\frac{1}{N}\sum_{i=1}^{K}{f^{*}\bigl(T(\mathbf{x}_i,\mathbf{y}_i)\bigr)} + \frac{N-K}{N} {J}_{f}^{\sigma}(T).
\end{equation}
However, by definition of Fenchel conjugate
\begin{equation}
    \frac{1}{N}\sum_{i=1}^{K}{T(\mathbf{x}_i,\mathbf{y}_i)-f^{*}\bigl(T(\mathbf{x}_i,\mathbf{y}_i)\bigr)} \leq 0,
\end{equation}
since for $t=1$
\begin{equation}
    u-f^*(u) \leq u - (ut -f(t)) = f(1) = 0.
\end{equation}
Hence, we can conclude that
\begin{equation}
    \mathcal{J}_{f}^{\pi}(T) \leq \frac{N-K}{N} {J}_{f}^{\sigma}(T).
\end{equation}
\end{proof}


\begin{lemma}[Related to Lemma~\ref{lemma:MI_finite_variance_gaus}]
Let $\hat{R}$ be the optimal density ratio and let  $X\sim \mathcal{N}(0,\sigma_X^2)$ and $N\sim \mathcal{N}(0,\sigma_N^2)$ be uncorrelated scalar Gaussian random variables such that $Y=X+N$. Assume $\text{Var}_{p_{XY}}[\log \hat{R}]$ exists. Let $p^M_{XY}$ be the empirical distribution of $M$ i.i.d. samples from $p_{XY}$ and let $\mathbb{E}_{p^M_{XY}}$ denote the sample average over $p^M_{XY}$. Then, it holds that

\begin{equation}
%\label{eq:MI_variance_f_dime}
\text{Var}_{p_{XY}}\bigl[\mathbb{E}_{p^M_{XY}} [\log \hat{R}] \bigr] = \frac{ 1-e^{-2I(X;Y)}}{M}.
\end{equation}
\end{lemma}

\begin{proof}
From the hypothesis, the density ratio can be rewritten as $\hat{R} = p_N(y-x)/ p_Y(y)$ and the output variance is clearly equal to $\sigma_Y^2 = \sigma_X^2 + \sigma_N^2$. Notice that this is equivalent of having correlated random variables $X$ and $Y$ with correlation coefficient $\rho$, since it is enough to study the case $\sigma_X = \rho$ and $\sigma_N=\sqrt{1-\rho^2}$.

It is easy to verify via simple calculations that
\begin{align}
I(X;Y) = & \; \mathbb{E}_{p_{XY}} [\log \hat{R}] \nonumber \\
= & \log \frac{\sigma_Y}{\sigma_N} + \mathbb{E}_{p_{XY}}\biggl[ \frac{y^2}{2\sigma_Y^2} - \frac{(y-x)^2}{2\sigma_N^2}\biggr] \nonumber \\
= & \dots = \log \frac{\sigma_Y}{\sigma_N} = \frac{1}{2}\log\biggl(1+\frac{\sigma_X^2}{\sigma_N^2}\biggr)=-\frac{1}{2}\log\bigl(1-\rho^2\bigr).
\label{eq:MI_gaussians}
\end{align}
Similarly,
\begin{align}
& \text{Var}_{p_{XY}}\bigl[\log \hat{R} \bigr] = \mathbb{E}_{p_{XY}}\biggl[\biggl(\log\biggl(\frac{\sigma_Y}{\sigma_N}\biggl) +  \frac{y^2}{2\sigma_Y^2} - \frac{(y-x)^2}{2\sigma_N^2}\biggr)^2\biggr] - I^2(X;Y) \nonumber \\
& = \frac{1}{4}\mathbb{E}_{p_{XY}}\biggl[ \biggl(\frac{y-x}{\sigma_N}\biggr)^4 + \biggl(\frac{y}{\sigma_Y}\biggr)^4 -2 \biggl(\frac{y}{\sigma_Y}\biggr)^2 \biggl(\frac{y-x}{\sigma_N}\biggr)^2 \biggr] \nonumber \\
& = \dots = \text{Kurt}[Z]\biggl(\frac{1}{2}-\frac{\sigma_N^2}{2\sigma_Y^2}\biggr) - \frac{\sigma_X^2}{2\sigma_Y^2} \nonumber \\
& = \frac{\sigma_X^2}{\sigma_Y^2} = 1-\frac{\sigma_N^2}{\sigma_Y^2} = 1-e^{-2I(X;Y)}=\rho^2,
\end{align}
where the last steps use the fact that the Kurtosis of a normal distribution is $3$ and that the mutual information can be expressed as in \eqref{eq:MI_gaussians}. 
Finally, the variance of the mean of $M$ i.i.d. random variables yields the thesis
\begin{equation}
\text{Var}_{p_{XY}}\bigl[\mathbb{E}_{p^M_{XY}} [\log \hat{R}] \bigr]  =\frac{\text{Var}_{p_{XY}}[\log \hat{R}]}{M}. 
\end{equation}
If $X$ and $N$ are multivariate Gaussians with diagonal covariance matrices $\rho^2\mathbb{I}_{d\times d}$ and $(1-\rho^2) \mathbb{I}_{d\times d}$, the results for both the MI and variance in the scalar case are simply multiplied by $d$.
\end{proof}

\begin{theorem}[Related to Theorem~\ref{theorem:MI_permutationsBound}]
Let the discriminator $D(\cdot)$ be with enough capacity. Let $N$ be the batch size and $f$ be the generator of the KL divergence. Let $\mathcal{J}_{KL}^{\pi}(D)$ be defined as
\begin{equation}
\mathcal{J}_{KL}^{\pi}(D) =  \mathbb{E}_{(\mathbf{x},\mathbf{y}) \sim p_{XY}(\mathbf{x},\mathbf{y})}\biggl[\log\biggl(D\bigl(\mathbf{x},\mathbf{y}\bigr)\biggr)-f^*\biggl(\log\biggl(D\bigl(\mathbf{x},\pi(\mathbf{y})\bigr)\biggr)\biggr)\biggr].
\end{equation}
Denote with $K$ the number of indices in the same position after the permutation (i.e., the fixed points), and with $R(\mathbf{x},\mathbf{y})$ the density ratio in \eqref{eq:MI_density_ratio_1}.
Then,
\begin{equation}
%\label{eq:MI_optimal_ratio_perm_KL}
\hat{D}(\mathbf{x},\mathbf{y}) =\arg \max_D \mathcal{J}_{KL}^{\pi}(D) = \frac{NR(\mathbf{x},\mathbf{y})}{KR(\mathbf{x},\mathbf{y})+N-K}.
\end{equation}
\end{theorem}

\begin{proof}
The idea of the proof is to express $\mathcal{J}_{KL}^{\pi}(D)$ via Monte Carlo approximation, in order to rearrange fixed points, and then go back to Lebesgue integration.
The value function $\mathcal{J}_{KL}(D)$ can be written as
\begin{equation}
    \mathcal{J}_{KL}(D) =  \mathbb{E}_{(\mathbf{x},\mathbf{y}) \sim p_{XY}(\mathbf{x},\mathbf{y})}\biggl[\log\bigl(D(\mathbf{x},\mathbf{y})\bigr)\biggr]  -\mathbb{E}_{(\mathbf{x},\mathbf{y}) \sim p_{X}(\mathbf{x})p_{Y}(\mathbf{y})}\biggl[D\bigl(\mathbf{x},\mathbf{y}\bigr)\biggr]+1.
\end{equation}

Similarly to \eqref{eq:MI_monte-carlo-1}, we can express $\mathcal{J}_{KL}^{\pi}(D)$ as
\begin{equation}
\label{eq:MI_monte-carlo-KL}
    \mathcal{J}_{KL}^{\pi}(D) = \frac{1}{N}\sum_{i=1}^{N}{\log\bigl(D(\mathbf{x}_i,\mathbf{y}_i)\bigr)}-  \frac{1}{N}\sum_{i=1}^{K}{D(\mathbf{x}_i,\mathbf{y}_{i})} -  \frac{1}{N}\sum_{i=1}^{N-K}{D(\mathbf{x}_i,\mathbf{y}_{j\neq i})} +1,
\end{equation}
where $K$ is the number of fixed points of the permutation $j=\pi(i), \forall i \in \{1,\dots,N\}$.
However, when $N \to \infty$, we can use Lebesgue integration and rewrite \eqref{eq:MI_monte-carlo-KL} as
\begin{align}
    \mathcal{J}_{KL}^{\pi}(D) = &\int_{\mathbf{x}}\int_{\mathbf{y}}{\biggl(p_{XY}(\mathbf{x},\mathbf{y})\log\bigl(D(\mathbf{x},\mathbf{y})\bigr) -\frac{K}{N}p_{XY}(\mathbf{x},\mathbf{y})D(\mathbf{x},\mathbf{y})\biggr)\diff \mathbf{x} \diff \mathbf{y}} \nonumber \\ 
    & -\int_{\mathbf{x}}\int_{\mathbf{y}}{\frac{N-K}{N}p_{X}(\mathbf{x})p_Y(\mathbf{y})D(\mathbf{x},\mathbf{y})\diff \mathbf{x} \diff \mathbf{y}}+1.
\end{align}
To maximize $\mathcal{J}_{KL}^{\pi}(D)$, it is enough to take the derivative of the integrand with respect to $D$ and equate it to $0$, yielding the following equation in $D$
\begin{equation}
    \frac{p_{XY}(\mathbf{x},\mathbf{y})}{D(\mathbf{x},\mathbf{y})}-\frac{K}{N}p_{XY}(\mathbf{x},\mathbf{y}) -\frac{N-K}{N}p_{X}(\mathbf{x})p_Y(\mathbf{y}) =0.
\end{equation}
Solving for $D$ leads to the thesis 
\begin{equation}
\hat{D}(\mathbf{x},\mathbf{y}) =  \frac{NR(\mathbf{x},\mathbf{y})}{KR(\mathbf{x},\mathbf{y})+N-K},
\end{equation}
since $\mathcal{J}_{KL}^{\pi}(\hat{D})$ is a maximum being the second derivative w.r.t. $D$ a non-positive function.
\end{proof}

\begin{corollary}[Related to Corollary~\ref{corollary:MI_permutationsUpperBound}]
Let KL-DIME be the estimator obtained via iterative optimization of $\mathcal{J}_{KL}^{\pi}(D)$, using a batch of size $N$ every training step. Then,
\begin{equation}
    I_{KL-DIME}^{\pi} := \mathbb{E}_{(\mathbf{x},\mathbf{y}) \sim p_{XY}(\mathbf{x},\mathbf{y})}\biggl[ \log \biggl(\hat{D}(\mathbf{x},\mathbf{y})\biggr) \biggr] < \log(N).
\end{equation}
\end{corollary}

\begin{proof}
Theorem \ref{theorem:MI_permutationsBound} implies that, when the batch size is much larger than the density ratio ($N >> R$), then the discriminator's output converges to the density ratio. Indeed,
\begin{equation}
    \lim_{N\to \infty}{\hat{D}(\mathbf{x},\mathbf{y})} = \lim_{N\to \infty}{\frac{NR(\mathbf{x},\mathbf{y})}{KR(\mathbf{x},\mathbf{y})+N-K}} = R(\mathbf{x},\mathbf{y}).
\end{equation}

Instead, when the density ratio is much larger than the batch size ($R>>N$), then the discriminator's output converges to a constant, in particular 
\begin{equation}
    \lim_{R\to \infty}{\hat{D}(\mathbf{x},\mathbf{y})} = \lim_{R\to \infty}{\frac{NR(\mathbf{x},\mathbf{y})}{KR(\mathbf{x},\mathbf{y})+N-K}} = \frac{N}{K}.
\end{equation}
However, from Lemma \ref{lemma:MI_M=1}, it is true that $K=1$ on average. Therefore, an iterative optimization algorithm leads to an upper-bounded discriminator, since
\begin{equation}
    \hat{D}(\mathbf{x},\mathbf{y}) < N,
\end{equation}
which implies the thesis.
\end{proof}

\begin{lemma}[Related to Lemma~\ref{lemma:MI_M=1}]
The average number of fixed points in a random permutation $\pi(\cdot)$ is equal to 1.
\end{lemma}
\begin{proof}
Let $\pi(\cdot)$ be a random permutation on $\left\{ 1, \dotsc, N \right\}$. Let the random variable $X$ represent the number of fixed points (i.e., the number of cycles of length $1$) of $\pi(\cdot)$. We define $X = X_1 + X_2 + \cdots + X_N$, where $X_i = 1$ when $\pi(i)=i$, and $0$ otherwise. $\mathbb{E}[X]$ is computed by exploiting the linearity property of expectation. Trivially,
\begin{equation}
    \mathbb{E}[X_i] = \mathbb{P}[\pi(i)=i] = \frac{1}{N},
\end{equation}
which implies 
\begin{equation}
    \mathbb{E}[X] = \sum_{i=1}^N \frac{1}{N} = 1.
\end{equation}
\end{proof}

\subsection{Experimental details}
\label{subsec:mi_appendix_experiment_details}

\subsubsection{Multivariate linear and non-linear Gaussians experiments}
\label{subsec:mi_appendix_staircases}

The NN architectures implemented for the linear and cubic Gaussian experiments are: \textit{joint}, \textit{separable}, \textit{deranged}, and the architecture of NJEE, referred to as \textit{ad hoc}. 

\textbf{Joint architecture}. The \textit{joint} architecture is a feed-forward fully connected NN with an input size equal to twice the dimension of the samples distribution ($2d$), one output neuron, and two hidden layers of $256$ neurons each. The activation function utilized in each layer (except from the last one) is ReLU.
The number of realizations ($\mathbf{x},\mathbf{y}$) fed as input of the neural network for each training iteration is $N^2$, obtained as all the combinations of the samples $\mathbf{x}$ and $\mathbf{y}$ drawn from $p_{XY}(\mathbf{x}, \mathbf{y})$. 

\textbf{Separable architecture}. The \textit{separable} architecture comprises two feed-forward NNs, each one with an input size equal to $d$, output layer containing $32$ neurons and $2$ hidden layers with $256$ neurons each. The ReLU activation function is used in each layer (except from the last one).
The first network is fed in with $N$ realizations of $X$, while the second one with $N$ realizations of $Y$.

\textbf{Deranged architecture}. The \textit{deranged} architecture is a feed-forward fully connected NN with an input size equal to twice the dimension of the samples distribution ($2d$), one output neuron, and two hidden layers of $256$ neurons each. The activation function utilized in each layer (except from the last one) is ReLU. 
The number of realizations ($\mathbf{x},\mathbf{y}$) the neural network is fed with is $2N$ for each training iteration: $N$ realizations drawn from $p_{XY}(\mathbf{x}, \mathbf{y})$ and $N$ realizations drawn from $p_X(\mathbf{x})p_Y(\mathbf{y})$ using the derangement procedure described in Sec. \ref{subsec:mi_derangements}.

The architecture \textit{deranged} is not used for $I_{CPC}$ because in \eqref{eq:MI_NCE} the summation at the denominator of the argument of the logarithm would require neural network predictions corresponding to the inputs $(\mathbf{x}_i, \mathbf{y}_j), \>  \forall i,j \in \{1,\dots,N\}$ with $i \neq j$.

\textbf{Ad hoc architecture}. The \textit{NJEE} MI estimator comprises $2d-1$ feed-forward NNs. Each NN is composed by an input layer with size between $1$ and $2d-1$, an output layer containing $N-k$ neurons, with $k \in \mathbb{N}$ small, and 2 hidden layers with $256$ neurons each. The ReLU activation function is used in each layer (except from the last one).
We implemented a Pytorch \cite{Pytorch2016} version of the 
code produced by the authors of \cite{shalev2022neural}, to unify NJEE with all the other MI estimators. 

Each neural estimator is trained using Adam optimizer \cite{kingma2014adam}, with learning rate $5 \times 10^{-4}$, $\beta_1 = 0.9$, $\beta_2=0.999$. The batch size is initially set to $N=64$.\\
For the \textit{Gaussian} setting, we sample a $20$-dimensional Gaussian distribution to obtain $\mathbf{x}$ and $\mathbf{n}$ samples, independently. Then, we compute $\mathbf{y}$ as linear combination of $\mathbf{x}$ and $\mathbf{n}$: $\mathbf{y} = \rho \, \mathbf{x} + \sqrt{1-\rho^2} \, \mathbf{n}$, where $\rho$ is the correlation coefficient. For the \textit{cubic} setting, the nonlinear transformation $\mathbf{y} \mapsto \mathbf{y}^3$ is applied to the Gaussian samples.
a
During the training procedure, every $4k$ iterations, the target value of the MI is increased by $2\> nats$, for $5$ times, obtaining a target staircase with $5$ steps. The change in target MI is obtained by increasing $\rho$, that affects the true MI according to
\begin{equation}
    I(X;Y) = -\frac{d}{2}\log(1 - \rho^2).
\end{equation}

\subsubsection{Supplementary analysis of the MI estimators performance}
Additional plots reporting the MI estimates obtained from MINE, NWJ, and SMILE with $\tau = \infty$, are outlined in Fig. \ref{fig:MI_stairs_d20_bs64_MINE_NW_SMILE}. The variance attained by these algorithms exponentially increases as the true MI grows, as stated in \eqref{eq:MI_exponentially_increasing_variance}. 

 \begin{figure}
	\centering
	\includegraphics[scale=0.42]{images/mi/allStaircases_d20_bs64_MINE_NWJ_SMILE.png}
	\caption{NWJ, SMILE ($\tau=\infty$), and MINE MI estimation comparison with $d=20$ and $N=64$. The \textit{Gaussian} setting is represented in the top row, while the \textit{cubic} setting is shown in the bottom row.}
	\label{fig:MI_stairs_d20_bs64_MINE_NW_SMILE}
\end{figure} 

We report in Fig. \ref{fig:MI_SMILE_no_trick} the behavior we obtained for $I_{SMILE}$ when the training of the neural network is performed by using the cost function in \eqref{eq:MI_SMILE}. The training diverges during the first steps when $\tau=1$ and $\tau=5$. Differently, when $\tau=\infty$, $I_{SMILE}$ corresponds to $I_{MINE}$ (without the moving average improvement), therefore the MI estimate does not diverge. Interestingly, by comparing $I_{SMILE}$ ($\tau=\infty$) trained with the JS divergence and with the MINE cost function (in Fig. \ref{fig:MI_stairs_d20_bs64_MINE_NW_SMILE} and Fig. \ref{fig:MI_SMILE_no_trick}, respectively), the variance of the latter case is significantly higher. Hence, the JS maximization trick seems to have an impact in lowering the estimator variance. 

\begin{figure}[htp]

\centering
\begin{subfigure}{.3\textwidth}
\includegraphics[width=1\textwidth]{images/mi/allStaircases_no_trick_tau_1.png}%\hfill
\caption{$\tau = 1$}
\end{subfigure}
\begin{subfigure}{.3\textwidth}
\includegraphics[width=1\textwidth]{images/mi/allStaircases_no_trick_tau_5.png}%\hfill
\caption{$\tau = 5$}
\end{subfigure}
\begin{subfigure}{.3\textwidth}
\includegraphics[width=1\textwidth]{images/mi/allStaircases_no_trick_tau_inf.png}
\caption{$\tau = \infty$}
\end{subfigure}
\caption{$I_{SMILE}$ behavior for different values of $\tau$, when the JS divergence is not used to train the neural network. The \textit{Gaussian} case is reported in the top row, while the \textit{cubic} case is reported in the bottom row.}
\label{fig:MI_SMILE_no_trick}

\end{figure}

\subsubsection{Analysis for different values of $d$ and $N$}

The class of $f$-DIME estimators is robust to changes in $d$ and $N$, as the estimators' variance decreases (see \eqref{eq:MI_variance_f_dime} and Fig. \ref{fig:MI_VarianceVsBs}) when $N$ increases and their achieved bias is not significantly influenced by the choice of $d$. Differently, $I_{NJEE}$ and $I_{CPC}$ are highly affected by variations of those parameters, as described in Fig. \ref{fig:MI_stairs_d5_bs64} and Fig. \ref{fig:MI_stairs_d20_bs1024}. 
More precisely, $I_{CPC}$ is not strongly influenced by a change of $d$, but the bias significantly increases as the batch size diminishes, since the upper bound lowers.
$I_{NJEE}$ achieves a higher bias both when $d$ decreases and when $N$ increases w.r.t. the default values $d=20, N=64$. In addition, when $d$ is large, the training of $I_{NJEE}$ is not feasible, as it requires a lot of time (see Fig. \ref{fig:MI_computationalTimeAnalysisMain}) and memory (as a consequence of the large number of neural networks utilized) requirements.

We show the achieved bias, variance, and mean squared error (MSE) corresponding to the three settings reported in Fig. \ref{fig:MI_stairs}, \ref{fig:MI_stairs_d5_bs64}, and \ref{fig:MI_stairs_d20_bs1024} in Fig. \ref{fig:MI_bias_var_mse_gaussian_d20_N64}, \ref{fig:MI_bias_var_mse_gaussian_d5_N64}, and \ref{fig:MI_bias_var_mse_gaussian_d20_N1024}, respectively. The achieved variance is bounded when the estimator used is $I_{KL-DIME}$ or $I_{CPC}$. In particular, Figures \ref{fig:MI_bias_var_mse_gaussian_d20_N64}, \ref{fig:MI_bias_var_mse_gaussian_d5_N64}, \ref{fig:MI_bias_var_mse_gaussian_d20_N1024}, and \ref{fig:MI_VarianceVsBs} demonstrate that $I_{KL-DIME}$ satisfies Lemma \ref{lemma:MI_finite_variance_gaus}.

Additionally, we report the achieved bias, variance and MSE when $d=20$ and $N$ varies according to Tab. \ref{tab:MI_bias_var_mse_gauss_N_varying}. We use the notation $N = [512, 1024]$ to indicate that each cell of the table reports the values corresponding to $N=512$ and $N=1024$, with this specific order, inside the brackets.
Similarly, we show the attained bias, variance, and MSE for $d=[5, 10]$ and $N=64$ in Tab. \ref{tab:MI_bias_var_mse_gauss_d_varying}.
The achieved bias, variance and MSE shown in Tab. \ref{tab:MI_bias_var_mse_gauss_N_varying} and Tab. \ref{tab:MI_bias_var_mse_gauss_d_varying} motivate that the class of $f$-DIME estimators attains the best values for bias and MSE. Similarly, $I_{KL-DIME}$ obtains the lowest variance, when excluding $I_{CPC}$ from the estimators comparison ($I_{CPC}$ should not be desirable as it is upper bounded).
The illustrated results are obtained with the \textit{joint} architecture (except for NJEE) because, when the batch size is small, such an architecture achieves slightly better results than the \textit{deranged} one, as it approximates the expectation over the product of marginals with more samples. %$in Eq. \ref{eq:discriminator_function_f}.

The variance of the $f$-DIME estimators achieved in the Gaussian setting when $N$ ranges from $64$ to $1024$ is reported in Fig. \ref{fig:MI_VarianceVsBs}. The behavior shown in such a figure demonstrates what is stated in Lemma \ref{lemma:MI_Lemma2}, i.e., the variance of the $f$-DIME estimators varies as $\frac{1}{N}$. 

\begin{figure}
	\centering
	\includegraphics[scale=0.4]{images/mi/bias_var_mse_cubicFalse_archjoint_d20_bs64.png}
	\caption{Bias, variance, and MSE comparison between estimators, using the joint architecture for the \textit{Gaussian} case with $d=20$ and $N=64$.}
	\label{fig:MI_bias_var_mse_gaussian_d20_N64}
\end{figure} 
\begin{figure}
	\centering
	\includegraphics[scale=0.4]{images/mi/bias_var_mse_cubicFalse_archjoint_d5_bs64.png}
	\caption{Bias, variance, and MSE comparison between estimators, using the joint architecture for the \textit{Gaussian} case with $d=5$ and $N=64$.}
	\label{fig:MI_bias_var_mse_gaussian_d5_N64}
\end{figure} 
\begin{figure}
	\centering
	\includegraphics[scale=0.4]{images/mi/bias_var_mse_cubicFalse_archjoint_d20_bs1024.png}
	\caption{Bias, variance, and MSE comparison between estimators, using the joint architecture for the \textit{Gaussian} case with $d=20$ and $N=1024$.}
	\label{fig:MI_bias_var_mse_gaussian_d20_N1024}
\end{figure} 

\begin{figure}
	\centering
	\includegraphics[scale=0.29]{images/mi/VarianceVsBs.png}
	\caption{Variance of the $f$-DIME estimators corresponding to different values of batch size.}
	\label{fig:MI_VarianceVsBs}
\end{figure} 

\begin{table}
\caption{Bias (B), variance (V), and MSE (M) of the MI estimators using the joint architecture, when $d=20$ and $N=[512, 1024]$, for the Gaussian setting. Each $f$-DIME estimator is abbreviated to $f$-D.}
\setlength{\arrayrulewidth}{0.5mm}
\centering
    \begin{tabular}{ |c|c|c c c c c|c c c c c| } 
     \hline
     & & \multicolumn{5}{|c|}{Gaussian} \\
     \hline
     & MI & 2 & 4 & 6 & 8 & 10 \\
     \hline
      & NJEE & [0.42, 0.44] & [0.40, 0.42] & [0.37, 0.41] & [0.34, 0.40] & [0.32, 0.38] \\
      & SMILE & [0.25, 0.27] & [0.48, 0.51] & [0.64, 0.67] & [0.74, 0.73] & [0.86, 0.83]\\
      B & GAN-D & [0.11, 0.09] & [0.15, 0.13] & [\textbf{0.16}, \textbf{0.12}] & [\textbf{0.14}, \textbf{0.04}] & [\textbf{0.01}, \textbf{0.16}] \\
      & HD-D & [0.08, 0.07] & [0.15, 0.12] & [0.24, 0.20] & [0.37, 0.30] & [0.47, 0.43]  \\
      & KL-D & [\textbf{0.07}, 0.06] & [\textbf{0.12}, \textbf{0.10}] & [0.21, 0.17] & [0.38, 0.31] & [0.69, 0.56] \\
      & CPC & [0.08, \textbf{0.05}] & [0.34, 0.23] & [1.07, 0.80] & [2.32, 1.87] & [3.96, 3.37] \\
     \hline
     & NJEE & [0.01, 0.00] & [0.01, 0.01] & [0.02, 0.01] & [0.02, 0.01] & [0.02, 0.01] \\
      & SMILE & [0.01, 0.01] & [0.03, 0.02] & [0.06, 0.03] & [0.11, 0.07] & [0.17, 0.11]  \\
      V & GAN-D & [0.01, 0.01] & [0.03, 0.02] & [0.06, 0.04] & [0.11, 0.07] & [0.17, 0.12] \\
      & HD-D & [0.01, 0.01] & [0.03, 0.02] & [0.05, 0.04] & [0.07, 0.06] & [0.09, 0.08]  \\
      & KL-D& [0.01, 0.01] & [0.01, 0.01] & [0.02, 0.01] & [0.02, 0.01] & [0.02, 0.01] \\
      & CPC & [\textbf{0.01}, \textbf{0.00}] & [\textbf{0.01}, \textbf{0.00}] & [\textbf{0.01}, \textbf{0.00}] & [\textbf{0.00}, \textbf{0.00}] & [\textbf{0.00}, \textbf{0.00}]  \\
     \hline
     & NJEE & [0.18, 0.20] & [0.18, 0.18] & [0.16, 0.18] & [0.14, 0.17] & [\textbf{0.12}, 0.16]  \\
      & SMILE & [0.08, 0.08] & [0.26, 0.28] & [0.47, 0.48] & [0.66, 0.61] & [0.90, 0.80] \\
      M & GAN-D & [0.03, 0.02] & [0.05, 0.04] & [0.09, 0.05] & [\textbf{0.13}, \textbf{0.08}] & [0.18, \textbf{0.15}]  \\
      & HD-D & [0.02, 0.01] & [0.05, 0.04] & [0.11, 0.08] & [0.21, 0.15] & [0.31, 0.26] \\
      & KL-D & [\textbf{0.01}, \textbf{0.01}] & [\textbf{0.03}, \textbf{0.02}] & [\textbf{0.06}, \textbf{0.04}] & [0.17, 0.11] & [0.49, 0.33]  \\
      & CPC & [0.01, 0.01] & [0.13, 0.06] & [1.16, 0.64] & [5.38, 3.48] & [15.67, 11.38]  \\
     \hline
    \end{tabular}
    \label{tab:MI_bias_var_mse_gauss_N_varying}
\end{table}

\begin{table}
\caption{Bias (B), variance (V), and MSE (M) of the MI estimators using the joint architecture, when $d=[5, 10]$ and $N=[64]$, for the Gaussian setting. Each $f$-DIME estimator is abbreviated to $f$-D.} 
\setlength{\arrayrulewidth}{0.5mm}
\centering
    \begin{tabular}{ |c|c|c c c c c|c c c c c| } 
     \hline
     & & \multicolumn{5}{|c|}{Gaussian} \\
     \hline
     & MI & 2 & 4 & 6 & 8 & 10 \\
     \hline
      & NJEE & [0.30, 0.29] & [\textbf{0.03}, 0.13] & [0.46, \textbf{0.06}] & [1.23, 0.38] & [2.35, 0.80] \\
      & SMILE & [0.29, 0.24] & [0.61, 0.52] & [0.76, 0.68] & [0.85, 0.71] & [0.96, 0.68]\\
      B & GAN-D & [0.06, 0.12] & [0.09, 0.17] & [\textbf{0.14}, 0.17] & [\textbf{0.06}, \textbf{0.20}] & [\textbf{0.30}, \textbf{0.18}] \\
      & HD-D & [0.04, 0.09] & [0.09, 0.14] & [0.15, 0.22] & [0.28, 0.39] & [0.53, 0.40]  \\
      & KL-D & [\textbf{0.04}, \textbf{0.07}] & [0.09, \textbf{0.13}] & [0.19, 0.30] & [0.40, 0.58] & [0.93, 1.05]  \\
      & CPC & [0.17, 0.20] & [0.80, 0.89] & [2.10, 2.20] & [3.89, 3.93] & [5.85, 5.86]  \\
     \hline
     & NJEE & [0.04, 0.05] & [0.06, 0.08] & [0.09, 0.10] & [0.15, 0.13] & [0.27, 0.13] \\
      & SMILE & [0.06, 0.06] & [0.09, 0.13] & [0.12, 0.20] & [0.23, 0.32] & [0.46, 0.46]  \\
      V & GAN-D & [0.05, 0.06] & [0.08, 0.12] & [0.13, 0.19] & [0.24, 0.30] & [0.69, 0.52] \\
      & HD-D & [0.05, 0.06] & [0.08, 0.11] & [0.12, 0.16] & [0.20, 0.24] & [0.57, 0.49]  \\
      & KL-D & [0.04, 0.05] & [0.06, 0.08] & [0.06, 0.10] & [0.06, 0.10] & [0.06, 0.10] \\
      & CPC & [\textbf{0.03}, \textbf{0.04}] & [\textbf{0.02}, \textbf{0.03}] & [\textbf{0.01}, \textbf{0.01}] & [\textbf{0.00}, \textbf{0.00}] & [\textbf{0.00}, \textbf{0.00}]  \\
     \hline
     & NJEE & [0.13, 0.13] & [\textbf{0.06}, \textbf{0.09}] & [0.30, \textbf{0.10}] & [1.66, \textbf{0.28}] & [5.78, 0.76]  \\
      & SMILE & [0.14, 0.11] & [0.46, 0.40] & [0.70, 0.66] & [0.95, 0.83] & [1.37, 0.93] \\
      M & GAN-D & [0.06, 0.08] & [0.09, 0.15] & [0.15, 0.22] & [0.24, 0.34] & [\textbf{0.78}, \textbf{0.55}]  \\
      & HD-D & [0.05, 0.07] & [0.09, 0.13] & [0.15, 0.21] & [0.28, 0.40] & [0.86, 0.65]  \\
      & KL-D & [\textbf{0.04}, \textbf{0.06}] & [0.07, 0.10] & [\textbf{0.10}, 0.19] & [\textbf{0.22}, 0.44] & [0.92, 1.20]  \\
      & CPC & [0.06, 0.08] & [0.67, 0.83] & [4.42, 4.84] & [15.14, 15.45] & [34.22, 34.32]  \\
     \hline
    \end{tabular}
    \label{tab:MI_bias_var_mse_gauss_d_varying}
\end{table}

\begin{table}
\caption{Bias (B), variance (V), and MSE (M) of the MI estimators using the joint architecture, when $d=20$ and $N=64$, for the Gaussian setting. Each $f$-DIME estimator is abbreviated to $f$-D.} 
\setlength{\arrayrulewidth}{0.5mm}
\centering
    \begin{tabular}{ |c|c|c c c c c|c c c c c| } 
     \hline
     & & \multicolumn{5}{|c|}{Gaussian} \\
     \hline
     & MI & 2 & 4 & 6 & 8 & 10 \\
     \hline
      & NJEE & 0.29 & \textbf{0.18} & \textbf{0.01} & \textbf{0.17} & 0.37 \\
      & SMILE & 0.18 & 0.37 & 0.44 & 0.50 & 0.52\\
      B & GAN-D & 0.17 & 0.27 & 0.35 & 0.34 & \textbf{0.26} \\
      & HD-D & 0.16 & 0.28 & 0.43 & 0.61 & 0.73  \\
      & KL-D & \textbf{0.13} & 0.25 & 0.48 & 0.87 & 1.44  \\
      & CPC & 0.25 & 0.98 & 2.29 & 3.99 & 5.88  \\
     \hline
     & NJEE & 0.06 & 0.10 & 0.13 & 0.17 & 0.16 \\
      & SMILE & 0.05 & 0.11 & 0.18 & 0.30 & 0.51  \\
      V & GAN-D & 0.06 & 0.11 & 0.19 & 0.32 & 0.55 \\
      & HD-D & 0.06 & 0.11 & 0.20 & 0.29 & 0.43  \\
      & KL-D & 0.05 & 0.09 & 0.11 & 0.12 & 0.11 \\
      & CPC & \textbf{0.04} & \textbf{0.03} & \textbf{0.01} & \textbf{0.00} & \textbf{0.00}  \\
     \hline
     & NJEE & 0.14 & \textbf{0.14} & \textbf{0.13} & \textbf{0.20} & \textbf{0.30}  \\
      & SMILE & 0.09 & 0.25 & 0.38 & 0.55 & 0.79 \\
      M & GAN-D & 0.09 & 0.19 & 0.31 & 0.43 & 0.62  \\
      & HD-D & 0.09 & 0.19 & 0.39 & 0.66 & 0.96  \\
      & KL-D & \textbf{0.07} & 0.15 & 0.34 & 0.87 & 2.19  \\
      & CPC & 0.10 & 0.99& 5.25 & 15.89 & 34.57  \\
     \hline
    \end{tabular}
    \label{tab:MI_bias_var_mse_gauss_d_varying2}
\end{table}

The class $f$-DIME is able to estimate the MI for high-dimensional distributions, as shown in Fig. \ref{fig:MI_allStairs_d100_bs64}, where $d=100$. In that figure, the estimates behavior is obtained by using the simple architectures described in Sec. \ref{subsec:mi_appendix_staircases}. Thus, the input size of these NNs ($200$) is comparable with the number of neurons in the hidden layers ($256$).
Therefore, the estimates could be improved by increasing the number of hidden layers and neurons per layer. The graphs in Fig. \ref{fig:MI_allStairs_d100_bs64} illustrate the advantage of the architecture \textit{deranged} over the \textit{separable} one.

\subsubsection{Considerations on derangements}
\label{subsec:mi_derangement_considerations}
To facilitate the understanding of the role of derangements during training, we provide a practical example in the following.
 
Suppose for simplicity that $N=3$. Then, a random permutation of $\mathbf{y} = [y_1, y_2, y_3]$ can be $[y_2, y_3, y_1]$, where the number of fixed points is $K=0$ as no elements remain in the same position after the permutation.
However, another permutation of $\mathbf{y}$ is $[y_1, y_3, y_2]$. In this case, it is evident that $y_1$ remains in the same initial position, and the number of fixed points is $K=1$.
A random derangement of $\mathbf{y} = [y_1, y_2, y_3]$, instead, ensures by definition that no element of $\mathbf{y}$ ends up in the same initial position, contrarily from a naive random permutation. This idea is essential to avoid having shuffled marginal samples that actually are realizations of the joint distribution.
In fact, we proved that a random permutation strategy would lead to a biased estimator (see the permutation bound in Corollary \ref{corollary:MI_permutationsUpperBound}).
It is extremely important to remark that the derangement sampling strategy it is not only applicable to $f$-divergence based estimators, rather, any discriminative variational estimator should use it to avoid upper bound MI estimates, as it can be observed from Fig. \ref{fig:MI_derangementVsPermutation_beyond}
\begin{figure}
	\centering
	\includegraphics[scale=0.24]{images/mi/FigureA_allStaircases_d20_bs128_arcderanged_deranged_vs_permutation.pdf}
	\caption{MI estimates when $d=20$ and $N=128$, top row: derangement strategy; bottom row: permutation strategy.}
	\label{fig:MI_derangementVsPermutation_beyond}
\end{figure} 
 
\begin{figure}
	\centering
	\includegraphics[scale=0.28]{images/mi/allStaircases_d100_bs64.png}
	\caption{MI estimates when $d=100$ and $N=64$. The \textit{Gaussian} setting is represented in the top row, while the \textit{cubic} setting is shown in the bottom row.}
	\label{fig:MI_allStairs_d100_bs64}
\end{figure} 

\subsubsection{Time complexity analysis}
\label{subsubsec:mi_Appendix_time_analysis}

\begin{figure}
\centering
\begin{subfigure}{.5\textwidth}
 \centering
	\includegraphics[scale=0.4]{images/mi/fixed_d20_varying_bs_separable_vs_deranged.png}
	\caption{5-step staircases for the \textit{separable}, random-based \textit{deranged}, and shift-based \textit{deranged}.}
	\label{fig:MI_SeparableVsCombined_time_analysis}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
\centering
	\includegraphics[scale=0.4]{images/mi/FigureD_fixed_d15_varying_bs.pdf}
	\caption{\textit{deranged}, \textit{separable} and \textit{joint} architectures complexity in linear scale, varying the batch size.}
	\label{fig:MI_computationalTimeAnalysisMain_beyond}
\end{subfigure}
\caption{Time comparison between sampling strategies.}
\label{fig:MI_extra_time_appendix}
\end{figure}

The computational time analysis is developed on a server with CPU "AMD Ryzen Threadripper 3960X 24-Core Processor" and GPU "MSI GeForce RTX 3090 Gaming X Trio 24G, 24GB GDDR6X".

Before analyzing the time requirements to complete the $5$-step MI staircases, we specify two different ways to implement the derangement of the $\mathbf{y}$ realizations in each batch:
\begin{itemize}
    \item \textbf{Random-based}. The trivial way to achieve the derangement is to randomly shuffle the elements of the batch until there are no fixed points (i.e., all the $\mathbf{y}$ realizations in the batch are assigned to a different position w.r.t. the starting location).
    \item \textbf{Shift-based}. Given $N$ realizations $(\mathbf{x}_i, \mathbf{y}_i)$ drawn from $p_{XY}(\mathbf{x},\mathbf{y})$, for $i \in \left\{ 1, \dotsc, N \right\}$, we obtain the deranged samples as $(\mathbf{x}_i, \mathbf{y}_{(i+1)\%N})$, where "\%" is the modulo operator. 
\end{itemize}
Although the MI estimates obtained by the two derangement methods are almost indistinguishable, all the results shown in the section are achieved by using the random-based method. We additionally demonstrate the time efficiency of the shift-based approach. 

We show in Fig. \ref{fig:MI_computationalTimeAnalysisMain} that the architectures \textit{deranged} and \textit{separable} are significantly faster w.r.t. \textit{joint} and NJEE ones, for a given batch size $N$ and input distribution size $d$.
However, Fig. \ref{fig:MI_computationalTimeAnalysisMain} exhibits no difference between the \textit{deranged} and \textit{separable} architectures. 
Fig. \ref{fig:MI_SeparableVsCombined_time_analysis} illustrates a detailed representation of the time requirements of these two architectures to complete the $5$-step stairs presented in Sec. \ref{sec:mi_results}.
As $N$ increases, the gap between the time needed by the architectures \textit{deranged} and \textit{separable} grows, demonstrating that the former is the fastest. For example, when $d=20$ and $N=30k$, $I_{GAN-DIME}$ needs about $55$ minutes when using the architecture \textit{separable}, but only $15$ minutes when using the \textit{deranged} one and less than $9$ minutes for the shift-based \textit{deranged} architecture.
In fact, if we plot the data in a linear scale (see Fig. \ref{fig:MI_computationalTimeAnalysisMain_beyond}), it is immediate to see that the computational requirements are $\Omega(N)$ for the deranged and separable architectures, and $\Omega(N^2)$ for the joint architecture. 

\subsubsection{Summary of the estimators}
\label{subsubsec:mi_appendix_final_considerations}

We give an insight on how to choose the best estimator in Tab. \ref{tab:MI_summary_estimators}, depending on the desired specifics. We assign qualitative grades to each estimator over different performance indicators.
All the indicators names are self-explanatory, except from \textit{scalability}, which describes the capability of the estimator to obtain precise estimates when $d$ and $N$ vary from the default values ($d=20$ and $N=64$). The grades ranking is, from highest to lowest: \cmark\cmark, \cmark, $\sim$, \xmark. When more than one architecture is available for a specific estimator, the grade is assigned by considering the best architecture within that particular case.
Even though the estimator choice could depend on the specific problem, we consider $I_{GAN-DIME}$ to be the best one. The rationale behind this decision is that $I_{GAN-DIME}$ achieves the best performance for almost all the indicators and lacks weaknesses. Differently, $I_{CPC}$ estimate is upper bounded, $I_{SMILE}$ achieves slightly higher bias, and $I_{NJEE}$ is strongly $d$ and $N$ dependent. However, if the considered problem requires the estimation of a low-valued MI, $I_{KL-DIME}$ is slightly more accurate than $I_{GAN-DIME}$.  

One limitation of this contribution is that the set of $f$-divergences analyzed is restricted to three elements. Thus, probably there exists a more effective $f$-divergence which is not analyzed here. 

\begin{table}
	\centering
	\caption{Summary of the MI estimators.}
	\begin{tabular}{c||c|c|c|c|c} 
        \toprule
		\multirow{2}{*}{\textbf{Estimator}} & \multicolumn{2}{c|}{\textbf{Low MI}} & \multicolumn{2}{c|}{\textbf{High MI}} & \multirow{2}{*}{\textbf{Scalability}} \\
		\cmidrule{2-5} 
		 & \textbf{Bias} 	& \textbf{Variance} & \textbf{Bias}	& \textbf{Variance} \\
		\midrule
		$I_{KL-DIME}$ & \cmark\cmark & \cmark\cmark & $\sim$ & \cmark\cmark & \cmark\cmark \\ 
        $I_{HD-DIME}$ & \cmark\cmark & \cmark\cmark & \cmark & \cmark & \cmark\cmark \\ 
        $I_{GAN-DIME}$ & \cmark\cmark & \cmark\cmark & \cmark\cmark & \cmark & \cmark\cmark \\ 
        \midrule
        $I_{SMILE} (\tau=1)$ & \cmark & \cmark\cmark & \cmark & \cmark & \cmark\cmark \\ 
        $I_{NJEE}$ & \cmark & \cmark\cmark & \cmark & \cmark\cmark & \xmark \\ 
        $I_{CPC}$ & $\sim$ & \cmark\cmark & \xmark& \cmark\cmark & \xmark \\ 
        \midrule
        $I_{SMILE} (\tau=\infty)$ & \cmark & $\sim$ & \cmark & \xmark & \cmark\cmark \\ 
        $I_{MINE}$ & \cmark & \xmark & \xmark& \xmark & \cmark\cmark \\ 
        $I_{NWJ}$ & \cmark & \xmark & \xmark& \xmark & \cmark\cmark \\ 
		\midrule
		%\bottomrule	
	\end{tabular}
	\label{tab:MI_summary_estimators}
\end{table}

\subsubsection{Self-consistency tests}
\label{subsec:mi_appendix_consistency_tests}
The benchmark considered for the self-consistency tests is similar to the one applied in prior work \cite{Song2020}. We use the images collected in MNIST \cite{deng2012mnist} and FashionMNIST \cite{xiao2017fashion} data sets. Here, we test three properties of MI estimators over images distributions, where the MI is not known, but the estimators consistency can be tested:
\begin{enumerate}
    \item \textbf{Baseline}. $X$ is an image, $Y$ is the same image masked in such a way to show only the top $t$ rows. The value of $\hat{I}(X;Y)$ should be non-decreasing in $t$, and for $t=0$ the estimate should be equal to 0, since $X$ and $Y$ would be independent. Thus, the ratio $\hat{I}(X;Y)/\hat{I}(X;X)$ should be monotonically increasing, starting from $0$ and converging to $1$.  
    \item \textbf{Data-processing}. $\bar{X}$ is a pair of identical images, $\bar{Y}$ is a pair containing the same images masked with two different values of $t$. We set $h(Y)$ to be an additional masking of $Y$ of $3$ rows. The estimated MI should satisfy $\hat{I}([X,X];[Y, h(Y)])/\hat{I}(X;Y) \approx 1$, since including further processing should not add information.  
    \item \textbf{Additivity}. $\bar{X}$ is a pair of two independent images, $\bar{Y}$ is a pair containing the masked versions (with equal $t$ values) of those images. The estimated MI should satisfy $\hat{I}([X_1,X_2];[Y_1, Y_2])/\hat{I}(X;Y) \approx 2$, since the realizations of the $X$ and $Y$ random variables are drawn independently. 
\end{enumerate}
These tests are developed for $I_{fDIME}$, $I_{CPC}$, and $I_{SMILE}$. Differently, $I_{NJEE}$ training is not feasible, since by construction $2d-1$ models should be created, with $d=784$ (the gray-scale image shape is $28 \times 28$ pixels).
The neural network architecture used for these tests is referred to as \textbf{conv}.

\textbf{Conv}. It is composed by two convolutional layers and one fully connected. The first convolutional layer has $64$ output channels and convolves the input images with $(5 \times 5)$ kernels, stride $2 \> px$ and padding $2 \> px$. The second convolutional layer has $128$ output channels, kernels of shape $(5 \times 5)$, stride $2 \> px$ and padding $2 \> px$. The fully connected layer contains $1024$ neurons. ReLU activation functions are used in each layer (except from the last one). The input data are concatenated along the channel dimension.  We set the batch size equal to $256$.

The comparison between the MI estimators for varying values of $t$ is reported in Fig. \ref{fig:MI_baselines}, \ref{fig:MI_data processings}, and \ref{fig:MI_additivities}. The behavior of all the estimators is evaluated for various random seeds. These results highlight that almost all the analyzed estimators satisfy the first two tests ($I_{HD-DIME}$ is slightly unstable), while none of them is capable of fulfilling the additivity criterion. Nevertheless, this does not exclude the existence of an $f$-divergence capable to satisfy all the tests. 
\begin{figure}
\centering
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[scale=0.4]{images/mi/bs_mask_True.png}
  \caption{Baseline property, MNIST digits data set.}
  \label{fig:MI_baseline_digits}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[scale=0.4]{images/mi/bs_mask_False.png}
  \caption{Baseline property, FashionMNIST data set.}
  \label{fig:MI_baseline_fashion}
\end{subfigure}
\caption{Comparison between different estimators for the baseline property, using MNIST data set on the left and FashionMNIST on the right.}
\label{fig:MI_baselines}
\end{figure}

\begin{figure}
\centering
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[scale=0.4]{images/mi/dp_mask_True.png}
  \caption{Data processing property, MNIST digits data set.}
  \label{fig:MI_data_processing_digits}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[scale=0.4]{images/mi/dp_mask_False.png}
  \caption{Data processing property, FashionMNIST data set.}
  \label{fig:MI_data_processing_fashion}
\end{subfigure}
\caption{Comparison between different estimators for the data processing property, using MNIST data set on the left and FashionMNIST on the right.}
\label{fig:MI_data processings}
\end{figure}

\begin{figure}
\centering
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[scale=0.4]{images/mi/ad_mask_True.png}
  \caption{Additivity property, MNIST digits data set.}
  \label{fig:MI_additivity_digits}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[scale=0.4]{images/mi/ad_mask_False.png}
  \caption{Additivity property, FashionMNIST data set.}
  \label{fig:MI_additivity_fashion}
\end{subfigure}
\caption{Comparison between different estimators for the additivity property, using MNIST data set on the left and FashionMNIST on the right.}
\label{fig:MI_additivities}
\end{figure}