\chapter{End-to-end Optimal Communications Schemes with Autoencoders} % Modeling PHY layer with Machine Learning techniques
\chaptermark{Autoencoders for communications}
%\thispagestyle{empty}
\label{sec:autoencoders}
The autoencoder (AE) concept has fostered the reinterpretation and the design of modern communication systems. It consists of an encoder, a channel, and a decoder block which modify their internal neural structure in an end-to-end learning fashion.

However, the current approach to train an AE relies on the use of the cross-entropy loss function. This approach can be prone to overfitting issues and often fails to learn an optimal system and signal representation (code). In addition, less is known about the AE ability to design channel capacity-approaching codes, i.e., codes that maximize the input-output MI under a certain power constraint. The task being even more formidable for an unknown channel for which the capacity is unknown and therefore it has to be learnt. 

In this chapter, we address the challenge of designing capacity-approaching codes by incorporating the presence of the communication channel into a novel loss function for the AE training. In particular, we exploit the MI between the transmitted and received signals as a regularization term in the cross-entropy loss function, with the aim of controlling the amount of information stored. By jointly maximizing the MI and minimizing the cross-entropy, we propose a theoretical approach that a) computes an estimate of the channel capacity and b) constructs an optimal coded signal approaching it. 
Theoretical considerations are made on the choice of the cost function and the ability of the proposed architecture to mitigate the overfitting problem.
Simulation results offer an initial evidence of the potentiality of the proposed method.

The results presented in this chapter are documented in \cite{Letizia2021}.

\section{Introduction}
\sectionmark{Introduction}
\label{sec:autoencoders_related}

Communication systems have reached a high degree of performance, meeting demanding requirements in numerous application fields due to the ability to cope with real-world effects exploiting various accomplished physical system models. Reliable transmission in a communication medium has been investigated in the milestone work of C. Shannon \cite{Shannon1948} who suggested to represent the communication system as a chain of fundamental blocks, i.d., the transmitter, the channel and the receiver. Each block is mathematically modeled in a bottom-up fashion, so that the full system results mathematically tractable. 

On the contrary, ML algorithms take advantage of the ability to work with and develop top-down models. In particular, the communication chain has been reinterpreted as an AE-based system \cite{Oshea2017}.
The AE can be trained end-to-end such that the block-error rate (BLER) of the full system is minimized. 
This idea pioneered a number of related works aimed at showing the potentiality of deep learning methods applied to wireless communications \cite{Dorner2018, TurboAE, Alberge2019, OFDM_AE}. In \cite{Dorner2018}, communication over-the-air has been proved possible without the need of any conventional signal processing block, achieving competitive bit error rates w.r.t. to classical approaches. Turbo AEs \cite{TurboAE} reached state-of-the-art performance with capacity-approaching codes at a low SNR. These methods rely on the a-priori channel knowledge (most of the time a Gaussian noise intermediate layer is assumed) and they fail to scale when the channel is unknown. To model the intermediate layers representing the channel, one approach is GANs as explained in Sec. \ref{sec:gan_ch_synthesis}. In this way, the generator implicitly learns the channel distribution $p_Y(y|x)$, resulting in a differentiable network which can be jointly trained in the AE model \cite{OsheaGAN, RighiniLetizia2019, Letizia2019a}. A recent work in \cite{Alberge2019} considered an AWGN channel with additive radar interference and demonstrated the AEs ability to produce optimal constellations in regions where no optimal solutions were available, outperforming standard configurations. 

None of the aforementioned methods explicitly considered the information rate in the cost function. In this direction, the work in \cite{Hoydis2019} included the information rate in the formulation and leveraged AEs to jointly perform geometric and probabilistic signal constellation shaping. Labeling schemes for the learned constellations have been discussed in \cite{Cammerer2020}, where the authors introduced the bit-wise AE. If the channel model is not available, the encoder can be independently trained to learn and maximize the MI between the input and output channel samples, as presented in \cite{Wunder2019}. But therein the decoder is independently designed from the encoder and it does not necessarily grant error-free decoding. Indeed, the decoding stage may not have enough capacity to learn the demapping scheme nor converge during training, especially for large networks. Therefore, the encoder and decoder learning process shall be done \textit{jointly}. In addition, the cost function used to train the AE shall be appropriately chosen. With this goal in mind, let us firstly look into the historical developments and progresses made in the ML field, strictly related to the challenge considered in this section.

The AE was firstly introduced as a non-linear principle component analysis method, exploiting neural networks \cite{Kramer1991}. Indeed, the original network contained an internal bottleneck layer which forced the AE to develop a compact and efficient representation of the input data, in an unsupervised manner.
Several extensions have been further investigated, such as the denoising autoencoder (DAE) \cite{Vincent2008}, trained to reconstruct corrupted input data, the contractive autoencoder (CAE) \cite{Rifai2011}, which attempts to find a simple encoding and decoding function, and the $k$-sparse AE \cite{Makhzani2013} in which only $k$ intermediate nodes are kept active.

However, all of them are particular forms of \textit{regularized} AEs. Regularization is often introduced as a penalty term in the cost function and it discourages complex and extremely detailed models that would poorly generalize on unseen data. In this context, the information bottleneck Lagrangian \cite{Tishby1999} was used as a regularization term to study the sufficiency (fidelity) and minimality (complexity) of the internal representation \cite{Alemi2017,Soatto2018}. 

In the context of communication systems design, regularization in the loss function has not been introduced yet. In addition, the decoding task is usually performed as a classification task. In ML applications, classification is usually carried out by exploiting a final softmax layer together with the categorical cross-entropy loss function. The softmax layer provides a probabilistic interpretation of each possible bits sequence so that the cross-entropy measures the dissimilarity between the reference and the predicted sequence of bits distribution, $p(s)$ and $q(\hat{s})$, respectively.
Nevertheless, training a classifier via cross-entropy suffers from the following problems: firstly, it does not guarantee any optimal latent representation. Secondly, it is prone to overfitting issues, especially in the case of large networks, thus, long codes design. 
Lastly, in the particular case of AEs for communications, the fundamental trade-off between the rate of transmission and reliability, namely, the channel capacity $C$, is not explicitly considered in the learning phase. 

These observations made us rethinking the problem by formulating the two following questions:
\begin{itemize}
\item[a)] Given a power constraint, is it possible to design capacity-approaching codes exploiting the principle of AEs?
\item[b)] Given a power constraint, is it possible to estimate channel capacity with the use of an AE?
\end{itemize}
The two questions are inter-related and the answer of the first one provides an answer to the second one in a constructive way, since if such a code is obtained, then the distribution of the input signal that maximizes the mutual information is also determined, and consequentially the channel capacity can also be obtained.

Inspired by the information bottleneck method \cite{Tishby1999} and by the notion of channel capacity, a novel loss function for AEs in communications is proposed in this chapter. The amount of information stored in the latent representation is controlled by a regularization term estimated using the recently introduced mutual information neural estimator (MINE) \cite{Mine2018}, enabling the theoretical design of nearly optimal codes. 
The influence of the channel appears in the \textit{end-to-end learning} phase in terms of mutual information. 

More specifically, the contributions to this topic are the following:
\begin{itemize}
\item A new loss function is proposed. It enables a new signal constellation shaping method.
\item Channel coding is obtained by \textit{jointly} minimizing the cross-entropy between the input and decoded message, and maximizing the mutual information between the transmitted and received signals.
\item A regularization term $\beta$ controls the amount of information stored in the symbols for a fixed message alphabet dimension $M$ and a fixed rate $R<C$, playing as a trade-off parameter between error-free decoding ability and maximal information transfer via coding. The NN architecture is referred to as \textit{rate-driven autoencoder}.
\item In addition, the label smoothing regularization technique is used during the AE learning process. An entropy description of the predicted messages is discussed and illustrated. 
\item By including the mutual information, we propose a new theoretical iterative scheme to built capacity-approaching codes of length $n$ and rate $R$ and consequently estimate channel capacity. This yields a scheme referred to as \textit{capacity-driven autoencoder}.
\item With the notion of explainable ML in mind, the rationale for the proposed metric and methodology is discussed in more fundamental terms following a) the concept of confidence penalty, b) the information bottleneck method \cite{Tishby1999} and c) by discussing the cross-entropy decomposition.  
\end{itemize} 

% Problems of cross-entropy, overfitting, no explicit channel, information bottleneck, lack of representiviness, only remembers the labels, problem becomes more evident for larger deep networks. --> need to include the probabilistic component --> mutual information. 

\section{Autoencoders for physical layer communications}
\sectionmark{Autoencoders base}
\label{sec:autoencoders_base}
The communication chain can be divided into three fundamental blocks: the transmitter, the channel, and the receiver. The transmitter attempts to communicate a message $s\in \mathcal{M} = \{1,2,\dots,M\}$. To do so, it transmits $n$ complex baseband symbols $\mathbf{x}\in \mathbb{C}^{n}$ at a rate $R=(\log_2 M)/n$ (bits per channel use) over the channel, under a power constraint. In general, the channel modifies $\mathbf{x}$ into a distorted and noisy version $\mathbf{y}$. The receiver takes as input $\mathbf{y}$ and produces an estimate $\hat{s}$ of the original message $s$. From an analytic point of view, the transmitter applies a transformation $f:\mathcal{M} \to \mathbb{C}^{n}$, $\mathbf{x}=f(s)$ where $f$ is referred to as the \textit{encoder}. The channel is described in probabilistic terms by the conditional transition probability density function $p_Y(y|x)$. The receiver, instead, applies an inverse transformation $g: \mathbb{C}^{n} \to \mathcal{M}$, $\hat{s}= g(\mathbf{y})$ where $g$ is referred to as the \textit{decoder}. Such communication scheme can be interpreted as an AE which learns internal robust representations $\mathbf{x}$ of the messages $s$ in order to reconstruct $s$ from the perturbed channel output samples $\mathbf{y}$ \cite{Oshea2017}. 

The AE is a deep NN trained end-to-end using stochastic gradient descent (SGD). The encoder block $f(s;\theta_E)$ maps $s$ into $\mathbf{x}$ and consists of an embedding layer followed by a feedforward NN with parameters $\theta_E$ and a normalization layer to fulfill a given power constraint. The channel is identified with a set of layers; a canonical example is the AWGN channel, a Gaussian noise layer which generates $y_i = x_i+w_i$ with $w_i\sim \mathcal{CN}(0,\sigma^2), i=1,\dots,n$. The decoder block $g(\mathbf{y};\theta_D)$ maps the received channel samples $\mathbf{y}$ into the estimate $\hat{s}$ by building the empirical probability mass function $p_{\hat{S}|Y}(\hat{s}|y;\theta_D)$. It consists of a feedforward NN, with parameters $\theta_D$, followed by a softmax layer which outputs a probability vector of dimension $M$ that assigns a probability to each of the possible $M$ messages. The encoder and decoder parameters $(\theta_E, \theta_D)$ are jointly optimized during the training process with the objective to minimize the categorical cross-entropy loss function
\begin{equation}
\label{eq:AE_CE}
\mathcal{L}(\theta_E, \theta_D) = \mathbb{E}_{(s,y)\sim p_{\hat{S}Y}(\hat{s},y)}[-\log(p_{\hat{S}|Y}(\hat{s}|y;\theta_D))],
\end{equation}
where $\mathbf{y}$ explicitly depends on the encoding block $\mathbf{x} = f(s;\theta_E)$, and thus, on the parameters $\theta_E$, while the decoder $g(\mathbf{y};\theta_D)$ calculates the probability mass function $p_{\hat{S}|Y}(\hat{s}|y;\theta_D)$.
The performance of the AE-based system is typically measured in terms of BER or BLER
\begin{equation}
\label{eq:AE_BLER}
P_e =  {P[\hat{s}\neq s]}.
\end{equation}

\section{Rate-driven autoencoders}
\sectionmark{Rate-driven autoencoders}
\label{sec:cacao}
The cross-entropy loss function does not guarantee any optimality in the code design and it is often prone to overfitting issues \cite{Tishby2017, Soatto2018}. In addition and most importantly, optimal system performance is measured in terms of achievable rates, thus, in terms of MI $I(X;Y)$ between the transmitted $\mathbf{x}$ and the received signals $\mathbf{y}$, as defined in \eqref{eq:fundamentals_MI}.
In communications, the trade-off between the rate of transmission and reliability is expressed in terms of channel capacity. For a memory-less channel, the capacity is defined as
\begin{equation}
\label{eq:AE_capacity}
C = \max_{p_X(x)} I(X;Y),
\end{equation}
where $p_X(x)$ is the input signal probability density function. Finding the channel capacity $C$ is at least as complicated as evaluating the MI. As a direct consequence, building capacity-approaching codes is a formidable task. 

Given a certain power constraint and rate $R$, the AE-based system, that is trained to minimize the cross-entropy loss function, is able, if large enough, to automatically build nearly zero-error codes. Nevertheless, there exists a code at the same rate that exhibits better performance and may even exist a higher rate error-free code (asymptotically). Therefore, the AE does not provide a capacity-achieving code. In other words, conventional autoencoding approaches, through cross-entropy minimization, allow to obtain excellent decoding schemes. Nevertheless, no guarantee to find an optimal encoding scheme is given, especially in deep NNs where problems such as vanishing and exploding gradients occur \cite{279181}. 
Hence, the starting point to design capacity-approaching codes is to redefine the loss function used by the AE.
In detail, we propose to include the MI quantity as a regularization term. The proposed loss function reads as follows

\begin{equation}
\label{eq:AE_CE_MI}
\hat{\mathcal{L}}(\theta_E, \theta_D) = \mathbb{E}_{(s,y)\sim p_{\hat{S}Y}(\hat{s},y)}[-\log(p_{\hat{S}|Y}(\hat{s}|y;\theta_D))]-\beta I(X;Y).
\end{equation}

The loss function in \eqref{eq:AE_CE_MI} forces the AE to jointly modify the network parameters $(\theta_E,\theta_D)$. The decoder reconstructs the original message $s$ with lowest possible error probability $P_e$, while the encoder finds the optimal input signal distribution $p_X(x)$ which maximizes $I(X;Y)$, for a given rate $R$ and code length $n$ and for a certain power constraint. We will denote such type of trained AE as \textit{rate-driven} AE.
It should be noted that such a NN architecture does not necessarily provide an optimal code capacity-wise, since we set a target rate which does not correspond to channel capacity. To solve this second objective, in Sec. \ref{sec:autoencoders_capacity} we will describe a theoretical methodology leading to a new scheme that we name \textit{capacity-driven} autoencoder.

To compute the MI $I(X;Y)$, we can exploit recent results such as MINE \cite{Mine2018}, as discussed below.

\subsection{Mutual information estimation}
\label{subsec:autoencoders_mutual_information_estimation}
The difficulty in computing $I(X;Y)$ resides in its dependence on the joint PDF $p_{XY}(x,y)$, which is usually unknown. Common approaches to estimate the MI rely on binning, density and kernel estimation \cite{Moon1995}, $k$-nearest neighbours \cite{Kraskov2004}, $f$-divergence functionals \cite{Nguyen2010}, and variational lower bounds (VLBs) \cite{Poole2019}. 

Recently, the MINE estimator \cite{Mine2018} proposed a NN-based method to estimate the MI based on the Donsker-Varadhan dual representation \cite{Donsker1983} of the KL divergence, in particular 
\begin{equation}
D_{\text{KL}}(p||q) = \sup_{T:\Omega \to \mathbb{R}} \mathbb{E}_{x\sim p(x)}[T(x)]-\log(\mathbb{E}_{x\sim q(x)}[e^{T(x)}]),
\end{equation}
where the supremum is taken over all functions $T$ such that the expectations are finite. Indeed, by parametrizing a family of functions $T_{\theta} : \mathcal{X}\times \mathcal{Y} \to \mathbb{R}$ with a deep NN with parameters $\theta \in \Theta$, the following bound \cite{Mine2018} holds
\begin{equation}
\label{eq:AE_lower_bound}
I(X;Y)\geq I_\theta(X;Y),
\end{equation}
where $I_{\theta}(X;Y)$ is the neural information measure  defined as
\begin{equation}
\label{eq:AE_MINE}
I_\theta(X;Y) = \sup_{\theta \in \Theta} \mathbb{E}_{(x,y)\sim p_{XY}(x,y)}[T_{\theta}(x,y)] -\log(\mathbb{E}_{(x,y)\sim p_X(x) p_Y(y)}[e^{T_{\theta}(x,y)}]).
\end{equation}
The neural information $I_\theta(X;Y)$ in \eqref{eq:AE_MINE} can be maximized using back-propagation and gradient ascent, leading to a tighter bound in \eqref{eq:AE_lower_bound}. To avoid biased gradients, the authors in \cite{Mine2018} suggested to replace the expectation in the denominator (coming from the derivative of the logarithm) with an exponential moving average. The consistency property of MINE guarantees the convergence of the estimator to the true MI value.

Estimating the MI $I(X;Y)$ is not enough to build capacity-approaching codes for a generic channel. A maximization over all possible input distribution $p_X(x)$ is also required. Therefore, to learn an optimal scheme, at each iteration the encoder needs both the cross-entropy gradient for the decoding phase and the MI gradient, from MINE, for the optimal input signal distribution. 
The proposed loss function in \eqref{eq:AE_CE_MI} (see also Fig. \ref{fig:AE_CE_MI}) shows such double role. In this way, the AE trained with the new loss function intrinsically designs codes for which the MI $I(X;Y)$ is known and maximal by construction, under the aforementioned constraints of power, rate $R$ and code-length $n$.

\begin{figure}
	\centering
	\includegraphics[scale=0.5]{images/autoencoder/CE_MI.pdf}
	\caption{Rate-driven autoencoder with the mutual information estimator block. The channel samples and the decoded message are used during the training process. The former allows to built an optimal encoding scheme exploiting the mutual information block, the latter, instead, allows to measure the decoding error through the cross-entropy loss function.}
	\label{fig:AE_CE_MI}
\end{figure} 

The rationale behind the proposed method is formally discussed in the next section.

\subsection{Mutual information regularization}
\label{sec:autoencoders_MI_reg}
The AE is a classifier network since the decoding block performs a classification task while attempting to recover the sequence of transmitted bits. The training of large AE networks may suffer from known NN degradation issues such as overfitting and vanishing gradients. In the following, we discuss in detail both problems motivating why the addition of the MI regularization term to the cross-entropy loss function, as in \eqref{eq:AE_CE_MI}, and the adoption of the label smoothing technique, offer better performance. Indeed, the cross-entropy can be minimized even for random labels as shown in \cite{Zhang2017}, leading to several overfitting issues. 
To combat overconfident predictions, the MI term plays the role of both an entropy penalty and an information bottleneck regularizer. Furthermore, its gradient directly influences the encoder parameter, mitigating the vanishing gradient problem.
  
\subsubsection{Overfitting mitigation}
\label{subsec:autoencoders_overfitting}
The objective of the AE is twofold: the encoder searches for a latent representation of the transmitted signal that is robust to channel and noise distortions while the decoder learns how to successfully recover the distorted transmitted signal. The latter, in particular, attempts to find a signal representation that provides robust performance in a more general domain than the one given by the observed/training dataset. This is rendered possible by the stochastic description of the channel. Nevertheless, fitting the empirical distribution often leads to overfitting, an undesired effect that appears when the network places most of the probability mass on a subset of the possible output classes. In other words, the decoder block produces a peaky leptokurtic conditional output distribution $p_{\hat{S}|Y}(\hat{s}|y;\theta_D)$ which may improve the training loss at the expenses of the generalization ability of the network. To avoid leptokurtic distributions, a strategy relies on entropy regularization, which penalizes confident output distributions by encouraging more entropic ones. In \cite{Pereyra2017}, the authors introduced the confidence penalty regularization term as an entropy term in the supervised learning setting:
\begin{equation}
H(p_{\hat{S}|Y}(\hat{s}|y;\theta_D)) = \sum_{i}{p_{\hat{S}|Y}(\hat{s_i}|y;\theta_D)\log(p_{\hat{S}|Y}(\hat{s_i}|y;\theta_D))},
\label{eq:AE_confidence}
\end{equation}
where $y$ and $\hat{s}$ are the input and output of the classifier, respectively. However, in the AE set up, $y$ is the output of the channel layer and is dependent itself to the training process. Indeed, the overfitting issue may occur while producing a peaky latent space distribution that does not maximize the entropy of $X$ and consequently, taking in consideration the channel layers, of $Y$. To combat such leptokurtic behavior in the encoding distribution while considering the channel dependence in the output, we propose to penalize it by exploiting the MI between the transmitted and received signals:
\begin{equation}
I(X;Y) = H(X) + H(Y) - H(X,Y),
\end{equation}
where $H(X,Y)$ is the joint entropy measure.
The objective of this penalty regularization term is to promote more entropic signal input-output distributions while reducing the joint entropy, thus, placing the joint probability mass in peaky clusters.  

The MI penalty regularizer directly influences the encoder network parameters during the learning phase. However, as mentioned before while discussing the confidence penalty in \eqref{eq:AE_confidence}, peakiness may appear also in the target output distribution $p_{\hat{S}|Y}(\hat{s}|y;\theta_D)$. To further improve the decoder generalization, we propose also to adopt the label smoothing technique \cite{Szegedy2015b}, a type of entropy regularizer particularly effective when dealing with hard target distributions such as the one-hot vectors of the AE scheme. The idea of label smoothing is to replace the target output distribution (also referred to as label distribution) $p_{\hat{S}|Y}(\hat{s}|y)=\delta_{\hat{S},S}$ with a mixture of the original ground-truth and the chosen distribution $u(\hat{s})$:
\begin{equation}
\hat{p}_{\hat{S}|Y}(\hat{s}|y)=(1-\epsilon)\delta_{\hat{S},S}+\epsilon u(\hat{s}),
\end{equation}
where $u(\hat{s})$ is chosen to be as the uniform distribution $u(\hat{s}) = 1/M$, and $\epsilon$ is a positive number. Label smoothing forces more entropic output messages $\hat{S}$ distributions, improving the decoder generalization ability. The cross-entropy loss function using label smoothing reads as follows
\begin{equation}
\hat{\mathcal{L}}(\theta_E, \theta_D) = \mathbb{E}_{(s,y)\sim \hat{p}_{\hat{S}|Y}(\hat{s}|y)\cdot p_{Y}(y|x)}[-\log(p_{\hat{S}|Y}(\hat{s}|y;\theta_D))].
\end{equation}

In Sec. \ref{subsec:autoencoders_results_overfitting}, we demonstrate how the combination of both the MI regularizer and the label smoothing technique leads to more entropic predicted output distributions.  

Another important benefit of the proposed MI penalty term consists in the mitigation of the vanishing gradient problem.

\subsubsection{Vanishing gradient mitigation}
\label{subsec:autoencoders_vanishing}
It is well known that adding layers with certain activation functions to the NN may result in small gradients of the loss function, inhibiting an effective update of the parameters of the first layers. In the case of large AE networks, the encoder block may suffer from such vanishing gradient problem, especially if the decoder block is itself a deep NN and the channel block is modeled by a multi-layer channel generator obtained via a GAN based training scheme \cite{OsheaGAN}.
Indeed, the generator in a GAN framework is typically a deep network that implicitly estimates the conditional channel distribution $p_Y(y|x)$. For this reason, the back-propagated gradient responsible for the update of the encoder parameters $\theta_E$ may be negligible when only cross-entropy is used as training cost function. Hence, a regularizer whose gradient influences directly the parameters of the encoder block can better guide the network in identifying the latent space. In particular, if the regularization term is the MI, then the encoder also attempts to find the optimal channel input distribution, tackling the achievement of channel capacity, which is exactly the aim of optimal communication schemes.

In detail, given the loss function in \eqref{eq:AE_CE_MI}
\begin{equation}
\hat{\mathcal{L}}(\theta_E, \theta_D) = \mathcal{L}(\theta_E, \theta_D) - \beta I_{\theta_E}(X;Y),
\end{equation}
and given a probabilistic channel generative model $y=h(x; \theta_h)$,
at each training iteration the gradient back-propagated from the decoder network $g(y;\theta_D)$ to the encoder network $f(s;\theta_E)$ can be computed as
\begin{equation}
\nabla_{\theta_E}\hat{\mathcal{L}}(\theta_E, \theta_D) = \nabla_{\theta_E} \mathcal{L}(\theta_E, \theta_D) - \beta \nabla_{\theta_E}I_{\theta_E}(X;Y)
\end{equation}
and using the chain rule
\begin{align}
\label{eq:AE_vanished}
\nabla_{\theta_E}\hat{\mathcal{L}}(\theta_E, \theta_D) = \; & \frac{\partial \mathcal{L}}{\partial g} \cdot \frac{\partial g}{\partial h} \cdot \frac{\partial h}{\partial f} \cdot \nabla_{\theta_E} f(s;\theta_E) \nonumber \\
& - \beta \frac{\partial I}{\partial h} \cdot \frac{\partial h}{\partial f} \cdot \nabla_{\theta_E} f(s;\theta_E) \nonumber \\ 
& - \beta \frac{\partial I}{\partial f} \cdot  \nabla_{\theta_E} f(s;\theta_E).
\end{align}
From the relations in \eqref{eq:AE_vanished}, it is clear that the regularization term with strength $\beta$ can in principle generate a more energetic gradient. Indeed, the first term in the RHS comes from the cross-entropy gradient and it depends on the decoder ($\partial g / \partial h$), while the second and third terms in the RHS contain the gradient of the MI regularizer and do not depend on the decoder.

\subsubsection{Information bottleneck method}
\label{subsec:autoencoders_IB}
In \cite{Soatto2018}, the authors proved how a deep NN can just memorize the dataset (in its weights) to minimize the cross-entropy, yielding to poor generalization. Hence, the authors proposed an information bottleneck (IB) regularization term to prevent overfitting, similarly to the IB Lagrangian, originally presented in \cite{Tishby1999}. Indeed, the IB method optimally compresses the input random variable by eliminating the irrelevant features which do not contribute to the prediction of the output random variable. 

From an AE-based communication systems point of view, let $S \rightarrow X \rightarrow Y$ be a prediction Markov chain, where $S$ represents the message to be sent, $X$ the compressed symbols and $Y$ the received symbols. The IB method solves
\begin{equation}
\label{eq:AE_IB}
\mathcal{L}(p(x|s)) = I(S;X)-\beta I(X;Y),
\end{equation}
where the positive Lagrange multiplier $\beta$ plays as a trade-off parameter between the complexity of the encoding scheme (rate) and the amount of relevant information preserved in it. 

The communication chain adds an extra Markov chain constraint $Y\rightarrow \hat{S}$, where $\hat{S}$ represents the decoded message. Therefore, in order to deal with the full AE chain, we decide to substitute the first term of the RHS in \eqref{eq:AE_IB} with the cross-entropy loss function, as presented in \eqref{eq:AE_CE_MI}. However, the Lagrange multiplier (or regularization parameter in ML terms) operates now as a trade-off parameter between the complexity to reconstruct the original message and the amount of information preserved in its compressed version.

\subsubsection{Cross-entropy decomposition}
\label{subsec:autoencoders_decomposition}
To further motivate the choice for the new loss function with the MI as the regularization term, let us consider the following decomposition of the cross-entropy loss function.

\begin{lemma}\emph{(See \cite{Hoydis2019})}
\label{lemma:autoencoders_lemma1}
Let $s \in \mathcal{M}$ be the transmitted message and let $(x,y)$ be samples drawn from the joint distribution $p_{XY}(x,y)$. If $x = f(s;\theta_E)$ is an invertible function representing the encoder and if $p_{\hat{S}|Y}(\hat{s}|y;\theta_D) = g(y;\theta_D) $ is the decoder block, then the cross-entropy function $\mathcal{L}(\theta_E,\theta_D)$ admits the following decomposition
\begin{equation}
\mathcal{L}(\theta_E,\theta_D) = H(S)-I_{\theta_E}(X;Y)+\mathbb{E}_{y\sim p_Y(y)}[D_{\text{KL}}(p_{X|Y}(x|y)||p_{\hat{S}|Y}(x|y;\theta_D))].
\end{equation}
\end{lemma}
We report a proof of the Lemma \ref{lemma:autoencoders_lemma1}, which was stated in \cite{Hoydis2019}, but herein the proof is complete and slightly different.
\begin{proof}
The cross-entropy loss function can be rewritten as follows
\begin{align}
& \mathcal{L}(\theta_E,\theta_D) = \mathbb{E}_{(x,y)\sim p_{XY}(x,y)}[-\log(p_{\hat{S}|Y}(\hat{s}|y;\theta_D))] \\ \nonumber
&= -\sum_{x,y}{p_{XY}(x,y)\log(p_{\hat{S}|Y}(\hat{s}|y;\theta_D))} \\ \nonumber
&= -\sum_{x,y}{p_{XY}(x,y)\log(p_X(x))} + \\
&+ \sum_{x,y}{p_{XY}(x,y)\log\biggl(\frac{p_X(x)}{p_{\hat{S}|Y}(\hat{s}|y;\theta_D)}\biggr)} \nonumber
\end{align}
Using the encoder hypothesis, the first term in the last expression corresponds to the source entropy $H(S)$. Therefore,
\begin{align*}
&\mathcal{L}(\theta_E,\theta_D) = H(S) + \sum_{x,y}{p_{XY}(x,y)\log\biggl(\frac{p_X(x)\cdot p_Y(y)}{p_{XY}(x,y)}\biggr)}+\\ \nonumber
&+ \sum_{x,y}{p_{XY}(x,y)\log\biggl(\frac{p_{XY}(x,y)}{p_{\hat{S}|Y}(\hat{s}|y;\theta_D)\cdot p_Y(y)}\biggr)} \\ \nonumber
&= H(S)-I(X;Y)+\sum_{x,y}{p_{XY}(x,y)\log\biggl(\frac{p_{X|Y}(x|y)}{p_{\hat{S}|Y}(\hat{s}|y;\theta_D)}\biggr)} \\ \nonumber
&= H(S)-I_{\theta_E}(X;Y)+\mathbb{E}_{y}[D_{\text{KL}}(p_{X|Y}(x|y)|| p_{\hat{S}|Y}(x|y;\theta_D))] \qedhere \nonumber
\end{align*} 
\end{proof}

The cross-entropy decomposition in Lemma \ref{lemma:autoencoders_lemma1} can be read in the following way: the first two terms are responsible for the conditional entropy of the received symbols. In the particular case of a uniform source, only the MI between the transmitted and received symbols is controlled by the encoding function during the training process. On the contrary, the last term measures the error in computing the divergence between the true posterior distribution and the decoder-approximated one. As discussed before, the network could minimize the cross-entropy just by minimizing the KL-divergence, concentrating itself only on the label information (decoding) rather than on the symbol distribution (coding). To avoid this, we propose \eqref{eq:AE_CE_MI} where the parameter $\beta$ helps in balancing the two different contributions as follows
\begin{align}
\label{eq:AE_CE_MI_decomp1}
\mathcal{L}(\theta_E,\theta_M, \theta_D) &= H(S)-I_{\theta_E}(X;Y)-\beta I_{\theta_E, \theta_M}(X;Y)+ \nonumber \\ 
&+\mathbb{E}_{y\sim p_Y(y)}[D_{\text{KL}}(p_{X|Y}(x|y)||p_{\hat{S}|Y}(x|y;\theta_D))].
\end{align}
Moreover, if the MI estimator is consistent, \eqref{eq:AE_CE_MI_decomp1} is equal to
\begin{align}
\label{eq:AE_CE_MI_decomp2}
\mathcal{L}(\theta_E, \theta_D) &= H(S)-(\beta+1)I_{\theta_E}(X;Y)+ \nonumber \\ 
&+\mathbb{E}_{y\sim p_Y(y)}[D_{\text{KL}}(p_{X|Y}(x|y)||p_{\hat{S}|Y}(x|y;\theta_D))].
\end{align}
It is immediate to notice that for $\beta<-1$, the network gets in conflict since it would try to minimize both the mutual information and the KL-divergence. Therefore, optimal values for $\beta$ lie on the semi-line $\beta>-1$.

\section{Capacity-driven autoencoders}
\label{sec:autoencoders_capacity}
Interestingly, the MI block can be exploited to obtain an estimate of the channel capacity. The AE-based system is subject to a power constraint coming from the transmitter hardware and it generally works at a fixed rate $R$ and channel uses $n$. For $R$ and $n$ fixed, the scheme discussed in Fig. \ref{fig:AE_CE_MI} optimally designs the coded signal distribution and provides an estimate $I_{\theta_M}(X;Y)$ of the MI $I(X;Y)$ which approaches $R$. However, a question remains still open: \textit{is the achieved rate with the designed code actually channel capacity}?

To find the channel capacity $C$ and determine the optimal signal distribution $p_X(x)$, a broader search on the coding rate needs to be conducted, relaxing both the constraints on $R$ and $n$. The flexibility on $R$ and $n$ requires to use different AEs. In the following, we denote with $AE(k,n)$ a rate-driven AE-based system that transmits $n$ complex symbols at a rate $R=k/n$, where $k=\log_2(M)$ and $M$ is the number of possible messages. The proposed methodology can be segmented in two phases: 
\begin{enumerate}
\item Training of a rate-driven AE $AE(k,n)$ for a fixed coding rate $R$ and channel uses $n$, enabled via the loss function in \eqref{eq:AE_CE_MI};
\item Adaptation of the coding rate $R$ to build capacity approaching codes $\mathbf{x}\sim p_X(x)$ and consequently find the channel capacity $C$.
\end{enumerate}
We remark that the capacity $C$ is the maximum data rate $R$ that can be conveyed through the channel at an arbitrarily small error probability. Therefore, the proposed algorithm makes an initial guess rate $R_0$ and smoothly increases it by playing on both $k$ and $n$. 

The basic idea is to iteratively train at the $i$-th iteration a pair of rate-driven AEs $AE^{(i)}(k_i,n_j), AE^{(i+1)}(k_{i+1},n_j)$ and evaluate both the MIs $I^{i}_{\theta_M}(X;Y), I^{(i+1)}_{\theta_M}(X;Y)$, at a fixed power constraint. The first AE works at a rate $R_i=k_i/n_j$, while the second one at $R_{i+1}=k_{i+1}/n_j$, with $R_{i+1}>R_i$. If the ratio 
\begin{equation}
\biggl|\frac{I^{(i+1)}_{\theta_M}(X;Y)-I^{(i)}_{\theta_M}(X;Y)}{I^{(i)}_{\theta_M}(X;Y)}\biggr|<\epsilon,
\end{equation}
where $\epsilon$ is an input positive parameter, the code is reaching the capacity limit for the fixed power. If the rate $R_{i+1}$ is not achievable (it does not exist a nearly error-free decoding scheme), a longer code $n_{j+1}$ is required. The algorithm in Tab.\ref{alg:autoencoders_1} describes the pseudocode that implements the channel capacity estimation and capacity-approaching code using as a building block the rate-driven AE.

\begin{algorithm}
\caption{Capacity Learning with Capacity-driven Autoencoders}
\label{alg:autoencoders_1}
\begin{algorithmic}[1]
\Inputs{$L$ SNR increasing values, $\epsilon$ threshold.}
\Initialize{$R_0 = k_0/n_0$ initial rate, $i=0, j = 0$.}
\For{$l=1$ to $L$}
	\State Train $AE^{(0)}(k_0,n_0)$;
	\State Compute $I^{(0)}_{\theta_M}(X;Y)$;
	\While{$\Delta>\epsilon$}
		\State $k_{i+1} = (R_{i}\cdot n_j) +1$;
		\State $R_{i+1}= k_{i+1}/n_j$;
		\State Train $AE^{(i+1)}(k_{i+1},n_j)$;
		\If{$R_{i+1}$ is not achievable}
			\State $n_{j+1}=n_j+1$;
			\State $j= j +1$;
		\Else
			\State Compute $I^{(i+1)}_{\theta_M}(X;Y)$;
			\State Evaluate $\Delta = \biggl|\frac{I^{(i+1)}_{\theta_M}(X;Y)-I^{(i)}_{\theta_M}(X;Y)}{I^{(i)}_{\theta_M}(X;Y)}\biggr|$;
		\EndIf
		\State $i=i+1$;
	\EndWhile
	\State $C_l = I^{(i)}_{\theta_M}(X;Y)$ estimated capacity.
\EndFor
\end{algorithmic}
\end{algorithm}

\subsubsection{Important remarks}
\label{subsec:autoencoders_remarks}
The proposed capacity-driven AE offers a constructive learning methodology to design a coding scheme that approaches capacity and to know what such a capacity is, even for channels that are unknown or for which a closed form expression for capacity does not exist. Indeed, training involves numerical procedures which may introduce some challenges. Firstly, the AE is a NN and it is well known that its performance depends on the training procedure, architecture design and hyper-parameters tuning. Secondly, the MINE block converges to the true MI mostly for a low number of samples. In practice, when $n$ is larger than $4$, the estimation often produces unreliable results, therefore, a further investigation on stable numerical estimators via NNs needs to be conducted. Lastly, the AE fails to scale with high code dimension. Indeed, for large values of $n$, the network could get stuck in local minima or, in the worst scenario, could not provide nearly zero-error codes due to limited resources or not large enough networks. The proposed approach transcends such limitations, although they have to be taken into account in the implementation phase.
In addition, the work follows the direction of explainable ML, in which the learning process is motivated by an information-theoretic approach.
Possible improvements are in defining an even tighter bound in \eqref{eq:AE_lower_bound} and in adopting different network structures (convolutional or recurrent NNs). 

It should be noted that the approach works also for non-linear channels where optimal codes have to be designed under an average power constraint and not for a given operating SNR which is appropriate for linear channels with additive noise.

\section{Numerical results}
\label{sec:autoencoders_results}
In this section, we present results obtained with the rate-driven AEs. They demonstrate an improvement in the decoding schemes (measured in terms of BLER) and show the achieved rates with respect to capacity in channels for which a closed form capacity formulation is known, such as the AWGN channel, and unknown, such as additive uniform noise and Rayleigh fading ones.

The following schemes consider an average power constraint at the transmitter side $\mathbb{E}[|\mathbf{x}|^2] = 1 $, implemented through a batch-normalization layer. Training of the end-to-end AE is performed w.r.t. to the loss function in \eqref{eq:AE_CE_MI}, implemented via a double minimization process since also the MINE block needs to be trained:
\begin{equation}
\label{eq:AE_CE_MI_final}
\min_{\theta_E,\theta_D} \min_{\theta_M} \mathbb{E}_{(s,y)\sim p_{\hat{S}Y}(\hat{s},y)}[-\log(p_{\hat{S}|Y}(\hat{s}|y;\theta_D))]-\beta I_{\theta_M}(X;Y).
\end{equation}

Furthermore, the training procedure was conducted with the same number of iterations for different values of the regularization parameter $\beta$, at a fixed value of $E_b/N_0 = 7$ dB. Unless otherwise specified, we included label smoothing with $\epsilon=0.2$ during the AEs training process.
We used Keras with TensorFlow \cite{TensorFlow} as backend to implement the proposed rate-driven AE. The code has been tested on a Windows-based operating system provided with Python 3.6, TensorFlow 1.13.1, Intel core i7-3820 CPU. 
To allow reproducible results and for clarity, the code is rendered publicly available \cite{CACAO_github}. 

\subsection{Coding-decoding capability}
\begin{figure}
	\centering
	\includegraphics[scale=0.34]{images/autoencoder/ber_4_4.pdf}
	\caption{BLER of the rate-driven AE($4,2$) for different values of the regularization $\beta$ and label smoothing parameter $\epsilon$, for an average power constraint, $k=4$ and $n=2$.}
	\label{fig:AE_Ber_4_4}
\end{figure}

As first experiment, we consider a rate-driven AE($4,2$) with rate $R=2$. The advantage of using a MI estimator block during the end-to-end training phase is expected to be more pronounced from $n>1$. To demonstrate the effective influence on the performance of the mutual information term controlled by $\beta$ in \eqref{eq:AE_CE_MI_final}, we investigate $4$ different representative values of the regularization parameter for both cases with and without label smoothing. Fig. \ref{fig:AE_Ber_4_4} illustrates the obtained BLER after the same number of training iterations when the regularization term is added in the cost function. We notice that the lowest BLER is achieved for $\beta=0.2$, therefore as expected, the MI contributes in finding better encoding schemes. Despite the small gain, the result highlights that better BLER can be obtained using the same number of iterations. As shown in \eqref{eq:AE_CE_MI_decomp2}, negative values of $\beta$ tend to force the network to just memorize the dataset, while large positive values create an unbalance. We remark that $\beta=0$ and $\epsilon=0$ corresponds to the classic AE approach proposed in \cite{Oshea2017}. Fig. \ref{fig:AE_Ber_4_4} also illustrates a slightly worse BLER when trained with label smoothing. However, we highlight the fact that label smoothing renders the network more robust to overfitting which does not necessarily reflect into higher accuracy of the model \cite{Muller2019}.
To identify optimal values of $\beta$, a possible approach can try to find the value of $\beta$ for which the two gradients (cross-entropy and mutual information) are equal in magnitude. In the following, we assume $\beta=0.2$.

\begin{figure}
	\centering
	\includegraphics[scale=0.35]{images/autoencoder/constellation.pdf}
	\caption{Constellation designed by the encoder during the end-to-end training with $\beta=0.2$ and $\epsilon=0.2$ and parameters $(k,n)$: a) ($4,1$), b) ($5,1$), c) 2 dimensional t-SNE of AE($4,2$) and d)  2 dimensional t-SNE of AE($5,2$).}
	\label{fig:AE_constellation}
\end{figure}

To assess the methodology even with higher dimension $M$ of the input alphabet, we illustrate the optimal constellation schemes when the number of possible messages is $M=16$ and $M=32$. Moreover, two cases are studied, when we transmit one complex symbol ($n=1$) and two dependent complex symbols ($n=2$) over the channel. Fig. \ref{fig:AE_constellation}a and Fig. \ref{fig:AE_constellation}b show the learned hexagonal spiral grid constellations when only one symbol is transmitted for an alphabet dimension of $M=16$ and $M=32$. Fig. \ref{fig:AE_constellation}c and Fig. \ref{fig:AE_constellation}d show, instead, an optimal projection of the coded signals in a 2D space through the learned two-dimensional t-distributed stochastic neighbor embedding
(t-SNE) \cite{vandermaaten2008visualizing}. We notice that the two pairs of constellations are similar, and therefore, even for codes of length $n=2$, the MI pushes the AE to learn the optimal signal constellation. 
As mentioned in the remarks subsection, the advantage is expected to be more pronounced the larger $n$ is. However, there is a practical limitation to obtain constructive results for higher values of $n$ since the MINE estimator is not stable. This can be solved with novel NN architectures and training algorithms, which is one of the challenges we discuss in Sec. \ref{sec:fDIME_autoencoders}.

\subsection{Entropy regularization}
\label{subsec:autoencoders_results_overfitting}
In this section, we evaluate the advantage given by the MI and label smoothing regularization techniques over the solely cross-entropy training method. We analyze the ability to generalize its applicability and the ability to mitigate the overfitting problem of the trained AE. In particular, we propose to study the distribution of the softmax output layer in the simple $4$-QAM AE($2,1$). The softmax output layer attempts to predict the output distribution $p_{\hat{S}|Y}(\hat{s}|y;\theta_D)$ and therefore provides insights about the generalization status of the network, where a smoother platikurtik output distribution is often synonym of generalization beyond the observed data \cite{Pereyra2017} while a peaky output distribution usually stands for poor generalization.

For visualization purposes, we report the analysis of the maximal output distribution $q_{Y}(y) = \max_{\hat{S}} p_{\hat{S}|Y}(\hat{s}|y)$. We consider, as an example, the behavior of the probability density function $q$ for a value of $E_b/N_0=7$ dB in three different training scenarios: only cross-entropy ($\epsilon = 0, \beta=0$), cross-entropy with label smoothing ($\epsilon = 0.2, \beta=0$), cross-entropy with mutual information and label smoothing regularizers ($\epsilon = 0.2, \beta=0.2$). From Fig. \ref{fig:AE_histograms}, we can observe that label smoothing forces a less confident prediction compared to the common cross-entropy. Interestingly, the entropic effect of the MI regularizer, which acts primarily on the encoder block, is present also at the AE output distribution even for a simple autoencoding scheme as the $4$-QAM. Indeed, for $100$ bins, the estimated entropy of $q_Y(y)$ without MI regularization is around $3.66$ Nat, while the entropy of the output with MI is around $3.72$ Nat. 

\begin{figure}
	\centering
	\includegraphics[scale=0.365]{images/autoencoder/histograms_7.pdf}
	\caption{Distribution of the maximal value of the softmax layer of AE($2,1$) for three different scenarios: only cross-entropy training, cross-entropy with label smoothing, cross-entropy with MI and label smoothing.}
	\label{fig:AE_histograms}
\end{figure}

We also report the entropy of $q_Y(y)$ varying the energy per bit to noise ratio $E_b/N_0$ for both cases with and without MI regularization. In both cases, label smoothing for a more stable estimation is used. The network should be capable to predict correct outputs outside the training region, thus, a high entropy is desired. As depicted in Fig. \ref{fig:AE_entropy}, the entropy of the maximal output when regularized with both MI and label smoothing is greater than the entropy of the maximal output when only label smoothing is used, for positive values of $E_b/N_0$. This is coherent with the intuition provided in Sec.\ref{subsec:autoencoders_overfitting}, i.e., the MI term helps in preventing overfitting. An unexpected result comes from the evolution of the estimated entropy varying the energy per bit to noise ratio: the entropy appears to have a local maximum around $0$ dB when the noise power is comparable to the signal's one. Therein, the network is essentially uniformly guessing the predicted message, without any confidence. When the noise power decreases, the network confidently places probability masses following the density in Fig. \ref{fig:AE_histograms}. However, for values of $E_b/N_0 >10$ dB, the entropies of the predictions $q_Y(y)$ start increasing again, and the rate-driven AE maintains a better generalization ability. The point of minimal entropy can be thought as a transition point from continuous (in amplitude) received signals, towards discrete valued signals. These results stimulate further analysis of entropy trends in future research work.  
\begin{figure}
	\centering
	\includegraphics[scale=0.4]{images/autoencoder/entropy.pdf}
	\caption{Estimated entropy of $q_Y(y)$ for different values of the energy per bit to noise ratio in the $4$-QAM Gaussian AE($2,1$).}
	\label{fig:AE_entropy}
\end{figure}

\subsection{Capacity-approaching codes over different channels}
The MI block inside the AE can be exploited to design capacity-approaching codes, as discussed in Sec. \ref{sec:autoencoders_capacity}. To show the potentiality of the method, we analyze the achieved rate, e.g. the MI, in three different scenarios. The first one considers the transmission over an AWGN channel, for which we know the exact closed form capacity. The second and third ones, instead, consider the transmission over an additive uniform noise channel and over a Rayleigh fading channel, for which we do not know the capacity in closed form. However, we expect the estimated MI to be a tight lower bound for the real channel capacity, especially at low SNRs.

\subsubsection{AWGN channel}
Let us consider a discrete memory-less channel with input-output relation given by (assuming complex signals)
\begin{equation}
\label{eq:AE_additive_noise}
Y_i=X_i+N_i,
\end{equation}
where the noise samples $N_i\sim \mathcal{CN}(0,\sigma^2)$ are i.i.d. and independent of $X_i$. It is well known that with a power constraint on the input signal $\mathbb{E}[|X_i|^2] \leq P$, the channel capacity is achieved by $X_i\sim \mathcal{CN}(0,P)$ and is equal to
\begin{equation}
C = \log_2(1+\text{SNR}) \; \; \text{[bits/ channel use]}.
\end{equation}

\begin{figure}
	\centering
	\includegraphics[scale=0.34]{images/autoencoder/gaussian_capacity.pdf}
	\caption{Estimated MI achieved with $\beta=0.2$ and $\epsilon=0.2$ for different dimension of the alphabet $M$ with code length $n=1$ over an AWGN channel.}
	\label{fig:AE_gaussian_capacity}
\end{figure}
The rate-driven AE attempts to maximize the MI during the training progress by modifying, at each iteration, the input distribution $p_X(x)$. Thus, given the input parameters, it produces optimal codes for which the estimation of the mutual information is provided by MINE. Fig. \ref{fig:AE_gaussian_capacity} illustrates the achieved and estimated MI when $\beta=0.2$ and $\epsilon=0.2$ for different values of the alphabet cardinality $M$. A comparison is finally made with established $M$-QAM schemes. We remark that for discrete-input signals with distribution $p_X(x)$, the MI is given by
\begin{equation}
I(X;Y) = \sum_x p_X(x)\cdot \mathbb{E}_{p_Y(y|x)}\biggl[\log \frac{p_Y(y|x)}{p_Y(y)}\biggr],
\end{equation}
and in particular with uniformly distributed symbols (only
geometric shaping), $p_X(x)=1/M$. It is found that the AE constructively provides a good estimate of MI. In addition, for the case $M=32$, the conventional QAM signal constellation is not optimal, since the AE($5,1$) performs geometric signal shaping and finds a constellation that can offer higher information rate as it is visible in the range $13-16$ dB. Lastly, if we code over two channel uses, i.e., $n=2$, an improvement in MI can be attained. This is shown in Fig. \ref{fig:AE_MI_all} where a comparison between channels affected by AWGN, uniform noise and Rayleigh fading is reported.  


\subsubsection{Additive uniform noise channel}
\begin{figure}
	\centering
	\includegraphics[scale=0.34]{images/autoencoder/additive_capacity.pdf}
	\caption{Estimated MI achieved with $\beta=0.2$ and $\epsilon=0.2$ for different dimension of the alphabet $M$ with code $n=1$ over an additive uniform noise channel.}
	\label{fig:AE_additive_capacity}
\end{figure}

\begin{figure}
	\centering
	\includegraphics[scale=0.20]{images/autoencoder/constellation_unif.pdf}
	\caption{Constellation designed by the encoder during the end-to-end training with $\beta=0.2$ and $\epsilon=0.2$, uniform noise layer and parameters $(k,n)$: a) ($4,1$), b) pairs of transmitted coded symbols AE($4,2$), c) 2 dimensional t-SNE of AE($4,2$).}
	\label{fig:AE_constellation_unif}
\end{figure}

No closed form capacity expression is known when the noise $N$ has uniform distribution $N\sim \mathcal{U}(-\frac{\Delta}{2},\frac{\Delta}{2})$ under an average power constraint. However, Shannon proved that the AWGN capacity is the lowest among all additive noise channels of the form \eqref{eq:AE_additive_noise}. Consistently, as depicted in Fig. \ref{fig:AE_additive_capacity}, it is rather interesting to notice that the estimated MI for the uniform noise channel is higher than the AWGN capacity for low SNRs until it saturates to the coding rate $R$. 
Moreover, differently from the AWGN coding signal set, Fig. \ref{fig:AE_MI_all}b also considers the complex signals generated by the encoder over two channel uses. As expected, the MI achieved by the code produced with the AE($4,2$) is higher than with AE($2,1$), consistently with the idea that $n>1$ introduces a temporal dependence in the code allowing to improve the decoding phase. In addition, Fig. \ref{fig:AE_constellation_unif}a illustrates the constellation produced by the rate-driven AE($4,1$) in the uniform noise case. Fig. \ref{fig:AE_constellation_unif}b shows how the transmitted coded symbols (transmitted complex coefficients) vary for different channel uses while Fig. \ref{fig:AE_constellation_unif}c, instead, displays the learned two-dimensional t-SNE constellation of the code produced by the AE($4,2$). 

\subsubsection{Rayleigh channel}
\begin{figure}
	\centering
	\includegraphics[scale=0.34]{images/autoencoder/rayleigh_capacity.pdf}
	\caption{Estimated MI achieved with $\beta=0.2$ and $\epsilon=0.2$ for different dimension of the alphabet $M$ with code length $n=1$ over a Rayleigh channel.}
	\label{fig:AE_rayleigh_capacity}
\end{figure}
As final experiment, we introduce fading in the communication channel, in particular we consider a Rayleigh fading channel of the form
\begin{equation}
\label{eq:AE_rayleigh_noise}
Y_i=h_iX_i+N_i,
\end{equation}
where $N_i\sim \mathcal{CN}(0,\sigma^2)$ and $h_i$ is a random variable whose amplitude $\alpha$ belongs to the Rayleigh distribution $p_R(r)$ and is independent of the signal and noise. The ergodic capacity is given by
\begin{equation}
C = \mathbb{E}_{\alpha\sim p_R(r)}\biggl[\log_2(1+\alpha ^2 \cdot \text{SNR})\biggr].
\end{equation}
Fig. \ref{fig:AE_rayleigh_capacity} shows the estimated MI attained by the AE over a Rayleigh channel with several alphabet dimensions $M$ and compares it with the conventional $M$-QAM schemes. In all the cases, the achieved MI is upper bounded by the ergodic Rayleigh capacity. Similarly to the uniform case, it is curious to notice that the achieved information rate with the rate-driven AE is in some cases higher than the one obtained with the $M$-QAM schemes.
In particular, with $M=32$ the AE($5,1$) exceeds by $0.5$ bit/channel uses at SNR$=15$ dB the $32$-QAM scheme. Lastly, Fig. \ref{fig:AE_MI_all}c highlights the advantage of coding over two channel uses, especially in the range $5-15$ dB.

\begin{figure}
	\centering
	\includegraphics[scale=0.21]{images/autoencoder/MI_all.pdf}
	\caption{Estimated MI achieved with $\beta=0.2$, $\epsilon=0.2$ and rate $R=2$ with code length $n=1,2$ over a) AWGN channel, b) Additive uniform noise channel, c) Rayleigh channel.}
	\label{fig:AE_MI_all}
\end{figure}

\section{Summary}
\label{sec:autoencoders_conclusions}
This chapter has firstly discussed the AE-based communication system, highlighting the limits of the current cross-entropy loss function used for the training process. A regularization term that accounts for the MI between the transmitted and the received signals has been introduced to design optimal coding-decoding schemes for fixed rate $R$, code-length $n$ and given power constraint. The rationale behind the MI choice has been motivated exploiting the confidence penalty entropy regularization approach, the information bottleneck principle and the fundamental concept of channel capacity. In addition, an adaptation of the coding rate $R$ allowed us to build a capacity learning algorithm enabled by the novel loss function in a scheme named capacity-driven AE. Remarkably, the presented methodology does not make use of any theoretical a-priori knowledge of the communication channel and therefore opens the door to several future studies on intractable channel models, an example of which is the power line communication channel.