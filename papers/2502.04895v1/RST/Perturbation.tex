\section{Trajectory perturbation}
\label{subsec:rst_perturbation}
In this section, we present the formal procedure in order to deal with uncertainties in the kinematic constraints. For notation convenience, we will denote $\frac{d^i}{dt^i}\tilde{x}_k(t)\bigr|_{t=t_j}$ as the perturbed constraint. 

\subsection{Uncertainty model}
We model the uncertainty in the constraints as an additive contribution $\varepsilon_i$ to the fixed deterministic part, in particular
\begin{equation}
\label{eq:rst_perturbation}
\frac{d^i}{dt^i}\tilde{x}_k(t)\bigr|_{t=t_j} = \underbrace{\frac{d^i}{dt^i}x_k(t)\bigr|_{t=t_j}}_{\text{deterministic}}+\underbrace{\varepsilon_i(t_j)}_{\text{stochastic}},
\end{equation}
where $\varepsilon_i\sim \mathcal{N}(0,\sigma^2_{i}(t_j))$ is a Gaussian random variable with zero mean and variance $\sigma^2_{i}(t_j)$. The uncertainty models the deviation from the expected value of the kinematic constraint.
As an example, \eqref{eq:rst_perturbation} can be used to analyze how the noise in the E-FMS system affects the kinematic constraints, thus the polynomial trajectory.
Due to the intrinsic linearity of RST and the perturbation model, the following theorem proves that it is possible to translate the uncertainties in the constraints into uncertainties in the polynomial coefficients. 


\begin{theorem}
\label{lemma:rst_Lemma6}
Let $\frac{d^i}{dt^i}\tilde{x}_k(t)\bigr|_{t=t_j}$ be the perturbed kinematic constraints for $i=0,1,\dots, k$ and let $x_k(t)$ be a feasible polynomial trajectory computed with RST. Then $\tilde{x}_k(t)$ represents the perturbed polynomial trajectory and it can be written as
\begin{equation}
\tilde{x}_k(t) = x_k(t)+r_k(t),
\end{equation} 
where $r_k(t)$ is a random polynomial whose coefficients belong to a multivariate Gaussian distribution $\mathcal{N(\mathbf{\mu,\Sigma})}$.
\end{theorem}

\begin{proof}
To prove the theorem we will proceed by induction on the ($i-1$)-partial trajectory. Consider the base case when $i=0$, i.e., $\tilde{x}_0(t)$. From Sec. \ref{subsec:rst_Lagrange}, 
\begin{equation}
\label{eq:rst_lagrange_perturbated}
\tilde{x}_0(t) = \sum_{j=0}^{N}{\tilde{x}(t_j) \ell_j(t)}
\end{equation}
where $\ell_j(t)$ is the Lagrange basis. By substituting the perturbation expression for the constraints \eqref{eq:rst_perturbation} in \eqref{eq:rst_lagrange_perturbated}, it follows that 
\begin{align}
\tilde{x}_0(t) & = \sum_{j=0}^{N}{x(t_j) \ell_j(t)}+\sum_{j=0}^{N}{\varepsilon_0(t_j) \ell_j(t)} \nonumber \\ 
& = x_0(t)+r_0(t).
\end{align}

Suppose that the statement of the theorem is true for the $(i-1)$-partial trajectory, which means that
\begin{equation}
\tilde{x}_{i-1}(t)= x_{i-1}(t)+r_{i-1}(t).
\end{equation}
Then, it is true also for the $i$-partial trajectory. Indeed, from the RST algorithm derived in Theorem \ref{theorem:rst_theorem1},
\begin{equation}
\tilde{s}_i(t_j)=\dfrac{\dfrac{d^i}{dt^i}\tilde{x}_k(t)\biggr|_{t=t_j} -\dfrac{d^i}{dt^i}\tilde{x}_{i-1}(t)\biggr|_{t=t_j}}{\displaystyle \Biggl(\prod_{\substack{n=0\\ n\neq j}}^{N}{(t_j-t_n)}\Biggr)^i},
\label{eq:rst_perturbed_recursive}
\end{equation}
and using the induction hypothesis and the linearity of the differential operator,

\begin{align}
\tilde{s}_i(t_j)& = \dfrac{\Biggl(\dfrac{d^i}{dt^i}x_k(t)\biggr|_{t=t_j}+\varepsilon_i(t_j)\Biggr) -\Biggl(\dfrac{d^i}{dt^i}x_{i-1}(t)\biggr|_{t=t_j}+\dfrac{d^i}{dt^i}r_{i-1}(t)\biggr|_{t=t_j}\Biggr)}{\displaystyle \Biggl(\prod_{\substack{n=0\\ n\neq j}}^{N}{(t_j-t_n)}\Biggr)^i} \nonumber \\
&= \dfrac{\dfrac{d^i}{dt^i}x_k(t)\biggr|_{t=t_j}-\dfrac{d^i}{dt^i}x_{i-1}(t)\biggr|_{t=t_j}}{\displaystyle \Biggl(\prod_{\substack{n=0\\ n\neq j}}^{N}{(t_j-t_n)}\Biggr)^i}+\dfrac{\varepsilon_i(t_j)-\dfrac{d^i}{dt^i}r_{i-1}(t)\biggr|_{t=t_j}}{\displaystyle \Biggl(\prod_{\substack{n=0\\ n\neq j}}^{N}{(t_j-t_n)}\Biggr)^i} \nonumber \\
&= s_i(t_j)+s_i^{\varepsilon}(t_j).
\end{align}

This result is important because it separates $\tilde{s}_i(t_j)$ in two components. Using again Lagrange interpolation as done for $i=0$ yields to $\tilde{s}_i(t) = s_i(t)+s_i^{\varepsilon}(t)$. By the RST properties and definition of $i$-partial trajectory 
\begin{align}
\tilde{x}_i(t) &= \tilde{x}_{i-1}(t)+\frac{a^i(t)}{i!}\cdot \tilde{s}_i(t) \nonumber \\
&= x_{i-1}(t)+r_{i-1}(t)+ \frac{a^i(t)}{i!}\cdot (s_i(t)+s_i^{\varepsilon}(t)) \nonumber \\
&= x_{i-1}(t)+\frac{a^i(t)}{i!}\cdot s_i(t) + r_{i-1}(t)+ \frac{a^i(t)}{i!}\cdot s_i^{\varepsilon}(t) \nonumber \\
&= x_i(t)+r_i(t).
\label{eq:rst_derivation_perturbation}
\end{align}
Calculating \eqref{eq:rst_derivation_perturbation} in $i=k$ concludes the proof because $r_k(t)$ is a polynomial whose coefficients are a weighted sum of the uncertainties $\varepsilon_i$ in the constraints, for $i=0,1,\dots,k$. The way in which the random polynomial coefficients depend one to each other is described by the covariance matrix $\mathbf{\Sigma}$ of a multivariate Gaussian distribution, which has to be estimated.
\end{proof}
To characterize and generate new perturbed trajectories, an estimation of the multivariate Gaussian distribution described by the random coefficients in $r_k(t)$ has to be carried out.

\subsection{Coefficients estimation}
The coefficients of the random polynomial $r_k(t)$ incorporate the stochastic information of the constraints. 
Under the Gaussian hypothesis, it is straightforward to state that the coefficients are themselves Gaussian random variables since they are the outcome of linear combinations of $\varepsilon_i(t_j)$. However, the way in which the random variables $\varepsilon_i(t_j)$ interact and correlate one to each other strongly depends on the points in time $t_j$, in particular on the differences $t_j-t_n$, for $j=0,\dots,N$ and $n=0,\dots,N$ with $j\neq n$. Therefore, no closed form expression is available to describe the covariance matrix $\mathbf{\Sigma}$. Another approach consists of estimating $\mathbf{\Sigma}$, and we will refer to $\mathbf{\hat{\Sigma}}$ as the estimated version.

To compute $\mathbf{\hat{\Sigma}}$, the procedure involves the following steps:
\begin{itemize}
\item take $P$ realizations of the uncertainty in the constraints $\varepsilon_i(t_j)\sim \mathcal{N}(0,\sigma^2_{i}(t_j))$;
\item for each realization, evaluate the coefficients of the trajectory $r_{k}(t)$ generated via RST;
\item evaluate the sample covariance matrix $\mathbf{\hat{\Sigma}}$ of the stored coefficients.
\end{itemize}
The sample covariance matrix $\mathbf{\hat{\Sigma}}$ is the unbiased estimator of the covariance matrix $\mathbf{\Sigma}$.

\subsection{Random trajectory generation}
The multivariate Gaussian distribution is characterized by the mean (in this case $\mathbf{\mu} = \mathbf{0}$) and the covariance matrix $\mathbf{\Sigma}$. To generate new polynomial coefficients, thus new feasible trajectories, it is enough to sample from the multivariate distribution. In particular, consider a vector $\mathbf{z}$ of uncorrelated normal random variables. If the matrix $\mathbf{C}$ is a square root of $\mathbf{\Sigma}$, such as $\mathbf{C}\cdot \mathbf{C}^T = \Sigma$ (for example using the Cholesky decomposition), it follows that $\mathbf{y}=\mathbf{\mu} + \mathbf{C}\cdot \mathbf{z}$ is a vector of Gaussian random variables representing the coefficients of the random polynomial $r_k(t)$.


\begin{algorithm}
\caption{Recursive smooth random trajectory (RSRT) generation}
\label{alg:RSRT}
\begin{algorithmic}[1]
\Inputs{$N+1$ points in time $t_0<t_1<\dots<t_N$; \\ Kin. constr. $\frac{d^i}{dt^i}x_k(t)\bigr|_{t=t_0}, \dots, \frac{d^i}{dt^i}x_k(t)\bigr|_{t=t_N}$; \\ Uncertainties $\varepsilon_i(t_j)$.}
\Initialize{$a(t)=(t-t_0)\cdots (t-t_N)$ polynomial; \\ $P$ number of realizations; \\ $x_{-1}(t) \equiv 0$ \\ $r_{-1}(t) \equiv 0$.}
\State Compute $x_k(t)$ with RST;
\For{$p=1$ to $P$}
		\State Get realizations of $\varepsilon_i(t_j)$;
		\State Compute $r_{k,p}(t)$ with RST and constraints $\varepsilon_i(t_j)$;
	\EndFor
	\State Estimate $\mathbf{\hat{\Sigma}}$ of the coefficients of $r_k(t)$;
	\State Generate a set of coefficients of $r_k(t)$;
		\State Generate a perturbed trajectory $\tilde{x}_k(t)=x_k(t)+r_k(t)$;
\end{algorithmic}
\end{algorithm}

This process offers a fast methodology for generating perturbed trajectories $\tilde{x}_k(t)=x_k(t)+r_k(t)$ from the estimated coefficients in $r_k(t)$. We will refer to it as recursive smooth random trajectory (RSRT) generation. The algorithm is described in Alg.~\ref{alg:RSRT}

So far we have only considered a multivariate Gaussian distribution for $\varepsilon_i(t_j)$. Nevertheless as a consequence of the central limit theorem, whenever the uncertainties have different distribution, correlated Gaussian random variables can approximate the statistics of the polynomial coefficients of $r_k(t)$.