\section{Comments and further discussion}
\label{subsec:rst_comment}
\subsection{Accuracy vs discontinuities}
\begin{figure}
\includegraphics[scale=0.50]{images/extra/flow.pdf}
\centering
\vspace{-0.1em}
\caption{Complexity and smoothness trade-off regions of RST algorithm depending on number of blocks and waypoints in each block.}
\vspace{-1.0em}
\label{fig:rst_regions}
\end{figure}
In this section, we present a useful rule of thumb for applying the RST and the RST$_{\text{opt}}$ algorithm according to the number of blocks the user considers and the number of waypoints inside each of these blocks. 
Indeed, Fig.~\ref{fig:rst_regions} and Fig.~\ref{fig:rst_example1} identify regions, suggesting which algorithm to apply according to the number of waypoints and blocks.
So far, most of path planning research has been focused on piecewise polynomial approaches, sticking to the first left strip in Fig.~\ref{fig:rst_regions}. There are, indeed, cases where numerical polynomial complexity is the main concern and in such situations a standard piecewise polynomial approach is sufficient. Nevertheless, we propose to use the PRST algorithm to compute the piecewise polynomial trajectory (e.g. RST with $2$ points in each block). The reason for this choice comes from the intrinsic optimality of RST when the number of waypoints in each block is $2$, as proved in Lemma \ref{lemma:rst_Lemma5}.

An unwanted side effect of the piecewise choice is the presence of discontinuities in the $p$-th derivative's interface. To reduce and eventually remove them, a good compromise is the BRST algorithm which balances polynomial complexity and discontinuity issues. For instance, instead of using $15$ piecewise polynomials for a trajectory consisting of $16$ points, one could choose to split the trajectory generation in $5$ blocks of $4$ points each, leading to a reduction of discontinuities, from $14$ to $4$ matching interfaces, while at the same time keeping a low level of complexity in polynomials. See Fig.~\ref{fig:rst_example1} for an example of such blockwise trajectory.
Whenever the complexity is not the issue to consider at first, one could build a smooth trajectory which passes through all the waypoints. In such a case RST is one possible way to proceed if no optimal condition is required, otherwise RST$_{\text{opt}}$ provides the optimality at expenses of a higher computational cost. 
Moreover, we empirically found that smooth trajectories with more than $15$ waypoints in the same block are unstable, therefore we suggest to always split them into at least $2$ consecutive blocks.
Fig.~\ref{fig:rst_regions} and \ref{fig:rst_example1} illustrate these concepts and guide the user to select the proper algorithm according to given specifics.

\begin{figure}
\includegraphics[scale=0.37]{images/extra/RST_example.pdf}
\centering
\vspace{-0.1em}
\caption{Example of blockwise trajectory (BRST, BRST$_{\text{opt}}$ and minimum-snap) that minimizes the integral of the acceleration squared.}
\vspace{-1.0em}
\label{fig:rst_example1}
\end{figure}

\subsection{Memory requirements}
\label{subsec:rst_time}
The number of coefficients of the RST polynomial trajectory to store is $(k+1)(N+1)$ (See Corollary \ref{corollary:rst_corollary1}) where $N+1$ is the number of waypoints and $k$ is the last kinematic constraint. Extending the results also to BRST leads to $M$ polynomials of degree $(k+1)(\frac{N+M}{M})-1$. Thus, the number of coefficients to store is equal to $(k+1)(N+M)$. The ratio between the number of coefficients to store for BRST and RST is equal to $1+\frac{M-1}{N+1}$ which means that BRST requires $100\cdot \frac{M-1}{N+1}$ percent more memory than RST. As an extreme case, PRST is the technique which requires the highest memory requirements since it stores $2N(k+1)$ coefficients, almost twice the memory required by RST.

\subsection{Computational complexity}
\label{subsec:rst_complexity}
To evaluate the computational complexity, it is convenient to segment the RST algorithm (See Alg. \ref{alg:RST}) in $3$ parts: the recursive formula, which computes the control points $s_i(t_j)$ as in \eqref{eq:rst_recursive} (line 5 of Alg. \ref{alg:RST}), the interpolation phase with Lagrange polynomials (line 7 of Alg. \ref{alg:RST}), and the generation of the $i$-partial trajectory (line 8 of Alg. \ref{alg:RST}).
The recursive formula provides $N+1$ control points $s_i(t_j)$ at the $i$-th iteration, with $i=0,1,\dots,k$ and $j=0,1,\dots,N$. In particular, to compute a single value $s_i(t_j)$ it needs a number of operations that goes as $\mathcal{O}(i\cdot \text{deg}(x_{i-1}(t))) \sim \mathcal{O}(i^2 \cdot N)$, where the first $i$ contribution comes from the $i$ derivatives of the $(i-1)$-partial trajectory. Hence, for $N+1$ points the complexity of the recursive formula is $N\mathcal{O}(i^2 \cdot N)$. The complexity of the Lagrange interpolation technique is $\mathcal{O}((N+1)^2) \sim \mathcal{O}(N^2)$. Lastly, the generation of the $i$-partial trajectory involves the product between $a^i(t)\cdot s_i(t)$, which requires a number of operations that grow as $\mathcal{O}((N+1)\cdot i \cdot (N+1)) \sim \mathcal{O}(i \cdot N^2)$. Since the number of iterations are $k+1$, the overall time complexity $T(k,N)$ reads as follows
\begin{align}
T(k,N) &= \sum_{i=0}^{k}{N\mathcal{O}(i^2 \cdot N)+\mathcal{O}(N^2)+\mathcal{O}(i \cdot N^2)} \nonumber \\
& \sim \mathcal{O}(k^3 \cdot N^2)+\mathcal{O}(k\cdot N^2)+\mathcal{O}(k^2 \cdot N^2) \nonumber \\
& \sim \mathcal{O}(k^3 \cdot N^2).
\end{align}
Although the estimated complexity is a rough approximation, it is interesting to highlight the following fact: the minimum degree polynomial trajectory $x_k(t)$ could have been derived in the classical approach just by evaluating the polynomial and its derivatives in the time stamps and by solving a system of linear equations. Alternative in a matrix form, $b = \mathbf{X}\cdot a$ where $\mathbf{X}$ is a $(k+1)(N+1)\times (k+1)(N+1)$ square matrix, badly conditioned from a numerical point of view, $a$ is the unknown vector of the polynomial coefficients and $b$ the vector of the kinematic constraints. To find the polynomial trajectory, thus, the coefficients, the matrix $\mathbf{X}$ needs to be inverted (when numerically possible). However, the matrix inversion operation involves a complexity of order $\mathcal{O}(k^3 \cdot N^3)$, that is higher than the RST complexity. Therefore, RST is not only numerically stable since no matrix inversion is required, but it is also faster than the classical interpolation approach (INV). Fig. \ref{fig:rst_time_complexity} illustrates the computational complexity advantage of RST over the classic interpolation method.

\begin{figure}
\includegraphics[scale=0.205]{images/extra/time_complexity.pdf}
\centering
\vspace{-0.1em}
\caption{Computational complexity comparison between RST and the classic interpolation approach through matrix inversion (INV).}
\vspace{-1.0em}
\label{fig:rst_time_complexity}
\end{figure}


\subsection{Extension of the proposed framework}
We presented the RST algorithm and extensions to block (BRST) and piecewise (PRST) approaches. Initial assumptions always considered time intervals with the same length or points in time following \eqref{Cheby}. The case that considers random initially located points in time can been studied under the optimization framework RST$_{\text{opt}}$. To tackle the oscillation problem (see Fig. \ref{fig:rst_Runge}), typical of high-order polynomial interpolation, a possible solution without involving the optimization step could either pass through spline interpolation or different interpolating polynomials such as barycentric Lagrange polynomials \cite{Berrut} or Newton ones.

Lastly and perhaps more fascinating, is the idea of mixing and eventually replacing polynomial trajectories with other basis functions. All the mathematical formulation and most of the derivation actually transcend the polynomial assumption. The only point in which this hypothesis plays a role is in the $h$-th derivative step (See Lemma \ref{lemma:rst_Lemma2}). The outcome of a further investigation is discussed in Sec. \ref{sec:rst_rrst}

\begin{figure}
\includegraphics[scale=0.50]{images/extra/runge_phenomenon}
\centering
\caption{Illustration of Runge's phenomenon: the Runge function (blue dashed line) is approximated with a $15$-th order polynomial (red solid line) that interpolates $16$ equally spaced nodes.}
\label{fig:rst_Runge}
\end{figure}