\section{Theoretical formulation of RST}
\label{subsec:rst_results}
This section presents the mathematical foundations behind the choice of the polynomial representation in \eqref{eq:rst_trajectory}. We show that such choice produces a closed form continuous polynomial trajectory and enables an iterative algorithm for its generation. In particular, Lagrange polynomials are exploited. For ease of notation, in the following we consider the trajectory in \eqref{eq:rst_trajectory} as unidimensional, assuming that the $3D$ extension is obtained by working component-wise.%
\subsection{Preliminaries on Lagrange polynomials}
\label{subsec:rst_Lagrange}
Lagrange polynomials offer a technique to interpolation problems. In particular, given a set of $N+1$ $1D$ control points (waypoints) $(t_0,x(t_0)), (t_1,x(t_1)), \dots, (t_N,x(t_N))$, the interpolation polynomial in the Lagrange form is defined as
\begin{equation}
\label{eq:rst_lagrange}
L(t) := \sum_{j=0}^{N}{x(t_j) \ell_j(t)}
\end{equation}
where $\ell_j(t)$ is the Lagrange polynomial basis whose form reads as follows
\begin{equation}
\ell_j(t):= \prod_{\substack{m=0 \\ m\neq j}}^{N}{\frac{t-t_m}{t_j-t_m}},
\end{equation}
with $0\leq j\leq N$. The main idea behind this definition is that, by construction, the $N+1$ basis functions are such that $\ell_j(t_i)\equiv 0$ in $i=0,\dots,N \; \wedge \; i\neq j$. So for each waypoint, only one single basis function contributes to the sum in \eqref{eq:rst_lagrange}.

\subsection{The recursive formula}
To discover the recursive property behind the formulation in \eqref{eq:rst_trajectory}, we start the mathematical derivation introducing two lemmas regarding a particular polynomial choice and its derivatives. Such type of choice enables a remarkable recursive property which is proved in Theorem \ref{theorem:rst_theorem1}, the main result of this chapter. The corollary \ref{corollary:rst_corollary1}, instead, establishes an upper bound for the minimum degree of the polynomial trajectory generated via the recursive formulation.

\begin{lemma}
\label{lemma:rst_Lemma1}
Let
\begin{equation}
a(t) = \prod_{n=0}^{N}{(t-t_n)},
\label{eq:rst_a_t}
\end{equation}
then
\begin{equation}
\frac{d}{dt}a(t) = a(t)\cdot \sum_{n=0}^{N}{\frac{1}{t-t_n}}
\label{eq:rst_da_t}
\end{equation}
and
\begin{equation}
\frac{d}{dt}a(t)\biggr|_{t=t_j} = \prod_{\substack{n=0\\ n\neq j}}^{N}{(t_j-t_n)}.
\label{eq:rst_da_tj}
\end{equation}
\end{lemma}
\begin{proof}
Using product rule
\begin{equation}
\frac{d}{dt}a(t) = \sum_{j=0}^{N}{\prod_{\substack{n=0\\ n\neq j}}^{N}{(t-t_n)}}.
\label{eq:rst_Lemma1}
\end{equation}
Dividing and multiplying by $a(t)$ gives \eqref{eq:rst_da_t}. For $t=t_j$ only one term of the summation in \eqref{eq:rst_Lemma1} contributes leading to \eqref{eq:rst_da_tj}. \qedhere  
\end{proof}

\begin{lemma}
\label{lemma:rst_Lemma2}
Let
\begin{equation*}
a(t) = \prod_{n=0}^{N}{(t-t_n)},
\end{equation*}
and let $s(t)$ be a polynomial. Then
\begin{equation}
\frac{d^h}{dt^h}\biggl[\frac{a^k(t)}{k!}\cdot s(t)\biggr]_{t=t_j} \equiv 0 \; \forall k>h
\label{eq:rst_diff_ak}
\end{equation}
and
\begin{equation}
\frac{d^h}{dt^h}\biggl[\frac{a^h(t)}{h!}\cdot s(t)\biggr]_{t=t_j} = \biggl(\frac{d}{dt}a(t)\biggr)^h\ \biggr|_{t=t_j} \cdot s(t_j).
\label{eq:rst_diff_ah}
\end{equation}
\end{lemma}
\begin{proof}
Since $t=t_j$ is a zero of $a^k(t)$ with multiplicity $k>h$, the factor $(t-t_j)$ appears in every term of the $h$-th derivative. If $k=h$, then
\begin{equation}
\frac{d^h}{dt^h}\biggl[\frac{a^h(t)}{h!}\cdot s(t)\biggr]_{t=t_j} = \frac{d^{h-1}}{dt^{h-1}}\biggl[\frac{d}{dt} \biggl(\frac{a^h(t)}{h!}\cdot s(t)\biggr)\biggr]_{t=t_j}.
\end{equation}
By the linearity of the differential operator and product rule, RHS can be rewritten as
\begin{equation}
\frac{d^{h-1}}{dt^{h-1}}\biggl[\frac{a^{h-1}(t)}{{h-1}!}\cdot \frac{d a(t)}{dt}\cdot s(t)\biggr]_{t=t_j} + \cancel{\frac{d^{h-1}}{dt^{h-1}}\biggl[\frac{a^h(t)}{h!}\cdot \frac{d}{dt}s(t)}\biggr]_{t=t_j}
\end{equation}
where the second term in the above expression vanishes because of the first part of the lemma. Proceeding in the same way until the $h$-th derivative leads to
\begin{equation}
\frac{d^h}{dt^h}\biggl[\frac{a^h(t)}{h!}\cdot s(t)\biggr]_{t=t_j} = \biggl(\frac{d}{dt}a(t)\biggr)^h\ \biggr|_{t=t_j} \cdot s(t_j)
%\label{eq:rst_diff_ah}
\end{equation}
which concludes the proof. \qedhere  
\end{proof}

To understand the terms involved while building the final trajectory $x_k(t)$, the concept of $m$-partial trajectory is introduced.
\begin{defn}
\label{def:rst_def1}
Let $\frac{d^i}{dt^i}x_k(t)\bigr|_{t=t_j}$ be given kinematic constraints, for $i=0,1,\dots, k$. An $m$-partial trajectory is a feasible trajectory which only fulfills the first $m$ kinematic constraints, i.e. 
\begin{equation}
x_m(t) = \sum_{i=0}^{m}{p_i(t)}, \; \text{with }\; m\leq k.
\end{equation} 
\end{defn}
As an example, if the kinematic constraints are set up to the acceleration ($k=2$), a $1$-partial trajectory is a polynomial trajectory that passes through the $N+1$ waypoints with the given velocities.

\begin{theorem}
\label{theorem:rst_theorem1}
Let $t_j$ be a point in time, for $j=0,1,\dots, N$, such that $\frac{d^i}{dt^i}x_k(t)\bigr|_{t=t_j}=\sigma_{i,j}$ is the associated given kinematic constraint, for $i=0,1,\dots, k$. Let $x_k(t)$ be a feasible polynomial trajectory defined as
\begin{equation}
x_k(t) = \sum_{i=0}^{k}{p_i(t)}.
\end{equation}
If
\begin{equation}
p_i(t)=\dfrac{1}{i!}\biggl(\prod_{n=0}^{N}{(t-t_n)}\biggr)^i\cdot s_i(t)
\end{equation}
then the $i$-partial trajectory $x_i(t)$ depends recursively on $x_{i-1}(t)$. In particular,
\begin{equation}
s_i(t_j)=\dfrac{\dfrac{d^i}{dt^i}x_k(t)\biggr|_{t=t_j} -\dfrac{d^i}{dt^i}x_{i-1}(t)\biggr|_{t=t_j}}{\displaystyle \Biggl(\prod_{\substack{n=0\\ n\neq j}}^{N}{(t_j-t_n)}\Biggr)^i}.
\label{eq:rst_recursive}
\end{equation}
\end{theorem}

\begin{proof}
For $i=0$, $x_{-1}(t):=0$ and the kinematic constraint is $x_k(t_j)$ which brings to the first polynomial $s_0(t)=p_0(t)=x_0(t)$ obtained using an interpolation technique, e.g. the Lagrange polynomials as explained in Sec. \ref{subsec:rst_Lagrange}. No other contributions $p_i(t)$ are taken into account since $p_i(t_j)=0 \; \forall i>0$. For brevity of notation it is now convenient to define a polynomial $a(t)$ as $a(t) = \prod_{n=0}^{N}{(t-t_n)}$.

For $i=1$ and constraint $\frac{d}{dt}x_k(t)\bigr|_{t=t_j}$, 
\begin{equation}
x_k(t) = x_0(t) + a(t)\cdot s_1(t) + \sum_{i=2}^{k}{\dfrac{a^i(t)}{i!}s_i(t)},
\label{eq:rst_i1}
\end{equation}
taking the derivative in $t_j$ in both sides of \eqref{eq:rst_i1} yields to
\begin{equation}
\frac{d}{dt}x_k(t)\biggr|_{t=t_j} = \frac{d}{dt}x_0(t)\biggr|_{t=t_j} + \frac{d}{dt}a(t)\biggr|_{t=t_j}\cdot s_1(t_j)
\end{equation}
where, using Lemma \ref{lemma:rst_Lemma2}, no contribution in $t_j$ comes from $i>1$. Rearranging,
\begin{equation}
s_1(t_j)=\dfrac{\dfrac{d}{dt}x_k(t)\biggr|_{t=t_j} -\dfrac{d}{dt}x_0(t)\biggr|_{t=t_j}}{\displaystyle \prod_{\substack{n=0\\ n\neq j}}^{N}{(t_j-t_n)}}
\end{equation}
where the denominator is a consequence of Lemma \ref{lemma:rst_Lemma1}.
To compute $s_1(t)$, it is convenient to interpolate the points $s_1(t_j)$ again with Lagrange polynomials. The $1$-partial trajectory has expression $x_1(t)=x_0(t)+a(t)\cdot s_1(t)$.

In general, for $i=h$ the constraint to be fulfilled is $\frac{d^h}{dt^h}x_k(t)\bigr|_{t=t_j}$. Thus
\begin{equation}
x_k(t) = x_{h-1}(t) + \dfrac{a^h(t)}{h!}\cdot s_h(t) + \sum_{i=h+1}^{k}{\dfrac{a^i(t)}{i!}\cdot s_i(t)},
\label{eq:rst_ih}
\end{equation}
where $x_{h-1}(t)$ is the $(h-1)$-partial trajectory. Taking the $h$-th derivative in $t_j$ in both sides of \eqref{eq:rst_ih} and using again Lemma \ref{lemma:rst_Lemma2} for the second and third term, yields to
\begin{equation}
\frac{d^h}{dt^h}x_k(t)\biggr|_{t=t_j} = \frac{d^h}{dt^h}x_{h-1}(t)\biggr|_{t=t_j} + \biggl(\frac{d}{dt}a(t)\biggr)^h\ \biggr|_{t=t_j} \cdot s_h(t_j).
\end{equation}
Finally, rearranging
\begin{equation}
s_h(t_j)=\dfrac{\dfrac{d^h}{dt^h}x_k(t)\biggr|_{t=t_j} -\dfrac{d^h}{dt^h}x_{h-1}(t)\biggr|_{t=t_j}}{\displaystyle \Biggl(\prod_{\substack{n=0\\ n\neq j}}^{N}{(t_j-t_n)}\Biggr)^h}
\end{equation}
concludes the proof. \\ \qedhere  
\end{proof}

Theorem \ref{theorem:rst_theorem1} provides a recursive formula to evaluate the points $s_i(t_j)$. Hence, it is sufficient to interpolate them, for example with Lagrange polynomials, in order to get $s_i(t)$ at each iteration. As a remark, the interpolation of $s_i(t_j)$ can be carried out also by adopting other polynomial basis functions, e.g., Newton polynomials, B-Spline, etc. Nevertheless, the polynomial assumption comes from Lemma \ref{lemma:rst_Lemma2}. Indeed, polynomials, when differentiated, do not introduce extra poles which could cancel with $a(t)$. On the contrary, other basis functions do not guarantee this property.

The following corollary provides an information on the minimum degree of the polynomial trajectory $x_k(t)$ defined in \eqref{eq:rst_trajectory}.

\begin{corollary}
\label{corollary:rst_corollary1}
Let $t_j$ be a point in time, for $j=0,1,\dots, N$, such that $\frac{d^i}{dt^i}x_k(t)\bigr|_{t=t_j}$ is the associated given kinematic constraint, for $i=0,1,\dots, k$. If $x_k(t)$ is a feasible polynomial trajectory defined as
\begin{equation}
x_k(t) = \sum_{i=0}^{k}{\dfrac{1}{i!}\biggl(\prod_{n=0}^{N}{(t-t_n)}\biggr)^i\cdot s_i(t)},
\end{equation}
then the minimum degree of $x_k(t)$ is less or equal to $(k+1)(N+1)-1$.
\end{corollary}
\begin{proof}
Each $s_i(t)$ is a Lagrange polynomial that passes through $N+1$ points, therefore its minimum degree is $N$. The highest contribution in terms of degree to $x_k(t)$ comes when $i=k$, so that $\prod_{n=0}^{N}{(t-t_n)^k}$ is a polynomial of degree equal to $(N+1)k$. Thus, the product of $\prod_{n=0}^{N}{(t-t_n)^k}$ and $s_k(t)$ gives a polynomial whose minimum degree is at most $(N+1)k+N = (k+1)(N+1)-1$.
This is somehow consistent with the idea that imposing $k+1$ constraints for each of the points in time $t_j$ gives $(k+1)(N+1)$ constraints and the minimum degree of a polynomial that satisfies them is $(k+1)(N+1)-1$. \qedhere
\end{proof}

\subsection{Recursive smooth trajectory generation}
\label{subsec:rst_code}

\begin{figure}[t]
\includegraphics[scale=0.40]{images/extra/RST_all}
\centering
\caption{RST block diagram.}
\label{fig:rst_RST}
\end{figure}
In the following, we will denote our approach as recursive smooth trajectory (RST) generation. The idea behind it, is that each component $p_i(t)$ in \eqref{eq:rst_trajectory} guarantees that $\dfrac{d^i}{dt^i}x_k(t)\bigr|_{t=t_j}=\sigma_{i,j}$ is fulfilled. Starting from the waypoints constraint which can be easily calculated through Lagrange polynomials, all the following higher-order differential kinematic constraints depend recursively on previous ones, according to \eqref{eq:rst_recursive}. 

The pseudocode in Alg.~\ref{alg:RST} provides a practical idea on how to iteratively design the trajectory $x_k(t)$ under the conditions aforementioned. The scheme in Fig.~\ref{fig:rst_RST} illustrates how RST operates.



We now discuss the influence of the distribution of the time instants $t_j$ for $j=0,\dots,N$ over the trajectory on oscillations and numerical limitations by proposing a known solution (Chebyshev nodes) and a novel hybrid solution, denoted as blockwise recursive smooth trajectory (BRST) that tackle said practical issues.

\begin{algorithm}
\caption{Recursive smooth trajectory (RST) generation}
\label{alg:RST}
\begin{algorithmic}[1]
\Inputs{$N+1$ points in time $t_0<t_1<\dots<t_N$; \\ Number of derivatives $k$ to fulfill; \\ Kin. constr. $\frac{d^i}{dt^i}x_k(t)\bigr|_{t=t_0}, \dots, \frac{d^i}{dt^i}x_k(t)\bigr|_{t=t_N}$.}
\Initialize{$a(t)=(t-t_0)\cdots (t-t_N)$ polynomial; \\ $x_{-1}(t) \equiv 0$.}
\For{$i=0$ to $k$}
	\For{$j=0$ to $N$}
		\State $s_i(t_j)=\dfrac{\dfrac{d^i}{dt^i}x_k(t)\biggr|_{t=t_j} -\dfrac{d^i}{dt^i}x_{i-1}(t)\biggr|_{t=t_j}}{\displaystyle \Biggl(\dfrac{d}{dt}a(t)\biggr|_{t=t_j}\Biggr)^i}$;
	\EndFor
	\State Interpolate $s_i(t_j)$ with Lagrange polynomial $s_i(t)$;
	\State $x_i(t) = x_{i-1}(t)+\dfrac{a^i(t)}{i!}\cdot s_i(t)$;
\EndFor
\end{algorithmic}
\end{algorithm}

\subsection{Remarks and insights on blockwise approach}
In the previous sections, we presented the formal generation approach (RST) of $x_k(t)$ for points in time $t_0<t_1< \dots <t_N$ with no constraints on $N$ and on the length of the interval $I_j = t_{j+1}-t_j$ for $j = 0,\dots, N-1$. 
Unfortunately, when the number of points $N+1$ is large and in particular when points in time $t_j$ are equally spaced ($I_j$ is constant), the Runge's phenomenon may occur \cite{Runge}. The Runge's phenomenon is an oscillation problem near the endpoints of the polynomial interpolation function, as illustrated in Fig. \ref{fig:rst_Runge}. To overcome it, one could either move to spline interpolation as mentioned in Sec. \ref{subsec:rst_introduction} or change the distribution of the nodes $t_j$ more densely towards the edges of the interval $[t_0, t_N]$ as proposed in \cite{Berrut}.
In the latter case, a standard choice considers the set of points in time as the set of Chebyshev nodes. In particular, for $N+1$ points in the interval $[t_0, t_N]$, nodes are transformed into
\begin{equation}
\hat{t}_j = \frac{1}{2}(t_0+t_N) + \frac{1}{2}(t_N-t_0)\cos\biggl[\dfrac{2j+1}{2(N+1)}\pi\biggr], \; j = 0,\dots, N.
\label{Cheby}
\end{equation}

Moreover, when the number of points $N+1$ increases, we encounter computational limitations in evaluating the powers $t^{(k+1)(N+1)-1}$. In such a case, a possible new blockwise approach that we name blockwise RST (BRST) concatenates intervals 
\begin{equation}
    [t_{0,1},t_{N,1}], [t_{0,2},t_{N,2}], \dots, [t_{0,M},t_{N,M}]
\end{equation} 
in $M$ blocks. For each block, the associated trajectory is separately calculated as described in Sec. \ref{subsec:rst_code}. Since kinematic constraints are intrinsically considered in the formulation of the trajectory, interfaces are already jointly matched (up to $k$-th derivative) without any need of optimization steps. Finally, when $N=1$, we bring blockwise back to piecewise polynomial trajectories under the recursive framework and we will refer to it as piecewise RST (PRST) algorithm. It is important to notice that the PRST approach provides a piecewise trajectory that is exactly the same as the one generated using the classic spline interpolation method. However, the two methods are intrinsically different: PRST finds the piecewise trajectory recursively by building $k+1$ partial trajectories, while the spline interpolation technique solves a system of linear equations, thus it needs a matrix inversion. A quick comparison of the computational cost is presented in Sec. \ref{subsec:rst_complexity}, while a more detailed study is left for future research, since the most efficient implementation of the RST needs to be studied.