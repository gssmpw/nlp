\chapter{Cooperative Channel Capacity Learning} % Cortical
\chaptermark{Cooperative Channel Capacity Learning}
%\thispagestyle{empty}
\label{sec:cortical}

In this chapter, the problem of determining the capacity of a communication channel is formulated as a cooperative game, between a generator and a discriminator, that is solved via deep learning techniques. The task of the generator is to produce channel input samples for which the discriminator ideally distinguishes conditional from unconditional channel output samples.

The learning approach, referred to as cooperative channel capacity learning (CORTICAL), provides both the optimal input signal distribution and the channel capacity estimate. Numerical results demonstrate that the proposed framework learns the capacity-achieving input distribution under challenging non-Shannon settings.

The results presented in this chapter are documented in \cite{LetiziaNIPS, CORTICAL}.

\section{Introduction}
\sectionmark{Introduction}
\label{sec:cortical_intro}
While the benefit of a DL approach \cite{Oshea2017} is evident for tasks like signal detection \cite{Ye2017}, decoding \cite{tonello2022mind} and channel estimation \cite{Ye2018,Soltani2019}, the fundamental problem of estimating the capacity of a channel remains elusive. Despite recent attempts via neural MI estimation \cite{Aharoni2020, Letizia2021,Farhad2022}, it is not clear yet whether DL can provide novel insights.

For a discrete-time continuous memoryless vector channel, the capacity is defined as
\begin{equation}
C = \max_{p_X(\mathbf{x})} I(X;Y),
\end{equation}
where $p_X(\mathbf{x})$ is the input signal PDF, $X$ and $Y$ are the channel input and output random vectors, respectively, and $I(X; Y)$ is the MI between $X$ and $Y$. 
The channel capacity problem involves both determining the capacity-achieving distribution and evaluating the maximum achievable rate. Only a few special cases, e.g., additive noise channels with specific noise distributions under input power constraints, have been solved so far. When the channel is not an additive noise channel, analytical approaches become mostly intractable leading to numerical solutions, relaxations \cite{CuttingPlane}, capacity lower and upper bounds \cite{McKellips2004}, and considerations on the support of the capacity-achieving distribution \cite{Smith1971, Dytso2020}. 
It is known that the capacity of a discrete memoryless channel can be computed using the Blahut-Arimoto (BA) algorithm \cite{BlahutArimoto}, whereas a particle-based BA method was proposed in \cite{Dauwels2005} to tackle the continuous case although it fails to scale to high-dimension vector channels.

In the following section, we show that a data-driven approach can be pursued to obtain the channel capacity. In particular, we propose to learn the capacity and the capacity-achieving distribution of any discrete-time continuous memoryless vector channel via a cooperative framework referred to as CORTICAL. The framework is inspired by GANs \cite{Goodfellow2014} but it can be interpreted as a dual version using an appropriately defined value function. In fact, CORTICAL comprises two blocks cooperating with each other: a generator that learns to sample from the capacity-achieving distribution, and a discriminator that learns to differentiate paired channel input-output samples from unpaired ones, i.e. it distinguishes the joint PDF $p_{XY}(\mathbf{x},\mathbf{y})$ from the product of marginal $p_{X}(\mathbf{x})p_{Y}(\mathbf{y})$, so as to estimate the MI. 


\section{Cooperative principle for capacity learning}
\sectionmark{Cortical}
\label{sec:cortical_theory}
To understand the working principle of CORTICAL and why it can be interpreted as a cooperative approach, it is useful to briefly review how GANs operate. 

In the GAN framework, the adversarial training procedure for the generator $G$ consists of maximizing the probability of the discriminator $D$ making a mistake. If $\mathbf{x}\sim p_{\text{data}}(\mathbf{x})$ are the data samples and $\mathbf{z}\sim p_{Z}(\mathbf{z})$ are the latent samples, the Nash equilibrium is reached when the minimization over $G$ and the maximization over $D$ of the value function
\begin{equation}
\mathcal{V}(G,D) = \mathbb{E}_{\mathbf{x} \sim p_{\text{data}}(\mathbf{x})}\biggl[\log \bigl(D(\mathbf{x})\bigr)\biggr]  +\mathbb{E}_{\mathbf{z} \sim p_{Z}(\mathbf{z})}\biggl[\log\bigl(1-D\bigl(G(\mathbf{z})\bigr)\bigr)\biggr],
\label{eq:CORTICAL_GAN_value_function}
\end{equation}
is attained so that $G(\mathbf{z})\sim p_{\text{data}}(\mathbf{x})$. Concisely, the minimization over $G$ forces the generator to implicitly learn the distribution that minimizes the given statistical distance. 

\begin{figure}
	\centering
	\includegraphics[scale=0.45]{images/capacity/Cooperation.pdf}
	\caption{CORTICAL, Cooperative framework for capacity learning: a generator produces input samples with distribution $p_X(\mathbf{x})$ and a discriminator attempts to distinguish between paired and unpaired channel input-output samples.}
	\label{fig:CORTICAL_Cooperative_networks}
\end{figure}

Conversely to GANs, the channel capacity estimation problem requires the generator to learn the distribution maximizing the MI measure. 
Therefore, the generator and discriminator need to play a cooperative max-max game with the objective for $G$ to produce channel input samples for which $D$ exhibits the best performance in distinguishing (in the KL sense) paired and unpaired channel input-output samples. Thus, the discriminator of CORTICAL is fed with both the generator and channel's output samples, $\mathbf{x}$ and $\mathbf{y}$, respectively.
Fig. \ref{fig:CORTICAL_Cooperative_networks} illustrates the proposed cooperative framework that learns both for the cooperative training, as discussed next.

\begin{theorem}
\label{theorem:CORTICAL_Theorem1}
 Let $X\sim p_X(\mathbf{x})$ and $Y|X\sim p_{Y|X}(\mathbf{y}|\mathbf{x})$ be the vector channel input and conditional output, respectively. Let $Y = H(X)$ with $H(\cdot)$ being the stochastic channel model and let $\pi(\cdot)$ be a permutation function \footnote{The permutation takes place over the temporal realizations of the vector $Y$ so that $\pi(Y)$ and $X$ can be considered statistically independent vectors, for instance with the derangement strategy.} over the realizations of $Y$ such that $p_{\pi(Y)}(\pi(\mathbf{y})|\mathbf{x}) = p_{Y}(\mathbf{y})$. In addition, let $G$ and $D$ be two functions in the non-parametric limit such that $X = G(Z)$ with $Z\sim p_Z(\mathbf{z})$ being a latent random vector. If $\mathcal{J}_{\alpha}(G,D)$, $\alpha>0$, is the value function defined as 
\begin{equation}
\mathcal{J}_{\alpha}(G,D) = \alpha \cdot \mathbb{E}_{\mathbf{z} \sim p_{Z}(\mathbf{z})}\biggl[\log \biggl(D\biggl(G(\mathbf{z}),H(G(\mathbf{z}))\biggr)\biggr)\biggr]  -\mathbb{E}_{\mathbf{z} \sim p_{Z}(\mathbf{z})}\biggl[D\biggl(G(\mathbf{z}),\pi(H(G(\mathbf{z})))\biggr)\biggr],
\label{eq:CORTICAL_value_function}
\end{equation}
then the channel capacity $C$ is the solution of
\begin{equation}
C = \max_{G} \max_{D} \frac{\mathcal{J}_{\alpha}(G,D)}{\alpha} + 1- \log(\alpha),
\end{equation}
and $\mathbf{x}=G^*(\mathbf{z})$ are samples from the capacity-achieving distribution, where 
\begin{equation}
G^* = \argmax_G \max_D \mathcal{J}_{\alpha}(G,D).
\end{equation}
\end{theorem}

\begin{proof}
The value function in (\ref{eq:CORTICAL_GAN_value_function}) can be written as
\begin{equation}
\mathcal{J}_{\alpha}(G,D) = \alpha \cdot \mathbb{E}_{(\mathbf{x},\mathbf{y}) \sim p_{XY}(\mathbf{x},\mathbf{y})}\biggl[\log \biggl(D(\mathbf{x},\mathbf{y})\biggr)\biggr] -\mathbb{E}_{(\mathbf{x},\mathbf{y}) \sim p_{X}(\mathbf{x}) p_Y(\mathbf{y})}\biggl[D(\mathbf{x},\mathbf{y})\biggr].
\label{eq:CORTICAL_value_function_p}
\end{equation}
Given a generator $G$ that maps the latent (noise) vector $Z$ into $X$, we need firstly to prove that $\mathcal{J}_{\alpha}(G,D)$ is maximized for 
\begin{equation}
D(\mathbf{x},\mathbf{y})=D^*(\mathbf{x},\mathbf{y})=\alpha  \frac{p_{XY}(\mathbf{x},\mathbf{y})}{p_{X}(\mathbf{x}) p_Y(\mathbf{y})}.
\end{equation}
Using the Lebesgue integral to compute the expectation
\begin{equation}
\mathcal{J}_{\alpha}(G, D) = \alpha  \int_{\mathbf{y}} \int_{\mathbf{x}}\biggl[p_{XY}(\mathbf{x},\mathbf{y}) \log \biggl(D(\mathbf{x},\mathbf{y})\biggr) - p_{X}(\mathbf{x}) p_Y(\mathbf{y}) \biggl(D(\mathbf{x},\mathbf{y})\biggr)\biggr] \diff \mathbf{x} \diff \mathbf{y},
\label{eq:CORTICAL_value_function_q}
\end{equation}
taking the derivative of the integrand with respect to $D$ and setting it to $0$, yields the following equation in $D$:
\begin{equation}
\alpha  \frac{p_{XY}(\mathbf{x},\mathbf{y})}{D(\mathbf{x},\mathbf{y})} - p_{X}(\mathbf{x}) p_Y(\mathbf{y})=0,
\end{equation}
whose solution is the optimum discriminator
\begin{equation}
D^*(\mathbf{x},\mathbf{y}) = \alpha  \frac{p_{XY}(\mathbf{x},\mathbf{y})}{p_{X}(\mathbf{x}) p_Y(\mathbf{y})}.
\end{equation}
In particular, $\mathcal{J}_{\alpha}(D^*)$ is a maximum since the second derivative of the integrand $-\alpha  \frac{p_{XY}(\mathbf{x},\mathbf{y})}{D^2(\mathbf{x},\mathbf{y})}$ is a non-positive function.
Therefore, substituting $D^*(\mathbf{x},\mathbf{y})$ in \eqref{eq:CORTICAL_value_function_q} yields
\begin{align}
\mathcal{J}_{\alpha}(G,D^*) = \; & \alpha \int_{\mathbf{y}} \int_{\mathbf{x}}{\biggl[p_{XY}(\mathbf{x},\mathbf{y}) \log \biggl(\alpha  \frac{p_{XY}(\mathbf{x},\mathbf{y})}{p_{X}(\mathbf{x}) p_Y(\mathbf{y})}} \biggr) \nonumber \\
& - p_{X}(\mathbf{x}) p_Y(\mathbf{y}) \biggl(\alpha \frac{p_{XY}(\mathbf{x},\mathbf{y})}{p_{X}(\mathbf{x}) p_Y(\mathbf{y})}\biggr)\biggr] \diff \mathbf{x} \diff \mathbf{y}.
\end{align}
Now, it is simple to recognize that the second term on the right hand side of the above equation is equal to $-\alpha$. Thus, 
\begin{align}
\mathcal{J}_{\alpha}(G,D^*) & = \; \alpha \cdot \mathbb{E}_{(\mathbf{x},\mathbf{y}) \sim p_{XY}(\mathbf{x},\mathbf{y})}\biggl[\log \biggl(\frac{p_{XY}(\mathbf{x},\mathbf{y})}{p_{X}(\mathbf{x}) p_Y(\mathbf{y})} \biggr)\biggr]  + \alpha \log(\alpha) - \alpha \nonumber \\
& = \alpha \bigl(I(X;Y)+\log(\alpha)-1\bigr).
\end{align}
Finally, the maximization over the generator $G$ results in the mutual information maximization since $\alpha$ is a positive constant. We therefore obtain,  
\begin{equation}
\max_{G} \mathcal{J}_{\alpha}(G,D^*)  + \alpha - \alpha \log(\alpha) = \alpha \max_{p_X(\mathbf{x})} {I}(X;Y).
\end{equation}
\end{proof}
Theorem \ref{theorem:CORTICAL_Theorem1} states that at the equilibrium, the generator of CORTICAL has implicitly learned the capacity-achieving distribution with the mapping $\mathbf{x}=G^*(\mathbf{z})$. 
In contrast to the BA algorithm, here the generator samples directly from the optimal input distribution $p_X(\mathbf{x})$ rather than explicitly modeling it. No assumptions on the input distribution's nature are made. Moreover, we have access to the channel capacity directly from the value function used for training as follows:
\begin{equation}
    \label{eq:CORTICAL_capacity}
    C = \frac{\mathcal{J}_{\alpha}(G^*,D^*)}{\alpha} + 1- \log(\alpha).
\end{equation}

In the following, we propose to parametrize $G$ and $D$ with NNs and we explain how to train CORTICAL.

\subsection{Parametric implementation}
\label{subsec:cortical_implementation}
It can be shown (see Sec.4.2 in \cite{Goodfellow2014}) that the alternating training strategy described in Alg. \ref{alg:CORTICAL_1}
converges to the optimal $G$ and $D$ under the assumption of having enough capacity and training time. Practically, instead of optimizing over the space of functions $G$ and $D$, it is reasonable to model both the generator and the discriminator with NNs $(G,D)=(G_{\theta_G},D_{\theta_D})$ and optimize over their parameters $\theta_G$ and $\theta_D$ (see Alg. \ref{alg:CORTICAL_1}).

We consider the distribution of the source $p_Z(\mathbf{z})$ to be a multivariate normal distribution with independent components. The function $\pi (\cdot)$ implements a random derangement of the batch $\mathbf{y}$ and it is used to obtain unpaired samples. We use simple NN architectures and we execute $K=10$ discriminator training steps every generator training step. Details of the architecture and implementation are reported in \cite{CORTICAL_github}, together with animated graphs.

It should be noted that in Theorem \ref{theorem:CORTICAL_Theorem1}, $p_X(\mathbf{x})$ can be subject to certain constraints, e.g., peak and/or average power. Such constraints are met by the design of $G$, for instance using a batch normalization layer. 
Alternatively, we can impose peak and/or average power constraints in the estimation of capacity by adding regularization terms (hinge loss) in the generator part of the value function \eqref{eq:CORTICAL_value_function}, as in constrained optimization problems, e.g., Sec. III of \cite{Faycal2001}. Specifically, the value function becomes
\begin{align}
\mathcal{J}_{\alpha}(G,D) = \; & \alpha \cdot \mathbb{E}_{\mathbf{z} \sim p_{Z}(\mathbf{z})}\biggl[\log \biggl(D\biggl(G(\mathbf{z}),H(G(\mathbf{z}))\biggr)\biggr)\biggr] -\mathbb{E}_{\mathbf{z} \sim p_{Z}(\mathbf{z})}\biggl[D\biggl(G(\mathbf{z}),\pi(H(G(\mathbf{z})))\biggr)\biggr] \nonumber \\ 
& - \lambda_A \max(||G(\mathbf{z})||^2_2-A^2,0) - \lambda_P \max(\mathbb{E}[||G(\mathbf{z})||^2_2]-P,0),
\label{eq:CORTICAL_value_function_lambda}
\end{align}
with $\lambda_A$ and $\lambda_P$ equal to $0$ or $1$.

\begin{algorithm}[t]
\caption{Cooperative Channel Capacity Learning}
\label{alg:CORTICAL_1}
\begin{algorithmic}[1]
\Inputs{$N$ training steps, $K$ discriminator steps, $\alpha$.}
\For{$n=1$ to $N$}
    \For{$k=1$ to $K$}
	\State \multiline{Sample batch of $m$ noise samples $\{\mathbf{z}^{(1)},\dots,\mathbf{z}^{(m)}\}$ from $p_Z(\mathbf{z})$;}
	\State \multiline{Produce batch of $m$ channel input/output paired samples $\{(\mathbf{x}^{(1)},\mathbf{y}^{(1)}),\dots,(\mathbf{x}^{(m)},\mathbf{y}^{(m)})\}$ using the generator $G_{\theta_G}$ and the channel model $H$;}
        \State \multiline{Shuffle (derangement) $\mathbf{y}$ and get input/output unpaired samples $\{(\mathbf{x}^{(1)},\mathbf{\tilde{y}}^{(1)}),\dots,(\mathbf{x}^{(m)},\mathbf{\tilde{y}}^{(m)})\}$;}
        \State \multiline{Update the discriminator by ascending its stochastic gradient:}
        \begin{equation*}
            \nabla_{\theta_D} \frac{1}{m} \sum_{i=1}^{m}{\alpha \log\bigl(D_{\theta_D}\bigl(\mathbf{x}^{(i)},\mathbf{y}^{(i)}\bigr)\bigr)-D_{\theta_D}}\bigl(\mathbf{x}^{(i)},\mathbf{\tilde{y}}^{(i)}\bigr).
        \end{equation*}
\EndFor
    \State \multiline{Sample batch of $m$ noise samples $\{\mathbf{z}^{(1)},\dots,\mathbf{z}^{(m)}\}$ from $p_Z(\mathbf{z})$;}
    \State \multiline{Update the generator by ascending its stochastic gradient:}
        \begin{align*}
            \nabla_{\theta_G} \frac{1}{m} & \sum_{i=1}^{m}{\alpha \log\biggl( D_{\theta_D}\biggl(G_{\theta_G}\bigl(\mathbf{z}^{(i)}\bigr),H\bigl(G_{\theta_G}\bigl(\mathbf{z}^{(i)}\bigr)\bigr)\biggr)\biggr)} \\
            & -D_{\theta_D}\biggl(G_{\theta_G}\bigl(\mathbf{z}^{(i)}\bigr),\pi \bigl(H\bigl(G_{\theta_G}\bigl(\mathbf{z}^{(i)}\bigr)\bigr)\bigr)\biggr).
        \end{align*}
\EndFor
\end{algorithmic}
\end{algorithm}

For an AWGN channel under an average power constraint the exact form of the capacity and the capacity-achieving input distribution are well known. Using CORTICAL to retrieve the optimal Gaussian (of variance $P$) input is not a sufficient proof of concept since one may argue that the neural generator implements a sort of central limit theorem approximation. Therefore, we decide to apply the cooperative framework in three, more representative, non-Shannon's setups.


\section{Applications to non-Shannon scenarios}
\sectionmark{non-Shannon scenarios}
\label{sec:cortical_results}
To demonstrate the ability of CORTICAL to learn the optimal channel input distribution, we evaluate its performance in three non-standard scenarios: 1) the AWGN channel subject to a peak-power constrained input; 2) an additive non-Gaussian noise channel subject to two different input power constraints; and 3) the Rayleigh fading channel known at both the transmitter and the receiver subject to an average power constraint. These are among the few scenarios for which analysis has been carried out in the literature and thus offer a baseline to benchmark CORTICAL. For both the first and third scenarios, it is known that the capacity-achieving distribution is discrete with a finite set of mass points \cite{Smith1971,Tchamkerten2004,Dytso2020}. For the second scenario the nature of the input distribution depends on the type of input power constraint \cite{Fahs2014}. Additional results including the study of the classical AWGN channel with average power constraint are reported in GitHub \cite{CORTICAL_github}.

\subsection{Peak power-limited Gaussian channels}
The capacity of the discrete-time memoryless vector Gaussian noise channel with unit-variance under peak-power constraints on the input is defined as \cite{Rassouli2016}
\begin{equation}
C(A) = \sup_{p_X(\mathbf{x}): ||X||_2\leq A} I(X;Y),
\end{equation}
where $p_X(\mathbf{x})$ is the channel input PDF and $A^2$ is the upper bound for the input signal peak power.
The AWGN Shannon capacity constitutes a trivial upper bound for $C(A)$,
\begin{equation}
C(A) \leq \frac{d}{2}\log_2\biggl(1+\frac{A^2}{d}\biggr),
\end{equation}
while a tighter upper bound for the scalar channel is provided in \cite{McKellips2004}
\begin{equation}
C(A) \leq \min\biggl\{\log_2\biggl(1+\frac{2A}{\sqrt{2\pi e}}\biggr), \frac{1}{2}\log_2\bigl(1+A^2\bigr) \biggr\}.
\end{equation}
\begin{figure}
	\centering
	\includegraphics[scale=0.35]{images/capacity/bifurcation_graph.pdf}
	\caption{AWGN scalar peak-power constrained channel: a) capacity-achieving distribution learned by CORTICAL, the marker's radius is proportional to the PMF; b) capacity estimate and comparisons.}
	\label{fig:CORTICAL_bifurcation}
\end{figure} 
For convenience of comparison and as a proof of concept, we focus on the scalar $d=1$ and $d=2$ channels. 
The first findings on the capacity-achieving discrete distributions were reported in \cite{Smith1971}. For a scalar channel, it was shown in \cite{Sharma2010} that the input has alphabet $\{-A,A\}$ with equiprobable values if $0<A \lessapprox 1.6$, while it has ternary alphabet $\{-A,0,A\}$ if $1.6 \lessapprox A \lessapprox 2.8$.
Those results are confirmed by CORTICAL which is capable of both understanding the discrete nature of the input and learning the support and probability mass function (PMF) for any value of $A$. Notice that no hypothesis on the input distribution is provided during training. Fig. \ref{fig:CORTICAL_bifurcation}a reports the capacity-achieving input distribution as a function of the peak-power constraint $A^2$. The figure illustrates the typical bifurcation structure of the distribution. Fig. \ref{fig:CORTICAL_bifurcation}b shows the channel capacity estimated by CORTICAL with \eqref{eq:CORTICAL_capacity} and compares it with the upper bounds known in the literature.  

\begin{figure}
	\centering
	\includegraphics[scale=0.4]{images/capacity/AWGN_2d_graph_new.pdf}
	\caption{AWGN $d=2$ channel input distributions learned by CORTICAL under a peak-power constraint: a) $A=\sqrt{10}$; b) $A=5$.}
	\label{fig:CORTICAL_heatmaps}
\end{figure} 

\begin{figure}
	\centering
	\includegraphics[scale=0.4]{images/capacity/MIMO_graph_new.pdf}
	\caption{MIMO channel input distributions learned by CORTICAL for different channels $\mathbf{H}$: a) $r_2=1$; b) $r_2=3$. Locus of points satisfying $||\mathbf{H}X||_2= 1$ is also shown.}
	\label{fig:CORTICAL_MIMO}
\end{figure} 
Similar considerations can be made for the $d=2$ channel under peak-power constraints \cite{Rassouli2016}, and in general for the vector Gaussian channel of dimension $d$ \cite{Dytso2019}. 
The case $d=2$ is considered in Fig. \ref{fig:CORTICAL_heatmaps}a ($A=\sqrt{10}$) and Fig. \ref{fig:CORTICAL_heatmaps}b ($A=5$) where CORTICAL learns optimal bi-dimensional distributions, matching the results in \cite{Rassouli2016}. In this case the amplitude is discrete. 

We now consider the MIMO channel where the analytical characterization of the capacity-achieving distribution under a peak-power constraint remains mostly an open problem \cite{Dytso2019-MIMO}. We analyze the particular case of $d=2$: 
\begin{equation}
C(\mathbf{H},r) = \sup_{p_{X}(\mathbf{x}): ||\mathbf{H}X||_2\leq r} I(X;\mathbf{H}X+N),
\end{equation}
where $N \sim \mathcal{N}(0,\mathbf{I}_2)$ and $\mathbf{H} \in \mathbb{R}^{2\times 2}$ is a given MIMO channel matrix known to both transmitter and receiver. We impose $r=1$, and we also impose a diagonal structure on $\mathbf{H}=\text{diag}(1,r_2)$ without loss of generality since diagonalization of the system can be performed. We study two cases: $r_2 = 1$, which is equivalent to a $d=2$ Gaussian channel with unitary peak-power constraint; and $r_2=3$, which forces an elliptical peak-power constraint. The former set-up produces a discrete input distribution in the magnitude and a continuous uniform phase, as shown in Fig. \ref{fig:CORTICAL_MIMO}a. The latter case produces a binary distribution, as shown in Fig. \ref{fig:CORTICAL_MIMO}b. To the best of our knowledge, no analytical results are available for $1<r_2<2$ and thus CORTICAL offers a guiding tool for the identification of capacity-achieving distributions. 

\subsection{Additive non-Gaussian channels}
The nature of the capacity-achieving input distribution remains an open challenge for channels affected by additive non-Gaussian noise. In fact, it is known that under an average power constraint, the capacity-achieving input distribution is discrete and the AWGN channel is the only exception \cite{Fahs2012}. Results concerning the number of mass points of the discrete optimal input have been obtained in \cite{Tchamkerten2004,Das2000}. If the transmitter is subject to an average power constraint, the support is bounded or unbounded depending on the decay rate (slower or faster, respectively) of the noise PDF tail.

\begin{figure}
	\centering
	\includegraphics[scale=0.4]{images/capacity/cauchy_subplot.pdf}
	\caption{AICN channel input distributions learned by CORTICAL at different training steps under: a) logarithmic power constraint; b) peak-power constraint.}
	\label{fig:CORTICAL_cauchy_subplot}
\end{figure} 
In this section, we consider a scalar additive independent Cauchy noise (AICN) channel with scale factor $\gamma$, under a specific type of logarithmic power constraint \cite{Fahs2014}. In particular, given the Cauchy noise PDF $\mathcal{C}(0,\gamma)$
\begin{equation}
    p_N(n) = \frac{1}{\pi \gamma}\frac{1}{1+\bigl(\frac{n}{\gamma}\bigr)^2},
\end{equation}
we are interested in showing that CORTICAL learns the capacity-achieving distribution that solves
\begin{equation}
C(A,\gamma) = \sup_{p_X(\mathbf{x}): \mathbb{E}\bigl[\log\bigl(\bigl(\frac{A+\gamma}{A}\bigr)^2+\bigl(\frac{X}{A}\bigr)^2\bigr)\bigr]\leq \log(4)} I(X;Y),
\end{equation}
for a given $A\geq \gamma$. From \cite{Fahs2014}, it is known that under such power constraint the channel capacity is $C(A,\gamma)=\log(A/\gamma)$ and the optimal input distribution is continuous with Cauchy PDF $\mathcal{C}(0,A-\gamma)$. For illustration purposes, we study the case of $\gamma=1$ and $A=2$ and report in Fig. \ref{fig:CORTICAL_cauchy_subplot}a the input distribution obtained by CORTICAL after $100$ and $10000$ training steps. For the same channel, we also investigate the capacity-achieving distribution under a unitary peak-power constraint. Fig. \ref{fig:CORTICAL_cauchy_subplot}b shows that the capacity achieving distribution is binary.

\subsection{Fading channels}
As the last representative scenario, we study the Rayleigh fading channel subject to an average power constraint $P$ with input-output relation given by
\begin{equation}
    Y = \alpha X + N,
\end{equation}
where $\alpha$ and $N$ are independent circular complex Gaussian random variables, so that the amplitude of $\alpha$ is Rayleigh-distributed.
\begin{figure}
	\centering
	\includegraphics[scale=0.25]{images/capacity/rayleigh_graph.pdf}
	\caption{Optimal input $U$ learned by CORTICAL at different training steps.}
	\label{fig:CORTICAL_rayleigh}
\end{figure} 
It is easy to prove that an equivalent model for deriving the distribution of the amplitude of the signal achieving-capacity is obtained by defining $U=|X|\sigma_{\alpha}/\sigma_N$ and $V=|Y|^2/\sigma_N^2$, which are non-negative random variables such that $\mathbb{E}[U^2]\leq P\frac{\sigma_{\alpha}^2}{\sigma_N^2}\triangleq a$ and whose conditional distribution is
\begin{equation}
    p_{V|U}(v|u) = \frac{1}{1+u^2} \exp \biggl(-\frac{v}{1+u^2}\biggr).
\end{equation}
A simplified expression for the conditional PDF is 
\begin{equation}
\label{eq:CORTICAL_rayleigh_channel}
    p_{V|S}(v|s) = s \cdot \exp (-sv)
\end{equation}
where $S = 1/(1+U^2) \in (0,1]$ and $\mathbb{E}[1/S-1]\leq a$.
From \cite{Faycal2001}, it is known that the optimal input signal amplitude $S$ (and thus $U$) is discrete and possesses an accumulation point at $S=1$ ($U=0)$. We use CORTICAL to learn the capacity-achieving distribution and verify that it matches \eqref{eq:CORTICAL_rayleigh_channel} for the case $a=1$ reported in \cite{Faycal2001}. 

Fig. \ref{fig:CORTICAL_rayleigh} illustrates the evolution of the input distribution for different CORTICAL training steps. Also in this scenario, the generator learns the discrete nature of the input distribution and the values of the PMF. %as predicted in \cite{Faycal2001}. 

\section{Summary}
\label{sec:cortical_conclusions}
In this chapter, we have presented CORTICAL, a novel deep learning-based framework that learns the capacity-achieving distribution for any discrete-time continuous memoryless channel. CORTICAL's framework consists of a cooperative max-max game between two NNs. Non-Shannon channel settings have validated the algorithm. The proposed approach offers a novel tool for studying the channel capacity in non-trivial channels and paves the way for the holistic and data-driven analysis/design of communication systems. 