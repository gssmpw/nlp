%%
%% This is file `sample-manuscript.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `all,proceedings,bibtex,manuscript')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-manuscript.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%%
%% Commands for TeXCount
%TC:macro \cite [option:text,text]
%TC:macro \citep [option:text,text]
%TC:macro \citet [option:text,text]
%TC:envir table 0 1
%TC:envir table* 0 1
%TC:envir tabular [ignore] word
%TC:envir displaymath 0 word
%TC:envir math 0 word
%TC:envir comment 0 0
%%
%%
%% The first command in your LaTeX source must be the \documentclass
%% command.
%%
%% For submission and review of your manuscript please change the
%% command to \documentclass[manuscript, screen, review]{acmart}.
%%
%% When submitting camera ready or to TAPS, please change the command
%% to \documentclass[sigconf]{acmart} or whichever template is required
%% for your publication.
%%
%%
% \documentclass[manuscript,screen,review]{acmart}
\documentclass[sigconf, nonacm=true, 10pt]{acmart}
\usepackage{enumitem}

%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
\setcopyright{acmlicensed}
\copyrightyear{2018}
\acmYear{2018}
\acmDOI{XXXXXXX.XXXXXXX}

%% These commands are for a PROCEEDINGS abstract or paper.
\acmConference[Conference acronym 'XX]{Make sure to enter the correct
  conference title from your rights confirmation emai}{June 03--05,
  2018}{Woodstock, NY}
%%
%%  Uncomment \acmBooktitle if the title of the proceedings is different
%%  from ``Proceedings of ...''!
%%
%%\acmBooktitle{Woodstock '18: ACM Symposium on Neural Gaze Detection,
%%  June 03--05, 2018, Woodstock, NY}
\acmISBN{978-1-4503-XXXX-X/18/06}


%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% For managing citations, it is recommended to use bibliography
%% files in BibTeX format.
%%
%% You can then either use BibTeX with the ACM-Reference-Format style,
%% or BibLaTeX with the acmnumeric or acmauthoryear sytles, that include
%% support for advanced citation of software artefact from the
%% biblatex-software package, also separately available on CTAN.
%%
%% Look at the sample-*-biblatex.tex files for templates showcasing
%% the biblatex styles.
%%

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}


%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{The Science of Evaluating Foundation Models}

\settopmatter{authorsperrow=4}
%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.
\author{Jiayi Yuan}
% \authornote{Both authors contributed equally to this research.}
\email{jy101@rice.edu}
% \orcid{1234-5678-9012}
% \author{G.K.M. Tobin}
% \authornotemark[1]
% \email{webmaster@marysville-ohio.com}
\affiliation{%
  \institution{Rice University}
  \city{}
  \state{}
  \country{}
}

\author{Jiamu Zhang}
\email{mz81@rice.edu}
\affiliation{%
  \institution{Rice University}
  \city{}
  \state{}
  \country{}
}

\author{Andrew Wen}
\email{aw153@rice.edu}
\affiliation{%
  \institution{Rice University}
  \city{}
  \state{}
  \country{}
}

\author{Xia Hu}
\email{xia.hu@rice.edu}
\affiliation{%
  \institution{Rice University}
  \city{}
  \state{}
  \country{}
}


%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
\renewcommand{\shortauthors}{Yuan et al.}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.

\begin{abstract}
  The emergent phenomena of large foundation models have revolutionized natural language processing. However, evaluating these models presents significant challenges due to their size, capabilities, and deployment across diverse applications. Existing literature often focuses on individual aspects, such as benchmark performance or specific tasks, but fails to provide a cohesive process that integrates the nuances of diverse use cases with broader ethical and operational considerations. This work focuses on three key aspects: (1) \textbf{Formalizing the Evaluation Process} by providing a structured framework tailored to specific use-case contexts, (2) \textbf{Offering Actionable Tools and Frameworks} such as checklists and templates to ensure thorough, reproducible, and practical evaluations, and (3) \textbf{Surveying Recent Work} with a targeted review of advancements in LLM evaluation, emphasizing real-world applications.
\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%
% \begin{CCSXML}
% <ccs2012>
%  <concept>
%   <concept_id>00000000.0000000.0000000</concept_id>
%   <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
%   <concept_significance>500</concept_significance>
%  </concept>
%  <concept>
%   <concept_id>00000000.00000000.00000000</concept_id>
%   <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
%   <concept_significance>300</concept_significance>
%  </concept>
%  <concept>
%   <concept_id>00000000.00000000.00000000</concept_id>
%   <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
%   <concept_significance>100</concept_significance>
%  </concept>
%  <concept>
%   <concept_id>00000000.00000000.00000000</concept_id>
%   <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
%   <concept_significance>100</concept_significance>
%  </concept>
% </ccs2012>
% \end{CCSXML}

% \ccsdesc[500]{Do Not Use This Code~Generate the Correct Terms for Your Paper}
% \ccsdesc[300]{Do Not Use This Code~Generate the Correct Terms for Your Paper}
% \ccsdesc{Do Not Use This Code~Generate the Correct Terms for Your Paper}
% \ccsdesc[100]{Do Not Use This Code~Generate the Correct Terms for Your Paper}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
% \keywords{Do, Not, Us, This, Code, Put, the, Correct, Terms, for,
%   Your, Paper}

% \received{20 February 2007}
% \received[revised]{12 March 2009}
% \received[accepted]{5 June 2009}

%%
%% This command processes the author and affiliation and title  
%% information and builds the first part of the formatted document.
\maketitle

\section{Introduction}

As the furor surrounding large language models (LLMs) shifts increasingly from their theoretical capabilities into examinations of practical applicability, relative comparisons between the multitude of models available on the market become ever more important. Questions such as ``between GPT-4, Claude 3.5, and Gemini, which is better?'' are becoming increasingly commonplace as individuals and organizations increasingly look to integrate LLMs into their workflows, particularly as the number of publicly available offerings increases~\cite{achiam2023gpt, yin2024llm, yuan2024large}.  

While at first glance, this might seem like a straightforward question with a simple one-word answer, it is nearly impossible to provide a definitive answer without knowing the specific task context: e.g., whether it is for customer service, code generation, or any other number of applications. This difficulty raises an important question: how do we evaluate LLMs effectively to identify the best choice for given applications? While the importance of rigorous evaluations is widely acknowledged~\cite{liang2022holistic, wang2024dhp}, the current research~\cite{peng2024survey, chang2024survey} lacks a comprehensive and structured discussion on how to systemically approach LLM evaluation, particularly when needing to consider task context. Existing literature often focuses on individual aspects, such as benchmark performance or specific tasks, and there exists no actionable evaluation guideline incorporating a cohesive process that integrates both the nuances of diverse use cases and the broader ethical and operational considerations.

In recognizance of this gap, in this paper, we aim to formalize the evaluation process for LLMs and offer actionable solutions. We do not aim to provide a new method for evaluation nor an exhaustive survey of the field. By breaking down the process step by step and grounding our approach in stringent literature, we aim to provide clarity and utility to researchers, practitioners, and decision-makers alike. Therefore, we named our paper ``The Science of Evaluation'', reflecting our intention to make this scattered effort more systematic, rigid, actionable, and near scientific work. The key contributions of this work are as follows:

\begin{itemize}[leftmargin=*, nosep]
    \item \textbf{Formalizing the Evaluation Process}: We present a structured framework that defines the critical steps and considerations involved in evaluating LLMs, emphasizing the importance of aligning evaluation methods with specific use-case contexts.

    \item \textbf{Providing Actionable Tools and Frameworks}: We introduce practical resources, such as checklists and documentation templates, to guide users through the evaluation process. These tools are designed to ensure evaluations are thorough, reproducible, and aligned with organizational needs.

    \item \textbf{Surveying Recent Work}: While not exhaustive, we feature a targeted survey of recent advancements and methodologies in LLM evaluation, focusing on their application to real-world scenarios.
\end{itemize}

\begin{figure*}
    \centering
    \includegraphics[width=0.7\linewidth]{figure/workflow_v3.pdf}
    \caption{Workflow of evaluation}
    \label{fig1}
\end{figure*}

\section{Preliminary: ABCD in Evaluation}

In this section, we introduce key preliminary concepts essential for understanding the evaluation of LLMs. With the rapid advancements in AI, systemic evaluation of LLMs requires an interdisciplinary approach that spans model design, data utilization, computational infrastructure, and domain-specific knowledge. To organize this diverse set of requirements systematically, we propose the ``\textbf{ABCD} in Evaluation'' framework, representing \textbf{A}lgorithm, \textbf{B}ig Data, \textbf{C}omputation Resources, and \textbf{D}omain Expertise. Each component addresses a fundamental aspect of the evaluation process: the underlying algorithms driving LLMs, the role of vast and diverse datasets in training and testing, the computational and storage requirements for model serving and inference, and the importance of domain-specific knowledge to design meaningful evaluation scenarios. This structured framework provides a comprehensive lens to view the multifaceted nature of LLM evaluation, offering a foundation for the more detailed discussions that follow.

\vspace{-0.5em}
\subsection{Algorithm – Models}

LLMs can be classified into closed-source and open-source models, each with distinct traits influencing deployment, accessibility, and adaptability. Closed-source models (e.g., GPT\footnote{\url{https://chatgpt.com/}}, Claude\footnote{\url{https://claude.ai/}}, and Gemini\footnote{\url{https://gemini.google.com/}}) are proprietary systems delivering strong performance through rigorous optimization. However, their architectures and training data remain hidden, limiting customization and transparency while tying users to external providers for access, pricing, and data privacy considerations.

Conversely, open-source models provide greater transparency, community-driven development, and extensive opportunities for customization. Examples like LLaMA~\cite{touvron2023llama}, Mistral~\cite{jiang2023mistral}, and Qwen~\cite{bai2023qwen} illustrate the diverse tasks and benchmarks they can address. Although they foster innovation and flexibility, open-source solutions can require substantial computational resources and technical expertise to deploy and maintain, posing challenges for organizations with limited infrastructure.

\vspace{-0.5em}
\subsection{Big Data – Evaluation Datasets}

Evaluating LLMs requires access to vast and diverse datasets to ensure robust and meaningful assessments. These datasets serve as the foundation for evaluating models across various dimensions, such as accuracy, robustness, ethical alignment, and domain-specific applicability. 

Large-scale evaluation datasets are essential for covering the breadth of tasks that LLMs are expected to handle, from natural language understanding (e.g., classification and retrieval tasks) to natural language generation (e.g., summarization and translation). Publicly available benchmarks, such as GLUE~\cite{wang2018glue} and SQuAD~\cite{rajpurkar2016squad}, provide standardized datasets for task-specific evaluations, enabling direct comparison across models. In addition to these benchmarks, domain-specific datasets—tailored for fields such as healthcare, legal text analysis, or code generation—play a critical role in evaluating models for specialized applications.

To comprehensively assess a model's capabilities, evaluation datasets must also capture diversity in language, culture, and demographic representation~\cite{liang2023holisticevaluationlanguagemodels}. This ensures that models perform equitably across a wide range of contexts and mitigate potential biases. Furthermore, datasets used in adversarial and safety evaluations help identify vulnerabilities, such as susceptibility to hallucination or ethical violations.

\vspace{-0.5em}
\subsection{Computing and Storage Resources}

Deploying LLMs requires significant computational and storage resources, particularly during the inference (serving) phase. When deploying models in-house for evaluation, it is crucial to consider both the model and data to ensure compatibility with the available hardware infrastructure. Key considerations are as follows,

\begin{itemize}[leftmargin=*, nosep]
    \item \textbf{Model Parameters.} The size of the model, determined by the number of parameters, directly influences memory requirements. For instance, a model with 7 billion parameters requires approximately 28 GB of memory, assuming 4 bytes per parameter. Larger models, such as those exceeding 100 billion parameters, demand exponentially more memory, often necessitating advanced hardware configurations. Selecting a model that aligns with available computational resources is vital for seamless evaluation.
    \item \textbf{GPU Memory.} Adequate GPU memory is essential for efficient inference, as it stores model parameters and facilitates fast computations. High-performance GPUs, such as NVIDIA’s A100 with 80 GB of memory or H100 with extended memory capacities, are widely used for deploying large models. For particularly large-scale models, distributed clusters of GPUs are required to handle memory-intensive operations, enabling parallel processing and reduced latency during inference.
    \item \textbf{Storage.} Storage capacity is another critical factor in deploying LLMs, as it must accommodate both the model parameters and associated datasets. High-speed storage solutions, such as NVMe SSDs, significantly enhance data retrieval times, improving overall system performance. While local storage is optimal for performance-critical tasks, network-attached storage (NAS) or cloud-based solutions can provide scalability and accessibility, particularly for collaborative projects or scenarios requiring extensive backup and sharing capabilities.

\end{itemize}

We summarize the relationship between model size, memory requirements, and approximate inference speed in Table~\ref{tab:resource}. 
The memory requirements for deploying large language models (LLMs) follow a general rule of thumb: loading the weights of a model with $\textbf{X}$ billion parameters requires approximately $2\times\textbf{X}$ GB of VRAM in bfloat16/float16 precision. Inference speed, while dependent on hardware configurations, optimization techniques, and model architectures, can vary significantly. The values provided in the table are approximate and intended as general guidance for planning computational resources.
By carefully planning for computational and storage needs, users can ensure efficient deployment and evaluation of LLMs.

\begin{table}[h!] 
\centering 
\caption{Model size, memory requirements, and approximate inference speed for various LLMs. Note that the numbers can vary based on hardware configurations, optimization techniques, and specific model architectures. The provided values are approximate and intended for general guidance.} 
\resizebox{\linewidth}{!}{
\begin{tabular}{c|c|c} 
\toprule \textbf{Model Size} & \textbf{Memory Required} & \textbf{Inference Speed} \\ 
(Parameters) & (GB) & (Tokens/s) \\
\midrule 345M & 0.69 & \textasciitilde1,000 \\ 1.3B & 2.6 & \textasciitilde600 \\ 2.7B & 5.4 & \textasciitilde500 \\ 6B & 12 & \textasciitilde350 \\ 7B & 14 & \textasciitilde300 \\ 13B & 26 & \textasciitilde200 \\ 30B & 60 & \textasciitilde100 \\ 70B & 140 & \textasciitilde50 \\ 175B & 350 & \textasciitilde20 \\ \bottomrule 
\end{tabular} 
}
\vspace{-0.5em}
\label{tab:resource} 
\end{table}

\subsection{Domain Expertise}

Domain expertise is crucial in evaluating LLMs, ensuring assessments are contextually relevant and aligned with specific application requirements. Experts guide the selection of evaluation metrics tailored to particular domains, enhancing the relevance of assessments. They also conduct human evaluations, providing qualitative insights into model outputs that automated metrics may miss. In high-stakes fields like healthcare, experts assess the accuracy and appropriateness of LLM-generated recommendations, identifying nuanced failure cases and offering actionable feedback for model improvement. This integration of domain knowledge bridges the gap between technical performance metrics and real-world applicability, underscoring the importance of multidisciplinary collaboration in advancing LLM evaluation.~\cite{tam2024framework}

Incorporating domain expertise into the evaluation process ensures that LLMs are rigorously tested and refined to meet the practical demands of their intended applications. Experts help develop more robust, reliable, and ethically sound AI systems by aligning technical assessments with domain-specific standards. This collaborative approach is essential for the responsible deployment of LLMs across various industries.~\cite{szymanski2024comparing}


\section{Dimensions of Evaluation}

\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{figure/dimension_v4.pdf}
    \caption{Dimensions of Evaluation}
    \label{fig2}
\end{figure*}

\subsection{Performance Metrics} 

In this section, we explore the evaluation of model capabilities across a wide spectrum of general domain NLP tasks, ranging from foundational tasks like classification and extraction that test basic language understanding to more intricate challenges such as advanced inference and summarization.

% \textcolor{red}{Jiamu: make each subsection into one paragraph}

\subsubsection{Natural Language Understanding} \hfill\\ 
\vspace{-.7em}

\textbf{Text classification} tasks are among the most fundamental in natural language understanding, requiring models to assign predefined labels to text inputs. This includes various applications, such as sentiment analysis, topic classification, spam detection, and intent recognition. Benchmarks such as SST-2 (Stanford Sentiment Treebank) \cite{socher2013recursive}, AG News \cite{zhang2015character}, and IMDB Reviews \cite{maas2011learning} are commonly used for evaluating sentiment and topic classification. Frameworks like HELM \citep{liang2023holisticevaluationlanguagemodels} combine the abstract taxonomy of scenarios and metrics with a clear set of practical selections of implemented scenarios, prioritizing coverage, value, and feasibility. \textbf{Entity/Word Extraction} are tasks involving entity or word extraction that require models to identify and label entities or specific spans of text within a document. This category encompasses named entity recognition (NER), part-of-speech tagging, and keyword extraction. Benchmarks such as CoNLL-2003 (for NER) \cite{sang2003introduction} and OntoNotes 5.0 \cite{pradhan2013towards} are widely used in this domain. These datasets challenge models to recognize names, locations, dates, and other key information in text, often reflecting real-world scenarios like legal or medical document analysis. \textbf{Natural Language Inference} (NLI) tasks evaluate a model's ability to determine logical relationships between pairs of sentences, such as entailment, contradiction, or neutrality. \citet{qin2023chatgptgeneralpurposenaturallanguage} evaluate ChatGPT's zero-shot learning ability on NLI tasks, and Lee et al. \citep{lee2023largelanguagemodelscapture} found that LLMs struggle with NLI tasks and fail to capture human disagreement, both highlighting its strengths and limitations. Popular datasets for NLI include SNLI (Stanford Natural Language Inference) \cite{bowman2015large}, and MultiNLI \cite{williams2017broad}, which feature sentence pairs across various domains. These benchmarks assess the reasoning capabilities of models, requiring them to comprehend context, infer unstated connections, and resolve ambiguities. Retrieval and ranking tasks test a model's ability to identify and rank the most relevant documents or passages from a corpus given a query. Datasets like MS MARCO \cite{bajaj2018msmarcohumangenerated}, TREC \cite{wang-etal-2007-jeopardy}, and Natural Questions (NQ) \cite{kwiatkowski-etal-2019-natural} are frequently used to evaluate these tasks. These benchmarks are particularly critical for search engines and question-answering systems.

\subsubsection{Natural Language Generation} \hfill\\
\vspace{-.7em}

\textbf{Summarization tasks} require models to condense long documents into concise summaries while retaining key information. Benchmarks such as CNN/Daily Mail \cite{nallapati2016abstractive}, XSum \cite{narayan-etal-2018-dont}, and Gigaword \cite{Rush_2015} are commonly used for this purpose. Summarization can be extractive (selecting key sentences) or abstractive (generating new, concise text). Recent studies suggest that LLMs demonstrate general proficiency in summarization tasks, with performance varying across model architectures and configurations. For instance, \citet{liang2022holistic} observed that TNLG v2 (530B) achieves state-of-the-art results, surpassing models like OPT (175B) and fine-tuned Bart. These findings highlight the growing potential of evaluating LLMs for summarization. \textbf{Text Completion} tasks challenge models to generate coherent and contextually appropriate continuations for a given prompt. OpenAI’s GPT-3 benchmarks \cite{brown2020language} for completion often rely on tasks involving story or sentence completion, and datasets like WikiText \cite{merity2016pointer} or BooksCorpus \cite{zhu2015aligningbooksmoviesstorylike} are commonly used. \textbf{Question Answering (QA)} tasks test a model's ability to provide precise and relevant answers to posed questions based on a context passage or general knowledge. Benchmarks like SQuAD (Stanford Question Answering Dataset) \cite{rajpurkar-etal-2016-squad}, Natural Questions \cite{kwiatkowski-etal-2019-natural}, and TriviaQA \cite{joshi-etal-2017-triviaqa} are widely recognized in this area. \textbf{Machine translation} tasks involve translating text from one language to another, serving as a cornerstone application for many language models. Benchmarks like WMT \cite{kocmi-etal-2024-findings} provide datasets that span multiple language pairs, allowing the evaluation of translation capabilities. Recent research highlights the growing potential of LLMs in such a domain. \citet{wang2023documentlevelmachinetranslationlarge} reveal that GPT-4 and ChatGPT achieve strong human-evaluated performance, often surpassing commercial machine translation systems and many document-level neural machine translation models.

\subsection{Robustness and Reliability} 

In this section, we discuss how to evaluate a model’s robustness and reliability, focusing on two main types of challenges: natural perturbations and adversarial attacks.

\subsubsection{Natural Perturbations} \hfill\\
\vspace{-.7em}

Evaluating robustness to natural perturbations examines how models perform under real-world variations in data distribution and input quality. Distribution shifts occur when the test data diverges from the training data, a common issue in applications like sentiment analysis or machine translation, where language use varies across regions, demographics, and platforms. Benchmarks like WILDS \cite{koh2021wildsbenchmarkinthewilddistribution} provide curated datasets reflecting these shifts, such as shifts in medical imaging data or demographic-specific Reddit comments. Noisy inputs include typographical errors, altered phrasing, or incomplete data that mimic real-world scenarios like chatbots encountering user typos. Benchmarks such as the NoiseQA \cite{ravichander-etal-2021-noiseqa} dataset for question answering or the TextFlint \cite{wang-etal-2021-textflint} toolkit for systematic noise injection simulate these challenges. The robustness of LLMs to prompts is among the most critical aspects of their evaluation. To assess this, Zhu et al. introduced a unified evaluation framework called PromptBench \cite{zhu2024promptbenchunifiedlibraryevaluation}, which comprehensively measures LLM robustness across four attack levels: character, word, sentence, and semantic. Additionally, Wang et al. proposed a novel multi-task benchmark, AdvGLUE++ \cite{wang2022adversarialgluemultitaskbenchmark}, specifically designed to evaluate LLM robustness against adversarial datasets. Both research studies demonstrate that Large Language Models are vulnerable to adversarial perturbations.

\subsubsection{Designed Adversarial Attacks} \hfill\\
\vspace{-.7em}

Adversarial attacks involve carefully crafted inputs designed to exploit model vulnerabilities, presenting a distinct challenge compared to natural perturbations. Textual adversarial attacks like word-level substitution, paraphrasing, or syntactic manipulation aim to deceive models into producing incorrect outputs without altering the meaning of the input. For instance, the TextFooler \cite{jin2020bertreallyrobuststrong} algorithm modifies keywords to retain semantics while misleading models and benchmarks like AdvGLUE \cite{wang2022adversarialgluemultitaskbenchmark} integrate adversarially perturbed data to stress-test systems. Gradient-based adversarial attacks exploit the internals of the model to generate adversarial examples. For instance, methods like HotFlip \cite{ebrahimi-etal-2018-hotflip} leverage gradients to identify critical words to perturb, directly targeting neural architectures like transformers. Evaluation often combines metrics such as adversarial robustness (accuracy post-attack) and perturbation cost (measuring the effort required to deceive the model). Research into countermeasures, such as adversarial training (e.g. adversarially augmented datasets: RobustBench \cite{croce2021robustbenchstandardizedadversarialrobustness}), aims to fortify models while maintaining general performance on clean data.

\subsection{Ethical and Fairness Considerations}

Ensuring that the outputs of LLMs adhere to well-defined ethical principles and fairness standards is not just necessary—it's imperative. These principles extend beyond the basic requirement of avoiding discriminatory outcomes; they also encompass the need for defining equitable treatment, respecting the autonomy and dignity of all individuals, and ensuring that system behaviors are in harmony with universally recognized human values. To effectively address these complex issues, we categorize the considerations into two fundamental types: social bias, which pertains to the model's behavior in a broader societal context, and individual fairness, which focuses on the fair treatment of each person.

\subsubsection{Social Bias} \hfill\\
\vspace{-.7em}

Social bias in language models refers to systematic prejudices embedded in their outputs, often reflecting biases present in training data. These biases manifest as gender, racial, or cultural stereotypes and pose significant risks when deploying models in sensitive applications such as hiring, healthcare, or legal systems. For instance, models trained on web-scraped data may disproportionately associate certain groups with negative contexts or perpetuate outdated stereotypes, potentially leading to harmful outcomes. To quantify and address these biases, various benchmarks and datasets have been developed. Bias-in-Bios~\cite{de2019bias}, StereoSet~\cite{nadeem2020stereoset}, and CrowS-Pairs~\cite{nangia2020crows} evaluate biases across diverse contexts. Social Bias Probing~\cite{manerba2023social} introduces a large-scale dataset and perplexity-based fairness score to analyze LLMs’ associations with societal categories and stereotypes. TWBias~\cite{hsieh2024twbias} focuses on biases in Traditional Chinese LLMs, incorporating chat templates to assess gender and ethnicity-related stereotypes within Taiwan's context. Similarly, BBQ (Bias Benchmark for QA)~\cite{parrish2021bbq} provides question sets to reveal social biases against protected classes in U.S. English-speaking contexts. These tools highlight the need for robust evaluations to mitigate social biases in AI systems. 

\subsubsection{Individual Fairness} \hfill\\
\vspace{-.7em}

Individual fairness emphasizes that similar individuals or inputs should receive consistent and equitable treatment from models, regardless of sensitive attributes such as gender, ethnicity, or age. This principle ensures that two inputs differing only in protected attributes yield equivalent predictions or scores. For instance, in a job recommendation system, the model should provide comparable job listings for resumes with similar qualifications, regardless of names that may indicate different genders. Datasets like ADULT~\cite{adult_2}, commonly used for income prediction, and COMPAS~\cite{dieterich2016compas}, utilized for recidivism risk prediction, are often employed to study individual fairness. These datasets enable researchers to evaluate biases that may arise in model predictions, offering valuable insights into whether models uphold equitable outcomes in practical scenarios.

\subsection{Explainability and Interpretability}

Technically, evaluating explanations involves human or automated model approaches. Human
evaluations assess plausibility via the similarity between model rationales and human rationales or subjective judgments. However, these methods usually overlook faithfulness~\cite{zhao2024explainability}.

\subsubsection{Plausibility} \hfill\\
\vspace{-.7em}

Evaluating the plausibility of LLM explanations involves assessing how well they align with human reasoning and expectations. Plausibility is often measured at the input text or token level, considering dimensions such as grammar, semantics, knowledge, reasoning, and computation~\cite{shen2022interpretability}. For local explanations, metrics such as Intersection-Over-Union (IOU), precision, recall, F1 score, and area under the precision-recall curve (AUPRC) are commonly used to compare predicted rationales with human-annotated ones~\cite{deyoung2019eraser}. These metrics gauge whether explanations are sufficient and compact, meaning they contain just enough information to support correct predictions without redundancy. Recent studies have also explored counterfactual simulatability in prompting paradigms—whether explanations help humans predict model behavior on diverse inputs. Metrics like simulation generality (diversity of counterfactuals) and simulation precision (alignment between human predictions and model outputs) reveal the limitations of current approaches. For instance, explanations from GPT-3.5 and GPT-4 often mislead humans, forming inaccurate mental models~\cite{chen2023models}. This highlights the necessity for robust methods that go beyond merely optimizing for subjective plausibility, ensuring that explanations truly augment human understanding.


\subsubsection{Faithfulness} \hfill\\
\vspace{-.7em}

Faithfulness examines whether explanations accurately reflect the model’s internal reasoning. Quantitative metrics like comprehensiveness (change in predicted probability after removing top tokens) and sufficiency (effectiveness of extracted rationales for prediction) are widely used~\cite{deyoung2019eraser}. Other measures, such as Decision Flip - Fraction Of Tokens (DFFOT) and Decision Flip - Most Informative Token (DFMIT), evaluate the influence of individual tokens on predictions~\cite{chrysostomou2021improving}. In the prompting paradigm, studies highlight that explanations, such as chain-of-thought (CoT) reasoning, can be systematically unfaithful. For instance, \citet{turpin2024language} showed that GPT-3.5 and Claude 1.0 failed to acknowledge biases in few-shot prompts, generating misleading rationales. Smaller models often produce more faithful explanations than larger ones, indicating a trade-off between model capability and reasoning transparency~\cite{lanham2023measuring}. To enhance faithfulness, decomposition methods that break tasks into subquestions have shown promise, improving alignment with underlying decision-making processes~\cite{radhakrishnan2023question}. These findings emphasize the need for robust evaluation frameworks to ensure explanations genuinely reflect the reasoning behind predictions.

\subsection{Safety and Controllability}

The evaluation of safety and controllability is critical, especially in high-stakes scenarios such as healthcare, legal systems, and financial applications. In these domains, outputs from LLMs can have profound real-world consequences, making it imperative to ensure they do not produce unsafe, erroneous, or harmful content. This section provides an in-depth examination of benchmarks and methodologies for evaluating safety, concentrating on addressing hallucination and the potential for misuse.

\subsubsection{Hallucination} \hfill\\
\vspace{-.7em}

A hallucination occurs when LLMs produce content that is factually incorrect, logically unsound, or fabricated, posing substantial risks in domains such as healthcare and law. In medical scenarios, faulty drug interactions or diagnoses could lead to severe patient harm, while in legal settings, fabricated references to case law or statutes may undermine the integrity of judicial processes. Several benchmarks have been introduced to measure and address hallucination. The Hallucination Leaderboard by Vectara~\cite{Hughes_Vectara_Hallucination_Leaderboard_2023} utilizes the Hughes Hallucination Evaluation Model (HHEM-2.1) to gauge hallucination frequency and factual consistency in document summaries. HaluEval~\cite{li2023halueval} comprises thousands of queries and task-specific examples to assess LLMs’ ability to detect fabricated information in QA, dialogue, and summarization. The Hallucinations Leaderboard by Hugging Face~\cite{hallucinations-leaderboard} evaluates LLMs on tasks like open-domain QA and fact-checking, while LongHalQA~\cite{qiu2024longhalqa} introduces long-context scenarios for multimodal models (MLLMs). AMBER~\cite{wang2023llm} tests for various hallucination types across both generative and discriminative tasks with efficient methods.

\subsubsection{Misuse and Risk} \hfill\\
\vspace{-.7em}

Misuse evaluation addresses scenarios where LLMs are deliberately employed to produce harmful, deceptive, or unethical outputs, such as misinformation campaigns, propaganda, or phishing attempts. In these high-stakes environments, it is essential to ensure that models remain robust and fail-safe when prompted with malicious inputs, thereby preventing the generation of unsafe content. Several benchmarks have been developed to assess and mitigate these risks. A proposed risk taxonomy and assessment framework~\cite{cui2024risk} systematically dissects potential threats by examining four modules—input, language model, toolchain, and output—and suggests targeted mitigation strategies. R-Judge~\cite{yuan2024r} evaluates models’ capacity to detect safety risks within multi-turn agent interactions. S-Eval~\cite{yuan2024s} introduces an LLM-based approach for large-scale safety evaluation, using 220,000 prompts to scrutinize various risk categories and adversarial instructions. AgentHarm~\cite{andriushchenko2024agentharm} focuses on LLM agents’ resilience to misuse, testing 110 detailed behaviors across 11 harm categories. Together, these tools furnish a comprehensive framework for risk detection and mitigation, guiding the development of more secure and trustworthy AI systems.

\section{Evaluation Methodologies}

\begin{figure*}
    \centering
    \includegraphics[width=0.8\linewidth]{figure/method.pdf}
    \caption{Evaluation Methodologies}
    \label{fig3}
\end{figure*}


\subsection{Quantitative Eval. for Objective Tasks}

Objective tasks are predominantly associated with natural language understanding (NLU) applications, where clear ground truth labels are available. Models are evaluated based on their ability to accurately replicate or predict these labels, enabling precise comparisons across different systems. These evaluations vary depending on the specific goals of each task, with distinct metrics tailored to capture performance effectively. In the following, we outline common NLU tasks and the metrics used to evaluate them. 

In tasks such as sentiment analysis, topic classification, and named entity recognition (NER), models are assessed using metrics like accuracy, precision, recall, and F1-score, which collectively provide a comprehensive view of performance. For instance, accuracy measures the proportion of correct predictions, while precision and recall address the trade-off between relevance and completeness in the results. % Benchmarks such as SST-2 \cite{socher2013recursive} (sentiment analysis) and CoNLL-2003 \cite{tjong-kim-sang-de-meulder-2003-introduction} (NER) frequently employ these metrics for performance evaluation.
% \subsubsection{Ranking and Retrieval Metrics}
In information retrieval and passage ranking tasks, where models are tasked with ordering outputs by relevance, metrics like Mean Reciprocal Rank (MRR) \cite{Craswell2009}, Normalized Discounted Cumulative Gain (nDCG) \cite{wang2013theoreticalanalysisndcgtype}, and Average Precision at k (AP@k) are commonly used. For example, in MS MARCO \cite{bajaj2018msmarcohumangenerated}, models are evaluated based on their effectiveness in ranking relevant documents at the top of the search results, and they reward both the precision of the highest-ranking results and the overall quality of the ranking.
% \subsubsection{Exact Match (EM) for QA Tasks}
For extractive question-answering tasks, metrics such as Exact Match (EM) assess whether the model’s output perfectly matches the ground truth, while F1-score evaluates partial overlap between predicted and true answers. %Datasets like SQuAD \cite{rajpurkar-etal-2016-squad} and Natural Questions \cite{kwiatkowski-etal-2019-natural} rely on these metrics to provide a robust measure of a model’s ability to retrieve precise and relevant information.

\subsection{Quantitative Eval. for Subjective Tasks}

Subjective tasks are more common in natural language generation (NLG) applications, where outputs are evaluated for qualities such as fluency, coherence, and semantic fidelity. Since ground truth in these tasks is often open to interpretation, evaluation relies on approximate metrics designed to capture content quality and similarity to reference outputs. To address the diverse requirements of NLG tasks, various metrics have been developed to evaluate different dimensions of content quality. These metrics can be broadly categorized into lexical, semantic, and diversity-based measures, each focusing on specific aspects of the generated text. Below, we discuss these categories in detail.

\subsubsection{Content Quality}  \hfill\\
\vspace{-.7em}

\textbf{Lexical Metrics}: Metrics like ROUGE \cite{lin-2004-rouge} (Recall-Oriented Understudy for Gisting Evaluation) and BLEU \cite{papineni2002bleu} (Bilingual Evaluation Understudy) measure lexical overlap between model outputs and reference texts. ROUGE is commonly used in summarization tasks, focusing on recall of n-grams, while BLEU, often applied in machine translation, emphasizes precision of n-gram matches. These metrics, even though they are straightforward, may still fail to account for semantic equivalence when lexical overlap is low.

\textbf{Semantic Metrics}:
To address the limitations of lexical metrics, semantic similarity measures like BERTScore \cite{zhang2020bertscoreevaluatingtextgeneration} and METEOR \cite{banerjee-lavie-2005-meteor} have gained popularity. BERTScore uses embeddings from large pre-trained models (e.g., BERT) to calculate token-level similarity, capturing meaning rather than surface forms. METEOR incorporates stemming and synonyms, improving evaluation for tasks like paraphrase generation and summarization.

\textbf{Diversity and Novelty Metrics}:
In creative tasks, such as storytelling or dialogue generation, metrics like Distinct-n \cite{li-etal-2016-diversity} measure the diversity of generated outputs by counting unique n-grams. Novelty assesses the deviation of the output from training data or references, ensuring models produce varied and original content.

\textbf{Quality Trade-offs in Subjective Tasks}:
Subjective task metrics often reflect trade-offs between coherence, relevance, and diversity. For example, a model optimizing for BLEU \cite{papineni2002bleu} may sacrifice creativity in favor of exact matches, while prioritizing BERTScore \cite{zhang2020bertscoreevaluatingtextgeneration} might enhance semantic fidelity at the cost of diversity. Balancing these trade-offs is a critical aspect of evaluating LLM-generated outputs.

\subsubsection{Factuality and Truthfulness} \hfill\\
\vspace{-.7em}

Ensuring factual accuracy and truthfulness is a critical aspect of evaluating language models, particularly in applications such as open-domain question answering, summarization, and conversational AI. Emerging metrics for factuality, including entailment-based metrics such as FactCC \cite{kryscinski-etal-2020-evaluating} and DAE \cite{goyal-durrett-2020-evaluating}, evaluate whether models generate factually accurate and truthful information. In addition, FEQA \cite{durmus-etal-2020-feqa} and QAGS \cite{wang-etal-2020-asking}, which leverage question generation and answering (QGA) techniques, serve as factuality metrics. These metrics are particularly critical for tasks such as open-domain question answering, summarization, and conversational AI, where hallucinations or fabricated content can significantly undermine user trust. 

Factuality extends beyond verifying the correctness of information; it also involves a thorough evaluation of whether the outputs are ethically aligned, fair, and consistently reliable under diverse conditions. To comprehensively address these broader concerns, we delve into two critical subsets of factuality: Ethics and Bias, and Trust and Calibration.

\textbf{Ethical and Bias}:
Metrics such as the fairness score and the bias amplification ratio aim to quantify the ethical alignment of models \cite{foulds2019intersectionaldefinitionfairness}. These metrics evaluate whether outputs perpetuate harmful stereotypes or exhibit fairness across demographic groups. For example, the Winogender schema tests whether pronoun resolution is influenced by gender stereotypes, and metrics like Equalized Odds measure the consistency of model predictions across protected attributes \cite{wang2024decodingtrustcomprehensiveassessmenttrustworthiness}. In addition, the Generalized Entropy Index \cite{speicher2018unified} provides a versatile framework for quantifying inequality, capturing disparities in model performance or outcomes across different demographic groups. These metrics are crucial for ensuring fairness and mitigating biases in AI systems.

\textbf{Trust and Calibration}:
Metrics such as Expected Calibration Error (ECE) assess whether the model's confidence scores align with the actual prediction accuracy \cite{guo2017calibrationmodernneuralnetworks}. Well-calibrated models are essential in high-stakes applications where overconfidence or underconfidence in predictions can have severe consequences. Additionally, metrics like robustness to adversarial prompts assess the model's reliability when tackled with adversarial scenarios or challenging inputs \cite{zhu2024promptrobustevaluatingrobustnesslarge}. Furthermore, the AUC of the selective accuracy and coverage provides a comprehensive measure of the trade-off between the accuracy of predictions and the proportion of covered data points, which allows the evaluation of model reliability in selective prediction tasks \cite{geifman2017selectiveclassificationdeepneural}.

\subsubsection{Emerging Metrics} \hfill\\
\vspace{-.7em}

There are also some very new metrics specifically designed for day-to-day usage with LLMs. For example, DRFR~\cite{qin2024infobench} evaluates the ability to follow instructions, while Human-AI Language-based Interaction Evaluation (HALIE)~\cite{lee2022evaluating} underscores the importance of assessing the interactive process itself. With the rise of more interactive AI applications, AntEval~\cite{liang2024anteval} has been proposed to assess social interaction competencies in LLM-driven agents. AntEval establishes complex multi-agent environments that encourage information exchange and intention expression, providing metrics such as Information Exchanging Precision (IEP) and Interaction Expressiveness Gap (IEG) to quantitatively measure interaction skills. These newer metrics emphasize the naturalness and responsiveness of LLMs in realistic, often open-ended scenarios—like conversational or collaborative tasks—and thereby complement more traditional, static evaluation approaches.
% None

\subsection{Qualitative Evaluation}

Qualitative evaluation focuses on human judgments and interpretative assessments of model output, providing insights that quantitative metrics often miss. These approaches are particularly useful for capturing nuances like contextual appropriateness, creativity, and ethical considerations. They involve subjective evaluation criteria and often require human annotators or expert reviewers. Fairness metrics include Demographic Parity (measuring uniform prediction distributions across groups), Equalized Odds (ensuring similar error rates across groups), and Counterfactual Fairness, which evaluates outcomes in counterfactual scenarios where sensitive attributes are altered \cite{wang2024decodingtrustcomprehensiveassessmenttrustworthiness}. 

\subsubsection{Human Evaluation}

Human evaluation remains the gold standard for assessing model output in tasks such as open-ended text generation, dialogue systems, and creative writing. Annotators are asked to rate the model output in predefined dimensions such as fluency, relevance, coherence, and engagement~\cite{liang2022holistic}.
For example, QUEST~\cite{tam2024framework} is a comprehensive framework for the human evaluation of LLMs in healthcare, and LalaEval~\cite{sun2024lalaeval} offers a holistic human evaluation framework for domain-specific LLMs.

\subsubsection{Case Study and Error Analysis}

Qualitative approaches also emphasize case studies and error analysis, where researchers manually inspect model output to understand specific failures and limitations. For example, in high-stakes domains like healthcare or law, analysts can examine whether models provide incorrect or misleading recommendations, offering insights into robustness and safety concerns. By categorizing errors into types, such as factual inaccuracies or ethical violations, error analysis can guide targeted improvements in model design. Error Analysis Prompting~\cite{lu2023error} is a method that enables LLMs to perform human-like translation evaluations. ~\citet{alemayehu2024error} conducted an error analysis of multilingual language models in machine translation, focusing on English-Amharic translation. 


\section{Framework for Evaluations}

\begin{table*}[h!]
\centering
\caption{Checklist for Preparing LLM Evaluations}
\resizebox{\textwidth}{!}{
\begin{tabular}{p{0.3\textwidth}|p{0.7\textwidth}}
\toprule
\textbf{Step} & \textbf{Description} \\
\midrule
\textbf{Define Objectives (D)} & Specify the LLM tasks and key criteria (e.g., robustness, fairness). Align objectives with domain needs to ensure context relevance. \\
\midrule
\textbf{Prioritize Dimensions (D)} & Assign relative importance to each evaluation dimension (e.g., accuracy, interpretability) based on domain-specific requirements. Document these priorities for transparency. \\
\midrule
\textbf{Select Datasets (B)} & Leverage diverse, representative datasets for real-world usage; include specialized domain datasets where appropriate to capture task complexity. \\
\midrule
\textbf{Identify Metrics (A, D)} & Choose suitable quantitative (e.g., accuracy, F1-score) and qualitative (e.g., expert human ratings) measures. Match metrics to the algorithm’s capabilities and the domain’s nuances. \\
\midrule
\textbf{Establish Baselines (A)} & Specify baseline models or benchmarks for algorithmic comparisons, covering both generic and domain-specific contexts. \\
\midrule
\textbf{Address Ethics \& Safety (D)} & Integrate fairness, bias, and safety checks into evaluation, particularly for high-impact or sensitive domain scenarios. \\
\midrule
\textbf{Allocate Resources (C)} & Assess computational and storage requirements in line with model size and data volume. Adjust evaluation plans based on available hardware. \\
\midrule
\textbf{Document the Process (A, B, C, D)} & Maintain transparent records of objectives, dataset sources, metrics used, resource decisions, and domain priorities. \\
\midrule
\textbf{Iterate \& Refine (A, B, C, D)} & Revisit evaluation strategies as insights emerge and requirements evolve, adjusting across algorithmic, data, resource, and domain dimensions. \\
\bottomrule
\end{tabular}
}
\label{tab:checklist}
\end{table*}

Evaluating large language models (LLMs) effectively is a nuanced process involving the selection of appropriate benchmarks, the identification of meaningful metrics, and the careful consideration of resource constraints and domain-specific needs. In this section, we propose a structured framework that guides practitioners through three stages: (1) establishing a checklist for evaluation preparation, (2) conducting applicability analysis and iterative refinement, and (3) maintaining comprehensive documentation for transparency and longevity. By following this framework, evaluators can systematically approach LLM assessment, ensuring that each evaluation is both task-appropriate and orderly documented.

\subsection{Checklist for Evaluation Preparation}

The checklist of evaluating an LLM is guided by the core \textbf{ABCD} principles—\textbf{A}lgorithm, \textbf{B}ig Data, \textbf{C}omputational Resources, and \textbf{D}omain Expertise. Table~\ref{tab:checklist} outlines a concise sequence of steps to ensure a solid foundation for your evaluations. By defining objectives and priorities early, practitioners can more efficiently align each step of the process with relevant algorithmic choices, data considerations, resource constraints, and domain-specific requirements.

Beginning with this ABCD-aligned checklist ensures a structured evaluation roadmap. By explicitly referencing \textbf{A}lgorithm, \textbf{B}ig Data, \textbf{C}omputational Resources, and \textbf{D}omain Expertise at each step, practitioners can tailor their approach to the specific modeling frameworks, data requirements, resource constraints, and context-critical considerations that define successful LLM evaluations.

\subsection{Applicability Analysis and Refinement}

In the preceding subsections, we have discussed various approaches to evaluating each of these dimensions. Although it is certainly true that it is desirable for an LLM to perform well across all evaluation dimensions, some models will inevitably excel in certain areas while being less effective in others. A comprehensive execution of these various benchmarks, depending on the size of the data involved, can be extremely resource consumptive, to the point of being prohibitive for practical implementation.

Similarly, in applications where domain specificity is required or important for evaluation (e.g., law and healthcare), datasets will likely need to be prepared for each evaluation. This is particularly pertinent to healthcare, as documentation practices differ significantly between individual healthcare institutions, causing substantial variations in task performance for data-driven algorithms. To require that localized datasets for every combination of dimension and evaluation methodology be created is thus largely infeasible.

We posit, however, that not all dimensions are necessary for any given task, or at the very least, only a subset of the evaluations within each dimension may be required. For instance, in use cases where the LLM is employed as a feature extraction method on controlled/internal datasets, model safety against adversarial attacks and faithfulness to its generated explanations may not be prioritized to the same degree as raw task performance. Conversely, in use cases where the LLM is used for synthetic data generation, interpretability or explainability during data generation might be largely irrelevant, while robustness against distributional shifts over time becomes a key consideration. Additional concerns, depending on the use case, may also focus on the ethical alignment and fairness of the generated outputs.

When considering the evaluation of LLMs, it is therefore critical to recognize the relative importance of each evaluation dimension (and/or sub-component) for a particular task, and, especially in resource-constrained environments, to selectively prune and refine the evaluations that are actually conducted. Even in unconstrained scenarios, having an internal weighting of the importance of these dimensions is valuable, as it guides comparisons between different models. Given the inherently subjective nature of such weighting, it is also essential to ensure that these details are transparently documented. This transparency not only acknowledges that other users may not share the same weight but also sets the stage for iterative refinement, where evaluation priorities can evolve as new insights and requirements emerge.

\subsection{Maintenance and Documentation}

After refining the evaluation process, maintaining comprehensive records and transparent documentation is essential. Proper documentation allows others to understand the context of the evaluation, replicate the methodology, and build upon the results. To achieve this, documentation includes:

\begin{itemize}[leftmargin=*, nosep]
    \item Evaluation Setup: Clearly state the task objectives, prioritized dimensions, chosen metrics, and justifications.
    \item Datasets and Benchmarks: Provide details about dataset sources, preprocessing steps, and representativeness, as well as benchmark models used.
    \item Model Details: Describe the models under evaluation, including training data characteristics, fine-tuning procedures, and any custom modifications.
    \item Prioritization and Weighting: Disclose how certain dimensions and metrics were weighted over others, allowing for fair comparisons and informing future research decisions.
    \item Results and Analysis: Present findings alongside appropriate baselines, confidence intervals, and contextual explanations, noting trade-offs (e.g., accuracy vs. fairness).
    \item Employing standardized documentation tools such as model cards, data sheets, or transparency reports can streamline this process. Thorough, organized documentation not only increases trust and reproducibility but also sets the stage for ongoing refinement. As evaluations become more established and better understood, these records will support incremental improvements and collaborative efforts throughout the research community.
\end{itemize}

\section{Challenges and Future Directions}

Evaluating LLMs remains a multifaceted endeavor, shaped by domain-specific requirements, evolving data distributions, and broader societal considerations. Existing benchmarks, such as GLUE or HELM, often lack the granularity to capture specialized tasks—for instance, clinical subtasks within MedQA—and typically focus on predominantly English-language datasets. These limitations underscore the need for domain-specific evaluations that address underrepresented languages and specialized domains (e.g., certain medical subspecialties). Handling dynamic environments presents an additional challenge: LLMs frequently encounter shifting data distributions and unforeseen requirements in real-world settings, necessitating continual evaluation frameworks and active monitoring methods (e.g., the ARPA-H PRECISE-AI\footnote{\url{https://arpa-h.gov/research-and-funding/programs/precise-ai}} effort) for early detection of aberrations and performance drift. Furthermore, optimizing solely for performance can exacerbate biases or obscure transparency, prompting the development of multi-objective frameworks that weigh interpretability and fairness alongside technical metrics. Finally, evaluations must look beyond immediate performance to anticipate long-term societal implications such as misinformation spread, highlighting the need for responsible governance and policy considerations.

A promising direction lies in adopting \emph{multiagent} evaluation frameworks that treats each stakeholder or component as an ``agent'' with distinct roles and objectives~\cite{guo2024large}. Domain experts would define specialized tasks and criteria; data curators would assemble representative datasets; metric designers would refine existing measures or propose new ones; and evaluators—human or automated—would apply metrics to yield timely insights~\cite{xu2024magic}. By enabling negotiation and collaboration among these agents, evaluations can adapt more fluidly to domain-specific needs, accommodate new metrics or data sources, and dynamically respond to emerging societal priorities. Moreover, this multiagent approach can systematically address challenges in specialized domains: for instance, agents specializing in clinical knowledge can generate targeted questions and updates to keep pace with evolving medical standards. Such a system also supports continuous learning and drift monitoring, making it easier to detect performance issues or biases early and adjust accordingly. Ultimately, multiagent frameworks and domain-specific strategies can help guide the development of more robust, ethical, and context-sensitive evaluations, paving the way for LLMs in serving diverse real-world applications.


\section{Related Literature}

\subsection{Surveys of LLM evaluation.}

Evaluating large language models has gained significant attention, leading to various comprehensive surveys that explore different facets of this domain. \citet{chang2024survey} provides an extensive overview of LLM evaluation methodologies, categorizing them into knowledge and capability evaluation, alignment evaluation, and safety evaluation. \citet{guo2023evaluating} delves into the challenges and limitations of current LLM evaluation practices, offering perspectives and recommendations to enhance reproducibility and reliability. \citet{wang2023aligning} focus on aligning LLMs with human expectations, discussing data collection methods, training methodologies, and evaluation techniques pertinent to this alignment. \citet{peng2024survey} propose a two-stage framework for assessing LLMs, emphasizing the progression from core abilities to agent applications and examining the associated evaluation methods at each stage. \citet{laskar2024systematic} provides the most recent challenges, limitations, and recommendations in evaluating LLMs. These surveys collectively contribute to a deeper understanding of LLM evaluation, offering frameworks and insights that inform the development of more robust, aligned, and safe language models.

\subsection{Automated tools}

The research community has developed various automated tools and benchmarks to systematically assess LLMs across multiple dimensions, ensuring that these models meet performance standards and adhere to ethical guidelines. 
Chatbot Arena~\cite{chiang2024chatbot} allows users to compare responses from anonymous AI models in a head-to-head format, contributing to a dynamic leaderboard that includes models from major organizations and startups and facilitating interactive assessments based on human preferences. 
fmeval~\cite{schwobel2024evaluating} is an open-source library designed to evaluate LLMs across various tasks, focusing on both performance and responsible AI dimensions, emphasizing simplicity, coverage, extensibility, and performance to provide practitioners with a comprehensive evaluation tool.
LalaEval~\cite{sun2024lalaeval} offers a holistic human evaluation framework for domain-specific LLMs, encompassing domain specification, criteria establishment, benchmark dataset creation, evaluation rubric construction, and thorough analysis of evaluation outcomes, ensuring tailored and accurate assessments.
Benchmarkthing\footnote{\url{https://www.benchmarkthing.com/}} is an AI evaluation platform that offers ``Evals as an API,'' enabling users to run out-of-the-box evaluations or benchmarks on the cloud, thereby streamlining the assessment process for AI models.
These automated tools and benchmarks represent significant strides in the systematic evaluation of LLMs. By providing structured and comprehensive assessment methodologies, they enable stakeholders to gain deeper insights into model performance, safety, and ethical considerations, thereby facilitating the responsible deployment of AI technologies.

\section{Conclusion}

The evaluation of large language models is a multifaceted challenge, requiring a balance between technical rigor, ethical alignment, and practical applicability. In this work, we formalized the process of LLM evaluation, introducing a systematic framework to address the complexities of assessing these powerful models. By structuring the evaluation process into key dimensions—performance, robustness, ethical considerations, explainability, safety, and controllability—we provided a comprehensive lens through which researchers and practitioners can assess LLMs. Additionally, the proposed checklist and actionable tools, including documentation standards and automated evaluation benchmarks, offer guidance to facilitate thorough and reproducible evaluations.

Our discussion of related works highlighted the progress made in LLM evaluation methodologies, while our analysis of challenges and open questions underscored the need for adaptable benchmarks, dynamic evaluation strategies, and frameworks that balance performance with fairness and interpretability. By incorporating domain expertise into counterfactual design and human evaluation, this work emphasizes the importance of nuanced, context-aware assessments, particularly in high-stakes applications.

Looking forward, the evaluation of LLMs must evolve to address their expanding capabilities and societal impact. Future research should focus on creating more inclusive benchmarks, refining evaluation methodologies for dynamic environments, and ensuring that ethical considerations remain at the forefront of model assessments. By advancing the science of evaluation, we can build more robust, equitable, and trustworthy AI systems, aligning their development with societal values and needs.

%%
%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
\bibliographystyle{ACM-Reference-Format}
\bibliography{sample-base}


%%
%% If your work has an appendix, this is the place to put it.
% \appendix

% \section{Appendix}

\end{document}
\endinput
%%
%% End of file `sample-manuscript.tex'.
