\section{Results: Why Are Web Agents Easier to Jailbreak?}
\label{sec:results}

In this section, we present the results of our component ablation studies on Web AI agents, evaluating their responses to malicious user input. 
Our experiments use 10 diverse harmful requests  (Appendix \ref{Appendix:Dataset Samples}), each tested three times to minimize randomness in agent responses. 
GPT-4o-2024-0806 serves as the backbone LLM for all evaluations. 

In some experiments, the model lacks web interaction capabilities due to the absence of the Event Stream. As a result, it cannot execute harmful actions, and we instead focus on whether the model generated harmful plans in these cases.
Conversely, experiments where the agent retains full web interaction capabilities—allowing us to measure harmful action execution—are marked with $^\ast$.


To create realistic test scenarios, we use mock-up websites proposed by \citet{kumar2024refusal},
which simulate popular platforms such as Instagram, LinkedIn, and Gmail.
These controlled environments enable consistent evaluations while maintaining representative web interactions.
Additionally, we compare evaluation results on real websites to assess the impact of using mock-up environments versus real-world settings.
These ablation studies specifically examine the key components described in section \ref{sec:Our Hypothesis}. The results of these evaluations are summarized in Table \ref{tab:ablation}. 


\begin{table}[t!]






\caption{\textbf{Fine-grained vulnerability evaluations of Web AI agents by modifying components and concepts.} A greater drop in ``Clear Denial(\%)" indicates increased vulnerabilities. Our key findings include: 
1) Adding \textit{SysGoal} to the \textit{standalone LLM} decreases Clear Denial rate by 6.7\%, and introducing \textit{Multi-step Action Gen.} further decreases that by 20\%.
2) Including \textit{Event Stream} further reduces Clear Denial rate by 20\%. 
3) Switching from \textit{Mock-up Web} to \textit{Real Web} lowers Clear Denial rate by 43.3\%, but prevents harmful actions due to webpage complexity.}
\label{tab:ablation}
    \tiny
    \centering
    \begin{threeparttable}
        \begin{tabularx}{\textwidth}{l|X|X|XXX}
            \toprule
            Components Integration & \parbox[c]{2cm}{\textbf{Clear Denial}} & \parbox[c]{2cm}{\textbf{Soft-Denial}} & \parbox[c]{2cm}{\textbf{Non-Denial}\\ response} & \parbox[c]{2.5cm}{Making \\ \textbf{Harmful Plans}} & \parbox[c]{2.5cm}{Completing \\ \textbf{Harmful Actions}} \\
            \midrule
            \rowcolor{light_gray} Standalone LLM & (100.0\%) & (0.0\%) & (0.0\%) & (0.0\%) & - \\
            \rowcolor{step_one_red}\tikzmark{llmGoalRowFrom}+ \cellcolor{step_one_red}SysGoal & \cellcolor{step_one_red}-6.7\% & +0.0\% & \cellcolor{step_one_red}+6.7\% & \cellcolor{step_one_red}+6.7\% & - \\
            + Single-step Action Gen. & +0.0\% & +0.0\% & +0.0\% & +0.0\% & - \\
            + Multi-step Action Gen. & +0.0\% & +0.0\% & +0.0\% & +0.0\% & - \\
            \midrule
            \rowcolor{light_gray}\tikzmark{llmGoalRowTo}Standalone LLM + SysGoal & (93.3\%) & (0.0\%) & (6.7\%) & (6.7\%) & - \\
            \rowcolor{step_two_red}+ Single-step Action Gen. & \cellcolor{step_two_red}-10.0\% & +0.0\% & \cellcolor{step_two_red}+10.0\% & \cellcolor{step_two_red}+10.0\% & - \\
            \rowcolor{step_four_red}\tikzmark{llmGoalMARowFrom}+ Multi-step Action Gen. & \cellcolor{step_four_red}-20.0\% & +0.0\% & \cellcolor{step_four_red}+20.0\% & \cellcolor{step_four_red}+20.0\% & - \\
            \midrule
            \rowcolor{light_gray} \tikzmark{llmGoalMARowTo} Standalone LLM + SysGoal + Multi-step Action Gen. & (73.3\%) & (0.0\%) & (26.7\%) & (26.7\%) & - \\
            \rowcolor{step_four_red}\tikzmark{llmGoalMAObsRowFrom}+ Event Stream$^{\ast}$ & \cellcolor{step_four_red}-20.0\% & +0.0\% & +20.0\% & +6.7\% & (33.3\%) \\
            \midrule
            \rowcolor{light_gray}\tikzmark{llmGoalMAObsRowTo} Web AI Agent$^{\ast}$ & (53.3\%) & (0.0\%) & (46.7\%) & (33.3\%) & (33.3\%) \\
            \rowcolor{light_green}$-$ Goal Paraphrasing$^{\ast}$ & +13.3\% & +0.0\% & -13.3\% & -0.0\% & -0.0\% \\
            \cellcolor{step_four_red}$-$ Mock-up Web + Real Web$^{\ast}$ & \cellcolor{step_four_red}-43.3\% & \cellcolor{step_four_red}+23.3\% & \cellcolor{step_four_red}+20.0\% & \cellcolor{light_green}-3.3\% & \cellcolor{light_green}-30.0\% \\
            \bottomrule
        \end{tabularx}
        \begin{tikzpicture}[overlay, remember picture]
            \draw[->] ($(pic cs:llmGoalRowFrom)+(-0.04,0.04)$) to[out=180, in=180] ($(pic cs:llmGoalRowTo)+(-0.04,0.04)$);
            \draw[->] ($(pic cs:llmGoalMARowFrom)+(-0.04,0.04)$) to[out=180, in=180] ($(pic cs:llmGoalMARowTo)+(-0.04,0.04)$);
            \draw[->] ($(pic cs:llmGoalMAObsRowFrom)+(-0.04,0.04)$) to[out=180, in=180] ($(pic cs:llmGoalMAObsRowTo)+(-0.04,0.04)$);
        \end{tikzpicture}
        \begin{tablenotes}
            \item +: Component Integrate, $-$: Component Ablate (Component changes (+/-) within the same block do not represent cumulative additions across rows.)
            \item $^{\ast}$ denotes having capability to interact with web browser (Thus, Completing Harmful Actions(\%) is reported).
            \item Gray row: Baseline of the section with actual percentage reported. (Two models that are connected with an arrow are the same.)
            \item SysGoal: Embedding user goal in the system prompt, Action Gen.: Action Generation, 
        \end{tablenotes}
    \end{threeparttable}
    \label{results_table}
    \vspace{-0.2in}
\end{table}



\subsection{Web Agents are Significantly More Vulnerable}
\label{sec:initialexp}
To evaluate the vulnerability of Web AI agents compared to standalone LLMs,
we first examine their jailbreaking success rates in responses to malicious instructions.
As illustrated in Fig. \ref{fig:experiment_email}, the two systems exhibit strikingly different behaviors when exposed to identical malicious inputs:
\begin{itemize}
    \item \textbf{Standalone LLM:} Maintains a 0\% jailbreaking success rate, consistently refusing all harmful requests.
    \item \textbf{Web AI Agent:} Successfully generates harmful plans and completes malicious tasks in 33.3\% of cases, with 46.7\% of responses classified as non-denials.
\end{itemize}
This stark contrast is particularly concerning, given that both systems are built on the \textit{same underlying LLM}, yet the Web AI agent demonstrates a substantially higher susceptibility to executing harmful commands. 

One of the failure mode we observe in Web AI agents is \textit{Inconsistent Rejection}, where the agent initially refuses a malicious request but later contradicts its stance and proceeds with compliance.
For example, we identify multiple cases where the agent responds, ``Sorry, I can't assist'' only to later change its position and fulfill the malicious request (as shown in Appendix \ref{Appendix:Inconsistent Rejection}). 
To provide a more comprehensive understanding of vulnerablities, 
we include a detailed breakdown of jailbreaking behaviors, highlighting susceptibility patterns and key failure modes in Web AI agents as shown in Table \ref{results_table}.


\subsection{Differences in the Method of Conveying User Goals}

\vspace{0.13in}
\begin{mdframed}[backgroundcolor=Factor1-blue,linecolor=black,innerleftmargin=5pt,innerrightmargin=5pt,innertopmargin=3pt,innerbottommargin=3pt]
\textbf{Result 1:} Embedding user goals in the system prompt significantly increases jailbreak success rates, while paraphrasing the goal reduces clear denials.
\end{mdframed}

\paragraph{User goals in the system prompt.}
To examine the impact of embedding user goals in the system prompt (Factor 1), we analyze jailbreak success rates under two conditions: \textit{Standalone LLM}, where the goal is provided only in the user prompt, and \textit{+SysGoal}, where the goal is embedded in both the user and system prompts (Table \ref{results_table}). 
All other conditions remain constant to ensure a fair comparison. 
The results indicate that when the goal is not embedded in the system prompt, all jailbreak attempts fail, even with additional modifications (as tested in the other two ablations within the same block). 
However, \textbf{embedding the goal in the system prompt increases the jailbreak success rate from zero to a measurable level, suggesting that this design choice directly contributes to higher vulnerability in Web AI agents.}



\paragraph{Paraphrasing user goals.} 
To evaluate the impact of goal paraphrasing on vulnerabilities (Factor 1), we compare jailbreak success rates in Web AI agent with and without paraphrasing of user-provided goals. 
As shown in Fig. \ref{llm_framework}, Web AI agents typically paraphrase user task descriptions before embedding them in the system prompt for action generation and planning. 
To evaluate the effect of this design choice, we conduct an experiment where the original user-provided goal is directly passed to the LLM (- Goal Paraphrasing) without modification.
The results indicate that disabling goal paraphrasing increases the rate of clear denials by 13.3\%, suggesting that goal paraphrasing introduces more vulnerabilities by potentially softening harmful requests or reinterpreting them in a way that makes them more acceptable to the agent.




\subsection{Differences in the Method of Action Generation and Action Instructions} 


\vspace{0.13in}
\begin{mdframed}[backgroundcolor=Factor2-yellow,linecolor=black,innerleftmargin=5pt,innerrightmargin=5pt,innertopmargin=3pt,innerbottommargin=3pt]
\textbf{Result 2:} Providing action space and action instructions increases system vulnerability, while a multi-step interaction strategy further exacerbates it.
\end{mdframed}


\paragraph{Impact of action space, instructions, and generation methods.} 
This section examines how action generation methods affect vulnerability rate (Factor 2). 
In the Web agents framework, the system prompt defines a predefined action space, guiding the LLM in selecting from available choices. 
This differs from the \textit{Standalone LLM}, which lacks predefined task constraints and instead relies on a default, general-purpose prompt (e.g., ``You are a helpful assistant'').
We evaluate two action generation strategies: \textbf{(1) Single-Step Planning (+Single-Step Action Gen.)} - the LLM plans the entire action sequence upfront before execution.
\textbf{(2) Multi-Step Execution (+Multi-Step Action Gen.)} - the LLM generates actions incrementally, adapting its decisions based on intermediate states.

The results indicate that:
\begin{itemize}
    \item Providing an action space or task-specific instructions alone does not significantly affect jailbreak success rates (as shown in the ablations on \textit{Standalone LLM} in Table \ref{tab:ablation}).
    \item However, when the goal is embedded in the system prompt, \textbf{both the single-step and multi-step action generation strategies} increased vulnerabilities (\textit{Standalone LLM + SysGoal} section in Table \ref{tab:ablation}). 
    \item Notably, multi-step execution leads to a higher jailbreak success (-20\% Clear Denial) than single-step planning, indicating that \textbf{step-by-step action generation increases susceptibility to vulnerabilities} compared to pre-planned sequences.
\end{itemize}
 








\subsection{Differences Due to Agent Event Stream}


\vspace{0.13in}
\begin{mdframed}[backgroundcolor=Factor3-purple,linecolor=black,innerleftmargin=5pt,innerrightmargin=5pt,innertopmargin=3pt,innerbottommargin=3pt]
\textbf{Result 3:} The presence of an Event Stream increases system vulnerability, while the controlled environment of mock-up websites may influence the interpretation of agent behavior in real-world scenarios.
\end{mdframed}


\paragraph{Impact of Event Stream on Vulnerability.}
This section examines how the Event Stream affects agent vulnerability (Factor 3). 
In Table \ref{tab:ablation}, the configuration labeled \textit{Standalone LLM + SysGoal + Multi-step Action Gen.} represents a \textit{Standalone LLM} augmented with all Web AI agent components except the \textit{Event Stream}. 
Under this setup, the system achieves a 73.3\% Clear Denial rate when responding to malicious commands. 
suggesting tracking action history and webpage observations increases susceptibility to jailbreaking. Possible reasons for this increased vulnerability include:
 \begin{itemize}
     \item \textbf{Extended context length}, making it harder to filter harmful requests.
     \item \textbf{Complex webpage structures}, which introduce additional variables the agent must process.
     \item \textbf{Dynamic webpage states}, which may lead the agent to modify its decisions iteratively, reducing its ability to maintain safety constraints.
 \end{itemize}
 These findings suggest that the design of Web AI agents incorporating an Event Stream inherently makes them more vulnerable than standalone LLMs.


\paragraph{Impact of mock-up vs. real websites on evaluation.}
This section compares the use of \textit{Real Web} and \textit{Mock-up Web} for evaluation, as outlined in Factor 3. 
As shown in Table \ref{tab:ablation}:
\begin{itemize}
    \item Testing on real websites significantly reduces the Clear Denial rate (-43.3\%), suggesting that Web AI agents struggle to assess the harmfulness of requests accurately in real-world conditions. 
    This difficulty may stem from the greater complexity and diversity of real webpages. 
    \item However, despite the lower denial rates, real websites recorded a 30\% reduction in harmful action completion compared to mock-ups.
\end{itemize}
This difference may stem from the fact that 
real websites require more complex interactions, often containing richer accessibility trees \citep{openhands, Mozilla} that Web AI agents struggle to navigate effectively.
Additionally, in real-world settings, Web AI agents sometimes exhibit \textbf{Inconsistent Rejection} (Appendix \ref{Appendix:Inconsistent Rejection}), where they initially refuse malicious requests but later attempt to bypass constraints while issuing harmful instructions. This trial-and-error behavior suggests that agents adapt their strategies dynamically, increasing the risk of unintended compliance with harmful commands.





