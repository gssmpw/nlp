\section{Related Works}\label{sec:related}



\paragraph{LLMs in Web AI agent systems.} Recent advancements in LLM techniques have expanded their role in AI agent systems, enabling them to generate and execute actions \citep{yang2024swe, zheng2024agentstudio, putta2024agent, gou2024navigating}. Among various applications, web browsing has emerged as a key domain for AI agents \citep{zheng2024gpt, shahbandeh2024naviqate, zhang2024webpilot, iong2024openwebagent}, where LLMs assist users in tasks ranging from simple navigation to more complex operations such as booking flights and interacting with web-based maps \citep{workarena2024, openhands, liao2024eia}. 
To support systematic evaluation,
researchers have introduced several benchmarks, including simulated and self-hostable webpage environments, to comprehensively assess the performance of Web AI agents \citep{zhou2023webarena, koh2024visualwebarenaevaluatingmultimodalagents, xu2024theagentcompanybenchmarkingllmagents}.





\paragraph{Security aspects of AI agents.} 







AI agents assist humans in daily computer tasks, often requiring access to private data and sensitive information, making their security and trustworthiness paramount.
Research in this area has focused on identifying security vulnerabilities, attack methods, and defense mechanisms. 


Several studies highlight significant security risks. \citet{liao2024eia} demonstrated that \textbf{injection attacks} can lead to privacy leaks, while \citet{zhang2024attackingvisionlanguagecomputeragents} examined how \textbf{pop-up blocks} distract agents and manipulate them into executing attacker-intended actions. 
Additionally, \citet{nakash2024breakingreactagentsfootinthedoor} revealed how \textbf{indirect prompt injection attacks} can coerce agents into performing malicious actions.

On the defense side, researchers have proposed various mitigation mechanisms. 
\citet{balunovic2024ai} introduced a \textbf{security analyzer} that enforces strict constraints on agent actions. \citet{wu2024systemleveldefenseindirectprompt} designed a \textbf{secure LLM system} that separates planning from execution, using \textbf{information flow control and security labels} to filter untrusted inputs. 
\citet{he2024securityaiagents} emphasized the importance of \textbf{session management, sandboxing, and encryption} to enhance AI agent security in real-world applications.

To systematically assess vulnerabilities, several benchmarks have been developed. \citet{andriushchenko2024agentharm} evaluated LLM robustness against jailbreak attacks. \citet{debenedetti2024agentdojo} introduced a dynamic framework for testing AI agent security against prompt injection attacks. 
\citet{zhang2024agent} created a benchmark covering over ten prompt injection attacks, including memory poisoning and Plan-of-Thought backdoor attacks. 
These studies consistently show that LLM-based Web AI agents are significantly more susceptible to jailbreak attacks than standalone LLM systems \citep{kumar2024refusal, liao2024eia, li2025commercial}.

However, the underlying causes of this heightened vulnerability remain unclear. 
Existing evaluations, primarily focus on binary jailbreak success or failure, failing to capture nuanced agent behaviors under varying levels of harmful influence. 
This work aims to provide a fine-grained analysis of Web AI agent vulnerabilities, identifying the specific components and design choices that contribute to their susceptibility.

