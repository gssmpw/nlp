\section{Conclusions}\label{sec:conc}

This study demonstrates that Web AI agents are significantly more vulnerable to harmful or malicious user inputs than standalone LLMs, highlighting the urgent need for safer and more robust Web AI agent designs. 
Through a fine-grained analysis of the key differences between Web AI agents and standalone LLMs, we systematically identified several design factors contributing to these vulnerabilities, as summarized in Table \ref{tab:ablation}. 
To our knowledge, this is the first comprehensive studies to systematically ablate and investigate the underlying components that drive these security risks. 

Our findings reveal several actionable insights:
\begin{itemize}
    \item 
    Embedding user goals within the system prompts significantly increases  jailbreak success rates. Paraphrasing user goals further heightens system vulnerabilities by potentially softening or misinterpreting harmful intent.
    \item Providing predefined action spaces, especially in multi-turn action strategies, makes the system more susceptible to executing harmful tasks. This risk is particularly pronounced when the user’s goal is embedded in the system prompt.
    \item Mock-up websites do not inherently promote harmful intent, but they facilitate more effective task execution for malicious objectives. This suggests that controlled environments can still unintentionally shape agent behavior in ways that affect security assessments.
    \item The presence of an Event Stream, which tracks action history and dynamic web observations, amplifies harmful behavior. This finding underscores the Event Stream as a critical vulnerability factor, as it allows the agent to iteratively refine its approach, potentially increasing susceptibility to adversarial manipulation.

\end{itemize}




These findings highlight how specific design elements—goal processing, action generation strategies, and dynamic web interactions—contribute to the overall risk of harmful behavior.

By offering a comprehensive understanding of these vulnerabilities, our study provides guidance for designing safer Web AI agents and lays the groundwork for future research on mitigating these security risks. Future work should explore defensive mechanisms to enhance robustness, including adaptive filtering, structured action constraints, and improved system prompt strategies to minimize unintended harmful behavior.


