\section{Understanding Web AI Agent Vulnerabilities: Fine-Grained Evaluation and Component Ablation}\label{sec:idvulner}




In previous sections, we highlighted the key differences between standalone LLMs and Web AI agents, emphasizing how Web AI agents encapsulate the LLM backbone \textbf{within a broader system}.
Prior studies have shown that Web AI agents are more susceptible to jailbreaking than standalone LLMs \citep{kumar2024refusal, li2025commercial}. However, the specific mechanisms and factors driving this increased vulnerability remain unclear. To systematically analyze these weaknesses, %
we categorize Web AI agent components into three key factors:
\textbf{Factor 1} (Goal Preprocessing), \textbf{Factor 2} (Action Space), and \textbf{Factor 3} (Event Stream / Web Browser). 
Our objective is to determine whether these design differences contribute to increased vulnerability, making Web AI agents more susceptible to executing 
malicious commands. 
By breaking down these components, we provide a fine-grained analysis of the underlying risks, moving beyond a high-level comparison to uncover the specific structural elements that heighten security risks in Web AI agents.


To quantify these risks, we refine the \textbf{agent harmfulness evaluation strategy} (Section \ref{sec:fine-grained study}) by introducing a fine-grained assessment framework that captures varying degrees of jailbreak susceptibility. Through extensive experiments, we systematically assess how each system component contributes to the agent's security vulnerabilities, providing deeper insights into the structural weaknesses of Web AI agents.






























\subsection{Hypothesis of why Web AI Agents are more vulnerable}
\label{sec:Our Hypothesis}


\vspace{0.13in}
\begin{mdframed}[backgroundcolor=Factor1-blue,linecolor=black,innerleftmargin=5pt,innerrightmargin=5pt,innertopmargin=3pt,innerbottommargin=3pt]
\textbf{Factor 1: The Preprocessing of User Goals} --- whether through paraphrasing, decomposition, or embedding them within system prompts --- can affect their resistance to harmful instructions.
\label{Factor1}
\end{mdframed}


\paragraph{User goals in a system prompt.} 
Unlike standalone LLMs, which typically use system prompts containing only high-level guidelines, Web AI agents often embed user task descriptions directly into the system prompt \citep{openhands}. 
While this approach helps preserve the original goal across multi-turn interactions, it could introduce vulnerabilities. 
Specifically, placing user goals within the system prompt deviates from the safety alignment strategies used to train LLMs, as this behavior is out-of-distribution (OOD) relative to their original safety alignment. This discrepancy could increase the susceptibility of Web AI agents to jailbreaking, making them more prone to executing harmful commands. In short, \textbf{we hypothesize that embedding user prompts within system prompts contributes to the heightened vulnerability of Web AI agents}.


\paragraph{Paraphrasing user goals.} Since user goals are often vague, complex, or ambiguous, many Web AI agents \citep{openhands} leverage LLMs to rephrase or decompose user instructions into structured agent goals for better execution.
However, we observe that in some cases, this process focuses purely on rephrasing or decomposition without assessing the safety of the original request.
As a result, harmful instructions may be reframed in a more lenient manner, increasing the likelihood that the agent will execute them.



Moreover, this reinterpretation can unintentionally introduce additional details that were not explicitly specified by the user, potentially amplifying security risks
(as shown in Appendix \ref{Appendix:Hypothesis}). 
Therefore, we hypothesize that \textbf{Web AI agents' vulnerabilities to jailbreaking stem, in part, from the reinterpretation of user goals within the system}.


\vspace{0.13in}
\begin{mdframed}[backgroundcolor=Factor2-yellow,linecolor=black,innerleftmargin=5pt,innerrightmargin=5pt,innertopmargin=3pt,innerbottommargin=3pt]
\textbf{Factor 2: Action Generation Mechanisms ---} such as predefined action spaces and step-by-step execution --- can affect agents' ability to assess and mitigate harmful intent.
\label{Factor2}
\end{mdframed}

\paragraph{Providing action space and action instruction.} 
For an LLM’s output to function as an executable action within a web browser, it must operate within a predefined action space or interact with designated functions/tools.
To ensure valid execution, Web AI agents supply the LLM with a structured description of the available action space, guiding it to generate outputs that conform to predefined constraints.

However, this approach may introduce security trade-offs. Embedding an extensive action space within the system prompt increases prompt length and content diversity, which could weaken the LLM’s ability to detect harmful user inputs.
Moreover, focusing the LLM on selecting predefined actions could divert its attention from assessing the intent behind a given task, increasing its increasing its susceptibility to executing harmful commands. 
Based on these observations, we hypothesize that \textbf{limiting an LLM’s output to a predefined action space can increase the system's susceptibility to jailbreaking.}



\paragraph{Method of action generation.} 
Certain tasks require multiple sequential interactions with the web browser rather than a single-step execution.
For example, composing an email to persuade someone to share sensitive information involves a series of actions, such as identifying webpage elements, entering an email, and drafting a message \citep{Significant_Gravitas_AutoGPT, openhands}. 
When tasks are broken down into discrete steps, the agent executes each action in isolation, potentially failing to recognize the overarching malicious intent.
By contrast, generating a high-level plan upfront could provide more contextual awareness, allowing for better assessment of harmfulness.
Therefore, we hypothesize that \textbf{multi-step action generation can inadvertently increase the likelihood of LLMs executing harmful tasks} compared to generating the full plan in advance.





\vspace{0.13in}

\begin{mdframed}
[backgroundcolor=Factor3-purple,linecolor=black,innerleftmargin=5pt,innerrightmargin=5pt,innertopmargin=3pt,innerbottommargin=3pt]
\textbf{Factor 3: Observational Capabilities and Their Impact on Vulnerability.} The ability of Web AI agents to observe and interpret web content, coupled with their potential recognition of mock-up environments as artificial, may influence their vulnerability.
\label{hypothesis3}
\end{mdframed}


\paragraph{Dynamic State and Event Stream.} 
Unlike standalone LLMs that rely solely on static textual context, Web AI agents actively interact with web pages and incorporate sequential event streams --- which include previous actions, observations, and auxiliary metadata --- into their decision-making process. 
This dynamic observation capability enables agents to adapt their strategies in real-time, allowing for more 
flexible task execution.
However, this adaptability may also introduce security risks.
For example, Web AI agents could revise their approach
over multiple iterations, gradually overriding initial constraints and proceeding with harmful actions based on newly observed event stream content.
We hypothesize that \textbf{by iteratively modifying their strategies through trial and error based on the dynamic event stream, these agents may eventually attempt actions} they initially deemed harmful, affecting the risk of unintended or malicious outcomes.


\paragraph{Impact of Mock-Up Websites on Agent Behavior.}


Previous studies have shown that when LLMs perceive a scenario as fictional or simulated, they are more likely to engage in risky behavior. For instance, an LLM tasked with designing a terrorist attack plan was more likely to comply when the request was framed as writing a fictional film script \citep{zhu2023autodan, xu2024bag}. Similarly, we hypothesize that Web AI agents \textbf{may detect the artificial nature of mock-up webpages, influencing their risk assessment and decision-making process.} This recognition could increase the agent’s leniency toward executing potentially harmful commands, as it may implicitly categorize the environment as non-threatening or hypothetical. (See Appendix \ref{Appendix:Hypothesis} for detailed examples and clues that Web AI Agent identify a mock-up website.)






\subsection{Fine-Grained Evaluation Protocol For Analyzing Agent Harmfulness}
\label{sec:fine-grained study}
While previous studies primarily compare the overall vulnerability rates of Web AI agents and standalone LLMs, they typically rely on binary jailbreak success/failure assessments \citep{kumar2024refusal}. Such an approach oversimplifies the issue and fails to capture nuanced jailbreak behaviors. To address this gap, we introduce a fine-grained evaluation framework that categorizes jailbreak attempts into five distinct levels, allowing for a more precise understanding of agent vulnerabilities. As shown in Fig. \ref{llm_framework}, various components of a Web AI agent—such as goal preprocessing, action space, and event stream—impact its likelihood of successfully executing or resisting harmful tasks at different levels. However, traditional binary jailbreak evaluations (success/failure) oversimplify the issue, failing to capture subtle signals of jailbreaking that emerge throughout an agent's decision-making process.




\paragraph{Fine-grained evaluation.} 
To address this gap, we introduce a fine-grained five-level assessment framework, where each level represents a different degree of harmfulness, as illustrated in Fig. \ref{fig:evaluation_scenarios}. 
This framework allows us to detect early signs of harmful planning, even in cases where the agent does not complete the entire task.


By tracking how jailbreak attempts evolve—from subtle indications of harmful intent to fully executed malicious actions—our framework provides a more intuitive and detailed understanding of agent behavior. Given the complexity of these assessments, we rely primarily on human evaluation to interpret nuanced cases accurately.


Within this framework, we define 5 distinct levels of jailbreaking:
\textbf{(1) Clear-Denial} – LLM outputs a denial message and immediately halts the system.
\textbf{(2) Soft-Denial} – LLM outputs a denial message at some point (initially, midway, or at the final step), but still proceeds to execute at least one action. This includes \textit{Inconsistent Rejection} introduced in Section \ref{sec:initialexp}.
\textbf{(3) Non-Denial} – LLM does not output any denial message throughout the interaction and continues executing actions.
\textbf{(4) Harmful Plans} – LLM generates harmful plans to successfully achieve the malicious request. In addition to Non-Denial, Harmful Plans produce a complete plan for achieving the harmful goal. Human evaluators verify the feasibility by following them.
\textbf{(5) Harmful Actions} – The agent executes the action sequence to fulfill the malicious request. In addition to Harmful Plans, Harmful Actions complete the intended malicious goal.

We also establish the following set relationships within our framework:
\begin{itemize}
    \item Non-Denial, Soft-Denial, and Clear-Denial are disjoint sets ($\text{Non-Denial} \cap \text{Soft-Denial} \cap \text{Clear-Denial} = \emptyset$).
    
    \item Harmful Plans is a subset of Non-Denial ($\text{Harmful Plans} \subseteq \text{Non-Denial}$).
    
    \item Harmful Actions is a subset of Harmful Plans ($\text{Harmful Actions} \subseteq \text{Harmful Plans}$).
\end{itemize}

This layered structure enables a more precise analysis of whether a jailbreak attempt occurred and how deeply the agent engaged with the harmful request. By refining our understanding of system vulnerabilities, this framework offers valuable insights into the \textbf{root causes of AI agent susceptibility} and informs future security improvements. (See Appendix \ref{Appendix:Harmful-Level Examples} for qualitative examples of each level.)





\begin{figure}[h]
    \centering
    \includegraphics[width=1.\textwidth]{\fighome/fig3.pdf} 
    \vspace{-0.3in}
    \caption{Fine-Grained Harmfulness Evaluation Scenarios}\label{fig:evaluation_scenarios}
    \vspace{-0.07in}
\end{figure}






\paragraph{Fine-grained ablation of Web AI agent components.}
In addition to fine-grained evaluation, we conduct an in-depth study by incrementally integrating components of Web AI agents into standalone LLMs.
By systematically adding each component step by step, we effectively simulate an ablation process without compromising the agent’s core functionality.


To ensure reliable and consistent results, each instruction was tested three times, reducing the influence of randomness in agent responses. This incremental approach allows us to test the hypotheses on agent vulnerabilities proposed in Section \ref{sec:Our Hypothesis} and isolate the specific contributions of each system component to security risks.


Following \citep{kumar2024refusal}, we conducted our experiments using OpenHands \citep{openhands} (previously known as OpenDevin \citep{wang2024opendevin}), a widely adopted and stable platform within both academic and industrial research communities. As illustrated in Fig. \ref{llm_framework}, a Web AI agent system
consists of an LLM and the surrounding modules that facilitate interaction with dynamic web environments. 

Using the OpenHands \citep{openhands}, we systematically isolate and integrate these components into the LLM framework, testing their responses to identical malicious user inputs. This incremental approach enables us to analyze how each component contributes to vulnerabilities across different stages of harmful behavior. Through this ablation process, we identify the specific roles of individual components in increasing susceptibility to harmful interactions, providing a deeper, more nuanced understanding of the factors influencing Web AI agent safety.

