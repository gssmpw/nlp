\section{Introduction}\label{sec:intro}
\begin{figure}[h]  
    \centering
    \includegraphics[width=1.0\textwidth]{figures/fig1_new.pdf}  
    \caption{
    Web AI agents exhibit a significantly higher jailbreak rate (46.6\%) compared to standalone LLMs (0\%), highlighting their increased vulnerability in real-world deployment.}
    \label{fig:experiment_email}
\end{figure}

Recent advancements in Large Language Models (LLMs) have demonstrated impressive reasoning capabilities and proficiency in solving complex problems. These capabilities are increasingly being extended to multi-step tasks, driving the evolution of LLM-based AI agent systems \citep{shen2024scribeagentspecializedwebagents,yang2024agentoccamsimplestrongbaseline,yang2024swe,putta2024agent,zhang2024webpilot}. 
One such system is the Web (browser) AI agent, which integrates an LLM with software tools and APIs to execute sequences of actions aimed at achieving specific goals within a web environment.
These agents leverage LLM capabilities for planning \citep{zheng2024naturalplanbenchmarkingllms}, reflection \citep{Pallagani_2024}, and effective tool utilization \citep{yao2024taubenchbenchmarktoolagentuserinteraction,shi2024learningusetoolscooperative}, enabling more autonomous and adaptive web-based interactions.


Many previous studies \citep{openhands,shen2024scribeagentspecializedwebagents,su2025learnbyinteractdatacentricframeworkselfadaptive} have highlighted significant advancements in autonomous web agents. 
However, despite their promising potential, their safety and security vulnerabilities have not yet been systematically studied. 
Given their direct integration with web browsers, these agents could be exploited to distribute malware or send phishing Emails to extract personal information, posing serious security risks (as shown in Fig. \ref{fig:experiment_email}). 


In this study, we highlight the heightened vulnerability of Web AI agent frameworks to jailbreaking compared to traditional LLMs. Through comprehensive experiments, we demonstrate that web agents, by design, exhibit a significant higher susceptibility to following malicious commands due to fundamental component-level differences from standalone LLMs. 
Notably, while a standalone LLM (such as a regular chatbot)
refuses malicious requests with a 0\% success rate, the Web AI agent follows them at a rate of 46.6\% (Fig. \ref{fig:experiment_email}).


Importantly, we identify three primary factors contributing to the increased vulnerabilities of Web AI agents:
\textbf{(1)} Directly embedding user input into the LLM system prompt, 
\textbf{(2)} Generating
actions in a multi-turn manner, and 
\textbf{(3)} Processing observations and action histories, which increases the likelihood of executing harmful instructions and weakens the system’s ability to assess risks. 
\textbf{Additionally,} we find that mock-up testing environments may inadvertently distort security evaluations by oversimplifying real-world interactions, potentially leading to misleading conclusions about an agent's robustness.



To better understand the heightened vulnerability of Web AI agents to jailbreaking and their increased susceptibility to executing malicious commands, we introduce a 5-level fine-grained ablative metric that goes beyond the conventional binary assessments of LLM vulnerabilities, offering a more nuanced evaluation of jailbreak signals. 
Ultimately, our study 
raises awareness of the security challenges posed by Web AI agents and advocates for proactive measures to design safer, more resilient agent frameworks. 















\textbf{Our contributions:}
\begin{itemize}[leftmargin=*]
\item \textbf{Empirical evidence of Web AI agents’ heightened vulnerability: }We systematically compare Web AI agents with standalone LLM chatbots, revealing that Web AI agents are significantly more susceptible to jailbreaking and executing malicious commands.
\item \textbf{Root cause analysis of Web AI agent vulnerabilities:} We investigate the design-level differences between Web AI agents and standalone LLMs, identifying key factors—such as system prompt manipulation, multi-turn action generation, and reliance on historical observations—that contribute to their increased vulnerability.
\item \textbf{A fine-grained evaluation protocol for jailbreak susceptibility:} We introduce a structured, five-level harmfulness evaluation framework that goes beyond binary assessments, enabling a more detailed and nuanced analysis of Web AI agent vulnerabilities.
\item \textbf{Actionable insights for targeted defense strategies:} Based on our findings, we provide recommendations for mitigating security risks in Web AI agents, focusing on improving system prompt handling, action generation mechanisms, and contextual awareness in agent architectures.

    
    
\end{itemize}





