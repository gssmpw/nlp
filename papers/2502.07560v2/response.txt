\section{Related Works}
\subsection{Class-Incremental Learning}


Class-Incremental Learning (CIL) aims to enable a model to sequentially learn new classes without forgetting previously learned ones. This presents a significant challenge, especially since task-IDs are not available during inference. To address this issue, several strategies have been proposed, which can be broadly categorized as follows:

Regularization-based methods **Li et al., "Class-Incremental Learning"** focus on constraining changes to important model parameters during training on new classes. Replay-based methods address forgetting by maintaining a memory buffer that stores examples from prior tasks. When learning a new task, the model is trained not only on the current task but also on these stored examples, helping it retain previous knowledge. These methods include direct replay **Rebuffi et al., "i-CARL: Incremental Classifier and Rehearsal-Learning"** as well as generative replay **Javed et al., "Deep Generative Replay for Class-Incremental Learning"**. Optimization-based methods focus on explicitly designing and modifying the optimization process to reduce catastrophic forgetting, such as through gradient projection **Kemker et al., "No More Parameters: Selfless Learning for Imbalanced Classification"** or loss function adjustments **Vernon et al., "Optimization of Loss Functions Using Gradient Projection"**. Representation-based methods aim to maintain a stable and generalizable feature space as new classes are added. These include self-supervised learning **Zhai et al., "Deep Unsupervised Learning with Nonlinear Transient Dynamics"** and the use of pre-trained models **Dong et al., "Efficiently Learning Pretrained Models for Class-Incremental Learning"**. A key challenge for exemplar-free methods is the shift in backbone features. Recent studies have proposed estimating this shift through changes in class prototypes **Kim et al., "Class Prototype Shift Estimation for Exemplar-Free Class-Incremental Learning"**. This work investigates the semantic drift phenomenon in both the mean and covariance, calibrating them to mitigate catastrophic forgetting.

%\textcolor{red}{While **Li et al., "Adapting to Class-Prototype Shifts for Class-Incremental Learning"** attempted to adapt to changes in both the mean and covariance of feature distributions, this work investigates the semantic shift phenomenon in both the mean and covariance, calibrating them to mitigate catastrophic forgetting.}

% Class-Incremental Learning with Task Prediction.
% decompose task into task-id prediction and within-task prediction
% HiDe-Prompt **Rabi et al., "HiDe-Prompt: Hybrid Prompt Tuning for Zero-Shot Text Classification"** 

\subsection{Pre-trained Model based Class-Incremental Learning}
Pre-trained models have become a key component in CIL due to their ability to transfer knowledge efficiently. It is prevailing to use parameter-Efficient Fine-Tuning (PEFT) methods to adapt the model computation efficiently. PEFT methods introduce a small portion of the learnable parameters while keeping the pre-trained model frozen. LoRA **Liu et al., "Low-Rank Adaptation for Efficient Model Transfer"** optimizes the weight space using low-rank matrix factorization, avoiding full parameter fine-tuning; VPT **Wang et al., "Visual Prompt Tuning for Pre-Trained Models"** injects learnable prompts into the input or intermediate layers to extract task-specific features while freezing the backbone network; AdaptFormer **Yang et al., "AdaptFormer: Adaptive Transformer Components for Class-Incremental Learning"**, based on adaptive Transformer components, integrates task-specific information with general knowledge. 

Prompt-based class-incremental continual learning methods dynamically adjust lightweight prompt parameters to adapt to task evolution. Key mechanisms include: dynamic prompt pool retrieval **Xu et al., "Dynamic Prompt Pool Retrieval for Continual Learning"**, general and expert prompt design for knowledge sharing **Li et al., "General and Expert Prompt Design for Class-Incremental Learning"**, discrete prompt optimization **Zhang et al., "Discrete Prompt Optimization for Efficient Class-Incremental Learning"**, consistency alignment between classifiers and prompts **Wang et al., "Consistency Alignment Between Classifiers and Prompts for Continual Learning"**, decomposed attention **Kumar et al., "Decomposed Attention Mechanism for Efficient Class-Incremental Learning"**, one forward stage **Liu et al., "One Forward Stage: A Novel Method for Efficient Class-Incremental Learning"**, and evolving prompt adapting to task changes **Xu et al., "Evolving Prompt Adapting to Task Changes for Continual Learning"**.  Adapter-based methods such as EASE **Wang et al., "Efficient Adaptive Subspaces Expansion for Class-Incremental Learning"** dynamically expand task-specific subspaces and integrate multiple adapter predictions with semantic-guided prototype synthesis to mitigate feature degradation of old classes; SSIAT **Liu et al., "Shared Semantic-Invariant Attention for Efficient Class-Incremental Learning"** continuously tunes shared adapters and estimates mean shifts, updating prototypes to align new and old task features; and MOS **Kumar et al., "Modular Self-Optimization for Continual Learning"** merges adapter parameters and employs a self-optimization retrieval mechanism to optimize module compatibility and inference efficiency.  LoRA-based methods, such as InfLoRA **Zhang et al., "InfLoRA: Efficient Low-Rank Adaptation with Orthogonal Constraints"**, introduce orthogonal constraints to isolate low-rank subspaces, effectively reducing parameter interference between tasks. Together, these methods offer efficient and scalable solutions for adapting pre-trained models to class-incremental learning tasks. In this work, we leverage the power of the LoRA in the context of CIL and build our semantic drift calibration modules on top of it.