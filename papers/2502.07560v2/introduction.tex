\section{Introduction}

\input{sections/fig_shift}

Continual Learning, also referred to as lifelong learning, aims to enable machine learning models to sequentially learn multiple tasks over their life-cycle without requiring retraining or access to data from previous tasks \cite{rebuffi2017icarl}. The primary goal of continual learning is to facilitate knowledge accumulation and transfer, allowing models to adapt quickly to new, unseen tasks while maintaining robust performance on previously learned tasks \cite{parisi2019continual}. This capability has broad applications in fields such as computer vision, robotics, and natural language processing.
% In the human perceptual system, neurosynaptic plasticity is a fundamental mechanism that enables the brain to adapt to dynamic environments by inducing physical changes in neural structures, allowing us to learn, remember, and evolve over time. However, neuroscientific studies have revealed that humans experience asymmetric interference effects during perceptual learning. To ensure the long-term durability of existing knowledge and avoid catastrophic interference when acquiring new tasks and skills, it is crucial to protect and consolidate previously learned information. These biological principles provide valuable insights into how machine learning models can mitigate catastrophic forgetting and enable dynamical adaptation, guiding researchers to address the classic \textit{Stability–Plasticity Dilemma}.

In recent years, the issue of model plasticity has become less prominent in deep learning-based approaches, primarily due to two factors: (1) the increasing capacity of deep models allows them to effectively over-fit new data, and (2) large-scale pre-training on extensive datasets equips models with powerful feature extraction capabilities \cite{he2019rethinking}. Parameter-efficient fine-tuning based on pretrained models has further enhanced model plasticity, as highlighted in numerous recent studies \cite{houlsby2019parameter,lester-etal-2021-power,hu2022lora}.

Despite these advancements, existing methods—such as regularization \cite{kirkpatrick2017overcoming}, memory replay \cite{lopez2017gradient}, and knowledge distillation \cite{li2017learning}—while improving stability to some extent, introduce additional costs. For example, (1) memory replay methods require storing and retraining on old task data, increasing storage and computational demands. (2) knowledge distillation involves additional computational overhead during the distillation process, complicating and slowing down training. These additional costs hinder the practical deployment of continual learning methods. Therefore, a key challenge in the field is to improve model stability while minimizing resource consumption and computational overhead \cite{wang2024comprehensive}.

In this paper, we build upon successful practices by leveraging parameter-efficient fine-tuning based on pretrained models to further analyze catastrophic forgetting. Through extensive experimental observations, we discovered that although low-rank adaptation (e.g., LoRA \cite{hu2022lora} ) based on pretrained models can effectively maintain model plasticity, the incremental integration of tasks and model updates induces feature mean and covariance shift—what we also term \textbf{Semantic Drift}. As illustrated in Figure~\ref{fig:teaser}, the feature distribution of the original task data undergoes significant mean shifts and changes in variance shape as more tasks are introduced.

Based on these observations, we propose to address semantic drift with both mean shift compensation and covariance calibration, which constrain the first-order and second-order moments of the features, respectively. Specifically, we compute mean class representations after learning a novel task as the average embedding of all samples in class. The shift between old and novel tasks is approximated by the weighted average of embedding shifts, where the weights are determined by the proximity of each embedding to the previous class mean. This approach effectively estimates the mean shift for all previously learned classes during each new task. We also introduce an implicit covariance calibration technique using Mahalanobis distance \cite{mahalanobis1936generalized} loss to address semantic drift. This method aligns the covariance matrices of embeddings from old and current networks for each class, ensuring consistent intra-class distributions. By leveraging the old network as "past knowledge", we compute class-specific covariance matrices and minimize the absolute differences in Mahalanobis distances between embedding pairs from both networks. This approach effectively mitigates covariance shift, maintaining model stability while allowing continual learning. As shown in Figure \ref{fig:shift_cali}, these constraints effectively maintain model stability while preserving plasticity. Additionally, we implement feature self-distillation for patch tokens, further enhancing feature stability.

In summary, our main contributions are:
\begin{itemize}
    \item We delve into the exploration of the semantic drift problem in class-incremental learning and further propose efficient and straightforward solutions—mean shift compensation and covariance calibration, which significantly alleviate this challenge.
    \item We orchestrate an efficient task-agnostic continual learning framework that outperforms existing methods across multiple public datasets, demonstrating the superiority of our approach.
\end{itemize}

%\input{sections/fig_calibrate}