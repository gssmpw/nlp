\begin{figure*}[!ht]
\centering
\includegraphics[width=\textwidth]{images/framework0128.pdf}
\caption{\small Illustration of our method at task \( t \). The feature extractor at task \( t \) uses a frozen pre-trained ViT backbone with learnable LoRA modules. The output class tokens (yellow) are passed through a classifier to compute the classification loss \( \mathcal{L}_{\text{cls}} \), and the mean and covariance of each class are stored for each session. During training, class tokens (yellow and blue) are used to align class distributions via a covariance calibration loss \( \mathcal{L}_{\text{cov}} \). Patch tokens from network \( t \) (yellow) distill knowledge from network \( t-1 \) (blue) through a distillation loss \( \mathcal{L}_{\text{distill}} \). After training, the class means are updated using a mean shift compensation module, and the classifier heads are retrained with the calibrated statistics.}
\label{fig:framework}
\vspace{-3mm}
\end{figure*}