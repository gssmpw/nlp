\section{Related Works}
\subsection{Class-Incremental Learning}


Class-Incremental Learning (CIL) aims to enable a model to sequentially learn new classes without forgetting previously learned ones. This presents a significant challenge, especially since task-IDs are not available during inference. To address this issue, several strategies have been proposed, which can be broadly categorized as follows:

Regularization-based methods ____ focus on constraining changes to important model parameters during training on new classes. Replay-based methods address forgetting by maintaining a memory buffer that stores examples from prior tasks. When learning a new task, the model is trained not only on the current task but also on these stored examples, helping it retain previous knowledge. These methods include direct replay ____ as well as generative replay ____. Optimization-based methods focus on explicitly designing and modifying the optimization process to reduce catastrophic forgetting, such as through gradient projection ____ or loss function adjustments ____. Representation-based methods aim to maintain a stable and generalizable feature space as new classes are added. These include self-supervised learning ____ and the use of pre-trained models ____. A key challenge for exemplar-free methods is the shift in backbone features. Recent studies have proposed estimating this shift through changes in class prototypes ____. This work investigates the semantic drift phenomenon in both the mean and covariance, calibrating them to mitigate catastrophic forgetting.

%\textcolor{red}{While ____ attempted to adapt to changes in both the mean and covariance of feature distributions, this work investigates the semantic shift phenomenon in both the mean and covariance, calibrating them to mitigate catastrophic forgetting.}

% Class-Incremental Learning with Task Prediction.
% decompose task into task-id prediction and within-task prediction
% HiDe-Prompt ____ ____

\subsection{Pre-trained Model based Class-Incremental Learning}
Pre-trained models have become a key component in CIL due to their ability to transfer knowledge efficiently. It is prevailing to use parameter-Efficient Fine-Tuning (PEFT) methods to adapt the model computation efficiently. PEFT methods introduce a small portion of the learnable parameters while keeping the pre-trained model frozen. LoRA ____ optimizes the weight space using low-rank matrix factorization, avoiding full parameter fine-tuning; VPT ____ injects learnable prompts into the input or intermediate layers to extract task-specific features while freezing the backbone network; AdaptFormer ____, based on adaptive Transformer components, integrates task-specific information with general knowledge. 

Prompt-based class-incremental continual learning methods dynamically adjust lightweight prompt parameters to adapt to task evolution. Key mechanisms include: dynamic prompt pool retrieval ____, general and expert prompt design for knowledge sharing ____, discrete prompt optimization ____, consistency alignment between classifiers and prompts ____, decomposed attention ____, one forward stage ____, and evolving prompt adapting to task changes ____.  Adapter-based methods such as EASE ____ dynamically expand task-specific subspaces and integrate multiple adapter predictions with semantic-guided prototype synthesis to mitigate feature degradation of old classes; SSIAT ____ continuously tunes shared adapters and estimates mean shifts, updating prototypes to align new and old task features; and MOS ____ merges adapter parameters and employs a self-optimization retrieval mechanism to optimize module compatibility and inference efficiency.  LoRA-based methods, such as InfLoRA ____, introduce orthogonal constraints to isolate low-rank subspaces, effectively reducing parameter interference between tasks. Together, these methods offer efficient and scalable solutions for adapting pre-trained models to class-incremental learning tasks. In this work, we leverage the power of the LoRA in the context of CIL and build our semantic drift calibration modules on top of it.