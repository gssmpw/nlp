%%%%%%%% ICML 2024 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2024} with \usepackage[nohyperref]{icml2024} above.
\usepackage{hyperref}

\usepackage{subcaption}
% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
\usepackage[accepted]{icml2024}

% If accepted, instead use the following line for the camera-ready submission:
%\usepackage[accepted]{icml2024}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{pifont}
\usepackage{diagbox}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}
\usepackage{bbding}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Navigating Semantic Drift in Task-Agnostic Class-Incremental Learning}

\begin{document}

\twocolumn[
\icmltitle{Navigating Semantic Drift in Task-Agnostic Class-Incremental Learning}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2024
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
%\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Fangwen Wu}{yyy}
\icmlauthor{Lechao Cheng\textsuperscript{\Envelope}}{sch1}
\icmlauthor{Shengeng Tang}{sch1}
\icmlauthor{Xiaofeng Zhu}{yyy}
\icmlauthor{Chaowei Fang}{sch2}\\
\icmlauthor{Dingwen Zhang}{sch3}
\icmlauthor{Meng Wang}{sch1}
%\icmlauthor{}{sch}
% \icmlauthor{Firstname8 Lastname8}{sch}
% \icmlauthor{Firstname8 Lastname8}{yyy,comp}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{yyy}{Zhejiang Lab}
\icmlaffiliation{sch1}{Hefei University of Technology}
\icmlaffiliation{sch2}{Xidian University}
\icmlaffiliation{sch3}{Northwestern Polytechnical University}

\icmlcorrespondingauthor{Lechao Cheng}{chenglc@hfut.edu.cn}
%\icmlcorrespondingauthor{Firstname2 Lastname2}{first2.last2@www.uk}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.
\printAffiliations{}
%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
%\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.


% \section{Electronic Submission}
% \label{submission}

% Submission to ICML 2024 will be entirely electronic, via a web site
% (not email). Information about the submission process and \LaTeX\ templates
% are available on the conference web site at:
% \begin{center}
% \textbf{\texttt{http://icml.cc/}}
% \end{center}

% The guidelines below will be enforced for initial submissions and
% camera-ready copies. Here is a brief summary:
% \begin{itemize}
% \item Submissions must be in PDF\@. 
% \item \textbf{New to this year}: If your paper has appendices, submit the appendix together with the main body and the references \textbf{as a single file}. Reviewers will not look for appendices as a separate PDF file. So if you submit such an extra file, reviewers will very likely miss it.
% \item Page limit: The main body of the paper has to be fitted to 8 pages, excluding references and appendices; the space for the latter two is not limited. For the final version of the paper, authors can add one extra page to the main body.
% \item \textbf{Do not include author information or acknowledgements} in your
%     initial submission.
% \item Your paper should be in \textbf{10 point Times font}.
% \item Make sure your PDF file only uses Type-1 fonts.
% \item Place figure captions \emph{under} the figure (and omit titles from inside
%     the graphic file itself). Place table captions \emph{over} the table.
% \item References must include page numbers whenever possible and be as complete
%     as possible. Place multiple citations in chronological order.
% \item Do not alter the style template; in particular, do not compress the paper
%     format by reducing the vertical spaces.
% \item Keep your abstract brief and self-contained, one paragraph and roughly
%     4--6 sentences. Gross violations will require correction at the
%     camera-ready phase. The title should have content words capitalized.
% \end{itemize}

% \subsection{Submitting Papers}

% \textbf{Paper Deadline:} The deadline for paper submission that is
% advertised on the conference website is strict. If your full,
% anonymized, submission does not reach us on time, it will not be
% considered for publication. 

% \textbf{Anonymous Submission:} ICML uses double-blind review: no identifying
% author information may appear on the title page or in the paper
% itself. \cref{author info} gives further details.

% \textbf{Simultaneous Submission:} ICML will not accept any paper which,
% at the time of submission, is under review for another conference or
% has already been published. This policy also applies to papers that
% overlap substantially in technical content with conference papers
% under review or previously published. ICML submissions must not be
% submitted to other conferences and journals during ICML's review
% period.
% %Authors may submit to ICML substantially different versions of journal papers
% %that are currently under review by the journal, but not yet accepted
% %at the time of submission.
% Informal publications, such as technical
% reports or papers in workshop proceedings which do not appear in
% print, do not fall under these restrictions.

% \medskip

% Authors must provide their manuscripts in \textbf{PDF} format.
% Furthermore, please make sure that files contain only embedded Type-1 fonts
% (e.g.,~using the program \texttt{pdffonts} in linux or using
% File/DocumentProperties/Fonts in Acrobat). Other fonts (like Type-3)
% might come from graphics files imported into the document.

% Authors using \textbf{Word} must convert their document to PDF\@. Most
% of the latest versions of Word have the facility to do this
% automatically. Submissions will not be accepted in Word format or any
% format other than PDF\@. Really. We're not joking. Don't send Word.

% Those who use \textbf{\LaTeX} should avoid including Type-3 fonts.
% Those using \texttt{latex} and \texttt{dvips} may need the following
% two commands:

% {\footnotesize
% \begin{verbatim}
% dvips -Ppdf -tletter -G0 -o paper.ps paper.dvi
% ps2pdf paper.ps
% \end{verbatim}}
% It is a zero following the ``-G'', which tells dvips to use
% the config.pdf file. Newer \TeX\ distributions don't always need this
% option.

% Using \texttt{pdflatex} rather than \texttt{latex}, often gives better
% results. This program avoids the Type-3 font problem, and supports more
% advanced features in the \texttt{microtype} package.

% \textbf{Graphics files} should be a reasonable size, and included from
% an appropriate format. Use vector formats (.eps/.pdf) for plots,
% lossless bitmap formats (.png) for raster graphics with sharp lines, and
% jpeg for photo-like images.

% The style file uses the \texttt{hyperref} package to make clickable
% links in documents. If this causes problems for you, add
% \texttt{nohyperref} as one of the options to the \texttt{icml2024}
% usepackage statement.


% \subsection{Submitting Final Camera-Ready Copy}

% The final versions of papers accepted for publication should follow the
% same format and naming convention as initial submissions, except that
% author information (names and affiliations) should be given. See
% \cref{final author} for formatting instructions.

% The footnote, ``Preliminary work. Under review by the International
% Conference on Machine Learning (ICML). Do not distribute.'' must be
% modified to ``\textit{Proceedings of the
% $\mathit{41}^{st}$ International Conference on Machine Learning},
% Vienna, Austria, PMLR 235, 2024.
% Copyright 2024 by the author(s).''

% For those using the \textbf{\LaTeX} style file, this change (and others) is
% handled automatically by simply changing
% $\mathtt{\backslash usepackage\{icml2024\}}$ to
% $$\mathtt{\backslash usepackage[accepted]\{icml2024\}}$$
% Authors using \textbf{Word} must edit the
% footnote on the first page of the document themselves.

% Camera-ready copies should have the title of the paper as running head
% on each page except the first one. The running title consists of a
% single line centered above a horizontal rule which is $1$~point thick.
% The running head should be centered, bold and in $9$~point type. The
% rule should be $10$~points above the main text. For those using the
% \textbf{\LaTeX} style file, the original title is automatically set as running
% head using the \texttt{fancyhdr} package which is included in the ICML
% 2024 style file package. In case that the original title exceeds the
% size restrictions, a shorter form can be supplied by using

% \verb|\icmltitlerunning{...}|

% just before $\mathtt{\backslash begin\{document\}}$.
% Authors using \textbf{Word} must edit the header of the document themselves.
\begin{abstract}
    Class-incremental learning (CIL) seeks to enable a model to sequentially learn new classes while retaining knowledge of previously learned ones. Balancing flexibility and stability remains a significant challenge, particularly when the task ID is unknown. To address this, our study reveals that the gap in feature distribution between novel and existing tasks is primarily driven by differences in mean and covariance moments. Building on this insight, we propose a novel semantic drift calibration method that incorporates mean shift compensation and covariance calibration. Specifically, we calculate each class's mean by averaging its sample embeddings and estimate task shifts using weighted embedding changes based on their proximity to the previous mean, effectively capturing mean shifts for all learned classes with each new task. We also apply Mahalanobis distance constraint for covariance calibration, aligning class-specific embedding covariances between old and current networks to mitigate the covariance shift. Additionally, we integrate a feature-level self-distillation approach to enhance generalization. Comprehensive experiments on commonly used datasets demonstrate the effectiveness of our approach. The source code is available at \href{https://github.com/fwu11/MACIL.git}{https://github.com/fwu11/MACIL.git}.
\end{abstract}


% \input{sections/abstract}
\section{Introduction}

\begin{figure}[!htb]
\centering
\subfigure[Semantic Drift]{
        \centering
        \includegraphics[trim=0.5cm 2cm 3cm 0.1cm, clip, width=0.46\linewidth]{images/original_shift.pdf}
        \label{fig:ori_shift}
        }
\subfigure[Calibration]{
        \centering
        \includegraphics[trim=0.1cm 1cm 3cm 1cm, clip, width=0.46\linewidth]{images/shift_cali.pdf}
        \label{fig:shift_cali}
        }
% \begin{subfigure}[b]{0.48\linewidth}
%         \centering
%         \includegraphics[width=\linewidth]{images/calibration_result.pdf}
%         \label{fig:shift_cali}
%         \caption{Calibration}
% \end{subfigure}%
% \includegraphics[width=0.23\textwidth]{images/original_vs_shift.pdf}
% \includegraphics[width=0.23\textwidth]{images/calibration_result.pdf}
%\vspace{-5mm}
\caption{\small As new tasks are learned, the categories from previously tasks in the latest updated model continuously experience shifts in their means and variances, referred to as (a) \textbf{Semantic Drift}. In this paper, we calibrate such semantic drift by applying explicit mean shift compensation and implicit variance constraints (b).}
\vspace{-5mm}
\label{fig:teaser}
\end{figure}
%\input{sections/fig_shif

Continual Learning, also referred to as lifelong learning, aims to enable machine learning models to sequentially learn multiple tasks over their life-cycle without requiring retraining or access to data from previous tasks \cite{rebuffi2017icarl}. The primary goal of continual learning is to facilitate knowledge accumulation and transfer, allowing models to adapt quickly to new, unseen tasks while maintaining robust performance on previously learned tasks \cite{parisi2019continual}. This capability has broad applications in fields such as computer vision, robotics, and natural language processing.
% In the human perceptual system, neurosynaptic plasticity is a fundamental mechanism that enables the brain to adapt to dynamic environments by inducing physical changes in neural structures, allowing us to learn, remember, and evolve over time. However, neuroscientific studies have revealed that humans experience asymmetric interference effects during perceptual learning. To ensure the long-term durability of existing knowledge and avoid catastrophic interference when acquiring new tasks and skills, it is crucial to protect and consolidate previously learned information. These biological principles provide valuable insights into how machine learning models can mitigate catastrophic forgetting and enable dynamical adaptation, guiding researchers to address the classic \textit{Stability–Plasticity Dilemma}.

In recent years, the issue of model plasticity has become less prominent in deep learning-based approaches, primarily due to two factors: (1) the increasing capacity of deep models allows them to effectively over-fit new data, and (2) large-scale pre-training on extensive datasets equips models with powerful feature extraction capabilities \cite{he2019rethinking}. Parameter-efficient fine-tuning based on pretrained models has further enhanced model plasticity, as highlighted in numerous recent studies \cite{houlsby2019parameter,lester-etal-2021-power,hu2022lora}.

Despite these advancements, existing methods—such as regularization \cite{kirkpatrick2017overcoming}, memory replay \cite{lopez2017gradient}, and knowledge distillation \cite{li2017learning}—while improving stability to some extent, introduce additional costs. For example, (1) memory replay methods require storing and retraining on old task data, increasing storage and computational demands. (2) knowledge distillation involves additional computational overhead during the distillation process, complicating and slowing down training. These additional costs hinder the practical deployment of continual learning methods. Therefore, a key challenge in the field is to improve model stability while minimizing resource consumption and computational overhead \cite{wang2024comprehensive}.

In this paper, we build upon successful practices by leveraging parameter-efficient fine-tuning based on pretrained models to further analyze catastrophic forgetting. Through extensive experimental observations, we discovered that although low-rank adaptation (e.g., LoRA \cite{hu2022lora} ) based on pretrained models can effectively maintain model plasticity, the incremental integration of tasks and model updates induces feature mean and covariance shift—what we also term \textbf{Semantic Drift}. As illustrated in Figure~\ref{fig:teaser}, the feature distribution of the original task data undergoes significant mean shifts and changes in variance shape as more tasks are introduced.

Based on these observations, we propose to address semantic drift with both mean shift compensation and covariance calibration, which constrain the first-order and second-order moments of the features, respectively. Specifically, we compute mean class representations after learning a novel task as the average embedding of all samples in class. The shift between old and novel tasks is approximated by the weighted average of embedding shifts, where the weights are determined by the proximity of each embedding to the previous class mean. This approach effectively estimates the mean shift for all previously learned classes during each new task. We also introduce an implicit covariance calibration technique using Mahalanobis distance \cite{mahalanobis1936generalized} loss to address semantic drift. This method aligns the covariance matrices of embeddings from old and current networks for each class, ensuring consistent intra-class distributions. By leveraging the old network as "past knowledge", we compute class-specific covariance matrices and minimize the absolute differences in Mahalanobis distances between embedding pairs from both networks. This approach effectively mitigates covariance shift, maintaining model stability while allowing continual learning. As shown in Figure \ref{fig:shift_cali}, these constraints effectively maintain model stability while preserving plasticity. Additionally, we implement feature self-distillation for patch tokens, further enhancing feature stability. In summary, our main contributions are:
\begin{itemize}
    %\setlength{\itemsep}{0pt}
    %\setlength{\parsep}{0pt}
    \setlength{\parskip}{0pt}
    \item We delve into the exploration of the semantic drift problem in class-incremental learning and further propose efficient and straightforward solutions—mean shift compensation and covariance calibration, which significantly alleviate this challenge.
    \item We orchestrate an efficient task-agnostic continual learning framework that outperforms existing methods across multiple public datasets, demonstrating the superiority of our approach.
\end{itemize}

% \input{sections/introducti
%\input{sections/related_work}
\section{Related Works}
\subsection{Class-Incremental Learning}

Class-Incremental Learning (CIL) aims to enable a model to sequentially learn new classes without forgetting previously learned ones. This presents a significant challenge, especially since task-IDs are not available during inference. To address this issue, several strategies have been proposed, which can be broadly categorized as follows:

Regularization-based methods \cite{li2017learning, rebuffi2017icarl, kirkpatrick2017overcoming, zenke2017continual} focus on constraining changes to important model parameters during training on new classes. Replay-based methods address forgetting by maintaining a memory buffer that stores examples from prior tasks. When learning a new task, the model is trained not only on the current task but also on these stored examples, helping it retain previous knowledge. These methods include direct replay \cite{lopez2017gradient, MER, AGEM, liu2021rmm} as well as generative replay \cite{shin2017continual, Zhu_2021_CVPR}. Optimization-based methods focus on explicitly designing and modifying the optimization process to reduce catastrophic forgetting, such as through gradient projection \cite{farajtabar2020orthogonal, saha2021gradient, lu2024visual} or loss function adjustments \cite{wang2021training, pmlr-v202-wen23b}. Representation-based methods aim to maintain a stable and generalizable feature space as new classes are added. These include self-supervised learning \cite{cha2021co2l, pham2021dualnet} and the use of pre-trained models \cite{wang2022s, Gao2024BeyondPL, mcdonnell2023ranpac}. A key challenge for exemplar-free methods is the shift in backbone features. Recent studies have proposed estimating this shift through changes in class prototypes \cite{yu2020semantic, gomez2025exemplar, goswami2024resurrecting}. This work investigates the semantic drift phenomenon in both the mean and covariance, calibrating them to mitigate catastrophic forgetting.

%\textcolor{red}{While \cite{2024taskrecency} attempted to adapt to changes in both the mean and covariance of feature distributions, this work investigates the semantic shift phenomenon in both the mean and covariance, calibrating them to mitigate catastrophic forgetting.}
% Class-Incremental Learning with Task Prediction.
% decompose task into task-id prediction and within-task prediction
% HiDe-Prompt \cite{wang2024hierarchical} \cite{huang2024ovor}

\subsection{Pre-trained Model based Class-Incremental Learning}
Pre-trained models have become a key component in CIL due to their ability to transfer knowledge efficiently. It is prevailing to use parameter-Efficient Fine-Tuning (PEFT) methods to adapt the model computation efficiently. PEFT methods introduce a small portion of the learnable parameters while keeping the pre-trained model frozen. LoRA \cite{hu2022lora} optimizes the weight space using low-rank matrix factorization, avoiding full parameter fine-tuning; VPT \cite{jia2022visual, wang2024revisiting} injects learnable prompts into the input or intermediate layers to extract task-specific features while freezing the backbone network; AdaptFormer \cite{chen2022adaptformer}, based on adaptive Transformer components, integrates task-specific information with general knowledge. 

Prompt-based class-incremental continual learning methods dynamically adjust lightweight prompt parameters to adapt to task evolution. Key mechanisms include: dynamic prompt pool retrieval \cite{wang2022learning}, general and expert prompt design for knowledge sharing \cite{wang2022dualprompt}, discrete prompt optimization \cite{jiao2024vector}, consistency alignment between classifiers and prompts \cite{gao2024consistent}, decomposed attention \cite{smith2023coda}, one forward stage \cite{kim2024one}, and evolving prompt adapting to task changes \cite{kurniawan2024evoprompt}.  Adapter-based methods such as EASE \cite{zhou2024expandable} dynamically expand task-specific subspaces and integrate multiple adapter predictions with semantic-guided prototype synthesis to mitigate feature degradation of old classes; SSIAT \cite{tan2024semantically} continuously tunes shared adapters and estimates mean shifts, updating prototypes to align new and old task features; and MOS \cite{sun2024mos} merges adapter parameters and employs a self-optimization retrieval mechanism to optimize module compatibility and inference efficiency.  LoRA-based methods, such as InfLoRA \cite{liang2024inflora}, introduce orthogonal constraints to isolate low-rank subspaces, effectively reducing parameter interference between tasks. Together, these methods offer efficient and scalable solutions for adapting pre-trained models to class-incremental learning tasks. In this work, we leverage the power of the LoRA in the context of CIL and build our semantic drift calibration modules on top of it.  
%\input{sections/method}
\section{Method}
\input{fig_framework}
\subsection{Preliminaries}
\subsubsection{Class-Incremental Learning}
Consider a dataset consisting of $T$ tasks $\{\mathcal{D}^t\}_{t=1}^T$. For each task, the dataset $\mathcal{D}^t = \{(x_i^t, y_i^t)\}_{i=1}^{n^t}$ contains $n^t$ inputs $x_i^t \in \mathbb{R}^d$ and their corresponding labels $y_i^t \in C^t$. We use $X^t$ and $Y^t$ to denote the collection of input data and label of task $t$, respectively and $C^t =\{c_i\}_{i=1}^{|C^t|}$ is the label set contains 
$|C^t|$ classes. In the class-incremental setting, for any task $i \neq j$, the input data from different tasks follow different distributions, i.e., $p(X^i) \neq p(X^j)$ and labels satisfy $C^i \cap C^j = \emptyset$.
The learning objective is to find the model $f^t: \mathbb{R}^{D} \to \mathbb{R}^{\mathcal{C}^t}$, where $\mathcal{C}^t = \cup_{i=1}^{t}C^i$ 
represents the total number of classes learned. This model is trained on all training datasets to perform well on all test dataset seen up to task $t$. In our scenario, the model $f^t$ is based on a pre-trained model, consisting of $f_{\theta}^t(x) = W^\top \phi_{\theta}^t(x)$, where $\phi_{\theta}^t:\mathbb{R}^D \to \mathbb{R}^d$ is a feature extractor composed of a a frozen pre-trained model $\phi$ and learnable parameters $\theta$ in the LoRA modules, and a classification head $W=\{w^t\}^{T}_{t=1}$ for each task, where we have $w^t \in \mathbb{R}^{d \times C_t}$. For a given task $t$, the old network $f_{\theta}^{t-1}(x)$ refers to the network trained on task $t-1$, and it is frozen in task $t$.
% \subsubsection{LoRA based PEFT}
% LoRA \cite{hu2022lora} introduces low-rank matrices into the model's weight updates, which allows the model to adapt without changing the full set of parameters. Consider the pre-trained weight matrix $W_0 \in \mathbb{R}^{d \times k}$, the update of the weight matrix is decomposed into the product of the low rank matrices $B \in \mathbb{R}^{d \times r}$ and $A \in \mathbb{R}^{r \times k}$, where $r$ is a value much smaller than the input dimension $d$ and the output dimension $k$. The forward pass of the low-rank update precess can be expressed as $h = W_0x+\Delta Wx = W_0x + BAx$. Here, $h$ and $x$ denote the input and output of the LoRA module. During training, only the low-rank weight matrices $A$ and $B$ are learnable, and the pre-trained weight $W_0$ is kept frozen.
% \vspace{-3mm}
\subsection{Overview}
Figure \ref{fig:framework} illustrates the overall architecture for class-incremental learning. We employ a frozen pre-trained ViT \cite{dosovitskiy2021an} model as the backbone with learnable task-specific LoRA modules. The output class tokens are forwarded through a task-specific classifier, which generates the class scores, while the angular penalty loss \cite{peng2022few,tan2024semantically} is used to compute the classification loss $\mathcal{L}_{\text{cls}}$:
\begin{equation}
    \mathcal{L}_{\text{cls}} = -\frac{1}{n^t} \sum_{j=1}^{n^t}\log \frac{\exp({s\cos(\theta_j)})}{\sum_{i=1}^{|C^t|} \exp({s\cos(\theta_i)})}
\end{equation}
where $\cos(\theta_j)=\frac{w_jf_{\theta_j}}{ \parallel w_j \parallel \parallel f_{\theta_j} \parallel}$, $s$ represents the scaling factor, and $n^t$ is the number of training samples in task $t$. The mean and
covariance of each class are stored for each learning session.
Before the training process, covariance of each class is precomputed from the class tokens generated by the network trained on previous tasks. These covariance matrices are then used to align the distribution of the representations generated by the current network with that of the old network, based on the Mahalanobis distance. This is referred to as the covariance calibration loss $\mathcal{L}_{\text{cov}}$. Furthermore, patch tokens are leveraged to preserve knowledge from earlier tasks at the feature level through a distillation loss $\mathcal{L}_{\text{distill}}$. 

After the training process, the class means are updated through the mean shift compensation process, and the classifier heads are retrained using the calibrated class statistics. The training pipeline is illustrated in Algorithm \ref{alg:algorithm}. In summary, the overall training objective is:
\begin{equation}
    \mathcal{L} = \mathcal{L}_{\text{cls}} + \mathcal{L}_{\text{cov}} + \lambda \mathcal{L}_{\text{distill}}
\end{equation}

\subsection{Low-Rank Adaptation}
In class-incremental learning (CIL), task IDs are not provided during the inference stage. For methods based on pre-trained models, the use of task-specific PEFT modules often involves a task ID prediction step during testing \cite{wang2022dualprompt,wang2024hierarchical,sun2024mos}. Other approaches avoid task ID prediction, as low prediction accuracy can negatively impact performance. For instance, some methods use weighted sums of prompts to determine the prompts applied during inference \cite{kurniawan2024evoprompt, smith2023coda}, or aggregate all previous PEFT modules \cite{zhou2024expandable, liang2024inflora}. Alternatively, some methods rely on a shared PEFT module across tasks \cite{huang2024ovor, tan2024semantically}.

Compared to other PEFT modules applied in CIL, such as prompts and adapters, LoRA~\cite{hu2022lora}  performs inference by adding the low-rank adaptation matrices to the original weight matrices. This enables LoRA to efficiently combine the pre-trained model weights with the task-specific adaptation matrices, as well as the information across the different task-specific matrices.

In this paper, we utilize task-specific LoRA modules, where each task is assigned a unique LoRA module. Considering the pre-trained weight matrix $W_0 \in \mathbb{R}^{d \times k}$, the update of the weight matrix is decomposed into the product of the low rank matrices $B \in \mathbb{R}^{d \times r}$ and $A \in \mathbb{R}^{r \times k}$, where $r$ is a value much smaller than the input dimension $d$ and the output dimension $k$.  The aggregation of all previous LoRA modules at task $t$ can be expressed as $W = W_0 + \sum_{i=1}^t B_iA_i$. Additionally, we explore two other LoRA structures: the task-shared LoRA module, which uses a common LoRA module for all tasks ($W = W_0 + BA$), and a hybrid structure that combines both task-specific and task-shared designs, inspired by HydraLoRA \cite{tian2024hydralora}. In this hybrid structure, the shared LoRA module's matrix $A$ is used across tasks, while the independent LoRA module's matrix $B_i$ is specific to each task ($W = W_0 + \sum_{i=1}^t B_iA$). During training, only the low-rank weight matrices $A_i$ and $B_i$ are learnable, and the pre-trained weight $W_0$ is frozen.
\subsection{Semantic Drift}
As tasks increase, we no longer have access to the data from previous tasks, and therefore cannot compute the true distribution of earlier classes under the incrementally trained network. Both the mean and variance of feature distributions of the old classes change. This phenomenon is referred to as semantic drift. When semantic drift occurs, it affects the classifier's performance. Thus, it is necessary to impose constraints on the semantic drift and calibrate the means and covariances of class distributions.

\subsubsection{Mean Shift Compensation}
We define the mean class representation of class $c$ in the embedding space after learning task $t$ as:
\begin{equation}
    \mu_c^t = \frac{1}{N_c} \sum_{i=1}^{N_c} [y_i = c] \phi_{\theta}^t(x_i)
\end{equation}
where $N_c$ is the number of samples for class $c$. The shift between the class mean obtained with the current network and the class mean obtained with the previous network can be defined as:
\begin{equation}
    \Delta \mu_c^{t-1 \rightarrow t} = \mu_c^t - \mu_c^{t-1}
\end{equation}
Previous work \cite{yu2020semantic} suggests that the shift between the true class mean and the estimated class mean can be approximated by the shift in the current class embeddings between the old and current models. Specifically, the shift of the class embeddings can be defined as:
\begin{equation}
    \Delta \phi_{\theta}^{t-1 \rightarrow t}(x_i) = \phi_{\theta}^t(x_i) - \phi_{\theta}^{t-1}(x_i)
\end{equation}
where $x_i$ belongs to class $c$. We can compute $\phi_{\theta}^{t-1}(x_i)$ using the model trained on task $t-1$ before the current task training. Then, we compute the drift $\phi_{\theta}^t(x_i)$ and use it to approximate the class mean shift $\Delta \mu_c^{t-1 \rightarrow t}$:
\begin{equation}
    \hat{\Delta} \mu_c^{t-1 \rightarrow t} = \frac{\sum_{i} w_i \Delta \phi_{\theta}^{t-1 \rightarrow t}(x_i)}{\sum_{i} w_i}
\end{equation}
with
\begin{equation}
    w_i = \exp({-\frac{\|\phi_{\theta}^{t-1}(x_i) - \mu_c^{t-1}\|^2}{2\sigma^2}})
\end{equation}
where $\sigma$ is the standard deviation of the Gaussian kernel, and the weight $w_i$ indicates that embeddings closer to the class mean contribute more to the mean shift estimation of that particular class. The proposed method can be used to compensate the mean shift of all previously learned classes at each new task with $\mu_c^t \approx \hat\mu_c^t =\hat\Delta \mu_c^{t-1 \rightarrow t} + \mu_c^{t-1}$.


\subsubsection{Covariance Calibration}
In this section, we address semantic shift from the perspective of the covariance matrix by introducing a novel covariance calibration technique, which is powered by a Mahalanobis distance-based loss. The objective is to ensure that, for each class in the new dataset, the embeddings generated by both the old and current networks follow the same covariance structure. Specifically, the covariance matrices of the embeddings from the current network should be aligned with those from the old network. To achieve this, we utilize the old network, trained on the previous task, as a form of "past knowledge" and use it to calculate the covariance for each class in the current task. Since the Mahalanobis distance directly depends on the covariance matrix, optimizing the difference between embedding pairs from the old and current networks in terms of Mahalanobis distance implicitly constrains the shape of the intra-class distribution, thus alleviating covariance shift.

Mathematically, the Mahalanobis distance \cite{mahalanobis1936generalized} is defined as the degree of difference between two random variables $x$ and $y$, which follow the same distribution and share the covariance matrix $\Sigma$:
\begin{equation}
d_M(x, y, \Sigma) = \sqrt{(x - y)^T \Sigma^{-1} (x - y)} 
\end{equation}
In our setting, we calculate the Mahalanobis distance $d_M( \phi_{\theta}^t(x_i),  \phi_{\theta}^t(x_j), \Sigma^{t-1}_c)$ using  the embedding pairs calculated with the data from the current task $t$ and as computed by covariance matrix $\Sigma^{t-1}_c$ of the class $c$ from the current task with the old network $f^{t-1}$.

Before training, for each class $c$, the covariance matrix is computed as:
\begin{equation}
    \Sigma^{t-1}_c = \frac{1}{N_c} \sum_{i=1}^{N_c} (x_i - \mu^{t-1}_c)(x_i - \mu^{t-1}_c)^T
\end{equation}
where $\mu_c^{t-1}$ is the class mean of the class $c$ calculated with the old network $f^{t-1}$. The loss function for minimizing the absolute difference in Mahalanobis distances of the sample input pairs $(x_i,x_j)$ between embeddings obtained from the old and new networks:
\begin{equation}
\begin{split}
\mathcal{L}_{\text{cov}} &= \sum_{c \in C^t} \sum_{i,j} | d_M( \phi_{\theta}^t(x_i),  \phi_{\theta}^t(x_j), \Sigma^{t-1}_c) \\
&- d_M( \phi_{\theta}^{t-1}(x_i),  \phi_{\theta}^{t-1}(x_j), \Sigma^{t-1}_c) |
\end{split}
\end{equation}

\subsection{Classifier Alignment}
The model exhibits a tendency to prioritize the categories associated with the current task, resulting in a degradation of classification performance for categories from previous tasks. Upon completing the training for each task, the classifier undergoes post hoc retraining using the statistics of previously learned classes, thereby enhancing its overall performance. It is assumed that the feature representations learned by the pre-trained model for each class follow a Gaussian distribution \cite{zhang2023slca}. In this framework, the mean $\mu_c$ and covariance $\Sigma_c$ of the feature representation for each class $c$, as described in previous sections, are calculated and stored. A number of $s_c$ samples $h^c$ are then drawn from the Gaussian distribution $\mathcal{N}(\mu_c, \Sigma_c)$ for each class as input. The classification head is subsequently retrained using a cross-entropy loss function:
\begin{equation}
    \mathcal{L}_{head} = -\frac{1}{s_c|\mathcal{C}|} \sum_{i=1}^{s_c|\mathcal{C}|} \log \left( \frac{e^{w_j{h^c_i}}}{\sum_{k \in \mathcal{C}} e^{w_k(h^c_i)}} \right)
\end{equation}
where $\mathcal{C}$ denotes all classes learned until current task, $w$ is the classifier. With the alignment of semantic drift, the true class mean and covariance are calibrated, which helps mitigate the classifier’s bias, typically induced by overconfidence in new tasks, and alleviates the issue of catastrophic forgetting.

\subsection{Feature-level Self-Distillation}
To enhance the model's resistance to catastrophic forgetting, we propose a self-distillation approach that focuses on improving the utilization of patch tokens in classification tasks, as suggested in \cite{li2024dynamic, wang2024improving}. In Vision Transformers (ViT), the information from patch tokens is often underutilized, which can limit the model's ability to generalize across tasks \cite{zhai2024fine}. To address this, we introduce a self-distillation loss based on patch tokens.

In this method, the class token output from the current network is treated as the essential feature information that needs to be learned for the current task. The feature-level self-distillation loss encourages the alignment of patch tokens from the current network output with the class token. Specifically, we compute the angular similarity, $\text{sim}_{\angle
}$, between the current task's patch tokens, denoted as \( p_j^t \), and the class token \( c^t \), with the features normalized using L2 normalization. The loss is then formulated as:
\begin{equation}
    \mathcal{L}_{\text{distill}} = \frac{1}{L} \sum_{j=1}^{L} \left( 1 - \text{sim}_{\angle
}(p_j^t, c^t) \right) \cdot \left\| p_j^t - p_j^{t-1} \right\|_2^2 
\end{equation}
where \( p_j^t \) is the \( j \)-th patch token for the current task, \( c^t \) is the class token for the current task, \( p_j^{t-1} \) is the patch token from the previous task, and \( L \) is the total number of patch tokens for the current task. We disable the gradient updates during angular similarity computation.

The low angular similarity between patch tokens and class tokens suggests that the patch tokens contribute less to the semantic representation of the current task. To encourage better feature reuse, patch tokens with low similarity are encouraged to resemble the patch tokens from the previous network, thereby improving the retention of important task-related features. This approach ensures that patch tokens are more effectively utilized, contributing to the model's robustness and its ability to mitigate forgetting when transitioning between tasks.

\input{algo_1}

% \input{sections/experiments}
\section{Experiments}
\subsection{Setup}
\input{tab_main_results}
\subsubsection{Datasets and Metrics.}
We train and validate our method using four popular CIL datasets. ImageNet-R \cite{hendrycks2021many} is generated by applying artistic processing to 200 classes from ImageNet. The dataset consists of 200 categories, and we split ImageNet-R into 5, 10, and 20 tasks, with each task containing 40, 20, and 10 classes, respectively. CIFAR-100 \cite{Krizhevsky2009LearningML} is a widely used dataset in CIL, containing 60,000 images across 100 categories. We also split CIFAR-100 into 5, 10, and 20 tasks with each task containing 20, 10, 5 classes, respectively. CUB-200 \cite{WahCUB_200_2011} is a fine-grained dataset containing approximately 11,788 images of 200 bird species with detailed class labels. ImageNet-A \cite{hendrycks2021natural}is a real-world dataset consisting of 200 categories, notable for significant class imbalance, with some categories having very few training samples. We split CUB-200 and ImageNet-A into 10 tasks with 20 classes each. 

% VTAB \cite{zhai2019large} is a complex dataset consisting of 19 tasks that span a broad range of domains and semantics. Following previous work \cite{zhou2024revisiting}, we select 5 tasks from VTAB to construct a cross-domain CIL dataset.

We follow the commonly used evaluation metrics in CIL. We denote $a_{i,j}$ as the classification accuracy evaluated on the test set of the $j$-th task (where $j \leq i$) after learning $i$ tasks in incremental learning. The final accuracy is calculated as $\mathcal{A}_{\text{last}} = \frac{1}{t}\sum_{j=1}^t a_{i,j}$
and the average accuracy of all incremental tasks is $\mathcal{A}_{\text{avg}} = \frac{1}{T}\sum_{i=1}^T \mathcal{A}_i$. In line with other studies, our evaluation results are based on three trials with three different seeds. We report both the mean and standard deviation of the trials.

\subsubsection{Implementation Details.}
In our experiment, we adopt ViT-B/16 \cite{dosovitskiy2021an} pre-trained on ImageNet21K \cite{russakovsky2015imagenet} as the backbone. We use the SGD optimizer with the initial learning rate set as 0.01 and we use the Cosine Annealing scheduler. We train the first session for 20 epochs and 10 epochs for later sessions. The batch size is set to 48 for all the experiments. LoRA module is inserted to the key and value of all the attention layers in the transformer. The distillation loss weight \( \lambda \) is set to 0.4, the LoRA rank \( r \) is set to 32, and the scale \( s \) in the angular penalty loss is set to 20. These values are determined through sensitivity analysis.


\subsection{Comparison with State-of-the-arts}
\input{tab_seq_results}
We conduct a comparative evaluation of our proposed method against state-of-the-art (SOTA) class-incremental learning (CIL) approaches based on pre-trained models, with a particular focus on techniques utilizing parameter-efficient fine-tuning (PEFT). To ensure a fair comparison, we evaluate all methods using the same ViT-B/16-IN21K pre-trained models, identical random seeds, and consistent class orders. Specifically, we compare prompt-based approaches, including L2P \cite{wang2022learning}, DualPrompt \cite{wang2022dualprompt}, CODAPrompt \cite{smith2023coda}, VQ-Prompt \cite{jiao2024vector}, OS-Prompt \cite{kim2024one}, and CPrompt \cite{gao2024consistent}; adapter-based methods such as SSIAT \cite{tan2024semantically}, EASE \cite{zhou2024expandable}, and MOS \cite{sun2024mos}; the LoRA-based method InfLoRA \cite{liang2024inflora} integrated with Classifier Alignment, referred to as InfLoRA+CA; the first-session adaptation method RanPAC \cite{mcdonnell2023ranpac}, which only trains PEFT modules using data from the first task; and the fine-tuning method SLCA \cite{zhang2023slca}, which also incorporates the Classifier Alignment step as in our approach. 

Table \ref{tab:results} summarizes the performance of these SOTA methods across four widely used benchmark datasets. We report both the accuracy on the last task ($\mathcal{A}_{\text{last}}$) and the average accuracy across all tasks ($\mathcal{A}_{\text{avg}}$), presenting the mean and standard deviation over three independent runs with different random seeds. The use of random seeds introduces variability in class order across runs, making the evaluation of model performance more challenging. Notably, our method achieves superior performance in both $\mathcal{A}_{\text{last}}$ and $\mathcal{A}_{\text{avg}}$. Our method demonstrates impressive results, particularly on the more challenging datasets with larger domain gaps, such as ImageNet-R and ImageNet-A. On ImageNet-R, our method achieves a final accuracy of 81.88\%, surpassing the second-best method, SSIAT, by a significant margin of 2.33\%. The $\mathcal{A}_{\text{avg}}$ also surpasses the second-best SSIAT by 2.25\%. On the ImageNet-A dataset, our method achieves a final accuracy of 64.14\%, surpassing the second-best SSIAT by 1.49\%. These results highlight the effectiveness of our PEFT-based approach in significantly improving performance on datasets with large domain shifts, outperforming both first-session adaptation methods and full fine-tuning methods, as well as other PEFT-based approaches.

In contrast, on the CIFAR-100 and CUB-200 datasets, our method performs well, though with marginal benefits compared to other methods. Notably, on the CUB-200 dataset, our method achieves superior performance. It is also important to note that the first-session adaptation-based method, RanPAC, performs well on both CIFAR-100 and CUB-200, likely due to the significant relevance between the pretraining dataset (ImageNet) and these two datasets.

\input{fig_per_task_results}

Additionally, we evaluate the performance of our method on both longer task sequences (20 tasks) and shorter task sequences (5 tasks) for CIFAR-100 and ImageNet-R, as reported in Tables \ref{tab:cifar_imgr_comparison}. Across these varied experimental settings, our method consistently outperforms competing approaches, demonstrating its stability and robustness in handling diverse CIL scenarios.

Figure \ref{fig:per_task_results} illustrates the incremental accuracy of each session for three ImageNet-R settings and one CIFAR-100 setting. The results show that on ImageNet-R, our method consistently achieves the best performance, with a clear distinction from other methods. On CIFAR-100, our method is relatively stable, and the final results are comparable to the first-session adaptation-based method, RanPAC, which is closely related to pretraining data characteristics.
% Our approach demonstrates superior performance on the more challenging datasets, ImageNet-R and ImageNet-A, which exhibit a larger domain gap from the pre-training data and have substantial intra-class diversity. This highlights the robustness of our method in addressing domain shifts.  
% Additionally, we evaluate the performance of our method on both longer task sequences (20 tasks) and shorter task sequences (5 tasks) for CIFAR-100 and ImageNet-R, as reported in Tables \ref{tab:cifar_imgr_comparison}. Across these varied experimental settings, our method consistently outperforms competing approaches, demonstrating its stability and robustness in handling diverse CIL scenarios.
% \input{sections/tab_imgr_seq_results}
% \input{sections/tab_cifar_seq_results}
\subsection{Ablation Study}
\subsubsection{The Impact of Each Component}
As shown in Table \ref{tab:ablation_comp}, we systematically assess the contributions of different components to the baseline method on the ImageNet-R dataset. The baseline, consisting of a task-specific LoRA structure with angular penalty loss for classification, achieves a competitive performance of 79.36\% in \( \mathcal{A}_{\text{Last}} \). In exp-II, we add the MSC module, which, in conjunction with classifier alignment, provides an improvement of over 1.4\%. The incorporation of Covariance Calibration with classifier alignment in exp-III also leads to an improvement of approximately 1.35\%. These results underscore the importance of both mean shift compensation and covariance calibration in aligning feature distributions across tasks, thereby reducing catastrophic forgetting and enhancing stability across task sequences. Combining the MSC and CC modules gives a significant boost, improving performance by approximately 2.3\% above the baseline method. Finally, the inclusion of patch distillation offers a further marginal improvement, resulting in a state-of-the-art performance of 81.88\% for \( \mathcal{A}_{\text{Last}} \) and 85.95\% for \( \mathcal{A}_{\text{Avg}} \), confirming the effectiveness of our method.
\input{tab_ablation}
\subsubsection{LoRA Structures Design}
While there has been significant exploration of task-specific and task-shared PEFT modules, particularly concerning prompts and adapters, research on LoRA-based modules is relatively limited. In this paper, we investigate the use of task-specific and task-shared LoRA modules, as well as a hybrid architecture that combines both, inspired by Hydra-LoRA \cite{tian2024hydralora}. In Table \ref{tab:lora_results}, we evaluate these designs across four datasets and report the final accuracy averages from three trials. The results indicate that the performance differences among the LoRA designs are minimal, with the task-specific design slightly outperforming the other two, except for the ImageNet-A dataset, where the task-shared LoRA module achieves a marginally higher performance.
\input{tab_ablation_lora_structures}
%\input{sections/conclusion}
\section{Conclusion}
We analyze catastrophic forgetting in machine learning models using parameter-efficient fine-tuning based on pretrained models. Our experiments reveal that low-rank adaptations like LoRA induce feature mean and covariance shifts, termed Semantic Drift. To address this, we propose mean shift compensation and covariance calibration to constrain feature moments, maintaining both model stability and plasticity. Additionally, we implement feature self-distillation for patch tokens to enhance feature stability. Our task-agnostic continual learning framework outperforms existing methods across multiple public datasets.
%, demonstrating its effectiveness in mitigating Semantic Drift and improving continual learning performance.

% \section{Related Work}

% All submissions must follow the specified format.

% \subsection{Title}

% The paper title should be set in 14~point bold type and centered
% between two horizontal rules that are 1~point thick, with 1.0~inch
% between the top rule and the top edge of the page. Capitalize the
% first letter of content words and put the rest of the title in lower
% case.

% \subsection{Author Information for Submission}
% \label{author info}

% ICML uses double-blind review, so author information must not appear. If
% you are using \LaTeX\/ and the \texttt{icml2024.sty} file, use
% \verb+\icmlauthor{...}+ to specify authors and \verb+\icmlaffiliation{...}+ to specify affiliations. (Read the TeX code used to produce this document for an example usage.) The author information
% will not be printed unless \texttt{accepted} is passed as an argument to the
% style file.
% Submissions that include the author information will not
% be reviewed.

% \subsubsection{Self-Citations}

% If you are citing published papers for which you are an author, refer
% to yourself in the third person. In particular, do not use phrases
% that reveal your identity (e.g., ``in previous work \cite{langley00}, we
% have shown \ldots'').

% Do not anonymize citations in the reference section. The only exception are manuscripts that are
% not yet published (e.g., under submission). If you choose to refer to
% such unpublished manuscripts \cite{anonymous}, anonymized copies have
% to be submitted
% as Supplementary Material via OpenReview\@. However, keep in mind that an ICML
% paper should be self contained and should contain sufficient detail
% for the reviewers to evaluate the work. In particular, reviewers are
% not required to look at the Supplementary Material when writing their
% review (they are not required to look at more than the first $8$ pages of the submitted document).

% \subsubsection{Camera-Ready Author Information}
% \label{final author}

% If a paper is accepted, a final camera-ready copy must be prepared.
% %
% For camera-ready papers, author information should start 0.3~inches below the
% bottom rule surrounding the title. The authors' names should appear in 10~point
% bold type, in a row, separated by white space, and centered. Author names should
% not be broken across lines. Unbolded superscripted numbers, starting 1, should
% be used to refer to affiliations.

% Affiliations should be numbered in the order of appearance. A single footnote
% block of text should be used to list all the affiliations. (Academic
% affiliations should list Department, University, City, State/Region, Country.
% Similarly for industrial affiliations.)

% Each distinct affiliations should be listed once. If an author has multiple
% affiliations, multiple superscripts should be placed after the name, separated
% by thin spaces. If the authors would like to highlight equal contribution by
% multiple first authors, those authors should have an asterisk placed after their
% name in superscript, and the term ``\textsuperscript{*}Equal contribution"
% should be placed in the footnote block ahead of the list of affiliations. A
% list of corresponding authors and their emails (in the format Full Name
% \textless{}email@domain.com\textgreater{}) can follow the list of affiliations.
% Ideally only one or two names should be listed.

% A sample file with author names is included in the ICML2024 style file
% package. Turn on the \texttt{[accepted]} option to the stylefile to
% see the names rendered. All of the guidelines above are implemented
% by the \LaTeX\ style file.

% \subsection{Abstract}

% The paper abstract should begin in the left column, 0.4~inches below the final
% address. The heading `Abstract' should be centered, bold, and in 11~point type.
% The abstract body should use 10~point type, with a vertical spacing of
% 11~points, and should be indented 0.25~inches more than normal on left-hand and
% right-hand margins. Insert 0.4~inches of blank space after the body. Keep your
% abstract brief and self-contained, limiting it to one paragraph and roughly 4--6
% sentences. Gross violations will require correction at the camera-ready phase.

% \subsection{Partitioning the Text}

% You should organize your paper into sections and paragraphs to help
% readers place a structure on the material and understand its
% contributions.

% \subsubsection{Sections and Subsections}

% Section headings should be numbered, flush left, and set in 11~pt bold
% type with the content words capitalized. Leave 0.25~inches of space
% before the heading and 0.15~inches after the heading.

% Similarly, subsection headings should be numbered, flush left, and set
% in 10~pt bold type with the content words capitalized. Leave
% 0.2~inches of space before the heading and 0.13~inches afterward.

% Finally, subsubsection headings should be numbered, flush left, and
% set in 10~pt small caps with the content words capitalized. Leave
% 0.18~inches of space before the heading and 0.1~inches after the
% heading.

% Please use no more than three levels of headings.

% \subsubsection{Paragraphs and Footnotes}

% Within each section or subsection, you should further partition the
% paper into paragraphs. Do not indent the first line of a given
% paragraph, but insert a blank line between succeeding ones.

% You can use footnotes\footnote{Footnotes
% should be complete sentences.} to provide readers with additional
% information about a topic without interrupting the flow of the paper.
% Indicate footnotes with a number in the text where the point is most
% relevant. Place the footnote in 9~point type at the bottom of the
% column in which it appears. Precede the first footnote in a column
% with a horizontal rule of 0.8~inches.\footnote{Multiple footnotes can
% appear in each column, in the same order as they appear in the text,
% but spread them across columns and pages if possible.}


% \subsection{Figures}

% You may want to include figures in the paper to illustrate
% your approach and results. Such artwork should be centered,
% legible, and separated from the text. Lines should be dark and at
% least 0.5~points thick for purposes of reproduction, and text should
% not appear on a gray background.

% Label all distinct components of each figure. If the figure takes the
% form of a graph, then give a name for each axis and include a legend
% that briefly describes each curve. Do not include a title inside the
% figure; instead, the caption should serve this function.

% Number figures sequentially, placing the figure number and caption
% \emph{after} the graphics, with at least 0.1~inches of space before
% the caption and 0.1~inches after it, as in
% \cref{icml-historical}. The figure caption should be set in
% 9~point type and centered unless it runs two or more lines, in which
% case it should be flush left. You may float figures to the top or
% bottom of a column, and you may set wide figures across both columns
% (use the environment \texttt{figure*} in \LaTeX). Always place
% two-column figures at the top or bottom of the page.

% \subsection{Algorithms}

% If you are using \LaTeX, please use the ``algorithm'' and ``algorithmic''
% environments to format pseudocode. These require
% the corresponding stylefiles, algorithm.sty and
% algorithmic.sty, which are supplied with this package.
% \cref{alg:example} shows an example.

% \begin{algorithm}[tb]
%    \caption{Bubble Sort}
%    \label{alg:example}
% \begin{algorithmic}
%    \STATE {\bfseries Input:} data $x_i$, size $m$
%    \REPEAT
%    \STATE Initialize $noChange = true$.
%    \FOR{$i=1$ {\bfseries to} $m-1$}
%    \IF{$x_i > x_{i+1}$}
%    \STATE Swap $x_i$ and $x_{i+1}$
%    \STATE $noChange = false$
%    \ENDIF
%    \ENDFOR
%    \UNTIL{$noChange$ is $true$}
% \end{algorithmic}
% \end{algorithm}

% \subsection{Tables}

% You may also want to include tables that summarize material. Like
% figures, these should be centered, legible, and numbered consecutively.
% However, place the title \emph{above} the table with at least
% 0.1~inches of space before the title and the same after it, as in
% \cref{sample-table}. The table title should be set in 9~point
% type and centered unless it runs two or more lines, in which case it
% should be flush left.

% % Note use of \abovespace and \belowspace to get reasonable spacing
% % above and below tabular lines.

% \begin{table}[t]
% \caption{Classification accuracies for naive Bayes and flexible
% Bayes on various data sets.}
% \label{sample-table}
% \vskip 0.15in
% \begin{center}
% \begin{small}
% \begin{sc}
% \begin{tabular}{lcccr}
% \toprule
% Data set & Naive & Flexible & Better? \\
% \midrule
% Breast    & 95.9$\pm$ 0.2& 96.7$\pm$ 0.2& $\surd$ \\
% Cleveland & 83.3$\pm$ 0.6& 80.0$\pm$ 0.6& $\times$\\
% Glass2    & 61.9$\pm$ 1.4& 83.8$\pm$ 0.7& $\surd$ \\
% Credit    & 74.8$\pm$ 0.5& 78.3$\pm$ 0.6&         \\
% Horse     & 73.3$\pm$ 0.9& 69.7$\pm$ 1.0& $\times$\\
% Meta      & 67.1$\pm$ 0.6& 76.5$\pm$ 0.5& $\surd$ \\
% Pima      & 75.1$\pm$ 0.6& 73.9$\pm$ 0.5&         \\
% Vehicle   & 44.9$\pm$ 0.6& 61.5$\pm$ 0.4& $\surd$ \\
% \bottomrule
% \end{tabular}
% \end{sc}
% \end{small}
% \end{center}
% \vskip -0.1in
% \end{table}

% Tables contain textual material, whereas figures contain graphical material.
% Specify the contents of each row and column in the table's topmost
% row. Again, you may float tables to a column's top or bottom, and set
% wide tables across both columns. Place two-column tables at the
% top or bottom of the page.

% \subsection{Theorems and such}
% The preferred way is to number definitions, propositions, lemmas, etc. consecutively, within sections, as shown below.
% \begin{definition}
% \label{def:inj}
% A function $f:X \to Y$ is injective if for any $x,y\in X$ different, $f(x)\ne f(y)$.
% \end{definition}
% Using \cref{def:inj} we immediate get the following result:
% \begin{proposition}
% If $f$ is injective mapping a set $X$ to another set $Y$, 
% the cardinality of $Y$ is at least as large as that of $X$
% \end{proposition}
% \begin{proof} 
% Left as an exercise to the reader. 
% \end{proof}
% \cref{lem:usefullemma} stated next will prove to be useful.
% \begin{lemma}
% \label{lem:usefullemma}
% For any $f:X \to Y$ and $g:Y\to Z$ injective functions, $f \circ g$ is injective.
% \end{lemma}
% \begin{theorem}
% \label{thm:bigtheorem}
% If $f:X\to Y$ is bijective, the cardinality of $X$ and $Y$ are the same.
% \end{theorem}
% An easy corollary of \cref{thm:bigtheorem} is the following:
% \begin{corollary}
% If $f:X\to Y$ is bijective, 
% the cardinality of $X$ is at least as large as that of $Y$.
% \end{corollary}
% \begin{assumption}
% The set $X$ is finite.
% \label{ass:xfinite}
% \end{assumption}
% \begin{remark}
% According to some, it is only the finite case (cf. \cref{ass:xfinite}) that is interesting.
% \end{remark}
% %restatable

% \subsection{Citations and References}

% Please use APA reference format regardless of your formatter
% or word processor. If you rely on the \LaTeX\/ bibliographic
% facility, use \texttt{natbib.sty} and \texttt{icml2024.bst}
% included in the style-file package to obtain this format.

% Citations within the text should include the authors' last names and
% year. If the authors' names are included in the sentence, place only
% the year in parentheses, for example when referencing Arthur Samuel's
% pioneering work \yrcite{Samuel59}. Otherwise place the entire
% reference in parentheses with the authors and year separated by a
% comma \cite{Samuel59}. List multiple references separated by
% semicolons \cite{kearns89,Samuel59,mitchell80}. Use the `et~al.'
% construct only for citations with three or more authors or after
% listing all authors to a publication in an earlier reference \cite{MachineLearningI}.

% Authors should cite their own work in the third person
% in the initial version of their paper submitted for blind review.
% Please refer to \cref{author info} for detailed instructions on how to
% cite your own papers.

% Use an unnumbered first-level section heading for the references, and use a
% hanging indent style, with the first line of the reference flush against the
% left margin and subsequent lines indented by 10 points. The references at the
% end of this document give examples for journal articles \cite{Samuel59},
% conference publications \cite{langley00}, book chapters \cite{Newell81}, books
% \cite{DudaHart2nd}, edited volumes \cite{MachineLearningI}, technical reports
% \cite{mitchell80}, and dissertations \cite{kearns89}.

% Alphabetize references by the surnames of the first authors, with
% single author entries preceding multiple author entries. Order
% references for the same authors by year of publication, with the
% earliest first. Make sure that each reference includes all relevant
% information (e.g., page numbers).

% Please put some effort into making references complete, presentable, and
% consistent, e.g. use the actual current name of authors.
% If using bibtex, please protect capital letters of names and
% abbreviations in titles, for example, use \{B\}ayesian or \{L\}ipschitz
% in your .bib file.

% \section*{Accessibility}
% Authors are kindly asked to make their submissions as accessible as possible for everyone including people with disabilities and sensory or neurological differences.
% Tips of how to achieve this and what to pay attention to will be provided on the conference website \url{http://icml.cc/}.

% \section*{Software and Data}

% If a paper is accepted, we strongly encourage the publication of software and data with the
% camera-ready version of the paper whenever appropriate. This can be
% done by including a URL in the camera-ready copy. However, \textbf{do not}
% include URLs that reveal your institution or identity in your
% submission for review. Instead, provide an anonymous URL or upload
% the material as ``Supplementary Material'' into the OpenReview reviewing
% system. Note that reviewers are not required to look at this material
% when writing their review.

% % Acknowledgements should only appear in the accepted version.
% \section*{Acknowledgements}

% \textbf{Do not} include acknowledgements in the initial version of
% the paper submitted for blind review.

% If a paper is accepted, the final camera-ready version can (and
% usually should) include acknowledgements.  Such acknowledgements
% should be placed at the end of the section, in an unnumbered section
% that does not count towards the paper page limit. Typically, this will 
% include thanks to reviewers who gave useful comments, to colleagues 
% who contributed to the ideas, and to funding agencies and corporate 
% sponsors that provided financial support.

% \section*{Impact Statement}

% Authors are \textbf{required} to include a statement of the potential 
% broader impact of their work, including its ethical aspects and future 
% societal consequences. This statement should be in an unnumbered 
% section at the end of the paper (co-located with Acknowledgements -- 
% the two may appear in either order, but both must be before References), 
% and does not count toward the paper page limit. In many cases, where 
% the ethical impacts and expected societal implications are those that 
% are well established when advancing the field of Machine Learning, 
% substantial discussion is not required, and a simple statement such 
% as the following will suffice:

% ``This paper presents work whose goal is to advance the field of 
% Machine Learning. There are many potential societal consequences 
% of our work, none which we feel must be specifically highlighted here.''

% The above statement can be used verbatim in such cases, but we 
% encourage authors to think about whether there is content which does 
% warrant further discussion, as this statement will be apparent if the 
% paper is later flagged for ethics review.


% % In the unusual situation where you want a paper to appear in the
% % references without citing it in the main text, use \nocite
% \nocite{langley00}
%\newpage
% \section*{Impact Statement}
% This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none of which we feel must be specifically highlighted here.
\bibliography{arxiv}
\bibliographystyle{icml2024}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \newpage
% \input{sections/appendix}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
