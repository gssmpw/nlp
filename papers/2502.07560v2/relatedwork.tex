\section{Related Works}
\subsection{Class-Incremental Learning}


Class-Incremental Learning (CIL) aims to enable a model to sequentially learn new classes without forgetting previously learned ones. This presents a significant challenge, especially since task-IDs are not available during inference. To address this issue, several strategies have been proposed, which can be broadly categorized as follows:

Regularization-based methods \cite{li2017learning, rebuffi2017icarl, kirkpatrick2017overcoming, zenke2017continual} focus on constraining changes to important model parameters during training on new classes. Replay-based methods address forgetting by maintaining a memory buffer that stores examples from prior tasks. When learning a new task, the model is trained not only on the current task but also on these stored examples, helping it retain previous knowledge. These methods include direct replay \cite{lopez2017gradient, MER, AGEM, liu2021rmm} as well as generative replay \cite{shin2017continual, Zhu_2021_CVPR}. Optimization-based methods focus on explicitly designing and modifying the optimization process to reduce catastrophic forgetting, such as through gradient projection \cite{farajtabar2020orthogonal, saha2021gradient, lu2024visual} or loss function adjustments \cite{wang2021training, pmlr-v202-wen23b}. Representation-based methods aim to maintain a stable and generalizable feature space as new classes are added. These include self-supervised learning \cite{cha2021co2l, pham2021dualnet} and the use of pre-trained models \cite{wang2022s, Gao2024BeyondPL, mcdonnell2023ranpac}. A key challenge for exemplar-free methods is the shift in backbone features. Recent studies have proposed estimating this shift through changes in class prototypes \cite{yu2020semantic, gomez2025exemplar, goswami2024resurrecting}. This work investigates the semantic drift phenomenon in both the mean and covariance, calibrating them to mitigate catastrophic forgetting.

%\textcolor{red}{While \cite{2024taskrecency} attempted to adapt to changes in both the mean and covariance of feature distributions, this work investigates the semantic shift phenomenon in both the mean and covariance, calibrating them to mitigate catastrophic forgetting.}

% Class-Incremental Learning with Task Prediction.
% decompose task into task-id prediction and within-task prediction
% HiDe-Prompt \cite{wang2024hierarchical} \cite{huang2024ovor}

\subsection{Pre-trained Model based Class-Incremental Learning}
Pre-trained models have become a key component in CIL due to their ability to transfer knowledge efficiently. It is prevailing to use parameter-Efficient Fine-Tuning (PEFT) methods to adapt the model computation efficiently. PEFT methods introduce a small portion of the learnable parameters while keeping the pre-trained model frozen. LoRA \cite{hu2022lora} optimizes the weight space using low-rank matrix factorization, avoiding full parameter fine-tuning; VPT \cite{jia2022visual} injects learnable prompts into the input or intermediate layers to extract task-specific features while freezing the backbone network; AdaptFormer \cite{chen2022adaptformer}, based on adaptive Transformer components, integrates task-specific information with general knowledge. 

Prompt-based class-incremental continual learning methods dynamically adjust lightweight prompt parameters to adapt to task evolution. Key mechanisms include: dynamic prompt pool retrieval \cite{wang2022learning}, general and expert prompt design for knowledge sharing \cite{wang2022dualprompt}, discrete prompt optimization \cite{jiao2024vector}, consistency alignment between classifiers and prompts \cite{gao2024consistent}, decomposed attention \cite{smith2023coda}, one forward stage \cite{kim2024one}, and evolving prompt adapting to task changes \cite{kurniawan2024evoprompt}.  Adapter-based methods such as EASE \cite{zhou2024expandable} dynamically expand task-specific subspaces and integrate multiple adapter predictions with semantic-guided prototype synthesis to mitigate feature degradation of old classes; SSIAT \cite{tan2024semantically} continuously tunes shared adapters and estimates mean shifts, updating prototypes to align new and old task features; and MOS \cite{sun2024mos} merges adapter parameters and employs a self-optimization retrieval mechanism to optimize module compatibility and inference efficiency.  LoRA-based methods, such as InfLoRA \cite{liang2024inflora}, introduce orthogonal constraints to isolate low-rank subspaces, effectively reducing parameter interference between tasks. Together, these methods offer efficient and scalable solutions for adapting pre-trained models to class-incremental learning tasks. In this work, we leverage the power of the LoRA in the context of CIL and build our semantic drift calibration modules on top of it.