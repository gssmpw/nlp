\section{Experiments}
\subsection{Setup}
\input{sections/tab_main_results}
\subsubsection{Datasets and Metrics.}
We train and validate our method using four popular CIL datasets. ImageNet-R \cite{hendrycks2021many} is generated by applying artistic processing to 200 classes from ImageNet. The dataset consists of 200 categories, and we split ImageNet-R into 5, 10, and 20 tasks, with each task containing 40, 20, and 10 classes, respectively. CIFAR-100 \cite{Krizhevsky2009LearningML} is a widely used dataset in CIL, containing 60,000 images across 100 categories. We also split CIFAR-100 into 5, 10, and 20 tasks with each task containing 20, 10, 5 classes, respectively. CUB-200 \cite{WahCUB_200_2011} is a fine-grained dataset containing approximately 11,788 images of 200 bird species with detailed class labels. ImageNet-A \cite{hendrycks2021natural}is a real-world dataset consisting of 200 categories, notable for significant class imbalance, with some categories having very few training samples. We split CUB-200 and ImageNet-A into 10 tasks with 20 classes each. 

% VTAB \cite{zhai2019large} is a complex dataset consisting of 19 tasks that span a broad range of domains and semantics. Following previous work \cite{zhou2024revisiting}, we select 5 tasks from VTAB to construct a cross-domain CIL dataset.

We follow the commonly used evaluation metrics in CIL. We denote $a_{i,j}$ as the classification accuracy evaluated on the test set of the $j$-th task (where $j \leq i$) after learning $i$ tasks in incremental learning. The final accuracy is calculated as $\mathcal{A}_{\text{last}} = \frac{1}{t}\sum_{j=1}^t a_{i,j}$
and the average accuracy of all incremental tasks is $\mathcal{A}_{\text{avg}} = \frac{1}{T}\sum_{i=1}^T \mathcal{A}_i$. In line with other studies, our evaluation results are based on three trials with three different seeds. We report both the mean and standard deviation of the trials.

\subsubsection{Implementation Details.}
In our experiment, we adopt ViT-B/16 \cite{dosovitskiy2021an} pre-trained on ImageNet21K \cite{russakovsky2015imagenet} as the backbone. We use the SGD optimizer with the initial learning rate set as 0.01 and we use the Cosine Annealing scheduler. We train the first session for 20 epochs and 10 epochs for later sessions. The batch size is set to 48 for all the experiments. LoRA module is inserted to the key and value of all the attention layers in the transformer. The distillation loss weight \( \lambda \) is set to 0.4, the LoRA rank \( r \) is set to 32, and the scale \( s \) in the angular penalty loss is set to 20. These values are determined through sensitivity analysis.


\subsection{Comparison with State-of-the-arts}
\input{sections/tab_seq_results}
We conduct a comparative evaluation of our proposed method against state-of-the-art (SOTA) class-incremental learning (CIL) approaches based on pre-trained models, with a particular focus on techniques utilizing parameter-efficient fine-tuning (PEFT). To ensure a fair comparison, we evaluate all methods using the same ViT-B/16-IN21K pre-trained models, identical random seeds, and consistent class orders. Specifically, we compare prompt-based approaches, including L2P \cite{wang2022learning}, DualPrompt \cite{wang2022dualprompt}, CODAPrompt \cite{smith2023coda}, VQ-Prompt \cite{jiao2024vector}, OS-Prompt \cite{kim2024one}, and CPrompt \cite{gao2024consistent}; adapter-based methods such as SSIAT \cite{tan2024semantically}, EASE \cite{zhou2024expandable}, and MOS \cite{sun2024mos}; the LoRA-based method InfLoRA \cite{liang2024inflora} integrated with Classifier Alignment, referred to as InfLoRA+CA; the first-session adaptation method RanPAC \cite{mcdonnell2023ranpac}, which only trains PEFT modules using data from the first task; and the fine-tuning method SLCA \cite{zhang2023slca}, which also incorporates the Classifier Alignment step as in our approach. 

Table \ref{tab:results} summarizes the performance of these SOTA methods across four widely used benchmark datasets. We report both the accuracy on the last task ($\mathcal{A}_{\text{last}}$) and the average accuracy across all tasks ($\mathcal{A}_{\text{avg}}$), presenting the mean and standard deviation over three independent runs with different random seeds. The use of random seeds introduces variability in class order across runs, making the evaluation of model performance more challenging. Notably, our method achieves superior performance in both $\mathcal{A}_{\text{last}}$ and $\mathcal{A}_{\text{avg}}$. Our method demonstrates impressive results, particularly on the more challenging datasets with larger domain gaps, such as ImageNet-R and ImageNet-A. On ImageNet-R, our method achieves a final accuracy of 81.88\%, surpassing the second-best method, SSIAT, by a significant margin of 2.33\%. The $\mathcal{A}_{\text{avg}}$ also surpasses the second-best SSIAT by 2.25\%. On the ImageNet-A dataset, our method achieves a final accuracy of 64.14\%, surpassing the second-best SSIAT by 1.49\%. These results highlight the effectiveness of our PEFT-based approach in significantly improving performance on datasets with large domain shifts, outperforming both first-session adaptation methods and full fine-tuning methods, as well as other PEFT-based approaches.

In contrast, on the CIFAR-100 and CUB-200 datasets, our method performs well, though with marginal benefits compared to other methods. Notably, on the CUB-200 dataset, our method achieves superior performance. It is also important to note that the first-session adaptation-based method, RanPAC, performs well on both CIFAR-100 and CUB-200, likely due to the significant relevance between the pretraining dataset (ImageNet) and these two datasets.

\input{sections/fig_per_task_results}

Additionally, we evaluate the performance of our method on both longer task sequences (20 tasks) and shorter task sequences (5 tasks) for CIFAR-100 and ImageNet-R, as reported in Tables \ref{tab:cifar_imgr_comparison}. Across these varied experimental settings, our method consistently outperforms competing approaches, demonstrating its stability and robustness in handling diverse CIL scenarios.

Figure \ref{fig:per_task_results} illustrates the incremental accuracy of each session for three ImageNet-R settings and one CIFAR-100 setting. The results show that on ImageNet-R, our method consistently achieves the best performance, with a clear distinction from other methods. On CIFAR-100, our method is relatively stable, and the final results are comparable to the first-session adaptation-based method, RanPAC, which is closely related to pretraining data characteristics.
% Our approach demonstrates superior performance on the more challenging datasets, ImageNet-R and ImageNet-A, which exhibit a larger domain gap from the pre-training data and have substantial intra-class diversity. This highlights the robustness of our method in addressing domain shifts.  
% Additionally, we evaluate the performance of our method on both longer task sequences (20 tasks) and shorter task sequences (5 tasks) for CIFAR-100 and ImageNet-R, as reported in Tables \ref{tab:cifar_imgr_comparison}. Across these varied experimental settings, our method consistently outperforms competing approaches, demonstrating its stability and robustness in handling diverse CIL scenarios.
% \input{sections/tab_imgr_seq_results}
% \input{sections/tab_cifar_seq_results}
\subsection{Ablation Study}
\subsubsection{The Impact of Each Component}
As shown in Table \ref{tab:ablation_comp}, we systematically assess the contributions of different components to the baseline method on the ImageNet-R dataset. The baseline, consisting of a task-specific LoRA structure with angular penalty loss for classification, achieves a competitive performance of 79.36\% in \( \mathcal{A}_{\text{Last}} \). In exp-II, we add the MSC module, which, in conjunction with classifier alignment, provides an improvement of over 1.4\%. The incorporation of Covariance Calibration with classifier alignment in exp-III also leads to an improvement of approximately 1.35\%. These results underscore the importance of both mean shift compensation and covariance calibration in aligning feature distributions across tasks, thereby reducing catastrophic forgetting and enhancing stability across task sequences. Combining the MSC and CC modules gives a significant boost, improving performance by approximately 2.3\% above the baseline method. Finally, the inclusion of patch distillation offers a further marginal improvement, resulting in a state-of-the-art performance of 81.88\% for \( \mathcal{A}_{\text{Last}} \) and 85.95\% for \( \mathcal{A}_{\text{Avg}} \), confirming the effectiveness of our method.
\input{sections/tab_ablation}
\subsubsection{LoRA Structures Design}
While there has been significant exploration of task-specific and task-shared PEFT modules, particularly concerning prompts and adapters, research on LoRA-based modules is relatively limited. In this paper, we investigate the use of task-specific and task-shared LoRA modules, as well as a hybrid architecture that combines both, inspired by Hydra-LoRA \cite{tian2024hydralora}. In Table \ref{tab:lora_results}, we evaluate these designs across four datasets and report the final accuracy averages from three trials. The results indicate that the performance differences among the LoRA designs are minimal, with the task-specific design slightly outperforming the other two, except for the ImageNet-A dataset, where the task-shared LoRA module achieves a marginally higher performance.
\input{sections/tab_ablation_lora_structures}