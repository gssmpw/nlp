\section{Method}
\input{sections/fig_framework}
\subsection{Preliminaries}
\subsubsection{Class-Incremental Learning}
Consider a dataset consisting of $T$ tasks $\{\mathcal{D}^t\}_{t=1}^T$. For each task, the dataset $\mathcal{D}^t = \{(x_i^t, y_i^t)\}_{i=1}^{n^t}$ contains $n^t$ inputs $x_i^t \in \mathbb{R}^d$ and their corresponding labels $y_i^t \in C^t$. We use $X^t$ and $Y^t$ to denote the collection of input data and label of task $t$, respectively and $C^t =\{c_i\}_{i=1}^{|C^t|}$ is the label set contains 
$|C^t|$ classes. In the class-incremental setting, for any task $i \neq j$, the input data from different tasks follow different distributions, i.e., $p(X^i) \neq p(X^j)$ and labels satisfy $C^i \cap C^j = \emptyset$.
The learning objective is to find the model $f^t: \mathbb{R}^{D} \to \mathbb{R}^{\mathcal{C}^t}$, where $\mathcal{C}^t = \cup_{i=1}^{t}C^i$ 
represents the total number of classes learned. This model is trained on all training datasets to perform well on all test dataset seen up to task $t$. In our scenario, the model $f^t$ is based on a pre-trained model, consisting of $f_{\theta}^t(x) = W^\top \phi_{\theta}^t(x)$, where $\phi_{\theta}^t:\mathbb{R}^D \to \mathbb{R}^d$ is a feature extractor composed of a a frozen pre-trained model $\phi$ and learnable parameters $\theta$ in the LoRA modules, and a classification head $W=\{w^t\}^{T}_{t=1}$ for each task, where we have $w^t \in \mathbb{R}^{d \times C_t}$. For a given task $t$, the old network $f_{\theta}^{t-1}(x)$ refers to the network trained on task $t-1$, and it is frozen in task $t$.
% \subsubsection{LoRA based PEFT}
% LoRA \cite{hu2022lora} introduces low-rank matrices into the model's weight updates, which allows the model to adapt without changing the full set of parameters. Consider the pre-trained weight matrix $W_0 \in \mathbb{R}^{d \times k}$, the update of the weight matrix is decomposed into the product of the low rank matrices $B \in \mathbb{R}^{d \times r}$ and $A \in \mathbb{R}^{r \times k}$, where $r$ is a value much smaller than the input dimension $d$ and the output dimension $k$. The forward pass of the low-rank update precess can be expressed as $h = W_0x+\Delta Wx = W_0x + BAx$. Here, $h$ and $x$ denote the input and output of the LoRA module. During training, only the low-rank weight matrices $A$ and $B$ are learnable, and the pre-trained weight $W_0$ is kept frozen.
% \vspace{-3mm}
\subsection{Overview}
Figure \ref{fig:framework} illustrates the overall architecture for class-incremental learning. We employ a frozen pre-trained ViT \cite{dosovitskiy2021an} model as the backbone with learnable task-specific LoRA modules. The output class tokens are forwarded through a task-specific classifier, which generates the class scores, while the angular penalty loss \cite{peng2022few,tan2024semantically} is used to compute the classification loss $\mathcal{L}_{\text{cls}}$:
\begin{equation}
    \mathcal{L}_{\text{cls}} = -\frac{1}{n^t} \sum_{j=1}^{n^t}\log \frac{\exp({s\cos(\theta_j)})}{\sum_{i=1}^{|C^t|} \exp({s\cos(\theta_i)})}
\end{equation}
where $\cos(\theta_j)=\frac{w_jf_{\theta_j}}{ \parallel w_j \parallel \parallel f_{\theta_j} \parallel}$, $s$ represents the scaling factor, and $n^t$ is the number of training samples in task $t$. The mean and
covariance of each class are stored for each learning session.
Before the training process, covariance of each class is precomputed from the class tokens generated by the network trained on previous tasks. These covariance matrices are then used to align the distribution of the representations generated by the current network with that of the old network, based on the Mahalanobis distance. This is referred to as the covariance calibration loss $\mathcal{L}_{\text{cov}}$. Furthermore, patch tokens are leveraged to preserve knowledge from earlier tasks at the feature level through a distillation loss $\mathcal{L}_{\text{distill}}$. 

After the training process, the class means are updated through the mean shift compensation process, and the classifier heads are retrained using the calibrated class statistics. The training pipeline is illustrated in Algorithm \ref{alg:algorithm}. In summary, the overall training objective is:
\begin{equation}
    \mathcal{L} = \mathcal{L}_{\text{cls}} + \mathcal{L}_{\text{cov}} + \lambda \mathcal{L}_{\text{distill}}
\end{equation}

\subsection{Low-Rank Adaptation}
In class-incremental learning (CIL), task IDs are not provided during the inference stage. For methods based on pre-trained models, the use of task-specific PEFT modules often involves a task ID prediction step during testing \cite{wang2022dualprompt,wang2024hierarchical,sun2024mos}. Other approaches avoid task ID prediction, as low prediction accuracy can negatively impact performance. For instance, some methods use weighted sums of prompts to determine the prompts applied during inference \cite{kurniawan2024evoprompt, smith2023coda}, or aggregate all previous PEFT modules \cite{zhou2024expandable, liang2024inflora}. Alternatively, some methods rely on a shared PEFT module across tasks \cite{huang2024ovor, tan2024semantically}.

Compared to other PEFT modules applied in CIL, such as prompts and adapters, LoRA~\cite{hu2022lora}  performs inference by adding the low-rank adaptation matrices to the original weight matrices. This enables LoRA to efficiently combine the pre-trained model weights with the task-specific adaptation matrices, as well as the information across the different task-specific matrices.

In this paper, we utilize task-specific LoRA modules, where each task is assigned a unique LoRA module. Considering the pre-trained weight matrix $W_0 \in \mathbb{R}^{d \times k}$, the update of the weight matrix is decomposed into the product of the low rank matrices $B \in \mathbb{R}^{d \times r}$ and $A \in \mathbb{R}^{r \times k}$, where $r$ is a value much smaller than the input dimension $d$ and the output dimension $k$.  The aggregation of all previous LoRA modules at task $t$ can be expressed as $W = W_0 + \sum_{i=1}^t B_iA_i$. Additionally, we explore two other LoRA structures: the task-shared LoRA module, which uses a common LoRA module for all tasks ($W = W_0 + BA$), and a hybrid structure that combines both task-specific and task-shared designs, inspired by HydraLoRA \cite{tian2024hydralora}. In this hybrid structure, the shared LoRA module's matrix $A$ is used across tasks, while the independent LoRA module's matrix $B_i$ is specific to each task ($W = W_0 + \sum_{i=1}^t B_iA$). During training, only the low-rank weight matrices $A_i$ and $B_i$ are learnable, and the pre-trained weight $W_0$ is frozen.
\subsection{Semantic Drift}
As tasks increase, we no longer have access to the data from previous tasks, and therefore cannot compute the true distribution of earlier classes under the incrementally trained network. Both the mean and variance of feature distributions of the old classes change. This phenomenon is referred to as semantic drift. When semantic drift occurs, it affects the classifier's performance. Thus, it is necessary to impose constraints on the semantic drift and calibrate the means and covariances of class distributions.

\subsubsection{Mean Shift Compensation}
We define the mean class representation of class $c$ in the embedding space after learning task $t$ as:
\begin{equation}
    \mu_c^t = \frac{1}{N_c} \sum_{i=1}^{N_c} [y_i = c] \phi_{\theta}^t(x_i)
\end{equation}
where $N_c$ is the number of samples for class $c$. The shift between the class mean obtained with the current network and the class mean obtained with the previous network can be defined as:
\begin{equation}
    \Delta \mu_c^{t-1 \rightarrow t} = \mu_c^t - \mu_c^{t-1}
\end{equation}
Previous work \cite{yu2020semantic} suggests that the shift between the true class mean and the estimated class mean can be approximated by the shift in the current class embeddings between the old and current models. Specifically, the shift of the class embeddings can be defined as:
\begin{equation}
    \Delta \phi_{\theta}^{t-1 \rightarrow t}(x_i) = \phi_{\theta}^t(x_i) - \phi_{\theta}^{t-1}(x_i)
\end{equation}
where $x_i$ belongs to class $c$. We can compute $\phi_{\theta}^{t-1}(x_i)$ using the model trained on task $t-1$ before the current task training. Then, we compute the drift $\phi_{\theta}^t(x_i)$ and use it to approximate the class mean shift $\Delta \mu_c^{t-1 \rightarrow t}$:
\begin{equation}
    \hat{\Delta} \mu_c^{t-1 \rightarrow t} = \frac{\sum_{i} w_i \Delta \phi_{\theta}^{t-1 \rightarrow t}(x_i)}{\sum_{i} w_i}
\end{equation}
with
\begin{equation}
    w_i = \exp({-\frac{\|\phi_{\theta}^{t-1}(x_i) - \mu_c^{t-1}\|^2}{2\sigma^2}})
\end{equation}
where $\sigma$ is the standard deviation of the Gaussian kernel, and the weight $w_i$ indicates that embeddings closer to the class mean contribute more to the mean shift estimation of that particular class. The proposed method can be used to compensate the mean shift of all previously learned classes at each new task with $\mu_c^t \approx \hat\mu_c^t =\hat\Delta \mu_c^{t-1 \rightarrow t} + \mu_c^{t-1}$.


\subsubsection{Covariance Calibration}
In this section, we address semantic shift from the perspective of the covariance matrix by introducing a novel covariance calibration technique, which is powered by a Mahalanobis distance-based loss. The objective is to ensure that, for each class in the new dataset, the embeddings generated by both the old and current networks follow the same covariance structure. Specifically, the covariance matrices of the embeddings from the current network should be aligned with those from the old network. To achieve this, we utilize the old network, trained on the previous task, as a form of "past knowledge" and use it to calculate the covariance for each class in the current task. Since the Mahalanobis distance directly depends on the covariance matrix, optimizing the difference between embedding pairs from the old and current networks in terms of Mahalanobis distance implicitly constrains the shape of the intra-class distribution, thus alleviating covariance shift.

Mathematically, the Mahalanobis distance \cite{mahalanobis1936generalized} is defined as the degree of difference between two random variables $x$ and $y$, which follow the same distribution and share the covariance matrix $\Sigma$:
\begin{equation}
d_M(x, y, \Sigma) = \sqrt{(x - y)^T \Sigma^{-1} (x - y)} 
\end{equation}
In our setting, we calculate the Mahalanobis distance $d_M( \phi_{\theta}^t(x_i),  \phi_{\theta}^t(x_j), \Sigma^{t-1}_c)$ using  the embedding pairs calculated with the data from the current task $t$ and as computed by covariance matrix $\Sigma^{t-1}_c$ of the class $c$ from the current task with the old network $f^{t-1}$.

Before training, for each class $c$, the covariance matrix is computed as:
\begin{equation}
    \Sigma^{t-1}_c = \frac{1}{N_c} \sum_{i=1}^{N_c} (x_i - \mu^{t-1}_c)(x_i - \mu^{t-1}_c)^T
\end{equation}
where $\mu_c^{t-1}$ is the class mean of the class $c$ calculated with the old network $f^{t-1}$. The loss function for minimizing the absolute difference in Mahalanobis distances of the sample input pairs $(x_i,x_j)$ between embeddings obtained from the old and new networks:
\begin{equation}
\begin{split}
\mathcal{L}_{\text{cov}} &= \sum_{c \in C^t} \sum_{i,j} | d_M( \phi_{\theta}^t(x_i),  \phi_{\theta}^t(x_j), \Sigma^{t-1}_c) \\
&- d_M( \phi_{\theta}^{t-1}(x_i),  \phi_{\theta}^{t-1}(x_j), \Sigma^{t-1}_c) |
\end{split}
\end{equation}

\subsection{Classifier Alignment}
The model exhibits a tendency to prioritize the categories associated with the current task, resulting in a degradation of classification performance for categories from previous tasks. Upon completing the training for each task, the classifier undergoes post hoc retraining using the statistics of previously learned classes, thereby enhancing its overall performance. It is assumed that the feature representations learned by the pre-trained model for each class follow a Gaussian distribution \cite{zhang2023slca}. In this framework, the mean $\mu_c$ and covariance $\Sigma_c$ of the feature representation for each class $c$, as described in previous sections, are calculated and stored. A number of $s_c$ samples $h^c$ are then drawn from the Gaussian distribution $\mathcal{N}(\mu_c, \Sigma_c)$ for each class as input. The classification head is subsequently retrained using a cross-entropy loss function:
\begin{equation}
    \mathcal{L}_{head} = -\frac{1}{s_c|\mathcal{C}|} \sum_{i=1}^{s_c|\mathcal{C}|} \log \left( \frac{e^{w_j{h^c_i}}}{\sum_{k \in \mathcal{C}} e^{w_k(h^c_i)}} \right)
\end{equation}
where $\mathcal{C}$ denotes all classes learned until current task, $w$ is the classifier. With the alignment of semantic drift, the true class mean and covariance are calibrated, which helps mitigate the classifier’s bias, typically induced by overconfidence in new tasks, and alleviates the issue of catastrophic forgetting.

\subsection{Feature-level Self-Distillation}
To enhance the model's resistance to catastrophic forgetting, we propose a self-distillation approach that focuses on improving the utilization of patch tokens in classification tasks, as suggested in \cite{li2024dynamic}. In Vision Transformers (ViT), the information from patch tokens is often underutilized, which can limit the model's ability to generalize across tasks \cite{zhai2024fine}. To address this, we introduce a self-distillation loss based on patch tokens.

In this method, the class token output from the current network is treated as the essential feature information that needs to be learned for the current task. The feature-level self-distillation loss encourages the alignment of patch tokens from the current network output with the class token. Specifically, we compute the angular similarity, $\text{sim}_{\angle
}$, between the current task's patch tokens, denoted as \( p_j^t \), and the class token \( c^t \), with the features normalized using L2 normalization. The loss is then formulated as:
\begin{equation}
    \mathcal{L}_{\text{distill}} = \frac{1}{L} \sum_{j=1}^{L} \left( 1 - \text{sim}_{\angle
}(p_j^t, c^t) \right) \cdot \left\| p_j^t - p_j^{t-1} \right\|_2^2 
\end{equation}
where \( p_j^t \) is the \( j \)-th patch token for the current task, \( c^t \) is the class token for the current task, \( p_j^{t-1} \) is the patch token from the previous task, and \( L \) is the total number of patch tokens for the current task. We disable the gradient updates during angular similarity computation.

The low angular similarity between patch tokens and class tokens suggests that the patch tokens contribute less to the semantic representation of the current task. To encourage better feature reuse, patch tokens with low similarity are encouraged to resemble the patch tokens from the previous network, thereby improving the retention of important task-related features. This approach ensures that patch tokens are more effectively utilized, contributing to the model's robustness and its ability to mitigate forgetting when transitioning between tasks.

\input{sections/algo_1}
