\section{Related Works}
\begin{table*}[!ht]
\renewcommand{\arraystretch}{1.2}
\vspace{-0.15in}
    \centering
    \caption{\textbf{Comparison between representative teleoperation systems and~\ourshort.} \textcolor{txcolor}{Cost:} total cost of each system. \textcolor{txcolor}{Arm and Dex-Hand Tracking:} method of tracking arm and hand poses. \textcolor{txcolor}{Loco-Manip.:} whether or not have loco-manipulation capability. \textcolor{txcolor}{Whole-body:} whether or not teleoperate the whole body of humanoid robots. \textcolor{txcolor}{No Mocap:} whether or not exclude MoCap data.}
    \label{tab:teleop_comp}
    \resizebox{.96\textwidth}{!}{
    \begin{tabular}{l|cccccc}
        \toprule
        \textbf{Teleop System} & Cost (\$) & Arm Tracking  & Dex-Hand Tracking & Loco-Manip. & Whole-body & No MoCap  \\
        \midrule
        Mobile-ALOHA~\cite{fu2024mobile} & 32k & Joint-matching & \no & \yes & \no & \yes \\
        GELLO~\cite{wu2023gello} & 0.6k & Joint-matching & \no & \no & \no & \yes \\
        AirExo~\cite{fang2024airexo} & 0.6k & Joint-matching & \no & \no & \no & \yes \\
        ACE~\cite{yang2024ace} & 0.6k & Joint-matching &  Vision Retarget & \no & \no & \yes \\
        DexCap~\cite{wang2024dexcap} & 4k & Vision Retarget & Mocap + SLAM & \no & \no & \yes \\ 
        AnyTeleop~\cite{qin2023anyteleop}  & $\sim$ 0.3k & Vision Retarget & Vision Retarget & \no & \no & \yes\\ 
        OpenTelevision~\cite{cheng2024open} & 4k & VR devices & VR devices & \no & \no & \yes \\
        % OPEN TEACH~\cite{iyer2024open} & 0.5k & VR devices & VR devices &\yes &\no &\no \\
        % Exbody~\cite{cheng2024express} &  & Vision Retarget & \no & 
        HumanPlus~\cite{fu2024humanplus} & 0.05k & Vision Retarget & Vision Retarget & \yes & \yes & \no \\
        OmniH2O~\cite{he2024omnih2o} & 0--3.5k & Vision / VR & Vision / VR & \yes & \yes & \no \\
        %Exbody2~\cite{ji2024exbody2} & xxk & Vision Retarget & \no & \no & \yes & \no \\
        Mobile-TeleVision~\cite{lu2024pmp} & 3.5k & VR devices & VR devices & \yes & \yes & \no \\
        \midrule
        \ourrow \ourshort (Ours) & 0.5k & Joint-matching & Joint-matching & \yes & \yes & \yes \\
        \bottomrule
    \end{tabular}}
    \vspace{-5pt}
\end{table*}
\subsection{Dual Arm Robot Teleoperation}
Teleoperating dual-arm robots to perform complex manipulation tasks is an efficient way to collect real-world expert demonstration, which can then be used by IL to learn autonomous skills~\cite{fu2024mobile,cheng2024open,ze2024humanoid_manipulation,black2024pi_0,lin2024learning}. Some researchers utilize robotic arms identical to the teleoperated ones~\cite{zhao2023learning,fu2024mobile,wu2023gello,fang2024airexo}, making joint-matching possible, thus ensuring high accuracy and fast response speed. However, due to the high cost of robotic arms, the establishment of such a system incurs significant expenses. Additionally, teleoperating dexterous hands with these systems is not feasible. An alternative approach is to use VR devices~\cite{cheng2024open,iyer2024open} or just a camera~\cite{Sivakumar-RSS-22,qin2023anyteleop,li2024okami}. These works use vision-based techniques to capture the operator's wrist postures and key points of the hands, which are used by IK to calculate the joint positions of the arms and hands. However, due to limitations in the accuracy, inference speed, and difficulty in handling occlusions of pose estimation, such approaches cannot guarantee rapid and accurate pose acquisition. Some researchers try to use MoCap methods~\cite{wang2024dexcap,caeiro2021systematic,liu2019high,liu2017glove} to acquire more accurate poses at higher frequencies, but MoCap equipment is very expensive. Moreover, since IK is an iterative method that approximates solutions, even when wrist and hand poses are captured accurately, the limitations of IK may prevent the robot from achieving the desired posture.  Another possible solution is an exoskeleton-based teleoperation system, which does not require an additional identical robot, thus the overall cost is relatively low. Some research calculates the end-effector pose of the exoskeleton using Forward Kinematics (FK) and then apply IK to determine the robot's joint positions, while using computer vision techniques to capture the hand poses~\cite{yang2024ace}. However, these systems are also limited by the inaccuracies of IK and pose estimation. Some studies utilize isomorphic exoskeletons~\cite{wu2023gello,fang2024airexo}, which can also employ joint-matching to teleoperate the robots, ensuring both low cost and high accuracy and control frequency. Neverthless, these systems typically handle robotic arms equipped with grippers, limiting their application to basic manipulation tasks rather than dexterous ones. Since some projects have introduced cheap and reliable motion-sensing gloves~\cite{Nepyope2023Project-Homunculus,dafarra2024icub3}, redesigning and combining them with an exoskeleton could potentially overcome this limitation, a solution that has not yet been realized in this field. \ourshort is designed to combine all the advantages mentioned above, integrating isomorphic exoskeleton arms with a pair of novel motion-sensing gloves. We will introduce this system in \cref{sec:hard_design}. A comparison between \ourshort and previous representative teleoperation systems can be found in \cref{tab:teleop_comp}.


\subsection{Humanoid Whole-body Loco-Manipualtion}

To enable robots to perform whole-body loco-manipulation tasks, some researchers focus on model-based optimization algorithms~\cite{miura1984dynamic,wensing2023optimization,moro2019whole,zhang2024whole,chignoli2021humanoid}, particularly generating locomotion control laws by solving optimal control problems (OCPs). Despite significant efforts to make OCPs computationally tractable, these algorithms still struggle with complex scenarios due to their high computational demands during online processing. Reinforcement Learning (RL)-based algorithms, especially those based on Proximal Policy Optimization (PPO)~\cite{schulman2017proximal}, offer a more powerful alternative. Using these methods, several studies successfully achieve whole-body loco-manipulation in quadruped robots~\cite{liu2024visual,pan2024roboduet,portela2024whole,ha2024umi}, and some teach humanoid robots to traverse various terrains~\cite{gu2024advancing,long2024learninghumanoidlocomotionperceptive,chen2024learning,dugar2024learning,li2024reinforcement,li2021reinforcement,liao2024berkeley,zhang2024wococo} or perform parkour~\cite{zhuang2024humanoid}. These achievements motivate researchers to apply the same techniques to humanoid whole-body loco-manipulation~\cite{gu2025humanoid}. Some studies train whole-body policies for humanoid robots~\cite{ji2024exbody2}, enabling them to act in a manner similar to human operators or even dance with people. Some other research separates the upper and lower body~\cite{cheng2024express,fu2024humanplus,he2024learning,he2024omnih2o,lu2024pmp}, using policies trained by RL to control the lower body while directly setting the joint positions of the upper body, thus helping robots achieve better balance. 
Despite achieving impressive results, these methods still face several common limitations. First, they often rely on retargeted MoCCap data~\cite{AMASS} to get motion prior~\cite{luo2023universal} for training robots. However, obtaining MoCap data is costly, and adapting robots to new poses necessitates additional data collection, which significantly hinders the scalability of these approaches. Second, many of these methods employ vision-based algorithms to estimate the operator's poses, which lack the precision of exoskeleton-based devices. This limitation reduces the accuracy required for humanoid robots to perform loco-manipulation tasks effectively. Third, these methods generally fail to incorporate the ability to control a robot's body height. Height control is crucial for handling objects at varying elevations, and its absence severely restricts the robot's operational workspace. Finally, when issuing locomotion commands, some studies rely on body movement data directly~\cite{cheng2024express,fu2024humanplus,he2024learning,he2024omnih2o}, while others use joysticks or pedals~\cite{lu2024pmp}. The former approach becomes impractical when operators need to control robots in large-scale environments, whereas the latter offers a more effective solution. However, controlling with joysticks necessitates the use of hands, which may already be occupied by other manual tasks, thereby highlighting the advantage of using pedals for locomotion commands.
% All the aforementioned works have some common problems such as relying on retargeted Motion Capture (MoCap) data to train robots, using vision-based algorithms to obtain the operators' poses, and having no ability to control the body height of robots. Acquiring MoCap data is expensive, and training robots with new poses requires collecting additional data, which hampers the scalability of this method. Vision-based algorithms do not achieve the high accuracy of exoskeleton-based devices, limiting the ability to drive humanoid robots to perform loco-manipulation tasks precisely. Moreover, controlling robots' height is essential for manipulating things at different heights, and the lack of this capability significantly limits the robots' workspace.. When issuing locomotion commands to robots, some studies use body movement information directly~\cite{cheng2024express,fu2024humanplus,he2024learning,he2024omnih2o}, while others employ joysticks or pedals~\cite{lu2024pmp}. The former approach is difficult to apply when operators need to control robots in large-scale environments, whereas the latter is more effective.