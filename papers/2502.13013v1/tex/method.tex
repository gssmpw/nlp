\section{Method}
\label{sec:method}
\begin{figure*}[!ht]
  \centering
  \includegraphics[width=1.00\textwidth]{./images/overview.pdf}
  \caption{\textbf{System Overview.} \textcolor{mycolor}{(a):} how an operator uses the exoskeleton-based hardware system to control humanoid robots in the real world and simulation. \textcolor{mycolor}{(b):} how $\pi_{loco}$ controls the robots, the data collection process for training $\pi_{auto}$, and how $\pi_{auto}$ takes over the operator to control the robots. Communication between the cockpit and the robot is achieved via Wi-Fi.}
  \label{fig:overview}
\end{figure*}

\subsection{System Overview}
As shown in \cref{fig:overview}, \ourshort consists of a low-level policy $\pi_{loco}$ and an exoskeleton-based hardware system. At any given time $t$, the operator inside the cockpit observes the first point of view (FPV) of the robot through the display. By stepping on 
the pedal, the operator provides the required locomotion commands \textbf{$C_t=[v_{x,t}, \omega_{yaw,t}, h_t]$} where $v_{x,t}$ is the desired forward or backward speed, $\omega_{yaw,t}$ is the turning speed, and $h_t$ is the target height of the robot's torso. The policy $\pi_{loco}$ controls the robot's lower-body based on $C_t$. Meanwhile, the operator controls the exoskeleton to provide the required joint angles $q_{upper}$ for the robot's upper-body, which are directly set to the robot. The upper and lower bodies work in coordination, continuously cycling through the process, ultimately enabling teleoperating robots to complete loco-manipulation tasks either in the real world or in simulation. Communications between the cockpit and the robot are achieved via Wi-Fi, allowing operation even when the robot is far from the hardware system. We can collect demonstrations while teleoperating the robot and use them to train an autonomous policy $\pi_{auto}$. Once trained successfully, $\pi_{auto}$ can take over the operator to give $C_t$ and $q_{upper}$, thus driving the robot to perform tasks autonomously.


\subsection{Locomotion Policy}
\label{sec:method_rl}
\begin{figure}[!ht]
  \centering
  \includegraphics[width=0.49\textwidth]{./images/rl.pdf}
  \caption{RL training framework of~\ourshort.}
  \label{fig:rl}
\end{figure}
To enable humanoid robots to perform loco-manipulation tasks, we design an RL-based training framework, which trains different robots to accomplish squatting and walking under continuously changing upper-body poses. We take Unitree G1 as an example and show the process of the framework in \cref{fig:rl}. The policy $\pi_{loco}$ trained by this process is capable of zero-shot sim-to-real transfer. We introduce the training settings and three key techniques of our framework in this section.

\subsubsection{Training Settings}
The observations of one step are defined as \textbf{$O_t=[C_t,\,\omega_t,\,g_t,\,q_t,\,\dot{q}_t,\,a_{t-1}]$}, where $C_t$ is the command, $\omega_t$ is the body's angular velocity, $g_t$ is the projection of $\vec{g}=[0,0,-1]$ in the robot's torso coordinate frame, $q_t$ is the joint angles of all joints of the robot, $\dot{q}_t$ is the joint velocities of all joints of robot, $a_{t-1}$ is the last time action. Then we can get the whole observations of $\pi_{loco}$ by concatenating $O_{t-5:t}$. The actions $a_t$ of the policy correspond one-to-one with the joints of the robot's lower body. After the neural network computes $a_t$ based on $O_{t-5:t}$, we use 
\begin{equation}
\tau_{t,i} = Kp_i \times (a_{t,i}-q_{0,t,i}) - Kd_i \times \dot{q_{t,i}} 
\end{equation}
to calculate the torques for joint motors, thereby driving the motors to work and enabling the robot's movement. In the equation, $i$ denotes the index of joints, $\{Kp_i\}$ and $\{Kd_i\}$ are stiffness and damping of each joint, $\{q_{0,t,i}\}$ are default joint positions of each joint. Our framework is implemented based on the code of ~\cite{long2024hybrid,rudin2022learning}, and more training details can be found in Appendix \ref{appendix:RL}.
\subsubsection{Upper-body Pose Curriculum}
% \begin{wrapfigure}{l}{0.3\textwidth}
%   \centering
%   \includegraphics[width=0.31\textwidth]{./images/prob.png}
%   \caption{Probability Distribution of $P(x|r_a), x\in(0,1), r_a\in[0, 1)$ and $p(x)=1, x\in[0,1]$}
%   \label{fig:prob}
% \end{wrapfigure}
We use a curriculum learning technique to ensure that $\pi_{loco}$ can still complete locomotion tasks under any continuously varying poses of the robot's upper-body. We adjust the sampling range of the upper body joint angles using the upper action ratio $r_a$. At the start of training, $r_a$ is set to 0. Each time the policy drives the robot to track the linear velocity with a reward function that reaches the threshold, $r_a$ increases by 0.05, eventually reaching 1. We first sample $r_a'$ from the probability distribution
\begin{equation}
    p(x|r_a) = \frac{20(1 - r_a) \, e^{-20(1 - r_a)x}}{1 - e^{-20(1 - r_a)}},\,\,\,r_a\in [0, 1)
\end{equation}
and then resample $a_i$ by $\mathcal{U}(0, r_a')$. We actually sample $a_i$ by
\begin{equation}
\label{equa:sample}
    a_i = \mathcal{U}(0, -\frac{1}{20 (1 - r_a)} \ln\left(1 - \mathcal{U}(0, 1)\left(1 - e^{-20 (1 - r_a)}\right)\right))
\end{equation}
As $r_a$ increases, the probability distribution gradually transitions from being close to 0 to $\mathcal{U}(0,1)$. This ensures that during the curriculum process, the probability distribution consistently satisfies $p(x|r_a)>0,\forall x\in[0,1],r_a\in[0,1)$. Compared to directly using $\mathcal{U}(0,r_a)$, this method approaches the final target in a more gradual and smoother manner. For better understanding of \cref{equa:sample}, we visualize it in Appendix \ref{appendix:RL}.
To simulate the continuous changes in upper body movements when controlled by our cockpit, we resample target upper-body poses every 1 second according to the above process. We then use uniform interpolation to ensure that the target movement gradually changes from the current value to the desired value over the 1-second interval. Without this approach, we find that the robot struggles to maintain balance under continuous motions.

\begin{figure}[!ht]
  \centering
  \includegraphics[width=0.48\textwidth]{./images/gym.pdf}
  \caption{Different robots are trained to walk and squat with continuous changing upper-body poses in Isaac Gym.}
  \label{fig:gym}
\end{figure}

\subsubsection{Height Tracking Reward}
\label{sec:height}
Tracking heights can significantly expands the feasible operational workspace of humanoid robots, thus helping the robots perform more loco-manipulation tasks. Therefore, $\pi_{loco}$ needs to enable the robot to squat to the target height $h_t$. To achieve this, we design a new reward function
\begin{equation}
\label{eq:rknee}
    r_{knee} = -\|(h_{r,t}-h_{t})\times(\frac{q_{knee,t}-q_{knee,min}}{q_{knee,max}-q_{knee,min}}-\frac{1}{2})\|,
\end{equation}
where $h_{r,t}$ is the robot's actual height, $q_{knee,min}$ and $q_{knee,max}$ are the maximum and minimum actions of knee joints, $q_{knee,t}$ is the current positions of robot's knee joints. $r_{knee}$ encourages flexion of the knee joints when $h_{r,t}< h_t$, and encourages extension when $h_{r,t}> h_t$. In the training process, we resample all commands every 4 seconds. At this point, one-third of the environments are randomly selected to train the robot to squat, while the remaining two-thirds focus on teaching the robot to stand and walk. This strategy helps balance the learning of squatting and walking. Additionally, the same environment switches between learning to squat and learning to walk, enabling the policy to smoothly transition between squatting and walking tasks. For better understanding of \cref{eq:rknee}, we visualize it in Appendix \ref{appendix:RL}.

\subsubsection{Symmetry Utilization}
We introduce the same trick as \cite{su2024leveraging} to our training framework. Each time we obtain a transition $T_t=(s_t, a_t, r_t, s_{t+1})$ from the simulation, we perform a flip operation on it. Specifically, we apply symmetry to the actor and critic observations with respect to the robot's x-z plane. This involves flipping elements such as the positions, velocities, and actions of the robot's left and right joints, as well as the desired turning velocity, across the x-z plane to obtain a mirrored transition $T_t'$. Both $T_t$ and $T_t'$ are then added to the rollout storage. This process helps to improve data efficiency and ensure symmetry in the sampled data, reducing the likelihood of the trained policy being asymmetrical in terms of left and right performance. In the learning phase, we also apply this procedure to the samples $T_t$ got from the rollout storage to get $T_t'$. Both $T_t$ and $T_t'$ are passed through the actor and critic networks to obtain $a_t$, $a_t'$, $V_t$, $V_t'$ respectively, which are used to calculate additional losses:
\begin{equation}
    \mathcal{L}^{actor}_{sym}=MSE(a_t,a_t'),
\end{equation}
\begin{equation}
    \mathcal{L}^{critic}_{sym}=MSE(V_t,V_t').
\end{equation}
These two losses are added to the network optimization process, thereby enforcing symmetry of the neural network.

\subsection{Hardware System Design}
\label{sec:hard_design}
To enable a single operator to control the full body of humanoid robots, we design a low-cost  exoskeleton-based hardware system as shown in the left part of \cref{fig:overview}. For the upper-body teleoperation of the humanoid robots, we  design 3D-printed 7-DoF isomorphic exoskeleton arms for precise mapping of the upper limb joint angles, specifically tailored for two types of humanoid robots: Unitree G1 and Fourier GR-1. Additionally, we design a pair of low-cost motion-sensing gloves capable of mapping up to  to 15 DoF of finger angles. For locomotion command acquisition, we design a foot pedal that simulates the press-and-release actions of the foot during driving, enabling control of the humanoid robot's movements such as walking and squatting. The operator can easily deploy this system and perform single-person teleoperation of the robot's loco-manipulation, similar to driving a car in a cockpit or playing a racing game.


\subsubsection{\textbf{Isomorphic Exoskeleton}}
To achieve accurate control and mapping of the upper limb joints of the humanoid robots, we employ an isomorphic exoskeleton as the teleoperation solution for controlling the robot's upper body. Based on the morphology of the Unitree G1 and Fourier GR-1, our isomorphic exoskeleton design consists of a symmetric pair of arms, each with 7 DoF, corresponding to the 7 DoF of each arm of the robot (3 DoF for the shoulder, 1 DoF for the elbow, and 3 DoF for the wrist). Each joint of the exoskeleton is equipped with a DYNAMIXEL XL330-M288-T servo, which provides joint angle readings and adjustments with an accuracy of 0.09Â°, enabling precise joint angle mapping and initial calibration. The exoskeleton's operational part is designed to match the length of the human arms. Considering the challenge of fully replicating the robot's upper arm structure, we align the servos with the robot's motor URDF joint coordinate system. By proportionally mapping the relative positions of the coordinate axes to the relative positions of the servos, we ensure that both the operator's experience and the  system's isomorphic nature are preserved. Our isomorphic exoskeleton is fixed to the operator's back using adjustable straps. This setup minimizes interfere with the operator's basic movements while covering most of the upper-body motion range of the humanoid robot.

\subsubsection{\textbf{Motion-sensing Gloves}}
For fine teleoperation of the fingers, we adopt a joint-matching approach. Based on the Nepyone glove project~\cite{Nepyope2023Project-Homunculus}, we design a low-cost motion-sensing glove that connects directly to the exoskeleton for assembly and use, providing up to 15 DoF for finger capture to control dexterous hands. Specifically, each finger is equipped with three sets of sensors, which map the pitch motion of the finger tip and finger pad, as well as the yaw motion of the finger pad. This setup is sufficient to enable the mapping of different dexterous hands for humanoid robots. We place Hall effect sensors and small neodymium magnets at each joint. When the joint rotates, the neodymium magnet rotates as well, thereby affecting the magnetic field sensed by the sensor and achieving the mapping of finger joint angles. Additionally, we design the glove's microcontroller, sensor modules, and structural model. The microcontroller is mounted on the back of the hand and can be directly connected to the packaged sensors using terminal connectors, allowing for easy plugging and unplugging to reassign and modify the mapping relationships. Our motion-sensing gloves can be easily attached to and detached from different exoskeletons, offering high versatility.

\subsubsection{\textbf{Foot Pedal}}
In our cockpit, the foot pedal is used as a replacement for a remote controller, enabling command control of the humanoid robot's lower body by giving commands $C_t$ to $\pi_{loco}$. The operator controls the acceleration and deceleration of the robot's lower body movement by pressing and releasing the foot pedal.
We use high-precision rotary potentiometers to map pedal pressure changes to electrical signals. In our system, we control the humanoid robot's locomotion through linear velocity, yaw velocity, and height adjustment. These commands allow  the robot to fully demonstrate its locomotion capabilities. To achieve this, we use three small pedals to control these commands. Additionally, a pair of mode-switching buttons (foot-operated with momentary switches) are used to toggle between forward/backward and left/right turning directions, as shown in \cref{fig:pedal}. Users can modify the pedal configuration and reassign commands to adapt to diverse movement combinations.
\begin{figure}[!ht]
  \centering
  \includegraphics[width=0.42\textwidth]{./images/pedal.pdf}
  \caption{\textbf{Pedal command control.} The three small pedals respectively control $ [0, \pm{\omega_{max}}] $, $ [{H_{min}}, {H_{max}}] $, and $ [0, \pm{V_{max}}] $. The left-side switching button is used to toggle between left and right modes, while the right-side switching button is used to toggle between forward and backward modes.}
  \vspace{-5pt}
  \label{fig:pedal}
\end{figure}


 