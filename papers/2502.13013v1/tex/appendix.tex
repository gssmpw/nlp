\clearpage
\newpage
\onecolumn
\begin{appendices}

\section{RL Training and Evaluation Details.}
\label{appendix:RL}
\subsection{Network Architecture}
\label{appendix:archi}
We use a network architecture similar to that employed in a previous quadruped locomotion work called HIM \cite{long2024hybrid}. The architecture consists of three main components: an estimator network $\mathcal{E}$, a follow-up network $\mathcal{N}$ that takes the output of $\mathcal{E}$ as part of its input, and a critic network $\mathcal{C}$. Together, $\mathcal{E}$ and $\mathcal{N}$ form the actor module, as illustrated in \cref{fig:rl}. All networks are implemented as 3-layer multilayer perceptrons (MLPs). Below, we describe their specific architectures, where $N_{joints} = N_{lower} + N_{upper}$, with $N_{lower}$ representing the number of lower-body joints and $N_{upper}$ the number of upper-body joints of the robot. Given that the dimensions of the input vectors are as follows: $Dim([C_t, \omega_t, g_t]) = 4 + 3 + 3 = 10$ and $Dim([v_x, v_y, \omega_{yaw}]) = 3$, we can determine the input dimensions for each network. The workflow is as follows: The sequence $O_{t-5:t}$ is fed into the encoder of $\mathcal{E}$. The encoder processes $O_t$ along with the ground truth values $([v_x, v_y, \omega_{yaw}])$ and outputs the encoded information $\hat{I}_t$. $\hat{I}_t$ is then passed to $\mathcal{N}$ and is also used for contrastive learning with the output $\hat{I}'_t$ of the target network. Combining $\hat{I}_t$ and $O_t$, $\mathcal{N}$ produces the action $a_t$, whose dimension equals $N_{lower}$. The critic network $\mathcal{C}$ takes $O_t$ and the ground truth $([v_x, v_y, \omega_{yaw}])$ as its input.
\lstset{
    backgroundcolor=\color{gray!10},
    basicstyle=\ttfamily\small,
    breaklines=true,
    frame=single,
    keywordstyle=\color{blue},
    commentstyle=\color{green!50!black},
    stringstyle=\color{red},
    showstringspaces=false,
    mathescape=true
}
\begin{lstlisting}[language=Python, caption={Architecture of Neural Networks Used by Our RL Training Framework.}]
 ($\mathcal{E}$): HIMEstimator(
    (encoder): Sequential(
      (0): Linear(in_features=$6\times(10+2\times N_{joints}+N_{lower})$, out_features=256, bias=True)
      (1): ELU(alpha=1.0)
      (2): Linear(in_features=256, out_features=256, bias=True)
      (3): ELU(alpha=1.0)
      (4): Linear(in_features=256, out_features=35, bias=True)
    )
    (target): Sequential(
      (0): Linear(in_features=$10+2\times N_{joints}+N_{lower}$, out_features=256, bias=True)
      (1): ELU(alpha=1.0)
      (2): Linear(in_features=256, out_features=256, bias=True)
      (3): ELU(alpha=1.0)
      (4): Linear(in_features=256, out_features=32, bias=True)
    )
    (proto): Embedding(64, 32)
  )
  ($\mathcal{N}$): Sequential(
    (0): Linear(in_features=$35+10+2\times N_{joints}+N_{lower}$, out_features=512, bias=True)
    (1): ELU(alpha=1.0)
    (2): Linear(in_features=512, out_features=256, bias=True)
    (3): ELU(alpha=1.0)
    (4): Linear(in_features=256, out_features=256, bias=True)
    (5): ELU(alpha=1.0)
    (6): Linear(in_features=256, out_features=$N_{lower}$, bias=True)
  )
  ($\mathcal{C}$): Sequential(
    (0): Linear(in_features=$3+10+2\times N_{joints}+N_{lower}$, out_features=512, bias=True)
    (1): ELU(alpha=1.0)
    (2): Linear(in_features=512, out_features=256, bias=True)
    (3): ELU(alpha=1.0)
    (4): Linear(in_features=256, out_features=256, bias=True)
    (5): ELU(alpha=1.0)
    (6): Linear(in_features=256, out_features=1, bias=True)
  )
\end{lstlisting}

\subsection{Reward Scales}
\label{appendix:rwdscale}
We present the reward functions and their corresponding scales used in training the Unitree G1 and Fourier GR-1 robots in \cref{tab:reward}. Our reward functions adapt from PIM~\cite{long2024learninghumanoidlocomotionperceptive}, a previous work on humanoid locomotion, with several modifications and new additions. In addition to the "Squat knee" term $r_{knee}$ introduced in \cref{sec:height}, we revise the "Base height tracking" reward to align it with other tracking terms, as our objective is to track the changing target base height. Furthermore, we decompose the linear velocity tracking reward into separate components for x and y velocity tracking to better distinguish the robot's performance in these directions. To encourage the robot to remain stationary when zero velocity is required, we introduce a "stand still" reward. However, we observe that this reward can cause the robot to become overly inclined to maintain stillness, leading to instability during transitions between stationary and moving states. Experimental results indicate that reducing the $K_p$ of the ankle joint alleviates this issue. The rationale is that a lower $K_p$ tends to produce smaller torque outputs, making the ankle joint more responsive to positional changes when the center of gravity shifts. This positional change provides proprioceptive feedback to the policy, prompting necessary adjustments. The reward scales used in the training processes of these two heterogeneous robots are largely similar, further demonstrating the generality of our framework.

\begin{table}[!ht]
    \centering
    \caption{Reward Functions and Weights Used to Train Low-manipulation Policy}
    \renewcommand{\arraystretch}{1.5}
    \begin{tabular}{llcc} 
    \toprule[1.5pt] Reward & Equation & Weight of Unitree G1 & Weight of Fourier GR-1 \\ 
    \midrule[1.5pt] 
    x Vel. tracking & $\exp \left\{-4 \|\mathbf{v}_{x}-\mathbf{v}_{r,x}\|_2^2  \right\}$ & 1.5 & 1.5 \\
    
    y Vel. tracking & $\exp \left\{-4 \|\mathbf{v}_{y}-\mathbf{v}_{r,y}\|_2^2  \right\}$ & 1.0 & 1.0 \\
    
    Ang. Vel. tracking & $\exp \left\{-4 \left\|\omega_{\text {yaw }}-\omega_{\text{r,yaw }}\right\|^2\right\}$ & 2.0 & 1.0 \\
    Base height tracking & $\exp \left\{-4 \left\|h_{\text {t}}-h_{\text {r,t}}\right\|^2\right\}$ &2.0&2.0\\
    Lin. Vel. z & $v_{r,z}^2$ & -0.5 & -0.5 \\
    Ang. Vel. xy & $\|\boldsymbol{\omega}_{r,x y}\|_2^2$ & -0.025 & -0.025 \\
    Orientation & $\|\mathbf{g}_{x}\|_2^2 + \|\mathbf{g}_{y}\|_2^2$ & -1.5 & -1.5 \\
    Action rate & $\|\mathbf{a}_t-\mathbf{a}_{t-1}\|_2^2$ & -0.01 & -0.01 \\
    Hip joint deviation & $\sum\limits_{\text{hip joints}}|\theta_{i} - \theta^{default}_{i}|^{2}$ & -0.2 & -0.5 \\
    Ankle joint deviation & $\sum\limits_{\text{ankle joints}}|\theta_{i} - \theta^{default}_{i}|^{2}$ & -0.5 & -0.75 \\
    Squat knee & $-\|(h_{r,t}-h_{t})\times(\frac{q_{knee,t}-q_{knee,min}}{q_{knee,max}-q_{knee,min}}-\frac{1}{2})\|$ & -0.75 & -0.75 \\
    Dof Acc. & $\sum\limits_{\text{all joints}} \frac{\|\dot q_{t,i}-\dot q_{t-1,i}\|^2}{dt}$ & $-2.5 \times 10^{-7}$ &  $-2.5 \times 10^{-7}$\\
    Dof pos limits & $\sum\limits_{\text{all joints}} \text{out}_{i}$ & -2.0 & -2.0 \\
    Feet air time & $\mathbf{1}_{\{\text{first contact}\}}(T_{air}-0.5)$ & 0.05 & 0.05\\
    Feet clearance & $\sum\limits_{feet} \left(p_{z}^{\text {target}}-p_{z}^{i}\right)^2 \cdot v_{xy}^{i}$ & -0.25 & -0.25 \\
    Feet lateral distance & $|y_{\text{left foot}}^{B} - y_{\text{right foot}}^{B}| - d_{min}$ & 0.5 & 0.5 \\
    Knee lateral distance & $|y_{\text{left knee}}^{B} - y_{\text{right knee}}^{B}| - d_{min}$ & 1.0 & 1.0 \\
    Feet ground parallel & $\sum\limits_{feet}Var(H_i)$ & -2.0 & -2.0 \\
    Feet parallel & $Var(D)$ & -3.0 & -3.0 \\
    Smoothness & $\|\mathbf{a}_t-2 \mathbf{a}_{t-1}+\mathbf{a}_{t-2}\|_2^2$ & -0.05 & -0.05 \\
    Joint power & $\frac{|\boldsymbol{\tau} \| \dot{\boldsymbol{\theta}}|^{T}}{\|\mathbf{v}\|_2^2 + 0.2 * \|\boldsymbol{\omega}\|_2^2}$ & $-2.0 \times 10^{-5}$ & $-2.0 \times 10^{-5}$ \\
    Feet stumble & $\mathbf{1}\left\{\exists i,\left|\mathbf{F}_i^{x y}\right|>3\left|F_i^z\right|\right\}$ & -1.5 & -1.5 \\
    Torques & $\sum\limits_{\text{all joints}} |\frac{\tau_i}{kp_{i}}|_{2}^{2}$ & $-2.5 \times 10^{-6}$ & $-2.5 \times 10^{-6}$ \\
    Dof Vel. & $\sum\limits_{\text{all joints}} \dot{\theta}_{i}|_{2}^{2}$ & $-1 \times 10^{-4}$ & $-1 \times 10^{-4}$ \\
    Dof Vel. limit & $\sum\limits_{\text{all joints}} RELU(\hat{\theta}_{i} - \hat{\theta}^{max}_{i})$ & $-2\times 10^{-3}$ & $-2\times 10^{-3}$ \\
    Torque limits & $\sum\limits_{\text{all joints}} RELU(\hat{\tau}_{i} - \hat{\tau}^{max}_{i})$ & -0.1 & -0.2 \\
    No fly & $\mathbf{1}\{\text{only one feet on ground}\}$ & 0.75 & 0.5 \\
    Joint tracking error & $\sum\limits_{\text{all joints}}|\theta_{i} - \theta^{target}_{i}|^{2}$ & -0.1 & -0.25 \\
    Feet slip & $\sum\limits_{feet}\left|\mathbf{v}_i^{\text {toot }}\right| * \sim \mathbf{1}_{\text {new contact }}$ & -0.25 & -0.25 \\
    Feet contact force & $\sum\limits_{feet} RELU(F_{i}^{z} - F_{th})$ & $-2.5 \times 10^{-4}$ & $-2.5 \times 10^{-4}$ \\
    Contact momentum & $\sum\limits_{feet}|v_{i}^{z} * F_{i}^{z}|$ & $2.5 \times 10^{-4}$ & $2.5 \times 10^{-4}$ \\
    Action vanish & $\sum\limits_{\text{all joints}}(\max\{0, a_{i,t}-a_{i,max}\}+\min\{0, a_{i,min}-a_{i,t}\})$&-1.0 & -1.0\\
    Stand still & $Num_{\{feet\,not\,on\,ground\}}\times \mathbf{1}_{stand\,still}$ &-0.15 & -0.2\\
    \bottomrule[1.5pt]
    \end{tabular}
    \label{tab:reward}
\end{table}
\subsection{Domain Randomization}
\label{appendix:Random}
To improve the robustness of the trained policy, we employ domain randomization to simulate several kinds of random noises that may occur while deploying in the real world. The terms used for randomization, along with their descriptions and ranges, are listed in \cref{tab:domain}. Specifically, we introduce a term to randomize the mass of the hands, enhancing the robot's capability to hold objects effectively. The ranges for the Unitree G1 and Fourier GR-1 are the same.
\begin{table}[!ht]
    \centering
    \caption{Randomization Terms, Description, and Ranges}
    \begin{tabular}{llc}
    \toprule[1.5pt] Term & Description & Ranges \\
    \midrule[1.5pt] 
    Actuation offset $(N\cdot m)$ & Random torque offsets applied to the computed motor torques & $[-0.05, 0.05]$ \\
    Torso payload mass $(Kg)$ & Additional random mass attached to the torso and hand links & $[-5.00, 10.00]$ \\
    Hand payload mass $(Kg)$ & Additional random mass attached to the hand links & $[-0.10, 0.30]$ \\
    CoM displacement $(m)$ & Random offsets applied to the center of mass (CoM) position of the torso link & $[-0.1, 0.1]$ \\
    Link mass $(-)$ & Random scaling factors applied to the masses of the robot's links & $[0.80, 1.20]$ \\
    Friction coefficient $(-)$ & Random friction coefficients applied to the robot's links & $[0.10, 2.00]$ \\
    Restitution $(-)$ & Random restitution coefficients applied to the robot's links & $[0.00, 1.00]$ \\
    $K_p$ $(N/rad)$ & Random scaling factors applied to the proportional gain ($K_p$) of the robot's joints & $[0.90, 1.10]$ \\
    $K_d$ $(N/(m/s))$ & Random scaling factors applied to the derivative gain ($K_d$) of the robot's joints & $[0.90, 1.10]$ \\
    Initial joint pos scale $(-)$ & Random scaling factors applied to the initial positions of the robot's joints & $[0.80, 1.20]$ \\
    Initial joint pos offset $(rad)$ & Random offsets added to the initial positions of the robot's joints & $[-0.10,0.10]$ \\
    Push robot $(m/s)$ & Random x and y velocities applied to the robot to simulate external pushes & $[-0.50, 0.50]$ \\
    Dof pos obs $(rad)$& Random dof velocity added to the observation of joint positions & $[-0.02, 0.02]$ \\
    Dof vel obs $(rad/s)$& Random dof velocity added to the observation of joint velocities & $[-2.00, 2.00]$  \\
    Ang vel obs $(rad/s)$& Random dof velocity added to the observation of body angular velocities & $[-0.50, 0.50]$ \\
    Gravity obs $(m/s^2)$& Random dof velocity added to the observation of gravities projected to robot's body frame & $[-0.05, 0.05]$ \\
    \bottomrule[1.5pt]
    \end{tabular}
    \label{tab:domain}
\end{table}
\subsection{Other Key Parameters}
\label{appendix:OtherKey}
We list other key parameters used to train the Unitree G1 and Fourier GR-1 in \cref{tab:para}. The same settings are applied for both training and evaluation. Additionally, we adjust the base height target value when the environment is used to train squatting; otherwise, the robot is required to track a constant height value while walking. Terms marked with $*$ indicate that exceeding the specified range will result in penalties through corresponding rewards.
\begin{table}[!ht]
    \centering
    \caption{Key Parameters Used to Train Robots}
    \begin{tabular}{lcc}
    \toprule[1.5pt] Term & Unitree G1 & Fourier GR-1 \\
    \midrule[1.5pt] 
    Height target while walking $(m)$& 0.74 & 0.90 \\
    X Lin. Vel. range $(m/x)$ & $[-0.80, 1.20]$ & $[-0.80, 1.20]$ \\
    Y Lin. Vel. range $(m/s)$& $[-0.50, 0.50]$ & $[-0.80, 0.80]$ \\
    Yaw Ang. Vel. range $(rad/s)$& $[-0.80, 0.80]$ & $[-1.00, 1.00]$ \\
    Squat height range $(m)$& $[-0.24, 0.74]$ & $[-0.30, 0.90]$ \\
    Soft dof pos limit scale * $(-)$& 0.975 & 0.975 \\
    Soft dof vel limit scale * $(-)$& 0.80 & 0.80 \\
    Soft dof torque limit scale * $(-)$& 0.95 & 0.95 \\
    Max contact force * $(N)$& 400.00 & 500.00 \\
    
    Least feet distance * $(m)$& 0.20 & 0.20 \\
    Least knee distance * $(m)$& 0.20 & 0.20 \\
    Most feet distance * $(m)$& 0.35 & 0.40 \\
    Most knee distance * $(m)$& 0.35 & 0.40 \\
    Clearance height target * $(m)$& 0.14 & 0.15 \\
    Push interval $(s)$& 4.00& 4.00\\
    Upper-body poses resampling interval $(s)$& 1.00& 1.00\\
    Commands resampling interval $(s)$& 4.00& 4.00\\
    \bottomrule[1.5pt]
    \end{tabular}
    \label{tab:para}
\end{table}
\subsection{Function Visualization}
\label{appendix:FunVisual}
For better understanding of the equations we proposed in \cref{equa:sample} and \cref{eq:rknee}, we visualize them in \cref{fig:visual}. As shown in the left figure, $p(x|r_a)$ can take any value in the range $[0,1]$ when $r_a \in [0,1)$. When $r_a$ is small, it is more likely to take smaller values of $x$. As $r_a$ increases, the probability of taking larger values of $x$ also increases. When $r_a \rightarrow 1$, the entire distribution becomes $\mathcal{U}(0,1)$. In the right figure, we can observe that regardless of the position of $(h, q)$, $r_{knee}$ encourages $q_{knee,t}$ to change in a direction that brings $h_{r,t}$ closer to $h_{t}$. This achieves the goal of guiding the robot to track the base height by either bending or straightening its knees.
\begin{figure}[!ht]
  \centering
  \includegraphics[width=0.8\textwidth]{./images/visual.pdf}
  \caption{Visualization of proposed functions. \textcolor{mycolor}{Left:} Visualization of $p(x|r_a)$ in \cref{equa:sample}. \textcolor{mycolor}{Right:} Visualization of $r_{knee}$ in \cref{eq:rknee}.}
  \label{fig:visual}
\end{figure}
\subsection{Terrain Traverse}
\label{appendix:Terrain}
In order to expand the feasible workspace of robots, we integrate our training framework with a previous humanoid locomotion method called PIM \cite{long2024learninghumanoidlocomotionperceptive} to enable our robots to traverse stairs. As shown in the left figure of \cref{fig:terrain}, we successfully train the Unitree G1 in Isaac Gym to traverse high stairs. However, when deploying the trained policy in the real world, as shown in the right figure of \cref{fig:terrain}, the robot struggles to walk stably and collides with the stair, despite eventually stepping onto it. This instability arises because the head of the Unitree G1 cannot remain fixed, causing movement of the LiDAR mounted on it. Additionally, the elevation map acquisition method used by PIM lacks high resolution, further exacerbating the sim2real gap. In the future, we will explore methods to truly enable our robots to traverse any terrain effectively.



\begin{figure}[!ht]
  \centering
  \includegraphics[width=0.98\textwidth]{./images/terrain.pdf}
  \caption{Training robots to traverse stairs. \textcolor{mycolor}{Left:} Training in simulation; \textcolor{mycolor}{Right:} Deployment in the real world.}
  \label{fig:terrain}
\end{figure}

\section{Hardware System Details}
\label{appendix:Hardware}
In \cref{fig:overview}, we present the hardware system design framework of HOMIE, which comprises three integral components: isomorphic exoskeleton arms, a pair of motion-sensing gloves, and a pedal. The primary structural elements of these components are fabricated using 3D printing technology with PLA basic material. 
\subsection{Isomorphic Exoskeleton Details}
\label{appendix:Exoskeleton}
The Isomorphic Exoskeleton adopts a hollowed-out and mortise-and-tenon design, which not only ensure structural integrity but also significantly reduce the overall weight and assembly complexity, as well as facilitate the routing of servo motor connections. The structural components are fixed to the servos through four types of connection methods: two methods that directly connect to the servo body \cref{fig:assemble}\textcolor{mycolor}{(a)}, \textcolor{mycolor}{(b)} and two methods that connect to the servo disks \cref{fig:assemble}\textcolor{mycolor}{(c)} To further enhance stability and strength, additional servo disks are installed on the opposite side of the servos at certain joints, allowing the structural components to connect directly to the servo disks on both sides, as illustrated in \cref{fig:assemble}\textcolor{mycolor}{(c)}. We present the physical models of the Isomorphic Exoskeletons adapted for the  Unitree G1 and Fourier GR1 in \cref{fig:back}\textcolor{mycolor}{(a)}. Due to the different configurations of the Humanoid, there are significant structural differences in the wrist and shoulder components between the two sets of Isomorphic Exoskeletons, while the other components and their usage remain identical. The two sets of Exoskeletons share the same back connector, which integrates functionalities for operator attachment, docking station fixation, U2D2 placement, and bilateral arm linkage, as illustrated in \cref{fig:back}\textcolor{mycolor}{(b)}.
\begin{figure}[!ht]
  \centering
  \includegraphics[width=1.0\textwidth]{./images/assemble.pdf}
  \caption{The assembly methods of the servos and structural components, along with the screw requirements, are as follows: \textcolor{mycolor}{(a)}, \textcolor{mycolor}{(b:)} The structural components are directly assembled and fixed to the servo body; \textcolor{mycolor}{(c:)} The structural components are assembled and fixed to the servos via one or two servo disks, respectively.}
  \label{fig:assemble}
\end{figure}
\begin{figure}[!ht]
  \centering
  \includegraphics[width=0.94\textwidth]{./images/back.pdf}
  \caption{\textcolor{mycolor}{(a:)} Physical models of two different Humanoids' Isomorphic Exoskeletons, equipped with servos, back connectors, and motion-sensing gloves; \textcolor{mycolor}{(b:)} Schematic diagram of the back connector assembly and functionality, where the connectors can be fixed using a dovetail structure and plugs. The U2D2 board and docking station are external physical components.}
  \label{fig:back}
\end{figure}

\subsection{Motion-sensing Glovesc Details}
\label{appendix:Glovesc}
The motion-sensing gloves are secured to the palm via a length-adjustable elastic strap and connected to the fingertips through five smaller length-adjustable elastic straps, which facilitate finger fixation and critical angle mapping, ensuring adaptability to operators with varying hand sizes. A microcontroller is embedded within the palm section of the gloves, featuring exposed ports that allow direct connection to the 15 Hall sensor modules located at the fingers, as depicted in \cref{fig:glove}\textcolor{mycolor}{(a)}.
For the joint mapping angle range and acquisition accuracy of each finger, we have listed the data in TABLEx. Each finger of the glove has three degrees of freedom, which are shown in \cref{fig:glove}\textcolor{mycolor}{(a)}, namely the pitch motion of the fingertip ($\alpha$), the pitch motion of the finger pad ($\beta$), and the yaw motion of the finger pad ($\gamma$). Due to differences in the structural length of the thumb, the pinky, and the other three fingers, we have divided them into three parts. It should be noted that the angular movement of the finger joints does not exhibit a significant linear relationship with the changes in the Hall sensor readings caused by the induced magnetic field variation. This is related to the positioning of the magnets and Hall sensors, as well as the structural design of the gloves. In our motion-sensing gloves design, the relationship between the two follows an exponential pattern within their transformation range, especially in the pitch motion of the fingers and finger pads, from open to fist. Furthermore, our gloves have undergone control testing on the Inspire Dexterous Hands RH56DFTP actual device, illustrated in \cref{fig:glove}\textcolor{mycolor}{(b)}.
\begin{figure}[!ht]
  \centering
  \includegraphics[width=0.9\textwidth]{./images/glove.pdf}
  \caption{\textcolor{mycolor}{(a):} The schematic diagrams of the three degrees of freedom for each finger and the rotation of the magnet affecting the magnetic field direction, as well as the photograph of the actual wearable device. \textcolor{mycolor}{(b):} Physical images of the Inspire Hands actual device in both open and clenched fist states.}
  \label{fig:glove}
\end{figure}

\begin{table}[!ht]
    \centering
    \caption{15 DoF Joint Mapping Angle Range and Acquisition Accuracy. (Acc:accuracy)}
    \begin{tabular}{llccc}
    \toprule[1.5pt] Term & Description & Angle Range & Acquisition Range & Acquisition Acc.\\
    \midrule[1.5pt] 
     $\alpha_{thumb}$ & Pitch motion of the thumb tip & 65\text{\textdegree} & 528 units & 0.123\( \text{\textdegree} / \text{unit} \) \\
     $\beta_{thumb}$ & Pitch motion of the thumb pad & 100\text{\textdegree} & 1024 units & 0.098\( \text{\textdegree} / \text{unit} \) \\
     $\gamma_{thumb}$ & Yaw motion of the thumb pad & 90\text{\textdegree} & 832 units & 0.108\( \text{\textdegree} / \text{unit} \) \\
     $\alpha_{pinky}$ & Pitch motion of the pinky tip & 70\text{\textdegree} & 880 units & 0.080\( \text{\textdegree} / \text{unit} \) \\
     $\beta_{pinky}$ & Pitch motion of the pinky pad & 90\text{\textdegree} & 1136 units & 0.079\( \text{\textdegree} / \text{unit} \) \\
     $\gamma_{pinky}$ & Yaw motion of the pinky pad & 45\text{\textdegree} & 416 units & 0.108\( \text{\textdegree} / \text{unit} \) \\
     $\alpha_{other}$ & Pitch motion of the index, middle, and ring finger tips & 70\text{\textdegree} & 928 units & 0.075\( \text{\textdegree} / \text{unit} \) \\
     $\beta_{other}$ & Pitch motion of the index, middle, and ring finger pads & 90\text{\textdegree} & 1072 units & 0.088\( \text{\textdegree} / \text{unit} \) \\
     $\gamma_{other}$ & Yaw motion of the index, middle, and ring finger pads & 40\text{\textdegree} & 512 units & 0.078\( \text{\textdegree} / \text{unit} \) \\
     
    \bottomrule[1.5pt]
    \end{tabular}
    \label{tab:glove_range}
\end{table}


\subsection{Foot Pedal Details}
\label{appendix:Pedal}
The foot pedal consists of three small pedals and two mode-switching buttons, all fixed onto a large base plate, as shown in \cref{fig:pedal}. The operator can press the small pedals, which cause the structural components to rotate, thereby driving the potentiometer at the bottom to rotate. The spring within the structure ensures that when the operator releases the pedal, it springs back, returning to the initial position, as shown in \cref{fig:pedal_app}\textcolor{mycolor}{(a)}. The potentiometer we use, model 0932, has a range of angular movement of 270\text{\textdegree}, with the actual pedal movement range being 40\text{\textdegree}. As for the mode-switching buttons, the operator can press the buttons on the surface, which will cause a change in the high and low levels of the micro switch, as shown in \cref{fig:pedal_app}\textcolor{mycolor}{(b)}. The function of the tapered spring is the same as the spring in the small pedals.

\begin{figure}[!ht]
  \centering
  \includegraphics[width=0.8\textwidth]{./images/Pedal_app.pdf}
  \caption{\textcolor{mycolor}{(a):} Schematic diagram of the small pedal principle, where the operator's foot press drives the potentiometer to rotate. \textcolor{mycolor}{(b):} Schematic diagram of the mode-switching buttons, where the operator's foot press changes the state of the micro switch.}
  \label{fig:pedal_app}
\end{figure}


\section{Deployment Details}
\label{appendix:deploydetail}

\subsection{System Deployment}

We deploy our trained policy $\pi_{loco}$ directly onto the Unitree G1’s onboard computing unit—a Nvidia Jetson Orin capable of 275 TOPS—allowing $\pi_{loco}$ to run at 50\,Hz using the robot’s state information to control walking and squatting, matching the frequency used during Isaac Gym training. We use an isomorphic exoskeleton-based approach to control the robot, as shown in \cref{fig:deploy}(b). A CPU-only host computer connects via four data lines to the isomorphic arm, left and right gloves, and the pedal’s microcontroller, reading real-time data. It then transmits $q_{upper}$ and $C_t$ to the G1 over Wi-Fi via TCP, enabling the robot to set upper-body poses and compute lower-body actions $a_t$ through $\pi_{loco}$ for full-body control, while simultaneously returning real-time images to the host. The D455 camera provides $640\times480$ images at roughly 30Hz over TCP. Due to hardware constraints and TCP network limitations, the G1 cannot directly process high-frequency data. In our deployment, we therefore update the arm joint position targets at 10Hz and interpolate these targets 50 times to drive the robot’s upper body smoothly. However, if a robot can accept higher-frequency control signals, our system can support an update frequency over 200Hz.
\begin{figure}[!ht]
  \centering
  \includegraphics[width=0.8\textwidth]{./images/deploy.pdf}
  \caption{Deployment of our system with cockpit and autonomous policy. \textcolor{mycolor}{(a):} Deployment with robot controlled by isomorphic exoskeleton cockpit. \textcolor{mycolor}{(b):} Deployment with robot controlled by autonomous policy $\pi_{auto}$.}
  \label{fig:deploy}
\end{figure}

\subsection{IL Deployment}
Once we train the policy $\pi_{auto}$ using the method described in \cref{sec:iltraining}, we deploy it as illustrated in \cref{fig:deploy}(b). We connect the Unitree G1 to a host equipped with an Nvidia RTX 4080 GPU via an Ethernet cable, enabling wired TCP communication. This wired setup provides the faster data transfer needed for transmitting images. The runtime involves two processes on the G1, labeled \textcolor{mycolor}{1} and \textcolor{mycolor}{2}, and one process on the host, labeled \textcolor{mycolor}{3}. Process \textcolor{mycolor}{1} captures images from the D455 camera mounted on the G1’s head and from the D435 cameras on its arms, along with the robot’s joint information $q_{t,i}$. This data is then sent to the host. Process \textcolor{mycolor}{3} receives the inputs, performs inference of $\pi_{auto}$ to compute $C_t$ and $q_{upper}$, and returns these results to the G1. Finally, process \textcolor{mycolor}{2} controls the robot’s motion using the inferred commands. The entire loop runs at a frequency of 10Hz.


\subsection{Simulation Deployment}
We train the policies in Isaac Gym, which is sufficient for training locomotion policies but lacks the capability to simulate realistic scenes. Therefore, we employ a sim2sim process to transfer our policies to Isaac Sim. The core of this process involves aligning the joint order and quaternion conventions between the two platforms. In Isaac Gym, the joint order follows depth-first ordering, and quaternions are formatted as xyzw. In contrast, Isaac Sim uses breadth-first ordering for joints and wxyz for quaternions~\cite{mittal2023orbit}. As a result, both the neural network's observation $O_{t-5:t}$ and the computed action $a_t$ must undergo corresponding order adjustments. To simulate the real-world camera perspective in the simulation, we directly add a Camera to the robot's USD file and place its prim path under the prim path of the link to which it is bound. This ensures that the camera moves along with the corresponding link during motion in the simulation. The camera parameters, such as resolution and focal length, are configured to match those used in the real world.


\end{appendices}