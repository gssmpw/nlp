\begin{figure*}[!ht]
  \centering
  \includegraphics[width=1.00\textwidth]{./images/aba.pdf}
  \caption{\textbf{Ablation experiments of our RL training framework.} Each row from top to bottom represents the ablation study for upper-body curriculum, height tracking reward, and symmetry utilization, respectively. Each column represents the evaluation of the corresponding metrics for checkpoints under different ablation settings. The $\uparrow$ and $\downarrow$ symbols beside the metrics indicate whether a higher or lower value is better for the respective metric.}
  \label{fig:rl_exp}
  \vspace{-8pt}
\end{figure*}
\section{Experiment}

\subsection{Locomotion Policy}

\subsubsection{Ablation of training framework}
\label{sec:abla}
In this section, we perform ablation experiments on the proposed upper-body pose curriculum, the height tracking reward, and the use of symmetry. All ablation experiments are conducted based on the methods described in \cref{sec:method_rl}. For each setting, we use three random seeds to train policies for Unitree G1 and evaluate them in 1000 environments over a 20-second evaluation period with random upper-body poses sampled from \cref{equa:sample} with $r_a\rightarrow1$. Metrics for evaluation are tracking linear velocity error, tracking angular velocity error, tracking height error, symmetry loss and living time. The final performance for each setting is obtained by computing the average and standard deviation of the results across the three policies trained from three random seeds. All trainings are conducted on Nvidia RTX 4090 and simulated by Isaac Gym with 4096 parallel environments, where components unrelated to the ablation are kept unchanged, and only relevant parts are modified for training. Detailed parameters used in training and evaluation processes are listed in Appendix \ref{appendix:RL}. We mark the setting of our proposed method as \textbf{ours} in the following sections.

\textbf{Upper-body Pose Curriculum.} We compare \textbf{ours} against two alternatives: \textbf{w/o cur}, which omits the curriculum and directly samples $a_i=\mathcal{U}(0,\mathcal{U}(0,1))$, and \textbf{rand}, which uses the same $r_a$ curriculum but replaces \cref{equa:sample} with $a_i=\mathcal{U}(0,\mathcal{U}(0,r_a))$. Since all three methods adopt the same sampling strategy $a_i=\mathcal{U}(0,\mathcal{U}(0,1))$ as $r_a\rightarrow1$, the final objective remains consistent, ensuring a fair comparison. The experimental results, shown in the first row of \cref{fig:rl_exp}, reveal that \textbf{ours} outperforms both \textbf{w/o cur} and \textbf{rand} in linear velocity tracking, angular velocity tracking, and height error, with faster convergence and smaller errors. There is no significant difference between \textbf{w/o cur} and \textbf{rand} in the final results for these metrics. Given that the symmetry loss can reach values on the order of 20 without constraints, no significant difference is observed across the three methods in terms of symmetry loss. All three configurations achieve similar final living times, but \textbf{ours} and \textbf{w/o cur} converge more quickly. \textbf{Rand}, despite employing some curriculum adjustments, is limited by $r_a$, and values in the range $(r_a,1]$ are not sampled during training, making it harder for the model to converge as $r_a$ increases. In contrast, both \textbf{ours} and \textbf{w/o cur} sample the full $[0,1]$ range from the beginning, enabling faster and more stable convergence. Thus, our curriculum approach leads to better performance compared to \textbf{rand}. Although \textbf{w/o cur} does not use a curriculum, allowing $a_i$ to continuously sample from $[0,1]$, the lack of difficulty smoothing leads to worse final tracking results, highlighting that our curriculum design offers a more effective training process.

\textbf{Height Tracking Reward.} We design two additional algorithms \textbf{w/o knee}, which does not use $r_{knee}$ described in \cref{eq:rknee} and \textbf{hei}, which also omits $r_{knee}$ but increases the scale of the height tracking reward. We show the results in the second row of \cref{fig:rl_exp}. As shown in the figure, none of the three settings cause significant changes in the symmetry loss during training. In terms of linear velocity error and angular velocity error, \textbf{ours} and \textbf{w/o knee} perform similarly, while \textbf{hei} shows much larger errors. For height error, our method converges faster than both \textbf{w/o knee} and \textbf{hei}, even though \textbf{hei} initially performs better (at 400 steps). There is no significant difference among the three settings in terms of living time. These results indicate that just scaling up the height tracking reward in \textbf{hei} may initially lead to faster reduction in height tracking error, but it negatively affects the feedback from other rewards, preventing the robot from balancing multiple tasks effectively. In fact, \textbf{hei} ultimately does not achieve faster convergence in height tracking compared to \textbf{ours}. In contrast, the inclusion of \textbf{rknee} in our method provides more specific guidance for squat tracking, allowing the robot to reduce tracking error and converge more quickly. This highlights the effectiveness of \textbf{rknee} in helping the robot learn squat motions. 

\textbf{Symmetry Utilization.} We introduce three algorithmic variants for comparison with \textbf{ours} in terms of symmetry utilization:  \textbf{w/ aug}, which uses only symmetrical data augmentation; \textbf{w/ sym}, which only uses symmetry loss; and \textbf{none}, which does not employ symmetrical data augmentation or symmetry loss. Testing results are presented in the third row of \cref{fig:rl_exp}. Except for symmetry loss, the performance of \textbf{ours} and \textbf{w/ aug} is similar. However, when considering overall tracking accuracy, ours performs slightly better. On the other hand, \textbf{w/ aug} exhibits a very high symmetry loss, suggesting that using symmetry loss helps maintain the robot's left-right symmetry in the learned policy. This indirectly supports the idea that a symmetric policy benefits the robot’s locomotion tasks~\cite{su2024leveraging}. Both \textbf{nsym} and \textbf{none} show a tendency for improvement, but their training speed is much slower. Notably, a direct comparison between \textbf{w/ sym} and \textbf{none} reveals that \textbf{w/ sym} achieves lower symmetry loss. However, due to slower training, \textbf{none} exhibits less symmetry breaking compared to \textbf{w/ aug}. In summary, symmetry data augmentation significantly improves training efficiency, while the use of symmetry loss effectively prevents the policy from sacrificing symmetry to complete tasks and also benefits the task itself.

\begin{figure}[!ht]
\setlength{\belowcaptionskip}{-10pt}
% \vspace{-10pt}
  \centering
  \includegraphics[width=0.35\textwidth]{./images/g1gr1.pdf}
  \caption{Key parameters of Unitree G1 (left) and Fourier GR-1 (right). Hand weight ratio = total weight / hands weight.}
  \label{fig:g1 & gr1}
\end{figure}
\subsubsection{Training on Different Robots}


We select another kind of robot, Fourier GR-1, which is quite different from Unitree G1, to demonstrate the generality of our approach across different robot models. As shown in \cref{fig:g1 & gr1}, Fourier GR-1 is much taller and heavier than Unitree G1 while having lower hand weight ratio. Compared to the training setting of Unitree G1, we only change the range of height tracking and some robot-specific distance values, without any other changes in reward scales or training pipeline. We evaluate the policy trained after 2k steps of each robot with metrics used in \cref{sec:abla} and present them in \cref{tab:gr1}. The results demonstrate that even though these two kinds of robots are quite different, our RL training framework can train them to converge to a policy which can drive robots to perform locomotion and squatting tasks robustly under any upper-body poses. Training details for Fourier GR-1 can be found in Appendix \ref{appendix:RL}.

\begin{table}[!ht]
    \centering
    \caption{Evaluation of different robots trained with our RL training framework}
    \resizebox{.47\textwidth}{!}{
    \begin{tabular}{l|cc}
        \toprule
        \textbf {Metrics} & Unitree G1 & Fourier GR-1 \\
        \midrule
        Lin. Vel Error (m/s)  & 0.194\ci{0.003} & 0.273\ci{0.003}\\
        Ang. Vel Error (rad/s) &  0.451\ci{0.006} & 0.540\ci{0.002}\\
        Height Error (m) & 0.022\ci{0.019}& 0.038\ci{0.003}\\
        symmetry loss (-) &  0.019\ci{0.017}& 0.009\ci{0.001}\\
        Living Time (s) &  19.947\ci{0.092} & 19.960\ci{0.035}\\
        \bottomrule
    \end{tabular}}
    \label{tab:gr1}
\end{table}

% \subsubsection{Traverse Stairs}
% To further extend the capabilities of humanoid robots in loco-manipulation tasks, we integrate our RL training framework into a humanoid locomotion framework named PIM~\cite{long2024learninghumanoidlocomotionperceptive} to train the Unitree G1. We incorporates ground height points $H_t$ to $O_{t-5,t}$. $H_t$ can be acquired by Lidar-based terrain reconstruction~\cite{miki2022elevation}. We sample 96 points, which are uniformly spaced within a rectangular region extending 0.35m to the left and right, and 0.55m to the front and back of the robot.During training, we use terrain level $\mathcal{L}$ to describe the robot’s ability to traverse stairs, where stair $width=0.2\mathcal{L} m$ and $height=0.12\mathcal{L} m$. As shown in Fig.1(e), after adapting the policy to the physical system, the robot can successfully climb up and down stairs, thereby increasing its reachable workspace and enhancing whole-body locomotion. Moreover, incorporating PIM does not noticeably slow the convergence of training. As illustrated in Fig.XX, in simulation, the robot achieves near-complete mastery of the most challenging terrain level in around 2,500 steps (approximately 2.5 hours on an Nvidia RTX4090), indicating that our approach can be readily integrated into existing locomotion methods.







\subsection{Teleoperation Hardware Performance}
\begin{table}[!ht]
    \centering
    \caption{Hardware Indicators of three component of the hardware system.(Freq.:frequency, Acc.:accuracy)}
    \resizebox{.47\textwidth}{!}{
    \begin{tabular}{lccc}
       \toprule%
        Hardware&Cost (\$) &Acquisition Freq.&Acquisition Acc. \\
        \cmidrule[0.6pt](lr{0.125em}){1-4}
        \textbf {Exoskeleton}& 430 &0.26 kHz&~$2^{12}$(with 360\textdegree)\\
        \textbf {Glove}& 30 (each)&0.3 kHz& ~$2^{12}$ \\
        \textbf {Pedal}& 20 &0.5 kHz& ~$2^{12}$(with 270\textdegree)\\
       \bottomrule
    \end{tabular}}
    \label{tab:Hardware}
\end{table}
We list a series of hardware indicators for our teleoperation hardware system consisting of isomorphic exoskeleton arms, a pair of motion-sensing exoskeleton gloves, and a pedal in \cref{tab:Hardware}. We detail their costs, with the primary expense attributed to the exoskeleton section. This is because we independently design and solder the control boards (PCBs) and sensor modules for the motion-sensing gloves and the pedal components. The acquisition frequency represents the update signal frequency measured between the hardware components of the teleoperation system and the host computer via a wired connection at a baud rate of 115200. Changing the baud rate can affect the acquisition frequency. The acquisition accuracy represents the range of angular change (in degrees) and the corresponding variation in acquisition readings, ranging from 0 to 4095(~$2^{12}$). Since the mapping relationship for the motion-sensing gloves is not a clearly defined linear one, and the mapping angles for each finger joint vary, more detailed information can be found in the Appendix \ref{appendix:Hardware}.

For upper-body teleoperation, the task can be divided into two parts: arm control and dexterous hand control. We select the arm pose frequency and hand pose frequency as evaluation metrics, which measures the smoothness and fluidity of teleoperation. In \cref{tab:Fre.}, we compare the visual and VR schemes with our joint-matching scheme. Since our joint-matching scheme directly sets the robot's upper-body poses without the need for additional time-consuming processes, the output frequency to the robot closely matches the acquisition frequency. Therefore, our approach achieves a very high output frequency without requiring GPU and System on Chip (SoC) intensive hardware.
\begin{table}[!ht]
    \centering
    \caption{Upper-body teleoperation frequency of output to the robot's arm and hand. (SoC: System on Chip)}
    \resizebox{.47\textwidth}{!}{
    \begin{tabular}{l|ccc}
        \toprule
        \textbf {Teleop system} & Hardware & Arm (Hz) & Hand (Hz) \\
        \midrule
        Telekinesis~\cite{Sivakumar-RSS-22}  & 2 RTX 3080 Ti & 16 & 24\\
        AnyTeleop~\cite{qin2023anyteleop} &  RTX 3090 & 125 & 111\\
        OpenTeleVision~\cite{cheng2024open} & M2 Chip & 60 & 60\\
        \ourrow Ours & \textbf{No GPU / SoC} & \textbf{263}& \textbf{293}\\
        \bottomrule
    \end{tabular}}
    \label{tab:Fre.}
\end{table}

To further demonstrate the extensibility of our motion-sensing gloves, we test different types of dexterous hands from the Dex Retargeting library in AnyTeleop~\cite{qin2023anyteleop} within the SAPIEN~\cite{Xiang_2020_SAPIEN} environment. The results are presented in \cref{fig:hand}, with the upper line shows names of tested dexterous hands while the lower line indicates number of joints of each hand.
\begin{figure}[!ht]
  \centering
  \includegraphics[width=0.48\textwidth]{./images/hands.pdf}
  \caption{Controlling different types of dexterous hands in simulation with our motion-sensing gloves.}
  \label{fig:hand}
  \vspace{-5pt}
\end{figure}


\subsection{Teleoperation System}
\subsubsection{Real World}
\begin{figure}[!ht]
  \centering
  \includegraphics[width=0.48\textwidth]{./images/yaocao.pdf}
  \caption{Desktop tasks for comparison of completion time. \textcolor{mycolor}{a:} Pick \& Place; \textcolor{mycolor}{b:} Scan Barcode; \textcolor{mycolor}{c:} Hand Over; \textcolor{mycolor}{d:} Open Oven.}
  \label{fig:compare}
\end{figure}
We deploy the trained policy on the Unitree G1 in the real world and teleoperate it to perform various loco-manipulation tasks using our isomorphic exoskeleton hardware system. The deployment code for G1 is derived from~\cite{margolis2023walk}. \cref{fig:teaser} \textcolor{mycolor}{(a)} and \cref{fig:teaser} \textcolor{mycolor}{(c)} demonstrate the robot's capability to squat, pick objects from lower shelves, and place them on higher ones, as well as to grasp and transfer boxes between shelves utilizing its locomotion abilities. \cref{fig:teaser} \textcolor{mycolor}{(b)} highlights the extensibility of our system, enabling two operators to control separate robots and collaboratively perform tasks, such as transferring apples. In \cref{fig:teaser} \textcolor{mycolor}{(d)}, the robot is controlled to push a 60 kg person sitting in a chair, who weighs roughly twice as much as the robot, demonstrating the robustness of the loco-manipulation system. \cref{fig:teaser} \textcolor{mycolor}{(e)} illustrates how the robot uses its loco-manipulation abilities to open an oven by grasping the handle and moving backward simultaneously. \cref{fig:teaser} \textcolor{mycolor}{(f)} shows that our teleoperation system is capable of performing dual-hand collaborative tasks, such as one hand passing an object to the other. \cref{fig:teaser} \textcolor{mycolor}{(g)} demonstrates the robot's ability to grasp objects from low ground, while \cref{fig:teaser} \textcolor{mycolor}{(h)} shows the robot’s capability to lift and place heavy items, such as a bundle of flowers, into a box using both arms. \cref{fig:teaser} \textcolor{mycolor}{(i)} demonstrates how the robot maintains balance with different upper-body poses. In all these tasks, each robot is controlled by a single operator, and the communication between the robot and operator is facilitated via Wi-Fi, without restricting the robot's movement space. These tasks showcase the robustness of our loco-manipulation policy and \ourshort’s ability to teleopeate humanoids perform a wide range of complex tasks in various environments.
\begin{figure}[!ht]
  \centering
  \includegraphics[width=0.46\textwidth]{./images/opentv.pdf}
  \caption{Comparison of completion time     to perform desktop tasks between our hardware system and OpenTelevision~\cite{cheng2024open}.}
  \label{fig:opentv}
\end{figure}


To demonstrate the efficiency of our teleoperation system, we compare the task completion time between our hardware system and a VR-based method, OpenTelevision~\cite{cheng2024open}, across four tasks as shown in 
\cref{fig:compare}. These tasks are designed to evaluate the system's ability to precisely control the robot's arms and hands in various 
scenarios: \textbf{Pick \& Place:} The robot is required to grasp a tomato from the table and place it into a fruit basket. \textbf{Scan Barcode:} 
The robot must hold a scanner, press its button using a finger, and scan a barcode on a box. \textbf{Hand Over:} The robot needs to grasp a tomato and pass it to another hand. \textbf{Open Oven:} The robot must insert its finger into a handle and open the oven door. These tasks test key 
capabilities of teleoperation, including precise positioning, bimanual coordination, and fine-grained finger control. The results, shown in 
\cref{fig:opentv}, indicate that our system achieves task completion times nearly half of those of the VR-based method. Notably, when tasks require precise positioning and orientation, the performance gap between our system and the VR method becomes even more pronounced. This is because VR-based pose estimation tends to perform poorly in tangential directions, whereas our exoskeleton-based approach avoids such issues entirely. These results demonstrate that our exoskeleton system enables operators to 
teleoperate robots more smoothly and efficiently, particularly in tasks requiring high precision and dexterity.




\subsubsection{Simulation}
We transfer the trained policies for Unitree G1 and Fourier GR-1 from Isaac Gym to a scene developed by GRUtopia~\cite{wang2024grutopia}, which is based on Isaac Sim and IsaacLab~\cite{mittal2023orbit}. This migration enables the use of~\ourshort to control robots within a variety of simulated environments. By leveraging these simulated scenes, the robots can perform diverse loco-manipulation tasks more cost-effectively and in a wider range of scenarios than would be feasible in the real world. As shown in \cref{fig:grutopia}, operators can seamlessly direct the robots' movements and actions in complex, realistic settings, demonstrating the versatility and applicability of \ourshort in diverse simulated contexts.
\begin{figure}[!ht]
  \centering
  \includegraphics[width=0.5\textwidth]{./images/sim.pdf}
  \caption{\textbf{Simulation migration.} The upper row illustrates how the operator controls the robot with FPV to perform loco-manipulation tasks. The lower row demonstrates the robot navigating through realistic simulated environments.}
  \label{fig:grutopia}
\end{figure}
\subsection{Autonomous Policy}

\subsubsection{Data Collection}
\begin{wrapfigure}{l}{0.2\textwidth}
  \centering
  \includegraphics[width=0.18\textwidth]{./images/imihard.pdf}
  \caption{Hardware Setup for Imitation Learning}
  \label{fig:imi}
\end{wrapfigure}
To validate the effectiveness of the demonstratons collected by \ourshort for IL algorithms, we design two distinct tasks: \textbf{Squat Pick}: squatting to pick a tomato on the lower sofa; \textbf{Pick \& Place}: picking and placing a tomato. We capture RGB images, robot states $q_t$, the upper-body commands $q_{upper}$, and the locomotion commands $C_t$ at 10Hz, and collect 50 episodes per task. The hardware setup for image capture can be found in \cref{fig:imi}.

\subsubsection{Training Setting}
\label{sec:iltraining}
We adopt an end-to-end visuomotor control policy that takes images and robot proprioceptive signals as inputs and continuously outputs robot control actions.
%
We employ a model named Seer~\cite{tian2024predictive}, which features an autoregressive transformer architecture.
%
Multi-view images are processed through a MAE-pretrained ViT encoder, and the features of robot proprioceptive states are extracted using an MLP. These features are subsequently concatenated into tokens. The information of these tokens are then integrated by a transformer encoder. The transformer encoder utilizes an autoregressive method to generate latent codes for controlling upper arm joints, dexterous hand movements, and height commands. The final control action output is generated by three distinct regression heads. The whole network are optimized using SmoothL1 loss.
%
In real-world training scenarios, we configure the sequence length to 7, with both visual foresight and action prediction steps set to 3. We employ the MAE pre-trained ViT-B encoder, using bfloat16 configuration to speed up inference. This model is trained on eight A100 GPUs for 40 epoches, and we selected the checkpoint with the lowest average validation loss for evaluation.
%
% For evaluation, we adopt the metric Success Rate (SR). The experimental results are presented in \cref{tab:imi}
\begin{table}[!ht]
    \centering
    \caption{Success Rate of Imitation Learning Tasks}
    \begin{tabular}{c|cc}
        \toprule
        \textbf {Tasks} & Squat Pick & Pick \& Place\\
        \midrule
        Success Rate (\%)  & 73.3 & 80.0\\
        \bottomrule
    \end{tabular}
    \label{tab:imi}
\end{table}
\subsubsection{Learning Results}

After training with collected data, we deploy the trained model to humanoid robot in the real world, with the trained $\pi_{auto}$ taking over operator to control the robot.
%Since the onboard computer of G1 is Nvidia Jetson Orin NX, which is not capable of running IL models, 
\begin{figure}[!ht]
  \centering
  \includegraphics[width=0.48\textwidth]{./images/iltask.pdf}
  \caption{\textbf{Autonomous policy controlling robot to perform tasks.} \textcolor{mycolor}{a:} Squat Pick; \textcolor{mycolor}{b:} Pick \& Place.}
  \label{fig:imitask}
\end{figure}
We employ an Nvidia RTX 4080 to run the trained model and send the output to robot. The detailed deployment configuration are introduced in Appendix \ref{appendix:deploydetail}. For evaluation, we adopt the metric Success Rate (SR) of each task. After testing each proposed task for 15 times, we report the result as task success rate in \cref{tab:imi}. This result shows that data collected by our teleopertion system can actually drive robots to complete complex whole-body loco-manipulation tasks. Robots that controlled by $\pi_{auto}$ to perform proposed tasks are shown in \cref{fig:imitask}.
