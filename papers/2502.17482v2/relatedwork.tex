\section{Related Works}
\label{sect:rw}
\subsection{Contrastive Learning}
Contrastive learning aims to learn invariant and discriminative feature representations from different views of data, which brings similar instances (positive pairs) closer and pushes dissimilar instances (negative pairs) apart in the feature space. Different contrastive learning approaches vary on loss functions, network architectures, and ways of choosing negative samples and positive samples. 

Most contrastive learning approaches highlight on unsupervised learning. SimCLR \cite{simclr2020} is a straightforward contrastive learning framework, which contains two augmented views and maximizes their consistency through the normalized temperature-scaled cross entropy loss. Augmented views originating from the same sample are considered positives, while all other augmented views from different samples are considered negatives. In contrast to SimCLR, MoCo \cite{he2020moco} employs a memory bank to accumulate a substantial number of negatives encoded by a momentum-updated encoder. A recent trend involves exploring contrastive learning without explicitly using negative samples. For instance, BYOL \cite{grill2020BYOL} achieves representation learning by bootstrapping representations from two neural networks that learn and interact collaboratively. SimSiam \cite{chen2021simsiam} departs from the reliance on negative samples and instead utilizes a siamese network and stop-gradient operation.

Compared with unsupervised contrastive learning, supervised contrastive learning was much less studied. The main idea is to additionally utilize class information into designing positive and negative pairs. For example, SupCon \cite{Khosla2020} considers contrasting the set of all samples from the same class as positives against the negatives from the remainder within a batch.

\subsection{Contrastive Learning for EEG-based BCIs}
For EEG-based BCIs, contrastive learning optimizes the extracted representations, aiming to better cope with variations in different subjects, tasks, and environments, thereby enhancing performance and generalization capabilities. \cite{cheng2020subject} incorporated a subject-specific contrastive loss and adversarial training to learn subject-invariant features. In seizure classification,  \cite{huang2023epilepsynet} employed contrastive learning to alleviate the reliance on extensive labeled data. For sleep stage classification,  \cite{jiang2021self} established a pretext task focused on identifying the right transformation pairs. \cite{lee2022self} utilized the attention mechanism to refine the quality of positive pairs. \cite{Mohsenvand2020} extended SimCLR to EEG data and developed a channel-wise feature extractor. \cite{zhang2022expert} conducted contrastive learning upon local representations and contextual representations, and incorporated expert knowledge to craft more accurate positive samples.  \cite{weng2023knowledge} integrated neurological theory to extract effective representations.

Current works that apply contrastive learning for EEG analysis mainly consider the unsupervised category. The performance thus highly relies on large quantity of EEG data and careful parameter tuning, which may not be the optimal improvement over most supervised learning strategies.

\subsection{EEG Data Augmentation} 
Existing EEG data augmentation strategies mainly include time, frequency, and spatial domain augmentations.

For time domain augmentations, \cite{Wang2018} introduced random Gaussian white noise to the original trials, \cite{Mohsenvand2020} selectively zeroed a random portion of the EEG trial, and \cite{Rommel2021} applied random trial flips or reversed the axis of time across all channels. For frequency domain augmentations, \cite{Schwabedal2018surr} randomized the phases of Fourier transforms for all channels, \cite{Mohsenvand2020} and \cite{cheng2020subject} randomly filtered narrow frequency bands across all channels, while \cite{Rommel2021} introduced random shifts to the power spectral density of all channels. For spatial domain augmentations, \cite{Saeed2021} involved zeroing the values of randomly selected channels or performing random permutations. \cite{Krell2017} interpolated channels at randomly rotated positions. \cite{pei2021hs} selected and recombined the left brain part and the right brain part of different samples.

However, existing approaches usually consider one single EEG data augmentation strategy at a time, without integrating multi-view knowledge together.