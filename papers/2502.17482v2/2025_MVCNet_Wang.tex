\documentclass[journal]{IEEEtran}
\usepackage{commath,graphicx,afterpage,subfigure,amsmath,amsfonts,amssymb,algorithm,algorithmic}
\usepackage{cite,multirow,bm,multicol,booktabs,threeparttable,extpfeil}
\usepackage{graphicx}
\newcommand{\vrulesep}{\unskip\ \vrule\ }
\renewcommand{\algorithmicrequire}{ \textbf{Input:}}
\renewcommand{\algorithmicensure}{ \textbf{Output:}}
\DeclareMathOperator*{\argmax}{arg\,max}

\begin{document}

\title{Multi-View Contrastive Network (MVCNet) for \\ Motor Imagery Classification}

\author{Ziwei~Wang, Siyang~Li, Xiaoqing~Chen, Wei~Li$^{\ast}$, and Dongrui~Wu$^{\ast}$, \IEEEmembership{Fellow,~IEEE}
\thanks{This research was supported by the Shenzhen Science and Technology Innovation Program JCYJ20220818103602004, and the Fundamental Research Funds for the Central Universities 2023BR024.}
\thanks{Z.~Wang, S.~Li, X.~Chen, W.~Li and D.~Wu are with the Ministry of Education Key Laboratory of Image Processing and Intelligent Control, School of Artificial Intelligence and Automation, Huazhong University of Science and Technology, Wuhan 430074, China. Z.~Wang, S.~Li, X.~Chen and D.~Wu are also with the Shenzhen Huazhong University of Science and Technology Research Institute, Shenzhen, 518000 China.}
\thanks{*Corresponding Authors: Wei Li (liwei0828@mail.hust.edu.cn) and Dongrui Wu (drwu09@gmail.com).}}

\markboth{IEEE Transactions on ...}
{Shell \MakeLowercase{Wang~\emph{et al.}}: }
\maketitle

\begin{abstract}
Objective: An electroencephalography (EEG)-based brain-computer interface (BCI) serves as a direct communication pathway between the human brain and an external device. While supervised learning has been extensively explored for motor imagery (MI) EEG classification, small data quantity has been a key factor limiting the performance of deep feature learning. Methods: This paper proposes a knowledge-driven time-space-frequency based multi-view contrastive network (MVCNet) for MI EEG decoding in BCIs. MVCNet integrates knowledge from the time, space, and frequency domains into the training process through data augmentations from multiple views, fostering more discriminative feature learning of the characteristics of EEG data. We introduce a cross-view contrasting module to learn from different augmented views and a cross-model contrasting module to enhance the consistency of features extracted between knowledge-guided and data-driven models. Results: The combination of EEG data augmentation strategies was systematically investigated for more informative supervised contrastive learning. Experiments on four public MI datasets and three different architectures demonstrated that MVCNet outperformed 10 existing approaches. Significance: Our approach can significantly boost EEG classification performance beyond designated networks, showcasing the potential to enhance the feature learning process for better EEG decoding.
\end{abstract}

\begin{IEEEkeywords}
Brain-computer interface, contrastive learning, data augmentation, electroencephalogram
\end{IEEEkeywords}

\section{Introduction}
A brain-computer interface (BCI) serves as a direct communication pathway between a user's brain and an external device \cite{Rosenfeld2017}. BCIs have a crucial role in mapping, assisting, augmenting, and potentially restoring human cognitive and/or sensory-motor functions \cite{Krucoff2016}. Furthermore, BCIs contribute significantly to cognitive behavior assessment, pain management, emotional regulation, neurogaming, etc \cite{van2012brain}. BCIs can be categorized into non-invasive, partially invasive, and invasive ones, based on the proximity of electrodes to the brain cortex \cite{wu2020transfer}. Among them, non-invasive electroencephalography (EEG)-based BCIs stand out due to their convenience and cost-effectiveness.

Currently, EEG decoding heavily relies on manual feature extraction under prior expert knowledge for effective classification tasks. Take the motor imagery (MI) \cite{Pfurtscheller2001} paradigm as an example, which involves users imagining the movement of specific body parts (e.g., left hand, right hand, both feet, or tongue), modulating different regions of the brainâ€™s motor cortex \cite{Wu2022NN}. Common Spatial Patterns (CSP) \cite{Blankertz2008} filtering remains the most prominent feature extraction approach for a decade. Although deep neural network architectures have shown promising results resorting to data-driven learning, current architectures are still largely inspired by the traditional feature extraction process. For example, the design of the popular EEGNet \cite{Lawhern2018EEGNet} and ShallowCNN \cite{deepshallow2017} architectures were inspired by Filter-Bank CSP \cite{Ang2008}, and even the most latest architecture Conformer \cite{song2022conformer} still relies on the ShallowCNN for effective feature extraction. Therefore, the decoding ability has been severely limited for task-specific information, and general properties and characteristics of EEG signals were less explored, thus greatly limiting the scalability and generalizability of current EEG decoding algorithms.

\begin{figure}[htpb] \centering
\includegraphics[width=\linewidth,clip]{intro}
\caption{Illustration of integrating prior knowledge into data-driven neural networks with supervised contrastive learning. Transformations to multiple views of the EEG data ensure that feature learning surpasses the limitation of designated networks. As an example, EEGNet, DeepCNN, and ShallowCNN architectures rely on the intuition of CSP or Filter-Bank CSP for spatial variance maximization across classes (red dotted box), while other important characteristics from time, spatial, and frequency domains of EEG data lack investigation.} \label{fig:intro}
\end{figure}

Contrastive learning has shown great advances in extracting powerful representations of data, mostly in the form of self-supervised learning using unlabeled data via contrasting different augmented views \cite{simclr2020,he2020moco,infomax2018}. In this work, we study supervised contrastive learning to fully unleash the potential of additional integration of prior expert knowledge in data-driven learning, as illustrated in Fig.~\ref{fig:intro}. Our research shows that the incorporation of knowledge-guided data augmentations is paramount to enable neural networks to learn invariant transformations of EEG data. Specifically, we propose time-space-frequency based multi-view contrastive network (MVCNet) to solve the current obstacles that hinder more discriminative EEG feature learning:
\begin{enumerate}
\item Limited data quantity. Typically, the number of EEG trials in one collected dataset is significantly lower than the vast datasets commonly available in fields like computer vision or natural language processing, limiting the capability of feature learning under data-driven optimization using neural networks.
\item Lack of effective data augmentation techniques. Limited data augmentation approaches have been proposed for EEG signals. Yet, transformations of EEG data are essential to integrate the prior expert knowledge of the properties and characteristics of data in data-driven learning.
\item Absence of the integration of knowledge-guided learning and data-driven optimization. Na\"ively transforming and augmenting the EEG data would not achieve the best results, and we show that supervised contrastive learning is a much more effective approach for such integration.
\end{enumerate}

The main contributions of this paper can be summarized as:
\begin{enumerate}
\item We systematically investigate the combination of EEG data augmentation approaches from three views, i.e., time, space, and frequency domains, which are essential for learning more generalized features and crucial for executing contrastive learning. 
\item We propose MVCNet, introducing a cross-view contrasting module to explore features of different augmented views, and a cross-model contrasting module to enhance the consistency of features extracted between knowledge-guided and data-driven models. 
\item Empirical results demonstrate that MVCNet consistently outperformed 10 baselines on three different architectures and with various data augmentation strategies on four public MI datasets.
\end{enumerate}

The remainder of this paper is organized as follows: Section~\ref{sect:rw} reviews related works. Section~\ref{sect:me} details the proposed MVCNet approach. Section~\ref{sect:er} discusses the experimental results and provides analyses. Section~\ref{sect:conclusions} draws conclusions.

\section{Related Works}\label{sect:rw}
\subsection{Contrastive Learning}
Contrastive learning aims to learn invariant and discriminative feature representations from different views of data, which brings similar instances (positive pairs) closer and pushes dissimilar instances (negative pairs) apart in the feature space. Different contrastive learning approaches vary on loss functions, network architectures, and ways of choosing negative samples and positive samples. 

Most contrastive learning approaches highlight on unsupervised learning. SimCLR \cite{simclr2020} is a straightforward contrastive learning framework, which contains two augmented views and maximizes their consistency through the normalized temperature-scaled cross entropy loss. Augmented views originating from the same sample are considered positives, while all other augmented views from different samples are considered negatives. In contrast to SimCLR, MoCo \cite{he2020moco} employs a memory bank to accumulate a substantial number of negatives encoded by a momentum-updated encoder. A recent trend involves exploring contrastive learning without explicitly using negative samples. For instance, BYOL \cite{grill2020BYOL} achieves representation learning by bootstrapping representations from two neural networks that learn and interact collaboratively. SimSiam \cite{chen2021simsiam} departs from the reliance on negative samples and instead utilizes a siamese network and stop-gradient operation.

Compared with unsupervised contrastive learning, supervised contrastive learning was much less studied. The main idea is to additionally utilize class information into designing positive and negative pairs. For example, SupCon \cite{Khosla2020} considers contrasting the set of all samples from the same class as positives against the negatives from the remainder within a batch.

\subsection{Contrastive Learning for EEG-based BCIs}
For EEG-based BCIs, contrastive learning optimizes the extracted representations, aiming to better cope with variations in different subjects, tasks, and environments, thereby enhancing performance and generalization capabilities. \cite{cheng2020subject} incorporated a subject-specific contrastive loss and adversarial training to learn subject-invariant features. In seizure classification,  \cite{huang2023epilepsynet} employed contrastive learning to alleviate the reliance on extensive labeled data. For sleep stage classification,  \cite{jiang2021self} established a pretext task focused on identifying the right transformation pairs. \cite{lee2022self} utilized the attention mechanism to refine the quality of positive pairs. \cite{Mohsenvand2020} extended SimCLR to EEG data and developed a channel-wise feature extractor. \cite{zhang2022expert} conducted contrastive learning upon local representations and contextual representations, and incorporated expert knowledge to craft more accurate positive samples.  \cite{weng2023knowledge} integrated neurological theory to extract effective representations.

Current works that apply contrastive learning for EEG analysis mainly consider the unsupervised category. The performance thus highly relies on large quantity of EEG data and careful parameter tuning, which may not be the optimal improvement over most supervised learning strategies.

\subsection{EEG Data Augmentation} 
Existing EEG data augmentation strategies mainly include time, frequency, and spatial domain augmentations.

For time domain augmentations, \cite{Wang2018} introduced random Gaussian white noise to the original trials, \cite{Mohsenvand2020} selectively zeroed a random portion of the EEG trial, and \cite{Rommel2021} applied random trial flips or reversed the axis of time across all channels. For frequency domain augmentations, \cite{Schwabedal2018surr} randomized the phases of Fourier transforms for all channels, \cite{Mohsenvand2020} and \cite{cheng2020subject} randomly filtered narrow frequency bands across all channels, while \cite{Rommel2021} introduced random shifts to the power spectral density of all channels. For spatial domain augmentations, \cite{Saeed2021} involved zeroing the values of randomly selected channels or performing random permutations. \cite{Krell2017} interpolated channels at randomly rotated positions. \cite{pei2021hs} selected and recombined the left brain part and the right brain part of different samples.

However, existing approaches usually consider one single EEG data augmentation strategy at a time, without integrating multi-view knowledge together. 

\section{Methodology} \label{sect:me}
This section describes the details of the proposed approach, shown in Fig.~\ref{fig:TSF}. Commonly, empirical risk minimization on training data using architectures like EEGNet enables supervised learning. However, this strategy might not capture the full spectrum of EEG data characteristics. To address this, we integrate a SCL module to aid traditional data-driven learning using knowledge-guided insights. The process is detailed as follows:
\begin{enumerate}
\item \textit{Multi-view data transformation}. EEG trials are transformed using three distinct views including time, space, and frequency domains, respectively. These multi-view representations enrich the contextual understanding of the data.
\item \textit{Cross-view contrastive learning}. The core component of our approach operates within an encoder-projector framework. A contrastive loss objective forces the transformations for a given trial to be largely different from the other trials, aiming to extract discriminative features for EEG trials regardless of the views considered. The objective is to achieve clear separability of features across various classes and independent of these views.
\item \textit{Cross-model consistency regularization}. Additionally, a cross-model contrasting module is employed to improve the consistency of the features extracted from the three views with those derived from the primary backbone network such as EEGNet.
\end{enumerate}

Through these learning strategies, we ensure robust, class-discriminative feature development that leverages both data-driven and knowledge-guided mechanisms. It is important to note that our contrastive learning strategy incorporates label information, and the backbone network is trained with a supervised objective. Consequently, the entire architecture is structured as a supervised learning model. The system is optimized end-to-end through a combination of cross-view contrasting, cross-model contrasting, and empirical risk minimization.
\begin{figure}[htpb] \centering
\includegraphics[width=\linewidth,clip]{TSF_framework}
\caption{Multi-view contrastive network (MVCNet) for EEG-based BCIs.} \label{fig:TSF}
\end{figure}

\subsection{EEG Data Augmentation}\label{sub:da_intro}
Seven data augmentation strategies from three views for MI EEG trials were used, including three time domain, two frequency domain, and two spatial domain data augmentation approaches. 
\begin{itemize}
\item Data flipping (Flip) \cite{Zhang2022MSDT}, which flips the EEG trial of each channel in the time domain, resulting in opposite voltage values.
\item Noise adding (Noise) \cite{Zhang2022MSDT}, which adds uniform noise to each EEG trial.
\item Data multiplication (Scale) \cite{Zhang2022MSDT}, which multiplies the original EEG trial by a coefficient around 1.
\item Frequency shift (Shift) \cite{Zhang2022MSDT}, which uses Hilbert transform to shift the frequency of EEG trials.
\item Frequency Surrogate (Surr) \cite{Schwabedal2018surr}, which replaces the Fourier phases of trials by new random numbers from the interval, and applies the inverse Fourier transform.
\item Channel Reflection (CR) \cite{Wang2024}, which exchanges the symmetrical left and right hemisphere channels, as well as the labels.
\item Half Sample (HS) \cite{pei2021hs}, which randomly selects the left brain part and the right brain part of different EEG trials, then recombines the two parts together to form a new sample.
\end{itemize}

%\begin{table}[htpb] 
%\footnotesize
% \centering \setlength{\tabcolsep}{0.5mm}
%\caption{Information of different types of data augmentation strategies.}  \label{tab:da_info}
%\begin{tabular}{c|c|c|c}
%\toprule
%Type & Strategy & Formulation & Parameter\\
%\midrule
%\multirow{3}{*}{Time}
%& Flip & $\tilde{X} = \max(X) - X$ & - \\
%%\cline{2-4}
%& Noise & $\tilde{X} = X + rand \ast std(X) / C_{\text{noise}}$ & $C_{\text{noise}}=2$ \\
%%\cline{2-4}
%& Scale & $\tilde{X} = X \ast (1 \pm C_{\text{scale}})$ & $C_{\text{scale}}=0.05$ \\
%\midrule
%\multirow{2}{*}{Frequency}
%& Shift & $\tilde{X} = F_{\text{shift}}(X, \pm C_{\text{shift}})$ & $C_{\text{shift}}=0.2$ \\
%%\cline{2-4}
%& Surr & $\tilde{X} = F_{\text{surr}}(X, C_{\text{surr}})$ & $C_{\text{surr}}=0.4$ \\
%\midrule
%\multirow{2}{*}{Space}
%& CR & $X_L \leftrightarrow X_R, Y = 1-Y$ & - \\
%%\cline{2-4}
%& HS & $\tilde{X} = X^i_L\oplus X^{j}_R, i \neq j$ & - \\
%\bottomrule
%\end{tabular}
%\end{table}

Visualizations of EEG trials before and after data augmentation are shown in Fig.~\ref{fig:AugExample}.
\begin{figure}[htpb]\centering
\includegraphics[width=.7\linewidth,clip]{aug_all}
\caption{Visualizations of EEG trials before (blue lines) and after (red lines) data augmentation.} \label{fig:AugExample}
\end{figure}

\subsection{Cross-View Contrasting}

Since the designated network already forces feature representation to be highly specific, e.g., highlighting spatial patterns, contrastive learning has to be conducted with an encoder-projector module for cross-view contrasting to learn other characteristics of EEG data.

Features of the three types of augmented data take turns to be the anchors, resulting in three pairs, e.g., time-space, time-frequency, and space-frequency pairs. The positive samples for the anchor are augmentations from the same trial (sample), and the negative ones are different trials and their augmentations. Randomly sampling a minibatch of $N$ trials results in $3N$ augmented trials. For each sample, there are $3(N-1)$ augmented trials within a minibatch as negative samples $\boldsymbol{z}^{neg}$, and others as positive ones.

We adopt the NT-Xent (the normalized temperature-scaled cross-entropy loss), which was widely used in contrastive learning \cite{simclr2020}, as distance function $d$ to maximize the similarity of a positive pair and minimize the similarity of a negative pair. Specifically, the time-space pair $(\boldsymbol{z}_{i}^{T}, \boldsymbol{z}_{i}^{S})$, time-frequency pair $(\boldsymbol{z}_{i}^{T}, \boldsymbol{z}_{i}^{F})$, or space-frequency pair $(\boldsymbol{z}_{i}^{S}, \boldsymbol{z}_{i}^{F})$ distance for $\mathbf{x}_i$ can be defined as:
\begin{align}
d(\boldsymbol{z}_{i}^{v}, \mathbf{z}_{i}^{k})=-\log \frac{\exp \left(\operatorname{sim}\left(\boldsymbol{z}_{i}^{v}, \boldsymbol{z}_{i}^{k}\right) / \tau\right)}{\sum_{j=1}^{N} \mathbb{I}_{[i \neq j]} \exp \left(\operatorname{sim} \left(\boldsymbol{z}_{i}^{v}, \boldsymbol{z}_{j}^{neg}\right) / \tau \right),} \label{eq:TFC}
\end{align}
where $v, k \in\{T,S,F\}$ and $v \neq k$, and $i$ is the $i$-th sample in the batch. $\operatorname{sim}\left(u, v\right) = u^{T}v / \Vert u \Vert \Vert v \Vert$ is the cosine similarity, the $\mathbb{I}_{[i \neq j]}$ is an indicator function that equals to 0 when $i = j$ and 1 otherwise, and $\tau$ a temperature parameter.

The cross-view contrasting loss function is formulated as:
\begin{align}
\mathcal{L}_\textmd{CVC} = \dfrac{1}{NV} \sum_{i=1}^{N} \sum_{v=1}^{V} d(\boldsymbol{z}_{i}^{v}, \boldsymbol{z}_{i}^{k}), \label{eq:cvc_loss}
\end{align}
where $N$ is the number of samples in one batch, and $V$ the number of views, which can be increased.

\subsection{Cross-Model Contrasting}
Cross-model contrasting module is further introduced to enhance the consistency of features extracted from the dual networks, i.e., the feature extractor backbone and the encoder with projector. The purpose is to ensure feature consistency across modules under supervised learning using the designated network and knowledge-guided learning. Note that this module is a learning objective and does not introduce any additional trainable architecture.

We chose the original trial as the anchor. The positive samples for the anchor are augmentations from itself, and negative samples are different trials and their augmentations. For each anchor, there are $4(N-1)$ augmented trials within a minibatch as negative samples $\boldsymbol{z}^{neg}$, and others as positive ones.

NT-Xent is also adopt as the distance between the original features $\boldsymbol{z}_i$ and the augmented features $\boldsymbol{z}_i^{v}$:
\begin{align}
d(\boldsymbol{z}_i, \boldsymbol{z}_i^{v})=-\log \frac{\exp \left(\operatorname{sim}\left(\boldsymbol{z}_{i}, \boldsymbol{z}_{i}^{v}\right) / \tau\right)}{\sum_{j=1}^{N} \mathbb{I}_{[i \neq j]} \exp \left(\operatorname{sim} \left(\boldsymbol{z}_{i}, \boldsymbol{z}_{j}^{neg}\right) / \tau \right)}, \label{eq:TSFC}
\end{align}
where $\boldsymbol{z}_i$ is the feature of the original trial extracted by the backbone network from $\mathbf{x}_i$, and $\boldsymbol{z}_i^{v}$ is the feature of the corresponding $v$-th augmented trial extracted by encoder and projector .

Then the cross-model contrasting loss is formulated as:
\begin{align}
\mathcal{L}_\textmd{CMC}&=\dfrac{1}{NV} \sum_{i=1}^{N} \sum_{v=1}^{V} d(\boldsymbol{z}_i, \boldsymbol{z}_i^{v}). \label{eq:cmc_loss}
\end{align}

\subsection{MVCNet}
Integrating cross-view contrasting, cross-model contrasting, and supervised classification modules, the overall loss function of MVCNet is:
\begin{align}
\mathcal{L}_\textmd{TSF} = \mathcal{L}_\textmd{CLS} + \lambda \cdot \mathcal{L}_\textmd{CVC} + \gamma \cdot \mathcal{L}_\textmd{CMC},\label{eq:TSF_loss}
\end{align}
where $\mathcal{L}_\textmd{CLS}$ is the classic cross-entropy loss of the model predictions (through the backbone and classifier) and the true labels, and $\lambda, \gamma > 0$ are trade-off hyperparameters.

The pseudo-code of MVCNet is given in Algorithm~\ref{alg:MVCNet}.
\begin{algorithm}[tb]
    \caption{Multi-View Contrastive Network (MVCNet).}
    \label{alg:MVCNet}
        \begin{algorithmic}%[1] enables line numbers
\REQUIRE Training data $\{(\mathbf{x}_i, y_i)\}_{i=1}^{n_s}$;\\
Test data $\{(\mathbf{x}_i^t)\}_{i=1}^{n_t}$;\\
Encoder $G$, projector $P$, backbone $E$, classifier $F$;\\
Data augmentation functions $A_T$, $A_S$, and $A_F$;\\
Batch size $N$;\\
Temperature scaling term $\tau$;\\
Trade-off weights $\lambda$ and $\gamma$;\\
\ENSURE Classification $\{\hat{y}_i^t\}_{i=1}^{n_t}$ for $\{(\mathbf{x}_i^t)\}_{i=1}^{n_t}$.
        \WHILE{$\mathcal{L}_{TSF}$ not converge}
        \STATE Sample a batch of $N$ samples from $\{(\mathbf{x}_i, y_i)\}_{i=1}^{n_s}$;\\
        \STATE $\mathbf{x}_i^T, \mathbf{x}_i^S, \mathbf{x}_i^F=A_T(\mathbf{x}_i), A_S(\mathbf{x}_i), A_F(\mathbf{x}_i)$;\\
        \STATE $h_i^T, h_i^S, h_i^F=G(\mathbf{x}_i^T), G(\mathbf{x}_i^S), G(\mathbf{x}_i^F)$;\\
        \STATE $\boldsymbol{z}_i^T, \boldsymbol{z}_i^S, \boldsymbol{z}_i^F=P(\mathbf{x}_i^T), P(\mathbf{x}_i^S), P(\mathbf{x}_i^F)$;\\
        \STATE $\boldsymbol{z}_i=E(\mathbf{x}_i)$;\\
        \STATE $\hat{y}_i=F(\boldsymbol{z}_i)$;\\
        \STATE Calculate $\mathcal{L}_\textmd{CVC}$ on $\boldsymbol{z}_i^v$ and $\boldsymbol{z}_i^k$ by (\ref{eq:TFC})-(\ref{eq:cvc_loss}); \\
        \STATE Calculate $\mathcal{L}_\textmd{CMC}$ on $\boldsymbol{z}_i$ and $\boldsymbol{z}_i^v$ by (\ref{eq:TSFC})-(\ref{eq:cmc_loss}); \\
        \STATE Minimize $\mathcal{L}_\textmd{TSF}$ in (\ref{eq:TSF_loss}) to update $G$, $P$, $E$, and $F$.\\
        \ENDWHILE \\
        \STATE \textbf{Return} $F(E(\{\mathbf{x}_i^t\}_{i=1}^{n_t}))$.\\
        \end{algorithmic}
\end{algorithm}

\section{Experiments and Results}\label{sect:er}
This section presents the datasets, experiments and analyses. Code is available on GitHub\footnote{https://github.com/wzwvv/MVCNet}.

\subsection{Datasets}
Four EEG-based MI benchmark datasets, namely Zhou2016 \cite{Zhou2016}, BNCI2014002 \cite{Steyrl2016BNCI2014002}, and BNCI2015001 \cite{Faller2012BNCI2015001} datasets from MOABB \cite{Jayaram2018}. An additional Blankertz2007 \cite{Blankertz2007MI1} dataset from BCI Competition IV-1 was also used. Their characteristics are summarized in Table \ref{tab:dataset_info}.
\begin{table*}[htpb]  \centering \setlength{\tabcolsep}{1mm} 
\caption{Summary of the used MI datasets.}  
\footnotesize
\label{tab:dataset_info}
\begin{tabular}{c|c|c|c|c|c|c}
\toprule
\multirow{2}{*}{Dataset} & Number of & Number of & Sampling & Trial Length & Number of & \multirow{2}{*}{Task Types} \\
 & Subjects & EEG Channels & Rate (Hz) & (seconds) & Total Trials & \\
\midrule
Zhou2016 & 4 & 14 & 250 & 5 & 409 & left/right hand\\
Blankertz2007 & 7 & 59 & 250 & 3 & 1,400 & left/right hand or left hand/right foot \\
BNCI2014002 & 14 & 15 & 512 & 5 & 1,400 & right hand/feet \\
BNCI2015001 & 12 & 13 & 512 & 5 & 2,400 & right hand/feet \\
\bottomrule
\end{tabular}
\end{table*}

For the Zhou2016, BNCI2014002, and BNCI2015001 datasets, the standard preprocessing steps in MOABB, including notch filtering, band-pass filtering, etc., were used to ensure the reproducibility. For the Blankertz2007 dataset, the EEG trials were first band-pass filtered between 8 and 30 Hz. Trials between [0.5, 3.5] seconds after the cue onset were used and then downsampled to 250 Hz. 

Euclidean Alignment (EA) \cite{He2020EA}, an effective unsupervised EEG data alignment approach \cite{Wu2022NN}, was utilized after pre-processing. For the target subject, the reference matrix of EA was updated as new test trials arrived on-the-fly, as in \cite{Li2024T-TIME}. 

\subsection{Algorithms}
EEGNet \cite{Lawhern2018EEGNet}, DeepCNN, and ShallowCNN \cite{deepshallow2017} were utilized as backbone networks. 
EEGNet is a compact CNN architecture tailored for EEG classification, featuring two convolutional blocks and a single classification block. In contrast to EEGNet, DeepCNN has a higher parameter size and includes three convolutional blocks along with a softmax layer for classification. ShallowCNN is a simplified version of DeepCNN inspired from filter bank common spatial patterns. 

We compared MVCNet with 10 approaches on three networks, including Baseline, data augmentation strategies, and contrastive learning approaches:
\begin{itemize}
\item Baseline, which was trained with standard cross-entropy loss without data augmentations.
\item Seven EEG data augmentation strategies were compared and also trained in a supervised manner. CR, HS and Flip are hyperparameter-free. Conversely, Noise, Scale, Freq, and Surr have hyper-parameters, of values based on \cite{Zhang2022MSDT,pei2021hs}.
\item SimCLR, which uses NT-Xent loss \cite{simclr2020} under the proposed SCL framework using two augmented views. 
\item InfoNCE, which uses momentum contrastive loss \cite{he2020moco} under the proposed SCL framework using two augmented views. 
\end{itemize}

Note that for SimCLR and InfoNCE, the number of branches under the proposed SCL framework is reduced to two, instead of three in MVCNet. The two views are randomly selected each time from the data augmentation pool.

\subsection{Implementation}
\textbf{Network architectures.} The network architectures are listed in Table~\ref{tab:network}. We used EEGNet, DeepCNN, or ShallowCNN as the backbone network $E$, a fully-connected layer as the classifier $F$, a lightweight 2-layer and 2-head Transformer \cite{vaswani2017attention} as the encoder $G$, and 2-layer MLPs with batch normalization and ReLU activation as the projector $P$. 

\begin{table}[htbp]   \centering \setlength{\tabcolsep}{0.5mm}
\footnotesize 
  \caption{Details of the four modules, where $d_f$ is the dimension of feature extracted from the backbone, $T$ is the number of time samples, and $C$ the number of EEG channels.}
    \begin{tabular}{c|c}
    \toprule
    Module & Architecture \\
    \midrule
    Backbone & EEGNet, DeepCNN, or ShallowCNN \\
    \midrule
    Classifier & Fully-connected layer with $d_f$ input features\\
    \midrule
    \multirow{2}{*}{Encoder} & 2-layer and 2-head Transformer encoder, with input dimension $T$, \\
    & and feedforward network dimension $2*T$ \\
    \midrule
    \multirow{6}{*}{Projector} 
    & Fully connected layer with input dimension $C*T$ \\
    & and output dimension $4*d_f$ \\
    & BatchNorm, ReLU\\
    & Fully connected layer with input dimension $4*d_f$ \\
    & and output dimension $d_f$ \\
    \bottomrule
    \end{tabular}
  \label{tab:network}
\end{table}

\noindent\textbf{Implementation details.} We performed leave-one-subject-out cross-validation on all datasets, as it is a more valuable and practical setting for EEG-based BCI applications. All experiments were repeated five times and average results were reported. For all datasets, the models were trained for 100 epochs using Adam optimizer with learning rate $10^{-3}$. $\tau$ was set to 0.2 as in \cite{chen2020simclrv2}. $\lambda$ and $\gamma$ were set to 0.1 for all datasets. The batch size was 32 for Baseline, 64 for data augmentation approaches, and 256 for SimCLR, InfoNCE and MVCNet based on the size of training data. Note that for MVCNet, three distinct data augmentation strategies are applied: Scale (time domain), Shift (frequency domain), and HS (space domain). 

\subsection{Main Results}
\begin{table*}[htpb]
\centering 
\setlength{\tabcolsep}{2mm}
\renewcommand\arraystretch{1}
\caption{Average cross-subject classification accuracies (\%) on four MI datasets using three networks. The best average performance of each network is marked in bold, and the second best by an underline.}  \label{tab:basic_results}
    \begin{tabular}{c|c|ccccc}   \toprule  
Backbone & Approach & Zhou2016 & Blankertz2007 & BNCI2014002 & BNCI2015001 & Average \\
\midrule
\multirow{11}{*}{ShallowCNN}
&Baseline & 81.97$_{\pm1.63}$ & 70.04$_{\pm0.79}$ & 68.13$_{\pm0.74}$ & 69.71$_{\pm0.67}$ & 72.46 \\
&Flip & 81.82$_{\pm0.74}$ & 74.10$_{\pm0.90}$ & 72.10$_{\pm0.37}$ & 69.80$_{\pm0.89}$ & 74.46 \\
&Noise & 82.62$_{\pm1.20}$ & 73.59$_{\pm0.90}$ & \textbf{72.97}$_{\pm0.41}$ & 69.31$_{\pm1.00}$ & \underline{74.62} \\
&Scale & \underline{83.67}$_{\pm0.91}$ & 71.96$_{\pm0.87}$ & 72.36$_{\pm0.74}$ & 68.98$_{\pm0.82}$ & 74.24 \\
&Shift & 81.67$_{\pm0.61}$ & 73.57$_{\pm0.44}$ & 72.28$_{\pm0.70}$ & 69.35$_{\pm0.90}$ & 74.22 \\
&Surr & 63.04$_{\pm1.20}$ & 61.51$_{\pm0.88}$ & 66.74$_{\pm1.14}$ & 68.60$_{\pm0.65}$ & 64.97 \\
&CR & 81.87$_{\pm0.78}$ & 73.89$_{\pm0.66}$ & 72.43$_{\pm0.45}$ & 69.50$_{\pm0.73}$ & 74.42 \\
&HS & 81.57$_{\pm0.58}$ & 73.38$_{\pm0.63}$ & 71.72$_{\pm0.67}$ & 68.32$_{\pm0.96}$ & 73.75 \\
&SimCLR & 80.76$_{\pm1.37}$ & 72.91$_{\pm1.08}$ & 72.32$_{\pm0.48}$ & 70.66$_{\pm1.10}$ & 74.16 \\
&InfoNCE & 80.64$_{\pm1.23}$ & \underline{74.55}$_{\pm0.97}$ & 71.92$_{\pm0.45}$ & \underline{70.83}$_{\pm1.05}$ & 74.49 \\
&MVCNet (Ours) & \textbf{84.06}$_{\pm1.02}$ & \textbf{75.09}$_{\pm0.63}$ & \underline{72.67}$_{\pm1.09}$ & \textbf{71.74}$_{\pm0.98}$ & \textbf{75.89} \\
\midrule
\multirow{11}{*}{DeepCNN}
&Baseline & 82.91$_{\pm0.85}$ & 71.44$_{\pm0.78}$ & 69.74$_{\pm0.94}$ & 70.42$_{\pm0.68}$ & 73.63 \\
&Flip & 84.01$_{\pm1.21}$ & 71.31$_{\pm0.76}$ & 74.76$_{\pm0.97}$ & 73.65$_{\pm0.82}$ & 75.93 \\
&Noise & 83.27$_{\pm0.44}$ & 71.06$_{\pm1.32}$ & 74.74$_{\pm1.12}$ & 74.20$_{\pm0.41}$ & 75.82 \\
&Scale & 83.22$_{\pm1.30}$ & 71.80$_{\pm0.46}$ & 74.98$_{\pm0.80}$ & 74.32$_{\pm0.86}$ & 76.08 \\
&Shift & 83.69$_{\pm1.13}$ & 70.87$_{\pm0.52}$ & \textbf{75.14}$_{\pm0.87}$ & 74.12$_{\pm0.75}$ & 75.96 \\
&Surr & 78.76$_{\pm1.73}$ & 67.39$_{\pm0.76}$ & 72.23$_{\pm0.46}$ & 73.86$_{\pm0.62}$ & 73.06 \\
&CR & 84.22$_{\pm1.26}$ & 72.10$_{\pm1.50}$ & 74.35$_{\pm1.20}$ & 73.93$_{\pm0.35}$ & 76.15 \\
&HS & 53.88$_{\pm1.43}$ & 50.72$_{\pm0.42}$ & 54.41$_{\pm2.60}$ & 61.78$_{\pm2.59}$ & 55.20 \\
&SimCLR & 85.19$_{\pm1.26}$ & 74.03$_{\pm1.43}$ & 74.36$_{\pm0.88}$ & \textbf{76.11}$_{\pm0.61}$ & \underline{77.42} \\
&InfoNCE & \underline{85.44}$_{\pm1.31}$ & \underline{74.12}$_{\pm0.89}$ & 74.04$_{\pm0.65}$ & 75.46$_{\pm0.82}$ & 77.26 \\
&MVCNet (Ours) & \textbf{86.70}$_{\pm0.43}$ & \textbf{75.11}$_{\pm1.24}$ & \underline{75.12}$_{\pm0.67}$ & \underline{76.01}$_{\pm0.32}$ & \textbf{78.24} \\
\midrule
\multirow{11}{*}{EEGNet}
&Baseline & 83.22$_{\pm1.73}$ & 71.17$_{\pm0.87}$ & 72.86$_{\pm0.38}$  & 71.89$_{\pm0.70}$  & 74.79 \\
&Flip & 81.19$_{\pm2.39}$ & 69.86$_{\pm1.49}$ & 73.75$_{\pm1.31}$ & 71.96$_{\pm1.11}$ & 74.19 \\
&Noise & 84.16$_{\pm0.96}$ & 71.87$_{\pm0.55}$ & 72.49$_{\pm0.69}$ & 72.28$_{\pm1.02}$ & 75.20 \\
&Scale& 83.99$_{\pm0.83}$ & 71.70$_{\pm0.73}$ & 72.59$_{\pm0.68}$ & 71.73$_{\pm1.30}$ & 75.00 \\
&Shift & 82.94$_{\pm1.99}$ & 70.96$_{\pm0.82}$ & 73.51$_{\pm1.07}$ & 73.11$_{\pm1.31}$ & 75.13 \\
&Surr & 83.82$_{\pm1.18}$ & 69.82$_{\pm0.56}$ & 72.21$_{\pm0.97}$ & 73.21$_{\pm1.20}$ & 74.77 \\
&CR & 84.82$_{\pm1.36}$ & 74.97$_{\pm0.96}$ & 72.43$_{\pm0.70}$ & 72.21$_{\pm0.93}$ & 76.11 \\
&HS & 80.18$_{\pm2.52}$ & 68.31$_{\pm2.68}$ & 69.99$_{\pm2.37}$ & 70.91$_{\pm2.17}$ & 72.35 \\
&SimCLR &\underline{86.04}$_{\pm1.21}$ & 75.21$_{\pm1.23}$ & \underline{74.26}$_{\pm0.87}$ & \underline{75.16}$_{\pm0.94}$ & \underline{77.67}\\
&InfoNCE &85.33$_{\pm3.06}$ & \underline{75.27}$_{\pm0.81}$ & 74.16$_{\pm0.55}$ & 75.15$_{\pm0.44}$ & 77.48\\
&MVCNet (Ours) & \textbf{87.20}$_{\pm1.87}$ & \textbf{76.24}$_{\pm1.81}$ & \textbf{74.95}$_{\pm0.71}$ & \textbf{75.95}$_{\pm0.37}$ & \textbf{78.59}  \\
\bottomrule
\end{tabular}
\end{table*}

The classification accuracies on four MI datasets are listed in Table~\ref{tab:basic_results}. Observe that:
\begin{enumerate}
\item EEGNet exhibited the best performance, DeepCNN followed, and ShallowCNN the worst.
\item Not all data augmentation approaches demonstrate universal effectiveness across all datasets. Surr failed on ShallowCNN and HS did not work on DeepCNN across all datasets. CR achieved the best performance among all data augmentation approaches, although performing comparatively worse on 2014002 and 2015001 due to lack of strict symmetry in right hand/both feet task.
\item The results of SimCLR, InfoNCE and the proposed MVCNet were better than other 7 baselines under augmentation techniques, demonstrating the effectiveness of the proposed SCL framework.
\item Our proposed MVCNet consistently outperformed 10 baseline methods across all backbone networks. In comparison with SimCLR, the most similar approach, MVCNet ensures that the captured features originate from three distinct domains, while SimCLR only considers two views that might come from the same domain. This suggests that MVCNet integrates more valuable information and learns more discriminative representations.
\end{enumerate}

\subsection{Combinations of Multiple Data Augmentations}
To explore the influence of the combination strategy of data augmentation, we evaluated the performance of all conceivable pairwise combinations of the seven data augmentation techniques in the proposed SCL framework. The results in Fig.~\ref{fig:augzoo} show that the effectiveness of data augmentation is more pronounced when utilizing combinations rather than relying on individual augmentation methods. For Zhou2016, the optimal combination achieved an accuracy of 87.01\%, which is lower than the performance of MVCNet when utilizing all three views simultaneously. Similar results can be observed for other datasets. It indicates the necessity of applying multiple forms of transformations, and also verifies the effectiveness of our proposed approach beyond bare augmentations.

\begin{figure*}[htpb]\centering
\subfigure[]{\includegraphics[width=.2\linewidth,clip]{aug_zoo_Zhou2016}}
\subfigure[]{\includegraphics[width=.2\linewidth,clip]{aug_zoo_MI1}}
\subfigure[]{\includegraphics[width=.2\linewidth,clip]{aug_zoo_14002}}
\subfigure[]{\includegraphics[width=.2\linewidth,clip]{aug_zoo_15001}}
\caption{Average cross-subject classification accuracies on (a) Zhou2016, (b) Blankertz2007, (c) BNCI2014002, and (d) BNCI2015001 datasets under a single or composition of data augmentations applied. Diagonal entries correspond to a single augmentation, and off-diagonals correspond to the composition of two augmentations.} \label{fig:augzoo}
\end{figure*}

\subsection{Ablation Study and Parameter Sensitivity Analysis}
Fig.~\ref{fig:ablation} shows results of ablation studies on $\mathcal{L}_\textmd{CVC}$ and $\mathcal{L}_\textmd{CMC}$. The results indicate that $\mathcal{L}_\textmd{CVC}$ and $\mathcal{L}_\textmd{CMC}$ both contributed to performance improvement, and using both together consistently lead to the best performance.

We further analyzed the impact of the weights in (\ref{eq:TSF_loss}), i.e., $\lambda$ and $\gamma$, in Fig.~\ref{fig:para_ana}. The results indicate that the proposed approach had stable performance and was insensitive to the trade-off parameters.

\begin{figure}[htpb]\centering
\includegraphics[width=.7\linewidth,clip]{ablation}
\caption{Ablation study of $\mathcal{L}_\textmd{CVC}$ and $\mathcal{L}_\textmd{CMC}$ on four datasets.} \label{fig:ablation}
\end{figure}

\begin{figure}[htpb]\centering
\includegraphics[width=\linewidth,clip]{TSF-SCL_para}
\caption{Parameter sensitivity analysis on $\lambda$ and $\gamma$. When one parameter is changed, the other is fixed to 0.1. A point denotes the average, and the shadow denotes standard deviation.} \label{fig:para_ana}
\end{figure}

\subsection{Visualizations}
Fig.~\ref{fig:tsne} shows the $t$-SNE \cite{VanderMaaten2008} visualizations of the Baseline (EEGNet) feature distributions and MVCNet feature distributions under different augmentation techniques in Blankertz2007 and BNCI2014002 datasets. Observe that the feature distributions of the proposed MVCNet are structurally tighter and can be classified more easily than the feature distributions extracted from EEGNet. It suggests the importance of considering all three views together and MVCNet can effectively minimize their gaps to learn the generalized representations. 

\begin{figure*}[htpb]\centering
\subfigure[]{\includegraphics[width=\linewidth,clip]{tsne_MI1-7_egn}}
\subfigure[]{\includegraphics[width=\linewidth,clip]{tsne_MI1-7_tsf}}
\subfigure[]{\includegraphics[width=\linewidth,clip]{tsne_BNCI2014002_egn}}
\subfigure[]{\includegraphics[width=\linewidth,clip]{tsne_BNCI2014002_tsf}}
\caption{$t$-SNE visualization of (a) EEGNet features and (b) MVCNet features extracted from Subject S1-S7 of Blankertz2007; (c) EEGNet features and (d) MVCNet features extracted from Subject S1-S14 of BNCI2014002. Different colors represent trials transformed through different augmentation approaches, including CR (spatial domain), Scale (time domain), and Shift (frequency domain). The dots and crosses represent trials of different categories.} \label{fig:tsne}
\end{figure*}

\section{Conclusion}\label{sect:conclusions}
Acquiring labeled calibration data from new subjects is a labor-intensive and time-consuming task. The limited quantity of available data has constrained the deep feature learning capabilities of EEG signals, hindering the widespread development of EEG-based BCIs. This paper introduces a novel MVCNet framework to address the common problem of insufficient data in EEG-based BCIs. MVCNet strategically incorporates knowledge from the time, space, and frequency domains into the training process, thereby augmenting the discriminative features extracted from EEG signals. The proposed framework consists of an end-to-end network, featuring a cross-view contrasting module for exploring discriminative features across distinct augmented views, a cross-model contrasting module to maximize feature consistency across diverse models, and a supervised classification module.

Through experiments conducted on four publicly available MI datasets, MVCNet demonstrated superior performance compared to 10 existing approaches. Moreover, it showcased flexibility in design by seamlessly incorporating various data augmentation strategies and proving effective on different backbone networks. These findings underscore the potential of MVCNet in significantly enhancing the effectiveness and practicality of future BCI systems. 

\bibliographystyle{IEEEtran} \bibliography{scl_wang}

\end{document}
