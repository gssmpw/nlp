\section{Related Work}
\paragraph{Position bias in transformers}
Position bias in transformer models has emerged as a critical challenge across diverse applications. In information retrieval and ranking, ____ demonstrated systematic degradation of performance due to positional dependencies. Similarly, in in-context learning, model performance can vary dramatically based solely on the order of examples ____. While mitigation strategies such as novel PEs  ____, alternative masking techniques____ and bootstrapping____ have been proposed, they remain task-specific and empirically driven. This gap between empirical observations and theoretical understanding highlights the need for a rigorous analysis of how transformers process and integrate positional information through attention.


\paragraph{The effect of attention masks and PEs in transformers} 
The role of attention masks and PEs in transformers has been explored from various perspectives. ____ analyzed the function approximation power of transformers under different masking schemes, while ____ investigated the role of attention masks in mitigating rank collapse. Moreover, ____ empirically examined how attention masks affect the emergence of attention sinks. As for PEs, ____ studied their role in length generalization, and ____ analyzed RoPEâ€™s use of feature dimensions. Additionally, ____ empirically observed that both causal masking and RoPE introduce position dependencies in LLMs. Despite these advances, fundamental questions remain about the mechanisms through which attention masks and PEs enable transformers to process and integrate positional information, as well as the nature of the systematic positional biases that emerge as a result.