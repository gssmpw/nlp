@inproceedings{Wu2024OnTR,
  title={On the Role of Attention Masks and LayerNorm in Transformers},
  author={Xinyi Wu and Amir Ajorlou and Yifei Wang and Stefanie Jegelka and Ali Jadbabaie},
  booktitle={NeurIPS},
  year={2024}
}

@inproceedings{Barbero2024TransformersNG,
  title={Transformers need glasses! Information over-squashing in language tasks},
  author={Federico Barbero and Andrea Banino and Steven Kapturowski and Dharshan Kumaran and Joao G.M. Ara'ujo and Alex Vitvitskyi and Razvan Pascanu and Petar Velivckovi'c},
  booktitle={NeurIPS},
  year={2024}
}


â€º@article{Yu2024MitigatePB,
  title={Mitigate Position Bias in Large Language Models via Scaling a Single Dimension},
  author={Yijiong Yu and Huiqiang Jiang and Xufang Luo and Qianhui Wu and Chin-Yew Lin and Dongsheng Li and Yuqing Yang and Yongfeng Huang and Lili Qiu},
  journal={ArXiv},
  year={2024},
  volume={abs/2406.02536}
}


@article{Sun2023RetentiveNA,
  title={Retentive Network: A Successor to Transformer for Large Language Models},
  author={Yutao Sun and Li Dong and Shaohan Huang and Shuming Ma and Yuqing Xia and Jilong Xue and Jianyong Wang and Furu Wei},
  journal={ArXiv},
  year={2023},
  volume={abs/2307.08621},
  url={https://api.semanticscholar.org/CorpusID:259937453}
}


@inproceedings{Press2021TrainST,
  title={Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation},
  author={Ofir Press and Noah A. Smith and Mike Lewis},
  booktitle={ICLR},
  year={2022}
}

@article{Wang2024EliminatingPB,
  title={Eliminating Position Bias of Language Models: A Mechanistic Approach},
  author={Ziqi Wang and Hanlin Zhang and Xiner Li and Kuan-Hao Huang and Chi Han and Shuiwang Ji and Sham M. Kakade and Hao Peng and Heng Ji},
  journal={ArXiv},
  year={2024},
  volume={abs/2407.01100}
}

@inproceedings{Xiao2023EfficientSL,
  title={Efficient Streaming Language Models with Attention Sinks},
  author={Guangxuan Xiao and Yuandong Tian and Beidi Chen and Song Han and Mike Lewis},
  booktitle={ICLR},
  year={2024}
}



@article{guo2024activedormantattentionheadsmechanistically,
      title={Active-Dormant Attention Heads: Mechanistically Demystifying Extreme-Token Phenomena in LLMs}, 
      author={Tianyu Guo and Druv Pai and Yu Bai and Jiantao Jiao and Michael I. Jordan and Song Mei},
      year={2024}
}


@inproceedings{Gu2024WhenAS,
  title={When Attention Sink Emerges in Language Models: An Empirical View},
  author={Xiangming Gu and Tianyu Pang and Chao Du and Qian Liu and Fengzhuo Zhang and Cunxiao Du and Ye Wang and Min Lin},
  booktitle={ICLR},
  year={2025},
}




@inproceedings{Kim2017StructuredAN,
  title={Structured Attention Networks},
  author={Yoon Kim and Carl Denton and Luong Hoang and Alexander M. Rush},
  booktitle={ICLR},
  year={2017}
}

@inproceedings{Bahdanau2014NeuralMT,
  title={Neural Machine Translation by Jointly Learning to Align and Translate},
  author={Dzmitry Bahdanau and Kyunghyun Cho and Yoshua Bengio},
  booktitle={ICLR},
  year={2015}
}


@inproceedings{Kazemnejad2023TheIO,
  title={The Impact of Positional Encoding on Length Generalization in Transformers},
  author={Amirhossein Kazemnejad and Inkit Padhi and Karthikeyan Natesan Ramamurthy and Payel Das and Siva Reddy},
  booktitle={NeurIPS},
  year={2023}
}



@article{Zhang2024FoundIT,
  title={Found in the Middle: How Language Models Use Long Contexts Better via Plug-and-Play Positional Encoding},
  author={Zhenyu (Allen) Zhang and Runjin Chen and Shiwei Liu and Zhewei Yao and Olatunji Ruwase and Beidi Chen and Xiaoxia Wu and Zhangyang Wang},
  journal={ArXiv},
  year={2024},
  volume={abs/2403.04797}
}

@inproceedings{Vaswani2017AttentionIA,
  title={Attention is All you Need},
  author={Ashish Vaswani and Noam M. Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
  booktitle={NeurIPS},
  year={2017}
}


@inproceedings{Yun2019AreTU,
  title={Are Transformers universal approximators of sequence-to-sequence functions?},
  author={Chulhee Yun and Srinadh Bhojanapalli and Ankit Singh Rawat and Sashank J. Reddi and Sanjiv Kumar},
  booktitle={ICLR},
  year={2020}
}

@article{su2023roformerenhancedtransformerrotary,
      title={RoFormer: Enhanced Transformer with Rotary Position Embedding}, 
      author={Jianlin Su and Yu Lu and Shengfeng Pan and Ahmed Murtadha and Bo Wen and Yunfeng Liu},
      year={2023}
}


@inproceedings{Min2022RethinkingTR,
  title={Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?},
  author={Sewon Min and Xinxi Lyu and Ari Holtzman and Mikel Artetxe and Mike Lewis and Hannaneh Hajishirzi and Luke Zettlemoyer},
  booktitle={EMNLP},
  year={2022}
}

@article{liu2024lost,
  title={Lost in the middle: How language models use long contexts},
  author={Liu, Nelson F and Lin, Kevin and Hewitt, John and Paranjape, Ashwin and Bevilacqua, Michele and Petroni, Fabio and Liang, Percy},
  journal={Transactions of the Association for Computational Linguistics},
  year={2024}
}

@article{Jiang2023Mistral7,
  title={Mistral 7B},
  author={Albert Qiaochu Jiang and Alexandre Sablayrolles and Arthur Mensch and Chris Bamford and Devendra Singh Chaplot and Diego de Las Casas and Florian Bressand and Gianna Lengyel and Guillaume Lample and Lucile Saulnier and L'elio Renard Lavaud and Marie-Anne Lachaux and Pierre Stock and Teven Le Scao and Thibaut Lavril and Thomas Wang and Timoth{\'e}e Lacroix and William El Sayed},
  journal={ArXiv},
  year={2023}
}

@article{Beltagy2020LongformerTL,
  title={Longformer: The Long-Document Transformer},
  author={Iz Beltagy and Matthew E. Peters and Arman Cohan},
  journal={ArXiv},
  year={2020}
}

@inproceedings{Alman2023FastAR,
  title={Fast Attention Requires Bounded Entries},
  author={Josh Alman and Zhao Song},
  booktitle={NeurIPS},
  year={2023}
}

@article{2020t5,
  author  = {Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},
  title   = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
  journal = {Journal of Machine Learning Research},
  year    = {2020}
}

@inproceedings{Lewis2019BARTDS,
  title={BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension},
  author={Mike Lewis and Yinhan Liu and Naman Goyal and Marjan Ghazvininejad and Abdel-rahman Mohamed and Omer Levy and Veselin Stoyanov and Luke Zettlemoyer},
  booktitle={ACL},
  year={2020}
}

@inproceedings{Reddy2023TheMB,
  title={The mechanistic basis of data dependence and abrupt learning in an in-context classification task},
  author={Gautam Reddy},
  booktitle={ICLR},
  year={2024}
}

@article{Sanford2024OnelayerTF,
  title={One-layer transformers fail to solve the induction heads task},
  author={Clayton Sanford and Daniel Hsu and Matus Telgarsky},
  journal={ArXiv},
  year={2024},
  volume={abs/2408.14332}
}

@article{Merrill2022ThePT,
  title={The Parallelism Tradeoff: Limitations of Log-Precision Transformers},
  author={William Merrill and Ashish Sabharwal},
  journal={Transactions of the Association for Computational Linguistics},
  year={2022},
  volume={11},
  pages={531-545}
}

@article{Li2024ChainOT,
  title={Chain of Thought Empowers Transformers to Solve Inherently Serial Problems},
  author={Zhiyuan Li and Hong Liu and Denny Zhou and Tengyu Ma},
  journal={ArXiv},
  year={2024},
  volume={abs/2402.12875}
}

@inproceedings{Lu2021FantasticallyOP,
  title={Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity},
  author={Yao Lu and Max Bartolo and Alastair Moore and Sebastian Riedel and Pontus Stenetorp},
  booktitle={ACL},
  year={2022}
}

@inproceedings{zhao2021calibrateuseimprovingfewshot,
      title={Calibrate Before Use: Improving Few-Shot Performance of Language Models}, 
      author={Tony Z. Zhao and Eric Wallace and Shi Feng and Dan Klein and Sameer Singh},
      booktitle={ICML},
      year={2021},
     
}

@inproceedings{Dong2021AttentionIN,
  title={Attention is Not All You Need: Pure Attention Loses Rank Doubly Exponentially with Depth},
  author={Yihe Dong and Jean-Baptiste Cordonnier and Andreas Loukas},
  booktitle={ICML},
  year={2021}
}

@inproceedings{Yun2020OnCA,
  title={O(n) Connections are Expressive Enough: Universal Approximability of Sparse Transformers},
  author={Chulhee Yun and Yin-Wen Chang and Srinadh Bhojanapalli and Ankit Singh Rawat and Sashank J. Reddi and Sanjiv Kumar},
  booktitle={NeurIPS},
  year={2020}
}

@inproceedings{Wu2023Demystify,
  title={Demystifying Oversmoothing in Attention-Based Graph Neural Networks},
  author={Xinyi Wu and Amir Ajorlou and Zihui Wu and Ali Jadbabaie},
  booktitle={NeurIPS},
  year={2023}
}

@article{Geshkovski2023AMP,
  title={A mathematical perspective on Transformers},
  author={Borjan Geshkovski and Cyril Letrouit and Yury Polyanskiy and Philippe Rigollet},
  journal={ArXiv},
  year={2023},
  volume={abs/2312.10794}
}

@inproceedings{Geshkovski2023TheEO,
  title={The emergence of clusters in self-attention dynamics},
  author={Borjan Geshkovski and Cyril Letrouit and Yury Polyanskiy and Philippe Rigollet},
  booktitle={NeurIPS},
  year={2023}
}


@inproceedings{Ahn2023TransformersLT,
  title={Transformers learn to implement preconditioned gradient descent for in-context learning},
  author={Kwangjun Ahn and Xiang Cheng and Hadi Daneshmand and Suvrit Sra},
  booktitle={NeurIPS},
  year={2024}
}

@inproceedings{Bietti2023BirthOA,
  title={Birth of a Transformer: A Memory Viewpoint},
  author={Alberto Bietti and Vivien A. Cabannes and Diane Bouchacourt and Herv{\'e} J{\'e}gou and L{\'e}on Bottou},
  booktitle={NeurIPS},
  year={2023}
}

@inproceedings{Li2023TransformersAA,
  title={Transformers as Algorithms: Generalization and Stability in In-context Learning},
  author={Yingcong Li and Muhammed Emrullah Ildiz and Dimitris Papailiopoulos and Samet Oymak},
  booktitle={ICML},
  year={2023}
}

@inproceedings{Bai2023TransformersAS,
  title={Transformers as Statisticians: Provable In-Context Learning with In-Context Algorithm Selection},
  author={Yu Bai and Fan Chen and Haiquan Wang and Caiming Xiong and Song Mei},
  booktitle={NeurIPS},
  year={2023}
}


@article{Barbero2024RoundAR,
  title={Round and Round We Go! What makes Rotary Positional Encodings useful?},
  author={Federico Barbero and Alex Vitvitskyi and Christos Perivolaropoulos and Razvan Pascanu and Petar Velivckovi'c},
  journal={ArXiv},
  year={2024},
  volume={abs/2410.06205}
}

@inproceedings{Hou2023LargeLM,
  title={Large Language Models are Zero-Shot Rankers for Recommender Systems},
  author={Yupeng Hou and Junjie Zhang and Zihan Lin and Hongyu Lu and Ruobing Xie and Julian McAuley and Wayne Xin Zhao},
  booktitle={ECIR},
  year={2024}
}

@book{Halliday2004AnIT,
  title={An Introduction to Functional Grammar},
  author={Michael A.K. Halliday},
  year={2004}
}

@inproceedings{Devlin2019BERTPO,
  title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author={Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
  booktitle={ACL},
  year={2019}
}

@article{Guo2024SerialPE,
  title={Serial Position Effects of Large Language Models},
  author={Xiaobo Guo and Soroush Vosoughi},
  journal={ArXiv},
  year={2024}
}

@article{Li2024EEGDT,
  title={EEG decoders track memory dynamics},
  author={Yuxuan Li and Jesse K. Pazdera and Michael J. Kahana},
  journal={Nature Communications},
  year={2024}
}

@article{Glanzer1966TwoSM,
  title={Two storage mechanisms in free recall},
  author={Murray Glanzer and Anita R. Cunitz},
  journal={Journal of Verbal Learning and Verbal Behavior},
  year={1966}
}

@inproceedings{Hollenstein2021MultilingualLM,
  title={Multilingual Language Models Predict Human Reading Behavior},
  author={Nora Hollenstein and Federico Pirovano and Ce Zhang and Lena A. J{\"a}ger and Lisa Beinborn},
  booktitle={NAACL},
  year={2021}
}

@inproceedings{Loshchilov2017DecoupledWD,
  title={Decoupled Weight Decay Regularization},
  author={Ilya Loshchilov and Frank Hutter},
  booktitle={ICLR},
  year={2019}
}

@inproceedings{Paszke2019PyTorchAI,
  title={PyTorch: An Imperative Style, High-Performance Deep Learning Library},
  author={Adam Paszke and Sam Gross and Francisco Massa and Adam Lerer and James Bradbury and Gregory Chanan and Trevor Killeen and Zeming Lin and Natalia Gimelshein and Luca Antiga and Alban Desmaison and Andreas K{\"o}pf and Edward Yang and Zach DeVito and Martin Raison and Alykhan Tejani and Sasank Chilamkurthy and Benoit Steiner and Lu Fang and Junjie Bai and Soumith Chintala},
  booktitle={NeurIPS},
  year={2019}
}

@inproceedings{Chi2022KERPLEKR,
  title={KERPLE: Kernelized Relative Positional Embedding for Length Extrapolation},
  author={Ta-Chung Chi and Ting-Han Fan and Peter J. Ramadge and Alexander I. Rudnicky},
  booktitle={NeurIPS},
  year={2022}
}

@article{Xiao2024DuoAttentionEL,
  title={DuoAttention: Efficient Long-Context LLM Inference with Retrieval and Streaming Heads},
  author={Guangxuan Xiao and Jiaming Tang and Jingwei Zuo and Junxian Guo and Shang Yang and Haotian Tang and Yao Fu and Song Han},
  journal={ArXiv},
  year={2024},
  volume={abs/2410.10819}
}

@inproceedings{Zheng2023JudgingLW,
  title={Judging LLM-as-a-judge with MT-Bench and Chatbot Arena},
  author={Lianmin Zheng and Wei-Lin Chiang and Ying Sheng and Siyuan Zhuang and Zhanghao Wu and Yonghao Zhuang and Zi Lin and Zhuohan Li and Dacheng Li and Eric P. Xing and Haotong Zhang and Joseph E. Gonzalez and Ion Stoica},
  booktitle={NeurIPS},
  year={2023}
}

@inproceedings{Fan2025InvICL,
  title={Rethinking Invariance in In-context Learning},
  author={Lizhe Fang and Yifei Wang, Khashayar Gatmiry and Lei Fang and Yisen Wang},
  booktitle={ICLR},
  year={2025}
}

@article{Dubey2024TheL3,
  title={The Llama 3 Herd of Models},
  author={Abhimanyu Dubey and et al.},
  journal={ArXiv},
  year={2024},
  volume={abs/2407.21783}
}