@article{Barbero2024RoundAR,
  title={Round and Round We Go! What makes Rotary Positional Encodings useful?},
  author={Federico Barbero and Alex Vitvitskyi and Christos Perivolaropoulos and Razvan Pascanu and Petar Velivckovi'c},
  journal={ArXiv},
  year={2024},
  volume={abs/2410.06205}
}

@inproceedings{Fan2025InvICL,
  title={Rethinking Invariance in In-context Learning},
  author={Lizhe Fang and Yifei Wang, Khashayar Gatmiry and Lei Fang and Yisen Wang},
  booktitle={ICLR},
  year={2025}
}

@inproceedings{Gu2024WhenAS,
  title={When Attention Sink Emerges in Language Models: An Empirical View},
  author={Xiangming Gu and Tianyu Pang and Chao Du and Qian Liu and Fengzhuo Zhang and Cunxiao Du and Ye Wang and Min Lin},
  booktitle={ICLR},
  year={2025},
}

@article{Guo2024SerialPE,
  title={Serial Position Effects of Large Language Models},
  author={Xiaobo Guo and Soroush Vosoughi},
  journal={ArXiv},
  year={2024}
}

@inproceedings{Hou2023LargeLM,
  title={Large Language Models are Zero-Shot Rankers for Recommender Systems},
  author={Yupeng Hou and Junjie Zhang and Zihan Lin and Hongyu Lu and Ruobing Xie and Julian McAuley and Wayne Xin Zhao},
  booktitle={ECIR},
  year={2024}
}

@inproceedings{Kazemnejad2023TheIO,
  title={The Impact of Positional Encoding on Length Generalization in Transformers},
  author={Amirhossein Kazemnejad and Inkit Padhi and Karthikeyan Natesan Ramamurthy and Payel Das and Siva Reddy},
  booktitle={NeurIPS},
  year={2023}
}

@inproceedings{Lu2021FantasticallyOP,
  title={Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity},
  author={Yao Lu and Max Bartolo and Alastair Moore and Sebastian Riedel and Pontus Stenetorp},
  booktitle={ACL},
  year={2022}
}

@inproceedings{Min2022RethinkingTR,
  title={Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?},
  author={Sewon Min and Xinxi Lyu and Ari Holtzman and Mikel Artetxe and Mike Lewis and Hannaneh Hajishirzi and Luke Zettlemoyer},
  booktitle={EMNLP},
  year={2022}
}

@article{Wang2024EliminatingPB,
  title={Eliminating Position Bias of Language Models: A Mechanistic Approach},
  author={Ziqi Wang and Hanlin Zhang and Xiner Li and Kuan-Hao Huang and Chi Han and Shuiwang Ji and Sham M. Kakade and Hao Peng and Heng Ji},
  journal={ArXiv},
  year={2024},
  volume={abs/2407.01100}
}

@inproceedings{Wu2023Demystify,
  title={Demystifying Oversmoothing in Attention-Based Graph Neural Networks},
  author={Xinyi Wu and Amir Ajorlou and Zihui Wu and Ali Jadbabaie},
  booktitle={NeurIPS},
  year={2023}
}

@inproceedings{Wu2024OnTR,
  title={On the Role of Attention Masks and LayerNorm in Transformers},
  author={Xinyi Wu and Amir Ajorlou and Yifei Wang and Stefanie Jegelka and Ali Jadbabaie},
  booktitle={NeurIPS},
  year={2024}
}

@inproceedings{Yun2020OnCA,
  title={O(n) Connections are Expressive Enough: Universal Approximability of Sparse Transformers},
  author={Chulhee Yun and Yin-Wen Chang and Srinadh Bhojanapalli and Ankit Singh Rawat and Sashank J. Reddi and Sanjiv Kumar},
  booktitle={NeurIPS},
  year={2020}
}

@article{Zhang2024FoundIT,
  title={Found in the Middle: How Language Models Use Long Contexts Better via Plug-and-Play Positional Encoding},
  author={Zhenyu (Allen) Zhang and Runjin Chen and Shiwei Liu and Zhewei Yao and Olatunji Ruwase and Beidi Chen and Xiaoxia Wu and Zhangyang Wang},
  journal={ArXiv},
  year={2024},
  volume={abs/2403.04797}
}

@inproceedings{Zheng2023JudgingLW,
  title={Judging LLM-as-a-judge with MT-Bench and Chatbot Arena},
  author={Lianmin Zheng and Wei-Lin Chiang and Ying Sheng and Siyuan Zhuang and Zhanghao Wu and Yonghao Zhuang and Zi Lin and Zhuohan Li and Dacheng Li and Eric P. Xing and Haotong Zhang and Joseph E. Gonzalez and Ion Stoica},
  booktitle={NeurIPS},
  year={2023}
}

@article{liu2024lost,
  title={Lost in the middle: How language models use long contexts},
  author={Liu, Nelson F and Lin, Kevin and Hewitt, John and Paranjape, Ashwin and Bevilacqua, Michele and Petroni, Fabio and Liang, Percy},
  journal={Transactions of the Association for Computational Linguistics},
  year={2024}
}

@inproceedings{zhao2021calibrateuseimprovingfewshot,
      title={Calibrate Before Use: Improving Few-Shot Performance of Language Models}, 
      author={Tony Z. Zhao and Eric Wallace and Shi Feng and Dan Klein and Sameer Singh},
      booktitle={ICML},
      year={2021},
     
}

