\section{Related Work}
\paragraph{Position bias in transformers}
Position bias in transformer models has emerged as a critical challenge across diverse applications. In information retrieval and ranking, \citet{liu2024lost, Guo2024SerialPE, Hou2023LargeLM, Zheng2023JudgingLW} demonstrated systematic degradation of performance due to positional dependencies. Similarly, in in-context learning, model performance can vary dramatically based solely on the order of examples \cite{Lu2021FantasticallyOP, Min2022RethinkingTR, zhao2021calibrateuseimprovingfewshot, Fan2025InvICL}. While mitigation strategies such as novel PEs  \cite{Kazemnejad2023TheIO, Zhang2024FoundIT}, alternative masking techniques~\cite{Wang2024EliminatingPB,Fan2025InvICL} and bootstrapping~\cite{Hou2023LargeLM} have been proposed, they remain task-specific and empirically driven. This gap between empirical observations and theoretical understanding highlights the need for a rigorous analysis of how transformers process and integrate positional information through attention.


\paragraph{The effect of attention masks and PEs in transformers} 
The role of attention masks and PEs in transformers has been explored from various perspectives. \citet{Yun2020OnCA} analyzed the function approximation power of transformers under different masking schemes, while \citet{Wu2024OnTR, Wu2023Demystify} investigated the role of attention masks in mitigating rank collapse. Moreover, \citet{Gu2024WhenAS} empirically examined how attention masks affect the emergence of attention sinks. As for PEs, \citet{Kazemnejad2023TheIO} studied their role in length generalization, and \citet{Barbero2024RoundAR} analyzed RoPEâ€™s use of feature dimensions. Additionally, \citet{Wang2024EliminatingPB} empirically observed that both causal masking and RoPE introduce position dependencies in LLMs. Despite these advances, fundamental questions remain about the mechanisms through which attention masks and PEs enable transformers to process and integrate positional information, as well as the nature of the systematic positional biases that emerge as a result.