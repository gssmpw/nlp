[
  {
    "index": 0,
    "papers": [
      {
        "key": "liu2024lost",
        "author": "Liu, Nelson F and Lin, Kevin and Hewitt, John and Paranjape, Ashwin and Bevilacqua, Michele and Petroni, Fabio and Liang, Percy",
        "title": "Lost in the middle: How language models use long contexts"
      },
      {
        "key": "Guo2024SerialPE",
        "author": "Xiaobo Guo and Soroush Vosoughi",
        "title": "Serial Position Effects of Large Language Models"
      },
      {
        "key": "Hou2023LargeLM",
        "author": "Yupeng Hou and Junjie Zhang and Zihan Lin and Hongyu Lu and Ruobing Xie and Julian McAuley and Wayne Xin Zhao",
        "title": "Large Language Models are Zero-Shot Rankers for Recommender Systems"
      },
      {
        "key": "Zheng2023JudgingLW",
        "author": "Lianmin Zheng and Wei-Lin Chiang and Ying Sheng and Siyuan Zhuang and Zhanghao Wu and Yonghao Zhuang and Zi Lin and Zhuohan Li and Dacheng Li and Eric P. Xing and Haotong Zhang and Joseph E. Gonzalez and Ion Stoica",
        "title": "Judging LLM-as-a-judge with MT-Bench and Chatbot Arena"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "Lu2021FantasticallyOP",
        "author": "Yao Lu and Max Bartolo and Alastair Moore and Sebastian Riedel and Pontus Stenetorp",
        "title": "Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity"
      },
      {
        "key": "Min2022RethinkingTR",
        "author": "Sewon Min and Xinxi Lyu and Ari Holtzman and Mikel Artetxe and Mike Lewis and Hannaneh Hajishirzi and Luke Zettlemoyer",
        "title": "Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?"
      },
      {
        "key": "zhao2021calibrateuseimprovingfewshot",
        "author": "Tony Z. Zhao and Eric Wallace and Shi Feng and Dan Klein and Sameer Singh",
        "title": "Calibrate Before Use: Improving Few-Shot Performance of Language Models"
      },
      {
        "key": "Fan2025InvICL",
        "author": "Lizhe Fang and Yifei Wang, Khashayar Gatmiry and Lei Fang and Yisen Wang",
        "title": "Rethinking Invariance in In-context Learning"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "Kazemnejad2023TheIO",
        "author": "Amirhossein Kazemnejad and Inkit Padhi and Karthikeyan Natesan Ramamurthy and Payel Das and Siva Reddy",
        "title": "The Impact of Positional Encoding on Length Generalization in Transformers"
      },
      {
        "key": "Zhang2024FoundIT",
        "author": "Zhenyu (Allen) Zhang and Runjin Chen and Shiwei Liu and Zhewei Yao and Olatunji Ruwase and Beidi Chen and Xiaoxia Wu and Zhangyang Wang",
        "title": "Found in the Middle: How Language Models Use Long Contexts Better via Plug-and-Play Positional Encoding"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "Wang2024EliminatingPB",
        "author": "Ziqi Wang and Hanlin Zhang and Xiner Li and Kuan-Hao Huang and Chi Han and Shuiwang Ji and Sham M. Kakade and Hao Peng and Heng Ji",
        "title": "Eliminating Position Bias of Language Models: A Mechanistic Approach"
      },
      {
        "key": "Fan2025InvICL",
        "author": "Lizhe Fang and Yifei Wang, Khashayar Gatmiry and Lei Fang and Yisen Wang",
        "title": "Rethinking Invariance in In-context Learning"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "Hou2023LargeLM",
        "author": "Yupeng Hou and Junjie Zhang and Zihan Lin and Hongyu Lu and Ruobing Xie and Julian McAuley and Wayne Xin Zhao",
        "title": "Large Language Models are Zero-Shot Rankers for Recommender Systems"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "Yun2020OnCA",
        "author": "Chulhee Yun and Yin-Wen Chang and Srinadh Bhojanapalli and Ankit Singh Rawat and Sashank J. Reddi and Sanjiv Kumar",
        "title": "O(n) Connections are Expressive Enough: Universal Approximability of Sparse Transformers"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "Wu2024OnTR",
        "author": "Xinyi Wu and Amir Ajorlou and Yifei Wang and Stefanie Jegelka and Ali Jadbabaie",
        "title": "On the Role of Attention Masks and LayerNorm in Transformers"
      },
      {
        "key": "Wu2023Demystify",
        "author": "Xinyi Wu and Amir Ajorlou and Zihui Wu and Ali Jadbabaie",
        "title": "Demystifying Oversmoothing in Attention-Based Graph Neural Networks"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "Gu2024WhenAS",
        "author": "Xiangming Gu and Tianyu Pang and Chao Du and Qian Liu and Fengzhuo Zhang and Cunxiao Du and Ye Wang and Min Lin",
        "title": "When Attention Sink Emerges in Language Models: An Empirical View"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "Kazemnejad2023TheIO",
        "author": "Amirhossein Kazemnejad and Inkit Padhi and Karthikeyan Natesan Ramamurthy and Payel Das and Siva Reddy",
        "title": "The Impact of Positional Encoding on Length Generalization in Transformers"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "Barbero2024RoundAR",
        "author": "Federico Barbero and Alex Vitvitskyi and Christos Perivolaropoulos and Razvan Pascanu and Petar Velivckovi'c",
        "title": "Round and Round We Go! What makes Rotary Positional Encodings useful?"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "Wang2024EliminatingPB",
        "author": "Ziqi Wang and Hanlin Zhang and Xiner Li and Kuan-Hao Huang and Chi Han and Shuiwang Ji and Sham M. Kakade and Hao Peng and Heng Ji",
        "title": "Eliminating Position Bias of Language Models: A Mechanistic Approach"
      }
    ]
  }
]