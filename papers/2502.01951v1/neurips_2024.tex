\documentclass{article}


% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
\PassOptionsToPackage{sort,numbers}{natbib}
% before loading neurips_2024




% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
\usepackage[preprint]{neurips_2024}


% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2024}


% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2024}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{systeme}   
\usepackage{amsthm}
\usepackage{enumitem}
\usepackage[thinc]{esdiff}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{bbm}
\usepackage{mathtools}
\usepackage{bm}
\usepackage{enumitem}
\usepackage{tcolorbox}
\usepackage{subfigure}



\usepackage[capitalise]{cleveref}





\renewenvironment{quote}
  {\list{}{\leftmargin=1em \rightmargin=1em}\item\relax}
  {\endlist}


\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}

% if you use cleveref..


\newcommand{\cA}{\mathcal{A}}
\newcommand{\cG}{\mathcal{G}}
\newcommand{\cV}{\mathcal{V}}
\newcommand{\cD}{\mathcal{D}}
\newcommand{\cM}{\mathcal{M}}
\newcommand{\cN}{\mathcal{N}}
\newcommand{\cH}{\mathcal{H}}
\newcommand{\cC}{\mathcal{C}}
\newcommand{\bN}{\mathbb{N}}
\newcommand{\bR}{\mathbb{R}}
\newcommand{\bS}{\mathbb{S}}
\newcommand{\bP}{\mathbb{P}}

\newcommand{\1}{\mathbf{1}}
\newcommand{\0}{\mathbf{0}}






\newcommand{\norm}[1]{ \left\| #1 \right\| }
\newcommand{\softmax}{\mathrm{softmax}}
\DeclareMathOperator{\Conv}{Conv}
\DeclareMathOperator{\LN}{LN}
\DeclareMathOperator{\Rank}{Rank}
\DeclareMathOperator{\SRank}{SRank}

\DeclareMathOperator{\dist}{dist}
\DeclareMathOperator{\diag}{diag}
\DeclareMathOperator{\ew}{\leq_{ew}}
\DeclareMathOperator{\res}{res}
\DeclareMathOperator{\Tr}{Tr}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\decay}{decay}
\DeclareMathOperator{\RoPE}{RoPE}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\newtheorem{remark}[theorem]{Remark}



\title{On the Emergence of Position Bias in Transformers}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.


\author{Xinyi Wu$^{1}$\qquad Yifei Wang$^2$ \qquad Stefanie Jegelka$^{3,2}$\qquad Ali Jadbabaie$^{1}$\\  
$^1$MIT IDSS \& LIDS \qquad $^2$MIT CSAIL \qquad $^3$TU Munich\\
\texttt{\{xinyiwu,yifei\_w,stefje,jadbabai\}@mit.edu}}



\begin{document}


\maketitle


\begin{abstract}
Recent studies have revealed various manifestations of position bias in transformer architectures, from the ``lost-in-the-middle" phenomenon to attention sinks, yet a comprehensive theoretical understanding of how attention masks and positional encodings shape these biases remains elusive. This paper introduces a novel graph-theoretic framework to analyze position bias in multi-layer attention.  Modeling attention masks as directed graphs, we quantify how tokens interact with contextual information based on their sequential positions. We uncover two key insights: First, causal masking inherently biases attention toward earlier positions, as tokens in deeper layers attend to increasingly more contextualized representations of earlier tokens. Second, we characterize the competing effects of the causal mask and relative positional encodings, such as the decay mask and rotary positional encoding (RoPE): while both mechanisms introduce distance-based decay within individual attention maps, their aggregate effect across multiple attention layers -- coupled with the causal mask -- leads to a trade-off between the long-term decay effects and the cumulative importance of early sequence positions. 
Through controlled numerical experiments, we not only validate our theoretical findings but also reproduce position biases observed in real-world LLMs. Our framework offers a principled foundation for understanding positional biases in transformers, shedding light on the complex interplay of attention mechanism components and guiding more informed architectural design.
\end{abstract}

\section{Introduction}

The attention mechanism is central to transformer architectures~\cite{Vaswani2017AttentionIA}, which form the backbone of state-of-the-art foundation models, including large language models (LLMs). Its success lies in its ability to dynamically weigh input elements based on their relevance, enabling efficient handling of complex dependencies~\cite{Kim2017StructuredAN, Bahdanau2014NeuralMT}. However, despite this widespread success, many questions remain unanswered regarding how these mechanisms process information and the artifacts they may introduce. Developing a deeper theoretical understanding of their inner workings is essential -- not only to better interpret existing models but also to guide the design of more robust and powerful architectures.


One particularly intriguing aspect that demands such a theoretical investigation is \emph{position bias}, i.e., the bias of the model to focus on certain regions of the input, which significantly impacts the performance and reliability of transformers and LLMs~\cite{Zheng2023JudgingLW, Wang2024EliminatingPB,Hou2023LargeLM}. For instance, these models often suffer from the ``lost-in-the-middle" problem, where retrieval accuracy significantly degrades for information positioned in the middle of the input sequence compared to information at the beginning or end~\cite{liu2024lost, Zhang2024FoundIT,Guo2024SerialPE}. Similarly, in-context learning is highly sensitive to the order of illustrative examples: simply shuffling independently and identically distributed (i.i.d.) examples can lead to significant performance degradation~\cite{Min2022RethinkingTR, Lu2021FantasticallyOP, zhao2021calibrateuseimprovingfewshot}. Moreover, recent research has also revealed that attention sinks~\cite{Xiao2023EfficientSL, Gu2024WhenAS, guo2024activedormantattentionheadsmechanistically} -- positions that attract disproportionately high attention weights -- arise at certain positions regardless of semantic relevance, suggesting an inherent positional bias.




These empirical findings suggest that while transformers effectively encode and process positional information through the combined use of attention masks and positional encodings (PEs)~\cite{Wang2024EliminatingPB,Fan2025InvICL}, these design elements also appear to introduce systematic positional biases, often independent of semantic content. This raises a fundamental and intriguing question about the role of positional information in attention mechanisms:
\begin{quote}
\hspace{-3ex}\textit{How do attention masks and positional encodings shape position bias in transformers?}
\end{quote}



% Understanding of these position-dependent effects is essential for improving model reliability, interpretability, and performance.
% Although attention masks and positional encodings (PEs) were initially designed to enable transformers to process sequential information effectively, growing evidence reveals that these mechanisms can also lead to unintended artifacts in model behavior. As transformer architectures continue to set the standard in state-of-the-art language modeling, a deeper understanding of these position-dependent effects is essential for improving model reliability, interpretability, and performance.

\begin{table*}[t] 
    \centering
       \caption{Summary of our results and their connections to empirical observations on position bias reported in the literature. }   
       \vspace{1ex}
     \resizebox{\textwidth}{!}{
    \begin{tabular}{cc}
    \toprule
        Empirical Observations on Position Bias & Our Results \\
         \midrule
         Positional information induced by the causal mask~{\cite{Kazemnejad2023TheIO, Wang2024EliminatingPB, Barbero2024TransformersNG}} & Theorem~\ref{thm: causal_mask}, Section~\ref{exp:causal}\\
         Decay effects induced by relative PEs~\cite{su2023roformerenhancedtransformerrotary} & Lemma~\ref{lem:decay_mask_attn}-\ref{lem: attn_rope}, Section~\ref{sec:depth}\\
         Interplay between the causal mask and relative PEs~\cite{Wang2024EliminatingPB} & Theorem~\ref{thm:aggregate_decay_effect}-\ref{thm: rope}, Section~\ref{sec:depth}\\
         Attention sinks~\cite{Xiao2023EfficientSL,Gu2024WhenAS} & Theorem~\ref{thm: causal_mask}-\ref{thm:prefix_mask}, Appendix~\ref{app:attn_sinks}\\
         The ``lost-in-the-middle" phenomenon~\cite{liu2024lost} & Section~\ref{exp:causal}\\
    \bottomrule
    \end{tabular}
    }
    \label{tab:summary}
\end{table*}





To address the question, we propose a novel graph-theoretic framework for analyzing attention score distributions in multi-layer attention settings. Building upon~\citet{Wu2024OnTR}, we model attention masks as directed graphs, enabling rigorous mathematical analysis of attention patterns. This approach proves particularly powerful for studying multi-layer attention mechanisms, as it allows us to precisely quantify how each token's contextual representation is composed from information at different positions in the sequence. By tracking the information flow through the attention layers, we can systematically examine how positional biases emerge and propagate across layers, providing insights into the complex interplay between attention masks, PEs, and the network's depth.

\textbf{Our contributions are summarized as follows:}
\begin{itemize}[leftmargin = 3ex]
    \item We develop a graph-theoretic framework that unifies and advances understanding of position bias in transformers, offering deeper insights into diverse empirical observations documented in the literature (\Cref{tab:summary}). 
    \vspace{-1ex}
    \item We show that causal masking in transformers inherently biases attention toward earlier positions in deep networks. This happens as tokens in deeper layers attend to increasingly more contextualized representations of earlier tokens, thereby amplifying the influence of initial positions. We derive analogous results for the sliding-window mask and the prefix mask, highlighting the generalizability of our framework.
    \vspace{-1ex}
    \item We uncover a nuanced interaction between causal masking and relative PEs, such as decay masks and rotary positional encoding (RoPE). Our findings highlight a trade-off in multi-layer attention networks, where local decay effects within individual layers are counterbalanced by the cumulative importance of early sequence positions. These results provide a deeper understanding of how PE and masking interact in deep attention-based architectures, with design implications about how to balance local and global context.
    \vspace{-1ex}
    \item We support our theoretical findings with experiments, empirically validating that deeper attention layers amplify the bias toward earlier parts of the sequence, while relative PEs partially mitigate this effect. Through carefully controlled numerical experiments, we further investigate the role of data in shaping position bias and how causal masking implicitly leverages positional information.
\end{itemize}

\section{Related Work}



\paragraph{Position bias in transformers}
Position bias in transformer models has emerged as a critical challenge across diverse applications. In information retrieval and ranking, \citet{liu2024lost, Guo2024SerialPE, Hou2023LargeLM, Zheng2023JudgingLW} demonstrated systematic degradation of performance due to positional dependencies. Similarly, in in-context learning, model performance can vary dramatically based solely on the order of examples \cite{Lu2021FantasticallyOP, Min2022RethinkingTR, zhao2021calibrateuseimprovingfewshot, Fan2025InvICL}. While mitigation strategies such as novel PEs  \cite{Kazemnejad2023TheIO, Zhang2024FoundIT}, alternative masking techniques~\cite{Wang2024EliminatingPB,Fan2025InvICL} and bootstrapping~\cite{Hou2023LargeLM} have been proposed, they remain task-specific and empirically driven. This gap between empirical observations and theoretical understanding highlights the need for a rigorous analysis of how transformers process and integrate positional information through attention.


\paragraph{The effect of attention masks and PEs in transformers} 
The role of attention masks and PEs in transformers has been explored from various perspectives. \citet{Yun2020OnCA} analyzed the function approximation power of transformers under different masking schemes, while \citet{Wu2024OnTR, Wu2023Demystify} investigated the role of attention masks in mitigating rank collapse. Moreover, \citet{Gu2024WhenAS} empirically examined how attention masks affect the emergence of attention sinks. As for PEs, \citet{Kazemnejad2023TheIO} studied their role in length generalization, and \citet{Barbero2024RoundAR} analyzed RoPE’s use of feature dimensions. Additionally, \citet{Wang2024EliminatingPB} empirically observed that both causal masking and RoPE introduce position dependencies in LLMs. Despite these advances, fundamental questions remain about the mechanisms through which attention masks and PEs enable transformers to process and integrate positional information, as well as the nature of the systematic positional biases that emerge as a result.

\section{Problem Setup}

\paragraph{Notation}

 We use the shorthand $[n]:=\{1,\ldots,n\}$. For a matrix \( M \), we denote its \( i \)-th row by \( M_{i,:} \) and its \( j \)-th column by \( M_{:,j} \). Throughout the analysis in the paper, we formalize the attention mask to be a directed graph $\cG$. Formally, we represent a directed graph with $N$ nodes by $\mathcal{G}$ and let $E(\cG)$ be the set of directed edges of $\cG$. A directed edge $(j,i)\in E(\cG)$  from node $j$ to $i$ in $\mathcal{G}$ means that in the attention mechanism, token $j$ serves as a direct context for token $i$ or token $i$ attends to token $j$. The set $\cN_i$ of all neighbors of node $i$ is then $\{k: (k,i)\in E(\cG)\}$.  We say a node $v$ is \emph{reachable} from node $u$ in a directed graph $\cG$ if there is a directed path $(u, n_1), (n_1, n_2), ..., (n_k,v )$ from $u$ to $v$. In the attention mechanism, this means that token $u$ serves as a direct or indirect context for token $v$.

Furthermore, we will be using the following graph-theoretic terminology (see~\Cref{fig:masks} for a schematic illustration):
\begin{definition} [Center Node]
    A node $v$ from which every node in the directed graph $\cG$ is reachable is called a \emph{center node}.
    \label{def:center_nodes}
\end{definition}

\subsection{(Masked) Attention Mechanism}
Given the representation $X\in\bR^{N\times d}$ of $N$ tokens, the raw attention score matrix is computed as 
\[Z = XW_Q (XW_K)^\top/\sqrt{d_{QK}}\,,\]
where $W_Q, W_K\in\bR^{d\times d'}$ are the query and the key matrix, respectively, and $\sqrt{d_{QK}}$ is a temperature term to control the scale of raw attention scores. Without loss of generality, we assume $d_{QK}=1$ in our analysis. To enforce a masked attention, we create a sparse attention matrix $A \in \bR^{N \times N}$ based on $Z$ whose sparsity pattern is specified by a directed graph $\cG$: we normalize $Z_{ij}$ among all allowed token attention interactions $(k,i) \in E(\cG)$ such that if $(j,i) \in E(\cG)$,
\[A_{ij} = {\softmax}_\cG (Z_{ij}) = \frac{\exp(Z_{ij})}{\sum_{k\in\cN_i}\exp(Z_{ik})}\, \;\,,\] and $A_{ij} = 0$ otherwise. 
% $A_{ij}$ relates to edge $(j,i)$ or $(i,j)$? It makes sense, just checking, all good as long as it's consistent. Check with Figure 1. 
% \sj{2nd: below $A$ is the full attention matrix. We could also have a binary masking matrix and then Hadamard product with the full attention $A$}

\subsection{Attention Update}
For our analysis, we consider %the following definition of 
single-head (masked) self-attention networks (SANs). The layerwise update rule can be written as
\vspace{-1ex}
\begin{equation*}
    A^{(t)} = \softmax_{\cG^{(t)}}\left( X^{(t)}W^{(t)}_Q (X^{(t)}W^{(t)}_K)^\top/\sqrt{d_{QK}} \right)  
\end{equation*}
\vspace{-2ex}
\begin{equation}
     X^{(t+1)} = A^{(t)}X^{(t)}W_V^{(t)}\,,\label{eq: update_no_LN}
\end{equation}
where $W_V^{(t)}\in\bR^{d\times d'}$ is the value matrix. For simplicity, throughout the paper, we assume that $d=d'$
and $\cG^{(t)}=\cG$. Yet the results can be
easily generalized to the case where masks are time-varying and satisfy regularity conditions.

\subsection{Relative Positional Encoding}
\paragraph{Decay Mask}
The decay mask represents the relative distance between two tokens by introducing an explicit bias favoring more recent tokens. Formally, it can be written as:
\begin{equation*}
     D_{ij} = 
    \begin{cases}
        {-(i-j)m} & \text{if } j \leq i \\
        0 & \text{otherwise}\,.
    \end{cases}
\end{equation*}
Then applying the decay mask is essentially 
\begin{equation}
   A_{\decay}^{(t)} = \softmax_{\cG}(X^{(t)}W_Q^{(t)}(X^{(t)}W_K^{(t)})^\top + D) \,.
   \label{eq:decay_mask_update}
\end{equation}
Note that while the decay mask formulation follows ALiBi~\cite{Press2021TrainST}, it can be generalized to more complex variants such as KERPLE~\cite{Chi2022KERPLEKR}.

\paragraph{Rotary Positional Encoding (RoPE)}

Another way to encode the relative positional information is through RoPE~\cite{su2023roformerenhancedtransformerrotary}, which applies a rotation to  query and key embeddings by an angle proportional to
the token’s position index within the sequence. Formally, the rotation operation applied to each query or key $X_{i,:}W_{\{Q,K\}}$ can be written as 
\begin{equation}
    (\hat{X}_{\{Q,K\}})_{i,:} = X_{i,:}W_{\{Q,K\}}R^d_{\Theta, i}
    \label{eq:rope_to_qk}
\end{equation}
where
\begin{equation*}
\begin{aligned}
    R^d_{\Theta, i} = &
    \resizebox{0.75\columnwidth}{!}{$
    \begin{bmatrix}
        \cos i\theta_1  & \sin i \theta_1  & 0 & 0 & \cdots & 0 & 0 \\
        -\sin i  \theta_1 & \cos i \theta_1  & 0 & 0 & \cdots & 0 & 0 \\
        0 & 0 & \cos i \theta_2 & \sin i \theta_2 & \cdots & 0 & 0 \\
        0 & 0 & -\sin i \theta_2 & \cos i \theta_2 & \cdots & 0 & 0 \\
        \vdots & \vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
        0 & 0 & & 0 & 0 & \cos i \theta_{d/2} & \sin i \theta_{d/2} \\
        0 & 0 & & 0 & 0 & -\sin i \theta_{d/2} & \cos i \theta_{d/2}
   \end{bmatrix}
    $}
\end{aligned}
\end{equation*}
is the rotation matrix with a set of pre-defined base rotational angles $\Theta = \{0\leq\theta_1\leq \cdots \leq \theta_{d/2}\}$. Then the raw attention
scores under RoPE $Z_{\RoPE}$ become
\begin{align*}
    (Z_{\RoPE})_{ij} & = (X_{i,:}W_{Q}R^d_{\theta, i})(X_{j,:}W_{K}R^d_{\theta, j})^\top\\
    & = X_{i,:}W_{Q}R^d_{\theta, i-j}W_{K}^\top X_{j,:}^\top,
\end{align*}
which distorts the original raw attention scores based on the relative token distances. The final attention scores under RoPE are calculated as $
    A^{(t)}_{\RoPE} = \softmax_{\cG}\left(Z_{\RoPE}^{(t)} \right)$.


\section{Main Results}

% \subsection{The Contextualization Effect of Attention }

In the transformer model, the attention mechanism is the sole module that allows tokens to interact with one another and incorporate contextual information from the sequence. It iteratively refines the contextual representation of each token across layers, allowing information to flow and accumulate based on relevance. This concept of contextualization through attention has its origins in the development of attention mechanisms, which predate transformers~\cite{Kim2017StructuredAN, Bahdanau2014NeuralMT}. From the perspective of contextualization, the attention mechanism can be expressed in the following form~\cite{Kim2017StructuredAN}:
\begin{align}
    X_{i,:}^{(t+1)} 
    & = \sum_{j=1}^N 
        \underbrace{(A^{(t)} \cdots A^{(0)})_{ij}}_{\mathclap{ \bP^{(t)}(z_i = j \mid X^{(0)})}} 
        \,\cdot\, 
        \underbrace{X^{(0)}_{j,:}W_V^{(0)} \cdots W_V^{(t)}}_{\mathclap{f^{(t)}(X^{(0)}_{z_i,:})}},
        \vspace{-3ex}
\end{align}
where \(z_i\) is a categorical latent variable with a sample space \(\{1, \ldots, N\}\) that selects the input \(X_{j,:}\) to provide context for token \(i\). In this formulation, \(A^{(t)}\) represents the attention matrix at layer \(t\), \(\bP^{(t)}(z_i = j \mid X^{(0)})\) denotes the cumulative probability of selecting input token \(j\) as the context for token \(i\) at depth $t$ , and \(f^{(t)}(\cdot)\) is a learned transformation function.

This probabilistic formulation reveals two key aspects of the attention mechanism: it acts as both a context selector and a feature aggregator. As a selector, it assigns probabilities $\bP^{(t)}$ that quantify the relevance of each token $j$ to target token $i$ at depth $t$. As an aggregator, it combines these selected contexts weighted by their respective probabilities $\bP^{(t)}$ to form the contextualized representation $X^{(t)}$.  Since position bias fundamentally manifests as systematic preferences in how tokens select and incorporate context from different positions, analyzing the attention mechanism's behavior is crucial for understanding these biases. By examining how attention masks and PEs affect the probability distribution $\bP^{(t)}$, we can investigate how position-dependent patterns emerge and propagate through multi-layer attention in transformers.

Finally, we adopt the following assumptions in our analysis:
\begin{enumerate}
    \item [\textbf{A1}] There exists  $C\in\bR$ such that  
    $\underset{t\in\bN}{\max}\big\{\|W_Q^{(t)}\|_2, \|W_K^{(t)}\|_2\big\} \leq C$.
    \item [\textbf{A2}] The sequence $\big\{\|\prod_{t=0}^k W_V^{(t)}\|_2\big\}_{k=0}^\infty$ is bounded.
\end{enumerate}
\textbf{A1} assumes that the key and query weight matrices are bounded, which is crucial for efficient attention computation in practice~\cite{Alman2023FastAR}, whereas \textbf{A2} is to ensure boundedness of the node representations' trajectories $X^{(t)}$ for all $t\geq 0$~\cite{Wu2024OnTR}. 

\begin{figure}
    \centering   \includegraphics[width=0.6\linewidth]{figs/icml-25-mask.pdf}
    \caption{\small{Three types of attention masks and their corresponding directed graphs $\cG$ used in the analysis (self-loops are omitted for clarity). A directed edge from token $j$ to $i$ indicates that $i$ attends to $j$. The center node(s) (\Cref{def:center_nodes}), highlighted in yellow, represent tokens that can be directly or indirectly attended to by all other tokens in the sequence. As depicted in the top row, the graph-theoretic formulation captures both direct and indirect contributions of tokens to the overall context, providing a comprehensive view of the token interactions under multi-layer attention.}
 }
 \vspace{-3ex}
    \label{fig:masks}
\end{figure}

\subsection{Attention Masks: A Graph-Theoretic View}


We first analyze the case without PEs, focusing on the effect of attention masks. A graph-theoretic perspective offers a powerful framework for analyze multi-layer attention: the flow of attention across tokens can be represented as paths in a directed graph defined by the mask, where each path captures how information is transmitted between tokens (see~\Cref{fig:masks} for an illustration). The number of steps in a path corresponds to the number of layers. By accounting for all such paths, we can quantify the cumulative influence of each token in the context computation of other tokens. 

Through the graph-theoretic view, our first result states that for a causal mask $\cG$, as tokens in deeper layers attend to increasingly more contextualized representations of earlier tokens, the context of each token converges exponentially toward the first token in the sequence.


\begin{theorem}
Let $\cG$ be the causal mask. Under $\textup{\textbf{A1}}$-$\textup{\textbf{A2}}$, 
    given $X^{(0)}\in\bR^{N\times d}$, for every token $i\in[N]$,
    \begin{equation*}
        \lim_{t\to\infty} \bP^{(t)}(z_i = 1|X^{(0)}) = 1\,.
    \end{equation*}
Moreover, there exist $0< C,\epsilon < 1$ where $N\epsilon < 1$ such that 
\begin{equation*}
        \bP^{(t)}(z_i = j|X^{(0)}) \leq  C(1-(j-1)\epsilon)^{t}\,.
    \end{equation*}
for all $1< j \leq i$ and $t\geq 0$.
\label{thm: causal_mask}
\end{theorem}

\Cref{thm: causal_mask} reveals that in multi-layer causal attention, positional bias toward earlier sequence positions intensifies with depth -- regardless of semantic content. This phenomenon arises from the nature of multi-layer attention: starting from the second layer, tokens no longer attend to raw inputs but instead to contextualized tokens, i.e., representations transformed by prior attention layers. Combined with the sequential structure of the causal mask, this iterative process amplifies the role of earlier tokens, as they influence later ones not only as direct context but also indirectly through intermediate tokens along the path. We discuss a few intriguing implications below. 

\paragraph{The role of softmax} 
      The key property that leads to the above result is that the softmax operation in the attention mechanism cannot fundamentally disconnect any directed edge in the graph $\cG$. As a result, center nodes (\cref{def:center_nodes}), which appear in the context directly or indirectly for all tokens in the sequence, will eventually gain a dominant role in the context as their direct and indirect contributions propagate through the graph. Empirically, \citet{Xiao2023EfficientSL, Gu2024WhenAS} found that changing softmax to ReLU, which can disconnect edges in the graph, indeed mitigates the emergence of attention sinks. 
\vspace{-1ex}
\paragraph{How No PE induces positional information} Previous works have observed that the causal mask alone amplifies the position bias~\cite{Yu2024MitigatePB,Wang2024EliminatingPB}. Despite these observations, it remains insufficiently understood how the causal mask captures positional information and what information is being captured. One hypothesis in~\citet{Kazemnejad2023TheIO} suggests that the causal mask may be simulating either an absolute PE or a relative PE with specific weight matrices.

\Cref{thm: causal_mask} offers a different perspective. The causal mask results in earlier tokens being utilized more frequently during computation, inducing a sequential order.
%
%inherently imposes a sequential order on tokens~\cite{Wu2024OnTR}, resulting in earlier tokens being utilized more frequently during computation. 
This %design choice introduces a 
bias aligns with the token order in the sequence. To validate this perspective, we present additional experimental results in \cref{exp:causal}, providing empirical evidence that the causal mask is not simulating any PE but instead just exhibits a bias toward the earlier parts of a sequence.


\paragraph{Trade-off between representation power and position bias} \Cref{thm: causal_mask} also highlights a trade-off between representational power and positional bias as the depth of attention layers increases. While numerous studies have demonstrated that deeper attention models are crucial for improving representation power~\cite{Yun2019AreTU, Merrill2022ThePT, Li2024ChainOT,Sanford2024OnelayerTF}, our findings reveal that these benefits come at a cost. As the model depth increases, the initial tokens in a sequence are utilized more frequently, amplifying the positional bias toward the beginning of the sequence. This trade-off underscores the importance of carefully balancing depth and positional bias in the design of attention-based architectures. 



% \subsection{Attention Masks: Sliding-Window and Prefix}

\Cref{thm: causal_mask} on the causal mask can be generalized to encompass other types of attention masks, notably the sliding-window mask~\cite{Jiang2023Mistral7, Beltagy2020LongformerTL} and the prefix mask~\cite{2020t5, Lewis2019BARTDS}. In the sliding-window mask, each token is allowed to attend to a fixed number of preceding tokens. Let $w$
denote the width of the sliding-window, representing the maximal number of tokens each token can access. The following result shows how limiting the context window size affects the propagation of contextual information in attention mechanism.
\begin{theorem}
Let $\cG$ be the sliding-window mask with width $w\geq 2$. Under $\textup{\textbf{A1}}$-$\textup{\textbf{A2}}$,  
    given $X^{(0)}\in\bR^{N\times d}$, for every token $i\in[N]$,
    \begin{equation*}\lim_{t\to\infty} \bP^{(t)}(z_i = 1|X^{(0)}) = 1\,.
    \end{equation*}
Moreover, there exist $0<C, \epsilon<1$ where $N\epsilon^{\left\lceil \frac{N-1}{w-1} \right\rceil} < 1$ such that 
\begin{equation*}
        \bP^{(t)}(z_i = j|X^{(0)}) \leq  C(1-(j-1)\epsilon^{\left\lceil \frac{N-1}{w-1} \right\rceil} )^{t/\left(2\left\lceil \frac{N-1}{w-1} \right\rceil\right)}\,.
    \end{equation*}
for all $1< j \leq i$ and $t\geq 0$.  
\label{thm:sliding-window}
\end{theorem}
The above result suggests that a smaller window size $w$ helps mitigate the model's bias toward early tokens in the sequence. However, such a moderating effect has its limit -- the contextual information will still exponentially converge toward the first token over successive layers, though at a rate determined by the ratio between the sequence length $N$ and the window size $w$.


Finally, for the case of a prefix mask, where the first $K$ tokens in the sequence serve as a prefix and all subsequent tokens attend to them, contextual information exponentially converges toward these $K$ tokens rather than being dominated by just the first one, with each of these $K$ tokens having a non-trivial influence.

\begin{theorem}
Let $\cG$ be the prefix mask with the first $K$ tokens being the prefix tokens. Under $\textup{\textbf{A1}}$-$\textup{\textbf{A2}}$, 
    given $X^{(0)}\in\bR^{N\times d}$, for every token $i\in[N]$, 
    \begin{equation*}\lim_{t\to\infty} \bP^{(t)}(z_i \in [K] |X^{(0)}) = 1\,,
        \label{eq: first_K_tokens_total}
    \end{equation*}
and there exists $\kappa > 0$ such that
 \begin{equation*} 
        \liminf_{t\to\infty} \bP^{(t)}(z_i = k|X^{(0)}) \geq  \kappa. \qquad\forall k\in [K]\,.
        \label{eq: first_k_distribution}
    \end{equation*}
Moreover, there exist $0<C,\epsilon < 1$ where $N\epsilon < 1$ such that
\begin{equation*} 
        \bP^{(t)}(z_i = j|X^{(0)}) \leq  C(1-(j-K)\epsilon)^t\,.
         \label{eq: prefix_exp}
    \end{equation*}
for all $K< j \leq i$ and $t\geq 0$.
\label{thm:prefix_mask}
\end{theorem}

\paragraph{Attention sink and center node} The above result connects the emergence of attention sinks to the structural role of center nodes in the graph $\cG$ defined by the mask. Specifically, in~\citet{Gu2024WhenAS}, the authors observed two interesting phenomena: 1) when using the sliding-window mask, attention sinks still appear on the absolute first token in the sequence, but not on the first token within each context window; 2) when using the prefix mask, attention sinks emerge on all prefix tokens, rather than just on the first token.

These empirical results align well with Theorems~\ref{thm:sliding-window} and \ref{thm:prefix_mask}. Our results suggest that the absolute first token and the prefix tokens act as center nodes for the sliding-window mask and prefix mask, respectively. The context for each token, after multi-layer attention, exponentially converges to these center nodes. This connection between attention sinks and center nodes suggests that attention sinks are not arbitrary artifacts but arise naturally from the underlying graph structure induced by the attention mask.

\subsection{Relative PEs: A Competing Decay Effect}

Having analyzed how attention masks bias the model toward the beginning of the sequence, we now shift our focus to studying PEs, the other key mechanism for representing positional information in transformers. 

Relative PE, as the name suggests, incorporates positional information by modifying the original attention scores in a way that reflects the relative positions of tokens. Among these, the decay mask~\cite{Press2021TrainST} explicitly introduces a distance-based decay effect into the attention mechanism. We begin by examining the effect of the decay mask on individual attention layers.

\begin{lemma}
    Consider the decay mask in~(\ref{eq:decay_mask_update}) where $\cG$ is causal. Under $\textup{\textbf{A1}}$-$\textup{\textbf{A2}}$, given $X^{(0)}\in\bR^{N\times d}$, there exists $C_{\max}, C_{\min} > 0$ such that for all $t\geq 0$,
    \[C_{\min} e^{-(i-j)m} \leq (A^{(t)}_{\decay})_{ij} \leq C_{\max} e^{-(i-j)m}.\]
    \vspace{-3ex}
\label{lem:decay_mask_attn}
\end{lemma}
\Cref{lem:decay_mask_attn} demonstrates that the decay mask introduces an exponential decay effect into each attention map, with the strength of the effect determined by the token distances. However, while this result characterizes the behavior of individual attention layers, the interaction between layers in a multi-layer setting leads to more intricate behaviors. Building on~\Cref{lem:decay_mask_attn} , \Cref{thm:aggregate_decay_effect} examines the cumulative effect of the decay mask across multiple layers when combined with the causal mask. 

\begin{theorem}
Consider the decay mask in~(\ref{eq:decay_mask_update}) where $\cG$ is causal. Fix $T \geq 0$. Under $\textup{\textbf{A1}}$-$\textup{\textbf{A2}}$, given $X^{(0)}\in\bR^{N\times d}$, it holds for all $t\leq T$, 
\[\bP_{_{\decay}}^{(t)}(z_i = j|X^{(0)})= \Theta\left({t+i-j \choose i-j} e^{-(i-j)m} \right)\,.\]
\label{thm:aggregate_decay_effect}
\vspace{-3ex}
\end{theorem}
Notably, if we denote\[L(x) = \log \left({t+x  \choose x} e^{-xm} \right)\,,\] then $L(x)$ is not a monotone function of the distance $x$ between two tokens. More precisely, under Stirling's approximation, the critical point, where the highest attention score occurs, is at $x^* = t/(e^m-1)\,.$ This means that increasing the decay strength $m$ decreases $x^*$, making the model more biased towards recent tokens,   whereas increasing the number of attention layers increases $x^*$, making the model more biased towards initial tokens.  

Compared to~\cref{lem:decay_mask_attn}, while the decay mask imposes a stronger decay effect on earlier tokens within individual attention layers, these tokens gain more cumulative importance across multiple layers. This trade-off between layer-wise decay and cross-layer accumulation transforms the initially monotonic decay pattern within each attention map into a more intricate, non-monotonic behavior when aggregated throughout the network.


\subsection{A Closer Look at RoPE}
Having analyzed the effect of the decay mask, which directly incorporates a distance-based decay into the attention score calculation, we now turn our attention to another popular form of relative positional encoding: RoPE~\cite{su2023roformerenhancedtransformerrotary}.

RoPE’s inherent complexity has made a clear theoretical understanding challenging. However, recent empirical observations in~\citet{Barbero2024RoundAR} suggest that in practice, LLMs tend to predominantly utilize feature dimensions that rotate slowly. This phenomenon introduces additional structure, enabling a more refined analysis of RoPE’s effects by focusing on these slowly rotating feature dimensions. For simplicity and without loss of generality, we consider the case where only the slowest-rotating feature dimensions with base rotational angle $\theta_1$ are used by the model, i.e. effectively reducing the embedding dimension to $d=2$. See~\cref{app:rope_d_geq_2} for more results on the general case $d\geq 2$.


Recall from \eqref{eq:rope_to_qk} that RoPE operates by rotating the original query and key embeddings by an angle proportional to the token's position index within the sequence. Similar to the decay mask, which incorporates distance-based decay into attention scores, RoPE adjusts raw attention scores via these rotations. To formalize this relationship mathematically, we define the original angle between query $q_i^{(t)} \vcentcolon= X^{(t)}_{i,:}W^{(t)}_{Q}$ and key $k_j^{(t)} \vcentcolon=X^{(t)}_{j,:}W^{(t)}_{K}$ as $\phi^{(t)}_{i,j}$. Then the following result analyzes how RoPE's position-dependent rotations systematically modify the computation of attention scores.

\begin{lemma}
    Let $\cG$ be the causal mask and $d=2$. Suppose for $t\geq 0$, $\|q_i^{(t)}\|_2, \|k_j^{(t)}\|_2 > 0$, and $|\phi^{(t)}_{i,j}| \leq \delta\theta_1$, where $\delta > 0$ and $(\delta+N-1)\theta_1 \leq \pi$. Then under $\textup{\textbf{A1}}$-$\textup{\textbf{A2}}$, given $X^{(0)}\in\bR^{N\times d}$, there exists $C_{\max}, C_{\min,} c, c' > 0$ such that
    \[C_{\min} e^{-c(i-j)^2\theta_1^2} \leq (A^{(t)}_{\RoPE})_{ij} \leq C_{\max} e^{-c'(i-j)^2\theta_1^2}\,.\]
    \label{lem: attn_rope}
    \vspace{-3ex}
\end{lemma}
The result shows that by solely leveraging feature dimensions that rotate slowly, RoPE effectively induces a distance-based decay effect, which aligns with the intuition in~\citet{su2023roformerenhancedtransformerrotary}. However, it is worth noting that the decay effect induced by RoPE is significantly smaller compared to that of the decay mask (\cref{lem:decay_mask_attn}). This is because the base rotational angle $\theta_1$ is typically chosen to be small, i.e. $\approx1/10000$ per token~\cite{su2023roformerenhancedtransformerrotary, Dubey2024TheL3}, resulting in a more gradual decay.

However, similar to the case of the decay mask, when considering the effect of RoPE across multiple layers of attention, the long-term decay effects within individual layers are counteracted by the increasing influence of earlier tokens given by the causal mask.

\begin{theorem}
   Fix $T> 0$. Under the same conditions as in~\cref{lem: attn_rope} for $t \leq T$, given $X^{(0)}\in\bR^{N\times d}$, there exists $c>0$ such that for all $t\leq T$, 
   \[\bP_{_{\RoPE}}^{(t)}(z_i = j|X^{(0)}) = \Theta\left({t+i-j  \choose i-j} e^{-c(i-j)^2\theta_1^2} \right)\,.\]
   \label{thm: rope}
   \vspace{-2ex}
\end{theorem}

Again, if we write\[L(x) = \log \left({t+i-j  \choose i-j} e^{-x^2\theta_1^2} \right)\,,\]
then, by implicit differentiation, the critical point $x^*$ is an increasing function of the depth $t$ and a decreasing function of the base rotational angle $\theta_1$ (see~\cref{app:implicit differentiation}). This implies that increasing the base rotational angle $\theta_1$ reduces the optimal distance $x^*$, amplifying the long-term decay effect and causing tokens to focus more on nearby tokens. In contrast, increasing the number of attention layers $t$ increases $x^*$ and hence deeper models become more biased toward initial tokens.


\section{Experiments}\label{sec:exp}
In this section, we validate our theoretical findings via carefully designed numerical experiments. To ensure a controlled setup that enables precise manipulation of positional biases in the data, we adopt the synthetic data-generating process and simplified self-attention network framework proposed in~\citet{Reddy2023TheMB}. This setup allows us to systematically isolate and examine the effects of different components on the emergence of position bias.
\paragraph{Task structure} Following~\citet{Reddy2023TheMB}, we adopt the following information retrieval task: The model is trained to predict the label $y_{\text{query}}$
of a target $x_{\text{query}}$ using the cross-entropy loss, given an alternating sequence of $n$ items and $n$ labels:  \( x_1, y_1, \dots, x_n, y_n, x_{\text{query}} \). The sequence is embedded in $d$ dimensions.  Each \( x_i \) is sampled from a Gaussian mixture model with $K$ classes, and $y_i$ is the corresponding class label assigned prior to training from the total $L$ labels ($L\leq K$). The burstiness $B$ is the number of occurrences of \( x_i \) from a particular class in an input sequence.  Importantly, at least one item in the context belongs to the same class as the query. To control position bias in the training data,  $x_\text{query}$ can either be explicitly assigned to the class of a specific $x_i$, introducing position-dependent bias in the data, or randomly assigned to the class of any $x_i$, simulating a scenario without position bias in the data. 
\paragraph{Tracking position bias} To quantify position bias, we evaluate model performance using sequences containing novel classes not seen during training. Specifically, by generating new class centers for the Gaussian mixture and randomly assigning one of the $L$ existing labels to these novel classes, we ensure that the model relies on contextual information rather than memorized class features. Crucially, we can systematically vary the position of the correct answer within test sequences to measure retrieval accuracy changes, thereby isolating and quantifying position-dependent biases in the model's behavior.
\paragraph{Network architecture}
The input sequences are passed through an attention-only network followed by a classifier. Each
attention layer has one attention head. The classifier is then a three-layer MLP with ReLU activations and a softmax layer which predicts the probabilities of the $L$
labels.

Following~\citet{Reddy2023TheMB}, we set $n=8$ and $d=64$. Additional experimental details are provided in~\cref{app:exps}. Despite our use of a simplified experimental setup, we observe the emergence of key phenomena documented in real-world LLMs, such as the ``lost-in-the-middle" phenomenon (\cref{exp:causal}) and the formation of attention sinks (\cref{app:attn_sinks}). This convergence between our controlled environment and real-world observations validates our choice of abstraction, suggesting that we have preserved the essential mechanisms driving position bias while enabling systematic investigation.




\subsection{The Effects of Depth and Relative PEs}\label{sec:depth}


To investigate the position bias arising solely from the architectural design of the attention mechanism, we use training sequences without positional bias, where the position of $x_i$ sharing the same class as $x_{\text{query}}$ is uniformly random in $\{1, 2, \ldots, n\}$. To evaluate the position bias in the trained model, we construct test sequences of the form $[\bm{a}, b]$. Here, the bolded term $\bm{a}$ explicitly marks the correct position, ensuring $y_a$ matches $y_{\text{query}}$, while position $b$ serves as a baseline. In these sequences, $x_a$ and $x_b$ are identical vectors, allowing us to control for the influence of semantic information on the model's retrieval accuracy. We then measure the retrieval accuracy gap between pairs of sequences where the content at positions $a$ and $b$ is identical, but the correct position varies. This gap, defined as $[\bm{a},b]-[\bm{b},a]$, quantifies the model’s positional preference independent of semantic information. To perform this evaluation, we construct three pairs of test sets, each containing $10,000$ sequences: [\textbf{first}, middle] vs. [\textbf{middle}, first], [\textbf{first}, last] vs. [\textbf{last}, first], and [\textbf{middle}, last] vs. [\textbf{last}, middle]. Here ``first'' (position~1), ``middle'' (position~\(n/2\)), and ``last'' (position~\(n\))  
denote fixed positions within a sequence. 

 \begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{figs/depth.pdf}\caption{Position bias arising solely from the architectural design of the attention mechanism, with \textbf{no positional bias in the training data}. $a$ vs.\ $b$ denotes the gap for the case $[\bm{a},b]-[\bm{b},a]$, where bar magnitude indicates gap size, positive indicates bias toward earlier position, and negative indicates bias toward later position. Deeper attention amplifies the bias toward earlier tokens, regardless of the PE used. Furthermore, decay mask introduce stronger distance-based decay effects that increase focus on recent tokens than RoPE. }
    \label{fig:depth-PE}
\end{figure*}

 \begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{figs/how_causal_learn.pdf}\caption{Position bias when \textbf{trained on data biased toward the first and last positions}. Compared with no mask, a causal mask without PE indeed introduces positional
dependencies. However, pure causal mask captures positional bias only at the first position but not at the last, whereas both sin PE and RoPE successfully capture biases at both ends. Moreover, the performance under PEs also displays a ``lost-in-the-middle" pattern, which is absent under other types of positional bias in the training data (see~\cref{app:additional_positional_bias} for more details). }
    \label{fig:how_causal_learn}
\end{figure*}

\Cref{fig:depth-PE} shows the average results over five runs, where $a$ vs.\ $b$ denotes the gap $[\bm{a},b]-[\bm{b},a]$.  The magnitude of each bar represents the size of the performance gap, and the sign of each bar reflects the direction of the bias: a positive sign indicates a bias toward earlier positions, while a negative sign indicates a bias toward later positions. We highlight several key observations. First, increasing model depth consistently amplifies the bias toward earlier parts of the sequence, regardless of the PE used. Also note that the performance gap between the middle and last positions is notably smaller than that involving the first position. This aligns with our theory, which suggests that as the model focuses more on the initial part of the sequence, information near the sequence's end becomes less distinguishable, consistent with the patterns observed in~\citet{Barbero2024TransformersNG}. Furthermore, both the decay mask and RoPE introduce distance-based decay effects that reduce the bias toward the beginning induced by the causal mask and increase the focus on recent tokens. However, the decay effect induced by the decay mask is substantially more pronounced than that by RoPE, as predicted by our theory.  
\subsection{Can Causal Mask Induce Usable Positional Information?}\label{exp:causal}
Next, we empirically examine how the causal mask leverages positional information. \citet{Kazemnejad2023TheIO} hypothesized that without PE (No PE), the causal mask can implicitly simulate absolute or relative PE through specific weight matrix configurations. To test this hypothesis, we train models on sequences with positional bias at either the beginning or the end. Specifically, in the training data, $x_\text{query}$ is assigned to the class of $x_1$ or $x_n$ with equal probability. We then evaluate two types of attention masks: no mask ($\cG$ is complete) and causal, and three types of PEs: No PE~\cite{Kazemnejad2023TheIO}, absolute sinusoidal PE (sin PE)~\cite{Vaswani2017AttentionIA}, and relative PE using RoPE. For evaluation, we construct six types of test sets as described in~\cref{sec:depth}, each with $10,000$ sequences. 

\Cref{fig:how_causal_learn} shows the average results using a $2$-layer network over five runs. Notably, in the left subplot, the causal mask without PE demonstrates a clear position bias toward the first position compared to the no mask without PE. This indicates that the causal mask indeed introduces a notion of position. However, when strong positional biases are present in the training data, both sin PE and RoPE allow the model to effectively capture these biases at both ends, regardless of the mask used. In contrast, a causal mask without PE only enables the model to learn a position bias at the beginning of the sequence. If the hypothesis by~\citet{Kazemnejad2023TheIO} were correct, that the causal mask uses positional information by simulating PEs, then the model should be able to capture positional bias at any location. This discrepancy suggests that the causal mask does not inherently implement PE but instead introduces a bias toward earlier positions via iterative attention, capturing positional bias only when it aligns with this predisposition.

\paragraph{The role of data in creating positional bias}
It is worth noting that in~\Cref{fig:how_causal_learn}, we observe the ``lost-in-the-middle" phenomenon~\cite{liu2024lost}, where information retrieval accuracy follows a U-shape relative to the position of the answer, with performance at the beginning of the sequence slightly better than at the end. More experimental results under different types of positional bias in the training data can be found in~\Cref{app:additional_positional_bias}. Notably, this phenomenon does not occur when the training data lacks positional bias (\Cref{fig:masks}) or contains other types of positional bias considered (\Cref{app:additional_positional_bias}).  This suggests that specific types of positional bias in the training data also play a role in how the model learns to prioritize positions within a sequence.

\section{Conclusion}
In this paper, we study position bias in transformers through a probabilistic and graph-theoretic lens, developing a theoretical framework that quantifies how positional information influences context construction across multi-layer attention. Our analysis reveals two key findings about position bias in transformers: the causal mask's inherent bias toward earlier tokens, as deeper layers increasingly attend to these positions through iterative attention, and the interplay between causal masking and relative positional encodings, which results in a nuanced, non-monotonic balance between distance-based decay effects and the cumulative influence of earlier positions. These findings open several promising directions for future work. One potential direction is leveraging these insights to design bias-free transformers, mitigating positional biases to improve model robustness and generalization capabilities. Alternatively, our framework can also inform the strategic exploitation of positional bias in specific applications, such as emphasizing early positions for text summarization or prioritizing recent interactions in recommendation systems. By deepening our understanding of how architectural choices in transformers shape positional dependencies, our work provides a foundation for designing attention mechanisms with predictable and task-aligned positional properties.


\bibliography{references}
\bibliographystyle{plainnat}


\appendix

\section{Proof of~\Cref{thm: causal_mask}}
\subsection{Auxiliary results}

\begin{lemma}
    Under~\textup{\textbf{A1}}-\textup{\textbf{A2}}, there exists $\epsilon > 0$ such that $A^{(t)}_{ij} \geq \epsilon$ for all $t\geq 0$, $(j,i) \in E$.
    \label{lem: matrix_A}
\end{lemma}

\begin{proof}
    Writing~\eqref{eq: update_no_LN} recursively, we get that the token trajectories 
    \begin{equation}
        X^{(t+1)} = A^{(t)}...A^{(0)}X^{(0)}W_V^{(0)}...W_V^{(t)}\,,
        \label{eq: token_traj_no_LN}
    \end{equation}
    stay uniformly bounded for all $t\geq 0$ by~\textup{\textbf{A2}}.  Then it follows from~\textup{\textbf{A1}} that there exists $C\in \bR$ such that for all $t\geq 0$,
\begin{equation}
\begin{aligned}\norm{\left(X^{(t)}W_Q^{(t)}\right)_{i,:}}_2 = \norm{X^{(t)}_{i,:}W_Q^{(t)}}_2 \leq C\,,\\
    \norm{\left(X^{(t)}W_K^{(t)}\right)_{i,:}}_2 = \norm{X^{(t)}_{i,:}W_K^{(t)}}_2 \leq C\,. 
    \end{aligned}
     \label{eq: qk_norm_bound}
    \end{equation}
   
    Hence for all $i,j\in[N]$,
    \[ - C^2 \leq (X^{(t)}W^{(t)}_Q (X^{(t)}W^{(t)}_K)^\top)_{ij} \leq C^2\,.\]
    This implies that there exists $\epsilon > 0$ such that 
    $A^{(t)}_{ij} \geq \epsilon$ for all $(j,i) \in E$.
\end{proof}

\subsection{Proof of~\cref{thm: causal_mask}}

We denote 
$P^{(t)} \vcentcolon = A^{(t)}\cdots A^{(0)}$. It suffices to show that there exists $ 0 < C < 1$ and $0 < \epsilon <1$ such that 
\begin{equation}
    P^{(t)}_{ij} \leq C(1-(j-1)\epsilon)^t
    \label{eq: causal_exponetial_decay}
\end{equation}
for all $1<j\leq i$ and $t\geq 0$. 

The proof will go by induction:
\paragraph{Base case}
By~\cref{lem: matrix_A}, it follows that 
\begin{equation*}
    P^{(0)}_{ij} \leq (1-\epsilon)
\end{equation*}
for all $1<j\leq i$. Then let $C\vcentcolon=1-\epsilon$.
\paragraph{Induction step} Assume that~\eqref{eq: causal_exponetial_decay} holds, it follows that for all $1<j\leq i$. 
\begin{equation*}
    P_{ij}^{(t+1)} = \sum_{k=j}^i A^{(t)}_{ik}P^{(t)}_{kj} \leq (1-(j-1)\epsilon)C(1-(j-1)\epsilon)^{t} = C(1-(j-1)\epsilon)^{t+1}\,.
\end{equation*}
From above, we conclude the theorem.


\section{Proof of~\cref{thm:sliding-window}}
For $t_0 \leq t_1$, we denote 
$$A^{(t_1:t_0)} = A^{(t_1)}\ldots A^{(t_0)}\,.$$
Without loss of generality, we assume in the following proof that $N-1$ can be divided by $w-1$.
\subsection{Auxiliary results}

\begin{lemma}
    Let $\cG$ be the sliding-window mask with the window size $w \geq 2$. Then there exists $c > 0$ such that for all $t_0\geq 0$, 
    \[c \leq A^{\left(t_0+\frac{N-1}{w-1}-1:t_0\right)}_{ij} \leq 1 \,,\quad\forall j\leq i\in[N]\,.\]
    \label{lem:batch_P_positive}
\end{lemma}

\begin{proof}
    Given the connectivity pattern of the sliding-window mask $\cG$ and~\cref{lem: matrix_A}, it follows that for all $t_0\geq 0$, $A^{\left(t_0+\frac{N}{w}-1:t_0\right)}$ is a lower triangular matrix. Moreover, since $A_{ij}^{\left(t_0+\frac{N}{w}-1:t_0\right)}$ counts the aggregate probability of the walks of length $\frac{N-1}{w-1}$
 between token $i$ and token $j$ where by~\cref{lem: matrix_A}, each walk has probability at least $\epsilon^{\frac{N-1}{w-1}}$.
 
 Thus we conclude that there exists $c>0$ such that for all $t_0\geq 0$,
    \begin{equation*}A_{ij}^{\left(t_0+\frac{N}{w}-1:t_0\right)} \geq c, \quad \forall j\leq i\in[ N]\,. 
    \end{equation*}
\end{proof}

\subsection{Proof of~\cref{thm:sliding-window}}

For $k\geq 0$, denote 
\[\tilde{A}^{(k)} = A^{\left((k+1)\left(\frac{N-1}{w-1}\right)-1:k\left(\frac{N-1}{w-1}\right)\right)}\,\]
and 
\[\tilde{P}^{(k)} = \tilde{A}^{(k)}\cdots \tilde{A}^{(0)}\,.\]

Then by~\cref{lem:batch_P_positive} and~\cref{thm: causal_mask}, we get that there exists $0<C<1$ and $0<c<1$ such that for all $k\geq 0$

\begin{equation*}
\tilde{P}_{ij}^{(k)} \leq C(1-(j-1)c)^k\,, \quad \forall j\leq i\in[N]\,.
\end{equation*}

Denote $Q_j^{(t)} = \underset{1\leq i\leq N}{\max} P_{ij}^{(t)}$ and $\tilde{Q}_j^{(k)} = \underset{1\leq i\leq N}{\max} \tilde{P}_{ij}^{(k)}$. 
Then it follows that for all $k\geq 0$,
\begin{equation*}
    \tilde{Q}_j^{(k)} \leq C(1-(j-1)c)^k.
\end{equation*}

Observe that 

\begin{equation}
    \forall i\in [N], P^{(t)}_{ij} \leq Q^{(t)}_j\,,
    \label{eq: obs_1}
\end{equation}
and
\begin{equation}
    \forall j\in [N], Q_j^{(t+1)}\leq Q_j^{(t)}\,.
    \label{eq: obs_2}
\end{equation}


Let $q_j = (1-(j-1)c)^{\frac{1}{2\frac{N-1}{w-1}}}$.
Then for all $k\geq 1$ and $0\leq r < \frac{N-1}{w-1}$,
\[q_j^{k\left(\frac{N-1}{w-1}\right) +r } \geq (1-(j-1)c)^k.\]
This implies that for all $t\geq \frac{N-1}{w-1}$,
\[P_{ij}^{(t)} \leq q_j^{t} = C(1-(j-1)c)^{t/\left(2\frac{N-1}{w-1} \right)}\,.\]

As for $t<\frac{N-1}{w-1}$, notice that $P_{ij}^{(0)}\leq 1-\epsilon$ for all $j\leq i\in[N]$ by~\cref{lem: matrix_A}. Then by~\eqref{eq: obs_1} and \eqref{eq: obs_2}, we deduce that 
\[P_{ij}^{(t)}\leq (1-\epsilon)^\frac{t+1}{\frac{N-1}{w-1}}, \quad \forall j\leq i \in [N]. \]

We thus conclude the statement.


\section{Proof of~\cref{thm:prefix_mask}}

First note that the first and third statements:
\begin{equation}\lim_{t\to\infty} \bP^{(t)}(z_i \in [K] |X^{(0)}) = 1\,,
        \label{eq: first_K_tokens_total}
    \end{equation}

for all $\in [N]$ and 

\begin{equation*}
        \bP^{(t)}(z_i = j|X^{(0)}) \leq  C(1-(j-K)\epsilon)^t\,.
         \label{eq: prefix_exp}
    \end{equation*}
for all $K< j \leq i$ and $t\geq 0$, follow immediately from~\Cref{thm: causal_mask} by regarding the first K tokens as a super node in the 
 causal graph $\cG$ and aggregate the edges accordingly. Thus it suffices to show that there exists $\kappa > 0$ such that
 \begin{equation}
        \liminf_{t\to\infty} \bP^{(t)}(z_i = k|X^{(0)}) \geq  \kappa. \qquad\forall k\in [K]\,.
        \label{eq: first_k_distribution}
    \end{equation}
 For $t > 0$, consider $$P^{(t)}_{ik} = \sum_{l=1}^N P_{il}^{(t:1)}A_{lk}^{(0)}\,.$$

 Then for $k_1, k_2 \in [K]$, 
$$\frac{P^{(t)}_{ik_1}}{P^{(t)}_{ik_2}} = \frac{\sum_{l=1}^{\max\{i,K\}} P_{il}^{(t:1)}A_{lk_1}^{(0)}}{\sum_{l=1}^{\max\{i,K\}} P_{il}^{(t:1)}A_{lk_2}^{(0)}}\,,$$

  which then follows 
$$\frac{P^{(t)}_{ik_1}}{P^{(t)}_{ik_2}} \geq \underset{1\leq l \leq \min\{i,K\}}{\min} \frac{A^{(0)}_{lk_1}}{A^{(0)}_{lk_2}}\,.$$


 Then by~\Cref{lem: matrix_A}, there exists $C > 0$ such that for all $k_1, k_2 \in [K]$, 
$$C \leq \liminf_{t\to \infty} \frac{P^{(t)}_{ik_1}}{P^{(t)}_{ik_2}}\,.$$
Since $\lim_{t\to\infty}\sum_{k=1}^K P_{ik}^{(t)} = 1$ by~\eqref{eq: first_K_tokens_total}, we deduce~\eqref{eq: first_k_distribution} as desired.

\section{Proof of~\cref{lem:decay_mask_attn}}
Fix $t\geq 0$. Let \[Z^{(t)}_{ij} = (X^{(t)}W^{(t)}_Q)_{i,:} (X^{(t)}W^{(t)}_K)_{:,j}.\]
Following from~\cref{lem: matrix_A}, there exists $I_{\min}, I_{\max} \in \bR$ such that for all $j\leq i\in [N]$, \[Z^{(t)}_{ij}\in[I_{\min}, I_{\max} ].\]
Consider the denominator in the $\softmax(\cdot)$ operation in the calculation of $(A^{(t)}_{\decay})_{ij}$:
\begin{align*}
\sum_{k=1}^i e^{Z^{(t)}_{ik}-(i-k)m} & \geq e^{I_{\min}}  \sum_{k=0}^i e^{-(i-k)m}\\
& = e^{I_{\min}}\frac{1-e^{-(i+1)m}}{1-e^{-m}} \\
& \geq e^{I_{\min}} \frac{1-e^{-2m}}{1-e^{-m}} \\
& = e^{I_{\min}} (1+e^{-m})
\end{align*}
and 
\begin{align*}
\sum_{k=1}^i e^{Z^{(t)}_{ik}-(i-k)m} & \leq e^{I_{\max}}  \sum_{k=0}^\infty e^{-km}\\
& = \frac{e^{I_{\max}}}{1-e^{-m}}
\end{align*}
It follows that 
\begin{align*}
    (A^{(t)}_{\decay})_{ij} \leq 
\frac{e^{I_{\max}-(i-j)m}}{e^{I_{\min}}(1+e^{-m})} = C_{\max} e^{-(i-j)m}
\end{align*}
and 
\begin{align*}
    (A^{(t)}_{\decay})_{ij} \geq 
\frac{e^{I_{\min}-(i-j)m}}{e^{I_{\max}}/(1-e^{-m})} = C_{\min} e^{-(i-j)m}
\end{align*}
where $C_{\max} \vcentcolon=e^{(I_{\max}-I_{\min})}/({1+e^{-m}})$ and $C_{\min} \vcentcolon=(1-e^{-m})e^{(I_{\min}-I_{\max})}$.

\section{Proof of~\cref{thm:aggregate_decay_effect}}
Note that in the causal graph $\cG$, there are $t+i-j \choose i-j$ paths of length $t+1$ from token $j$ to token $i$.

Since going from token $j$ to token $i$ in the causal graph, the connectivity patterns ensure that the token indices along the path are non-decreasing, i.e. if we denote the directed path as $(j,l_1), (l_1,l_2),...,(l_t,i)$, it holds that $j\leq l_1\leq l_2 \leq ...\leq l_{t}\leq i$. Together with~\cref{lem:decay_mask_attn}, we conclude the theorem statement.

\section{Proof of~\cref{lem: attn_rope}}
    Fix $t\ge0$. Denote the angle after rotation to be $\psi^{(t)}_{i,j}$. Then it follows from the definition of RoPE that 
    \begin{equation*}
        \psi^{(t)}_{i,j} = \phi^{(t)}_{i,j} - (i-j)\theta_1\,.
    \end{equation*}
    Thus 
    \begin{equation*}
        |\psi^{(t)}_{i,j}| = |\phi^{(t)}_{i,j} - (i-j)\theta_1| \geq| |(i-j)\theta_1| - |\phi^{(t)}_{i,j}|| \geq |(i-j)-\delta|\theta_1\,.
    \end{equation*}
 
    
    \begin{equation*}
        |\psi^{(t)}_{i,j}| = |\phi^{(t)}_{i,j} - (i-j)\theta_1| \leq |(i-j)\theta_1| +  |\phi^{(t)}_{i,j}| \leq (i-j+\delta)\theta_1\,.
    \end{equation*}

Let the original query $i$ and key $j$ embeddings be $q^{(t)}_i \vcentcolon= X^{(t)}_{i,:}W^{(t)}_Q$ and $k^{(t)}_j\vcentcolon=  X^{(t)}_{j,:}W^{(t)}_K$, respectively, and the corresponding query $i$ and key $j$ embeddings after rotation be $q_i'^{(t)}$ and $k_j'^{(t)}$, respectively.

Since $\langle q'^{(t)}_i, k'^{(t)}_j\rangle = \|q^{(t)}_i\|_2 \|k^{(t)}_j\|_2 \cos \psi^{(t)}_{i,j}$, it follows from  that there exists $C_{\min}, C_{\max} \geq 0$ such that for all $i,j\in[N]$,
\begin{equation*}
    C_{\min} \cos ((i-j + \delta)\theta_1) \leq \langle q'^{(t)}_i, k'^{(t)}_j\rangle \leq C_{\max} \cos (|(i-j)-\delta|\theta_1)\,.
\end{equation*}
Since for $|x| \leq \pi$ there exists $c>0$ such that $1-x^2/2 \leq \cos x  \leq 1-x^2/c$, we get that
\begin{equation*}
    C_{\min} \left(1-\frac{((i-j) + \delta)^2\theta_1^2}{2}\right) \leq \langle q'^{(t)}_i, k'^{(t)}_j\rangle \leq C_{\max} \left(1-\frac{((i-j)-\delta)^2 \theta_1^2}{c}\right) \,.
\end{equation*}
and hence 
\begin{equation*}
    C_{\min} \left(1 - \delta^2\theta_1^2-(i-j)^2 \theta_1^2\right) \leq \langle q'^{(t)}_i, k'^{(t)}_j\rangle \leq C_{\max} \left(1-\frac{((i-j)^2/2-\delta^2) \theta_1^2}{c}\right) \,.
\end{equation*}

Consider $Y^{(t)}_i = \sum_{k=1}^{i}e^{\langle q'^{(t)}_i, k'^{(t)}_j \rangle}$. Then by~\eqref{eq: qk_norm_bound}, we get that there exists $Y_{\max}, Y_{\min} > 0$ such that 
\begin{equation*}
    Y_{\max} \leq Y^{(t)}_i \leq Y_{\min}\,.
\end{equation*}
We thus conclude the statement.


\section{Proof of~\cref{thm: rope}}

    Notice that
    \begin{equation}
        (P^{(t)}_{\RoPE})_{ij} = \sum_{l_1\leq\cdots\leq l_{t-1}\in[N]^{t-1}} A^{(t-1)}_{il_{t-1}}A^{(t-2)}_{l_{t-1}l_{t-2}}\cdots A^{(0)}_{l_1j}
    \end{equation}
    Given that when $\cG$ is the causal graph, due to the connectivity the directed path of length $t$ from token $j$ to token $i$ must be non-decreasing, i.e. $j\leq l_1 \leq l_2 \leq \cdots\leq l_{t-1} \leq i$, and there would be in total ${t+i-j  \choose i-j}$ such paths. For each such path $j\leq l_1 \leq l_2 \leq \cdots\leq l_{t-1} \leq i$, notice that by~\Cref{lem: attn_rope}, we get that fix $T\geq0$, there exists $C_{\min}$, $C_{\max} > 0$ such that 
    \begin{equation}
        A^{(t-1)}_{i,l_{t-1}}A^{(t-2)}_{l_{t-1},l_{t-2}}\cdots A^{(0)}_{l_1,j} \geq C_{\min} e^{-c((i-l_{t-1})^2 + (l_{t-1}-l_{t-2})^2 + \cdots + 
        (l_1-j)^2)\theta_1^2}
        \label{eq: rope_P_lower_raw}
    \end{equation}
    and 
    \begin{equation}
        A^{(t-1)}_{i,l_{t-1}}A^{(t-2)}_{l_{t-1},l_{t-2}}\cdots A^{(0)}_{l_1,j} \leq C_{\max} e^{-c'((i-l_{t-1})^2 + (l_{t-1}-l_{t-2})^2 + \cdots + 
        (l_1-j)^2)\theta_1^2}
        \label{eq: rope_P_upper_raw}
    \end{equation}

From~\eqref{eq: rope_P_lower_raw}, since $j\leq l_1 \leq l_2 \leq \cdots\leq l_{t-1} \leq i$, we further get that
  \begin{equation}
        A^{(t-1)}_{i,l_{t-1}}A^{(t-2)}_{l_{t-1},l_{t-2}}\cdots A^{(0)}_{l_1,j} \geq C_{\min} e^{-c(i-j)^2\theta_1^2}\,,
        \label{eq: rope_P_lower_final}
    \end{equation}
and similarly
 \begin{equation}
        A^{(t-1)}_{i,l_{t-1}}A^{(t-2)}_{l_{t-1},l_{t-2}}\cdots A^{(0)}_{l_1,j} \leq C_{\max} e^{-\frac{c'}{2}(i-j)^2\theta_1^2}\,.
        \label{eq: rope_P_lower_final}
    \end{equation}

\section{Implicit differentiation of $x$ with respect to $\theta_1$ and $t$}\label{app:implicit differentiation}

Recall that under RoPE, $$L(x) = \log \left({t+i-j  \choose i-j} e^{-(i-j)^2\theta_1^2} \right)\,.$$
Then by Stirling's approximation,
$$L(x) \approx \left((t+x)\log(t+x) - (t+x) \right) - (x\log x -x) - \theta_1^2x^2\,,$$
and thus 
$$L'(x) = \log\left(\frac{t+x}{x}\right) -2\theta_1 x\,.$$

Taking implicit differentiation of $t$:

\[\frac{\partial}{\partial t}\log \left(\frac{t+x}{x}\right)= \frac{-t}{x(x+t)}\frac{\partial x}{\partial t} + \frac{1}{x+t}\]

and 
\[\frac{\partial}{\partial t} 2\theta_1 x = 2\theta_1 \frac{\partial x}{\partial t}.\]

So let
\[\frac{1}{x+t} = (2\theta_1 +\frac{t}{(x+t)x})\frac{\partial x}{\partial t}\]
and thus
\[\frac{\partial x}{\partial t} = \frac{1}{2\theta_1(x+t)+\frac{t}{x}} >0\,.\]
Taking implicit differentiation of $\theta_1$:

\[\frac{\partial}{\partial \theta_1}\log \left(\frac{t+x}{x}\right)= \frac{-t}{x(x+t)}\frac{\partial x}{\partial \theta_1}\]
and 
\[\frac{\partial}{\partial \theta_1} 2\theta_1 x = 2x +2\frac{\partial x}{\partial \theta_1} \theta_1.\]
So let
\[\left(2\theta_1 +\frac{t}{x(x+t)}\right)\frac{\partial x}{\partial \theta_1} = -2x\,,\]
and thus 
\[\frac{\partial x}{\partial \theta_1} = \frac{-2x}{2\theta_1+\frac{t}{x(x+t)}} < 0\,.\]
Hence we observe that $x^*$ is an increasing function of $t$ and a decreasing function of $\theta_1$. This implies that increasing the base rotational angle $\theta_1$ reduces the optimal distance $x^*$, amplifying the long-term decay effect and causing tokens to focus more on nearby tokens. In contrast, increasing the number of attention layers $t$ increases $x^*$ and hence deeper models become more biased toward initial tokens.


\section{The effect of RoPE: case for $d\geq 2$}\label{app:rope_d_geq_2}
In this section, we present a generalized version of~\cref{thm: rope} for the case $d\geq2$. 

Let the query $q$ and key $k$ be vectors in $\bR^d$, where $d$ is even, and let $\phi$ be the angle between $q$ and $k$, which we assume to be well-defined, with:
\[\cos\phi = \frac{\langle q, k\rangle}{\|q\|_2\|k\|_2}\,.\] 

Define the length-2 segments of query $q$ and 
and key $k$ as 
\[q_l = (q_{2l-1}, q_{2l}),  \quad k_l = (q_{2l-1}, q_{2l}),\]
for $l\in [d/2]$, and let $\phi_l$ be the angle between $q_l$ and $k_l$, with:
\[\cos\phi_l = \frac{\langle q_l, k_l\rangle}{\|q_l\|_2\|k_l\|_2}\,.\]


Without loss of generality, we make the following assumption:

\begin{enumerate}[leftmargin=*, labelindent=2em]
   \item [\textbf{A3}] There exists $\beta_q, \beta_k>0$ such that $\|q^{(t)}_{l}\|_2 \geq  \beta_q \|q^{(t)}\|_2$ and $\|k^{(t)}_{l}\|_2 \geq  \beta_k \|k^{(t)}\|_2$ for all $l\in[d/2]$ for all $t\geq 0$.
\end{enumerate}

The condition means that all segments makes a nontrivial contribution to the norm. In practice, since LLMs tend to tend to predominantly utilize feature dimensions that rotate slowly~\cite{Barbero2024RoundAR}, the effective $d/2$ tends to be a small number.


Given the pre-defined set of base rotational angles $\Theta = \{0\leq\theta_1\leq \cdots \leq\theta_{d/2}\}$, we reparametrize as $\theta_i = \alpha_i\theta_1$. 
\subsection{Results}
We present the general version of~\cref{lem: attn_rope} and~\cref{thm: rope} as follows:
\begin{lemma}
    Let $\cG$ be the causal mask and $\textup{\textbf{A1}}$-$\textup{\textbf{A3}}$ hold. Suppose for $t\geq 0$, $\|q_i\|_2, \|k_j\|_2 > 0$, and $|\phi^{(t)}_{i,j}| \leq \delta\theta_1$, where $\delta > 0$ and $$\left(\sqrt{\frac{1}{\beta_q\beta_k}}\delta\pi+2(N-1)\alpha_{d/2}\right)\theta \leq 2\pi\,.$$ Then there exists $C_{\max}, C_{\min}, c,c' > 0$ such that 
    $$C_{\min} e^{-c\sum_{l=1}^{d/2}(i-j)^2\alpha_l^2\theta_1^2} \leq (A^{(t)}_{\RoPE})_{i,j} \leq C_{\max} e^{-c'\sum_{l=1}^{d/2}(i-j)^2\alpha_l^2\theta_1^2}\,.$$
    \label{lem: attn_rope_general}
\end{lemma}

\begin{theorem}
   Fix $T> 0$. Under the same conditions as in~\cref{lem: attn_rope_general} for $t\leq T$, there exists $c>0$ such that for all $t\leq T$, 
   $$(P^{(t)}_{\RoPE})_{i,j} = \Theta\left({t+i-j  \choose i-j} e^{-c\sum_{l=1}^{d/2}(i-j)^2\alpha_l^2\theta_1^2} \right)\,.$$
   \label{thm: rope_general}
   \vspace{-3ex}
\end{theorem}




\subsection{Proofs of~\cref{lem: attn_rope_general}}
We first show the following auxiliary result:
\begin{lemma}
    Under \textup{\textbf{A3}},  it holds that
    \[|\phi_l| \leq \frac{\pi}{2}\sqrt{\frac{1}{\beta_q\beta_k}} |\phi|\,,\]
    for all $l\in[d/2]$.
\end{lemma}
\begin{proof}
    By definition, since
    \[\sum_{l=1}^{d/2}\|q_l\|_2\|k_l\|_2 \cos \phi_l = \|q\|_2\|k\|_2\cos\phi\,,\]
    then the Cauchy–Schwarz inequality implies that 
    \begin{equation}
        \sum_{l=1}^{d/2}\|q_l\|_2\|k_l\|_2 (1-\cos \phi_l) \leq \|q\|_2\|k\|_2(1-\cos\phi)\,.
        \label{eq:inner_product_identity_cs}
    \end{equation}
    By~\textbf{A3}, \eqref{eq:inner_product_identity_cs} becomes 
     \begin{equation*}
        \sum_{l=1}^{d/2} (1-\cos \phi_l) \leq \frac{1}{\beta_q\beta_k}(1-\cos\phi)\,.
    \label{eq:inner_product_identity_cs_2}
    \end{equation*}
    Given the trigonometric identity $1-\cos 2x = 2\sin^2 x$, we get that 
     \begin{equation}
        \sum_{l=1}^{d/2} \sin^2\left(\frac{\phi_l}{2}\right) \leq \frac{1}{\beta_q\beta_k}\sin^2\left(\frac{\phi}{2}\right) \,.
    \label{eq:inner_product_identity_cs_2}
    \end{equation}

    Notice for all $x\in\bR$,
    \begin{equation}
        \sin^2 x\leq x^2\,,
        \label{eq:ineq_1}
    \end{equation}
    and  for all $|x|\leq \pi/2$, 
     \begin{equation}
        \frac{4}{\pi^2}x^2 \leq \sin^2x\,.
         \label{eq:ineq_2}
    \end{equation}
    Apply~\eqref{eq:ineq_1} and~\eqref{eq:ineq_2} to~\eqref{eq:inner_product_identity_cs_2}, we get that 
    \begin{equation*}
        \sum_{i=1}^{d/2} \phi_l^2 \leq \frac{\pi^2}{4\beta_q\beta_k} \phi^2\,.
    \end{equation*}
   Hence for all $l\in[d/2]$, it follows that
   \begin{equation*}
       |\phi_l| \leq \frac{\pi}{2}\sqrt{\frac{1}{\beta_q\beta_k}} |\phi|\,.
   \end{equation*}
\end{proof}


 Denote the angle after rotation to be $\psi_{i,j,l}$. Then it follows that 
    \begin{equation*}
        \psi_{i,j,l} = \phi_{i,j,l} - (i-j)\alpha_l\theta_1\,.
    \end{equation*}
It follows similarly as in the proof of~\cref{lem: attn_rope} that 
\begin{equation*}
    C_{\min} \left(1 - \delta'^2\theta^2-(i-j)^2 \alpha_l^2 \theta_1^2\right) \leq \langle (q'_i)_l, (k'_j)_l\rangle \leq C_{\max} \left(1-\frac{((i-j)^2\alpha_l^2/2-\delta'^2) \theta_1^2}{c}\right) \,,
\end{equation*}
where $\delta' = \frac{\pi}{2}\sqrt{\frac{1}{\beta_q\beta_k}}  \delta  $, for all $l\in[d/2]$.

Since \[\langle q_i',k_j'\rangle = \sum_{l=1}^{d/2} \langle (q'_i)_l, (k'_j)_l \rangle,\] we conclude the statement.

\subsection{Proof of~\cref{thm: rope_general}}
The result is a direct corollary of~\cref{lem: attn_rope} and~\cref{thm: rope}.


\section{Experiments}\label{app:exps}

Here we provide more details on the numerical experiments presented in~\cref{sec:exp}. All models were
implemented with PyTorch~\cite{Paszke2019PyTorchAI}.

\paragraph{Parameterizing the data distribution} As defined in~\cref{sec:exp}, the input data distribution is modulated by tuning various parameters. In addition to the parameters described in the main text, for the Gaussian mixture with $K$ classes, 
each class $k$ is defined by a $d$-dimensional
vector $\mu_k$ whose components are sampled $i.i.d.$ from a normal distribution with mean zero and variance $1/d$. Then the value of $x_i$ is given by $\frac{\mu_k+\gamma\eta}{\sqrt{1+\gamma^2}}$, where $\eta$ is drawn from the same distribution as the $\mu_k$’s and $\gamma$ sets the within-class variability. Each class is assigned to one of $L$ labels ($L\leq K$). The contents of the labels are drawn prior to training from the same distribution as the $\mu_k$'s. 







In~\citet{Reddy2023TheMB}, the author found that different configurations of the data generating process give rise to different learning regimes. To enable better information retrieval ability of the model, we choose the configuration suggested by~\citet{Reddy2023TheMB} that corresponds to the difficult in-weight learning and easy in-context-learning regime to ensure the information retrieval ability of the model. Specifically, we set $\gamma=0.75$, $K=2048$, $L=32$, and $B = 4$.

\paragraph{Relative PE hyperparameters} For the decay mask, we set $m=0.8$. For RoPE, we set $\theta_i = 10000^{-2(i-1)/d}$, as in~\citet{su2023roformerenhancedtransformerrotary}.


\paragraph{Compute} We trained all of our models on a Tesla V100 GPU.

\paragraph{Training details} In all experiments, we used the AdamW optimizer~\cite{Loshchilov2017DecoupledWD} with a learning rate of $10^{-3}$, a weight decay of $10^{-6}$, a batch size of $128$, and trained for $100,000$ iterations.

\section{Additional Experimental Results}\label{app:additional_exps}
\subsection{The role of training data on positional bias}\label{app:additional_positional_bias}


In this section, we present additional experimental results building on the experiment described in~\cref{exp:causal}, but focusing on cases where positional bias in the training sequences is introduced at other positions. Specifically, we consider three types of training sequences where $x_\text{query}$ is assigned the class of 1) $x_1$ (the first position), 2) $x_{n/2}$ (the middle position), or 3) $x_n$ (the last position). The corresponding results are shown in~\Cref{fig:bias_0}, \Cref{fig:bias_4}, and \Cref{fig:bias_-1}, respectively.



Observe that, compared with no mask, the causal mask without PEs indeed introduces a sense of position across all cases. Specifically, it enables the model to learn a positional bias favoring the beginning of the sequence, as earlier tokens tend to receive more attention through the mechanism of iterative attention. In contrast, both sin PE and RoPE allow the model to effectively capture different positional biases regardless of their location in the training sequences.

\begin{figure}[h]
        \centering
        \includegraphics[width=\linewidth]{figs/bias_0.pdf}
        \caption{Position bias when trained on data biased toward the \textbf{first} position.}
        \label{fig:bias_0}
        \vspace{1cm}
        \centering
        \includegraphics[width=\linewidth]{figs/bias_4.pdf}
        \caption{Position bias when trained on data biased toward the \textbf{middle} positions.}
        \label{fig:bias_4}
        \vspace{1cm}
        \centering
        \includegraphics[width=\linewidth]{figs/bias_-1.pdf}
        \caption{Position bias when trained on data biased toward the \textbf{last} positions.}
        \label{fig:bias_-1}
    \end{figure}

Interestingly, when comparing this behavior to the case shown in~\Cref{fig:how_causal_learn}, we note that the ``lost-in-the-middle'' phenomenon only emerges when the training sequences are biased toward both the beginning and the end. This suggests that specific types of positional bias in the training data play a crucial role in shaping how the model learns to process and prioritize positions within a sequence.




As the structure of positional bias in natural language remains unclear, this observation raises the following question:
\begin{quote}
    \textit{Does positional bias in natural language sequences shape the ``lost-in-the-middle'' phenomenon in a similar way to what we observe in this simplified case?}
\end{quote}

This question connects to broader inquiries about the parallels between artificial and human attention. In neuroscience, the primacy-recency effect highlights that human attention often gravitates toward the beginning and end of sequences~\cite{Glanzer1966TwoSM,Li2024EEGDT}, a phenomenon that may have influenced the structure of human languages, where critical information is frequently placed in these positions~\cite{Halliday2004AnIT}. As demonstrated in~\cref{exp:causal}, when such patterns are present in training data, attention-based architectures seem to develop analogous biases~\cite{Hollenstein2021MultilingualLM}, aligning with natural language characteristics for improved performance. This raises deeper, perhaps philosophical questions: To what extent are these biases intrinsic to effective sequential processing? How closely should neural networks emulate human cognitive patterns? Investigating these connections can deepen our understanding of both human and artificial intelligence while guiding the design of more effective machine learning models. 


\subsection{Attention sinks}\label{app:attn_sinks}


Despite our use of a simplified experimental setup in this work, we observe the emergence of key phenomena documented in more complex settings. In addition to the ``lost-in-the-middle" phenomenon discussed in~\cref{exp:causal} and~\cref{app:additional_positional_bias}, in this section, we report the formation of attention sinks in our setting.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.75\linewidth]{figs/causal_layers.pdf}
    \caption{Example of the emergence of attention sinks in our experimental setting. In particular, the sequences used for training and inference are all \textbf{free of position bias}.}
    \label{fig:attn_sinks_causal}
\end{figure}

\Cref{fig:attn_sinks_causal} shows an example of the attention maps of a two-layer self-attention networks under the causal mask without PEs, where the sequences used for training and inference are all free of position bias. We observe the similar phenomenon of attention sinks as reported in~\citet{Xiao2023EfficientSL}.

More quantitatively, following~\citet{Gu2024WhenAS}, we calculate their metric for measuring the emergence of attention sinks, over $10,000$ sequences free of position bias. 
Specifically, denote the adjacency matrix of the mask $\cG$ to be $M$. Then the metric for attention sink at token $j$ is calculated as 
\[\text{Attention Sink}_{j} = \frac{1}{T}\sum_{t=0}^{T-1} \frac{1}{\sum_{i=1}^NM_{ij}}\sum_{i=1}^{N} \1\{A^{(t)}_{ij} > \tau\}\,.\]
The threshold $\tau$ we choose is $0.2$. The results for the causal mask, the sliding-window masks (with $w=5,9,13$), and the prefix masks (with $K=2,4,6$) are shown in Figures ~\ref{fig:attnsink_causal}, \ref{fig:attnsink_window}, and \ref{fig:attnsink_prefix}, respectively.
In particular, we make the following observations: 
\begin{enumerate}
     \item Attention sinks emerge on the absolute first token under the causal mask. 
    \item Attention sinks tend to emerge on the absolute first token when the window size $w$ is larger, under the sliding-window mask.
    \item Attention sinks emerge on the $K$ prefix tokens, not just on the first token alone, under the prefix mask.
\end{enumerate}
   All of these phenomena have been observed in real-world LLMs in~\citet{Gu2024WhenAS}. This alignment between our controlled setup and real-world observations affirms the validity of our abstraction, indicating that we have captured the key mechanisms underlying position bias while facilitating a systematic analysis.

    \begin{figure}[h]
        \centering\includegraphics[width=0.34\linewidth]{figs/attnsink_causal.pdf}
        \caption{Attention sinks emerge on the first token under the causal attention mask.}
        \label{fig:attnsink_causal}
     \vspace{1cm} % Adjust vertical spacing if needed
        \centering
        \includegraphics[width=\linewidth]{figs/attnsink_window.pdf}
        \caption{Attention sinks tend to emerge on the absolute first token when the context window size $w$ is larger, under the sliding-window
mask.}
        \label{fig:attnsink_window}
    \vspace{1cm}
        \centering
        \includegraphics[width=\linewidth]{figs/attnsink_prefix.pdf}
        \caption{Attention sinks emerge on the $K$ prefix tokens, not just on the first token alone, under the prefix mask.}
        \label{fig:attnsink_prefix}
    \end{figure}









\end{document}