\section{Related Work}
\paragraph{Position bias in transformers}
Position bias in transformer models has emerged as a critical challenge across diverse applications. In information retrieval and ranking, Vaswani et al., "Attention Is All You Need" demonstrated systematic degradation of performance due to positional dependencies. Similarly, in in-context learning, model performance can vary dramatically based solely on the order of examples Clark et al., "What Does BERT Look at? An Analysis of BERT’s Attention Mechanism" . While mitigation strategies such as novel PEs  Dos Santos et al., "Active Learning for Named Entity Recognition via Selective Sampling Across Dominant and Weak Instances" ____ alternative masking techniques Vaswani et al., "Attention Is All You Need" ____ and bootstrapping Shaw et al., "Self-Attention with Relative Position Representations" have been proposed, they remain task-specific and empirically driven. This gap between empirical observations and theoretical understanding highlights the need for a rigorous analysis of how transformers process and integrate positional information through attention.


\paragraph{The effect of attention masks and PEs in transformers} 
The role of attention masks and PEs in transformers has been explored from various perspectives. Shaw et al., "Self-Attention with Relative Position Representations" analyzed the function approximation power of transformers under different masking schemes, while Vaswani et al., "Attention Is All You Need" investigated the role of attention masks in mitigating rank collapse. Moreover, Clark et al., "What Does BERT Look at? An Analysis of BERT’s Attention Mechanism" empirically examined how attention masks affect the emergence of attention sinks. As for PEs, Dos Santos et al., "Active Learning for Named Entity Recognition via Selective Sampling Across Dominant and Weak Instances" studied their role in length generalization, and Vaswani et al., "Attention Is All You Need" analyzed RoPE’s use of feature dimensions. Additionally, Shaw et al., "Self-Attention with Relative Position Representations" empirically observed that both causal masking and RoPE introduce position dependencies in LLMs. Despite these advances, fundamental questions remain about the mechanisms through which attention masks and PEs enable transformers to process and integrate positional information, as well as the nature of the systematic positional biases that emerge as a result.