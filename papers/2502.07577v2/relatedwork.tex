\section{Related Work}
\label{sec:related}

\textbf{Open-Ended Discovery with Foundation Models.}
The field of open-endedness~\citep{stanley2019open} aims to continually discover diverse and novel artifacts forever.
Recent methods leverage the generative capabilities and vast prior knowledge of foundation models (FMs) to accelerate this process~\citep{zhang2024omni,faldor2024omni,lehman2022evolutionlargemodels,hu2024automateddesignagenticsystems} by harnessing a foundation model's intrinsic notion of interestingness~\citep{zhang2024omni, faldor2024omni,lu2024intelligentgoexplorestandingshoulders,hu2024automateddesignagenticsystems} to construct the next proposal, analogous to human innovation.
Notable examples include ELM~\citep{lehman2022evolutionlargemodels} which evolves novel robot morphologies; OMNI-EPIC~\citep{faldor2024omni}, which automatically designs novel environments for reinforcement learning (RL) agents; DiscoPOP which discovers new loss functions for preference optimization algorithms~\citep{lu2024discopop}; ADAS~\citep{hu2024automateddesignagenticsystems}, which evolves novel designs for LLM-based agentic systems; and The AI Scientist~\citep{lu2024aiscientist}, which seeks to automate the entire scientific process by proposing novel ideas, conducting experiments, and writing a scientific paper summarizing the results.

\textbf{Automated Evaluation of Foundation Models.} Recent research also investigates automated evaluation of FMs, moving beyond static, human-designed test suites.
Rainbow Teaming~\citep{samvelyan2024rainbow} applies Quality-Diversity algorithms~\citep{mouret2015illuminatingsearchspacesmapping,pugh2016quality} to find novel adversarial attacks that stress-test FMs for safety.
Similarly, works like~\citep{zheng2024ali, zhou2024autoredteamer, jiang2024automatedRedTeaming, pavlova2024automatedRedTeaming} automate the red teaming (probing a system for weaknesses) process.
These works expand the comprehensiveness of existing safety checks but do not have the ability to create new tasks that might reveal completely unknown FM capabilities.
Other techniques generate new debate topics and evaluate FMs through multi-round debate between them~\citep{zhao2024auto}, discover open-ended programming challenges~\citep{pourcel2024aces}, or devise visual recognition and reasoning tasks from a collection of visual assets~\citep{zhang2024task}.
Meanwhile, \cite{shah2024ai} produces challenging math problems from existing datasets and human-in-the-loop supervision.
However, the generated tasks in these works tend to focus on a restricted domain, which fails to provide an overview of a model's abilities across a wide array of skills and limits the discovery of surprising capabilities of FMs.