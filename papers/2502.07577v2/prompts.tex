\section{ACD Prompts}
\label{appsec:prompts}

This section provides the complete set of prompts used by the scientist model in \ouralgolong (\ouralgo), as discussed in \Cref{sec:algo}.

\subsection{Task Creation Prompts}
\label{appsubsec:task_creation_prompts}

The system prompt for task creation instructs the scientist model to propose novel, diverse task families to uncover the subject model's capabilities:

\begin{tcolorbox}[
    breakable,
    boxrule=0.5pt,
    sharp corners,
    fontupper=\small,
    colback=orange!5!white,
    colframe=orange!80!black,
    title={Task Creation System Prompt}
]
You are an expert in designing task families to assess the capabilities of a particular new large language model (LLM).
Your goal is to create novel, diverse task families that can reveal the breadth and depth of the model's skills.

We especially want you to uncover tasks that could reveal surprising capabilities or failures.
Successfully written task families will be added to a task archive.

In each generation, previously generated task families will be provided as context so that you can generate different tasks.
Be creative, explore different domains, and design tasks that can distinguish between models with varying levels of expertise.
Make sure each task is coherent and does not try to combine too many concepts into one.

Each task family should be designed according to the METR Standard, which requires the following Python format:

\begin{lstlisting}
class TaskFamily:
    @staticmethod
    def get_tasks() -> dict[str, dict]:
        # Returns a mapping from task names to arbitrary data that is specific to each task.
        # Each task family should contain 2 tasks in the dictionary, labelled "1" and "2".
        # ... implementation ...

    @staticmethod
    def get_instructions(t: dict) -> str:
        # This method should return a string containing initial task instructions for the agent.
        # ... implementation ...

    @staticmethod
    def score(t: dict, submission: str) -> float | None:
        # `score` is called after the agent submits a solution.
        # Return 0 on failure and 1 on success. Do not give partial scores.
        # Throwing an exception will result in a score of 0.
        # ... implementation ...
\end{lstlisting}

The \texttt{score} function may optionally call a helper function that calls a GPT-4 based LLM judge:

\begin{lstlisting}
# Automated LLM judge helper function
def eval_with_llm_judge(
        instructions: str,  # The instructions for the task
        submission: str,    # The submission to evaluate
        criteria: Optional[List[str]] = None,  # Optional additional criteria
) -> bool:
    # Returns a boolean indicating whether the agent is deemed to have succeeded
    # at the task, and meets any additional criteria.
\end{lstlisting}

You should use this function unless the task is better evaluated through code.
DO NOT RE-IMPLEMENT THIS FUNCTION!
There is no need to repeat any criteria that are already in the instructions.
One possible use of the criteria field is if you already have an answer in mind for the task.
An example usage in \texttt{score} with an optional criteria is as follows:

\begin{lstlisting}
@staticmethod
def score(t: dict, submission: str) -> float | None:
    from src.eval_helper import eval_with_llm_judge
    instructions = TaskFamily.get_instructions(t)
    criteria = ["The response should include the name Alice."]
    return 1.0 if eval_with_llm_judge(instructions, submission, criteria) else 0.0
\end{lstlisting}

Respond precisely in the following format including the JSON start and end markers:\\

\textbf{THOUGHT}: \texttt{<THOUGHT>}

\textbf{RESPONSE JSON}: \texttt{<JSON>}\\

In \texttt{<THOUGHT>}, first briefly think and reason about what kind of task family you want to propose.
Thoughts may also include (but are not limited to): your motivation for investigating the capability, whether you think the model will succeed or fail, its novelty relative to what you have already generated, how to ensure the tasks are valid, and whether it is suitable to invoke an LLM judge for scoring.

In \texttt{<JSON>}, provide a JSON response with the following fields:
\begin{itemize}[leftmargin=2em]
    \item \texttt{"name\_of\_task"}: A concise, descriptive label (lowercase, no spaces, e.g., \texttt{"name\_capital\_city"}).
    \item \texttt{"description\_of\_task"}: A clear explanation of what the task entails (e.g., \texttt{"Return the capital city of a country"}).
    \item \texttt{"capability\_being\_measured"}: The specific LLM capability being evaluated (e.g., knowledge, reasoning, creativity, etc.).
    \item \texttt{"estimated\_human\_difficulty"}: An estimate of the task difficulty on a 1--5 scale (1 = very easy, 5 = very difficult).
    \item \texttt{"done"}: By default, set to \texttt{"False"}. Tasks will only be saved if flagged \texttt{"done"} by the final iteration. Do not mark \texttt{"True"} until you are satisfied.
    \item \texttt{"task\_family"}: The fully implemented Python code for the \texttt{TaskFamily} class. Write good human-readable code.
\end{itemize}

All values in the JSON should be strings.
You may only use standard Python packages and libraries to implement the tasks.
Required library imports should be included either at the top of the file or in the class method where they are used.
DO NOT download additional data from the internet or access the file system.
Your response will be automatically parsed and used for evaluation, so ensure all components MUST be fully implemented and adhere to the METR standard.
\end{tcolorbox}

At each iteration, the scientist model is prompted with:

\begin{tcolorbox}[
    breakable,
    boxrule=0.5pt,
    sharp corners,
    fontupper=\small,
    colback=orange!5!white,
    colframe=orange!80!black,
    title={Initial Task Prompt}
]
A previous generated task family that the agent succeeded at is provided below (with code):
\begin{verbatim}
{prev_json}
\end{verbatim}

Summaries of other previously generated tasks for context are:
\begin{verbatim}
{other_task_jsons}
\end{verbatim}

Remember if a previous task failed, either the agent could not solve it or the task was incorrectly implemented.
Generate the next interestingly new task family.
\end{tcolorbox}

If the scientist model requires multiple refinements (e.g., due to errors or repeated failure by the subject), it sees:

\begin{tcolorbox}[
    breakable,
    boxrule=0.5pt,
    sharp corners,
    fontupper=\small,
    colback=orange!5!white,
    colframe=orange!80!black,
    title={Task Creation Reflexion Prompt}
]
Current round = \{\texttt{current\_round}\}/\{\texttt{num\_rounds}\}.

In your thoughts, first carefully consider the correctness of the task family you just created. Factors may include but are not limited to:
\begin{itemize}[leftmargin=2em]
    \item Whether sufficient synthetic data has been given in the instructions to challenge the models.
    \item Incomplete class definitions, syntax errors, or misplaced import statements.
    \item Feasibility of the evaluation criteria.
    \item Any ambiguity or issues in the clarity of the tasks.
    \item The consistency and accuracy of all fields in the JSON.
    \item Whether precisely two entries in \texttt{get\_tasks} have been defined.
    \item Whether agents can get a successful score in an unintended manner.
    \item Whether the desired response format for the agent has been given in the instructions.
    \item Under no circumstances should the instructions contain the answer or direct hints. Ensure that the task remains challenging without giving away the solution.
    \item If the agent fails, it may be due to the difficulty of the task, and that is acceptable as long as the task is valid.
\end{itemize}

Here is the result of attempting to evaluate the tasks on the given model:
\begin{verbatim}
{eval_response}
\end{verbatim}

In the next attempt, try to improve the implementation. Respond in the same JSON format as before.
If there is nothing to improve, simply repeat the previous response \textbf{EXACTLY} after the thought and set the \texttt{"done"} flag to \texttt{"True"}.
Do not set \texttt{"done"} to \texttt{"True"} if you are still improving the task on the current turn.
Do not set \texttt{"done"} to \texttt{"True"} on the final step if there are major issues with the task family.
\end{tcolorbox}

\subsection{Evaluation Prompts}
\label{appsubsec:evaluation_prompts}

We can configure the subject model to provide an answer in a zero-shot style or to show chain-of-thought reasoning. For zero-shot style:

\begin{tcolorbox}[
    breakable,
    boxrule=0.5pt,
    sharp corners,
    fontupper=\small,
    colback=orange!5!white,
    colframe=orange!80!black,
    title={Zero-Shot Evaluation System Prompt}
]
You are a helpful assistant.
You are given a task to complete.

Please provide your final answer directly starting with "Answer: ".
Only the text after "Answer: " will be evaluated.
This will be automatically parsed so ensure you follow the format exactly.
Respect any other output constraints given in the task prompt after "Answer: ".
\end{tcolorbox}

To solicit chain-of-thought~\citep{wei2022chain} reasoning, we instruct the model as follows:

\begin{tcolorbox}[
    breakable,
    boxrule=0.5pt,
    sharp corners,
    fontupper=\small,
    colback=orange!5!white,
    colframe=orange!80!black,
    title={Chain-of-Thought Evaluation System Prompt}
]
You are a helpful assistant.
You are given a task to complete.

First, reason about the problem and your plan for solving the task, and then provide your final answer starting with "Answer: ".
Only the text after "Answer: " will be evaluated.
This will be automatically parsed so ensure you follow the format exactly.
Respect any other output constraints given in the task prompt after "Answer: ".
\end{tcolorbox}

\subsection{Task Embedding Prompt}
\label{appsubsec:task_embedding_prompt}

We generate embeddings using \texttt{text-embedding-3-small}~\citep{text_embed_openai} for each new task to facilitate clustering and retrieval. The prompt used is:

\begin{tcolorbox}[
    breakable,
    boxrule=0.5pt,
    sharp corners,
    fontupper=\small,
    colback=orange!5!white,
    colframe=orange!80!black,
    title={Task Embedding Prompt}
]
Name of task family: \{\texttt{name\_of\_task}\}

Description: \{\texttt{description\_of\_task}\}

Capability being measured: \{\texttt{capability\_being\_measured}\}

Estimated human difficulty: \{\texttt{estimated\_human\_difficulty}\}

Example instruction: \{\texttt{example\_question}\}

Agent succeeded at task: \{\texttt{agent\_succeeded}\}
\end{tcolorbox}

\subsection{Novelty Assessment Prompts}
\label{appsubsec:novelty_assessment_prompts}

To determine whether a newly generated task is interestingly new compared to existing tasks in the archive, we use the following system prompt:

\begin{tcolorbox}[
    breakable,
    boxrule=0.5pt,
    sharp corners,
    fontupper=\small,
    colback=orange!5!white,
    colframe=orange!80!black,
    title={Interestingly New System Prompt}
]
You are an expert in designing task families to assess the capabilities of a particular new large language model (LLM).
You will be given a newly created task family that you just generated along with its closest saved neighbours.
Determine whether the task family is interestingly new and should be added to the task archive.

Factors may include but are not limited to:
\begin{itemize}[leftmargin=2em]
    \item The novelty of the subject area
    \item The difficulty of the tasks
    \item The capabilities being measured
    \item Whether the agent succeeded at the task
\end{itemize}

First, briefly think and reason about the task family in relation to the existing closest task families.

Then, provide your decision as "Decision: Yes" or "Decision: No" to indicate whether the task family should be added to the task archive.

Your response will be automatically parsed, so ensure you follow the above format exactly.
\end{tcolorbox}

We then prompt with:

\begin{tcolorbox}[
    breakable,
    boxrule=0.5pt,
    sharp corners,
    fontupper=\small,
    colback=orange!5!white,
    colframe=orange!80!black,
    title={Interestingly New Prompt}
]
You have just generated the task family:

\begin{verbatim}
{new_task}
\end{verbatim}

The closest task families that have already been created are:

\begin{verbatim}
{closest_tasks}
\end{verbatim}

Is the new task family interestingly new and should be added to the task archive?
\end{tcolorbox}

To make evaluations consistent across different scientists, we fix GPT-4o as the ``interestingly new'' checker.

\subsection{LLM Judge Prompts}
\label{appsubsec:llm_judge_prompt}

For tasks where \texttt{score} calls a GPT-4o-based judge, we use the following system and user prompts. (See the main paper \Cref{subsec:definition_of_task_families} for context.)

\begin{tcolorbox}[
    breakable,
    boxrule=0.5pt,
    sharp corners,
    fontupper=\small,
    colback=orange!5!white,
    colframe=orange!80!black,
    title={LLM Judge System Prompt}
]
You are an impartial judge evaluating the quality of responses provided by an AI assistant.
You will be provided the instructions for the task and the assistant's submission in response.
You will decide whether or not the agent succeeded at the task.
Please consider factors such as helpfulness, relevance, accuracy, depth, creativity, and level of detail of the response.
You may additionally be given a set of criteria to evaluate the response against.
Do not allow the length of the response to influence your decision.

Respond precisely in the following format including the JSON start and end markers:\\

\textbf{THOUGHT}: \texttt{<THOUGHT>}

\textbf{RESPONSE JSON}: \texttt{<JSON>}\\

In \texttt{<THOUGHT>}, first briefly reason about the agent's response and whether it meets the additional criteria.

In \texttt{<JSON>}, provide a JSON response with only the following key:
\begin{itemize}[leftmargin=2em]
    \item \texttt{"decision"}: Your answer as a string, either \texttt{"Yes"} or \texttt{"No"}.
\end{itemize}
\end{tcolorbox}

\begin{tcolorbox}[
    breakable,
    boxrule=0.5pt,
    sharp corners,
    fontupper=\small,
    colback=orange!5!white,
    colframe=orange!80!black,
    title={LLM Judge Prompt}
]
Instruction: \{\texttt{instructions}\}

Submission: \{\texttt{submission}\}

Additional Evaluation Criteria:

\begin{verbatim}
{criteria}
\end{verbatim}
\end{tcolorbox}
