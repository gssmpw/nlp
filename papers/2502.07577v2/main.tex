%

\documentclass{article}

\PassOptionsToPackage{dvipsnames,svgnames}{xcolor}
\PassOptionsToPackage{sort, compress}{natbib}

%
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} %
\usepackage{pdfpages}

%
%
%
\usepackage{hyperref}

%
\newcommand{\theHalgorithm}{\arabic{algorithm}}

%
\usepackage[accepted]{include/icml2025}

%
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

%
\usepackage[capitalize,noabbrev]{cleveref}

%
%
%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

%
%
%
\usepackage[textsize=tiny]{todonotes}


%
%
\icmltitlerunning{Automated Capability Discovery via Foundation Model Self-Exploration}

%
\usepackage[utf8]{inputenc} %
\usepackage[T1]{fontenc}    %
\usepackage{hyperref}       %
\usepackage{url}            %
\usepackage{booktabs}       %
\usepackage{amsfonts}       %
\usepackage{nicefrac}       %
\usepackage{microtype}      %
\usepackage[dvipsnames,svgnames]{xcolor}         %

\usepackage{listings}
\lstset{
basicstyle=\small\ttfamily,
columns=flexible,
breaklines=true,
breakautoindent=false,
breakindent=0ex,
xleftmargin=.3in,
}

\lstdefinestyle{pythonstyle}{
    language=Python,
    basicstyle=\ttfamily\small,
    keywordstyle=\color{blue},
    commentstyle=\color{green!50!black},
    stringstyle=\color{red},
    showstringspaces=false,
    numbers=left,
    numberstyle=\tiny\color{gray},
    frame=single,
    breaklines=true,
    tabsize=4,
}

\usepackage{amsmath}
\usepackage{xspace}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{colortbl}
\usepackage{cleveref}
\usepackage{graphicx}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{subcaption}
\usepackage{wrapfig}
\usepackage{sidecap}
\usepackage{soul}
\usepackage{enumitem}

\newlist{todolist}{itemize}{2}
\setlist[todolist]{label=$\square$}
\setlist[itemize]{noitemsep, topsep=0pt, leftmargin=0.7cm}
\setlist[enumerate]{noitemsep, topsep=0pt, leftmargin=0.7cm}

\sidecaptionvpos{figure}{t}
\usepackage{multicol}
\usepackage{pifont}
\usepackage[normalem]{ulem}
\usepackage{titletoc}

\newcommand{\xmark}{\ding{55}}

\usepackage[most]{tcolorbox}

\newcommand{\fon}[1]{\fontfamily{#1}\selectfont}

\tcbset {
  base/.style={
    arc=0mm, 
    bottomtitle=-0.25mm,
    boxrule=0mm,
    colbacktitle=black!10!white, 
    coltitle=black, 
    fonttitle=\bfseries, 
    left=2.5mm,
    leftrule=1mm,
    right=3.5mm,
    title={#1},
    toptitle=0.25mm,
  }
}

\definecolor{brandblue}{rgb}{0.34, 0.7, 1}
\newtcolorbox{mybox}[1]{
  colframe=brandblue, 
  base={#1}
}


\newcommand{\ouralgolong}{\textsc{Automated Capability Discovery}\xspace}
\newcommand{\ouralgolonglower}{Automated Capability Discovery\xspace}
\newcommand{\ouralgo}{\textsc{ACD}\xspace}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\addtiny}[1]{\footnotesize{#1}}
\newcommand{\negphantom}[1]{\settowidth{\dimen0}{#1}\hspace*{-\dimen0}}
\definecolor{dodgerblue}{rgb}{0.12, 0.56, 1.0}

\newcommand{\cl}[1]{\textcolor{red}{CL: {#1}}}
\newcommand{\clout}[1]{\textcolor{red}{CL: \sout{#1}}}
\newcommand{\sh}[1]{\textcolor{blue}{SH: {#1}}}
\newcommand{\shout}[1]{\textcolor{blue}{SH: \sout{#1}}}
\newcommand{\jc}[1]{\textcolor{brown}{Jeff: {#1}}}
\newcommand{\jcout}[1]{\textcolor{brown}{Jeff: \sout{#1}}}


\begin{document}

\twocolumn[
\icmltitle{Automated Capability Discovery via Foundation Model Self-Exploration}
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Cong Lu}{equal,ubc,vector}
\icmlauthor{Shengran Hu}{equal,ubc,vector}
\icmlauthor{Jeff Clune}{ubc,vector,cifar}
\end{icmlauthorlist}

\icmlaffiliation{ubc}{University of British Columbia}
\icmlaffiliation{vector}{Vector Institute}
\icmlaffiliation{cifar}{Canada CIFAR AI Chair}

\icmlcorrespondingauthor{Cong Lu}{conglu@cs.ubc.ca}

%
%
%
\icmlkeywords{large language models, foundation models, automated evaluation, model self-exploration}

\vskip 0.3in
]

\printAffiliationsAndNotice{\icmlEqualContribution}
%

\begin{abstract}
Foundation models have become general-purpose assistants, exhibiting diverse capabilities across numerous domains through training on web-scale data.
It remains challenging to precisely characterize even a fraction of the full spectrum of capabilities and potential risks in any new model.
Existing evaluation approaches often require significant human effort, and it is taking increasing effort to design ever harder challenges for more capable models.
We introduce \ouralgolong (\ouralgo), a framework that designates one foundation model as a \emph{scientist} to systematically propose open-ended tasks probing the abilities of a \emph{subject} model (potentially itself).
By combining frontier models with ideas from the field of open-endedness, \ouralgo automatically and systematically uncovers both surprising capabilities and failures in the subject model.
We demonstrate \ouralgo across a range of foundation models (including the GPT, Claude, and Llama series), showing that it automatically reveals thousands of capabilities that would be challenging for any single team to uncover.
We further validate our method's automated scoring with extensive human surveys, observing high agreement between model-generated and human evaluations.
By leveraging foundation models' ability to both create tasks and self-evaluate, \ouralgo is a significant step toward scalable, automated evaluation of novel AI systems.
All code and evaluation logs are open-sourced at \url{https://github.com/conglu1997/ACD}.
\end{abstract}

\section{Introduction}
\label{sec:intro}
Large Language Models (LLMs; \citealp{openai2024gpt4, geminiteam2024gemini, touvron2023llama}), trained on internet-scale datasets, have revolutionized natural language processing by demonstrating strong general-purpose capabilities.
These ``Foundation Models'' (FMs; \citealp{Bommasani2021FoundationModels}) display exceptional performance on tasks requiring common-sense knowledge~\citep{talmor2019commonsense}, reasoning~\citep{wei2022chain}, and comprehension~\citep{chang2024survey}, enabling applications ranging from conversational agents~\citep{brown2020language} to code generation~\citep{aider}.
Recently, agentic systems powered by foundation models have even shown the capacity to propose and investigate new scientific ideas~\citep{lu2024aiscientist} and provide ever-better agentic systems~\citep{hu2024automateddesignagenticsystems}.
However, identifying unknown abilities or failure modes in FMs remains a major challenge, especially because such knowledge is crucial to ensuring both safe deployment and maximizing real-world utility.

\begin{figure*}[t!]
\centering
\vspace{-3mm}
%
\includegraphics[width=0.95\textwidth]{figures/acd_overview.pdf}
%
\vspace{-4mm}
\caption{
\small
\textbf{(Left)} Humans typically evaluate novel foundation models through trial and error, alongside benchmarks. They often discover new surprising capabilities or failures: like counting how many ``r''s are in ``strawberry'' or identifying which is bigger, $0.9$ or $0.11$.
\textbf{(Center and Right)} \ouralgolong (\ouralgo) mirrors human evaluation efforts by using a \emph{scientist} model to automatically discover and assess the capabilities of a \emph{subject} model in an open-ended manner.
Illustrated here are two \textcolor{Red}{\textbf{surprising failures}} (the model fails to perform three arithmetic operations in sequence, and fails to correctly continue a symbol pattern with `\#\#\#') and a selected \textcolor{DarkGreen}{\textbf{success}} (the model successfully solves a variant of Einstein's riddle with 17 clues) uncovered by \ouralgo on GPT-4o.
See \Cref{appsubsec:gpt4o_manual_selection} for more examples. Models are evaluated using chain-of-thought~\citep{wei2022chain}.
}
\label{fig:overview}
\vspace{-4mm}
\end{figure*}

Traditional evaluation techniques—centered around human-created benchmarks~\citep{mmlu, bigbench, gsm8k, humaneval, math_benchmark, drop, hellaswag,phan2025hle}—are labor-intensive to create and limited by predefined categories, often failing to capture the full spectrum of a model's capacities or unearthing surprising new behaviors pre-deployment.
Moreover, as models become more advanced, they may saturate or overfit these benchmarks, so those metrics may not reflect broader performance gains.
Users also commonly encounter unique use cases and failure modes not covered by benchmarks.
While frequently updating or creating new test suites~\citep{livebench, phan2025hle} attempts to address these issues, continually devising new tasks is expensive, not model-specific and will fail to probe the `unknown unknowns' (things that benchmark creators do not think to include).
This underscores the need for scalable, efficient evaluation methods that are cheap and require minimal overhead to keep pace with rapidly evolving foundation models~\citep{bowman2022measuringprogressscalableoversight}.

We introduce \textit{\ouralgolong} (\ouralgo), a framework that augments existing evaluation approaches by automating the discovery of a foundation model's capabilities and failure modes.
It designates one model as a \emph{scientist} to systematically propose open-ended tasks for a \emph{subject} model, which could be itself or a different foundation model (\Cref{sec:algo}).
Concretely, \ouralgo instructs the scientist to propose \emph{interesting new} challenges~\citep{zhang2024omni, faldor2024omni, lu2024aiscientist, pourcel2024acesgeneratingdiverseprogramming, zhang2024task, shah2024aiassistedgenerationdifficultmath}, asks the subject to attempt them, and evaluates performance~\citep{zheng2023judging}, all automatically.
This mirrors how humans might try everything from their favorite model gotcha questions to new challenging problems when exploring a new model—though with \ouralgo, the model takes on the role of evaluator.
By removing manual task design from the process, \ouralgo can automatically and relatively inexpensively expose a wide range of strengths and weaknesses in the subject model.

We demonstrate \ouralgo on several foundation models, including GPT-4o~\citep{openai2024gpt4}, Claude Sonnet 3.5~\citep{claude3}, and Llama3-8B~\citep{llama3} (\Cref{sec:eval}).
We show that \ouralgo uncovers a large variety of capabilities, ranging from elementary arithmetic tasks to complex puzzle solving, resulting in thousands of automatically discovered tasks.
Many tasks illustrate useful model capabilities, such as multi-step reasoning and structured workflows, whereas others reveal surprising failure modes that would seem trivial to humans (\Cref{fig:overview}).
We provide numerous examples in our evaluations, spanning cryptography, code generation, memory-based logic, advanced mathematics, legal queries, puzzle design, and creative writing (\Cref{appsubsec:gpt4o_manual_selection}). 
To validate \ouralgo's automated task generation and scoring, we conduct large-scale human surveys on tasks discovered by GPT-4o, showing high rates of tasks being deemed valid and agreement between the model's self-evaluation and human judgments (\Cref{subsec:gpt4_eval}).
Furthermore, \ouralgo automatically compiles a concise \textbf{``Capability Report''} of discovered capabilities and failure modes (\Cref{sec:report_generation}), enabling quick inspection and easier dissemination of results or flagging issues pre-deployment (\Cref{sec:safety}).

By harnessing the capacity of foundation models to self-assess, \ouralgo paves the way for scalable, automated evaluation of these models.
It can help systematically identify emergent and potentially concerning behaviors before real-world deployment.
As foundation models continue to advance, techniques like \ouralgo will be crucial to align their development with human values and ensure responsible use by uncovering beneficial and risky behaviors before real-world deployment.
Finally, \ouralgo could enable models to generate interesting challenges for themselves to learn on, potentially driving self-improvement in the future~\citep{faldor2024omni,clune2019aigas}.

\begin{figure*}[t!]
\centering
\vspace{-2mm}
\includegraphics[width=0.95\textwidth]{figures/gpt4o_vis.pdf}
\vspace{-3mm}
\caption{
\small
Task families discovered by \ouralgolong on GPT-4o (serving as both \emph{scientist} and \emph{subject}) over 5000 generations.
Each point represents one of the 1330 tasks that passed the ``interestingly new'' filter, visualized in 2D via t-SNE.
\ouralgo enables GPT-4o to \emph{self-discover} diverse capabilities and failure modes, with tasks that cluster into \textbf{25} high-level categories \textcolor{DarkBlue}{(different colors, listed in \Cref{appsubsec:cluster_listings})}, spanning \emph{puzzle-solving, code generation, scientific reasoning, creative writing, and legal interpretation}.
See \Cref{sec:algo,subsec:gpt4_eval} for full details, and \Cref{appsubsec:gpt4o_manual_selection} for selected examples.
}
\label{fig:task_viz_gpt}
\vspace{-3mm}
\end{figure*}

\section{Background}
\label{sec:background}
\subsection{Open-ended Discovery Algorithms}
\label{subsec:open_background}

Open-ended algorithms~\citep{stanley2015greatness,stanley2017open} aim to continuously generate novel and diverse artifacts~\citep{pmlr-v235-hughes24a} within a search space, rather than focusing on a fixed objective.
These algorithms emulate human creativity by autonomously exploring new artifacts, increasingly supported by large foundation models that can encode intrinsic notions of ``interestingness''~\citep{zhang2024omni, faldor2024omni,lu2024intelligentgoexplorestandingshoulders}.
They have been applied to evolving novel robot morphologies in code~\citep{lehman2022evolutionlargemodels}, generating new reinforcement learning environments~\citep{faldor2024omni,wang2019paired,wang2020enhanced}, discovering novel loss functions~\citep{lu2024discopop} and agentic systems~\citep{hu2024automateddesignagenticsystems}, and investigating scientific hypotheses~\citep{lu2024aiscientist}.

Generally, these algorithms maintain and update an archive $\mathcal{A}$ of discovered artifacts.
At iteration $t$, they sample a new artifact $a_t$ from a foundation model $M$ conditioned on a subset $C_{t-1}$ of previously discovered artifacts, typically limited in size for computational feasibility.
The generated artifact $a_t$ is evaluated for novelty (e.g., via embedding-based similarity), and then added to the archive if sufficiently different from others in $\mathcal{A}$.
\ouralgo adapts these principles to systematically reveal a foundation model's capabilities, treating each novel \textit{capability or failure} as a generated ``artifact''.

\section{Related Work}
\label{sec:related}

\textbf{Open-Ended Discovery with Foundation Models.}
The field of open-endedness~\citep{stanley2019open} aims to continually discover diverse and novel artifacts forever.
Recent methods leverage the generative capabilities and vast prior knowledge of foundation models (FMs) to accelerate this process~\citep{zhang2024omni,faldor2024omni,lehman2022evolutionlargemodels,hu2024automateddesignagenticsystems} by harnessing a foundation model's intrinsic notion of interestingness~\citep{zhang2024omni, faldor2024omni,lu2024intelligentgoexplorestandingshoulders,hu2024automateddesignagenticsystems} to construct the next proposal, analogous to human innovation.
Notable examples include ELM~\citep{lehman2022evolutionlargemodels} which evolves novel robot morphologies; OMNI-EPIC~\citep{faldor2024omni}, which automatically designs novel environments for reinforcement learning (RL) agents; DiscoPOP which discovers new loss functions for preference optimization algorithms~\citep{lu2024discopop}; ADAS~\citep{hu2024automateddesignagenticsystems}, which evolves novel designs for LLM-based agentic systems; and The AI Scientist~\citep{lu2024aiscientist}, which seeks to automate the entire scientific process by proposing novel ideas, conducting experiments, and writing a scientific paper summarizing the results.

\textbf{Automated Evaluation of Foundation Models.} Recent research also investigates automated evaluation of FMs, moving beyond static, human-designed test suites.
Rainbow Teaming~\citep{samvelyan2024rainbow} applies Quality-Diversity algorithms~\citep{mouret2015illuminatingsearchspacesmapping,pugh2016quality} to find novel adversarial attacks that stress-test FMs for safety.
Similarly, works like~\citep{zheng2024ali, zhou2024autoredteamer, jiang2024automatedRedTeaming, pavlova2024automatedRedTeaming} automate the red teaming (probing a system for weaknesses) process.
These works expand the comprehensiveness of existing safety checks but do not have the ability to create new tasks that might reveal completely unknown FM capabilities.
Other techniques generate new debate topics and evaluate FMs through multi-round debate between them~\citep{zhao2024auto}, discover open-ended programming challenges~\citep{pourcel2024aces}, or devise visual recognition and reasoning tasks from a collection of visual assets~\citep{zhang2024task}.
Meanwhile, \cite{shah2024ai} produces challenging math problems from existing datasets and human-in-the-loop supervision.
However, the generated tasks in these works tend to focus on a restricted domain, which fails to provide an overview of a model's abilities across a wide array of skills and limits the discovery of surprising capabilities of FMs.

\section{\ouralgolonglower}
\label{sec:algo}

Given a foundation model we wish to evaluate (the \emph{subject}), \ouralgolong (\ouralgo) designates another foundation model as a \emph{scientist} to propose new tasks and then evaluate how well the subject model performs.
The scientist and subject could be the same model or different, but in either case, they are both foundation models, so we refer to this as ``foundation model self-exploration.''
By iteratively refining tasks to uncover interesting or surprising outcomes, \ouralgo aims to automate much of the process of revealing a model's capabilities.
Below, we outline the key stages of \ouralgo. 
(\emph{See \Cref{appsec:prompts} for the full \ouralgo prompts.})

\subsection{Definition of Task Families}
\label{subsec:definition_of_task_families}

We adopt a simplified version of the METR Task Standard~\citep{metr}, an established format for packaging tasks to evaluate foundation models. 
In particular, \ouralgo instructs the scientist to define and generate broad ``task families'' as a systematic way to cover entire categories of capabilities—ranging from simple knowledge recall to more complex reasoning or coding.
Each family has metadata which includes a name, a description, and the exact capability being measured.
\Cref{tab:seed_task_family_metadata} shows an example of how such metadata is seeded for a trivial ``Hello World''-style string repetition task.

\begin{table}[h!]
\centering
\vspace{-1mm}
\caption{
\small
Example metadata for a simple ``Hello World'' task family.}
\vspace{-2mm}
\label{tab:seed_task_family_metadata}
\begin{tabular}{ll}
\toprule
\textbf{Key} & \textbf{Value} \\
\midrule
name & hello\_world \\
description & return a greeting string \\
capability being measured & basic string manipulation \\
\bottomrule
\end{tabular}
\vspace{-1mm}
\end{table}

We leverage the LLM's coding abilities to translate high-level task descriptions into Python code that can be automatically evaluated.
Each task family~\citep{metr} is structured with:
\begin{enumerate}[labelsep=0.5em, itemsep=0.2em, left=0pt, topsep=0em, partopsep=0em]
\item \textbf{Specific Task Instances}: Subtasks are generated with unique data to probe different nuances of the same capability.
\item \textbf{Instruction Provision}: Each subtask includes instructions for the subject model. 
\item \textbf{Scoring Mechanism}: A programmatic check for tasks with a single correct answer, or a GPT-4o-based judge~\citep{zheng2023judging} if the task requires more open-ended judgment (\Cref{appsubsec:eval_free_form}).
\end{enumerate}
\Cref{appsubsec:example_family_code} shows a full code snippet for the ``Hello World'' example in \Cref{tab:seed_task_family_metadata}. This task family may include the strings ``Hello, world!'' or ``Greetings, universe!'' as subtasks, the instructions to the subject model may be ``Please repeat the following message exactly as it is: \{...\}'', and the scoring mechanism may be an exact string comparison.
For more open-ended tasks, we demonstrate that using foundation models as open-ended automated judges can work, since often it is easier to recognize the successful solution to a particular task than generate one.

\subsection{Generating Tasks}
\label{subsec:generating_tasks}

Following principles from the field of open-endedness (\Cref{subsec:open_background}), \ouralgo operates in a loop:
\begin{enumerate}[leftmargin=1.2em]
    \item \textbf{Maintain an Archive:} An archive~\citep{mouret2015illuminatingsearchspacesmapping,lehman2011novelty} of tasks that have been discovered thus far is kept. It is seeded with trivial tasks (like those in \Cref{subsec:definition_of_task_families}). At each iteration, the scientist sees a randomly sampled subset of these tasks as context.
    \item \textbf{Propose a New Task Family:} The scientist proposes a new task family (written in Python code), using chain-of-thought~\citep{wei2022chain} and self-reflection~\citep{shinn2023reflexion} to catch errors (\Cref{appsec:prompts}). 
    During self-reflection, the scientist also checks how easily the subject solves the current task family and adapts difficulty accordingly.
    \item \textbf{Filter for Novelty:} The scientist discards proposals that overlap too closely with existing tasks, by considering whether the task is ``interestingly new''~\citep {zhang2024omni} with respect to its nearest neighbors computed via \texttt{text-embedding-3-small}~\citep{text_embed_openai} (\Cref{appsubsec:task_embedding_prompt}).
    \item \textbf{Test the Subject Model:} The subject attempts these tasks using chain-of-thought (\Cref{appsubsec:evaluation_prompts}) as a lightweight way to elicit greater capabilities from the FM. The scientist uses $n$-shot evaluation to robustly score each task. All completed tasks are stored in the archive, logged as ``discovered capabilities'' when consistently solved or ``failure modes'' when consistently failed.
\end{enumerate}
We can repeat these steps for thousands of iterations until sufficiently many task families have been discovered.

\section{Empirical Evaluation}
\label{sec:eval}
We now demonstrate \ouralgo's performance in discovering diverse capabilities across several foundation models, including GPT-4o~\citep{openai2024gpt4}, Claude Sonnet 3.5~\citep{claude3}, and Llama3-8B~\citep{llama3}.
First, we provide an in-depth examination of GPT-4o acting as both scientist and subject, followed by experiments with different scientist-subject pairings and cross-model analyses.
We run our algorithm for 5000 generations for all evaluations.
Further details on hyperparameters and evaluation protocols appear in \Cref{appsec:hyperparameters,appsec:human_survey}.

\subsection{Case Study and Human Evaluation on GPT-4o}
\label{subsec:gpt4_eval}

We begin by analyzing \ouralgo with GPT-4o serving as both scientist and subject.
In \Cref{fig:task_viz_gpt}, we visualize all discovered tasks by embedding each task's description in a 2D t-SNE~\citep{van2008visualizing} plot, grouped by HDBSCAN~\citep{mcinnes2017hdbscan}.
From these 5000 generations, we discover 1330 interestingly new tasks, which fall into 25 distinct clusters (\Cref{tab:gpt4o_gpt4o_clusters} in \Cref{appsubsec:cluster_listings}).
The tasks span wide-ranging areas, including puzzle-solving and creation (e.g., Sudoku, logic riddles, custom word puzzles), code generation and debugging, advanced math, creative writing, and legal text interpretation. 
We provide many examples from our evaluations, spanning cryptography, code generation, memory-based logic, advanced mathematics, legal queries, puzzle design, and creative writing in \Cref{appsubsec:gpt4o_manual_selection}.

Here, we examine a few in detail. \Cref{fig:overview}~(right) highlights three surprising tasks discovered by \ouralgo that reveal GPT-4o sometimes fails at seemingly trivial operations.
For instance, it incorrectly computes a sequence of three arithmetic operations and fails to continue a simple pattern for three symbols as requested.
On the other hand, it correctly solves a novel variant of Einstein's riddle (that we could not find online) with 17 logical clues, demonstrating unexpectedly strong multi-step reasoning.
We show two further discovered tasks below, which illustrate a surprisingly good spatial reasoning success \textcolor{DarkGreen}{\textbf{(left box)}} and a puzzling failure to follow basic, multi-step instructions \textcolor{Red}{\textbf{(right box)}}.
These discovered tasks show that \ouralgo can \emph{entirely autonomously} pinpoint subtle failure modes that GPT-4o exhibits, just as human evaluators might, at a low cost of approximately \$0.1 per task created (\Cref{appsubsec:cost}).
Such insights could strongly inform future training or target fixes prior to user deployment.

\begin{center}
\begin{minipage}[h]{0.54\linewidth}
\begin{tcolorbox}[
colback=green!10,
colframe=green!50!black,
boxrule=0.5pt,
sharp corners,
left=3pt, right=3pt, top=3pt, bottom=3pt,
title=Surprising Spatial Reasoning,
fonttitle=\tiny,
fontupper=\tiny,
equal height group=boxgroup
]
\textbf{Instructions:}\\
Your task is to visualize and describe the 3D object based on 2D projections. Projections:\\
\begin{itemize}[leftmargin=2em,itemsep=0.3em,topsep=0.3em]
\item Front view: A square base with a triangular top. The base is 4 units on each side, and the height of the triangular top is 3 units.
\item Side view: A rectangle with a triangular top. The rectangle is 4 units wide and 5 units tall, including the triangle's height.
\item Top view: A square with 4 units on each side.
\end{itemize}

\textbf{GPT-4o's Response:}\\
The object is a square pyramid with a base that is a square with 4 units on each side. 
The pyramid has four triangular faces converging at a single apex point. 
The height of the pyramid from the base to the apex is 3 units. 
The total height of the object, including the base and the apex, is 5 units. 
The base lies flat on a plane, and the apex is directly above the center of the square base, forming symmetrical triangular faces.
\end{tcolorbox}
\end{minipage}
\hspace{0.02\linewidth}
\begin{minipage}[h]{0.40\linewidth}
\begin{tcolorbox}[
colback=red!10,
colframe=red!50!black,
boxrule=0.5pt,
sharp corners,
left=3pt, right=3pt, top=3pt, bottom=3pt,
title=Unable to Follow Simple Instructions in Sequence,
fonttitle=\tiny,
fontupper=\tiny,
equal height group=boxgroup
]
\textbf{Instructions:}\\
Your task is to perform the following text transformations on the given input text: ``The quick brown fox jumps over the lazy dog.''\\

Transformations:
\begin{enumerate}[leftmargin=2em,itemsep=0.3em,topsep=0.3em]
\item Reverse the order of words
\item Capitalize the first letter of each word
\item Replace 'dog' with 'cat'
\end{enumerate}

\textbf{GPT-4o's Response:}\\
Dog. Lazy The Over Jumps Fox Brown Quick The
\end{tcolorbox}
\end{minipage}
\end{center}

\begin{figure}[t!]
\centering
\vspace{-2mm}
\includegraphics[width=\columnwidth]{figures/gpt4o_human_eval.pdf}
\vspace{-8mm}
\caption{
\small
Human evaluation results for tasks discovered by \ouralgo when GPT-4o serves as both \emph{scientist} and \emph{subject} (\Cref{sec:algo}).
(a) Most automatically generated tasks were judged clear and valid. (b) The confusion matrix shows that automated scoring largely agrees with human judgments, but exhibits a slight positive bias (more false positives than false negatives). The F1 score of LLM-Human agreement is quite high at 0.86.
(c) The automated judge's F1 is reasonably high for tasks rated by humans as ``Very Easy'' to ``Difficult,'' though it drops on ``Very Difficult'' tasks.
Error bars are 95\% bootstrapped confidence intervals across generated task families.
}
\label{fig:human_eval}
\vspace{-6mm}
\end{figure}

\textbf{Human Evaluation.}
To verify that these generated tasks are coherent and to assess the accuracy of automated scoring at scale, we conducted a human survey via CloudResearch (details in \Cref{appsec:human_survey}).
Participants were shown each task's short description, the exact instructions, and the subject model's final submission.
They then evaluated whether the task was valid and coherent, estimated the difficulty on a five-point scale (``Very Easy'' through ``Very Difficult''), and judged whether the model's submission was correct.

\Cref{fig:human_eval}(a) shows that 92.2\% of tasks were considered clear and valid by human participants—that is, the instructions were judged to align well with the task description and the capability being measured. Additionally, as seen in \Cref{fig:human_eval}(b), the automated scoring largely agrees with human judgments (with an F1 score of 0.86). We observe a slight positive bias from the automated judge, consistent with previous findings~\citep{zheng2023judging}. Finally, \Cref{fig:human_eval}(c) breaks down F1 scores (the harmonic mean of precision and recall, typically preferred over accuracy when dealing with class imbalances) by user-estimated difficulty level, indicating good agreement on easier tasks but lower agreement on very difficult ones. \Cref{sec:judge_failures} provides some representative failure examples of the FM judge.

Even after thousands of iterations, \ouralgo continues to discover novel task families at a high rate (around 20\% of newly proposed tasks are considered interestingly new even after 5000 generations; \Cref{fig:discovery_rate_by_generation}), suggesting \ouralgo has not fully uncovered GPT-4o's capabilities.
Repeated runs across different seeds also yield a consistent final collection of discovered tasks (\Cref{fig:gpt4o_3seeds}), showing that \ouralgo can generate stable ``capability signatures'' for a given model.
\Cref{fig:success_rates_and_task_counts} shows that the \ouralgo scientist can discover tasks across each difficulty category.

\subsection{Varying the Subject Model and Cross-Model Analysis}
\label{subsec:varying_subject}

\begin{figure}[t!]
\centering
\vspace{2mm}
\includegraphics[width=\columnwidth]{figures/cluster_radar_mpl.pdf}
\vspace{-4mm}
\caption{
\small
Comparison between GPT-4o \textcolor{Cerulean}{\textbf{(blue)}} and Llama3-8B \textcolor{BurntOrange}{\textbf{(orange)}} on the tasks originally discovered by GPT-4o.
Each radial axis corresponds to a major task cluster (listed in \Cref{tab:gpt4o_gpt4o_clusters}), with the radius indicating each model's success rate.
We observe that the performance of Llama3-8B is nearly a complete subset of GPT-4o but has a few areas where the gap is narrower (e.g.\ imaginative or open-ended text generation).
This illustrates how a single ACD-curated archive can provide a detailed, high-level visual snapshot of the overall capabilities of newly developed models. 
\emph{Task cluster names are very small; zoom in to see them.}
}
\label{fig:radar_llama_eval}
\vspace{-4mm}
\end{figure}

We next investigate how a single repository of discovered tasks for GPT-4o might generalize to testing a different, weaker subject model. 
Concretely, we take all 1330 tasks discovered when GPT-4o served as both scientist and subject (\Cref{subsec:gpt4_eval}) and re-evaluate Llama3-8B~\citep{llama3} on these tasks without altering any instructions or scoring functions. 
\Cref{fig:radar_llama_eval} summarizes the performance gap across several broad task clusters. 
We find that Llama3-8B struggles substantially more than GPT-4o on categories requiring multi-step reasoning or structured workflows, though the gap is narrower in creative tasks such as imaginative writing.
These observations highlight \ouralgo's potential to create automated ``task repositories'' that can be used to evaluate future models under identical instructions and scoring, enabling direct comparisons in the same manner as current benchmarks.
Because the tasks were originally tailored to GPT-4o, some may be far too difficult for Llama3-8B.
As such, the performance drop provides useful insight into the weaker model's failure modes and partial successes. 

We also consider the original setting of running \ouralgo anew, with GPT-4o as the scientist and Llama3-8B as the subject. 
This time, the scientist tailors difficulties and scope to Llama3-8B's observed performance.
\ouralgo discovers 1395 interestingly new tasks, summarized in \Cref{fig:llama_subject_embedding} and \Cref{tab:gpt4o_llama_clusters}.
Although the overall categories (programming, math, puzzle solving, creative writing, etc.) are similar, there are many revealing failures, with many examples shown in \Cref{appsubsec:llama_manual_selection}.
Two typical examples are shown \textcolor{Red}{\textbf{(top-right)}}: one is a trivial $3\times3$ matrix-rotation problem, while the other is a logic puzzle that leads Llama3-8B into a never-ending loop that exhausts the token limit.

\begin{center}
\begin{minipage}[h]{0.40\linewidth}
\begin{tcolorbox}[
  colback=red!10,
  colframe=red!50!black,
  boxrule=0.5pt,
  sharp corners,
  left=3pt, right=3pt, top=3pt, bottom=3pt,
  title=Failure on Basic Spatial Reasoning,
  fonttitle=\tiny,
  fontupper=\tiny,
  equal height group=boxgroup2
]
\textbf{Instructions:}\\
Rotate the given shape 90 degrees clockwise.

Shape:
\begin{verbatim}
1 0 0
1 1 1
0 0 1
\end{verbatim}

\textbf{Llama3-8B's Response:}
\begin{verbatim}
1 1 0
0 1 1
0 0 1
\end{verbatim}
\end{tcolorbox}
\end{minipage}
\hspace{0.02\linewidth}
\begin{minipage}[h]{0.54\linewidth}
\begin{tcolorbox}[
  colback=red!10,
  colframe=red!50!black,
  boxrule=0.5pt,
  sharp corners,
  left=3pt, right=3pt, top=3pt, bottom=3pt,
  title=Infinite Repetition during Output Reasoning,
  fonttitle=\tiny,
  fontupper=\tiny,
  equal height group=boxgroup2
]
\textbf{Instructions:}\\
Solve the following logical puzzle based on the given clues…

\textbf{Llama3-8B's Response:}\\
\emph{Since the Norwegian lives in the first house, the person who smokes Pall Mall must live in the second house,
but the Swede lives in the second house,
so the person who smokes Pall Mall must live in the first house,
but the Norwegian lives in the first house,
so the person who smokes Pall Mall must live in the second house,
but the Swede lives in the second house,
so the person who smokes Pall Mall must live in the first house, [repeats indefinitely]…}
\end{tcolorbox}
\end{minipage}
\end{center}

Such failures, while known for smaller models, represent genuine new discoveries for the GPT-4o scientist, since it has a ``knowledge cut-off'' at October 2023~\citep{openai2024gpt4} and the version of Llama3-8B we used was released in July 2024.
This underscores \ouralgo's potential to automatically surface a diverse set of novel failure modes. 

\subsection{Varying the Scientist Model}
\label{subsec:varying_scientist}

\begin{figure}[b!]
\centering
\vspace{-4mm}
\includegraphics[width=\columnwidth]{figures/gpt4o_vs_sonnet_scientist.pdf}
\vspace{-2mm}
\caption{
\small
Embedding comparison of tasks generated for GPT-4o by two different scientist models: GPT-4o \textcolor{Cerulean}{\textbf{(blue)}} and Claude Sonnet 3.5 \textcolor{JungleGreen}{\textbf{(green)}}. 
Each point represents a discovered capability or failure; the color reflects which scientist proposed it. 
We observe broad coverage of the GPT-4o regions by Sonnet 3.5, with additional more open-ended or creatively oriented tasks from Sonnet 3.5 not covered by GPT-4o. 
See also \Cref{tab:sonnet_gpt4o_clusters} for a cluster-level breakdown.
This demonstrates that different scientist models can probe different capability profiles for the same subject model, motivating ensembling-based approaches.
}
\label{fig:claude_embedding_comparison}
%
\end{figure}

Finally, we examine how changing the scientist model shapes the distribution of discovered tasks, while keeping GPT-4o as the subject. 
Rather than GPT-4o generating tasks, we let Claude Sonnet 3.5~\citep{claude3} serve as the scientist.
\Cref{fig:claude_embedding_comparison} and \Cref{fig:task_viz_claude} show that Sonnet 3.5 generates many tasks in similar high-level categories, but also proposes more interdisciplinary, creative, and unusual tasks (e.g., quantum-inspired biology, cross-cultural language design, and synesthesia-based reasoning). 
This is likely an interesting artifact of the Sonnet model being trained by Anthropic to have a distinct, more ``creative personality''~\citep{claude3} that has been noted in the community.

Below, we show an example discovered failure \textcolor{Red}{\textbf{(left box)}}, in which GPT-4o ignores the prompt's request to use color words as its cipher key, and a success \textcolor{DarkGreen}{\textbf{(right box)}}, where it provides a coherent ``xenolinguistic'' conlang design.
Both have a distinct ``flavour'' that is not present in the GPT-4o scientist.
More examples can be found in \Cref{appsubsec:claude_manual_selection}.

\begin{center}
\begin{minipage}[h]{0.40\linewidth}
\begin{tcolorbox}[
colback=red!10,
colframe=red!50!black,
boxrule=0.5pt,
sharp corners,
left=3pt, right=3pt, top=3pt, bottom=3pt,
title=Not Following Instructions for Substitution Cipher,
fonttitle=\tiny,
fontupper=\tiny,
equal height group=boxgroup3
]
\textbf{Instructions:}\\
Create a simple color-themed substitution cipher using these words:
\textit{red, blue, green, yellow, purple, orange, pink, brown, gray, black}.
\begin{enumerate}[leftmargin=2em,itemsep=0.3em,topsep=0.3em]
\item Assign each letter (A--Z) to one of the words. 
\item Encode the message: ``THE QUICK BROWN FOX''.
\item Provide your final answer in the format:
\begin{itemize}[leftmargin=1em]
\item Cipher key: A:color1, B:color2, ..., Z:color?
\item Encoded message: color color color ...
\end{itemize}
\end{enumerate}

\textbf{GPT-4o's Response:}\\
Cipher key: A:dog, B:cat, C:fish, D:bird, ...\\
Encoded message: deer wolf lion bear dog fox fish cat ...\\

\end{tcolorbox}
\end{minipage}
\hspace{0.02\linewidth}
\begin{minipage}[h]{0.54\linewidth}
\begin{tcolorbox}[
colback=green!10,
colframe=green!50!black,
boxrule=0.5pt,
sharp corners,
left=3pt, right=3pt, top=3pt, bottom=3pt,
title=A Xenolinguistic Conlang for Resonants,
fonttitle=\tiny,
fontupper=\tiny,
equal height group=boxgroup3
]
\textbf{Instructions (abridged):}\\
Design a constructed language (conlang) for the \textit{Resonants}, an echolocation-based species in a dark, aquatic world...
\begin{enumerate}[leftmargin=2em,itemsep=0.3em,topsep=0.3em]
\item Outline how they communicate using sound waves.
\item Show how the language encodes spatial relationships and tenses.
\item Provide examples for time, quantity, and relationships.
\item Give a short sample “sentence” and explain its meaning.
\item Discuss potential challenges for human learners.
\end{enumerate}

\textbf{GPT-4o's Response:}\\
\textit{Structure \& Mode:} Pulsed echolocation clicks, each representing phonemic units. \\
\textit{Spatial \& Tense:} Directional echoes for “near” or “far,” plus immediate and potential time states.\\
\textit{Examples:}\\
- \textbf{Time:} Distinct echo bursts for past/present/future.\\
- \textbf{Quantity:} Pulse repetition indicates single/few/many...
\end{tcolorbox}
\end{minipage}
\end{center}

\begin{figure*}[t!]
\centering
\vspace{-2mm}
\includegraphics[width=0.95\textwidth]{figures/sonnet_vis.pdf}
\vspace{-2mm}
\caption{
\small
Capabilities discovered by \ouralgo when Claude Sonnet 3.5 is the scientist and GPT-4o is the subject.
Each point represents one of the 2873 interestingly new discovered tasks, visualized in 2D via t-SNE.
We observe 46 clusters across diverse domains, including \emph{quantum-inspired biological systems, cross-cultural generative linguistics, musical composition with advanced theory}, as enumerated in \Cref{tab:sonnet_gpt4o_clusters}. Compared to GPT-4o as the scientist (\Cref{fig:task_viz_gpt}), Sonnet tends to propose much more abstract, interdisciplinary, and creative tasks. 
}
\label{fig:task_viz_claude}
\vspace{-3mm}
\end{figure*}

While this conlang example is imaginative and intriguing, it is certainly quite out-of-distribution of traditional foundation model benchmarks. Nonetheless, such examples illustrate the out-of-the-box probing \ouralgo can do, which could prove massively helpful for AI safety, where we want systems that check for out-of-distribution or unexpected capabilities (``the unknown unknowns'').
Such tasks are also extremely difficult to automatically score definitively, highlighting the need for more advanced oversight mechanisms~\citep{bowman2022measuringprogressscalableoversight}. Our results show that different scientist models produce different \textit{styles} of tasks probed for the same subject model, surfacing novel strengths and weaknesses. This motivates using an \emph{ensemble} of scientist models to broaden the coverage of potential capabilities and failure modes, rather than relying on a single scientist.

\section{Report Generation}
\label{sec:report_generation}
Once tasks and evaluations have been collected, \ouralgo can automatically compile a \textbf{Capability Report} summarizing each discovered capability, highlighting consistent successes, failures, and key insights about the subject model.
This mirrors recent developments where foundation models have been used for extensive scientific writing~\citep{lu2024aiscientist,wang2024autosurveylargelanguagemodels,Steinruecken2019aiStatistician}.

The advantage is twofold: (1) The resulting report serves as a compact overview of discovered capabilities and failure modes, providing an interpretable reference for developers or safety auditors; (2) By automating the summarization, we reduce some of the manual effort involved in curating large numbers of tasks; however, some manual review is still beneficial to identify the most illustrative or surprising examples (\Cref{appsubsec:manual_selection}).
As such, we currently advocate using \ouralgo to \emph{augment}, not replace, existing model safety, alignment, and capability tests.

\textbf{Workflow.}
To generate the report, \ouralgo feeds all clusters (obtained via t-SNE and HDBSCAN, as in \Cref{sec:eval}) and tasks, their automated evaluations, and the subject model's responses into the scientist and prompts it to \emph{(1) Identify Notable Examples:} 
Select surprising successes and failures per cluster by checking which tasks deviate significantly from expected performance or demonstrate unusual behavior;
\emph{(2) Provide Cluster-Level Explanations:} 
Explain the common theme of each cluster, identify what it believes are the surprising capabilities and failure modes from example tasks, and discuss the subject model's strengths or vulnerabilities revealed by those tasks; and finally,
\emph{(3) Generate an Overall Summary:} 
\ouralgo merges the per-cluster analyses into a cohesive report. It lists the subject model's key capabilities, typical mistakes, and high-level trends.

This yields a structured document containing a detailed breakdown of each task cluster (including the subject model's responses), highlight sections on surprising results, and an overall conclusion.
Users can thus quickly review new or unexpected insights about a subject model and pinpoint areas needing more human scrutiny.
\Cref{fig:report_sample} shows sample pages of the report generated for GPT-4o.

\begin{figure}[h!]
\centering
\vspace{2mm}
\includegraphics[width=0.9\columnwidth]{figures/report_thumbnail.png}
\vspace{-2mm}
\caption{\small
Sample pages from the automated report generated for GPT-4o; more details in \Cref{appsubsec:report_example}. The full PDF is provided on GitHub.
}
\vspace{-6mm}
\label{fig:report_sample}
\end{figure}

\section{Safety Considerations}
\label{sec:safety}

\textbf{Secure Execution and Containerization.}
All code generated by our system for defining and evaluating tasks is executed within containerized environments. This approach prevents unauthorized network access, restricts access to the host machine's filesystem, and mitigates other potentially unsafe behaviors. Our methodology adheres to widely adopted community standards for secure code generation and execution~\citep{jimenez2024swebench,hu2024automateddesignagenticsystems,chen2021evaluating}, ensuring that any inadvertent or harmful commands are effectively sandboxed. Furthermore, we explicitly instruct \ouralgo not to access the internet or the filesystem, and static analysis confirms that there are no such attempts (e.g., no `os' system calls are present). These measures substantially reduce the likelihood of deploying dangerous code.

\textbf{Safety Advantages of \ouralgolong.}
By design, \ouralgo systematically uncovers both surprising successes and unanticipated failure modes in foundation models.
Identifying unexpected or emergent capabilities is not only crucial for assessing model performance, but also for understanding potential safety risks~\citep{perez2022red,ganguli2022red,perez2022ignore,dong2024attacks}.
For instance, if \ouralgo reveals a novel method of circumventing certain guardrails for LLMs, that discovery can prompt a fix or mitigation strategy before real-world deployment.
Likewise, mapping out areas where the model systematically fails informs developers about possible hazards, such as incorrect legal interpretations or flawed mathematical reasoning, which could pose significant risks in high-stakes applications.
In this way, \ouralgo serves as an additional safeguard, helping practitioners pinpoint and address potentially dangerous or ethically sensitive behaviors early in the model lifecycle and help prevent dangerous behaviors prior to the deployment of the model~\citep{bengio2024managing,bengio2024international}.

\section{Conclusion and Limitations}
\label{sec:conclusion}

We have introduced \emph{\ouralgolong} (\ouralgo), a framework in which one foundation model, acting as a \emph{scientist}, autonomously discovers and evaluates the capabilities of another \emph{subject} model, thereby reducing the need for manual task design.
Through systematic exploration and automated evaluation, \ouralgo reveals a wide range of surprising capabilities and unexpected failures in the foundation models it evaluates, such as the GPT and Llama models.
Human evaluation of GPT-4o tasks confirms that most automatically generated tasks are coherent and that self-assessment aligns well with human judgments, albeit with a slight positive bias.
Future work could focus on improving the automated judge, for instance by using more sophisticated or even learned agentic systems~\citep{hu2024automateddesignagenticsystems}.
A further path for automation could be enhancing the selection of examples in our Capability Reports to match the quality of the manually curated highlights (\Cref{appsubsec:manual_selection}).
With better filtering and scaling, we envision being able to entrust larger portions of the model evaluation process to \ouralgo, greatly enhancing AI safety.

Although our experiments focused on single-turn, text-based tasks, future extensions could target more complex agentic tasks or incorporate multimodal inputs and outputs~\citep{zhang2024task}.
A particularly exciting target for \ouralgo is the new (at the time of writing) class of powerful yet underexplored reasoning models (e.g., OpenAI's o1~\citep{o1_system_card} or DeepSeek's r1~\citep{r1_deepseek}).
\ouralgo could play a significant role in systematically discovering and characterizing a range of behaviors in these emerging models.
Conversely, these improved models could act as much more effective scientists, enabling \ouralgo to perform even more detailed analyses of existing systems.
Finally, the tasks generated by \ouralgo could also represent an interesting way to generate new challenges for models to solve themselves~\citep{pmlr-v232-colas23a, schaul2024boundlesssocraticlearninglanguage}, potentially facilitating model self-improvement via open-ended~\citep{zhang2024omni,faldor2024omni,stanley2019NeuroEvo} and AI-generating algorithms~\citep{clune2019aigas}.

\clearpage

\section*{Acknowledgments}
This work was supported by the Vector Institute, the Canada CIFAR AI Chairs program, grants from Schmidt Futures and Open Philanthropy, an NSERC Discovery Grant, and a generous donation from Rafael Cosman.
We thank Aaron Dharna, Ben Norman, Jenny Zhang, Noah Goodman, and Rory Greig for insightful discussions and feedback on early drafts of this work.

%
%
%
%
%

\bibliography{references}
\bibliographystyle{include/icml2025}

\clearpage
\appendix
\onecolumn

\include{appendix}

\end{document}
