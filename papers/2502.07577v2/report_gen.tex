\section{Report Generation}
\label{appsec:report_generation}

As discussed in the main paper \Cref{sec:report_generation}, \ouralgolong can automatically produce a structured report summarizing each discovered capability and highlighting consistent successes, failures, and key insights. Below, we provide details about our workflow for generating these reports, including how clusters and tasks are automatically aggregated and summarized.

\subsection{Task Cluster Labeling}
\label{appsubsec:task_cluster_label}

After task generation, we cluster the discovered tasks (see \Cref{appsubsec:task_embedding_prompt}) and apply t-SNE + HDBSCAN to group them. To label these clusters concisely, we prompt GPT-4o to summarize each cluster as follows:

\begin{tcolorbox}[breakable,boxrule=0.5pt,sharp corners,fontupper=\small,
colback=orange!5!white, colframe=orange!80!black, title={Task Cluster Labelling System Prompt}]
You are a helpful assistant. You are given a set of tasks within a cluster.

Reply concisely and exactly in JSON format with only the following keys:
\begin{itemize}
    \item \texttt{"thought"}: First, reason about the essence of the given tasks in the cluster.
    \item \texttt{"label"}: Your summary label for the cluster of tasks.
    \item \texttt{"capability\_being\_measured"}: The overall capability being measured by the tasks in this cluster.
\end{itemize}

This will be automatically parsed so ensure that the string response is precisely in the correct format.
\end{tcolorbox}

\begin{tcolorbox}[breakable,boxrule=0.5pt,sharp corners,fontupper=\small,
colback=orange!5!white, colframe=orange!80!black, title={Task Cluster Labelling User Prompt}]
\textbf{[DATA]}

Cluster \{\texttt{cluster\_id}\} tasks:\\

Name: \{\texttt{name\_of\_task1}\}  \\
Description: \{\texttt{description\_of\_task1}\}  \\
Capability: \{\texttt{capability\_being\_measured1}\}  \\

Name: \{\texttt{name\_of\_task2}\}  \\
Description: \{\texttt{description\_of\_task2}\}  \\
Capability: \{\texttt{capability\_being\_measured2}\}  \\

... (any additional tasks in the cluster) ...\\

\textbf{[INSTRUCTION]}

Consider the above tasks in this cluster. Please provide a concise label (a natural language phrase within 10 words) for the cluster. Ensure that the label is very specific to the tasks; avoid being general. Avoid including general terms such as "problem-solving". Include more specific keywords from the tasks, such as "poem", "logic puzzles", etc.

Also, provide the overall capability being measured by the tasks in this cluster.

Return your answer as valid JSON with only the keys \texttt{"thought"}, \texttt{"label"}, and \texttt{"capability\_being\_measured"}.
\end{tcolorbox}

These labels are then used to form summaries of the discovered tasks in our final analysis.

\subsection{Report Generation Prompts}
\label{appsubsec:report_prompts}

Below are the prompt templates used for generating the analysis sections in the final report. 
This complements the discussion in the main paper \Cref{sec:report_generation}.

\subsubsection{Cluster Analysis Prompts}

\begin{tcolorbox}[breakable,boxrule=0.5pt,sharp corners,fontupper=\small,
colback=orange!5!white, colframe=orange!80!black, title={Cluster Analysis System Prompt}]
You are an expert in designing task families to assess the capabilities of large language models (LLMs). You will write an analytical section for a report examining the capabilities and limitations of large language models.

Your goal is to analyze and synthesize insights about LLM capabilities by examining:
1) The LLM's performance and solutions on tasks designed to test specific capabilities.
2) Any patterns, strengths, or limitations revealed through this analysis.
Focus on identifying surprising successes and failures from the point of view of an expert human evaluator.

You will be given a cluster of related task families that evaluate specific LLM capabilities, along with the LLM's responses and performance on these tasks.

Your goal is to:
1) Carefully examine the example tasks and the LLM's responses
2) Analyze the LLM's proficiency level on the evaluated capabilities
3) How these examples provide meaningful insights about the model's capabilities or limitations
4) Draw meaningful conclusions about the LLM's strengths and limitations in this capability area

Respond precisely in the following format including the JSON start and end markers:\\

\textbf{THOUGHT}: \texttt{<THOUGHT>}

\textbf{RESPONSE JSON}: \texttt{<JSON>}\\

In \texttt{<THOUGHT>}, first deeply think and reason about the patterns and insights revealed by examining this cluster of related tasks.

In \texttt{<JSON>}, provide a JSON response with the following fields:
\begin{itemize}[leftmargin=2em]
    \item \texttt{"overall\_analysis"}: A brief conclusion based on examining the example tasks and the LLM's responses, including key capabilities demonstrated and limitations revealed
    \item \texttt{"surprising\_example\_analysis\_X"}: Analysis of why this success or failure was surprising and what it reveals about the LLM's capabilities or limitations (one such field per example)
    \item \texttt{"insights"}: Key insights and takeaways about the LLM's capabilities based on analyzing this cluster of related tasks
\end{itemize}

For EACH provided example, include a \texttt{"surprising\_example\_analysis\_X"} field in the JSON response, where \texttt{X} is replaced with the example's index number. This will be automatically parsed so ensure that the string response is precisely in the correct format.
\end{tcolorbox}

\begin{tcolorbox}[breakable,boxrule=0.5pt,sharp corners,fontupper=\small,
colback=orange!5!white, colframe=orange!80!black, title={Cluster Analysis Prompt}]
\textbf{Task Cluster Analysis}

Cluster Name: \{cluster\_name\}

Capabilities Being Evaluated

\{capabilities\}

\textit{Note: Please examine the examples carefully to verify which capabilities are actually being tested.}

Tasks in Cluster

\{task\_names\}

Performance Statistics

Overall Success Rate: \{overall\_success\_rate\}

Success Rate by Task Difficulty:
\{difficulty\_breakdown\}

\textbf{Surprising Example}

Below are examples where the LLM succeeded or failed on tasks that reveal its capabilities or limitations.

\{surprising\_examples\}

Please analyze:
\begin{enumerate}[leftmargin=2em]
    \item What specific capabilities were demonstrated or lacking in the examples
    \item Any patterns in the successes and failures
    \item Notable or surprising results that reveal insights about the LLM's abilities
    \item What this suggests about the LLM's understanding and limitations
    \item How these insights connect to broader questions about LLM capabilities
\end{enumerate}
\end{tcolorbox}

\subsubsection{Example Selection Prompts}

\begin{tcolorbox}[breakable,boxrule=0.5pt,sharp corners,fontupper=\small,
colback=orange!5!white, colframe=orange!80!black, title={Example Selection System Prompt}]
You are an expert in designing task families to assess the capabilities of large language models (LLMs). You will write an analytical section for a report examining the capabilities and limitations of large language models.

Your goal is to analyze and synthesize insights about LLM capabilities by examining:
\begin{enumerate}[leftmargin=2em]
    \item The LLM's performance and solutions on tasks designed to test specific capabilities.
    \item Any patterns, strengths, or limitations revealed through this analysis.
\end{enumerate}
Focus on identifying surprising successes and failures from the point of view of an expert human evaluator.

You will be given a cluster of related task families that evaluate specific LLM capabilities, along with the LLM's responses and performance on these tasks. Your goal is to identify surprising successes and failures that reveal meaningful insights about LLM capabilities.

Respond precisely in the following format including the JSON start and end markers:\\

\textbf{THOUGHT}: \texttt{<THOUGHT>}

\textbf{RESPONSE JSON}: \texttt{<JSON>}\\

In \texttt{<THOUGHT>}, carefully analyze which examples demonstrate unexpected or notable behavior. Consider:
\begin{enumerate}[leftmargin=2em]
    \item Surprising successes on challenging tasks that demonstrate unexpected capabilities
    \item Unexpected failures on seemingly simple tasks that reveal limitations
    \item Examples that challenge common assumptions about LLM capabilities
\end{enumerate}

In \texttt{<JSON>}, provide a JSON response with the following fields:
\begin{itemize}[leftmargin=2em]
    \item \texttt{"surprising\_success\_example\_idx"}: List of indices for the most surprising or noteworthy successful tasks (0-3 indices)
    \item \texttt{"surprising\_failure\_example\_idx"}: List of indices for the most surprising or noteworthy failed tasks (0-3 indices)
\end{itemize}

Format for index lists: Empty list \texttt{[]}, single index \texttt{[1]}, or multiple indices \texttt{[0, 1, 3]}. This will be automatically parsed so ensure that the string response is precisely in the correct format.
\end{tcolorbox}

\begin{tcolorbox}[breakable,boxrule=0.5pt,sharp corners,fontupper=\small,
colback=orange!5!white, colframe=orange!80!black, title={Example Selection Prompt}]
\textbf{Task Cluster Analysis}

Cluster Name: \{cluster\_name\}

Capabilities Being Evaluated

\{capabilities\}

\textbf{Tasks Overview}

Total Tasks: \{num\_tasks\}

Overall Success Rate: \{overall\_success\_rate\}

\textbf{Task Examples}

\{task\_examples\}

Please analyze these examples carefully to identify:
\begin{enumerate}[leftmargin=2em]
    \item Which examples show surprising or unexpected successes, particularly:
    \begin{itemize}[leftmargin=1.25em]
        \item Complex tasks handled with sophisticated reasoning
        \item Challenging edge cases solved successfully
        \item Tasks requiring capabilities not typically associated with LLMs
    \end{itemize}
    \item Which examples show surprising or unexpected failures, particularly:
    \begin{itemize}[leftmargin=1.25em]
        \item Simple tasks that unexpectedly failed
        \item Inconsistent performance on similar tasks
        \item Failures that reveal interesting limitations
    \end{itemize}
\end{enumerate}

Focus on examples that would be genuinely surprising to an LLM expert researcher and provide meaningful insights about the model's capabilities or limitations.

In your response, briefly reason about EACH provided example and explain why it is (or isn't) surprising from the perspective of an LLM expert researcher.
\end{tcolorbox}

\subsubsection{Overall Summary Prompts}

\begin{tcolorbox}[breakable,boxrule=0.5pt,sharp corners,fontupper=\small,
colback=orange!5!white, colframe=orange!80!black, title={Overall Summary System Prompt}]
You are an expert in designing task families to assess the capabilities of large language models (LLMs). You will write an analytical section for a report examining the capabilities and limitations of large language models.

Your goal is to analyze and synthesize insights about LLM capabilities by examining:
\begin{enumerate}[leftmargin=2em]
    \item The LLM's performance and solutions on tasks designed to test specific capabilities.
    \item Any patterns, strengths, or limitations revealed through this analysis.
\end{enumerate}
Focus on identifying surprising successes and failures from the point of view of an expert human evaluator.

You are an expert researcher and engineer in Language Models. You are writing a very professional technical report to inform readers about the summary of the tested LLM's capabilities and limitations.

You will now provide an overall analysis and summary of the LLM's capabilities based on all the surprising tasks identified across various clusters. Your goal is to synthesize insights about the LLM's strengths and limitations, referencing specific results from the clusters using ``\#Cluster\_i'' to refer to examples.

Respond precisely in the following format including the JSON start and end markers:\\

\textbf{THOUGHT}: \texttt{<THOUGHT>}

\textbf{RESPONSE JSON}: \texttt{<JSON>}\\

In \texttt{<THOUGHT>}, deeply analyze the patterns observed across all clusters, considering both the surprising successes and failures. Your analysis should be detailed and reference specific results, using ``\#Cluster\_i'' to refer to examples from clusters.

In \texttt{<JSON>}, provide a JSON response with the following fields:
\begin{itemize}[leftmargin=2em]
    \item \texttt{"abstract"}: An abstract to this report. The first sentence should introduce the use of the \{scientist\} model as a scientist to study the \{subject\} model's capabilities. Then summarize the main contents.
    \item \texttt{"overall\_summary"}: A comprehensive summary of the LLM's capabilities based on your analysis. Introduce the context for the reader, e.g.\ start with sentences like ``In this report, we examine this LLM's \dots The LLM shows \dots''
    \item \texttt{"insight"}: A very detailed and long analysis to elaborate the above summary. Be very specific. Should be a list of \texttt{str}.
    \item \texttt{"surprising\_capabilities"}: Key surprising capabilities demonstrated by the LLM. Should be a list of \texttt{str}, and the analysis should be detailed and long.
    \item \texttt{"surprising\_failures"}: Notable limitations or failures revealed. Should be a list of \texttt{str}, and the analysis should be detailed and long.
    \item \texttt{"data\_insights"}: Analysis and interpretation of the numerical data provided (e.g.\ success rates, performance statistics). Should be a list of \texttt{str}, and the analysis should be detailed and long.
\end{itemize}

This will be automatically parsed so ensure that the string response is precisely in the correct format.
\end{tcolorbox}

\begin{tcolorbox}[breakable,boxrule=0.5pt,sharp corners,fontupper=\small,
colback=orange!5!white, colframe=orange!80!black, title={Overall Summary Prompt}]
\textbf{Overall Summary}

You have analyzed the LLM's performance across multiple task clusters and identified surprising successes and failures.

\textbf{Scientist and Subject}

You are now using the \{scientist\} model as a scientist to study the \{subject\} model's capabilities.

\textbf{Cluster Summaries}

\{cluster\_summaries\}

\textbf{Overall Statistics}

\{overall\_statistics\}

Please synthesize a comprehensive analysis of the LLM's capabilities based on the information above. In your analysis:
\begin{enumerate}[leftmargin=2em]
    \item Refer to specific results from clusters using ``\#Cluster\_i'' to refer to examples.
    \item Provide detailed observations about patterns in the LLM's performance across different clusters.
    \item Highlight surprising capabilities that challenge established understanding of LLM behavior.
    \item Discuss surprising failures that reveal significant limitations.
    \item Include analysis of numerical data, such as success rates and performance statistics.
\end{enumerate}

In your response \texttt{<THOUGHT>}, provide a detailed reasoning process that leads to your conclusions.

After your analysis, provide the JSON response with the required fields.
\end{tcolorbox}

\subsection{Generated Report Example}
\label{appsubsec:report_example}

Here we provide the first few pages of the generated report by \ouralgolong on GPT-4o (serving as both scientist and subject), as described in \Cref{sec:report_generation}.
Please find full reports for all evaluation settings in \Cref{sec:eval} at \url{https://github.com/conglu1997/ACD/tree/main/reports}.

\includepdf[
    clip=0mm 0mm 0mm 0mm,
    pages={1-4},
    frame,
    scale=.65,
    pagecommand={}
 ]{reports/gpt4_report.pdf}
