\appendix
\subsection{More Experimental Details}\label{app:implementation}
\paragraphbegin{Hardware Setup.} We conducted our experiments using the Unitree G1 humanoid robot, which has a mass of 35 kg, a height of 1.32 m, and 23 actuated degrees of freedom (6 per leg, 5 per arm, and 1 in the waist). The robot is equipped with a Jetson Orin NX for onboard computation and uses an IMU and joint encoders to provide proprioceptive feedback.


\begin{table*}[h]
    \centering
    \caption{Reward functions and groups used for learning standing-up control. Reward functions within the same group are independently normalized, whose associated advantaged functions are estimated via a distinct critic. The bold symbols represent vectors. The $H$ with subscripts represents the threshold height of standing-up stages defined in \cref{subsec:multi_critic}. The $f_\mathrm{tol}$ is a gaussian-style function with a saturation bound, referring to~\cite{tassa2018deepmind,tao2022learning} for more details. 'G' denotes ground, and the letters in 'PSW' denote platform, slope, and wall, respectively.}
    \renewcommand{\arraystretch}{1.05} % Default value: 1
    \resizebox{1\linewidth}{!}{
    \begin{tabular}{l l l l}
    \toprule 
    Term & Expression & Weight & Description \\
    
    \midrule 
    % \cmidrule(r){1-21}
    \noalign{\vskip -0.2mm}
  \ourrow \textbf{(a) Task Reward}  & $r^\mathrm{task}$ & $w^\mathrm{task}=2.5$ & It specifies the high-level task objectives.\\ 
  \noalign{\vskip 0.4mm}\cdashline{1-4}\noalign{\vskip 0.8mm}
    Head height & $f_\mathrm{tol}\left(h_\mathrm{head},[1,\mathrm{inf}],1,0.1\right)$ & 1 & The head of robot head $h_\mathrm{head}$ in the world frame.\\
    Base orientation & $f_\mathrm{tol}\left(-{\theta}_{\mathrm{base}}^{\mathrm{z}},[0.99,\mathrm{inf}],1,0.05\right)$  & 1 & The orientation of the robot base represented by projected gravity vector.\\
    
    \cmidrule(r){1-4}
    \noalign{\vskip -0.2mm}
     \ourrow \textbf{(b) Style Reward}  & $r^\mathrm{style}$ & $w^\mathrm{style}=1$ & It specifies the style of standing-up motion.\\ 
  \noalign{\vskip 0.4mm}\cdashline{1-4}\noalign{\vskip 0.8mm}
    Waist yaw deviation & $\mathds{1}(| q_{\mathrm{waist}}| > 1.4)$  & $-10$ & It penalizes the large joint angle of the waist yaw. \\
    Hip roll/yaw deviation & $\mathds{1}( \max(|\bm{q}_{\mathrm{hip}}^{\mathrm{l,r}}|) > 1.4)\;|\;\mathds{1}( \min(|\bm{q}_{\mathrm{hip}}^{\mathrm{l,r}}|) > 0.9)$ & $-10$/$-10$ & It penalizes the large joint angle of hip roll/yaw joints. \\
    Knee deviation & $\mathds{1}( \max(|\bm{q}_{\mathrm{knee}}^{\mathrm{l,r}}|) > 2.85)\;|\;\mathds{1}( \min(|\bm{q}_{\mathrm{knee}}^{\mathrm{l,r}}|) < -0.06)$ & $\frac{-0.25 (G)}{-10 (PSW)}$ & It penalizes the large joint angle of knee joints. \\
    Shoulder roll deviation & $\mathds{1}( \max(|q_{\mathrm{shoulder}}^{l}|) < -0.02)\;|\;\mathds{1}( \min(|q_{\mathrm{shoulder}}^{r}|) > 0.02)$ & $-2.5$ & It penalizes the large joint angle of shoulder roll joint.  \\
    Foot displacement & $\exp\left(-2\times\|\bm{q}_{\mathrm{base}}^{\mathrm{xy}} - \bm{q}_{\mathrm{foot}}^{\mathrm{xy}}  \|^2.\mathrm{clip}(0.3,\mathrm{inf}) \right) 
    % \times \mathds{1}(\bm{q}_{\mathrm{foot}}^{\mathrm{z}} < 0.3) 
    \times \mathds{1}(h_{\mathrm{base}} > H_{\mathrm{stage2}}) $ & $2.5/2.5$ & It encourages robot CoM locates in support polygon, inspired by~\cite{Goswami2004RateOC}.\\
    Ankle parallel & $(\mathrm{var}(\bm{q}_{\mathrm{left\;ankle}}^z) + \mathrm{var}(\bm{q}_{\mathrm{right\;ankle}}^z))/2 < 0.05$ & $20$ & It encourages the ankles to be parallel to the ground via ankle keypoints.\\
    Foot distance & $\|\bm{q}_{\mathrm{feet}}^l - \bm{q}_{\mathrm{feet}}^r \|^2 > 0.9$ & $-10$ & It penalizes a far distance between feet.\\
    Feet stumble & $\mathds{1}(\exists i,|\mathbf{F}_i^{\mathrm{x y}}|>3|F_i^\mathrm{z}|)$ & $\frac{0 (G)}{-25 (PSW)}$ & It penalizes a horizontal contact force with the environment. \\
    Shank orientation & $f_\mathrm{tol}(\mathrm
    {mean}(\mathrm{\bm{\theta}_{\mathrm{shank}}^{\mathrm{l,r}}[2]}),[0.8, \mathrm{inf}], 1, 0.1) \times \mathds{1}(h_{\mathrm{base}} > H_{\mathrm{stage1}}) $ & $10$ & It encourages the left/right shank to be perpendicular to the ground.\\
    Ankle parallel & $(\mathrm{var}(\bm{q}_{\mathrm{left\;ankle}}^z) + \mathrm{var}(\bm{q}_{\mathrm{right\;ankle}}^z))/2 < 0.05$ & $20$ & It encourages the ankles to be parallel to the ground via ankle keypoints.\\
    Base angular velocity & $\mathrm{exp}(-2\times\|\bm{\omega}^\mathrm{xy}_\mathrm{base}\|^2) \times \mathds{1}(h_{\mathrm{base}} > H_{\mathrm{stage1}}) $ & $1$ & It encourages low angular velocity of the during rising up.  \\
    
    \cmidrule(r){1-4}
    \noalign{\vskip -0.2mm}
     \ourrow \textbf{(c) Regularization Reward}  & $r^\mathrm{regu}$ & $w^\mathrm{regu}=0.1$ & It specifies the regulariztaion on standing-up motion.\\ 
  \noalign{\vskip 0.4mm}\cdashline{1-4}\noalign{\vskip 0.8mm}
    Joint acceleration & $\|\ddot{p}\|^2$ & $-2.5 e^{-7}$ & It penalizes the high joint accelrations.\\
    Action rate & $\|a_t-{a}_{t-1}\|^2$ & $-1e^{-2}$ & It penalizes the high changing speed of action.\\
    Smoothness & $\|{a}_t-2 {a}_{t-1}+{a}_{t-2}\|^2$ & $-1e^{-2}$ & It penalizes the discrepancy between consecutive actions.  \\
    Torques & $\|\bm{\tau}\|^2$ & $-2.5e^{-6}$ & It penalizes the high joint torques.\\
    Joint power & $|\bm{\tau} \| \dot{p}|^{T}$ & $-2.5e^{-5}$ & It penalizes the high joint power\\
    Joint velocity & $ \|\dot{p}\|_{2}^{2}$ & $-1 e^{-4}$ & It penalizes the high joint velocity.\\
    Joint tracking error & $\|{p}_t - {p}^{\mathrm{target}}_{t}\|^{2}$ & $-2.5e^{-1}$ & It penalizes the error between PD target (\cref{eq:pd}) and actual joint position.  \\
    Joint pos limits & $\sum_i[(p_i -p_i^{\mathrm{Lower}}).\mathrm{clip}(-\mathrm{inf},0) + (p_i -p_i^{\mathrm{Higher}}).\mathrm{clip}(0,\mathrm{inf}) ]$ & $-1e^2$ & It penalizes the joint position that beyond limits.\\
    Joint vel limits & $\sum_i[(|\dot{p}_i| -\dot{p}_i^{\mathrm{Limit}}).\mathrm{clip(0,\mathrm{inf})}]$ & $-1$ & It penalizes the joint velocity that beyond limits. \\

    \cmidrule(r){1-4}
    \noalign{\vskip -0.2mm}
     \ourrow \textbf{(d) Post-task Reward}  & $r^\mathrm{post}$ & $w^\mathrm{post}=1$ & It specifies the desired behaviors after a successful standing up.\\ 
  \noalign{\vskip 0.4mm}\cdashline{1-4}\noalign{\vskip 0.8mm}
    Base angular velocity & $\mathrm{exp}(-2\times\|\bm{\omega}^\mathrm{xy}_\mathrm{base}\|^2) \times \mathds{1}(h_{\mathrm{base}} > H_{\mathrm{stage2}}) $ & $10$ & It encourages low angular velocity of robot base after standing up.  \\
    Base linear velocity & $\mathrm{exp}(-5\times\|\bm{v}^\mathrm{xy}_\mathrm{base}\|^2) \times \mathds{1}(h_{\mathrm{base}} > H_{\mathrm{stage2}}) $ & $10$ & It encourages low linear velocity of robot base after standing up.  \\
    Base orientation & $\exp(-5\times\|\bm{\theta}_{\mathrm{base}}^{\mathrm{xy}}\|^2 \times \mathds{1}(h_{\mathrm{base}} > H_{\mathrm{stage2}})$ & 10 & It encourages the robot base to be perpendicular to the ground. \\
    Base height & $\exp(-20\times\|{h}_{\mathrm{base}} - {h}_{\mathrm{base}}^{\mathrm{target}}\|^2 \times \mathds{1}(h_{\mathrm{base}} > H_{\mathrm{stage2}})$  & 10 & It encourages the robot base to reach a target height. \\
    Upper Body Posture & $\exp(-0.1\times \| p_{\mathrm{upper}}-p_{\mathrm{upper}}^\mathrm{target} \|^2) \times \mathds{1}(h_{\mathrm{base}} > H_{\mathrm{stage2}})$ & 10 & It encourages the robot to track a target upper body postures. \\
    Feet parallel & $\exp(-20\times |h_{\mathrm{feet}}^l - h_{\mathrm{feet}}^r|.\mathrm{clip}(0.02, \mathrm{inf})) \times \mathds{1}(h_{\mathrm{base}} > H_{\mathrm{stage2}})$ & 2.5 & In encourages the feet to be parallel to each other.\\

    \bottomrule
    \end{tabular}}
    % \caption{Rewards}
    \vspace{-0.1in}
    \label{table:reward_functions} 
\end{table*}


\paragraphbegin{Curriculum Setup.} The curriculum adjustment condition is consistent for both the vertical force and action bound: the head height $h_{\mathrm{head}}$ must reach a target height $H_{\mathrm{head}}$ by the end of each episode. Initially, the vertical force $\mathcal{F}$ is set to 200 N, and the action bound $\beta$ is set to 1. Upon reaching the target head height, the vertical force decreases by 20 N, and the action bound decreases by 0.02. The lower bounds for the vertical force and action bound are 0 N and 0.25, respectively.

\paragraphbegin{Stage Division.} The first stage involves righting the body, where we set $H_{\mathrm{stage1}}$ to 0.45 m. The second stage involves rising the body, with $H_{\mathrm{stage2}}$ set to 0.65 m.

\paragraphbegin{Evaluation Protocol.} Each policy is evaluated on each terrain with 5 repetitions of 250 episodes each, totaling 1250 episodes. We report the mean and standard deviation of performance. The target standing-up height is set to 0.6 m for the slope terrain and 0.7 m for all other terrains during evaluation.

\paragraphbegin{Robustness Test.} The CoM bias and sagittal force are set on the x-axis direction of the robot. The initial joint angle offset is applied to all joints of the robot. The random torque dropout is applied to each simulation step (200Hz), where the torques are set to zero if being dropout. 

\subsection{More Implementation Details}\label{app:hyperparams}

\paragraphbegin{PD Controller.} In simulation, the stiffness values are set as 100 for the upper body, 40 for the ankle, 150 for the hip, and 200 for the knee. The damping values are set to 4 for the upper body, 2 for the ankle, 4 for the hip, and 6 for the knee. High stiffness values for the hip and knee are used due to the high torque demands during the standing-up process. During real-world deployment, we observe a significant torque gap between simulation and reality (see \cref{fig:sim2real}). Thus, the stiffness of the hip and knee are adjusted to 200 and 275, respectively.

\paragraphbegin{Reward Functions.} We present the complete set of reward functions and their detailed descriptions in \cref{table:reward_functions}. Several regularization reward terms are adapted from prior work~\cite{kumar2021rma,long2024learning,He2024OmniH2OUA}. Additionally, we incorporate a tolerance reward, $f_\mathrm{tol}(i, b, m, v)$, as defined in~\cite{tassa2018deepmind, tao2022learning}. This reward is computed as a function of an input value $i$, which is constrained by three parameters: bounds $b$, margin $m$, and value $v$. The bounds $b$ define the region where the reward is 1 if $i$ lies within the bounds. Outside this region, the reward smoothly decreases according to a Gaussian function, reaching the value $v$ at a distance determined by the margin $m$.

\paragraphbegin{PPO Implementation.} Our PPO implementation follows the framework outlined in~\cite{Rudin2021LearningTW}. The actor network consists of a 3-layer MLP with hidden dimensions [512, 256, 128], while each critic network is a 2-layer MLP with hidden dimensions [512, 256]. Each iteration includes 50 steps per environment, with 5 learning epochs and 4 mini-batches per epoch. The discount factor $\gamma$ is set to 0.99, the clip ratio is set to 0.2, and the entropy coefficient is 0.01. The multi-critic architecture is based on previous work~\cite{mysore2022multi}, where each advantage function is independently calculated and normalized within its corresponding reward group.

\paragraphbegin{Baseline Implementations.} \ours-w/o-MuC represents a baseline with a single value network, essentially a standard RL implementation. \ours-w/o-Force-RND removes the vertical force curriculum and introduces an RND reward with a coefficient of 0.2~\cite{burda2019exploration}. \ours-Bound0.25 uses a fixed action bound of $\beta = 0.25$ without a curriculum. \ours-w/p-$r^{\mathrm{style}}$ eliminates all style-related reward functions. 
Lastly, \ours-History modifies the history length of states while keeping other implementations unchanged.

\paragraphbegin{Terrains.} The heights of the platforms range from 20cm to 92cm. The slope inclination varies from approximately 1째 to 14째. The wall inclination spans from approximately 14째 to 84째.
% \subsubsection{Terrain Descriptions} 


