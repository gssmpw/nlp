\section{Introduction}
Can humanoid robots stand up from a sofa, walk to a table, and pick up coffee, seamlessly like humans?
Fortunately, recent advancements in humanoid robot hardware and control have enabled significant progress in bipedal locomotion~\cite{radosavovic2024real,li2024reinforcement,long2024learning,zhuang2024humanoid} and bimanual manipulation~\cite{Cheng2024OpenTeleVisionTW,Li2024OKAMITH,Fu2024HumanPlusHS,Jiang2024HarmonWM}, allowing robots to navigate environment and interact with objects effectively. However, the fundamental capability—standing-up control~\cite{stuckler2006getting,kanehiro2003first}—remains underexplored. Most existing systems assume the robots start from a pre-standing posture, limiting their applicability to many scenes, such as transitioning from a seated position or recovering after a loss of balance. We envision that unlocking this standing-up capability would broaden the real-world applications of humanoid robots. To this end, we investigate how humanoid robots can learn to stand up across diverse postures in real environments.

A classical approach for this control task involves tracking handcrafted motion trajectories through model-based motion planning or trajectory optimization~\cite{kanehiro2003first,kanehiro2007getting,kuniyoshi2004dynamic,stuckler2006getting}. 
Although effective in generating motions, these methods require extensive tuning of analytical models and often perform suboptimally in real-world settings with external disturbances~\cite{luo2014multi,lee2019robust} or inaccurate actuator modeling~\cite{hwangbo2019learning}. Besides, real-time optimization on the robot makes these methods computationally intensive, prompting workarounds such as reduced optimization precision or offload computations to external machines~\cite{neunert2017trajectory,farshidian2017efficient}, though both are with practical limitations.

Reinforcement learning (RL) offers an alternative effective framework for humanoid locomotion and whole-body control~\cite{Peng2022ASE,He2024OmniH2OUA,Cheng2024ExpressiveWC,Zhang2024WoCoCoLW}, benefiting from minimal modeling assumptions. However, compared to these tasks that partially decouple upper- and lower-body dynamics, RL-based standing-up control involves a highly dynamic and synergistic maneuver on both halves of the body. This complex maneuver features time-varying contact points~\cite{kanehiro2003first}, multi-stage motor skills~\cite{luo2014multi}, and precise angular momentum control~\cite{Goswami2004RateOC}, making RL exploration challenging. Although predefined motion trajectories can guide RL exploration, they are typically limited to ground-specific postures~\cite{peng2018deepmimic,Peng2022ASE,yang2023learning,haarnoja2024learning}, leaving the scalability to other postures unclear. Conversely, training RL agents from scratch with wide explorative strategies on the ground can lead to violent and abrupt motions that hinder real-world deployment~\cite{tao2022learning}, particularly for robots with many actuators and wide joint limits. In summary, learning posture-adaptive, real-world deployable standing-up control with RL remains an open problem (see \cref{table:comparision_method}).

In this work, we address this problem by proposing \ours, an RL-based framework that learns humanoid standing-up control across diverse postures from scratch. To enable posture-adaptive motion beyond the ground, we introduce multiple terrains for training and a vertical pull force during the initial stages to facilitate exploration. Given the multiple stages of the task, we adopt multi-critic RL~\cite{mysore2022multi} to optimize distinct reward groups independently for a better reward balance. To ensure real-world deployment, we apply smoothness regularization and motion speed constraints to mitigate oscillatory and violent motions. Our control policies, trained in simulation~\cite{makoviychuk2021isaac} with domain randomization~\cite{tobin2017domain}, can be directly deployed on the Unitree G1 humanoid robot. The resulting motions, tested in both laboratory and outdoor environments, demonstrate high smoothness, stability, and robustness to external disturbances, including forces, stumbling blocks, and heavy payloads. 

We overview the real-world performance of our controllers in \cref{fig:teaser} and summarize our core contributions as follows:
\begin{itemize}[leftmargin=4mm]
\vspace{-0.02in}
    \item \textbf{Real-world posture-adaptive motions} are well achieved through our proposed RL-based method, without relying on predefined trajectories or sim-to-real adaptation techniques.
    \item \textbf{Smoothness, stability, and robustness}  are consistently demonstrated by our learned control policies, even under challenging external disturbances.
    \item \textbf{Evaluation protocols} are elaborately designed to analyze standing-up control comprehensively, aiming to guide future research and development in this control task.
\vspace{-0.02in}
\end{itemize}