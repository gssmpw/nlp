\section{Problem Formulation}
We formulate the problem of humanoid standing up as a Markov decision process (MDP; \cite{puterman2014markov}) with finite horizon, which is defined by the tuple $\mathcal{M} = \langle \mathcal{S}, \mathcal{A}, \mathcal{T}, \mathcal{R}, \gamma \rangle$.  
At each timestep $t$, the agent (\ie the robot) perceives the state $s_t\in\mathcal{S}$ from the environment and executes an action $a_t\in\mathcal{A}$ produced by its policy $\pi_\theta(\cdot|s_t)$. The agent then observes a successor state $s_{t+1}\sim \mathcal{T}(\cdot|s_t, a_t)$ following the environment transition function $\mathcal{T}$ and receives a reward signal $r_t\in\mathcal{R}$. To solve the MDP, we employ reinforcement learning (RL;~\cite{sutton2018reinforcement}), whose goal learn an optimal policy $\pi_\theta$ that maximizes the expected cumulative reward (return) $\mathbb{E}_{\pi_\theta}[\sum_{t=0}^{T-1}\gamma^t r_t]$ the agent receives during the whole $T$-length episode, where $\gamma\in[0, 1]$ is the discount factor. The expected return is estimated by a value function (critic) $V_\phi$. In this paper, we adopt Proximal Policy Optimization (PPO;~\cite{schulman2017proximal}) as our RL algorithm because of its stability and efficiency in large-scale parallel training. 

% \begin{figure*}[t]
%     \centering
%     % \vspace{0.1in}
%     \includegraphics[width=1\linewidth]{arxiv/figures/website_method.pdf}
%     \caption{\textbf{Framework overview}. (a) We train control policies in simulation from scratch with multiple critics, action bound (rescaler), and motion constraints applied by rewards and smoothness regularization. (b) The trained polices can be directly deployed in the real robot to produce standing-up motions.}
%     \label{fig:website_method}
%     \vspace{-0.05in}
% \end{figure*}

\begin{figure}[t]
    \centering
    % \vspace{0.1in}
    \includegraphics[width=1\linewidth]{figures/method.pdf}
    \caption{\textbf{Framework overview}. (a) We train policies in simulation from scratch with multiple critics and motion constraints operationalized by rewards, smoothness regularization, and action bound (rescaler). (b) The trained polices can be directly deployed in the real robot to produce standing-up motions.}
    \label{fig:framework}
    \vspace{-0.05in}
\end{figure}

\subsubsection{State Space} We hypothesize that the proprioceptive states of robots provide sufficient information for standing-up control in our target environments.
We thus include the proprioceptive information read from robot's Inertial Measurement Unit (IMU) and joint encoders into the state $s_t=[\omega_t, r_t,p_t, \dot{p}_t, a_{t-1}, \beta]$, where $\omega_t$ is the angular velocity of robot base,  $r_t$ and $p_t$ are the roll and pitch, $p_t$ and $\dot{p}_t$ are positions and velocities of the joints, $a_{t-1}$ is the last action, and $\beta\in (0, 1]$ is a scalar that scale the output action. 
Given the contact-rich nature of the standing-up task, we implicitly enhance contact detection by feeding the policy with the previous five states~\cite{hwangbo2019learning}. 

\subsubsection{Action Space} We employ a PD controller for torque-based robot actuation. The action $a_t$ represents the difference between the current and next-step joint positions, with the PD target computed as $p_t^d = p_t + \beta a_t$, where each dimension of $a_t$ is constrained to $[-1, 1]$. The action rescalar $\beta$ restricts the action bounds to regulate the motion speed implicitly. This is essential to constrain the standing-up motion and will be discussed in later sections. The torque at timestep $t$ is computed as:
\begin{equation}\label{eq:pd}
    \tau_t = K_p\cdot(p_t^d - p_t) - K_d\cdot \dot{p}_t,
\end{equation}
where $K_p$ and $K_d$ represent the stiffness and damping coefficients of the PD controller. The dimension of action space $|A|$ corresponds to the number of robot actuators.

\section{Method}
This section introduces \ours (\underline{H}uman\underline{o}id \underline{St}anding-up Control), a reinforcement learning (RL)-based framework for learning humanoid robots to stand up across diverse postures, as summarized in \cref{fig:framework}. This control task is highly dynamic, multi-stage, and contact-rich, posing challenges for conventional RL approaches. We first outline the key challenges addressed in this work in~\cref{subsec:key_challenges}, then describe the core components of the framework in the following sections.

\subsection{Key Challenges \& Overview}\label{subsec:key_challenges}
\subsubsection{Reward Design \& Optimization (\cref{subsec:multi_critic})} The standing-up task involves multiple motor skills: righting the body, kneeling, and rising. Learning a control policy for these stages is challenging without explicit stage separation~\cite{li2023robust,kim2024stage}. We address this by dividing the task into three stages and activating corresponding reward functions at each stage. The complexity of these skills requires multiple reward functions, which can complicate policy optimization. To mitigate this, we employ multi-critic RL~\cite{mysore2022multi}, grouping reward functions to balance objectives effectively.

\subsubsection{Exploration Challenges (\cref{subsec:force})} Despite multi-critic RL, exploration remains difficult due to the robotâ€™s high degrees of freedom and wide joint limits. Drawing inspiration from human infant skill development~\cite{claxton2012control}, we facilitate exploration by applying a curriculum-based vertical pulling force.

\subsubsection{Motion Constraints (\cref{subsec:motion_smoothness})} With only reward functions, the agent tends to learn violent and jerky motions, driven by high torque limits and numerous actuators. Such behaviors are impractical for real-world deployment. To address this, we introduce an action rescaler $\beta$ to gradually tighten action output bounds, implicitly limiting joint torques and motion speed. Additionally, we incorporate smoothness regularization~\cite{Kobayashi2022L2C2LL} to mitigate motion oscillation.

\subsubsection{Sim-to-Real Gap (\cref{subsec:simulation})} A significant challenge is the sim-to-real gap. We address this through two strategies: (1) designing diverse terrains to better simulate real-world starting postures, and (2) applying domain randomization~\cite{tobin2017domain} to reduce the influence of physical discrepancies between simulation and real world.


\subsection{Reward Functions \& Multiple Critics}\label{subsec:multi_critic}
Considering the multi-stage nature of the task, we divide the task into three stages: righting the body $h_\mathrm{base}<H_{\mathrm{stage1}}$, rising the body $h_\mathrm{base}>H_{\mathrm{stage2}}$, and standing $h_\mathrm{base}>H_{\mathrm{stage2}}$, indicated by the height of the robot base $h_\mathrm{base}$. Corresponding reward functions are activated at each stage. We then classify reward functions into four groups: (1) \textbf{task reward} $r^{\mathrm{task}}$ that specifies the high-level task objectives, (2) \textbf{style reward} $r^{\mathrm{style}}$ that shapes the style of standing-up motion, (3)  \textbf{regularization reward} $r^{\mathrm{regu}}$ that further regularizes the motionw, and (4) \textbf{post-task reward} $r^{\mathrm{post}}$ that specify the desired behaviors after successful standing up, \ie stay standing. The overall reward function is expressed as follows:
\begin{equation*}
    r_t = w^{\mathrm{task}}\cdot r^{\mathrm{task}}_t + w^{\mathrm{style}}\cdot r^{\mathrm{style}}_t +  w^{\mathrm{regu}}\cdot r^{\mathrm{regu}}_t + w^{\mathrm{post}}\cdot r^{\mathrm{post}}_t,
\end{equation*}
where $w$ with superscript represents the corresponding reward weight. Each reward group contains multiple reward functions. A comprehensive list of all reward functions and groups is provided in~\cref{table:reward_functions}.

\begin{figure}[t]
    \centering
    % \vspace{0.1in}
    \includegraphics[width=1\linewidth]{figures/scenes_1col.png}
    \caption{\textbf{Simulation terrains and real-world scenes}. We design four terrains in simulation: ground, platform, wall, and slope to create initial robot postures that are likely to be met in real-world environments. Examples of these real-world environments are shown at the bottom of the figure.}
    \label{fig:terrain_examples}
\end{figure}

However, we observe that using a single value function (critic) presents significant challenges in learning effective standing-up motions. Besides, the large number of reward functions makes hyperparameter tuning computationally intensive and difficult to balance. To address these challenges, we implement multiple critics (MuC;~\cite{mysore2022multi,xu2023composite,zargarbashi2024robotkeyframing}) to estimate returns for each reward group independently, where each reward group is regarded as a separate task with its own assigned critic $V_{\phi_i}$. These multiple critics are then integrated into the PPO framework for optimization as follows:
 \begin{equation}
    \mathcal{L}(\phi_i) = \mathbb{E}\big[ \|r_{t}^i + \gamma V_{\phi_i}(s_t) - \bar{V}_{\phi_i}(s_{t+1})\|^2\big],
\end{equation}
where $r_{t}^i$ is the total reward and $\bar{V}_{\phi_i}$ is the target value function of reward group $i$. Each critic independently computes its advantage function $A_{\phi_i}$ estimated through GAE~\cite{Schulman2015HighDimensionalCC}. These individual advantages are then aggregated into an overall weighted advantage: $A = \sum_{i}  w^i \cdot \frac{A_{\phi_i} - \mu_{A_{\phi_i}}}{\sigma_{A_{\phi_i}}} $, where $\mu_{A_{\phi_i}}$ and $\sigma_{A_{\phi_i}}$ are the batch mean and standard deviation of each advantage. The critics are updated simultaneously with the policy network $\pi_\theta$ according to:
\begin{equation}
    \mathcal{L}(\theta) = \mathbb{E} \left[ \min \left( \alpha_t (\theta)A_t, \mathrm{clip}(\alpha_t(\theta), 1-\epsilon, 1+\epsilon)A_t \right) \right],
\end{equation}
where $\alpha_t(\theta)$ and $\epsilon$ are the probability ratio and the clipping hyperparameter, respectively.


\subsection{Force Curriculum as Exploration Strategy}\label{subsec:force}
 The primary exploration challenges emerge during the transition from falling to stable kneeling, a stage that proves difficult to explore effectively through random action noise alone.
While human infants are likely to learn motor skills with external supports~\cite{claxton2012control}, it inspires us to design environmental assistance to accelerate the exploration. Specifically, we apply an upward force $\mathcal{F}$ on the robot base, which is largely set at the start of training. This force takes effect only when the robot's trunk achieves a near-vertical orientation, indicating a successful ground-sitting posture. The force magnitude decreases progressively as the robot can maintain a target height at the end of the episode. See more details in \cref{app:implementation}.

\subsection{Motion Smoothness}\label{subsec:motion_smoothness}
\subsubsection{Action Bound (Rescaler)} Humanoid robots often feature many DoFs, each equipped with wide position limits and high-power actuators. This configuration often results in violent motions after RL training, characterized by violent ground hitting and rapid bouncing movements. While setting low action bounds could mitigate this behavior, it might prevent the robot from exploring effective standing-up motions. To this end, we introduce an action rescaler $\beta$ to scale the action output, implicitly controlling the bound of the maximal torques on each actuator. This scale coefficient gradually decreases like vertical force reduction. See more details in \cref{app:implementation}.

\begin{table}[t]
    \centering
    \caption{Domain randomization settings for standing-up control.}
    \begin{tabular}{ll}
    \toprule 
    Term & Value \\
    \midrule 
    Trunk Mass & $\mathcal{U}(-2,5)$kg  \\
    Base CoM offset & $\mathcal{U}(-0.03,0.03)$m \\
    Link mass & $\mathcal{U}(-0.8,1.2)\times$ default kg  \\ 
    Fiction & $\mathcal{U}(0.1,1)$ \\
    Restitution & $\mathcal{U}(0,1)$ \\
    P Gain & $\mathcal{U}(0.85,1.15)$ \\
    D Gain & $\mathcal{U}(0.85,1.15)$ \\
    Torque RFI~\cite{campanaro2024learning} & $\mathcal{U}(-0.05,0.05)\times$ torque limit N$\cdot$m  \\
    Motor Strength & $\mathcal{U}(0.9,1.1)$ \\
    Control delay & $\mathcal{U}(0, 100)$ms \\
    Initial joint angle offset & $\mathcal{U}(-0.1, 0.1)$rad \\
    Initial joint angle scale & $\mathcal{U}(0.9, 1.1)\times$ default joint angle rad \\
    \bottomrule
    \end{tabular}
    % \caption{Rewards}
    \label{table:domain_randomization}
\end{table}

\input{tables/main_sim_results}
\begin{figure*}[t]
    \centering
    \includegraphics[width=0.97\textwidth]{figures/traj_visualization.pdf}
    \caption{\textbf{Motion analysis in simulation}. (Left) UMAP visualization of joint-space trajectories demonstrates similar but distinct motion patterns on the terrains except for the wall. Besides, the trajectories of each terrain are overall consistent, with variation to handle the difference of starting postures. (Right) 3D trajectory visualizations reveal stable, coordinated hand-foot motion and dynamic posture adaptability, demonstrating effective whole-body coordination and validating the proposed framework. Point color in the plot indicates motion progression, with lighter shades for earlier and darker for later times.}
    \label{fig:trajectory}
    \vspace{-0.1in}
\end{figure*}


\subsubsection{Smoothness Regularization} To prevent motion oscillation, we adopt the smoothness regularization method L2C2~\cite{Kobayashi2022L2C2LL} into our multi-critic formulation. This method applies regularization to both the actor-network $\pi_\theta$ and critics $V_{\phi_i}$ by introducing a bounded sampling distance between consecutive states $s_t$ and $s_{t+1}$:
\begin{equation*}
   \mathcal{L}_{\mathrm{L2C2}} = \lambda_\pi D(\pi_\theta(s_t), \pi_\theta(\bar{s}_t)) +\lambda_V \sum D(V_{\phi_i}(s_t), V_{\phi_i}(\bar{s}_t)),
\end{equation*}
where $D$ is a distance metric, $\lambda_\pi$ and $\lambda_V$ are weight coefficient, $\bar{s}_t=s_t+(s_{t+1}-s_t)\cdot u$ is the interpolated state given a uniform noise $u\sim \mathcal{U}(\cdot)$. We combine this objective function with ordinary PPO objectives to train our control policies. 
% Huge difficulties are observed when training RL policies without any curriculum setup. We find that two factors are useful to overcome: (1) external force that assists robots in easing the difficulty of the standing-up phase; (2) limit on delta joint position to slow down the motion speed

\subsection{Training in Simulation \& Sim-to-Real Transfer}\label{subsec:simulation}
We use Isaac Gym~\cite{makoviychuk2021isaac} simulator with 4096 parallel environments and the 23-DoF Unitree G1 robot to train standing-up control policies with the PPO~\cite{schulman2017proximal} algorithm.

\subsubsection{Terrain Design}\label{subsec:terrain} To model the diverse starting postures in the real world, we design 4 terrains to diversify the starting postures: (1) \textbf{ground} that is flat, (2) \textbf{platform} that supports the trunk of robot, (3) \textbf{wall} that supports the trunk of the robot, and (4) \textbf{slope} with a benign inclination that supports the whole robot.  We visualize these terrains and examples of their corresponding scenes in the real world in \cref{fig:terrain_examples}. 
% More terrain descriptions can be found in Appendix~\red{x}. 


\subsubsection{Domain Randomization}\label{subsec:domain_randomization} To enhance real-world deployment, we employ domain randomization~\cite{tobin2017domain} to bridge the physical gap between simulation and reality. The randomization parameters, detailed in~\cref{table:domain_randomization}, include body mass, base center of mass (CoM) offset, PD gains, torque offset, and initial pose, following~\cite{campanaro2024learning,long2024learning}. Notably, the CoM offset is critical, as it enhances controller robustness against real-world CoM position noise, which may arise from insufficient torques or discrepancies between simulated and real robot models.

\subsection{Implementation Details} Our implementation of PPO is based on \cite{Rudin2021LearningTW}. The actor and critic networks are structured as 3-layer and 2-layer MLPs, respectively. Each episode has a rollout length of 500 steps. For smoothness regularization, the weight coefficients $\lambda_\pi$ and $\lambda_V$ are set to 1 and 0.1, respectively. The PD controller operates at 200 Hz in simulation and 500 Hz on the real robot to ensure accurate tracking of the PD targets, while the control policies run at 50 Hz. Additional implementation details and hardware setup are provided in \cref{app:implementation}.
% \begin{table}[h]
%     \centering
%     \caption{Reward functions for standing-up control.}
%     \begin{tabular}{lll}
%     \toprule Reward & Equation $\left(r_i\right)$ & Weight $\left(w_i\right)$ \\
%     \midrule Lin. velocity tracking & $\exp \left\{- \frac{\|\mathbf{v}_{x y}^{\text {cmd }}-\mathbf{v}_{x y}\|_2^2}{\sigma}  \right\}$ & 1.0 \\
%     % Ang. velocity tracking & $\exp \left\{- \frac{\left(\omega_{\text {yaw }}^{\text {cmd }}-\omega_{\text {yaw }}\right)^2}{\sigma} \right\}$ & 1.0 \\
%     % Linear velocity $(z)$ & $v_z^2$ & -0.5 \\
%     % Angular velocity $(x y)$ & $\|\boldsymbol{\omega}_{x y}\|_2^2$ & -0.025 \\
%     % Orientation & $\|\mathbf{g}_{x}\|_2^2 + \|\mathbf{g}_{y}\|_2^2$ & -1.25 \\
%     % Joint accelerations & $\|\ddot{\boldsymbol{\theta}}\|_2^2$ & $-2.5 \times 10^{-7}$ \\
%     % Joint power & $\frac{|\boldsymbol{\tau} \| \dot{\boldsymbol{\theta}}|^{T}}{\|\mathbf{v}\|_2^2 + 0.2 * \|\boldsymbol{\omega}\|_2^2}$ & $-2.5 \times 10^{-5}$ \\
%     % Body height w.r.t. feet & $\left(h^{\text {target}}-h\right)^2$ & 0.1 \\
%     % Feet clearance & $\sum\limits_{feet} \left(p_{z}^{\text {target}}-p_{z}^{i}\right)^2 \cdot v_{xy}^{i}$ & -0.25 \\
%     % Action rate & $\|\mathbf{a}_t-\mathbf{a}_{t-1}\|_2^2$ & -0.01 \\
%     % Smoothness & $\|\mathbf{a}_t-2 \mathbf{a}_{t-1}+\mathbf{a}_{t-2}\|_2^2$ & -0.01 \\
%     % Feet stumble & $\mathbf{1}\left\{\exists i,\left|\mathbf{F}_i^{x y}\right|>3\left|F_i^z\right|\right\}$ & -3.0 \\
%     % Torques & $\sum\limits_{\text{all joints}} |\frac{\tau_i}{kp_{i}}|_{2}^{2}$ & $-2.5 \times 10^{-6}$ \\
%     % Joint velocity & $\sum\limits_{\text{all joints}} \dot{\theta}_{i}|_{2}^{2}$ & $-1 \times 10^{-4}$ \\
%     % Joint tracking error & $\sum\limits_{\text{all joints}}|\theta_{i} - \theta^{target}_{i}|^{2}$ & -0.25 \\
%     % Arm joint deviation & $\sum\limits_{\text{arm joints}}|\theta_{i} - \theta^{default}_{i}|^{2}$ & -0.1 \\
%     % Hip joint deviation & $\sum\limits_{\text{hip joints}}|\theta_{i} - \theta^{default}_{i}|^{2}$ & -0.5 \\
%     % Waist joint deviation & $\sum\limits_{\text{waist joints}}|\theta_{i} - \theta^{default}_{i}|^{2}$ & -0.25 \\
%     % Joint pos limits & $\sum\limits_{\text{all joints}} \text{out}_{i}$ & -2.0 \\
%     % Joint vel limits & $\sum\limits_{\text{all joints}} RELU(\hat{\theta}_{i} - \hat{\theta}^{max}_{i})$ & -0.1 \\
%     % Torque limits & $\sum\limits_{\text{all joints}} RELU(\hat{\tau}_{i} - \hat{\tau}^{max}_{i})$ & -0.1 \\
%     % No fly & $\mathbf{1}\{\text{only one feet on ground}\}$ & 0.25 \\
%     % Feet lateral distance & $|y_{\text{left foot}}^{B} - y_{\text{right foot}}^{B}| - d_{min}$ & 2.5 \\
%     % Feet slip & $\sum\limits_{feet}\left|\mathbf{v}_i^{\text {toot }}\right| * \sim \mathbf{1}_{\text {new contact }}$ & -0.25 \\
%     % Feet ground parallel & $\sum\limits_{feet}Var(H_i)$ & -2.0 \\
%     % Feet contact force & $\sum\limits_{feet} RELU(F_{i}^{z} - F_{th})$ & $-2.5 \times 10^{-4}$ \\
%     % Feet parallel & $Var(D)$ & -2.5 \\
%     % Contact momentum & $\sum\limits_{feet}|v_{i}^{z} * F_{i}^{z}|$ & $-2.5 \times 10^{-4}$ \\
%     \bottomrule
%     \end{tabular}
%     % \caption{Rewards}
%     \label{table:reward}
% \end{table}


