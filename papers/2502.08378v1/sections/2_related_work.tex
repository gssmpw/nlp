
\begin{table}[t]
    \centering
    \vspace{0.07in}
    \caption{Comparison with existing methods on standing-up control.}
    \resizebox{1\linewidth}{!}{
    \begin{tabular}{lcc ccc}
    \toprule 
    \multirow{2}{*}{Method} & \multirow{2}{*}{\shortstack{Real \\ Robot}} & \multirow{2}{*}{\shortstack{w/o Prior \\ Trajectory }} & \multirow{2}{*}{\shortstack{Beyond \\ Ground}} & \multirow{2}{*}{\shortstack{High \\ DoF}} & \multirow{2}{*}{\shortstack{1-stage \\ Training}} \\ \\
    \midrule
    \citet{Peng2022ASE} & \XSolidBrush & \XSolidBrush & \XSolidBrush & \Checkmark & \XSolidBrush \\
    \citet{yang2023learning} & \XSolidBrush & \XSolidBrush & \XSolidBrush &  \Checkmark &  \Checkmark \\
    \citet{tao2022learning} & \XSolidBrush & \Checkmark & \XSolidBrush & \Checkmark & \XSolidBrush \\ 
    \citet{haarnoja2024learning} & \Checkmark & \XSolidBrush & \XSolidBrush & \Checkmark & \Checkmark \\ 
    \citet{gaspard2024frasa} & \Checkmark & \Checkmark & \XSolidBrush & \XSolidBrush & \Checkmark \\ 
    \cmidrule(r){1-6}
    \ours (ours) & \Checkmark & \Checkmark & \Checkmark & \Checkmark & \Checkmark \\

    \bottomrule
    \end{tabular}}
    \vspace{-0.1in}
    % \caption{Rewards}
    \label{table:comparision_method}
\end{table}


\section{Related Work}
\subsection{Learning Humanoid Standing-up Control}
Classical approaches to standing-up control rely on tracking handcrafted motion trajectories through model-based optimization~\cite{kanehiro2003first,kanehiro2007getting,kuniyoshi2004dynamic,stuckler2006getting}. While effective, these methods are computationally intensive, sensitive to disturbances~\cite{luo2014multi,lee2019robust}, and require precise actuator modeling~\cite{hwangbo2019learning}, limiting their real-world applicability. In contrast, RL-based methods learn control policies with minimal modeling assumptions, either by leveraging predefined motion trajectories to guide exploration~\cite{peng2018deepmimic,Peng2022ASE,yang2023learning,haarnoja2024learning} or employing exploratory strategies to learn from scratch~\cite{tao2022learning}.  However, none of these methods have demonstrated real-world standing-up motion across diverse postures. Our proposed RL framework addresses these limitations by achieving posture adaptivity and real-world deployability without predefined motions, enabling smooth, stable, and robust standing-up across a wide range of laboratory and outdoor environments.

\subsection{Reinforcement Learning for Humanoid Control}
Reinforcement learning (RL) has been effectively applied to various humanoid control tasks, showcasing its versatility and effectiveness. For example, RL has enabled humanoid robots to achieve robust locomotion on diverse terrains~\cite{radosavovic2024real,li2024reinforcement,zhuang2024humanoid,long2024learning}, whole-body control for expressive human-like motions~\cite{peng2018deepmimic,Peng2022ASE,He2024OmniH2OUA,he2024hover,Cheng2024ExpressiveWC}, versatile jumping~\cite{Zhang2024WoCoCoLW}, and loco-manipulation~\cite{dao2024sim,liu2024opt2skill,wang2024hypermotion}. Building on these advances, we address humanoid standing-up control, a parallel problem presenting unique challenges due to its dynamic nature and the need for precise coordination of multi-stage motor skills and time-varying contact points~\cite{kanehiro2003first,luo2014multi}. We propose a novel approach that integrates a multi-critic framework, motion constraints, and a training curriculum to facilitate real-world deployment, setting it apart from prior methods.

\subsection{Learning Quadrupedal Robot Standing-up Control}
Standing-up control for quadrupedal robots shares similarities with humanoid robots but faces distinct challenges due to morphological differences, such as quadrupedal designs. Classical approaches for quadrupedal robots often rely on model-based optimization and predefined motion primitives~\cite{castano2019design,saranli2004model}, which work well in controlled environments but struggle with adaptability to diverse postures and real-world uncertainties. Recent RL-based methods have enabled quadrupedal robots to recover from falls and transition between poses~\cite{lee2019robust,ma2023learning,yang2023learning}, using exploratory learning to manage complex dynamics and environmental interactions. Our work draws inspiration from these advances, extending them to humanoid robots by addressing the unique challenges of bipedal standing-up control. By incorporating posture adaptivity, motion constraints, and a structured training curriculum, our framework bridges the gap between quadrupedal and humanoid robot control, enabling robust standing-up motions across diverse environments.
