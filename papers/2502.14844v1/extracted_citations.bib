@article{HaCohen2024LTXVideo,
  title={LTX-Video: Realtime Video Latent Diffusion},
  author={HaCohen, Yoav and Chiprut, Nisan and Brazowski, Benny and Shalem, Daniel and Moshe, Dudu and Richardson, Eitan and Levin, Eran and Shiran, Guy and Zabari, Nir and Gordon, Ori and Panet, Poriya and Weissbuch, Sapir and Kulikov, Victor and Bitterman, Yaki and Melumian, Zeev and Bibi, Ofir},
  journal={arXiv preprint arXiv:2501.00103},
  year={2024}
}

@article{HunyuanVideo,
  title={HunyuanVideo: A Systematic Framework For Large Video Generative Models},
  author={Kong, Weijie and Tian, Qi and Zhang, Zijian and Min, Rox and Dai, Zuozhuo and Zhou, Jin and Xiong, Jiangfeng and Li, Xin and Wu, Bo and Zhang, Jianwei and others},
  journal={arXiv preprint arXiv:2412.03603},
  year={2024}
}

@inproceedings{RIN,
    author = {Jabri, Allan and Fleet, David J. and Chen, Ting},
    title = {Scalable adaptive computation for iterative generation},
    year = {2023},
    publisher = {JMLR.org},
    abstract = {Natural data is redundant yet predominant architectures tile computation uniformly across their input and output space. We propose the Recurrent Interface Network (RIN), an attention-based architecture that decouples its core computation from the dimensionality of the data, enabling adaptive computation for more scalable generation of high-dimensional data. RINs focus the bulk of computation (i.e. global self-attention) on a set of latent tokens, using cross-attention to read and write (i.e. route) information between latent and data tokens. Stacking RIN blocks allows bottomup (data to latent) and top-down (latent to data) feedback, leading to deeper and more expressive routing. While this routing introduces challenges, this is less problematic in recurrent computation settings where the task (and routing problem) changes gradually, such as iterative generation with diffusion models. We show how to leverage recurrence by conditioning the latent tokens at each forward pass of the reverse diffusion process with those from prior computation, i.e. latent self-conditioning. RINs yield state-of-the-art pixel diffusion models for image and video generation, scaling to 1024\texttimes{}1024 images without cascades or guidance, while being domain-agnostic and up to 10\texttimes{} more efficient than 2D and 3D U-Nets.},
    booktitle = {Proceedings of the 40th International Conference on Machine Learning},
    articleno = {594},
    numpages = {21},
    location = {Honolulu, Hawaii, USA},
    series = {ICML'23}
}

@inproceedings{U-net,
  title={U-net: Convolutional networks for biomedical image segmentation},
  author={Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
  booktitle={Medical Image Computing and Computer-Assisted Intervention--MICCAI 2015: 18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18},
  pages={234--241},
  year={2015},
  organization={Springer}
}

@misc{atomovideo,
      title={AtomoVideo: High Fidelity Image-to-Video Generation},
      author={Gong, Litong and Zhu, Yiran and Li, Weijie and Kang, Xiaoyang and Wang, Biao and Ge, Tiezheng and Zheng, Bo},
      year={2024},
      eprint={arXiv:2403.01800},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@article{avrahami2023break,
  title={Break-A-Scene: Extracting Multiple Concepts from a Single Image},
  author={Avrahami, Omri and Aberman, Kfir and Fried, Ohad and Cohen-Or, Daniel and Lischinski, Dani},
  journal={arXiv preprint arXiv:2305.16311},
  year={2023}
}

@article{bai2024uniedit,
            title={UniEdit: A Unified Tuning-Free Framework for Video Motion and Appearance Editing},
            author={Bai, Jianhong and He, Tianyu and Wang, Yuchi and Guo, Junliang and Hu, Haoji and Liu, Zuozhu and Bian, Jiang},
            journal={arXiv preprint arXiv:2402.13185},
            year={2024}
          }

@article{blattmann2023stable,
  title={Stable video diffusion: Scaling latent video diffusion models to large datasets},
  author={Blattmann, Andreas and Dockhorn, Tim and Kulal, Sumith and Mendelevitch, Daniel and Kilian, Maciej and Lorenz, Dominik and Levi, Yam and English, Zion and Voleti, Vikram and Letts, Adam and others},
  journal={arXiv preprint arXiv:2311.15127},
  year={2023}
}

@inproceedings{cai2023genren,
                            author={Cai, Shengqu and Ceylan, Duygu and Gadelha, Matheus
                                    and Huang, Chun-Hao and Wang, Tuanfeng and Wetzstein, Gordon.},
                            title={Generative Rendering: Controllable 4D-Guided Video Generation with 2D Diffusion Models},
                            booktitle={CVPR},
                            year={2024}
                        }

@misc{dai2023animateanything,
      title={AnimateAnything: Fine-Grained Open Domain Image Animation with Motion Guidance}, 
      author={Zuozhuo Dai and Zhenghao Zhang and Yao Yao and Bingxue Qiu and Siyu Zhu and Long Qin and Weizhi Wang},
      year={2023},
      eprint={2311.12886},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@article{gal2022image,
  title={An image is worth one word: Personalizing text-to-image generation using textual inversion},
  author={Gal, Rinon and Alaluf, Yuval and Atzmon, Yuval and Patashnik, Or and Bermano, Amit H and Chechik, Gal and Cohen-Or, Daniel},
  journal={arXiv preprint arXiv:2208.01618},
  year={2022}
}

@article{guo2023animatediff,
  title={Animatediff: Animate your personalized text-to-image diffusion models without specific tuning},
  author={Guo, Yuwei and Yang, Ceyuan and Rao, Anyi and Liang, Zhengyang and Wang, Yaohui and Qiao, Yu and Agrawala, Maneesh and Lin, Dahua and Dai, Bo},
  journal={arXiv preprint arXiv:2307.04725},
  year={2023}
}

@misc{he2024idanimatorzeroshotidentitypreservinghuman,
      title={ID-Animator: Zero-Shot Identity-Preserving Human Video Generation}, 
      author={Xuanhua He and Quande Liu and Shengju Qian and Xin Wang and Tao Hu and Ke Cao and Keyu Yan and Jie Zhang},
      year={2024},
      eprint={2404.15275},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2404.15275}, 
}

@article{ho2022imagen,
  title={Imagen video: High definition video generation with diffusion models},
  author={Ho, Jonathan and Chan, William and Saharia, Chitwan and Whang, Jay and Gao, Ruiqi and Gritsenko, Alexey and Kingma, Diederik P and Poole, Ben and Norouzi, Mohammad and Fleet, David J and others},
  journal={arXiv preprint arXiv:2210.02303},
  year={2022}
}

@article{hong2022cogvideo,
  title={Cogvideo: Large-scale pretraining for text-to-video generation via transformers},
  author={Hong, Wenyi and Ding, Ming and Zheng, Wendi and Liu, Xinghan and Tang, Jie},
  journal={arXiv preprint arXiv:2205.15868},
  year={2022}
}

@misc{hu2021loralowrankadaptationlarge,
      title={LoRA: Low-Rank Adaptation of Large Language Models}, 
      author={Edward J. Hu and Yelong Shen and Phillip Wallis and Zeyuan Allen-Zhu and Yuanzhi Li and Shean Wang and Lu Wang and Weizhu Chen},
      year={2021},
      eprint={2106.09685},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2106.09685}, 
}

@misc{jones2024customizingtexttoimagemodelssingle,
      title={Customizing Text-to-Image Models with a Single Image Pair}, 
      author={Maxwell Jones and Sheng-Yu Wang and Nupur Kumari and David Bau and Jun-Yan Zhu},
      year={2024},
      eprint={2405.01536},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2405.01536}, 
}

@inproceedings{kara2024rave,
  title={RAVE: Randomized Noise Shuffling for Fast and Consistent Video Editing with Diffusion Models},
  author={Ozgur Kara and Bariscan Kurtkaya and Hidir Yesiltepe and James M. Rehg and Pinar Yanardag},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  year={2024}
}

@article{liang2023flowvid,
  title={FlowVid: Taming Imperfect Optical Flows for Consistent Video-to-Video Synthesis},
  author={Liang, Feng and Wu, Bichen and Wang, Jialiang and Yu, Licheng and Li, Kunpeng and Zhao, Yinan and Misra, Ishan and Huang, Jia-Bin and Zhang, Peizhao and Vajda, Peter and others},
  journal={arXiv preprint arXiv:2312.17681},
  year={2023}
}

@misc{liu2024unziploraseparatingcontentstyle,
      title={UnZipLoRA: Separating Content and Style from a Single Image}, 
      author={Chang Liu and Viraj Shah and Aiyu Cui and Svetlana Lazebnik},
      year={2024},
      eprint={2412.04465},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2412.04465}, 
}

@misc{ma2024magicmeidentityspecificvideocustomized,
      title={Magic-Me: Identity-Specific Video Customized Diffusion}, 
      author={Ze Ma and Daquan Zhou and Chun-Hsiao Yeh and Xue-She Wang and Xiuyu Li and Huanrui Yang and Zhen Dong and Kurt Keutzer and Jiashi Feng},
      year={2024},
      eprint={2402.09368},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2402.09368}, 
}

@inproceedings{materzynska2024newmove,
  title={NewMove: Customizing text-to-video models with novel motions},
  author={Materzy{\'n}ska, Joanna and Sivic, Josef and Shechtman, Eli and Torralba, Antonio and Zhang, Richard and Russell, Bryan},
  booktitle={Proceedings of the Asian Conference on Computer Vision},
  pages={1634--1651},
  year={2024}
}

@inproceedings{menapace2024snap,
  title={Snap video: Scaled spatiotemporal transformers for text-to-video synthesis},
  author={Menapace, Willi and Siarohin, Aliaksandr and Skorokhodov, Ivan and Deyneka, Ekaterina and Chen, Tsai-Shien and Kag, Anil and Fang, Yuwei and Stoliar, Aleksei and Ricci, Elisa and Ren, Jian and others},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={7038--7048},
  year={2024}
}

@misc{molad2023dreamixvideodiffusionmodels,
      title={Dreamix: Video Diffusion Models are General Video Editors}, 
      author={Eyal Molad and Eliahu Horwitz and Dani Valevski and Alex Rav Acha and Yossi Matias and Yael Pritch and Yaniv Leviathan and Yedid Hoshen},
      year={2023},
      eprint={2302.01329},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2302.01329}, 
}

@article{polyak2024movie,
  title={Movie gen: A cast of media foundation models},
  author={Polyak, Adam and Zohar, Amit and Brown, Andrew and Tjandra, Andros and Sinha, Animesh and Lee, Ann and Vyas, Apoorv and Shi, Bowen and Ma, Chih-Yao and Chuang, Ching-Yao and others},
  journal={arXiv preprint arXiv:2410.13720},
  year={2024}
}

@article{qi2023fatezero,
        title={FateZero: Fusing Attentions for Zero-shot Text-based Video Editing}, 
        author={Chenyang Qi and Xiaodong Cun and Yong Zhang and Chenyang Lei and Xintao Wang and Ying Shan and Qifeng Chen},
        year={2023},
        journal={arXiv:2303.09535},
}

@misc{qian2024omniidholisticidentityrepresentation,
      title={Omni-ID: Holistic Identity Representation Designed for Generative Tasks}, 
      author={Guocheng Qian and Kuan-Chieh Wang and Or Patashnik and Negin Heravi and Daniil Ostashev and Sergey Tulyakov and Daniel Cohen-Or and Kfir Aberman},
      year={2024},
      eprint={2412.09694},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2412.09694}, 
}

@article{ren2024consisti2v,
  title={ConsistI2V: Enhancing Visual Consistency for Image-to-Video Generation},
  author={Ren, Weiming and Yang, Harry and Zhang, Ge and Wei, Cong and Du, Xinrun and Huang, Stephen and Chen, Wenhu},
  journal={arXiv preprint arXiv:2402.04324},
  year={2024}
}

@article{ren2024customize,
  title={Customize-a-video: One-shot motion customization of text-to-video diffusion models},
  author={Ren, Yixuan and Zhou, Yang and Yang, Jimei and Shi, Jing and Liu, Difan and Liu, Feng and Kwon, Mingi and Shrivastava, Abhinav},
  journal={arXiv preprint arXiv:2402.14780},
  year={2024}
}

@inproceedings{ruiz2023dreambooth,
  title={Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation},
  author={Ruiz, Nataniel and Li, Yuanzhen and Jampani, Varun and Pritch, Yael and Rubinstein, Michael and Aberman, Kfir},
  booktitle={CVPR},
  pages={22500--22510},
  year={2023}
}

@article{ruiz2023hyperdreambooth,
  title={Hyperdreambooth: Hypernetworks for fast personalization of text-to-image models},
  author={Ruiz, Nataniel and Li, Yuanzhen and Jampani, Varun and Wei, Wei and Hou, Tingbo and Pritch, Yael and Wadhwa, Neal and Rubinstein, Michael and Aberman, Kfir},
  journal={arXiv preprint arXiv:2307.06949},
  year={2023}
}

@article{singer2022make,
  title={Make-a-video: Text-to-video generation without text-video data},
  author={Singer, Uriel and Polyak, Adam and Hayes, Thomas and Yin, Xi and An, Jie and Zhang, Songyang and Hu, Qiyuan and Yang, Harry and Ashual, Oron and Gafni, Oran and others},
  journal={arXiv preprint arXiv:2209.14792},
  year={2022}
}

@article{sora,
  title={SORA},
  author={OPENAI},
  journal={https://openai.com/sora/},
  year={2024}
}

@article{tokenflow2023,
        title = {TokenFlow: Consistent Diffusion Features for Consistent Video Editing},
        author = {Geyer, Michal and Bar-Tal, Omer and Bagon, Shai and Dekel, Tali},
        journal={arXiv preprint arxiv:2307.10373},
        year={2023}
        }

@misc{wang2024moamixtureofattentionsubjectcontextdisentanglement,
      title={MoA: Mixture-of-Attention for Subject-Context Disentanglement in Personalized Image Generation}, 
      author={Kuan-Chieh Wang and Daniil Ostashev and Yuwei Fang and Sergey Tulyakov and Kfir Aberman},
      year={2024},
      eprint={2404.11565},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2404.11565}, 
}

@inproceedings{wei2023dreamvideo,
  title={DreamVideo: Composing Your Dream Videos with Customized Subject and Motion},
  author={Wei, Yujie and Zhang, Shiwei and Qing, Zhiwu and Yuan, Hangjie and Liu, Zhiheng and Liu, Yu and Zhang, Yingya and Zhou, Jingren and Shan, Hongming},
  booktitle={CVPR},
  year={2024}
}

@misc{wu2023tuneavideooneshottuningimage,
      title={Tune-A-Video: One-Shot Tuning of Image Diffusion Models for Text-to-Video Generation}, 
      author={Jay Zhangjie Wu and Yixiao Ge and Xintao Wang and Weixian Lei and Yuchao Gu and Yufei Shi and Wynne Hsu and Ying Shan and Xiaohu Qie and Mike Zheng Shou},
      year={2023},
      eprint={2212.11565},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2212.11565}, 
}

@article{wu2024customttt,
        title={CustomTTT: Motion and Appearance Customized Video Generation via Test-Time Training},
        author={Bi, Xiuli and Lu, Jian and Liu, Bo and Cun, Xiaodong and Zhang, Yong and Li, WeiSheng and Xiao, Bin},
        journal={arXiv preprint arXiv:2412.15646},
        year={2024}
      }

@article{yang2024cogvideox,
  title={CogVideoX: Text-to-Video Diffusion Models with An Expert Transformer},
  author={Yang, Zhuoyi and Teng, Jiayan and Zheng, Wendi and Ding, Ming and Huang, Shiyu and Xu, Jiazheng and Yang, Yuanming and Hong, Wenyi and Zhang, Xiaohan and Feng, Guanyu and others},
  journal={arXiv preprint arXiv:2408.06072},
  year={2024}
}

@article{zhang2023motioncrafter,
  title={MotionCrafter: One-Shot Motion Customization of Diffusion Models},
  author={Zhang, Yuxin and Tang, Fan and Huang, Nisha and Huang, Haibin and Ma, Chongyang and Dong, Weiming and Xu, Changsheng},
  journal={arXiv preprint arXiv:2312.05288},
  year={2023}
}

@misc{zhang2024moonshotcontrollablevideogeneration,
      title={Moonshot: Towards Controllable Video Generation and Editing with Multimodal Conditions}, 
      author={David Junhao Zhang and Dongxu Li and Hung Le and Mike Zheng Shou and Caiming Xiong and Doyen Sahoo},
      year={2024},
      eprint={2401.01827},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2401.01827}, 
}

@article{zhao2023motiondirector,
  title={MotionDirector: Motion Customization of Text-to-Video Diffusion Models},
  author={Zhao, Rui and Gu, Yuchao and Wu, Jay Zhangjie and Zhang, David Junhao and Liu, Jiawei and Wu, Weijia and Keppo, Jussi and Shou, Mike Zheng},
  journal={arXiv preprint arXiv:2310.08465},
  year={2023}
}

@misc{zhou2024storydiffusionconsistentselfattentionlongrange,
      title={StoryDiffusion: Consistent Self-Attention for Long-Range Image and Video Generation}, 
      author={Yupeng Zhou and Daquan Zhou and Ming-Ming Cheng and Jiashi Feng and Qibin Hou},
      year={2024},
      eprint={2405.01434},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2405.01434}, 
}

