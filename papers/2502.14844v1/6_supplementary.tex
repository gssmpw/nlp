% \clearpage
\section{Architecture and Training Details}

We build our framework as a video diffusion model operating in the latent space of a video autoencoder.

The latent representation is based on a causal video autoencoder following the architecture of MAGVITv2~\cite{yu2024language}. Our autoencoder presents a high compression ratio of $8\times16\times16$ on the time and spatial dimensions, respectively, with a bottleneck dimensionality of 32 channels. Full model details are given in Table~\ref{tab:autoencoder_magvit}.

The video diffusion backbone consists in a 11.5B parameters DiT~\cite{Peebles2023DiT} detailed in Table~\ref{tab:backbone_dit}. The model is organized into 32 DiT Blocks with a hidden dimensionality of 4096 channels, each of which consists of a self-attention layer, followed by a cross-attention layer to attend to text conditioning, and a final MLP with an expansion factor of $4\times$. To further reduce the input dimensionality, a ViT-like~\cite{Dosovitskiy2021ViT} $1\times2\times2$ input patchification operation is applied, increasing the effective video compression factor to $8\times32\times32$. This allows modeling of a 121 frames $1024\times576$px video using only 9216 tokens, and enables the use of full 3D self-attention for high-quality motion modeling~\cite{polyak2024movie} without incurring a large computational penalty associated with its quadratic cost, which is further reduced by the use of a 6144 tokens attention window. Each self-attention block consists of 32 attention heads with QK-Normalization~\cite{EsserKSD3} and is augmented with 3D-RoPE~\cite{su2024roformer} embeddings, separately applied to the attention head channels in a ratio of $2:1:1$ for the temporal and spatial dimensions, respectively. Text prompts are encoded by the T5~\cite{raffel2020t5} model and combined with video tokens through the cross-attention layers. Following \cite{Peebles2023DiT}, diffusion timestep information is injected through modulation.

We perform pretraining of the model by jointly training it on a mixture of image and video data with a resolution of $512$ or $1024$ px, aspect ratios of $16:9$ and $9:16$ for videos, and $16:9$, $1:1$, and $9:16$ for image content. We adopt a progressive training strategy on the video temporal dimension, progressively extending the number of frames from 17 to 121, corresponding to 5 seconds at our fixed framerate of 24 fps. During the pretraining stage, we use the AdamW~\cite{loshchilov2018decoupled} optimizer with a fixed learning rate of $1e-4$, 10k steps warmup, a weight decay of 0.01, $\beta$ = [0.9, 0.99], $\epsilon=1e-8$ and a total of 822k training steps. Model training is accelerated through flash attention~\cite{dao2023flashattention2} with bf16 precision, and is distributed on 256 H100 GPUs using FSDP~\cite{zhao2023pytorchfsdpexperiencesscaling}.

%Our framework is a latent-based diffusion model, using MAGVIT~\cite{} and DiT~\cite{}  as the autoencoder and the video generation backbone respectively. We detail
%the hyperparameters of our model architecture in Table 5. To accelerate the model, we utilize the positional embeddings and self-attention in RoPE~\cite{} and adopt flash attention~\cite{} and fused layer normalization~\cite{}.


The training details of our \textit{Set-and-Sequence} approach are summarized in Table~\ref{tab:training_stages}. The model is trained in two stages. For a single video, Stage I of the two-stage approach without regularization is trained for 150 steps, while Stage II is trained for 400 steps, requiring a total of approximately 90 minutes to converge. However, for our final method, which incorporates dropout regularization, convergence is slower. The number of training steps varies based on the complexity and number of videos. For a single video, Stage I is trained for 600 steps and Stage II for 900 steps. For multiple dynamic concepts and videos with complex motions, such as athletic dance sequences, Stage II requires extended training of 2k to 2.5k steps. To optimize text tokens effectively while avoiding overfitting, we use a lower learning rate of $1e-5$. We observe that our method is able to generalize without optimizing for these special tokens as well. Our training uses the AdamW optimizer~\cite{loshchilov2018decoupled} with a constant learning rate of $1e-4$. To ensure stable training, we set $\beta = [0.9, 0.99]$, apply a weight decay of 0.01, and use gradient clipping with a value of 0.05. Additionally, text prompt tokens are randomly dropped with a probability of 0.1. All experiments are conducted on NVIDIA A100 GPUs with 80GB of memory, using a batch size of 8. We use a  \textit{cfg} value of 8 to generate the results.

% Training details of our \textit{set-and-sequence} approach is shown in Table~\ref{tab:training_stages}. We train the model in two stages. For a single video, the Stage I of the Two-Stage without regularization is trained for 150 steps and the Stage II for 400 steps. This takes a total of about 90 minutes to converge. For our final method, since we use dropout regualrization, the convergence is slower. Depending on the number of videos, the training steps are different. For a single video case, Stage I is trained for 600 number of steps and Stage II is trained for 900 number of steps. For videos with complex motions like the atheletic dance, we train Stage II for 2k - 2.5k steps. We use the same warmup schedule as the model pre-training. The optimization of text token requires a lower learning rate to avoid overfitting. Choosing lower learning rates like $1e-5$ or omitting the optimization avoids this overfitting. We use the AdamW~\cite{loshchilov2018decoupled} optimizer with a constant learning rate of $1e-4$. To achieve stable training, we set $\beta$ = [0.9, 0.99], a
% weight decay of 0.01, gradient clipping with the value of 0.05. We randomly drop the text prompt tokens with 0.1 probability. We perform these experiments on Nvidia 80GB A100 GPUs with a batch size of 8.

% Diffusion steps ???
% Time (training / generation) ??




\begin{table}[h!]
\centering
\begin{tabular}{@{}ll@{}}
\toprule
\textbf{Autoencoder}      & \textbf{MAGVIT}                        \\ \midrule
Base channels             & 16                                     \\
Channel multiplier        & [1, 4, 16, 32, 64]                     \\
Encoder blocks count      & [1, 1, 2, 8, 8]                        \\
Decoder blocks count      & [4, 4, 4, 4, 4]                        \\
Stride of frame           & [1, 2, 2, 2, 1]                        \\
Stride of h and w         & [2, 2, 2, 2, 1]                        \\
Padding mode              & replicate                              \\
Compression rate          & \(8 \times 16 \times 16\)              \\
Bottleneck channels       & 32                                     \\
Use KL divergence         & \checkmark                             \\
Use adaptive norm         & \checkmark (decoder only)              \\ \bottomrule
\end{tabular}
\caption{Autoencoder and MAGVIT specifications.}
\label{tab:autoencoder_magvit}
\end{table}


\begin{table}[h!]
\centering
\begin{tabular}{@{}ll@{}}
\toprule
\textbf{Backbone}               & \textbf{DiT}                     \\ \midrule
Input channels                  & 32                               \\
Patch size                      & \(1 \times 2 \times 2\)          \\
Latent token channels           & 4096                             \\
Positional embeddings           & 3D-RoPE                          \\
DiT blocks count                & 32                               \\
Attention heads count           & 32                              \\
Window size                     & 6144 (center)                    \\
Normalization                   & Layer normalization              \\
Use flash attention             & \checkmark                       \\
Use QK-normalization            & \checkmark                       \\
Use self conditioning           & \checkmark                       \\
Self conditioning prob.         & 0.9                              \\
Context channels                & 1024                             \\ \bottomrule
\end{tabular}
\caption{Backbone and DiT specifications.}
\label{tab:backbone_dit}
\end{table}





\begin{table}[t]
\centering
\begin{tabular}{@{}lcc@{}}
\toprule
% \textbf{Stage}              & \textbf{I} & \textbf{II}  \\ \midrule
% Steps                       & X      & X             \\
% Warmup steps                & X      & X            \\
% Samples seen                & X      & X            \\ \midrule
Optimizer                   & \multicolumn{2}{c}{AdamW}              \\
Learning rate               & \multicolumn{2}{c}{$1 \times 10^{-4}$} \\
LR scheduler                & \multicolumn{2}{c}{constant}           \\
Beta                        & \multicolumn{2}{c}{[0.9, 0.99]}        \\
Weight decay                & \multicolumn{2}{c}{0.01}               \\
Gradient clipping           & \multicolumn{2}{c}{0.05}               \\
Dropout (Stage I)           & \multicolumn{2}{c}{0.8}                \\
Dropout (Stage II)           & \multicolumn{2}{c}{0.5}                \\\bottomrule
\end{tabular}
\caption{Training stages and optimization settings.}
\label{tab:training_stages}
\end{table}


\section{Prompts} Providing detailed prompts at the initialization stage is crucial for achieving high-quality editing and composition. Prompts describe not only the appearance but also their environment and dynamic behavior in detail. This allows the model to align the text description with the corresponding video frames effectively, leading to better identity and motion preservation during editing and composition.

For example, in the dancer case shown in Figure 2 of the main paper, the prompt explicitly defines the appearance, surroundings, and action: "A [v] man in black track pants, gray shirt, and cap near a road on a bridge with hands down and legs up. The man is performing [u] dancing motion with hands and legs." Here, the description of the attire (black track pants, gray shirt, and cap) ensures the model captures the subject's visual identity, while the mention of the environment (a road on a bridge) provides spatial context. The prompt also includes details about the motion ("dancing motion with hands and legs"), guiding the model to encode temporal dynamics accurately.

% \section{Ethical Concerns}
% The advancements in generative AI, including personalized video generation, bring remarkable opportunities for creativity, education, entertainment, and other constructive applications. However, these capabilities also come with ethical challenges that must be acknowledged. The potential misuse of this technology to create deepfakes, manipulate identities, or generate misleading content is a serious concern. Unauthorized use of personal data or the replication of an individual's appearance and mannerisms without consent could lead to privacy violations, reputational damage, and erosion of trust in digital media. Additionally, biases inherent in generative models might result in unfair or stereotypical representations, further emphasizing the need for responsible development and deployment practices. We strongly emphasize that such technology must be used ethically and responsibly. As researchers, we do not condone any misuse of generative AI for malicious purposes, including spreading misinformation, violating privacy, or creating harmful content. Instead, we advocate for its application in areas like education, storytelling, virtual production, and accessibility.

