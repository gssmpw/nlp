\section{Related Work}
\begin{figure*}[t!]
        \centering
        
        \includegraphics[width=\linewidth]{images/editing.jpg}
        % \vspace{-0.6cm}
        \caption{\textbf{Local and Global Editing.} Our \textit{Set-and-Sequence} framework enables text-driven edits of dynamic concepts while preserving both their appearance and motion. Edits can be global (e.g., background and lighting) or local (e.g., clothing and object replacement), ensuring high fidelity to the original dynamic concepts.}
        % \vspace{-0.5cm}
        \label{fig:editing}
    \end{figure*}




\subsection{Foundational Video Models.}

Foundational video models, such as Imagen~Video~\cite{ho2022imagen}, Sora~\cite{sora}, CogVideoX~\cite{yang2024cogvideox}, Veo2~\cite{Veo2}, MovieGen~\cite{polyak2024movie} and others have made significant strides in synthesizing visually stunning and semantically aligned videos from textual descriptions. They were originally based on U-net-like~\cite{U-net} architectures~\cite{hong2022cogvideo,singer2022make,guo2023animatediff,blattmann2023stable} and were extending image generators to video synthesis by training additional temporal layers to model dynamics. However, in the pursuit of greater scalability, the community switched to transformer-based backbones with joint spatio-temporal modeling (e.g., \cite{sora,RIN,menapace2024snap}), which quickly became the dominant paradigm for large-scale video generation (e.g., \cite{polyak2024movie,sora,HunyuanVideo,yang2024cogvideox}). While these models excel at generating coherent content, they primarily rely on generic motion trajectories, limiting their ability to capture nuanced human expressions, individualized mannerisms, or complex dynamic interactions within a shared scene~\cite{sora,menapace2024snap}.
These limitations highlight the need for methods capable of personalization, dynamic scene composition, and precise editing in generative video models.
To address these challenges, we build on the video DiT (DiT version of Snap Video~\cite{menapace2024snap}) architecture and extend its capabilities with our proposed Set-and-Sequence framework, enabling the representation and compositionality of dynamic concepts with unprecedented fidelity and adaptability.







\subsection{Video Personalization and Motion Representation.}
While personalization in image generation has seen significant advancementsâ€”enabling identity preservation, stylization, and tailored manipulation~\cite{ruiz2023dreambooth,ruiz2023hyperdreambooth,gal2022image, liu2024unziploraseparatingcontentstyle,jones2024customizingtexttoimagemodelssingle}---video personalization remains relatively underexplored. In the video domain, personalization methods predominantly build upon UNet-based architectures~\cite{he2024idanimatorzeroshotidentitypreservinghuman,ma2024magicmeidentityspecificvideocustomized,zhang2024moonshotcontrollablevideogeneration,zhou2024storydiffusionconsistentselfattentionlongrange,bai2024uniedit,wu2023tuneavideooneshottuningimage}, inheriting their shortcomings.
Furthermore, approaches in this domain can be broadly categorized into three domains.
First, works like Token Flow~\cite{tokenflow2023} focus on video stylization~\cite{cai2023genren,kara2024rave,liang2023flowvid,zhang2023motioncrafter}.
Second, methods like DreamVideo~\cite{wei2023dreamvideo} and others~\cite{wu2024customttt,materzynska2024newmove,zhao2023motiondirector} emphasize extracting motion dynamics from several videos to perform motion transfer.
Third, approaches like Customize-a-Video~\cite{ren2024customize} , Fate/Zero~\cite{qi2023fatezero}, and DreamMix~\cite{molad2023dreamixvideodiffusionmodels} perform local editing on single videos by optimizing specific parts.
Although promising, these methods, such as Customize-a-Video~\cite{ren2024customize} are architecture specific and operate on the assumption that motion and appearance are disentangled, optimizing distinct LoRA~\cite{hu2021loralowrankadaptationlarge} modules or layers for each.
This rigid separation often leads to artifacts, losing fidelity and contextual realism.
Moreover, they primarily target applications like motion transfer, diverting focus from video personalization that captures the inherent entanglement of appearance and motion in dynamic concepts.
To solve this, we introduce a \textit{shared} spatio-temporal weight space that cohesively encodes dynamic concepts using a two-stage LoRA~\cite{hu2021loralowrankadaptationlarge} training.


\subsection{Scene Composition in Video Models.}
Scene composition and dynamic editing remain significant challenges in video synthesis due to the complexities of maintaining temporal coherence and contextual fidelity.
Approaches like Break-A-Scene~\cite{avrahami2023break} enable concept-level blending but are limited to static, image-like representations, relying heavily on predefined masks and cross-attention mechanisms.
In video models, scene composition often involves generating composed images using personalized text-to-image methods~\cite{qian2024omniidholisticidentityrepresentation,wang2024moamixtureofattentionsubjectcontextdisentanglement} and then applying image-to-video techniques to synthesize motion dynamics on top of the static image~\cite{chen2024videoalchemy, ren2024consisti2v,atomovideo,blattmann2023stable,dai2023animateanything,HaCohen2024LTXVideo}.
However, these models face several inherent limitations. First, they depend on powerful image composition models capable of blending multiple objects into a cohesive scene effectively ignoring the motion~\cite{blattmann2023stable,dai2023animateanything}. Second, they lack awareness of object-specific attributes, such as viewpoints, dynamic evolution of motion, and spatial relationships~\cite{blattmann2023stable,dai2023animateanything}.
Third, they fail to account for nuanced expressions and intricate motion patterns that cannot be adequately captured through text descriptions alone~\cite{dai2023animateanything,chen2024videoalchemy}.
These shortcomings render such models incompatible with the goals of video personalization and advanced compositionality.
For the first time, we demonstrate advanced compositionality by merging disparate dynamics, such as fire and water, while capturing both appearance as well as motion from single videos.
Our approach overcomes the limitations of previous methods, offering a unified framework that enables personalization of dynamic concepts.