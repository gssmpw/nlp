
% \vspace{-0.25cm}
\section{Method}


\begin{figure}[t!]
    \centering
    % \vspace{-0.25cm}
    \includegraphics[width=\linewidth]{images/pixar.jpg}
    % \vspace{-0.7cm}
    \caption{\textbf{Stylization.} Top: Stylization of dynamic concepts achieved by reweighting the identity basis. Bottom: Stylization and motion editing performed using prompt derived from the video in the top row. } 
    % \vspace{-0.6cm}
    \label{fig:pixar}
\end{figure}



\begin{figure*}[t!]
    \centering
    \includegraphics[width=\linewidth]{images/compose.jpg}
    \caption{\textbf{Dynamic Concepts Composition.} Composition results achieved by our framework showcasing seamless integration of dynamic concepts. with each concept color-coded for clarity. For a more comprehensive demonstration, refer to the supplementary videos.}
    \label{fig:compose}
\end{figure*}






We propose \textit{Set-and-Sequence} (See Fig.~\ref{fig:pipeline}), a novel framework for personalizing text-to-video models using dynamic concepts extracted from single-video examples. Our approach learns these dynamic concepts as a decomposition of appearance and motion into a unified spatio-temporal weight space inspired by the state-of-the-art generators~\cite{menapace2024snap, sora}. We impose this weight space in DiT-based diffusion architecture~\cite{Peebles2023DiT}, an architecture that does not explicitly separate spatial and temporal features unlike UNet-based architectures~\cite{ren2024customize}, resulting in seamless compositionality, editing, and adaptation. Central to our framework is a two-stage learning technique. In the first stage, \textit{Identity Basis Learning}, we train Low-Rank Adaptation (LoRA) layers on an unordered set of video frames, extracting a static, motion-independent identity basis that captures the appearance of the concept. In the second stage, \textit{Motion Residual Encoding}, the identity basis is augmented with motion dynamics by fine-tuning coefficients on the full video sequence. We employ additional regularizations and employ text conditioning at each stage, using static prompts for appearance learning and a combination of static and dynamic prompts for encoding motion dynamics. At inference time, this enables intuitive reprompting, recomposing, and editing of content using only text descriptions, facilitating advanced personalization and dynamic scene composition.



\subsection{Preliminaries}

\paragraph{Video Diffusion with Flow Matching Loss}
Our framework builds on a video diffusion model trained with a flow matching loss~\cite{FlowStraightAndFast, NormFlowStochInterp}. This objective aligns the predicted and true velocity fields and is defined as:
\begin{equation}
\mathcal{L}_{\text{flow}} = \mathbb{E}_{\mathbf{x}, t} \left[ \left\| \mathbf{v}_\theta (\mathbf{x}_t, t) - \frac{\partial \mathbf{x}_t}{\partial t} \right\|_2^2 \right],
\end{equation}
where $\mathbf{x}_t$ represents the perturbed data at time $t$, $\mathbf{v}_\theta$ is the predicted velocity field, and $\frac{\partial \mathbf{x}_t}{\partial t}$ is the true data flow.

\paragraph{Low-Rank Adaptation (LoRA)}
LoRA fine-tunes a pretrained model by introducing low-rank updates to its weight matrices:
\begin{equation}
\mathbf{W}' = \mathbf{W} + \Delta \mathbf{W}, \quad \Delta \mathbf{W} = \mathbf{A} \mathbf{B},
\end{equation}
where $\mathbf{W}$ is the original weight matrix, $\mathbf{A} \in \mathbb{R}^{m \times r}$ and $\mathbf{B} \in \mathbb{R}^{r \times n}$ are low-rank matrices with rank $r \ll min(m,n)$. 

LoRA’s parameter efficiency and adaptability make it an ideal choice for disentangling identity and motion in video data.

\subsection{Stage 1: Identity Basis Learning}





In the first stage, we extract static identity features from an unordered set of frames as images in the input video. This stage creates a time-independent identity representation, forming the foundation for subsequent motion encoding. By decomposing and separating identity from motion, it enables the independent editing of appearance and motion during inference. The LoRA weight modification for this stage is defined as:

\begin{equation}
\mathbf{W}' = \mathbf{W} + \mathbf{A}_1 \mathbf{B}_1,
\end{equation}
where $\mathbf{A}_1 \in \mathbb{R}^{m \times r}$ and $\mathbf{B}_1 \in \mathbb{R}^{r \times n}$ represent the low-rank parameters capturing the identity. Static text tokens $\mathbf{T}_{\text{static}}$ are used to describe the subject’s appearance (e.g., as an illustration in Fig.~\ref{fig:pipeline}, “a [v] person”). In practice, for efficient editing and composition; appearance, background and expression information is also included in the static prompts to make it detailed (See supplementary materials). The [v] token is initialized with \textit{zeros}. The resulting conditional velocity field is defined as:
\begin{equation}
\mathbf{v}_\theta (\mathbf{x}_t, t; \mathbf{T}_{\text{static}}).
\end{equation}

The identity-specific flow matching loss ensures accurate reconstruction of static features. The learned parameters \(\mathbf{A}_1\) and \(\mathbf{B}_1\) are obtained by solving the following optimization problem:
\begin{equation}
(\mathbf{A}_1, \mathbf{B}_1) = \arg \min_{\mathbf{A}_1, \mathbf{B}_1} \mathbb{E}_{\mathbf{x}, t} \left[ \left\| \mathbf{v}_\theta (\mathbf{x}_t, t; \mathbf{A}_1, \mathbf{B}_1, \mathbf{T}_{\text{static}}) - \frac{\partial \mathbf{x}_t}{\partial t} \right\|_2^2 \right].
\end{equation}

This stage creates a robust, low-dimensional basis for identity representation.

\begin{figure*}[t!]
% \vspace{-0.2cm}
        \centering
        \includegraphics[width=\linewidth]{images/compare.jpg}
        % \vspace{-0.7cm}
        \caption{\textbf{Comparison with baselines.} Comparison of our method with baseline approaches (NewMove~\cite{materzynska2024newmove}, DreamVideo~\cite{wei2023dreamvideo}, DB-LoRA~\cite{simo,ruiz2023dreambooth}, and DreamMix~\cite{molad2023dreamixvideodiffusionmodels}) on two editing scenarios: changing the background and shirt, and adding a glass. Our method demonstrates superior adherence to the prompt while preserving the subject identity, outperforming the baselines.}
        \label{fig:compare}
    \end{figure*}

\subsection{Stage 2: Motion Residual Encoding}

Building upon the static identity basis established in Stage 1, the second stage introduces an additional low-rank matrix $\mathbf{B}_2$, encoding motion as a residual deformation on top of the identity. This stage captures the temporal evolution of motion dynamics, enabling independent manipulation and composition of motion during inference. The weight modification for this stage is defined as:
\begin{equation}
\mathbf{W}' = \mathbf{W} + \mathbf{A}_1 \mathbf{B}_1 + \mathbf{A}_1 \mathbf{B}_2,
\end{equation}
where $\mathbf{A}_1$ and $\mathbf{B}_1$ remain \textbf{fixed} to preserve identity, and $\mathbf{B}_2 \in \mathbb{R}^{r \times n}$ encodes motion-specific deformations. Motion encoding uses a union of static and motion-specific text tokens:
\begin{equation}
\mathbf{T}_{\text{motion}} := \mathbf{T}_{\text{static}} \cup \mathbf{T}_{\text{dynamic}},
\end{equation}
where $\mathbf{T}_{\text{dynamic}}$ describes motion attributes (e.g., as illustrated in the Fig.~\ref{fig:pipeline}, "... in [u] motion"). In practice, we also augment the dynamic part with the course action e.g., "... dancing with legs up" and camera motion e.g., "... as the camera zooms in" (See supplementary materials). The predicted velocity field is conditioned on both text components:
\begin{equation}
\mathbf{v}_\theta (\mathbf{x}_t, t; \mathbf{T}_{\text{motion}}).
\end{equation}

The flow matching loss for motion encoding ensures that the motion is reconstructed as a deformation. The learned parameters \(\mathbf{B}_2\) is obtained by solving the following optimization problem:
\begin{equation}
\mathbf{B}_2 = \arg \min_{\mathbf{B}_2} \mathbb{E}_{\mathbf{x}, t} \left[ \left\| \mathbf{v}_\theta (\mathbf{x}_t, t; \mathbf{A}_1, \mathbf{B}_1, \mathbf{B}_2, \mathbf{T}_{\text{motion}}) - \frac{\partial \mathbf{x}_t}{\partial t} \right\|_2^2 \right].
\end{equation}

This stage ensures that motion is complementary to identity, enabling robust, adaptable representations for dynamic content.


\subsection{Regularization}
\label{sec:reg}

Regularization is integral to our framework, ensuring robust training, preventing overfitting, and maintaining efficient representations of identity and motion. We employ four distinct regularization strategies to achieve these goals:

\subsubsection{Prior Preservation~\cite{ruiz2023dreambooth,roich2022pivotal}}
To ensure fidelity to the pretrained base model, we regularize both training stages to reconstruct videos generated by the base model. We sample videos with text having two parts i.e. appearance and motion, for example, in case of humans, "A man in a blue t-shirt. He is walking in a park". These prompts generalize for non-human dynamic concepts as well.



\subsubsection{High Dropout Regularization for High-Rank LoRA}
Training on a single video introduces challenges of underfitting with low-rank LoRA and overfitting with high-rank LoRA. To address these issues, we adopt a high-rank LoRA configuration combined with selective dropout applied exclusively to the $\mathbf{B}$ matrix in the LoRA update. Dropout regularization is applied to $\mathbf{B}$ LoRAs in both stages as follows:
\begin{equation}
\mathbf{B}' = \mathbf{B} \odot \mathbf{M},
\end{equation}
where $\mathbf{M}$ is a binary mask with dropout probability $p$ (e.g., $p = 0.8$). This selective dropout ensures that $\mathbf{A}_h$ remains stable, providing a consistent basis for encoding appearance and motion. By introducing sparsity in the learned coefficients, this approach mitigates overfitting while encouraging exploration of diverse parameter combinations facilitating applications like dynamic concept composition (See Fig.~\ref{fig:teaser}).



\subsubsection{Context-Aware Regularization.}

To enhance robustness and generalization, we incorporate text token masking and self-conditioning as complementary regularization techniques.

\textbf{Text Token Masking:} Random tokens in the input text are masked during training, requiring the model to infer the missing information from the remaining context. This prevents overfitting to specific token patterns and improves adaptability to diverse or incomplete prompts which we leverage for editing and re-composition.

\textbf{Self-Conditioning:} Inspired by~\cite{chen2023fit}, intermediate model outputs are reintroduced as inputs during subsequent steps, enabling iterative refinement. This feedback loop improves temporal consistency, ensuring stable identity and motion across frames.

