\section{Experiment Settings}








\begin{figure}[t!]
        \centering
         % \vspace{-0.6cm}
        \includegraphics[width=\linewidth]{images/ablation.jpg}
         % \vspace{-0.6cm}
        \caption{\textbf{Ablation.} Ablation of design choices on the editing task of adding a different shirt and background. Low-rank LoRA (LoRA-1) results in underfitting, failing to capture sufficient detail, while high-rank LoRA (LoRA-8) overfits, compromising adaptability. Our two-stage approach with added regularization achieves a balanced trade-off, preserving both fidelity and editability.}
        % \vspace{-0.6cm}
        \label{fig:ablation}
    \end{figure}






\subsection{Evaluation Dataset} We evaluate our framework on a curated dataset of human-centric videos, as identity preservation poses a significant challenge in editing and composition. Unlike general objects, humans are highly sensitive to subtle inconsistencies in appearance, motion, and expressions, making deviations immediately perceptible. The dataset includes \textit{five} distinct identities performing actions such as dancing and walking, as well as scenario with two identities interacting in the same video. This setup tests our frameworkâ€™s ability to preserve identity, maintain motion coherence, and achieve compositional fidelity compared to existing baselines. We evaluate our method using this dataset on the local and global editing tasks.

\subsection{Baselines.} We evaluate our method against several baselines, including state-of-the-art UNet-based approaches, architecture-agnostic methods, and LoRA-based variations as part of an ablation study. Among UNet-based models, we compare against DreamVideo~\cite{wei2023dreamvideo} and NewMove~\cite{materzynska2024newmove} which are state-of-the-art frameworks for video personalization. Additionally, code for methods like Customize-a-Video~\cite{ren2024customize} is unavailable, restricting direct comparison. We adapt UNet-based DreamMix~\cite{molad2023dreamixvideodiffusionmodels} (Code not available) to the DiT architecture, enabling mixed image-video training to benchmark its performance. To ensure a broader evaluation, we include architecture-agnostic methods like DreamBooth  LoRA~\cite{ruiz2023dreambooth, simo} and Textual Inversion~\cite{textual_inversion}. In our ablation study, we compare several LoRA setups to highlight the efficacy of our approach.











\subsection{Evaluation Metrics}

We employ the following metrics to quantitatively assess the quality of the generated videos:

 \textbf{Semantic Alignment.} We utilize \textit{CLIP-Text Similarity~\cite{CLIP} } \textbf{(C-T)} to measure the alignment between the generated video and the input text prompt. This metric computes the cosine similarity between the text embedding and the aggregated embeddings of all video frames, providing a global assessment of semantic consistency.
 

 
\textbf{Identity Preservation.} Maintaining identity consistency is crucial, especially in videos featuring human subjects. We utilize \textit{ArcFace Identity Similarity} (\textbf{ID})~\cite{ArcFace} to measure how well the identity of a person is preserved between the generated and reference videos. 

\textbf{Reconstruction Fidelity.} To quantify pixel-level fidelity, we compute the \textit{Mean Squared Error} (\textbf{MSE}) between corresponding frames in the generated and reference videos.

\textbf{Temporal Coherence.} Ensuring smooth transitions and motion consistency across frames is critical. For Temporal Coherence (\textbf{TC}), we compute  as CLIP image embeddings on all generated frames and report the average cosine similarity between all pairs of consecutive frames.


















\section{Experiment Results}

\subsection{Quantitative Evaluation}

In order to show the effectiveness of our approach, we first ablate with the baselines and show that our two stage approach is essential for the model to integrate the dynamic concepts into the prior.  Table~\ref{tab:ablation_results} and Fig.~\ref{fig:ablation} illustrates the results of our ablation study. They are evaluated on two editing tasks i.e. \textit{"changing the shirt and background"} and \textit{"holding a glass"}.  When using a \textit{low-rank LoRA}, we observe a significant loss in identity preservation due to underfitting, as the rank is insufficient to model the complexity of dynamic concepts. Conversely, a \textit{high-rank LoRA} overfits, resulting in diminished adaptability to new prompts. In contrast, our \textit{two-stage approach} strikes a balance by using the same rank to separately train an Identity Basis and Motion Residual. This enables better adaptation to novel prompts while preserving both appearance and motion. Finally, with the added \textit{dropout regularization} discussed in Sec~\ref{sec:reg}, our framework achieves seamless integration of local edits (e.g., clothing changes) and global edits (e.g., background replacement) while maintaining motion fidelity, as shown in the supplementary videos.

To evaluate the effectiveness of our method with state-of-the-art approaches, we compare the results of various baseline methods in Fig.~\ref{fig:compare}. Each method is provided with the same editing prompt, and we assess their ability to preserve both the identity and adherence to the textual description. As shown in the figure, our framework achieves high fidelity in identity preservation and text adherence, significantly outperforming other methods. For motion preservation and coherence, we provide additional results in the supplementary videos, highlighting the seamless integration of dynamic motion with edits. Table~\ref{tab:compare} shows the quantitative analysis of the results, where our two-stage \textit{set-and-sequence} results in a better trade-off compared to other methods.
% \vspace{-0.25cm}
\subsection{Qualitative Results}

Our framework demonstrates significant advancements in both editing and composition of dynamic concepts, setting a new benchmark in personalized video generation. In this section, we explore two primary applications: \textit{editing} and \textit{composition}.

\subsubsection{Editing.} Our framework excels in both \textit{local} and \textit{global editing}, as demonstrated in Fig.~\ref{fig:editing}. The core objective is to capture intricate expressions and mannerisms while adapting dynamic concepts to new scenarios. For example, in Fig.~\ref{fig:editing}, our method models complex athletic movements that current video generation models fail to replicate. These results are further demonstrated in the supplementary videos, where the coherency of motion and appearance across edits is observed. Moreover, our framework supports a wide range of edits, such as changing expressions, age, and camera angles, or seamlessly integrating new objects and backgrounds into the scene. This level of control is achieved without compromising motion fidelity or identity preservation. For instance, by adjusting the weights of the LoRAs or modifying associated text prompts, our framework can produce creative outputs such as Pixar-style (See Fig.~\ref{fig:pixar}) characterizations or adapt the same dynamic concept to a completely different context. The ability to selectively adapt parts of a video, as shown in the supplementary videos, further emphasizes the flexibility of our approach.

\subsubsection{Composition.} One of the most significant contributions of our framework is the ability to compose \textit{dynamic concepts} in novel and diverse settings. Leveraging the shared spatio-temporal weight space and our regularization techniques, we use single examples of multiple dynamic concepts and jointly train them using a unified identity basis and associated motion residual module. Here, after the first stage, identity basis jointly represents multiple concepts and in the second stage, the motion deformations for each dynamic concept is learned jointly. For instance, we demonstrate the composition of multiple entities, such as intricate movements, ocean waves and a bonfire, within the same scene. Fig.~\ref{fig:compose} illustrates how our method ensures coherence across these entities while maintaining their distinct characteristics. Additionally, we address challenges such as \textit{identity leakage} (see Fig.~\ref{fig:stitch}), which arises when semantically similar concepts are combined (humans in our case). To mitigate this, we employ a simple yet effective strategy of additionally training on stitched videos as a regularization. These videos are trained less frequently and although not necessary, removing backgrounds in the such stitched videos helps further in composing results involving two or more humans. More examples are provided in the supplementary videos.








\begin{table}[h]
\centering
% \vspace{-0.2cm}
\caption{\textbf{Ablation of Baselines.} Table evaluating Mean Square Error (MSE), Identity Preservation (ID), CLIP-T (C-T), and Temporal Coherency (TC) on the editing task. Our method demonstrates better reconstruction-edibility trade-off.}
\label{tab:ablation_results}
\begin{tabular}{lcccccc}
\toprule
\textbf{Method} & \textbf{MSE $\downarrow$} & \textbf{ID $\uparrow$} & \textbf{C-T $\uparrow$}  & \textbf{TC $\uparrow$}\\
\midrule
LoRA-1 & 0.0432  & 0.622  &0.226 & \textbf{0.9974}\\
LoRA-8  & \underline{0.0223}  & \textbf{0.703} & 0.224  & 0.9969 \\
+ Two-Stage & 0.0461  & 0.629 & \textbf{0.250} &  0.9971 \\
+ Reg       & \textbf{0.0221} & \underline{0.680}  & \underline{0.239}  & \underline{0.9972} \\

\bottomrule
\end{tabular}
\end{table}

\begin{table}[h]
\centering
% \vspace{-0.3cm}
\caption{\textbf{Editing Task Evaluation.} Table evaluating Mean Square Error (MSE), Identity Preservation (ID), CLIP-T (C-T), and Temporal Coherency (TC) on the editing task. Our method achieves a superior reconstruction-editability trade-off compared to the competing approaches.}
\label{tab:compare}
\begin{tabular}{lcccccc}
\toprule
\textbf{Method} & \textbf{MSE $\downarrow$} & \textbf{ID $\uparrow$} & \textbf{C-T $\uparrow$}  & \textbf{TC $\uparrow$}\\
\midrule
Tex-Inv & 0.0714  & 0.145 & 0.201  & 0.9927 \\
DB-LoRA  & \underline{0.0223}  & \textbf{0.703} & 0.224   & \underline{0.9969} \\
NewMove & 0.2223 & 0.270 & 0.204		& 0.9914 \\
DreamVideo       &0.2021  & 0.118 & 0.218	 	 & 0.9657 \\
DreamMix         & 0.0429  & 0.579  & \underline{0.226}  & 0.9965 \\
Ours              & \textbf{0.0221}   &  \underline{0.680} & \textbf{0.239}  & \textbf{0.9972}  \\
\bottomrule
\end{tabular}
\end{table}


\subsection{User Study}

To evaluate the quality of identity preservation, motion fidelity, and adherence to prompts on the editing task, we conducted a user study with 10 participants. We omit UNet based methods due to the overall lower quality (See Supplementary Videos). Participants were presented with pairs of videos generated by different methods and were asked to separately select the video that performed better in terms of identity preservation, motion fidelity, and adherence to the prompt. The results of the user study, summarized in Table~\ref{tab:user_study_results}, demonstrate that our method consistently outperforms competing approaches by achieving a better tradeoff on the editing task.

\begin{table}[h]
\centering
\caption{\textbf{User Study.} User study results comparing methods on Identity Preservation (ID), Motion Preservation (MP), Adherence to Prompt (AP), and Overall Preference of the edits (OP). Preference is computed in percentages.}
\label{tab:user_study_results}
\begin{tabular}{lcccc}
\toprule
\textbf{Method} & \textbf{IP} & \textbf{MP} & \textbf{AP} & \textbf{OP} \\
\midrule
Ours \textit{vs} DreamMix     & 87\% & 88\% & 98\% & 100\% \\
Ours \textit{vs} LoRA-1      & 99\% & 95\% & 94\% & 100\% \\
Ours \textit{vs} LoRA-8 (DB-LoRA)     & 78\% & 75\% & 98\% & 98\% \\
Ours \textit{vs} Two-Stage    & 86\% & 97\% & 76\% & 90\% \\
\bottomrule
\end{tabular}
\end{table}


\begin{figure}[t!]
        \centering
        \includegraphics[width=\linewidth]{images/stitch.jpg}
        \caption{\textbf{Stitched Example.} To address identity leakage when generating multiple identities and motions from multiple videos (Second Column), we augment training with stitched examples by combining videos side by side to generate new compositions with preserved identities (Third Column).}
        % \vspace{-0.3cm}
        \label{fig:stitch}
    \end{figure}
