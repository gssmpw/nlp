% \vspace{-0.3cm}

\section{Introduction}


\input{figures/method_overview}

    
The advent of generative models has revolutionized content creation, enabling the synthesis and manipulation of high-quality visual media with remarkable fidelity. Recent advances in text-to-image~\cite{saharia2022photorealistic,ramesh2022hierarchical,rombach2022high} and text-to-video~\cite{bar2024lumiere} models have further expanded these capabilities, opening up unprecedented opportunities for creative expression and personalization.

Personalization has been a well-established area of research in the image domain, allowing models to learn user-specific concepts that can be customized, edited, and composed into diverse contexts~\cite{ruiz2023dreambooth,gal2022image}. However, while video models have shown improvement in quality and capability, the task of personalizing these models remains an open problem. Unlike images, videos introduce an additional temporal dimension, making personalization significantly more challenging. In particular, video concepts are inherently dynamic, encompassing both appearance and motion, which must be learned and represented cohesively.

In this work, we propose a novel approach to personalizing generative text-to-video models, focusing on the idea of \emph{dynamic concepts} -- objects or subjects characterized by their entangled appearance and motion. 
Describing dynamic concepts through text is challenging, hence, embedding them into the output domain of video model and representing them as tokens,  facilitates a wide range of editing and compositional tasks.


Our work is built upon the state-of-the-art Diffusion Transformers (DiTs) architecture for video generation~\cite{menapace2024snap,sora}, which processes spatial and temporal tokens simultaneously. Unlike video generators using factorized spatial and temporal modeling (UNet-based)~\cite{blattmann2023stable} suffering from rigid artifacts, this joint spatio-temporal modeling is necessary for high-quality video generation~\cite{sora}. While DiTs achieve superior quality, they lack the innate inductive bias to disentangle spatial and temporal features. This absence of built-in separation poses a challenge for embedding dynamic concepts effectively in the model’s weight space.

Furthermore, directly fine-tuning low-rank adapters (LoRAs) on a single video often fails to capture both appearance and motion, resulting in a non-reusable representation that fails to generalize across diverse contexts or support meaningful compositionality of dynamic concepts. To address these limitations, we introduce a novel framework called \emph{Set-and-Sequence}, designed to impose a spatial-temporal structure in the weight space, enabling the representation of dynamic concepts in the weights space.

The proposed Set-and-Sequence framework operates in two stages:
(i) Identity Basis: We train LoRAs on a static \emph{unordered set} of frames extracted from the video, focusing solely on the appearance of the dynamic concept to achieve high fidelity without temporal distractions.
(ii) Motion Residuals: The Basis of the Identity LoRAs is frozen and the coefficient part is augmented with new coefficients trained on the \emph{temporal sequence} of full video clip, allowing the model to capture the motion dynamics of the concept.

This two-stage approach unlocks transformative capabilities in video generation. For the first time, we demonstrate seamless scene composition and adaptation with preserved motion and appearance. Tasks such as blending disparate video components—e.g., combining the fluid motion of ocean waves with the flickering dynamics of a bonfire—are achieved with unprecedented fidelity as shown in Fig.~\ref{fig:teaser} and in the supplementary video. Moreover, our framework enables intuitive editing of camera motion, refining expressions, and introducing localized changes, all driven by text prompts. These advancements represent a significant leap in compositionality, scalability, and adaptability, setting a new benchmark for personalized generative video models.