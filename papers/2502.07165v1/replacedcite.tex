\section{Related work}
\subsubsection{Demonstration and label relationship}  
Supervised ML models rely heavily on drawing mappings between representations of training examples and their label information to make predictions on unseen examples. Surprisingly, early research on ICL shows that ground truth in demonstration-label mapping is not as important, as showing demonstrations with random labels only leads to minimal performance drops on a range of classification tasks ____. However, later research points out the limitations of this study and arrives at a different conclusion: the correct correspondence between examples and labels is essential to ensure ICL performance ____. The previous biased conclusion could be attributed to the use of binary (accuracy) instead of probabilistic metrics, relatively weaker LLMs that are mostly under 20B parameters, and focus on only one few-shot setting (16 demos). Thus, although LLMs predominantly rely on knowledge acquired during pre-training to perform downstream tasks, they indeed can learn new tasks from in-context information, which motivates this work to find an alternative approach to providing more effective context information for LLM ICL than the commonly used demonstration-based approach. In our experiments, we also conduct ablation studies to explore the importance of label information on the quality of principles generated.
\subsubsection{Number of demonstrations}
Supervised ML algorithms are data-hungry and require a substantial amount of labeled training data to ensure model performance. Under ICL few-shot settings, previous work shows that adding more than one demonstration might not be necessary due to only marginal performance improvements ____. As Chen suggests, this indicates that the use of demonstrations is inefficient and the information provided by randomly selected demonstrations is most likely redundant. In some cases, multiple demonstrations can even hurt performance due to misguidance or negative interference among them ____. This leads to our research question: under the same input length constraint, can we design more concise but knowledge-intensive contexts as alternatives to few-shot demonstrations to better guide LLMs in performing downstream classification tasks? We also conduct ablation studies to explore the importance of the number of demonstrations on the quality of principles generated.

\subsubsection{Single-Agent vs. Multi-Agent LLM Framework}
Text classification, as one of the most fundamental NLP tasks, appears to be straightforward in the sense that LLMs only need to output one or more class labels from a predefined label space. However, it can actually be quite complicated and even more challenging due to the implicit nature of the reasoning process in comparison to other tasks. Most research on LLM ICL attempts to enhance model performance either by decomposing complex tasks into multiple steps or by providing LLMs with relevant domain- and task-specific data as additional context, such as the Retrieval Augmented Generation (RAG) approach. For instance, Chain-of-Thought (CoT) prompting first prompts the LLM to break problem-solving into multiple steps and then derives the final answer by following a step-by-step thought process ____. Focusing on QA questions, stepback prompting ____ runs inference on the same LLM twice by first asking LLMs to provide abstract principles or concepts to help resolve the original question before answering it. To improve LLMs' performance on text classification, for each data point, Clue And Reasoning Prompting (CARP) ____ includes multiple steps in a single prompt by asking the same LLM to first find superficial clues (e.g., keywords, tones, semantic relations, references, etc.) based on which final decisions are made after reasoning steps. CARP also leverages knowledge acquired through supervised fine-tuning on labeled datasets to search for more effective demonstrations for ICL. 

Recently, the multi-agent framework has gained popularity and has been shown to greatly improve LLMs' performance on complicated tasks such as long-context QA, multi-hop QA, math, and reasoning ____. For instance, the multi-agent debate framework can improve LLMs' reasoning capability, factuality, and inter-consistency in mathematical and multiple-choice commonsense reasoning tasks, as well as output quality in open-ended generation tasks, in comparison to their single-agent counterparts ____. In our multi-agent implementation of the principle-based approach, we try competitive and collaborative paradigms and evaluate their effectiveness.

Performance improvements provided by single- or multi-agent solutions mentioned above, using either self-ensemble (multiple inferences on the same LLM agent) or heterogeneous ensemble (multiple inferences on different LLM agents) approaches, usually come at significantly increased inference costs due to multiple LLM inferences and/or communication costs across different agents. Our research question is: can we achieve the same performance improvement without significantly increasing inference costs for text classification? Unlike other tasks such as long-context QA or text generation tasks, the label space for most text classification tasks is finite and relatively limited. Thus, the search space for an optimal principle should also be bounded. Accordingly, we propose to implement an effective and efficient multi-agent LLM framework to auto-generate a single all-inclusive SOP for each task and reuse it for inference on all data points. We believe that, in addition to improving performance, the shared principle can also help ensure consistency in classification predictions.