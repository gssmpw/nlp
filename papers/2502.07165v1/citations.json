[
  {
    "index": 0,
    "papers": [
      {
        "key": "min2022rethinking",
        "author": "Min, Sewon and Lyu, Xinxi and Holtzman, Ari and Artetxe, Mikel and Lewis, Mike and Hajishirzi, Hannaneh and Zettlemoyer, Luke",
        "title": "Rethinking the role of demonstrations: What makes in-context learning work?"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "kossen2023context",
        "author": "Kossen, Jannik and Rainforth, Tom and Gal, Yarin",
        "title": "In-context learning in large language models learns label relationships but is not conventional learning"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "chen2023many",
        "author": "Chen, Jiuhai and Chen, Lichang and Zhu, Chen and Zhou, Tianyi",
        "title": "How Many Demonstrations Do You Need for In-context Learning?"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "chen2023many",
        "author": "Chen, Jiuhai and Chen, Lichang and Zhu, Chen and Zhou, Tianyi",
        "title": "How Many Demonstrations Do You Need for In-context Learning?"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "wei2022chain",
        "author": "Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others",
        "title": "Chain-of-thought prompting elicits reasoning in large language models"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "zheng2023take",
        "author": "Zheng, Huaixiu Steven and Mishra, Swaroop and Chen, Xinyun and Cheng, Heng-Tze and Chi, Ed H and Le, Quoc V and Zhou, Denny",
        "title": "Take a step back: Evoking reasoning via abstraction in large language models"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "sun2023text",
        "author": "Sun, Xiaofei and Li, Xiaoya and Li, Jiwei and Wu, Fei and Guo, Shangwei and Zhang, Tianwei and Wang, Guoyin",
        "title": "Text classification via large language models"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "shridhar2022distilling",
        "author": "Shridhar, Kumar and Stolfo, Alessandro and Sachan, Mrinmaya",
        "title": "Distilling reasoning capabilities into smaller language models"
      },
      {
        "key": "wang2022self",
        "author": "Wang, Xuezhi and Wei, Jason and Schuurmans, Dale and Le, Quoc and Chi, Ed and Narang, Sharan and Chowdhery, Aakanksha and Zhou, Denny",
        "title": "Self-consistency improves chain of thought reasoning in language models"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "du2023improving",
        "author": "Du, Yilun and Li, Shuang and Torralba, Antonio and Tenenbaum, Joshua B and Mordatch, Igor",
        "title": "Improving factuality and reasoning in language models through multiagent debate"
      },
      {
        "key": "xiong2023examining",
        "author": "Xiong, Kai and Ding, Xiao and Cao, Yixin and Liu, Ting and Qin, Bing",
        "title": "Examining inter-consistency of large language models collaboration: An in-depth analysis via debate"
      },
      {
        "key": "chan2023chateval",
        "author": "Chan, Chi-Min and Chen, Weize and Su, Yusheng and Yu, Jianxuan and Xue, Wei and Zhang, Shanghang and Fu, Jie and Liu, Zhiyuan",
        "title": "Chateval: Towards better llm-based evaluators through multi-agent debate"
      }
    ]
  }
]