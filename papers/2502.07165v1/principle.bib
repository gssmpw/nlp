%% This BibTeX bibliography file was created using BibDesk.
%% https://bibdesk.sourceforge.io/

%% Created for Wei, Peipei at 2024-05-01 13:10:33 -0700 


%% Saved with string encoding Unicode (UTF-8) 

@article{chen2023distinguish,
  title={Distinguish before answer: Generating contrastive explanation as knowledge for commonsense question answering},
  author={Chen, Qianglong and Xu, Guohai and Yan, Ming and Zhang, Ji and Huang, Fei and Si, Luo and Zhang, Yin},
  journal={arXiv preprint arXiv:2305.08135},
  year={2023}
}
@inproceedings{chen2023many,
  title={How Many Demonstrations Do You Need for In-context Learning?},
  author={Chen, Jiuhai and Chen, Lichang and Zhu, Chen and Zhou, Tianyi},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2023},
  pages={11149--11159},
  year={2023}
}

@inproceedings{jang2023can,
  title={Can large language models truly understand prompts? a case study with negated prompts},
  author={Jang, Joel and Ye, Seonghyeon and Seo, Minjoon},
  booktitle={Transfer Learning for Natural Language Processing Workshop},
  pages={52--62},
  year={2023},
  organization={PMLR}
}
@article{webson2021prompt,
  title={Do prompt-based models really understand the meaning of their prompts?},
  author={Webson, Albert and Pavlick, Ellie},
  journal={arXiv preprint arXiv:2109.01247},
  year={2021}
}
@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}
@article{paranjape2021prompting,
  title={Prompting contrastive explanations for commonsense reasoning tasks},
  author={Paranjape, Bhargavi and Michael, Julian and Ghazvininejad, Marjan and Zettlemoyer, Luke and Hajishirzi, Hannaneh},
  journal={arXiv preprint arXiv:2106.06823},
  year={2021}
}
@article{kossen2023context,
  title={In-context learning in large language models learns label relationships but is not conventional learning},
  author={Kossen, Jannik and Rainforth, Tom and Gal, Yarin},
  journal={arXiv preprint arXiv:2307.12375},
  year={2023}
}
@article{yoo2022ground,
  title={Ground-truth labels matter: A deeper look into input-label demonstrations},
  author={Yoo, Kang Min and Kim, Junyeob and Kim, Hyuhng Joon and Cho, Hyunsoo and Jo, Hwiyeol and Lee, Sang-Woo and Lee, Sang-goo and Kim, Taeuk},
  journal={arXiv preprint arXiv:2205.12685},
  year={2022}
}
@article{liu2023pre,
  title={Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing},
  author={Liu, Pengfei and Yuan, Weizhe and Fu, Jinlan and Jiang, Zhengbao and Hayashi, Hiroaki and Neubig, Graham},
  journal={ACM Computing Surveys},
  volume={55},
  number={9},
  pages={1--35},
  year={2023},
  publisher={ACM New York, NY}
}
@article{wu2022self,
  title={Self-adaptive in-context learning: An information compression perspective for in-context example selection and ordering},
  author={Wu, Zhiyong and Wang, Yaoxiang and Ye, Jiacheng and Kong, Lingpeng},
  journal={arXiv preprint arXiv:2212.10375},
  year={2022}
}
@article{dong2022survey,
  title={A survey on in-context learning},
  author={Dong, Qingxiu and Li, Lei and Dai, Damai and Zheng, Ce and Wu, Zhiyong and Chang, Baobao and Sun, Xu and Xu, Jingjing and Sui, Zhifang},
  journal={arXiv preprint arXiv:2301.00234},
  year={2022}
}
@article{min2022rethinking,
  title={Rethinking the role of demonstrations: What makes in-context learning work?},
  author={Min, Sewon and Lyu, Xinxi and Holtzman, Ari and Artetxe, Mikel and Lewis, Mike and Hajishirzi, Hannaneh and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:2202.12837},
  year={2022}
}
@inproceedings{jang2023can,
  title={Can large language models truly understand prompts? a case study with negated prompts},
  author={Jang, Joel and Ye, Seonghyeon and Seo, Minjoon},
  booktitle={Transfer Learning for Natural Language Processing Workshop},
  pages={52--62},
  year={2023},
  organization={PMLR}
}
@article{razeghi2022impact,
  title={Impact of pretraining term frequencies on few-shot reasoning},
  author={Razeghi, Yasaman and Logan IV, Robert L and Gardner, Matt and Singh, Sameer},
  journal={arXiv preprint arXiv:2202.07206},
  year={2022}
}
@article{sun2023text,
  title={Text classification via large language models},
  author={Sun, Xiaofei and Li, Xiaoya and Li, Jiwei and Wu, Fei and Guo, Shangwei and Zhang, Tianwei and Wang, Guoyin},
  journal={arXiv preprint arXiv:2305.08377},
  year={2023}
}
@article{su2022selective,
  title={Selective annotation makes language models better few-shot learners},
  author={Su, Hongjin and Kasai, Jungo and Wu, Chen Henry and Shi, Weijia and Wang, Tianlu and Xin, Jiayi and Zhang, Rui and Ostendorf, Mari and Zettlemoyer, Luke and Smith, Noah A and others},
  journal={arXiv preprint arXiv:2209.01975},
  year={2022}
}
@article{barbieri2020tweeteval,
  title={TweetEval: Unified benchmark and comparative evaluation for tweet classification},
  author={Barbieri, Francesco and Camacho-Collados, Jose and Neves, Leonardo and Espinosa-Anke, Luis},
  journal={arXiv preprint arXiv:2010.12421},
  year={2020}
}
@article{zheng2023take,
  title={Take a step back: Evoking reasoning via abstraction in large language models},
  author={Zheng, Huaixiu Steven and Mishra, Swaroop and Chen, Xinyun and Cheng, Heng-Tze and Chi, Ed H and Le, Quoc V and Zhou, Denny},
  journal={arXiv preprint arXiv:2310.06117},
  year={2023}
}
@inproceedings{de2023semantic,
  title={Semantic matching for text classification with complex class descriptions},
  author={De Silva, Brian and Huang, Kuan-Wen and Lee, Gwang and Hovsepian, Karen and Xu, Yan and Shen, Mingwei},
  booktitle={Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  pages={7654--7680},
  year={2023}
}
@article{qin2023large,
  title={Large language models are effective text rankers with pairwise ranking prompting},
  author={Qin, Zhen and Jagerman, Rolf and Hui, Kai and Zhuang, Honglei and Wu, Junru and Shen, Jiaming and Liu, Tianqi and Liu, Jialu and Metzler, Donald and Wang, Xuanhui and others},
  journal={arXiv preprint arXiv:2306.17563},
  year={2023}
}
@article{malo2014good,
  title={Good debt or bad debt: Detecting semantic orientations in economic texts},
  author={Malo, Pekka and Sinha, Ankur and Korhonen, Pekka and Wallenius, Jyrki and Takala, Pyry},
  journal={Journal of the Association for Information Science and Technology},
  volume={65},
  number={4},
  pages={782--796},
  year={2014},
  publisher={Wiley Online Library}
}
@article{sailunaz2019emotion,
  title={Emotion and sentiment analysis from Twitter text},
  author={Sailunaz, Kashfia and Alhajj, Reda},
  journal={Journal of computational science},
  volume={36},
  pages={101003},
  year={2019},
  publisher={Elsevier}
}
@article{chung2024scaling,
  title={Scaling instruction-finetuned language models},
  author={Chung, Hyung Won and Hou, Le and Longpre, Shayne and Zoph, Barret and Tay, Yi and Fedus, William and Li, Yunxuan and Wang, Xuezhi and Dehghani, Mostafa and Brahma, Siddhartha and others},
  journal={Journal of Machine Learning Research},
  volume={25},
  number={70},
  pages={1--53},
  year={2024}
}
@article{tay2022ul2,
  title={Ul2: Unifying language learning paradigms},
  author={Tay, Yi and Dehghani, Mostafa and Tran, Vinh Q and Garcia, Xavier and Wei, Jason and Wang, Xuezhi and Chung, Hyung Won and Shakeri, Siamak and Bahri, Dara and Schuster, Tal and others},
  journal={arXiv preprint arXiv:2205.05131},
  year={2022}
}
@article{kojima2022large,
  title={Large language models are zero-shot reasoners},
  author={Kojima, Takeshi and Gu, Shixiang Shane and Reid, Machel and Matsuo, Yutaka and Iwasawa, Yusuke},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={22199--22213},
  year={2022}
}
@article{ma2023zero,
  title={Zero-shot listwise document reranking with a large language model},
  author={Ma, Xueguang and Zhang, Xinyu and Pradeep, Ronak and Lin, Jimmy},
  journal={arXiv preprint arXiv:2305.02156},
  year={2023}
}
@article{sun2023chatgpt,
  title={Is chatgpt good at search? investigating large language models as re-ranking agent},
  author={Sun, Weiwei and Yan, Lingyong and Ma, Xinyu and Ren, Pengjie and Yin, Dawei and Ren, Zhaochun},
  journal={arXiv preprint arXiv:2304.09542},
  year={2023}
}

@article{wei2022chain,
  title={Chain-of-thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={24824--24837},
  year={2022}
}

@article{chen2024grimoire,
  title={Grimoire is All You Need for Enhancing Large Language Models},
  author={Chen, Ding and Song, Shichao and Yu, Qingchen and Li, Zhiyu and Wang, Wenjin and Xiong, Feiyu and Tang, Bo},
  journal={arXiv preprint arXiv:2401.03385},
  year={2024}
}

@article{mann2020language,
  title={Language models are few-shot learners},
  author={Mann, Ben and Ryder, N and Subbiah, M and Kaplan, J and Dhariwal, P and Neelakantan, A and Shyam, P and Sastry, G and Askell, A and Agarwal, S and others},
  journal={arXiv preprint arXiv:2005.14165},
  volume={1},
  year={2020}
}
@article{zhao2023survey,
  title={A survey of large language models},
  author={Zhao, Wayne Xin and Zhou, Kun and Li, Junyi and Tang, Tianyi and Wang, Xiaolei and Hou, Yupeng and Min, Yingqian and Zhang, Beichen and Zhang, Junjie and Dong, Zican and others},
  journal={arXiv preprint arXiv:2303.18223},
  year={2023}
}
@article{jiang2023mistral,
  title={Mistral 7B},
  author={Jiang, Albert Q and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lucile and others},
  journal={arXiv preprint arXiv:2310.06825},
  year={2023}
}
@article{shridhar2022distilling,
  title={Distilling reasoning capabilities into smaller language models},
  author={Shridhar, Kumar and Stolfo, Alessandro and Sachan, Mrinmaya},
  journal={arXiv preprint arXiv:2212.00193},
  year={2022}
}
@article{wang2022self,
  title={Self-consistency improves chain of thought reasoning in language models},
  author={Wang, Xuezhi and Wei, Jason and Schuurmans, Dale and Le, Quoc and Chi, Ed and Narang, Sharan and Chowdhery, Aakanksha and Zhou, Denny},
  journal={arXiv preprint arXiv:2203.11171},
  year={2022}
}
@article{levy2022diverse,
  title={Diverse demonstrations improve in-context compositional generalization},
  author={Levy, Itay and Bogin, Ben and Berant, Jonathan},
  journal={arXiv preprint arXiv:2212.06800},
  year={2022}
}
@article{du2023improving,
  title={Improving factuality and reasoning in language models through multiagent debate},
  author={Du, Yilun and Li, Shuang and Torralba, Antonio and Tenenbaum, Joshua B and Mordatch, Igor},
  journal={arXiv preprint arXiv:2305.14325},
  year={2023}
}
@article{xiong2023examining,
  title={Examining inter-consistency of large language models collaboration: An in-depth analysis via debate},
  author={Xiong, Kai and Ding, Xiao and Cao, Yixin and Liu, Ting and Qin, Bing},
  journal={arXiv preprint arXiv:2305.11595},
  year={2023}
}
@article{chan2023chateval,
  title={Chateval: Towards better llm-based evaluators through multi-agent debate},
  author={Chan, Chi-Min and Chen, Weize and Su, Yusheng and Yu, Jianxuan and Xue, Wei and Zhang, Shanghang and Fu, Jie and Liu, Zhiyuan},
  journal={arXiv preprint arXiv:2308.07201},
  year={2023}
}