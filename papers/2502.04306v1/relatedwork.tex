\section{Related Work}
\subsection{Agentic Workflow Optimization}




\paragraph{Automated Optimizations for Prompt and Hyperparameter} 
 
Automated optimization methods emphasizing prompt optimization \citep{fernando2023promptbreeder, yuksekgonul2024textgrad, yang2023large, khattab2024dspy} or hyperparameter optimization \citep{saad2024archon} can enhance performance; however, they impose limitations on the workflow structure and often require manual modifications to accommodate new tasks, restricting their adaptability and scalability.

\paragraph{Automated Optimizations for Workflow Structure} Workflow optimization methods \citep{zhou2024symbolic, zhuge2023mindstorms, hu2024automated, zhang2024aflow, chen2023autoagents, li2024autoflow, liu2024dynamic, song2024adaptive, zhang2024g} focus on refining the structure of workflows, making them more robust for handling diverse tasks.
However, the inflexibility and limitations in workflow representation, such as the loss of conditional states within the graph structure, may restrict the search space and consequently hinder the ability to accommodate diverse and complex workflows.
To address this challenge, ADAS \citep{hu2024automated} and Aflow \citep{zhang2024aflow} adopt code as a representation for workflows.
However, the performance of ADAS is constrained by its accumulated irrelevant information and increased complexity in optimization, hindering agents' ability. Aflow employs a Monte Carlo Tree Search-based method to efficiently identify optimal workflows; however, its tendency toward premature convergence on workflow structures limits the exploration of the search space. Moreover, the discrete optimization method, which involves randomly selecting failed cases and feeding them back to the optimizer LLM to refine the workflow, imposes significant limitations on scalability.






\subsection{Learning from Preferences for Language Models}

\paragraph{PPO} Proximal Policy Optimization (PPO) \citep{schulman2017proximal} process preference feedback in two stages. First, a reward model \( R_{\phi} \) is trained on the preference dataset \( D_{R} \), where each entry \( (x, y_w, y_l) \) consists of a prompt \( x \), a preferred response \( y_w \), and a rejected response \( y_l \). The reward model is optimized by minimizing the following loss function, which is inspired by the Bradley-Terry (BT) model \citep{bradley1952rank} for pairwise ranking:
\begin{align}
\label{pporewardobj}
- \mathbb{E}_{(x, y_w, y_l) \sim D_{R}} \big[\log \sigma \big(R_{\phi}(x, y_w) - R_{\phi}(x, y_l)\big)\big].
\end{align}
Next, the policy model \( \pi_{\theta} \) is refined by maximizing the reward assigned to its generated responses, while maintaining a soft KL divergence constraint to prevent degeneration. The objective is expressed as:
\begin{align}
\label{ppopolicyobj}
\mathbb{E}_{x \sim D_{\pi}, y \sim \pi_{\theta}(y \mid x)} \big[R_{\phi}(x, y)\big] - \beta \mathbb{D}_{KL} (\pi_{\theta} || \pi_{ref}),
\end{align}
where \( \pi_{ref} \) represents the reference policy, and \( \beta \) is a hyperparameter controlling the KL penalty.




\paragraph{DPO} Direct Preference Optimization (DPO) \citep{rafailov2024direct} facilitates direct policy optimization using preference data, eliminating the need for explicit reward models or active policy sampling. This approach enhances both the efficiency and stability of the optimization process. From the closed-form solution of Equation~\ref{ppopolicyobj}, the implicit reward can be expressed as \( R_{\phi}(x, y) =  \beta \log \big(\pi_{\theta^{\star}}(y \mid x) / \pi_{ref}(y \mid x)\big) + \beta Z(x)\), where \( \pi_{\theta^{\star}} \) is the optimal policy and $Z(x)$ is a partition function. The policy model can then be directly optimized using the reward objective in Equation~\ref{pporewardobj}, resulting in the DPO loss:
\[
- \mathbb{E}_{(x, y_w, y_l) \sim D_{R}} \big[\log \sigma \big(r(x, y_w) - r(x, y_l)\big)\big],
\]
where $r(x, y) := \beta \log \big(\pi_{\theta^{\star}}(y \mid x) / \pi_{ref}(y \mid x)\big)$.

When the data format includes associated evaluation scores for each sample (as in our setting), rather than solely chosen and rejected pairs, we propose Score-DPO, which integrates these scores into the training process to enhance performance. This approach achieves improved performance over standard DPO while maintaining its efficiency and stability in our applications.



\begin{figure*}[tp]
    \centering
    \includegraphics[width=\textwidth]{figures/pipeline.png}
    \caption{\textbf{Pipeline of ScoreFlow.} First, for each problem in the dataset, multiple workflows are generated. Next, an executor is employed to execute these workflows for corresponding problems, resulting in evaluation scores. Based on these scores, preference data is collected. Subsequently, incorporating the score information, the Score-DPO algorithm is used to fine-tune the generator. This process is iterated until the maximum number of iterations is reached or convergence is achieved. }
    \label{pipline}
\vspace{-3mm}
\end{figure*}