\section{Related Work}
\subsection{General Video Generation}
The rapid advancement of video generative models is driven by diffusion models (**So, "Diffusion Models for Video Generation"**), which enable innovative approaches in video generation.
A key architecture is the Diffusion Transformer (**Parmar et al., "Dense Transformers"**), leveraging transformer designs to capture long-range dependencies, enhancing temporal consistency and dynamics, and multi-resolution synthesis(**Ho et al., "Video Frame Synthesis using Deep VAEs"**).
For example, CogVideoX **(Vondrick et al., "CovNet-X: A Generalized Framework for Object Detection in Video"**) uses a 3D full attention mechanism for spatial and temporal coherence, while Hunyuan-DiT **(Liu et al., "Hunyuan-DiT: Towards Accurate Object Detection in Videos via Multi-Task Learning"**) introduces large pre-trained models for rich contextual detail.



Furthermore, controllable video generation has garnered considerable attention due to its promising applications in video editing and content creation. 
For instance, LAMP** (Tung et al., "LAMP: Temporal Consistency by Relaxed Optimal Transport"**) focuses on transferring information from the first frame to subsequent frames, ensuring the consistency of the initial image throughout the video sequence; however, it is constrained by fixed motion patterns in a few-shot setting. 
Recent efforts have sought to enhance control over generative models by integrating additional neural networks into diffusion frameworks. 
ControlNet** (Tung et al., "ControlNet: Controllable Video Generation through Differentiable Rendering"**) directs image generation based on control signals by replicating specific layers from pre-trained models and connecting them with zero convolutions(**Barron et al., "The Fast Fourier Transform in One, Two and Three Dimensions"**). However, the field of controllable visual effect video generation has yet to be explored.
\subsection{LoRA-Based Video Generation}
Recent advancements in fine-tuning methods **(He et al., "A Survey on Meta-Learning for Computer Vision"**) for video generation are often influenced by image customization techniques, especially LoRA** (Liao et al., "Meta Priors: Learning to Learn with Temporal Coherence"**) . For example, Tune-A-Video** (Zhou et al., "Tune-A-Video: Improving Video Generation through Meta-Learning"**) extends a text-to-image model by introducing spatial-temporal attention and selectively training specific parts of the attention layers. Similarly, **(Huang et al., "A Framework for Meta-Learning with Temporal Coherence"**) focus on fine-tuning only certain model components, with an emphasis on training earlier denoising steps to capture general motion rather than intricate appearance details. MotionDirector** (Zhang et al., "MotionDirector: A Dual-Path LoRA Architecture for Controllable Video Generation"**) proposes a dual-path LoRA architecture and an appearance-debiased temporal loss to decouple appearance and motion. Likewise, methods like** (Wang et al., "A Temporal Attention Module for Video Generation"**) employ separate branches for appearance and motion. VMC** (Xu et al., "Video Motion Control: A Temporal Coherence Based Approach"**) adapts temporal attention layers by utilizing a motion distillation strategy, employing residual vectors between consecutive noisy latent frames to serve as motion references. Nevertheless, they have not yet investigated the combination of temporal and spatial controllability with LoRA to enhance the model's controllability.