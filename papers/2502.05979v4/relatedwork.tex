\section{Related Work}
\subsection{General Video Generation}
The rapid advancement of video generative models is driven by diffusion models (DMs) \cite{sohl2015deep, rombach2022highresolutionimagesynthesislatent, ho2020denoising, song2020score, nichol2021glide}, which enable innovative approaches in video generation.
A key architecture is the Diffusion Transformer \cite{peebles2023scalable}, leveraging transformer designs to capture long-range dependencies, enhancing temporal consistency and dynamics, and multi-resolution synthesis~\cite{yang2024cogvideox, ma2024latte, shao2024human4dit, videoworldsimulators2024, chen2023videocrafter1, kuaishou2024keling, genmo2024mochi}.
For example, CogVideoX \cite{yang2024cogvideox} uses a 3D full attention mechanism for spatial and temporal coherence, while Hunyuan-DiT \cite{li2024hunyuanditpowerfulmultiresolutiondiffusion} introduces large pre-trained models for rich contextual detail. 



Furthermore, controllable video generation has garnered considerable attention due to its promising applications in video editing and content creation. 
For instance, LAMP~\cite{wu2023lamp} focuses on transferring information from the first frame to subsequent frames, ensuring the consistency of the initial image throughout the video sequence; however, it is constrained by fixed motion patterns in a few-shot setting. 
Recent efforts have sought to enhance control over generative models by integrating additional neural networks into diffusion frameworks. 
ControlNet~\cite{zhang2023adding} directs image generation based on control signals by replicating specific layers from pre-trained models and connecting them with zero convolutions~\cite{wang2024disco}. However, the field of controllable visual effect video generation has yet to be explored.
\subsection{LoRA-Based Video Generation}
Recent advancements in fine-tuning methods~\cite{wu2024motionbooth,wu2024customcrafter,wang2024customvideo,guo2023animatediff,wang2024motionctrl,ouyang2024i2vedit} for video generation are often influenced by image customization techniques, especially LoRA~\cite{hu2021lora}. For example, Tune-A-Video~\cite{wu2023tune} extends a text-to-image model by introducing spatial-temporal attention and selectively training specific parts of the attention layers. Similarly, ~\cite{materzynska2023customizing} focus on fine-tuning only certain model components, with an emphasis on training earlier denoising steps to capture general motion rather than intricate appearance details. MotionDirector\cite{zhao2025motiondirector} proposes a dual-path LoRA architecture and an appearance-debiased temporal loss to decouple appearance and motion. Likewise, methods like~\cite{wei2024dreamvideo, zhang2023motioncrafter, ren2024customize} employ separate branches for appearance and motion. VMC~\cite{jeong2024vmc} adapts temporal attention layers by utilizing a motion distillation strategy, employing residual vectors between consecutive noisy latent frames to serve as motion references. Nevertheless, they have not yet investigated the combination of temporal and spatial controllability with LoRA to enhance the model's controllability.