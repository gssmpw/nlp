\documentclass[USenglish]{article}

\usepackage[small]{dgruyter}
% \usepackage{microtype}
% to fix it https://tex.stackexchange.com/questions/10706/pdftex-error-font-expansion-auto-expansion-is-only-possible-with-scalable
\usepackage{lmodern} % added by Ramon to fix compilation problems

\usepackage{pdflscape} % for landscape tables
\usepackage{etoolbox}
\usepackage[natbibapa]{apacite} % Citation support using apacitep.sty. Commands using natbib.sty MUST be deactivated first!

\newcommand{\loglikelihood}{{\cal L}}

\newtoggle{anonymous}
\togglefalse{anonymous}

\newtoggle{squib}
\togglefalse{squib}

\newtoggle{extra}
\togglefalse{extra}

\begin{document}

\iftoggle{anonymous}{
   \author{...}
   \runningauthor{...}
   \affil{...}
}
{
   \author*{Ramon Ferrer-i-Cancho}
   \runningauthor{Ferrer-i-Cancho}
   \affil{Quantitative, Mathematical and Computational Linguistics Research Group, Department of Computer Science, Universitat Politècnica de Catalunya, 08034 Barcelona, Catalonia (Spain)}
}


%  \articletype{...}

   \title{The exponential distribution of the orders of demonstrative, numeral, adjective and noun} 
   \runningtitle{Exponential distribution of word orders} % it must have 40 characters or less including spaces
%  \subtitle{...}
   \abstract{The frequency of the preferred order for a noun phrase formed by demonstrative, numeral, adjective and noun has received significant attention over the last two decades. We investigate the actual distribution of the preferred 24 possible orders. There is no consensus on whether it can be well-fitted by an exponential or a power law distribution. We find that an exponential distribution is a much better model. This finding and other circumstances where an exponential-like distribution is found challenge the view that power-law distributions, e.g., Zipf's law for word frequencies, are inevitable. We also investigate which of two exponential distributions gives a better fit: an exponential model where the 24 orders have non-zero probability or an exponential model where the number of orders that can have non-zero probability is variable. When parsimony and generalizability are prioritized, we find strong support for the exponential model where all 24 orders have non-zero probability. This finding suggests that there is no hard constraint on word order variation and then unattested orders merely result from undersampling, consistently with Cysouw's view. }
   \keywords{exponential distribution; noun phrase; power law distribution; word order}

%  \classification[PACS]{...}
%  \communicated{...}
%  \dedication{...}
%  \received{...}
%  \accepted{...}
%  \journalname{...}
%  \journalyear{...}
%  \journalvolume{..}
%  \journalissue{..}
%  \startpage{1}
%  \aop
%  \DOI{...}

\maketitle

\section{Introduction}

\label{sec:introduction}

% David Peter Medeiros personal website https://davidpmedeiros.com/publications/

The frequency of the preferred order of a noun phrase formed by demonstrative, numeral, adjective and noun has received substantial attention over the last 20 years \citep{Cinque2005a, Cysouw2010a, Dryer2018a, Medeiros2018a, Martin2020a}. 
Researchers have attempted to shed light on the actual variation of frequency among the 24 possible orders with some degree of precision \citep{Cinque2005a,Cysouw2010a,Dryer2018a}. 
% \citet{Dryer2018a} proposed 5 descriptive principles that correlate with the actual frequency of word orders.
Concerning the distribution of the rank $r$ of a preferred order (the most frequent order has rank 1, the second most preferred order has rank 2, and so on), shown in Figure \ref{fig:rank_distribution} top,
\citet{Cysouw2010a} proposed an exponential distribution while \citet{Martin2020a} 
assumed a power law distribution. $f(r)$, the frequency of rank $r$, would follow a power law distribution if $f(r)$ could be approximated by 
\begin{equation}
f(r) = c r^{-\alpha},
\label{eq:power_law_distribution_template}
\end{equation}
where $c$ and $ \alpha$ are some positive constants. $\alpha$ is the so-called exponent of the power law. 
$f(r)$ would follow an exponential distribution if it could be approximated by  
\begin{equation}
f(r) = c e^{-\beta r},
\label{eq:exponential_distribution}
\end{equation}
where $c$ and $\beta$ are some positive constants. \iftoggle{squib}{}
{\footnote{$\alpha, \beta \geq 0$ by the definition of $r$.}}

\begin{figure}[h]
% \includegraphics[width=\textwidth]{figures/word_order_distribution_2018}
\includegraphics[width=\textwidth]{figures/word_order_distribution_2006_2018}
\caption{\label{fig:rank_distribution}
$f(r)$, frequency of a word order of rank $r$ in three distinct scales: normal (top), linear-log (middle) and log-log (bottom). Left. Frequency is measured in languages according to \citet{Dryer2006a}.  
Right. Frequency is measured in languages, genera and adjusted number of languages according to \citet{Dryer2018a}.  }
\end{figure}

We will show that an exponential distribution is a much better model than a power law distribution and will discuss its implications. 
 
\section{Methods}

\subsection{Data}

\label{subsec:data}

\begin{table}[h]
\begin{center}
\begin{tabular}{llllllll}
dataset & unit & $F_0$ & $\left< r \right>$ & $r_{max}$ \\
\input tables/elementary_summary_table
\end{tabular}
\end{center}
\caption{\label{tbl:elementary_summary}
Summary of the elementary statistical properties of each dataset: the total frequency ($F_0$), the average frequency rank ($\left< r \right>$) and the maximum frequency rank in the sample ($r_{max}$). }
\end{table}

We borrow the frequency of the preferred order of demonstrative, numeral, adjective and noun from two datasets \citep{Dryer2006a, Dryer2018a}. 
\citet[Appendix]{Cysouw2010a} displays the frequency of each order in languages according to \citet{Dryer2006a},
In \citet{Dryer2018a}, the frequency of each order is available in languages, genera and adjusted number of languages. The later is intended to control for geographic proximity or genetic relatedness.  
Frequencies in languages or genera are integer numbers while the frequency in adjusted number of languages is non-integer.
 
For convenience, we define $F_x$ as  
\begin{equation*}
F_x = \sum_{r=1}^{r_{max}}f(r)r^x,
\end{equation*}
where $r_{max}$ is the maximum rank in the sample. Thus, $r_{max}$ is also the number of attested orders, i.e. the number of distinct orders observed in the sample.
$F_0$ is just the total frequency. $F_0$ is also the sample size. 
The average rank is 
\begin{equation*}
\left< r \right> = F_1/F_0.
\end{equation*}

Table \ref{tbl:elementary_summary} summarizes the elementary statistical properties of the datasets. Within the 2018 dataset, the sample size ($F_0$) reduces while the average rank increases as one moves from languages, to genera and then to adjusted number of languages.

\subsection{The models}

As there cannot be more than $N = 24$ orders, here we are interested in right-truncated models for $p(r)$, namely models that give $p(r) = 0$ for $r > N$.
Among these models, we are interested in models with early right-truncation, namely models that have a parameter $R \leq N$ such that $p(r) = 0$ when $r < 1$ or $r > R$ and $p(r) > 0$ for $1 \leq r \leq R$.
A power-law-like distribution (Equation \ref{eq:power_law_distribution_template}) can be modelled by the zeta distribution, namely 
\begin{equation}
p(r) = c r^{-\alpha},
\label{eq:power_law_distribution}
\end{equation}
where the normalization factor is $c = 1/\zeta(\alpha)$. $\zeta(\alpha)$ is the Riemann zeta function, i.e. 
\begin{equation}
\zeta(\alpha) = \sum_{r=1}^{\infty} r^{-\alpha}.
\end{equation}
A zeta distribution is not an adequate power-law model for our setting because that model predicts $p(r) > 0$ for any finite $r$ such that $r > N$. 
For this reason, we consider instead a right-truncated power law distribution with two parameters, $\alpha$ and $R$, such that the normalization factor becomes 
\begin{equation}
c = \frac{1}{H(\alpha,R)}.
\label{eq:normalization_factor_right_truncated_power_law}
\end{equation}
$H(R,\alpha)$ is the generalized harmonic number in power $\alpha$ % https://mathworld.wolfram.com/HarmonicNumber.html
i.e. 
\begin{equation*}
H(\alpha,R) = \sum_{r=1}^{R} r^{-\alpha}.
\end{equation*} 

An exponential distribution (Equation \ref{eq:exponential_distribution}) can be modelled by a geometric distribution. 
\begin{equation}
p(r) = c(1-q)^{r - 1}
\label{eq:geometric_distribution}
\end{equation}
where $c$ is a normalization factor and $q \in (0, 1)$ is a parameter. 
The untruncated geometric distribution is obtained with $c=q$ and then $q$ is the only parameter. An adequate version in our setting is the 2-parameter right-truncated geometric distribution, where (Appendix \ref{app:right_truncated_geometric})
\begin{equation}
c = \frac{q}{1 - (1 - q)^R}.
\label{eq:normalization_factor_right_truncated_geometric}
\end{equation}
Technically, the geometric distribution is the discrete analog of the exponential distribution, that is usually defined on continuous random variables (see Appendix \ref{app:exponential_distribution} for the relationship between Equation \ref{eq:exponential_distribution} and \ref{eq:geometric_distribution}). 

In this article, we use the following ensemble of models (the nickname of the model is followed by its definition):
\begin{itemize}
\item 
{\em Power 2.} The 2-parameter right-truncated power law (Equation \ref{eq:power_law_distribution} with $c$ defined by Equation \ref{eq:normalization_factor_right_truncated_power_law}). \iftoggle{squib}{}{The two parameters are $\alpha$ and $R$.}
\item
{\em Power 1.} The 1-parameter right-truncated power law that is obtained by setting $R = N$ in the Power 2 model. \iftoggle{squib}{}{The only parameter is $\alpha$.}
\item
{\em Geometric 2.} The 2-parameter right-truncated geometric (Equation \ref{eq:geometric_distribution} with $c$ defined by Equation \ref{eq:normalization_factor_right_truncated_geometric}). \iftoggle{squib}{}{The two parameters are $q$ and $R$.}
\item
{\em Geometric 1.}  A 1-parameter right-truncated geometric that is obtained by setting $R = N$ in the Geometric 2 model. \iftoggle{squib}{}{The only parameter is $q$.} 
\end{itemize}
Table \ref{tbl:models} summarizes the mathematical definition of each model and its parameters.
\iftoggle{squib}{}{
Recall that we have excluded from the ensemble popular 1-parameter models such as the (untruncated) geometric model or the zeta distribution because $r$ cannot be larger than $N=24$. 
}

\begin{table}[h]
\begin{center}
\begin{tabular}{llll}
model & definition & parameters & constraints \\
\toprule
Power 1 & \(\displaystyle p(r) = \left\{
                                        \begin{array}{ll} 
                                        \frac{1}{H(N, \alpha)}r^{-\alpha} & \text{if~} r \in [1, N]\\
                                        0 & \text{if~} r \notin [1, N] \\
                                        \end{array}
                                     \right. 
              \) 
                 & $\alpha$ & $0 \leq \alpha$ \\
Power 2 & \(\displaystyle p(r) = \left\{
                                        \begin{array}{ll} 
                                        \frac{1}{H(R, \alpha)}r^{-\alpha} & \text{if~} r \in [1, R]\\
                                        0 & \text{if~} r \notin [1, R] \\
                                        \end{array}
                                     \right. 
              \) 
                 & $\alpha$, $R$ & $0 \leq \alpha$, $1 \leq R \leq N$ \\
Geometric 1 & \(\displaystyle p(r) = \left\{
                                        \begin{array}{ll} 
                                        \frac{q}{1 - (1 - q)^N} (1-q)^r & \text{if~} r \in [1, N]\\
                                        0 & \text{if~} r \notin [1, N] \\
                                        \end{array}
                                     \right. 
              \) 
                 & $q$ & $0 < q < 1$ \\
Geometric 2 & \(\displaystyle p(r) = \left\{
                                        \begin{array}{ll} 
                                        \frac{q}{1 - (1 - q)^R} (1-q)^r & \text{if~} r \in [1, R]\\
                                        0 & \text{if~} r \notin [1, R] \\
                                        \end{array}
                                     \right. 
              \) 
                 & $q$, $R$ & $0 < q < 1$, $1 \leq R \leq N$ \\
\end{tabular}
\end{center}
\caption{\label{tbl:models} The ensemble of models. For each model, we show its definition, the free parameters and the theoretical constraints on the parameters. } 
\end{table}

\subsection{Visual diagnostic}

\label{subsec:visual_diagnostic}

Consider a plot with $p(r)$ on the $y$-axis and $r$ on the $x$-axis (Figure \ref{fig:rank_distribution}).
A preliminary conclusion about the functional dependency between $p(r)$ and $r$ can be obtained by taking logarithms on one of the axes or both. 
If $r$ followed a power law (Equation \ref{eq:power_law_distribution}), $\log p(r)$ would be a linear function of $\log r$ since
\begin{equation*}
\log p(r) = -\alpha \log r + \log c.
\end{equation*}
Then the plot in double logarithmic scale should show a straight line with a negative slope $-\alpha$. 
If $r$ followed a geometric distribution (Equation \ref{eq:geometric_distribution}), $\log p(r)$ would be a linear function of $r$ since
\begin{equation*}
\log p(r) = (r - 1) \log (1 - q) + \log c.
\end{equation*}
Then the plot in linear scale for the $x$-axis and logarithmic scale for the $y$ axis should show a straight line with a negative slope $\log (1-q)$. 

\subsection{Model selection}
 
We use information-theoretic model selection to find the best model in the ensemble.  
% We use the corrected Akaike Information criterion ($AIC_c$), that does not assume that the true model is in the ensemble, and the Bayesian Information Criterion ($BIC$), that assumes that true model is the ensemble \citep{Burnham2002a,Wagenmakers2004a}. These scores are defined as \citep{Burnham2002a}
We use the corrected Akaike Information criterion ($AIC_c$) and the Bayesian Information Criterion ($BIC$), that are defined as \citep{Burnham2002a,Wagenmakers2004a}
\begin{eqnarray}
AIC_c & = & -2\loglikelihood + 2 K \frac{F_0}{F_0 - K - 1} \nonumber \\
BIC   & = & -2\loglikelihood + K \log F_0, \label{eq:BIC}
\end{eqnarray}
where $\loglikelihood$ is the maximum log-likelihood of the parameters of the model, and $K$ is the number of parameters of the model.
The best model is the one that minimizes a criterion.

% Suppose that models in our ensemble are tagged with numbers from 1 to 4. 

We define $\Delta_i(x)$ as the difference in a score $x$ between the $i$-th model and the best model. 
\iftoggle{squib}{}
{
Thus, $\Delta_i(BIC)$ is the difference in $BIC$ between the $i$-th model and the best model.
}
We define $w_i(x)$, the weight of model $i$ according to a score $x$, as
\begin{equation*}
w_i(x) = \frac{\exp\left(-\frac{1}{2}\Delta_i(x)\right)}{\sum_j \exp\left(-\frac{1}{2}\Delta_j(x)\right)}.
\end{equation*}
$w_i(AIC_c)$, the $AIC$ weight, estimates the probability that model $i$ is the true model of the ensemble \citep{Wagenmakers2004a,Burnham2002a}. 
$w_i(BIC)$, the $BIC$ weight, estimates the probability that model $i$ is the quasi-true model in the ensemble \citep[p. 297]{Burnham2002a}.
% $BIC$ weights are called Schwarz weights \citet{Wagenmakers2004a}.
\iftoggle{squib}{}
{
The evidence of model $i$ over model $j$ with respect to some score $x$ is defined as the ratio $\frac{w_i(x)}{w_j(x)}$.
For $AIC$ and $BIC$, one has \citep{Wagenmakers2004a}
\begin{eqnarray*}
\frac{w_i(AIC)}{w_j(AIC)} & = & \frac{L_i}{L_j}exp(K_j-K_i)\\
\frac{w_i(BIC)}{w_j(BIC)} & = & \frac{L_i}{L_j}n^{\frac{1}{2}(K_j-K_i)},
\end{eqnarray*} 
where $L_i$, $K_i$, are the likelihood and the number of parameters of model $i$. For $AIC_c$, it is easy to see that
\begin{eqnarray*}
\frac{w_i(AIC_c)}{w_j(AIC_c)} & = & \frac{L_i}{L_j} exp\left[F_0 \left(\frac{K_j}{F_0-K_j-1} - \frac{K_i}{F_0-K_i-1} \right) \right]\\
\end{eqnarray*} 
} 

An obvious difference between $AIC_c$ and $BIC$ is that $BIC$ introduces a stronger penalty for parsimony than $AIC_c$ \citep{Wagenmakers2004a}. $BIC$ is more useful in selecting a correct model, that is our primary aim, while $AIC$ is more appropriate in finding the best model for predicting future observations \citep{Chakrabarti2011a}. $AIC$ is not a consistent score in the sense that
as the number of observations $F_0$ tends to infinity, the probability that $AIC$ recovers a true low-dimensional model does not converge to 1 \citep{Wagenmakers2004a}.
% $AIC$ neglects the sampling variability of the estimated parameters \citep{Wagenmakers2004a}

We revisit the derivation of $\loglikelihood$ for the Power 2 model
\iftoggle{anonymous}{{\bf(author, year)}}
{\citep{Baixeries2012c}} 
and extend it to the Geometric 2 model. 
The likelihood of a sample of ranks $\{r_1,...,r_i,...r_{F_0}\}$ can be expressed as
\begin{equation}
L = \prod_{i=1}^{F_0} p(r_i)
\label{eq:likelihood}
\end{equation} 
and then $\loglikelihood = \log L$ can be expressed as 
\begin{equation}
\loglikelihood = \sum_{r=1}^{r_{max}} f(r) \log p(r).
\label{eq:loglikelihood}
\end{equation}
For the Power 2 model (Equation \ref{eq:power_law_distribution} with Equation \ref{eq:normalization_factor_right_truncated_power_law}), the log-likelihood is
\begin{equation}
\loglikelihood = - \alpha \sum_{r=1}^{r_{max}}f(r) \log r - F_0 \log H(R, \alpha).
\label{eq:loglikelihood_power_2}
\end{equation}

For the Geometric 2 model, we derive $\loglikelihood$ by plugging Equation \ref{eq:geometric_distribution} (with $c$ defined as in Equation \ref{eq:normalization_factor_right_truncated_geometric}) into the general definition of log-likelihood in Equation \ref{eq:loglikelihood}. After some algebra, one obtains
\begin{equation}
\loglikelihood = F_0 \log \frac{q}{1 - (1 - q)^R} + (F_1 - F_0)\log(1-q).
\label{eq:loglikelihood_geometric_2}
\end{equation}
To obtain the best parameters of a model by maximum likelihood, we proceed as follows. If the model has one parameter, we use one-dimensional optimization. For the Geometric 1 model, the parameter $q$ is optimized in the interval $(0,1)$. For the Power 1 model, the parameter $\alpha$ is optimized in the interval $[0,10^6]$. For the models that have two parameters, we note that maximum $\loglikelihood$ requires $R = r_{max}$ (Appendix \ref{app:optimal_truncation}). Therefore, to maximise $\loglikelihood$, we set parameter $R$ to $r_{max}$ and optimize the other parameter following the same procedure as for the 1-parameter models.

\section{Results}

\subsection{Visual diagnostic}

We examine the look of plots of $p(r)$ as a function of $r$ (Figure \ref{fig:rank_distribution}).
In logarithmic scale only for $p(r)$, a straight line with negative slope appears, which is compatible with a geometric distribution (Figure \ref{fig:rank_distribution} middle). 
In double logarithmic scale, $p(r)$ curves, which is incompatible with a power law function (Figure \ref{fig:rank_distribution} bottom). 

\begin{figure}[h]
\includegraphics[width=\textwidth]{figures/word_order_distribution_power_geometric_2006}
\caption{\label{fig:rank_distribution_power_geometric_2006}
$f(r)$, the frequency of a word order of rank $r$ in \citet{Dryer2006a} in three distinct scales: normal (top), linear-log (middle) and log-log (bottom). The solid line is the real curve and the discontinuous lines are the expected value of $f(r)$, that is $\mathbb{E}[f(r)] = F_0 p(r)$, where $p(r)$ is defined by the best fit of a model (Table \ref{tbl:best_parameters_summary}). Left. Real curves versus the best fit of the Power 1 model (dotted line) and that of Power 2 model (dashed line). Right. Real curves versus the best fit of Geometric 1 (dotted line) and that of Geometric 2 (dashed line). }
\end{figure}

\begin{figure}[h]
\includegraphics[width=\textwidth]{figures/word_order_distribution_power_geometric_2018}
\caption{\label{fig:rank_distribution_power_geometric_2018}
$f(r)$, the frequency of a word order of rank $r$ in \citet{Dryer2018a}. The format is the same as in Figure \ref{fig:rank_distribution_power_geometric_2006}. }
\end{figure}

\subsection{Model selection}

\label{sec:model_selection_results}

\begin{table}[h]
\begin{center}
\begin{tabular}{llllll}
dataset & unit & model & $R$ & $\alpha$ & $q$ \\
\toprule
\input tables/best_parameters_table 
\end{tabular}
\end{center}
\caption{\label{tbl:best_parameters_summary}
Summary of the best parameters. For every dataset, frequency unit and model, we show the value of the parameters that maximize 
\iftoggle{squib}{$\loglikelihood$. }{$\loglikelihood$ ($R$ is the number of non-zero probability ranks, $\alpha$ is the exponent of the power-law models and $q$ is a parameter of the geometric models).} 
}
\end{table}

\begin{landscape} 
\begin{table}[h]
\begin{center}
\begin{tabular}{lllllllllll}
\textsc{dataset} & \textsc{unit} & \textsc{model} & ${\cal L}$ & $AIC_c$ & $\Delta(AIC_c)$ & $w(AIC_c)$ & $BIC$ & $\Delta(BIC)$ & $w(BIC)$\\
\toprule
\input tables/model_selection_table 
\end{tabular}
\end{center}
\caption{\label{tbl:model_selection_summary}
Summary of the model selection. For every dataset, frequency unit and model, we show the log-likelihood ($\loglikelihood$), the corrected Akaike Information Criterion ($AIC_c$), the $AIC_c$ difference ($\Delta(AIC_c)$), the $AIC_c$ weight ($w(AIC_c)$), the Bayesian Information Criterion ($BIC$), the BIC difference ($\Delta(BIC)$) and the $BIC$ weight ($w(BIC)$). }
\end{table}
\end{landscape}

A stronger conclusion on the best model is obtained by inspecting the $AIC/BIC$ scores (Table \ref{tbl:model_selection_summary}). First, the geometric distribution models give lower $AIC_c$ or $BIC$ than the power law distribution models. The $AIC_c$ weights indicate that the power law model is unlikely to be the true model of the ensemble ($w < 5\cdot 10^{-5}$ for power law models).
Similarly, the $BIC$ weights indicate that the power law model is unlikely to be the quasi-true model of the ensemble ($w < 3\cdot 10^{-5}$ for power law models). Visual inspection confirms it: the theoretical curve of the geometric model is closer to the actual distribution than the theoretical model of the power law (Figures \ref{fig:rank_distribution_power_geometric_2006} and \ref{fig:rank_distribution_power_geometric_2018}). The power-law fails simultaneously by overestimating $p(1)$ and not capturing the faster decay of the actual distribution.
The best values of $q_i$, the value of $q$ of Geometric $i$ model, indicate that $|q_1 - q_2|$ is small but always $q_1 > q_2$ (Table \ref{tbl:best_parameters_summary}). 
In addition, $q_i$ is close to $1/\left< r \right>$, the maximum likelihood estimator of $q$ for an untruncated geometric distribution (Equation \ref{eq:geometric_distribution} with $c = q$).
% Whether $R = 24$ (Geometric 1) or $R = r_{max}$ (Geometric 2), makes a very little difference on the value of $q$.
% $q$ decreases in value as more robust units of frequency are used. 

When comparing Geometric 1 and Geometric 2 with information criteria, the results depend on the score. $AIC_c$ provides more support for Geometric 2 while $BIC$ provides more support for Geometric 1. In particular,
\begin{itemize}
\item
{\em $AIC_c$}. Geometric 2 is better than Geometric 1 in terms of $AIC_c$, except in the 2006 dataset, where the $AIC$ of Geometric 2 is just 0.1 nats above Geometric 1. 
% Therefore, the right-truncated geometric models are practically indistinguishable in terms of the quality of their fit.
We consider the evidence of Geometric 2 over Geometric 1 by means of $w_2(AIC_c)/w_1(AIC_c)$. In the 2006 dataset, Geometric 2 is about as likely to be the true model as Geometric 1 ($w_2(AIC_c)/w_2(AIC_c) \approx 1$). In the 2018 dataset, Geometric 2 is about 2 times more likely to be the quasi-true model than Geometric 1 when frequency is measured in languages, about 3 times more likely when frequency is measured in genera and about 7 times higher when frequency is measured in adjusted number of languages. 
\item
{\em $BIC$}. Geometric 1 is better than Geometric 2 in terms of $BIC$, except for adjusted number languages in the 2018 dataset, where the $BIC$ of Geometric 1 is just 0.6 nats above Geometric 2. However, the sample size is the smallest for adjusted number of languages (Table \ref{tbl:elementary_summary}) which is a hindrance for model selection with $BIC$ \citep[p. 288]{Burnham2002a}; recall the dependence of BIC on sample size in Equation \ref{eq:BIC}. 
% Therefore, the right-truncated geometric models are practically indistinguishable in terms of the quality of their fit.
We consider the evidence of Geometric 1 over Geometric 2 by means of $w_1(BIC)/w_2(BIC)$.
The $BIC$ weights indicate that Geometric 1 is about 6 times more likely to be the quasi-true model than Geometric 2 in the 2006 dataset ($w_2(BIC)/w_2(BIC) \approx 6$). In the 2018 dataset, Geometric 1 is about 4 times more likely to be the quasi-true model than Geometric 2 when frequency is measured in languages, about 7/3 times more likely when frequency is measured in genera but about 3/4 ``higher'' when frequency is measured in adjusted number of languages.
\end{itemize}

\section{Discussion}

\subsection{Exponential distributions}

We have shown that the geometric distribution, a discrete exponential-like distribution, is a much better model than a power law distribution, both according to visual diagnostic and model selection. Our conclusion is consistent with Cysouw's proposal \citep{Cysouw2010a} and at odds with Martin et al's assumption of a power law \citep{Martin2020a}. The expectation of a power law has led to misclassify the rank distribution of vocalizations produced by other species as power laws by means of visual diagnostic \citep{Howes-Jones1988a,Dreher1961a,Janik2006a}. In Figure 12 (p. 24), \citet{Howes-Jones1988a} show the frequency of the calls of the warbling vireo ({\em Vireo vilgus vilgus}) as a function of their rank. 
In figure 2 (p. 1800), \citet{Dreher1961a} shows the frequency of the ``tonemes'' produced primarily by dolphins ({\em Tursiops
truncatus}). In both cases, authors conclude that finding a straight line in linear-log scale agrees with Zipf's law for word frequencies. However, we have shown that a straight line in that scale is indeed indicative of an exponential distribution (Section \ref{subsec:visual_diagnostic}). Thus power laws are less ubiquitous than usually believed.  

The abundance of exponential-like distribution has implications for the debate on the meaningfulness of linguistics laws \citep[Box 2]{Semple2021a}. Since Zipf's foundational research \citep{Zipf1949a}, many researchers have cast doubts on the depth and utility of linguistic laws such as Zipf's rank-frequency law, the power law that characterizes the distribution of word ranks \citep{Zipf1949a,Moreno2016a,Mehri2017a}. The recurrent criticism that linguistic laws are inevitable can be falsified by finding patterns across different species and systems that do not concord with the law that is claimed to be inevitable \citep{Semple2021a}. Various sources of evidence demonstrate that Zipf's rank-frequency law, is not inevitable. First, our finding that an exponential distribution yields a better fit to word order ranks than a power-law. Second, the exponential distribution that is found in part-of-speech tags \citep{Tuzzi2010a} and potentially other linguistic units \citep{Cysouw2010a}. Third, the exponential rank distribution in the species mentioned above as well as in ``key signs'' produced by rhesus monkeys \citep[Figure 3, p. 367]{Schleidt1973a}.
\iftoggle{extra}{
Fourth, the exponential distribution of other linguistic variables such as the distance between syntactically related words \citep{Ferrer2004b,Petrini2022c}.
Finally, in non-linguistic contexts, the projection distances between cortical areas exhibit an exponential distribution \citep{Ercsey2013a} and a double exponential distribution characterizes the average distance traversed by foraging ants \citep{Campos2016a}, just to name a few. 
}
{}

\subsection{The correct exponential distribution}

We have found that the best model depends on the score. According to $AIC_c$, Geometric 2 has more evidence than Geometric 1 (except for the 2006 dataset). According to $BIC$, Geometric 1 has more evidence (except for adjusted number of languages in the 2018 dataset). However, a stronger conclusion can be reached by looking at the ability of the models to generalize in perspective. 
\citet{Cinque2005a} reported only 14 attested orders. Although he did not report the frequencies of each attested on a sample, we can be totally certain that the best fit of Geometric 2 by maximum likelihood would conclude that only $R = 14$ orders have non-zero probability (justification in Appendix \ref{app:optimal_truncation}), which we know it would be wrong because 17 and 18 orders were found later on \citep{Dryer2006a,Dryer2018a}. We also know that the best fit of Geometric 2 to the 2006 dataset, namely $R = 17$ is misleading because $R=18$ for the 2018 dataset (Table \ref{tbl:best_parameters_summary}). Thus, Geometric 2 fails to generalize two times. 

When integrating likelihood into an information theoretic criterion and considering again the 2006 dataset, $AIC_c$ weights conclude that Geometric 1 is as likely as Geometric 2 (recall $w_2(AIC_c)/w_2(AIC_c) \approx 1$) but $BIC$ weights are able to realize that Geometric 1 is more likely to be the best (recall $w_1(BIC_c)/w_2(BIC) \approx 6$). Crucially, $AIC_c$ does not foresee that the best model in the 2006 dataset, Geometric 2, produces a zero likelihood when applied to the 2018 dataset (notice that the application of the best fit of Geometric 2 for the 2006 dataset to the 2018 dataset yields $p(18) = 0$, which produces $L = 0$ when applied to equation \ref{eq:likelihood}). In contrast, the best model for the 2006 dataset according to BIC yields a non-zero likelihood when applied to the 2018 dataset. This is consistent with the view of $BIC$ as more useful in selecting a correct model (in this case, discarding an incorrect model) than AIC \citep{Chakrabarti2011a}.
In sum, BIC catches early the model that generalizes while $AIC_c$ is unable to realize that one of the models overfits the 2006 dataset. 
% This could be expanded by my recent research showing that it is likely that there are >18 orders
% Thus, we have demonstrated that a geometric model that imposes that the 24 orders must have non-zero probability (Geometric 1) is a better model than another that does not (Geometric 2).

Above, we have provided empirical evidence of the failure of $AIC_c$ to find a correct model in our application. However, we wish to highlight the theoretical power of $BIC$ for our research problem and our datasets.
Once we have discarded the power-law models, the model selection reduces to choosing between two geometric models. $BIC$ assumes equal prior probability on each model \citep{Burnham2002a}. That implies assigning initial equal chance to early truncation (Geometric 2) and to late truncation (Geometric 1).
Indeed, such a contest between two nested models, Geometric 1 and Geometric 2, can be seen as a dimensionality guessing problem (Geometric 2 adds on dimension with respect to Geometric 1) and $BIC$ is a consistent estimator of $K$, the dimensions of the ``true model'' \citep[p. 284]{Burnham2002a}. $BIC$ implicitly assumes that {\em ``truth is of fairly low dimension (e.g., $K = 1-5$) and that the data-generating (true) model is fixed as sample size increases''} \citep[286]{Burnham2002a}. 
Consistently, \citet{Dryer2018a} accurately modelled the frequency rank of the orders by means of 5 binary descriptive principles, suggesting that the true number of dimensions of the distribution is bounded above by $K = 5$. Indeed, just $\lfloor \log_2 N \rfloor + 1 = 5$ binary parameters suffice to sort $N$ orders so as to match the desired frequency rank. Previously, \citet{Cysouw2010a} had proposed exponential models for the frequency of orders with 3, 4, or 6 binary parameters. In fact, $BIC$ always provides more evidence for Geometric 1 except for adjusted number of languages in the 2018 dataset, that coincides with the smallest sample (Table \ref{tbl:elementary_summary}). The consistency property of $BIC$ requires a large enough sample (Equation \ref{eq:BIC} and \citet[p. 288]{Burnham2002a}). 
Therefore, our findings so far and the conditions where we are applying $BIC$ suggest that Geometric 1 is a stronger model for the underlying distribution.

\subsection{Are there hard constraints on word order?}

The evidence for Geometric 1 is a challenge for the existence of a hard constraints limiting word order variation in languages that would explain why not all 24 possible orders are attested \citep{Cinque2005a, Cinque2013a,Medeiros2016a, Medeiros2018a}. A hard constraint is a constraint that makes certain word orders impossible. A soft constraint is one that reduces the probability of certain orders, but never to zero probability. If there were no hard constraints (only soft constraints), the best model should be Geometric 1; if strong constraints existed, Geometric 2 (with $R < 24$) should be the best model.
The strength of Geometric 1 is consistent with Cysouw's hypothesis: all orders are {\em a priori} possible but some are not attested due to undersampling \citep{Cysouw2010a}. The challenge of proponents of a hard constraint is two-fold. First, to make a robust proposal, one that does not fall any time that new orders are attested. For instance, the proposal that the 14 orders attested by \citet{Cinque2005a} result from universal grammar or some universal cognitive mechanism, still stands as soft constraint but not as hard constraint  \citep{Cinque2005a, Cinque2013a, Medeiros2016a, Medeiros2018a} because so far, 18 orders have been attested \citep{Dryer2018a}. Second, to deal with parsimony. % In the language of classic statistics, the non-existence of a hard constraint is the null hypothesis while the existence of some hard constraint is the alternative hypothesis. 
Postulating a hard constraint implies a loss of parsimony but is it rewarding enough in terms of accuracy or prediction capacity? Information criteria address this question and give a compelling answer: when generalizability and parsimony are prioritized (this is the emphasis of $BIC$ with respect to $AIC_c$), the absence of a hard constraint (Geometric 1) becomes more likely than its presence (Geometric 2).

\section*{Appendix}

\appendix

\section{Exponential versus geometric distribution}

\label{app:exponential_distribution}

The customary definition of an exponential distribution for a discrete random variable, e.g., frequency rank (Equation \ref{eq:exponential_distribution}) and the definition of a geometric distribution (Equation \ref{eq:geometric_distribution}) are equivalent. To see it, notice that 
\begin{eqnarray*}
p(r) & = & c (1-q)^{r - 1} \\
     & = & c e^{(r - 1)\log(1-q) } \\
     & = & c' e^{-\beta r}, 
\end{eqnarray*}
where $c' = c/(1 - q)$ and $\beta = -\log(1-q)$.

\section{The right-truncated geometric distribution}

\label{app:right_truncated_geometric}

The 2-parameter right-truncated geometric distribution is obtained when the normalization factor in Equation \ref{eq:geometric_distribution} is  
$c=1/S(1, R)$, where 
\begin{equation*}
S(1, R) = \sum_{r=1}^{R}(1 - q)^{r - 1}.
\end{equation*}
A compact expression for $S(1, R)$ is easy to obtain. By the self-similarity property of the geometric series,
\begin{equation*}
(1-q)S(1, R) = S(1, R) - 1 + (1-q)^{R}.
\end{equation*}
After some simple algebraic manipulations, one obtains 
\begin{equation*}
S(1, R) = \frac{1 - (1 - q)^{R}}{q}
\end{equation*}
and then 
\begin{equation*}
c = \frac{q}{1 - (1 - q)^{R}}.
\end{equation*}
It is easy to check that 
\begin{equation*}
\lim_{R \rightarrow \infty} c = q,
\end{equation*}
which is the normalization factor of the untruncated geometric distribution. 

\section{The value of $R$ that maximizes log-likelihood}

\label{app:optimal_truncation}

We aim to show that maximum $\loglikelihood$ requires $R = r_{max}$ for the Power 2 model and the Geometric 2 model.
First, we show that maximum $\loglikelihood$ requires $R \geq r_{max}$. As these models are such that $p(r) = 0$ for $r > R$, setting $R < r_{max}$ implies that the likelihood of the model is $L=0$ (recall Eq. \ref{eq:likelihood}) and the log-likelihood $\loglikelihood$ goes to $-\infty$.   
Second, we show that the log-likelihood functions of these models are monotonically decreasing functions of $R$ when the other parameter ($\alpha$ or $q$) is constant and thus maximum $\loglikelihood$ requires $R = r_{max}$. Let us assume that $\alpha$ and $q$ are constant. For Power 2, we revisit previous arguments 
\iftoggle{anonymous}{{\bf(author, year).}}
{\citep{Baixeries2012c}. }

The only summand in Eq. \ref{eq:loglikelihood_power_2} that depends on $R$ is $-F_0 \log H(R, \alpha)$. 
The recursive definition  
\begin{equation}
H(R, \alpha) = \left\{ 
               \begin{array}{ll}
               1 & \text{if~} R = 1 \\
               H(R - 1, \alpha) + R^{-\alpha} & \text{if~} R > 1
               \end{array}
               \right.
\end{equation}
clearly shows that $H(R, \alpha)$ is a monotonically increasing function of $R$ (when $\alpha$ is fixed)
and that $-F_0 \log H(R, \alpha) <0$ since $F_0>0$ and $H(R, \alpha) \geq 1$. Therefore, $\loglikelihood$ is a monotonically decreasing function of $R$ (for constant $\alpha$).
For Geometric 2, we recall the assumption $q \in (0,1)$ and note that the 1st derivative of $\loglikelihood$ (Eq. \ref{eq:loglikelihood_geometric_2}) with respect to $R$ is
\begin{eqnarray*}
\frac{\partial \loglikelihood}{\partial R} & = & -F_0 \frac{\partial \log (1 - (1 - q)^R)}{\partial R} \\
                                           & = & F_0 \frac{(1-q)^R \log(1-q)}{1 - (1-q)^R}.
\end{eqnarray*}
It is easy to see that $\partial \loglikelihood / \partial R <0$ because all terms in the expression are strictly positive except $\log(1-q) < 0$.

% \section*{Data availability/Supplementary files}

% Data and code are available from \href{https://osf.io/n2xqy/?view_only=261e2f5ba138432a98d3ff1207c2ebaa}{here}.

\iftoggle{anonymous}{}
{

\begin{acknowledgement}
We are grateful to D. Dediu for very valuable comments and to G. Ramchand % Gillian Ramchand
for making us aware of Medeiro's research. This research is supported by a recognition 2021SGR-Cat (01266 LQMC) from AGAUR (Generalitat de Catalunya) and the grant AGRUPS-2024 from Universitat Politècnica de Catalunya.
\end{acknowledgement}
}

\bibliographystyle{apacite}

\bibliography{../../../../Dropbox/biblio/rferrericancho,../../../../Dropbox/biblio/complex,../../../../Dropbox/biblio/ling,../../../../Dropbox/biblio/cl,../../../../Dropbox/biblio/cs,../../../../Dropbox/biblio/maths}

\end{document}
