% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[preprint]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

\usepackage{booktabs}
\usepackage{multirow}
\usepackage{colortbl}
\usepackage{makecell}

\NewExpandableDocumentCommand\Gap{}{\XGap{3pt}}
%\usepackage[table,xcdraw]{xcolor}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}
\usepackage{amssymb} % For \checkmark
\usepackage{caption} % For customizing captions
\usepackage{amsmath}
\usepackage{subcaption}   
\DeclareMathOperator*{\argmin}{argmin}

\usepackage{nicefrac,xfrac}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{Rotate, Clip, and Partition: Towards W2A4KV4 Quantization by Integrating Rotation and Learnable Non-uniform Quantizer}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author{Euntae Choi\thanks{these authors contributed equally.}, Sumin Song\footnotemark[1], Woosang Lim, Sungjoo Yoo \\
  Seoul National University \\
  \texttt{euntae.choi175@gmail.com, songsm921@snu.ac.kr,} \\ \texttt{ftyg656512@snu.ac.kr, sungjoo.yoo@gmail.com} \\}


%\author{
%  \textbf{First Author\textsuperscript{1}},
%  \textbf{Second Author\textsuperscript{1,2}},
%  \textbf{Third T. Author\textsuperscript{1}},
%  \textbf{Fourth Author\textsuperscript{1}},
%\\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
%\\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
%\\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
%\\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
%\\
%\\
%  \textsuperscript{1}Affiliation 1,
%  \textsuperscript{2}Affiliation 2,
%  \textsuperscript{3}Affiliation 3,
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
%\\
%  \small{
%    \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
%  }
%}

\begin{document}
\maketitle
\begin{abstract}
%The growing size of large language models (LLMs) demands efficient deployment strategies. Quantization is emerging as a promising solution for reducing memory and computation costs with acceptable performance degradation.
We propose Rotate, Clip, and Partition (RCP), a Quantization-Aware Training (QAT) approach that first realizes extreme compression of LLMs with W2A4KV4(2-bit weight, 4-bit activation, and 4-bit KV-cache) configuration. RCP integrates recent rotation techniques with a novel non-uniform weight quantizer design, by quantitatively analyzing the impact of random rotation on 2-bit weight quantization. Our weight quantizer features Learnable Direct Partitioning (LDP), which introduces learnable parameters to directly learn non-uniform intervals jointly with LLM weights. We also present a specialized GPU kernel that supports GEMV on non-uniform W2A4. Experiments show that RCP can compress LLaMA-2-7B to W2A4KV4 with a loss of only 2.84 WikiText2 ppl and 5.29 times reduced memory footprint. Furthermore, RCP can quantize challenging mobile-targeted LLaMA-3.2 models and domain-specific WizardCoder-7B and MetaMath-7B with no critical problems such as convergence failure and repetition. Code will be made available at \url{blind_review}.
\end{abstract}


\section{Introduction}

\label{sec:intro}
% Large language models (LLMs) have made significant progress in recent years, displaying strong capabilities across a wide range of tasks. Nonetheless, the increasing complexity and scale of contemporary LLMs present significant challenges for deployment, particularly in resource-constrained or edge environments. The substantial computational and memory demands of such models often impede their direct adoption in latency-sensitive scenarios or on mobile devices.
Large language models (LLMs) have made significant advancements, but their growing size and resource demands create challenges for deployment across data centers and mobile devices. To address these constraints, extensive research efforts have focused on improving the efficiency of LLM serving through various strategies. Quantization, one of the various methods, has emerged as a particularly straightforward and effective method for reducing memory consumption and inference costs without severely compromising accuracy. By lowering the numerical precision of parameters and intermediate representations, quantization leverages hardware capabilities such as half-precision and integer tensor cores. 

Post-Training Quantization (PTQ) is a quantization technique that performs well up to levels like W4A4\footnote{We call $l$-bit weight, $m$-bit activation and $n$-bit KV-cache W$l$A$m$KV$n$ like W2A4KV4.}, maintaining acceptable accuracy compared to original models. However, as the bit-width decreases, the representable information becomes insufficient, making it difficult to address challenges such as salient weights, outliers, and the quantization of activations and the KV-cache.
\begin{figure}[t!] % 두 컬럼을 차지하는 그림, t 옵션으로 상단 배치
    \centering
    \includegraphics[width=\columnwidth]{latex/figs/ppl_compare.pdf} % 그림의 폭을 전체 텍스트 폭으로 설정
    \caption{Bit-Level scaling laws for perplexity for LLaMA-3~\cite{llama3modelcard} (1B, 3B, 8B). }
    \vspace{-0.5cm}
    \label{fig:pploverview}
\end{figure}
To address these issues, rotation techniques~\cite{quarot,spinquant} have proven effective at W4A4KV4. Nevertheless, extending these methods to even lower bit widths, such as W2 or W3, remains insufficient, as the severe information bottleneck cannot be overcome with a uniform quantization and PTQ, ultimately resulting in a loss of generation quality.
\begin{figure*}[t!] % 두 컬럼을 차지하는 그림, t 옵션으로 상단 배치
    \centering
    \includegraphics[width=0.9\textwidth]{latex/figs/math_example.pdf} % 그림의 폭을 전체 텍스트 폭으로 설정
    \caption{A chain of thought (CoT) reasoning example from GSM8K~\cite{gsm8k} benchmark, conducted by two differently quantized MetaMath-7B~\cite{metamathQA} models. The result on the left is from a state-of-the-art QAT method BitDistiller~\cite{BitDistiller}. On the right, our proposed RCP is applied. Both methods employ exactly the same 4-bit quantization setting for activation and KV-cache.}
    \vspace{-0.3cm}
    \label{fig:gen_math}
\end{figure*}

In this work, we propose a Quantization-Aware Training (QAT) approach called Rotate, Clip, and Partition (RCP), which integrates rotation techniques with non-uniform quantization strategies to achieve extreme compression of LLMs, specifically realizing W2A4KV4 configurations. Our method jointly optimizes quantization parameters and model weights by systematically incorporating rotation and non-uniform group quantization. We recover meaningful model performance and demonstrate that even 2-bit weight precision, once deemed unattainable, is now within reach. We further extend our techniques to more challenging models, such as LLaMA-3.2~\cite{llama3modelcard}, as well as smaller, mobile-targeted architectures, thereby broadening the applicability of these methods. To facilitate practical deployment, we design efficient W2A4 inference kernels for decoding phase on modern GPUs, thus enabling real-time, resource-efficient LLM serving at extreme compression levels.

Our contributions are summarized as follows:
\begin{itemize}
\item{We propose a QAT algorithm that successfully leverages the benefit of random rotation to first achieve W2A4KV4 quantization of LLMs.}
\item{We demonstrate the scalability of our approach to more challenging (e.g., LLaMA-3.2) and smaller, mobile-targeted models.}
\item{Additionally, we develop efficient W2A4 inference kernels for the decoding stage on modern GPUs, enabling real-time, resource-efficient LLM serving under extremely low-bit precisions.}
\end{itemize}


\section{Preliminaries}
\label{sec:prelim}

\subsection{Random Rotation for LLM Quantization}
QuaRot~\cite{quarot} proposed to apply random rotations while keeping the computational invariance suggested in SliceGPT~\cite{slicegpt}. Random rotation suppresses activation outliers and helps quantization, successfully achieving W4A4KV4 with minimal performance loss.

As shown in Fig. \ref{fig:overview}, $R_1$ rotates all transformer decoder layers' input and output activations and its inverse ($R_1^T$) is usually fused into the neighboring model weights. 
$R_2$ and $R_4$ both require online rotation during inference as they target the intermediate activations in the MHA and FFN layers, respectively. $R_2$ is decomposed into two orthogonal matrices: one with head dimension size ($R_{H}$), applied during the V projection, and another with the number of heads ($R_{H}^\prime$), applied to the self-attention activation. Its transpose ($R_2^T$) is applied as a whole to the out-projection in the self-attention layers. $R_3$ rotates query (Q) and key (K) vectors after RoPE, ensuring that the KV-cache can be compressed without affecting the self-attention output.

\subsection{Asymmetric Quantization and Clipping}
% \begin{itemize}
% \iffalse
% \item{Symmetric} \\
% Symmetric quantization employs a symmetrical range centered around the origin point for value quantization. This method enables straightforward implementation by setting the zero-point to 0. However, it may not provide optimal results when handling skewed data distributions. Symmetric quantization equation can be formulated as follows: 
% \begin{equation}\label{eq:symmetric}
% \begin{split}
%     \mathbf{W_q} = \mathrm{clamp}(\lfloor \frac{\mathbf{W}}{h} \rceil, -2^{N-1}, 2^{N-1}-1),\\
%     \mathrm{where}\,\, h=\frac{\max(\mathbf{\lvert W \rvert})}{2^{N-1}-1},\\ 
% \end{split}
% \end{equation}
% \fi

Asymmetric quantization uses min/max of data to match data's asymmetric distribution.
This approach can offer higher quantization accuracy than symmetric quantization. The asymmetric quantization equation can be formulated as follows:
\vspace{-0.3cm}
\begin{equation}\label{eq:asymmetric}
\small
\begin{split}
    \mathbf{W_q} = \mathrm{clamp}(\lfloor \frac{\mathbf{W}}{h} \rceil + z, 0, 2^{N}-1),\\
    \mathrm{where}\,\, h=\frac{\max(\mathbf{W})-\min(\mathbf{W} )}{2^{N}-1},\\ 
    z=-\lfloor \frac{\min(\mathbf{W})}{h} \rceil
\end{split}
\end{equation}
\normalsize
\vspace{-0.5cm}

Clipping is an optimization technique that truncates outlier values to a predetermined range before quantization. It improves quantization precision within the compressed range, preserving performance while increasing compression ratios. Our proposed method, RCP is based on the learnable weight clipping (LWC)~\cite{omniquant}, an asymmetric quantization to obtain asymmetric clipping values that minimize quantization error for each linear layer and then uses them as initial values for learnable clipping $f$ as follows:
\vspace{-0.2cm}
\begin{equation} \label{eqn:clip}
\small
%\begin{split}
f(\textbf{W}, \beta, \gamma)=clip(\textbf{W}, \sigma(\beta)\text{min}(\textbf{W}), \sigma(\gamma)\text{max}(\textbf{W}))
%\end{split}
\end{equation}
\normalsize
\vspace{-0.7cm}


where $\beta$ and $\gamma$ are learnable parameters for clipping and $\sigma$ is the sigmoid function. 
%Post-training quantization (PTQ) has been a dominant method in the LLM optimization field since quantization-aware training (QAT) often demands heavy computational resources and long training time due to the enormous size of model parameters and their optimizer state.
%However, this can no longer be the case under extremely low-bit quantization settings (weight bits less than or equal to 2) where PTQ algorithms suffer from significant performance drops or even convergence failure, explaining a recently growing interest in QAT~\cite{bitdistiller,efficientqat,lrqat}.
%The key aspect of designing a QAT method, therefore, is efficiency in terms of training data and execution speed of algorithm.


\section{Observation: Rotation Sharpens Weight Distribution}

\begin{figure}[h!]
    \centering
    % 첫 번째 그림

    \begin{subfigure}[t]{\linewidth} % 전체 너비 사용
        \centering
        \includegraphics[width=1.0\linewidth]{latex/figs/w_kurt.pdf} % 그림 크기 조정
        \caption{Per-group kurtosis measured on the 15th key projection weight in LLaMA-2 7B model before and after random Hadamard transform with layernorm fusion.}
        \label{fig:kurtosis_w}
    \end{subfigure}    
    \vspace{1em} % 두 그림 사이 간격 추가
    
    \begin{subfigure}[t]{\linewidth} % 전체 너비 사용
        \centering
        \includegraphics[width=1.0\linewidth]{latex/figs/a_kurt.pdf} % 그림 크기 조정
        \caption{Per-group kurtosis measured on the input activation of the 15th key projection weight in the same manner as \ref{fig:kurtosis_w} in LLaMA-2 7B model. Due to a large difference of average value, the y axis is presented in log scale. Since almost all groups had small negative kurtosis value after transform, we take the absolute value of them.}
        \label{fig:kurtosis_a}
    \end{subfigure}

    
    % 두 번째 그림

    
    \caption{Kurtosis analysis results to demonstrate the effect of random rotation on model weight and activation.}
    \vspace{-0.3cm}
    \label{fig:kurtosis}
\end{figure}

As shown in Fig. \ref{fig:pploverview}, simply applying rotation techniques before LLM quantization can't guarantee the performance in extremely low-bit scenarios like W2A4. Building on SmoothQuant~\cite{smoothquant}'s insights, outlier suppression methods improve quantization possibility in the activation space while accepting modest weight space compromises. But recent rotation based PTQ techniques predominantly emphasize the advantages in the activation space, not the disadvantages in the weight space. While rotation-based PTQ techniques work well for moderate quantization up to W4, at W2, the quantization possibility significantly drops. 

In this section, we analyze quantization possibility after random Hadamard transformation in both weight and activation spaces in LLaMA-2 7B model.
%using representative statistical measure, kurtosis.
Kurtosis, which measures the fourth standardized moment of a distribution, serves as a key indicator for evaluating outlier frequency and concentration~\cite{amxfp4} that intensify the difficulties in quantization. We selected the 15th key projection module for analysis because it showed one of the highest mean squared errors (MSE) when comparing the distributions before and after transformation among all modules. We analyze kurtosis of weight and input activation in group size of 128 following their channels to fully consider the effects of groupwise quantization configuration used in most quantization methods. 

Our analysis in Fig. \ref{fig:kurtosis_a} reveals that rotation effectively distributes activation values along their channel direction, similar to methods like SliceGPT~\cite{slicegpt}, QuaRot~\cite{quarot} and PrefixQuant~\cite{prefixquant}. The original space shows infinite kurtosis due to extreme outliers, while the rotated space shows predominantly near-zero values. From the perspective of per-channel uniform quantization in the activation, the transformation reduces quantization error compared to the original space.

Conversely, our analysis in Fig. \ref{fig:kurtosis_w} shows contrasting circumstances. After going through essential processes such as LayerNorm Fusion and Random Hadamard Transformation, an initially platykurtic distribution with mean kurtosis near 1 becomes more leptokurtic with mean kurtosis near 3.75. These observations indicate that the weight distribution deviates further from the uniform distribution, leading to significant quantization errors when using W2 uniform quantizers, as most weights cluster in two center quantization bins. This finding motivates the development of non-uniform quantizers to optimize bin width distributions. This analysis also explains why FP3 variants outperform INT3 quantization in W3 regime, as NF3's approach better handles bell-shaped weight distributions, as demonstrated in AFPQ~\cite{afpq}.
\vspace{-0.5cm}

\section{Methodology}
\vspace{-0.5cm}


We propose a non-linear weight quantization (Section \ref{sec:NL2}), an efficient dequantization for non-linear weight partitions (Section \ref{sec:dequant}), and a GPU inference kernel for extreme low-bit weights (Section \ref{sec:W2A4Kernel}).

\label{sec:method}
\begin{figure*}[t!] % 두 컬럼을 차지하는 그림, t 옵션으로 상단 배치
    \centering
    \includegraphics[width=0.9\textwidth]{latex/figs/overview.pdf} % 그림의 폭을 전체 텍스트 폭으로 설정
    \caption{An overview of our QAT-based KD framework of RCP. RCP performs rotation-aware clipping, followed by learnable non-uniform quantization (LDP). During this process, knowledge distillation is employed as the training method.}
    \vspace{-0.5cm}
    \label{fig:overview}
\end{figure*}


\subsection{Differentiable Non-uniform INT2 Group Quantizer}
\label{sec:NL2}

\paragraph{Rotation-aware Clipping Initialization}
We initialize clipping parameters in a rotation-aware manner as follows:
\vspace{-0.2cm}
\begin{equation} \label{eqn:clipinit}
\small
%\begin{split}
\argmin_{\beta, \gamma} \|Q(\textbf{W}_{R})\textbf{X}_{R} - \textbf{W}_{R}\textbf{X}_{R}\|^2
%where~&W_{R}=R^T_{front}WR_{rear}, \\
%&X_R=XR_{front}
%\end{split}
\vspace{-0.2cm}
\end{equation}
\normalsize
where $\textbf{W}_{R}=R^T_{front}f(\textbf{W},\beta,\gamma)R_{rear}$ is the weight matrix obtained by applying appropriate rotations after the clipping $f$ in Eqn. \ref{eqn:clip}. For instance, $R_{front}$ and $R_{rear}$ are set to $R_1$ and $I$ for up and gate projections, respectively. Rotation configuration for the other types of weights can be found on the left side of Fig. \ref{fig:overview}.


\vspace{-0.5cm}
\paragraph{Learnable Direct Partitioning}
\begin{figure}[h!] % 두 컬럼을 차지하는 그림, t 옵션으로 상단 배치
    \centering
    \includegraphics[width=0.85\columnwidth]{latex/figs/ldp.pdf} % 그림의 폭을 전체 텍스트 폭으로 설정
    \caption{A diagram of Learnable Direct Partitioning (LDP). }
    \vspace{-0.3cm}
    \label{fig:ldp}
\end{figure}
The key component of our proposed RCP is a weight quantizer module named learnable direct partitioning (LDP).
In order to realize asymmetric weight quantization, we determine value range by LWC and the partitions within the value range by LDP.
LDP starts by normalizing the weight $\textbf{W}$ by a dynamic range $h=\sigma(\gamma)\text{max}(\textbf{W})-\sigma(\beta)\text{min}(\textbf{W})$ determined by the LWC. In LDP, two partitioning variables $s_1$ and $s_2$ are introduced per weight group\footnote{For brevity, we do not use a separate notation for group quantization unless necessary since our method can be applied likewise simply by reshaping a weight so that the last dimension becomes the group size.}, which split the range $h$ into three subsections.
\vspace{-0.3cm}
\begin{equation}\label{eqn:partition}
\begin{split}
p_1=\sigma(s_1), p_2=(1-p_1)\sigma(s_2), \\
p_3=(1-p_1)(1-p_2)
\end{split}
\end{equation}
\vspace{-0.5cm}


As shown in Fig. \ref{fig:ldp} and Eqn. \ref{eqn:partition}, $s_1$ defines the portion of the first partition $p_1$ in the whole dynamic range $h$. $s_2$ defines the portion of the second partition $p_2$ from the remaining range $(1-p_1)h$. The third one is trivially calculated. This formulation guarantees that i) the dynamic range $h$ is seamlessly filled out, ii) each portion is constrained between 0 and 100\% via the sigmoid re-parametrization, and iii) no matter how the parameters are updated, the order of the partitions stays the same. The partitioning parameters are initialized as $s_1=\sigma^{-1}(1/3)$ and $s_2=\sigma^{-1}(1/2)=0$ to evenly split the quantization range in the beginning. The AWQ-styled grid search~\cite{awq} can also be used to find the optimal partition widths; however, the computational burden will grow exponentially since we have to iterate over a four-dimensional loop (two for LWC and two for LDP).

The quantization process of our LDP can then be derived as follows:
\begin{equation}\label{eqn:quant}
\small
%\begin{split}
\textbf{W}_{q} = \text{u}(\frac{\textbf{W}}{h}-t_1) + \text{u}(\frac{\textbf{W}}{h}-t_2) + \text{u}(\frac{\textbf{W}}{h}-t_3)
%\end{split}
\end{equation}
\normalsize

where $u(x)$ is the step function and transition point $t_i$ (i.e., the right edge of each quantization bin) is set to the center of each partition, computed as $t_1=p_1/2$ and $t_i=t_{i-1} + (p_{i-1} + p_i)/2$. The straight-through estimator is applied to all step functions so that every parameter (including LLM weights, clipping, and partitioning parameters) can be updated via backpropagation.
\begin{figure*}[t!] % 두 컬럼을 차지하는 그림, t 옵션으로 상단 배치
    \centering
    \includegraphics[width=0.85\textwidth]{latex/figs/gemv.pdf} % 그림의 폭을 전체 텍스트 폭으로 설정
    \caption{An overview of our GPU GEMV kernel with data path along memory hierarchy, pipelining, and epilogue concisely illustrated. $\texttt{wid}$ is the warp index and the per-thread accumulator is simplified (warp lane dimension is not shown).}
    \vspace{-0.5cm}
    \label{fig:gemv}
\end{figure*}

\subsection{Dequantization for Non-Linear Weight Partitions} 
\label{sec:dequant}
Unlike the usual uniform quantization scheme, mapping the quantized weights back to real values is not trivial in NU quantizers as the design space of NU dequantization method is large and the inference pipeline is directly affected. In NU2U~\cite{nu2u}, the quantized weight $\textbf{W}_q$ is simply dequantized to a uniform grid. Similarly, LLT~\cite{learnablelut} trains a learnable lookup table for weight and activation to adjust the quantization bin widths but keeps the dequantization the same as other common uniform quantizers.

This design choice has an obvious advantage: Inference can be done on existing hardware without any modification. However, we propose non-uniform dequantization as described in Eqn. \ref{eqn:dequant}, based on our observation that uniform dequantization can lead to performance drop under extremely low bit configurations, especially on smaller models.
\vspace{-0.2cm}
%Dequantization method must be carefully designed as it directly affects inference pipeline and hardware compatibility since almost all accelerators can only support linear dequantization natively. 
%Although an increasing amount of effort is being made on specialized architectures for efficient non-uniform dequantization through LUT,
\begin{equation}\label{eqn:dequant}
\small
\begin{split}
\textbf{W}_{deq} =& \sigma(\beta)\text{min}(\textbf{W}) \\
+ \frac{h}{3}\biggl( &\text{u}(\frac{\textbf{W}}{h}-t_1)(w_1-w_0) \\
+ &\text{u}(\frac{\textbf{W}}{h}-t_2)(w_2-w_1) \\
+ &\text{u}(\frac{\textbf{W}}{h}-t_3)(w_3-w_2) \biggr)
\end{split}
\end{equation}
\normalsize
\vspace{-0.3cm}

The procedure is designed upon Eqn. \ref{eqn:quant} with several modifications. The value range is shifted and scaled from $[0, 1]$ to $[\sigma(\beta)\text{min}(\textbf{W}), \sigma(\gamma)\text{max}(\textbf{W})]$. The dequantization grid $w_i$ is defined in Eqn. \ref{eqn:dequantgrid}; the first and last values are the minimum and maximum values of the normalized weight and the middle values are set to the center of the two consecutive transition points.
\vspace{-0.2cm}
\begin{equation}\label{eqn:dequantgrid}
w_i=\begin{cases}
    0, & \text{if } i=0 \\
    \frac{t_i+t_{i+1}}{2}, & \text{if } 0 < i < 3 \\
    1, & \text{if } i=3
\end{cases}
\end{equation}

The dequantization LUT for a quantization unit (a weight group of size 128 in this paper's experiments) can be pre-computed without any runtime overhead as follows:
\begin{equation}\label{eqn:deqgrid}
\hat{\textbf{W}} = \{\hat{\text{W}}_0, \hat{\text{W}}_1, \hat{\text{W}}_2, \hat{\text{W}}_3\}
\end{equation}
where $\hat{\text{W}}_i=\sigma(\beta)\text{min}(\textbf{W}) + h \cdot w_j$.

\subsection{An NF3 Variant of LDP}
We apply not only 2-bit weight quantization but also 3-bit quantization using the asymmetric NF format of AFPQ~\cite{afpq} where separate scale values are computed for the negative and positive weights ($s_{neg}=\text{max}(|\textbf{W}_{neg}|)$, $s_{pos}=\text{max}(\textbf{W}_{pos})$). Although shown effective, such NF3 quantizer can lead to suboptimal performance when the distribution is not zero-centered. Therefore, we make a further improvement by applying the proposed LDP to this situation.


The idea is to employ the same learnable clipping parameters ($\beta$, $\gamma$) to obtain the quantization range $h$ and one partitioning parameter $s_1$ to express the learnable center point as $c = \sigma(\beta)\text{min}(\textbf{W}) + h \cdot \sigma(s_1)$. Then, the two scale values are updated as follows:
\begin{equation}\label{eqn:nf3scales}
\small
\begin{split}
s_{neg} = |c-\sigma(\beta)\text{min}(\textbf{W})|, \\
s_{pos} = |\sigma(\gamma)\text{max}(\textbf{W}) - c|,
\end{split}
\end{equation}
\normalsize
and the quantization process is derived as follows:
\begin{equation}\label{eqn:nf3}
\textbf{W}_q = \begin{cases}
\lfloor \frac{\textbf{W}-c}{s_{pos}} \rceil, & \text{if } \textbf{W} > c \\
\lfloor \frac{\textbf{W}-c}{s_{neg}} \rceil, & \text{otherwise.}
\end{cases}
\end{equation}
The dequantization is done simply by multiplying the scales to $\textbf{W}_q$ and adding $c$.

%It requires additional training only for weight updates and zero points, unlike INT2. The quantization and dequantization processes follow the methods used in the standard NF3 format.


\subsection{W2A4 Look-up Table (LUT) Inference}
\label{sec:W2A4Kernel}

Designing the inference pipeline of non-uniform W2A4-quantized models poses a big challenge. First, efficient INT tensor cores cannot be utilized since accumulating the multiplication results in INT quantized space makes it impossible to dequantize the weights back to correct non-uniform real values in the LUT $\hat{\textbf{W}}$. Second, both weights and activations must undergo online dequantization to support dynamic quantization, which adds a large amount of computation overhead.

Therefore, we focus on designing GEMV kernel for LUT decoding predominantly bounded by memory bandwidth, which is ideal for featuring the advantage of extreme W2A4KV4 compression. We report our early exploration on GEMM design in Section \ref{sec:gemm}.

% SJ: 아래 문단은 Appendix로 보내자. 본문에 제시하는 내용이 아닌 다른 내용이 중간에 끼는 건 자연스럽지 않은 듯. 물론, 이런 내용도 의미는 있어서 appendix가 적당할 듯.
% ET: 보내고 정리했습니다.
%In our initial GEMM implementation, we attempted to leverage the asynchronous copy 
%%capabilities introduced with the NVIDIA Ampere architecture by concurrently performing 
%to perform dequantization and MMA operations while loading quantized weights and activations, which resulted in slower performance compared to half-precision kernels. We suggest two underlying reasons; 1) dequantization requires multiple iterations of shifting, masking, and casting to half-precision instruction, and these are typically expensive on the GPU, further deepening the compute-bound nature of the GEMM problem and 2) packing four quantized weights into a single UINT8 and two quantized activation elements into a single INT8 reduces the width of per-block global memory loads, thereby narrowing the chance for latency hiding. Therefore, we decided to leave the prefill acceleration as future work and instead focus on designing a GEMV kernel to accelerate decoding.

\paragraph{Kernel Arguments and Block Tiling}
We define the input channel dimension as C, the output channel dimension as H, and the number of groups per channel as N. The quantized activation tensor $\textbf{X}_q$ has a shape of 1 × C/2 and is INT8, with each element holding two INT4 activation elements. The activation scale $\text{S}$ is an FP16 scalar. The quantized weight tensor $\textbf{W}_q$ has a shape of H × C/4 and is INT8, with each element holding four UINT2 weights. The dequantization grid $\hat{\textbf{W}}$ has a shape of H × N$\cdot$4 and is FP16. The output activation $\textbf{O}$ is an FP16 tensor of shape 1 × H.

Each thread block consists of 128 threads (4 warps) and we only tile along the output dimension and define the tile size as BH. The reason we do not follow the traditional 2-dimensional tiling is that both the input tokens and weights are stored in row-major format and have sub-byte packing along the column direction, which makes it hard to efficiently use high-bandwidth memory that performs best when reading 128B data consecutively. Also, global loads with small transactions and repeated shared stores complicate the pipeline design for latency hiding and degrade overall performance.

%To hide latency, the block executes the computation of BH/2 output elements twice using four warps.

%For example, if BH is 4, then two warps compute one output element, and after warp-level sum reduction, the values from the two warps must be summed again. To implement this, we allocate a shared output buffer $\text{s}\textbf{O}$ with twice the size of the number of warps. 

\paragraph{Data Path and Latency Hiding Pipeline}\label{sec:gemv_datapath}
As demonstrated in Fig. \ref{fig:gemv}, we store the dequantized input activation $\text{s}\textbf{X}$ (1 × C, FP16), the quantized weight tile $\text{s}\textbf{W}_q$ (BH × C/4, INT8), the corresponding dequantization grid tile $\text{s}\hat{\textbf{W}}$ (BH × N$\cdot$4, FP16), and a shared output array $\text{s}\textbf{O}$ (1 × 8, FP16) in shared memory.

To make our kernel efficient via latency hiding, we design a pipelining strategy where a thread block handles a half of the output elements (BH/2) and iterates twice. At the beginning, an asynchronous copy of $\hat{\textbf{W}}$ and the first $\textbf{W}_q$ chunk (of size BH/2 × C/4) is issued using \texttt{cp.async} instruction (\textbf{1-1} in Fig. \ref{fig:gemv}). Simultaneously, $\textbf{X}_q$ is synchronously loaded from global memory and dequantized to be stored into $\text{s}\textbf{X}$ (\textbf{1-2}), overlapping activation dequantization latency with loading the first weight chunk.

Subsequently, while we bring in the second $\textbf{W}_q$ chunk using \texttt{cp.async} (\textbf{2-1}), we perform dequantization, inner product, and warp reduce on the first $\textbf{W}_q$ chunk at the same time (\textbf{2-2}), thereby hiding the second chunk loading latency with computation of the first chunk. Finally, the computation on the second chunk is performed (\textbf{3}) and the shared output array is reduced once more if necessary.

Additional details (e.g., dequantization implementation, shared output) not mentioned here are provided in Section \ref{sec:gemvdetail}. 

%In each iteration, a half-sized chunk of 

%Data loading, dequantization, and the inner product are sequentially dependent, but since data loading supports asynchronous instructions, we use pipelining to attempt latency hiding. The key idea is partitioning the $\textbf{W}_q$ tile into two chunks. While asynchronously loading the second chunk, we perform dequantization and inner product computation on the first chunk, thereby hiding the weight loading latency. Specifically, while we bring in $\hat{\textbf{W}}$ and the first $\textbf{W}_q$ chunk using \texttt{cp.async} instructions, we simultaneously read and dequantize the entire $\textbf{X}_q$ from global memory and store it into $\text{s}\textbf{X}$, effectively overlapping activation dequantization latency with weight loading. Subsequently, while we bring in the second $\textbf{W}_q$ chunk using \texttt{cp.async}, we perform dequantization and the inner product on the first $\textbf{W}_q$ chunk, thereby hiding the latency associated with loading the weights.
\begin{table*}[htbp]
    \centering
    \setlength{\tabcolsep}{5pt}
    \renewcommand{\arraystretch}{1.0}
    \resizebox{\textwidth}{!}{%
    {
    \begin{tabular}{cccc|ccc|ccc|ccc|ccc|ccc}
        \toprule[4pt]
       \raisebox{-4ex}{ \textbf{\#Bits \(\text{(W-A-KV)}\)}} & \multicolumn{3}{c}{ \raisebox{-2ex}{\textbf{Configuration}} }& \multicolumn{3}{c}{ \raisebox{-2ex}{\textbf{LLaMA-1 7B}}} & \multicolumn{3}{c} {\raisebox{-2ex}{\textbf{LLaMA-2 7B}}} & \multicolumn{3}{c}{ \raisebox{-2ex}{\textbf{LLaMA-3 8B}}} & \multicolumn{3}{c}{ \raisebox{-2ex}{\textbf{LLaMA-3.2 1B}}} & \multicolumn{3}{c}{ \raisebox{-2ex}{\textbf{LLaMA-3.2 3B}}}   \\
        \cmidrule(lr){2-4} \cmidrule(lr){5-7} \cmidrule(lr){8-10} \cmidrule(lr){11-13} \cmidrule(lr){14-16} \cmidrule(lr){17-19}
        & \textbf{Method} & \textbf{Rotation} & \textbf{LDP} & \textbf{MMLU} & \textbf{0-shot$^\dagger$} & \textbf{Wiki$^\downarrow$} & \textbf{MMLU} & \textbf{0-shot$^\dagger$} & \textbf{Wiki$^\downarrow$} & \textbf{MMLU} & \textbf{0-shot$^\dagger$} & \textbf{Wiki$^\downarrow$} & \textbf{MMLU} & \textbf{0-shot$^\dagger$} & \textbf{Wiki$^\downarrow$} &  \textbf{MMLU} & \textbf{0-shot$^\dagger$} & \textbf{Wiki$^\downarrow$}   \\
        \midrule
        \multirow{1}{*}{16-16-16} &  &  &  & 35.10 & 68.40 & 5.68 &46.45 & 61.67 & 5.47 & 68.40 & 72.93 & 6.10 & 32.20 & 58.90 & 9.74 &58.00 &65.30 & 7.82 \\
        \midrule
        \multirow{3}{*}{2-4-16} 
        & BitDistiller &  &  & 25.88 & 42.56 & 23.19 & 26.24 & 43.36 & 16.47 & 23.11 & 39.46&Inf &25.00&36.82 & Inf & 24.41 &37.89 &  Inf \\
        & Ours & \checkmark &  & 26.75 & 52.28 & 8.79 & 26.04 & \textbf{51.49} & 8.93 & 29.80 & 50.59 & 13.68 & 25.00 & 41.08 & 31.32 & 29.60 & 45.29 & 18.79    \\
        & Ours & \checkmark & \checkmark & \textbf{26.98} & \textbf{52.46} & \textbf{8.28} & \textbf{28.04} & 51.10 & \textbf{8.18} & \textbf{31.87} & \textbf{50.86} & \textbf{12.48} & \textbf{26.30} & \textbf{41.35} & \textbf{27.46} & \textbf{31.40} & \textbf{45.71} & \textbf{16.96 }  \\
        \midrule
        \multirow{3}{*}{2-4-4} 
        & BitDistiller &   &   & 24.45 & 43.08 & 19.98 & 26.59 & 44.93 & 17.40 & 23.29 & 39.75 & Inf & 24.66 & 37.55 & Inf & 24.62 & 37.26 & Inf   \\
        & Ours & \checkmark &   & 26.98 & 52.21 & 8.92 & 26.41 & 51.10 & 8.93 & 29.66 & 49.80 & 14.05 & 24.74 & 40.77 & 33.86 & \textbf{31.44} & 44.26 & 19.58   \\
        & Ours & \checkmark & \checkmark & \textbf{27.34} & \textbf{52.29} & \textbf{8.28} & \textbf{26.92} & \textbf{51.22} & \textbf{8.31} & \textbf{31.01} & \textbf{50.41} & \textbf{12.69} & \textbf{25.62} & \textbf{41.80} & \textbf{29.30} & 30.33 & \textbf{45.56} & \textbf{17.52}   \\
        \midrule
        \multirow{3}{*}{3-4-16} 
        & BitDistiller &   &   & 26.88 & 55.68 & 7.47 & 31.72 & 56.15 & 7.04 & 42.24 & 55.39 & 10.19 & 26.06 & 37.53 & Inf & 25.22 & 37.32 & Inf   \\
        & Ours & \checkmark &  & 28.70 & 58.52 & 6.44 & 34.30 & 59.28 & 6.25 & 54.16 & 61.06 & 7.92 & 26.45 & 47.88 & 13.75 & \textbf{47.34} & 55.66 & 9.82   \\
        & Ours & \checkmark & \checkmark & \textbf{29.46} & \textbf{59.39} & \textbf{6.39} & \textbf{37.33} & \textbf{59.74} & \textbf{6.23} & \textbf{55.33} & \textbf{61.53} & \textbf{7.80} & \textbf{27.77} & \textbf{48.18} & \textbf{13.68} & 47.31 & \textbf{55.87} & \textbf{9.74}   \\
        \midrule
        \multirow{3}{*}{3-4-4} 
        & BitDistiller &   &   & 27.04 & 56.05 & 7.54 & 30.19 & 55.51 & 7.15 & 40.70 & 56.35 & 10.46 & 25.48 & 38.75 & Inf & 25.91 & 37.27 & Inf    \\
        & Ours & \checkmark &   & 28.80 & 58.48 & 6.45 & 33.46 & 58.53 & 6.36 & 51.74 & 59.69 & 8.04 & 26.11 & 47.14 & 14.58 & 46.08 & 55.08 & 10.05   \\
        & Ours & \checkmark & \checkmark & \textbf{30.00} & \textbf{58.55} & \textbf{6.39} & \textbf{36.07} & \textbf{59.27} & \textbf{6.33} & \textbf{52.55} & \textbf{61.11} & \textbf{7.95} & \textbf{26.54} & \textbf{47.71} & \textbf{14.44} & \textbf{46.40} & \textbf{55.12} & \textbf{9.99}   \\
        \bottomrule[4pt]
    \end{tabular}
    }
    }
    \caption{Comparison of the perplexity score on WikiText2, MMLU (5s), and 0-shot$^\dagger$. 0-shot$^\dagger$ is average score of 4 Zero-shot Common Sense Reasoning tasks. We show the perplexity results
>100 by Inf. Full results of Zero-shot tasks are in the Appendix. }
\vspace{-0.1cm}
\label{tab:main_result}
\end{table*}

% \subsection{Key Observation and Analysis}

% \subsubsection{Hadamard Transform Hinders Weight Quantization}
% Table X shows that applying rotation as implemented in QuaRot (which uses Hadamard matrices for all rotation operators) worsens weight-only quantization performance when the bit width is extremely low.
% To explain this phenomenon, we focus on the Hadamard transform's nature that it is equivalent to a multidimensional discrete Fourier transform~\cite{hadamardeqdft} and Hadamard matrix is a case among three different forms of Walsh matrix~\cite{walsh}; sequency-, dyadic-, and natural (Hadamard)-ordering.

% Fig. X (a) illustrates that after rotation with the Walsh matrix, the model weights are transformed into frequency components determined by the number of zero crossings in each row of the Walsh matrix. Now we compare the two cases in Fig. X (b): natural vs. sequency. In the former, 

% Hadamard case에서는 inter-group frequency variance가 크게, sequency case에서는 적게 매핑된다. 이 때 original model weight matrix는 frequency 성분들이 고르게 분포하고 있기 때문에, 가까운 frequency component끼리는 유사도가 높고, 먼 component끼리는 낮다.
% 즉, Hadamard transform을 수행할 경우 

% \subsubsection{Saliency Analysis}

% \subsection{Rotation as Pre-conditioning}

% \subsection{Learnable Non-uniform Quantizer}

% \paragraph{Trainable Clipping and Partitioning}

% \paragraph{Saliency-based Initialization}


\section{Experiments}
\label{sec:experiments}

\subsection{Experimental Settings}
\paragraph{Models and Tasks}
We evaluate RCP on LLaMA-1~\cite{llama} 7B, LLaMA-2~\cite{llama2} 7B, LLaMA-3~\cite{llama3modelcard}(1B, 3B, 8B). Our evaluation of RCP was carried out on PIQA~\cite{piqa}, HellaSwag~\cite{hellaswag}, WinoGrande~\cite{winogrande}, ARC-c~\cite{arc} and MMLU~\cite{mmlu}. We use LLM-Humaneval-Benchmarks~\cite{humaneval} and GSM8K~\cite{gsm8k} for reasoning tasks evaluation. We also report the perplexity score on WikiText2~\cite{wiki2} for our evaluation.
\paragraph{Training Data}
For a fair comparison with our baseline, we use the instruction-tuning data from Alpaca~\cite{alpaca} and the training set of WikiText2 for general language tasks.
For understanding and generating code, we use Evol-Instruct-Code\footnote{https://github.com/nickrosh/evol-teacher}. For math reasoning we use MetaMathQA~\cite{metamathQA}.

\paragraph{Experiment Configuration}
We compare our proposed RCP with the state-of-the-art QAT method, BitDistiller~\cite{BitDistiller}.
We employ a symmetric uniform quantizer for activations and an asymmetric uniform quantizer with a group size of 128 for KV-cache. Clipping ratio is set to 0.9 and 0.95 for activations and KV-cache, respectively. We set the weight learning rate to 8e-7 for W$l$A4 and 1e-6 for W$l$A4KV4, while the learning rate for LWC and LDP was set to 1e-5. All training was conducted for 8 epochs, using a batch size of 4 or 8. We set the training sequence length to 1k and the evaluation sequence length to 2k.

% ET: 아래 문장 필요 없을듯? [Checked]
%For a fair comparison, we re-implemented the baseline BitDistiller method by applying the INT quantizer for W2 and both the INT and NF quantizers for W3. Note that the results for W3 below are obtained by applying the NF quantizer to both activations and the KV-cache.

\subsection{Results}

\paragraph{Language Modeling Tasks}
The results are summarized in Table \ref{tab:main_result}. From the perspective of general language tasks, our method demonstrates the ability to quantize activations and KV-cache under the W2 settings to 4-bit, which was previously unattainable using existing QAT methods. The application of rotation effectively addresses the outlier issues, a common bottleneck in quantization, enabling stable performance even in extremely low-bit quantization scenarios. Furthermore, the addition of LDP not only improves performance on general language tasks but also enhances the accuracy of zero/few shot tasks, which were not adequately addressed by rotation alone. In case of LLaMA-2 7B W2A4KV4, a performance degradation is observed when using rotation only compared to the baseline. However, by incorporating LDP, consistent performance improvements were achieved. 
% SJ: 아래 문장 다듬기 또는 삭제 필요. 7.57 숫자 표에서 찾기 힘들고. 문장 뜻도 분명하지 않은 것 같아.
% Additionally, the perplexity of Llama-2 7B with BitDistiller W2-only quantization is observed to be 7.57, demonstrating that compressing both activations and KV-cache by a factor of four incurs a negligible perplexity drop.


\paragraph{Reasoning Tasks}
\begin{table}[htbp]
    \centering
    \setlength{\tabcolsep}{2pt}
    \renewcommand{\arraystretch}{0.8}
    \resizebox{\columnwidth}{!}{%
    {
    \begin{tabular}{cccc|c|c}
        \toprule[2pt]
       \raisebox{-1ex}{ \textbf{\#Bits \(\text{(W-A-KV)}\)}} & \multicolumn{3}{c}{ \raisebox{-0ex}{\textbf{Configuration}} }& \multicolumn{1}{c}{ \raisebox{-0ex}{\textbf{WizardCoder 7B}}} & \multicolumn{1}{c} {\raisebox{-0ex}{\textbf{MetaMath 7B}}} \\
        \cmidrule(lr){2-6} 
        & \textbf{Method} & \textbf{Rotation} & \textbf{LDP} & \textbf{HumanEval} & \textbf{GSM8K}   \\
        \midrule
        \multirow{1}{*}{16-16-16} &  &  &  & 54.88 & 66.41  \\
        \midrule
        \multirow{3}{*}{2-4-16} 
        & BitDistiller &  &  & 2.43 & 0.0 \\
        & Ours & \checkmark &  & 14.63 & 1.25    \\
        & Ours & \checkmark & \checkmark & \textbf{27.44} & \textbf{41.64}   \\
        \midrule
        \multirow{3}{*}{2-4-4} 
        & BitDistiller &   &   & 3.50 & 5.39    \\
        & Ours & \checkmark &   & 6.09 & 0.16   \\
        & Ours & \checkmark & \checkmark & \textbf{23.20} & \textbf{40.16}   \\
        \midrule
        \multirow{3}{*}{3-4-16} 
        & BitDistiller &   &   & 0.0 & 0.0  \\
        & Ours & \checkmark &  & 39.02 & 0.0    \\
        & Ours & \checkmark & \checkmark & \textbf{40.85} & \textbf{54.69}   \\
        \midrule
        \multirow{3}{*}{3-4-4} 
        & BitDistiller &   &   & 0.0 & 0.0    \\
        & Ours & \checkmark &   & 41.46 & 0.0    \\
        & Ours & \checkmark & \checkmark & \textbf{43.29} & \textbf{52.73}  \\
        \bottomrule[2pt]
    \end{tabular}
    }
    }
    \caption{Reasoning task results of RCP on domain-specific LLMs.}
    \label{tab:reason}
    \vspace{-0.1cm}
\end{table}

% SJ: 표 2에서 W3 BitDistiller는 score 0 인데 W2에서는 오히려 2.43 처럼 조금이나마 동작하는데 맞나? [checked]

The results of the reasoning tasks are summarized in Table \ref{tab:reason}. We evaluate reasoning capabilities in the domains of coding and mathematics. 

For the coding domain-specific model, WizardCoder~\cite{luo2023wizardcoder}, BitDistiller failed to offer the functional quantized models in both W3 and W2 settings. 
In our method, applying rotation alone was not effective in W2 settings and recovered some output quality in W3 settings.
By incorporating LDP, we achieved up to a threefold improvement in performance, with accuracy increasing from 6.09\% to 23.20\% under the W2A4KV4 configuration.
% SJL 바로 위 문장에서 3배가 어떤 값에서 어떤 값으로의 개선인지 표에 있는 값들을 제시해 주기 바래. [Checked]
As shown in Fig. \ref{fig:coding}  with the application of LDP, we were able to produce logically correct code outputs and eliminate repetition of meaningless code generation.

For the mathematical reasoning model, MetaMath~\cite{metamathQA}, the baseline BitDistiller and ours without LDP failed to offer functional quantized models while ours with LDP could produce working quantized models. These results highlight the critical role of LDP in enabling proper task performance for reasoning models under extreme low-bit quantization. The output comparison for this task is summarized in Fig. \ref{fig:gen_math}.
\paragraph{Inference}
\begin{table}[h!]
\resizebox{\columnwidth}{!}{%
\centering
\renewcommand{\arraystretch}{0.9} % 행 높이 조절
% \setlength{\tabcolsep}{8pt} % 열 간격 조절
\begin{tabular}{l|ccc}
\toprule[2pt]
\textbf{Layer Size} & \textbf{(2048, 2048)} & \textbf{(3072, 3072)} & \textbf{(4096, 4096)} \\ \specialrule{0.5pt}{0pt}{0pt}
\textbf{FP16} & 0.042 & 0.047 & 0.051 \\
\textbf{QuaRot} & 0.077 & 0.057 & 0.078 \\
\textbf{QuaRot+FP16Had} & 0.158 & 0.210 & 0.159 \\
\textbf{QuaRot+FP32Had} & 0.194 & 0.238 & 0.191 \\
\textbf{RCP} & 0.028 & 0.03 & 0.040 \\
\textbf{RCP+FP16Had} & 0.114 & 0.167 & 0.110 \\
\textbf{RCP+FP32Had} & 0.136 & 0.204 & 0.148 \\
\bottomrule[2pt]
\end{tabular}
}

\caption{GEMV latency without activation quantization overhead. The layer size is composed as (input channel, output channel). All latency numbers are in milliseconds. Full results are in the Appendix.}
    \vspace{-0.2cm}
\label{tab:latency}
\end{table}

\begin{table}[h!]
\resizebox{\columnwidth}{!}{%
\centering
\renewcommand{\arraystretch}{1.2} % 행 높이 조절
\begin{tabular}{c|cccc}
\toprule[2pt]
\textbf{} & \textbf{3.2-1B} & \textbf{3.2-3B} & \textbf{1.2-7B} & \textbf{3-8B} \\ \specialrule{2pt}{0pt}{0pt}
\textbf{FP16} & 2.47GB & 6.43GB & 13.48GB & 16.06GB \\ \hline
\textbf{RCP W3} & 1.46GB (1.69x) & 2.77GB (2.32x) & 3.26GB (4.14x) & 5.05GB (3.18x) \\
\textbf{RCP W2} & 1.35GB (1.82x) & 2.46GB (2.62x) & 2.55GB (5.29x) & 4.28GB (3.75x) \\
\bottomrule[2pt]
\end{tabular}}
\caption{Memory footprint comparison for different weight precisions. Note that 1.2-7B refers to LLaMA-1 and LLaMA-2.}
\label{tab:memory}
    \vspace{-0.1cm}
\end{table}
Table \ref{tab:latency} and \ref{tab:memory} present the results for GEMV in terms of latency and memory comsumption. The latency of GEMV, excluding the activation quantization overhead, is faster compared to FP16 and QuaRot~\cite{quarot}. This improvement can be attributed to the lower bit precision, which enhances computational efficiency. Table \ref{tab:memory} measures the peak memory footprint for W2A4 and W3A4. For W2A4, a significant reduction on 5.29x in memory footprint was achieved compared to FP16. Note that in the LLaMA-3.2 series, it is necessary to separate the embedding table and head modules to satisfy the invariance arising from their tying. Furthermore, as the size of the embedding table has increased compared to previous models, the compression ratio has decreased accordingly.

% \begin{table*}[]
% \centering
% \resizebox{0.9\textwidth}{!}{%
% \begin{tabular}{@{}cc|c|c|cccc|c@{}}
% \toprule
% \multicolumn{2}{c|}{\textbf{LLaMA-2-7B}}                                                                               & PPL                                   & MMLU (5s)                              & PIQA                                   & Hella.                        & Wino.                                  & ARC-c                                  & Avg.                                   \\ \midrule \midrule
% \multicolumn{2}{c|}{BF16}                                                                                              & 5.47                                  & 46.45                                  & 77.86                                  & 57.14                         & 68.35                                  & 43.34                                  & 58.63                                  \\ \midrule
% W2 g128                                                                        & BD                                    & 8.08                                  & 29.25                                  & 73.61                                  & 48.70                         & 61.09                                  & 33.27                                  & 49.18                                  \\ \midrule
%                                                                                & BD                                    & 17.40                                 & 26.59                                  & 62.70                                  & 37.18                         & 53.91                                  & 25.93                                  & 41.26                                  \\
%                                                                                & BD+Rotation                           & 8.93                                  & 26.41                                  & 69.53                                  & \textbf{45.67}                & 59.35                                  & 29.86                                  & 46.16                                  \\
% \multirow{-3}{*}{\begin{tabular}[c]{@{}c@{}}W2A4KV4\\ W, KV g128\end{tabular}} & \cellcolor[HTML]{C0C0C0}\textbf{Ours} & \cellcolor[HTML]{C0C0C0}\textbf{8.39} & \cellcolor[HTML]{C0C0C0}\textbf{26.46} & \cellcolor[HTML]{C0C0C0}\textbf{69.91} & \cellcolor[HTML]{C0C0C0}44.58 & \cellcolor[HTML]{C0C0C0}\textbf{59.70} & \cellcolor[HTML]{C0C0C0}\textbf{30.69} & \cellcolor[HTML]{C0C0C0}\textbf{46.27} \\ \bottomrule
% \end{tabular}%
% }
% \end{table*}

% \begin{table*}[]
% \centering
% \resizebox{0.6\textwidth}{!}{%
% \begin{tabular}{@{}cc|c|c@{}}
% \toprule
% \multicolumn{2}{c|}{\textbf{Domain-specific LLMs}}                                                                     & \begin{tabular}[c]{@{}c@{}}HumanEval @\\ WizardCoder 7B\end{tabular} & \begin{tabular}[c]{@{}c@{}}GSM8K @\\ MetaMath 7B\end{tabular} \\ \midrule \midrule
% \multicolumn{2}{c|}{BF16}                                                                                              & 54.88                                                                & 66.41                                                         \\ \midrule
% W2 g128                                                                        & BD                                    & 36.59                                                                & 51.02                                                         \\ \midrule
%                                                                                & BD                                    & 3.50                                                                 & 5.39                                                          \\
%                                                                                & BD+Rotation                           & 6.09                                                                 & 1.10                                                          \\
% \multirow{-3}{*}{\begin{tabular}[c]{@{}c@{}}W2A4KV4\\ W, KV g128\end{tabular}} & \cellcolor[HTML]{C0C0C0}\textbf{Ours} & \cellcolor[HTML]{C0C0C0}\textbf{23.20}                               & \cellcolor[HTML]{C0C0C0}\textbf{40.16}                        \\ \bottomrule
% \end{tabular}%
% }
% \end{table*}

\subsection{Ablation Studies}
\begin{table}[htbp]
\centering
\renewcommand{\arraystretch}{0.8}
\resizebox{0.8\columnwidth}{!}{ % 현재 컬럼의 너비에 맞춰 조절
{\small
\begin{tabular}{c|c|c|c|c}
\toprule[1pt]
\textbf{\#Bits} & \textbf{Rotation} & \textbf{LWC} & \textbf{LDP} & \textbf{PPL$^\downarrow$} \\ \hline
\multirow{4}{*}{2-4-4} &  &  &  & 17.40 \\ 
                       & \checkmark &  &  & 8.93 \\ 
                       & \checkmark & \checkmark & \ & 10.59 \\ 
                       & \checkmark & \checkmark & \checkmark & 8.31 \\ \bottomrule[1pt]
\end{tabular}
}
}
\captionsetup{justification=justified, singlelinecheck=false}
\caption{Ablation study on the impact of each component of our proposed RCP on performance for LLaMA-2 7B.}
\vspace{-0.5cm}
\label{tab:ablation}
\end{table}

\paragraph{Impact of RCP Components}
As shown in Table \ref{tab:ablation}, we conducted an ablation study to analyze the impact of removing each component of RCP on model performance. In 4-bit activation quantization, addressing the outliers in activations was crucial, and this was effectively resolved using rotation, which led to the largest performance gain compared to the baseline. This demonstrates that rotation is a viable solution when quantizing activations to low bit-width.

However, we found that the narrow weight distribution caused by rotation hindered successful training of LWC. Specifically, when examining the training process with rotation applied during LWC training, the training loss curve exhibited instability. The combination of low bit-width quantization challenges and the difficulty in finding an optimal LWC required training stabilization, which was achieved by LDP. LDP reduced PPL from 10.59 to 8.31, demonstrating a clear advantage.
\paragraph{Impact of Rotation Configuration}
\begin{table}[htbp]
\centering
\renewcommand{\arraystretch}{0.6}
\resizebox{0.5\columnwidth}{!}{ 
\begin{tabular}{lcc}
\toprule[1pt]
\textbf{W2A4KV4} & \textbf{PPL$^\downarrow$}   \\ \midrule
RCP       & \textbf{8.31}         \\ \midrule
-$R_3$ & 8.48                     \\
-[$R_2$,$R_3$]   & 8.83                      \\
-[$R_3$,$R_4$]     & 12.24                \\ 
-[$R_2$,$R_3$,$R_4$]     & 12.76                 \\ 
-[$R_1$,$R_2$,$R_3$,$R_4$]     & 25.05                \\ 

\bottomrule[1pt]
\end{tabular}
}
\captionsetup{
    justification=justified,
    singlelinecheck=false
}
\caption{Ablation study on the impact of rotation configuration for LLaMA-2 7B.}
\label{tab:ablation_rotation}
\end{table}
Since the rotation requires additional processes before and after inference, we investigated the performance trend by incrementally adding rotation matrices ($R_1$,$R_2$,$R_3$,$R_4$) to different components to find an appropriate balance between accuracy and overhead. The results are presented in Table \ref{tab:ablation_rotation}. 
%Note that the results were obtained by using LWC and LDP as the base, with rotation added sequantially.  
The table demonstrates that the impact of the rotation was most significant with $R_1$ and $R_4$. Especially, $R_1$, which applies rotation matrix to the input weight and input activation of all modules thereby having the largest impact on quantization performance. Additionally, our analysis revealed that in LLaMA-2 7B, the input to the down projection layer (of the MLP) exhibited a significant number of outliers, which was effectively addressed through $R_4$ online rotation to activation. 

% SJ: 우선 현재 방법에서 rotation matrix는 Hadamard 사용. 즉, 고정됨. 아래 factorized rotation 내용 보면, 현재 돌아가는 버전에서는 fused rotation 경우, 학습 시작할 때 rotation matrix를 weight에 1번만 곱하고 이후 fused weight을 학습에서 update한다. 그런데, rotation 적용하면서 fuse 되지 않은 weight을 update 하는 경우는 학습 결과 별로 좋지 않다는 얘기지?
% 그렇다면, 우리와 update하는 대상은 다르지만 fusion을 하지 않은 weight 사용하는 면에서는 같은 SpinQuant에서는 fusion 안해도 학습이 되는데 왜 우리 경우는 학습이 잘 안되는걸까? [checked->내용보충(efficientQAT와의 연계)+ GEMV inference 분량 공간 확보를 위해 내용보충하여 Appendix로 이동했습니다.  ] 

\section{Conclusion}
\label{sec:conclusion}
RCP enables weights to be quantized to extreme low-bit precision through learnable non-uniform quantization while harmonizing with rotation to optimize both activations and KV-cache to 4-bit. RCP has achieved the first W2A4KV4 configuration and implemented optimized kernels for inference, facilitating LLM serving even in resource-constrained environments.
\section*{Limitations}

Although our proposed RCP first enables challenging W2A4KV4 quantization of commonly used LLM models, we report key limitations of our work.

First, the online rotation operators ($R_2$ through $R_4$) inevitably introduce additional latency for training and evaluation. Custom CUDA kernels or FlashAttention3~\cite{flashattention3} can minimize such speed-down, however, it might not be a viable option for many edge application scenarios where no hardware support for fast Hadamard transform is available.

Second, RCP requires heavier hyperparameter tuning than BitDistiller since rotation tends to make the model weights more sensitive to the choice of learning rate. This can be prohibitive when a user is under a strict budget limit.

In future work, we could explore applying an optimized rotation matrix that achieves comparable performance to Cayley-optimized rotation matrices used in SpinQuant~\cite{spinquant} while maintaining similar computational costs to the Random Hadamard rotation matrices employed in QuaRot~\cite{quarot}. \\


% Additionally, we can investigate methods to minimize accuracy degradation while using fused rotations to reduce the computational overhead of online rotations, which are currently used in various papers leveraging rotation transformations.


% Bibliography entries for the entire Anthology, followed by custom entries
%\bibliography{anthology,custom}
% Custom bibliography entries only
\bibliography{latex/acl_latex}

\appendix

\section{Appendix}
\label{sec:appendix}

\subsection{Related Works}
\label{sec:related}
%\subsection{Quantization for LLMs}
\paragraph{PTQ and QAT}
GPTQ~\cite{gptq} introduced an accurate post-training quantization (PTQ) method based on approximate second-order information that enables weight-only quantization down to 3-4 bits through block-wise reconstruction.
SmoothQuant~\cite{smoothquant} proposed smoothing activation outliers by offline migrating quantization difficulty from activations to weights through equivalent transformation, enabling accurate 8-bit weight-activation quantization.
AWQ~\cite{awq} built upon SmoothQuant's equivalent transformation concept but introduced activation-aware channel-wise scaling to protect salient weights during weight-only quantization. 
OmniQuant~\cite{omniquant} enhanced quantization by introducing learnable weight clipping and equivalent transformation parameters that are jointly optimized through block-wise reconstruction.\\
LLM-QAT~\cite{llmqat} was the first to explore quantization-aware training (QAT) for LLMs using data-free knowledge distillation from the full-precision model to guide low-bit quantization.
BitDistiller~\cite{BitDistiller} improved upon LLM-QAT by introducing a self-distillation framework with confidence-aware KL divergence to enable sub-4-bit quantization while maintaining efficiency.
EfficientQAT~\cite{efficientqat} made QAT more practical by introducing block-wise training of all parameters followed by end-to-end training of quantization parameters.
\paragraph{Rotation}
QuaRot~\cite{quarot} introduced a rotation-based approach using Hadamard transformations to eliminate outliers in activations and KV-cache, enabling end-to-end 4-bit quantization including weights, activations and KV-cache.
SpinQuant~\cite{spinquant} enhanced this rotation-based approach by learning optimal rotation matrices instead of using random ones. 
\paragraph{Non-uniform Quantization}
PACT~\cite{pact} introduced a learnable clipping parameter for activation quantization during training to help preserve model accuracy. SqueezeLLM~\cite{squeezellm} took a different direction by focusing on identifying and extracting outlier values into a sparse format while quantizing the remaining dense values. NU2U~\cite{nu2u} proposed learning flexible non-uniform input thresholds while maintaining uniform output levels to balance quantization accuracy with hardware efficiency.
\paragraph{Serving Optimization}
Atom~\cite{atom} first introduced W4A4 quantization for LLM serving but faced performance challenges from dequantization overhead. QServe~\cite{qserve} addressed the challenges by introducing W4A8KV4 quantization with progressive group quantization FLUTE~\cite{flute} focused on developing efficient GPU kernels for flexible lookup table-based quantization methods that can support arbitrary bit widths including 3-bit and 4-bit quantization.

\subsection{Reasoning Task Example: HumanEval}
We evaluate the capability of the WizardCoder 7B model to generate solutions for coding problems. The results are presented in Fig. \ref{fig:coding}. The orange box in Fig. \ref{fig:coding} represent the model output after applying rotation and quantizing the weights to W2A4KV4 using a uniform asymmetric quantizer. Under uniform quantization, it is evident that the model fails to perform logical generation tasks even applying rotation; it merely produces the structural template of code without generating functionality correct code. In contrast, the green box shows the results when the weights are quantized to W2A4KV4 using LDP. Unlike the uniform quantizer, the LDP approach yields code that not only adheres faithfully to the given instructions and generates a functionality correct algorithm, but also provides detailed explanatory comments. While perplexity on standard language modeling tasks did not reveal significant differences between the two cases, these findings suggest that LDP plays a crucial role in enabling logical reasoning tasks under extreme low-bit quantization.

\subsection{Implementation Details}
All model parameters are in BF16 format throughout training and evaluation since we observe overflow in the hidden activation of the last two FFNs on several models set to FP16. 

In existing rotation-based PTQ methods~\cite{quarot,spinquant}, rotations are done in FP32 to avoid precision issues. However, this leads to computational overhead due to a large number of typecasting. When fusing rotations to model weights, they are temporarily promoted to FP32, multiplied by an appropriate rotation matrix, and then demoted back to their original precision. For online rotations ($R_2$, $R_3$, and $R_4$), all tensors are processed in BF16.

\subsection{More Ablation Studies}
\paragraph{Factorized Rotation}
\begin{table}[htbp]
\centering
\resizebox{0.9\columnwidth}{!}{ 
\renewcommand{\arraystretch}{1.0} % 행 높이 조절
\setlength{\tabcolsep}{8pt} % 열 간격 조절
\begin{tabular}{c|c|c|c|c}
\toprule[1pt]
\textbf{\#Bits} & \textbf{Factorized} & \textbf{Batch} & \textbf{Epoch}&\textbf{PPL$^\downarrow$}\\ \hline
\multirow{2}{*}{W2} &                  & 8      &    8    & 7.6          \\ 
\textbf{}   & \checkmark                 & 1     &    64     & 12.5         \\ \bottomrule[1pt]
\end{tabular}
}
\caption{Comparison of factorized configurations.}
\label{tab:factorized}
\end{table}
In our algorithm, rotation serves as a pre-conditioning tool for reducing outliers in activation and KV-cache. All rotations except the matrices that should be applied online ($R_3$ and $R_4$) are fused into the corresponding model weight at the beginning of the QAT process. This means their orthogonality is not guaranteed during backpropagation steps with AdamW optimizer.

We investigate the impact of preserving the orthogonality of the rotations by modifying the LLaMA-2 model implementation to apply all rotation operators online while freezing the rotation matrices.
Table \ref{tab:factorized} presents the results. Applying factorized rotation prevents the fusion of the rotation matrix into the weight tensor, resulting in an increase in the number of intermediate tensors (rotation matrix and intermediate activation), which significantly raises VRAM requirements. For instance, applying only $R_1$ needs to reduce the training batch size from 8 to 1. Under the condition of maintaining an equal total number of tokens processed by the model, we compared the performance of W2A16KV16 with only $R_1$ applied. The perplexity of BitDistiller with $R_1$ fused was 7.6, whereas applying QAT with factorized rotation resulted in a PPL of 12.5. This indicates that performing weight updates through QAT while preserving $R1$ orthogonality hinders QAT optimization. This is because the factorization constrains the weight updates to a restricted space defined by the factorized condition, requiring the backpropagation process to maintain within this space. This limitation reduces the flexibility of optimization, making it challenging to efficiently adjust the weights. Consequently, this leads to suboptimal training dynamics and ultimately results in degraded model performance. Furthermore, extending factorization to $R_2$ and $R_4$ would lead to an even greater increase in VRAM usage. In contrast, training fused weight effectively alters only the distribution and is analogous to standard LLM training, which is well-known to perform effectively.
In summary, given that resource consumption increases while performance degrades, we have decided not to explicitly preserve orthogonality and instead allow the algorithm to handle this aspect.
\paragraph{Layerwise vs. End-to-end QAT}
Recent work introduced layerwise QAT~\cite{efficientqat}, which updates one layer at a time while freezing others, allowing training on a single GPU. We extended this approach by applying rotation but observed significant performance degradation. The main issue stemmed from fuse rotation matrices in the weights; layerwise updates disrupted orthogonality, preventing the activation space from restoring its original space, leading to cumulative errors and reduced accuracy. In contrast, end-to-end methods like BitDistiller naturally mitigate this issue during updates. While factorized rotation could help, its high GPU memory requirements for holding rotation matrices and intermediate tensors on GPU memory offsets the advantage. Despite these challenges, exploring single GPU training using rotation matrix remains a promising direction for future work.
%Special optimizers such as Cayley-SGD or Cayley-Adam~\cite{cayley} can preserve




\subsection{GEMM Kernel Design for Non-uniform W2A4 Quantization} \label{sec:gemm}
In our initial GEMM implementation, we attempted to leverage the asynchronous copy 
%%capabilities introduced with the NVIDIA Ampere architecture by concurrently performing 
to perform dequantization and MMA operations while loading quantized weights and activations, which resulted in slower performance compared to half-precision PyTorch kernel (approx. 480$\mu$s versus 330$\mu$s on a single (4,096 $\times$ 4,096) linear layer with 2,048 tokens as input). We suggest two underlying reasons; 1) dequantization requires multiple iterations of shifting, masking, and casting to half-precision instruction, and these are typically expensive on the GPU, further deepening the compute-bound nature of the GEMM problem and 2) packing four quantized weights into a single UINT8 and two quantized activation elements into a single INT8 reduces the width of per-block global memory loads, thereby narrowing the chance for latency hiding. Therefore, we decided to leave the prefill acceleration as future work and instead focus on designing a GEMV kernel to accelerate decoding.

\subsection{Details and More Results on GEMV} \label{sec:gemvdetail}

\begin{figure}[h!]
    \centering
    % 첫 번째 그림
    \begin{subfigure}[t]{\linewidth} % 전체 너비 사용
        \centering
        \includegraphics[width=0.9\linewidth]{latex/figs/x_deq.pdf} % 그림 크기 조정
        \caption{Dequantization process of two INT4 activations packed in INT8.}
        \label{fig:x_deq}
    \end{subfigure}
    
    \vspace{1em} % 두 그림 사이 간격 추가
    
    % 두 번째 그림
    \begin{subfigure}[t]{\linewidth} % 전체 너비 사용
        \centering
        \includegraphics[width=1.0\linewidth]{latex/figs/w_deq.pdf} % 그림 크기 조정
        \caption{Dequantization process of 4 UINT2 weights packed in UINT8.}
        \label{fig:w_deq}
    \end{subfigure}
    
    \caption{Online dequantization of INT4 activations and UINT2 weights.}
    \label{fig:deq}
\end{figure}

\paragraph{Online Dequantization and Vectorization}
Fig. \ref{fig:deq} illustrates how the activations and weights are dequantized in our GEMV kernel. For activations, there are two INT4 elements ($\text{X}_{hi}, \text{X}_{low}$) in a packed INT8 $\text{X}_q$. For $\text{X}_{hi}$, $\text{X}_q$ is copied to an INT8 register, and the register is right-shifted by 4 bits with sign-filling. For $\text{X}_{low}$, $\text{X}_q$ is also copied to an INT8 register, which is left-shifted by 4 bits first to put the sign bit of $\text{X}_{low}$ to the MSB and then right-shifted by 4 bits with sign filling. This process is shown in Fig. \ref{fig:x_deq}.

For weights, there are four UINT2 elements ($\text{W}_{q0}, \text{W}_{q1}, \text{W}_{q2}, \text{W}_{q3}$) in a packed UINT8 $\text{W}_q$. $\text{W}_q$ is copied to 4 UINT8 registers (for each UINT2 element) that are used as indices to look up the LUT $\hat{\textbf{W}}$. For $\text{W}_{q0}$, the register is right-shifted by 6 bits. For $\text{W}_{q1}$, the register is right-shifted by 4 bits, and a logical AND operation with a bit mask $\texttt{0x03}$ is applied to select only two LSBs. For $\text{W}_{q2}$, the register is right-shifted by 2 bits and also performs logical AND with a bit mask $\texttt{0x03}$. For $\text{W}_{q3}$, the register only does a logical AND with a bit mask $\texttt{0x03}$.

The unit dequantization operations can be vectorized to increase memory throughput so that each thread writes 16B of data to shared memory. For activations, 4 $\text{X}_q$s are loaded from global memory at once by type casting via $\texttt{reinterpret\_cast<char4 *>}$, which produces 8 FP16 dequantized activations to be written in $\text{s}\textbf{X}$. The dequantization is performed the same on each $\text{X}_q$ in a $\texttt{char4}$ struct. For weights, 2 $\text{W}_q$s are loaded from memory via $\texttt{reinterpret\_cast<uint16\_t *>}$. Unlike the activation case, the right-shift and logical AND operation can be naturally iterated 8 times to generate 8 FP16 dequantized weights that are directly multiplied to the corresponding activation from $\text{s}\textbf{X}$.

\paragraph{Shared Epilogue}
As mentioned in Section \ref{sec:gemv_datapath}, a shared output can be necessary due to our chunking strategy. For example, if BH is 4, then two warps will compute one output element to process a weight chunk of size BH/2 × C/4, and after warp-level sum reduction, the reduced values from the two warps must be summed once again. To implement this, we allocate a shared output buffer $\text{s}\textbf{O}$ with twice the number of warps.

After the inner product stage for the first weight chunk, each thread in a block will have an FP32 accumulator with a shape of (4, 32). Applying the warp-shuffle primitive $\texttt{\_\_shfl\_xor\_sync}$ 5 times allows us to sum all accumulations to the first thread of each warp without any global nor shared memory access, producing 4 FP32 values to be cast to FP16 and stored in $\text{s}\textbf{O}[0:4]$. The first and the last two values are summed up as the first and the second output elements, respectively. Repeating the same process on the second weight chunk will produce the next 4 FP32 values for $\text{s}\textbf{O}[4:8]$ to compute the third and the fourth output elements accordingly.

\paragraph{Latency Benchmark}
Our GEMV kernel is fully written in CUDA 12.1 and compiled for Nvidia A100 SXM 40GB model. We build our benchmarking framework upon QuaRot's~\cite{quarot} implementation that provides proper PyTorch bindings and a basic activation quantizer that combines a max reduction function written in PyTorch and a symmetric INT quantizer with INT4 sub-byte data handler from CUTLASS\footnote{https://github.com/NVIDIA/cutlass}.

Since the reduction part is neither a specialized implementation nor compiler-optimized, a huge overhead induced by the QuaRot's activation quantizer is observed (about 100$\mu$s on average). Therefore in the main results, we assume that the symmetric quantization is natively supported by hardware and replace the quantizer with a dummy class that outputs random quantized activation and scale tensors. The results with the inefficient quantizer implementation are listed in Table \ref{tab:gemvQo:v} and \ref{tab:gemvQo:down} for value and down projection weight, respectively. We also report the latency values without activation overhead for the down projection weight in Table \ref{tab:gemvQx:down}.

% \subsection{Results of GEMV Latency} 

\begin{table*}[htbp]
    \centering
    \setlength{\tabcolsep}{5pt}
    \renewcommand{\arraystretch}{1.0}
    \resizebox{\textwidth}{!}{%
    {\normalsize
    \begin{tabular}{c cccccccc|c}
        \toprule[2pt]
         \raisebox{-4ex}{\textbf{Model}} & \raisebox{-4ex}{ \textbf{\#Bits \(\text{(W-A-KV)}\)}} & \multicolumn{3}{c}{ \raisebox{-2ex}{\textbf{Configuration}} } & \multicolumn{1}{c}{ \raisebox{-5ex}{\textbf{PIQA}}} & \multicolumn{1}{c}{ \raisebox{-5ex}{\textbf{Hella.}}} & \multicolumn{1}{c}{ \raisebox{-5ex}{\textbf{Wino.}}} & \multicolumn{1}{c}{ \raisebox{-5ex}{\textbf{ARC-c}}} & \multicolumn{1}{c}{ \raisebox{-5ex}{\textbf{Avg.}}}   \\
        \cmidrule(lr){3-5} 
        & & \textbf{Method} & \textbf{Rotation} & \textbf{LDP}  \\
        \midrule
        \multirow{13}{*}{1-7B} & \multirow{1}{*}{16-16-16} &  &  &  & 79.80 & 76.10 & 70.10 & 47.60 & 68.4  \\
        \cmidrule(lr){2-10}
        & \multirow{3}{*}{2-4-16} 
        & BitDistiller &  &  & 61.53 & 35.98 & 49.25 & 23.46 & 43.56  \\
        & & Ours & \checkmark &  & 70.67 & 45.86 & 62.03 & 30.54 & 52.28    \\
        & & Ours & \checkmark & \checkmark & 70.62& 46.41 & 61.48 & 31.32& \textbf{52.46}   \\
        \cmidrule(lr){2-10}
        & \multirow{3}{*}{2-4-4} 
        & BitDistiller &   &   & 63.38 & 34.32 & 50.82 & 23.80 & 43.08   \\
        & & Ours & \checkmark &   & 71.10 & 45.91 & 59.82 & 32.00 & 52.21    \\
        & & Ours & \checkmark & \checkmark & 72.36 & 45.91 & 58.64 & 32.25 & \textbf{52.29}   \\
        \cmidrule(lr){2-10}
        & \multirow{3}{*}{3-4-16} 
        & BitDistiller &   &   & 73.34 & 50.94 & 63.61 & 34.81 & 55.68   \\
        & & Ours & \checkmark &  & 76.71 & 53.96 & 68.19 & 35.23 & 58.52   \\
        & & Ours & \checkmark & \checkmark & 77.20 & 53.11 & 68.43 & 38.82 & \textbf{59.39} \\
        \cmidrule(lr){2-10}
        & \multirow{3}{*}{3-4-4} 
        & BitDistiller &   &   & 73.06 & 50.78 & 65.03 & 35.32 & 56.05    \\
        & & Ours & \checkmark &   & 76.98 & 53.12 & 66.77 & 37.03 & 58.48    \\
        & & Ours & \checkmark & \checkmark & 75.46 & 53.06 & 67.88 & 37.80 & \textbf{58.55}   \\
        \midrule[1.5pt]
        \multirow{13}{*}{2-7B} & \multirow{1}{*}{16-16-16} &  &  &  & 77.86 & 57.14 & 68.35& 43.34 & 61.67 \\
        \cmidrule(lr){2-10}
        & \multirow{3}{*}{2-4-16} 
        & BitDistiller &  &  & 62.95 & 37.33 & 50.20 & 22.95 & 43.36  \\
        & & Ours & \checkmark &  & 70.13 & 45.02 & 60.77 & 30.03 &  \textbf{51.49}   \\
        & & Ours & \checkmark & \checkmark & 69.48 & 45.22 & 59.75 & 29.95 & 51.10 \\
        \cmidrule(lr){2-10}
        & \multirow{3}{*}{2-4-4} 
        & BitDistiller &   &   & 62.70 & 37.18 & 53.91 & 25.93 & 44.93   \\
        & & Ours & \checkmark &  & 69.53 & 45.67 & 59.35 & 29.86 & 51.10   \\
        & & Ours & \checkmark & \checkmark & 69.91 & 44.58 & 59.70 & 30.69 & \textbf{51.22} \\

        \cmidrule(lr){2-10}
        & \multirow{3}{*}{3-4-16} 
        & BitDistiller &   &   & 74.42 & 51.36 & 62.66 & 36.17 & 56.15   \\
        & & Ours & \checkmark &   & 76.06 & 54.26 & 66.45 & 40.35 & 59.28    \\
        & & Ours & \checkmark & \checkmark & 76.65 & 54.25 & 67.80 & 40.35 & \textbf{59.74}   \\
        \cmidrule(lr){2-10}
        & \multirow{3}{*}{3-4-4} 
        & BitDistiller &   &   & 72.41 & 50.51 & 63.29 & 35.83 & 55.51    \\
        & & Ours & \checkmark &   & 76.55 & 53.55 & 65.90 & 39.33 & 58.83    \\
        & & Ours & \checkmark & \checkmark & 76.71 & 53.88 & 65.43 & 41.04 & \textbf{59.27}   \\
        \midrule[1.5pt]
        \multirow{13}{*}{3-8B} & \multirow{1}{*}{16-16-16} &  &  &  & 80.70 & 79.60 & 73.70 & 57.70 & 72.93 \\
        \cmidrule(lr){2-10}
        & \multirow{3}{*}{2-4-16} 
        & BitDistiller &  &  & 57.23 & 29.96 & 49.48 & 21.16 & 39.46 \\
        & & Ours & \checkmark &  & 69.96 & 44.30 & 59.43 & 28.66 & 50.59   \\
        & & Ours & \checkmark & \checkmark & 69.16 & 44.67 & 59.91 & 29.69 & \textbf{50.86}   \\
        \cmidrule(lr){2-10}
        & \multirow{3}{*}{2-4-4} 
        & BitDistiller &   &   & 56.42 & 29.57 & 52.09 & 20.90 & 39.75   \\
        & & Ours & \checkmark &   & 69.15 & 43.62 & 57.85 & 28.58 & 49.80    \\
        & & Ours & \checkmark & \checkmark & 69.97 & 44.32 & 59.51 & 27.82 & \textbf{50.41}   \\
        \cmidrule(lr){2-10}
        & \multirow{3}{*}{3-4-16} 
        & BitDistiller &   &   & 72.47 & 49.72 & 62.43 & 36.94 & 55.39   \\
        & & Ours & \checkmark &  & 77.25 & 55.18 & 68.90 & 42.91 & 61.06   \\
        & & Ours & \checkmark & \checkmark & 77.64 & 55.21 & 69.93 & 43.34 & \textbf{61.53} \\
        \cmidrule(lr){2-10}
        & \multirow{3}{*}{3-4-4} 
        & BitDistiller &   &   & 73.32 & 49.97 & 64.87 & 37.45 & 56.35   \\
        & & Ours & \checkmark &   & 75.35 & 53.95 & 67.64 & 41.80 & 59.69    \\
        & & Ours & \checkmark & \checkmark & 76.16 & 54.35 & 71.19 & 42.75 & \textbf{61.11}   \\
        \bottomrule[2pt]
    \end{tabular}
    }
    }
    \caption{Complete comparison of accuracy on Zero-shot Common Sense Reasoning tasks on LLaMA models with parameter sizes of at least 7B. }
\end{table*}



\begin{table*}[htbp]
    \centering
    \setlength{\tabcolsep}{5pt}
    \renewcommand{\arraystretch}{1.2}
    \resizebox{\textwidth}{!}{%
    {\normalsize
    \begin{tabular}{c cccccccc|c}
        \toprule[2pt]
         \raisebox{-4ex}{\textbf{Model}} & \raisebox{-4ex}{ \textbf{\#Bits \(\text{(W-A-KV)}\)}} & \multicolumn{3}{c}{ \raisebox{-2ex}{\textbf{Configuration}} } & \multicolumn{1}{c}{ \raisebox{-5ex}{\textbf{PIQA}}} & \multicolumn{1}{c}{ \raisebox{-5ex}{\textbf{Hella.}}} & \multicolumn{1}{c}{ \raisebox{-5ex}{\textbf{Wino.}}} & \multicolumn{1}{c}{ \raisebox{-5ex}{\textbf{ARC-c}}} & \multicolumn{1}{c}{ \raisebox{-5ex}{\textbf{Avg.}}}   \\
        \cmidrule(lr){3-5} 
        & & \textbf{Method} & \textbf{Rotation} & \textbf{LDP}  \\
        \midrule
        \multirow{13}{*}{1B} & \multirow{1}{*}{16-16-16} &  &  &  & 75.30 & 60.70 & 60.90 & 38.70 & 58.90  \\
        \cmidrule(lr){2-10}
        & \multirow{3}{*}{2-4-16} 
        & BitDistiller &  &  & 51.95 & 27.41 & 48.46 & 19.45 & 36.82  \\
        & & Ours & \checkmark &  & 61.15 & 30.66 & 50.67 & 21.84 &  41.08   \\
        & & Ours & \checkmark & \checkmark & 61.42& 31.55 & 51.78 & 20.65& \textbf{41.08}   \\
        \cmidrule(lr){2-10}
        & \multirow{3}{*}{2-4-4} 
        & BitDistiller &   &   & 55.33 & 26.62 & 48.46 & 19.79 & 37.55   \\
        & & Ours & \checkmark &   & 61.75 & 30.05 & 51.22 & 20.05 & 40.77    \\
        & & Ours & \checkmark & \checkmark & 60.71 & 31.54 & 53.51 & 21.42 & \textbf{41.80}   \\
        \cmidrule(lr){2-10}
        & \multirow{3}{*}{3-4-16} 
        & BitDistiller &   &   & 53.53 & 28.35 & 48.61 & 19.62 & 37.53   \\
        & & Ours & \checkmark &  & 69.53 & 40.31 & 55.40 & 26.27 & 47.88   \\
        & & Ours & \checkmark & \checkmark & 69.64 & 40.57 & 56.12 & 26.37 & \textbf{48.18} \\
        \cmidrule(lr){2-10}
        & \multirow{3}{*}{3-4-4} 
        & BitDistiller &   &   & 54.18 & 28.26 & 50.90 & 21.67 & 38.75    \\
        & & Ours & \checkmark &   & 68.98 & 37.80 & 55.40 & 26.36 & 47.14    \\
        & & Ours & \checkmark & \checkmark & 68.12 & 39.30 & 56.12 & 26.11 & \textbf{47.41}   \\
        \midrule[1.5pt]
        \multirow{13}{*}{3B} & \multirow{1}{*}{16-16-16} &  &  &  & 76.00 & 71.00 & 66.60 & 47.60 & 65.30 \\
        \cmidrule(lr){2-10}
        & \multirow{3}{*}{2-4-16} 
        & BitDistiller &  &  & 54.02 & 26.80 & 52.48 & 18.25 & 37.89  \\
        & & Ours & \checkmark &  & 65.99 & 36.51 & 52.48 & 26.19 & 45.29  \\
        & & Ours & \checkmark & \checkmark & 65.43 & 37.35 & 54.70 & 25.43 & \textbf{45.71} \\
        \cmidrule(lr){2-10}
        & \multirow{3}{*}{2-4-4} 
        & BitDistiller &   &   & 51.84 & 26.70 & 51.38 & 19.11 & 37.26   \\
        & & Ours & \checkmark &  & 64.30 & 36.26 & 51.38 & 25.08 & 44.26   \\
        & & Ours & \checkmark & \checkmark & 65.45 & 36.66 & 53.75 & 26.37 & \textbf{45.56} \\

        \cmidrule(lr){2-10}
        & \multirow{3}{*}{3-4-16} 
        & BitDistiller &   &   & 52.72 & 26.66 & 50.43 & 19.45 & 37.32   \\
        & & Ours & \checkmark &   & 74.04 & 49.56 & 63.22 & 35.83 & 55.66    \\
        & & Ours & \checkmark & \checkmark & 73.77 & 49.52 & 62.65 & 37.54 & \textbf{55.87}   \\
        \cmidrule(lr){2-10}
        & \multirow{3}{*}{3-4-4} 
        & BitDistiller &   &   & 53.91 & 26.82 & 48.03 & 20.30 & 37.27    \\
        & & Ours & \checkmark &   & 74.31 & 49.19 & 60.06 & 36.77 & 55.08    \\
        & & Ours & \checkmark & \checkmark & 73.18 & 48.87 & 62.43 & 36.01 & \textbf{55.12}   \\
        \bottomrule[2pt]
    \end{tabular}
    }
    }
    \caption{Complete comparison of accuracy on Zero-shot Common Sense Reasoning tasks on LLaMA-3.2 models with parameter sizes of at most 3B. }
\end{table*}

\begin{table*}[htbp]
\resizebox{\textwidth}{!}{%
\centering
\renewcommand{\arraystretch}{1} % 행 높이 조절
% \setlength{\tabcolsep}{8pt} % 열 간격 조절
\begin{tabular}{c|ccccccc}
\toprule[2pt]
\textbf{Layer Size} & \textbf{FP16} & \textbf{RCP} & \textbf{RCP+FP16Had}&\textbf{RCP+FP32Had} & \textbf{QuaRot}&\textbf{QuaRot+FP16Had} &\textbf{QuaRot+FP32Had} \\ \hline
(2048, 8192)  & 0.054 &  0.036 & 0.110 & 0.146 & 0.073 & 0.155 & 0.186  \\
(3072, 8192)  & 0.054 & 0.035 & 0.169 & 0.198 & 0.074 & 0.212 & 0.237  \\
(4096, 11008) & 0.077 &  0.048 & 0.120 & 0.148 & 0.088 & 0.157 & 0.186  \\
(4096, 14336) & 0.110 &  0.059 & 0.121 & 0.149 & 0.079 & 0.157 & 0.183  \\
\bottomrule[2pt]
\end{tabular}
}
\caption{GEMV latency for the down projection is measured except activation quantization overhead. The layer size is composed as (input channel, output channel). All latency numbers are in milliseconds.}
\label{tab:gemvQx:down}
\end{table*}

\begin{table*}[htbp]
\resizebox{\textwidth}{!}{%
\centering
\renewcommand{\arraystretch}{1} % 행 높이 조절
% \setlength{\tabcolsep}{8pt} % 열 간격 조절
\begin{tabular}{c|cccccc}
\toprule[2pt]
\textbf{Layer Size}  & \textbf{RCP} & \textbf{RCP+FP16Had}&\textbf{RCP+FP32Had} & \textbf{QuaRot}&\textbf{QuaRot+FP16Had} &\textbf{QuaRot+FP32Had} \\ \hline
(2048, 2048) &  0.131 & 0.214 & 0.248 & 0.170 & 0.248 & 0.276  \\
(3072, 3072) &  0.131 & 0.265 & 0.295 & 0.168 & 0.304 & 0.331  \\
(4096, 4096) &  0.133 & 0.221 & 0.250 & 0.174 & 0.250 & 0.282  \\
\bottomrule[2pt]
\end{tabular}
}
\caption{GEMV latency for the value projection is measured including activation quantization overhead. The layer size is composed as (input channel, output channel). All latency numbers are in milliseconds.}
\label{tab:gemvQo:v}
\end{table*}

\begin{table*}[htbp]
\resizebox{\textwidth}{!}{%
\centering
\renewcommand{\arraystretch}{1} % 행 높이 조절
% \setlength{\tabcolsep}{8pt} % 열 간격 조절
\begin{tabular}{c|cccccc}
\toprule[2pt]
\textbf{Layer Size}  & \textbf{RCP} & \textbf{RCP+FP16Had}&\textbf{RCP+FP32Had} & \textbf{QuaRot}&\textbf{QuaRot+FP16Had} &\textbf{QuaRot+FP32Had} \\ \hline
(2048, 8192) &  0.143 & 0.218 & 0.240 & 0.186 & 0.261 & 0.289  \\
(3072, 8192) &  0.140 & 0.271 & 0.294 & 0.177 & 0.318 & 0.340  \\
(4096, 11008) &  0.143 & 0.223 & 0.250 & 0.177 & 0.264 & 0.288  \\
(4096, 14336) &  0.142 & 0.226 & 0.247 & 0.177 & 0.259 & 0.285  \\
\bottomrule[2pt]
\end{tabular}
}
\caption{GEMV latency for the down projection is measured including activation quantization overhead. The layer size is composed as (input channel, output channel). All latency numbers are in milliseconds.}
\label{tab:gemvQo:down}
\end{table*}


\begin{figure*}[!t] % 두 컬럼을 차지하는 그림, t 옵션으로 상단 배치
    \centering
    \includegraphics[width=\textwidth]{latex/figs/coding_example.pdf} % 그림의 폭을 전체 텍스트 폭으로 설정
    \caption{A reasoning task example from HumanEval ~\cite{humaneval} benchmark, conducted by two differently quantized WizardCoder-7B ~\cite{luo2023wizardcoder} models. The results in the orange box is from state-of-the-art QAT method BitDistiller~\cite{BitDistiller} with applying rotation. In the green box, our proposed RCP is applied. Both methods employ exactly the same 4-bit quantization setting for activation and KV-cache.}
    \label{fig:coding}
\end{figure*}

\subsection{Information About Use of AI Assistants}
AI assistance was strictly limited to linguistic perspectives, such as grammar and spell checking, and finding synonyms.
\end{document}
