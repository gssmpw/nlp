\appendices
\section{The balance of the attack strategy}


According to Assumption \ref{chapter2:attack_assum}, at each moment, the attacker randomly selects $q_i$ sensors from the neighbor set $\mathcal{N}_i$ of sensor $i$ to launch the FDIAs, and the attack strategy satisfies the dynamic attack strategy in Definition \ref{semi-dynamic attack defn}.  Lemma \ref{lem41} will analyze the balance between the number of attacked sensors and the attack intensity in each subset.
\begin{lemma}\label{lem41}
    Based on the Assumption \ref{chapter2:attack_assum}, during the entire time period, the expectation of the number of attacked sensors in the $g_i$-th subset $\mathcal{N} _{i,g_i}$ of sensor $i$ is $\mathbb{E}[ q _{i,g_i}]= \lfloor q_i/m_i \rfloor$. In addition, the intensity of the attack on each subset is also balanced. Both are independent of the sensor set partitioning strategy, provided that the attacker selects the sensor to be attacked completely randomly at each moment, and the cardinality of each subset is approximately average.
\end{lemma}
% \begin{proof}
%     The proof process is omitted here. Please refer to the full version of this paper in \hl{[]}.
% \end{proof}
\begin{proof}
    Assume that the set of neighboring sensors of sensor $i$ has been approximately divided into $m_i$ subsets,  the expected number of attacked sensors and the balance of attack intensity in each subset will be analyzed.

First, let's analyze the expected number of attacked sensors in each subset. Since the set of attacked sensors changes randomly at each moment, we estimate the mathematical expectation of the total number of attacked sensors over the entire period using the Monte Carlo method. The probability of a sensor being selected as an attack target is $q_i / |\mathcal{N}_i|$. Therefore, the expected number of attacked sensors in each subset is given by:
\[
\mathbb{E}[q_{i,g_i}] = |\mathcal{N}_{i,g_i}| \times \left( \frac{q_i}{|\mathcal{N}_i|} \right) \approx \frac{|\mathcal{N}_i|}{m_i} \times \frac{q_i}{|\mathcal{N}_i|} = \left\lfloor \frac{q_i}{m_i} \right\rfloor.
\]

Next, we consider the balance of attack intensity. In the literature \cite{Suo2024attack}, the average malicious disturbance power of the attack signal $z_{ij}(k)$ is defined as the average of the sum of the squared values of the attack signal $z_{ij}(k)$ on the communication link between sensor $j$ and sensor $i$ over the entire time period. However, since the attacked communication links vary dynamically at each moment, and the same link may experience different attacks at different moment, the definition in \cite{Suo2024attack} is not applicable in this paper.

Therefore, this paper redefines the average malicious disturbance power from the attacker's perspective, rather than from the specific communication link's perspective. For the $l$-th attacker surrounding sensor $i$, where $l=1:q_i$, the average malicious disturbance power $\phi_{i,l}$ of the injected attack signal $z_{i,l}$ is defined as
\begin{equation}
\phi_{i,l} = \lim_{T \to \infty} \frac{1}{T} \sum_{k=1}^T \left( z_{i,l}(k) \right)^2.
\end{equation}
Based on the previous analysis, if the attacked sensors are completely random at each moment, the expected number of attacked sensors in each subset is $\left\lfloor q_i/{m_i} \right\rfloor$. Based on Assumption \ref{chapter2:attack_assum}, the ratio of the average malicious disturbance power of each subset to the total malicious disturbance power of all attacks is $\left\lfloor q_i/m_i \right\rfloor/q_i= 1/m_i
$.
This indicates that the attack intensity in each subset is balanced. This completes the proof.
\end{proof}

Then, a basic numerical simulation is utilized to verify that the attacker satisfies Lemma \ref{lem41}. Based on the set partitioning strategy in Algorithm \ref{alg:3-4}, the 100 sensors are divided into four subsets: sensors 1–25 form subset 1, sensors 26–50 form subset 2, sensors 51–75 form subset 3, and sensors 76–100 form subset 4. Without considering specific attack types, at each time step, the attacker randomly selects 40 sensors for attack. The number of attacked sensors in each subset is shown in Fig. \ref{chapter3:attack num}. During the entire time period, the average number of sensors attacked in each subset was 9.83, 9.97, 10.04, and 10.16, respectively, which is close to the ideal average of 10.

Building on this, when specific attack types are considered, the attack intensity distribution for each subset is shown in Fig. \ref{chapter3:attack density}. The vertical axis represents the ratio of the attack intensity in each subset to the total attack signal intensity, which is ideally 0.25. It can be seen from Fig. \ref{chapter3:attack density} that, the intensity ratios fluctuate around 0.25. Specifically, in Fig. \ref{chapter3:attack density}(a), the mean attack intensity ratios for each subset are 0.2497, 0.2500, 0.2504, and 0.2499, while in Fig. \ref{chapter3:attack density}(b), the mean ratios are 0.2507, 0.2490, 0.2499, and 0.2504.

\begin{figure}[ht] \centering \includegraphics[width=0.47\textwidth]{attack_number.pdf} \caption{The number of attacked sensors in each subset} \label{chapter3:attack num} \end{figure}

\begin{figure}[ht] \centering \begin{minipage}{\linewidth} \subfigure[Unstealthy attack]{ \includegraphics[width=0.94\linewidth]{attack_strength_unstealthy.pdf} } \subfigure[Stealthy attack]{ \includegraphics[width=0.94\linewidth]{attack_strength_stealthy.pdf} } \caption{Attack intensity ratio under unstealthy and stealthy attacks} \label{chapter3:attack density} \end{minipage} \end{figure}

\section{Distributed Attack Detection Scheduling Algorithm}

The ADS algorithm in literature \cite{Suo2024Security} can be extended to the D-ADS algorithm, as shown in Algorithm \ref{alg:3-1}. The main difference between the D-ADS and ADS algorithms lies in the sensor selection method, specifically whether it uses centralized selection or distributed two-stage sensor selection. For the $l$-th selection at time step $k$, a suspicious sensor is first chosen from each subset (Stage 1 of sensor selection) and added to the set $\mathcal{S}^{(l)}_k$. Then, a sensor is randomly selected from $\mathcal{S}^{(l)}_k$ as the result of the $l$-th selection (Stage 2 of sensor selection). The algorithm terminates when $|\mathcal{A}_{i,k}^{(l)}|=q_i$. 

It should be noted that in steps 4-7, the gains of all remaining sensors $j \in \mathcal{N}_i \backslash \mathcal{A}_{i,k}^{(l-1)}$ are updated. However, with the improved partitioning strategy, only the gains of one subset of sensors need to be updated here. The meanings of the parameters in Algorithm \ref{alg:3-1} are summarized in TABLE \ref{tab:distributed parameter}\footnote{Other variable symbols that are not explained here are  the same  as literature \cite{Suo2024Security}}.

\begin{algorithm}[t]  
%\setstretch{1.3}
\renewcommand{\thealgorithm}{3.3}
	\caption{Distributed Attack Detection Scheduling Algorithm}
	\label{alg:3-1}  
 
	\begin{algorithmic}[1]  
		\Require  
		Neighbor set $\mathcal{N}_i$ of sensor $i$, maximum number of attacked neighboring sensors $q_i$, historical information values $W_{kj}$, $j\in\mathcal {N}_i$.  
		\Ensure  
		Suspicious sensor set $\mathcal{A}_{i,k}$ at time $k$ and $\mathcal{A}_{i,k,g_i}=\mathcal{A}_{i,k,g_i}^{(l)}$, where $g_i=1:m_i$, $k=1,2,...,T$.
		
		\State Initialize weight vector $\omega_{k}^{(l)}=[\omega_{kj}^{(l)}]_{j\in\mathcal{N}_{i}}$, 
  where each element $\omega _{kj}=1$, and set $\mathcal{A}_{i,k,g_i}^{(0)}=\emptyset$ for $p=1:m_i$, $k=1:T$, $l=l_{g_i}=1$.

		\While{$l<q_i$}
		\State Set $\mathcal{S}_{k}^{(l)}=\mathcal{A}_{i,k}^{(l)}=\emptyset$.
        \For {all $j\in \mathcal{N}_i\backslash\mathcal{A}_{i,k}^{(l-1)}$} 
        \Statex \quad\quad\quad $\%$ With optimized grouping strategies, only a subset of sensors' gains needs to be updated here.
		\State Compute $G_{kj}^{(l)} = f_k( \mathcal{A}_{i,k}^{(l-1)} )-f_k( \mathcal{A}_{i,k}^{(l-1)}\cup \left\{ j \right\} ) $.
    \State Update $w_{k}^{(l)}$ as $w_{kj}^{(l)}= w_{k,j}^{(l-1)}e^{ -G_{kj}^{(l)}}$.
  \EndFor
		
		\For{$g_i=1:m_i$}
		\State Set $w_{g_i,k}^{(l_{g_i})}=\left[ w_{kj} \right] _{j\in \mathcal{N} _{i,g_i}\backslash \mathcal{A} _{i,k,g_i}^{(l_{g_i}-1)}}$.
		\State Compute $p_{g_i,k}^{(l_{g_i})}=w_{g_i,k}^{(l_{g_i})}/\| w_{g_i,k}^{(l_{g_i})}\|_1$.
		\State Select an element $j^{(l_{g_i})}_{g_i,select}$ based on the distribution vector $p_{g_i,k}^{(l_{g_i})}$. $\%$ Stage 1 sensor selection.
		\State Obtain $\mathcal{S}_{k}^{(l)}=\mathcal{S}_{k}^{(l)}\cup \{j^{(l_{g_i})}_{g_i,select}\}$.

		\EndFor

		\State Randomly select an element $j_{g_i^s,select}^{(l_{g_i^s})}$ from $\mathcal{S}_{k}^{(l)}$. $\%$ Stage 2 sensor selection.
		\State Obtain $\mathcal{A}_{i,k}^{(l)}=\mathcal{A}_{i,k}^{(l-1)}\cup \{j_{g_i^s,select}^{(l_{g_i^s})}\}$.
		\State Obtain $\mathcal{A}_{i,k,g_i^s}^{(l_{g_i^s})}=\mathcal{A}_{i,k,g_i^s}^{(l_{g_i^s}-1)}\cup \{j^{(l_{g_i^s})}_{{g_i^s},select}\}$.
		\State Update $l=l+1$.
		\State Update $l_{g_i^s}=l_{g_i^s}+1$.
		\EndWhile

		\State \Return $\mathcal{A}_{i,k}$ and $\mathcal{A}_{i,k,g_i}=\mathcal{A}_{i,k,g_i}^{(l)}$, where $g_i=1:m_i$.
	\end{algorithmic}  
\end{algorithm}

\begin{table*}[ht]
\small
\renewcommand{\arraystretch}{1.3}
\centering
\caption{Summary of Variables in Algorithm \ref{alg:3-1}}
\begin{tabular*}{0.76\textwidth}{@{\extracolsep{\fill}}cc}
\toprule
Parameter & Description \\
\midrule
$\omega_{g_i,k}^{(l_{g_i})}$ & The weight vector to be updated at the $l_{g_i}$-th selection for the $g_i$-th subset at moment $k$ \\

$\mathcal{A}_{i,k,g_i^s}^{l_{g_i^s}}$ & The set of suspicious sensors after the $l_{g_i^s}$-th selection for the $g_i^s$-th subset at moment $k$ \\

$p_{g_i,k}^{(l_{g_i})}$ & The distribution proportion vector at the $l_{g_i}$-th selection for the $g_i^s$-th subset at moment $k$ \\

$\mathcal{S}^{(l)}_{k}$ & The set of $g_i$ sensors selected from all subsets at the $l$-th selection at moment $k$ \\

$j_{g_i^s,select}^{l_{g_i^s}}$ & The sensor selected in the $l_{g_i^s}$-th selection for the $g_i^s$-th subset \\

$g_i^s$ & The subset to which the sensor $j_{g_i^s,select}^{l_{g_i^s}}$ selected from $\mathcal{S}^{(l)}_{k}$ belongs \\

\bottomrule
\end{tabular*}
\label{tab:distributed parameter}
\end{table*}


Next, the performance relationship between the D-ADS algorithm in this paper and the ADS algorithm in literature \cite{Suo2024Security} is analyzed. Both algorithms aim to select the set of suspicious sensors that have the greatest impact on the objective function (\ref{f_A}). Therefore, ideally, their performance is consistent, meaning that $\mathcal{A}_{i,k} = \cup^{m_i}_{g_i=1}{\mathcal{A}_{i,k,g_i}}
$.

\begin{lemma}
For $g_i\in\{1,...,m_i\}$, $l\in\{0,1,...,q_i\}$, define $\delta_{l,g_i}$ as $\delta _{l,g_i}=\sum_{k=1}^T{(}\frac{1}{m_i}f_k(\mathcal{A} _{i,k}^{*})-f_k(\mathcal{A} _{i,k,g_i}^{(l_{g_i})}))$
, where $\mathcal{A}_{i,k,g_i}^ {(l_{g_i})}$ represents the set of suspicious sensors after the $l_{g_i}$-th selection of the $g_i$-th subset at moment $k$.
Then, the relationship in literature \cite{Suo2024Security} can be transformed as $\sum^{m_i}_{g_i=1}\delta_{l+1,g_i} - (1-\frac{1}{q_i})^{l+1}\sum^{m_i}_{g_i=1}\delta_{0,g_i}\le \frac{m_i}{q_i}\sum_{j=1}^{l+1}(1-\frac{1}{q_i})^{l+1-j}  B_i^{(j)}$.
% \begin{equation}\label{relationship}		\sum^{m_i}_{g_i=1}\delta_{l+1,g_i} - (1-\frac{1}{q_i})^{l+1}\sum^{m_i}_{g_i=1}\delta_{0,g_i}\le \frac{m_i}{q_i}\sum_{j=1}^{l+1}(1-\frac{1}{q_i})^{l+1-j}  B_i^{(j)}.
% \end{equation}
The parameter $B_i^{(l)}$ is described in detail in the proof. At this time, the Lemma 3.2 in literature\cite{Suo2024Security} is extended to the distributed case.
\end{lemma}
% \begin{proof}
%     The proof process is omitted here. Please refer to the full version of this paper in \hl{[]}.
% \end{proof}
\begin{proof}
For the D-ADS algorithm, by summing $\delta_{l,g_i}$ over all $m_i$ subsets and taking the average, we obtain

\begin{small}
    \begin{eqnarray}\label{delta_lp_relax}
&&\frac{1}{m_i}\sum^{m_i}_{g_i=1}\delta_{l,g_i} \nonumber\\
% &=& \frac{1}{m_i}\sum^T_{k=1}(f_k(\mathcal{A}_{i,k}^{*})-\sum^{m_i}_{g_i=1}f_k(\mathcal{A}_{i,k,g_i}^{(l_{g_i})}))) \nonumber \\
&\le&\frac{1}{m_i}\sum^T_{k=1}(\sum^{m_i}_{g_i=1}(f_k(\mathcal{A}_{i,k,g_i}^{*})-f_k(\mathcal{A}_{i,k,g_i}^{(l_{g_i})}))) \nonumber \\
&\le& \frac{1}{m_i}\sum_{k=1}^T (\sum^{m_i}_{g_i=1} \sum_{j\in \mathcal{N}_{i,k,g_i}\backslash\mathcal{A}_{i,k,g_i}^*} ( f_k(\mathcal{A}_{i,k,g_i}^{(l_{g_i})}\cup \{j_{kl}^*\})-f_k(\mathcal{A}_{i,k,g_i}^{(l_{g_i})}) )) \nonumber \\
&=& \frac{1}{m_i}\sum_{k=1}^T (  \sum^{m_i}_{g_i=1} ( -\sum_{j \in \mathcal{N}_{i,k,g_i}\backslash\mathcal{A}_{i,k,g_i}^*}G_{kj}^{(l_{g_i}+1)} ) ),
\end{eqnarray}
\end{small}

\!\!where the first inequality follows from $f_k(\mathcal{A}_{i,k}^{*}) \le \sum^{m_i}_{g_i=1}f_k(\mathcal{A}_{i,k,g_i}^{(l_{g_i})})$, the second inequality follows from the submodularity of $f_k$, and the third equality follows from the definition of $G_{kj}^{(l+1)}$.  

Based on Lemma 3 in literature \cite{matsuoka2021tracking}, as well as the fact that $\delta_l \le \sum^{m_i}_{g_i=1} \delta_{l,g_i}$ and $\delta_{l,g_i} \ge \delta_{l+1,g_i}$, we have
\[
\frac{1-q_i}{m_i}\sum^{m_i}_{g_i=1}\delta_{l,g_i} \le -\frac{q_i}{m_i} \left( \sum^{m_i}_{g_i=1} \delta_{l+1,g_i} \right) + B^{(l+1)}_i,
\]
where
\[
B^{(l)}_i = \sum_{j=1}^{q_i}\sum_{k=1}^T \left( \frac{1}{m_i}\sum^{m_i}_{g_i=1} G_{k,g_i}^{(l_{g_i})} p_{g_i,k}^{(l_{g_i})} - G_{kj_{kl}^{*}}^{(l)} \right),
\]
and $G_{k,g_i}^{(l_{g_i})} = [ G_{kj}^{(l)} ]_{j \in \mathcal{N}_{i,k,g_i} \backslash \mathcal{A}_{i,k,g_i}^{(l_{g_i}-1)}}$, with $G_{kj_{kl}^{*}}^{(l)}$ denoting the gain of the optimal sensor $j_{kl}^{*}$ at time $k$ in the $l$-th selection. 

Thus, for all $l \in \{0, 1, ..., q_i\}$, the inequality 
\[
\sum^{m_i}_{g_i=1} \delta_{l+1,g_i} - \left(1 - \frac{1}{q_i} \right) \sum^{m_i}_{g_i=1} \delta_{l,g_i} \le \frac{m_i}{q_i} B_i^{(l+1)}
\]
holds. After iterating, we obtain
\begin{small}
\begin{equation}\label{relationship}
   \sum^{m_i}_{g_i=1} \delta_{l+1,g_i} - \left( 1 - \frac{1}{q_i} \right)^{l+1} \sum^{m_i}_{g_i=1} \delta_{0,g_i} \le \frac{m_i}{q_i} \sum_{j=1}^{l+1} \left( 1 - \frac{1}{q_i} \right)^{l+1-j} B_i^{(j)}. 
\end{equation}
\end{small}
Thus, Lemma 1 in literature \cite{Suo2024Security} has been extended to the distributed case.
This completes the proof.
\end{proof}


\begin{theorem}\label{thm42}
    For the dynamic attack strategy in the Definition \ref{semi-dynamic attack defn}, the proposed D-ADS algorithm can ensure that the theoretical lower bound of the average optimization rate of suspicious sensor selection for any subset is $1-1/e$, and the error expectation is bounded.
\end{theorem}
% \begin{proof}
%     The proof process is omitted here. Please refer to the full version of this paper in \hl{[]}.
% \end{proof}
\begin{proof}
To prove that the theoretical lower bound of the average optimization rate over the entire time period is 
$1 - 1/e$, 
we need to show that the expectation of the error 
\begin{equation}\label{chapter3:difference_between_real_and_predict}
\mathbb{E} \left[ \left( 1 - \frac{1}{e} \right) \sum_{k=1}^T{\frac{1}{m_i} f_k\left( \mathcal{A}_{i,k}^{*} \right)} - \sum_{k=1}^T{f_k( \mathcal{A}_{i,k,g_i} )} \right]  
\end{equation}
is bounded, where $f_k\left( \mathcal{A}_{i,k,g_i} \right)$ represents the submodular function value for the selected suspicious sensor set in subset $g_i$ at time $k$.

Essentially, the expectation of the error represents the expected difference between the objective function value of the selected suspicious sensor set and $1 - 1/e$ times the optimal suspicious sensor set's objective function value. Therefore, the larger the objective function value of a subset, the greater the upper bound of the error expectation. Consequently, the subset with the largest objective function value has the highest upper bound for error expectation, as shown in equation (\ref{chapter3:Theorem32_25})
\begin{eqnarray}\label{chapter3:Theorem32_25}
&&( 1 - \frac{1}{e} ) \sum_{k=1}^T{\frac{1}{m_i} f_k\left( \mathcal{A}_{i,k}^{*} \right)} - \sum_{k=1}^T{\max_{g_i} f_k( \mathcal{A}_{i,k,g_i} )} \nonumber\\ 
&\le& \frac{1}{m_i} \left[ (1 - \frac{1}{e}) \sum_{k=1}^T{ f_k\left( \mathcal{A}_{i,k}^{*} \right)} - \sum_{k=1}^T{ f_k( \mathcal{A}_{i,k} )} \right] \\
&\le&\frac{1}{m_i} \left[ (1 - (1 - \frac{1}{q_i})^{q_i})\sum_{k=1}^T{f_k\left( \mathcal{A}_{i,k}^{*} \right)} - \sum_{k=1}^T{f_k\left( \mathcal{A}_{i,k} \right)} \right],\nonumber
\end{eqnarray}
where the first inequality follows from literature \cite{mirzasoleiman2016distributed}, which states that $\max_{g_i} f_k(\mathcal{A}_{i,k,g_i}) \ge \frac{1}{m_i} f_k(\mathcal{A}_{i,k})$, and the second inequality follows from $(1 - 1/k)^{k} \le 1/e$. 

It is important to note that $\sum_{k=1}^T 1/{m_i} \cdot f_k\left( \mathcal{A}_{i,k}^{*}\right)$ does not imply that the objective function of each subset satisfies $\sum_{k=1}^T 1/{m_i} \cdot f_k\left( \mathcal{A}_{i,k}^{*}\right) = \sum_{k=1}^T f_k\left( \mathcal{A}_{i,k,g_i}^{*}\right)$. Instead, it follows from the assumption that the impact of attack signals on each subset is balanced over the entire time period.

Based on the Lemma 3 from literature \cite{matsuoka2021tracking} and the definition $\delta_l = \sum_{k=1}^T \left( f_k( \mathcal{A}_{i,k}^{*} ) - f_k ( \mathcal{A}_{i,k}^{(l)} ) \right)$ for $l=1:q_i$, we obtain that,
$\sum_{k=1}^T{f_k\left( \mathcal{A}_{i,k}^{*} \right)} - \sum_{k=1}^T{f_k \left( \mathcal{A}_{i,k} \right)} + (1 - (1 - \frac{1}{q_i})^{q_i})\sum_{k=1}^T{(f_k( \mathcal {A}_{i,k}^{(0)} ) - f_k( \mathcal{A}_{i,k}^{*} ))} \le \delta_{q_i} - (1 - \frac {1}{q_i})^{q_i} \delta_0 \le \sum^{m_i}_{g_i=1} \delta_{l,g_i} - (1 - \frac{1}{q_i})^{q_i} \sum^{m_i}_{g_i=1} \delta_{0,g_i}
$, where the inequality $\delta_{q_i} \le \sum^{m_i}_{g_i=1} \delta_{l,g_i}$ follows from the diminishing marginal returns property of the submodular function, and the initial condition satisfies $\delta_0 = \sum_{g_i=1}^{m_i} \delta_{0,g_i}$. Combining equation (\ref{relationship}) with the above relation, equation (\ref{chapter3:Theorem32_25}) can be rewritten as
% \begin{equation}
% ( 1 - \frac{1}{e} ) \sum_{k=1}^T{\frac{1}{m_i} f_k\left( \mathcal{A}_{i,k}^{*} \right)} - \sum_{k=1}^T{\max_{g_i} f_k( \mathcal{A}_{i,k,g_i} )}
% \le \frac{m_i}{q_i} \sum_{j=1}^{q_i} (1 - \frac{1}{q_i})^{q_i-j} \frac{B_i^{(j)}}{m_i}.
% \end{equation}
\begin{eqnarray}
   &&( 1 - \frac{1}{e} ) \sum_{k=1}^T{\frac{1}{m_i} f_k\left( \mathcal{A}_{i,k}^{*} \right)} - \sum_{k=1}^T{\max_{g_i} f_k( \mathcal{A}_{i,k,g_i} )}\nonumber\\
&\le& \frac{m_i}{q_i} \sum_{j=1}^{q_i} (1 - \frac{1}{q_i})^{q_i-j} \frac{B_i^{(j)}}{m_i}. 
\end{eqnarray}



Thus, to prove that equation (\ref{chapter3:Theorem32_25}) is bounded, it suffices to show that $\mathbb{E} [B_i^{(l)}/m_i]$ is bounded
\begin{eqnarray}\label{equ4.8}
&&\mathbb{E} \left[ \frac{B_i^{(l)}}{m_i} \right] \nonumber\\
&=& \frac{1}{m_i} \sum_{j=1}^{q_i} \mathbb{E} \left[ \sum_{k=1}^T \left( \frac{1}{m_i} \sum^{m_i}_{g_i=1} G_{k,g_i}^{\left( l_{g_i} \right)} p_{g_i,k}^{\left( l_{g_i} \right)} - G_{kj_{kl}^{*}}^{\left( l \right)} \right) \right] 
\nonumber\\
&\le& \frac{1}{m_i} \sum_{j=1}^{q_i} \mathbb{E} \left[ \sum_{k=1}^T ( G_{k}^{\left( l \right)} p_{k}^{\left( l \right)} - G_{kj_{kl}^{*}}^{\left( l \right)} ) \right]  
\nonumber\\
&\le& \frac{2q_i}{m_i} \sqrt{{( 2\varDelta _T\log ( |\mathcal{N}_i|T ) + T ( 2\log ( |\mathcal{N}_i|T ) + \log ( T ) ) )}},\nonumber\\
\end{eqnarray}
where the first inequality follows from the arithmetic mean-geometric mean inequality, and the second follows from Theorem 2 in literature \cite{Suo2024Security}.

Thus, the expectation of the error in equation (\ref{chapter3:difference_between_real_and_predict}) is bounded
\begin{eqnarray}\label{chapter3:dynamic_expextation}
   && \mathbb{E} \left[ \left( 1 - \frac{1}{e} \right) \sum_{k=1}^T{\frac{1}{m_i} f_k\left( \mathcal{A}_{i,k}^{*} \right)} - \sum_{k=1}^T{f_k( \mathcal{A}_{i,k,g_i} )} \right]  \nonumber\\
&\le& \tilde{\mathcal{O}} \left( \frac{q_i}{m_i} \sqrt{3T+2\varDelta _T} \right),
\end{eqnarray}
where $\mathbb{E} [\cdot]$ represents expectation, and $\tilde{\mathcal{O}}$ hides logarithmic terms.
This completes the proof.
\end{proof}




