\pdfoutput=1
\documentclass[lettersize,journal]{IEEEtran}
\usepackage{amsmath,amsfonts}
\usepackage{widetext}
\usepackage{lipsum} 
%\usepackage{blindtext}
%\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{array}
% \usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
\usepackage{subfigure}
\usepackage{textcomp}
\usepackage{stfloats}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{amsthm}
\usepackage{cite}
\usepackage{booktabs}
\usepackage{diagbox}
%\usepackage{subfigure}
\newcommand\emptyDiag[2][]{
\diagbox[innerwidth=\widthof{#2},height=\line, #1]{}{}%%
}
\newcommand{\upcite}[1]{\textsuperscript{\cite{#1}}}

%\usepackage{hyperref}
%\definecolor{navyblue}{RGB}{0,0,128}   %增加海军蓝颜色
% \hypersetup{
%   bookmarksnumbered,%
%   linktoc=all,
%   colorlinks=true,
%   %citecolor=black,
%   citecolor=blue,
%   filecolor=cyan,
%   % linkcolor=black,
%   % linkbordercolor=black,
%   % urlcolor=black,
%   linkcolor=blue,
%   linkbordercolor=blue,
%   urlcolor=blue,
%   plainpages=false,%
%   pdfstartview=FitH
% }
%\usepackage{flushend}%底部自动对齐
%\usepackage{cuted}
%\usepackage{soul, color, xcolor}%for highlight
%\usepackage{hyperref}%??????
%for pseudo code
\usepackage{algpseudocode}  
%\usepackage{subfigure}
\usepackage{multicol,multirow}

\usepackage{amsmath}  

\newtheorem{problem}{Problem}
\newtheorem{theorem}{Theorem}
\newtheorem{remark}{Remark}
\newtheorem{lemma}{Lemma}
\newtheorem{definition}{Definition}
\newtheorem{assum}{Assumption}
\renewcommand{\proofname}{Proof}

\renewcommand{\algorithmicrequire}{\textbf{Input:}}  % Use Input in the format of Algorithm  
\renewcommand{\algorithmicensure}{\textbf{Output:}} % Use Output in the format of Algorithm  

\newlength{\halfpagewidth}
\setlength{\halfpagewidth}{2.1\linewidth}
%\divide\halfpagewidth by 2

\newcommand{\leftsep}{%
\noindent\raisebox{5pt}[0ex][0ex]{%
\makebox[0.98\halfpagewidth]{\hrulefill}\hbox{\vrule height 0pt}}%
\vspace*{-2mm}%
}

\newcommand{\rightsep}{%
\noindent\hspace*{\halfpagewidth}%
\rlap{\raisebox{-5pt}[0ex][0ex]{\hbox{\vrule height 0pt}}}%
\makebox[0.98\halfpagewidth]{\hrulefill}%
}



\hyphenation{op-tical net-works semi-conduc-tor IEEE-Xplore}
% updated with editorial comments 8/9/2021

\begin{document}

\title{How to Divide: A Set Partitioning Strategy Balancing the Trade-off Between Intra-Subset Correlation and Inter-Subset Gain Mutual Influence \\ in Distributed Attack Detection Scheduling Task}
%Interference
%\title{Divide or not: Performance of the Distributed\\ Attack Detection Scheduling Algorithm\\ in Large-Scale Networks}

\author{Yuhan Suo,  Runqi Chai,~\IEEEmembership{Senior Member,~IEEE,} Senchun Chai,~\IEEEmembership{Senior Member,~IEEE,}\\  Zhong-Hua Pang,~\IEEEmembership{Senior Member,~IEEE,} Jiping Xu, and  Yuanqing Xia,~\IEEEmembership{Fellow,~IEEE,} 
	% <-this % stops a space
	\thanks{Yuhan Suo,  Runqi Chai, Senchun Chai, and Yuanqing Xia are with the School of Automation, Beijing Institute of Technology, Beijing 100081, China (e-mail: yuhan.suo@bit.edu.cn;  r.chai@bit.edu.cn; chaisc97@bit.edu.cn; xia$\_$yuanqing@bit.edu.cn). (\emph{Corresponding author: Runqi Chai})} 
	
	\thanks{Zhong-Hua Pang is with the Key Laboratory of Fieldbus Technology
		and Automation of Beijing, North China University of Technology,
		Beijing 100144, China (e-mail:
		zhonghua.pang@ia.ac.cn).}
	\thanks{Jiping Xu is with the School of Computer and Artificial Intelligence, Beijing
 Technology and Business University, Beijing 100048, China  (e-mail:
		xujp@th.btbu.edu.cn).}
	% \thanks{Guoping Liu is with the Department of Artificial Intelligence,
	% 	and Automation, Wuhan University, Wuhan 430072, China (e-mail:
	% 	guoping.liu@southwales.ac.uk).}
        }

%% The paper headers
%\markboth{IEEE Internet of Things Journal, ~Vol.~X, No.~X, XX~XXXX}%
%{Shell \MakeLowercase{\textit{et al.}}: A Sample Article Using IEEEtran.cls for IEEE Journals}
%
%\IEEEpubid{0000--0000/00\$00.00~\copyright~2021 IEEE}
%% Remember, if you use this you must call \IEEEpubidadjcol in the second
%% column for its text to clear the IEEEpubid mark.

\maketitle

\begin{abstract}
Recently, the efficiency of attack detection in large-scale sensor networks has remained a critical research challenge. Studies have shown that while distributed algorithms offer higher efficiency compared to centralized approaches, they often come at the cost of reduced performance. To strike a balance between detection efficiency and  performance in large-scale sensor networks, this paper explores the feasibility of extending existing algorithms to a distributed framework.
Starting from the perspective of set partitioning strategies, this study analyzes the key factor that contributes to the performance differences between distributed and centralized algorithms. By examining the gain mutual influence of sensor subsets, an optimal set partitioning strategy is designed to minimize inter-subset mutual influence while enhancing intra-subset correlation. To further reduce the computational cost of gain updates, a suboptimal partitioning strategy based on Grassmann distance is proposed, improving the efficiency of selecting suspicious sensors. Theoretical analysis demonstrates that this approach effectively reduces the computational cost of gain updates while maintaining detection performance. Finally, simulation results validate the effectiveness of the proposed method in enhancing attack detection performance.

\end{abstract}

\begin{IEEEkeywords}
Networks security, large-scale network, attack detection scheduling, set partitioning strategy, security analysis.
\end{IEEEkeywords}


\section{Introduction}
With the rapid development of information technology, automation and intelligent systems are profoundly reshaping the way modern society operates. As a key supporting technology for this change, sensor networks have been widely used in various fields, including smart cities, the Internet of Things, and industrial manufacturing \cite{heo2009earq,habibzadeh2017large,hatami2022demand}. These distributed networks consist of a large number of interconnected sensor nodes that work together to perform environmental monitoring, data collection, and intelligent control, greatly improving the system's intelligence and autonomous decision-making capabilities \cite{wei2022uav,tao2022digital}.  

However, as the scale of sensor networks continues to expand, application scenarios are becoming increasingly complex, and security challenges are becoming more prominent, especially in open and dynamic network environments \cite{xie2018data}. Adversaries may interfere with the system's state estimation by means of data tampering and information deception, resulting in system malfunctions or even serious security incidents, bringing significant risks to infrastructure and key businesses \cite{ge2019distributed,pang2021false}. Therefore, in the context of large-scale distributed sensor networks, the development of an efficient and accurate security defense mechanism has emerged as a crucial research direction to ensure system stability and reliability.

Attack detection has long been a core area of research in cybersecurity,  encompassing critical domains such as intrusion detection, anomaly detection, and fault diagnosis \cite{wang2025locational,suo2024opinion,li2023fast,li2024detecting,zhang2021deep,zhao2023sparse,balta2023digital}. Existing methods primarily fall into two categories: rule-based and learning-based approaches. 
Rule-based approaches rely on predefined security policies and pattern matching techniques, such as anomaly threshold settings and signature detection. For example, Wang et al. \cite{wang2025locational} extracted topological relationships measured by individual nodes and used reconstruction residuals to pinpoint the location of injection attacks. Suo et al. \cite{suo2024opinion} explored the Grassmann distance of opinion evolution direction and leveraged its latent characteristics to iteratively filter out malicious agents. Li et al. \cite{li2023fast} constructed a dynamic relationship between raw data and decrypted results to enable rapid detection of malicious attacks. In the realm of manufacturing system security, Li et al. \cite{li2024detecting} developed detection rules based on processing procedures and key parameters, identifying network attacks through rule matching. However, rule-based approaches struggle to adapt to novel and complex attacks.
In contrast, learning-based approaches leverage data-driven techniques such as statistical analysis, machine learning, and deep learning to detect and recognize attack patterns, offering superior generalization and adaptability \cite{zhang2021deep}. For instance, Zhao et al. \cite{zhao2023sparse} designed a data-driven attack detector based on subspace identification, detecting attacks by computing the system’s stable kernel representation. Balta et al. \cite{balta2023digital} proposed a detection framework based on digital twins, identifying attacks by analyzing controlled transient behaviors. Nevertheless, existing research indicates that these methods still face significant challenges in model training and data dependency \cite{li2024detecting,zhang2021deep}. 
Thus, improving adaptability and computational efficiency while ensuring detection accuracy remains a critical research direction in attack detection.

Beyond the research of attack detection, enhancing the system’s resilience is also crucial for ensuring stability. Secure state estimation aims to accurately recover the state of system even in the presence of attack. This problem has become a key research focus in the field of cyber-physical system security, particularly in control systems and distributed sensor networks \cite{ding2020secure,mustafa2022secure}.  
In distributed sensor networks, redundant information is considered essential for ensuring secure state estimation. Even when some sensors are compromised, redundancy allows the system to maintain normal operation \cite{shoukry2017secure}. However, identifying compromised sensors efficiently while avoiding the combinatorial explosion associated with NP-hard problems remains a major research challenge \cite{lu2023secure,an2022fast,lu2023polynomial}. 
To enhance system resilience, researchers have proposed various secure state estimation strategies. For instance, when core sensors are compromised, Xin et al. \cite{xin2025secure} introduced an estimation method based on virtual sensors, integrating deep reinforcement learning for online optimization to improve estimation accuracy and reliability. This approach aligns well with the concept of redundant information.  Xia et al. \cite{xia2025resilient} proposed a robust distributed Kalman filtering algorithm, which leverages attack detection and robust data fusion strategies to mitigate the impact of malicious network attacks, thereby enhancing the accuracy and stability of distributed state estimation.  
However, in large-scale distributed sensor networks, computational complexity remains a significant challenge. Balancing security and computational efficiency continues to be a pressing issue that requires further investigation in this field.



For large-scale networks, a sufficient condition to ensure system security is that each sensor can directly exclude potential attacked sensors from the neighbor set. For instance, the ADS algorithm proposed in literature \cite{Suo2024Security} aims to achieve this objective. However, the algorithm is centralized for each node, which means that its efficiency may decrease as the neighborhood cardinality increases. Given that a distributed approach can enhance the efficiency of a centralized algorithm, this paper explores the feasibility of extending the ADS algorithm into a distributed form. Existing studies indicate that distributed algorithms generally lag behind centralized ones in performance, with partitioning strategies considered one of the key factors influencing this gap \cite{mirzasoleiman2016distributed}. 

To more effectively design partitioning strategies, this paper will extract the inherent characteristics of each sensor. 
Based on this, the paper aims to optimize the design of partitioning strategies to balance the combined effect of inter-subset benefits and intra-subset correlations. Through optimal partitioning strategy design, the goal of this paper is to narrow the performance gap between distributed and centralized ADS algorithms, achieving a trade-off between efficiency and performance in the suspicious sensor selection task.

The main contributions of this paper are summarized as follows: The concept of mutual influence is introduced to quantify the marginal benefit interference between different sensor subsets. Based on the observable space characteristics of sensors, this metric effectively guides the set partitioning process, aiming to minimize inter-subset mutual influence. To address the imbalance in sensor gain updates caused by differences in observable spaces within subsets, a suboptimal set partitioning strategy based on Grassmann distance is proposed. This strategy maximizes the correlation among elements within each subset while minimizing mutual influence between subsets, providing strong support for extending the ADS algorithm to its distributed form (D-ADS). Under the proposed set partitioning strategy, gain updates are required only within the selected subset, significantly reducing computational costs. Theoretical results demonstrate that the error in the probability distribution vector remains bounded, ensuring the detection performance of suspicious sensor selection while improving algorithm efficiency. Finally, extensive simulation results under various scenarios validate the effectiveness of the proposed algorithm.


This paper is organized as follows: Section II presents the system model and problem formulation, including the description of the sensor network, attacker model, and the suspicious sensor selection problem in dynamic environments. Section III focuses on the analysis of the key factors affecting the performance of distributed algorithms, particularly the mutual influence of benefits between subsets and the design of the set partitioning strategy based on Grassmann distance. Section IV evaluates the performance of the proposed strategies through numerical simulations. Finally, Section V concludes the paper by summarizing the main contributions,  and suggesting potential directions for future research.



\section{Problem Formulation}
\subsection{System Model}
Consider a distributed sensor network that monitors the discrete-time linear time-invariant system state $x(k)$, as described in equation (\ref{chapter2:state_equation})
\begin{equation} \label{chapter2:state_equation}
	x\left( k+1 \right) =Ax\left( k \right) +\omega \left( k \right) ,
\end{equation}
where \( x\left( k \right)\in \mathbb{R} ^{n} \) and \( \omega \left( k \right)\in \mathbb{R} ^{n} \) represent the system state and process noise, respectively. 
Moreover, \( \omega \left( k \right) \) follows a Gaussian distribution with zero mean and a positive definite covariance matrix \( Q \), i.e., \( \omega \left( k \right) \sim \mathcal{N} \left( 0,Q\right) \).



 The network is represented by an undirected graph $\mathcal{G} = (\mathcal{N}, \mathcal{E})$, where $\mathcal{N}$ and $\mathcal{E} \subseteq \mathcal{N} \times \mathcal{N}$ denote the set of sensors and the set of edges, respectively. The neighbor set of sensor $i$ is denoted as $\mathcal{N}_i = \{j \in \mathcal{N} : (i, j) \in \mathcal{E} \}$. Thus, we obtain $\mathcal{N} = \mathcal{N}_1 \cup \mathcal{N}_2 \cup \ldots \cup \mathcal{N}_{|\mathcal{N}|}$. For sensor $i$, where $i \in \mathcal{N}$, its measurement model follows equation (\ref{measurement_equation})
\begin{equation} \label{measurement_equation}
	y_{i}\left( k \right) =C_{i}x\left( k \right) +\nu_{i} \left( k \right) ,
\end{equation}
where \( y_{i}\left( k \right)\in \mathbb{R} ^{m} \) and \( \nu_{i} \left( k \right)\in \mathbb{R} ^{m} \) represent the measurement and measurement noise of sensor \( i \), respectively. The matrix \( A\in\mathbb{R} ^{n\times n} \) in equation (\ref{chapter2:state_equation}) and the matrix \( C_i\in\mathbb{R} ^{m\times n} \) in equation (\ref{measurement_equation}) are both real matrices with appropriate dimensions. 
Similarly, \( \nu_{i} \left( k \right) \) follows a Gaussian distribution with zero mean and a positive definite covariance matrix \( R_{i} \), i.e., \( \nu_{i} \left( k \right) \sim \mathcal{N} \left( 0,R_{i} \right) \). 
In this section, both the process noise \( ||\omega \left( k \right)|| \) and the measurement noise \( ||\nu_{i} \left( k \right)|| \) are upper-bounded by certain positive scalars. 
%For the neighborhood set \( \mathcal{N}_i \) of sensor \( i \), the pair \( ( A, [C_{1}^{\mathrm{T}},...,C_{|\mathcal{N}_i| }^{\mathrm{T}}]^{\mathrm{T}} ) \) is observable.


Furthermore, for the neighbor set $\mathcal{N}_i$ of sensor $i$, the system $( A, [C_{1}^{\mathrm{T}},...,C_{|\mathcal{N}_i| }^{\mathrm{T}}]^{\mathrm{T}} )$ is observable. The distributed estimator of sensor $i$ is then given by equation (\ref{distri_estimat})
\begin{eqnarray}
\label{distri_estimat}
	\hat{x}_i\left( k+1 \right) &=&A\hat{x}_i\left( k \right) +K_i\left( k \right) \left( y_i\left( k \right) -C_i\hat{x}_i\left( k \right) \right)\nonumber\\ 
	&-&\lambda A\sum_{j\in \mathcal{N} _i}{ \left( \hat{x}_i\left( k \right) -\hat{x}_{j}\left( k \right) \right)},
\end{eqnarray}
where \( \hat{x}_i(k) \) is the estimate of the state \( x(k) \) for sensor \( i \), where \( \hat{x}_i(0) = x(0) \). 
Additionally, \( \hat{x}_j(k) \) represents the estimate of sensor \( j \), \( K_i(k) \) is the gain matrix, and \( \lambda \) is the consensus parameter, which takes values within the range \( ( 0, \min(1/|\mathcal{N}_i| )) \). 

\subsection{Attacker Model}
 At each moment $k$, the attacker selects communication links to launch FDIAs based on the dynamic attack strategy defined in Definition \ref{semi-dynamic attack defn}.
\begin{definition}\label{semi-dynamic attack defn}\textbf{(Dynamic Attack Strategy)}\cite{Suo2024Security}
For the sets of compromised sensors at two consecutive moments, \( \mathcal{A}_{k} \) and \( \mathcal{A}_{k-1} \), if they differ, i.e., \( \mathcal{A}_{k} \neq \mathcal{A}_{k-1} \), we refer to this as a dynamic attack strategy. The difference between the two sets can be calculated as
\begin{equation}
    \varDelta _k=( \mathcal{A}_{k}\backslash \mathcal{A}_{k-1} ) \cup ( \mathcal {A}_{k-1}\backslash\mathcal{A}_{k} ).
\end{equation}
Thus, over the entire time period \( T \), the total number of changes in compromised sensors is given by \( \varDelta _T=\sum_{k=1}^{T-1}{\varDelta _k} \).
\end{definition}


At moment $k$, suppose that the estimated state $\hat{x}_j(k)$ of sensor $j$ is compromised by injecting malicious data $z_{ij}(k)$ during its transmission to sensor $i$. The compromised estimation received by sensor $i$, denoted as $\hat{x}_{ij}^{a}(k)$, is given by
\begin{equation}
	\hat{x}_{ij}^{a}(k) = \hat{x}_j(k) + z_{ij}(k).
\end{equation}
In addition, the attacker's strategy satisfies the following Assumption \ref{chapter2:attack_assum}.

\begin{assum}\label{chapter2:attack_assum}
For the attacker's strategy, the following assumptions are made:

\begin{itemize}
\item At any moment \( k \), the number of compromised sensors \( q_i \) in the neighborhood of sensor \( i \) does not exceed half the size of the neighborhood, i.e., \( q_i \le \lfloor |\mathcal{N}_i|/2 \rfloor \)\upcite{yang2021secure,lu2023polynomial}
 \item At any moment \( k \), if the attacker reselects the set of compromised sensors, the selection process is entirely random, and the attack intensity is balanced.
\end{itemize}
\end{assum}



\subsection{Suspicious Sensor Selection Problem}



 Lemma 1 in literature \cite{Suo2024Security} states that selecting the suspicious sensor set from the set $\mathcal{N}_i$ is an NP-hard problem and that the objective function can be transformed into a submodular function as shown in equation (\ref{f_A})
    \begin{equation} \label{f_A}
 %\label{sub_func}
	f_k\left( \mathcal{A}_{i,k} \right) =\left\| \varLambda_{i,k} \cdot [\mu_{ij}(k)]_{j\in\mathcal{N}_i} \right\|,
\end{equation}
where $\varLambda_{i,k}$ is the augmented error
matrix, which is obtained by summarizing the estimated error $||\hat{x}_i( k )- \hat{x}_{ij}^{a}(k) ||$, i.e., $\varLambda_{i,k} =\sum^{|\mathcal{N}_i|}_{j=1}{\left(\theta_{|\mathcal{N}_i|}^j\otimes||\hat{x}_i\left( k \right) -\hat{x}_{ij}^{a}\left( k \right)||\right)}$, and $[\mu_{ij}(k)]_{j\in\mathcal{N}_i}$ denotes the augmented matrix of $\mu_{ij}(k)$, where $\mu_{ij}(k)=1$ for $j\in \mathcal{ A}_{i,k}$ and $ \mu_{ij}(k)=0$ for $j\in \mathcal{N}_i\backslash\mathcal{A}_{i,k}$.
Consequently, the suspicious sensor selection problem is reformulated as a solvable submodular maximization problem under a cardinality constraint, as presented in Problem \ref{chapter3:prob31}.
\begin{problem}\cite{Suo2024Security}\label{chapter3:prob31} 
The problem of selecting suspicious sensors essentially involves choosing no more than \( q_i \) sensors to maximize the objective function (\ref{f_A}), i.e.,
\begin{equation} \label{problem1}
    \max_{\mathcal{A}_{i,k} \subseteq \mathcal{N}_i} f_k\left( \mathcal{A}_{i,k} \right), ~~\text{s.t.} \left| \mathcal{A}_{i,k} \right|\leq q_{i},
\end{equation}
where \( q_{i} \) is defined in Assumption \ref{chapter2:attack_assum}. 
\end{problem}




\section{Main Results}
It is shown that distributed algorithms have obvious advantages over centralized algorithms in terms of efficiency, but from the perspective of performance, distributed algorithms generally have a certain gap with centralized algorithms. Therefore, this section focuses on the following issues: analyzing the key factors that affect the performance difference between D-ADS and ADS algorithm, and designing effective partitioning strategies to minimize the performance gap.

\subsection{The mutual influence of benefits between subsets}
In this section, the neighbor set of each sensor is  divided into several subsets, and the ADS algorithm in the literature \cite{Suo2024Security} is deployed on each subset. At this time, the ADS algorithm becomes a distributed case, that is, the D-ADS algorithm.

Specifically, the neighbor set $\mathcal{N}_i$ of sensor $i$ is divided into $m_i$ subsets, expressed as $\mathcal{N} _i=\mathcal{N} _{i,1}\cup \mathcal{N} _{i,2}\cup \cdots \cup \mathcal{N} _{i,m_i}$. It should be noted that the parameter $m_i$ is predetermined and has nothing to do with $|\mathcal{N} _i|$.

The method of approximate average partitioning is adopted to ensure that the cardinality of each subset is approximately equal\footnote{At this point, the balance between the number of attacked sensors in each subset and the attack intensity can be obtained intuitively. And the results from literature \cite{Suo2024Security} indicate that the D-ADS algorithm guarantees a performance lower bound for each subset consistent with that of the ADS algorithm. The proof process is omitted here. Please refer to the Lemma 2 in APPENDIX A and B.}.  The cardinality $|\mathcal{N} _{i,g_i}|$ of the $g_i$-th subset can be expressed as:
\begin{equation}\label{divide_card}
    |\mathcal{N} _{i,g_i}|=\lfloor|\mathcal{N}_i|/m_i \rfloor + \mathbb{I}(g_i\le(|\mathcal{N}_i|\  \text{mod}\  m_i|)),
\end{equation}
where $g_i=1:m_i$, and the indicator function $\mathbb{I}(\cdot)$ is defined as follows: when $g_i$ is less than or equal to the remainder of $|\mathcal{N}_i|$ divided by $m_i$, $\mathbb{I}(\cdot)$ is $1$, otherwise $\mathbb{I}(\cdot)$ is $0$. 


However, to ensure this performance lower bound, the gain values of all sensors need to be updated after each selection, and existing literature shows that this will bring huge computational complexity \cite{mirzasoleiman2016distributed}. In fact, at the $l$-th selection at moment $k$, only the sensor $j_{g_i^s,select}^{l_{g_i^s}}$ in the $g_i^s$-th subset is selected. To reduce the amount of calculation, this section starts with the gain update method and explores the feasibility of only updating  the gains of the remaining elements of the subset where the selected sensor is located.



Based on the inherent characteristics of the observable space of each sensor, this section investigates the mutual influence of gains between sensor subsets and proposes a set partitioning strategy to minimize the mutual influence of gains between subsets.


For the observable discrimination matrix $Q_{j,o}$ of neighbor sensor $j \in \mathcal{N}_i$ of sensor $i$, the indicator function for the $\ell$-th row of the $j$-th sensor is defined as a binary indicator function:
\begin{equation}\label{indicator_function}
    I_{j,\ell} =
    \begin{cases}
        1, & \exists m \in \{1, \dots, n\}, \ (Q_{j,o})_{\ell,m} \neq 0, \\
        0, & (Q_{j,o})_{\ell,:} = 0.
    \end{cases}
\end{equation}

By calculating the indicator function for each row of $Q_{j,o}$, a column vector $[I_{j,\ell}]_{\ell=1:n} \in \mathbb{R}^{n}$ can be obtained.
Then, the mutual influence is defined in Definition \ref{chapter4:defn42}:
\begin{definition}\label{chapter4:defn42}
    \textbf{(Mutual Influence)} For any two sensors $j_{g_i}$ and $j_{q_i}$ in any neighbor sensor subsets $\mathcal{N}_{i,k,g_i}$ and $\mathcal{N}_{i,k,q_i}$ of sensor $i$, the  mutual influence of gains between them is defined as the number of dimensions $[I_{j_{g_i},\ell}]_{\ell=1:n}$ and $[I_{j_{q_i},\ell}]_{\ell=1:n}$ that are not $0$ simultaneously in all dimensions $\ell=1:n$, denoted as 
    \begin{equation}
        E(j_{g_i},j_{q_i})=[I_{j_{g_i},\ell}]_{\ell=1:n}\land [I_{j_{q_i},\ell}]_{\ell=1:n}.
    \end{equation}
\end{definition}
And the problem of minimizing the mutual influence between subsets is given as shown below:
\begin{problem}\label{chapter3:prob4.1}
During the selection of a suspicious sensor, assume that the sensor $j_{g_i}$ from the $g_i$-th neighbor subset of sensor $i$, i.e., $j_{g_i} \in \mathcal{N}_{i,k,g_i}$, is selected.
According to equation (\ref{indicator_function}), the $Q_{j,o}$ of each sensor $j$ can be transformed into an indicator function vector. Therefore, it is only necessary to ensure that mutual influence between the elements in other subsets $q_i$ and the remaining elements of subset $g_i$ is minimized, that is,
    \begin{equation}
        \min_{q_i,j_{q_i}} \ \ \sum_{q_i=1, q_i\neq g_i} ^{m_i}\sum_{j_{q_i}\in\mathcal{N}_{i,q_i}}
        E(j_{g_i},j_{q_i}),
    \end{equation}
\end{problem}





The observable part of each sensor is extracted based on the observable discrimination matrix, so the mutual influence of gain only occurs between sensors with intersections in the observable space. 
Therefore, based on the optimal set partitioning strategy, after each suspicious sensor is selected, only the gains of the sensors in the subset need to be updated. However, the observable spaces of the sensors in the same subset may not be exactly the same. This means that after each sensor selection, the number of dimensions that need to be updated in the estimated error vector of each sensor in the subset will be different, resulting in an inaccurate allocation ratio vector $p_{p,k}^{(l_p)}$. 


\subsection{The Suboptimal set partitioning strategy based on Grassmann distance}


Based on the previous analysis, this paper considers exploring a suboptimal partitioning strategy to strike a balance between the mutual influence between subsets and the correlation within subsets.

Inspired by the fact that Grassmann distance describes the angular differences in the directions of the vector spanning the subspace, the directional differences between the observable spaces of sensors in the subset are used to evaluate the correlation between sensors.
First, the definition of Grassmann distance is introduced:

\begin{definition}
    \textbf{(Grassmann Distance)}\cite{edelman1998geometry} %Grassmann distance is a method for measuring the similarity between two subspaces. 
    For two subspaces $U$ and $V$, the Grassmann distance $d_G(U, V)$ can be calculated by performing singular value decomposition on the bases of these two subspaces, i.e., 
\begin{equation}\label{chapter4:defn43_equation}
    d_G(U,V)= \sqrt{\sum_{i=1}^{m} \theta_i^2},
\end{equation} 
where $m$ is the smaller dimension of $U$ and $V$, and $\theta_i$ is the principal angle between $U$ and $V$. Specifically, when the subspaces $U$ and $V$ are each spanned by one-dimensional vectors, $d_G(U,V) = \theta$.
\end{definition}


Based on the  indicator function column vector $[I_{j,\ell}]_{\ell=1:n}$ of all sensors $j \in \mathcal{N}_i$ in equation (\ref{indicator_function}), the Grassmann distance between the observable spaces of each pair of sensors can be obtained. Then, the problem of assigning completely correlated sensors into the same subset can be described as  Problem \ref{chapter4:prob43}:

\begin{problem}\label{chapter4:prob43}
For the $g_i$-th subset, the Grassmann distance between each pair of sensors $j_{g_i,1}, j_{g_i,2} \in \mathcal{N}_i$ can be obtained. Then, we only need to minimize the Grassmann distances between sensors within each subset $g_i = 1:m_i$ to maximize the intra-subset correlation, i.e.,
\begin{equation}\label{subset_main_theta}
        \min_{\theta_{j_{g_i,1},j_{g_i,2}}} \ \  \sum_{g_i=1:m_i} \| \boldsymbol{\theta_{g_i}}\|_2 = \|[\theta_{j_{g_i,1},j_{g_i,2}}]_{j_{g_i,1},j_{g_i,2}\in\mathcal{N}_{i,g_i}}\|_2.
    \end{equation}
\end{problem}

For any two sensors $j_1$ and $j_2$ in the set $\mathcal{N}_i$, the cosine of the angle between their indicator function column vectors can be calculated using the vector dot product formula, i.e., 
\begin{equation}
\label{cos_value}
    \cos(\theta_{j_1,j_2}) = \frac{[I_{j_1,\ell}]_{\ell=1:n} \cdot [I_{j_2,\ell}]_{\ell=1:n}}{\|[I_{j_1,\ell}]_{\ell=1:n}\| \cdot \|[I_{j_2,\ell}]_{\ell=1:n}\|}.
\end{equation}

The Grassmann distance between the observable spaces of sensors \( j_1 \) and \( j_2 \) can be determined using equation (\ref{Grassmann_distance}):  
\begin{equation}\label{Grassmann_distance}
    d_G([I_{j_1,\ell}]_{\ell=1:n},[I_{j_2,\ell}]_{\ell=1:n})= \arccos(\cos(\theta_{j_1,j_2})),
\end{equation}  
where the result is given in radians.
\begin{remark}
    It should be noted that equation (\ref{indicator_function}) has already converted rows containing nonzero elements into binary values, either \( 1 \) or \( 0 \). As a result, the Grassmann distance calculated between the indicator function column vectors only has two possible outcomes: either the vectors are perfectly aligned (\(\theta_{j_1,j_2}=0\)) or they are orthogonal (\(\theta_{j_1,j_2}=\pi/2\)). Ideally, for any \( g_i \)-th subset, the result of equation (\ref{subset_main_theta}) should be zero. 
\end{remark}
 

Based on the above analysis, the partitioning of the set $\mathcal{N}_i$ aims to find a compromise solution for two objectives: maximizing the intra-subset correlation and minimizing the mutual influence between subsets, and the correlation within the subset has a greater priority.



\begin{algorithm}[htb]  
%\setstretch{1.3}
\renewcommand{\thealgorithm}{3.1}
	\caption{Sensor Partitioning Strategy Based on Grassmann Distance}  
	\label{alg:3-3}  
	\begin{algorithmic}[1]  
		\Require  The observable discrimination matrix $Q_{j,o}$ for all sensors $j \in \mathcal{N}_i$.
		\Ensure  The partitioning result of the sensor set $\mathcal{N}_i$.
  \State Initialize $\mathcal{N}_{i,g_i}$, with $g_i=1:m_i$, and set the initial value of $m_i$ to 1, which will increase dynamically.
 \For{$j$ in $\mathcal{N}_i$}
 \For{$\ell=1:n$}
 \State Calculate the indicator function $I_{j,\ell}$ for the $\ell$-th row of $Q_{j,o}$ based on equation (\ref{indicator_function}).
 \EndFor
  \State Obtain the indicator function column vector $[I_{j,\ell}(k)]_{\ell=1:n} \in \mathbb{R}^{n}$.
 \EndFor

\For{$idx_1 = 1:|\mathcal{N}_i|$}
\State found group = 0.
\For{$g_i=1:m_i$}
\State $idx_2 = \mathcal{N}_{i,g_i}\{1\}$. $\%$ Take the first element from the set $g_i$.
% \EndFor
% \For{$idx_2 = idx_1+1:|\mathcal{N}_i|$}
\State Calculate the cosine value of the angle between the indicator function column vectors of sensors $j_{idx_1}$ and $j_{idx_2}$, which span a subspace, using equation (\ref{cos_value}).
  \State Calculate the Grassmann distance between the two subspaces using equation (\ref{Grassmann_distance}).
  \If{$d_G([I_{j_{idx_1},\ell}]_{\ell=1:n}, [I_{j_{idx_2},\ell}]_{\ell=1:n}) == 0$}
  \State $\mathcal{N}_{i,g_i} = \mathcal{N}_{i,g_i} \cup \{j_{idx_2}\}$.
  \State found group = 1.
  \State break.
  % Sensors $j_{idx_1}$ and $j_{idx_2}$ belong to the same subset.
  % \State $m_i$ 
  % \State Sensors $j_{idx_1}$ and $j_{idx_2}$ do not belong to the same subset.
  \EndIf
  \EndFor
  \If{found group == 0}   $\%$ Create a new subset.
  \State $m_i = m_i + 1$.
  \State $\mathcal{N}_{i,m_i} = \mathcal{N}_{i,m_i} \cup \{j_{idx_2}\}$.
  \EndIf
  \EndFor

  \State \Return{The $g_i$-th sensor subset $\mathcal{N}_{i,g_i}$,  $g_i=1:m_i$.}
	\end{algorithmic}  
\end{algorithm}







To reduce the computational cost incurred by calculating the Grassmann distance in step 13 of Algorithm \ref{alg:3-3}, the improved algorithm is proposed in Algorithm \ref{alg:3-4}. First, we present the following Lemma.
\begin{lemma}\label{chapter3:lem46}
    A necessary but not sufficient condition for maximizing the intra-subset correlation is that all sensors within the subset have the same observable space dimension.
\end{lemma}

\begin{proof}

For any pair of two elements \( j_{g_i,1} \) and \( j_{g_i,2} \) within the \( g_i \)-th subset of sensor \( i \), 
% their correlation is determined by comparing their indicator function vectors \( [I_{j_{g_{i},1},\ell}]_{\ell=1:n} \) and \( [I_{j_{g_{i},2},\ell}]_{\ell=1:n} \) to check if they are identical. Consequently, 
the intra-subset correlation of the \( g_i \)-th subset is defined as
%\begin{small}
     \begin{equation} \label{chapter3:equ4.19}
    \frac{\sum_{(j_{g_{i},1}, j_{g_{i},2}) \in \mathcal{N}_{i,g_i} \times \mathcal{N}_{i,g_i}}  \textbf{1}([I_{j_{g_{i},1},\ell}]_{\ell=1:n} =
    [I_{j_{g_{i},2},\ell}]_{\ell=1:n} )   
    }{|\mathcal{N}_{i,g_i}|\cdot|\mathcal{N}_{i,g_i}|}.
    \end{equation}
%\end{small}

    According to equation (\ref{chapter3:equ4.19}), when the indicator function vectors of two sensors are identical, they are fully correlated. In this case, their observable space dimensions must be the same. However, the converse does not necessarily hold.  
    This complete the proof.
\end{proof}


Inspired by Lemma \ref{chapter3:lem46}, we first partition all sensors with the same dimension \( Q_{j,o} \) into the same initial set. Then, we only need to apply Algorithm \ref{alg:3-3} separately in each set to partition the subsets. Theoretically, this strategy greatly reduces the computational cost required to calculate the Grassmann distance. The detailed steps are given in Algorithm \ref{alg:3-4}.
\begin{algorithm}[htb]  
%\setstretch{1.3}
\renewcommand{\thealgorithm}{3.2}
	\caption{The improved sensor set partitioning strategy based on Grassmann distance}  
	\label{alg:3-4}  
	\begin{algorithmic}[1]  
		\Require  The Cell array $Q_{total,o}$ consists of the observable matrices $Q_{j,o}$ of all sensors $j\in\mathcal{N}_i$.
		\Ensure  Set partitioning results of sensor set $\mathcal{N}_i$.

    \State Initialize an empty Map object $rankGroups$.
    
    \For{$j = 1:|\mathcal{N}_i|$}
        \State Obtain the observable matrix of sensor $j$ by $Q_{j,o}=Q_{total,o}\{j\}$.
        \State Calculate the rank $rank(Q_{j,o})$ of each observable matrix $Q_{j,o}$.
        
        \If{$isKey(rankGroups, rank(Q_{j,o}))$}
            \State $rankGroups(rank(Q_{j,o})) = [rankGroups(rank(Q_{j,o}))\  \{Q_{j,o}\}]$.
        \Else
            \State Create a key-value pair $rankGroups(rank(Q_{j,o}))$ $ = {Q_{j,o}}$.
        \EndIf
    \EndFor

\State $keys = rankGroups.keys$.
\For{$r = 1:length(keys)$}
\State Output all observable matrices $Q_{j,o}\in rankGroups(keys\{r\}))$ to Algorithm \ref{alg:3-3}, and obtain the set partitioning result.
\EndFor

  
           \State \Return{The $g_i$-th sensor subset $\mathcal{N}_{i,g_i}$, $g_i=1:m_i$.}
	\end{algorithmic}  
\end{algorithm}




\begin{remark}(\textbf{Computational Cost})
    Given that the indicator function column vectors have the same dimension, the computational cost is proportional to the number of Grassmann distance calculations. In Algorithm \ref{alg:3-3}, the number of Grassmann distance calculations is \( \binom{|\mathcal{N}_i|}{2} \). Suppose the sensor set \( \mathcal{N}_i \) can be divided into \( m_i^{\prime} \) subsets based on dimension, then each subset contains approximately \( |\mathcal{N}_i|/m_i^{\prime} \) sensors. In the improved algorithm, the required number of Grassmann distance calculations is
$ m_i\cdot\binom{|\mathcal{N}_i|/m_i^{\prime}}{2}$.
Comparing the two, the reduction in the number of Grassmann distance calculations is:
\begin{equation}
    \Delta C = \frac{|\mathcal{N}_i|^2(m_i^{\prime}-1)+|\mathcal{N}_i|}{2m_i^{\prime}}.
\end{equation}
Notably, when \( m_i^{\prime} > 1 \), this reduction is significant, implying that the improved algorithm effectively reduces the computational cost.
\end{remark}

\begin{remark}\label{chapter3:remark4.1}
    The proposed Algorithm \ref{alg:3-4} can ensure that the sensor observable space of each subset is consistent, but based on the D-ADS Algorithm, the theoretical performance of each subset is guaranteed only when the number of sensors in each subset is equal. Therefore, in the offline pre-setting stage, we need to ensure that the number of sensors with different observation spaces is equal, which is feasible.
\end{remark}


Theorem \ref{thm48} will prove that based on the proposed set partitioning strategy, although the computational cost of updating the gain is reduced, the impact on the sensor selection performance is limited.
\begin{theorem}\label{thm48}
    For the neighbour set $\mathcal{N}_i$ of sensor $i$, the set $\mathcal{N}_i$ is divided by Algorithm \ref{alg:3-4}. At the $l-1$-th selection, suppose that an element $j_s^{(l-1)}$ is selected from one of the subset, and only the gains of the remaining sensors in the subset where the element is located are updated. Then, at the $l$-th selection, the distribution ratio vector of the sensor selection of any $g_i$-th subset $\mathcal{N}_{i,k,g_i}$ is accurate, or the error is tolerable.
\end{theorem}
    
\begin{proof}
    According to the aforementioned analysis, there are $3$ types of relationships between two sensors:  Completely correlated, 
Partially correlated,
Completely uncorrelated.
    Based on the set partitioning strategy shown in algorithm \ref{alg:3-4}, the sensors within each subset are completely correlated, while these sensors are partially correlated or completely uncorrelated with the sensors in other subsets.
    
  
    The distribution ratio vector error under $3$ types of  relationships are analyzed as follows:

    For the case where the $l-1$-th selected sensor is completely uncorrelated with the $l$-th selected sensor, that is, there is no intersection in the observation spaces of the two sensors, the gain does not need to be updated at this time, and the distribution ratio vector error is $0$.

  
    For the case where the $l-1$-th selected sensor is completely correlated with the $l$-th selected sensor, that is, the observation spaces of the two sensors are exactly the same, the sensor gains are updated. Therefore, the distribution ratio vector error is also $0$.

However, for the case where the $l-1$-th selected sensor is partially correlated with the $l$-th selected sensor, that is, the observation spaces of the two sensors intersect but are not exactly the same, so the gains of the two sensors have mutual influence. However, to reduce the computational cost, after the $l-1$th selection, the gain of the $l$th selected sensor is not updated, so there must be an error in the distribution ratio vector. The following proves that the distribution ratio vector error is bounded in this case.

  
    For the $l-1$-th selected sensor $j_s^{(l-1)}$ and the $l$-th selected sensor $j_s^{(l)}$, the indicator function vectors are partially correlated. Considering the effect of diminishing marginal returns, the gain is most affected when the sensor is initially selected, and gradually decreases for subsequent selections. Therefore, the distribution ratio vector error is tolerable as long as the effect of the $l-1=1$-th selected sensor $j_s^{(1)}$ on the distribution ratio vector error of the $l=2$-th selected sensor $j_s^{(2)}$ is bounded.



Assuming that the diminishing marginal benefit of selecting sensor $j_s^{(1)}$ on the gain of $j_s^{(2)}$ is ignored, the gain $G_{k,j_s^{(2)}}^{(2)}$ of sensor $j_s^{(2)}$ is calculated as $G_{k,j_s^{(2)}}^{(2)}=f_k(\{j_s^{(2)}\})$. However, if the influence of selecting element $j_s^{(1)}$ on the gain of element $j_s^{(2)}$ is considered, the gain $G_{k,j_s^{(2)}}^{(2)}$ of sensor $j_s^{(2)}$ is calculated as $G_{k,j_s^{(2)}}^{(2)}=f_k(\mathcal{A}_{i,k}^{(1)}\cup\{j_s^{(2)}\})-f_k(\mathcal{A}_{i,k}^{(1)})$. The absolute value of the gain error in the two cases is $\Delta_{j_s^{(1)},j_s^{(2)}}$, which actually indicates the change in the gain of $j_s^{(2)}$ caused by the influence of $j_s^{(1)}$ on the gain.
Therefore, in the two cases, the distribution ratio vectors of the subset where the element $j^{(2)}$ belongs are respectively as shown in equation (\ref{equ424}):
\vspace*{-10pt}%

\begin{widetext}
     \begin{equation}\label{equ424}
    p^{\prime}=\frac{[f_k(\{j^{(2)}\}]_{j^{(2)}\in\mathcal{N}_{i,k,g_i}}}{\sum_{j^{(2)}\in\mathcal{N}_{i,k,g_i}}f_k(\{j^{(2)}\})},\quad\quad p^{\prime\prime}=\frac{[f_k(\{j^{(2)}\})+\Delta_{j_s^{(1)},j^{(2)}}]_{j^{(2)}\in\mathcal{N}_{i,k,g_i}}}{\sum_{j^{(2)}\in\mathcal{N}_{i,k,g_i}}(f_k(\{j^{(2)}\})+\Delta_{j_s^{(1)},j^{(2)}})}.
\end{equation}   

%\begin{multicols}{2}
Therefore, the distribution ratio error $|p^{\prime}(j_s^{(2)})-p^{\prime\prime}(j_s^{(2)})|$ of any element $j_s^{(2)}$ in the set $\mathcal{N}_{i,k,g_i}$ is
%\end{multicols}


%\leftsep
       \begin{eqnarray}\label{distribution_error}
    &&|p^{\prime}(j_s^{(2)})-p^{\prime\prime}(j_s^{(2)})|=\frac{|f_k(\{j_s^{(2)}\}\cdot\sum_{j^{(2)}\in\mathcal{N}_{i,k,g_i}}\Delta_{j_s^{(1)},j^{(2)}}-\sum_{j^{(2)}\in\mathcal{N}_{i,k,g_i}}f_k(\{j^{(2)}\})\cdot\Delta_{j_s^{(1)},j_s^{(2)}}|}{(\sum_{j^{(2)}\in\mathcal{N}_{i,k,g_i}}f_k(\{j^{(2)}\}))\cdot(\sum_{j^{(2)}\in\mathcal{N}_{i,k,g_i}}(f_k(\{j^{(2)}\})+\Delta_{j_s^{(1)},j^{(2)}}))}\nonumber\\
    &\le&\frac{\max_{j^{(2)}} \Delta_{j_s^{(1)},j^{(2)}}\cdot|f_k(\{j_s^{(2)}\}\cdot|\mathcal{N}_{i,k,g_i}|-\sum_{j^{(2)}\in\mathcal{N}_{i,k,g_i}}f_k(\{j^{(2)}\})|}{(\sum_{j^{(2)}\in\mathcal{N}_{i,k,g_i}}f_k(\{j^{(2)}\}))\cdot(\sum_{j^{(2)}\in\mathcal{N}_{i,k,g_i}}(f_k(\{j^{(2)}\})+\Delta_{j_s^{(1)},j^{(2)}}))},
\end{eqnarray} 
%\leftsep
%\rightsep
~\\where the  inequality holds because of $\Delta _{j s^{(1)},js^{(2)}}\le\max_{j^{(2)}} \Delta_{j_s^{(1)},j^{(2)}}=\Delta_{j_s^{(1)},j_{max}^{(2)}}$.

%\begin{multicols}{2}
First, consider the special case where there is \emph{no attack}. At this time, the gain of each sensor is only affected by noise. Therefore, from the perspective of the entire time period, the expectation of the gain $f_k(\{j^{(2)}\})$ of each element is theoretically $0$, that is, $|f_k(\{j_s^{(2)}\}\cdot|\mathcal{N}_{i,k,g_i}|-\sum_{j^{(2)}\in\mathcal{N}_{i,k,g_i}}f_k(\{j^{(2)}\})|$ is also close to $0$, which means that the increase of element $j_s^{(1)}$ leads to a negligible change in the gain of element $j^{(2)}$.

Furthermore, consider the case where there \emph{exists attack}. Assume that $q_{i,g_i}$ sensors in the $g_i$-th subset are attacked, while the remaining $|\mathcal{N}_{i,k,g_i}|-q_{i,g_i}$ sensors are normal.

Divide the equation (\ref{distribution_error}) into two parts: $|f_k(\{j_s^{(2)}\}\cdot|\mathcal{N}_{i,k,g_i}|-\sum_{j^{(2)}\in\mathcal{N}_{i,k,g_i}}f_k(\{j^{(2)}\})|$ and $\Delta_{j_s^{(1)},j_{max} ^{(2)}}/\left((\sum_{j^{(2)}\in\mathcal{N}_{i,k,g_i}}f_k(\{j^{(2)}\}))\cdot(\sum_{j^{(2)}\in\mathcal{N}_{i,k,g_i}}(f_k(\{j^{(2)}\})+\Delta_{j_s^{(1)},j^{(2)}}))\right)$.

For the former, define a $0-1$ binary parameter $\rho$, whose values $0$ and $1$ represent the states of sensor $j_s^{(2)}$ is under attack (denoted as $j_s^{(2),u}$) and without attack (denoted as $j_s^{(2),w}$), respectively. At this time, the following derivation is obtained
%\end{multicols}
%\leftsep
\begin{eqnarray}\label{chapter3:proof_fangsuo}
    &&|f_k(\{j_s^{(2)}\}\cdot|\mathcal{N}_{i,k,g_i}|-\sum_{j^{(2)}\in\mathcal{N}_{i,k,g_i}}f_k(\{j^{(2)}\})|\nonumber\\
    &=&|f_k(\{j_s^{(2)}\}\cdot|\mathcal{N}_{i,k,g_i}|-\sum_{j^{(2),w}\in\mathcal{N}_{i,k,g_i}\backslash I_{i,k,g_i}}f_k(\{j^{(2),w}\})-\sum_{j^{(2),u}\in I_{i,k,g_i}}f_k(\{j^{(2),u}\})  |\nonumber\\
    &\le&\rho \cdot \left( |\mathcal{N}_{i,k,g_i}|-q_{i,g_i} \right) |\min_{j^{(2),w}\in\mathcal{N}_{i,k,g_i}\backslash I_{i,k,g_i}} f_k(\{j^{(2),w}\}) -\max_{j^{(2),u}\in I_{i,k,g_i}}f_k(\{j^{(2),u}\}) |\nonumber\\
    &&+(1-\rho)\cdot q_{i,g_i} \cdot|\max_{j^{(2),u}\in I_{i,k,g_i}} f_k(\{j^{(2),u}\}) -\min_{j^{(2),w}\in\mathcal{N}_{i,k,g_i}\backslash I_{i,k,g_i}} f_k(\{j^{(2),w}\}) |\nonumber\\
    &=& \max\{|\mathcal{N}_{i,k,g_i}|-q_{i,g_i} ,q_{i,g_i}\} |f_k(\{j_{max}^{(2),u}\}) -f_k(\{j_{min}^{(2),w}\}) |,
\end{eqnarray}
%\leftsep
%\rightsep
%\begin{multicols}{2}
where $I_{i,k,g_i}$ in the first equation represents the set of all attacked sensors in the set $N_{i,k,g_i}$, and the first inequality holds because the maximum error that the selected sensor may bring is the maximum gain in the attacked sensor set and the minimum gain in the normal sensor set. In the second equation, $j_{max}^{(2),u}$ and $j_{min}^{(2),w}$ are equal to $\max_{j^{(2),u}\in I_{i,k,g_i}} f_k(\{j^{(2),u}\})$ and $\min_{j^{(2),w}\in\mathcal{N}_{i,k,g_i}\backslash I_{i,k,g_i}} f_k(\{j^{(2),w}\})$, respectively.
%\end{multicols}
%\leftsep

In addition, the parameter $\mu _{ij}^{(u)}(k)$ ($\mu _{ij}^{(w)}(k)$) is defined as the estimation error when the communication link between sensor $j$ and $i$ is under attack (without attack), which only depends on the network effect and the measurement noise and process noise of sensors $i$ and $j$. By taking the expectation of equation (\ref{chapter3:proof_fangsuo}) and omitting the subscripts $max$ and $min$, we can get:
\begin{eqnarray}
    %&&\mathbb{E}[|\sum_{A\in \mathcal{N}_{i,k,g_i}}{g_A}(S\cup \{B\})-g_A(S\cup \{B\})\cdot |\mathcal{N}_{i,k,g_i}||]\nonumber\\
    && \mathbb{E}\left[\max\{|\mathcal{N}_{i,k,g_i}|-q_{i,g_i} ,q_{i,g_i}\}\cdot |f_k(\{j^{(2),u}\}) -f_k(\{j^{(2),w}\}) |\right] \nonumber\\
    % &\le& \mathbb{E}\left[max\{|\mathcal{N}_{i,k,g_i}|-q_{i,g_i} ,q_{i,g_i}\} (z_{ij}(k)+\mu_{ij}(k))\right] \nonumber\\
    &\le& \mathbb{E}\left[\max\{|\mathcal{N}_{i,k,g_i}|-q_{i,g_i} ,q_{i,g_i}\}\cdot \frac{|f_k(\{j^{(2),u}\})^2 -f_k(\{j^{(2),w}\})^2 |}{|f_k(\{j^{(2),u}\}) +f_k(\{j^{(2),w}\}) |}\right] \nonumber\\
    &=&\mathbb{E}\left[\max\{|\mathcal{N}_{i,k,g_i}|-q_{i,g_i} ,q_{i,g_i}\}\cdot \frac{(z_{i,j^{(2),u}}+\mu_{i,j^{(2),u}}^{u})^2-(\mu_{i,j^{(2),w}}^{w})^2}{|z_{i,j^{(2),u}}+\mu_{i,j^{(2),u}}^{u}+\mu_{i,j^{(2),w}}^{w}|}\right] \nonumber\\
    &\le&   \mathbb{E}\left[\max\{|\mathcal{N}_{i,k,g_i}|-q_{i,g_i} ,q_{i,g_i}\}\cdot \frac{(z_{i,j^{(2),u}})^2+2z_{i,2}\mu_{i}^{u}+(\mu_{i}^{u})^2-(\mu_{i}^{w})^2}{|z_{i,j^{(2),u}}+\mu_{i,j^{(2),u}}^{u}+\mu_{i,j^{(2),w}}^{w}|}\right]  \nonumber\\
    &\le& \max \left\{\lfloor\frac{|\mathcal{N}_i|-q_i}{m_i} \rfloor + \mathbb{I}(g_i\le(|\mathcal{N}_i|\  mod\  m_i|)) ,\frac{q_i}{m_i}\right\} \cdot \frac{(\sum_{l=1}^{q_i}\phi_{i,q_i}(k)/q_i+2||\mu_{i}(k)||^2_{\infty})}{\bar{\mu_{i}}(k)} ,
\end{eqnarray}
%\leftsep
%\rightsep
%\begin{multicols}{2}
where the first inequality is obtained by multiplying both the numerator and the denominator by $|f_k(\{j^{(2),u}\}) +f_k(\{j^{(2),w}\}) |$. The first equality holds because the values of $f_k(\{j^{(2),u}\})$ and $f_k(\{j^{(2),w}\})$ are essentially related to the attack signal $z_{i,j^{(2),u}}$ and the estimation error $\mu_{i,j^{(2),u}}^u$ ($\mu_{i,j^{(2),w}}^w$). The third inequality holds because $z_{i,j^{(2),u}}(k)$ and $\mu_{i,j^{(2),u}}^u(k)$ are completely independent over the entire time period, and the expectation of the attack signal $z_{i,j^{(2),u}}^2$ on any sensor $j^{(2),u}$ is approximately $1/q_i$, which is the sum of the average malicious perturbation power of all attacks, expressed as $\mathbb{E}[z_{i,j^{(2),u}}(k)^2]=\sum_{l=1}^{q_i}\phi_{i,q_i}(k)/q_i$. In addition, the estimated error $\mu _{ij}\left( k \right)$ satisfies $||\mu _i\left( k \right) ||_{\infty}=\max _{j\in \mathcal {N} _i}\left\{ ||\mu _{ij}\left( k \right) ||_{\infty}\right\} $ and the estimation error $\mu _{ij}\left( k \right)$ satisfies $|z_{i,j^{(2),u}}(k)|\ge\left\|\mu_i(k) \right\| _{\infty}\ge |\sum_{j\in \mathcal{N}_i}{\mu_{ij}(k)} |/|\mathcal{N}_i|=\bar{\mu_i}$.
%\end{multicols}
%\leftsep

For the latter of equation (\ref{distribution_error}), dividing both the numerator and denominator by $\Delta_{j_s^{(1)},j_{max}^{(2)}}$, we get
\begin{equation}\label{equ427}
1/\left((\sum_{j^{(2)}\in\mathcal{N}_{i,k,g_i}}f_k(\{j^{(2)}\}))\cdot\left(\frac{\sum_{j^{(2)}\in\mathcal{N}_{i,k,g_i}}(f_k(\{j^{(2)}\})+\Delta_{j_s^{(1)},j^{(2)}})}{\Delta_{j_s^{(1)},j_{max}^{(2)}}}\right)\right).
\end{equation}
Since $\sum_{j^{(2)}\in\mathcal{N}_{i,k,g_i}}(f_k(\{j^{(2)}\})+\Delta_{j_s^{(1)},j^{(2)}})\gg f_k(\{j^{(2)}\})+\Delta_{j_s^{(1)},j^{(2)}}> \Delta_{j_s^{(1)},j_{max}^{(2)}}$ always holds, equation (\ref{equ427}) is an infinitesimal positive number.


%\leftsep
\end{widetext}



Considering the two parts of equation (\ref{distribution_error}), ideally, $|p^{\prime}(j_s^{(2)})-p^{\prime\prime}(j_s^{(2)})|\rightarrow 0$, because a bounded positive number multiplied by an infinitesimal number tends to $0$. In practice, we only need to ensure $|p^{\prime}(j_s^{(2)})-p^{\prime\prime}(j_s^{(2)})|\le \epsilon$, which is obviously achievable. This also means that based on the proposed set partitioning strategy, only the gains of the remaining sensors in the subset where the selected sensor is located need to be updated, instead of the gains of all the remaining sensors, which will significantly reduce the calculation of the gains.
This complete the proof.
\end{proof}








\section{Simulation Results}
In this section, simulation experiments using the vehicle target state estimation are presented to illustrate the effectiveness of the proposed algorithm.  
Consider the following moving vehicle model\upcite{zhou2022security},  
\begin{equation} \label{vehicle}
\left[ \begin{array}{c}
	p_x\left( k+1 \right)\\
	p_y\left( k+1 \right)\\
	v_x\left( k+1 \right)\\
	v_y\left( k+1 \right)\\
\end{array} \right] = \left[ \begin{matrix}
	1 & 0 & 1/50 & 0\\
	0 & 1 & 0 & 1/50\\
	0 & 0 & 1 & 0\\
	0 & 0 & 0 & 1\\
\end{matrix} \right] \left[ \begin{array}{c}
	p_x\left( k \right)\\
	p_y\left( k \right)\\
	v_x\left( k \right)\\
	v_y\left( k \right)\\
\end{array} \right] + \omega \left( k \right),
\end{equation}
where $p_x\left( k \right)$, $p_y\left( k \right)$, $v_x\left( k \right)$, and $v_y\left( k \right)$ represent the position and velocity of the vehicle in the $x$ and $y$ directions at moment $k$, respectively.  
The initial state of the vehicle is $x(0) = \left[ \begin{matrix} 50 & 0 & 5 & 0 \end{matrix} \right]^T$.  
% In an ideal scenario, the vehicle would move in a uniform straight line along the $x$-axis direction. 
In the subsequent simulations, the initial state estimates of all sensors, $\hat{x}_{i}(0)$, are set to $x(0)$.

This simulation considers 2 kinds of sensor network scenarios to validate the performance of the proposed algorithm:
\begin{enumerate}
    \item Scenario 1: A single central sensor and 100 neighboring sensors in an ultra-large-scale network.
    \item Scenario 2: A complex distributed network with 500 sensors.
\end{enumerate}


\textbf{Scenario 1}: 
In this scenario, a central sensor (labeled 0) is considered, along with 100 neighboring sensors (labeled 1–100). Each sensor independently measures the state of the vehicle, with the measurement model given by:
\begin{equation}
    y_{i}\left( k \right) =C_{i}x\left( k \right) +\nu _{i}\left( k \right).
\end{equation}
where the measurement matrix for the central sensor is $C_0 = [ \begin{matrix} 1 & 0 & 0 & 0 \end{matrix} ]$.  
The measurement matrices for the neighboring sensors differ based on their observable state space, and are given by the following four types:
\begin{small}
\begin{eqnarray} \label{obse_matrix}
C_1&=&[ \begin{matrix}
		1&		0&		0&		0\\
	\end{matrix} ]
 ,\quad
C_2=[ \begin{matrix}
		0&		1&		0&		0\\
	\end{matrix} ]
, \nonumber\\
C_3&=&[ \begin{matrix}
		0&		0&		1&		0\\
	\end{matrix} ]
 ,\quad
C_4=[ \begin{matrix}
		0&		0&		0&		1\\
	\end{matrix} ].
 \end{eqnarray}
 \end{small}
The process noise $\omega$ and measurement noise $\nu_i$ are set with parameters $Q = 0.5I$ and $R_i = 0.5I$, respectively. Additionally, both the process noise and the measurement noise are bounded, i.e., $||\omega(k)||_{\infty} \leq 0.05$ and $||\nu_i(k)||_{\infty} \leq 0.05$ at all times. The observability matrices for each sensor are given by:

$$Q_{1,o} = \left[ \begin{matrix}  
1 & 1 & 1 & 1 \\  
0 & 0 & 0 & 0 \\  
0 & 1 & 2 & 3 \\  
0 & 0 & 0 & 0  
\end{matrix} \right], \quad  
Q_{2,o} = \left[ \begin{matrix}  
0 & 0 & 0 & 0 \\  
1 & 1 & 1 & 1 \\  
0 & 0 & 0 & 0 \\  
0 & 1 & 2 & 3  
\end{matrix} \right],$$ $$Q_{3,o} = \left[ \begin{matrix}  
0 & 0 & 0 & 0 \\  
0 & 0 & 0 & 0 \\  
1 & 1 & 1 & 1 \\  
0 & 0 & 0 & 0  
\end{matrix} \right], \quad  
Q_{4,o} = \left[ \begin{matrix}  
0 & 0 & 0 & 0 \\  
0 & 0 & 0 & 0 \\  
0 & 0 & 0 & 0 \\  
1 & 1 & 1 & 1  
\end{matrix} \right].$$

It can be seen that the observability dimensions for the different types of sensors are varied, with dimensions of 1 and 3, 2 and 4, 3, and 4, respectively. It is assumed that the measurement matrix of sensors 1–25 is  $C_1$, the measurement matrix of sensors 26–50 is $C_2$, the measurement matrix of sensors 51–75 is $C_3$, and the measurement matrix of sensors 76–100 is $C_4$. Additionally, all neighboring sensors $j \in \mathcal{N}_0$ are capable of transmitting information to the central sensor, with the transmitted information being the state estimate $\hat{x}_{0,j}(k)$, which may be compromised by FDIAs. Based on the set partitioning strategy in Algorithm \ref{alg:3-4}, the 100 sensors are divided into four subsets: sensors 1–25 form subset 1, sensors 26–50 form subset 2, sensors 51–75 form subset 3, and sensors 76–100 form subset 4.

At any moment, the attacker randomly selects 40 sensors to attack from  100 neighboring sensors. For any consecutive moments, the sets of attacked sensors $\mathcal{A}_{0,k}$ and $\mathcal{A}_{0,k-1}$ in the neighborhood of sensor 0 satisfy $\varDelta _{0,k}=( \mathcal{A}_{0,k}\backslash \mathcal{A}_{0,k-1} ) \cup ( \mathcal{A}_{0,k-1}\backslash \mathcal{A}_{0,k} ) \ge 0$.

In this simulation, only the velocity estimate $\hat{x}_{0,j}(k)$ in the sensor's estimated results is tampered with by the attacker, while the position estimate remains unaffected. The attack signals are classified into two types: one type corresponds to a unstealthy attack, which exhibits large magnitudes most of the time; the other type corresponds to a stealthy attack, whose magnitude is close to the estimation noise most of the time. To better simulate different attack scenarios, the attack signals are designed based on the magnitude of the estimation noise, ensuring that stealthy and unstealthy attacks do not appear in the same simulation experiment. This setup meets the prerequisites of Lemma 2 shown in the
APPENDIX A.

According to Kalman decomposition \cite{Suo2024attack}, by first deinfluence and then reconstruction, the estimated error $\Delta\hat{x}_j^{re}$ of each sensor $i$ and its neighbor sensor $j\in\mathcal{N}_i$, which only contains the observable part, can be obtained. The augmented error matrix $\varLambda_{i,k}$ in equation (\ref{f_A}) is obtained by summarizing the $\Delta\hat{x}_j^{re}$ of all neighbors $j$. It should be noted that the number of non-zero rows of $\Delta\hat{x}_j^{re}$ and $Q_{j,o}$ is the same.


Next, we compare the distribution of inter-subset gain mutual influence and intra-subset correlation under different partitioning strategies. Three partitioning strategies are considered: the partitioning strategy based on minimizing inter-subset gain mutual influence (Partitioning Strategy $1$), the suboptimal sensor partitioning strategy based on the Grassmann distance (Partitioning Strategy $2$), and the random division of $100$ sensors into $4$ groups (Partitioning Strategy $3$).  
For Partitioning Strategy $1$, the observable spaces of $C_1$ ($C_2$) and $C_3$ ($C_4$) overlap. Consequently, the partitioning result is that subset $1$ includes all sensors with measurement matrices $C_1$ and $C_3$, while subset $2$ includes all sensors with measurement matrices $C_2$ and $C_4$. As shown in TABLE \ref{chapter3:tab:min_wai_max_nei}, the inter-subset benefit influence is $0$, while the intra-subset correlation is $0.5$, indicating that this strategy minimizes the inter-subset gain mutual influence.  
For Partitioning Strategy $2$, sensors are fully correlated only when their measurement matrices are identical. Thus, the partitioning result consists of 4 subsets, each containing one type of sensor. As shown in TABLE \ref{chapter3:tab:min_wai_max_nei}, the inter-subset gain mutual influence is $625$, while the intra-subset correlation is $1$, indicating that this strategy maximizes intra-subset correlation.  
For Partitioning Strategy $3$, by averaging the results of $100$ completely independent partitioning instances, we obtain the results shown in TABLE \ref{chapter3:tab:min_wai_max_nei}, where neither the inter-subset gain mutual influence nor the intra-subset correlation is optimal.  
Therefore, the proposed suboptimal partitioning strategy achieves the goal of minimizing inter-subset gain mutual influence while maximizing intra-subset correlation.

\begin{table*}[ht]
    \small
    \renewcommand{\arraystretch}{1.4}
    \centering
    \caption{Inter-subset Interaction and intra-subset Correlation under different Partitioning Strategy}
    \label{chapter3:tab:min_wai_max_nei}
    \begin{tabular*}{0.9\textwidth}{@{\extracolsep{\fill}}cccccccc}
       
        \toprule
      Strategy & Set  & Subset 1                         & Subset 2                         & Subset 3                         & Subset 4  & \multicolumn{2}{c}{ Intra-subset Correlation}                       \\ \midrule
       \multirow{2}{*}{Strategy 1} & Subset 1& \emptyDiag{11581}& 0 &\emptyDiag{11581} &\emptyDiag{11581} & Subset 1 & 0.5\\
       & Subset 2& 0&\emptyDiag{11581} &\emptyDiag{11581} &\emptyDiag{11581} & Subset 2 & 0.5
        \\\bottomrule
        %\midrule
       \multirow{4}{*}{Strategy 2} & Subset 1    &         \emptyDiag{11581}                    & 0   & 625 & 0 & Subset 1 & 1  \\ 
        &Subset 2    & 0   &               \emptyDiag{11581}                & 0   & 625 & Subset 2 & 1  \\ 
        &Subset 3    & 625 & 0   &                        \emptyDiag{11581}       & 0  & Subset 3 & 1 \\
        &Subset 4    & 0   & 625 & 0   &                \emptyDiag{11581}     & Subset 4 & 1          \\\bottomrule
        \multirow{4}{*}{Strategy 3} &Subset 1                         & \emptyDiag{11581}                & 158  & 160  & 167  & Subset 1 & 0.293 \\ 
       &Subset 2                         & 158  & \emptyDiag{11581}             & 140  & 156  & Subset 2 & 0.28  \\ 
        &Subset 3                         & 160  & 140  & \emptyDiag{11581}           & 170  & Subset 3 & 0.306 \\
       & Subset 4                         & 167  & 156  & 170  & \emptyDiag{11581} & Subset 4 & 0.2608 \\\bottomrule
    \end{tabular*}
\end{table*}


   

Again, to verify the conclusion of Theorem \ref{thm48}, this simulation considers a special scenario where, at the first selection of each moment, the D-ADS algorithm always selects an attacked sensor from the subset $4$. 


According to Theorem \ref{thm48}, the gain interactions only exist  between $C_2$ and $C_4$, that is, under these two gain update strategies, the error in the distribution ratio vector only appears in subset $2$. The error curves are shown in Fig. \ref{chapter3:fig:p_error}. To make the error curves smoother and more intuitive, the evaluation metric used is windowed RMSEs (W-RMSEs), with a window size of $10$ moments. It can be observed that the distribution ratio vector error in subset $2$ is bounded, with an average value of only $2.2871e-04$ over the entire time period, while the errors in subsets $1$, $3$, and $4$ remain zero throughout, which is consistent with the conclusion of Theorem \ref{thm48}.  


\begin{figure}[ht]
	\centering
	\includegraphics[width=0.45\textwidth]{prob_error_junyun_eng.eps}
	\caption{Distribution ratio vector error of the proposed partitioning strategy under different gain update methods}
	\label{chapter3:fig:p_error}
\end{figure}






\textbf{Scenario 2}: Next, this simulation verifies that the proposed algorithm can ensure the secure state estimation of the system.  
Consider a complex distributed network scenario in which $500$ sensors (labeled $1-500$) are randomly deployed in a $200$m $\times 200$m area. Each sensor has the capability to communicate with sensors within a $30$m radius, forming an undirected graph. Similar to Scenario $1$, each sensor can independently measure the state of the vehicles, and the measurement matrix is classified into the $4$ types given in equation (\ref{obse_matrix}).  
First, measurement matrices are randomly assigned to the $500$ sensors, and the number of each type of sensor in the neighbor set of each sensor is recorded. The statistical results indicate that, for almost all sensors, their neighbor sets fail to satisfy the requirement stated in Remark \ref{chapter3:remark4.1}, which mandates an equal number of sensors observing different state spaces.  
% In this case, the proposed Algorithm 3.4 as shown in the APPENDIX C of the full version of this paper \hl{[]} is applied to the entire network to dynamically adjust the communication directions between sensors, thereby controlling the distribution of neighbor sensors for each sensor. 
In this case, based on the set partitioning result, the communication direction between sensors is dynamically adjusted to control the distribution of neighbor sensors of each sensor. Ultimately, the entire network is transformed into a directed graph, ensuring that the neighbor set of each sensor satisfies Remark \ref{chapter3:remark4.1}.  
Therefore, based on the proposed Algorithm \ref{alg:3-3} or Algorithm \ref{alg:3-4}, the neighbor set of each sensor can be divided into $4$ subsets.
\begin{figure}[!h]
	\centering
	\includegraphics[width=0.45\textwidth]{RMSE_distribued_eng.pdf}
	\caption{RMSE Curves under Different Cases}
	\label{chapter3:fig:RMSE_distribued}
\end{figure}

At each moment $k=1:100$, the attacker randomly selects no more than half of the target sensors from the set of neighboring sensors of each sensor and launches stealthy attacks on their communication links. Therefore, the estimation results transmitted between sensors may be affected by FDIAs. Based on the D-ADS Algorithm, each sensor can preliminarily exclude a suspicious sensor set before fusing the information from neighboring sensors.
This simulation compares the security of the system under different cases, including the case without attack detection, the case using the proposed D-ADS algorithm, and the case without any attack. For the no-attack scenario, the mean of the estimation errors of all sensors at each moment is used to calculate the RMSE. In contrast, for the case where attacks exist, considering that the security of the system can be intuitively described by the maximum estimation error, this simulation calculates the RMSE using the maximum estimation error at each moment.
Fig. \ref{chapter3:fig:RMSE_distribued} presents the RMSE curves under these cases, where there are three curves in total: the no-attack case (red), the proposed D-ADS algorithm detection (blue), and the case without  attack detection (green). It can be seen that the blue curve is significantly lower than the green curve, indicating that the proposed algorithm ensures system security. However, there is still a certain gap between the blue and red curves, as stealthy attacks are challenging to detect accurately. This observation is consistent with the simulation results in Scenario $1$.

\vspace{-3mm}
\section{Conclusions}
This paper investigates the performance of distributed attack detection scheduling algorithms in large-scale sensor networks. By analyzing the gain mutual influence between sensor subsets, an set partitioning strategy based on Grassmann distance is proposed, effectively reducing inter-subset mutual influence while enhancing intra-subset correlation. Theoretical analysis and simulation results demonstrate that the proposed strategy not only significantly narrows the performance gap between the D-ADS and ADS algorithm but also reduces computational cost. Future research can explore more advanced feature extraction methods to better capture complex information within the network, thereby further improving the efficiency and accuracy of attack detection.

\input{Appendix}


\vspace{-3mm}
\bibliographystyle{IEEEtran} 
\bibliography{ref}






\newpage


 


\vfill

\end{document}


