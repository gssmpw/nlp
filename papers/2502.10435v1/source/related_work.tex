\section{Related Work}

\paragraph{Multi-modal Representation Learning.}
% \stitle{Multi-modal representation learning.}
Emotion recognition has progressed from uni-modal approaches~\cite{huang2021audio,saha2020towards}, which rely on a single modality, to multi-modal methods~\cite{mittal2020m3er,lv2021progressive,Tailor} that exploit complementary features across modalities. While uni-modal approaches often face recognition biases~\cite{huang2021audio}, multi-modal learning has gained significant attention, with a key challenge being the effective integration of heterogeneous modalities. Early fusion methods, such as concatenation~\cite{ngiam2011multimodal}, tensor fusion~\cite{liu2018efficient}, and averaging~\cite{fusenet}, struggle with modality gaps that hinder effective feature alignment. To address this, attention-based methods~\cite{adversarial_masking,cross_adaptation} leverage cross-attention mechanisms to dynamically align features in the latent space, while contrastive learning~\cite{chen2020simple_CL,CARAT} further improves robustness. However, most attention-based methods consolidate modalities into a joint embedding, often overlooking the unique characteristics of each modality.

\begin{figure*}[t] % 这四个字母可以出现一个或多个：htbp 代表图片插入位置的设置
    \centering % 图片居中
    \includegraphics[width=0.95\linewidth]{images/Framework12.pdf}
    % \vspace{-2em}
    \caption{The framework of RAMer. Given incomplete multi-modal inputs, RAMer first encodes each individual modality through an auxiliary task, then feeds the features into a reconstruction-based adversarial network to extract specificity and commonality. Finally, a stacked shuffle layer is employed to learn enhanced representations.}
    \label{framework}
    \vspace{-1.em}
\end{figure*}

\paragraph{Multi-label Emotion Recognition in Videos.}
Multi-label emotion recognition in videos involves assigning multiple emotion labels to a target individual in a video sequence. Early methods treated multi-label classification as independent binary tasks~\cite{boutell2004learning}, but recent advancements explore label correlations using techniques like Adjacency-based Similarity Graph Embedding(ASGE)~\cite{ASGE}, Graph Convolutional Network (GCN)~\cite{chen2019multi}, and multi-task pattern~\cite{tsai2020order} to explore label correlations. Some noteworthy strategies~\cite {zhang2021bilabel,Tailor} focus on modeling label-feature correlations through label-specific representations enabled by visual attention~\cite{chen2019multi} and transformers~\cite{Tailor}. Beyond monologue settings, MMER in conversations has gained interest~\cite{MELD}, using GCN~\cite{Dialoguegcn} and memory networks~\cite{memory_network} to model dynamic speaker interactions. However, multi-party MMER presents significant challenges as it extends beyond recognizing emotions for individual speakers to handling multiple characters with incomplete modalities. Additionally, the influence of individual personalities on label-feature correlations remains underexplored.

\paragraph{Adversarial Training.}
Adversarial training (AT)~\cite{GAN}, involves two models: a discriminator that estimates the probability of samples, and a generator that creates samples indistinguishable from actual data. This setup forms a minimax two-player game, enhancing the robustness of the model. The technique has since been adapted for CV and NLP applications~\cite{GAN_survey}. For instance, Miyato et al.~\cite{miyato2016adversarial} extended AT to text categorization by introducing perturbations to word embeddings. Wu et al.~\cite{adversarial_relation_extraction} applied it within a multi-label learning framework to facilitate relationship extraction. Additionally, AT has been used to learn joint distributions between multi-modal~\cite{tsai2018learning}. More recently, Ge et al.~\cite{adversarial_masking} applied AT to reduce modal and data biases in MMER tasks. However, Zhang et al.~\cite{Tailor} implemented AT to extract multi-modal commonality and diversity, but suffered a significant loss of modality information due to inadequate cross-modal information fusion.
% The formatting instructions contained in these style files are summarized in
% Sections \ref{gen_inst}, \ref{headings}, and \ref{others} below.