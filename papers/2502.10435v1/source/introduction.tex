\section{Introduction}

\begin{figure}[t] % htbp 代表图片插入位置的设置
    \centering % 图片居中
    \includegraphics[width=\columnwidth]{images/motivation-v3.pdf}
    \vspace{-1.5em}
    \caption{(a) shows the conventional approach for MMER in monologue and dyadic conversations with complete modalities in a uniform representation; (b) depicts our approach for multi-party conversations with incomplete modalities, reconstructing and projecting them into both specificity and commonality representations.}
    \vspace{-1em}
    \label{fig: motivation}
\end{figure}

Emotion recognition from videos is crucial for advancing human-computer interaction and social intelligence. Multi-modal, multi-label emotion recognition (MMER) leverages visual, textual, and acoustic signals to identify multiple emotions (e.g., happy, sad) simultaneously~\cite{MMER1,MMER2}.
Traditional MMER methods, as shown in Figure~\ref{fig: motivation}(a), typically focus on monologue or dyadic settings, assuming all modalities are fully available. However, real-world conversations often involve multiple participants (i.e., \textit{multi-party} scenarios) with incomplete modality data for non-speakers who always lack acoustic and textual signals.

\textbf{\textit{Multi-party} MMER}, a more complex and practical setting, introduces three key challenges. 
%First, incomplete modalities reduce recognition accuracy. Although some approaches~\cite{Dialoguegcn,DialogueCRN} capture conversational context, they generally assume full modality data and are not designed to handle missing inputs. Second, effectively representing diverse modalities is difficult: fusing them into a single feature space often overlooks the unique characteristics of each modality, while fully isolating them may miss cross-modal commonalities~\cite{Tailor,CARAT}. Third, the multi-label nature of emotional expressions requires modeling intricate correlations among different emotions, which can vary across modalities.
Firstly, handling incomplete modalities is a significant challenge. Most existing approaches~\cite{MMER2,Tailor,adversarial_masking} assume full modality availability and independently encode each modality, ignoring missing data. While some methods~\cite{Dialoguegcn,DialogueCRN} model conversational context by capturing speaker dependencies, they struggle in multi-party scenarios where non-speakers often lack key modalities, leading to poor emotion recognition performance. Secondly, representing diverse modalities effectively remains challenging. Current fusion strategies, such as aggregation-based methods (e.g., concatenation, averaging)\cite{MEmoR} and hybrid approaches~\cite{multimodal_survey2}, project modalities into a shared subspace, often neglecting their unique characteristics and reducing discriminative ability. Recent methods~\cite{Tailor} attempt to separate modality-specific and shared features but often suffer from information loss due to inadequate handling of inter-modal correlations. Similarly, methods preserving modality-specific information~\cite{CARAT} may overlook cross-modal commonalities, limiting their ability to fully capture inter-modal relationships.
Finally, multi-label learning presents challenges in modeling robust label correlations and capturing complex interdependency between modalities and labels. Existing approaches~\cite{bloom_filters,ma2021label} often fail to fully exploit collaborative label relationships. Moreover, emotions vary across modalities, and different emotions rely on distinct modality features, further complicating the task.

To address these issues, we propose \textbf{RAMer}, a novel framework designed to tackle the challenges of the \textbf{\textit{Multi-party} MMER} problem. RAMer integrates multimodal representation learning with multi-label modeling to effectively handle incomplete modalities in multi-party settings.

As illustrated in Figure~\ref{fig: motivation}(b), RAMer addresses the challenge of multi-party MMER by following techniques.
To address the challenge of incomplete modalities, we propose an auxiliary task that incorporates external knowledge, such as personality traits, to complement the existing modalities. Leveraging this, we employ modality-level attention mechanisms to capture both inter- and intra-personal features. A reconstruction-based network is utilized to recover the features of any modality by leveraging information from the other modalities.

To represent diverse modalities effectively and capture discriminative features, we design an adversarial network that extracts commonality across modalities while amplifying the specificity inherent to each one. This helps ensure minimal information loss during the fusion process. 

Additionally, to model robust interconnections between modalities and labels, we propose a novel modality shuffle strategy. This strategy enriches the feature space by shuffling both samples and modalities, based on the commonality and specificity of the modalities, improving the model's ability to capture label correlations and modality-to-label relationships.


In summary, the contributions of this work are:
\begin{itemize}
  \item 
  \textit{A Novel Model for the Multi-party MMER Problem.}
We present \textbf{RAMer}, a new model designed to address the Multi-party Multi-modal Multi-label Emotion Recognition problem. \textbf{RAMer} uses adversarial learning to capture both commonality and specificity across multiple modalities, improving emotion recognition even with incomplete modality data.
  \item \textit{Optimization Techniques.} To enhance the robustness of multi-party emotion recognition, \textbf{RAMer} employs contrastive learning to enrich reconstructed features and integrates a personality auxiliary task to capture modality-level attention. We also propose a stack shuffle strategy, enhancing the modeling of label correlations and modality-to-label relationships by leveraging the commonality and specificity of different modalities.
  \item \textit{Extensive Experiments.} We conduct comprehensive experiments on three benchmarks, i.e., MEmoR, CMU-MOSEI, and $M^3$ED, across various conversation scenarios. Results show that \textbf{RAMer} surpasses existing approaches and achieves state-of-the-art performance in both dyadic and multi-party MMER problems. 
\end{itemize}
