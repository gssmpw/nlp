\section{Experiments}
% \label{headings}

\subsection{Experimental setup}

\paragraph{Datasets and Metrics.}
We evaluated RAMer on three multimodal, multi-label benchmark datasets: MEmoR~\cite{MEmoR}, a multi-party conversation dataset that includes personality, and CMU-MOSEI~\cite{Cmu-mosei} and $M^3$ED~\cite{M3ED}, which are dyadic conversation datasets that do not include personality information. The evaluation is conducted under the protocols of these datasets. For CMU-MOSEI and $M^3$ED, we employed four commonly used evaluation metrics: Accuracy (Acc), Micro-F1, Precision (P), and Recall (R). Due to data imbalance in MEmoR, we followed the benchmark's protocol and used Micro-F1, Macro-F1, and Weighted-F1 metrics. 
% Detailed descriptions of the datasets and preprocessing steps are provided in the Appendix.

% MEmor dataset comparison
\begin{table*}[t!]
\begin{center}
\caption{Performance comparison on the MEmoR dataset under primary and fine-grained settings.}
\vspace{-1em}
\parbox{\textwidth}{\centering \small \em With various modality combinations (visual($v$), acoustic($a$)), textual($t$), personality($p$)).}  
\vspace{-1em}
\label{tab:primary} 
\resizebox{0.95\textwidth}{!} % limit the width of the table to  text width
{ 
\begin{tabular}{cc|ccc|ccc}
\hline
\hline
\multirow{2}{*}{Methods} 
& \multirow{2}{*}{Modality} 
& \multicolumn{3}{c|}{Primary}       
& \multicolumn{3}{c}{Fine-grained}  \\ 
\cline{3-8} 

&                           
& Micro-F1 & Macro-F1 & Weighted-F1 
& Micro-F1 & Macro-F1 & Weighted-F1 
\\ 
\hline

MDL with Personality     & $v,a,t,p$                   
& 0.429    & 0.317    & 0.423       
& 0.363    & 0.217    & 0.345       
\\

MDAE                    & $v,a,t,p$                   
& 0.421    & 0.303    & 0.410       
& 0.363    & 0.219    & 0.341       
\\

BiLSTM+TFN             & $v,a,t,p$                   
& 0.470    & 0.310    & 0.454       
& 0.366    & 0.207    & 0.350       
\\

BiLSTM+LMF             & $v,a,t,p$                   
&0.449    & 0.294    & 0.432       
& 0.364    & 0.198    & 0.351       
\\

DialogueGCN            & $v,a,t,p$                   
& 0.441    & 0.310    & 0.425       
& 0.373    & 0.229    & 0.373       
\\

AMER w/o Personality     & $v,a,t$                     
& 0.446    & 0.339    & 0.440       
& 0.401    & 0.246    & 0.379       
\\

AMER                & $v,a,t,p$                   
& 0.477    & 0.353    & 0.465       
& 0.419    & 0.262    & 0.400       
\\ 

DialogueCRN     & $v,a,t,p$
& 0.441    & 0.310    & 0.425 
& 0.373    & 0.229    & 0.373 
\\
TAILOR               & $v,a,t,p$
& 0.341 & 0.287 & 0.326 
& 0.303 & 0.069 & \textbf{0.490} 
\\
CARAT              & $v,a,t,p$
& 0.399 & 0.224 & 0.422 
& 0.346 & 0.090 & 0.483 
\\
\hline
\textbf{RAMer}                   & $v,a,t,p$                  
& \textbf{0.499}    & \textbf{0.402}    & \textbf{0.503}       
& \textbf{0.431}    & \textbf{0.299}   & 0.404       
\\ 
\hline
\hline
\end{tabular}
} 
\end{center}
\end{table*}

% \usepackage{multirow}
\begin{table}[t]
\begin{center}
\caption{Performance Comparison on CMU-MOSEI dataset.} \label{tab:cmu-mosei} 
\vspace{-1em}
\resizebox{\columnwidth}{!}
{ 
\begin{tabular}{c|cccc|cccc}
    \hline
    \hline

    \multirow{2}{*}{Methods} & \multicolumn{4}{c|}{Aligned}      & \multicolumn{4}{c}{Unaligned}    \\ 
    \cline{2-9} 
                                  
                             
    & Acc   & P     & R     & Micro-F1 
    & Acc   & P     & R     & Micro-F1 \\ 
    \hline

    CC                    
    & 0.225 & 0.306 & 0.523 & 0.386    
    & 0.235 & 0.320 & 0.550 & 0.404    
    \\

    ML-GCN                   
    & 0.411 & 0.546 & 0.476 & 0.509    
    & 0.437 & 0.573 & 0.482 & 0.524    
    \\
    
    MulT                    
    & 0.445 & 0.619 & 0.465 & 0.531    
    & 0.423 & 0.636 & 0.445 & 0.523    
    \\
    
                                    
    MISA                    
    & 0.43  & 0.453 & \textbf{0.582} & 0.509    
    & 0.398 & 0.371 & \textbf{0.571} & 0.45     
    \\ 
    
    
    MMS2S                    
    & 0.475 & 0.629 & 0.504 & 0.56     
    & 0.447 & 0.619 & 0.462 & 0.529    
    \\  
    
    HHMPN                  
    & 0.459 & 0.602 & 0.496 & 0.556    
    & 0.434 & 0.591 & 0.476 & 0.528    
    \\ 
    
    TAILOR                  
    & 0.488 & 0.641 & 0.512 & 0.569    
    & 0.46  & 0.639 & 0.452 & 0.529    
    \\ 
    
    AMP                      
    & 0.484 & 0.643 & 0.511 & 0.569    
    & 0.462 & 0.642 & 0.459 & 0.535    
    \\  
    
    CARAT                   
    & 0.494 & 0.661 & 0.518 & 0.581    
    & 0.466 & 0.652 & 0.466 & 0.544    
    \\ 
    \hline 
    
    \textbf{RAMer}                   
    & \textbf{0.505} & \textbf{0.668} & 0.551 & \textbf{0.604}    
    & \textbf{0.469} & \textbf{0.660} & 0.486 & \textbf{0.560}    
    \\ 
    \hline
    \hline
\end{tabular}
}
\vspace{-1em}
\end{center}
\end{table}

\begin{table}[t]
% \vspace{-.3cm}
\begin{center}
\caption{Performance Comparison on the M$^3$ED dataset} \label{tab:m3ed} 
\vspace{-1em}
% \resizebox{\columnwidth}{!}
% { 
\small
\begin{tabular}{ccccc}
\hline
\hline
Methods & Acc   & P     & R     & Micro-F1 \\ 
\hline
MMS2S   & 0.645 & 0.813 & 0.737 & 0.773    \\
HHMPN   & 0.648 & 0.816 & 0.743 & 0.778    \\
TAILOR  & 0.647 & 0.814 & 0.739 & 0.775    \\
AMP     & 0.654 & 0.819 & 0.748 & 0.782    \\
CARAT   & 0.664 & 0.824 & 0.755 & 0.788    \\ 
\hline
\textbf{RAMer}  & \textbf{0.665} & \textbf{0.826} & \textbf{0.759} & \textbf{0.791}     \\ 
\hline
\hline
\end{tabular}
% }
\end{center}
\vspace{-1em}
\end{table}

% \subsubsection{Headings: third level}

\paragraph{Baselines.}
For the MEmoR dataset, we compare RAMer with multi-party conversation baselines, including MDL, MDAE~\cite{MDAE}, BiLSTM+TFN~\cite{TFN}, BiLSTM+LMF~\cite{LMF}, DialogueGCN~\cite{Dialoguegcn}, DialogueCRN~\cite{DialogueCRN}, and AMER~\cite{MEmoR}. We also evaluate its robustness against recent models designed for dyadic conversations, such as CARAT~\cite{CARAT} and TAILOR~\cite{Tailor}. For the CMU-MOSEI and $M^3$ED datasets, we test three categories of methods. 1) Classic methods. CC~\cite{CC}, which concatenates all available modalities as input for binary classifiers. 2) Deep-based methods. ML-GCN~\cite{chen2019multi}, using Graph Convolutional Networks to map label representations and capture label correlations. 3) Multi-modal multi-label methods. These include MulT~\cite{cross_adaptation} for cross-modal interactions, MISA~\cite{MISA} for learning modality-invariant and modality-specific features, and methods like MMS2S~\cite{MMER1}, HHMPN~\cite{MMER2}, TAILOR~\cite{Tailor}, AMP~\cite{adversarial_masking}, and CARAT~\cite{CARAT}.
% 



\begin{figure*}[thb]
    \centering
    \begin{subfigure}[b]{0.2\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/wo_adv2.png}
        \caption{w/o Adversarial Training}
        \label{fig:top-left}
    \end{subfigure}
    \hspace{0.02\textwidth}
    \begin{subfigure}[b]{0.2\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/w_adv2.png}
        \caption{w/ Adversarial Training}
        \label{fig:top-right}
    \end{subfigure}
    \hspace{0.02\textwidth} 
    \begin{subfigure}[b]{0.2\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/wo_rec2.png}
        \caption{w/o RN and CLN}
        \label{fig:bottom-left}
    \end{subfigure}
    \hspace{0.02\textwidth}
    \begin{subfigure}[b]{0.2\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/w_rec2.png}
        \caption{w/ RN and CLN}
        \label{fig:bottom_right}
    \end{subfigure}
    \vspace{-.5em}
    \caption{(a) and (b), the t-SNE visualization of specific and common embedding without/with adversarial training. The red, green, and blue colors represent textual(t), visual(v), and acoustic(a) modalities respectively. Dark colors correspond to specific parts, and light colors denote common parts. (c) and (d), the t-SNE visualization of reconstruction embedding without/with RN and CLN. Different colors indicate different modalities and different saturation represents different emotions.}
    \label{fig:tsne}
    \vspace{-1em}
\end{figure*}

\subsection{Comparison with the state-of-the-arts}
% main experiments
We present the performance comparisons of RAMer on the MEmoR, CMU-MOSEI, and M3ED datasets in Table~\ref{tab:primary}, Table~\ref{tab:cmu-mosei}, and Table~\ref{tab:m3ed}, respectively, with following observations.

1) On the MEmoR dataset, RAMer outperforms all baselines by a significant margin. While TAILOR achieves a high weighted-F1 score in the fine-grained setting, its overall performance is weaker due to biases toward frequent and easier-to-recognize classes. RAMer consistently delivers strong results across all settings, demonstrating its ability to learn more effective representations. 
2) On the CMU-MOSEI and M3ED datasets, RAMer surpasses state-of-the-art methods on all metrics except recall, which is less critical compared to accuracy and Micro-F1 in these contexts. 
3) Deep-based methods outperform classical ones, highlighting the importance of capturing label correlations for improved classification performance. 
4) Multimodal methods like HHMPN and AMP significantly outperform the unimodal ML-GCN, emphasizing the necessity of multimodal interactions.
5) Models optimized for dyadic conversations, such as CARAT, experience a notable performance drop in multi-party settings with incomplete modalities. In contrast, RAMer excels in both scenarios, achieving substantial improvements in Macro-F1 scores on the MEmoR dataset, outperforming CARAT by 0.178 and 0.209, respectively. 


\subsection{Ablation Study}
To better understand the importance of each component of RAMer, we compared various ablated variants. 

As shown in Table ~\ref{tab:ablation}, we make the following observations:

\begin{itemize}
\item The specificity and commonality enhance MMER performance. Variants (1), (2), and (3) exhibit an approximately 0.05 decrease in Micro-F1 performance compared to variant (11). This indicates that jointly learning specificity and commonalities yields superior performance, underscoring the importance of capturing both modality-specific specificity and shared commonality.

\item  Contrastive learning benefits the MMER. The inclusion of loss functions $\mathcal{L}_{scl}$ in adversarial training leads to progressive performance improvements, as evidenced by the superior results of (4). 
%Moreover, (6) validates the rationality of exploring the intrinsic vectors in the latent space.

% \textbf{3)} Variants (3) and (4) perform worse than (12), indicating that jointly learning modality-specific features and commonalities yields better performance.

\item Feature reconstruction net benefits MMER. Variants (5), (6), (7) are worse than (11), and (8) shows an 0.045 decrease in Micro-F1, which indicates that feature reconstruction can improve model performance. When the entire reconstruction process is omitted, the performance of (8) declines even more compared to (6) and (7), confirming the effectiveness of multi-level feature reconstruction in achieving multi-modal fusion.

% \textbf{4)} The Stack Shuffle strategy is effective in improving model performance. When either sample-wise or modality-wise shuffling is removed, variants (8) and (9) perform worse than (11). The performance further declines when both dimensions of shuffling are removed, indicating that stack shuffle aids in feature enhancement.
\item Changing the fusion order leads to poor performance, variants (9) and (10) perform worse than (11). It validates the rationality and optimality of feature fusion.
\end{itemize}
% \textbf{5)} Personality information benefits both complete and incomplete modalities. As shown in Table 5, incorporating personality traits consistently improves RAMer’s performance across all modalities.
 

\begin{table}[t]
\begin{center} 
\caption{Ablation study on the aligned CMU-MOSEI dataset. $\Lambda$ refers to the fusion order, and ${{L}_{sc}}$ represents the specific and common loss. ``w/o $\varepsilon ^m$, $d^m$'' denotes the removal of the encoding and decoding processes.}\label{tab:ablation} 
\vspace{-1em}
\resizebox{\columnwidth}{!}
{ 
\begin{tabular}{lcccc}
\hline
\hline
Approaches                                                                                   
& Acc   & P     & R     & Micro-F1 
\\ 
\hline

% (1) w/o LA                                                                                       
% & 0.491 & 0.653 & 0.519 & 0.582    
% \\

(1) w/o ${{L}_{sc}}$                                                                                      
& 0.474 & 0.610  & 0.517 & 0.573    
\\

(2) w/o $\bm{C}^{\{v,a,t\}}$                                                             
& 0.467 & 0.612 & 0.501 & 0.552    
\\

(3) w/o $\bm{S}^{\{v,a,t\}}$                                                            
& 0.460  & 0.599 & 0.491 & 0.552    
\\
\hline

(4) w/o ${{L}_{scl}}$                                                                               
& 0.492 & 0.651 & 0.540  & 0.588    
\\

(5) w/o $\varepsilon ^m$, $d^m$                                             
& 0.480  & 0.633 & 0.524 & 0.580     
\\

(6) w/o $\bm{\mathcal{X}}_\beta^m$                                              
& 0.481 & 0.641 & 0.538 & 0.590     
\\

(7) w/o $\bm{\mathcal{X}}_\gamma^m$                                               
& 0.485 & 0.620  & 0.523 & 0.586    
\\

(8) w/o $\bm{\mathcal{X}}_\beta^m$ + $\bm{\mathcal{X}}_\gamma^m$ 
& 0.477 & 0.603 & 0.490  & 0.557    
\\ 

\hline

(9) $\Lambda {\{v,t,a,\bm{C}\}}$                                                               
& 0.489 & 0.603 & 0.514 & 0.564    
\\ 

(10) $\Lambda {\{t,a,v,\bm{C}\}} $                                                               
& 0.494 & 0.650  & 0.525 & 0.582    
\\

\hline

(11) \textbf{RAMer}                                                                                       
&\textbf{0.502} &\textbf{0.672} & \textbf{0.545} & \textbf{0.602}   
\\ 
\hline
\hline
\end{tabular}
}
\end{center}
\vspace{-1em}
\end{table}

\subsection{Qualitative Analysis}
\subsubsection{Visualization of Learned Modality Representations.}
To evaluate the effectiveness of reconstruction-based adversarial training in generating distinguishable representations, we used t-SNE to visualize the commonality representations $\bm{C}_{\left\{v,a,t \right\}}$ and specificity representations $\bm{S}_{\left\{v,a,t \right\}}$ learned in the aligned CMU-MOSEI dataset. In figure \ref{fig:tsne}(a), without adversarial training, specificity and commonality are loosely separated, but their distributions overlap in certain areas, such as the lower-right corner. In contrast, Figure~\ref{fig:tsne}(b) shows that with adversarial training, commonality and specificity are clearly separated in latent subspaces, forming distinct boundaries and effectively distinguishing emotions across modalities. 
%However, the different modalities in both spaces are separated and much easier to distinguish. This indicates that adversarial training is effective in aligning distributions across different modalities. 
Figure \ref{fig:tsne}(c) demonstrates that without the reconstruction net (RN) and contrastive learning net (CLN), representations of different modalities are distinguishable, but emotion labels within the same modality remain intermixed. In contrast, Figure \ref{fig:tsne}(d) shows that embeddings of both different modalities and labels are distinctly separable, highlighting that the reconstruction-based module effectively enhances modal specificity. Overall, RAMer accurately captures both the commonality and specificity of different modalities.

% \begin{figure}[thb]
%     \centering
%     \subfigure{
%         \includegraphics[width=0.45\columnwidth]{images/freq_w_adv.pdf}
%         \label{fig:left}
%     }
%     \hspace{0.05\textwidth} % 控制图片间的距离
%     \subfigure{
%         \includegraphics[width=0.45\columnwidth]{images/freq_wo_adv.pdf}
%         \label{fig:right}
%     }
%     \caption{The visualization of modality-to-label dependencies with/without (left/right) adversarial training, and the rows indicate different emotions and the columns denotes different modalities. Darker colors indicate stronger correlations.}
%     \label{fig:modality-to-label}
% \end{figure}

\subsubsection{Visualization of Modality-to-Label Correlations.}
To explore the relationship between modalities and labels, we visualized the correlation of labels with their most relevant modalities. As shown in Figure \ref{fig:modality-to-label}, regardless of the presence of adversarial training, different emotion label is influenced by different modalities. For instance, surprise is predominantly correlated with the acoustic modality, while anger is primarily associated with the visual modality. This indicates that each modality captures the distinguishable semantic information of the labels from distinct perspectives.

\begin{figure}[t]
    \centering
\includegraphics[width=0.6\linewidth]{images/freq_rec_horizontal_wo_adv.pdf}
    % \caption{With AT}
    \vspace{-1.0em}
    \caption{The correlation of modality-to-label dependencies.}
    \label{fig:modality-to-label}
\vspace{-0.5em}
\end{figure}

\begin{figure}[t!] 
% \vspace{-.3cm}
    \centering % 图片居中
\includegraphics[width=0.98\linewidth]{images/case_small.pdf}
    \vspace{-0.6em}
    \caption{An example of the case study results.}
    \label{fig:case study}
    \vspace{-1em}
\end{figure}

\subsubsection{Case Study}
% 
To demonstrate RAMer’s robustness in complex scenarios, Figure~\ref{fig:case study} shows an example of multi-party emotion recognition on MEmoR dataset where specific target persons have incomplete modality signals. The top three rows display different modalities from a video clip, segmented semantically with aligned multi-modal signals. Key observations include: 1) The target moment requires recognizing emotions for both the speaker (e.g., Howard) and non-speakers (e.g., Penny and Leonard). While the speaker typically has complete multi-modal signals, non-speakers often lack certain modalities. TAILOR, relying on incomplete modalities, produced partial predictions as its self-attention mechanisms struggled to align labels with missing features. 2) Limitations of single or incomplete modalities. A single modality, such as text, is often insufficient for accurate emotion inference (e.g., only Howard’s Joy is detectable from text alone). Although CARAT attempts to reconstruct missing information, it fails to capture cross-modal commonality, leading to incorrect predictions. 3)The importance of inter-person interactions and external knowledge is evident in emotion recognition, where inter-person attention enables individuals with incomplete modalities to gain supplementary information from others. Moreover, integrating external knowledge, such as personality traits, enhances emotion reasoning across participants and contexts, emphasizing the synergy between user profiling and emotion recognition. Experimental results validate that RAMer demonstrates superior robustness and effectiveness in these challenging, real-world scenarios.
