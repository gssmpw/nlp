
\section{Problem Formulation}
\label{gen_inst}

In this section, we introduce the notations used and formally define the \textit{Multi-party} Multi-modal Multi-label Emotion Recognition (\textit{Multi-party} MMER) problem.

\paragraph{Notations.}
We use lowercase letters for scalars (e.g., $v$), uppercase letters for vectors (e.g., $Y$), and boldface for matrices (e.g., $\bm{X}$).
A data sample is represented by the tuple $\left( V, P_{t}, S_{r}, E_{t,r} \right)$, where:

\begin{itemize}
    \item $V = \left( \left\{ P_i \right\}_{i=1}^T, \left\{ S_j \right\}_{j=1}^R \right)$ is a video clip containing $T$ persons and $R$ semantic segments.
    \item $\left\{ P_i \right\}_{i=1}^T$ refers to the set of target persons, and $\left\{ S_j \right\}_{j=1}^R$ represents the target segments, each annotated with an emotion moment.
    \item $E_{t,r}$ denotes the labeled emotion for person $P_{t}$ in  $S_{r}$.
\end{itemize}

Each sample is characterized by multiple modalities, including visual ($v$), acoustic ($a$), textual ($t$), and personality traits ($p$). 

For each modality $m \in \left\{ v, a, t, p \right\}$, the corresponding features are represented as $\left( \bm{\mathcal X}^{1}, \bm{\mathcal X}^{2}, \cdots, \bm{\mathcal X}^{m} \right)$, where $\bm{\mathcal X}^{k} \in \mathbb{R}^{l_k \times d_k}$ represents the feature space of the $k$-th modality. Here:
$l_k$ denotes the sequence length, and
$d_k$ denotes the dimension of the modality.

Let $\mathcal{Y} = \left\{ y_{1}, y_{2}, \cdots, y_{\zeta} \right\}$ represent a label space with $\zeta$ possible emotion labels. 



% Each sample is represented by a set of modalities, including visual ($v$), acoustic ($a$), textual ($t$), and personality ($p$). For each modality $m \in \left\{ v, a, t, p \right\}$, the corresponding features are represented as $\left( \bm{\mathcal X}^{1}, \bm{\mathcal X}^{2}, \cdots, \bm{\mathcal X}^{m} \right)$, where $\bm{\mathcal X}^{k} \in \mathbb{R}^{l_k \times d_k}$ is the feature space of the $k$-th modality, with $l_k$ as the sequence length and $d_k$ as the modality dimension.
% Let $\mathcal{Y}= \left\{ y_{1}, y_{2}, \cdots, y_{\zeta} \right\}$ be a label space of $\zeta$ possible emotion labels. 

% \begin{itemize}
%     \item $\bm{X}_{\tau}^{m} \in \bm{\mathcal X}^{m}$ are the features for each modality $m$ in sample $\tau$.
%     \item $Y_{\tau} = \left\{ 0, 1 \right\}^{\zeta}$ is a multi-hot vector indicating the presence ($1$) or absence ($0$) of emotion labels, where $Y_{\tau}^{\upsilon}=1$ if sample $\tau$ belongs to class $\upsilon$, and $Y_{\tau}^{\upsilon}=0$ otherwise.
% \end{itemize}



\paragraph{Multi-party MMER Problem.}
% \begin{definition}[Multi-party MMER]
Given a training dataset $\mathcal{D} = \left\{ \bm{X}_{\tau}^{\left\{ 1, 2, \cdots, m \right\}}, Y_{\tau} \right\}_{\tau=1}^{N}$ with $N$ data samples, where:
(1) $\bm{X}_{\tau}^{m} \in \bm{\mathcal{X}}^{m}$ represents the features for each modality $m$ in sample $\tau$, and
(2) $Y_{\tau} = \left\{ 0, 1 \right\}^{\zeta}$ is a multi-hot vector indicating the presence ($1$) or absence ($0$) of emotion labels, where $Y_{\tau}^{\upsilon} = 1$ indicates that sample $\tau$ belongs to class $\upsilon$, and $Y_{\tau}^{\upsilon} = 0$ otherwise.
% 
% \begin{itemize}
%     \item $\bm{X}_{\tau}^{m} \in \bm{\mathcal{X}}^{m}$ represents the features for each modality $m$ in sample $\tau$,
%     \item $Y_{\tau} = \left\{ 0, 1 \right\}^{\zeta}$ is a multi-hot vector indicating the presence ($1$) or absence ($0$) of emotion labels, where $Y_{\tau}^{\upsilon} = 1$ indicates that sample $\tau$ belongs to class $\upsilon$, and $Y_{\tau}^{\upsilon} = 0$ otherwise.
% \end{itemize}
% 
The \textbf{goal} of the Multi-party MMER problem is to learn a function $\mathcal{F}: \bm{\mathcal{X}}^{1} \times \bm{\mathcal{X}}^{2} \times \cdots \times \bm{\mathcal{X}}^{m} \mapsto \mathcal{Y}$ that predicts the target emotion $E_{t,r}$ for person $P_{t}$ in segment $S_{r}$, leveraging contextual information from multiple modalities.
% \end{definition}


% \stitle{Multi-party MMER.}
% Given a dataset $\mathcal D = \left\{ \bm{X}_{\tau}^{\left\{ 1,2,\cdots,m \right\}},Y_{\tau}\right\}_{\tau=1}^{N}$ with $N$ data samples, where $\bm{X}_{\tau}^{m} \in \bm{\mathcal X}^{m}$, $Y_{\tau}=\left\{0,1 \right\}^{\zeta}$ is the multi-hot vector, and sign $Y_{\tau}^{\upsilon}=1$ indicates that sample $\tau$ belongs to class $\upsilon$, otherwise $Y_{\tau}^{\upsilon}=0$.
% The problem of Multi-party MMER is to learn a function $\mathcal{F}: \bm{\mathcal X}^{1} \times \bm{\mathcal X}^{2} \times \cdots \times \bm{\mathcal X}^{m} \mapsto \mathcal{Y}$ that predicts the target emotion $E_{t,r}$ of person $P_{t}$ in segment $S_{r}$, leveraging the contextual information from multiple modalities.


\paragraph{Discussion.}
It is important to note that the target person $P_{t}$ may have incomplete modality information, meaning they may not simultaneously possess visual, textual, or acoustic representations. This introduces uncertainty in the modality of the target segment $S_{r}$, making the prediction task more challenging.
% It's worth noting that the target person $P_{t}$ may be represented by incomplete modality, meaning it does not simultaneously possess visual, textual, or acoustic representation. This also implies that the modality of the target segment $S_{r}$ exhibits uncertainty. 


\section{Methodology}

Figure~\ref{framework} shows the framework of RAMer, which consists of three components: auxiliary uni-modal embedding, reconstruction-based adversarial Learning, and stack shuffle feature augmentation.
% For the $\tau$-th target emotion $E_{t,r}$, $\bm{X}_{\tau}^{\left\{1,2,\cdots,M \right\}} $ $\in$ $\bm{\mathcal X}^{\left\{1,2,\cdots,M \right\}}$ is the modality feature and $Y_{\tau} \in  \mathcal {Y}$ is the labels.


\subsection{Auxiliary Uni-modal Embedding}
\label{uni_modal_freature_ext}

 To extract contextual information from each modality, we employ four independent transformer encoders~\cite{transformer}, each dedicated to a specific modality $m$.Each encoder consists of $n_m$ identical layers to ensure consistent and deep representation. For real-world multi-party conversation videos with $T$ participants and incomplete modalities, we introduce an optional auxiliary task leveraging personality to complement missing modalities. 
 %A transformer encoder is composed of $n_m$ identical layers to maintain a consistent and deep enough representation capability. Given that a real-world multi-party conversation video that contains $T$ person with incomplete modality, we introduce an auxiliary task that leverages personality to complement other modalities. It is worth noting that the personality in our RAMer is optional. 
 Specifically, we concatenate personality embedding $\bm{\mathcal X}^{p}$ with each modality $\bm{\mathcal X}^{m} \in \left\{v,t,a \right\}$ to enrich the feature space. We then apply the scaled dot-product attention to compute inter-person attention across the person dimension within each segment, and intra-person attention along the segment dimension for each individual~\cite{MEmoR}. This modality-level attention mechanism is designed to enhance the model's emotion reasoning ability by effectively capturing both interpersonal dynamics and temporal patterns within the data.
% This approach not only refines the emotional granularity captured but also aligns the emotional recognition process more closely with the nuances presented by individual personality dynamics and interactions within the scene. 
In this way, we obtain personality enhanced representation $\bm{\mathcal X}_{\alpha}^{m} \in \mathbb{R}^{l\times d}$.

\subsection{Reconstruction-based Adversarial Learning}
\label{Reconstruction_based_Adversarial}

The second component focuses on leveraging multiple modalities by capturing inter-modal commonalities while enhancing the unique characteristics of each modality. To address the limitations of adversarial networks~\cite{GAN,Tailor}, which can result in information loss and difficulty in learning modality-label dependencies in incomplete modality scenarios, we introduce a reconstruction-based approach. This method uses contrastive learning to create modality-independent but label-relevant representations. By reconstructing missing modalities during training, the model ensures more robust performance.


\paragraph{Adversarial Training.} Considering the significance of both specificity and commonality, we employ adversarial training to extract discriminative features. The uni-modal embeddings $\bm{\mathcal X}_{\alpha}^{m}$ are fed to three fully connected networks $f_m$ to extract specificity $\bm{S}^{m}, m \in \left\{ v, a, t\right\}$. In parallel, $\bm{\mathcal X}_{\alpha}^{m}$ are also passed through a reconstruction network, which is coupled with a contrastive learning network, followed by a generator $ G\left ( \cdot;\theta_{G} \right )$ to derive the commonality $\bm{C}^{m}$. Both specificity and commonality are then passed through linear layers with softmax activation in the discriminator $ D\left ( \cdot;\theta_{D} \right )$ that is designed to distinguish which modality the inputs come from. The generator captures commonality representation ${C}^{m}$ by projecting different reconstructed embedding $\bm{\mathcal{X}}_{\gamma }^{m}$ into a shared latent subspace, ensuring distributional alignment across modalities.
%
Consequently, this architecture encourages the generator $G(\cdot; \theta_{G})$ to produce outputs that challenge the discriminator $ D\left ( \cdot;\theta_{D} \right )$ by obscuring the source modality of the representations $\bm{C}^{m}$. That means the generator and discriminator are trained simultaneously in a game-theoretic scenario, which intends to enhance the robustness of the generated features against modality-specific biases. Both the commonality adversarial loss $\mathcal{L}_C$ and the specificity adversarial loss $\mathcal{L}_S$ are calculated by cross-entropy loss.
%

In the shared subspace, it is advantageous to employ a unified representation of various modalities to facilitate multi-label classification. This representation is designed to eliminate redundant information and extract the elements common to the different modalities, thereby introducing a common semantic loss defined as,
% 
\begin{equation}
\vspace{-.1cm}
\small
\label{L_cml}
\mathcal{L}_{cml} = - \sum_{m \in \{v, t, a\}} \sum_{\tau=1}^{N} \sum_{\upsilon=1}^{\zeta} y_{\tau}^{\upsilon} \log \hat{y}_{\tau}^{\upsilon,m} + (1 - y_{\tau}^{\upsilon}) \log(1 - \hat{y}_{\tau}^{\upsilon,m}),
\end{equation}
where $\hat{y}_{\tau}^{\upsilon,m}$ is predicted with $\bm{C}_{m}$ and $y_{\tau}^{\upsilon}$ is the ground-truth label.
In an effort to encode diverse aspects of multi-modal data, an orthogonal loss $\mathcal{L}_{orth}$ is induced to ensure that the encoded subspaces for commonality $\bm{C}^{m}$ and specificity $\bm{S}^{m}$ representations maintain distinctiveness by minimizing overlap.
\begin{equation}
\vspace{-.1cm}
\small
\label{L_orth}
\mathcal{L}_{orth} = - \sum_{m \in \{v, t, a\}} \sum_{\tau=1}^{N} \left\| (\bm{C}_{\tau}^{m})^{T} \bm{S}_{\tau}^{m} \right\|_{F}^{2},
\end{equation}
where $\left\| \cdot \right\|_{F}$ is Frobenius norm. Hereby, the objective of adversarial training $\mathcal{L}_{adv}$ is
\begin{equation}
\small
\label{L_adv}
    \mathcal{L}_{adv} = \lambda_a\left ( \mathcal{L}_{C} + \mathcal{L}_{S}\right )+ \lambda_o \mathcal{L}_{orth}+ \lambda_c \mathcal{L}_{cml} ,
\end{equation}
where $\lambda_a,\lambda_o$ and $\lambda_c$ are trade-off parameters.



\paragraph{Multi-modal Feature Reconstruction.} To reconstruct the features of any modality by leveraging information from the other modalities. We employ a reconstruction network that is composed of modality-specific encoders $ \varepsilon^m$, decoders $d^m$, and a two-level reconstruction process utilizing multi-layer linear networks $g(\cdot)$. Given input $\bm{\mathcal X}_{\alpha}^{m}$ from different modality, three encoders $\varepsilon ^m$ that consist of MLPs are utilized to project $\bm{\bm{\mathcal{X}}}_{\alpha }^{m}$ into latent embedding $\bm{Z}_{\alpha }^{m}$ within the latent space $\mathcal{S}^z$. Subsequently, three corresponding decoders $d^m$ transform these latent vectors into the decoded vectors $\bm{\mathcal{\widetilde{X}}}_{\alpha }^{m}$. 
% we have: 
% \begin{equation}
% \label{Enc_Dec}
%     \bm{Z}_{\alpha }^{m} = \varepsilon^m(\bm{\mathcal X}_{\alpha }^{m};\theta_{m}),\  \mathcal{\widetilde{X}}_{\alpha }^{m} = d^m(\bm{Z}_{\alpha }^{m};\theta_{m}).
% \end{equation}
At the first level of reconstruction network, the intrinsic vector $\bm{\widetilde{D}}^{m}$ that derived from contrastive learning network and semantic features $\mathcal{\widetilde{X}}_{\alpha }^{\left\{v,t,a \right\}\backslash m}$ are concatenated to form the input, which is processed to produce $\bm{\mathcal{X}}_{\beta }^{m}$ used for the second-level reconstruction network. Hereby, the reconstruction network can be formulated as,
\begin{equation}
% \vspace{-.2cm}
    \bm{\bm{\mathcal{X}}}_{\gamma }^{m}=g\left ( g\left ( d^m(\varepsilon^m(\bm{\mathcal X}_{\alpha}^{m};\theta_{m}) ), \bm{\widetilde{D}}^{m} \right ) \right ).
\end{equation}
The obtained three embedding $\bm{\mathcal{X}}_{\alpha }^{m},\bm{\mathcal{X}}_{\beta }^{m}$, and $\bm{\mathcal{X}}_{\gamma }^{m}$ from three distinct feature spaces are fed into fully connected network followed by max pooling.
% as,
We can formulate the reconstruction loss $\mathcal{L}_{rec}$ and classification loss $\mathcal{L}_{cls}^{lsr}$ as,
\begin{equation}
% \vspace{-.1em}
\small
\label{L_rec}
\mathcal{L}_{rec} = \sum_{m=1}^{M} \left( \left\|\bm{\mathcal{X}}_{\alpha}^{m} - \widetilde{\bm{\mathcal{X}}}_{\alpha}^{m} \right\|_{F} + \left\|\bm{\mathcal{X}}_{\alpha}^{m} - \bm{\mathcal{X}}_{\beta}^{m} \right\|_{F} \right),
\end{equation}

\begin{equation}
\small
    \mathcal{L}_{cls}^{lsr}=\lambda_\alpha\mathcal{L}_{B}\left ( \mathcal{S}^\alpha,Y \right )+\lambda_\beta\mathcal{L}_{B}\left ( \mathcal{S}^\beta,Y \right )+\lambda_\gamma\mathcal{L}_{B}\left ( \mathcal{S}^\gamma,Y \right ),
\end{equation}
where $\left\| \cdot \right\|_{F}$ is the Frobenius norm, $\lambda_{\alpha,\beta,\gamma}$ are trade-off parameters, $\mathcal{L}_{B}$ is the binary cross entropy (BCE) loss. 

To capture the feature distributions of different modalities and use them to guide the restoration of incomplete modalities, intrinsic vectors $\bm{\widetilde{D}}^{m}$ obtained through a supervised contrastive learning network~\cite{SCL} are incorporated into the reconstruction network. 
The encoders $\varepsilon^m$ project input $\bm{\mathcal X}_{\sigma}^{m}$ to contrastive embeddings $\bm{Z}_{\sigma}^{m}$, $\sigma \in \left\{\alpha,\beta,\gamma \right\}$. Given a contrastive embedding set $\bm{\mathcal{Z}}=\left\{ \bm{Z}_{\sigma}^{\left\{ v, t, a\right\}} \right\}$, an anchor vector $z_i \in \mathcal{Z}$ and assuming the prototype vector updated during the training process based on the moving average is $\mu_{j,k}^{m}$, where modality $m \in \left\{v,t,a \right\}$, label category $j \in \left [ \zeta \right ]$, label polarity $k \in \left\{pos,neg \right\}$, then the intrinsic vector $\widetilde{D}^{m}$ can be derived from:
\begin{equation}
\delta_j^m = \sum_{k}^{\left\{ pos, neg\right\}} o_{j,k}^m \cdot u_{j,k}^m, \quad  o_{j,k}^m = \frac{\exp(z_i \cdot u_{j,k}^m)}{\sum_{k'}^{\left\{ pos, neg\right\}} \exp(z_i \cdot u_{j,k'}^m)}
\end{equation}
\begin{equation}
    \widetilde{D}^{m}=d^m\left ( \left [ \delta_{1}^{m}, \cdots, \delta_{\zeta}^{m}  \right ]; \theta_m \right ).
\end{equation}
The loss of contrastive learning network is defined as, 
\begin{equation}
% \vspace{-.1cm}
\small
\label{L_scl}
\mathcal{L}_{scl}\left ( i, \mathcal{Z} \right ) = \sum_{i \in \mathcal{Z}} -\frac{1}{|\bm{P}(i)|} \sum_{p \in \bm{P}(i)} \log \frac{\exp(z_i \cdot z_p / \eta)}{\sum\limits_{r \in \bm{A}(i)} \exp(z_i \cdot z_r / \eta)},
\end{equation}
where $\bm{P}(i)$ is the positive set, $\eta \in \mathbb{R}^+$ is a temperature parameter, and $\bm{A}(i)= \bm{\mathcal{Z}}\ \backslash\ {\left\{i \right\}}$. 


\subsection{Stack Shuffle for Feature Augmentation}
\label{stack_shuffle}

To construct more robust correlations among labels and model the complex interconnections between modalities and labels, we propose a multi-modal feature augmentation strategy that incorporates a stack shuffle mechanism. As shown by Figure~\ref{step3} in Appendix, after obtaining the commonality and specificity representations, we perform sample-wise and modality-wise shuffling processes sequentially on a batch of samples. To strengthen the correlations between labels, we first apply a sample-wise shuffle. The features derived from $\bm{C}$ and $\bm{S}^{m}$ are split into $k$ stacks along the sample dimension, with the top elements of each stack cyclically popped and appended to form new vectors.  
%  
Next, a modality-wise shuffle is introduced to help the model capture and integrate information across different modalities. For each sample, features are divided into stacks along the modality dimension, and iterative pop-and-append operations are applied.
% as,
% \begin{equation}
% \small
% \begin{aligned}
% \widetilde{\mathbf{V}}_{[i,:]} &= \begin{bmatrix}
% \widetilde{v}_i^1, \ldots, \widetilde{v}_i^k; \widetilde{v}_i^{k+1}, \ldots, \widetilde{v}_i^{2k};\widetilde{v}_i^{2k+1}, \ldots, \widetilde{v}_i^M
% \end{bmatrix} \\
% &\quad \xrightarrow{\text{mwps}} \widehat{\mathbf{V}}_{[i,:]} = \begin{bmatrix}
% \widetilde{v}_i^{s_1}, \ldots, \widetilde{v}_i^{s_M}
% \end{bmatrix}, 
% \end{aligned}
% \end{equation}
% where $k=\frac{\left| \widetilde{\mathbf{V}}_{[i,:]} \right|}{n}$ is the size of stacks, $\{s_i\}_{i=1}^M$ are the new indices of modalities. 
Finally, the shuffled samples $\mathbf{V}$ are used to fine-tune the classifier $c_{\zeta}$ with the binary cross-entropy (BCE) loss.
\begin{equation}
% \vspace{-.2cm}
\small
\label{L_cls_suf}
\mathcal{L}_{cls}^{suf} = -\frac{1}{N} \sum_{m \in \{v,t,a\}} \sum_{\tau=1}^{N} \Big(Y^{m} \log \big(c_{\zeta}(\mathbf{V}^m )\big)\Big),
\end{equation}
Combing the Eq.\eqref{L_adv}, Eq.\eqref{L_rec} $\sim$ Eq.\eqref{L_scl} and Eq.\eqref{L_cls_suf}, the final objective function $\mathcal{L}$ is formulated as,
\begin{equation}
% \vspace{-.2cm}
% \small
\mathcal{L} = \mathcal{L}_{cls}^{suf} + \mathcal{L}_{cls}^{lsr} + \lambda_r \mathcal{L}_{rec} + \lambda_s \mathcal{L}_{scl} + \mathcal{L}_{adv}
\end{equation}
where $\lambda_r$, $\lambda_s$ are trade-off parameters.

