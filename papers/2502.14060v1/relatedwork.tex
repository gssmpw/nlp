\section{Related Work and Classes Comparisons}
\label{sec:comparisons}
	Various classes of non-convex functions have received attention in the optimization literature. Notable examples include the Polyak-Åojasiewicz (PL) condition~\citep{polyak1963gradient}, Error Bounds (EB)~\citep{luo1993error}, Quadratic Growth (QG)~\citep{anitescu2000degenerate}, Essential Strong Convexity (ESC)~\citep{liu2014asynchronous}, Restricted Secant Inequality (RSI)~\citep{polyak1963gradient}, Restricted Strong Convexity (RSC)~\citep{zhang2013gradient}, and Quasi-Strong Convexity~\citep{necoara2019linear}. All these classes can be seen as gradual weakening of the strong convexity (SC) assumption, till including non-convex functions~\citep{karimi2016linear}.
	
	Other classes of non-convex functions have been introduced as relaxations of convexity, as the quasar-convex functions that we consider~\citep{hardt2018gradient, hinder2020near}.\footnote{Also referred to as weakly quasi-convex.}
	This class is characterized by a parameter $\tau \in (0,1]$ (see the definition in Section~\ref{sec:intro}), where the special case $\tau = 1$ corresponds to the class of star-convex functions \citep{nesterov2006cubic}. 	In the deterministic setting, \citet{guminov2023accelerated, nesterov2021primal} established upper bounds of the order $\mathcal{O}(L\norm{\bx_0 - \bx^*}^2/(\tau^2 T))$ under the assumption of $L$-smoothness and quasar-convexity. More recently, \citet{hinder2020near} showed that this bound is tight, up to logarithmic factors, by deriving a matching lower bound.    
	A subclass of quasar-convex functions, known as Strongly Quasar-Convex (SQC) functions, was also analyzed in \citet{hinder2020near} in the deterministic setting. 
	Notably, the class of strongly quasar-convex functions includes quasar-convex functions that satisfy the quadratic growth condition, as we show later in this section.
	
	Most of the previous work on both upper and lower bounds has focused on the deterministic setting, i.e., when the gradients are exact, or on in the finite-sum setting.
		
	In the stochastic optimization setting,   guarantees for SGD were developed for $L$-smooth $\mu$-PL objective functions by\footnote{The guarantee in \citet{karimi2016linear} needs the additional assumption of bounded stochastic gradients.} \cite{li2021second} and \citet{khaled2020better}, independently and at the same time. They showed that using a step size $\alpha_t = \mathcal{O}(1/(\mu t))$, the guarantee $\mathbb{E}[f(\bx_t)] - f^* = \mathcal{O}(L\sigma^2/(\mu^2 T))$ can be achieved. This is in contrast to the class of $\mu$-strongly convex functions, where the known optimal guarantees are $\mathcal{O}(\sigma^2/(\mu T))$~\citep{agarwal2009information, raginsky2011information}. 
	
	The presence of the gap between the rates of PL and strongly convex functions begs the question if these non-convex optimization rates are optimal or not.
	Yet, to the best of our knowledge, we are not aware of any result for the stochastic case that covers the classes of functions we list above. Our work fills the knowledge gap by providing the first lower bounds for some of the above class of functions and at the same time proving their (almost) optimality.
	
	\paragraph{Classes Comparison.}
	To better understand our results and the relationship with known upper bounds, it is important to take into account the relationships between these different classes. Indeed, it is known that these classes are included one into the other, as proved in Theorem 5 of \citet{karimi2016linear}.
	Hence, we recall the comparison of several non-convex function classes,
	including functions that satisfy Star Strong Convexity (*SC)---we give the precise definitions of the classes in Section~\ref{sec:compare_a_1}:
	\begin{equation}\label{eq:inclusions}
		(\text{SC}) 
		\rightarrow 
		(\text{*SC}) 
		\rightarrow 
		(\text{RSI}) 
		\rightarrow 
		(\text{EB}) \equiv (\text{PL}) 
		\rightarrow 
		(\text{QG}),
	\end{equation}
	where $\rightarrow$ indicates an implication and $\equiv$ indicates an equivalence with potentially difference constants. Note that a more detailed comparison, incorporating class parameters, can be found in \cite{guille2021study}. Moreover, we complement these comparisons with additional results presented in Lemma~\ref{lem:compare} below (proved in Section~\ref{sec:compare_a_2}).
	\begin{lemma}\label{lem:compare}
		Let $\mu >0$ and $\tau \in (0,1]$. We have:
		\begin{itemize}
			\item $\mu\text{-RSI} \subsetneq \mu\text{-EB}$.
			\item $\tau\text{-QC} \cap \mu\text{-QG} \subsetneq (\frac{\tau}{2}, \frac{\mu}{2})\text{-SQC}$.
			\item $\exists f:\R^2\rightarrow\R$, $f\in\mu\text{-EB}$ such that $f\notin a\text{-RSI}$ for any $a>0$.
			\item $\exists f:\R\rightarrow\R$, $f\in \mu\text{-RSI}$ such that $f\notin a\text{-*SC}$ for any $a>0$.
		\end{itemize}
	\end{lemma}
	In particular, Lemma~\ref{lem:compare} shows that the lower bound derived for the class of $\mu$-RSI functions also holds for the class of $\mu$-EB functions. Likewise, the lower bound for the class of $\tau$-QC $\cap$ $\mu$-QG  functions extends to the class of $(\tau, \mu)$-strongly quasar functions. Furthermore, it confirms that the inclusion of RSI functions is strict, as is the inclusion of star strongly convex functions within RSI functions. Equipped with this lemma and the inclusions in \eqref{eq:inclusions}, we can summarize  upper and lower bounds in Table~\ref{tab:complexity}, including our new results.
	
	
	\begin{table}[t]
		\centering
		\caption{Complexity bounds for various classes under the \( L \)-smoothness condition. The notation \( \tilde{\Omega} \) hides a \( 1/\log(2/\tau) \) factor for \( \tau \)-QC and \( \tau \)-QC+\(\mu\)-QG, and a \( 1/\log(2\kappa) \) factor for \( \mu \)-RSI. Here, \( \sigma^2 \) represents the variance of the stochastic gradient. For $\tau$-QC in the last column, \( D \) denotes the diameter of the optimization domain. }
		\label{tab:complexity}
		\resizebox{1\textwidth}{!}{
			\begin{tabular}{c c c c c c | c}
				\toprule
				& \multicolumn{5}{c|}{\textbf{Fast Rate}} & \textbf{Slow Rate} \\
				\midrule
				\textbf{Setting} & \textbf{$\mu$-SC}  & \textbf{$\tau$ QC+$\mu$-QG} & \textbf{$\mu$-RSI} & \textbf{$\mu$-EB} & \textbf{$\mu$-PL} & \textbf{$\tau$-QC} \\
				\midrule
				Upper Bound & 
				\makecell{\(\mathcal{O}(\frac{\sigma^2}{\mu T})\) \\ \textcolor{blue}{\citep{hazan2014beyond}}}& \makecell{
					\(\mathcal{O}(\frac{\sigma^2}{\tau^2\mu T})\)\\(Theorem~\textcolor{blue}{\ref{thm:ub1}})} & 
				\makecell{\(\mathcal{O}(\frac{\kappa\sigma^2}{\mu T})\) \\ (Theorem~\textcolor{blue}{\ref{thm:ub1}})}& 
				\makecell{\(\mathcal{O}(\frac{\kappa^3\sigma^2}{\mu T})\) \\  \textcolor{blue}{\citep{li2021second}}}& 
				\makecell{\(\mathcal{O}(\frac{\kappa\sigma^2}{\mu T})\) \\  \textcolor{blue}{\citep{li2021second}}} & 
				\makecell{\(\mathcal{O}(\frac{D\sigma}{\tau \sqrt{T}})\) \\ \textcolor{blue}{\citep{jin2020convergence}}}  \\ 
				
				Lower Bound & 
				\makecell{\(\Omega(\frac{\sigma^2}{\mu T})\) \\ \textcolor{blue}{\citep{agarwal2009information}}} & 
				\makecell{\(\tilde{\Omega}(\frac{\sigma^2}{\tau^2\mu T})\) \\ (Theorem~\textcolor{blue}{\ref{thm:1}})} & 
				\makecell{\(\tilde{\Omega}(\frac{\kappa\sigma^2}{\mu T})\) \\ (Theorem~\textcolor{blue}{\ref{thm:2}})}& 
				\makecell{\(\tilde{\Omega}(\frac{\kappa\sigma^2}{\mu T})\) \\ (Theorem~\textcolor{blue}{\ref{thm:2}})} & 
				\makecell{\(\Omega(\frac{\sigma^2}{\mu T})\) \\ \textcolor{blue}{\citep{agarwal2009information}}} & 
				\makecell{\(\tilde{\Omega}(\frac{D\sigma}{\tau \sqrt{T}})\) \\ (Theorem~\textcolor{blue}{\ref{thm:3}})} \\
				\bottomrule
			\end{tabular}
		}
	\end{table}