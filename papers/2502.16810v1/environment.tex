\section{A Benchmark for Grounded Persuasion}

\paragraph{Motivations and Challenges}
The first task of our study is to build a consistent and effective evaluation benchmark. However, this effort faces several key challenges. A fundamental challenge is the inherent subjectivity of persuasion, which depends heavily on human feedback. Unlike many other LLM capabilities, such as reasoning and planning, 
which have objective criteria for evaluation, persuasiveness lacks standardized metrics. The persuasiveness of a message is determined by its recipient and can vary significantly with individual preferences and context.  

Another challenge arises from the multifaceted nature of persuasion --- a process broadly studied in fields such as psychology, economics, and communication.  Each field offers distinct models of influence. 
Thus, effective persuasion techniques can differ significantly across domains. In the realm of LLMs, most existing research has focused on political or opinion-based contexts, where persuasion often takes on an adversarial nature. Such contexts complicate evaluation due to the strong influence of subjective beliefs and cognitive biases. For instance, studies like those conducted by \citet{hackenburg2024evaluating} and \citet{matz2024potential} reached differing conclusions regarding the effectiveness of LLMs in personalized persuasion, despite using similar experimental setups. 
Moreover, \citet{durmus2024measuring} noted that opinion-based persuasion is susceptible to the anchoring effect, where participants' initial beliefs strongly influence them, making opinion shifts difficult to measure accurately.
Additionally, persuasion in these settings often resorts to deception, as fact-checking is hard even for experienced humans. \citet{durmus2024measuring} found that prompting models to fabricate information was the most effective strategy under current evaluation. 

These challenges underscore the critical need to go beyond existing studies and develop new benchmarks that evaluate persuasion in more controlled, fact-based contexts.
Thus, we develop an evaluation framework customized for the task of grounded persuasion. 

\textbf{Real Estate Marketing as Testbed}\quad A crucial aspect of our design is to identify a domain that aligns well with grounded persuasion. We select real estate marketing as the primary testbed for several reasons. First, the real estate sector is characterized by high-stakes economic decisions, where potential buyers tend to hold more fact-based, rational beliefs compared to political or emotionally charged domains. In this environment, persuasive language must not only resonate with potential buyers but also remain truthful and contextually relevant. This makes it an ideal setting for testing the principles of grounded persuasion. Second, real estate marketing involves complex decision-making processes, where strong persuasive capabilities can significantly influence outcomes. Experienced realtors earn commissions reflecting the economic value of their persuasion skills. In addition, the potential of generative AI in this domain has been highlighted by a notable anecdote~\citep{reddit_2023} claiming an LLM successfully facilitated a home sale without an agent.
Third, the availability of extensive datasets in the real estate sector enables us to extract valuable domain-specific knowledge for training LLMs and conducting thorough empirical evaluations. 
Leveraging these resources, our benchmark provides a robust and consistent means to assess LLMs' grounded persuasion abilities, offering a scalable solution for evaluating persuasion in high-stakes scenarios.

\textbf{A Realistic Evaluation Interface}\quad
We design the framework to ensure a realistic evaluation interface based on two key criteria. First, we aim to create an immersive experience to gather authentic human feedback on marketing content persuasiveness.  
Second, we need to naturally elicit human preferences to properly test dynamic personalized content generation. As such, we collect real data from more than 50k real estate listings on the market and design a web interface that mimics online platforms, allowing the model to observe buyers' general profiles and behaviors (e.g., recently browsed or liked listing). See Appendix~\ref{app: interface} and \ref{app: dataset} for a full description of the web interface and dataset.

\textbf{Measuring the Fact-based Persuasiveness}\quad
To evaluate model performance in 
grounded persuasion,
we are particularly interested in how buyer behaviors are influenced by the generated content and whether it is factually accurate. Hence, we set an interface to present to the potential buyers each time a single listing along with two descriptions generated by two distinct models, then ask them to choose which description makes them more interested.
We use the Elo score~\citep{elo1967proposed} to measure the relative persuasiveness of text generated by different models.
Additionally, we assess whether the generated content is factually accurate. We defer the detailed experiment setup to \cref{sec: evaluation}.

