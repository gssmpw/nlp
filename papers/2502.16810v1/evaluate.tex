

\section{Evaluations}
\label{sec: evaluation}

\begin{figure*}[h]
    \centering
    \hspace{0.7cm}
    \begin{subfigure}[t]{0.8\linewidth}
        \centering
        \includegraphics[width=\textwidth]{figures/elo_ratings_grouped.pdf}
        \caption{Elo Ratings}
        \label{fig:elo}
    \end{subfigure}
    \hspace{-3.3cm}
    \begin{subfigure}[t]{0.32\linewidth}
        \centering
        \raisebox{0.3cm}{
            \includegraphics[width=\textwidth]{figures/comparison_win_rates_improved.pdf}
        }
        \caption{Win Rates}
        \label{fig:win_rates}
    \end{subfigure}
    \caption{Comparison of model performance using Elo ratings and win rates. Elo ratings represent overall persuasiveness, and win rates reflect relative persuasiveness. Both metrics are based on evaluations by human subjects.}
    \vspace{-0.4cm}
    \label{fig:main_exp}
\end{figure*}

\subsection{Evaluation by Human Feedback}
\label{sec:survey}
To evaluate the effectiveness of listing descriptions generated by different models, we draw inspiration from Chatbot Arena~\citep{zheng2023judging} and conduct an online survey to collect pairwise human feedback comparing different models' outputs.  In summary, systematic evaluation by human feedback shows that our \agentname  clearly outperforms human experts and other model variants, measured by standard metric of Elo ratings \citep{elo1967proposed}. Below, we detail the design of our user survey platform, baseline setup, and evaluation metrics, followed by a report on the human evaluation results.   

\textbf{Quality Assurance}\quad
We focus on the major US city \emph{Chicago} with a highly active housing market. We recruit about 100 participants from the popular \emph{Prolific} platform for human-subject experiments, selecting in-state residents familiar with Chicago's housing market and curating approximately 1,000 listings of varied sizes and price ranges.  Each human subject is tasked with comparing 10 pairs of house descriptions. During each comparison, the human subject sees pictures and all basic information about a house, and then faces two listing descriptions without knowing what methods (human realtor or AI agents) generate them, and is asked to choose which description is preferred, and by how much (see Appendix \ref{app: comparison-interface} for details). Notably, \agentname generates personalized descriptions on the fly for each human subject, based on their preferences elicited while they join the survey (see Appendix \ref{app: preference-interface} for details).  

To ensure feedback quality, we implement several measures: (1) \textit{Screening tests} to confirm participants can extract information from listings and follow specific home search motives (See \cref{app: screening-interface} for details); (2) \textit{Attention checks} using pairs of nearly identical descriptions to ensure participants carefully compare and identify differences; (3) \textit{Control experiments} where participants compare human-written, engaging descriptions against LLM-generated descriptions intentionally prompted to be plain and unappealing, verifying their ability to favor high-quality descriptions; and (4) \textit{Incentives} on the platform, including bonus payments and requests for written reasoning behind choices, to encourage consistent, well-justified feedback.

\textbf{Metrics}\quad  
We adopt the Elo rating score
as our main metric. We use a typical choice of the initial Elo rating as $1000$, scaling parameter $c = 400$, and learning rate $K = 32$, which corresponds to a model win rate:  
$\left[{1 + 10^{(e_1 - e_0)/c}}\right]^{-1}$,  
where $e_0$ and $e_1$ are the Elo ratings of two models being compared.  

\textbf{Baseline Models}\quad  
In addition to our primary persuasion model \agentname, we evaluate several baseline models, including:  
\textit{Vanilla}, an LLM prompted with all attributes of the listing;  
\textit{SFT}, an LLM fine-tuned with supervised training and prompted with all features of the listing;  
\textit{Human}, listing descriptions sourced from Zillow, written by professional realtors;  
\textit{Control}, the model used in the control experiment described earlier.  We also include two ablation models based on \agentname: one that only uses the marketable feature from the Grounding module, the other excludes surprisal features from the Marketing module.
Additionally, we experiment with two LLM variants, GPT-4o and GPT-4o-mini, while keeping the prompt instructions consistent across models.  

\textbf{Results}\quad 
We plot the  Elo ratings of different models in~\cref{fig:elo}. The results reflect a clear trend: while vanilla GPT-4o   performs on par with humans (1052 vs 947), each of our designed module enhancement progressively improves the persuasiveness of the generation, ultimately surpassing human performance with a clear margin (1318 vs 947). Also observe that using GPT-4o to generate listing description does have a clear edge compared to that of GPT-4o-mini. Moreover, we plot empirical win rates among three major competitors (\textit{Vanilla}, \textit{Human} and \agentname)) in \cref{fig:win_rates}, which directly illustrates how much \agentname outperforms the other two.\footnote{Human subjects in our experiments are also asked about how much they prefer one description with a 1-5 rating.}
Please see \cref{sec: case-studies-all} for case studies of our model-generated descriptions with more nuanced observations. 

\subsection{Evaluation through AI Feedback}
Human feedback can be costly, especially as we scale the training and evaluation of our task. In this section, we report our empirical evaluation by using AIs to simulate human feedback based on our data collected from the above human-subject experiments.

\textbf{Simulation Setup}\quad 
We employ an LLM to simulate the responses of buyers in the previous experiment. We use the first $K$ pairwise comparison results as $K$-shot in-context learning samples and prompt the LLM to predict the same buyer's selections for the remaining samples. We also adopt the chain-of-thought prompting format~\citep{wei2022chain} and provide the buyer's rationale comments as the information for in-context learning (see \cref{app: simulation_prompt} for the exact prompt). We use the Sotopia framework~\citep{zhou2024sotopia} to configure this simulation agent with GPT-4o-mini~\citep{openai2024gpt4omini} as the base model. 

\textbf{Metrics}\quad 
We use two metrics to evaluate the reliability of AI feedback compared to human feedback: 1) \textit{Shot-wise Simulation Accuracy (SSA)}: the prediction accuracy averaged across users for each shot; 2) \textit{User-wise Simulation Accuracy (USA)}: the prediction accuracy for each user, averaged across \#(shots). The first metric measures overall simulation accuracy across the entire population, while the second one measures simulation accuracy for each user.  

\textbf{Effectiveness of AI Feedback}\quad
The simulation results under both metrics are shown in \cref{fig: simulation_ssa} and \ref{fig: simulation_usa}. The model achieves $61.6\%$ accuracy across users and exhibits non-trivial ($>50\%$) performance for $79.2\%$ of users, suggesting potential for leveraging AI feedback. However, the accuracy remains unsatisfactory for reliable evaluation. Additionally, the variance in the USA metric is high and increases with more provided shots, underscoring the challenges of personality simulation, as highlighted in \citep{wang2024learning}. While the upward trend in variance is expected due to fewer data points, it highlights the difficulty of predicting user preferences dynamically.

To further understand the limitations of AI-simulated feedback, we conduct a manual analysis of simulation errors. Excluding the $56.1\%$ error cases that lack clearly explainable patterns, we attribute the rest of them to several key error sources in \cref{fig: simulation_diagnostic}:
1) \textit{Length Bias}: Similar to the observation in Chatbot Arena~\citep{zheng2023judging}, the model overly favors longer responses; 
2) \textit{Tie Comments}: Buyers consider the influence from descriptions as indifferent yet still cast confident votes in one of the choices; 
3) \textit{Emergent Preference}: While the model only has access to a buyer's pre-established preference, a buyer's selections in some cases reflect some unspecified preferences or ones in contradiction;
4) \textit{Only Until Late}: Correct predictions about a buyer's selection only emerge after sufficient in-context samples; 
5) \textit{Model Confusion}:
The model's prediction appears random, which indicates that the model may not have sufficient information to simulate such a buyer.
Some of these errors can be mitigated by collecting more selection data from each buyer or improving the preference elicitation process in future work.

\begin{figure*}[h]
    \centering
        \begin{subfigure}[t]{0.3\linewidth}
            \centering
            \includegraphics[width=\textwidth]{figures/user_simulation/shotwise_accuracy.pdf}
            \caption{Shot-wise Accuracy}
            \label{fig: simulation_ssa}
        \end{subfigure}
        \begin{subfigure}[t]{0.3\linewidth}
            \centering
            \raisebox{0cm}{
            \includegraphics[width=\textwidth]{figures/user_simulation/accuracy_histogram.pdf}}
            \caption{User-wise Accuracy}
            \label{fig: simulation_usa}
        \end{subfigure}
    \hspace{0.02\textwidth}
    \begin{subfigure}[t]{0.3\linewidth}
        \centering
        \includegraphics[width=\textwidth]{figures/user_simulation/simulation_diagnostic_exclude_all_wrong.pdf}
        \caption{Error Case Attribution}
        \label{fig: simulation_diagnostic}
    \end{subfigure}
    \caption{Analyses of Simulating Human Feedback with AI Feedback.}
    \label{fig: simulation-accuracy-hist}
\end{figure*}

\subsection{Hallucination Checks}
\label{sec: exp_hallucination_verification}
For  grounded persuasion, it is important to ensure   minimal risks of hallucination. Hence, we evaluate the amount of misinformation in the marketing content through the fine-grained fact-checking test~\citep{min2023factscore}, where we use GPT-4o to assist our hallucination check and set the listing attributes in the dataset as atomic facts. Specifically, we consider two types of factual attributes to check, $X_{\text{hard}}$ and $X_{\text{soft}}$. 
For attributes in $X_{\text{hard}}$, we require the attribute description to be completely accurate (e.g., the number of bathrooms), whereas we allow attributes in $X_{\text{soft}}$ to be roughly accurate (e.g., the home address). 

For a given attribute set $X$ and description $L$, we ask the model to perform the following functions (with the exact prompts in \cref{app: hallucination_experiments_details}): $\text{supp}(L, X)$ outputs the set of attributes in $X$ that are mentioned in the description $L$; $\text{eval}_{\text{hard}}(L, x)$ outputs the binary value whether attribute $x$ is accurately described; $\text{eval}_{\text{soft}}(L, x)$ outputs how accurate $x$ is described with a value on 0-10 scale. We then compute the faithfulness score for attributes in $X_{\text{hard}}$ as follows,
\begin{equation*}
    \text{Faithful}_{\text{hard}}(L) = \frac{\sum_{x \in \text{supp}(L, X_{\text{hard}})}\text{eval}_{\text{hard}}(L, x)}{ |\text{supp}(L, X_{\text{hard}})|},
\end{equation*}
and for attributes in $X_{\text{soft}}$ as follows,
\begin{equation*}
    \text{Faithful}_{\text{soft}}(L) = \frac{\sum_{x \in \text{supp}(L, X_{\text{soft}})}\text{eval}_{\text{soft}}(L, x)/10}{ |\text{supp}(L, X_{\text{soft}})|}. 
\end{equation*}

\begin{figure}
  \centering
  \includegraphics[width=0.4\linewidth]{figures/model_hallucination_comparison.pdf}
   \vspace{-0.3cm}
    \caption{Faithfulness Scores for Hallucination Checks.}
    \label{fig: hallucination_comparison}
\end{figure}

As shown in \cref{fig: hallucination_comparison}, the model-generated descriptions are mostly faithful to listing information with minimal hallucination under both metrics. In contrast, the descriptions from human realtors or SFT model show an even higher level of hallucination. After digging into details, we found that this is  due to  human realtors' (also SFT's) vague description of attributes in $X_{\text{hard}}$ such as the following example,  
``\textit{This 4 bedroom, 3.5 bathroom home offers {\color{red}nearly 2,000} ({\color{blue} 1,828}) sqft of living space...}''.
Our \agentname, however, tends to accurately describe factual attributes whenever mentioned, likely due to its preference to copy from context --- interestingly, this preference seems to be forgotten by the model after supervised fine-tuning on human-written descriptions.  That said, it is debatable whether such vague descriptions of attributes is a true kind of hallucination, though some buyers did complain about this kind of language in the comments of their responses.

