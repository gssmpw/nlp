

\section{A Micro-foundation of Marketing}
Marketing fundamentally is about communicating product information, often selectively, to shape potential buyers' perceptions and influence their purchasing decisions. This process of information signaling, also known as persuasion, has been extensively studied in decision theory and information economics~\citep{spence1978job,arrow1996economics,kamenica2011bayesian,connelly2011signaling}, though typically within stylized mathematical models. However, to enable practical applications of automated marketing in natural language, we need to conceptualize previous mathematical models/findings to develop a framework that is ready to be plugged into today's language generation technology; this is what we embark on next.   

Formally, we represent a generic \emph{product}  $X$ (e.g., a house on sale or an item on Amazon) as an $n$-dimensional vector $X = (X_1, X_2, \dots, X_n)$. Each $X_i$ is called a raw attribute (or simply \emph{attribute}). Attributes capture the factual and measurable characteristics of the product, such as the square footage of a house, floor number, or distance to metro stations. A specific product instance is denoted by vector $\mathbf{x} = (x_1, \cdots, x_n)$ where $x_i \in \mathcal{X}_i$ is the \emph{realized} value of attribute $X_i$. Let $\mathcal{X} = \Pi_{i} \mathcal{X}_i$ be the domain of $\mathbf{x}$.  

Marketers often choose to emphasize certain attractive properties of a product, which are grounded in its underlying raw attributes. We refer to these ``attractive properties'' as signaling features (or simply \emph{features}).  For example, common features in real estate marketing include ``spacious layout'', ``bright room'', ``prime location''. Importantly, features differ from attributes: while some attributes may directly serve as features, features generally capture the more abstract (and sometimes ambiguous) properties.

We denote the feature set as $S = (S_1, \cdots, S_m)$, with a feature vector $\mathbf{s} = (s_1, \cdots, s_m)$, where each value $s_i\in [0,1]$ is normalized to capture the \emph{intensity} of  feature $S_i$ for this product (alternatively, $s_i$ can be interpreted as the probability of having feature $S_i$). For example, $S_i$ could be ``bright room'' and correspondingly $s_i$ denotes the extent to which rooms of the house are bright. In practice, the value of $x_i$ and $s_j$ can be assessed by domain experts. 

 \textbf{Signaling via the Attribute-Feature Mapping}\quad In our model, signaling features convey partial information to influence potential buyers' beliefs, leveraging the inherent cognitive mapping in human natural language. For example, a feature ``bright room'' may imply multiple facts to potential buyers:  a high floor,  modern light system, large room size, sun-side facing, etc., though all in a probabilistic sense. Such partial information carried by the signaling features would shape the buyers' perceptions of the product and influence their behaviors (e.g., to visit the open house).

We denote such a mapping as $\pi: \mathcal{X} \to [0,1]^m$ that maps any product represented by raw attribute values $\mathbf{x}\in \mathcal{X}$ to its feature values $\mathbf{s} \in [0,1]^m$. That is, $\mathbf{s} = \pi(\mathbf{x})$. Sometime, we use $\mathbf{s}(\mathbf{x})$ to emphasize the dependence of $\mathbf{s}$ on the underlying attributes $\mathbf{x}$, and $s_j(\mathbf{x})$ is its $j$-th entry. In practice, this attribute-feature mapping $\pi$ describes the common sense that, given attribute values  $\mathbf{x}$ about the product, to an extent of $s_j$ that we can say the product has feature $S_j$. 

This type of attribute-feature mapping $\pi$ is widely used in both machine learning and economics. In the Bayesian statistics literature,  $X_i$ is called the observable variable whereas  $S_j$ is the latent variable and the mapping $\pi$ is the probabilistic model that captures the dependence between random variables $\mathbf{x}$ and $\mathbf{s}$. In information economics, $X_i$ is a state, $S_j$ is a \emph{signal} of the states, and $\pi$ is known as a \emph{signaling scheme}. 
Generally, signals can be strategically designed to carry certain partial information about the states, and economists have made significant progress in their optimal design to influence the equilibrium outcomes~\citep{kamenica2011bayesian, bergemann2015limits,bergemann2019information}.  
This work extends beyond the economic modeling and analysis of prior studies, which are largely rooted in Bayesian theory, to address the nuanced role of natural language that has previously been abstracted away, and to focus on uncovering the signaling mapping of \emph{common sense} rather than designing new signaling schemes.

\textbf{Marketing Design under Information Asymmetry}\quad The essence of marketing is to take advantage of the information asymmetry between sellers and buyers~\citep{grossman1981informational, lewis2011asymmetric, dimoka2012product, kurlat2021signalling}. This important insight, along with its broader implications in general economic markets, was notably recognized by the 2002 Nobel Economics Prize~\citep{akerlof1978market, spence1978job, stiglitz1975theory, lofgren2002markets}. 
In our case, the seller or seller agent may perfectly know the product attribute values $\mathbf{x}$ and its feature values $\mathbf{s}(\mathbf{x})$ but a buyer enters the market with only a prior belief $\mu$ on the distribution of product attributes in $\mathcal{X}$. Hence, lacking concrete details of the particular product $\mathbf{x}$, the buyer begins with an expected belief on the distribution of features as follows,
 \begin{equation}
     \text{Initial belief of features: } \,\,  \bar{\mathbf{s}}(\mu) = \int_{\mathbf{x} \in \mathcal{X}} \mathbf{s}(\mathbf{x}) d \mu( \mathbf{x}). 
 \end{equation}
Given the asymmetric feature beliefs between the buyer and seller, the purpose of marketing can be described as revealing features, subject to communication constraints, to shift the buyer's belief from $\bar{\mathbf{s}}(\mu)$ towards $\mathbf{s}(\mathbf{x})$ with the goal of increasing the product's attractiveness to the buyer.   

 \textbf{Grounded Persuasion in Natural Language}\quad
 The remaining part of our model is to optimize the persuasiveness of marketing content.  The typical approach in economic theory is to develop models to capture buyers' belief updates and decision-making processes. However, these models are challenging to implement in practice due to the lack of concrete buyer utility functions and behavior modeling. Instead, our idea is to harness the power of LLM in persuasive language generation using instructions and heuristics tailored for grounded persuasion.

At a high level, our approach is to use the attribute-feature mapping $\pi$ to propose and design heuristics to select a subset of features $\mathcal{S}^*$ to emphasize the generated content. We also elicit and provide the user preference $\mathbf{r}$  to use in an LLM prompt $\mathcal{I}^*$ for personalization. We hypothesize that the LLM could approximately solve an implicit optimization problem for persuasive language generation:
 \begin{equation}\label{eq:llm-opt}
     \text{}  L^* =  \argmax_{L\in \mathcal{L}} \Pr(L| \mathcal{I}^*, \mathcal{S}^*, \mathbf{r}) \approx \argmax_{L\in \mathcal{L}(\mathbf{x})} U^{\mathbf{r}}( L ).  
 \end{equation}
That is, the language $L^*$, output by an LLM provided carefully designed prompts $\mathcal{I}^*$, selected features $\mathcal{S}^*$ and user preferences $\mathbf{r}$, could approximately maximize users' preference-adjusted persuasiveness function  $U^{\mathbf{r}}$. Moreover, the generated language $L$ will obey product facts (i.e., is \emph{grounded}), or concretely, be drawn from set $\mathcal{L}(\mathbf{x})$ that includes all languages consistent with the product attribute $\mathbf{x}$. Our later agent implementation and its empirical performance validates this hypothesis to a very good extent. 

Based on this hypothesis expressed in Equation (\ref{eq:llm-opt}), our design boils down to better assist an LLM  in solving this optimization problem by providing effective prompts $\mathcal{I}^*$, a good selection of features $\mathcal{S}^*$ and representation of preferences $\mathbf{r}$. 
In the sequel, we discuss implementation details developed upon this model.

\section{The Agentic Design of \agentname}
This section describes our core design ideas of \agentname, an AI agent that can automatically process multiple levels of marketing information to compose persuasive descriptions for real estate listings and   actively learn to adapt the generated language towards the personal preference of each buyer. 
At a high level, our approach seeks to operationalize previous micro-economic models by practically implementing the following three key ingredients:
\begin{itemize}  
    \item Grounding Module: identify the attribute-feature mapping $\pi$ in accordance with the conventions from the domain-specific marketing problem;  
    \item Personalization Module: elicit and represent   buyer preferences $\mathbf{r}$;  
    \item Marketing Module: select useful yet factual marketing features $\mathcal{S}^*$ based on $\pi, \mathbf{r}$ above to ensure genuine generation of the marketing content. 
\end{itemize}
Armed with these ingredients, we then employ  prompt engineering to solve the $ \argmax_{L\in \mathcal{L}} \Pr(L| \mathcal{I}^*, \mathcal{S}^*, \mathbf{r})$ problem in Equation (\ref{eq:llm-opt}) with a proper prompt $\mathcal{I}^*$. The entire pipeline of our approach can be found in Figure \ref{fig:ai-realtor_pipeline}.  Below we highlight   novel ingredients in the above three modules (blue-colored in Figure \ref{fig:ai-realtor_pipeline}) and defer the  full implementation description (including the prompting engineering details as the green-colored in Figure \ref{fig:ai-realtor_pipeline}) to Appendix~\ref{app: implementation-details}.

\begin{figure*}[h]
    \centering
    \includegraphics[width=0.65\linewidth]{figures/AI-realtor.pdf}
    \caption{Illustration of the Design Pipeline of \agentname.  }
    \label{fig:ai-realtor_pipeline}
\end{figure*} 

\subsection{Grounding Module: Predicting Credible Features for Marketing}
\label{sec: highlight_model}
Our model assumes the existence of attribute-feature mappings in the marketing problem,  which can be used by marketers to influence potential buyers' beliefs and behaviors. However, a key challenge lies in determining how to accurately obtain such mappings. Specifically, we must identify under what conditions it is valid to market a house as possessing a particular feature (e.g., convenient transportation) in order to signal its attractiveness. The traditional approach of acquiring this knowledge from human experts for each house listing is labor-intensive, hence is costly to scale up and difficult to personalize.  
Instead, we take a machine learning approach to uncover the mapping from our experiment dataset. 
While our raw dataset contains no annotation of any signaling feature, we employ LLMs to construct a high-quality feature schema and label the dataset accordingly in preparation for learning the attribute-feature mapping. 
This approach presents a novel unsupervised learning paradigm that harnesses the broad knowledge of LLMs to distill expert-level insights from unlabeled data with minimal human supervision.

\begin{figure*}[h]
    \centering
    \includegraphics[width=0.85\linewidth]{figures/schema_induction_icml_ver.png}
    \caption{Illustration of the inductive feature schema construction pipeline.  }
    \label{fig:highlight_model_pipeline}
    % \vspace{-0.5cm}
\end{figure*}

Our dataset only contains the raw attributes of each listing appearing on the Zillow platform. To learn a high-quality attribute-feature mapping, a key challenge we face is 
that there are too many possible tokens that can serve as the signaling features in the natural language space, and many of these tokens might have duplicate or similar meanings. Without a structured representation of features, the resulting label classes may be too sparse for effective learning. Indeed, we discover that the features obtained by directly prompting an LLM include many similar features while missing some important ones. This would hinder the learning process of attribute-feature mapping. 

To overcome the above challenge of sparse label class, the first step of our approach is to obtain a good representation of feature schema $S$.  
Specifically, we design a more sophisticated prompting strategy to inductively improve the quality and representation of the feature schema.  A high-level sketch of our constructed pipeline can be found in \cref{fig:highlight_model_pipeline}, but the key idea here is to provide LLMs with sufficiently many feature candidates from the dataset and condense them into a hierarchical schema. Finally, we only need to employ a small number of human annotators to evaluate the quality of the generated feature schema to monitor potential hallucinations and make additional refinements based on their feedback. 

With the constructed feature schema above, we then guide the LLM to annotate for each house listing, described by attributes $\mathbf{x}$, whether each feature ${s}_i$ is described in the human-written text.  With LLM assistance, we are able to obtain curated labeled data, after a few standard cleaning procedures (e.g., removing low-quality texts, normalizing and embedding listing attributes).  We then train a neural network to learn the attribute-feature mapping. With a random train-test split of $4:1$ ratio in our dataset, we achieved testing accuracy $69.39\%$ and F1 score $67.43\%$. This accuracy is already high, given the large amount of available features and stochastic nature of the signaling process.

To guarantee the grounded use of signaling features, we implement a deterministic feature selection strategy to only use feature $S_j$ with value $s_j$ (recall its interpretation as the feature intensity)  above some threshold $\alpha$.
As a simple heuristics in our implementation, we set the threshold $\alpha=1/2$ and we will refer to this set of features as, 
\begin{equation}\label{eq:marketable-feature}
     \text{Marketable Features: } \quad  \mathcal{S}_1(\mathbf{x}) = \{S_j:  s_j(\mathbf{x}) \geq \alpha \}.
 \end{equation}

\subsection{Personalization Module: Aligning with Preferences}
\label{sec: user_preference}

This stage seeks to steer the persuasive language generation toward the buyers' preference, which is another crucial objective of grounded persuasion. 
Our solution has two steps. The first step is to effectively elicit useful information to infer a user's preference and structure it in a good representation. On real-world platforms such as Zillow and Redfin, this could be done with mature ML techniques by analyzing each potential house hunter's browsing behaviors on their website. However, we did not have access to these platforms. Fortunately, since the validation of our methods is through systematic human subject experiments, we thus designed preference elicitation questions to elicit human subjects' preferences. Specifically, our web interface programs an LLM to act like a human realtor and to effectively narrow down the features that each buyer cares about. We then survey human subjects to provide a rating $r_j$ on the importance of each feature $S_j$ to them, before they start evaluation tasks. While it certainly could be more sophisticated,  this preference elicitation procedure already suffices to build a strong AI Realtor that very nicely adapts to human preferences, as shown in our experiments. 

The second step is to select a subset of features based on  user preference in order to positively shift the user's belief. 
However, we cannot simply rely on a data-driven machine learning approach for personalization because   real-world marketing texts are not optimized for individual users hence cannot supervise the learning. 
Instead, we use a scoring approach to produce a score for each feature that balances its validity and user preference. Specifically, we adjust the population-level feature scores $\mathbf{s}(\mathbf{x})$ with the user's rating over each feature $\mathbf{r}$ and only select those features whose scores are above some threshold $\alpha$: 
\begin{equation*}
     \text{Personalized Features: }  \mathcal{S}_2(\mathbf{x}) = \{ s_j | s_j(\mathbf{x}) +   c (r_j - r_0) \geq \alpha  \},  
 \end{equation*} 
where the constant $c$ reflects the intensity of personal preference, $r_0$ is the basis rating of each attribute.  We feed these features to LLMs, allowing them to decide which personalized features to emphasize in the final generation.

\subsection{Marketing Module: Capturing Surprisal via RAG}
\label{sec: surprisal}
The last stage is designed to better ground persuasive language generation in factual evidence, problem contexts and localized information in automated marketing. Our design here is inspired by rich marketing strategy research ~\citep{lindgreen2005viral, ludden2008surprise, ely2015suspense}, which have shown that  buyers would derive entertainment utility from \emph{surprising} effects/features and have a deeper impression. 
In our setting of real estate marketing, such surprising features are   those that are relatively rare compared to their surrounding area.
Formally, we determine a set of surprising features based on their percentile in the feature distribution as follows,
 \begin{align*}
     &\text{Surprising Features: } \mathcal{S}_3(\mathbf{x})   =  \{ S_j \subset \mathcal{S}_1: \nonumber \\
     &s_j(\mathbf{x}) \text{ is within $\beta$-quantile  of   distribution } s_j(\mu) \}.
 \end{align*}
This gives the LLMs localized feature information at different levels of granularity obtained through Retrieval Augmented Generation (RAG) \citep{lewis2020retrieval}. Such behavioral economics-driven design proves to be highly effective; citing one of the human subjects in our experiment (see the full description in~\cref{sec: case-for-surprisal}), who was asked about why they liked a listing description (without knowing it was AI-generated): 
\vspace{-3mm}
\begin{myquote}{0.1in}
 \it
 ...Description B specifically points out the rarity of the ample storage and built-in cabinetry in similarly priced listings, making the property stand out.
\end{myquote}
