% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Remove the "review" option to generate the final version.
\usepackage[]{acl}
\usepackage{enumitem}

% Standard package includes
\usepackage{contour}
\usepackage{xcolor}
\usepackage{times}
\usepackage{latexsym}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{mwe}
\usepackage{xcolor}
\usepackage{booktabs}
\usepackage{amsmath,amsfonts,amssymb,amsthm}
\usepackage{multicol}\usepackage{xcolor}         % colors
\usepackage{diagbox}
\contourlength{0.8pt}
\usepackage{makecell}

\newcommand{\orangegloss}[1]{\contour{orange}{\textcolor{white}{#1}}}
\newcommand{\bluegloss}[1]{\contour{blue}{\textcolor{black}{#1}}}
\newcommand{\xhdr}[1]{\vspace{1mm}\noindent{{\bf #1.}}}

\newcommand{\cnote}[1]{\textcolor{red}{$\ll$\textsf{#1 | Chen}$\gg$}}
\newcommand{\snote}[1]{\textcolor{blue}{$\ll$\textsf{#1 | Sarah}$\gg$}}
\newcommand{\fnote}[1]{\textcolor{burgundy}{$\ll$\textsf{#1 | Fede}$\gg$}}

\definecolor{amaranth}{HTML}{E52B50}
\definecolor{garnet}{HTML}{733635}
\definecolor{burgundy}{HTML}{800020}
\definecolor{midnightgreenn}{rgb}{0.0, 0.29, 0.33}
\definecolor{navyblue}{rgb}{0.0, 0.0, 0.5}

\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc} % The default since 2018
\usepackage[english]{babel}
\newcommand\dedicationfont{\fontfamily{qcr}\itshape\mdseries\selectfont}
\newcommand{\wc}{{\dedicationfont \textcolor{navyblue}{Word Confusion}}\xspace}
\newcommand{\remove}[1]{}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out.
% However, it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}
\usepackage{booktabs}
\setlength{\belowcaptionskip}{-10pt}
% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.



\title{Rethinking Word Similarity:\\Semantic Similarity through Classification Confusion}


% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a seperate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author{
  Kaitlyn Zhou, 
  Haishan Gao, 
  Sarah Chen, 
  Dan Edelstein,   
  Dan Jurafsky, 
  Chen Shani\\
  % Kaitlyn Zhou, \hspace{3pt}
  % Haishan Gao, \hspace{3pt}
  % Sarah Chen,\\
  % % \textbf{Federico Bianchi, \hspace{3pt}}
  % \textbf{Dan Edelstein, \hspace{3pt}}  
  % \textbf{Dan Jurafsky, \hspace{3pt}}
  % \textbf{Chen Shani}\\
  Stanford University\\
\texttt{\{katezhou, hsgao, sachen, danedels, jurafsky, cshani\}@stanford.edu} \\  
}

\begin{document}
\maketitle

\begin{abstract}
\input{Sections/0_abstract.tex}
\end{abstract}
\input{Sections/1_introduction.tex}
\input{Sections/2_identity_probe.tex}
\input{Sections/3_probe_vs_cosine}
\input{Sections/5_domain_specificity}
\input{Sections/7_related_work}
\input{Sections/8_discussion_conclusion.tex}

\section*{Limitations}
Our proof-of-concept suggests a promising path where cosine similarity can be replaced by a more sophisticated method that involves self-supervision. However, the boost in performance also comes with some caveats. Because \wc is a supervised classifier, it requires an extra training step that simple cosine doesn't require.  Furthermore, potential users will need a basic understanding of model training and the pitfalls of over-fitting data.

As mentioned earlier, while we implemented \wc as a linear classifier, the method naturally extends to non-linear models. Additionally, various transformations commonly applied to embeddings before measuring distances \cite{Mu2018AllbuttheTopSA} can also be incorporated prior to using \wc, as our method relies on the resulting error signal to assess word similarity. Although non-linear models offer a promising direction, we have not yet examined whether the error function preserves its useful properties in such settings—an important avenue for future work. Introducing non-linearity into the classifier is known to alter its behavior in various ways, but its impact on confusion-based similarity remains uncertain. Further research is needed to evaluate its potential advantages and limitations.

Another key limitation of our approach is that we used three simple implementations of cosine similarity without exploring many possible augmentations to cosine, like normalizing it across the dataset (as was shown to be effective by \cite{timkey2021all}). Further refining both our classifier and cosine similarity implementations could lead to improved results for both,  as well as a deeper understanding of \wc.

Another important limitation of our analysis is that our results might be affected by the choice of seed words and the mechanisms on how we sample the ones used to represent the concepts. Changing seed words can impact the similarities. While we explored different sets of seed words without seeing drastic changes in results, a robust evaluation of the effect of different seed words should be considered in future work.

Lastly, we are not aware if changing the model used to create the embeddings can degrade the performance.

\section*{Ethics Statement}
As with all language technologies, there are a number of ethical concerns surrounding their usage and societal impact. It is likely that with this method, the biases known in contextualized embeddings can continue to propagate through downstream tasks, leading to representation or allocation harms. Additionally, the use of large language models for building contextualized embeddings is expensive and requires time and energy resources. To our knowledge, the method we have developed does not exacerbate any of these pre-existing ethical concerns but we recognize our work here also does not mitigate or avoid them.

\section*{Acknowledgments}
We thank Dallas Card, Nelson Liu, Kyle Hsu, Amelia Hardy, Kawin Ethayarajh, and Tianyi Zhang for their helpful feedback and discussion. This work was supported in part by the NSF via award number IIS-2128145, by the Hoffman–Yee Research Grants Program and the Stanford Institute for Human-Centered Artificial Intelligence, and by the Koret Foundation grant for Smart Cities and Digital Living.

% Entries for the entire Anthology, followed by custom entries
\bibliography{anthology,custom}
% \bibliographystyle{acl_natbib}

\appendix
\input{Sections/9_appendix.tex}
\end{document}