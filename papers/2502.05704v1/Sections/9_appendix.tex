\section{Additional Details}
\label{section:appendix_identity_probe}

Here, we provide additional details about the experimental set-up of \wc.

We used the logistic regression model from the scikit-learn library using a one-vs-rest (OvR) scheme.

\textit{Did you try other ways of creating embeddings?}
We explored alternative methods of creating word embeddings, such as various ways of concatenating layers, but they produced almost identical results. 

\textit{Did you perform any preprocessing?}
We filtered out short (<20 characters) and long (>512 characters) sentences, and matched keywords on token IDs to ensure punctuation and casing are consistent across examples.

\textit{Which hyperparameters did you use?}
Our task is also trained without any use of hyperparameters or special pre-processing steps to help address the concerns pointed out by \citet{liu-etal-2019-linguistic, hewitt-liang-2019-designing}.

\textit{How does this differ from BERT's training task and other works?}
The identity retrieval task differs from the masked LM training task: in masked LM training, the  word identity must be predicted from its \textbf{surrounding context} rather than the embedding itself. Our task is also related to but different from  the ``word identity'' classifier of \citet{zhang-bowman-2018-language} which predicts the identity of a \textbf{neighboring} word.

\textit{What about OOV words?}
\label{section:OOV}
For the error analysis, we used the embedding of the first subtoken. Throughout the rest of the paper, we average the subtokens following \citet{pilehvar-camacho-collados-2019-wic} and \citet{blevins-zettlemoyer-2020-moving}. Our decision to use the first subtoken in the error analysis section was to investigate the impacts of tokenization and perform analysis on token frequencies of the first subtokens when words were OOV.

\textit{In the benchmarking tasks, does your decision to represent a word via the embedding of its first token impact a word’s identifiability?} 
We find this is largely not the case. BERT-Base has a \textasciitilde30,000 token vocabulary, with words that occurred over \textasciitilde10,000 times in its original training data considered in the vocabulary. The word “intermission”, is out-of-vocabulary and is tokenized into “inter” and “\#\#mission”, and we would use the (extremely ambiguous) first token “inter” to represent “intermission”. 

Surprisingly, using only the first token to represent an OOV word had little impact on the identifiability of words, suggesting that these embeddings could capture enough context to differentiate themselves from words with identical prefixes. We find that words tokenized into multiple pieces had lower error rates (4\%) than words that remained whole (17\%) (see figure \ref{fig:tokens}). In other words, the words “intermission”, “interpromotional”, “interwar”, and “interwoven” are distinguishable from one another even though each is tokenized into “inter” and subsequent tokens and only the first token’s embedding is used. That is, the context (namely, the subsequent token “\#\#mission”) sufficiently changed the BERT embedding for “inter” to make it identifiable in context. The fact that single tokens words (which are in vocabulary and generally more frequent) performed worse as a group is likely explained by our prior finding that high frequency words have lower performance on this task (see figure \ref{fig:first_token}).

\begin{figure}
\begin{subfigure}{0.48\columnwidth} 
\includegraphics[width=\textwidth]{Images/Findings1/mistakesbynumberoftokens.png} 
\caption{Tokens}
\label{fig:tokens}
\end{subfigure}  
\hfill 
\begin{subfigure}{0.48\columnwidth} 
\includegraphics[width=\textwidth]{Images/Findings1/mistakesbywordfrequencyoffirsttoken.png} 
\caption{1st Token Freq}
\label{fig:first_token}
\end{subfigure}
\caption{The bar charts above highlight the percentage of errors for words binned by tokens and frequencies of the first subtoken for OOV words. (a) errors by number of tokens (b) errors by frequency of the first token} 
\label{fig:OOV_figure}
\end{figure}


\subsection{Error Analysis}
Although \wc is relatively accurate (> 90\% accuracy), it can still makes mistakes, particularly with highly frequent or polysemous words.
%\footnote{Although not critical to this paper, we also include error analysis on the impacts of tokenization and OOV words in Appendix \ref{section:OOV}.}

\begin{figure}
  \begin{subfigure}{0.48\columnwidth}
  \includegraphics[width=\textwidth]{Images/Findings1/mistakesbywordfrequency.png}
  \caption{Frequency}
  \label{fig:frequency}
  \end{subfigure}
  \begin{subfigure}{0.48\columnwidth}
  \includegraphics[width=\textwidth]{Images/Findings1/mistakesbywordsenses.png}
  \caption{Senses} 
  \label{fig:senses}
  \end{subfigure} 
  \caption{The percentage of errors for words binned by frequency and number of senses.}
  \label{fig: radius vs word frequency}
\end{figure}

\paragraph{Frequency} We find that a word's training data frequency correlates negatively with identifiability. For example, the error rate of words with over 10 million training data occurrences is 42\%, compared to an error rate of 3\% for rare words with between 100 and 1000 training data occurrences.

\paragraph{Polysemy}
One explanation for the poor performance of high-frequency words could be the high polysemy of these words \citep{zipf1945meaning}. Indeed, \wc makes more errors with polysemous words. Very polysemous words (more than 10 senses in WordNet) are 8 times more likely than monosemous words to be misidentified (34\% versus 4\%, see figure \ref{fig:senses}). 

\paragraph{Geometric Space}
Another explanation for lower linear separability of high frequency words is that embeddings of high frequency words are typically more dispersed in geometric space than low frequency words \cite{zhou-etal-2022-problems}. This would most likely lead to difficulty in identifying them with a simple logistic regression model. 



\section{Details and Full Results from Section \ref{sec:validation_experiments}}
\label{appendix:validation_details}
\paragraph{Implementation}
Out-of-vocabulary words here are represented as the average of the words' tokens, following \citet{pilehvar-camacho-collados-2019-wic} and \citet{blevins-zettlemoyer-2020-moving}.
We experiment with a variety of embedding methods, taking the last layer and taking the first subtoken of out-of-vocabulary words and find comparable results.

\paragraph{Similarity Experiments}
For cosine, we took 30 samples of each word and we took the average embedding (this is standard practice). For \wc, we again took 30 samples and we averaged the vectors of the predicted probabilities before taking the target probability values.

\paragraph{Feature Extraction Experiments}
Word sampling for target and seed words is done to speed up the computation, we did not find significant differences with different samples (nonetheless, having at least 1000 embeddings to train \wc is necessary to get good and stable results).

\paragraph{Models used:}
\begin{itemize}
    \item ``bert-base-cased"
    \item ``dbmdz/bert-base-italian-cased"
    \item ``dbmdz/bert-base-french-europeana-cased"
\end{itemize}

\subsection{Seed and Target Words Used}
\label{sec:seeds}
\textbf{Sentiment Classification} 
\begin{itemize}
    \item  \textbf{Task}: Classifying concepts based on sentiment by using the NRC corpus \cite{mohammad-etal-2013-nrc}. Target words: 98 positive and 98 negative words. Seed words: ``positive'' and ``negative''.
    \item \textbf{Corpus}: wikitext-103-v1 from HuggingFace. We remove sentences that are shorter than 15 tokens and longer than 200 tokens.
    \item \textbf{Sampling}: We sample 1000 occurrences of ``positive'' and 1000 occurrences of ``negative''. For each target word, we sample 30 occurrences.
\end{itemize}
    

\textbf{Grammatical Gender in French and Italian} 

Experiment 1: 
\begin{itemize}
    \item \textbf{Task}: Classifying 
concepts by the grammatical gender of nouns. 
\item \textbf{Corpus}: Latest Italian Wikipedia abstracts from DBPedia. We removed sentences shorter than 20 tokens and longer than 100 tokens.
\item \textbf{Sampling}: Target words: 140 Italian nouns. Seed words: 59 Italian masculine and feminine adjectives. For each target word, we sample 30 occurrences. For each seed word, we sample 20 occurrences. Seed and target words have been filtered with respect to frequency. Data comes from Flex-IT~\cite{pescuma2021form}.
\end{itemize}


Experiment 2: 

\begin{itemize}
    \item \textbf{Task}: Classifying 
concepts by the grammatical gender of nouns.
\item \textbf{Corpus}: Latest French Wikipedia abstracts from DBPedia. We removed sentences shorter than 20 tokens and longer than 100 tokens.
\item \textbf{Sampling}: Target words: 201 French nouns. Seed words: 65 French masculine and feminine adjectives. Seed and target words have been filtered with respect to frequency. Data comes form Lexique383~\cite{npbf04}.
\end{itemize}


\textbf{BERT Concept Net Classification Land-Sea} 
\begin{itemize}
    \item \textbf{Task}: Classifying concepts by classes based on the ConceptNet dataset \cite{dalvi2022discovering}, predicting if an animal is a sea or land animal.
    \item \textbf{Corpus}: wikitext-103-v1 from HuggingFace. We remove sentences that are shorter than 15 tokens and longer than 200 tokens.
    \item \textbf{Sampling}: Target words: 64 land or sea animals. Seed words: category names: ``land'' and ``sea''. We sample 1000 occurrences of each seed word. For each target word, we sample 30 occurrences.
\end{itemize}

\textbf{BERT Concept Net Classification Fashion-Gaming} 

\begin{itemize}
    \item Task: Classifying concepts by classes based on the ConceptNet dataset \cite{dalvi2022discovering}, predicting if a concept comes from the fashion domain or the design domain.
    \item Corpus: wikitext-103-v1 from HuggingFace. We remove sentences that are shorter than 15 tokens and longer than 200 tokens.
    \item Sampling: Target words: 29 terms related to fashion or gaming. Seed words: category names: ``fashion, clothes'' and ``gaming, games''. We sample 500 occurrences of each seed word. For each target word, we sample 30 occurrences.
\end{itemize}


% \label{appendix:cosine_results}
% \begin{table*}[]
%     \centering
%     \resizebox{\textwidth}{!}{%
%     \begin{tabular}{lccccc} \toprule
%     \textbf{Experiment} & \textbf{\wc} & \textbf{Cosine 1} & \textbf{Cosine 2} & \textbf{Cosine 3} & \textbf{Ave. Cosine} \\ \midrule
%      Sentiment Classification & \textbf{0.79} & 0.75 & 0.71 & 0.84 & 0.73 \\
%      Grammatical Gender (Italian) & \textbf{0.93} & 0.80 & 0.80 & 0.71 & 0.77 \\
%      Grammatical Gender (French) & 0.85 & \textbf{0.86} & 0.86 & 0.83 & 0.85 \\
%      ConceptNet Domain (Fashion-Gaming) & 0.90 & \textbf{0.93} & 0.93 & 0.90 & 0.92 \\
%      ConceptNet Domain (Sea-Land Animals) & \textbf{0.83} & 0.79 & 0.80 & 0.61 & 0.73 \\
%      \midrule
%      Average & \textbf{0.86} & 0.83 & 0.82 & 0.76 & 0.80 \\ \bottomrule
%     \end{tabular}
%     }
%     \caption{Full results from Section \ref{sec:validation_experiments}. We compare the results of \wc to cosine similarity which we operationalize in one of three ways: we measure cosine similarity in one of three ways 1) the distance between the centroids of the seed words and the target words 2) the average distance each of the target word to the centroid of the seed words 3) the average distance of each target word to each seed word (no centroids)}
%     \label{tab:cosine_results}
% \end{table*}

\section{Capturing Trends in Inflation}
\label{sec:finance_experiment}

In a very preliminary experiment, we also apply {\wc} to a novel social science domain:  representation of economical value or financial meaning. Here we test whether we can recover the financial value of goods from their embeddings and use them to predict changes in those values  -- inflation. We choose inflation since it is easy to quantify and explores a novel domain for this sort of computational meaning. However, the results are preliminary, these trends are extremely complex, and more diverse and domain-specific data could help improve our understanding of applications to this domain.    

We used the California Digital Newspaper Collection (CDNC)\footnote{\url{https://cdnc.ucr.edu/}}, a newspaper corpus that covers the years 1846-2023. We segmented the data into temporal periods based on trends in the Dow Jones Index (DJI)\footnote{\url{https://www.macrotrends.net/1319/dow-jones-100-year-historical-chart}}, aggregating intervals that exhibited the same index fluctuation directions. At the end of the process, we had 17 different data segments, spanning the years 1915-2009. We then further trained the last layer of a 12-layer BERT model for each temporal segment, to create embeddings that capture a particular historical period, with the goal of capturing the temporal change in the value of money.

To quantify the change in the value of money, we trained \wc for every temporal segment of the data. Its goal was to map from the contextual embedding of the `` \$'' token to the (bucketed) monetary value that accompanied that dollar sign. Thus, for each temporal segment, we extract all sentences containing ``\$'', and use the contextual embedding of \$ for predicting the bucketed monetary value from the original sentence. For example, if the sentence is ``The price of gas increased to \$3 per gallon!'', we train a linear regression model to correctly map the \$ embedding to the bucket that contains 3.\footnote{The average correlation coefficient of the trained \wc regressors across the different temporal segments is 0.790, indicating a strong correlation between the  \$ embeddings and their numerical values in context.}

We used all of the temporal \wc classifiers to predict the monetary values of items in a typical basket of goods (e.g., egg, milk, gasoline, car, etc)\footnote{To make the analysis as similar to the real CPI as possible, we used the reported products from the website of the U.S. Bureau of labor statistics, keeping only products that were found in all segments (to avoid biasing our results by using products that were not invented in the past).}.
We then compare these predictions with two measures -- the historical Consumer Product Index (CPI) and the Dow Jones Index (DJI).

The correlation between CPI and DJI, is very high (0.966), indicating they capture similar trends. The correlations of \wc values with CPI (0.187) and DJI (0.169) are positive and significant but low. This low correlation indicates that inflation prediction is a complicated task, which \wc gives us only a very partial window on; the weakness of fit is clear in inspecting Figure \ref{fig:finance}.
While this particular application of our measure is thus inconclusive, the results do suggest that further study involving domain experts could be instructive on whether \wc or similar methods could be used to study financial values in text. 

\begin{figure}[h!]
    \includegraphics[width=0.48\textwidth]{Images/finance.png}
    \caption{Average CPI, DJI, and \wc values between the years 1915-2009. 
    For each temporal segment, the \wc values were calculated using the mean predicted value for each item in the basket of goods. We can see that until the 1970s \wc values followed the increasing CPI trend, but then dropped.  This could be a problem in our method, or could be caused by changes in the training text itself at that period of time, in any case  require further investigation that includes domain experts.}
    \label{fig:finance}
\end{figure}


\section{Details and Full Results from Section \ref{sec:finance_experiment}}
\label{appendix:financial_report}
\paragraph{Data Segmentation}
We segment the temporal data based on the Dow Jones Index  trend\footnote{\url{https://www.macrotrends.net/1319/dow-jones-100-year-historical-chart}} and aggregate intervals with the same fluctuation directions (see Table \ref{tab:DJITrends}).
\begin{table*}[h!]
    \centering
    \begin{tabular}{lc} \toprule
    \textbf{Year} & \textbf{DJI Avg. Annual Change} \\ \midrule
      1915 & 81.49\% \\ 
 1916-1917 & -12.95\%  \\
 1918-1919 & 20.48\%  \\
 1921-1928 & 20.48\% \\
 1929-1932 & -31.67\%  \\
 1933-1936 & 30.02\%  \\ 
 1937-1941 & -7.16\%  \\ 
 1956-1961 & 9.97\%  \\ 
 1962-1972 & 3.86\%  \\ 
 1973-1974 & -22.08\%  \\
 1975-1976 & 12.35\%  \\
 1988-1995 & 13.53\%  \\
 1996-1999 & 22.49\%  \\
 2000-2002 & -10.01\%  \\
 2003-2007 & 11.04\% \\
 2008 & -33.84\% \\
 2009 & 18.82\%  \\ 
 \bottomrule
    \end{tabular}
    \caption{Years aggregated by DJI fluctuation directions}
    \label{tab:DJITrends}
\end{table*}

\paragraph{Data Pre-processing}
We use California Digital Newspaper Collection \cite{CDNC2024data} spanning from 1915 to 2008. The data is pre-processed in the following manner for model continual training:
\begin{itemize}
    \item Convert all text to lowercase.
    \item Remove low-quality text corpuses, defined as those where more than 20\% of the characters are non-alphanumeric symbols or where more than 20\% of words are highly segmented (a single word tokenized into more than two segments), due to poor optical character recognition from scans of historical documents. 
    \item The dataset of each training segment has 10,240 training documents, 1280 test documents and 1280 validation documents, each containing an average of 350 tokens.
\end{itemize}

\paragraph{Continual Training}
We fine-tune the last layer of the 12-layer bert-base-uncased model, which comprises 7,087,872 trainable parameters. We use a learning rate of $2 \times 10^{-5}$ and a weight decay of 0.01. Each model takes 3 hours to fine-tune with Google Cloud T4 GPUs.\footnote{\url{https://cloud.google.com/compute/docs/gpus\#t4-gpus}}. 
\paragraph{Training \wc}
We extract 2,000 occurrences of the "\$" token from each segment. Each token is part of a 128-character window and must be followed by a numeric value. We get the contextualized embedding of the tokens using the fine-tuned models and bucketize the 2000 numeric values into 60 buckets to reduce noise in the data. We then train a linear regression for each time segment.

\paragraph{Calculating CPI}
To calculate the Consumer Price Index (CPI), we construct a basket of goods consisting of the following items: \{"car", "rent", "hat", "wine", "jewelry", "shirt", "chicken", "milk", "furniture", "egg", "shoe", "pork", "gasoline", "beef", "coffee", "bus"\}. We identify occurrences of the "\$" token that are followed by a numeric value and keep those where terms from our basket of goods appear within a 20-word window. The numeric values are then masked, and the trained \wc classifier is used to predict the value associated with each "\$" token.

\paragraph{Models used:}
\begin{itemize}
    \item ``bert-base-uncased"
\end{itemize}

\paragraph{Rate of change in CPI, DJI, and \wc values:}
Rate of change in \wc values compared with the rate of change in CPI and DJI values (the mean annual change in values per temporal segment). The correlation between the change in CPI and DJI values is almost zero (-.006), suggesting they capture quite different trends. The correlation of CPI change and \wc change is negative (-0.226), and the correlation between the changes in DJI and \wc values is positive and significant (0.387). 

