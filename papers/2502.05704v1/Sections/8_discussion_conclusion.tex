\section{Discussion and Conclusion}
% In this paper, we reframe the task of semantic similarity from one of measuring distances to one of classification confusions. Our method is self-supervised and allows for researchers and downstream practitioners to measure the similarity of words with ease. We illustrate the performance of the identity probe on a number of word similarity tasks as well as provide examples of other tasks the identity probe can generalize to. In addition to the identity probe's high performance, we also find that the identity probe is able to produce asymmetric measures of similarity, opening a new line of future research. Lastly, we conclude with theoretical intuition of how a simple reframing of the problem results in new decision boundaries of similarity. We're excited for the NLP and broader community to use the identity probe as a measure of semantic similarity and discover other new ways in which the probe can be used for classification tasks.


In this paper, we reframe the task of semantic similarity from one of measuring distances to one of classification confusion. This formulation highlights the context-dependency of similarity judgments, meanwhile avoiding the pitfalls of geometric similarity measures \cite{evers2014revisiting}.

This new framing of semantic similarity in terms of classification confusion introduces new properties that are inspired by cognitive models of similarity \cite{tversky1977features} and accounts for the asymmetric nature of semantic similarity, captures different aspects of both similarity and multi-faceted words and ofter a measure that has interpretability benefits. 

Our proof-of-concept method, \wc, demonstrates the practical applicability and effectiveness of this reframing. Empirical results show that it outperforms cosine similarity on standard datasets.
For computational social science or cultural analytics applications, \wc can serve as a way to learn to represent words using target features (e.g., ``school'' in terms of \{\textit{positive}, \textit{negative}\}, and can be used to trace the meaning of a word as a function of time (like the word ``r\'{e}volution'').

The theoretical underpinnings of \wc allow it to learn complex word identity boundaries and capture the directional nature of similarity, offering a richer and more flexible framework for understanding word meanings. 

%To conclude, we reframe semantic similarity using classification confusion, to align it better with psychology literature. We implement a proof-of-concept framework that highlights the desired aspect(s) of a multi-faceted word by classifying it based on better-defined adjacent terms. 

While we implemented \wc as a linear classifier, the method naturally extends to capturing non-linear relationships among embedding components by replacing the linear projection with neural networks. Investigating whether the error function preserves its useful properties in non-linear settings remains an open question for future work.

While our experiments are preliminary and the space of possible similarity measures is enormous, we hope this reimagining of semantic similarity will inspire the development of new tools that better capture the multi-faceted and dynamic nature of language, advancing the fields of computational social science and cultural analytics and beyond.



