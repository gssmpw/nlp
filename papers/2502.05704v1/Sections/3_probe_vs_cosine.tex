\section{Theoretical Intuitions}

In this section, we discuss the importance of word identifiability and how it enables the core mechanics of \wc, and discuss some theoretical differences between \wc and cosine similarity.  

\subsection{The Identifiability of Contextualized Word Embeddings}

\wc depends on the ability of a classifier to identify a word based on its contextual embedding; here we confirm that this classification task is indeed solvable, and examine some error cases to better understand it.

While contextualized word embeddings vary in their representation based on context, prior work showed that tokens of the same word still cluster together in geometric space \cite{zhou-etal-2022-problems}.

To test whether these boundaries are indeed learnable, we test how well a model can identify a contextualized word embedding after seeing one other example of the same word's contextualized embedding. We randomly sampled 26,000 words from English Wikipedia, trained 1000-class one-shot classifiers, and tested them on 10,000 examples (ten examples per class). Indeed, we found that the average test set accuracy on all our classifiers is 90\%, suggesting that the contextualized word embeddings are highly \textit{identifiable}. Thus, given an embedding, it is possible to identify its symbolic representation. See \ref{section:appendix_identity_probe} for experimental details.

\subsection{Differences Between \wc and Cosine Similarity}

\wc and cosine similarity give different kinds of distances.
We can see one way to visualize this in Figure~\ref{fig_concept_prob_boundaries}. Note the differences in the decision surface between {\wc} and cosine similarity: cosine boundaries emerge from the origin, whereas boundaries from \wc are not restricted in the same way.

Using a linear classifier in \wc also introduces new parameters that transform the input vectors into a different space, effectively redefining the notion of distance compared to the raw embeddings. To see this, consider two normalized 2-dimensional vectors, $x$ and $y$, and a real linear transformation, $A$ applied to each. Using the singular value decomposition (SVD) of $A= U \Sigma {V}^\intercal$, the singular values of $A$ (${\sigma}{u}{{v}^\intercal}$) allow us to rewrite the transformed vectors $Ax, Ay$ as ${\sigma_1}{u_1}{{v_1}^\intercal}{x_1} + {\sigma_2}{u_2}{{v_2}^\intercal}{x_2}$ and  ${\sigma_1}{u_1}{{v_1}^\intercal}{y_1} + {\sigma_2}{u_2}{{v_2}^\intercal}{y_2}$ respectively. 

The cosine distance between the transformed vectors is $1-({\sigma_1}^2({v_1}^{\intercal}x_1)({v_1}^{\intercal}y_1) + {\sigma_2}^2({v_2}^{\intercal}{x_2})({v_2}^{\intercal}{y_2}))$ compared to the original cosine distance $1 - (x_1y_1 + x_2y_2)$.\footnote{Terms cancel out as ${\sigma_1}{\sigma_1}=1$ and ${\sigma_1}{\sigma_2}=0$.} Similarly, the Euclidean distance between the transformed vectors is ${\sigma_1}{u_1v_1}^\intercal({x_1-y_1}) + {\sigma_2}{u_2v_2}^\intercal({x_2-y_2})$ compared to the original Euclidean distance of $(x_1-y_1)^2 + (x_2-y_2)^2$. In both cases, the distances between the two transformed vectors differ from the original vectors based on the linear transformation applied.

In other words, a linear transformation introduces additional parameters, allowing the model to reshape the geometry of word vectors and adjust the distances between words and their predicted semantic similarities.

Our method also shares some properties with cosine similarity.  Because linear classifiers learn a weight vector for each category that represents a kind of prototype of the category, the weight vectors learned by our classifier will be approximations of each word vector itself. Like cosine, our method thus computes similarity as the dot product between the word vector input and an errorful representation of the word vector encoded in the weights of the final classifier. What makes this approach effective is its reliance on small yet informative prediction errors that encode a meaningful signal, making the confusion matrix a source of linguistic insight.


% will geometry of the embeddings, giving a model additional parameters to represent targeted words and 
% we discuss differences between \wc and cosine similarity, arguing that feature-based similarity can produce more flexible decision boundaries, capture asymmetrical relations, highlight specific aspects of the analyzed word, and output more meaningful scores. 



\subsection{Advantageous Properties of \wc as a Similarity Measure}

Using a trainable linear classifier and analyzing its error signal for word-similarity purposes introduces a few advantages for measuring similarity:

\xhdr{Asymmetry} Human perceived similarity is not symmetric \cite{tversky1977features}. Yet cosine, like many distance functions commonly used to calculate semantic similarity, is symmetric. One of the advantages of using a model's confusion matrix for measuring semantic similarity is that these scores are \textit{asymmetric}; i.e., $p_{ij} \not= {p_{ji}}$. For example, \wc assigns lower probabilities for \textit{animal} being predicted as \textit{cat} than for \textit{cat} being predicted as \textit{animal}. The ability to measure asymmetric semantic similarity opens interesting new directions of understanding semantic similarity which are not possible with cosine.

\xhdr{Domain Adaptability} The fact that {\wc} requires training leads to more flexible similarity measures. Class selection enables measuring the semantic similarity of words relative to just a \textbf{subset} of features; we propose that this is particularly useful for practitioners who are interested in computing the similarity of words within a niche domain (we explore this in section \ref{sec:domain_spec}). 
% One limitation of our method is that it is self-supervised and would need to be retrained for out-of-domain words, which should still be quick given it is training a logistic regression 

\xhdr{Interpretability}
Probabilistic similarity measures have the advantage of being more interpretable for humans than non-probabilistic measures like cosine  \cite{sohangir2017improved}. Using a classifier's confusion matrix gives similarity scores that represent real probabilities. Moreover, since the choice of classifier classes is an implementation decision, one could choose them based on desired aspects of a word for a task. For example, we could interpret attitudes toward school by asking for the confusion matrix for the word ``school'' with a sentiment analysis classifier that contains the classes \{\textit{negative}, \textit{positive}\}, or the classes \{\textit{fun}, \textit{work}\}. 