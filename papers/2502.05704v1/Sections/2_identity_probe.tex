\section{Introducing \wc}
\label{sec:identity_probe}


Our method begins by defining a set of words, or features. For example, we might choose the set $W=\{red,\ green,\ blue\}$ if we wanted to study similarity related to colors. These words will act as features that can be selected by the analyst to focus on a particular dimension or question.


Our process then has two phases: training and inference. In training (illustrated in part (a) of Figure \ref{fig:word_confusion})) we extract from a corpus a set of sentences containing each of these words, such as \textit{``The sunset painted the sky a brilliant shade of red''} for the word ``red''. We then use BERT to extract the contextual embeddings of these feature-words, and train a classifier to map from an embedding to its corresponding word identity. The classifier's training objective is to correctly classify the embedding to the word that corresponds to it. 

More formally, given a chosen set of word $W$ and embeddings $\{e_1, e_2 .... e_i\} \in E$ that correspond to word identities $\{w_1, w_2, ..., w_i\} \in W$, we train a logistic regression classifier on all pairs of $\{e_i, w_i\}$.

At inference (part (b) in Figure \ref{fig:word_confusion}), we wish to define the semantic similarity of a word in terms of the classifier's classes, which can be thought of as features.\footnote{Thus the choice of a different set of classes is a way of selecting different features  to describe the input word.} Now suppose we would like to compute the similarity of the new word ``burgundy'' to various colors. We extract the contextual embedding of  ``burgundy'' given the sentence \textit{``Burgundy is a deep reddish-brown shade inspired by wine''}, and  use the trained classifier to compute the probability that the \textit{``burgundy''}-embedding corresponds to each class $W=\{red,\ green,\ blue\}$. We then use the classifier's confusion matrix to understand which primary colors burgundy is similar to.  For example, the similarity of ``burgundy'' to  ``red'' is the probability our classifier assigns to the class \textit{``red''}.

More formally, the probability distribution predicted by the model, $\vec{p_j} \in \mathbb{R}^{|W|}$, is used to quantify the semantic similarity between $w_j$ (Burgundy) and each $w_i, \forall w_i \in W$ (in this case $W=$\{{red},\ green,\ blue\}).   Thus:
\begin{eqnarray}
\text{sim}_{\text{WC}}(w_i,w_j) \stackrel{\mathrm{def}}{=}
 p(w_i|e_j)
\end{eqnarray}
The set of distractor words chosen to train the initial classifier thus act as features that can be selected by the analyst to focus on a particular dimension or question.
 
Note that as with the example ``burgundy",  the input word at inference can be out-of-vocabulary with respect to the classifier, or the target word can be one of the classifier's classes (in which case we ignore the probability it assigns to that word and use the other $N-1$ features.)

 


\label{sec:cosine_vs_probe}
\begin{figure*}[h!]
    \centering
    \includegraphics[width=0.9\textwidth]{Images/decision_surface.png}
    \caption{Differences in decision boundaries between \wc and cosine similarity. The $x$ and $y$ axes represent two dimensions of an artificially constructed set of data points. Note how cosine similarity's boundaries originate from the origin whereas {\wc}'s are not limited in the same way.}
    \label{fig_concept_prob_boundaries}
\end{figure*}

\subsection{Benchmarking \wc}
\label{sec:initial_eval}

The intuition behind \wc is that if it struggles to distinguish between contextual embeddings of \textit{burgundy} and \textit{red}, this could indicate they are similar. To test this hypothesis, we use \wc on three semantic similarity benchmarks. For each task, we trained a model using sentences from English Wikipedia. Our classes contained all the words from the benchmark. We then built word embeddings by averaging the last four hidden layers of BERT-base-cased (Details in Appendix~\ref{appendix:validation_details}). 

To calculate the similarity between two words $w_i, w_j$, we first extract all the sentences containing $w_i$ from English Wikipedia. We average the contextual token embeddings of $w_i$ using these sentences. This average token embedding was the input to the trained classifier (with classes containing all the words in the benchmark). We then use the probability \wc assigned to $w_i$  to set the similarity score between $w_i$ and $w_j$. We tested three benchmarks:

\begin{itemize}
\itemsep 0pt
    \item \textbf{MEN} contains 3000-word pairs annotated by 50 humans  based on their ``relatedness'' \cite{agirre-etal-2009-study}. For example \{berry, seed\}, \{game, hockey\}, and \{truck, vehicle\} received high relatedness scores, where \{hot, zombi\}, \{interior, mushroom\}, and \{bakery, zebra\} received low scores. 
To approximate human agreement, two annotators labeled all 3000 pairs on a 1-7 Likert scale; their Spearman correlation is 0.68, and the correlation of their average ratings with the general MEN scores is 0.84. 
%This high correlation suggests that MEN contains meaningful semantic ratings.
\item \textbf{WordSim353 (WS353)} contains 2000 word-pairs along with human-assigned association judgements \cite{bruni2014multimodal}. For example \{bank, money\}, \{Jerusalem, Israel\}, and \{Maradona, football\} received high scores whereas \{noon, string\}, \{sugar, approach\}, and \{professor, cucumber\} were ranked low. 
The authors report an inter-annotator agreement of 84\%. 

\item \textbf{SimLex} contains 1000 word-pairs and directly measures similarity, rather than relatedness or association \cite{hill-etal-2015-simlex}. The authors defined similarity as synonymy and instructed their annotators to rank accordingly. For example \{happy, glad\}, \{fee, payment\}, and \{wisdom, intelligence\} received high relatedness scores, where \{door, floor\}, \{trick, size\}, and \{old, new\} received low scores. 
Inter-rater agreement (the average of pairwise Spearman correlations between the ratings of all respondents) was reported as 0.67.

\end{itemize}

\begin{table}[]
    \centering
    \begin{tabular}{l|ccc} \toprule
         \diagbox[dir=NW]{{Method}}{{Dataset}} & {MEN} & {WS353} & {SimLex}\\ \midrule
         Cosine & {0.59} & 0.54 & 0.39 \\
         \makecell[l]{{\dedicationfont \textcolor{navyblue}{Word}}\\{{\dedicationfont \textcolor{navyblue}Confusion}}} & \textbf{0.66} &\textbf{0.67} & \textbf{0.44} \\ \bottomrule
    \end{tabular}
    \caption{Spearman's $\rho$ correlation between \wc and cosine similarity results as compared to humans. These three benchmarks focus on slightly different aspects of word similarity. We measure the correlation between human scores and cosine similarity between the language model embeddings versus {\wc}'s similarity scores. As can be seen, our method slightly outperforms cosine similarity.}
    \label{tab:semantic_similarity_results}
\end{table}

Across MEN, WS353, and SimLex, \wc slightly outperforms cosine similarity. This illustrates the meaningfulness of classification confusions, compared to cosine similarity. We note that our probability distribution spanned only the classes we chose in advance (all of the words in the dataset), which yields a different vocabulary compared to the original language model.


