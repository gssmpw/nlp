\section{Related works}
\label{sec2}
\subsection{Safety-Critical Driving Scenario Generation}

In recent years, research on generating safety-critical driving scenarios has gained attention as a means to evaluate and validate autonomous driving models. This field has attracted significant attention from various stakeholders, with regulatory bodies like NHTSA establishing frameworks for testable cases and pre-crash scenarios __**Müller, "Safety-Related Test Cases"**__, while industry leaders such as Waymo focusing on reconstructing real-world fatal crashes in simulations __**Zhang, "Safe Driving Scenario Generation"**__. Within the academic community, researchers have explored various approaches to advance this field __**Gao et al., "Scenario-Based Autonomous Driving"**__. For instance, __**Kim and Choi, "Reinforcement Learning for Scenario Generation"**__ reformulated scenario generation as a reinforcement learning problem, where the generation model acts as an agent and the driving algorithm to be evaluated serves as the environment. SceneGen __**Li et al., "Scene-Based Neural Autoregressive Model"**__ proposed a neural autoregressive model to sequentially insert actors into the scene while synthesizing their states. However, SceneGen only focuses on generating static traffic scene snapshots without considering subsequent vehicle trajectories. STRIVE __**Wang and Lin, "Traffic Motion Model for Scenario Generation"**__ addressed this limitation by introducing a learned traffic motion model to generate safety-critical vehicle trajectories. More recently, TrafficGen __**Li et al., "Autoregressive Neural Architecture for Traffic Scene Generation"**__ developed an autoregressive neural architecture to generate both realistic initial states and long-term trajectories. To improve the controllability of scenario generation, diffusion models with guided sampling __**Kim et al., "Diffusion Models with Guided Sampling"**__ have been leveraged to enable flexible control over generated scenes. Despite these advances, most existing approaches focus primarily on scenario generation and testing without effectively utilizing generated scenarios for closed-loop training of AV agents.


LLMs have recently emerged as powerful tools for enhancing traffic scenario generation through their superior natural language understanding and reasoning capabilities. CTG++ __**Song et al., "CTG++: A Framework for Traffic Scenario Generation with LLMs"**__ pioneered this direction by employing LLMs to translate user queries into differentiable loss functions, which then guide a scene-level diffusion model to generate query-compliant traffic scenarios. Along similar lines, ChatScene __**Kumar et al., "ChatScene: Generating Textual Descriptions for Safety-Critical Scenarios"**__ utilized LLMs to generate textual descriptions for safety-critical scenarios, which are then decomposed into sub-descriptions for behavior, geometry, and spawn positions. These descriptions are matched with a pre-built code snippet database to generate executable Scenic code for CARLA simulation. TTSG __**Wang et al., "TTSG: Traffic Scenario Generation with LLMs"**__ leveraged LLMs for road selection and agent planning, supporting diverse traffic scene generation without requiring predefined spawning points. These works demonstrate the potential of LLMs in autonomous driving, especially their capabilities to understand complex traffic contexts and generate corresponding descriptions. Inspired by these advances, our work aims to further leverage VLMs and LLMs to analyze agent behavior patterns, enabling personalized safety-critical training curricula to enhance autonomous driving safety.


\subsection{Curriculum Learning in Autonomous Driving}

Inspired by the way humans learn progressively from simpler to more complex tasks, curriculum learning __**Schmidhuber et al., "Curriculum Learning"**__ involves structuring the learning process in stages based on task difficulty or the learner’s capabilities. This methodology has shown great potential in accelerating training and improving performance across various machine learning tasks __**Bengio et al., "Deep Learning Methods for Curriculum Learning"**__. In autonomous driving, curriculum learning has emerged as a promising approach to tackle the challenge of learning complex driving behaviors __**Liu et al., "Curriculum Learning for Autonomous Driving"**__. For instance, __**Kim et al., "End-to-End Curriculum Learning Framework"**__ proposed an end-to-end curriculum learning framework that divides reinforcement learning into multiple stages of increasing difficulty to guide the agent towards better driving policies. ____ further demonstrated how to design effective curricula by gradually increasing task complexity in terms of traffic density, driving routes, and spatial constraints. Their curriculum starts with basic skills like lane keeping in sparse traffic, then progressively introduces more complex tasks such as collision avoidance and lane change in denser scenarios. 
However, most works rely on manual curriculum design, which may require significant expert knowledge and make it challenging to generalize across different driving scenarios. To address these limitations, recent research has begun exploring automated curriculum generation. Most notably, CLIC proposed by __**Wang et al., "CLIC: Continual Driving Policy Optimization with Closed-Loop Individualized Curricula"**__ developed a continual driving policy optimization framework with closed-loop individualized curricula. CLIC trains a discriminator network to predict collision probabilities in different scenarios and leverages these predictions to customize individualized curricula for the current AV's capability level.
Different from their work which relies solely on a black-box collision prediction model for curriculum design, our approach leverages VLM's visual understanding and reasoning abilities to assess agent behavior and performance, providing interpretable textual analysis and enabling more comprehensive and targeted curriculum generation.


\subsection{Autonomous Driving with LLMs/VLMs}

LLMs and VLMs have demonstrated remarkable capabilities across various domains, showcasing their potential in natural language understanding, visual reasoning, and complex decision-making tasks __**Radford et al., "Transformers"**__. 
Recently, there has been a growing interest in integrating these powerful models into autonomous driving systems to enhance their perception, reasoning, and decision-making capabilities __**Wang et al., "LLMs for Autonomous Driving"**__. 
GPT-Driver __**Kim et al., "GPT-Driver: Integrating LLMs into Autonomous Driving Planning"**__ pioneered this integration by incorporating LLMs into autonomous driving planning, utilizing GPT-based models to generate high-level driving decisions from natural language descriptions of traffic scenarios. 
Building upon this foundation, DiLu __**Liu et al., "DiLU: A Comprehensive Framework for Autonomous Driving with LLMs"**__ introduced a comprehensive framework that combines LLM-based reasoning, reflection, and memory modules, enabling decision-making based on common sense knowledge while continuously accumulating driving experience for self-reflection. 
DriveLM __**Wang et al., "DriveLM: Graph Visual Question Answering for Autonomous Driving"**__ proposed a novel graph visual question answering task, reformulating autonomous driving's perception, prediction, and planning processes as a sequence of graph-structured question-answering interactions. 
Taking inspiration from human cognition, LeapAD __**Kim et al., "LeapAD: A Dual-Decision Architecture for Autonomous Driving with LLMs"**__ developed a dual-decision architecture that leverages LLMs for in-depth analysis and reasoning while employing lightweight models for rapid experience-based decision-making. 
Furthermore, ELM __**Liu et al., "ELM: Enhancing VLMs' Spatial Perception and Long-Horizon Extrapolation Capabilities"**__ enhanced VLMs' spatial perception and long-horizon extrapolation capabilities in driving scenarios through the utilization of large-scale open-world data.
These pioneering works inspire us to leverage the powerful reasoning capabilities of VLMs and LLMs for analyzing agent behaviors and generating personalized safety-critical scenarios.