
\begin{figure}[t!]
    \centering
    \includegraphics[width=0.45\textwidth]{./Beamforming_structure.pdf}
    \caption{Joint transmit and pinching beamforming architecture.}
    \label{beamforming_structure}
\end{figure}

\section{Joint Transmit and Pinching Beamforming} \label{sec:beamforing}

Based on the signal model, this section studies a PASS enabled downlink multi-user communication system and the resultant joint transmit and pinching beamforming optimization problem.

\subsection{System Model}

We consider a downlink PASS consisting of $N$ dielectric waveguides serving $K$ communication users, where $N \ge K$. The joint transmit and pinching beamforming architecture is depicted in Fig. \ref{beamforming_structure}, where each waveguide is fed by a dedicated RF chain and incorporates $M$ pinching antennas. Let $\mathbf{r}_k = [x_{\mathrm{u},k}, y_{\mathrm{u},k}, z_{\mathrm{u},k}]^T \in \mathbb{R}^{3 \times 1}$ denote the user $k$'s position and $\mathbf{p}_{n,m} = [x_{n,m}, y_{\mathrm{g},n}, z_{\mathrm{g},n}]^T \in \mathbb{R}^{3 \times 1}$ denote the position of the $m$-th pinching-antenna on the $n$-th waveguide.

\subsubsection{Transmit Signal} 
Let $\mathbf{c} = [c_1,\dots,c_K]^T \in \mathbb{C}^{K \times 1}$ denote the complex information symbols with $c_k$ intended for user $k$, which satisfies $\mathbb{E} \{\mathbf{c} \mathbf{c}^H\} = \mathbf{I}_K$. The signal after the transmit beamforming is given by
\begin{equation}
    \mathbf{s} = \mathbf{W}  \mathbf{c} = \sum_{k=1}^K \mathbf{w}_k c_k,
\end{equation}
% The signal emitted by the $m$-th pinching antennas on the $n$-th waveguide is given by 
% \begin{equation}
%     \tilde{x}_{n,m} = \alpha_m e^{-j \beta_{\mathrm{g}} x_{n,m}} x_n.
% \end{equation}
where $\mathbf{W} = [\mathbf{w}_1,\dots,\mathbf{w}_K] \in \mathbb{C}^{N \times K}$ denote the overall transmit beamforming matrix and $\mathbf{w}_k \in \mathbb{C}^{N \times 1}$ is the beamforming vector for user $k$. 

\subsubsection{Receive Signal}
Let $s_n$ denote the $n$-th entry of the signal $\mathbf{s}$, which represents the signal introduced into the $n$-th waveguide. According to \eqref{multiple_basis_model}, the signal received at user $k$ can be expressed as 
\begin{equation} \label{received_signal_1}
    y_k = \sum_{n=1}^N \mathbf{h}_k^H(\mathbf{x}_n) \mathbf{g}(\mathbf{x}_n) s_n + n_k,
\end{equation} 
where $\mathbf{h}_k(\mathbf{x}_n) \in \mathbb{C}^{M \times 1}$ and $\mathbf{g}(\mathbf{x}_n) \in \mathbb{C}^{M \times 1}$ are the free-space and in-waveguide channel vectors, respectively, and $n_k \sim \mathcal{CN}(0, \sigma_k^2)$ denotes the additive white Gaussian noise. According to \eqref{basic_channel_model} and \eqref{basic_pinching_beamforming_model}, we have
\begin{align}
    \mathbf{h}_k(\mathbf{x}_n) = &\left[\frac{\eta e^{-j \beta_0 r_{k,n,1}}}{r_{k,n,1}},\dots,\frac{\eta e^{-j \beta_0 r_{k,n,M}}}{r_{k,n,M}}  \right]^H, \\
    \mathbf{g}(\mathbf{x}_n) = &\left[ \alpha_1 e^{-j \beta_{\mathrm{g}} x_{n, 1}},\dots,\alpha_M e^{-j \beta_{\mathrm{g}} x_{n, M}} \right]^T,
\end{align}
where $\mathbf{x}_n = [x_{n,1},\dots,x_{n,M}]^T$ denotes the x-axis position vector for all pinching antennas on the $n$-th waveguide, and $r_{k,n,m}$ denotes the distance between user $k$ and the $m$-th pinching antenna on the $n$-th waveguide, given by     
\begin{align}
    r_{k,n,m} = &\| \mathbf{r}_k - \mathbf{p}_{n,m} \| = \sqrt{ (x_{n,m} - x_{\mathrm{u},k})^2 + \omega_{k,n} },
\end{align}
with $\omega_{k,n} = (y_{\mathrm{g},n} - y_{\mathrm{u},k})^2 + z_{\mathrm{g},n}^2$. The expression \eqref{received_signal_1} for the signal received at user $k$ can be rewritten more compactly as follows:
\begin{align}
    y_k = & \mathbf{h}_k^H (\mathbf{X}) \mathbf{G}(\mathbf{X}) \mathbf{s} + n_k \nonumber \\
    = & \underbrace{\vphantom{\sum_{i=1, i\neq k}^K} \mathbf{h}_k^H (\mathbf{X}) \mathbf{G}(\mathbf{X}) \mathbf{w}_k c_k}_{\text{desired signal}} +  \underbrace{\sum_{i=1, i\neq k}^K \mathbf{h}_k^H (\mathbf{X}) \mathbf{G}(\mathbf{X}) \mathbf{w}_i c_i}_{\text{inter-user interference}} + n_k,
\end{align}
where $\mathbf{X} = [\mathbf{x}_1,\dots,\mathbf{x}_N] \in \mathbb{R}^{M \times N}$ consists of the positions of all pinching antennas, $\mathbf{h}_k (\mathbf{X}) = \big[\mathbf{h}_k^T(\mathbf{x}_1),\dots,\mathbf{h}_k^T(\mathbf{x}_N)\big]^T \in \mathbb{C}^{NM \times 1}$ denotes the overall free-space channel vector for user $k$, and $\mathbf{G}(\mathbf{X}) \in \mathbb{C}^{NM \times N}$ denotes the overall in-waveguide channel matrix, given by 
\begin{equation}
    \mathbf{G}(\mathbf{X}) = \begin{bmatrix}
        \mathbf{g}(\mathbf{x}_1) & \mathbf{0} & \cdots & \mathbf{0}\\
        \mathbf{0} & \mathbf{g}(\mathbf{x}_2) & \cdots & \mathbf{0} \\
        \vdots & \vdots & \ddots &  \vdots \\
        \mathbf{0} & \mathbf{0} & \cdots & \mathbf{g}(\mathbf{x}_N)
    \end{bmatrix}.
\end{equation}
Accordingly, the signal-to-interference-plus-noise ratio (SINR) for user $k$ to decode its own signal is given by
\begin{equation} \label{SINR_formula}
    \mathrm{SINR}_k(\mathbf{W}, \mathbf{X}) = \frac{ \left| \mathbf{h}_k^H(\mathbf{X}) \mathbf{G}(\mathbf{X}) \mathbf{w}_k \right|^2 }{\sum_{i = 1, i \neq k}^K \left| \mathbf{h}_k^H(\mathbf{X}) \mathbf{G}(\mathbf{X}) \mathbf{w}_i \right|^2 + \sigma_k^2}.
\end{equation} 

% \subsubsection{Transmit Signal} 
% Let $\mathbf{s} = [s_1,\dots,s_K]^T \in \mathbb{C}^{K \times 1}$ denote the complex information symbols, where $s_k$ is intended for user $k$, satisfying $\mathbb{E} \{\mathbf{s} \mathbf{s}^H\} = \mathbf{I}_K$. The position vector for all pinching antennas on the $n$-th waveguide is denoted as $\mathbf{x}_n = [x_{n,1},\dots,x_{n,M}]^T$. By defining $\mathbf{X} = [\mathbf{x}_1,\dots,\mathbf{x}_N] \in \mathbb{R}^{M \times N}$, the signal after the transmit beamforming and the pinching beamforming, as shown in Fig. \ref{beamforming_structure}, is given by
% \begin{equation}
%     \mathbf{x} = \mathbf{G}(\mathbf{X}) \mathbf{W} \mathbf{s} = \mathbf{G}(\mathbf{X}) \sum_{k=1}^K \mathbf{w}_k s_k.
% \end{equation}
% Here, $\mathbf{W} = [\mathbf{w}_1,\dots,\mathbf{w}_K] \in \mathbb{C}^{N \times K}$ is the transmit beamforming matrix, where $\mathbf{w}_k \in \mathbb{C}^{N \times 1}$ is the beamforming vector for user $k$. The pinching beamforming matrix, $\mathbf{G}(\mathbf{X})\in \mathbb{C}^{NM \times N}$, is derived from \eqref{multiple_basis_model} as
% \begin{align}
%     \mathbf{G}(\mathbf{X}) = &\begin{bmatrix}
%         \mathbf{g}(\mathbf{x}_1) & \mathbf{0} & \cdots & \mathbf{0}\\
%         \mathbf{0} & \mathbf{g}(\mathbf{x}_2) & \cdots & \mathbf{0} \\
%         \vdots & \vdots & \ddots &  \vdots \\
%         \mathbf{0} & \mathbf{0} & \cdots & \mathbf{g}(\mathbf{x}_N)
%     \end{bmatrix}, \\
%     \mathbf{g}(\mathbf{x}_n) = &\left[ \alpha_1 e^{-j \beta_{\mathrm{g}} x_{n, 1}},\dots,\alpha_M e^{-j \beta_{\mathrm{g}} x_{n, M}} \right]^T.
% \end{align}


% \subsubsection{Receive Signal}
% According to \eqref{multiple_basis_model}, the signal received by user $k$ can be expressed as 
% \begin{align} \label{received_signal_1}
%     y_k = &\mathbf{h}_k^H(\mathbf{X}) \mathbf{x} + n_k \nonumber \\
%     = & \underbrace{\vphantom{\sum_{i=1, i\neq k}^K} \mathbf{h}_k^H (\mathbf{X}) \mathbf{G}(\mathbf{X}) \mathbf{w}_k s_k}_{\text{desired signal}} +  \underbrace{\sum_{i=1, i\neq k}^K \mathbf{h}_k^H (\mathbf{X}) \mathbf{G}(\mathbf{X}) \mathbf{w}_i s_i}_{\text{inter-user interference}} + n_k,
% \end{align} 
% where $\mathbf{h}_k(\mathbf{X}) \in \mathbb{C}^{NM \times 1}$ denotes the overall free-space channel vector for user $k$ and is given by 
% \begin{align}
%     \mathbf{h}_k(\mathbf{X}) = &\left[\mathbf{h}_k^T(\mathbf{x}_1),\dots,\mathbf{h}_k^T(\mathbf{x}_N)\right]^T, \\
%     \mathbf{h}_k(\mathbf{x}_n) = &\left[\frac{\eta e^{j \beta_0 r_{k,n,1}}}{r_{k,n,1}},\dots,\frac{\eta e^{j \beta_0 r_{k,n,M}}}{r_{k,n,M}}  \right]^T. 
% \end{align}
% More specifically, $r_{k,n,m}$ denotes the distance between user $k$ and the $m$-th pinching antenna on the $n$-th waveguide, which is given by     
% \begin{align}
%     r_{k,n,m} = &\| \mathbf{r}_k - \mathbf{p}_{n,m} \| = \sqrt{ (x_{n,m} - z_{\mathrm{u},k})^2 + c_{k,n} },
% \end{align}
% where $c_{k,n} = \sqrt{(x_{\mathrm{g},n} - x_{\mathrm{u},k})^2 + y_{\mathrm{g},n}^2}$.
% Accordingly, the SINR for user $k$ to decode its own signal is given by
% \begin{equation} \label{SINR_formula}
%     \mathrm{SINR}_k(\mathbf{W}, \mathbf{X}) = \frac{ \left| \mathbf{h}_k^H(\mathbf{X}) \mathbf{G}(\mathbf{X}) \mathbf{w}_k \right|^2 }{\sum_{i = 1, i \neq k}^K \left| \mathbf{h}_k^H(\mathbf{X}) \mathbf{G}(\mathbf{X}) \mathbf{w}_i \right|^2 + \sigma_k^2}.
% \end{equation} 

\subsection{Problem Formulation}

In this paper, we aim to minimize the total transmission power at the BS by jointly optimizing the transmit beamforming at the BS and the pinching beamforming facilitated by the pinching antennas, under the constraint of meeting the individual minimum SINR requirements for all users. Let $\mathbf{W} = [\mathbf{w}_1,\dots,\mathbf{w}_K]$ and $\mathbf{H}(\mathbf{X}) = [\mathbf{h}_1(\mathbf{X}),\dots,\mathbf{h}_K(\mathbf{X})]$. The problem can be formulated as
\begin{subequations} \label{hybrid_beamforming_problem}
    \begin{align}
        \min_{\mathbf{W}, \mathbf{X}} \quad & \sum_{k=1}^K \|\mathbf{w}_k\|^2 \\
        \mathrm{s.t.} \quad & \mathrm{SINR}_k(\mathbf{W}, \mathbf{X}) \ge \gamma_k, \forall k, \\
        \label{theta_constraint_1}
        & x_{n,m} - x_{n,m-1} \ge \Delta x, \forall n, m\neq 1, \\
        \label{theta_constraint_2}
        & x_{n,m} \in \mathcal{S},
    \end{align}
\end{subequations}
where $\gamma_k > 0$ represents the minimum SINR requirement of user $k$, $\Delta x$ is the minimum spacing required to prevent mutual coupling between the pinching antennas, and $\mathcal{S}$ denotes the feasible set of the positions of the pinching antennas. In this paper, we consider both the continuous and discrete activation scenarios for the pinching antennas, resulting in the following two distinct types of feasible sets:
\begin{itemize}
    \item \textbf{Continuous Activation:} In this case, the pinching antennas can be activated at arbitrary position on the waveguide, leading to the following the feasible set $\mathcal{S}$:
    % \begin{equation}
    %     \mathcal{S} = \left\{ \mathbf{X} \left| \begin{matrix}
    %         0 \le x_{n,m} \le x_{\max}, \forall n,m \\
    %         x_{n,m} - x_{n,m-1} \ge \Delta x, \forall n, m\neq 1
    %     \end{matrix} \right. \right\},
    % \end{equation} 
    \begin{equation}
        \mathcal{S} = [0, x_{\max}],
    \end{equation} 
    where $x_{\max} > 0$ represents the maximum deployment range of the pinching antennas. This is the ideal case for pinching antenna deployment, which yields the performance upper bound.
    \item \textbf{Discrete Activation:} In this case, the pinching antennas can be activated at preconfigured discrete positions only, leading to the following feasible set $\mathcal{S}$: 
    % \begin{equation}
    %     \mathcal{S} = \left\{ \mathbf{X} \left| \begin{matrix} x_{n,m} \in \left\{0, \frac{x_{\max}}{Q-1}, \dots,x_{\max} \right\}, \forall n,m\\
    %         x_{n,m} - x_{n,m-1} \ge \Delta x, \forall n, m\neq 1
    %     \end{matrix} \right. \right\},
    % \end{equation} 
    \begin{equation}
        \mathcal{S} = \left\{0, \frac{x_{\max}}{Q-1}, \dots,x_{\max} \right\},
    \end{equation} 
    where $Q$ denotes the number of discrete positions available for deploying the pinching antennas. Compared to the continuous activation, the discrete activation can reduce the hardware complexity and is thus more practical. 
\end{itemize}

\begin{figure}[t!]
    \centering
    \includegraphics[width=0.45\textwidth]{./channel_gain_example.eps}
    \caption{Illustration of the highly multimodal channel gain with respect to the pinching antenna positions in a “simple” scenario where $N = 1$, $M=2$, and $K=1$. More specifically, $x_1$ and $x_2$ are the positions of the two pinching antennas on the waveguide.}
    \label{fig_multimodal}
\end{figure}

Although the objective function of problem \eqref{hybrid_beamforming_problem} is convex, solving it remains challenging due to the coupling between the transmit beamforming matrix $\mathbf{W}$ and the antenna position matrix $\mathbf{X}$, as well as the highly coupled and nonlinear dependency of $\mathbf{H}(\mathbf{X})$ and $\mathbf{G}(\mathbf{X})$ on the position matrix $\mathbf{X}$ of the pinching antennas. It is worth noting that a similar antenna position optimization problem arises in the beamforming design for fluid and movable antenna systems \cite{zhu2023movable}, where a common approach to solving this problem is gradient descent. However, this method is not suitable for optimizing the positions of pinching antennas for two key reasons. First, the optimization problem in \eqref{hybrid_beamforming_problem} is highly multimodal, meaning it contains a large number of local optima, as illustrated in Fig. \ref{fig_multimodal}. Second, the performance gap between local optima can be extremely large due to the significant oscillations in free-space path loss, which result from the wide deployment range of pinching antennas. To overcome these challenges, two optimization algorithms are proposed in the following: 1) the penalty-based alternating optimization algorithm and 2) the ZF-based low-complexity algorithm.


\subsection{Penalty-based Alternating Optimization Algorithm}

In this section, we propose a penalty-based alternating optimization algorithm to address the joint beamforming optimization problem \eqref{hybrid_beamforming_problem}. The key idea of alternating optimization is to alternatively fix one subset of variables, either the transmit beamforming matrix $\mathbf{W}$ or the antenna position matrix $\mathbf{X}$, and optimize the objective function with respect to the other. 

However, it is important to note that the objective function in \eqref{hybrid_beamforming_problem} depends only on the matrix $\mathbf{W}$. As a result, when optimizing $\mathbf{X}$ while keeping $\mathbf{W}$ fixed, the corresponding subproblem with respect to $\mathbf{X}$ becomes a feasibility-checking problem, i.e., with the objective function being a constant. Consequently, directly solving \eqref{hybrid_beamforming_problem} using alternating optimization does not ensure sequential minimization of the objective function across iterations, which can lead to poor convergence performance. To get rid of this issue, we define $\mathbf{v}_k = \mathbf{w}_k/\sqrt{P}$, where $P = \sum_{k=1}^K \|\mathbf{w}_k\|^2$, and reformulate problem \eqref{hybrid_beamforming_problem} into the following equivalent form:   
\begin{subequations} \label{joint_beamforming_problem_2}
    \begin{align}
        \min_{\mathbf{V}, \mathbf{X}, P > 0} \quad & P \\
        \mathrm{s.t.} \quad & \overline{\mathrm{SINR}}_k(\mathbf{V}, \mathbf{X}, P)  \ge \gamma_k, \forall k, \\
        \label{unit_power_constraint}
        & \sum_{k=1}^K \| \mathbf{v}_k \|^2 = 1, \\
        & \eqref{theta_constraint_1}, \eqref{theta_constraint_2},
    \end{align}
\end{subequations}
where 
\begin{equation}
    \overline{\mathrm{SINR}}_k(\mathbf{V}, \mathbf{X}, P) = \frac{ P \left| \mathbf{h}_k^H(\mathbf{X}) \mathbf{G}(\mathbf{X}) \mathbf{v}_k \right|^2 }{\sum_{i = 1, i \neq k}^K P \left| \mathbf{h}_k^H(\mathbf{X}) \mathbf{G}(\mathbf{X}) \mathbf{v}_i \right|^2 + \sigma_k^2 }.
\end{equation}
As can be observed, in problem \eqref{joint_beamforming_problem_2}, the objective function is related to both $\mathbf{W}$ and $\mathbf{X}$ through the SINR constraint, which facilitates the convergence when the alternating optimization technique is applied. 

The primary challenge in solving the reformulated problem \eqref{joint_beamforming_problem_2} arises from the highly coupled and nonlinear nature of $\mathbf{H}(\mathbf{X})$ and $\mathbf{G}(\mathbf{X})$ within the SINR constraints. The penalty method is a well-established approach in the literature for addressing such coupling issues by introducing appropriate equality constraints and incorporating them into the objective function \cite{liu2018mu, shi2020penalty, wu2020joint}. To effectively apply the penalty method, we first define the following decomposition of the matrices $\mathbf{H}(\mathbf{X})$ and $\mathbf{G}(\mathbf{X})$:
\begin{align} \label{PH_formula}
    \mathbf{G}^H(\mathbf{X}) \mathbf{H}(\mathbf{X})  
    & = \begin{bmatrix}
        \mathbf{g}^H(\mathbf{x}_1) \mathbf{h}_1(\mathbf{x}_1) & \!\! \cdots \!\! & \mathbf{g}^H(\mathbf{x}_1) \mathbf{h}_K(\mathbf{x}_1) \\ \vdots & \!\! \ddots \!\!  & \vdots \\
        \mathbf{g}^H(\mathbf{x}_N) \mathbf{h}_1(\mathbf{x}_N) & \!\! \cdots \!\! & \mathbf{g}^H(\mathbf{x}_N) \mathbf{h}_K(\mathbf{x}_N)
    \end{bmatrix} \nonumber \\ &= \sum_{m=1}^M \mathbf{\Phi}_m(\mathbf{X}),
\end{align}
where the entry of $\mathbf{\Phi}_m(\mathbf{X})$ in the $n$-th row and $k$-th column is given by 
\begin{equation}
   \left[ \mathbf{\Phi}_m(\mathbf{X}) \right]_{n,k} = \frac{\eta \alpha_m }{r_{k,n,m}} e^{j\left( \beta_0 r_{k,n,m} + \beta_{\mathrm{g}} x_{n,m} \right)}.
\end{equation}
It will be shown in the following that the above decomposition significantly reduces the complexity of optimizing the pinching antenna positions and addresses the issue of multimodality. Leveraging this decomposition, problem \eqref{joint_beamforming_problem_2} can be reformulated into the following equivalent form:
\begin{subequations} \label{joint_beamforming_problem_3}
    \begin{align}
        \min_{\mathbf{U}, \mathbf{U}_m, \mathbf{V}, \mathbf{X}, P > 0} \quad & P \\
        \label{joint_beamforming_problem_3_0}
        \mathrm{s.t.} \quad & \mathbf{U} = \sum_{m=1}^M \mathbf{U}_m, \mathbf{U}_m = \mathbf{\Phi}_m(\mathbf{X}), \forall m, \\
        \label{joint_beamforming_problem_3_1}
        & \overline{\overline{\mathrm{SINR}}}_k(\mathbf{V}, \mathbf{U}, P)  \ge \gamma_k, \forall k, \\
        & \eqref{theta_constraint_1}, \eqref{theta_constraint_2}, \eqref{unit_power_constraint},
    \end{align}
\end{subequations}
where 
\begin{equation}
    \overline{\overline{\mathrm{SINR}}}_k(\mathbf{V}, \mathbf{X}, P) = \frac{ P \left| \mathbf{u}_k^H \mathbf{v}_k \right|^2 }{\sum_{i = 1, i \neq k}^K P \left| \mathbf{u}_k^H \mathbf{v}_i \right|^2 + \sigma_k^2 }.
\end{equation}
In this new problem, $\mathbf{U} = [\mathbf{u}_1,\dots,\mathbf{u}_k] \in \mathbb{C}^{NM \times K}$ is an auxiliary channel matrix. By integrating the equality constrains into the objective function as penalty terms, problem \eqref{joint_beamforming_problem_3} becomes
\begin{subequations} \label{joint_beamforming_problem_4}
    \begin{align}
        \min_{\mathbf{U}, \mathbf{U}_m, \mathbf{V}, \mathbf{X}, P > 0} \quad P &+ \frac{1}{\rho} \| \mathbf{U} - \sum_{m=1}^M \mathbf{U}_m \|_F^2 \nonumber \\
        & + \frac{1}{\rho} \sum_{m=1}^M \| \mathbf{U}_m - \mathbf{\Phi}_m(\mathbf{X}) \|_F^2 \\
        \mathrm{s.t.} \quad &\eqref{theta_constraint_1}, \eqref{theta_constraint_2}, \eqref{unit_power_constraint}, \eqref{joint_beamforming_problem_3_1}, 
    \end{align}
\end{subequations}
where $\rho > 0$ is the penalty factor. In particular, when $\rho \to 0$, the penalty terms can be exactly zero, ensuring that the original equality constraints are strictly satisfied. However, this approach renders the transmit power $P$ a dummy objective function, leading to significant performance degradation. To mitigate this issue, the penalty factor is initially set to a large value to ensure sufficient minimization of the transmit power. It is then gradually reduced, allowing the penalty terms to converge to zero, i.e., forcing the auxiliary channel matrix $\mathbf{U}$ to follow the channel structure in PASS. Therefore, the penalty method typically has a double-loop optimization structure. In the outer loop, the penalty factor is progressively decreased, while in the inner loop, the optimization variables are updated using an alternating optimization approach. The solutions for each subset of optimization variables, while keeping the other subsets fixed, are presented in the following.

\subsubsection{Subproblem With Respect to $\mathbf{V}$} 
Based on the definition $\mathbf{v}_k = \mathbf{w}_k/\sqrt{P}$ and $P = \sum_{k=1}^K \|\mathbf{w}_k\|^2$, this subproblem can be rewritten as
\begin{subequations} \label{subproblem_W}
    \begin{align}
        \min_{\mathbf{W}} \quad &\sum_{k=1}^K \|\mathbf{w}_k\|^2 \\
        \mathrm{s.t.} \quad & \frac{1}{\gamma_k}\left| \mathbf{u}_k^H \mathbf{w}_k \right|^2 \ge \sum_{i=1, i \neq k}^K  \left| \mathbf{u}_k^H \mathbf{w}_i \right|^2 + \sigma_k^2, \forall k.
    \end{align}
\end{subequations}
Note that the above problem is the conventional power minimization problem for multi-user transmit beamforming. This problem can be transformed into an equivalent convex second-order cone (SOC) programming, and then solved by the fixed-point iteration. After obtaining the optimal $\mathbf{W}$, the optimal $\mathbf{V}$ can be calculated as 
\begin{equation}
    \mathbf{V} = \frac{\mathbf{W}}{\|\mathbf{W}\|_F}.
\end{equation}  

\subsubsection{Subproblem With Respect to $\mathbf{U}$} This subproblem can be expressed as 
\begin{subequations} \label{subproblem_U}
    \begin{align}
        \min_{\mathbf{U}, P > 0} \quad &P + \frac{1}{\rho} \| \mathbf{U} - \sum_{m=1}^M \mathbf{U}_m \|_F^2 \\
        \mathrm{s.t.} \quad & \frac{1}{\gamma_k}\left| \mathbf{u}_k^H \mathbf{v}_k \right|^2 \ge \sum_{i=1, i \neq k}^K  \left| \mathbf{u}_k^H \mathbf{v}_i \right|^2 + \frac{\sigma_k^2}{P}, \forall k.
    \end{align}
\end{subequations}
This subproblem is non-convex due to the quadratic term $\left| \mathbf{u}_k^H \mathbf{v}_k \right|^2$ on the left-hand side of the inequality constraints. We exploit the successive convex approximation method to address it. Let $\mathbf{U}^{(t)} = \big[\mathbf{u}_1^{(t)},\dots,\mathbf{u}_K^{(t)}\big]$ denote the solution of $\mathbf{U}$ obtained in the previous iteration of the inner loop. Then, according to the first-order Taylor expansion, a low bound on the quadratic term $\left| \mathbf{u}_k^H \mathbf{v}_k \right|^2$ can be obtained as follows:
\begin{align}
    \left| \mathbf{u}_k^H \mathbf{v}_k \right|^2 &\ge - \left( \mathbf{u}_k^{(t)}\right)^H \mathbf{v}_k \mathbf{v}_k^H \mathbf{u}_k^{(t)} \nonumber \\
    &+ 2 \Re \left\{ \left( \mathbf{u}_k^{(t)}\right)^H \mathbf{v}_k \mathbf{v}_k^H \mathbf{u}_k \right\} \triangleq h \left( \mathbf{u}_k | \mathbf{u}_k^{(t)} \right),
\end{align} 
where the equality is achieved at $\mathbf{u}_k = \mathbf{u}_k^{(t)}$. Therefore, problem \eqref{subproblem_U} can be approximated by 
\begin{subequations} \label{subproblem_U_sca}
    \begin{align}
        \min_{\mathbf{U}, P > 0} \quad & P + \frac{1}{\rho} \|\mathbf{U} - \sum_{m=1}^M \mathbf{U}_m \|_F^2  \\
        \mathrm{s.t.} \quad & \frac{1}{\gamma_k} h \left( \mathbf{u}_k | \mathbf{u}_k^{(t)} \right) \ge \sum_{i = 1, i \neq k}^K \left| \mathbf{u}_k^H \mathbf{v}_i \right|^2 + \frac{\sigma_k^2}{P}, \forall k.
    \end{align}
\end{subequations}
This problem is a convex and therefore can be solved optimally by standard convex program solvers such as CVX.

\subsubsection{Subproblem With Respect to $\mathbf{U}_m$}
This subproblem is given by 
\begin{align}
    \min_{\mathbf{U}_m} \quad &\| \mathbf{U} - \sum_{m=1}^M \mathbf{U}_m \|_F^2 + \sum_{m=1}^M \| \mathbf{U}_m - \mathbf{\Phi}_m(\mathbf{X}) \|_F^2.
\end{align}
This problem is an unconstrained convex optimization problem, allowing us to use the first-order optimality condition to determine the optimal solution. By computing the partial derivatives of the objective function with respect to each $\mathbf{U}_m$ and setting these derivatives to zero, we derive the following conditions for the optimal solution:
\begin{equation}
    \sum_{i=1}^M \mathbf{U}_i + \mathbf{U}_m = \mathbf{U} + \mathbf{\Phi}_m(\mathbf{X}), \forall m,
\end{equation}
while yields the following optimal solution:
\begin{align} \label{subproblem_U_optimal}
    \mathbf{U}_m = \frac{1}{M+1} \left( \mathbf{U} + M \mathbf{\Phi}_m(\mathbf{X}) - \sum_{i=1, i \neq m}^M \mathbf{\Phi}_i(\mathbf{X})    \right).
\end{align}
\subsubsection{Subproblem With Respect to $\mathbf{X}$}
This subproblem is given by 
\begin{subequations} \label{subproblem_theta}
    \begin{align}
        \min_{\mathbf{X}} \quad & \sum_{m=1}^M \| \mathbf{U}_m - \mathbf{\Phi}_m(\mathbf{X})\|_F^2 \\
        \label{sub_theta_constraint}
        \mathrm{s.t.} \quad &  x_{n,m} - x_{n,m-1} \ge \Delta x, \forall n, m\neq 1, \\
        & x_{n,m} \in \mathcal{S}.
    \end{align}
\end{subequations}
The objective function of this problem can be reformulated as 
\begin{equation}
    \sum_{m=1}^M \| \mathbf{U}_m - \mathbf{\Phi}_m(\mathbf{X}) \|_F^2 = \sum_{n=1}^N \sum_{m=1}^M f(x_{n,m}),
\end{equation}
where 
\begin{align}
    f(x_{n,m}) = \sum_{k=1}^K \left| u_{k,n,m}  - \frac{\eta \alpha_m }{r_{k,n,m}} e^{j\left( \beta_0 r_{k,n,m} + \beta_{\mathrm{g}} x_{n,m} \right)} \right|^2,
\end{align}
with $u_{k,n,m}$ being the entry in the $n$-th row and $k$-th column of $\mathbf{U}_m$. It can be observed that each optimization variable $x_{n,m}$ is decoupled in the objective function. However, these variables remain coupled in the constraint \eqref{sub_theta_constraint}. 

\begin{algorithm}[tb]
    \caption{Element-wise Algorithm for Solving \eqref{subproblem_theta}}
    \label{alg:element_wise}
    \begin{algorithmic}[1]
        \STATE{initialize the optimization variables}
        \REPEAT
            \FOR{$n \in  \{1,\dots,N\}$ and $m \in \{ 1,\dots,M \}$}
            \STATE{update $x_{n,m}$ by solving problem \eqref{search_problem_1} through one-dimensional search}
            \ENDFOR
        \UNTIL{the fractional decrease of the objective value of problem \eqref{subproblem_theta} falls below a predefined threshold}
    \end{algorithmic}
\end{algorithm}

To tackle this issue, we propose an element-wise alternating optimization algorithm to solve problem \eqref{subproblem_theta}, where each $x_{n,m}$ is optimized by fixed the others. Specifically, the subproblem with respect to \eqref{subproblem_theta} is given by 
\begin{subequations} \label{search_problem_1}
    \begin{align}
        \min_{x_{n,m}} \quad & f(x_{n,m}) \\
        \label{search_problem_1_constraint}
        \mathrm{s.t.} \quad & x_{n,m} \in \mathcal{S}_{n,m},
    \end{align}
\end{subequations}
where 
\begin{equation} \label{search_set}
    \mathcal{S}_{n,m} = [x_{n,m-1}+\Delta x, x_{n,m+1} - \Delta x] \cap \mathcal{S}.
\end{equation}
It can be observed that problem \eqref{search_problem_1} is to find the minimum of a single-variable function over a finite interval. Consequently, it can be efficiently solved using a low-complexity one-dimensional search method, applicable to both continuous and discrete activation scenarios. For continuous activation, the feasible set $\mathcal{S} = [0, x_{\max}]$ is discretized into a fine grid. The optimal solution that minimizes $g(x_{n,m})$ is then searched within the sampled feasible set, subject to the constraint in \eqref{search_problem_1_constraint}. For discrete activation, the optimal solution can be directly identified by searching over the discrete set of feasible values. Based on the above approaches, the overall element-wise algorithm for solving problem \eqref{subproblem_theta} is summarized in \textbf{Algorithm \ref{alg:element_wise}}. It is worth noting that the one-dimensional search effectively addresses the multimodality issue by preventing the optimization from getting stuck in local optima.



% Although the objective function of this problem is intricate due to the complex forms of the matrices $\mathbf{G}(\mathbf{X})$ and $\mathbf{H}(\mathbf{X})$ with respect to $\mathbf{X}$, the problem is unconstrained. Therefore, gradient-based optimization methods, such as the gradient descent method or quasi-Newton methods, can be effectively utilized to locate a stationary point for this problem. 
% For simplicity, we define 
% \begin{align}
%     g_1(\mathbf{X}) = &\| \mathbf{\Psi} - \mathbf{X} + \rho \boldsymbol{\phi} \|^2, \\
%     g_2(\mathbf{X}) = &\|\mathbf{U} - \mathbf{\Psi}(\mathbf{X}) + \rho \mathbf{\Lambda} \|_F^2,
% \end{align}
% where $\mathbf{\Psi}(\mathbf{X}) = \mathbf{G}^H(\mathbf{X}) \mathbf{H}(\mathbf{X})$. The gradient of $g_1(\mathbf{X})$ with respect to $\mathbf{X}$ is given by 
% \begin{align}
%     \frac{\partial g_1(\mathbf{X})}{\partial \mathbf{X}} = 2 ( \mathbf{X} -  \mathbf{\Psi} - \rho \boldsymbol{\phi} ).
% \end{align}
% To calculate the gradient of $g_2(\mathbf{X})$ with respect to $\mathbf{X}$, we first calculate the gradient with respect to each entry of $\mathbf{X}$. According to the chain rule, we have
% \begin{equation}
%     \frac{\partial g_2(\mathbf{X})}{\partial x_{n,m}} = 2 \Re \left\{ \mathrm{tr} \left( \frac{\partial g_2(\mathbf{X})}{\partial \mathbf{\Psi}^T(\mathbf{X})} \frac{\partial \mathbf{\Psi}(\mathbf{X})}{\partial x_{n,m}} \right) \right\}.
% \end{equation}
% Then, we have 
% \begin{equation}
%     \frac{\partial g_2(\mathbf{X})}{\partial \mathbf{\Psi}^T(\mathbf{X})} = \mathbf{\Psi}^H(\mathbf{X}) - \left( \mathbf{U} + \rho \mathbf{\Lambda} \right)^H.
% \end{equation}
% The entry at the $n'$-th row and $k$-th column of $\mathbf{\Psi}(\mathbf{X})$, denoted by $\psi_{n',k}(\mathbf{X})$, is given by 
% \begin{align}
%     \psi_{n',k}(\mathbf{X}) &= \mathbf{g}_{n'}^H (\mathbf{X}_{n'}) \mathbf{h}_{k,n'} (\mathbf{X}_{n'}) \nonumber \\
%     & =\sum_{m=1}^M \frac{\eta \alpha_m}{r_{k,n',m}} e^{j (\beta_0 r_{k,n',m} + \beta_{\mathrm{g}} \theta_{n',m})}.
% \end{align}
% Then, if $n'=n$, we have 
% \begin{equation}
%     \frac{\partial \psi_{n,k}(\mathbf{X})}{\partial x_{n,m}} = 
%         \left( \left(j \beta_0 - \frac{\eta \alpha_m}{r_{k,n,m}^2} \right) \frac{\partial r_{k,n,m}}{\partial x_{n,m}} + \beta_{\mathrm{g}} \right) e^{j (\beta_0 r_{k,n,m} + \beta_{\mathrm{g}} x_{n,m})}
% \end{equation}



\subsubsection{Overall Algorithm}
Based on the above solutions for each subset of the optimization variables, the overall penalty-based alternating optimization algorithms is summarized in \textbf{Algorithm \ref{alg:PDD}}. Specifically, $0< \epsilon < 1$ is used to reduce the penalty factor in the outer loop, and $\varepsilon$ is the constraint violation function, which is defined as the maximum value within the penalty terms:
\begin{equation}
    \varepsilon = \max \bigg\{ \| \mathbf{U} - \sum_{m=1}^M \mathbf{U}_m \|_{\infty}, \|\mathbf{U}_m -  \mathbf{\Phi}_m(\mathbf{X}) \|_{\infty}, \forall m \bigg\}.
\end{equation}
The computational complexity of each iteration in \textbf{Algorithm \ref{alg:PDD}} is determined by the complexity of updating its key variables. Specifically, updating $\mathbf{V}$ requires solving problem \eqref{subproblem_W}, which involves optimizing $NK$ variables with $K$ SOC constraints of dimension $NK$, resulting in a complexity of $O(N^4K^2)$. Similarly, updating $\mathbf{U}$ involves solving problem \eqref{subproblem_U_sca}, where $(NK+1)$ variables are optimized subject to $K$ SOC constraints of dimension $(NK+1)$, leading to a complexity of $O((NK+1)^4)$. For the update of $\mathbf{U}_m$ for all $m$, the computation of the closed-form expression in \eqref{subproblem_U_optimal} incurs a complexity of $O(MNK)$. Finally, updating $\mathbf{X}$ requires running \textbf{Algorithm \ref{alg:element_wise}}, which has a complexity of $O(I_{\mathrm{iter}} Q M N K)$, where $I_{\mathrm{iter}}$ denotes the number of iterations and $Q$ represents the size of the search space for the one-dimensional search.


% If the constraint violation is not greater than a predefined threshold $\epsilon$ at an outer-loop iteration, the penalty factor $\rho$ remains unchanged, while the dual variables are updated using gradient descent as follows:
% \begin{align}
%     \label{update_dual_1}
%     &\mathbf{\Upsilon} \leftarrow  \mathbf{\Upsilon} + \frac{1}{\rho} \left( \mathbf{U} - \sum_{m=1}^M \mathbf{U}_m \right), \\
%     \label{update_dual_2}
%     &\mathbf{\Lambda}_m \leftarrow  \mathbf{\Lambda}_m + \frac{1}{\rho} \left( \mathbf{U}_m - \mathbf{\Phi}_m(\mathbf{X}) \right).
% \end{align}
% If the constraint violation exceeds the predefined threshold, the dual variables remain unchanged, and the penalty factor is updated as $\rho \leftarrow c \rho$, where $0 < c < 1$.   

\begin{algorithm}[tb]
    \caption{Penalty-based Alternating Optimization Algorithm for Joint Beamforming Problem \eqref{hybrid_beamforming_problem}}
    \label{alg:PDD}
    \begin{algorithmic}[1]
        \STATE{initialize the optimization variables, set $0 < \epsilon < 1$}
        \REPEAT
            \REPEAT
            \STATE{update $\mathbf{W}$ by solving problem \eqref{subproblem_W} and calculate $\mathbf{V} = \mathbf{W}/\|\mathbf{W}\|_F$  }
            \STATE{update $\mathbf{U}$ by solving problem \eqref{subproblem_U_sca}}
            \STATE{update $\mathbf{\mathbf{U}}_m$ as \eqref{subproblem_U_optimal}}
            \STATE{update $\mathbf{X}$ by \textbf{Algorithm \ref{alg:element_wise}} }
            \UNTIL{convergence}
            % \IF{$v \le \epsilon$  }
            % \STATE{update the dual variables $\mathbf{\Upsilon}$ and $\mathbf{\Lambda}$ by \eqref{update_dual_1} and \eqref{update_dual_2}}
            % \ELSE 
            \STATE{update the penalty factor as $\rho \leftarrow \epsilon \rho$ }
            % \ENDIF
            % \STATE{set $\epsilon = 0.9 v$}
        \UNTIL{$\varepsilon$ falls below a predefined threshold}
    \end{algorithmic}
\end{algorithm}

\subsection{ZF-based Low-complexity Algorithm}

In this section, we further propose a low-complexity alternating optimization algorithm by using the ZF beamforming, which is capable of eliminate the inter-user interference completely. In this case, the SINR expression can be simplified significantly, thus reducing the optimization complexity. 

Given that $N \ge K$, we can obtain the following ZF beamforming matrix for any given pinching beamforming matrix \cite{bjornson2014optimal}:
\begin{equation} \label{ZF_BF}
    \mathbf{W} = \mathbf{\Psi}(\mathbf{X}) \left( \mathbf{\Psi}^H(\mathbf{X}) \mathbf{\Psi}(\mathbf{X}) \right)^{-1} \mathbf{P}^{\frac{1}{2}},
\end{equation}  
where $\mathbf{\Psi}(\mathbf{X}) \triangleq \mathbf{G}^H(\mathbf{X}) \mathbf{H}(\mathbf{X})$ is the equivalent channel matrix and $\mathbf{P} \in \mathbb{R}^{K \times K}$ is the diagonal power control matrix, given by 
\begin{equation}
    \mathbf{P} = \mathrm{diag}\left( P_1,\dots,P_K \right),
\end{equation}  
and $P_k$ is the power coefficient for user $k$. Consequently, the transmit power can be reformulated as 
\begin{align}
    \sum_{k=1}^K \|\mathbf{w}_k\|^2 = &\mathrm{tr}\left( \mathbf{W} \mathbf{W}^H \right) = \mathrm{tr}\left( \left( \mathbf{\Psi}^H(\mathbf{X}) \mathbf{\Psi}(\mathbf{X}) \right)^{-1} \mathbf{P} \right).
\end{align}
Substituting \eqref{ZF_BF} into \eqref{SINR_formula} yields the following simplified SINR expression:
\begin{equation}
    \mathrm{SINR}_k = \frac{P_k}{\sigma_k^2}.
\end{equation} 
Therefore, the transmit power minimization problem in \eqref{hybrid_beamforming_problem} can be simplified as 
\begin{subequations} \label{ZF_BF_problem}
    \begin{align}
        \min_{\mathbf{P}, \mathbf{X}} \quad & \mathrm{tr}\left( \left( \mathbf{\Psi}^H(\mathbf{X}) \mathbf{\Psi}(\mathbf{X}) \right)^{-1} \mathbf{P} \right) \\
        \mathrm{s.t.} \quad & \frac{P_k}{\sigma_k^2} \ge \gamma_k, \forall k, \\
        & \eqref{theta_constraint_1}, \eqref{theta_constraint_2}.
    \end{align}
\end{subequations}
It can be readily shown that the optimal $P_k, \forall k,$ to problem \eqref{ZF_BF_problem} is irrelevant to $\mathbf{X}$, which is given by 
\begin{equation} \label{optimal_ZF_P}
    P^{\mathrm{opt}}_k = \gamma_k \sigma_k^2, \forall k.
\end{equation}  
Therefore, the alternating optimization is no longer required. Given the optimal $\mathbf{P}^{\mathrm{opt}} = \mathrm{diag}(P^{\mathrm{opt}}_1,\dots,P^{\mathrm{opt}}_K )$, the optimization problem with respect to $\mathbf{X}$ is given by 
\begin{subequations} \label{ZF_BF_problem_Theta}
    \begin{align}
        \label{ZF_BF_problem_obj}
        \min_{\mathbf{X}} \quad & \mathrm{tr}\left( \left( \mathbf{\Psi}^H(\mathbf{X}) \mathbf{\Psi}(\mathbf{X}) \right)^{-1} \mathbf{P}^{\mathrm{opt}} \right) \\
        \mathrm{s.t.} \quad & \eqref{theta_constraint_1}, \eqref{theta_constraint_2}.
    \end{align}
\end{subequations}
This problem can also be solved using an element-wise one-dimensional search algorithm. Specifically, the optimal value of each element $x_{n,m}$ in $\mathbf{X}$ is determined by searching over the set $\mathcal{S}_{n,m}$ defined in \eqref{search_set}, by keeping all other entries fixed. 

However, searching for the optimal $x_{n,m}$ to minimize the objective function in \eqref{ZF_BF_problem_obj} can result in extremely high computational complexity, as matrix inversion must be computed at each search point.  To mitigate this issue, we propose a decomposition method for the objective function in \eqref{ZF_BF_problem_obj} in the case of $N > K$. Specifically, for optimizing $x_{n,m}, \forall m,$ associated with the $n$-th RF chain, we introduce the following decomposition:  
\begin{equation}
    \mathbf{\Psi}^H(\mathbf{X}) \mathbf{\Psi}(\mathbf{X}) = \mathbf{a}_n \mathbf{a}_n^H + \mathbf{B}_n \mathbf{B}_n^H, 
\end{equation}  
where $\mathbf{a}_n \in \mathbb{C}^{K \times 1}$ is the $n$-th column vector of $\mathbf{\Psi}^H(\mathbf{X})$, while $\mathbf{B}_n \in \mathbb{C}^{K \times (N-1)}$ is the matrix containing all column vectors of $\mathbf{\Psi}^H(\mathbf{X})$ except $\mathbf{a}_n$. According to \eqref{PH_formula}, the expression of $\mathbf{a}_n$ is given by 
\begin{equation}
    \mathbf{a}_n = \left[ \mathbf{h}_1^H(\mathbf{x}_n)\mathbf{g}(\mathbf{x}_n),\dots,\mathbf{h}_K^H(\mathbf{x}_n)\mathbf{g}(\mathbf{x}_n) \right]^T.
\end{equation} 
The expression of $\mathbf{B}_n$  can be derived in a similar manner. Given that $N > K$, the matrix $\mathbf{B}_n \mathbf{B}_n^H$ must be full-rank. As a result, the matrix inversion in the objective function \eqref{ZF_BF_problem_obj} can be rewritten as 
\begin{align} \label{ZF_decomposition}
    &\left( \mathbf{\Psi}^H(\mathbf{X}) \mathbf{\Psi}(\mathbf{X}) \right)^{-1} = \left( \mathbf{a}_n \mathbf{a}_n^H + \mathbf{B}_n \mathbf{B}_n^H \right)^{-1} \nonumber \\
    &\hspace{0.5cm} \overset{(a)}{=} \left(\mathbf{B}_n \mathbf{B}_n^H\right)^{-1} -  \frac{\left(\mathbf{B}_n \mathbf{B}_n^H\right)^{-1} \mathbf{a}_n \mathbf{a}_n^H \left(\mathbf{B}_n \mathbf{B}_n^H\right)^{-1}}{1 + \mathbf{a}_n^H \left(\mathbf{B}_n \mathbf{B}_n^H\right)^{-1} \mathbf{a}_n},
\end{align} 
where step $(a)$ follows from the Sherman-Morrison formula. By substituting \eqref{ZF_decomposition} into \eqref{ZF_BF_problem_Theta}, the subproblem for optimizing $x_{n,m}$ is formulated as
\begin{subequations} \label{ZF_search_problem}
    \begin{align}
        \max_{x_{n,m}} \quad & \frac{ \mathbf{a}_n^H \left(\mathbf{B}_n \mathbf{B}_n^H\right)^{-1} \mathbf{P}^{\mathrm{opt}} \left(\mathbf{B}_n \mathbf{B}_n^H\right)^{-1} \mathbf{a}_n}{1 + \mathbf{a}_n^H \left(\mathbf{B}_n \mathbf{B}_n^H\right)^{-1} \mathbf{a}_n} \\
        \mathrm{s.t.} \quad & x_{n,m} \in \mathcal{S}_{n,m}.
    \end{align}
\end{subequations}
When applying the one-dimensional search method to solve this problem, only a single matrix inversion, $\left(\mathbf{B}_n \mathbf{B}_n^H\right)^{-1}$, needs to be computed for all search points, which significantly reduce the computational complexity.  

\begin{algorithm}[tb]
    \caption{ZF-based Low-complexity Algorithm for Joint Beamforming Problem \eqref{hybrid_beamforming_problem}}
    \label{alg:ZF}
    \begin{algorithmic}[1]
        \STATE{calculate $P_k, \forall k,$ as \eqref{optimal_ZF_P}, and initialize $\mathbf{X}$}
        \REPEAT
            \FOR{$n \in  \{1,\dots,N\}$ and $m \in \{ 1,\dots,M \}$}
            \STATE{update $x_{n,m}$ by solving problem \eqref{ZF_search_problem} through one-dimensional search}
            \ENDFOR
        \UNTIL{the fractional decrease of the objective value of problem \eqref{ZF_BF_problem_obj} falls below a predefined threshold}
        \STATE{calculate $\mathbf{W}$ as \eqref{ZF_BF}}
    \end{algorithmic}
\end{algorithm}

The overall ZF-based low-complexity algorithm is summarized in \textbf{Algorithm \ref{alg:ZF}}. The computational complexity of each iteration is analyzed as follows. In particular, for each $n$, the matrix inversion $\left(\mathbf{B}_n \mathbf{B}_n^H\right)^{-1}$ need to be computed, resulting in a complexity of a complexity of $O(K^3)$. Furthermore, The one-dimensional search has a complexity of $O(Q K^2)$. Therefore, the overall computational complexity per iteration is given by $O(N K^2 + Q MN K^2)$.  

