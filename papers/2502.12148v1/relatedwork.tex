\section{Related Work}
\subsection{Unified Multimodal Understanding and Generation}
In recent years, a growing number of studies \citep{dong2023dreamllm, ge2024seed, wu2023next, ye2024x, ma2024janusflow, shi2024llamafusion} have explored unified multimodal models capable of both visual understanding and generation. Early methods \citep{dong2023dreamllm, tong2024metamorph, ge2024seed, sun2024generative, zhuang2024towards,zhang2024realcompo} leveraged diffusion models as external tools, where MLLMs generate conditions for visual generation \citep{yang2024mastering,tian2024videotetris} without having direct generative capabilities. For instance, DreamLLM \citep{dong2023dreamllm} introduces learnable embeddings called dream queries, which encapsulate the semantics encoded by MLLMs and serve as conditions for the diffusion decoder. More recently, inspired by the success of autoregressive paradigms, many studies \citep{team2024chameleon, xie2024show, zhou2024transfusion, qu2024tokenflow, xie2024muse, zhang2024fate, wang2024emu3} have shifted focus to representing and generating images using discrete visual tokens within a single transformer framework. For instance, Emu3 \citep{wang2024emu3} is trained solely with next-token prediction on a mixture of multimodal sequences using a single transformer. Janus \citep{wu2024janus} separates visual encoding into distinct pathways for multimodal understanding and generation while maintaining a unified transformer architecture. However, no existing research has focused on the relationship between the strengths of understanding and generation capabilities in MLLMs, which is essential for the balanced and sustainable development of these models.




\subsection{DPO in Multimodal LLMs}
Direct Preference Optimization (DPO) \citep{rafailov2024direct,zhang2024itercomp,yang2025supercorrect,yang2025reasonflux} enhances the performance of multimodal LLMs through the post-training process. In \cref{fig:compare}, we categorize these approaches into three types. Some methods \citep{zhou2024aligning, zhou2024calibrated, he2024self, zhang2024critic} utilize DPO to enhance understanding capability, as shown in \cref{fig:compare} (a). For instance, CSR \citep{zhou2024calibrated} enables the model to self-improve by iteratively generating candidate responses, evaluating the reward for each response, and curating preference data for finetuning. Other methods \citep{wang2024emu3} improve the generation capability of MLLMs through DPO as illustrated in \cref{fig:compare} (b). Emu3 \citep{wang2024emu3} generates a data pool and constructs a preference dataset through manual ranking, which is then used to optimize the model's generation capabilities via DPO. However, these models focus exclusively on enhancing either understanding or generation capabilities. In contrast, our approach uses Pair-DPO to effectively narrow the gap between the two, achieving mutual improvement.