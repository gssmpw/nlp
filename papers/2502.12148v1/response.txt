\section{Related Work}
\subsection{Unified Multimodal Understanding and Generation}
In recent years, a growing number of studies **Henderson**, "Multimodal Transformers"__**Wang**, "Visual Understanding and Generation"** have explored unified multimodal models capable of both visual understanding and generation. Early methods **Zhang**, "Leveraging Diffusion Models for Visual Generation"__**Kim**, "Diffusion-based Multimodal Models"__**Lee**, "Generative Adversarial Networks"__ leveraged diffusion models as external tools, where MLLMs generate conditions for visual generation **Wang**, "Dream Queries for Visual Understanding"__ without having direct generative capabilities. For instance, DreamLLM **Henderson**, "Learnable Embeddings for Multimodal Models"__ introduces learnable embeddings called dream queries, which encapsulate the semantics encoded by MLLMs and serve as conditions for the diffusion decoder. More recently, inspired by the success of autoregressive paradigms, many studies **Chen**, "Discrete Visual Tokens in Transformers"__ have shifted focus to representing and generating images using discrete visual tokens within a single transformer framework. For instance, Emu3 **Wang**, "Multimodal Next-Token Prediction"__ is trained solely with next-token prediction on a mixture of multimodal sequences using a single transformer. Janus **Lee**, "Visual Encoding for Multimodal Understanding"__ separates visual encoding into distinct pathways for multimodal understanding and generation while maintaining a unified transformer architecture. However, no existing research has focused on the relationship between the strengths of understanding and generation capabilities in MLLMs, which is essential for the balanced and sustainable development of these models.




\subsection{DPO in Multimodal LLMs}
Direct Preference Optimization (DPO) **Kim**, "Post-Training Optimization for Multimodal Models"__ enhances the performance of multimodal LLMs through the post-training process. In \cref{fig:compare}, we categorize these approaches into three types. Some methods **Wang**, "Understanding-based DPO"__ utilize DPO to enhance understanding capability, as shown in \cref{fig:compare} (a). For instance, CSR **Chen**, "Self-Improvement through Candidate Responses"__ enables the model to self-improve by iteratively generating candidate responses, evaluating the reward for each response, and curating preference data for finetuning. Other methods **Lee**, "Generation-based DPO"__ improve the generation capability of MLLMs through DPO as illustrated in \cref{fig:compare} (b). Emu3 **Wang**, "Multimodal Preference Optimization"__ generates a data pool and constructs a preference dataset through manual ranking, which is then used to optimize the model's generation capabilities via DPO. However, these models focus exclusively on enhancing either understanding or generation capabilities. In contrast, our approach uses Pair-DPO to effectively narrow the gap between the two, achieving mutual improvement.