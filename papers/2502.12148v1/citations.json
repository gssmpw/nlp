[
  {
    "index": 0,
    "papers": [
      {
        "key": "dong2023dreamllm",
        "author": "Dong, Runpei and Han, Chunrui and Peng, Yuang and Qi, Zekun and Ge, Zheng and Yang, Jinrong and Zhao, Liang and Sun, Jianjian and Zhou, Hongyu and Wei, Haoran and others",
        "title": "Dreamllm: Synergistic multimodal comprehension and creation"
      },
      {
        "key": "ge2024seed",
        "author": "Ge, Yuying and Zhao, Sijie and Zhu, Jinguo and Ge, Yixiao and Yi, Kun and Song, Lin and Li, Chen and Ding, Xiaohan and Shan, Ying",
        "title": "Seed-x: Multimodal models with unified multi-granularity comprehension and generation"
      },
      {
        "key": "wu2023next",
        "author": "Wu, Shengqiong and Fei, Hao and Qu, Leigang and Ji, Wei and Chua, Tat-Seng",
        "title": "Next-gpt: Any-to-any multimodal llm"
      },
      {
        "key": "ye2024x",
        "author": "Ye, Hanrong and Huang, De-An and Lu, Yao and Yu, Zhiding and Ping, Wei and Tao, Andrew and Kautz, Jan and Han, Song and Xu, Dan and Molchanov, Pavlo and others",
        "title": "X-VILA: Cross-Modality Alignment for Large Language Model"
      },
      {
        "key": "ma2024janusflow",
        "author": "Ma, Yiyang and Liu, Xingchao and Chen, Xiaokang and Liu, Wen and Wu, Chengyue and Wu, Zhiyu and Pan, Zizheng and Xie, Zhenda and Zhang, Haowei and Zhao, Liang and others",
        "title": "Janusflow: Harmonizing autoregression and rectified flow for unified multimodal understanding and generation"
      },
      {
        "key": "shi2024llamafusion",
        "author": "Shi, Weijia and Han, Xiaochuang and Zhou, Chunting and Liang, Weixin and Lin, Xi Victoria and Zettlemoyer, Luke and Yu, Lili",
        "title": "LlamaFusion: Adapting Pretrained Language Models for Multimodal Generation"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "dong2023dreamllm",
        "author": "Dong, Runpei and Han, Chunrui and Peng, Yuang and Qi, Zekun and Ge, Zheng and Yang, Jinrong and Zhao, Liang and Sun, Jianjian and Zhou, Hongyu and Wei, Haoran and others",
        "title": "Dreamllm: Synergistic multimodal comprehension and creation"
      },
      {
        "key": "tong2024metamorph",
        "author": "Tong, Shengbang and Fan, David and Zhu, Jiachen and Xiong, Yunyang and Chen, Xinlei and Sinha, Koustuv and Rabbat, Michael and LeCun, Yann and Xie, Saining and Liu, Zhuang",
        "title": "MetaMorph: Multimodal Understanding and Generation via Instruction Tuning"
      },
      {
        "key": "ge2024seed",
        "author": "Ge, Yuying and Zhao, Sijie and Zhu, Jinguo and Ge, Yixiao and Yi, Kun and Song, Lin and Li, Chen and Ding, Xiaohan and Shan, Ying",
        "title": "Seed-x: Multimodal models with unified multi-granularity comprehension and generation"
      },
      {
        "key": "sun2024generative",
        "author": "Sun, Quan and Cui, Yufeng and Zhang, Xiaosong and Zhang, Fan and Yu, Qiying and Wang, Yueze and Rao, Yongming and Liu, Jingjing and Huang, Tiejun and Wang, Xinlong",
        "title": "Generative multimodal models are in-context learners"
      },
      {
        "key": "zhuang2024towards",
        "author": "Zhuang, Yiyu and He, Yuxiao and Zhang, Jiawei and Wang, Yanwen and Zhu, Jiahe and Yao, Yao and Zhu, Siyu and Cao, Xun and Zhu, Hao",
        "title": "Towards Native Generative Model for 3D Head Avatar"
      },
      {
        "key": "zhang2024realcompo",
        "author": "Zhang, Xinchen and Yang, Ling and Cai, Yaqi and Yu, Zhaochen and Wang, Kai-Ni and Tian, Ye and Xu, Minkai and Tang, Yong and Yang, Yujiu and Bin, CUI and others",
        "title": "Realcompo: Balancing realism and compositionality improves text-to-image diffusion models"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "yang2024mastering",
        "author": "Yang, Ling and Yu, Zhaochen and Meng, Chenlin and Xu, Minkai and Ermon, Stefano and Bin, CUI",
        "title": "Mastering text-to-image diffusion: Recaptioning, planning, and generating with multimodal llms"
      },
      {
        "key": "tian2024videotetris",
        "author": "Tian, Ye and Yang, Ling and Yang, Haotian and Gao, Yuan and Deng, Yufan and Chen, Jingmin and Wang, Xintao and Yu, Zhaochen and Tao, Xin and Wan, Pengfei and others",
        "title": "VideoTetris: Towards Compositional Text-to-Video Generation"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "dong2023dreamllm",
        "author": "Dong, Runpei and Han, Chunrui and Peng, Yuang and Qi, Zekun and Ge, Zheng and Yang, Jinrong and Zhao, Liang and Sun, Jianjian and Zhou, Hongyu and Wei, Haoran and others",
        "title": "Dreamllm: Synergistic multimodal comprehension and creation"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "team2024chameleon",
        "author": "Team, Chameleon",
        "title": "Chameleon: Mixed-modal early-fusion foundation models"
      },
      {
        "key": "xie2024show",
        "author": "Xie, Jinheng and Mao, Weijia and Bai, Zechen and Zhang, David Junhao and Wang, Weihao and Lin, Kevin Qinghong and Gu, Yuchao and Chen, Zhijie and Yang, Zhenheng and Shou, Mike Zheng",
        "title": "Show-o: One single transformer to unify multimodal understanding and generation"
      },
      {
        "key": "zhou2024transfusion",
        "author": "Zhou, Chunting and Yu, Lili and Babu, Arun and Tirumala, Kushal and Yasunaga, Michihiro and Shamis, Leonid and Kahn, Jacob and Ma, Xuezhe and Zettlemoyer, Luke and Levy, Omer",
        "title": "Transfusion: Predict the next token and diffuse images with one multi-modal model"
      },
      {
        "key": "qu2024tokenflow",
        "author": "Qu, Liao and Zhang, Huichao and Liu, Yiheng and Wang, Xu and Jiang, Yi and Gao, Yiming and Ye, Hu and Du, Daniel K and Yuan, Zehuan and Wu, Xinglong",
        "title": "TokenFlow: Unified Image Tokenizer for Multimodal Understanding and Generation"
      },
      {
        "key": "xie2024muse",
        "author": "Xie, Rongchang and Du, Chen and Song, Ping and Liu, Chang",
        "title": "MUSE-VL: Modeling Unified VLM through Semantic Discrete Encoding"
      },
      {
        "key": "zhang2024fate",
        "author": "Zhang, Jiawei and Wu, Zijian and Liang, Zhiyang and Gong, Yicheng and Hu, Dongfang and Yao, Yao and Cao, Xun and Zhu, Hao",
        "title": "FATE: Full-head Gaussian Avatar with Textural Editing from Monocular Video"
      },
      {
        "key": "wang2024emu3",
        "author": "Wang, Xinlong and Zhang, Xiaosong and Luo, Zhengxiong and Sun, Quan and Cui, Yufeng and Wang, Jinsheng and Zhang, Fan and Wang, Yueze and Li, Zhen and Yu, Qiying and others",
        "title": "Emu3: Next-token prediction is all you need"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "wang2024emu3",
        "author": "Wang, Xinlong and Zhang, Xiaosong and Luo, Zhengxiong and Sun, Quan and Cui, Yufeng and Wang, Jinsheng and Zhang, Fan and Wang, Yueze and Li, Zhen and Yu, Qiying and others",
        "title": "Emu3: Next-token prediction is all you need"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "wu2024janus",
        "author": "Wu, Chengyue and Chen, Xiaokang and Wu, Zhiyu and Ma, Yiyang and Liu, Xingchao and Pan, Zizheng and Liu, Wen and Xie, Zhenda and Yu, Xingkai and Ruan, Chong and others",
        "title": "Janus: Decoupling visual encoding for unified multimodal understanding and generation"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "rafailov2024direct",
        "author": "Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Manning, Christopher D and Ermon, Stefano and Finn, Chelsea",
        "title": "Direct preference optimization: Your language model is secretly a reward model"
      },
      {
        "key": "zhang2024itercomp",
        "author": "Zhang, Xinchen and Yang, Ling and Li, Guohao and Cai, Yaqi and Xie, Jiake and Tang, Yong and Yang, Yujiu and Wang, Mengdi and Cui, Bin",
        "title": "Itercomp: Iterative composition-aware feedback learning from model gallery for text-to-image generation"
      },
      {
        "key": "yang2025supercorrect",
        "author": "Yang, Ling and Yu, Zhaochen and Zhang, Tianjun and Xu, Minkai and Gonzalez, Joseph E and Cui, Bin and Yan, Shuicheng",
        "title": "SuperCorrect: Supervising and Correcting Language Models with Error-Driven Insights"
      },
      {
        "key": "yang2025reasonflux",
        "author": "Yang, Ling and Yu, Zhaochen and Cui, Bin and Wang, Mengdi",
        "title": "ReasonFlux: Hierarchical LLM Reasoning via Scaling Thought Templates"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "zhou2024aligning",
        "author": "Zhou, Yiyang and Cui, Chenhang and Rafailov, Rafael and Finn, Chelsea and Yao, Huaxiu",
        "title": "Aligning modalities in vision large language models via preference fine-tuning"
      },
      {
        "key": "zhou2024calibrated",
        "author": "Zhou, Yiyang and Fan, Zhiyuan and Cheng, Dongjie and Yang, Sihan and Chen, Zhaorun and Cui, Chenhang and Wang, Xiyao and Li, Yun and Zhang, Linjun and Yao, Huaxiu",
        "title": "Calibrated self-rewarding vision language models"
      },
      {
        "key": "he2024self",
        "author": "He, Jiayi and Lin, Hehai and Wang, Qingyun and Fung, Yi and Ji, Heng",
        "title": "Self-correction is more than refinement: A learning framework for visual and language reasoning tasks"
      },
      {
        "key": "zhang2024critic",
        "author": "Zhang, Di and Lei, Jingdi and Li, Junxian and Wang, Xunzhi and Liu, Yujie and Yang, Zonglin and Li, Jiatong and Wang, Weida and Yang, Suorong and Wu, Jianbo and others",
        "title": "Critic-v: Vlm critics help catch vlm errors in multimodal reasoning"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "zhou2024calibrated",
        "author": "Zhou, Yiyang and Fan, Zhiyuan and Cheng, Dongjie and Yang, Sihan and Chen, Zhaorun and Cui, Chenhang and Wang, Xiyao and Li, Yun and Zhang, Linjun and Yao, Huaxiu",
        "title": "Calibrated self-rewarding vision language models"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "wang2024emu3",
        "author": "Wang, Xinlong and Zhang, Xiaosong and Luo, Zhengxiong and Sun, Quan and Cui, Yufeng and Wang, Jinsheng and Zhang, Fan and Wang, Yueze and Li, Zhen and Yu, Qiying and others",
        "title": "Emu3: Next-token prediction is all you need"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "wang2024emu3",
        "author": "Wang, Xinlong and Zhang, Xiaosong and Luo, Zhengxiong and Sun, Quan and Cui, Yufeng and Wang, Jinsheng and Zhang, Fan and Wang, Yueze and Li, Zhen and Yu, Qiying and others",
        "title": "Emu3: Next-token prediction is all you need"
      }
    ]
  }
]