\section{Related Work}
\subsection{Unified Multimodal Understanding and Generation}
In recent years, a growing number of studies ____ have explored unified multimodal models capable of both visual understanding and generation. Early methods ____ leveraged diffusion models as external tools, where MLLMs generate conditions for visual generation ____ without having direct generative capabilities. For instance, DreamLLM ____ introduces learnable embeddings called dream queries, which encapsulate the semantics encoded by MLLMs and serve as conditions for the diffusion decoder. More recently, inspired by the success of autoregressive paradigms, many studies ____ have shifted focus to representing and generating images using discrete visual tokens within a single transformer framework. For instance, Emu3 ____ is trained solely with next-token prediction on a mixture of multimodal sequences using a single transformer. Janus ____ separates visual encoding into distinct pathways for multimodal understanding and generation while maintaining a unified transformer architecture. However, no existing research has focused on the relationship between the strengths of understanding and generation capabilities in MLLMs, which is essential for the balanced and sustainable development of these models.




\subsection{DPO in Multimodal LLMs}
Direct Preference Optimization (DPO) ____ enhances the performance of multimodal LLMs through the post-training process. In \cref{fig:compare}, we categorize these approaches into three types. Some methods ____ utilize DPO to enhance understanding capability, as shown in \cref{fig:compare} (a). For instance, CSR ____ enables the model to self-improve by iteratively generating candidate responses, evaluating the reward for each response, and curating preference data for finetuning. Other methods ____ improve the generation capability of MLLMs through DPO as illustrated in \cref{fig:compare} (b). Emu3 ____ generates a data pool and constructs a preference dataset through manual ranking, which is then used to optimize the model's generation capabilities via DPO. However, these models focus exclusively on enhancing either understanding or generation capabilities. In contrast, our approach uses Pair-DPO to effectively narrow the gap between the two, achieving mutual improvement.