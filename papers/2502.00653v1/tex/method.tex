




\section{Methodology}\label{sec:method}
\subsection{Model Overview}
Given a benign MLLM with parameters $\boldsymbol\theta$, our goal is to learn a robust MLLM with parameters $\boldsymbol\theta^{*}$. This process can be represented as $\boldsymbol{\theta} \xrightarrow{\Delta\boldsymbol\theta^{*}} \boldsymbol{\theta}^{*}$, where $\Delta\boldsymbol\theta^*$ denotes the finetuned parameters optimized to defend against jailbreak attacks while preserving the model's utility in standard interactions.  
% \jc{ $\theta \to \boldsymbol\theta$? I think the notation here is a bit confusing as $[,]$ is typically used for concatenating non-overlapping parts where your case is more like  $\boldsymbol\theta^* = \boldsymbol\theta + \Delta\boldsymbol\theta^*$? or you can use $[,]$ to combine image encoder, and llm part together? \textcolor{blue}{\checkmark}}
% tuned from an existing benign MLLM parameterized by $\theta$ to defend against jailbreak attacks while maintaining its utility in normal interactions. 
Note that the trainable parameters $\Delta\boldsymbol\theta^*$ are obtained from the cross-modal adapter and LLM decoder, optimized using LoRA~\citep{hu2021lora}, while the parameters of the visual encoder are fixed, following existing MLLM training methods~\citep{liu2024visual,dai2023instructblip}. 
After tuning, the learned parameters $\boldsymbol\theta^*$ and the corresponding gradient information will be publicly released to potential attackers. To achieve this goal, we propose {\name}, which is an adversarial tuning framework to enhance the robustness of MLLMs.
As shown in Figure~\ref{fig:method}, the proposed {\name} operates in two iterative steps -- generating the most substantial attack perturbations (Step I) and mitigating their impact through model tuning (Step II).
Next, we will introduce the details of {\name} in each step.
 % Given a benign MLLM $\mathcal{M}$, our task is to update its parameters $\theta$ to defend against jailbreak attacks while maintaining its utility in normal interactions. In our problem setting, the model will be publicly released after the updating, allowing a potential attacker to perform jailbreak attacks using full knowledge of the model's parameters and gradient information. To address this, we propose the {\name} training algorithm, which enhances the robustness of the MLLM through an adversarial tuning framework.

% \textbf{Step I} -- As discussed in Section~\ref{sec:related_work}, three types of jailbreak attacks exist to generate perturbations on different modalities. An ideal robust MLLM can successfully defend against any jailbreak attack. Toward this end, we propose the Contrastive Embedding Attack ({\attack}) 
% % \jc{not a big deal but "CE" can be confused with cross-entropy. a different acronym?\textcolor{blue}{\checkmark}} 
% strategy to generate strong attack perturbations by directly injecting perturbations into the model's embeddings and optimizing them via a contrastive goal. 
% \textbf{Step II} -- After generating the jailbreak perturbations, we update the MLLM in the second step. The updating is performed by combining two objectives: the new model should mitigate the harmful jailbreak effects based on the perturbed prompts while maintaining normal performance on benign queries. Finally, we obtain the fine-tuned MLLM by repeating the above steps for $T$ iterations. 
% Next, we will introduce the details of {\name} in each step.

% The key idea of {\name} relies on tuning the model parameters $\theta$ to defend against the worst-case adversarial prompts. As a result, the whole process consists of two steps at each training iteration. In the first step, we start by sampling a small corpus consisting of $N$ malicious queries $\{\mathbf{x}_1\cdots\mathbf{x}_{N}\}$, e.g.,
% $\mathbf{x}_1 = \textit{how to create a bomb?}$. Based on these queries, we aim to find a jailbreak prompt under the worst-case scenario. Considering that most existing attack methods add perturbations to only one modality input, which may not be strong enough in our framework. Thus, we propose the Contrastive Embedding Attack ({\attack}) strategy, which directly injects perturbations into model's embeddings and optimizes them via a contrastive goal. 




% As a result, our proposed {\name} algorithm builds on an adversarial tuning framework. The framework aims to train a based on a toxic query dataset and a benign instruction-tuning dataset C.

% For each iteration, the training process
% At the first step, the  .
% At the second step, we optimize the model parameters $\theta$ based on the generated perturbations. During the optimization, we apply the lora strategy and thus enabling the overall computational cost.

% Inspired by the Projected Gradient Desent algorithm (PGD), our motivation builds. Next, we will first introduce a unified view of finding these strong adversarial perturbations on $\mathcal{M}$. Then, we detail the overall adversarial training algorithm of the proposed {\name}. 


\subsection{Step I: Contrastive Embedding Attacks (CoE-Attack)}\label{sec:step_1}

Existing jailbreak attack approaches achieve the attacks usually through introducing adversarial perturbations across different modalities, such as placing an adversarial image $\mathbf{I}^{\prime}$ before the malicious query $\mathbf{x}_n \in \mathcal{X}$~\citep{niu2024jailbreaking} or appending a string suffix $\mathbf{x}^{\prime}$ after the query~\citep{zou2023universal}, where $\mathcal{X}$ denotes the collection of malicious queries. However, only perturbing a specific modality may lead to a weak attack under the multimodal scenario. One straightforward approach to seeking the worst-case attack is to simultaneously optimize an adversarial image $\mathbf{I}^{\prime}$ and a text suffix $\mathbf{x}^{\prime}$ by maximizing the likelihood of generating the positive affirmation $\mathbf{c}_n$ (e.g., ``\textit{Sure, here are steps for a bad thing}'') of the malicious query $\mathbf{x}_n$. 

This naive strategy will face two challenges. On the one hand, this process could be highly computationally intensive, as the text suffix requires a greedy search over the vocabulary, while the image perturbations need to be processed through a heavy vision encoder. 
On the other hand, as noted in existing work~\citep{xu2024safedecoding}, the probability of generating token sequences that align with negative responses (e.g., ``\textit{As an AI language model, I cannot \dots}'') is not small enough after the attack, which makes the model still output a refusal answer after the decoding strategies.
To tackle these challenges, we propose a novel {\attack} strategy, where the adversarial perturbations are injected directly as token embeddings, thus reducing overall computing resources. Additionally, we further introduce a contrastive loss based on a negative response $\mathbf{r}_n$ of $\mathbf{x}_n$ to enhance the attack strength. Consequently, the proposed {\attack} can perform a powerful jailbreak attack without intensive computational consumption.

\textbf{Data Preparation}. During each training iteration $i$, we first sample a small corpus of malicious queries $\mathcal{X}_i = \{\mathbf{x}_1, \cdots, \mathbf{x}_N\}$ from the toxic dataset $\mathcal{X}$, i.e., $\mathcal{X}_i \subset \mathcal{X}$. For each query $\mathbf{x}_n \in \mathcal{X}_i$, we adopt \texttt{gpt-4-turbo} to generate the affirmative response $\mathbf{c}_n$ 
% \jc{maybe explain here we don't get the full malicious response just the affirmative sencetence?\textcolor{blue}{\checkmark}} 
and the negative response $\mathbf{r}_n$ based on the prompt detailed in Appendix \textcolor{red}{\ref{app:gpt_prompts}}. Here, we only collect the positive affirmation rather than the full malicious responses, as designing precise harmful replies tailored to different queries is inherently difficult and requires inevitable manual efforts.
When generating the responses $\mathbf{c}_n$ and $\mathbf{r}_n$, we explicitly request \texttt{gpt-4-turbo} to generate them with different semantic styles and structures, allowing us to train adversarial perturbations on more diverse linguistic patterns. 

\textbf{Perturbation Initialization}. Based on these responses, {\attack} will optimize the adversarial perturbations from the token embedding level. 
Specifically, we first randomly initialize two perturbation matrices  $\mathbf{P}_{0}^{h} \in \mathbb{R}^{K \times C}$ and $\mathbf{P}_{0}^{t} \in \mathbb{R}^{K \times C}$ from word token embeddings, where $K$ denotes the number of tokens and $C$ is the embedding dimension using the query set $\mathcal{X}_i$. Thus, we initialize these two perturbation matrices at each iteration due to the change of the new malicious query set. We position $\mathbf{P}_{0}^{h}$ in front of the text query to act as the adversarial image $\mathbf{I}^{\prime}$. This design is based on the fact that in all MLLMs, the image is always placed before the text as input. Similarly, $\mathbf{P}_{0}^{t}$ is positioned after the text query to act as the adversarial string suffix $\mathbf{x}^{\prime}$. As a result, we omit $\mathbf{I}^{\prime}$ and $\mathbf{x}^{\prime}$ in the inputs and directly optimize the perturbations on $\mathbf{P}_{0}^{h}$ and $\mathbf{P}_{0}^{t}$ based on $N$ query-response pairs and the following attack objective. 


\textbf{Attack Objectives}. As discussed above, a strong jailbreak attack should fulfill the following two objectives: (1) amplifying the probability of generating tokens aligned with the attacker's goal and (2) diminishing the probability of generating tokens aligned with safety instructions or negative responses simultaneously. The first objective can be easily achieved by optimizing the following loss:
\begin{gather} 
L_{\rm adv}^{\rm target}=-\sum_{n=1}^{N}\log[p(\mathbf{c}_{n}|\mathbf{P}_{0}^{h}, \mathbf{x}_n, \mathbf{P}_{0}^{t})],
\label{eq:1}
\end{gather}
where $p$ is the likelihood probability of generating the target response based on the model parameters $\boldsymbol\theta_{i-1}$ in the current ${i}$-th iteration.

% \begin{algorithm}[t]
%  \small
% 	\renewcommand{\algorithmicrequire}{\textbf{Input:}}
        
% 	\renewcommand{\algorithmicensure}{\textbf{Output:}}
% 	\caption{{\name}}
% 	\label{alg1}
%         \begin{algorithmic}[1]
%         \Require A benign MLLM $\mathcal{M}$ parameterized by $\boldsymbol\theta$,  a dataset $\mathcal{X}$ composed of malicious queries, a dataset $\mathcal{V}$ composed of benign multimodal samples.
%         \renewcommand{\algorithmicrequire}{\textbf{Parameters:}}
%         \Require  $\lambda$, $\epsilon$, training steps for attack loop $M$, total training steps $T$, and $\boldsymbol\theta_0=\boldsymbol\theta$.
        
%         \For{$i = 1, \cdots, T$}
%          \State \textcolor{red}{//Step I: Adopting the {\attack} strategy to generate adversarial perturbations}
%         \State Sample $N$ malicious queries $\{\mathbf{x}_1,\cdots, \mathbf{x}_N\}$ from $\mathcal{X}$;
%         \State For each $\mathbf{x}_n$, get the corresponding affirmative response $\mathbf{c}_n$ and negative response $\mathbf{r}_n$:
%         \State \qquad $\mathbf{c}_n,\mathbf{r}_n=LLM.\text{get\_response}(\mathbf{x}_n,\text{Prompt})$;
%         \State Initialize two token sequences, and get their token embeddings $\mathbf{P}_0^{h}$,\;$\mathbf{P}_0^{t}$;
         
%             \For{$m =1,\cdots,M$}
%             \State Calculate the adversarial attack loss $L_{\rm adv}$ based on Eq.~(\ref{eq:3});
%             \State Update the adversarial embeddings $\{\mathbf{P}_{m-1}^{h},\mathbf{P}_{m-1}^{t}\}$ to $\{\mathbf{P}_{m}^{h},\mathbf{P}_{m}^{h}\}$ based on the gradient descent 
%             \State of $L_{\rm adv}$ with $\epsilon$;
%             % \State \qquad $\mathbf{P}_{k+1}^{h}=\mathbf{P}_{k}^{h}+\epsilon \nabla L_{\rm adv}$,\quad $\mathbf{P}_{k+1}^{t}=\mathbf{P}_{k}^{t}+\epsilon\nabla L_{\rm adv}$
%             \EndFor
%             \State \textcolor{red}{//Step II: Model training for defending against jailbreak attacks}
%             \State Calculate the defense loss $L_{\rm def}$ based on $\mathbf{P}_{M}^{h}$,\;$\mathbf{P}_{M}^{t}$ and Eq.~(\ref{eq:5});
%             \State Sample $H$ benign image-test pairs from $\mathcal{V}$;
%             \State Calculate the utility loss $L_{\rm utility}$ based on  Eq.~(\ref{eq:6});
%             \State Update the model parameters to $\boldsymbol\theta_{i}$ by minimizing $L_{\rm def} + L_{\rm utility}$;
%             % \State \qquad $L = L_{\rm def} + L_{\rm SFT}$
%         \EndFor
%         \State \textbf{return} $\boldsymbol\theta^{*}=\boldsymbol\theta_{T}$.
% \end{algorithmic}
% \label{alg:1}
% \end{algorithm}



To achieve the second objective, a naive solution is to reduce the model's log probabilities of generating a rejective response $\mathbf{r}_n$, e.g., 
$\sum_{n=1}^{N}\log[p(\mathbf{r}_{n}|\mathbf{P}_{0}^{h}, \mathbf{x}_n, \mathbf{P}_{0}^{t})]$. However, directly applying this term may yield even worse results, as simply reducing the probability of generating a pre-defined sentence can be too strong, causing the model to generate meaningless texts after the attack. 
% (\textit{An in-depth analysis of this observation can be found in Section~\ref{sec:ablation} Figure~\ref{fig:logp}.})
As a result, we propose using a contrastive loss to \textit{relatively} suppress the model's log probability of generating $\mathbf{r}_n$.
Specifically, the contrastive loss encourages the model to choose the affirmative tone $\mathbf{c}_n$ over the negative tone $\mathbf{r}_n$, thereby guiding the victim model to avoid generating refusal tokens without producing nonsense texts after the attack. The proposed loss $L_{\rm adv}^{\rm contra}$ can be formulated as follows:
\begin{align}\label{eq:2}
    L_{\rm adv}^{\rm contra} = 
    & -\sum_{n=1}^{N} \log \sigma \bigg[ \log \big( p(\mathbf{c}_{n}|\mathbf{P}_{0}^{h}, \mathbf{x}_n, \mathbf{P}_{0}^{t}) \big) \notag \\
    & \hspace{2em} - \log \big( p(\mathbf{r}_{n}|\mathbf{P}_{0}^{h}, \mathbf{x}_n, \mathbf{P}_{0}^{t}) \big) \bigg],
\end{align}
where $\sigma$ is the Sigmoid function. The final attack objective at the $i$-th iteration is obtained by combining the above loss terms with a scalar hyperparameter $\lambda$, which yields: 
\begin{equation}\label{eq:3}
    L_{\rm adv}= L_{\rm adv}^{\rm target}+\lambda \cdot L_{\rm adv}^{\rm contra}.
\end{equation}

\textbf{Perturbation Optimization.} We optimize $\{\mathbf{P}_0^{h}, \mathbf{P}_0^{t}\}$ by minimizing the attack loss $L_{\rm adv}$ via a multi-step process, where the MLLM parameters are fixed. At the step $m-1$, the adversarial embeddings $\{\mathbf{P}_{m-1}^{h}, \mathbf{P}_{m-1}^{t}\}$ are updated based on the gradient descent of $L_{\rm adv}$ with a learning rate of $\epsilon$, resulting in $\{\mathbf{P}_{m}^{h}, \mathbf{P}_{m}^{t}\}$. We repeat this process for $M$ iterations, and obtain the final adversarial token embeddings $\{\mathbf{P}_{M}^h, \mathbf{P}_{M}^t\}$.
% \begin{equation}\label{eq:4}
%    \mathbf{P}_{m}^{h}=\mathbf{P}_{m-1}^{h}+\epsilon\cdot\frac{\partial L_{\rm adv}}{\partial \mathbf{P}_{m-1}^{h}},\qquad \mathbf{P}_{ m}^{t}=\mathbf{P}_{m-1}^{t}+\epsilon\cdot \frac{\partial L_{\rm adv}}{\partial \mathbf{P}_{m-1}^{t}},\\
% \end{equation}
% \jc{if space is not a issue, maybe don't need this equation, just say gradient ascent\textcolor{blue}{\checkmark}} \jc{also fix algorithm?}
% Finally, We repeat this process for $M$ times, which results in the adversarial token embeddings $\{\mathbf{P}_{M}^h,\mathbf{P}_{M}^t\}$. 

\subsection{Step II: Model Training for Defending Against Jailbreak Attacks}
% In this section, we introduce our proposed adversarial training approach. The key idea of {\name} relies on optimizing the benign MLLM $\mathcal{M}$ against the adversarial token embeddings, which are built on the malicious queries $\{\mathbf{x}_1\cdots\mathbf{x}_N\}$ and the corresponding affirmative and rejective labels as mentioned above.
% Next, we will detail our training strategy from the label initialization, inner-attack loop, and model optimization.

% \textbf{Label Initialization.} At each training iteration, we first sample a small corpus consisting of $N$ malicious queries $\{\mathbf{x}_1\cdots\mathbf{x}_{N}\}$ from the training set $\mathcal{X}$. For each toxic query, we need a response $\mathbf{c}_i$ with an affirmative tone and another response $\mathbf{r}_i$ with a negative tone as labels. Considering that a potential attacker might launch an attack by setting the affirmative tone in different expressions, the selection of $\mathbf{c}_i$ and $\mathbf{r}_i$ should also exhibit diversity in our training.

% Consequently, we directly leverage the LLM to generate $\mathbf{r}_i$ and $\mathbf{c}_i$ for each query $\mathbf{x}_i$. We explicitly request the LLM to generate these responses with different semantic styles and structures, allowing us to train the model on more diverse linguistic patterns. Finally, the responses $\{(\mathbf{c}_1,\mathbf{r}_1)\cdots(\mathbf{c}_N,\mathbf{r}_N)\}$ will be served as labels for the subsequent training. 

% \textbf{Inner attack loop.} We now perform the inner-attack loop based on $\{\mathbf{x}_1\cdots\mathbf{x}_N\}$ and their corresponing labels  $\{(\mathbf{c}_1,\mathbf{r}_1)\cdots(\mathbf{c}_N,\mathbf{r}_N)\}$. As mentioned before, we first initiate two embedding matrices $\{\mathbf{P}_0^{h}, \mathbf{P}_0^{t}\}$, where each one is an embedding of a token sequence with a length of $K$. During the attack, they are placed at the beginning and end of each malicious query, acting as jailbreak prompts under different modalities.
% We optimize $\{\mathbf{P}_0^{h}, \mathbf{P}_0^{t}\}$ to minimize the attack loss $L_{\rm adv}$ via a multi-step process. At each step, the adversarial embeddings are updated by applying the gradient of $L_{\rm adv}$ with a learning rate of $\epsilon$, which yields:

% \begin{equation}\label{eq:4}
%    \mathbf{P}_{m+1}^{h}=\mathbf{P}_{m}^{h}+\epsilon\cdot{\partial L_{\rm adv}}/{\partial \mathbf{P}_{m}^{h}},\qquad \mathbf{P}_{m+1}^{t}=\mathbf{P}_{m}^{t}+\epsilon\cdot{\partial L_{\rm adv}}/{\partial \mathbf{P}_{m}^{t}},\\
% \end{equation}
% where $\{\mathbf{P}_m^h,\mathbf{P}_m^t\}$ are adversarial embeddings at the $m$-th step. We repeat this process for $T_s$ times, which results in the adversarial token embeddings $\{\mathbf{P}_{T_s}^h,\mathbf{P}_{T_s}^t\}$. 

Now we need to update the model parameters $\boldsymbol\theta_{i-1}$ in the $i$-th iteration. As mentioned earlier, the update of $\boldsymbol\theta_{i-1}$ needs to satisfy two objectives: (1) mitigating the impact of perturbations $\{\mathbf{P}_{M}^h,\mathbf{P}_{M}^t\}$ on toxic inputs and (2) ensuring the performance unchanged on regular inputs. Therefore, we build the training loss based on two terms, including a defense loss $L_{\rm def}$ for attack mitigation and another utility term $L_{\rm utility}$. Note that both loss terms are computed on different inputs, and the summation of these two losses will be used to update $\boldsymbol\theta_{i-1}$ to $\boldsymbol\theta_{i}$ simultaneously.

Specifically, given the malicious query $\mathbf{x}_n$ along with the perturbed embeddings as model inputs, the defense loss $L_{\rm def}$ first ensures that the model can output the safety statement $\mathbf{r}_n$. Additionally, we also apply the contrastive loss to encourage the model to select $\mathbf{r}_n$ over the affirmative response $\mathbf{c}_n$, thereby further reducing the probability of generating $\mathbf{c}_n$ and mitigating the effect of these adversarial perturbations.
Mathematically, we have $L_{\rm def}$ formulated as follows:
{\small
\begin{align}\label{eq:5}
    L_{\rm def}^{\rm target} &= -\sum_{n=1}^{N} 
    \log\left[p(\mathbf{r}_{n}|\mathbf{P}_{M}^{h},\mathbf{x}_n,\mathbf{P}_{M}^{t})\right], \\
    L_{\rm def}^{\rm contra} &= -\sum_{n=1}^{N} \log \sigma \bigg[
    \log \big( p(\mathbf{r}_{n}|\mathbf{P}_{M}^{h},\mathbf{x}_n,\mathbf{P}_{M}^{t}) \big) \notag \\
    &\quad - \log \big( p(\mathbf{c}_{n}|\mathbf{P}_{M}^{h},\mathbf{x}_n,\mathbf{P}_{M}^{t}) \big)
    \bigg],  \\
    L_{\rm def} \phantom{^{\rm pl}} &=\phantom{^{\rm pl}} L_{\rm def}^{\rm target} + \lambda \cdot L_{\rm def}^{\rm contra}.
\end{align}
}










where $\lambda$ is the coefficient as defined in $L_{\rm adv}$, and the pair of $\{\mathbf{P}_{M}^h,\mathbf{P}_{M}^t\}$ is fixed. For the utility loss term $L_{\rm utility}$, we directly build it on $H$ benign image-question pairs extracted from a multimodal instruction-tuning dataset $\mathcal{V}$, which yields: 
\begin{equation}\label{eq:6}
   L_{\rm utility} =-\sum_{j=1}^{H}\log\left[p(\mathbf{y}_{j}|\mathbf{I}_{j},\mathbf{q}_j)\right], 
\end{equation}
where $\mathbf{I}_j$, $\mathbf{q}_j$, and $\mathbf{y}_j$ represent the reference image, question, and ground-truth answer, respectively. We update the trainable LoRA parameters and obtain $\boldsymbol\theta_i$ by minimizing $L_{\rm def}+L_{\rm utility}$. Finally, we obtain the fine-tuned MLLM with parameters $\boldsymbol\theta^{*}=\boldsymbol\theta_{T}$ by repeating the above two steps at each iteration. The overall algorithm is also summarized in Algorithm~\ref{alg:1} of Appendix~\ref{apd:algorithm}.

% \textcolor{blue}{-----------Below is the old version!!!!--------}



% As mentioned above, a jailbreak attack on $\{\mathbf{x}_1\cdots\mathbf{x}_N\}$ involves introducing adversarial perturbations across different modalities, such as placing an adversarial image before the malicious query~\citep{niu2024jailbreaking} or appending a string suffix after the query~\citep{zou2023universal}. Therefore, one straightforward approach to seeking the worst-case attack is to simultaneously optimize an adversarial image $\mathbf{I}^{\prime}$ and a text suffix $\mathbf{x}^{\prime}$ by maximizing the likelihood of generating the positive affirmation. However, this process could be highly computationally intensive, as the text suffix requires a greedy search over the vocabulary, while the image perturbations need to be processed through a heavy vision encoder. 
% To solve this problem, we propose {\attack}, where the adversarial perturbations are injected directly as token embeddings, thus reducing overall computing resources while maintaining attack effectiveness. 


% \textbf{Adversarial Perturbation Inputs}. Before each training iteration, we first sample a small corpus of malicious queries $\{\mathbf{x}_1\cdots\mathbf{x}_N\}$ from the toxic dataset $\mathcal{X}$. Specifically, before conducting an attack, we first initiate a perturbation set that contains two matrices $\{\mathbf{P}_{0}^{h}, \mathbf{P}_{0}^{t}\}$. Each matrix is an embedding of a sequence consisting of $K$ tokens, resulting in a shape of $K \times C$, where $C$ is the embedding dimension. 
% As shown in Figure 1, during the attack loop, we skip the image input and directly place $\mathbf{P}_{0}^{h}, \mathbf{P}_{0}^{t}$ at the beginning and end of each malicious query, respectively. \textit{From a universal perspective, the two embeddings act as image and text perturbations in our threat model.}
% These adversarial embeddings will be updated based on the attack objective detailed below.

% \textbf{Attack Objective}. Now our target is to optimize perturbations  $\{\mathbf{P}_{0}^{h}, \mathbf{P}_{0}^{t}\}$  based on $N$ malicious queries $\{\mathbf{x}_1,\cdots,\mathbf{x}_N\}$. One initial attack objective is to increase the probability of generating an affirmative tone $\mathbf{c}_i$, e.g., \textit{Sure, here are the steps to \dots}, for each toxic query $\mathbf{x}_i$. This can be achieved by minimizing the following term:
% \begin{gather} 
% L_{\rm target}=-\sum_{i=1}^{N}\log[p_{\theta}(\mathbf{c}_{i}|\mathbf{P}_{0}^{h}, \mathbf{x}_i, \mathbf{P}_{0}^{t})],
% \label{eq:2}
% \end{gather}
% where $p_{\theta}$ is the likelihood probability of generating the target response.
% Although this objective is effective, it remains insufficient. One important reason is the probability of generating token sequences that align with safety instructions (e.g., ``\textit{As an AI language model, I cannot\dots}'') is not small enough after the attack~\citep{xu2024safedecoding,zhang2024jailbreak}. Consequently, the model may still output a refusal answer after the decoding strategies.
% To overcome this issue, we propose that a strong jailbreak attack should fulfill the following two objectives: (1) amplify the probability of generating tokens aligned with the attacker's goal, and (2) diminish the probability of generating tokens aligned with safety instructions or refusal tones at the same time.

% We directly apply the term $L_{\rm target}$ to achieve the first objective. For the second objective, a naive solution is to reduce the model's log probabilities of generating a rejective response $\mathbf{r}_i$, e.g., 
% $\sum_{i=1}^{N}\log[p_{\theta}(\mathbf{r}_{i}|\mathbf{P}_{0}^{h}, \mathbf{x}_i, \mathbf{P}_{0}^{t})]$. However, directly applying this term may yield even worse results, as simply reducing the probability of generating a pre-defined sentence can be too strong, causing the model to generate meaningless texts after the attack.
% As a result, we propose using a contrastive loss to \textit{relatively} suppress the model's log probability of generating $\mathbf{r}_i$.
% Specifically, the contrastive loss encourages the model to choose the affirmative tone $\mathbf{c}_i$ over the negative tone $\mathbf{r}_i$, thereby guiding the victim model to avoid generating refusal tokens without producing nonsense texts after the attack. The proposed loss $L_{\rm contrastive}$ can be formulated as follows:
% \begin{equation}\label{eq:2}
%     L_{\rm contrastive}=-\sum_{i=1}^{N}{\log\sigma\left[\log(p_{\theta}(\mathbf{c}_{i}|\mathbf{P}_{0}^{h}, \mathbf{x}_i, \mathbf{P}_{0}^{t}))-\log(p_{\theta}(\mathbf{r}_{i}|\mathbf{P}_{0}^{h}, \mathbf{x}_i, \mathbf{P}_{0}^{t}))\right]}, \\
% \end{equation}

% where $\sigma$ is the sigmoid function. For the affirmative label $\mathbf{c}_i$ and negative label $\mathbf{r}_i$, we explicitly request the LLM to generate them with different semantic styles and structures, allowing us to train the perturbed embeddings on more diverse linguistic patterns.  The final attack objective can be formulated as follows: 
% \begin{equation}\label{eq:3}
%     L_{\rm adv}= L_{\rm target}+\lambda \cdot L_{\rm contrastive},
% \end{equation}
% where $\lambda$ is a scalar hyperparameter. By minimizing the attack loss $L_{\rm adv}$, we can generate strong adversarial embeddings, which enable subsequent model optimization against these perturbations.

% \textbf{Optimization.} 
% We optimize $\{\mathbf{P}_0^{h}, \mathbf{P}_0^{t}\}$ by minimizing the attack loss $L_{\rm adv}$ via a multi-step process. At each step, the adversarial embeddings are updated by applying the gradient of $L_{\rm adv}$ with a learning rate of $\epsilon$, which yields:

% \begin{equation}\label{eq:4}
%    \mathbf{P}_{m+1}^{h}=\mathbf{P}_{m}^{h}+\epsilon\cdot{\partial L_{\rm adv}}/{\partial \mathbf{P}_{m}^{h}},\qquad \mathbf{P}_{m+1}^{t}=\mathbf{P}_{m}^{t}+\epsilon\cdot{\partial L_{\rm adv}}/{\partial \mathbf{P}_{m}^{t}},\\
% \end{equation}
% where $\{\mathbf{P}_m^h,\mathbf{P}_m^t\}$ are adversarial embeddings at the $m$-th step. We repeat this process for $T_s$ times, which results in the adversarial token embeddings $\{\mathbf{P}_{T_s}^h,\mathbf{P}_{T_s}^t\}$. 





% % \footnote{ We realize there is a more intuitive objective is to maximize the difference between the log probabilities of generating $\mathbf{c}_i$ and $\mathbf{r}_i$, e.g., $L_{\rm adv}=\log(p_{\theta}(\mathbf{c}_{i}|\mathbf{P}_{k}^{h}\oplus\mathbf{x}_i\oplus\mathbf{P}_{k}^{t}))-\log(p_{\theta}(\mathbf{r}_{i}|\mathbf{P}_{k}^{h}\oplus\mathbf{x}_i\oplus\mathbf{P}_{k}^{t}))$ . However, we found it may result in poor attack performance as it will . We put more empircal analysis in Section B}

% % \begin{gather}
% % L_{\rm choice}=\sum_{i=1}^{N}{\log\sigma\left(\log(p_{\theta}(\mathbf{c}_{i}|\mathbf{P}_{k}^{h}\oplus\mathbf{x}_i\oplus\mathbf{P}_{k}^{t}))-\log(p_{\theta}(\mathbf{r}_{i}|\mathbf{P}_{k}^{h}\oplus\mathbf{x}_i\oplus\mathbf{P}_{k}^{t}))\right)} \\
% % L_{\rm adv}=L_{\rm target}+\alpha\cdot L_{\rm choice}
% % \label{eq:2}
% % \end{gather}

% % Based on the above attack objective, we can now optimize target is to optimize the by minimize the shown above. Specifically, the adversarial perturbation sets can be optimized as following:



% \subsection{Model Training}
% % In this section, we introduce our proposed adversarial training approach. The key idea of {\name} relies on optimizing the benign MLLM $\mathcal{M}$ against the adversarial token embeddings, which are built on the malicious queries $\{\mathbf{x}_1\cdots\mathbf{x}_N\}$ and the corresponding affirmative and rejective labels as mentioned above.
% % Next, we will detail our training strategy from the label initialization, inner-attack loop, and model optimization.

% % \textbf{Label Initialization.} At each training iteration, we first sample a small corpus consisting of $N$ malicious queries $\{\mathbf{x}_1\cdots\mathbf{x}_{N}\}$ from the training set $\mathcal{X}$. For each toxic query, we need a response $\mathbf{c}_i$ with an affirmative tone and another response $\mathbf{r}_i$ with a negative tone as labels. Considering that a potential attacker might launch an attack by setting the affirmative tone in different expressions, the selection of $\mathbf{c}_i$ and $\mathbf{r}_i$ should also exhibit diversity in our training.

% % Consequently, we directly leverage the LLM to generate $\mathbf{r}_i$ and $\mathbf{c}_i$ for each query $\mathbf{x}_i$. We explicitly request the LLM to generate these responses with different semantic styles and structures, allowing us to train the model on more diverse linguistic patterns. Finally, the responses $\{(\mathbf{c}_1,\mathbf{r}_1)\cdots(\mathbf{c}_N,\mathbf{r}_N)\}$ will be served as labels for the subsequent training. 

% % \textbf{Inner attack loop.} We now perform the inner-attack loop based on $\{\mathbf{x}_1\cdots\mathbf{x}_N\}$ and their corresponing labels  $\{(\mathbf{c}_1,\mathbf{r}_1)\cdots(\mathbf{c}_N,\mathbf{r}_N)\}$. As mentioned before, we first initiate two embedding matrices $\{\mathbf{P}_0^{h}, \mathbf{P}_0^{t}\}$, where each one is an embedding of a token sequence with a length of $K$. During the attack, they are placed at the beginning and end of each malicious query, acting as jailbreak prompts under different modalities.
% % We optimize $\{\mathbf{P}_0^{h}, \mathbf{P}_0^{t}\}$ to minimize the attack loss $L_{\rm adv}$ via a multi-step process. At each step, the adversarial embeddings are updated by applying the gradient of $L_{\rm adv}$ with a learning rate of $\epsilon$, which yields:

% % \begin{equation}\label{eq:4}
% %    \mathbf{P}_{m+1}^{h}=\mathbf{P}_{m}^{h}+\epsilon\cdot{\partial L_{\rm adv}}/{\partial \mathbf{P}_{m}^{h}},\qquad \mathbf{P}_{m+1}^{t}=\mathbf{P}_{m}^{t}+\epsilon\cdot{\partial L_{\rm adv}}/{\partial \mathbf{P}_{m}^{t}},\\
% % \end{equation}
% % where $\{\mathbf{P}_m^h,\mathbf{P}_m^t\}$ are adversarial embeddings at the $m$-th step. We repeat this process for $T_s$ times, which results in the adversarial token embeddings $\{\mathbf{P}_{T_s}^h,\mathbf{P}_{T_s}^t\}$. 

% Now we need to update the LoRA parameters $\Delta\theta$. As mentioned earlier, the update of $\Delta\theta$ needs to satisfy two objectives: mitigating the impact of perturbations $\{\mathbf{\mathbf{P}_{T_s}^h},\mathbf{\mathbf{P}_{T_s}^t}\}$ on toxic inputs and ensuring the performance unchanged on regular inputs. Therefore, we build the training loss based on two terms, including a defense loss $L_{\rm def}$ for attack mitigation and another utility term $L_{\rm utility}$. Note that both loss terms are computed on different inputs, and we sum the losses to update $\Delta\theta$.

% Specifically, given the malicious query $\mathbf{x}_i$ along with the perturbed embeddings as model inputs, the defense loss $L_{\rm def}$ first ensures that the model can output the safety statement $\mathbf{r}_i$. Additionally, we also apply the contrastive loss to encourage the model to select $\mathbf{r}_i$ over the affirmative response $\mathbf{c}_i$, thereby further reducing the probability of generating $\mathbf{c}_i$ and mitigating the effect of these adversarial perturbations.
% Mathematically, we have $L_{\rm def}$ formulated as follows:


% \small
% \begin{equation}\label{eq:5}
%    L_{\rm def} = -\sum_{i=1}^{N}\underbrace{\log\left[p_{\theta}(\mathbf{r}_{i}|\mathbf{P}_{T_s}^{h},\mathbf{x}_i,\mathbf{P}_{T_s}^{t})\right]}_{\text{Target Loss} \; L_{\rm target}} 
%    - \underbrace{\lambda \log \sigma \left[ \log(p_{\theta}(\mathbf{r}_{i}|\mathbf{P}_{T_s}^{h},\mathbf{x}_i,\mathbf{P}_{T_s}^{t})) - \log(p_{\theta}(\mathbf{c}_{i}|\mathbf{P}_{T_s}^{h},\mathbf{x}_i,\mathbf{P}_{T_s}^{t})) \right]}_{\text{Contrastive Loss}\; L_{\rm contrastive}},
% \end{equation}
% where $\lambda$ is the scalar coefficient. For the utility loss term $L_{\rm utility}$, we directly build it on $M$ benign image-question pairs extracted from a multi-modal instruction-tuning dataset $\mathcal{V}$, which yields: 
% \begin{equation}\label{eq:6}
%    L_{\rm utility} =-\sum_{i=1}^{M}\log\left[p_{\theta}(\mathbf{y}_{i}|\mathbf{I}_{i},\mathbf{q}_i)\right], 
% \end{equation}
% where $\mathbf{I}_i$, $\mathbf{q}_i$, and $\mathbf{y}_i$ represent the reference image, question, and ground-truth answer, respectively. Finally, we update $\theta$ by minimizing $L_{\rm def}+L_{\rm utility}$. When performing optimization, {\name} keeps the parameters of the visual encoder fixed and only optimizes the cross-modal adapter and the LLM decoder. In addition, we adopt the low-rank adaptation training (LoRA)~\citep{hu2021lora} strategy on the LLM decoder, thus further reducing the trainable parameters and making the training process more efficient. The overall algorithm is shown in Algorithm~\ref{alg:1}.







%