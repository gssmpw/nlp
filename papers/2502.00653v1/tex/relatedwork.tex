\section{Related Work}\label{sec:related_work}
% \subsection{Multimodal Large Language Models (MLLMs)}
\textbf{Multimodal Large Language Models (MLLMs).}
c

% \subsection{Jailbreak Attacks on MLLMs}\label{sec:jailbreakattacks}
\textbf{Jailbreak Attacks on MLLMs.} 
Existing jailbreak attacks on MLLMs can be categorized based on the modalities they exploit, such as images, text, or both.
\textbf{Image-based} attacks~\citep{niu2024jailbreaking,qi2024visual} attempt to bypass the model's internal safeguards by pairing toxic queries with adversarial images. These images can be optimized either to increase the likelihood of generating a positive response to the harmful query~\citep{niu2024jailbreaking}, or by training on a small dataset of toxic text~\citep{qi2024visual}.
{Most \textbf{text-based} jailbreaks~\citep{zou2023universal,liuautodan,DBLP:journals/corr/abs-2310-08419,DBLP:journals/corr/abs-2309-10253} are originally designed for LLMs. One approach is to craft semantically meaningful prompts that fool a targeted LLM in a black-box scenario. For example, GPTFuzzer~\citep{DBLP:journals/corr/abs-2309-10253} transforms human-curated templates to craft jailbreak prompts, and PAIR~\citep{DBLP:journals/corr/abs-2310-08419} directly utilizes another LLM to produce these prompts.
Another approach is injecting non-word adversarial noise in the white-box scenario. For example, GCG~\citep{zou2023universal} modifies the original query by optimizing an adversarial suffix, while AutoDAN~\citep{liuautodan} injects natural text segments into toxic queries via a genetic algorithm.
\textbf{Image-text-based} methods~\citep{li2024images,gong2023figstep,liu2023mm} leverage domain transfer techniques to obscure harmful keywords by embedding them into typography within images on various backgrounds, making detection more difficult.
In this paper, we introduce {\name}, a defense mechanism designed to mitigate all the above attack methods in white-box scenarios.

% \subsection{Jailbreak Defenses on MLLMs}\label{sec:jailbreakdefenses}
\textbf{Jailbreak Defenses on MLLMs.}
Current defense strategies for MLLMs generally fall into two categories. One approach involves introducing additional modules~\citep{helff2024llavaguard,pi2024mllm,wang2024adashield} at the inference stage, such as using an LLM-based detoxifier to neutralize toxic output~\citep{pi2024mllm} or embedding an adaptive safety statement into the MLLM's system prompts~\citep{wang2024adashield}. However, these methods are often accompanied by high computational overhead and are limited by the capabilities of external resources.
The second approach is to perform safety-alignment fine-tuning of the target MLLM, either by fine-tuning on new datasets~\citep{zong2024safety} or using reinforcement learning from human feedback (RLHF)~\citep{chen2024dress}. 
 In contrast to these methods, our proposed {\name} offers robust defenses against jailbreak attacks in white-box scenarios without requiring additional modules.



