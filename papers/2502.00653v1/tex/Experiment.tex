
\section{Experimental Setups}\label{sec:exp_setup}



\textbf{Jailbreak Methods.}
We conduct experiments on jailbreak attacks across different modalities.
For \textit{image-based jailbreak attacks}, we first evaluate the \textbf{ImgJP Attack} method~\citep{niu2024jailbreaking}, which applies image perturbations to induce affirmative responses to toxic queries. Following the setup in~\citep{niu2024jailbreaking}, we assess performance on the first 100 prompts. We also compare against the \textbf{Visual Adversarial Attack (VAA)}~\citep{qi2024visual}, which directly optimizes image noise to maximize the likelihood of generating toxic text. For this, we follow~\citep{qi2024visual} and evaluate on the Harmful Instructions dataset, which contains 40 toxic prompts.
For \textit{text-based jailbreak attacks}, we test the suffix attack method \textbf{GCG}~\citep{zou2023universal} and \textbf{AutoDAN}~\citep{liuautodan}, which uses a genetic algorithm to inject more naturally adversarial strings. Both attacks are evaluated on the first 100 queries from the AdvBench dataset, following their original settings.
Finally, for \textit{image-text jailbreak attacks}, we evaluate \textbf{FigStep}~\citep{gong2023figstep}, following the setup in~\citep{gong2023figstep} on the SafeBench-Tiny dataset. We also compare \textbf{MM-SafetyBench}~\citep{liu2023mm} following the setup in~\citep{liu2023mm} on the MM-SafetyBench dataset.
Detailed implementations and attack configurations for these methods are provided in Appendix \textcolor{red}{\ref{app:mllms}}.

\textbf{Datasets.} For each jailbreak method, we use the same dataset and implementations as in the corresponding papers to ensure optimal hyperparameter settings in the attack setup. Specifically, we use four toxic query datasets—AdvBench~\citep{zou2023universal}, Harmful Instructions~\citep{qi2024visual}, SafeBench-Tiny~\citep{gong2023figstep}, and MM-SafetyBench~\citep{liu2023mm}—for robustness evaluation.
For the utility evaluation, we first evaluate using 100 samples from the LLaVA-Instruct-80K dataset~\citep{li2023llava}. Following LLaVA~\citep{li2023llava}, we use \texttt{gpt-4-turbo} to evaluate the models' responses to these questions. Additionally, we adopt the widely used MLLM evaluation benchmark, MM-Vet~\citep{mmvet}, to comprehensively evaluate the impact of the fine-tuned model on benign image-text questions.
Detailed descriptions of these datasets are provided in Appendix \textcolor{red}{\ref{app:toxic_datasets}}.


\textbf{Victim MLLMs.} We validate the effectiveness of {\name} on \textbf{six widely used MLLMs}, including MiniGPT-v4-7B, MiniGPT-v4-13B~\citep{zhu2023minigpt4}, InstructBLIP-7B, InstructBLIP-13B~\citep{dai2023instructblip}, LLaVA-7B, and LLaVA-13B~\citep{liu2024visual}.
Detailed descriptions of these models are provided in Appendix \textcolor{red}{\ref{app:victim_models}}. 
% All jailbreak attacks are conducted in a white-box setting.

\textbf{Baselines.}
To the best of our knowledge, {\name} is the first approach to implement adversarial training on MLLMs. Therefore, in our experiments, we first evaluate the defense performance of the \textbf{original} MLLM  without any adversarial training by subjecting it to the aforementioned attacks. We also compare an MLLM defense method \textbf{VLGuard}~\citep{zong2024safety}, which directly fine-tunes the original MLLM on a safety dataset consisting of toxic images and questions and safe response labels. For a fair comparison, we evaluated the fine-tuned LLAVA-7B and LLAVA-13B models officially released by~\cite{zong2024safety}.
\footnote{https://github.com/ys-zong/VLGuard?tab=readme-ov-file}
Given that each MLLM uses an LLM as its text decoder, another intuitive solution is to directly apply existing LLM-based adversarial training methods to the decoder. For this, we adopt \textbf{R2D2}~\citep{mazeika2024harmbench} and \textbf{CAT}~\citep{xhonneux2024efficient} as baselines, where we first tune the LLM decoder with these methods and then connect the fine-tuned LLM with the visual encoder and cross-modal adapter. For hyperparameter settings and implementation details of {\name}, please refer to Appendix \textcolor{red}{\ref{app:implementation}}.





\section{Experimental Results} 
\subsection{Robustness \& Utility Evaluation}\label{exp:evaluation}
In this section, we evaluate the robustness of all methods across six attack strategies and six MLLMs. For this, we use the Attack Success Rate (\textbf{ASR}) as the primary metric, which measures the proportion of toxic outputs generated after the attacks. To determine whether a response is toxic or unsafe, we follow the protocols used in~\citep{qi2024fine} and~\citep{cao2024personalized}, using \texttt{gpt-4-turbo} to provide a binary ``\textit{Yes}'' or ``\textit{No}'' answer,  along with a brief explanation based on the prompt, which is detailed in Appendix \textcolor{red}{\ref{app:gpt_prompts}}. For more information on the attack datasets and settings used in this evaluation, refer to Appendix Sections \textcolor{red}{\ref{app:toxic_datasets}} and \textcolor{red}{\ref{app:mllms}}.

\input{tex/big_table}
\begin{figure*}[t]
% \vspace{-0.4in}
\begin{center}
\includegraphics[width=0.99\linewidth]{Figure/exp_utility.png}
\end{center}
\vspace{-0.15in}
\caption{The utility evaluation of different methods on six MLLMs. The experiment is conducted on 100 samples from the \texttt{LLaVA-Instruct-80K} dataset, and we follow~\cite {liu2024visual} to evaluate the quality of responses based on scores generated by \texttt{gpt-4-turbo}.}
\label{fig:sample}
% \vspace{-0.15in}
\label{fig:utility_main}
\end{figure*}
\begin{table*}[t]
\setlength{\tabcolsep}{3pt}
% \vspace{-0.1in}
\caption{
Ablation study results of module removal in ASR (\%).
Attacks are conducted on 13B models using the ImgJP attack method on the AdvBench dataset. ``$\times$'' denotes that we remove the corresponding modules in {\name} when fine-tuning the target model. $\mathbf{P}_0^h$ and $\mathbf{P}_0^t$ are the token embedding matrices placed before and after the query, respectively.
$L^{\rm target}_{\rm adv}$ and $L^{\rm contra}_{\rm adv}$ are the target and contrastive loss defined in Eq.~(\ref{eq:1}) and Eq.~(\ref{eq:2}), respectively.
$L^{\rm target}_{\rm def}$ and $L^{\rm contra}_{\rm def}$ are the target and contrastive loss used for updating the model parameters, and they are defined in Eq.~(\ref{eq:5}). We remove the target and contrastive losses simultaneously for both the attack stage (step I) and the defense stage (step II). We report the percentage of ASR ($\downarrow$) for the \textbf{robustness} evaluation and GPT scores ($\uparrow$) for the \textbf{utility} evaluation. }
\vspace{-0.1in}
    \begin{center}
        \resizebox{\linewidth}{!}{
    \begin{tabular}{c|>{\centering\arraybackslash}p{0.4in}|>{\centering\arraybackslash}p{0.4in}|>{\centering\arraybackslash}p{0.4in}|>{\centering\arraybackslash}p{0.4in}|>{\centering\arraybackslash}p{0.4in}|>{\centering\arraybackslash}p{0.4in}|>{\centering\arraybackslash}p{0.4in}|ccc}
    \toprule
    Test&$\mathbf{P}_0^{h}$&$\mathbf{P}_0^{t}$&$L^{\rm target}_{\rm adv}$&$L^{\rm contra}_{\rm adv}$&$L^{\rm target}_{\rm def}$&$L^{\rm contra}_{\rm def}$& $L_{\rm utility}$&MiniGPT-v4 & InstructBLIP & LLaVA \\ 
    \hline
   \multirow{5}{*}{\makecell{\rotatebox{90}{\small{\textbf{Robustness}}}}}&$\times$&&&&&& & 5.00 & 23.00 & 1.00  \\ 
    &&$\times$&&&&& & 2.00 & 1.00 & 0.00  \\ 
   &&&$\times$&&$\times$&& &8.00&20.00&0.00  \\ 
   &&&&$\times$&&$\times$&  &23.00 &18.00& 0.00 \\ \cline{2-11}
   &\multicolumn{7}{c|}{{\name}} &\cellcolor{gray!15} 0.00 &\cellcolor{gray!15} 0.00&\cellcolor{gray!15} 0.00 \\ 
   \hline\hline
   \multirow{2}{*}{\makecell{\rotatebox{90}{\small{\textbf{Utility}}}}}&&&&&&&$\times$  & 2.10&1.97&7.29  \\\cline{2-11}
   &\multicolumn{7}{c|}{{\name}}&\cellcolor{gray!15} 6.81&\cellcolor{gray!15}7.34&\cellcolor{gray!15}7.45 \\
   \bottomrule

    
    \end{tabular}
    }
    \end{center}
    
    \label{tab:ablation}
    \vspace{-0.1in}
\label{tab:ablation_robust}
\end{table*}

\textbf{Robustness Evaluation.} 
We first evaluate the performance of ImgJP, VAA, GCG, and AutoDAN, which use adversarial images and texts to conduct jailbreak attacks. The results are presented in Table~\ref{tab:attack_results}. We use ASR as the evaluation metric, where \textit{a lower value indicates better defense performance}. 
We can first observe that the existing safety-alignment training, VLGuard, can not defend against the white-box attacks, which is aligned with the conclusion in~\citep{zong2024safety}. In addition, our proposed {\name} significantly outperforms all baselines across the six target MLLMs. Specifically, it achieves an average improvement of 17.6\%, 4.2\%, 9.7\% and 25.8\% on the ImgJP, VAA, GCG and AutoDAN, respectively. Additionally, {\name} exhibits lower ASR scores on MLLMs with larger model sizes (13B vs. 7B), which we attribute to the increased number of trainable parameters facilitating adversarial training and enhancing robustness. Overall, these results clearly demonstrate the effectiveness of {\name} in defending against image-based jailbreak attacks.

{\name} also demonstrates its robustness in defending against black-box attacks, including the FigStep and MM-SafetyBench methods. As shown in Table~\ref{tab:attack_results}, we can observe that the safety fine-tuning method VLGuard can perform well.
The LLM-based adversarial training methods R2D2 and CAT are not effective in defending against such attacks, as they primarily inject toxic content into texts. 
Although {\name} focuses on white-box scenarios, it still performs well against both black-box attacks.
Thus, these results have demonstrated the extraordinary generalization ability of {\name} in defending against jailbreak attacks across different modalities and scenarios.



\textbf{Utility Evaluation.}
We use 100 image-text questions extracted from LLaVA-Instruct-80K, ensuring no overlap with the prompts used in our adversarial training to evaluate the utility of the fine-tuned MLLMs.
Following~\citep{liu2024visual}, we use the \texttt{gpt-4-turbo} to generate scores based on the helpfulness, relevance, accuracy, and level of detail of each response. Scores are ranged from 1 to 10. We adopt the same GPT prompt in ~\citep{liu2024visual}.
The results are illustrated in Figure~\ref{fig:utility_main}, showing that our proposed {\name} effectively defends against white-box jailbreak attacks while ensuring that regular users' interactions remain minimally affected. We put the utility results on the MM-Vet benchmark in Appendix \textcolor{red}{\ref{app:more_utility}}.

\subsection{Ablation Study}\label{sec:ablation}

\textbf{Ablation study on the robustness design}.
We first analyze the impact of removing different modules from {\name} on the robustness. The experiments are conducted using ImgJP on the 13B models. We report the ASR (\%) values as illustrated in Table~\ref{tab:ablation_robust}. For the LLaVA model, we observe that removing any module does not significantly affect its ASR performance. We attribute this to the fact that the LLM decoder of LLaVA is built on the safety-aligned Vicuna-1.5. 
However, removing any single component negatively impacts the overall robustness of MiniGPT-v4 and InstructBLIP. The impact is significant after removing the contrastive loss, where the average ASR is dropped by 13.67\%. We provide more analysis of these ablation results in Appendix~\ref{app:ablationresults}.


\textbf{Ablation study on the utility loss $L_{\rm utility}$}. We also evaluate the impact of removing the utility loss $L_{\rm utility}$. We use the same 100 image-question pairs as mentioned in the utility evaluation in Section~\ref{exp:evaluation} and conduct the experiments on MLLMs with 13B parameters. GPT scores are shown in Table~\ref{tab:ablation_robust}. From the table, we can observe that the utility score decreases after removing $L_{\rm utility}$, with the largest performance gap at 5.37 points among all models. We attribute this to the fact that not using $L_{\rm utility}$ results in numerous rejective responses, which leads to a very low score. We have included more samples in Appendix~\textcolor{red}{\ref{app:case_study}}. 

\textbf{Extra Experimental Results.} They include a discussion of the perturbation set $\{\mathbf{P}_0^h, \mathbf{P}_0^t\}$ in Appendix \textcolor{red}{\ref{app:efficiency}}, the hyperparameter analysis of $\lambda$ used in Eqs.~(\ref{eq:3}) and~(\ref{eq:5}) and the token length $K$ defined in $\mathbf{P}_0^h$ and $\mathbf{P}_0^t$ in Appendix \textcolor{red}{\ref{app:hyperparameter}}, and a case study in Appendix \textcolor{red}{\ref{app:case_study}}.



\section{Conclusion}
% \vspace{-0.1in}
This paper aims to defend against diverse jailbreak attacks by fine-tuning multimodal large language models (MLLMs). Correspondingly, we propose the {\name} framework, which uses the {\attack} strategy to generate adversarial embeddings and iteratively update model parameters, mitigating attacks while preserving benign performance. Substantive experimental results across six MLLMs and six jailbreak methods demonstrate {\name}'s effectiveness in safeguarding MLLMs while maintaining their functionality.


% \newpage
\section{Limitation}
In this work, we have proposed {\name} against jailbreak attacks on multimodal large language models. We acknowledge that our current method has the following two limitations. First, SafeMLLM focuses solely on image- and text-based attack methods. Therefore, it may be ineffective if malicious users exploit other modalities, such as audio or video, for attacks. Based on this, extending SafeMLLM to defend against potential jailbreak threats across a broader range of modalities is crucial, and we leave this as our future work. Another limitation is that SafeMLLM currently focuses solely on defending against jailbreak attacks. Expanding SafeMLLM to address a wider range of security threats on MLLMs is worth exploring, which we leave for future exploration.

\section{Ethical Statements}
In this paper, we focus on defending against jailbreak attacks on multimodal large language models (MLLMs). The proposed {\name} framework demonstrates its ability to secure a robust MLLM capable of mitigating jailbreak attacks across various modalities in different scenarios. 
We believe that {\name} MLLMs can provide valuable inspiration for building safer MLLM applications in the future.
In designing {\name}, we clearly acknowledge that the data used in both the training and testing processes may include, but is not limited to, harmful suggestions on toxic behaviors, hate speech, and discriminatory content. \textbf{We claim that all toxic data used in this paper is publicly available, has undergone corresponding safety reviews, and is strictly limited to the model training and testing processes in our paper.} We will release the {\name} training framework and the corresponding fine-tuned modes in the near future, thereby contributing to the construction of safer AI systems.

\noindent{\textbf{AI assistants in this research.}} We only adopt the AI assistant tool at the sentence level for fixing grammar and polishing sentences.









% \subsection{Experiment VII: Hyperparameter study on $\alpha$ (MiniGPT4-13B,visual gcg)}
% \begin{tabular}{c|ccccc}
% \hline
% $\alpha$ & 0.001& 0.01  & 0.1 & 1.0 & 10.0 \\ 
% \hline
%  training&4 & 2 & 2 & 3 & 5 \\ 
%  testing& &  & 2 & & \\ 
% \hline
% \end{tabular}


