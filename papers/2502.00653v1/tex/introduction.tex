\begin{figure}[t] 
    \centering
   \includegraphics[width=0.9\linewidth]{Figure/intrr_acl.pdf}  % 200 units wide, 100 units tall empty frame
   \vspace{-0.1in}
    \caption{Illustration of the vulnerability of existing safety-tuning methods compared with our model {\name}. The defender first fine-tunes the original MLLM in step 1. The attackers then attack the fine-tuned MLLMs in step 2 in different ways. In step 3, the fine-tuned MLLMs generate outputs. Details of the experiment settings can be found in Section~\ref{sec:exp_setup}. }
    \label{fig:intro}
    \vspace{-0.15in}
\end{figure}

\section{Introduction}

Multimodal large language models (MLLMs) have demonstrated remarkable success across various tasks~\citep{liu2024visual,driess2023palm,fu2024video}. However, recent studies also reveal their security threats~\citep{qi2024visual,bailey2023image,lu2024test} in different domains. Among these risks, a rising concern is \textbf{jailbreak attacks}, where attackers can bypass the safety guardrails of MLLMs and prompt them to generate harmful content or illegal suggestions. There are several widely used ways to defend against jailbreak attacks on MLLMs, including content filtering based on post-processing~\citep{pi2024mllm,gou2024eyes,helff2024llavaguard} and safety fine-tuning~\citep{zong2024safety,chen2024dress}.
 % robust prompt engineering~\citep{wang2024adashield}, 

Implementing strong content filters is required to introduce a third-party large language model (LLM) or MLLM to scan generated output and block harmful or inappropriate responses before they are delivered. However, these filters are not inherently designed to function as harmful content discriminators, and simply relying on their capabilities may lead to inaccurate filtering results~\citep{cao2023defending}. 
Safety fine-tuning approaches have been proposed to directly align MLLM outputs with human values to alleviate these issues. These methods typically involve either fine-tuning the model on an instruction-tuning dataset~\citep{zong2024safety} containing toxic image and question inputs paired with safety response labels, or employing reinforcement learning from human feedback (RLHF)~\citep{chen2024dress}. Despite these efforts, such alignment strategies can still be circumvented by carefully crafted adversarial perturbations, particularly in \textbf{white-box} scenarios, where the attacker has access to the model's parameters and gradient information~\citep{zong2024safety}. As shown in Figure~\ref{fig:intro}, we evaluate a representative safety-tuning approach, VLGuard~\citep{zong2024safety}, on the LLaVA model~\citep{liu2024visual}. The results indicate that VLGuard fails to withstand two typical white-box attack methods, ImgJP~\citep{niu2024jailbreaking} and GCG~\citep{zou2023universal}, which introduce adversarial perturbations to either the image or text modality. This contrasts with its performance in defending against another black-box attack, FigStep~\citep{gong2023figstep}, an image-text attack method that directly transforms toxic keywords into an image.
Based on these results, it is critical to explore a novel, robust defense paradigm capable of mitigating various jailbreak attacks across different modalities in MLLMs, especially in white-box scenarios.



% To illustrate this vulnerability, we conducted an experiment examining the susceptibility of a representative safety-tuning approach, VLGuard~\citep{zong2024safety}, under various attacks across different modalities. We adopt ImgJP~\citep{niu2024jailbreaking} and GCG~\citep{zou2023universal} as white-box attack methods, which add adversarial noise to images and texts, respectively. 
%  The results are shown in Figure~\ref{fig:intro}, our average results on LLaVA 7B and 13B models~\citep{liu2024visual} indicate that VLGuard fails to withstand these ``white-box'' attacks that introduce adversarial perturbations to either the image or text modality. This contrasts with its performance in defending against 'black-box' attacks, such as FigStep~\citep{gong2023figstep}, an image-text attack method that directly transforms toxic keywords into an image.
% Based on these results, it is critical to explore a novel, robust defense paradigm capable of mitigating various jailbreak attacks across different modalities in MLLMs, especially in white-box scenarios.



\begin{figure*}[t]  % 't' places the frame at the top of the page
    \centering
   \includegraphics[width=1.0\linewidth]{Figure/method.pdf}  % 200 units wide, 100 units tall empty frame
   \vspace{-0.25in}
    \caption{Overview of the proposed {\name}, which contains two iterative steps. In Step I, we fix the parameters of the MLLM. {\name} optimizes two noise matrices initialized by $\mathbf{P}^h_0$ and $\mathbf{P}^t_0$ with $M$ steps. 
    Step II aims to update the parameters of MLLMs by fixing the learned $\mathbf{P}^h_M$ and $\mathbf{P}^t_M$ when calculating the defense loss $L_{\rm def}$. To guarantee the utility of the fined-tuned MLLM, we also introduce a utility loss $L_{\rm utility}$. The updated model parameters are then used in Step I again.}
    \label{fig:method}
    \vspace{-0.2in}
\end{figure*}

A straightforward solution to these issues is to apply existing adversarial training techniques~\citep{bai2021recent}, generating adversarial samples and using them to fine-tune the target model. However, most current adversarial training methods focus on closed-set classification tasks~\citep{DBLP:conf/iclr/MadryMSTV18,shafahi2020universal}, making them unsuitable for direct deployment on MLLMs, which involve open-ended generation tasks.
While some efforts have been validated on LLMs~\citep{mazeika2024harmbench,xhonneux2024efficient,DBLP:journals/corr/abs-2406-06622}, 
significant barriers remain when applying these methods to MLLMs due to the \textbf{multimodal nature} of jailbreak attacks on MLLMs, where the attacks can be executed on images, text, or both modalities, as discussed earlier. Furthermore, the adversarial perturbations used in these approaches are usually generated by approximating gradients on discrete text~\citep{mazeika2024harmbench,DBLP:journals/corr/abs-2406-06622}, which renders the fine-tuned model insufficiently robust against stronger attacks, such as noisy images with continuous values.

To address these challenges, we propose a novel adversarial training framework, {\name}, the \textit{first} to perform \textbf{adversarial tuning on MLLMs}. As illustrated in Figure~\ref{fig:method}, {\name} iteratively generates adversarial perturbations (Step I) and updates the model to mitigate their effects (Step II).
In \textbf{Step I}, we introduce a contrastive embedding attack ({\attack}) that injects adversarial noise at the token embedding level (i.e., $\mathbf{P}^h_0$ and $\mathbf{P}^t_0$) to simulate toxic prompts across modalities. The noise is optimized by maximizing the likelihood of producing a positive affirmation. To further strengthen the attack, we incorporate a contrastive loss term that minimizes the model's probability of generating safety responses.
In \textbf{Step II}, the model parameters are updated to counteract the fixed adversarial noise ($\mathbf{P}^h_M$ and $\mathbf{P}^t_M$). We also leverage a utility loss based on benign image-question pairs to preserve normal user interactions. 

% The resulting fine-tuned MLLM effectively resists diverse jailbreak attacks, even in fully accessible model settings. In our experimentWe evaluate {\name} using six jailbreak attack methods across six MLLMs. Experimental results demonstrate that {\name} effectively defends against white-box attacks across different modalities. Additionally, utility evaluations on benign image-text pairs show that {\name} preserves the model's ability to handle normal interactions without degradation. 

The main contribution of this work can be summarized into the following three folds: (1) To the best of our knowledge, this is the first study to use adversarial training as a defense strategy against jailbreak attacks on MLLMs. We hope it can serve as a valuable reference for enhancing MLLM safety in future research. (2) We propose an adversarial training framework {\name}, which incorporates a novel {\attack} module and a contrastive loss to enhance MLLM's robustness against jailbreak attacks on diverse modalities. (3) We evaluate {\name} using six jailbreak attack methods across six MLLMs. Experimental results demonstrate that {\name} effectively defends against white-box attacks across different modalities. Additionally, utility evaluations on benign image-text pairs show that {\name} preserves the model's ability to handle normal interactions without degradation. 






