@incollection{Bengio+chapter2007,
author = {Bengio, Yoshua and LeCun, Yann},
booktitle = {Large Scale Kernel Machines},
publisher = {MIT Press},
title = {Scaling Learning Algorithms Towards {AI}},
year = {2007}
}

@article{Shieber1994,
  title={Lessons from a restricted Turing test},
  author={Shieber, Stuart M},
  journal={Communications of the ACM},
  volume={37},
  number={6},
  pages={70--78},
  year={1994},
  publisher={ACM}
}

@inproceedings{Geman2015,
  title={Visual Turing Test for Computer Vision Systems},
  author={Donald Geman and Stuart Geman and Neil Hallonquist and Laurent Younes},
  booktitle={Proceedings of the National Academy of Sciences},
  year={2015},
  volume={112},
  pages={3618-3623},
  doi={10.1073/pnas.1418077112}
}



@article{Marcus2018,
  title={Deep Learning: A Critical Appraisal},
  author={Gary Marcus},
  journal={arXiv},
  year={2018},
  eprint={1801.00631},
  archivePrefix={arXiv},
  primaryClass={cs.AI}
}


@inproceedings{VonAhn2003CAPTCHA,
  title={CAPTCHA: Using Hard AI Problems for Security},
  author={Luis von Ahn and Manuel Blum and Nicholas J. Hopper and John Langford},
  booktitle={Advances in Cryptology — EUROCRYPT 2003},
  year={2003},
  pages={294-311},
  publisher={Springer Berlin Heidelberg},
  doi={10.1007/3-540-39200-9_18}
}


@article{Bringsjord2001Creativity,
  title={Creativity, the Turing Test, and the (Better) Lovelace Test},
  author={Bringsjord, Selmer and Bello, Paul and Ferrucci, David},
  journal={Minds and Machines},
  volume={11},
  pages={3--27},
  year={2001},
  publisher={Kluwer Academic Publishers}
}


@article{Searle1980,
  author  = {John R. Searle},
  title   = {Minds, Brains, and Programs},
  journal = {Behavioral and Brain Sciences},
  year    = {1980},
  volume  = {3},
  number  = {3},
  pages   = {417--424},
}


@inproceedings{Levesque2011Winograd,
  author    = {H. J. Levesque},
  title     = {The Winograd Schema Challenge},
  booktitle = {AAAI Spring Symposium: Logical Formalizations of Commonsense Reasoning},
  year      = {2011}
}

@article{lipton2018mythos,
  title={The mythos of model interpretability: In machine learning, the concept of interpretability is both important and slippery.},
  author={Lipton, Zachary C},
  journal={Queue},
  volume={16},
  number={3},
  pages={31--57},
  year={2018},
  publisher={ACM New York, NY, USA}
}

@article{Hu2016NetworkTA,
  title={Network Trimming: A Data-Driven Neuron Pruning Approach towards Efficient Deep Architectures},
  author={Hengyuan Hu and Rui Peng and Yu-Wing Tai and Chi-Keung Tang},
  journal={ArXiv},
  year={2016},
  volume={abs/1607.03250},
}

@book{Grunwald2005,
  title={Advances in Minimum Description Length: Theory and Applications},
  editor={Grünwald, Peter D. and Myung, Jay Injae and Pitt, Mark A.},
  year={2005},
  publisher={MIT Press},
  address={Cambridge, MA},
  isbn={9780262072625},
  series={Neural Information Processing}
}

@inproceedings{radford2021learning,
  title={Learning transferable visual models from natural language supervision},
  author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
  booktitle={International conference on machine learning},
  pages={8748--8763},
  year={2021},
  organization={PMLR}
}

@inproceedings{Levesque2012Winograd,
  author    = {H. J. Levesque and E. Davis and L. Morgenstern},
  title     = {The Winograd Schema Challenge},
  booktitle = {Proceedings of the Thirteenth International Conference on Principles of Knowledge Representation and Reasoning},
  year      = {2012}
}


@inproceedings{kocijan2023Winogradscheme,
title = {The defeat of the Winograd Schema Challenge},
journal = {Artificial Intelligence},
volume = {325},
pages = {103971},
year = {2023},
issn = {0004-3702},
author = {Vid Kocijan and Ernest Davis and Thomas Lukasiewicz and Gary Marcus and Leora Morgenstern},
keywords = {Commonsense reasoning, Winograd Schema Challenge}
}

@article{levesque2014onourbestbehaviour,
title = {On our best behaviour},
journal = {Artificial Intelligence},
volume = {212},
pages = {27-35},
year = {2014},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2014.03.007},
url = {https://www.sciencedirect.com/science/article/pii/S0004370214000356},
author = {Hector J. Levesque},
keywords = {IJCAI Research Excellence},
abstract = {The science of AI is concerned with the study of intelligent forms of behaviour in computational terms. But what does it tell us when a good semblance of a behaviour can be achieved using cheap tricks that seem to have little to do with what we intuitively imagine intelligence to be? Are these intuitions wrong, and is intelligence really just a bag of tricks? Or are the philosophers right, and is a behavioural understanding of intelligence simply too weak? I think both of these are wrong. I suggest in the context of question-answering that what matters when it comes to the science of AI is not a good semblance of intelligent behaviour at all, but the behaviour itself, what it depends on, and how it can be achieved. I go on to discuss two major hurdles that I believe will need to be cleared.}
}


@book{Epstein2008,
  title={Parsing the Turing Test: Philosophical and Methodological Issues in the Quest for the Thinking Computer},
  author={Epstein, Robert and Roberts, Gary and Beber, Grace},
  year={2008},
  publisher={Springer Science \& Business Media}
}


@misc{cerf1972parry,
  author = {Cerf, Vinton},
  title = {PARRY encounters ELIZA},
  note = {Based on a demonstration at the International Conference on Computer Communication (ICCC) 1972},
  year = {1972},
  howpublished = {\url{https://datatracker.ietf.org/doc/html/rfc439}}
}

@article{weizenbaum1966eliza,
  title={ELIZA—A computer program for the study of natural language communication between man and machine},
  author={Weizenbaum, Joseph},
  journal={Communications of the ACM},
  volume={9},
  number={1},
  pages={36--45},
  year={1966},
  publisher={ACM}
}


@article{Tomasello2005,
  title={Understanding and sharing intentions: the origins of cultural cognition},
  author={Tomasello, M. and Carpenter, M. and Call, J. and Behne, T. and Moll, H.},
  journal={Behavioral and Brain Sciences},
  volume={28},
  number={5},
  pages={675-691},
  year={2005},
  month={Oct},
  publisher={Cambridge University Press},
  doi={10.1017/S0140525X05000129},
  PMID={16262930},
  discussion={691-735}
}


@book{klir1995fuzzy,
  title={Fuzzy Sets and Fuzzy Logic: Theory and Applications},
  author={Klir, George J. and Yuan, Bo},
  isbn={9780131011717},
  year={1995},
  publisher={Prentice Hall},
  address={Upper Saddle River, NJ, USA}
}

@article{chiang2024chatbot,
  title={Chatbot arena: An open platform for evaluating llms by human preference},
  author={Chiang, Wei-Lin and Zheng, Lianmin and Sheng, Ying and Angelopoulos, Anastasios Nikolas and Li, Tianle and Li, Dacheng and Zhang, Hao and Zhu, Banghua and Jordan, Michael and Gonzalez, Joseph E and others},
  journal={arXiv preprint arXiv:2403.04132},
  year={2024}
}

@article{zheng2024judging,
  title={Judging llm-as-a-judge with mt-bench and chatbot arena},
  author={Zheng, Lianmin and Chiang, Wei-Lin and Sheng, Ying and Zhuang, Siyuan and Wu, Zhanghao and Zhuang, Yonghao and Lin, Zi and Li, Zhuohan and Li, Dacheng and Xing, Eric and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@inproceedings{feifei2009ImageNet,
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Kai Li and Li Fei-Fei},
  booktitle={2009 IEEE Conference on Computer Vision and Pattern Recognition}, 
  title={ImageNet: A large-scale hierarchical image database}, 
  year={2009},
  volume={},
  number={},
  pages={248-255},
  keywords={Large-scale systems;Image databases;Explosions;Internet;Robustness;Information retrieval;Image retrieval;Multimedia databases;Ontologies;Spine},
  doi={10.1109/CVPR.2009.5206848}}


@article{Jain1968,
 author    = {Jain, N. and Orey, S.},
  title     = {On the range of random walk},
  journal   = {Israel J. Math.},
volume = {6},
pages = {373-380} ,
year = {1968}
}

@article{Alexiewicz1948,
 author    = {Andrzej Alexiewicz},
  title     = {Linear functionals on Denjoy-integrable functions},
  journal   = {Colloq. Math. },
volume = {1},
pages = {289-293},
year = {1948}
}

@article{Moser2014RandomWalk,
  title={The Range of a Simple Random Walk on Z: An Elementary Combinatorial Approach},
  author={Bernhard A. Moser},
  journal={Electron. J. Comb.},
  year={2014},
  volume={21},
  issue={4},
doi={https://doi.org/10.37236/4106}
}



@article{Silver2016AlphaGo,
  title={Mastering the game of Go with deep neural networks and tree search},
  author={Silver, David and Huang, Aja and Maddison, Chris J. and Guez, Arthur and Sifre, Laurent and van den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and Dieleman, Sander and Grewe, Dominik and Nham, John and Kalchbrenner, Nal and Sutskever, Ilya and Lillicrap, Timothy and Leach, Madeleine and Kavukcuoglu, Koray and Graepel, Thore and Hassabis, Demis},
  journal={Nature},
  volume={529},
  number={7587},
  pages={484--489},
  year={2016},
  publisher={Nature Publishing Group}
}

@article{McCullochPitts1943,
  author = {McCulloch, W.S. and Pitts, W.},
  title = {A logical calculus of the ideas immanent in nervous activity},
  journal = {Bulletin of Mathematical Biophysics},
  volume = {5},
  pages = {115--133},
  year = {1943}
}


@inproceedings{He2016residual,
  title={Deep Residual Learning for Image Recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}


@article{Rosenblatt1958perceptron,
  title={The Perceptron: A Probabilistic Model for Information Storage and Organization in the Brain},
  author={Rosenblatt, Frank},
  journal={Psychological Review},
  volume={65},
  number={6},
  pages={386--408},
  year={1958},
  publisher={American Psychological Association}
}

@article{Turing1950,
  title={Computing Machinery and Intelligence},
  author={Turing, Alan M},
  journal={Mind},
  volume={59},
  number={236},
  pages={433--460},
  year={1950},
  publisher={Oxford University Press}
}


@article{keup2022origami,
  title={Origami in N dimensions: How feed-forward networks manufacture linear separability},
  author={Keup, Christian and Helias, Moritz},
  journal={arXiv preprint arXiv:2203.11355},
  year={2022}
}

@article{naitzat2020topology,
  title={Topology of Deep Neural Networks.},
  author={Naitzat, Gregory and Zhitnikov, Andrey and Lim, Lek-Heng},
  journal={JMLR},
  volume={21},
  number={184},
  pages={1--40},
  year={2020}
}

@book{edelsbrunner2010computational,
  title={Computational Topology: An Introduction},
  author={Edelsbrunner, Herbert and Harer, John},
  year={2010},
  publisher={American Mathematical Society}
}

@inproceedings{yuanzhi17convergence2layersnnrelu,
 author = {Li, Yuanzhi and Yuan, Yang},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Convergence Analysis of Two-layer Neural Networks with ReLU Activation},
 url = {https://proceedings.neurips.cc/paper/2017/file/a96b65a721e561e1e3de768ac819ffbb-Paper.pdf},
 volume = {30},
 year = {2017}
}

@article{yu2021unifiedframeworkformultidistributiondre,
  title={A Unified Framework for Multi-distribution Density Ratio Estimation},
  author={Yu, Lantao and Jin, Yujia and Ermon, Stefano},
  journal={arXiv preprint arXiv:2112.03440},
  year={2021}
}


@article{hutter2006sequential,
  title={Sequential predictions based on algorithmic complexity},
  author={Hutter, Marcus},
  journal={Journal of Computer and System Sciences},
  volume={72},
  number={1},
  pages={95--117},
  year={2006},
  publisher={Elsevier}
}

@article{hendrycks2016gaussian,
  title={Gaussian error linear units (gelus)},
  author={Hendrycks, Dan and Gimpel, Kevin},
  journal={arXiv preprint arXiv:1606.08415},
  year={2016}
}

@inproceedings{arora2018stronger,
  title={Stronger generalization bounds for deep nets via a compression approach},
  author={Arora, Sanjeev and Ge, Rong and Neyshabur, Behnam and Zhang, Yi},
  booktitle={International Conference on Machine Learning},
  pages={254--263},
  year={2018},
  organization={PMLR}
}

@article{terven2023loss,
  title={Loss functions and metrics in deep learning. A review},
  author={Terven, Juan and Cordova-Esparza, Diana M and Ramirez-Pedraza, Alfonzo and Chavez-Urbiola, Edgar A},
  journal={arXiv preprint arXiv:2307.02694},
  year={2023}
}

@article{rumelhart1986learning,
  title={Learning representations by back-propagating errors},
  author={Rumelhart, David E and Hinton, Geoffrey E and Williams, Ronald J},
  journal={Nature},
  volume={323},
  number={6088},
  pages={533--536},
  year={1986},
  publisher={Nature Publishing Group}
}


@article{lecun1989backpropagation,
  title={Backpropagation applied to handwritten zip code recognition},
  author={LeCun, Yann and Boser, Bernhard E and Denker, John S and Henderson, Donnie and Howard, Richard E and Hubbard, Wayne and Jackel, Lawrence D},
  journal={Neural computation},
  volume={1},
  number={4},
  pages={541--551},
  year={1989},
  publisher={MIT Press}
}

@article{nagarajan2019uniform,
  title={Uniform convergence may be unable to explain generalization in deep learning},
  author={Nagarajan, Vaishnavh and Kolter, J Zico},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@article{schmidhuber1997discovering,
  title={Discovering Neural Nets with Low Kolmogorov Complexity and High Generalization Capability},
  author={Schmidhuber, J{\"u}rgen},
  journal={Neural Networks},
  volume={10},
  number={5},
  pages={857--873},
  year={1997},
  publisher={Elsevier}
}

@article{masden2022algorithmic,
      title={Algorithmic Determination of the Combinatorial Structure of the Linear Regions of ReLU Neural Networks},
      author={Marissa Masden},
      journal={arXiv preprint arXiv:2207.07696},
      year={2022}
}


@article{goldblum2023no,
  title={The No Free Lunch Theorem, Kolmogorov Complexity, and the Role of Inductive Biases in Machine Learning},
  author={Goldblum, Micah and Finzi, Marc and Rowan, Keefer and Wilson, Andrew Gordon},
  journal={arXiv preprint arXiv:2304.05366},
  year={2023}
}

@article{ZMESKAL2013135,
title = {Entropy of fractal systems},
journal = {Computers \& Mathematics with Applications},
volume = {66},
number = {2},
pages = {135-146},
year = {2013},
issn = {0898-1221},
author = {Oldrich Zmeskal and Petr Dzik and Michal Vesely},
keywords = {Fractal physics, Fractal geometry, Fractal dimension, Fractal measure, Kolmogorov entropy, Rényi entropy, Shannon entropy, Thermodynamic entropy},
abstract = {The Kolmogorov entropy is an important measure which describes the degree of chaoticity of systems. It gives the average rate of information loss about a position of the phase point on the attractor. Numerically, the Kolmogorov entropy can be estimated as the Rényi entropy. A special case of Rényi entropy is the information theory of Shannon entropy. The product of Shannon entropy and Boltzmann constant is the thermodynamic entropy. Fractal structures are characterized by their fractal dimension. There exists an infinite family of fractal dimensions. A generalized fractal dimension can be defined in an E-dimensional space. The Rényi entropy and generalized fractal dimension are connected by a straight relation.}
}

@InProceedings{Mandelbrot1995,
    author = {Mandelbrot, Benoit B.},
    title = {Measures of Fractal Lacunarity: Minkowski Content and Alternatives},
    booktitle = {Fractal Geometry and Stochastics},
    year = {1995},
    publisher = {Birkh{\"a}user Basel},
    address = {Basel},
    pages = {15--42},
    series = {Progress in Probability},
    abstract = {Linear Cantor dusts with identical values of fractal dimension and Hausdorff measure may differ violently from one another. Some look clearly fractal, while others look to the unassisted eye as nearly filled intervals (they are said to be of low lacunarity), and others nearly seem to reduce to the end points of an interval (they are said to be of high lacunarity). It is shown that Minkowski content can be used as one of several quantitative measures of a broad concept called fractal lacunarity. This content is evaluated for an important example. Even more convenient and intuitive than Minkowski content is a related new quantity V, called ``crossover parameter.'' The paper also sketches the impact of low lacunarity fractals on modeling of nature. The last section sketches an alternative measure of lacunarity, based on antipodal correlation.}
}



@article{chen2020group,
  title={A group-theoretic framework for data augmentation},
  author={Chen, Shuxiao and Dobriban, Edgar and Lee, Jane H},
  journal={Journal of Machine Learning Research},
  volume={21},
  number={245},
  pages={1--71},
  year={2020}
}

@inproceedings{hein2019relu,
  title={Why relu networks yield high-confidence predictions far away from the training data and how to mitigate the problem},
  author={Hein, Matthias and Andriushchenko, Maksym and Bitterwolf, Julian},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={41--50},
  year={2019}
}

@inproceedings{HaninR19reluhavefewactivation,
  author    = {Boris Hanin and
               David Rolnick},
  title     = {Deep ReLU Networks Have Surprisingly Few Activation Patterns},
  booktitle = {NeurIPS},
  year      = {2019}
}

@article{grosse2017statistical,
  title={On the (statistical) detection of adversarial examples},
  author={Grosse, Kathrin and Manoharan, Praveen and Papernot, Nicolas and Backes, Michael and McDaniel, Patrick},
  journal={arXiv preprint arXiv:1702.06280},
  year={2017}
}

@article{kanamori2011fdivergence_twosampletest,
  title={$f$-Divergence Estimation and Two-Sample Homogeneity Test Under Semiparametric Density-Ratio Models},
  author={Kanamori, Takafumi and Suzuki, Taiji and Sugiyama, Masashi},
  journal={IEEE transactions on information theory},
  volume={58},
  number={2},
  pages={708--720},
  year={2011},
  publisher={IEEE}
}

@article{tolstikhin2021mlp,
  title={Mlp-mixer: An all-mlp architecture for vision},
  author={Tolstikhin, Ilya and Houlsby, Neil and Kolesnikov, Alexander and Beyer, Lucas and Zhai, Xiaohua and Unterthiner, Thomas and Yung, Jessica and Keysers, Daniel and Uszkoreit, Jakob and Lucic, Mario and others},
  journal={arXiv preprint arXiv:2105.01601},
  year={2021}
}


@inproceedings{dosovitskiy2020image1616,
title	= {An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
author	= {Alexander Kolesnikov and Alexey Dosovitskiy and Dirk Weissenborn and Georg Heigold and Jakob Uszkoreit and Lucas Beyer and Matthias Minderer and Mostafa Dehghani and Neil Houlsby and Sylvain Gelly and Thomas Unterthiner and Xiaohua Zhai},
booktitle = {ICML},
year	= {2021}
}

@incollection{oh2019towards,
  title={Towards reverse-engineering black-box neural networks},
  author={Oh, Seong Joon and Schiele, Bernt and Fritz, Mario},
  booktitle={Explainable AI: Interpreting, Explaining and Visualizing Deep Learning},
  pages={121--144},
  year={2019},
  publisher={Springer}
} 

@inproceedings{milli2019model,
  title={Model reconstruction from model explanations},
  author={Milli, Smitha and Schmidt, Ludwig and Dragan, Anca D and Hardt, Moritz},
  booktitle={Proceedings of the Conference on Fairness, Accountability, and Transparency},
  pages={1--9},
  year={2019}
}

@InProceedings{kompa21secondopinionneeded,
    title = { Second opinion needed: communicating uncertainty in medical machine learning},
    author = {Kompa, Benjamin and Snoek, Jasper},
    year = {2021},
    month = {5 Jan},
    doi = {10.1038/s41746-020-00367-3},
    series = {Digital Medicine}
}

@article{Noether1916,
author = {Noether, E.},
journal = {Mathematische Annalen},
pages = {89-92},
title = {Der Endlichkeitssatz der Invarianten endlicher Gruppen.},
url = {http://eudml.org/doc/158716},
volume = {77},
year = {1916},
}

@InProceedings{rolnick20reverse_ReLU,
  title = 	 {Reverse-engineering deep {R}e{LU} networks},
  author =       {Rolnick, David and Kording, Konrad},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {8178--8187},
  year = 	 {2020},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/rolnick20a/rolnick20a.pdf},
  url = 	 {https://proceedings.mlr.press/v119/rolnick20a.html},
  abstract = 	 {The output of a neural network depends on its architecture and weights in a highly nonlinear way, and it is often assumed that a network’s parameters cannot be recovered from its output. Here, we prove that, in fact, it is frequently possible to reconstruct the architecture, weights, and biases of a deep ReLU network by observing only its output. We leverage the fact that every ReLU network defines a piecewise linear function, where the boundaries between linear regions correspond to inputs for which some neuron in the network switches between inactive and active ReLU states. By dissecting the set of region boundaries into components associated with particular neurons, we show both theoretically and empirically that it is possible to recover the weights of neurons and their arrangement within the network, up to isomorphism.}
}

@inproceedings{rastegari2016xnor,
  title={Xnor-net: Imagenet classification using binary convolutional neural networks},
  author={Rastegari, Mohammad and Ordonez, Vicente and Redmon, Joseph and Farhadi, Ali},
  booktitle={Computer Vision--ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11--14, 2016, Proceedings, Part IV},
  pages={525--542},
  year={2016},
  organization={Springer}
}

@inproceedings{Courbariaux2015binary_connect,
author = {Courbariaux, Matthieu and Bengio, Yoshua and David, Jean-Pierre},
title = {BinaryConnect: Training Deep Neural Networks with Binary Weights during Propagations},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Deep Neural Networks (DNN) have achieved state-of-the-art results in a wide range of tasks, with the best results obtained with large training sets and large models. In the past, GPUs enabled these breakthroughs because of their greater computational speed. In the future, faster computation at both training and test time is likely to be crucial for further progress and for consumer applications on low-power devices. As a result, there is much interest in research and development of dedicated hardware for Deep Learning (DL). Binary weights, i.e., weights which are constrained to only two possible values (e.g. -1 or 1), would bring great benefits to specialized DL hardware by replacing many multiply-accumulate operations by simple accumulations, as multipliers are the most space and power-hungry components of the digital implementation of neural networks. We introduce BinaryConnect, a method which consists in training a DNN with binary weights during the forward and backward propagations, while retaining precision of the stored weights in which gradients are accumulated. Like other dropout schemes, we show that BinaryConnect acts as regularizer and we obtain near state-of-the-art results with BinaryConnect on the permutation-invariant MNIST, CIFAR-10 and SVHN.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3123–3131},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@ARTICLE{fukushima1969first_work_on_relu,
  author={Fukushima, Kunihiko},
  journal={IEEE Transactions on Systems Science and Cybernetics}, 
  title={Visual Feature Extraction by a Multilayered Network of Analog Threshold Elements}, 
  year={1969},
  volume={5},
  number={4},
  pages={322-333},
  doi={10.1109/TSSC.1969.300225}}

@inproceedings{jarrett2009best,
  title={What is the best multi-stage architecture for object recognition?},
  author={Jarrett, Kevin and Kavukcuoglu, Koray and Ranzato, Marc'Aurelio and LeCun, Yann},
  booktitle={2009 IEEE 12th international conference on computer vision},
  pages={2146--2153},
  year={2009},
  organization={IEEE}
}

@ARTICLE{koutroumbas2003sigmoid,
title = {On the partitioning capabilities of feedforward neural networks with sigmoid nodes},
author = {Koutroumbas K.},
pages = {2457–2481},
year = {2003},
doi = {10.1162/089976603322362437},
journal = {Neural computation},
}

@ARTICLE{makhoul1991partitioning_capabilities,
  author={Makhoul, J. and El-Jaroudi, A. and Schwartz, R.},
  journal={IEEE Transactions on Signal Processing}, 
  title={Partitioning capabilities of two-layer neural networks}, 
  year={1991},
  volume={39},
  number={6},
  pages={1435-1440},
  doi={10.1109/78.136554}}

@INPROCEEDINGS{chechern1994studyonpartitioningcapabilities,
  author={Che-Chern Lin and El-Jaroudi, A.},
  booktitle={Proceedings of 1994 IEEE International Conference on Neural Networks (ICNN'94)}, 
  title={A study on the partitioning capabilities of two-layer neural networks}, 
  year={1994},
  volume={1},
  number={},
  pages={360-365 vol.1},
  doi={10.1109/ICNN.1994.374190}}



@article{Barredo20XAI,
title = {Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI},
journal = {Information Fusion},
volume = {58},
pages = {82-115},
year = {2020},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2019.12.012},
url = {https://www.sciencedirect.com/science/article/pii/S1566253519308103},
author = {Alejandro {Barredo Arrieta} and Natalia Díaz-Rodríguez and Javier {Del Ser} and Adrien Bennetot and Siham Tabik and Alberto Barbado and Salvador Garcia and Sergio Gil-Lopez and Daniel Molina and Richard Benjamins and Raja Chatila and Francisco Herrera},
keywords = {Explainable Artificial Intelligence, Machine Learning, Deep Learning, Data Fusion, Interpretability, Comprehensibility, Transparency, Privacy, Fairness, Accountability, Responsible Artificial Intelligence},
abstract = {In the last few years, Artificial Intelligence (AI) has achieved a notable momentum that, if harnessed appropriately, may deliver the best of expectations over many application sectors across the field. For this to occur shortly in Machine Learning, the entire community stands in front of the barrier of explainability, an inherent problem of the latest techniques brought by sub-symbolism (e.g. ensembles or Deep Neural Networks) that were not present in the last hype of AI (namely, expert systems and rule based models). Paradigms underlying this problem fall within the so-called eXplainable AI (XAI) field, which is widely acknowledged as a crucial feature for the practical deployment of AI models. The overview presented in this article examines the existing literature and contributions already done in the field of XAI, including a prospect toward what is yet to be reached. For this purpose we summarize previous efforts made to define explainability in Machine Learning, establishing a novel definition of explainable Machine Learning that covers such prior conceptual propositions with a major focus on the audience for which the explainability is sought. Departing from this definition, we propose and discuss about a taxonomy of recent contributions related to the explainability of different Machine Learning models, including those aimed at explaining Deep Learning methods for which a second dedicated taxonomy is built and examined in detail. This critical literature analysis serves as the motivating background for a series of challenges faced by XAI, such as the interesting crossroads of data fusion and explainability. Our prospects lead toward the concept of Responsible Artificial Intelligence, namely, a methodology for the large-scale implementation of AI methods in real organizations with fairness, model explainability and accountability at its core. Our ultimate goal is to provide newcomers to the field of XAI with a thorough taxonomy that can serve as reference material in order to stimulate future research advances, but also to encourage experts and professionals from other disciplines to embrace the benefits of AI in their activity sectors, without any prior bias for its lack of interpretability.}
}

@misc{rolnick2018deepLrobust,
      title={Deep Learning is Robust to Massive Label Noise}, 
      author={David Rolnick and Andreas Veit and Serge Belongie and Nir Shavit},
      year={2018},
      eprint={1705.10694},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{fan2021interpretability,
  title={On interpretability of artificial neural networks: A survey},
  author={Fan, Feng-Lei and Xiong, Jinjun and Li, Mengzhou and Wang, Ge},
  journal={IEEE Transactions on Radiation and Plasma Medical Sciences},
  year={2021},
  publisher={IEEE}
}

@article{veale2021demystifying,
  title={Demystifying the Draft EU Artificial Intelligence Act—Analysing the good, the bad, and the unclear elements of the proposed approach},
  author={Veale, Michael and Borgesius, Frederik Zuiderveen},
  journal={Computer Law Review International},
  volume={22},
  number={4},
  pages={97--112},
  year={2021},
  publisher={Verlag Dr. Otto Schmidt}
}

@inproceedings{weyand2016planet,
  title={Planet-photo geolocation with convolutional neural networks},
  author={Weyand, Tobias and Kostrikov, Ilya and Philbin, James},
  booktitle={European Conference on Computer Vision},
  pages={37--55},
  year={2016},
  organization={Springer}
}

@article{Howard2017MobileNetsEC,
  title={MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications},
  author={Andrew G. Howard and Menglong Zhu and Bo Chen and Dmitry Kalenichenko and Weijun Wang and Tobias Weyand and Marco Andreetto and Hartwig Adam},
  journal={ArXiv},
  year={2017},
  volume={abs/1704.04861}
}

@inproceedings{zoph2018learning,
  title={Learning transferable architectures for scalable image recognition},
  author={Zoph, Barret and Vasudevan, Vijay and Shlens, Jonathon and Le, Quoc V},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={8697--8710},
  year={2018}
}

@article{nicolae2018plu,
  title={PLU: The piecewise linear unit activation function},
  author={Nicolae, Andrei},
  journal={arXiv preprint arXiv:1809.09534},
  year={2018}
}

@article{rudin2019stop,
  title={Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead},
  author={Rudin, Cynthia},
  journal={Nature Machine Intelligence},
  volume={1},
  number={5},
  pages={206--215},
  year={2019},
  publisher={Nature Publishing Group}
}

@INPROCEEDINGS{Maas13rectifiernonlinearities,
    author = {Andrew L. Maas and Awni Y. Hannun and Andrew Y. Ng},
    title = {Rectifier nonlinearities improve neural network acoustic models},
    booktitle = {ICML Workshop on Deep Learning for Audio, Speech and Language Processing},
    year = {2013}
}

@article{team2023gemini,
  title={Gemini: A Family of Highly Capable Multimodal Models},
  author={Team, Gemini and Anil, Rohan and Borgeaud, Sebastian and Wu, Yonghui and Alayrac, Jean-Baptiste and Yu, Jiahui and Soricut, Radu and Schalkwyk, Johan and Dai, Andrew M and Hauth, Anja and others},
  journal={arXiv preprint arXiv:2312.11805},
  year={2023}
}


@article{kirkpatrick2017overcoming,
  title={Overcoming catastrophic forgetting in neural networks},
  author={Kirkpatrick, James and Pascanu, Razvan and Rabinowitz, Neil and Veness, Joel and Desjardins, Guillaume and Rusu, Andrei A and Milan, Kieran and Quan, John and Ramalho, Tiago and Grabska-Barwinska, Agnieszka and Hassabis, Demis and Clopath, Claudia and Kumaran, Dharshan and Hadsell, Raia},
  journal={Proceedings of the National Academy of Sciences},
  volume={114},
  number={13},
  pages={3521--3526},
  year={2017},
  publisher={National Academy of Sciences}
}

@inproceedings{devlin2019bert,
  title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  booktitle={Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  pages={4171--4186},
  year={2019}
}


@article{battaglia2018relational,
  title={Relational inductive biases, deep learning, and graph networks},
  author={Battaglia, Peter W and Hamrick, Jessica B and Bapst, Victor and Sanchez-Gonzalez, Alvaro and Zambaldi, Vinicius and Malinowski, Mateusz and Tacchetti, Andrea and Raposo, David and Santoro, Adam and Faulkner, Ryan and others},
  journal={arXiv preprint arXiv:1806.01261},
  year={2018}
}

@article{pascanu2012understandingExploding,
  title={Understanding the exploding gradient problem},
  author={Pascanu, Razvan and Mikolov, Tomas and Bengio, Yoshua},
  journal={CoRR},
  volume={abs/1211.5063},
  number={2.417},
  pages={1},
  year={2012}
}


@article{weyl1916disrepancy,
author="Hermann Weyl",
title= "Uber die Gleichverteilung von Zahlen",
journal="Eins.Mathematische Annalen",
year="1916",
pages="331--352",
volume="77"}




@article{demaine98folding,
title = "Folding and cutting paper",
abstract = "We present an algorithm to find a flat folding of a piece of paper, so that one complete straight cut on the folding creates any desired plane graph of cuts. The folds are based on the straight skeleton, which lines up the desired edges by folding along various bisectors; and a collection of perpendiculars that make the crease pattern foldable. We prove that the crease pattern is flat foldable by demonstrating a family of folded states with the desired properties.",
author = "Demaine, {Erik D.} and Demaine, {Martin L.} and Anna Lubiw",
year = "2000",
month = jan,
day = "1",
doi = "10.1007/978-3-540-46515-7_9",
language = "English",
volume = "1763",
pages = "104--118",
journal = "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",
issn = "0302-9743",
publisher = "Springer Verlag",
}


@article{Rieck2019NeuralPA,
  title={Neural Persistence: A Complexity Measure for Deep Neural Networks Using Algebraic Topology},
  author={Bastian Alexander Rieck and Matteo Togninalli and Christian Bock and Michael Moor and Max Horn and Thomas Gumbsch and Karsten M. Borgwardt},
  journal={ICLR},
  year={2019}
}

@article{elsken2019neural,
  title={Neural Architecture Search: A Survey},
  author={Elsken, Thomas and Metzen, Jan Hendrik and Hutter, Frank},
  journal={Journal of Machine Learning Research},
  volume={20},
  number={55},
  pages={1--21},
  year={2019}
}

@inproceedings{miller2003new,
  title={A new class of entropy estimators for multi-dimensional densities},
  author={Miller, E. G.},
  booktitle={Proceedings of the 2003 IEEE International Conference on Acoustics, Speech, and Signal Processing},
  volume={3},
  pages={III--297},
  year={2003},
  organization={IEEE}
}


@book{li2008introduction,
  title={An introduction to Kolmogorov complexity and its applications},
  author={Li, Ming and Vit{\'a}nyi, Paul},
  year={2008},
  publisher={Springer Science \& Business Media}
}

@article{lecun2015deep,
  title={Deep learning},
  author={LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
  journal={Nature},
  volume={521},
  number={7553},
  pages={436--444},
  year={2015},
  publisher={Nature Publishing Group}
}

@article{jacot2018neural,
  title={Neural tangent kernel: Convergence and generalization in neural networks},
  author={Jacot, Arthur and Gabriel, Franck and Hongler, Cl{\'e}ment},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@article{schmidhuber2015deep,
  title={Deep learning in neural networks: An overview},
  author={Schmidhuber, J{\"u}rgen},
  journal={Neural networks},
  volume={61},
  pages={85--117},
  year={2015},
  publisher={Elsevier}
}

@inproceedings{petri2020on,
title={On The Topological Expressive Power of Neural Networks},
author={Giovanni Petri and Ant{\'o}nio Leit{\~a}o},
booktitle={NeurIPS 2020 Workshop on Topological Data Analysis and Beyond},
year={2020}
}

@article{gamba2023lipschitz,
  title={On the lipschitz constant of deep networks and double descent},
  author={Gamba, Matteo and Azizpour, Hossein and Bj{\"o}rkman, M{\aa}rten},
  journal={arXiv preprint arXiv:2301.12309},
  year={2023}
}

@InProceedings{Jamil2023hammingsimilarity,
    author    = {Jamil, Huma and Liu, Yajing and Caglar, Turgay and Cole, Christina and Blanchard, Nathaniel and Peterson, Christopher and Kirby, Michael},
    title     = {Hamming Similarity and Graph Laplacians for Class Partitioning and Adversarial Image Detection},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops},
    month     = {6},
    year      = {2023},
    pages     = {590-599}
}

@INPROCEEDINGS{boufounos2008onebitcompressivesensing,
  author={Boufounos, Petros T. and Baraniuk, Richard G.},
  booktitle={2008 42nd Annual Conference on Information Sciences and Systems}, 
  title={1-Bit compressive sensing}, 
  year={2008},
  volume={},
  number={},
  pages={16-21},
  keywords={Quantization;Dictionaries;Sampling methods;Image reconstruction;Reconstruction algorithms;Image coding;Hardware;Electric variables measurement;Testing;Robustness},
  doi={10.1109/CISS.2008.4558487}}


@ARTICLE{knudson2016onebitcompressivesensing,
  author={Knudson, Karin and Saab, Rayan and Ward, Rachel},
  journal={IEEE Transactions on Information Theory}, 
  title={One-Bit Compressive Sensing With Norm Estimation}, 
  year={2016},
  volume={62},
  number={5},
  pages={2748-2758},
  keywords={Compressed sensing;Measurement uncertainty;Quantization (signal);Standards;Estimation;Random variables;Programming;compressed sensing;norm estimation;quantization;Compressed sensing;norm estimation;quantization},
  doi={10.1109/TIT.2016.2527637}}


@article{Baccelli2019stochasticgeometry,
  title   = {The Stochastic Geometry of Unconstrained One-Bit Data Compression},
  author  = {François Baccelli and Eliza O'Reilly},
  journal = {Electronic Journal of Probability},
  volume  = {24},
  number  = {138},
  pages   = {1--27},
  year    = {2019},
  doi     = {10.1214/19-EJP389},
}


@InProceedings{gamba22arealllinearregions,
  title = 	 { Are All Linear Regions Created Equal? },
  author =       {Gamba, Matteo and Chmielewski-Anders, Adrian and Sullivan, Josephine and Azizpour, Hossein and Bjorkman, Marten},
  booktitle = 	 {Proceedings of The 25th International Conference on Artificial Intelligence and Statistics},
  pages = 	 {6573--6590},
  year = 	 {2022},
  volume = 	 {151},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {3},
  publisher =    {PMLR},
  abstract = 	 { The number of linear regions has been studied as a proxy of complexity for ReLU networks. However, the empirical success of network compression techniques like pruning and knowledge distillation, suggest that in the overparameterized setting, linear regions density might fail to capture the effective nonlinearity. In this work, we propose an efficient algorithm for discovering linear regions and use it to investigate the effectiveness of density in capturing the nonlinearity of trained VGGs and ResNets on CIFAR-10 and CIFAR-100. We contrast the results with a more principled nonlinearity measure based on function variation, highlighting the shortcomings of linear regions density. Furthermore, interestingly, our measure of nonlinearity clearly correlates with model-wise deep double descent, connecting reduced test error with reduced nonlinearity, and increased local similarity of linear regions. }
}

@incollection{neal2011mcmc,
  title={MCMC using Hamiltonian dynamics},
  author={Neal, Radford M.},
  booktitle={Handbook of Markov Chain Monte Carlo},
  pages={113--162},
  year={2011},
  publisher={CRC Press}
}

@article{metropolis1953equation,
  title={Equation of state calculations by fast computing machines},
  author={Metropolis, Nicholas and Rosenbluth, Arianna W. and Rosenbluth, Marshall N. and Teller, Augusta H. and Teller, Edward},
  journal={The Journal of Chemical Physics},
  volume={21},
  number={6},
  pages={1087--1092},
  year={1953},
  publisher={American Institute of Physics}
}

@inproceedings{welling2011bayesian,
  title={Bayesian learning via stochastic gradient Langevin dynamics},
  author={Welling, Max and Teh, Yee W.},
  booktitle={ICML},
  year={2011}
}

@article{sobol1967distribution,
  title={On the distribution of points in a cube and the approximate evaluation of integrals},
  author={Sobol, Ilya M.},
  journal={Zhurnal Vychislitel'noi Matematiki i Matematicheskoi Fiziki},
  year={1967}
}



@article{gamba2020hyperplane,
  title={Hyperplane arrangements of trained convnets are biased},
  author={Gamba, Matteo and Carlsson, Stefan and Azizpour, Hossein and Bj{\"o}rkman, M{\aa}rten},
  journal={arXiv preprint arXiv:2003.07797},
  year={2020}
}

@article{grigsby2022local,
  title={Local and global topological complexity measures OF ReLU neural network functions},
  author={Grigsby, J Elisenda and Lindsey, Kathryn and Masden, Marissa},
  journal={arXiv preprint arXiv:2204.06062},
  year={2022}
}


@article{grigsby2022transversality,
  title={On transversality of bent hyperplane arrangements and the topological expressiveness of ReLU neural networks},
  author={Grigsby, J Elisenda and Lindsey, Kathryn},
  journal={SIAM Journal on Applied Algebra and Geometry},
  volume={6},
  number={2},
  pages={216--242},
  year={2022},
  publisher={SIAM}
}


@article{hajij2020topological,
  title={A topological framework for deep learning},
  author={Hajij, Mustafa and Istvan, Kyle},
  journal={arXiv preprint arXiv:2008.13697},
  year={2020}
}

@inproceedings{hanin2021deep,
  title={Deep ReLU Networks Preserve Expected Length},
  author={Hanin, Boris and Jeong, Ryan and Rolnick, David},
  booktitle={International Conference on Learning Representations},
  year={2021}
}

@INPROCEEDINGS{ilan2021trajectory,
  author={Price, Ilan and Tanner, Jared},
  booktitle={2021 20th IEEE International Conference on Machine Learning and Applications (ICMLA)}, 
  title={Trajectory growth lower bounds for random sparse deep ReLU networks}, 
  year={2021},
  volume={},
  number={},
  pages={1004-1009},
  keywords={Deep learning;Conferences;Neural networks;Trajectory;Sparse matrices;deep learning;random curves;random sparse matrices;expected arc length;expressivity},
  doi={10.1109/ICMLA52953.2021.00165}}

@article{bendavid10theoryoflearningfromdifferentdomains,
author = {Ben-David, Shai and Blitzer, John and Crammer, Koby and Kulesza, Alex and Pereira, Fernando and Vaughan, Jennifer Wortman},
title = {A Theory of Learning from Different Domains},
year = {2010},
issue_date = {May       2010},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {79},
number = {1–2},
issn = {0885-6125},
url = {https://doi.org/10.1007/s10994-009-5152-4},
doi = {10.1007/s10994-009-5152-4},
abstract = {Discriminative learning methods for classification perform well when training and test data are drawn from the same distribution. Often, however, we have plentiful labeled training data from a source domain but wish to learn a classifier which performs well on a target domain with a different distribution and little or no labeled training data. In this work we investigate two questions. First, under what conditions can a classifier trained from source data be expected to perform well on target data? Second, given a small amount of labeled target data, how should we combine it during training with the large amount of labeled source data to achieve the lowest target error at test time?We address the first question by bounding a classifier's target error in terms of its source error and the divergence between the two domains. We give a classifier-induced divergence measure that can be estimated from finite, unlabeled samples from the domains. Under the assumption that there exists some hypothesis that performs well in both domains, we show that this quantity together with the empirical source error characterize the target error of a source-trained classifier.We answer the second question by bounding the target error of a model which minimizes a convex combination of the empirical source and target errors. Previous theoretical work has considered minimizing just the source error, just the target error, or weighting instances from the two domains equally. We show how to choose the optimal combination of source and target error as a function of the divergence, the sample sizes of both domains, and the complexity of the hypothesis class. The resulting bound generalizes the previously studied cases and is always at least as tight as a bound which considers minimizing only the target error or an equal weighting of source and target errors.},
journal = {Mach. Learn.},
month = may,
pages = {151–175},
numpages = {25},
keywords = {Domain adaptation, Transfer learning, Learning theory, Sample-selection bias}
}






@ARTICLE{Hinton12dnnforacousticmodeling,
  author={Hinton, Geoffrey and Deng, Li and Yu, Dong and Dahl, George E. and Mohamed, Abdel-rahman and Jaitly, Navdeep and Senior, Andrew and Vanhoucke, Vincent and Nguyen, Patrick and Sainath, Tara N. and Kingsbury, Brian},
  journal={IEEE Signal Processing Magazine}, 
  title={Deep Neural Networks for Acoustic Modeling in Speech Recognition: The Shared Views of Four Research Groups}, 
  year={2012},
  volume={29},
  number={6},
  pages={82-97},
  doi={10.1109/MSP.2012.2205597}}

@article{yousefzadeh2019investigating,
  title={Investigating decision boundaries of trained neural networks},
  author={Yousefzadeh, Roozbeh and O'Leary, Dianne P},
  journal={arXiv preprint arXiv:1908.02802},
  year={2019}
}

@article{Jannai2023HumanOrNot,
  title={Human or Not? A Gamified Approach to the Turing Test},
  author={Jannai, Daniel and Meron, Amos and Lenz, Barak and Levine, Yoav and Shoham, Yoav},
  journal={arxiv},
  year={2023},
}
%-----
%adv attacks llms
@article{perez2022red,
  title={Red Teaming Language Models with Language Models},
  author={Perez, E. and others},
  journal={arXiv preprint arXiv:2202.03286},
  year={2022}
}

@article{ganguli2022red,
  title={Red Teaming Language Models to Reduce Harms: Methods, Scaling Behaviors, and Lessons Learned},
  author={Ganguli, D. and others},
  journal={arXiv preprint arXiv:2209.07858},
  year={2022}
}

@inproceedings{aghajanyan2023scaling,
  title={Scaling laws for generative mixed-modal language models},
  author={Aghajanyan, Armen and Yu, Lili and Conneau, Alexis and Hsu, Wei-Ning and Hambardzumyan, Karen and Zhang, Susan and Roller, Stephen and Goyal, Naman and Levy, Omer and Zettlemoyer, Luke},
  booktitle={ICML},
  year={2023}
}

@article{kaplan2020scaling,
      title={Scaling Laws for Neural Language Models},
      author={Jared Kaplan and Sam McCandlish and Tom Henighan and Tom B. Brown and Benjamin Chess and Rewon Child and Scott Gray and Alec Radford and Jeffrey Wu and Dario Amodei},
      year={2020},
      journal={arXiv:2001.08361 [cs.LG]},
      eprint={2001.08361},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}



@article{mehrabi2023flirt,
  title={FLIRT: Feedback Loop In-context Red Teaming},
  author={Mehrabi, N. and others},
  journal={arXiv preprint arXiv:2308.04265},
  year={2023}
}

@article{casper2023explore,
  title={Explore, Establish, Exploit: Red Teaming Language Models from Scratch},
  author={Casper, J. and others},
  journal={arXiv preprint arXiv:2306.09442},
  year={2023}
}

@article{xie2023defending,
  title={Defending ChatGPT against Jailbreak Attack via Self-Reminder},
  author={Xie, P. and others},
  journal={Research Square},
  year={2023}
}

@article{jones2023automatically,
  title={Automatically Auditing Large Language Models via Discrete Optimization},
  author={Jones, R. and others},
  journal={arXiv preprint arXiv:2303.04381},
  year={2023}
}

@article{greshake2023compromising,
  title={Compromising Real-World LLM-Integrated Applications with Indirect Prompt Injection},
  author={Greshake, B. and others},
  journal={arXiv preprint arXiv:2302.12173},
  year={2023}
}





@article{elbrachter2021deep,
  title={Deep Neural Network Approximation Theory},
  author={Elbr{\"a}chter, D. and Perekrestenko, D. and Grohs, P. and B{\"o}lcskei, H.},
  journal={IEEE Transactions on Information Theory},
  volume={67},
  number={5},
  pages={2581--2623},
  year={2021},
  month={5},
  publisher={IEEE},
  doi={10.1109/TIT.2021.3062161}
}

@phdthesis{Li2022,
  author = {Li, Zhiyuan},
  title = {Bridging Theory and Practice in Deep Learning: Optimization and Generalization},
  school = {Princeton University},
  year = {2022},
  address = {Princeton, NJ},
  type = {Ph.D.},
  url = {http://arks.princeton.edu/ark:/88435/dsp01qb98mj69x},
  note = {Computer Science Department},
  abstract = {Deep learning has been hugely successful for several important applications in the past decade, yet mathematical understanding has lagged behind its breathtaking empirical success. ... [Abstract truncated for brevity]}
}


@phdthesis{Yun2021,
  author = {Yun, Chulhee},
  title = {Optimization for Deep Learning: Bridging the Theory-Practice Gap},
  school = {Massachusetts Institute of Technology},
  year = {2021},
  month = {9},
  address = {Massachusetts Institute of Technology. Department of Electrical Engineering and Computer Science},
  url = {https://hdl.handle.net/1721.1/139962},
  note = {Ph.D. thesis}
}


@article{Grohs2023,
  title = {Proof of the Theory-to-Practice Gap in Deep Learning via Sampling Complexity bounds for Neural Network Approximation Spaces},
  author = {Grohs, Philipp and Voigtlaender, Felix},
  journal = {Foundations of Computational Mathematics},
  year = {2023},
  date = {2023-07-12},
  volume = {},
  number = {},
  pages = {},
  abstract = {We study the computational complexity of (deterministic or randomized) algorithms based on point samples for approximating or integrating functions that can be well approximated by neural networks. Such algorithms (most prominently stochastic gradient descent and its variants) are used extensively in the field of deep learning. One of the most important problems in this field concerns the question of whether it is possible to realize theoretically provable neural network approximation rates by such algorithms. We answer this question in the negative by proving hardness results for the problems of approximation and integration on a novel class of neural network approximation spaces. In particular, our results confirm a conjectured and empirically observed theory-to-practice gap in deep learning. We complement our hardness results by showing that error bounds of a comparable order of convergence are (at least theoretically) achievable.},
  doi = {10.1007/s10208-023-09607-w},
  url = {https://doi.org/10.1007/s10208-023-09607-w},
  issn = {1615-3383},
}

@article{bach2017breaking,
  author  = {Francis Bach},
  title   = {Breaking the Curse of Dimensionality with Convex Neural Networks},
  journal = {Journal of Machine Learning Research},
  year    = {2017},
  volume  = {18},
  number  = {19},
  pages   = {1--53},
  url     = {http://jmlr.org/papers/v18/14-546.html}
}


@article{Berner2020analysis,
author = {Berner, Julius and Grohs, Philipp and Jentzen, Arnulf},
title = {Analysis of the Generalization Error: Empirical Risk Minimization over Deep Artificial Neural Networks Overcomes the Curse of Dimensionality in the Numerical Approximation of Black--Scholes Partial Differential Equations},
journal = {SIAM Journal on Mathematics of Data Science},
volume = {2},
number = {3},
pages = {631-657},
year = {2020}
}


@article{jain2023baseline,
  title={Baseline Defenses for Adversarial Attacks Against Aligned Language Models},
  author={Jain, A. and others},
  journal={arXiv preprint arXiv:2309.00614},
  year={2023}
}

@article{wei2023jailbroken,
  title={Jailbroken: How Does LLM Safety Training Fail?},
  author={Wei, J. and others},
  journal={arXiv preprint arXiv:2307.02483},
  year={2023}
}


%----
@article{khachatryan2023text2video,
  title={Text2video-zero: Text-to-image diffusion models are zero-shot video generators},
  author={Khachatryan, Levon and Movsisyan, Andranik and Tadevosyan, Vahram and Henschel, Roberto and Wang, Zhangyang and Navasardyan, Shant and Shi, Humphrey},
  journal={arXiv preprint arXiv:2303.13439},
  year={2023}
}
@inproceedings{huang2023freebloom,
  title={Free-Bloom: Zero-Shot Text-to-Video Generator with LLM Director and LDM Animator},
  author={Huang, Hanzhuo and Feng, Yufan and Shi, Cheng and Xu, Lan and Yu, Jingyi and Yang, Sibei},
  booktitle={Advances in Neural Information Processing Systems},
  year={2023}
}

@misc{Midjourney,
  author={Midjourney},
  year={2022},
  title = {Midjourney: Text-to-Image Model},
  howpublished = {\url{https://www.midjourney.com}}
}

@article{ho2020denoising,
  title={Denoising Diffusion Probabilistic Models},
  author={Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
  journal={Advances in Neural Information Processing Systems},
  year={2020},
  publisher={Curran Associates, Inc.}
}


@article{xu2015empirical,
  title={Empirical evaluation of rectified activations in convolutional network},
  author={Xu, Bing and Wang, Naiyan and Chen, Tianqi and Li, Mu},
  journal={arXiv preprint arXiv:1505.00853},
  year={2015}
}

@inproceedings{yao2023tree,
  title={{Tree of Thoughts}: Deliberate Problem Solving with Large Language Models},
  author={Shunyu Yao and Dian Yu and Jeffrey Zhao and Izhak Shafran and Thomas L. Griffiths and Yuan Cao and Karthik Narasimhan},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  year={2023},
  eprint={2305.10601},
  archivePrefix={arXiv},
  primaryClass={cs.CL}
}


@inproceedings{Rafailov2023DirectPreferenceOptimization,
  title={Direct Preference Optimization: Your Language Model is Secretly a Reward Model},
  author={Rafael Rafailov and Archit Sharma and Eric Mitchell and Stefano Ermon and Christopher D. Manning and Chelsea Finn},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  year={2023},
}


@article{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={27730--27744},
  year={2022}
}

@book{Kahneman2011ThinkingFS,
  title={Thinking, Fast and Slow},
  author={Daniel Kahneman},
  year={2011},
  publisher={Farrar, Straus and Giroux}
}


@misc{Davidoff1986Mafia,
  author = {Davidoff, Dmitry},
  title = {Mafia},
  howpublished = {Board Game},
  year = {1986},
  note = {Originally designed at the Psychology Department of Moscow State University}
}


@inproceedings{Krizhevsky2012ImageNetCompetition,
 author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
 booktitle = {NeurIPS},
 title = {ImageNet Classification with Deep Convolutional Neural Networks},
 year = {2012}
}

@ARTICLE{uspsdataset,
  author={J. J. {Hull}},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={A database for handwritten text recognition research}, 
  year={1994},
  volume={16},
  number={5},
  pages={550-554}
}


@book{moller1994lectures,
  title={Lectures on Random Voronoi Tessellations},
  author={M{\o}ller, J.},
  series={Lecture Notes in Statistics},
  volume={87},
  year={1994},
  publisher={Springer-Verlag}
}

@book{minsky1969perceptrons,
  title={Perceptrons: An Introduction to Computational Geometry},
  author={Minsky, Marvin and Papert, Seymour},
  year={1969},
  publisher={MIT Press}
}


@article{sharpnack20222,
  title={On L 2-consistency of Nearest Neighbor Matching},
  author={Sharpnack, James},
  journal={IEEE Transactions on Information Theory},
  year={2022},
  publisher={IEEE}
}

@book{okabe2000spatial,
  title={Spatial Tessellations: Concepts and Applications of Voronoi Diagrams},
  author={Okabe, A. and Boots, B. and Sugihara, K. and Chiu, S. N.},
  series={Wiley Series in Probability and Mathematical Statistics},
  year={2000},
  publisher={Wiley}
}

@article{abraham2014volume,
  title={Volume in General Metric Spaces},
  author={Abraham, I. and Bartal, Y. and Neiman, O. and Schulman, L.J.},
  journal={Discrete Comput Geom},
  volume={52},
  pages={366--389},
  year={2014},
  publisher={Springer},
  url={https://doi.org/10.1007/s00454-014-9615-4}
}


@phdthesis{hochreiter1991untersuchungen,
  title={Untersuchungen zu dynamischen neuronalen Netzen},
  author={Hochreiter, Sepp},
  year={1991},
  school={Technische Universit{\"a}t M{\"u}nchen}
}


@inproceedings{pascanu2013difficulty,
  title={On the difficulty of training recurrent neural networks},
  author={Pascanu, Razvan and Mikolov, Tomas and Bengio, Yoshua},
  booktitle={International conference on machine learning},
  pages={1310--1318},
  year={2013},
  organization={PMLR}
}

@inproceedings{glorot2010understanding,
  title={Understanding the difficulty of training deep feedforward neural networks},
  author={Glorot, Xavier and Bengio, Yoshua},
  booktitle={Proceedings of the thirteenth international conference on artificial intelligence and statistics},
  pages={249--256},
  year={2010}
}

@article{huchette2023deep,
  title={When deep learning meets polyhedral theory: A survey},
  author={Huchette, Joey and Mu{\~n}oz, Gonzalo and Serra, Thiago and Tsay, Calvin},
  journal={arXiv preprint arXiv:2305.00241},
  year={2023}
}

@article{chen2022lower,
  title={Lower and Upper Bounds for Numbers of Linear Regions of Graph Convolutional Networks},
  author={Chen, Hao and Wang, Yu Guang and Xiong, Huan},
  journal={arXiv preprint arXiv:2206.00228},
  year={2022}
}

@article{stanojevic2022exact,
  title={An Exact Mapping From ReLU Networks to Spiking Neural Networks},
  author={Stanojevic, Ana and Wo{\'z}niak, Stanis{\l}aw and Bellec, Guillaume and Cherubini, Giovanni and Pantazi, Angeliki and Gerstner, Wulfram},
  journal={arXiv preprint arXiv:2212.12522},
  year={2022}
}

@INPROCEEDINGS{stanojevic2022approximating,
  author={Stanojevic, Ana and Eleftheriou, Evangelos and Cherubini, Giovanni and Woźniak, Stanisław and Pantazi, Angeliki and Gerstner, Wulfram},
  booktitle={2022 IEEE International Conference on Image Processing (ICIP)}, 
  title={Approximating Relu Networks by Single-Spike Computation}, 
  year={2022},
  volume={},
  number={},
  pages={1901-1905},
  doi={10.1109/ICIP46576.2022.9897692}}


@inproceedings{cohan2022understanding,
      author = {Cohan, Setareh and Kim, Nam Hee and Rolnick, David and van de Panne, Michiel},
      booktitle = {Advances in Neural Information Processing Systems},
      title = {Understanding the Evolution of Linear Regions in Deep Reinforcement Learning},
      url = {https://arxiv.org/pdf/2210.13611v2.pdf},
      year = {2022}
     }

@inproceedings{clevert2015fast,
  title={Fast and accurate deep network learning by exponential linear units (ELUs)},
  author={Clevert, Djork-Arn{\'e} and Unterthiner, Thomas and Hochreiter, Sepp},
  booktitle={3rd International Conference on Learning Representations, ICLR 2016},
  year={2016}
}

@inproceedings{choi2017towards,
  title={Towards the limit of network quantization},
  author={Choi, Yoojin and El-Khamy, Mostafa and Lee, Jungwon},
  booktitle={5th International Conference on Learning Representations, ICLR 2017},
  year={2017}
}


@inproceedings{he2015delving,
  title={Delving deep into rectifiers: Surpassing human-level performance on imagenet classification},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={1026--1034},
  year={2015}
}
@inproceedings{chen2018learning,
  title={Learning to explain: An information-theoretic perspective on model interpretation},
  author={Chen, Jianbo and Song, Le and Wainwright, Martin and Jordan, Michael},
  booktitle={International conference on machine learning},
  pages={883--892},
  year={2018},
  organization={PMLR}
}

@inproceedings{ribeiro2016should,
  title={``Why should I trust you''' Explaining the predictions of any classifier},
  author={Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
  booktitle={Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining},
  pages={1135--1144},
  year={2016}
}

@inproceedings{lundberg2017unified,
 author = {Lundberg, Scott M and Lee, Su-In},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {A Unified Approach to Interpreting Model Predictions},
 volume = {30},
 year = {2017}
}




@InProceedings{coates15stl10,
  title = 	 {An Analysis of Single-Layer Networks in Unsupervised Feature Learning},
  author = 	 {Coates, Adam and Ng, Andrew and Lee, Honglak},
  booktitle = 	 {Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics},
  pages = 	 {215--223},
  year = 	 {2011},
  volume = 	 {15},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Fort Lauderdale, FL, USA},
  month = 	 {4},
}

@article{ZELLINGER2019cmd,
title = {Robust unsupervised domain adaptation for neural networks via moment alignment},
journal = {Information Sciences},
volume = {483},
pages = {174-191},
year = {2019},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2019.01.025},
url = {https://www.sciencedirect.com/science/article/pii/S0020025519300301},
author = {Werner Zellinger and Bernhard A. Moser and Thomas Grubinger and Edwin Lughofer and Thomas Natschläger and Susanne Saminger-Platz},
keywords = {Transfer learning, Domain adaptation, Neural networks, Moment distance, Integral probability metric},
abstract = {A novel approach for unsupervised domain adaptation for neural networks is proposed. It relies on metric-based regularization of the learning process. The metric-based regularization aims at domain-invariant latent feature representations by means of maximizing the similarity between domain-specific activation distributions. The proposed metric results from modifying an integral probability metric such that it becomes less translation-sensitive on a polynomial function space. The metric has an intuitive interpretation in the dual space as the sum of differences of higher order central moments of the corresponding activation distributions. Under appropriate assumptions on the input distributions, error minimization is proven for the continuous case. As demonstrated by an analysis of standard benchmark experiments for sentiment analysis, object recognition and digit recognition, the outlined approach is robust regarding parameter changes and achieves higher classification accuracies than comparable approaches.}
}



@inproceedings{ganin2015unsupervised,
  title={Unsupervised domain adaptation by backpropagation},
  author={Ganin, Yaroslav and Lempitsky, Victor},
  booktitle={International conference on machine learning},
  pages={1180--1189},
  year={2015},
  organization={PMLR}
}

@inproceedings{xiong2020numberLR_CNN,
  title={On the number of linear regions of convolutional neural networks},
  author={Xiong, Huan and Huang, Lei and Yu, Mengyang and Liu, Li and Zhu, Fan and Shao, Ling},
  booktitle={International Conference on Machine Learning},
  year={2020},
  organization={PMLR}
}

@article{jumper2021alphafold,
  title={Highly accurate protein structure prediction with AlphaFold},
  author={Jumper, J and others},
  journal={Nature},
  year={2021},
  publisher={Nature Publishing Group}
}

@article{varadi2022alphafold,
  title={AlphaFold Protein Structure Database: massively expanding the structural coverage of protein-sequence space with high-accuracy models},
  author={Varadi, M and others},
  journal={Nucleic Acids Research},
  year={2022},
  publisher={Oxford University Press}
}


@article{shen2023study,
  title={A Study on ReLU and Softmax in Transformer},
  author={Shen, Kai and Guo, Junliang and Tan, Xu and Tang, Siliang and Wang, Rui and Bian, Jiang},
  journal={arXiv preprint arXiv:2302.06461},
  year={2023}
}

@article{Sugiyama08directimportance,
  title={Direct importance estimation for covariate shift adaptation},
  author={Masashi Sugiyama and Taiji Suzuki and S. Nakajima and H. Kashima and P. V. B{\"u}nau and M. Kawanabe},
  journal={Annals of the Institute of Statistical Mathematics},
  year={2008},
  volume={60},
  pages={699-746}
}

@book{Vapnik2000thenature,
	publisher = {Springer: New York},
	author = {Vladimir Vapnik},
	editor = {},
	year = {2000},
	title = {The Nature of Statistical Learning Theory}
}

@inproceedings{tishby2000information,
  title={The information bottleneck method},
  author={Tishby, Naftali and Pereira, Fernando C and Bialek, William},
  booktitle={The 37th annual Allerton Conference on Communication, Control, and Computing},
  year={2000}
}


@article{eghbal2020data,
  title={On Data Augmentation and Adversarial Risk: An Empirical Analysis},
  author={Eghbal-zadeh, Hamid and Koutini, Khaled and Primus, Paul and Haunschmid, Verena and Lewandowski, Michal and Zellinger, Werner and Moser, Bernhard A and Widmer, Gerhard},
  journal={arXiv preprint arXiv:2007.02650},
  year={2020}
}

@article{lewandowski2024turinggame,
  title={The Turing Game},
  author={Lewandowski, Michal and Schmid, Simon and Mederitsch, Patrick and Aufreiter, Alexander and Aichinger, Gregor and Bergsmann, Severin and Nessler, Felix and Szolga, Viktor and Nessler, Bernhard},
  journal={NeurIPS W System~2 Reasoning at Scale},
  year={2024}
}

@inproceedings{nair10relu,
author = {Nair, Vinod and Hinton, Geoffrey E.},
title = {Rectified Linear Units Improve Restricted Boltzmann Machines},
year = {2010},
isbn = {9781605589077},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {Restricted Boltzmann machines were developed using binary stochastic hidden units.
These can be generalized by replacing each binary unit by an infinite number of copies
that all have the same weights but have progressively more negative biases. The learning
and inference rules for these "Stepped Sigmoid Units" are unchanged. They can be approximated
efficiently by noisy, rectified linear units. Compared with binary units, these units
learn features that are better for object recognition on the NORB dataset and face
verification on the Labeled Faces in the Wild dataset. Unlike binary units, rectified
linear units preserve information about relative intensities as information travels
through multiple layers of feature detectors.},
booktitle = {Proceedings of the 27th International Conference on International Conference on Machine Learning},
pages = {807–814},
numpages = {8},
location = {Haifa, Israel},
series = {ICML'10}
}

@inproceedings{ho1995random,
  title={Random decision forests},
  author={Ho, Tin Kam},
  booktitle={Proceedings of 3rd international conference on document analysis and recognition},
  volume={1},
  pages={278--282},
  year={1995},
  organization={IEEE}
}

@article{bengio2013representation,
  title={Representation learning: A review and new perspectives},
  author={Bengio, Yoshua and Courville, Aaron and Vincent, Pascal},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={35},
  number={8},
  pages={1798--1828},
  year={2013},
  publisher={IEEE}
}

@article{liang2023high,
  title={High-modality multimodal transformer: Quantifying modality \& interaction heterogeneity for high-modality representation learning},
  author={Liang, Paul Pu and Lyu, Yiwei and Fan, Xiang and Tsaw, Jeffrey and Liu, Yudong and Mo, Shentong and Yogatama, Dani and Morency, Louis-Philippe and Salakhutdinov, Ruslan},
  journal={Transactions on Machine Learning Research (TMLR)},
  year={2023},
}


@article{mizrahi20244m,
  title={4m: Massively multimodal masked modeling},
  author={Mizrahi, David and Bachmann, Roman and Kar, Oguzhan and Yeo, Teresa and Gao, Mingfei and Dehghan, Afshin and Zamir, Amir},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2023}
}

@inproceedings{Bachmann20244m21,
title = {4M-21: An Any-to-Any Vision Model for Tens of Tasks and Modalities},
booktitle = {NeurIPS},
author = {Roman Bachmann* and Oğuzhan Fatih Kar* and David Mizrahi* and Ali Garjani and Mingfei Gao and David Griffiths and Jiaming Hu and Afshin Dehghan and Amir Zamir},
year = {2024},
URL = {https://arxiv.org/abs/2406.09406}
}

@article{schmidhuber2022annotated,
  title={Annotated history of modern ai and deep learning},
  author={Schmidhuber, Juergen},
  journal={arXiv preprint arXiv:2212.11279},
  year={2022}
}

@article{hochreiter1997long,
  title={Long short-term memory},
  author={Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  journal={Neural computation},
  volume={9},
  number={8},
  pages={1735--1780},
  year={1997},
  publisher={MIT Press}
}

@inproceedings{mikolov13word2vec,
  author    = {Tom{\'{a}}s Mikolov and
               Kai Chen and
               Greg Corrado and
               Jeffrey Dean},
  editor    = {Yoshua Bengio and
               Yann LeCun},
  title     = {Efficient Estimation of Word Representations in Vector Space},
  booktitle = {1st International Conference on Learning Representations, {ICLR} 2013,
               Scottsdale, Arizona, USA, May 2-4, 2013, Workshop Track Proceedings},
  year      = {2013},
  url       = {http://arxiv.org/abs/1301.3781},
  timestamp = {Mon, 28 Dec 2020 11:31:01 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1301-3781.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{qi2017pointnet,
  title={Pointnet: Deep learning on point sets for 3d classification and segmentation},
  author={Qi, Charles R and Su, Hao and Mo, Kaichun and Guibas, Leonidas J},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={652--660},
  year={2017}
}

@article{Hyunha2015drestimationConvNets,
  title={Direct Density Ratio Estimation with Convolutional Neural Networks with Application in Outlier Detection},
  author={Hyunha Nam and Masashi Sugiyama},
  journal={IEICE Transactions on Information and Systems},
  volume={E98.D},
  number={5},
  pages={1073-1079},
  year={2015}
}

@inproceedings{savkin2020kliepbaseddrestimationRealImages,
  title={KLIEP-based Density Ratio Estimation for Semantically Consistent Synthetic to Real Images Adaptation in Urban Traffic Scenes},
  author={Savkin, Artem and Tombari, Federico},
  booktitle={2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  pages={5901--5908},
  year={2020},
  organization={IEEE}
}

@inproceedings{rhodes20telescoping,
 author = {Rhodes, Benjamin and Xu, Kai and Gutmann, Michael U.},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {4905--4916},
 title = {Telescoping Density-Ratio Estimation},
 volume = {33},
 year = {2020}
}



@inproceedings{chizat2020implicit,
  title={Implicit bias of gradient descent for wide two-layer neural networks trained with the logistic loss},
  author={Chizat, Lenaic and Bach, Francis},
  booktitle={Conference on Learning Theory},
  pages={1305--1338},
  year={2020},
  organization={PMLR}
}

@article{sugiyama2010dimensionality,
  title={Dimensionality reduction for density ratio estimation in high-dimensional spaces},
  author={Sugiyama, Masashi and Kawanabe, Motoaki and Chui, Pui Ling},
  journal={Neural Networks},
  volume={23},
  number={1},
  pages={44--59},
  year={2010},
  publisher={Elsevier}
}

@InProceedings{Hsu21approximationpowerof2layerReLU,
  title = 	 {On the Approximation Power of Two-Layer Networks of Random ReLUs},
  author =       {Hsu, Daniel and Sanford, Clayton H and Servedio, Rocco and Vlatakis-Gkaragkounis, Emmanouil Vasileios},
  booktitle = 	 {Proceedings of Thirty Fourth Conference on Learning Theory},
  pages = 	 {2423--2461},
  year = 	 {2021},
  editor = 	 {Belkin, Mikhail and Kpotufe, Samory},
  volume = 	 {134},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {15--19 Aug},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v134/hsu21a/hsu21a.pdf},
  url = 	 {https://proceedings.mlr.press/v134/hsu21a.html},
  abstract = 	 {This paper considers the following question: how well can depth-two ReLU networks with randomly initialized bottom-level weights represent smooth functions? We give near-matching upper- and lower-bounds for L2-approximation in terms of the Lipschitz constant, the desired accuracy, and the dimension of the problem, as well as similar results in terms of Sobolev norms. Our positive results employ tools from harmonic analysis and ridgelet representation theory, while our lower-bounds are based on (robust versions of) dimensionality arguments.}
}

@article{grohs2021proof,
  title={Proof of the theory-to-practice gap in deep learning via sampling complexity bounds for neural network approximation spaces},
  author={Grohs, Philipp and Voigtlaender, Felix},
  journal={arXiv preprint arXiv:2104.02746},
  year={2021}
}

@article{grohs2022lowerbounds,
title = "Lower bounds for artificial neural network approximations: A proof that shallow neural networks fail to overcome the curse of dimensionality",
author = "Philipp Grohs and Shokhrukh Ibragimov and Arnulf Jentzen and Sarah Koppensteiner",
year = "2022",
doi = "10.48550/arXiv.2103.04488",
language = "English",
journal = "Journal of Complexity",
issn = "0885-064X",
publisher = "Elsevier",
}

@article{ghorbani2020neural,
  title={When do neural networks outperform kernel methods?},
  author={Ghorbani, Behrooz and Mei, Song and Misiakiewicz, Theodor and Montanari, Andrea},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={14820--14830},
  year={2020}
}

@inproceedings{Burgess96equivalentkernelsfornn,
 author = {Burgess, A.},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M.C. Mozer and M. Jordan and T. Petsche},
 pages = {},
 publisher = {MIT Press},
 title = {Estimating Equivalent Kernels for Neural Networks: A Data Perturbation Approach},
 url = {https://proceedings.neurips.cc/paper_files/paper/1996/file/c70daf247944fe3add32218f914c75a6-Paper.pdf},
 volume = {9},
 year = {1996}
}


@article{cybenko1989approximation,
  TITLE = {{Approximation by superpositions of a sigmoidal function}},
  AUTHOR = {Cybenko, G.},
  URL = {https://hal.science/hal-03753170},
  JOURNAL = {{Mathematics of Control, Signals, and Systems}},
  PUBLISHER = {{Springer Verlag}},
  VOLUME = {2},
  NUMBER = {4},
  PAGES = {303-314},
  YEAR = {1989},
  MONTH = Dec,
  DOI = {10.1007/BF02551274},
  KEYWORDS = {Neural networks ; Approximation ; Completeness},
  PDF = {https://hal.science/hal-03753170/file/Cybenko1989.pdf},
  HAL_ID = {hal-03753170},
  HAL_VERSION = {v1},
}

@article{lu2019dying,
  title={Dying relu and initialization: Theory and numerical examples},
  author={Lu, Lu and Shin, Yeonjong and Su, Yanhui and Karniadakis, George Em},
  journal={arXiv preprint arXiv:1903.06733},
  year={2019}
}


@article{rossignac05shape_complexity,
  author    = {Jarek Rossignac},
  title     = {Shape complexity},
  journal   = {Vis. Comput.},
  volume    = {21},
  number    = {12},
  pages     = {985--996},
  year      = {2005},
  url       = {https://doi.org/10.1007/s00371-005-0362-7},
  doi       = {10.1007/s00371-005-0362-7},
  timestamp = {Thu, 04 Jun 2020 01:00:00 +0200},
  biburl    = {https://dblp.org/rec/journals/vc/Rossignac05.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{lei2020geometric,
  title={A geometric understanding of deep learning},
  author={Lei, Na and An, Dongsheng and Guo, Yang and Su, Kehua and Liu, Shixia and Luo, Zhongxuan and Yau, Shing-Tung and Gu, Xianfeng},
  journal={Engineering},
  volume={6},
  number={3},
  pages={361--374},
  year={2020},
  publisher={Elsevier}
}

@article{sun2015large,
  title={Large margin deep neural networks: Theory and algorithms},
  author={Sun, Shizhao and Chen, Wei and Wang, Liwei and Liu, Tie-Yan},
  journal={arXiv preprint arXiv:1506.05232},
  volume={148},
  year={2015}
}

@article{soudry2018implicit,
  title={The implicit bias of gradient descent on separable data},
  author={Soudry, Daniel and Hoffer, Elad and Nacson, Mor Shpigel and Gunasekar, Suriya and Srebro, Nathan},
  journal={The Journal of Machine Learning Research},
  volume={19},
  number={1},
  pages={2822--2878},
  year={2018},
  publisher={JMLR. org}
}

@article{karimi2019characterizing,
  title={Characterizing the decision boundary of deep neural networks},
  author={Karimi, Hamid and Derr, Tyler and Tang, Jiliang},
  journal={arXiv preprint arXiv:1912.11460},
  year={2019}
}

@article{pascanu2013number,
  title={On the number of response regions of deep feed forward networks with piece-wise linear activations},
  author={Pascanu, Razvan and Montufar, Guido and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1312.6098},
  year={2013}
}

@inproceedings{delalleau11shallowvsdeep,
 author = {Delalleau, Olivier and Bengio, Yoshua},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K. Q. Weinberger},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Shallow vs. Deep Sum-Product Networks},
 url = {https://proceedings.neurips.cc/paper/2011/file/8e6b42f1644ecb1327dc03ab345e618b-Paper.pdf},
 volume = {24},
 year = {2011}
}

@inproceedings{cohen2016expressive,
  title={On the expressive power of deep learning: A tensor analysis},
  author={Cohen, Nadav and Sharir, Or and Shashua, Amnon},
  booktitle={Conference on learning theory},
  pages={698--728},
  year={2016},
  organization={PMLR}
}


@inproceedings{bengio11expressivepowerofdeeparchitectures,
author = {Bengio, Yoshua and Delalleau, Olivier},
title = {On the Expressive Power of Deep Architectures},
year = {2011},
isbn = {9783642244766},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Deep architectures are families of functions corresponding to deep circuits. Deep
Learning algorithms are based on parametrizing such circuits and tuning their parameters
so as to approximately optimize some training objective. Whereas it was thought too
difficult to train deep architectures, several successful algorithms have been proposed
in recent years. We review some of the theoretical motivations for deep architectures,
as well as some of their practical successes, and propose directions of investigations
to address some of the remaining challenges.},
booktitle = {Proceedings of the 14th International Conference on Discovery Science},
pages = {1},
numpages = {1},
location = {Espoo, Finland},
series = {DS'11}
}

@article{kanamori2011f,
  title={$ f $-Divergence Estimation and Two-Sample Homogeneity Test Under Semiparametric Density-Ratio Models},
  author={Kanamori, Takafumi and Suzuki, Taiji and Sugiyama, Masashi},
  journal={IEEE transactions on information theory},
  volume={58},
  number={2},
  pages={708--720},
  year={2011},
  publisher={IEEE}
}

@inproceedings{kanamari07uLSIF,
 author = {Kanamori, Takafumi and Hido, Shohei and Sugiyama, Masashi},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Efficient Direct Density Ratio Estimation for Non-stationarity Adaptation and Outlier Detection},
 volume = {21},
 year = {2009}
}



@InProceedings{croce19provablerobustness, 
title = {Provable Robustness of ReLU networks via Maximization of Linear Regions}, 
author = {Croce, Francesco and Andriushchenko, Maksym and Hein, Matthias}, 
booktitle = {Proceedings of the Twenty-Second International Conference on Artificial Intelligence and Statistics}, 
pages = {2057--2066}, 
year = {2019}, 
volume = {89}, 
series = {Proceedings of Machine Learning Research}, 
month = {4}, 
publisher = {PMLR},  
abstract = {It has been shown that neural network classifiers are not robust. This raises concerns about their usage in safety-critical systems. We propose in this paper a regularization scheme for ReLU networks which provably improves the robustness of the classifier by maximizing the linear regions of the classifier as well as the distance to the decision boundary. Using our regularization we can even find the minimal adversarial perturbation for a certain fraction of test points for large networks. In the experiments we show that our approach improves upon pure adversarial training both in terms of lower and upper bounds on the robustness and is comparable or better than the state of the art in terms of test error and robustness.} }

@article{choi2021featurized,
  title={Featurized Density Ratio Estimation},
  author={Choi, Kristy and Liao, Madeline and Ermon, Stefano},
  journal={arXiv preprint arXiv:2107.02212},
  year={2021}
}

@INPROCEEDINGS{loog12veronoitessellation,  
    author={Loog, Marco},  
    booktitle={2012 IEEE International Workshop on Machine Learning for Signal Processing},   
    title={Nearest neighbor-based importance weighting},   year={2012},  
    volume={},  
    number={},  
    pages={1-6}
}

@article{xu08topologicalvoronoiprops,
author = {T. Xu  and  M. Li},
title = {Topological and statistical properties of a constrained Voronoi tessellation},
journal = {Philosophical Magazine},
volume = {89},
number = {4},
pages = {349-374},
year  = {2009},
publisher = {Taylor & Francis},
doi = {10.1080/14786430802647065},
URL = {https://doi.org/10.1080/14786430802647065},
eprint = {https://doi.org/10.1080/14786430802647065}
}

@inproceedings{szegedy14intriguing,
title	= {Intriguing properties of neural networks},
author	= {Christian Szegedy and Wojciech Zaremba and Ilya Sutskever and Joan Bruna and Dumitru Erhan and Ian Goodfellow and Rob Fergus},
year	= {2014},
URL	= {http://arxiv.org/abs/1312.6199},
booktitle	= {International Conference on Learning Representations}
}



@INPROCEEDINGS{Dalvi04adversarialclassification,
    author = {Nilesh Dalvi and Pedro Domingos and Mausam Sumit and Sanghai Deepak Verma},
    title = {Adversarial classification},
    booktitle = {In Proceedings of the Tenth International Conference on Knowledge Discovery and Data Mining},
    year = {2004},
    pages = {99--108},
    publisher = {ACM Press}
}


@article{gezer21statisticalpoissonvoronoi,
author = {Fatih Gezer and Robert G. Aykroyd and Stuart Barber},
title = {Statistical properties of Poisson-Voronoi tessellation cells in bounded regions},
journal = {Journal of Statistical Computation and Simulation},
volume = {91},
number = {5},
pages = {915-933},
year  = {2021},
publisher = {Taylor & Francis},
doi = {10.1080/00949655.2020.1836184},
URL = {https://doi.org/10.1080/00949655.2020.1836184},
eprint = {https://doi.org/10.1080/00949655.2020.1836184}
}




@inproceedings{trimmel2021tropex,
title={TropEx: An Algorithm for Extracting Linear Terms in Deep Neural Networks},
author={Martin Trimmel and Henning Petzka and Cristian Sminchisescu},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=IqtonxWI0V3}
}

@article{maragos21tropicalgeometryandml,
  author={Maragos, Petros and Charisopoulos, Vasileios and Theodosis, Emmanouil},
  journal={Proceedings of the IEEE}, 
  title={Tropical Geometry and Machine Learning}, 
  year={2021},
  volume={109},
  number={5},
  pages={728-755},
  doi={10.1109/JPROC.2021.3065238}}

@article{alfarra21bounderies,
  author    = {Motasem Alfarra and
               Adel Bibi and
               Hasan Hammoud and
               Mohamed Gaafar and
               Bernard Ghanem},
  title     = {On the Decision Boundaries of Deep Neural Networks: {A} Tropical Geometry
               Perspective},
  journal   = {CoRR},
  volume    = {abs/2002.08838},
  year      = {2020},
  url       = {https://arxiv.org/abs/2002.08838},
  archivePrefix = {arXiv},
  eprint    = {2002.08838},
  timestamp = {Fri, 09 Apr 2021 18:27:17 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2002-08838.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{Montufar2015,
title = {When does a mixture of products contain a product of mixtures?},
author = {Mont{\'u}far, {Guido F.} and Jason Morton},
year = {2015},
month = {1},
day = {1},
language = {English (US)},
volume = {29},
pages = {321--347},
journal = {SIAM Journal on Discrete Mathematics},
publisher = {Society for Industrial and Applied Mathematics Publications},
number = {1}
}

@INPROCEEDINGS{Indyk04low-distortionembeddings,
    author = {Piotr Indyk and Jiri Matousek},
    title = {Low-Distortion Embeddings of Finite Metric Spaces},
    booktitle = {Handbook of Discrete and Computational Geometry},
    year = {2004},
    pages = {177--196}}

@inproceedings{cuturi2013sinkhorn,
  title={Sinkhorn distances: lightspeed computation of optimal transport.},
  author={Cuturi, Marco},
  booktitle={NIPS},
  volume={2},
  number={3},
  pages={4},
  year={2013}
}

@ARTICLE{arora16understandingReLU,
       author = {{Arora}, Raman and {Basu}, Amitabh and {Mianjy}, Poorya and {Mukherjee}, Anirbit},
        title = "{Understanding Deep Neural Networks with Rectified Linear Units}",
      journal = {ICLR},
     keywords = {Computer Science - Machine Learning, Condensed Matter - Disordered Systems and Neural Networks, Computer Science - Artificial Intelligence, Computer Science - Computational Complexity, Statistics - Machine Learning},
         year = 2018
}



@inproceedings{izbicki2014high,
  title={High-dimensional density ratio estimation with extensions to approximate likelihood computation},
  author={Izbicki, Rafael and Lee, Ann and Schafer, Chad},
  booktitle={Artificial Intelligence and Statistics},
  pages={420--429},
  year={2014},
  organization={PMLR}
}

@article{JMLR:v9:vandermaaten08a,
  author  = {Laurens van der Maaten and Geoffrey Hinton},
  title   = {Visualizing Data using t-SNE},
  journal = {Journal of Machine Learning Research},
  year    = {2008},
  volume  = {9},
  number  = {86},
  pages   = {2579-2605}
}

@inproceedings{courty2017learning,
  TITLE = {{Learning Wasserstein Embeddings}},
  AUTHOR = {Courty, Nicolas and Flamary, R{\'e}mi and Ducoffe, M{\'e}lanie},
  BOOKTITLE = {{ICLR 2018 - 6th International Conference on Learning Representations}},
  ADDRESS = {Vancouver, Canada},
  PAGES = {1-13},
  YEAR = {2018},
  MONTH = Apr,
  PDF = {https://hal.inria.fr/hal-01956306/file/iclr2018_main.pdf},
  HAL_ID = {hal-01956306},
  HAL_VERSION = {v1},
}



@article{xiao2018bourgan,
	title={BourGAN: Generative Networks with Metric Embeddings},
	author={Xiao, Chang and Zhong, Peilin and Zheng, Changxi},
	journal={Advances in Neural Information Processing Systems},
	pages={2269--2280},
	year={2018}
}

@ARTICLE{2018arXiv181205944S,
       author = {{Su{\'a}rez-D{\'\i}az}, Juan Luis and {Garc{\'\i}a}, Salvador and {Herrera}, Francisco},
        title = "{A Tutorial on Distance Metric Learning: Mathematical Foundations, Algorithms, Experimental Analysis, Prospects and Challenges (with Appendices on Mathematical Background and Detailed Algorithms Explanation)}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
         year = 2018,
        month = dec,
        pages = {arXiv:1812.05944},
archivePrefix = {arXiv},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}


@article{moser12weylzonotope,
author = {Moser, Bernhard A.},
title = {Geometric Characterization of Weyl's Discrepancy Norm in Terms of Its n-Dimensional Unit Balls},
year = {2012},
issue_date = {December  2012},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {48},
number = {4},
abstract = {Weyl's discrepancy measure induces a norm on \'{z}n which shows a monotonicity and a Lipschitz property when applied to differences of index-shifted sequences. It turns out that its n-dimensional unit ball is a zonotope that results from a multiple sheared projection from the (n+1)-dimensional hypercube which can be interpreted as a discrete differentiation. This characterization reveals that this norm is the canonical metric between sequences of differences of values from the unit interval in the sense that the n-dimensional unit ball of the discrepancy norm equals the space of such sequences.},
journal = {Discrete Comput. Geom.},
pages = {793-806},
numpages = {14},
keywords = {Hypercube, Zonotope, Discrepancy, Lipschitz property}
}

@Inbook{David2011orderstatistics,
author="David, Herbert A.",
editor="Lovric, Miodrag",
title="Order Statistics",
bookTitle="International Encyclopedia of Statistical Science",
year="2011",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="1039--1040",
isbn="978-3-642-04898-2",
doi="10.1007/978-3-642-04898-2_436",
url="https://doi.org/10.1007/978-3-642-04898-2_436"
}



@INPROCEEDINGS{zhao21convergenceratesKNN,
  author={Zhao, Puning and Lai, Lifeng},
  booktitle={2021 IEEE International Symposium on Information Theory (ISIT)}, 
  title={On the Convergence Rates of KNN Density Estimation}, 
  year={2021},
  volume={},
  number={},
  pages={2840-2845},
  doi={10.1109/ISIT45174.2021.9518025}}

@article{lima08redshiftdistribution,
    author = {Lima, Marcos and Cunha, Carlos E. and Oyaizu, Hiroaki and Frieman, Joshua and Lin, Huan and Sheldon, Erin S.},
    title = "{Estimating the redshift distribution of photometric galaxy samples}",
    journal = {Monthly Notices of the Royal Astronomical Society},
    volume = {390},
    number = {1},
    pages = {118-130},
    year = {2008},
    month = {10},
    abstract = "{We present an empirical method for estimating the underlying redshift distribution N(z) of galaxy photometric samples from photometric observables. The method does not rely on photometric redshift (photo-z) estimates for individual galaxies, which typically suffer from biases. Instead, it assigns weights to galaxies in a spectroscopic subsample such that the weighted distributions of photometric observables (e.g. multiband magnitudes) match the corresponding distributions for the photometric sample. The weights are estimated using a nearest neighbour technique that ensures stability in sparsely populated regions of colour–magnitude space. The derived weights are then summed in redshift bins to create the redshift distribution. We apply this weighting technique to data from the Sloan Digital Sky Survey as well as to mock catalogues for the Dark Energy Survey, and compare the results to those from the estimation of photo-zs derived by a neural network algorithm. We find that the weighting method accurately recovers the underlying redshift distribution, typically better than the photo-z reconstruction, provided the spectroscopic subsample spans the range of photometric observables covered by the photometric sample.}",
    issn = {0035-8711},
    doi = {10.1111/j.1365-2966.2008.13510.x},
    url = {https://doi.org/10.1111/j.1365-2966.2008.13510.x},
    eprint = {https://academic.oup.com/mnras/article-pdf/390/1/118/2955170/mnras0390-0118.pdf},
}

@article{cuzzolin2020knowing,
  author    = {Cuzzolin, Fabio and Morelli, Alessio and Cîrstea, Bogdan and Sahakian, Barbara J.},
  title     = {Knowing me, knowing you: theory of mind in AI},
  journal   = {Psychological Medicine},
  year      = {2020},
  volume    = {50},
  number    = {7},
  pages     = {1057--1061},
  month     = {May},
  doi       = {10.1017/S0033291720000835},
  pmid      = {32375908},
  pmcid     = {PMC7253617},
  note      = {Epub 2020 May 7}
}




@article{reygner2020reweighting,
  title={Reweighting samples under covariate shift using a Wasserstein distance criterion},
  author={Reygner, Julien and Touboul, Adrien},
  journal={arXiv preprint arXiv:2010.09267},
  year={2020}
}

@InProceedings{zhang18tropical,
  title = 	 {Tropical Geometry of Deep Neural Networks},
  author =       {Zhang, Liwen and Naitzat, Gregory and Lim, Lek-Heng},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {5824--5832},
  year = 	 {2018},
  volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {7},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v80/zhang18i/zhang18i.pdf},
  url = 	 {http://proceedings.mlr.press/v80/zhang18i.html},
  abstract = 	 {We establish, for the first time, explicit connections between feedforward neural networks with ReLU activation and tropical geometry — we show that the family of such neural networks is equivalent to the family of tropical rational maps. Among other things, we deduce that feedforward ReLU neural networks with one hidden layer can be characterized by zonotopes, which serve as building blocks for deeper networks; we relate decision boundaries of such neural networks to tropical hypersurfaces, a major object of study in tropical geometry; and we prove that linear regions of such neural networks correspond to vertices of polytopes associated with tropical rational functions. An insight from our tropical formulation is that a deeper network is exponentially more expressive than a shallow network.}
}

@article{suter2011timeandmoraljudgement,
title = {Time and moral judgment},
journal = {Cognition},
volume = {119},
number = {3},
pages = {454-458},
year = {2011},
issn = {0010-0277},
doi = {https://doi.org/10.1016/j.cognition.2011.01.018},
url = {https://www.sciencedirect.com/science/article/pii/S0010027711000448},
author = {Renata S. Suter and Ralph Hertwig},
keywords = {Morality, Judgment, Reasoning, Intuition, Moral dilemmas, Time pressure},
abstract = {Do moral judgments hinge on the time available to render them? According to a recent dual-process model of moral judgment, moral dilemmas that engage emotional processes are likely to result in fast deontological gut reactions. In contrast, consequentialist responses that tot up lives saved and lost in response to such dilemmas would require cognitive control to override the initial response. Cognitive control, however, takes time. In two experiments, we manipulated the time available to arrive at moral judgments in two ways: by allotting a fixed short or large amount of time, and by nudging people to answer swiftly or to deliberate thoroughly. We found that faster responses indeed lead to more deontological responses among those moral dilemmas in which the killing of one to save many necessitates manhandling an innocent person and in which this action is depicted as a means to an end. Thus, our results are the first demonstration that inhibiting cognitive control through manipulations of time alters moral judgments.}
}

@incollection{clark1991grounding,
  title={Grounding in communication},
  author={Clark, Herbert H. and Brennan, Susan E.},
  booktitle={Perspectives on Socially Shared Cognition},
  editor={Resnick, Lauren B. and Levine, John M. and Teasley, Stephanie D.},
  pages={127--149},
  publisher={APA Press},
  year={1991}
}

@article{zhang2024mutual,
  title={Mutual theory of mind in human-ai collaboration: An empirical study with llm-driven ai agents in a real-time shared workspace task},
  author={Zhang, Shao and Wang, Xihuai and Zhang, Wenhao and Chen, Yongshan and Gao, Landi and Wang, Dakuo and Zhang, Weinan and Wang, Xinbing and Wen, Ying},
  journal={arXiv preprint arXiv:2409.08811},
  year={2024}
}

@inproceedings{shapira2024clever,
    title = "Clever Hans or Neural Theory of Mind? Stress Testing Social Reasoning in Large Language Models",
    author = "Shapira, Natalie  and
      Levy, Mosh  and
      Alavi, Seyed Hossein  and
      Zhou, Xuhui  and
      Choi, Yejin  and
      Goldberg, Yoav  and
      Sap, Maarten  and
      Shwartz, Vered",
    booktitle = "Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics",
    month = "3",
    year = "2024",
    address = "St. Julian{'}s, Malta",
    publisher = "Association for Computational Linguistics",
    pages = "2257--2273",
    abstract = "The escalating debate on AI`s capabilities warrants developing reliable metrics to assess machine {\textquotedblleft}intelligence.{\textquotedblright} Recently, many anecdotal examples were used to suggest that newer Large Language Models (LLMs) like ChatGPT and GPT-4 exhibit Neural Theory-of-Mind (N-ToM); however, prior work reached conflicting conclusions regarding those abilities. We investigate the extent of LLMs' N-ToM through an extensive evaluation of 6 tasks and find that while LLMs exhibit certain N-ToM abilities, this behavior is far from being robust. We further examine the factors impacting performance on N-ToM tasks and discover that LLMs struggle with adversarial examples, indicating reliance on shallow heuristics rather than robust ToM abilities. We caution against drawing conclusions from anecdotal examples, limited benchmark testing, and using human-designed psychological tests to evaluate models."
}

@article{wimmer1983beliefsaboutbeliefs,
title = {Beliefs about beliefs: Representation and constraining function of wrong beliefs in young children's understanding of deception},
journal = {Cognition},
volume = {13},
number = {1},
pages = {103-128},
year = {1983},
issn = {0010-0277},
author = {Heinz Wimmer and Josef Perner},
abstract = {Understanding of another person's wrong belief requires explicit representation of the wrongness of this person's belief in relation to one's own knowledge. Three to nine year old children's understanding of two sketches was tested. In each sketch subjects observed how a protagonist put an object into a location x and then witnessed that in the absence of the protagonist the object was transferred from x to location y. Since this transfer came as a surprise they had to assume that the protagonist still believed that the object was in x. Subjects had to indicate where the protagonist will look for the object at his return. None of the 3–4-year old, 57% of 4–6-year old, and 86% of 6–9-year old children pointed correctly to location x in both sketches. Of the many cases where 4–6-year olds made an error they failed in only about 20% to remember the initial location correctly. As a test of the stability of children's representation of the protagonist's wrong belief the sketches continued with a statement about the protagonist's intention to either deceive an antagonist or truthfully inform a friend about the object's location. Independent of age, of those children who correctly thought that the protagonist would search in x, 85% of the time they also correctly thought that he would direct his antagonist to location y and his friend to location x. This shows that once children can represent a person's beliefs they can constrain their interpretation of this person's stated intentions to the person's beliefs. In a more story-like situation another group of children had to infer a deceptive plan from the depiction of a goal conflict between two story characters and one character's expedient utterance. At the age of 4–5 years children correctly judged this utterance as a lie only 28% of the time while 5–6-year olds did so 94% of the time. These results suggest that around the ages of 4 to 6 years the ability to represent the relationship between two or more person's epistemic states emerges and becomes firmly established.
Résumé
Comprendre que ce que croit un tiers est erroné requiert une représentation explicitée de cette fausse croyance en relation avec son savoir propre. On a testé la compréhension de deux sketches par des enfants de 3 à 9 ans. Dans chacun des sketches les sujets observent un protagoniste placer un objet dans un lieu ‘x’, puis sont témoins du transfert de cet objet de ‘x’ en ‘y’ en l'absence du protagoniste. Ce transfert doit causer une surprise chez le protagoniste dont on assume qu'il croit que l'objet se trouve toujours en ‘x’. Les sujets doivent dire où le protagoniste va chercher l'objet. Aucun 3–4 ans n'indique correctement le lieu ‘x’, 57% des 4–6 ans et 86% des 6–9 ans le font. Parmi les nombreuses erreurs des 4–6 ans seules 20% sont attribuables à une incapacité de se souvenir du lieu ‘x’. Pour tester la stabilité de la représentation de la croyance erronée, on dit que le protagoniste a l'intention soit de tromper un adversaire soit d'informer un ami sur le lieu où se trouve l'objet. Indépendamment de leur âge, les enfants ayant donné des réponses correctes disent correctement dans 85% des cas que le protagoniste conduirait l'adversaire en ‘y’ et l'ami en ‘x’. Lorsque les enfants se représentent les croyances d'une personne, ils peuvent faire dépendre leurs interprétations des intentions exprimées par celles-ci à partir de ses croyances. Dans une situation de type histoire, un autre groupe d'enfants doit inférer un essai de tromperie à partir de la représentation d'un but conflictuel entre deux des personnages de l'énoncé tactique d'un des personnages. A 4–5 ans les enfants ne jugent correctement cet énoncé comme mensonger que dans 28% des cas alors qu'à 5–6 on a 94% de reponses correctes. Les résultats indiquent que vers 4–6 ans la capacité de représenter une relation entre les états épistémiques de deux personnes ou plus émerge et se confirme.}
}

@inproceedings{Gandhi2023understanding,
 author = {Gandhi, Kanishk and Fraenken, Jan-Philipp and Gerstenberg, Tobias and Goodman, Noah},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
 pages = {13518--13529},
 publisher = {Curran Associates, Inc.},
 title = {Understanding Social Reasoning in Language Models with Language Models},
 volume = {36},
 year = {2023}
}


@inproceedings{Sclar2023MindingLM,
  title={Minding Language Models’ (Lack of) Theory of Mind: A Plug-and-Play Multi-Character Belief Tracker},
  author={Melanie Sclar and Sachin Kumar and Peter West and Alane Suhr and Yejin Choi and Yulia Tsvetkov},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  year={2023},
}

@inproceedings{Sap2022NeuralTO,
  title={Neural Theory-of-Mind? On the Limits of Social Intelligence in Large LMs},
  author={Maarten Sap and Ronan Le Bras and Daniel Fried and Yejin Choi},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  year={2022},
  url={https://api.semanticscholar.org/CorpusID:253098632}
}

@article{Yim2024EvaluatingAE,
  title={Evaluating and Enhancing LLMs Agent based on Theory of Mind in Guandan: A Multi-Player Cooperative Game under Imperfect Information},
  author={Yauwai Yim and Chunkit Chan and Tianyu Shi and Zheye Deng and Wei Fan and Tianshi ZHENG and Yangqiu Song},
  journal={ArXiv},
  year={2024},
  volume={abs/2408.02559},
  url={https://api.semanticscholar.org/CorpusID:271710034}
}

@inproceedings{chan2024negotiationtom,
    title = "{N}egotiation{T}o{M}: A Benchmark for Stress-testing Machine Theory of Mind on Negotiation Surrounding",
    author = "Chan, Chunkit  and
      Jiayang, Cheng  and
      Yim, Yauwai  and
      Deng, Zheye  and
      Fan, Wei  and
      Li, Haoran  and
      Liu, Xin  and
      Zhang, Hongming  and
      Wang, Weiqi  and
      Song, Yangqiu",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2024",
    month = "11",
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    pages = "4211--4241",
    abstract = "Large Language Models (LLMs) have sparked substantial interest and debate concerning their potential emergence of Theory of Mind (ToM) ability. Theory of mind evaluations currently focuses on testing models using machine-generated data or game settings prone to shortcuts and spurious correlations, which lacks evaluation of machine ToM ability in real-world human interaction scenarios. This poses a pressing demand to develop new real-world scenario benchmarks. We introduce NegotiationToM, a new benchmark designed to stress-test machine ToM in real-world negotiation surrounding covered multi-dimensional mental states (i.e., desires, beliefs, and intentions). Our benchmark builds upon the Belief-Desire-Intention (BDI) agent modeling theory and conducts the necessary empirical experiments to evaluate large language models. Our findings demonstrate that NegotiationToM is challenging for state-of-the-art LLMs, as they consistently perform significantly worse than humans, even when employing the chain-of-thought (CoT) method."
}

@inproceedings{kim2023fantom,
    title = "{FANT}o{M}: A Benchmark for Stress-testing Machine Theory of Mind in Interactions",
    author = "Kim, Hyunwoo  and
      Sclar, Melanie  and
      Zhou, Xuhui  and
      Bras, Ronan  and
      Kim, Gunhee  and
      Choi, Yejin  and
      Sap, Maarten",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    pages = "14397--14413",
    abstract = "Theory of mind (ToM) evaluations currently focus on testing models using passive narratives that inherently lack interactivity. We introduce FANToM, a new benchmark designed to stress-test ToM within information-asymmetric conversational contexts via question answering. Our benchmark draws upon important theoretical requisites from psychology and necessary empirical considerations when evaluating large language models (LLMs). In particular, we formulate multiple types of questions that demand the same underlying reasoning to identify illusory or false sense of ToM capabilities in LLMs. We show that FANToM is challenging for state-of-the-art LLMs, which perform significantly worse than humans even with chain-of-thought reasoning or fine-tuning."
}

@inproceedings{Sclar2022SymmetricMT,
  title={Symmetric Machine Theory of Mind},
  author={Melanie Sclar and Graham Neubig and Yonatan Bisk},
  journal={ICML},
  year={2022},
}

@article{xu2023exploring,
  title={Exploring large language models for communication games: An empirical study on werewolf},
  author={Xu, Yuzhuang and Wang, Shuo and Li, Peng and Luo, Fuwen and Wang, Xiaolong and Liu, Weidong and Liu, Yang},
  journal={arXiv preprint arXiv:2309.04658},
  year={2023}
}

@inproceedings{kano2023aiwolfdial,
    title = "{AIW}olf{D}ial 2023: Summary of Natural Language Division of 5th International {AIW}olf Contest",
    author = "Kano, Yoshinobu  and
      Watanabe, Neo  and
      Kagaminuma, Kaito  and
      Aranha, Claus  and
      Lee, Jaewon  and
      Hauer, Benedek  and
      Shibata, Hisaichi  and
      Miki, Soichiro  and
      Nakamura, Yuta  and
      Okubo, Takuya  and
      Shigemura, Soga  and
      Ito, Rei  and
      Takashima, Kazuki  and
      Fukuda, Tomoki  and
      Wakutani, Masahiro  and
      Hatanaka, Tomoya  and
      Uchida, Mami  and
      Abe, Mikio  and
      Mikami, Akihiro  and
      Otsuki, Takashi  and
      Qi, Zhiyang  and
      Harada, Kei  and
      Inaba, Michimasa  and
      Katagami, Daisuke  and
      Osawa, Hirotaka  and
      Toriumi, Fujio",
    editor = "Mille, Simon",
    booktitle = "Proceedings of the 16th International Natural Language Generation Conference: Generation Challenges",
    month = sep,
    year = "2023",
    address = "Prague, Czechia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.inlg-genchal.13/",
    pages = "84--100",
    abstract = "We held our 5th annual AIWolf international contest to automatically play the Werewolf game {\textquotedblleft}Mafia{\textquotedblright}, where players try finding liars via conversations, aiming at promoting developments in creating agents of more natural conversations in higher level, such as longer contexts, personal relationships, semantics, pragmatics, and logics, revealing the capabilities and limits of the generative AIs. In our Natural Language Division of the contest, we had six Japanese speaking agents from five teams, and three English speaking agents, to mutually run games. By using the game logs, We performed human subjective evaluations and detailed log analysis. We found that the entire system performance has largely improved over the previous year, due to the recent advantages of the LLMs. However, it is not perfect at all yet; the generated talks are sometimes inconsistent with the game actions, it is still doubtful that the agents could infer roles by logics rather than superficial utterance generations. It is not explicitly observed in this log but it would be still difficult to make an agent telling a lie, pretend as a villager but it has an opposite goal inside. Our future work includes to reveal the capability of the LLMs, whether they can make the duality of the {\textquotedblleft}liar{\textquotedblright}, in other words, holding a {\textquotedblleft}true{\textquotedblright} and a {\textquotedblleft}false{\textquotedblright} circumstances of the agent at the same time, even holding what these circumstances look like from other agents."
}

@article{xu2023language,
  title={Language agents with reinforcement learning for strategic play in the werewolf game},
  author={Xu, Zelai and Yu, Chao and Fang, Fei and Wang, Yu and Wu, Yi},
  journal={arXiv preprint arXiv:2310.18940},
  year={2023}
}

@inproceedings{jin2024learning,
  title     = {Learning to Discuss Strategically: A Case Study on One Night Ultimate Werewolf},
  author    = {Jin, Xuanfa and Wang, Ziyan and Du, Yali and Fang, Meng and Zhang, Haifeng and Wang, Jun},
  booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
  year      = {2024}
}


@article{dubey2024llama,
  title={The llama 3 herd of models},
  author={Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024}
}

@article{premack1978theoryofmind,
  title     = {Does the chimpanzee have a theory of mind?},
  author    = {Premack, David and Woodruff, Guy},
  journal   = {Behavioral and Brain Sciences},
  volume    = {1},
  number    = {4},
  pages     = {515--526},
  year      = {1978},
  publisher = {Cambridge University Press},
}


@inproceedings{lewandowski2025turinggame,
  title     = {The Turing Game},
  author    = {Michal Lewandowski and Simon Schmid and Patrick Mederitsch and Alexander Aufreiter and Gregor Aichinger and Felix Nessler and Severin Bergsmann and Viktor Szolga and Tobias Halmdienst and Bernhard Nessler},
  booktitle = {NeurIPS Workshop on System 2 Reasoning at Scale},
  year      = {2024},
  note      = {Also presented at the AAAI Workshop on Theory of Mind 2025. Both workshops are non-archival.}
}

@article{whitney1947mannwhitneytest,
author = {H. B. Mann and D. R. Whitney},
title = {{On a Test of Whether one of Two Random Variables is Stochastically Larger than the Other}},
volume = {18},
journal = {The Annals of Mathematical Statistics},
number = {1},
publisher = {Institute of Mathematical Statistics},
pages = {50 -- 60},
year = {1947},
doi = {10.1214/aoms/1177730491},
URL = {https://doi.org/10.1214/aoms/1177730491}
}


@article{Dobson2003,
  author  = {Dobson, Christopher M.},
  title   = {Protein folding and misfolding},
  journal = {Nature},
  volume  = {426},
  number  = {6968},
  pages   = {884--890},
  year    = {2003},
  doi     = {10.1038/nature02261},
}

@article{Cohen1992powerprimer,
  author    = {Jacob Cohen},
  title     = {A power primer},
  journal   = {Psychological Bulletin},
  year      = {1992},
  volume    = {112},
  number    = {1},
  pages     = {155--159},
  month     = {7},
  doi       = {10.1037//0033-2909.112.1.155}
}



@inproceedings{zhang2020ternarybert,
    title = "{T}ernary{BERT}: Distillation-aware Ultra-low Bit {BERT}",
    author = "Zhang, Wei  and
      Hou, Lu  and
      Yin, Yichun  and
      Shang, Lifeng  and
      Chen, Xiao  and
      Jiang, Xin  and
      Liu, Qun",
    editor = "Webber, Bonnie  and
      Cohn, Trevor  and
      He, Yulan  and
      Liu, Yang",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.37/",
    doi = "10.18653/v1/2020.emnlp-main.37",
    pages = "509--521",
    abstract = "Transformer-based pre-training models like BERT have achieved remarkable performance in many natural language processing tasks. However, these models are both computation and memory expensive, hindering their deployment to resource-constrained devices. In this work, we propose TernaryBERT, which ternarizes the weights in a fine-tuned BERT model. Specifically, we use both approximation-based and loss-aware ternarization methods and empirically investigate the ternarization granularity of different parts of BERT. Moreover, to reduce the accuracy degradation caused by lower capacity of low bits, we leverage the knowledge distillation technique in the training process. Experiments on the GLUE benchmark and SQuAD show that our proposed TernaryBERT outperforms the other BERT quantization methods, and even achieves comparable performance as the full-precision model while being 14.9x smaller."
}

@inproceedings{engels2025not,
  title={Not all language model features are linear},
  author={Engels, Joshua and Michaud, Eric J and Liao, Isaac and Gurnee, Wes and Tegmark, Max},
  booktitle={ICLR},
  year={2025}
}



@InProceedings{hou2018lossaware,
    author    = {Hou, Lu and Kwok, James T.},
    title     = {Loss-Aware Weight Quantization of Deep Networks},
    booktitle = {ICLR},
    year      = {2018},
}


@inproceedings{bai2021binarybert,
    title = "{B}inary{BERT}: Pushing the Limit of {BERT} Quantization",
    author = "Bai, Haoli  and
      Zhang, Wei  and
      Hou, Lu  and
      Shang, Lifeng  and
      Jin, Jin  and
      Jiang, Xin  and
      Liu, Qun  and
      Lyu, Michael  and
      King, Irwin",
    editor = "Zong, Chengqing  and
      Xia, Fei  and
      Li, Wenjie  and
      Navigli, Roberto",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.334/",
    doi = "10.18653/v1/2021.acl-long.334",
    pages = "4334--4348",
    abstract = "The rapid development of large pre-trained language models has greatly increased the demand for model compression techniques, among which quantization is a popular solution. In this paper, we propose BinaryBERT, which pushes BERT quantization to the limit by weight binarization. We find that a binary BERT is hard to be trained directly than a ternary counterpart due to its complex and irregular loss landscape. Therefore, we propose ternary weight splitting, which initializes BinaryBERT by equivalently splitting from a half-sized ternary network. The binary model thus inherits the good performance of the ternary one, and can be further enhanced by fine-tuning the new architecture after splitting. Empirical results show that our BinaryBERT has only a slight performance drop compared with the full-precision model while being 24x smaller, achieving the state-of-the-art compression results on the GLUE and SQuAD benchmarks. Code will be released."
}

@article{kim2021bert,
  title={I-BERT: Integer-only BERT Quantization},
  author={Kim, Sehoon and Gholami, Amir and Yao, Zhewei and Mahoney, Michael W and Keutzer, Kurt},
  journal={ICML},
  year={2021}
}

@InProceedings{lewandowski2024cantornet,
    author = {Lewandowski, Michal and Eghbal-zadeh, Hamid and A.Moser, Bernhard},
    series = 	 {Proceedings of Machine Learning Research},
  booktitle = 	 {NeurIPS Workshop On Symmetry and Geometry in Neural Networks},
    title = {CantorNet: A Sandbox for Testing Topological and  Geometrical Measures},
    year = {2024}
}


@InProceedings{Rabinowitz2018machineToM,
  title = 	 {Machine Theory of Mind},
  author =       {Rabinowitz, Neil and Perbet, Frank and Song, Francis and Zhang, Chiyuan and Eslami, S. M. Ali and Botvinick, Matthew},
  booktitle = 	 {ICML},
  pages = 	 {4218--4227},
  year = 	 {2018},
  editor = 	 {Dy, Jennifer and Krause, Andreas},
  volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {7},
  publisher =    {PMLR},
  abstract = 	 {Theory of mind (ToM) broadly refers to humans’ ability to represent the mental states of others, including their desires, beliefs, and intentions. We design a Theory of Mind neural network {–} a ToMnet {–} which uses meta-learning to build such models of the agents it encounters. The ToMnet learns a strong prior model for agents’ future behaviour, and, using only a small number of behavioural observations, can bootstrap to richer predictions about agents’ characteristics and mental states. We apply the ToMnet to agents behaving in simple gridworld environments, showing that it learns to model random, algorithmic, and deep RL agents from varied populations, and that it passes classic ToM tasks such as the "Sally-Anne" test of recognising that others can hold false beliefs about the world.}
}


@article{brown2019superhumanaipoker,
author = {Noam Brown and Tuomas Sandholm},
title = {Superhuman AI for multiplayer poker},
journal = {Science},
volume = {365},
number = {6456},
pages = {885-890},
year = {2019},
abstract = {Computer programs have shown superiority over humans in two-player games such as chess, Go, and heads-up, no-limit Texas hold'em poker. However, poker games usually include six players—a much trickier challenge for artificial intelligence than the two-player variant. Brown and Sandholm developed a program, dubbed Pluribus, that learned how to play six-player no-limit Texas hold'em by playing against five copies of itself (see the Perspective by Blair and Saffidine). When pitted against five elite professional poker players, or with five copies of Pluribus playing against one professional, the computer performed significantly better than humans over the course of 10,000 hands of poker. Science, this issue p. 885; see also p. 864 An AI dubbed Pluribus performs significantly better than human professionals in six-player no-limit Texas hold’em poker. In recent years there have been great strides in artificial intelligence (AI), with games often serving as challenge problems, benchmarks, and milestones for progress. Poker has served for decades as such a challenge problem. Past successes in such benchmarks, including poker, have been limited to two-player games. However, poker in particular is traditionally played with more than two players. Multiplayer games present fundamental additional issues beyond those in two-player games, and multiplayer poker is a recognized AI milestone. In this paper we present Pluribus, an AI that we show is stronger than top human professionals in six-player no-limit Texas hold’em poker, the most popular form of poker played by humans.}}


@article{meta2022diplomacy,
author = {FAIR},
title = {Human-level play in the game of Diplomacy by combining language models with strategic reasoning},
journal = {Science},
volume = {378},
number = {6624},
pages = {1067-1074},
year = {2022},
abstract = {Despite much progress in training artificial intelligence (AI) systems to imitate human language, building agents that use language to communicate intentionally with humans in interactive environments remains a major challenge. We introduce Cicero, the first AI agent to achieve human-level performance in Diplomacy, a strategy game involving both cooperation and competition that emphasizes natural language negotiation and tactical coordination between seven players. Cicero integrates a language model with planning and reinforcement learning algorithms by inferring players’ beliefs and intentions from its conversations and generating dialogue in pursuit of its plans. Across 40 games of an anonymous online Diplomacy league, Cicero achieved more than double the average score of the human players and ranked in the top 10\% of participants who played more than one game. The game Diplomacy has been a major challenge for artificial intelligence (AI). Unlike other competitive games that AI has recently mastered, such as chess, Go, and poker, Diplomacy cannot be solved purely through self-play; it requires the development of an agent to understand other players’ motivations and perspectives and to use natural language to negotiate complex shared plans. The Meta Fundamental AI Research Diplomacy Team (FAIR) et al. developed an agent that is able to play the full natural language form of the game and demonstrates performance well above the human average in an online Diplomacy league. The present work has far-reaching implications for the development of cooperative AI and language models for communication with people, even when interactions involve a mixture of aligned and competing interests. —YS Artificial intelligence demonstrates human-level performance in the strategic board game Diplomacy.}}

@inproceedings{tolstikhin2021MLPMixer,
 author = {Tolstikhin, Ilya O and Houlsby, Neil and Kolesnikov, Alexander and Beyer, Lucas and Zhai, Xiaohua and Unterthiner, Thomas and Yung, Jessica and Steiner, Andreas and Keysers, Daniel and Uszkoreit, Jakob and Lucic, Mario and Dosovitskiy, Alexey},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {24261--24272},
 publisher = {Curran Associates, Inc.},
 title = {MLP-Mixer: An all-MLP Architecture for Vision},
 url = {https://proceedings.neurips.cc/paper_files/paper/2021/file/cba0a4ee5ccd02fda0fe3f9a3e7b89fe-Paper.pdf},
 volume = {34},
 year = {2021}
}



@TECHREPORT{Miller04hyperspacings,
    author = {Erik G. Learned-Miller},
    booktitle = {Proceedings of the 35th International Conference on Machine Learning},
    pages = {04-104},
    title = {Hyperspacings and the estimation of information theoretic quantities},
    institution = {University of Massachusetts Amherst},
    year = {2004}
}

@article{lewandowski25spacefolds,
    author = {Michal Lewandowski and Hamid Eghbalzadeh and Bernhard Heinzl and Raphael Pisoni and Bernhard A.Moser},
    journal = {TMLR},
    title = {On Space Folds of ReLU Neural Networks},
    year = {2025}
}


@article{Moser22tessellationfiltering,
    author = {Bernhard A. Moser and Michal Lewandowski and Somayeh Kargaran and Werner Zellinger and Battista Biggio and Christoph Koutschan},
    journal = {IJCAI},
    title = {Tessellation-Filtering ReLU Neural Networks},
    year = {2022}
}

@incollection{Biggio_2013,
	doi = {10.1007/978-3-642-40994-3_25},
	url = {https://doi.org/10.10072F978-3-642-40994-3_25},
	year = 2013,
	publisher = {Springer Berlin Heidelberg},
	pages = {387--402},
	author = {Battista Biggio and Igino Corona and Davide Maiorca and Blaine Nelson and Nedim {\v{S}
}rndi{\'{c}} and Pavel Laskov and Giorgio Giacinto and Fabio Roli},
title = {Evasion Attacks against Machine Learning at Test Time},
booktitle = {Advanced Information Systems Engineering}
}



@article{khoury2018geometry,
  title={On the geometry of adversarial examples},
  author={Khoury, Marc and Hadfield-Menell, Dylan},
  journal={arXiv preprint arXiv:1811.00525},
  year={2018}
}

@article{bronstein2021geometric,
  title={Geometric deep learning: Grids, groups, graphs, geodesics, and gauges},
  author={Bronstein, Michael M and Bruna, Joan and Cohen, Taco and Veli{\v{c}}kovi{\'c}, Petar},
  journal={arXiv preprint arXiv:2104.13478},
  year={2021}
}

@inproceedings{find2004objectclassification,
 author = {Fink, Michael},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {L. Saul and Y. Weiss and L. Bottou},
 pages = {},
 publisher = {MIT Press},
 title = {Object Classification from a Single Example Utilizing Class Relevance Metrics},
 url = {https://proceedings.neurips.cc/paper_files/paper/2004/file/ef1e491a766ce3127556063d49bc2f98-Paper.pdf},
 volume = {17},
 year = {2004}
}

@article{Higgins2022SymmetryBasedRF,
  title={Symmetry-Based Representations for Artificial and Biological General Intelligence},
  author={Irina Higgins and S{\'e}bastien Racani{\`e}re and Danilo Jimenez Rezende},
  journal={Frontiers in Computational Neuroscience},
  year={2022},
  volume={16},
  url={https://api.semanticscholar.org/CorpusID:247518606}
}

@article{sejnowski2020unreasonable,
  title={The unreasonable effectiveness of deep learning in artificial intelligence},
  author={Sejnowski, Terrence J.},
  journal={Proceedings of the National Academy of Sciences},
  volume={117},
  number={48},
  pages={30033--30038},
  year={2020},
}



@article{fefferman2016testing,
  title={Testing the manifold hypothesis},
  author={Fefferman, Charles and Mitter, Sanjoy and Narayanan, Hariharan},
  journal={Journal of the American Mathematical Society},
  volume={29},
  number={4},
  pages={983--1049},
  year={2016}
}

@article{Smale2007LearningTE,
  title={Learning Theory Estimates via Integral Operators and Their Approximations},
  author={S. Smale and Ding-Xuan Zhou},
  journal={Constructive Approximation},
  year={2007},
  volume={26},
  pages={153-172}
}

@inproceedings{lu2017expressive,
  title={The expressive power of neural networks: A view from the width},
  author={Lu, Zhou and Pu, Hongming and Wang, Feicheng and Hu, Zhiqiang and Wang, Liwei},
  booktitle={Proceedings of the 31st International Conference on Neural Information Processing Systems},
  pages={6232--6240},
  year={2017}
}

@article{SugiyamaMuller200,
author = {Masashi Sugiyama and Klaus-Robert Müller},
doi = {doi:10.1524/stnd.2005.23.4.249},
url = {https://doi.org/10.1524/stnd.2005.23.4.249},
title = {Input-dependent estimation of generalization error under covariate shift},
journal = {},
number = {4},
volume = {23},
year = {2005},
pages = {249--279}
}


@TECHREPORT{UCI_MLrepo,
author = "Dua, Dheeru and Graff, Casey",
year = "2017",
title = "{UCI} Machine Learning Repository",
url = "http://archive.ics.uci.edu/ml",
institution = "University of California, Irvine, School of Information and Computer Sciences" }


@inproceedings{Huang06sample_selection_bias,
 author = {Huang, Jiayuan and Gretton, Arthur and Borgwardt, Karsten and Sch\"{o}lkopf, Bernhard and Smola, Alex},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {},
 publisher = {MIT Press},
 title = {Correcting Sample Selection Bias by Unlabeled Data},
 volume = {19},
 year = {2007}
}




@inproceedings{roth2019odds,
  title={The odds are odd: A statistical test for detecting adversarial examples},
  author={Roth, Kevin and Kilcher, Yannic and Hofmann, Thomas},
  booktitle={International Conference on Machine Learning},
  pages={5498--5507},
  year={2019},
  organization={PMLR}
}

@article{Pope21imagedimensions,
  title={The intrinsic dimension of images and its impact on learning.},
  author={Phillip Pope and Chen Zhu and Ahmed Abdelkader and Micah Goldblum and Tom Goldstein},
  journal={International Conference on Learning Representations},
  year={2021},
  month={Apr}
}


@article{lopespaz17classifier2sample,
  title={ Revisiting classifier two-sample tests.},
  author={David Lopez-Paz, Maxime Oquab.},
  journal={International Conference on Learning Representations},
  year={2017},
  month={Apr}
}

@inproceedings{xiao18ganaugment,
  title     = {Generating Adversarial Examples with Adversarial Networks},
  author    = {Chaowei Xiao and Bo Li and Jun-yan Zhu and Warren He and Mingyan Liu and Dawn Song},
  booktitle = {Proceedings of the Twenty-Seventh International Joint Conference on
               Artificial Intelligence, {IJCAI-18}},
  publisher = {International Joint Conferences on Artificial Intelligence Organization},             
  pages     = {3905--3911},
  year      = {2018},
  month     = {7},
  doi       = {10.24963/ijcai.2018/543},
  url       = {https://doi.org/10.24963/ijcai.2018/543},
}



@article{zhang2018mixup,
title={mixup: Beyond Empirical Risk Minimization},
author={Hongyi Zhang and Moustapha Cisse and Yann N. Dauphin and David Lopez-Paz},
journal={International Conference on Learning Representations},
year={2018},
url={https://openreview.net/forum?id=r1Ddp1-Rb},
}

 @article{Gizewski21radonnikodym,
  title={ On a regularization of unsupervised domain adaptation in RKHS.},
  author={Elke R. Gizewski and Lukas Mayer and Bernhard A. Moser and Duc Huon Nguyen and Sergiy Pereverzyev and Natalia Shepeleva and Werner Zellinger.},
  journal={RICAM-Report 2021-17},
  year={2021},
}
@article{gilmer2018adversarial,
  title={Adversarial spheres},
  author={Gilmer, Justin and Metz, Luke and Faghri, Fartash and Schoenholz, Samuel S and Raghu, Maithra and Wattenberg, Martin and Goodfellow, Ian},
  journal={arXiv preprint arXiv:1801.02774},
  year={2018}
}

@article{raghu2017expressive,
  title={On the expressive power of deep neural networks},
  author={Raghu, Maithra and Poole, Ben and Kleinberg, Jon and Ganguli, Surya and Sohl-Dickstein, Jascha},
  journal={ICML},
  year={2017}
}

@inproceedings{song2018pixeldefend,
  title={Pixeldefend: Leveraging generative models to understand and defend against adversarial examples},
  author={Song, Yang and Kim, Taesup and Nowozin, Sebastian and Ermon, Stefano and Kushman, Nate},
  booktitle={International Conference on Learning Representations},
  year={2018}
}

@article{goujon2024number,
  title={On the number of regions of piecewise linear neural networks},
  author={Goujon, Alexis and Etemadi, Arian and Unser, Michael},
  journal={Journal of Computational and Applied Mathematics},
  volume={441},
  pages={115667},
  year={2024},
  publisher={Elsevier}
}

@article{higgins2018towards,
  title={Towards a definition of disentangled representations},
  author={Higgins, Irina and Amos, David and Pfau, David and Racaniere, Sebastien and Matthey, Loic and Rezende, Danilo and Lerchner, Alexander},
  journal={arXiv preprint arXiv:1812.02230},
  year={2018}
}

@article{Voronoi1908original,
url = {https://doi.org/10.1515/crll.1908.133.97},
title = {Nouvelles applications des paramètres continus à la théorie des formes quadratiques. Premier mémoire. Sur quelques propriétés des formes quadratiques positives parfaites.},
title = {},
author = {Georges Voronoi},
pages = {97--102},
volume = {1908},
number = {133},
journal = {Journal für die reine und angewandte Mathematik (Crelles Journal)},
doi = {doi:10.1515/crll.1908.133.97},
year = {1908},
lastchecked = {2024-12-28}
}



@InProceedings{gopi2013onebitcompressednsensing,
  title = 	 {One-Bit Compressed Sensing: Provable Support and Vector Recovery},
  author = 	 {Gopi, Sivakant and Netrapalli, Praneeth and Jain, Prateek and Nori, Aditya},
  booktitle = 	 {Proceedings of the 30th International Conference on Machine Learning},
  pages = 	 {154--162},
  year = 	 {2013},
  editor = 	 {Dasgupta, Sanjoy and McAllester, David},
  volume = 	 {28},
  number =       {3},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Atlanta, Georgia, USA},
  month = 	 {6},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v28/gopi13.pdf},
  url = 	 {https://proceedings.mlr.press/v28/gopi13.html},
  abstract = 	 {In this paper, we study the problem of one-bit compressed sensing (1-bit CS), where the goal is to design a measurement matrix A and a recovery algorithm s.t. a k-sparse vector \x^* can be efficiently recovered back from signed linear measurements, i.e., b=\sign(A\x^*). This is an important problem in the signal acquisition area and has several learning applications as well, e.g., multi-label classification \citeHsuKLZ10. We study this problem in two settings: a) support recovery: recover \supp(\x^*), b) approximate vector recovery: recover a unit vector \hx s.t. || \hatx-\x^*/||\x^*|| ||_2≤ε. For support recovery, we propose two novel and efficient solutions based on two combinatorial structures: union free family of sets and expanders. In contrast to  existing methods for  support recovery, our methods are universal i.e. a single measurement matrix A can recover almost all the signals. For approximate recovery, we propose the first  method to recover sparse vector using a near optimal number of measurements.  We also empirically demonstrate  effectiveness of our algorithms; we show that our algorithms are able to recover signals with smaller number of measurements than several existing methods. }
}


@ARTICLE{laska2012regimechange,
  author={Laska, Jason N. and Baraniuk, Richard G.},
  journal={IEEE Transactions on Signal Processing}, 
  title={Regime Change: Bit-Depth Versus Measurement-Rate in Compressive Sensing}, 
  year={2012},
  volume={60},
  number={7},
  pages={3496-3505},
  keywords={Noise measurement;Signal to noise ratio;Quantization;Measurement uncertainty;Upper bound;Distortion measurement;Analog-to-digital conversion;compressed sensing;quantization},
  doi={10.1109/TSP.2012.2194710}}


@article{sohl2024boundary,
  title={The boundary of neural network trainability is fractal},
  author={Sohl-Dickstein, Jascha},
  journal={arXiv preprint arXiv:2402.06184},
  year={2024}
}


@inproceedings{Zaslavsky1975FacingUT,
  title={Facing Up to Arrangements: Face-Count Formulas for Partitions of Space by Hyperplanes},
  author={Thomas Zaslavsky},
  year={1975},
  booktitle={American Mathematical Society}
}

@article{goodfellow2014explaining,
  title={Explaining and harnessing adversarial examples},
  author={Goodfellow, Ian J and Shlens, Jonathon and Szegedy, Christian},
  journal={arXiv preprint arXiv:1412.6572},
  year={2014}
}

@article{montufar2021sharp,
  title={Sharp bounds for the number of regions of maxout networks and vertices of Minkowski sums},
  author={Mont{\'u}far, Guido and Ren, Yue and Zhang, Leon},
  journal={arXiv preprint arXiv:2104.08135},
  year={2021}
}

@article{Metz2023,
  author = {Cade Metz},
  title = {OpenAI CEO Sam Altman: The Age of Giant AI Models Is Already Over},
  journal = {Wired},
  year = {2023},
  url = {https://www.wired.com/story/openai-ceo-sam-altman-the-age-of-giant-ai-models-is-already-over/},
  note = {Accessed: 2024-11-01}
}


@inproceedings{wang2019universal,
  title={Universal approximation property and equivalence of stochastic computing-based neural networks and binary neural networks},
  author={Wang, Yanzhi and Zhan, Zheng and Zhao, Liang and Tang, Jian and Wang, Siyue and Li, Jiayu and Yuan, Bo and Wen, Wujie and Lin, Xue},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={33},
  number={01},
  pages={5369--5376},
  year={2019}
}

@article{ma2024era,
  title={The Era of 1-Bit LLMs: All Large Language Models Are in 1.58 Bits},
  author={Ma, Shuming and Wang, Hongyu and Ma, Lingxiao and Wang, Lei and Wang, Wenhui and Huang, Shaohan and Dong, Li and Wang, Ruiping and Xue, Jilong and Wei, Furu},
  year={2024},
  journal={arXiv preprint arXiv:2402.17764},
  doi={10.48550/arXiv.2402.17764},
  url={https://doi.org/10.48550/arXiv.2402.17764},
  eprint={2402.17764},
  archivePrefix={arXiv},
  primaryClass={cs.AI}
}


@article{wang2024bitnet,
  title={1-Bit AI Infra: Part 1.1, Fast and Lossless BitNet B1.58 Inference on CPUs},
  author={Wang, Jinheng and Zhou, Hansong and Song, Ting and Mao, Shaoguang and Ma, Shuming and Wang, Hongyu and Xia, Yan and Wei, Furu},
  year={2024},
  journal={arXiv preprint arXiv:2410.16144},
  url={http://arxiv.org/abs/2410.16144},
  eprint={2410.16144},
  archivePrefix={arXiv},
  primaryClass={cs.AI}
}


@article{BaezBunn2005,
  title={The meaning of Einstein's equation},
  author={Baez, John C. and Bunn, Emory F.},
  journal={American Journal of Physics},
  volume={73},
  number={7},
  pages={644--652},
  year={2005},
  doi={10.1119/1.1852541}
}

@article{Giryes2016DeepNN,
  title={Deep Neural Networks with Random Gaussian Weights: A Universal Classification Strategy?},
  author={Raja Giryes and G. Sapiro and A. Bronstein},
  journal={IEEE Transactions on Signal Processing},
  year={2016},
  volume={64},
  pages={3444-3457}
}

@Article{Conti2018,
  author              = {Conti, Francesco and Schiavone, Pasquale D. and Benini, Luca},
  title               = {XNOR Neural Engine: a Hardware Accelerator IP for 21.6 fJ/op Binary Neural Network Inference},
  journal             = {IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},
  year                = {2018},
  volume              = {37},
  number              = {11},
  pages               = {2940 - 2951},
  issn                = {0278-0070},
  abbrev_source_title = {IEEE trans. comput.-aided des. integr. circuits syst.},
  address             = {Piscataway, NJ},
  copyright           = {In Copyright - Non-Commercial Use Permitted},
  keywords            = {Binary neural networks (BNNs); Hardware accelerator; Microcontroller system},
  language            = {en},
  publisher           = {IEEE},
  size                = {11 p.},
}


@inproceedings{Meloni2019,
author = {Meloni, Paolo and Loi, Daniela and Busia, Paola and Deriu, Gianfranco and Pimentel, Andy D. and Sapra, Dolly and Stefanov, Todor and Minakova, Svetlana and Conti, Francesco and Benini, Luca and Pintor, Maura and Biggio, Battista and Moser, Bernhard and Shepeleva, Natalia and Fragoulis, Nikos and Theodorakopoulos, Ilias and Masin, Michael and Palumbo, Francesca},
title = {Optimization and Deployment of CNNs at the Edge: The ALOHA Experience},
year = {2019},
isbn = {9781450366854},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Deep learning (DL) algorithms have already proved their effectiveness on a wide variety of application domains, including speech recognition, natural language processing, and image classification. To foster their pervasive adoption in applications where low latency, privacy issues and data bandwidth are paramount, the current trend is to perform inference tasks at the edge. This requires deployment of DL algorithms on low-energy and resource-constrained computing nodes, often heterogenous and parallel, that are usually more complex to program and to manage without adequate support and experience. In this paper, we present ALOHA, an integrated tool flow that tries to facilitate the design of DL applications and their porting on embedded heterogenous architectures. The proposed tool flow aims at automating different design steps and reducing development costs. ALOHA considers hardware-related variables and security, power efficiency, and adaptivity aspects during the whole development process, from pre-training hyperparameter optimization and algorithm configuration to deployment.},
booktitle = {Proceedings of the 16th ACM International Conference on Computing Frontiers},
pages = {326–332},
numpages = {7},
keywords = {hardware accelerators, convolution neural networks, FPGAs},
location = {Alghero, Italy},
series = {CF '19}
}

@article{balestriero2019geometry,
  title={The geometry of deep networks: Power diagram subdivision},
  author={Balestriero, Randall and Cosentino, Romain and Aazhang, Behnaam and Baraniuk, Richard},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  pages={15832--15841},
  year={2019}
}

@inproceedings{pennington2014glove,
  author = {Jeffrey Pennington and Richard Socher and Christopher D. Manning},
  booktitle = {Empirical Methods in Natural Language Processing (EMNLP)},
  title = {GloVe: Global Vectors for Word Representation},
  year = {2014},
  pages = {1532--1543},
  url = {http://www.aclweb.org/anthology/D14-1162},
}


@InProceedings{menon16linkinglossesfordensityratioestandclassprobabilityest,
  title = 	 {Linking losses for density ratio and class-probability estimation},
  author = 	 {Menon, Aditya and Ong, Cheng Soon},
  booktitle = 	 {Proceedings of The 33rd International Conference on Machine Learning},
  pages = 	 {304--313},
  year = 	 {2016},
  volume = 	 {48},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {New York, New York, USA},
  month = 	 {20--22 Jun},
  pdf = 	 {http://proceedings.mlr.press/v48/menon16.pdf},
  url = 	 {https://proceedings.mlr.press/v48/menon16.html},
  abstract = 	 {Given samples from two densities p and q, density ratio estimation (DRE) is the problem of estimating the ratio p/q. Two popular discriminative approaches to DRE are KL importance estimation (KLIEP), and least squares importance fitting (LSIF). In this paper, we show that KLIEP and LSIF both employ class-probability estimation (CPE) losses. Motivated by this, we formally relate DRE and CPE, and demonstrate the viability of using existing losses from one problem for the other. For the DRE problem, we show that essentially any CPE loss (eg logistic, exponential) can be used, as this equivalently minimises a Bregman divergence to the true density ratio. We show how different losses focus on accurately modelling different ranges of the density ratio, and use this to design new CPE losses for DRE. For the CPE problem, we argue that the LSIF loss is useful in the regime where one wishes to rank instances with maximal accuracy at the head of the ranking. In the course of our analysis, we establish a Bregman divergence identity that may be of independent interest.}
}


@article{Hinton06,
author = {Hinton, Geoffrey E. and Osindero, Simon and Teh, Yee Whye},
journal = {Neural Computation},
pages = {1527--1554},
title = {A Fast Learning Algorithm for Deep Belief Nets},
volume = {18},
year = {2006}
}

@techreport{Krizhevsky09learningmultiple,
  added-at = {2021-01-21T03:01:11.000+0100},
  author = {Krizhevsky, Alex},
  keywords = {},
  pages = {32--33},
  title = {Learning Multiple Layers of Features from Tiny Images},
  year = {2009}
}


@article{MNIST,
    title={Gradient-based learning applied to document recognition},
    author={LeCun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},
    journal={Proceedings of the IEEE},
    volume={86},
    number={11},
    pages={2278--2324},
    year={1998}
}


@book{Bishop-2006,
    title={Pattern recognition and machine learning},
    author={Bishop, C. M.},
    publisher={Springer},
    year={2006}
}
@book{Sutton-Barto-2018,
    title={Reinforcement learning: An introduction},
    author={Sutton, R. S. and Barto, A. G.},
    publisher={MIT press},
    year={2018}
}
@misc{xiao2017fashionmnist,
      title={Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms}, 
      author={Han Xiao and Kashif Rasul and Roland Vollgraf},
      year={2017},
      eprint={1708.07747},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}



@article{lewandowski2023densityratio,
  title={Density Ratio Estimation with the ReLU-based tessellation},
  author={Michal Lewandowski and Bernhard Moser},
  journal={EPIA Student Symposium},
  year={2021}
}


@article{lewandowski2021discriminability,
  title={On the discriminability of samples using binarized ReLU activations},
  author={Michal Lewandowski and Werner Zellinger and Hamid Eghbal-zadeh and Natalia Shepeleva and Bernhard Moser},
  journal={ICDSMLA},
  year={2021}
}


@article{Frechet57,
author = {M. Maurice Frechet},
journal = {C.R. Acad. Sci. Paris},
pages = {689--692},
title = {Sur la distance de deux lois de probabilit\'e},
volume = {244},
year = {1957}
}

@misc{sturua2024jinaembeddingsv3multilingualembeddingstask,
      title={jina-embeddings-v3: Multilingual Embeddings With Task LoRA}, 
      author={Saba Sturua and Isabelle Mohr and Mohammad Kalim Akram and Michael Günther and Bo Wang and Markus Krimmel and Feng Wang and Georgios Mastrapas and Andreas Koukounas and Andreas Koukounas and Nan Wang and Han Xiao},
      year={2024},
      eprint={2409.10173},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
}

@InProceedings{roy2003convexitydiscrete,
author="Roy, Anthony J.
and Stell, John G.",
editor="Kuhn, Walter
and Worboys, Michael F.
and Timpf, Sabine",
title="Convexity in Discrete Space",
booktitle="Spatial Information Theory. Foundations of Geographic Information Science",
year="2003",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="253--269",
abstract="This paper looks at Coppel's axioms for convexity, and shows how they can be applied to discrete spaces. Two structures for a discrete geometry are considered: oriented matroids, and cell complexes. Oriented matroids are shown to have a structure which naturally satisfies the axioms for being a convex geometry. Cell complexes are shown to give rise to various different notions of convexity, one of which satisfies the convexity axioms, but the others also provide valid notions of convexity in particular contexts. Finally, algorithms are investigated to validate the sets of a matroid, and to compute the convex hull of a subset of an oriented matroid.",
isbn="978-3-540-39923-0"
}



@book{Boissonnat_Yvinec_1998, 
place={Cambridge}, 
title={Algorithmic Geometry},
publisher={Cambridge University Press}, 
author={Boissonnat, Jean-Daniel and Yvinec, Mariette}, 
year={1998}
} 

@InProceedings{marshall2018origami,
author="Bern, Marshall
and Hayes, Barry",
editor="Laber, Eduardo Sany
and Bornstein, Claudson
and Nogueira, Loana Tito
and Faria, Luerbio",
title="Origami Embedding of Piecewise-Linear Two-Manifolds",
booktitle="LATIN 2008: Theoretical Informatics",
year="2008",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="617--629",
abstract="We show that any compact, orientable, piecewise-linear two-manifold with Euclidean metric can be realized as a flat origami, meaning a set of non-crossing polygons in Euclidean 2-space ``plus layers''. This result implies a weak form of a theorem of Burago and Zalgaller: any orientable, piecewise-linear two-manifold can be embedded into Euclidean 3-space ``nearly'' isometrically. We also correct a mistake in our previously published construction for cutting any polygon out of a folded sheet of paper with one straight cut.",
isbn="978-3-540-78773-0"
}



@InProceedings{demaine1998folding_and_cutting,
author="Demaine, Erik D.
and Demaine, Martin L.
and Lubiw, Anna",
editor="Akiyama, Jin
and Kano, Mikio
and Urabe, Masatsugu",
title="Folding and Cutting Paper",
booktitle="Discrete and Computational Geometry",
year="2000",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="104--118",
abstract="We present an algorithm to find a flat folding of a piece of paper, so that one complete straight cut on the folding creates any desired plane graph of cuts. The folds are based on the straight skeleton, which lines up the desired edges by folding along various bisectors; and a collection of perpendiculars that make the crease pattern foldable. We prove that the crease pattern is flat foldable by demonstrating a family of folded states with the desired properties.",
isbn="978-3-540-46515-7"
}



@inproceedings{poole2016exponential,
 author = {Poole, Ben and Lahiri, Subhaneil and Raghu, Maithra and Sohl-Dickstein, Jascha and Ganguli, Surya},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Lee and M. Sugiyama and U. Luxburg and I. Guyon and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Exponential expressivity in deep neural networks through transient chaos},
 url = {https://proceedings.neurips.cc/paper_files/paper/2016/file/148510031349642de5ca0c544f31b2ef-Paper.pdf},
 volume = {29},
 year = {2016}
}


@article{balestriero2024geometry,
  title={On the Geometry of Deep Learning},
  author={Balestriero, Randall and Humayun, Ahmed Imtiaz and Baraniuk, Richard},
  journal={arXiv preprint arXiv:2408.04809},
  year={2024}
}


@book{christian2020alignment,
  title={The Alignment Problem: Machine Learning and Human Values},
  author={Christian, Brian},
  year={2020},
  publisher={W. W. Norton \& Company},
  edition={1st}
}

@article{deci1999meta,
  title={A meta-analytic review of experiments examining the effects of extrinsic rewards on intrinsic motivation},
  author={Deci, Edward L and Koestner, Richard and Ryan, Richard M},
  journal={Psychological Bulletin},
  volume={125},
  number={6},
  pages={627--668},
  year={1999},
  publisher={American Psychological Association}
}


@article{przybylski2010motivational,
  title={A motivational model of video game engagement},
  author={Przybylski, Andrew K and Rigby, Scott and Ryan, Richard M},
  journal={Review of General Psychology},
  volume={14},
  number={2},
  pages={154--166},
  year={2010},
  publisher={American Psychological Association}
}

@article{Bringsjord2001CreativityTT,
  title={Creativity, the Turing Test, and the (Better) Lovelace Test},
  author={Selmer Bringsjord and Paul Bello and David Ferrucci},
  journal={Minds and Machines},
  year={2001},
  volume={11},
  pages={3-27},
  publisher={Kluwer Academic Publishers}
}


@article{kurzban2001social,
  title={The social psychophysics of cooperation: Nonverbal communication in collective action},
  author={Kurzban, Robert},
  journal={Journal of Nonverbal Behavior},
  volume={25},
  pages={241–259},
  year={2001},
  publisher={Human Science Press, Inc.}
}

@incollection{Tajfel1979Integrative,
  title={An integrative theory of intergroup conflict},
  author={Tajfel, Henri and Turner, John C.},
  booktitle={The Social Psychology of Intergroup Relations},
  pages={33--47},
  year={1979},
  publisher={Brooks/Cole Publishing Company}
}


@article{amodei2016concrete,
  title={Concrete problems in AI safety},
  author={Amodei, Dario and Olah, Chris and Steinhardt, Jacob and Christiano, Paul and Schulman, John and Man{\'e}, Dan},
  journal={arXiv preprint arXiv:1606.06565},
  year={2016}
}

@article{Frith2006,
  title     = {The Neural Basis of Mentalizing},
  author    = {Frith, Chris D. and Frith, Uta},
  journal   = {Neuron},
  volume    = {50},
  number    = {4},
  pages     = {531--534},
  year      = {2006},
  publisher = {Elsevier}
}

@article{Stanovich2000,
  title     = {Individual Differences in Reasoning: Implications for the Rationality Debate?},
  author    = {Stanovich, Keith E. and West, Richard F.},
  journal   = {Behavioral and Brain Sciences},
  volume    = {23},
  number    = {5},
  pages     = {645--665},
  year      = {2000},
  publisher = {Cambridge University Press}
}


@article{yu2024distilling,
  title={Distilling system 2 into system 1},
  author={Yu, Ping and Xu, Jing and Weston, Jason and Kulikov, Ilia},
  journal={arXiv preprint arXiv:2407.06023},
  year={2024}
}


@inproceedings{goodfellow2014generative,
      title={Generative Adversarial Networks}, 
      author={Ian J. Goodfellow and Jean Pouget-Abadie and Mehdi Mirza and Bing Xu and David Warde-Farley and Sherjil Ozair and Aaron Courville and Yoshua Bengio},
      year={2014},
      booktitle = {Advances in Neural Information Processing Systems},
}

@inproceedings{Foote1999,
  author    = {Jonathan Foote},
  title     = {Visualizing Music and Audio Using Self-Similarity},
  booktitle = {Proceedings of the Seventh ACM International Conference on Multimedia (Part 1)},
  pages     = {77--80},
  year      = {1999},
  doi       = {10.1145/319463.319472},
}


@article{crescenzi1998complexity,
  title     = {On the Complexity of Protein Folding},
  author    = {Crescenzi, Pierluigi and Goldman, Daniel and Papadimitriou, Christos and Piccolboni, Alberto and Yannakakis, Mihalis},
  journal   = {Journal of Computational Biology},
  volume    = {5},
  number    = {3},
  pages     = {423--465},
  year      = {1998},
  publisher = {Mary Ann Liebert, Inc.},
  doi       = {10.1089/cmb.1998.5.423},
}

@inproceedings{bern1996complexity,
  author    = {Marshall Bern and
               Barry Hayes},
  title     = {The Complexity of Flat Origami},
  booktitle = {Proceedings of the Seventh Annual ACM-SIAM Symposium on Discrete Algorithms},
  pages     = {175--183},
  year      = {1996},
  publisher = {Society for Industrial and Applied Mathematics},
  address   = {Philadelphia, PA, USA},
  isbn      = {0-89871-366-8},
}

@incollection{demaine2005survey,
  author    = {Erik D. Demaine and
               Martin L. Demaine},
  title     = {A Survey of Folding and Unfolding in Computational Geometry},
  booktitle = {Combinatorial and Computational Geometry},
  editor    = {Jacob E. Goodman and
               J{\'a}nos Pach and
               Emo Welzl},
  series    = {Mathematical Sciences Research Institute Publications},
  volume    = {52},
  pages     = {167--211},
  publisher = {Cambridge University Press},
  year      = {2005},
  address   = {Cambridge, UK},
  isbn      = {9780521848620},
}

@inproceedings{AlemanFlores2004,
  author    = {Miguel Alemán-Flores and Luis Álvarez-León},
  title     = {Video Segmentation through Multiscale Texture Analysis},
  booktitle = {Image Analysis and Recognition, ICIAR 2004, Lecture Notes in Computer Science},
  editor    = {Aurélio Campilho and Mohamed Kamel},
  volume    = {3212},
  pages     = {339--346},
  year      = {2004},
  publisher = {Springer, Berlin, Heidelberg}
}


@article{Wang2020,
  author    = {Xiang Wang and Kai Wang and Shiguo Lian},
  title     = {A survey on face data augmentation for the training of deep neural networks},
  journal   = {Neural Computing and Applications},
  year      = {2020},
  volume    = {32},
  number    = {19},
  pages     = {15503--15531},
  doi       = {10.1007/s00521-020-04748-3},
  issn      = {1433-3058},
  month     = {10},
  abstract  = {The quality and size of training set have a great impact on the results of deep learning-based face-related tasks. However, collecting and labeling adequate samples with high-quality and balanced distributions still remains a laborious and expensive work, and various data augmentation techniques have thus been widely used to enrich the training dataset. In this paper, we review the existing works of face data augmentation from the perspectives of the transformation types and methods, with the state-of-the-art approaches involved. Among all these approaches, we put the emphasis on the deep learning-based works, especially the generative adversarial networks which have been recognized as more powerful and effective tools in recent years. We present their principles, discuss the results and show their applications as well as limitations. Different evaluation metrics for evaluating these approaches are also introduced. We point out the challenges and opportunities in the field of face data augmentation and provide brief yet insightful discussions.}
}


@article{plan2014dimension,
  title={Dimension reduction by random hyperplane tessellations},
  author={Plan, Yaniv and Vershynin, Roman},
  journal={Discrete \& Computational Geometry},
  volume={51},
  number={2},
  pages={438--461},
  year={2014},
  publisher={Springer}
}


@article{trier1995evaluation,
  title={Evaluation of binarization methods for document images},
  author={Trier, Oeivind Due and Taxt, Torfinn},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={17},
  number={3},
  pages={312--315},
  year={1995},
  publisher={IEEE}
}

@inproceedings{bar2014classification,
  title={Classification of artistic styles using binarized features derived from a deep neural network},
  author={Bar, Yaniv and Levy, Noga and Wolf, Lior},
  booktitle={European conference on computer vision (ECCV)},
  pages={71--84},
  year={2014},
  organization={Springer}
}

@inproceedings{ding2019regularizing,
  title={Regularizing activation distribution for training binarized deep networks},
  author={Ding, Ruizhou and Chin, Ting-Wu and Liu, Zeye and Marculescu, Diana},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  pages={11408--11417},
  year={2019}
}


@article{yarotsky2017error,
  title={Error bounds for approximations with deep ReLU networks},
  author={Yarotsky, Dmitry},
  journal={Neural Networks},
  volume={94},
  pages={103--114},
  year={2017},
  publisher={Elsevier}
}

@article{wouter18intro,
  author    = {Wouter M. Kouw},
  title     = {An introduction to domain adaptation and transfer learning},
  journal   = {CoRR},
  volume    = {abs/1812.11806},
  year      = {2018},
  archivePrefix = {arXiv},
  eprint    = {1812.11806},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{Neyshabur2018overparametrization,
      title={The role of over-parametrization in generalization of neural networks}, journal={ICLR},
      author={Behnam Neyshabur and Zhiyuan Li and Srinadh Bhojanapalli and Yann LeCun and Nathan Srebro},
      year={2018}
      }



@article{Mhaskar17deepervsshallow,
author = {Mhaskar, Hrushikesh and Liao, Qianli and Poggio, Tomaso},
title = {When and Why Are Deep Networks Better than Shallow Ones?},
year = {2017},
abstract = {While the universal approximation property holds both for hierarchical and shallow networks, deep networks can approximate the class of compositional functions as well as shallow networks but with exponentially lower number of training parameters and sample complexity. Compositional functions are obtained as a hierarchy of local constituent functions, where "local functions" are functions with low dimensionality. This theorem proves an old conjecture by Bengio on the role of depth in networks, characterizing precisely the conditions under which it holds. It also suggests possible answers to the the puzzle of why high-dimensional deep networks trained on large training sets often do not seem to show overfit.},
journal = {AAAI},
pages = {2343–2349},
numpages = {7},
location = {San Francisco, California, USA},
year={2018}
}


@InProceedings{eldan16powerofdepth, title = {The Power of Depth for Feedforward Neural Networks}, author = {Ronen Eldan and Ohad Shamir}, booktitle = {29th Annual Conference on Learning Theory}, pages = {907--940}, year = {2016}, editor = {Vitaly Feldman and Alexander Rakhlin and Ohad Shamir}, volume = {49}, series = {Proceedings of Machine Learning Research}, address = {Columbia University, New York, New York, USA}, month = {23--26 Jun}, publisher = {PMLR}, pdf = {http://proceedings.mlr.press/v49/eldan16.pdf}, url = {http://proceedings.mlr.press/v49/eldan16.html}, abstract = {We show that there is a simple (approximately radial) function on \mathbbR^d, expressible by a small 3-layer feedforward neural networks, which cannot be approximated by any 2-layer network, to more than a certain constant accuracy, unless its width is exponential in the dimension. The result holds for virtually all known activation functions, including rectified linear units, sigmoids and thresholds, and formally demonstrates that depth – even if increased by 1 – can be exponentially more valuable than width for standard feedforward neural networks. Moreover, compared to related results in the context of Boolean functions, our result requires fewer assumptions, and the proof techniques and construction are very different. } }


@article{telgarsky2015representation,
  title={Representation benefits of deep feedforward networks},
  author={Telgarsky, Matus},
  journal={arXiv preprint arXiv:1509.08101},
  year={2015}
}

@InProceedings{telgarsky16benefitsofdepth, 
title = {Benefits of depth in neural networks}, 
author = {Matus Telgarsky}, 
booktitle = {29th Annual Conference on Learning Theory}, 
pages = {1517--1539}, 
year = {2016}, 
editor = {Vitaly Feldman and Alexander Rakhlin and Ohad Shamir}, 
volume = {49}, 
series = {Proceedings of Machine Learning Research}, 
address = {Columbia University, New York, New York, USA}, month = {23--26 Jun}, publisher = {PMLR}, 
pdf = {http://proceedings.mlr.press/v49/telgarsky16.pdf}, 
url = {http://proceedings.mlr.press/v49/telgarsky16.html}, 
abstract = {For any positive integer k, there exist neural networks with Θ(k^3) layers, Θ(1) nodes per layer, and Θ(1) distinct parameters which can not be approximated by networks with O(k) layers unless they are exponentially large — they must possess Ω(2^k) nodes. This result is proved here for a class of nodes termed \emphsemi-algebraic gates which includes the common choices of ReLU, maximum, indicator, and piecewise polynomial functions, therefore establishing benefits of depth against not just standard networks with ReLU gates, but also convolutional networks with ReLU and maximization gates, sum-product networks, and boosted decision trees (in this last case with a stronger separation: Ω(2^k^3) total tree nodes are required). } }

@article{szymanski2014,
  author={Szymanski, Lech and McCane, Brendan},
  journal={IEEE Transactions on Neural Networks and Learning Systems}, 
  title={Deep Networks are Effective Encoders of Periodicity}, 
  year={2014},
  volume={25},
  number={10},
  pages={1816-1827},
  doi={10.1109/TNNLS.2013.2296046}}

@inproceedings{ioffe2015batch,
  title={Batch normalization: Accelerating deep network training by reducing internal covariate shift},
  author={Ioffe, Sergey and Szegedy, Christian},
  booktitle={International conference on machine learning},
  pages={448--456},
  year={2015},
  organization={PMLR}
}

@misc{simonyan2015deep,
      title={Very Deep Convolutional Networks for Large-Scale Image Recognition}, 
      author={Karen Simonyan and Andrew Zisserman},
      year={2015},
      eprint={1409.1556},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@book{mandelbrot1983fractal,
  title={The Fractal Geometry of Nature},
  author={Mandelbrot, Benoit B.},
  year={1983},
  publisher={Macmillan},
  isbn={978-0-7167-1186-5}
}



@InProceedings{vardi23implicit,
  title = 	 {Implicit Regularization Towards Rank Minimization in ReLU Networks},
  author =       {Timor, Nadav and Vardi, Gal and Shamir, Ohad},
  booktitle = 	 {Proceedings of The 34th International Conference on Algorithmic Learning Theory},
  pages = 	 {1429--1459},
  year = 	 {2023},
  editor = 	 {Agrawal, Shipra and Orabona, Francesco},
  volume = 	 {201},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {2},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v201/timor23a/timor23a.pdf},
  url = 	 {https://proceedings.mlr.press/v201/timor23a.html},
  abstract = 	 {We study the conjectured relationship between the implicit&nbsp;regularization in neural&nbsp;networks, trained with gradient-based methods, and rank&nbsp;minimization of their weight&nbsp;matrices. Previously, it was proved that for linear&nbsp;networks (of depth&nbsp;$2$ and vector-valued&nbsp;outputs), gradient&nbsp;flow&nbsp;(GF) w.r.t. the square&nbsp;loss acts as a rank&nbsp;minimization&nbsp;heuristic. However, understanding to what extent this generalizes to nonlinear&nbsp;networks is an open&nbsp;problem. In this paper, we focus on nonlinear&nbsp;ReLU&nbsp;networks, providing several new positive and negative results. On the negative side, we prove (and demonstrate empirically) that, unlike the linear&nbsp;case, GF on ReLU&nbsp;networks may no&nbsp;longer tend to minimize&nbsp;ranks, in a rather&nbsp;strong sense (even approximately, for “most” datasets of size&nbsp;$2$). On the positive side, we reveal that ReLU&nbsp;networks of sufficient&nbsp;depth are provably biased towards low-rank&nbsp;solutions in several reasonable settings.}
}

@inproceedings{vardi2021implicit,
  title={Implicit regularization in relu networks with the square loss},
  author={Vardi, Gal and Shamir, Ohad},
  booktitle={Conference on Learning Theory},
  pages={4224--4258},
  year={2021},
  organization={PMLR}
}

@article{dropout,
  author  = {Nitish Srivastava and Geoffrey Hinton and Alex Krizhevsky and Ilya Sutskever and Ruslan Salakhutdinov},
  title   = {Dropout: A Simple Way to Prevent Neural Networks from Overfitting},
  journal = {Journal of Machine Learning Research},
  year    = {2014},
  volume  = {15},
  number  = {56},
  pages   = {1929-1958},
  url     = {http://jmlr.org/papers/v15/srivastava14a.html}
}

@inproceedings{pascanu14responeregions,
 author = {Pascanu, Razvan  and  Mont{\'u}far, Guido and Bengio, Yoshua},
 booktitle = {International Conference on Learning
Representations},
 title = { On the number of response regions of deep
feed forward networks with piece-wise linear activations},
 year = {2014}
}

@inproceedings{montufar2014number,
 author = {Mont{\'u}far, Guido F and Pascanu, Razvan and Cho, Kyunghyun and Bengio, Yoshua},
 booktitle = {NeurIPS},
 pages = {},
 title = {On the Number of Linear Regions of Deep Neural Networks},
 volume = {27},
 year = {2014}
}




@article{zellinger2020generalization,
  title={On generalization in moment-based domain adaptation},
  author={Zellinger, Werner and Moser, Bernhard A and Saminger-Platz, Susanne},
  journal={Annals of Mathematics and Artificial Intelligence},
  year={2021},
  issue={89},
  pages={333--369},
  publisher={Springer}
}

@article{Moser2014TheRO,
  title={The Range of a Simple Random Walk on Z: An Elementary Combinatorial Approach},
  author={Bernhard Alois Moser},
  journal={Electron. J. Comb.},
  year={2014},
  volume={21},
  pages={4}
}


@book{goodfellow2016deep,
title={Deep learning},
author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron and Bengio, Yoshua},
volume={1},
year={2016},
publisher={MIT Press}
}

@article{barron1994approximation,
  title={Approximation and estimation bounds for artificial neural networks},
  author={Barron, Andrew R},
  journal={Machine learning},
  volume={14},
  pages={115--133},
  year={1994},
  publisher={Springer}
}

@book{Alberts2015,
  author    = {Alberts, Bruce and Johnson, Alexander and Lewis, Julian and Raff, Martin and Roberts, Keith and Walter, Peter},
  title     = {Molecular Biology of the Cell},
  edition   = {6th},
  year      = {2015},
  publisher = {Garland Science},
}

@book{Chiles2012,
  author    = {Chil{\`e}s, Jean-Paul and Delfiner, Pierre},
  title     = {Geostatistics: Modeling Spatial Uncertainty},
  edition   = {2nd},
  year      = {2012},
  publisher = {Wiley},
}

@incollection{Kendall1989,
  author    = {Kendall, Wilfrid S.},
  title     = {Stochastic Geometry},
  booktitle = {Probability, Statistics and Mathematics: Papers in Honor of Samuel Karlin},
  editor    = {Heyde, Christopher C. and Seneta, Eugene and Gross, B.},
  pages     = {187--200},
  year      = {1989},
  publisher = {Academic Press},
}

@book{Kittel2004,
  author    = {Kittel, Charles},
  title     = {Introduction to Solid State Physics},
  edition   = {8th},
  year      = {2004},
  publisher = {Wiley},
}

@book{Stoyan1995,
  author    = {Stoyan, Dietrich and Kendall, Wilfrid S. and Mecke, Joseph},
  title     = {Stochastic Geometry and Its Applications},
  edition   = {2nd},
  year      = {1995},
  publisher = {Wiley},
}

@book{Turner2001,
  author    = {Turner, Monica G. and Gardner, Robert H. and O'Neill, Robert V.},
  title     = {Landscape Ecology in Theory and Practice: Pattern and Process},
  year      = {2001},
  publisher = {Springer},
}


@inproceedings{wilson2017marginal,
  title={The marginal value of adaptive gradient methods in machine learning},
  author={Wilson, Ashia C. and Roelofs, Rebecca and Stern, Mitchell and Srebro, Nati and Recht, Benjamin},
  booktitle={Advances in Neural Information Processing Systems},
  pages={4148--4158},
  year={2017},
  url={https://proceedings.neurips.cc/paper/2017/file/20a0c9a899f6ecd6e7160f2cf97890c2-Paper.pdf}
}

@article{shorten2019survey,
  title={A Survey on Image Data Augmentation for Deep Learning},
  author={Shorten, Connor and Khoshgoftaar, Taghi M.},
  journal={Journal of Big Data},
  volume={6},
  number={1},
  pages={60},
  year={2019},
  publisher={Springer},
  doi={10.1186/s40537-019-0197-0},
  url={https://doi.org/10.1186/s40537-019-0197-0}
}

@article{funahashi1989approximate,
  title={On the approximate realization of continuous mappings by neural networks},
  author={Funahashi, Ken-Ichi},
  journal={Neural networks},
  volume={2},
  number={3},
  pages={183--192},
  year={1989},
  publisher={Elsevier}
}

@article{shepeleva2020relu,
      title={ReLU Code Space: A Basis for Rating Network Quality Besides Accuracy}, journal={ICLR, NAS workshop},
      author={Natalia Shepeleva and Werner Zellinger and Michal Lewandowski and Bernhard Moser},
      year={2020},
      eprint={2005.09903},
      archivePrefix={arXiv}
      }

@misc{sriperumbudur2009integral,
      title={On integral probability metrics, $\phi$-divergences and binary classification}, 
      author={Bharath K. Sriperumbudur and Kenji Fukumizu and Arthur Gretton and Bernhard Schölkopf and Gert R. G. Lanckriet},
      year={2009},
      eprint={0901.2698},
      archivePrefix={arXiv},
      primaryClass={cs.IT}
}


@InProceedings{yang2018Goodness-of-Fit, 
title = {Goodness-of-Fit Testing for Discrete Distributions via Stein Discrepancy}, 
author = {Yang, Jiasen and Liu, Qiang and Rao, Vinayak and Neville, Jennifer}, 
booktitle = {Proceedings of the 35th International Conference on Machine Learning}, 
pages = {5561--5570}, 
year = {2018}, 
editor = {Jennifer Dy and Andreas Krause}, 
volume = {80}, 
series = {Proceedings of Machine Learning Research}, 
address = {Stockholmsmässan, Stockholm Sweden}, 
month = {7}, 
pdf = {http://proceedings.mlr.press/v80/yang18c/yang18c.pdf}, 
abstract = {Recent work has combined Stein’s method with reproducing kernel Hilbert space theory to develop nonparametric goodness-of-fit tests for un-normalized probability distributions. However, the currently available tests apply exclusively to distributions with smooth density functions. In this work, we introduce a kernelized Stein discrepancy measure for discrete spaces, and develop a nonparametric goodness-of-fit test for discrete distributions with intractable normalization constants. Furthermore, we propose a general characterization of Stein operators that encompasses both discrete and continuous distributions, providing a recipe for constructing new Stein operators. We apply the proposed goodness-of-fit test to three statistical models involving discrete distributions, and our experiments show that the proposed test typically outperforms a two-sample test based on the maximum mean discrepancy.} }


@InProceedings{sugiyama10a_cde_density_ration, 
title = {Conditional Density Estimation via Least-Squares Density Ratio Estimation}, 
author = {Sugiyama, Masashi and Takeuchi, Ichiro and Suzuki, Taiji and Kanamori, Takafumi and Hachiya, Hirotaka and Okanohara, Daisuke}, 
booktitle = {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics}, 
pages = {781--788}, 
year = {2010}, 
volume = {9}, 
series = {Proceedings of Machine Learning Research}, 
address = {Chia Laguna Resort, Sardinia, Italy}, 
month = {5}, 
abstract = {Estimating the conditional mean of an input-output relation is the goal of regression. However, regression analysis is not sufficiently informative if the conditional distribution has multi-modality, is highly asymmetric, or contains heteroscedastic noise. In such scenarios, estimating the conditional distribution itself would be more useful. In this paper, we propose a novel method of conditional density estimation that is suitable for multi-dimensional continuous variables. The basic idea of the proposed method is to express the conditional density in terms of the density ratio and the ratio is directly estimated without going through density estimation. Experiments using benchmark and robot transition datasets illustrate the usefulness of the proposed approach.} }

@article{hido2011statistialoutlierdetection,
 author = {Hido, Shohei and Tsuboi, Yuta and Kashima, Hisashi and Sugiyama, Masashi and Kanamori, Takafumi},
 journal = {Knowledge and Information Systems},
 keywords = {},
 number = {2},
 pages = {309-336},
 title = {Statistical outlier detection using direct density ratio estimation},
 volume = {26},
 year = {2011}
}

@inproceedings{chen2020simple,
  title={A simple framework for contrastive learning of visual representations},
  author={Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
  booktitle={ICML},
  pages={1597--1607},
  year={2020},
  organization={PMLR}
}

@article{kohonen1990self,
  title={The Self-Organizing Map},
  author={Kohonen, Teuvo},
  journal={IEEE},
  volume={78},
  number={9},
  pages={1464--1480},
  year={1990},
  publisher={IEEE},
  doi={10.1109/5.58325}
}



@inproceedings{kingma2014auto,
  title={Auto-Encoding Variational Bayes},
  author={Kingma, Diederik P and Welling, Max},
  booktitle={ICLR},
  year={2014},
  url={https://arxiv.org/abs/1312.6114}
}


@article{balestriero2022batch,
  title={Batch normalization explained},
  author={Balestriero, Randall and Baraniuk, Richard G},
  journal={arXiv preprint arXiv:2209.14778},
  year={2022}
}

@article{humayun2024deep,
  title={Deep Networks Always Grok and Here is Why},
  author={Humayun, Ahmed Imtiaz and Balestriero, Randall and Baraniuk, Richard},
  journal={arXiv preprint arXiv:2402.15555},
  year={2024}
}

@inproceedings{Sugiyama07KLIEP,
author = {Sugiyama, Masashi and Nakajima, Shinichi and Kashima, Hisashi and B\"{u}nau, Paul von and Kawanabe, Motoaki},
title = {Direct Importance Estimation with Model Selection and Its Application to Covariate Shift Adaptation},
year = {2007},
isbn = {9781605603520},
address = {Red Hook, NY, USA},
abstract = {A situation where training and test samples follow different input distributions is
called covariate shift. Under covariate shift, standard learning methods such as maximum
likelihood estimation are no longer consistent—weighted variants according to the
ratio of test and training input densities are consistent. Therefore, accurately estimating
the density ratio, called the importance, is one of the key issues in covariate shift
adaptation. A naive approach to this task is to first estimate training and test input
densities separately and then estimate the importance by taking the ratio of the estimated
densities. However, this naive approach tends to perform poorly since density estimation
is a hard task particularly in high dimensional cases. In this paper, we propose a
direct importance estimation method that does not involve density estimation. Our
method is equipped with a natural cross validation procedure and hence tuning parameters
such as the kernel width can be objectively optimized. Simulations illustrate the
usefulness of our approach.},
booktitle = {Proceedings of the 20th International Conference on Neural Information Processing Systems},
pages = {1433–1440},
numpages = {8},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'07}
}

@INPROCEEDINGS{hido2008inlieroutlier,
  author={Hido, Shohei and Tsuboi, Yuta and Kashima, Hisashi and Sugiyama, Masashi and Kanamori, Takafumi},
  booktitle={2008 Eighth IEEE International Conference on Data Mining}, 
  title={Inlier-Based Outlier Detection via Direct Density Ratio Estimation},
  year={2008},
  volume={},
  number={},
  pages={223-232},
  doi={10.1109/ICDM.2008.49}}

@article{Pearson1900,
author = {Karl Pearson   F.R.S. },
title = {On the criterion that a given system of deviations from the probable in the case of a correlated system of variables is such that it can be reasonably supposed to have arisen from random sampling},
journal = {The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science},
volume = {50},
number = {302},
pages = {157-175},
year  = {1900},
publisher = {Taylor & Francis},
doi = {10.1080/14786440009463897},
URL = {https://doi.org/10.1080/14786440009463897},
eprint = {https://doi.org/10.1080/14786440009463897}
}

@inproceedings{phuong2020functional,
  title={Functional vs. Parametric Equivalence of ReLU Networks},
  author={Phuong, Mary and Lampert, Christoph H.},
  booktitle={International Conference on Learning Representations},
  year={2020}
}

@InProceedings{grigsby23hiddensymmetries,
  title = 	 {Hidden Symmetries of {R}e{LU} Networks},
  author =       {Grigsby, Elisenda and Lindsey, Kathryn and Rolnick, David},
  booktitle = 	 {International Conference on Machine Learning},
  year = 	 {2023},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v202/grigsby23a/grigsby23a.pdf},
  url = 	 {https://proceedings.mlr.press/v202/grigsby23a.html},
  abstract = 	 {The parameter space for any fixed architecture of feedforward ReLU neural networks serves as a proxy during training for the associated class of functions - but how faithful is this representation? It is known that many different parameter settings $\theta$ can determine the same function $f$. Moreover, the degree of this redundancy is inhomogeneous: for some networks, the only symmetries are permutation of neurons in a layer and positive scaling of parameters at a neuron, while other networks admit additional hidden symmetries. In this work, we prove that, for any network architecture where no layer is narrower than the input, there exist parameter settings with no hidden symmetries. We also describe a number of mechanisms through which hidden symmetries can arise, and empirically approximate the functional dimension of different network architectures at initialization. These experiments indicate that the probability that a network has no hidden symmetries decreases towards 0 as depth increases, while increasing towards 1 as width and input dimension increase.}
}



@article{sugiyama2012density,
  title={Density-ratio matching under the Bregman divergence: a unified framework of density-ratio estimation},
  author={Sugiyama, Masashi and Suzuki, Taiji and Kanamori, Takafumi},
  journal={Annals of the Institute of Statistical Mathematics},
  volume={64},
  number={5},
  pages={1009--1044},
  year={2012},
  publisher={Springer}
}

@InProceedings{kirchler2020twosample, 
title = {Two-sample Testing Using Deep Learning}, 
author = {Kirchler, Matthias and Khorasani, Shahryar and Kloft, Marius and Lippert, Christoph}, 
booktitle = {Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics}, pages = {1387--1398}, year = {2020}, 
editor = {Silvia Chiappa and Roberto Calandra}, 
volume = {108}, 
series = {Proceedings of Machine Learning Research}, 
month = {8},  
pdf = {http://proceedings.mlr.press/v108/kirchler20a/kirchler20a.pdf}, 
abstract = {We propose a two-sample testing procedure based on learned deep neural network representations. To this end, we define two test statistics that perform an asymptotic location test on data samples mapped onto a hidden layer. The tests are consistent and asymptotically control the type-1 error rate. Their test statistics can be evaluated in linear time (in the sample size). Suitable data representations are obtained in a data-driven way, by solving a supervised or unsupervised transfer-learning task on an auxiliary (potentially distinct) data set. If no auxiliary data is available, we split the data into two chunks: one for learning representations and one for computing the test statistic. In experiments on audio samples, natural images and three-dimensional neuroimaging data our tests yield significant decreases in type-2 error rate (up to 35 percentage points) compared to state-of-the-art two-sample tests such as kernel-methods and classifier two-sample tests.} }


@article{shazeer2020glu,
  title={Glu variants improve transformer},
  author={Shazeer, Noam},
  journal={arXiv preprint arXiv:2002.05202},
  year={2020}
}


@inproceedings{Yamada2011relativedensityratio,
 author = {Yamada, Makoto and Suzuki, Taiji and Kanamori, Takafumi and Hachiya, Hirotaka and Sugiyama, Masashi},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K. Q. Weinberger},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Relative Density-Ratio Estimation for Robust Distribution Comparison},
 url = {https://proceedings.neurips.cc/paper/2011/file/d1f255a373a3cef72e03aa9d980c7eca-Paper.pdf},
 volume = {24},
 year = {2011}
}

@article{kullback1951,
  author = {Solomon Kullback and Richard A. Leibler},
  title = {On Information and Sufficiency},
  journal = {Annals of Mathematical Statistics},
  volume = {22},
  number = {1},
  pages = {79--86},
  month = {3},
  year = {1951},
}


@article{Apperly2009,
  author = {Apperly, Ian A. and Butterfill, Stephen A.},
  title = {Do humans have two systems to track beliefs and belief-like states?},
  journal = {Psychological Review},
  year = {2009},
  volume = {116},
  number = {4},
  pages = {953--970},
  doi = {10.1037/a0016923},
  pmid = {19839692},
  month = {October}
}


@inproceedings{xu2024opentom,
    title = "{O}pen{T}o{M}: A Comprehensive Benchmark for Evaluating Theory-of-Mind Reasoning Capabilities of Large Language Models",
    author = "Xu, Hainiu  and
      Zhao, Runcong  and
      Zhu, Lixing  and
      Du, Jinhua  and
      He, Yulan",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    pages = "8593--8623",
    abstract = "Neural Theory-of-Mind (N-ToM), machine`s ability to understand and keep track of the mental states of others, is pivotal in developing socially intelligent agents. However, prevalent N-ToM benchmarks have several shortcomings, including the presence of ambiguous and artificial narratives, absence of personality traits and preferences, a lack of questions addressing characters' psychological mental states, and limited diversity in the questions posed. In response to these issues, we construct OpenToM, a new benchmark for assessing N-ToM with (1) longer and clearer narrative stories, (2) characters with explicit personality traits, (3) actions that are triggered by character intentions, and (4) questions designed to challenge LLMs' capabilities of modeling characters' mental states of both the physical and psychological world. Using OpenToM, we reveal that state-of-the-art LLMs thrive at modeling certain aspects of mental states in the physical world but fall short when tracking characters' mental states in the psychological world."
}

@inproceedings{zeng2024mr-ben,
  title={MR-BEN: A Comprehensive Meta-Reasoning Benchmark for Large Language Models},
  author={Zeng, Zhongshen and Liu, Yinhong and Wan, Yingjia and Li, Jingyao and Chen, Pengguang and Dai, Jianbo and Yao, Yuxuan and Xu, Rongwu and Qi, Zehan and Zhao, Wanru and others},
  booktitle={NeurIPS},
  year={2024}
}


@article{sugiyama11ratio_dimreduction,
title = {Direct density-ratio estimation with dimensionality reduction via least-squares hetero-distributional subspace search},
journal = {Neural Networks},
volume = {24},
number = {2},
pages = {183-198},
year = {2011},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2010.10.005},
author = {Masashi Sugiyama and Makoto Yamada and Paul {von Bünau} and Taiji Suzuki and Takafumi Kanamori and Motoaki Kawanabe},
keywords = {Density-ratio estimation, Dimensionality reduction, Unconstrained least-squares importance fitting},
abstract = {Methods for directly estimating the ratio of two probability density functions have been actively explored recently since they can be used for various data processing tasks such as non-stationarity adaptation, outlier detection, and feature selection. In this paper, we develop a new method which incorporates dimensionality reduction into a direct density-ratio estimation procedure. Our key idea is to find a low-dimensional subspace in which densities are significantly different and perform density-ratio estimation only in this subspace. The proposed method, D3-LHSS (Direct Density-ratio estimation with Dimensionality reduction via Least-squares Hetero-distributional Subspace Search), is shown to overcome the limitation of baseline methods.}
}

@inproceedings{kato2021non,
  title={Non-negative bregman divergence minimization for deep direct density ratio estimation},
  author={Kato, Masahiro and Teshima, Takeshi},
  booktitle={International Conference on Machine Learning},
  pages={5320--5333},
  year={2021},
  organization={PMLR}
}

@article{Worden2000demagedetectionusingoutliers,
title = {Damage detection using outlier analysis},
journal = {Journal of Sound and Vibration},
volume = {229},
number = {3},
pages = {647-667},
year = {2000},
issn = {0022-460X},
doi = {https://doi.org/10.1006/jsvi.1999.2514},
url = {https://www.sciencedirect.com/science/article/pii/S0022460X99925142},
author = {K. Worden and G. Manson and N.R.J. Fieller},
abstract = {This paper constitutes a study of a statistical method for damage detection. The lowest level of fault detection is considered so that the methods are simply required to signal deviations from normal condition; i.e., the problem is one of novelty detection. In this paper, the concept of discordancy from the statistical discipline of outlier analysis is used to signal deviance from the norm. The method is demonstrated on four case studies of engineering interest: one simulation, two pseudo-experimental and one experimental.}
}

@article{langerodi2020domain_adaptation_chemistry,
title = {Domain adaptation for regression under Beer–Lambert’s law},
journal = {Knowledge-Based Systems},
volume = {210},
pages = {106447},
year = {2020},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2020.106447},
url = {https://www.sciencedirect.com/science/article/pii/S0950705120305761},
author = {Ramin Nikzad-Langerodi and Werner Zellinger and Susanne Saminger-Platz and Bernhard A. Moser},
keywords = {Transfer learning, Domain adaptation, Moment alignment, Chemometrics, Calibration model adaptation, Partial least squares},
abstract = {We consider the problem of unsupervised domain adaptation (DA) in regression under the assumption of linear hypotheses (e.g. Beer–Lambert’s law) – a task recurrently encountered in analytical chemistry. Following the ideas from the non-linear iterative partial least squares (NIPALS) method, we propose a novel algorithm that identifies a low-dimensional subspace aiming at the following two objectives: (i) the projections of the source domain samples are informative w.r.t. the output variable and (ii) the projected domain-specific input samples have a small covariance difference. In particular, the latent variable vectors that span this subspace are derived in closed-form by solving a constrained optimization problem for each subspace dimension adding flexibility for balancing the two objectives. We demonstrate the superiority of our approach over several state-of-the-art (SoA) methods on different DA scenarios involving unsupervised adaptation of multivariate calibration models between different process lines in Melamine production and equality to SoA on two well-known benchmark datasets from analytical chemistry involving (unsupervised) model adaptation between different spectrometers. The former dataset is published with this work1 1https://github.com/RNL1/Melamine-Dataset.}
}

@inproceedings{saito2018MMDforUDA,
  title={Maximum classifier discrepancy for unsupervised domain adaptation},
  author={Saito, Kuniaki and Watanabe, Kohei and Ushiku, Yoshitaka and Harada, Tatsuya},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={3723--3732},
  year={2018}
}

@article{bradley1997auc,
title = {The use of the area under the ROC curve in the evaluation of machine learning algorithms},
journal = {Pattern Recognition},
volume = {30},
number = {7},
pages = {1145-1159},
year = {1997},
issn = {0031-3203},
doi = {https://doi.org/10.1016/S0031-3203(96)00142-2},
url = {https://www.sciencedirect.com/science/article/pii/S0031320396001422},
author = {Andrew P. Bradley},
keywords = {The ROC curve, The area under the ROC curve (AUC), Accuracy measures, Cross-validation, Wilcoxon statistic, Standard error},
abstract = {In this paper we investigate the use of the area under the receiver operating characteristic (ROC) curve (AUC) as a performance measure for machine learning algorithms. As a case study we evaluate six machine learning algorithms (C4.5, Multiscale Classifier, Perceptron, Multi-layer Perceptron, k-Nearest Neighbours, and a Quadratic Discriminant Function) on six “real world” medical diagnostics data sets. We compare and discuss the use of AUC to the more conventional overall accuracy and find that AUC exhibits a number of desirable properties when compared to overall accuracy: increased sensitivity in Analysis of Variance (ANOVA) tests; a standard error that decreased as both AUC and the number of test samples increased; decision threshold independent; and it is invariant to a priori class probabilities. The paper concludes with the recommendation that AUC be used in preference to overall accuracy for “single number” evaluation of machine learning algorithms.}
}

@article{uehara2016generative,
  title={Generative adversarial nets from a density ratio estimation perspective},
  author={Uehara, Masatoshi and Sato, Issei and Suzuki, Masahiro and Nakayama, Kotaro and Matsuo, Yutaka},
  journal={arXiv preprint arXiv:1610.02920},
  year={2016}
}

@misc{Radford2018ImprovingLU,
    author = {Alec Radford and Karthik Narasimhan and Tim Salimans and Ilya Sutskever},
    title = {Improving Language Understanding by Generative Pre-Training},
    year = {2018},
    howpublished = {OpenAI},
    url = {https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf}
}



@inproceedings{salimans2016improved,
author = {Salimans, Tim and Goodfellow, Ian and Zaremba, Wojciech and Cheung, Vicki and Radford, Alec and Chen, Xi},
title = {Improved Techniques for Training GANs},
year = {2016},
isbn = {9781510838819},
address = {Red Hook, NY, USA},
abstract = {We present a variety of new architectural features and training procedures that we apply to the generative adversarial networks (GANs) framework. Using our new techniques, we achieve state-of-the-art results in semi-supervised classification on MNIST, CIFAR-10 and SVHN. The generated images are of high quality as confirmed by a visual Turing test: our model generates MNIST samples that humans cannot distinguish from real data, and CIFAR-10 samples that yield a human error rate of 21.3%. We also present ImageNet samples with unprecedented resolution and show that our methods enable the model to learn recognizable features of ImageNet classes.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {2234–2242},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}



@inproceedings{lopezpaz2018revisiting,
  TITLE = {{Revisiting classifier two-sample tests}},
  AUTHOR = {Lopez-Paz, David and Oquab, Maxime},
  BOOKTITLE = {{International Conference on Learning Representations}},
  ADDRESS = {Toulon, France},
  YEAR = {2017},
  MONTH = Apr,
  PDF = {https://hal.inria.fr/hal-01862834/file/classifier_tests.pdf},
  HAL_ID = {hal-01862834},
  HAL_VERSION = {v1},
}

@inproceedings{zellinger2019central,
      title={Central Moment Discrepancy (CMD) for Domain-Invariant Representation Learning}, 
      author={Werner Zellinger and Thomas Grubinger and Edwin Lughofer and Thomas Natschläger and Susanne Saminger-Platz},
      year={2019},
      booktitle = {International Conference on Learning
Representations},
      eprint={1702.08811},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@InProceedings{long2017deep, title = {Deep Transfer Learning with Joint Adaptation Networks}, author = {Mingsheng Long and Han Zhu and Jianmin Wang and Michael I. Jordan}, booktitle = {Proceedings of the 34th International Conference on Machine Learning}, pages = {2208--2217}, year = {2017}, editor = {Doina Precup and Yee Whye Teh}, volume = {70}, series = {Proceedings of Machine Learning Research}, address = {International Convention Centre, Sydney, Australia}, month = {8},  pdf = {http://proceedings.mlr.press/v70/long17a/long17a.pdf},  abstract = {Deep networks have been successfully applied to learn transferable features for adapting models from a source domain to a different target domain. In this paper, we present joint adaptation networks (JAN), which learn a transfer network by aligning the joint distributions of multiple domain-specific layers across domains based on a joint maximum mean discrepancy (JMMD) criterion. Adversarial training strategy is adopted to maximize JMMD such that the distributions of the source and target domains are made more distinguishable. Learning can be performed by stochastic gradient descent with the gradients computed by back-propagation in linear-time. Experiments testify that our model yields state of the art results on standard datasets.} }


@article{ganin2016domainadversarial,
author = {Ganin, Yaroslav and Ustinova, Evgeniya and Ajakan, Hana and Germain, Pascal and Larochelle, Hugo and Laviolette, Fran\c{c}ois and Marchand, Mario and Lempitsky, Victor},
title = {Domain-Adversarial Training of Neural Networks},
year = {2016},
issue_date = {January 2016},
publisher = {JMLR.org},
volume = {17},
number = {1},
issn = {1532-4435},
abstract = {We introduce a new representation learning approach for domain adaptation, in which data at training and test time come from similar but different distributions. Our approach is directly inspired by the theory on domain adaptation suggesting that, for effective domain transfer to be achieved, predictions must be made based on features that cannot discriminate between the training (source) and test (target) domains.The approach implements this idea in the context of neural network architectures that are trained on labeled data from the source domain and unlabeled data from the target domain (no labeled target-domain data is necessary). As the training progresses, the approach promotes the emergence of features that are (i) discriminative for the main learning task on the source domain and (ii) indiscriminate with respect to the shift between the domains. We show that this adaptation behaviour can be achieved in almost any feed-forward model by augmenting it with few standard layers and a new gradient reversal layer. The resulting augmented architecture can be trained using standard backpropagation and stochastic gradient descent, and can thus be implemented with little effort using any of the deep learning packages.We demonstrate the success of our approach for two distinct classification problems (document sentiment analysis and image classification), where state-of-the-art domain adaptation performance on standard benchmarks is achieved. We also validate the approach for descriptor learning task in the context of person re-identification application.},
journal = {J. Mach. Learn. Res.},
month = jan,
pages = {2096–2030},
numpages = {35},
keywords = {person re-identification, sentiment analysis, neural network, domain adaptation, synthetic data, image classification, representation learning, deep learning}
}


@article{wei2022emergent,
      title={Emergent Abilities of Large Language Models},
      author={Jason Wei and Yi Tay and Rishi Bommasani and Colin Raffel and Barret Zoph and Sebastian Borgeaud and Dani Yogatama and Maarten Bosma and Denny Zhou and Donald Metzler and Ed H. Chi and Tatsunori Hashimoto and Oriol Vinyals and Percy Liang and Jeff Dean and William Fedus},
      journal={Transactions on Machine Learning Research},
      year={2022}
}


@inproceedings{Heusel2017FID,
author = {Heusel, Martin and Ramsauer, Hubert and Unterthiner, Thomas and Nessler, Bernhard and Hochreiter, Sepp},
title = {GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium},
year = {2017},
isbn = {9781510860964},
address = {Red Hook, NY, USA},
abstract = {Generative Adversarial Networks (GANs) excel at creating realistic images with complex models for which maximum likelihood is infeasible. However, the convergence of GAN training has still not been proved. We propose a two time-scale update rule (TTUR) for training GANs with stochastic gradient descent on arbitrary GAN loss functions. TTUR has an individual learning rate for both the discriminator and the generator. Using the theory of stochastic approximation, we prove that the TTUR converges under mild assumptions to a stationary local Nash equilibrium. The convergence carries over to the popular Adam optimization, for which we prove that it follows the dynamics of a heavy ball with friction and thus prefers flat minima in the objective landscape. For the evaluation of the performance of GANs at image generation, we introduce the 'Fr\'{e}chet Inception Distance" (FID) which captures the similarity of generated images to real ones better than the Inception Score. In experiments, TTUR improves learning for DCGANs and Improved Wasserstein GANs (WGAN-GP) outperforming conventional GAN training on CelebA, CIFAR-10, SVHN, LSUN Bedrooms, and the One Billion Word Benchmark.},
booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
pages = {6629–6640},
numpages = {12},
location = {Long Beach, California, USA},
series = {NIPS'17}
}


@article{bolley2005weighted,
     author = {Bolley, Fran\c{c}ois and Villani, C\'edric},
     title = {Weighted Csisz\'ar-Kullback-Pinsker inequalities and applications to transportation inequalities},
     journal = {Annales de la Facult\'e des sciences de Toulouse : Math\'ematiques},
     publisher = {Universit\'e Paul Sabatier, Institut de Math\'ematiques},
     address = {Toulouse},
     volume = {Ser. 6, 14},
     number = {3},
     year = {2005},
     pages = {331-352},
     zbl = {1087.60008},
     mrnumber = {2172583},
     language = {en}
}

@inproceedings{serra2018bounding,
  title={Bounding and counting linear regions of deep neural networks},
  author={Serra, Thiago and Tjandraatmadja, Christian and Ramalingam, Srikumar},
  booktitle={International Conference on Machine Learning},
  pages={4558--4566},
  year={2018}
}
@inproceedings{yoshida2017spectral,
  title={Spectral Norm Regularization for Improving the Generalizability of Deep Learning},
  author={Yoshida, Yasuhiro and Miyato, Takeru},
  booktitle={arXiv preprint arXiv:1705.10941},
  year={2017}
}


@Inbook{dembo2010LDT,
author="Dembo, Amir
and Zeitouni, Ofer",
title="Applications-The Finite Dimensional Case",
bookTitle="Large Deviations Techniques and Applications",
year="2010",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="71--114",
abstract="This chapter consists of applications of the theory presented in Chapter 2. The LDPs associated with finite state irreducible Markov chains are derived in Section 3.1 as a corollary of the G{\"a}rtner---Ellis theorem. Varadhan's characterization of the spectral radius of nonnegative irreducible matrices is derived along the way. (See Exercise 3.1.19.) The asymptotic size of long rare segments in random walks is found by combining, in Section 3.2, the basic large deviations estimates of Cram{\'e}r's theorem with the Borel---Cantelli lemma. The Gibbs conditioning principle is of fundamental importance in statistical mechanics. It is derived in Section 3.3, for finite alphabet, as a direct result of Sanov's theorem. The asymptotics of the probability of error in hypothesis testing problems are analyzed in Sections 3.4 and 3.5 for testing between two a priori known product measures and for universal testing, respectively. Shannon's source coding theorem is proved in Section 3.6 by combining the classical random coding argument with the large deviations lower bound of the G{\"a}rtner---Ellis theorem. Finally, Section 3.7 is devoted to refinements of Cram{\'e}r's theorem in ℝ. Specifically, it is shown that for $\beta$∈(0,1/2), satisfies the LDP with a Normal-like rate function, and the pre-exponent associated with is computed for appropriate values of q.",
isbn="978-3-642-03311-7"
}




@Article{MoserPatent2018,
  author   = {Bernhard A. Moser},
  title         = {{C}omputer {I}mplementiertes {V}erfahren zur {B}ewertung der {I}ntegrit\"at von {N}euronalen {N}etzen ({M}ethod for the integrity evaluation of neural networks)},
  journal  = {International Patent PCT/EP2019/072830, filed 27th August 2019; priority date 10th of Sept. 2018, DPMA S2959},
  year     = {2018},
  month = {Sept.}
}

@misc{theis2015note,
    title={A note on the evaluation of generative models},
    author={Lucas Theis and Aäron van den Oord and Matthias Bethge},
    year={2015},
    eprint={1511.01844},
    archivePrefix={arXiv},
    primaryClass={stat.ML}
}

@incollection{LeCun1998EfficientBackprop,
  title={Efficient backprop},
  author={LeCun, Yann and Bottou, Léon and Orr, Genevieve B. and M{\"u}ller, Klaus-Robert},
  booktitle={Neural Networks: Tricks of the Trade},
  pages={9--50},
  year={1998},
  publisher={Springer}
}


@misc{borji2018pros,
    title={Pros and Cons of GAN Evaluation Measures},
    author={Ali Borji},
    year={2018},
    eprint={1802.03446},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}

@inproceedings{hanin2019complexity,
  title={Complexity of linear regions in deep networks},
  author={Hanin, Boris and Rolnick, David},
  booktitle={International Conference on Machine Learning},
  pages={2596--2604},
  year={2019},
  organization={PMLR}
}

@misc{zhang2024relu2,
      title={ReLU$^2$ Wins: Discovering Efficient Activation Functions for Sparse LLMs}, 
      author={Zhengyan Zhang and Yixin Song and Guanghui Yu and Xu Han and Yankai Lin and Chaojun Xiao and Chenyang Song and Zhiyuan Liu and Zeyu Mi and Maosong Sun},
      year={2024},
      eprint={2402.03804},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{tsuzuku2018lipschitz,
  title={Lipschitz-margin training: Scalable certification of perturbation invariance for deep neural networks},
  author={Tsuzuku, Yusuke and Sato, Issei and Sugiyama, Masashi},
  journal={Advances in Neural Information Processing Systems},
  volume={31},
  year={2018}
}

@InProceedings{cem2019sortinglipschitzapproximation,
  title = 	 {Sorting Out {L}ipschitz Function Approximation},
  author =       {Anil, Cem and Lucas, James and Grosse, Roger},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {291--301},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/anil19a/anil19a.pdf},
  url = 	 {https://proceedings.mlr.press/v97/anil19a.html},
  abstract = 	 {Training neural networks under a strict Lipschitz constraint is useful for provable adversarial robustness, generalization bounds, interpretable gradients, and Wasserstein distance estimation. By the composition property of Lipschitz functions, it suffices to ensure that each individual affine transformation or nonlinear activation is 1-Lipschitz. The challenge is to do this while maintaining the expressive power. We identify a necessary property for such an architecture: each of the layers must preserve the gradient norm during backpropagation. Based on this, we propose to combine a gradient norm preserving activation function, GroupSort, with norm-constrained weight matrices. We show that norm-constrained GroupSort architectures are universal Lipschitz function approximators. Empirically, we show that norm-constrained GroupSort networks achieve tighter estimates of Wasserstein distance than their ReLU counterparts and can achieve provable adversarial robustness guarantees with little cost to accuracy.}
}

@INPROCEEDINGS{Ilan2021Trajectorygrowth,
  author={Price, Ilan and Tanner, Jared},
  booktitle={2021 20th IEEE International Conference on Machine Learning and Applications (ICMLA)}, 
  title={Trajectory growth lower bounds for random sparse deep ReLU networks}, 
  year={2021},
  volume={},
  number={},
  pages={1004-1009},
  keywords={Deep learning;Conferences;Neural networks;Trajectory;Sparse matrices;deep learning;random curves;random sparse matrices;expected arc length;expressivity},
  doi={10.1109/ICMLA52953.2021.00165}}



@article{bonicelli2022effectiveness,
  title={On the effectiveness of lipschitz-driven rehearsal in continual learning},
  author={Bonicelli, Lorenzo and Boschini, Matteo and Porrello, Angelo and Spampinato, Concetto and Calderara, Simone},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={31886--31901},
  year={2022}
}

@inproceedings{virmaux2018lipschitz,
 author = {Virmaux, Aladin and Scaman, Kevin},
 booktitle = {Advances in Neural Information Processing Systems},
 title = {Lipschitz regularity of deep neural networks: analysis and efficient estimation},
 volume = {31},
 year = {2018}
}


@InProceedings{cisse2017parseval,
  title = 	 {Parseval Networks: Improving Robustness to Adversarial Examples},
  author =       {Moustapha Cisse and Piotr Bojanowski and Edouard Grave and Yann Dauphin and Nicolas Usunier},
  booktitle = 	 {Proceedings of the 34th International Conference on Machine Learning},
  pages = 	 {854--863},
  year = 	 {2017},
  editor = 	 {Precup, Doina and Teh, Yee Whye},
  volume = 	 {70},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {06--11 Aug},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v70/cisse17a/cisse17a.pdf},
  url = 	 {https://proceedings.mlr.press/v70/cisse17a.html},
  abstract = 	 {We introduce Parseval networks, a form of deep neural networks in which the Lipschitz constant of linear, convolutional and aggregation layers is constrained to be smaller than $1$. Parseval networks are empirically and theoretically motivated by an analysis of the robustness of the predictions made by deep neural networks when their input is subject to an adversarial perturbation. The most important feature of Parseval networks is to maintain weight matrices of linear and convolutional layers to be (approximately) Parseval tight frames, which are extensions of orthogonal matrices to non-square matrices. We describe how these constraints can be maintained efficiently during SGD. We show that Parseval networks match the state-of-the-art regarding accuracy on CIFAR-10/100 and Street View House Numbers (SVHN), while being more robust than their vanilla counterpart against adversarial examples. Incidentally, Parseval networks also tend to train faster and make a better usage of the full capacity of the networks.}
}


@article{campbell2002deepblue,
title = {Deep Blue},
journal = {Artificial Intelligence},
volume = {134},
number = {1},
pages = {57-83},
year = {2002},
issn = {0004-3702},
author = {Murray Campbell and A.Joseph Hoane and Feng-hsiung Hsu},
keywords = {Computer chess, Game tree search, Parallel search, Selective search, Search extensions, Evaluation function},
abstract = {Deep Blue is the chess machine that defeated then-reigning World Chess Champion Garry Kasparov in a six-game match in 1997. There were a number of factors that contributed to this success, including: •a single-chip chess search engine,•a massively parallel system with multiple levels of parallelism,•a strong emphasis on search extensions,•a complex evaluation function, and•effective use of a Grandmaster game database. This paper describes the Deep Blue system, and gives some of the rationale that went into the design decisions behind Deep Blue.}
}

@inproceedings{qin2022cosformer,
  title={cosFormer: Rethinking Softmax in Attention},
  author={Zhen Qin and Weixuan Sun and Hui Deng and Dongxu Li and Yunshen Wei and Baohong Lv and Junjie Yan and Lingpeng Kong and Yiran Zhong},
  booktitle={International Conference on Learning Representations},
  year={2022},
}

@article{moser2017similarity,
  author    = {Bernhard A. Moser},
  title     = {Similarity recovery from threshold-based sampling under general conditions},
  journal   = {IEEE Transactions on Signal Processing},
  volume    = {65},
  number    = {17},
  pages     = {4645--4654},
  year      = {2017}
}

@article{moser2019quasi,
  author    = {Bernhard A. Moser and Michael Lunglmayr},
  title     = {On Quasi-Isometry of Threshold-Based Sampling},
  journal   = {IEEE Transactions on Signal Processing},
  volume    = {67},
  number    = {14},
  pages     = {3832--3841},
  year      = {2019}
}

@article{moser2024spiking,
  author    = {Bernhard A. Moser and Michael Lunglmayr},
  title     = {Spiking neural networks in the Alexiewicz topology: A new perspective on analysis and error bounds},
  journal   = {Neurocomputing},
  year      = {2024},
  pages     = {128190}
}


@inproceedings{zhang2021sparse,
  title={Sparse Attention with Linear Units},
  author={Biao Zhang and Ivan Titov and Rico Sennrich},
  booktitle={Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  year={2021},
}


@inproceedings{choromanski2021rethinking,
  title={Rethinking Attention with Performers},
  author={Krzysztof Choromanski and Valerii Likhosherstov and David Dohan and Xingyou Song and Andreea Gane and Tamas Sarlos and Peter Hawkins and Jared Davis and Afroz Mohiuddin and Lukasz Kaiser and David Belanger and Lucy Colwell and Adrian Weller},
  booktitle={International Conference on Learning Representations},
  year={2021},
}


@inproceedings{brown2017depthlimited,
 author = {Brown, Noam and Sandholm, Tuomas and Amos, Brandon},
 booktitle = {NeurIPS},
 title = {Depth-Limited Solving for Imperfect-Information Games},
 year = {2018}
}


@article{harsanyi1967incmpleteinformation,
 ISSN = {00251909, 15265501},
 URL = {http://www.jstor.org/stable/2628894},
 abstract = {Parts I and II of this paper have described a new theory for the analysis of games with incomplete information. Two cases have been distinguished: consistent games in which there exists some basic probability distribution from which the players' subjective probability distributions can be derived as conditional probability distributions; and inconsistent games in which no such basic probability distribution exists. Part III will now show that in consistent games, where a basic probability distribution exists, it is essentially unique. It will also be argued that, in the absence of special reasons to the contrary, one should try to analyze any given game situation with incomplete information in terms of a consistent-game model. However, it will be shown that our theory can be extended also to inconsistent games, in case the situation does require the use of an inconsistent-game model.},
 author = {John C. Harsanyi},
 journal = {Management Science},
 number = {7},
 pages = {486--502},
 publisher = {INFORMS},
 title = {Games with Incomplete Information Played by "Bayesian" Players, I-III. Part III. The Basic Probability Distribution of the Game},
 urldate = {2024-02-15},
 volume = {14},
 year = {1968}
}



@article{cullen2009imitation,
  author  = {Jamie Cullen},
  title   = {Imitation Versus Communication: Testing for Human-Like Intelligence},
  journal = {Minds \& Machines},
  year    = {2009},
  volume  = {19},
  pages   = {237--254},
  doi     = {10.1007/s11023-009-9149-3},
}

@article{french2000thefirst50,
title = {The Turing Test: the first 50 years},
journal = {Trends in Cognitive Sciences},
volume = {4},
number = {3},
pages = {115-122},
year = {2000},
issn = {1364-6613},
doi = {https://doi.org/10.1016/S1364-6613(00)01453-4},
url = {https://www.sciencedirect.com/science/article/pii/S1364661300014534},
author = {Robert M. French},
abstract = {The Turing Test, originally proposed as a simple operational definition of intelligence, has now been with us for exactly half a century. It is safe to say that no other single article in computer science, and few other articles in science in general, have generated so much discussion. The present article chronicles the comments and controversy surrounding Turing’s classic article from its publication to the present. The changing perception of the Turing Test over the last 50 years has paralleled the changing attitudes in the scientific community towards artificial intelligence: from the unbridled optimism of 1960s to the current realization of the immense difficulties that still lie ahead. I conclude with the prediction that the Turing Test will remain important, not only as a landmark in the history of the development of intelligent machines, but also with real relevance to future generations of people living in a world in which the cognitive capacities of machines will be vastly greater than they are now.}
}

@misc{openai2019dota,
      title={Dota 2 with Large Scale Deep Reinforcement Learning}, 
      author={Berner, Christopher and Brockman, Greg and Chan, Brooke and Cheung, Vicki and Debiak, Przemyslaw and Dennison, Christy and Farhi, David and Fischer, Quirin and Hashme, Shariq and Hesse, Chris and others},
      year={2019},
      eprint={1912.06680},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{radford2023whisper,
  title={Robust speech recognition via large-scale weak supervision},
  author={Radford, Alec and Kim, Jong Wook and Xu, Tao and Brockman, Greg and McLeavey, Christine and Sutskever, Ilya},
  booktitle={ICML},
  year={2023},
}

@article{masum2003turingratio,
	author = {Hassan Masum and Steffen Christensen},
	journal = {Journal of Evolution and Technology},
	number = {2},
	title = {The Turing Ratio: A Framework for Open-Ended Task Metrics},
	volume = {13},
	year = {2003}
}


@article{Vinyals2019starcraft,
  author    = {Oriol Vinyals, Igor Babuschkin, Wojciech M. Czarnecki, Michael Mathieu, Andrew Dudzik, Junyoung Chung, David H. Choi, Richard Powell, Timo Ewalds, Petko Georgiev, Junhyuk Oh, Dan Horgan, Manuel Kroiss, Ivo Danihelka, Aja Huang, Laurent Sifre, Trevor Cai, John P. Agapiou, Max Jaderberg, Alexander S. Vezhnevets, Rémi Leblond, Tobias Pohlen, Valentin Dalibard, David Budden, Yury Sulsky, James Molloy, Tom L. Paine, Caglar Gulcehre, Ziyu Wang, Tobias Pfaff, Yuhuai Wu, Roman Ring, Dani Yogatama, Dario Wunsch, Katrina McKinney, Oliver Smith, Tom Schaul, Timothy Lillicrap, Koray Kavukcuoglu, Demis Hassabis, Chris Apps, David Silver },
  title     = {Grandmaster level in StarCraft II using multi-agent reinforcement learning},
  journal   = {Nature},
  volume    = {575},
  number    = {7782},
  pages     = {350--354},
  year      = {2019},
  issn      = {1476-4687}
}


@article{legg2007universal,
  title={Universal intelligence: A definition of machine intelligence},
  author={Legg, Shane and Hutter, Marcus},
  journal={Minds and machines},
  volume={17},
  pages={391--444},
  year={2007},
  publisher={Springer}
}

@misc{arjovsky2017wasserstein,
    title={Wasserstein GAN},
    author={Martin Arjovsky and Soumith Chintala and Léon Bottou},
    year={2017},
    eprint={1701.07875},
    archivePrefix={arXiv},
    primaryClass={stat.ML}
} 

@article{cantor1883,
  author    = {Georg Cantor},
  title     = {{\"U}ber unendliche, lineare Punktmannigfaltigkeiten},
  journal   = {Math. Annalen},
  volume    = {21},
  number    = {4},
  pages     = {545--591},
  year      = {1883}
}



@article{kolmogorov1965,
  title={Three approaches to the quantitative definition of information},
  author={Kolmogorov, Andrey N},
  journal={Problems of Information Transmission},
  volume={1},
  number={1},
  pages={1--7},
  year={1965}
}

@article{solomonoff1964,
  title={A formal theory of inductive inference. Part I},
  author={Solomonoff, Ray J},
  journal={Information and Control},
  volume={7},
  number={1},
  pages={1--22},
  year={1964}
}

@article{hertrich2021towards,
  title={Towards lower bounds on the depth of ReLU neural networks},
  author={Hertrich, Christoph and Basu, Amitabh and Di Summa, Marco and Skutella, Martin},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={3336--3348},
  year={2021}
}

@article{chaitin1966,
  title={On the length of programs for computing finite binary sequences},
  author={Chaitin, Gregory J},
  journal={Journal of the ACM (JACM)},
  volume={13},
  number={4},
  pages={547--569},
  year={1966}
}


@article{BianchiniS14,
  author    = {Monica Bianchini and
               Franco Scarselli},
  title     = {On the Complexity of Neural Network Classifiers: {A} Comparison Between
               Shallow and Deep Architectures},
  journal   = {{IEEE} Trans. NN Learn. Syst.},
  volume    = {25},
  number    = {8},
  pages     = {1553--1565},
  year      = {2014},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{li2018measuring,
  title={Measuring the intrinsic dimension of objective landscapes},
  author={Li, Chunyuan and Farkhoor, Heerad and Liu, Rosanne and Yosinski, Jason},
  booktitle={International Conference on Learning Representations},
  year={2018}
}

@article{ansuini2019intrinsic,
  title={Intrinsic dimension of data representations in deep neural networks},
  author={Ansuini, Alessio and Laio, Alessandro and Macke, Jakob H and Zoccolan, Davide},
  journal={Advances in Neural Information Processing Systems},
  pages={6111--6122},
  year={2019}
}


@article{bartlett2019nearly,
  title={Nearly-tight VC-dimension and pseudodimension bounds for piecewise linear neural networks},
  author={Bartlett, Peter L and Harvey, Nick and Liaw, Christopher and Mehrabian, Abbas},
  journal={The Journal of Machine Learning Research},
  volume={20},
  number={1},
  pages={2285--2301},
  year={2019},
  publisher={JMLR. org}
}

@inproceedings{graves2009offline,
  title={Offline Handwriting Recognition with Multidimensional Recurrent Neural Networks},
  author={Graves, Alex and Schmidhuber, J{\"u}rgen},
  booktitle={Advances in Neural Information Processing Systems 22 (NIPS'22)},
  pages={545--552},
  editor={Bengio, Yoshua and Schuurmans, Dale and Lafferty, John and Williams, Chris K. I. and Culotta, Aron},
  year={2009},
  organization={Neural Information Processing Systems (NIPS) Foundation},
  address={Vancouver, BC},
  month={12}
}

@article{graves2009connectionist,
  title={A Novel Connectionist System for Improved Unconstrained Handwriting Recognition},
  author={Graves, A. and Liwicki, M. and Fernandez, S. and Bertolami, R. and Bunke, H. and Schmidhuber, J.},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume={31},
  number={5},
  year={2009},
}

@article{neyshabur2017exploring,
  title={Exploring generalization in deep learning},
  author={Neyshabur, Behnam and Bhojanapalli, Srinadh and McAllester, David and Srebro, Nati},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@inproceedings{zhang2017understanding,
  title={Understanding deep learning requires rethinking generalization},
  author={Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
  booktitle={ICLR},
  year={2017}
}

@article{anthony2009neural,
  title={Neural network learning: Theoretical foundations},
  author={Anthony, Martin and Bartlett, Peter L},
  journal={Journal},
  publisher={Cambridge University Press},
  year={2009}
}

@article{bartlett2002rademacher,
  title={Rademacher and Gaussian complexities: Risk bounds and structural results},
  author={Bartlett, Peter L and Mendelson, Shahar},
  journal={Journal of Machine Learning Research},
  volume={3},
  pages={463--482},
  year={2002}
}

@article{mallat2012group,
  title={Group invariant scattering},
  author={Mallat, St{\'e}phane},
  journal={Communications on Pure and Applied Mathematics},
  volume={65},
  number={10},
  pages={1331--1398},
  year={2012},
  publisher={Wiley Online Library}
}

@article{dill2008protein,
  title={The protein folding problem},
  author={Dill, Ken A. and Ozkan, S. Banu and Shell, M. Scott and Weikl, Thomas R.},
  journal={Annual Review of Biophysics},
year={2008}
}

@inproceedings{belov22tessellatoinbasedde,
author = {Belov, Vladislav and Marik, Radek},
title = {Tessellation-Based Kernel Density Estimation},
year = {2022},
isbn = {9781450385053},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Kernel density estimation is a complex task that plays an essential role in a variety of applications. In this paper, we introduce an approach to the task that converts the problem of bandwidth evaluation in the Parzen-window-like framework into the non-parametric evaluation of a fine-grained density estimate which can then be scaled by means of the Scale-Space theory to achieve the desired level of smoothness. The detailed estimate is realized through the Delaunay space tessellation method and properties of its output simplices. Additionally, in the experimental part of the paper, we showcase the new method and demonstrate its outputs at various scales, reaching results that perceivably outperform its counterparts.},
booktitle = {Proceedings of the 2021 4th International Conference on Algorithms, Computing and Artificial Intelligence},
articleno = {36},
numpages = {6},
keywords = {Scale-Space theory, Delaunay tessellation, kernel density estimation},
location = {Sanya, China},
series = {ACAI '21}
}

@article{mcallester1999pac,
  title={PAC-Bayesian model averaging},
  author={McAllester, David A},
  journal={Proceedings of the 12th Annual Conference on Computational Learning Theory (COLT)},
  year={1999},
  pages={164--170}
}


@article{neyshabur2017pac,
  title={A pac-bayesian approach to spectrally-normalized margin bounds for neural networks},
  author={Neyshabur, Behnam and Bhojanapalli, Srinadh and Srebro, Nathan},
  journal={ICLR},
  year={2018}
}


@InProceedings{polianskii22voronoide,
  title = 	 {Voronoi density estimator for high-dimensional data: Computation, compactification and convergence},
  author =       {Polianskii, Vladislav and Marchetti, Giovanni Luca and Kravberg, Alexander and Varava, Anastasiia and Pokorny, Florian T. and Kragic, Danica},
  booktitle = 	 {Proceedings of the Thirty-Eighth Conference on Uncertainty in Artificial Intelligence},
  pages = 	 {1644--1653},
  year = 	 {2022},
  editor = 	 {Cussens, James and Zhang, Kun},
  volume = 	 {180},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {8},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v180/polianskii22a/polianskii22a.pdf},
  url = 	 {https://proceedings.mlr.press/v180/polianskii22a.html},
  abstract = 	 {The Voronoi Density Estimator (VDE) is an established density estimation technique that adapts to the local geometry of data. However, its applicability has been so far limited to problems in two and three dimensions. This is because Voronoi cells rapidly increase in complexity as dimensions grow, making the necessary explicit computations infeasible. We define a variant of the VDE deemed Compactified Voronoi Density Estimator (CVDE), suitable for higher dimensions. We propose computationally efficient algorithms for numerical approximation of the CVDE and formally prove convergence of the estimated density to the original one. We implement and empirically validate the CVDE through a comparison with the Kernel Density Estimator (KDE). Our results indicate that the CVDE outperforms the KDE on sound and image data.}
}





@article{hinton2006reducingdimensionality,
author = {G. E. Hinton  and R. R. Salakhutdinov },
title = {Reducing the Dimensionality of Data with Neural Networks},
journal = {Science},
volume = {313},
number = {5786},
pages = {504-507},
year = {2006},
abstract = {High-dimensional data can be converted to low-dimensional codes by training a multilayer neural network with a small central layer to reconstruct high-dimensional input vectors. Gradient descent can be used for fine-tuning the weights in such “autoencoder” networks, but this works well only if the initial weights are close to a good solution. We describe an effective way of initializing the weights that allows deep autoencoder networks to learn low-dimensional codes that work much better than principal components analysis as a tool to reduce the dimensionality of data.}}

@article{alquier2024user,
  title={User-friendly introduction to PAC-Bayes bounds},
  author={Alquier, Pierre and others},
  journal={Foundations and Trends{\textregistered} in Machine Learning},
  volume={17},
  number={2},
  pages={174--303},
  year={2024},
  publisher={Now Publishers, Inc.}
}

@inproceedings{bartlerr2017spectrally,
 author = {Bartlett, Peter L and Foster, Dylan J and Telgarsky, Matus J},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Spectrally-normalized margin bounds for neural networks},
 volume = {30},
 year = {2017}
}


@inproceedings{mitchell2023detectgpt,
  title={Detectgpt: Zero-shot machine-generated text detection using probability curvature},
  author={Mitchell, Eric and Lee, Yoonho and Khazatsky, Alexander and Manning, Christopher D and Finn, Chelsea},
  booktitle={International Conference on Machine Learning},
  pages={24950--24962},
  year={2023},
  organization={PMLR}
}

@article{dirksen2022separation,
  title={The separation capacity of random neural networks},
  author={Dirksen, Sjoerd and Genzel, Martin and Jacques, Laurent and Stollenwerk, Alexander},
  journal={Journal of Machine Learning Research},
  volume={23},
  number={309},
  pages={1--47},
  year={2022}
}

@InProceedings{miranda2024undetectablewatermarks,
  title = 	 {Undetectable Watermarks for Language Models},
  author =       {Christ, Miranda and Gunn, Sam and Zamir, Or},
  booktitle = 	 {Proceedings of Thirty Seventh Conference on Learning Theory},
  pages = 	 {1125--1139},
  year = 	 {2024},
  volume = 	 {247},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {7},
  publisher =    {PMLR},
  abstract = 	 {Recent advances in the capabilities of large language models such as GPT-4 have spurred increasing concern about our ability to detect AI-generated text.  Prior works have suggested methods of embedding watermarks in model outputs, by *noticeably* altering the output distribution. We ask: Is it possible to introduce a watermark without incurring *any detectable* change to the output distribution? To this end, we introduce a cryptographically-inspired notion of undetectable watermarks for language models.  That is, watermarks can be detected only with the knowledge of a secret key; without the secret key, it is computationally intractable to distinguish watermarked outputs from those of the original model. In particular, it is impossible for a user to observe any degradation in the quality of the text. Crucially, watermarks remain undetectable even when the user is allowed to adaptively query the model with arbitrarily chosen prompts. We construct undetectable watermarks based on the existence of one-way functions, a standard assumption in cryptography.}
}

@misc{alex2018gilbo,
    title={GILBO: One Metric to Measure Them All},
    author={Alexander A. Alemi and Ian Fischer},
    year={2018},
    eprint={1802.04874},
    archivePrefix={arXiv},
    primaryClass={stat.ML}
}

@inproceedings{diederik15adam,
  author    = {Diederik P. Kingma and
               Jimmy Ba},
  title     = {Adam: {A} Method for Stochastic Optimization},
  booktitle = {ICLR},
  year      = {2015},
  timestamp = {Thu, 25 Jul 2019 14:25:37 +0200},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{duchi11adagrad,
  author  = {John Duchi and Elad Hazan and Yoram Singer},
  title   = {Adaptive Subgradient Methods for Online Learning and Stochastic Optimization},
  journal = {Journal of Machine Learning Research},
  year    = {2011},
  volume  = {12},
  number  = {61},
  pages   = {2121-2159},
  url     = {http://jmlr.org/papers/v12/duchi11a.html}
}


@InProceedings{sutskever13momentum, title = {On the importance of initialization and momentum in deep learning}, author = {Ilya Sutskever and James Martens and George Dahl and Geoffrey Hinton}, booktitle = {Proceedings of the 30th International Conference on Machine Learning}, pages = {1139--1147}, year = {2013}, 
volume = {28}, series = {Proceedings of Machine Learning Research}, 
address = {Atlanta, Georgia, USA}, month = {17--19 Jun}, 
pdf = {http://proceedings.mlr.press/v28/sutskever13.pdf}, 
url = {http://proceedings.mlr.press/v28/sutskever13.html}, 
abstract = {Deep and recurrent neural networks (DNNs and RNNs respectively) are powerful models that were considered to be almost impossible to train using stochastic gradient descent with momentum. In this paper, we show that when stochastic gradient descent with momentum uses a well-designed random initialization and a particular type of slowly increasing schedule for the momentum parameter, it can train both DNNs and RNNs (on datasets with long-term dependencies) to levels of performance that were previously achievable only with Hessian-Free optimization. We find that both the initialization and the momentum are crucial since poorly initialized networks cannot be trained with momentum and well-initialized networks perform markedly worse when the momentum is absent or poorly tuned. Our success training these models suggests that previous attempts to train deep and recurrent neural networks from random initializations have likely failed due to poor initialization schemes. Furthermore, carefully tuned momentum methods suffice for dealing with the curvature issues in deep and recurrent network training objectives without the need for sophisticated second-order methods. } }

@inproceedings{
zhang2020empirical,
title={Empirical Studies on the Properties of Linear Regions in Deep Neural Networks},
author={Xiao Zhang and Dongrui Wu},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=SkeFl1HKwr}
}

@inproceedings{macdonald2022complete,
  title={A Complete Characterisation of ReLU-Invariant Distributions},
  author={Macdonald, Jan and W{\"a}ldchen, Stephan},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={1457--1484},
  year={2022},
  organization={PMLR}
}

@article{hornik1989universalapprox,
title = {Multilayer feedforward networks are universal approximators},
journal = {Neural Networks},
volume = {2},
number = {5},
pages = {359-366},
year = {1989},
issn = {0893-6080},
author = {Kurt Hornik and Maxwell Stinchcombe and Halbert White},
keywords = {Feedforward networks, Universal approximation, Mapping networks, Network representation capability, Stone-Weierstrass Theorem, Squashing functions, Sigma-Pi networks, Back-propagation networks},
abstract = {This paper rigorously establishes that standard multilayer feedforward networks with as few as one hidden layer using arbitrary squashing functions are capable of approximating any Borel measurable function from one finite dimensional space to another to any desired degree of accuracy, provided sufficiently many hidden units are available. In this sense, multilayer feedforward networks are a class of universal approximators.}
}

@article{dekker2013exploring,
  title={Exploring the three-dimensional organization of genomes: interpreting chromatin interaction data},
  author={Dekker, Job and Marti-Renom, Marc A. and Mirny, Leonid A.},
  journal={Nature Reviews Genetics},
  volume={14},
  number={6},
  pages={390--403},
  year={2013},
  doi={10.1038/nrg3454}
}


@article{robbins1951sgd,
  author    = {Herbert Robbins and Sutton Monro},
  title     = {A Stochastic Approximation Method},
  journal   = {The Annals of Mathematical Statistics},
  year      = {1951},
  volume    = {22},
  number    = {3},
  pages     = {400--407},
  doi       = {10.1214/aoms/1177729586}
}



@article{fukushima1980neocognitron,
  author    = {Kunihiko Fukushima},
  title     = {Neocognitron: A Self-organizing Neural Network Model for a Mechanism of Pattern Recognition Unaffected by Shift in Position},
  journal   = {Biological Cybernetics},
  year      = {1980},
  volume    = {36},
  number    = {4},
  pages     = {193--202},
  doi       = {10.1007/BF00344251}
}


@article{linnainmaa1976,
  author    = {Seppo Linnainmaa},
  title     = {Taylor Expansion of the Accumulated Rounding Error},
  journal   = {BIT Numerical Mathematics},
  year      = {1976},
  volume    = {16},
  number    = {2},
  pages     = {146--160},
  doi       = {10.1007/BF01931367}
}


@mastersthesis{linnainmaa1970,
  author       = {Seppo Linnainmaa},
  title        = {The Representation of the Cumulative Rounding Error of an Algorithm as a Taylor Expansion of the Local Rounding Errors},
  school       = {University of Helsinki},
  year         = {1970},
  note         = {Master's Thesis (in Finnish)},
  url          = {https://people.idsia.ch/~juergen/linnainmaa1970thesis.pdf}
}


@book{ivakhnenko1965cybernetic,
  title     = {Cybernetic Predicting Devices},
  author    = {A. G. Ivakhnenko and V. G. Lapa},
  year      = {1965},
  publisher = {CCM Information Corporation}
}

@article{bruna2013invariant,
  title={Invariant scattering convolution networks},
  author={Bruna, Joan and Mallat, St{\'e}phane},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={35},
  number={8},
  pages={1872--1886},
  year={2013},
  publisher={IEEE}
}

@article{yayla2021universal,
  title={Universal Approximation Theorems of Fully Connected Binarized Neural Networks},
  author={Yayla, Mikail and G{\"u}nzel, Mario and Ramosaj, Burim and Chen, Jian-Jia},
  journal={arXiv preprint arXiv:2102.02631},
  year={2021}
}

@inproceedings{Anderson2017TheHG,
  title={The High-Dimensional Geometry of Binary Neural Networks},
  author={Alexander G. Anderson and Cory P. Berg},
booktitle={International Conference on Learning Representations},
year={2018},
  volume={abs/1705.07199}
}

@inproceedings{yu2020bdd100k,
  title={Bdd100k: A diverse driving dataset for heterogeneous multitask learning},
  author={Yu, Fisher and Chen, Haofeng and Wang, Xin and Xian, Wenqi and Chen, Yingying and Liu, Fangchen and Madhavan, Vashisht and Darrell, Trevor},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={2636--2645},
  year={2020}
}

@article{vapnik1971uniform,
  title={On the uniform convergence of relative frequencies of events to their probabilities},
  author={Vapnik, Vladimir N and Chervonenkis, Alexey Y},
  journal={Theory of Probability \& Its Applications},
  volume={16},
  number={2},
  pages={264--280},
  year={1971},
  publisher={SIAM}
}


@article{kremer2015astronomy,
title = {Nearest neighbor density ratio estimation for large-scale applications in astronomy},
journal = {Astronomy and Computing},
volume = {12},
pages = {67-72},
year = {2015},
issn = {2213-1337},
doi = {https://doi.org/10.1016/j.ascom.2015.06.005},
url = {https://www.sciencedirect.com/science/article/pii/S2213133715000657},
author = {J. Kremer and F. Gieseke and K. {Steenstrup Pedersen} and C. Igel},
keywords = {Methods: data analysis, Methods: statistical, Galaxies: distances and redshifts, Sample selection bias, Nearest neighbors, Large-scale learning},
abstract = {In astronomical applications of machine learning, the distribution of objects used for building a model is often different from the distribution of the objects the model is later applied to. This is known as sample selection bias, which is a major challenge for statistical inference as one can no longer assume that the labeled training data are representative. To address this issue, one can re-weight the labeled training patterns to match the distribution of unlabeled data that are available already in the training phase. There are many examples in practice where this strategy yielded good results, but estimating the weights reliably from a finite sample is challenging. We consider an efficient nearest neighbor density ratio estimator that can exploit large samples to increase the accuracy of the weight estimates. To solve the problem of choosing the right neighborhood size, we propose to use cross-validation on a model selection criterion that is unbiased under covariate shift. The resulting algorithm is our method of choice for density ratio estimation when the feature space dimensionality is small and sample sizes are large. The approach is simple and, because of the model selection, robust. We empirically find that it is on a par with established kernel-based methods on relatively small regression benchmark datasets. However, when applied to large-scale photometric redshift estimation, our approach outperforms the state-of-the-art.}
}


@inproceedings{vaswani17attention,
 author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Attention is All you Need},
 url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
 volume = {30},
 year = {2017}
}


@misc{news_art_2018,
url={https://www.studyinternational.com/news/unis-use-qr-codes-mark-attendance-now-theres-no-skipping-class/},
journal={Study International},
key=3,
year={February 13, 2018}
}

@article{bisht_hinrichs_skrupsky_venkatakrishnan_2014,
title={Automated detection of parameter tampering opportunities and vulnerabilities in web applications}, 
volume={22},
DOI={10.3233/jcs-140498},
number={3},
journal={Journal of Computer Security},
author={Bisht, Prithvi and Hinrichs, Timothy and Skrupsky, Nazari and Venkatakrishnan, V.N.},
year={2013},
pages={415-465}
}

@misc{fung_wang_cheung_wong_2014,
title={Scanning of real-world web applications for parameter tampering vulnerabilities},
author={Fung, Adonis P.H. and Wang, Tielei and Cheung, K. W. and Wong, T. Y.},
year={2014}
}

@INPROCEEDINGS{stanojevic22exactmapping,
  doi = {10.48550/ARXIV.2212.12522},
  url = {https://arxiv.org/abs/2212.12522},
  author = {Stanojevic, Ana and Woźniak, Stanisław and Bellec, Guillaume and Cherubini, Giovanni and Pantazi, Angeliki and Gerstner, Wulfram},
  keywords = {Neural and Evolutionary Computing (cs.NE), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {An Exact Mapping From ReLU Networks to Spiking Neural Networks},
  publisher = {arXiv},
  year = {2022},
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@INPROCEEDINGS{makhoul89firstlinearregions,
  author={Makhoul, J. and Schwartz, R. and El-Jaroudi, A.},
  booktitle={International Conference on Acoustics, Speech, and Signal Processing,}, 
  title={Classification capabilities of two-layer neural nets}, 
  year={1989},
  volume={},
  number={},
  pages={635-638 vol.1},
  doi={10.1109/ICASSP.1989.266507}}
  
  
@inproceedings{huang06sampleselectionbias,
author = {Huang, Jiayuan and Smola, Alexander J. and Gretton, Arthur and Borgwardt, Karsten M. and Scholkopf, Bernhard},
title = {Correcting Sample Selection Bias by Unlabeled Data},
year = {2006},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We consider the scenario where training and test data are drawn from different distributions, commonly referred to as sample selection bias. Most algorithms for this setting try to first recover sampling distributions and then make appropriate corrections based on the distribution estimate. We present a nonparametric method which directly produces resampling weights without distribution estimation. Our method works by matching distributions between training and testing sets in feature space. Experimental results demonstrate that our method works well in practice.},
booktitle = {Proceedings of the 19th International Conference on Neural Information Processing Systems},
pages = {601–608},
numpages = {8},
location = {Canada},
series = {NIPS'06}
}

@article{unterthiner2020predicting,
  title={Predicting neural network accuracy from weights},
  author={Unterthiner, Thomas and Keysers, Daniel and Gelly, Sylvain and Bousquet, Olivier and Tolstikhin, Ilya},
  journal={arXiv preprint arXiv:2002.11448},
  year={2020}
}

@inproceedings{maennel20whatnnlearnwhentrainedwithrandomsamples,
author={Maennel, Hartmut and Alabdulmohsin, Ibrahim and Tolstikhin, Ilya and Baldock, Robert JN and Bousquet, Olivier and Gelly, Sylvain and Keysers, Daniel},
title = {What Do Neural Networks Learn When Trained With Random Labels?},
year = {2020},
booktitle = {Proceedings of the 33rd International Conference on Neural Information Processing Systems},
series = {NIPS'20}
}



@inproceedings{grandvalet2005semi,
  title={Semi-supervised learning by entropy minimization},
  author={Grandvalet, Yves and Bengio, Yoshua},
  booktitle={Advances in neural information processing systems},
  pages={529--536},
  year={2005}
}


@article{møller_schwarz_2014,
title={Automated Detection of Client-State Manipulation Vulnerabilities},
volume={23},
DOI={10.1145/2531921},
number={4},
journal={ACM Transactions on Software Engineering and Methodology},
author={Møller, Anders and Schwarz, Mathias},
year={2014},
pages={1-30}
}

@article{peyre2019computational,
  title={Computational Optimal Transport: With Applications to Data Science},
  author={Peyr{\'e}, Gabriel and Cuturi, Marco and others},
  journal={Foundations and Trends{\textregistered} in Machine Learning},
  volume={11},
  pages={355--607},
  year={2019},
  publisher={Now Publishers, Inc.}
}


@article{masalha_hirzallah_2014,
title={A Students Attendance System Using QR Code},
volume={5},
DOI={10.14569/ijacsa.2014.050310},
number={3},
journal={International Journal of Advanced Computer Science and Applications},
author={Masalha, Fadi and Hirzallah, Nael},
year={2014}
}

%
%
@article{barth_jackson_mitchell_2008,
AUTHOR="Barth, A., Jackson, C., Mitchell, J. C.",
TITLE="Robust defenses for cross-site request forgery",
JOURNAL="Proceedings of the 15th ACM conference on Computer and communications security",
YEAR="2008",
PAGES={75-88}
}

@article{jovanoic_kirda_kruegel_2006,
AUTHOR="Jovanovic, N., Kirda, E., Kruegel, C.",
TITLE="Preventing cross site request forgery attacks",
JOURNAL="Securecomm and Workshops",
YEAR="2006",
PAGES={1-10}
}

@article{siddiqui_verma_2011,
AUTHOR="Siddiqui, M. S., Verma, D.",
TITLE="Cross site request forgery: A common web application weakness",
JOURNAL="Communication Software and Networks (ICCSN), 2011 IEEE 3rd International Conference",
YEAR="2011",
PAGES={538-543}
}

@article{yadav_parekh_2017,
AUTHOR="Yadav, P., Parekh, C. D.",
TITLE="A report on CSRF security challenges \& prevention techniques",
JOURNAL="Innovations in Information, Embedded and Communication Systems (ICIIECS), 2017 International Conference",
YEAR="2017",
PAGES={1-4}
}


@article{sharma_nagpal_2015,
AUTHOR="Sharma, P., Nagpal, B. ",
TITLE="A STUDY ON URL MANIPULATION ATTACK METHODS AND THEIR COUNTERMEASURES",
JOURNAL="International Journal of Emerging Technology in Computer Science \& Electronics (IJETCSE)",
YEAR="2015", 
VOLUME={15},
}

@article{gretton2012kernel,
  title={A kernel two-sample test},
  author={Gretton, Arthur and Borgwardt, Karsten M and Rasch, Malte J and Sch{\"o}lkopf, Bernhard and Smola, Alexander},
  journal={Journal of Machine Learning Research},
  volume={13},
  number={Mar},
  pages={723--773},
  year={2012}
}


@inproceedings{jitkrittum2018informative,
  title={Informative features for model comparison},
  author={Jitkrittum, Wittawat and Kanagawa, Heishiro and Sangkloy, Patsorn and Hays, James and Sch{\"o}lkopf, Bernhard and Gretton, Arthur},
  booktitle={Advances in Neural Information Processing Systems},
  pages={808--819},
  year={2018}
}

@article{Saygin2000,
  title={Turing Test: 50 Years Later},
  author={Saygin, A. Pinar and Cicekli, Ilyas and Akman, Varol},
  journal={Minds and Machines},
  volume={10},
  pages={463--518},
  year={2000},
  publisher={Kluwer Academic Publishers},
  doi={10.1023/A:1011288000451}
}

@article{Mitchell2022TheDO,
  title={The debate over understanding in AI’s large language models},
  author={Melanie Mitchell and David C. Krakauer},
  journal={Proceedings of the National Academy of Sciences of the United States of America},
  year={2022},
  volume={120},
  url={https://api.semanticscholar.org/CorpusID:253107905}
}

@inproceedings{hayes1995harmfulturing,
author = {Hayes, Patrick and Ford, Kenneth},
title = {Turing test considered harmful},
year = {1995},
abstract = {Passing the Turing Test is not a sensible goal for Artificial Intelligence. Adherence to Turing's vision from 1950 is now actively harmful to our field. We review problems with Turing's idea, and suggest that, ironically, the very cognitive science that he tried to create must reject his research goal.},
booktitle = {IJCAI},
}


@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}


@InProceedings{glorot11deepsparse,
  title = 	 {Deep Sparse Rectifier Neural Networks},
  author = 	 {Glorot, Xavier and Bordes, Antoine and Bengio, Yoshua},
  booktitle = 	 {Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics},
  pages = 	 {315--323},
  year = 	 {2011},
  editor = 	 {Gordon, Geoffrey and Dunson, David and Dudík, Miroslav},
  volume = 	 {15},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Fort Lauderdale, FL, USA},
  month = 	 {4},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v15/glorot11a/glorot11a.pdf},
  url = 	 {https://proceedings.mlr.press/v15/glorot11a.html},
  abstract = 	 {While logistic sigmoid neurons are more biologically plausible than hyperbolic tangent neurons, the latter work better for training multi-layer neural networks. This paper shows that rectifying neurons are an even better model of biological neurons and yield equal or better performance than hyperbolic tangent networks in spite of the hard non-linearity and non-differentiability at zero, creating sparse representations with true zeros which seem remarkably suitable for naturally sparse data. Even though they can take advantage of semi-supervised setups with extra-unlabeled data, deep rectifier networks can reach their best performance without requiring any unsupervised pre-training on purely supervised tasks with large labeled datasets. Hence, these results can be seen as a new milestone in the attempts at understanding the difficulty in training deep but purely supervised neural networks, and closing the performance gap between neural networks learnt with and without unsupervised pre-training.}
}


@article{mirzadeh2023relu,
  title={Relu strikes back: Exploiting activation sparsity in large language models},
  author={Mirzadeh, Iman and Alizadeh, Keivan and Mehta, Sachin and Del Mundo, Carlo C and Tuzel, Oncel and Samei, Golnoosh and Rastegari, Mohammad and Farajtabar, Mehrdad},
  journal={arXiv preprint arXiv:2310.04564},
  year={2023}
}

@article{Koch1904kochcurve,
  author = {Helge von Koch},
  title = {Sur une courbe continue sans tangente, obtenue par une construction géométrique élémentaire},
  journal = {Arkiv för matematik, astronomi och fysik},
  volume = {1},
  pages = {681--704},
  year = {1904},
  JFM = {35.0387.02}
}


@misc{openai2023gpt4,
      title={GPT-4 Technical Report}, 
      author={OpenAI},
      year={2023},
      eprint={2303.08774},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{bowling2015pokersolved,
author = {Michael Bowling  and Neil Burch  and Michael Johanson  and Oskari Tammelin },
title = {Heads-up limit hold’em poker is solved},
journal = {Science},
volume = {347},
number = {6218},
pages = {145-149},
year = {2015},
doi = {10.1126/science.1259433},
URL = {https://www.science.org/doi/abs/10.1126/science.1259433},
eprint = {https://www.science.org/doi/pdf/10.1126/science.1259433},
abstract = {One of the fundamental differences between playing chess and two-handed poker is that the chessboard and the pieces on it are visible throughout the entire game, but an opponent's cards in poker are private. This informational deficit increases the complexity and the uncertainty in calculating the best course of action—to raise, to fold, or to call. Bowling et al. now report that they have developed a computer program that can do just that for the heads-up variant of poker known as Limit Texas Hold 'em (see the Perspective by 
 Sandholm). Science, this issue p. 145; see also p. 122 A computer goes to Las Vegas. [Also see Perspective by Sandholm] Poker is a family of games that exhibit imperfect information, where players do not have full knowledge of past events. Whereas many perfect-information games have been solved (e.g., Connect Four and checkers), no nontrivial imperfect-information game played competitively by humans has previously been solved. Here, we announce that heads-up limit Texas hold’em is now essentially weakly solved. Furthermore, this computation formally proves the common wisdom that the dealer in the game holds a substantial advantage. This result was enabled by a new algorithm, CFR+, which is capable of solving extensive-form games orders of magnitude larger than previously possible.}}




@book{vonNeumannMorgenstern1944,
  author    = {John von Neumann and Oskar Morgenstern},
  title     = {Theory of Games and Economic Behavior},
  publisher = {Princeton University Press}, 
  address   = {Princeton, NJ},
  year      = {1944},
}

@inproceedings{reif2019visualingmeasuringgeometryofbert,
 author = {Reif, Emily and Yuan, Ann and Wattenberg, Martin and Viegas, Fernanda B and Coenen, Andy and Pearce, Adam and Kim, Been},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Visualizing and Measuring the Geometry of BERT},
 url = {https://proceedings.neurips.cc/paper_files/paper/2019/file/159c1ffe5b61b41b3c4d8f4c2150f6c4-Paper.pdf},
 volume = {32},
 year = {2019}
}


@inproceedings{hewitt2019structuralprobe,
    title = "{A} Structural Probe for Finding Syntax in Word Representations",
    author = "Hewitt, John  and
      Manning, Christopher D.",
    editor = "Burstein, Jill  and
      Doran, Christy  and
      Solorio, Thamar",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1419/",
    doi = "10.18653/v1/N19-1419",
    pages = "4129--4138",
    abstract = "Recent work has improved our ability to detect linguistic knowledge in word representations. However, current methods for detecting syntactic knowledge do not test whether syntax trees are represented in their entirety. In this work, we propose a structural probe, which evaluates whether syntax trees are embedded in a linear transformation of a neural network`s word representation space. The probe identifies a linear transformation under which squared L2 distance encodes the distance between words in the parse tree, and one in which squared L2 norm encodes depth in the parse tree. Using our probe, we show that such transformations exist for both ELMo and BERT but not in baselines, providing evidence that entire syntax trees are embedded implicitly in deep models' vector geometry."
}

@InProceedings{Fawzi2018empiricalstudyoftopology,
author = {Fawzi, Alhussein and Moosavi-Dezfooli, Seyed-Mohsen and Frossard, Pascal and Soatto, Stefano},
title = {Empirical Study of the Topology and Geometry of Deep Networks},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2018}
}

@article{wortsman2023replacing,
  title={Replacing softmax with relu in vision transformers},
  author={Wortsman, Mitchell and Lee, Jaehoon and Gilmer, Justin and Kornblith, Simon},
  journal={arXiv preprint arXiv:2309.08586},
  year={2023}
}

@article{Hashimoto2024symmetriesunification,
doi = {10.1088/2632-2153/ad5927},
url = {https://dx.doi.org/10.1088/2632-2153/ad5927},
year = {2024},
month = {jun},
publisher = {IOP Publishing},
volume = {5},
number = {2},
pages = {025079},
author = {Koji Hashimoto and Yuji Hirono and Akiyoshi Sannai},
title = {Unification of symmetries inside neural networks: transformer, feedforward and neural ODE},
journal = {Machine Learning: Science and Technology},
abstract = {Understanding the inner workings of neural networks, including transformers, remains one of the most challenging puzzles in machine learning. This study introduces a novel approach by applying the principles of gauge symmetries, a key concept in physics, to neural network architectures. By regarding model functions as physical observables, we find that parametric redundancies of various machine learning models can be interpreted as gauge symmetries. We mathematically formulate the parametric redundancies in neural ODEs, and find that their gauge symmetries are given by spacetime diffeomorphisms, which play a fundamental role in Einstein’s theory of gravity. Viewing neural ODEs as a continuum version of feedforward neural networks, we show that the parametric redundancies in feedforward neural networks are indeed lifted to diffeomorphisms in neural ODEs. We further extend our analysis to transformer models, finding natural correspondences with neural ODEs and their gauge symmetries. The concept of gauge symmetries sheds light on the complex behavior of deep learning models through physics and provides us with a unifying perspective for analyzing various machine learning architectures.}
}


@article{radford2019language,
  title={Language Models are Unsupervised Multitask Learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  journal={OpenAI Blog},
  year={2019},
  url={https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf}
}


@article{Sejnowski2022LargeLM,
  title={Large Language Models and the Reverse Turing Test},
  author={Terrence J. Sejnowski},
  journal={Neural Computation},
  year={2022},
  volume={35},
  pages={309-342},
}

@book{elo1978rating,
  title={The Rating of Chess Players, Past and Present},
  author={Elo, Arpad E.},
  year={1978},
  publisher={Arco Publishing},
}


@inproceedings{herbrich2006trueskill,
 author = {Herbrich, Ralf and Minka, Tom and Graepel, Thore},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {B. Sch\"{o}lkopf and J. Platt and T. Hoffman},
 pages = {},
 publisher = {MIT Press},
 title = {TrueSkill\texttrademark : A Bayesian Skill Rating System},
 volume = {19},
 year = {2006}
}


@article{Zhang2022HumanOM,
  title={Can Machines Imitate Humans? Integrative Turing Tests for Vision and Language Demonstrate a Narrowing Gap},
   author={Zhang, Mengmi and Dellaferrera, Giorgia and Sikarwar, Ankur and Armendariz, Marcelo and Mudrik, Noga and Agrawal, Prachi and Madan, Spandan and Barbu, Andrei and Yang, Haochen and Kumar, Tanishq and others},
  journal={ArXiv},
  year={2022},
  volume={abs/2211.13087},
  url={https://api.semanticscholar.org/CorpusID:253801749}
}

@inproceedings{hingston2010new,
  title={A new design for a turing test for bots},
  author={Hingston, Philip},
  booktitle={Proceedings of the 2010 IEEE Conference on Computational Intelligence and Games},
  pages={345--350},
  year={2010},
  organization={IEEE}
}

@article{Gunderson1964ImitationGame,
  title={The Imitation Game},
  author={Gunderson, Keith},
  journal={Mind},
  volume={73},
  number={290},
  pages={234--245},
  year={1964},
  publisher={Oxford University Press}
}


@article{Marcus2016BeyondTuring,
  title={Beyond the Turing Test},
  author={Marcus, Gary and Rossi, Francesca and Veloso, Manuela},
  journal={AI Magazine},
  volume={37},
  number={1},
  pages={34},
  year={2016},
  publisher={AAAI Press}
}


@article{orallo2000beyond,
 ISSN = {09258531, 15729583},
 URL = {http://www.jstor.org/stable/40180237},
 abstract = {The main factor of intelligence is defined as the ability to comprehend, formalising this ability with the help of new constructs based on descriptional complexity. The result is a comprehension test, or C-test, which is exclusively defined in computational terms. Due to its absolute and non-anthropomorphic character, it is equally applicable to both humans and non-humans. Moreover, it correlates with classical psychometric tests, thus establishing the first firm connection between information theoretical notions and traditional IQ tests. The Turing Test is compared with the C-test and the combination of the two is questioned. In consequence, the idea of using the Turing Test as a practical test of intelligence should be surpassed, and substituted by computational and factorial tests of different cognitive abilities, a much more useful approach for artificial intelligence progress and for many other intriguing questions that present themselves beyond the Turing Test.},
 author = {Jose Hernandez-Orallo},
 journal = {Journal of Logic, Language, and Information},
 number = {4},
 pages = {447--466},
 publisher = {Springer},
 title = {Beyond the Turing Test},
 urldate = {2024-01-26},
 volume = {9},
 year = {2000}
}



@article{jones2023does,
  title={Does GPT-4 Pass the Turing Test?},
  author={Jones, Cameron and Bergen, Benjamin},
  journal={arXiv preprint arXiv:2310.20216},
  year={2023}
}


@misc{CouncilEU2023,
  author = {{Council of the European Union}},
  title = {Artificial Intelligence Act: Council and Parliament Strike a Deal on the First Worldwide Rules for AI},
  year = {2023},
  howpublished = {\url{https://www.consilium.europa.eu/en/press/press-releases/2023/12/09/artificial-intelligence-act-council-and-parliament-strike-a-deal-on-the-first-worldwide-rules-for-ai/}},
  note = {Accessed: 24.01.2024}
}

