\section{Related work}
\label{sec:related_work}


\paragraph{Activation Space.}
The pioneering study by____ has investigated  partitioning the input space with neural networks with 2-hidden-layers,  thresholding neurons with the ReLU activation function (without naming it). 
With two hidden layers, the first layer creates hyperplanes that divide the input space into regions, adjacent if they have a Hamming distance of 1____. Connected regions are defined by a path through adjacent regions.  The interest in the number of these regions was revived in 2014 by____, with several follow-up works, e.g.,____. The authors provided ever tighter  bounds on the number of activation regions, and  used them as a proxy for its expressiveness, among others.  


\paragraph{Space Folds.} The idea of  folding the space has been investigated in the computational geometry____.
____ surveyed the phenomenon, focusing on the type of object being folded, e.g., paper, or polyhedra.
____ explored whether a given crease pattern can be folded into a flat origami (non-crossing polygons in 2D with layers). Later, ____ showed that any compact, orientable, piecewise-linear 2-manifold with a Euclidean metric can achieve this structure.
In____ in Section 2.4, the authors briefly mentioned the folding phenomena, although through the lens of  linear regions. They argue that each hidden layer in a neural network acts as a folding operator, recursively collapsing input-space regions. This folding depends on the network's weights, biases, and activation functions, resulting in input regions that vary in size and orientation, highlighting the network's flexible partitioning. 
In____, in the Appendix A.2 the authors explored the folding operation by ReLU neural networks, but leave the exploration quite early on.  In____, the authors argued that it is through the folding operation that the neural networks arrive at their approximation power. 

\paragraph{Self-Similarity and Symmetry.} 
Self-similarity and symmetry are related but distinct concepts, often found in nature, mathematics, and physics. Self-similarity means that a structure or pattern looks similar to itself at different scales, and is also present in numerical data, e.g., images____, audio tracks____ or videos____. Symmetry implies that an object or pattern is invariant under certain transformations, e.g., reflection, rotation, or translation. In the context of neural networks, in____, the authors describe a number of mechanisms through which hidden symmetries can arise. Their experiments indicate that the probability that a network has no hidden symmetries decreases towards 0 as depth increases, while increasing towards 1 as width and input dimension increase. Many fractal shapes, such as the Mandelbrot set____ or CantorNet____, exhibit both self-similarity and certain symmetries.
Moreover, both concepts relate to the folding operation: invariance under reflection (symmetry) can be equivalently understood as a space fold, and self-similarity can be interpreted as recursive folding or scaling operations that replicate the pattern across different levels. In neural network architectures, these principles can manifest through hierarchical structures, where each layer effectively ``folds'' information from previous layers, producing patterns that may repeat or reflect across layers or nodes____.



\paragraph{Distance Alteration.}
Lipschitz constant, a well established concept in mathematical analysis,  bounds  how much function's output can change in proportion to a change in its input. 
In context of neural networks, it has been linked to adversarial robustness, e.g.,____, or generalization properties, e.g.,____. ____ showed that the Lipschitz constant of a neural network can grow exponentially with its depth.
____ observe  that enforcing the Lipschitz property leads to some limitations, and show that   norm-constrained
ReLU networks are less expressive than unconstrained ones.  The exact computation of the Lipschitz constant, even for shallow neural networks (two layers), is NP-hard____. In____, the authors experimentally studied  the (empirical) Lipschitz constant of deep networks undergoing double descent, and highlighted non-monotonic trends strongly correlating with the test error.
Finally, ____ prove that the expected length distortion slightly shrinks  for ReLU networks with standard random
initialization, building on the results of____.
While important, none of the aforementioned works touch on the activation space of neural networks, nor do they investigate the monotonicity of a mapped straight line. Our analysis extends beyond the concept of the Lipschitz constant by exploring the convolution of the input space under a neural network.