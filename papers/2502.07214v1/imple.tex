\label{sec:scalability}
In Section~\ref{subsec:algo}, we show that finding all Pareto optimal on the actionability graph is crucial to the size of the Pareto tables $\tau$, edge size $|E|$, and number of iterations $\eta$. Since $\eta$ corresponds to the number of hops of the output paths, which is usually as a constant (considering in reality that a long feasible path is redundant and non-interpretable), the bottleneck of the running time is mainly on $\tau$ and $|E|$. Additionally, $\tau$ is bounded by the number of different paths from each pair of nodes, which highly depends on the size of the vertices and the connectivity. To decrease the graph size and simplify the connectivity structure, one idea is to shrink the vertices of the graph such that there are only a small number of ``representative'' nodes, and the shortest path in this shrunk graph still preserves or approximates the distance of the original graph. This is the idea of core-set from the computational geometry perspective (see survey in~\cite{agarwal2005geometric}). The challenge here is that the cost functions are not specific but highly general. Additionally,  we also need to incorporate all the $k$ cost functions to get the Pareto optimal. Fortunately, the computation of the cost functions usually contains some structures rather than arbitrary values for any pair of points. This inspires us to utilize the idea of $\epsilon$-net~\cite{haussler1986epsilon} to shrink the size of the graph and also ensure the quality. 

To explicitly explain our idea, we first define the notation of shrinkable.''

\begin{definition}
\label{def: shrinkable}
    Given $G=(V,E)$ and a cost function $c$, we say a vertex $i$ is $\kappa$-shrinkable to vertex $j$ if and only if $\forall (p,i) \in E$
    $$ 
    (p,j) \in E \text{ and } c(p,j) \leq \kappa c(p,i)
    $$
\end{definition}

Given the approximation factor $\kappa$, one can iteratively shrink all the shrinkable vertices in the graph until there is no shrinkable vertex anymore. We call this induced subgraph a shrunk graph $G_S$ and the one with the smallest cardinality is $G_S^*$. Obviously, any $G_S$  preserves $\kappa$-approximation factor. The shortest path between any pair of the nodes in $G$ has another path in $G_S$ which is at most $\kappa l$ times, where $l$ is the number of the hops of the path. However, finding the $G_S^*$ is highly non-trivial. It depends on the order of vertices in the shrinking procedure. A toy example is in the following. Consider a graph with vertices $\{p,i,j,r\}$ and edges $\{(p,i),(p,j),(p,r)\}$ where $c(p,i)=\kappa c(p,j)= \kappa^2 c(p,r)$, if $j$ shrinks to $i$, then $i,r$ are not shrinkable. On the other hand, if $i$ shrinks to $j$, then $r$ can shrink to $j$ too. Thus, we want to have another subgraph that catches most properties of $G_S$ and can be generated efficiently. This is the place where the $\epsilon$-net joins into our work. In the following, we will first introduce the formal definition of $\epsilon$-net and then show how to utilize it under our context.

\begin{definition}
\label{def: epsilon-net}
Given a range space $(\mathcal{X}$,$\mathcal{R})$, let $\mathcal{A} \subset \mathcal{X}$ be a finite subset, and $0<\epsilon<1$. Then a subset $\mathcal{N} \subset \mathcal{A}$ is called an $\epsilon$-net of $\mathcal{A}$ w.r.t to $\mathcal{R}$ if 

$$
\forall r \in \mathcal{R}, |r \cap \mathcal{A}|>\epsilon |\mathcal{A}| \rightarrow r\cap \mathcal{N} \neq \emptyset
$$
\end{definition}

Now, we define the $\epsilon$-net under our context. 

\begin{definition}
\label{def: our epsilon-net}
We say $G_\epsilon$ is an $\epsilon$-net of $G$ if for any vertex $v$ in some shrunk graph $G_S$ that is shrunk by more than $\epsilon n$ vertices, then either $v \in G_\epsilon$ or $u \in G_\epsilon$, where $v$ is shrinkable to $u$.    
\end{definition}

To see Definition~\ref{def: epsilon-net} and Definition~\ref{def: our epsilon-net} are equivalent, one can see the element $r$ of a range $r \in \mathcal{R}$ is a subset of $V$ which is an instance of the shrinking procedure. That is all the elements in $r$ shrink into a point in some graph $G_S$. Thus, an $\epsilon$-net should include one of the points in $r$, which leads to Definition~\ref{def: our epsilon-net}. The interpretation of $G_\epsilon$ is that it includes the majority of vertices (i.e., which is shrunk from $\epsilon n$ vertices) among all the $G_S$.

Notice that with an arbitrary cost function, one cannot have an $G_\epsilon$ or a $G_S$ with a small size. However, if the cost function has some structure, one can analyze the VC-dimension of the range space and utilize the theorem proved by Haussler-Welzl~\cite{haussler1986epsilon}, which states that any random sample set $S$ of $G$ with size 
\begin{equation}
\label{eq:Haussler-Welzl}
O(\frac{|VC|}{\epsilon} \log \frac{1}{\epsilon} + \frac{1}{\epsilon} \log \frac{1}{\delta})  
\end{equation}
is an $\epsilon$-net, with probaility more than $1-\delta$. In addition, we can use one sample set $S$ to fit all the cost functions. Thus, assume $\delta$ is a constant, the size of $S$ for the multi-cost function is $O(|VC|^*/\epsilon \log 1/\epsilon)$, where $|VC|^*$ is the largest VC-dimension among all the cost functions, which is highly scalable in respect to the graph size $n$ and the number of cost functions $k$.

\paragraph{Demonstration}
We demonstrate how to analyze the $VC$-dimension for a cost function $c$ with certain properties to generate the $\epsilon$-net. Assume function $c$ has a property that for every $(i,j) \in E$,  $\Delta(i,j) \leq c(i,j) \leq \kappa \Delta(i,j)$, where $\Delta(i,j)$ is the distance metric of the data space. Notice that this property is the same as discrete Lipschitz continuity~\cite{jiang2011free} except it also has the lower bound on $c(i,j)$. This property is commonly true when the cost function has a similar structure in the metric of the data space, L-norm class for instance. In real analysis, it can be interpreted that the cost of two points cannot be too high concerning the distance of the data space (e.g., the effort of increasing the income from 10,000 to 12,000 can not be too large) but also not too low if the distance of the data space is far (e.g., there should be non-negligible effort of increasing the income from 8,000 to 15,000). 

Now, observe that this property implies that the two points are $\kappa$-shrinkable if and only if they are within the distance of 1 in the data space. Thus, an instance of the shrinking procedure is the same as putting a ball with a radius $1/2$ in the data space and all the points in this ball can shrink into a point. We can see the range $\mathcal{R}$ is the balls in the data space and The $\epsilon$-net here is asking what is the smallest sample set so that any ball containing at least $\epsilon n$ points also contains one of the sampled points. Fortunately, the $VC$-dimension of shattering the points via balls is at most shattering the points via the hyperplanes, which has the VC-dimension as $d+1$. Via Haussler-Welzl's theorem in Equation~\ref{eq:Haussler-Welzl}, a random sampled set with size $O( (d+1)^*/\epsilon \log 1/\epsilon)$ is an $\epsilon$-net for function $c$.  