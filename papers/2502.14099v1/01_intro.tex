\section{Introduction}
\label{sec:intro}
\glspl{pc} have emerged as a fundamental representation for spatial data, comprising collections of points sampled from object surfaces in 3D space. Each point is characterized by its spatial coordinates and may include additional attributes such as color components and surface normals. The prominence of 3D representations, particularly PCs, has grown significantly due to three key factors: their ability to create immersive environments, their support for six-degrees-of-freedom navigation, and their capacity for accurate environmental representation. These characteristics have established PCs as the de facto standard in various applications, including virtual and augmented reality, autonomous navigation, and cultural heritage preservation \cite{camuffo2022recent}.  However, achieving high-fidelity scene representation often requires PCs with millions of points, resulting in substantial storage and bandwidth demands. 

The growing importance of PCs, coupled with their considerable raw data size, has made the development of efficient \gls{pcc} algorithms crucial for practical applications.
\IEEEpubidadjcol 
\gls{pcc} presents unique challenges due to two inherent characteristics of PCs: their unstructured nature and the non-uniform point density resulting from acquisition processes. To address these challenges, standardization efforts have emerged:
\begin{enumerate}
    \item MPEG has developed two distinct standards \cite{graziosi2020overview}:
    \begin{itemize}
        \item Geometry-based Point Cloud Compression (G-PCC), originally targeting static or dynamically acquired PCs (e.g. LiDAR PCs);
        \item Video codec-based Point Cloud Compression (V-PCC), originally targeting dynamic PCs.
    \end{itemize}
    \item JPEG is finalizing the development of \gls{jpeg-pcc} \cite{guarda2024jpeg}, the first learning-based standard for static \gls{pcc}.
\end{enumerate}

JPEG's effort in particular, being \gls{jpeg-pcc} learning-based, aims to create a standard that delivers competitive \gls{rd} performance while providing effective compressed domain representations for both human visualization and machine processing \cite{seleem2023deep}.

While RD performance is crucial, real-world applications demand additional features, including stream scalability. Scalability enables serving content at various qualities, resolutions, or framerates through a single bitstream, rather than maintaining multiple independent streams. This capability becomes particularly valuable given the heterogeneity of receiving devices, which often operate under different network conditions and hardware constraints. Implementing this feature can thus allow a codec to properly adapt to a wide variety of receiving conditions.
A scalable bitstream is generally organized into a base layer providing minimal rate decoding capabilities, and multiple enhancement layers allowing progressive refinement of the visual fidelity.

The growing importance of scalability in point cloud coding has prompted standardization organizations to incorporate this feature into their codec requirements \cite{jpeg-pleno-cttc} and to propose extensions to existing solutions \cite{vpccscal}. In particular, according to these requirements, some of the most relevant forms of scalability for static PCs are:
\begin{itemize}
    \item Quality Scalability: Enhancement layers improve visual fidelity without affecting resolution.
    \item Resolution Scalability: Enhancement layers increase content resolution, with quality improvement being a secondary effect.
\end{itemize}

Recent advancements in this direction can be found in \cite{mari2024point} 
where the authors presented a method for geometry quality scalability in \gls{jpeg-pcc} by working in the latent domain. Even if only quality scalability is considered, the approach demonstrates how visual fidelity can be progressively enhanced with minimal additional information by correctly exploiting the previously encoded base and enhancement layers.

Some PC codecs (e.g. \gls{jpeg-pcc}) reduce the resolution of the input PC as a form of point domain quantization, controlled by a scaling factor $sf$. Downscaling is thus an effective RD tuning strategy for PCs since it helps obtaining lower bitrates through the reduction of the information in the content. Additionally, it can help increasing the spatial redundancy in sparser PCs (making them more efficient to compress). Therefore, reducing the resolution of the input is not only useful for addressing the needs of devices that don't support higher resolutions, but it is also an effective way to improve RD performance and increase the number of covered rate points. 

While \gls{jpeg-pcc} includes a learned super-resolution module to reverse this process, it does not provide true resolution scalability as it operates without enhancement layers, potentially resulting in lower visual fidelity compared to non-super-resolved reconstructions.
Additionally, existing solutions, like Mari et al. \cite{mari2024point} only address quality scalability, so they cannot handle bitstreams for PC coded using different scaling factors. This limitation restricts the codec's adaptability and prevents full utilization of its capabilities when scalable bitstreams are required. 

To address these challenges, this paper proposes Scalable Resolution and Quality Hyperprior (\gls{esqh}), a novel approach providing joint resolution and quality scalability for PC geometry coding.  Integrated into  
\gls{jpeg-pcc}, \gls{esqh} enables scalability while maintaining compatibility with the base codec's non-scalable operation mode. This integration is achieved by replacing the hyper-analysis and hyper-synthesis transforms with the \gls{squlpe} model during enhancement layer coding.

The main key innovations and contributions brought by \gls{esqh} are:
\begin{enumerate}
    \item Joint scalability of point cloud resolution and quality: \gls{esqh} enables \gls{jpeg-pcc} to serve diverse devices through a single scalable bitstream. 
    \item Minimal RD performance impact: the proposed approach pays a small price for scalability compared to non-scalable \gls{jpeg-pcc} where, for one RD point, the PC can be decoded only at one specific resolution and quality.
    \item Reduced memory requirements: \gls{esqh} requires smaller neural networks compared to state-of-the-art solutions.
    \item Minimal computational overhead: when using \gls{esqh} encoding time increases by less than $<$10\% and decoding time by less than $<$20\% per decoded enhancement layer w.r.t. \gls{jpeg-pcc}.
    \item Generic design: \gls{esqh} can be easily integrated in autoencoder-based deep learning codecs for other multimedia modalities.
    \item Correlated latent spaces: this work shows that the correlation between the latent spaces in the different \gls{jpeg-pcc} models arises from sequential training. This property helps providing effective scalability algorithms and it can be easily introduced in other autoencoder based codecs such as \cite{balle2018variational, minnen2018joint, quach2020improved}
\end{enumerate}

The rest of the paper is organized as follows. Section~\ref{sec:related} overviews
the current state-of-the-art in \gls{pc} coding, while Section~\ref{sec:vm} describes the \gls{jpeg-pcc} codec, serving as reference and base layer codec for the proposed approach and the \gls{sqh} algorithm proposed in \cite{mari2024point} which is improved upon in this work. 
Section~\ref{sec:extending_sqh} proposes the novel \gls{esqh} algorithm for joint resolution and quality scalability across a wide variety of coding configurations, and the newly proposed \gls{squlpe} which is the main neural network used in \gls{esqh}. Finally, Section~\ref{sec:rqulpe results} reports
and analyses the most relevant experimental results and Section~\ref{sec:conclusions}
discusses future work directions.