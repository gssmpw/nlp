\section{Related Works}
\label{sec:related}
State-of-the-art PCC encompasses various approaches, ranging from traditional signal processing techniques to modern learning-based solutions. Among the conventional approaches, the most relevant are G-PCC and V-PCC \cite{graziosi2020overview}, the two MPEG standards for \gls{pcc}.
G-PCC, or Geometry-based Point Cloud Compression, leverages octree representations for efficient geometry coding, and uses predictive or hierarchical transforms for attribute coding. On the other hand, V-PCC, or Video-based Point Cloud Compression, projects 3D PC data into the 2D domain, creating images that represent the geometry and texture information, which are compressed using very efficient and established video codecs like HEVC and VVC \cite{bross2021overview}.
While G-PCC inherently supports resolution scalability for both geometry and attributes, achieving scalability in V-PCC presents challenges due to its reliance on video coding frameworks. Although MPEG has initiated investigations into various scalability techniques for V-PCC \cite{vpccscal}, these features remain to be specified in the current version of the standard.

More recently, the advent of \gls{dl} has revolutionized PC compression, yielding numerous high-performing solutions \cite{guarda2024jpeg,quach2020improved,wang2021lossy, wang2022sparse, liu2022pcgformer}. Nevertheless, among the many \gls{dl}-based PCC solutions, few approaches address any form of scalability. DL-PCSC \cite{guarda2020point} implements quality scalability by channelwise partitioning of latent representations, enabling progressive quality enhancement through incremental transmission. However, this approach faces limitations: the requirement for zero-padding untransmitted latents constrains the latent space design, and the reduced latent space dimensionality at lower rates leads to reduced modeling capabilities \cite{balle2018variational}. These constraints significantly impact the rate-distortion performance making scalability less appealing.

GRASP-Net \cite{pang2022grasp} offers an alternative approach, by implementing a \gls{dl}-based enhancement layer atop a G-PCC base layer. However, scalability is limited to this two-layer structure, and the extremely low-resolution base layer may prove impractical for real-world applications.

The work by Ulhaq et al. \cite{ulhaq2024scalable} implements scalability by adapting content for human and machine consumption. In particular, the base layer can be used to solve computer vision tasks (e.g. PC classification) while the enhancement layer allows reconstructing the PC for human visualization. This approach thus addresses scalability requirements that are different from those explored in this work.

SparsePCGC \cite{wang2022sparse} and its successor, Unicorn \cite{wang2024versatile1,wang2024versatile2}, represent significant advances in resolution scalability for PCC, due to their inherently multiscale nature. At the encoder side, Unicorn employs a hierarchical downscaling approach, encoding the information necessary for losslessly upscaling it at the decoder.
When this enhancement information is unavailable, the decoder employs a lossy thresholding strategy for upscaling.
By dividing the bitstream in different enhancement layers, required to upscale the PC, Unicorn achieves resolution scalability. Furthermore, Unicorn has a very competitive coding performance when compared with other PC codecs. The intrinsic scalability mechanism of Unicorn, which results from its architecture, contrasts with \gls{esqh}, which offers a modular, plug-and-play solution applicable to various codecs.

Additionally, it is also important to mention that recently the MPEG group issued the call for proposal for the new AI-based PCC \cite{aigccfp} which is currently under development.