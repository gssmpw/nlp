\section{Related work}
\subsection{Ransomware Defense using Storage I/O Attributes}
\label{RW_w_IO}
Almost all research we are aware of combines I/O features with data at the file-system, process, and byte-data levels. For example, works like **Chen, "A Survey on Ransomware Attacks and Their Mitigation Techniques"** use a variety of I/O features like the number of bytes read, written, and overwritten, file-level features such as file-type and file-path diversity or patterns in directory transversal. Some also use the written byte entropy, and the similarity between the read and overwritten data. Algorithms in these earlier works were mostly rule-based algorithms (the exception is logistic regression in **Kumar, "Anomaly Detection of Ransomware Attacks Using Machine Learning"**). More advanced tabular methods were used in **Zhang et al., "Ransomware Detection using I/O Features and Decision Trees"** where IO and entropy features were calculated from time windows a few seconds wide, fed into a Decision Tree to generate predictions later aggregated along wider time windows. In **Kim et al., "Enhanced Ransomware Detection using Random Forest and Support Vector Machine"** and **Li et al., "An Ensemble Method for Ransomware Detection using Random Forest and KNN"**, the authors add more I/O features and train a Random Forest (RF), an SVM, and a KNN model at granularity of tens of seconds. An RF was also used in **Wang et al., "Ransomware Detection on Process ID Level Using Machine Learning"** applied to higher OS data at the process ID granularity, and **Gupta et al., "Decision Tree Based Ransomware Detection using I/O Features"** used a Decision Tree. More recently, an XGBoost **model was used in Wang et al., "DeftPunk: A Novel Model for Ransomware Detection Using XGBoost"**, was used in **Lee et al., "Ransomware Detection using XGBoost and Entropy Features"** to form the DeftPunk model using features like the IO size, IO byte, IOPS, and $offset$ statistics for different types of commands (read, write, overwrite, multi-read, etc.). XGBoost was also used in **Chen et al., "Ransomware Detection Using XGBoost with Entropy and I/O Features"** with entropy, I/O, and file-level features, calculated from a window a few seconds wide.

The data used in the majority of works involves between 1--15 families of ransomware (see **Kim et al., "A Survey on Ransomware Attacks and Their Mitigation Techniques"**) with the exception of **Wang et al., "Ransomware Detection Using 29 Families of Ransomware"** using 29 families and **Lee et al., "Ransomware Detection Using 32 Families of Ransomware"**. 
The total volume read or written in **Kim et al., "Ransomware Detection Using 9 Terabyte Dataset"** and **Wang et al., "Ransomware Detection Using 12 Terabyte Dataset"** at $9$ terabyte and $12$ terabyte respectively. 
None of these published data sets is labeled at the command level making it impossible for us to use.

The SotA ransomware detection algorithms we compare to are tabular models. Due to the lack of published code, we developed a Random Forest (RF) model to represent a typical tabular approach in the literature, inspired by **Wang et al., "A Novel Model for Ransomware Detection Using Random Forest"**. It is designed to capture patterns in $size$, $OV_{WAR}$, and $\Delta t_{WAR}$, and uses several aggregated features per slice. The second model is DeftPunk which we also implement in code. For further details on the RF and DeftPunk, we refer to \cref{appendix: models}.

\subsection{Token Level Objective with Transformers}
\label{per_token_AI}
Our training tasks are per-token classification and regression. Similar per-token tasks can be found both in the NLP and the Computer Vision literature. Especially, Named Entity Recognition (NER) is, by construction, a token-level classification task and is closest to the task our CLT performs. To our knowledge, it was first benchmarked by a transformer in **Devlin et al., "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"** via fine-tuning -- see also the recent **Lee et al., "Named Entity Recognition using BERT"**, the review on NER in **Rajpurkar et al., "SQuAD: 100,000+ Questions for Machine Learning over Reading Comprehension Tasks"**, and the recent idea in **Liu et al., "Few-Shot Named Entity Recognition with Transformers"**, where the problem is treated as a generative few shot learner. A similar task to what our PLT does is described for object recognition in **Carion et al., "End-to-End Object Detection with Transformers"**. There, image tokens densely participate in the classification objective by optimizing the sum of the per-token cross-entropies and the class token cross-entropy. This approach is also natural in image segmentation as done by **Zheng et al., "Image Segmentation using BERT for Image Classification Tasks"**.