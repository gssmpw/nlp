\section{Related work}
\subsection{Ransomware Defense using Storage I/O Attributes}
\label{RW_w_IO}
Almost all research we are aware of combines I/O features with data at the file-system, process, and byte-data levels. For example, works like ____ use a variety of I/O features like the number of bytes read, written, and overwritten, file-level features such as file-type and file-path diversity or patterns in directory transversal. Some also use the written byte entropy, and the similarity between the read and overwritten data. Algorithms in these earlier works were mostly rule-based algorithms (the exception is logistic regression in ____). More advanced tabular methods were used in ____ where IO and entropy features were calculated from time windows a few seconds wide, fed into a Decision Tree to generate predictions later aggregated along wider time windows. In ____ and ____ the authors add more I/O features and train a Random Forest (RF), an SVM, and a KNN model at granularity of tens of seconds. An RF was also used in ____ applied to higher OS data at the process ID granularity, and ____ used a Decision Tree. More recently, an XGBoost ____, was used in ____ to form the DeftPunk model using features like the IO size, IO byte, IOPS, and $offset$ statistics for different types of commands (read, write, overwrite, multi-read, etc.). XGBoost was also used in ____ with entropy, I/O, and file-level features, calculated from a window a few seconds wide.

The data used in the majority of works involves between 1--15 families of ransomware (see ____) with the exception of ____ using 29 families and ____ using 32. 
The total volume read or written in ____ and ____ at $9$ terabyte and $12$ terabyte respectively. 
None of these published data sets is labeled at the command level making it impossible for us to use.

The SotA ransomware detection algorithms we compare to are tabular models. Due to the lack of published code, we developed a Random Forest (RF) model to represent a typical tabular approach in the literature, inspired by ____. It is designed to capture patterns in $size$, $OV_{WAR}$, and $\Delta t_{WAR}$, and uses several aggregated features per slice. The second model is DeftPunk which we also implement in code. For further details on the RF and DeftPunk, we refer to \cref{appendix: models}.

\subsection{Token Level Objective with Transformers}
\label{per_token_AI}
Our training tasks are per-token classification and regression. Similar per-token tasks can be found both in the NLP and the Computer Vision literature. Especially, Named Entity Recognition (NER) is, by construction, a token-level classification task and is closest to the task our CLT performs. To our knowledge, it was first benchmarked by a transformer in ____ via fine-tuning -- see also the recent ____, the review on NER in ____, and the recent idea in ____, where the problem is treated as a generative few shot learner. A similar task to what our PLT does is described for object recognition in ____. There, image tokens densely participate in the classification objective by optimizing the sum of the per-token cross-entropies and the class token cross-entropy. This approach is also natural in image segmentation as done by ____ with BERT.