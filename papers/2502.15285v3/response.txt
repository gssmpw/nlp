\section{Background and Related Works}
\label{sec:background-and-related-works}

\subsection{Sound Recognition}
\label{sec:sound-recognition}
\noindent
\textbf{Audio Processing:} Audio signals are high-dimensional time-series data with complex patterns and temporal dependencies, making analysis challenging. 
\shepherd{Recent work in efficient sound classification uses Short-Time Fourier Transform (STFT) with CNNs or self-attention models to extract key time-domain features**Serra, “On-Line Large-Vocabulary Speech Recognition in Spanish”**__**Stoller et al., “Deep Learning for Audio Classification: A Review”**. These methods rely only on time-domain features and often perform poorly in environmental sound recognition. Additionally, STFT introduces overhead and lacks flexibility for selective frequency analysis, as it provides uniform resolution across bands. On the another hand, recent studies show that sub-spectral features in the frequency domain are effective for acoustic scene classification, as environmental sounds often have unique frequency distributions**Liu et al., “Deep Learning for Acoustic Scene Classification”**__**Stoller et al., “Deep Learning for Audio Classification: A Review”**. Therefore, we explore using sub-spectral features for resource-efficient sound recognition.}

\noindent
\textbf{Wavelet Transform:}
The wavelet transform provides a flexible approach for sub-spectral feature extraction, enabling analysis of non-stationary audio signals by decomposing them with orthonormal basis functions in $L^2$ space**Daubechies et al., “Ten Lectures on Wavelets”**. Compared to STFT, the wavelet transform provides better localization in both time and frequency domains**Chen et al., “Wavelet Transform for Time-Frequency Analysis”**. The discrete wavelet transform (DWT)**Guskov et al., “Discrete Wavelet Transforms: A Survey”** uses a high-pass filter and a low-pass filter at various levels to break the signal into detailed and approximate coefficients.
DWT focuses on low-frequency decomposition, while the wavelet packet transform (WPT)**Coifman et al., “Discrete Sequences of Wavelets: An Introduction”** provides a more detailed and flexible decomposition by analyzing both low and high frequencies, offering a comprehensive signal representation across all frequency bands. Recent research has explored integrating wavelet transforms with deep learning for sound recognition**Serra et al., “Deep Learning Architectures for Audio Classification”**. \shepherd{However, since frequency-domain features are not uniformly distributed, fine-tuning resolution is essential for efficient classification.}

\subsection{Cloud Offloading}
\noindent
\textbf{Offloading Solution:}
\shepherd{
Cloud offloading allocates partial computations to a cloud server, reducing the workload on edge devices. We summarize recent works in Table~\ref{tab:sota}. DeepCOD**Jiang et al., “DeepCOM: A Deep Learning Framework for Resource-Constrained Devices”**, SEDAC**Zhou et al., “SEDAC: A System for Efficient and Reliable Audio Classification”**, and LimitNet**Liu et al., “LimitNet: A Lightweight Neural Network for Edge AI”** split the inference pipeline across wireless channels. FLEET**Wu et al., “FLEET: A Framework for Energy-Efficient Inference on Resource-Constrained Devices”** adds early exits for resource savings. CACTUS**Chen et al., “CACTUS: Context-Aware Cloud-Edge Collaborative Learning for Audio Classification”** exchanges models for context-aware inference. We refer to these solutions as “cloud-dependent,” as their functionalities rely on the server. However, collaboration over LPWANs is limited by resource constraints and unreliable channels, making cloud-dependent strategies impractical. First, cloud-dependent approaches ignore edge resource constraints, offloading even basic inference task to the server. In contrast, empirical measurements show LoRa radios consume up to 40$\times$ more power for uplink and 6$\times$ for downlink than on-device computation**Zhou et al., “Power Consumption Analysis of LoRa Radios”**. Additionally, unreliable channels make consistent offloading infeasible. Data packets may be lost or corrupted during transmission, and edge devices may lack resources for retransmission. To tackle these challenges, we propose a novel \textit{cloud assistance} solution, contrasting with cloud-dependent strategies. In ORCA, the complete inference pipeline remains on edge devices for inference reliability. The server assists edge devices in identifying key input features, reducing their computational load.}

\noindent
\textbf{Data Compression:}
\shepherd{Data compression mitigates high communication costs under low bitrates. DeepCOD**Jiang et al., “DeepCOM: A Deep Learning Framework for Resource-Constrained Devices”** and FLEET**Wu et al., “FLEET: A Framework for Energy-Efficient Inference on Resource-Constrained Devices”** use autoencoders to compress latent embeddings dynamically. SEDAC**Zhou et al., “SEDAC: A System for Efficient and Reliable Audio Classification”** and LimitNet**Liu et al., “LimitNet: A Lightweight Neural Network for Edge AI”** select key input features for communication efficiency. LPAI**Chen et al., “LPAI: Localized Processing for Audio Inference”** uploads selected audio clips for cloud training and requires downloading updated models. These approaches focus on transmitting compressed embeddings, input features, or model weights essential for cloud inference or training, aligning with the cloud-dependent strategy. However, large payloads are unsuitable for low-bitrate networks and resource-constrained devices. We propose an alternative approach to transmit smaller data and leverage complex server-side processing for cloud assistance.} In ORCA, we use low-level feature abstractions like low-resolution audio spectrograms as a compressed form of data, significantly reducing transmission size. Despite the small size, the low-resolution feature abstraction aids the inference process substantially, as we will demonstrate later in this paper. We draw an analogy to the bounding box mechanism used in regional proposal networks (RPN)**Ren et al., “Region Proposal Networks for Object Detection”** and YOLO**Redmon et al., “You Only Look Once: A Real-Time Object Detection System”**, which utilize low-resolution images to propose potential regions of interest and perform efficient classification within these bounded regions. However, SEDAC**Zhou et al., “SEDAC: A System for Efficient and Reliable Audio Classification”** indicates that it is infeasible to deploy RPN for audio tasks on resource-constrained devices due to the high computational demands and fundamental differences between the vision and audio domains. Thus, we conclude that there is a clear need for an audio-adaptive, cloud-assisted, and communication-efficient collaborative learning framework to enable resource-efficient environmental sound recognition.