\section{Related Work}
Non-autoregressive (NAR) models predict words independently, 
and are initially developed to increase the inference speed of neural machine translation **Bahdanau, "Scheduled Sampling for Sequence Prediction"**.
Recently, NAR models have been adapted for length-control summarization in our previous work **Lin, "Length-Controlled Non-Autoregressive Neural Machine Translation"**, where we find that NAR's independent word predictions allow the length-control tasks to be divided into several independent sub-tasks, resulting in an efficient exploration of the NAR output space. Therefore, we have developed dynamic programming algorithms based on the Connectionist Temporal Classification (CTC) model **Graves, "Connectionist Temporal Classification: Labelling Unsegmented Sequence Data with Recurrent Neural Networks"**.

Our work introduces a novel decoding algorithm that leverages the Directed Acyclic Transformer \cite[DAT,][]{huang2022directed}. Unlike CTC, which preserves the order of the source sequence **Graves, "Connectionist Temporal Classification: Labelling Unsegmented Sequence Data with Recurrent Neural Networks"**, DAT offers greater flexibility in word selection and generation order. While recognizing the value of existing DAT-based decoding methods **Huang, "Directed Acyclic Transformer for Efficient Non-Autoregressive Machine Translation"** for managing length, we identify their limitations and propose a new SeqMAP approach.

Our proposed reranker is inspired by the reranking methods in machine translation **Meng, "Improving Neural Machine Translation with Jointly Learning to Align and Predict"** and summarization **See, "Get to the Point: Summarization of Scientific Papers with Deep Attention and Coattention Mechanisms"**, where a list of $n$-best sequences are presented to an external model for scoring.