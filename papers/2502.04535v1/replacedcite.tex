\section{Related Work}
Non-autoregressive (NAR) models predict words independently, 
and are initially developed to increase the inference speed of neural machine translation ____.
Recently, NAR models have been adapted for length-control summarization in our previous work ____, where we find that NAR's independent word predictions allow the length-control tasks to be divided into several independent sub-tasks, resulting in an efficient exploration of the NAR output space. Therefore, we have developed dynamic programming algorithms based on the Connectionist Temporal Classification (CTC) model ____.

Our work introduces a novel decoding algorithm that leverages the Directed Acyclic Transformer \cite[DAT,][]{huang2022directed}. Unlike CTC, which preserves the order of the source sequence ____, DAT offers greater flexibility in word selection and generation order. While recognizing the value of existing DAT-based decoding methods ____ for managing length, we identify their limitations and propose a new SeqMAP approach.

Our proposed reranker is inspired by the reranking methods in machine translation ____ and summarization ____, where a list of $n$-best sequences are presented to an external model for scoring.