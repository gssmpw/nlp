\section{Related Work}
Non-autoregressive (NAR) models predict words independently, 
and are initially developed to increase the inference speed of neural machine translation \cite{gu2018nonautoregressive,lee-etal-2018-deterministic,qian2020glancing,gu-kong-2021-fully,huang-etal-2023-multilingual}.
Recently, NAR models have been adapted for length-control summarization in our previous work \cite{liu-etal-2022-learning,liucharacter}, where we find that NAR's independent word predictions allow the length-control tasks to be divided into several independent sub-tasks, resulting in an efficient exploration of the NAR output space. Therefore, we have developed dynamic programming algorithms based on the Connectionist Temporal Classification (CTC) model \cite{graves2006connectionist}.

Our work introduces a novel decoding algorithm that leverages the Directed Acyclic Transformer \cite[DAT,][]{huang2022directed}. Unlike CTC, which preserves the order of the source sequence \cite{chuang-etal-2021-investigating,shao2022}, DAT offers greater flexibility in word selection and generation order. While recognizing the value of existing DAT-based decoding methods \cite{shao-etal-2022-viterbi} for managing length, we identify their limitations and propose a new SeqMAP approach.

Our proposed reranker is inspired by the reranking methods in machine translation \cite{och-etal-2004-smorgasbord,lee-etal-2021-discriminative} and summarization \cite{ravaut-etal-2022-summareranker}, where a list of $n$-best sequences are presented to an external model for scoring.