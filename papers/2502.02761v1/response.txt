\section{Background and Related Works}
\subsection{Tensor decomposition}\label{subsection:tensor}
Given a tensor $\boldsymbol{\mathcal{X}} \in \mathbb{R}^{n_1 \times n_2 ... \times n_d}$, its matricization (i.e., mode-$k$ unfolding) is the rearrangement of the $k$th dimension into the first dimension of a matrix, and denoted by $\boldsymbol{\mathcal{X}}_{(k)}\in \R^{n_k \times \Pi_{i=1, i \neq k}^d n_i}$. A tensor can be characterized by its rank and the (multilinear) rank of a tensor consists of the column rank of its mode-$k$ unfolding. Tensor-times-matrix (TTM) in mode-$k$ for tensor $\boldsymbol{\mathcal{X}} \in \mathbb{R}^{n_1 \times ... \times n_d}$ with matrix $\mathbf{S} \in \mathbb{R}^{n_k \times r}$ is defined as $\boldsymbol{\mathcal{Y}} 
 = \boldsymbol{\mathcal{X}} \times_k \mathbf{S}$. The result $\boldsymbol{\mathcal{Y}}$ is a tensor of shape $n_1 \times ... \times n_{k-1} \times r \times n_{k+1} \times ... \times n_d$ such that $\mathcal{Y}(i_1, ..., i_{k-1}, j, i_{k+1}, ..., i_d) = \sum_{i_k = 1}^{n_k} \mathcal{X}(i_1, ..., i_k, ..., i_d) \mathbf{S}(i_k, j)$, $j \in [r]$.
Furthermore, given $\mathbf{S}_k \in \mathbb{R}^{n_k \times r_k}$, $k\in[d]$, if $\boldsymbol{\mathcal{Y}} = \boldsymbol{\mathcal{X}} \times_1 \mathbf{S}_1 \times_2 ... \times_d \mathbf{S}_d$, the mode-$k$ unfolding $
\boldsymbol{\mathcal{Y}}_{(k)} = \mathbf{S}_k \boldsymbol{\mathcal{X}}_{(k)}(\mathbf{S}_d \otimes ... \otimes \mathbf{S}_{k+1}\otimes \mathbf{S}_{k-1} \otimes ... \otimes \mathbf{S}_1)^T
$ where $\otimes$ denotes the Kronecker product of two matrices. 

Tensor representation is a powerful tool for handling complex high-dimensional datasets and models. Among the many tensor decompositions, Tucker decomposition is particularly popular because it can be considered an extension of the Singular Value Decomposition (SVD) to higher dimensions **De Lathauwer et al., "A Multilinear Singular Value Decomposition"**. 
The Tucker rank-$(r_1, ..., r_d)$ decomposition of tensor $\boldsymbol{\mathcal{X}} \in \mathbb{R}^{n_1 \times ... \times n_d}$ is defined as 
$\boldsymbol{\mathcal{X}} \approx [\![\boldsymbol{\mathcal{G}}; \mathbf{S}_1, ..., \mathbf{S}_d]\!]\triangleq \boldsymbol{\mathcal{G}} \times_1 \mathbf{S}_1 \times_2 ... \times_d \mathbf{S}_d$, where $\boldsymbol{\mathcal{G}} \in \mathbb{R}^{r_1 \times ... \times r_d}$ and $\mathbf{S}_k \in \mathbb{R}^{n_k\times r_k}$ such that the columns of $\mathbf{S}_k$ are orthogonal. Let $\boldsymbol{\mathcal{G}}^i \in \mathbb{R}^{r_1\times...\times r_d}$ and $\mathbf{S}^i_k \in \mathbb{R}^{n_k\times r_k}$ such that the columns of $\mathbf{S}^i_k$ are orthogonal. Let $\boldsymbol{\mathcal{X}}^i = [\![\boldsymbol{\mathcal{G}}^i; \mathbf{S}_1^i, ..., \mathbf{S}_d^i]\!]$, $\mathbf{W} = \big[\boldsymbol{\mathcal{X}}^1_{(k)} \hspace{5pt} ... \hspace{5pt} \boldsymbol{\mathcal{X}}^N_{(k)} \big]$ and $\mathbf{Y} = \big[\mathbf{S}_k^1\boldsymbol{\mathcal{G}}^1_{(k)} \hspace{5pt} ... \hspace{5pt} \mathbf{S}_k^N\boldsymbol{\mathcal{G}}^N_{(k)}\big]$. Then $\mathbf{W}$ and $\mathbf{Y}$ have the same set of singular values and if $\mathbf{U}$ and $\mathbf{\bar{U}}$ are the left singular matrix of $\mathbf{W}$ and $\mathbf{Y}$ respectively, then $\mathbf{U} = \bar{\mathbf{U}}\mathbf{P}$ where $\mathbf{P}$ is a unitary block diagonal matrix. 
Similarly to **Huang et al., "Federated Learning for Multi-Modal Inverse Problems"**, we exploit the orthogonality of the factor matrices in the aggregation process and extend it by incorporating randomized sketching to improve computational efficiency. Note that this paper focuses on feature extraction while we focus on a multimodal inverse problem, which requires new concepts, ideas, and tricks.

\subsection{Federated tomographic reconstruction}
In this paper, we extend the multimodal image reconstruction problem in **Huang et al., "FIRM: Federated Inverse Problem Reconstruction Method"**, which is given by
\begin{equation}\label{eq:firm}
    \min_{\boldsymbol{\mathcal{X}}^1, ..., \boldsymbol{\mathcal{X}}^N} \sum_{i=1}^N f\big(\boldsymbol{\mathcal{X}}^i; \mathit{D}^i\big) \quad
    \text{s.t.} \quad \boldsymbol{\mathcal{X}}_{N} = \sum_j^{N-1} c_j \boldsymbol{\mathcal{X}}^j_{j}
\end{equation}
where $\boldsymbol{\mathcal{X}}^N$ denotes the client with X-ray transmission (XRT) data and $\boldsymbol{\mathcal{X}}^j$, $j\in[N-1]$ are clients with X-ray fluorescence (XRF) data. The coefficient $c_j$ denotes the mass attenuation coefficient of the element of interest and is well characterized.

FIRM, the approach developed in **Huang et al., "FIRM: Federated Inverse Problem Reconstruction Method"**, enables clients with data obtained from different modalities to collaborate and improve their respective solution quality with convergence guarantee **Li et al., "Convergence Analysis of FIRM"**. Specifically, at epoch $t$, clients use gradient descent to locally update the previous solution $\boldsymbol{\mathcal{X}}^i(t-1)$ and obtain $\tilde{\boldsymbol{\mathcal{X}}}^i$, $i\in[N]$. After receiving the full-size tensors $\tilde{\boldsymbol{\mathcal{X}}}^i$, the server performs a series of arithmetic operations to enforce the multimodality constraint as follows.

\begin{equation}\label{eq:firm_update}
\begin{aligned}
    \Sigma &\leftarrow \sum_{i=1}^{N-1} c_i\tilde{\boldsymbol{\mathcal{X}}}^i\\
    \boldsymbol{\mathcal{X}}^i(t) &\leftarrow \tilde{\boldsymbol{\mathcal{X}}}^i + \dfrac{c_i}{2}(\tilde{\boldsymbol{\mathcal{X}}}^N - \Sigma), i\in[N-1]\\ 
    \boldsymbol{\mathcal{X}}^N(t) &\leftarrow \dfrac{\tilde{\boldsymbol{\mathcal{X}}}^N + \Sigma}{2}\\
\end{aligned}
\end{equation}
Previous works have shown that the low-rank regularization via tensor decomposition improves standard tomographic reconstruction quality **Zhang et al., "Low-Rank Regularization for Tomographic Reconstruction"**. In this paper, we introduce low-rank regularization and incorporate tensor decomposition to the FIRM framework to further improve the solution of Problem~\ref{eq:firm}.