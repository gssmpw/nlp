\section{Background and Related Works}
\subsection{Tensor decomposition}\label{subsection:tensor}
Given a tensor $\boldsymbol{\mathcal{X}} \in \mathbb{R}^{n_1 \times n_2 ... \times n_d}$, its matricization (i.e., mode-$k$ unfolding) is the rearrangement of the $k$th dimension into the first dimension of a matrix, and denoted by $\boldsymbol{\mathcal{X}}_{(k)}\in \R^{n_k \times \Pi_{i=1, i \neq k}^d n_i}$. A tensor can be characterized by its rank and the (multilinear) rank of a tensor consists of the column rank of its mode-$k$ unfolding. Tensor-times-matrix (TTM) in mode-$k$ for tensor $\boldsymbol{\mathcal{X}} \in \mathbb{R}^{n_1 \times ... \times n_d}$ with matrix $\mathbf{S} \in \mathbb{R}^{n_k \times r}$ is defined as $\boldsymbol{\mathcal{Y}} 
 = \boldsymbol{\mathcal{X}} \times_k \mathbf{S}$. The result $\boldsymbol{\mathcal{Y}}$ is a tensor of shape $n_1 \times ... \times n_{k-1} \times r \times n_{k+1} \times ... \times n_d$ such that $\mathcal{Y}(i_1, ..., i_{k-1}, j, i_{k+1}, ..., i_d) = \sum_{i_k = 1}^{n_k} \mathcal{X}(i_1, ..., i_k, ..., i_d) \mathbf{S}(i_k, j)$, $j \in [r]$.
Furthermore, given $\mathbf{S}_k \in \mathbb{R}^{n_k \times r_k}$, $k\in[d]$, if $\boldsymbol{\mathcal{Y}} = \boldsymbol{\mathcal{X}} \times_1 \mathbf{S}_1 \times_2 ... \times_d \mathbf{S}_d$, the mode-$k$ unfolding $
\boldsymbol{\mathcal{Y}}_{(k)} = \mathbf{S}_k \boldsymbol{\mathcal{X}}_{(k)}(\mathbf{S}_d \otimes ... \otimes \mathbf{S}_{k+1}\otimes \mathbf{S}_{k-1} \otimes ... \otimes \mathbf{S}_1)^T
$ where $\otimes$ denotes the Kronecker product of two matrices. 

Tensor representation is a powerful tool for handling complex high-dimensional datasets and models. Among the many tensor decompositions, Tucker decomposition is particularly popular because it can be considered an extension of the Singular Value Decomposition (SVD) to higher dimensions ____. 
The Tucker rank-$(r_1, ..., r_d)$ decomposition of tensor $\boldsymbol{\mathcal{X}} \in \mathbb{R}^{n_1 \times ... \times n_d}$ is defined as 
$\boldsymbol{\mathcal{X}} \approx [\![\boldsymbol{\mathcal{G}}; \mathbf{S}_1, ..., \mathbf{S}_d]\!]\triangleq \boldsymbol{\mathcal{G}} \times_1 \mathbf{S}_1 \times_2 ... \times_d \mathbf{S}_d$, where $\boldsymbol{\mathcal{G}} \in \mathbb{R}^{r_1 \times ... \times r_d}$ is called the core tensor and  $\mathbf{S}_k \in \mathbb{R}^{n_k \times r_k}$, $k \in [d]$ is a set of factor matrices as the $r_k$ leading left singular vectors of the mode-$k$ unfolding of $\boldsymbol{\mathcal{X}}$. 
Note that this decomposition is not necessarily unique. More detailed discussions of tensors can be found in ____. 

Tucker decomposition can be achieved via the higher-order SVD (HOSVD) algorithm ____. The sequentially truncated HOSVD (ST-HOSVD) is proposed to improve the computational efficiency of its predecessor ____. Unlike the original algorithm, which treats each dimension independently, ST-HOSVD finds a factor matrix and immediately compresses the tensor in the corresponding dimension. The truncated tensor is then used to find the factor matrix in the next dimension.

Recently, randomized sketching is used to further reduce the cost of SVD. The key idea is to reduce the dimensions of a matrix by generating a randomized sketch, which is smaller but is a good approximation of the original matrix ____. In particular, the Rand-Tucker algorithm ____ obtains a sketch 
$\mathbf{Z}$ from the mode-$k$ unfolding of tensor $\boldsymbol{\mathcal{X}}$ and sets the factor matrix $\mathbf{S}_k$  as the orthonormal basis of $\mathbf{Z}$ (via QR decomposition). In this work, we adopt the core idea of Rand-Tucker and modify the randomized procedure to the federated setting.

The framework that underpins most tensor-based data analysis tasks is the generalized low-rank tensor estimation problem ____. Given a dataset $\mathit{D}$, this problem is described as
\begin{equation}\label{eq:estimation}
    \min_{\boldsymbol{\mathcal{X}}} f\big(\boldsymbol{\mathcal{X}}; \mathit{D}\big) \quad
    \text{s.t.} \quad \text{rank}\big(\boldsymbol{\mathcal{X}}_{(k)}\big) \leq r_k, k\in[d]
\end{equation}
where $f\big(\boldsymbol{\mathcal{X}}; \mathit{D}\big)$ is a loss function, and tuple $(r_1, ..., r_d)$ is the expected (multilinear) rank of the tensor of interest. 
In general, this problem can be solved by the projected gradient descent method (e.g., ____) where the gradient descent iterate is projected onto the feasible set via tensor decomposition such as HOSVD to satisfy the multilinear rank constraint. 

\subsection{Tensor decomposition in FL}
Tensor decomposition in the FL framework has previously been studied ____. A key aspect in the integration of tensor decomposition in FL is how the server aggregates the factorized components. Mainly, there are two aggregation schemes: the server either reconstructs the full weights before taking the average ____, or adapts FedAVG and directly averages the components ____. A drawback of the former scheme is the significant increase in computational efforts. The latter combines latent spaces that are independently calculated from separate datasets and can suffer from compounding errors. As noted in ____, a potential remedy is joint factorization, whereby these tensors are decomposed into a shared latent factor space. Indeed, joint factorization is a popular approach in data fusion ____ as it provides a systemic framework for merging data obtained from different sources. However, it is still underutilized as an aggregation scheme in FL. In this work, we benchmark against the current aggregation schemes and address their shortcomings using joint factorization. 

Mathematically, given $N$ tensors $\boldsymbol{\mathcal{X}}^i \in \mathbb{R}^{n_1\times...\times n_d}$, let their concatenation be denoted by the tensor $\boldsymbol{\mathcal{X}}$ such that $\boldsymbol{\mathcal{X}} = \big[\boldsymbol{\mathcal{X}}^1\hspace{5pt}|\hspace{5pt} \boldsymbol{\mathcal{X}}^2 \hspace{5pt}|\hspace{5pt} ... \hspace{5pt}|\hspace{5pt} \boldsymbol{\mathcal{X}}^N\big]$ and $\boldsymbol{\mathcal{X}} \in \mathbb{R}^{n_1 \times ... \times n_d \times N}$. The joint factorization of $N$ tensors $\boldsymbol{\mathcal{X}}^i$ is equivalent to finding the factor matrices of the concatenated tensor $\boldsymbol{\mathcal{X}}$ in the first $d$ dimensions. Gao et al. ____ propose the first joint factorization-based scheme for the federated feature extraction task. The authors leverage the result in Lemma \ref{lemma:gao} to approximate the left singular vectors of the mode-$k$ unfolding $\boldsymbol{\mathcal{X}}_{(k)}$ as those of $\mathbf{Y} = \big[\mathbf{S}_k^1\boldsymbol{\mathcal{G}}^1_{(k)} \hspace{5pt} ... \hspace{5pt} \mathbf{S}_k^N\boldsymbol{\mathcal{G}}^N_{(k)}\big]$, which effectively bypasses the reconstruction of full-size tensors. 
\begin{lemma}\label{lemma:gao}
(From Gao et al. ____). For $i=1,...,N$ and $k=1,...,d$, let $\boldsymbol{\mathcal{G}}^i \in \mathbb{R}^{r_1\times...\times r_d}$ and $\mathbf{S}^i_k \in \mathbb{R}^{n_k\times r_k}$ such that the columns of $\mathbf{S}^i_k$ are orthogonal. Let $\boldsymbol{\mathcal{X}}^i = [\![\boldsymbol{\mathcal{G}}^i; \mathbf{S}_1^i, ..., \mathbf{S}_d^i]\!]$, $\mathbf{W} = \big[\boldsymbol{\mathcal{X}}^1_{(k)} \hspace{5pt} ... \hspace{5pt} \boldsymbol{\mathcal{X}}^N_{(k)} \big]$ and $\mathbf{Y} = \big[\mathbf{S}_k^1\boldsymbol{\mathcal{G}}^1_{(k)} \hspace{5pt} ... \hspace{5pt} \mathbf{S}_k^N\boldsymbol{\mathcal{G}}^N_{(k)}\big]$. Then $\mathbf{W}$ and $\mathbf{Y}$ have the same set of singular values and if $\mathbf{U}$ and $\mathbf{\bar{U}}$ are the left singular matrix of $\mathbf{W}$ and $\mathbf{Y}$ respectively, then $\mathbf{U} = \bar{\mathbf{U}}\mathbf{P}$ where $\mathbf{P}$ is a unitary block diagonal matrix. 
\end{lemma}
Similarly to ____, we exploit the orthogonality of the factor matrices in the aggregation process and extend it by incorporating randomized sketching to improve computational efficiency. Note that ____ focuses on feature extraction while we focus on a multimodal inverse problem, which requires new concepts, ideas, and tricks.

\subsection{Federated tomographic reconstruction}
In this paper, we extend the multimodal image reconstruction problem in ____, which is given by
\begin{equation}\label{eq:firm}
    \min_{\boldsymbol{\mathcal{X}}^1, ..., \boldsymbol{\mathcal{X}}^N} \sum_{i=1}^N f\big(\boldsymbol{\mathcal{X}}^i; \mathit{D}^i\big) \quad
    \text{s.t.} \quad \boldsymbol{\mathcal{X}}_{N} = \sum_j^{N-1} c_j \boldsymbol{\mathcal{X}}^j_{j}
\end{equation}
where $\boldsymbol{\mathcal{X}}^N$ denotes the client with X-ray transmission (XRT) data and $\boldsymbol{\mathcal{X}}^j$, $j\in[N-1]$ are clients with X-ray fluorescence (XRF) data. The coefficient $c_j$ denotes the mass attenuation coefficient of the element of interest and is well characterized.

FIRM, the approach developed in ____, enables clients with data obtained from different modalities to collaborate and improve their respective solution quality with convergence guarantee ____. Specifically, at epoch $t$, clients use gradient descent to locally update the previous solution $\boldsymbol{\mathcal{X}}^i(t-1)$ and obtain $\tilde{\boldsymbol{\mathcal{X}}}^i$, $i\in[N]$. After receiving the full-size tensors $\tilde{\boldsymbol{\mathcal{X}}}^i$, the server performs a series of arithmetic operations to enforce the multimodality constraint as follows.

\begin{equation}\label{eq:firm_update}
\begin{aligned}
    \Sigma &\leftarrow \sum_{i=1}^{N-1} c_i\tilde{\boldsymbol{\mathcal{X}}}^i\\
    \boldsymbol{\mathcal{X}}^i(t) &\leftarrow \tilde{\boldsymbol{\mathcal{X}}}^i + \dfrac{c_i}{2}(\tilde{\boldsymbol{\mathcal{X}}}^N - \Sigma), i\in[N-1]\\ 
    \boldsymbol{\mathcal{X}}^N(t) &\leftarrow \dfrac{\tilde{\boldsymbol{\mathcal{X}}}^N + \Sigma}{2}\\
\end{aligned}
\end{equation}
Previous works have shown that the low-rank regularization via tensor decomposition improves standard tomographic reconstruction quality ____. In this paper, we introduce low-rank regularization and incorporate tensor decomposition to the FIRM framework to further improve the solution of Problem~\ref{eq:firm}.