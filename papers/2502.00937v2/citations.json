[
  {
    "index": 0,
    "papers": [
      {
        "key": "lee2024characterizing",
        "author": "Lee, Yejin and Sun, Anna and Hosmer, Basil and Acun, Bilge and Balioglu, Can and Wang, Changhan and Hernandez, Charles David and Puhrsch, Christian and Haziza, Daniel and Guessous, Driss and others",
        "title": "Characterizing and Efficiently Accelerating Multimodal Generation Model Inference"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "hou2022characterizing",
        "author": "Hou, Xiaofeng and Xu, Cheng and Liu, Jiacheng and Tang, Xuehan and Sun, Lingyu and Li, Chao and Cheng, Kwang-Ting",
        "title": "{Characterizing and understanding end-to-end multi-modal neural networks on GPUs}"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "ning2024infmllm",
        "author": "Zhenyu Ning and Jieru Zhao and Qihao Jin and Wenchao Ding and Minyi Guo",
        "title": "{Inf-MLLM}: Efficient Streaming Inference of Multimodal Large Language Models on a Single {GPU}"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "liu2025efficient",
        "author": "Liu, Zuyan and Liu, Benlin and Wang, Jiahui and Dong, Yuhao and Chen, Guangyi and Rao, Yongming and Krishna, Ranjay and Lu, Jiwen",
        "title": "{Efficient inference of vision instruction-following models with elastic cache}"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "huang2024dynamic",
        "author": "Huang, Wenxuan and Zhai, Zijie and Shen, Yunhang and Cao, Shaoshen and Zhao, Fei and Xu, Xiangfeng and Ye, Zheyu and Lin, Shaohui",
        "title": "{Dynamic-LLaVA}: Efficient Multimodal Large Language Models via Dynamic Vision-language Context Sparsification"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "lin2024boosting",
        "author": "Lin, Zhihang and Lin, Mingbao and Lin, Luxi and Ji, Rongrong",
        "title": "Boosting Multimodal Large Language Models with Visual Tokens Withdrawal for Rapid Inference"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "li2024inference",
        "author": "Li, Kevin Y and Goyal, Sachin and Semedo, Joao D and Kolter, J Zico",
        "title": "Inference Optimal {VLMs} Need Only One Visual Token but Larger Models"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "patel2024splitwise",
        "author": "Patel, Pratyush and Choukse, Esha and Zhang, Chaojie and Shah, Aashaka and Goiri, {\\'I}{\\~n}igo and Maleki, Saeed and Bianchini, Ricardo",
        "title": "{Splitwise: Efficient generative LLM inference using phase splitting}"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "zhong2024distserve",
        "author": "Zhong, Yinmin and Liu, Shengyu and Chen, Junda and Hu, Jianbo and Zhu, Yibo and Liu, Xuanzhe and Jin, Xin and Zhang, Hao",
        "title": "{DistServe: Disaggregating Prefill and Decoding for Goodput-optimized Large Language Model Serving}"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "qin2024mooncake",
        "author": "Qin, Ruoyu and Li, Zheming and He, Weiran and Zhang, Mingxing and Wu, Yongwei and Zheng, Weimin and Xu, Xinran",
        "title": "{Mooncake: A KVCache-centric Disaggregated Architecture for LLM Serving}"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "hu2024memserve",
        "author": "Hu, Cunchen and Huang, Heyang and Hu, Junhao and Xu, Jiang and Chen, Xusheng and Xie, Tao and Wang, Chenxi and Wang, Sa and Bao, Yungang and Sun, Ninghui and others",
        "title": "{MemServe: Context caching for disaggregated LLM serving with elastic memory pool}"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "vllm",
        "author": "Woosuk Kwon and Zhuohan Li and Siyuan Zhuang and Ying Sheng and Lianmin Zheng and Cody Hao Yu and Joseph E. Gonzalez and Hao Zhang and Ion Stoica",
        "title": "{Efficient Memory Management for Large Language Model Serving with PagedAttention}"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "orca",
        "author": "Gyeong-In Yu and Joo Seong Jeong and Geon-Woo Kim and Soojeong Kim and Byung-Gon Chun",
        "title": "{Orca: A Distributed Serving System for Transformer-Based Generative Models}"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "aiops2024qiu",
        "author": "Qiu, Haoran and Mao, Weichao and Patke, Archit and Cui, Shengkun and Jha, Saurabh and Wang, Chen and Franke, Hubertus and Kalbarczyk, Zbigniew T. and Ba\\c{s}ar, Tamer and Iyer, Ravishankar K.",
        "title": "{Efficient Interactive LLM Serving with Proxy Model-based Sequence Length Prediction}"
      },
      {
        "key": "patke2024queue",
        "author": "Patke, Archit and Reddy, Dhemath and Jha, Saurabh and Qiu, Haoran and Pinto, Christian and Narayanaswami, Chandra and Kalbarczyk, Zbigniew and Iyer, Ravishankar",
        "title": "{Queue Management for SLO-Oriented Large Language Model Serving}"
      },
      {
        "key": "sun2024llumnix",
        "author": "Sun, Biao and Huang, Ziming and Zhao, Hanyu and Xiao, Wencong and Zhang, Xinyi and Li, Yong and Lin, Wei",
        "title": "{Llumnix: Dynamic Scheduling for Large Language Model Serving}"
      },
      {
        "key": "sarathi-serve",
        "author": "Agrawal, Amey and Kedia, Nitin and Panwar, Ashish and Mohan, Jayashree and Kwatra, Nipun and Gulavani, Bhargav and Tumanov, Alexey and Ramjee, Ramachandran",
        "title": "{Taming Throughput-Latency Tradeoff in LLM Inference with Sarathi-Serve}"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "stojkovic2025tapas",
        "author": "Stojkovic, Jovan and Zhang, Chaojie and Goiri, {\\'I}{\\~n}igo and Choukse, Esha and Qiu, Haoran and Fonseca, Rodrigo and Torrellas, Josep and Bianchini, Ricardo",
        "title": "{TAPAS: Thermal-and Power-Aware Scheduling for LLM Inference in Cloud Platforms}"
      },
      {
        "key": "stojkovic2024dynamollm",
        "author": "Stojkovic, Jovan and Zhang, Chaojie and Goiri, {\\'I}{\\~n}igo and Torrellas, Josep and Choukse, Esha",
        "title": "{DynamoLLM: Designing LLM Inference Clusters for Performance and Energy Efficiency}"
      },
      {
        "key": "qiu2024muserve",
        "author": "Haoran Qiu and Weichao Mao and Archit Patke and Shengkun Cui and Saurabh Jha and Chen Wang and Hubertus Franke and Zbigniew Kalbarczyk and Tamer Ba{\\c s}ar and Ravishankar K. Iyer",
        "title": "{Power-aware Deep Learning Model Serving with $\\mu$-Serve}"
      }
    ]
  }
]