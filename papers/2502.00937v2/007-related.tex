\section{Related Work}

\myparagraph{\lmm Characterization}
Lee \etal{}~\cite{lee2024characterizing} provides a comprehensive characterization of multimodal generation models at Meta, while we focus on multimodal inputs.
Hou \etal{}~\cite{hou2022characterizing} focus on traditional multimodal models employing small-scale convolutional neural networks.
In contrast, our work presents a detailed analysis of multimodal input workloads on both open-source \lmm models and production traces, highlighting their unique execution and workload patterns.

\myparagraph{\lmm Serving}
Recent research has introduced several techniques to optimize \lmm serving by addressing key inefficiencies in inference computation and memory usage.
Inf-MLLM~\cite{ning2024infmllm} employs token caching strategies and attention bias to maintain performance with long contexts while reducing KV cache memory consumption.
Elastic Cache~\cite{liu2025efficient} utilizes an importance-driven cache merging strategy to prune KV caches efficiently during inference.
Dynamic-LLaVA~\cite{huang2024dynamic}, VTW~\cite{lin2024boosting}, and QueCC~\cite{li2024inference} present various vision token sparsification and compression techniques to dynamically reduce redundancy in vision tokens.
These optimizations primarily operate at the model level, trading off computational overhead with model performance.
They are orthogonal to our proposed system-level design for SLO-driven \lmm serving that does not impact model performance, which can further benefit from such model-level advancements, e.g., faster image encoding through token compression.

\myparagraph{Text-Centric LLM Serving}
Recent studies have delved into disaggregating LLM prefill and decode phases for text-only LLM serving.
Examples include Splitwise~\cite{patel2024splitwise}, DistServe~\cite{zhong2024distserve}, Mooncake~\cite{qin2024mooncake}, and MemServe~\cite{hu2024memserve}.
Other optimizations for LLM serving include key-value cache management~\cite{vllm}, continuous batching~\cite{orca}, request scheduling~\cite{aiops2024qiu,patke2024queue,sun2024llumnix,sarathi-serve}, and energy optimization~\cite{stojkovic2025tapas,stojkovic2024dynamollm,qiu2024muserve}.
While these optimizations can be applied in \sysname{} to enhance LLM backend prefill and decode efficiency, our work focuses on the unique characteristics of multimodal models.