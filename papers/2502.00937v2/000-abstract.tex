\begin{abstract}
Large multimodal models (\lmms{}) demonstrate impressive capabilities in understanding images, videos, and audio beyond text.
However, efficiently serving \lmms{} in production environments poses significant challenges due to their complex architectures and heterogeneous characteristics across their multi-stage inference pipelines.

We present the first comprehensive systems analysis of two prominent \lmm architectures, decoder-only and cross-attention, across six representative open-source models, revealing key systems design implications.
We also present an in-depth analysis of production \lmm inference traces, uncovering unique workload characteristics, including variable, heavy-tailed request distributions and bursty traffic patterns.

Based on these insights, we propose \sysname{}, a modular \lmm{} serving system that decouples stages for independent optimization and adaptive scaling. \sysname{} dynamically reconfigures stages and handles bursty traffic with modality-aware scheduling and autoscaling to meet tail latency SLOs while minimizing costs.
\sysname{} achieves 3.3--5.5$\times$ higher throughput (leading to 25--41.3\% cost saving) while meeting SLOs on a 128-GPU cluster with production traces.
\end{abstract}