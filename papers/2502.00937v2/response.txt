\section{Related Work}
\myparagraph{\lmm Characterization}
Lee \etal{}, "A Multimodal Generation Model at Meta" provides a comprehensive characterization of multimodal generation models at Meta, while we focus on multimodal inputs.
Hou \etal{}, "Traditional Multimodal Models for Small-Scale Convolutional Neural Networks" focus on traditional multimodal models employing small-scale convolutional neural networks.
In contrast, our work presents a detailed analysis of multimodal input workloads on both open-source \lmm models and production traces, highlighting their unique execution and workload patterns.

\myparagraph{\lmm Serving}
Recent research has introduced several techniques to optimize \lmm serving by addressing key inefficiencies in inference computation and memory usage.
Inf-MLLM__ employs token caching strategies and attention bias to maintain performance with long contexts while reducing KV cache memory consumption.
Elastic Cache__, VTW__, QueCC__, Dynamic-LLaVA__ present various vision token sparsification and compression techniques to dynamically reduce redundancy in vision tokens.
These optimizations primarily operate at the model level, trading off computational overhead with model performance.
They are orthogonal to our proposed system-level design for SLO-driven \lmm serving that does not impact model performance, which can further benefit from such model-level advancements, e.g., faster image encoding through token compression.

\myparagraph{Text-Centric LLM Serving}
Recent studies have delved into disaggregating LLM prefill and decode phases for text-only LMM serving.
Examples include Splitwise__, DistServe__, Mooncake__, MemServe__.
Other optimizations for LLM serving include key-value cache management__, continuous batching__, request scheduling__, and energy optimization__.
While these optimizations can be applied in \sysname{} to enhance LLM backend prefill and decode efficiency, our work focuses on the unique characteristics of multimodal models.