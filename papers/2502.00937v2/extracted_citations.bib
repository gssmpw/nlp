@inproceedings{aiops2024qiu,
  author  = {Qiu, Haoran and Mao, Weichao and Patke, Archit and Cui, Shengkun and Jha, Saurabh and Wang, Chen and Franke, Hubertus and Kalbarczyk, Zbigniew T. and Ba\c{s}ar, Tamer and Iyer, Ravishankar K.},
  title   = {{Efficient Interactive LLM Serving with Proxy Model-based Sequence Length Prediction}},
  year    = {2024},
  booktitle = {The 5th International Workshop on Cloud Intelligence / AIOps at ASPLOS 2024},
}

@article{hou2022characterizing,
  title={{Characterizing and understanding end-to-end multi-modal neural networks on GPUs}},
  author={Hou, Xiaofeng and Xu, Cheng and Liu, Jiacheng and Tang, Xuehan and Sun, Lingyu and Li, Chao and Cheng, Kwang-Ting},
  journal={IEEE Computer Architecture Letters},
  volume={21},
  number={2},
  pages={125--128},
  year={2022},
}

@article{hu2024memserve,
  title={{MemServe: Context caching for disaggregated LLM serving with elastic memory pool}},
  author={Hu, Cunchen and Huang, Heyang and Hu, Junhao and Xu, Jiang and Chen, Xusheng and Xie, Tao and Wang, Chenxi and Wang, Sa and Bao, Yungang and Sun, Ninghui and others},
  journal={arXiv preprint arXiv:2406.17565},
  year={2024}
}

@article{huang2024dynamic,
  title={{Dynamic-LLaVA}: Efficient Multimodal Large Language Models via Dynamic Vision-language Context Sparsification},
  author={Huang, Wenxuan and Zhai, Zijie and Shen, Yunhang and Cao, Shaoshen and Zhao, Fei and Xu, Xiangfeng and Ye, Zheyu and Lin, Shaohui},
  journal={arXiv preprint arXiv:2412.00876},
  year={2024}
}

@article{lee2024characterizing,
  title={Characterizing and Efficiently Accelerating Multimodal Generation Model Inference},
  author={Lee, Yejin and Sun, Anna and Hosmer, Basil and Acun, Bilge and Balioglu, Can and Wang, Changhan and Hernandez, Charles David and Puhrsch, Christian and Haziza, Daniel and Guessous, Driss and others},
  journal={arXiv preprint arXiv:2410.00215},
  year={2024}
}

@article{li2024inference,
  title={Inference Optimal {VLMs} Need Only One Visual Token but Larger Models},
  author={Li, Kevin Y and Goyal, Sachin and Semedo, Joao D and Kolter, J Zico},
  journal={arXiv preprint arXiv:2411.03312},
  year={2024}
}

@article{lin2024boosting,
  title={Boosting Multimodal Large Language Models with Visual Tokens Withdrawal for Rapid Inference},
  author={Lin, Zhihang and Lin, Mingbao and Lin, Luxi and Ji, Rongrong},
  journal={arXiv preprint arXiv:2405.05803},
  year={2024}
}

@inproceedings{liu2025efficient,
  title={{Efficient inference of vision instruction-following models with elastic cache}},
  author={Liu, Zuyan and Liu, Benlin and Wang, Jiahui and Dong, Yuhao and Chen, Guangyi and Rao, Yongming and Krishna, Ranjay and Lu, Jiwen},
  booktitle={European Conference on Computer Vision (ECCV)},
  year={2025},
}

@article{ning2024infmllm,
  title={{Inf-MLLM}: Efficient Streaming Inference of Multimodal Large Language Models on a Single {GPU}}, 
  author={Zhenyu Ning and Jieru Zhao and Qihao Jin and Wenchao Ding and Minyi Guo},
  year={2024},
  journal={arXiv preprint arXiv:2409.09086},
}

@inproceedings{orca,
    author = {Gyeong-In Yu and Joo Seong Jeong and Geon-Woo Kim and Soojeong Kim and Byung-Gon Chun},
    title = {{Orca: A Distributed Serving System for Transformer-Based Generative Models}},
    booktitle = {Proceedings of the 16th USENIX Symposium on Operating Systems Design and Implementation (OSDI)},
    year = {2022}
}

@inproceedings{patel2024splitwise,
  title={{Splitwise: Efficient generative LLM inference using phase splitting}},
  author={Patel, Pratyush and Choukse, Esha and Zhang, Chaojie and Shah, Aashaka and Goiri, {\'I}{\~n}igo and Maleki, Saeed and Bianchini, Ricardo},
  booktitle={Proceedings of the ACM/IEEE 51st Annual International Symposium on Computer Architecture (ISCA)},
  year={2024},
}

@inproceedings{patke2024queue,
  title={{Queue Management for SLO-Oriented Large Language Model Serving}},
  author={Patke, Archit and Reddy, Dhemath and Jha, Saurabh and Qiu, Haoran and Pinto, Christian and Narayanaswami, Chandra and Kalbarczyk, Zbigniew and Iyer, Ravishankar},
  booktitle={Proceedings of the ACM Symposium on Cloud Computing (SoCC)},
  year={2024}
}

@article{qin2024mooncake,
  title={{Mooncake: A KVCache-centric Disaggregated Architecture for LLM Serving}},
  author={Qin, Ruoyu and Li, Zheming and He, Weiran and Zhang, Mingxing and Wu, Yongwei and Zheng, Weimin and Xu, Xinran},
  journal={arXiv preprint arXiv:2407.00079},
  year={2024}
}

@inproceedings{qiu2024muserve,
  title = {{Power-aware Deep Learning Model Serving with $\mu$-Serve}},
  author = {Haoran Qiu and Weichao Mao and Archit Patke and Shengkun Cui and Saurabh Jha and Chen Wang and Hubertus Franke and Zbigniew Kalbarczyk and Tamer Ba{\c s}ar and Ravishankar K. Iyer},
  booktitle = {USENIX Annual Technical Conference (USENIX ATC 2024)},
  year = {2024},
}

@inproceedings{sarathi-serve,
  title={{Taming Throughput-Latency Tradeoff in LLM Inference with Sarathi-Serve}},
  author={Agrawal, Amey and Kedia, Nitin and Panwar, Ashish and Mohan, Jayashree and Kwatra, Nipun and Gulavani, Bhargav and Tumanov, Alexey and Ramjee, Ramachandran},
  booktitle={Proceedings of the 18th USENIX Symposium on Operating Systems Design and Implementation (OSDI)},
  year={2024},
}

@inproceedings{stojkovic2024dynamollm,
  title={{DynamoLLM: Designing LLM Inference Clusters for Performance and Energy Efficiency}},
  author={Stojkovic, Jovan and Zhang, Chaojie and Goiri, {\'I}{\~n}igo and Torrellas, Josep and Choukse, Esha},
  year={2025},
  booktitle={Proceedings of the IEEE International Symposium on High Performance Computer Architecture (HPCA)}, 
}

@article{stojkovic2025tapas,
  title={{TAPAS: Thermal-and Power-Aware Scheduling for LLM Inference in Cloud Platforms}},
  author={Stojkovic, Jovan and Zhang, Chaojie and Goiri, {\'I}{\~n}igo and Choukse, Esha and Qiu, Haoran and Fonseca, Rodrigo and Torrellas, Josep and Bianchini, Ricardo},
  journal={arXiv preprint arXiv:2501.02600},
  year={2025}
}

@article{sun2024llumnix,
  title={{Llumnix: Dynamic Scheduling for Large Language Model Serving}},
  author={Sun, Biao and Huang, Ziming and Zhao, Hanyu and Xiao, Wencong and Zhang, Xinyi and Li, Yong and Lin, Wei},
  journal={arXiv preprint arXiv:2406.03243},
  year={2024}
}

@inproceedings{vllm,
  title={{Efficient Memory Management for Large Language Model Serving with PagedAttention}},
  author={Woosuk Kwon and Zhuohan Li and Siyuan Zhuang and Ying Sheng and Lianmin Zheng and Cody Hao Yu and Joseph E. Gonzalez and Hao Zhang and Ion Stoica},
  booktitle={Proceedings of the Symposium on Operating Systems Principles (SOSP)},
  year={2023}
}

@inproceedings{zhong2024distserve,
  title={{DistServe: Disaggregating Prefill and Decoding for Goodput-optimized Large Language Model Serving}},
  author={Zhong, Yinmin and Liu, Shengyu and Chen, Junda and Hu, Jianbo and Zhu, Yibo and Liu, Xuanzhe and Jin, Xin and Zhang, Hao},
  booktitle={Proceedings of the 18th USENIX Symposium on Operating Systems Design and Implementation (OSDI)},
  year={2024}
}

