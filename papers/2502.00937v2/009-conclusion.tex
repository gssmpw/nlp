\section{Conclusion}

We present the first comprehensive systems analysis of \lmms{} on both open-source models and production \lmm{} inference traces.
Our insights lead to the design of \sysname{}, a scalable and resource-efficient \lmm{}-serving framework that decouples inference stages for dynamic reconfiguration, adaptive scaling, and modality-aware scheduling.
Evaluations show that \sysname{} achieves 25--41\% cost savings compared to the state-of-the-art while efficiently serving production-scale \lmm{} inference workloads.