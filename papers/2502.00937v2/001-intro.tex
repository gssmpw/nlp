\section{Introduction}

\noindent
The rapid advancement in generative AI has led to the development of large multimodal models (\lmms{}) capable of processing inputs across various modalities such as text, image, video, and audio.
These models have demonstrated remarkable capabilities in tasks like image captioning~\cite{chen2022visualgpt,mokady2021clipcap,hu2023promptcap}, visual question answering~\cite{schwenk2022okvqa,shao2023prompting}, and multimodal dialogue systems~\cite{li2024llava,chen2024internvl,team2024gemini}.
This has led to a rapid adoption of \lmms{} in production services, including online applications where latency service-level objectives (SLOs) are critical.

Unlike traditional large language models (LLMs) that process purely textual inputs using a single component, a decoder-based transformer architecture~\cite{transformer}, \lmms{} handle fundamentally different types of inputs, each requiring distinct processing approaches.
This heterogeneity introduces unique serving complexities that demand novel analysis and serving strategies.
For \textit{Image-Text-to-Text} models~\cite{ittt}, the inference pipeline consists of multiple specialized stages:
image preprocessing to transform raw images into tensor representations, image encoding to convert these tensors into image tokens, and a language model backend that combines text prompts with image tokens to generate text outputs.
Currently, these stages are typically served as a monolithic system~\cite{vllm,hf,deepspeed}, where all components are integrated within a single serving instance and scaled together as a unified entity.

\begin{figure}[!t]
    \raggedright
    \includegraphics[width=0.97\linewidth]{figs/intro-monolith-vs-decoupled.pdf}
    \caption{Impact of image workload on \lmm{} inference TTFT for state-of-the-art implementation of Llama3.2-11B on vLLM vs. \sysname{} with an 8-A100 GPU server.
    The ``Monolith'' setup deploys the full model using 8 GPUs while the ``Decoupled'' setup deploys the LLM backend on 4 GPUs and four image encoders on the other 4 GPUs.
    }
    \label{fig:monolith-vs-decoupled}
\end{figure}

Unfortunately, existing monolithic inference serving systems fail to account for multimodality, making them unable to scale effectively while meeting time-to-first-token (TTFT) SLOs, which now include image processing and encoding times.
\Cref{fig:monolith-vs-decoupled} shows how a monolithic deployment struggles to scale as the number of images per request increases (a common scenario in multi-image or video workloads) resulting in sharp TTFT degradation.
As a result, image-heavy requests can result in head-of-line (HoL) blocking, reducing system responsiveness and causing overprovisioning.

\jheading{Our Work}
In this paper, we present the first comprehensive systems analysis of two leading \lmm{} architectures:
cross-attention (\emph{CroAttn}) and decoder-only (\emph{DecOnly}), on both open-source \lmms{} and novel production \lmm{} inference traces in Azure datacenters.
We analyze their multi-stage inference pipelines, performance-resource tradeoffs, and production workload patterns, including variable request rates, diverse multimodal inputs, and bursty traffic.
We focus on Image-Text-to-Text but our insights extend to other multimodal scenarios, such as Video-Text-to-Text, where videos are processed as image frame sequences~\cite{llava-ov}, and Audio-Text-to-Text tasks~\cite{attt}, which share similar model architectures with the models we study.

Our analysis identifies three key insights for optimizing \lmm{} inference.
First, different \lmm{} inference stages exhibit diverse performance characteristics and varying sensitivity to resource and model configurations (\eg{}, batching and model sharding), necessitating \emph{\textbf{decoupled execution}}.
Second, image encoding is a major bottleneck for TTFT, requiring efficient \emph{\textbf{encoder parallelization}} to reduce both latency and HoL blocking.
Finally, production multimodal traffic exhibits distinct bursty patterns driven by increased images per request, highlighting the need for \emph{\textbf{modality-aware routing}} strategies to manage bursts and mitigate tail latency spikes.

Based on these insights, we propose \sysname{}, a novel \textit{\textbf{modular architecture}} for scalable and resource-efficient \lmm{} serving which directly addresses the challenges identified in our analysis.
\sysname{} separates image- and text-specific inference stages into distinct instances for decoupled execution.
In \sysname{}, \emph{Image Instances} handle image preprocessing and encoding, while \emph{Text Instances} manage LLM prefill and decoding (\Cref{fig:monolith-vs-decoupled}).
Text-only requests are served by \emph{Text Instances}, whereas image-text requests go through \emph{Image Instances} where images are converted to tokens before being forwarded to \emph{Text Instances} for text generation.

\sysname{}'s modular architecture unlocks stage-specific optimizations. \sysname{} manages \emph{Image} and \emph{Text Instances} independently with stage-aware autoscaling, model sharding, and batching.
By autoscaling the stages separately, it minimizes resource overprovisioning.
For example, during image-driven bursts observed in production traffic, \emph{Image Instances} can scale out independently, making \sysname{} more resource-efficient than monolithic inference systems.
To navigate the image encoding bottleneck, \sysname{} parallelizes encoding of a single request across multiple \emph{Image Instances} (\Cref{fig:monolith-vs-decoupled}), leveraging our finding that the images within a request do not attend to each other during encoding, and hence the requests can be parallelized at the image level.

Further, to manage image-driven bursts, \sysname{} implements modality-aware routing for \emph{Image and Text Instances}.
For example, images from image-text requests are routed to \emph{Image Instances} with the fewest pending image tokens to encode, reducing HoL blocking and tail latency spikes.

We implement \sysname{} on top of a high-performance inference system, vLLM~\cite{vllm}, and demonstrate the effectiveness of \sysname{} through extensive evaluations on a 16-server (128 GPUs) cluster running production Azure \lmm{} inference traces.
Compared to state-of-the-art baselines, \sysname{} achieves \textbf{\textit{3.3--5.5$\times$ higher throughput}} under static allocation and \textbf{\textit{reduces \lmm{} serving cost by 25--41.3\%}} while meeting the P99 TTFT SLOs.

\myparagraph{Summary}
This paper makes the following contributions:
\begin{itemize}[leftmargin=*]
\item A comprehensive system characterization on \lmm{} serving, examining performance profiles and resource utilization patterns across diverse workloads in both open-source \lmm deployments and production environment.
\item A large open-source dataset containing sampled production Azure \lmm{} inference traces.
\item Design and implementation of \sysname{}, a modular architecture for scalable and efficient \lmm{} serving.
\item A thorough evaluation of \sysname{} in a 128-GPU cluster using large-scale production traces.
\end{itemize}