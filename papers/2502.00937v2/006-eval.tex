\section{Evaluation}
\label{sec:eval}

\subsection{Experimental Setup}
\label{sec:eval:setup}

\myparagraph{Models and Workloads}
We use Llama3.2-11B and InternVL-26B as representative models for CroAttn and DecOnly \lmms{}, respectively.
To ensure realistic workload distribution, we reuse the \lmm{} dataset from \Cref{sec:characterization:open-source} and adopt the inter-arrival timestamps of requests and the number of images associated with each request (ranges from 0 to 16) from the production LMM inference trace (\Cref{sec:characterization:production}).

\myparagraph{Hardware}
We evaluate \sysname{} on a cluster with 16 DGX-A100 servers~\cite{a100azure} (128 GPUs).
Each server has the same configuration as the server used in our characterization study (\Cref{sec:characterization:open-source}).
The GPUs within a server are connected with NVLINK 3.0 while cross-server connection is via InfiniBand.

\myparagraph{Baselines and Systems}
We compare \sysname{} against the state-of-the-art generative model inference serving system, vLLM~\cite{vllm}, which supports \lmm{} inference as a monolithic setup.
We also evaluate \sysname{} with a few variants of \sysname{} implemented on top of vLLM:
(1) vLLM with decoupled Image/Text Instances (\ie{}, \sysname{}-Decoup),
(2) \sysname{}-Decoup plus modality-aware scheduling (\ie{}, \sysname{}-Sched), and
(3) \sysname{}-Sched plus modality-aware routing (\ie{}, \sysname{}), for ablation study.

\myparagraph{SLO Definition}
We define the SLO metrics for \lmm{} inference based on the TTFT/TBT during the isolated run of a single text-only request and text-image request (with one image) on the monolith baseline setup.
Then, we scale the SLO metrics with a constant factor (\ie{}, SLO factor) to evaluate how \sysname{} performs under tight/relaxed SLOs (\Cref{sec:eval:sensitivity}).
The SLOs are defined on P99 tail latency.

\subsection{End-to-end Performance}
\label{sec:eval:e2e}

\input{figs-tex/static-eval}

\myparagraph{Static Resource Allocation}
We begin by evaluating \sysname{} under a static resource allocation setup, where a fixed number of servers remain active at all times without autoscaling.
This setup isolates the benefits of decoupling, modality-aware request scheduling, and routing from pool autoscaling (which we explore independently).
\Cref{fig:eval:static-latency} shows the average and tail (P99) TTFT achieved by \sysname{} and the baselines when serving different input loads over fixed resources (16 servers with 128 GPUs in total).
In this setup, vLLM (monolith) deploys 32 instances (each with TP-4) while the other approaches (decoupled) deploy 20 Text Instances (TP-4) and 48 Image Instances (TP-1).

Compared to vLLM, statically decoupling (\sysname{}-Decoup) improves the average and P99 TTFT by 27\% and 42\% (for Llama3.2), 46\% and 47\% (for InternVL).
This is because monolithic deployments process all modalities on shared GPU resources, leading to contention and inefficient utilization under imbalanced modality traffic. 
In addition, \sysname{}-Decoup with the same number of GPUs can deploy 16 extra Image Instances and enables image encoding parallelization that reduces TTFT significantly compared to the monolithic deployment on vLLM.

\sysname{} shows a more pronounced TTFT improvement over the monolith baseline when serving InternVL.
This is because the monolith deployment faces resource contention with DecOnly models due to their high prefill latency (\Cref{insight:3}), which contends with image encoding.
Additionally, InternVL's image encoder has higher batching performance degradation (\Cref{insight:4}) and thus benefits more from parallelization.
Adding modality-aware request scheduling (\sysname{}-Sched) further reduces the average and P99 TTFT by 12\% and 25\%, modality-aware routing (\sysname{}) reduces the average and P99 TTFT by 14\% and 32\%, as it reduces HoL blocking and mitigates tail latency spikes.

Overall, \sysname{} achieves the lowest TTFT across all load levels, demonstrating the effectiveness of modular inference pipelines.
We observe similar TBT performance in all approaches due to its compute insensitivity (as indicated by \Cref{fig:batch-size}).
\Cref{fig:eval:static-capacity} further evaluates the maximum throughput under the TTFT and TBT SLO when varying the static resource allocation from 4 to 16 servers.
\sysname{} achieves a 3.3$\times$ and 5.5$\times$ throughput improvement over vLLM (monolith) for Llama3.2 and InternVL, respectively, which confirms that DecOnly models benefit more from decoupling. 

\input{figs-tex/autoscaling-timeseries}

\myparagraph{Resource Allocation with Autoscaling}
We now assess how \sysname{} and vLLM (monolith) baseline handle image-driven bursts seen in the production trace (\Cref{fig:qpm-prod}).
Fundamentally, to serve traffic bursts, a system needs to scale up the resources to meet the workload demand while scaling down to avoid overprovisioning.
Therefore, we enable autoscaling in both \sysname{} and vLLM and evaluate them on a one-day interval of the production trace that contains an image-driven burst.
For a fair comparison, both \sysname{} and vLLM (monolith) use similar SLO-driven autoscaling heuristics based on offline model profiling (\Cref{sec:design:resource-management}).

\Cref{fig:eval:autoscaling} compares the number of GPUs used by \sysname{} and vLLM (monolith) to serve the image-driven burst in the production trace. \sysname{} takes 41.3\% and 25\% fewer GPUs compared to vLLM to serve Llama-3.2 (CroAttn) and InternVL (DecOnly) models respectively while meeting the tail latency SLOs.
\sysname{}'s cost reduction is higher for Llama-3.2 (CroAttn) model as the increase in image tokens caused by image-driven bursts not overwhelming LLM backend in CroAttn models as observed in its latency profile (\Cref{fig:variation-ttft-mixed-tokens}).
However, in InternVL (DecOnly), the LLM backend's latency increases with the increase in image tokens due to homogeneous self-attention.
Therefore, to meet SLOs, \sysname{} scales up the number of Text Instances for InternVL more than for Llama-3.2 during image-driven bursts (light pink in \Cref{fig:eval:autoscaling}).
% Additionally, \sysname{} lowers P99 TTFT compared to vLLM (1753 ms vs. 2713 ms in Llama-3.2 and 5149 ms vs. 13192 ms in InternVL) due to parallel image preprocessing and encoding.
Overall, \sysname{}'s stage-aware autoscaling prevents unnecessarily scaling up LLM backends (done by vLLM due to monolith deployment) during image-driven bursts and prevents resource over-provisioning.

\subsection{Sensitivity Study}
\label{sec:eval:sensitivity}

\input{figs-tex/sensitivity-slo-scales}

\myparagraph{Impact of SLO Scale}
\Cref{fig:eval:sensitivity-slo-scale} shows the maximum throughput \sysname{} can achieve when changing the SLO scale (higher values refer to more relaxed SLOs).
As the SLO scale increases, \sysname{} consistently outperforms the vLLM, achieving up to 4.3$\times$ higher throughput for Llama-3.2 and 6.8$\times$ for InternVL.
This trend highlights that \sysname{} better utilizes resources under the same latency requirements.

\input{figs-tex/sensitivity-ratio}

\myparagraph{Impact of Image-to-Text Instance Ratio}
\Cref{fig:eval:sensitivity-ratio} shows the effect of varying the ratio of Image and Text Instances on 64 GPUs (8 servers) along the $X$-axis, in comparison to vLLM monolith with 16 instances.
For instance, ``4:48'' denotes a configuration with 4 Text Instances (TP-4) and 48 Image Instances (TP-1).
As the ratio of Text Instances increases, we observe that \sysname{} consistently achieves superior TTFT performance compared to vLLM (monolith) until the ratio reaches 10:24.
However, at 12:16, the decoupled configuration contains the same number of image encoders but 4 fewer LLM backends, resulting in inferior performance.
Moreover, reducing image encoders below the monolith baseline contradicts the core goal of decoupling to scale up/out the image encoders independently for multimodal processing.

\myparagraph{Impact of Image:Text Request Ratio}
\Cref{fig:eval:sensitivity-ratio} also shows the impact of varying image-text request percentages in the workload ($Y$-axis).
As this percentage increases from 10\% to 90\% (more image-heavy), TTFT for Llama-3.2 (CroAttn) increases.
InternVL (DecOnly) follows a similar trend, except at lower \emph{Text Instance} ratios (\eg{}, 4:48), where P99 TTFT decreases from 3.8 to 3.3 seconds due to reduced text load.
This stems from DecOnly models' poor prefill efficiency.
For the same reason, at low image-text request percentages (\eg{}, 10\%), InternVL sees a lower P99 TTFT as more \emph{Text Instances} help distribute the text-heavy load.

On the other hand, across all image-text request percentages, increasing the number of \emph{Text Instances} raises P99 TTFT in Llama3.2 due to a reduced number of \emph{Image Instances}, leading to longer image encoding times.
However, regardless of distribution, \sysname{} outperforms the monolith baseline (by up to 18.4$\times$ for Llama3.2 and 9.2$\times$ for InternVL) when Image:Text Instance ratio exceeds 2.4, demonstrating its efficiency handling multimodal workloads.