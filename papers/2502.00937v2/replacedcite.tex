\section{Related Work}
\myparagraph{\lmm Characterization}
Lee \etal{}____ provides a comprehensive characterization of multimodal generation models at Meta, while we focus on multimodal inputs.
Hou \etal{}____ focus on traditional multimodal models employing small-scale convolutional neural networks.
In contrast, our work presents a detailed analysis of multimodal input workloads on both open-source \lmm models and production traces, highlighting their unique execution and workload patterns.

\myparagraph{\lmm Serving}
Recent research has introduced several techniques to optimize \lmm serving by addressing key inefficiencies in inference computation and memory usage.
Inf-MLLM____ employs token caching strategies and attention bias to maintain performance with long contexts while reducing KV cache memory consumption.
Elastic Cache____ utilizes an importance-driven cache merging strategy to prune KV caches efficiently during inference.
Dynamic-LLaVA____, VTW____, and QueCC____ present various vision token sparsification and compression techniques to dynamically reduce redundancy in vision tokens.
These optimizations primarily operate at the model level, trading off computational overhead with model performance.
They are orthogonal to our proposed system-level design for SLO-driven \lmm serving that does not impact model performance, which can further benefit from such model-level advancements, e.g., faster image encoding through token compression.

\myparagraph{Text-Centric LLM Serving}
Recent studies have delved into disaggregating LLM prefill and decode phases for text-only LLM serving.
Examples include Splitwise____, DistServe____, Mooncake____, and MemServe____.
Other optimizations for LLM serving include key-value cache management____, continuous batching____, request scheduling____, and energy optimization____.
While these optimizations can be applied in \sysname{} to enhance LLM backend prefill and decode efficiency, our work focuses on the unique characteristics of multimodal models.