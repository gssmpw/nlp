\section{Large Multimodal Models Background}
\label{sec:background:lmm}

\lmms{} extend text-centric LLMs by integrating multimodal understanding capabilities for tasks like visual question answering~\cite{shao2023prompting} and computer-using agents~\cite{cua2025,niu2024screenagent}.
\Cref{fig:lmm-background} shows the typical pipeline of \lmm{} inference in visual understanding tasks~\cite{ittt}, which consists of three key stages: (1) \emph{image preprocessing}, where raw images are transformed into uniform-sized tiles; (2) \emph{image encoding}, where an encoder extracts visual features and produces a sequence of image tokens; and (3) \emph{text generation}, where an LLM backend processes the image and text tokens to generate output text tokens.
There are two dominant \lmm{} architectures that differ in how the LLM backend handles image tokens and text tokens:
(1) \emph{decoder-only} (DecOnly), used in models like DeepSeek's Janus~\cite{janus}, LLaVA-OneVision~\cite{llava-ov}, InternVL~\cite{chen2024internvl}, and NVLM-D~\cite{nvlm}; and (2) \emph{cross-attention-based} (CroAttn), found in Llama-3.2 Vision~\cite{llama3}, NVLM-X~\cite{nvlm}, and Flamingo~\cite{alayrac2022flamingo}.
In this work, we analyze six open-source \lmms{} (listed in \Cref{table:model-config}) across these architectures, varying image encoder sizes (400M–6B) and LLM scales (7B–72B).

\input{tables/open-source-model-description}

\input{figs-tex/lmm-background}

\myparagraph{Image Preprocessing}
\lmms{} typically follow three preprocessing steps on CPU:
(1) resize, rescale, pad, and normalize the raw image, (2) segment it into tiles~\cite{llama3,nvlm,chen2024internvl} or patches~\cite{llava-ov}, and (3) apply tile/patch-level transformations and sampling.
The number of tiles varies, with higher image dimensions resulting in more tiles, which ultimately increases the number of image tokens.
For example, an image with 896\texttimes{}896 pixels generates 4, 5, or 10 tiles of different sizes after preprocessing for six open-source \lmms{} (\Cref{table:model-config}).

\myparagraph{Image Encoding}
The image encoder takes processed image tiles as input and produces image tokens that are then passed to the language model backend.
Today's image encoders predominantly use the vision transformer architecture~\cite{alexey2020image} to extract visual features from images.
\Cref{table:model-config} shows that different \lmms{} use different encoders~\cite{llava-ov,alexey2020image,zhai_sigmoid_2023,chen2024internvl}, leading to variations in the number of image tokens when running image encoders on the same ShareGPT-4o dataset~\cite{chen2024far} (\Cref{fig:img-token-distribution-open}).
This is due to differences in the number of tiles and image tokens generated per tile by each encoder.

\input{figs-tex/img-token-distribution}

\myparagraph{Text Generation}
Image and text prompt tokens are combined and passed through LLM prefill and decode to generate output tokens, typically using one of two architectures:


\myparagraphemph{Decoder-Only (DecOnly) \lmms{}}
An unmodified LLM backend is reused in DecOnly \lmms{} (\eg{}, LLaVA-OV reuses Qwen2 LLM~\cite{llava-ov}), processing text and image tokens uniformly (``DecOnly'' box in \Cref{fig:lmm-background}).
While valued for their simplicity and unified modality handling, DecOnly models often require long sequences for high-resolution images, resulting in computational inefficiencies during inference.

\myparagraphemph{Cross-Attention (CroAttn) \lmms{}}
Unlike DecOnly \lmms{}, which leave the LLM backend unchanged, CroAttn-based models (\eg{}, Llama-3.2 Vision) integrate \emph{cross-attention layers} to process image tokens, treating visual inputs like a ``foreign language'' in the LLM backend.
While more complex to train, they improve inference efficiency by avoiding full image token unrolling in the LLM decoder, making them ideal for high-resolution inputs.
Self-attention operates on text tokens, while the cross-attention layer attends to both text and image tokens (``CroAttn'' box in \Cref{fig:lmm-background}).


\myparagraph{SLO Metrics for \lmm{} Inference}
Production \lmm{} serving systems need to satisfy SLOs defined on tail latency (\eg{}, P99) for worst-case performance.
These SLO metrics include \emph{Time to First Token (TTFT)} and \emph{Time Between Tokens (TBT)}.
%
TTFT measures the latency from query (text/images) to the first response token, critical for interactive applications.
In contrast to text-centric LLM serving, \lmm{} TTFT includes the following stages of \lmms{} inference pipeline:
(1) image preprocessing,
(2) encoding, and
(3) language model prefill time.
%
TBT captures the delay between consecutive token generations during decoding, affecting output fluency.
As multimodal preprocessing and encoding primarily influence TTFT, in this work, we focus on TTFT while leveraging state-of-the-art techniques~\cite{sarathi-serve,patel2024splitwise} to optimize TBT.

An ideal \lmm{}-serving system should meet TTFT/TBT SLOs while maximizing request \emph{throughput} (\ie{}, goodput) and compute \emph{utilization} (GPU cost).


\myparagraph{\lmm{} Deployments Today}
State-of-the-art serving frameworks~\cite{vllm,hf,deepspeed} deploy \lmms{} as \emph{monolithic} systems to meet latency SLOs.
In this setup, all inference components (\ie{}, image preprocessor, image encoder, and LLM backend) are co-located on the same hardware server as a single unit.
These tightly coupled components share uniform batching and model parallelism strategies across the pipeline.
\Cref{table:model-config} details the default model parallelism for our open-source \lmms{}.
While this monolithic design is straightforward to implement and common in open-source \lmm{} serving, it limits flexibility and suffers from sharp TTFT degradation under image-heavy workloads (\Cref{fig:monolith-vs-decoupled}).