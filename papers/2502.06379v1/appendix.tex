

\section{Proofs}
\label{app:proofs}
\subsection{Equality between marginals}
The marginal distributions $p_\theta^0(\xt)$ and $p_\theta(\xt)$ are equal, as follows from the following proposition, inspired by \citet{zhang_improving_2024}.
\begin{proposition}
Conditioning on a sample $\xt \sim p(\xt)$ to sample $\x_0 \sim p(\x_0|\xt)$, and then forgetting about $\xt$ when sampling $\xtone \sim p(\xtone|\x_0)$, leads to a draw from $p(\xtone)$. 
\label{prop:dapsprop}
\end{proposition}
\begin{proof}
\begin{align}
p(\xtone) &= 
\int \underbrace{p(\xtone |\x_0)}_{(*)}\underbrace{p(\x_0)}_{(**)}d\x_0 = \\
&= \int  \underbrace{p(\xtone |\x_0)}_{(*)} \underbrace{\int p(\x_0|\xt) p(\xt) d\x_t}_{(**)} d\x_0 \\
&= \int \int p(\xtone |\x_0) p(\x_0|\xt) p(\xt) d\x_0 d\x_t \\
&= \mathbb{E}_{\xt}
\mathbb{E}_{\x_0|\xt}
p(\xtone|\x_0).
\label{eq:prop1proof}
\end{align}
\end{proof}
\begin{remark}
Proposition \ref{prop:dapsprop} is the same as the proposition in \citep{zhang_improving_2024}, but without conditioning on $\y$. Our proof can be used also for the conditional case (by only adding the corresponding conditioning on $\y$), but is different from the one by \citet{zhang_improving_2024} as it does not rely on the assumption of their graphical model.
\end{remark}

\section{DDSMC for DDPM/VP diffusion models}
\label{app:ddsmc_ddpm}
In the DDPM \citep{ho_denoising_2020-2} or Variance-preserving (VP) \citep{song_score-based_2021} formulation of a diffusion model, the forward process can described by
\begin{align}
    q(\xtplusone|\xt) = \mathcal{N}(\xtplusone|\sqrt{1-\beta_{t+1}}\xt, \beta_{t+1}I), \label{eq:ddpmforward}
\end{align}
which leads to 
\begin{align}
    q(\xt|\x_0) = \mathcal{N}(\xt|\sqrt{\bar\alpha_t}\x_0, (1-\bar\alpha_t)I), \label{eq:ddpmtzero}
\end{align}
where $\bar \alpha_t = \prod_{s=1}^t\alpha_s$ with $\alpha_s = 1 - \beta_s$.

\subsection{Target distribution}
The target distribution changes as the transition $p_\theta^0(\xt|\xtplusone) = \int q(\xt|\x_0)\delta_{\recon(\xtplusone)}(\x_0)d\x_0$ changes when $q(\xt|\x_0)$ changes. The new expression is now $p_\theta^0(\xt|\xtplusone) = \mathcal{N}\left(\sqrt{\bar \alpha_t}\recon(\xtplusone), (1-\bar \alpha_t)I\right)$

\subsection{Proposal}
In the DDPM formulation, we reuse the same Gaussian prior over $\x_0$ given $\xt$ as before, $\tilde p_\theta(\x_0|\xt) = \mathcal{N}(\recon(\xt), \rho_t^2I)$ and hence the same posterior $\tilde p_\theta(\x_0|\xtplusone, \y) = \mathcal{N}(\tilde \mu_\theta^t(\xtplusone, \y), \mM_{t+1}^{-1})$. However, we use $\tilde q(\xt|\x_0) = \mathcal{N}(\xt|\sqrt{\bar \alpha_t}\xt, \lambda_t^2I)$ which means that the proposal becomes
\begin{align}
    r_t(\xt|\xtplusone, \y) 
    &= \int \tilde q(\xt|\x_0) \tilde p_\theta(\x_0|\xtplusone, \y) d\x_0 \\
    &= \mathcal{N}(\sqrt{\bar \alpha_t}\tilde \mu^t_\theta(\xtplusone, \y), \lambda_t^2I + \bar \alpha_t \mM_{t+1}^{-1}).
\end{align}
To match the prior in case of non-informative measurements, we set $\lambda_t^2 = 1- \bar \alpha_t - \bar \alpha_{t} \rho^2_{t+1} = 1 - \bar \alpha_t(1 + \rho_{t+1}^2)$.

\section{Comparison with TDS and MCGDiff}
\label{app:smc_comparison}
In this section, we write down the main components in an SMC algorithm, the target distributions and proposals, for Twisted Diffusion Sampler (TDS) \citep{wu_practical_2023} and Monte Carlo Guided Diffusion (MCGDiff) \citep{cardoso_monte_2023-2},

\subsection{TDS}
\paragraph{Target distributions} TDS approximates the intractable likelihood $p(\y|\xt)$ by $\hat p(\y|\xt) \eqdef p(\y|\x_0=\recon(\xt))$. The intermediate target distributions for TDS are hence
\begin{align}
\gamma_t^{TDS}(\xtt) &= \hat p(\y|\xt) p(\x_T)\prod_{s=t}^{T-1} \ptheta(\x_s|\x_{s+1}), 
\end{align}
where
\begin{align}
p_\theta(\xs|\xsplusone) 
&= \mathcal{N}\left(\xs|\xsplusone + \beta_{s+1} s_\theta(\x_{s+1}, s+1), \beta_{s+1}^2I\right). \\
&= \mathcal{N}\left(\xs |\left(1 -\frac{\beta_{s+1}}{\sigma_{s+1}^2}\right)\xsplusone + \frac{\beta_{s+1}}{\sigma_{s+1}^2}\recon(\xsplusone), \beta_{t+1}\right),
\end{align}
and the last equality comes from rewriting the score as $(\recon(\xsplusone) - \xsplusone)/\sigma_{s+1}^2$ according to Tweedie's formula.

\paragraph{Proposal}
In TDS, they use the proposal
\begin{align}
    r_t^{TDS}(\xt|\xtplusone, \y) 
    &= \mathcal{N}\left(\xt|\xtplusone + \beta_{t+1}(s_\theta(\xtplusone, t+1) + \nabla_{\xtplusone} \log \hat p(\y|\xtplusone)), \beta_{t+1}\right) \\
    &= \mathcal{N}\left(\xt|\left(1-\frac{\beta_{t+1}}{\sigma_{t+1}^2}\right)\xtplusone + \frac{\beta_{t+1}}{\sigma_{t+1}^2}\recon(\xtplusone) + \nabla_{\xtplusone} \log \hat p(\y|\xtplusone), \beta_{t+1}\right),
\end{align}
where again the last equality is from rewriting the score according to Tweedie's formula. This proposal is reminiscent of classifier guidance \citep{dhariwal_diffusion_2021-1}, where the unconditional score is combined with the gradient of some log likelihood, in this case the log likelihood $\log \hat p(\y|\xtplusone)$ from the target distribution. As this likelihood relies on the reconstruction $\recon(\xtplusone)$, computing the gradient of $\log \hat p(\y|\xtplusone)$ with respect to $\xtplusone$ means differentiating through the reconstruction, which could be computationally very expensive. On the other hand, this proposal is not limited to the linear-Gaussian case, but works with any differentiable $\hat p(\y|\xtplusone)$%
. 

\subsection{MCGDiff}
MCGDiff relies on the DDPM or Variance-preserving formulation of diffusion models when describing their method, and we will use it here. To discuss MCGDiff, we will work under their initial premises that $\y = \ox + \sigma_y\epsilon$, i.e., $A$ merely extracts the top coordinates of $\x$ (denoted $\ox$), and $\y$ is a potentially noisy observation of these coordinates\footnote{For general $A$, they also make use of the singular value decomposition of $A$ to make a change of basis where the "new" $A$ has this property.}. The variable $\ux$ represents the completely unobserved coordinates. We also follow their presentation, starting with the assumption of a noise-free observation $\y$, then continuing to the noisy case.

\subsubsection{Noiseless case}
\paragraph{Target distributions}
In the noiseless case, they target for $t=0$
\begin{align}
    \gammazero = p_\theta(\x_T) \left[\prod_{t=1}^{T-1}p_\theta(\xt|\xtplusone) \right]\underbrace{p_\theta(\y \concat \ux_0)|\x_1)}_{\delta_\y(\ox_0)p_\theta(\ux_0|\x_1)}.
\end{align}
where $p_\theta$ is the regular diffusion model. By then defining a likelihood for $t=0$ as $\bar q_{0|0}(\y|\ox_0) = \delta_\y(\ox_0)$ and $\bar q_{t|0}(\y|\ox_t) = \mathcal{N}(\sqrt{\abt}\y, (1-\abt)I)$ for $t>0$, they have a target on the general form
\begin{align}
    \gammat = \frac{\bar q_{t|0}(\y|\ox_t)}{\bar q_{t+1|0}(\y|\ox_{t+1})}p(\xt|\xtplusone)\gammatplusone.
\end{align}

\paragraph{Proposal}
For $0<t<T$, they construct their proposal
\begin{align}
    r_t(\xt|\xtplusone, \y) = \frac{p_\theta(\xt|\xtplusone)\bar q_{t|0}(\y|\ox_t)}{\int p_\theta(\xt|\xtplusone)\bar q_{t|0}(\y|\ox_t) d\xt} 
    = p_\theta(\ux_t|\xtplusone)\frac{p_\theta(\ox_t|\xtplusone)\bar q_{t|0}(\y|\ox_t)}{\int p_\theta(\ox_t|\xtplusone)\bar q_{t|0}(\y|\ox_t) d\ox_t},
\end{align}
and as everything is Gaussian (transition and likelihood), all terms can be computed exactly. In words, they use the "regular" diffusion model $p_\theta(\ux_t|\xtplusone)$ for the unobserved coordinates, and for the observed coordinates, they also weigh in the likelihood in the update. For $t=0$, they instead use
\begin{align}
    r_0(\x_0|\x_1, \y) = \delta_\y(\ox_0)p_\theta(\ux_0|\x_1)
\end{align}

\paragraph{Weighting function}
We have not written out the weighting function $\gammat / (r_t(\xt|\xtplusone)\gammatplusone)$ for DDSMC and TDS, but we do for MCGDiff. With the target and proposal above, the weighting function becomes
\begin{align}
    w_t(\xt,\xtplusone) &= \underbrace{\frac{\int p_\theta(\xt|\xtplusone)\bar q_{t|0}(\y|\ox_t) d\xt} {p_\theta(\xt|\xtplusone)\bar q_{t|0}(\y|\ox_t)} }_{1/q_t(\xt|\xtplusone)}
    \underbrace{
    \frac{\bar q_{t|0}(\y|\ox_t)}{\bar q_{t+1|0}(\y|\ox_{t+1})}p_\theta(\xt|\xtplusone)
    }_{\gammat/\gammatplusone} = \\
    &= \frac{\int p_\theta(\xt|\xtplusone)\bar q_{t|0}(\y|\ox_t) d\xt}{\bar q_{t+1|0}(\y|\ox_{t+1})}.
\end{align}
We do this as this weight does not actually depend on $\xt$, and can hence be computed \emph{before} sampling from the proposal. This opens up the possibility for a so called fully adapted particle filter, where one can sample exactly from the target and the importance weights are always $1/N$.

\subsubsection{Noisy Case}
In the noisy case, they assume that there exists a timestep $\tau$ such that $\sigma_y^2 = \frac{1-\abtau}{\abtau}$. They further define $\ytildetau = \sqrt{\abtau} \y$

They use as targets for $t\geq\tau$
\begin{align}
    \gammat 
    &= g_t(\xt)p_\theta(\x_T)\prod_{s=t}^{T-1}p_\theta(\xs|\xsplusone) \\
    &= \frac{g_t(\xt)}{g_{t+1}(\xtplusone)}p_\theta(\xt|\xtplusone)\gammatplusone
\end{align}
with the potential
\begin{align}
    g_t(\xt) = \mathcal{N}\left(\xt|\sqrt{\abt}\y, (1-(1-\kappa)\frac{\abt}{\abtau})I\right).
\end{align}
For $t=0$, they target
\begin{align}
    \gamma_0(\x_0, \x_{\tau:T}) 
    &= p(\y|\ox_0)p_\theta(\x_0|\x_\tau)p_\theta(\x_T)\prod_{s=\tau}^{T-1}p(\xs|\xsplusone) \\
    &= \frac{p(\y|\ox_0)p_\theta(\x_0|\x_\tau)}{g_\tau(\x_\tau)}\gamma_\tau(\x_{\tau:T})
\end{align}
What this means is that they between $T$ and $\tau$ run their algorithm conditioning on $\y$, and then sample unconditionally between $\tau$ and $0$.

\section{Detailed Expressions for General DAPS Prior}
\label{app:general_DAPS_prior}
We generalize the DAPS prior by tempering the "likelihood" $q(\xtplusone|\xt)$, and denote 
\begin{align}
    q_\eta(\xt|\xtplusone, \x_0) \propto q(\xtplusone|\xt)q(\xt|\x_0). \label{eq:q_eta_general_app}
\end{align}

\subsection{Variance Exploding}
In the "Variance Exploding" setting that we have used throughout the paper, \Cref{eq:q_eta_general_app} becomes
\begin{align}
    q_\eta(\xt|\xtplusone, \x_0) = \mathcal{N}\left(\frac{\beta_{t+1}}{\eta\sigma_t^2 + \beta_{t+1}}\x_0 + \frac{\eta\sigma_t^2}{\eta\sigma_t^2 + \beta_{t+1}}\xtplusone, \frac{\beta_{t+1}}{\eta\sigma_t^2 + \beta_{t+1}}I\right).
\end{align}
We then get our generalized backward kernel $p_\theta^{\eta}(\xt|\xtplusone)$ by replacing $\x_0$ with the reconstruction $\recon(\xtplusone)$, i.e.,
\begin{align}
    p_\theta^\eta(\xt|\xtplusone) =\mathcal{N}\left(\frac{\beta_{t+1}}{\eta\sigma_t^2 + \beta_{t+1}}\recon(\xtplusone)
    +
    \frac{\eta\sigma_t^2}{\eta\sigma_t^2 + \beta_{t+1}}\xtplusone, \frac{\beta_{t+1}\sigma_t^2}{\eta\sigma_t^2 + \beta_{t+1}}I\right)
\end{align}
For the proposal, the posterior $\tilde p(\x_0|\xtplusone, \y)$ remains as in \Cref{eq:x0_posterior_closed_form}, but we use a generalization 
\begin{align}
    r_t(\xt|\xtplusone, \y) = \int \tilde q_\eta(\xt|\xtplusone, \x_0)\tilde p(\x_0|\xtplusone, \y)d\x_0
\end{align}
where $\tilde q_\eta(\xt|\xtplusone, \x_0)$ is on the form
\begin{align}
    q_\eta(\xt|\xtplusone, \x_0) = \mathcal{N}\left(\frac{\beta_{t+1}}{\eta\sigma_t^2 + \beta_{t+1}}\x_0 + \frac{\eta\sigma_t^2}{\eta\sigma_t^2 + \beta_{t+1}}\xtplusone, \lambda_t^2I\right).
\end{align}
This gives the proposal
\begin{gather}
    r_t(\xt|\xtplusone, \y) = \nonumber \\
    =\mathcal{N}\left(
    \frac{\beta_{t+1}}{\eta\sigma_t^2 + \beta_{t+1}} \mM_{t+1}^{-1}\rvb_{t+1} + \frac{\gamma\sigma_t^2}{\eta\sigma_t^2 + \beta_{t+1}}\xtplusone, \lambda_t^2 I + \left(\frac{\beta_{t+1}}{\eta\sigma_t^2 + \beta_{t+1}}\right)^2 \mM_{t+1}^{-1}), \label{eq:general_eta_proposal}
    \right)
\end{gather}
where we choose
\begin{align}
    \lambda_t^2 = \frac{\beta_{t+1}\sigma_t^2}{\eta\sigma_t^2 + \beta_{t+1}} - \left(\frac{\beta_{t+1}\rho_{t+1}}{\eta\sigma_t^2 + \beta_{t+1}}\right)^2 
\end{align}
so that the proposal matches the prior in case of non-informative measurements ($\sigma_y \rightarrow \infty$).

\subsection{DDPM/VE}
Using the same techniques as for VE but replacing $q(\cdot)$ with its corresponding expressions, we get the transition 
\begin{align}
    p_\theta^\eta(\xt|\xtplusone) = \mathcal{N}\left(
    \frac{\sqrt{\abt}\beta_{t+1}\recon(\xtplusone)
    +
    \eta\sqrt{1-\beta_{t+1}}(1-\abt)\xtplusone}{\eta-\eta\beta_{t+1} - \eta\abtplusone + \beta_{t+1}},
    \frac{\beta_{t+1}(1-\abt)}{\eta-\eta\beta_{t+1} - \eta\abtplusone + \beta_{t+1}}I
    \right),
\end{align}
and with the same posterior $\tilde p(\x_0|\xt, \xtplusone)$ and the same procedure as before, we get the proposal as
\begin{align}
    r_t(\xt|\xtplusone, \y) = \mathcal{N}\left( \frac{\sqrt{\abt}\beta_{t+1}\mM_{t+1}^{-1}\rvb_{t+1}
    +
    \eta\sqrt{1-\beta_{t+1}}(1-\abt)\xtplusone}{\eta-\eta\beta_{t+1} - \eta\abtplusone + \beta_{t+1}},
    \lambda^2I +
    \frac{\beta_{t+1}(1-\abt)}{\eta-\eta\beta_{t+1} - \eta\abtplusone + \beta_{t+1}}I
    \right)
\end{align}
with 
\begin{align}
    \lambda_t^2 = 1-\abt - \left(\frac{\sqrt{\abt}\beta_{t+1}\rho_{t+1}}{\eta-\eta\beta_{t+1} - \eta\abtplusone + \beta_{t+1}}\right)^2
\end{align}

\section{discrete Version}
\label{app:d3smc}
\subsection{Discrete Denoising Diffusion Probabilistic Models}
DDSMC relies on the data $\x_0$ being a vector of continuous variables. For discrete data, there are a number of different formulations developed like \cite{austin_structured_2021, campbell_continuous_2022-1, dieleman_continuous_2022, sun_score-based_2023}. In our work, we focus on the Discrete Denoising Diffusion Probabilistic Models (D3PM) model \citep{austin_structured_2021}. 

To introduce D3PM, we will denote by bold $\x$ a one-hot encoding of a single discrete variable. In this case, the forward noise process can be described as
\begin{align}
    q(\xtplusone|\xt) = \xt Q_{t+1},
\end{align}
and as for the continuous case we have a closed form expression for $q(\xt|\x_0)$ as
\begin{align}
    q(\xt|\x_0) = \x_0 \bar Q_{t}
\end{align}
where $\bar Q_t = Q_1\hdots Q_{t-1} Q_t$. The backward kernel $q(\xt|\xtplusone, \x_0)$ is also known, and as we are in the discrete regime, the backward kernel can hence be parametrized by marginalizing over $\x_0$ as
\begin{align}
    p_\theta(\xt|\xtplusone) = \sum_{x_0} q(\xt|\xtplusone, \x_0)\tilde p_\theta(\x_0|\xtplusone),
\end{align}
where $\tilde p_\theta(\xt|\xtplusone)$ is a categorical distribution predicted by a neural network. 

\paragraph{Generalizing to $D$ Variables}
The reasoning above can be extended to general $D$-dimensional data by assuming a noise process where the noise is added independently to each ov the variables, meaning $q(\cdot)$ factorizes over these variables. Similarly, it is also assumed that the backward process $p(\cdot|\xtplusone)$ factorizes over the different variables. A single forward pass through the neural networks hence produces probability vectors in $D$ difference categorical distributions.

\subsection{Measurements}
For the discrete setting where we have a single variable, we assume that we have a measurement $\y$ on the form
\begin{align}
    p(\y|\x_0) = \x_0 Q_y. \label{eq:discrete_measurement_model}
\end{align}
For the general $D$-dimensional case, we hence assume that we have $D$ measurements, one for each variable in $\x_0$, and that these are obtained independently from each other. 

\subsection{D3SMC}
In the discrete setting, it might be tempting to find an "optimal" proposal 
\begin{align}
    r_t(\xt|\xtplusone, \y) =  \frac{p(\y|\xt)p(\xt|\xtplusone)}{p(\y|\xtplusone)}.
\end{align} 
However, it is not possible to compute 
\begin{align}
    p(\y|\xt) = \sum_{\x_0}p(\y|\x_0)p(\x_0|\xt)
\end{align}
in a single evaluation as it requires evaluations of the neural network prior for all possible values of $\xt$. If only a single variable is updated at each denoising step, like in an order-agnostic autoregressive model \citep{uria_deep_2014}, this could be feasible \cite{ekstrom_kelvinius_discriminator_2024}, but for the $D$-dimensional case will require $d^D$ evaluations (where $d$ is the size of the state space of $\xt$) for D3PM with uniform noise as all $D$ variables are updated each step. Hence, we have created a proposal which only requires a single forward pass at each sampling step in the general $D$-dimensional case. 

Deriving the D3SMC algorithm is rather straight forward. In the derivation, we will continue using the notation with tilde. However, when we made Gaussian approximations around a reconstruction in the continuous case, we will in the discrete case use the categorical distribution predicted by our neural network, $\tilde p_\theta(\x_0|\xtplusone)$. 

\paragraph{Target Distribution}
In analogy with DDSMC, we write our target distribution $\gamma_t(\xt)$ as
\begin{align}
    \gamma_t(\xt) 
    = \frac{\tilde p(\y|\xt)}{\tilde p(\y|\xtplusone)}
    p^0_\theta(\xt|\xtplusone)\gamma_{t+1}(\xtplusone) 
\end{align}
where $p_\theta^0(\xt|\xtplusone) = \text{Categorical}(\rvp = \x^{\theta}_{0, t+1}\bar Q_t)$ with $\x^{\theta}_{0, t+1}$ being the predicted distribution over $\x_0$ by the D3PM model (i.e., $\tilde p_\theta(\x_0|\xtplusone)$), and $\tilde p(\y|\xt) = \sum_{\x_0}p(\y|\x_0)\tilde p_\theta(\x_0|\xt)$. The distribution $p_\theta^0(\xt|\xtplusone)$ is a D3PM analogy to the DAPS kernel, as we predict $\x_0$ and then propagate this forward in time.

\paragraph{Proposal Distribution}
The discrete proposal starts, just as for the continuous case, with a posterior over $\x_0$, 
\begin{align}
    \tilde p_\theta(\x_0|\xtplusone, \y) 
    &= \frac{p(\y|\x_0) \tilde p_\theta(\x_0|\xtplusone)}{\sum_{\x_0} p(\y|\x_0) p_\theta(\x_0|\xtplusone)}\\
    &= \frac{\y Q_y^T \odot \x_{0,t+1}^{\theta}}{Z_{t+1}},
\end{align}
where $\odot$ means element-wise multiplication and $Z_t$ can be computed by summing the elements in the numerator. With this, we can obtain a proposal distribution in analogy with the continuous case 
\begin{gather}
    r_t(\xt|\xtplusone, \y) = \sum_{\x_0} p(\xt|\x_0)\tilde p(\x_0|\xtplusone, \y) = \nonumber \\
    = \sum_{\x_0} \x_0 \bar Q_t \left(\left(\frac{\y Q_y^T \odot \x_{0, t+1}^{\theta}}{Z_{t+1}}\right)(\x_0)^T\right). \label{eq:discrete_proposal}
\end{gather}

\section{Additional Experimental Details and Results}
\subsection{Gaussian Mixture Model}
\label{app:gmm}
\subsubsection{Prior and Posterior}
For the GMM experiments, we tried following MCGDiff as closely as possible, and therefore used and modified code from the MCGDiff repository\footnote{\url{https://github.com/gabrielvc/mcg_diff}}. We briefly outline the setup here, but refer to their paper for the full details.

The data prior is a Gaussian Mixture with 25 components, each having unit covariances. Under the DDPM/VP model (see \Cref{app:ddsmc_ddpm}), the intermediate marginal distributions $q(\xt) = \int q(\xt|\x_0)q(\x_0)d\x_0$ will also be a mixture of Gaussians, and it is hence possible to compute $\nabla_{\xt}\log q(\xt)$ exactly. 

The sequence of $\beta_t$ is a linearly decreasing sequence between $0.02$ and $10^{-4}$.

The mixture weights and the measurement model $(A, \sigma_y)$ are randomly generated, and a measurement is then drawn by sampling first $\x_0^* \sim q(\x_0)$, then $\epsilon\sim \mathcal{N}(0, I)$, and finally setting $y=A\x^* + \sigma_y\epsilon$. From this measurement, it is possible to compute the posterior exactly, and 10k samples were drawn from the posterior. Additionally, 10k samples were generated by each of the algorithms, and the Sliced Wassertstein Distance between the exact posterior samples and the SMC samples were computed using the Python Optimal Transport (POT) package\footnote{\url{https://pythonot.github.io/index.html}} \cite{flamary_pot_2021}

\subsubsection{Implementation Details}
For the three SMC methods, we used 256 particles for all algorithms, and 20 DDIM timesteps \cite{song_denoising_2021}. In this case, the timesteps were chosen using the method described in MCGDiff. 

For DDSMC and DAPS, we used the DDIM ODE solver with the update rule
\begin{align}
    \xt = \frac{\bar \alpha_{t}}{\bar \alpha_{t+1}}\xtplusone + \left(
    (1-\bar \alpha_{t+1})\sqrt{\frac{\bar \alpha_{t}}{\bar \alpha_{t+1}}} - \sqrt{(1-\bar \alpha_{t})(1-\bar \alpha_{t+1})}
    \right)\nabla_{\xtplusone}q(\xtplusone). \label{eq:ddim_ode}
\end{align}

\paragraph{DDSMC}
We use $\rho_t^2 = (1-\bar \alpha_t)/\sqrt{2}$

\paragraph{MCGDiff}
We used code from the official implementation\footnote{\url{https://github.com/gabrielvc/mcg_diff}}. As per their paper, we used $\kappa=10^{-2}$. 

\paragraph{TDS}
We implemented the method ourselves according to their Algorithm 1, making the adaptions for the DDPM/VP setting (also described in their Appendix A.1).
\paragraph{DAPS}
We implemented the method ourselves. As we are in the linear-Gaussian setting, we replaced their approximate sampling of $\tilde p(\x_0|\xt, \y)$ with the exact expression (given by our \Cref{eq:x0_posterior_closed_form,eq:x0_posterior_mean_precisionMat}). We used the same schedule for $r_t$ as for our $\rho_t$. 

\paragraph{DDRM}
We used code from the official repository\footnote{\url{https://github.com/bahjat-kawar/ddrm}}, and used the same hyper-parameters $\eta$ and $\eta_b$ as in their paper. We all 999 timesteps.

\paragraph{DCPS}
We used code from the official repository \footnote{\url{https://github.com/Badr-MOUFAD/dcps/}}. We used $L=3$, $K=2$, and $\gamma=10^{-2}$ as specified in their paper. We used all 999 timesteps.

\subsubsection{Additional Qualitative Results}
In \Cref{fig:gmm_ddsmc_appendix_8,fig:gmm_ddsmc_appendix_80,fig:gmm_ddsmc_appendix_800} we present additional qualitative results to compare DDSMC with different $\eta$ and reconstruction functions. In \Cref{fig:gmm_appendix_8,fig:gmm_appendix_80,fig:gmm_appendix_800}, we present extended qualitative comparisons with other methods.
\input{figures/code/appendix_gmm_ddsmc_only_8}
\input{figures/code/appendix_gmm_ddsmc_only_80}
\input{figures/code/appendix_gmm_ddsmc_only_800}
\input{figures/code/appendix_gmm_8}
\input{figures/code/appendix_gmm_80}
\input{figures/code/appendix_gmm_800}

\subsection{Image restoration}
\label{app:image}
\subsubsection{DDSMC Implementation Details and Hyperparameters}
For the image restoration tasks, we used $N=5$ particles.
\paragraph{Time and $\sigma_t$ Schedule}
We follow DAPS which uses a noise-schedule $\sigma_{t_i} = t_i$ where the timepoints $t_i$ ($i=1, \dots, N$) are chosen as 
\begin{align}
    t_i = \left( t_\text{max}^{1/7} + \frac{i}{M-1}\left(t_\text{min}^{1/7} - t_\text{max}^{1/7}\right)\right).\label{eq:sigma_schedule}
\end{align} 
We used $t_\text{max} = 100$, $t_\text{min}=0.1$ and $M=200$

\paragraph{$\rho_t$ schedule}
In their public implementation\footnote{\url{https://github.com/zhangbingliang2019/DAPS/}}, DAPS uses $\rho_t=\sigma$. However, we find that for when using the exact solution $\tilde p(\x_0|\xtplusone, \y)$, these values are too high. We therefore lower $\rho_t$ by using a similar schedule as for $\sigma_t$, 
\begin{align}
    \rho_{t_i} = \left( \rho_\text{max}^{1/7} + \frac{i}{M-1}\left(\rho_\text{min}^{1/7} - \rho_\text{max}^{1/7}\right)\right).
\end{align}
For the image experiments, we used $\rho_\text{max} = 50$ and $\rho_\text{min}=0.03$. 

\paragraph{PF-ODE}
Again, we followed DAPS and used an Euler solver to solve the PF-ODE by \citet{karras_elucidating_2022-1}, using $M_\text{ODE}=5$ steps with $\sigma$ according to the same procedure as for the outer diffusion process (i.e., \Cref{eq:sigma_schedule}), at each step $t_i$ starting with $t_\text{max} = t_i$ and ending with $t_\text{min} = 0.01$.

\subsubsection{Implementation of Other Methods}
For the other methods, we used the DCPS codebase \footnote{\url{https://github.com/Badr-MOUFAD/dcps/}} as basis where we implemented DDSMC and transferred DAPS.

\paragraph{DAPS}
We used the hyperparameters as in their public repository\footnote{\url{https://github.com/zhangbingliang2019/DAPS}}.

\paragraph{DCPS}
We used the settings from their public repository, running the algorithm for 300 steps as that was what was specified in their repository.

\paragraph{DDRM}
Just as for DCPS, we used 300 steps.

\paragraph{MCGDiff}
We used 32 particles. 

\subsubsection{Additional Qualitative Results}
We provide additional results on the image reconstruction task in \Cref{fig:ffhq_appendix_outpainting_half,fig:ffhq_appendix_inpainting_middle,fig:ffhq_appendix_sr4}.
\input{figures/code/appendix_outpainting_half}
\input{figures/code/appendix_inpainting_middle}
\input{figures/code/appendix_sr4}

\subsection{Binary MNIST}
\label{app:bmnist}
In \Cref{fig:bmnist_10runs_5particles} and \ref{fig:bmnist_10runs_100particles} we provide extended results when running D3SMC on binary MNIST, using the same measurement model as in the main text and $N=5$ or $N=100$ particles, respectively.
\input{figures/code/bmnist_20_10runs_5particles} 
\input{figures/code/bmnist_20_10runs_100particles}
