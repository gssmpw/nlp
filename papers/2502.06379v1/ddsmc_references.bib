@inproceedings{austin_structured_2021,
  title = {Structured {{Denoising Diffusion Models}} in {{Discrete State-Spaces}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Austin, Jacob and Johnson, Daniel D. and Ho, Jonathan and Tarlow, Daniel and van den Berg, Rianne},
  year = {2021},
  month = nov,
  urldate = {2022-11-07},
  abstract = {Denoising diffusion probabilistic models (DDPMs) [Ho et al. 2021] have shown impressive results on image and waveform generation in continuous state spaces. Here, we introduce Discrete Denoising Diffusion Probabilistic Models (D3PMs), diffusion-like generative models for discrete data that generalize the multinomial diffusion model of Hoogeboom et al. [2021], by going beyond corruption processes with uniform transition probabilities. This includes corruption with transition matrices that mimic Gaussian kernels in continuous space, matrices based on nearest neighbors in embedding space, and matrices that introduce absorbing states. The third allows us to draw a connection between diffusion models and autoregressive and mask-based generative models. We show that the choice of transition matrix is an important design decision that leads to improved results in image and text domains. We also introduce a new loss function that combines the variational lower bound with an auxiliary cross entropy loss. For text, this model class achieves strong results on character-level text generation while scaling to large vocabularies on LM1B. On the image dataset CIFAR-10, our models approach the sample quality and exceed the log-likelihood of the continuous-space DDPM model.},
  langid = {english},
  file = {C\:\\Users\\filek51\\Zotero\\storage\\GP46NRI2\\Austin et al. - 2021 - Structured Denoising Diffusion Models in Discrete .pdf;C\:\\Users\\filek51\\Zotero\\storage\\5SFL5M3A\\forum.html}
}

@misc{boys_tweedie_2024,
  title = {Tweedie {{Moment Projected Diffusions For Inverse Problems}}},
  author = {Boys, Benjamin and Girolami, Mark and Pidstrigach, Jakiw and Reich, Sebastian and Mosca, Alan and Akyildiz, O. Deniz},
  year = {2024},
  month = sep,
  number = {arXiv:2310.06721},
  eprint = {2310.06721},
  primaryclass = {stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2310.06721},
  urldate = {2024-12-18},
  abstract = {Diffusion generative models unlock new possibilities for inverse problems as they allow for the incorporation of strong empirical priors in scientific inference. Recently, diffusion models are repurposed for solving inverse problems using Gaussian approximations to conditional densities of the reverse process via Tweedie's formula to parameterise the mean, complemented with various heuristics. To address various challenges arising from these approximations, we leverage higher order information using Tweedie's formula and obtain a statistically principled approximation. We further provide a theoretical guarantee specifically for posterior sampling which can lead to a better theoretical understanding of diffusion-based conditional sampling. Finally, we illustrate the empirical effectiveness of our approach for general linear inverse problems on toy synthetic examples as well as image restoration. We show that our method (i) removes any time-dependent step-size hyperparameters required by earlier methods, (ii) brings stability and better sample quality across multiple noise levels, (iii) is the only method that works in a stable way with variance exploding (VE) forward processes as opposed to earlier works.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Computation},
  file = {C\:\\Users\\filek51\\Zotero\\storage\\QD4A5QZW\\Boys m. fl. - 2024 - Tweedie Moment Projected Diffusions For Inverse Problems.pdf;C\:\\Users\\filek51\\Zotero\\storage\\5YBB2FTQ\\2310.html}
}

@inproceedings{campbell_continuous_2022-1,
  title = {A {{Continuous Time Framework}} for {{Discrete Denoising Models}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Campbell, Andrew and Benton, Joe and Bortoli, Valentin De and Rainforth, Tom and Deligiannidis, George and Doucet, Arnaud},
  year = {2022},
  month = may,
  urldate = {2024-01-09},
  abstract = {We provide the first complete continuous time framework for denoising diffusion models of discrete data. This is achieved by formulating the forward noising process and corresponding reverse time generative process as Continuous Time Markov Chains (CTMCs). The model can be efficiently trained using a continuous time version of the ELBO. We simulate the high dimensional CTMC using techniques developed in chemical physics and exploit our continuous time framework to derive high performance samplers that we show can outperform discrete time methods for discrete data. The continuous time treatment also enables us to derive a novel theoretical result bounding the error between the generated sample distribution and the true data distribution.},
  langid = {english},
  file = {C:\Users\filek51\Zotero\storage\FA8JMA6N\Campbell et al. - 2022 - A Continuous Time Framework for Discrete Denoising.pdf}
}

@inproceedings{cardoso_monte_2023-2,
  title = {Monte {{Carlo}} Guided {{Denoising Diffusion}} Models for {{Bayesian}} Linear Inverse Problems.},
  booktitle = {The {{Twelfth International Conference}} on {{Learning Representations}}},
  author = {Cardoso, Gabriel and el Idrissi, Yazid Janati and Corff, Sylvain Le and Moulines, Eric},
  year = {2024},
  urldate = {2024-11-19},
  abstract = {Ill-posed linear inverse problems arise frequently in various applications, from computational photography to medical imaging. A recent line of research exploits Bayesian inference with informative priors to handle the ill-posedness of such problems. Amongst such priors, score-based generative models (SGM) have recently been successfully applied to several different inverse problems. In this study, we exploit the particular structure of the prior defined by the SGM to define a sequence of intermediate linear inverse problems. As the noise level decreases, the posteriors of these inverse problems get closer to the target posterior of the original inverse problem. To sample from this sequence of posteriors, we propose the use of Sequential Monte Carlo (SMC) methods. The proposed algorithm, {\textbackslash}algo, is shown to be theoretically grounded and we provide numerical simulations showing that it outperforms competing baselines when dealing with ill-posed inverse problems in a Bayesian setting.},
  langid = {english},
  file = {C:\Users\filek51\Zotero\storage\56D8NLYN\Cardoso m. fl. - 2023 - Monte Carlo guided Denoising Diffusion models for Bayesian linear inverse problems..pdf}
}

@misc{chen_wavegrad_2020,
  title = {{{WaveGrad}}: {{Estimating Gradients}} for {{Waveform Generation}}},
  shorttitle = {{{WaveGrad}}},
  author = {Chen, Nanxin and Zhang, Yu and Zen, Heiga and Weiss, Ron J. and Norouzi, Mohammad and Chan, William},
  year = {2020},
  month = oct,
  number = {arXiv:2009.00713},
  eprint = {2009.00713},
  primaryclass = {eess},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2009.00713},
  urldate = {2025-01-27},
  abstract = {This paper introduces WaveGrad, a conditional model for waveform generation which estimates gradients of the data density. The model is built on prior work on score matching and diffusion probabilistic models. It starts from a Gaussian white noise signal and iteratively refines the signal via a gradient-based sampler conditioned on the mel-spectrogram. WaveGrad offers a natural way to trade inference speed for sample quality by adjusting the number of refinement steps, and bridges the gap between non-autoregressive and autoregressive models in terms of audio quality. We find that it can generate high fidelity audio samples using as few as six iterations. Experiments reveal WaveGrad to generate high fidelity audio, outperforming adversarial non-autoregressive baselines and matching a strong likelihood-based autoregressive baseline using fewer sequential operations. Audio samples are available at https://wavegrad.github.io/.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing,Statistics - Machine Learning},
  file = {C\:\\Users\\filek51\\Zotero\\storage\\ZK78N786\\Chen m. fl. - 2020 - WaveGrad Estimating Gradients for Waveform Generation.pdf;C\:\\Users\\filek51\\Zotero\\storage\\ZU5Y6NFZ\\2009.html}
}

@inproceedings{chung_diffusion_2023,
  title = {Diffusion {{Posterior Sampling}} for {{General Noisy Inverse Problems}}},
  booktitle = {The {{Eleventh International Conference}} on {{Learning Representations}}},
  author = {Chung, Hyungjin and Kim, Jeongsol and Mccann, Michael Thompson and Klasky, Marc Louis and Ye, Jong Chul},
  year = {2023},
  urldate = {2025-01-07},
  abstract = {Diffusion models have been recently studied as powerful generative inverse problem solvers, owing to their high quality reconstructions and the ease of combining existing iterative solvers. However, most works focus on solving simple linear inverse problems in noiseless settings, which significantly under-represents the complexity of real-world problems. In this work, we extend diffusion solvers to efficiently handle general noisy (non)linear inverse problems via the Laplace approximation of the posterior sampling. Interestingly, the resulting posterior sampling scheme is a blended version of diffusion sampling with the manifold constrained gradient without a strict measurement consistency projection step, yielding a more desirable generative path in noisy settings compared to the previous studies. Our method demonstrates that diffusion models can incorporate various measurement noise statistics such as Gaussian and Poisson, and also efficiently handle noisy nonlinear inverse problems such as Fourier phase retrieval and non-uniform deblurring.},
  langid = {english},
  file = {C:\Users\filek51\Zotero\storage\UBPLE5BB\Chung m. fl. - 2022 - Diffusion Posterior Sampling for General Noisy Inverse Problems.pdf}
}

@inproceedings{corso_diffdock_2023,
  title = {{{DiffDock}}: {{Diffusion Steps}}, {{Twists}}, and {{Turns}} for {{Molecular Docking}}},
  shorttitle = {{{DiffDock}}},
  booktitle = {The {{Eleventh International Conference}} on {{Learning Representations}}},
  author = {Corso, Gabriele and St{\"a}rk, Hannes and Jing, Bowen and Barzilay, Regina and Jaakkola, Tommi S.},
  year = {2023},
  urldate = {2025-01-22},
  abstract = {Predicting the binding structure of a small molecule ligand to a protein---a task known as molecular docking---is critical to drug design. Recent deep learning methods that treat docking as a regression problem have decreased runtime compared to traditional search-based methods but have yet to offer substantial improvements in accuracy. We instead frame molecular docking as a generative modeling problem and develop DiffDock, a diffusion generative model over the non-Euclidean manifold of ligand poses. To do so, we map this manifold to the product space of the degrees of freedom (translational, rotational, and torsional) involved in docking and develop an efficient diffusion process on this space. Empirically, DiffDock obtains a 38\% top-1 success rate (RMSD{$<$}2A) on PDBBind, significantly outperforming the previous state-of-the-art of traditional docking (23\%) and deep learning (20\%) methods. Moreover, while previous methods are not able to dock on computationally folded structures (maximum accuracy 10.4\%), DiffDock maintains significantly higher precision (21.7\%). Finally, DiffDock has fast inference times and provides confidence estimates with high selective accuracy.},
  langid = {english},
  file = {C:\Users\filek51\Zotero\storage\7G24NBQ2\Corso m. fl. - 2022 - DiffDock Diffusion Steps, Twists, and Turns for Molecular Docking.pdf}
}

@article{dauparas_robust_2022,
  title = {Robust Deep Learning--Based Protein Sequence Design Using {{ProteinMPNN}}},
  author = {Dauparas, J. and Anishchenko, I. and Bennett, N. and Bai, H. and Ragotte, R. J. and Milles, L. F. and Wicky, B. I. M. and Courbet, A. and {de Haas}, R. J. and Bethel, N. and Leung, P. J. Y. and Huddy, T. F. and Pellock, S. and Tischer, D. and Chan, F. and Koepnick, B. and Nguyen, H. and Kang, A. and Sankaran, B. and Bera, A. K. and King, N. P. and Baker, D.},
  year = {2022},
  month = oct,
  journal = {Science},
  volume = {378},
  number = {6615},
  pages = {49--56},
  publisher = {American Association for the Advancement of Science},
  doi = {10.1126/science.add2187},
  urldate = {2024-03-01},
  abstract = {Although deep learning has revolutionized protein structure prediction, almost all experimentally characterized de novo protein designs have been generated using physically based approaches such as Rosetta. Here, we describe a deep learning--based protein sequence design method, ProteinMPNN, that has outstanding performance in both in silico and experimental tests. On native protein backbones, ProteinMPNN has a sequence recovery of 52.4\% compared with 32.9\% for Rosetta. The amino acid sequence at different positions can be coupled between single or multiple chains, enabling application to a wide range of current protein design challenges. We demonstrate the broad utility and high accuracy of ProteinMPNN using x-ray crystallography, cryo--electron microscopy, and functional studies by rescuing previously failed designs, which were made using Rosetta or AlphaFold, of protein monomers, cyclic homo-oligomers, tetrahedral nanoparticles, and target-binding proteins.},
  file = {C:\Users\filek51\Zotero\storage\SKKEZK4W\Dauparas et al. - 2022 - Robust deep learning–based protein sequence design.pdf}
}

@inproceedings{dhariwal_diffusion_2021-1,
  title = {Diffusion {{Models Beat GANs}} on {{Image Synthesis}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Dhariwal, Prafulla and Nichol, Alexander},
  year = {2021},
  volume = {34},
  pages = {8780--8794},
  publisher = {Curran Associates, Inc.},
  urldate = {2025-01-02},
  file = {C:\Users\filek51\Zotero\storage\BCKA5Z9H\Dhariwal and Nichol - 2021 - Diffusion Models Beat GANs on Image Synthesis.pdf}
}

@misc{dieleman_continuous_2022,
  title = {Continuous Diffusion for Categorical Data},
  author = {Dieleman, Sander and Sartran, Laurent and Roshannai, Arman and Savinov, Nikolay and Ganin, Yaroslav and Richemond, Pierre H. and Doucet, Arnaud and Strudel, Robin and Dyer, Chris and Durkan, Conor and Hawthorne, Curtis and Leblond, R{\'e}mi and Grathwohl, Will and Adler, Jonas},
  year = {2022},
  month = dec,
  number = {arXiv:2211.15089},
  eprint = {2211.15089},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2211.15089},
  urldate = {2023-02-07},
  abstract = {Diffusion models have quickly become the go-to paradigm for generative modelling of perceptual signals (such as images and sound) through iterative refinement. Their success hinges on the fact that the underlying physical phenomena are continuous. For inherently discrete and categorical data such as language, various diffusion-inspired alternatives have been proposed. However, the continuous nature of diffusion models conveys many benefits, and in this work we endeavour to preserve it. We propose CDCD, a framework for modelling categorical data with diffusion models that are continuous both in time and input space. We demonstrate its efficacy on several language modelling tasks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {C\:\\Users\\filek51\\Zotero\\storage\\FKJ8UKLM\\Dieleman et al. - 2022 - Continuous diffusion for categorical data.pdf;C\:\\Users\\filek51\\Zotero\\storage\\AYQYQIA3\\2211.html}
}

@inproceedings{dou_diffusion_2023-1,
  title = {Diffusion {{Posterior Sampling}} for {{Linear Inverse Problem Solving}}: {{A Filtering Perspective}}},
  shorttitle = {Diffusion {{Posterior Sampling}} for {{Linear Inverse Problem Solving}}},
  booktitle = {The {{Twelfth International Conference}} on {{Learning Representations}}},
  author = {Dou, Zehao and Song, Yang},
  year = {2023},
  month = oct,
  urldate = {2025-01-21},
  abstract = {Diffusion models have achieved tremendous success in generating high-dimensional data like images, videos and audio. These models provide powerful data priors that can solve linear inverse problems in zero shot through Bayesian posterior sampling. However, exact posterior sampling for diffusion models is intractable. Current solutions often hinge on approximations that are either computationally expensive or lack strong theoretical guarantees. In this work, we introduce an efficient diffusion sampling algorithm for linear inverse problems that is guaranteed to be asymptotically accurate. We reveal a link between Bayesian posterior sampling and Bayesian filtering in diffusion models, proving the former as a specific instance of the latter. Our method, termed filtering posterior sampling, leverages sequential Monte Carlo methods to solve the corresponding filtering problem. It seamlessly integrates with all Markovian diffusion samplers, requires no model re-training, and guarantees accurate samples from the Bayesian posterior as particle counts rise. Empirical tests demonstrate that our method generates better or comparable results than leading zero-shot diffusion posterior samplers on tasks like image inpainting, super-resolution, and deblurring.},
  langid = {english},
  file = {C:\Users\filek51\Zotero\storage\6C5A8ERL\Dou and Song - 2023 - Diffusion Posterior Sampling for Linear Inverse Problem Solving A Filtering Perspective.pdf}
}

@inproceedings{ekstrom_kelvinius_discriminator_2024,
  title = {Discriminator {{Guidance}} for {{Autoregressive Diffusion Models}}},
  booktitle = {Proceedings of {{The}} 27th {{International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Ekstr{\"o}m Kelvinius, Filip and Lindsten, Fredrik},
  year = {2024},
  month = apr,
  pages = {3403--3411},
  publisher = {PMLR},
  issn = {2640-3498},
  urldate = {2024-04-22},
  abstract = {We introduce discriminator guidance in the setting of Autoregressive Diffusion Models. The use of a discriminator to guide a diffusion process has previously been used for continuous diffusion models, and in this work we derive ways of using a discriminator together with a pretrained generative model in the discrete case. First, we show that using an optimal discriminator will correct the pretrained model and enable exact sampling from the underlying data distribution. Second, to account for the realistic scenario of using a sub-optimal discriminator, we derive a sequential Monte Carlo algorithm which iteratively takes the predictions from the discriminator into account during the generation process. We test these approaches on the task of generating molecular graphs and show how the discriminator improves the generative performance over using only the pretrained model.},
  langid = {english},
  file = {C:\Users\filek51\Zotero\storage\G5H6RF4U\Kelvinius and Lindsten - 2024 - Discriminator Guidance for Autoregressive Diffusio.pdf}
}

@article{flamary_pot_2021,
  title = {{{POT}}: {{Python Optimal Transport}}},
  shorttitle = {{{POT}}},
  author = {Flamary, R{\'e}mi and Courty, Nicolas and Gramfort, Alexandre and Alaya, Mokhtar Z. and Boisbunon, Aur{\'e}lie and Chambon, Stanislas and Chapel, Laetitia and Corenflos, Adrien and Fatras, Kilian and Fournier, Nemo and Gautheron, L{\'e}o and Gayraud, Nathalie T. H. and Janati, Hicham and Rakotomamonjy, Alain and Redko, Ievgen and Rolet, Antoine and Schutz, Antony and Seguy, Vivien and Sutherland, Danica J. and Tavenard, Romain and Tong, Alexander and Vayer, Titouan},
  year = {2021},
  journal = {Journal of Machine Learning Research},
  volume = {22},
  number = {78},
  pages = {1--8},
  issn = {1533-7928},
  urldate = {2024-12-10},
  abstract = {Optimal  transport  has  recently  been  reintroduced  to  the  machine  learning  community thanks in part to novel efficient optimization procedures allowing for medium to large scale applications.  We propose a Python toolbox that implements several key optimal transport ideas  for  the  machine  learning  community.   The  toolbox  contains  implementations  of  a number  of  founding  works  of  OT  for  machine  learning  such  as  Sinkhorn  algorithm  and Wasserstein barycenters, but also provides generic solvers that can be used for conducting novel fundamental research.  This toolbox, named POT for Python Optimal Transport, is open source with an MIT license.},
  file = {C:\Users\filek51\Zotero\storage\FXAIZIZI\Flamary m. fl. - 2021 - POT Python Optimal Transport.pdf}
}

@misc{ho_classifier-free_2022,
  title = {Classifier-{{Free Diffusion Guidance}}},
  author = {Ho, Jonathan and Salimans, Tim},
  year = {2022},
  month = jul,
  number = {arXiv:2207.12598},
  eprint = {2207.12598},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2207.12598},
  urldate = {2024-04-22},
  abstract = {Classifier guidance is a recently introduced method to trade off mode coverage and sample fidelity in conditional diffusion models post training, in the same spirit as low temperature sampling or truncation in other types of generative models. Classifier guidance combines the score estimate of a diffusion model with the gradient of an image classifier and thereby requires training an image classifier separate from the diffusion model. It also raises the question of whether guidance can be performed without a classifier. We show that guidance can be indeed performed by a pure generative model without such a classifier: in what we call classifier-free guidance, we jointly train a conditional and an unconditional diffusion model, and we combine the resulting conditional and unconditional score estimates to attain a trade-off between sample quality and diversity similar to that obtained using classifier guidance.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {C\:\\Users\\filek51\\Zotero\\storage\\955S5GX5\\Ho and Salimans - 2022 - Classifier-Free Diffusion Guidance.pdf;C\:\\Users\\filek51\\Zotero\\storage\\UV7VEEML\\2207.html}
}

@inproceedings{ho_denoising_2020-2,
  title = {Denoising {{Diffusion Probabilistic Models}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
  year = {2020},
  volume = {33},
  pages = {6840--6851},
  publisher = {Curran Associates, Inc.},
  urldate = {2024-11-21},
  abstract = {We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN.},
  file = {C:\Users\filek51\Zotero\storage\RNGSW9SV\Ho m. fl. - 2020 - Denoising Diffusion Probabilistic Models.pdf}
}

@inproceedings{hoogeboom_equivariant_2022,
  title = {Equivariant {{Diffusion}} for {{Molecule Generation}} in {{3D}}},
  booktitle = {Proceedings of the 39th {{International Conference}} on {{Machine Learning}}},
  author = {Hoogeboom, Emiel and Satorras, V{\'{\i}}ctor Garcia and Vignac, Cl{\'e}ment and Welling, Max},
  year = {2022},
  month = jun,
  pages = {8867--8887},
  publisher = {PMLR},
  issn = {2640-3498},
  urldate = {2022-10-14},
  abstract = {This work introduces a diffusion model for molecule generation in 3D that is equivariant to Euclidean transformations. Our E(3) Equivariant Diffusion Model (EDM) learns to denoise a diffusion process with an equivariant network that jointly operates on both continuous (atom coordinates) and categorical features (atom types). In addition, we provide a probabilistic analysis which admits likelihood computation of molecules using our model. Experimentally, the proposed method significantly outperforms previous 3D molecular generative methods regarding the quality of generated samples and the efficiency at training time.},
  langid = {english},
  file = {C:\Users\filek51\Zotero\storage\7SP2GK4B\Hoogeboom et al. - 2022 - Equivariant Diffusion for Molecule Generation in 3.pdf}
}

@inproceedings{janati_divide-and-conquer_2024,
  title = {Divide-and-{{Conquer Posterior Sampling}} for {{Denoising Diffusion}} Priors},
  booktitle = {The {{Thirty-eighth Annual Conference}} on {{Neural Information Processing Systems}}},
  author = {Janati, Yazid and Moufad, Badr and Durmus, Alain Oliviero and Moulines, Eric and Olsson, Jimmy},
  year = {2024},
  month = nov,
  urldate = {2024-11-21},
  abstract = {Recent advancements in solving Bayesian inverse problems have spotlighted denoising diffusion models (DDMs) as effective priors. Although these have great potential, DDM priors yield complex posterior distributions that are challenging to sample from. Existing approaches to posterior sampling in this context address this problem either by retraining model-specific components, leading to stiff and cumbersome methods, or by introducing approximations with uncontrolled errors that affect the accuracy of the produced samples. We present an innovative framework, divide-and-conquer posterior sampling, which leverages the inherent structure of DDMs to construct a sequence of intermediate posteriors that guide the produced samples to the target posterior. Our method significantly reduces the approximation error associated with current techniques without the need for retraining. We demonstrate the versatility and effectiveness of our approach for a wide range of Bayesian inverse problems. The code is available at {\textbackslash}url\{https://github.com/Badr-MOUFAD/dcps\}},
  langid = {english},
  file = {C:\Users\filek51\Zotero\storage\3XZHT5EU\Janati m. fl. - 2024 - Divide-and-Conquer Posterior Sampling for Denoising Diffusion priors.pdf}
}

@inproceedings{kadkhodaie_stochastic_2021,
  title = {Stochastic {{Solutions}} for {{Linear Inverse Problems}} Using the {{Prior Implicit}} in a {{Denoiser}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Kadkhodaie, Zahra and Simoncelli, Eero},
  year = {2021},
  volume = {34},
  pages = {13242--13254},
  publisher = {Curran Associates, Inc.},
  urldate = {2025-01-07},
  abstract = {Deep neural networks have provided state-of-the-art solutions for problems such as image denoising, which implicitly rely on a prior probability model of natural images. Two recent lines of work -- Denoising Score Matching and Plug-and-Play -- propose methodologies for drawing samples from this implicit prior and using it to solve inverse problems, respectively. Here, we develop a parsimonious and robust generalization of these ideas. We rely on a classic statistical result that shows the least-squares solution for removing additive Gaussian noise can be written directly in terms of the gradient of the log of the noisy signal density. We use this to derive a stochastic coarse-to-fine gradient ascent procedure for drawing high-probability samples from the implicit prior embedded within a CNN trained to perform blind denoising. A generalization of this algorithm to constrained sampling provides a method for using the implicit prior to solve any deterministic linear inverse problem, with no additional training, thus extending the power of supervised learning for denoising to a much broader set of problems. The algorithm relies on minimal assumptions and exhibits robust convergence over a wide range of parameter choices. To demonstrate the generality of our method, we use it to obtain state-of-the-art levels of unsupervised performance for deblurring, super-resolution, and compressive sensing.},
  file = {C:\Users\filek51\Zotero\storage\CTBGHLTT\Kadkhodaie and Simoncelli - 2021 - Stochastic Solutions for Linear Inverse Problems using the Prior Implicit in a Denoiser.pdf}
}

@article{karras_elucidating_2022-1,
  title = {Elucidating the {{Design Space}} of {{Diffusion-Based Generative Models}}},
  author = {Karras, Tero and Aittala, Miika and Aila, Timo and Laine, Samuli},
  year = {2022},
  month = dec,
  journal = {Advances in Neural Information Processing Systems},
  volume = {35},
  pages = {26565--26577},
  urldate = {2025-01-28},
  langid = {english},
  file = {C:\Users\filek51\Zotero\storage\HI5BNSJV\Karras m. fl. - 2022 - Elucidating the Design Space of Diffusion-Based Generative Models.pdf}
}

@inproceedings{karras_style-based_2019,
  title = {A {{Style-Based Generator Architecture}} for {{Generative Adversarial Networks}}},
  booktitle = {2019 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Karras, Tero and Laine, Samuli and Aila, Timo},
  year = {2019},
  month = jun,
  pages = {4396--4405},
  issn = {2575-7075},
  doi = {10.1109/CVPR.2019.00453},
  urldate = {2025-01-28},
  abstract = {We propose an alternative generator architecture for generative adversarial networks, borrowing from style transfer literature. The new architecture leads to an automatically learned, unsupervised separation of high-level attributes (e.g., pose and identity when trained on human faces) and stochastic variation in the generated images (e.g., freckles, hair), and it enables intuitive, scale-specific control of the synthesis. The new generator improves the state-of-the-art in terms of traditional distribution quality metrics, leads to demonstrably better interpolation properties, and also better disentangles the latent factors of variation. To quantify interpolation quality and disentanglement, we propose two new, automated methods that are applicable to any generator architecture. Finally, we introduce a new, highly varied and high-quality dataset of human faces.},
  keywords = {Deep Learning,Image and Video Synthesis,Representation Learning},
  file = {C\:\\Users\\filek51\\Zotero\\storage\\3FDPT8KM\\Karras m. fl. - 2019 - A Style-Based Generator Architecture for Generative Adversarial Networks.pdf;C\:\\Users\\filek51\\Zotero\\storage\\XVLVUJLA\\8953766.html}
}

@article{kawar_denoising_2022,
  title = {Denoising {{Diffusion Restoration Models}}},
  author = {Kawar, Bahjat and Elad, Michael and Ermon, Stefano and Song, Jiaming},
  year = {2022},
  month = dec,
  journal = {Advances in Neural Information Processing Systems},
  volume = {35},
  pages = {23593--23606},
  urldate = {2025-01-02},
  langid = {english},
  file = {C:\Users\filek51\Zotero\storage\HK8ZRXBI\Kawar m. fl. - 2022 - Denoising Diffusion Restoration Models.pdf}
}

@inproceedings{kawar_snips_2021,
  title = {{{SNIPS}}: {{Solving Noisy Inverse Problems Stochastically}}},
  shorttitle = {{{SNIPS}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Kawar, Bahjat and Vaksman, Gregory and Elad, Michael},
  year = {2021},
  volume = {34},
  pages = {21757--21769},
  publisher = {Curran Associates, Inc.},
  urldate = {2025-01-10},
  abstract = {In this work we introduce a novel stochastic algorithm dubbed SNIPS, which draws samples from the posterior distribution of any linear inverse problem, where the observation is assumed to be contaminated by additive white Gaussian noise. Our solution incorporates ideas from Langevin dynamics and Newton's method, and exploits a pre-trained minimum mean squared error (MMSE) Gaussian denoiser. The proposed approach relies on an intricate derivation of the posterior score function that includes a singular value decomposition (SVD) of the degradation operator, in order to obtain a tractable iterative algorithm for the desired sampling. Due to its stochasticity, the algorithm can produce multiple high perceptual quality samples for the same noisy observation. We demonstrate the abilities of the proposed paradigm for image deblurring, super-resolution, and compressive sensing. We show that the samples produced are sharp, detailed and consistent with the given measurements, and their diversity exposes the inherent uncertainty in the inverse problem being solved.}
}

@inproceedings{kim_refining_2023-1,
  title = {Refining {{Generative Process}} with {{Discriminator Guidance}} in {{Score-based Diffusion Models}}},
  booktitle = {Proceedings of the 40th {{International Conference}} on {{Machine Learning}}},
  author = {Kim, Dongjun and Kim, Yeongmin and Kwon, Se Jung and Kang, Wanmo and Moon, Il-Chul},
  year = {2023},
  month = jul,
  pages = {16567--16598},
  publisher = {PMLR},
  issn = {2640-3498},
  urldate = {2023-08-02},
  abstract = {The proposed method, Discriminator Guidance, aims to improve sample generation of pre-trained diffusion models. The approach introduces a discriminator that gives explicit supervision to a denoising sample path whether it is realistic or not. Unlike GANs, our approach does not require joint training of score and discriminator networks. Instead, we train the discriminator after score training, making discriminator training stable and fast to converge. In sample generation, we add an auxiliary term to the pre-trained score to deceive the discriminator. This term corrects the model score to the data score at the optimal discriminator, which implies that the discriminator helps better score estimation in a complementary way. Using our algorithm, we achive state-of-the-art results on ImageNet 256x256 with FID 1.83 and recall 0.64, similar to the validation data's FID (1.68) and recall (0.66). We release the code at https://github.com/alsdudrla10/DG.},
  langid = {english},
  file = {C:\Users\filek51\Zotero\storage\U8Q3DWTU\Kim et al. - 2023 - Refining Generative Process with Discriminator Gui.pdf}
}

@inproceedings{kong_diffwave_2021,
  title = {{{DiffWave}}: {{A Versatile Diffusion Model}} for {{Audio Synthesis}}},
  shorttitle = {{{DiffWave}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Kong, Zhifeng and Ping, Wei and Huang, Jiaji and Zhao, Kexin and Catanzaro, Bryan},
  year = {2021},
  urldate = {2025-01-27},
  abstract = {In this work, we propose DiffWave, a versatile diffusion probabilistic model for conditional and unconditional waveform generation. The model is non-autoregressive, and converts the white noise signal into structured waveform through a Markov chain with a constant number of steps at synthesis. It is efficiently trained by optimizing a variant of variational bound on the data likelihood. DiffWave produces high-fidelity audios in different waveform generation tasks, including neural vocoding conditioned on mel spectrogram, class-conditional generation, and unconditional generation. We demonstrate that DiffWave matches a strong WaveNet vocoder in terms of speech quality (MOS: 4.44 versus 4.43), while synthesizing orders of magnitude faster. In particular, it significantly outperforms autoregressive and GAN-based waveform models in the challenging unconditional generation task in terms of audio quality and sample diversity from various automatic and human evaluations.},
  langid = {english}
}

@inproceedings{liu_correcting_2024,
  title = {Correcting {{Diffusion Generation}} through {{Resampling}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Liu, Yujian and Zhang, Yang and Jaakkola, Tommi and Chang, Shiyu},
  year = {2024},
  pages = {8713--8723},
  urldate = {2024-11-29},
  langid = {english},
  file = {C:\Users\filek51\Zotero\storage\5XY254K2\Liu m. fl. - 2024 - Correcting Diffusion Generation through Resampling.pdf}
}

@inproceedings{lugmayr_repaint_2022,
  title = {{{RePaint}}: {{Inpainting Using Denoising Diffusion Probabilistic Models}}},
  shorttitle = {{{RePaint}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Lugmayr, Andreas and Danelljan, Martin and Romero, Andres and Yu, Fisher and Timofte, Radu and Van Gool, Luc},
  year = {2022},
  pages = {11461--11471},
  urldate = {2025-01-07},
  langid = {english},
  file = {C:\Users\filek51\Zotero\storage\6SBVKYPU\Lugmayr m. fl. - 2022 - RePaint Inpainting Using Denoising Diffusion Probabilistic Models.pdf}
}

@inproceedings{mardani_variational_2023,
  title = {A {{Variational Perspective}} on {{Solving Inverse Problems}} with {{Diffusion Models}}},
  booktitle = {The {{Twelfth International Conference}} on {{Learning Representations}}},
  author = {Mardani, Morteza and Song, Jiaming and Kautz, Jan and Vahdat, Arash},
  year = {2023},
  month = oct,
  urldate = {2025-01-07},
  abstract = {Diffusion models have emerged as a key pillar of foundation models in visual domains. One of their critical applications is to universally solve different downstream inverse tasks via a single diffusion prior without re-training for each task. Most inverse tasks can be formulated as inferring a posterior distribution over data (e.g., a full image) given a measurement (e.g., a masked image). This is however challenging in diffusion models since the nonlinear and iterative nature of the diffusion process renders the posterior intractable. To cope with this challenge, we propose a variational approach that by design seeks to approximate the true posterior distribution. We show that our approach naturally leads to regularization by denoising diffusion process (RED-diff) where denoisers at different timesteps concurrently impose different structural constraints over the image. To gauge the contribution of denoisers from different timesteps, we propose a weighting mechanism based on signal-to-noise-ratio (SNR). Our approach provides a new variational perspective for solving inverse problems with diffusion models, allowing us to formulate sampling as stochastic optimization, where one can simply apply off-the-shelf solvers with lightweight iterates. Our experiments for various linear and nonlinear image restoration tasks demonstrate the strengths of our method compared with state-of-the-art sampling-based diffusion models. The code is available online {\textbackslash}footnote\{{\textbackslash}url\{https://github.com/NVlabs/RED-diff\}\}.},
  langid = {english},
  file = {C:\Users\filek51\Zotero\storage\8GRE3IMI\Mardani m. fl. - 2023 - A Variational Perspective on Solving Inverse Problems with Diffusion Models.pdf}
}

@article{NaessethLS:2019a,
  title = {Elements of Sequential {{Monte Carlo}}},
  author = {Naesseth, Christian A. and Lindsten, Fredrik and Sch{\"o}n, Thomas B.},
  year = {2019},
  journal = {Foundations and Trends in Machine Learning},
  volume = {12},
  number = {3},
  pages = {307--392},
  doi = {10.1561/2200000074}
}

@article{park_has_2024,
  title = {Has Generative Artificial Intelligence Solved Inverse Materials Design?},
  author = {Park, Hyunsoo and Li, Zhenzhu and Walsh, Aron},
  year = {2024},
  month = jul,
  journal = {Matter},
  volume = {7},
  number = {7},
  pages = {2355--2367},
  issn = {2590-2385},
  doi = {10.1016/j.matt.2024.05.017},
  urldate = {2025-01-16},
  abstract = {The directed design and discovery of compounds with pre-determined properties is a long-standing challenge in materials research. We provide a perspective on progress toward achieving this goal using generative models for chemical compositions and crystal structures based on a set of powerful statistical techniques drawn from the artificial intelligence community. We introduce the central concepts underpinning generative models of crystalline materials. Coverage is provided of early implementations for inorganic crystals based on generative adversarial networks and variational autoencoders through to ongoing progress involving autoregressive and diffusion models. The influence of the choice of chemical representation and the generative architecture is discussed, along with metrics for quantifying the quality of the hypothetical compounds produced. While further developments are required to enable realistic predictions drawn from richer structure and property datasets, generative artificial intelligence is already proving to be complementary to traditional materials design strategies.}
}

@inproceedings{rombach_high-resolution_2022,
  title = {High-{{Resolution Image Synthesis With Latent Diffusion Models}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj{\"o}rn},
  year = {2022},
  pages = {10684--10695},
  urldate = {2023-09-29},
  langid = {english},
  file = {C:\Users\filek51\Zotero\storage\C5T5AAJ2\Rombach et al. - 2022 - High-Resolution Image Synthesis With Latent Diffus.pdf}
}

@inproceedings{ronneberger_u-net_2015,
  title = {U-{{Net}}: {{Convolutional Networks}} for {{Biomedical Image Segmentation}}},
  shorttitle = {U-{{Net}}},
  booktitle = {Medical {{Image Computing}} and {{Computer-Assisted Intervention}} -- {{MICCAI}} 2015},
  author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
  editor = {Navab, Nassir and Hornegger, Joachim and Wells, William M. and Frangi, Alejandro F.},
  year = {2015},
  pages = {234--241},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-319-24574-4_28},
  abstract = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net.},
  isbn = {978-3-319-24574-4},
  langid = {english},
  keywords = {Convolutional Layer,Data Augmentation,Deep Network,Ground Truth Segmentation,Training Image},
  file = {C:\Users\filek51\Zotero\storage\UAF78BV7\Ronneberger m. fl. - 2015 - U-Net Convolutional Networks for Biomedical Image Segmentation.pdf}
}

@article{saharia_photorealistic_2022,
  title = {Photorealistic {{Text-to-Image Diffusion Models}} with {{Deep Language Understanding}}},
  author = {Saharia, Chitwan and Chan, William and Saxena, Saurabh and Li, Lala and Whang, Jay and Denton, Emily L. and Ghasemipour, Kamyar and Gontijo Lopes, Raphael and Karagol Ayan, Burcu and Salimans, Tim and Ho, Jonathan and Fleet, David J. and Norouzi, Mohammad},
  year = {2022},
  month = dec,
  journal = {Advances in Neural Information Processing Systems},
  volume = {35},
  pages = {36479--36494},
  urldate = {2025-01-27},
  langid = {english}
}

@article{schon2011manipulating,
  title = {Manipulating the Multivariate {{Gaussian}} Density},
  author = {Sch{\"o}n, Thomas B and Lindsten, Fredrik},
  year = {2011},
  journal = {Div. Automat. Control, Link{\"o}ping Univ., Linkping, Sweden, Tech. Rep},
  volume = {4},
  number = {3.4},
  pages = {4},
  file = {C:\Users\filek51\Zotero\storage\PE4QR3MQ\Schön and Lindsten - 2011 - Manipulating the multivariate Gaussian density.pdf}
}

@inproceedings{sohl-dickstein_deep_2015,
  title = {Deep {{Unsupervised Learning}} Using {{Nonequilibrium Thermodynamics}}},
  booktitle = {Proceedings of the 32nd {{International Conference}} on {{Machine Learning}}},
  author = {{Sohl-Dickstein}, Jascha and Weiss, Eric and Maheswaranathan, Niru and Ganguli, Surya},
  year = {2015},
  month = jun,
  pages = {2256--2265},
  publisher = {PMLR},
  issn = {1938-7228},
  urldate = {2024-01-25},
  abstract = {A central problem in machine learning involves modeling complex data-sets using highly flexible families of probability distributions in which learning, sampling, inference, and evaluation are still analytically or computationally tractable. Here, we develop an approach that simultaneously achieves both flexibility and tractability. The essential idea, inspired by non-equilibrium statistical physics, is to systematically and slowly destroy structure in a data distribution through an iterative forward diffusion process. We then learn a reverse diffusion process that restores structure in data, yielding a highly flexible and tractable generative model of the data. This approach allows us to rapidly learn, sample from, and evaluate probabilities in deep generative models with thousands of layers or time steps, as well as to compute conditional and posterior probabilities under the learned model. We additionally release an open source reference implementation of the algorithm.},
  langid = {english},
  file = {C:\Users\filek51\Zotero\storage\DD6LLLRE\Sohl-Dickstein et al. - 2015 - Deep Unsupervised Learning using Nonequilibrium Th.pdf}
}

@inproceedings{song_denoising_2021,
  title = {Denoising {{Diffusion Implicit Models}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Song, Jiaming and Meng, Chenlin and Ermon, Stefano},
  year = {2021},
  urldate = {2022-02-04},
  abstract = {Denoising diffusion probabilistic models (DDPMs) have achieved high quality image generation without adversarial training, yet they require simulating a Markov chain for many steps in order to...},
  langid = {english},
  file = {C:\Users\filek51\Zotero\storage\5HBS2A4I\Song et al. - 2020 - Denoising Diffusion Implicit Models.pdf}
}

@inproceedings{song_score-based_2021,
  title = {Score-{{Based Generative Modeling}} through {{Stochastic Differential Equations}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Song, Yang and {Sohl-Dickstein}, Jascha and Kingma, Diederik P. and Kumar, Abhishek and Ermon, Stefano and Poole, Ben},
  year = {2021},
  urldate = {2024-01-25},
  abstract = {Creating noise from data is easy; creating data from noise is generative modeling. We present a stochastic differential equation (SDE) that smoothly transforms a complex data distribution to a known prior distribution by slowly injecting noise, and a corresponding reverse-time SDE that transforms the prior distribution back into the data distribution by slowly removing the noise. Crucially, the reverse-time SDE depends only on the time-dependent gradient field (a.k.a., score) of the perturbed data distribution. By leveraging advances in score-based generative modeling, we can accurately estimate these scores with neural networks, and use numerical SDE solvers to generate samples. We show that this framework encapsulates previous approaches in score-based generative modeling and diffusion probabilistic modeling, allowing for new sampling procedures and new modeling capabilities. In particular, we introduce a predictor-corrector framework to correct errors in the evolution of the discretized reverse-time SDE. We also derive an equivalent neural ODE that samples from the same distribution as the SDE, but additionally enables exact likelihood computation, and improved sampling efficiency. In addition, we provide a new way to solve inverse problems with score-based models, as demonstrated with experiments on class-conditional generation, image inpainting, and colorization. Combined with multiple architectural improvements, we achieve record-breaking performance for unconditional image generation on CIFAR-10 with an Inception score of 9.89 and FID of 2.20, a competitive likelihood of 2.99 bits/dim, and demonstrate high fidelity generation of \$1024{\textbackslash}times 1024\$ images for the first time from a score-based generative model.},
  langid = {english},
  file = {C:\Users\filek51\Zotero\storage\BRVMEP65\Song et al. - 2020 - Score-Based Generative Modeling through Stochastic.pdf}
}

@inproceedings{song2023pseudoinverseguided,
  title = {Pseudoinverse-Guided Diffusion Models for Inverse Problems},
  booktitle = {International Conference on Learning Representations},
  author = {Song, Jiaming and Vahdat, Arash and Mardani, Morteza and Kautz, Jan},
  year = {2023},
  file = {C:\Users\filek51\Zotero\storage\FLRQ2EAI\Song m. fl. - 2023 - Pseudoinverse-guided diffusion models for inverse problems.pdf}
}

@inproceedings{sun_score-based_2023,
  title = {Score-Based {{Continuous-time Discrete Diffusion Models}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Sun, Haoran and Yu, Lijun and Dai, Bo and Schuurmans, Dale and Dai, Hanjun},
  year = {2023},
  month = feb,
  urldate = {2023-02-02},
  abstract = {Score-based modeling through stochastic differential equations (SDEs) has provided a new perspective on diffusion models, and demonstrated superior performance on continuous data. However, the gradient of the log-likelihood function, {\textbackslash}ie, the score function, is not properly defined for discrete spaces. This makes it non-trivial to adapt SDE with score functions to categorical data. In this paper, we extend diffusion models to discrete variables by introducing a stochastic jump process where the reverse process denoises via a continuous-time Markov chain. This formulation admits an analytical simulation during backward sampling. To learn the reverse process, we extend score matching to general categorical data, and show that an unbiased estimator can be obtained via simple matching of the conditional marginal distributions. We demonstrate the effectiveness of the proposed method on a set of synthetic and real-world music and image benchmarks.},
  langid = {english},
  file = {C:\Users\filek51\Zotero\storage\9JZN5SBU\Sun et al. - 2023 - Score-based Continuous-time Discrete Diffusion Mod.pdf}
}

@inproceedings{trippe_diffusion_2023,
  title = {Diffusion {{Probabilistic Modeling}} of {{Protein Backbones}} in {{3D}} for the Motif-Scaffolding Problem},
  booktitle = {The {{Eleventh International Conference}} on {{Learning Representations}}},
  author = {Trippe, Brian L. and Yim, Jason and Tischer, Doug and Baker, David and Broderick, Tamara and Barzilay, Regina and Jaakkola, Tommi S.},
  year = {2023},
  urldate = {2025-01-21},
  abstract = {Construction of a scaffold structure that supports a desired motif, conferring protein function, shows promise for the design of vaccines and enzymes. But a general solution to this motif-scaffolding problem remains open. Current machine-learning techniques for scaffold design are either limited to unrealistically small scaffolds (up to length 20) or struggle to produce multiple diverse scaffolds. We propose to learn a distribution over diverse and longer protein backbone structures via an E(3)-equivariant graph neural network. We develop SMCDiff to efficiently sample scaffolds from this distribution conditioned on a given motif; our algorithm is the first to theoretically guarantee conditional samples from a diffusion model in the large-compute limit. We evaluate our designed backbones by how well they align with AlphaFold2-predicted structures. We show that our method can (1) sample scaffolds up to 80 residues and (2) achieve structurally diverse scaffolds for a fixed motif.},
  langid = {english},
  file = {C:\Users\filek51\Zotero\storage\LHZ9A9GN\Trippe m. fl. - 2022 - Diffusion Probabilistic Modeling of Protein Backbones in 3D for the motif-scaffolding problem.pdf}
}

@inproceedings{uria_deep_2014,
  title = {A {{Deep}} and {{Tractable Density Estimator}}},
  booktitle = {Proceedings of the 31st {{International Conference}} on {{Machine Learning}}},
  author = {Uria, Benigno and Murray, Iain and Larochelle, Hugo},
  year = {2014},
  month = jan,
  pages = {467--475},
  publisher = {PMLR},
  issn = {1938-7228},
  urldate = {2023-05-04},
  abstract = {The Neural Autoregressive Distribution Estimator (NADE) and its real-valued version RNADE are competitive density models of multidimensional data across a variety of domains. These models use a fixed, arbitrary ordering of the data  dimensions. One can easily condition on variables at the beginning of the ordering, and marginalize out variables at the end of the ordering, however other inference tasks require approximate inference. In this work we introduce an efficient procedure to simultaneously train a NADE model for each possible ordering of the variables, by sharing parameters across all these models. We can thus use the most convenient model for each inference task at hand, and ensembles of such models with different orderings are immediately available. Moreover, unlike the original NADE, our training procedure scales to deep models. Empirically, ensembles of Deep NADE models obtain state of the art density estimation performance.},
  langid = {english},
  file = {C:\Users\filek51\Zotero\storage\9Z6W795R\Uria et al. - 2014 - A Deep and Tractable Density Estimator.pdf}
}

@inproceedings{vignac_digress_2023,
  title = {{{DiGress}}: {{Discrete Denoising}} Diffusion for Graph Generation},
  shorttitle = {{{DiGress}}},
  booktitle = {The {{Eleventh International Conference}} on {{Learning Representations}}},
  author = {Vignac, Clement and Krawczuk, Igor and Siraudin, Antoine and Wang, Bohan and Cevher, Volkan and Frossard, Pascal},
  year = {2023},
  month = feb,
  urldate = {2023-03-22},
  abstract = {This work introduces DiGress, a discrete denoising diffusion model for generating graphs with categorical node and edge attributes. Our model utilizes a discrete diffusion process that progressively edits graphs with noise, through the process of adding or removing edges and changing the categories. A graph transformer network is trained to revert this process, simplifying the problem of distribution learning over graphs into a sequence of node and edge classification tasks. We further improve sample quality by introducing a Markovian noise model that preserves the marginal distribution of node and edge types during diffusion, and by incorporating auxiliary graph-theoretic features. A procedure for conditioning the generation on graph-level features is also proposed. DiGress achieves state-of-the-art performance on molecular and non-molecular datasets, with up to 3x validity improvement on a planar graph dataset. It is also the first model to scale to the large GuacaMol dataset containing 1.3M drug-like molecules without the use of molecule-specific representations.},
  langid = {english},
  file = {C:\Users\filek51\Zotero\storage\PSB5H2KT\Vignac et al. - 2023 - DiGress Discrete Denoising diffusion for graph ge.pdf}
}

@article{wu_practical_2023,
  title = {Practical and {{Asymptotically Exact Conditional Sampling}} in {{Diffusion Models}}},
  author = {Wu, Luhuan and Trippe, Brian and Naesseth, Christian and Blei, David and Cunningham, John P.},
  year = {2023},
  month = dec,
  journal = {Advances in Neural Information Processing Systems},
  volume = {36},
  pages = {31372--31403},
  urldate = {2024-10-01},
  langid = {english},
  file = {C:\Users\filek51\Zotero\storage\YLW43TZM\Wu m. fl. - 2023 - Practical and Asymptotically Exact Conditional Sampling in Diffusion Models.pdf}
}

@inproceedings{xu_geodiff_2022,
  title = {{{GeoDiff}}: {{A Geometric Diffusion Model}} for {{Molecular Conformation Generation}}},
  shorttitle = {{{GeoDiff}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Xu, Minkai and Yu, Lantao and Song, Yang and Shi, Chence and Ermon, Stefano and Tang, Jian},
  year = {2022},
  urldate = {2022-01-31},
  abstract = {Predicting molecular conformations from molecular graphs is a fundamental problem in cheminformatics and drug discovery. Recently, significant progress has been achieved with machine learning...},
  langid = {english},
  file = {C\:\\Users\\filek51\\Zotero\\storage\\EYS4A3Z9\\Xu et al. - 2021 - GeoDiff A Geometric Diffusion Model for Molecular.pdf;C\:\\Users\\filek51\\Zotero\\storage\\RQXUAP95\\forum.html}
}

@inproceedings{yim_se3_2023,
  title = {{{SE}}(3) Diffusion Model with Application to Protein Backbone Generation},
  booktitle = {Proceedings of the 40th {{International Conference}} on {{Machine Learning}}},
  author = {Yim, Jason and Trippe, Brian L. and Bortoli, Valentin De and Mathieu, Emile and Doucet, Arnaud and Barzilay, Regina and Jaakkola, Tommi},
  year = {2023},
  month = jul,
  pages = {40001--40039},
  publisher = {PMLR},
  issn = {2640-3498},
  urldate = {2025-01-22},
  abstract = {The design of novel protein structures remains a challenge in protein engineering for applications across biomedicine and chemistry. In this line of work, a diffusion model over rigid bodies in 3D (referred to as frames) has shown success in generating novel, functional protein backbones that have not been observed in nature. However, there exists no principled methodological framework for diffusion on SE(3), the space of orientation preserving rigid motions in R3, that operates on frames and confers the group invariance. We address these shortcomings by developing theoretical foundations of SE(3) invariant diffusion models on multiple frames followed by a novel framework, FrameDiff, for estimating the SE(3) equivariant score over multiple frames. We apply FrameDiff on monomer backbone generation and find it can generate designable monomers up to 500 amino acids without relying on a pretrained protein structure prediction network that has been integral to previous methods. We find our samples are capable of generalizing beyond any known protein structure.},
  langid = {english},
  file = {C:\Users\filek51\Zotero\storage\FGLWA9AT\Yim m. fl. - 2023 - SE(3) diffusion model with application to protein backbone generation.pdf}
}

@misc{zhang_improving_2024,
  title = {Improving {{Diffusion Inverse Problem Solving}} with {{Decoupled Noise Annealing}}},
  author = {Zhang, Bingliang and Chu, Wenda and Berner, Julius and Meng, Chenlin and Anandkumar, Anima and Song, Yang},
  year = {2024},
  month = jul,
  number = {arXiv:2407.01521},
  eprint = {2407.01521},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2407.01521},
  urldate = {2024-10-01},
  abstract = {Diffusion models have recently achieved success in solving Bayesian inverse problems with learned data priors. Current methods build on top of the diffusion sampling process, where each denoising step makes small modifications to samples from the previous step. However, this process struggles to correct errors from earlier sampling steps, leading to worse performance in complicated nonlinear inverse problems, such as phase retrieval. To address this challenge, we propose a new method called Decoupled Annealing Posterior Sampling (DAPS) that relies on a novel noise annealing process. Specifically, we decouple consecutive steps in a diffusion sampling trajectory, allowing them to vary considerably from one another while ensuring their time-marginals anneal to the true posterior as we reduce noise levels. This approach enables the exploration of a larger solution space, improving the success rate for accurate reconstructions. We demonstrate that DAPS significantly improves sample quality and stability across multiple image restoration tasks, particularly in complicated nonlinear inverse problems. For example, we achieve a PSNR of 30.72dB on the FFHQ 256 dataset for phase retrieval, which is an improvement of 9.12dB compared to existing methods.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C\:\\Users\\filek51\\Zotero\\storage\\GP5HAKQC\\Zhang m. fl. - 2024 - Improving Diffusion Inverse Problem Solving with Decoupled Noise Annealing.pdf;C\:\\Users\\filek51\\Zotero\\storage\\DU744H3Y\\2407.html}
}

@inproceedings{zhang_unreasonable_2018,
  title = {The {{Unreasonable Effectiveness}} of {{Deep Features}} as a {{Perceptual Metric}}},
  booktitle = {2018 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Zhang, Richard and Isola, Phillip and Efros, Alexei A. and Shechtman, Eli and Wang, Oliver},
  year = {2018},
  month = jun,
  pages = {586--595},
  publisher = {IEEE},
  address = {Salt Lake City, UT},
  doi = {10.1109/CVPR.2018.00068},
  urldate = {2025-01-02},
  abstract = {While it is nearly effortless for humans to quickly assess the perceptual similarity between two images, the underlying processes are thought to be quite complex. Despite this, the most widely used perceptual metrics today, such as PSNR and SSIM, are simple, shallow functions, and fail to account for many nuances of human perception. Recently, the deep learning community has found that features of the VGG network trained on ImageNet classification has been remarkably useful as a training loss for image synthesis. But how perceptual are these so-called ``perceptual losses''? What elements are critical for their success? To answer these questions, we introduce a new dataset of human perceptual similarity judgments. We systematically evaluate deep features across different architectures and tasks and compare them with classic metrics. We find that deep features outperform all previous metrics by large margins on our dataset. More surprisingly, this result is not restricted to ImageNet-trained VGG features, but holds across different deep architectures and levels of supervision (supervised, self-supervised, or even unsupervised). Our results suggest that perceptual similarity is an emergent property shared across deep visual representations.},
  isbn = {978-1-5386-6420-9},
  langid = {english},
  file = {C:\Users\filek51\Zotero\storage\VSWLRIPD\Zhang m. fl. - 2018 - The Unreasonable Effectiveness of Deep Features as a Perceptual Metric.pdf}
}

@misc{Zhao2024rsta,
  title = {Conditional Sampling within Generative Diffusion Models},
  author = {Zhao, Zheng and Luo, Ziwei and Sj{\"o}lund, Jens and Sch{\"o}n, Thomas B.},
  year = {2024},
  month = sep,
  number = {arXiv:2409.09650},
  eprint = {2409.09650},
  primaryclass = {stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2409.09650},
  urldate = {2025-01-21},
  abstract = {Generative diffusions are a powerful class of Monte Carlo samplers that leverage bridging Markov processes to approximate complex, high-dimensional distributions, such as those found in image processing and language models. Despite their success in these domains, an important open challenge remains: extending these techniques to sample from conditional distributions, as required in, for example, Bayesian inverse problems. In this paper, we present a comprehensive review of existing computational approaches to conditional sampling within generative diffusion models. Specifically, we highlight key methodologies that either utilise the joint distribution, or rely on (pre-trained) marginal distributions with explicit likelihoods, to construct conditional generative samplers.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\filek51\\Zotero\\storage\\T32DVPA5\\Zhao m. fl. - 2024 - Conditional sampling within generative diffusion models.pdf;C\:\\Users\\filek51\\Zotero\\storage\\NIZPN6H5\\2409.html}
}
