\documentclass{proc}


\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{multirow}
\usepackage{booktabs} %

\usepackage{xcolor}
\usepackage{xspace}

\usepackage[round]{natbib}
\renewcommand{\bibname}{References}
\renewcommand{\bibsection}{\subsubsection*{\bibname}}


\bibliographystyle{apalike}

\usepackage{hyperref}


\newcommand{\theHalgorithm}{\arabic{algorithm}}
\newcommand{\suppmat}{appendix\xspace}


\input{math_commands}

\usepackage[capitalize,noabbrev]{cleveref}
\usepackage{algorithm}
\usepackage{algorithmic}

\renewcommand\algorithmiccomment[1]{\{\textit{#1}\}}


\usepackage[textsize=tiny]{todonotes}

\title{Solving Linear-Gaussian Bayesian Inverse Problems with\\Decoupled Diffusion Sequential Monte Carlo}

\author{Filip Ekström Kelvinius, Zheng Zhao, Fredrik Lindsten
\\
Linköping University
\\
\texttt{\{filip.ekstrom, zheng.zhao, fredrik.lindsten\}@liu.se}
}

\begin{document}
\maketitle

\begin{abstract}
A recent line of research has exploited pre-trained generative diffusion models as priors for solving Bayesian inverse problems. We contribute to this research direction by designing a sequential Monte Carlo method for linear-Gaussian inverse problems which builds on ``decoupled diffusion", where the generative process is designed such that larger updates to the sample are possible. The method is asymptotically exact and we demonstrate the effectiveness of our Decoupled Diffusion Sequential Monte Carlo (DDSMC) algorithm on both synthetic data and image reconstruction tasks. Further, we demonstrate how the approach can be extended to discrete data.
\end{abstract}

\section{Introduction}
Generative diffusion models \cite{sohl-dickstein_deep_2015, ho_denoising_2020-2, song_score-based_2021} have sparked an enormous interest from the research community, and shown impressive results on a wide variety of modeling task, ranging from image synthesis \citep{dhariwal_diffusion_2021-1, rombach_high-resolution_2022, saharia_photorealistic_2022} and audio generation \cite{chen_wavegrad_2020,kong_diffwave_2021} to molecule and protein generation \cite{hoogeboom_equivariant_2022, xu_geodiff_2022, corso_diffdock_2023}. A diffusion model consists of a neural network which implicitly, through a generative procedure, defines an approximation of the data distribution, $p_\theta(\x)$. While methods~\citep[e.g.][]{ho_classifier-free_2022} exist where the model is explicitly trained to define a conditional distribution $p_\theta(\x|\y)$, this conditional training is not always possible. Alternatively, a domain-specific likelihood $p(\y|\x)$ might be known, and in this case, using $p_\theta(\x)$ as a prior in a Bayesian inverse problem setting, i.e., sampling from the posterior distribution $p_\theta(\x|\y) \propto p(\y|\x)p_\theta(\x)$, becomes an appealing alternative. This approach is flexible since many different likelihoods can be used with the same diffusion model prior, without requiring retraining or access to paired training data. Previous methods for posterior sampling with diffusion priors, while providing impressive results on tasks like image reconstruction \citep{kawar_denoising_2022, chung_diffusion_2023, song2023pseudoinverseguided}, often rely on approximations and fail or perform poorly on simple tasks \citep[and our \Cref{sec:gmm_exp}]{cardoso_monte_2023-2}, making it uncertain to what extent they can solve Bayesian inference problems in general.

Sequential Monte Carlo (SMC) 
is a well-established method for Bayesian inference,
and its use of sequences of distributions makes it a natural choice to combine with diffusion priors. It also offers asymptotical guarantees, and the combination of SMC and diffusion models has recently seen a spark of interest \cite{trippe_diffusion_2023,dou_diffusion_2023-1,wu_practical_2023, cardoso_monte_2023-2}. Moreover, the design of an efficient SMC algorithm involves a high degree of flexibility while guaranteeing asymptotic exactness, which makes it an interesting framework for continued exploration. 

As previous (non-SMC) works on posterior sampling has introduced different approximations, and SMC offers a flexible framework with asymptotic guarantees, we aim to further investigate the use of SMC for Bayesian inverse problems with diffusion priors. In particular, we target the case of linear-Gaussian likelihood models. We develop a method which we call Decoupled Diffusion SMC (DDSMC) utilizing and extending a previously introduced technique for posterior sampling based on decoupled diffusion \citep{zhang_improving_2024}. With this approach, it is possible to make larger updates to the sample during the generative procedure, and it also opens up new ways of taking the conditioning on $\y$ into account in the design of the SMC proposal distribution. We show how this method can effectively perform posterior sampling for both synthetic data and image reconstruction tasks. We also further generalize DDSMC for discrete data.

\section{Background}
As mentioned in the introduction, we are interested in sampling from the posterior distribution
\(
    p_\theta(\x|\y) \propto p(\y|\x)p_\theta(\x),
\)
where the prior $p_\theta(\x)$ is implicitly defined by a pre-trained diffusion model and the likelihood is linear-Gaussian, i.e., 
\(
    p(\y|\x) = \mathcal{N}(\y|A\x, \sigma_y^2I).
\)
We will use the notation $p_\theta(\cdot)$ for any distribution that is defined via the diffusion model, the notation $p(\cdot)$ for the likelihood (which is assumed to be known), and $q(\cdot)$ for any distribution related to the fixed \emph{forward} diffusion process (see next section).

\subsection{Diffusion Models}
\label{sec:diffmodels}
Diffusion models are based on transforming data, $\x_0$, to Gaussian noise by a Markovian \emph{forward} process of the form\footnote{In principle, $\xt$ could also be multiplied by some time-dependent parameter, but we skip this here for simplicity.}
\begin{align}
    q(\xtplusone|\xt) = \mathcal{N}(\xtplusone|\xt, \beta_{t+1}I). \label{eq:difftrans}
\end{align}
The generative process then consists in trying to reverse this process, and is parametrized as a backward Markov process
\begin{align}
    p_\theta(\x_{0:T}) = p_\theta(\x_T)\prod_{t=0}^{T-1} p_\theta(\xt|\xtplusone). \label{eq:diffbackward_seq}
\end{align}
The reverse process $p_\theta$ is fitted to approximate the reversal of the forward process, i.e., $p_\theta(\xt|\xtplusone) \approx q(\xt|\xtplusone)$, where the latter can be expressed as 
\begin{align}
q(\xt|\xtplusone) = \int q(\xt|\xtplusone, \x_0)q(\x_0|\xtplusone)d\x_0. \label{eq:diffbackwardint}
\end{align}
While $q(\xt|\xtplusone, \x_0)$ is available in closed form, $q(\x_0|\xtplusone)$ is not, thereby rendering \Cref{eq:diffbackwardint} intractable. In practice, this is typically handled by replacing the conditional $q(\x_0|\xtplusone)$ with a point estimate $\delta_{\recon(\xtplusone)}(\x_0)$, where $\recon(\xtplusone)$ is a ``reconstruction" of $\x_0$, computed by a neural network. This is typically done with Tweedie's formula, where $\recon(\xtplusone) = \xt + \sigma_{t+1}^2 s_\theta(\xtplusone, t+1) \approx \mathbb{E}[\x_0|\xtplusone]$ and $s_\theta(\xtplusone, t+1)$ is a neural network which approximates the score $\nabla_{\xtplusone}\log q(\xtplusone)$ (and using the true score would result in the reconstruction being equal to the expected value). With this approximation, the backward kernel becomes $p_\theta(\xt|\xtplusone) = q(\xt|\xtplusone, \x_0=\recon(\xtplusone))$. 

\paragraph{Probability Flow ODE}
\citet{song_score-based_2021} described how diffusion models can be generalized as time-continuous stochastic differential equations (SDEs), and how sampling form a diffusion model can be viewed as solving the corresponding reverse-time SDE. They further derive a ``probability flow ordinary differential equation (PF-ODE)'', which allows sampling from the same distribution $p_\theta(\xt)$ as the SDE by solving a deterministic ODE (initialized randomly).

It can be noted that the probability flow formulation conceptually can be used to sample from \Cref{eq:diffbackwardint}. By solving the PF-ODE from state $\xtplusone \sim q(\xtplusone)$ to time $t=0$ we obtain a sample from the conditional $q(\x_0|\xtplusone)$, which can be plugged in to \Cref{eq:diffbackwardint} to generate a sample from $q(\xt|\xtplusone)$. For unconditional sampling this is a convoluted way of performing the generation, since the $\x_0$ obtained after solving the PF-ODE is already a sample from the approximate data distribution. However, as we discuss in \Cref{sec:ddsmc}, this approach does provide an interesting possibility for conditional sampling.

\subsection{Sequential Monte Carlo}
\label{sec:smc_background}
Sequential Monte Carlo \citep[SMC; see, e.g.,][for an overview]{NaessethLS:2019a} is a class of methods that enable sampling from a sequence of distributions $\{\pi_t(\xtt)\}_{t=0}^T$, which are known and can be evaluated up to a normalizing constant, i.e., $\pi_{t}(\xtt) = \gammat/Z_t$, where $\gammat$ can be evaluated pointwise. In an SMC algorithm, a set of $N$ samples, or \emph{particles}, are generated in parallel, and they are weighted such that a set of (weighted) particles $\{(\xtt^i, w_t^i)\}_{i=1}^N$ are approximate draws from the target distribution $\pi_t(\xtt)$. For each $t$, an SMC algorithm consists of three steps. The \emph{resampling} step samples a new set of particles $\{(\xtt^i)\}_{i=1}^N$ from $\text{Multinomial}(\{(\xtt^i)\}_{i=1}^N; \{w_t^i\}_{i=1}^N)$\footnote{The resampling procedure is a design choice. In this paper we use Multinomial with probabilities $\{w_t^i\}_{i=1}^N$ for simplicity.}. The second step is the \emph{proposal} step, where new samples $\{\xtonet^i\}_{i=1}^N$ are proposed as $\xtone^i\sim r_{t-1}(\xtone^i|\xtt^i), \ \xtonet^i = (\xtone^i, \xtt^i)$, and finally a \emph{weighting} step, $w^i_{t-1} \propto \gamma_{t-1}(\xtonet^i) / (r_{t-1}(\xtone^i|\xtt^i) \gamma_{t}(\xtt^i))$. To construct an SMC algorithm, it is hence necessary to determine two components: the target distributions $\{\pi_t(\xtt)\}_{t=0}^T$ (or rather, the unnormalized distributions $\{\gammat\}_{t=0}^T$), and the proposals $\{r_{t-1}(\xtone|\xtt)\}_{t=1}^{T}$. 

\paragraph{SMC as a General Sampler}
Even though SMC relies on a sequence of (unnormalized) distributions, it can still be used as a general sampler to sample from some ``static'' distribution $\phi(\x)$ by introducing auxiliary variables $\x_{0:T}$ and \emph{constructing} a sequence of distributions over $\{ \x_{t:T} \}_{t=0}^T$. As long as the marginal distribution of $\x_0$ w.r.t.\ the \emph{final} distribution $\pi_0(\xzerot)$ is equal to $\phi(\x_0)$, the SMC algorithm will provide a consistent approximation of $\phi(\x)$ (i.e.,\ increasingly accurate as the number of particles $N$ increases). The \emph{intermediate target} distributions $\{\pi_t(\xtt)\}_{t=1}^T$ are then merely a means for approximating the final target
$\pi_0(\xzerot)$.

\section{Decoupled Diffusion SMC}
\label{sec:ddsmc}
\subsection{Target Distributions}
\label{sec:ddsmc_prior}
The sequential nature of the generative diffusion model naturally suggests that SMC can be used for the Bayesian inverse problem, by constructing a sequence of target distributions based on \Cref{eq:diffbackward_seq}. This approach has been explored in several recent works \citep[see also \Cref{sec:related_work}]{wu_practical_2023, cardoso_monte_2023-2}. However, \citet{zhang_improving_2024} recently proposed an alternative simulation protocol for tackling inverse problems with diffusion priors, based on a ``decoupling" argument: they propose to simulate (approximately) from $p_\theta(\x_0|\xtplusone, \y)$ and then push this sample forward to diffusion time $t$ by sampling from the forward kernel $q(\xt|\x_0)$ instead of $q(\xt|\x_0, \xtplusone)$. The motivation is to reduce the autocorrelation in the generative process to enable making transitions with larger updates and thus correct larger, global errors. The resulting method is referred to as Decoupled Annealing Posterior Sampling (DAPS).

To leverage this idea, we can realize that the SMC framework is in fact very general and, as discussed in \Cref{sec:smc_background}, the sequence of target distributions can be seen as a design choice, as long as the final target $\pi_0(\xzerot)$ admits the distribution of interest as a marginal. Thus, it is possible to use the DAPS sampling protocol as a basis for SMC. This corresponds to redefining the prior over trajectories, from \Cref{eq:diffbackward_seq} to $p_\theta^0(\xzerot) = p_\theta(\x_T)\prod_{t=0}^{T-1}p_\theta^0(\xt|\xtplusone)$ where
\begin{align}
    p_\theta^0(\xt|\xtplusone) = q(\xt|\x_0=\recon(\xtplusone)).
\end{align}
Conceptually, this corresponds to reconstructing $\x_0$ conditionally on the current state $\xtplusone$, followed by adding noise to the reconstructed sample using the forward model. As discussed by \citet{zhang_improving_2024}, and also proven by us in \Cref{app:proofs}, the two transitions can still lead to the same marginal distributions for all time points $t$, i.e., $\int p_\theta^0(\xtt)d\xtplusoneT = \int p_\theta(\xtt)d\xtplusoneT$,  under the assumption that $\x_0 = \recon(\xtplusone)$ is a sample from $p_\theta(\x_0)$. This can be approximately obtained by, e.g., solving the reverse-time PF-ODE, and using this solution as reconstruction model $\recon(\xtplusone)$.

\paragraph{Generalizing the DAPS Prior}
By rewriting the conditional forward kernel $q(\xt|\xtplusone, \x_0)$ using Bayes theorem,
\begin{align}
    q(\xt|\xtplusone, \x_0) = \frac{q(\xtplusone|\xt)q(\xt|\x_0)}{q(\xtplusone|\x_0)},
\end{align}
we can view the standard diffusion backward kernel \Cref{eq:diffbackwardint} as applying the DAPS kernel, but conditioning the sample on the previous state $\xtplusone$, which acts as an ``observation'' with likelihood $q(\xt|\xtplusone)$. We can thus generalize the kernel in \Cref{eq:diffbackwardint} by annealing this likelihood with the inverse temperature $\eta$, 
\begin{align}
    p_\theta^\eta(\xt|\xtplusone) 
    \propto \int 
    q(\xtplusone|\xt)^\eta q(\xt|\x_0)
    \delta_{\recon(\xtplusone)}(\x_0)
    d\x_0
\end{align}
which allows us to smoothly transition between the DAPS ($\eta=0$) and standard ($\eta=1$) backward kernels.

\paragraph{Likelihood}
Having defined the prior as the generalized DAPS prior $p_\theta^\eta(\xtt)$, we also need to incorporate the conditioning on the observation $\y$. 
A natural starting point would be to choose as targets
\begin{align}
    \gammat = p(\y|\xt)p_\theta^\eta(\xtt) \label{eq:smc_ideal_target},
\end{align}
as this for $t=0$ leads to a distribution with marginal $p_\theta^\eta(\x|\y)$. However, the likelihood $p(\y|\xt) = \int p(\y|\x_0) p_\theta(\x_0|\xt)d\x_0$ in \Cref{eq:smc_ideal_target} is not tractable for $t>0$, and needs to be approximated. As the reconstruction $\recon(\xtplusone)$ played a central role in the prior, we can utilize this also for our likelihood. \citet{song2023pseudoinverseguided} proposed to use a Gaussian approximation $\tilde p_\theta(\x_0|\xt) \eqdef \mathcal{N} \left(\x_0|\recon(\xt), \rho_t^2I\right)$, resulting in
\begin{align}
p(\y|\xt) 
&\approx \tilde p(\y|\xt)
=\mathcal{N}\bigl(\y|A\recon(\xt), \sigma_y^2 I + \rho_t^2 AA^T\bigr), \label{eq:tilde_likelihood}
\end{align}
as the likelihood is linear and Gaussian. From hereon we will use the notation $\tilde p_\theta$ for any distribution derived from the Gaussian approximation $\tilde p_\theta(\x_0|\xt)$. As for $t=0$ the likelihood is known, we can rely on the consistency of SMC to obtain asymptotically exact samples, even if using approximate likelihoods in the intermediate targets.

\paragraph{Putting it Together}
To summarize, we define a sequence of intermediate target distributions for SMC according to
\begin{align}
    \gammat 
    &= \tilde p(\y|\xt) p_\theta(\x_T)\prod_{s=t}^{T-1} p^\eta_\theta(\xs|\xsplusone) \nonumber \\
    &= \frac{\tilde p(\y|\xt)}{\tilde p(\y|\xtplusone)} p_\theta^\eta(\xt|\xtplusone)\gammatplusone,
\end{align}
where, for $t>0$,
\begin{align}
    \tilde p(\y|\xt) 
    &= \mathcal{N}\left(\y|A\recon(\xt), \sigma_y^2 I + \rho_t^2 AA^T\right) \label{eq:ddsmc_likelihood}
\end{align}
and, for $\eta=0$,\footnote{See \Cref{app:general_DAPS_prior} for an expression for general $\eta$.}
\begin{align}
    p_\theta^{\eta=0}(\xt|\xtplusone) 
    &= \mathcal{N}\left(\xt|\recon(\xtplusone),
    \sigma^2_t I\right).
    \label{eq:ddsmc_target_transition}
\end{align}

\input{algorithms/ddsmc}
\subsection{Proposal}
\label{sec:ddsmc_proposal}
As the efficiency of the SMC algorithm in practice very much depends on the proposal, we will, as previous works on SMC (e.g., TDS \citep{wu_practical_2023} and MCGDiff \citep{cardoso_monte_2023-2}), use a proposal which incorporates information about the measurement $\y$. Again we are inspired by DAPS where the Gaussian approximation $\tilde p(\x_0|\xt)$ plays a central role. In their method, they use this approximation as a prior over $\x_0$, which together with the likelihood $p(\y|\x_0)$ form an (approximate) posterior
\begin{align}
    \tilde p_\theta(\x_0|\xtplusone, \y) \propto p(\y|\x_0)\tilde p_\theta(\x_0|\xtplusone).\label{eq:x0_posterior_general}
\end{align}
The DAPS method is designed for general likelihoods and makes use of Langevin dynamics to sample from this approximate posterior. However, in the linear-Gaussian case we can, similarly to \Cref{eq:tilde_likelihood}, obtain a closed form expression as
\begin{align}
    \tilde p_\theta(\x_0|\xtplusone, \y) = \mathcal{N}(\x_0|\tilde \mu_\theta^t(\xtplusone, \y), \mM^{-1}_{t+1}), \label{eq:x0_posterior_closed_form}
\end{align}
with $\tilde \mu_\theta^t(\xtplusone, \y) = \mM^{-1}_{t+1}\rvb_{t+1}$ and
\begin{subequations}
\label{eq:x0_posterior_mean_precisionMat}
\begin{align}
    \mM_{t+1} &= \frac{1}{\sigma_y^2}\mA^T\mA + \frac{1}{\rho_{t+1}^{2}}I, \\ 
    \rvb_{t+1} &= \frac{1}{\sigma_y^2}\mA^T\y + \frac{1}{\rho_{t+1}^2}\recon(\xtplusone).
\end{align}
\end{subequations}
Although this is an approximation of the true posterior, it offers an interesting venue for an SMC proposal by propagating a sample from the posterior forward in time. Assuming $\eta=0$ for notational brevity (see \cref{app:general_DAPS_prior} for the general expression) we can write this as 
\begin{align}
    r_t(\xt|\xtplusone, \y) = \int \tilde q(\xt|\x_0)\tilde p_\theta(\x_0|\xtplusone, \y)d\x_0. \label{eq:ddsmc_proposal_general}
\end{align}
This proposal is similar to one step of the generative procedure used in DAPS, where in their case the transition would correspond to using the diffusion forward kernel, i.e., $\tilde q(\xt|\x_0)=q(\xt|\x_0)$. We make a slight adjustment based on the following intuition. Considering the setting of non-informative measurements, we expect our posterior $p_\theta(\x_0|\y)$ to coincide with the prior. To achieve this, we could choose $\tilde q(\xt|\x_0)$ such that $r_t(\xt|\xtplusone) = p_\theta^0(\xt|\xtplusone)$. Using $q(\xt|\x_0)$ together with the prior $\tilde p_\theta(\x_0|\xtplusone)$ will, however, not match the covariance in $p^0_\theta(\xt|\xtplusone)$, since the Gaussian approximation $\tilde p_\theta(\x_0|\xtplusone)$ will inflate the variance by a term $\rho_{t+1}^2$.
To counter this effect, we instead opt for a covariance $\lambda_t^2I$ (i.e., $\tilde q(\xt|\x_0) = \mathcal{N}\left(\xt|\x_0, \lambda_t^2I\right)$) such that in the unconditional case, $r_t(\xt|\xtplusone) = p_\theta^0(\xt|\xtplusone)$. As everything is Gaussian, the marginalization in \Cref{eq:ddsmc_proposal_general} can be computed exactly, and we obtain our proposal for $t>0$ as
\begin{align}
    r_t(\xt|\xtplusone, \y) 
    = 
    \mathcal{N}\left( \xt|
    \tilde \mu_\theta^t(\xtplusone, \y), \lambda_{t}^2I + \mM_{t+1}^{-1}
    \right)  %
    \label{eq:ddsmc_proposal}
\end{align}
where $\lambda_t^2 = \sigma^2_t - \rho_{t+1}^2$ and, for $t=0$, $r_0(\x_0|\x_1, \y) = \delta_{\tilde \mu_\theta^0(\x_1, \y)}(\x_0)$. We can directly see that in the non-informative case (i.e., $\sigma_y \rightarrow \infty$), we will recover the prior as $\mM_{t+1}^{-1} = \rho_{t+1}^2I$ and $\rvb_{t+1} = \frac{1}{\rho_{t+1}^2}f_\theta(\xtplusone)$. 

If using the generalized DAPS prior from the previous section, we can also replace $\tilde q(\xt|\x_0)$ with $\tilde q_\eta(\xt|\xtplusone, \x_0)$ and make the corresponding matching of covariance.


\paragraph{Detailed Expressions for Diagonal $A$}
If first assuming that $A$ is diagonal in the sense that it is of shape $d_y \times d_x$ ($d_y \leq d_x$) with non-zero elements $(a_1, \dots, a_{d_y})$ only along the main diagonal, we can write out explicit expressions for the proposal. In this case, the covariance matrix $\mM_{t+1}^{-1}$ is diagonal with the $i$:th diagonal element ($i=1, \dots, d_x$) becoming
\begin{align}
    (\mM_{t+1}^{-1})_{ii} = 
    \begin{cases}
    \frac{\rho_{t+1}^2\sigma_y^2}{a_i^2\rho_{t+1}^2 + \sigma_y^2} & i\leq d_y\\
    \rho_{t+1}^2 & i>d_y
    \end{cases}.
\end{align}
This means that the covariance matrix in the proposal is diagonal with the elements $\sigma_t^2 - \rho_{t+1}^2 + \frac{\rho_{t+1}^2\sigma_y^2}{a_i^2\rho_{t+1}^2 + \sigma_y^2}$ for $i\leq d_y$, and $\sigma_t^2$ for $i> d_y$. Further, the mean of the $i$:th variable in the proposal becomes
\begin{multline}
    \tilde \mu_\theta^t(\xtplusone, \y)_i =  \\
    \begin{cases}
    \frac{a_i\rho_{t+1}^2}{a_i^2\rho_{t+1}^2 + \sigma_y^2} y_i + \frac{\sigma_y^2}{a_i^2\rho^2_{t+1} + \sigma_y^2}\recon(\xtplusone)_i & i\leq d_y \\
    \recon(\xtplusone)_i & i>d_y.
    \end{cases}
\end{multline}
We can see the effect of the decoupling clearly by considering the noise-less case, $\sigma_y=0$. In that case, we are always using the known observed value $(\x_0)_i=y_i/a_i$ as the mean, not taking the reconstruction, nor the previous value $(\xtplusone)_i$, into account. 

As the proposal is a multivariate Gaussian with diagonal covariance matrix, we can efficiently sample from it using independent samples from a standard Gaussian. We provide expressions for general choices of $\eta$ in \Cref{app:general_DAPS_prior}.

\paragraph{Non-diagonal $A$}
For a general non-diagonal $A$, the proposal at first glance looks daunting as it requires inverting $M_{t+1}$ which is a (potentially very large) non-diagonal matrix, and sampling from a multivariate Gaussian with non-diagonal covariance matrix (which requires a computationally expensive Cholesky decomposition). However, similar to previous work \citep{kawar_snips_2021, kawar_denoising_2022, cardoso_monte_2023-2}, by writing $A$ in terms of its singular value decomposition $A=USV^T$ where $U\in\R^{d_y\times d_y}$ and $V\in \R^{d_x\times d_x}$ are orthonormal matrices, and $S$ a $d_y \times d_x$-dimensional matrix with non-zero elements only on its main diagonal, we can multiply both sides of the measurement equation by $U^T$ to obtain
\begin{align}
    U^T \y = SV^T \x + \sigma_y U^T\epsilon.
\end{align}
By then defining $\y' = U^T\y$, $\x' = V^T \x$, $A'=S$, and $\epsilon' = U^T \eps \sim\mathcal{N}(0, UIU^T) = \mathcal{N}(0, I)$, we obtain the new measurement equation
\begin{align}
    \y' = A'\x' + \sigma_y \epsilon', \ \eps' \sim \mathcal{N}(0, I).
\end{align}
We can now run our DDSMC algorithm in this new basis and use the expressions from the diagonal case by replacing all variables with their corresponding primed versions, enabling efficient sampling also for non-diagonal $A$.

An algorithm outlining an implementation of DDSMC can be found in \Cref{algo:ddsmc}.

\input{tables/GMM_results_ddsmc}
\input{figures/code/ddsmc_xdim=800}


\subsection{D3SMC -- A Discrete Version}
We also extend our DDSMC algorithm to a discrete setting which we call Discrete Decoupled Diffusion SMC (D3SMC). Using $\x$ to denote a one-hot encoding of a single variable, we target the case where the measurement can be described by a transition matrix $Q_y$ as
\begin{align}
    p(\y|\x_0) = \text{Categorical}(\rvp = \x_0 Q_y), \label{eq:discrete_meas}
\end{align}
and when the data consists of $D$ variables, the measurement factorizes over these variables (i.e., for each of the $D$ variables, we have a corresponding measurement). This setting includes both inpainting (a variable is observed with some probability, otherwise it is in an ``unknown'' state) and denoising (a variable randomly transitions into a different state). To tackle this problem, we explore the D3PM-uniform model \citep{austin_structured_2021} and derive a discrete analogy to DDSMC which uses the particular structure of D3PM. We elaborate on the D3PM model in general and our D3SMC algorithm in particular in \Cref{app:d3smc}.

\section{Related Work}
\label{sec:related_work}
\paragraph{SMC and Diffusion Models} 
The closest related SMC methods for Bayesian inverse problems with diffusion priors are the Twisted Diffusion Sampler (TDS) \citep{wu_practical_2023} and Monte Carlo Guided Diffusion (MCGDiff) \citep{cardoso_monte_2023-2}. TDS is a general approach for solving inverse problems, while MCGDiff specifically focuses on the linear-Gaussian setting. These methods differ in the choices of intermediate targets and proposals.
TDS makes use of the reconstruction network to approximate the likelihood at time $t$ as $p(\y \mid \x_0 = f_\theta(\x_{t}))$ and then add the score of this approximate likelihood as a drift in the transition kernel. This requires differentiating through the reconstruction model w.r.t. $\x_{t}$, which can incur a significant computational overhead. MCGDiff instead uses the forward diffusion model to push the observation $\y$ "forward in time". Specifically, they introduce a potential function at time $t$ which can be seen as a likelihood corresponding to the observation model $\hat \y_t = A\x_t$, where $\hat \y_t$ is a noised version (according to the forward model at time $t$) of the original observation $\y$. 
DDSMC differs from both of these methods, and is conceptually based on reconstructing $\x_0$ from the current state $\x_t$, performing an explicit conditioning on $\y$ at time $t=0$, and then pushing the \emph{posterior distribution} forward to time $t-1$ using the forward model.
We further elaborate on the differences between TDS, MCGDiff, and DDSMC in \Cref{app:smc_comparison}~(see also the discussion by \citet{Zhao2024rsta}). 

SMCDiff \citep{trippe_diffusion_2023} and FPS \citep{dou_diffusion_2023-1} are two other SMC algorithms that target posterior sampling with diffusion priors, but these rely on the assumption that the learned backward process is an exact reversal of the forward process, and are therefore not consistent in general. SMC has also been used as a type of \emph{discriminator} guidance \cite{kim_refining_2023-1} of diffusion models in both the continuous \cite{liu_correcting_2024} and discrete \cite{ekstrom_kelvinius_discriminator_2024} setting.

\paragraph{Posterior Sampling with Diffusion Priors}
The closest non-SMC work to ours is of course DAPS \cite{zhang_improving_2024}, which we build upon, but also generalize in several ways. DDSMC is also related to $\Pi$GDM \citep{song2023pseudoinverseguided}, which introduces the Gaussian approximation used in \Cref{eq:tilde_likelihood,eq:x0_posterior_general}. %
Other proposed methods include DDRM \cite{kawar_denoising_2022} which defines a task-specific \emph{conditional} diffusion process which depends on a reconstruction $\recon(\xt)$, but where the optimal solution can be approximated with a model trained on the regular unconditional task. Recently, \citet{janati_divide-and-conquer_2024} proposed a method referred to as DCPS, which also builds on the notion of intermediate targets, but not within an SMC framework. Instead, to sample from these intermediate targets, they make use of Langevin sampling and a variational approximation that is optimized with stochastic gradient descent within each step of the generative model. 
All of these methods include various approximations, and contrary to DDSMC, none of them provides a consistent approximation of a given posterior. 


\section{Experiments}
\subsection{Gaussian Mixture Model}
\label{sec:gmm_exp}
We first experiment on synthetic data, and use the Gaussian mixture model problem from \citet{cardoso_monte_2023-2}. Here, the prior on $\x$ is a Gaussian mixture, and both the posterior and score are therefore known on closed form\footnote{This includes using the DDPM \citep{ho_denoising_2020-2} or "Variance-preserving" formulation of a diffusion model. We have included a derivation of DDSMC for this setting in \Cref{app:ddsmc_ddpm}} (we give more details in \Cref{app:gmm}), enabling us verify the efficiency of DDSMC while ablating all other errors. As a metric, we use the sliced Wasserstein distance \cite{flamary_pot_2021} between 10k samples from the true posterior and each sampling algorithm. 


\input{tables/GMM_results_other_methods}
\input{figures/code/gmm_comparison}



We start with investigating the influence of $\eta$ and the reconstruction function $\recon$ in \Cref{tab:gmm-table-ddsmc}. 
We run DDSMC with $N=256$ particles, and use $T=20$ steps in the generative process. 
As a reconstruction, we compare Tweedie's formula and the DDIM ODE solver \cite{song_denoising_2021}, see \Cref{eq:ddim_ode} in the \suppmat, where we solve the ODE for the ``remaining steps" $t, t-1, \dots, 0$. In the table, we see that using Tweedie's reconstruction requires a larger value of $\eta$. This can be explained by the fact that $\eta=0$ (DAPS prior) requires exact samples from $p_\theta(\x_0|\xt)$ for the DAPS prior to agree with the original prior (see \Cref{app:proofs}), and this distribution is more closely approximated using the ODE reconstruction. For higher dimensions (i.e., $d_x=800$) and using $\eta >0$, Tweedie's formula works slightly better than the ODE solver. Qualitatively, we find that using a smaller $\eta$ and/or using Tweedie's reconstruction tends to concentrate the samples around the different modes, see \Cref{fig:ddsmc-gmm} for an example of $d_x=800$ and $d_y=1$ (additional examples in \Cref{app:gmm}). This plot also shows how, in high dimensions, using a smaller $\eta$ is preferable, which could be attributed to lower $\eta$ enabling larger updates necessary in higher dimensions. 

Next, we compare DDSMC with other methods, both SMC-based (MCGDiff and TDS) and non-SMC-based (DDRM, DCPS, DAPS). For all SMC methods, we use 256 particles, and accelerated sampling according to DDIM \cite{song_denoising_2021} with 20 steps, which we also used for DAPS to enabling evaluating the benefits of our generalization of the DAPS method without confounding the results with the effect of common hyperparameters. For DDRM and DCPS, however, we used \thsnd{1} steps. More details are available in \Cref{app:gmm}. We see quantitatively in \Cref{tab:gmm-table-other} that DDSMC outperforms all other methods, even when using Tweedie's reconstruction. A qualitative inspection of generated samples in \Cref{fig:gmm-comparison} shows how DDSMC is the most resistant method to sampling from spurious modes, while DAPS, which we have based our method on, struggles to sample from the posterior. These are general trends we see when repeating with different seeds, see \Cref{app:gmm}.

\subsection{Image Restoration}
\input{figures/code/ffhq_main}
\input{tables/image_table_lpips}
We now turn our attention to the problem of image restoration, and use our method for inpainting, outpainting, and super-resolution on the FFHQ \citep{karras_style-based_2019} dataset, using a pretrained model by \citet{chung_diffusion_2023}. We implement DDSMC in the codebase of DCPS\footnote{\url{https://github.com/Badr-MOUFAD/dcps/}}, and follow their protocol of using 100 images from the validation set and compute LPIPS \citep{zhang_unreasonable_2018} as a quantitative metric. The results can be found in \Cref{tab:image-table-lpips}, in addition to numbers for DDRM, DCPS, DAPS, and MCGDiff. We used 5 particles for our method, and when using the PF-ODE as reconstruction, we used 5 ODE steps. Further implementation details are available in \Cref{app:image}. It should be noted that LPIPS measures perceptual similarity between the sampled image and the ground truth, which is not the same as a measurement of how well the method samples from the true posterior. Specifically, it does not say anything about diversity of samples or how well the model captures the posterior uncertainty. Nevertheless, the numbers indicate that we perform on par with previous methods. Notably, MCGDiff, which is also an SMC method and have the same asymptotic guarantees in terms of posterior sampling, performs similar or worse to DDSMC. 

Inspecting the generated images, we can see a visible effect when altering the reconstruction function and $\eta$, see \Cref{fig:ffhq_main} and more examples in \Cref{app:image}. It seems like increasing $\eta$ and/or using the PF-ODE as a reconstruction function adds more details to the image. With the GMM experiments in mind, where we saw that lower $\eta$ and using Tweedie's formula as reconstruction function made samples more concentrated around the modes, it can be argued that there is a similar effect here: using Tweedie's formula and lower $\eta$ lead to sampled images closer to a ``mode'' of images, meaning details are averaged out. On the other hand, changing to ODE-reconstruction and increasing $\eta$ further away from the mode, which corresponds to more details in the images. For the case of ODE and $\eta=1.0$, we see clear artifacts which explains the poor quantitative results.

\subsection{Discrete Data}\input{figures/code/bmnist_results}
As a proof of concept of our discrete algorithm D3SMC (see detailed description of the algorithm in \Cref{app:d3smc}), we try denoising on the binary MNIST dataset (i.e., each pixel is either 0 or 1), cropped to $24 \times 24$ pixels to remove padding. As a backbone neural network, we used a U-Net \citep{ronneberger_u-net_2015} trained with cross-entropy loss. We use a measurement model $\y = \x Q_y$ with $Q_y = (1-\beta_y) I + \beta_y \mathbbm{1}\mathbbm{1}^T/d$ and $\beta_y=0.6$, i.e., for the binary case, this means that the observed pixel has the same value as the original image with probability $0.7$, and changed with probability $0.3$. We present qualitative results for running D3SMC with $N=5$ particles in \Cref{fig:bmnist}, and more qualitative results can be found in \Cref{app:bmnist}. While the digits are often recovered, there are cases when they are not (e.g., a 4 becoming a 9, and a 9 becoming a 7). However, looking at multiple draws in the \suppmat, we can see how the model in such cases can sample different digits, suggesting a multi-modal nature of the posterior. 

\section{Discussion \& Conclusion}
In this paper, we have designed an SMC algorithm, DDSMC, for posterior sampling with diffusion priors which we also extended to discrete data. The method is based on decoupled diffusion, which we generalize by introducing a hyperparameter that allows bridging between full decoupling and standard diffusion (no decoupling). We demonstrate the superior performance of DDSMC compared to the state-of-the-art on a synthetic problem, which enables a quantitative evaluation of how well the different methods approximate the true posterior. We additionally test DDSMC on image reconstruction where it performs on par with previous methods. Specifically, it performs slightly better than the alternative SMC-based method MCGDiff, while slightly underperforming compared to DCPS. However, while LPIPS indeed is a useful quantitative metric for evaluating image reconstruction, it does not inherently capture how well a method approximates the true posterior, which is the aim of our method. We found that the methods performing particularly well on image reconstruction struggle on the synthetic task, highlighting a gap between perceived image quality and their ability to approximate the true posterior. As DDSMC still performs on par with previous work on image reconstruction while at the same time showing excellent performance on the GMM task, we view DDSMC as a promising method for bridging between solving challenging high-dimensional inverse problems and maintaining exact posterior sampling capabilities.
 

\section*{Acknowledgments}
\input{acknolwedgments}

\bibliography{ddsmc_references}


\newpage
\appendix
\onecolumn
\input{appendix}
\end{document}
