@misc{Zhao2024rsta,
  title = {Conditional Sampling within Generative Diffusion Models},
  author = {Zhao, Zheng and Luo, Ziwei and Sj{\"o}lund, Jens and Sch{\"o}n, Thomas B.},
  year = {2024},
  month = sep,
  number = {arXiv:2409.09650},
  eprint = {2409.09650},
  primaryclass = {stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2409.09650},
  urldate = {2025-01-21},
  abstract = {Generative diffusions are a powerful class of Monte Carlo samplers that leverage bridging Markov processes to approximate complex, high-dimensional distributions, such as those found in image processing and language models. Despite their success in these domains, an important open challenge remains: extending these techniques to sample from conditional distributions, as required in, for example, Bayesian inverse problems. In this paper, we present a comprehensive review of existing computational approaches to conditional sampling within generative diffusion models. Specifically, we highlight key methodologies that either utilise the joint distribution, or rely on (pre-trained) marginal distributions with explicit likelihoods, to construct conditional generative samplers.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\filek51\\Zotero\\storage\\T32DVPA5\\Zhao m. fl. - 2024 - Conditional sampling within generative diffusion models.pdf;C\:\\Users\\filek51\\Zotero\\storage\\NIZPN6H5\\2409.html}
}

@inproceedings{cardoso_monte_2023-2,
  title = {Monte {{Carlo}} Guided {{Denoising Diffusion}} Models for {{Bayesian}} Linear Inverse Problems.},
  booktitle = {The {{Twelfth International Conference}} on {{Learning Representations}}},
  author = {Cardoso, Gabriel and el Idrissi, Yazid Janati and Corff, Sylvain Le and Moulines, Eric},
  year = {2024},
  urldate = {2024-11-19},
  abstract = {Ill-posed linear inverse problems arise frequently in various applications, from computational photography to medical imaging. A recent line of research exploits Bayesian inference with informative priors to handle the ill-posedness of such problems. Amongst such priors, score-based generative models (SGM) have recently been successfully applied to several different inverse problems. In this study, we exploit the particular structure of the prior defined by the SGM to define a sequence of intermediate linear inverse problems. As the noise level decreases, the posteriors of these inverse problems get closer to the target posterior of the original inverse problem. To sample from this sequence of posteriors, we propose the use of Sequential Monte Carlo (SMC) methods. The proposed algorithm, {\textbackslash}algo, is shown to be theoretically grounded and we provide numerical simulations showing that it outperforms competing baselines when dealing with ill-posed inverse problems in a Bayesian setting.},
  langid = {english},
  file = {C:\Users\filek51\Zotero\storage\56D8NLYN\Cardoso m. fl. - 2023 - Monte Carlo guided Denoising Diffusion models for Bayesian linear inverse problems..pdf}
}

@inproceedings{dou_diffusion_2023-1,
  title = {Diffusion {{Posterior Sampling}} for {{Linear Inverse Problem Solving}}: {{A Filtering Perspective}}},
  shorttitle = {Diffusion {{Posterior Sampling}} for {{Linear Inverse Problem Solving}}},
  booktitle = {The {{Twelfth International Conference}} on {{Learning Representations}}},
  author = {Dou, Zehao and Song, Yang},
  year = {2023},
  month = oct,
  urldate = {2025-01-21},
  abstract = {Diffusion models have achieved tremendous success in generating high-dimensional data like images, videos and audio. These models provide powerful data priors that can solve linear inverse problems in zero shot through Bayesian posterior sampling. However, exact posterior sampling for diffusion models is intractable. Current solutions often hinge on approximations that are either computationally expensive or lack strong theoretical guarantees. In this work, we introduce an efficient diffusion sampling algorithm for linear inverse problems that is guaranteed to be asymptotically accurate. We reveal a link between Bayesian posterior sampling and Bayesian filtering in diffusion models, proving the former as a specific instance of the latter. Our method, termed filtering posterior sampling, leverages sequential Monte Carlo methods to solve the corresponding filtering problem. It seamlessly integrates with all Markovian diffusion samplers, requires no model re-training, and guarantees accurate samples from the Bayesian posterior as particle counts rise. Empirical tests demonstrate that our method generates better or comparable results than leading zero-shot diffusion posterior samplers on tasks like image inpainting, super-resolution, and deblurring.},
  langid = {english},
  file = {C:\Users\filek51\Zotero\storage\6C5A8ERL\Dou and Song - 2023 - Diffusion Posterior Sampling for Linear Inverse Problem Solving A Filtering Perspective.pdf}
}

@inproceedings{ekstrom_kelvinius_discriminator_2024,
  title = {Discriminator {{Guidance}} for {{Autoregressive Diffusion Models}}},
  booktitle = {Proceedings of {{The}} 27th {{International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Ekstr{\"o}m Kelvinius, Filip and Lindsten, Fredrik},
  year = {2024},
  month = apr,
  pages = {3403--3411},
  publisher = {PMLR},
  issn = {2640-3498},
  urldate = {2024-04-22},
  abstract = {We introduce discriminator guidance in the setting of Autoregressive Diffusion Models. The use of a discriminator to guide a diffusion process has previously been used for continuous diffusion models, and in this work we derive ways of using a discriminator together with a pretrained generative model in the discrete case. First, we show that using an optimal discriminator will correct the pretrained model and enable exact sampling from the underlying data distribution. Second, to account for the realistic scenario of using a sub-optimal discriminator, we derive a sequential Monte Carlo algorithm which iteratively takes the predictions from the discriminator into account during the generation process. We test these approaches on the task of generating molecular graphs and show how the discriminator improves the generative performance over using only the pretrained model.},
  langid = {english},
  file = {C:\Users\filek51\Zotero\storage\G5H6RF4U\Kelvinius and Lindsten - 2024 - Discriminator Guidance for Autoregressive Diffusio.pdf}
}

@inproceedings{janati_divide-and-conquer_2024,
  title = {Divide-and-{{Conquer Posterior Sampling}} for {{Denoising Diffusion}} Priors},
  booktitle = {The {{Thirty-eighth Annual Conference}} on {{Neural Information Processing Systems}}},
  author = {Janati, Yazid and Moufad, Badr and Durmus, Alain Oliviero and Moulines, Eric and Olsson, Jimmy},
  year = {2024},
  month = nov,
  urldate = {2024-11-21},
  abstract = {Recent advancements in solving Bayesian inverse problems have spotlighted denoising diffusion models (DDMs) as effective priors. Although these have great potential, DDM priors yield complex posterior distributions that are challenging to sample from. Existing approaches to posterior sampling in this context address this problem either by retraining model-specific components, leading to stiff and cumbersome methods, or by introducing approximations with uncontrolled errors that affect the accuracy of the produced samples. We present an innovative framework, divide-and-conquer posterior sampling, which leverages the inherent structure of DDMs to construct a sequence of intermediate posteriors that guide the produced samples to the target posterior. Our method significantly reduces the approximation error associated with current techniques without the need for retraining. We demonstrate the versatility and effectiveness of our approach for a wide range of Bayesian inverse problems. The code is available at {\textbackslash}url\{https://github.com/Badr-MOUFAD/dcps\}},
  langid = {english},
  file = {C:\Users\filek51\Zotero\storage\3XZHT5EU\Janati m. fl. - 2024 - Divide-and-Conquer Posterior Sampling for Denoising Diffusion priors.pdf}
}

@article{kawar_denoising_2022,
  title = {Denoising {{Diffusion Restoration Models}}},
  author = {Kawar, Bahjat and Elad, Michael and Ermon, Stefano and Song, Jiaming},
  year = {2022},
  month = dec,
  journal = {Advances in Neural Information Processing Systems},
  volume = {35},
  pages = {23593--23606},
  urldate = {2025-01-02},
  langid = {english},
  file = {C:\Users\filek51\Zotero\storage\HK8ZRXBI\Kawar m. fl. - 2022 - Denoising Diffusion Restoration Models.pdf}
}

@inproceedings{kim_refining_2023-1,
  title = {Refining {{Generative Process}} with {{Discriminator Guidance}} in {{Score-based Diffusion Models}}},
  booktitle = {Proceedings of the 40th {{International Conference}} on {{Machine Learning}}},
  author = {Kim, Dongjun and Kim, Yeongmin and Kwon, Se Jung and Kang, Wanmo and Moon, Il-Chul},
  year = {2023},
  month = jul,
  pages = {16567--16598},
  publisher = {PMLR},
  issn = {2640-3498},
  urldate = {2023-08-02},
  abstract = {The proposed method, Discriminator Guidance, aims to improve sample generation of pre-trained diffusion models. The approach introduces a discriminator that gives explicit supervision to a denoising sample path whether it is realistic or not. Unlike GANs, our approach does not require joint training of score and discriminator networks. Instead, we train the discriminator after score training, making discriminator training stable and fast to converge. In sample generation, we add an auxiliary term to the pre-trained score to deceive the discriminator. This term corrects the model score to the data score at the optimal discriminator, which implies that the discriminator helps better score estimation in a complementary way. Using our algorithm, we achive state-of-the-art results on ImageNet 256x256 with FID 1.83 and recall 0.64, similar to the validation data's FID (1.68) and recall (0.66). We release the code at https://github.com/alsdudrla10/DG.},
  langid = {english},
  file = {C:\Users\filek51\Zotero\storage\U8Q3DWTU\Kim et al. - 2023 - Refining Generative Process with Discriminator Gui.pdf}
}

@inproceedings{liu_correcting_2024,
  title = {Correcting {{Diffusion Generation}} through {{Resampling}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Liu, Yujian and Zhang, Yang and Jaakkola, Tommi and Chang, Shiyu},
  year = {2024},
  pages = {8713--8723},
  urldate = {2024-11-29},
  langid = {english},
  file = {C:\Users\filek51\Zotero\storage\5XY254K2\Liu m. fl. - 2024 - Correcting Diffusion Generation through Resampling.pdf}
}

@inproceedings{song2023pseudoinverseguided,
  title = {Pseudoinverse-Guided Diffusion Models for Inverse Problems},
  booktitle = {International Conference on Learning Representations},
  author = {Song, Jiaming and Vahdat, Arash and Mardani, Morteza and Kautz, Jan},
  year = {2023},
  file = {C:\Users\filek51\Zotero\storage\FLRQ2EAI\Song m. fl. - 2023 - Pseudoinverse-guided diffusion models for inverse problems.pdf}
}

@inproceedings{trippe_diffusion_2023,
  title = {Diffusion {{Probabilistic Modeling}} of {{Protein Backbones}} in {{3D}} for the Motif-Scaffolding Problem},
  booktitle = {The {{Eleventh International Conference}} on {{Learning Representations}}},
  author = {Trippe, Brian L. and Yim, Jason and Tischer, Doug and Baker, David and Broderick, Tamara and Barzilay, Regina and Jaakkola, Tommi S.},
  year = {2023},
  urldate = {2025-01-21},
  abstract = {Construction of a scaffold structure that supports a desired motif, conferring protein function, shows promise for the design of vaccines and enzymes. But a general solution to this motif-scaffolding problem remains open. Current machine-learning techniques for scaffold design are either limited to unrealistically small scaffolds (up to length 20) or struggle to produce multiple diverse scaffolds. We propose to learn a distribution over diverse and longer protein backbone structures via an E(3)-equivariant graph neural network. We develop SMCDiff to efficiently sample scaffolds from this distribution conditioned on a given motif; our algorithm is the first to theoretically guarantee conditional samples from a diffusion model in the large-compute limit. We evaluate our designed backbones by how well they align with AlphaFold2-predicted structures. We show that our method can (1) sample scaffolds up to 80 residues and (2) achieve structurally diverse scaffolds for a fixed motif.},
  langid = {english},
  file = {C:\Users\filek51\Zotero\storage\LHZ9A9GN\Trippe m. fl. - 2022 - Diffusion Probabilistic Modeling of Protein Backbones in 3D for the motif-scaffolding problem.pdf}
}

@article{wu_practical_2023,
  title = {Practical and {{Asymptotically Exact Conditional Sampling}} in {{Diffusion Models}}},
  author = {Wu, Luhuan and Trippe, Brian and Naesseth, Christian and Blei, David and Cunningham, John P.},
  year = {2023},
  month = dec,
  journal = {Advances in Neural Information Processing Systems},
  volume = {36},
  pages = {31372--31403},
  urldate = {2024-10-01},
  langid = {english},
  file = {C:\Users\filek51\Zotero\storage\YLW43TZM\Wu m. fl. - 2023 - Practical and Asymptotically Exact Conditional Sampling in Diffusion Models.pdf}
}

@misc{zhang_improving_2024,
  title = {Improving {{Diffusion Inverse Problem Solving}} with {{Decoupled Noise Annealing}}},
  author = {Zhang, Bingliang and Chu, Wenda and Berner, Julius and Meng, Chenlin and Anandkumar, Anima and Song, Yang},
  year = {2024},
  month = jul,
  number = {arXiv:2407.01521},
  eprint = {2407.01521},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2407.01521},
  urldate = {2024-10-01},
  abstract = {Diffusion models have recently achieved success in solving Bayesian inverse problems with learned data priors. Current methods build on top of the diffusion sampling process, where each denoising step makes small modifications to samples from the previous step. However, this process struggles to correct errors from earlier sampling steps, leading to worse performance in complicated nonlinear inverse problems, such as phase retrieval. To address this challenge, we propose a new method called Decoupled Annealing Posterior Sampling (DAPS) that relies on a novel noise annealing process. Specifically, we decouple consecutive steps in a diffusion sampling trajectory, allowing them to vary considerably from one another while ensuring their time-marginals anneal to the true posterior as we reduce noise levels. This approach enables the exploration of a larger solution space, improving the success rate for accurate reconstructions. We demonstrate that DAPS significantly improves sample quality and stability across multiple image restoration tasks, particularly in complicated nonlinear inverse problems. For example, we achieve a PSNR of 30.72dB on the FFHQ 256 dataset for phase retrieval, which is an improvement of 9.12dB compared to existing methods.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C\:\\Users\\filek51\\Zotero\\storage\\GP5HAKQC\\Zhang m. fl. - 2024 - Improving Diffusion Inverse Problem Solving with Decoupled Noise Annealing.pdf;C\:\\Users\\filek51\\Zotero\\storage\\DU744H3Y\\2407.html}
}

