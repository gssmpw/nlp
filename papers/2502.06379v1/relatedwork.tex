\section{Related Work}
\label{sec:related_work}
\paragraph{SMC and Diffusion Models} 
The closest related SMC methods for Bayesian inverse problems with diffusion priors are the Twisted Diffusion Sampler (TDS) \citep{wu_practical_2023} and Monte Carlo Guided Diffusion (MCGDiff) \citep{cardoso_monte_2023-2}. TDS is a general approach for solving inverse problems, while MCGDiff specifically focuses on the linear-Gaussian setting. These methods differ in the choices of intermediate targets and proposals.
TDS makes use of the reconstruction network to approximate the likelihood at time $t$ as $p(\y \mid \x_0 = f_\theta(\x_{t}))$ and then add the score of this approximate likelihood as a drift in the transition kernel. This requires differentiating through the reconstruction model w.r.t. $\x_{t}$, which can incur a significant computational overhead. MCGDiff instead uses the forward diffusion model to push the observation $\y$ "forward in time". Specifically, they introduce a potential function at time $t$ which can be seen as a likelihood corresponding to the observation model $\hat \y_t = A\x_t$, where $\hat \y_t$ is a noised version (according to the forward model at time $t$) of the original observation $\y$. 
DDSMC differs from both of these methods, and is conceptually based on reconstructing $\x_0$ from the current state $\x_t$, performing an explicit conditioning on $\y$ at time $t=0$, and then pushing the \emph{posterior distribution} forward to time $t-1$ using the forward model.
We further elaborate on the differences between TDS, MCGDiff, and DDSMC in \Cref{app:smc_comparison}~(see also the discussion by \citet{Zhao2024rsta}). 

SMCDiff \citep{trippe_diffusion_2023} and FPS \citep{dou_diffusion_2023-1} are two other SMC algorithms that target posterior sampling with diffusion priors, but these rely on the assumption that the learned backward process is an exact reversal of the forward process, and are therefore not consistent in general. SMC has also been used as a type of \emph{discriminator} guidance \cite{kim_refining_2023-1} of diffusion models in both the continuous \cite{liu_correcting_2024} and discrete \cite{ekstrom_kelvinius_discriminator_2024} setting.

\paragraph{Posterior Sampling with Diffusion Priors}
The closest non-SMC work to ours is of course DAPS \cite{zhang_improving_2024}, which we build upon, but also generalize in several ways. DDSMC is also related to $\Pi$GDM \citep{song2023pseudoinverseguided}, which introduces the Gaussian approximation used in \Cref{eq:tilde_likelihood,eq:x0_posterior_general}. %
Other proposed methods include DDRM \cite{kawar_denoising_2022} which defines a task-specific \emph{conditional} diffusion process which depends on a reconstruction $\recon(\xt)$, but where the optimal solution can be approximated with a model trained on the regular unconditional task. Recently, \citet{janati_divide-and-conquer_2024} proposed a method referred to as DCPS, which also builds on the notion of intermediate targets, but not within an SMC framework. Instead, to sample from these intermediate targets, they make use of Langevin sampling and a variational approximation that is optimized with stochastic gradient descent within each step of the generative model. 
All of these methods include various approximations, and contrary to DDSMC, none of them provides a consistent approximation of a given posterior.