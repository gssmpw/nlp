\section{Modeling Challenge}
\label{sec:model}

%As LLMs have unique model architectures and text encoding strategies, we are also interested in whether these modeling features affect their performances on ARAOC. 
In this section, we examine whether LLMs' modeling features affect their fluid intelligence from both the model architecture perspective and the information encoding perspective.




\iffalse
\begin{table}[tb]
\small
\centering
\setlength{\tabcolsep}{1mm}
\begin{tabular}{l|cccccc}
\toprule
\multirow{2}{*}{\textbf{Direction}} & \multicolumn{2}{c}{\textbf{Mistral}} & \multicolumn{2}{c}{\textbf{Llama-3}} & \textbf{GPT-3.5} & \textbf{GPT-4o} \\
\cmidrule(lr){2-3} \cmidrule(lr){4-5} %\cmidrule(lr){6-7}
~ & \textbf{7B} & \textbf{8*7B} & \textbf{8B} & \textbf{70B} & &\\
\midrule[0.5pt]
Left &4.00 && 2.00&11.00 &11.00 & 13.00\\

Right &4.00& &6.00&17.00 &20.00** & 28.00**\\

\bottomrule
\end{tabular}
\caption{Acc (in percentage) of LLMs with two mirroring directions. ``**'' means 
 the bottom result is significant better than the upper one with p < 0.05. See~\tref{tab:autoregressive_plus} for the Not M\% scores.}
\vspace{-0.2in}
\label{tab:autoregressive}
\end{table}
\fi
\subsection{The Bias of Model Architecture}
\label{sec:model architecture}
%\paragraph{Does the Autoregressive Characteristic of LLMs Affects Their Performances?}


When predicting output tokens given an input prompt, existing LLMs use the autoregressive decoding strategy~\cite{bahdanau2014neural}, which predicts the next token based solely on previous tokens. However, in some ARAOC tasks like Mirror, the newly generated part in the testing output grid may locate before the original part. This prevents LLMs from using information in the testing input grid to generate the new part, thus lowering their performances. For example, if the Mirror example in~\tref{tab:atom operations} is a testing input-output grid pair, LLMs cannot reference the bottom two green grids (the original subgrid) while generating the upper two green grids (the new subgrid), which makes the generation process more challenging.



To investigate this hypothesis, we conduct an experiment using the Mirror operation. Specifically, 
we first randomly generate 100 new input grids for Mirror, while lowering the number of rows and columns of input grids within [3, 7] to get more significant results. 
We then mirror each input grid towards left and right to create two individual tasks and overall leads to 100 tasks for left and right, respectively. We evaluate all the LLMs on these tasks and perform a binomial significance test to examine the differences in their performance across both directions. %We test all the LLMs on these tasks and present the 

Results are shown in~\tref{tab:autoregressive}. Noting that stronger models (the GPT models) perform significantly better when the mirroring direction is to the right, i.e., when the original subgrid is predicted before the mirrored one. This supports our hypothesis that the autoregressive nature of LLMs hinders their performance, as it prevents the simultaneous back and forth processing required by fluid intelligence. For weaker models, their relatively low scores render their results less conclusive, although Llama-3 still achieves higher Acc scores when the mirroring direction is to the right, which aligns with our hypothesis. Additionally, to further explore whether the above findings hold for LLMs of different sizes, we evaluate Mistral-8*7B and Llama-3-70B on these tasks and provide a detailed analysis in Appendix~\ref{appendix:additional table12}.


%\mo{Conclusion: autoregressive decoding}

\begin{table}[tb]
\small
\centering
\setlength{\tabcolsep}{2mm}
\begin{tabular}{l|llll}
\toprule
\textbf{Direction}& \textbf{Mistral}& \textbf{Llama-3} & \textbf{GPT-3.5}& \textbf{GPT-4o} \\
\midrule[0.5pt]
Left &4.00 & 2.00 &11.00 & 13.00\\

Right &4.00 &6.00 &20.00** & 28.00**\\

\bottomrule
\end{tabular}

\caption{Acc (in percentage) of LLMs with two mirroring directions. ``**'' means 
 the bottom result is significant better than the upper one with p < 0.05. See~\tref{tab:autoregressive_plus} for the Not M\% scores.}
\vspace{-0.2in}
\label{tab:autoregressive}
\end{table}


\subsection{Challenge on Information Usage}
\label{sec:information usage}
Since each task in ARAOC includes 3 in-context examples, %where the transformation rule is learned from several input-output grids and then applied to predict a testing output grid, 
the ability of LLMs to identify useful information from the in-context examples may affect their performances on ARAOC. We investigate this claim by calculating the saliency score~\cite{simonyan2013deep} of one of Mistral's incorrect predictions with respect to the in-context examples, %as saliency scores highlight which parts of the input most affect the prediction, 
with higher scores indicating a larger impact. %Specifically, we select a task representing the Move operation that Mistral fails to complete in~\tref{tab:large matrix}. Then, we calculate and visualize the saliency scores of tokens in the task's input prompt with respect to the number "6" that Mistral is supposed to move, using the LIT tool~\cite{tenney2020language}.


As shown in~\fref{fig:saliency}, Mistral should move "6" two steps to the right, %, as illustrated by the in-context examples. 
yet it incorrectly keeps "6" fixed in the output grid. % leading to the prediction error. 
With the saliency scores, we find that Mistral does not focus much on the moved parts in the in-context examples (e.g., all the "7"s in the first example). Instead, it focuses more on %the title and 
the unchanged parts, which leads it mistakenly assume that "6" should also be fixed. These observations illustrate that the inability to identify relevant information in in-context examples also explains why LLMs struggle with ARAOC tasks. In addition, we provide a saliency analysis example where Mistral makes a correct prediction in Appendix~\ref{appendix:additional saliency} for comparison.

\textbf{Overall, we conclude that LLMs' %inherently lack fluid intelligence because their
internal architecture also
%(the autoregressive scanning direction (\sref{sec:model architecture}) and the (triangular) causal attention mask (\sref{sec:information usage})) 
limits their ability to access global information, which is important for illustrating fluid intelligence.
%LLM's autoregressive decoding characteristic makes them lack the global understanding for demostrating fluid intelligence, which requires both forth and back information.
} While the findings on LLMs' fluid intelligence in previous sections are drawn from ARC and ARAOC tasks, they can be generalized to other real-world tasks and a further discussion on the applicability of our findings can be found in Appendix~\ref{appendix:generalization}.

\begin{figure}
    \centering
    \includegraphics[width=0.48\textwidth]{figures/saliency.pdf}
    \caption{A saliency analysis example, where darker means higher saliency corresponds to the boxed token.
} 
\vspace{-0.2in}
    \label{fig:saliency}
\end{figure}

%Mistral tends to focus more on parts that do not contribute to the final predictions. For instance, when predicting the number 8 for the Move case, an ideal LLM should concentrate on the number 8 in the input grid. However, Mistral directs more attention to the number 0 following the 8, causing it to incorrectly predict the position of 8 in the output grid. These observations lead us to conclude that the inability to correctly identify relevant information in in-context examples is a significant factor in why LLMs struggle with ARAOC tasks.

