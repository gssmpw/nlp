\section{Further details regarding the experiments in~\tref{tab:inductive reasoning examples}}
\label{appendix:inductive examples} 
In~\tref{tab:inductive reasoning examples}, we also measure GPT-4o and human's performances on the first three inductive reasoning tasks. The details are as follows:
\begin{enumerate}
    \item \textit{II} refers to \textit{Instruction Induction}. We conduct experiment on the ``Synonyms'' task in~\citet{honovich2023instruction}, since models in~\cite{honovich2023instruction} obtain the worst performance on this subtask. After GPT-4o has generated all the responses, instead of using BERT-Score to evaluate the responses, we ask a human annotator to give 0/1 (incorrect/correct) points to each response based on the ground truths. Then we calculate GPT-4o's average score as the final performance. Human's performance is extracted from~\citet{honovich2023instruction}.
    \item As for Deer, since it does not provide human performance, we invite an annotator to manually finish the first 50 tasks in its test set, and ask another annotator to give 0/1 (incorrect/correct) points to each human response based on the ground truths. We also ask GPT-4o to finish the first 50 tasks and do the same. Then we calculate GPT-4o and human's average score as the final performance.
    \item As for Mini Scan, the results are extracted from~\cite{qiuphenomenal}.
\end{enumerate}
\textit{II} refers to \textit{Instruction Induction}. We experiment on its ``Synonyms'' task, where models obtain the worst performance. As for Deer, we evaluate on the first 50 tasks of its test set

\section{The Prompts We Use in this Paper}
\label{appendix:prompts}
All the prompt templates we use in this paper are listed in~\fref{fig:original prompt}, \fref{fig:visual prompt}, \fref{fig:visual+textual prompt}, \fref{fig:visual+textual prompt1}, \fref{fig:matrix property prompt}, \fref{fig:without location prompt}, and \fref{fig:natural language prompt}. 



\begin{figure*}
  \begin{tcolorbox}
  \textbf{SYSTEM}:\\
  You are a helpful assistant.\\\\
  \textbf{USER}: \\
        You will be playing a game that need to find common patterns from input examples and apply the pattern for prediction on new examples.\\
Lets play a game where you are transforming an input grid of numbers into an output grid of numbers.\\

The numbers represent different colors:\\
0 = black\\
1 = blue\\
2 = red\\
3 = green\\
4 = yellow\\
5 = gray\\
6 = magenta\\
7 = orange\\
8 = cyan\\
9 = brown\\

Here are examples of input grids and its corresponding output grids:\\
Example input grid:\\
\{INPUT GRID 1\} \\
Example output grid:\\
\{OUTPUT GRID 1\} \\

Example input grid:\\
\{INPUT GRID 2\} \\
Example output grid:\\
\{OUTPUT GRID 2\} \\

Example input grid:\\
\{INPUT GRID 3\} \\
Example output grid:\\
\{OUTPUT GRID 3\} \\

The input grid is:

\{TESTING INPUT GRID\} \\

What is the output grid? Please only output your answer without analysis in the following format:

Output grid:

    \end{tcolorbox}
    \caption{The standard prompt we use in this paper that converts ARC/ARAOC tasks into matrix-format inputs. Also the prompt for the textual input/textual output setting in~\tref{tab:different format}.}
    \label{fig:original prompt}
\end{figure*}

\begin{figure*}
  \begin{tcolorbox}
  \textbf{SYSTEM}:\\
  You are a helpful assistant.\\\\
  \textbf{USER}: \\
        You will be playing a game that need to find common patterns from input examples and apply the pattern for prediction on new examples.\\

\{IMAGE\} \\

In the given image, there are two columns of matrices with elements represented by different colors. The left column contains the input matrices, and the right column contains the corresponding output matrices. The last row includes only an input matrix, while the other rows include both input and output matrices. Your task is to identify the pattern from the given input-output matrix pairs and apply this pattern to predict the output matrix for the input matrix in the last row. \\

Please complete the task by generating an image that includes only the predicted output matrix.

    \end{tcolorbox}
    \caption{The prompt for the visual input/visual output setting in~\tref{tab:different format}.}
    \label{fig:visual prompt}
\end{figure*}


\begin{figure*}
  \begin{tcolorbox}
  \textbf{SYSTEM}:\\
  You are a helpful assistant.\\\\
  \textbf{USER}: \\
        You will be playing a game that need to find common patterns from input examples and apply the pattern for prediction on new examples.\\
Lets play a game where you are transforming an input grid of numbers into an output grid of numbers.\\

The numbers represent different colors:\\
0 = black\\
1 = blue\\
2 = red\\
3 = green\\
4 = yellow\\
5 = gray\\
6 = magenta\\
7 = orange\\
8 = cyan\\
9 = brown\\

Here are examples of input grids and its corresponding output grids:\\
Example input grid:\\
\{INPUT GRID 1\} \\
Example output grid:\\
\{OUTPUT GRID 1\} \\

Example input grid:\\
\{INPUT GRID 2\} \\
Example output grid:\\
\{OUTPUT GRID 2\} \\

Example input grid:\\
\{INPUT GRID 3\} \\
Example output grid:\\
\{OUTPUT GRID 3\} \\

\{IMAGE\} \\

The 2D format of these input and output grids are also provided in the given image for your reference. \\

The input grid is:

\{TESTING INPUT GRID\} \\

What is the output grid? Please generate an image of the output grid similar to those in the given image, do not output any text.


    \end{tcolorbox}
    \caption{The prompt for the visual+textual input/visual output setting in~\tref{tab:different format}.}
    \label{fig:visual+textual prompt}
\end{figure*}



\begin{figure*}
  \begin{tcolorbox}
  \textbf{SYSTEM}:\\
  You are a helpful assistant.\\\\
  \textbf{USER}: \\
        You will be playing a game that need to find common patterns from input examples and apply the pattern for prediction on new examples.\\
Lets play a game where you are transforming an input grid of numbers into an output grid of numbers.\\

The numbers represent different colors:\\
0 = black\\
1 = blue\\
2 = red\\
3 = green\\
4 = yellow\\
5 = gray\\
6 = magenta\\
7 = orange\\
8 = cyan\\
9 = brown\\

Here are examples of input grids and its corresponding output grids:\\
Example input grid:\\
\{INPUT GRID 1\} \\
Example output grid:\\
\{OUTPUT GRID 1\} \\

Example input grid:\\
\{INPUT GRID 2\} \\
Example output grid:\\
\{OUTPUT GRID 2\} \\

Example input grid:\\
\{INPUT GRID 3\} \\
Example output grid:\\
\{OUTPUT GRID 3\} \\

\{IMAGE\} \\

The 2D format of these input and output grids are also provided in the given image for your reference. \\

The input grid is:

\{TESTING INPUT GRID\} \\

What is the output grid? Please only output your answer without analysis in the following format:

Output grid:

    \end{tcolorbox}
    \caption{The prompt for the visual+textual input/textual output setting in~\tref{tab:different format}.}
    \label{fig:visual+textual prompt1}
\end{figure*}

\begin{figure*}
  \begin{tcolorbox}
  \textbf{SYSTEM}:\\
  You are a helpful assistant.\\\\
  \textbf{USER}: \\
Given a matrix in the format of numpy array, please answer the following questions:

1. What is the size of this matrix?  Output in the format of (a,b).

2. What is the location of the non-zero subgrids. Please first find out all the corner elements of the subgrids, then output their locations in the order of [top-left, top-right, bottom-left, bottom-right], in the format of (which row, which col).

3. What is the transpose of this matrix? Output the transposed matrix in the format of a numpy array with elements separated by commas and enclosed in square brackets for each row like "[[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]]".

4. What is the rank of this matrix? Output the rank of the matrix.\\

Please only output your answer without analysis in the following format:

1.Size: 

2.Location: 

3.Transpose:

4.Rank:\\

Input Matrix: 

\{INPUT MATRIX\}

    \end{tcolorbox}
    \caption{The prompt for asking LLMs about matrix properties, which is used in~\sref{sec:understand matrix}.} 
    \label{fig:matrix property prompt}
\end{figure*}


\begin{figure*}
  \begin{tcolorbox}
  \textbf{SYSTEM}:\\
  You are a helpful assistant.\\\\
  \textbf{USER}: \\
        You will be playing a game that need to find common patterns from input examples and apply the pattern for prediction on new examples.\\
Lets play a game where you are transforming an input grid of numbers into an output grid of numbers.\\

The numbers represent different colors:\\
0 = black\\
1 = blue\\
2 = red\\
3 = green\\
4 = yellow\\
5 = gray\\
6 = magenta\\
7 = orange\\
8 = cyan\\
9 = brown\\

Here are examples of input grids and its corresponding output grids:\\
Example input grid:\\
\{INPUT GRID 1\} \\
Example output grid:\\
\{OUTPUT GRID 1\} \\

Example input grid:\\
\{INPUT GRID 2\} \\
Example output grid:\\
\{OUTPUT GRID 2\} \\

Example input grid:\\
\{INPUT GRID 3\} \\
Example output grid:\\
\{OUTPUT GRID 3\} \\

The input grid is:

\{TESTING INPUT GRID\} \\

What is the output grid? When answering this question, please avoid using information about: 1) the sizes of the input grids and the output grids; 2) the locations of different numbers in the input grids and the output grids. \\

Please only output your answer without analysis in the following format:

Output grid:

    \end{tcolorbox}
    \caption{Prompt that bans the use of location information.}
    \label{fig:without location prompt}
\end{figure*}

\begin{figure*}
  \begin{tcolorbox}
  \textbf{SYSTEM}:\\
  You are a helpful assistant.\\\\
  \textbf{USER}: \\
        You will be playing a game that need to find common patterns from input examples and apply the pattern for prediction on new examples.\\
Lets play a game where you are transforming an input grid of numbers into an output grid of numbers.\\

The numbers represent different colors:\\
0 = black\\
1 = blue\\
2 = red\\
3 = green\\
4 = yellow\\
5 = gray\\
6 = magenta\\
7 = orange\\
8 = cyan\\
9 = brown\\

Here are examples of input grids and its corresponding output grids:\\
Example input grid:\\
The matrix dimensions are \{\} columns by \{\} rows. Coordinates are based on a Cartesian coordinate system with the origin (0,0) at the bottom-left corner. The coordinates of the non-zero elements, listed from top to bottom and left to right, are: \{\} \\
Example output grid:\\
The matrix dimensions are \{\} columns by \{\} rows. Coordinates are based on a Cartesian coordinate system with the origin (0,0) at the bottom-left corner. The coordinates of the non-zero elements, listed from top to bottom and left to right, are: \{\}\\

...... (leave out input-output grid pairs 2 and 3) \\

The input grid is:

The matrix dimensions are \{\} columns by \{\} rows. Coordinates are based on a Cartesian coordinate system with the origin (0,0) at the bottom-left corner. The coordinates of the non-zero elements, listed from top to bottom and left to right, are: \{\} \\

What is the output grid?  \\

Please only output your answer without analysis in the following format:

Output grid:

    \end{tcolorbox}
    \caption{Prompt that converts matrix-format input to natural language.}
    \label{fig:natural language prompt}
\end{figure*}

\section{Detailed analysis regarding the failure of visual-based LLMs on ARC}
\label{appendix:visual analysis}
In~\tref{tab:different format} we find that it is extremely for visual-based LLMs to finish ARC tasks. After manually checking the model responses, we conlcude that it is because when answering ARC tasks, the visual-based LLMs needs to generate every small pixel (grid) correctly to form a totally correct output grid, which is extremely challenging for visual-based LLMs like GPT-4o. 

To take a deeper look at how visual-based GPT-4o fails on ARC, we sample a few grids from ARC instances where each grid has a size $\leq 10\times10$. We then take steps to ask GPT-4o to recognize the grid from the image and convert it to the textual matrix format. GPT-4o manages to recognize the sizes with around 50\% accuracy (considering each instance consists of more than 6 grids, this would result in large error propagation). Additionally, GPT-4o fails to correctly convert any grid to the matrix format. This study further illustrates that GPT-4o lacks the ability to ground the figures of grids to the symbolic space, consequently limiting its reasoning performance.

\section{Inference Configurations of LLMs}
\label{appendix:inference config}
For GPT models, we use their default inference configurations mentioned in \url{https://platform.openai.com/docs/guides/text-generation/completions-api}. As for Mistral and Llama, we set the maximum output length to be 3000 tokens, and follow their default settings for other configurations. During inference, for all the models, we maintain their official prompt templates unchanged.

\section{Fine-tuning Details}
\label{appendix:lora}
For all the fine-tuning experiments, we do not fine-tune all the LLM's parameters, and use LoRA instead, as mentioned in~\sref{sec:evaluate on original arc}. We fine-tune each model for 3 epochs with a batch size of 4. The dimension of LoRA's attention layer is set to 64, and the $\alpha$ and dropout rates are set to 16 and 0.1, respectively. The learning rate and weight decay are set to 2e-4 and 0.001, respectively.

For the fine-tuning data used in~\sref{sec:complex composition}, we generated an additional 500 tasks for each atomic operation beyond the 100 tasks in ARAOC, resulting in a total of 3000 tasks for the FT-atomic fine-tuning.

\section{Analysis on whether LLMs Learn Atomic Operations During Fine-tuning}
\label{appendix:further fine-tuning}

\begin{table*}[tb]
\renewcommand\arraystretch{1.1}
\centering
\setlength{\tabcolsep}{1.7mm}
\small
\begin{tabular}{lcccccc}
\toprule[1pt]
\multirow{2}*{LLM} & \multicolumn{6}{c}{\textbf{Individual Atomic Operation}} \\
\cmidrule{2-7}
& \multicolumn{1}{c}{\textbf{Move}} & \multicolumn{1}{c}{\textbf{Change Color}} & \multicolumn{1}{c}{\textbf{Copy}} & \multicolumn{1}{c}{\textbf{Mirror}} & \multicolumn{1}{c}{\textbf{Fill Internal}} & \multicolumn{1}{c}{\textbf{Scale}} \\

\midrule[0.5pt]

Llama-3 & 1.00 & 39.00 & 4.00 & 2.00 & 63.00 & 1.00   \\
$\text{Llama-3-FT-atomic}$ & 13.00 & 98.00 & 14.00 & 27.00 & 97.00 & 78.00  \\
$\text{Llama-3-FT-atomic w/o own}$ &10.00 (17.00)&94.00 (4.00)&6.00 (22.00)&5.00 (81.00)&58.00 (0.00)&2.00 (96.00) \\

\bottomrule[1pt]
\end{tabular}
\caption{Results of LLMs fine-tuned on different atomic operations. Not M\% scores of Llama-3-FT-atomic w/o own are shown in brackets. Not M\% scores for other models are listed in~\tref{tab:fine-tune arc performance_plus}.}
\vspace{-0.2in}
\label{tab:fine-tune different}
\end{table*}


In~\tref{tab:fine-tune arc performance}, we observe that fine-tuning on atomic operations enhances LLM performance on ARAOC tasks. However, it is possible that these improvements come from the LLMs learning the new matrix format of the input/output, rather than truly learning the atomic operations. To further investigate this, we conducted additional experiments. For Llama-3-FT-atomic in~\tref{tab:fine-tune arc performance}, we fine-tuned Llama-3 on all six atomic operations, using 500 tasks for each. In the new experiments, for each atomic operation, we fine-tuned Llama-3 on the other five atomic operations, using 600 tasks for each, ensuring the total number of fine-tuning examples remained consistent. The resulting model (Llama-3-FT-atomic w/o own) was then tested on the excluded atomic operation. The rationale behind this setup is that if the performance improvement observed in Llama-3-FT-atomic was solely due to the model learning the new matrix format of the input and output, rather than the atomic operations, the performance of Llama-3-FT-atomic w/o own should be similar to Llama-3-FT-atomic.

The results are presented in~\tref{tab:fine-tune different}. As shown, while Llama-3-FT-atomic w/o own improves upon Llama-3, performance gaps remain between Llama-3-FT-atomic w/o own and Llama-3-FT-atomic. Based on these results and the analysis in~\tref{tab:fine-tune different}, we conclude that, to a large extent, the fine-tuning process enhances the original LLM's understanding of atomic operations.


\section{Details on Crafting ARAOC}
\label{appendix:araoc}
\begin{enumerate}
    \item \textbf{Move}: for the Move tasks, the numbers of rows and columns of the input and the output grids are randomly initialized from [1,16], where the numbers of rows and columns of the subgrid is randomly initialized from [1, min(a, b)+1]. The number of moving step is sampled from [1, 8]
       \item \textbf{Change Colour}: for the Change Color tasks, the numbers of rows and columns of the input and the output grids are randomly initialized from [1,16], where the numbers of rows and columns of the subgrid is randomly initialized from [1, min(a, b)+1]. The new color is randomly sampled from the ten colors, while not overlapping with the original color.
   \item \textbf{Copy}: for the Copy tasks, the numbers of rows and columns of the input and the output grids are randomly initialized from [1,16], where the numbers of rows and columns of the subgrid is randomly initialized from [1, min(a, b)+1]. The number of steps between the copied subgrid and the original subgrid is sampled from [1, 8].
   \item \textbf{Mirror}: for the Mirror tasks, the numbers of rows and columns of the input and the output grids are randomly initialized from [1,16], where the numbers of rows and columns of the subgrid is randomly initialized from [1, min(a, b)+1]. 
   \item \textbf{Fill Internal}: for the Fill Internal tasks, the numbers of rows and columns of the input and the output grids are randomly initialized from [3,16], where the numbers of rows and columns of the subgrid is randomly initialized from [1, min(a, b)+1]. The filled color is randomly sampled from the ten colors.
\item \textbf{Scale}: for the Scale tasks, the numbers of rows and columns of the input and the output grids are randomly initialized from [2,5], where the numbers of rows and columns of the subgrid is randomly initialized from [1, min(a, b)+1]. 
\end{enumerate}

\begin{table}[tb]
\small
\centering
\setlength{\tabcolsep}{4mm}
\begin{tabular}{l|ccc}
\toprule
LLM& Move & Copy \\
\midrule
%$\text{Mistral-FT}_{\text{Move+Copy}}$ &25.00 &32.00 \\
 %\quad \quad \quad w/o Location &31.00 & 29.00\\
 %\midrule
%$\text{Mistral-FT}_{\text{Atom}}$ & 12.00&20.00 \\
 %\quad \quad \quad w/o Location & & \\
%\midrule
$\text{Llama-3-FT}_{\text{Move+Copy}}$ &26.00 &27.00 \\
 \quad \quad \quad w/o Location &18.00 &22.00 \\
 \midrule
%$\text{Llama-3-FT}_{\text{Atom}}$ & 13.00&14.00 \\
 %\quad \quad \quad w/o Location & & \\
%\midrule
GPT-4o & 13.00& 15.00\\
 \quad \quad \quad w/o Location &11.00 & 14.00\\
\bottomrule
\end{tabular}
\caption{Acc (in percentage) of LLMs without location information. See~\tref{tab:location_plus} for Not M\% scores.}%\lemao{There are some exceptions in this table. Maybe you should readjust the prompts for w/o location.}

\vspace{-0.1in}
\label{tab:location}
\end{table}

\section{Details on crafting the small-size tasks in~\tref{tab:large matrix}}
\label{appendix:small size}
Specifically, we randomly initialized 100 tasks for Move and Copy from a range that is half of the original range listed in Appendix~\ref{appendix:araoc}. This results in 100 new tasks for both atomic operations, with an average size of $4.96 \times 4.89$ and $4.81 \times 4.80$, respectively. For comparison, the original average sizes are $10.07 \times 10.16$ and $9.72 \times 9.62$, respectively.

\section{Additional Results on banning the location information}
\label{appendix:banning}

To further study the effect of LLMs' understanding 
of matrix-format input, we evaluate two example LLMs that have strong performances on Move and Copy ($\text{Llama-3-FT}_{\text{Move+Copy}}$ in~\tref{tab:composition}, and GPT-4o) tasks in ARAOC since they could provide more reliable results. Specifically, we require them to finish the Move and Copy tasks again without using the location information of subgrids (the prompt is listed in~\fref{fig:without location prompt}), which should be important for finishing such tasks. 

As can be seen in~\tref{tab:location}, banning location information do significantly decrease these LLMs' performances on both tasks. These results again indicate that a fundamental understanding of matrices is crucial for completing ARAOC and ARC tasks.%The only exception appears on Copy with GPT-4o, which may be because GPT-4o is strong enough to infer transformation rules only using relative distances between two subgrids in the same grid. 

\section{Additional Results of~\tref{tab:autoregressive}}
\label{appendix:additional table12}

\begin{table}[tb]
\small
\centering
\setlength{\tabcolsep}{3.5mm}
\begin{tabular}{l|ll}
\toprule
\textbf{Direction}& \textbf{Mistral-8*7B}& \textbf{Llama-3-70B} \\
\midrule[0.5pt]
Left & 0.00 (73.00) & 11.00 (55.00)\\

Right & 7.00** (74.00)& 17.00 (49.00)\\

\bottomrule
\end{tabular}
\caption{Acc (in percentage) of LLMs with two mirroring directions. Not \% scores are listed in brackets.``**'' means 
 the bottom result is significant better than the upper one with p < 0.05.}
%\vspace{-0.2in}
\label{tab:autoregressive_addition}
\end{table}

In this section, we present the results of Mistral-8*7B and Llama-3-70B in~\tref{tab:autoregressive}. As shown, both LLMs continue to perform better when the mirroring direction is to the right, with a significant difference observed for Mistral-8*7B. This further reinforces the findings in~\tref{tab:autoregressive}.

\section{Additional Saliency Analysis Example}
\label{appendix:additional saliency}

\begin{figure*}
    \centering
    \includegraphics[width=0.96\textwidth]{figures/saliency_new.pdf}
    \caption{A saliency analysis example where Mistral makes a correct prediction. Darker means higher saliency corresponds to the boxed token. As can be seen, 
} 
\vspace{-0.2in}
    \label{fig:saliency_new}
\end{figure*}

In this section, we present an additional saliency analysis example in~\fref{fig:saliency_new}, where Mistral correctly predicts a Change Color task. As shown, Mistral not only accurately focuses on the "8" that needs to be changed in the testing input grid but also pays sufficient attention to the other modified parts in the in-context examples. This allows it to gather enough information about the task requirements, and finally leading to the correct prediction.



\section{Generalization of Our Findings}
\label{appendix:generalization}

In sections~\sref{sec: atom operation}, \sref{sec:factor}, \sref{sec:matrix}, \sref{sec:model}, we conclude several findings on LLMs’ fluid intelligence. In this section, we further discussing how can our findings generalize to other real-world tasks.

\begin{enumerate}
    \item \textbf{LLMs is largely affected by the superficial properties of the input, and fail to grasp the underlying concept of the operations.} LLMs often focus on superficial input properties and fail to understand the underlying concepts of operations. In real-world tasks like code generation, this manifests as a tendency to replicate syntax patterns from the input without looking for deeper logical relationships. For example, LLMs might generate syntactically correct but semantically incorrect code, similar to their reliance on superficial features in ARC and ARAOC tasks.

    \item \textbf{Fine-tuning does not teach LLMs how to induct operations from the in-context examples.} While fine-tuning can improve LLMs' performance on specific tasks (e.g., completing function templates or implementing algorithms in code generation), it does not enable LLMs to inductively generalize from in-context examples to unseen scenarios. For example, in code generation, after fine-tuning on demands requiring the KMP algorithm, an LLM might still struggle to apply the KMP algorithm to novel demands, reflecting its challenges with generalization in ARC and ARAOC.

    \textbf{LLMs' limitations on obtaining global representations (also partly due to the auto-regressive generation characteristic) of the input tasks affect their fluid intelligence.} LLMs' limitations in forming global representations, partly due to their autoregressive generation nature, also impact their performance in real-world tasks. In real-world LLM tasks especially when the input context is long, LLMs often fail to maintain consistent naming (e.g., variable and function names in code generation) or follow through on multi-step logical dependencies when processing the input. This is similar to their inability to compose atomic operations into a holistic solution in ARC and ARAOC tasks.
\end{enumerate}


\section{Additional Not M\% Results}

For simplicity, we do not list the Not M\% scores for several tables. Here we list the Not M\% scores for these tables in~\tref{tab:controllable_plus}, \tref{tab:large matrix_plus}, \tref{tab:composition_plus}, \tref{tab:fine-tune arc performance_plus}, \tref{tab:location_plus}, \tref{tab:natural language input_plus}, and~\tref{tab:autoregressive_plus}.

\begin{table}[tb]
\small
\centering
\setlength{\tabcolsep}{3mm}
\begin{tabular}{llcc}
\toprule
& \textbf{COMB}& \textbf{Llama-3} & \textbf{GPT-4o} \\
\midrule[0.5pt]
\multirow{6}{*}{\textbf{Move}}&Up 1 &30.00&2.00\\
&Up 2 &24.00&1.00\\
&Up 3 &28.00&2.00\\
\cmidrule{2-4}
&Up-right 1 &16.00&3.00\\
&Up-right 2 &26.00&1.00\\
&Up-right 3 &28.00&0.00\\
\midrule[0.5pt]
\multirow{6}{*}{\textbf{Copy}}&Up 1&6.00&2.00\\
&Up 2 &14.00&1.00\\
&Up 3 &16.00&0.00\\
\cmidrule{2-4}
&Up-right 1 &12.00&0.00\\
&Up-right 2 &14.00&1.00\\
&Up-right 3 &20.00&1.00\\
\bottomrule
\end{tabular}
\caption{Not M\% scores for~\tref{tab:controllable}.}

\label{tab:controllable_plus}
\end{table}

\begin{table}[tb]
\small
\centering
\setlength{\tabcolsep}{1.3mm}
\begin{tabular}{ll|cccc}
\toprule
& \textbf{Setting} & \textbf{Mistral} & \textbf{Llama-3} & \textbf{GPT-3.5} & \textbf{GPT-4o} \\
\midrule[0.5pt]
\multirow{2}{*}{\textbf{Move}}& Ori &36.00 &19.00 &27.00 &0.00 \\
& Small & 8.00&14.00 &1.00 &0.00\\
\midrule
\multirow{2}{*}{\textbf{Copy}}&Ori &43.00 &13.00 &29.00 &0.00\\
&Small & 13.00& 10.00& 1.00&0.00 \\
\bottomrule
\end{tabular}
\caption{Not M\% scores for~\tref{tab:large matrix}.}

\label{tab:large matrix_plus}
\end{table}

\begin{table}[tb]
\small
\centering
\setlength{\tabcolsep}{2mm}
\begin{tabular}{lcc|cc}
\toprule
\textbf{LLM} &\textbf{Move} & \textbf{Copy} & \textbf{Comp} %& \textbf{Rank}
\\
\midrule
$\text{Mistral-FT}_{\text{Move}}$ &2.00&34.00&44.00  \\
$\text{Mistral-FT}_{\text{Copy}}$ &6.00&12.00&7.00\\
$\text{Mistral-FT}_{\text{Move+Copy}}$ &6.00&6.00&15.00\\
\midrule
$\text{Llama-3-FT}_{\text{Move}}$ &9.00&8.00&11.00\\
$\text{Llama-3-FT}_{\text{Copy}}$ &5.00&1.00&11.00\\
$\text{Llama-3-FT}_{\text{Move+Copy}}$ &6.00&1.00&3.00\\
\midrule
GPT-3.5 &27.00&29.00&42.00\\
GPT-4o &3.00&6.00&1.00\\
\bottomrule
\end{tabular}
\caption{Not M\% scores for~\tref{tab:composition}}

\label{tab:composition_plus}
\end{table}

\begin{table*}[tb]
\renewcommand\arraystretch{1.1}
\centering
\setlength{\tabcolsep}{2.5mm}
\small
\begin{tabular}{lcccccc|c}
\toprule[1pt]
\multirow{2}*{LLM} & \multicolumn{6}{c}{\textbf{Individual Atomic Operation}} & \multicolumn{1}{c}{\textbf{ Composition}} \\
\cmidrule{2-8}
& \multicolumn{1}{c}{\textbf{Move}} & \multicolumn{1}{c}{\textbf{Change Color}} & \multicolumn{1}{c}{\textbf{Copy}} & \multicolumn{1}{c}{\textbf{Mirror}} & \multicolumn{1}{c}{\textbf{Fill Internal}} & \multicolumn{1}{c}{\textbf{Scale}} & \multicolumn{1}{c}{\textbf{ARC}}  \\
% & Acc$\uparrow$ & Acc$\uparrow$ & Acc$\uparrow$ & Acc$\uparrow$ & Acc$\uparrow$ & Acc$\uparrow$ & Acc$\uparrow$ \\
\midrule[0.5pt]
%Mistral &36.00&30.00&43.00&97.00&31.00&98.00&48.00 \\
%$\text{Mistral-FT}$  &11.00&0.00&6.00&52.00&2.00&0.00&38.00 \\
%%\midrule
Llama-3 &19.00&17.00&13.00&96.00&6.00&89.00&33.00  \\
$\text{Llama-3-FT-arc}$ &20.00&14.00&7.00&92.00&1.00&95.00& 29.00\\
$\text{Llama-3-FT-atomic}$ &9.00&1.00&8.00&54.00&1.00&4.00&40.00  \\
$\text{Llama-3-FT-atomic-arc}$ &14.00&0.00&10.00&39.00&0.00&6.00& 30.00 \\
\midrule
%GPT-3.5 &27.00&13.00&29.00&89.00&12.00&80.00&35.00 \\
%GPT-4 &3.00&0.00&6.00&52.00&0.00&70.00&16.00  \\
GPT-4o&0.00&0.00&0.00&48.00&0.00&72.00&11.00 \\
\bottomrule[1pt]
\end{tabular}
\caption{Not M\% scores for \tref{tab:fine-tune arc performance}.}
\vspace{-0.2in}
\label{tab:fine-tune arc performance_plus}
\end{table*}

\begin{table}[tb]
\small
\centering
\setlength{\tabcolsep}{4mm}
\begin{tabular}{l|cccc}
\toprule
LLM& Move & Copy \\
\midrule
%$\text{Mistral-FT}_{\text{Move+Copy}}$ &25.00 &32.00 \\
 %\quad \quad \quad w/o Location &31.00 & 29.00\\
 %\midrule
%$\text{Mistral-FT}_{\text{Atom}}$ & 12.00&20.00 \\
 %\quad \quad \quad w/o Location & & \\
%\midrule
$\text{Llama-3-FT}_{\text{Move+Copy}}$ &6.00&1.00 \\
 \quad \quad \quad w/o Location &6.00&0.00\\
 \midrule
%$\text{Llama-3-FT}_{\text{Atom}}$ & 13.00&14.00 \\
 %\quad \quad \quad w/o Location & & \\
%\midrule
GPT-4o &0.00&0.00\\
 \quad \quad \quad w/o Location &0.00&1.00 \\
\bottomrule
\end{tabular}
\caption{Not M\% scores for~\tref{tab:location}.}

\vspace{-0.1in}
\label{tab:location_plus}
\end{table}

\begin{table}[tb]
\small
\centering
\setlength{\tabcolsep}{0.5mm}
\begin{tabular}{ll|cccc}
\toprule
& \textbf{Method}& \textbf{Mistral}& \textbf{Llama-3} & \textbf{GPT-3.5}& \textbf{GPT-4o} \\
\midrule[0.5pt]
\multirow{2}{*}{\textbf{Move}}& w/o NL &-&-&-&-\\
& NL &82.00&0.00&0.00& 1.00\\
\midrule[0.5pt]
\multirow{2}{*}{\textbf{Color}}& w/o NL &-&-&-&- \\
& NL &83.00&0.00&1.00&0.00\\
\midrule[0.5pt]
\multirow{2}{*}{\textbf{Copy}}& w/o NL &-&-&-&- \\
& NL &83.00&4.00&2.00&0.00 \\
\midrule[0.5pt]
\multirow{2}{*}{\textbf{Mirror}}& w/o NL &-&-&-&- \\
& NL &80.00&30.00&27.00&0.00\\
\midrule[0.5pt]
\multirow{2}{*}{\textbf{Fill Internal}}& w/o NL &-&-&-&-\\
& NL &84.00&1.00&0.00&0.00\\
\midrule[0.5pt]
\multirow{2}{*}{\textbf{Scale}}& w/o NL &-&-&-&-\\
& NL &98.00&75.00&63.00&41.00 \\
\bottomrule
\end{tabular}
\caption{Not M\% scores for~\tref{tab:natural language input}. For scores under ``w/o NL'', please refer to~\tref{tab:araoc results}.}
\vspace{-0.1in}
\label{tab:natural language input_plus}
\end{table}

\begin{table}[tb]
\small
\centering
\setlength{\tabcolsep}{2mm}
\begin{tabular}{l|cccc}
\toprule
\textbf{Direction}& \textbf{Mistral}& \textbf{Llama-3} & \textbf{GPT-3.5}& \textbf{GPT-4o} \\
\midrule[0.5pt]
Left &68.00&76.00&52.00&33.00\\

Right &71.00&74.00&49.00&22.00\\

\bottomrule
\end{tabular}
\caption{Not M\% scores for~\tref{tab:autoregressive}.}
\vspace{-0.1in}
\label{tab:autoregressive_plus}
\end{table}