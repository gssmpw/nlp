

\begin{table*}[tb]
  \renewcommand\arraystretch{1.1}
  \centering
  \setlength{\tabcolsep}{1.5mm}
  \small
  \begin{tabular}{p{3cm}|p{3.5cm}|p{3.5cm}|p{4.3cm}}
    \toprule[1pt]
    \multicolumn{1}{c}{\textbf{II}~\cite{honovich2023instruction}} & \multicolumn{1}{c}{\textbf{Deer}~\cite{yang2024language}} & \multicolumn{1}{c}{\textbf{Mini Scan}~\cite{qiuphenomenal}} & \multicolumn{1}{c}{\textbf{ARC}~\cite{chollet2019measure}} \\
    \midrule[0.5pt]

    \begin{minipage}{3cm}
    \vspace{-4mm}
      \footnotesize
      \textbf{Input}: turn \newline
      \textbf{Output}: play \newline
      \newline
      \textbf{Input}: floor \newline
      \textbf{Output}: level \newline
      \newline
      \textbf{Input}: embrace \newline
      \textbf{Output}: cover \newline
      ......
      %%Output: communicate \newline
      %%Input: wash \newline
      %Output: washing
      %\newline
      \newline
      \textbf{Instruction: ?}
    \end{minipage}
    &
    \begin{minipage}{3.5cm}
      \footnotesize
      \textbf{Rule Type}: \newline
      There exists \_ , which \_. \newline
      \newline
      \textbf{Fact 1}: \newline
      Crabs are generally covered with a thick exoskeleton. \newline
      \newline
      \textbf{Fact 2}: \newline
      Lobsters are invertebrates with a hard protective exoskeleton. \newline
      %\newline
      %Fact3: The mollusc shell is typically a calcareous exoskeleton which encloses, supports, and protects the soft parts of an animal. \newline
        %\textbf{Fact 3}: \newline 
        ......
      \newline
      \textbf{Rule: ?}
    \end{minipage}
    &\begin{minipage}{3cm}
      \vspace{-3mm} % This reduces the space between the top of the page and the image
      \centering
      \includegraphics[width=3cm]{figures/mini_scan1.pdf}
    \end{minipage}
    & 
    \begin{minipage}{4.3cm}
      \vspace{-3mm} % This reduces the space between the top of the page and the image
      \centering
      \includegraphics[width=4.3cm]{figures/figure1_new.pdf}
    \end{minipage}
    \\
    \midrule[0.5pt]
    \begin{minipage}{3cm}
      \centering
      \includegraphics[width=2.5cm]{figures/instruction_induction.pdf}
    \end{minipage} &
    \begin{minipage}{3.5cm}
      \centering
      \includegraphics[width=2.5cm]{figures/deer.pdf}
    \end{minipage} &
    \begin{minipage}{3cm}
      \centering
      \includegraphics[width=2.5cm]{figures/mini_scan.pdf}
    \end{minipage} &
    \begin{minipage}{5cm}
      \centering
      \includegraphics[width=2.5cm]{figures/arc.pdf}
    \end{minipage} \\
\bottomrule[1pt]
  \end{tabular}
  \caption{Examples of different inductive reasoning tasks with GPT-4o and human's scores. Check Appendix~\ref{appendix:inductive examples} for details of score calculation on the first three tasks. The transformation rule of the ARC example can be decomposed into two atomic operations in the dotted box %\lemao{Add more information about task composition.}
    %Two decomposed atomic operations 
    %described in~\tref{tab:atom operations}
    :1) move down the blue subgrid for one step; 2) change its color to red.}
  \vspace{-0.2in}
  \label{tab:inductive reasoning examples}
\end{table*}



\section{Introduction}
\label{intro}
Large language models (LLMs) have demonstrated impressive performance on a range of challenging NLP tasks~\cite{davis2023benchmarks, zhao2024large, wang2023hypothesis, yang2024language, frieder2024mathematical}, %These successes 
which naturally leads to the question: \emph{how close are LLMs to achieving human-level intelligence?}

To explore this question, it is useful to draw on established research in human intelligence~\cite{cattell1963theory, cattell1971abilities}, which categorizes intelligence into two major types: \textbf{crystallized intelligence}, the ability to apply prior knowledge to solve problems, and \textbf{fluid intelligence}, the ability to tackle new problems without relying on pre-existing knowledge. Of the two, fluid intelligence is often viewed as more indicative of general cognitive ability~\cite{jaeggi2008improving, chollet2019measure, barak2024investigating}, as it captures the capability to solve novel problems. %As a result, most widely used intelligence assessments focus on tasks involving fluid intelligence. 
Moreover, evaluating fluid intelligence is essential for assessing the reasoning abilities of LLMs since LLMs have been exposed to and memorized vast amounts of knowledge during pre-training~\cite{kaplan2020scaling, biderman2024emergent}, potentially blurring the line between their reasoning ability and memorization capacity.



% Large language models (LLMs) have exhibited strong performances on various challenging real-word tasks, %such as 
% %various reasoning tasks such as 
% %reasoning
% ~\cite{davis2023benchmarks, zhao2024large, wang2023hypothesis, yang2024language, frieder2024mathematical} %, inductive reasoning~\cite{honovich2022instruction, wang2023hypothesis, yang2024language} 
% %and mathematical question answering
% %~\cite{imani2023mathprompter, frieder2024mathematical}. %\lemao{To Junjie: please add more references here.} 
% which inspires people to think of a question: how intelligent existing LLMs are and how to measure their level of intelligence? According to the widely accepted Catellâ€™s theory~\cite{cattell1963theory, cattell1971abilities}, intelligence can be categorized into \textit{crystallized intelligence} (the ability to use prior knowledge to solve tasks) and \textit{fluid intelligence} (the capacity to address new problems without relying on existing knowledge), while fluid intelligence is considered to be more representative since it indicates the human capacity to address novel questions without prior knowledge~\cite{jaeggi2008improving, chollet2019measure, barak2024investigating}. 
\iffalse
\begin{table}[tb]
\small
\centering
\setlength{\tabcolsep}{1mm}
\begin{tabular}{lcccc}
\toprule
 &Instruction Induct(synonym) & Deer&
%& \textbf{$\text{P-Acc}_{\text{A}}$}$\uparrow$ & \textbf{$\text{P-Acc}_{\text{M}}$}$\uparrow$ 
Mini Scan & ARC \\
\midrule
GPT-4o &95.00 &84.00 &61.70 & 19.00\\
\midrule
Human & 80.00 &86.00 & \textasciitilde80.00& \textasciitilde75.00\\

\bottomrule
\end{tabular}
\caption{GPT-4o's performances on other abstract reasoning benchmarks.
}
\vspace{-0.2in}
\label{tab:existing abstract tasks}
\end{table}

\begin{figure}
    \centering
    %\vspace{-0.2in}
    \includegraphics[width=0.45\textwidth]{latex/figures/figure1_new.pdf}
    %\vspace{-0.2in}
    \caption{A decomposed example from ARC task whose input (leftmost) and output (rightmost) grids are listed outside the dotted box. The transformation rule of this task can be decomposed into two atomic operations in the dotted box %\lemao{Add more information about task composition.}
    %Two decomposed atomic operations 
    %described in~\tref{tab:atom operations}
    : 1) move down the blue subgrid for one step (\textbf{Move}); 2) change its color to red (\textbf{Change Color}), which is illustrated inside the dotted box.} 
    \label{fig:example}
    \vspace{-0.2in}
\end{figure}
\fi

Drawing from both cognitive~\cite{jensen1998factor} and AI research~\cite{chollet2019measure, barak2024investigating}, an ideal approach to evaluate fluid intelligence in LLMs involves evaluating their ability to perform abstract inductive reasoning, i.e., induct a general pattern solely from given input-output examples and apply this pattern to deduce the correct outputs for new inputs. %By incorporating patterns that are either counterfactual or highly infrequent, this evaluation could ensure that LLMs must rely on their reasoning capabilities rather than pre-existing knowledge.
The Abstraction and Reasoning Challenge (ARC)~\cite{chollet2019measure}, which requires models to induct transformation rules from input-output grid pairs (as shown in \tref{tab:inductive reasoning examples}), is a benchmark well-suited for this purpose. Due to the abstract nature of ARC tasks, LLMs cannot rely on memorization or external knowledge to solve them. In contrast, many existing inductive reasoning tasks~\cite{honovich2023instruction, yang2024language, qiuphenomenal} fail to prevent the use of memorization shortcuts, making those tasks easier for LLMs to solve, as shown in~\tref{tab:inductive reasoning examples}.

Therefore, the ARC task has become the de facto standard for measuring machine fluid intelligence, sparking a wave of recent studies aimed at improving LLM performance on it~\cite{acquaviva2022communicating, xullms, wang2023hypothesis, wang2024speak}. Despite these efforts, LLMs continue to struggle with the ARC task. For example, even the state-of-the-art GPT-4o with careful prompting can only correctly solve 19\% of tasks (see~\tref{tab:arc performance}), which falls far short of the average human performance of $\sim$75\%~\cite{legris2024h}. These observations lead us to explore a fundamental question: {\em why is the ARC task so challenging for LLMs?}


% To evaluate the fluid intelligence of large language models (LLMs), a suitable approach is to assess their performance on abstract reasoning tasks, where the model must infer a general pattern solely from given input-output examples and apply this pattern to deduce outputs for new inputs~\cite{chollet2019measure, barak2024investigating}. In alignment with the concept of fluid intelligence, an ideal abstract reasoning task should exhibit the following characteristics: 1) it should be based on counterfactual natural language or non-natural language, otherwise LLMs would have strong prior knowledge to solve the task since they are pre-trained on massive natural language data. 2) It should be challenging enough for LLMs to provide informative findings. Therefore, as shown in~\tref{tab:existing abstract tasks}, most existing abstract reasoning benchmarks proposed by~\citet{honovich2022instruction}, \citet{yang2024language}, and \citet{qiu2023phenomenal} are inadequate for evaluating the fluid intelligence of LLMs.

% On the contrary, 
% %LLM performances on a more challenging yet important type of reasoning task, i.e., the inductive reasoning task. % which requires models to summarize abstract concepts from dozens of examples. 
% the Abstraction and Reasoning Challenge (ARC) introduced by~\cite{chollet2019measure} seems to be the most suitable benchmark for this purpose. As illustrated in~\fref{fig:example}, it requires models to predict output(s) for specific input(s) based on a transformation rule inducted from elaborately synthetic input-output grid pairs, thus avoiding LLMs from having extra knowledge to address the tasks. Moreover, %Recently, there has been 
% a surge of recent studies on ARC for LLMs~\cite{acquaviva2022communicating, xu2023llms, wang2023hypothesis, wang2024speak}
% %Despite modest progress made in these studies, unfortunately, 
% have shown that LLMs are still struggle in addressing the ARC task, or even more seriously speaking, LLMs almost fail on this task. For instance, even for the most powerful GPT-4o model
% %\footnote{\scriptsize{\url{https://openai.com/index/gpt-4/}}}
% , it can only correctly answer 18 of 100 instances when carefully prompted with the corresponding input-output pairs (see~\sref{sec:evaluate on original arc}), which is far away from our expectation. These facts motivates us to conduct comprehensive fluid intelligence investigation of LLMs using ARC and attempt to answer a fundamental question: {\em why the ARC task is so challenging for LLMs? }

\begin{table}[tb]
\small
\centering
\setlength{\tabcolsep}{2mm}
\begin{tabular}{lcc}
\toprule
\textbf{Input Format} & \textbf{Output Format} 
%& \textbf{$\text{P-Acc}_{\text{A}}$}$\uparrow$ & \textbf{$\text{P-Acc}_{\text{M}}$}$\uparrow$ 
& \textbf{Correct Num}$\uparrow$ \\
\midrule
Visual & Visual & 0 \\
Visual & Textual & 1\\
Visual + Textual & Visual & 0\\
Visual + Textual & Textual & 16 \\
Textual & Textual & 19 \\

\bottomrule
\end{tabular}
\caption{Evaluation results of GPT-4o on the 100 ARC tasks using different input/output formats.}

\vspace{-0.1in}
\label{tab:different format}
\end{table}


To this end, this paper proposes to investigate the answers to the above question from multiple perspectives.
Our first intuition is inspired by an observation that the transformation rule of each ARC task can be regarded as a composition of atomic operations (e.g., the transformation rule in~\tref{tab:inductive reasoning examples} can be split into two atomic operations).
This motivates us to study the ARC tasks through task decomposition. Hence, we first decompose ARC tasks into atomic operations as transformation rules to construct a benchmark \textbf{A}bstraction and \textbf{R}easoning on \textbf{A}tom \textbf{O}peration  \textbf{C}orpus (\textbf{ARAOC}). However, LLMs perform poorly on some atomic operations on ARAOC, while this task is trivial to humans (\sref{sec: atom operation}), which indicates that their fluid intelligences are limited. Then, from a perspective of task decomposition, we %conduct experiments to 
evaluate the composition ability for LLMs on both ARAOC and ARC benchmarks. Our finding reveals that the limited composition ability for LLMs also contributes to their failures on fluid intelligence evaluation tasks (\sref{sec:factor}). 

% To remedy this issue, these works further propose several ways to enhance LLMs performances on ARC through adding extra information to the input prompt, yet the results are still far from ideal.


% Different from previous works, this paper instead investigates why LLMs easily fail on ARC. Our intuition is inspired by an observation that the transformation rule of each ARC task can be regarded as a composition of several basic operations (e.g., the transformation rule in~\fref{fig:example} can be split into two operations: 1): moving down the cyan subgrid for one step; 2): changing the color of the subgrid from cyan to red), which are defined as \textit{atomic operations}. Therefore, \textbf{LLMs' capability to induct atomic operations from input-output pairs may largely affect their performances on ARC.} To further study this claim, we first propose six atomic operations that could be used to constitute most of the ARC tasks, then use these atomic operations as transformation rules to construct a benchmark \textbf{A}bstraction and \textbf{R}easoning on \textbf{A}tom \textbf{O}peration  \textbf{C}orpus (\textbf{ARAOC}), following the task structure of ARC (Section X). To our surprised, Table Y (LLM on atomic operations) and Table Z (children on atomic operations) indicate that LLMs perform extremely poor on ARAOC tasks relate to some atomic operations like \textit{``Move''} and \textit{``Copy''}, which can be easily solved by kindergarten kids. Therefore, here arises an important research question: \textbf{why LLMs lack the ability to perform basic inductive reasoning tasks built upon atomic operations?} 
%regard each ARC task as a composition of several atomic operations and evaluate the inductive reasoning ability of LLMs on these atomic operations. As described in Figure X, the transformation rule in this example can actually be splited into two operations: 1) copy the orange subgrid; 2) move the copied subgrid towards the red subgrid. 
%Our intuition of evaluating these atomic operations is straightforward: if LLMs can easily induct transformation rules of atomic operations like "move" and "copy", then the poor performance on ARC may due to the complex composition of different atomic operations. Otherwise we need to start from simple operations to evaluate and study the inductive reasoning capability of LLMs.
%To this end, we first propose six different atomic operations that could be used to constitute most of the transformation rules in ARC. For each atomic operation, we follow the input-output structure of ARC to generate numbers of evaluation examples and test different LLMs on these examples (Section X). 
%The experiment results demonstrate that even for some of the basic operations, it is challenging for LLMs to abstract accurate transformation rules from the corresponding training input-output pairs, making it not surprising that they perform worse on ARC. 

% To answer this question, %further investigate the reason for LLMs' struggle towards atomic operations, 
% we conduct experiments from four different perspectives. First, we argue that the challenge may relate to specific features of each atomic operations. For example, the number of steps and the moving direction of the Move atomic operation may largely affect LLMs' performances. Hence, we perform controllable studies on these factors to test their effects. 

Next, we explore the challenge from the abstract representation format perspective. Our another intuition is that LLMs may lack the ability to understand two-dimensional NumPy arrays (matrices) that are commonly used to represent the 2D pixel grid inputs in ARC and ARAOC tasks~\cite{xullms,wang2023hypothesis}, which hinders their performances. We thereby design experiments to investigate whether LLMs understand such matrices-form inputs, and also convert matrices into natural language to see whether it enhances LLM's performances (\sref{sec:matrix}). 
Finally, we investigate the challenge from the modeling perspective. We conduct experiments to analyze the effect of left-to-right autoregressive decoding on model performances and analyze whether LLMs could correctly utilize important information on ARAOC tasks (\sref{sec:model}). Extensive experimental results in~\sref{sec:matrix} and~\sref{sec:model} give us several hints on why LLMs cannot perform ARC and ARAOC tasks well, which further motivating us to design strategies to enhance their corresponding capabilities. 
Overall, the contributions of this paper are summarized as follows:
\begin{enumerate}[noitemsep,nolistsep,leftmargin=*]
    \item This paper makes an initial attempt to study fluid intelligence of LLMs using the ARC tasks and conduct an in-depth study from multiple perspectives.
    \item We propose the ARAOC benchmark that assesses the fluid intelligence over atomic operations from ARC, which is extremely simple to humans yet surprisingly challenging for LLMs.
    % as a decomposed version of ARC, which is extremely simple to humans yet surprisingly challenging for LLMs as a good testbed for inductive reasoning.
    \item We obtain several valuable findings through controllable experiments on ARAOC and ARC, and reveal the challenges of LLMs on internal factors, task composition, input format as well as modeling with left-to-right Transformer. 
\end{enumerate}

% \begin{enumerate}
%     \item This paper makes the first attempt to study the challenge of LLMs on ARC tasks.
%     We propose to investigate the inductive reasoning capability of LLMs using atomic operations extracted from tasks in ARC.
%     six atomic operations from tasks. Following this idea, we first abstract six atomic operations from ARC, then generate many ARC-styles tasks whose transformation rules are based on atomic operations to form a new benchmark \textbf{ARAOC}. \mo{Suggestions: (1) merge 2.4 and 3, drawing a conclusion that LLMs fail on atoms; (2) draw a conclusion from section 4 that this is a challenge of true long sequence understanding; (3) draw a conclusion that autoregressive decoding is unfit here with evidence of wrong salience.}
% \mo{Organize the contributions as the four points (the first one is ARAOC)}We evaluate various LLMs on ARAOC and find that even GPT-4 cannot handle tasks that can be easily solved by kids. To dive deeper into this issue, we conduct analysis from four different perspectives, which leads to several insightful findings.
%     \item Based on our findings, we propose several ways to enhance LLMs' performances on ARAOC. Experimental results illustrate that our proposed methods can indeed improve the inductive reasoning capability of LLMs on ARAOC, and further leading to better performances on ARC.
% \end{enumerate}

