

\section{Breaking ARC into Atomic Operations}
\label{sec: atom operation}
%The poor performance of LLMs on ARC tasks motivates us to explore the reasons behind this issue. 
As mentioned in~\sref{intro}, the transformation rule of each ARC task can be decomposed into several atomic operations (e.g., the rule in~\tref{tab:inductive reasoning examples} can be broken into moving the subgrid and changing its color), which motivates us to analyze the challenges of LLMs from a task decomposition perspective. To this end, we first decompose the ARC tasks into simplified tasks and form the ARAOC benchmark that consists of various atomic operations, then use ARAOC to evaluate the fluid intelligence of LLMs.


\subsection{ARAOC Benchmark}

%\lemao{Remove the finetuning results from Table 3 and combine them into Table 6 to show performance gap on ARAOC and ARC, which demonstrates the composition challenge in sec 4.}

\label{sec:araoc benchmark}
To evaluate LLMs' fluid intelligence with atomic operations, we first manually go through all the tasks in ARC's training and evaluation sets, then conclude six atomic operations that can compose the transformation rules for most of the ARC tasks. Check~\tref{tab:atom operations} for atomic operations' descriptions. 

For each atomic operation, we use it as the transformation rule to build 100 tasks with 3 input-output training pairs and 1 testing pair, which follows the standard ARC setting (check Appendix~\ref{appendix:araoc} for the crafting details). This finally leads to a benchmark named \textbf{A}bstraction and \textbf{R}easoning on \textbf{A}tom \textbf{O}peration  \textbf{C}orpus (\textbf{ARAOC}) with 600 distinct tasks. %Specifically, for each task in ARAOC, we have three input-output grid pairs as the few-shot examples, and a single input grid that needs LLMs to infer its corresponding output grid. 
We evaluate all LLMs in~\sref{evaluated llms} on ARAOC and additionally include Mistral-8*7B and Llama-3-70B to study the impact of model size.

%list the results in~\tref{tab:araoc results}.



\iffalse
\begin{table}[tb]
\small
\centering
\setlength{\tabcolsep}{1mm}
\begin{tabular}{llcccc}
\toprule
& \textbf{COMB}& \textbf{Mistral}& \textbf{Llama-3} & \textbf{GPT-3.5}& \textbf{GPT-4o} \\
\midrule[0.5pt]
\multirow{6}{*}{\textbf{Move}}&Up 1 &0.00&12.00&0.00&26.00\\
&Up 2 &2.00&6.00&2.00&12.00\\
&Up 3 &4.00&4.00&0.00&8.00\\
\cmidrule{2-6}
&Up-right 1 &0.00&2.00&0.00&10.00\\
&Up-right 2 &0.00&0.00&0.00&0.00\\
&Up-right 3 &2.00&2.00&0.00&4.00\\
\midrule[0.5pt]
\multirow{6}{*}{\textbf{Copy}}&Up 1 &4.00&16.00&6.00&40.00\\
&Up 2 &8.00&10.00&6.00&26.00\\
&Up 3 &10.00&12.00&8.00&16.00\\
\cmidrule{2-6}
&Up-right 1 &2.00&8.00&0.00&16.00\\
&Up-right 2 &4.00&4.00&4.00&4.00\\
&Up-right 3 &2.00&4.00&0.00&2.00\\
\bottomrule
\end{tabular}
\caption{Further analysis results regarding Move and Copy. \textbf{COMB} is the abbreviation of combination. We only list Acc scores (in percentage) here for simplicity, and other metric scores are listed in Table Y.}
\label{tab:controllable}
\end{table}
\fi



\paragraph{Results.}
\label{sec:araoc results}
As shown in~\tref{tab:araoc results}, GPT-4o largely outperforms other LLMs across almost all tasks in the ARAOC benchmark, achieving nearly 100\% Acc scores on the Change Color and Fill Internal tasks, demonstrating its high fluid intelligence. Additionally, %GPT-3.5 and Llama-3 produce comparable results, 
Llama-3/Llama-3-70B outperforms Mistral/Mistral-8*7B, suggesting that pre-training %on higher-quality data 
with a greater number of parameters can enhance the fluid intelligence of LLMs. Also, similar to~\tref{tab:arc performance}, larger LLMs continue to outperform smaller ones across tasks, further illustrating the above point. However, all LLMs still encounter substantial difficulties with tasks related to Move, Copy, Mirror, and Scale, failing to predict the correct shapes of output grids for the latter two atomic operations on more than \textasciitilde50 tasks.



\subsection{Further Analysis}
%Moreover, the results in~\tref{tab:araoc results} %and~\tref{tab:composition} 
%show that LLMs' Acc scores on Move and Copy tasks in ARAOC are still less than 20\%, even they have been trained on similar data. 
\paragraph{Analysis I: Internal Factors.}
As concluded from~\sref{sec:araoc results}, all the LLMs exhibit poor performances on Move and Copy tasks in ARAOC. To analyze whether this is caused by the internal complexity of Move and Copy, we investigate factors that may affect the complexity of Move and Copy, and their influences on LLMs' performances. Given that Copy can actually be viewed as first copying the original subgrid, then moving the copied subgrid several steps in specific directions, we intuitively consider two factors in this study: 1) the number of steps the subgrid/copied subgrid moves; 2) the direction in which the subgrid/copied subgrid moves.

\paragraph{Setup.}
Specifically, we choose {Up, Up-right} and {1 step, 2 steps, 3 steps} as our candidate moving directions and steps, respectively. We then generate 50 input grids for each atomic operation, ensuring that these grids can be transformed into valid output grids based on any combination of the two candidate sets (e.g., Up for 1 step). For each input grid, we create 6 tasks corresponding to all 6 combinations of the candidate sets, and evaluate the closed-source (GPT-4o) and open-source (Llama-3) LLMs, which performed better in~\tref{tab:araoc results}, as representatives on these tasks.


\paragraph{Results.}
Results are shown in~\tref{tab:controllable}. We observe that 
%Mistral and GPT-3.5 can hardly finish the given tasks, and even scoring 0 under ``Up'', ``Up-right 1'' and ``Up-right 2'', rendering their results not indicative. 
%For GPT-4 and Llama-3, we notice that 
for both Move and Copy, a larger number of steps would lead to lower Acc scores. This could be because as the number of steps increases, LLMs need to focus on a longer context to induce the atomic operation, which leads to more challenges. Additionally, LLMs appear to be more adept with subgrids that move in a straight direction, as their performance on "Up"-related tasks is significantly higher than on "Up-right"-related tasks. Even when considering "Up-right 1" as a two-step move (one step "Up" and one step "Right"), LLMs' Acc scores on "Up-right 1" are still lower than those on "Up 2", further supporting our previous assertion. %Overall, we conclude that the inductive reasoning ability of LLMs can be influenced by various intrinsic factors related to different operations, which control the complexity of such atomic operations. %Future work should pay more attention to these factors when evaluating LLMs' inductive reasoning capabilities with atomic operations.

\begin{table}[tb]
\small
\centering
\setlength{\tabcolsep}{3mm}
\begin{tabular}{llcc}
\toprule
& \textbf{COMB}& \textbf{Llama-3} & \textbf{GPT-4o} \\
\midrule[0.5pt]
\multirow{6}{*}{\textbf{Move}}&Up 1 &12.00&24.00\\
&Up 2 &6.00&26.00\\
&Up 3 &4.00&17.00\\
\cmidrule{2-4}
&Up-right 1 &2.00&9.00\\
&Up-right 2 &0.00&2.00\\
&Up-right 3 &2.00&1.00\\
\midrule[0.5pt]
\multirow{6}{*}{\textbf{Copy}}&Up 1 &16.00&46.00\\
&Up 2 &10.00&38.00\\
&Up 3 &12.00&27.00\\
\cmidrule{2-4}
&Up-right 1 &8.00&11.00\\
&Up-right 2 &4.00&6.00\\
&Up-right 3 &4.00&10.00\\
\bottomrule
\end{tabular}
\caption{Analysis I's Acc scores. \textbf{COMB} refers to combination. See~\tref{tab:controllable_plus} for the Not M\% scores.}
\vspace{-0.2in}
\label{tab:controllable}
\end{table}


\paragraph{Analysis II: Effect on Input Size.}
%Another reason LLMs struggle with ARAOC may relate to the size of input grids, where larger input grids should bring more difficult tasks and vice versa. To test this hypothesis, 
We %evaluate LLMs on Move and Copy tasks for the effect on input size via 
evaluate LLMs on 100 Move and Copy tasks with smaller sizes (crafting details are included in Appendix~\ref{appendix:small size}). 
%and we randomly initialized the 100 tasks for each atomic operation from a range that is half of the original range listed in Appendix~\ref{appendix:araoc}. This results in 100 new tasks for both atomic operations, with an average size of $4.96 \times 4.89$ and $4.81 \times 4.80$, respectively. For comparison, the original average sizes are $10.07 \times 10.16$ and $9.72 \times 9.62$, respectively. 
The evaluation results on these tasks are listed in Table \ref{tab:large matrix}, where LLMs perform significantly better on Move and Copy tasks with smaller input sizes. This indicates that the size of matrix-format input does affect LLMs' understanding of ARAOC tasks and thus influences their performance on ARAOC. %See Appendix X for prompts used in this section.
%\mo{Conclusion: ARC is a natural long sequence understanding task (Combined with Conclusion 2 and 4.3)}

\textbf{Overall, this section shows that the performances of LLMs is largely affected by the superficial properties of the input grids, 
and LLMs fail to grasp the underlying concept of the operations. This result further suggests that LLMs rely more on pattern recognition and memorization, akin to crystallized intelligence, rather than reasoning through abstract, novel relationships (fluid intelligence).}
In the following, we provide further insights into LLMs' deficiencies through the lens of three challenges on ARC and ARAOC: task composition (\sref{sec:factor}), LLMs' understanding of task inputs (\sref{sec:matrix}), and LLMs' modeling strategies (\sref{sec:model}).

% Overall, we conclude that the inductive reasoning capabilities of LLMs are still far from ideal, as they even fail to abstract simple transformation rules, such as atomic operations, from the given examples.
% %, let alone ARC tasks that require composing different atomic operations to form transformation rules. 

% Therefore, in the following sections, we analyze three challenges that cause LLMs to fail on ARC and ARAOC tasks: task composition (\sref{sec:factor}), LLMs' understanding of task inputs (\sref{sec:matrix}), and LLMs' modeling of ARAOC tasks (\sref{sec:model}).



%\paragraph{Input Size.}

%As concluded from Section \ref{sec:araoc results}, all the LLMs exhibit poor performances on ARAOC tasks involving Move and Copy. To analyze whether this is caused by the internal complexity of Move and Copy, we investigate factors that may affect the complexity of these two atomic operations, and their influences on LLMs' performances. Given that Copy can actually be viewed as first copying the original subgrid, then moving the copied subgrid several steps in specific directions, we intuitively consider two factors in this study: 

%1. The number of steps the subgrid/copied subgrid moves.
%2. The direction in which the subgrid/copied subgrid moves.

%Specifically, we select \{Up, Up-right\} and \{1 step, 2 ste%ps, 3 steps\} as our candidate moving directions and steps, respectively. Subsequently, we generate 50 input grids for each atomic operation, ensuring that these input grids can be transformed into valid output grids following any combination of the two candidate sets (e.g., Up for 1 step). For each input grid, we then craft 6 tasks with all the 6 combinations of the two candidate sets, and evaluate all the LLMs on these tasks.




%which further explains LLMs' poor performances on ARC.

%Although these atomic operations are decomposed from ARC tasks and should make ARAOC tasks less challenging, there exists a possibility that tasks in ARAOC are still difficult to answer, which leads to LLMs' poor performances. Therefore, we invite three kindergarten kids to finish ARAOC tasks for comparison. Specifically, we select 100 tasks from Move and Copy (50 for each) that GPT-4 wrongly answers and ask the kids to finish these tasks. To our surprise, these kids obtain an average Acc of xxx and xxx on Move and Copy, indicating that tasks in ARAOC are actually not challenging for kids. Considering that GPT-4 only obtains 14.00 and 13.00 Acc scores on Move and Copy in ARAOC, \textbf{a thorough analysis on why LLMs cannot handle inductive reasoning tasks well is necessary}.


