%\section{Introducing atomic operations}
\iffalse
\begin{figure}
    \centering
    \includegraphics[width=0.45\textwidth]{figures/figure2.pdf}
    \caption{An example of the input format conversion. We follow~\citet{wang2023hypothesis} to convert 2D input grids into matrices (represented by NumPy arrays), where each pixel is transformed into a number denoting a specific color (e.g., ``8'' denotes blue and ``2'' denotes red).\lemao{Remove this Figure.}} 
    \label{fig:example matrix input}
\end{figure}
\fi



\section{Evaluating Fluid Intelligence on ARC}
\label{evaluate llm on arc}

\subsection{ARC Benchmark}
\label{sec:arc setting}
%\paragraph{ARC Benchmark.}
We start by evaluating the fluid intelligences of existing LLMs using the ARC benchmark, which comprises 400 training and 400 evaluation tasks. As shown in~\tref{tab:inductive reasoning examples}, each ARC task includes several 2D input-output grid pairs that define a unique transformation rule, with each grid ranging from $1 \times 1$ to $30 \times 30$ pixels, and each pixel being one of ten colors (see~\fref{fig:original prompt} for the names of the ten colors). An LLM must induct the transformation rule from the given input-output grid pairs and use it to predict the output grid for a testing input grid. Due to the high cost of closed-source LLMs, we follow~\citet{wang2023hypothesis} and use a subset of 100 training tasks in ARC for evaluation~\footnote{\scriptsize{Additionally, we evaluated GPT-4o on all 400 training tasks, where it achieved an Acc score of 18.50. This result aligns with the score reported in~\tref{tab:arc performance}, further supporting the representativeness of the subset.
}}.  



\begin{table}[tb]
\small
\centering
\setlength{\tabcolsep}{4mm}
\begin{tabular}{lcc}
\toprule
\textbf{LLM} & \textbf{Acc}$\uparrow$ 
%& \textbf{$\text{P-Acc}_{\text{A}}$}$\uparrow$ & \textbf{$\text{P-Acc}_{\text{M}}$}$\uparrow$ 
& \textbf{Not M\%}$\downarrow$ \\
\midrule
Mistral & 2.00 
%& 32.59 & 62.67 
& 48.00 \\
Llama-3 & 5.00 
%& 49.56 & 73.98 
& 33.00 \\
\midrule
$\text{Mistral-FT}_{\text{ARC}}$ & 3.00 
%&44.74 &67.79 
&34.00 \\
$\text{Llama-3-FT}_{\text{ARC}}$ & 9.00
%& 54.20& 76.34
& 29.00\\
\midrule
$\text{Mistral-8*7B}$ &3.00
%&44.74 &67.79 
& 27.00\\
$\text{Llama-3-70B}$ &9.00
%& 54.20& 76.34
& 24.00\\
\midrule
GPT-3.5 & 6.00 
%& 46.38 & 71.35
& 35.00 \\
%GPT-4 & \textbf{17.00} 
%& \textbf{69.52} & \textbf{82.76} 
%& \textbf{16.00} \\
GPT-4o & \textbf{19.00} &\textbf{11.00} \\
\midrule
GPT-o1* & 18.00 & 10.00 \\
\bottomrule
\end{tabular}
\caption{Evaluation results on the 100 ARC tasks, where Acc %$\text{P-Acc}_{\text{A}}$, and $\text{P-Acc}_{\text{M}}$ 
is represented as percentages. $\text{FT}_{\text{ARC}}$ denotes fine-tuning on ARC tasks. The best results in each column are \textbf{boldfaced}. *GPT-o1 is evaluated on a partial subset, where GPT-4o obtains \emph{16.00} and \emph{10.00} for both scores.
}
\vspace{-0.2in}
\label{tab:arc performance}
\end{table}

\begin{table*}[tb]
  \renewcommand\arraystretch{1.1}
  \centering
  \setlength{\tabcolsep}{2mm}
  \small
  \begin{tabular}{p{2cm}p{7cm}p{5cm}}
    \toprule[1pt]
   
     \textbf{Name} & \textbf{Description/Transformation Rule} & \textbf{Example} \\
     \midrule[0.5pt]
    \textbf{Move} & Move a subgrid in the input grid for several steps towards a single direction in one of \{Up, Down, Left, Right, Up-left, Up-right, Down-left, Down-right\} to form the output grid. Note that the moved subgrid could not surpass the boundary of the input grid. & \begin{center}\vspace{-1mm}\includegraphics[width=5cm]{figures/move.pdf}\vspace{-3mm}\end{center} \\
    \midrule[0.5pt]
   \textbf{Change Color} & Change the color of a subgrid in the input grid to another color other than black to form the output grid. & \vspace{-3.5mm} \begin{center}\includegraphics[width=5cm]{figures/change_color.pdf}\end{center}\vspace{-5mm} \\
   \midrule[0.5pt]
   \textbf{Copy} & Copy a subgrid in the input grid and move it with Move to form the output grid, while making sure that the copied subgrid could neither surpass the boundary of the input grid, nor overlap with the original subgrid. & \vspace{-3.5mm} \begin{center}\includegraphics[width=5cm]{figures/copy.pdf}\end{center} \vspace{-5.5mm} \\
   \midrule[0.5pt]
   \textbf{Mirror} & Mirror the input grid towards a single direction in one of \{Up, Down, Left, Right\} to form the output grid. & \vspace{-4mm} \begin{center}\includegraphics[width=5cm]{figures/mirror.pdf}\end{center} \vspace{-5.5mm}\\
   \midrule[0.5pt]
   \textbf{Fill Internal} & The input grid has a closed subgrid whose internal is black. Fill the internal black part of this subgrid with another color to form the output grid. &\vspace{-3mm} \begin{center}\includegraphics[width=5cm]{figures/fill_internal.pdf}\end{center} \vspace{-6.5mm} \\
   \midrule[0.5pt]
   \textbf{Scale} & Some pixels in the input grid are colored with a specific color. Let the number of rows and columns of the input grid be \(a\) and \(b\), respectively. First, the input grid will be copied \(a \times b\) times. These copies will then be arranged in an output grid with \(a \times a\) rows and \(b \times b\) columns, placed from top to bottom and left to right. Finally, if the position \((i, j)\) in the input grid is black, the \(i \times j\)-th copy in the output grid will be converted to black. & \vspace{-4.5mm} \begin{center}\includegraphics[width=5cm]{figures/scale.pdf}\end{center} \vspace{-6mm} \\
    \bottomrule[1pt]
  \end{tabular}
  \caption{Descriptions and examples of the six atomic operations we use in this paper.}
  \vspace{-0.1in}
  \label{tab:atom operations}
\end{table*}

\begin{table*}[tb]
\renewcommand\arraystretch{1.1}
\centering
\setlength{\tabcolsep}{0.8mm}
\small
\begin{tabular}{lcc|cc|cc|cc|cc|cc}
\toprule[1pt]
\multirow{2}*{LLM} & \multicolumn{2}{c}{\textbf{Move}} & \multicolumn{2}{c}{\textbf{Change Color}} & \multicolumn{2}{c}{\textbf{Copy}} & \multicolumn{2}{c}{\textbf{Mirror}} & \multicolumn{2}{c}{\textbf{Fill Internal}} & \multicolumn{2}{c}{\textbf{Scale}} \\
 & Acc$\uparrow$ & Not M\%$\downarrow$ & Acc$\uparrow$ & Not M\%$\downarrow$ & Acc$\uparrow$ & Not M\%$\downarrow$ & Acc$\uparrow$ & Not M\%$\downarrow$ & Acc$\uparrow$ & Not M\%$\downarrow$ & Acc$\uparrow$ & Not M\%$\downarrow$ \\
\midrule[0.5pt]
Mistral & 2.00 & 36.00 & 15.00 & 30.00 & 2.00 & 43.00 & 1.00 & 97.00 & 9.00 & 31.00 & 0.00 & 98.00 \\
Llama-3 & 1.00 & 19.00 & 39.00 & 17.00 & 4.00 & 13.00 & 2.00 & 96.00 & 63.00 & 6.00 & 1.00 & 89.00 \\
\midrule
Mistral-8*7B &2.00&10.00&57.00&5.00&2.00&7.00&5.00&95.00&50.00&3.00&\textbf{3.00}&81.00 \\
Llama-3-70B &8.00&15.00&92.00&1.00&4.00&11.00&7.00&75.00&64.00&3.00&\textbf{3.00}&80.00 \\
\midrule
GPT-3.5 & 4.00 & 27.00 & 48.00 & 13.00 & 4.00 & 29.00 & 6.00 & 89.00 & 58.00 & 12.00 & 1.00 & 80.00 \\
%GPT-4 & \textbf{14.00} & \textbf{3.00} & \textbf{97.00} & \textbf{0.00} & \textbf{13.00} & \textbf{6.00} & \textbf{14.00} & \textbf{52.00} & \textbf{100.00} & \textbf{0.00} & \textbf{3.00} & \textbf{70.00} \\
GPT-4o &\textbf{13.00}&\textbf{0.00}&\textbf{98.00}&\textbf{0.00}&\textbf{15.00}&\textbf{0.00}&\textbf{12.00}&\textbf{48.00}&\textbf{96.00}&\textbf{0.00}&2.00&\textbf{72.00} \\
%\midrule
%$\text{Mistral-FT}_{\text{Atom}}$ & 12.00 & 11.00 & 100.00 & 0.00 & 20.00 & 6.00 & 26.00 & 52.00 & 97.00 & 2.00 & 89.00 & 0.00 \\
%$\text{Llama-3-FT}_{\text{Atom}}$ & 13.00 & 9.00 & 98.00 & 1.00 & 14.00 & 8.00 & 27.00 & 54.00 & 97.00 & 1.00 & 78.00 & 99.08 \\
\bottomrule[1pt]
\end{tabular}
\caption{Results on ARAOC. %$\text{FT}_{\text{Atom}}$ denotes fine-tuning on atomic operation data. 
Acc is shown in percentage. The best results under each column are \textbf{boldfaced}.}
\vspace{-0.2in}
\label{tab:araoc results}
\end{table*}

\subsection{Comparing Text- and Visual-Based LLMs}
Since ARC tasks are presented in a 2D visual grid format, we can employ both visual-based LLMs (\textbf{Visual}) and text-based LLMs through
%Since LLMs cannot process visual inputs directly, 
converting input-output grids into matrices represented by NumPy arrays following existing works~\cite{xullms,wang2023hypothesis} (\textbf{Textual}). Therefore, we first investigate the performances of these two types of LLMs on ARC by prompting GPT-4o with 5 different input-output formats (check Appendix~\ref{appendix:prompts} for the prompts). As shown in~\tref{tab:different format}, prompting GPT-4o solely with textual input-output format yielding the best performance on the 100 ARC tasks. On the other hand, it seems extremely challenging for visual-based LLMs to finish ARC tasks, where we provide detailed analysis in Appendix~\ref{appendix:visual analysis}. \textbf{Based on the results, we apply the textual only input-output format and refer \textit{``LLMs''} to text-based LLMs in the rest of the paper.}
%, as illustrated in~\fref{fig:example matrix input}.

\subsection{Comparing Different LLMs on ARC}
\label{sec:evaluate on original arc}
\paragraph{Evaluated LLMs.}
\label{evaluated llms}
We evaluate both closed-source and open-source LLMs. For closed-source models, we use GPT-4o and GPT-3.5. For open-source LLMs, we select Mistral (\texttt{Mistral-7B-Instruct-v0.2})~\cite{jiang2023mistral} and Llama-3 (\texttt{Llama-3-8B-Instruct})~\cite{llama3}. Additionally, we include the recently released GPT-o1 (\texttt{o1-preview}) model, known for its strong reasoning abilities, for comparison. Due to the slow inference speed and limited quota of GPT-o1, we evaluate it on a subset of 50 tasks and report the performance of both GPT-4o and GPT-o1 on this subset. Check Appendix~\ref{appendix:inference config} for details on the inference configurations.

%For all the models, we maintain their official prompt templates unchanged and the inference configurations are listed in Appendix~\ref{appendix:inference config}.

\paragraph{Evaluation Metrics.}
The primary metric we use to evaluate the performance of LLMs is the accuracy of their predictions (Acc). Additionally, since we observe that the shape of the LLMs' predicted output grids does not always align with the ground truth, we report the percentage of mismatched predictions for each LLM (Not M\%), where lower scores indicate better performance.



\iffalse
\begin{table*}[tb]
\renewcommand\arraystretch{1.1}
\centering
\setlength{\tabcolsep}{0.2mm}
\small
\begin{tabular}{l|cccc|cccc|cccc}
\toprule[1pt]
\multirow{2}*{LLM} & \multicolumn{4}{c}{\textbf{Move}} & \multicolumn{4}{c}{\textbf{Change Color}} & \multicolumn{4}{c}{\textbf{Copy}} \\
 & \textbf{Acc}$\uparrow$ & \textbf{$\text{P-Acc}_{\text{A}}$}$\uparrow$ & \textbf{$\text{P-Acc}_{\text{M}}$}$\uparrow$ & \textbf{Not M\%}$\downarrow$ & \textbf{Acc}$\uparrow$ & \textbf{$\text{P-Acc}_{\text{A}}$}$\uparrow$ & \textbf{$\text{P-Acc}_{\text{M}}$}$\uparrow$ & \textbf{Not M\%}$\downarrow$ & \textbf{Acc}$\uparrow$ & \textbf{$\text{P-Acc}_{\text{A}}$}$\uparrow$ & \textbf{$\text{P-Acc}_{\text{M}}$}$\uparrow$ & \textbf{Not M\%}$\downarrow$ \\
\midrule[0.5pt]
Mistral & 2.00 & 50.23 & 78.48 & 36.00 & 15.00 & 59.33 & 84.76 & 30.00 & 2.00 & 48.09 & 84.37 & 43.00 \\
Llama-3 & 1.00 & 65.91 & 81.36 & 19.00 & 39.00 & 73.85 & 88.98 & 17.00 & 4.00 & 78.06 & \underline{\textbf{89.72}} & 13.00 \\
\midrule
GPT-3.5 & 4.00 & 60.88 & 83.39 & 27.00 & 48.00 & 80.64 & 92.68 & 13.00 & 4.00 & 61.75 & 86.97 & 29.00 \\
GPT4o & \underline{\textbf{14.00}} & \underline{\textbf{85.93}} & \underline{\textbf{88.59}} & \underline{\textbf{3.00}} & \underline{97.00} & \underline{99.67} & \underline{99.67} & \underline{\textbf{0.00}} & \underline{13.00} & \underline{\textbf{84.00}} & 89.36 & \underline{\textbf{6.00}}\\
\midrule
$\text{Mistral-FT}_{\text{Atom}}$&12.00&78.31&87.99&11.00&\textbf{100.00}&\textbf{100.00}&\textbf{100.00}&\textbf{0.00}&\textbf{20.00}&83.96&89.32&\textbf{6.00} \\
$\text{Llama-3-FT}_{\text{Atom}}$ &13.00&79.69&87.57&9.00&98.00&98.99&99.99&1.00&14.00&82.54&89.71&8.00 \\
\midrule[1pt]
\multirow{2}*{LLM} & \multicolumn{4}{c}{\textbf{Mirror}} & \multicolumn{4}{c}{\textbf{Fill Internal}} & \multicolumn{4}{c}{\textbf{Scale}} \\
 & \textbf{Acc}$\uparrow$ & \textbf{$\text{P-Acc}_{\text{A}}$}$\uparrow$ & \textbf{$\text{P-Acc}_{\text{M}}$}$\uparrow$ & \textbf{Not M\%}$\downarrow$ & \textbf{Acc}$\uparrow$ & \textbf{$\text{P-Acc}_{\text{A}}$}$\uparrow$ & \textbf{$\text{P-Acc}_{\text{M}}$}$\uparrow$ & \textbf{Not M\%}$\downarrow$ & \textbf{Acc}$\uparrow$ & \textbf{$\text{P-Acc}_{\text{A}}$}$\uparrow$ & \textbf{$\text{P-Acc}_{\text{M}}$}$\uparrow$ & \textbf{Not M\%}$\downarrow$ \\
\midrule[0.5pt]
Mistral & 1.00 & 2.29 & 76.39 & 97.00 & 9.00 & 58.96 & 85.45 & 31.00 & 0.00 & 1.32 & 66.05 & 98.00 \\
Llama-3 & 2.00 & 2.58 & 64.58 & 96.00 & 63.00 & 89.12 & 94.81 & 6.00 & 1.00 & 8.38 & 76.15 & 89.00 \\
\midrule
GPT-3.5 & 6.00 & 10.48 & \underline{95.30} & 89.00 & 58.00 & 83.44 & 94.82 & 12.00 & 1.00 & 14.85 & 74.26 & 80.00 \\
GPT4o & \underline{14.00} & \underline{42.16} & 87.84 & \underline{\textbf{52.00}} & \underline{\textbf{100.00}} & \underline{\textbf{100.00}} & \underline{\textbf{100.00}} & \underline{\textbf{0.00}} & \underline{3.00} & \underline{24.28} &\underline{80.93}& \underline{70.00} \\
\midrule
$\text{Mistral-FT}_{\text{Atom}}$&26.00&\textbf{44.64}&92.99&\textbf{52.00}&97.00&97.91&99.91&2.00&\textbf{89.00}&\textbf{99.51}&\textbf{99.51}&\textbf{0.00} \\
$\text{Llama-3-FT}_{\text{Atom}}$ &\textbf{27.00}&43.97&\textbf{95.59}&54.00&97.00&98.96&99.96&1.00&78.00&95.11&4.00&99.08 \\
\bottomrule[1pt]
\end{tabular}
\caption{Evaluation results on ARAOC on atomic operation-level. $\text{FT}_{\text{Atom}}$ denotes fine-tuning on atomic operation data. Acc
%$\text{P-Acc}_{\text{A}}$ and $\text{P-Acc}_{\text{M}}$ are 
is shown in percentage. 
The best results under each column are \textbf{boldfaced}, and the best results among not fine-tuned LLMs are \underline{underlined}.
}
\label{tab:araoc results}
\end{table*}
\fi



\paragraph{Results.}
The evaluation results are presented in~\tref{tab:arc performance}. We observe that, although GPT-4o significantly outperforms other LLMs, its performance remains far from ideal. Moreover, GPT-o1 shows almost no improvement over GPT-4o on the evaluated subset. Hence, due to its low speed and limited quota, we do not include GPT-o1 in the following experiments. 

For the other LLMs, handling ARC tasks seems extremely challenging, with more than one-third of their predictions failing to match the shape of the corresponding ground truth. To examine the impact of model size on ARC performance, we further experiment with Mistral-8*7B (\texttt{Mixtral-8x7B-Instruct-v0.1}) and Llama-3-70B (\texttt{Llama-3-70B-Instruct}). As shown in~\tref{tab:arc performance}, larger LLMs consistently outperform smaller ones across all tasks, indicating that models with more parameters exhibit stronger fluid intelligence on ARC tasks. However, their overall performance remains poor. We hypothesize that this poor performance is due to the LLMs' unfamiliarity with the style of these tasks. Consequently, we further fine-tuned Mistral and Llama-3 on a separate ARC evaluation set that do not overlap with the 100 ARC tasks used in~\tref{tab:arc performance} using LoRA~\cite{hu2021lora}, and evaluated them on the 100 ARC tasks (check fine-tuning details in Appendix~\ref{appendix:lora}). However, as shown in~\tref{tab:arc performance}, even though fine-tuning on ARC tasks improves the LLMs' performance, the results remain suboptimal, with Acc scores below 10\%.

 

In summary, these experiments demonstrate the significant challenge LLMs face in successfully completing ARC tasks, motivating us to further investigate the underlying reasons for this difficulty.


%\lemao{Please insert finetuning experiments into this subsection to further highlight the challenge of ARC tasks for LLMs.}

