% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[preprint]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{url}            % simple URL typesetting
%\usepackage[colorlinks=true]{hyperref} % hyperlinks
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{multirow}
\usepackage{graphicx}


\newcommand{\fref}[1]{Figure~\ref{#1}}
\newcommand{\tref}[1]{Table~\ref{#1}}
\newcommand{\sref}[1]{\S\ref{#1}}
\newcommand{\jw}[1]{\textcolor{orange}{\bf\small [#1 --Junjie]}}

\usepackage{microtype}
\usepackage{graphicx}
\usepackage{amsmath}
%\usepackage{hyperref}
\usepackage{bbm}
\usepackage{tcolorbox}
\newcommand\BibTeX{B\textsc{ib}\TeX}

\newtheorem{definition}{Definition}
\usepackage{enumitem}
\usepackage{CJKutf8}

\usepackage[]{todonotes}
\newcommand{\fixme}[2][]{{\todo[color=yellow,size=\scriptsize,fancyline,caption={},#1]{#2}}}
\newcommand{\note}[4][]{{\todo[author=#2,color=#3,size=\scriptsize,fancyline,caption={},#1]{#4}}}
\newcommand{\mo}[2][]{{\note[#1]{MO}{blue!20}{#2}}}
\newcommand{\Mo}[2][]{\mo[inline,#1]{#2}\noindent}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

%\title{Evaluate LLM Inductive Reasoning}
\title{On the Fluid Intelligence Evaluation for LLMs}
\title{Understanding LLMs' Fluid Intelligence Deficiency: \\ An Analysis of the ARC Task}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\newcommand{\authorsep}{\quad}
\newcommand{\footnotemarksep}{\enspace\space\!\!}

\author{
Junjie Wu$^1$\authorsep
Mo Yu$^2$\thanks{Co-corresponding authors.}\authorsep
Lemao Liu$^2$\authorsep
Dit-Yan Yeung$^1$\footnotemark[1]\authorsep
Jie Zhou$^2$\authorsep
\\
\textsuperscript{1}Hong Kong University of Science and Technology\\
\textsuperscript{2}WeChat AI, Tencent\\
\texttt{junjie.wu@connect.ust.hk} \quad \texttt{moyumyu@global.tencent.com} \\ \texttt{\{redmondliu, withtomzhou\}@tencent.com} \quad \texttt{dyyeung@ust.hk}
}

%\author{
%  \textbf{First Author\textsuperscript{1}},
%  \textbf{Second Author\textsuperscript{1,2}},
%  \textbf{Third T. Author\textsuperscript{1}},
%  \textbf{Fourth Author\textsuperscript{1}},
%\\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
%\\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
%\\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
%\\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
%\\
%\\
%  \textsuperscript{1}Affiliation 1,
%  \textsuperscript{2}Affiliation 2,
%  \textsuperscript{3}Affiliation 3,
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
%\\
%  \small{
%    \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
%  }
%}

\begin{document}
\maketitle
\begin{abstract}
While LLMs have exhibited strong performance on various NLP tasks, it is noteworthy that most of these tasks rely on utilizing the vast amount of knowledge encoded in LLMs' parameters, rather than solving new problems without prior knowledge. In cognitive research, the latter ability is referred to as fluid intelligence, which is considered to be critical for assessing human intelligence. Recent research on fluid intelligence assessments has highlighted significant deficiencies in LLMs' abilities. In this paper, we analyze the challenges LLMs face in demonstrating fluid intelligence through controlled experiments, using the most representative ARC task as an example.
Our study revealed three major limitations in existing LLMs: limited ability for skill composition, unfamiliarity with abstract input formats, and the intrinsic deficiency of left-to-right decoding.
Our data and code can be found in \url{https://wujunjie1998.github.io/araoc-benchmark.github.io/}.

% The ARC task is widely recognized as an assessment of machines' inductive reasoning capabilities, on which all existing approaches, including powerful large language models (LLMs), have failed.
% We aim to analyze the challenge that ARC presents to current LLMs from multiple perspectives through controlled experiments. 
% First, according to task decomposition, we decompose ARC tasks into several atomic operations, which leads to a simplified version of ARC for evaluation. Then we evaluate the composition ability for LLMs from the task composition perspective. %We define inductive reasoning examples that reflect atomic operations over input grids.
% Moreover, we investigate the challenges from the abstract input-format perspective as well as modeling perspective. By thorough empirical analyses, we obtain the following key findings: 1)
% on the simplified ARC tasks, LLMs surprisingly fail in most cases. 2) Their composition ability on atomic operations is limited. 3) The abstract input-format is a crucial challenge for LLMs on ARC tasks. 4) The dominant left-to-right paradigm of LLMs have an intrinsic deficiency in achieving advanced inductive reasoning. Our data and code will be publicly released, and the data is also attached in the submission.
% % A comprehensive analysis suggests that the dominant left-to-right paradigm of LLMs may have an intrinsic deficiency in achieving advanced inductive reasoning. First, inductive reasoning is inherently a long-sequence understanding task, as comprehending each example in the input requires mining its connections to others in a back-and-forth manner. Second, a solution to inductive reasoning typically follows a hierarchical structure with planning, resulting in a mismatch with autoregressive decoding, which often emphasizes irrelevant parts (salience) of the inputs.

\end{abstract}

\newcommand{\lemao}[1]{\textcolor{red}{\textbf{#1 --Lemao}}}

\input{sections/introduction.tex}
\input{sections/results_on_arc.tex}
\input{sections/Investigate_on_ARAOC.tex}
\input{sections/factor.tex}
\input{sections/input.tex}
\input{sections/model.tex}
\input{sections/related_work.tex}
%\input{latex/sections/enhance performance.tex}


\section{Conclusion}
This paper presents an in-depth study of LLMs' fluid intelligence deficiencies using the ARC tasks, with a series of controlled experiments from multiple perspectives. %and uncovering several key findings. 
Through task decomposition, we introduce the atomic ARAOC benchmark, revealing that LLMs struggle with atomic operations despite their simplicity for humans. We further demonstrate that LLMs' task composition abilities are limited, as improvements on the decomposed ARAOC tasks via fine-tuning do not lead to better performance on ARC tasks. Additionally, our study shows that LLMs' difficulty in encoding abstract input formats is a major obstacle in addressing ARC tasks. Lastly, it shows an intrinsic limitation in the left-to-right paradigm of LLMs, which hinders their ability to achieve advanced fluid intelligence.



\section*{Limitations}
Due to the experiment budget, on all the ARC related experiments, we only evaluate LLMs on 100 tasks rather than the whole corpus following~\citet{wang2023hypothesis}, which may lead to potential bias in the evaluation results. Also, although most of the ARC tasks can be composed by the six atomic operations proposed by our work, there may still exist very few tasks that cannot be composed by our atomic operations, which may also introducing few bias to~\tref{tab:fine-tune arc performance}. We will try to provide more comprehensive results in future works once we get more experimental budgets, and propose more atomic operations that could be used to cover more ARC tasks.

%\section*{Ethical Considerations}
%Since this paper includes many responses generated by LLMs, it is possible that these LLM generated contents include toxic and harmful parts, requiring users to perform comprehensive data processing if they want to use our methods.

\section*{Acknowledgment}
This work has been made possible by a Research Impact Fund project (RIF R6003-21) and a General Research Fund project (GRF 16203224) funded by the Research Grants Council (RGC) of the Hong Kong Government.

\bibliography{custom}

\appendix
\clearpage

\input{sections/appendix.tex}

\end{document}
