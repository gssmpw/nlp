\section{Experiments}\label{sec:exp}
For all the experiments, we use the LDM1.5 ~\citep{rombach2022high} as the
pre-trained text-to-image (T2I) model. We studied four  different T2I models and found this model outperforming the rest. For more details see Appendix~\ref{app:choice_of_gens}.

\textbf{Datasets.}
We validate our framework on two datasets. ImageNet-100~\citep{tian2020contrastive, sariyildiz2023fake}, a subset of ImageNet-1k~\citep{deng2009imagenet}, containing 100 classes and 5,000 validation examples, where the real validation set is used for evaluation and the real training set (126,689 examples) serves as a held-out test set.
We also conduct experiment ImageNet-1k, using the 50,000 validation examples to monitor performance and reserving the real training set (1.3 million examples) as a held-out test set.

\begin{figure*}
\centering
    \includegraphics[width=.49\linewidth]{scale.pdf}
    \hspace{0mm}
        \includegraphics[width=.49\linewidth]{scale_1k.pdf}
    \caption{\textbf{Scaling laws of synthetic data.} 
        Real Validation accuracy versus total dataset size  for the Static (pink $\times$), and Deliberate Practice (blue o) setups on ImageNet-100 (left) and ImageNet-1k (right). DP significantly outperforms Static data generation, achieving higher  accuracy with fewer synthetic examples. DP achieves the same accuracy as the static setup using 7.5$\times$ less data on ImageNet-100 and 20$\times$ less data while outperforming it on ImageNet-1K.
        }
\label{fig:scaling-laws}
\end{figure*}

\subsection{Scaling Laws of Synthetic Data}
We train a Vision Transformer (ViT-B)~\citep{dosovitskiy2020image} classifier with synthetic data. We study two scenarios: 1) Static data generation and 2) Deliberate Practice (DP). In all the experiments in this section we have a fixed and controlled setup. We train the models for 100k and 50k iterations for ImageNet-1k and ImageNet-100 respectively. For additional details, see Appendix~\ref{app:exp_details}.

\textbf{Static data generation.} 
In this setup, all data is generated before training, and the classifier is trained on a fixed dataset. We experiment with different dataset sizes to see its impact on accuracy.

\textbf{Deliberate Practice data generation.}
Hyperparameters \(\omega\) and \(\lambda\) are tuned on ImageNet-100 and found effective for ImageNet-1k as well (see Section \ref{app:exp_details} for details). We track validation accuracy throughout training and use it to determine when to generate new data, following a patience-based criterion. To ensure the model has not over-fitted to the validation set, we also report accuracy on the full real training sets of ImageNet-100 and ImageNet-1k, used as \textit{held-out} test sets.

Figure~\ref{fig:scaling-laws} compares the scaling laws of the  \textbf{Static} and \textbf{Deliberate Practice (DP)} on ImageNet-100 and ImageNet-1k. On both datasets, we note that DP scales well with dataset size and it consistently outperforms the Static setup, achieving higher validation accuracy at any given dataset size. On ImageNet-100 we observe that DP can reach the best accuracy of the static setup (with 3 million examples) using only 400k examples. This means that DP requires 7.5$\times$ less data to reach the same performance. On ImageNet-1k, we observe that DP can outperform the best accuracy of the static setup (with 13 million examples), using only 640k examples. This translates to DP requiring 20$\times$ less data to outperform the Static setup. For additional details on the hyper-parameters of these experiments, see Appendix~\ref{app:details_scaling}. Refer to Figure~\ref{fig:vis_init_vs_final} for a visualization of how the dataset evolves from the start to the end of training.

\begin{table*}[h]
\centering
\caption{\textbf{Comparison with previous work.} DP outperforms other models on both ImageNet-100 and ImageNet-1k while requiring significantly less data and fewer training iterations. Note that DP experiments reported in this table are trained longer than models reported in the previous section and, consistent with other work, use a smaller classifier free guidance scale of $\lambda=2$.}
\vspace{0.1cm}
{\scriptsize
\begin{tabular}{llrrllllll}
\toprule
 & Task & \# Iters & Data size & IN real Val.& IN real tr.& IN-v2 & IN-Sk & IN-R & IN-A \\
\midrule
\midrule
Real & IN-100 & 100k & 130k & 88.5 & - & \textbf{76.4}&  37.1&  60.8& \textbf{33.5} \\
Syn. Static - \cite{sariyildiz2023fake}& IN-100 & 13k & 130k & 63.5 & - & 62.7&  41.8&  64.2& 13.7 \\
Syn. Static - \cite{sariyildiz2023fake} & IN-100 & 635k & 6.5M & 73.3 &- &72.3& 42.0& 59.4 & 17.1  \\
Syn. DP (ours) & IN-100  &100k & 1.9M & \textbf{74.3} & 75.0 & 66.3 &  \textbf{52.0}&  \textbf{76.6}& 25.9 \\
		
\midrule
Real & IN-1k & 200k & 1.3M & 82.6&-& \textbf{70.9}&  32.5&  44.6& \textbf{29.4} \\
Syn. Static - \cite{sariyildiz2023fake}& IN-1k  & 130k& 1.3M& 42.9& - & 43.0 &  16.6 &  26.3& 3.6 \\
Syn. Static - \cite{fan2024scaling}& IN-1k  & 210k& 2M & 50 & - & 42.2 & 27.2 & 45.7 & 6.6\\
Syn. Static - \cite{fan2024scaling}& IN-1k  & 315k& 64M& 54 & - & 46.0 & 32.4 &52.5 & 9.4\\
Syn. DP (ours)& IN-1k  & 200k & 6.5M & {54.1} & 54.84 &48.5 & 34.7 &  56.0&  12.3 \\
Syn. DP (ours) & IN-1k  & 200k & 9.1M & \textbf{55.1} & 55.73 &{49.3}&\textbf{36.0} &\textbf{57.2} & {13.4} \\
\bottomrule
\end{tabular}\label{tab:compare}
}
\end{table*}

\subsection{Comparison with Previous Work}
We compare DP with prior works on synthetic data generation for image classification~\citep{sariyildiz2023fake, fan2024scaling}. Specifically, we evaluate setups that use classnames for prompting and publicly available models for sample generation. Performance is assessed on real ImageNet (held-out) training and validation sets, as well as on ImageNet-V2~\citep{recht2019imagenet}, ImageNet-Sketch~\citep{wang2019learning}, ImageNet-R~\citep{hendrycks2021many}, and ImageNet-A~\citep{hendrycks2021natural} to measure out-of-distribution (OOD) generalization.

The results in Table~\ref{tab:compare} show that DP outperforms prior benchmarks on both ImageNet-100 and ImageNet-1k while requiring significantly less data and fewer training iterations. On ImageNet-100, DP generated 4.6 million fewer samples and trained for only one-sixth of the iterations compared to previous works, yet achieved superior performance on the real data. Similarly, on ImageNet-1k, DP reduced sample generation by 56.2 million and cut training iterations by over 30\%, while still outperforming previous results.

Furthermore, models trained with DP exhibit strong performance on out-of-distribution datasets, even surpassing models trained on real data on ImageNet-R and ImageNet-Sketch, with improvements of up to 15\%.

\subsection{Connection Between Pruning and DP}

In Section~\ref{sec:2}, we discussed how DP approximates direct sampling from a pruned distribution. Here, we validate this experimentally on ImageNet-100 using two setups:
\begin{enumerate}
    \item \textbf{Oversampling then Pruning:} Generate a large pool and select high-entropy samples.
    \item \textbf{Direct entropy-guided generation:} Generate only informative samples (a special case of DP with a single step of data addition).
\end{enumerate}

We start with 130k generated samples (regular vanilla sampling), train for 17k iterations, then add a one-time additional 130k samples, increasing the total data size to 260k and training for an additional 33k iterations.

In setup 1, we vary the pool size, ranging from no pruning (130k pool) up to an oversampling ratio of 18 (2.4M pool), selecting the top 130k high-entropy samples. In setup 2, we generate exactly 130k entropy-guided samples, varying the entropy-gauidance coefficient.

Figure~\ref{fig:explicit_prune_vs_DP} (a, b) shows that both methods improve performance up to a point, after which excessive selection of high-entropy samples leads to degradation—likely due to selecting high-entropy but harmful outliers. This aligns with our theoretical predictions in Figure~\ref{fig:explicit_prune_vs_DP} (c).

Regarding computational costs, generating a single image with entropy-guidance on an Nvidia H100 takes 1.82$\times$ longer than standard vanilla sampling. However, achieving similar performance through oversampling requires significantly more data, leading to a linear increase in cost. As a result, DP is $5\times$ more efficient while also providing higher absolute improvements compared to pruning-based selection. See Figure~\ref{fig:explicit_prune_vs_DP} for details and Figure~\ref{fig:wolfs} for some visualizations.


\begin{figure}[th]
    \centering
    \includegraphics[width=1\linewidth]{explicit_prune_vs_DP_arxiv.pdf}
    \caption{Plots describing the performance of DP compared to explicit pruning and theory prediction while changing the oversampling ratio or the DP coefficient. (a) Over-sampling with entropy-based selection – Generate a large pool of samples (ranging from 130k to 2.4M) and select the 130k highest-entropy examples. (b) Generate 130k high-entropy examples directly using DP with varying entropy guidance strength through $\omega$. (c) The theory prediction on the accuracy based on the over-sampling ration. (d) Comparing the compute cost of DP vs oversampling then pruning.
    We observe that DP exhibits a similar accuracy curve compared to explicit pruning and theoretical prediction when changing the over-sampling/DP coefficient. However, DP is computationally remarkably more efficient while gaining more accuracy delta.}
    \label{fig:explicit_prune_vs_DP}
    \vspace{-0.2cm}
\end{figure}

\subsection{The evolution of hard examples over time}

``Does the sample hardness change as training progresses?''

To answer this question, Figure~\ref{fig:everystep} (left) tracks the error on examples that were misclassified at the time they were added. As expected, once introduced, the model gradually learns to classify them correctly. However, an interesting trend emerges: even before these examples were added, their error was lower than at the moment of inclusion. This suggests that the notion of hardness is dynamic—what is considered challenging at one point may become easier over time. Conversely, examples that were once easy might later become difficult due to shifts in the learned decision boundaries. This highlights a key limitation of static pruning approaches and underscores the importance of dynamically adapting the selection of informative examples throughout training, as done in Deliberate Practice (DP). See Figure~\ref{fig:vis_stages} for some visualization of generations through training.

Figure~\ref{fig:everystep} (right) shows the evolution of generated examples from the class of ``school bus'' throughout training. Early samples often have atypical colors or grayscale tones, indicating the model's initial struggle with changes in the \textit{color} features. As training progresses, more challenging examples with unusual shapes and viewpoints emerge, reflecting the model’s shifting focus towards more complicated features such as \textit{shape}. See additional samples in Figure~\ref{fig:merged}.


\begin{figure}[ht]
    \centering
\includegraphics[width=1\linewidth]{everystep_arxiv.pdf}
    \caption{\textbf{Left:} Error trajectories of hard (misclassified) examples added at different training stages. The red curve highlights the first batch of added data for better visibility, but the same trend applies to all batches. Notably, even before being trained on, these examples exhibit a lower error rate than at their point of inclusion, indicating that hardness is not static, it evolves throughout training. \textbf{Right:} Examples of the prompt ``school bus'' generated at different epochs with varying entropy guidance scales (\(\omega\)). We observe that the model's training needs evolve over time, starting with simpler challenges like color recognition before progressing to harder examples with unusual shapes or viewpoints. }
    \label{fig:everystep}
\end{figure}
