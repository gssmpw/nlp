\section{Problem Formulation}
\label{sec:2}

\paragraph{Problem Setup.} 
Standard supervised learning relies on a large real labeled training set. Here, however, we assume no real training data is available, and instead, we must rely on a generative model to synthesize training examples.

Formally, let \( \mathcal{Y} \) denote the set of class labels. Our goal is to train a classifier \( f_\phi: \mathcal{X} \to \mathcal{Y} \), parameterized by \( \phi \), which maps inputs \( x \in \mathcal{X} \) (\textit{e.g.}, images) to labels \( y \in \mathcal{Y} \). We are given a predefined label set \( \mathcal{Y} \), a fixed (small) validation set \( \mathcal{D}^{\text{val}} = \{(x_i, y_i)\}_{i=1}^n \) consisting of real data for evaluation, and a generative model \( g_\theta \) capable of sampling synthetic data conditioned on a label, \textit{i.e.}, \( x \sim g_\theta(y) \). However, no real training data is available, \textit{i.e.}, \( \mathcal{D}^{\text{tr}} = \varnothing \). The objective is to train \( f_\phi \) using \emph{as few generated examples as possible} while maximizing generalization to real data as measured by performance on \( \mathcal{D}^{\text{val}} \). The key challenge is to generate minimal yet effective training data, requiring a principled mechanism to select/generate  informative examples.

\paragraph{The Need for Informative Examples.}

Not all synthetic samples contribute equally to learning. Prior work shows that simply increasing the synthetic dataset size leads to diminishing returns, as many generated samples are redundant or too easy ~\citep{fan2024scaling}. Instead, training should focus on examples that maximize learning efficiency.

Given a measure of \textit{informativeness} for a synthetic sample \( x \), one approach is to generate a large dataset and \textbf{prune uninformative examples}. Formally, let \( \mathcal{D}^{\text{pool}} = \{(x_i, y_i)\}_{i=1}^N \) be a large set of $N$ generated samples. We define a \emph{pruned dataset} as $\mathcal{D}' := \{(x_i, y_i) \mid i \in [N], q_i = 1\}$, where $q_i \in \{0,1\}$ is a selection variable determining whether a data point $(x_i, y_i) \in \mathcal{D}^{\text{pool}}$ is retained. The subset size is constrained by \( m = \sum_{i=1}^N q_i \). The quantity $N / m$ is referred to as the over-sampling ratio.

Let $P$ and $Q$ denote the distributions of the original and pruned datasets, respectively. The pruning process operates as an importance sampling scheme:
\begin{equation}
    \label{eq:importance}
    \mathrm{d} Q = \pi \, \mathrm{d} P,
\end{equation}
where $\pi$ is a normalized weighting function that retains the  informative samples. The generate-then-prune approach ensures that only informative examples are kept, it is \textbf{computationally inefficient}, as many generated samples are discarded. This motivates the need to devise mechanisms to directly sample the informative examples.

\paragraph{Approximate  Sampling of Informative Examples.}
Suppose that \( \mathcal{D}^{\text{pool}} \) is generated using a diffusion model with induced probability \( P \). The generative process is governed by a reverse SDE~\citep{song2019generative}:
{
\begin{equation}
\begin{aligned}
    \mathrm{d} x &= \left[v(x, t) - g(t)^2 \nabla \log p_t(x) \right] \mathrm{d} t + g(t) \, \mathrm{d} W(t),
\end{aligned}
\label{eq:standard-reverse}
\end{equation}}
where \( W(t) \) is a Wiener process, modeling stochastic noise, \( v(x, t) \) is a drift term, \( g(t) \) is a coefficient controlling the noise level at time \( t \), and \( \nabla \log p_t(x) \) is the score function.

Instead of sampling from \( P \), we aim to sample directly from \( Q \) as in Eq. \eqref{eq:importance}. By Girsanovâ€™s theorem \citep{oksendal2013stochastic}, modifying the probability measure from \( P \) to \( Q \) introduces a correction term in the reverse SDE:
{
\begin{equation}
\begin{aligned}
    \mathrm{d} x &= \left[v(x, t) - g(t)^2 (\nabla \log p_t(x) + \nabla \log \pi(x, t)) \right] \mathrm{d} t
     + g(t) \, \mathrm{d} W(t).
\end{aligned}
\label{eq:modified-reverse}
\end{equation}}

The term \( \nabla \log \pi(x, t) \) effectively modifies the score function and biases the sampling distribution according to the weighting function \( \pi(x, t) \).  This modification allows approximating direct sampling from the pruned distribution \( Q \), eliminating the need to first sample uniformly from \( P \) and later prune the data.

\subsection{Efficient Entropy-Guided Sampling with DDIM.} 

We leverage denoising diffusion implicit models (DDIMs)~\citep{song2020denoising} for efficient sampling. At each step \( t \), the reverse update for generating a conditional sample is:
{
\begin{equation*}
    x_{t-1} = \sqrt{\xi_{t-1}} \hat{x}_{0, t} + \underbrace{\sqrt{1 - \xi_{t-1} - \sigma_t^2} \cdot \epsilon_\theta^{(t)}(x_t, y)}_{\text{direction pointing to } x_t} + \underbrace{\sigma_t \epsilon_t}_{\text{random noise}},
\end{equation*}}
where \( \epsilon_t \) is random noise and \( \sigma_t \) and \( \xi_{t-1} \) are time-dependent coefficients. The term \( \hat{x}_{0, t} \) approximates the final denoised sample:
\begin{equation}
    \hat{x}_{0, t} = \frac{x_t - \sqrt{1 - \xi_t} \epsilon_\theta^{(t)}(x_t, y)}{\sqrt{\xi_t}},
\end{equation}
in which $\epsilon_\theta^{(t)}(x_t, y)$ approximates the conditional score function using a pretrained denoising network~\citep{ho2022classifier}:
{
\begin{equation}
    \epsilon_\theta(x_t, y) \approx (1+\lambda)\tilde{\epsilon}_\theta(x, y) - \lambda \tilde{\epsilon}_{\theta}(x)
\end{equation}
}
where $\lambda$ is called the classifier-free guidance coefficient which controls the strength of conditional sampling on the label.\looseness-1

An efficient way of sampling from a modified diffusion mode as described in Eq.~\ref{eq:modified-reverse} was proposed by~\citet{hemmat2023feedback}, where the weighting function is derived from the entropy of the downstream learner, such that,
{
\begin{equation}
    \label{eq:ent}
    \log \pi \propto H(f_\phi(x_0)) = - \sum_{y \in \mathcal{Y}} f_\phi(y \mid x_0) \log f_\phi(y \mid x_0).
\end{equation}}
To compute the entropy as in Eq. \ref{eq:ent}, we need the denoised sample $x_0$. The term \( \hat{x}_{0, t} \) can be used to cheaply approximate entropy mid-generation. This allows direct sampling of high-entropy examples by modifying the score function:
{
\begin{equation}
    \tilde{\epsilon}_\theta^{(t)}(x_t, y) = \epsilon_\theta^{(t)}(x_t, y) + \omega \nabla_{x_t} H(f_\phi(\hat{x}_{0, t})),
\end{equation}}
where $\omega$ controls the contribution of the entropy-guidance.

In \cite{hemmat2023feedback}, real data is used to pre-train the learner, enabling an accurate estimation of \( \nabla_{x_t} H(f_\phi(\hat{x}_{0, t})) \). However, when real data is unavailable, alternative approaches are needed to assess sample informativeness. In the next section, we propose to leverage the learner itself during training to evaluate entropy and determine the informativeness of generated samples dynamically.

