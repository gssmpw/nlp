
\begin{figure}[ht]
    \centering
\includegraphics[width=1.0\linewidth]{acc_vs_iter_three_branch_outs.pdf}
\caption{
    Training loss (left) and validation accuracy (right) of Deliberate Practice on ImageNet-100.
    The classifier begins training on an initial static dataset (130k samples) until validation accuracy plateaus. At this point, additional samples are generated using entropy-guided sampling, focusing on hard/informative examples. The two dashed vertical lines indicate points where new data is added.
    We compare three setups: 
    (1) \textcolor[rgb]{0.8, 0.47, 0.39}{Orange}: No additional data is added, training only on the initial dataset. 
    (2) \textcolor[rgb]{0.6, 0.23, 0.64}{Purple}: One round of entropy-guided data generation adds 130k samples. 
    (3) \textcolor[rgb]{0.08, 0.04, 0.62}{Blue}: Two rounds of entropy-guided data generation, adding 260k samples in total. 
    Each data addition leads to an accuracy boost, demonstrating the effectiveness of DP in improving performance with fewer training iterations.
    For clarity, this figure shows only two rounds of data addition, but in practice, more rounds occur based on the allowed maximum patience. 
    Notably, while training loss increases with new data, validation accuracy steadily improves, showing that the model benefits from progressively challenging examples, ultimately reducing the generalization gap.
}
    \label{fig:branch_outs}
\end{figure}
\section{The deliberate Practice Framework for Synthetic Data Generation}\label{sec:dp}

In this section, we describe our Deliberate Practice framework, in which we efficiently train the learner with synthetic data in absence of any real data. In particular, we move to a  setup where we dynamically expand the dataset throughout the training.
Our framework is summarized in Algorithm~\ref{alg:framework}.

\begin{algorithm}
  \caption{Deliberate Practice for Synthetic Data Generation}
  \label{alg:framework}
  \textbf{Input:} Class labels $\mathcal{Y}$, Generative model $g_\theta$, Validation set $\mathcal{D}^{\text{val}}$, Initial dataset size $N$, New data size $P$, Patience $T_{\text{max}}$, Evaluation interval $\tau$. \\
  \textbf{Output:} Trained classifier $f_\phi$
  \begin{algorithmic}[1]
    \State \textbf{Initialize:} Generate $\mathcal{D}_0^{\text{tr}}$ with $N$ examples from $g_\theta$. Start training $f_\phi$ with learning-rate warm-up.
    \State Set patience counter $T \gets 0$.
    \While{training}
        \State Update $f_\phi$ on a mini-batch drawn uniformly from $\mathcal{D}_k^{\text{tr}}$.
        \If{(every $\tau$ iterations)}
            \State Evaluate validation accuracy $\mathcal{A}(f_\phi, \mathcal{D}^{\text{val}})$.
            \State Reset $T \leftarrow 0$ if accuracy improves; else increment $T \leftarrow T + 1$.
        \EndIf
        \If{$T \geq T_{\text{max}}$}
            \State Generate $P$ new examples $\mathcal{D}_{\text{new}}$ with feedback:
            \[
            \nabla_{z_t} \log p(x_t | y) = \nabla_{z_t} \log p_\theta(z_t) + \omega \nabla_{z_t} H(f_\phi(\hat{x}_{0, t}))
            \]
            \State Augment training set: $\mathcal{D}_{k+1}^{\text{tr}} \gets \mathcal{D}_k^{\text{tr}} \cup \mathcal{D}_{\text{new}}$.
            \State Reset $T \gets 0$.
        \EndIf
    \EndWhile
    \State \textbf{Finalize:} Apply learning rate decay.
  \end{algorithmic}
\end{algorithm}

\textbf{The initial training data.}
The framework begins by generating an initial set of $N$ synthetic training examples $\mathcal{D}^{{tr}}_0 = \{(x_i, y_i)\}_{i=1}^N$ using a pre-trained generative model $g_\theta$. For each class $y_i \in \mathcal{Y}$, the generative model samples  images $x_i \sim g_\theta(y_i)$ in a class-conditional manner. The classifier $f_\phi$ starts training on this dataset, with a learning-rate warm-up phase.

\textbf{Iterative training and additional data.}
Training proceeds iteratively with a mechanism to dynamically augment the dataset whenever the classifier's performance stagnates. The process alternates between training the classifier and generating new synthetic examples.

\textbf{Patience mechanism.} 
At regular iteration intervals, $\tau$, the validation accuracy $\mathcal{A}(f_\phi, \mathcal{D}^{\text{val}})$ is evaluated. If no improvement is observed for $T_{\text{max}}$ intervals (patience threshold), the framework triggers new data generation. 

\textbf{Entropy guided sampling.}  
When the patience mechanism triggers, $P$ new examples $\mathcal{D}_{\text{new}} = \{(x_j, y_j)\}_{j=1}^P$ are generated. We directly generate samples from the entropy pruned distribution through entropy guided sampling. The entropy is computed based on the current stage of the classifier $f_\phi$. The $\omega$ coefficient controls the effect of entropy-guidance. With $\omega=0$, we fall back into regular sampling of diffusion models, while  $\omega>0$ results in generations that have a higher entropy under the classifier.

\textbf{Training resumption.}  
The newly generated examples are added to the dataset, $\mathcal{D}_{k+1}^{\text{tr}} = \mathcal{D}_k^{\text{tr}} \cup \mathcal{D}_{\text{new}}$. After augmenting the dataset, training resumes with a constant learning rate until the patience mechanism is triggered again. Mini-batches are drawn uniformly from the updated pool, which grows dynamically from size $N$ to $N + kP$ after $k$ iterations of augmentation. This cycle is continued until we reach the cool-down phase where the learning rate is decreased and no more new data is added. See Figure~\ref{fig:branch_outs} for training dynamics of a classifier training with DP.

In Section \ref{sec:3}, we provide an intuitive theoretical framework to study the scaling behavior of a simplified DP. In Section \ref{sec:exp}, we validate the effectiveness of DP in large-scale experiments.

