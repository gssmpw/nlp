\section{Further Theoretical Analysis and proofs}\label{app:theory}
\subsection{The Unregularized Regime}

We now consider our theory in the limit  $\lambda \to 0^+$. Thus, the parameter vector for the classifier is the least-squares estimate for $w_0$, i.e $\hat w=\hat w_{LS} = {X'}^\dagger Y'$. We have the following important corollary to Theorem \ref{thm:main}.

\begin{corollary}
It holds that 
\begin{eqnarray}
E_{test}(\hat w) \to \Phi(-\frac{a}{\sqrt{b-a^2}})\text{ in the limit }n,d \to \infty,\,d/n \to \phi,\,\lambda\to 0^+,
\end{eqnarray}
where the constants $a$ and $b$ are given as follows:

(A) If $\phi < p$, then
\begin{align}
 a &:= \beta \sqrt{\frac{2}{\pi}} \frac{r_0}{p-\phi},\quad b := \frac{p^2\phi + \beta^2\cdot\left(r'_0-2\phi r_0\right)}{(p-\phi)^3},\\
 \text{with }
r_0 &:= 1-\rho^2+\rho^2\cdot p/\gamma,\quad
r'_0 := p\cdot \left(1-\rho^2 + \rho^2\cdot ((p-\phi)p/\gamma^2+\phi/\gamma)\right).
\end{align}

(B) If $\phi > p$, then
\begin{align}
a &:= \beta\sqrt{\frac{2}{\pi}}c_0r_0,\quad b := c_0\cdot\left(p\phi - \beta^2 r_0\right),\\
\text{with }c_0 &:=1-p/\phi,\quad r_0 := 1-\rho^2 + \frac{\rho^2}{\gamma/\phi + c_0}.
\end{align}
\label{cor:ridgeless}
\end{corollary}
The result is empirically verified in Figure \ref{fig:figcool}(a). 


\begin{figure}[!h]
    \centering
    \begin{subfigure}{0.6\linewidth}
        \centering
        \includegraphics[width=\linewidth]{real-err_theory_unreg.pdf}
        \caption{Regularization parameter $\lambda=10^{-6}$.}
        \label{fig:subfig1}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.6\linewidth}
        \centering
        \includegraphics[width=\linewidth]{real-err_theory.pdf}
        \caption{Regularization parameter $\lambda=10^{-2}$.}
        \label{fig:subfig2}
    \end{subfigure}

    \caption{\textbf{Empirical verification of Theorem \ref{thm:main} and Corollary \ref{cor:ridgeless}.} For this experiment, the input dimension is $d=350$, and each subplot corresponds to a different value of the original sample size $n$. The experiment for $\lambda=10^{-6}$ is a proxy for the unregularized case $\lambda\to 0^+$. Solid lines correspond to observed values of the test error $E_{\text{test}}(\hat{w})$, while broken lines are the theoretical prediction of Theorem \ref{thm:main} (\textbf{bottom row}) and Corollary \ref{cor:ridgeless} (\textbf{top row}). Notice the excellent match between the experimental results and our theory. Also, observe the multiple-descent patterns, reminiscent of a non-trivial effect of different pruning strategies in different regimes of the pruned training dataset size $n_0=np$; the vertical line corresponds to an interpolation threshold at $p=\phi$, i.e., $n_0=d$.}
    \label{fig:figcool}
\end{figure}

\subsection{Some Important Examples of Pruning Strategies}
\paragraph{Keep Hard Examples (KH).} Consider the case where the pruning strategy is given by $q_i = q_{KH}(x_i^\top w_s)$ for all $i$, where
\begin{eqnarray}
q_{KH}(t) := 1[|t| \le \xi] =  \begin{cases}
    1,&\mbox{ if }|t| \le \xi,\\
    0,&\mbox{ else,}
    \end{cases}
\end{eqnarray}
for some $\xi \ge 0$. Define $\alpha:=\xi/\|w_s\|$. We have explicit formula for the constants $\beta$ and $\tilde \beta$ appearing in Theorem \ref{thm:main}. Viz,
\begin{lemma}
With $\tau:=\rho/\sqrt{1-\rho^2}$, $\epsilon_1 := 2\Phi(\alpha/\sqrt{1-\rho^2})-1$, and $\epsilon_2 := 2\Phi(\tau \alpha)-1$, it holds that
\begin{align}
    \tilde \beta(q_{KH}) &= 2(\rho\varphi(0)\epsilon_1-\varphi(\alpha)\epsilon_2),\quad
    \beta(q_{KH}) = 2\varphi(0)\sqrt{1-\rho^2}\cdot \epsilon_1.
\end{align}
\label{lm:KH}
\end{lemma}

\paragraph{Example 2: Keep Easy Examples (KE).} Here, the pruning strategy is $q_i = q_{KE}(x_i^\top w_s)$, where
\begin{eqnarray}
 q_{KE}(t) := 1[|t| > \xi] = \begin{cases}
    0,&\mbox{ if }|t| \le \xi,\\
    1,&\mbox{ else.}
\end{cases}
\end{eqnarray}
\begin{lemma}
\label{lm:KE}
With $\tau:=\rho/\sqrt{1-\rho^2}$, $\epsilon_1 := 2(1-\Phi(\alpha/\sqrt{1-\rho^2}))$, $\epsilon_2 := 2\Phi(\tau \alpha)-1$, it holds that
\begin{align}
    \tilde \beta(q_{KE})&= 2(\rho\varphi(0)\epsilon_1+\varphi(\alpha)\epsilon_2),\quad \beta(q_{KE}) = 2\varphi(0)\sqrt{1-\rho^2}\cdot \epsilon_1.
\end{align}
\end{lemma}

\paragraph{Example 3: Interpolation between Keep Hard and Keep Easy Strategies.} Consider the following pruning strategy proposed in \cite{kolossov2024towards}
\begin{eqnarray}
    q(t) \propto \sigma(t)^\omega (1-\sigma(t))^\omega,
\end{eqnarray}
for some tuning parameter $\omega$. Here, $\sigma$ is the sigmoid function. We can associate $q(x_i^\top w_s)$ with the probability the auxiliary classifier $x \mapsto sign(x^\top w_s)$ assigns to an example $x_i$. Thus, positive values of $\omega$ correspond to keeping examples considered uncertain (i.e hard) by this classifier, while negative values correspond to examples considered easy.


\subsection{Main Ingredients of Proofs}
\subsubsection{Deterministic Equivalent for the Resolvent Matrix $R$}
\begin{definition}[Deterministic Equivalents]
    Given a sequence of random $N \times N$ matrices $(R_N)_N$, a deterministic equivalent thereof is a sequence of deterministic  $N \times N$  matrices $(\overline R_N)_N$ such that
    \begin{eqnarray}
    \trace A_N (R_N -\overline R_N) \overset{a.s}{\to} 0,
    \end{eqnarray}
    for all sequences of $N \times N$ matrices $(A_N)_N$ with bounded Frobenious norm.
\end{definition}

  Let $\Pi$ (resp. $\Pi_\perp=I_d-\Pi$) be the projection onto the span (resp. orthogonal complement of the span) of $w_s$. Define the following auxiliary vectors and scalars
\begin{align}
v &= \Sigma^{1/2}w_s,\quad v_1 = \frac{v^\top w_s}{\|w_s\|},\quad v_\perp = \Pi_\perp v.
\label{eq:projproj}
\end{align}
Note that $v_\perp$ is $(d-1)$-dimensional and $\|v_\perp\| = \sqrt{\|v\|^2-v_1^2}$.

Henceforth we make the replacement $z=-\lambda<0$, so that the resolvent matrix $R$ now writes
\begin{eqnarray}
    R = R(z) := (X^\top D X/n-zI_d)^{-1}.
\end{eqnarray}
 Let $\delta(z)$ be the unique positive solution to the fixed-point equation
\begin{align}
m(z) &= d^{-1}\trace\bar R_b(z),\quad \delta(z) = n^{-1}\trace\Sigma \bar R_b(z),\\
\bar R_b(z) &= \left(\mathbb E_{x \sim \mathcal N(0,\Sigma)}\,\left[\frac{q(x^\top w_s)}{1+q(x^\top w_s)\delta(z)}\right]\Sigma-zI_d\right)^{-1}.
\end{align}

Note that the inner expectation evaluates to
$$
\mathbb E_{x \sim \mathcal N(0,\Sigma)}\,\left[\frac{q(x^\top w_s)}{1+q(x^\top w_s)\delta(z)}\right] = \frac{p}{1+\delta(z)}=:t(z),
$$
and so $\bar R_b(z) = (t(z)\Sigma-z I_d)^{-1}$. Observe that $\bar R_b(z)(t(z)\Sigma-zI_d) = I_d$, and so $t(z)\Sigma\bar R_b(z) = I_d+z\bar R_b(z)$. We deduce that
\begin{align*}
t(z)\delta(z) &= n^{-1}\trace t(z)\Sigma \bar R_b(z) = n^{-1}\trace (I_d+z \bar R_b(z))=\phi\cdot\left(1+zm(z)\right).
\end{align*}
Thus the equations defining $m(z)$ and $\delta(z)$ can be rewritten as 
\begin{align}
m(z) &=  d^{-1}\trace (t(z)\Sigma-z I_d)^{-1},\\
t(z) &= \frac{p}{1+\delta(z)},\\
\phi\cdot(1+zm(z)) &= t(z)\delta(z) = t(z)\left(\frac{p}{t(z)}-1\right)=p-t(z).
\end{align}
Solving for $\phi z m(z)$ in terms of $t(z)$ in the last equation gives
$$
\phi zm(z) = \frac{p\delta(z)}{1+\delta(z)}-\phi = p-\phi - \frac{p}{1+\delta(z)}=p-\phi-t(z).
$$
Plugging this into the first equation gives the following fixed-point equation for $t(z)$
\begin{equation}
p-\phi-t(z) = zn^{-1}\trace(t(z)\Sigma-zI_d)^{-1}.
\label{eq:tz}
\end{equation}
The following result shows that $\bar R$ is a deterministic equivalent for $R$.
\begin{proposition}
Recall the function $t(z)$ as the unique positive solution to the equation \eqref{eq:tz}. Then,
\begin{align}
    R &\simeq \bar R,\text{ with }\bar R = \Sigma^{-1/2}(\bar m(z)\Pi_\perp + \tilde m(z)\Pi)\Sigma^{-1/2},\\
    \text{where }\bar m(z) &= \frac{1}{t(z)-z},\quad \tilde m(z) = \frac{1}{s(z)-z},\quad s(z) = \frac{\gamma}{1+\delta(z)}=(\gamma/p)t(z),\\
\gamma &:= \mathbb E [h(v_1 G_1 + \|v_\perp\|G_\perp )G_1 ^2],\\
h(x) &:= \frac{q(x)}{1+q(x)\delta(z)},\quad (G_1 ,G_\perp) \sim \mathcal N(0,I_2).
\end{align}
\label{prop:deterministic-equivalents}
\end{proposition}


\subsection{Isotropic Case}
Consider the special case where the covariance matrix is $\Sigma=I_d$. It is not hard to see that we must have $\bar m(z) \equiv m(z) \equiv \delta(z)/\phi$.
Let us now compute $m(z)$.

\begin{lemma}
For every $z=-\lambda<0$, $m(z)$ is given by formula \eqref{eq:meq-mp}.
\label{lm:mp}
\end{lemma}
\begin{proof}
Indeed, observe that in the isotropic case the equation \eqref{eq:tz} reduces to $p-\phi-t(z) = \phi z / (t(z)-z)$, or equivalently
\begin{eqnarray*}
    0 = \phi z + (t(z)-p+\phi)(t(z)-z) = t(z)^2-(p-\phi+z)t(z) + pz.
\end{eqnarray*}
The discriminant of this quadratic equation evaluates to
\begin{align*}
(p-\phi+z)^2 -4pz &= (p-\phi-z + 2z)^2 - 4pz\\
&= (p-\phi-z)^2+4z^2+4z(p-\phi-z) - 4pz\\
&= (p-\phi-z)^2-4\phi z,
\end{align*}
and so because $z=-\lambda<0$, the positive solution is
\begin{eqnarray}
    t(z) = \frac{p-\phi + z + \sqrt{(p-\phi-z)^2 - 4\phi z}}{2}.
\end{eqnarray}
We deduce that
\begin{align*}
    m(z) &= \frac{1}{t(z)-z} = \left(\frac{p-\phi - z + \sqrt{(p-\phi-z)^2 - 4\phi z}}{2}\right)^{-1}\\
    &=2 \cdot \frac{p-\phi-z-\sqrt{(p-\phi-z)^2-4\phi z}}{(p-\phi-z)-((p-\phi-z)^2-4\phi z)}\\
    &= \frac{p-\phi-z-\sqrt{(p-\phi-z)^2-4\phi z}}{2\phi z},
\end{align*}
which is precisely the claimed formula given in \eqref{eq:meq-mp}.
\end{proof}


The following result then follows directly from Proposition \ref{prop:deterministic-equivalents}.
\begin{corollary}
In the isotropic setting, we have the following deterministic equivalents:
\begin{align}
R &\simeq \bar R,\text{ with }\bar R = m(z)\Pi_\perp +s(z) \Pi,\\
R^2 &\simeq m'(z)\Pi_\perp +  \tilde m'(z)\Pi.
\end{align}
where $\tilde m(z) := 1/(s(z)-z)$, $s(z) = \gamma/(1+\phi m(z))$, and $\gamma \ge 0$ is as given in \eqref{eq:constants}.

\label{corr:deterministic-equivalents-isotropic}
\end{corollary}

\begin{eqnarray}
    \begin{split}
\rho = \frac{w_s^\top w_0}{\|w_s\|\|w_0\|},\,
\beta &:= \mathbb E\,[q(\|w_s\| G_2)|G_1|],\,
\gamma := \mathbb E\,[q(\|w_s\|G_1)G_1^2],
    \end{split}
\label{eq:constants}
\end{eqnarray}

\subsection{Test Error Representation ("Scaling Laws")}
We are now ready to state our main theoretical results, which is a generalization of Theorem \ref{thm:main}.

\begin{remark}
For simplicity of presentation, all our theoretical results only consider symmetric pruning strategies for which $q(-t) \equiv q(t)$. This includes the "keep hard" and "keep easy" pruning strategies considered in \citep{sorscher2022beyond}.
\end{remark}

\begin{proposition}
    For a random test point $(x,y) \sim P$ independent of training data, it holds that $yx^\top \hat w \overset{\mathcal L}{\to} N(m, \nu-m^2)$ in the limit \eqref{eq:proportionate}, where
\begin{align}
m &:= \frac{m_0}{1+\delta},\quad m_0 := \mu^\top \bar R\, c\\
\nu &:= \frac{\nu_0}{(1+\delta)^2},\quad \nu_0 :=\frac{p}{n}\trace\Sigma \Sigma' + c^\top \Sigma' c-\frac{2c^\top\bar R c}{1+\delta}\frac{1}{n}\trace \Sigma \Sigma',\\
\text{with }c &:= \mathbb E_{(x,y) \sim P'}[q(x^\top w_s)yx],\quad \Sigma' := \mathbb E\,[R\Sigma R].
\end{align}

Consequently, the limiting test error of $\hat w$ is given by 
\begin{eqnarray}
E_{test}(\hat w) \to \Phi\left(-\frac{m_0}{\sqrt{\nu_0-m_0^2}}\right).
\end{eqnarray}
\label{prop:testrep}
\end{proposition}

\subsection{Proof of Proposition \ref{prop:testrep}}
The proof follows standard \citep{Couillet_Liao_2022,Firdoussi2024} "leave-one-out" techniques which are now standard for analyses based on random matrix theory.

We start with the Woodbury identity tells us that
\begin{align*}
Rx_i &= (X^\top D X/n + \lambda I_d)^{-1}x_i = (n^{-1}\sum_{j=1}^nq_j x_jx_j^\top + \lambda I_d)^{-1}x_i\\
&= (R_{-i}^{-1} + q_ix_ix_i^\top/n)^{-1}x_i= \frac{R_{-i}x_i}{1+q_ix_i^\top R_{-i} x_i/n},
\end{align*}
where $R_{-i} := (n^{-1}\sum_{j \ne i}q_j x_jx_j^\top + \lambda I_d)^{-1}$ is a version of the resolvent matrix constructed without the $i$th data point. This "leave-one-out" trick is well-known in random matrix theory calculations. 

On the other hand $q_i x_i^\top R_{-i} x_i/n$ concentrates around its mean which is
\begin{align*}
\mathbb E\,[q_i x_i^\top R_{-i} x_i/n] &= \trace\left(\mathbb E[q_i x_ix_i^\top] R_{-i}/n\right) = \frac{\alpha}{n}\trace \Sigma R_{-i} \simeq \delta,\\
\text{with }\delta &:= \frac{p}{n}\trace\Sigma\bar R,\quad p := \mathbb E[q_i].
\end{align*}
Therefore, we have the following identities holding for every $i,j \in [n]$ with $i \ne j$:
\begin{align}
Rx_i &\simeq \frac{R_{-i}x_i}{1+\delta},\\
R_{-i} &\simeq R_{-ij}-\frac{R_{-ij}x_jx_j^\top R_{-ij}}{1+\delta}.
\end{align}

Now, let $x$ be a random test point from class $y$, independent of training data. Following a route similar to \citep{Firdoussi2024}, we shall compute the first two moments of the margin $yx^\top \hat w$. First observe that
\begin{align}
yx^\top \hat w &= \frac{1}{n}\sum_{i=1}^n q_i y_i yx^\top R x_i = \frac{1}{n}\sum_{i=1}^n q_i y_i yx^\top R x_i\nonumber\\
&=\frac{1}{(1+\delta)n}\sum_{i=1}^n q_i y_i yx^\top R_{-i} x_i
\label{eq:pred-decomp}
\end{align}

\subsection{First Moment of Test Margin}
From \eqref{eq:pred-decomp}, one computes for a random test point $(x,y) \sim P$,
\begin{align*}
\mathbb E\,[yx^\top \hat w] &= \frac{1}{(1+\delta)n}\sum_{i=1}^n \mathbb E\,[q_i y_i yx^\top R_{-i} x_i]\\
&= \frac{1}{(1+\delta)n}\sum_{i=1}^n \mathbb E\,[yx]^\top \mathbb E\,[R_{-i}]\mathbb E[q_i y_i x_i]\\
&= \frac{1}{(1+\delta)}\mu^\top\bar R\frac{1}{n}\sum_{i=1}^n \mathbb E[q_i y_ix_i]\\
&= \frac{1}{(1+\delta)}\mu^\top\bar R\,c,\\
\text{where }\mu &= \mathbb E_{(x,y)}\,[yx],\quad  c := \mathbb E_{(x,y)}\,[q(x^\top w_s) y x].
\end{align*}
The following result computes the mean vectors $\mu$ and $c$.
\begin{lemma}
Let $\rho \in [-1,1]$ be the cosine of the angle between $\bar w_s:=\Sigma^{1/2}w_s$ and $\bar w_0:=\Sigma^{1/2}w_0$. Let $u$ be the unit-vector in the direction of $\bar w_s$  and let $v$ be its completion to an orthonormal basis for the span of $\bar w_s$ and $\bar w_0$ (if $\bar w_s$ and $\bar w_0$ are parallel, i.e if $\rho=\pm 1$, we simply set $v=0$).
\begin{eqnarray}
 \mu  :=  \mathbb E_{(x,y) \sim P}[yx],\quad c :=  \mathbb E_{(x,y) \sim P}[q(x^\top w_s)y x]
\end{eqnarray}
Then, $\mu=\sqrt{2/\pi}\cdot \Sigma w_0/\|w_0\|_\Sigma$, and $c = \tilde \beta u + \beta v$, where
\begin{align}
    \tilde \beta = \beta_1 &: = 2\mathbb E\left[q(\|\bar w_s\|G)\Phi\left(\tau G\right)G\right],\quad \beta = \beta_2 := 2\mathbb E\left[q(\|\bar w_s\|G)\varphi(\tau G)\right],\quad \text{with }G\sim \mathcal N(0,1).
\end{align}

In particular, when $\rho = \pm 1$ (i.e pruning along the data generator), 
\begin{align}
    \beta_1 = \mathbb E[q(\|\bar w_s\|G)|G|],\quad \beta_2=0. 
\end{align}

\label{lm:means}
\end{lemma}

\subsection{Second Moment of Test Margin $yx^\top \hat w$}
Squaring \eqref{eq:pred-decomp} gives
\begin{align*}
(yx^\top \hat w)^2 &= \frac{1}{(1+\delta)^2n^2}\sum_{i=1}^n q_i\cdot (x^\top R_{-i} x_i)^2 + \frac{1}{(1+\delta)^2n^2}\sum_{i \ne j} q_iq_jy_iy_j (x^\top R_{-i} x_i)(x^\top R_{-j} x_j)
\end{align*}
For the expectation first some, note that
\begin{align*}
\frac{1}{n}\mathbb E\,[q_i \cdot (x^\top R_{-i} x_i)^2] = \frac{1}{n}\mathbb E[q_i x^\top R_{-i} x_ix_i^\top R_{-i}x]&= \frac{1}{n}\trace\left(\mathbb E\,[xx^\top]\mathbb E\,[q_i R_{-i}x_ix_i^\top R_{-i}]\right)= \frac{p}{n}\trace\Sigma \Sigma',
\end{align*}
with $\Sigma':= \mathbb E[R\Sigma R]$. We deduce that
\begin{align*}
\mathbb E\,\frac{1}{(1+\delta)^2n^2}\sum_{i=1}^n q_i\cdot (x^\top R_{-i} x_i)^2 &= \frac{1}{(1+\delta)^2}\frac{p}{n}\trace \Sigma \mathbb E\,[R \Sigma R]\nonumber\\
&= \frac{p}{(1+\delta)^2}\cdot \begin{cases}
n^{-1}\trace \mathbb E\,[R^2] \Sigma,&\mbox{ if isotropic},\\
\text{hard life!},&\mbox{ otherwise.}
\end{cases}
\end{align*}

Now, let $i,j \in [n]$ with $i \ne j$. One computes
\begin{align*}
\mathbb E\,[q_iq_jy_iy_j \cdot (x^\top R_{-i} x_i)(x^\top R_{-j} x_j)]
&= \frac{1}{1+\delta}\mathbb E\,\left[q_i q_j y_i y_jx_i^\top T_{ij}\Sigma T_{ji}x_j\right],\\
&= \frac{1}{1+\delta}(A_1-A_2-A_3+A_4),\\
\text{where }
T_{ij} &:= R_{-ij}-S_{ij}/n,\\
S_{ij} &:= \frac{R_{-ij}x_jx_j^\top R_{-ij}}{1+\delta},\\
A_1 &:= \mathbb E\,[q_iq_jy_iy_jx_i^\top R_{-ij}\Sigma R_{-ij}x_j],\\
A_2 &:= \frac{1}{(1+\delta)n}\mathbb E\,[q_iq_jy_iy_jx_i^\top S_{ij}\Sigma R_{-ij}x_j],\\
A_3 &:= \frac{1}{(1+\delta)n}\mathbb E\,[q_iq_jy_iy_jx_i^\top R_{-ij}\Sigma S_{ji}x_j],\\
A_4 &:= \frac{1}{(1+\delta)^2n^2}\mathbb E\,[q_iq_jy_i y_jx_i^\top S_{ij}\Sigma S_{ji}x_j]
\end{align*}
We now compute the terms $A_1,A_2,A_3,A_4$.
\begin{align*}
A_1 &= \mathbb E\,[q_iq_jy_iy_jx_i^\top R_{-ij}\Sigma R_{-ij}x_j] = \mathbb E\,[q_iq_jy_iy_jx_i^\top R\Sigma R x_j]\\
&= \trace\left(\mathbb E\,[(q_j y_j x_j)(q_iy_ix_i)^\top]\mathbb E\,[R\Sigma R]\right)= c^\top \Sigma' c,\\
\text{where }
\Sigma' &:= \mathbb E[R\Sigma R].
\end{align*}

Similarly, $A_3=A_2$ with
\begin{align*}
A_2 = \mathbb E\,[q_iq_jy_iy_jx_i^\top S_{ij}\Sigma R_{-ij}x_j] &= \frac{1}{(1+\delta)n}\mathbb E\,[q_iq_jy_iy_jx_i^\top R_{-ij}x_jx_j^\top R_{-ij}\Sigma R_{-ij}x_j]\\
&= \frac{1}{(1+\delta)n}\trace\left(\mathbb E\,[q_iq_jy_iy_jx_jx_i^\top R_{-ij} x_j x_j^\top]\mathbb E\,[R_{-ij}\Sigma R_{-ij}]\right)
\end{align*}
Now, computes 
\begin{align*}
\mathbb E\,[q_iy_iq_jy_jx_i^\top R_{-ij}x_j] &= \mathbb E\,[(q_iy_ix_i)^\top R_{-ij}(q_jy_jx_j)]= c^\top\mathbb E\,[R_{-ij}] c \simeq c^\top\mathbb E\,[R] c \simeq c^\top\bar R c,\\
\mathbb E[R_{-ij}\Sigma R_{-ij}] &\simeq \mathbb E[R\Sigma R] =: \Sigma',
\end{align*}
 from which it follows that
\begin{eqnarray}
A_3 = A_2 \simeq \frac{c^\top \bar R c}{1+\delta}\frac{1}{n}\trace \Sigma \Sigma'.
\end{eqnarray}

Finally, it is easy to show that $A_4=O(1/n)=o(1)$.

Putting things together gives the result. \qed

\subsection{Proof of Lemma \ref{lm:means}}

Observe that by instead considering $\Sigma^{-1/2}\mu$, $\Sigma^{-1/2}c$, and defining $v:=\Sigma^{1/2}w_s$ and  $u:=\Sigma^{1/2}w_0$ when computing $\mu$, and then  $u=\Sigma^{1/2}w_0$ when computing $c$, we reduce the problem to the isotropic case $x \sim \mathcal N(0,I_d)$.

So let $u=\Sigma^{1/2}w_0$, and WLOG, assume $u$ is aligned with the first canonical axis in $\mathbb R^d$, i.e $u=\|u\| e_1$. Write $x=(x_1,x_\perp)$ and $v = (v_1,v_\perp)$, where $x_\perp := \sum_{j=2}^d x_j e_j  \in \mathbb R^{d-1}$, and $v_\perp := \sum_{j=2}^d v_j e_j  \in \mathbb R^{d-1}$. It is clear that $x^\top u = \|u\|x_1$, and $x^\top v = v_1 x_1 + g$, where  $g=x_\perp^\top v_\perp$. Furthermore, $x_1$ and $g$ are independent with distributions $\mathcal N(0,1)$ and $\mathcal N(0,\|v_\perp\|^2)$ respectively. It follows that
    \begin{align*}
        \Sigma^{-1/2}\mu &= \mathbb E\,[sign(x^\top u)x] = \mathbb E\,[sign(\|u\|x_1)x_1]e_1 = \mathbb E\,[|x_1|]e_1\\
        &= \sqrt{\frac{2}{\pi}}e_1 = \sqrt{\frac{2}{\pi}}\frac{u}{\|u\|} = \sqrt{\frac{2}{\pi}}\frac{\Sigma^{1/2}w_0}{\|w_0\|_\Sigma},
    \end{align*}
    from which we deduce the prescribed formula for the vector $\mu$. This proves the first part of the claim.


We now establish the formula $c=\beta_1 u + \beta_2 v$. The proof for the formula for $\mu$ follows a similar (but simpler) path.

Observe that by instead considering $\Sigma^{-1/2}c$, we reduce the problem to the isotropic case $x \sim \mathcal N(0,I_d)$. We can explicitly write
\begin{align}
    u = \frac{\bar w_s}{\|\bar w_s\|},\quad v = \frac{\Pi^\perp \bar w_0}{\|\Pi^\perp\bar w_0\|},\quad \rho = \frac{\bar w_s^\top \bar w_0}{\|\bar w_s\|\|\bar w_0\|},
\end{align}
where $\Pi=uu^\top$ and $\Pi^\perp=I_d-\Pi$. One can decompose $x=G_1u + G_2v + G_\perp$ and $\bar w_0 = c_1u + c_2v + c_\perp$
\begin{align}
    G_1 &:= x^\top u, \quad G_2 := x^\top v,\quad G_\perp := P^\perp x,\\
    c_1 &:= w_0^\top u,\quad c_2:= x^\top v,\quad c_\perp :=\quad P^\perp \Sigma^{1/2}w_0,
\end{align}
where $P$ is the projector onto the span of $u$ and $v$.
Note that $G_1$, $G_2$, and $G_\perp$ forms a set of independent random variables. Moreover, $G_1$ and $G_2$ have distribution $\mathcal N(0,1)$, while $G_\perp$ has distribution $\mathcal N(0,I_{d-2})$. We obtain
\begin{align}
    \mathbb E[q(x^\top w_s)sign(x^\top w_0)x] &= \mathbb E\,[q(x^\top w_s)sign(x^\top w_0)x] = \mathbb E\,[q(x^\top w_s)sign(x^\top w_0)x]\\
    &= \mathbb E\,[q(\|w_s\|G_1)sign(c_1 G_1 + c_2G_2)G_1]\cdot u\\
    &\quad + \mathbb E\,[q(\|w_s\|G_1)sign(c_1 G_1 + c_2G_2)G_2]\cdot v\\
    &\quad + \mathbb E\,[q(\|w_s\|G_1)sign(c_1 G_1 + c_2G_2)G_\perp].
\end{align}
Now, due independence, the third term decomposes as
$$
\mathbb E\,[q(\|w_s\|_\Sigma \cdot G_1)sign(c_1 G_1 + c_2G_2)]\cdot \mathbb E\,[G_\perp]=0.
$$
We deduce that
\begin{align*}
\mathbb E[q(x^\top w_s)sign(x^\top w_0)x] &= \beta_1 u + \beta_2 v,\\
\end{align*}
where $\beta_1$ and $\beta_2$ are as specified in the lemma
and we have used the fact that
$$
c_1/\|\bar w_0\| = \rho,\quad c_2/\|\bar w_0\| = \sqrt{1-\rho^2}.
$$

In particular, if $\rho = \pm 1$ (meaning that $w_0$ and $w_s$ are parallel), then 
\begin{align}
    \beta_k &= \mathbb E\left[sign(\pm G_1)q(\|\bar w_s\|\cdot G_1)G_k\right] = \begin{cases}
        \pm \beta,&\mbox{ if }k=1,\\
        0,&\mbox{ otherwise.}
    \end{cases}
\end{align}

We now compute the coefficients $\beta_1$ and $\beta_2$. Observe that thanks to Lemma \ref{lm:angel}, one has
\begin{align*}
\mathbb E[sign(G_3) \mid G_1]
&=  \mathbb E[sign(\rho G_1 + \sqrt{1-\rho^2}G_2)\mid G_1] = 2\Phi\left(\tau G_1\right)-1,\\
\mathbb E[sign(G_3)G_2) \mid G_1] &=  \mathbb E[sign(\rho G_1 + \sqrt{1-\rho^2}G_2)G_2 \mid G_1] = 2\varphi(\tau G_1).
\end{align*}
Therefore, with $r := \|\bar w_s\|$, we have
\begin{align*}
    \beta_1 &:= \mathbb E[q(rG_1) sign(G_3) G_1] = 2\mathbb E\left[q(rG_1)\Phi\left(\tau G_1\right)G_1\right] - \mathbb E\left[q(rG_1)G_1 \right]=2\mathbb E\left[q(rG_1)\Phi\left(\tau G_1\right)G_1\right],\\
    \beta_2 &:= \mathbb E[q(rG_1) sign(G_3) G_2] = 2\mathbb E\left[q(rG_1)\varphi(\tau G_1)\right],
\end{align*}
where we have used the oddness of the function $t \mapsto tq(rt)$ in the last equation on the first line. \qed

\begin{lemma}
\label{lm:angel}
    Let $G \sim \mathcal N(0,1)$, and let $a,b \in \mathbb R$ with $a \ne 0$. Then,
    \begin{align}
        \mathbb E[sign(aG+b)] &= 2\Phi(b/|a|)-1,\quad 
        \mathbb E[sign(aG+b)G] = 2\varphi(b/a).
    \end{align}
Furthermore, it holds that
\begin{align}
    \lim_{a \to 0} \mathbb E[sign(aG+b)] &= sign(b),\quad \lim_{a \to 0} \mathbb E[sign(aG+b)G] = 0. 
\end{align}
\end{lemma}
\begin{proof}
    Indeed, one computes
    \begin{align*}
    \mathbb E[sign(aG+b)] &= \mathbb P(aG+b>0)-\mathbb P(aG+b<0)=2\mathbb P(aG>-b)-1\\
    &= \begin{cases}
        2\mathbb P(G>-b/a) - 1 = 2\Phi(b/a)-1,&\mbox{ if }a>0,\\
        2\mathbb P(G<-b/a)-1=2\Phi(-b/a)-1,&\mbox{ if }a<0.
    \end{cases}
    \end{align*}
    We deduce that $\mathbb E[sign(aG+b)]=2\Phi(b/|a|)-1$ as claimed.
\end{proof}

\section{Proof of Lemma \ref{lm:KH} and Lemma \ref{lm:KE}}
\textbf {"Keep Hard" Examples (Lemma \ref{lm:KH}).}
Let $b=\tau$, $t=\sqrt{1+b^2}=\sqrt{1+\tau^2}=1/\sqrt{1-\rho^2}$. Using Lemma \ref{lm:means} and standard formulae\footnote{For example, see Wikipedia \url{https://en.wikipedia.org/wiki/List_of_integrals_of_Gaussian_functions}.} for the anti-derivative of the function $z \mapsto z\varphi(bz)\varphi(z)$
\begin{align*}
\beta = \beta_2 &= 2\mathbb E\left[q(rG)\varphi(\tau G)\right] = 2\int_{-\alpha}^\alpha \varphi(\tau z)\varphi(z)\mathrm dz = \frac{2}{t}\varphi(0)\Phi(tz)\bigg]_{z=-\alpha}^\alpha\\
&= 2\sqrt{1-\rho^2} \varphi(0)\left(2\Phi(\alpha/\sqrt{1-\rho^2})-1\right) =2\varphi(0)\sqrt{1-\rho^2}\epsilon_2.
\end{align*}
On the other hand, we have $\tilde\beta = \beta_1 = 2\mathbb E\left[q(rG)\Phi (\tau G)G\right] = 2\int_{-\alpha}^\alpha z\Phi(\tau z)\varphi(z)\mathrm d z$ with
\begin{align*}
\int_{-\alpha}^\alpha z\Phi(\tau z)\varphi(z)\mathrm d z % = \int_{-\alpha}^\alpha z\Phi(tz)\varphi(z)\mathrm d z
&= (b/t)\varphi(0)\Phi(tz) -\varphi(z)\Phi(bz)\bigg]_{z=-\alpha}^\alpha \\
&=(b/t)\varphi(0)(2\Phi(t\alpha)-1) - \varphi(\alpha)(2\Phi(b\alpha)-1)\\
&= \rho\varphi(0)(2\Phi(\alpha/\sqrt{1-\rho^2})-1)-\varphi(\alpha)(2\Phi(\tau \alpha)-1)\\
&= \rho\varphi(0)\epsilon_1-\varphi(\alpha)\epsilon_2,
\end{align*}
which proves Lemma \ref{lm:KH}

\textbf{"Keep Easy" Examples (Lemma \ref{lm:KE}).}
Indeed, since $q_{KE} = 1-q_{KH}$, we know from the previous lemma (KH strategy) that
\begin{align*}
    \tilde \beta(q_{KE}) &= 2\mathbb E\,[q_{KE}(rG)\Phi(\tau G)G] = 2\mathbb E\,[\Phi(\tau G)G]-2\mathbb E\,[q_{KH}(rG)\Phi(\tau G)G]\\
    &= 2\mathbb E\,[\Phi(\tau G)G]-2\tilde \beta(q_{KH}) = 2(\rho\varphi(0)-\varphi(\alpha))-\tilde \beta(q_{KH})\\
    &= 2\rho\varphi(0)-2\rho \varphi(0)\epsilon_1(q_{KH})+2\varphi(\alpha)\epsilon_2(q_{KH})\\
    &= 2(\rho\varphi(0)(1-\epsilon_1(q_{KH}))+\varphi(\alpha)\epsilon_2(q_{KH}))\\
    &= 2(\rho\varphi(0)\epsilon_1 + \varphi(\alpha)\epsilon_2).
\end{align*}

The computation of $\beta_2(q_{KE})$ uses a completely analogous idea:
\begin{align*}
    \beta(q_{KE}) &= 2\mathbb E[q_{KE}(rG)\varphi(\tau G)]=2\mathbb E[\varphi(\tau G)]-2\mathbb E[q_{KH}(rG)\varphi(\tau G)]\\
    &= 2\varphi(0)\sqrt{1-\rho^2} - 2\beta(q_{KH})\\
    &= 2\left(\varphi(0)\sqrt{1-\rho^2}-\varphi(0)\sqrt{1-\rho^2}\epsilon_1(q_{KH})\right)\\
    &= 2\varphi(0)\sqrt{1-\rho^2}\left(1-\epsilon_1(q_{KH})\right)\\
    &= 2\varphi(0)\sqrt{1-\rho^2}\epsilon_1(q_{KE})
\end{align*}
This proves Lemma \ref{lm:KE}. \qed



\subsection{Proof of Proposition \ref{prop:deterministic-equivalents}}
Using Theorem 4 of Liao and Mahoney's "Hessian Eigenspectra of More Realistic Nonlinear Models"
 \url{https://arxiv.org/abs/2103.01519} and some basic manipulations, we can write
\begin{align}
R &\simeq \bar R,\\
\text{where }\bar R^{-1} &= \mathbb E_x\,\left[\frac{q}{1+q\delta(z)}(\Sigma^{1/2}\Pi_\perp\Sigma^{1/2} + \alpha\alpha^\top)\right]-zI_d,
\end{align}
where $q:=q(x^\top w_s)$ for $x \sim \mathcal N(0,\Sigma)$, $\alpha:=\Sigma^{1/2}\Pi x$. Since $q$ is Bernoulli with mean $p := \mathbb P(q=1)$, it is clear that
$$
\mathbb E_x\,\left[\frac{q}{1+q\delta(z)}\right] = \frac{p}{1+\delta(z)}:=t(z).
$$
This further gives
\begin{eqnarray}
    \begin{split}
\bar R^{-1} &= t(z)\Sigma^{1/2}\Pi_\perp \Sigma^{1/2}-zI_d+\Sigma^{1/2} \Pi K \Pi \Sigma^{1/2},\\
\text{with }K &:= \mathbb E_u\,\left[\frac{q(u^\top v)}{1+q(u^\top v)\delta(z)} uu^\top\right],
\end{split}
\label{eq:magical}
\end{eqnarray}
where $u := \Sigma^{-1/2}x \sim \mathcal N(0,I_d)$ and $v := \Sigma^{1/2}w_s$.

Now, to determine the matrix $K$, we first rewrite $u=(u_\parallel ,u_\perp)$ and $v=(v_1 ,v_\perp)$, where
\begin{align}
u_\parallel  &:= \frac{u^\top  w_s}{\|w_s\|} \in \mathbb R,\quad v_1  := \frac{v^\top w_s}{\|w_s\|} \in \mathbb R,\\
u_\perp &:= \Pi_\perp u \in \mathbb R^{d-1},\quad v_\perp := \Pi_\perp v \in \mathbb R^{d-1}.
\end{align}

The advantage of this representation is that
\begin{itemize}
\item  $u_\perp$ and $v_\perp$ are orthogonal to $w_s$.
\item $u_\parallel $ and $u_\perp$ are  statistically independent.
\item $u_\parallel $ has distribution $\mathcal N(0,1)$.
\item $u_\perp$ has distribution $\mathcal N(0,I_{d-1})$.
\end{itemize}
By symmetry of the situation, we know that
\begin{align*}
K&= s(z)  \Pi + s_\perp(z) \Pi_\perp,\\
\text{where }
s(z)  &:= \mathbb E [h(w^\top g)G_1 ^2],\quad
s_\perp(z) := \mathbb E [h(w^\top g) G_\perp^2]\\
w &:= (v_1 ,\|v_\perp\|) \in \mathbb R^2,\quad g := (G_1 ,G_\perp) \sim \mathcal N(0,I_2),\\
h(q) &:= \frac{q}{1+q\delta(z)}.
\end{align*}
Combining with \eqref{eq:magical}, we get
\begin{align}
\bar R^{-1}
&= \Sigma^{1/2}(a(z)I_d+b(z)\Pi)\Sigma^{1/2},\\
\text{where }
a(z) &= t(z) - z,\quad t(z) = \frac{p}{1+\delta(z)},\quad b(z) = s(z)-t(z).
\end{align}
Now, using the \emph{Matrix-Inversion Lemma}, one can obtain $\bar R$ from $\bar R^{-1}$ as follows:
$$
\Sigma^{1/2}\bar R\Sigma^{1/2}=(a(z)I_d+b(z)\Pi)^{-1} = \frac{1}{a(z)}\left(I_d - \frac{b(z)/a(z)}{b(z)/a(z)+1}\Pi\right) = \frac{1}{a(z)}\Pi_\perp + \frac{1}{b(z)+a(z)}\Pi.
$$
It suffices to notice that $1/(b(z)+a(z))=1/(s(z)-z) =\tilde m(z)$ and $1/a(z) = \bar m(z)$ by definition, and the result follows.

\subsection{Proof of Theorem \ref{thm:main}}
Set $z=-\lambda$. Recall from Lemma \ref{lm:means} that $\mu = \sqrt{2/\pi}w_0/\|w_0\|$ and $c = \beta_1 u + \beta_2 v$. In Theorem \ref{thm:main}, we have the identification $\beta=\beta_2$ and $\tilde \beta=\beta_1$. We know that $R \simeq \bar R = m(z)\Pi^\perp + \tilde m(z)\Pi$, where $\Pi=uu^\top$. One computes
\begin{align*}
       m_0 \simeq \mu^\top \bar R c &= \sqrt{\frac{2}{\pi}}\frac{1}{\|w_0\|}w_0^\top\left(m(z)\Pi^\perp + \tilde m(z) \Pi \right)(\beta_1 u + \beta_2 v),\\
       &= \sqrt{\frac{2}{\pi}}\frac{1}{\|w_0\|}w_0^\top\left(\beta_1 \tilde m(z)u + \beta_2m(z)v\right),\\
        \text{with }\frac{w_0^\top u}{\|w_0\|} &=\rho,\quad
        \frac{w_0^\top v}{\|w_0\|} = \frac{w_0^\top w_0/\|w_0\|-\rho\|w_0\|(u^\top w_0/\|w_0\|)}{\|w_0\|\sqrt{1-\rho^2}}\\
        &= \frac{\rho-\rho^2}{\sqrt{1-\rho^2}} =\sqrt{1-\rho^2} =:\omega/\beta_2,
\end{align*}
Putting things together gives $m_0 \simeq \sqrt{2/\pi}\cdot \left(\omega m(z) + \tilde\omega \tilde m(z)\right)$ as claimed.



Likewise, one computes
\begin{align*}
    \frac{1}{n}\trace R^2 &\simeq \frac{1}{n}\trace \left(m'(z)\Pi^\perp + \tilde m'(z)\Pi\right) \simeq \phi m'(z),\\
    c^\top \bar R c &= c^\top \left(m(z)\Pi^\perp + \tilde m(z)\Pi\right)c=(\beta_1 u + \beta_2v)^\top (\tilde m(z)\Pi + m(z)\Pi^\perp)(\beta_1 u + \beta_2 v)\\
    &= \beta_2^2 m(z) + \beta_1^2 \tilde m(z) = \beta^2 m(z) + \tilde \beta^2 \tilde m(z) =: r(z),\\
    c^\top \Sigma' c &= c^\top \mathbb E\,[R^2] c \simeq c^\top \left(m'(z)\Pi^\perp + \tilde m'(z)\Pi\right)c=\beta^2 m'(z) + \tilde \beta^2 \tilde m'(z)= r'(z),
\end{align*}
which the claimed formula for $\nu$ follows.\qed

\subsection{Proof of Corollary \ref{cor:ridgeless}}
As usual, set $z:=-\lambda<0$.

(A) For $\phi < p$, it is easy to see from formula \eqref{eq:meq-mp} and Lemma \ref{lm:primes} that in the limit $z \to 0$, one has
\begin{align*}
    m(z) &\to \frac{1}{p-\phi},\\
    \bar m(z) &\to 0,\\
    \tilde m(z) &\to \frac{p/\gamma}{p-\phi},\\
    m'(z) &\to \frac{p}{(p-\phi)^3},\\
    \bar m'(z) &\to \frac{1}{p-\phi},\\
    \tilde m'(z) &\to \frac{p/\gamma^2}{(p-\phi)^3}\left(p(p-\phi)+\phi\gamma\right)=\frac{p}{(p-\phi)^3}\left((p-\phi)p/\gamma^2+\phi/\gamma\right),\\
    \frac{ m'(z)}{1+\phi m(z)} &\to \frac{1}{(p-\phi)^2}.
\end{align*}
Furthermore, with $m_0$ and $\nu_0$  as defined in Theorem \ref{thm:main}, one computes
\begin{align*}
    r(z) &= \beta^2 m(z) + \tilde\beta^2 \tilde m(z) \to \beta^2\frac{1}{p-\phi} + \tilde \beta^2\frac{p/\gamma}{p-\phi}=\frac{r_0}{p-\phi},\\
    r'(z) &= \beta^2 m'(z) + \tilde \beta^2 \tilde m'(z) \to \beta^2\cdot \frac{p}{(p-\phi)^3} + \tilde \beta^2 \cdot\frac{p/\gamma^2}{(p-\phi)^3}(p(p-\phi)+\phi\gamma)=\frac{r'_0}{(p-\phi)^3},
\end{align*}
where $r_0$ and $r'_0$ are as defined in the claim. We deduce that $m_0/\sqrt{\nu_0-m_0^2} = a/\sqrt{b-a^2}$ and the result follows from Theorem \ref{thm:main}.

(B) Now consider the case $\phi>p$. Observe that $m_0 = \sqrt{\nu_0-m_0^2}=-zm_0/\sqrt{z^2-z^2m_0^2}$. On the other hand, from \eqref{eq:meq-mp} we know that
\begin{eqnarray}
    -zm(z) = \frac{\sqrt{(p-\phi-z)^2-4\phi z}-(p-\phi-z)}{2\phi}
\end{eqnarray}
Combining with Lemma \ref{lm:primes}, we deduce the following limits
\begin{align*}
-zm(z),z^2m'(z) &\to c_0:=1-p/\phi>0,\\
\bar m'(z) &\to \frac{p/\phi}{\phi-p},\\
-z\tilde m(z), z^2\tilde m'(z) &\to \frac{c_0}{\gamma/\phi+c_0},\\
\frac{-z m'(z)}{1+\phi m(z)} &\to \frac{1}{\phi}.
\end{align*}
Furthermore, one computes
\begin{align*}
-zr(z) &= \beta_2^2\cdot (-z m(z)) + \beta_1^2\cdot (-z \tilde m(z)) = \beta_2^2c_0 + \beta_1^2 \frac{c_0}{\gamma/\phi+c_0}=:c_0r_0,\\
z^2 r'(z) &= \beta_2^2 z^2m'(z) + \beta_1^2 z^2\tilde m(z) = \beta_2^2 c_0 + \beta_1^2 \frac{c_0}{\gamma/\phi+c_0}=c_0r_0,\\
-zm_0 &= \sqrt{2/\pi}\cdot (-zm(z)\omega-z\tilde m(z)\tilde \omega) \to \sqrt{2/\pi}c_0\cdot(\omega+\tilde\omega/(\gamma/\phi+c_0)):=a,\\
    z^2\nu_0 &= p\phi z^2 m'(z) + z^2r'(z)-2\phi\frac{-zm'(z)}{1+\phi m(z)}\cdot (-z r(z)) \\
    &\to p\phi c_0 + r_0c_0-2r_0c_0=c_0\cdot( p\phi - r_0)=:b.
\end{align*}
 We deduce that
$$
m_0/\sqrt{\nu_0-m_0^2}=-zm_0/\sqrt{z^2\nu_0-z^2m_0^2} = a/\sqrt{b-a^2},
$$
and the result follows from Theorem \ref{thm:main}.
\qed

\begin{lemma}
\label{lm:primes}
    We have the following identities:
    \begin{align*}
        m'(z) &= 
        \frac{m(z)^2}{1-(1+\bar m(z))^2\phi/p},\\
        \bar m'(z) &= \frac{p}{(z+\phi \bar m(z))^2/\bar m(z)^2-p\phi} = \frac{p}{(\phi+1/m(z))^2-p\phi}, \\
        \tilde m'(z) &= \tilde m(z)^2\left(\frac{\gamma\phi m'(z)}{(1+\phi m(z))^2} + 1\right),\\
        r'(z) &= \beta ^2 m'(z) + \tilde \beta ^2 \tilde m'(z).
    \end{align*}
\end{lemma}
\section{Additional Experimental}
\subsection{Choice of Generative Model}\label{app:choice_of_gens}
We evaluate the capabilities of four open-source large-scale pre-trained text-to-image models~\citep{rombach2022high} in a controlled setup to determine which one performs best for the image-classification task. Each synthetic image is generated with a simple prompt (\texttt{class name}). We create a dataset of size 130,000 examples and train a ViT-B model on the synthetic data. Our results (Table~\ref{tab:choice_of_gens}) show that LDM-1.5 outperforms its more recent counterparts, LDM-XL and LDM-2.1, despite being an older model. We hypothesize that this is due to the lower diversity of generations in more recent models. This finding is consistent with previous work~\citep{astolfi2024consistency}, which observed lower diversity in more recent latent diffusion models. For all of our experiments, we use LDM-1.5 as it is the best performing model.

\begin{table}[ht]
\vspace{-1em}
\centering
\caption{Study on the choice of generative model for the task of ImageNet-100 classification with synthetic data. All experiments are trained for 50k iterations and the dataset size is a static size of 130k.}\label{tab:choice_of_gens}
\vspace{0.4cm}
{\small
\begin{tabular}{lc}
\toprule
 Syn. Data Source&Real Val. Acc.\\ 
\midrule
\midrule
 LDM-1.4& 59.06  \\
 LDM-1.5&\textbf{59.24}  \\
 LDM-2.1& 55.92\\
 LDM-XL &52.8 \\
\bottomrule
\end{tabular}
}
\vspace{-1em}
\end{table}

\subsection{Ablations}\label{app:ablations}
\paragraph{$\omega=0$ vs\ \ $\omega>0$}
To understand the effect of different components of our framework, we ablate the case where data is generated through the DP framework, but with a coefficient of zero for the term $\omega$. We also report results using different values of $\omega$. See results in Table~\ref{tab:in100_ab} comparing row 1 with rows 2, 3 and 4.

\paragraph{Incremental patience}
In our experiments, setting the maximum patience value ($T_{\text{max}}$) to a fixed number resulted in the model requesting too much data when the size of the dataset was grown too big. For example, with a fixed patience of $T_\textrm{max}=7$, for an experiment with initial dataset of size 130k samples, monitoring the validation accuracy every 130k iterations, meant that in the beginning every example was seen on average 7 times before the patience reached $T_{\text{max}}$. But as we generate more examples throughout the training, with a fixed patience value, each example would not be able to be seen even at least once. When the dataset grows to be 1.3 million, each example is seen on average of 0.7 times. This resulted in the model hitting the maximum patience very often. As a result, we incrementally increase the maximum patience value as the dataset increases in size. See Table~\ref{tab:in100_ab} for the result that compares the two scenarios (comparing rows 1 and row 5). Note we found using an incremental patience to be significantly easier to tune. We often start with a patience of 1 and continue training. However, fixed patience requires more tuning depending on the size of the dataset and the number of training iterations.

\begin{table}[h]
\centering
\caption{Ablation study on ImageNet-100. Given a baseline method with DP, we modify each component of the framework one-by-one and study the effect of each change. All the experiments are trained for 50k iterations and have the same final size.}\label{tab:in100_ab}
\vspace{0.4cm}
{\small
\begin{tabular}{llllll}
\toprule
\# & $\omega$ & $T_{max}$ & Sampling & Real Val. Acc. & Real tr. Acc. \\ 
\midrule
\midrule
1   & 0.05     & inc. & uniform & 68.04  &  69.25           \\
2                         & \textbf{0}        & inc. & uniform & 61.58 &  63.09            \\
3                        & \textbf{0.03}     & inc.  & uniform & 66.70  &  68.00           \\
4                       & \textbf{0.07}     & inc.  & uniform & 66.88 &    68.66            \\
5    & 0.05     & \textbf{fixed}   & uniform & 67.22 & 70.38 \\
6           & 0.05     & inc. & \textbf{non-uni. }& 68.01 &  69.11  \\
\bottomrule
\end{tabular}
}
\end{table}





\paragraph{Dataset sampling probabilities}
One can assume that newly generated examples could be more valuable then previously generated examples. As a result, we experiment the case where every newly generated example has twice as much probability to be selected when sampling the data batch for a given iteration. We observe that having higher probability does not lead to statistically significant improvements. See results in Table~\ref{tab:in100_ab} comparing rows 1 and 7.

    
\subsection{Intermediate stages of reverse sampling}
In section~\ref{sec:2} we mentioned that DDIM's $x_0$ approximation is a good approximation to guide the sampling process. In this section we plot these intermediate examples which are fed to the classifier to compute the entropy of the sample and use it for guidance in the sampling process. Figure~\ref{fig:intermediate-sampling} shows that although intermediate samples are noisy, they contain the key features. 
\begin{figure}
    \centering
    \includegraphics[width=0.8\linewidth]{x_0_time.pdf}
    \caption{\textbf{Intermediate stages of reverse sampling.} Samples of the $x_0$ approximation using the DDIM sampler. While blurry, these intermediate samples provide sufficient gradients for entropy guidance, with key features like color and shape discernible even in early stages.
    }\label{fig:intermediate-sampling}
\end{figure}

\subsection{Studying the effect of $\omega$}
In this section we study the effect of dynamically generating the data without entropy guidance versus generating it uniformly from the beginning. In the first scenario, we use the DP framework, with monitoring the patience variable but using an $\omega=0$ which effectively generates with naive sampling. In the second case all the data is generated in advance and no data is added during training. As it can be see in Figure~\ref{fig:acc_dyn_vs_static}, there is close to no difference between generating all the data in advance or generating it dynamically if we allow for enough iterations for training. 

In this experiment, we evaluate on ImageNet-100 validation set and train all the models for 50,000 iterations. 

\begin{figure}
    \centering
    \includegraphics[width=0.5\linewidth]{acc_dyn_vs_static.pdf}
    \caption{there is close to no difference between generating all the data in advance or generating it dynamically if we allow for enough iterations for training. Both cases have the same scaling behavior.}
    \label{fig:acc_dyn_vs_static}
\end{figure}

\subsection{Experimental Details}\label{app:exp_details}
\subsubsection{Scaling plots}\label{app:details_scaling}
We have used the Warmup-Stable-Decay (WSD) learning rate scheduler~\citep{hu2024minicpm}, which stabilizes the learning rate throughout most of the training, ensuring effective adaptation to newly generated data.
For ImageNet-100, we train on 4 nodes, each with 8 GPUs with a batchsize of 64. For ImageNet-1k, we train on 4 nodes, each with 8 GPUs with a batchsize of 128. For all the experiments, initial 10\% of the iterations is done with linear-warmup and the last 20\% of the iterations is for cool-down with Cosine Annealing. The intermediate steps are constant learning rate. For all these experiments we use $\lambda=3$ and $\omega=0.05$.

For ImageNet 100, the learning rate is $0.003$ with an EMA momentum of $0.001$. For ImageNet-1k, the learning rate is set to $0.0016$ with an EMA momentum of $0.001$. We also use label smoothing with a value of 0.11. We use Mixup with an alpha of 0.5 and CutMix with an alpha of 1.0. Furthermore, we use the AdamW optimizer. 

Furthermore, for each setup in our experiments, we apply branch-outs. A branch-out is the same experiment as an initial setup except that it does not allow additional data starting from a specific epoch. The epoch is selected based on the times that the $T_{max}$ was hit. Meaning a branch out is just before additional data is added to the training set.
\begin{table}[h]
\centering
\begin{tabular}{cccccccccc}
    \toprule
    \textbf{N} & \textbf{P} & \textbf{k} & \textbf{N + kP} & \textbf{$\omega$} & \textbf{Init. $T_{max}$} & \textbf{Branch out Epoch} & \textbf{IN Val.} & \textbf{IN-Sk} & \textbf{IN-R*} \\
    \midrule
    32000 & 16000 & 3& 80000 & 0.05 & 6 & 662 & 59.48 & 31.49 & 58.92 \\
    32000 & 16000 & 4& 96000 & 0.05 & 6 & 701 & 60.54 & 33.69 & 59.97 \\
    32000 & 16000 & 5& 112000 & 0.05 & 6 & 767 & 61.80 & 35.03 & 61.24 \\
    32000 & 16000 & 6& 128000 & 0.05 & 6 & 859 & 62.68 & 35.95 & 62.55 \\
    32000 & 16000 & 8& 160000 & 0.05 & 6 & 951 & 64.40 & 38.08 & 63.87 \\
    64000 & 32000 & 6& 256000 & 0.05 & 4 & 469 & 65.52 & 43.42 & 67.32 \\
    64000 & 32000 & 8& 320000 & 0.05 & 4 & 606 & 66.28 & 44.33 & 67.94 \\
    64000 & 32000 & 11& 416000 & 0.05 & 4 & 782 & 66.92 & 44.99 & 68.81 \\
    64000 & 32000 & 18& 640000 & 0.05 & 4 & 1001 & 67.80 & 45.25 & 68.46 \\
    130000 & 130000 & 6& 910000 & 0.05 & 14 & - & 68.28 & 45.06 & 70.87 \\
    130000 & 64000 & 27& 1794000 & 0.05 & 5 & 494 & 68.46 & 46.33 & 71.04 \\
    130000 & 64000 & 47& 3138000 & 0.05 & 5 & 618 & 68.88 & 45.76 & 71.26 \\
    64000 & 0 & - & 64000 & 0& inf & - & 56.56 & 27.86 & 52.97 \\
    130000 & 0 & - & 130000 & 0& inf & - & 59.44 & 33.32 & 55.95 \\
    260000 & 0 & - & 260000 & 0& inf & - & 60.02 & 33.79 & 56.74 \\
    400000 & 0 & - & 400000 & 0& inf & - & 61.92 & 36.03 & 59.75 \\
    2000000 & 0 & - & 2000000 & 0& inf & - & 62.16 & 34.97 & 60.15 \\
    4000000 & 0 & - & 4000000 & 0& inf & - & 62.32 & 36.43 & 60.89 \\
    \bottomrule
\end{tabular}
\caption{Details of the results reported in Figure~\ref{fig:scaling-laws} for the ImageNet-100 dataset. All the experiments are trained for 50k iterations. The variables are based on the notations defined in Algorithm~\ref{alg:framework}. Note that $T_{max}$ is incremental.}
\label{tab:your_table_label}
\end{table}

\begin{table}[h]
\centering
\begin{tabular}{cccccccccc}
    \toprule
    \textbf{N} & \textbf{P} & \textbf{k} & \textbf{N + kP} & \textbf{$\omega$} & \textbf{Init. $T_{max}$} & \textbf{Branch out Epoch} & \textbf{IN Val.} & \textbf{IN-Sk} & \textbf{IN-R} \\
    \midrule
    160000  & 160000  & 1  & 320000  & 0.05 & 1  & 134  & 42.572 & 39.363 & 20.987 \\
    320000  & 160000  & 1  & 480000  & 0.05 & 1  & 191  & 44.880 & 41.987 & 23.095 \\
    320000  & 320000  & 1  & 640000  & 0.05 & 1  & 71   & 47.910 & 46.887 & 27.568 \\
    654000  & 654000  & 1  & 1308000 & 0.05 & 1  & 124  & 50.226 & 49.867 & 29.843 \\
    654000  & 654000  & 2  & 1962000 & 0.05 & 1  & 156  & 50.670 & 51.027 & 29.944 \\
    1300000 & 650000  & 10 & 7800000 & 0.05 & 1  & 246  & 50.908 & 49.820 & 31.217 \\
    654000  & 654000  & 19 & 13080000 & 0.05 & 1  & -    & 51.198 & -      & 16.776 \\
    320000  & 0       & -  & 320000  & 0.0  & inf & -    & 39.334 & 32.653 & 18.495 \\
    654000  & 0       & -  & 654000  & 0.0  & inf & -    & 42.514 & 33.883 & 21.303 \\
    1300000 & 0       & -  & 1300000 & 0.0  & inf & -    & 44.116 & 37.337 & 23.653 \\
    2600000 & 0       & -  & 2600000 & 0.0  & inf & -    & 45.006 & 38.667 & 24.298 \\
    10000000 & 0      & -  & 10000000 & 0.0  & inf & -   & 45.614 & 40.050 & 24.762 \\
    13000000 & 0      & -  & 13000000 & 0.0  & inf & -   & 45.628 & 40.357 & - \\
    \bottomrule
\end{tabular}

\caption{Details of the results reported in Figure~\ref{fig:scaling-laws} for the ImageNet-1k dataset. All the experiments are trained for 100k iterations. The variables are based on the notations defined in Algorithm~\ref{alg:framework}. Note that $T_{max}$ is incremental.}
\label{tab:experiment_results}
\end{table}

\subsection{Visual examples}
Below we provide additional examples of generations throughout time with different $\omega$ coefficients (x-axis) of [0.0001, 0.1, 0.3, 0.5, 0.7]. All samples are generated with the same seed. As from top to bottom the epoch number increases.
\begin{figure}[h]
    \centering
    \begin{subfigure}{00.48\linewidth}
        \centering
        \includegraphics[width=\linewidth]{image_grid_n01944390.png}
        \caption{Class snail.}
        \label{fig:subfig1}
    \end{subfigure}
    \hfill
    \begin{subfigure}{00.48\linewidth}
        \centering
        \includegraphics[width=\linewidth]{image_grid_n11939491.png}
        \caption{Class daisy.}
        \label{fig:subfig5}
    \end{subfigure}
    \vspace{0.5cm}

    \begin{subfigure}{00.48\linewidth}
        \centering
        \includegraphics[width=\linewidth]{image_grid_n04162706.png}
        \caption{Class seat belt.}
        \label{fig:subfig3}
    \end{subfigure}
    \hfill
    \begin{subfigure}{00.48\linewidth}
        \centering
        \includegraphics[width=\linewidth]{image_grid_n09472597.png}
        \caption{Class volcano.}
        \label{fig:subfig4}
    \end{subfigure}

    \vspace{0.5cm}

    

    \caption{Examples of generated samples for different class prompts across training epochs, with varying entropy guidance coefficient (\(\omega\)) (left to right) as the training progresses (top to bottom).}

    \label{fig:merged}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{wolfs.pdf}
    \caption{\textbf{Efficient and Diverse Sampling with DP:}  Instead of inefficiently over-sampling and selecting high-entropy examples, DP directly generates high-entropy samples. This not only improves computational efficiency but also results in greater visual diversity.}
    \label{fig:wolfs}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{vis_stages.pdf}
    \caption{\textbf{Evolution of High-Entropy Samples During Training:} Early-stage generations show mainly color diversity, while later stages exhibit a richer set of transformations, aligning with the classifier's evolving uncertainties.}
    \label{fig:vis_stages}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.93\linewidth]{vis_init_vs_final.pdf}
    \caption{\textbf{Comparison of Initial and Final Training Data:} The initial training data lacks entropy guidance, as the classifier is untrained. By the end of training, the accumulated dataset contains progressively harder/diverse examples.}
    \label{fig:vis_init_vs_final}
\end{figure}
