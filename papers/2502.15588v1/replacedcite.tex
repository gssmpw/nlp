\section{Related Work}
\textbf{Synthetic data for training neural networks.} Synthetic data has become a powerful tool for training machine learning models across various domains. For instance, text-to-image diffusion models have been successfully used for visual representation learning____. However,  limitations of synthetic data are highlighted by____, emphasizing the importance of generating more challenging and informative examples. Addressing distribution shifts between synthetic and real data, ____ and ____ propose synthesizing training data that matches real data distributions or conditioning on real examples to reduce this gap. Expanding small-scale datasets has also been studied, see e.g.\ ____.
Another related line of work involves using VLMs and LLMs to generate descriptions for augmenting datasets____.

Synthetic data is increasingly used to train (LLMs). For example, LLaMA3____ employs AI-generated data for fine-tuning. Similarly, self-play approaches, e.g.,\ ____, align with our framework by generating increasingly difficult examples for training.

\textbf{Continual learning and active learning.}
 Our work is also closely related to principles from active learning____ and continual learning, which prioritize iterative model updates with tailored data. These methods highlight the importance of selecting informative samples based on the model's current state.
 ____ showed that pruning static datasets using metrics like margin scores can improve scaling laws by retaining the most informative examples, albeit in a non-adaptive manner.
 
\textbf{Challenges and risks of synthetic data.}
The challenges of training models on synthetic data, have gained significant attention. ____ studied “model collapse”, a phenomenon where iterative training on synthetic data degrades performance. 
They emphasize that data verification mechanisms can mitigate this risk and enable scaling with synthetic data. Similarly, our framework by generating informative examples through a dynamic loop, improves sample efficiency.