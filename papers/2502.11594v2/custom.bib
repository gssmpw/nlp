@inproceedings{langley00,
 author    = {P. Langley},
 title     = {Crafting Papers on Machine Learning},
 year      = {2000},
 pages     = {1207--1216},
 editor    = {Pat Langley},
 booktitle     = {Proceedings of the 17th International Conference
              on Machine Learning (ICML 2000)},
 address   = {Stanford, CA},
 publisher = {Morgan Kaufmann}
}

@TechReport{mitchell80,
  author = 	 "T. M. Mitchell",
  title = 	 "The Need for Biases in Learning Generalizations",
  institution =  "Computer Science Department, Rutgers University",
  year = 	 "1980",
  address =	 "New Brunswick, MA",
}

@phdthesis{kearns89,
  author = {M. J. Kearns},
  title =  {Computational Complexity of Machine Learning},
  school = {Department of Computer Science, Harvard University},
  year =   {1989}
}

@Book{MachineLearningI,
  editor = 	 "R. S. Michalski and J. G. Carbonell and T.
		  M. Mitchell",
  title = 	 "Machine Learning: An Artificial Intelligence
		  Approach, Vol. I",
  publisher = 	 "Tioga",
  year = 	 "1983",
  address =	 "Palo Alto, CA"
}

@Book{DudaHart2nd,
  author =       "R. O. Duda and P. E. Hart and D. G. Stork",
  title =        "Pattern Classification",
  publisher =    "John Wiley and Sons",
  edition =      "2nd",
  year =         "2000"
}

@misc{anonymous,
  title= {Suppressed for Anonymity},
  author= {Author, N. N.},
  year= {2021}
}

@InCollection{Newell81,
  author =       "A. Newell and P. S. Rosenbloom",
  title =        "Mechanisms of Skill Acquisition and the Law of
                  Practice", 
  booktitle =    "Cognitive Skills and Their Acquisition",
  pages =        "1--51",
  publisher =    "Lawrence Erlbaum Associates, Inc.",
  year =         "1981",
  editor =       "J. R. Anderson",
  chapter =      "1",
  address =      "Hillsdale, NJ"
}


@Article{Samuel59,
  author = 	 "A. L. Samuel",
  title = 	 "Some Studies in Machine Learning Using the Game of
		  Checkers",
  journal =	 "IBM Journal of Research and Development",
  year =	 "1959",
  volume =	 "3",
  number =	 "3",
  pages =	 "211--229"
}


@article{trace,
  title   = {TRACE: Temporal Grounding Video LLM via Causal Event Modeling},
  author  = {Yongxin Guo and Jingyu Liu and Mingda Li and Xiaoying Tang and Qingbin Liu and Xi Chen},
  year    = {2024},
  journal = {arXiv preprint arXiv: 2410.05643}
}


@String(PAMI = {IEEE Trans. Pattern Anal. Mach. Intell.})
@String(IJCV = {Int. J. Comput. Vis.})
@String(CVPR= {IEEE Conf. Comput. Vis. Pattern Recog.})
@String(ICCV= {Int. Conf. Comput. Vis.})
@String(ECCV= {Eur. Conf. Comput. Vis.})
@String(NIPS= {Adv. Neural Inform. Process. Syst.})
@String(ICPR = {Int. Conf. Pattern Recog.})
@String(BMVC= {Brit. Mach. Vis. Conf.})
@String(TOG= {ACM Trans. Graph.})
@String(TIP  = {IEEE Trans. Image Process.})
@String(TVCG  = {IEEE Trans. Vis. Comput. Graph.})
@String(TMM  = {IEEE Trans. Multimedia})
@String(ACMMM= {ACM Int. Conf. Multimedia})
@String(ICME = {Int. Conf. Multimedia and Expo})
@String(ICASSP=	{ICASSP})
@String(ICIP = {IEEE Int. Conf. Image Process.})
@String(ACCV  = {ACCV})
@String(ICLR = {Int. Conf. Learn. Represent.})
@String(IJCAI = {IJCAI})
@String(PR   = {Pattern Recognition})
@String(AAAI = {AAAI})
@String(CVPRW= {IEEE Conf. Comput. Vis. Pattern Recog. Worksh.})
@String(CSVT = {IEEE Trans. Circuit Syst. Video Technol.})

@String(SPL	= {IEEE Sign. Process. Letters})
@String(VR   = {Vis. Res.})
@String(JOV	 = {J. Vis.})
@String(TVC  = {The Vis. Comput.})
@String(JCST  = {J. Comput. Sci. Tech.})
@String(CGF  = {Comput. Graph. Forum})
@String(CVM = {Computational Visual Media})


@String(PAMI  = {IEEE TPAMI})
@String(JMLR  = {JMLR})
@String(IJCV  = {IJCV})
@String(IJCAI  = {IJCAI})
@String(CVPR  = {CVPR})
@String(ICCV  = {ICCV})
@String(ICLR  = {ICLR})
@String(ACL   = {ACL})
@String(NAACL   = {NAACL})
@String(EMNLP = {EMNLP})
@String(ICML  = {ICML})
@String(ECCV  = {ECCV})
@String(NIPS  = {NeurIPS})
@String(ICPR  = {ICPR})
@String(BMVC  =	{BMVC})
@String(TOG   = {ACM TOG})
@String(TIP   = {IEEE TIP})
@String(TVCG  = {IEEE TVCG})
@String(TCSVT = {IEEE TCSVT})
@String(TMM   =	{IEEE TMM})
@String(ACMMM = {ACM MM})
@String(ICME  =	{ICME})
@String(ICASSP=	{ICASSP})
@String(ICIP  = {ICIP})
@String(ACCV  = {ACCV})
@String(ICLR  = {ICLR})
@String(IJCAI = {IJCAI})
@String(PR = {PR})
@String(AAAI = {AAAI})
@String(CVPRW= {CVPRW})
@String(CSVT = {IEEE TCSVT})


@inproceedings{llava,
  title={Visual instruction tuning},
  author={Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae},
  booktitle = NIPS,
  year={2023}
}

@inproceedings{instruct_blip,
  title={InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning},
  author={Wenliang Dai and Junnan Li and Dongxu Li and Anthony Meng Huat Tiong and Junqi Zhao and Weisheng Wang and Boyang Li and Pascale Fung and Steven Hoi},
  booktitle = NIPS,
  year={2023}
}

@inproceedings{blip2,
	title = {BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models},
	booktitle = ICML,
    year = 2023,
	author = {Li, Junnan and Li, Dongxu and Savarese, Silvio and Hoi, Steven},
}

@article{video-llava,
  title={Video-llava: Learning united visual representation by alignment before projection},
  author={Lin, Bin and Zhu, Bin and Ye, Yang and Ning, Munan and Jin, Peng and Yuan, Li},
  journal={arXiv preprint arXiv:2311.10122},
  year={2023}
}

@article{video-llama,
  title={Video-llama: An instruction-tuned audio-visual language model for video understanding},
  author={Zhang, Hang and Li, Xin and Bing, Lidong},
  journal={arXiv preprint arXiv:2306.02858},
  year={2023}
}

@inproceedings{mvbench,
  title={Mvbench: A comprehensive multi-modal video understanding benchmark},
  author={Li, Kunchang and Wang, Yali and He, Yinan and Li, Yizhuo and Wang, Yi and Liu, Yi and Wang, Zun and Xu, Jilan and Chen, Guo and Luo, Ping and others},
  booktitle=CVPR,
  year={2024}
}

@article{pllava,
  title={Pllava: Parameter-free llava extension from images to videos for video dense captioning},
  author={Xu, Lin and Zhao, Yilin and Zhou, Daquan and Lin, Zhijie and Ng, See Kiong and Feng, Jiashi},
  journal={arXiv preprint arXiv:2404.16994},
  year={2024}
}

@inproceedings{video-chatgpt,
  title={Video-chatgpt: Towards detailed video understanding via large vision and language models},
  author={Maaz, Muhammad and Rasheed, Hanoona and Khan, Salman and Khan, Fahad Shahbaz},
  booktitle=ACL,
  year={2024}
}

@inproceedings{timechat,
  title={Timechat: A time-sensitive multimodal large language model for long video understanding},
  author={Ren, Shuhuai and Yao, Linli and Li, Shicheng and Sun, Xu and Hou, Lu},
  booktitle=CVPR,
  year={2024}
}

@inproceedings{vtimellm,
  title={Vtimellm: Empower llm to grasp video moments},
  author={Huang, Bin and Wang, Xin and Chen, Hong and Song, Zihan and Zhu, Wenwu},
  booktitle=CVPR,
  year={2024}
}

@inproceedings{instructgpt,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  booktitle=NIPS,
  year={2022}
}

@inproceedings{dpo,
  title={Direct preference optimization: Your language model is secretly a reward model},
  author={Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Manning, Christopher D and Ermon, Stefano and Finn, Chelsea},
  booktitle=NIPS,
  year={2023}
}

@article{dpo-video,
  title={Direct Preference Optimization of Video Large Multimodal Models from Language Model Reward},
  author={Zhang, Ruohong and Gui, Liangke and Sun, Zhiqing and Feng, Yihao and Xu, Keyang and Zhang, Yuanhan and Fu, Di and Li, Chunyuan and Hauptmann, Alexander and Bisk, Yonatan and others},
  journal={arXiv preprint arXiv:2404.01258},
  year={2024}
}

@inproceedings{rlhf-v,
  title={Rlhf-v: Towards trustworthy mllms via behavior alignment from fine-grained correctional human feedback},
  author={Yu, Tianyu and Yao, Yuan and Zhang, Haoye and He, Taiwen and Han, Yifeng and Cui, Ganqu and Hu, Jinyi and Liu, Zhiyuan and Zheng, Hai-Tao and Sun, Maosong and others},
  booktitle=CVPR,
  year={2024}
}


@article{llava-rlhf,
  title={Aligning large multimodal models with factually augmented rlhf},
  author={Sun, Zhiqing and Shen, Sheng and Cao, Shengcao and Liu, Haotian and Li, Chunyuan and Shen, Yikang and Gan, Chuang and Gui, Liang-Yan and Wang, Yu-Xiong and Yang, Yiming and others},
  journal={arXiv preprint arXiv:2309.14525},
  year={2023}
}

@article{videohallucer,
  title={VideoHallucer: Evaluating Intrinsic and Extrinsic Hallucinations in Large Video-Language Models},
  author={Wang, Yuxuan and Wang, Yueqian and Zhao, Dongyan and Xie, Cihang and Zheng, Zilong},
  journal={arXiv preprint arXiv:2406.16338},
  year={2024}
}

@misc{llavanext,
    title={LLaVA-NeXT: Stronger LLMs Supercharge Multimodal Capabilities in the Wild},
    url={https://llava-vl.github.io/blog/2024-05-10-llava-next-stronger-llms/},
    author={Li, Bo and Zhang, Kaichen and Zhang, Hao and Guo, Dong and Zhang, Renrui and Li, Feng and Zhang, Yuanhan and Liu, Ziwei and Li, Chunyuan},
    month={May},
    year={2024}
}

@misc{phi3.5-vision,
    title={microsoft/Phi-3-vision-128k-instruct},
    url={https://huggingface.co/microsoft/Phi-3.5-vision-instruct},
    author={Microsoft},
    year={2024}
}

@inproceedings{two-stream,
  title={Two-stream convolutional networks for action recognition in videos},
  author={Simonyan, Karen and Zisserman, Andrew},
  booktitle=NIPS,
  year={2014}
}

@inproceedings{tsn,
  title={Temporal segment networks: Towards good practices for deep action recognition},
  author={Wang, Limin and Xiong, Yuanjun and Wang, Zhe and Qiao, Yu and Lin, Dahua and Tang, Xiaoou and Van Gool, Luc},
  booktitle=ECCV,
  year={2016},
}

@article{tempcompass,
  title={TempCompass: Do Video LLMs Really Understand Videos?},
  author={Liu, Yuanxin and Li, Shicheng and Liu, Yi and Wang, Yuxiang and Ren, Shuhuai and Li, Lei and Chen, Sishuo and Sun, Xu and Hou, Lu},
  journal={arXiv preprint arXiv:2403.00476},
  year={2024}
}

@article{self-reward,
  title={Self-rewarding language models},
  author={Yuan, Weizhe and Pang, Richard Yuanzhe and Cho, Kyunghyun and Sukhbaatar, Sainbayar and Xu, Jing and Weston, Jason},
  journal={arXiv preprint arXiv:2401.10020},
  year={2024}
}

@article{internvideo2,
  title={Internvideo2: Scaling video foundation models for multimodal video understanding},
  author={Wang, Yi and Li, Kunchang and Li, Xinhao and Yu, Jiashuo and He, Yinan and Chen, Guo and Pei, Baoqi and Zheng, Rongkun and Xu, Jilan and Wang, Zun and others},
  journal={arXiv preprint arXiv:2403.15377},
  year={2024}
}

@inproceedings{vit,
  title={An image is worth 16x16 words: Transformers for image recognition at scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
  booktitle=ICLR,
  year={2020}
}

@inproceedings{clip,
  title={Learning transferable visual models from natural language supervision},
  author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
  booktitle={International conference on machine learning},
  pages={8748--8763},
  year={2021},
  organization={PMLR}
}
@article{alexnet,
  title={Imagenet classification with deep convolutional neural networks},
  author={Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  journal={Advances in neural information processing systems},
  volume={25},
  year={2012}
}

@article{deco,
  title={DeCo: Decoupling Token Compression from Semantic Abstraction in Multimodal Large Language Models},
  author={Yao, Linli and Li, Lei and Ren, Shuhuai and Wang, Lean and Liu, Yuanxin and Sun, Xu and Hou, Lu},
  journal={arXiv preprint arXiv:2405.20985},
  year={2024}
}

@article{videogpt+,
  title={VideoGPT+: Integrating Image and Video Encoders for Enhanced Video Understanding},
  author={Maaz, Muhammad and Rasheed, Hanoona and Khan, Salman and Khan, Fahad},
  journal={arXiv preprint arXiv:2406.09418},
  year={2024}
}

@article{slowfast-llava,
  title={SlowFast-LLaVA: A Strong Training-Free Baseline for Video Large Language Models},
  author={Xu, Mingze and Gao, Mingfei and Gan, Zhe and Chen, Hong-You and Lai, Zhengfeng and Gang, Haiming and Kang, Kai and Dehghan, Afshin},
  journal={arXiv preprint arXiv:2407.15841},
  year={2024}
}

@inproceedings{charades-sta,
  title={Tall: Temporal activity localization via language query},
  author={Gao, Jiyang and Sun, Chen and Yang, Zhenheng and Nevatia, Ram},
  booktitle=ICCV,
  pages={5267--5275},
  year={2017}
}

@inproceedings{localizing,
  title={Localizing moments in video with temporal language},
  author={Hendricks, Lisa Anne and Wang, Oliver and Shechtman, Eli and Sivic, Josef and Darrell, Trevor and Russell, Bryan},
  booktitle=EMNLP,
  year={2018}
}

@inproceedings{activitynet,
  title={Activitynet: A large-scale video benchmark for human activity understanding},
  author={Caba Heilbron, Fabian and Escorcia, Victor and Ghanem, Bernard and Carlos Niebles, Juan},
  booktitle=CVPR,
  pages={961--970},
  year={2015}
}

@inproceedings{youcook,
  title={Towards automatic learning of procedures from web instructional videos},
  author={Zhou, Luowei and Xu, Chenliang and Corso, Jason},
  booktitle=AAAI,
  volume={32},
  number={1},
  year={2018}
}

@inproceedings{qvhighlights,
  title={Detecting moments and highlights in videos via natural language queries},
  author={Lei, Jie and Berg, Tamara L and Bansal, Mohit},
  booktitle=NIPS,
  pages={11846--11858},
  year={2021}
}

@inproceedings{tvsum,
  title={Tvsum: Summarizing web videos using titles},
  author={Song, Yale and Vallmitjana, Jordi and Stent, Amanda and Jaimes, Alejandro},
  booktitle=CVPR,
  year={2015}
}


@inproceedings{momentor,
  title={Momentor: Advancing video large language model with fine-grained temporal reasoning},
  author={Qian, Long and Li, Juncheng and Wu, Yu and Ye, Yaobo and Fei, Hao and Chua, Tat-Seng and Zhuang, Yueting and Tang, Siliang},
  booktitle=ICML,
  year={2024}
}

@article{vtgllm,
  title={VTG-LLM: Integrating Timestamp Knowledge into Video LLMs for Enhanced Video Temporal Grounding},
  author={Guo, Yongxin and Liu, Jingyu and Li, Mingda and Tang, Xiaoying and Chen, Xi and Zhao, Bo},
  journal={arXiv preprint arXiv:2405.13382},
  year={2024}
}

@article{hawkeye,
  title={Hawkeye: Training video-text llms for grounding text in videos},
  author={Wang, Yueqian and Meng, Xiaojun and Liang, Jianxin and Wang, Yuxuan and Liu, Qun and Zhao, Dongyan},
  journal={arXiv preprint arXiv:2403.10228},
  year={2024}
}

@article{lita,
  title={Lita: Language instructed temporal-localization assistant},
  author={Huang, De-An and Liao, Shijia and Radhakrishnan, Subhashree and Yin, Hongxu and Molchanov, Pavlo and Yu, Zhiding and Kautz, Jan},
  journal={arXiv preprint arXiv:2403.19046},
  year={2024}
}

@inproceedings{yang2023vid2seqlargescalepretrainingvisual,
  title={Vid2seq: Large-scale pretraining of a visual language model for dense video captioning},
  author={Yang, Antoine and Nagrani, Arsha and Seo, Paul Hongsuck and Miech, Antoine and Pont-Tuset, Jordi and Laptev, Ivan and Sivic, Josef and Schmid, Cordelia},
  booktitle=CVPR,
  pages={10714--10726},
  year={2023}
}

@inproceedings{lora,
  title={Lora: Low-rank adaptation of large language models},
  author={Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  booktitle=ICLR,
  year={2022}
}

@article{sharegpt4video,
      title={ShareGPT4Video: Improving Video Understanding and Generation with Better Captions}, 
      author={Lin Chen and Xilin Wei and Jinsong Li and Xiaoyi Dong and Pan Zhang and Yuhang Zang and Zehui Chen and Haodong Duan and Bin Lin and Zhenyu Tang and Li Yuan and Yu Qiao and Dahua Lin and Feng Zhao and Jiaqi Wang},
      year={2024},
      journal={arXiv preprint arXiv:2406.04325},

}

@inproceedings{nextgqa,
  title={Can i trust your answer? visually grounded video question answering},
  author={Xiao, Junbin and Yao, Angela and Li, Yicong and Chua, Tat-Seng},
  booktitle=CVPR,
  year={2024}
}

@article{numerologic,
  title={NumeroLogic: Number Encoding for Enhanced LLMs' Numerical Reasoning},
  author={Schwartz, Eli and Choshen, Leshem and Shtok, Joseph and Doveh, Sivan and Karlinsky, Leonid and Arbelle, Assaf},
  journal={arXiv preprint arXiv:2404.00459},
  year={2024}
}

@inproceedings{webvid,
  title={Frozen in time: A joint video and image encoder for end-to-end retrieval},
  author={Bain, Max and Nagrani, Arsha and Varol, G{\"u}l and Zisserman, Andrew},
  booktitle=ICCV,
  pages={1728--1738},
  year={2021}
}

@inproceedings{internvid,
  title={Internvid: A large-scale video-text dataset for multimodal understanding and generation},
  author={Wang, Yi and He, Yinan and Li, Yizhuo and Li, Kunchang and Yu, Jiashuo and Ma, Xin and Li, Xinhao and Chen, Guo and Chen, Xinyuan and Wang, Yaohui and others},
  booktitle=ICLR,
  year={2024}
}

@inproceedings{panda70m,
  title={Panda-70m: Captioning 70m videos with multiple cross-modality teachers},
  author={Chen, Tsai-Shien and Siarohin, Aliaksandr and Menapace, Willi and Deyneka, Ekaterina and Chao, Hsiang-wei and Jeon, Byung Eun and Fang, Yuwei and Lee, Hsin-Ying and Ren, Jian and Yang, Ming-Hsuan and others},
  booktitle=CVPR,
  pages={13320--13331},
  year={2024}
}

@inproceedings{nextqa,
  title={Next-qa: Next phase of question-answering to explaining temporal actions},
  author={Xiao, Junbin and Shang, Xindi and Yao, Angela and Chua, Tat-Seng},
  booktitle=CVPR,
  year={2021}
}

@article{gpt4,
  title={Gpt-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@article{sentence-embedding,
  title={Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks},
  author={Reimers, N},
  journal={arXiv preprint arXiv:1908.10084},
  year={2019}
}

@article{videomme,
  title={Video-MME: The First-Ever Comprehensive Evaluation Benchmark of Multi-modal LLMs in Video Analysis},
  author={Fu, Chaoyou and Dai, Yuhan and Luo, Yondong and Li, Lei and Ren, Shuhuai and Zhang, Renrui and Wang, Zihan and Zhou, Chenyu and Shen, Yunhang and Zhang, Mengdan and others},
  journal={arXiv preprint arXiv:2405.21075},
  year={2024}
}

@inproceedings{activityqa,
  title={Activitynet-qa: A dataset for understanding complex web videos via question answering},
  author={Yu, Zhou and Xu, Dejing and Yu, Jun and Yu, Ting and Zhao, Zhou and Zhuang, Yueting and Tao, Dacheng},
  booktitle=AAAI,
  year={2019}
}

@inproceedings{msqa,
  title={Video question answering via gradually refined attention over appearance and motion},
  author={Xu, Dejing and Zhao, Zhou and Xiao, Jun and Wu, Fei and Zhang, Hanwang and He, Xiangnan and Zhuang, Yueting},
  booktitle=ACMMM,
  year={2017}
}

@article{videochat,
  title={Videochat: Chat-centric video understanding},
  author={Li, KunChang and He, Yinan and Wang, Yi and Li, Yizhuo and Wang, Wenhai and Luo, Ping and Wang, Yali and Wang, Limin and Qiao, Yu},
  journal={arXiv preprint arXiv:2305.06355},
  year={2023}
}

@article{valley,
  title={Valley: Video assistant with large language model enhanced ability},
  author={Luo, Ruipu and Zhao, Ziwang and Yang, Min and Dong, Junwei and Li, Da and Lu, Pengcheng and Wang, Tao and Hu, Linmei and Qiu, Minghui and Wei, Zhongyu},
  journal={arXiv preprint arXiv:2306.07207},
  year={2023}
}

@inproceedings{sevila,
  title={Self-chained image-language model for video localization and question answering},
  author={Yu, Shoubin and Cho, Jaemin and Yadav, Prateek and Bansal, Mohit},
  booktitle=NIPS,
  year={2023}
}

@inproceedings{frozenbilm,
  title={Zero-shot video question answering via frozen bidirectional language models},
  author={Yang, Antoine and Miech, Antoine and Sivic, Josef and Laptev, Ivan and Schmid, Cordelia},
  booktitle=NIPS,
  year={2022}
}

@inproceedings{violetv2,
  title={An empirical study of end-to-end video-language transformers with masked visual modeling},
  author={Fu, Tsu-Jui and Li, Linjie and Gan, Zhe and Lin, Kevin and Wang, William Yang and Wang, Lijuan and Liu, Zicheng},
  booktitle=CVPR,
  year={2023}
}

@article{llovi,
  title={A simple llm framework for long-range video question-answering},
  author={Zhang, Ce and Lu, Taixi and Islam, Md Mohaiminul and Wang, Ziyang and Yu, Shoubin and Bansal, Mohit and Bertasius, Gedas},
  journal={arXiv preprint arXiv:2312.17235},
  year={2023}
}

@article{langrepo,
  title={Language repository for long video understanding},
  author={Kahatapitiya, Kumara and Ranasinghe, Kanchana and Park, Jongwoo and Ryoo, Michael S},
  journal={arXiv preprint arXiv:2403.14622},
  year={2024}
}

@article{streaming,
  title={Streaming long video understanding with large language models},
  author={Qian, Rui and Dong, Xiaoyi and Zhang, Pan and Zang, Yuhang and Ding, Shuangrui and Lin, Dahua and Wang, Jiaqi},
  journal={arXiv preprint arXiv:2405.16009},
  year={2024}
}

@inproceedings{chatunivi,
  title={Chat-univi: Unified visual representation empowers large language models with image and video understanding},
  author={Jin, Peng and Takanobu, Ryuichi and Zhang, Wancai and Cao, Xiaochun and Yuan, Li},
  booktitle=CVPR,
  year={2024}
}

@inproceedings{moviechat,
  title={Moviechat: From dense token to sparse memory for long video understanding},
  author={Song, Enxin and Chai, Wenhao and Wang, Guanhong and Zhang, Yucheng and Zhou, Haoyang and Wu, Feiyang and Chi, Haozhe and Guo, Xun and Ye, Tian and Zhang, Yanting and others},
  booktitle=CVPR,
  year={2024}
}

@inproceedings{vista-llama,
  title={VISTA-LLAMA: Reducing Hallucination in Video Language Models via Equal Distance to Visual Tokens},
  author={Ma, Fan and Jin, Xiaojie and Wang, Heng and Xian, Yuchen and Feng, Jiashi and Yang, Yi},
  booktitle=CVPR,
  year={2024}
}

@article{llama-vid,
  title={Llama-vid: An image is worth 2 tokens in large language models},
  author={Li, Yanwei and Wang, Chengyao and Jia, Jiaya},
  journal={arXiv preprint arXiv:2311.17043},
  year={2023}
}

@inproceedings{st-llm,
  title={ST-LLM: Large Language Models Are Effective Temporal Learners},
  author={Liu, Ruyang and Li, Chen and Tang, Haoran and Ge, Yixiao and Shan, Ying and Li, Ge},
  booktitle=ECCV,
  year={2024}
}

@article{otter,
  title={Mimic-it: Multi-modal in-context instruction tuning},
  author={Li, Bo and Zhang, Yuanhan and Chen, Liangyu and Wang, Jinghao and Pu, Fanyi and Yang, Jingkang and Li, Chunyuan and Liu, Ziwei},
  journal={arXiv preprint arXiv:2306.05425},
  year={2023}
}

@article{mplug-owl,
  title={mplug-owl: Modularization empowers large language models with multimodality},
  author={Ye, Qinghao and Xu, Haiyang and Xu, Guohai and Ye, Jiabo and Yan, Ming and Zhou, Yiyang and Wang, Junyang and Hu, Anwen and Shi, Pengcheng and Shi, Yaya and others},
  journal={arXiv preprint arXiv:2304.14178},
  year={2023}
}

@inproceedings{longvlm,
  title={Longvlm: Efficient long video understanding via large language models},
  author={Weng, Yuetian and Han, Mingfei and He, Haoyu and Chang, Xiaojun and Zhuang, Bohan},
  booktitle=ECCV,
  year={2024}
}

@inproceedings{sodac,
  title={SODA: Story oriented dense video captioning evaluation framework},
  author={Fujita, Soichiro and Hirao, Tsutomu and Kamigaito, Hidetaka and Okumura, Manabu and Nagata, Masaaki},
  booktitle=ECCV,
  year={2020},
}

@inproceedings{meteor,
  title={METEOR: An automatic metric for MT evaluation with improved correlation with human judgments},
  author={Banerjee, Satanjeev and Lavie, Alon},
  booktitle={Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization},
  pages={65--72},
  year={2005}
}

@misc{phi3,
      title={Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone}, 
      author={Marah Abdin and Jyoti Aneja and Hany Awadalla and Ahmed Awadallah and Ammar Ahmad Awan and Nguyen Bach and Amit Bahree and Arash Bakhtiari and Jianmin Bao and Harkirat Behl and Alon Benhaim and Misha Bilenko and Johan Bjorck and Sébastien Bubeck and Martin Cai and Qin Cai and Vishrav Chaudhary and Dong Chen and Dongdong Chen and Weizhu Chen and Yen-Chun Chen and Yi-Ling Chen and Hao Cheng and Parul Chopra and Xiyang Dai and Matthew Dixon and Ronen Eldan and Victor Fragoso and Jianfeng Gao and Mei Gao and Min Gao and Amit Garg and Allie Del Giorno and Abhishek Goswami and Suriya Gunasekar and Emman Haider and Junheng Hao and Russell J. Hewett and Wenxiang Hu and Jamie Huynh and Dan Iter and Sam Ade Jacobs and Mojan Javaheripi and Xin Jin and Nikos Karampatziakis and Piero Kauffmann and Mahoud Khademi and Dongwoo Kim and Young Jin Kim and Lev Kurilenko and James R. Lee and Yin Tat Lee and Yuanzhi Li and Yunsheng Li and Chen Liang and Lars Liden and Xihui Lin and Zeqi Lin and Ce Liu and Liyuan Liu and Mengchen Liu and Weishung Liu and Xiaodong Liu and Chong Luo and Piyush Madan and Ali Mahmoudzadeh and David Majercak and Matt Mazzola and Caio César Teodoro Mendes and Arindam Mitra and Hardik Modi and Anh Nguyen and Brandon Norick and Barun Patra and Daniel Perez-Becker and Thomas Portet and Reid Pryzant and Heyang Qin and Marko Radmilac and Liliang Ren and Gustavo de Rosa and Corby Rosset and Sambudha Roy and Olatunji Ruwase and Olli Saarikivi and Amin Saied and Adil Salim and Michael Santacroce and Shital Shah and Ning Shang and Hiteshi Sharma and Yelong Shen and Swadheen Shukla and Xia Song and Masahiro Tanaka and Andrea Tupini and Praneetha Vaddamanu and Chunyu Wang and Guanhua Wang and Lijuan Wang and Shuohang Wang and Xin Wang and Yu Wang and Rachel Ward and Wen Wen and Philipp Witte and Haiping Wu and Xiaoxia Wu and Michael Wyatt and Bin Xiao and Can Xu and Jiahang Xu and Weijian Xu and Jilong Xue and Sonali Yadav and Fan Yang and Jianwei Yang and Yifan Yang and Ziyi Yang and Donghan Yu and Lu Yuan and Chenruidong Zhang and Cyril Zhang and Jianwen Zhang and Li Lyna Zhang and Yi Zhang and Yue Zhang and Yunan Zhang and Xiren Zhou},
      year={2024},
      eprint={2404.14219},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2404.14219}, 
}

@article{mmbench-video,
  title={MMBench-Video: A Long-Form Multi-Shot Benchmark for Holistic Video Understanding},
  author={Fang, Xinyu and Mao, Kangrui and Duan, Haodong and Zhao, Xiangyu and Li, Yining and Lin, Dahua and Chen, Kai},
  journal={arXiv preprint arXiv:2406.14515},
  year={2024}
}

@inproceedings{mathgpt,
  title={Mathematical capabilities of chatgpt},
  author={Frieder, Simon and Pinchetti, Luca and Griffiths, Ryan-Rhys and Salvatori, Tommaso and Lukasiewicz, Thomas and Petersen, Philipp and Berner, Julius},
  booktitle=NIPS,
  year={2023}
}

@article{rope,
  title={Roformer: Enhanced transformer with rotary position embedding},
  author={Su, Jianlin and Ahmed, Murtadha and Lu, Yu and Pan, Shengfeng and Bo, Wen and Liu, Yunfeng},
  journal={Neurocomputing},
  volume={568},
  pages={127063},
  year={2024},
  publisher={Elsevier}
}

@inproceedings{conv_two_stream,
  title={Convolutional two-stream network fusion for video action recognition},
  author={Feichtenhofer, Christoph and Pinz, Axel and Zisserman, Andrew},
  booktitle=CVPR,
  year={2016}
}

@article{pca,
  title={Principal component analysis},
  author={Abdi, Herv{\'e} and Williams, Lynne J},
  journal={Wiley interdisciplinary reviews: computational statistics},
  volume={2},
  number={4},
  pages={433--459},
  year={2010},
  publisher={Wiley Online Library}
}

@inproceedings{adamw,
  author       = {Ilya Loshchilov and
                  Frank Hutter},
  title        = {Decoupled Weight Decay Regularization},
  booktitle    = ICLR,
  year         = {2019},
}


@article{zeng2024timesuite,
  title   = {TimeSuite: Improving MLLMs for Long Video Understanding via Grounded Tuning},
  author  = {Xiangyu Zeng and Kunchang Li and Chenting Wang and Xinhao Li and Tianxiang Jiang and Ziang Yan and Songze Li and Yansong Shi and Zhengrong Yue and Yi Wang and Yali Wang and Yu Qiao and Limin Wang},
  year    = {2024},
  journal = {arXiv preprint arXiv: 2410.19702}
}


@article{chen2024timemarker,
  title   = {TimeMarker: A Versatile Video-LLM for Long and Short Video Understanding with Superior Temporal Localization Ability},
  author  = {Shimin Chen and Xiaohan Lan and Yitian Yuan and Zequn Jie and Lin Ma},
  year    = {2024},
  journal = {arXiv preprint arXiv: 2411.18211}
}

@article{huang2023ram_plus,
  title={Open-set image tagging with multi-grained text supervision},
  author={Huang, Xinyu and Huang, Yi-Jie and Zhang, Youcai and Tian, Weiwei and Feng, Rui and Zhang, Yuejie and Xie, Yanchun and Li, Yaqian and Zhang, Lei},
  journal={arXiv e-prints},
  pages={arXiv--2310},
  year={2023}
}

@inproceedings{liu2023groundingdino,
  author       = {Shilong Liu and
                  Zhaoyang Zeng and
                  Tianhe Ren and
                  Feng Li and
                  Hao Zhang and
                  Jie Yang and
                  Qing Jiang and
                  Chunyuan Li and
                  Jianwei Yang and
                  Hang Su and
                  Jun Zhu and
                  Lei Zhang},
  title        = {Grounding {DINO:} Marrying {DINO} with Grounded Pre-training for Open-Set Object Detection},
  booktitle    = {ECCV},
  pages        = {38--55},
  year         = {2024},
}


@article{yang2024samurai,
  title   = {SAMURAI: Adapting Segment Anything Model for Zero-Shot Visual Tracking with Motion-Aware Memory},
  author  = {Cheng-Yen Yang and Hsiang-Wei Huang and Wenhao Chai and Zhongyu Jiang and Jenq-Neng Hwang},
  year    = {2024},
  journal = {arXiv preprint arXiv: 2411.11922}
}


@article{wang2024qwen2vl,
  title   = {Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution},
  author  = {Peng Wang and Shuai Bai and Sinan Tan and Shijie Wang and Zhihao Fan and Jinze Bai and Keqin Chen and Xuejing Liu and Jialin Wang and Wenbin Ge and Yang Fan and Kai Dang and Mengfei Du and Xuancheng Ren and Rui Men and Dayiheng Liu and Chang Zhou and Jingren Zhou and Junyang Lin},
  year    = {2024},
  journal = {arXiv preprint arXiv: 2409.12191}
}


@article{wang2024groundedvideollm,
  title   = {Grounded-VideoLLM: Sharpening Fine-grained Temporal Grounding in Video Large Language Models},
  author  = {Haibo Wang and Zhiyang Xu and Yu Cheng and Shizhe Diao and Yufan Zhou and Yixin Cao and Qifan Wang and Weifeng Ge and Lifu Huang},
  year    = {2024},
  journal = {arXiv preprint arXiv: 2410.03290}
}


@article{deng2024seq2time,
  title   = {Seq2Time: Sequential Knowledge Transfer for Video LLM Temporal Grounding},
  author  = {Andong Deng and Zhongpai Gao and Anwesa Choudhuri and Benjamin Planche and Meng Zheng and Bin Wang and Terrence Chen and Chen Chen and Ziyan Wu},
  year    = {2024},
  journal = {arXiv preprint arXiv: 2411.16932,}
}

@article{zhang2024llava-video,
  title   = {Video Instruction Tuning With Synthetic Data},
  author  = {Yuanhan Zhang and Jinming Wu and Wei Li and Bo Li and Zejun Ma and Ziwei Liu and Chunyuan Li},
  year    = {2024},
  journal = {arXiv preprint arXiv: 2410.02713}
}

@inproceedings{pite,
  title={PiTe: Pixel-Temporal Alignment for Large Video-Language Model},
  author={Liu, Yang and Ding, Pengxiang and Huang, Siteng and Zhang, Min and Zhao, Han and Wang, Donglin},
  booktitle={European Conference on Computer Vision},
  pages={160--176},
  year={2024},
  organization={Springer}
}
@article{timesuite,
  title={Timesuite: Improving mllms for long video understanding via grounded tuning},
  author={Zeng, Xiangyu and Li, Kunchang and Wang, Chenting and Li, Xinhao and Jiang, Tianxiang and Yan, Ziang and Li, Songze and Shi, Yansong and Yue, Zhengrong and Wang, Yi and others},
  journal={arXiv preprint arXiv:2410.19702},
  year={2024}
}

@inproceedings{moon2023querydependentvideorepresentationmoment,
  title={Query-dependent video representation for moment retrieval and highlight detection},
  author={Moon, WonJun and Hyun, Sangeek and Park, SangUk and Park, Dongchan and Heo, Jae-Pil},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={23023--23033},
  year={2023}
}

@inproceedings{yan2023unlocunifiedframeworkvideo,
  title={Unloc: A unified framework for video localization tasks},
  author={Yan, Shen and Xiong, Xuehan and Nagrani, Arsha and Arnab, Anurag and Wang, Zhonghao and Ge, Weina and Ross, David and Schmid, Cordelia},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={13623--13633},
  year={2023}
}

@article{munasinghe2023pgvideollava,
  title   = {PG-Video-LLaVA: Pixel Grounding Large Video-Language Models},
  author  = {Shehan Munasinghe and Rusiru Thushara and Muhammad Maaz and Hanoona Abdul Rasheed and Salman Khan and Mubarak Shah and Fahad Khan},
  year    = {2023},
  journal = {arXiv preprint arXiv: 2311.13435}
}

@article{wang2024elysium,
  title     = {Elysium: Exploring Object-level Perception in Videos via MLLM},
  author    = {Hang Wang and Yanjie Wang and Yongjie Ye and Yuxiang Nie and Can Huang},
  journal   = {ECCV},
  year      = {2024},
}

@article{munasinghe2024videoglamm,
  title   = {VideoGLaMM: A Large Multimodal Model for Pixel-Level Visual Grounding in Videos},
  author  = {Shehan Munasinghe and Hanan Gani and Wenqi Zhu and Jiale Cao and Eric Xing and Fahad Shahbaz Khan and Salman Khan},
  year    = {2024},
  journal = {arXiv preprint arXiv: 2411.04923}
}

@article{peng2024instit,
  title   = {Inst-IT: Boosting Multimodal Instance Understanding via Explicit Visual Prompt Instruction Tuning},
  author  = {Wujian Peng and Lingchen Meng and Yitong Chen and Yiweng Xie and Yang Liu and Tao Gui and Hang Xu and Xipeng Qiu and Zuxuan Wu and Yu-Gang Jiang},
  year    = {2024},
  journal = {arXiv preprint arXiv: 2412.03565}
}


@article{athar2024vicas,
  title   = {ViCaS: A Dataset for Combining Holistic and Pixel-level Video Understanding using Captions with Grounded Segmentation},
  author  = {Ali Athar and Xueqing Deng and Liang-Chieh Chen},
  year    = {2024},
  journal = {arXiv preprint arXiv: 2412.09754}
}

@article{yuan2025sa2va,
  title   = {Sa2VA: Marrying SAM2 with LLaVA for Dense Grounded Understanding of Images and Videos},
  author  = {Haobo Yuan and Xiangtai Li and Tao Zhang and Zilong Huang and Shilin Xu and Shunping Ji and Yunhai Tong and Lu Qi and Jiashi Feng and Ming-Hsuan Yang},
  year    = {2025},
  journal = {arXiv preprint arXiv: 2501.04001}
}

@article{yuan2024videorefer,
  title   = {VideoRefer Suite: Advancing Spatial-Temporal Object Understanding with Video LLM},
  author  = {Yuqian Yuan and Hang Zhang and Wentong Li and Zesen Cheng and Boqiang Zhang and Long Li and Xin Li and Deli Zhao and Wenqiao Zhang and Yueting Zhuang and Jianke Zhu and Lidong Bing},
  year    = {2024},
  journal = {arXiv preprint arXiv: 2501.00599}
}

@misc{deng2024seq2timesequentialknowledgetransfer,
      title={Seq2Time: Sequential Knowledge Transfer for Video LLM Temporal Grounding}, 
      author={Andong Deng and Zhongpai Gao and Anwesa Choudhuri and Benjamin Planche and Meng Zheng and Bin Wang and Terrence Chen and Chen Chen and Ziyan Wu},
      year={2024},
      eprint={2411.16932},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2411.16932}, 
}

@inproceedings{song2024moviechat,
  title={Moviechat: From dense token to sparse memory for long video understanding},
  author={Song, Enxin and Chai, Wenhao and Wang, Guanhong and Zhang, Yucheng and Zhou, Haoyang and Wu, Feiyang and Chi, Haozhe and Guo, Xun and Ye, Tian and Zhang, Yanting and others},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={18221--18232},
  year={2024}
}

@article{ye2024mplug,
  title={mplug-owl3: Towards long image-sequence understanding in multi-modal large language models},
  author={Ye, Jiabo and Xu, Haiyang and Liu, Haowei and Hu, Anwen and Yan, Ming and Qian, Qi and Zhang, Ji and Huang, Fei and Zhou, Jingren},
  journal={arXiv preprint arXiv:2408.04840},
  year={2024}
}
@misc{llava2024,
  author = {Yuanhan Zhang and Bo Li and Haotian Liu and Yong Jae Lee and Liangke Gui and Di Fu and Jiashi Feng and Ziwei Liu and Chunyuan Li},
  title = {Llava Next Video},
  howpublished = {\url{https://llava-vl.github.io/blog/2024-04-30-llava-next-video/}},
  year = {2024}
}

@misc{internvl2024,
  author = {Chen, Zhe and Wang, Weiyun and Tian, Hao and Ye, Shenglong and Gao, Zhangwei and Cui, Erfei and Tong, Wenwen and Hu, Kongzhi and Luo, Jiapeng and Ma, Zheng and others},
  title = {InternVL2: Better than the Best—Expanding Performance Boundaries of Open-Source Multimodal Models with the Progressive Scaling Strategy},
  howpublished = {\url{https://internvl.github.io/blog/2024-07-02-InternVL-2.0/}},
  year = {2024}
}
@misc{phi3v,
  author = {Microsoft.},
  title = {Phi-3.5 Vision Instruct},
  howpublished = {\url{https://huggingface.co/microsoft/Phi-3.5-vision-instruct}},
  year = {2024}
}

@inproceedings{chen2024internvl,
  title={Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks},
  author={Chen, Zhe and Wu, Jiannan and Wang, Wenhai and Su, Weijie and Chen, Guo and Xing, Sen and Zhong, Muyan and Zhang, Qinglong and Zhu, Xizhou and Lu, Lewei and others},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={24185--24198},
  year={2024}
}

@article{abdin2024phi,
  title={Phi-3 technical report: A highly capable language model locally on your phone},
  author={Abdin, Marah and Aneja, Jyoti and Awadalla, Hany and Awadallah, Ahmed and Awan, Ammar Ahmad and Bach, Nguyen and Bahree, Amit and Bakhtiari, Arash and Bao, Jianmin and Behl, Harkirat and others},
  journal={arXiv preprint arXiv:2404.14219},
  year={2024}
}
@inproceedings{fujita2020soda,
  title={SODA: Story oriented dense video captioning evaluation framework},
  author={Fujita, Soichiro and Hirao, Tsutomu and Kamigaito, Hidetaka and Okumura, Manabu and Nagata, Masaaki},
  booktitle={Computer Vision--ECCV 2020: 16th European Conference, Glasgow, UK, August 23--28, 2020, Proceedings, Part VI 16},
  pages={517--531},
  year={2020},
  organization={Springer}
}
@inproceedings{banerjee2005meteor,
  title={METEOR: An automatic metric for MT evaluation with improved correlation with human judgments},
  author={Banerjee, Satanjeev and Lavie, Alon},
  booktitle={Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization},
  pages={65--72},
  year={2005}
}
@inproceedings{
wu2024longvideobench,
title={LongVideoBench: A Benchmark for Long-context Interleaved Video-Language Understanding},
author={Haoning Wu and Dongxu Li and Bei Chen and Junnan Li},
booktitle={The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track},
year={2024},
url={https://openreview.net/forum?id=3G1ZDXOI4f}
}

@inproceedings{krishna2017dense,
  title={Dense-captioning events in videos},
  author={Krishna, Ranjay and Hata, Kenji and Ren, Frederic and Fei-Fei, Li and Carlos Niebles, Juan},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={706--715},
  year={2017}
}

@article{wu2025large,
  title={A large cross-modal video retrieval dataset with reading comprehension},
  author={Wu, Weijia and Zhao, Yuzhong and Li, Zhuang and Li, Jiahong and Zhou, Hong and Shou, Mike Zheng and Bai, Xiang},
  journal={Pattern Recognition},
  volume={157},
  pages={110818},
  year={2025},
  publisher={Elsevier}
}
