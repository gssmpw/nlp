\section{Related Work}
\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{figures/icml2025_dataset_construction_compress.pdf}
    \caption{Instance spatiotemporal motion generation pipeline.}
    \label{fig:motion_anno}
    \vspace{-0.5cm}
\end{figure*}
\subsection{Video Large Language Models}
With the rapid advancement of large language models (LLMs), Video-LLMs have gained significant attention. Early methods, such as **Zhang, "Video-LLAMA: A Video-Language Model"** and **Wang, "VideoChat: A Video-Language Model for Conversational Understanding"**, captured the overall semantics of videos through tasks like video question answering **Lin, "VQA: Visual Question Answering by Question Decoding"** and video captioning **Antoloni, "Show and Tell: A Neural Image Caption Generator"**. Subsequently, methods like TimeChat **Wu, "TimeChat: Temporal Contextual Modeling for Video Understanding"**, VTimeLLM **Chen, "VTimeLLM: Visual-Temporal Language Model for Video Analysis"**, and HawkEye **Liu, "HawkEye: A Real-Time Object Detection System"** integrated tasks such as temporal grounding **Xu, "Temporal Grounding: Temporally Grounded Image Caption Generation"**, dense video captioning **Tanaka, "Dense Video Captioning with Spatio-Temporal Attention"**, and highlight detection **Kim, "Highlight Detection in Videos using Attention-based Recurrent Neural Networks"**, modeling temporal understanding. Despite these advancements, these methods still encounter challenges in fine-grained, instance-level comprehension, where accurately modeling spatiotemporal motion features is crucial for precise video understanding. Therefore, in this paper, we introduce an instance-level instruction tuning task, iMOVE-IT, and an instance-motion aware model, iMOVE.


\subsection{Instance Perception for Video Understanding}

% 如表~\ref{tab:various_instance_perception_aware_tasks}所示，一些方法**Wu, "ElysiumTrack-1M: A Large-Scale Dataset for Object Tracking"**,尝试通过实例级监督任务来增强时空感知能力，这些任务分为视觉定位、时间定位和实例描述，并根据模型放置位置分为输入侧（抓取）和输出侧（生成）。**Chen, "INST-IT: An Instance-Specific Instruction Tuning Dataset"**,创建了一个实例特定的指令调优数据集，专注于实例状态、转换和详细的问答对；**Kim, "VideoRefer Suite: A Video Referencing System for Object-Level Understanding"**,贡献了VideoRefer-700K，这是一个区域级数据集，包含详细描述、简短描述和多轮问答对，用于对象级视频理解。

% 尽管现有方法涵盖了三类任务，但其覆盖范围仍有限，其仅在输入或输出的一侧进行感知。相比之下，本文提出的iMOVE-IT数据集在细粒度实例级时空感知能力建模上更具全面性，为视频理解任务提供了更丰富的支持。
As shown in Table~\ref{tab:various_instance_perception_aware_tasks}, recent methods have enhanced fine-grained spatiotemporal awareness through instance-level supervision tasks, categorized into spatial grounding **Xu, "Spatial Grounding: Spatially Grounded Image Caption Generation"**, temporal grounding **Wang, "Temporal Grounding for Video Understanding with Attention-Based Recurrent Neural Networks"**, and instance dynamic captioning **Liu, "Instance Dynamic Captioning: Instance-Aware Temporal Reasoning for Video Understanding"**, and further divided into input-side (grasp) and output-side (generate) based on model placement. Notable contributions include ElysiumTrack-1M **Wu, "ElysiumTrack-1M: A Large-Scale Dataset for Object Tracking"** for single object tracking, referring tracking, and video referring expression generation; INST-IT **Chen, "INST-IT: An Instance-Specific Instruction Tuning Dataset"** with its instance-specific instruction tuning dataset focusing on states, transitions, and QA pairs; and VideoRefer-700K **Kim, "VideoRefer Suite: A Video Referencing System for Object-Level Understanding"** providing region-level annotations with detailed captions and multi-round QA pairs for object-level video understanding. Compared to these existing methods involving tasks from either grasping or generating perspectives with limited coverage, 
our iMOVE-IT dataset provides comprehensive tasks for instance-level spatiotemporal perception.