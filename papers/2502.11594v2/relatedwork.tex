\section{Related Work}
\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{figures/icml2025_dataset_construction_compress.pdf}
    \caption{Instance spatiotemporal motion generation pipeline.}
    \label{fig:motion_anno}
    \vspace{-0.5cm}
\end{figure*}
\subsection{Video Large Language Models}
With the rapid advancement of large language models (LLMs), Video-LLMs have gained significant attention. Early methods, such as Video-LLAMA~\citep{video-llama} and VideoChat~\citep{videochat}, captured the overall semantics of videos through tasks like video question answering~\citep{video-chatgpt, zhang2024llava-video} and video captioning~\citep{sharegpt4video, zhang2024llava-video}. Subsequently, methods like TimeChat~\citep{timechat}, VTimeLLM~\citep{vtimellm}, and HawkEye~\citep{hawkeye} integrated tasks such as temporal grounding~\citep{charades-sta}, dense video captioning~\citep{activitynet}, and highlight detection~\citep{qvhighlights}, modeling temporal understanding. Despite these advancements, these methods still encounter challenges in fine-grained, instance-level comprehension, where accurately modeling spatiotemporal motion features is crucial for precise video understanding. Therefore, in this paper, we introduce an instance-level instruction tuning task, iMOVE-IT, and an instance-motion aware model, iMOVE.


\subsection{Instance Perception for Video Understanding}

% 如表~\ref{tab:various_instance_perception_aware_tasks}所示，一些方法~\citep{munasinghe2023pgvideollava, wang2024elysium, pite, peng2024instit, yuan2025sa2va}尝试通过实例级监督任务来增强时空感知能力，这些任务分为视觉定位、时间定位和实例描述，并根据模型放置位置分为输入侧（抓取）和输出侧（生成）。Elysium~\citep{wang2024elysium}提出了ElysiumTrack-1M，支持单目标跟踪、参考跟踪和视频参考表达式生成；INST-IT~\citep{peng2024instit}创建了一个实例特定的指令调优数据集，专注于实例状态、转换和详细的问答对；VideoRefer Suite~\citep{yuan2024videorefer}贡献了VideoRefer-700K，这是一个区域级数据集，包含详细描述、简短描述和多轮问答对，用于对象级视频理解。

% 尽管现有方法涵盖了三类任务，但其覆盖范围仍有限，其仅在输入或输出的一侧进行感知。相比之下，本文提出的iMOVE-IT数据集在细粒度实例级时空感知能力建模上更具全面性，为视频理解任务提供了更丰富的支持。
As shown in Table~\ref{tab:various_instance_perception_aware_tasks}, recent methods have enhanced fine-grained spatiotemporal awareness through instance-level supervision tasks, categorized into spatial grounding, temporal grounding, and instance dynamic captioning, and further divided into input-side (grasp) and output-side (generate) based on model placement. Notable contributions include ElysiumTrack-1M~\citep{wang2024elysium} for single object tracking, referring tracking, and video referring expression generation; INST-IT~\citep{peng2024instit} with its instance-specific instruction tuning dataset focusing on states, transitions, and QA pairs; and VideoRefer-700K~\citep{yuan2024videorefer} providing region-level annotations with detailed captions and multi-round QA pairs for object-level video understanding. Compared to these existing methods involving tasks from either grasping or generating perspectives with limited coverage, 
our iMOVE-IT dataset provides comprehensive tasks for instance-level spatiotemporal perception.