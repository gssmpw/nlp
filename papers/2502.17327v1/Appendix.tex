This \ifappendix{Appendix }\else{Supplementary }\fi provides additional details to complement the information presented in the main paper. While the main paper is self-contained, the details provided here offer further insights and clarifications.

In \Cref{sec:imp_detail}, we provide implementation details of \algoname, and in \Cref{sec:data}, we elaborate on our preprocessing and augmentation pipelines. Finally, in \Cref{sec:comp_subset} and present additional quantitative results beyond those in the main paper.


\section{Implementation Details} \label{sec:imp_detail}
The maximum topological distance we allow in $\dd$ is $d_{max}=6$, and our \emph{\tempattn} is applied on temporal windows of length $W=31$. For our model inputs, we allow maximum number of joints $J=143$. \emph{During training}, we use cropped sequences of $N=40$ frames. To enable our model handle higher frame positions and generate longer sequences, we incorporate positional encoding relative to the cropping index.
For training, we used batch size of 16 when training on the entire dataset, and a batch size of 8 to train in the data subsets.


\section{Data} \label{sec:data}
\myparagraph{Truebones Zoo dataset} In addition to the data misalignment issues discussed in the main paper, the dataset also contains vulnerabilities such as excessive dummy joints, qualitative artifacts like foot sliding and floating, and 20\% of the frames involve skeletons connected to the origin via an additional bone, resulting in artefacts such as walking or running in place. We address some of these issues as part of our data processing pipeline, which is detailed in the main paper and further extended in the following paragraph.

\myparagraph{Data Preprocessing}
In this section, we provide further details on the preprocessing steps mentioned in the main paper, as well as describe additional refinements applied to the dataset. 
As part of the alignment process, we ensure that all skeletons are properly grounded. This is achieved by using the textual descriptions of the joints to identify the foot joints of each skeleton. Based on their height in the rest pose, we determine the ground height for each skeleton and subtract it from the corresponding root height in the motion data. For skeletons that do not interact with the ground, such as flying birds or swimming fish, the ground height is determined by the position of the lowest joint in the rest pose.
Another important preprocessing step is ensuring that the rest poses of all skeletons are natural. This is essential for two key reasons. 
First, many animals feature a similar span of rotation angles in organs that have similar functionality, \eg, the forearm. We would like this span of rotations to constitute a manifold representing multiple animals.
Once all rotation angles are defined relative to a character's rest-pose, we have a common representation basis, hence the desired manifold can be obtained.

Second, the rest pose is encoded as a single frame within the motion sequence. To maintain consistency with the other frames, which represent natural poses, the rest pose must also exhibit a natural configuration.
To accomplish this, we transform all motion rotations so that they are relative to a natural rest pose, which can either be provided as an additional motion capture (mocap) file or selected from the skeleton's idle motion.

In addition to the alignment procedure, we also extract relevant information from the skeletons and motion data. First, we use foot joints labels to generate foot-contact indicators for each frame, which are concatenated with the motion features. Next, we compute the mean and standard deviation for each skeleton's frames and use these statistics to normalize the motions before feeding them into the model during training. 

\myparagraph{Input Preprocessing}
\balance
The input to our model is a skeleton $\mss = {\pp_\mss, \Rcal_\mss, \dd_\mss, \nn_\mss}$, derived from the raw rest pose of the character, represented as $(\mgg_\mss, O_\mss)$, along with the corresponding joint names. Both $(\mgg_\mss, O_\mss)$ and the joint names can be obtained from standard motion capture formats (\eg, bvh, fbx).

The skeletal features
$\mss = \{\pp_\mss, \Rcal_\mss, \dd_\mss, \nn_\mss\}$ are computed as follows: First, to compute $\pp_\mss$, we apply forward kinematics with zero rotations on $(\mgg_\mss, O_\mss)$ obtaining the global joint positions in the rest pose. These positions are then converted to root-relative coordinates. To align the rest pose with the format of individual frames in a motion sequence, we append to each root-relative position a 6D representation of zero rotation, zero velocity, and foot contact indicators. The topological conditions, $\Rcal_\mss$ and $\dd_\mss$, are derived through a traversal of the skeletal hierarchy $\mgg_\mss$. Finally, the joint names $\nn_\mss$ are extracted from the motion capture data and undergo text-preprocessing, which includes the removal of digits, symbols, irrelevant words, and redundant prefixes. Additionally, side indicators such as 'L/R' are replaced with 'Left/Right', non-English joint names are translated, and similar actions are standardized.



\myparagraph{Data augmentation}
\emph{Skeletal Augmentation} exposes our model to a wider variety of skeletons, as described in the Method section of the main paper. Next, we further elaborate about this process. The first augmentation we apply is \emph{joint removal}, which randomly removes up to 30\% of the joints from the skeleton, where feet joints are never removed to maintain physical correctness. For efficiency consideration, we exclude joints with more than a single child from the removal procedure. The second augmentation is \emph{joint addition}, which introduces a new joint at the midpoint of a randomly selected edge.
After removing or adding joints to the skeleton, we update $\Rcal_\mss$, $\dd_\mss$ and $\nn_\mss$ accordingly. Note that updating $\dd_\mss$ is computationally expensive with a complexity of $0(J^2)$, as it requires recomputing the path between each pair of joints in the DAG. 

\section{Comparison with Baselines on Subset Models} \label{sec:comp_subset}
\input{Tables/sub_datasets}
We provide a quantitative evaluation of \algoname trained on the data subsets defined in the Experiments section of the main paper. To maintain fairness in comparison, we train the adapted MDM baseline separately for each subset. Since SinMDM is independently trained for each skeleton, no additional adjustments are needed. Each model is evaluated using the corresponding skeletons from our benchmark that match the relevant data subset. The results, shown in \cref{tab:sub_datasets}, indicate that \algoname achieves the optimal coverage-diversity tradeoff compared to all other baselines presented.

