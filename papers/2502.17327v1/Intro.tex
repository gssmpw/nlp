\section{Introduction}\label{sec:intro}
Character animation is a fundamental task in computer animation, playing a crucial role in industries such as film, gaming, and virtual reality. 
Animating 3D characters is a complex and time-consuming task that requires manual high-skill effort. 
Typically, animation pipelines involve a \emph{unique} skeleton for each character, defining its motion span, over which the animation is carefully crafted. 

In recent years, neural network-based approaches have simplified the animation process, showing impressive results in tasks such as motion generation and editing \cite{dabral2022mofusion,tevet2023human,holden2016deep,zhang2024motiondiffuse}. However, most existing methods cannot handle different skeletons and focus on a single topology \cite{shafir2024human,kapon2023mas}, target skeletons that differ only in bone proportions \cite{tripathi2024humos, yang2023omnimotiongpt}, or rely on skeletal homeomorphism \cite{aberman2020skeleton}. 

While effective within their scopes, these methods overlook the broader opportunities presented by diverse character animation, which require handling a wide variety of skeletal topologies. Conversely, methods designed to handle multiple skeletons often lack scalability, relying on topology-specific adjustments such as additive functional blocks for each skeleton \cite{Li2024walkthedog} or entirely distinct instances of the model \cite{raab2024single, li2022ganimator}.

There are two main reasons keeping arbitrary skeleton animation generation largely under-explored. First, the irregular nature of the data, with skeletons varying in the number of joints and their connectivity, challenges standard methods for  processing and analysis. Second, the lack of datasets encompassing diverse topologies presents significant challenges for data-driven approaches.


In this work, we introduce \algoname, a diffusion framework designed to generate motions for arbitrary skeletal structures, as illustrated in \cref{fig:teaser}. \algoname is carefully designed to handle any skeleton in a general manner with no need for topology-specific adjustments. 


\algoname is based on a transformer encoder, specifically adapted for graph learning. 
While many works embed an entire pose in one tensor \cite{han2024amd,xie2023omnicontrol}, we embed each joint independently at each frame \cite{aberman2020skeleton, agrawal2024skel}, enabling capturing both joint interactions within the skeleton and universal joint behaviors across diverse skeletal structures.
\algoname applies attention along both the temporal and skeletal axes. Notably, the skeletal attention is between \textit{all} joints. This is in contrast to previous art, and is made possible thanks to our topological conditioning scheme; 
we integrate graph characteristics \cite{park2022grpe, ying2021do}, such as joint parent-child relations, into the attention maps.
Consequently, each joint has access to information from all skeletal parts while also being able to prioritize topologically closer joints.
Furthermore, to bridge the gap between similarly behaved parts in different skeletons, \algoname incorporates textual descriptions of joints into the latent feature representation. 

\algoname is trained on Truebones Zoo dataset \cite{truebones}, which includes motion captures of diverse skeletal structures.
We contribute a processed version, aligned with the popular HumanML3D~\cite{guo2022generating} representation, which will be made publicly available. Using quantitative and qualitative evaluations, we show that \algoname outperforms current art.

Our model demonstrates three forms of generalization in its generations: \emph{\Ingen} allows for new motion variants that preserve the character's original motion motifs; \emph{\Crossgen} 
facilitates generating motions that adapt motifs from several characters;
and \emph{\Unseengen} enables motion generation for skeletons not encountered during training.
Beyond its generative capabilities, \algoname's highly informative Diffusion Features (DIFT) \cite{tang2023emergent} 
enable
various downstream applications, including unsupervised correlation, temporal segmentation, and motion editing.

The approach presented here, and its ability to share information across characters, opens doors for 
more flexible generation, better equipped to learn and operate on more complex characters and scenarios, 
that better fit the real-world needs of 3D content creators.





