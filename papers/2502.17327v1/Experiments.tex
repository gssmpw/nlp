\section{Experiments}\label{sec:experiments}
\ifarxiv
\input{Figures/unseen}
\fi
\subsection{Dataset and Preprocessing} \label{sec:dataset}

The Truebones Zoo \cite{truebones} dataset comprises motion captures featuring 70 diverse skeletons, including mammals, birds, insects, dinosaurs, fish, and snakes. 
The number of motions per skeleton ranges from 3 to 40,
adding up to 1219 motions and 147,178 frames in total. 
The dataset includes variations in orientation, root definition, and scale. 
Additionally, the skeletons vary in joint order, naming conventions, and connectivity standards. 
To address these variations, we have performed comprehensive preprocessing of the data, including aligning all motions to the same orientation and average bone length, centering the first frame at the origin, and ensuring it is located on the ground.
This process is described in details in \supp{sec:data} and the processed data will be made available.






\myparagraph{Skeletal Subsets} 
In addition to experimenting with the full dataset, 
we categorize the skeletons into four groups based on their motion dynamics and train \algoname on these subsets, alongside a model trained on the entire dataset.
The four skeletal categories are \emph{Quadrupeds}, \emph{Bipeds}, \emph{Flying}, and \emph{Insects}. 
These subsets allow us to constrain \crossgen to characters with similar behavior.
Our visualizations illustrate generations from models trained on the entire dataset or sub-datasets, depending on the context (\eg, \cref{fig:unseen}). 



\subsection{Implementation details}
We use $T=100$ diffusion steps, $L=4$ \stt layers, and latent dimension $F=128$. 
We train the model using a single NVIDIA RTX A6000 GPU for 24 hours. Inference runs on an NVIDIA GeForce RTX 2080 Ti GPU. More implementation details can be found in \supp{sec:imp_detail}.

\subsection{Evaluation}
\myparagraph{Benchmark}
To evaluate \algoname, we introduce a benchmark comprising 30 skeletons randomly selected from those with cumulative frame counts ranging between 600 and 1200. The benchmark includes 43\% Quadrupeds, 17\% Bipeds, 23\% Flying, and 17\% Insects,  reflecting the relative proportions of these categories in the dataset. 

\myparagraph{Metrics} 
We report four metrics that measure different aspects of the generated motions, following \citet{raab2024single,li2022ganimator}. 
The metrics are calculated separately for each skeleton, and the mean and standard deviation across all tested skeletons are reported in the form \emph{$\text{mean}^{\pm\text{std}}$}. For each skeleton, we evaluate a number of samples proportional to its sample count in the dataset.
Let $M$,$G$ denote the group of ground truth (GT) and generated motions of the assessed skeleton, respectively. The metrics that we use are 
(a) \emph{coverage}, which is the rate of temporal windows in $M$, that are reproduced in $G$, (b) \emph{local diversity}, which is the average distance between windows in $G$ and their nearest neighbors in $M$, and (c) \emph{inter diversity}, the diversity between synthesized motions.
We define \emph{intra diversity} to be the diversity between sub-windows internal to a motion and define (d) \emph{intra diversity diff}, which is the difference between the intra diversity of $G$ and that of $M$.
Metrics (a) and (d) evaluate fidelity to the GT, while metrics (b) and (c) assess diversity. 
An ideal score features both high fidelity and high diversity.
High fidelity with low diversity suggests overfitting, while low fidelity with high diversity indicates divergence and noise.




\subsection{Baselines} 
To the best of our knowledge, no current works address such a diverse range of skeletal structures within a single model. Hence, we compare \algoname to adaptations of two baselines. The first is \emph{MDM} \cite{tevet2023human}, originally designed for a single humanoid skeleton. 
MDM uses per-frame embedding, so to match its representation format, we concatenate all joint features for each character, and pad them to a length of $J \times D$.
For fairness, we also concatenate the vectorized rest-pose embedding $\pp_{\mss}$ along the temporal axis as frame 0. Since MDM accepts textual conditions, we use the skeleton's name (e.g., Cat, Dragon) as the input text.
Additionally, since MDM's original configuration was designed for a dataset 14 times larger than ours \cite{Guo_2022_CVPR}, we reduced its latent dimension size to mitigate overfit.

The second baseline is \emph{SinMDM} \cite{raab2024single}, designed to be trained on a \emph{single} motion sequence. We modify it to enable training on multiple sequences of the same character, resulting in a separate model for each skeleton. 

\ifarxiv
\input{Figures/segment}
\input{Figures/inpainting}
\fi
\subsection{Quantitative Results}
\input{Tables/quant_comp}

\Cref{tab:quant_comp} shows a quantitative comparison of \algoname and the baselines.
\algoname outperforms MDM in all categories and SinMDM in all but coverage, which is expected since SinMDM is trained separately for each skeleton. 
Note the significant gap in diversity metrics, where the table shows \algoname generalizes well, while the others struggle to do so.
We also report the models' parameter count, showing ours uses fewer parameters, enabling lower computation and faster inference.
In \supp{sec:comp_subset}, we provide a comparison with the baselines on the data subsets, demonstrating our model's superiority on these as well.

\subsection{Qualitative Results}
Our supp. video reflects the quality of our results. 
It presents generated motions for various skeletons and comparisons to baselines.

\ifarxiv
\input{Figures/baseline_comp}
\fi
In \cref{fig:baseline_comp}, we show that
\algoname produces natural and lively motions while MDM produces static, jittery motions. Moreover, MDM's results 
in our video 
show jittery transitions and unnatural poses.
\Cref{fig:in_gen} and our supp. video show that \algoname can generate novel poses by effectively combining joints from different ground truth poses.
In contrast, SinMDM is limited to temporal in-skeleton generalization and cannot handle spatial composition, due to its reliance on per-frame features.
Moreover, since SinMDM trains a separate model per skeleton, it cannot feature cross-skeleton or unseen-skeleton generalization.
As accurate foot contact is one of the
major factors of motion quality, we follow 
 \citet{li2022ganimator,raab2023modi} and use an IK post-process to ensure proper contact.

\myparagraph{Unseen skeleton}
We present two unseen skeleton motions. One is a komodo dragon, generated by the \emph{Bipeds} model.
The second is a Cat, generated by a model trained on \emph{Quadrupeds}, excluding the cat.
\Cref{fig:unseen} and our supp. video demonstrate \algoname generalizes well to unseen skeletons, while adapted MDM under the same settings generates static and jittery motions. 


\subsection{Ablation}

\input{Tables/ablation}
In \cref{tab:ablation}, we explore three key components of \algoname's architecture. 
First, the results confirm that without access to topological information, the model struggles to prioritize joints based on their hierarchical relations. Omitting the incorporation of $\dd$ and $\Rcal$ leads to degradation in all metrics.
Next, excluding the rest pose $\pp_{\mss}$ produces inferior results, reinforcing the idea that $\pp_{\mss}$ encodes vital information about joint offsets and bone lengths.
Lastly, we examine cross-skeletal prior sharing via the addition of joint name embeddings. 
While \crossgen improves motion diversity, it introduces a tradeoff, as generated motions may exhibit motifs absent in the skeletonâ€™s ground truth, reducing coverage. Results show that removing joint name embeddings increases coverage but severely sacrifices diversity and cross-skeleton generalization.

\ifarxiv
\else
\input{Figures/unseen}
\fi

