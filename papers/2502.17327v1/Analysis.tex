\section{Analysis}\label{sec:analysis}

\subsection{Latent Space Analysis}
In this section, we examine \algoname's latent space and show that it features a unified manifold for joints across all skeletons.
We use DIFT \cite{tang2023emergent}, a framework designed for detecting correspondence in the latent space of models undergoing diffusion.
DIFT features are intermediate activations from layer $l_{corr}$, extracted during a single denoising pass on a sample that has been noised directly to diffusion step $t_{corr}$. These features serve as effective semantic descriptors for predicting correspondence.
Note that the values we choose for $l_{corr}$ and $t_{corr}$ align with those used in the original DIFT work.
Let $X^{ref}$ denote a reference motion, and let $X^{tgt}$ denote a motion in which we search for corresponding parts.
Let $S^{ref}, S^{tgt}$ denote their skeletons, respectively.

Our spatial and temporal correspondence results are illustrated in \cref{fig:spatial_cor,fig:temoral_cor} respectively, and in the supplementary video.

\myparagraph{Spatial Correspondence}
\input{Figures/joint_cor}
We show that manifold features of semantically similar skeletal joints across different characters are close to each other.
Our objective is to find the most similar \emph{joint} in $\mss^{ref}$ for each joint in $\mss^{tgt}$.
To achieve this, we extract DIFT features for both motions $X^{ref}, X^{tgt}$ at diffusion step $t_{corr} = 2$ and layer $l_{corr} = 0$, average them along the temporal axis, and obtain a single feature vector per joint.
Using cosine similarity, we detect the closest counterpart for each joint in $S_{tgt}$. 


\myparagraph{Temporal Correspondence}
\input{Figures/temporal_cor}
We show that \algoname can recognize pose-level similarities and identify analogous actions across different skeletons.
This time, our objective is to find the most similar \emph{frame} in $X^{ref}$ for each frame in $X^{tgt}$.
To accomplish this goal, we extract DIFT features at diffusion step $t_{corr}=3$ and layer $l_{corr}=1$, and average them along the skeletal axis, resulting in a single feature vector per frame.
We use cosine similarity to detect the closest counterpart for each frame in $X_{tgt}$. 

\ifarxiv
\input{Figures/in_gen}
\fi
\subsection{Generalization Forms}



We identify three forms of generalization in our generated motions.

\myparagraph{\Ingen} dubbed \ingen, refers to generalization within a specific skeleton, featured as both \emph{temporal composition} -- combining motion segments from dataset instances, and \emph{spatial composition} -- introducing novel poses by combining skeletal parts of ground truth poses. Notably, \emph{spatial composition} is enabled by our per-joint encoding, which provides the flexibility required for such diversity. In \cref{fig:in_gen} and in our supp. video, we showcase \algoname's \ingen and highlight how other methods, which embed the entire pose, fail to achieve a comparable variety.

\myparagraph{\Crossgen} dubbed \crossgen, 
is expressed through shared motion motifs across different characters. This form of generalization enables the adaptation of motion behaviors originally performed by other skeletons, as shown in \cref{fig:cross_gen} and our supp. video. 
When motions must strictly align with typical behaviors, the training dataset can be restricted accordingly.

\myparagraph{\Unseengen} 
extends to skeletons not encountered during training, and illustrated in \cref{fig:unseen} and the video.

