\section{Related Work}\label{sec:related_work}
\input{Figures/arch_overview}
\myparagraph{Skeletal variability in generative motion models }

\input{Tables/skeletal_similarity_typs}

We refer to four types of skeletal variability (\cref{tab:skeletal_sim}). The naming draws from terminology in the graph domain, hence we interchangeably use the terms joint and vertex, as well as edge and bone. 
A \emph{single} skeleton type refers to identical skeletons â€” that is, skeletons with the same vertices, connectivity, and edge lengths.
\emph{Isomorphic} skeletons correspond to isomorphic graphs, sharing vertices and edges but potentially differing in edge proportions.
\emph{Homeomorphic} skeletons may vary in structure, yet correspond to homeomorphic graphs, \ie,
use topologies obtained from the same primal graph by subdivision of edges. Specifically, homeomorphic skeletons share the same number of kinematic chains and end-effectors.
Finally, \emph{non-homeomorphic} skeletons vary in their structure and have no common primal graph. 

Most motion generative methods focus on a single skeletal structure \cite{raab2023modi, petrovich2021actor, karunratanakul2023gmd}. Others train on isomorphic skeletons \cite{zhang2023skinned,Villegas2021ContactAwareRO}, including works that use the SMPL \cite{loper2015smpl} 
body model
\cite{tripathi2024humos,jang2024geometry,petrovich2022temos} and SMAL \cite{zuffi20173d} 
body model
or its derivatives \cite{yang2023omnimotiongpt,rueegg2023barc}. A smaller portion of generative works support homeomorphic skeletons \cite{lee2023same,zhang2024skinned,cao2024car,studer2024factorized, ponton2024dragposer}. Among these works, some \cite{aberman2020skeleton} require a designated encoder and decoder per skeleton, and some \cite{zhang2024unified} 
offer a unified framework for all skeletons.

Only a handful of works can handle non-homeomorphic skeletons. \citet{martinelli2024moma} performs motion retargeting by learning a shared manifold for all skeletons, and decoding it to motions using learned skeleton-specific tokens.
The learned tokens capture the skeletal information of characters in the dataset, 
limiting the model's ability to generalize to skeletons unseen during training. Its results are shown exclusively on bipeds, leaving the applicability to other character families (\eg, quadrupeds, millipedes) unexplored.
WalkTheDog \cite{Li2024walkthedog} uses a latent space that encodes motion phases and accommodates non-generative motion matching. 

A different class of generative models bypasses the handling of the skeletal structure by generating motion directly from point clouds \cite{mo2025motion}, shape-handles \cite{zhang2023tapmo} or meshes \cite{song2023unsupervised,ye2024skinned,muralikrishnan2025temporal,zhang2024dnf}. These works demonstrate great flexibility in target character structure, but overlook the advantage of skeletons, which are more compact and semantically meaningful, easier to manipulate via rig-based animation, and compatible with physics engines \cite{tevet2024closd} and inverse kinematics systems. Some works \cite{wang2024mmr} perform automatic rigging after the generation, but automatic rigging often necessitates manual adjustments.

Finally, methods that support arbitrary skeletons \cite{raab2024single,li2022ganimator} involve a separate training process for each skeleton, exhibiting scaling issues and lacking \Crossgen.

\algoname addresses training on non-homeomorphic skeletons and is the only skeletal-based approach capable of generating natural, smooth motions on a diverse range of characters, including bipeds (e.g., raptor, bird), quadrupeds (e.g., dog, bear), multi-legged arthropods (e.g., spider, centipede), and limbless creatures (e.g., snakes). To the best of our knowledge, our work is the only one capable of accepting an input topology, including unseen ones, and generating motions based on that topology.









\myparagraph{Transformer-based Graph Learning}
Early versions of deep networks on graphs relied on convolutional architectures \cite{kipf2016semi}. The emergence of transformers has sparked a new avenue of research, integrating graphs and transformers.
GAT \cite{velivckovic2017graph}
replace the graph-convolution operation with a self-attention module, where attention is restricted to neighboring nodes. \citet{rong2020self} iteratively stack self-attention layers alongside graph convolutional ones to account for long-range interactions between nodes.
Unlike transformers in the language and imaging domains, and due to the irregular structure of graphs, these earlier works do not use positional encoding.

Subsequent works \cite{dwivedi2021generalization,kreuzer2021rethinking} linearize the graphs into an array of nodes and add absolute positional encoding to each node.
However, linearization is unnatural to the graph structure, requiring a reconsideration of the approach.

Encoding relative positional information has been explored to maintain positional precision while adhering to the graph's structure. Works using it \cite{ying2021transformers, shaw2018self, park2022grpe} integrate relative positional encoding into the attention map based on relative measures, such as shortest path distance between nodes or edge type.

The aforementioned approaches are \emph{discriminative}, applied to tasks such as regression and segmentation.
\algoname leverages the relative positional encoding approach for \emph{generative} tasks and tailors it to the \emph{motion}
domain. In particular, our work redefines edge types to capture joint relations within skeletal structures and considers a temporal axis, which is not present in the graph domain.



