
\section{Method}\label{sec:method}


\algoname is a diffusion model synthesizing motions for multiple different characters with arbitrary skeletons. Given a skeletal structure for input, it generates a natural motion sequence with high fidelity to ground-truth characters.
\algoname is based on a transformer encoder, specifically adapted for graph learning, as depicted in \cref{fig:arch}.

\subsection{Preliminaries} \label{sec:prelim}

\myparagraph{Motion Representation}
We represent motion as a 3D tensor $X\in \R ^ {N \times J \times D}$, where $N$ and $J$ are the maximum number of frames and joints across all motions in the dataset, and D is the number of motion features per joint. 
As motions vary in duration and skeletal structure, we pad the original number of frames and joints of each motion to match the maximum values $N$ and $J$, respectively.
We adopt a redundant representation, where each joint $j$ (except the root) consists of its root-relative position $p_j\in\R^3$, 6D joint rotation $r_j\in\R^6$ \cite{DBLP:journals/corr/abs-1812-07035}, linear velocity $v_j\in\R^3$, and foot contact label $fc_j \in \{0, 1\}$.
Altogether a joint is represented by $\{p_j,r_j,v_j,fc_j\}\in\R^{13}$, hence
 $D=13$. 
For the root joint, features include its rotational velocity, linear velocity and height, which are concatenated and zero-padded to match the size $D$.
Our representation is inspired by \citet{guo2022generating}; however, our approach maintains features at the joint level by representing each joint as a separate tensor, resulting in $J$ tokens per frame. In contrast, \citeauthor{guo2022generating} concatenate features from all joints into one tensor, resulting in a single token per frame. 


\myparagraph{Skeletal structure Representation}
In the context of 3D motion, \emph{topology} is a directed, acyclic, and connected graph (DAG). Adding geometric information to this graph makes it a \emph{skeleton}. 
We use the terms ``topology" and ``skeleton" interchangeably throughout this work, clarifying any distinction when necessary.
A \emph{rest-pose} is the character's natural pose, represented by $(\mgg, O)$, where $\mgg$ is a DAG defining the topological hierarchy and $O\in \R^{J\times 3}$ is a set of 3D offsets, specifying each joint's parent-relative position.
In our work, we represent a skeleton by $\mss = \{\pp_\mss, \Rcal_\mss, \dd_\mss, \nn_\mss\}$. The first term, $\pp_\mss \in \R^{J \times D}$, is the rest-pose, converted to the format of individual poses in the motion sequence. The second term, $\Rcal_\mss\in \N_0^{J\times J}$, is the joints relations, where $\Rcal_\mss[i, j]$ holds the relation type between $i$ and $j$. We allow six types of relations, which are \emph{child}, \emph{parent}, \emph{sibling}, \emph{no-relation}, \emph{self} and \emph{end-effector}. \emph{Self} and \emph{end-effector} are valid only in case $i=j$, and \emph{end-effector} specifies if the joint is a leaf in $\mgg_\mss$. 
The third term, $\dd_\mss\in \N_0^{J\times J}$, represents the graph distances, where $\dd_\mss[i,j]$ holds the topological distance between $i$ and $j$ in $\mgg_\mss$, up to a maximal distance $d_{max}$. 
The topological conditions, $\Rcal_\mss$ and $\dd_\mss$, are illustrated in \cref{fig:topo_cond}.
Finally, $\nn_\mss$ is the joints' textual descriptions, which are typically included in 3D asset formats (\eg, bvh, fbx). 

\subsection{Architecture}

\algoname is a generative Denoising Diffusion Probabilistic Model (DDPM) \cite{ho2020denoising}.
At each denoising step $t \in [1,T]$ it gets a noisy motion $X_t$ and a skeleton $\mss=\{\pp_\mss, \Rcal_\mss, \dd_\mss, \nn_\mss\}$ as input, and predicts the clean motion $\hat{X}_{0}$ \cite{tevet2023human} rather than the noise $\epsilon_t$.

\algoname consists of two primary components, illustrated in \cref{fig:arch}. The first is an \emph{Enrichment Block}, which integrates skeleton-specific information into the noised motion. The second is a \emph{\Stt Block}, which employs attention across both skeletal and temporal axes while embedding topological information into the skeletal attention maps.






\input{Figures/topo_cond}

\myparagraph{Enrichment block}
This block incorporates semantic information from the rest-pose $\pp_\mss$ and the joint descriptions $\nn_\mss$, into the noised sample $X_t$. 
It projects $\pp_\mss$ to feature lentgh $F$ and concatenates it with $X_t$ along the temporal axis, effectively making it frame 0. The joint descriptions $\nn_\mss$ are encoded by a T5 model, projected to length $F$, and added to their corresponding joint features across all frames.
Finally, the block outputs enhanced data of shape $\R^{(N+1) \times J \times F}$.
 
\myparagraph{\Stt block}
The inputs to this block are the embedded tokens of $X_t$ emitted from the \emph{Enrichment Block}, the diffusion step $t$, and the precomputed values $\dd_\mss$, $\Rcal_\mss$. 
The block comprises a stack of $L$ identical \emph{Skeletal Temporal Transformer} (STT) encoder layers, each consisting of three parts.
The first component is a \emph{\skelattn}, which performs spatial self-attention across joints within the same frame. Unlike concurrent approaches that limit attention or convolution to adjacent joints within the skeletal hierarchy, our method enables each joint to attend to all others, capturing long-range relations. To regain the local joint knowledge, we incorporate topology information $\Rcal_\mss$ and $\dd_\mss$ into the attention maps. This allows each joint to access information from all skeletal parts while also prioritizing topologically closer joints.


The second component is a \emph{\tempattn}, which applies self-attention along the temporal axis for each joint independently, observing its motion over time. 
To enhance efficiency and mitigate overfitting, the temporal attention is applied within a temporal window of length $W$. 
The third component is a feed-forward block.
Finally, the output is projected to the original motion dimension, enabling motion reconstruction.
\myparagraph{Topological Conditioning Scheme}
We extend transformers for graph-based learning by incorporating both graph topology and node interaction information through our \emph{Skeletal Attention} mechanism. Inspired by \emph{discriminative} works in the \emph{graphs} domain \cite{ying2021transformers}, \algoname introduces a novel method for \emph{generative} tasks, specifically tailored to the \emph{motion} domain.
We integrate graph properties directly into attention maps, enabling the structural characteristics of the graph to influence the learning process. 
Our work uses two types of node affinity, the topological distance, $\dd_\mss$, and relations, $\Rcal_\mss$, as detailed in \cref{sec:prelim}.
We incorporate the graph information into the attention maps \cite{park2022grpe}, by learning distinct query and key embeddings for distances, denoted by 
$E^\dd_q$, $E^\dd_k \in \R^{d_{max} \times F}$,
and embeddings for relation, denoted by 
$E^\Rcal_q$, $E^\Rcal_k \in \R^{6\times F}$, 
where $E^{(\cdot)}_q$ and $E^{(\cdot)}_k$ denote embeddings that relate to queries and keys, respectively, and $F$ is the latent feature size. 
These embeddings are used to form two new attention maps, $a^{\dd}$ and $a^{\Rcal}$ defined for a given pair of joints $i, j \in [J]$: 
    \begin{align}
        a^{\dd}_{ij}=q_i \cdot E^\dd_q [\dd_{ij}] + k_j \cdot E^\dd_k [\dd_{ij}],
    \end{align}
    \begin{align}
        a^{\Rcal}_{ij}=q_i \cdot E^\Rcal_q [\Rcal_{ij}] + k_j \cdot E^\Rcal_k [\Rcal_{ij}],
    \end{align}
where $q_i$, $k_j$ denote the $i$'th joint query and $j$'th joint key, respectively, and $[\cdot]$ denotes an index in the embedding matrix.
Finally, we incorporate graph information by adding the two attention maps to the standard attention map and scaling their sum:
\begin{align}
        a_{ij} = \frac{q_i \cdot k_j + a^{\dd}_{ij} + a^{\Rcal}_{ij}}{\sqrt{F}}.
\end{align}
The final attention score is computed by applying the standard row-wise softmax to $a_{ij}$.

\subsection{Training}\label{subsec:training}
\myparagraph{Data Sampling and Augmentations}
We train \algoname using minibatches sampled with a \emph{Balancing Sampler} to address the imbalanced nature of the data (described in \cref{sec:dataset}) and mitigate the dominance of specific skeletons. To further enhance generalization, we apply skeletal augmentations to the data samples, including randomly removing 10\% to 30\% of the joints and adding new joints at the midpoint of existing edges. Further details on our data augmentation are provided in \supp{sec:data}.

\myparagraph{Training Objectives}
Given a motion $X_0$ of skeleton $\mss$, its noised counterpart $X_t$, with diffusion step $t\sim [1,T]$, our model predicts the clean  motion,  $\hat{X}_{0} = \algoname(X_{t}, t, \mss) $.
Our main objective is defined by the \emph{simple} formulation \cite{ho2020denoising}, namely,
\begin{align}
   \mll_{simple} =  E_{t \sim [1,T]}\left\lVert \hat{X}_{0} - X_{0}\right\rVert^2_2.
\end{align}

The Mean Squared Error (MSE) over rotations does not directly correlate to their distance in the rotation space, hence we apply a geodesic loss \cite{Huang_2017_CVPR, tripathi2024humos} over the learned rotations. Let $r, \hat{r} \in \R^{N\times J \times 6}$ denote the 6D rotations of $X_0$ and $\hat{X}_0$ respectively. The geodesic loss is defined as follows: 
\begin{align}
    \mathcal{L}_{rot} = \sum_{n=1}^N {\sum_{j=1}^J {\arccos\frac{Tr(GS(r_{n,j})(GS(\hat{r}_{n,j})^T) - 1 }{2}}},
\end{align}
where $GS$ is the Gram-Schmidt process, used to convert 6D rotations to rotation matrices \cite{zhou2019continuity}, and $Tr$ is the matrix Trace operation. 
Overall, the final training objective is 
\begin{align}
    \mathcal{L} = \mll_{simple} + \lambda_{rot}\mll_{rot}.
\end{align}




