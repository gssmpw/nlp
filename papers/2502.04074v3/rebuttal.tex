\documentclass[10pt,twocolumn,letterpaper]{article}
\usepackage[rebuttal]{cvpr}

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}


% Import additional packages in the preamble file, before hyperref

\def\bv #1{\boldsymbol{\rm{#1}}}%%%%vector
\def\bm #1{\boldsymbol{#1}}%%%%%matrix

\def\etc{{etc}\onedot} \def\vs{{vs}\onedot}
\def\wrt{w.r.t\onedot} \def\dof{d.o.f\onedot}

\definecolor{SeaGreen2}{RGB}{67,205,128}
\definecolor{redd}{RGB}{255,106,106}
\definecolor{bluee}{RGB}{30,144,255}

\newcommand{\1}[2]{\noindent\textcolor{redd}{\textbf{R1:}} \textbf{#2}}
\newcommand{\2}[2]{\noindent\textcolor{bluee}{\textbf{R2:}} \textbf{#2}}
\newcommand{\3}[2]{\noindent\textcolor{SeaGreen2}{\textbf{R3:}} \textbf{#2}}

\newcommand{\one}[0]{\textcolor{redd}{\textbf{R1}}}
\newcommand{\two}[0]{\textcolor{bluee}{\textbf{R2}}}
\newcommand{\three}[0]{\textcolor{SeaGreen2}{\textbf{R3}}}

\definecolor{sred}{rgb}{0.8,0.0,0.0}
\definecolor{sgreen}{rgb}{0.2,0.6,0.15}
\definecolor{sblue}{RGB}{33,130,200}

\definecolor{sgreen}{RGB}{237, 115, 117}
\definecolor{hgreen}{RGB}{110, 202, 135}
\definecolor{hblue}{RGB}{69, 142, 247}

\newcommand{\cH}{\mathcal{H}}
\newcommand{\question}[1]{\noindent\textbf{\textcolor{sblue}{#1}}}
\newcommand{\reviewer}[1]{\noindent{\large \textbf{#1. Response to the Reveiwer #1 (R#1)}}}
\newcommand{\novel}[1]{\textbf{\textcolor{sgreen}{Novelty#1}}}


% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\definecolor{cvprblue}{rgb}{0.21,0.49,0.74}
\usepackage[pagebackref,breaklinks,colorlinks,allcolors=cvprblue]{hyperref}

% If you wish to avoid re-using figure, table, and equation numbers from
% the main paper, please uncomment the following and change the numbers
% appropriately.
%\setcounter{figure}{2}
%\setcounter{table}{1}
%\setcounter{equation}{2}

% If you wish to avoid re-using reference numbers from the main paper,
% please uncomment the following and change the counter value to the
% number of references you have in the main paper (here, 100).
%\makeatletter
%\apptocmd{\thebibliography}{\global\c@NAT@ctr 100\relax}{}{}
%\makeatother

%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\paperID{4072} % *** Enter the Paper ID here
\def\confName{CVPR}
\def\confYear{2025}

\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{Responses for ``3D Prior Is All You Need: Cross-Task Few-shot 2D Gaze Estimation"}  % **** Enter the paper title here

\maketitle
\thispagestyle{empty}
\appendix

The authors sincerely thank all the reviewers for their valuable comments.
\one: Reviewer \textit{reWG}, \two: Reviewer \textit{XrmV}, \three: Reviewer \textit{cPxr}.
We are encouraged that they found our task meaningful (\one, \three), interesting (\one, \two), and valuable to the community (\three).
We also appreciate their recognition of the novelty (\one, \two, \three), the interesting aspects (\one, \three), and the clarity of our writing (\one, \three).
In the following, we have addressed all the reviewer questions. 

% 1. The training procedure includes several alternative operations. It would be clear if the authors could summarize it in an Algorithm.
\1{Q1.}{Summary of training procedure.} We thank for the kind suggestion. 
%Our method dynamically generates pseudo-labels during each interaction for model fine-tuning. 
We have already provided a summary in the supplementary material due to the paper page limit.

% Some details are missing in the experimental settings.

% What training data are used to pre-train the 3D gaze estimation model H_3D（；\beta_0）?
% Are the proposed method pre-trained and trained (with few shots) using data from the same dataset or different datasets?
\1{Q2.}{Pre-training data.}
We thank the reviewer for the insightful comment.
Our work utilizes the Gaze360 dataset for pre-training, which is distinct from the few-shot sets. This setup aligns well with real-world scenarios and demonstrates the advantages of our experimental design. For instance, in real-world applications, users can download a pre-trained model online, collect a few samples on their own device for fine-tuning, and then deploy the model locally.

% In Table 2, the methods in the second row use training samples from all the datasets. However, EFE、IVGaze and the proposed methods only use 10 samples. Are the 10 samples in the same dataset as the test set?
\1{Q3.}{Details in Table 2.}
Using MPIIGaze as an example, the second-row methods are trained on MPIIGaze, whereas EFE, IVGaze, and our method use Gaze360 for pre-training and only 10 MPIIGaze samples for few-shot training. Despite this, our method shows competitive performance with second-row methods, demonstrating its effectiveness. 
We have added this clarification in the revised manuscript.

%Although the task is meaningful and the authors provide a solution, I wonder if the process could be simplified. Is it necessary to fine-tune H_3D during the few-shot learning? Finetuning H_3D makes the output of H_3D not in the original camera coordinate system. To address the issue, the authors further introduce a transformation T to transform the output back to the camera coordinate system and design an alternative updating procedure. Why not learn the r and t with frozen H_3D and simplify the procedure? I am also curious about the experimental results if the H_3D is frozen.

\1{Q5}{Fine-tuning $\cH_{3D}$.}
We thank the reviewer for raising this interesting discussion.
Freezing $\cH_{3D}$ will lead to model collapse for two reasons: 1) The model contains only six additional parameters beyond $\cH_{3D}$, increasing the fitting complexity.
2) $\cH_{3D}$ encounters cross-environment issues, resulting in initial gaze estimation errors, which are difficult to resolve by simply changing the screen pose.
However, we believe that this process can be simplified \eg, using LoRA to reduce the number of parameters or further improving the projection module. These are promising research directions for this work, further highlighting the potential of our proposed task.
We will add this discussion into our manuscript.

%1.	The technical novelties of the proposed method is limited as the proposed 2D projection from 3D gaze is not new.
\2{Q1}{Novelty on 2D projection.}
We thank the reviewer for this comment.
Our work aims to mitigate the domain gap between 3D and 2D gaze estimation, which is a novel and challenging issue.
To tackle this, we leverage the 2D projection, a well-established concept in geometric computation, which might have caused some misunderstanding.
We would like to emphasize that our projection fundamentally differs from the conventional projection due to the assumption of unknown screen poses. 
Unlike traditional methods that fail under such conditions, our approach incorporates learnable parameters to represent the screen pose. This distinction was also noted as a strength by both R1 and R3.

%2.	The discussion of the pseudo-labelling method is unclear. It is unclear what 3D flipping is performed and why its 2D gaze is the same. It is also unclear if 3D flipping requires corresponding 3D gazes, which is not possible to recover from their 2D gazes. In addition, the experiments seem to suggest the pseudo-labeling term is not very effective on some datasets.

\2{Q2}{Dynamic pseudo-labelling.}
We thank the reviewer for the comment. We will revise our manuscript for clear statements.
Specifically, 3D flipping refers to horizontally flipping the face image and adjusting the 3D label by negating the x-coordinate. We respectfully clarify that 3D flipping and 2D flipping are distinct due to the screen projection.
In this work, we convert 2D gaze into 3D gaze and then perform 3D flipping.
As the reviewer pointed out, this conversion is challenging due to the unknown screen pose. 
To tackle it, we use the differentiable projection module within our framework, which includes learnable screen poses.

Notably, we observed that directly reprojecting 2D to 3D using learnable screen pose brings large performance drop due to shifted coordinate systems.
To overcome this, we define two anchors in both the original and shifted coordinate systems and compute transformation matrix. 
This approach represents a key technical contribution of our work.

% The proposed method depends on the pre-trained 3D gaze model, its performance can vary with devices. Experiments should include a study on the sensitivity of their method to different pre-trained 3D gaze models

\2{Q3}{Experiment with different models.} We already conducted experiments with other models, as shown below, demonstrating that our strategy performs reasonably well across different models. ResNet50 shows relatively larger error due to the increased number of parameters. Some efficient fine-tuning methods, such as  LoRA, could further enhance performance.
We will provide more complete experimental results with discussion in the final version.

\vspace{-4mm}

\begin{table}[h]
	\setlength\tabcolsep{6pt}
	\renewcommand\arraystretch{1}
	\centering
	%\small
	%\caption{We conducted a within-dataset evaluation of our approach using IVGaze and also compared them on other datasets. Our method achieved a good result on IVGaze, which supports the validity of our data collection procedure. We mark the \underline{highest} value among compared methods.}
	\begin{tabular}{cccc}
		\toprule[1pt]
            &ResNet18 &ResNet50 &GazeTR\\
            \hline
            MPIIGaze (mm)&63.8 & 67.1& 56.7\\
		\bottomrule[1pt]
	\end{tabular}
	\label{tab:within}
\end{table}
\vspace{-4mm}


%-------------------------------------- R3 ---------------------------------------


%I think more evaluation of the gap between the 3D gaze estimation and 2D gaze estimation can be provided:
%	For example, in Table 3, I am curious about the reason why the "Direct Projection" shows a very large error. If the pre-trained 3D gaze estimation is accurate enough, this Euclidean distance should not be very large. This may indicate that there are already domain gaps between Gaze360 and EVE/MPIIGaze even in 3D gaze estimation. Therefore, this gives me a feeling that the adaptation not only adapts between 3D, but also adapts between Gaze360 and EVE/MPIIGaze.
% Therefore, I would suggest to do some within-domain experiments: for example, splitting MPIIGaze, using MPIIGaze-Train 3D label to pre-train a 3D gaze estimator, and then adapting to 2D using a few samples from MPIIGaze-Test.


\3{Q1.}{Within-domain experiments}
We thanks for the suggestion.
We have conducted such experiment on MPIIGaze. Specifically, we pre-trained 3D models following the leave-one-person-out protocol.
The 3D models achieved $4.4^\circ$ on MPIIGaze.
Next, we adapted these models for 2D estimation, resulting in 48.8 mm accuracy, which shows $15\%$ improvement ($48.8~\mathrm{vs}.~56.7$) compared to cross-domain pre-trained models.
Notably, the 2D performance 48.8 mm typically corresponds to 3D performance $4.1^\circ$ on MPIIGaze.

This result shows: 1) Our work successfully adapts  3D models for the 2D task, even achieving better performance, which validates the effectiveness of our method.
2) The accuracy can be further improved by enhancing 3D models, demonstrating a key advantage of our work: breakthroughs in 3D gaze models can directly benefit 2D applications.


%	I am wondering the reason why the RAT [5] does not work well because I feel it should also be helpful because it is a similar image-augmentation strategy as the flipping.
%	One observation from me: The planar rotation of 2D images is not guaranteed to be equivalent to the rotation of the 3D gaze vector (e.g., when the gaze vector is facing toward the camera center, the rotation of the roll-axis won’t change the vector direction).
%	I am not sure if this is the main reason, as the implementation details are not shown.

\3{Q2.}{Result of RAT.}
We thank the reviewer for the comment and agree that planar rotation is one contributing factor. We would like to highlight two additional reasons:
1) Gaze datasets apply normalization to eliminate roll rotation, whereas RAT performs image rotation outside the data space while flipping operates within it.
2) Rotation does not significantly expand the training space. For instance, when a user gazes at one side of the screens, flipping can generate a complementary data point on the opposite side, while rotation creates similar points near the original ones.


 
\end{document}

