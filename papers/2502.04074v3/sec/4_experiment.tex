
\section{Experiment}

\subsection{Setup}
In this paper, we propose a cross-task few-shot 2D gaze estimation task. We first build the evaluation benchmark.

\noindent\textbf{Datasets:} We evaluate methods on three datasets: MPIIGaze~\cite{Zhang_2017_CVPRW}, EVE~\cite{park_2020_eccv}, and GazeCapture~\cite{Krafka_2016_CVPR}. These datasets were collected in different devices, including laptops, desktop computers, and mobile devices. By assessing performance across these datasets, we demonstrate the generalization capability of methods across various devices.

\noindent\textbf{Data Preprocessing:}
Image normalization~\cite{Cheng_2024_pami} is usually used to enhance 3D gaze estimation performance.  In our work, we utilize the normalized images provided by the MPIIGaze and EVE datasets, and implement the method~\cite{Zhang_2018_etra} for normalizing the GazeCapture. Note that, the normalization changes 3D gaze with a rotation matrix.
Although our work does not use the 3D label, the predicted 3D gaze should be transformed back for projection.
Furthermore, the MPIIGaze dataset augments 3D gaze estimation data by flipping images, which is not applicable for 2D gaze estimation. We exclude the flipped images for consistency.
The EVE dataset provides videos along with corresponding gaze trajectories. We  sample one frame for every 20 frames to construct the benchmark.
We sample 20 subjects in GazeCapture dataset, ensuring that each has at least 500 images. We clean the dataset to remove images without face. Notably, four of the 20 subjects used a tablet for data collection, while the rest used phones.
Please refer the supplementary materials for more details.

% 27 in GC, 33 in EVE and 43% in MPIIGaze
\begin{table}[t]
    \arrayrulecolor[rgb]{0,0,0}
    \setlength\tabcolsep{1pt}
    
    \renewcommand\arraystretch{1.0}
    \small
    \caption{Quantitative evaluation. Our method achieves best result among comparison methods. We also report the performance of 2D gaze estimation methods in the second row for reference.\vspace{-2mm}}
      \centering
        \begin{tabular}{l|cccc}
        \toprule[1.0pt]
       Method & \makecell{Training\\ Samples} & \makecell{EVE\\\cite{park_2020_eccv}} &\makecell{MPIIGaze\\\cite{Zhang_2015_CVPR}}  & \makecell{GazeCapture\\\cite{Krafka_2016_CVPR}}\\
        \hline
        iTracker~\cite{Krafka_2016_CVPR}&\multirow{5}{*}{\makecell{All \\dataset}}&-&-&26.8\\
        EyeNet~\cite{park_2020_eccv}&&49.7&-&-\\
        Full-Face~\cite{Zhang_2017_CVPRW}& &38.6& 42.0&-\\
        AFF-Net~\cite{Bao_2020_ICPR}& &-&39.0&19.6\\
        EFE~\cite{Balim_2023_CVPR} & & 38.5& 38.9&20.5\\
        
        \hline
        EFE~\cite{Balim_2023_CVPR}&10& 64.9 \textcolor{red}{$\blacktriangledown 33\%$ } & 100.2 \textcolor{red}{$\blacktriangledown 43\%$ }& 48.5 \textcolor{red}{$\blacktriangledown 26\%$ } \\
        IVGaze~\cite{cheng2024ivgaze}& 10&177.7 \textcolor{red}{$\blacktriangledown 75\%$ } &132.2 \textcolor{red}{$\blacktriangledown 57\%$ }&68.1 \textcolor{red}{$\blacktriangledown 47\%$ }\\
        \rowcolor{rowcolor}Ours&10&43.4 & 56.7& 35.7\\
        \bottomrule[1.0pt]
    \end{tabular}
     \label{tab:exp1}
     \vspace{-4mm}
\end{table}

\noindent\textbf{Evaluation Metric:}
We perform person-specific evaluation and report the average performance across subjects for comparison. 
Performance is measured as the Euclidean distance (in mm) between predictions and ground truth, where lower values indicate better accuracy.

%-------------------------------------------------------------------------

\subsection{Quantitative Comparison}
We first compare our method with existing approaches EFE~\cite{Balim_2023_CVPR} and IVGaze~\cite{cheng2024ivgaze}. EFE is an end-to-end gaze estimation method that includes a projection module to convert 3D gaze predictions into 2D gaze. IVGaze utilizes a basis tri-plane for projection, followed by a lightweight transformer to refine the projection points.
For a fair comparison, we re-implement both methods using the same 3D gaze estimation network and pre-trained weights as our method. Our goal is to evaluate the performance differences resulting from different projection strategies. Notably, EFE requires screen calibration for the projection; to ensure fairness, we set these screen parameters as learnable and initialize them with the same values used in our method.
The results of these comparisons are presented in \tabref{tab:exp1}.

IVGaze includes a transformer to refine projection points. While this transformer performs well when trained on the full dataset, it struggles with limited data, leading to underfitting when trained on just 10 samples. This results in poor performance on the EVE and MPIIGaze datasets, highlighting the advantage of our approach.
In contrast, our method avoids the use of complex architectures that can suffer from underfitting in few-shot learning tasks. Instead, we directly model the projection process, leading to superior performance.
On the other hand, EFE demonstrates reasonable performance, but our method achieves over $25\%$ improvement across all three datasets. This significant boost is attributed to our more comprehensive modelling of the projection process, which reduces fitting complexity and naturally enhances overall performance.


We also report the performance of 2D gaze estimation methods trained on the entire dataset for reference. Note that they are not directly comparable to our method since both the training and test sets differ. These results are summarized in the second row of \figref{tab:exp1}. Our method achieves similar performance using only 10 images.                                                               
\begin{table}[t]
    \arrayrulecolor[rgb]{0,0,0}
    \setlength\tabcolsep{4pt}
    
    \renewcommand\arraystretch{1.0}
    \small
    \caption{Comparison with different 3D to 2D adaption strategy. We direct project 3D gaze to 2D gaze using the known screen pose without fine-tuning, which shows the advantage of our learning framework. We directly learn 2D gaze from 3D gaze with MLP, which highlights the challenges in the adaption from 3D model to 2D gaze estimation. We also show the performance when the learnable parameters is set as known pose in our method.\vspace{-2mm} }
      \centering
        \begin{tabular}{l|cccc}
        \toprule[1.0pt]
         Strategy& EVE &MPIIGaze &GazeCapture\\
        \hline
        Direct Projection &80.5& 101.9 & N/A\\
        % Direct Projection ($\leq$ 1500) &85.3 &78.1&N/A\\
        Direct Learning  &180.6&133.9&74.23\\
        Direct Learning (with $\bo$ ) &116.6&108.2&149.7\\
        \hline
        Learning with Known Pose&39.4&56.6&N/A\\
        \rowcolor{rowcolor}Ours& 43.4& 56.7&35.7\\
        \bottomrule[1.0pt]
    \end{tabular}
    \vspace{-2mm}
     \label{tab:diffproj}
\end{table}


\subsection{Comparison with Different Adaption Strategy}
In this section, we evaluate the accuracy of different adaption strategies for obtaining 2D gaze from 3D predictions.

\begin{table}[t]
    \arrayrulecolor[rgb]{0,0,0}
    \setlength\tabcolsep{4pt}
    
    \renewcommand\arraystretch{1}
    
    \small
    \caption{We perform an ablation study to evaluate the impact of the dynamic pseudo-labeling strategy (PS-Label) and the loss to minimize uncertainty across jittered images ($\mathcal{L}_{unc}$). Both the two modules contribute to performance improvements.\vspace{-2mm}
    }
      \centering
        \begin{tabular}{ccc|cccc}
        \toprule[1.0pt]
        Proj. &PS-Label & $\cL_{unc}$ & EVE &MPIIGaze &GazeCapture\\
        \hline
      % \checkmark & & &  & 17.76&13.22 &29.58\\
       \checkmark&&&46.6 & 60.3 &36.8 \\
        \checkmark &\checkmark& &45.3 &57.9 & 35.7 &  \\
         \checkmark&\checkmark&\checkmark&43.4 & 56.7& 35.7\\
        \bottomrule[1.0pt]
    \end{tabular}
     \label{tab:ab}
     \vspace{-2mm}
\end{table}

\noindent \textbf{Direct Projection:}
We directly project the 3D gaze predictions from our pre-trained 3D gaze estimation network onto the screen using the known screen pose, providing a baseline performance measure for the network. This is not performed on GazeCapture, as it lacks reliable screen pose.




\noindent \textbf{Direct Learning:}
We retain the architecture of the 3D gaze network and directly fine-tune it using the 2D annotations. Additionally, we concatenate the gaze origin $\bo$ with the predicted gaze and use a MLP to map them to 2D gaze predictions. We then fine-tune this extended network and report the performance in Direct Learning (with $\bo$).

\noindent \textbf{Learning with Known Pose:}
 Our method assumes the screen pose is unavailable. In this strategy, we change the learnable parameters as the ground truth screen pose.

The result is shown in \tabref{tab:diffproj}. 
The Direct Projection method struggles to perform effectively on the EVE and MPIIGaze datasets without fine-tuning. However, integrating it into our framework yields over $40\%$ improvement, demonstrating the critical role of our learning framework.
The Direct Learning strategy, on the other hand, fails to achieve reasonable performance due to the substantial domain gap between 3D and 2D gaze estimation. 
We compare its performance with Direct Projection. The learning strategy does not show any performance gains, which highlights the challenge of adapting 3D gaze models to 2D tasks.
Even when the gaze origin is included as an additional feature, the limited training data makes it challenging for the model to learn the complex mapping. In contrast, our framework leverages physics-based differentiable projection, enabling it to achieve superior performance.
The Learning with Known Pose method outperforms our method due to access to the known screen pose, highlighting the importance of accurate screen pose information for 2D gaze estimation.



\begin{figure}[t]
	\begin{center}
		\includegraphics[width=\columnwidth]{exp/04_Aug.pdf}	
	\end{center}
    \vspace{-4mm}
	\caption{We compare the performance across different pseudo-labelling strategies. The red bar represents the projection without pseudo-labelling, serving as a baseline for comparison. We evaluated our method without the transformation $\cT$. The unreliable pseudo-labels lead to significant performance drop on the MPIIGaze and EVE. Interestingly, omitting $\cT$ led to improved results on the GazeCapture dataset. We found that this was because the initial screen pose happened to be same as the actual screen pose. \vspace{-4mm}}
	\label{fig:plcom}
\end{figure}

\subsection{Ablation Study}
We perform an ablation study to demonstrate the contribution of each module in our work. We first evaluate the performance when only the projection module is added to the pre-trained 3D gaze estimation network and fine-tuned. The results are shown as \textit{Proj.} in \tabref{tab:ab}. Compared to the results in \tabref{tab:diffproj}, the projection module provides a significant performance improvement as it explicitly modelling the projection process, which effectively bridges the gap between 3D and 2D gaze estimation.
Next, we introduce our dynamic pseudo-labeling strategy and minimize the uncertainty across jittered images. Both mechanisms bring performance improvements across all datasets.

The dynamic pseudo-labeling strategy is a key contribution of our work. To better understand its impact, we conduct a detailed comparison, as shown in \figref{fig:plcom}. We perform an ablation on the learning transformation $\mathcal{T}$ in our strategy. The results show a significant performance drop on the MPIIGaze and EVE datasets without $\mathcal{T}$, as unreliable pseudo-labels can cause model collapse during learning, especially with small training dataset sizes.
Interestingly, we observe improved performance on the GazeCapture dataset without using $\mathcal{T}$. The authors of GazeCapture create a unified prediction space for 2D gaze, centered at the phone camera position. Our model initializes the screen pose as $\bt = (0, 0, 0)$, making the initial pose closely approximate the real one. However, it is important to note that such cases are uncommon in real-world scenarios.
Our method first converts 2D gaze to 3D space and learns $\mathcal{T}$ to align this space with the camera coordinate system. When the screen pose aligns exactly with ground truth, the 3D space already corresponds to the camera coordinate system. To establish the alignment, we use the predictions from the 3D gaze network as anchors for the camera coordinate system, which may introduce some bias. Nonetheless, our method demonstrates performance improvements compared to methods without pseudo-labeling.

We also implement existing method RAT~\cite{Bao_2022_CVPR}, which assigns pseudo-label for rotated images.
We convert 2D gaze into 3D gaze using learnable screen parameters, and perform RAT to augment training. RAT cannot bring performance improvement compared with the baseline.

\begin{figure}
    \begin{minipage}{0.5\linewidth}
        \centering
        \includegraphics[width=\linewidth]{exp/02_num.pdf}
        \caption{Performance with different number of training images.}
        \label{fig:num}
    \end{minipage}
    \quad
    \begin{minipage}{0.43\linewidth}
    \centering
    \resizebox{\linewidth}{!}{
            \begin{tabular}{|c|c|}
                \hline
                \makecell{\#Training\\Images} & \makecell{Speed\\(sec/epoch)} \\
                \hline
                3 & 0.89 \\
                5 & 0.90 \\
                10 & 0.91 \\
                20 & 0.96 \\
                50 & 1.16 \\
                \hline
            \end{tabular}
    }
    \captionof{table}{The model training time with different number of training samples.}
    \label{tab:num}
\end{minipage}
\vspace{-5mm}
\end{figure}

\subsection{Different Numbers of Training Images }
In this section, we evaluate the effect of the number of training images on model performance. We experiment with different numbers of training images set to 3, 5, 10, 20, and 50, respectively. The performance is assessed across all three datasets, with results depicted in \figref{fig:num}. As shown, increasing the number of training images consistently improves the model performance.

Additionally, we measure the model training time when using varying numbers of training images, as summarized in \tabref{tab:num}. On average, each epoch takes approximately 0.9 seconds to process. Since our method does not require a large dataset, all images can be efficiently processed within a single epoch. With a total of 80 epochs, the complete training time is approximately 1.2 minute. Notably, this timing was tested in a Python environment and could be further optimized to achieve even faster performance with specific optimizations. This demonstrates significant real-time application potential for our method. 


\subsection{Repeatability Experiment}
In this section, we conduct a robustness evaluation by training our method 10 times using different training samples in MPIIGaze to assess the impact of sample variability on model performance. We evaluate the performance on all 15 subjects for each trial and report the performance distribution. The results are visualized in a boxplot in \figref{fig:repeat}.
The horizontal axis represents each of the 10 trials and each trial contains performance of 15 subjects. The box depicts the interquartile range ($25\%$ to $75\%$), while the error bars cover the entire performance distribution. The triangle symbol indicates the average performance, and the black line represents the median performance.
The average performance across all 10 trials is $55.6$, which is slightly better than our previously reported value of $56.7$. These results demonstrate the stability and robustness of our method despite variations in the training samples.


\begin{figure}[t]
	\begin{center}
		\includegraphics[width=\columnwidth]{exp/01_repeat.pdf}	
	\end{center}
    \vspace{-2mm}
	\caption{We train our method 10 times using different image samples in MPIIGaze for robustness evaluation.  The horizontal axis corresponds to each of the 10 trials, while each bar shows the accuracy distribution across 15 subjects.  The box depicts the interquartile range ($25\%$ to $75\%$), while the error bars covers the entire accuracy distribution. The average accuracy across  10 trials is $55.6$, demonstrating the stability and robustness of our method. \vspace{-6mm} }
	\label{fig:repeat}
\end{figure}


\subsection{The Trajectories of Pseudo-Label}
Our method contains a dynamic pseudo-labeling strategy to assign pseudo 2D labels for flipped images. To gain deeper insights into this process, we visualize the trajectories of the pseudo-labels over the course of 80 epochs in \figref{fig:trajectory}.
In addition, we compare the effect of our transformation strategy by plotting the pseudo-label positions without the transformation, \ie, the difference  between \eqnref{equ:flip} and \eqnref{equ:newflip}. 
Both approaches share the same initial pseudo-labels. For reference, we also compute the ground truth labels using the calibrated screen pose for flipped images.

As shown in  \figref{fig:trajectory}, the initial pseudo-labels have a significant offset from the ground truth. However, our method dynamically updates the pseudo-labels based on the fine-tuned network, progressively aligning them closer to the ground truth with each iteration. By the end of training, the pseudo-labels have only minimal offsets from the ground truth, demonstrating the effectiveness of our approach.
In contrast, the strategy without transformation fails to produce reliable pseudo-labels, leading to consistently large offsets from the ground truth. 

\begin{figure}[t]
	\begin{center}
		\includegraphics[width=\columnwidth]{exp/03_trajectory.pdf}	
	\end{center}
    \vspace{-4mm}
	\caption{ We visualize four trajectories of the pseudo-labels in our dynamic pseudo-labeling strategy. The ground truth for flipped images is computed using known screen pose. It is evident that our method progressively aligns the pseudo-labels closer to the ground truth. Additionally, we plot the pseudo-labels without applying $\cT$ in our strategy, which shows a failure to produce reliable pseudo-labels, resulting in significant deviations from the ground truth. \vspace{-4mm}}
	\label{fig:trajectory}
\end{figure}

\section{Conclusion and Discussion}
In this work, we introduce a novel cross-task few-shot 2D gaze estimation method. By leveraging few-shot 2D samples, we adapt a 3D gaze model to 2D gaze estimation on unseen devices. Since the 3D gaze network is trained in 3D space without being tied to specific devices, it theoretically maintains robust performance across different platforms. Our experiments validate this by proving results on three datasets. Besides, the adaption is rapid and source-free, significantly broadening its practical applicability.

\noindent \textbf{Limitation:} Our method infers 2D gaze through mathematical derivation within the differentiable projection module. While this approach enhances model interpretability and reliability, it can occasionally result in failure cases. For instance, when the input images lack visible faces, the predicted 3D gaze can become erratic. In such scenarios, the intersection point between the 3D gaze vector and the screen plane may significantly deviate from the ground truth. This issue arises because, unlike neural networks that constrain outputs to a plausible range, a purely mathematical projection may yield extreme values, \eg, when the 3D gaze is nearly parallel to the plane. Although these cases can be easily flagged in real-world applications, they may introduce biases during evaluation.

\noindent \textbf{Future Directions:} 
In this paper, we address the challenge of 2D pseudo-labeling. However, several open questions remain. For instance, can we leverage unlabeled face images to further enhance performance? Traditional methods often utilize a standard calibration pattern, could we incorporate a similar strategy? It is worth noting that our approach requires collecting samples initially, akin to a calibration process. We argue that this step is essential as it provides the necessary anchors for adapting to unseen devices. Nonetheless, exploring user-unaware calibration techniques is also a promising direction for future research.


