
\section{Methodology}



\subsection{Task Definition}
Given a pre-trained 3D gaze estimation network $\cH_{3D}\left(\bI;  \beta\right)$, which takes face images $\bI$ as input and outputs 3D gaze direction $\bg$, \ie, $\cH_{3D}:\bI\rightarrow\bg$,
our objective is to develop a  2D gaze estimation network $\cH_{2D}\left(\bI;  \theta\right)$. Using few-shot training samples $\cD = \{(\bI_i, \bp_i)\}_{i=1}^N$, where $N$ is the number of training samples, this network estimates 2D pixel coordinates $\bp$ from face images, \ie, $\cH_{2D}:\bI \rightarrow\bp$. We consider a restricted setting where: 1) the method is source-free, as the training set of $\cH_{3D}$ is unavailable, and 2) it is screen calibration-free, with the screen pose unspecified. These restrictions make our method convenient for practical applications while upholding data privacy. 


\subsection{Physics-Based Differentiable Projection}

Our work aims to learn a new $\cH_{2D}$ using few-shot samples.
The primary challenge lies in transferring knowledge from $\cH_{3D}$ to $\cH_{2D}$. However, the two networks perform different tasks, making some conventional methods such as feature distillation unsuitable.
To address this, our idea is to decompose the 2D gaze estimation into 3D gaze estimation and gaze projection. Specifically, we incorporate $\cH_{3D}$ as part of $\cH_{2D}$, supplemented with an additional module for projecting gaze directions onto a 2D screen.
Unlike existing gaze projection strategies that often rely on post-processing~\cite{Cheng_2024_pami} or require screen calibration~\cite{Balim_2023_CVPR}, we introduce a physics-based differentiable projection module. This module models screen pose as learnable weights, enabling the projection process to occur in a differentiable and adaptable manner.


In detail, we define learnable weights $\br\in\nR^3$ as the rotation vector and $\bt\in\nR^3$ as the translation vector within the projection module,
establishing the transformation from the screen coordinate system to the camera coordinate system.
To transform $\br$ into the rotation matrix $\bR\in\nR^{3\times3}$, we apply the Rodrigues formula, which preserves the orthogonality of $\bR$ so that $\bR \in SO(3)$.
The input of projection module contains gaze direction $\bg\in\nR^3$ and the 3D position of the face center $\bo\in\nR^3$, the latter of which can be computed using existing 3D landmark estimation methods~\cite{Guo_2020_ECCV}. Overall, the module $\cP$ could be denoted as:
\begin{equation}
    \hat{\bp} = \cP(\bg, \bo; \br, \bt), 
\end{equation}
\noindent where $\hat{\bp}\in\nR^2$ represents the estimated screen coordinate. 

We first compute the intersection points between gaze directions and the learnable screen denoted with $(\br, \bt)$. To establish the screen pose, we need a normal vector $\bn$ and a point coordinate on the screen. The normal vector can be derived using $\bR[:,2]$, \ie, the third column of the rotation matrix~\cite{Cheng_2024_pami}, while $\bt$ serves as a reference point on the plane. 
Given that the dot product between the normal vector of a plane and the vector connecting any point on the plane to a fixed point is constant, it is obvious that the intersection point $\bp_{3D}$ is

\begin{equation}
    \bp_{3D} = \bo + \frac{(\bt - \bo)\cdot{\bn}}{\bg\cdot{\bn}} \bg,
\end{equation}

Note that $\bp_{3D}$ represents coordinates in the camera coordinate system. To convert it to the screen coordinate system, we apply
\begin{equation}
    \bp = \bR^{-1}(\bp_{3D} - \bt),
    \label{equ:rot}
\end{equation}

\noindent We slightly abuse the notation $\bp$ in \eqnref{equ:rot}, where the final 2D gaze coordinate corresponds to the first two components of $\bp$. These values can then be further converted into pixel coordinates by utilizing the screen's PPI (pixels per inch), which is easily obtainable as a screen parameter.

Therefore, the network $\cH_{2D}$ can be denoted as 
\begin{equation}
    \cH_{2D}(\bI, \bo; \beta, \br, \bt) = \cP(\cH_{3D}(\bI), \bo),
\end{equation}
\noindent and the objective function is denoted as 
\begin{equation}
    \min_{\beta, \br, \bt} \sum_{i=1}^{N}\left\|\cH_{2D}(\bI_i, \bo_i) - \bp_i\right\|_1  
    \label{equ:loss1}
\end{equation}
\noindent We illustrate the projection module in \figref{fig:method}.


\subsection{Dynamic Pseudo-Labeling for 2D Gaze}
Data augmentation is a typical technique to improve model performance, particularly with limited dataset sizes. In this section, we apply flipping to expand the data space.
In 3D gaze estimation, the flipping involves horizontally flipping face image and adjusting the label by negating the x-coordinate value. We formally define the operation in label as $\cF(\bg)$.
However, generating reliable pseudo labels after flipping is challenging for 2D gaze estimation. 

Our core idea is to dynamically generate pseudo-labels during training by leveraging the differentiable projection module within our framework, which includes learnable screen parameters. 
This enables us to address the challenges of assigning 2D pseudo-labels by reversing the projection process, \ie, converting 2D screen coordinates into 3D gaze directions, where we can then apply flipping in 3D space. The pseudo-labeling function $\cQ(\bp)$ is defined as 
\begin{equation}
    \cQ(\bp) = \cP(\cF(\cP^{-1}(\bp))),
    \label{equ:flip}
\end{equation}
\noindent where $\cP^{-1}$ represents the reverse projection process.
Specifically, we first transform $\bp$ into the camera coordinate system. The gaze direction is then defined as the vector originating from the face center and directed toward the gaze point,

\begin{equation}
    \cP^{-1}(\bp, \bo) = (\bR\bp + \bt) - \bo,
\end{equation}
\noindent which we normalize to ensure the vector has a unit length.

However, we observed that assigning pseudo-labels as \eqnref{equ:flip} led to model collapse, with the pseudo-labels diverging to large values during training.
On the other hand, we found that $\cH_{2D}$ struggled to learn the correct screen parameters, and noted substantial changes in the 3D gaze estimation network itself.
Our intuition suggests that \textit{changing the screen pose should theoretically allow us to find an optimal screen pose, but this could also be approached by rotating the camera instead, \ie, optimize $\cH_{3D}$.}

Based on the observations, we find that \eqnref{equ:flip} is not consistently reliable.
The key insight is that human gaze direction is inherently defined in the camera coordinate system. Flipping image affects the camera coordinate system itself, meaning the gaze label should be adjusted accordingly, \ie, \textit{flipping should be performed in camera coordinate system}. However, since the 3D gaze estimation network undergoes updates during fine-tuning, it is shifted into an unknown coordinate system.
This change in coordinate systems disrupts the alignment of gaze labels, leading to model collapse. 


\begin{figure}[t]
	\begin{center}
		\includegraphics[width=\columnwidth]{fig/Rotation.pdf}	
	\end{center}
   \vspace{-3mm}
	\caption{
    The dynamic pseudo-labeling strategy for 2D gaze involves reversing the projection process to convert 2D gaze into 3D space, where we compute pseudo-labels.
    To align the camera coordinate system (CCS) with the unknown coordinate system (UCS), we use the same image sets as input to both the initial and the updated 3D model. The initial model, trained on the CCS, while the updated model operates within the UCS. By leveraging the outputs from these models as two anchors, we derive the transformation $\cT$ to align the coordinate systems. Notably, $\cT$ should be invertible. \vspace{-6mm}}
	\label{fig:calibration}
\end{figure}

To solve this problem, we aim to learn a transformation $\cT$ that maps the unknown coordinate system to camera coordinate system.
Our idea is to identify anchors in the two coordinate systems, allowing us to model this problem as an alignment task.
Specifically, we denote the initial pre-trained 3D gaze estimation network as $\cH_{3D}(;\beta_0)$ and the fine-tuned network as $\cH_{3D}(;\beta_k)$.
Notably, $\cH_{3D}(;\beta_0)$ is pre-trained in the camera coordinate system, while $\cH_{3D}(;\beta_k)$ operates in the unknown coordinate system. Therefore, we can acquire the anchor as $\{\cH_{3D}(\bI_i;\beta_0)\}_{i=1}^N$ and $\{\cH_{3D}(\bI_i;\beta_k)\}_{i=1}^N$ using training set $\cD$. The alignment problem can then be formulated as:
\begin{equation}
    \min_\cT \sum_{i=1}^N \|\cT\cH_{3D}(\bI_i;\beta_k) - \cH_{3D}(\bI_i;\beta_0)\|_2
\end{equation}
Notably, $\cT$ should be invertible. Therefore, we model the transformation as a rotation operation, enabling us to solve it using singular value decomposition (SVD). We have 
\begin{equation}
    [U, S, V] = \mathrm{SVD}(\cH_{3D}(\bI_i;\beta_i)*\cH_{3D}(\bI_i;\beta_0)^T),
\end{equation}
and $\cT = VU^T$. Consequently, we can update \eqnref{equ:flip} as follows
\begin{equation}
    \cQ(\bp) = \cP(\cT^{-1}*\cF(\cT*\cP^{-1}(\bp))),
    \label{equ:newflip}
\end{equation}
\noindent where $*$ represents matrix multiplication.
$\cQ(\bp)$ is also dynamic and re-computed during each iteration since the coordinate system continues to change throughout fine-tuning.

The objective function is denoted as 
\begin{equation}
    \min_{\beta, \br, \bt} \sum_{i=1}^{N}\left\|\cH_{2D}(\bI_i', \bo_i) - \cQ(\bp_i)\right\|_1
    \label{equ:loss2}
\end{equation}
Where $\bI_i'$ is the flipping image of $\bI_i$. 


\subsection{Minimize Uncertainty across Jittered Images}
We also perform color jitter and minimize uncertainty across jittered images to enhance model robustness. 
Given a face image $\bI$, we apply color jitter $\cJ$ to create a set of augmented images, $\{\cJ_k(\bI)\}_{k=1}^K$, where $k$ represents the number of random color jitters performed. We minimize the variance in the gaze predictions for this set. Specifically, we pass each augmented image through the model, obtaining predictions $\{\cH_{2D}(\cJ_k(\bI))\}_{k=1}^K$.
We calculate the centroid of these predictions and minimize the distance between each prediction and the centroid. Additionally, we also minimize the distance between the predictions and the ground truth.
To stabilize training, we introduce a temporal weight $\tau=\frac{t-1}{t}$ for the variance loss, starting with a smaller weight that increases over epoch $t$. The loss is defined as 
\begin{equation}
\begin{aligned}
    \cL_{unc} = & \frac{1}{NK} \sum_{i=1}^N\sum_{k=1}^K (\|\cH_{2D}(\cJ_k(\bI_i)) - \bp_i\|_1 + \\
    &\tau*\|\cH_{2D}(\cJ_k(\bI_i)) - \frac{1}{K}\sum_{j=1}^K\cH_{2D}(\cJ_j(\bI_i))\|_2)
\end{aligned}
\label{equ:loss3}
\end{equation}
The temporal weight mitigates the risk of model collapse, as we observe that the second term of $\cL_{unc}$ tends to be large at the start of training, and a high initial learning weight can lead to instability. Additionally, we apply L2 regularization  to the second term since it assigns greater weight to outliers.

\subsection{Implementation Details}
Our model is optimized using the loss functions defined in \eqnref{equ:loss1}, \eqnref{equ:loss2} and \eqnref{equ:loss3}, with corresponding weights of 1, 0.4, and 0.25, respectively.
For training, we set $N=10$, meaning the training set contains 10 samples, and $K=4$, meaning we apply four random color jitter augmentations per iteration. The model is implemented in PyTorch and trained on an NVIDIA RTX 3090. We train for 80 epochs, setting the learning rate initially to 0.001, with a 5-epoch warmup phase. After 60 epochs, the learning rate decays to 0.0005. We use GazeTR~\cite{cheng2022icpr} (ResNet18 + 6-layer transformer) pretrained on Gaze360~\cite{Kellnhofer_2019_ICCV} as the basic 3D model. 
Please refer the supplementary material for more details.






