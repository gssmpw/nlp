\begin{figure*}[t]
	\begin{center}
		\includegraphics[width=2\columnwidth]{fig/method.pdf}	
	\end{center}
    \vspace{-6mm}
	\caption{We propose a framework for the cross-task few-shot 2D gaze estimation. The framework contains a physics-based differentiable projection module with learnable parameters $\br$ and $\bt$ to model screen, and project 3D gaze into 2D gaze. The framework is fully differentiable and can integrate into existing 3D gaze networks without modifying their original architecture. Leveraging this framework, we can quickly adapt a 3D gaze model for 2D gaze estimation using only a small number of images.\vspace{-6mm}}
	\label{fig:method}
\end{figure*}
\section{Related Works}

\subsection{Gaze Estimation}

Gaze estimation methods are generally classified into 3D and 2D gaze estimation based on output~\cite{Cheng_2024_pami}.  3D gaze estimation defines gaze as a directional vector originating from the face toward gaze targets~\cite{Zhang_2015_CVPR,Cheng_2020_tip}. It typically focuses on enhancing accuracy and generalizability across diverse environments. Related research spans several fields, including supervised learning~\cite{Cheng_2020_AAAI,cheng2022icpr,Cheng_2023_ICCV}, unsupervised domain adaptation~\cite{Cai_2023_CVPR,Bao2_2024_CVPR,Bao_2024_CVPR}, feature disentanglement~\cite{Park_2019_ICCV,cheng_2022_aaai,yin_2024_eccv}, \etc.

On the other hand, 2D gaze estimation is primarily applied in screen-based contexts, where gaze is represented as a pixel coordinate in the screen coordinate system~\cite{Krafka_2016_CVPR,Bao_2020_ICPR,gudi2020efficiency}. Compared to 3D gaze estimation, 2D gaze estimation is more directly applicable to human-computer interaction~\cite{Huynh_2022_ubi, cast_2023_etra, valliappan2020accelerating}. However, it becomes entangled with multiple device-specific factors, such as screen size and camera-screen pose, which complicate generalization across various setups. The adaptation of 2D gaze estimation methods remains a notable research challenge.

Although these gaze estimation methods fundamentally capture eye movement, the distinct differences between 3D and 2D gaze annotations define them as separate research areas. In this work, we propose a framework to bridge the gap between 2D and 3D gaze estimation, enabling the direct application of 3D gaze research to 2D gaze estimation.



\subsection{2D Gaze Estimation via Projection}
It is typical to compute the intersection between 3D gaze and a 2D plane for 2D gaze, a process referred to as gaze projection in this work. The most common application is in VR~\cite{2017_gazevr,2021_gazevr}, where the head-mounted display provides 3D gaze estimation, and developers can easily obtain a plane pose in VR space. They project the gaze onto this plane or determine if it intersects for interaction.

This strategy is also used in deep-learning based gaze estimation methods.
They calibrate the screen pose during the post-processing stage and convert 3D gaze into 2D gaze on the screen~\cite{Cheng_2024_pami,Zhang_2019_CHI}.
Recently, some methods have attempted to inject the projection into the deep learning framework. Balim \etal ~\cite{Balim_2023_CVPR} first require screen calibration to obtain screen parameters and then model the projection process using the calibrated pose.
Cheng \etal~\cite{cheng2024ivgaze} focus on estimating gaze zones on vehicle windshields. They define a basis tri-plane, project 3D gaze onto this plane, and then learn a mapping from the interaction points to the gaze zone.


In our work, we model the full projection process by defining the screen pose with six learnable parameters. The projection module is parameter-efficient. More importantly, our method does not require screen calibration, which can be challenging for non-expert users.

