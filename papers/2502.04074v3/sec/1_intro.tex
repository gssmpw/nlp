\section{Introduction}
\label{sec:intro}

Gaze estimation tracks eye movements to predict human attention~\cite{Cheng_2024_pami}. It is a highly applied research topic, where various application scenarios, such as intelligent vehicles~\cite{cheng2024ivgaze, s19245540}, VR/AR~\cite{Palazzi_2019_tpami, patney2016towards, mania2021gaze}, and disease diagnosis~\cite{Wang2022app,bhattacharya2022gazeradar} demand distinct and specialized gaze estimation solutions.

Recent gaze estimation methods primarily focus on 3D gaze estimation~\cite{Zhang_2015_CVPR,cheng2022icpr}, wherein 3D direction vectors are derived from facial images.
Such methods exhibit high adaptability, facilitating straightforward application in diverse environments~\cite{Zhang_2018_CHI}. 
However, they present limitations in practical applications, such as human-computer interaction, where precise gaze targets are essential. Existing approaches often require post-processing to calibrate the pose of interacted objects, e.g., a screen, and compute the intersection between gaze and objects~\cite{Zhang_2019_CHI}. This process poses significant challenges, particularly for non-expert users.

\begin{figure}[t]
	\begin{center}
		\includegraphics[width=\linewidth]{fig/teaser.pdf}	
	\end{center}
        \vspace{-4mm}
	\caption{We introduce a novel cross-task few-shot 2D gaze estimation approach. Our method leverages a pre-trained 3D gaze estimation network and few-shot 2D gaze samples to achieve 2D gaze estimation on unseen devices. It contains a physics-based differentiable projection module to bridge 3D and 2D gaze, along with a dynamic pseudo-labelling strategy for 2D labels under unknown screen poses. Our approach is both screen-calibration-free and source-free, significantly expanding its application potential.\vspace{-7mm}}
	\label{fig:teaser}
\end{figure}

Conversely, some methods directly estimate 2D gaze within screen coordinate systems. Deep learning-based approaches utilize large training datasets to map facial images to 2D gaze~\cite{Krafka_2016_CVPR,Bao_2020_ICPR}.
However, these models are often entangled with multiple device-specific factors.
Traditional approaches construct 3D eye models using prior anatomical knowledge and fit these models with few-shot calibration images~\cite{hansen2009eye}. Although such methods require specialized equipment for precise eye tracking, they raise the question, \textit{Can we achieve similar patterns within the deep learning paradigm for quick adaptation across various devices? }



In this work, we explore a novel topic, cross-task few-shot 2D gaze estimation. 
We observe that 3D gaze estimation has recently gained significant attention in the research community. It is performed within 3D space, free from entanglement with specific devices.
These insights suggest that 3D gaze estimation models would be a great prior, similar to the 3D anatomical eye model in traditional methods.
Therefore, our approach aims to utilize 3D gaze estimation as prior and adapt it efficiently for 2D gaze estimation. 
However, this setting introduces several significant challenges, such as the domain gap between 3D and 2D gaze tasks, unseen device settings, \ie, \textit{w/o} screen calibration, and insufficient training data. We show a comparison between our task with common methods in \tabref{tab:settingcomparison}. 

To address these challenges, we first propose a novel framework to bridge the gap between 2D and 3D gaze estimation. We decomposes 2D gaze estimation into two components: 3D gaze estimation and gaze projection. 
We first estimate 3D gaze from face images, and then project 3D gaze onto a specific 2D plane to infer the 2D gaze.
Unlike existing methods that require screen calibration to obtain the screen pose~\cite{Cheng_2024_pami,Balim_2023_CVPR}, our framework includes a physics-based differentiable projection module. This module models screen pose using six learnable parameters, \ie, rotation and translation vectors that map screen coordinate system to camera coordinate system. By implementing the projection in a fully differentiable manner, our framework enables seamless integration of the projection module into any existing 3D gaze estimation model without changing its original architecture. 
Furthermore, since the framework is fully differentiable, it supports fine-tuning on 2D annotated data. 

We further propose a dynamic pseudo-labelling strategy for 2D label in our framework. 
Specifically, we perform flipping on face images and aim to assign pseudo-label for the flipped images.
While this process is straightforward for 3D gaze annotations, it becomes more complex for 2D gaze due to dependencies on factors like head position and screen pose, especially the screen pose is unknown. 
To address this, we perform dynamic pseudo-labelling during training. In each iteration, we reverse the projection process using the learnable screen parameters to convert 2D labels into 3D labels. This allows us to perform flipping directly in the 3D gaze space.
A key insight is that flipping needs to occur in the camera coordinate system, while accounting for a shift in coordinate systems during training.
To handle this, we learn a dynamic transformation that maps the shifted system back to the camera coordinate system, ensuring reliable pseudo-label generation.
Additionally, we apply color jittering during training, which does not alter the 2D gaze labels, and minimize uncertainty across jittered images to improve robustness.

\begin{table}[t]
    \arrayrulecolor[rgb]{0,0,0}
    \setlength\tabcolsep{4pt}
    
    \renewcommand\arraystretch{1.0}
    
    \small
    \caption{Comparison between our method with existing methods. We introduce an unexplored task in gaze estimation, which aims to adapt 3D gaze models for 2D gaze estimation with few-shot data. \vspace{-2mm}}
      \centering
        \begin{tabular}{c|ccccc}
        \toprule[0.9pt]
        Category & Train & Test & \makecell{Cross\\Env.} &\makecell{Cross\\Task} & Methods\\
        \hline
        \makecell{3D Gaze Estimation}& \3d &\3d &$\times$&$\times$&\cite{cheng2022icpr,Jindal_2024_CVPR,cheng_2022_aaai}\\
        
        \makecell{2D Gaez Estiamtion}& \2d &\2d &$\times$&$\times$&\cite{Bao_2020_ICPR,Krafka_2016_CVPR}\\
        \multirow{2}{*}{Personalize} & \2d & \2d&$\times$&$\times$&
        \cite{He_2019_ICCV}\\
        &\3d&\3d&$\checkmark$&$\times$&\cite{Lin_2019_iccvw,Park_2019_ICCV}\\
        \makecell{Domain Adaption}& \3d &\3d &$\checkmark$&$\times$&\cite{Cai_2023_CVPR,Bao2_2024_CVPR,Bao_2024_CVPR}\\
        \hline
        Ours& \3d & \2d &$\checkmark$&$\checkmark$&None\\
        \bottomrule[1.0pt]
    \end{tabular}
    \vspace{-4mm}
     \label{tab:settingcomparison}
\end{table}

Overall, our main contribution contains four-folds:

\begin{enumerate}
    
    \item We explore the novel topic of cross-task few-shot 2D gaze estimation. This topic not only extends the application of 3D gaze research to the 2D domain but also provides a promising direction for real-world applications.
    
    \item We propose a framework to bridge 3D and 2D gaze estimation, which includes a physics-based differentiable projection module with six learnable screen parameters to convert 3D gaze to 2D gaze.
    By leveraging this framework, we can quickly adapt a 3D gaze model for 2D gaze estimation using only a small number of images. 

    \item We propose a dynamic pseudo-labeling strategy for 2D labels in our framework. We reverse the projection using learnable screen parameters to convert 2D labels back into 3D labels and perform pseudo-labeling in the 3D gaze space. Furthermore, we learn a dynamic transformation to address the shifted coordinate system problem.
    
    
    \item We establish a benchmark for the cross-task few-shot 2D gaze estimation, and evaluate our method on three datasets covering daily scenarios, including laptop, desktop computer and mobile devices. The superior performance demonstrates the advantage of our approach.

\end{enumerate}

