
\documentclass{article}

\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} %
\usepackage[table]{xcolor}

\usepackage{hyperref}


\newcommand{\theHalgorithm}{\arabic{algorithm}}


\usepackage[accepted]{arxiv}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{bm}
\usepackage{lipsum}

\usepackage{multirow}
\usepackage{stmaryrd}  %
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\renewcommand\algorithmicrequire{\textbf{Input:}}
\renewcommand\algorithmicensure{\textbf{Output:}}

\usepackage[capitalize,noabbrev]{cleveref}

\input{macros}

\usepackage[conf={end, restate, no link to proof}]{proof-at-the-end}
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

\usepackage[textsize=tiny]{todonotes}


\icmltitlerunning{Transformers are Scalable Graph Generators}

\begin{document}

\twocolumn[
\icmltitle{Flatten Graphs as Sequences: Transformers are Scalable Graph Generators}



\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Dexiong Chen}{mpib}
\icmlauthor{Markus Krimmel}{mpib}
\icmlauthor{Karsten Borgwardt}{mpib}
\end{icmlauthorlist}

\icmlaffiliation{mpib}{Max Planck Institute of Biochemistry, Martinsried, Germany}

\icmlcorrespondingauthor{Dexiong Chen}{dchen@biochem.mpg.de}

\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]



\printAffiliationsAndNotice{}  %

\begin{abstract}
We introduce \method{}, a novel autoregressive framework for generating large attributed graphs using decoder-only transformers. At the core of our approach is a reversible ``flattening'' process that transforms graphs into random sequences. By sampling and learning from these sequences, \method{} enables transformers to model and generate complex graph structures in a manner akin to natural language. 
In contrast to diffusion models that rely on computationally intensive node features, our approach operates exclusively on these sequences. 
The sampling complexity and sequence length scale linearly with the number of edges, making \method{} highly scalable for generating large sparse graphs. 
Empirically, \method{} achieves state-of-the-art performance across diverse synthetic and molecular graph generation benchmarks, while delivering a 100-fold generation and a 3-fold training speedup compared to leading diffusion models. Additionally, it demonstrates promising transfer capabilities and supports substructure-conditioned generation without additional fine-tuning. 
By extending language modeling techniques to graph generation, this work paves the way for developing graph foundation models.
\end{abstract}


\section{Introduction}\label{sec:introduction}

Recent advancements in deep generative models have revolutionized various domains of artificial intelligence, demonstrating remarkable capabilities in generating complex data types such as images~\citep{rombach2022high}, natural language~\citep{gpt3,touvron2023llama,touvron2023llama2}, and audio~\citep{dhariwal2020jukebox,huang2024audiogpt}. These achievements have been primarily driven by the development of advanced architectures or methods such as transformers and diffusion models, alongside increasingly large-scale data resources. However, the generation of graph-structured data, which is fundamental to numerous scientific applications including drug discovery~\citep{vignac2023digress,lim2020scaffold}, protein design~\citep{ingraham2019generative}, and program synthesis~\citep{brockschmidt2019generative}, remains a significant challenge. This disparity primarily stems from the inherent complexity of preserving structural validity, maintaining invariance properties within graphs, and achieving scalability in real-world graph generation tasks.

To this end, diffusion-based models have emerged as a promising direction for graph generation, demonstrating effectiveness in synthesizing both classic unattributed graphs and molecules~\citep{jo2022score,vignac2023digress}. These approaches typically implement a denoising process in discrete graph space, simultaneously predicting edge connectivity and attributes. Yet, their practical applications are constrained by fundamental scalability limitations. The requirement for full adjacency matrix operations imposes quadratic memory complexity with respect to the number of nodes. Moreover, computing additional features in each denoising step such as spectral features, often involving cubic complexity, further increases the computational overhead.

Autoregressive approaches represent an alternative paradigm, constructing graphs sequentially by generating nodes and edges in a step-by-step manner~\citep{liao2019efficient,you2018graphrnn}. These models have demonstrated strong performance in generating small to medium-sized graphs by leveraging their ability to maintain structural validity through the generation process. Nevertheless, these models face inherent limitations: they require specialized architectures, primarily based on recurrent neural networks, to process the complex ad-hoc sequential representations of graphs, preventing them from directly leveraging the remarkable advances in large language models (LLMs). Moreover, these specialized architectures often struggle with long-range dependencies and ensuring global structural consistency, leading to significantly inferior performance compared to recent diffusion models~\citep{vignac2023digress}. This architectural constraint not only limits their scalability but also creates a growing performance gap as general-purpose LLMs continue to advance rapidly.

In light of these challenges, we introduce a novel paradigm that bridges the gap between graph generation and LLMs through a graph-to-sequence transformation. Our approach fundamentally reconceptualizes graphs as sequential structures while maintaining their topological properties. Instead of requiring specialized architectures or operating on graph structures, we propose a method to linearize graphs into random sequences that encode local connectivity patterns. This transformation enables direct utilization of language models for graph generation while achieving optimally linear complexity with respect to the number of edges in both computational and memory requirements. Our approach effectively addresses the limitations of both diffusion-based and autoregressive methods: it maintains structural validity while enabling efficient scaling to large graphs and leveraging the powerful capabilities of modern language models.

Our work presents several technical contributions to the field of graph generation. (1) We introduce the concept of segmented Eulerian neighborhood trails (SENTs), a specialized class of Eulerian trails that permit breaks and incorporate neighborhood information. We establish sufficient conditions under which they can be employed for effective graph generation. (2) We propose an efficient flattening algorithm that transforms graphs into sequences and vice versa by sampling these SENTs, enabling lossless sequence representation of graphs. (3) Our method, termed \method{}, achieves state-of-the-art (SOTA) performance across diverse synthetic and molecular graph generation benchmarks, delivering a 100-fold generation and a 3-fold training speedup compared to diffusion-based models while maintaining the ability to scale to graphs of possibly immense size. (4) Additionally, \method{} demonstrates strong transfer learning capabilities and supports substructure-conditioned generation without additional fine-tuning. Our work not only advances the field of graph generation but also opens new avenues for applying LLMs to graph-centric tasks, paving the way for building foundation models for graphs.

\section{Methods}\label{sec:methods}
\begin{figure*}[!htb]
    \centering
    \includegraphics[width=\textwidth]{img/autograph_pipeline_new.pdf}
    \vskip-.1in
    \caption{Overview of \method{}: (1) We use Algorithm~\ref{algo:sent_sampling} to sample a SENT $s$ from the input graph: $s=(s_1,s_2)$ with $s_1=((v_1,\emptyset),(v_2,\emptyset),(v_3,\emptyset))$ and $s_2=((v_5, \{v_2\}), (v_4,\emptyset))$. (2) We tokenize it by reindexing the vertices based on their first occurrence order in $s$ and adding special tokens (`\textbf{/}' represents breakage between segments, `$\bm{<}$' and `$\bm{>}$' indicate the start and end of a neighborhood set). (3) We perform the next token prediction on the tokenized sequences using a decoder-only transformer or any language model.}
    \label{fig:overview}
    \vskip -0.1in
\end{figure*}

In this section, we present an approach to transforming graphs into sequences, enabling their modeling akin to natural language. Our method hinges on a specialized class of random trail segments that ensure complete graph coverage. We begin by introducing the concept of segmented Eulerian trails (SET) and demonstrate theoretically why this representation alone is insufficient for effective graph generation. Subsequently, we propose an extension of SET, namely the segmented Eulerian neighborhood trail (SENT), which additionally incorporates neighborhood information alongside the trails. We elucidate sufficient conditions for effective generation and develop an efficient sampling strategy to obtain such SENTs. The section concludes with extensions and discusses how to model the SENTs autoregressively using language models, thus bridging the gap between graph learning and language modeling paradigms. An overview of \method{} is illustrated in Figure~\ref{fig:overview}, and backgrounds and proofs are provided in Appendix~\ref{app:sec:remarks} and \ref{app:sec:proofs}.

\subsection{Segmented Eulerian Trail}
To formalize our approach, we begin by introducing fundamental concepts in graph theory.
Let $V$ be a set of vertices and $E:=V\times V$ a set of edges. A graph is defined as a tuple $G=(V_G, E_G)$, where $V_G\subseteq V$ is a finite set of vertices and $E_G\subseteq V_G\times V_G$ is the set of edges. For simplicity and without loss of generality, we restrict our attention to undirected graphs without isolated vertices, where each edge is represented as an unordered pair $(u,v)$ for $u,v \in V$. We begin by defining the concept of a trail in a graph:
\begin{definition}[Walk and trail]
    A walk is a sequence of nodes connected by edges in $G$ and a trail is a walk in which all edges are distinct. Given a graph $G$, the set of trails in $G$ is denoted as $\Tcal_G$.
\end{definition}
Next, we generalize the concept of trails beyond the context of a specific graph:
\begin{definition}[Generalized trail]
    A generalized trail of length $k$ is defined as a sequence of nodes $w:=(w_0,\dots,w_k)\in V^{k+1}$ for $k\geq 0$ \st $(w_{i-1},w_{i})\neq (w_{j-1},w_{j})$, $\forall i,j\in [k]$ and $i\neq j$.
\end{definition}
The set of all generalized trails is denoted as $\Tcal$, noting that $\Tcal_G\subseteq \Tcal$ for any $G$. For a generalized trail $w\in\Tcal$, we define $V_w\subseteq V$ and $E_w\subseteq E$ as the sets of vertices and edges traversed by $w$, respectively, termed the \emph{generated sets} of $w$.
An \emph{Eulerian trail} is a trail that visits every edge in a graph exactly once. Such trails are of particular interest as they capture the complete topology of the graph. However, the existence of an Eulerian trail depends on specific conditions related to vertex degrees and connectivity~\citep{biggs1986graph}. To generalize this concept to arbitrary graphs, we introduce the notion of trail segments:
\begin{definition}[Segmented Eulerian trail (SET)]
    A segmented Eulerian trail (SET) in $G$ is a sequence of trail segments such that each edge is visited exactly once across all segments, and segments do not need to be connected. 
    Formally, a SET of size $k$ in $G$ is defined as $s:=(s_1,\dots,s_k)$ \st $s_i\in \Tcal_G$, and the generated edge sets of its segments form a partition of $E_G$, \ie $\cup_{i=1}^k E_{s_i}=E_G$ and $E_{s_i}\cap E_{s_j}=\emptyset, \forall i,j\in [k], i\neq j$. 
    Similarly, a SET (without relying on a specific graph) is defined as a sequence of generalized trails whose generated edge sets are disjoint.
\end{definition}
The set of all SETs in $G$ is denoted as $\Scal_G$, and the set of all SETs is denoted as $\Scal$.
For a SET $s=(s_i)_{i=1}^k$, we define the \emph{generated node and edge sets} as $V_s:=\cup_{i=1}^k V_{s_i}$ and $E_s:=\cup_{i=1}^k E_{s_i}$. The graph $G_s:=(V_s,E_s)$ is termed \emph{generated graph} of $s$. It is easy to show that $s$ is a SET in $G$ if $G_s\simeq G$.
Moreover, SETs can be classified into equivalence classes based on graph isomorphism, as formalized below:
\begin{definition}[SET isomorphism]
    For any two SETs $s,t\in \Scal$, we say they are isomorphic $s\simeq t$ if there is a bijection $\pi:V_s\to V_t$ between their generated node sets and $\pi(s)=t$ where $\pi$ applies elementwise to all nodes in $s$.
\end{definition}
This isomorphism partitions $\Scal$ into equivalence classes. Moreover, we have the following relationship between SETs and graphs, relevant for our tokenization (Sec.~\ref{sec:tokenization}):
\begin{theoremEnd}{theorem}\label{thm:set_isomorphism}
    For any SETs $s,t\in \Scal$, their generated graphs are isomorphic, \ie $G_s\simeq G_t$, if $s\simeq t$. Conversely, if two graphs $G\simeq H$, then for any SET $s\in\Scal_G$, there exists a SET $t\in\Scal_H$ \st $s\simeq t$.
\end{theoremEnd}
\begin{proofEnd}
    By definition of the isomorphism between $s$ and $t$, there exists a bijection $\pi:V_s\to V_t$ \st $\pi(s)=t$. Now if $u, v\in V_s$ are adjacent in $G_s$, \ie $(u,v)\in E_s$, then $(\pi(u),\pi(v))$ is an edge visited by $\pi(s)=t$, thus $(\pi(u),\pi(v))\in E_t$. Similarly, the reverse is also true. Consequently, $G_s\simeq G_t$.

    Now assume that $G\simeq H$ with an isomorphism $\pi$ and $s\in\Scal_G$. It is easy to show that $\pi(s)$ is also a SET and its generated graph $G_{\pi(s)}=H$. By taking $t=\pi(s)$, we obtain the result.
\end{proofEnd}
While a SET in $G$ fully characterizes its structure, we show below the prefixes of the SET do not necessarily describe the substructures of $G$, a critical property for effective autoregressive graph generation.
\begin{definition}[Flattening]
    The flattening of a sequence of sequences $s$ is the concatenation of all its sequences, denoted as $\concat s$.
\end{definition}
\begin{definition}[Prefix of a SET]
    For $s\in \Scal$, we call $t$ a prefix of $s$ if $\concat t$ is a prefix of $\concat s$.
\end{definition}
\begin{theoremEnd}{lemma}
    For any graph $G$ and SET $s$ in $G$, the generated graph of any prefix of $s$ is a subgraph of $G$, but not necessarily an induced subgraph.
\end{theoremEnd}
\begin{proofEnd}
    Assume that $t$ is a prefix of $s$. Then $V_t\subseteq V_s=V_G$ and $E_t\subseteq E_s$. However, $G_t$ is not necessarily an induced subgraph of $G_s$. We consider the following counter-example: $s=((1,2,3,4,1,3))$, $V_s=\{1,2,3,4\}$, and $E_s=\{(1,2),(2,3),(3,4),(1,4),(1,3)\}$. Let $t=((1,2,3,4,1))$. $t$ is clearly a prefix of $s$, but its generated graph is not an induced subgraph of $G_s$ as its generated edge set does contain $(1,3)$.
\end{proofEnd}
This result motivates us to extend the definition of generalized trails to incorporate the full structural information of the \emph{induced subgraphs}, rather than arbitrary subgraphs, to constrain the generation space better and address long-range dependency challenges. 
Without this extension, dependencies between neighboring nodes may span a long sequence of generation steps, making it more difficult for the model to learn such dependencies. Empirically, we show that SET fails to accurately generate graphs in Section~\ref{sec:ablation_experiments}.

\subsection{Segmented Eulerian Neighborhood Trail}
To make the prefixes of a SET encode richer information, we need to extend SET to contain neighborhood information in a graph. Thus, we consider the following definition:
\begin{definition}[Neighborhood sequence]
    A neighborhood sequence is a sequence of tuples $w:=(w_0,\dots,w_k)$ where $w_i=(v_i,A_i)$ with a node $v_i\in V$ and a neighborhood set $A_i\subseteq V$, $\forall i\in \{0,\dots,k\}$. $w$ is called Hamiltonian if its node sequence $n(w):=(v_0,\dots,v_k)$ has non-repeated elements. Moreover, $w$ is called \emph{causal} if $A_i$ only contains visited nodes, \ie $A_i\subseteq \{v_0,\dots,v_{i-1}\}$ $\forall i\in [k]$.
\end{definition}
We now extend this to trails:
\begin{definition}[Neighborhood trail]
    A neighborhood trail is a neighborhood sequence that satisfies two conditions. (i) $n(w)$ is a generalized trail. (ii) If we define the generated edge set of $w_i$ as $E_{w_i}=\{(v_i,u)\, |\, u\in A_i\}$, the family $\{ E_{n(w)}, E_{w_1},\dots, E_{w_k}\}$ is pairwise disjoint. Its union is called the generated edge set of $w$.
\end{definition}
The set of all neighborhood trails is denoted by $\TNcal$. For any $w\in\TNcal$, we denote by $G_w:=(V_w,E_w)$ the generated graph of $w$ where $V_w:=(\cup_{i=1}^k A_i)\cup V_{n(w)}$ is the generated node set and $E_w$ is the generated edge set. Note that a generalized trail is a neighborhood trail with $A_i=\emptyset, \forall i$. We extend SET to incorporate neighborhood information:
\begin{definition}[Segmented Eulerian neighborhood trail (SENT)]
    A segmented Eulerian neighborhood trail (SENT) of size $k$ is a sequence of neighborhood trails $s:=(s_1,\dots,s_k)$ with pairwise disjoint generated edge sets, \ie $s_i\in\TNcal$ and $E_{s_i}\cap E_{s_j}=\emptyset, \forall i,j\in [k], i\neq j$. 
\end{definition}
Similarly to SETs, the generated graph of a SENT $s$ is denoted by $G_s=(V_s,E_s)$. If a graph $G\simeq G_s$, we say that $s$ is a SENT in $G$. We denote by $\SNcal$ and $\SNcal_G$ the set of SENTs and SENTs in $G$. Analogously to SETs, we define an isomorphism over $\SNcal$ and obtain the same relationship as in Thm.~\ref{thm:set_isomorphism}. A prefix of a SENT is defined similarly to that of a SET. We give below conditions to force generated graphs of prefixes of a SENT to be induced subgraphs.
\begin{definition}[Causal SENT]
    A SENT $s$ is called causal if its flattening $\concat s$ is causal. 
\end{definition}
\begin{definition}[Hamiltonian and semi-hamiltonian SENT]
    A SENT $s$ is called Hamiltonian if its flattening $\concat s$ is Hamiltonian. $s$ is called semi-hamiltonian if $s$ is Hamiltonian, or for any nodes visited more than once, their occurrences after the first time should be in a start tuple of a neighborhood trail and their associated neighborhood sets are empty.
\end{definition}
    
    
    
    


    



    

\begin{theoremEnd}{theorem}\label{thm:induced_subgraph}
    For any causal SENT $s\in\SNcal$, the generated graph of any prefix $t$ of $s$ is an induced subgraph of $G_s$ if and only if $s$ is semi-hamiltonian. In this case, $s$ is called subgraph-induced. %
\end{theoremEnd}
\begin{proofEnd}
    Let us first introduce some notations.
    We denote by $R_s$ the sequence of the start tuples across all neighborhood trails in $s$, which is also a neighborhood sequence. By definition of semi-hamiltonian, the occurrences after the first time of a node in $s$ should be in $R_s$. We denote by $n(s)$ the associated node sequence of SENT $s$, \ie $n(s):=n(\concat s)$.

    Let us first assume that $s$ is semi-hamiltonian.
    
    Assume that $t$ is a prefix of $s$. It is easy to show that $G_t$ is a subgraph of $G_s$. Now assume that $u,v\in V_t$ \st $(u,v)\in E_s$, we want to show that $(u,v)\in E_t$. There are two cases:

    1) Assume that $u,v\in n(t)$. Since $s$ is semi-hamiltonian, $n(s)\setminus n(t)$ either does not contain $u$ or $v$, or even if one of them, say $u\in n(s)\setminus n(t)$, we have $u\in n(R_s)$ and its associated neighborhood set is empty. In both cases, the edge $(u,v)$ does not belong to the generated edge set of the neighborhood subsequence after $\concat t$. By the disjointness of the generated edge sets of $s$, it can only be included in the generated edge set of $t$, we thus have $(u,v)\in E_t$.

    2) Assume that one of them, say $u\notin n(t)$. There exists a neighborhood set $A$ in a tuple of $\concat t$ such as $u\in A$. Since $t$ is causal, we have $u\in n(t)$ which contradicts the assumption.

    In all the above cases, we have $(u,v)\in E_t$.

    Now let us assume that the generated graph of any prefix of $s$ is an induced subgraph of $G_s$.

    Let us prove that $s$ is semi-hamiltonian by contradiction. Assume that there exist two tuples in $\concat s$ with the same nodes $s_i=(v, A_i)$ and $s_j=(v,A_{j})$ with $i<j$. There are two cases: 1) $s_j\notin R_s$. A tuple $(u,A_u)$ exists one step before $s_j$ in the same neighborhood trail. We consider the prefix $t$ ending at $(u, A_u)$. We have $v,u\in V_t$ and $(u,v)\in E_s$, but $(u,v)\notin E_t$, by the disjointness of $s$ and since $(u,v)$ is visited at $s_j$ after $t$. 2) $s_j\notin R_s$ and $A_j\neq\emptyset$. Since $s$ is causal, there exists $s_u:=(u,A_u)$ before $s_j$ \st $u\in A_j$. We consider the prefix $t$ ending at exactly this tuple. We have $u,v\in V_t$ and $(u,v)\in E_s$, but $(u,v)\notin E_t$, by the disjointness of $s$ and since $(u,v)$ is an edge visited at $(v,A_j)$ after $t$.
\end{proofEnd}
Now let us find the conditions for a causal and Hamiltonian SENT. For any SENT $s$ and a tuple $w:=(v,A)$ in $s$, we denote by $V_s(w)$ the set of nodes visited by $s$ before $w$, excluding the node linked to $v$ through the trail if it exists. We have the following necessary and sufficient conditions:
\begin{theoremEnd}{theorem}\label{thm:causal_hamiltonian_sent}
    For $s\in\SNcal_G$, $s$ is causal and Hamiltonian if and only if every tuple $w:=(v, A_v)$ in $\concat s$ satisfies $A_v=\Ncal_G(v)\cap V_s(w)$. In this case, every node is visited exactly once. Moreover, $s$ is causal and semi-hamiltonian if and only if every tuple $w:=(v, A_v)$ in $s$ satisfies either $A_v=\Ncal_G(v)\cap V_s(w)$ or $A_v=\emptyset$.
\end{theoremEnd}
\begin{proofEnd}
    Let us first assume that for any tuple $w:=(v, A_v)$ in $\concat s$, $A_v=\Ncal_G(v)\cap V_s(w)$.
    Since $A_v\subseteq V_s(w)$ which is a subset of the set of visited nodes, $s$ is causal. Now we prove $s$ is Hamiltonian by contradiction. Assume that there exist two tuples in $\concat s$, $s_u:=(u,A_u)$ and a later visited one $s_v:=(v,A_v)$ \st $u=v$. Then, $A_v=\Ncal_G(v)\cap V_s(s_v)=\Ncal_G(u)\cap V_s(s_v)$ should contain the node visited before that is a neighbor of $u$ (either through a trail or the neighborhood set of $u$), denoted by $u'$. Thus, the edge $(u,u')$ has been visited twice, which contradicts the disjointness of $s$.

    Assuming that $A_v=\Ncal_G(v)\cap V_s(w)$ or $A_v=\emptyset$ for any tuple $(v, A_v)$ in $s$, we can also prove $s$ is semi-hamiltonian by contradiction. Assume that there exist two tuples in $\concat s$, $s_u:=(u,A_u)$ and a later visited one $s_v:=(v,A_v)$ \st $u=v$ and $A_v\neq \emptyset$. Then, $A_v=\Ncal_G(v)\cap V_s(s_v)=\Ncal_G(u)\cap V_s(s_v)$ by assumption. And using the same argument as above, we have the contradiction.

    Now assume that $s$ is causal and Hamiltonian. Let us prove the other direction by contradiction. There exists a tuple $w:=(v,A_v)$ in $s$ \st $A_v\neq \Ncal_G(v)\cap V_s(w)$. As $s$ is causal, $A_v\subseteq V_s(w)$. $A_v\subseteq \Ncal_G(v)$ as $s\in\SNcal_G$. Thus, $A_v\subset \Ncal_G(v)\cap V_s(w)$, which means that there exists $u\in \Ncal_G(v)\cap V_s(w)$ and $u\notin A_v$. Hence, $(u, v)\in E_G$ and $u$ is visited before $v$. However, as $u\notin A_v$, $(u, v)\in E_G$, and $s$ is Hamiltonian, there exists a tuple $(u, A_u)$ in $\concat s$ \st $v\in A_u$. By causality of $s$, $v$ is visited before $u$, which contradicts the fact that $s$ is Hamiltonian.

    Assuming that $s$ is causal and semi-hamiltonian. Let us prove the other direction by contradiction. There exists a tuple $w:=(v,A_v)$ in $s$ \st $A_v\neq \Ncal_G(v)\cap V_s(w)$ and $A_v\neq \emptyset$. Using the same arguments as above, there exists $(u, v)\in E_G$, and $u$ is visited before $v$. However, as $u\notin A_v$ and $(u, v)\in E_G$, $s$ should visit the edge $(u,v)$ at some point. Since $s$ is semi-hamiltonian, if $s$ visits again $u,v$ they can only be the first nodes and their associated neighborhood sets are empty. Hence, there is no means for $s$ to visit $(u,v)$ after $v$, leading to contradiction.
\end{proofEnd}
This theorem offers a simple sufficient condition for subgraph-induced SENTs. We provide in the following an implementation through a random path sampling strategy. 

\subsection{Sampling Algorithm for SENT}
Thm.~\ref{thm:causal_hamiltonian_sent} offers a simple strategy to sample a causal and Hamiltonian SENT: one needs to traverse the graph and choose the neighborhood set as all neighbors of the current node that have been visited. The traversing strategy could be achieved through a random path sampling or a depth-first search. In Algorithm~\ref{algo:sent_sampling}, we provide a sampling strategy based on random path sampling with breaks.
\begin{algorithm}
    \caption{Causal and Hamiltonian SENT Sampling}\label{algo:sent_sampling}
    \begin{algorithmic}[1]
        \Require $G=(V, E)$
        \Ensure A SENT $s$ in $G$
        \State Set of unvisited nodes $U \gets V$
        \State $s \gets []$
        \State $v \gets \texttt{RandomSample}(U)$; $U \gets U\setminus \{v\}$
        \State $t \gets [(v,\emptyset)]$ \Comment{first neighborhood trail}
        \While{$U \neq \emptyset$}
        \If{$\Ncal_G(v)\cap U=\emptyset$} \Comment{start a new trail}
        \State $s.\texttt{append}(t)$
        \State $v \gets \texttt{RandomSample}(U)$; $U \gets U\setminus \{v\}$
        \State $A \gets \Ncal_G(v)\cap (V\setminus U)$
        \State $t \gets [(v, A)]$
        \Else \Comment{sample the next node in the trail}
        \State $u \gets \texttt{RandomSample}(\Ncal_G(v)\cap U)$
        \State $U \gets U\setminus \{u\}$
        \State $A \gets (\Ncal_G(u)\setminus \{v\})\cap (V\setminus U)$
        \State $t.\texttt{append}((u, A))$
        \State $v \gets u$
        \EndIf
        \EndWhile
    \end{algorithmic}
\end{algorithm}

\vspace{-.15in}
\paragraph{Complexity analysis.}
The length of a SENT including the sizes of neighborhood sets (in other words, tokenized SENT defined in Section~\ref{sec:tokenization}) is bounded by the number of edges as it can visit each edge only once. Therefore, both the time and space complexity of sampling a SENT from graph $G$ are $\mathcal{O}(m)$ where $m$ is the number of edges. 

\subsection{Tokenization of SENT}\label{sec:tokenization}
Previous works have explored related concepts of sequences in graphs. For example,
\citet{you2018graphrnn} investigated causal Hamiltonian neighborhood sequences generated through breadth-first search, while \citet{liao2019efficient,goyal2020graphgen} constructed SENT-like sequences using depth-first search. However, neither of these works interpreted these sequences as a language. Here, we present a method to bridge the gap between graph generation and language modeling.

The tokenization process starts by mapping all isomorphic SENTs to the same sequence, by reindexing the vertices according to their first occurrence order within the sequence. Specifically, if we denote this ordering function for a SENT $s$ by $\pi: V_s\to \{1,\dots,|V_s|\}$, $s$ is then replaced with its ordered representation $\pi(s)$. Thanks to the isomorphism property of SENT (Thm.~\ref{thm:set_isomorphism}), $\pi(s)$ generates a graph isomorphic to $G_s$ while ensuring the obtained sequence invariant to the node ordering of the input graph.

To convert an (ordered) SENT into a machine-readable sequence, we tokenize it into a sequence of indices using special tokens. These tokens include symbols such as ` \textbf{/} ' to indicate a breakage between segments, and `$\bm{<}$' and `$\bm{>}$' to mark the start and end of a neighborhood set. Specifically, for any $s:=(s_1,\dots, s_k)\in\SNcal$, we define the tokenization function \texttt{Token} as follows:
\begin{equation*}
    \texttt{Token}(s):=\texttt{Token}(s_1)\concat [~\textbf{/}~] \concat 
    \cdots \concat [~\textbf{/}~] \concat \texttt{Token}(s_k),
\end{equation*}
where
\begin{equation*}
    \texttt{Token}(s_i):=\concat_{w\in s_i} \texttt{Token}(w),
\end{equation*}
and for each tuple $w:=(v,A)$ with the \emph{sorted set} $A= \{u_1,\dots,u_p\}$ (due to the reindexing by $\pi$), we define:
\begin{equation*}
    \texttt{Token}(w):=\left[v, \bm{<}, u_1,\dots, u_p, \bm{>}\right].
\end{equation*}
This process converts a SENT into a sequence of tokens that a language model can effectively model. Using an equivalent form, the resulting tokenization induces a \emph{non-Markovian} random walk in the graph, incorporating additional virtual nodes labeled with the above special tokens (see Appendix~\ref{app:sec:random_walk_interpretation} for more details). \emph{Language modeling of SENTs aims to learn the state transition probabilities}.

\subsection{Extension to Attributed Graphs}
Our method can be easily extended to graphs with categorical (or discretized) attributes by inserting node and edge attributes in an interleave fashion into the tokenized SENT sequence. Specifically, let $L_{\mathrm{node}}(v)$ and $L_{\mathrm{edge}}(u,v)$ be the attributes of a node $v$ and an edge $(u,v)$ respectively. Using the same notation as above, we define for any $s_i:=(w_1,\dots, w_q)\in\TNcal$ with $w_i=(v_i,\cdot)$:
\begin{equation*}
    \begin{aligned}
        \texttt{Token}(s_i)&:=\texttt{Token}(w_1) \concat [L_{\mathrm{edge}}(v_1,v_2)] \concat \texttt{Token}(w_2) \\
        & \concat \dots \concat \texttt{Token}(w_q),  \\
        \texttt{Token}(w)&:=[v, L_{\mathrm{node}}(v), \bm{<}, L_{\mathrm{edge}}(v,u_1), u_1,\\
        & \dots, L_{\mathrm{edge}}(v,u_p),u_p, \bm{>}].
    \end{aligned}
\end{equation*}

\subsection{Autoregressive Modeling of Tokenized SENTs}
The sampling and tokenization of SENTs in graphs allows for transforming graphs into sequences, which could be modeled by language models. Specifically, given a graph $G$ represented as a SENT $s$, which consists of a sequence of tokens $(s_1,\dots, s_n)$, a standard language modeling objective is to maximize the following log-likelihood:
\begin{equation}
    p(s)=\sum_{i=1}^n \log p_\theta(s_i\,|\, s_1,\dots, s_{i-1}),
\end{equation}
where the conditional probability $p_\theta$ is modeled using a neural network with parameters $\theta$. The architecture of the neural network can be any SOTA sequence model.




\section{Related Work}
\paragraph{Autoregressive models for graph generation.}
Autoregressive models generate graphs by sequentially adding nodes and edges. GraphRNN~\citep{you2018graphrnn} pioneered this approach by framing graph generation as a sequence prediction task, demonstrating the capacity of recurrent neural networks~(RNNs)~\citep{chung2014empirical} to capture complex structures. DeepGMG~\citep{li2018learning} introduced a probabilistic policy framework for conditional generation, while GRAN~\citep{liao2019efficient} and BiGG~\citep{dai2020bigg} enhanced efficiency and scalability by generating multiple nodes and edges in parallel.

Recent research has focused on optimizing the generation order. \citet{chen2021order} highlighted that the ordering of node and edge additions impacts graph quality, and GraphARM~\citep{kong2023autoregressive} applied reinforcement learning to dynamically refine this order. \citet{goyal2020graphgen} incorporated logical constraints to improve domain-specific generation, and \citet{bacciu2020edge} proposed Bayesian reasoning to better capture graph dependencies.

Although these models have shown to be efficient and effective in synthetic datasets, they face inherent unification limitations, reducing their applicability to large-scale real-world datasets. Our proposed unified and powerful sequence representation of graphs aims to address these challenges.

\vspace{-.1in}
\paragraph{Other graph generative models.}

Other graph generative models include variational, GAN-based, and diffusion-based approaches. GraphVAEs~\citep{kipf2016variational,simonovsky2018graphvae} employ variational autoencoders to learn continuous latent representations, effectively generating small graphs but struggling with more complex structures. GAN-based models, such as NetGAN~\citep{bojchevski2018netgan} and SPECTRE~\citep{martinkus2022spectre}, generate graphs by modeling graph descriptors like random walks and spectral features.

Diffusion-based models iteratively refine noise into structured graphs through reverse diffusion steps. Continuous diffusion models~\citep{niu2020permutation,jo2022score} adapt denoising diffusion probabilistic models for graph generation. To leverage graph sparsity and structure, discrete diffusion models~\citep{vignac2023digress,kong2023autoregressive} have been developed. However, a key challenge for these models is the slow sampling process due to the long reverse diffusion chain. To mitigate this limitation, several efficient diffusion techniques have been proposed, including EDGE~\citep{chen2023efficient}, HiGen~\citep{karami2024higen}, ESGG~\citep{bergmeister2024efficient}, and Pard~\citep{zhao2024pard}.

\vspace{-.1in}
\paragraph{Random walks for graph learning.}
Random walks have been widely used in graph learning due to their strong expressive power. GCKN~\citep{chen2020convolutional} and RWGNN~\citep{nikolentzos2020random} utilize path and walk kernels to learn graph representations. Several recent works~\citep{ivanov2018anonymous,wang2021inductive,yin2022algorithm} explicitly integrate random walk sequences with positional encodings, inspiring subsequent methods such as CRaWL~\citep{tonshoff2023crawl}, NeuralWalker~\citep{chen2024neuralwalker} and RWNN~\citep{kim2024revisiting}, which further enhance graph representation learning via random walk sequence modeling. GraphGPT~\citep{zhao2023graphgpt} leverages Eulerian paths to improve graph property prediction. Our work uniquely explores random sequence representations of graphs focusing on graph generation, introducing a novel perspective on combining random walks and language modeling for scalable graph generative modeling.

\section{Experiments}
In this section, we evaluate the performance of \method{} on several graph generation benchmarks, including both small and large graphs, and synthetic and real-world molecular datasets. Our experiments compare its performance to several SOTA methods and particularly focus on evaluating the following aspects: (1) We show its ability to generate relatively small graphs with a 100-fold inference speedup compared to diffusion-based models while maintaining or even improving structural validity. (2) We show its ability to scale to large graphs without loss of performance. (3) We demonstrate its effectiveness in generating real-world graphs with attributes with a focus on molecular generation, outperforming SOTA diffusion models. (4) We showcase its strong transfer capabilities and its ability to perform substructure-conditioned generation without any additional fine-tuning.
Additional details on experimental settings and evaluation are provided in Appendix~\ref{app:sec:experimental_details}.

\begin{table}[tbp]
    \centering
    \vskip -0.1in
    \caption{Comparison of \method{} to SOTA methods on Planar}
    \label{tab:planar}
    \begin{small}
    \begin{sc}
    \resizebox{\columnwidth}{!}{
    \begin{tabular}{lcccccc}\toprule
         & \multicolumn{6}{c}{Planar Graphs} \\ 
         & \multicolumn{6}{c}{$n_{\mathrm{graphs}}=128$, $|V|=64$} \\ \cmidrule{2-7}
        Model & Deg. & Clus. & Orbit & Spec. & Ratio & VUN  \\ \midrule
        Training set &  0.0002 & 0.0310 & 0.0005 & 0.0038 & 1.0 & --  \\ \midrule
        GraphRNN &  0.0049 & 0.2779 & 1.2543 & 0.0459 & 638.5 & 0.0 \\
        GRAN & 0.0007 & 0.0426 & 0.0009 & 0.0075 & 2.1 & 0.0 \\
        SPECTRE &  0.0005 & 0.0785 & 0.0012 & 0.0112 & 2.6 & 25.0 \\
        EDGE &  0.0761 & 0.3229 & 0.7737 & 0.0957 & 490.9 & 0.0 \\
        DiGress &  0.0007 & 0.0780 & 0.0079 & 0.0098 & 6.1 & 77.5 \\ 
        ESGG & 0.0005 & 0.0626 & 0.0017 & 0.0075 & 2.5 & \textbf{95.0} \\ \midrule
        \method{} & 0.0004 & 0.0605 & 0.0003 & 0.0064 & \textbf{1.5} & 87.5 \\
        \bottomrule
    \end{tabular}
    }
    \end{sc}
    \end{small}
    \vskip -0.2in
\end{table}

\vspace{-.15in}
\paragraph{Implementation details.}

We employ the LLaMA model with 12 layers and a hidden dimension of 768 as our sequence model backbone across all experiments, aligning with the architecture of GPT-2's smallest variant~\citep{radford2019gpt2}. Although prior works have used smaller models, we argue that our approach still demonstrates better scalability and faster training and inference speeds compared to diffusion models. For inference, we adopt the commonly used top-k sampling strategy~\citep{fan2018topk}. Our implementation leverages the Hugging Face framework~\citep{jain2022huggingface}, providing users with a flexible interface to experiment with SOTA language models for graph generation.

\vspace{-.15in}
\paragraph{Evaluation.}
For fair comparison, we align our evaluation methodology with established practices from prior works~\citep{you2018graphrnn,martinkus2022spectre,vignac2023digress}. Our evaluation compares generated samples against the test set using maximum mean discrepancy (MMD)~\citep{gretton2012kernel}, computed across multiple graph descriptors: node degree distributions (\textsc{Deg.}), clustering coefficients (\textsc{Clus.}), orbit count statistics (\textsc{Orbit}), and eigenvalue spectra (\textsc{Spec.}). 
As a reference, we also compute these metrics on the training set and report the mean ratio across all properties (\textsc{Ratio}).

For synthetic datasets, we additionally assess model performance using the VUN metric, the proportion of generated graphs that are valid, unique, and novel. Our efficiency analysis includes two measurements: inference speed, calculated as the per-graph generation time when producing 1024 graphs, and training efficiency, measured as the time required to achieve a VUN score of 75.0 for the Planar dataset and 60.0 for the SBM dataset. All efficiency measurements are performed on one NVIDIA H100 GPU.

For molecular generation datasets, we strictly follow the evaluation metrics used in DiGress~\citep{vignac2023digress}. More details about evaluation are provided in Appendix~\ref{app:sec:evaluation_metrics}.

\subsection{Comparison to State-of-the-Art Methods}
We evaluate the performance of \method{} compared to other SOTA graph generative models using the standard setting without pre-training. 

\begin{table}[tbp]
    \centering
    \vskip -0.1in
    \caption{Comparison of \method{} to SOTA methods on SBM}
    \label{tab:sbm}
    \begin{small}
    \begin{sc}
    \resizebox{\columnwidth}{!}{
    \begin{tabular}{lcccccc}\toprule
         & \multicolumn{6}{c}{Stochastic Block Models} \\ 
         & \multicolumn{6}{c}{$n_{\mathrm{graphs}}=128$, $|V|_{\max}=187$, $|V|_{\avg}\approx 104$} \\ \cmidrule{2-7}
        Model & Deg. & Clus. & Orbit & Spec. & Ratio & VUN  \\ \midrule
        Training set & 0.0008 & 0.0332 & 0.0255 & 0.0027 & 1.0 & -- \\ \midrule
        GraphRNN & 0.0055 & 0.0584 & 0.0785 & 0.0065 & 3.5 & 5.0 \\
        GRAN & 0.0113 & 0.0553 & 0.0540 & 0.0054 & 5.0 & 25.0 \\
        SPECTRE & 0.0015 & 0.0521 & 0.0412 & 0.0056 & \textbf{1.8} & 52.5\\
        EDGE & 0.0279 & 0.1113 & 0.0854 & 0.0251 & 12.7 & 0.0 \\
        DiGress & 0.0018 & 0.0485 & 0.0415 & 0.0045 & \textbf{1.8} & 60.0   \\ 
        ESGG & 0.0119 & 0.0517 & 0.0669 & 0.0067 & 5.4 & 45.0 \\ \midrule
        \method{} & 0.0077 & 0.0519 & 0.0439 & 0.0040 & 3.4 & \textbf{92.5} \\
        \bottomrule
    \end{tabular}
    }
    \end{sc}
    \end{small}
    \vskip -0.1in
\end{table}

\subsubsection{Small Synthetic Graph Generation}\label{sec:small_synthetic_graph}
We first evaluate our method on the small synthetic graph datasets introduced by~\citet{martinkus2022spectre}, including the Planar and SBM datasets. We compare the performance of \method{} against GraphRNN~\citep{you2018graphrnn}, GRAN~\citep{liao2019efficient}, SPECTRE~\citep{martinkus2022spectre}, EDGE~\citep{chen2023efficient}, DiGress~\citep{vignac2023digress}, ESGG~\citep{bergmeister2024efficient}. As shown in Tables~\ref{tab:planar} and \ref{tab:sbm}, \method{} achieves the best and second-best MMD ratios on average while ranking second-best and best in terms of VUN scores for the Planar and SBM datasets, respectively. Notably, all previous methods exhibit limited structural validity on the SBM dataset, with the best VUN scores reaching only 60.0.

Additionally, we assess the training and inference times of \method{} against representative models, including DiGress, GRAN, and ESGG. As presented in Table~\ref{tab:time_comparison}, \method{} is approximately 3 times faster during training and 100 times faster during inference compared to diffusion-based models. This substantial speedup over diffusion-based models is even more pronounced than that observed in other data modalities such as images~\citep{tian2024var}.

\begin{table}[tbp]
    \centering
    \vskip -0.1in
    \caption{Time comparison of {\scriptsize \method{}} to representative models. OOT indicates the model never reaches the target VUN.}
    \label{tab:time_comparison}
    \begin{small}
    \begin{sc}
    \resizebox{\columnwidth}{!}{
    \begin{tabular}{llcccc}\toprule
        Dataset & Time& DiGress & Gran & ESGG & \method{} \\ \midrule
        \multirow{2}{*}{Planar} & Training & 25.9h & OOT & 7.4h & \textbf{6.2h (4.2$\times$)} \\
        & Inference & 2.84s & 0.03s & 4.60s & \textbf{0.01s (284$\times$)} \\ \midrule
        \multirow{2}{*}{SBM} & Training & 47.7h & OOT & OOT & \textbf{13.8h (3.5$\times$)} \\
        & Inference & 13.05s & \textbf{0.13s} & 30.0s & \textbf{0.14s (93$\times$)} \\
        \bottomrule
    \end{tabular}
    }
    \end{sc}
    \end{small}
    \vskip -0.25in
\end{table}

\begin{table*}[tbp]
    \centering
    \vskip -0.1in
    \caption{Comparison of \method{} to SOTA methods on the Proteins and Point Clouds datasets. OOM indices out of memory.}
    \label{tab:protein_and_pointcloud}
    \begin{small}
    \begin{sc}
    \resizebox{.8\textwidth}{!}{
    \begin{tabular}{lccccc|ccccc}\toprule
         & \multicolumn{5}{c}{Proteins} &  \multicolumn{5}{c}{Point Clouds} \\ %
         & \multicolumn{5}{c}{$n_{\mathrm{graphs}}=587$, $|V|_{\max}=500$, $|V|_{\avg}\approx 258$} & \multicolumn{5}{c}{$n_{\mathrm{graphs}}=26$, $|V|_{\max}=5037$, $|V|_{\avg}\approx 1332$} \\ \cmidrule{2-11}
        Model & Deg. & Clus. & Orbit & Spec. & Ratio & Deg. & Clus. & Orbit & Spec. & Ratio \\ \midrule
        Training set & 0.0003 & 0.0068 & 0.0032 & 0.0005 & 1.0 & 0.0000 & 0.1768 & 0.0049 & 0.0043 & 1.0 \\ \midrule
        GraphRNN & 0.0040 & 0.1475 & 0.5851 & 0.0152 & 62.1 & OOM & OOM & OOM & OOM & OOM \\
        GRAN & 0.0479 & 0.1234 & 0.3458 & 0.0125 & 77.7 & 0.0201 & 0.4330 & 0.2625 & 0.0051 & 19.1 \\
        SPECTRE & 0.0056 & 0.0843 & 0.0267 & 0.0052 & 12.5 & OOM & OOM & OOM & OOM & OOM \\
        EDGE & 0.1863 & 0.3406 & 0.6786 & 0.1075 & 274.5 & 0.4441 & 0.3298 & 1.0730 &  0.4006 & 104.7 \\
        DiGress &  0.0041 & 0.0489 & 0.1286 & 0.0018 & 16.2 & OOM & OOM & OOM & OOM & OOM \\ 
        ESGG & 0.0030 & 0.0309 & 0.0047 & 0.0013 & 4.7 & 0.0139 & 0.5775 & 0.0780 & 0.0055 & 6.8 \\ \midrule
        \method{} & 0.0004 & 0.0244 & 0.0056 & 0.0013 & \textbf{2.3} & 0.0307 & 0.3031 & 0.0167 & 0.0171 & \textbf{3.0} \\
        \bottomrule
    \end{tabular}
    }
    \end{sc}
    \end{small}
    \vskip -0.1in
\end{table*}

\subsubsection{Large Graph Generation}\label{sec:large_graph_generation}

To understand the scalability of \method{}, we evaluate its performance on the Proteins and Point Clouds datasets used by~\citet{liao2019efficient}. The results, shown in Table~\ref{tab:protein_and_pointcloud}, demonstrate that even when using a context window shorter than the longest sequence sampled from the dataset, \method{} achieves MMD ratios comparable to those observed on the Planar and SBM datasets. Furthermore, \method{} outperforms all existing methods in terms of MMD ratio, achieving a twofold or more improvement over the previous best model, ESGG. More significantly, while ESGG was specifically designed for generating unattributed graphs, \method{} demonstrates versatility by being applicable to both unattributed and attributed graphs.


\begin{table}[tbp]
    \centering
    \vskip -0.1in
    \caption{Comparison of {\scriptsize \method{}} to SOTA methods on QM9}
    \label{tab:qm9}
    \begin{small}
    \begin{sc}
    \resizebox{\columnwidth}{!}{
    \begin{tabular}{lccccc}\toprule
         & \multicolumn{5}{c}{QM9 with hydrogen atoms} \\ %
         & \multicolumn{5}{c}{$n_{\mathrm{graphs}}=100$K, $|V|_{\max}=29$, $|V|_{\avg}\approx 18$} \\ \cmidrule{2-6}
        Model & Valid$\shortuparrow$ & Unique$\shortuparrow$ & Novel$\shortuparrow$ & Atom stable$\shortuparrow$ & Mol stable$\shortuparrow$ \\ \midrule
        DiGress & 95.4 & \textbf{97.6} & 33.4 & 98.1 & 79.8 \\ \midrule
        \method{} & \textbf{97.7} & 96.7 & \textbf{45.5} & \textbf{98.6} &  \textbf{87.3}\\
        \bottomrule
    \end{tabular}
    }
    \end{sc}
    \end{small}
    \vskip -0.1in
\end{table}

\subsubsection{Molecular Graph Generation}

We demonstrate the applicability of our method to generating real-world attributed graphs, such as molecular structures. We evaluate \method{} on the same datasets used by DiGress~\citep{vignac2023digress}, including QM9 (all atoms)~\citep{wu2018moleculenet}, MOSES~\citep{polykovskiy2020moses}, and GuacaMol~\citep{brown2019guacamol}. Following the data splits and experimental setup from DiGress, we benchmark \method{} against a variety of SOTA models, including DiGress, VAE on SMILES~\citep{polykovskiy2020moses}, JT-VAE~\citep{jin2018junction}, GraphINVENT~\citep{mercado2021graphinvent}, NAGVAE~\citep{kwon2020nagvae}, LSTM and MCTS~\citep{brown2019guacamol}. On the QM9 dataset (Table~\ref{tab:qm9}), \method{} outperforms DiGress across all metrics except uniqueness, showing its superiority for attributed graphs.

For the more challenging MOSES and GuacaMol datasets, \method{} also demonstrates superior performance, achieving higher validity and improved distributional alignment as measured by metrics like FCD, as shown in Tables~\ref{tab:moses} and \ref{tab:guacamol}. Notably, to our best knowledge, \method{} is the first autoregressive model for graphs to surpass diffusion-based approaches on these datasets. It is worth mentioning that all metrics were computed using SMILES representations rather than molecular graphs. Due to the non-reversible nature of converting SMILES to graphs and back, where approximately 20\% of molecules cannot be mapped back to their original SMILES~\citep{vignac2023digress}, some discrepancies are introduced when calculating these metrics. Despite these challenges, \method{} achieves validity and FCD scores comparable to SMILES-based methods.

Furthermore, \method{} demonstrates remarkable efficiency, with training times of less than one day on both datasets, compared to up to one week for DiGress~\citep{vignac2023digress}. This substantial reduction in training time underscores \method{}'s practical advantages in large-scale and high throughput molecular graph generation tasks.


\begin{table}[tbp]
    \centering
    \vskip -0.1in
    \caption{Comparison of {\scriptsize \method{}} to SOTA methods on MOSES}
    \label{tab:moses}
    \begin{small}
    \begin{sc}
    \resizebox{\columnwidth}{!}{
    \begin{tabular}{llcccccc}\toprule
         & & \multicolumn{6}{c}{MOSES} \\ %
         & & \multicolumn{6}{c}{$n_{\mathrm{graphs}}= 1.58$M, $|V|_{\max}=27$, $|V|_{\avg}\approx 22$} \\ \cmidrule{3-8}
        Model & Type & Valid$\shortuparrow$ & Unique$\shortuparrow$ & Novel$\shortuparrow$ & Filters$\shortuparrow$ & FCD$\shortdownarrow$ & SNN$\shortdownarrow$ \\ \midrule %
        VAE & SMILES & 97.7 & 99.8 & 69.5 & 99.7 & 0.57 & 0.58 \\ %
        JT-VAE & Fragments & 100 & 100 & 99.9 & 97.8 & 1.00 & 0.53 \\ %
        GraphINVENT & Graph & 96.4 & 99.8 & – & 95.0  & 1.22 & 0.54 \\ %
        DiGress & Graph & 85.7 & 100 & 95.0 & 97.1 & 1.19 & 0.52 \\ \midrule %
        \method{} & Graph & 87.4 & 100 & 85.9 & 98.6 & 0.91 & 0.55 \\  %
        \bottomrule
    \end{tabular}
    }
    \end{sc}
    \end{small}
    \vskip -0.1in
\end{table}

\begin{table}[tbp]
    \centering
    \vskip -0.1in
    \caption{Comparison of {\scriptsize \method{}} to SOTA methods on {\scriptsize GuacaMol}}
    \label{tab:guacamol}
    \begin{small}
    \begin{sc}
    \resizebox{\columnwidth}{!}{
    \begin{tabular}{llccccc}\toprule
         & & \multicolumn{5}{c}{GuacaMol} \\ %
         & & \multicolumn{5}{c}{$n_{\mathrm{graphs}}=1.1$M, $|V|_{\max}=88$, $|V|_{\avg}\approx 28$} \\ \cmidrule{3-7}
        Model & Type & Valid$\shortuparrow$ & Unique$\shortuparrow$ & Novel$\shortuparrow$ & KL div$\shortuparrow$ & FCD$\shortuparrow$ \\ \midrule
        LSTM & SMILES & 95.9 & 100 & 91.2 & 99.1 & 91.3 \\
        NAGVAE & Graph & 92.7 & 95.5 & 100 & 38.4 & 0.9 \\
        MCTS & Graph & 100 & 100 & 99.4 & 52.2 & 1.5 \\
        DiGress & Graph & 85.2 & 100 & 99.9 & 92.9 & 68.0 \\ \midrule
        \method{} & Graph & 91.6 & 100 & 97.7 & 97.5 & 79.2  \\
        \bottomrule
    \end{tabular}
    }
    \end{sc}
    \end{small}
    \vskip -0.15in
\end{table}

\subsection{Transfer Performance of \method{}}
We evaluate the transferability of \method{} by pre-training it on a large dataset of synthetic graphs generated using NetworkX~\citep{hagberg2008networkx} and fine-tuning it on the unattributed graph datasets. Dataset and experimental details are provided in Appendix~\ref{app:sec:experimental_details}. As shown in Table~\ref{tab:transferability}, the pre-trained model consistently outperforms the baseline on small synthetic datasets in terms of the VUN score, achieving near-perfect validity. On larger graph datasets, the pre-trained model also surpasses the baseline across MMD metrics, demonstrating its ability to generalize to more complex structures. However, on small synthetic datasets, the pre-trained model shows a slight decline in MMD metrics compared to the baseline. These findings highlight the potential of building foundation models for graph generation and underscore the need for more comprehensive benchmark datasets, similar to those established in other domains.

\begin{table}[tbp]
    \centering
    \vskip -0.1in
    \caption{Transfer performance on downstream tasks using \method{} pre-trained on the NetworkX dataset. Red and green colors indicate relative decreases and increases respectively, compared to \method{} without pre-training.}
    \label{tab:transferability}
    \begin{small}
    \begin{sc}
    \resizebox{\columnwidth}{!}{
    \begin{tabular}{lcccccc}\toprule
        Dataset & Deg. & Clus. & Orbit & Spec. & Ratio & VUN (improv.) \\ \midrule
        NetworkX  & 0.0016 & 0.0073 & 0.0068 & 0.0020 & -- & -- \\ \midrule
        Planar & \cellcolor{red!75} 0.0007 & \cellcolor{red!34} 0.0811 & \cellcolor{red!66} 0.0005 & \cellcolor{green!5} 0.0061 & \cellcolor{red!47} 2.2 & \cellcolor{green!9} 95.0 (+7.5) \\
        SBM & \cellcolor{red!29} 0.0099 & \cellcolor{red!9} 0.0566 & \cellcolor{red!95} 0.0854 & \cellcolor{red!62.5} 0.0065 & \cellcolor{red!41} 4.8 & \cellcolor{green!5} 97.5 (+5) \\
        Proteins & \cellcolor{green!100} 0.0002 & \cellcolor{green!33} 0.0183 & \cellcolor{green!47} 0.0038 & \cellcolor{green!9} 0.0012 & \cellcolor{green!35} 1.7 & -- \\
        Point Clouds & \cellcolor{green!100} 0.0154 & \cellcolor{green!17} 0.2591 & \cellcolor{green!100} 0.0076 & \cellcolor{red!38} 0.0236 & \cellcolor{green!7} 2.8 & --  \\ \bottomrule
    \end{tabular}
    }
    \end{sc}
    \end{small}
    \vskip -0.1in
\end{table}

\subsection{Substructure Conditioned Generation}\label{sec:substructure_conditioned_generation}
We explore the ability of \method{} to perform substructure-conditioned generation without requiring fine-tuning. Given a subgraph $S$ (which could represent a functional motif of interest in drug discovery), we flatten the subgraph into a SENT sequence and condition the generation process on this sequence. This approach guarantees that the generated graph will contain $S$ as an induced subgraph (Thm.~\ref{thm:induced_subgraph}). As a proof-of-concept, we follow the methodology of~\citet{vignac2023digress,maziarz2022learning} and generate molecular graphs starting from a specific motif, called 1,4-Dihydroquinoline\footnote{\url{https://pubchem.ncbi.nlm.nih.gov/compound/1_4-Dihydroquinoline}}, using the model pre-trained on the GuacaMol dataset. Our results in Table~\ref{tab:substructure_conditioned_generation} demonstrate that this approach maintains similar validity, uniqueness, and novelty to unconditional generation (Table~\ref{tab:guacamol}). To further showcase the flexibility of this method, we test more extreme cases by replicating the same motif multiple times before performing the conditional generation. While validity decreases significantly when using an unrealistically large number of copies (\eg 5), the model still generates some visually plausible molecules (Appendix~\ref{app:sec:substructure_conditioned_generation}), showing superior flexibility over \citet{vignac2023digress}. These results highlight the potential of \method{} for important applications in drug discovery, particularly in motif scaffolding.



\begin{table}[tbp]
    \centering
    \vskip -0.1in
    \caption{Motif scaffolding using the motif 1,4-Dihydroquinoline.}
    \label{tab:substructure_conditioned_generation}
    \begin{small}
    \begin{sc}
    \resizebox{.75\columnwidth}{!}{
    \begin{tabular}{lccc}\toprule
        \# Copies of the motif & Valid & Unique & Novelty \\ \midrule
        1 & 92.0 & 98.8 & 99.6 \\
        2 & 88.8 & 99.7 & 100.0\\
        5 & 66.0 & 100.0 & 100.0 \\ \bottomrule
    \end{tabular}
    }
    \end{sc}
    \end{small}
    \vskip -0.1in
\end{table}

\subsection{Ablation Experiments}\label{sec:ablation_experiments}
In this study, we aim to understand the effectiveness of key components in \method{}.
\vspace{-.1in}
\paragraph{Comparison of sequence model architectures.}
\method{} provides a novel framework for evaluating the capability of current LLM architectures in graph generation and, more broadly, in structural reasoning tasks. In Table~\ref{tab:architecture}, we compare several state-of-the-art architectures on the Planar dataset, including GPT-2~\citep{radford2019gpt2}, Mamba~\citep{gu2023mamba}, and LLaMA~\citep{touvron2023llama}. While all models achieve comparable MMD ratios, transformer-based architectures, particularly LLaMA, demonstrate significantly better performance in terms of VUN scores compared to state-space models. These findings highlight the potential of \method{} to serve as a valuable benchmark for assessing sequence/language models' capabilities in graph generation tasks.

\begin{table}[tbp]
    \centering
    \vskip -0.1in
    \caption{Comparison of sequence model architectures on Planar}
    \label{tab:architecture}
    \begin{small}
    \begin{sc}
    \resizebox{\columnwidth}{!}{
    \begin{tabular}{lcccccc}\toprule
        Architecture & Deg. & Clus. & Orbit & Spec. & Ratio & VUN  \\ \midrule
        GPT-2 & 0.0004 & 0.0720 & 0.0010 & 0.0053 & 1.8 & 85.0 \\
        Mamba & 0.0002 &  0.0429 & 0.0014 & 0.0087 & 1.6 & 55.0 \\
        LLaMA & 0.0005 & 0.0651 & 0.0005 & 0.0056 & \textbf{1.6} & \textbf{90.0} \\ \bottomrule
    \end{tabular}
    }
    \end{sc}
    \end{small}
    \vskip -0.1in
\end{table}
\vspace{-.1in}
\paragraph{Effect of top-k sampling.}
A key advantage of \method{} over diffusion-based approaches is the flexibility to apply top-k sampling~\citep{fan2018topk} during inference, which can improve generation quality. As shown in Figure~\ref{fig:topk}, a smaller $k$ improves the VUN score on the Planar dataset, whereas it is not beneficial on the SBM dataset. In contrast, increasing $k$ generally improves MMD ratios across both datasets. These observations suggest that top-k sampling can be optimized based on dataset characteristics. In our experiments, we select the best $k$ that maximizes the VUN score for small synthetic datasets and minimizes the validation MMD ratios for other datasets. Importantly, this flexibility allows practitioners to select $k$ based on the specific performance criteria they aim to prioritize.

\begin{figure}[tbp]
    \centering
    \includegraphics[width=0.49\columnwidth]{img/ablation_k_planar.pdf}
    \includegraphics[width=0.49\columnwidth]{img/ablation_k_sbm.pdf}
    \vskip-.1in
    \caption{The effect of top-k sampling on the Planar and SBM datasets.}
    \label{fig:topk}
    \vskip -0.1in
\end{figure}

\vspace{-.1in}
\paragraph{Comparison of SET and SENT.}
As discussed in Section~\ref{sec:methods}, SENT is preferred over SET for graph generation, as incorporating neighborhood information is essential to ensure structural coherence. To empirically validate this, we compare the performance of SENT and SET on the Planar dataset and present the training curves in Figure~\ref{fig:set_vs_sent}. Consistent with our theoretical analysis, SET fails to produce high-validity graphs, resulting in a VUN score close to zero, whereas SENT successfully generates valid planar graphs.

\begin{figure}[tbp]
    \centering
    \includegraphics[width=\columnwidth]{img/SET_vs_SENT/SET_vs_SENT.pdf}
    \vskip-.1in
    \caption{SET vs SENT on the Planar dataset}
    \vskip-.1in
    \label{fig:set_vs_sent}
\end{figure}

\section{Conclusion}
We proposed \method{}, a scalable and efficient autoregressive model for attributed graph generation. \method{} demonstrates the ability to handle large graphs while preserving high generation quality. It enables substructure-conditioned generation without requiring additional fine-tuning and shows promising transfer capabilities. More importantly, it establishes a critical connection between graph modeling and language modeling, representing a significant step toward leveraging language modeling paradigms to address challenges in graph generation or more general graph learning tasks.

\section*{Acknowledgements}
The authors thank Dr.\ Till Hendrik Schulz, Philip Hartout, and Błażej Banaszewski for their insightful discussions and valuable feedback on the manuscript.

\section*{Impact Statement}
Our research focuses on advancing the algorithmic development of graph generative models, strongly emphasizing their responsible and ethical application in specialized fields. In domains such as drug discovery and synthetic biology, ensuring the trustworthiness and appropriate use of our methods is essential to prevent potential misuse. Through our experiments, we showcase the potential of our approach in these fields, underscoring its promise to deliver meaningful societal benefits while acknowledging the need to address potential risks.

\bibliography{mybib}
\bibliographystyle{arxiv}

\newpage
\appendix
\onecolumn
\input{arxiv_appendix}


\end{document}
