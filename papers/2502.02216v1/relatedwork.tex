\section{Related Work}
\paragraph{Autoregressive models for graph generation.}
Autoregressive models generate graphs by sequentially adding nodes and edges. GraphRNN~\citep{you2018graphrnn} pioneered this approach by framing graph generation as a sequence prediction task, demonstrating the capacity of recurrent neural networks~(RNNs)~\citep{chung2014empirical} to capture complex structures. DeepGMG~\citep{li2018learning} introduced a probabilistic policy framework for conditional generation, while GRAN~\citep{liao2019efficient} and BiGG~\citep{dai2020bigg} enhanced efficiency and scalability by generating multiple nodes and edges in parallel.

Recent research has focused on optimizing the generation order. \citet{chen2021order} highlighted that the ordering of node and edge additions impacts graph quality, and GraphARM~\citep{kong2023autoregressive} applied reinforcement learning to dynamically refine this order. \citet{goyal2020graphgen} incorporated logical constraints to improve domain-specific generation, and \citet{bacciu2020edge} proposed Bayesian reasoning to better capture graph dependencies.

Although these models have shown to be efficient and effective in synthetic datasets, they face inherent unification limitations, reducing their applicability to large-scale real-world datasets. Our proposed unified and powerful sequence representation of graphs aims to address these challenges.

\vspace{-.1in}
\paragraph{Other graph generative models.}

Other graph generative models include variational, GAN-based, and diffusion-based approaches. GraphVAEs~\citep{kipf2016variational,simonovsky2018graphvae} employ variational autoencoders to learn continuous latent representations, effectively generating small graphs but struggling with more complex structures. GAN-based models, such as NetGAN~\citep{bojchevski2018netgan} and SPECTRE~\citep{martinkus2022spectre}, generate graphs by modeling graph descriptors like random walks and spectral features.

Diffusion-based models iteratively refine noise into structured graphs through reverse diffusion steps. Continuous diffusion models~\citep{niu2020permutation,jo2022score} adapt denoising diffusion probabilistic models for graph generation. To leverage graph sparsity and structure, discrete diffusion models~\citep{vignac2023digress,kong2023autoregressive} have been developed. However, a key challenge for these models is the slow sampling process due to the long reverse diffusion chain. To mitigate this limitation, several efficient diffusion techniques have been proposed, including EDGE~\citep{chen2023efficient}, HiGen~\citep{karami2024higen}, ESGG~\citep{bergmeister2024efficient}, and Pard~\citep{zhao2024pard}.

\vspace{-.1in}
\paragraph{Random walks for graph learning.}
Random walks have been widely used in graph learning due to their strong expressive power. GCKN~\citep{chen2020convolutional} and RWGNN~\citep{nikolentzos2020random} utilize path and walk kernels to learn graph representations. Several recent works~\citep{ivanov2018anonymous,wang2021inductive,yin2022algorithm} explicitly integrate random walk sequences with positional encodings, inspiring subsequent methods such as CRaWL~\citep{tonshoff2023crawl}, NeuralWalker~\citep{chen2024neuralwalker} and RWNN~\citep{kim2024revisiting}, which further enhance graph representation learning via random walk sequence modeling. GraphGPT~\citep{zhao2023graphgpt} leverages Eulerian paths to improve graph property prediction. Our work uniquely explores random sequence representations of graphs focusing on graph generation, introducing a novel perspective on combining random walks and language modeling for scalable graph generative modeling.