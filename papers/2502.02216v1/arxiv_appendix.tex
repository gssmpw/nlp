\vspace*{0.3cm}
\begin{center}
    {\huge Appendix}
\end{center}
\vspace*{0.5cm}
This appendix provides both theoretical and experimental materials. It is organized as follows: Section~\ref{app:sec:background} provides additional background on sequence models. Section~\ref{app:sec:limitations} provides information about model availability and limitations. Section~\ref{app:sec:remarks} provides additional details and remarks on our method. Section~\ref{app:sec:proofs} provides proofs for the theorems presented in the main manuscript. Section~\ref{app:sec:experimental_details} provides experimental details. Section~\ref{app:sec:additional_results} provides additional quantitative and qualitative results.

\section{Background on Sequence Model Architectures}\label{app:sec:background}
Our sequence model architectures are fully based on established natural language models. In particular, we consider three prominent models, including GPT-2~\citep{radford2019gpt2}, LLaMA~\citep{touvron2023llama,touvron2023llama2}, and Mamba~\citep{gu2023mamba} to demonstrate the effectiveness of our approach. Notably, our methodology is not restricted to these specific models; it can be applied to any sequence or language model.

\paragraph{GPT-2.}
GPT-2 represents one of the earliest large language models based on the transformer architecture~\citep{vaswani2017attention}. The model employs pre-normalization with LayerNorm, the GeLU activation function, and absolute positional embeddings to encode token positions in sequences. These design choices laid the foundation for many subsequent models.

\paragraph{LLaMA.}
LLaMA~\citep{touvron2023llama,touvron2023llama2} builds upon the transformer framework with several key enhancements. It incorporates pre-normalization through RMSNorm~\citep{zhang2019rmsnorm} and employs the SwiGLU activation function~\citep{shazeer2020glu}. Additionally, LLaMA replaces absolute positional embeddings with rotary positional embeddings~\citep{su2024rope}, enabling better generalization to longer sequences.

\paragraph{Mamba.}
Mamba~\citep{gu2023mamba} is a state-space model (SSM) that maps input sequences to outputs using continuous-time dynamics. It introduces a selection mechanism that dynamically controls how input data flows into hidden states, making the model parameters adaptive to time and data. This innovation enables Mamba to achieve superior performance compared to other SSMs across various tasks.

\section{Availability and Limitations}\label{app:sec:limitations}

\subsection{Code and Model Availability}
Our code and all pre-trained models will be released upon publication.

\subsection{Limitations}
While \method{} demonstrates strong scalability on current graph generation benchmarks, we acknowledge that the datasets used in our study remain relatively small-scale compared to those used in pre-training LLMs. To push the boundaries of more powerful graph generative models or eventually foundation models for graphs, we draw the community's attention to building more comprehensive graph generation benchmarks and well-curated pre-training datasets.

\section{Additional Details about \method{}}\label{app:sec:remarks}
\subsection{Background on Graph Theory}
We provide additional background on graph theory necessary for the definitions and theories of SETs and SENTs. The background is largely based on~\citet{diestel2005graph}.

We first give the formal definition of \emph{graph isomorphism}:
\begin{definition}[Graph isomorphism]
    An isomorphism of graphs $G$ and $H$ is a bijection between the vertex sets of $G$ and $H$: $\pi:V_G\to V_H$ such that any two vertices $u$ and $v$ of $G$ are adjacent in $G$ if and only if $\pi(u)$ and $\pi(v)$ are adjacent in $H$, \ie $(u,v)\in E_G$ if and only if $(\pi(u),\pi(v))\in E_H$.
\end{definition}
Graph isomorphism is an equivalence relation on graphs and as such it partitions the class of all graphs into equivalence classes. A set of graphs isomorphic to each other is called an isomorphism class of graphs. It is worth noting that our SENT isomorphism also partitions the class of all SENTs into equivalence classes in a similar fashion.

We also provide the formal definition of \emph{induced subgraph}:
\begin{definition}[Induced subgraph]
    An induced subgraph of a graph is another graph, formed from a subset of the vertices of the graph and all of the edges, from the original graph, connecting pairs of vertices in that subset. Formally, let $S\subseteq V_G$ be any subset of vertices of $G:=(V_G,E_G)$. Then, the induced subgraph $G[S]$ is the graph whose vertex set is $S$ and whose edge set consists of all of the edges in $E_G$ that have endpoints in $S$. That is, for any two vertices $u, v\in S$, $(u,v)\in E_{G[S]}$ if and only if $(u,v)\in E_G$.
\end{definition}

\subsection{Remarks on Tokenized SENTs}\label{app:sec:random_walk_interpretation}
In Section~\ref{sec:tokenization}, we showed that an (ordered) SENT can be converted into a sequence of tokens. Here, we extend this idea by interpreting the tokenized sequence as a random walk on a slightly modified graph. We first introduce an alternative tokenization scheme that is equivalent to the one described earlier but offers enhanced interpretability. The proposed tokenization remains largely unchanged except for how tuples are handled. For each $w=(v,A)$ with $A=\{u_1,\dots,u_p\}$, we now define
\begin{equation*}
    \texttt{Token}(w):=\left[v, \bm{<}, u_1, \bm{<}, u_2\dots, \bm{<}, u_p, \bm{>}\right].
\end{equation*}
We now detail how to modify the original graph $G$: we introduce three virtual nodes, labeled $\textbf{/}$, $\bm{<}$, and $\bm{>}$ respectively. These virtual nodes are connected to all other virtual nodes and original nodes in the graph. This modification ensures that for any non-special token in the tokenized sequence, its subsequent token can either be one of its neighbors or one of the virtual nodes ($\textbf{/}$, $\bm{<}$, or $\bm{>}$). Consequently, each token has a direct connection to the node corresponding to the current token, and the language model amounts to learning the state transition functions for these random walks. Since these random walks are non-Markovian, this perspective further justifies our choice of using autoregressive models instead of one-step generative models. Furthermore, as random walks are random sequences on graphs, sampling random walks amounts to sampling from those random sequences. 

\subsection{Remarks on Model Inference}
The model inference is straightforward following the same process as LLMs such as LLaMA~\citep{touvron2023llama,touvron2023llama2}. An alternative way is to enforce the semantic correctness of the generated sequences of tokens by adjusting the logits at a certain token to obey the semantic rule of the tokenization. For instance, the token `$\bm{>}$' can only occur after a token `$\bm{<}$' or no special tokens can appear right after $\textbf{/}$. We manually implemented these transition constraints and incorporated them into the inference. We compared this strategy with the constraint-free counterpart. Surprisingly, our experiments demonstrate that the constraint-free variant could always generate semantically correct tokenized SENTs and perform similarly to the one with the transition constraints. Therefore, we did not use any transition constraints during the inference in our experiments.

\section{Proofs}\label{app:sec:proofs}
In this section, we provide proof for the theorems stated in the manuscript.
\printProofs

\section{Experimental Details}\label{app:sec:experimental_details}

\subsection{Datasets}
We provide details of the datasets used in our experiments. we adopt the standard train/validation/test splits provided in the original sources. The statistics about the datasets are summarized in Table~\ref{app:tab:datasets}.

\paragraph{Small synthetic graphs: Planar and SBM.}
Both of these datasets are from \citet{martinkus2022spectre}. The Planar dataset consists of 200 planar graphs with 64 nodes each, generated via Delaunay triangulation on points uniformly sampled in the unit square. The SBM dataset contains 200 graphs comprising 2 to 5 communities, with each community having between 20 and 40 nodes. An edge is placed between two nodes with probability 0.3 if they belong to the same community, and 0.05 otherwise. We follow the same splits as \citet{martinkus2022spectre}.

\paragraph{Large graphs: Proteins and Point Clouds.}
The Proteins dataset includes graph representations (contact maps) of proteins from \citet{dobson2003distinguishing}. In these graphs, each node represents an amino acid, and an edge connects two nodes if their corresponding amino acids are within 6 angstroms of each other. We use the same data splits as \citet{liao2019efficient}. The Point Clouds dataset, also from \citet{liao2019efficient}, consists of 41 point clouds of household objects \citep{neumann2013graph}. As many of these graphs are disconnected, we retain only the largest connected component of each, following \citet{bergmeister2024efficient}, and again employ the splits used by \citet{liao2019efficient}.

\paragraph{QM9.} 
The QM9 dataset, from \citet{wu2018moleculenet}, comprises small molecules with up to nine heavy atoms (carbon, oxygen, nitrogen, and fluorine). In this work, we adopt the more challenging setting proposed by \citet{vignac2023digress}, where hydrogen atoms are modeled explicitly, and we follow the same data splits as in that reference.

\paragraph{MOSES and GuacaMol.} 
The MOSES and GuacaMol datasets are obtained from the respective benchmark tools of \citet{polykovskiy2020moses} and \citet{brown2019guacamol}. Both consist of drug-like molecules, with those in GuacaMol typically being larger on average. For each dataset, we convert generated molecular graphs to SMILES using the code from \citet{jo2022score}, which permits partial charges. We employ the standard data splits provided by the corresponding benchmarks.

\paragraph{NetworkX.}
We generate the graphs using the generators from the NetworkX library\footnote{\url{https://networkx.org/documentation/stable/reference/generators.html}}~\citep{hagberg2008networkx}, categories including ``Classic'', ``Lattice'', ``Small'', ``Random Graphs'', ``Geometric'', ``Trees'', ``Community'', ``Social Networks''. We ensure that this dataset \emph{does not contain any graphs in the downstream datasets}. The summary of the code for generating these graphs is provided in Table~\ref{app:tab:networkx_dataset}. Notably, the largest graph has up to 5999 nodes.

\begin{table}[ht]
    \centering
    \caption{Dataset statistics}
    \label{app:tab:datasets}
    \begin{sc}
    \begin{tabular}{lccccccc} \toprule
        Dataset & \multicolumn{3}{c}{$n_{\mathrm{graphs}}$} & $|V|_{\max}$ & $|V|_{\avg}$ & $|E|_{\max}$ & $|E|_{\avg}$ \\ \cmidrule{2-4}
        & Train & Val & Test & \\ \midrule
        Unattributed Graphs \\
        Planar & 128 & 32 & 40 & 64 & 64 & 181 & 178 \\ 
        SBM & 128 & 32 & 40 & 187 & 104 & 1129 & 500 \\
        Proteins & 587 & 147 & 184 & 500 & 258 & 1575 & 646 \\
        Point Clouds & 26 & 7 & 8 & 5037 & 1332 & 10886 & 2971 \\ \midrule
        Attributed Graphs \\
        QM9 & 97734 & 20042 & 13055 & 29 & 18 & 28 & 19 \\
        MOSES & 1584663 & 176225 & 176074 & 27 & 22 & 31 & 23  \\
        GuacaMol & 1118633 & 69926 & 209654 & 88 & 28 & 88 & 30  \\ \midrule
        Pre-training unattributed graphs \\
        NetworkX & 24957 & 2516 & --- & 5999 & 459 & 5999 & 751 \\ 
        \bottomrule
    \end{tabular}
    \end{sc}
\end{table}

\begin{table}[ht]
    \centering
    \caption{Summary of the code for generating graphs in the NetworkX dataset}
    \label{app:tab:networkx_dataset}
    \begin{sc}
    \begin{lrbox}{\tablebox}
    \begin{tabular}{lcc}\toprule
        Generator & $n_{\mathrm{graphs}}$ & Python Code \\ \midrule
        \textbf{Category: Classic} \\
        balanced tree & 10 & \verb|nx.balanced_tree(2, np.random.randint(4, 10))| \\
        barbell graph & 100 & \verb|nx.barbell_graph(np.random.randint(3, 31), np.random.randint(41))| \\
        binomial tree &  10 & \verb|nx.binomial_tree(np.random.randint(2, 9))| \\
        complete graph & 10 & \verb|nx.complete_graph(np.random.randint(3, 31))| \\
        circular ladder graph & 300 & \verb|nx.circular_ladder_graph(np.random.randint(10, 501))| \\
        cycle graph & 2000 & \verb|nx.cycle_graph(np.random.randint(10, 6001))| \\
        Dorogovtsev Goltsev Mendes graph & 5 & \verb|nx.dorogovtsev_goltsev_mendes_graph(np.random.randint(2, 7))| \\
        ladder graph & 500 & \verb|nx.ladder_graph(np.random.randint(10, 1001))| \\
        lollipop graph & 200 & \verb|nx.lollipop_graph(np.random.randint(3, 21), np.random.randint(10, 51))| \\
        star graph & 200 & \verb|nx.star_graph(np.random.randint(10, 501))| \\
        turan graph & 100 & \verb|nx.turan_graph(np.random.randint(10, 41), 2)| \\
        wheel graph & 100 & \verb|nx.wheel_graph(np.random.randint(10, 201))| \\ \midrule
        \textbf{Category: Lattices} \\
        grid 2d graph & 400 & \verb|nx.grid_2d_graph(np.random.randint(5, 31), np.random.randint(5, 31))| \\
        triangular lattice graph & 400 & \verb|nx.triangular_lattice_graph(np.random.randint(5, 41), np.random.randint(5, 41))| \\ \midrule
        \textbf{Category: Small} \\
        all but the LCF graph & 1 (each) & \verb|nx.{method}()| \\ \midrule
        \textbf{Category: Random graphs} \\
        Erdos Renyi graph & 4000 & \verb|nx.erdos_renyi_graph(np.random.randint(20, 101), 0.2)| \\
        random regular graph & 2000 & \verb|nx.random_regular_graph(np.random.randint(3, 11), np.random.choice([20,30,...,500]))| \\
        Barabasi Albert graph & 4000 & \verb|nx.barabasi_albert_graph(np.random.randint(20, 501), np.random.randint(2, 6))| \\ 
        random lobster & 4000 & \verb|nx.random_lobster(80, 0.7, 0.7)| \\ \midrule
        \textbf{Category: Geometric} \\
        random geometric graph & 3000 & \verb|nx.random_geometric_graph(np.random.choice([20,30,...,100]), 0.3)| \\
        Waxman graph & 2000 & \verb|nx.waxman_graph(np.random.choice([50,100,150,...,300]))| \\ \midrule
        \textbf{Category: Trees} \\
        random unlabeled tree & 1000 & \verb|nx.random_unlabeled_tree(np.random.randint(20, 501))| \\ \midrule
        \textbf{Category: Community} \\ 
        connected Caveman graph & 300 & \verb|nx.connected_caveman_graph(np.random.randint(10, 101), np.random.randint(2, 5))| \\
        Windmill graph & 300 & \verb|nx.windmill_graph(np.random.randint(10, 101), np.random.randint(2, 5))| \\ \midrule
        \textbf{Category: Social networks} \\
        All social networks & 1 (each) & \verb|nx.{method}()| \\
        \bottomrule
    \end{tabular}
    \end{lrbox}
    \resizebox{\textwidth}{!}{\usebox{\tablebox}}
    \end{sc}
\end{table}

\subsection{Evaluation Metrics}\label{app:sec:evaluation_metrics}

We follow \citet{martinkus2022spectre} and \citet{vignac2023digress} in comparing our model’s performance with other graph generative approaches. Specifically, we measure the maximum mean discrepancy (MMD) between the generated and test graphs for degree distribution, clustering coefficient, orbit counts, and spectrum. As a reference, we also compute these metrics on the training set and report the mean ratio across all properties as a global indicator of statistical discrepancy between the generated samples and test samples. Note that for the Point Clouds dataset, which is defined by a $k$-nearest-neighbor structure, the degree MMD is always zero and is therefore excluded from the mean ratio. While we utilize these metrics to maintain consistency with previous research, we acknowledge their limitations, particularly regarding arbitrary kernel hyperparameter selection, as highlighted by~\citet{obray2022evaluation,thompson2022evaluation}.

We additionally track uniqueness and novelty: \emph{uniqueness} is the fraction of generated graphs that are not isomorphic to each other, and \emph{novelty} is the fraction of generated graphs that are not isomorphic to any training graph.

Below, we describe additional metrics specific to each dataset.

\paragraph{Planar and SBM.} 
Following \citet{martinkus2022spectre}, we report a \emph{validity score} for synthetic datasets. For Planar graphs, it verifies whether the generated graphs remain planar; for SBM graphs, it measures how likely they are to be generated under the original SBM parameters.

\paragraph{QM9.} 
For QM9, we report the \emph{validity}, \emph{uniqueness}, and \emph{novelty} defined for general molecules, as described in the following paragraph.
We also report \emph{atom stability} and \emph{molecule stability} as defined by \citet{hoogeboom2022equivariant} and \citet{vignac2023digress}.

\paragraph{MOSES and GuacaMol.} Since MOSES \citep{polykovskiy2020moses} and GuacaMol \citep{brown2019guacamol} are benchmarking platforms, each comes with its own suite of metrics, which we use to evaluate our model. These include: 
\begin{itemize} 
\item \textbf{Validity}: Proportion of molecules passing basic valency checks. 
\item \textbf{Uniqueness}: Proportion of generated molecules with distinct SMILES strings (indicating non-isomorphic structures). 
\item \textbf{Novelty}: Proportion of generated molecules not present in the training set. 
\item \textbf{Filter score}: Proportion of molecules passing the same filters used to create the test set. 
\item \textbf{Fréchet ChemNet Distance (FCD)}: Similarity measure between generated and training sets based on learned neural embeddings. 
\item \textbf{SNN}: Similarity to the nearest neighbor, computed via Tanimoto distance. 
\item \textbf{Scaffold similarity}: Comparison of Bemis–Murcko scaffold frequencies. 
\item \textbf{KL divergence}: Differences in the distributions of various physicochemical descriptors. 
\end{itemize}

\subsection{Computing Details}
We implemented our sequence models using the model hub of Hugging Face. Users can easily test their preferred sequence or language models using our code.
Experiments were conducted on a shared computing cluster with various CPU and GPU configurations,
including 16 NVIDIA H100 (80GB) GPUs. Each experiment was allocated
resources on a single GPU, along with 8 CPUs and up to 48GB of system RAM. The run-time of
each model was measured on a single NVIDIA H100 GPU.


\subsection{Hyperparameters}
Unlike prior studies that adjust model sizes across datasets, we maintain a consistent model architecture and size throughout all experiments, specifically using the small GPT configuration (768 hidden dimensions, 12 layers, 12 attention heads). Training hyperparameters are aligned with established practices from popular LLMs such as GPT-3~\citep{gpt3} and LLaMA~\citep{touvron2023llama}. We fix the context length to 2048 and use a batch size of 128 if possible, otherwise 64 for larger graphs. In particular, we employ the AdamW optimizer with a gradient clipping threshold of 1.0, a weight decay of 0.1, and a learning rate schedule with a linear warmup followed by cosine decay, peaking at 6e-4. The AdamW hyperparameters are set to $\beta=(0.9, 0.95)$. Due to the small dataset sizes of previous benchmarks, we tune the only training hyperparameter dropout in $\{0, 0.5\}$, and find the model achieves better validation loss with the value of 0.5 on the small synthetic datasets.

Inference hyperparameters, including $k$ (top-k sampling) and $\tau$ (temperature), are reported in Table~\ref{app:tab:inference_hyperparameters} and analyzed in detail in Section~\ref{sec:ablation_experiments} in the main manuscript.

\begin{table}[tbp]
    \centering
    \caption{Inference hyperparameters for each dataset.}
    \label{app:tab:inference_hyperparameters}
    \begin{sc}
    \begin{tabular}{lcccc}\toprule
        & \multicolumn{2}{c}{w/o pre-training} & \multicolumn{2}{c}{w/ pre-training} \\ \cmidrule{2-5}
        Dataset & Top-$k$ & Temperature $\tau$ & Top-$k$ & Temperature $\tau$ \\  \midrule
        Planar & 10 & 1.0 & 30 & 0.9 \\
        SBM & 60 & 1.0 & 150 & 1.0 \\
        Proteins & 40 & 1.0 & 30 & 1.05 \\
        Point Clouds & 60 & 1.0 & 20 & 0.9 \\ 
        NetworkX & 120 & 1.0 & --- & --- \\ \midrule
        QM9 & 5 & 1.0 & --- & ---\\
        MOSES & 5 & 1.0 & --- & --- \\
        GuacaMol & 5 & 1.0 & --- & --- \\ \bottomrule
    \end{tabular}
    \end{sc}
\end{table}

\section{Additional Results}\label{app:sec:additional_results}

\subsection{Additional Results on MOSES}
Due to space constraints, we provide additional metrics on the MOSES dataset in Table~\ref{app:tab:moses}.

\begin{table}[tbp]
    \centering
    \vskip -0.1in
    \caption{Comparison of {\scriptsize \method{}} to SOTA methods on the MOSES dataset}
    \label{app:tab:moses}
    \begin{small}
    \begin{sc}
    \begin{tabular}{llccccccc}\toprule
         & & \multicolumn{7}{c}{MOSES} \\ %
         & & \multicolumn{7}{c}{$n_{\mathrm{graphs}}= 1.58$M, $|V|_{\max}=27$, $|V|_{\avg}\approx 22$} \\ \cmidrule{3-9}
        Model & Type & Valid$\shortuparrow$ & Unique$\shortuparrow$ & Novel$\shortuparrow$ & Filters$\shortuparrow$ & FCD$\shortdownarrow$ & SNN$\shortdownarrow$ & Scaf$\shortuparrow$ \\ \midrule
        VAE & SMILES & 97.7 & 99.8 & 69.5 & 99.7 & 0.57 & 0.58 & 5.9 \\
        JT-VAE & Fragments & 100 & 100 & 99.9 & 97.8 & 1.00 & 0.53 & 10 \\
        GraphINVENT & Graph & 96.4 & 99.8 & – & 95.0  & 1.22 & 0.54 & 12.7 \\
        DiGress & Graph & 85.7 & 100 & 95.0 & 97.1 & 1.19 & 0.52 & 14.8 \\ \midrule
        \method{} & Graph & 87.4 & 100 & 85.9 & 98.6 & 0.91 & 0.55 & 10.2  \\
        \bottomrule
    \end{tabular}
    \end{sc}
    \end{small}
    \vskip -0.1in
\end{table}

\subsection{Transfer Performance of \method{}}\label{app:sec:transfer}
We provide here additional results for the transfer learning of \method{}. We compare the training curves of \method{} models with and without pre-training on the Planar datasets in Figure~\ref{app:fig:with_vs_without_pretraining}. The result suggests that the model with pre-training converges clearly faster.

\begin{figure}
    \centering
    \includegraphics[width=0.8\linewidth]{img/w_wo_pretraining.pdf}
    \caption{Comparison of \method{} with and without pre-training on the Planar dataset with 50000 training steps. The model with pre-training converges clearly faster than the model without pre-training.}
    \label{app:fig:with_vs_without_pretraining}
\end{figure}

\subsection{Substructure Conditioned Generation}\label{app:sec:substructure_conditioned_generation}

As presented in Section~\ref{sec:substructure_conditioned_generation}, we test more extreme cases by replicating the same motif multiple times before initiating the conditional generation. Figure~\ref{app:fig:1_motif_samples}, \ref{app:fig:2_motif_samples}, and \ref{app:fig:5_motif_samples} demonstrate non-curated samples generated by \method{} (trained on the GuacaMol dataset without any additional fine-tuning) conditioned on $p$ copies of the same motif, where $p=1, 2, 5$ respectively.

\begin{figure}[ht]
    \centering
    \includegraphics[width=\linewidth]{img/motif_generation/1_motif.pdf}
    \caption{Substructure conditioned generation on one copy of the motif.}
    \label{app:fig:1_motif_samples}
\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[width=\linewidth]{img/motif_generation/2_motif.pdf}
    \caption{Substructure conditioned generation on two copies of the motif.}
    \label{app:fig:2_motif_samples}
\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[width=\linewidth]{img/motif_generation/5_motif.pdf}
    \caption{Substructure conditioned generation on five copies of the motif.}
    \label{app:fig:5_motif_samples}
\end{figure}

\subsection{Comparison of Sequence Model Architectures}
We provide in Figure~\ref{app:fig:sequence_model} the curves of VUN scores vs training steps, which further demonstrates the advantages of transformer-based models compared to state-space models such as Mamba.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.5\linewidth]{img/architectures.pdf}
    \caption{Comparison of sequence model architectures: VUN score vs training steps. Transformer-based models clearly outperform state-space models such as Mamba.}
    \label{app:fig:sequence_model}
\end{figure}






\subsection{Visualization of Graphs Generated by \method{}}

\subsubsection{Results without Pre-training}
We provide visualization of non-curated samples generated by \method{} without pre-training on all datasets in Figure~\ref{app:fig:planar_samples}, \ref{app:fig:sbm_samples}, \ref{app:fig:protein_samples}, \ref{app:fig:pointcloud_samples}, \ref{app:fig:qm9_samples}, \ref{app:fig:moses_samples}, and \ref{app:fig:guacamol_samples}. The results on NetworkX are illustrated in Figure~\ref{app:fig:networkx_samples}. Node colors in unattributed graphs represent the eigenvectors associated with the second-smallest eigenvalues of the graph Laplacian.


\begin{figure}[ht]
    \centering
    \includegraphics[width=0.6\linewidth]{img/generated_graphs/planar.pdf}
    \caption{Non-curated samples generated by \method{} (without pre-training) trained on the Planar dataset.}
    \label{app:fig:planar_samples}
\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.6\linewidth]{img/generated_graphs/sbm.pdf}
    \caption{Non-curated samples generated by \method{} (without pre-training) trained on the SBM dataset.}
    \label{app:fig:sbm_samples}
\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.6\linewidth]{img/generated_graphs/protein.pdf}
    \caption{Non-curated samples generated by \method{} (without pre-training) trained on the Proteins dataset.}
    \label{app:fig:protein_samples}
\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.6\linewidth]{img/generated_graphs/point_cloud.pdf}
    \caption{Non-curated samples generated by \method{} (without pre-training) trained on the Point Clouds dataset.}
    \label{app:fig:pointcloud_samples}
\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[width=.8\linewidth]{img/generated_graphs/qm9.pdf}
    \caption{Non-curated samples generated by \method{} (without pre-training) trained on the QM9 dataset.}
    \label{app:fig:qm9_samples}
\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[width=\linewidth]{img/generated_graphs/moses.pdf}
    \caption{Non-curated samples generated by \method{} (without pre-training) trained on the MOSES dataset.}
    \label{app:fig:moses_samples}
\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[width=\linewidth]{img/generated_graphs/guacamol.pdf}
    \caption{Non-curated samples generated by \method{} (without pre-training) trained on the GuacaMol dataset.}
    \label{app:fig:guacamol_samples}
\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[width=\linewidth]{img/generated_graphs/networkx.pdf}
    \caption{Non-curated samples generated by \method{} trained on the NetworkX dataset.}
    \label{app:fig:networkx_samples}
\end{figure}

\subsubsection{Results with pre-training}
We provide visualization of non-curated samples generated by \method{} with pre-training (on the NetworkX dataset) trained on the non-attributed datasets including Planar, SBM, Proteins, and Point Clouds, illustrated in Figure~\ref{app:fig:planar_samples_pretrained}, \ref{app:fig:sbm_samples_pretrained}, \ref{app:fig:protein_samples_pretrained}, \ref{app:fig:pointcloud_samples_pretrained}.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.6\linewidth]{img/generated_graphs/finetune/planar.pdf}
    \caption{Non-curated samples generated by \method{} (with pre-training on the NetworkX dataset) trained on the Planar dataset.}
    \label{app:fig:planar_samples_pretrained}
\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.6\linewidth]{img/generated_graphs/finetune/sbm.pdf}
    \caption{Non-curated samples generated by \method{} (with pre-training on the NetworkX dataset) trained on the SBM dataset.}
    \label{app:fig:sbm_samples_pretrained}
\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.6\linewidth]{img/generated_graphs/finetune/protein.pdf}
    \caption{Non-curated samples generated by \method{} (with pre-training on the NetworkX dataset) trained on the Proteins dataset.}
    \label{app:fig:protein_samples_pretrained}
\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.6\linewidth]{img/generated_graphs/finetune/point_cloud.pdf}
    \caption{Non-curated samples generated by \method{} (with pre-training on the NetworkX dataset) trained on the Point Clouds dataset.}
    \label{app:fig:pointcloud_samples_pretrained}
\end{figure}