\section{Related Work}
\paragraph{Autoregressive models for graph generation.}
Autoregressive models generate graphs by sequentially adding nodes and edges. GraphRNN____ pioneered this approach by framing graph generation as a sequence prediction task, demonstrating the capacity of recurrent neural networks~(RNNs)____ to capture complex structures. DeepGMG____ introduced a probabilistic policy framework for conditional generation, while GRAN____ and BiGG____ enhanced efficiency and scalability by generating multiple nodes and edges in parallel.

Recent research has focused on optimizing the generation order. ____ highlighted that the ordering of node and edge additions impacts graph quality, and GraphARM____ applied reinforcement learning to dynamically refine this order. ____ incorporated logical constraints to improve domain-specific generation, and ____ proposed Bayesian reasoning to better capture graph dependencies.

Although these models have shown to be efficient and effective in synthetic datasets, they face inherent unification limitations, reducing their applicability to large-scale real-world datasets. Our proposed unified and powerful sequence representation of graphs aims to address these challenges.

\vspace{-.1in}
\paragraph{Other graph generative models.}

Other graph generative models include variational, GAN-based, and diffusion-based approaches. GraphVAEs____ employ variational autoencoders to learn continuous latent representations, effectively generating small graphs but struggling with more complex structures. GAN-based models, such as NetGAN____ and SPECTRE____, generate graphs by modeling graph descriptors like random walks and spectral features.

Diffusion-based models iteratively refine noise into structured graphs through reverse diffusion steps. Continuous diffusion models____ adapt denoising diffusion probabilistic models for graph generation. To leverage graph sparsity and structure, discrete diffusion models____ have been developed. However, a key challenge for these models is the slow sampling process due to the long reverse diffusion chain. To mitigate this limitation, several efficient diffusion techniques have been proposed, including EDGE____, HiGen____, ESGG____, and Pard____.

\vspace{-.1in}
\paragraph{Random walks for graph learning.}
Random walks have been widely used in graph learning due to their strong expressive power. GCKN____ and RWGNN____ utilize path and walk kernels to learn graph representations. Several recent works____ explicitly integrate random walk sequences with positional encodings, inspiring subsequent methods such as CRaWL____, NeuralWalker____ and RWNN____, which further enhance graph representation learning via random walk sequence modeling. GraphGPT____ leverages Eulerian paths to improve graph property prediction. Our work uniquely explores random sequence representations of graphs focusing on graph generation, introducing a novel perspective on combining random walks and language modeling for scalable graph generative modeling.