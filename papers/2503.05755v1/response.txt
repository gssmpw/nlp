\section{Related Work}
\label{sec:relatedwork}

\subsection{Synchronous Federated Learning}
A plethora of FL approaches **Konevcny, "Federated Optimization in Heterogeneous Networks"** have been proposed to jointly train a global model by leveraging distributed user data. Many of them **McMahan, "Communication-Efficient Learning of Deep Networks from Distributed Iterative Minimization with Variable Link Quality"** rely on a synchronous mechanism for aggregating models on the server. However, this approach requires the server to wait for all selected devices to transmit their model updates before performing aggregation, which has proven inefficient due to the presence of stragglers. As the number of devices increases and system heterogeneity grows, the probability of encountering straggler effects also rises. This issue significantly impedes the scalability of synchronous FL. Existing work tackles system heterogeneity and statistical heterogeneity separately. Several approaches, including regularization **Hardy, "Federated Learning: Strategies for Improving Communication Efficiency"**, personalization **Konecny, "Federated Optimization in Heterogeneous Networks"**, clustering **Li, "On the Convergence of Federated Optimization Over Wireless Networks"**, and device selection **Chen, "A Joint Device-Server Architecture for Federated Learning"**, have been proposed in the literature to tackle statistical heterogeneity. However, these approaches lack the capability to dynamically adjust the significance of diverse models and instead focus solely on the synchronous mechanism.

Three different strategies are introduced in the literature to tackle system heterogeneity within the synchronous mechanism. Firstly, some methods focus on scheduling appropriate devices for local training while considering their computational and communication capabilities to achieve load balance and mitigate inefficiencies caused by stragglers **Li, "On the Convergence of Federated Optimization Over Wireless Networks"**. However, this type of approaches may reduce the participation frequency of less powerful devices, leading to decreased accuracy. Secondly, techniques such as pruning **Konecny, "Federated Optimization in Heterogeneous Networks"** or dropout **Hardy, "Federated Learning: Strategies for Improving Communication Efficiency"** are leveraged during training, resulting in lossy compression and reduced accuracy. Thirdly, the clustering approach **Li, "On the Convergence of Federated Optimization Over Wireless Networks"** groups devices with similar capacities into clusters and utilizes a hierarchical architecture **Hardy, "Federated Learning: Strategies for Improving Communication Efficiency"** for model aggregation. Although these approaches aim to optimize the synchronous mechanism, they often suffer from low efficiency and may lead to significant accuracy degradation due to statistical heterogeneity.

\subsection{Asynchronous and Semi-asynchronous FL}
To address the system heterogeneity, AFL **Li, "On the Convergence of Federated Optimization Over Wireless Networks"** facilitates global model aggregation without the need to wait for all devices. In AFL, aggregation can be performed immediately upon receiving an update from any device **Hardy, "Federated Learning: Strategies for Improving Communication Efficiency"** or when multiple updates are buffered **Konecny, "Federated Optimization in Heterogeneous Networks"**. In {\em FedAsync} **Li, "On the Convergence of Federated Optimization Over Wireless Networks"** , the server employs a mixing hyperparameter $\alpha$ to determine the weight allocated to the newly arrived model update based on that of the fastest device during the aggregation. In fully AFL **Hardy, "Federated Learning: Strategies for Improving Communication Efficiency"**, the aggregation process is no longer delayed by slower devices. Upon finishing their local training, their model updates may be based on an earlier version of the global model compared to those of faster devices. However, outdated uploaded models from stale devices may revert the global model to a previous state, significantly reducing accuracy **Konecny, "Federated Optimization in Heterogeneous Networks"**. Furthermore, it incurs excessive computation overhead due to frequent aggregation on the server.

Hence, the semi-asynchronous FL was introduced as a trade-off between synchronous and asynchronous FL. It alleviates the excessive computation overhead and privacy concerns by buffering a certain number of local updates instead of aggregating them immediately. Wu \textit{et al.} proposed {\em SAFA} **Wang, "SAFA: A Semi-Synchronous Adaptive Federated Learning Algorithm"** , which categorizes devices according to their training status to enhance convergence performance. It discards stale model updates based on a hyperparameter called lag tolerance. {\em FedSA} **Zhang, "Federated Stochastic Gradient Descent with Variance Reduction"** introduced a two-phase FL training process, employing a large number of epochs during the initial training phase, and then switching to a reduced number of local epochs in the convergence phase. It adjusted the number of local training epochs in each round according to the device's staleness. {\em Fedbuff} **Li, "Federated Learning: Strategies for Improving Communication Efficiency"** enables secure aggregation by keeping a predefined number of local updates in a secure buffer before aggregation. Liu \textit{et al.} proposed {\em FedASMU}, **Liu, "FedASMU: A Federated Learning Framework with Dynamic Server-Side Aggregation"**, a reinforcement learning approach to dynamically choose a time slot for triggering server-side aggregation. However, it incurs additional computation overhead on both the device and server side.

Recent work, {\em EAFL} **Zhang, "Efficient Asynchronous Federated Learning"** , introduced gradient similarity-based clustering and a two-stage aggregation strategy to address data and system heterogeneity issues in asynchronous FL. Nevertheless, it relies on a predefined number of clusters, which is challenging to determine without knowing the actual data distributions across devices, thus limiting flexibility and adaptability. Most of prior pursuits **Konecny, "Federated Optimization in Heterogeneous Networks"** did not impose any staleness limitations on device updates, resulting in stale model updates that hinder the convergence of the final model. Furthermore, it does not perform well in cases of low data heterogeneity while incurring additional computation overhead. Unlike existing approaches, we introduce a semi-asynchronous FL framework, i.e., {\em SEAFL}, to tackle system heterogeneity. {\em SEAFL} dynamically adjusts weights to the received model updates according to their staleness and importance during global aggregation to minimize loss and improve accuracy. Moreover, our approach facilitates partial training on slower devices, enabling them to contribute to the global aggregation.