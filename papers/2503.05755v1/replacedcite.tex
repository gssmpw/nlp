\section{Related Work}
\label{sec:relatedwork}

\subsection{Synchronous Federated Learning}
A plethora of FL approaches ____ have been proposed to jointly train a global model by leveraging distributed user data. Many of them ____ rely on a synchronous mechanism for aggregating models on the server. However, this approach requires the server to wait for all selected devices to transmit their model updates before performing aggregation, which has proven inefficient due to the presence of stragglers. As the number of devices increases and system heterogeneity grows, the probability of encountering straggler effects also rises. This issue significantly impedes the scalability of synchronous FL. Existing work tackles system heterogeneity and statistical heterogeneity separately. Several approaches, including regularization ____, personalization ____, clustering ____, and device selection ____, have been proposed in the literature to tackle statistical heterogeneity. However, these approaches lack the capability to dynamically adjust the significance of diverse models and instead focus solely on the synchronous mechanism.

Three different strategies are introduced in the literature to tackle system heterogeneity within the synchronous mechanism. Firstly, some methods focus on scheduling appropriate devices for local training while considering their computational and communication capabilities to achieve load balance and mitigate inefficiencies caused by stragglers ____. However, this type of approaches may reduce the participation frequency of less powerful devices, leading to decreased accuracy. Secondly, techniques such as pruning ____ or dropout ____ are leveraged during training, resulting in lossy compression and reduced accuracy. Thirdly, the clustering approach ____ groups devices with similar capacities into clusters and utilizes a hierarchical architecture ____ for model aggregation. Although these approaches aim to optimize the synchronous mechanism, they often suffer from low efficiency and may lead to significant accuracy degradation due to statistical heterogeneity. 
\subsection{Asynchronous and Semi-asynchronous FL}
To address the system heterogeneity, AFL ____ facilitates global model aggregation without the need to wait for all devices. In AFL, aggregation can be performed immediately upon receiving an update from any device ____ or when multiple updates are buffered ____. In {\em FedAsync} ____, the server employs a mixing hyperparameter $\alpha$ to determine the weight allocated to the newly arrived model update based on that of the fastest device during the aggregation. In fully AFL ____, the aggregation process is no longer delayed by slower devices. Upon finishing their local training, their model updates may be based on an earlier version of the global model compared to those of faster devices. However, outdated uploaded models from stale devices may revert the global model to a previous state, significantly reducing accuracy ____. Furthermore, it incurs excessive computation overhead due to frequent aggregation on the server.

Hence, the semi-asynchronous FL was introduced as a trade-off between synchronous and asynchronous FL. It alleviates the excessive computation overhead and privacy concerns by buffering a certain number of local updates instead of aggregating them immediately. Wu \textit{et al.} proposed {\em SAFA} ____, which categorizes devices according to their training status to enhance convergence performance. It discards stale model updates based on a hyperparameter called lag tolerance. {\em FedSA} ____ introduced a two-phase FL training process, employing a large number of epochs during the initial training phase, and then switching to a reduced number of local epochs in the convergence phase. It adjusted the number of local training epochs in each round according to the device's staleness. {\em Fedbuff} ____ enables secure aggregation by keeping a predefined number of local updates in a secure buffer before aggregation. Liu \textit{et al.} proposed {\em FedASMU}, ____ a reinforcement learning approach to dynamically choose a time slot for triggering server-side aggregation. However, it incurs additional computation overhead on both the device and server side. 

Recent work, {\em EAFL} ____, introduced gradient similarity-based clustering and a two-stage aggregation strategy to address data and system heterogeneity issues in asynchronous FL. Nevertheless, it relies on a predefined number of clusters, which is challenging to determine without knowing the actual data distributions across devices, thus limiting flexibility and adaptability.  Most of prior pursuits ____ did not impose any staleness limitations on device updates, resulting in stale model updates that hinder the convergence of the final model. Furthermore, it does not perform well in cases of low data heterogeneity while incurring additional computation overhead. Unlike existing approaches, we introduce a semi-asynchronous FL framework, i.e., {\em SEAFL}, to tackle system heterogeneity. {\em SEAFL} dynamically adjusts weights to the received model updates according to their staleness and importance during global aggregation to minimize loss and improve accuracy. Moreover, our approach facilitates partial training on slower devices, enabling them to contribute to the global aggregation.