%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
% 新增
\usepackage{multirow}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Jakiro}

\begin{document}

\twocolumn[
\icmltitle{\raisebox{-0.25cm}{\includegraphics[height=0.8cm]{images/jakiro.pdf}}\hspace{0.02cm}Jakiro: Boosting Speculative Decoding with Decoupled Multi-Head via MoE}


% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Haiduo Huang}{comp,yyy}
\icmlauthor{Fuwei Yang}{comp}
\icmlauthor{Zhenhua Liu}{comp}
\icmlauthor{Yixing Xu}{comp}
\icmlauthor{Jinze Li}{sch}
\icmlauthor{Yang Liu}{comp}
\icmlauthor{Xuanwu Yin}{comp}
\icmlauthor{Dong Li}{comp}
\icmlauthor{Pengju Ren}{yyy}
\icmlauthor{Emad Barsoum}{comp}

\end{icmlauthorlist}

\icmlaffiliation{comp}{Advanced Micro Devices, Inc., Beijing, China}
\icmlaffiliation{yyy}{Institute of Artificial Intelligence and Robotics, Xi'an Jiaotong University, Xi'an, China}
\icmlaffiliation{sch}{University of HongKong, HongKong, China}
% \icmlaffiliation{crf}{Chinese Academy of Sciences, Academy of Mathematics and Systems Science, Beijing, China}

\icmlcorrespondingauthor{Haiduo Huang}{huanghd@stu.xjtu.edu.cn}
% \icmlcorrespondingauthor{Fuwei Yang}{fuwei.yang@amd.com}
% \icmlcorrespondingauthor{Zhenhua Liu}{zhenhua.liu@amd.com}
% \icmlcorrespondingauthor{Yixing Xu}{yixing.xu@amd.com}
% \icmlcorrespondingauthor{Jinze Li}{lijinze-hku@connect.hku.hk}
% \icmlcorrespondingauthor{Yang Liu}{liuyang2020@amss.ac.cn}
% \icmlcorrespondingauthor{Xuanwu Yin}{xuanwu.yin@amd.com}
% \icmlcorrespondingauthor{Dong Li}{d.li@amd.com}
% \icmlcorrespondingauthor{Pengju Ren}{pengjuren@xjtu.edu.cn}
% \icmlcorrespondingauthor{Emad Barsoum}{ebarsoum@amd.com}


% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, Large Language Models, Mixture of Expert, Speculative Decoding}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
% \printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\hbadness=10000 %禁用警告
\begin{abstract}
      Speculative decoding (SD) accelerates large language model inference by using a smaller draft model to predict multiple tokens, which are then verified in parallel by the larger target model. However, the limited capacity of the draft model often necessitates tree-based sampling to improve prediction accuracy, where multiple candidates are generated at each step. We identify a key limitation in this approach: the candidates at the same step are derived from the same representation, limiting diversity and reducing overall effectiveness. To address this, we propose Jakiro, leveraging Mixture of Experts (MoE), where independent experts generate diverse predictions, effectively decoupling correlations among candidates. Furthermore, we introduce a hybrid inference strategy, combining autoregressive decoding for initial tokens with parallel decoding for subsequent stages, and enhance the latter with contrastive mechanism in features to improve accuracy. Our method significantly boosts prediction accuracy and achieves higher inference speedups. Extensive experiments across diverse models validate the effectiveness and robustness of our approach, establishing a new SOTA in speculative decoding. Our codes are available at \url{https://github.com/haiduo/Jakiro}.
\end{abstract}

\section{Introduction}
\label{Introduction}
\begin{figure}[ht]
    \centering
    \includegraphics[width=0.85\linewidth]{images/differ_methods.pdf}
    \caption{Comparison of different speculative decoding methods.}
    \label{fig:differ_methods}
%     \vspace{-1.5em}
    \vskip -0.2in
\end{figure}
Large language models (LLMs), such as GPT-4o~\cite{jaech2024openai}, LLaMA3~\cite{dubey2024llama} and Deepseek-r1~\cite{guo2025deepseek}, have demonstrated remarkable capabilities across a wide range of applications, including question-answering, code synthesis, and machine translation. However, their token-by-token decoding process, combined with the growing size of the models, leads to significant inference latency, posing challenges for real-world deployment. Recently, speculative decoding~\cite{leviathan2023fast,chen2023accelerating} has emerged as an effective technique to accelerate LLM inference. This approach utilizes an efficient but weak draft model to predict multiple tokens in sequence, which are then verified in parallel by a more powerful but expensive target model. Since LLMs inference is often memory access bound~\cite{shazeer2019fast}, the verification stage can efficiently leverage hardware parallelism, achieving substantial and lossless speedups.

\begin{figure*}[ht]
    \centering
    \includegraphics[width=0.9\linewidth]{images/A100-40G_MTbench_S_Comparison_Temperature_1.pdf}
    \caption{Speedup ratio of Vicuna, LLaMA2-chat, and LLaMA3-instruct models inference latency on the MT-bench for non-greedy (Temperature=1) settings. The above reproduction results are based on the open-source code from the original paper and are averaged over four inference runs on an A100-40G GPU. In this paper, we only compare with speculative sampling based methods that do not need to finetune the backbone models, ensuring the output text distribution remains constant.}
    \label{fig:MTbench_S_Comparison_T=1}
%     \vspace{-1.0em}
    \vskip -0.2in
\end{figure*}

Recent works, such as Medusa~\cite{cai2024medusa} and Hydra~\cite{ankner2024hydra}, leverage multiple independent heads to predict the next N tokens, with all heads relying on the same final-layer features of the target model. This shared dependency limits the decoupling of predictions for different tokens, thereby constraining the overall prediction accuracy. In contrast, Eagle~\cite{li2024eagle} and Eagle2~\cite{li2024eagle2} adopts an autoregressive approach, where predictions at different time steps are based on the features from the previous step. Although the Eagle-style approaches successfully decouples draft tokens across different time steps, the top-$k$ tokens within the same layer of the draft tree remain coupled, limiting the diversity of predicted tokens and constraining their immense potential.

To enhance prediction accuracy, both Medusa-style and Eagle-style utilize a tree-based attention mechanism~\cite{miao2023specinfer} to generate multiple candidate tokens simultaneously. However, we observe that these candidates are derived from the same feature representation, resulting in inherent correlations that limit the predictive accuracy of the draft model. To address this, we propose Jakiro with a dynamic decoupling mechanism that leverages an MoE mechanism~\cite{jiang2024mixtral} to construct the attention tree, which retains the independence between layers of the traditional draft tree while introducing decoupling within the layer. By assigning different experts to generate candidate tokens, our method effectively decouples the correlations among predictions. This approach increases the draft model's representation diversity while introducing minimal computational cost. As a result, our method significantly improves the accuracy of the draft model's prediction, leading to a superior overall speedup.

Moreover, due to the inherent exposure bias~\cite{arora2022exposure} in the autoregressive inference of LLMs, the prediction error increases as the sequence length grows. Considering the inference overhead of the draft model itself, we observe that it is not wise to adopt an autoregressive approach to construct the draft sequence in every step. Instead, we adopt a parallel decoding strategy~\cite{xia2024unlocking} for token generation in the last two steps, further improving the overall inference efficiency of LLMs. To further enhance performance during the parallel decoding phase, we introduce a contrastive mechanism~\cite{li2022contrastive} into MoE's dual-branch heads to improve the helpfulness of predictions, further boosting SD performance in the last two steps. Unlike SCMoE~\cite{shi2024unchosen}, which utilizes unselected experts in a self-contrastive manner during inference, we only perform contrastive operations between the two activated expert heads, introducing almost no additional latency. An overview comparison of different speculative decoding methods is shown in~\cref{fig:differ_methods}.

Compared to existing methods, extensive experiments conducted on common models and benchmarks have validated the effectiveness and robustness of our method. For example, we achieve notable advancements over previous state-of-the-art (SOTA) method on the MT-bench for non-greedy mode, as shown in~\cref{fig:MTbench_S_Comparison_T=1}. Our main contributions can be described as:
\begin{itemize}
      \vspace{-0.8em}
      \item {\bf Dynamic Decoupling with MoE in Draft Attention Tree Construction}: This paper is the first to propose leveraging Mixture of Experts (MoE) to achieve dynamic decoupling during the construction of the draft attention tree. This approach enhances the diversity of draft tokens and improves the efficiency of speculative decoding, regardless of whether a static or dynamic tree structure is used.
      \item {\bf Hybrid Inference Strategy}: A hybrid inference strategy is introduced, combining autoregressive token-by-token prediction for initial tokens with parallel decoding for later stages. During the parallel decoding phase, we innovatively incorporate a contrastive mechanism in the output features to further enhance performance.
      \item {\bf Comprehensive Benchmark Evaluation}: We conduct comprehensive experiments across various benchmarks and target models to demonstrate the effectiveness and robustness of our approach. Our approach achieves SOTA performance in speculative decoding.
      \vspace{-0.8em}
\end{itemize}


\section{Preliminaries}
\subsection{Speculative Decoding}

Speculative decoding~\cite{leviathan2023fast} is designed to accelerate inference from autoregressive LLMs without altering the output distribution. A smaller, computationally efficient draft model is employed to generate candidate tokens in the drafting phase, while a larger, high-quality target model verifies and refines these candidates. The key idea behind speculative decoding is to parallelize the token generation process, allowing multiple tokens to be proposed at once. This is achieved by observing that complex language-modeling tasks often contain subtasks that can be approximated by smaller models with adequate accuracy. By using these approximations, speculative decoding achieves significant speedup while maintaining the statistical properties of the target model's output. 

Suppose $t_i$ denotes the $i$-th token, and let $T_{x:y}$ represent the token sequence $t_x, t_{x+1}, \cdots, t_y$. The speculative decoding process involves three steps:
\begin{enumerate}
    \vspace{-0.8em}
    \item {\bf Drafting:} Given a prefix $T_{1:j}$, the draft model $q(.|.)$ autoregressively generates a sequence of candidate tokens $T_{j+1:j+\gamma}$, each accompanied by a probability distribution $q_{j+1:j+\gamma}$, where $\gamma$ is the number of inference steps per round for the draft model.
    \item {\bf Verification:} The target model $p(.|.)$ computes the conditional probabilities $p_{j+1:j+\gamma}$ for the same sequence in a single forward pass. Each candidate token $t_{j+i}$ is sequentially evaluated based on an acceptance criterion, such as $\min(1,p_{j+i}(t_{j+i})/q_{j+i}(t_{j+i}))$.
    \item {\bf Refinement:} Accepted tokens are appended to the output sequence, while rejected tokens are resampled using a normalized probability distribution $\text{norm}(\max(0,p_{j+i}-q_{j+i}))$ to replace $t_{j+i}$ and discards the remaining tokens in the drafting.
    \vspace{-0.8em}
\end{enumerate}
The method ensures that the output distribution remains consistent with vanilla autoregressive decoding for both greedy and non-greedy sampling strategies, as proven in prior works~\cite{leviathan2023fast,chen2023accelerating}. 

\subsection{Medusa-style Decoding}
Medusa-style~\cite{cai2024medusa,ankner2024hydra} decoding extends speculative decoding by employing multiple lightweight draft heads, typically implemented as small multi-layer perceptrons (MLPs), atop the last hidden state of the target LLM. These heads independently predict tokens based on the last hidden state of the target LLM, and their predictions are combined into candidate continuations. However, the shared use of the last hidden state across all heads results in incomplete decoupling, limiting the ability of each head to make diverse and independent predictions.

\subsection{Eagle-style Decoding}
In Eagle-style~\cite{li2024eagle,li2024eagle2} decoding, the drafting stage decouples draft tokens at different time steps by performing autoregression at the feature level (before the LM head). The LM head of the original LLM is then used to convert features into draft tokens. This approach reduces uncertainty in the feature sequence, resulting in improved draft predictions. However, the top-$k$ tokens within the same layer of the draft tree remain coupled, which limits the diversity and potential of the predictions.

Eagle1 adopts a static tree-structured draft~\cite{miao2023specinfer} in the verification stage, allowing branching paths to explore alternative continuations when draft tokens are rejected. This structure increases robustness by avoiding the full discard of draft sequences when individual tokens fail to meet the target model's criteria. Eagle2 improves upon Eagle1 by introducing dynamically adjustable draft trees. This allows the tree to adapt based on runtime conditions, optimizing token prediction and verification. Despite this enhancement, the lack of intra-layer decoupling in draft trees still limits the diversity of generated tokens, particularly in non-greedy sampling modes.

\section{Jakiro}
\subsection{Dynamic Decoupling and MoE Tree Construction}
\subsubsection{Dynamic Decoupling with MoE Heads}
% Our proposed {\bf Jakiro} (Mixture of Experts Speculative Decoding) method addresses the limitations of both Eagle-style and Medusa-style decoding by introducing a {\bf dynamic decoupling mechanism} for draft token prediction. 
Unlike Medusa, which relies on the last hidden state for all draft heads, Jakiro employs multiple MoE heads that dynamically allocate expert modules to predict tokens. This mechanism accounts for inherent differences between tokens and ensures the decoupling of token predictions across heads. As a result, prediction confidence is improved while maintaining computational efficiency. 

\begin{figure}[ht]
      \centering
      \includegraphics[width=0.75\linewidth]{images/compare_eagel.pdf}
      \caption{Comparison of different building methods of draft tree.}
      \label{fig:compare_eagel}
      % \vspace{-1.5em}
      \vskip -0.1in
\end{figure}

The structure of our draft model follows the design principles of the Eagle-style, utilizing a single decoder layer that includes a reduction dimension layer, an LLM attention layer, and parallel expert heads (lightweight MLP layers). And, the embedding layer and head layer remain consistent with the target model without introducing additional parameters. This ensures that inference efficiency is not compromised by the increase in the number of parameters. The detailed structure is shown in~\cref{fig:compare_eagel} (2).

\subsubsection{ MoE Tree Construction}
Jakiro introduces a novel tree construction method that decouples the intra-layer dependencies of traditional draft trees while retaining the independence between inter-layers. This design ensures that draft tokens across different layers are generated independently, preserving the consistency of the auto-regressive decoding draft model with the target model and significantly enhancing the accuracy and factuality of predictions. At the same time, the decoupling of intra-layer tokens enables Jakiro to explore a broader range of candidate continuations or diversity, especially in non-greedy modes, without compromising the integrity of the output distribution.

Specifically, the original Eagle inference process can be summarized as follows: when a new token $t_i$ is passed through the embedding layer, a token embedding is obtained. This embedding is then concatenated with the hidden states (denoted as feature $\mathbf{f}_{i-1}$) from the previous step. The concatenated result is processed through the Reduction layer for dimensionality reduction, and the output hidden state $\mathbf{h}_i$ subsequently through the LLM decoder layer (including the attention layer $\operatorname{Attn}$ and MLP layer) to produce the hidden states $\mathbf{f}_i$ for the current step. The next token $t_{i+1}$ is sampled via the Head layer and used for the next step of inference. This process iterates until an end-of-sequence token is encountered or the maximum token length is reached.

\begin{figure}[ht]
      \centering
      \includegraphics[width=0.75\linewidth]{images/tree_mask.pdf}
      \caption{The building process of tree attention mask mechanism.}
      \label{fig:tree_mask}
      % \vspace{-1.5em}
      % \vskip -0.1in
\end{figure}

In contrast, our Jakiro inference process replaces the MLP layer in the LLM with an MoE layer $\operatorname{Expert}_j$ and modifies the draft tree construction. Assuming the number of candidate experts is $N$ and the final number of activated experts is $K$ (default value is 2), the MoE router and the LLM head layer are shared across different expert layers. At each step, two candidate logits distributions are outputed, which are then processed through the MoE Tree decoding. The logits are sorted based on the scores $s_{j,i}$ corresponding to the selected experts: expert with larger score is placed on the left side of the current layer, while those with smaller score is placed on the right. The formulation is as follows:
\begin{gather}
      \mathbf{u}_i = \operatorname{Attn}\left( \mathbf{h}_i \right) + \mathbf{h}_i, \\
      s_{j,i}  = \operatorname{Softmax}_j \left( {\mathbf{u}_i}^{T} \mathbf{e}_j \right), \\
      \mathbf{f}_{i}   = \sum_{j=1}^{N} \left( {g_{j,i} \operatorname{Expert}_{j}\left( \mathbf{u}_{i} \right)} \right) + \mathbf{u}_{i}, \\
      \mathbf{logits}_i^{\text{left}}, \mathbf{logits}_i^{\text{right}} = \operatorname{Head} \left( s_i^{\text{top1}} \mathbf{f}_i, s_i^{\text{top2}} \mathbf{f}_i \right),\\
      g_{j,i}  = \begin{cases} 
            s_{j,i}, s_{j,i} \in \operatorname{Topk} (\{ s_{j, i} | 1 \leq j \leq N \}, K), \\
                  0, \text{otherwise}, 
            \end{cases}
\end{gather}
where $\mathbf{e}_j$ is the centroid of the $j$-th expert, the $g_{j,i}$ is sparse, indicating that only $K$ out of $N$ gate values are nonzero. This sparsity property ensures computational efficiency in MoE layer, i.e., each token will be assigned to and computed in only $K$ experts. Also, in the above formulations, we omit the layer normalization operation for brevity. This entire process adheres to the top-$k$ construction principle of the Eagle tree, and the corresponding tree attention mask mechanism is illustrated in~\cref{fig:tree_mask}.

The specific construction process of MoE tree is illustrated in~\cref{fig:compare_eagel} (4). Its implementation mechanism can be combined with the predefined static tree of Eagle1 or the dynamically constructed tree of Eagle2. Experimental results demonstrate that both approaches lead to significant improvements in inference efficiency. 

{\bf Impact on Non-Greedy Modes}
While traditional speculative decoding methods struggle with diversity in non-greedy sampling, Jakiro excels by introducing mechanisms that maximize both diversity and prediction confidence. This makes it particularly effective in scenarios where alternative continuations need to be explored comprehensively.

\subsection{Efficient Integration of Contrastive Mechanism}
\label{sect:const}
Although Jakiro significantly enhances performance in non-greedy modes through its decoupled tree structure, it does not show a substantial advantage in strictly top-1 accuracy tasks, such as HumanEval and GSM8K, which are optimized for greedy decoding. However, by incorporating the Medusa parallel decoding mechanism, we reduce the number of forward passes during inference without compromising the final speedup, enabling Jakiro to achieve performance on par with Eagle2.

Moreover, LLMs are prone to hallucinations~\cite{tonmoy2024comprehensive,liu2024exploring}, where they generate content that does not align with the facts seen during pretraining. Due to the limited capabilities of the draft model itself, this issue is further exacerbated, making it difficult to further optimize under greedy decoding (i.e., strict top-1 verification). Although Eagle-style has improved the performance of the draft model by introducing feature distillation, due to the limitations on model parameters, it is hard to gain further improvements through additional training techniques. 

However, we explore that the currently popular contrastive decoding technique achieves significant out-of-the-box improvements compared to greedy decoding, without requiring additional training. Previous works~\cite{chuang2023dola,o2023contrastive} have shown that contrastive decoding improves generation quality by searching for strings that maximize the weighted likelihood difference between the strong and weak models. Unlike their use of two highly divergent models in capability, we only apply the contrastive mechanism to the output hidden states, {\bf but not to the logits}, of the two highest-scoring experts in the MoE layer. This approach further improves the generation quality of draft tokens in greedy mode, bringing them closer to the top-1 token of the target model and improving inference speed. The schematic diagram of the mechanism is shown in~\cref{fig:contrast_moe}. Below is the detailed derivation of its formulas:
\begin{gather}
      \mathbf{f}_i^{\text{moe}} = s_i^{\text{top1}} \mathbf{f}_i^{\text{top1}} + s_i^{\text{top2}} \mathbf{f}_i^{\text{top2}}, \\
      \mathbf{f}_i^{\text{const}} = \beta \mathbf{f}_i^{\text{top1}} - \alpha \mathbf{f}_i^{\text{top2}}, \\
      \mathbf{logits}_i^{\text{moe}}, \mathbf{logits}_i^{\text{const}} = \operatorname{Head}\left( \mathbf{f}_i^{\text{moe}} , \mathbf{f}_i^{\text{const}} \right),
\end{gather}
where the $\mathbf{logits}_i^{\text{moe}}$ indicates that the inference is based on the logits output from the previous {\small $[1:\gamma-1]$} steps for autoregression decoding, while $\mathbf{logits}_i^{\text{const}}$ is used for parallel decoding at step {\small $\gamma-1$}, resulting in one fewer draft process compared to the Eagle-style method. Additionally, $\beta$ and $\alpha$ are learnable parameters corresponding to the scores of the candidate experts being top1 and top2, respectively, and are used to adaptively adjust the difference between them for better contrastive learning. Compared to the non-greedy mode of Jakiro, no additional parameters are introduced, meaning {\bf the same weights} are used.

\begin{figure}[ht]
      \centering
      \includegraphics[width=0.8\linewidth]{images/contrast_moe.pdf}
      \caption{Efficient integration of contrastive mechanism.}
      \label{fig:contrast_moe}
      % \vspace{-1.5em}
      % \vskip -0.1in
\end{figure}

Additionally, it should be noted that our Jakiro is not limited to parallel decoding for the last two steps. When \(K > 2\), it theoretically supports parallel decoding for the last \(K\) steps. However, this is not the focus of the current study and will be explored in future work.


\subsection{Training of the Draft Models}
Similar to Eagle-style, in order to reduce training costs, we use a preprocessed fixed dataset for training the draft models. Other data augmentation hyperparameters are kept consistent with Eagle. Additionally, we adopt the Smooth L1 loss for predicting the next feature as a regression task and use cross-entropy to ensure the accuracy of tokens sequence. However, considering that the entire process of the draft model is autoregressive and computationally expensive, we use a parallel decoding mechanism during the penultimate step. Thus, the prediction of the next token is achieved through contrast between the two expert heads of the MoE, introducing an additional loss (similar to the Medusa implementation). The optimize objective is as follows:
\begin{gather}
      L_{reg}^{\text{moe}}     = \text{Smooth L1}(\mathbf{f}_{i+1}^p, \mathbf{f}_i^{\text{moe}}),\\
      L_{reg}^{\text{const}}   = \text{Smooth L1}(\mathbf{f}_{i+2}^p, \mathbf{f}_i^{\text{const}}),\\
      q_{i+2}, q_{i+3}         = \text{Softmax}(\mathbf{logits}_i^{\text{moe}}, \mathbf{logits}_i^{\text{const}}), \\
      L_{cls}^{\text{moe}}     = \text{Cross\_Entropy}(p_{i+2}, q_{i+2}),\\
      L_{cls}^{\text{const}}   = \text{Cross\_Entropy}(p_{i+3}, q_{i+3}), \\
      L=L_{reg}^{\text{moe}}+w_{cls}^{\text{moe}}L_{cls}^{\text{moe}} + L_{reg}^{\text{const}}+w_{cls}^{\text{const}}L_{cls}^{\text{const}}
\end{gather}
We use a combined loss function \( L \) to train the autoregressive draft model. Given that the classification loss is an order of magnitude larger than the regression loss numerically, and the importance of $\operatorname{const}$ is lower compared to $\operatorname{moe}$, we set $w_{cls}^{\text{moe}}$ and $w_{cls}^{\text{const}}$ to 0.1 and 0.05, respectively.

\section{Experiments}

{\bf Models and tasks.} Following the current mainstream works, we conduct experiments on Vicuna models (7B, 13B, 33B)~\cite{vicuna2023}, LLaMA2-chat models (7B, 13B, 70B)~\cite{touvron2023llama}, and LLaMA3-Instruct (8B, 70B)~\cite{llama3}, encompassing the common sizes of current mainstream LLMs. To evaluate the generality and robustness of our method, our Jakiro is evaluated across multiple tasks including multi-turn dialogue, code generation, mathematical reasoning, and instruction following, employing the MT-bench~\cite{zheng2023judging}, HumanEval~\cite{chen2021evaluating}, GSM8K~\cite{cobbe2021training}, Alpaca \cite{alpaca}, CNN/Daily Mail \cite{nallapati2016abstractive}, and Natural Questions \cite{kwiatkowski2019natural} datasets, respectively. Because Speculative decoding~\cite{leviathan2023fast} conduct experiments with a batch size of 1, a setting subsequently adopted by other works such as Medusa-style~\cite{cai2024medusa,ankner2024hydra} and Eagle-style~\cite{li2024eagle,li2024eagle2}. Similarly, the majority of our experiments also adopt this setting. 

{\bf Metrics.} Similar to prior speculative sampling approaches, our Jakiro method primarily focuses on reducing latency rather than optimizing throughput. To measure its acceleration effects, we utilize the following metrics:
\begin{itemize} 
      \vspace{-0.8em}
      \item {\bf Walltime speedup ratio:} Compared to traditional autoregressive decoding, the ratio of speedup achieved in actual tests of end-to-end. 
      \item {\bf Average acceptance length $\tau$:} The average number of tokens accepted from speculative decoding per forward pass of the target LLM.
      \vspace{-0.8em}
\end{itemize}
Similar to methods that use a strict speculative sampling mechanism, the acceleration achieved by Jakiro ensures that the output distribution of the target LLMs is maintained. Therefore, evaluating the quality of the generated results is both unnecessary and irrelevant, as the focus is on efficiency rather than output fidelity.

{\bf Training.} We keep the target LLMs fixed throughout the training process. Our proposed Jakiro model is trained on the ShareGPT\footnote{We can download ShareGPT from Huggingface at \href{https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered}{link}.} dataset using 68,000 dialogue iterations without needing extensive retraining or additional data beyond the pre-trained models, and with a learning rate set at 9e-5. The AdamW optimizer is employed with beta values $(\beta_1, \beta_2) = (0.9, 0.95)$, and gradient clipping is applied with a threshold of 0.5. The number of trainable parameters for Jakiro varies across model sizes: 0.35B for the 7B model, 0.56B for the 8B model, 0.88B for the 13B model, 1.42B for the 33B model, and 1.87B for the 70B model. For example, training the Jakiro draft model for the 70B model takes approximately 2-3 days on 4$\times$A100 40GB GPU.

{\bf Testing.} Our experiments are conducted on different devices (AMD $\text{Instinct}^\text{TM}$ MI250-64G, NVIDIA A40-45G, and NVIDIA A100-40G) and the limitation of hardware GPU memory. For models of size (7B, 8B, 13B), (33B), and (77B), we perform single-GPU, two GPUs, and four GPUs, respectively. 
\subsection{Effectiveness}
\begin{table*}[h]
    \centering
    \caption{Speedup ratios and average acceptance lengths $\tau$ of different methods on MI250. V represents Vicuna, L2 represents LLaMA2-Chat, and L3 represents LLaMA3-Instruct. SpS denotes standard speculative sampling, with its draft model being Vicuna-68M. Methods like Medusa relax acceptance conditions under non-greedy settings, which do not guarantee lossless acceleration. Therefore, we do not compare Jakiro with these methods. Where the symbol * indicates that our method uses MoE-based weighted decoupling to construct the draft tree, as shown in~\cref{fig:compare_eagel}(2). Without the symbol *, it refers to the construction of the draft tree using weighted non-decoupling combined with the contrastive mechanism, as shown in~\cref{fig:contrast_moe}. The results on other devices can be found in the appendix.}
    \small
    \resizebox{0.965\linewidth}{!}{
      \begin{tabular}{cccccccccccccccc}
      \toprule
            &       & \multicolumn{2}{c}{MT-bench} & \multicolumn{2}{c}{HumanEval} & \multicolumn{2}{c}{GSM8K} & \multicolumn{2}{c}{Alpaca} & \multicolumn{2}{c}{CNN/DM} & \multicolumn{2}{c}{Natural Ques.} & \multicolumn{2}{c}{Mean} \\
      \midrule
      Model & Method & Speedup & $\tau$     & Speedup & $\tau$     & Speedup & $\tau$     & Speedup & $\tau$     & Speedup & $\tau$     & Speedup & $\tau$     & Speedup & $\tau$ \\
      \midrule
      \multicolumn{16}{c}{Temperature=0} \\
      \midrule
      \multirow{6}[2]{*}{V 7B} 
            & SpS       & 1.82x & 2.36  & 1.99x & 2.61  & 1.71x & 2.26  & 1.65x & 2.21  & 1.81x & 2.44  & 1.60x & 2.16  & 1.76x & 2.34 \\
            & Medusa    & 1.91x & 2.52  & 2.02x & 2.67  & 1.89x & 2.59  & 1.79x & 2.48  & 1.42x & 2.02  & 1.51x & 2.09  & 1.76x & 2.40 \\
            & Hydra     & 2.69x & 3.60  & 2.98x & 3.79  & 2.73x & 3.66  & 2.66x & 3.58  & 2.01x & 2.70  & 2.25x & 2.86  & 2.55x & 3.37 \\
            & Eagle1     & 2.52x & 3.97  & 2.82x & 4.30  & 2.58x & 4.01  & 2.37x & 3.87  & 2.15x & 3.43  & 2.05x & 3.22  & 2.42x & 3.80 \\
            & Eagle2   & 2.75x & 4.94  & 3.12x & 5.35  & 2.81x & 4.94  & 2.63x & {\bf 4.85}  & 2.25x & 4.11  & 2.17x & 3.84  & 2.62x & 4.67 \\
            & Jakiro    & {\bf 2.99x} & {\bf 4.96} & {\bf 3.43x} & {\bf 5.36} & {\bf 3.11x} & {\bf 4.95} & {\bf 2.87x} & 4.82 & {\bf 2.50x} & {\bf 4.20} & {\bf 2.38x} & {\bf 3.84} & {\bf 2.88x} & {\bf 4.69} \\
      \midrule
      \multirow{2}[2]{*}{L2 7B} 
            & Eagle2   & 2.70x & {\bf 4.70}  & 3.12x & {\bf 5.38}  & 2.78x & {\bf 4.76}  & 2.67x & {\bf 4.65}  & 2.27x & {\bf 4.09}  & 2.41x & {\bf 4.16}  & 2.66x & {\bf 4.63} \\
            & Jakiro    & {\bf 2.89x} & 4.61 & {\bf 3.32x} & 5.26 & {\bf 2.97x} & 4.66 & {\bf 2.81x} & 4.47 & {\bf 2.42x} & 4.01 & {\bf 2.56x} & 4.07 & {\bf 2.83x} & 4.51 \\
      \midrule
      \multirow{2}[2]{*}{V 13B} 
            & Eagle2   & 3.02x & {\bf 4.84}  & 3.51x & {\bf 5.42}  & 3.11x & {\bf 4.82}  & 2.95x & {\bf 4.89}  & 2.60x & {\bf 4.28}  & 2.37x & 3.69  & 2.93x & {\bf 4.66} \\
            & Jakiro    & {\bf 3.18x} & 4.74 & {\bf 3.70x} & 5.36 & {\bf 3.30x} & 4.81 & {\bf 3.03x} & 4.68 & {\bf 2.69x} & 4.20 & {\bf 2.52x} & {\bf 3.70} & {\bf 3.07x} & 4.58 \\
      \midrule
      \multirow{2}[2]{*}{L2 13B}
            & Eagle2   & 3.06x & {\bf 4.74}  & 3.62x & {\bf 5.53}  & 3.19x & 4.88  & 2.96x & 4.62  & 2.66x & 4.24  & 2.68x & 4.12  & 3.03x & 4.69 \\
            & Jakiro    & {\bf 3.22x} & 4.72 & {\bf 3.76x} & 5.41 & {\bf 3.41x} & {\bf 4.96} & {\bf 3.18x} & {\bf 4.67} & {\bf 2.83x} & {\bf 4.32} & {\bf 2.87x} & {\bf 4.17} & {\bf 3.21x} & {\bf 4.71} \\
      \midrule
      \multicolumn{16}{c}{Temperature=1} \\
      \midrule
      \multirow{5}[2]{*}{ V 7B} 
            & SpS       & 1.50x & 1.87  & 1.55x & 1.95  & 1.53x & 1.82  & 1.56x & 1.85  & 1.63x & 1.91  & 1.33x & 1.72  & 1.52x & 1.85 \\
            & Eagle1     & 2.18x & 3.59  & 2.37x & 3.82  & 2.16x & 3.56  & 2.12x & 3.72  & 1.88x & 3.10  & 1.78x & 2.91  & 2.08x & 3.45 \\
            & Eagle2   & 2.39x & 4.26  & 2.64x & 4.67  & 2.37x & 4.49  & 2.28x & 4.28  & 2.01x & 3.77  & 1.95x & 3.54  & 2.27x & 4.17 \\
            & Jakiro    & 2.50x       & 4.22       & {\bf 2.92x} & {\bf 4.70} & 2.60x       & 4.44       & 2.48x       & 4.28       & 2.16x       & 3.77       & 2.08x       & 3.49       & 2.46x       & 4.15 \\
            & Jakiro*   & {\bf 3.18x} & {\bf 5.65} & 2.48x       &  4.52      & {\bf 2.91x} & {\bf 5.50} & {\bf 2.99x} & {\bf 5.67} & {\bf 2.70x} & {\bf 5.33} & {\bf 2.80x} & {\bf 5.05} & {\bf 2.84x} & {\bf 5.32} \\
      \midrule
      \multirow{3}[2]{*}{L2 7B} 
            & Eagle2   & 2.61x & 4.48  & 2.86x & 5.04  & 2.67x & 4.72  & 2.48x & 4.37  & 2.14x & 3.92  & 2.30x & 4.06  & 2.51x & 4.43 \\
            & Jakiro    & 2.68x       & 4.45       & {\bf 3.04x} & {\bf 4.94} & {\bf 2.82x} & {\bf 4.61} & 2.64x       & 4.32       & 2.27x       & 3.87       & 2.44x       & 3.98       & 2.65x       & 4.36 \\
            & Jakiro*   & {\bf 2.93x} & {\bf 5.16} & 2.62x       & 4.73       & 2.70x       & 4.59       & {\bf 3.04x} & {\bf 5.22} & {\bf 2.99x} & {\bf 5.37} & {\bf 2.73x} & {\bf 4.81} & {\bf 2.84x} & {\bf 4.98} \\
      \midrule
      \multirow{2}[2]{*}{V 13B} 
            & Eagle2   & 2.72x & 4.30  & 3.09x & 4.78  & 2.85x & 4.52  & 2.63x & 4.33  & 2.38x & 3.94  & 2.22x & 3.57  & 2.65x & 4.24 \\
            & Jakiro    & {\bf 2.84x} & {\bf 4.27} & {\bf 3.31x} & {\bf 4.95} & {\bf 3.00x} & {\bf 4.50} & {\bf 2.73x} & {\bf 4.31} & {\bf 2.45x} & {\bf 3.88} & {\bf 2.36x} & {\bf 3.54} & {\bf 2.78x} & {\bf 4.24} \\
      \midrule
      \multirow{2}[2]{*}{L2 13B} 
            & Eagle2   & 2.90x & 4.58  & 3.45x & 5.37  & 3.04x & 4.73  & 2.86x & 4.44  & 2.56x & 4.21  & 2.62x & 4.04  & 2.90x & {\bf 4.56} \\
            & Jakiro    & {\bf 3.06x} & {\bf 4.49}   & {\bf 3.60x} & {\bf 5.22} & {\bf 3.26x} & {\bf 4.76}    & {\bf 3.01x}    & {\bf 4.43} & {\bf 2.71x}   & {\bf 4.15}    & {\bf 2.83x} & {\bf 4.13}     & {\bf 3.08x} & 4.53 \\
      \midrule
      \multirow{2}[2]{*}{V 33B} 
            & Eagle2   & 3.04x & 4.22  & 3.52x & 4.78  & 3.41x & 4.55  & 2.87x & 3.99  & 2.56x & 3.71  & 2.40x & 3.27  & 2.97x & 4.09 \\
            & Jakiro    & {\bf 3.16x} & {\bf 4.31} & {\bf 3.64x} & {\bf 4.84} & {\bf 3.52x} & {\bf 4.63}   & {\bf 2.98x} & {\bf 4.02}  & {\bf 2.68x} & {\bf 3.85}  & {\bf 2.52x} & {\bf 3.38} & {\bf 3.08x} & {\bf 4.17} \\
      \midrule
      \multirow{3}[2]{*}{L2 70B} 
            & Eagle2   & 2.96x & 4.48  & 3.49x & 5.18  & 3.09x & 4.59  & 2.97x & 4.49  & 2.34x & 3.64  & 2.54x & 3.84  & 2.90x & 4.37 \\
            & Jakiro    & 3.08x & 4.56 & {\bf 3.62x} & {\bf 5.25} & 3.18x & 4.61   & 3.05x & 4.47  & 2.53x & 3.87  & 2.65x & 3.90 & 3.02x & 4.44 \\
            & Jakiro*   & {\bf 3.68x} & {\bf 5.16} & 3.18x & 4.37 & {\bf 3.43x} & {\bf 4.71}   & {\bf 3.60x} & {\bf 4.90}  & {\bf 3.55x} & {\bf 5.18}  & {\bf 3.56x} & {\bf 4.86} & {\bf 3.50x} & {\bf 4.86} \\
      \midrule
      \multirow{2}[2]{*}{L3 8B} 
            & Eagle2   & 2.31x & 3.97  & 2.96x & 4.83  & 2.65x & 4.36  & 2.52x & 4.50  & 1.99x & 3.52  & 2.05x & 3.36  & 2.41x & 4.09 \\
            & Jakiro    & {\bf 2.58x} & {\bf 4.66} & {\bf 3.02x} & {\bf 4.91} & {\bf 2.70x} & {\bf 4.44}   & {\bf 2.64x} & {\bf 4.53}  & {\bf 2.06x} & {\bf 3.59}  & {\bf 2.32x} & {\bf 3.95} & {\bf 2.55x} & {\bf 4.35} \\
      \midrule
      \multirow{2}[2]{*}{L3 70B} 
            & Eagle2   & 3.11x & 4.98  & 2.83x & 4.35  & 2.73x & 4.67  & 2.10x & 3.64  & 2.34x & 3.52  & 2.30x & 3.42  & 2.57x & 4.10 \\
            & Jakiro    & {\bf 3.26x} & {\bf 5.04} & {\bf 2.89x} & {\bf 4.42} & {\bf 2.91x} & {\bf 4.77}   & {\bf 2.46x} & {\bf 3.78}  & {\bf 2.35x} & {\bf 3.75}  & {\bf 2.32x} & {\bf 3.56} & {\bf 2.70x} & {\bf 4.22} \\
      \bottomrule
      \end{tabular}%
      }
    \label{tab:Mi-250_all_models}%
    \vskip -0.2in
\end{table*}%

As shown in~\cref{tab:Mi-250_all_models} and~\cref{tab:big_models}, our proposed Jakiro consistently outperforms existing methods in terms of speedup ratios across various datasets and models on MI250. The speedup ratios achieved by Jakiro, such as 2.99x on the MT-bench and 3.43x on the HumanEval task for the Vicuna 7B model under greedy mode, significantly surpass those of other methods, including Eagle2, which achieved a maximum speedup of 2.88x on average. The results across different tasks further highlight the effectiveness of Jakiro. For example, on the GSM8K task, Jakiro achieves a 3.11x speedup, maintaining a high average acceptance length of 4.95 tokens. This demonstrates that Jakiro not only accelerates inference but also retains high-quality output, maintaining the target LLM's performance without compromising accuracy.

\begin{table}[h]
      \centering
      \caption{Speedup ratios and average acceptance lengths $\tau$ with other models as the original LLMs, with the temperature set to 0.}
      \resizebox{\linewidth}{!}{
      \begin{tabular}{cccccccc}
      \toprule
            &           & \multicolumn{2}{c}{MT-bench} & \multicolumn{2}{c}{HumanEval} & \multicolumn{2}{c}{GSM8K} \\
      \midrule
      Model & Method    & Speedup   & $\tau$  & Speedup & $\tau$ & Speedup & $\tau$ \\ 
      \midrule
      \multirow{3}[2]{*}{LLaMA3-Instruct 8B} 
            & Eagle1     & 1.84x       & 3.07        & 2.27x       &  3.72       & 2.25x   &  3.70   \\
            & Eagle2   & 2.63x       & {\bf 4.32}  & 3.08x       & {\bf 5.06}  & 2.79x   & {\bf 4.46}    \\
            & Jakiro    & {\bf 2.74x} &  4.23       & {\bf 3.25x}  & 5.01        & {\bf 2.92x} &  4.38   \\
      \midrule
      \multirow{3}[2]{*}{Vicuna 33B} 
            & Eagle1     & 2.87x           &  3.69          & 3.41x           & 4.28            & 3.16x       & 3.93  \\
            & Eagle2   & 3.21x           &  {\bf 4.43}    & 3.89x           & {\bf 5.20}      & 3.52x       & 4.66  \\
            & Jakiro    & {\bf 3.34x}     &  4.40          & {\bf 3.96x}     & 5.16            & {\bf 3.67x} & {\bf 4.67}  \\
      \midrule
      \multirow{3}[2]{*}{LLaMA2-Chat 70B} 
            & Eagle1     & 2.83x           & 3.84            & 3.33x           & 4.44            & 2.95x        & 3.92  \\
            & Eagle2   & 2.98x           & {\bf 4.51}      & 3.54x           & {\bf 5.25}      & 3.10x        & {\bf 4.63} \\
            & Jakiro    & {\bf 3.01x}     & 4.46            & {\bf 3.58x}     & 5.19            & {\bf 3.16x}  & 4.58  \\
      \midrule
      \multirow{3}[2]{*}{LLaMA3-Instruct 70B} 
            & Eagle1     & 2.43x         & 3.33       & 3.01x         &  4.15       & 2.53x        & 3.87 \\
            & Eagle2     & 2.58x         & {\bf 4.14} & 3.10x         &  {\bf 5.09} & 2.82x        & {\bf 4.34} \\
            & Jakiro    & {\bf 2.65x}   & 4.06       & {\bf 3.28x}   & 5.03        & {\bf 2.97x}  & 4.29 \\
      \bottomrule
      \end{tabular}%
      }
      \label{tab:big_models}%
      \vskip -0.2in
\end{table}%

Since our Jakiro involves one less drafting step than Eagle2, there is no significant improvement in the average acceptance length of tokens. However, considering that the ultimate goal of speculative decoding is to improve speedup, our method is still effective. For instance, in the greedy mode for LLaMA2-Chat 7B, although the average acceptance length of our method is slightly lower than that of Eagle2 (4.51 vs. 4.63), the final speedup of our method is notably improved (2.83x vs. 2.66x). Furthermore, we observe that Jakiro excels across both code generation and natural language tasks. This is particularly notable because code generation tasks often benefit from greedy sampling, where the inherent structure of code makes it easier to predict subsequent tokens efficiently. Similarly, on Natural Questions and summarization tasks, Jakiro maintains superior performance.

Additionally, in non-greedy mode, the comparison methods with speculative sampling also reveal that Jakiro achieves significantly higher speedup ratios while also maintaining a higher average acceptance length. Such as Vicuna 7B, compared to Eagle2, our method achieves a 15.4\% improvement in the final average speedup across all benchmark test sets (2.84 vs. 2.46). This suggests that Jakiro benefits from a more efficient drafting process that allows for longer and more stable sequences of tokens to be accepted, reducing the need for frequent re-sampling and minimizing the risk of errors during the inference process.



\subsection{Ablation Study}
{\bf N-k Setting of MoE.} The number of candidate experts $N$ and activated experts $K$ involved in the computation for each token in a typical MoE architecture is 8 and 2, respectively. If speculative decoding uses the original chain-based construction of draft tokens, the number of experts activated each time is fixed and equal to 2. However, current mainstream speculative decoding methods use a tree structure to construct draft tokens. Although the number of candidate experts activated for each token is fixed, the process of constructing the tree results in the top-k candidate tokens being selected at each layer, which leads to a number of experts activated K$\geq 2$ during each inference. Additionally, the more candidate experts there are, the better the model's predictive ability, but with an increased computational cost. Therefore, it is necessary to choose an appropriate N-K setting to achieve optimal inference speed. The results under different N-K settings are shown in~\cref{tab:aba_NK}.

\begin{table}[h]
      \centering
      \caption{Ablation experiment results with temperature set to 0 on Vicuna 7B under the MI250. ``N" indicates the candidate experts, and ``K" indicates the experts activated each time.}
      \resizebox{1.0\linewidth}{!}{
      \begin{tabular}{cccccccccc}
      \toprule
          &           & \multicolumn{2}{c}{MT-bench} & \multicolumn{2}{c}{HumanEval} & \multicolumn{2}{c}{GSM8K} & \multicolumn{2}{c}{Alpaca} \\
      \midrule
      N   &     K     & Speedup     & $\tau$    & Speedup   & $\tau$    & Speedup   & $\tau$ & Speedup   & $\tau$ \\
      \midrule
      5   &     2     & 2.67x       & {\bf 5.13}  & 3.03x       & {\bf 5.59}  & 2.71x        & {\bf 5.14} & 2.56x         & {\bf 5.10}  \\
      4   &     2     & 2.69x       & 5.09        & 3.07x       & 5.54        & 2.75x        & 5.12       & 2.57x         & 5.04  \\
      3   &     2     & 2.75x       & 5.09        & 3.13x       & 5.54        & 2.79x        & 5.04       & 2.61x         & 5.04  \\
      2   &     2     & {\bf 2.92x} & 4.97        & {\bf 3.36x} & 5.45        & {\bf 3.01x}  & 4.97       & {\bf 2.83x}   & 4.98   \\
      \bottomrule
      \end{tabular}
      }
      \label{tab:aba_NK}
      \vskip -0.2in
\end{table}%

From the above results, it can be seen that as the number of candidate experts increases, both the average acceptance length and the computational load increase accordingly. However, we believe that combining hardware and algorithmic optimizations can mitigate these costs, further unlocking the potential of MoE, which remains an area for future exploration.

{\bf Combine with Parallel Decoding and Contrastive Mechanism.} The contrastive mechanism (perhaps similar to a form of contrastive learning) and its implementation combined with parallel decoding is discussed in~\cref{sect:const}. This approach aims to reduce one draft inference step compared to conventional autoregressive decoding LLMs (e.g., Eagle2) without significant loss in prediction performance, thereby achieving optimal speedup. However, the essence of these two mechanisms is different. The contrastive mechanism focuses on better predicting the next token in the sequence, while parallel decoding aims to output multiple future tokens in one step. Therefore, it is necessary to conduct further experiments to evaluate their respective impacts on final performance, with the results shown in~\cref{tab:aba_PD}.

As can be seen from the above results, using parallel decoding or the contrastive mechanism alone does lead to some improvement compared to using MoE alone, which demonstrates that the two proposed mechanisms can effectively complement Jakiro. However, combining both mechanisms further unleashes the potential of Jakiro, maintaining a comparable acceptance rate (similar to average accpet length) while further improving its speedup.

\begin{table}[h]
      \vskip -0.2in
      \centering
      \caption{Ablation experiment results with temperature set to 0 on Vicuna 7B under the MI250. ``PD” refers to whether the penultimate inference in the draft process uses the parallel decoding mechanism. ``Const” indicates whether contrastive mechanism on features is applied in the penultimate inference of the draft process.}
      \resizebox{1.0\linewidth}{!}{
      \begin{tabular}{ccccccccc}
      \toprule
                  & \multicolumn{2}{c}{MT-bench}   & \multicolumn{2}{c}{HumanEval}  & \multicolumn{2}{c}{GSM8K} & \multicolumn{2}{c}{Alpaca} \\
      \midrule
      Method      & Speedup     & $\tau$           & Speedup     & $\tau$           & Speedup     & $\tau$    & Speedup     & $\tau$ \\
      \midrule
      w/o both    & 2.92x           & 4.97        & 3.36x         & 5.45         & 3.01x        & 4.97       & 2.83x         & 4.98   \\
      w/o PD~~~   & 2.98x           & {\bf 5.04}  & 3.41x         & {\bf 5.50}   & 3.10x        & {\bf 5.11} & 2.83x         & {\bf 4.92}     \\
      w/o Const   & 2.98x           & 4.86        & 3.42x         & 5.33         & 3.05x        & 4.86       & 2.86x         & 4.82      \\
      Jakiro      & {\bf 2.99x}     & 4.96        & {\bf 3.43x}   & 5.36         & {\bf 3.11x}  & 4.95       & {\bf 2.87x}   & 4.82      \\
      \bottomrule
      \end{tabular}
      }
      \label{tab:aba_PD}
      \vskip -0.2in
\end{table}%

\section{Related Work}
{\bf Speculative decoding:}
Speculative decoding (SD)has emerged as a powerful technique to accelerate LLMs inference by reducing memory bandwidth bottlenecks. Early speculative decoding methods, such as those by~\cite{stern2018blockwise} and~\cite{sun2021instantaneous}, focused on greedy decoding strategies, while~\cite{leviathan2023fast} and~\cite{chen2023accelerating} expanded speculative sampling to non-greedy decoding. Recent works in SD have enhanced draft model efficiency, with methods like SpecInfer~\cite{miao2023specinfer} employing tree attention to verify multiple draft tokens in parallel, and Medusa~\cite{cai2024medusa} using extra MLP heads to generate token drafts. Although these methods have achieved notable acceleration, they face limitations in token diversity and decoupling between draft and target models. Eagle~\cite{li2024eagle} introduces a more dynamic approach by decoupling the draft tokens across different time steps. However, Eagle still maintains coupling between the Top-k tokens at the same layer in the draft tree, which restricts the diversity and specialization of tokens. Our approach builds upon these limitations by introducing a dynamic decoupling mechanism with Mixture of Experts (MoE) heads, which enables draft tokens to consider inherent differences between them, leading to more diversity and confident predictions.

{\bf Mixture of Experts:}
MoE has been explored extensively for improving the specialization of models. MoE techniques were originally proposed by~\cite{ori_moe1, ori_moe2} and later adapted to language models, such as in GShard~\cite{gshard} and Switch Transformer~\cite{switch}, which scale MoE to large models with top-k routing strategies. More recently, the integration of MoE in Transformer-based architectures has garnered significant interest, with methods like StableMoE~\cite{stablemoe} exploring fixed routing strategies for more stable training. MoE heads have also been used in multi-modal settings~\cite{glam,openmoe}, where they enable specialization across different modalities. Furthermore, in the context of speculative decoding, our method combines MoE's dual-branch heads with contrastive decoding techniques, a strategy inspired by recent works~\cite{moe,stablemoe} to improve the usefulness of draft token predictions, especially in greedy modes. By integrating these strategies, we can achieve more reliable predictions with faster inference, as demonstrated through our experiments.

{\bf Parallel decoding:}
Parallel decoding is known for its efficiency in machine translation~\cite{parallel_dec_1} and code generation \cite{meta_multitoken}, has also been integrated into SD frameworks to further enhance efficiency. Although the use of parallel decoding in speculative frameworks has been under-explored, works like \cite{monea2023pass} and \cite{yi-etal-2024-generation} have pioneered its application. These methods, however, still face challenges in achieving a perfect alignment between draft distributions and target models, which can limit their effectiveness in lossless acceleration. Our approach addresses these challenges by improving the decoupling mechanism within the MoE heads, ensuring better alignment and more diverse token predictions in both greedy and non-greedy modes.

% Finally, various works have investigated model compression, quantization, and pruning as techniques for accelerating LLM inference. These methods aim to reduce computational cost but often sacrifice some degree of model performance. In contrast, speculative decoding frameworks, such as the one proposed in this paper, achieve lossless acceleration by utilizing efficient draft models, enabling faster generation without compromising output quality.

\section{Conclusion}
In summary, while speculative decoding has proven effective for accelerating LLM inference, challenges remain in improving token diversity and maintaining distribution alignment between draft and target models. Our work makes significant strides in addressing these challenges by introducing a dynamic decoupling mechanism with MoE heads, which improves prediction confidence, diversity, and factuality. Additionally, by combining contrastive mechanism with MoE's dual-branch heads, we further enhance the utility of our predictions, demonstrating the potential of this approach in both greedy and non-greedy generation tasks. Our proposed Jakiro demonstrates superior speedup ratios, high acceptance lengths, and overall efficiency across a variety of tasks and model sizes, making it a compelling solution for improving inference speed without sacrificing model output quality.


\section*{Impact Statement}
This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here.


% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
\nocite{langley00}

\bibliography{example_paper}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
\section{Supplementary experiments}

\subsection{Average Acceptance Length.}

\begin{figure*}[ht]
      \centering
      \includegraphics[width=0.9\linewidth]{images/A100-40G_MTbench_L_Comparison_Temperature_1.pdf}
      \caption{Average acceptance length of Vicuna, LLaMA2-chat, and LLaMA3-instruct models inference on the MT-bench for non-greedy (Temperature=1) settings. The above reproduction results are based on the open-source code from the original paper and are averaged over four inference runs on an A100-40G GPU. In this paper, we only compare with speculative sampling based methods that do not need to finetune the backbone models, ensuring the output text distribution remains constant.}
      \label{fig:MTbench_L_Comparison_T=1}
  %     \vspace{-1.0em}
      % \vskip -0.2in
\end{figure*}

From~\cref{fig:MTbench_L_Comparison_T=1}, it can be observed that our Jakiro achieves the highest acceptance rate (approximately average acceptance length) across different models, further demonstrating the robustness of our method. Under the advantage of the MoE mechanism, it fully utilizes the diversity prediction of draft tokens in the non-greedy mode, resulting in an increased number of accepted tokens and thus improving the average length accepted by the target model during the validation phase.


\subsection{Results of Other Devices.}
% \begin{figure*}[ht]
%       \centering
%       \includegraphics[width=0.95\linewidth]{images/Vicuna_7B_Mean_Length_Sorted.pdf}
%       \caption{Average acceptance length of Vicuna 7B inference on all tasks under different devices. Where the suffix ``T0" represents greedy mode sampling, while suffix ``T1" represents non-greedy sampling. The above reproduction results are based on the open-source code from the original paper and are averaged over four inference runs. In this paper, we only compare with speculative sampling based methods that do not need to finetune the backbone models, ensuring the output text distribution remains constant.}
%       \label{fig:all-tasks_L_Comparison}
%   %     \vspace{-1.0em}
%       % \vskip -0.2in
% \end{figure*}

\begin{figure*}[ht]
      \centering
      \includegraphics[width=0.9\linewidth]{images/Vicuna_7B_Mean_Speedup_Sorted.pdf}
      \caption{Speedup ratios of Vicuna 7B inference on all tasks under different devices.  Where the suffix ``T0" represents greedy mode sampling, while suffix ``T1" represents non-greedy sampling. The above reproduction results are based on the open-source code from the original paper and are averaged over four inference runs. In this paper, we only compare with speculative sampling based methods that do not need to finetune the backbone models, ensuring the output text distribution remains constant.}
      \label{fig:all-tasks_S_Comparison}
  %     \vspace{-1.0em}
      % \vskip -0.2in
\end{figure*}

As shown~\cref{fig:all-tasks_S_Comparison}, our proposed Jakiro achieves consistent improvements across different devices, indicating that our method is robust and has the capability of performance transferability across devices.

{\bf Results in A40-45G:}
\begin{table*}[h]
      \centering
      \caption{Speedup ratios and average acceptance lengths $\tau$ of different methods. V represents Vicuna, L2 represents LLaMA2-Chat. SpS denotes standard speculative sampling, with its draft model being Vicuna-68M. Methods like Medusa relax acceptance conditions under non-greedy settings, which do not guarantee lossless acceleration. Therefore, we do not compare Jakiro with these methods. Where the symbol * indicates that our method uses MoE-based weighted decoupling to construct the draft tree, as shown in~\cref{fig:compare_eagel}(2). Without the symbol *, it refers to the construction of the draft tree using weighted non-decoupling combined with the contrastive mechanism, as shown in~\cref{fig:contrast_moe}.}
      \resizebox{1.0\linewidth}{!}{
        \begin{tabular}{cccccccccccccccc}
        \toprule
              &       & \multicolumn{2}{c}{MT-bench} & \multicolumn{2}{c}{HumanEval} & \multicolumn{2}{c}{GSM8K} & \multicolumn{2}{c}{Alpaca} & \multicolumn{2}{c}{CNN/DM} & \multicolumn{2}{c}{Natural Ques.} & \multicolumn{2}{c}{Mean} \\
        \midrule
        Model & Method & Speedup & $\tau$     & Speedup & $\tau$     & Speedup & $\tau$     & Speedup & $\tau$     & Speedup & $\tau$     & Speedup & $\tau$     & Speedup & $\tau$ \\
        \midrule
        \multicolumn{16}{c}{Temperature=0} \\
        \midrule
        \multirow{3}[2]{*}{V 7B} 
            & Eagle1     & 2.69x & 3.98  & 2.98x & 4.29  & 2.74x & 4.00  & 2.56x & 3.87  & 2.21x & 3.42  & 2.17x & 3.21  & 2.56x & 3.80 \\
            & Eagle2   & 2.92x       & 4.98       & 3.24x & 5.34  & 2.94x      & 4.95        & 2.76x      & {\bf 4.87}  & 2.28x & 4.12        & 2.26x      & 3.81  & 2.73x & 4.68 \\
            & Jakiro    & {\bf 3.02x} & {\bf 4.98} & {\bf 3.40x}   & {\bf 5.37} & {\bf 3.08x} & {\bf 4.97} & {\bf 2.86x} & 4.82  & {\bf 2.38x} & {\bf 4.17} & {\bf 2.36x} & {\bf 3.83} & {\bf 2.85x} & {\bf 4.69} \\
        \midrule
        \multirow{3}[2]{*}{L2 7B} 
            & Eagle1     & 2.62x & 3.82  & 2.94x & 4.30  & 2.66x & 3.90  & 2.53x & 3.70  & 2.24x & 3.41  & 2.34x & 3.44  & 2.55x & 3.76 \\
            & Eagle2   & 2.81x       & {\bf 4.71}  & 3.23x & {\bf 5.39}  & 2.86x & {\bf 4.76}  & 2.79x & {\bf 4.66}  & 2.31x & {\bf 4.12}  & 2.51x & {\bf 4.19}  & 2.75x & {\bf 4.64} \\
            & Jakiro    & {\bf 2.87x} & 4.61 & {\bf 3.29x} & 5.25 & {\bf 2.93x} & 4.64 & {\bf 2.81x} & 4.47 & {\bf 2.32x} & 4.01 & {\bf 2.56x} & 4.07 & {\bf 2.80x} & 4.51 \\
        \midrule
        \multirow{3}[2]{*}{V 13B} 
            & Eagle1     & 2.90x & 4.00  & 3.28x & 4.39  & 2.94x & 3.97  & 2.74x & 3.95  & 2.45x & 3.52  & 2.28x & 3.11  & 2.77x & 3.82 \\
            & Eagle2   & 2.97x & {\bf 4.83}  & 3.45x & {\bf 5.41}  & 3.03x & 4.79  & 2.89x & {\bf 4.89}  & 2.46x & {\bf 4.22}  & 2.33x & {\bf 3.74}  & 2.86x & {\bf 4.65} \\
            & Jakiro    & {\bf 3.11x} & 4.76 & {\bf 3.61x} & 5.36 & {\bf 3.22x} & {\bf 4.81} & {\bf 2.96x} & 4.69 & {\bf 2.49x} & 4.13 & {\bf 2.45x} & 3.67 & {\bf 2.97x} & 4.57 \\
        \midrule
        \multirow{3}[2]{*}{L2 13B}
            & Eagle1     & 2.89x & 3.93  & 3.39x & 4.52  & 2.99x & 4.03  & 2.81x & 3.83  & 2.54x & 3.59  & 2.58x & 3.47  & 2.87x & 3.89 \\
            & Eagle2   & 2.96x & {\bf 4.74}  & 3.52x & {\bf 5.52}  & 3.09x & 4.90  & 2.88x & 4.60  & 2.53x & 4.25  & 2.60x & 4.11  & 2.93x & 4.69 \\
            & Jakiro    & {\bf 3.11x} & 4.73 & {\bf 3.62x} & 5.42 & {\bf 3.30x} & {\bf 4.94} & {\bf 3.07x} & {\bf 4.65} & {\bf 2.62x} & {\bf 4.30} & {\bf 2.79x} & {\bf 4.16} & {\bf 3.08x} & {\bf 4.70} \\
        \midrule
        \multicolumn{16}{c}{Temperature=1} \\
        \midrule
        \multirow{4}[2]{*}{ V 7B} 
            & Eagle1     & 2.23x & 3.54  & 2.41x & 3.82  & 2.23x & 3.66  & 2.23x & 3.63  & 1.91x & 3.13  & 1.86x & 2.95  & 2.15x & 3.45 \\
            & Eagle2   & 2.46x & 4.29  & 2.72x & 4.63  & 2.47x & 4.41  & 2.41x & 4.47  & 2.09x & 3.82  & 2.03x & 3.50  & 2.36x & 4.19 \\
            & Jakiro    & 2.54x       & 4.15       & {\bf 2.95x} & {\bf 4.72} & 2.63x       & 4.48       & 2.54x       & 4.47       & 2.19x       & 3.89       & 2.12x       & 3.51       & 2.50x       & 4.20      \\
            & Jakiro*   & {\bf 3.24x} & {\bf 5.64} & 2.67x       & 4.12       & {\bf 2.84x} & {\bf 5.76} & {\bf 2.97x} & {\bf 5.67} & {\bf 2.84x} & {\bf 5.59} & {\bf 2.85x} & {\bf 5.04} & {\bf 2.90x} & {\bf 5.47} \\
        \midrule
        \multirow{4}[2]{*}{L2 7B} 
            & Eagle1     & 2.24x & 3.59  & 2.55x & 4.08  & 2.39x & 3.80  & 2.21x & 3.51  & 1.96x & 3.18  & 2.04x & 3.24  & 2.23x & 3.56 \\
            & Eagle2   & 2.63x & 4.53  & 2.97x & 5.04  & 2.79x & 4.76  & 2.63x & 4.53  & 2.18x & 3.93  & 2.34x & 3.99  & 2.59x & 4.46 \\
            & Jakiro    & 2.66x       & 4.45       & {\bf 2.99x} & {\bf 4.93} & 2.80x       & 4.59       & 2.66x       & 4.40       & 2.19x       & 3.93       & 2.37x       & 3.89       & 2.61x       & 4.37 \\
            & Jakiro*   & {\bf 2.97x} & {\bf 5.13} & 2.70x       & 4.68       & {\bf 2.80x} & {\bf 4.69} & {\bf 3.06x} & {\bf 5.19} & {\bf 2.97x} & {\bf 5.46} & {\bf 2.80x} & {\bf 4.85} & {\bf 2.88x} & {\bf 5.00} \\
        \midrule
        \multirow{4}[2]{*}{V 13B} 
            & Eagle1     & 2.51x & 3.66  & 2.84x & 4.02  & 2.58x & 3.74  & 2.49x & 3.77  & 2.22x & 3.32  & 2.11x & 3.03  & 2.46x & 3.59 \\
            & Eagle2   & 2.68x & 4.38  & 3.03x & 4.86  & 2.69x & 4.46  & 2.53x & 4.48  & 2.24x & 3.91  & 2.15x & 3.53  & 2.55x & 4.27 \\
            & Jakiro    & 2.78x & 4.23 & {\bf 3.28x} & {\bf 4.89} & 2.90x & 4.40 & {\bf 2.70x} & {\bf 4.46} & 2.35x & 3.90 & 2.31x & 3.55 & {\bf 2.72x} & 4.24 \\
            & Jakiro*   & {\bf 3.04x} & {\bf 4.70} & 3.00x & 4.73 & {\bf 2.90x} & {\bf 5.01} & 2.61x & 4.77 & {\bf 2.04x} & {\bf 3.94} & {\bf 2.44x} & {\bf 3.88} & 2.67x & {\bf 4.50} \\
        \midrule
        \multirow{4}[2]{*}{L2 13B} 
            & Eagle1     & 2.59x & 3.69  & 3.03x & 4.23  & 2.69x & 3.81  & 2.54x & 3.64  & 2.34x & 3.45  & 2.37x & 3.34  & 2.59x & 3.69 \\
            & Eagle2   & 2.82x & 4.61  & 3.37x & 5.40  & 2.97x & 4.79  & 2.75x & 4.52  & 2.44x & 4.20  & 2.54x & 4.08  & 2.81x & 4.60 \\
            & Jakiro    & 2.97x & 4.56        & {\bf 3.53x} & {\bf 5.29} & 3.19x       & 4.79       & 2.95x       & {\bf 4.52} & 2.55x       &  4.22      & 2.76x       & {\bf 4.15} & {\bf 2.99x} & {\bf 4.59} \\
            & Jakiro*   & {\bf 3.14x}       & {\bf 4.72}  & 2.75x       &   4.03     & {\bf 3.33x} & {\bf 4.90} & {\bf 2.79x} &  4.11      & {\bf 2.91x} & {\bf 4.50} & {\bf 2.77x} & 4.10       & 2.95x       & 4.39 \\
        \bottomrule
        \end{tabular}%
        }
      \label{tab:A40_all_models}%
  \end{table*}%

{\bf Results in A100-40G:}
\begin{table*}[h]
      \centering
      \caption{Speedup ratios and average acceptance lengths $\tau$ of different methods. V represents Vicuna, L2 represents LLaMA2-Chat. SpS denotes standard speculative sampling, with its draft model being Vicuna-68M. Methods like Medusa relax acceptance conditions under non-greedy settings, which do not guarantee lossless acceleration. Therefore, we do not compare Jakiro with these methods. Where the symbol * indicates that our method uses MoE-based weighted decoupling to construct the draft tree, as shown in~\cref{fig:compare_eagel}(2). Without the symbol *, it refers to the construction of the draft tree using weighted non-decoupling combined with the contrastive mechanism, as shown in~\cref{fig:contrast_moe}.}
      \resizebox{\linewidth}{!}{
        \begin{tabular}{cccccccccccccccc}
        \toprule
              &       & \multicolumn{2}{c}{MT-bench} & \multicolumn{2}{c}{HumanEval} & \multicolumn{2}{c}{GSM8K} & \multicolumn{2}{c}{Alpaca} & \multicolumn{2}{c}{CNN/DM} & \multicolumn{2}{c}{Natural Ques.} & \multicolumn{2}{c}{Mean} \\
        \midrule
        Model & Method & Speedup & $\tau$     & Speedup & $\tau$     & Speedup & $\tau$     & Speedup & $\tau$     & Speedup & $\tau$     & Speedup & $\tau$     & Speedup & $\tau$ \\
        \midrule
        \multicolumn{16}{c}{Temperature=0} \\
        \midrule
        \multirow{3}[2]{*}{V 7B} 
              & Eagle1     & 2.84x & 3.98  & 3.23x & 4.30  & 3.18x & 3.99  & 2.75x & 3.87  & 2.48x & 3.42  & 2.42x & 3.21  & 2.82x & 3.80 \\
              & Eagle2   & 3.29x & 4.98  & 3.71x & 5.35  & 3.15x & 4.94  & 2.97x & 4.86  & 2.62x & 4.11  & 2.46x & 3.82  & 3.03x & 4.68 \\
              & Jakiro    & {\bf 3.34x} & {\bf 5.03} & {\bf 3.81x} & {\bf 5.48} & {\bf 3.22x} & {\bf 4.98} & {\bf 3.06x} & {\bf 4.97} & {\bf 2.68x} & {\bf 4.13} & {\bf 2.50x} & {\bf 3.86} & {\bf 3.10x} & {\bf 4.74} \\
        \midrule
        \multirow{3}[2]{*}{L2 7B} 
              & Eagle1     & 2.94x & 3.82  & 3.24x & 4.30  & 2.98x & 3.91  & 2.81x & 3.68  & 2.58x & 3.41  & 2.67x & 3.43  & 2.87x & 3.76 \\
              & Eagle2   & {\bf 3.20x} & {\bf 4.70}  & {\bf 3.63x} & {\bf 5.38}  & {\bf 3.21x} & {\bf 4.77}  & {\bf 3.12x} & {\bf 4.66}  & {\bf 2.68x} & {\bf 4.10}  & {\bf 2.77x} & {\bf 4.16}  & {\bf 3.10x} & {\bf 4.63} \\
              & Jakiro    & 3.15x       & 4.61        & 3.60x       & 5.24 & 3.20x & 4.65 & 3.06x & 4.48 & 2.62x & 4.01 & 2.70x & 4.07 & 3.05x & 4.51 \\
        \midrule
        \multirow{3}[2]{*}{V 13B} 
              & Eagle1     & 2.94x & 4.00  & 3.34x & 4.39  & 2.93x & 3.98  & 2.96x & 3.95  & 2.55x & 3.52  & 2.35x & 3.10  & 2.85x & 3.82 \\
              & Eagle2   & 2.96x       & {\bf 4.83}  & 3.47x       & 5.42  & 3.07x & 4.79  & 2.94x & {\bf 4.90}  & 2.45x & {\bf 4.21}  & 2.34x & {\bf 3.71}  & 2.87x & {\bf 4.64} \\
              & Jakiro    & {\bf 3.05x} & 4.74        & {\bf 3.55x} & {\bf 5.32} & {\bf 3.23x} & {\bf 4.81} & {\bf 2.95x} & 4.70 & {\bf 2.56x} & 4.17 & {\bf 2.43x} & 3.67 & {\bf 2.96x} & 4.57 \\
        \midrule
        \multirow{3}[2]{*}{L2 13B}
              & Eagle1     & 2.97x & 3.93  & 3.39x & 4.52  & 3.20x & 4.02  & 3.00x & 3.83  & 2.65x & 3.58  & 2.61x & 3.46  & 2.97x & 3.89 \\
              & Eagle2   & 3.00x       & {\bf 4.75}   & {\bf 3.57x} & {\bf 5.52}  & 3.14x & {\bf 4.89}  & 2.94x & {\bf 4.60}  & 2.54x & {\bf 4.26}  & 2.67x & {\bf 4.12}  & 2.98x & {\bf 4.69} \\
              & Jakiro    & {\bf 3.03x} & 4.67         & 3.56x       & 5.42 & {\bf 3.16x} & 4.79 & {\bf 2.96x} & 4.51 & {\bf 2.55x} & 4.16 & {\bf 2.67x} & 4.04 & {\bf 2.99x} & 4.60 \\
        \midrule
        \multicolumn{16}{c}{Temperature=1} \\
        \midrule
        \multirow{4}[2]{*}{ V 7B} 
              & Eagle1    & 2.40x & 3.55  & 2.75x & 3.85  & 2.35x & 3.67  & 2.27x & 3.62  & 2.15x & 3.10  & 2.04x & 2.93  & 2.33x & 3.45 \\
              & Eagle2    & 2.69x & 4.27  & 3.11x & 4.66  & 2.68x & 4.45  & 2.60x & 4.37  & 2.32x & 3.84  & 2.12x & 3.49  & 2.59x & 4.18 \\
              & Jakiro    & 2.66x & 4.31  & 2.96x & 4.57  & 2.69x & 4.49  & 2.50x & 4.30  & 2.20x & 3.85  & 2.09x & 3.52  & 2.52x & 4.17 \\
              & Jakiro*   & {\bf 3.86x} & {\bf 5.68} & {\bf 2.97x} & {\bf 4.88} & {\bf 3.32x} & {\bf 5.89} & {\bf 3.35x} & {\bf 5.54} & {\bf 3.29x} & {\bf 5.51} & {\bf 3.46x} & {\bf 5.14} & {\bf 3.38x} & {\bf 5.44} \\
              \midrule
        \multirow{4}[2]{*}{L2 7B} 
              & Eagle1    & 2.52x & 3.64  & 2.79x & 4.05  & 2.56x & 3.77  & 2.47x & 3.58  & 2.15x & 3.15  & 2.30x & 3.23  & 2.46x & 3.57 \\
              & Eagle2    & 2.95x & 4.49  & 3.39x & 5.04  & 3.05x & 4.69  & 2.94x & 4.56  & 2.42x & 3.94  & 2.48x & 4.06  & 2.87x & 4.46 \\
              & Jakiro    & 2.91x & 4.43       & {\bf 3.17x} & {\bf 4.87} & 2.90x & 4.57 & 2.76x & 4.38 & 2.32x & 3.89 & 2.37x & 3.98 & 2.74x & 4.35 \\
              & Jakiro*   & {\bf 3.36x} & {\bf 5.17} & 3.03x & 4.68       & {\bf 3.02x} & {\bf 4.65} & {\bf 3.31x} & {\bf 5.23} & {\bf 3.38x} & {\bf 5.36} & {\bf 3.30x} & {\bf 4.86} & {\bf 3.23x} & {\bf 4.99} \\
        \midrule
        \multirow{4}[2]{*}{V 13B} 
              & Eagle1    & 2.57x & 3.60  & 2.71x & 3.92  & 2.64x & 3.78  & 2.35x & 3.73  & 2.14x & 3.30  & 2.01x & 3.01  & 2.40x & 3.56 \\
              & Eagle2    & 2.62x & 4.31  & 3.02x & 4.80  & 2.75x & 4.46  & 2.61x & 4.54  & 2.28x & 4.00  & 2.17x & 3.49  & 2.57x & 4.27 \\
              & Jakiro*   & {\bf 3.00x} & {\bf 4.62} & {\bf 2.92x} & {\bf 4.76} & {\bf 2.93x} & {\bf 4.96} & {\bf 2.51x} & {\bf 4.84} & {\bf 1.96x} & {\bf 4.02} & {\bf 2.53x} & {\bf 4.03} & {\bf 2.64x} & {\bf 4.54} \\
        \midrule
        \multirow{4}[2]{*}{L2 13B} 
              & Eagle1    & 2.73x & 3.67  & 3.08x & 4.27  & 2.68x & 3.80  & 2.52x & 3.66  & 2.22x & 3.37  & 2.42x & 3.36  & 2.61x & 3.69 \\
              & Eagle2    & 2.75x & 4.58  & {\bf 3.31x} & 5.33  & 2.93x & 4.68  & 2.78x & {\bf 4.56}  & 2.41x & 4.16  & 2.52x & 4.04  & 2.78x & {\bf 4.56} \\
              & Jakiro*   & {\bf 3.15x} & {\bf 4.67} & 2.93x & {\bf 4.10} & {\bf 3.43x} & {\bf 4.94} & {\bf 2.83x} & 4.14 & {\bf 2.88x} & {\bf 4.42} & {\bf 2.83x} & {\bf 4.16} & {\bf 3.01x} & 4.41 \\
        \bottomrule
        \end{tabular}%
        }
      \label{tab:A100_all_models}%
  \end{table*}%


\end{document}

