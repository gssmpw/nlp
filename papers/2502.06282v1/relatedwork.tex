\section{Related Work}
{\bf Speculative decoding:}
Speculative decoding (SD)has emerged as a powerful technique to accelerate LLMs inference by reducing memory bandwidth bottlenecks. Early speculative decoding methods, such as those by~\cite{stern2018blockwise} and~\cite{sun2021instantaneous}, focused on greedy decoding strategies, while~\cite{leviathan2023fast} and~\cite{chen2023accelerating} expanded speculative sampling to non-greedy decoding. Recent works in SD have enhanced draft model efficiency, with methods like SpecInfer~\cite{miao2023specinfer} employing tree attention to verify multiple draft tokens in parallel, and Medusa~\cite{cai2024medusa} using extra MLP heads to generate token drafts. Although these methods have achieved notable acceleration, they face limitations in token diversity and decoupling between draft and target models. Eagle~\cite{li2024eagle} introduces a more dynamic approach by decoupling the draft tokens across different time steps. However, Eagle still maintains coupling between the Top-k tokens at the same layer in the draft tree, which restricts the diversity and specialization of tokens. Our approach builds upon these limitations by introducing a dynamic decoupling mechanism with Mixture of Experts (MoE) heads, which enables draft tokens to consider inherent differences between them, leading to more diversity and confident predictions.

{\bf Mixture of Experts:}
MoE has been explored extensively for improving the specialization of models. MoE techniques were originally proposed by~\cite{ori_moe1, ori_moe2} and later adapted to language models, such as in GShard~\cite{gshard} and Switch Transformer~\cite{switch}, which scale MoE to large models with top-k routing strategies. More recently, the integration of MoE in Transformer-based architectures has garnered significant interest, with methods like StableMoE~\cite{stablemoe} exploring fixed routing strategies for more stable training. MoE heads have also been used in multi-modal settings~\cite{glam,openmoe}, where they enable specialization across different modalities. Furthermore, in the context of speculative decoding, our method combines MoE's dual-branch heads with contrastive decoding techniques, a strategy inspired by recent works~\cite{moe,stablemoe} to improve the usefulness of draft token predictions, especially in greedy modes. By integrating these strategies, we can achieve more reliable predictions with faster inference, as demonstrated through our experiments.

{\bf Parallel decoding:}
Parallel decoding is known for its efficiency in machine translation~\cite{parallel_dec_1} and code generation \cite{meta_multitoken}, has also been integrated into SD frameworks to further enhance efficiency. Although the use of parallel decoding in speculative frameworks has been under-explored, works like \cite{monea2023pass} and \cite{yi-etal-2024-generation} have pioneered its application. These methods, however, still face challenges in achieving a perfect alignment between draft distributions and target models, which can limit their effectiveness in lossless acceleration. Our approach addresses these challenges by improving the decoupling mechanism within the MoE heads, ensuring better alignment and more diverse token predictions in both greedy and non-greedy modes.

% Finally, various works have investigated model compression, quantization, and pruning as techniques for accelerating LLM inference. These methods aim to reduce computational cost but often sacrifice some degree of model performance. In contrast, speculative decoding frameworks, such as the one proposed in this paper, achieve lossless acceleration by utilizing efficient draft models, enabling faster generation without compromising output quality.