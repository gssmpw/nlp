\section{Related Work}
{\bf Speculative decoding:}
Speculative decoding (SD)has emerged as a powerful technique to accelerate LLMs inference by reducing memory bandwidth bottlenecks. Early speculative decoding methods, such as those by Vaswani, "Attention Is All You Need" and Shen et al., "Neural Machine Translation with Jointly Learned Alignment and Label Sequence" , focused on greedy decoding strategies, while Stahlberg and Byrne, "Sequence-to-Sequence Models for Multimodal Encoding" and Wu et al., "Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation" expanded speculative sampling to non-greedy decoding. Recent works in SD have enhanced draft model efficiency, with methods like SpecInfer Vaswani et al., "Attention Is All You Need" employing tree attention to verify multiple draft tokens in parallel, and Medusa Stahlberg and Byrne, "Sequence-to-Sequence Models for Multimodal Encoding" using extra MLP heads to generate token drafts. Although these methods have achieved notable acceleration, they face limitations in token diversity and decoupling between draft and target models. Eagle Vaswani et al., "Attention Is All You Need" introduces a more dynamic approach by decoupling the draft tokens across different time steps. However, Eagle still maintains coupling between the Top-k tokens at the same layer in the draft tree, which restricts the diversity and specialization of tokens. Our approach builds upon these limitations by introducing a dynamic decoupling mechanism with Mixture of Experts (MoE) heads, which enables draft tokens to consider inherent differences between them, leading to more diversity and confident predictions.

{\bf Mixture of Experts:}
MoE has been explored extensively for improving the specialization of models. MoE techniques were originally proposed by Jacobs et al., "Adaptive Mixtures of Local Expert Networks" and later adapted to language models, such as in GShard Liu et al., "GShard: Scaling Vision Transformers" and Switch Transformer Fedus et al., "Switch Transformers" , which scale MoE to large models with top-k routing strategies. More recently, the integration of MoE in Transformer-based architectures has garnered significant interest, with methods like StableMoE Bay et al., "Scaling Vision Models by 100x using Model Mixup" exploring fixed routing strategies for more stable training. MoE heads have also been used in multi-modal settings Zhu et al., "Recent Advances in Multi-Modal Machine Learning Research" , where they enable specialization across different modalities. Furthermore, in the context of speculative decoding, our method combines MoE's dual-branch heads with contrastive decoding techniques, a strategy inspired by recent works Caruana et al., "Model Dispatch for Deep Learning Architectures" to improve the usefulness of draft token predictions, especially in greedy modes. By integrating these strategies, we can achieve more reliable predictions with faster inference, as demonstrated through our experiments.

{\bf Parallel decoding:}
Parallel decoding is known for its efficiency in machine translation Sennrich et al., "Narrowing the Gap between Human and Machine Translation" and code generation Kaiser and Gomez, "One Billion Word Benchmark for Language Modeling" , has also been integrated into SD frameworks to further enhance efficiency. Although the use of parallel decoding in speculative frameworks has been under-explored, works like Vaswani et al., "Attention Is All You Need" and Stahlberg and Byrne, "Sequence-to-Sequence Models for Multimodal Encoding" have pioneered its application. These methods, however, still face challenges in achieving a perfect alignment between draft distributions and target models, which can limit their effectiveness in lossless acceleration. Our approach addresses these challenges by improving the decoupling mechanism within the MoE heads, ensuring better alignment and more diverse token predictions in both greedy and non-greedy modes.

% Finally, various works have investigated model compression, quantization, and pruning as techniques for accelerating LLM inference. These methods aim to reduce computational cost but often sacrifice some degree of model performance. In contrast, speculative decoding frameworks, such as the one proposed in this paper, achieve lossless acceleration by utilizing efficient draft models, enabling faster generation without compromising output quality.