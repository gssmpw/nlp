%!TEX root=../paper.tex
\section{Experimental Setup}
\label{sec:experimentalsetup}

\subsection{Research Questions}
Our primary objectives are to evaluate whether the ensemble of small language models enhances fault localisation effectiveness and to investigate the cost-benefit trade-off of ensembles.

\begin{itemize}
    \item \textbf{RQ1. Effectiveness:} To what extent does our ensemble technique improve fault localisation performance? To address this, we conducted initial runs with seven open-source SLMs to assess model orthogonality, running each model five times. Based on these preliminary results, we selected four models that demonstrated the most complementary fault localisation performance in combination. For the final evaluation, we ran AutoFL on each selected model 30 times and sampled a varying number of runs to compare two ensemble weighting strategies: equal and DE-optimised weightings.
    
    \item \textbf{RQ2. Efficiency:} How does the ensemble technique perform in terms of the cost-performance trade-off, and does it contribute to balancing these factors? RQ2 focuses on computational efficiency, considering model inference time, energy consumption, and number of tokens as the measure of cost. We report the sum of input and output tokens as the number of tokens, as this reflects the pricing models of closed-source LLMs more closely.
\end{itemize}

\subsection{Language Models}

To align with the characteristics of smaller language models, we redesign the agent's task from a chat-completion to instruction-following. Each model has been downloaded and served using Ollama~\cite{Ollama2024}, which is chosen for its convenient setup and support for multiple models. We focus on 4-bit quantised models for the sake of memory usage, and choose the following open-source SLMs: \emph{CodeLlama 7B}~\cite{roziereCodeLlamaOpen2024}, \emph{Gemma2 9B}~\cite{teamGemma2Improving2024}, \emph{grantie3 8B}~\cite{Granite30Language2024}, \emph{Llama3 8B}~\cite{dubeyLlama3Herd2024}, \emph{Llama3.1 8B}~\cite{dubeyLlama3Herd2024}, \emph{Mistral NeMo 12B}~\cite{MistralNeMo2024}, and \emph{Qwen2.5-Coder 7B}~\cite{huiQwen25CoderTechnicalReport2024}. These models are all below the size of 8GB (when quantised for 4-bit): we expect them to be compatible with a wider range of machines without GPUs.

Techniques involving inherent randomness require sufficient repetitions to ensure reliable performance measurement~\cite{Arcuri2011ee}. We employ sampling to stabilise our performance metrics and account for stochastic variation of language models. We first run AutoFL with each selected model 30 times. Then, for a given number of runs (R) ranging from 4 to 24, we sample R runs from the 30 runs 20 times for each model. For the case of ensembles, we allocated an equal number of runs to each model, ranging from 1 to 6, resulting in R of multiples of 4.

\subsection{Dataset}

Our evaluation dataset is a subset of Defects4J~\cite{just2014defects4j} used by AutoFL, comprising a total of 353 bugs. We calculate \textit{acc@k} as the number of bugs for which at least one buggy method is correctly ranked within the top \textit{k} places, ensuring consistency with the prior work~\cite{kangQuantitativeQualitativeEvaluation2024a}. Table~\ref{tab:dataset_details} summarises the number of bugs, Lines of Code (LOC) measured using \texttt{cloc}~\cite{adanial_cloc}, and the number of methods and tests for each project.

\begin{table}[htbp]
\caption{Evaluation Dataset Details}
\begin{center}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{lrrrr}
\toprule
Project & \#Bugs & LOC             & \#Methods    & \#Tests    \\ \midrule
Chart   & 26     & 78,564--96,382  & 6,378--8,041 & 1,598--2,201 \\
Closure & 131    & 58,989--104,131 & 4,621--8,700 & 2,692--8,625 \\
Lang    & 64     & 16,593--21,810  & 1,794--2,248 & 1,587--2,265 \\
Math    & 106    & 9,471--84,317   & 1,174--6,015 &   686--3,548  \\
Time    & 26     & 26,589--27,795  & 3,535--3,696 & 3,802--4,054 \\
\bottomrule
\end{tabular}}
\label{tab:dataset_details}
\end{center}
\end{table}

\subsection{Hyperparameters \& Environment}

We utilise DEAP~\cite{DEAP_JMLR2012}, a framework for evolutionary compuation, to implement the differential evolution algorithm. To reduce the over-fitting during the optimisation process, we apply 10-fold cross-validation. We select the DE parameters referring to Storn et al.~\cite{stornUsageDifferentialEvolution1996}, a population size of 40 and 30 generations. To foster exploratory behaviour during the optimisation process, we set high differential weight, 1.5, and also the crossover probability to 0.8.

EnergyMeter~\cite{argerichMeasuringImprovingEnergy2024} is an open-source Python project that measures the energy consumption incurred by the hardware. As we focus on the language model inference cost, we only utilise the GPU energy consumption enabled by \texttt{nvidia-smi}~\cite{developer2021nvidia}.

We conduct all our experiments using the Docker image \texttt{nvidia/cuda:11.3.1-runtime-ubuntu20.04} on a Linux server equipped with 252 GB RAM and 40 Intel Xeon Silver 2.40GHz CPUs. A single NVIDIA GeForce RTX 3090 GPU is utilised to accelerate model inference.