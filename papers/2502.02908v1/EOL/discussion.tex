%!TEX root=../paper.tex

\section{Discussion}
\label{sec:discussion}

\begin{figure*}[t]
\centerline{\includegraphics[width=0.95\textwidth]{EOL/figs/grid_landscape.pdf}}
\caption{Landscapes of Pairwise $acc@1$ explored by grid search}
\label{fig:grid_landscape}
\end{figure*}

% \fixme{Supplementary for grid search results?}

To better understand the performance characteristics of the two ensemble schemes, we conduct a grid search over weights for pairwise ensembles of models, increasing the number of runs ($R_{m}$) from one to five, as illustrated in Figure~\ref{fig:grid_landscape}. Interestingly, the optimal weights in most scenarios are close to equal, supporting the efficacy of equal weight ensembles. As $R_{m}$ increases, the $acc@1$ landscape becomes more rugged, implying a heightened risk of DE converging to local minima that do not generalise well to validation sets.

We conjecture that introducing a more robust optimisation scheme could further harness the potential of the models. In this context, emerging techniques like expert choice routing~\cite{zhou2022mixture}, which originated from Mixture-of-Experts architectures, present promising avenues. Such approaches could potentially direct computational resources to the most suitable model dynamically, thereby enhancing performance and efficiency. Recently, RouteLLM~\cite{ong2024routellm} exploring router designs for compound language model systems provides early evidence of cutting costs while maintaining capabilities.

In contrast to existing works leveraging multiple language models~\cite{ong2024routellm, shen2024decodecollaboratively}, which focus on model specialization or task delegation, our approach emphasizes orthogonality. Specifically, we aim to construct ensembles where each component possesses comparable yet complementary capabilities, rather than relying on stark performance differences. Exploring the combination of models with substantial capability disparities within \name remains an avenue for future work.

% In addition, we are working to generalize \name to other software engineering tasks. Our ongoing efforts include aggregating results from LLM-based automated program repair techniques, with plans to extend this approach to other language model-driven methodologies.

This study focuses on the quantitative aspect of the FL performance. However, the original work~\cite{kangQuantitativeQualitativeEvaluation2024a} also points that LLMs have the potential to provide rationales for their decisions. Since we now have a diverse set of runs, it would be interesting to see if we can rank generated explanations better based on the larger number of runs sampled from different SLMs, with more practical impact for the developers.