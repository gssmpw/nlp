%!TEX root=../paper.tex

\section{Introduction}
\label{sec:introduction}

Large Language Models (LLMs) are rapidly being adopted by software engineers to automate various tasks across the software development lifecycle~\cite{Fan2023yu}. While LLMs are essentially autocompletion engines trained on a vast amount of data~\cite{Vaswani2017aa}, they have exhibited many useful emergent behaviour, including their capability to perform in-context learning. This has led to many advanced prompting/inference techniques, such as Chain-of-Thought~\cite{Wei2024aa}, self-consistency~\cite{wang2022self}, and ReAct~\cite{Yao2022qf}. Increasingly, these techniques are being used to build LLM-based \emph{agents}~\cite{Feldt2023ax,Bouzenia2024aa,Yoon2024aa}.

%In the meantime, most existing works are coupled with proprietary models, especially the GPT. This hinders the wider application of emerging techniques, depending on the environments.
One challenge in broader adoption of these techniques is the dependence on commercial and closed LLMs. In addition to the financial cost of using those models, organisations may not want to reveal their source code as part of any prompt that are fed to external LLMs, for security reasons. Finally, while growing LLM sizes have so far been accompanied with improving performance, there are concerns about the environmental impact that these large models have~\cite{Strubell2019aa,Rillig2023aa}. 

% These dependencies often restrict their applicability across diverse environments, hindering the practical integration and generalization of LLM-based techniques.

%Small language models (SLMs) are on a rise in terms of capability, thus as an alternative for the above problem. Yet individual capabilities are unmatched to that of GPT. Thus, we tried to tackle this problem by leveraging the power of multiple SLMs.
Recently, Small Language Models (SLMs) with open source licenses have started gaining traction due to their improving capabilities and ability to be hosted and served more locally, offering an alternative to the larger, closed models and their limitations~\cite{dubeyLlama3Herd2024,teamGemma2Improving2024,huiQwen25CoderTechnicalReport2024}. However, individual SLMs typically fall short of the comprehensive performance delivered by larger language models like GPT-4~\cite{achiam2023gpt}, creating a noticeable gap in their stand-alone effectiveness~\cite{Kang2024aa}. Consequently, a potential user of LLM-based software engineering technique is presented with two options: high performance with high cost, or low performance with low cost. It would be ideal to have a broader range of choices in the cost-benefit trade-off.

We propose \name (\textbf{CO}llection of \textbf{S}mall Language \textbf{Mo}del\textbf{s}), an ensemble of SLMs: it builds upon self-consistency, a prompting technique that states, for logical tasks, it is better to take multiple samples of LLM answers and marginalise them~\cite{wang2022self}. While self-consistency has been widely accepted as a simple yet effective validation technique that can improve the correctness of LLM-generated solutions~\cite{Kang2024aa,Ahmed2023aa,kangQuantitativeQualitativeEvaluation2024a}, it exacerbates the issue of cost due to its need to sample multiple LLM-generated solutions. \name aims to both exploit and alleviate challenges that self-consistency introduces. Instead of taking multiple samples from a closed LLM, \name forms an ensemble of SLMs and aggregate their answers to form the final solution. In turn, we hope to lower the cost of the self-consistency based inferences by using SLMs, while maintain high FL accuracy. 

In this paper, we concretely evaluate \name by instantiating it with ensembles of LLM-based Fault Localisation (FL) technique, AutoFL~\cite{kangQuantitativeQualitativeEvaluation2024a}, to obtain \cosmosfl. We first choose the membership of the ensemble based on the FL performance of individual SLMs. Subsequently, the ensemble of SLMs provide the multiple inference samples that result in the final ranking of the likely faulty methods via voting. We evaluate two ensemble schemes: one where each member SLM has the equal voting power, and another where we optimise the relative voting weights with the aim of improving the resulting FL accuracy. We report not only the FL accuracy of the ensemble, but also various cost measures including number of tokens, inference time, and the overall energy consumption. Our evaluation of \cosmosfl using Defects4J~\cite{just2014defects4j} shows that ensembles can indeed outperform individual models when the FL task is constrained by energy consumption or token count. To facilitate further research and reproducibility, we publicly release our implementation at GitHub\footnote{https://github.com/coinse/cosmosfl}. Detailed technical contributions are as follows:

\begin{itemize}
\item We introduce \name, an ensemble of multiple open source SLMs. While ensembles of LLMs have been proposed for token level decoding, \name is the first to introduce task-level voting based ensembles.

\item We compare two ensemble methods: a vanilla voting-based ensemble, and a weight-optimised ensemble. The weight optimisation uses Differential Evolution to optimise the weights that are applied to votes cast by membership SLMs.

\item We instantiate \name with an LLM-based FL technique, AutoFL, to obtain \cosmosfl. We evaluate its performance using the widely studied Defects4J benchmark. We report the trade-off between FL accuracy and various costs, such as power consumption, token size, and inference time.
\end{itemize}

The remainder of the paper is organised as follows. Section~\ref{sec:approach} presents AutoFL and the design of \name. Section~\ref{sec:experimentalsetup} presents the settings of the empirical evaluation, the results of which are shown in Section~\ref{sec:results}. Section~\ref{sec:discussion} discusses details of our findings. Section~\ref{sec:relatedwork} presents related work that are relevant to ours, and Section~\ref{sec:conclusion} finally concludes.