%!TEX root=../paper.tex

\section{Results}
\label{sec:results}

\subsection{RQ1. Effectiveness}

The orthogonality among the four selected models—\textit{Llama3 8B}, \textit{Llama3.1 8B}, \textit{Mistral NeMo 12B}, and \textit{Qwen2.5-Coder 7B}—is visualized in Figure~\ref{fig:overlap_of_models}. We identify these models as the most orthogonal combination, collectively ranking 180 bugs first, highlighting their complementary strengths. Building on this, we examine the effectiveness of the ensemble approach for fault localization.

\begin{figure}[htbp]
\centerline{\includegraphics[width=0.75\columnwidth]{EOL/figs/orthogonal.pdf}}
\caption{Overlap of bugs ranked at first by Llama3, Llama3.1, Mistral NeMo, and Qwen2.5-Coder. Each model is run 5 times without applying ensemble.}
\label{fig:overlap_of_models}
\end{figure}

\begin{figure}[htbp]
\centerline{\includegraphics[width=\columnwidth]{EOL/figs/accatks.pdf}}
\caption{acc@k for R=20 for each model and ensemble approaches, alongside AutoFL’s reported GPT-3.5 performance.}
\label{fig:acc_at_k}
\end{figure}

Figure~\ref{fig:acc_at_k} presents the mean $acc@k$ for $k$ ranging from 1 to 5 across individual models and two ensemble approaches. The results also include the reported performance of GPT-3.5 for $R=5$. Both ensembles tend to outperform individual models as $k$ increases. Since our experiments use more runs ($R=20$) compared to the prior work ($R=5$), a direct comparison is unfair. However, the observation that even the least effective model, Llama3.1, achieves higher accuracies at $k=4$ and 5 suggests that increasing the number of runs could address the underperformance at higher $k$ values relative to SBFL methods, previously attributed to the inability to \textit{dig deep} into a repository~\cite{kangQuantitativeQualitativeEvaluation2024a}.

\begin{figure}[htbp]
\centerline{\includegraphics[width=0.9\columnwidth]{EOL/figs/overlapatk.pdf}}
\caption{Overlap of top-ranked bugs at each $k$ for a single sample ($R=20$) of DE and Equal Weight Ensembles.}
\label{fig:overlap_at_k}
\end{figure}

Since DE-optimised weights and equal weights show similar performance, we further analyse overlap for a sample of five runs from each model, totalling $R=20$. Figure~\ref{fig:overlap_at_k} indicates that applying different weights for the same runs yields mild variation in bugs ranked at top, suggesting potential for further optimisations to harness model orthogonality. As $k$ increases, however, the top-ranked bug sets become more consistent across weighting methods. SLMs list a limited number of methods as suspicious, allocating confidence score only for those. Thus, when we count the number of methods ranked at top five, they rather show higher consistency; in contrast, for the $acc@1$, they are more sensitive to the weight changes.

\begin{figure}[htbp]
\centerline{\includegraphics[width=\columnwidth]{EOL/figs/accatone.pdf}}
\caption{Mean of $acc@1$ across runs for four single models and two ensemble approaches. Note that the ensemble techniques are only available at multiples of four runs.}
\label{fig:acc_at_one_all}
\end{figure}

\begin{figure}[htbp]
\centerline{\includegraphics[width=0.8\columnwidth]{EOL/figs/best_weights.pdf}}
\caption{Mean of optimised weights for each model over cross-validation folds and samples.}
\label{fig:mean_best_weights}
\end{figure}

High accuracy at the first rank is crucial for FL tasks, so we focus on $acc@1$ in Figure~\ref{fig:acc_at_one_all}. The ensemble does not surpass the best single model’s performance when given the same number of runs. This outcome can be attributed to how runs are distributed: assigning all weights to a single model effectively limits that model to R/4 runs in the ensemble setup, rather than the full R runs it would receive if evaluated independently. Consequently, mean optimised weights over samples and cross-validation folds, depicted in Figure~\ref{fig:mean_best_weights}, ranged from 0.15 to 0.37 for all four models. Although the DE-based optimisation underperforms the equal weighting, resulting weights still utilize information from all runs.

While individual models tend to converge as $R$ increases, ensembles maintain a relatively stable variance level across runs, despite generally outperforming individual models. Figure~\ref{fig:acc_at_one_each} illustrates the distribution of $acc@1$ over samples, capturing this trend. This highlights the need to explore more advanced methods for constructing ensembles.

\begin{figure}[htbp]
\centerline{\includegraphics[width=\columnwidth]{EOL/figs/distributions.pdf}}
\caption{acc@1 across runs for each model and ensemble.}
\label{fig:acc_at_one_each}
\end{figure}
\begin{figure*}[h]
\centering
\begin{subfigure}[t]{0.32\textwidth}
\includegraphics[width=\textwidth]{EOL/figs/acc_energy.pdf}
\caption{Energy Consumption vs. $acc@1$\label{fig:acc_energy}}
\end{subfigure}
\begin{subfigure}[t]{0.32\textwidth}
\includegraphics[width=\textwidth]{EOL/figs/acc_time.pdf}
\caption{Execution Time vs. $acc@1$\label{fig:acc_time}}
\end{subfigure}
\begin{subfigure}[t]{0.32\textwidth}
\includegraphics[width=\textwidth]{EOL/figs/acc_token.pdf}
\caption{\# of Tokens vs. $acc@1$\label{fig:acc_tokens}}
\end{subfigure}
\caption{Cost-Benefit Tradeoff: average energy consumption, inference time, and \# of input and output tokens, across 20 samples}
\end{figure*}

\subsection{RQ2. Efficiency}

% \begin{figure}[htbp]
% \centerline{\includegraphics[width=\columnwidth]{EOL/figs/acc_energy.pdf}}
% \caption{FL Performance over Energy Consumption. We caculated the mean energy consumption over 20 samples, each sample composed of R runs on the 353 bugs.}
% \label{fig:acc_energy}
% \end{figure}

% \begin{figure}[htbp]
% \centerline{\includegraphics[width=\columnwidth]{EOL/figs/acc_time.pdf}}
% \caption{FL Performance over Execution Time. We caculated the mean of execution time per sample.}
% \label{fig:acc_time}
% \end{figure}

% \begin{figure}[htbp]
% \centerline{\includegraphics[width=\columnwidth]{EOL/figs/acc_token.pdf}}
% \caption{FL Performance over Tokens read and generated. We summed up the number of input and output tokens for queries and calculated the mean count per sample.}
% \label{fig:acc_tokens}
% \end{figure}

% \begin{subfigure}[b]{0.15\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{figures/Figure_lstm_FAA_ROC_AUC.pdf}
%         \caption{LSTM}
%         \label{fig4_lstm_roc}
%     \end{subfigure}

\begin{figure*}[ht]
\centering
\begin{subfigure}[t]{0.32\textwidth}
\includegraphics[width=\textwidth]{EOL/figs/best4_energy.pdf}
\caption{Energy consumption per run\label{fig:original_cost}}
\end{subfigure}
\begin{subfigure}[t]{0.32\textwidth}
\includegraphics[width=\textwidth]{EOL/figs/best4_execution_time.pdf}
\caption{Execution time (sec) per run\label{fig:time_cost}}
\end{subfigure}
\begin{subfigure}[t]{0.32\textwidth}
\includegraphics[width=\textwidth]{EOL/figs/best4_energy_per_seconds.pdf}
\caption{Energy consumption per second\label{fig:watt_cost}}
\end{subfigure}
\caption{Boxplots of cost measures per model. Note that $y$-axis of (a) and (b) is log of the measure.}
\end{figure*}



Figures~\ref{fig:acc_energy} to \ref{fig:acc_tokens} show the average cost of each sample, composed of R runs, and its mean $acc@1$. The best performing model, Qwen2.5-Coder consumes the most amount of energy and time, while Llama3 requires the least. Ensemble cost and performance tend to lie between these two extremes. We note that Qwen2.5-Coder is less of an outlier for the number of tokens in Figure~\ref{fig:acc_tokens}, due to the imbalance in its performance. In our evaluation, Qwen2.5-Coder spends significant amounts of time and energy when generating tokens, but input tokens take up the majority of the number of tokens used by an inference run, balancing out the energy and time cost.

We visualize the distribution of energy consumptions and execution time in Figure~\ref{fig:original_cost} and \ref{fig:time_cost} to further investigate the consumption pattern. Unlike the per sample consumptions, all four models exhibit similar median value. The median consumption of Qwen2.5-Coder is actually the second smallest among the four models. However, Llama3.1 and Qwen2.5-Coder show a cluster of outliers at around 10,000 times the median value, which we suspect is linked to an issue in Ollama, given that there is an issue report about some models generating tokens endlessly~\cite{OllamaGetsStuck}. This erroneous behaviour may have inflated Qwen2.5-Coder's overall energy consumption. Since the total energy consumption and execution time follow similar trends, the energy consumption per second remains relatively constant, as shown in Figure~\ref{fig:watt_cost}.


