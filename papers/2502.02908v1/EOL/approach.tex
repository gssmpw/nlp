%!TEX root=../paper.tex

\section{Approach}
\label{sec:approach}

\subsection{Preliminaries}

\subsubsection{Self-Consistency} Self-consistency is a property of LLM-based reasoning that, for a complex reasoning task, taking multiple samples of LLM inferences and marginalising over them tend to yield more accurate answers when compared to greedy decoding~\cite{wang2022self}. This has been widely adopted by applications of LLMs for software engineering tasks~\cite{Ahmed2023aa,kangQuantitativeQualitativeEvaluation2024a}.

\subsubsection{AutoFL} AutoFL~\cite{kangQuantitativeQualitativeEvaluation2024a} is a LLM agent for repository-level FL, incorporating four tools to gather project-related information: 1) class-level coverage of the failing test, 2) method-level coverage within a covered class, 3) the code snippet of the given method, and 4) comments associated with the method. The original AutoFL utilised GPT-3.5 and GPT-4.

To improve accuracy and reliability, AutoFL leverages self-consistency~\cite{wang2022self} by performing FL $R$ times independently for a single bug. In each inference run, it predicts a set of likely buggy methods, and the results are aggregated using a voting-based mechanism. Specifically, each predicted method in a run is assigned a score equal to 1 divided by the total number of predicted methods in that run. These scores are averaged across all runs, ensuring that the sum of the scores for all methods equals 1, and are then used to derive the final ranking.
For example, if the predicted methods from five runs ($R=5$) are \(\{m_1, m_2\}\), \(\{m_2\}\), \(\{m_2\}\), \(\{m_2\}\), and \(\{m_3\}\), the score of \(m_2\) is \(\left(\frac{1}{2} + 1 + 1 + 1 + 0\right)/5 = 0.7\), while \(m_1\) and \(m_3\) get \(0.1\) and \(0.2\), respectively. In AutoFL, the confidence in its result is defined by the maximum score of the methods, e.g., \(0.7\) in the previous example: a higher maximum score indicates better alignment of results across multiple runs.

AutoFL demonstrates that LLMs can be effectively applied to FL when augmented with specialised tools to extract contextual information. However, its original evaluation is limited to GPT models, which are proprietary and commercially available LLMs. While GPT models generally exhibit strong performance, their use may not always be feasible due to concerns such as code security and monetary costs.

\subsection{Ensemble of Small Language Models}

\begin{figure*}[t]
\centerline{\includegraphics[width=0.9\textwidth]{EOL/figs/overview.pdf}}
\caption{Overview of our approach against AutoFL~\cite{kangQuantitativeQualitativeEvaluation2024a} with differences colored in red.}
\label{fig:overview}
\end{figure*}

Intuitively, \name is a voting-based ensemble of SLMs that makes use of the multiplicity of reasoning samples required by self-consistency. Ensembles of SLMs can be constructed by collecting multiple reasoning samples from participating SLMs, instead of repeated sampling of a single LLM. Subsequently, \name marginalises over the samples using the voting-based ensemble mechanism. We posit that \name can be applied to any task for which self-consistency is shown to improve the performance of LLMs.

We instantiate \name with Fault Localisation (FL) problem and propose COSMosFL (\textbf{CO}llection of \textbf{S}mall Language \textbf{Mo}del\textbf{s} for FL), a novel approach that enables the application of diverse SLMs for FL (the overview of \cosmosfl is shown in Figure~\ref{fig:overview}, along with AutoFL). We evaluate whether an ensemble of heterogeneous open-source SLMs, capable of running on local machines without network access (ensuring high security), can also effectively address the FL problem. Further, \cosmosfl will allow us to investigate the cost-benefit trade-offs of our ensemble approach, \name, with respect to a concrete task.

% We propose COSMosFL(\textbf{CO}llection of \textbf{S}mall Lanague \textbf{Mo}del\textbf{s} for FL) to enable application of diverse SLMs for the FL problem. Figure \ref{fig:overview} illustrates the overview of our technique, highlighting the difference between the original work and ours with red boxes.


To explore the effectiveness of SLMs in performing the FL task within the AutoFL framework, we first replaced GPT with open-source modelsâ€”Llama3 8B~\cite{dubeyLlama3Herd2024} and Gemma2 9B~\cite{teamGemma2Improving2024}. Our initial experiments found that smaller models frequently called the first tool (class-level coverage) redundantly, diminishing performance. Based on this observation, we remove the class-level coverage tool for \cosmosfl. 
%Table \ref{tab:performance_comparison} demonstrates an improvement in FL performance from the template modification.

% \begin{table}[t]
% \caption{Performance Comparison on Defects4J~\cite{just2014defects4j} (353 Bugs)}
% \begin{center}
% \resizebox{\columnwidth}{!}{%
% \begin{tabular}{llrrrrr}
% \toprule
% \multirow{2}{*}{\textbf{Model}} & \multirow{2}{*}{\textbf{Template}} & \multicolumn{5}{c}{\textbf{Accuracy (acc@k)}} \\
% \cmidrule{3-7}
% & & \textbf{\textit{acc@1}} & \textbf{\textit{acc@2}} & \textbf{\textit{acc@3}} & \textbf{\textit{acc@4}} & \textbf{\textit{acc@5}} \\ \midrule
% Llama3 8B & original &  80 & 115 & 131 & 140 & 148 \\
% Llama3 8B & modified & \textbf{108} & \textbf{147} & \textbf{168} & \textbf{180} & \textbf{190} \\ \midrule
% Gemma2 9B & original & 105 & 134 & 150 & 160 & 166 \\
% Gemma2 9B & modified & \textbf{112} & \textbf{145} & \textbf{159} & \textbf{169} & \textbf{182} \\
% \bottomrule
% \end{tabular}}
% \label{tab:performance_comparison}
% \end{center}
% \end{table}

% \begin{figure}[t]
% \centerline{\includegraphics[width=0.65\columnwidth]{EOL/figs/approach_overlap.pdf}}
% \caption{Overlap of top-ranked bugs identified by Llama3 8B (left) and Gemma2 9B (right) over five individual repetitions.}
% \label{fig:appraoch_overlap}
% \end{figure}

The initial evaluation of two models also show that they rank distinct sets of bugs at the top, a characteristic we refer to as orthogonality. Comparing bugs ranked at the top by Llama3 8B and Gemma2 9B over five repetitions, we see that 71 bugs are ranked at the top by both models, while 37 exclusively by Llama3 8B and 41 exclusively by Gemma2 9B: approximately 35\% of correctly localised bugs are uniquely identified by each model. This orthogonality suggests that individual models may excel at localizing certain types of faults missed by others, providing a foundation for an ensemble of heterogeneous SLMs to enhance FL performance by leveraging their complementary behaviour. 
% This motivates us to explore ensembles with a wider range of SLMs.

Building on the confidence scores generated by AutoFL, we develop a voting-based ensemble technique. Since the original AutoFL already uses voting as the aggregation mechanism, it is straightforward to implement a voting-based ensemble: instead of $R$ inference runs from a single LLM, \cosmosfl takes $R_M$ inference runs per each of the $M$ member models of the ensemble, such that $R_M \times M = R$. Subsequently, the same voting-based aggregation takes place, producing the final confidence score based on the ensemble of SLMs.

% This approach aggregates rankings by calculating a weighted sum of the confidence scores from each model, producing a combined ranking that leverages the strengths of individual models. Unlike the original use of confidence scores in AutoFL, which focuses only on the maximum score, we extend the concept to encompass all methods associated with a bug so that the information collected during individual executions is fully utilized.

\SetKwComment{Comment}{/* }{ */}
\begin{algorithm}
\caption{Differential Evolution\label{alg:de}}
\KwIn{Problem Dimension $n$, Fitness Evaluator \textbf{fitness}}
\KwOut{Best Agent $best$}

$n_{pop} \gets$ Population Size\;
$n_{gen} \gets$ Number of Generations\;
$p_{cx} \gets$ Crossover Probability\;
$w_{d} \gets$ Differential Weight\;
$pop \gets$ randomly generate $n_{pop}$ agents\;

$gen \gets 0$\;
\While{$gen < n_{gen}$} {
    \For{$agent$ in $pop$}{
        $agent_{ref} \gets$ \textbf{clone}($agent$)\;
        $a, b, c \gets$ \textbf{sample\_three\_from}($pop$)\;
        $R \gets$ \textbf{random}(\textit{[1, ..., $n$]})\;
        \For{$i$ in [1, ..., $n$]}{
            \If{$i = R$ or \textbf{uniform}(0, 1) $<$ $p_{cx}$} {
                $agent_{ref}[i] \gets a[i] + w_{d} * (b[i] - c[i])$\;
            }
        }
        \If{\textbf{fitness}($agent$) $<$ \textbf{fitness}($agent_{ref}$)} {
            $agent \gets agent_{ref}$\;
        }
    }
    $best_{gen} \gets$ \textbf{select\_best}($pop$)\;
    \If{\textbf{fitness}($best$) $<$ \textbf{fitness}($best_{gen}$)}{
        $best \gets best_{gen}$\;
    }
    $gen\gets gen + 1$\;
}
\end{algorithm}

We investigate two ensemble strategies to aggregate scores from individual models. The first, equal weighting, naively sums the scores from each model with uniform weights, providing a straightforward approach. The second, DE-optimised weighting, refines the voting weights using Differential Evolution (DE) algorithm~\cite{storn1995differential}. A distinctive feature of DE, as shown in line 14 of Algorithm~\ref{alg:de}, is its use of scaled differences between agents to guide the generation of new candidates, enabling exploration of the search space. This characteristic has been shown to make DE particularly effective for problems in continuous search spaces~\cite{das2010differential, sohn2023arachne}, making it an appropriate candidate for our weight optimisation task. This optimisation aims to maximise $acc@1$ -- prioritising the buggy method in the top rank -- while minimising wasted effort as a secondary objective in case of ties. With these objectives, we aim to rank the buggy method at top while minimising the number of irrelevant method inspections.

Considering the high inference cost of language models, we further hypothesise that the ensemble approach can better balance the cost-performance trade-off when utilizing multiple models. To enable further analysis, we integrate EnergyMeter~\cite{argerichMeasuringImprovingEnergy2024} to track GPU energy consumption throughout experiments. In addition, we also report the number of tokens and the time taken for the inference as cost of \cosmosfl.