\documentclass{article}

\usepackage{PRIMEarxiv}

\usepackage{algorithm}
\usepackage{algorithmic}

\usepackage[numbers]{natbib}
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{booktabs} %
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
    pdftitle={Overleaf Example},
    pdfpagemode=FullScreen,
    }

\newcommand{\theHalgorithm}{\arabic{algorithm}}


\usepackage{tikz}
\usetikzlibrary{bayesnet}
\usetikzlibrary{arrows}
\usetikzlibrary{positioning, arrows.meta}
\usepackage{subcaption}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{makecell}
\usepackage{amsfonts}
\usepackage{bm}
\usepackage{thmtools} 
\usepackage{thm-restate}
\usepackage{enumitem}
\usepackage{authblk}
\usepackage{longtable}
\usepackage{wrapfig}
\usepackage{multirow}
\usepackage{adjustbox}
\usepackage{colortbl}
\newcolumntype{H}{>{\setbox0=\hbox\bgroup}c<{\egroup}@{}}
\usepackage{makecell}
\usepackage{array}
\usepackage{listings}

\usepackage[capitalize,noabbrev]{cleveref}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{example}{Example}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}



\usepackage[textsize=tiny]{todonotes}

\makeatletter%
\setlength{\@fptop}{5pt}
\makeatother


\pagestyle{fancy}
\thispagestyle{empty}
\rhead{ \textit{ }} 

\fancyhead[LO]{EquivaMap: Leveraging LLMs for Automatic Equivalence Checking of Optimization Formulations}



  
\title{EquivaMap: Leveraging LLMs for Automatic Equivalence Checking of Optimization Formulations
}

\usepackage{authblk}
\renewcommand\Authand{}
\renewcommand\Authands{, }

\author[1]{Haotian Zhai}
\author[2]{Connor Lawless}
\author[3]{Ellen Vitercik}
\author[4]{Liu Leqi}

\affil[1,4]{%
The University of Texas at Austin}
\affil[2,3]{%
Stanford University\thanks{
Corresponding authors: {\texttt {haotian.zhai@utexas.edu}, \texttt {lawlessc@stanford.edu}}.}
}



\begin{document}
\maketitle


\begin{abstract}

A fundamental problem in combinatorial optimization is 
identifying equivalent formulations, which can lead to more efficient solution strategies and deeper insights into a problem's computational complexity.
The need to \emph{automatically} identify equivalence 
between problem formulations 
has grown as optimization copilots---systems that generate problem formulations from natural language descriptions---have proliferated.
However, existing approaches to checking formulation equivalence lack grounding, relying on simple heuristics which are insufficient for rigorous validation.
Inspired by Karp reductions, in this work
we introduce \emph{quasi-Karp equivalence}, a formal criterion for determining when two optimization formulations are equivalent
based on the existence of a mapping 
between their decision variables. 
We propose \emph{EquivaMap}, 
a framework that leverages large language models to automatically discover such mappings, 
enabling scalable and reliable equivalence verification.
To evaluate our approach, 
we construct the first open-source dataset of 
equivalent optimization formulations, generated
by applying transformations 
such as adding slack variables or valid inequalities
to existing formulations.
Empirically, \emph{EquivaMap} 
significantly outperforms existing methods, achieving substantial 
 improvements in correctly identifying formulation equivalence.\footnote{Code for the paper can be found at \href{https://github.com/HumainLab/EquivaMap}{https://github.com/HumainLab/EquivaMap}.}


\end{abstract}


\keywords{Combinatorial Optimization \and Large Language Models \and Mixed Integer Linear Programming}

\section{Introduction}\label{sec:intro}

Combinatorial optimization lies at the heart of many of todayâ€™s most pressing challenges in operations research,  theoretical computer science, and machine learning. Its applications range from classic problems such as shortest path \cite{Korte02} and maximum flow \cite{Schrijver83} to modern challenges in neural architecture search \cite{Elsken19} and hyperparameter optimization \cite{Khadka24}. 

A fundamental problem in combinatorial optimization is identifying equivalent formulations. 
Historically, establishing equivalence has played a pivotal role in unifying problem-solving techniques and advancing theoretical characterizations of a problem's computational complexity. 
In theoretical computer science, equivalence between problems underpins the concept of NP-completeness \cite{Cook71, Karp10}, which unifies many seemingly distinct problems---such as SAT, Vertex Cover, and Subset Sum---into the same equivalence class. 
This unification enables researchers to prioritize the development of algorithms for canonical problems while ensuring their applicability across equivalent problems. 
Similarly, in applied fields such as network design \cite{Johnson78} and semiconductor scheduling \cite{fang23}, recognizing equivalence between optimization problems has historically facilitated the transfer of algorithms, reducing duplication of effort.

The advent of large language models (LLMs) has exposed a new frontier in combinatorial optimization, introducing opportunities to automate problem formulation, while also presenting new challenges---chief among them, the need for reliable equivalence checking.
Recent research has focused on developing \emph{optimization copilots}, systems that automate the translation of natural language descriptions into formal optimization formulations, %
particularly for mixed-integer linear programming (MILP) problems \cite{ramamonjison23, xiao24, Ahmaditeshizi24, astorga24}. %
These advancements hold significant potential for democratizing access to optimization techniques, broadening the reach of powerful tools for better decision-making \cite{wasserkrug2024large}. %
However, the widespread adoption of optimization copilots hinges on reliable evaluation mechanisms capable of verifying whether the generated formulations are equivalent to their ground-truth counterparts. Moreover, automatic formulation equivalence checking is critical to improving optimization copilots by serving as an intermediate step, facilitating more efficient formulation search and refinement \cite{astorga24}.  

Despite the importance of equivalence checking in combinatorial optimization, existing automatic approaches rely heavily on heuristics (e.g., comparing optimal objective values) and lack a precise, universally accepted definition of what constitutes formulation equivalence. Existing informal, ad hoc criteria are insufficient for validating the outputs of advanced systems such as optimization copilots \cite{huang24, tang24, wang24, xing24}. Historical methods such as Karp reductions \cite{Karp10} offer valuable theoretical insights into problem equivalence, but were not designed for modern automated settings.  This gap underscores the need for a framework to precisely define and reliably verify formulation equivalence, addressing the shortcomings of existing methods and supporting the development of optimization copilots.

To address these challenges, we propose a formal definition of formulation equivalence---\emph{quasi-Karp Equivalence}---grounded in the principles of Karp reductions.
Quasi-Karp Equivalence determines whether two formulations are equivalent by checking for the existence of a mapping between their decision variables.
We propose \emph{EquivaMap}, an approach that operationalizes this checking by using 
LLMs to identify such mappings.
Grounded in a precise definition of formulation equivalence, \emph{EquivaMap} allows for automatic 
equivalence verification for optimization formulations. 

Our contributions can be summarized as follows: 
\begin{itemize}[left=0pt, topsep=0pt]
    \item We identify pitfalls of existing equivalence checking methods (\cref{sec:pitfall}).
    \item We propose quasi-Karp equivalence as a formalism for defining the equivalence between two optimization formulations (\cref{sec:quasi-karp-defn}) and a scalable approach---\emph{EquivaMap}---for automatically checking formulation equivalence using LLMs (\cref{sec:equivamap}). 
    \item To evaluate the performance of equivalence-checking methods, 
    we introduce, to the best of knowledge, the first dataset---\emph{EquivaFormulation}---that documents both equivalent formulations and the transformation between them (\cref{sec:data}).
    Empirically, we show that \emph{EquivaMap} outperforms existing methods across various equivalent transformations (\cref{sec:performance}). 
\end{itemize} 


\section{Background and Related Work}\label{sec:related-work}

Our work connects important lines of research on combinatorial optimization (especially MILPs), LLMs for MILP modeling, and automatic equivalence-checking methods for optimization formulations.

\subsection{Combinatorial Optimization and MILPs}

Combinatorial optimization (CO) broadly deals with finding an optimal object from a finite (or countably infinite) set of feasible candidates. Such problems arise in diverse fields, including operations research, computer science, and engineering, where discrete variables model decisions in practical scenarios such as routing, scheduling, or allocation of limited resources \cite{Papadimitriou82}. 

A foundational tool for combinatorial optimization is \emph{mixed-integer linear programming} (MILP), formulated as:
\begin{equation}
\label{eq:milp}
\begin{aligned}
& \min_{x \in \mathbb{R}^p \times \mathbb{Z}^{n-p}}  c^\top x \\
& \text{subject to} \quad A x \circ b, \quad \ell \leq x \leq u,
\end{aligned}
\end{equation}
where \( x \) is the vector of decision variables, \( c \) is the cost vector, \( A \) is the constraint coefficient matrix, and \( b \) is the vector of constraint bounds. The notation \( A x \circ b \) represents a system of linear constraints, where \( \circ \) denotes relational operators from the set \(\{\leq, \geq, =\}\). The variables \( x \) are partitioned into \( p \) continuous variables and \( n - p \) integer variables. Let \( x^* \) denote an optimal solution to \eqref{eq:milp}, and let \(z^* = c^\top x^*\) be the corresponding optimal objective value. If all decision variables are continuous (\( p = n \)), the problem is a \emph{linear program} (LP).
 MILPs capture many prominent combinatorial problems such as the traveling salesman problem (TSP) \cite{Applegate06}, knapsack problem \cite{Kellerer04}, and network design problems \cite{Johnson78}.

Many fundamental CO problems---including TSP and Knapsack---are known to be NP-hard. A key contribution to the theory of NP-completeness was provided by \citet{Karp10}, who demonstrated that a number of widely studied problems are mutually reducible in polynomial time (often referred to as ``Karp reductions''). These reductions establish deep structural connections among CO problems, showing that if a polynomial-time algorithm exists for one, it can be systematically adapted to solve many others. 


\subsection{Language Models for MILP Modeling}

The use of language models for MILP modeling has sparked considerable interest in the AI-for-OR community. The NL4Opt competition \cite{ramamonjison23} focused on using natural language processing (NLP) methods to formulate optimization problems based on their text descriptions. More recently, with the advent of LLMs, a number of LLM-based \emph{optimization copilots} aim to automate MILP modeling \cite{ mostajabdaveh24, ahmed24,li23,yu24,huang2024large,kadiouglu24, yang2024optibench}. %
Both the Chain-of-Experts \cite{xiao24} and OptiMUS \cite{Ahmaditeshizi24} frameworks designed LLM-based multi-agent systems to automate the modeling of complex optimization problems by leveraging the reasoning capabilities of the LLMs.  
\citet{tang24} further demonstrated the potential of LLMs by fine-tuning open-source models with synthetic data tailored for modeling optimization problems, achieving significant performance improvements over baseline methods. Building on these capabilities, LLM-powered chatbots have been used to allow users to interact with optimization models in a number of contexts including supply chain management \cite{li2023large}, meeting scheduling \cite{lawless2024want}, debugging infeasible models \cite{chen2023diagnosing}, and improving solver configurations \cite{lawless2024llms}.
These advancements highlight why LLMs are particularly suitable for MILP modeling: their ability to process and generate structured information from natural language aligns well with the requirements of optimization problem formulation.  
The rapid development of optimization copilots underscores
the need for reliable, scalable evaluation techniques.



\subsection{Existing Automatic Equivalence Checking Methods}%

The central task of evaluating optimization copilots 
is automatically checking 
whether the generated formulation 
is equivalent to a ground-truth correct one.
The earliest method used in the NL4OPT benchmark \citep{ramamonjison23} for evaluating formulation equivalence is \emph{canonical accuracy}, which looks at direct equivalence between declarations (e.g., objective, constraints) between a reference correct formulation and a generated formulation. 
This method is sensitive to permutations of the order of the declarations in a formulation and fails when multiple valid formulations exist for the same problem. 
The method used in benchmarks such as NLP4LP \citep{Ahmaditeshizi24}, MAMO \cite{huang24}, and IndustryOR \citep{tang24} is \emph{execution accuracy}, which evaluates whether two MILP formulations are equivalent by solving them (using a MILP solver such as Gurobi) and checking if they have the same optimal objective value. 
Execution accuracy is sensitive to variable re-scaling, which can create inconsistencies even when the formulations are functionally equivalent. 
To address these issues, recent approaches utilize Graph Edit Distance \citep{xing24} and a modified Weisfeiler-Lehman (WL) test \cite{wang24} to measure structural similarity between the generated and reference formulations. 
These methods offer insights into equivalence beyond the optimal objective value but have limitations.
They are particularly sensitive to structural modifications, such as adding cutting planes, 
which keeps the formulation equivalent but changes its structural information. 
Beyond these methods, \citet{steever22} proposed an image-based approach to detect structural similarity among large-scale MILPs. 

\section{Methodology}\label{sec:methodology}

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.8\textwidth]{Figures/equivalent_example.png} %
    \caption{A classic stable set problem, where the two formulations correspond to the same problem description. Formulation $\alpha$ uses the standard formulation, while formulation $\alpha'$ rescales the objective function and adds cutting planes based on cliques. LLMs are used to find the mapping function %
    $f$ that maps the variables of $\alpha'$ into the variable space of $\alpha$. 
    }
    \label{fig:example1}
\end{figure*}

This section details the theoretical foundation of formation equivalence 
and the design of our method for leveraging LLMs to automatically check such equivalence---\emph{EquivaMap}. 
In the general setup, we have two formulations $\alpha$ and $\alpha'$ corresponding to the same (feasible) optimization problem $\mathcal{P}$, with optimal objective values $z^*$ and $z'^{*}$ respectively. For example, Figure~\ref{fig:example1} presents two formulations $\alpha$ and $\alpha'$ of an optimization problem $\mathcal{P}$---the \emph{stable set problem}. Our method aims to evaluate the equivalence of $\alpha$ and $\alpha'$ for a given \emph{instantiation} of the problem. In Figure~\ref{fig:example1}, an instantiation of $\mathcal{P}$ would be defined by a specific input graph. %


\subsection{Pitfalls of Existing Equivalence Checking {Methods}}\label{sec:pitfall}


We discuss existing methods for evaluating formulation equivalence, including canonical accuracy, execution accuracy, and the WL-test, 
and exhibit settings where these methods fail.



\textbf{Canonical accuracy} 
is based on matching declarations between predicted and reference programs, where a declaration represents either an optimization objective or a constraint \citep{ramamonjison23}.

\begin{definition}[Canonical Accuracy]
Given a reference declaration $d$ (objective or constraint) and a generated declaration $\hat{d}$, they are said to be matched if $d = \widehat{d}$. Let $\mathcal{D}$ and $\widehat{\mathcal{D}}$ denote the sets of reference and generated declarations, respectively. A False Positive (FP) is a generated declaration $\hat{d}$ that is unmatched, while a False Negative (FN) is a reference declaration $d$ that is unmatched. The canonical accuracy is defined as:
$$
1 - \frac{\min(|\text{FP}| + |\text{FN}|, |\mathcal{D}|)}{|\mathcal{D}|}
$$
where any score under 100\% indicates that the formulations are not equivalent. 
\end{definition}


Canonical accuracy imposes a strong assumption that generated MILPs must adhere to the same variable order as the ground-truth MILP. As illustrated in Figure~\ref{fig:example1}, if the constraints in $\alpha$ are permuted differently from those in $\alpha'$, they are erroneously treated as nonequivalent, despite being fundamentally identical. More broadly, canonical accuracy fails in cases where the two formulations differ based on variable or constraint permutations.

\textbf{Execution accuracy} captures whether two optimization problems have the same optimal objective value \citep{Ahmaditeshizi24}.

\begin{definition}[Execution Accuracy] 
$\alpha$ and $\alpha'$ are considered equivalent if $z^* = z'^*$. %
\end{definition}



Execution accuracy has a clear limitation: it is not robust to rescaling, a common transformation in MILPs that may simply reflect a change in units. For example, in Figure~\ref{fig:example1}, the objective function in $\alpha'$ is rescaled, which would lead execution accuracy to incorrectly classify $\alpha$ and $\alpha'$ as non-equivalent.



Previous studies have shown that MILPs can be represented as bipartite graphs \cite{chen23,chen23_2,khalil17, gasse19}, providing a foundation for defining equivalence using graph-isomorphism based approaches such as the \textbf{WL-test}. %
To construct this bipartite graph, a node is added for each variable and each constraint of the graph. An edge connects a variable node to a constraint node if that variable has a non-zero coefficient in the corresponding constraint. The nodes and edges are endowed with various real-valued attributes describing the MILP (for example, a variable node's attributes will include its coefficient in the objective function). The WL-test tests whether two graphs are isomorphic.%

\begin{definition}[WL-test \citep{douglas11}]
Let \( G = (V, E) \) and \( H = (V', E') \) be two graphs. The Weisfeiler-Lehman test is an iterative label refinement procedure used to determine whether \( G \) and \( H \) are distinguishable.
Initially, each vertex \( v \in V \) is assigned a label \( \ell_0(v) \). At each iteration \( t \), the label of each vertex \( v \) is updated as follows:
\[
\ell_{t+1}(v) = \text{hash} \left( \ell_t(v), \{ \ell_t(u) \mid u \in \mathcal{N}(v) \} \right)
\]
where \( \mathcal{N}(v) \) denotes the set of neighbors of \( v \), and the function \( \text{hash}(\cdot) \) provides a unique encoding neighboring nodes' labels. The process continues iteratively until convergence, meaning no further label changes occur.
To compare graphs, the WL-test computes the multisets of final labels for \( G \) and \( H \). If these multisets differ at any iteration, the graphs are determined to be non-isomorphic, which indicates that they are not equivalent. 
\end{definition}

Modifications of the WL-test were proposed by \citet{wang24} to evaluate formulation equivalence. \citet{xing24} also introduced a related method based on graph-edit distance, which is a softer version of the WL-test. Since graph-based methods evaluate equivalence after transforming formulations $\alpha$ and $\alpha'$ into bipartite graphs, they will treat the two formulations as non-equivalent if structural modifications change the number of variables or constraints. Such modifications are extremely common (and desired) in MILPs, as techniques like adding cutting planes, reformulating constraints, or introducing auxiliary variables are frequently used to improve solver efficiency and tighten linear relaxations. For example, the second formulation in Figure~\ref{fig:example1} includes \emph{clique cutting planes}: \[ \sum_{i \in k} y_i \leq 1 \quad \forall k \in \mathcal{K} \] for cliques $k \in {\cal K}$ in the graph. These cutting planes are well-known to strengthen the linear relaxation of the stable set MILP formulation \citep{IPref}. 



\subsection{MILP Equivalence Based on Karp Reduction}\label{sec:quasi-karp-defn}

Towards a more formal notion of MILP formulation equivalence, we introduce a new definition inspired by a classical tool from complexity theory called a \textit{Karp Reduction}:
\begin{definition}[Karp Reduction]\label{def:karp}
Two decision problems $\mathcal{P}, \mathcal{Q}$ are said to be equivalent if there exists a function $f$ that maps \emph{arbitrary instances} of $\mathcal{P}$ to $\mathcal{Q}$ such that:
\begin{itemize}[left=0pt, topsep=0pt]
    \item If $p$ is a yes-instance of $\mathcal{P}$, then $f(p)$ is a yes-instance of $\mathcal{Q}$,
    \item If $p$ is a no-instance of $\mathcal{P}$, then $f(p)$ is a no-instance of $\mathcal{Q}$, and 
    \item $f$ can be computed in polynomial time.
\end{itemize}
\end{definition}

A Karp reduction can be used to show that two decision problems are equivalent (i.e., a solution to one can be used to find a solution to the other). These reductions hold for arbitrary instances of the two decision problems, but we leverage a similar approach to establish the equivalence between two specific formulations of an MILP problem instance. Consider two optimization problem formulations $\alpha$, $\alpha'$ that correspond to the same optimization problem ${\cal P}$. Our goal is to formally check that an optimal solution to one formulation can be used to generate an optimal solution to the other formulation for a specific instantiation of the problem. Unlike traditional Karp reductions, which define mappings for arbitrary instances, we focus on \emph{instance-specific} mappings. Moreover, our approach maps between solutions of the optimization problem rather than the instance iteself.

We also relax the condition that a no-instance (which, in our setting, corresponds to an infeasible or suboptimal solution) under one formulation needs to be mapped to a no-instance of the other. This distinction is important in settings where a MILP formulation may exclude some, but not all, optimal solutions to improve efficiency. For example, adding symmetry-breaking constraints to an optimization model is a common modeling practice that removes functionally equivalent solutions. With these distinctions in mind, we formalize a new notion of equivalence for MILP formulations which we call \textit{Quasi-Karp Equivalence}:

\begin{definition}[Quasi-Karp Equivalence]\label{def:quasi_karp}
Suppose $\alpha$ and $\alpha'$ are two optimization problems over $\mathbb{R}^d$ and $\mathbb{R}^{d'}$, respectively. We say $\alpha$ and $\alpha'$ are \emph{Quasi-Karp equivalent} if there exists an algorithm $\mathcal{A}(\alpha, \alpha')$ that produces a mapping $f: \mathbb{R}^{d'} \to \mathbb{R}^d$ such that:
\begin{itemize}[left=0pt, topsep=0pt]
    \item If $x^*$ is an optimal solution to $\alpha'$, then $f(x^*)$ is an optimal solution to $\alpha$, 
    \item $f$ can be computed in polynomial time, and
    \item $\mathcal{A}(\alpha, \alpha')$ runs in polynomial time for all $\alpha$, $\alpha'$.
\end{itemize}
\end{definition}

In Figure~\ref{fig:example1}, an example of one such mapping $f$ would be $x_i = y_i, \forall i \in \mathcal{V}$, which is a linear function. 
Intuitively, the notion of \textit{Quasi-Karp Equivalence} is meaningful only when the optimization problem is NP-hard and both optimization formulations admit feasible solutions with finite optimal values. If both formulations are infeasible, then neither has a valid solution, making any comparison between them trivial and uninformative. Declaring two infeasible problems equivalent does not provide any insight into their structural or computational properties. Likewise, if one formulation is infeasible while the other is feasible, then no valid mapping \( f \) can transform an optimal solution of one into the other. Finally, if a formulation is unbounded, then it lacks a finite optimal solution, so no single ``optimal'' point can be mapped from one formulation to another. Thus, we use \textit{Quasi-Karp Equivalence} to check equivalence between feasible, bounded formulations.









\subsection{EquivaMap: Leveraging LLMs for Mapping Finding}\label{sec:equivamap}

\begin{figure*}[t]
    \centering
    \includegraphics[width=1\textwidth]{Figures/workflow.png} %
    \caption{Workflow of \emph{EquivaMap}.
    The method evaluates equivalence between two formulations (\(\alpha\) and \(\alpha'\)) of the same optimization problem instance \(\mathcal{P}\). 
    An LLM generates a mapping function (\(f\)) to map between the variable spaces of \(\alpha\) and \(\alpha'\). 
   The mappings are applied to transform the optimal solution of \(\alpha'\) into a candidate solution in \(\alpha\). 
    A verifier assesses whether the candidate solution is feasible and optimal for \(\alpha\). 
    If the verification succeeds, \(\alpha\) and \(\alpha'\) are deemed equivalent; otherwise, they are classified as not equivalent.}
    \label{fig:workflow}
\end{figure*}




To determine the mapping between $\alpha$ and $\alpha'$, we propose \emph{EquivaMap}, a framework that leverages LLMs as the map-finding algorithm $\mathcal{A}$ from Definition~\ref{def:quasi_karp}. 
Specifically, given two formulations $\alpha$ and $\alpha'$ corresponding to a given instance of the problem $\mathcal{P}$, the algorithm $\mathcal{A}$ returns a mapping function $f$ that aligns their solutions:
$
  f = \mathcal{A}(\alpha, \alpha').
$




In \emph{EquivaMap} (\cref{alg:EquivaMap}), 
first, we use the LLM $\mathcal{A}$ to find the mapping $f$ for the pair of formulations $(\alpha, \alpha')$ using an instance-specific prompt (\cref{sec:expt-details}).
Using a solver $S$, we compute an optimal solution $x'^*$ to $\alpha'$.
With the learned mapping $f$, we obtain a candidate solution $\hat{x} =  f(x^*)$ for $\alpha$. 
We verify whether $\hat{x}$ is an optimal solution of $\alpha$ by substituting $\hat{x}$ into $\alpha$ and verifying that $\hat{x}$ is feasible and optimal (i.e., $c^\top {\hat{x}} = c^\top x^*$).


\begin{algorithm}[H]
\caption{\emph{EquivaMap}}
\label{alg:EquivaMap}
\begin{algorithmic}[] %
    \STATE \textbf{Input:} 
        Two optimization formulations $\alpha, \alpha'$ with objective $\min c^\top x$, $\min c'^\top x'$ respectively. A solver $S$ that finds an optimal solution $x^*$ for $\alpha$ and $x'^*$ for $\alpha'$.
    
    \STATE \textbf{Output:} A Boolean value indicating whether $\alpha$ and $\alpha'$ are quasi-Karp equivalent.

    \STATE // \COMMENT{\textcolor{blue}{Call an LLM with instance-dependent prompt to find a mapping}}
    \STATE $f \leftarrow \mathcal{A}(\alpha, \alpha')$
    
    \STATE // \COMMENT{\textcolor{blue}{Obtain an optimal solution of $\alpha'$ using solver $S$}}
    \STATE $x'^* \leftarrow S(\alpha')$ 
    
    \STATE // \COMMENT{\textcolor{blue}{Map the solution $x'^*$ to a candidate solution in $\alpha$}}
    \STATE $\hat{x} \leftarrow f(x'^*)$ 
    
    \STATE // \COMMENT{\textcolor{blue}{Check if $\hat{x}$ is optimal and feasible for $\alpha$}}
    \IF{$c^\top \hat{x} = c^\top x^*$ and $\hat{x}$ feasible for $\alpha$}
        \STATE \textbf{return} \texttt{True}
    \ELSE 
        \STATE \textbf{return} \texttt{False}
    \ENDIF
\end{algorithmic}
\end{algorithm}
\vspace{-1.25em}

Below we illustrate this procedure using the example in Figure~\ref{fig:example1}. Suppose the optimal solution of $\alpha'$ is $y_i^*$ for all $i \in \mathcal{V}$. Applying the identity mapping function $f$, we compute $\hat{x}_i = f(y_i^*) = y_i^*$ for all $i \in \mathcal{V}$. We confirm $\hat{x}$ is feasible, and substitute $\hat{x}_i$ into the objective function \[ \sum_{i \in \mathcal{V}} x_i,\] to verify whether \[ \sum_{i \in \mathcal{V}} \hat{x}_i = \sum_{i \in \mathcal{V}} x_i^*.\]




\paragraph{Stochasticity in LLMs and Aggregation.}
Since LLMs have stochastic outputs, 
we run \cref{alg:EquivaMap} $K$ times
and then aggregate the outputs %
by declaring $(\alpha, \alpha')$ equivalent if at least one of the $K$ attempts produces a valid mapping.





Comparing \emph{EquivaMap} to Definition~\ref{def:quasi_karp} of Quasi-Karp Equivalence, we note that, under the reasonable assumption that the LLM's inference time is polynomial in the length of its input prompt, \emph{EquivaMap} runs in polynomial time. Moreover, by restricting the mapping function $f$ to be linear, we ensure that it can be computed in polynomial time.

















A key component of \emph{EquivaMap} is 
the instance-specific prompt, which guides the LLM in finding the mapping function $f$.
The prompt includes a structured description of each variable in the formulation (\(\alpha\)), including its textual description, the constraints in which it appears, and whether it appears in the objective function. 
We provide an analogous description of the formulation (\(\alpha'\)).  
The prompt then instructs the LLM to generate a linear mapping for each variable in \(\alpha\), expressed as a list of coefficients and corresponding variable names in \(\alpha'\).
For more details, we defer to \cref{sec:expt-details}.






 
\section{Experiments}\label{sec:experiments}


We conduct a comprehensive evaluation of \emph{EquivaMap}
by introducing \emph{EquivaFormulation}---to the best of our knowledge, the first dataset that contains equivalent formulations of MILP instances. Moreover, the dataset includes details about the transformations used to create these equivalent formulations (\cref{sec:data}). \footnote{Data for the paper can be found at \href{https://huggingface.co/datasets/humainlab/EquivaFormulation}{https://huggingface.co/datasets/humainlab/EquivaFormulation}.}

Next, we evaluate \emph{EquivaMap} on this dataset and compare its performance against established baselines including canonical accuracy, execution accuracy, and the WL-test (\cref{sec:performance}).

\begin{table*}[ht]
\centering
\caption{Overview of the equivalent and nonequivalent transformations between formulations considered in \emph{EquivaFormulation}. \textbf{Transformation Name} describes the type of transformation; \textbf{How It Is Transformed} explains the modification applied to the problem; \textbf{Example (Before/After)} provides a short snippet demonstrating the difference; \textbf{Equivalent?} indicates whether the transformation preserves the original problemâ€™s optimal solutions; and \textbf{Size} shows the number of affected instances, reported as the count of LP and MILP problems.}
\label{tab:dataset-variations}
\begin{adjustbox}{width=\textwidth}
\begin{tabular}{l p{4.5cm} p{6.5cm} c c}
\toprule
\textbf{Transformation Name} 
 & \textbf{How It Is Transformed} 
 & \textbf{Example (Before/After)} 
 & \textbf{Equivalent?} 
 & \textbf{Size} \\
\midrule
\textbf{Substitute Objective Functions} 
& Replace objective function $\min c^\top x$ with an auxiliary variable $z$, adding new constraint $z = c^\top x$ 
& \makecell[l]{\\\textbf{Before:} $\min c^\top x$ \\[2pt]
               \textbf{After:} $\min z,\;\text{s.t. } z = c^\top x$}
& Yes 
& 92LP + 140MILP \\
\midrule
\textbf{Add Slack Variables}
& Transform constraint $g(\mathbf{x}) \le b$ into $g(\mathbf{x}) + s = b,\; s \ge 0$
& \makecell[l]{\\\textbf{Before:} $x + 2y \le 5$ \\[2pt]
               \textbf{After:} $x + 2y + s = 5,\; s \ge 0$}
& Yes 
& 59LP + 134MILP \\
\midrule
\textbf{Replace by Base-10 Representation}
& Express an integer variable $N$ in its decimal expansion
& \makecell[l]{\\\textbf{Before:} $x \le 10^6$ \\[2pt]
               \textbf{After:} $x = \sum_{i=0}^{6} d_i \cdot 10^i,\;
                              0 \le d_i \le 9,\; d_i \in \mathbb{Z}$}
& Yes 
& 44LP + 123MILP \\
\midrule
\textbf{Add Valid Inequalities}
& Include cutting planes or valid linear combinations that do not exclude any integer feasible solution
& \makecell[l]{\\\textbf{Before:} $\{\,x+2y \le 3,\; x \le 1.5\,\}$ \\[2pt]
               \textbf{After:} $\{\,x+2y \le 3,\; x \le 1.5,\; 2x + 2y \le 4.5\,\}$}
& Yes 
& 92LP + 142MILP \\
\midrule
\textbf{Rescaling}
& Change units/scales for variables or objectives (e.g., hours to minutes)
& \makecell[l]{\\\textbf{Before:} $x\;\text{(hours)}$ \\[2pt]
               \textbf{After:} $60x'\;\text{(minutes)}$}
& Yes 
& 60LP + 133MILP \\
\midrule
\textbf{Replace by Linear Combinations}
& Decompose a variable $x$ into $x = x^+ - x^-$ with $x^+, x^- \ge 0$
& \makecell[l]{\\\textbf{Before:} $x$ \\[2pt]
               \textbf{After:} $x^+ - x^-$}
& Yes 
& 77LP + 115MILP \\
\midrule
\textbf{Random Order}
& Substitute the original instance with a completely unrelated, randomly chosen instance
& \makecell[l]{\\\textbf{Before:} $\min z,\;\text{s.t. } z = c^\top x$ \\[2pt]
               \textbf{After:} $\max y,\;\text{s.t. } y = 3$}
& No 
& 87LP + 142MILP \\
\midrule
\textbf{Loose Constraints}
& Delete certain constraints that are tight at the optimum, altering the feasible set
& \makecell[l]{\\\textbf{Before:} $x + 2y \le 3$\; (binding) \\[2pt]
               \textbf{After:} remove $x + 2y \le 3$}
& No 
& 53LP + 120MILP \\
\midrule
\textbf{Feasibility}
& Turn both the original and a randomly chosen instance into feasibility problems (replace objectives with $0$)
& \makecell[l]{\\\textbf{Before:} $\min 0,\;\text{s.t. } z = c^\top x$ \\[2pt]
               \textbf{After:} $\max 0,\;\text{s.t. } y = 3$}
& No 
& 87LP + 142MILP \\
\bottomrule
\end{tabular}
\end{adjustbox}
\end{table*}



\subsection{EquivaFormulation: a dataset of equivalent MILP formulations }\label{sec:data}

We construct \emph{EquivaFormulation} based on the NLP4LP dataset \cite{Ahmaditeshizi24}. 
NLP4LP comprises a diverse set of 
optimization problems with 
distinct problem sizes, objective functions, and constraints. 
Each instance in NLP4LP is composed of three components: 
(1) A description file with a high-level description of
the problem in natural language. 
(2) An information file which contains the 
corresponding mathematical formulation of the optimization instance, written in LaTeX.  
(3) A file that contains the GurobiPy code corresponding to the mathematical formulation.

As discussed in \cref{sec:quasi-karp-defn}, 
we carefully select optimization problems in the {NLP4LP} dataset by 
removing the infeasible and unbounded instances. 
In \emph{EquivaFormulation}, we introduce seven (quasi-Karp) equivalent transformations
and three non-equivalent transformations 
to transform the formulations in NLP4LP to corresponding (quasi-Karp) equivalent and nonequivalent counterparts, respectively (\cref{tab:dataset-variations}).
Our proposed equivalent transformations capture widely used and important modeling techniques in MILPs. Standard practices such as substituting the objective function, adding slack variables, and decomposing variables into positive and negative components help simplify constraints and enforce non-negativity. 
Additionally, robustness to rescaling is crucial, as quantities can be represented in different units (e.g., 
1 $kg$ rather than 1000 $g$). Similarly, certain structural transformations---such as adding valid inequalities, reformulating constraints, or introducing auxiliary variables---are commonly employed to enhance solver efficiency and tighten relaxations while preserving the formulation's optimal solution.
Finally, we incorporate non-equivalent transformations to evaluate the susceptibility of equivalence-checking methods to false positives.
These selected transformations in \emph{EquivaMap} are designed to test the robustness of different equivalence-checking methods in handling diverse MILP formulations. 








\begin{table*}[!ht]
\captionsetup{
    font=small, %
    labelfont=bf, %
    justification=centering, %
    skip=7pt %
}
\caption{Accuracy of equivalence-checking methods on formulations obtained from equivalent and non-equivalent transformations.}
\label{tab:variation_highlighted}
\small
\centering
\begin{adjustbox}{width=\textwidth}
\begin{tabular}{lccccHc}
\toprule
& 
\textbf{Canonical Acc.} &
\textbf{Execution Acc.} & \textbf{WL-test} & 
\textbf{naive-LLM} &
\textbf{EquivaMap (Maj.)} & \textbf{EquivaMap} \\ 
\midrule
\multicolumn{7}{l}{\textbf{Equivalent Transformations}} \\
\midrule
\rowcolor[gray]{0.9} %
\textbf{Worst Case} & 0\% & 0\% & 0\% & 3.3\% & 99.3\% & \textbf{100\%} \\ 
Substitute Objective Functions & 0\% & \textbf{100\%} & 0\% & 91.2\% & \textbf{100\%} & \textbf{100\%} \\ 
Add Slack Variables & 0\% & \textbf{100\%} & 0\% & 36.1\% & \textbf{100\%} & \textbf{100\%} \\
Replace by Base-10 Representation & 0\% & \textbf{100\%} & 0\% & 53.1\% & \textbf{100\%} & \textbf{100\%} \\
Add Valid Inequalities & 0\% & \textbf{100\%} & 0\% & 3.3\% & \textbf{100\%} & \textbf{100\%} \\
Rescaling & 0\% & 0\% & 0\% & 69.9\% & \textbf{100\%} & \textbf{100\%} \\
Replace by Linear Combinations & 0\% & \textbf{100\%} & 0\% & 24.4\% & 99.3\% & \textbf{100\%} \\
\midrule
\multicolumn{7}{l}{\textbf{Non-Equivalent Transformations}} \\
\midrule
\rowcolor[gray]{0.9} %
\textbf{Worst Case} & \textbf{100\%} & 0\% & \textbf{100\%} & 93.6\% & \textbf{100\%} & \textbf{100\%} \\ 
Random Order & \textbf{100\%} & \textbf{100\%} & 
\textbf{100\%} & 98.7\% & \textbf{100\%} & \textbf{100\%} \\
Loose Constraints & \textbf{100\%} & \textbf{100\%} & \textbf{100\%} & 93.6\% & \textbf{100\%} & \textbf{100\%} \\
Feasibility & \textbf{100\%} & 0\% & \textbf{100\%} & \textbf{100\%} & \textbf{100\%} & \textbf{100\%} \\
\bottomrule
\end{tabular}
\end{adjustbox}
\end{table*}

\paragraph{Additional details.}
Since the selected transformations in \emph{EquivaFormulation} are deterministic, 
to prevent the mapping finder or other equivalence-matching methods from exploiting shortcutsâ€”such as mapping decision variables based on their order (e.g., if variable names are assigned alphabetically)â€”we apply several transformations to the formulation before processing. Specifically, we randomly permute the order of problem parameters, decision variables, and constraints in the information file. Additionally, we assign distinct names to all decision variables and use GPT-4o to generate varied natural language descriptions for them. These transformations are implemented to reduce the similarity between formulation $\alpha$ and $\alpha'$, ensuring that LLMs cannot exploit recognizable transformation patterns to deduce the mapping directly. 







\subsection{Performance}\label{sec:performance}


We use GPT-4 \cite{achiam2023gpt} as the mapping finder in \emph{EquivaMap}, and evaluate our method against existing baselines, plus a naive LLM baseline (naive-LLM). The naive-LLM baseline uses a prompt that includes two formulations $\alpha$ and $\alpha'$ and directly checks if they are equivalent. The prompt can be found in \cref{sec:expt-details}. 
We set $K=3$, and report the accuracy as the percentage of paired formulations $\alpha$ and $\alpha'$ that are correctly identified as equivalent or nonequivalent, and summarize the results in Table~\ref{tab:variation_highlighted}.

The results demonstrate that our method consistently outperforms all baseline approaches, achieving perfect or near-perfect accuracy in almost every scenario. Notably, \emph{EquivaMap} performs perfectly even in cases where execution accuracy completely fails, highlighting its reliability. %


For the equivalent transformations (Table~\ref{tab:variation_part1}), our method performs exceptionally well under challenging transformations, such as \textit{Add Valid Inequalities}, \textit{Rescaling}, and \textit{Replace by Linear Combinations}. Execution accuracy and the WL-test fail universally in these settings, achieving 0\% accuracy across all variations. In contrast, \textit{EquivaMap} achieves 100\% accuracy.
These transformations are critical test cases because they highlight the fundamental weaknesses of existing approaches: they struggle with capturing key modeling techniques such as cutting planes and variable rescaling. 
For example, execution accuracy fails at \textit{Rescaling}, since a naive solver run does not recognize scaled problem instances as equivalent, while WL-test fails at \textit{Add Valid Inequalities} or \textit{Replace by Linear Combinations} since these transformations break isomorphisms in the graph representation. 
Our method reliably handles them, achieving near-perfect accuracy. 


The results for non-equivalent variations (Table~\ref{tab:variation_part2_reversed}) further highlight the reliability of our method. For the \textit{Feasibility} transformation, execution accuracy fails with 0\% accuracy, yet our method achieves perfect accuracy for all instances. Under other non-equivalent settings, our method also works well, achieving 100\% accuracy. %

\paragraph{Key observations.}
\emph{EquivaMap} outperforms all existing equivalence-checking methods, including the naive-LLM baseline.
This highlights the necessity of our algorithm---without the explicit map finding and optimality verification steps,
naively using LLMs with strong reasoning capabilities will not ensure reliability in checking formulation equivalence.
Moreover, 
these results demonstrate that our method is \emph{reliable} across diverse transformations. It consistently outperforms all baselines, especially in scenarios where execution accuracy and other methods fail. %


\section{Discussion}



How can we define the equivalence of two formulations of the same optimization problem instance? In this paper, we address this conceptual gap by proposing quasi-Karp equivalence
and a framework, \emph{EquivaMap}, to systematically check such equivalence. 
Through extensive experiments on MILP problems, we demonstrate that \emph{EquivaMap}  outperforms existing approaches by large. Additionally, by introducing the first well-documented pool of equivalent optimization formulations, encompassing diverse transformations such as the addition of cutting planes, we provide a valuable dataset for advancing research in this domain. 

Beyond examining whether two formulations represent the same problem instance, a promising direction for future research is to leverage \emph{EquivaMap} to check equivalences across diverse optimization problems, 
such as the equivalence between max-flow and min-cut problems. 
It is also worth noting that 
our current work focuses on relatively straightforward transformations, thereby omitting more intricate reformulations such as those emerging from decomposition algorithms. 
As more complex optimization copilots are developed, 
we may need equivalence-checking tools that can tackle these more complex transformations.%
















\bibliographystyle{plainnat}
\bibliography{main}  

\newpage
\appendix
\onecolumn

\section{Additional Experimental Details and Results}\label{sec:expt-details}


\subsection{Supplementary Tables}
In this section we segment our main results by problem class (i.e., LP vs. MILP), and equivalence (i.e., equivalent vs. nonequivalent). We also include the fraction of each instance solved correctly for each problem type. Our results are consistent across both LP and MILP instances, highlight that \emph{EquivaMap} outperforms all baseline methods in all settings.

\begin{table*}[ht]
\captionsetup{
    font=small, %
    labelfont=bf, %
    justification=centering, %
    skip=7pt %
}
\caption{Accuracy of equivalence-checking methods on formulations obtained from \emph{equivalent} transformations. Rows partitioned by whether the problems are linear programming problems  (LP) or mixed-integer linear programing problems (MILP). Numbers in parenthesis correspond to raw fraction of instances solved correctly.}
\label{tab:variation_part1}
\small
\begin{adjustbox}{width=\textwidth}
\centering
\begin{tabular}{lccccHc}
\toprule
\textbf{Transformation} & 
\textbf{Canonical Acc.} &
\textbf{Execution Acc.} & \textbf{WL-test} & 
\textbf{naive-LLM} &
\textbf{MapEquiv (Maj.)} & \textbf{EquivaMap} \\
\midrule
\textbf{LP} \\ \midrule
\textbf{Substitute Objective Functions} & 0\%(0/92) & 100\%(92/92) &0\%(0/92) &94.6\%(87/92) &100\%(92/92) &100\%(92/92)\\
\textbf{Add Slack Variables} & 0\%(0/59) &100\%(59/59) &0\%(0/59) &49.1\%(29/59) &100\%(59/59) &100\%(59/59)\\
\textbf{Replace by Base-10 Representation} & 0\%(0/44) &100\%(44/44) &0\%(0/44) &50\%(22/44)&100\%(44/44) &100\%(44/44)\\
\textbf{Add Valid Inequalities} & 0\%(0/92) &100\%(92/92) &0\%(0/92) &6.5\%(6/92)& 100\%(92/92) &100\%(92/92)\\
\textbf{Rescaling} & 0\%(0/60) &0\%(0/60) &0\%(0/60) &76.7\%(46/60) &100\%(60/60) &100\%(60/60)\\
\textbf{Replace by Linear Combinations} & 0\%(0/77) &100\%(77/77) &0\%(0/77) &13.0\%(10/77) &98.7\%(76/77) &100\%(77/77)\\
\midrule
\textbf{MILP} \\ \midrule
\textbf{Substitute Objective Functions} & 0\%(0/140) &100\%(140/140) &0\%(0/140) &87.9\%(123/140) &100\%(140/140) &100\%(140/140)\\
\textbf{Add Slack Variables} & 0\%(0/134) &100\%(134/134) &0\%(0/134) &23.1\%(31/134) & 100\%(134/134) &100\%(134/134)\\
\textbf{Replace by Base-10 Representation} & 0\%(0/123) &100\%(123/123) &0\%(0/123) &56.1\%(69/123) &100\%(123/123) &100\%(123/123)\\
\textbf{Add Valid Inequalities} & 0\%(0/142) &100\%(142/142) &0\%(0/142) &0\%(0/142) &100\%(142/142) &100\%(142/142)\\
\textbf{Rescaling} & 0\%(0/133) &0\%(0/133) &0\%(0/133) &63.2\%(84/133) &100\%(133/133) &100\%(133/133)\\
\textbf{Replace by Linear Combinations} & 0\%(0/115) &100\%(115/115) &0\%(0/115) &35.7\%(41/115) &100\%(115/115) &100\%(115/115)\\
\bottomrule
\end{tabular}
\end{adjustbox}
\end{table*}



\begin{table*}[ht]
\captionsetup{
    font=small, %
    labelfont=bf, %
    justification=centering, %
    skip=7pt %
}
\caption{Accuracy of equivalence-checking methods on formulations obtained from \emph{nonequivalent} transformations. Rows partitioned by whether the problems are linear programming problems (LP) or mixed-integer linear programing problems (MILP). Numbers in parenthesis correspond to raw fraction of instances solved correctly.}
\label{tab:variation_part2_reversed}
\small
\begin{adjustbox}{width=\textwidth}
\centering
\begin{tabular}{lccccHc}
\toprule
\textbf{Transformation} & 
\textbf{Canonical Acc.} &
\textbf{Execution Acc.} & \textbf{WL-test} & 
\textbf{naive-LLM} &
\textbf{MapEquiv (Maj.)} & \textbf{EquivaMap} \\
\midrule
\textbf{LP} \\ \midrule
\textbf{Random Order} &100\%(87/87) &100\%(87/87) &100\%(87/87) &98.9\%(86/87) &100\%(87/87) &100\%(87/87)\\
\textbf{Loose Constraints} &100\%(53/53) &100\%(53/53) &100\%(53/53) &88.7\%(47/53) &100\%(53/53) &100\%(53/53)\\
\textbf{Feasibility} &100\%(87/87) &0\%(0/87) &100\%(87/87) &100\%(87/87) &100\%(87/87) &100\%(87/87)\\
\midrule
\textbf{MILP} \\ \midrule
\textbf{Random Order} &100\%(142/142) &100\%(142/142) &100\%(142/142) &98.6\%(140/142) &100\%(142/142) &100\%(142/142)\\
\textbf{Loose Constraints} &100\%(120/120) &100\%(120/120) &100\%(120/120) &96.7\%(116/120) &100\%(120/120) &100\%(120/120)\\
\textbf{Feasibility} &100\%(142/142) &0\%(0/142) &100\%(142/142) &100\%(142/142) &100\%(142/142)&100\%(142/142) \\
\bottomrule
\end{tabular}
\end{adjustbox}
\end{table*}

\subsection{Prompts}

\subsubsection{naive-LLM Prompt}

\lstset{
    showspaces=false,
    showstringspaces=false,
    basicstyle=\ttfamily\small,
    keywordstyle=\color{black},
    commentstyle=\color{black},
    stringstyle=\color{black},
    breaklines=true,
    frame=single,
}

\begin{lstlisting}[language=Python, caption=naive-LLM Prompt]
You are given two optimization problem formulations (both declared as MIP).
Decide if they are equivalent formulations.

First problem formulation (Problem A):
{
  "parametrized_description": "A laundromat can buy two types of washing machines, a top-loading model and a front-loading model. The top-loading model can wash WashRateTopLoading items per day while the front-loading model can wash WashRateFrontLoading items per day. The top-loading model consumes EnergyConsumptionTopLoading kWh per day while the front-loading model consumes EnergyConsumptionFrontLoading kWh per day. The laundromat must be able to wash at least MinItemsPerDay items per day and has available MaxEnergyPerDay kWh per day. Since the top-loading machines are harder to use, at most MaxFractionTopLoading of the machines can be top-loading. Further, at least MinNumFrontLoading machines should be front-loading. How many of each machine should the laundromat buy to minimize the total number of washing machines?",
  "keywords": [
    "N.A."
  ],
  "parameters": {
    "WashRateTopLoading": {
      "description": "Number of items washed per day by a top-loading machine",
      "shape": []
    },
    "WashRateFrontLoading": {
      "description": "Number of items washed per day by a front-loading machine",
      "shape": []
    },
    "EnergyConsumptionTopLoading": {
      "description": "Energy consumed per day by a top-loading machine (kWh)",
      "shape": []
    },
    "EnergyConsumptionFrontLoading": {
      "description": "Energy consumed per day by a front-loading machine (kWh)",
      "shape": []
    },
    "MinItemsPerDay": {
      "description": "Minimum number of items to wash per day",
      "shape": []
    },
    "MaxEnergyPerDay": {
      "description": "Maximum available energy per day (kWh)",
      "shape": []
    },
    "MaxFractionTopLoading": {
      "description": "Maximum fraction of machines that can be top-loading",
      "shape": []
    },
    "MinNumFrontLoading": {
      "description": "Minimum number of front-loading machines",
      "shape": []
    }
  },
  "variables": {
    "NumTopLoading": {
      "description": "The number of top-loading machines",
      "type": "continuous",
      "shape": []
    },
    "NumFrontLoading": {
      "description": "The number of front-loading machines",
      "type": "continuous",
      "shape": []
    }
  },
  "constraints": [
    {
      "description": "A top-loading machine washes WashRateTopLoading items per day and a front-loading machine washes WashRateFrontLoading items per day. The total number of items washed per day must be at least MinItemsPerDay.",
      "formulation": "WashRateTopLoading \\cdot NumTopLoading + WashRateFrontLoading \\cdot NumFrontLoading \\geq MinItemsPerDay",
      "code": {
        "gurobipy": "model.addConstr(WashRateTopLoading * NumTopLoading + WashRateFrontLoading * NumFrontLoading >= MinItemsPerDay)"
      }
    },
    {
      "description": "A top-loading machine consumes EnergyConsumptionTopLoading kWh per day and a front-loading machine consumes EnergyConsumptionFrontLoading kWh per day. The total energy consumption per day cannot exceed MaxEnergyPerDay kWh.",
      "formulation": "NumTopLoading \\times EnergyConsumptionTopLoading + NumFrontLoading \\times EnergyConsumptionFrontLoading \\leq MaxEnergyPerDay",
      "code": {
        "gurobipy": "model.addConstr(EnergyConsumptionTopLoading * NumTopLoading + EnergyConsumptionFrontLoading * NumFrontLoading <= MaxEnergyPerDay)"
      }
    },
    {
      "description": "At most MaxFractionTopLoading fraction of the total machines can be top-loading.",
      "formulation": "NumTopLoading \\leq MaxFractionTopLoading \\times (NumTopLoading + NumFrontLoading)",
      "code": {
        "gurobipy": "model.addConstr(NumTopLoading <= MaxFractionTopLoading * (NumTopLoading + NumFrontLoading))"
      }
    },
    {
      "description": "At least MinNumFrontLoading machines must be front-loading.",
      "formulation": "NumFrontLoading \\geq MinNumFrontLoading",
      "code": {
        "gurobipy": "model.addConstr(NumFrontLoading >= MinNumFrontLoading)"
      }
    }
  ],
  "objective": {
    "description": "Minimize the total number of washing machines purchased.",
    "formulation": "Min \\ NumTopLoading + NumFrontLoading",
    "code": {
      "gurobipy": "model.setObjective(NumTopLoading + NumFrontLoading, GRB.MINIMIZE)"
    }
  }
}

Second problem formulation (Problem B):
{
  "parametrized_description": "A laundromat can buy two types of washing machines, a top-loading model and a front-loading model. The top-loading model can wash V items per day while the front-loading model can wash T items per day. The top-loading model consumes F kWh per day while the front-loading model consumes A kWh per day. The laundromat must be able to wash at least J items per day and has available R kWh per day. Since the top-loading machines are harder to use, at most S of the machines can be top-loading. Further, at least W machines should be front-loading. How many of each machine should the laundromat buy to minimize the total number of washing machines?",
  "keywords": [
    "N.A."
  ],
  "parameters": {
    "W": {
      "description": "The smallest quantity of front-loading machines.",
      "shape": []
    },
    "A": {
      "description": "Daily electricity usage of a front-loading washer (kWh)",
      "shape": []
    },
    "R": {
      "description": "The highest amount of energy that can be obtained in a single day (kWh).",
      "shape": []
    },
    "S": {
      "description": "The highest percentage of machines that can have a top-loading feature.",
      "shape": []
    },
    "F": {
      "description": "Daily energy usage of a top-loading washing machine in kilowatt-hours",
      "shape": []
    },
    "J": {
      "description": "The smallest quantity of items that need to be cleaned on a daily basis",
      "shape": []
    },
    "V": {
      "description": "Quantity of objects cleaned daily using a top-loading washer",
      "shape": []
    },
    "T": {
      "description": "The quantity of objects cleaned daily with a front-loading washing machine.",
      "shape": []
    }
  },
  "variables": {
    "a": {
      "description": "The quantity of top-loading appliances",
      "type": "continuous",
      "shape": []
    },
    "g": {
      "description": "The quantity of front-loading machines",
      "type": "continuous",
      "shape": []
    }
  },
  "constraints": [
    {
      "description": "A top-loading washer cleans V items daily, while a front-loading washer cleans T items daily. The combined total of items cleaned each day should not fall below J.",
      "formulation": "J \\leq V \\cdot a + T \\cdot g",
      "code": {
        "gurobipy": "model.addConstr(V * a + T * g >= J)"
      }
    }
  ],
  "objective": {
    "description": "Reduce the overall quantity of washing machines bought.",
    "formulation": "Min \\ g + a",
    "code": {
      "gurobipy": "model.setObjective(a + g, GRB.MINIMIZE)"
    }
  }
}

Based on the data, please respond with exactly one of the following:
- "Equivalent" if these two are the same formulation. Be rigorous in your reasoning. 
- "Not Equivalent" if they are different. When you are not sure, say "Not Equivalent".

Briefly explain the reasoning in 1-2 sentences, then end with the word "Equivalent" or "Not Equivalent" on its own line.
\end{lstlisting}

\subsubsection{\emph{EquivaMap} Prompt}


\begin{lstlisting}[language=Python, caption=\emph{EquivaMap} Prompt]
You are an AI language model assisting in mapping variables between two optimization problems by analyzing their roles in constraints and the objective function.

**Variable from Problem 1:**
- **Name:** OdorRemovingChemicalUnits
- **Description:** The number of units of odor-removing chemical used per house
- **Constraints involving OdorRemovingChemicalUnits:**
  - Description: The total number of chemical units used per house cannot exceed MaxTotalUnits.
    Formulation: CleansingChemicalUnits + OdorRemovingChemicalUnits \leq MaxTotalUnits
  - Description: The number of cleansing chemical units used cannot exceed MaxCleansingToOdorRatio times the number of odor-removing chemical units used.
    Formulation: CleansingChemicalUnits \leq MaxCleansingToOdorRatio \cdot OdorRemovingChemicalUnits
- **In Objective Function:** Yes

**Variables from Problem 2:**
- **Name:** v_0
  **Description:** Digit 0 of the The quantity of cleaning solution units utilized per household
  **Constraints involving v_0:**
    - Description: The quantity of cleansing chemical units applied must not surpass H times the quantity of odor-removing chemical units used.
      Formulation: H \cdot (f_0*10^0 + f_1*10^1) \geq (v_0*10^0 + v_1*10^1 + v_2*10^2)
    - Description: The cumulative quantity of chemical components utilized for each residence must not surpass T.
      Formulation: T \geq (f_0*10^0 + f_1*10^1) + (v_0*10^0 + v_1*10^1 + v_2*10^2)
    - Description: The company is required to utilize a minimum of G units of the cleaning solution per household.
      Formulation: G \leq (v_0*10^0 + v_1*10^1 + v_2*10^2)
  **In Objective Function:** Yes

- **Name:** v_1
  **Description:** Digit 1 of the The quantity of cleaning solution units utilized per household
  **Constraints involving v_1:**
    - Description: The quantity of cleansing chemical units applied must not surpass H times the quantity of odor-removing chemical units used.
      Formulation: H \cdot (f_0*10^0 + f_1*10^1) \geq (v_0*10^0 + v_1*10^1 + v_2*10^2)
    - Description: The cumulative quantity of chemical components utilized for each residence must not surpass T.
      Formulation: T \geq (f_0*10^0 + f_1*10^1) + (v_0*10^0 + v_1*10^1 + v_2*10^2)
    - Description: The company is required to utilize a minimum of G units of the cleaning solution per household.
      Formulation: G \leq (v_0*10^0 + v_1*10^1 + v_2*10^2)
  **In Objective Function:** Yes

- **Name:** v_2
  **Description:** Digit 2 of the The quantity of cleaning solution units utilized per household
  **Constraints involving v_2:**
    - Description: The quantity of cleansing chemical units applied must not surpass H times the quantity of odor-removing chemical units used.
      Formulation: H \cdot (f_0*10^0 + f_1*10^1) \geq (v_0*10^0 + v_1*10^1 + v_2*10^2)
    - Description: The cumulative quantity of chemical components utilized for each residence must not surpass T.
      Formulation: T \geq (f_0*10^0 + f_1*10^1) + (v_0*10^0 + v_1*10^1 + v_2*10^2)
    - Description: The company is required to utilize a minimum of G units of the cleaning solution per household.
      Formulation: G \leq (v_0*10^0 + v_1*10^1 + v_2*10^2)
  **In Objective Function:** Yes

- **Name:** f_0
  **Description:** Digit 0 of the The quantity of odor-neutralizing chemical applied in each household
  **Constraints involving f_0:**
    - Description: The quantity of cleansing chemical units applied must not surpass H times the quantity of odor-removing chemical units used.
      Formulation: H \cdot (f_0*10^0 + f_1*10^1) \geq (v_0*10^0 + v_1*10^1 + v_2*10^2)
    - Description: The cumulative quantity of chemical components utilized for each residence must not surpass T.
      Formulation: T \geq (f_0*10^0 + f_1*10^1) + (v_0*10^0 + v_1*10^1 + v_2*10^2)
  **In Objective Function:** Yes

- **Name:** f_1
  **Description:** Digit 1 of the The quantity of odor-neutralizing chemical applied in each household
  **Constraints involving f_1:**
    - Description: The quantity of cleansing chemical units applied must not surpass H times the quantity of odor-removing chemical units used.
      Formulation: H \cdot (f_0*10^0 + f_1*10^1) \geq (v_0*10^0 + v_1*10^1 + v_2*10^2)
    - Description: The cumulative quantity of chemical components utilized for each residence must not surpass T.
      Formulation: T \geq (f_0*10^0 + f_1*10^1) + (v_0*10^0 + v_1*10^1 + v_2*10^2)
  **In Objective Function:** Yes


Based on the above information, find the best mapping from variables in Problem 2 for the variable 'OdorRemovingChemicalUnits' from Problem 1. The mapping can be a linear combination of variables from Problem 2, possibly with constant multipliers. Your goal is to express 'OdorRemovingChemicalUnits' in terms of variables from Problem 2, as accurately as possible, based on their roles in the constraints and objective functions.

**Important Instructions:**

- **Provide only the mapping for 'OdorRemovingChemicalUnits' as a JSON object.**
- **Do not include any additional text, explanations, or formatting.**
- **The JSON object must follow this exact structure:**

{
  "OdorRemovingChemicalUnits": [
    {
      "constant": constant_value_1,
      "variable": "variable_name_1"
    },
    {
      "constant": constant_value_2,
      "variable": "variable_name_2"
    },
    ...
  ]
}

- **If there is only one term in the mapping, the list should contain a single object.**
- **Use numerical values for constants (decimals), and enclose variable names in double quotes ("").**

**Examples:**

1. If the best mapping is '0.1*a', your response should be:

{
  "OdorRemovingChemicalUnits": [
    {
      "constant": 0.1,
      "variable": "a"
    }
  ]
}

2. If the best mapping is '0.1*a + 0.01*b', your response should be:

{
  "OdorRemovingChemicalUnits": [
    {
      "constant": 0.1,
      "variable": "a"
    },
    {
      "constant": 0.01,
      "variable": "b"
    }
  ]
}

3. If the best mapping is a single variable 'a' with a coefficient of 1, your response should be:

{
  "OdorRemovingChemicalUnits": [
    {
      "constant": 1,
      "variable": "a"
    }
  ]
}

4. If there is no direct mapping, your response should be:

{
  "OdorRemovingChemicalUnits": [
    {
      "constant": "none",
      "variable": "none"
    }
  ]
}

Please ensure your response is a valid JSON object that can be parsed by standard JSON parsers. 
\end{lstlisting}


\end{document}
