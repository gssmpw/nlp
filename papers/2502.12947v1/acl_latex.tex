% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[preprint]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.


% Packages We Use
\usepackage{algorithm}
\usepackage{algorithmicx, algpseudocode}
%\usepackage{subfig}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{bm}
\usepackage{multirow}
\algrenewcommand\algorithmicrequire{\textbf{Input:}}
\algrenewcommand\algorithmicensure{\textbf{Output:}}
\allowdisplaybreaks



%Not All Experts are Equal: Efficient Expert Pruning and Skipping for Mixture-of-Experts Large Language Models <- 이거랑 비슷한 구조인데 반대되는 느낌으로..??
% Every Answer Matters: Evaluating Commonsense with Probabilistic Measures <- 이거랑 제목이 너무 비슷한 느낌이 드나..???
\title{Every Expert Matters: Towards Effective Knowledge Distillation for Mixture-of-Experts Language Models}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author{Gyeongman Kim\textnormal{\textsuperscript{1}}$^*$ \quad Gyouk Chu\textnormal{\textsuperscript{1}}$^*$ \quad Eunho Yang\textnormal{\textsuperscript{1,2}} \vspace{0.03in}\\
  \textsuperscript{1}Korea Advanced Institute of Science and Technology (KAIST), South Korea \\ \textsuperscript{2}AITRICS, South Korea\\
  \texttt{\{gmkim, kyouwook, eunhoy\}@kaist.ac.kr}
}
% \author{First Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\\And
%   Second Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\}

%\author{
%  \textbf{First Author\textsuperscript{1}},
%  \textbf{Second Author\textsuperscript{1,2}},
%  \textbf{Third T. Author\textsuperscript{1}},
%  \textbf{Fourth Author\textsuperscript{1}},
%\\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
%\\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
%\\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
%\\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
%\\
%\\
%  \textsuperscript{1}Affiliation 1,
%  \textsuperscript{2}Affiliation 2,
%  \textsuperscript{3}Affiliation 3,
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
%\\
%  \small{
%    \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
%  }
%}

\begin{document}
\maketitle
\def\thefootnote{*}\footnotetext{\,Equal Contribution.}\def\thefootnote{\arabic{footnote}}
\begin{abstract}
% 최근 MoE의 도입으로 LLM의 성능이 증가함과 동시에 크기도 증가하였다.
%With the recent development of Mixture of Experts (MoE), the performance of language models has remarkably enhanced; however, it has also led to a substantial increase in their size.
% KD는 LLM을 압축하는 방법으로 주로 쓰이는 방법이지만, 이런 MoE를 효과적으로 압축하는 방법에 대한 연구는 부족하다.
%To mitigate this growth, knowledge distillation (KD) is a widely considered approach.
%However, research on KD to effectively compress MoE-based models remains underexplored.
With the emergence of Mixture-of-Experts (MoE), the efficient scaling of model size has accelerated the development of large language models in recent years.
However, their high memory requirements prevent their use in resource-constrained environments. While knowledge distillation (KD) has been a proven method for model compression, its application to MoE teacher models remains underexplored.
% 우리는 현재 KD들이 MoE를 distill하는데에는 최적화되어있지 않다는 것을 demonstrate했다.
%In light of this, we demonstrate that existing KD methods are not well-optimized for distilling MoE models.
% 이런 문제의 원인으로 MoE의 본질적인 특징인 top-k selection으로 인해 사용되지 않는 experts들을 꼽았다.
%We identify the primary reason for this issue as the Top-$k$ selection mechanism, an inherent characteristic of MoE, as knowledge cannot be leveraged from unutilized experts.
Through our investigation, we discover that non-activated experts in MoE models possess valuable knowledge that benefits student models. We further demonstrate that existing KD methods are not optimal for compressing MoE models, as they fail to leverage this knowledge effectively.
% To tackle this, 우리는 for the first time, MoE에 특화된 KD 방법을 2가지 제안한다.
%To address this, we propose, for the first time, two KD methods particularly designed for MoE: knowledge augmentation (KA) and student-aware router (SAR).
% Specifically, (1) we augment the knowledge of teacher by randomly sampling experts (2) we train the weight of each expert to be conducive to student understanding.
To address this, we propose two intuitive MoE-specific KD methods for the first time: Knowledge Augmentation (KA) and Student-Aware Router (SAR), both designed to effectively extract knowledge from all experts.
%Specifically, (1) for KA, we augment the knowledge of teachers by randomly sampling experts. (2) For SAR, we train the weight of each expert to be conducive to student understanding by training the router of MoE before KD.
Specifically, KA augments knowledge by sampling experts multiple times, while SAR uses all experts and adjusts the expert weights through router training to provide optimal knowledge.
% 실험 결과는 우리의 방식이 MoE를 distilling하는데 기존 방식 대비 매우 효과적임을 보여주고 있다.
%Extensive experimental results show that our approach outperforms conventional KD methods in distilling MoE-based models.
Extensive experiments show that our methods outperform conventional KD methods, demonstrating their effectiveness for MoE teacher models.



\end{abstract}

\section{Introduction}

% 1번 문단 - MoE는 dense 모델에 비해 resource requirement가 높다. MoE to "dense" KD 필요성 얘기

Mixture-of-Experts (MoE) architecture~\citep{jacobs1991adaptive, shazeer2017outrageously} is one of the major contributing factors to the rapid advancements of Large Language Models (LLMs)~\citep{jiang2024mixtral, team2024qwen1, liu2024deepseek}.
% 1. 최근 LLM의 급격한 발전 속에서, MoE 아키텍쳐는 한 단계 더 step up하게 된 이유 중 하나다.
It allows the model to scale up while effectively improving the computational cost by utilizing only a subset of multiple experts during inference.
%This efficiency is beneficial for resource-intensive applications.
%Even so, the practical application of MoE models in resource-constrained environments is generally challenging. Therefore, research into effectively compressing high-performing MoE models becomes imperative.
%This efficiency is beneficial for resource-intensive applications.
% 2. 그러나, MoE 모델은 (파라미터 로딩 때문에) 많은 memory를 필요로 하기에, limited resource에서 이를 이용하기 위해서는 model compression을 거의 필수로 한다.
Despite the advantages afforded by MoE architectures in scaling model capacity, several limitations persist. One such challenge is that it requires significant GPU memory compared to the dense model due to a number of non-active parameters.
For this reason, the practical application of MoE models in resource-limited environments is generally challenging.
Hence, research into effectively compressing recent large-scale MoE models becomes imperative, particularly for deployment in resource-constrained environments. %d%

% 2번 문단 & 3번 문단 - MoE에 특화된 KD는 없다. 특화된 KD라는 차별점 얘기
% 2번 문단에서 "MoE가 성공적이었다" 얘기 하고, 따로 문단 구분 해서 "MoE에 특화된 KD는 없다" 이렇게 얘기 할 것. 그래야 주장이 문단 처음에 와서 잘 보임!

% 1. 이러한 방법 중 하나는 작은 모델로 지식을 전달하는 KD가 있다. Hinton KD부터 이어져서 최근에는 generative model에 대한 KD 관련 많은 연구가 이어지고 있다.
One of the notable compression techniques is knowledge distillation (KD)~\citep{hinton2015distilling}.
To facilitate student learning under teacher guidance, both the approach of using the teacher's output as supervised data~\citep{kim2016sequence, peng2023instruction, fu2023specializing} and the method to match the teacher's distribution with appropriate objective functions are widely adopted and actively researched.
Specifically, concerning the second method, many works have focused on designing suitable objective functions~\citep{wen2023f, ko2024distillm, agarwal2024generalized, wu2024rethinking} or on using student-generated output~\citep{lin2020autoregressive, gu2024minillm, agarwal2024generalized}.
% Distillm의 skewedKL, f-divergence, fusionKL도.
Indeed, several models have successfully employed KD in practice, such as Phi~\citep{abdin2024phi} and Minitron~\citep{muralidharan2024compact, sreenivas2024llm}.
% Phi3는 gpt의 output 일부를 SFT에 활용 & minitron은 pruning 후 KD 활용 <- KD를 활용한 성공적인 모델들

% 2. 이런 MoE teacher에 특화된 KD가 없다.
However, there has been no systematic development of KD methods specifically designed for the MoE teacher.
% 위랑 아래랑 같은 내용의 문장이긴 한데, 강조하기 위해서 그냥 한 번 더 언급.
Recent KD studies have largely overlooked scenarios where the model to be compressed is based on the MoE structure.
% 3. MoE teacher에 대한 연구가 있긴한데, (KD가 아닌 teacher output을 SFT했다던가,) KD 방법 자체는 Hinton KD approach를 활용한다던가 했다.
While a few studies have applied KD to MoE teacher models~\citep{artetxe2021efficient, fedus2022switch, xue2022one}, they have used the conventional KD and have not thoroughly explored the effectiveness or challenges of distilling knowledge from MoE.
% 4. KD를 더 잘 활용하기 위해 더 강력한 teacher 모델인 MoE 모델을 사용하고자 한다면, KD 방법에서 “선생이 MoE라는 특징”을 고려해주는 것이 유용할 것이다.
Therefore, these generalized approaches might not fully exploit the potential of MoE as a teacher.
% 5. 이 과정에서 특히, student 모델도 MoE가 아닌 dense라면 resource requirement를 줄일 수 있을 것이다.
% Moreover, compressing MoE to dense student model would be especially beneficial as it can further address the storage constraints.

% 3번 문단 - 우리의 방법 소개

% 1. 그래서 우리는 MoE teacher의 모든 expert로부터 효율적으로 정보를 가져올 수 있는 MoE2DenseKD 를 제안한다.
In this paper, we introduce \textbf{MoE-specific knowledge distillation}, which can effectively distill knowledge from the MoE teacher.
% 2. 이러한 MoE-to-dense 상황에 특화된 KD를 고안하기 위해, MoE에다가 일반적인 KD 를 할 때를 분석해보았다.
To design such a specialized mechanism, we first conduct an in-depth analysis of MoE teacher during the basic KD process proposed by~\citet{sanh2019distilbert}.
% 3. 우리는 "significant amount of potentially useful knowledge remains unutilized" 인것을 발견했다.
We found that even non-selected experts have a significant amount of potentially useful knowledge, which remains unutilized. %d%
% 4. 
Inspired by this observation, we propose two different intuitive solutions for effectively extracting knowledge from all experts (see Figure \ref{fig3}). %d%
% 5. The first method, Knowledge Augmentation, ~~~
The first method, \textit{knowledge augmentation} (KA), employs sampling multiple times to decide which experts to activate based on their gate probabilities. Through this approach, a student can be provided with a variety of augmented knowledge from a single input data. %d%
% 6. The second method, Student-Aware Router, ~~~
The second method, \textit{student-aware router} (SAR), optimizes the router based on student feedback before distillation, enabling the router to determine optimal weights to aggregate knowledge from all experts. %d%

% 4번 문단 - 실험 결과, 분석 결과

We apply our two approaches to Llama-MoE~\citep{zhu2024llama} models with five instruction datasets.
Considering the common practice of employing KD in memory-constrained settings, we utilize a dense student Sheared-Llama~\citep{xia2023sheared} rather than a MoE student.
The experimental results show that when the teacher model is MoE, our method consistently outperforms the existing KD baselines.
% KA - KA 방식에 대한 자세한 분석을 통해 실제로 augment되는 지식이 "적당량이 있으면" 도움이 된다는 걸 밝혔다.
Additionally, the analysis of KA confirms that having a moderate amount of augmented knowledge is indeed beneficial.
% SAR - SAR 방식에 있어서 router의 변화를 explore해서, 진짜 student-friently knowledge로 이어지는지 gate value의 변화를 실제로 확인하였다.
Moreover, in SAR, we confirm that router updates in fact induce subtle changes in gate values, and these changes demonstrably enhance the performance of KD.

% 5번 문단
To summarize, our contributions are three-fold:
\begin{itemize}
    \item We empirically found that non-activated experts from MoE teacher also possess knowledge that is of great benefit to a student, yet remains unexploited by existing methods. % Approach의 Motivation part
    \item We propose two novel methods, knowledge augmentation (KA) and student-aware router (SAR), effectively utilizing the distributed knowledge from the entire experts. To the best of our knowledge, these are the first KD methods specifically designed for MoE teacher. % Approach의 2가지 방법론 제안
    \item We evaluate our framework on 5 instruction-following datasets. The result shows that KA and SAR outperform the existing KD methods, underscoring the effectiveness and importance of leveraging the architectural characteristics of MoE teacher. % Experiments 결과 및 분석 -> 우리 꺼 효과적
\end{itemize}

\section{Related Works}

\paragraph{Knowledge distillation}
% 1. KD는 더 크고 강한 teacher model의 지식을 작은 student model에 transfer하는 거다.
Knowledge distillation (KD)~\citep{hinton2015distilling} is a prevalent model compression technique, transferring knowledge from a large teacher model to a small student model.
% 2. 최근에는 LLM에 대한 KD 연구가 진행되고 있다.
Most of the early works focused on applying KD to the text classification tasks by imitating all the possible things of the teacher model, from output distribution~\citep{song2020lightpaff, liang2020mixkd} to hidden states~\citep{jiao2020tinybert, sun2019patient, park-etal-2021-distilling}, attention scores~\citep{wang2020minilm}, and so forth.
%However, the mere acquisition of extensive information from robust teachers does not inherently guarantee practical advancements in student learning~\citep{cho2019efficacy}.
However, these methods relied on a fixed teacher that generates knowledge without being aware of the student’s learning characteristics, which often limited its effectiveness.
Thus, several methods are also devised to provide student-friendly knowledge~\citep{park2021learning, zhou2022bert, ren-etal-2023-tailoring}.

On the other hand, various studies are actively examining its application to text generation tasks.
The standard KD method minimizes the forward KL divergence between the output distributions of student and teacher at each time step~\citep{sanh2019distilbert} or directly trains the student with the generated text from the teacher~\citep{kim2016sequence, taori2023stanford, chiang2023vicuna, peng2023instruction}.
% 3. minillm에 대한 설명
Recently, MiniLLM~\citep{gu2024minillm} explores a method to mix the distribution of the teacher with that of the student and use a policy gradient approach by optimizing the reverse KL divergence.
% 4. gkd에 대한 설명
GKD~\citep{agarwal2024generalized} utilizes the student-generated on-policy data to receive feedback from the teacher with a generalized Jensen–Shannon (JS) divergence objective.
% distillLLM은 넣어야 되나?
% 6. 하지만 이들 모두, teacher가 dense 모델일 때의 실험 결과가 기반이다.
DistiLLM~\citep{ko2024distillm} applies skew KL divergence with their proposed adaptive off-policy mechanism.
Although these methods have shown remarkable results, all of the experiments have used dense models, and whether they also show good results for distilling the Mixture-of-Experts model has not yet been studied.


\paragraph{Mixture-of-Experts}
% 1. 모델을 scale up하기 위해 MoE라는 architecture가 제안되었다.
Mixture-of-Experts (MoE) \citep{shazeer2017outrageously, lepikhin2020gshard, fedus2022switch} is an efficient way to increase the model size by replacing the feed-forward network (FFN) with multiple experts and a gating network.
It dynamically activates different experts for each input token instead of using all parameters.
% 2. 그 이후, 실제로 많은 모델들(mixtral, deepseek)이 MoE 또는 그 변형을 채택해 매우 강력한 LLM을 개발하였다.
Since it has been known that MoE provides advantages including more efficient training~\citep{he2022fastermoe, gale2023megablocks} and faster inference than a dense model of the same size, many models such as Mixtral~\citep{jiang2024mixtral} and DeepseekMoE~\citep{dai2024deepseekmoe} have introduced MoE or its variants, demonstrating remarkably strong performance.
% 3. 더 강력한 이런 MoE를 더 작은 모델로 compressing 하려는 연구?실험?도 있었다.
However, due to the disadvantage of high memory requirements, there have been some efforts to compress MoE into smaller dense models~\citep{artetxe2021efficient, fedus2022switch, xue2022one, guo2025deepseek}.
% 4. 하지만, MoE의 output으로 SFT를 하거나(Deepseek-R1), KD를 한다고 해도 naive한 approach (Swith transformer, Meta MoE, One-student-knows-all)가 전부였다.
Nevertheless, they use the conventional KD~\citep{sanh2019distilbert} or train on the teacher's output sentence directly. To the best of our knowledge, there has been no attempt to develop the KD specifically optimized for MoE teacher.

\section{Preliminary}
% 여기 KD 수식이랑 MoE 수식이랑 넣어서 설명하고, 뒤에 식들은 그냥 여기 식이랑 세팅 똑같게.

%\paragraph{Knowledge distillation}
\subsection{Knowledge Distillation}

KD minimizes the token-level distributional discrepancy between teacher and student. A standard approach to accomplish this minimization in the instruction-following setting is using the forward KL divergence~\citep{sanh2019distilbert}:
\begin{equation}
\mathcal{L}_{\text{KD}}=D_{KL}\big( p(\bm{y}|\bm{x}) \parallel q_\theta(\bm{y}|\bm{x}) \big),
\label{eqn:hintonkd}
\end{equation}
where $(\bm{x}, \bf{y})\in \mathcal{D}$, $\mathcal{D}$ denotes a dataset.
$\bm{x}$ and $\bm{y}$ represent the request and response, respectively, and this objective guides the student to learn by minimizing the distributional discrepancy in the only response part. $p$ and $q_\theta$ denote the probability distributions of the teacher and student, respectively.
%$p(\bm{y}|\bm{x})$ and $q_\theta(\bm{y}|\bm{x})$ correspond to a fixed teacher model and a trainable student model, respectively.
%While theoretically $\bm{y} \sim p(\cdot|\bm{x})$ should be used, following previous studies~\citep{gu2024minillm, agarwal2024generalized}, we approximate it by $(\bm{x}, \bf{y})\in \mathcal{D}$ assuming that the teacher is so powerful that the teacher's output is similar to the data $\mathcal{D}$.

Recently, MiniLLM~\citep{gu2024minillm} and GKD~\citep{agarwal2024generalized} suggest using reverse KL divergence and student-generated sequences to address the exposure bias problem. The objective reflecting these is as follows:
\begin{equation}
\mathcal{L}_{\text{student}}=D_{KL}\big( q_\theta(\bm{y}|\bm{x}) \parallel p(\bm{y}|\bm{x}) \big),
\label{eqn:student}
\end{equation}
where $(\bm{x}, \cdot)\in \mathcal{D}$ and $\bm{y} \sim q_\theta(\cdot|\bm{x})$.


%\paragraph{MoE}
\subsection{Mixture-of-Experts}
% Noisy Top-k Gating and Load Balance

The sparse MoE layer consists of $N$ expert networks $\{E_{1}, \cdots, E_{N}\}$ and a router network $G$.
The router first computes the gate logits $H(x)\in \mathbb{R}^{N}$ for a single token representation $x$, which determines the likelihood of selecting each expert.
After normalization with a softmax function, top $k$ experts are selected based on this distribution, and their outputs are aggregated through a weighted sum.
In this work, we only focus on the noisy Top-$k$ gating introduced by~\citet{shazeer2017outrageously}.
This gating adds trainable Gaussian noise before Top-$k$ experts selection. The process can be described as follows:
\begin{align}
  \begin{split}
    H(x)_i = (x \cdot W_g)_i \ &+ \text{Standard}\text{Normal}() \ \cdot \\
    &\text{Softplus}((x \cdot W_{\text{noise}})_i),
\end{split} \label{eqn:noisytopk-logit} \\
  G(x) = \text{Softmax}&(\text{KeepTopK}(H(x), k)), \\
  y = \sum_{i=1}^N& G(x)_i E_i(x),
\end{align}
where $G(x)_{i}$ denotes the probability of $i$th experts being selected and \begin{equation}
\text{KeepTopK}(v, k)_{i}=
\begin{cases}
 v_{i} & \text{if } v_{i} \in \text{TopK}(v, k), \\
 -\infty & \text{otherwise}.
\end{cases} \nonumber
\end{equation}

The intrinsic characteristic of Top-$k$ routing may lead to a scenario where certain experts are always favored in the selection process.
In order not to negate the potential benefits of the MoE, distributing the workload across multiple experts to ensure their collective engagement is essential, which is called load balancing.
The noise term in $H(x)$ or the auxiliary loss as in Eq.~\eqref{eqn:LBL} helps prevent the model from always relying on the same few experts, allowing a more balanced distribution of workload among experts. %d%
The auxiliary loss~\citep{zhu2024llama} is as follows:
\begin{equation}
\label{eqn:LBL}
    %\mathcal{L}_{b}=N \cdot \sum_{i=1}^{N}m_{i} \cdot P_{i},
    \mathcal{L}_{\text{b}}=CV(\boldsymbol{m})^2+CV(\boldsymbol{P})^2,
    %\text{CV}(\boldsymbol{X})=\text{std}(\boldsymbol{X})/\text{mean}(\boldsymbol{X})
\end{equation}
where $\boldsymbol{m} \in \mathbb{R}^{N}$ represents the set of token counts assigned to each of the $N$ experts within a batch, and $\boldsymbol{P} \in \mathbb{R}^{N}$ denotes the set of summed probabilities assigned to each expert in the batch. The coefficient of variation ($CV$) is defined as the ratio of the standard deviation $\sigma$ to the mean $\mu$, i.e., $CV(\boldsymbol{x}) = \sigma(\boldsymbol{x})/\mu(\boldsymbol{x})$. Minimizing this encourages a more uniform distribution, which is desirable for balancing the expert load. 
%where $m_{i}$ is the fraction of tokens assigned to the $i$th expert and $P_{i}$ is the probability fraction to the $i$th expert across all tokens.
%The criticality of load balance in ensuring the utilization of a truly diverse spectrum of experts has motivated extensive research efforts aimed at optimizing load balancing~\citep{wang2024auxiliary, qiu2025demons}.
% auxiliary-loss-free(wang씨거, deepseekv3에 썻던), 혹은 global LBL(qwen꺼)

\section{Method}

\subsection{Motivation}
\label{approach:motivation}

\begin{figure*}[t!]
  \centering
  %\hspace{-0.5cm}
    \subfloat[Llama-MoE-3.5B (4/16)]
    {\label{fig1:subfig-a}
    \includegraphics[width=0.32\linewidth]{figure/motivation_llamamoe_3p5_416.pdf}
    }
    \hfill
    %\hspace{-1cm}
    \subfloat[Llama-MoE-3.5B (2/8)]
    {\label{fig1:subfig-b}
    \includegraphics[width=0.32\linewidth]{figure/motivation_llamamoe_3p5_28.pdf}
    }
    \hfill
    %\hspace{-1cm}
    \subfloat[Llama-MoE-3.0B (2/16)]
    {\label{fig1:subfig-c}
    \includegraphics[width=0.32\linewidth]{figure/motivation_llamamoe_3p0_216.pdf}
    }
  \caption{Sum of the gate probabilities for activated and non-activated experts per layer during distillation. The $(k/N)$ after each model name indicates that $k$ out of $N$ experts are activated. Across most layers of all Llama-MoE models, the sum of gated probabilities of activated experts is less than 50\%.}
  \label{fig1}

\end{figure*}

% 1. 일반적인 KD를 하면 활성화 안 된 expert gate proba.가 절반이상이다.

To investigate how the MoE teacher distills the knowledge during the classical KD process, we first analyze the distribution of gate probabilities. %d%
The gate probability refers to the normalized values of the gate logits $H$ through the softmax function. The Top-$k$ experts are selected based on these gate logits, and gate logits are also used to compute the weights during the weighted summation of expert outputs. Therefore, the gate probability can be considered an indicator of how useful each expert is.
In this section, we use Llama-MoE~\citep{zhu2024llama} models as teachers and do the conventional KD~\citep{sanh2019distilbert} into Sheared-Llama~\citep{xia2023sheared} model which is a dense model. The training data is a subset of Dolly~\citep{DatabricksBlog2023DollyV2}, and we evaluate our model on five instruction datasets. For further details, please see the Section~\ref{experiments:setup}.
%(See Section~\ref{experiments:setup} for further details.)

Figure~\ref{fig1} presents a visualization of the average of the sum of gate probabilities for used experts and that for unused experts in each layer across all training data during distillation. As shown in Figure~\ref{fig1}, the sum of gate probabilities for the group of activated experts never exceeds 50\% in most of the layers of all models.
Although this may be an effect of the auxiliary loss for load balancing, considering that gate probability reflects how useful an expert is, 
% a. 이는 잠재적으로 유용한 지식을 KD과정에서 활용하지 못했다고 볼 수 있다.
it implies that a significant portion of potentially valuable knowledge from non-activated experts is not being leveraged. %d%
% b. Potential reason 중 하나로는, MoE 학습 과정에서 load balance를 고려해 지식이 여러 expert에 골고루 퍼지도록 한다. 하지만 naive KD는 top-k만 쓰기에 이런 점을 고려하지 않는다.
%One possible reason is that during the training process of MoE, the knowledge is evenly distributed across the experts considering the load balance. However, conventional KD does not take this into account by virtue of its reliance on only the Top-$k$ experts.
% c. 이런 non-activated experts의 지식을 잘 활용한다면 개선 될 여지가 있을것으로 기대된다.
Thus, effective extraction and utilization of this unexploited knowledge could bring additional benefits to the student model during the distillation process, as more diverse and complementary knowledge would be incorporated into the learning. %d%

% 2. MoE 활성화하는 expert #를 늘렸을 때 KD 성능 & teacher 자체 성능.
% a. 그래서 단순하게 일단 더 많이 써봤다.
To reflect this observation, we simply increase the number of selected experts $k$ during the distillation process.
%Specifically, for Llama-MoE-3.5B (4/16) model, we increase the number of selected experts following the gate distribution during KD instead of using just Top-4 experts.
Using the Llama-MoE-3.5B (4/16) model as the teacher model, we perform knowledge distillation by gradually increasing $k$ from 4 to 16 and evaluate the performance of both the teacher and student models.
The model performance is measured by the average ROUGE-L scores across five instruction-following datasets (Section~\ref{experiments:setup} for more details).
%In addition, we measure the performance of MoE teacher itself over the number of used experts. 
The results are shown in Figure~\ref{fig2}.

% 아래 사진에, 그냥 단순 teacher 성능 그래프도 추가해야 함.
\begin{figure}[t]
  \subfloat[Distilled student under MoE teacher]
  {\label{fig2:subfig-a}
  \includegraphics[width=\columnwidth]{figure/student_N.pdf}
  }
  \vspace{3mm}
  \subfloat[MoE teacher]
  {\label{fig2:subfig-b}
  \includegraphics[width=\columnwidth]{figure/teacher_N.pdf}
  }
  %\caption{MoE teacher performance and student performance after basic KD over different number of utilized experts. Performance of student is improved as more experts are utilized. In contrast, performance of teacher is not necessarily increased just by using more experts.}
  \caption{Performance of the MoE teacher model and the student model after distillation with varying numbers of utilized experts $k$ (originally 4). As $k$ increases, the effectiveness of distillation improves, leading to better student performance. However, the performance of the teacher model itself does not necessarily improve with a larger $k$.}
  \label{fig2}
  \vspace{-0.2in}
\end{figure}

\begin{figure*}[t]
  \centering
    %\subfloat[MoE]
    %{\label{fig3:subfig-a}
    %\includegraphics[width=0.3\linewidth]{figure/original_moe_temp.png}
    %}
    %\hfill
    \subfloat[Knowledge Augmentation (KA)]
    {\label{fig3:subfig-b}
    \includegraphics[width=0.48\linewidth]{figure/explanation_KA.png}
    }
    \hfill
    \subfloat[Student-Aware Router (SAR)]
    {\label{fig3:subfig-c}
    \includegraphics[width=0.48\linewidth]{figure/explanation_SAR.png}
    }
  \caption{An overview of our proposed KD methods specifically designed for the MoE. In knowledge augmentation, we either select the $\text{top }N-1$ experts or sample $N-1$ experts based on the gate probability. We do this $M$ times to augment various knowledge. In student-aware router, we train the router network with student feedback before the distillation. It enables the router to determine the optimal weights, thereby facilitating the student's acquisition of knowledge from all experts.}
  \label{fig3}
  %\vspace{-0.1in}
\end{figure*}

% b. 결과 설명
% i. 선생 -> 무조건 더 많은 expert를 쓴다고 성능이 증가하는 건 아니더라.
% ii. 학생 -> 더 많은 expert를 쓴다고 성능이 증가하더라. (16개 전체 쓰는 경우 제외)
Based on the results, we observe that using more experts does not necessarily increase the performance of the teacher, but it certainly increases the performance of the student, except when all are used.
% iii. load balance 언급 -> 지식은 균등하게 분포되어 있다.
%In light of the preceding discussion, load balancing also influences this result.
% iv. non-activated experts들의 지식을 더 쓴다고 선생 자체의 성능은 더 좋아지지 않는다 할지라도, 학생의 학습 과정에서는 분명히 이런 지식이 도움이 된다고 해석!!
%There is considerable knowledge among non-activated experts, and even if the teacher's own performance does not improve by using this, such knowledge is clearly helpful in the student's learning process.
This suggests that the improvement in the student’s performance is not directly due to the teacher’s performance enhancement. Nevertheless, we observe that using most of the non-activated experts proves to be practically beneficial for the student, and this implies that non-activated experts hold valuable knowledge. The reason for this could be that during the MoE training process, due to load balancing, different sets of experts are activated for the same input data, causing the knowledge to be distributed across multiple experts. However, conventional KD typically relies on using only the Top-k experts, which fails to account for this.
% 먼저, teacher의 성능이 좋아져서 student의 성능이 좋아진 것은 아니라는 점. -> 또한, 대부분의 non-activated expert를 사용한 것이 student에게 실질적으로 도움이 된다는 것 -> 이러한 이유로는 MoE의 학습 과정에서 load balancing때문에 같은 input data에 대해서도 사용되는 expert가 달라져서, knowledge가 여러 expert에 골고루 흩어지게 되기 때문으로 추측됨 -> 그러나, conventional KD는 Top-k만 그대로 사용하기 때문에 이러한 점을 고려하지 못한다

% 3. Motivation의 결론 -> non-activated expert의 knowledge를 쓰자.
Therefore, the core challenge in knowledge distillation for MoE teacher lies in \textbf{effectively extracting and transferring the knowledge that is distributed across all experts} to empower student learning. Successfully addressing this challenge is the key to fully leveraging the architectural characteristics of MoE teacher models in guiding student models. %d%

\subsection{Knowledge Augmentation}
\label{KA}

\begin{algorithm}[t]
\caption{: Knowledge Augmentation (KA)}
\label{alg:KA}
\begin{algorithmic}
\Require student model $q_\theta$, data distribution $p_x$, number of teacher forward $M$, training step $K$, learning rate $\eta$
\For{each step $k=1,...,K$}
    \State Sample a request $\bm{x}$ from $p_x$
    \State Sample a response $\bm{y}$ from $q_\theta(\cdot|\bm{x})$
    \For{each step $m=1,...,M$}
        \State Update $\theta \leftarrow \theta-\eta \nabla \mathcal{L}_{\text{student}}$  \quad $\triangleright$ Eq.~\eqref{eqn:student}
    \EndFor
\EndFor
\State \Return $\theta$
\end{algorithmic}
\end{algorithm}

The first method to effectively utilize distributed knowledge across all experts is the knowledge augmentation (KA).
Following the previous observation, we use $N-1$ experts for each layer where $N$ is the total number of experts.
%Specifically, top $N-1$ experts are selected with probability $1-\lambda$ so that we can consistently use more knowledge than Top-$k$ selection.
%Moreover, $N-1$ experts are used via sampling from a gate probability with probability $\lambda$.
Specifically, in each MoE layer, $N-1$ experts are selected by sampling from a gate probability distribution with probability $\lambda$.
Therefore, by selecting the Top $N-1$ experts with probability $1-\lambda$, we can consistently generate knowledge that is similar to the Top-k selection while incorporating slightly different knowledge.
This strategy allows the augmentation of diverse knowledge and balances the trade-off between consistency and diversity of knowledge with parameter $\lambda$.
The formulation of KA is as follows:
\begin{align}
&\mathbf{E} =
\begin{cases}
 \text{Sampled } N-1 \ \text{experts} & \text{w.p. } \lambda, \\
 \text{Top } N-1 \ \text{experts} & \text{w.p. } 1-\lambda,
\end{cases} \nonumber\\[5pt]
&\text{KA}(v, \mathbf{E})_{i} =
\begin{cases}
 v_{i} & \text{if } (i \text{th expert}) \in \mathbf{E}, \\
 -\infty & \text{otherwise},
\end{cases} \\[5pt]
&G^{\text{KA}}(x)= \ \text{Softmax}(\text{KA}(H(x), \mathbf{E})),
\end{align}
where $\mathbf{E}$ denotes the set of selected experts.

In each iteration, the teacher is forwarded $M$ times for the same input using the KA method, augmenting $M$ pieces of knowledge, which are transferred to the student.
% Unlike the experiments in Section~\ref{approach:motivation}, the response part $\bm{y}$ of the input is generated by the student, treating it as a pseudo-target, to mitigate exposure bias~\citep{arora2022exposure} following GKD~\citep{agarwal2024generalized}.
Following GKD~\citep{agarwal2024generalized}, the response part $\bm{y}$ of the input is generated by the student, treating it as a pseudo-target, to mitigate exposure bias~\citep{arora2022exposure}.
Furthermore, the student's learning objective is the reverse KL divergence.
We summarize the entire KA procedure in Algorithm~\ref{alg:KA}.

% $$\mathcal{L}_\text{student}=D_{KL}(q_{\theta}(\bm{y}|\bm{x})\ || \ p(\bm{y}|\bm{x}))$$

\subsection{Student-Aware Router}
\label{SAR}

The second method is the student-aware router (SAR). Instead of merely selecting which experts to use, SAR takes a step further by directly optimizing the router to achieve an optimal weighted sum across all expert outputs. Inspired by the concept of student-friendly knowledge distillation, SAR updates the teacher’s router using student feedback, ensuring that the generated knowledge is more useful to the student.
%The second novel method is the student-aware router (SAR). While KA selects the $N-1$ experts for distillation, SAR utilizes the entire experts. However, simply using all experts yields worse results, as shown in Figure~\ref{fig2}. To address this issue, we first optimize the weight of routers before the distillation to ensure the resulting knowledge becomes useful to the student.

% % 그래서 SAR은 2단계로 나눌 수 있다.
% So, SAR consists of two stages: (1) Router Update (2) Knowledge Distillation. 
% % Step 1. Router training
% For the first stage, SAR only trains the weights of routers, $W_{g}$ and $W_{noise}$ (see Eq.~\eqref{eqn:noisytopk-logit}), while all other weights are fixed. Then, without any selection of experts, the output of a router is directly transformed into a softmax function.
% % Step 2. KD
% After then, we do the distillation process as usual, but now equipped with student-friendly routers.
SAR undergoes two stages in each iteration: router update and knowledge distillation. First, the router weights, $W_g$ and $W_{\text{noise}}$ in Eq.~\eqref{eqn:noisytopk-logit}, are optimized using student feedback~\citep{kim-etal-2024-promptkd} and auxiliary loss for load balancing. Only the router components of the MoE teacher are updated, while all other parameters remain fixed. After updating the router, the modified router is used to generate knowledge, which is then distilled into the student. At this stage, all experts are activated, and their outputs are aggregated through a weighted sum based on the modified router.

Similar to KA, SAR also uses pseudo-targets generated by the student and trains the router using reverse KL divergence: \begin{align}
    \mathcal{L}_{\text{SAR}}=&D_{KL}\big( p(\bm{y}|\bm{x}) \parallel q_\theta(\bm{y}|\bm{x}) \big) + \beta\mathcal{L}_{\text{b}}.
    \label{eqn:SAR}
\end{align} 
Here, $\beta$ is a coefficient for the auxiliary loss, which is set to 0.01 following the teacher model~\citep{zhu2024llama}. 
%This also ensures that the router has reflected the student feedback during the router update. 
The entire SAR process is summarized in Algorithm~\ref{alg:SAR}.

\section{Experiments}

\subsection{Experimental Setup}
\label{experiments:setup}

% 세팅: 데이터셋 5개, ROUGE-L 
\paragraph{Settings} 
Following~\citet{gu2024minillm}, \texttt{databricks -dolly-15k}\,~\citep{DatabricksBlog2023DollyV2} is partitioned into 14k samples for the training set, 500 samples for the validation and test sets, respectively. In addition to the test set of Dolly, we evaluate 4 extra instruction-following datasets: SelfInst~\citep{wang-etal-2023-self-instruct}, 252 user-oriented instruction-following samples, Vicuna~\citep{chiang2023vicuna}, 80 questions used in the Vicuna evaluation, S-NI, 9k samples from the test set of \textsc{Super-NaturalInstructions}~\citep{wang2022super}, and UnNI, randomly sampled 10k samples from the core set of \textsc{UnnaturalInstructions}~\citep{honovich-etal-2023-unnatural}. We adopt the ROUGE-L~\citep{lin2004rouge} score as the evaluation metric.

\begin{algorithm}[t]
\caption{: Student-Aware Router (SAR)}
\label{alg:SAR}
\begin{algorithmic}
\Require student model $q_\theta$, data distribution $p_x$, teacher's router $W_{g}$ and $W_{\text{noise}}$, training step $K$, learning rate $\eta$

\For{each step $k=1,...,K$}
\State Sample a request $\bm{x}$ from $p_x$
\State Sample a response $\bm{y}$ from $q_\theta(\cdot|\bm{x})$
\State Update $W_{g} \leftarrow W_{g}-\eta \nabla \mathcal{L}_{\text{SAR}}$  \quad $\triangleright$ Eq.~\eqref{eqn:SAR}
\State Update $W_{\text{noise}} \leftarrow W_{\text{noise}}-\eta \nabla \mathcal{L}_{\text{SAR}}$
\State Update $\theta \leftarrow \theta-\eta \nabla \mathcal{L}_{\text{student}}$  \quad $\triangleright$ Eq.~\eqref{eqn:student}
\EndFor
\State \Return $\theta$
\end{algorithmic}
\end{algorithm}

\begin{table*}[t]
\centering
\begin{tabular}{llcccccc}
\hline
\multirow{2}{*}{\shortstack{Model\\(Teacher $\rightarrow$ Student)}} & \multirow{2}{*}{Method} & \multicolumn{5}{c}{Instruction-following datasets} & \multirow{2}{*}{Average} \\ 
\cline{3-7} & & Dolly & SelfInst & Vicuna & S-NI & UnNI \\ 
\hline
\hline
Llama-MoE-3.5B (4/16) & SFT & 26.20 & 18.61 & 16.88 & 30.29 & 31.79 & 24.75 \\
Llama-MoE-3.5B (2/8) & SFT & 26.39 & 16.97 & 17.20 & 30.40 & 32.81 & 24.76 \\
Llama-MoE-3.0B (2/16) & SFT & 26.35 & 17.64 & 16.86 & 27.59 & 30.42 & 23.77 \\
Sheared-Llama-2.7B & SFT & 26.07 & 18.55 & 17.50 & 27.64 & 31.13 & 24.18 \\
Sheared-Llama-1.3B & SFT & 23.83 & 14.82 & 15.93 & 26.33 & 28.21 & 21.82 \\
\hline
\multirow{2}{*}{\shortstack{Sheared-Llama-2.7B\\$\rightarrow$ Sheared-Llama-1.3B}} 
 & KD & 24.68 & 13.44 & 16.16 & 26.37 & 29.09 & 21.95\\ 
%\cline{2-8}
& GKD & \textbf{26.36} & \textbf{16.67} & \textbf{18.20} & \textbf{29.09} & \textbf{34.12} & \textbf{24.89} \\
\hline
\multirow{5}{*}{\shortstack{Llama-MoE-3.5B (4/16)\\$\rightarrow$ Sheared-Llama-1.3B}} 
 & KD & 23.58 & 13.82 & 15.25 & 24.59 & 27.37 & 20.92\\
 & GKD & 25.86 & 16.72 & \textbf{18.61} & 29.61 & 34.55 & 25.07\\
%\cline{2-8}
& ALL (Ours)& 26.03 & 16.98 & 18.59 & 30.13 & 34.88 & 25.32 \\
& KA (Ours)& \textbf{26.58} & 16.98 & 18.38 & 30.51 & \textbf{36.11} & 25.71 \\
& SAR (Ours)& 26.32 & \textbf{18.24} & 18.06 & \textbf{31.88} & 35.05 & \textbf{25.91}\\
\hline
\multirow{5}{*}{\shortstack{Llama-MoE-3.5B (2/8)\\$\rightarrow$ Sheared-Llama-1.3B}} 
 & KD & 23.07 & 13.92 & 15.29 & 24.87 & 27.40 & 20.91\\
 & GKD & 25.64 & 15.54 & 18.29 & 29.11 & 32.80 & 24.28\\
%\cline{2-8}
& ALL (Ours)& \textbf{26.40} & 16.78 & \textbf{18.45} & 28.68 & 33.57 & 24.78 \\
& KA (Ours)& 26.32 & 17.30 & 17.11 & \textbf{32.49} & \textbf{37.58} & \textbf{26.16} \\
& SAR (Ours)& 26.30 & \textbf{18.31} & 17.11 & 31.47 & 35.00 & 25.64\\
\hline
\multirow{5}{*}{\shortstack{Llama-MoE-3.0B (2/16)\\$\rightarrow$ Sheared-Llama-1.3B}} 
 & KD & 23.20 & 13.51 & 15.01 & 23.85 & 26.92 & 20.50\\
 & GKD & 25.43 & 16.43 & \textbf{18.52} & 28.15 & 34.71 & 24.65\\
%\cline{2-8} 					
& ALL (Ours)& 25.99 & 15.05 & 18.06 & 29.15 & 33.55 & 24.36 \\
& KA (Ours)& \textbf{26.06} & 16.18 & 18.30 & 30.10 & \textbf{35.92} & 25.31 \\
& SAR (Ours)& 25.87 & \textbf{17.39} & 17.84 & \textbf{31.20} & 34.92 & \textbf{25.44}\\
\hline
\end{tabular}
\caption{Evaluation results on five instruction-following datasets and their average, assessed using the ROUGE-L metric. Each reported score represents the average across five distinct random seeds. The best score for each case is highlighted in \textbf{boldface}.}
\label{tab:main_result}
%\vspace{-0.1in}
\end{table*}

% 모델: MoE, sheared-Llama / 왜 이 모델을 선정했는지 이유 설명이 중요함!
\paragraph{Models}
To verify the effectiveness of proposed KD methods tailored for MoE, we need to compare the performance of KD from dense to dense with that from MoE to dense.
For this comparison to be fair, dense teacher and MoE teacher should have comparable performances.
This ensures that any performance improvements can be directly ascribed to the proposed method rather than the teacher's inherent capability.
Additionally, the tokenizers of both models must be the same to compare token-level distributions.

To satisfy the above critical conditions, we use three Llama-MoE~\citep{zhu2024llama} models as the MoE teachers, Sheared-Llama~\citep{xia2023sheared} 2.7B as the dense teacher, and Sheared-Llama 1.3B as the dense student. Sheared-Llama 2.7B  exhibits comparable performance to Llama-MoE model, with a lower number of activated parameters.
%Moreover, they are pre-trained on almost the same dataset: Redpajama~\citep{weber2024redpajama} and its subset, Slimpajama~\citep{soboleva2023slimpajama}, ensuring a controlled environment for KD. 
Both teacher models and the student model were initially fine-tuned with the Dolly training set before knowledge distillation, following the previous works~\citep{agarwal2024generalized, gu2024minillm}.

% Baseline: SFT, KD, GKD
\paragraph{Baseline}
We compare our two approaches with three baselines: (1) supervised fine-tuning (SFT) directly fine-tunes the model on golden responses, which does not involve knowledge distillation; (2) KD~\citep{sanh2019distilbert} uses the teacher's distribution with forward KL divergence; (3) GKD~\citep{agarwal2024generalized} uses the mixture of fixed data and on-policy student-generated outputs.
Despite recent advancements and variants, GKD remains a representative study utilizing KL divergence or its variants and student-generated outputs, making it a suitable baseline for our experiment.
Based on their reported performance, GKD computes reverse KL divergence with only student-generated outputs in this paper.
For our methods, we set a sampling ratio $\lambda=0.05$ and the number of augmented samples $M=2$ in the KA method.
%To facilitate a direct comparison with SAR, we also activate all experts without router training and conduct a GKD.
%This approach is termed ALL.
To validate our observation on the MoE teacher, we exclude the router update stage from SAR and simply activate all experts. This approach is referred to as ALL.
Further details on the experimental setup are summarized in the Appendix~\ref{sec:appendix-setup-detail}.


\subsection{Results}
\label{experiments:results}

We present the results of KA and SAR with baselines on 5 datasets in Table~\ref{tab:main_result}.

% 1번. SFT한 결과
% 1. 같은 16개 experts에서 더 activate 될수록 성능이 좋더라.
% 2. distill 이전, MoE teacher와 dense teacher의 성능이 진짜로 비슷하더라.
First, when comparing the SFT results of three Llama-MoE models, the performance is better when there are more activated experts with the same total number of experts. % 3.5B(4/16) vs 3.0B(2/16)
Also, if the total activated parameters are similar, the performance is also comparable. % 3.5B(4/16) vs 3.5B(2/8)
Note that the dense teacher Sheared-Llama-2.7B indeed shows a similar performance compared to MoE teachers.

% 2번. KD, GKD는 d2d가 m2d보다 좋더라. (Interestingly)
Second, we compare the performance between dense and MoE teachers for the two baselines, KD and GKD.
Surprisingly, despite the MoE teacher having performance that is similar to or even slightly better than the dense teacher, % SFT 결과들 끼리 비교하면 그러한데
both methods demonstrate that the dense model serves as a better teacher for the student. %d%
For KD, the student trained by the dense teacher always outperforms the student trained by the MoE teachers. % 21.95 vs 20.92, 20.91, 20.50
This holds true under GKD as well, except for the Llama-MoE-3.5B (4/16) case. % 24.89 vs 25.07(요거는 예외), 24.28, 24.65
These results highlight that the existing KD methods are not optimized for extracting knowledge from the MoE teacher. %d%

% 3번. 
Third, our proposed methods, knowledge augmentation and student-aware router, achieve higher performance than baselines when the teacher model is MoE.
%This result indicates the effectiveness of two methods which are specifically designed considering the structural properties of the MoE teacher.
This result highlights that both methods are specifically designed for the MoE teacher.
Therefore, when the teacher model is MoE, it is important to carefully consider the architectural characteristics of MoE and effectively extract knowledge that is distributed across all experts.

% 4번.
% 우리의 방법과 단순히 모든 expert를 사용한 ALL을 비교했을 때, 우리의 방법의 성능이 더 좋았다. 이는 Student-generated output을 사용할지다로 Figure 2에서의 observation과 analogous하다. 또한, router를 학습하지 않는 ALL에 비해 SAR처럼 router를 학습해 student-friendly하게 adapt하는 게 정말로 효과가 있다는 것을 보여주는 것이다.

% Lastly, compared to ALL, which simply activates all experts, both of our methods achieved better performance.
% This result is analogous to the observations in Figure~\ref{fig2}, which indicated that using more experts improves performance compared to existing baseline methods, but simply using them might not be the best method.
% Rather, both of our methods are more effective than merely using all experts.
% Moreover, it clearly highlights the benefit of incorporating router training for student-friendly adaptation in SAR, compared to ALL without router training.
Lastly, the ALL approach, which simply activates all experts, outperforms the baselines in most cases but falls short of our proposed methods.
This result aligns with the observation in Section~\ref{approach:motivation}, suggesting that while non-activated experts contain useful knowledge, simply utilizing all of them may not be the optimal strategy.
Furthermore, the comparison with SAR demonstrates the effectiveness of router updates.

The qualitative results of our methods and the baselines are summarized in Appendix~\ref{sec:qualitative_results}, demonstrating that our methods produce responses most closely resembling the ground truth.

\begin{figure}[t]
  \includegraphics[width=0.93\linewidth]{figure/ablation_M.pdf}
  \caption{Average performance of KA for a different number of samples, $M$, across all test data. $\lambda$ is fixed at 0.05. For each MoE teacher, the best performing $M$ differs. If $M$ is too large, all models exhibit reduced performance.}
  \label{fig:effect_of_M}
  \vspace{-0.1in}
\end{figure}

\subsection{Analysis}
% 1. KA hyperparameter 비교
\paragraph{Hyperparameters in KA}
% Note that the ground truth $(\bm{x}, \bf{y})\in \mathit{D}$ is used in  \ref{approach:motivation} following~\citet{hinton2015distilling}, but student-generated output $(\bm{x}, \cdot)\in \mathit{D}$, $\bm{y} \sim q_\theta(\bm{y}|\bm{x})$ is used in \ref{experiments:results} following~\citet{gu2024minillm}.
% In the KA method, we perform $M$ iterations either directly using top-$(N-1)$ experts with probability $(1-\lambda)$ or randomly sampling $(N-1)$ experts based on the gate distribution with probability $\lambda$.
% We ablate $M$, the number of augmented samples, and $\lambda$, the probability of doing $(N-1)$ expert random sampling, in KA for various values.
%We ablate $M$, the number of augmented samples in KA for various values.
We ablate various values of $M$, the number of augmented samples in KA.
% Figure~\ref{fig:effect_of_M} shows the performance for different numbers of samples, $M$, and Table~\ref{tab:ablation_lambda} represents the performance for different values of the probability of random expert sampling, $\lambda$.
Figure~\ref{fig:effect_of_M} shows the performance for different numbers of samples, $M$.
% 적당히 큰 M또는 lambda를 통해 augment되는 지식이 좋긴 한데, 너무 큰 값으로하면 같은 input에 대해 너무 다양한 knowledge가 생긴다. 이런 다양성은 오히려 해가 된다. randomness로 인해, 이들 중 이상한 knowledge나 또는 학생에게 도움이 안되는 것들이 있을 수 있다.
It indicates that the optimal $M$ value varied across different models.
Nevertheless, the appropriate value of $M$ generally leads to beneficial augmentation.
However, when $M$ is excessively large, performance consistently degrades across all models.
This is because too large values can lead to the generation of overly diverse knowledge for identical input due to the inherent randomness of sampling.
Consequently, such excessive diversity can be detrimental to the overall performance, as it may introduce nonsense or unhelpful knowledge.

% lambda 값도 마찬가지로 적절한 값은 augmentation으로 이득을 볼 수 있으나, 너무 크면 망가진다. <- 이렇게 짧게 Appendix에 언급하기.
We also ablate various values of $\lambda$, the probability of randomly sampling experts. The results are in Appendix~\ref{sec:appendix-KA-lambda}.

% 2. SAR router gate distribution 변화 체크
\paragraph{Shift of gate probability in SAR}

% 첫번째. 우리는 Table 1에서, router를 학습하는게 진짜 도움이 되는 걸 확인했었다. 더 엄밀한 분석을 위해~~ (바로 뒷 문장 'In SAR' 지우기)
In Table~\ref{tab:main_result}, we compared the results of ALL and SAR and verified that training the routers of MoE teacher is indeed helpful.
For a more rigorous analysis, we examine the changes in the gate probability distribution that occurred as the router network learned to be more student-aware.

% Figure~\ref{}는 전체 train data에 대해, layer마다 기본 (SFT된) teacher 모델의 router의 gate probability와 SAR로 학습한 후의 teacher 모델의 router의 gate probability 간 KL divergence를 plot한 것이다.
Figure~\ref{fig:sar_analysis} presents the layer-wise KL divergence of gate probabilities between the original teacher MoE and the teacher whose routers are trained with SAR. For all tokens of the training data, the maximum and average values are shown.
% Layer가 깊어질수록 KL divergence가 증가한다. 이는 Layer가 깊어질수록 변형된 router의 영향으로 서로 다른 representation를 생성하다보니, 이 영향이 누적되어 후반부 layer일수록 기존 router와 다른 gate probability를 갖게 된다.
For every teacher model, KL divergence increases with greater layer depth.
The reason is that by learning the router in a student-friendly way, the modified gate probability affects the representation of the layer immediately following.
This effect accumulates so that later layers have more different gate probabilities than the existing router.
Eventually, these changes in gate probability have led to a more effective knowledge delivery to the student.

\begin{figure}[t]
  \includegraphics[width=0.96\linewidth]{figure/SAR_analysis.pdf}
  \caption{KL divergence of gate probabilities between original router and router trained with SAR method. The mean value is averaged over all tokens in training data. Consistently, KL divergence increases with layer depth.}
  \label{fig:sar_analysis}
  \vspace{-0.1in}
\end{figure}

\section{Conclusion}
%In this paper, we assert that the existing KD methods are suboptimal when the teacher model is MoE, and the main reason is that not all experts are fully utilized.
%To tackle this, we design two methods called knowledge augmentation and student-aware router, which are MoE-specific KD proposed for the first time.
%By using as many experts as possible, not simply but effectively, our methods become beneficial.
In this paper, we first observe that non-activated experts in MoE teachers contain valuable knowledge that can benefit the student model.
Based on this observation, we assert that existing KD methods are suboptimal for distilling MoE models, as they do not fully utilize all experts.
To address this issue, we propose two MoE-specific KD methods for the first time: knowledge augmentation and student-aware router.
Our experimental results show that our methods outperform the baseline, clearly demonstrating the effectiveness of our approach in leveraging the full potential of MoE teacher models.
%Experimental results show that our very first assertion is certainly true and our methods outperform the baseline, demonstrating the effectiveness of our approach.

\section*{Limitations}
% 1. 우리의 방법이 "dense2dense에 비해" MoE2dense에 특화된 점을 확인하기 위해, 실험 조건 상 1. teacher-student가 같은 tokenizer를 쓰고 2. teacher dense가 teacher moe와 성능이 비슷해야 되는데, 이 조건을 만족하는 실험 세팅이 너무 제한적임.
We acknowledge the limitations arising from the rigorous experimental conditions. In addition to the common yet imperfect situation where teacher and student must use the same tokenizer, dense teacher and MoE teacher should have comparable performances.
This condition is necessary to show that our method is an effective KD specialized for MoE.
However, it is difficult to find a setup that satisfies these conditions other than the setting that we used in our experiment (Llama-MoE~\citep{zhu2024llama} for the teacher and Sheared-Llama~\citep{xia2023sheared} for the student).
% 2. 이는 Future work로 남긴다.
We leave this for future work to explore, in conjunction with either emerging new methods~\citep{boizard2024towards, zhang2024dual} or by combining our method with existing ways~\citep{xue2022one}.
% 1번 citation. Cross-tokenizer KD
% 2번 citation. One-student-knows-all 에 합치는 방식
% 둘 다 MoE KD와 직접적인 관련은 없지만, "in conjunction with" 표현과 어울리는 방법임.

%%%% Motivation 부분에서의 Observation이 가장 중요하다고 생각한다. 우리가 고안한 2가지 방법이 아닌 다른 "모든 expert로 부터 학생에게 유용한 정보를 효율적으로 뽑는 방법"이 고안될 수 있다. 이를 Future work로 남긴다. %%%%



% Bibliography entries for the entire Anthology, followed by custom entries
%\bibliography{anthology,custom}
% Custom bibliography entries only
\bibliography{custom}

\appendix

\section{Experimental Setup Details}
\label{sec:appendix-setup-detail}

For training, we utilize the AdamW optimizer~\citep{loshchilov2017decoupled} with a batch size of 16. The learning rates for both the router and student models are set to 1e-5, and training is conducted for 10 epochs. The training and generation processes are conducted with a maximum sequence length of 512 and a maximum request length of 256. During generation, we apply top-k and top-p sampling with values of 0 and 1.0, respectively, while maintaining a fixed temperature of 1.0.
All experiments in this study are conducted on 4 Intel Gaudi v2 accelerators using SynapseAI 1.18.0.

To ensure consistency in instruction-following tasks, all datasets are pre-processed by converting instruction-response pairs into a standardized sentence structure, following the approach used in previous studies~\citep{gu2024minillm}. Model evaluation is performed using the ROUGE-L score~\citep{lin2004rouge}, which has been shown to correlate well with human preferences in instruction-following assessments~\citep{wang2022super}. The best-performing checkpoint on the validation set, determined by the ROUGE-L score, is selected for final evaluation. All evaluations are performed across five different random seeds, and the reported results reflect the average performance.


\section{Effects of $\lambda$ in KA}
\label{sec:appendix-KA-lambda}

\begin{table}[h]
\centering
\resizebox{\columnwidth}{!}{%
\begin{tabular}{c|cccccc}
\hline
$\lambda$ & Dolly & SelfInst & Vicuna & S-NI & UnNI & Average \\
\hline 				
0.01 & 26.23 & 17.55 & 18.10 & 31.19 & 35.52 & 25.72 \\
0.03& 26.35 & 17.15 & 18.65 & 29.90 & 34.68 & 25.35 \\
0.05& 26.58 & 16.98 & 18.38 & 30.51 & 36.11 & 25.71 \\
0.1 & 26.34 & 16.32 & 18.96 & 30.99 & 36.01 & 25.73 \\
0.2 & 26.49 & 16.24 & 18.88 & 29.99 & 34.08 & 25.14 \\
\hline
\end{tabular}
}
\caption{Instruction-following performance of KA across all test data for different values of $\lambda$, which represents the probability of sampling experts. Here, we only use Llama-MoE-3.5B (4/16) model and set $M=2$.}
\label{tab:ablation_lambda}
\vspace{-0.1in}
\end{table}

\begin{table*}[!t]
\centering
\begin{tabular}{l|p{0.8\linewidth}}
\hline
\multicolumn{2}{c}{Case 1} \\
\hline
Instruction & Convert a disfluent question to a proper question. A disfluent question is a question that has some interruptions in it while framing. A proper question is the correct form of the question without any disfluency. \\
\hline
Input & What do no I meant Who do some tax agents want to make an impression on during an arrest? \\
\hline
Ground Truth & Who do some tax agents want to make an impression on during an arrest? \\
\hline
\hline
SFT & Tax agents want to make an impression on during an arrest. \\
\hline
KD & An arrest is usually a life changing event. Ensure that the police officer Who makes the arrest understands that you do not do it on a conscious level. Use the hook offered to manually fill in the blank to make sure that the question sounds proper. \\
\hline
GKD & They want to make an impression on during an arrest that they will be able to collect from the tax payer on the charge of fraud. They want to make an impression that they will be able collect from the tax pater on the charge of fraude. \\
\hline
ALL & During an arrest, tax agents want to make an impression on who do some tax agents want \\
\hline
KA & The proper form of the question is ``Who do some tax agents wantto make and impression on during a arrest?". \\
\hline
SAR & I meant Who do some tax agent want to make and impression upon during an arrest? \\
\hline
\hline
\multicolumn{2}{c}{Case 2} \\
\hline
Instruction & Given an entity, a before event, an after event, and an attribute related to the entity, generate a sentence as output. Your sentence should show the changes in the attribute of the entity. \\
\hline
Input & entity: rope, before: separate from ball, after: through slot in ball, attr: location \\
\hline
Ground Truth & The location of the rope was separate from the ball before and through a slot in the ball afterwards. \\
\hline
\hline
SFT & Rope is strong because it can be wrapped around many things, including a ball. Rope can be wrapped around a ball, and this ability to be wrapped around things makes rope stronger. \\
\hline
KD & Rope is stronger when it is through the slot in the ball \\
\hline
GKD & rope: separation from ball, after: slot in ball, attr: location \\
\hline
ALL & Rope separates from ball.\textbackslash nRope through slot in ball.\textbackslash nAttr location changes. \\
\hline
KA & The rope has changed location. It was once separated from the ball. Now it is through the slot in the ball. \\
\hline
SAR & The rope was separated from the ball. The rope was through the slot in the ball. \\
\hline
\end{tabular}
\caption[Response examples on the S-NI dataset for student models distilled from MoE teacher.]{Response examples on the S-NI dataset for student models distilled from MoE teacher. Response examples from the S-NI dataset demonstrate that student models trained using our proposed MoE-specific KD methodologies (KA, SAR) follow instructions more accurately.}
\label{tab:moe_qualitative}
\vspace{-0.05in}
\end{table*}

Table~\ref{tab:ablation_lambda} shows the performance for different values of $\lambda$, which represent the probability of sampling experts.
%the probability of random expert sampling, $\lambda$.
In this experiment, we use the Llama-MoE-3.5B (4/16) model as a teacher and fix the value $M$, the number of augmented samples, as 2.
%Based on the results, there is no significant performance variation attributable to changes in the $\lambda$ value. While this deviates from the pattern observed in Figure~\ref{fig:effect_of_M}, we expect that further increments in the $\lambda$ value will likely induce a performance degradation due to the analogous reason.
The result indicates that too large $\lambda$ leads to performance degradation.
This result is similar to the pattern observed in Figure~\ref{fig:effect_of_M}, likely due to the analogous reason.
In other words, the proper value of $\lambda$ generally makes augmentation helpful, whereas the excessive value of $\lambda$ compromises the knowledge.


\section{Qualitative Results}
\label{sec:qualitative_results}

For the qualitative results, we present samples generated by student models trained using various methods. The samples are drawn from the S-NI dataset and utilize LLaMA-MoE-3.5B (4/16) as the teacher model, with Sheared-LLaMA-1.3B employed as the student model. Results are shown in Table~\ref{tab:moe_qualitative}. It is shown that our proposed methods generate responses most similar to the ground truth.

\end{document}
