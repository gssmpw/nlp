[
  {
    "index": 0,
    "papers": [
      {
        "key": "hinton2015distilling",
        "author": "Hinton, Geoffrey",
        "title": "Distilling the Knowledge in a Neural Network"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "song2020lightpaff",
        "author": "Song, Kaitao and Sun, Hao and Tan, Xu and Qin, Tao and Lu, Jianfeng and Liu, Hongzhi and Liu, Tie-Yan",
        "title": "LightPAFF: A two-stage distillation framework for pre-training and fine-tuning"
      },
      {
        "key": "liang2020mixkd",
        "author": "Liang, Kevin J and Hao, Weituo and Shen, Dinghan and Zhou, Yufan and Chen, Weizhu and Chen, Changyou and Carin, Lawrence",
        "title": "Mixkd: Towards efficient distillation of large-scale language models"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "jiao2020tinybert",
        "author": "Jiao, Xiaoqi and Yin, Yichun and Shang, Lifeng and Jiang, Xin and Chen, Xiao and Li, Linlin and Wang, Fang and Liu, Qun",
        "title": "TinyBERT: Distilling BERT for Natural Language Understanding"
      },
      {
        "key": "sun2019patient",
        "author": "Sun, Siqi and Cheng, Yu and Gan, Zhe and Liu, Jingjing",
        "title": "Patient Knowledge Distillation for BERT Model Compression"
      },
      {
        "key": "park-etal-2021-distilling",
        "author": "Park, Geondo  and\nKim, Gyeongman  and\nYang, Eunho",
        "title": "Distilling Linguistic Context for Language Model Compression"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "wang2020minilm",
        "author": "Wang, Wenhui and Wei, Furu and Dong, Li and Bao, Hangbo and Yang, Nan and Zhou, Ming",
        "title": "Minilm: Deep self-attention distillation for task-agnostic compression of pre-trained transformers"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "cho2019efficacy",
        "author": "Cho, Jang Hyun and Hariharan, Bharath",
        "title": "On the efficacy of knowledge distillation"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "park2021learning",
        "author": "Park, Dae Young and Cha, Moon-Hyun and Kim, Daesin and Han, Bohyung and others",
        "title": "Learning student-friendly teacher networks for knowledge distillation"
      },
      {
        "key": "zhou2022bert",
        "author": "Zhou, Wangchunshu and Xu, Canwen and McAuley, Julian",
        "title": "BERT Learns to Teach: Knowledge Distillation with Meta Learning"
      },
      {
        "key": "ren-etal-2023-tailoring",
        "author": "Ren, Yuxin  and\nZhong, Zihan  and\nShi, Xingjian  and\nZhu, Yi  and\nYuan, Chun  and\nLi, Mu",
        "title": "Tailoring Instructions to Student`s Learning Levels Boosts Knowledge Distillation"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "sanh2019distilbert",
        "author": "Sanh, V",
        "title": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "kim2016sequence",
        "author": "Kim, Yoon and Rush, Alexander M",
        "title": "Sequence-Level Knowledge Distillation"
      },
      {
        "key": "taori2023stanford",
        "author": "Taori, Rohan and Gulrajani, Ishaan and Zhang, Tianyi and Dubois, Yann and Li, Xuechen and Guestrin, Carlos and Liang, Percy and Hashimoto, Tatsunori B",
        "title": "Stanford alpaca: An instruction-following llama model"
      },
      {
        "key": "chiang2023vicuna",
        "author": "Chiang, Wei-Lin and Li, Zhuohan and Lin, Zi and Sheng, Ying and Wu, Zhanghao and Zhang, Hao and Zheng, Lianmin and Zhuang, Siyuan and Zhuang, Yonghao and Gonzalez, Joseph E and others",
        "title": "Vicuna: An open-source chatbot impressing gpt-4 with 90\\%* chatgpt quality, March 2023"
      },
      {
        "key": "peng2023instruction",
        "author": "Peng, Baolin and Li, Chunyuan and He, Pengcheng and Galley, Michel and Gao, Jianfeng",
        "title": "Instruction tuning with gpt-4"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "gu2024minillm",
        "author": "Gu, Yuxian and Dong, Li and Wei, Furu and Huang, Minlie",
        "title": "MiniLLM: Knowledge distillation of large language models"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "agarwal2024generalized",
        "author": "Agarwal, Rishabh and Vieillard, Nino and Zhou, Yongchao and Stanczyk, Piotr and Ramos, Sabela and Geist, Matthieu and Bachem, Olivier",
        "title": "Generalized knowledge distillation for auto-regressive language models"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "ko2024distillm",
        "author": "Ko, Jongwoo and Kim, Sungnyun and Chen, Tianyi and Yun, Se-Young",
        "title": "Distillm: Towards streamlined distillation for large language models"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "shazeer2017outrageously",
        "author": "Shazeer, Noam and Mirhoseini, Azalia and Maziarz, Krzysztof and Davis, Andy and Le, Quoc and Hinton, Geoffrey and Dean, Jeff",
        "title": "Outrageously large neural networks: The sparsely-gated mixture-of-experts layer"
      },
      {
        "key": "lepikhin2020gshard",
        "author": "Lepikhin, Dmitry and Lee, HyoukJoong and Xu, Yuanzhong and Chen, Dehao and Firat, Orhan and Huang, Yanping and Krikun, Maxim and Shazeer, Noam and Chen, Zhifeng",
        "title": "Gshard: Scaling giant models with conditional computation and automatic sharding"
      },
      {
        "key": "fedus2022switch",
        "author": "Fedus, William and Zoph, Barret and Shazeer, Noam",
        "title": "Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "he2022fastermoe",
        "author": "He, Jiaao and Zhai, Jidong and Antunes, Tiago and Wang, Haojie and Luo, Fuwen and Shi, Shangfeng and Li, Qin",
        "title": "Fastermoe: modeling and optimizing training of large-scale dynamic pre-trained models"
      },
      {
        "key": "gale2023megablocks",
        "author": "Gale, Trevor and Narayanan, Deepak and Young, Cliff and Zaharia, Matei",
        "title": "Megablocks: Efficient sparse training with mixture-of-experts"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "jiang2024mixtral",
        "author": "Jiang, Albert Q and Sablayrolles, Alexandre and Roux, Antoine and Mensch, Arthur and Savary, Blanche and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Hanna, Emma Bou and Bressand, Florian and others",
        "title": "Mixtral of experts"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "dai2024deepseekmoe",
        "author": "Dai, Damai and Deng, Chengqi and Zhao, Chenggang and Xu, RX and Gao, Huazuo and Chen, Deli and Li, Jiashi and Zeng, Wangding and Yu, Xingkai and Wu, Y and others",
        "title": "Deepseekmoe: Towards ultimate expert specialization in mixture-of-experts language models"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "artetxe2021efficient",
        "author": "Artetxe, Mikel and Bhosale, Shruti and Goyal, Naman and Mihaylov, Todor and Ott, Myle and Shleifer, Sam and Lin, Xi Victoria and Du, Jingfei and Iyer, Srinivasan and Pasunuru, Ramakanth and others",
        "title": "Efficient large scale language modeling with mixtures of experts"
      },
      {
        "key": "fedus2022switch",
        "author": "Fedus, William and Zoph, Barret and Shazeer, Noam",
        "title": "Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity"
      },
      {
        "key": "xue2022one",
        "author": "Xue, Fuzhao and He, Xiaoxin and Ren, Xiaozhe and Lou, Yuxuan and You, Yang",
        "title": "One student knows all experts know: From sparse to dense"
      },
      {
        "key": "guo2025deepseek",
        "author": "Guo, Daya and Yang, Dejian and Zhang, Haowei and Song, Junxiao and Zhang, Ruoyu and Xu, Runxin and Zhu, Qihao and Ma, Shirong and Wang, Peiyi and Bi, Xiao and others",
        "title": "Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "sanh2019distilbert",
        "author": "Sanh, V",
        "title": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter"
      }
    ]
  }
]