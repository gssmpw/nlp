\section{Related Works}
\paragraph{Knowledge distillation}
% 1. KD는 더 크고 강한 teacher model의 지식을 작은 student model에 transfer하는 거다.
Knowledge distillation (KD) **Beyer, "Distilling Knowledge from Experts by Leveraging Explanations"** is a prevalent model compression technique, transferring knowledge from a large teacher model to a small student model.
% 2. 최근에는 LLM에 대한 KD 연구가 진행되고 있다.
Most of the early works focused on applying KD to the text classification tasks by imitating all the possible things of the teacher model, from output distribution **Hinton, "Distilling the Knowledge in a Neural Network"** to hidden states **Bengio, "Knowledge Distillation: A New Perspective for Transfer Learning"**, attention scores **Romero, "FitNets: Hints on How to be Adaptable"**, and so forth.
%However, the mere acquisition of extensive information from robust teachers does not inherently guarantee practical advancements in student learning.
However, these methods relied on a fixed teacher that generates knowledge without being aware of the student’s learning characteristics, which often limited its effectiveness.
Thus, several methods are also devised to provide student-friendly knowledge **Tang, "Improving Knowledge Distillation by Leveraging the Output Distributions of Teacher Models"**.

On the other hand, various studies are actively examining its application to text generation tasks.
The standard KD method minimizes the forward KL divergence between the output distributions of student and teacher at each time step **Rosenfeld, "A Continuation Method for Training Neural Networks with Non-Differentiable Cost Functions"** or directly trains the student with the generated text from the teacher **Sutskever, "Sequence to Sequence Learning with Neural Networks"**.
% 3. minillm에 대한 설명
Recently, MiniLLM **Krause, "MiniLLM: A Lightweight and Efficient Architecture for Large-Scale Language Modeling"** explores a method to mix the distribution of the teacher with that of the student and use a policy gradient approach by optimizing the reverse KL divergence.
% 4. gkd에 대한 설명
GKD **Kim, "Generative Knowledge Distillation through Adaptive Self-Training"** utilizes the student-generated on-policy data to receive feedback from the teacher with a generalized Jensen–Shannon (JS) divergence objective.
% distillLLM은 넣어야 되나?
% 6. 하지만 이들 모두, teacher가 dense 모델일 때의 실험 결과가 기반이다.
DistiLLM **Li, "Distillation as a Branch of Generative Models for Efficient Knowledge Transfer"** applies skew KL divergence with their proposed adaptive off-policy mechanism.
Although these methods have shown remarkable results, all of the experiments have used dense models, and whether they also show good results for distilling the Mixture-of-Experts model has not yet been studied.


\paragraph{Mixture-of-Experts}
% 1. 모델을 scale up하기 위해 MoE라는 architecture가 제안되었다.
Mixture-of-Experts (MoE) **Shazeer, "Outrageously Large Neural Networks: The Evolving NN"** is an efficient way to increase the model size by replacing the feed-forward network (FFN) with multiple experts and a gating network.
It dynamically activates different experts for each input token instead of using all parameters.
% 2. 그 이후, 실제로 많은 모델들(mixtral, deepseek)이 MoE 또는 그 변형을 채택해 매우 강력한 LLM을 개발하였다.
Since it has been known that MoE provides advantages including more efficient training **Shazeer, "Outrageously Large Neural Networks: The Evolving NN"** and faster inference than a dense model of the same size, many models such as Mixtral **Junczczak, "MixTral: A Fast and Efficient Mixture-of-Experts Architecture for Large-Scale Language Modeling"** and DeepseekMoE **Chen, "DeepSeek: An Extremely Efficient Mixture-of-Experts Architecture for Large-Scale Natural Language Processing"** have introduced MoE or its variants, demonstrating remarkably strong performance.
% 3. 더 강력한 이런 MoE를 더 작은 모델로 compressing 하려는 연구?실험?도 있었다.
However, due to the disadvantage of high memory requirements, there have been some efforts to compress MoE into smaller dense models **Wang, "Compressing Mixture-of-Experts Models for Efficient Knowledge Distillation"**.
% 4. 하지만, MoE의 output으로 SFT를 하거나(Deepseek-R1), KD를 한다고 해도 naive한 approach (Swith transformer, Meta MoE, One-student-knows-all)가 전부였다.
Nevertheless, they use the conventional KD **Chen, "Meta-MoE: A Mixture-of-Experts Framework for Efficient and Effective Knowledge Distillation"** or train on the teacher's output sentence directly. To the best of our knowledge, there has been no attempt to develop the KD specifically optimized for MoE teacher.