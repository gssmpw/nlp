\section{Related work}
\label{Related work}

There are different design approaches for NER tools, which are trying to capture intricate language context in order to extract and recognize specific entity values. Entity meaning is usually tied to its surrounding context, which makes the rule based \cite{alfred2014malay, chiticariu2013rule}, and dictionary based methods \cite{hanisch2005prominer} less favourable in contrast to machine learning (ML) based approaches \cite{geng2017clinical, lee2020biobert, chalkidis2020legal} or hybrid architectures \cite{petasis2001using, nastou2024improving}.

Traditional NER methods were typically relying on handcrafted features capturing short-distance relations between the words in the sequence, and lacking the ability to consider bidirectional word relationships. As a result, they often fell short when dealing with complex linguistic scenarios where an entity's meaning reflects the surrounding context. Supervised NER solves a multi-class classification problem or a sequence labeling task, where each of the labeled training samples is represented by the corresponding feature vector, and the corresponding ML model is used to recognize new samples from the text. Depending on the classification model, there had been various learning approaches mainly based on sequence modelling capabilities of Hidden Markov Models (HMMs) \cite{bikel1997nymble}, Conditional Random Fields (CRFs) \cite{lafferty2001conditional, mccallumli2003early}.  Such approaches were usually relying on fixed word embeddings and limited length observation windows over tokens (a single words or subword units in the input text) for feature engineering, as well as decision trees \cite{szarvas2006multilingual} or a set of binary Support Vector Machines (SVMs) \cite{mcnamee2002entity} on the part of the learning task. A typical example of semi-supervised sequence modeling approach for NER is \cite{chiu2016named}, where the K-Nearest Neighbors (KNN) classifier is used for pre-labeling of tweet data, after which the CRF model is applied in the sequential manner in order to produce the final predicted labels sequence.

In order to capture non-trivial long-distance dependencies in word or token sequences, neural networks capable of processing variable length inputs, like the recurrent neural network (RNN) and the long-short term memory (LSTM) units with the forget gate, were applied to NER classification tasks, which brought a significant performance improvement over the previous approaches \cite{chiu2016named}. Most recently, the concepts of bi-directional LSTMs and CNNs that learn both character- and word-level features were further improved with the introduction of pre-trained transformer based bi-directional representations provided by BERT type \cite{kenton2019bert} language models. Such contextualized language-model embeddings, comprising of token position, segment and token embedding are usually characterized as hybrid representations.

The key for development of cost effective solutions for different NLP tasks is ability to exploit learned representations of input data (inherently learned by LLM pre-training) and perform low-resource model adaptation in domain specific downstream tasks. It was made possible by recent advancements in self-supervised training of LLM architectures that are designed in the style of encoder, decoder or encoder-decoder deep neural networks (DNNs). BERT \cite{kenton2019bert} or bidirectional encoder representations from transformers are particularly well suited for NER task due to self-attention mechanism, which means that the encoder considers the entire context (e.g. in total up to 512 tokens for sentence, or multiple sentences in the paragraph) when predicting the category for a specific token, including observations from the past and future (i.e. both preceding and following tokens), due to its bidirectional training and structure. On the other hand, decoder type PTMs, like GPT \cite{brown2020language}, are generally considered as less suitable for NER and similar NLP tasks like sentiment analysis and masked word prediction,  due to unidirectional structure of decoder type PTMs that is well suited for word prediction and NLP tasks involving text generation, like text summarization, text completition or machine type translation. This was made possible by proposal of various learning strategies that have significantly improved representation learning by exploiting vast corpora of unannotated data. These include word-level objectives like causal language modeling (CLM) in \cite{brown2020language}, masked language modeling (MLM) in \cite{kenton2019bert} and  its span-level generalization in \cite{joshi2020spanbert}, replaced (token detection) language modeling (RLM) in \cite{Clark2020ELECTRA}, or denoising language modeling (DLM) in \cite{lewis2020bart}. Similarly, their sentence-level counter parts like next sentence prediction (NSP) in \cite{kenton2019bert}, sentence order prediction (SOP) in \cite{Lan2020ALBERT} or sentence contrastive learning (SCL) in \cite{kim2021self}. However, it should be noted that PTM performance varies depending on the type of downstream task, as well as the implementation, as suggested by \cite{ryu2021re}, where it was shown that BERT type \cite{kenton2019bert} baseline performs better in comparison to ALBERT model \cite{Lan2020ALBERT} on NER tasks, despite improvements that were brought by \cite{Lan2020ALBERT} over \cite{kenton2019bert} (e.g. lower memory consumption and increased training speed, without the NSP strategy \cite{liu2019roberta}).

When it comes to PTM based solutions for Serbian NLP, besides BERTić \cite{ljubevsic2021bertic} and its derivatives for QA \cite{cvetanovic2023} and NER \cite{ljubesic2023}, notable works relying on other PTMs also include learned embedding models proposed in \cite{milutin2024} and \cite{zivanic2024}. Significant efforts were also put into Serbian specific NER solutions proposed in \cite{perisic2023sr ,todorovic2021serbian}, while \cite{ikonic2024bert} considers the problem of using Serbian specific BERT based PTMs instead of multilingual BERTs \cite{conneau2020unsupervised, kenton2019bert} or south Slavic BERT models \cite{ljubevsic2021bertic}. As pointed out by \cite{skoric24modeli}, in the recent period there have been several attempts of developing Serbian specific PTMs like the \cite{Jerteh355} PTM based on RoBERTa architecture\cite{liu2019roberta}. However, when it comes to downstream tasks, according to \cite{skoric24modeli} and \cite{ikonic2024bert}, NER solutions based on Serbian specific PTMs achieve similar performance to NER models fine-tuned on BERTić \cite{ljubevsic2021bertic}, as measured by NER experiments involving seven entities: demonyms (DEMO), professions and titles (ROLE), works of art (WORK), person names (PERS), places (LOC), events (EVENT) and organizations (ORG); defined in SrpELTeC dataset proposed in \cite{todorovic2021serbian}.