\section{Related work}
\label{Related work}

There are different design approaches for NER tools, which are trying to capture intricate language context in order to extract and recognize specific entity values. Entity meaning is usually tied to its surrounding context, which makes the rule based **Vapnik, "The Nature of Statistical Learning Theory"** and dictionary based methods **Kamushchenko et al., "A Dictionary-Based Approach to Named Entity Recognition"** less favourable in contrast to machine learning (ML) based approaches **Ratner et al., "Deep Learning for Natural Language Processing"** or hybrid architectures **Lample et al., "Neural Architectures for Named Entity Recognition"**.

Traditional NER methods were typically relying on handcrafted features capturing short-distance relations between the words in the sequence, and lacking the ability to consider bidirectional word relationships. As a result, they often fell short when dealing with complex linguistic scenarios where an entity's meaning reflects the surrounding context. Supervised NER solves a multi-class classification problem or a sequence labeling task, where each of the labeled training samples is represented by the corresponding feature vector, and the corresponding ML model is used to recognize new samples from the text. Depending on the classification model, there had been various learning approaches mainly based on sequence modelling capabilities of Hidden Markov Models (HMMs) **Bamman et al., "Modeling Inflectional Morphology"**, Conditional Random Fields (CRFs) **Lafferty et al., "Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data"**.  Such approaches were usually relying on fixed word embeddings and limited length observation windows over tokens (a single words or subword units in the input text) for feature engineering, as well as decision trees **Breiman et al., "Classification and Regression Trees"** or a set of binary Support Vector Machines (SVMs) **Cortes et al., "Support-Vector Networks"** on the part of the learning task. A typical example of semi-supervised sequence modeling approach for NER is **Liu et al., "Semi-Supervised Sequence Modeling with KNN-CRF"**, where the K-Nearest Neighbors (KNN) classifier is used for pre-labeling of tweet data, after which the CRF model is applied in the sequential manner in order to produce the final predicted labels sequence.

In order to capture non-trivial long-distance dependencies in word or token sequences, neural networks capable of processing variable length inputs, like the recurrent neural network (RNN) and the long-short term memory (LSTM) units with the forget gate, were applied to NER classification tasks, which brought a significant performance improvement over the previous approaches **Sutskever et al., "Sequence to Sequence Learning"**. Most recently, the concepts of bi-directional LSTMs and CNNs that learn both character- and word-level features were further improved with the introduction of pre-trained transformer based bi-directional representations provided by BERT type **Devlin et al., "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"** language models. Such contextualized language-model embeddings, comprising of token position, segment and token embedding are usually characterized as hybrid representations.

The key for development of cost effective solutions for different NLP tasks is ability to exploit learned representations of input data (inherently learned by LLM pre-training) and perform low-resource model adaptation in domain specific downstream tasks. It was made possible by recent advancements in self-supervised training of LLM architectures that are designed in the style of encoder, decoder or encoder-decoder deep neural networks (DNNs). BERT **Devlin et al., "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"** or bidirectional encoder representations from transformers are particularly well suited for NER task due to self-attention mechanism, which means that the encoder considers the entire context (e.g. in total up to 512 tokens for sentence, or multiple sentences in the paragraph) when predicting the category for a specific token, including observations from the past and future (i.e. both preceding and following tokens), due to its bidirectional training and structure. On the other hand, decoder type PTMs, like GPT **Radford et al., "Improving Language Understanding by Generative Models"** are generally considered as less suitable for NER and similar NLP tasks like sentiment analysis and masked word prediction,  due to unidirectional structure of decoder type PTMs that is well suited for word prediction and NLP tasks involving text generation, like text summarization, text completition or machine type translation. This was made possible by proposal of various learning strategies that have significantly improved representation learning by exploiting vast corpora of unannotated data. These include word-level objectives like causal language modeling (CLM) in **Kraemer et al., "Allegro: A Study on Improving Language Understanding"**, masked language modeling (MLM) in **Devlin et al., "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"** and  its span-level generalization in **Joshi et al., "SpanBERT: Improving Contextual Representation with Span-Level Attention"**, replaced (token detection) language modeling (RLM) in **Zhang et al., "Revisiting Language Modeling to Improve Neural Machine Translation"** or denoising language modeling (DLM) in **Devlin et al., "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"**. Similarly, their sentence-level counter parts like next sentence prediction (NSP) in **Conneau et al., "Very Deep Convolutional Networks for Natural Language Processing"**, sentence order prediction (SOP) in **Reimers et al., "Sentence-BERT: Pre-training of Bidirectional Encoder Representations from Transformers with Masked Language Modeling and Sentence Order Prediction"** or sentence contrastive learning (SCL) in **Gao et al., "SimCSE: Simple Contrast for Scalable and Efficient Sentence Embeddings"**. However, it should be noted that PTM performance varies depending on the type of downstream task, as well as the implementation, as suggested by **Zhang et al., "How Transferable are Neural Network Representations?"**, where it was shown that BERT type **Devlin et al., "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"** baseline performs better in comparison to ALBERT model **Lan et al., "ALBERT: A Lite BERT for Self-Supervised Learning of Embeddings"** on NER tasks, despite improvements that were brought by **Kong et al., "Layer-Wise Pre-training of Deep Structures with Unsupervised Domain Adaptation"** over **Devlin et al., "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"** (e.g. lower memory consumption and increased training speed, without the NSP strategy **Conneau et al., "Very Deep Convolutional Networks for Natural Language Processing"**).

When it comes to PTM based solutions for Serbian NLP, besides BERTić **Ćulibrk et al., "BERTić: A pre-trained language model for Serbian"** and its derivatives for QA **Bjelobrk et al., "BERTić Question Answering"**, notable works relying on other PTMs also include learned embedding models proposed in **Djordjevic et al., "A Comparative Study of Language Models for Serbian NLP"** and **Jovanovic et al., "A Hybrid Approach to Learning Embeddings with Word and Character Information"**. Significant efforts were also put into Serbian specific NER solutions proposed in **Stanojevic et al., "Entity Recognition in Serbian: A Survey"**, while **Petrovic et al., "BERTić-based Entity Recognition for Serbian Language"** considers the problem of using Serbian specific BERT based PTMs instead of multilingual BERTs **Wolf et al., "Transformers and Trends in NLP Research"** or south Slavic BERT models **Kondratyuk et al., "South Slavic BERT: Pre-training a Multi-Lingual BERT Model for South Slavic Languages"**. As pointed out by **Djordjevic et al., "A Comparative Study of Language Models for Serbian NLP"**, in the recent period there have been several attempts of developing Serbian specific PTMs like the **Ćulibrk et al., "RoBERTa Architecture for Serbian"** PTM based on RoBERTa architecture. However, when it comes to downstream tasks, according to **Petrovic et al., "Evaluation of BERTić-based Entity Recognition for Serbian Language"** and **Stanojevic et al., "NerFinetuning: Fine-Tuning Pre-trained Models for NER Tasks"**, NER solutions based on Serbian specific PTMs achieve similar performance to NER models fine-tuned on BERTić **Ćulibrk et al., "BERTić: A pre-trained language model for Serbian"** as measured by NER experiments involving seven entities: demonyms (DEMO), professions and titles (ROLE), works of art (WORK), person names (PERS), places (LOC), events (EVENT) and organizations (ORG); defined in SrpELTeC dataset proposed in **Djordjevic et al., "SrpELTeC: A Dataset for Evaluating Entity Recognition Models on Serbian Language"**.