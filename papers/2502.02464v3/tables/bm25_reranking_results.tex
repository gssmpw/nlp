
\begin{comment}
\begin{table*}[!ht]
\addtolength{\tabcolsep}{-0.65pt}
\small
\centering
\resizebox{0.6\textwidth}{!}{
\begin{tabular}{@{}l |l| c c c c c  | c c c c  c@{}}\toprule
Reranking/ &  Model  &  \multicolumn{5}{c}{NQ-Test} & \multicolumn{5}{c}{WebQ}  \\

& & Top-1 & Top-5 &Top-10 & Top-20 &Top-50 & Top-1 & Top-5 &Top-10 & Top-20 &Top-50   \\
\midrule
BM25      & - & 22.10 &  43.76 & 54.45 & 62.93 & 72.74 & 18.80  & 41.83  &  52.16 &   62.40 &  71.70 \\


\midrule

\multirow{9}{*}{UPR~\cite{sachan2022improving}}  %& T5-small& 23.60 & 49.44 & 59.97 & 67.70 & 75.15 &  18.55 &  43.65 &  55.56 & 64.91  &  72.68  \\


  & T5-base & 26.81 &  52.93 & 63.32 & 69.77 &  76.12 &  20.66 &  48.52 & 58.56 &  66.48 & 72.68  \\


  & T5-large&  29.75 & 55.84 &  65.67& 72.02 &  76.48 &  24.21 & 51.77 &   60.38 &  67.27 &73.32 \\


  & T0-3B & 35.42 & 60.44 & 67.56 & 72.68 & 76.75 &  32.48 &  56.39 &  64.17 &  69.09 &   73.67 \\
  %&gpt-neo-2.7B &28.75 &  55.01 & 64.81 & 71.13 &  76.56 &24.75   & 48.91  & 59.64 &  66.83 &  72.63  \\
    &gpt2 & 25.95 &  50.08 & 60.47  &  69.05 &  75.87 &  20.47 & 45.32  & 56.49 & 65.69  &  72.78 \\
& gpt2-medium &26.75 &  52.99 &  63.04 & 69.80 & 75.95 &  22.39 & 49.01  &  59.54 &  66.92 &  72.44 \\
&gpt2-large & 26.59 &  52.35 & 62.68 & 70.13 & 75.95 & 24.06  & 49.85  &  59.84 &  67.07 & 72.78  \\

  %& gpt2-xl & 27.28 & 53.85 & 63.24 & 70.11 & 75.84 &  23.57 & 50.78  & 60.48 &  66.87 &  72.73  \\

\midrule

 \multirow{1}{*}{RankGPT~\cite{rankgpt}} & llamav3.1-8b & 41.55 & 60.30 &  66.17 & 70.60 & 75.42 & 38.77 &  58.36 & 62.69  & 67.71 &  73.12   \\
 \midrule
 \multirow{5}{*}{FlashRank~\cite{Damodaran2024FlashRank}} & TinyBERT-L-2-v2& 31.49 & 53.90 &  61.57 & 68.33 & 74.95 & 28.54 & 51.87  &  60.62 & 67.42 & 73.17     \\
%& MiniLM-L-12-v2  &41.02 & 62.07 &  67.50 & 72.49 & 76.26 & 37.05 &  58.07 &  64.96 & 70.02 &  73.96  \\
  & MultiBERT-L-12 & 11.99&  31.74 & 43.54 &  55.01 &  69.63 &12.54   &  34.30 & 45.91 & 55.90  &   67.91\\
  %& T5-flan & 7.95 & 24.51 & 36.14 & 49.36 & 66.67 &   12.05 & 30.65  & 42.96 &  53.98  & 67.27  \\
  & ce-esci-MiniLM-L12-v2 & 34.70 &  57.47 & 64.81 & 70.60 &  76.17& 31.84  & 55.01  & 62.54 &68.60   &  73.47  \\
 \midrule
  
  \multirow{3}{*}{RankT5~\cite{zhuang2023rankt5}}&base & 43.04 &  62.99 & 68.47 & 72.99 &76.28  & 36.95  &  58.75 & 64.27 &  70.07 & 74.45  \\
  & large &45.54 & 64.98 &  70.02 &  73.85 & 76.81 &  38.77 &  60.28 & 66.48 &   70.96 &  74.31 \\
  &3b & 47.17& 66.45 &70.85  &  74.18 & 76.89 &   40.40 & 60.33  & 66.58 &71.16   & 74.45  \\


 %\midrule

 %\multirow{2}{*}{ListT5} & base & 8.31 & 23.10 & 30.55 & 37.09  & 65.73 &  10.58 & 26.96  & 35.43 & 42.61 & 65.20 \\
%& 3b &  7.59 & 21.32 & 27.89 & 33.93  & 65.01 & 9.35  & 25.39  &33.41 &  39.81 & 65.00 \\
\midrule
\multirow{3}{*}{Inranker~\cite{laitz2024inranker}}  & small & 15.90 & 40.11 &46.84 &56.23   &69.83  & 14.46  &  36.36 &46.25& 57.48 & 69.98 \\
&base  &15.90 & 40.69 & 48.11 & 56.62  &   69.66 &14.46   &   37.45 & 46.80 &  56.54 & 69.68 \\
&3b &15.90 & 40.22 & 48.06 & 57.97 &  69.00 &  14.46  & 37.30  & 46.11 &  57.48 & 69.34 \\
\midrule
\multirow{1}{*}{LLM2Vec~\cite{behnamghader2024llm2vec}} &Meta-Llama-31-8B & 24.32 & 49.22 & 59.55 & 68.08 & 75.26  &  26.72   &  50.39  &60.48&  68.40 &  73.47\\
\midrule
\multirow{1}{*}{MonoBert~\cite{monobert}} & large & 39.05 & 61.71 & 67.89 &  72.63 & 76.56 & 34.99  & 57.33  &64.56&  70.12 &  73.96 \\
\midrule
\multirow{1}{*}{Twolar~\cite{baldelli2024twolar}}& twolar-xl & 46.84 & 65.29 &  70.22 & 73.62  &  76.86 &41.68   & 60.82  &67.07& 71.55 &  74.40 \\
\midrule
\multirow{2}{*}{Echorank~\cite{rashid2024ecorank}}& flan-t5-large& 36.73 & 55.09 & 59.11 &  62.38 &  62.38 &  31.74 & 53.69  &58.75& 61.51 &61.51 \\

&flan-t5-xl&   41.68 & 55.15 & 59.05 & 62.38  & 62.38  & 36.22  & 52.55  & 57.18 &  61.51 &  61.51\\
\midrule

\multirow{2}{*}{\makecell[l]{Incontext\\ Reranker~\cite{chen2024attentionlargelanguagemodels}}} &   \multirow{2}{*}{llamav3.1-8b} & \multirow{2}{*}{15.15} & \multirow{2}{*}{38.50} & \multirow{2}{*}{57.11} & \multirow{2}{*}{69.66} & \multirow{2}{*}{76.48}  & \multirow{2}{*}{18.89}  &  \multirow{2}{*}{41.83} & \multirow{2}{*}{52.16} &  \multirow{2}{*}{62.40} & \multirow{2}{*}{71.70}\\
&&&&&&&&&&&\\
\midrule
\multirow{6}{*}{Lit5~\cite{tamber2023scaling}}&  LiT5-Distill-base  & 40.05 & 59.80 & 65.95 &  70.80 & 75.73  & 36.76  &  56.79 & 63.48 &  67.81 &  73.12 \\
&  LiT5-Distill-large  & 44.40 &  62.57 & 67.59 & 71.74 &76.01   &  39.66  &  58.02 &  64.56 &  68.94 &  73.67\\
&  LiT5-Distill-xl  &  47.81 &  64.07 &  68.55 &  72.07 & 76.26  &  42.37  & 59.44 &  65.55 & 69.38 & 73.62 \\
& LiT5-Distill-base-v2   &  42.57 & 61.49 &  66.73 &  70.83 & 75.56  &   39.61 & 57.97 & 64.22 &68.55&  73.32\\
&  LiT5-Distill-large-v2  & 46.53 &  63.10 &  67.83 &   71.91&  75.87  &  41.97  & 59.49 & 65.64 & 68.65 & 72.98 \\
&  LiT5-Distill-xl-v2  & 47.92 &  64.18 & 69.03 & 72.40 & 76.17  & 41.53 &  60.13 & 65.69 & 69.04 &73.27  \\
\midrule
%\multirow{1}{*}{Zephyr Reranker} &  Zephyr 7b  &  49.25 & 65.20 &    69.58 &  72.68  & 76.14  & &  &  & &  \\
%\midrule
\multirow{4}{*}{ \makecell[l]{Sentence \\ Transformer \\ Reranker} }
& GTR-base~\cite{ni2021large} & 39.41 &59.08  & 65.95 & 71.32 & 76.03  & 36.56  &58.21  & 64.32 &69.14 &73.62  \\

%&   all-MiniLM-L6-v2~\cite{wang2020minilmv2} & 33.35 & 57.34 & 65.37 & 70.72 &  76.01 & 30.95  & 53.44 & 62.10 & 68.01 & 73.52 \\

& GTR-large & 40.63 & 62.07 & 68.25 & 72.32 & 76.73  & 38.97  &  58.51 &  65.30 & 70.02 & 73.57 \\
%& GTR-xl   & 41.55 &61.82  & 67.78 &  72.65 & 76.81 &  38.92  &  59.69 &  66.04 & 70.57 & 74.01 \\

%& GTR-xxl   & 42.93 & 63.32 &  68.55 & 73.01 &  77.00 &39.41   & 60.03 & 65.89 & 70.47 & 74.01 \\

& T5-base~\cite{raffel2020exploring}   &  31.19 & 55.84 & 63.60 & 70.22 & 76.06  &  29.77 &  54.92 &  62.84 &68.70 & 73.52 \\

%&  T5-xxl  & 38.89 & 61.46 &  67.78 & 72.49 &76.64   &  35.82 & 58.26 & 65.20 & 70.32 & 74.01 \\

&  T5-large  & 30.80 & 55.59 & 63.35 & 70.02 &76.37   & 30.51  & 55.06 & 61.71 & 68.35 &  73.37 \\
           
%&   distilbert-multilingual& 17.34 & 41.13 & 52.68 & 62.85 & 73.04  &   18.01&  40.99 &  53.39 &  62.40 &  71.30 \\


%&  msmarco-bert-co-condensor  & 30.96 & 54.57 & 61.91 &68.55  &  75.20 &  32.43 &  54.57 &  62.20 & 67.66 & 73.08 \\
%&  msmarco-roberta-base-v2  & 32.60 & 55.92 & 63.24 &  69.47 & 75.42  &  31.34 &  56.05 &  62.64 & 68.60 &  73.37 \\
\midrule
%&    &  &  &  &  &   &   &  &  & &  \\
%&    &  &  &  &  &   &   &  &  & &  \\
%\midrule
\end{tabular}}
\caption{Performance of re-ranking methods on BM25-retrieved documents for NQ Test and WebQ Test, reported using Top-1, Top-5, Top-10, Top-20, and Top-50 accuracy. Some results may differ from original papers (e.g., UPR) as we used the top 100 retrieved documents, while the original studies used 1,000.
}
\label{tab:reranking-result}
\end{table*}

    
\end{comment}


\begin{table}[!ht]
\caption{Performance of re-ranking methods on BM25-retrieved documents for NQ Test and WebQ Test. Results are reported in terms of Top-1, Top-5, Top-10, Top-20, and Top-50 accuracy, highlighting the impact of various re-ranking models on retrieval effectiveness. Please note that some results may differ from the original papers (e.g., UPR) as our experiments were conducted with the top 100 retrieved documents, whereas the original studies used 1,000 documents for ranking.
}
%\addtolength{\tabcolsep}{-0.65pt}
\small
\centering
\resizebox{0.40\textwidth}{!}{
\begin{tabular}{@{}l |l| c c c  | c c c@{}}\toprule
Reranking/ &  Model  &  \multicolumn{3}{c}{NQ} & \multicolumn{3}{c}{WebQ}  \\

& & Top-1 & Top-10 & Top-50 & Top-1 & Top-10 & Top-50   \\
\midrule
BM25      & - & 23.46 & 56.32 & 74.57 & 19.54  & 53.44 & 72.34 \\

\midrule
 
\multirow{9}{*}{UPR~\cite{sachan2022improving}}  & T5-small& 23.60 & 59.97  & 75.15 &  18.55 &  55.56   &  72.68  \\

  & T5-base & 26.81 & 63.32 & 76.12 & 20.66 & 58.56 & 72.68  \\
  & T5-large& 29.75 & 65.67 & 76.48 & 24.21 & 60.38 & 73.32 \\
  & T0-3B & 35.42 & 67.56 & 76.75 & 32.48 & 64.17 & 73.67 \\
  & gpt2 & 25.95 & 60.47 & 75.87 & 20.47 & 56.49 & 72.78 \\
  & gpt2-medium &26.75 & 63.04 & 75.95 & 22.39 & 59.54 & 72.44 \\
  &gpt2-large & 26.59 &  62.68  & 75.95 & 24.06   &  59.84  & 72.78  \\

  & gpt2-xl & 27.28  & 63.24 & 75.84 &  23.57   & 60.48 &  72.73  \\
  &gpt-neo-2.7B &28.75  & 64.81  &  76.56 &24.75    & 59.64 &  72.63  \\


\midrule

 \multirow{1}{*}{RankGPT~\cite{rankgpt}} & llamav3.1-8b & 41.55 & 66.17 & 75.42 & 38.77 & 62.69 & 73.12 \\
 
\midrule

 \multirow{4}{*}{FlashRank~\cite{Damodaran2024FlashRank}} 
  & TinyBERT-L-2-v2& 31.49 & 61.57 & 74.95 & 28.54 & 60.62 & 73.17 \\
  & MultiBERT-L-12 & 11.99&  43.54 & 69.63 & 12.54  & 45.91 & 67.91 \\
  & ce-esci-MiniLM-L12-v2 & 34.70 & 64.81 & 76.17 & 31.84 & 62.54 & 73.47  \\
 & T5-flan & 7.95  & 36.14  & 66.67 &   12.05 & 42.96   & 67.27  \\
\midrule
  
  \multirow{3}{*}{RankT5~\cite{zhuang2023rankt5}}
  & base & 43.04 & 68.47 & 76.28 & 36.95 & 64.27 & 74.45  \\
  & large &45.54 & 70.02 & 76.81 & 38.77 & 66.48 & 74.31 \\
  &3b & 47.17& 70.85 & 76.89 & 40.40 & 66.58 & 74.45  \\

\midrule

\multirow{3}{*}{Inranker~\cite{laitz2024inranker}}  
& small & 15.90 & 46.84 & 69.83  & 14.46  & 46.25 & 69.98 \\
&base  &15.90 & 48.11 & 69.66 &14.46 & 46.80 & 69.68 \\
&3b &15.90 & 48.06 & 69.00 & 14.46 & 46.11 & 69.34 \\
\midrule

\multirow{1}{*}{LLM2Vec~\cite{behnamghader2024llm2vec}} 
&Meta-Llama-31-8B & 24.32 & 59.55 & 75.26  &  26.72 &60.48 & 73.47\\

\midrule

\multirow{1}{*}{MonoBert~\cite{monobert}} 
& large & 39.05 & 67.89 & 76.56 & 34.99 &64.56 & 73.96 \\

\midrule

\multirow{1}{*}{Twolar~\cite{baldelli2024twolar}} 
& twolar-xl & 46.84 & 70.22 & 76.86 &41.68 &67.07 &74.40 \\

\midrule

\multirow{2}{*}{Echorank~\cite{rashid2024ecorank}} 
& flan-t5-large& 36.73 & 59.11 & 62.38 & 31.74 &58.75 &61.51 \\
&flan-t5-xl&   41.68 & 59.05 & 62.38  & 36.22 &57.18 &61.51\\

\midrule


\multirow{2}{*}{\makecell[l]{Incontext\\ Reranker~\cite{chen2024attentionlargelanguagemodels}}} &  \multirow{2}{*}{llamav3.1-8b} & \multirow{2}{*}{15.15} & \multirow{2}{*}{57.11}  & \multirow{2}{*}{76.48}  & \multirow{2}{*}{18.89}   & \multirow{2}{*}{52.16} &  \multirow{2}{*}{71.70} \\
&&&&&&&\\


\midrule

\multirow{6}{*}{Lit5~\cite{tamber2023scaling}}
&  LiT5-Distill-base  & 40.05 & 65.95 & 75.73  & 36.76 & 63.48 & 73.12 \\
&  LiT5-Distill-large  & 44.40 & 67.59 & 76.01   &  39.66  &64.56 &73.67\\
&  LiT5-Distill-xl  &  47.81 & 68.55 & 76.26  &  42.37  &65.55 &73.62 \\
& LiT5-Distill-base-v2   &  42.57 &  66.73 & 75.56  & 39.61 &64.22 &73.32\\
&  LiT5-Distill-large-v2  & 46.53 &  67.83 & 75.87  & 41.97 &65.64 &72.98 \\
&  LiT5-Distill-xl-v2  & 47.92 & 69.03 & 76.17  & 41.53 &65.69 &73.27  \\

\midrule

\multirow{10}{*}{ \makecell[l]{Sentence \\ Transformer \\ Reranker} }
& GTR-base~\cite{ni2021large} & 39.41 &65.95 & 76.03  & 36.56 &64.32 &73.62  \\
& GTR-large & 40.63 &68.25 & 76.73  & 38.97 &65.30 &73.57 \\
& T5-base~\cite{raffel2020exploring} & 31.19 &63.60 & 76.06  & 29.77 &62.84 &73.52 \\
& T5-large & 30.80 &63.35 &76.37   & 30.51  &61.71 &73.37 \\
&   all-MiniLM-L6-v2~\cite{wang2020minilmv2} & 33.35  & 65.37  &  76.01 & 30.95   & 62.10  & 73.52 \\
& GTR-xl   & 41.55  & 67.78  & 76.81 &  38.92   &  66.04  & 74.01 \\
& GTR-xxl   & 42.93 &  68.55&  77.00 &39.41    & 65.89 & 74.01 \\
&  T5-xxl  & 38.89  &  67.78  &76.64   &  35.82  & 65.20  & 74.01 \\
&  Bert-co-condensor  & 30.96  & 61.91   &  75.20 &  32.43  &  62.20  & 73.08 \\
&  Roberta-base-v2  & 32.60  & 63.24  & 75.42  &  31.34  &  62.64 &  73.37 \\

\bottomrule           
\end{tabular}}

\label{tab:reranking-result}
\end{table}


%\vspace{-6pt}%