\section{Related Work}
\label{s:related_work}
Research in information retrieval (IR), re-ranking, and retrieval-augmented generation (RAG) has progressed significantly over the past decade. Traditional retrieval models like BM25~\cite{bm25} offered robust lexical matching capabilities but struggled with capturing semantic relationships. This limitation led to the development of dense retrieval methods~\cite{lee-etal-2019-latent}, which leverage pre-trained neural encoders to represent queries and documents in a shared semantic space. Notable approaches, such as DPR~\cite{dpr}, ANCE~\cite{ance}, and multi-vector models like ColBERT~\cite{khattab2020colbertefficienteffectivepassage}, have demonstrated substantial improvements in retrieval effectiveness. Hybrid retrievers, combining sparse and dense signals~\cite{DBLP:conf/ecir/GaoDCFDC21,ma2021replication}, further enhance performance by leveraging both lexical and semantic features. Recent advancements, including knowledge distillation~\cite{qu-etal-2021-rocketqa} and curriculum learning~\cite{cldrd}, continue to refine retrieval performance across diverse datasets.

Re-ranking methods have also evolved alongside retrieval techniques to improve the ordering of retrieved documents. Traditional pointwise~\cite{zhu2021leveraging} and pairwise~\cite{cao2007learning} approaches have given way to listwise methods like LambdaRank and ListNet~\cite{burges2010ranknet,liu2017listnet}. Deep neural models, including cross-encoders and transformer-based architectures, have demonstrated remarkable success in re-ranking tasks by capturing complex interactions between queries and documents. Zero-shot and in-context re-ranking with large language models (LLMs), such as GPT-4~\cite{achiam2023gpt} and RankT5~\cite{zhuang2023rankt5}, now enable effective ranking adjustments without task-specific training.

Retrieval-Augmented Generation (RAG) has emerged as a powerful paradigm for enhancing generative models in knowledge-intensive tasks~\cite{lewis2020retrieval}. By retrieving relevant documents and integrating them into the generative process, RAG systems improve factual accuracy and reduce hallucinations. Techniques like self-consistency~\cite{wang2022self} and noise filtering~\cite{fang2024enhancing} have been proposed to further improve the reliability of RAG outputs. However, the effectiveness of these systems heavily depends on the quality of retrieved and re-ranked documents, highlighting the need for robust retrieval frameworks.

In response to the growing complexity of IR, re-ranking, and RAG tasks, several frameworks have been introduced. \textit{Rerankers}~\cite{clavi2024rerankers} provides a lightweight Python interface for common re-ranking models, while \textit{RankLLM} focuses on listwise re-ranking with LLMs. Other frameworks like \textit{FlashRAG}~\cite{jin2024flashrag} and \textit{AutoRAG}~\cite{kim2024autoragautomatedframeworkoptimization} offer modular components for RAG experimentation, though they often lack support for diverse datasets and advanced retriever configurations. Tools such as LangChain~\cite{chase2022langchain}, LlamaIndex~\cite{Liu_LlamaIndex_2022}, and DSPy~\cite{khattab2023dspy} further contribute to the ecosystem by simplifying model integration and workflow design.  A comparison of retrieval, re-ranking, and RAG toolkits is presented in Table~\ref{tab:framework_comparison}, highlighting the advantages of \framework in dataset diversity, retriever and re-ranker support, and modularity.




\begin{figure}[t]
  \centering
  \includegraphics[width=0.90\columnwidth]{images/overview.drawio.pdf}
  \caption{The architecture of the \framework, showing the interplay between its core modules: \textit{Datasets}, \textit{Retrievers}, \textit{Re-Rankers}, and \textit{RAG Evaluation}. Each module operates independently while seamlessly integrating with others, enabling end-to-end retrieval and ranking workflows.}
  \label{fig:rankify_framework}
\end{figure}


\begin{comment}