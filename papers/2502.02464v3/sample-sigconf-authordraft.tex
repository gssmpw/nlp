%%
%% This is file `sample-sigconf-authordraft.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `all,proceedings,bibtex,authordraft')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-sigconf-authordraft.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%%
%% Commands for TeXCount
%TC:macro \cite [option:text,text]
%TC:macro \citep [option:text,text]
%TC:macro \citet [option:text,text]
%TC:envir table 0 1
%TC:envir table* 0 1
%TC:envir tabular [ignore] word
%TC:envir displaymath 0 word
%TC:envir math 0 word
%TC:envir comment 0 0
%%
%% The first command in your LaTeX source must be the \documentclass
%% command.
%%
%% For submission and review of your manuscript please change the
%% command to \documentclass[manuscript, screen, review]{acmart}.
%%
%% When submitting camera ready or to TAPS, please change the command
%% to \documentclass[sigconf]{acmart} or whichever template is required
%% for your publication.natbib=true,
%%
%%
\documentclass[sigconf,natbib=true,final]{acmart}

%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
\setcopyright{acmlicensed}
\copyrightyear{2018}
\acmYear{2018}
\acmDOI{XXXXXXX.XXXXXXX}
%% These commands are for a PROCEEDINGS abstract or paper.
\acmConference[SIGIR '25]{Make sure to enter the correct
  conference title from your rights confirmation emai}{July 13--18,
2025}{Padova, IT}
%%
%%  Uncomment \acmBooktitle if the title of the proceedings is different
%%  from ``Proceedings of ...''!
%%
%%\acmBooktitle{Woodstock '18: ACM Symposium on Neural Gaze Detection,
%%  June 03--05, 2018, Woodstock, NY}
\acmISBN{978-1-4503-XXXX-X/18/06}


%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% For managing citations, it is recommended to use bibliography
%% files in BibTeX format.
%%
%% You can then either use BibTeX with the ACM-Reference-Format style,
%% or BibLaTeX with the acmnumeric or acmauthoryear sytles, that include
%% support for advanced citation of software artefact from the
%% biblatex-software package, also separately available on CTAN.
%%
%% Look at the sample-*-biblatex.tex files for templates showcasing
%% the biblatex styles.
%%

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}
\usepackage{xspace}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{pifont}
\usepackage{makecell}
\usepackage{xspace}
\usepackage{pifont}  % For \xmark
\newcommand{\xmark}{\ding{55}} % Cross symbol from pifont
\newcommand{\blackcircle}[1]{%
    \raisebox{0.8pt}{\textcircled{\raisebox{-0.8pt}{#1}}}
}
\definecolor{codegreen}{RGB}{93, 168, 128} % Light pleasant blue
\definecolor{codeblue}{RGB}{74, 74, 250}  % Pleasant light green
\definecolor{codered}{RGB}{209, 106, 114}
\definecolor{text}{RGB}{68,114,157}
\usepackage{listings}
\usepackage{xcolor}

% Define new colors
\definecolor{codepurple}{rgb}{0.58,0,0.82} % Purple for keywords
\definecolor{codeorange}{rgb}{0.85,0.33,0.1} % Orange for strings
\definecolor{codeteal}{rgb}{0,0.5,0.5} % Teal for comments
\definecolor{codebackground}{rgb}{0.95,0.98,1} % Light blue background

\lstset{
  language=Python,
  basicstyle=\scriptsize\ttfamily, % Small and compact font
  keywordstyle=\bfseries\color{codepurple}, % Purple bold keywords
  stringstyle=\color{codeorange}, % Orange strings
  commentstyle=\itshape\color{codeteal}, % Teal italic comments
  backgroundcolor=\color{codebackground}, % Light blue background
  breaklines=true,
  showstringspaces=false,
  tabsize=6,
  captionpos=t, % Caption at the top
  numbers=left, % Line numbers on the left
  numbersep=6pt, % Space between code and line numbers
  xleftmargin=10pt, % Aligns code with numbers
  xrightmargin=5pt, % Small right margin
  lineskip=0.5pt, % Keeps spacing compact
}





\newcommand{\framework}{\textsc{Rankify}\xspace}
%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
%\title[Rankify: A Comprehensive Python Toolkit for Retrieval, Re-Ranking, and RAG]{Rankify \raisebox{-0.5ex}{\includegraphics[width=0.05\textwidth]{images/logo-3-modified.png}} : A Comprehensive Python Toolkit  for Retrieval, Re-Ranking, and Retrieval-Augmented Generation
%}
\title[Rankify: A Comprehensive Python Toolkit for Retrieval, Re-Ranking, and RAG]{Rankify: A Comprehensive Python Toolkit  for Retrieval, Re-Ranking, and Retrieval-Augmented Generation}
%}
%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.

\author{Abdelrahman Abdallah}
\orcid{0000-0001-8747-4927}
\affiliation{%
  \institution{University of Innsbruck}
  \city{Innsbruck}
  \state{Tyrol}
  \country{Austria}
  }
\email{abdelrahman.abdallah@uibk.ac.at}

\author{Bhawna Piryani}
\orcid{0009-0005-3578-2393}
\affiliation{%
  \institution{University of Innsbruck}
  \city{Innsbruck}
  \state{Tyrol}
  \country{Austria}
  }
\email{bhawna.piryani@uibk.ac.at}


\author{Jamshid Mozafari}
\orcid{0000-0003-4850-9239}
\affiliation{%
  \institution{University of Innsbruck}
 \city{Innsbruck}
  \state{Tyrol}
  \country{Austria}
  }
\email{jamshid.mozafari@uibk.ac.at}



\author{Mohammed Ali}
\affiliation{%
  \institution{University of Innsbruck}
  \city{Innsbruck}
  \state{Tyrol}
  \country{Austria}
  }
\email{Mohammed.ali@uibk.ac.at	}

\author{Adam Jatowt}
\orcid{0000-0001-7235-0665}
\affiliation{%
  \institution{University of Innsbruck}
  \city{Innsbruck}
 \state{Tyrol}
  \country{Austria}
  }
\email{adam.jatowt@uibk.ac.at}


%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
\renewcommand{\shortauthors}{Abdallah et al.}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.

\begin{abstract}
Retrieval, re-ranking, and retrieval-augmented generation (RAG) are critical components of modern applications in information retrieval, question answering, or knowledge-based text generation. However, existing solutions are often fragmented, lacking a unified framework that easily integrates these essential processes. The absence of a standardized implementation, coupled with the complexity of retrieval and re-ranking workflows, makes it challenging for researchers to compare and evaluate different approaches in a consistent environment. While existing toolkits such as Rerankers and RankLLM provide general-purpose reranking pipelines, they often lack the flexibility required for fine-grained experimentation and benchmarking.  In response to these challenges, we introduce \textbf{\framework}, a powerful and modular open-source toolkit designed to unify retrieval, re-ranking, and RAG within a cohesive framework. \framework supports a wide range of retrieval techniques, including dense and sparse retrievers, while incorporating state-of-the-art re-ranking models to enhance retrieval quality. Additionally, \framework includes a collection of pre-retrieved datasets to facilitate benchmarking, available at Huggingface\footnote{\url{https://huggingface.co/datasets/abdoelsayed/reranking-datasets-light}}. To encourage adoption and ease of integration, we provide comprehensive documentation\footnote{\url{http://rankify.readthedocs.io/}}, an open-source implementation on GitHub\footnote{\url{https://github.com/DataScienceUIBK/rankify}}, and a PyPI package for easy installation\footnote{\url{https://pypi.org/project/rankify/}}. As a unified and lightweight framework, \framework allows researchers and practitioners to advance retrieval and re-ranking methodologies while ensuring consistency, scalability, and ease of use.
\end{abstract}
%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%
\begin{CCSXML}
<ccs2012>
<concept>
<concept_id>10002951.10003317</concept_id>
<concept_desc>Information systems~Information retrieval</concept_desc>
<concept_significance>500</concept_significance>
</concept>
<concept>
<concept_id>10002951.10003317.10003359</concept_id>
<concept_desc>Information systems~Evaluation of retrieval results</concept_desc>
<concept_significance>500</concept_significance>
</concept>
<concept>
<concept_id>10002951.10003317.10003359.10011699</concept_id>
<concept_desc>Information systems~Presentation of retrieval results</concept_desc>
<concept_significance>500</concept_significance>
</concept>
<concept>
<concept_id>10002951.10003317.10003338.10003346</concept_id>
<concept_desc>Information systems~Top-k retrieval in databases</concept_desc>
<concept_significance>500</concept_significance>
</concept>
<concept>
<concept_id>10002951.10003317.10003338.10003339</concept_id>
<concept_desc>Information systems~Rank aggregation</concept_desc>
<concept_significance>500</concept_significance>
</concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Information systems~Information retrieval}
\ccsdesc[500]{Information systems~Evaluation of retrieval results}
\ccsdesc[500]{Information systems~Presentation of retrieval results}
\ccsdesc[500]{Information systems~Top-k retrieval in databases}
\ccsdesc[500]{Information systems~Rank aggregation}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{Neural IR, Dense Retrieval, Sparse Retrieval, Re-ranking, Toolkit }
%% A "teaser" image appears between the author and affiliation
%% information and the body of the document, and typically spans the
%% page.

\received{20 February 2007}
\received[revised]{12 March 2009}
\received[accepted]{5 June 2009}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\begin{figure}[!t]
  \centering
  \includegraphics[width=0.25\columnwidth]{images/rankify-crop.png}
  \caption{\framework logo.}
  \label{fig:rankify_logo}
\end{figure}

\section{Introduction}

Information retrieval (IR) systems~\cite{singhal2001modern,chowdhury2010introduction} are fundamental to many  applications~\cite{wang2024utilizing,liu2024information}, including question-answering~\cite{khamnuansin2024mrrank,abdallah-etal-2025-dynrank}, search engines~\cite{croft2010search,xiong2024search}, and knowledge-based generation~\cite{long2024generative}. These systems often rely on a two-stage pipeline: a retriever that efficiently identifies a set of candidate documents and a re-ranker that refines these results to maximize relevance to the query~\cite{dpr,monobert,rocketqav2}. This approach has proven highly effective, with retrieval and re-ranking methods achieving state-of-the-art performance across diverse NLP benchmarks. 

\begin{table*}[ht]
\caption{Comparison of Retriever, Re-ranking, and RAG toolkits. Modular Design indicates if the toolkit uses modular components. Automatic Evaluation refers to the availability of built-in evaluation tools. Corpus shows whether the toolkit provides utilities for corpus processing, such as cleaning and chunking. The Combination column represents the total number of possible configurations, calculated as the product of the number of datasets, retrievers, re-rankers, and RAG methods (e.g., for \framework: 40 × 7 × 24 × 3 = 20,160).}
\centering
\resizebox{1.0\textwidth}{!}{
\begin{tabular}{lcccccccc}
\toprule
\textbf{Toolkit} & \textbf{\# Datasets} & \textbf{\# Retrievers} & \textbf{\# Re-rankers} & \textbf{\# RAG Methods} & \textbf{Modular Design} & \textbf{Automatic Evaluation} & \textbf{Corpus} & \textbf{Combination}\\
\midrule
FlashRAG~\cite{jin2024flashrag} & 32 & 3 & 2 & 12 & \checkmark & \checkmark & \checkmark&2,304 \\
%DSPy~\cite{turn0search9} & 0 & 0 & 0 & 0 & \checkmark & \checkmark & \xmark \\
FastRAG~\cite{abane2024fastrag} & 0 & 1 & 3 & 7 & \checkmark & \xmark & \xmark&  21\\
AutoRAG~\cite{kim2024autoragautomatedframeworkoptimization} & 4 & 2 & 7 & 1 & \checkmark & \checkmark & \xmark&56 \\
LocalRQA~\cite{yu2024localrqa} & 0 & 2 & 0 & 0 & \xmark & \checkmark & \xmark & 2\\
Rerankers~\cite{clavi2024rerankers} & 0 & 0 & 15 & 0 & \checkmark & \checkmark & \xmark&15 \\
CHERCHE~\cite{sourty2022cherche} & 0 & 7 & 2 & 0 & \xmark & \xmark & \xmark & 14\\
\textbf{\framework} & \textbf{40} & \textbf{7} & \textbf{24} & \textbf{3} & \checkmark & \checkmark & \checkmark &20,160\\
\bottomrule
\end{tabular}
}

\label{tab:framework_comparison}
\end{table*}
Retrievers form the backbone of information retrieval systems by identifying a subset of documents relevant to a user's query. The retrieval landscape includes sparse, dense, and hybrid methods. Sparse retrievers, such as BM25~\cite{bm25}, rely on exact term matching by representing queries and documents as high-dimensional sparse vectors. These methods are highly effective when query terms closely align with document content but they struggle to capture semantic relationships. Dense retrievers, including models like DPR~\cite{dpr}, BPR~\cite{yamada2021bpr}, ColBERT~\cite{santhanam2021colbertv2}, BGE~\cite{bge_m3}, Contriever~\cite{izacard2021contriever} and ANCE~\cite{ance}, overcome this limitation by encoding queries and documents into low-dimensional dense vectors using neural networks, enabling retrieval based on semantic similarity even when lexical overlap is minimal. However, dense retrieval requires significant computational resources for training and inference. Hybrid retrieval methods~\cite{yang2019hybrid,chen2022out} integrate the strengths of both approaches, combining the precision of sparse retrievers with the semantic understanding of dense models to achieve a more balanced and robust retrieval system.






Re-ranking techniques enhance the initial retrieval results by ensuring the most relevant documents appear at the top. These methods are categorized into three main approaches:
(1) Pointwise reranking~\cite{monobert,abdallah-etal-2025-dynrank,abdallah2025asrankzeroshotrerankinganswer,sachan2022improving} treats reranking as a regression or classification task, assigning independent relevance scores to each document.
(2) Pairwise reranking~\cite{huang2024pairdistill,qin2023large,sinhababu2024few} refines ranking by comparing document pairs and optimizing their relative order based on relevance. However, this approach treats all document pairs equally, sometimes improving lower-ranked results at the expense of top-ranked ones.
(3) Listwise reranking~\cite{rankgpt,pradeep2023rankzephyr,yoon2024listt5listwisererankingfusionindecoder} considers the entire document list, prompting LLMs with the query and a subset of candidates for reranking. Due to input length limitations, these methods tend to employ a sliding window strategy, progressively refining rankings from back to front.

Retrieval-Augmented Generation (RAG)~\cite{lewis2020retrieval,gao2023retrieval} enhances language models by integrating retrieval and generation, making them more effective in knowledge-intensive tasks. Rather than relying solely on pre-trained knowledge, RAG dynamically retrieves relevant documents from external sources during inference, incorporating them into the generation process. 

The growing complexity and diversity of retrieval, re-ranking, and RAG methods pose significant challenges for benchmarking, reproducibility, and integration. Existing toolkits, such as Pyserini~\cite{Lin_etal_SIGIR2021_Pyserini}, Rerankers~\cite{clavi2024rerankers} and RankLLM~\cite{pradeep2023rankvicunazeroshotlistwisedocument} often lack flexibility, enforce rigid implementations, and require extensive preprocessing, making them less suitable for research-driven experimentation. Additionally, retrieval and re-ranking datasets are scattered across different sources, complicating evaluation and comparison. To address these challenges, we introduce \framework, an open-source framework that unifies retrieval, re-ranking, and RAG into a modular and extensible ecosystem (logo shown in Figure \ref{fig:rankify_logo}). \framework supports diverse retrieval techniques, integrates the state-of-the-art re-ranking models, and provides curated pre-retrieved datasets to streamline experimentation. Designed for maximizing flexibility, it enables researchers to efficiently build, evaluate, and extend retrieval pipelines while ensuring consistency in benchmarking. %With its lightweight design, comprehensive documentation, and easy installation.


The main contributions of this paper are as follows:


\begin{comment}
    
\begin{itemize} 

\item \textbf{Curated dataset collection for retrieval benchmarking:} \framework introduces a comprehensive dataset collection tailored for diverse retrieval tasks. Each dataset includes 1,000 pre-retrieved documents per query, covering 40 datasets across various domains such as question answering, Temporal QA, dialogue generation, entity linking, fact verification, long-form QA, multi-hop QA, multiple-choice QA, open-domain summarization, and slot filling.

\item \textbf{Integrated retriever implementations:} \framework integrates multiple retrieval methods, including dense retrievers (DPR, ANCE, BPR, ColBERT, BGE, and Contriever) and sparse retrievers (BM25), within a single framework. Researchers can retrieve relevant documents with minimal effort, requiring only two lines of code, enabling efficient and reproducible retrieval experiments.

\item \textbf{Comprehensive re-ranking models:} \framework provides 24 state-of-the-art re-ranking models, allowing users to easily load, apply, and swap between different methods without modifying their codebase. 

\item \textbf{RAG integration for evaluation:} \framework bridges retrieval, re-ranking, and RAG pipelines by allowing retrieved documents to be directly passed to large language models for evaluation and answer extraction. This integration facilitates comprehensive benchmarking and improves retrieval performance in knowledge-intensive NLP tasks.
%Pre-computed retrieval corpora with ready-to-use search indices:
\item \textbf{ Passage embeddings pre-computed with different retrievers: }  \framework includes two widely used retrieval corpora—Wikipedia and MS MARCO—allowing researchers to perform retrieval experiments on well-established benchmarks without the need for additional preprocessing. Additionally, each corpus is pre-encoded for every supported retriever, providing a ready-to-use index.
\item \textbf{Comprehensive evaluation tools and accessibility:} \framework offers a diverse range of evaluation metrics and tools for retrieval and question answering. The framework is accompanied by extensive online documentation, making it easy for users to explore its features. Additionally, \framework is freely available on PyPI and GitHub, ensuring accessibility for researchers and practitioners.
\end{itemize}
\end{comment}

\begin{itemize} 

\item \textbf{Curated retrieval datasets and precomputed embeddings:} \framework provides 40 datasets, each with 1,000 pre-retrieved documents per query, across various domains (QA, dialogue, entity linking, etc.). It also includes pre-computed Wikipedia and MS MARCO corpora for multiple retrievers, eliminating preprocessing overhead.

\item \textbf{Diverse retriever and re-ranking support:} \framework integrates dense (DPR, ANCE, BPR, ColBERT, BGE, Contriever) and sparse (BM25) retrievers, along with 24 state-of-the-art re-ranking models, enabling seamless retrieval and ranking experimentation.

\item \textbf{RAG integration and evaluation tools:} \framework bridges retrieval, re-ranking, and RAG by passing retrieved documents to LLMs for evaluation. 

\item  \textbf{Comprehensive evaluation tools:} \framework offers a diverse range of evaluation metrics and tools for retrieval and question answering. The framework is accompanied by extensive online documentation, making it easy for users to explore its features. Additionally, \framework is freely available on PyPI and GitHub, ensuring accessibility for researchers and practitioners.

\end{itemize}


\begin{figure*}
    \centering
    \includegraphics[width=0.8\linewidth]{images/Presentation1.pdf}
    \caption{An overview of the Rankify pipeline, demonstrating its dual capability for document retrieval. Users can interact with the system either by providing a query to retrieve relevant documents in real-time or by leveraging pre-retrieved datasets already indexed by the framework. The process starts with Pre-Retrieved Datasets \& Corpus Indexing, where documents are indexed using both dense (e.g., DPR) and sparse (e.g., BM25) retrievers across corpus like Wikipedia and MS MARCO. Next, in the Retrieval \& Re-Ranking stage, the system retrieves candidate documents using dense, or sparse retrieval methods and re-ranks them with pointwise pairwise, or listwise models powered by large language models (LLMs). Finally, the Generator stage applies prompt augmentation and uses models like Fusion-in-Decoder (FiD) to generate accurate and contextually informed answers.}
    \label{fig:workflow}
\end{figure*}

\section{Related Work}\label{s:related_work}
Research in information retrieval (IR), re-ranking, and retrieval-augmented generation (RAG) has progressed significantly over the past decade. Traditional retrieval models like BM25~\cite{bm25} offered robust lexical matching capabilities but struggled with capturing semantic relationships. This limitation led to the development of dense retrieval methods~\cite{lee-etal-2019-latent}, which leverage pre-trained neural encoders to represent queries and documents in a shared semantic space. Notable approaches, such as DPR~\cite{dpr}, ANCE~\cite{ance}, and multi-vector models like ColBERT~\cite{khattab2020colbertefficienteffectivepassage}, have demonstrated substantial improvements in retrieval effectiveness. Hybrid retrievers, combining sparse and dense signals~\cite{DBLP:conf/ecir/GaoDCFDC21,ma2021replication}, further enhance performance by leveraging both lexical and semantic features. Recent advancements, including knowledge distillation~\cite{qu-etal-2021-rocketqa} and curriculum learning~\cite{cldrd}, continue to refine retrieval performance across diverse datasets.

Re-ranking methods have also evolved alongside retrieval techniques to improve the ordering of retrieved documents. Traditional pointwise~\cite{zhu2021leveraging} and pairwise~\cite{cao2007learning} approaches have given way to listwise methods like LambdaRank and ListNet~\cite{burges2010ranknet,liu2017listnet}. Deep neural models, including cross-encoders and transformer-based architectures, have demonstrated remarkable success in re-ranking tasks by capturing complex interactions between queries and documents. Zero-shot and in-context re-ranking with large language models (LLMs), such as GPT-4~\cite{achiam2023gpt} and RankT5~\cite{zhuang2023rankt5}, now enable effective ranking adjustments without task-specific training.

Retrieval-Augmented Generation (RAG) has emerged as a powerful paradigm for enhancing generative models in knowledge-intensive tasks~\cite{lewis2020retrieval}. By retrieving relevant documents and integrating them into the generative process, RAG systems improve factual accuracy and reduce hallucinations. Techniques like self-consistency~\cite{wang2022self} and noise filtering~\cite{fang2024enhancing} have been proposed to further improve the reliability of RAG outputs. However, the effectiveness of these systems heavily depends on the quality of retrieved and re-ranked documents, highlighting the need for robust retrieval frameworks.

In response to the growing complexity of IR, re-ranking, and RAG tasks, several frameworks have been introduced. \textit{Rerankers}~\cite{clavi2024rerankers} provides a lightweight Python interface for common re-ranking models, while \textit{RankLLM} focuses on listwise re-ranking with LLMs. Other frameworks like \textit{FlashRAG}~\cite{jin2024flashrag} and \textit{AutoRAG}~\cite{kim2024autoragautomatedframeworkoptimization} offer modular components for RAG experimentation, though they often lack support for diverse datasets and advanced retriever configurations. Tools such as LangChain~\cite{chase2022langchain}, LlamaIndex~\cite{Liu_LlamaIndex_2022}, and DSPy~\cite{khattab2023dspy} further contribute to the ecosystem by simplifying model integration and workflow design.  A comparison of retrieval, re-ranking, and RAG toolkits is presented in Table~\ref{tab:framework_comparison}, highlighting the advantages of \framework in dataset diversity, retriever and re-ranker support, and modularity.




\begin{figure}[t]
  \centering
  \includegraphics[width=0.90\columnwidth]{images/overview.drawio.pdf}
  \caption{The architecture of the \framework, showing the interplay between its core modules: \textit{Datasets}, \textit{Retrievers}, \textit{Re-Rankers}, and \textit{RAG Evaluation}. Each module operates independently while seamlessly integrating with others, enabling end-to-end retrieval and ranking workflows.}
  \label{fig:rankify_framework}
\end{figure}


\begin{comment}
    

\section{\framework}\label{s:rankify}

The goal of \framework was to create a unified and user-friendly tool for integrating retrieval, re-ranking, and retrieval-augmented generation (RAG) systems. Leveraging Python's dominance in machine learning through libraries like PyTorch~\cite{10.5555/3454287.3455008} and TensorFlow~\cite{10.5555/3026877.3026899}, \framework was developed in Python and is made available via PyPI\footnote{\url{https://pypi.org/project/rankify/}}, allowing straightforward installation:

\begin{lstlisting}[numbers=none]
$ pip install rankify
\end{lstlisting}

This paper uses version \texttt{0.0.1}, the initial release. Users are encouraged to always use the latest version and consult the project repository\footnote{\url{https://github.com/abdoelsayed2016/reranking}} for updates. Contributions are welcome through pull requests to enhance the framework's functionality. Comprehensive online documentation\footnote{\url{http://rankify.readthedocs.io/}} is available, providing a detailed guide to using the framework effectively. Inline documentation is also included through extensive docstrings for all functions and classes, ensuring that developers can access contextual information directly within their IDEs. Figure~\ref{fig:function_docstring} illustrates an example of a function's docstring for the \texttt{rank} method in the \texttt{UPR} model within the \texttt{Reranking} framework, which is responsible for reranking document contexts using GPT or T5 models (explained in Sec.~\ref{ss:rerankers}).



\framework consists of four main modules: \textit{Datasets}, \textit{Retrievers}, \textit{Re-Rankers}, and \textit{RAG}. The \textit{Datasets} module allows users to load preprocessed datasets or create their own tailored datasets, with pre-retrieved documents for benchmarking. The \textit{Retrievers} module supports multiple retrieval methods, enabling users to retrieve documents with just a few lines of code. The \textit{Re-Rankers} module provides a wide array of ranking techniques, ranging from pointwise and pairwise to listwise models. Finally, the \textit{RAG} module integrates the entire pipeline by passing the retrieved and re-ranked documents to large language models for final response generation and assessment. 

Figure~\ref{fig:rankify_framework} illustrates the architecture of \framework and its workflow, demonstrating how the modules interact to streamline research and experimentation. Examples of the framework's functionality will be elaborated in subsequent sections.

\end{comment}



\section{\framework}\label{s:rankify}

Modern information retrieval systems rely on a combination of retrieval, ranking, and generative techniques to surface relevant content and generate accurate responses. However, existing solutions are often fragmented, requiring researchers and practitioners to integrate multiple toolkits to experiment with different retrieval and ranking strategies. \framework addresses this challenge by offering an end-to-end retrieval and ranking ecosystem, enabling seamless experimentation across a wide range of retrieval, re-ranking, and retrieval-augmented generation (RAG) models.

Unlike existing retrieval and ranking frameworks that focus on a single aspect of the pipeline, \framework is designed to be modular, extensible, and lightweight, allowing users to plug in different retrievers, ranking models, and RAG techniques with minimal effort. Built in Python, it leverages machine learning libraries such as TensorFlow~\cite{10.5555/3026877.3026899}, PyTorch~\cite{10.5555/3454287.3455008}, Spacy~\cite{honnibal2020spacy},  and Pyserini~\cite{Lin_etal_SIGIR2021_Pyserini}  . The framework is available on PyPI\footnote{\url{https://pypi.org/project/rankify/}}, making installation straightforward:

\begin{lstlisting}[numbers=none]
$ pip install rankify
\end{lstlisting}

\framework supports both precomputed and real-time retrieval workflows, allowing users to benchmark with pre-retrieved datasets or perform live document retrieval on large-scale corpora. The framework includes ready-to-use indices for Wikipedia and MS MARCO, eliminating the need for time-consuming indexing.  

The architecture of \framework consists of four core modules:  
\begin{itemize}
    \item \textbf{Datasets}: Provides standardized access to datasets like Natural Questions, TriviaQA, and HotpotQA, with support for custom dataset creation.  
    \item \textbf{Retrievers}: Integrates diverse retrieval methods such as BM25, DPR, ANCE, BGE, and ColBERT, enabling flexible retrieval strategies.  
    \item \textbf{Re-Rankers}: Includes 24 models with 41 sub-methods, supporting pointwise, pairwise, and listwise re-ranking.  
    \item \textbf{RAG}: Facilitates retrieval-augmented generation with models like LLAMA, GPT, and T5, supporting in-context learning, Fusion-in-Decoder (FiD), and Incontext RALM.  
\end{itemize}

A key advantage of \framework is its unified API, which abstracts implementation details and simplifies experimentation across different models. Figure~\ref{fig:workflow} and ~\ref{fig:rankify_framework} illustrates the interactions between these modules within the pipeline. The framework is scalable, adaptable, and suited for both research and real-world applications like search engines and question-answering systems.  \framework is open-source, actively maintained\footnote{\url{https://github.com/abdoelsayed2016/reranking}}, and accompanied by comprehensive documentation\footnote{\url{http://rankify.readthedocs.io/}} to support easy adoption for both beginners and experts.


\begin{comment}
    

\begin{figure}[t]
  \centering
  \includegraphics[width=\columnwidth]{images/Docstring (2).pdf}
  \caption{A docstring for the \texttt{rank} function within the \texttt{UPR} model of the \texttt{Reranking} framework. The docstring provides: 
    \blackcircle{1} a detailed function description, followed by 
    \blackcircle{2} notes on the reranking approach using GPT or T5 models. 
    It includes \blackcircle{3} a practical Example demonstrating its use for ranking documents. 
    \blackcircle{4} The References section cites relevant scholarly publications, and 
    \blackcircle{5} The Params section details the input documents, while 
    \blackcircle{6} the Returns section specifies the output—a reranked list of documents.}
  \label{fig:function_docstring}
\end{figure}

\end{comment}


\begin{comment}
    
\begin{figure*}
    \centering
    
    \includegraphics[width=0.8\textwidth]{images/aligned_dataset_sizes_.pdf}
    \caption{Summary of datasets used in \framework. The figure illustrates the dataset sizes across different categories. The bubble size represents the total number of samples (Train + Dev + Test) for each dataset. Categories include QA, Multi-Hop QA, Long-Form QA, Multiple-Choice, Entity-Linking, Slot Filling, Fact Verification, Dialog Generation, Summarization, and Other specialized datasets. The log scale is used for better visualization of dataset size variations.}
    
    \label{fig:Dataset-size}
\end{figure*}
\end{comment}


\begin{comment}

\begin{figure}[t]
  \centering
  \includegraphics[width=0.6\columnwidth]{images/dataset_schema.pdf}
  \caption{Schema of the Dataset class, illustrating the objects used to represent a dataset in \framework.}
  \label{fig:dataset_schema}
\end{figure}
\end{comment}





\begin{table}
\caption{Summary of datasets used in \framework.  Categories include QA, Multi-Hop QA, Long-Form QA, Multiple-Choice, Entity-Linking, Slot Filling, Fact Verification, Dialog Generation, Summarization, and Other specialized datasets.}
%\setlength\tabcolsep{5pt}
%\renewcommand{\arraystretch}{1.1}
\small
\centering

\resizebox{0.45\textwidth}{!}{
\begin{tabular}{l l c c c}  % Fixed column specifier
\toprule
    Task & Dataset Name & \# Train & \# Val & \# Test \\ 
\midrule
    \multirow{14}{*}{{\textbf{QA} }} & NQ~\cite{naturalquestion}& 79,168 & 8,757 & 3,610 \\ 
    &TriviaQA~\cite{triviaqa} &  78,785 & 8,837 & 11,313 \\ 
    &WebQ~\cite{webquestions}  & 3,778 & - & 2,032 \\ 
    &SQuAD~\cite{squad} &  87,599 & 10,570 & - \\ 
    &NarrativeQA~\cite{narrativeqa} &  32,747 & 3,461 & 10,557 \\ 

    &MSMARCO-QA~\cite{msmarco} &  808,731 & 101,093 & - \\ 
     &PopQA~\cite{popqa} & - & - & 14,267 \\ 

    &SIQA~\cite{siqa} & 33,410 & 1,954 & - \\ 
    &Fermi~\cite{fermi} &  8,000 & 1,000 & 1,000 \\ 
    &WikiQA~\cite{wikiqa}  & 20,360 & 2,733 & 6,165 \\ 
    &AmbigQA~\cite{ambigqa,naturalquestion} &  10,036 & 2,002 & - \\ 

    &CommenseQA~\cite{commonsenseqa}  & 9,741 & 1,221 & - \\ 
    &PIQA~\cite{piqa} & 16,113 & 1,838 & - \\ 
    &BoolQ~\cite{boolq} &  9,427 & 3,270 & - \\ 


\midrule
    \multirow{4}{*}{{\textbf{Multi-Hop QA} }} &2WikiMultiHopQA~\cite{2wikimultihop}  & 15,000 & 12,576 & - \\ 
    &Bamboogle~\cite{selfask_2023}  & - & - & 125 \\ 
    &Musique~\cite{musique}  & 19,938 & 2,417 & - \\ 

     &HotpotQA~\cite{hotpotqa} &  90,447 & 7,405 & - \\ 

\midrule
   \multirow{2}{*}{{\textbf{Temp QA} }} & ArchivalQA~\cite{wang2022archivalqa} & 384,426 & 48,304 & 48760 \\ 
    &ChroniclingQA~\cite{piryani2024chroniclingamericaqa}& 385,629   & 21,739    & 21,735   \\
\midrule
   \multirow{2}{*}{{\textbf{Long-Form QA} }}  &ELI5~\cite{eli5} &  272,634 & 1,507 & - \\
   
   & ASQA~\cite{asqa} & 4,353 & 948 & - \\ 
    
\midrule
    \multirow{5}{*}{{\textbf{Multiple-Choice} }}& MMLU~\cite{mmlu,mmlu_ethics}  & 99,842 & 1,531 & 14,042 \\ 
    &TruthfulQA~\cite{truthfulqa} & - & 817 & - \\ 
    &HellaSwag~\cite{hellaswag}  & 39,905 & 10,042 & - \\ 
    &ARC~\cite{arc_challenge}  & 3,370 & 869 & 3,548 \\ 
    &OpenBookQA~\cite{OpenBookQA2018}  & 4,957 & 500 & 500 \\ 
\midrule
   \multirow{2}{*}{{\textbf{Entity-linking} }}  
   &WNED~\cite{wned,kilt_2021}  & - & 8,995 & - \\ 
   & AIDA CoNLL-YAGO~\cite{AIDA_CONLL,kilt_2021}  & 18,395 & 4,784 & - \\ 
    
\midrule
    \multirow{2}{*}{{\textbf{Slot filling} }} 
    & Zero-shot RE~\cite{levy-etal-2017-zero,kilt_2021}  & 147,909 & 3,724 & - \\ 

    &T-REx~\cite{trex,kilt_2021}  & 2,284,168 & 5,000 & - \\ 

\midrule
    \multirow{-1}{*}{{\textbf{Dialog Generation} }} & WOW~\cite{dinan2018wizard,kilt_2021} & 63,734 & 3,054 & - \\ 

\midrule
    \multirow{1}{*}{{\textbf{Fact Verification} }} &FEVER~\cite{fever,kilt_2021} & 104,966 & 10,444 & - \\ 
\midrule
    \multirow{1}{*}{{\textbf{\text{Open-domain Summarization}} }} &WikiAsp~\cite{wikiasp}  & 300,636 & 37,046 & 37,368 \\ 
\bottomrule
\end{tabular}
}
\label{tab:Dataset-size}
\end{table}

\subsection{Prebuilt Retrieval Corpora and Indexes}

\framework provides two large-scale retrieval corpora: \textbf{Wikipedia}~\cite{karpukhin2020dense} and \textbf{MS MARCO}~\cite{msmarco}, enabling researchers to conduct retrieval experiments on well-established benchmarks. To support efficient retrieval, \framework includes pre-built index files for each retriever, allowing users to directly query the corpora without requiring costly indexing. Each corpus has been indexed for multiple retrieval models, including BM25, DPR, ANCE, BGE, Contriever, and ColBERT. By providing precomputed indexes, \framework simplifies large-scale retrieval research while maintaining consistency across different retrieval pipelines. Users can view all available datasets using the following command (see Listing~\ref{lst:available_datasets}):
\begin{lstlisting}[language=Python, caption={This script retrieves the latest information about the available datasets, and displays metadata for each dataset in the terminal.}, label={lst:available_datasets} , captionpos=b]
from rankify.dataset.dataset import Dataset
Dataset.avaiable_dataset()
\end{lstlisting}




\subsection{Datasets}\label{s:dataset}
\framework provides a standardized and efficient way to handle datasets for retrieval, re-ranking, and retrieval-augmented generation (RAG). To simplify retrieval workflows, \framework integrates pre-retrieved datasets with structured annotations, enabling users to experiment with various retrieval and re-ranking methods with minimal code. The datasets in \framework are categorized based on task type and size, as shown in Table~\ref{tab:Dataset-size}. Each dataset follows a standardized schema with three main components: the \textbf{Question}, which represents the query requiring relevant information; the \textbf{Answers}, which are the expected correct responses to the query; and the \textbf{Contexts} (or \textbf{Retrieved Documents)}, which provide a ranked list of candidate documents retrieved by a specific retriever.

To support benchmarking and reproducibility, \framework provides pre-retrieved datasets, each with 1,000 top-ranked documents from Wikipedia\footnote{Note: We are currently processing different retrievers based on  MS MARCO to generate and store 1,000 top-ranked documents per query for each dataset.}, processed by various retrievers such as BM25, DPR, ANCE, BGE, Contriever, and ColBERT. These datasets cover diverse Re-Ranking and RAG  tasks.%, including question answering, multi-hop reasoning, temporal retrieval, entity linking, fact verification, dialogue generation, and summarization. By applying the same retrieval pipeline across all datasets, \framework eliminates the need for computationally expensive indexing while ensuring consistent evaluation.


\begin{comment}

With \framework, we aim to provide a standardized and efficient way to handle datasets for retrieval, re-ranking, and retrieval-augmented generation (RAG). Given the complexity of retrieval workflows, \framework integrates pre-retrieved datasets with structured annotations, allowing users to experiment with different retrieval and re-ranking methods using just a few lines of code. Our dataset design unifies multiple annotation styles and storage formats, enabling a seamless interface for loading, processing, and benchmarking retrieval-based tasks.

The datasets included in \framework fall into multiple categories, as shown in Table~\ref{tab:Dataset-size}, which showcases their distribution based on task type and dataset size. Each dataset in \framework follows a standardized schema that consists of three main components: (1) \textbf{Question:} The input query for which relevant information needs to be retrieved. (2) \textbf{Answers:} The expected correct responses associated with the question. (3) \textbf{Contexts} (or \textbf{Retrieved Documents):} A ranked list of candidate documents retrieved by a specific retriever.
The \texttt{Dataset} class in \framework encapsulates these attributes and provides easy access to retrieval, evaluation, and benchmarking.

To facilitate benchmarking and ease of use, \framework provides pre-retrieved datasets where each query is associated with the top 1,000 retrieved documents from different retrievers. These datasets allow users to test different re-ranking and retrieval models without the need for time-consuming indexing.

As mentioned above, to ensure  standardized retrieval benchmarking, \framework provides pre-retrieved datasets, where each query is associated with 1,000 retrieved documents from Wikipedia. These datasets span multiple retrieval tasks, including question answering, multi-hop reasoning, temporal retrieval, entity linking, fact verification, dialogue generation, and summarization. To enable fair comparisons across different retrieval methods, each dataset has been processed using multiple retrievers, including  BM25, DPR, ANCE, BGE, Contriever, and  ColBERT. By applying the same retrieval pipeline across all the datasets, \framework eliminates the need for computationally expensive indexing while maintaining consistency in evaluation. 

\end{comment}


\framework allows users to load and explore datasets with minimal effort. Below is an example demonstrating how to \textbf{download and inspect a dataset} using \framework’s API:

\begin{lstlisting}[language=Python, caption={Loading a dataset in Rankify and inspecting its structure.}, captionpos=b]
from rankify.dataset.dataset import Dataset
# Load the nq-test dataset retrieved with BM25
dataset = Dataset(retriever="bm25", dataset_name="nq-test")
documents = dataset.download(force_download=False)
\end{lstlisting}

%# Inspect the first document
%print(documents[0])



Beyond preloaded datasets, \framework enables users to define their own datasets using the Dataset class. Below is an example demonstrating how to create a custom retrieval dataset:

\begin{lstlisting}[language=Python, caption={Creating a custom dataset in Rankify.}, label={lst:custom_dataset}, captionpos=b]
from rankify.dataset.dataset import Dataset, Document, Question, Answer, Context
question = Question("What is the capital of France?")
answer = Answer(["Paris"])
context = [Context(score=0.9, has_answer=True, id=1, title="France", text="The capital is Paris.")]
document = Document(question, answer, context)
custom_dataset = Dataset(retriever="custom", dataset_name="MyDataset")
custom_dataset.documents = [document]
custom_dataset.save_dataset("my_dataset.json")
\end{lstlisting}

Users can load their own datasets using \texttt{load\_dataset} and \texttt{load\_dataset\_qa} functions. The \texttt{load\_dataset} function is designed for structured datasets that include both queries and retrieved documents. On the other hand, \texttt{load\_dataset\_qa} is intended for question-answering datasets that contain only queries and answers, allowing users to use them directly for retrieval tasks. The following examples demonstrate how users can load these datasets in \framework:

\begin{lstlisting}[language=Python, caption={Loading a dataset with documents and a QA-only dataset.}, label={lst:load_dataset}, captionpos=b]
from rankify.dataset.dataset import Dataset
retrieval_dataset = Dataset.load_dataset("path/to/retrieval_dataset.json")
qa_dataset = Dataset.load_dataset_qa("path/to/qa_dataset.jsonl")
\end{lstlisting}



\subsection{Retriever Models}\label{ss:retrievers}

\framework supports a diverse set of retriever models to facilitate document retrieval for various information retrieval tasks. 
%The current version includes both sparse and dense retrievers. Sparse retrievers, such as BM25~\cite{bm25}, rely on exact term matching, while dense retrievers, such as DPR~\cite{dpr}, ANCE~\cite{ance}, and BGE~\cite{bge_m3}, Contriever~\cite{izacard2021contriever} and ColBERT~\cite{khattab2020colbertefficienteffectivepassage}.
The current version includes both sparse retrievers, such as BM25~\cite{bm25}, and dense retrievers, such as DPR~\cite{dpr}, ANCE~\cite{ance}, and BGE~\cite{bge_m3}, Contriever~\cite{izacard2021contriever} and ColBERT~\cite{khattab2020colbertefficienteffectivepassage}. The retrievers implemented in \framework are accessible via a unified interface, where users can specify the retrieval method and parameters. %The available retriever models include BM25, DPR, ANCE, Contriever, BGE, and ColBERT. 

\framework allows users to retrieve documents efficiently using different retrieval models. The retrievers can be applied to any dataset supported by \framework, making it flexible for experimentation and benchmarking. Users can easily initialize a retriever and apply it to a list of query-document pairs, as demonstrated in Listing~\ref{lst:retriever_usage}.

\begin{lstlisting}[language=Python, caption={Using a retriever model in Rankify to retrieve documents.}, label={lst:retriever_usage}, captionpos=b]
from rankify.dataset.dataset import Document, Question, Answer
from rankify.retrievers.retriever import Retriever

# Define sample documents
documents = [
    Document(question=Question("the cast of a good day to die hard?"), 
             answers=Answer(["Jai Courtney", "Bruce Willis"]), contexts=[]),
    Document(question=Question("Who wrote Hamlet?"), 
             answers=Answer(["Shakespeare"]), contexts=[])
]
# Initialize a retriever (example: ColBERT)
retriever = Retriever(method="colbert", model="colbert-ir/colbertv2.0", n_docs=5, index_type="msmarco")
# Retrieve documents
retrieved_documents = retriever.retrieve(documents)
# Print retrieved documents
for i, doc in enumerate(retrieved_documents):
    print(f"\nDocument {i+1}:")
    print(doc)
\end{lstlisting}

In Listing~\ref{lst:retriever_usage}, users can specify the retrieval corpus by setting \texttt{index\_type} to \texttt{msmarco} or \texttt{wiki}, select the desired retriever model, and define the number of documents to retrieve. This flexibility allows researchers to experiment with different retrieval settings, optimizing their retrieval strategy based on the task requirements.

\subsection{Re-Ranking Models}\label{ss:rerankers}

In information retrieval, two-stage pipelines are widely used to maximize retrieval performance. The first stage involves retrieving a set of candidate documents using a computationally efficient retriever, followed by a second stage where these documents are re-ranked using a stronger, typically, neural network-based model. This re-ranking step enhances retrieval quality by considering deeper semantic relationships between the query and documents. While effective, re-ranking models vary significantly in their architectures, trade-offs, and implementations, making it challenging for users to select the most suitable method for their specific needs.

To address this, \framework provides a unified interface for re-ranking, enabling users to effortlessly switch between different models with minimal modifications. The framework supports a diverse set of 24 primary re-ranking models with 41 sub-methods, spanning different re-ranking strategies such as pointwise, pairwise, and listwise approaches. The implemented models include MonoBERT~\cite{nogueira2019passage}, MonoT5~\cite{nogueira2020document}, RankT5~\cite{zhuang2023rankt5}, ListT5~\cite{yoon2024listt5listwisererankingfusionindecoder}, ColBERT~\cite{santhanam2021colbertv2}, RankGPT~\cite{rankgpt}, and various transformer-based re-rankers.


Users can apply these models to re-rank retrieved documents using a simple interface, as demonstrated in Listing~\ref{lst:reranker_usage}.

\begin{lstlisting}[language=Python, caption={Applying re-ranking in Rankify.}, label={lst:reranker_usage}, captionpos=b]
from rankify.dataset.dataset import Document, Question, Answer
from rankify.retrievers.retriever import Retriever
from rankify.rerankers.reranker import Reranker
# Define sample documents
documents = [
    Document(question=Question("the cast of a good day to die hard?"), 
             answers=Answer(["Jai Courtney", "Bruce Willis"]), contexts=[]),
    Document(question=Question("Who wrote Hamlet?"), 
             answers=Answer(["Shakespeare"]), contexts=[])
]
# Initialize a retriever
retriever = Retriever(method="colbert", model="colbert-ir/colbertv2.0", n_docs=5, index_type="msmarco")
retrieved_documents = retriever.retrieve(documents)
# Initialize a re-ranker
reranker = Reranker(method="monot5",model_name="monot5-base")
# Apply re-ranking
reranked_documents = reranker.rerank(retrieved_documents)
# Print re-ranked documents
for i, doc in enumerate(reranked_documents):
    print(f"\nDocument {i+1}:")
    print(doc)
\end{lstlisting}

\framework supports both pre-trained models from Hugging Face and API-based re-ranking models such as RankGPT, Cohere, and Jina Reranker. Users can select from a variety of models, including cross-encoders, T5-based ranking models, and late-interaction retrieval models like ColBERT. The flexibility to switch between different re-ranking strategies allows researchers and practitioners to optimize ranking performance based on their task requirements.


\subsection{Retrieval-Augmented Generation (RAG) Models}\label{ss:rag}

Retrieval-Augmented Generation (RAG) enhances language models by integrating retrieval mechanisms, allowing them to generate responses based on dynamically retrieved documents rather than relying solely on pre-trained knowledge. This approach is particularly effective for knowledge-intensive tasks such as open-domain question answering, fact verification, and knowledge-based text generation. \framework provides a modular and extensible interface for applying multiple RAG methods, including zero-shot generation, Fusion-in-Decoder (FiD)~\cite{izacard2020leveraging}, and in-context learning ~\cite{ram-etal-2023-context}.

In \framework, the Generator module enables seamless integration of RAG techniques, allowing users to experiment with different generative approaches. Users can specify the desired RAG method and model, applying generation strategies across retrieved documents. 

Users can apply these methods to generate responses based on retrieved documents. Listing~\ref{lst:generator_usage} demonstrates how to use \framework's RAG module with an in-context learning approach.

\begin{lstlisting}[language=Python, caption={Applying Retrieval-Augmented Generation (RAG) in Rankify.}, label={lst:generator_usage}, captionpos=b]
from rankify.dataset.dataset import Document, Question, Answer, Context
from rankify.generator.generator import Generator
# Sample question and contexts
question = Question("What is the capital of France?")
answers = Answer(["Paris"])
contexts = [
    Context(id=1, title="France", text="The capital of France is Paris.", score=0.9),
    Context(id=2, title="Germany", text="Berlin is the capital of Germany.", score=0.5)
]
# Create a Document
doc = Document(question=question, answers=answers, contexts=contexts)
# Initialize Generator with In-Context RALM
generator = Generator(method="in-context-ralm", model_name='meta-llama/Llama-3.1-8B')
# Generate answer
generated_answers = generator.generate([doc, doc])
print(generated_answers)
\end{lstlisting}

\framework allows users to leverage large-scale language models such as LLaMA~\cite{touvron2023llama}, GPT-4~\cite{achiam2023gpt}, and T5-based models~\cite{raffel2020exploring} for retrieval-augmented generation. By supporting both encoder-decoder architectures (FiD~\cite{izacard2020leveraging}) and decoder-only models (e.g., GPT, LLaMA), the framework provides flexibility for optimizing generation quality based on task-specific requirements. 


\subsection{Evaluation Metrics}\label{ss:evaluation}
\input{tables/bm25_results}
\framework provides evaluation metrics for retrieval, re-ranking, and retrieval-augmented generation (RAG). For retrieval and re-ranking, Top-k accuracy measures whether documents containing the correct answer appear within the top-k results. This definition of relevance, while task-specific, aligns with the goal of downstream applications like question answering. Listing~\ref{lst:evaluation_retrieval} demonstrates how to compute Top-k accuracy.

\begin{lstlisting}[language=Python, caption={Computing Top-k accuracy for retrieval and re-ranking in Rankify.}, label={lst:evaluation_retrieval}, captionpos=b]
from rankify.metrics.metrics import Metrics
metrics = Metrics(documents)
# Compute Top-k accuracy before and after re-ranking
before_ranking = metrics.calculate_retrieval_metrics(ks=[1,5,10,20,50,100], use_reordered=False)
after_ranking = metrics.calculate_retrieval_metrics(ks=[1,5,10,20,50,100], use_reordered=True)
print("Before Ranking:", before_ranking)
print("After Ranking:", after_ranking)
\end{lstlisting}

For QA and RAG, \framework supports Exact Match (EM), recall, precision, and containment to evaluate generated answers. Listing~\ref{lst:evaluation_rag} shows how these metrics are computed.

\begin{lstlisting}[language=Python, caption={Computing QA and RAG evaluation metrics in Rankify.}, label={lst:evaluation_rag}, captionpos=b]
from rankify.metrics.metrics import Metrics
qa_metrics = Metrics(documents)
qa_results = qa_metrics.calculate_generation_metrics(generated_answers)
print(qa_results)
\end{lstlisting}

By providing a unified evaluation module, \framework ensures consistent benchmarking across retrieval, re-ranking, and RAG tasks.

\section{Experimental Result and Discussion}

\subsection{Experiment Setup}

\framework enables researchers to benchmark retrieval, re-ranking, and RAG methods, evaluate their own approaches, and explore optimizations within these tasks. To demonstrate its capabilities, we conducted multiple experiments to provide reproducible benchmarks and performance insights. All main experiments were conducted on 2x NVIDIA A100 GPUs.

Experiments were performed on a diverse set of datasets, covering open-domain and multi-hop QA, fact verification, and temporal retrieval tasks. All the experiments in our study use preprocessed
English Wikipedia dump from December 2018 as
released by~\cite{dpr} as evidence passages. Each Wikipedia article is split into non-overlapping 100-word passages, with over 21 million passages in total.


We evaluated retrieval performance on datasets such as Natural Questions (NQ), TriviaQA, HotpotQA, 2WikiMultiHopQA, and ArchivalQA, while re-ranking performance was analyzed on MSMARCO, WebQuestions, and PopQA. For retrieval evaluation, we measured Top-k accuracy at k={1, 5, 10, 20, 50, 100}. Exact match (EM), Precision, Recall, Contains, and F1 were used as the primary evaluation metrics for QA tasks.

We conducted experiments across all supported retrieval, re-ranking, and RAG methods. Retrieval models BM25, DPR, ANCE, BGE, and ColBERT. Re-ranking methods tested included MonoT5, RankT5, RankGPT, and various transformer-based re-rankers. For RAG, we evaluated zero-shot generation, Fusion-in-Decoder (FiD), in-context learning (RALM). These experiments highlight the adaptability of \framework for benchmarking retrieval, ranking, and knowledge-grounded text generation.
\input{tables/different_retriever_datasets}



\input{tables/bm25_reranking_results}




\subsection{Retrieval Results}

The retrieval performance of BM25 across multiple datasets is shown in Table~\ref{tab:bm25_results}. The results report Top-1, Top-5, Top-10, Top-20, Top-50, and Top-100 retrieval accuracy for each dataset split. Performance varies significantly based on dataset characteristics, with high Top-k accuracy achieved on open-domain QA datasets such as TriviaQA and Natural Questions. On the other hand, more complex multi-hop datasets like 2WikiMultiHopQA and HotpotQA are characterized by lower retrieval rates, reflecting the challenges in retrieving supporting evidence across multiple documents. Temporal datasets like ArchivalQA and ChroniclingAmericaQA exhibit moderate retrieval performance, likely due to the temporal nature of their content. It is important to note that some datasets in Table~\ref{tab:bm25_results} were either not officially recorded in previous studies or rely on different underlying corpora, making direct comparison difficult. Additionally, some datasets (e.g., ChroniclingAmericaQA) do not report retrieval accuracy as a standard Top-k metric, further complicating direct validation. Despite these variations, our results shown in Table~\ref{tab:bm25_results} provide a comprehensive overview of BM25's strengths and limitations. %across diverse retrieval tasks.



Table~\ref{tab:combined_qa} presents the retrieval performance of various dense retrievers across three benchmark datasets: NQ, WebQ, and TriviaQA. The results report Top-1, Top-5, Top-10, Top-20, Top-50, and Top-100 retrieval accuracy, providing insights into how different retrieval models rank relevant documents. Among the tested retrievers, dense retrieval methods such as DPR, MSS-DPR, and ColBERT consistently outperform retrievers like MSS and Contriever. MSS-DPR achieves the highest Top-1 accuracy, reaching 50.16\% on NQ, 44.24\% on WebQ, and 61.63\% on TriviaQA, demonstrating the effectiveness of fine-tuned dense retrieval. Standard DPR also performs well, particularly on NQ and TriviaQA, where it reaches 48.67\% and 57.47\% Top-1 accuracy, respectively. ColBERT, which employs late-interaction ranking, shows competitive performance, particularly in higher recall settings (Top-50 and Top-100).







\begin{figure*}[h!]
    \centering
    \includegraphics[width=0.75\linewidth]{images/generator_updated_no_mss.pdf}
    \caption{Exact Match (EM) for BM25, Contriever, and DPR retrievers across three datasets (NQ, TriviaQA, WebQ) using various language models (LLaMA V3/V3.1, Gemma 2B/9B, LLaMA 2 13B, and Mistral 7B).}
    \label{fig:generator}
\end{figure*}

\subsection{Comparison with Original Implementations}

To validate the correctness and reliability of the \framework's retrieval module, we compare its performance against original implementations and Pyserini-based baselines for multiple retrievers. Specifically, we evaluate DPR, Contriever, BM25, and ColBERT across three open-domain QA datasets:  NQ, TriviaQA, and WebQ. The retrieval performance is measured using Top-20 and Top-100 accuracy.

Table~\ref{tab_dpr} presents the results of our \framework implementation alongside the original models and Pyserini-based implementations. Our results show that \framework achieves retrieval performance identical to Pyserini for DPR, BM25 as we utilize Pyserini as the backend for indexing and retrieval.  However, for Contriever BGE, and ColBERT, we implemented their methods independently, and our results closely match the original implementations, demonstrating the correctness of our approach.

Additionally, we observe improvements in the performance of BM25 and DPR compared to their original baselines. These improvements can be attributed to the optimizations present in Pyserini, such as better indexing techniques and parameter tuning. For ColBERT and Contriever, we replicate the results reported in their respective papers by running their official GitHub implementations on our datasets, yielding identical outcomes.

\begin{comment}
    
\subsection{Comparison with Original Implementations}
To ensure the reliability and effectiveness of Rankify's retrieval module, we compare our implementation with the original implementations of each retriever. Specifically, we evaluate DPR, Contriever, BM25, and ColBERT on three open-domain QA datasets: Natural Questions (NQ), TriviaQA, and WebQuestions. The retrieval performance is measured using Top-20 and Top-100 accuracy metrics.

Table~\ref{tab_dpr} presents the results of our retriever implementations (\framework) against their respective original versions. Our implementations achieve comparable or improved retrieval accuracy across different datasets. Notably, DPR\_\framework consistently outperforms the original DPR, with improvements of +1.1\% on NQ, +0.3\% on TriviaQA, and +1.9\% on WebQuestions in Top-20 accuracy. Similarly, BM25\_\framework shows a performance gain over standard BM25, highlighting the effectiveness of Rankify’s optimizations.

For Contriever and ColBERT, our implementation achieves near-identical results to the original models, demonstrating that Rankify successfully replicates their performance while integrating them within a unified retrieval framework. The improvements in DPR\_\framework and BM25\_\framework stem from using Pyserini as the backend for indexing and retrieval. Pyserini, built on Lucene, benefits from ongoing optimizations, including improved indexing techniques and parameter tuning beyond the original papers. These enhancements lead to more accurate and efficient retrieval while maintaining consistency with the original model architectures.

\end{comment}





\subsection{Re-Ranking Results}

The performance of various re-ranking methods applied to BM25-retrieved documents is presented in Table~\ref{tab:reranking-result}. The table reports Top-1, Top-10, and Top-50 accuracy for the NQ-Test and WebQ datasets. The baseline BM25 retrieval scores serve as a reference, demonstrating the extent to which re-ranking improves document ranking quality across different models. Across all methods, re-ranking consistently improves retrieval effectiveness, with notable gains observed in Top-1 accuracy. Transformer-based models such as UPR (T5-based), RankGPT, FlashRank (MiniLM and TinyBERT), and RankT5 achieve substantial improvements over the BM25 baseline. In particular, RankT5-3B outperforms other models, achieving a Top-1 accuracy of 47.17\% on NQ-Test and 40.40\% on WebQ, highlighting the effectiveness of T5-based models for large-scale retrieval. RankGPT, leveraging LLaMA-3.1-8B, also shows strong performance, particularly in WebQ, where it improves Top-1 accuracy from 18.80\% (BM25) to 38.77\%.

Among lighter-weight models, FlashRank (MiniLM and TinyBERT) and MonoBERT-large provide solid improvements while maintaining efficiency. MiniLM-L-12-v2 achieves 41.02\% Top-1 accuracy on NQ-Test and 37.05\% on WebQ, making it a strong candidate for deployment scenarios where computational efficiency is critical. The MonoBERT-large model reaches 39.05\% on NQ-Test and 34.99\% on WebQ, reinforcing the effectiveness of BERT-based cross-encoders for document ranking. Models that integrate listwise and contrastive ranking techniques, such as Twolar-xl and LiT5, show competitive performance. Twolar-xl achieves 46.84\% Top-1 accuracy on NQ-Test and 41.68\% on WebQ, while LiT5-Distill-xl reaches 47.81\% on NQ-Test and 42.37\% on WebQ, demonstrating that fine-tuned T5 models can significantly improve ranking quality. The sentence-transformer rerankers, particularly GTR-xxl, achieve 42.93\% and 39.41\% Top-1 accuracy on NQ-Test and WebQ, respectively, making them effective for diverse retrieval tasks.

\begin{comment}
    
\subsection{Re-Ranking Results}

The performance of various re-ranking methods applied to BM25-retrieved documents is presented in Table~\ref{tab:reranking-result}. The table reports Top-1, Top-5, Top-10, Top-20, and Top-50 accuracy for two datasets: Natural Questions (NQ-Test) and WebQuestions (WebQ). The baseline BM25 retrieval scores serve as a reference, demonstrating the extent to which re-ranking improves document ranking quality across different ranking models. Across all models, re-ranking consistently improves retrieval effectiveness, with notable gains observed in Top-1 accuracy. Transformer-based ranking models such as UPR (T5-based models), RankGPT, FlashRank (MiniLM and TinyBERT), and RankT5 achieve substantial improvements over the BM25 baseline. In particular, RankT5-3B outperforms other ranking models, achieving a Top-1 accuracy of 47.17\% on NQ-Test and 40.40\% on WebQ, highlighting the effectiveness of T5-based ranking models for large-scale retrieval. RankGPT, leveraging LLaMA-3.1-8B, also demonstrates strong performance, particularly in WebQ, where it improves Top-1 accuracy from 18.80\% (BM25) to 38.77\%.

Among lighter-weight models, FlashRank (MiniLM and TinyBERT) and MonoBERT-large provide solid improvements while maintaining efficiency. MiniLM-L-12-v2 achieves 41.02\% Top-1 accuracy on NQ-Test and 37.05\% on WebQ, making it a strong candidate for deployment scenarios where computational efficiency is critical. The MonoBERT-large model reaches 39.05\% on NQ-Test and 34.99\% on WebQ, reinforcing the effectiveness of BERT-based cross-encoders for document ranking.

Models that integrate listwise and contrastive ranking techniques, such as Twolar, and LiT5 re-rankers, show competitive performance. Twolar-xl achieves 46.84\% Top-1 accuracy on NQ-Test and 41.68\% on WebQ, while LiT5-Distill-xl reaches 47.81\% on NQ-Test and 42.37\% on WebQ, demonstrating that fine-tuned T5 models can significantly improve ranking quality. The sentence-transformer rerankers, particularly GTR-xxl, achieve 42.93\% and 39.41\% Top-1 accuracy on NQ-Test and WebQ, respectively, making them effective in ranking diverse retrieval outputs.
\end{comment}

%Overall, the results highlight the importance of re-ranking in improving retrieval effectiveness. While BM25 provides a strong initial retrieval, the application of neural re-rankers significantly enhances retrieval quality, particularly for Top-1 and Top-5 results. The effectiveness of different models varies based on dataset characteristics, with larger transformer-based models excelling in ranking accuracy, while lighter-weight models offer a balance between accuracy and efficiency.



\subsection{Generator Results}

In this section, we present the performance of Rankify's generator module when integrated with Retrieval-Augmented Language Modeling (RALM)~\cite{ram-etal-2023-context}. We evaluate the effectiveness of various retrieval models, including BM25, Contriever, and DPR, across three open-domain QA datasets: NQ, TriviaQA, and WebQ. The experiments utilize multiple LLMs: LLaMA V3 8B, LLaMA V3.1 8B, Gemma 2B/9B, LLaMA 2 13B, and Mistral 7B to assess the robustness of the retrieval and generation pipeline. We focus on the Exact Match (EM) metric, which measures the proportion of generated answers that exactly match the ground-truth references. Figure~\ref{fig:generator} provides a consolidated view of the EM scores for all datasets and models. From the figure, we observe the following key patterns: For the NQ dataset, DPR consistently outperforms other retrievers, achieving its peak performance with the LLaMA V3 8B model (28.08\%). On the TriviaQA dataset, BM25 achieves the highest EM score (57.55\%) when paired with the Gemma 2 9B model, outperforming both DPR and Contriever. For the WebQuestions (WebQ) dataset, DPR again exhibits strong performance, achieving its best EM score (19.83\%) with the LLaMA V3 8B model.


%\input{tables/generator}
\section{Conclusion}\label{sec:conclusion}

We introduced \framework, a modular toolkit that unifies retrieval, re-ranking, and retrieval-augmented generation (RAG) within a cohesive framework. By integrating diverse retrievers, state-of-the-art re-rankers, and seamless RAG pipelines, \framework streamlines experimentation and benchmarking in information retrieval. Our experiments demonstrate its effectiveness in enhancing ranking quality and retrieval-based text generation.

Designed for extensibility, \framework allows for easy integration of new models and evaluation methods. Future work will focus on improving retrieval efficiency, optimizing re-ranking strategies, and advancing RAG capabilities. The framework is open-source, actively maintained, and freely available for the research community. Future work will focus on incorporating alternative retrieval evaluation metrics such as MAP, Precision, and NDCG, while expanding dataset coverage to include BEIR for broader benchmarking. The framework is open-source, actively maintained, and freely available for the research community.








%%
%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
\bibliographystyle{ACM-Reference-Format}
\bibliography{software}



\end{document}
\endinput
%%
%% End of file `sample-sigconf-authordraft.tex'.
