\section{Related Work}
\label{s:related_work}
Research in information retrieval (IR), re-ranking, and retrieval-augmented generation (RAG) has progressed significantly over the past decade. Traditional retrieval models like BM25____ offered robust lexical matching capabilities but struggled with capturing semantic relationships. This limitation led to the development of dense retrieval methods____, which leverage pre-trained neural encoders to represent queries and documents in a shared semantic space. Notable approaches, such as DPR____, ANCE____, and multi-vector models like ColBERT____, have demonstrated substantial improvements in retrieval effectiveness. Hybrid retrievers, combining sparse and dense signals____, further enhance performance by leveraging both lexical and semantic features. Recent advancements, including knowledge distillation____ and curriculum learning____, continue to refine retrieval performance across diverse datasets.

Re-ranking methods have also evolved alongside retrieval techniques to improve the ordering of retrieved documents. Traditional pointwise____ and pairwise____ approaches have given way to listwise methods like LambdaRank and ListNet____. Deep neural models, including cross-encoders and transformer-based architectures, have demonstrated remarkable success in re-ranking tasks by capturing complex interactions between queries and documents. Zero-shot and in-context re-ranking with large language models (LLMs), such as GPT-4____ and RankT5____, now enable effective ranking adjustments without task-specific training.

Retrieval-Augmented Generation (RAG) has emerged as a powerful paradigm for enhancing generative models in knowledge-intensive tasks____. By retrieving relevant documents and integrating them into the generative process, RAG systems improve factual accuracy and reduce hallucinations. Techniques like self-consistency____ and noise filtering____ have been proposed to further improve the reliability of RAG outputs. However, the effectiveness of these systems heavily depends on the quality of retrieved and re-ranked documents, highlighting the need for robust retrieval frameworks.

In response to the growing complexity of IR, re-ranking, and RAG tasks, several frameworks have been introduced. \textit{Rerankers}____ provides a lightweight Python interface for common re-ranking models, while \textit{RankLLM} focuses on listwise re-ranking with LLMs. Other frameworks like \textit{FlashRAG}____ and \textit{AutoRAG}____ offer modular components for RAG experimentation, though they often lack support for diverse datasets and advanced retriever configurations. Tools such as LangChain____, LlamaIndex____, and DSPy____ further contribute to the ecosystem by simplifying model integration and workflow design.  A comparison of retrieval, re-ranking, and RAG toolkits is presented in Table~\ref{tab:framework_comparison}, highlighting the advantages of \framework in dataset diversity, retriever and re-ranker support, and modularity.




\begin{figure}[t]
  \centering
  \includegraphics[width=0.90\columnwidth]{images/overview.drawio.pdf}
  \caption{The architecture of the \framework, showing the interplay between its core modules: \textit{Datasets}, \textit{Retrievers}, \textit{Re-Rankers}, and \textit{RAG Evaluation}. Each module operates independently while seamlessly integrating with others, enabling end-to-end retrieval and ranking workflows.}
  \label{fig:rankify_framework}
\end{figure}


\begin{comment}