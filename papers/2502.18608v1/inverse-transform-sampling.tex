\section{Watermark via inverse transform sampling}
\label{sec:inverse-transform}
%
In this section, we provide our main theoretical result in watermarking data. Particularly, we concretely show: 1) our decoder is distortion-free and 2) an upper bound on the $p$-value of the generated text as a function of the generated text length and the watermark key sequence. We measure the strength of this $p$-value upper bound through \emph{watermark potential}.
%
\begin{definition}[Watermark potential]
    The watermark potential of a string $y$ of length $k$ is %
    %
    \begin{equation*}
        \alpha(y) \coloneqq 1 - \frac{1}{k} \sum_{i=1}^{k} p(y_i | y_{:i-1})
    \end{equation*}
    %
    \label{def:potential}
\end{definition}
%
The potential of a string generated from a deterministic language model is always zero. On the other hand, if the model is highly randomized, the potential will approach one. This disparity is important because if the text is generated from a deterministic language model, any distortion-free watermark applied to it will have no statistical power. We formalize this intuition via a lower bound on the detection accuracy of any watermarking scheme. Particularly, we want to lower bound the error of any classifier $h: \cV^* \times \Xi^* \rightarrow \{-1,1\}$ that seeks to differentiate between watermarked (labeled $1$) and non-watermarked (labeled $-1$) given a watermark key sequence $\xi$. 
%
\begin{lemma}[Lemma 2.2 in \citet{kuditipudi2024robust}]
Let $Y'_i \sim p(\cdot | Y'_{:i-1})$ for $i \in [m]$. Let $Y = Y'$ and $\xi \in \Xi^*$ be a random variable independent of $Y'$. Let $h: \cV^* \times \Xi^* \rightarrow \{-1,1\}$ be a classifier. Let $c > 0$ and define the set $\cV_c \subset \cV^m$ by:
%
\begin{equation*}
    \cV_{c} = \{ y: p(y_i | y_{:i-1}) \geq \exp(-c/2) \quad \forall i \in [m] \}
\end{equation*}
Then, we have
%
\begin{align*}
    &\Pr[h(Y, \xi) = -1] + \Pr[h(Y', \xi) = 1] \\
    &\quad \geq \E[\exp(-c \cdot m \cdot \alpha(Y)] \cdot \1\{Y \in \cV_c \}    
\end{align*}
%
\end{lemma}
%
Our main technical contribution is watermarking via inverse transform sampling. At a high level, we sample from a univariate distribution using the pushforward of a uniform random variable via its inverse CDF. Particularly, we design a decoder that maps a sequence of uniform random variables and permutations to tokens using inverse transform sampling. Then, to detect the watermark, the detector checks for a correlation between the sequence of permuted indices of tokens and the watermark key sequence of uniform random variables. If the text is not watermarked, the sequence of permuted token indices will be independent from the sequence of uniform random variables. 

Formally, we let $\Pi$ denote the space of permutations over the vocabulary $[N]$ and $\xi = (u, \pi) \in [0,1] \times \Pi \coloneqq \Xi$. For any distribution $\mu \in \Delta([N])$, we define the decoder as
%
\begin{equation}
    \Gamma(\xi, \mu) \coloneqq \pi^{-1} (\min \{ \pi(i): \mu( \{j: \pi(j) \leq \pi(i) \} ) \geq u \} )
    \label{eq:its-decoder}
\end{equation}
%
That is, $\Gamma(\xi, \mu)$ is the token with the smallest index in the permutation $\pi$ such that the CDF of $\mu$ w.r.t $\pi$ is at least $u$. We formally show that this decoder is distortion-free in the following theorem.
%
\begin{theorem}[Distortion-free]
    Let $\pi \in \Pi$ be an arbitrary permutation over the vocabulary and $U \sim \unif([0,1])$. Define the private key sequence $\xi \coloneqq (U, \pi)$. Then, $\Gamma$ as defined in \Cref{eq:its-decoder} is distortion-free w.r.t the key sequence $\xi$. 
    \label{thm:distortion-free}
\end{theorem}
%
\begin{proof}
    From \Cref{def:distortion-free}, it suffices to show that for any distribution $\mu \in \Delta([N])$ and string $y \in [N]$, we have $\Pr[\Gamma(\mu, \xi) = y] = \mu(y)$. By construction of the decoder in \Cref{eq:its-decoder}, we have 
    %
    \begin{align*}
        &\Gamma(\mu, \xi) = y \iff \\
        &U \in [\mu(\{ y': \pi(y') < \pi(y)\}), \mu(\{ y': \pi(y') \leq \pi(y) \})]
    \end{align*}
    %
    Since the width of this interval is exactly $\mu(y)$, we arrive at the claim.
\end{proof}
%

