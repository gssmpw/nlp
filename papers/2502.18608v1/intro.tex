\section{Introduction}
\label{sec:intro}
%
Recent advances in generative models have significantly improved their capabilities and applicability across various real-world domains. Notably, models like ChatGPT and other LLMs can now generate text that closely resembles human-written content. However, as generative models have been rapidly adopted by both businesses and individuals, their is a growing concern within the research community about their potential for malicious use. To address this issue, a growing body of research around \emph{watermarking} LLM-generated text has recently emerged \citep{kirchenbauer2023watermark, kuditipudi2024robust, aaronson2023openai, piet2024markwordsanalyzingevaluating, zhang2024remark, ning2024mcgmark}.  The primary strategy in this research involves embedding a \emph{hidden} signal (i.e., a secret watermark key) within the generated text, which can later be reliably detected by any third party who possesses knowledge of the secret watermark key.

While these watermarking techniques offer reliable and robust statistical guarantees to verify LLM-generated texts, they still fall short in addressing the potential attack models posed by malicious actors \cite{jovanovic2024watermarkstealinglargelanguage, zhang2024largelanguagemodelwatermark, pang2024attacking, wu2024bypassingllmwatermarkscoloraware, gloaguen2024black, gloaguen2024discovering}. Previous research on LLM watermarking often focuses on common attacks, such as deletion, insertion, or substitution, to simulate the behavior of users attempting to evade content detectors. For instance, a student might slightly modify a machine-generated essay, altering a few sentences with the hope of avoiding detection by their professor. However, a determined adversary could go further by reverse-engineering the watermarking scheme. By repeatedly querying the API of the watermarked LLM, they could \textquote{steal} the watermark by approximating the hidden secret key. Once estimated, the most significant threat is \emph{spoofing}, where an attacker generates (potentially harmful) text that appears to be watermarked when it is not. If large volumes of  \textquote{spoofed} content can be generated with minimal computational effort, the watermark becomes effectively useless, undermining its intended purpose and damaging the reputation of LLM providers by falsely attributing harmful or incorrect content to them.

Prior work on \emph{watermark stealing} attacks mostly studies the distribution-modifying algorithm by \citet{kirchenbauer2023watermark}. In contrast, our focus is on distribution-free watermarking \citep{kuditipudi2024robust}, which does not change the underlying token distribution. A major difference between the two watermarking techniques is that \citet{kuditipudi2024robust} uses a randomized watermark key, creating a correlation between the LLM-generated text and this secret key. During detection, a third party with knowledge of this secret watermark key can efficiently check for this correlation and verify whether the text is watermarked or not. With this approach in mind, we propose a mixed integer linear programming model that can accurately estimate the secret watermark key and enable \textquote{spoofing} attacks with only a few samples from the watermarked LLM. Overall, we make the following contributions:
%
\begin{itemize}
    \item We provide a framework that accurately estimates the secret watermark key used by the distortion-free watermarking algorithm \citep{kuditipudi2024robust}. We show that our mixed integer linear programming is robust to watermarked input after it is shifted and corrupted by the LLM user.  
    %
    \item With this secret watermark key estimation, we demonstrate a simple \textquote{spoofing} attack with a high success rate for both the stylized setting of binary vocabulary and a real-world LLM. 
\end{itemize}