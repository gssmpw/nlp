\section{Preliminary}
\label{sec:prelim}
%
\paragraph{Interaction protocol.}
Let $\cV$ be a discrete set of vocabulary, and let $p \in \cV^* \rightarrow \Delta(\cV)$ be an autoregressive \emph{language model} that maps a string of arbitrary length to a distribution over the vocabulary. Given a prefix $x \in \cV^*$, we write $p(\cdot | x)$ as the distribution of the next token. Let $\Xi$ denote the space of watermark key elements. The interaction protocol is as follows. 
%
\begin{enumerate}
    \item The LM provider shares a random key sequence $\xi \in \Xi$ with the detector. 
    %
    \item The user sends a prompt $x \in \cV^*$ to the LM provider.
    %
    \item The LM provider generates text $Y \in \cV^*$ as $Y = \generate(x, \xi)$. 
    %
    \item The user publishes text $\widetilde{Y}$, which is either 1) edited version of $Y$ or 2) text independent of $Y$. 
    %
    \item The detector determines if $\widetilde{Y}$ is watermarked or not.
\end{enumerate}
%
\subsection{Detection}
%
Given a prompt $x$ and the private key sequence $\xi$, the LM provider generates text using a \emph{decoder} function $\Gamma: \Xi \times \Delta(\cV) \rightarrow \cV$ defined as follows.
%
\begin{definition}[Distortion-free decoder]
    A decoder $\Gamma: \Xi \times \Delta(\cV) \rightarrow \cV$ is \emph{distortion-free} with respect to the distribution of a random watermark key sequence $\xi \in \Xi$ if for any $\mu \in \Delta(\cV)$ and $y \in \cV$, we have $\Pr[\Gamma(\xi, \mu) = y] = \mu(y)$. 
    \label{def:distortion-free}
\end{definition}
%
Formally, we define the generation algorithm as follows.
%
\begin{algorithm}[t]
%
\caption{$\generate$: Watermarked text generation }\label{alg:generate}
%
\begin{algorithmic}[1]
%
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
%
\REQUIRE Watermark key sequence $\xi \in \Xi^{n}$, generation length $m$, language model $p$, decoder $\Gamma$.
\ENSURE Generated string $y \in \cV^m$.
%
\FOR{$i \in [m]$}
%
\STATE $y_i \coloneqq \Gamma(\xi_i, p(\cdot | y_{: i-1}))$
%
\ENDFOR
%
\RETURN Generated string $y$
\end{algorithmic}
\end{algorithm}
%
Intuitively, as long as the key sequence is long enough such that the LM provider does not reuse any part of it for generation, calling $\generate$ is essentially equivalent to sampling directly from the language model. We formally show this claim in the following lemma.
%
\begin{lemma}
    Let $m, n \in \mathbb{N}$ with $n \geq m$. Let $\Gamma$ be distortion free w.r.t a distribution $\nu \in \Delta(\Xi)$ and let $\{ \xi_{i} \}_{i = 1}^{n} \sim \nu$ be a sequence of i.i.d random key variables. Let $Y = \generate(\xi; m, p, \Gamma)$. Then, $Y_i \sim p(\cdot | Y_{: i-1})$ for $i \in [m]$.   
\end{lemma}
%
\begin{proof}
%
    As $n \geq m$, we also have $\{ \xi_i \}_{i=1}^m \sim \nu$ is also a sequence of i.i.d key sequence. Then, by construction, we have $\forall i \in [m], y_i \coloneqq \Gamma(\xi_i, p(\cdot | y_{: i-1}))$. Since $\Gamma$ is distortion-free, by definition, we have $\Pr[\Gamma(\xi, \mu) = y] = \mu(y)$. Then, the claim follows by combining these two conditions. 
%
\end{proof}
%
For detection, the detector uses a hypothesis test defined as:
%
\begin{align}
    &H_0: \widetilde{Y} \text{ is not watermarked} \\
    %
    &H_1: \widetilde{Y} \text{ is watermarked}
    \label{eq:hypothesis-test}
\end{align}
%
Specifically, with $T$ samples, the detector computes a $p$-value with respect to a test statistic $\phi: \cV^* \times \Xi^* \rightarrow \mathbb{R}$ for $H_0$, \ie $\widetilde{Y}$ is independent of $\xi$. The output of detection progress is a non-asymptotic $p$-value: if $\phi$ returns a small $p$-value then the text is likely to be watermarked; otherwise if the $p$-value is large then the text is likely not to be watermarked. Hence, our goal is to design a test statistic $\phi$ such that $\hat{p}$ will be small when $\widetilde{Y}$ is watermarked. To this end, we rely on the notion of alignment cost $d: (\cV^* \times \Xi^*) \rightarrow \mathbb{R}$ to measure the quality of a match between a subsequence of the input text and a subsequence of the watermark key. Then, the test statistics $\phi$ can be set as the minimum alignment cost between any length $k$ subsequences of text and watermark key. 