\section{Methodology}
\label{sec:methodology}

This section outlines the process of generating watermarked text from an LLM and converting it into binary sequences, introduces the mixed integer linear programming approach for secret watermark key estimation, and details the detection procedure for watermarked text.

\subsection{Generation and Interaction Protocol}
\label{methodology}

Let $\cV$ be a discrete set of vocabulary. Let $\mathcal{R}$ and $\mathcal{G}$ represent two disjoint sets of vocabulary tokens, such that $\mathcal{R} \cap \mathcal{G} = \emptyset$ and $\mathcal{R} \cup \mathcal{G} = \cV$. Let $p: \cV^* \rightarrow \Delta(\cV)$ be an autoregressive \emph{language model} that maps a string of arbitrary length to a distribution over the vocabulary $\Delta(\cV)$. Given a prefix $x \in \cV^*$, we write $p(\cdot | x)$ as the distribution of the next token. Let $\Xi$ denote the space of watermark key elements; for simplicity we assume each element of a random key sequence $\left\{\xi_i\right\}_{i=1}^m \in [0,1]$ . The interaction protocol is as follows:
%
\begin{enumerate}
    \item The LM provider shares a random key sequence $\xi \in \Xi$ with the detector, and the sets $\mathcal{R}$ and $\mathcal{G}$. 
    %
    \item The user sends a prompt $x \in \cV^*$ to the LM provider.
    %
    \item The LM provider generates binary text $Y \in \cV^*$ as $Y = \generate(x, \xi, \mathcal{R}, \mathcal{G})$. 
    %
    \item The user publishes text $\widetilde{Y}$, which is either 1) as-is or edited version of $Y$ or 2) text independent of $Y$.
    %
    \item The detector determines if $\widetilde{Y}$ is watermarked or not.
\end{enumerate}
%

Algorithm~\ref{alg:generate} provides detail on the binary watermarked text generation. In order to convert tokens to binary values, for every token generation step we calculate the probability mass for all tokens in the sets $\mathcal{R}$  and $\mathcal{G}$, namely $\mathbb{P}(\mathcal{R})$ and $\mathbb{P}(\mathcal{G})$\footnote{By construction, $\mathbb{P}(\mathcal{R}) = 1 - \mathbb{P}(\mathcal{G})$.}. The generation of the $j^{\rm th}$ binary token is equal to $y_i = \mathbb{I}\left( \xi_j > \mathbb{P}_j(\mathcal{G}) \right)$, where $\mathbb{I}(\cdot)$ is the indicator function. Note that the binary conversion is a simplifying step we adopt in this work. One could generate text following Algorithm~\ref{alg:generate} by sampling the token $y_j$ from the $\mathcal{G}$ set of tokens if $\xi_j > \mathbb{P}(\mathcal{G})$, and from the $\mathcal{R}$ set if not.


\begin{algorithm}[t]
%
\caption{Binary watermarked text generation}\label{alg:generate}
%
\begin{algorithmic}[1]
%
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
%
\REQUIRE Watermark key sequence $\xi \in \Xi^{n}$, generation length $m$, language model $p$, token sets $\mathcal{R}$ and $\mathcal{G}$ over vocabulary $\cV$.
\ENSURE Generated string $y \in \cV^m$.
%
\FOR{$j \in [m]$}
%
\STATE $\mathbb{P}_j(\mathcal{G}) = \sum_{x_l \in \mathcal{G}} p(x_l | y_{: i-1})$
%
\IF{$\xi_j > \mathbb{P}_j(\mathcal{G})$}
\STATE $y_j = 1$
\ELSE
\STATE $y_j=0$
\ENDIF
\ENDFOR
%
\RETURN Generated binary string $y$
\end{algorithmic}
\end{algorithm}


\subsection{Watermark Key Estimation}

Given $n$ samples of generated binary strings $\{y_i\}_{i=1}^n$ of length $m$, we propose to use a linear programming approach to estimate the underlying watermark key $\xi^*$ used for the generation of those samples. For each watermark key $\xi^*_j$ in the sequence, our proposed approach estimates a lower bound $\ubar{\xi}^*_j$ and an upper bound $\bar{\xi}^*_j$ to construct an estimation interval $[\ubar{\xi}^*_j, \bar{\xi}^*_j]$, with the estimated key selected to be the mid-point of this interval.


We consider 3 cases, which methodologically build on each other:
\begin{enumerate}
    \item \textit{no alteration:} the available binary strings $y_i$ are available as-generated,
    \item \textit{watermark key shifting:} the elements of underlying key $\xi^*$ have been shifted by an unknown amount $k_i$ before the generation of each binary string $y_i$, and
    \item \textit{corruption:} some of the binary strings $y_i$ are corrupted, and hence not reliable for estimation.
\end{enumerate}


\subsubsection{No Alteration Case.}

For ease of notation, let the probability mass of the set $\mathcal{G}$ for the $i^{\rm th}$ sample in the $j^{\rm th}$ token be defined as $q_{j}^i$. We solve the following linear program to estimate simultaneously all the lower bounds $\ubar{\xi}^*$ for the watermark key sequence:

\begin{align}
    & \ubar{\xi}^* = \underset{\ubar{\xi}^*_j \in \mathbb{R}^m}{\text{minimize}} && \sum_{j=1}^m \ubar{\xi}^*_j \label{eq:baselower}\\
    & \text{s.t.} && \ubar{\xi}^*_j \geq  \mathbb{I}(y_{j}^i = 1) q_{j}^i \quad \forall i, j \notag
\end{align}


The form of the constraint mirrors the generation process of the binary string $y$. For the $j^{\rm th}$ token in the $i^{\rm th}$ sample to be $y_{j}^i = 1$, then it must be that $\xi^*_j > q_{j}^i$, so $q_{j}^i$ should be included as a lower bound for $\xi^*_j$. Conversely, if $y_{j}^i = 0$, then the lower bound should be $0$, i.e., the $i^{\rm th}$ sample does not provide any information on lower bounding $\xi^*_j$. In other words, this linear program finds the maximum of the lower bounds given by the samples $y_i$.

For estimating the upper bounds, we solve an equivalent linear program to the above:

\begin{align}
    & \bar{\xi}^* = \underset{\bar{\xi}^*_j \in \mathbb{R}^m}{\text{maximize}} && \sum_{j=1}^m \bar{\xi}^*_j \label{eq:baseupper}\\
    & \text{s.t.} && \bar{\xi}^*_j \leq  \mathbb{I}(y_{j}^i = 0) (q_{j}^i-1) + 1 \quad \forall i, j, \notag
\end{align}

for which the constraints again include the information carried by the samples $y_i$ for upper bounding $\xi^*$.

\subsubsection{Watermark Key Shifting Case.} In this setup, each sample watermark key might have been shifted by an unknown amount $k_i \in [0,...m] \subset \mathbb{N}_0$, i.e., for the $i^{\rm th}$ sample the watermark key would be $\xi^*_0 = \xi^*_{0+k_i}$\footnote{We assume the watermark key sequence starts from the beginning if the sum of the indexes exceeds the length of the watermark key.}. To account for that, we introduce a set of binary variables $z_i^k \in \{0,1\}$ in the linear program, where $z_i^k = 1$ if the sample $i$ has been shifted by an amount $k$, and $z_i^k = 0$ otherwise. The resulting lower bounds mixed integer linear program is as follows:


\begin{align}
    & \ubar{\xi}^* = \underset{\ubar{\xi}^*_j \in \mathbb{R}^m \, z^0, \dots, z^{m} \in \{0, 1\}^n}{\text{minimize}} \quad  \sum_{j=1}^m \ubar{\xi}^*_j \label{eq:shiftlower}\\
    &\text{s.t.} \quad \quad \quad \ubar{\xi}^*_j \geq  \mathbb{I}(y_{j+k}^i = 1) q_{j + k}^{i} - C (1 - z_i^k)\quad \forall i, j, k \notag \\
    & \quad \quad \quad \quad \sum_{k=1}^m z_i^k = 1 \, \, \forall i, \notag
\end{align}

where $C > 1 \in \mathbb{R}$. The intuition behind the constraints is that only the constraint relative to the correct shift $k$ should be satisfied in each sample $i$; in this form, if $z_i^k =0$, i.e., the sample $i$ has not been shifted by $k$, then the constraint becomes automatically satisfied and is vacuous. An equivalent form for the upper bound optimization can be written by adapting the constraints from problem (\ref{eq:baseupper}).


\subsubsection{Samples Corruption Case.} In this final setting, we allow the mixed integer programming to ignore a pre-set amount of samples $T$ so to make it robust to sample-level corruption. Such types of corruption can go from single token substitution to adversarial attacks, such as purposely injecting $y_i$ samples completely independent of the watermark key $\xi^*$. To achieve this, we introduce a further $n$-dimensional boolean variable $v \in \{0,1\}^n$, where $v_i = 1$ indicates the mixed integer programming has chosen to ignore the sample $i$. The resulting mixed integer linear program for determining the lower bounds is equal to:


\begin{align}
    & \ubar{\xi}^* = \underset{\ubar{\xi}^*_j \in \mathbb{R}^m \, z^0, \dots, z^{m}, v \in \{0, 1\}^n}{\text{minimize}} \quad  \sum_{j=1}^m \ubar{\xi}^*_j \label{eq:shiftcorrlower}\\
    &\text{s.t.} \quad \quad \quad \ubar{\xi}^*_j \geq  \mathbb{I}(y_{j+k}^i = 1) q_{j + k}^{i} - C (1 - z_i^k)\quad \forall i, j, k \notag \\
    & \quad \quad \quad \quad \sum_{k=1}^m z_i^k \geq (1-v_i) \, \, \forall i \quad \quad \sum_{i=1}^n v_i \leq T, \notag \notag 
\end{align}

where a value of $v_i=1$ allows for all constraints for sample $i$ to be vacuous. As in the other cases, an equivalent form for the upper bound optimization can be written by adapting the constraints from problem (\ref{eq:baseupper}).



\subsection{Detection}
%
For detection, the detector uses a hypothesis test defined as:
%
\begin{align}
    &H_0: \widetilde{Y} \text{ is not watermarked} \\
    %
    &H_1: \widetilde{Y} \text{ is watermarked}
    \label{eq:hypothesis-test}
\end{align}
%
Specifically, with $T$ samples, the detector computes a $p$-value with respect to a test statistic $\phi: \cV^* \times \Xi^* \rightarrow \mathbb{R}$ for $H_0$, \ie $\widetilde{Y}$ is independent of $\xi$. The output of detection progress is a non-asymptotic $p$-value: if $\phi$ returns a small $p$-value then the text is likely to be watermarked; otherwise if the $p$-value is large then the text is likely not watermarked. Hence, our goal is to design a test statistic $\phi$ such that $\hat{p}$ will be small when $\widetilde{Y}$ is watermarked. To this end, we rely on the notion of alignment cost $d: (\cV^* \times \Xi^*) \rightarrow \mathbb{R}$ to measure the quality of a match between a subsequence of the input text and a subsequence of the watermark key. Then, the test statistics $\phi$ can be set as the minimum alignment cost between any length $k$ subsequences of text and the watermark key. 