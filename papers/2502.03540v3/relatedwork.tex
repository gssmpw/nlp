\section{Related Works}
Masked Diffusion Models (MDMs) represent a promising alternative to autoregressive models for discrete data generation, particularly in language modeling. Recent advancements have focused on simplifying and generalizing the MDM framework to improve performance and training efficiency~\citep{md4, mdlm}. These studies introduced a continuous-time variational objective for MDMs, expressed as a weighted integral of cross-entropy losses, facilitating the training of models with state-dependent masking schedules. At the GPT-2 scale, these MDMs outperformed prior diffusion-based language models and demonstrated superior capabilities in zero-shot language modeling tasks~\citep{nie2024scalingmaskeddiffusionmodels,gong2024scalingdiffusionlanguagemodels}. 

MDMs generate sequences starting from a fully masked input and progressively unmasking positions until a clean sequence is reached. Once a token is unmasked, it will stay unchanged. However, there is not guarantee that the state is correct, considering the approximation errors arise from the imperfect fit to real-world data distributions. Additionally, time discretization~\citep{zhao2024informedcorrectorsdiscretediffusion} and numerical errors~\citep{zheng2024maskeddiffusionmodelssecretly} may further the error incurred during sampling processes.

To address these challenges, several solutions have been proposed. These include methods allowing models to revise prior predictions and guiding sampling trajectories using internal or external knowledge. Examples include informed correctors~\citep{zhao2024informedcorrectorsdiscretediffusion}, greedy ancestral methods~\citep{gong2024scalingdiffusionlanguagemodels}, and RDM sampling techniques~\citep{RDM, DPLM}, which leverage model scores to replace random masking with targeted corrections. None of these works, however, allow for the use of an external planner, and ~\citep{RDM, DPLM} are simply using a top-k sampling strategy without any concern for the theoretical underpinnings of the sampling strategies viability.

In terms of theoretically-backed methods for selecting the denoising order during a generative model's sampling process, the current literature is quite sparse. \cite{shih2022traininginferenceanyorderautoregressive,li2021discoveringnonmonotonicautoregressiveorderings} discuss this task from the perspective of Any-Order Autoregressive models,  with \cite{li2021discoveringnonmonotonicautoregressiveorderings} requiring a specially-trained external planner model using a specially designed architecture and \citet{shih2022traininginferenceanyorderautoregressive} taking the perspective that a fixed family of possible generation orders should be chosen a priori to eliminate redundancy.

The most closely related work to ours is likely the recent
DDPD~\citep{ddpd} introduced a generative process divided into a planner, which identifies corrupted positions, and a denoiser, which refines these positions. Though they discuss the ability to employ a MDM denoiser within their framework, their analysis and sampling is through the lens of uniform discrete diffusion models. In particular, as with \cite{li2021discoveringnonmonotonicautoregressiveorderings}, the success of their strategy is contingent upon training a large specialized planner model of comparable size to the denoiser itself. Moreover, in their framework, since they are based on uniform diffusion models, the partially de-noised sequence never contains any masked states, and there is no way for the planner to be separated into masked and unmasked components to design a sampling strategy with guaranteed finite-time along the lines of our Algorithm \ref{alg:OURpracticalsampling}. Given the possible perceived similarity of this concurrent work with ours, we provide a thorough comparison of DDPD with P2 in Appendix \ref{alg:DDPDsampling}, highlighting the greater flexibility and difference in role of P2s' planners.
 
%In contrast to these works, our approach extends the theoretical foundation of planner-guided sampling to the sequence level using continuous-time Markov chain (CTMC) theory. Unlike prior studies~\citep{md4, mdlm, DDPD}, which operate at the token level and assume independence among tokens, our mathematical framework enables sequence-level path planning in a principle manner. Moreover, we derive the Evidence Lower Bound (ELBO) for our path-planning framework, which mathematically supports the assessment of pretrained models as effective planners using ELBO. Compared to DDPD, our contributions include: 
%1. Planner Decomposition: We separate the planner into mask and unmask components, effectively mitigating planner biases.
%2. Practical Sampling Frameworks: We instantiate our framework with two practical samplers, self-planning, and BERT planner, neither of which requires a dedicated pretrained planner model. This broadens accessibility for practitioners by providing diverse and efficient solutions.


% \subsection{Modified Sampling Schemes for Masked Diffusion Models}

% \subsection{Planning-based Sampling}