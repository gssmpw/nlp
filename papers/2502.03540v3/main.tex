%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}


\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} 
\usepackage[utf8]{inputenc}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{enumitem}

\definecolor{codegray}{rgb}{0.95,0.95,0.95}
\lstset{
    backgroundcolor=\color{codegray},
    basicstyle=\ttfamily\small,
    columns=fullflexible,
    breaklines=true,
    language=python,
    keywordstyle=\color{blue}\bfseries,
    commentstyle=\color{green!50!black},
    stringstyle=\color{red!70!black},
    showstringspaces=false,
    tabsize=4,
}

%\usepackage{algorithm}
\usepackage{amssymb} % Add this to the preamble for \xmark and \checkmark
\usepackage{pifont}  % For \ding

%\newcommand{\xmark}{\ding{55}} % Define \xmark as a cross


\usepackage{algpseudocode}

% http://ctan.org/pkg/algorithmicx

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
% \newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[preprint]{style/icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{listings} % Add this to your preamble for code formatting
\usepackage{adjustbox} % For flexible table width adjustment


% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \theoremstyle{plain}
% \newtheorem{theorem}{Theorem}[section]
% \newtheorem{proposition}[theorem]{Proposition}
% \newtheorem{lemma}[theorem]{Lemma}
% \newtheorem{corollary}[theorem]{Corollary}
% \theoremstyle{definition}
% \newtheorem{definition}[theorem]{Definition}
% \newtheorem{assumption}[theorem]{Assumption}
% \theoremstyle{remark}
% \newtheorem{remark}[theorem]{Remark}
\parskip=4pt
\newcommand{\zerodisplayskips}{%
  \setlength{\abovedisplayskip}{2mm}%
  \setlength{\belowdisplayskip}{2mm}%
  \setlength{\abovedisplayshortskip}{1mm}%
  \setlength{\belowdisplayshortskip}{1mm}}
\appto{\normalsize}{\zerodisplayskips}
\appto{\small}{\zerodisplayskips}
\appto{\footnotesize}{\zerodisplayskips}
%\makeatother
\linespread{0.99}
\input{math_commands}
%\newcommand\alex[1]{\noindent{\color{orange} {\bf \fbox{Alex}} {\it#1}}}
%\newcommand\fred[1]{\noindent{\color{red} {\bf \fbox{Fred}} {\it#1}}}
%\newcommand\zack[1]{\noindent{\color{blue} {\bf \fbox{Zack}} {\it#1}}}
%\newcommand\sawan[1]{\noindent{\color{blue} {\bf \fbox{Sawan}} {\it#1}}}
%\newcommand\jarrid[1]{\noindent{\color{green} {\bf \fbox{Jarrid}} {\it#1}}}
%\renewenvironment{itemize}{\tempone\addtolength{\itemsep}{0.5\baselineskip}}{\temptwo}

%\setenumerate{noitemsep,topsep=0pt,parsep=0pt,partopsep=0pt}
%\usepackage[textsize=tiny]{todonotes}
\newcommand{\beginsupplement}{%
        \setcounter{table}{0}
        \renewcommand{\thetable}{S\arabic{table}}%
        \setcounter{figure}{0}
        \renewcommand{\thefigure}{S\arabic{figure}}%
}
\begin{document}

\twocolumn[
\icmltitle{
Path Planning for Masked Diffusion Model Sampling
% \\with Applications to Biological Sequence Generation
}
% \icmltitlerunning{Path Planning for Masked Diffusion Models with Applications to Biological Sequence Generation}


\icmlsetsymbol{equal}{*}
\icmlsetsymbol{equalsup}{$\dagger$}



\begin{icmlauthorlist}
\icmlauthor{Fred Zhangzhi Peng}{dukebme,equal}
\icmlauthor{Zachary Bezemek}{dukemath,equal}
\icmlauthor{Sawan Patel}{atom}
\icmlauthor{Jarrid Rector-Brooks}{mila,umontreal}\\
\icmlauthor{Sherwood Yao}{atom}
\icmlauthor{Alexander Tong}{mila,umontreal,equalsup}
\icmlauthor{Pranam Chatterjee}{dukebme,dukecs,equalsup}
\end{icmlauthorlist}

\icmlaffiliation{dukebme}{Department of Biomedical Engineering, Duke University, Durham, USA}
\icmlaffiliation{dukecs}{Department of Computer Science, Duke University, Durham, USA}
\icmlaffiliation{dukemath}{Department of Mathematics, Duke University, Durham, USA}
\icmlaffiliation{atom}{Atom Bioworks, Cary, NC, USA}
\icmlaffiliation{mila}{Mila -- Quebec AI Institute, Montréal, Canada}
\icmlaffiliation{umontreal}{Université de Montréal, Montréal, Canada}

\icmlcorrespondingauthor{Alexander Tong}{alexander.tong@mila.quebec}
\icmlcorrespondingauthor{Pranam Chatterjee}{pranam.chatterjee@duke.edu}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]
\newcommand{\icmlEqualSupContribution}{\textsuperscript{$\dagger$}Equal senior-author contribution.}
\printAffiliationsAndNotice{\icmlEqualContribution; \icmlEqualSupContribution}

\begin{abstract}
In this paper, we explore how token unmasking order influences generative quality in masked diffusion models (MDMs). We derive an expanded evidence lower bound (ELBO) that introduces a \textit{planner} to select which tokens to unmask at each step. Our analysis reveals that alternative unmasking strategies can enhance generation performance. Building on this, we propose \textit{Path Planning (P2)}, a sampling framework that uses a pre-trained BERT model or the denoiser itself to guide unmasking decisions. P2 generalizes all known MDM sampling strategies and significantly improves performance across diverse domains, including language generation (in-context learning, code generation, story infilling, mathematical reasoning, reverse curse correction) and biological sequence generation (protein and RNA sequences).

% In this work, we propose Path Planning (P2), a novel inference strategy for sample generation in Masked Diffusion Models (MDMs). P2 introduces a planner that skews the unmasking order of the MDMs towards paths that lead to higher-quality samples. P2 is general enough that it is able to encapsulate all existing MDM sampling strategies in the literature. We instantiate P2 into two practically viable samplers: self-planning and BERT-planning, both of which do not require the training of a specialized planner model. We validate the effectiveness of these samplers by the evaluation of a rigorously derived evidence lower bound (ELBO). We exhibit how our general yet simple modification to existing MDM strategies can improve sample quality across diverse set of sequence generation tasks, including language generation (in-context learning, code generation, story infilling, mathematical reasoning, reverse curse correction) and biological sequence generation (protein and RNA sequences). 



\end{abstract}


\section{Introduction}
%Autoregressive models (ARMs) have long stood as the gold standard for sequence generation, primarily due to their straightforward next-token prediction derived from the chain rule akin to human language~\citep{Touvron2023Llama2O,deepseekai2025deepseekr1incentivizingreasoningcapability}. 
%However, ARMs face intrinsic limitations in scenarios requiring bidirectional context or more flexible decoding orders~\citep{gong2024scalingdiffusionlanguagemodels}. These limitations are particularly apparent in language reasoning tasks such as math and biological sequences such as protein and nucleic acid sequences where the dependency does not follow strict left-to-right order~\citep{DPLM,Nijkamp2022ProGen2ET}. 

Inspired by the success of diffusion models in continuous space and the desire for bidirectional reasoning, much work has sought to design performant training algorithms for discrete diffusion models. While there are many possible discrete noising processes, most successful discrete diffusion approaches have converged to absorbing state diffusion \citep{Austin2021StructuredDD,Lou2023DiscreteDM} with new, simplified training objectives resulting in scalable masked diffusion models (MDMs) \citep{mdlm,md4,gat2024discreteflowmatching}. 

While most recent work has focused on improving MDM training, considerably less attention has been given to the impact of inference techniques on overall generative performance. This raises a question: \textit{Can we design new inference strategies to improve generative quality?} In this paper, we answer affirmatively by investigating how the order in which tokens are unmasked during MDM inference affects generative quality. While the MDM reverse process requires that each token is uniformly likely to be unmasked at a given step, this correctly reconstructs the true data distribution only under a perfect denoiser (for further discussion of this perspective, see Appendix \ref{subsection:AOARMs}). However, since any trained MDM is inherently imperfect and does not yield a tight ELBO, it has been empirically observed that a uniformly random unmasking order is suboptimal in many settings \cite{ou2024,shih2022traininginferenceanyorderautoregressive,li2021discoveringnonmonotonicautoregressiveorderings}.

We begin our study by reexamining the typical MDM ELBO and show that, for a fixed denoiser, we can expand the ELBO to include two additional terms, both involving a ``planner''\footnote{We adopt the term ``planner'' introduced by \citet{ddpd}.} whose role is to select which tokens should be unmasked at a given inference step \textit{as well as} optionally choosing already unmasked tokens to be remasked. Our ELBO shows that while the optimal planner for the optimal denoiser is indeed uniform unmasking, the strategy prescribed by the reverse process, one can obtain better generative quality for an imperfect denoiser through the use of a well tuned, non-uniform planner. Of particular note is that the ELBO's planner terms are effectively a reweighting of the typical MLM objective with additional small differences due to an added dependence on the denoiser.

These observations lead to our proposed method, \textit{Path Planning (P2)}, which makes use of the expanded ELBO to introduce a family of planners for use at inference time. Crucially, by noting the similarity between the planner ELBO terms and the typical MLM objective we show that in practice we can obtain effective planners \textbf{fully training-free} by employing either pre-trained BERT-type models or simply using the already trained denoiser. Moreover, P2 is shown the generalize all known existing sampling strategies in the MDM literature (see Table \ref{table:method_generalization}). We validate our training-free planning framework across a diverse set of experimental settings, showing that by using P2 a 1B parameter MDM model can outperform a 7B Llama model in math reasoning while far outpacing state-of-the-art ARMs for code generation on same same-sized models. At the same time, for biological sequence design we show that the combination of P2 and DPLM~\citep{DPLM} leads to state-of-the-art generation quality for proteins, while for RNA design we outperform competitive models and observe that our sequences lead to higher structural plausibility than even true, naturally occurring sequences.
%Recently, as an alternative framework able to overcome these limitations, masked diffusion models (MDMs)~\citep{Austin2021StructuredDD, Lou2023DiscreteDM, mdlm, md4} have gained significant traction. The standard ancestral sampling strategy is equivalent to an ``any-order autoregressive model'' that progressively unmasks each position but with a random decoding order rather than left-to-right \cite{UriaML14}. This implies that once a token is unmasked, it will stay unchanged, and mistakes made in the early stages can compromise subsequent predictions resulting in error propagation~\citep{ddpd,zhao2024informedcorrectorsdiscretediffusion}. Moreover, MDMs are trained on the average KL divergence between the data distribution and the distribution of the ancestral sampling schemes' output among all possible denoising orders. Conditionally upon a fixed denoising order, the distance between the true data distribution and model's generation output have empirically been observed to vary wildly \cite{ou2024,shih2022traininginferenceanyorderautoregressive,li2021discoveringnonmonotonicautoregressiveorderings}. For convenience, we include further discussion of the connection between MDMs and Any-Order Autoregressive Models in Appendix \ref{subsection:AOARMs}.
%This motivates us to explore finding a principled way to remask tokens and to select denoising orders in a more informed way than uniformly at random. We propose \textit{Path Planning} (P2), a general framework for sampling from MDMs using a principled unmasking order and allowing for the possibility of error correction for previously unmasked tokens. P2 generalizes existing sampling strategies~\citep{DPLM,RDM,DFM,md4,mdlm,ddpd} (see table~\ref{table:method_generalization}) in that the planner has access to information both about the state of the partially masked sequence in generation as well as the predictions about the data distribution from the MDM denoiser at each time step. 
%We first put forward a general framework for modifying the Markov chain used for sampling in MDMs in a way that allows for the incorporation of a prediction mechanism determining the next token to mask or resample in Subsection \ref{subsec:MathematicalFormulations}. We then derive an ELBO which decomposes into three parts, describing the role of the denoiser, the planner as it pertains to selecting masked tokens to unmasked, and the planner as it pertains to selecting unmasked tokens to remask in Proposition \ref{prop:ELBO}. This ELBO is demonstrated to correlate with the performance of a given planner, and can be used to evaluate the expected performance of a pretrained model as a predictor, as well as serve as a training objective for fine-tuning or training models from scratch (see Table \ref{tab:comparison-elbo}).
%Exploiting the generality of the form of the planner, we then decompose the planning strategy into unmasking and remasking phases, introduce a``stochasticity strength'' parameter, which allows for tuning the frequency of correction steps while still preserving guaranteed finite time convergence to a fully unmasked sequence in our sampling algorithm in Subsection \ref{subsec:samplingstrat}.
%In Subsection \ref{subsec:plugandplay}, we propose two novel and effective path planning methodologies within our general framework, (1) \textit{self path planning} (P2-self) which plans based only on the outputs of the denoising model without requiring an external model and \textit{BERT path planning}, which uses a pretrained BERT for remkasing planning (Section~\ref{subsec:plugandplay}). We empirically show that an 8M BERT can effectively guide a 150M MDM without much computation overhead. In contrast to previous works\citep{ddpd}, these methodologies completely forgo the need for a large, specially-trained planner model.
%We finally validate p2, a drop-in replacement of existing mask diffusion model sampling in three modalities---text (in-context learning, code generation, story infilling, mathematical reasoning and reverse curse correction), protein, and RNA sequence generation. Particularly, P2 sampling on a 1B MDM outperforms a 7B Llamma~\citep{Touvron2023Llama2O} in math reasoning and improves DiffuLLama (7B) to 17.6\% pass@1 in code generation where Llama7B achieves pass@1 of 1.7. In protein sequence generation, we improve the leading method - DPLM~\citep{DPLM} - with our P2, achieving state-of-the-art generation quality, with a foldability of 53.43\%. In RNA sequence generation, we outperform the language model counterpart, achieving state-of-the-art structural plausibility, and exceeding the natural sequences.  

%Recent advances have shown promise in text and protein sequence generations with comparable generation quality to ARMs. 


% Our main contributions are as follows:
% \begin{itemize}[noitemsep,topsep=0pt,parsep=0pt,partopsep=0pt]
%     \item First we present \textit{generalized path planning} (P2) for masked diffusion models, providing theoretical foundations for the design of planners for the token denoising order for MDMs. This framework encapsulates all sampling strategies currently found in the MDM literature.
%     \item Grounded in this theory, we design a novel sampling strategy - ``P2 sampling'' - which is guaranteed to converge in a finite number of iterations and allows for full exploitation of both the MDM denoiser and planners' model certainties. It also allows for the introduction of a ``stochastic remasking paramater'' to control the frequency at which previously unmasked tokens will be remasked during generation. 
%     \item Within this framework, we introduce two viable families of planners: \textit{self-planning}, where the denoiser functions as its own path planner, and \textit{BERT-planning}, where the denoiser is coupled with a pretrained BERT, empirically improving state-of-the-art masked diffusion models without training dedicated planner.
%     \item We demonstrate the empirical performance of three modalities of task, including
% language (in-context learning, code generation,
% story infilling, mathematical reasoning and reverse curse correction), and biological sequence
% generation (protein and RNA).
% \end{itemize}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figs/method.png}
    \vspace{-5pt}
    \caption{An example of the P2 sampling strategy (see Algorithm \ref{alg:OURpracticalsampling}). In each step, the denoiser $D^\theta(\cdot)$ makes predictions and the planner $G^\theta(\cdot)$ ranks and selects positions to unmask (green colored) and remask (red colored).
    % The parameters here are $\eta=1$ and $\kappa(t)=t$, so that at each time step $t$, the top $t$ positions as determined by the score from $G^\theta$ will be unmasked in the next generation step.
    }
    \label{fig:overview}
    \vspace{-5pt}
\end{figure}

\section{Background}

\paragraph{Notation}
We will denote by $S=\lbrace 1,\ldots,N\rbrace$ a finite dictionary of tokens, by $\bar{S}=S\cup \lbrace M\rbrace$ the extension of this dictionary via the addition of some masked state $M$. For a  metric space $\mc{X}$, we define by $\mc{P}(\mc{X})$ the space of Borel probability measures on $\mc{X}$. When $\mc{X}$ is finite we endow $\mc{X}$ with the discrete metric and let $|\mc{X}|$ denote the cardinality of $\mc{X}$. With some abuse of notation we freely identify $\mu\in \mc{P}(\mc{X})$ with a column vector in $[0,1]^{|X|}$  corresponding to the associated probability mass function. We denote by $\delta_x\in \mc{P}(\mc{X})$ the probability measure such that $\delta_x(y)\coloneqq 1$ if $x=y$ and $0$ otherwise and by $\text{Unif}(\mc{X}) \in \mc{P}(\mc{X})$ the uniform probability measure on $\mc{X}$. We suppose that we are interested in generating sequences of length $L$ comprised of elements of $S$ from some data distribution $p_{\text{data}}\in \mc{P}(S^L)$. We use $x_i$ to denote the $i$'th coordinate of an elements $x\in \bar{S}^L$, and $x^{-i}$ to denote the element in $\bar{S}^{L-1}$ which is the same as $x$ but with the $i$'th token removed. For $x\in\bar{S}^L$ and $y\in\bar{S}$, we denote by $x^{-i,y}$ the element in $\bar{S}^{L}$ which is the same as $x$ but with the $i$'th token replaced by $y$. We denote by $M^L\coloneqq(M,\dots,M)\in \bar{S}^L$.

\subsection{Masked Diffusion Models}\label{subsection:maskeddiffusionmodels}
%Here we provide a brief introduction to masked diffusion models. For more details and background on Discrete Diffusion Models, see Appendix \zack{ref}.
In a masked diffusion model, one starts with a a collection of probability mass functions given by, for $y\in \bar{S}^L$ and $t\in [0,1]$:
\begin{align}\label{eq:forwarddynamics}
P_t(y;p_{\text{data}})\coloneqq \alpha(t)p_{\text{data}}(y)+(1-\alpha(t))\delta_{M^L}(y)
\end{align}
for a monotone-decreasing, continuously differentiable noise scheduler $\alpha:[0,1]\tto [0,1]$ with $\alpha(0)=1$ and $\alpha(1)=0$, and finds continuous time Markov chain $\overset{\leftarrow}{X}_t$ such that $\Prob(X_t=x)=P_{1-t}(x;p_{\text{data}})$. 

A rate matrix generating $\overset{\leftarrow}{X}_t$ is given for $x\neq y\in \bar{S}^L$, by:{\small
\begin{align*}
\overset{\leftarrow}{Q}(y,x)=
-\frac{\dot{\alpha}(1-t)}{1-\alpha(1-t)}\sum_{i=1}^L\delta_{M}(x_i)p^i_{data}(y_i|x_{\neq M})\delta_{y^{-i}}(x^{-i})
\end{align*}}%
and $\overset{\leftarrow}{Q}(x,x)=\frac{\dot{\alpha}(1-t)}{1-\alpha(1-t)}\sum_{i=1}^L \delta_{M}(x_i)$ (see e.g. \cite{ou2024} Theorem 1). Here for $z\in \bar{S}^L$, $z_{\neq M}$ denotes the coordinates of $z$ which are not equal to $M$, and for $i\in \lbrace 1, \ldots, L\rbrace$, and $j\in S$:
\begin{align*}
p^i_{data}(j|z_{\neq M})\coloneqq p_{data}(\lbrace x : x_i=j\rbrace | z_{\neq M}).
\end{align*}

One then attempts to approximate $\overset{\leftarrow}{X}_t$ with $X^{\theta,\text{mask}}_t$ with transition matrix given for $x\neq y$ by:{\small
\begin{align}\label{eq:maskedbackwardsmatrix}
Q^{\theta,\text{mask}}_t(y,x)&\coloneqq
-\frac{\dot{\alpha}(1-t)}{1-\alpha(1-t)}\sum_{i=1}^L\delta_{M}(x_i)D_{i,y_i}^{\theta}(x)\delta_{y^{-i}}(x^{-i}).
\end{align}}%

Here we are using the ``mean parametrization'' of the approximate backwards matrix. That is, we have a neural network parameterized by $\theta$ which gives a ``denoising function'' $D^{\theta}:\bar{S}^L\tto \mc{P}(S)^L$, with the hope that
\begin{align}\label{eq:Dtheta}
D_{i,\cdot}^{\theta}(x)\approx p^i_{data}(\cdot | x_{\neq M}) \in \mc{P}(S).
\end{align}
In particular, one enforces during inference that $D^{\theta}_{i,y_i}(x)=\delta_{x_i}(y_i)$ if $x_i\neq M$.

Approximate samples from the data distribution are then obtained via simulating the Markov chain $X^{\theta,\text{mask}}$ with $X^{\theta,\text{mask}}_0=M^L$ to time 1.

%\zack{Despite the fact that masked diffusion models perform the best out of the possible noising processes reported in the  Discrete Diffusion/Flow Model framework, they still underperform autoregressive models at many generative tasks. - Zack maybe this goes in the intro? references?}.

%We identify two possible areas for improvement:
%\begin{enumerate}
%\item Once a masked token is unmasked, the state it is unmasked to is not able to be corrected. 
%\item While if $D_{i,j}^{\theta}(x)$ perfectly reconstructed $p^i_{data}(j|x_{\neq M})$, unmasking tokens in any order would yield $X^{\theta,\text{mask}}_1\sim P^{\theta,\text{mask}}_1 \approx p_{data}$, it has been empirically confirmed that there certain paths among the $L!$ possible unmasking orders that lead to higher quality samples. In particular, the loss used to train an MDM is the average KL divergence between the distribution of $X^{\theta}_1$ and $p_{data}$ among all possible denoising orders, and conditionally upon a fixed denoising order the distance between the distribution of $X^{\theta}_1$ and $p_{data}$ may vary wildly.
%\end{enumerate}
%\alex{Can we make the above into a proposition? Think it would be nice to say precisely if A = B then C = D. Otherwise we have no guarantees about the best sampling path.} \zack{What do you want the proposition to say? The ELBO is given by the average $KL$ divergence between $p_{data}$ and the distribution of $X^{\theta,\text{mask}}_1$ over all paths. So if you train to a loss of 10, you might have half of the paths are perfect and half have $KL$ divergence 20. I updated point 2 to explain this better. I thought about it a bit more, we really cant say anything without some information about the concentration about the mean, like the variance, which we dont have any access to.}

%In order to unpack on these points, it helps to take the perspective that masked diffusion models are equivalent to Any-Order Autoregressive Models \cite{UriaML14}. For reference, a further discussion of this connection and pseudocode describing the corresponding Gillespie algorithm \cite{gillespie_exact_1977,GILLESPIE1976403} can be found in Appendix \ref{subsection:AOARMs}.%That is, in order to sample $X^{\theta,\text{mask}}_1$, one can instead simply simulate the effective time-inhomogenous jump chain $\lbrace X^{\theta,\text{mask}}_{\tau_{k}}\rbrace_{k=1}^L$, $X^{\theta,\text{mask}}_0=M^L$, (see Appendix \ref{subsection:CTMC}) which transitions by unmasking one token at a time via choosing among the masked coordinates uniformly at random and then unmasking the chosen coordinate to $y_i\in S$ with probability $D^{\theta}_{i,y_i}(x)$.  

\section{Path Planning}

\begin{table*}[th]
\centering
\caption{Generalization of Existing Sampling Methods within Our P2 Framework. \textbf{Mask Planner ($G^M_j(y,x)$)} gives the likelihood that a mask token should be unmasked. \textbf{Unmask Planner ($G^U_j(y,x)$)} gives the likelihood that an unmask token should be kept. $D^{\theta}_{j,y_j}(x)$ gives the prediction probability of the denoiser at position j for token $y_j$. $B^{\theta}(\cdot)$ is a BERT. $G^\theta(\cdot)$ is an external planner.
}
\label{table:method_generalization}
\resizebox{\linewidth}{!}{
\begin{tabular}{lccccc}
\hline
\textbf{Method} & \textbf{Remasking} & \textbf{Planning} & \textbf{Stochasticity Control} & \textbf{Mask Planner ($G^M_j(y,x)$)} & \textbf{Unmask Planner ($G^U_j(y,x)$)} \\ \hline
Ancestral~\citep{md4,mdlm}                     & \xmark     & \xmark     & \xmark          & $\mathcal{U}(0,1)$               & 1                           \\
Greedy Ancestral~\citep{gong2024scalingdiffusionlanguagemodels}             & \xmark     & \checkmark & \xmark          & $D^{\theta}_{j,y_j}(x)$          & 1                           \\
DFM Sampling~\citep{DFM}                 & \xmark     & \xmark     & \checkmark      & $\mathcal{U}(0,1)$               & $\mathcal{U}(0,1)$          \\
RDM Sampling~\citep{RDM,DPLM,DPLM2}                  & \checkmark & \checkmark & \xmark          & $D^{\theta}_{j,y_j}(x)$          & $D^{\theta}_{j,y_j}(x)$     \\
DDPD~\citep{ddpd}                          & \checkmark & \checkmark & \xmark          & $G^{\theta}_j(y)$                & $G^{\theta}_j(y)$           \\
\textbf{Path Planning (Self-Planning, ours)} & \checkmark & \checkmark & \checkmark      & $D^{\theta}_{j,y_j}(x)$          & $D^{\theta}_{j,y_j}(x)$     \\
\textbf{Path Planning (BERT Planner, ours)}  & \checkmark & \checkmark & \checkmark      & $D^{\theta}_{j,y_j}(x)$          & $B^{\theta}_{j,y_j}(y)$     \\ \hline
\end{tabular}
}
\end{table*}


\subsection{Mathematical Formulation}\label{subsec:MathematicalFormulations}

In order to formulate P2 we begin by modifying the jump matrix for the approximate backwards process (\eqref{eq:maskedbackwardsmatrix}), introducing a new function $G^{\theta}:S^L\times\bar{S}^L\tto [0,1]^L$, which we refer to as the planner. $G_j^\theta(y,x)$ approximates the likelihood that the $j$'th token in a partially denoised sequence $x\in \bar{S}^L$ should be (re)sampled given the conditional information about the rest of the sequence $x$ and of the clean data $y$ as predicted by $D^{\theta}$. In Section \ref{subsec:plugandplay}, we discuss potential choices of planners and how previous works fall into this general framework.

We next define $F^{\theta}:\bar{S}^L\times \bar{S}^L\tto [0,1]^L$ by 
\begin{align*}
F^{\theta}_j(y,x)&\coloneqq \delta_{M}(x_j)\bb{E}_{Y \sim D^{\theta}(x)}[G^{\theta}_j(Y^{-j,y_j},x)]\\ 
&+(1-\delta_{M}(x_j))\bb{E}_{Y\sim D^{\theta}(x)}[G^{\theta}_j(Y^{-j,x_j},x)]
\end{align*}
where here we use the shorthand $Y\sim D^{\theta}(x)$ to mean $Y\sim \otimes_{i=1}^L D^{\theta}_{i,\cdot}(x)$.

Via our interpretation of the role of $G^\theta$, $F_{j}^\theta(y,x)$ gives the likelihood that the $j$'th position of $x$ should be (re)sampled given the information about the rest of the sequence $x$ and the data's $j$'th token via averaging out the information provided about the rest of the data's tokens from $D^{\theta}$. 

Finally, we define {\small
\begin{align*}
\hat{D}^{\theta}_{i,y_i}(x)=D^{\theta}_{i,y_i}(x)\delta_{M}(x_i)+\frac{D^{\theta}_{i,y_i}(x^{-i,M})}{1-D^{\theta}_{i,x_i}(x^{-i,M})}(1-\delta_{M}(x_i)).
\end{align*}}%

That is, when $x_i$ is masked $\hat{D}^{\theta}_{i,y_i}(x)$ approximates the probability that the $i$'th token of $x$ should be unmasked to $y_i$ given the conditional information about the unmasked tokens in $x$, and when $x^i$ is not masked, $\hat{D}^{\theta}_{i,y_i}(x)$ approximates the probability that $i$'th token of $x$ should be resampled to a value other than $x_i$, given the conditional information about the unmasked tokens in $x$ other than $x_i$.

We now seek to modify $Q^{\theta,\text{mask}}$ from \eqref{eq:maskedbackwardsmatrix} in a way so that $F^\theta$ - by way of the planner $G^\theta$ - plays the role of selecting which position should be unmasked/resampled and $\hat{D}^{\theta}$ plays the role of choosing what it should be (re)sampled to.

For $x\neq y\in \bar{S}^L$, we thus set: {\small
\begin{align}\label{eq:P2ratematrix}
Q^{\theta}_t(y,x)\coloneqq -\frac{\dot{\alpha}(1-t)}{1-\alpha(1-t)}\sum_{i=1}^L F^{\theta}_{i}(y,x)\hat{D}^{\theta}_{i,y_i}(x) \delta_{y^{-i}}(x^{-i}).
\end{align}
}%

For reference, we provide a computationally viable Gillespie sampling method \cite{gillespie_exact_1977,GILLESPIE1976403} which approximates samples from $X^{\theta}$ with jump matrix $Q^{\theta}$ and provides intuition for the role of the Planner is given by Algorithm \ref{alg:ourgillespiesampler} in Appendix \ref{subsection:Gillespie}.   

Observing Algorithm \ref{alg:ourgillespiesampler}, we see that P2 allows for the planner $G^\theta$ to guide the denoising process towards a more optimal path of denoising orders using the information from both the partially noised sequence $x_t$ and the predicted clean sequence $y$ from the denoiser, and further introduces the ability to resample previously masked tokens using information from both the partially generated sequence and the output of the denoiser. 

The interpretation of the Planner as a mechanism for guiding the denoising process toward an optimal path is furthered by the following:
\begin{proposition}\label{prop:ELBO}
Define $P^{\theta}_1\in \mc{P}(S)$ by $P^{\theta}_1(x)=\Prob(X^{\theta}_1=x)$, where $X^\theta$ is the CTMC with rate matrix given in Equation \eqref{eq:P2ratematrix}. Then we have an ``Evidence Based Lower Bound'' $E(x^0)\leq \log (P^{\theta}_1(x^0))$ for each fixed $x^0\in S^L$ given by $E(x^0)=E_{MP}(x^0)+E_{UP}(x^0)+E_D(x^0)$, where:{\small\allowdisplaybreaks
\begin{align*}
E_{MP}(x^0)&=-\int_0^1 \frac{\dot{\alpha}(t)}{1-\alpha(t)} \bb{E}_{X_t\sim P_{t }(\cdot;\delta_{x^0})}\biggl[\sum_{i=1}^L\delta_{M}([X_t]_i)\\ 
&\times\bb{E}_{Y\sim D^{\theta}(X_t)}[\log(G^{\theta}_i(Y^{-i,x^0_i},X_t))]\biggr]dt\\ 
E_{UP}(x^0)&=-\int_0^1 \frac{\dot{\alpha}(t)}{1-\alpha(t)} \bb{E}_{X_t\sim P_{t }(\cdot;\delta_{x^0})}\biggl[\sum_{i=1}^L(1-\delta_M([X_t]_i))\\ 
&\times\bb{E}_{Y\sim D^{\theta}(X_t)}[\log(1-G^{\theta}_i(Y^{-i,x_i^0},X_t))]\biggr]dt\\
E_D(x^0)&=-\int_0^1 \frac{\dot{\alpha}(t)}{1-\alpha(t)} \bb{E}_{X_t\sim P_{t }(\cdot;\delta_{x^0})}\biggl[\sum_{i=1}^L\delta_{M}([X_t]_i)\\ 
&\times\log(D^{\theta}_{i,x^0_i}(X_t))\biggr]dt.
\end{align*}}%
Here $P_t$ is defined per \eqref{eq:forwarddynamics}.
\end{proposition}

This ELBO offers a simple interpretation, recalling we seek to maximize the expected value of each term with respect to $x_0\sim p_{data}$. $E_{MP}(x^0)$ optimizes the role of the Planner as it pertains to masked tokens in a partially denoised sequence. That is, as a mechanism for selecting the a viable masked position to insert a ``clean'' token as suggested by $D^{\theta}$. If $D^{\theta}$ suggests to unmask the coordinate $i$ to a value which is representative of the data distribution, then $G^{\theta}_i$ should be large so that the $i$'th position is selected. $E_{UP}(x^0)$ optimizes the role of the Planner as it pertains to unmasked tokens in a partially denoised sequence. That is, as a mechanism for selecting the an unmasked token to resample via remasking and inserting back into $D^{\theta}$. If the $i$'th token already contains a token which is representative of the data distribution, then $G^{\theta}_i$ should be small, so that the $i$'th token remains in the sequence. $E_D(x^0)$ is the the ELBO used for the denoiser of a standard masked diffusion model (see \eqref{eq:MDMELBO}). 

It is worth observing that $E(x^0)\leq E_D(x^o)$, so our ELBO is necessarily a worse lower bound than that arrived at via a standard masked diffusion model. One can observe that setting $G^{\theta}_i(y,x)=\delta_M(x)$, $E_{MP}(x)=E_{UP}(x)=0$, and a standard masked diffusion model is recovered. However, the ELBO is only a bound on the KL divergence between the true data distribution and the approximate one (see the discussion in Appendix \ref{subsection:ELBOrole}). Moreover, our ELBO provides a mathematically-backed methodology for assessing when a choice of pretrained model may serve as an effective planner for a given denoiser. In Table~\ref{tab:comparison-elbo}, we show that planners ranging from 8M to 3B parameters have similar ELBO and thus have similar generation performance (Figure ~\ref{fig:ablation_planner}).  Lastly, it provides a methodology for training a Planner for a given denoiser, or training both in tandem, in a principled way. Training models for this specific purpose is an interesting avenue for future research. 


\subsection{A Family of Planners: The P2 Sampling Strategy}\label{subsec:samplingstrat}

Here we introduce the P2 sampling strategy, which allows for controllability over the role of the planner, exploitation of the information provided about all tokens in the sequence from $G^{\theta}$ and $D^{\theta}$, and guaranteed convergence of the sampling procedure to a fully unmasked sequence.
 
We decompose the planner into two components:
\begin{align*}
G_j^\theta(y,x)&=\delta_{M}(x_j)G_j^{\theta,M}(y,x)\\ 
&+ (1-\delta_M(x_j))(1-G_j^{\theta,U}(y,x)).
\end{align*}

That is, the ``masked token planner'' $G_j^{\theta,M}(y,x)$ predicts the liklihood that a masked token at the $j$'th position should be unmasked, and the ``unmasked token planner'' $G_j^{\theta,U}(y,x)$ predicts the likelihood that an unmasked token at the $j$'th position should be kept.

We then employ a modified ``top k'' sampling strategy, which introduces the possibility of changing multiple tokens per iteration and better exploits the information provided by the scheduler. We define $\kappa:\lbrace 1,\ldots,L\rbrace \tto \lbrace 1,\ldots,L\rbrace$ to be any monotone non-decreasing function with $\kappa(L)=L$, which will serve as an ``unmasking scheduler'' for how many tokens should be denoised at a given time step. In particular, at the  $t$'th iteration, $\kappa(t)$ tokens are guaranteed to be unmasked in the partially generated sequence.

We further introduce a stochasticity strength parameter $\eta$, and define the family of probability measures: 
{\small
\begin{align}
\label{eq:planner_with_eta}\tilde{G}^{\eta}_j(x,y)\propto\eta\delta_{M}(x_j)G_j^M(y,x)+(1-\delta_M(x_j))G_j^U(y,x)
\end{align}}%
for $\eta\geq 0$. Note that while the Planner $G^\theta_j$ determines if the $j$'th token is a valid candidate to change (a masked token to an unmasked one or vice versa), $\tilde{G}^{\eta}_j$ determines whether the $j$'th token is valid to be unmasked (or kept unmasked if it already is). As $\eta$ increases, we will keep fewer unmasked tokens, so the frequency of remasking increases. Tuning $\eta$ allows us to control the stochasticity (frequency of remasking) of the sampling process as proposed in DFM~\citep{DFM}, which is overlooked in existing sampling strategies~\citep{md4,gong2024scalingdiffusionlanguagemodels,RDM,DPLM,DPLM2,ddpd}.

Letting TopPos$_k(v)$ return the indices of the largest $k$ values in a non-negative vector $v$, our sampling algorithm is given in Alg. \ref{alg:OURpracticalsampling}. See also \ref{fig:overview} for a diagram exhibiting a toy example of generation with P2 Sampling.

\begin{algorithm}[h]
\small
\caption{P2 Sampler (Pytorch Implementation in Appendix~\ref{sec:pytorch_impl}).}
\label{alg:OURpracticalsampling}
\begin{algorithmic}[1]
\State \textbf{Initialize:} $t \gets 0$, $x_0 \gets (M, \dots, M)$, planner $G^\theta$, denoiser $D^\theta$, scheduler $K$
\For{$t = 1 : L$}
    \State \colorbox{gray!20}{\textbf{Plan:}} 
    \State Sample $y \sim D^\theta(x_t)$
    \State UpdatePos $\gets \text{TopPos}_{\kappa(t)}\big(\tilde{G}^\theta_{\cdot}(y, x_t)\big)$
    \State \colorbox{gray!20}{\textbf{Denoise:}}
    \For{$j \in \text{UpdatePos}$}
        \If{$[x_t]_j = M$}
            \State $[x_t]_j \gets y_j$
        \EndIf
    \EndFor
    \For{$j \notin \text{UpdatePos}$}
        \If{$[x_t]_j \neq M$}
            \State $[x_t]_j \gets M$
        \EndIf
    \EndFor
\EndFor
\State \textbf{return} $x_L$
\end{algorithmic}
\end{algorithm}



\subsection{Plug-and-Play Path Planning Sampler}\label{subsec:plugandplay}
% Based on the generalized path planning (P2) theory, we propose two practically viable sampling strategies: self-planning and BERT planning. Self-planning utilizes the denoiser itself as the planner without an external model. In BERT planning, we argue that training a dedicated planner~\citep{ddpd} is unnecessary under the P2 framework, and a pretrained BERT can function as the planner effectively.


\subsubsection{Self-Planning with Denoiser-Predicted Probabilities}

We propose a self-planning mechanism by leveraging denoiser-predicted probabilities to guide unmasking and remasking decisions. Within the P2 framework, the unmask planner and mask planner are unified by setting $G^U_j(y, x) = G^M_j(y, x) = D^{\theta}_{j, y_j}(x)$, that is, the denoiser itself serves as the planner.
For mask positions, the denoiser is trained to predict tokens given the surrounding context, and the predicted probabilities serve as confidence estimates for the correctness of token predictions. This methodology aligns with established practices in the literature~\citep{gong2024scalingdiffusionlanguagemodels, Chang_2022_CVPR, RDM, DPLM, DPLM2}.
However, a concern arises for unmasked positions, as these tokens act as context during training and are not directly supervised. This raises the question: \emph{Are the predicted probabilities for unmask positions meaningful?} Our empirical evaluation demonstrates that, despite the absence of supervision for unmask positions, the ELBO (weighted cross-entropy loss, see Prop.~\ref{prop:ELBO}) for unmasked tokens surpasses that of BERT, which explicitly trains on both masked and unmasked tokens (see Table~\ref{tab:comparison-elbo}). Furthermore, ablating the denoiser-predicted probabilities for unmasked positions by replacing them with uniformly sampled values results in significant performance degradation (see Table~\ref{tab:ablation_self_planning}). This evidence confirms that the probabilities for unmask tokens are indeed informative, even without direct training.
We hypothesize two key factors behind this phenomenon. 1) During masked token prediction, the model inherently learns robust representations of unmasked tokens for predicting the masked positions. 2) The model's output layer projects embeddings of both masked and unmasked tokens into a shared logits space. Consequently, unmasked tokens can yield meaningful logits.


\subsubsection{BERT-planning}
In BERT-planning, we introduce a class of special planner BERT~\citep{Devlin2019BERTPO}, a bidirectional language model trained to predict the correct tokens given the corrupted sequences (15\% of tokens masked and 1.5\% of tokens uniformly flipped to other tokens). 
Despite such a simple training objective, BERT learns to estimates the naturalness of a token with the predicted probabilities which demonstrates wide application in zero-shot mutation prediction~\citep{Hie2022EfficientEO}. Compared to training a dedicated planner that is equal-size to denoiser as in DDPD~\citep{ddpd}, BERT is more versatile, flexible in sizes and often available in common tasks such as text~\citep{Devlin2019BERTPO,Liu2019RoBERTaAR,Lan2019ALBERTAL}, protein~\cite{esm2,esm3,DPLM,DPLM2} and RNA~\citep{penic2024rinalmo}.

Let $B^{\theta}:S^L\tto \mc{P}(S)^L$ be a pretrained BERT model, so that $B^{\theta}_{j,y_j}(y)$ is assigning the probability that the jth token in the sequence $y$ is clean. In BERT planning we set unmask planner to be the BERT $G_j^U(y,x)=B^{\theta}_{j,y_j}(y)$ and mask planner to be the denoiser $G_j^M(y,x)=D_{j,y_j}^\theta(x)$.


\subsection{P2 Generalizes Existing Sampling Methods}


In Table~\ref{table:method_generalization}, we show the existing sampling methods fit into our P2 framework with specific parameters. Ancestral sampling disables the remasking by setting the Unmasked Planner ($G^U_j(y,x)$) to always output 1, i.e., the likelihood that an unmask token should be kept is always 1, and the mask planner $G^M_j(y,x)$ functions as a uniform sampler as it randomly selects mask positions. Greedy ancestral sampling improves open this by using the denoiser $D^{\theta}_{j,y_j}(x)$ as the mask planner $G^M_j(y,x)$. DFM sampling randomly selects positions, and enables remasking by introducing a tunable stochasticity strength $\eta$. RDM functions identically to our self-planning by using the denoiser for both mask and unmask planning but it omits the stochasticity control with the default stochasticity strength $\eta=1$. DDPD introduces external planners and purely relies on the planner for both mask and unmask position planning with default stochasticity strength $\eta=1$. See Appendix \ref{subsec:DDPDcomparsion} for further comparison of P2 with DDPD.
\section{Experiments}



\subsection{Protein Sequence Generation}
\label{sec:protein sequence generation}
\begin{table}[ht]
\scriptsize  % Compact font size
\setlength{\tabcolsep}{4pt}  % Adjust column spacing
\caption{Protein Sequence Generation Benchmark. The arrows indicate whether higher (↑) or lower (↓) values are better. See Figure~\ref{fig:perf_vs_len} for a detailed comparison.}
\centering
\resizebox{\columnwidth}{!}{%
\begin{tabular}{lrrrrrr}
\toprule
Model Name & pLDDT (↑) & pTM (↑) & pAE (↓) & Foldability (\%) (↑) & Entropy (↑) & Diversity (\%) (↑) \\
\midrule
EvoDiff & 31.84 & 0.21 & 24.76 & 0.43 & \textbf{4.05} & 93.19 \\
ESM3 & 34.13 & 0.23 & 24.65 & 1.50 & 3.99 & \textbf{93.44} \\
Progen2-small & 49.38 & 0.28 & 23.38 & 4.48 & 2.55 & 89.31 \\
Progen2-large & 55.07 & 0.35 & 22.00 & 11.87 & 2.73 & 91.48 \\
Progen2-medium & 57.94 & 0.38 & 20.81 & 12.75 & 2.91 & 91.45 \\
DPLM-150M & 80.23 & 0.65 & 12.07 & 48.14 & 3.14 & 92.80 \\
\textbf{DPLM-150M + P2} & \textbf{80.98} & \textbf{0.68} & 11.43 & 49.86 & 3.25 & 92.63 \\
DPLM-650M & 80.02 & 0.67 & 11.69 & 51.86 & 3.20 & 91.45 \\
\textbf{DPLM-650M + P2} & 80.78 & \textbf{0.68} & \textbf{11.39} & \textbf{53.43} & 3.24 & 91.97 \\
\bottomrule
\end{tabular}
}  % End resizebox
\label{tab:protein_performance}
\end{table}


\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figs/protein_vis_structure.png}
    \vspace{-10pt}
    \caption{Visualizing the predicted structures of generated protein (top) and RNA (bottom) sequences. The protein structures are predicted by ESMFold~\citep{esm2} and the RNA structures are predicted by AlphaFold3 \citep{abramson2024accurate}. Additional structures depicted in Figure~\ref{fig:protein_structures_group1}.}
    \label{fig:protein_rna_vis}
\end{figure}

\textbf{Setup and Evaluation.}
We benchmark our method against state-of-the-art protein sequence generation models, including discrete diffusion models (DPLM~\citep{DPLM}, EvoDiff~\citep{Alamdari2024ProteinGW}, and ESM3~\citep{esm3}), an autoregressive model (ProGen2~\citep{Nijkamp2022ProGen2ET}), and masked language models (ESM2~\citep{esm2}). Each model generates 100 sequences across lengths in $[200, 300, \dots, 800]$, following their respective sampling strategies, with modifications ensuring fair evaluation.
Protein sequence quality is assessed using ESMFold~\citep{esm2}, measuring foldability through pLDDT, pTM, and pAE scores. We define foldability as the percentage of sequences satisfying pLDDT $> 80$, pTM $> 0.7$, and pAE $< 10$. Additionally, we analyze token entropy and sequence diversity to detect mode collapse. Further details on experimental settings and evaluation metrics are provided in the Appendix~\ref{sec:protein_benchmark_eval}.

\textbf{Results.} 
As summarized in Table~\ref{tab:protein_performance}, our P2 algorithm applied to DPLM (150M and 650M) consistently improves all folding metrics—pLDDT, pTM, and pAE—outperforming the default RDM sampling strategy~\citep{RDM}. Importantly, this improvement does not compromise token entropy or sequence diversity, highlighting P2's ability to maintain diversity while enhancing quality. 

When compared to baselines, including the 2.7B ProGen2-Large autoregressive model and discrete diffusion counterparts ESM3 and EvoDiff, P2 demonstrates remarkable foldability improvements. Visualizations of predicted structures for generated sequences are shown in Figure~\ref{fig:protein_rna_vis}, illustrating P2's ability to generate highly foldable, structurally plausible proteins. Detailed performance comparisons across sequence lengths are provided in Figure~\ref{fig:perf_vs_len}. Overall, these results motivated us to experimentally validate generated sequences. 

\subsection{The Design Space of Path Planning}
\label{sec:exp:sampling}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figs/P2_design_space.png}
    \caption{The Design Space of P2 (See Figure~\ref{fig:appendix_design_space_p2} for more). P2 Generalizes existing sampling algorithms with specific stochasticity strength and planner choice. 
    }
    \label{P2_design_space}
\end{figure}
Our Path Planning (P2) framework generalizes existing sampling strategies, including vanilla ancestral sampling, greedy ancestral sampling, RDM sampling, and DFM sampling, by incorporating specific parameterizations. In Figure~\ref{P2_design_space}, we instantiate these sampling algorithms and evaluate their performance on protein sequence generation, focusing on foldability (additional metric results are provided in Figure~\ref{fig:appendix_design_space_p2}). 

Vanilla and greedy ancestral sampling employ a stochasticity strength of 0, effectively disabling remasking, which results in poor performance. DFM sampling introduces tunable stochasticity, leading to improved performance over ancestral sampling; however, it lacks trajectory planning, which limits its effectiveness. RDM sampling, by contrast, enables remasking with a default stochasticity strength of 1 and utilizes the denoiser’s confidence for self-planning, yielding better sampling quality.

P2 combines the advantages of these existing algorithms, offering both controllable stochasticity strength and planning guidance. By tuning stochasticity strength, P2 can enhance RDM sampling and optionally leverage an external BERT planner to further steer the sampling trajectory toward generating high-quality sequences.





\subsection{Ablation of Path Planning}


\begin{table}[ht]
\scriptsize  % Compact font size
\setlength{\tabcolsep}{4pt}  % Adjust column spacing
\caption{Ablation of Sampling Strategies. Path planning (P2) outperforms existing sampling strategies, including DDPD. The arrows indicate whether higher (↑) or lower (↓) values are better. }
\centering
\resizebox{\columnwidth}{!}{%
\begin{tabular}{lrrrrrr}
\toprule
Sampling Algorithm & pLDDT (↑) & pTM (↑) & pAE (↓) & Foldability (\%) (↑) & Entropy (↑) & Diversity (\%) (↑) \\
\midrule
Vanilla Ancestral & 44.08 & 0.34 & 20.61 & 2.00 & \textbf{4.03} & \textbf{93.63} \\
RDM Sampling & 74.67 & 0.71 & 10.33 & 43.00 & 3.85 & 93.12 \\
\textbf{P2 + 8M BERT Planner} & \textbf{78.24} & \textbf{0.74} & \textbf{9.11} & \textbf{44.50} & 3.80 & 92.77 \\
DDPD + 8M BERT Planner & 46.51 & 0.24 & 23.20 & 0.25 & 0.31 & 51.69 \\
Ancestral & 52.67 & 0.46 & 17.64 & 7.75 & 3.98 & 93.42 \\
\bottomrule
\end{tabular}
}  % End resizebox
\label{tab:ablation_planner}
\end{table}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figs/ablation_planner.png}
    \caption{Ablation of the Planner Size: an 8M BERT planner functions similarly to a 3B BERT. Self-planning performs better in a default temperature of 1. We sweep the temperature from 0.1 to 2.0 and plot the scaling between the resultant sequence entropy and the foldability. For more see Figure~\ref{fig:more_ablation_planner}.}
    \label{fig:ablation_planner}
\end{figure}

\begin{table}[t]
\centering
\caption{Comparison of negative ELBOs for Path Planning Planners and self-planning, averaged on 20 runs. Lower values (\(\downarrow\)) indicate better ELBO. The ELBO is computed at default temperature 1, corresponding to the star-annotation results in Figure~\ref{fig:ablation_planner}.}
\label{tab:comparison-elbo}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{lcc}
\toprule
\textbf{Method} & \textbf{Unmasked pos.-ELBO
(\(\downarrow\))} & \textbf{Masked pos.-ELBO (\(\downarrow\))} \\
\midrule
P2 + Planner ESM2-8M    & 22.5 & 13.4 \\
P2 + Planner ESM2-35M   & 22.0 & 13.4 \\
P2 + Planner ESM2-150M  & 21.8 & 13.4 \\
P2 + Planner ESM2-650M  & 21.7 & 13.4 \\
P2 + Planner ESM2-3B    & 21.6 & 13.4 \\
\midrule
P2 (self-planning)      & 15.7 & 13.4 \\
\bottomrule
\end{tabular}%
}
\end{table}


\begin{table}[t]
\centering
\caption{Ablation study of self-planning. We compare self-planning using denoiser-predicted probabilities with a uniformly sampled probability baseline. 
finetuned MDM refers to MDM fine-tuned from BERT (DPLM-150M~\citep{DPLM}), while tfs-MDM refers to MDM trained from scratch.
}
\label{tab:ablation_self_planning}
\setlength{\tabcolsep}{4pt} % Adjust column spacing for better fit
\resizebox{\columnwidth}{!}{%
\begin{tabular}{lccccccc}
\toprule
\textbf{Configuration} & \textbf{pLDDT (↑)} & \textbf{pTM (↑)} & \textbf{pAE (↓)} & \textbf{Foldability (↑)} & \textbf{Entropy (↑)} & \textbf{Diversity (↑)} \\
\midrule
finetuned MDM  & 82.62 & 0.72 & 9.15  & 63.00 & 3.40 & 93.05 \\
finetuned MDM + Uniform  & 72.61 & 0.66 & 11.82 & 39.00 & 4.01 & 93.62 \\
\midrule
tfs-MDM  & 74.67 & 0.71 & 10.33 & 43.00 & 3.85 & 93.12 \\
tfs-MDM + Uniform  & 59.88 & 0.52 & 15.57 & 20.00 & 4.00 & 93.57 \\
\bottomrule
\end{tabular}%
}
\end{table}


In this section, we utilize the protein sequence generation task as an ablation benchmark to analyze the implications of our Path Planning (P2) design choices. We experiment with the ESM2~\citep{esm2} family of protein language models, including versions with 8M, 35M, 150M, 650M, and 3B parameters, for variants incorporating a BERT planner. For the denoiser, we train a 150M MDM from scratch, using the same architecture as ESM2-150M and DPLM-150M, for 500k steps with approximately 320k tokens per step. Training details are provided in Appendix~\ref{sec:training-detail-MDM-protein}.

\textbf{Results.} 
Table~\ref{tab:ablation_planner} demonstrates that our P2 approach consistently outperforms existing sampling strategies across all folding metrics, while maintaining strong token entropy and sequence diversity. Notably, results are further enhanced when an external BERT planner is utilized. To provide a comparative perspective, we perform an apple-to-orange evaluation against a planner-based sampling algorithm, DDPD, equipped with the same BERT planner. DDPD is prone to generating low-entropy, repetitive sequences with poor foldability, as it relies exclusively on the planner to dictate both unmasking and remasking. In contrast, P2 separates these responsibilities: remasking is delegated to the BERT planner, while unmasking is guided by the denoiser itself. This decomposition mitigates the planner's bias and leverages the denoiser’s planning capabilities effectively.

In Figure~\ref{fig:ablation_planner}, we ablate the size of the planner and evaluate foldability under varying temperatures (entropy). Additional metric results are shown in Figure~\ref{fig:more_ablation_planner}. Our findings reveal that an 8M BERT planner is sufficient to guide a 150M MDM, achieving competitive performance relative to its 3B counterpart across a broad range of entropy values. Furthermore, the BERT planner demonstrates superior scalability compared to the self-planning variant, preserving foldability under extreme high and low temperature conditions.

\textbf{Self-Planning Analysis.}
In our self-planning approach, we leverage the predicted probabilities from unmasked positions to guide unmasking decisions. This raises a key question: Are the predicted probabilities from unmasked tokens meaningful?
We conducted an ablation study where we replaced predicted probabilities for unmasked tokens with uniformly random values and performed the experiments on two MDM variants: one trained from scratch and another fine-tuned from a BERT-based model (DPLM-150M~\citep{DPLM}). The DPLM-150M was fine-tuned from ESM2, which was pretrained to predict both masked and randomly mutated tokens, making it more likely to inherit meaningful logits for unmasked positions.
As shown in Table~\ref{tab:ablation_self_planning}, randomizing unmasked token probabilities leads to a substantial decline in performance across both variants. This finding confirms that unmasked token logits are informative, despite the lack of direct supervision.
It is also evidenced by the ELBO from Proposition~\ref{prop:ELBO} in Table~\ref{tab:comparison-elbo} where self-planning displays an even better ELBO compared with BERT planners, further validating its effectiveness.


\subsection{Sampling Efficiency}
% \begin{figure}
%     \centering
%     \includegraphics[width=1\linewidth]{figs/efficiency_scaling.png}
%     \caption{Performance vs. Sampling Time.}
%     \label{fig:efficiency_scaling}
    
%     \vspace{0.5cm} % Adjust vertical spacing between the two figures if needed

%     \includegraphics[width=1\linewidth]{figs/len_vs_efficiency.png}
%     \caption{Sampling Efficiency vs. Length.}
%     \label{fig:len_vs_efficiency}
% \end{figure}


\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figs/effiency_benchmark.png}
    \vspace{-10pt}
    \caption{Top: Performance vs. Sampling Time (steps). Bottom: Running Time (left) and Speed (right) vs. Sequence Length.}
    \label{fig:efficiency_benchmark}
\end{figure}

Increasing the number of sampling steps generally enhances generative quality, albeit with increased computational time. To evaluate the scaling efficiency, we benchmark three sampling algorithms—ancestral sampling, P2 (self-planning), and P2 augmented with an 8M BERT planner—on the task of protein sequence generation. We measure the foldability across increasing sampling steps in terms of elapsed time (benchmarked on NVIDIA A100 GPUs). In Figure~\ref{fig:efficiency_benchmark} top,  P2 achieves superior foldability compared to ancestral sampling, while the inclusion of the external BERT planner demonstrates exceptional scalability, particularly at higher sampling steps.
In Figure~\ref{fig:efficiency_benchmark} bottom, we further analyze inference efficiency by examining elapsed time and speed (tokens per second) as a function of sequence length. P2 with self-planning maintains the same inference cost as ancestral sampling, as it does not rely on an external model. Conversely, P2 with the BERT planner doubles the number of sampling steps due to one additional BERT evaluation. However, since the planner is a lightweight 8M model compared to the 150M MDM, the overhead is negligible. This is evident in the figure, where the performance gap between P2 (self-planning) and P2 with the 8M BERT planner becomes indistinguishable at higher sampling scales.

\subsection{Language Generation}
\label{sec:Language Understanding}

\begin{table}[t]
\centering
\caption{Language generation benchmarks, including reading comprehension (TriQA), last word completion task (LAMBADA), math reasoning (GSM8K), story infilling (ROCStories), and code generation. 
Baseline results are adopted from~\citep{nie2024scalingmaskeddiffusionmodels,gong2024scalingdiffusionlanguagemodels}. For the infilling task, we use ROUGE-1/2/L
score; for other tasks, we use the accuracy (\%) metric. We employ P2 for the MDMs (1.1B)~\citep{nie2024scalingmaskeddiffusionmodels} and DiffuLLama (7B)~\citep{nie2024scalingmaskeddiffusionmodels} and show consistent improvement.
}
\label{tab:language_benchmark}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{lccccc}
\toprule
\textbf{Model}                      & \textbf{TriQA (↑)} & \textbf{LAMBADA (↑)} & \textbf{GSM8K (↑)} & \textbf{ROCStories (↑)}        & \textbf{Code (↑)} \\ 
\midrule
GPT2-S (127M)                       & 4.0               & 25.9                 & 44.8               & (7.8/0.8/7.4)                  & 1.6               \\
DiffuGPT-S (127M)                   & 2.0               & 45.0                 & 50.2               & 13.7/1.4/12.6                  & 0.3               \\
SEDD-S (170M)                       & 1.5               & 12.4                 & 45.3               & 11.9/0.7/10.9                  & 0.7               \\
GPT2-M (355M)                       & 6.7               & 37.7                 & 50.7               & (8.6/0.9/8.2)                  & 2.6               \\
DiffuGPT-M (355M)                   & 3.8               & 60.5                 & 52.6               & 18.7/2.7/17.0                  & 2.9               \\
SEDD-M (424M)                       & 1.8               & 23.1                 & 53.5               & 13.1/1.4/12.2                  & 0.5               \\
Plaid1B (1.3B)                      & 1.2               & 8.6                  & 32.6               & 12.1/1.1/11.2                  & 0.1               \\
TinyLlama (1.1B)                    & -                 & 43.22               & -                  & -                              & -                 \\
GPT-2 (1.5B)                        & -                 & 44.61               & -                  & -                              & -                 \\
Llama-2 (7B)                        & 45.4              & 68.8                & 58.6               & (11.6/2.1/10.5)                & 1.7               \\
MDM (1.1B)                          & -                 & 52.73               & 58.5               & -                              & -                 \\
\textbf{MDM (1.1B) + P2}            & \textbf{-}        & \textbf{52.88}      & \textbf{60.9}      & \textbf{-}                     & \textbf{-}        \\
DiffuLLama (7B)                     & 18.5              & 53.72               & -                  & 20.31/2.83/18.16               & 13.2             \\
\textbf{DiffuLLama (7B) + P2}       & \textbf{18.8}     & \textbf{54.80}      & \textbf{-}         & \textbf{25.44/7.10/23.41}      & \textbf{17.6}    \\ 
\bottomrule
\end{tabular}%
}
\end{table}


It has been widely pointed out that the existing evaluation such as toy datasets and NLL in text generation can be easily gamed to achieve low perplexity~\citep{Zheng2024MaskedDM}. 
In our evaluation, we follow the language benchmarking from SMDM~\citep{gong2024scalingdiffusionlanguagemodels} and DiffuLLama~\citep{nie2024scalingmaskeddiffusionmodels}, and investigate the capabilities of MDMs in real-world evaluation language generation tasks that have been largely overlooked in prior works~\citep{Austin2021StructuredDD, Lou2023DiscreteDM, mdlm, md4}. We additionally provide the experiments of breaking the reverse curse in the Appendix~\ref{sec:BREAKING THE REVERSE CURSE}.

\textbf{Benchmarks.}
We consider TriviaQA~\citep{joshi-etal-2017-triviaqa} to test the reading comprehension of
models and the last word completion task Lambada~\citep{Paperno2016TheLD} to test how models capture
long-range dependencies in text. These two tasks are measured by exact match accuracy, i.e., given a prompt, we use MDMs to generate responses and calculate matching accuracy against the ground truth. 
Additionally, we employ complex tasks such as GSM8K~\citep{Cobbe2021TrainingVT},  grade school math problem, to assess the \emph{math reasoning} and story-infilling task using ROCStories~\citep{Mostafazadeh2016ACA} and evaluate
using ROUGE score~\citep{lin-2004-rouge}. To test the code infilling, we also adopted Humaneval~\citep{Bavarian2022EfficientTO} single line infilling task, which is evaluated by pass@1 rate. We employ  Language Model Evaluation Harness framework~\citep{Biderman2024LessonsFT} for performance assessment. 

\textbf{Baselines.} We adopt the baselines and their results from previous works \citep{nie2024scalingmaskeddiffusionmodels,gong2024scalingdiffusionlanguagemodels}, including continuous diffusion model Plaid1B (1.3B)~\citep{Gulrajani2023LikelihoodBasedDL}, discrete diffusion  model SEDD-S (170M), SEDD-M (424M) 
 \citep{Lou2023DiscreteDM}, MDM (1B) \citep{gong2024scalingdiffusionlanguagemodels}, DiffuLLama(7B) \citep{nie2024scalingmaskeddiffusionmodels}, DiffuGPT-S (127M), DiffuGPT-M (355M) \citep{nie2024scalingmaskeddiffusionmodels}, and autoregressive models GPT2-S (127M), GPT2-M (355M), GPT-2 (1.5B) \citep{radford2019language},  TinyLlama (1.1B) \citep{Zhang2024TinyLlamaAO} and Llama-2 (7B) \citep{Touvron2023Llama2O}.


\textbf{Setup.}
We equip existing mask diffusion models MDM (1.1B) and DiffuLLama (7B) with our path planning and compare them with the default ancestral sampling results. For P2, we sweep the stochasticity strength from 0 to 2.0 with a step size of 0.2 and report the best results.

\textbf{Results.}
As shown in Table~\ref{tab:language_benchmark}, equipping with P2, we consistently improve the generation performance in the five benchmarks. 
In tasks that require more extensive global bidirectional reasoning, math reasoning GSM8K story infilling ROCStories, and code generation,
P2 consistently exhibits improved performance by a large margin compared to the ancestral sampling. Compared to AR models that rely solely on left-toright modeling capabilities, P2 presents impressive generation accuracy; in code generation, where P2 achieves 17.6\% pass@1 rate (vs. 1.7\% of respective autoregressive model Llama-2 (7B)). In math reasoning, P2 enables a 1.1B-parameter MDM to outperform 7B-parameter Llama2 (60.9\% vs. 58.5\%).   We attribute the success of P2 in complex language generation task to the remasking that corrects potential mistakes made in previous steps and promotes MDMs to generate robust answers.



\subsection{RNA Sequence Generation}

\begin{table}[ht]
\scriptsize
\setlength{\tabcolsep}{4pt} 
\caption{RNA Sequence Generation Benchmark. The "Native" row represents subsampled natural RNA sequences. "MDM" refers to a pretrained 150M Masked Diffusion Model trained on RNACentral \citep{rnacentral2021rnacentral}.}
\centering
\resizebox{\columnwidth}{!}{%
\begin{tabular}{lrrrr}
\toprule
\textbf{Sequence Source} & \textbf{pLDDT (↑)} & \textbf{MFE (kcal/mol) (↓)} & \textbf{Entropy (↑)} & \textbf{GC Content (\%) (↑)} \\
\midrule
Native & 48.26 & -35.83 & \textbf{1.96} & 49.64 \\
RiNALMo-150M & 59.01 & -30.12 & 1.29 & 29.50 \\
RiNALMo-650M & 46.99 & -31.90 & 1.33 & 28.06 \\
MDM + Ancestral & 68.12 & -48.46 & 1.93 & 60.84 \\
MDM + RDM & 67.35 & -47.54 & 1.89 & 59.42 \\
\textbf{MDM + P2 (self-planning)} & \textbf{69.41} & -48.21 & 1.89 & 59.84 \\
\textbf{MDM + P2 + Planner RiNALMo-150M} & \textbf{73.28} & \textbf{-51.91} & 1.86 & \textbf{65.47} \\
\bottomrule
\end{tabular}
} 
\label{tab:performance_metrics}
\end{table}

\textbf{Experimental Setup.}  
We train a 150M Masked Diffusion Model (MDM) trained on 27M RNA sequences from RNACentral \citep{rnacentral2021rnacentral} over 100K steps with a batch size of 320K tokens. 

We adopted the protein sequence evaluation protocols, using an external folding model \citep{shen2024accurate} to estimate structural quality via pLDDT. We additionally calculate the Minimum Free Energy (MFE), GC Content (\%), and sequence entropy.  We generate 100 RNA sequences of 100 base pairs (bp) each. Visualizations are described in Appendix~\ref{sec:rna_vis}.

\textbf{Baselines.}  
Two RNA language models, RiNALMo-150M and RiNALMo-650M \citep{penic2024rinalmo}, served as primary language model baselines. Additionally, a reference set of 100 native 100-bp RNA sequences was included for comparative purposes. 
We apply the existing sampling strategies along with the two P2 variants self-planning and BERT-planning (RiNALMo-150M) to the MDM. We evaluated stochasticity parameters ranging from 0 to 2 in 0.02 increments.

\textbf{Results.}  
As summarized in Table~\ref{tab:performance_metrics}, self-planning outperforms native sequences baseline models (RiNALMo), and existing sampling strategies. Employing the RiNALMo planner further improves the key metrics, including pLDDT, predicted minimum free energy (MFE), and GC content with slight compromises in MFE and GC content. 

% \subsection{Redesigning Green Fluorescent Protein}
% We redesign the residues of an existing Fluorescent Protein (Uniprot ID P42212) by fixing the key residues for the fluorescent and randomly selecting 50\% % of the rest residues to fix the backbone structures. Therefore the input to the model is 
% We apply stochastic remasking to a 650M MDMs trained on around 40M Uniref50 and 200M OMGProt50 sequences. Both of the two datasets use a sequence identity of 50\% to cluster the training sequences for diverse samples. Overall, the model has been trained for around 1 million steps using 600 hours. 






\section{Conclusion}
We demonstrated that unmasking order significantly impacts the generative performance of masked diffusion models (MDMs). By expanding the ELBO formulation, we introduced a \textit{planner} that optimizes token selection during inference. We proposed \textit{Path Planning (P2)}, a sampling framework that generalizes all existing MDM sampling strategies. P2 delivers state-of-the-art improvements across diverse tasks, including language generation and biological sequence design, enabling MDMs to outperform larger autoregressive models. Our findings highlight the importance of inference strategies in discrete diffusion models, paving the way for more efficient and effective sequence generation. 

\section{Acknowledgments}
Fred extends sincere gratitude to Jiaxin Shi, Xinyou Wang, Zaixiang Zheng, Chengtong Wang, and Bowen Jing, Kaiwen Zheng for their invaluable insights on DPLM. Fred devotes his special thank-you to Tian Wang for playing ping-pong with him during the project.  Zack likewise extends his gratitude to Jim Nolen for his invaluable insights.

The authors acknowledge funding from UNIQUE, CIFAR, NSERC, Intel, and Samsung. The research was enabled in part by computational resources provided by the Digital Research Alliance of Canada (\url{https://alliancecan.ca}), Mila (\url{https://mila.quebec}), and NVIDIA.

\section{Author Contributions}

F.Z.P. proposed the initial idea and conducted the experiments on language and protein modeling. Z.B. formulated the mathematical framework. S.P. carried out the experiments on RNA. F.Z.P. and Z.B. jointly wrote the manuscript, with all other authors contributing revisions and refinements. A.T., S.Y., and P.C. supervised the project.


%Our results highlight that self-planning and BERT-planning approaches effectively leverage model confidence to refine the sampling trajectory, leading to superior sequence generation with minimal computational overhead. 
%Notably, P2's ability to correct early-stage errors and introduce controlled remasking improves structural plausibility in biomolecular sequence design and enhances bidirectional reasoning in natural language generation~\citep{gong2024scalingdiffusionlanguagemodels, Lou2023DiscreteDM, Austin2021StructuredDD}. As MDMs continue to gain traction in generative modeling, P2 provides a robust and adaptable strategy to enhance their reliability and applicability across scientific and engineering domains. %Future work may explore extending P2 to multimodal diffusion models and further integrating reinforcement learning to optimize path selection dynamically.



\section{Impact Statement}
We introduce Path Planning (P2), a principled framework for optimizing the sampling order in masked diffusion models (MDMs), improving their generative quality across diverse sequence modeling tasks. By integrating path planning into the diffusion sampling process, P2 corrects early-stage errors, enhances sample fidelity, and generalizes existing sampling strategies. Our results demonstrate that P2 significantly improves state-of-the-art performance in protein and RNA sequence generation, as well as in language modeling applications such as reasoning and code generation. However, like any powerful generative method, P2 carries potential risks, including unintended applications in adversarial sequence design. We encourage responsible use and ethical oversight to ensure that P2 advances beneficial scientific and medical discoveries while mitigating risks of misuse.


\bibliographystyle{style/icml2025}
\bibliography{references}

\newpage
\appendix
\beginsupplement
\onecolumn

\section*{\centering Appendix}


\section{Reproducibility Statement}
We provide the PyTorch implementation in Appendix Section~\ref{sec:pytorch_impl}. For the experiments, we integrate our approach into the SMDM~\citep{gong2024scalingdiffusionlanguagemodels} GitHub codebase\footnote{\url{https://github.com/ML-GSAI/SMDM}} to obtain the results for "MDM (1.1B) + P2" reported in Table~\ref{tab:language_benchmark}. Similarly, the results for "DiffuLLaMA (7B) + P2" in Table~\ref{tab:language_benchmark} are derived using the DiffuLLaMA~\citep{nie2024scalingmaskeddiffusionmodels} GitHub codebase\footnote{\url{https://github.com/HKUNLP/DiffuLLaMA}}. 
For the protein sequence generation experiments, we utilize the DPLM~\citep{DPLM} open-source codebase\footnote{\url{https://github.com/bytedance/dplm}}. The RNA sequence generation results are obtained by adapting the DPLM codebase for MDM training, combined with the RiNALMo~\citep{penic2024rinalmo} language model architecture.


\section{Related Works}

Masked Diffusion Models (MDMs) represent a promising alternative to autoregressive models for discrete data generation, particularly in language modeling. Recent advancements have focused on simplifying and generalizing the MDM framework to improve performance and training efficiency~\citep{md4, mdlm}. These studies introduced a continuous-time variational objective for MDMs, expressed as a weighted integral of cross-entropy losses, facilitating the training of models with state-dependent masking schedules. At the GPT-2 scale, these MDMs outperformed prior diffusion-based language models and demonstrated superior capabilities in zero-shot language modeling tasks~\citep{nie2024scalingmaskeddiffusionmodels,gong2024scalingdiffusionlanguagemodels}. 

MDMs generate sequences starting from a fully masked input and progressively unmasking positions until a clean sequence is reached. Once a token is unmasked, it will stay unchanged. However, there is not guarantee that the state is correct, considering the approximation errors arise from the imperfect fit to real-world data distributions. Additionally, time discretization~\citep{zhao2024informedcorrectorsdiscretediffusion} and numerical errors~\citep{zheng2024maskeddiffusionmodelssecretly} may further the error incurred during sampling processes.

To address these challenges, several solutions have been proposed. These include methods allowing models to revise prior predictions and guiding sampling trajectories using internal or external knowledge. Examples include informed correctors~\citep{zhao2024informedcorrectorsdiscretediffusion}, greedy ancestral methods~\citep{gong2024scalingdiffusionlanguagemodels}, and RDM sampling techniques~\citep{RDM, DPLM}, which leverage model scores to replace random masking with targeted corrections. None of these works, however, allow for the use of an external planner, and ~\citep{RDM, DPLM} are simply using a top-k sampling strategy without any concern for the theoretical underpinnings of the sampling strategies viability.

In terms of theoretically-backed methods for selecting the denoising order during a generative model's sampling process, the current literature is quite sparse. \cite{shih2022traininginferenceanyorderautoregressive,li2021discoveringnonmonotonicautoregressiveorderings} discuss this task from the perspective of Any-Order Autoregressive models,  with \cite{li2021discoveringnonmonotonicautoregressiveorderings} requiring a specially-trained external planner model using a specially designed architecture and \citet{shih2022traininginferenceanyorderautoregressive} taking the perspective that a fixed family of possible generation orders should be chosen a priori to eliminate redundancy.

The most closely related work to ours is likely the recent
DDPD~\citep{ddpd} introduced a generative process divided into a planner, which identifies corrupted positions, and a denoiser, which refines these positions. Though they discuss the ability to employ a MDM denoiser within their framework, their analysis and sampling is through the lens of uniform discrete diffusion models. In particular, as with \cite{li2021discoveringnonmonotonicautoregressiveorderings}, the success of their strategy is contingent upon training a large specialized planner model of comparable size to the denoiser itself. Moreover, in their framework, since they are based on uniform diffusion models, the partially de-noised sequence never contains any masked states, and there is no way for the planner to be separated into masked and unmasked components to design a sampling strategy with guaranteed finite-time along the lines of our Algorithm \ref{alg:OURpracticalsampling}. Given the possible perceived similarity of this concurrent work with ours, we provide a thorough comparison of DDPD with P2 in Appendix \ref{alg:DDPDsampling}, highlighting the greater flexibility and difference in role of P2s' planners.
 
%In contrast to these works, our approach extends the theoretical foundation of planner-guided sampling to the sequence level using continuous-time Markov chain (CTMC) theory. Unlike prior studies~\citep{md4, mdlm, DDPD}, which operate at the token level and assume independence among tokens, our mathematical framework enables sequence-level path planning in a principle manner. Moreover, we derive the Evidence Lower Bound (ELBO) for our path-planning framework, which mathematically supports the assessment of pretrained models as effective planners using ELBO. Compared to DDPD, our contributions include: 
%1. Planner Decomposition: We separate the planner into mask and unmask components, effectively mitigating planner biases.
%2. Practical Sampling Frameworks: We instantiate our framework with two practical samplers, self-planning, and BERT planner, neither of which requires a dedicated pretrained planner model. This broadens accessibility for practitioners by providing diverse and efficient solutions.


% \subsection{Modified Sampling Schemes for Masked Diffusion Models}

% \subsection{Planning-based Sampling}

\section{Additional Background}
\subsection{Discrete Diffusion/Flow Models: Problem Setup}
Here we discuss the general formulation of the problem setup and motivation behind discrete diffusion \cite{Austin2021StructuredDD,Lou2023DiscreteDM,Sun2022,campbell2022continuoustimeframeworkdiscrete} and discrete flow models \cite{DFM,gat2024discreteflowmatching}. This helps contextualize this manuscript in the broader landscape of the generative modeling framework, as well as introduce some additional notation that will be useful for the mathematical derivations in Appendix \ref{sec:mathematicaldetails}.

Suppose we have a set if $N$ tokens, $S=\lbrace 1,\ldots,N\rbrace$, and samples of sequences of length $L$ comprised of elements of $S$ from some distribution $p_{data}\in \mc{P}(S^L)$. We seek to generate new samples from $p_{data}$ via learning a ``denoising'' function $D^{\theta}$ which allows one to sample from $p^{\theta}\approx p_{data}$.

To find such a function, we choose a family of probability measures $\lbrace P_t(\cdot;\mu)\rbrace_{t\in [0,1],\mu \in \mc{P}(S^L)}$ such that $P_0(\cdot;\mu)=\mu$ and $P_1=\pi$, where $\pi\in \mc{P}(S^L)$ is some easily-sampled from reference distribution. Then we find $\lbrace \overset{\leftarrow}{X}_t\rbrace_{t\in [0,1]}$ a continuous-time Markov chain with $\Prob(\overset{\leftarrow}{X}_t=x)=\overset{\leftarrow}{P}(x;p_{data})\coloneqq P_{1-t}(x;p_{data})$, and seek to use the ``denoising function'' $D^{\theta}$ to simulate a continuous time Markov chain $\lbrace X^{\theta}_t\rbrace_{t\in [0,1]}$ which is close in distribution to $\overset{\leftarrow}{X}$. In the end, we will have that taking $X^{\theta}_0\sim \pi$ and simulating the chain to time 1, $X^{\theta}_1\overset{d}{\approx}\overset{\leftarrow}{X}_1\sim p_{data}$. To understand what this process $X^{\theta}$ is and why the use of this intermediary Markov chain is useful for finding a choice of $D^{\theta}$, we first briefly review the theory of continuous time Markov chains in Appendix \ref{subsection:CTMC}.  
\subsection{Time-Inhomogeneous Continuous Time Markov Chains (CTMC)}\label{subsection:CTMC}
A (time-inhomogenous) continuous-time Markov chain $\lbrace X_t\rbrace_{t\geq 0}$ on a finite set $\mc{X}$ is a stochastic process satisfying the Markov property, which can be formally summarized $\Prob(X_t=y|X_{s_1}=x_1,\ldots,X_{s_k}=x_k,X_s=x)=\Prob(X_t=y|X_s=x),\forall y,x_1,\ldots,x_k,x\in \mc{X},0\leq s_1<s_2<\ldots<s_k<s<t\leq 1$. One can construct such a process by specifying a "rate matrix" $Q_{t}\in \R^{|\mc{X}|\times|\mc{X}|}$ with $Q_t(y,x)>0$ and $Q_t(x,x)=-\sum_{y\neq x}Q(y,x)$ for all $x\neq y\in \mc{X}$ and $t\geq 0$. Along with an initial distribution $\mu\in\mc{P}(\mc{X})$, $Q$ determines the 1-dimensional time marginals $\Prob(X_t=\cdot)\in\mc{P}(\mc{X})$ via the Kolmogorov equation:
\begin{align}\label{eq:kolmogoroveq}
\frac{d}{dt}\Prob(X_t=\cdot)&=Q_t\Prob(X_t=\cdot),\qquad t\geq 0\\ 
\Prob(X_0=x)&=\mu(x),\qquad x\in \mc{X}.\nonumber
\end{align}
When the above holds, we will say $Q$ ``generates'' $X$. Note that one can see necessarily that if $Q$ generates $X$, $Q_t(y,x)\coloneqq \lim_{s\downarrow t}\frac{d}{ds}\Prob(X_s=y|X_t=x),x\neq y\in \mc{X}$. Knowing the entries of $Q$ also provides a means of generating samples from $X_t$ at any given time, since paths of $\lbrace X_t\rbrace_{t\geq 0}$ can be realized via a sequence of jump times $\lbrace \tau_{n}\rbrace_{n\in\bb{N}}$, with $\tau_i=\inf\lbrace t>\tau_{i-1}:X_t\neq X_{\tau_{i-1}}\rbrace$ and the effective discrete-time jump process $\lbrace X_{\tau_i}\rbrace_{i\in\bb{N}}$. Then 
\begin{align}\label{eq:transitionprobabilities}
\Prob(X_{\tau_{i+1}}=y|X_{\tau_{i}}=x,\tau_i=t)=-\frac{Q_t(y,x)}{Q_t(x,x)},
\end{align}
and 
$$\log(\Prob(\tau_{i+1}>t|X_{\tau_{i+1}}=x,\tau_i=s))=\int_s^t Q_{p}(x,x)dp.$$ 
For more background on time-inhomogenous continuous-time Markov chains, see e.g. Chapter 2 of \cite{yin_continuous-time_2013} or the appendix of \cite{ren2024}.

\subsection{The Role of the Denoiser and the Approximate Backwards Process}
In the ``discrete diffusion model'' framework, one in fact starts with specifying a rate matrix $Q_t$ generating some Markov chain $\lbrace X_t\rbrace_{t\in [0,1]}$ with $X_0\sim p_{data}$ and $X_1\sim \pi$ and defines $P_t(x;p_{data})\coloneqq \Prob(X_t=x)$. $\overset{\leftarrow}{X}_t$ is then simply defined as $X_{1-t}$, and a rate matrix $\overset{\leftarrow}{Q}_t$ which generates $\overset{\leftarrow}{X}$ can be found from $Q_t$ via an application of Bayes' rule (see Prop. 3.2 in \cite{Sun2022}). In the ``Discrete Flow Model'' framework, one instead starts with a desired interpolation $P_t(\cdot;p_{data})$ between $p_{data}$ and $\pi$, and constructs a rate matrix $\overset{\leftarrow}{Q}_t$ generating a $\overset{\leftarrow}{X}_t$ with one-dimensional time marginals $\overset{\leftarrow}{P}_t(\cdot;p_{data})$ a posteriori. 

As explained above, in order to generate samples of $\overset{\leftarrow}{X}_t$ at a given time (and in particular of $\overset{\leftarrow}{X}_1\sim p_{data}$), it is sufficient to have access to the entries of $\overset{\leftarrow}{Q}_t$. In both settings, however, the entries of $\overset{\leftarrow}{Q}_t$ will naturally depend on the unknown distribution $p_{data}$, and hence, using the form of this dependence, a the denoiser function $D^{\theta}$ is constructed in an attempt to approximate these unknown quantities. This results in a rate matrix $Q^{\theta}_t\approx \overset{\leftarrow}{Q}_t$, which generates the approximate backwards Markov chain $\lbrace X^{\theta}_t\rbrace_{t\in [0,1]}$. The distribution of the output of the resulting sampling scheme is then 

$$p^\theta = P^{\theta}_1=\Prob(X^{\theta}_1=\cdot).$$

The form of the denoiser, as well as the choice of $P_t,\overset{\leftarrow}{Q},$ and $Q^\theta$ in our particular setup are introduced in Sections \ref{subsection:maskeddiffusionmodels} and \ref{subsection:maskeddiffusionmodels}.

\subsection{The Conditional Backwards Process}
A pervasive assumption made in the literature is that for any fixed $x^0\in S^L$, 
\begin{align}\label{eq:factorizationassumption}
P_t(y;\delta_{x^0})=\prod_{i=1}^L p_t(y_i|x^0_i)
\end{align}
for a family of probability measures $\lbrace p_t(\cdot|x^0_i)\rbrace_{t\in [0,1]}\subset \mc{P}(S)$. We denote by $\overset{\leftarrow}{X}^{x^0}$ the ``conditional backwards process,'' on the point $x^0$, defined as the Markov chain with distribution $\Prob(\overset{\leftarrow}{X}^{x_0}_t=y)=\overset{\leftarrow}{P}(y;\delta_{x^0})$, and by $\overset{\leftarrow}{Q}^{x^0}$ its rate matrix. The coordinates $(\overset{\leftarrow}{X}^{x_0}_1,\ldots,\overset{\leftarrow}{X}^{x_0}_L)$ of $\overset{\leftarrow}{X}^{x_0}$ are thus assumed independent, and each described by a continuous-time Markov chain $\lbrace\overset{\leftarrow}{x}^i_t\rbrace_{t\in [0,1]}$ with rate matrix $\hat{\overset{\leftarrow}{Q}}^{x^0_i}_t\in \R^{N\times N}$ for $i=1,\ldots,L$, $t\in [0,1]$ that yields $\Prob(\overset{\leftarrow}{x}^i_t=y_i)=\overset{\leftarrow}{p}_t(y_i|x^0_i)$ for all $t\in [0,1]$ and $y_i\in S$. The hope in making this assumption is that each coordinate of $X^{\theta}_t\approx \overset{\leftarrow}{X}_t$ will be able to be simulated independently in parallel without introducing significant error \cite{Sun2022}.

$P_t(y;\mu)$ is taken to be linear in $\mu$, so we have $P_t(y;p_{data})=\sum_{x\in S^L}P_t(y;\delta_{x})p_{data}(x)$, and hence specifying $p_t(j|i),i,j\in S$ is what ultimately what determines the form of $\overset{\leftarrow}{Q}_t$ and hence the functions needed to be approximated by $D^{\theta}$ in order to construct $Q^{\theta}$. The most common choices explored this far in the literature are the ``uniform diffusion,'' \cite{Lou2023DiscreteDM,schiff2024simpleguidancemechanismsdiscrete} which sets 
\begin{align}\label{eq:uniformcoordinateforwardprocess}
p_t(j|i)=\alpha(t)\delta_i(j) + \frac{1-\alpha(t)}{S}
\end{align}
for $\alpha:[0,1]\rightarrow [0,1]$ with $\alpha(0)=1,\alpha(1)=0$ and the ``masked diffusion,'' which is out subject of focus. 

Note that in the Discrete Diffusion Model framework, $p_t(j|i)$ is not always defined explicitly, and is often implicitly prescribed by asserting the ``forward noising'' process is the independent evolution of a CTMC on $S$ with rate matrix $\hat{Q}_t\in \R^{N\times N}$ on each coordinate (see e.g.\ Equations (15) and (16) in \cite{Lou2023DiscreteDM}). $p_t(j|i)$ is then found by solving \eqref{eq:kolmogoroveq} with $Q=\hat{Q}$ and $\mu=\delta_i$.

In the case of a ``masked diffusion model,'' one extends $S$ to $\bar{S}=S\cup\lbrace M\rbrace$ for $M$ some ``masked state'' outside the dictionary of tokens $S$, and takes: 
\begin{align}\label{eq:maskedcoordinateforwardprocess}
p_t(j|i)=\alpha(t)\delta_i(j) + (1-\alpha(t))\delta_M(j),\quad i,j\in \bar{S}
\end{align}
for a monotone-decreasing, continuously differentiable noise scheduler $\alpha:[0,1]\tto [0,1]$ with $\alpha(0)=1$ and $\alpha(1)=0$. This choice of forward/noising process has been seen to outperform the uniform diffusion process \cite{schiff2024simpleguidancemechanismsdiscrete} as well as other choices of $p_t$ \cite{Austin2021StructuredDD} consistently among applications. If corresponds to the coordinate-wise forward matrix $\hat{Q}_t(j,i)=\sigma(t)\delta_{M}(j)(1-\delta_M(i)),i\neq j\in \bar{S}$ with $\sigma(t)=-\frac{d}{dt}\log(\alpha(t))$, and through \eqref{eq:factorizationassumption} yields \eqref{eq:forwarddynamics}.

In the masked-diffusion setting, both the Discrete Flow Model and Discrete Diffusion Model framework use the rate matrices for the conditional reversed process' coordinates (\cite{DFM} Appendix F.1.):
\begin{align*}
\hat{\overset{\leftarrow}{Q}}^{x^0_i}_t(j,i)& = -\frac{\dot{\alpha}(1-t)}{1-\alpha(1-t)}\delta_{M}(i)\delta_{x^0_i}(j),\quad i,j\in \bar{S}.
\end{align*}


The resulting conditional rate matrix generating $\overset{\leftarrow}{X}^{x^0}_t$ is then, for $x\neq y \in \bar{S}^L$:
\begin{align}\label{eq:conditionalbackwardsmatrix}
\overset{\leftarrow}{Q}_t^{x^0}(y,x) = -\frac{\dot{\alpha}(1-t)}{1-\alpha(1-t)}\sum_{i=1}^L\delta_{M}(x_i)\delta_{x^0_i}(y_i) \delta_{y^{-i}}(x^{-i})
\end{align}
with $\overset{\leftarrow}{Q}_t^{x^0}(x,x)=\frac{\dot{\alpha}(1-t)}{1-\alpha(1-t)}\sum_{i=1}^L \delta_{M}(x_i)$.

\subsection{Role of the ELBO}\label{subsection:ELBOrole}
The training objective in general is obtained via the same methodology in both the Discrete Flow and Discrete Diffusion Model framework -- in fact this methodology can also be used for continuous diffusion models and denoising processes described by more general Markovian dynamics \cite{Benton2024}.

We seek to minimize the KL divergence:
\begin{align*}
D_{KL}(p_{data}||P^{\theta}_1)& = \sum_{x\in S^L}p_{data}(x)\log\left(\frac{p_{data}(x)}{P^{\theta}_1(x)}\right)\\ 
& = \sum_{x\in S^L}p_{data}(x)\log p_{data}(x) - \sum_{x\in S^L}p_{data}(x)\log (P^{\theta}_1(x)).
\end{align*}
The first term, the entropy of $p_{data}$ is constant in $\theta$, and so we turn our attention to finding an ``Evidence Based Lower Bound'' 
$$E(x^0)\leq \log (P^{\theta}_1(x^0))$$ 
for each fixed $x^0\in S^L$.The loss that we seek to minimize will is then defined as:
\begin{align}\label{eq:loss}
\mc{L}_E\coloneqq-\sum_{x\in S^L}p_{data}(x)E(x).
\end{align}

Letting $\mathbb{P}^x\in \mc{P}(D([0,1];S^L))$ denote the Law (on the Skorokhod space of all c\'adl\'ag paths from $[0,1]$ to $S^L$) of $\overset{\leftarrow}{X}^{x^0}$ and $\mathbb{P}^\theta\in \mc{P}(D([0,1];S^L))$ the same but for $X^{\theta}$, we have, by the data-processing inequality (see, e.g. \cite{budhiraja_analysis_2019} Lemma 2.4 (f)):
\begin{align*}
\log (P^{\theta}_1(x^0))=-D_{KL}(\delta_{x^0}||P^{\theta}_1)\geq -D_{KL}(\mathbb{P}^{x^0}||\mathbb{P}^\theta)\coloneqq E(x^0),
\end{align*}

That is, in order to make sure the approximate reverse process has the desired terminal distribution, by minimizing $\mc{L}_E$ we attempt to make it so that the entire path of the approximate reverse process matches that of the exact one.

$E(x^0)$ can be found via an application of Girsanov's Theorem for Markov Jump processes (see e.g. Theorem III.5.34 in \cite{jacod2013} for a general result or \cite{ren2024} Theorem 3.3  for the specific Markov Chain setting), and is expressed solely in terms of $\overset{\leftarrow}{Q}^{x^0}$, $D^{\theta}$, and $P_t(\cdot;\delta_x)$. In the masked diffusion setting, where $Q^\theta$ is given by $Q^{\theta,\text{mask}}$ from \eqref{eq:maskedbackwardsmatrix} and $\overset{\leftarrow}{Q}^{x^0}$ is given by \eqref{eq:conditionalbackwardsmatrix}, this expression is (\cite{mdlm} Equation (10)):
\begin{align}\label{eq:MDMELBO}
E_{mask}(x^0)&=-\int_0^1\frac{\dot{\alpha}(t)}{1-\alpha(t)} \bb{E}_{X_t\sim P_t(\cdot;\delta_{x^0})}\left[\sum_{i:[X_t]_i = M} \log(D_{i,x^0_i}^\theta(X_t))\right]dt,
\end{align}
with $P_t$ as in \eqref{eq:forwarddynamics}. This is exactly $E_D$ from Proposition \ref{prop:ELBO}.

\section{Mathematical Details}\label{sec:mathematicaldetails}
\subsection{Equivalence of MDMs with AOARMs}\label{subsection:AOARMs}
Here, for completeness, we recall the connection between Masked Diffusion Models and Any-Order Autoregressive Models \cite{UriaML14,HoogeboomARDM22} as described in \cite{zheng2024maskeddiffusionmodelssecretly,ou2024}. We start by providing a simplified derivation of the equivalence of the two types of models' sampling schemes.

We begin by obtaining the diagonals for the matrix \eqref{eq:maskedbackwardsmatrix}. Recalling $D^{\theta}_{i,y_i}(x)=\delta_{x_i}(y_i)$ if $x_i\neq M$, and $\sum_{y_i=1}^N D^{\theta}_{i,y_i}(x)=1$ if $x_i=M$: 
\begin{align*}
-\sum_{y\neq x}Q^{\theta}_t(y,x)&=\frac{\dot{\alpha}(1-t)}{1-\alpha(1-t)}\sum_{i=1}^L \delta_{M}(x_i)\sum_{y_i\neq x_i}D_{i,y_i}^{\theta}(x)\\ 
&=\frac{\dot{\alpha}(1-t)}{1-\alpha(1-t)}\sum_{i=1}^L \delta_{M}(x_i)\sum_{y_i=1}^N D_{i,y_i}^{\theta}(x)\\ 
&=\frac{\dot{\alpha}(1-t)}{1-\alpha(1-t)}\sum_{i=1}^L \delta_{M}(x_i).
\end{align*}
Then, if one considers the effective jump chain's transition probabilities as described in \eqref{eq:transitionprobabilities}, we have, for $x\neq y$:
\begin{align*}
\Prob(X^{\theta,\text{mask}}_{\tau_{k+1}}=y|X^{\theta,\text{mask}}_{\tau_{k}}=x,\tau_k=t)=\Prob(X^{\theta,\text{mask}}_{\tau_{k+1}}=y|X^{\theta,\text{mask}}_{\tau_{y}}=x)= \frac{\sum_{i=1}^L\delta_{M}(x_i)D_{i,y_i}^{\theta}(x)\delta_{y^{-i}}(x^{-i})}{\sum_{i=1}^L \delta_{M}(x_i)}.
\end{align*}
We note that this is zero when the Hamming distance $d_{HAM}(x,y)\neq 1$.

Then, for any $j\in\lbrace 1,\ldots,N\rbrace$:
\begin{align*}
\Prob([X^{\theta,\text{mask}}_{\tau_{k+1}}]_{j}\neq [X^{\theta,\text{mask}}_{\tau_{k+1}}]_{j}|X^{\theta,\text{mask}}_{\tau_{k}}=x,\tau_k=t)& = \sum_{y\in \bar{S}^L : y_j\neq x_j}\Prob([X^{\theta,\text{mask}}_{\tau_{k+1}}]_{j}=y|X^{\theta,\text{mask}}_{\tau_{k}}=x)\\ 
& = \sum_{y\in \bar{S}^L : y_j\neq x_j} \frac{\sum_{i=1}^L\delta_{M}(x_i)D_{i,y_i}^{\theta}(x)\delta_{y^{-i}}(x^{-i})}{\sum_{i=1}^L \delta_{M}(x_i)}\\ 
& = \sum_{y_j\neq x_j} \frac{\delta_{M}(x_j)D_{j,y_j}^{\theta}(x)}{\sum_{i=1}^L \delta_{M}(x_i)}\\ 
& =  \frac{\delta_{M}(x_j)\sum_{y_j=1}^ND_{j,y_j}^{\theta}(x)}{\sum_{i=1}^L \delta_{M}(x_i)}\\ 
& = \frac{\delta_{M}(x_j)}{\sum_{i=1}^L \delta_{M}(x_i)}
\end{align*}
and, for $x$ such that $x_j=M$:
\begin{align*}
&\Prob([X^{\theta,\text{mask}}_{\tau_{k+1}}]_{j}=y'_j|X^{\theta,\text{mask}}_{\tau_{k}}=x,\tau_k=t,[X^{\theta,\text{mask}}_{\tau_{k+1}}]_{j}\neq [X^{\theta,\text{mask}}_{\tau_{k+1}}]_{j})\\ 
& = \frac{\Prob([X^{\theta,\text{mask}}_{\tau_{k+1}}]_{j}=y'_j,[X^{\theta,\text{mask}}_{\tau_{k+1}}]_{j}\neq [X^{\theta,\text{mask}}_{\tau_{k+1}}]_{j})|X^{\theta,\text{mask}}_{\tau_{k}}=x,\tau_k=t)}{\Prob([X^{\theta,\text{mask}}_{\tau_{k+1}}]_{j}\neq [X^{\theta,\text{mask}}_{\tau_{k+1}}]_{j})|X^{\theta,\text{mask}}_{\tau_{k}}=x,\tau_k=t)}\\
& = \frac{\sum_{i=1}^L \delta_{M}(x_i)}{\delta_{M}(x_j)}\sum_{y\in \bar{S}^L : y_j=y'_j\neq x_j}\Prob([X^{\theta,\text{mask}}_{\tau_{k+1}}]_{j}=y|X^{\theta,\text{mask}}_{\tau_{k}}=x)\\ 
& = \sum_{y\in \bar{S}^L : y_j=y'_j\neq x_j}\sum_{i=1}^L\delta_{M}(x_i)D_{i,y_i}^{\theta}(x)\delta_{y^{-i}}(x^{-i})\\ 
& = \delta_M(x_j)D_{j,y'_j}^{\theta}(x)\\ 
& = D_{j,y'_j}^{\theta}(x).
\end{align*}

Defining for $x\in\bar{S}^L$, $M(x)\coloneqq\lbrace j\in \lbrace 1,\dots,L\rbrace:x_j=M\rbrace$, the corresponding Gillespie sampling scheme \cite{gillespie_exact_1977,GILLESPIE1976403} for a standard masked diffusion model is thus as follows:
\begin{algorithm}[h]
\small
\caption{Gillespie Sampler for Masked Diffusion Models}
\label{alg:MDMsampling}
\begin{algorithmic}[1]
\State \textbf{Initialize:} $x_0 \gets (M, M, \dots, M)$, denoiser $D^\theta$
\For{$t = 1 : L$}
    \State {\colorbox{gray!20}{\textbf{Choose Random Coordinate for Unmasking:}}} 
    \State Sample dimension $d' \sim \operatorname{Unif}\big(M(x_t)\big)$
    \State {\colorbox{gray!20}{\textbf{Denoise:}}}
    \State Sample $y_{d'} \sim D^\theta_{d', \cdot}(x_t)$
    \State $[x_{t+1}]_{d'} \gets y_{d'}$
\EndFor
\State \textbf{return} $x_L$
\end{algorithmic}
\end{algorithm}

Letting $\mathbb{S}_L$ be the set of all permutations of $\lbrace 1,\ldots,L\rbrace$, we then have:
\begin{align*}
\Prob(X^{\theta,\text{mask}}_1=x)&=\frac{1}{L!}\sum_{\sigma \in \mathbb{S}_L} \prod_{i=1}^L D^{\theta}_{\sigma(i),x_{\sigma(i)}}(x^{-\sigma(\geq i),M})\\ 
&=\bb{E}_{\sigma\sim \text{Unif}(\mathbb{S}_L)}\left[\Prob(X^{\theta,\text{mask}}_1=x|\sigma)\right]
\end{align*}
where $x^{\sigma(<i),M}\in \bar{S}^L$ is $x$ but with $x_{\sigma(j)}=M,\forall j\geq i$. Here $\sigma(i)$ represents the coordinate which is unmasked at time $\tau_{i}$. From this it is clear that with each unmasking, $D^{\theta}$ is gaining additional conditional information about the sequence it is denoising, and could potentially benefit from backtracking and remasking previously unmasked tokens.

Moreover, in \cite{ou2024}, it is proved that the loss that $D^{\theta}$ is trained on (see \eqref{eq:loss}  and \eqref{eq:MDMELBO}) is equivalent to:
\begin{align*}
\mc{L}_{\text{mask}}(\theta)&=-\bb{E}_{x\sim p_{data}}\left[\bb{E}_{\sigma\sim \text{Unif}(\mathbb{S}_L)}\left[\log\left(\Prob(X^{\theta,\text{mask}}_1=x|\sigma)\right)\right]\right]\\ 
&= \bb{E}_{\sigma\sim \text{Unif}(\mathbb{S}_L)}\left[D_{KL}(p_{data}||\Prob(X^{\theta,\text{mask}}_1=\cdot|\sigma))\right]+H(p_{data}),
\end{align*}
where $H$ is the Shannon Entropy of $p_{data}$. This is minimized with value $H(p_{data})$ if and only if $\Prob(X^{\theta,\text{mask}}_1=\cdot|\sigma)=p_{data},\forall \sigma \in \mathbb{S}_L$;
that is, if every choice of unmasking order exactly recovers the data distribution. 

It becomes clear that if the training objective used for a Masked Diffusion Model was made uniformly $0$, every choice of unmasking order would exactly recover the data distribution (the KL divergence is $0$ if and only if the distributions are equal - see e.g. \cite{budhiraja_analysis_2019} Lemma 2.1). In practice, however, $D^{\theta}$ is far from perfect (and even if it were, it is trained using samples form $p_{data}$, so would just recover those samples). As such, not all such orders will be created equal - that is there will be denoising orders $\sigma,\hat{\sigma}\in \bb{S}_L$ such that $$D_{KL}(p_{data}||\Prob(X^{\theta,\text{mask}}_1=\cdot|\sigma))>>D_{KL}(p_{data}||\Prob(X^{\theta,\text{mask}}_1=\cdot|\hat{\sigma})).$$ This was observed empirically in \cite{ou2024} Appendix G.4, \cite{shih2022traininginferenceanyorderautoregressive}, and \cite{li2021discoveringnonmonotonicautoregressiveorderings} Section 6.

\subsection{Comparison with DDPD}\label{subsec:DDPDcomparsion}

As it the most similar work to ours in the existing literature, here we provide a thorough comparison with DDPD \cite{ddpd}. Given that our objective is to plan a denoising order assuming access to a Masked Diffusion Model for our denoiser (as with DDPD-MaskD) and not to train a uniform diffusion-based denoiser from scratch (as with DDPD-DFM-Uni), we focus on their framework in the former setting. 

Even with DDPD-MaskD, the framework uses a ``uniform discrete diffusion'' \eqref{eq:uniformcoordinateforwardprocess} as the starting-point for their token-wise forward noising process, as opposed to the ``masked diffusion'' forward noising process \eqref{eq:maskedcoordinateforwardprocess} used in our work. They modify the state space $S^L$ to $\tilde{S}^L$, where $\tilde{S}= S \times \lbrace N,D\rbrace$. Dor $(y,z)\in \tilde{S}^L$, $(y_i,z_i)$ denotes the pair describing the state $y_i\in S$ in of $i$'th token and $z_i\in \lbrace N,D\rbrace$ denotes whether that token is noise $(N)$ or data $(D)$. They then modify the forward noising process to:
\begin{align*}
p_t((j,\zeta)|i)=\alpha(t)\delta_{(i,D)}(j,\zeta)+\frac{1-\alpha(t)}{S} \delta_{N}(\zeta),\quad i,j\in S,\quad \zeta\in\lbrace N,D\rbrace,
\end{align*}
see Equation (17) therein.

Thus, their reference distribution $\pi \in \mc{P}(\tilde{S}^L)$ is given by $\pi= \text{Unif}(S^L) \otimes \delta_{N^L}$, where $N^L\in \lbrace N,D\rbrace^L$ consists of all $N$'s, and the corresponding backwards processes' $S^L$ marginal is initialized at the $\text{Unif}(S^L)$ as opposed to $\delta_{M^L}$ as in our setting. 

They approximate a resulting true backward process on $S^L$'s rate matrix $\overset{\leftarrow}{Q}_t$ (given by Proposition 3.1 therein) with $Q^{\theta,\text{DDPD}}_t$ given by:
\begin{align*}
Q^{\theta,\text{DDPD}}_t(y,x)& = -\frac{\dot{\alpha}(1-t)}{1-\alpha(1-t)} \sum_{i=1}^L G^{\theta,\text{DDPD}}_{i,N}(x)\bb{E}_{Z\sim G^{\theta}(x)}[D^{\theta}_{i,y_i}(x^{Z,-i,M})]\delta_{y^{-i}}(x^{-i})
\end{align*}
where $D^{\theta}:\bar{S}^L\tto \mc{P}(S)^L$ is a denosier for a masked diffusion model trained via the ELBO \eqref{eq:MDMELBO} as in \eqref{eq:Dtheta}. Here for $x\in S^L$, $z\in \lbrace N,D\rbrace^L$, $x^{z,-i,M}\in \bar{S}^L$ is obtained from $x$ via:
\begin{align*}
x^{z}_j=\begin{cases}
M,&\quad z_j=N\\
x_j,&\quad z_j=D,j\neq i\\ 
M,&\quad j=i
\end{cases}.
\end{align*} 

$G^{\theta,\text{DDPD}}:S^L\tto \mc{P}(\lbrace N,D\rbrace)^L$ is another neural network with $G^{\theta,\text{DDPD}}_{i,N}(x)$ approximating the probability that the $i$'th coordinate of $x\in \mc{S}^L$ is noise, and is trained via \eqref{eq:loss} with $E(x^0)=E^{\text{DDPD}}(x^0)$ given by:
\begin{align*}
E^{\text{DDPD}}(x^0)&= E^{\text{DDPD}}_P(x^0)+E^{\text{DDPD}}_D(x^0) \\ 
E^{\text{DDPD}}_P(x^0)&=-\int_0^1 \frac{\dot{\alpha}(t)}{1-\alpha(t)}\bb{E}_{(\tilde{X}_t,Z_t)\sim P^{\text{DDPD}}_t(\cdot|\delta_{(x^0,D^L)})}\biggl[\sum_{i=1}^L \log \left(G^{\theta,\text{DDPD}}_{i,[Z_t]_i}(\tilde{X}_t)\right)\biggr]dt\\ 
E^{\text{DDPD}}_D(x^0)&=-\int_0^1 \frac{\dot{\alpha}(t)}{1-\alpha(t)}\bb{E}_{(\tilde{X}_t,Z_t)\sim P^{\text{DDPD}}_t(\cdot|\delta_{(x^0,D^L)})}\biggl[\sum_{i=1}^L\delta_{[Z_t]_i=N}\bb{E}_{\hat{Z}\sim G^{\theta,\text{DDPD}}(\tilde{X}_t)}\left[\log \left(D^{\theta}_{i,x^0_i}(\tilde{X}_t^{\hat{Z},-i,M})\right)\right]\biggr]dt\\
P_t((y,z)|\delta_{(x^0,D^L)})&\coloneqq\alpha(t)\delta_{(x^0,D^L)}(y,z)+\frac{(1-\alpha(t))}{S^L} \delta_{N^L}(z),\qquad y\in S^L,z\in\lbrace N,D\rbrace^L
\end{align*}


Note that in the above ELBO, $E^{\text{DDPD}}_D$ is slightly modified from what which they present in Theorem 4.1. As written, they would take the expected value with respect to $G^{\theta,\text{DDPD}}$ inside the second $\log$, which requires $2^{L-1}$ function evaluations of $D^{\theta}$. When the denoiser $D^{\theta}$ is given by that of a masked diffusion, one should instead use the above, which can be readily arrived at the same proof with an extra application of Jensen's inequality.

Comparing this with our Proposition \eqref{prop:ELBO}, the comparison between DDPD and P2 becomes evident: $E^{\text{DDPD}}_P(x^0)$ is playing the role of $E_{UP}(x^0)+E_{MP}(x^0)$ (that is, it yields the training objective for the Planner) and $E^{\text{DDPD}}_D(x^0)$ is playing the role of $E_D(x^0)$ (that is, it yields the training objective for the denoiser). However, we note the following key distinguishing factors:
\begin{enumerate}
\item In P2, $E_D$ is the same as the ELBO originally used to train the denoiser $D^\theta$: that is, $D^\theta$ has already be trained to maximize $\bb{E}_{x_0\sim p_{\text{data}}}[E_D(x^0)]$. Meanwhile, $E^{\text{DDPD}}_D$ depends on the output of $G^{\theta,\text{DDPD}}$, increasing the importance of the role of planner in the quality of the generations output. For this reason, DDPD must train an external Planner whose model size is comparable to that of the denoiser - they are essentially asking the planner to play a role akin to the denoiser in a uniform diffusion model. Meanwhile, due to the ``flipped'' importance of the roles of the planner and denoiser in P2, we show that we can use lightweight BERT models or even the denoiser itself as an effective Planner. See Table \ref{tab:ablation_planner}, where we confirm DDPD's inability to make use of such lightweight models. 
\item In P2, we separate the Planner's training objective into two components. This is natural because our planner may use information both from the partially masked data $X_t$ and the output of the denoiser $Y$. Meanwhile, in DDPD, the Planner only has access to $\tilde{X}_t$-unmasked data perturbed by random flips of its tokens. Because DDPD's generation process is grounded in a uniform diffusion process, there is no ability to separate the Planner into unmasked and masked components as we do in Section \eqref{subsec:samplingstrat}. In particular, their framework does not allow for a general enough planner to introduce our stochasticity strength parameter $\eta$ and design an algorithm analogous to the P2 Sampler \ref{alg:OURpracticalsampling}.
\end{enumerate}

The practical differences between DDPD and P2 are further elucidated by comparing their Gillespie sampling strategy (Algorithm 1 therein) with ours (see Alg. \ref{alg:ourgillespiesampler}). For convenience, we reproduce it here.

Letting $\hat{G}^{\theta,\text{DDPD}}: S^L\tto  \mc{P}(\lbrace 1,\ldots,L\rbrace)$ be given by $\hat{G}^{\theta,\text{DDPD}}_j(x)=\frac{G^{\theta,\text{DDPD}}_{j,N}(x)}{\sum_{j=1}^L G^{\theta,\text{DDPD}}_{j,N}(x)}$, DDPD's Gillespie sampling algorithm is given by Alg. \ref{alg:DDPDsampling}.

\begin{algorithm}[!h]
\caption{DDPD Sampler}
\label{alg:DDPDsampling}
\begin{algorithmic}[1]
\State \textbf{init} $i \gets 0, x_0 \sim \operatorname{Unif}(S^L)$, planner $G^{\theta,\text{DDPD}}$, denoiser $D^{\theta}$, maximum steps $T$
\For{$t=1:T$}
\State {\colorbox{gray!20}{\textbf{Plan}}} Sample dimension $d' \sim \hat{G}_{\cdot}^{\theta,\text{DDPD}}(x_t)$
\State {\colorbox{gray!20}{\textbf{Denoise}}} Sample  
$z \sim G^{\theta,\text{DDPD}}$
\State Sample $y_{d'} \sim D^{\theta}_{d',\cdot}(x_t^{z,-i,M})$
\State Update: $[x_{t+1}]_{d'} \gets y_{d'}$
\EndFor
\State \textbf{return} $x_T$
\end{algorithmic}
\end{algorithm}

As is clear from Alg. \ref{alg:DDPDsampling}, in DDPD, the input to the Planner only depends on some unmasked, randomly flipped sequence of tokens, and does not depend on the output of the denoiser, and the input to the denoiser is entirely dependent on the output of the planner. Meanwhile, in P2, the Planner may use the both the information about the partially unmasked sequence (whose unmasked tokens all result from samples from the denoiser) and the output of the denoiser, and the input to the denoiser only depends on the output of the planner insofar as it may choose to remask a single token.

\hspace{.1cm}\\

\subsection{Deriving the P2 Gillespie Scheme Alg. \ref{alg:ourgillespiesampler}}\label{subsection:Gillespie}
Let $\lbrace \tau_k\rbrace_{k\in\bb{N}}$ be the jump times for the CTMC $X^\theta$ with rate matrix $Q^{\theta}$ as described in Equation \eqref{eq:P2ratematrix} (see Appendix \ref{subsection:CTMC}). To derive a Gillespie sampling scheme, we need to find the transition probabilities for the effective jump chain as described in \eqref{eq:transitionprobabilities}. We first need to obtain the diagonal entries for the jump matrix $Q^{\theta}$. We have for $x\in\bar{S}^L$:
\begin{align*}
-\sum_{y\neq x}Q^{\theta}_t(y,x)&=\frac{\dot{\alpha}(1-t)}{1-\alpha(1-t)}\sum_{i=1}^L\sum_{y_i=1,y_i\neq x_i}^N F^{\theta}_{i,N}(y,x)\hat{D}^{\theta}_{i,y_i}(x)\\ 
&= \frac{\dot{\alpha}(1-t)}{1-\alpha(1-t)}\sum_{i=1}^L  \biggl[\delta_{M}(x_i)\sum_{y_i=1,y_i\neq x_i}^N \bb{E}_{Z\sim D^{\theta}(x)}[G^{\theta}_i(Z^{-i,y_i})]D^{\theta}_{i,y_i}(x)\\ 
&+\frac{(1-\delta_{M}(x_i))}{1-D^{\theta}_{i,x_i}(x^{-i,M})}\sum_{y_i=1,y_i\neq x_i}^N \bb{E}_{Z\sim D^{\theta}(x)}[G^{\theta}_i(Z^{-i,x_i},x)]D^{\theta}_{i,y_i}(x^{-i,M})\biggr]\\ 
& =  \frac{\dot{\alpha}(1-t)}{1-\alpha(1-t)}\sum_{i=1}^L \delta_{M}(x_i) \bb{E}_{Z\sim D^{\theta}(x)}[G^{\theta}_i(Z,x)]+(1-\delta_{M}(x_i))\bb{E}_{Z\sim D^{\theta}(x)}[G^{\theta}_i(Z^{-i,x},x)]\\ 
& = Q^{\theta}_t(x,x).
\end{align*}
Then for $x\neq y\in \bar{S}^L$, $k\in\bb{N}$, and $t\in [0,1]$:
\begin{align*}
\Prob(X^{\theta}_{\tau_{k+1}}=y|X^{\theta}_{\tau_{k}}=x,\tau_k=t) = \frac{\sum_{i=1}^LF^{\theta}_{i}(y,x)\hat{D}^{\theta}_{i,y_i}(x) \delta_{y^{-i}}(x^{-i})}{\sum_{i=1}^L \delta_{M}(x_i) \bb{E}_{Z\sim D^{\theta}(x)}[G^{\theta}_i(Z,x_i)]+(1-\delta_{M}(x_i))\bb{E}_{Z\sim D^{\theta}(x)}[G^{\theta}_i(Z^{-i,x},x)]}.
\end{align*}
We note that this is zero when the Hamming distance $d_{HAM}(x,y)\neq 1$ and independent of $t$ and $k$. 

Then, for $j\in\lbrace 1,\ldots,N\rbrace$ and $x,y,k,t$ as before:
\begin{align*}
&\Prob([X^{\theta}_{\tau_{k+1}}]_{j}\neq [X^{\theta}_{\tau_{k+1}}]_{j}|X^{\theta}_{\tau_{k}}=x,\tau_k=t)\\ 
& = \sum_{y\in \bar{S}^L : y_j\neq x_j}\Prob([X^{\theta,\text{mask}}_{\tau_{k+1}}]_{j}=y|X^{\theta,\text{mask}}_{\tau_{k}}=x)\\ 
& = \sum_{y\in \bar{S}^L : y_j\neq x_j} \frac{\sum_{i=1}^LF^{\theta}_{i}(y,x)\hat{D}^{\theta}_{i,y_i}(x) \delta_{y^{-i}}(x^{-i})}{\sum_{i=1}^L \delta_{M}(x_i) \bb{E}_{Z\sim D^{\theta}(x)}[G^{\theta}_i(Z,x)]+(1-\delta_{M}(x_i))\bb{E}_{Z\sim D^{\theta}(x)}[G^{\theta}_i(Z^{-i,x},x)]}\\ 
& = \sum_{y_j=1,y_j\neq x_j}^N\frac{F^{\theta}_{j}(y,x)\hat{D}^{\theta}_{j,y_j}(x)}{\sum_{i=1}^L \delta_{M}(x_i) \bb{E}_{Z\sim D^{\theta}(x)}[G^{\theta}_i(Z,x)]+(1-\delta_{M}(x_i))\bb{E}_{Z\sim D^{\theta}(x)}[G^{\theta}_i(Z^{-i,x},x)]}\\ 
&=\frac{\delta_{M}(x_j) \bb{E}_{Z\sim D^{\theta}(x)}[G^{\theta}_j(Z,x)]+(1-\delta_{M}(x_j))\bb{E}_{Z\sim D^{\theta}(x)}[G^{\theta}_j(Z^{-j,x_j},x)]}{\sum_{i=1}^L \delta_{M}(x_i) \bb{E}_{Z\sim D^{\theta}(x)}[G^{\theta}_i(Z,x)]+(1-\delta_{M}(x_i))\bb{E}_{Z\sim D^{\theta}(x)}[G^{\theta}_i(Z^{-i,x_i},x)]}\\ 
&=:P(j,x)
\end{align*}
and for $y'_j\in S$ with $y'_j\neq x_j$:
\begin{align*}
&\Prob([X^{\theta}_{\tau_{k+1}}]_{j}=y'_j|X^{\theta}_{\tau_{k}}=x,\tau_k=t,[X^{\theta}_{\tau_{k+1}}]_{j}\neq [X^{\theta}_{\tau_{k+1}}]_{j})\\ 
& = \frac{\Prob([X^{\theta}_{\tau_{k+1}}]_{j}=y'_j,[X^{\theta}_{\tau_{k+1}}]_{j}\neq [X^{\theta}_{\tau_{k+1}}]_{j}|X^{\theta}_{\tau_{k}}=x,\tau_k=t)}{\Prob([X^{\theta}_{\tau_{k+1}}]_{j}\neq [X^{\theta}_{\tau_{k+1}}]_{j})|X^{\theta}_{\tau_{k}}=x,\tau_k=t)}\\
& = \sum_{y\in \bar{S}^L : y_j=y'_j\neq x_j} \frac{\Prob([X^{\theta}_{\tau_{k+1}}]_{j}=y'_j|X^{\theta}_{\tau_{k}}=x)}{\Prob([X^{\theta}_{\tau_{k+1}}]_{j}\neq [X^{\theta}_{\tau_{k+1}}]_{j})|X^{\theta}_{\tau_{k}}=x,\tau_k=t)}\\ 
& = \frac{F^{\theta}_{j}(x^{-j,y'_j},x)\hat{D}^{\theta}_{j,y'_j}(x)}{\delta_{M}(x_j) \bb{E}_{Z\sim D^{\theta}(x)}[G^{\theta}_j(Z,x)]+(1-\delta_{M}(x_j))\bb{E}_{Z\sim D^{\theta}(x)}[G^{\theta}_j(Z^{-j,x_j},x)]}\\ 
&= \frac{\delta_{M}(x_j)\bb{E}_{Z \sim D^{\theta}(x)}[G^{\theta}_j(Z^{-j,y'_j},x)]D^{\theta}_{j,y'_j}(x)+(1-\delta_{M}(x_j))\bb{E}_{Z\sim D^{\theta}(x)}[G^{\theta}_j(Z^{-j,x_j},x)]\frac{D^{\theta}_{j,y'_j}(x^{-i,M})}{1-D^{\theta}_{i,x_i}(x^{-i,M})}}{\delta_{M}(x_j) \bb{E}_{Z\sim D^{\theta}(x)}[G^{\theta}_j(Z,x)]+(1-\delta_{M}(x_j))\bb{E}_{Z\sim D^{\theta}(x)}[G^{\theta}_j(Z^{-j,x_j},x)]}\\ 
& = \delta_{M}(x_j)\frac{\bb{E}_{Z \sim D^{\theta}(x)}[G^{\theta}_j(Z^{-j,y'_j},x)]}{\bb{E}_{Z\sim D^{\theta}(x)}[G^{\theta}_j(Z,x)]}D^{\theta}_{j,y'_j}(x) + (1-\delta_{M}(x_j))\frac{D^{\theta}_{j,y'_j}(x^{-i,M})}{1-D^{\theta}_{i,x_i}(x^{-i,M})}\\ 
&=:\tilde{P}(j,x,y'_j).
\end{align*}
Thus, an exact Gillespie sampling scheme would be given by \cite{gillespie_exact_1977,GILLESPIE1976403}: 

When the chain is in state $x\in \bar{S}^L$, sample a dimension $d'\sim \hat{P}(\cdot,x)$ to change, then sample a value $y\sim \tilde{P}(d',x,\cdot)$ to change it to.

In practice it is impractical to approximate these expected values with respect to $Z\sim D^{\theta}(x)$, as this would require many function evaluations of the denoiser. However, assuming that the token space is large, conditioning on the value of one coordinate should have little impact on the expected output of the Planner over the entire sequence (see e.g. the discussion under Proposition 3.5. and Appendix E.4 in \cite{ddpd}). Given that Alg. \ref{alg:ourgillespiesampler} is provided for the purpose of exposition and in practice we make use of Alg. \ref{alg:OURpracticalsampling} in sampling, we use this intuition to formally approximate:
\begin{align*}
\tilde{P}(j,x,y'_j)\approx \delta_{M}(x_j)D^{\theta}_{j,y'_j}(x) + (1-\delta_{M}(x_j))\frac{D^{\theta}_{j,y'_j}(x^{-i,M})}{1-D^{\theta}_{i,x_i}(x^{-i,M})}
\end{align*}
and 
\begin{align*}
P(j,x)\approx \frac{\bb{E}_{Z\sim D^{\theta}(x)}[G^{\theta}_j(Z,x)]}{\sum_{i=1}^L \bb{E}_{Z\sim D^{\theta}(x)}[G^{\theta}_i(Z,x)]}\approx \bb{E}_{Z\sim D^{\theta}(x)}[\hat{G}_j(Z,x)],
\end{align*}
where : $\hat{G}^\theta: S^L\times\bar{S}^L \tto  \mc{P}(\lbrace 1,\ldots,L\rbrace)$ is given by:
\begin{align*}
\hat{G}_j(y,x)\coloneqq\frac{G^{\theta}_j(y,x)}{\sum_{j=1}^L G^\theta_{j}(y,x)}.
\end{align*}

We then arrive at:
\begin{algorithm}[!h]
\small
\caption{Our Gillespie Sampler}
\label{alg:ourgillespiesampler}
\begin{algorithmic}[1]
\State \textbf{Initialize:} $t \gets 0, x_0 \gets (M, \dots, M)$, planner $G^\theta$, denoiser $D^\theta$, maximum steps $T$
\For{$t = 1 : T$}
    \State {\colorbox{gray!20}{\textbf{Plan}}} Sample $y \sim D^\theta(x_t)$
    \State Sample dimension $d' \sim \hat{G}^\theta_{\cdot}(y, x_t)$
    \State {\colorbox{gray!20}{\textbf{Denoise}}}
    \If{$[x_t]_{d'} \neq M$}
        \State $[x_t]_d \gets M$
        \State Resample $y_{d'} \sim D^\theta_{d', \cdot}(x_t)$
        \State $[x_{t+1}]_{d'} \gets y_{d'}$
    \Else
        \State $[x_{t+1}]_{d'} \gets y_{d'}$
    \EndIf
\EndFor
\State \textbf{return} $x_T$
\end{algorithmic}
\end{algorithm}
\subsection{Proof of the ELBO Proposition \ref{prop:ELBO}}

As per the discussion in Section \ref{subsection:ELBOrole}, it suffices to find a lower bound on $-D_{KL}(\mathbb{P}^{x^0}||\mathbb{P}^\theta)$, where $\mathbb{P}^{x^0}$ is the Law of the continuous time Markov chain $\overset{\leftarrow}{X}^{x^0}$ with rate matrix $\overset{\leftarrow}{Q}^{x^0}$ given by \eqref{eq:conditionalbackwardsmatrix}, $\mathbb{P}^\theta$ is the Law of the continuous time Markov chain $X^{\theta}$ with rate matrix $Q^{\theta}$ given by \eqref{eq:P2ratematrix}, and $\overset{\leftarrow}{X}^{x^0}_0=X^\theta_0=M^L$. Via an application of Girsanov's Theorem for CTMCs (see e.g. Theorem III.5.34 in \cite{jacod2013} for a general result or \cite{ren2024} Theorem 3.3  for the specific CTMC setting):
\begin{align*}
&-D_{KL}(\mathbb{P}^{x^0}||\mathbb{P}^\theta)\\
&=-\int_0^1 \bb{E}_{X_t\sim P_{1-t}(\cdot;\delta_{x^0})}\biggl[\sum_{y\neq X_t} Q^{\theta}_t(y,X_t)-\overset{\leftarrow}{Q}^{x^0}(y,X_t)+\overset{\leftarrow}{Q}^{x^0}(y,X_t)\log\left(\frac{\overset{\leftarrow}{Q}^{x^0}(y,X_t)}{Q^{\theta}_t(y,X_t)}\right)\biggr]dt\\ 
& = -\int_0^1 \bb{E}_{X_t\sim P_{1-t}(\cdot;\delta_{x^0})}\biggl[-Q^{\theta}_t(X_t,X_t)+\overset{\leftarrow}{Q}^{x^0}(X_t,X_t)+\sum_{y\neq X_t}\overset{\leftarrow}{Q}^{x^0}(y,X_t)\log\left(\frac{\overset{\leftarrow}{Q}^{x^0}(y,X_t)}{Q^{\theta}_t(y,X_t)}\right)\biggr]dt\\ 
& = -\int_0^1 \frac{\dot{\alpha}(t)}{1-\alpha(t)} \bb{E}_{X_t\sim P_{t }(\cdot;\delta_{x^0})}\biggl[\sum_{i=1}^L  \delta_{M}([X_t]_i)(1- \bb{E}_{Y\sim D^{\theta}(X_t)}[G^{\theta}_i(Y,X_t)])\\ 
&-(1-\delta_M([X_t]_i))\bb{E}_{Y\sim D^{\theta}(X_t)}[G^{\theta}_i(Y^{-i,[X_t]_i},X_t)]+\delta_{M}([X_t]_i)\log(F^{\theta}_{i}(x^0,X_t)\hat{D}^{\theta}_{i,x^0_i}(X_t))\biggr]dt,
\end{align*}
where in the third equality we have inserted the definitions of $\overset{\leftarrow}{Q}^{x^0}$ and $Q^\theta$ and reversed the role of the time parameter $t\mapsto 1-t$, and $P_t$ is as in \eqref{eq:forwarddynamics}.

We consider this as 4 parts:
\begin{align*}
E_1(x^0)&\coloneqq-\int_0^1 \frac{\dot{\alpha}(t)}{1-\alpha(t)} \bb{E}_{X_t\sim P_{t }(\cdot;\delta_{x^0})}\biggl[\sum_{i=1}^L  \delta_{M}([X_t]_i)(1- \bb{E}_{Y\sim D^{\theta}(X_t)}[G^{\theta}_i(Y,X_t)]\biggr]dt\\ 
E_2(x^0)&\coloneqq -\int_0^1 \frac{\dot{\alpha}(t)}{1-\alpha(t)} \bb{E}_{X_t\sim P_{t }(\cdot;\delta_{x^0})}\biggl[\sum_{i=1}^L  (1-\delta_M([X_t]_i))\left(-\bb{E}_{Y\sim D^{\theta}(X_t)}[G^{\theta}_i(Y^{-i,[X_t]_i},X_t)]\right)\biggr]dt\\
E_3(x^0)&\coloneqq -\int_0^1 \frac{\dot{\alpha}(t)}{1-\alpha(t)} \bb{E}_{X_t\sim P_{t }(\cdot;\delta_{x^0})}\biggl[\sum_{i=1}^L \delta_{M}([X_t]_i)\log(F^{\theta}_{i}(x^0,X_t))\biggr]dt\\
E_4(x^0)&\coloneqq -\int_0^1 \frac{\dot{\alpha}(t)}{1-\alpha(t)} \bb{E}_{X_t\sim P_{t }(\cdot;\delta_{x^0})}\biggl[\sum_{i=1}^L \delta_{M}([X_t]_i)\log(\hat{D}^{\theta}_{i,x^0_i}(X_t))\biggr]dt
\end{align*}

Recalling that $\dot{\alpha}(t)\leq 0$ for all $t\in [0,1]$ and $G^{\theta}_i(y,x)\in [0,1]$ for all $i\in\lbrace 1,\dots,L\rbrace$, $y\in S^L$ and $x\in\bar{S}^L$, we see $E_1(x^0)$ is positive for all $x^0\in S^L$, and artificially attempting to ensure that the rates of the original CTMC and our modified one do not differ too much out of masked positions (see the discussion of the ``Rate Forcing Term'' in Appendix C.2 of \cite{DFM}). Hence we simply bound it below by zero:
\begin{align*}
E_1(x^0)\geq 0,
\end{align*}
because we are only interested in $P^{\theta}_1$ being close to $p_{data}$, not the entire trajectory of the chains $X^\theta$ and $\overset{\leftarrow}{X}$ being close.

For the $E_2(x^0)$ we note that, by definition of $P_t$, when $[X_t]_i\neq M$, it is equal to its initial value $x^0_i$. Along with the bound $-z\geq \log(1-z),\forall z\in [0,1)$, this yields:
\begin{align*}
E_2(x^0)\geq -\int_0^1 \frac{\dot{\alpha}(t)}{1-\alpha(t)} \bb{E}_{X_t\sim P_{t }(\cdot;\delta_{x^0})}\biggl[\sum_{i=1}^L(1-\delta_M([X_t]_i))\log(\bb{E}_{Y\sim D^{\theta}(X_t)}[1-G^{\theta}_i(Y^{-i,x_i^0},X_t)])\biggr]dt.
\end{align*}
Applying Jensen's inequality to move the expected value with respect to $D^{\theta}(X_t)$ outside of the log yields:
\begin{align*}
E_2(x^0)\geq E_{UP}(x^0),\forall x^0\in S^L.
\end{align*}

For $E_3(x^0)$ we note that, by definition, when $[X_t]_i=M$, $F_i^{\theta}(x^0,X_t)=\bb{E}_{Y\sim D^{\theta}(X_t)}[G^{\theta}_j(Y^{-i,x^0_i},X_t)]$. An application of Jensen's inequality to $D^{\theta}(X_t)$ outside of the log yields:
\begin{align*}
E_3(x^0)\geq E_{MP}(x^0),\forall x^0\in S^L.
\end{align*}

Finally, for $E_4(x^0)$, we note that, by definition, when $[X_t]_i=M$, $\hat{D}^{\theta}_{i,x^0_i}(X_t)=D^{\theta}_{i,x^0_i}(X_t)$, so 
\begin{align*}
E_4(x^0) = E_{MP}(x^0),\forall x^0\in S^L.
\end{align*}

This results in the desired bound.

\section{Implementation Details}
\label{sec:pytorch_impl}
In Listing~\ref{lst:pps-code}, we provide a self-contained PyTorch implementation of our \emph{Path-Planning Sampling} procedure. The code consists of three core components, each addressing a distinct step in the sampling process:

\paragraph{1) \texttt{topk\_lowest\_masking:}} 
Given a matrix of scalar scores, this function returns a boolean mask that flags the ``lowest-scoring'' positions per row. The user can specify how many positions should be re-masked by providing a \texttt{cutoff\_len} tensor. Internally, the function sorts the score matrix and determines the threshold score for each row before comparing every score to this cutoff.

\paragraph{2) \texttt{stochastic\_sample\_from\_categorical:}}
This function draws samples from a categorical distribution using Gumbel noise. It first applies Gumbel noise to the input logits (if a non-zero temperature is specified), then computes the \(\log\)-softmax to obtain token probabilities. The sampled tokens and their corresponding log probabilities are returned.

\paragraph{3) \texttt{path\_planning\_sampling:}}
Positions initially set to the \texttt{mask\_token\_id} are iteratively predicted and updated. At each iteration, we:
\begin{enumerate}
    \item Compute model logits and identify positions that remain masked.
    \item Sample from the model outputs via \texttt{stochastic\_sample\_from\_categorical}.
    \item Integrate a \texttt{planner} (if provided) to re-score predictions for currently unmasked positions, giving users the flexibility to incorporate any additional guidance or constraints.
    \item Construct a \texttt{score} and re-mask positions with the lowest scores. Fixed positions are ignored by assigning them infinite scores so that they cannot be re-masked.
    \item Scale the scores of unmasked positions by the factor \(\eta\), which adjusts how aggressively new tokens are updated.
\end{enumerate}
The function continues for \texttt{num\_steps}, revealing high-confidence predictions and re-masking uncertain positions. Finally, any remaining masks are replaced with the last sampled tokens. The key parameters are:
\begin{itemize}
    \item \texttt{xt}: The initial token matrix of shape \([B, L]\), containing masked tokens.
    \item \texttt{model}: A callable mapping tokens to logits.
    \item \texttt{tokenizer}: Provides the special \texttt{mask\_token\_id}.
    \item \texttt{num\_steps}: Number of refinement iterations.
    \item \texttt{tau}: Temperature for controlling sampling noise.
    \item \texttt{kappa\_fn}: A schedule function in \([0,1]\) that dictates how many positions remain masked vs.\ unmasked over time.
    \item \texttt{eta}: A multiplier for scores in unmasked positions.
    \item \texttt{planner}: An optional model for additional re-scoring.
    \item \texttt{score\_type}: Either \texttt{'confidence'} (uses log probabilities) or \texttt{'random'} (random re-masking).
\end{itemize}

\begin{lstlisting}[
  language=python,
  caption={Path-Planning Sampling procedure in PyTorch},
  label={lst:pps-code},
  floatplacement=p
]
import torch

def topk_lowest_masking(scores, cutoff_len):
    sorted_scores, _ = scores.sort(dim=-1)
    threshold = sorted_scores.gather(dim=-1, index=cutoff_len)
    return scores < threshold

def stochastic_sample_from_categorical(logits, temperature=1.0, noise_scale=1.0):
    logits = logits.double()
    if temperature != 0.0:
        gumbel = -torch.log(-torch.log(torch.rand_like(logits) + 1e-8) + 1e-8)
        logits = logits / temperature + noise_scale * gumbel
    scores, tokens = logits.log_softmax(dim=-1).max(dim=-1)
    return tokens, scores

@torch.inference_mode()
@torch.cuda.amp.autocast()
def path_planning_sampling(
    xt, 
    model, 
    tokenizer, 
    num_steps, 
    tau=1.0, 
    kappa_fn=lambda t: t, 
    eta=1.0, 
    planner=None, 
    score_type='confidence'
):
    fix_mask = (xt != tokenizer.mask_token_id)
    dt = 1.0 / num_steps

    for step in range(1, num_steps + 1):
        t = step * dt
        kappa_t = kappa_fn(t)
        logits = model(xt).double()

        last_mask = (xt == tokenizer.mask_token_id)
        unmask_candidates = ~last_mask & ~fix_mask

        x0, logp = stochastic_sample_from_categorical(logits, temperature=tau)

        if planner is not None:
            planner_logits = planner(x0).double()
            planner_logp = planner_logits.log_softmax(dim=-1).gather(-1, x0.unsqueeze(-1)).squeeze(-1)
            logits[unmask_candidates] = planner_logits[unmask_candidates]
            logp[unmask_candidates] = planner_logp[unmask_candidates]

        if score_type == 'confidence':
            score = logp
        elif score_type == 'random':
            score = torch.rand_like(logp).log()
        else:
            raise ValueError("Invalid score_type.")

        score = score.masked_fill(fix_mask, float('inf'))
        score[unmask_candidates] *= eta

        num_to_mask = ((~fix_mask).sum(dim=1, keepdim=True).float() * (1 - kappa_t)).long()
        mask = topk_lowest_masking(score, num_to_mask)
        xt[mask] = tokenizer.mask_token_id

        mask_to_x0 = last_mask & ~mask
        xt[mask_to_x0] = x0[mask_to_x0]

    remaining_mask = (xt == tokenizer.mask_token_id)
    xt[remaining_mask] = x0[remaining_mask]

    return xt
\end{lstlisting}


\section{Experimental Details}

\subsection{Example of Language generation Task}

We provide Table~\ref{tab:examples_benchmarks} consisting of examples for the five language generation tasks.

\begin{table}[h!]
\centering
\caption{Examples from language understanding benchmarks.}
\label{tab:examples_benchmarks}
\begin{adjustbox}{max width=\textwidth}
\begin{tabular}{|l|p{9cm}|p{6cm}|}
\hline
\textbf{Metric} & \textbf{Question} & \textbf{Answer} \\ \hline
LAMBADA & 
"Again, he left that up to you. However, he was adamant in his desire that it remain a private ceremony. He asked me to make sure, for instance, that no information be given to the newspaper regarding his death, not even an obituary. I got the sense that he didn’t want anyone, aside from the three of us, to know that he’d even \_\_." 
& died \\ \hline
GSM8K & 
Weng earns \$12 an hour for babysitting. Yesterday, she just did 50 minutes of babysitting. How much did she earn? 
& 10 \\ \hline
TriQA & 
The Dodecanese Campaign of WWII that was an attempt by the Allied forces to capture islands in the Aegean Sea was the inspiration for which acclaimed 1961 commando film? 
& The Guns of Navarone \\ \hline
ROCStories & 
Morgan and her family lived in Florida. They heard a hurricane was coming. \textbf{(Story infills here...)} They arrived and learned from the news that it was a terrible storm. They felt lucky they had evacuated when they did. 
& They decided to evacuate to a relative's house. \\ \hline
Code & 
\begin{lstlisting}[language=Python, basicstyle=\scriptsize\ttfamily, breaklines=true, frame=single]
from typing import List

def has_close_elements(numbers: List[float], threshold: float) -> bool:
    """
    Check if in given list of numbers, are any two numbers closer
    to each other than given threshold.

    >>> has_close_elements([1.0, 2.0, 3.0], 0.5)
    False
    >>> has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3)
    True
    """
    # Infill Code
    if distance < threshold:
        return True
    return False
\end{lstlisting}
& 
\begin{lstlisting}[language=Python, basicstyle=\scriptsize\ttfamily, breaklines=true, frame=single]
for idx, elem in enumerate(numbers):
    for idx2, elem2 in enumerate(numbers):
        if idx != idx2:
            distance = abs(elem - elem2)
\end{lstlisting} \\ \hline
\end{tabular}
\end{adjustbox}
\end{table}




\subsection{Protein Sequence Generation}

\label{sec:protein_benchmark_eval}

\paragraph{Setup} We compare our method with state-of-the-art protein sequence generation models, including three discrete diffusion models—DPLM~\citep{DPLM}, EvoDiff~\citep{Alamdari2024ProteinGW}, and ESM3~\citep{esm3}—and an autoregressive model, ProGen2~\citep{Nijkamp2022ProGen2ET}, across three model sizes: small, medium, and large. Additionally, we benchmark masked language models, ESM2~\citep{esm2}, at three scales: 150M, 650M, and 3B parameters.

For our path-planning algorithm (P2), we vary the stochasticity strength from 1.0 to 2.0 in increments of 0.1 and report optimal results. Baselines are evaluated with default sampling strategies. Since ESM2 lacks a masked diffusion loss, it uses ancestral sampling. Each model generates 100 sequences for sequence lengths in $[200, 300, \dots, 800]$. DPLM employs a sequence length matching the number of sampling steps and a temperature of 0.9, with rejection-resampling disabled for fairness. ESM3 is sampled with a temperature of 1, a cosine schedule, top-$p = 1$, and 500 steps. Special tokens are removed to ensure valid amino acid sequences.

\paragraph{Evaluation.} Protein sequence generation quality is evaluated via protein folding models, using ESMFold~\citep{esm2} as a proxy for structural stability. We extract three folding metrics:
\begin{itemize}
\item \textbf{pLDDT} (predicted Local Distance Difference Test): Measures local structural accuracy.
\item \textbf{pTM} (predicted Template Modeling): Assesses global structural plausibility.
\item \textbf{pAE} (predicted Alignment Error): Evaluates overall compactness.
\end{itemize}
A sequence can achieve high pLDDT while exhibiting poor global compactness (high pAE). To ensure robust evaluation, we define \textit{foldability} as the proportion of sequences satisfying pLDDT $> 80$, pTM $> 0.7$, and pAE $< 10$. This metric effectively identifies low-quality sequences, such as repetitive patterns (e.g., “ABABABAB”), which tend to have high pAE.

Beyond folding scores, we compute:
\begin{itemize}
\item \textbf{Token entropy}, excluding tokens not present in generated sequences.
\item \textbf{Sequence diversity}, defined as $1 -$ pairwise sequence identity within a batch. Since all sequences in a batch share equal length, no sequence alignment is needed.
\end{itemize}
These metrics detect mode collapse, where models generate highly repetitive sequences.

\section{Protein Sequence Generation}

\subsubsection{Training Details of the 150M MDM.}
\label{sec:training-detail-MDM-protein}
We train a 150M mask diffusion model on protein sequences for the ablation of self-planning. The 150M MDM is trained using the open-sourced DPLM code\footnote{https://github.com/bytedance/dplm}. We use the same transformer architecture as DPLM-150M as well as ESM2-150M. We train our MDM from scratch for 500k steps with a total of 320K tokens in each iteration, which is achieved by multi-GPU and multi-node training with gradient accumulation. The training data is Uniref50, consisting of around 40M protein sequences with 50\% sequence-identity cutoff, namely, the sequences in uniref50 are at least higher than 50\% dissimilar. Uniref50 is widely used for training protein language models.




\subsection{Computing the ELBO}

The Evidence Lower Bound (ELBO) serves as the training objective of mask diffusion models and can be used to assess how well the model fits the data. The ELBO experiments are conducted on protein sequence generation tasks. We compute the negative ELBO for five planners, namely ESM-8M, ESM-35M, ESM-150M, ESM-650M, and ESM-3B, alongside the self-planning ELBO, using a weighted cross-entropy loss function to quantify reconstruction accuracy.
\paragraph{Dataset Preparation.}
We utilize sequences from the UniRef50 dataset, filtering to include only test sequences with lengths shorter than 300 residues to align with the experiments in Figure~\ref{fig:ablation_planner} and mitigate memory constraints. The dataset is loaded into a PyTorch DataLoader using a sequence length of 1022 tokens and a maximum token budget of 60,000. For consistent evaluation, we run the ELBO calculation over 20 independent simulations and report the average across these runs.

\paragraph{Masking Strategy.}
For each sequence, we randomly generate a mask ratio uniformly sampled from the range $[1/500, 1 - 1/500]$. Positions are masked based on this ratio, but masking is constrained to avoid altering non-maskable tokens (e.g., special symbols). The masked tokens are replaced with a designated mask token provided by the denoiser model.

\paragraph{Loss Calculation.}
To compute the ELBO, the denoiser and planner models predict the original tokens for both masked and unmasked positions. The cross-entropy loss is calculated separately for these categories. Both masked and unmasked loss values are weighted inversely by the mask ratio to ensure probabilistic consistency in the evaluation. Each model is evaluated across 20 independent simulations, and the average ELBO is reported to capture the robustness of the planners under stochastic settings. 

\section{Additional Results}

\subsection{Language Generation}


\subsubsection{BREAKING THE REVERSE CURSE}
\label{sec:BREAKING THE REVERSE CURSE}


\begin{table*}[t]
\caption{Results on breaking the reverse curse: Performance comparison of models on DescriptionToName and NameToDescription tasks. Metrics include accuracy (Acc.) and BLEU scores (BLEU) for both same and reverse directions.}
\label{tab:breaking_reverse_curse}
\centering
\resizebox{\linewidth}{!}{%
\begin{tabular}{lcccccc}
\hline
 & \multicolumn{2}{c}{\textbf{DescriptionToName}} & \multicolumn{4}{c}{\textbf{NameToDescription}} \\
 & \textbf{Same direction} & \textbf{Reverse direction} & \textbf{Same direction} & \textbf{BLEU ↑} & \textbf{Reverse direction} & \textbf{BLEU ↑} \\
 & \textbf{Acc. ↑} & \textbf{Acc. ↑} & \textbf{Acc. ↑} & \textbf{BLEU ↑} & \textbf{Acc. ↑} & \textbf{BLEU ↑} \\
\hline
GPT3 (175B)    & 97 & 0  & 50 & -    & 0  & -    \\
Llama-2 (13B)  & 99 & 0  & -  & 74   & -  & 19   \\
T5 (3B)        & \textbf{100} & 0  & 47 & \textbf{87}  & 0  & 20   \\
MDM (1.1B)     & 97 & 92 & \textbf{49} & 76   & \textbf{37} & 67   \\
MDM (1.1B) + \textbf{Path Planning (P2)} & 96 & \textbf{93} & 48 & 78 & 36 & \textbf{68} \\
\hline
\end{tabular}
}
\end{table*}

\textbf{Benchmark.}
\citet{berglund2023reversal} introduced the concept of the reverse curse, which refers to the difficulty of
ARMs in generalizing bidirectional relationships. Specifically, this occurs when a model is trained
on information in the form “A is B” but fails to infer the reverse relationship “B is A.” For example, a model trained on the fact “Valentina Tereshkova was the first woman to travel to space”
may not correctly answer the reverse question “Who was the first woman to travel to space?” This
limitation raises concerns about whether large language models genuinely possess logical reasoning
capabilities. 

\textbf{Baselines.}
We compare with the leading AR models including GPT3 (175B), Llama-2 (13B), and the T5 consisting of both bidirectional encoder and unidirectional decoder, finetuned on the reverse curse dataset. For the MDM baseline, We use the existing MDM (1.1B) from~\cite {gong2024scalingdiffusionlanguagemodels} with its default greedy ancestral sampling strategy.  

\textbf{Setup.} It is observed in SMDM\citep{gong2024scalingdiffusionlanguagemodels} that MDMs easily break the reverse curse, displaying near-perfect reverse accuracy where ARs achieve 0 accuracy.
We follow SMDM\citep{gong2024scalingdiffusionlanguagemodels} and evaluate MDMs on the same reverse curse dataset used by Berglund et al. (2023),
which consists of fictitious statements in the format “⟨name⟩ is ⟨description⟩” and the reversals.
We use the pretrained MDMs and baseline results from SMDM~\citep{gong2024scalingdiffusionlanguagemodels} which on these statements and assess their performance using questions not seen during training. Following the same protocol
as ~\citep{berglund2023reversal}, we generate responses and report the exact match
accuracy and use the BLEU metric~\citep{papineni2002bleu} to evaluate the quality of
name-to-description generation~\citep{lv2023we}.  


\textbf{results.}
As shown in Table~\ref{tab:breaking_reverse_curse}, both the T5 model and ARMs achieve
zero accuracy and low BLEU scores with reverse queries.
Equipping with P2, we successfully improve the accuracy of MDMs in Reverse direction of task Description To Name and the BLEU metric of Name To Description in both directions.


\subsection{Protein Sequence Generation}

\paragraph{Performance Across Length Categories.}  
We analyze the performance of protein generation models across various sequence lengths, ranging from 200 to 800 base pairs. Certain models, such as ProGen, do not generate proteins of fixed lengths; therefore, we group results into length categories to facilitate meaningful comparisons. As shown in Figure~\ref{fig:perf_vs_len}, the performance of these models varies with length, highlighting their capabilities and limitations across diverse length categories.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=1\linewidth]{figs//appendix/perf_vs_len.png}
    \caption{Protein Sequence Generation Benchmark: Performance across length categories (200–800).}
    \label{fig:perf_vs_len}
\end{figure}

\subsubsection{Design Space of P2.}
We explore the design space of our proposed P2 framework using key metrics, including pLDDT, pAE, pTM, entropy, and diversity. As illustrated in Figure~\ref{fig:appendix_design_space_p2}, P2 demonstrates a strong ability to balance structural accuracy and diversity, underscoring its versatility and robustness in protein generation tasks.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=1\linewidth]{figs/appendix/design_space_p2.png}
    \caption{Design space of P2, characterized by pLDDT, pAE, pTM, entropy, and diversity metrics.}
    \label{fig:appendix_design_space_p2}
\end{figure}

\subsubsection{Ablation Study on the Planner.}
We investigate the impact of planner size on model performance through an ablation study. Figure~\ref{fig:more_ablation_planner} shows how varying the planner size affects key metrics such as pLDDT and diversity. These results emphasize the importance of planner size in optimizing the quality and consistency of generated sequences.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=1\linewidth]{figs/appendix/more_ablation_planner.png}
    \caption{Ablation study of planner size and its impact on protein generation performance.}
    \label{fig:more_ablation_planner}
\end{figure}

\subsubsection{Inference-Time Scaling: Performance vs. Sampling Time.}
To evaluate the trade-off between inference time and performance, we investigate how sampling time scales with model performance. These results will be detailed in future work, but they highlight the scalability of our approach for efficient protein generation.

\subsubsection{Generated Protein Sequences and Their Predicted Structures.}
We fold the protein sequences generated by our model using ESMFold and visualize their predicted structures in Figures~\ref{fig:protein_structures_group1}--\ref{fig:protein_structures_group4}. For each length category—200, 300, 400, 500, 600, 700, and 800—we display 15 representative proteins. These visualizations highlight the structural diversity and consistency of the generated sequences, providing evidence of the model’s ability to predict biologically plausible structures across diverse lengths.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=1\linewidth]{figs/appendix/standard_figure_2.png}
    \includegraphics[width=1\linewidth]{figs/appendix/standard_figure_3.png}
    \caption{Predicted structures of generated protein sequences (Group 1). Each panel represents structures generated for specific length categories.}
    \label{fig:protein_structures_group1}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=1\linewidth]{figs/appendix/standard_figure_4.png}
    \includegraphics[width=1\linewidth]{figs/appendix/standard_figure_5.png}
    \caption{Predicted structures of generated protein sequences (Group 2). Each panel corresponds to different length categories.}
    \label{fig:protein_structures_group2}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=1\linewidth]{figs/appendix/standard_figure_6.png}
    \includegraphics[width=1\linewidth]{figs/appendix/standard_figure_7.png}
    \caption{Predicted structures of generated protein sequences (Group 3). These structures illustrate the diversity and robustness of the generation process.}
    \label{fig:protein_structures_group3}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=1\linewidth]{figs/appendix/standard_figure_8.png}
    \caption{Predicted structures of generated protein sequences (Group 4). This group emphasizes structures for the longest generated sequences.}
    \label{fig:protein_structures_group4}
\end{figure}

\subsubsection{RNA RDM Training Implementation.}
\label{sec:rna_training}
The RNA RDM follows the same discrete diffusion described in \citep{RDM}. The RDM was trained using a machine mounted with 4 A100 GPUs, each with 40GB memory. The training implementation is otherwise identical to the second-stage fine-tuning described in \citep{DPLM}, where we continued from a RiNALMo \citep{penic2024rinalmo} checkpoint instead of ESM-2 \citep{esm2}.

\subsubsection{Visualizing the Predicted 
Structures of Generated RNA Sequences.}
\label{sec:rna_vis}
We extend our analysis to RNA sequence generation by folding RNA sequences of 200 base pairs using AlphaFold3~\citep{abramson2024accurate}. The predicted folding structures, visualized in Figures~\ref{fig:rna-vis-appendix-3d} and \ref{fig:rna-vis-appendix-200}, highlight the diversity and consistency of the RNA structures generated by the model. Particularly, predicted structures exhibit greater diversity as sequence length increases, as is observed in nature, while their pLDDT's mirroring those computed for natural sequences. We also include the predicted secondary structures of generated RNAs in Figure~\ref{fig:rna-vis-2d}. These results demonstrate the model’s ability to generate biologically plausible RNA sequences suitable for downstream applications.

% \begin{center}
% \centering
% \begin{table}[ht]
% \label{tab:RNA_RDM_training}
% \caption{RNA RDM Training details.}
% \begin{tabular}{lr}
% \toprule
% Optimizer & adamw \\
% Learning Rate & [4e-5, 1e-5] \\
% Learning Rate Schedule & Polynomial  \\
% Diffusion Weighting & Linear  \\
% Weight Decay & 0.01  \\
% \bottomrule
% \end{tabular}
% \end{table}
% \end{center}


\begin{figure}[htbp]
    \centering
    \includegraphics[width=1\linewidth]{figs/appendix/RNA_3d_appendix.png}
    \caption{Predicted structures of additional generated RNA sequences (100 bps).}
    \label{fig:rna-vis-appendix-3d}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=1\linewidth]{figs/appendix/rna_vis.png}
    \caption{Predicted structures of generated RNA sequences (200 bps). This figure showcases the structural diversity of RNA sequences generated by the model as sequence length increases, which is observed in nature.}
    \label{fig:rna-vis-appendix-200}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=1\linewidth]{figs/appendix/rna_2d_struct.png}
    \caption{Predicted secondary structures of generated RNA sequences of length 100 (top) and 200 bp (bottom). Predictions were made using ViennaRNA \citep{lorenz2011viennarna} and visualized with forna \citep{kerpedjiev2015forna}.}
    \label{fig:rna-vis-2d}
\end{figure}

\end{document}

