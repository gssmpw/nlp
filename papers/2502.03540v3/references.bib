@misc{gong2024scalingdiffusionlanguagemodels,
      title={Scaling Diffusion Language Models via Adaptation from Autoregressive Models}, 
      author={Shansan Gong and Shivam Agarwal and Yizhe Zhang and Jiacheng Ye and Lin Zheng and Mukai Li and Chenxin An and Peilin Zhao and Wei Bi and Jiawei Han and Hao Peng and Lingpeng Kong},
      year={2024},
      eprint={2410.17891},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2410.17891}, 
}

@misc{nie2024scalingmaskeddiffusionmodels,
      title={Scaling up Masked Diffusion Models on Text}, 
      author={Shen Nie and Fengqi Zhu and Chao Du and Tianyu Pang and Qian Liu and Guangtao Zeng and Min Lin and Chongxuan Li},
      year={2024},
      eprint={2410.18514},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2410.18514}, 
}

@article{abramson2024accurate,
  title={Accurate structure prediction of biomolecular interactions with AlphaFold 3},
  author={Abramson, Josh and Adler, Jonas and Dunger, Jack and Evans, Richard and Green, Tim and Pritzel, Alexander and Ronneberger, Olaf and Willmore, Lindsay and Ballard, Andrew J and Bambrick, Joshua and others},
  journal={Nature},
  pages={1--3},
  year={2024},
  publisher={Nature Publishing Group UK London}
}

@article{lorenz2011viennarna,
  title={ViennaRNA Package 2.0},
  author={Lorenz, Ronny and Bernhart, Stephan H and H{\"o}ner zu Siederdissen, Christian and Tafer, Hakim and Flamm, Christoph and Stadler, Peter F and Hofacker, Ivo L},
  journal={Algorithms for molecular biology},
  volume={6},
  pages={1--14},
  year={2011},
  publisher={Springer}
}

@article{kerpedjiev2015forna,
  title={Forna (force-directed RNA): Simple and effective online RNA secondary structure diagrams},
  author={Kerpedjiev, Peter and Hammer, Stefan and Hofacker, Ivo L},
  journal={Bioinformatics},
  volume={31},
  number={20},
  pages={3377--3379},
  year={2015},
  publisher={Oxford University Press}
}

@article{rnacentral2021rnacentral,
  title={RNAcentral 2021: secondary structure integration, improved sequence search and new member databases},
author={Anton I. Petrov},
  journal={Nucleic acids research},
  volume={49},
  number={D1},
  pages={D212--D220},
  year={2021},
  publisher={Oxford University Press}
}

@article{shen2024accurate,
  title={Accurate RNA 3D structure prediction using a language model-based deep learning approach},
  author={Shen, Tao and Hu, Zhihang and Sun, Siqi and Liu, Di and Wong, Felix and Wang, Jiuming and Chen, Jiayang and Wang, Yixuan and Hong, Liang and Xiao, Jin and others},
  journal={Nature Methods},
  pages={1--12},
  year={2024},
  publisher={Nature Publishing Group US New York}
}

@InProceedings{Chang_2022_CVPR,
    author    = {Chang, Huiwen and Zhang, Han and Jiang, Lu and Liu, Ce and Freeman, William T.},
    title     = {MaskGIT: Masked Generative Image Transformer},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2022},
    pages     = {11315-11325}
}

@article{penic2024rinalmo,
  title={Rinalmo: General-purpose rna language models can generalize well on structure prediction tasks},
  author={Peni{\'c}, Rafael Josip and Vla{\v{s}}i{\'c}, Tin and Huber, Roland G and Wan, Yue and {\v{S}}iki{\'c}, Mile},
  journal={arXiv preprint arXiv:2403.00043},
  year={2024}
}

@article{DFM,
  title={Generative Flows on Discrete State-Spaces: Enabling Multimodal Flows with Applications to Protein Co-Design},
  author={Andrew Campbell and Jason Yim and Regina Barzilay and Tom Rainforth and T. Jaakkola},
  journal={ArXiv},
  year={2024},
  url={https://api.semanticscholar.org/CorpusID:267523194}
}

@article{DPLM2,
  title={DPLM-2: A Multimodal Diffusion Protein Language Model},
  author={Xinyou Wang and Zaixiang Zheng and Fei Ye and Dongyu Xue and Shujian Huang and Quanquan Gu},
  journal={ArXiv},
  year={2024},
  volume={abs/2410.13782},
  url={https://api.semanticscholar.org/CorpusID:273403705}
}
@article{DPLM,
  title={Diffusion Language Models Are Versatile Protein Learners},
  author={Xinyou Wang and Zaixiang Zheng and Fei Ye and Dongyu Xue and Shujian Huang and Quanquan Gu},
  journal={ArXiv},
  year={2024},
  volume={abs/2402.18567},
  url={https://api.semanticscholar.org/CorpusID:268063857}
}
@article{RDM,
  title={A Reparameterized Discrete Diffusion Model for Text Generation},
  author={Lin Zheng and Jianbo Yuan and Lei Yu and Lingpeng Kong},
  journal={ArXiv},
  year={2023},
  volume={abs/2302.05737},
  url={https://api.semanticscholar.org/CorpusID:256826865}
}

@article{ddpd,
  title={Think While You Generate: Discrete Diffusion with Planned Denoising},
  author={Sulin Liu and Juno Nam and Andrew Campbell and Hannes St{\"a}rk and Yilun Xu and T. Jaakkola and Rafael G'omez-Bombarelli},
  journal={ArXiv},
  year={2024},
  volume={abs/2410.06264},
  url={https://api.semanticscholar.org/CorpusID:273229043}
}

@article{md4,
  title={Simplified and Generalized Masked Diffusion for Discrete Data},
  author={Shi, Jiaxin and Han, Kehang and Wang, Zhe and Doucet, Arnaud and Titsias, Michalis K},
  journal={arXiv preprint arXiv:2406.04329},
  year={2024}
}

@inproceedings{
mdlm,
title={Simple and Effective Masked Diffusion Language Models},
author={Subham Sekhar Sahoo and Marianne Arriola and Aaron Gokaslan and Edgar Mariano Marroquin and Alexander M Rush and Yair Schiff and Justin T Chiu and Volodymyr Kuleshov},
booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
year={2024},
url={https://openreview.net/forum?id=L4uaAR4ArM}
}















@article{
esm3,
author = {Thomas Hayes  and Roshan Rao  and Halil Akin  and Nicholas J. Sofroniew  and Deniz Oktay  and Zeming Lin  and Robert Verkuil  and Vincent Q. Tran  and Jonathan Deaton  and Marius Wiggert  and Rohil Badkundri  and Irhum Shafkat  and Jun Gong  and Alexander Derry  and Raul S. Molina  and Neil Thomas  and Yousuf A. Khan  and Chetan Mishra  and Carolyn Kim  and Liam J. Bartie  and Matthew Nemeth  and Patrick D. Hsu  and Tom Sercu  and Salvatore Candido  and Alexander Rives },
title = {Simulating 500 million years of evolution with a language model},
journal = {Science},
volume = {0},
number = {0},
pages = {eads0018},
year = {2025},
doi = {10.1126/science.ads0018},
URL = {https://www.science.org/doi/abs/10.1126/science.ads0018},
eprint = {https://www.science.org/doi/pdf/10.1126/science.ads0018},
abstract = {More than three billion years of evolution have produced an image of biology encoded into the space of natural proteins. Here we show that language models trained at scale on evolutionary data can generate functional proteins that are far away from known proteins. We present ESM3, a frontier multimodal generative language model that reasons over the sequence, structure, and function of proteins. ESM3 can follow complex prompts combining its modalities and is highly responsive to alignment to improve its fidelity. We have prompted ESM3 to generate fluorescent proteins. Among the generations that we synthesized, we found a bright fluorescent protein at a far distance (58\% sequence identity) from known fluorescent proteins, which we estimate is equivalent to simulating five hundred million years of evolution.}}


@article{
esm2,
author = {Zeming Lin  and Halil Akin  and Roshan Rao  and Brian Hie  and Zhongkai Zhu  and Wenting Lu  and Nikita Smetanin  and Robert Verkuil  and Ori Kabeli  and Yaniv Shmueli  and Allan dos Santos Costa  and Maryam Fazel-Zarandi  and Tom Sercu  and Salvatore Candido  and Alexander Rives },
title = {Evolutionary-scale prediction of atomic-level protein structure with a language model},
journal = {Science},
volume = {379},
number = {6637},
pages = {1123-1130},
year = {2023},
doi = {10.1126/science.ade2574},
URL = {https://www.science.org/doi/abs/10.1126/science.ade2574},
eprint = {https://www.science.org/doi/pdf/10.1126/science.ade2574},
abstract = {Recent advances in machine learning have leveraged evolutionary information in multiple sequence alignments to predict protein structure. We demonstrate direct inference of full atomic-level protein structure from primary sequence using a large language model. As language models of protein sequences are scaled up to 15 billion parameters, an atomic-resolution picture of protein structure emerges in the learned representations. This results in an order-of-magnitude acceleration of high-resolution structure prediction, which enables large-scale structural characterization of metagenomic proteins. We apply this capability to construct the ESM Metagenomic Atlas by predicting structures for \&gt;617 million metagenomic protein sequences, including \&gt;225 million that are predicted with high confidence, which gives a view into the vast breadth and diversity of natural proteins. Machine learning methods for protein structure prediction have taken advantage of the evolutionary information present in multiple sequence alignments to derive accurate structural information, but predicting structure accurately from a single sequence is much more difficult. Lin et al. trained transformer protein language models with up to 15 billion parameters on experimental and high-quality predicted structures and found that information about atomic-level structure emerged in the model as it was scaled up. They created ESMFold, a sequence-to-structure predictor that is nearly as accurate as alignment-based methods and considerably faster. The increased speed permitted the generation of a database, the ESM Metagenomic Atlas, containing more than 600 million metagenomic proteins. â€”MAF A protein language model enables structure prediction and analysis of more than 600 million metagenomic proteins.}}


@article{Paperno2016TheLD,
  title={The LAMBADA dataset: Word prediction requiring a broad discourse context},
  author={Denis Paperno and Germ{\'a}n Kruszewski and Angeliki Lazaridou and Quan Ngoc Pham and Raffaella Bernardi and Sandro Pezzelle and Marco Baroni and Gemma Boleda and R. Fern{\'a}ndez},
  journal={ArXiv},
  year={2016},
  volume={abs/1606.06031},
  url={https://api.semanticscholar.org/CorpusID:2381275}
}
@article{Zheng2024MaskedDM,
  title={Masked Diffusion Models are Secretly Time-Agnostic Masked Models and Exploit Inaccurate Categorical Sampling},
  author={Kaiwen Zheng and Yongxin Chen and Hanzi Mao and Mingying Liu and Jun Zhu and Qinsheng Zhang},
  journal={ArXiv},
  year={2024},
  volume={abs/2409.02908},
  url={https://api.semanticscholar.org/CorpusID:272397565}
}

@inproceedings{Lou2023DiscreteDM,
  title={Discrete Diffusion Modeling by Estimating the Ratios of the Data Distribution},
  author={Aaron Lou and Chenlin Meng and Stefano Ermon},
  booktitle={International Conference on Machine Learning},
  year={2023},
  url={https://api.semanticscholar.org/CorpusID:264451832}
}
@inproceedings{joshi-etal-2017-triviaqa,
    title = "{T}rivia{QA}: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension",
    author = "Joshi, Mandar  and
      Choi, Eunsol  and
      Weld, Daniel  and
      Zettlemoyer, Luke",
    editor = "Barzilay, Regina  and
      Kan, Min-Yen",
    booktitle = "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2017",
    address = "Vancouver, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P17-1147/",
    doi = "10.18653/v1/P17-1147",
    pages = "1601--1611",
    abstract = "We present TriviaQA, a challenging reading comprehension dataset containing over 650K question-answer-evidence triples. TriviaQA includes 95K question-answer pairs authored by trivia enthusiasts and independently gathered evidence documents, six per question on average, that provide high quality distant supervision for answering the questions. We show that, in comparison to other recently introduced large-scale datasets, TriviaQA (1) has relatively complex, compositional questions, (2) has considerable syntactic and lexical variability between questions and corresponding answer-evidence sentences, and (3) requires more cross sentence reasoning to find answers. We also present two baseline algorithms: a feature-based classifier and a state-of-the-art neural network, that performs well on SQuAD reading comprehension. Neither approach comes close to human performance (23{\%} and 40{\%} vs. 80{\%}), suggesting that TriviaQA is a challenging testbed that is worth significant future study."
}
@article{Cobbe2021TrainingVT,
  title={Training Verifiers to Solve Math Word Problems},
  author={Karl Cobbe and Vineet Kosaraju and Mohammad Bavarian and Mark Chen and Heewoo Jun and Lukasz Kaiser and Matthias Plappert and Jerry Tworek and Jacob Hilton and Reiichiro Nakano and Christopher Hesse and John Schulman},
  journal={ArXiv},
  year={2021},
  volume={abs/2110.14168},
  url={https://api.semanticscholar.org/CorpusID:239998651}
}

@article{Mostafazadeh2016ACA,
  title={A Corpus and Cloze Evaluation for Deeper Understanding of Commonsense Stories},
  author={N. Mostafazadeh and Nathanael Chambers and Xiaodong He and Devi Parikh and Dhruv Batra and Lucy Vanderwende and Pushmeet Kohli and James F. Allen},
  journal={ArXiv},
  year={2016},
  volume={abs/1604.01696},
  url={https://api.semanticscholar.org/CorpusID:1726501}
}
@article{Bavarian2022EfficientTO,
  title={Efficient Training of Language Models to Fill in the Middle},
  author={Mohammad Bavarian and Heewoo Jun and Nikolas A. Tezak and John Schulman and Christine McLeavey and Jerry Tworek and Mark Chen},
  journal={ArXiv},
  year={2022},
  volume={abs/2207.14255},
  url={https://api.semanticscholar.org/CorpusID:251135268}
}
@inproceedings{lin-2004-rouge,
    title = "{ROUGE}: A Package for Automatic Evaluation of Summaries",
    author = "Lin, Chin-Yew",
    booktitle = "Text Summarization Branches Out",
    month = jul,
    year = "2004",
    address = "Barcelona, Spain",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W04-1013/",
    pages = "74--81"
}

@article{Biderman2024LessonsFT,
  title={Lessons from the Trenches on Reproducible Evaluation of Language Models},
  author={Stella Biderman and Hailey Schoelkopf and Lintang Sutawika and Leo Gao and Jonathan Tow and Baber Abbasi and Alham Fikri Aji and Pawan Sasanka Ammanamanchi and Sid Black and Jordan Clive and Anthony DiPofi and Julen Etxaniz and Benjamin Fattori and Jessica Zosa Forde and Charles Foster and Mimansa Jaiswal and Wilson Y. Lee and Haonan Li and Charles Lovering and Niklas Muennighoff and Ellie Pavlick and Jason Phang and Aviya Skowron and Samson Tan and Xiangru Tang and Kevin A. Wang and Genta Indra Winata and Franccois Yvon and Andy Zou},
  journal={ArXiv},
  year={2024},
  volume={abs/2405.14782},
  url={https://api.semanticscholar.org/CorpusID:269982020}
}
@article{Gulrajani2023LikelihoodBasedDL,
  title={Likelihood-Based Diffusion Language Models},
  author={Ishaan Gulrajani and Tatsunori Hashimoto},
  journal={ArXiv},
  year={2023},
  volume={abs/2305.18619},
  url={https://api.semanticscholar.org/CorpusID:258967177}
}

@article{Zhang2024TinyLlamaAO,
  title={TinyLlama: An Open-Source Small Language Model},
  author={Peiyuan Zhang and Guangtao Zeng and Tianduo Wang and Wei Lu},
  journal={ArXiv},
  year={2024},
  volume={abs/2401.02385},
  url={https://api.semanticscholar.org/CorpusID:266755802}
}

@article{lv2023we,
  title={Are we falling in a middle-intelligence trap? an analysis and mitigation of the reversal curse},
  author={Lv, Ang and Zhang, Kaiyi and Xie, Shufang and Tu, Quan and Chen, Yuhan and Wen, Ji-Rong and Yan, Rui},
  journal={arXiv preprint arXiv:2311.07468},
  year={2023}
}
@inproceedings{papineni2002bleu,
  title={Bleu: a method for automatic evaluation of machine translation},
  author={Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing},
  booktitle={Proceedings of the 40th annual meeting of the Association for Computational Linguistics},
  pages={311--318},
  year={2002}
}
@article{berglund2023reversal,
  title={The reversal curse: Llms trained on" a is b" fail to learn" b is a"},
  author={Berglund, Lukas and Tong, Meg and Kaufmann, Max and Balesni, Mikita and Stickland, Asa Cooper and Korbak, Tomasz and Evans, Owain},
  journal={arXiv preprint arXiv:2309.12288},
  year={2023}
}
@article{Touvron2023Llama2O,
  title={Llama 2: Open Foundation and Fine-Tuned Chat Models},
  author={Hugo Touvron and Louis Martin and Kevin R. Stone and Peter Albert and Amjad Almahairi and Yasmine Babaei and Nikolay Bashlykov and Soumya Batra and Prajjwal Bhargava and Shruti Bhosale and Daniel M. Bikel and Lukas Blecher and Cristian Cant{\'o}n Ferrer and Moya Chen and Guillem Cucurull and David Esiobu and Jude Fernandes and Jeremy Fu and Wenyin Fu and Brian Fuller and Cynthia Gao and Vedanuj Goswami and Naman Goyal and Anthony S. Hartshorn and Saghar Hosseini and Rui Hou and Hakan Inan and Marcin Kardas and Viktor Kerkez and Madian Khabsa and Isabel M. Kloumann and A. V. Korenev and Punit Singh Koura and Marie-Anne Lachaux and Thibaut Lavril and Jenya Lee and Diana Liskovich and Yinghai Lu and Yuning Mao and Xavier Martinet and Todor Mihaylov and Pushkar Mishra and Igor Molybog and Yixin Nie and Andrew Poulton and Jeremy Reizenstein and Rashi Rungta and Kalyan Saladi and Alan Schelten and Ruan Silva and Eric Michael Smith and R. Subramanian and Xia Tan and Binh Tang and Ross Taylor and Adina Williams and Jian Xiang Kuan and Puxin Xu and Zhengxu Yan and Iliyan Zarov and Yuchen Zhang and Angela Fan and Melissa Hall Melanie Kambadur and Sharan Narang and Aur{\'e}lien Rodriguez and Robert Stojnic and Sergey Edunov and Thomas Scialom},
  journal={ArXiv},
  year={2023},
  volume={abs/2307.09288},
  url={https://api.semanticscholar.org/CorpusID:259950998}
}
@article{radford2019language,
  title={Language Models are Unsupervised Multitask Learners},
  author={Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  year={2019},
journal={preprint}
}

@article{Nijkamp2022ProGen2ET,
  title={ProGen2: Exploring the Boundaries of Protein Language Models},
  author={Erik Nijkamp and Jeffrey A. Ruffolo and Eli N. Weinstein and Nikhil Vijay Naik and Ali Madani},
  journal={Cell systems},
  year={2022},
  url={https://api.semanticscholar.org/CorpusID:250089293}
}
@article{Alamdari2024ProteinGW,
  title={Protein generation with evolutionary diffusion: sequence is all you need},
  author={Sarah Alamdari and Nitya Thakkar and Rianne van den Berg and Alex X. Lu and Nicolo Fusi and Ava P. Amini and Kevin Kaichuang Yang},
  journal={bioRxiv},
  year={2024},
  url={https://api.semanticscholar.org/CorpusID:261893498}
}
@misc{zhao2024informedcorrectorsdiscretediffusion,
      title={Informed Correctors for Discrete Diffusion Models}, 
      author={Yixiu Zhao and Jiaxin Shi and Lester Mackey and Scott Linderman},
      year={2024},
      eprint={2407.21243},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2407.21243}, 
}
@misc{zheng2024maskeddiffusionmodelssecretly,
      title={Masked Diffusion Models are Secretly Time-Agnostic Masked Models and Exploit Inaccurate Categorical Sampling}, 
      author={Kaiwen Zheng and Yongxin Chen and Hanzi Mao and Ming-Yu Liu and Jun Zhu and Qinsheng Zhang},
      year={2024},
      eprint={2409.02908},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2409.02908}, 
}
@article{Hie2022EfficientEO,
  title={Efficient evolution of human antibodies from general protein language models and sequence information alone},
  author={Brian L. Hie and Duo Xu and Varun R. Shanker and Theodora U. J. Bruun and Payton A.-B. Weidenbacher and Shaogeng Tang and Peter S. Kim},
  journal={bioRxiv},
  year={2022},
  url={https://api.semanticscholar.org/CorpusID:248151609}
}



@article{Lan2019ALBERTAL,
  title={ALBERT: A Lite BERT for Self-supervised Learning of Language Representations},
  author={Zhenzhong Lan and Mingda Chen and Sebastian Goodman and Kevin Gimpel and Piyush Sharma and Radu Soricut},
  journal={ArXiv},
  year={2019},
  volume={abs/1909.11942},
  url={https://api.semanticscholar.org/CorpusID:202888986}
}
@article{Liu2019RoBERTaAR,
  title={RoBERTa: A Robustly Optimized BERT Pretraining Approach},
  author={Yinhan Liu and Myle Ott and Naman Goyal and Jingfei Du and Mandar Joshi and Danqi Chen and Omer Levy and Mike Lewis and Luke Zettlemoyer and Veselin Stoyanov},
  journal={ArXiv},
  year={2019},
  volume={abs/1907.11692},
  url={https://api.semanticscholar.org/CorpusID:198953378}
}
@inproceedings{Devlin2019BERTPO,
  title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author={Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
  booktitle={North American Chapter of the Association for Computational Linguistics},
  year={2019},
  url={https://api.semanticscholar.org/CorpusID:52967399}
}
@misc{deepseekai2025deepseekr1incentivizingreasoningcapability,
      title={DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning}, 
      author={DeepSeek-AI and Daya Guo and Dejian Yang and Haowei Zhang and Junxiao Song and Ruoyu Zhang and Runxin Xu and Qihao Zhu and Shirong Ma and Peiyi Wang and Xiao Bi and Xiaokang Zhang and Xingkai Yu and Yu Wu and Z. F. Wu and Zhibin Gou and Zhihong Shao and Zhuoshu Li and Ziyi Gao and Aixin Liu and Bing Xue and Bingxuan Wang and Bochao Wu and Bei Feng and Chengda Lu and Chenggang Zhao and Chengqi Deng and Chenyu Zhang and Chong Ruan and Damai Dai and Deli Chen and Dongjie Ji and Erhang Li and Fangyun Lin and Fucong Dai and Fuli Luo and Guangbo Hao and Guanting Chen and Guowei Li and H. Zhang and Han Bao and Hanwei Xu and Haocheng Wang and Honghui Ding and Huajian Xin and Huazuo Gao and Hui Qu and Hui Li and Jianzhong Guo and Jiashi Li and Jiawei Wang and Jingchang Chen and Jingyang Yuan and Junjie Qiu and Junlong Li and J. L. Cai and Jiaqi Ni and Jian Liang and Jin Chen and Kai Dong and Kai Hu and Kaige Gao and Kang Guan and Kexin Huang and Kuai Yu and Lean Wang and Lecong Zhang and Liang Zhao and Litong Wang and Liyue Zhang and Lei Xu and Leyi Xia and Mingchuan Zhang and Minghua Zhang and Minghui Tang and Meng Li and Miaojun Wang and Mingming Li and Ning Tian and Panpan Huang and Peng Zhang and Qiancheng Wang and Qinyu Chen and Qiushi Du and Ruiqi Ge and Ruisong Zhang and Ruizhe Pan and Runji Wang and R. J. Chen and R. L. Jin and Ruyi Chen and Shanghao Lu and Shangyan Zhou and Shanhuang Chen and Shengfeng Ye and Shiyu Wang and Shuiping Yu and Shunfeng Zhou and Shuting Pan and S. S. Li and Shuang Zhou and Shaoqing Wu and Shengfeng Ye and Tao Yun and Tian Pei and Tianyu Sun and T. Wang and Wangding Zeng and Wanjia Zhao and Wen Liu and Wenfeng Liang and Wenjun Gao and Wenqin Yu and Wentao Zhang and W. L. Xiao and Wei An and Xiaodong Liu and Xiaohan Wang and Xiaokang Chen and Xiaotao Nie and Xin Cheng and Xin Liu and Xin Xie and Xingchao Liu and Xinyu Yang and Xinyuan Li and Xuecheng Su and Xuheng Lin and X. Q. Li and Xiangyue Jin and Xiaojin Shen and Xiaosha Chen and Xiaowen Sun and Xiaoxiang Wang and Xinnan Song and Xinyi Zhou and Xianzu Wang and Xinxia Shan and Y. K. Li and Y. Q. Wang and Y. X. Wei and Yang Zhang and Yanhong Xu and Yao Li and Yao Zhao and Yaofeng Sun and Yaohui Wang and Yi Yu and Yichao Zhang and Yifan Shi and Yiliang Xiong and Ying He and Yishi Piao and Yisong Wang and Yixuan Tan and Yiyang Ma and Yiyuan Liu and Yongqiang Guo and Yuan Ou and Yuduan Wang and Yue Gong and Yuheng Zou and Yujia He and Yunfan Xiong and Yuxiang Luo and Yuxiang You and Yuxuan Liu and Yuyang Zhou and Y. X. Zhu and Yanhong Xu and Yanping Huang and Yaohui Li and Yi Zheng and Yuchen Zhu and Yunxian Ma and Ying Tang and Yukun Zha and Yuting Yan and Z. Z. Ren and Zehui Ren and Zhangli Sha and Zhe Fu and Zhean Xu and Zhenda Xie and Zhengyan Zhang and Zhewen Hao and Zhicheng Ma and Zhigang Yan and Zhiyu Wu and Zihui Gu and Zijia Zhu and Zijun Liu and Zilin Li and Ziwei Xie and Ziyang Song and Zizheng Pan and Zhen Huang and Zhipeng Xu and Zhongyu Zhang and Zhen Zhang},
      year={2025},
      eprint={2501.12948},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2501.12948}, 
}
@misc{ren2024,
      title={How Discrete and Continuous Diffusion Meet: Comprehensive Analysis of Discrete Diffusion Models via a Stochastic Integral Framework}, 
      author={Yinuo Ren and Haoxuan Chen and Grant M. Rotskoff and Lexing Ying},
      year={2024},
      eprint={2410.03601},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2410.03601}, 
}
@book{jacod2013,
  title     = {Limit theorems for stochastic processes},
  author    = {Jacod, Jean and Shiryaev, Albert},
  volume    = {288},
  year      = {2013},
  publisher = {Springer Science \& Business Media}
}
@article{Sun2022,
  title={Score-based Continuous-time Discrete Diffusion Models},
  author={Haoran Sun and Lijun Yu and Bo Dai and Dale Schuurmans and Hanjun Dai},
  journal={ArXiv},
  year={2022},
  volume={abs/2211.16750},
  url={https://api.semanticscholar.org/CorpusID:254096040}
}
@misc{gat2024discreteflowmatching,
      title={Discrete Flow Matching}, 
      author={Itai Gat and Tal Remez and Neta Shaul and Felix Kreuk and Ricky T. Q. Chen and Gabriel Synnaeve and Yossi Adi and Yaron Lipman},
      year={2024},
      eprint={2407.15595},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2407.15595}, 
}
@article{Austin2021StructuredDD,
  author       = {Jacob Austin and
                  Daniel D. Johnson and
                  Jonathan Ho and
                  Daniel Tarlow and
                  Rianne van den Berg},
  title        = {Structured Denoising Diffusion Models in Discrete State-Spaces},
  journal      = {CoRR},
  volume       = {abs/2107.03006},
  year         = {2021},
  url          = {https://arxiv.org/abs/2107.03006},
  eprinttype    = {arXiv},
  eprint       = {2107.03006},
  timestamp    = {Mon, 25 Oct 2021 07:55:50 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2107-03006.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@misc{campbell2022continuoustimeframeworkdiscrete,
      title={A Continuous Time Framework for Discrete Denoising Models}, 
      author={Andrew Campbell and Joe Benton and Valentin De Bortoli and Tom Rainforth and George Deligiannidis and Arnaud Doucet},
      year={2022},
      eprint={2205.14987},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/2205.14987}, 
}

@book{yin_continuous-time_2013,
	address = {New York, NY},
	series = {Stochastic {Modelling} and {Applied} {Probability}},
	title = {Continuous-{Time} {Markov} {Chains} and {Applications}},
	volume = {37},
	copyright = {http://www.springer.com/tdm},
	isbn = {978-1-4614-4345-2 978-1-4614-4346-9},
	url = {http://link.springer.com/10.1007/978-1-4614-4346-9},
	urldate = {2025-01-29},
	publisher = {Springer},
	author = {Yin, G. George and Zhang, Qing},
	year = {2013},
	doi = {10.1007/978-1-4614-4346-9},
	keywords = {continuous-time systems, Markov chains, probability vectors, stochastic processes},
	file = {Full Text PDF:C\:\\Users\\bezem\\Zotero\\storage\\Q6NBAJ2I\\Yin and Zhang - 2013 - Continuous-Time Markov Chains and Applications.pdf:application/pdf},
}
@misc{schiff2024simpleguidancemechanismsdiscrete,
      title={Simple Guidance Mechanisms for Discrete Diffusion Models}, 
      author={Yair Schiff and Subham Sekhar Sahoo and Hao Phung and Guanghan Wang and Sam Boshar and Hugo Dalla-torre and Bernardo P. de Almeida and Alexander Rush and Thomas Pierrot and Volodymyr Kuleshov},
      year={2024},
      eprint={2412.10193},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2412.10193}, 
}

@article{Benton2024,
    author = {Benton, Joe and Shi, Yuyang and De Bortoli, Valentin and Deligiannidis, George and Doucet, Arnaud},
    title = {From denoising diffusions to denoising Markov models},
    journal = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
    volume = {86},
    number = {2},
    pages = {286-301},
    year = {2024},
    month = {01},
    abstract = {Denoising diffusions are state-of-the-art generative models exhibiting remarkable empirical performance. They work by diffusing the data distribution into a Gaussian distribution and then learning to reverse this noising process to obtain synthetic datapoints. The denoising diffusion relies on approximations of the logarithmic derivatives of the noised data densities using score matching. Such models can also be used to perform approximate posterior simulation when one can only sample from the prior and likelihood. We propose a unifying framework generalizing this approach to a wide class of spaces and leading to an original extension of score matching. We illustrate the resulting models on various applications.},
    issn = {1369-7412},
    doi = {10.1093/jrsssb/qkae005},
    url = {https://doi.org/10.1093/jrsssb/qkae005},
    eprint = {https://academic.oup.com/jrsssb/article-pdf/86/2/286/57219053/qkae005.pdf},
}
@misc{ou2024,
      title={Your Absorbing Discrete Diffusion Secretly Models the Conditional Distributions of Clean Data}, 
      author={Jingyang Ou and Shen Nie and Kaiwen Xue and Fengqi Zhu and Jiacheng Sun and Zhenguo Li and Chongxuan Li},
      year={2024},
      eprint={2406.03736},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2406.03736}, 
}

@inproceedings{UriaML14,
  author    = {Benigno Uria and
               Iain Murray and
               Hugo Larochelle},
  title     = {A Deep and Tractable Density Estimator},
  booktitle = {Proceedings of the 31th International Conference on Machine Learning},
  year      = {2014}
}

@inproceedings{HoogeboomARDM22,
  author    = {Emiel Hoogeboom and
               Alexey A. Gritsenko and
               Jasmijn Bastings and
               Ben Poole and
               Rianne van den Berg and
               Tim Salimans},
  title     = {Autoregressive Diffusion Models},
  booktitle = {10th International Conference on Learning Representations},
  year      = {2022},
}


@book{budhiraja_analysis_2019,
	address = {New York, NY},
	series = {Probability {Theory} and {Stochastic} {Modelling}},
	title = {Analysis and {Approximation} of {Rare} {Events}: {Representations} and {Weak} {Convergence} {Methods}},
	volume = {94},
	copyright = {http://www.springer.com/tdm},
	isbn = {978-1-4939-9577-6 978-1-4939-9579-0},
	shorttitle = {Analysis and {Approximation} of {Rare} {Events}},
	url = {http://link.springer.com/10.1007/978-1-4939-9579-0},
	language = {en},
	urldate = {2025-01-29},
	publisher = {Springer US},
	author = {Budhiraja, Amarjit and Dupuis, Paul},
	year = {2019},
	doi = {10.1007/978-1-4939-9579-0},
	keywords = {Discrete time processes, large deviation, large deviation principle, moderate deviation, Monte Carlo Approximation, Rare events, relative entropy, representation formulas, stochastic analysis, weak convergence, weak convergence methods},
	file = {Full Text PDF:C\:\\Users\\bezem\\Zotero\\storage\\CETYP8EG\\Budhiraja and Dupuis - 2019 - Analysis and Approximation of Rare Events Represe.pdf:application/pdf},
}
@misc{shih2022traininginferenceanyorderautoregressive,
      title={Training and Inference on Any-Order Autoregressive Models the Right Way}, 
      author={Andy Shih and Dorsa Sadigh and Stefano Ermon},
      year={2022},
      eprint={2205.13554},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2205.13554}, 
}

@misc{li2021discoveringnonmonotonicautoregressiveorderings,
      title={Discovering Non-monotonic Autoregressive Orderings with Variational Inference}, 
      author={Xuanlin Li and Brandon Trabucco and Dong Huk Park and Michael Luo and Sheng Shen and Trevor Darrell and Yang Gao},
      year={2021},
      eprint={2110.15797},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2110.15797}, 
}

@article{GILLESPIE1976403,
title = {A general method for numerically simulating the stochastic time evolution of coupled chemical reactions},
journal = {Journal of Computational Physics},
volume = {22},
number = {4},
pages = {403-434},
year = {1976},
issn = {0021-9991},
doi = {https://doi.org/10.1016/0021-9991(76)90041-3},
url = {https://www.sciencedirect.com/science/article/pii/0021999176900413},
author = {Daniel T Gillespie},
}

@article{gillespie_exact_1977,
	title = {Exact stochastic simulation of coupled chemical reactions},
	volume = {81},
	issn = {0022-3654},
	url = {https://doi.org/10.1021/j100540a008},
	doi = {10.1021/j100540a008},
	number = {25},
	urldate = {2025-01-29},
	journal = {The Journal of Physical Chemistry},
	author = {Daniel T. Gillespie},
	month = {dec},
	year = {1977},
	note = {Publisher: American Chemical Society},
	pages = {2340--2361},
}







































