In this paper, we propose stochastic remasking,
a sampling algorithm that unmasks tokens based on the model's confidence and controllably remasks previously unmasked tokens. In each step, stochastic sampling will score all positions based on the prediction confidence, first set the lowest K percent low-score tokens to mask, and select the rest of the mask tokens to unmask, where K is determined by the sampling schedule.
Using the model's confidence can ground the sampling trajectory to align with the learned conditional distribution, leading to coherent sequence generation; and Remasking enables the model to correct prior mispredictions by remasking previously unmasked positions. One concern is using the prediction confidence of unmasked tokens for score ranking, which is not supervised in the training. In section xxx, We ablate and show the logits of the unmasked positions are untrained it still offer informative signals representing the model confidence owing to the representation and shared architectures. However, it is uncalibrated. Thus we introduce a coefficient stochasticity strength $\eta$ to recalibrate the confidence ranking between masked and unmasked positions.
We show that existing sampling algorithms including ancestral~\citep{md4, mdlm}, greedy ancestral~\citep{gong2024scalingdiffusionlanguagemodels,Chang_2022_CVPR}, DFM sampling~\citep{DFM}, RDM sampling~\citep{RDM,DPLM,DPLM2}, and planner-based sampling~\citep{DDPD} are instances of stochastic remasking with specific stochasticity strengths and scores as shown in Table . In planner-based sampling, we demonstrate that a dedicated, pretrained planner as in DDPD~\citep{DDPD} is unnecessary; stochastic remasking inexplicitly uses itself as a planner because of the score ranking. In addition, we show that a lightweight BERT trained with masked language modeling can function effectively as a planner. Remarkably, we show that an 8M-parameter ESM2~\citep{esm2} achieves comparable results to a 3B-parameter variant in guiding a 150M-parameter MDM, where an 8M planner introduces negligible overhead in practice. We also derive the ELBO for BERT-guided sampling under the planner-denoiser framework, justifying its theoretical guarantee. 

Stochastic remasking is a drop-in replacement of existing mask diffusion model sampling. We validate its wide applicability in three modalities of tasks, including language (math reasoning), protein, and RNA sequence generation. In math reasoning, stochastic remasking equipped with a 1B MDM outperforms the 7B Liamma. In protein sequence generation, we improve the leading method DPLM with our stochastic remasking, achieving state-of-the-art generation quality, where 50\% of sequences length ranging from 200 to 800 have high structural plausibility (pLDDT > 80, pTM > 0.8, pAE < 10), we term the metric as foldability. In RNA sequence generation, we outperform the autoregressive counterpart with achieve state-of-the-art structural plausibility, exceeding the natural sequences. 
%Finally, to bridge the algorithm with practical utility, we experimentally synthesize and express 10 de novo generated protein (200 residues each), and show a functional expression. These results affirm the broader impacts of MDMs in scientific domains such as protein design and therapeutic development.
