\vspace{-0.1in}
\section{Introduction}
\vspace{-0.1pt}


Recent decades have witnessed many impressive advancements in computer vision, with state-of-the-art systems capable of high-accuracy object detection, segmentation, classification, and more. One of the main applications of these advances is in robotic manipulation and perception, where autonomous robots are expected to find and retrieve objects and navigate through complex environments spanning warehouses, manufacturing plants, and smart homes.


However, relying primarily on optical perception (e.g., cameras, LiDARs, etc) inherently limits existing systems to line-of-sight (LOS).\footnote{While some visible-light systems can rely on reflected laser light to image objects around the corner, they cannot image fully occluded items (e.g., within a closed box).} For example, a camera-based system cannot see inside a closed box to confirm e-commerce orders are correct or detect broken objects during shipping. Nor is it feasible for camera-based robots to efficiently plan complex, multi-step tasks in practical environments where required objects may be fully occluded. 

Instead, it is possible to produce non-line-of-sight (NLOS) images using millimeter-wave (mmWave) radars~\cite{ti_iwr1443}. Unlike visible light, mmWave wireless signals can traverse everyday occlusions such as cardboard, fabric, and plastic. This allows mmWave radars to image objects in non-line-of-sight, similar to how airport security scanners produce high-resolution NLOS images of passengers to detect hidden weapons. With the emergence of low-cost mmWave radars, it is possible to build pervasive autonomous systems with this NLOS imaging capability. For example, using mmWave radars attached to a robotic arm (Fig.~\ref{fig:intro}), we can produce a NLOS mmWave image of a wrench within a closed box.



\begin{figure}
\centering
    \includegraphics[width=0.45\textwidth]{figures/intro.pdf}
    \caption{\footnotesize{\textbf{\name.}} \textnormal{We use a robotic arm to move mmWave radars throughout the environment. While an RGB-D camera's output cannot see inside the box, we produce high-resolution non-line-of-sight mmWave images. } }
    \label{fig:intro}
    \vspace{-0.2in}
\end{figure}


However, to achieve the many possible applications of NLOS imaging, we need to be able to \textit{perceive} (e.g., segment, classify, etc) objects within these images. Unfortunately, these images are fundamentally different from visible-light images in multiple ways. First, they suffer from radar-specific artifacts; for example, the much longer wavelength of millimeter-waves versus visible light (hundreds of nanometers) results in fundamentally different specular and diffuse properties of imaged objects. Second, mmWave images have no color information. Third, they are complex-valued.\footnote{Interestingly, the values are dependent on not only the location of the object, but also the material and texture.} Therefore, since the majority of existing perception algorithms have not been trained on these mmWave images, 
they need to be adapted for NLOS perception; in some cases, we may require entirely new algorithms or models to enable mmWave-based perception to approach the accuracy of visual-light-based perception (as we show in this paper). 

Inspired by how early imaging datasets, such as MSRC~\cite{msrc_dataset}, Pascal~\cite{pascal_dataset}, Berkeley 3D~\cite{berkely_3d_dataset}, and YCB~\cite{ycb}, paved the way for many advancements within computer vision, we believe that a dataset of mmWave images of everyday objects is fundamental for enabling pervasive NLOS perception. To summarize, we make the following key contributions: 

\begin{itemize}

    \item \textbf{Dataset.} We create MITO, a novel dataset of mmWave images for everyday objects, using a custom-built robotic imaging system and signal processing pipeline. We capture over 24 million mmWave frames, and use them to generate 550 images of 76 objects from the YCB dataset (out of a total of 80 objects) in both line-of-sight and non-line-of-sight. For each object, we capture mmWave images at multiple different frequencies. We also provide RGB-D images of the objects (when in LOS) and ground truth object segmentations. 
    \item \textbf{Simulator.} We develop an open-source simulator that can produce synthetic mmWave images for any 3D triangle mesh. Since different materials interact differently with mmWave signals and, therefore, produce different images, we build two different modeling methods into our simulation, the combination of which can be used to simulate a wide range of material properties. 
    \item \textbf{Non-line-of-sight Applications. }Using \name, we establish baselines for two computer vision tasks {in non-line-of-sight}. First, we show object segmentation, using the segment-anything model (SAM)~\cite{sam} and a mmWave power-based prompter. Second, we show shape classification, which is trained entirely on synthetic images and can classify real-world images.
\end{itemize}

The dataset, code, and a video demonstration are available here: 

\href{https://github.com/signalkinetics/MITO\_Codebase}{https://github.com/signalkinetics/MITO\_Codebase}

% A video demonstration is available here: 
% \href{https://www.youtube.com/watch?v=ciWm7eSuLLg}{https://www.youtube.com/watch?v=ciWm7eSuLLg}

