\vspace{-0.1pt}
\section{\name\ Dataset}
\label{sec:dataset}
\vspace{-0.03in}




In this section, we describe the design choices and process of creating \name. The dataset consists of over 24 million mmWave frames (where each frame is a complex-valued time series). We used these frames to generate 550 unique, high-resolution 3D mmWave SAR images of 76 diverse objects.


\subsection{Object Selection}
\vspace{-0.03in}

\name\ builds on the YCB dataset~\cite{ycb}, a standard dataset of everyday objects used for robotic manipulation. We select 76 out of 80 objects from the YCB dataset (the remaining 4 were discarded due to not being able to obtain the exact object). Unlike previous mmWave datasets, these objects cover a diverse range of shapes, sizes, materials, and categories.\footnote{Additional details on object diversity  are provided in  supplementary.} The YCB dataset also provides 2D images and 3D object meshes, which we use to simulate mmWave images. 

\begin{figure*}
\centering
\vspace{-0.1in}
    \includegraphics[width=0.75\textwidth]{new_figures/all_new.pdf}
    \vspace{-0.125in}
    \caption{\footnotesize{\textbf{Sample Images.}} \textnormal{A number of example mmWave images within \name.}  }
    \label{fig:all_img}
    \vspace{-0.25in}
\end{figure*}


\vspace{-0.03in}
\subsection{Data Collection Protocol}
\vspace{-0.03in}

\subsubsection{Real-World mmWave Images}
\vspace{-0.03in}

Fig.~\ref{fig:intro} visualizes our data collection setup. We connect a 77~GHz radar (TI IWR1443Boost~\cite{ti_iwr1443} and DCA1000EVM~\cite{dca1000evm}), 24~GHz radar (Infineon Position2Go~\cite{position2go}), and Intel Realsense D415~\cite{realsense} to an end-effector of a UR5e robotic arm~\cite{UR5} using custom-designed 3D-printed parts. Since the robot and radars are controlled from different computers, we synchronize them by setting both computers to the same NTP time server.

We place the objects in two setups that minimize background reflections in different ways. In the first, we fix each object onto a tripod by hot-gluing a 3D-printed nut to each object and screwing it onto a threaded tripod. Because there is no surface directly below the object, any strong reflections from the background are avoided. In the second, we place the object directly on a styrofoam block, since styrofoam is largely transparent to mmWave signals~\cite{ti_styrofoam}. We also include experiments where the tripod/styrofoam is placed in the scene without any objects so that the impact of the tripod/styrofoam can be removed by subtracting the empty image from the main image~\cite{RFCapture}. 

At the start of an experiment, the robot moves the camera to the middle of the workspace to take an RGB-D image prior to any potential occlusion (i.e., in the NLOS scenario). Then, it moves the radars throughout the workspace, covering a rectangular space. The aperture of our experiments range from 55cm by 22cm to 60cm by 45cm, ensuring that each experiment uses an aperture which covers the full object. The radars continuously measure mmWave signals while the robot is moving, and the robot records its trajectory over time. After we complete an experiment in LOS, we run an experiment in NLOS without moving the object. We place a layer of cardboard over the object to simulate the case where the object was placed inside a box. We also collect experiments with four layers of fabric over the cardboard to simulate more dense occlusions.


\subsubsection{Synthetic mmWave Images}
In addition to the real-world data, we also provide a simulation tool to generate synthetic mmWave images, which can be used to increase the volume of data to aid in model training. We include at least 2 different simulation images for each object, along with the simulator itself for producing additional images (see Sec.~\ref{sec:simulation} for more details). 


\vspace{-0.1pt}
\subsection{Data Processing}
\vspace{-0.1pt}


To process a real-world mmWave image, we need to know the location where each measurement was taken. To do this, we interpolate the timestamped robot locations to match the timestamp of the radar measurements. The interpolated locations, in addition to the raw radar measurements, are then processed into a 3D mmWave image by applying Eq.~\ref{eq:sar}. Example images can be seen in Fig.~\ref{fig:all_img}, which shows the segmented RGB images (1st column), 77~GHz LOS (2nd column), 77~GHz NLOS (3rd column), 24~GHz LOS (4th column), and 24~GHz NLOS (5th column) mmWave images. Additional images are provided in the supplementary.



We produce 550 3D, complex-valued mmWave images at two different frequencies (24~GHz \& 77~GHz) in both LOS and NLOS settings using 24 million raw measurements and locations.  We include the raw data in \name~ to enable flexibility in leveraging and/or postprocessing the data using a custom pipeline. We also include images of objects at different angles and images with multiple objects.




For each mmWave image, we use its corresponding RGB-D image (in LOS) to provide a ground truth object segmentation mask. The segmentation masks are produced by using an interactive interface for the segment-anything model (SAM)~\cite{sam} to select the points on the RGB picture as a prompt for the model. The human annotator continues selecting points until the mask covers the full object. We then use the camera's location to transform and align the RGB-D ground-truth mask with the mmWave image. More details on the alignment process are in the supplementary.



\vspace{-0.1pt}
\subsection{Analysis of Different Frequencies}
\vspace{-0.1pt}

Recall from Sec.~\ref{sec:bg_freq} that the frequency of the mmWave signal impacts both the resolution of the resulting image and the ability for the signal to traverse through occlusions. 
We analyze this phenomenon in real-world images in \name\ in Fig.~\ref{fig:multispectral}. Fig.\ref{fig:multispectral}a-b show a 77~GHz and 24~GHz image of a padlock in LOS, respectively. The 77~GHz image is higher resolution, as expected. On the other hand, Fig.~\ref{fig:multispectral}c-d show 77~GHz and 24~GHz images of the same object in NLOS (placed underneath cardboard). In this case, the 24~GHz image is very similar to the LOS case, while the 77~GHz shows some artifacts that were not present in LOS. This is because the 24~GHz signal travels through the cardboard relatively unaffected, while the 77~GHz is partially reflected and absorbed. We hope that by including two different frequencies, future algorithms can leverage both images to benefit from high-resolution and robustness to occlusions.

\begin{figure}
\centering
    \includegraphics[width=0.8\linewidth]{figures/los_nlos_new2.pdf}
    \vspace{-0.07in}
    \caption{\footnotesize{\textbf{Multi-Spectral Imaging.}} \textnormal{a-b) show 77~GHz \& 24~GHz images for a padlock in LOS. c-d) show the images in NLOS. } }
    \label{fig:multispectral}
    \vspace{-0.25in}
\end{figure}



\vspace{-0.12pt}
\section{\name\ Simulator}\label{sec:simulation}
\vspace{-0.1pt}


In this section, we describe our open-source 
mmWave image simulator, and evaluate the simulator's accuracy. 

\vspace{-0.1pt}
\subsection{mmWave Simulation}
\vspace{-0.12pt}

Our simulator takes as input a 3D triangle mesh and radar locations to simulate mmWave signals from. The mesh files can be sourced from object datasets (e.g., YCB~\cite{ycb}), online repositories (e.g., 3D Warehouse, Turbo Squid), or custom-generated on smartphones (e.g., PolyCam iPhone app).

At a high level, our simulator works by estimating the raw signals expected at each of these locations, given reflections off the target object. Then, synthetic images are produced by feeding these simulated signals into the same image processing pipeline used for real world data (Eq.~\ref{eq:sar}).


\begin{figure*}
\begin{minipage}[t]{0.73\linewidth}
\centering
    \includegraphics[width=1\textwidth]{figures/sim2.pdf}
    \vspace{-0.265in}
    \caption{\footnotesize{\textbf{Simulation Output.}} \textnormal{Our simulation uses a a) 3D mesh to produce images assuming different reflection types: b) specular \& c) edge. d/e) We combine these with different weights to simulate different materials.}}
    \label{fig:sim}
    \vspace{-0.2in}
\end{minipage}
\hfill
\begin{minipage}[t]{0.26\linewidth}
\label{table:sim}
\vspace{-0.6in}
\footnotesize
\label{table:sim}
\begin{tabular}{ |c|c|c|c| }
 \hline \textbf{Simulation} &\textbf{25\textsuperscript{th}} &\textbf{50\textsuperscript{th}} &\textbf{75\textsuperscript{th}}     \\  
    \Xhline{3.5\arrayrulewidth} %\hline \hline
 Combined   &  \textbf{85\%} & \textbf{94\%} & \textbf{98\%}  \\  \Xhline{3.5\arrayrulewidth} %\hline \hline
 Specular & 68\% & 90\% & 97\%\\ \hline
    Edge  & 49\% & 71\%  & 88\%\\\hline
\end{tabular}
\vspace{-0.08in}
\captionof{table}{\footnotesize{\textbf{Simulation 3D F-Score} \textnormal{for combined, specular, \& edge simulations}}}
\vspace{-0.2in}
\label{table:sim}
\end{minipage}
\end{figure*}


Estimating raw signals at a given location is as follows:
\begin{enumerate}
    \item First, we find which mesh vertices in the mesh are visible from that location~\cite{mesh_visibility}.
    \item Next, for a given visible vertex, we measure the distance to the measurement location. Given the distance, we simulate the reflection from this vertex: 

    
    \vspace{-0.1pt}
    { \eqsize
    \begin{equation} \label{eq:single_sim}
        s_j(l,v) = e^{-j \frac{4 \pi |l-v|}{\lambda_j}}
    \end{equation}
    }
    
    \noindent where $s_j(l,v)$ is the j\textsuperscript{th} sample of the reflection from  location $l$ to vertex $v$ and back. 
    \item Finally, we sum the simulated reflections from all visible vertices to produce the raw signal for this measurement location. Formally: 

    
    \vspace{-0.1pt}
    { \eqsize
    \begin{equation} 
    \label{eq:sum_sim}
        S_j = \sum_{v\in V} s_j(l,v)
    \end{equation}
    }
    
    \noindent where $S_j$ is the j\textsuperscript{th} sample of the simulated signal from measurement location $l$, V is the set of visible vertices.
\end{enumerate}

\noindent After repeating this for each measurement location, we feed the raw signals through our imaging processing pipeline to produce the final synthetic image. Note that we can rotate the mesh files to produce synthetic images at various angles.


Recall from Sec.~\ref{sec:bg_refl} that the object's material and texture may result in different types of reflections. To accurately model different objects, we introduce 2 variations of the simulation to account for different types of reflections. 


\vspace{-0.1pt}
\subsubsection{Specular Reflections}
\vspace{-0.1pt}

When an object exhibits purely specular reflections, the signal will reflect off each vertex in only one direction. In some cases, the signal is reflected away from the radar's receivers, and therefore should not be added to $S_j$. Specifically, the signal will only be reflected back towards the radar if the surface normal is pointing towards the radar's location. Otherwise, the signal will be reflected away from the receiver.

We update our above simulation to account for this by removing reflections where the angle between the normal ($\boldsymbol{n}$) and the vector from the radar to the vertex ($l-v$) is less than a threshold $\tau$. Formally, we can update Eq.~\ref{eq:single_sim} to:


\vspace{-0.1pt}
{ \eqsize
\begin{equation}
    s_j(l,v) =  \begin{cases} 
      0 & \cos^{-1}(\frac{\boldsymbol{n} \cdot (l-v)}{|\boldsymbol{n}| |(l-v)|}) \geq \tau \\
      e^{-j \frac{4 \pi |l-v|}{\lambda_j}} & \cos^{-1}(\frac{\boldsymbol{n} \cdot (l-v)}{||\boldsymbol{n}| |(l-v)|}) < \tau
   \end{cases}
\end{equation}}

\noindent Fig.~\ref{fig:sim}b shows an example when simulating entirely based on specular reflections.

\vspace{-0.1pt}
\subsubsection{Edge Reflections}
\vspace{-0.1pt}

In other cases, the signal experiences scattering effects from the edges of objects.
First, we define a vertex as being on an edge of an object when the angle between two of its faces is greater than an angle $\tau_e$, and we find the set of all vertices that lie on an edge, $V_e$. 

Then, we replace Eq.~\ref{eq:single_sim} such that it only allows reflections from edge vertices.


\vspace{-0.1pt}
{ \eqsize
\begin{equation}
    s_j(l,v) =  \begin{cases} 
      0 & v \notin V_e \\
      e^{-j \frac{4 \pi |l-v|}{\lambda_j}} & v \in V_e
   \end{cases}
\end{equation}}

\noindent Fig.~\ref{fig:sim}c shows an example when simulating edge reflections.

\vspace{-0.1pt}
\subsubsection{Combing Reflections}
\vspace{-0.1pt}
In practice, materials rarely exhibit only one type of reflection, but instead some combination of the two. To account for this, our synthetic images can be added together with different weights to simulate different materials:


\vspace{-0.1pt}
{ \eqsize
\begin{equation}
    I_{syn}(\alpha_1, \alpha_2) = \frac{\alpha_1}{\alpha_1 + \alpha_2} I_s + \frac{\alpha_2}{\alpha_1 + \alpha_2} I_e
\end{equation}}

\noindent where $I_{syn}(\alpha_1, \alpha_2)$ is the combined synthetic image for weights $\alpha_1, \alpha_2$, and $I_s$ and $I_e$ are the images from the specular and edge simulations, respectively.
Fig.~\ref{fig:sim}d-e shows two examples of combining simulations with different weights representing two different materials. 



\vspace{-0.5pt}
\subsection{Simulation Accuracy}
\vspace{-0.1pt}
To evaluate the accuracy of our simulator, we use the standard 3D F-score metric~\cite{3d_fscore} to compare the mmWave outputs from real-world and simulation. 

\vspace{-0.1pt}
\subsubsection{Aligning the Point Clouds}
\vspace{-0.1pt}

Before we compute the 3D F-score, we first need to convert both the real-world and simulation images to a point cloud. To do so, we apply a standard mmWave point cloud generation procedure\cite{LauraEPFL} by
creating one point in the center of each mmWave voxel that has a power above a threshold $\tau_P$: 

    \vspace{-0.1in}
    {\eqsize
    \begin{equation}
        P_R = \{ p\ |\  |I(p)| > \tau_P \} \hspace{0.3in} P_S = \{ p\ |\  |I_{syn}(p)| > \tau_P \} \notag
    \end{equation}
    }

    \noindent where $P_R$ and $P_S$ are the set of points created for the real and synthetic mmWave images, respectively. 

Next, we align the real-world and simulation point clouds using a standard iterative closest point implementation~\cite{icp}. This is necessary because our mmWave simulation places the object in the center of the image, while our real world experiments may have objects offset from the center. 
    

\vspace{-0.1pt}
\subsubsection{3D F-Score}
\vspace{-0.1pt}

We evaluate the point cloud similarity using a standard metric, 3D F-score~\cite{3d_fscore}, defined as:
    
\vspace{-0.05in}
{\eqsize
\begin{equation}
\begin{aligned}
    PR &= \frac{1}{N_R} \sum_{i=1}^{N_R} \mathds{1}_{d(P_{R}(i),P_{S})<\tau_F}
\end{aligned}
\end{equation}
}
\vspace{-0.05in}
{\eqsize
\begin{equation}
\begin{aligned}
    RE &= \frac{1}{N_{S}} \sum_{j=1}^{N_{S}} \mathds{1}_{d(P_S(j),P_{R})<\tau_F}, \hspace{0.1in}
    F = \frac{2\ PR \ RE}{PR+RE}  
\end{aligned}
\end{equation}
}
\vspace{-0.05in}

\noindent where $PR$, $RE$, and $F$ are the precision, recall, and F-score. $\mathds{1}$ is an indicator variable. $N_R$ ($N_S$) is the number of points in the real (synthetic) point cloud. ${\small \tau_F}$ is the F-score distance threshold. $d(x, P)$ is the distance from point x to its nearest neighbor in cloud P: ${\footnotesize d(x, P) = \min\limits_{x'\in P} ||x-x'||}$.  

    
    Since our goal is not to select a single set of weights for each object, but to allow the simulation to represent many different objects, we compute this metric across a range of weights $\{ \alpha_1, \alpha_2\}$ and choose the combined simulation which produces the best F-score (e.g., the simulation which best matches the properties of this real-world object). 
    
    \vspace{-0.1pt}
    {
    \eqsize
    \begin{equation}
        F = \max_{(\alpha_1, \alpha_2) \in W} F(I_{syn}(\alpha_1, \alpha_2), I)
        \label{eq:metric}
    \vspace{-0.1pt}
    \end{equation}
    }

    \noindent where $F(I_{syn}, I)$ is the F-score for a synthetic image $I_{syn}$ and real-world image $I$, and $W$ is the set of all weights.
    

\vspace{-0.1pt}
\subsubsection{Results}
\vspace{-0.1pt}



\begin{figure*}
\begin{minipage}[t]{0.3\linewidth}
\centering
    \includegraphics[width=0.9\linewidth]{figures/seg4.pdf}
    \vspace{-0.1in}
    \caption{\footnotesize{\textbf{Segmentation}} \textnormal{a)Colorize 2D image b)Select prompt points c)Segment with SAM}}
    \label{fig:segmentation}
    \vspace{-0.165in}
    % \vspace{-0.3in}
\end{minipage}
\hfill
\begin{minipage}[t]{0.69\linewidth}
\centering
\vspace{-0.97in}
\footnotesize
\label{table:seg}
\setlength{\tabcolsep}{4pt}
\begin{tabular}[t]{ |c V{3} c|c|c|c|c 
V{3} c|c|c|c|c| }
 \hline  &\multicolumn{5}{c V{3}}{\textbf{LOS}} & \multicolumn{5}{c|}{\textbf{NLOS}}      \\   \cline{2-11}
  \textbf{Input} &\textbf{Pre.} &\textbf{Recall} &\textbf{F1} & \textbf{IoU} & \textbf{Acc.} &\textbf{Pre.}&\textbf{Recall} &\textbf{F1}  & \textbf{IoU} & \textbf{Acc.}    \\  
    \Xhline{3.5\arrayrulewidth} %\hline \hline
Camera   & \cellcolor{Green!50}\textbf{99.7\%}  & \cellcolor{Green!50}\textbf{99.5\%} & \cellcolor{Green!50}\textbf{99.4\%} & \cellcolor{Green!50}\textbf{98.9\%} & \cellcolor{Green!50}\textbf{99.9\%} & 
                           \cellcolor{Red!50}3.6\% & \cellcolor{Green!50}100\%* & \cellcolor{Red!50}7.0\%   & \cellcolor{Red!50}3.6\%  & \cellcolor{Red!50}14.0\%   \\  \hline %hline \hline
24~GHz    & \cellcolor{BurntOrange!25}57.9\% & \cellcolor{BurntOrange!25}50.1\% &\cellcolor{BurntOrange!25}37.3\%  &\cellcolor{BurntOrange!25}22.9\% &\cellcolor{Green!50}93.7\% & 
            \cellcolor{BurntOrange!25}53.0\% & \cellcolor{BurntOrange!25}38.7\% & \cellcolor{BurntOrange!25}29.1\% &\cellcolor{BurntOrange!25}17.0\% &\cellcolor{Green!50}92.1\%\\ \hline
77~GHz    & \cellcolor{Green!50}98.2\% & \cellcolor{BurntOrange!25}56.4\%  &\cellcolor{Green!25}71.4\%  & \cellcolor{BurntOrange!25}55.5\% & \cellcolor{Green!50}97.9\% & 
            \cellcolor{Green!50}95.4\% & \cellcolor{BurntOrange!25}57.7\% & \cellcolor{Green!25}70.2\% & \cellcolor{BurntOrange!25}54.1\% & \cellcolor{Green!50}\textbf{97.7\%} \\\hline
Multi     & \cellcolor{Green!50}     & \cellcolor{Green!25}     & \cellcolor{Green!25}  & \cellcolor{BurntOrange!25} & \cellcolor{Green!50}    
            & \cellcolor{Green!50}              & \cellcolor{Green!25}& \cellcolor{Green!25} & \cellcolor{BurntOrange!25} & \cellcolor{Green!50} \\ 
Spectral  & \cellcolor{Green!50}97.1\% & \cellcolor{Green!25}62.6\% & \cellcolor{Green!25}74.4\%& \cellcolor{BurntOrange!25}59.3\% & \cellcolor{Green!50}98.0\% 
            & \cellcolor{Green!50}\textbf{95.1\%} & \cellcolor{Green!25}\textbf{63.1\%} & \cellcolor{Green!25}\textbf{70.8\%} & \cellcolor{BurntOrange!25}\textbf{54.9\%} & \cellcolor{Green!50}\textbf{97.6\%} \\\hline
\end{tabular}
    \vspace{-0.1in}
\captionof{table}{\footnotesize{\textbf{Segmentation Accuracy.} \textnormal{Precision (Pre), Recall, F1 Score, IoU, and Accuracy (Acc) when segmenting camera, 24~GHz, 77~GHz, and multi-spectral images, in both LOS and NLOS scenarios.}} }
    \vspace{-0.165in}
\label{table:seg}
\end{minipage}
% \vspace{-0.15in}
\end{figure*}



We use the above process to compute the 3D F-score for all objects in the dataset.
We also compare to the accuracy when using only specular simulation images (i.e., $\alpha_1=1, \alpha_2=0$), and when using only edge simulation images (i.e., $\alpha_1=0, \alpha_2=1$). 

Table~\ref{table:sim} reports the 25\textsuperscript{th}, 50\textsuperscript{th}, and 75\textsuperscript{th} percentile F-scores for \name\ (1st row), specular only (2nd row) and edge only (3rd row). \name's simulator achieves a median F-score of 94\%, while the specular-only or edge-only simulators achieve median F-scores of 90\% and 71\%, respectively. 
An even larger improvement can be seen in the 25\textsuperscript{th} percentile (85\% vs 68\% and 49\%).
This shows the value of \name's techniques for combining multiple simulation types. 



