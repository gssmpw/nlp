\vspace{-0.1pt}
\section{Non-Line-of-Sight Applications} 
\label{sec:tasks}





In this section, we evaluate the ability of 
our dataset and simulator
to enable and establish baselines for NLOS perception for diverse, everyday objects. We describe our experimental setup and analyze the evaluation metrics for NLOS segmentation and classification. 

Such applications would be useful in a variety of areas, such as confirming customer orders are properly packaged in e-commerce warehouses, detecting broken objects during shipping, and efficiently finding objects for robotic search.

\vspace{-0.1pt}
\subsection{Object Segmentation}\label{sec:seg}\label{sec:segmentation}
\vspace{-0.1pt}

We show how \name\ can be used to build a mmWave object segmentation tool that works in LOS and NLOS. 

\subsubsection{Setup}

We build on the state-of-the-art segment-anything model (SAM) \cite{sam}, which takes as input an image to segment and a prompt, either in the form of points in the image or a bounding box. The model returns one or multiple masks, and a score associated with the predicted mask quality. 

Directly using an off-the-shelf object segmentation network for mmWave images faces two challenges. First, SAM is trained on 2D RGB images, but mmWave images are single-channel, complex-valued, 
and 3D. Second, the SAM model requires a prompt before returning segmentation masks. We note that while there are fully automatic methods using SAM (e.g., iterating through a grid of points as a prompt)\cite{sam}, they aim to provide all masks within an image, and would not be applicable in our case since they do not identify which mask contains the object of interest.

To convert our mmWave images to 2D RGB images, we start by averaging the magnitudes along the depth of the 3D image, and use a rainbow colormap for colorization, as shown in Fig.~\ref{fig:segmentation}a. Second, we randomly select a subset of the highest power reflection points as prompts, as shown in Fig.~\ref{fig:segmentation}b. By feeding the resulting RGB image and prompts to the SAM network, we can obtain mmWave segmentationm as shown in Fig.~\ref{fig:segmentation}c. We apply the same approach for segmenting both 24~GHz and 77~GHz images. Additional details can be found in the supplementary.


\noindent \textbf{Multi-Spectral Images.} Beyond segmenting 77~GHz and 24~GHz images separately, we also propose a method for fusing the two images to improve overall segmentation accuracy. To do so, we combine the images during prompt point selection: instead of using a power threshold based only on a single image, the points must exceed thresholds in both 24~GHz and 77~GHz images. Then, we use these points and the 77~GHz colorized image as input to the SAM network. This allows the 24~GHz image to act as a filter, removing the impact of noise (e.g., from cardboard occluders) on the point selection and improving segmentation quality.





\subsubsection{Experiments}
\textbf{Baseline.} We compare our segmentation performance to an RGB-D camera baseline. We use the camera image from the experiment directly as input to the SAM model. To select prompt points, we use the depth image to automatically filter out pixels greater than a certain depth (i.e., the background), and randomly select prompt points from the remaining pixels. In the LOS scenarios, this corresponds to points on the object. In NLOS scenarios, this corresponds to points on the occluding object (i.e., the cardboard).   


 
\begin{figure*}
\begin{minipage}[t]{0.6\linewidth}
    \centering
    \includegraphics[width=\linewidth]{figures/shiny_new2.pdf}
\vspace{-0.25in}
    \caption{\footnotesize{\textbf{Qualitative Segmentation Result.}} \textnormal{An example where the depth image is unable to capture the entire object, but the mmWave image does.} }
\vspace{-0.2in}
    \label{fig:res_shiny}
    \end{minipage}
    \hfill
\begin{minipage}[t]{0.37\linewidth}
\footnotesize
\centering
\begin{tabular}[b]{ |c|c||c|c|c|}
\hline 
 \multicolumn{2}{|c||}{Training Data} & \multicolumn{3}{c|}{Accuracy}\\
   \Xhline{5\arrayrulewidth}  
  \textbf{Edge}  & \textbf{Specular}   & \textbf{LOS}  & \textbf{NLOS}  & \textbf{Overall}     \\  
 \Xhline{5\arrayrulewidth}  
  X & &  61.1\% & 75.0\% &   67.6\% \\ 
    \hline 
  & X & 77.7\% &  75.0\% &   76.4\% \\  
   \hline 
 X & X &  \textbf{88.8\%} &  \textbf{81.2\%} &   \textbf{85.2\%} \\ 
   \hline 
\end{tabular}
\vspace{-0.2in}
\captionof{table}{\footnotesize{\textbf{Classifier Accuracy} \textnormal{for LOS, NLOS,\& overall using specular, edge,\& combined simulations for training. }}}
\label{table:classifier}
    \vspace{-0.2in}
\end{minipage}
\end{figure*} 



\noindent \textbf{Quantitative Results.}
Tab.~\ref{table:seg} shows the median Precision, Recall, F-score, Intersection-over-Union (IoU), and Accuracy when using the camera, 24~GHz, 77~GHz, and multi-spectral (24~GHz \& 77~GHz) images for both LOS and NLOS scenarios. In LOS, segmenting camera images achieves a median precision, recall and F-score of 99.7\%, 99.5\%, and 99.4\%, respectively. However, in NLOS, the camera only achieves 3.6\% precision and 7.0\% F-score, which is expected since cameras (and other visible-light-based sensors) are unable to operate through occlusions. We note that the camera still achieves 100\% recall since SAM segments the cardboard occluder, covering the full object.

In contrast, the various mmWave image inputs achieve better performance NLOS scenarios, demonstrating how \name\ enables NLOS object segmentation. In particular, the F-score of NLOS for 24~GHz, 77~GHz, and multi-spectral exceeds that of the camera. Notably, multi-spectral images achieve the highest performance. When comparing LOS and NLOS, multi-spectral images only experience a 2.0\%, 0.5\%, and 3.6\% change in precision, recall, and F-score, respectively. This demonstrates that \name\ is able to accurately image and perceive fully occluded objects with a similar performance as in LOS.

Interestingly, the recall when segmenting mmWave images is lower (63.1\%) than the precision (95.1\%). This is primarily because mmWaves are specular, as described earlier, preventing certain portions of the object from appearing in the image. We provide a more detailed discussion on this phenomenon in the supplementary and Sec. \ref{sec:discussion} highlight how future work can improve recall via shape completion.

The IoU and accuracy reported in Table 2 show similar trends. Notably, 24~GHz, 77~GHz, and multi-spectral all outperform the camera in NLOS. We also note that the accuracy is high because it is dominated by the background.


\noindent \textbf{Qualitative Results.} In some cases, the depth camera is unable to measure the depth to every point on the object. This is typically the case for very reflective objects (e.g., mirror-like metallic objects) or clear objects (e.g., glass or plastic). In these cases, \name\ is able to reconstruct more of the object than the depth camera, as shown in Fig.~\ref{fig:res_shiny}. In Fig.~\ref{fig:res_shiny}b, the image from the depth camera has holes due to the object's material reflectivity, whereas in Fig.~\ref{fig:res_shiny}c, \name's mmWave image captures the whole object. Fig.~\ref{fig:res_shiny}d shows our 77~GHz mmWave segmentation masks the entire object.




\vspace{-0.03in}
\subsection{Sim2Real Shape Classification}
\vspace{-0.03in}


Next, we demonstrate how \name\ can be used to classify objects in NLOS based on their geometry. 


\subsubsection{Setup}

We design a classifier which takes 77~GHz mmWave images (after applying the segmentation mask from Sec.~\ref{sec:segmentation}) as input, and outputs the estimated class label. The classifier is a neural network consisting of 5 convolutional layers, followed by 3 fully connected layers, and a softmax activation. We \textit{train the classifier entirely on synthetic data}, and \textit{evaluate on real-world} images in both LOS and NLOS. To generate synthetic images for training, we leverage object meshes provided by YCB dataset.

One challenge in training from synthetic images is to determine which types of reflections dominate for a given object. Recall from Sec. \ref{sec:simulation} that our simulator models two different reflection types. However, it is difficult to accurately measure the material/texture properties of all objects to identify how to correctly combine these simulation images. Instead, we introduce a new data augmentation, where we choose random weights for each simulation type and sum the weighted images. This allows the network to train on different material properties for each object. We also apply standard image augmentations (rotation, translation, masking, blur). More training details are in supplementary.

\vspace{-0.1pt}
\subsubsection{Experiments}


We report the shape classification results on real-world data in LOS and NLOS in Tab.~\ref{table:classifier}. Our classifier performs with an overall accuracy of 85.2\%. Specifically, it achieves an accuracy of 88.8\% and 81.2\% for LOS and NLOS scenarios, respectively, showing that \name\ can be used to enable novel capabilities such as NLOS shape classification.

In Tab.~\ref{table:classifier}, we also include results using a classifier trained on either only edge images (i.e., without combining them with the specular images) or only specular images in order to evaluate the impact of our different simulation types on the classifier accuracy. We observe that these models are only able to achieve 67.6\% and 76.4\%, whereas using both simulation types together achieves 85.2\% accuracy. This shows the benefit of our simulator's different outputs, and our data augmentation technique to combine them.

These results shows that \name\ has the potential to be used for novel NLOS applications, such as confirming packaged orders in closed boxes. Furthermore, by training on synthetic images and evaluating on real-world data, we show that our simulator is representative of the real-world. 


