\section{Related Work}
Machine Unlearning is a fast-growing but still relatively new field. Several research directions have emerged in the last years, providing the literature with innovative unlearning methods \cite{jin2023ntk, eldan2023harry_potter} %fosteER024selective_synapse} 
and clever ways of evaluating them \cite{lynch2024eight, hong2024intrinsic}. 
One part of the literature has focused on efficient methods without certified guarantees  \cite{kurmanji2024towards_unbounded, eldan2023harry_potter} %, fosteER024selective_synapse},
whose unlearning performance are verified empirically by assessing various metrics such as the accuracy of Membership Inference Attacks (MIAs).

In the case of unlearning, the assumption is that the worse the performance of MIAs over the forget set, the better the quality of unlearning. Recently, the efficacy of these methods has been questioned \cite{aubinais2023fundamental, hayes2024false_sense}.
At the same time, the theoretical analysis of MU has known important advances, with new certified unlearning methods \cite{chourasia2023forget_unlearning, ullah2023adaptive, georgiev2024attribute}, which allow for provable robustness against any MIA. 

\textbf{Certified Unlearning.}
Studies offering certified unlearning guarantees are based on either exact or approximate schemes. While the former offers stronger guarantees by generating models corresponding to exact retraining, they need to modify the training process, usually through some form of sharding \cite{sisa, yan2022arcane, wang2023fedcsa}, or tree-based approaches \cite{ullah2021machine, ullah2023adaptive}.
For this reason, in this work we choose to focus on certified approximate unlearning methods,
a less explored approach to MU that is steadily gaining attention in the field. Some approximate methods rely on KL-based metrics \cite{Golatkar_2020_CVPR, jin2023ntk, georgiev2024attribute}, or  to linear modeling methods \cite{izzo2021approximate} to ensure unlearning.

We note that the majority of these approaches provide unlearning certification relying on the theory of Differential Privacy \cite{guo2020certified, DescentToDelete, gupta2021adaptive, chourasia2023forget_unlearning, allouah2024utility}.

\textbf{Machine Unlearning and Differential Privacy.}
 Differential Privacy (DP \cite{DP_book} ensures that the protected data (the forget set in our case) has a small statistical impact on the final model. One notable advantage of DP-based unlearning certification is its provable robustness against any MIA. 
 Protecting every sample through DP would ensures the privacy of the entire dataset, and thus eliminate the need for unlearning, but would come at the cost of a significant decrease in model utility \cite{chaudhuri2011DPERM, abadi2016deep}. In contrast, in the most common definition of unlearning \cite{ginart2019making}, MU only needs to guarantee the privacy of a small subset of samples, making it a far less restrictive approach. This is emphasized by the fact that several papers in both centralized and decentralized MU offer significant gains in performance and running time as compared to the systematic application of DP \cite{fraboni2024sifu, allouah2024utility}. The literature of DP-based MU relies on starting from the original optimum, retraining on the retain set (the full dataset minus the forget set) and adding noise to the parameters in order to achieve unlearning. The forget set is thus ignored in order to remove its impact on the weights of the model.

\textbf{Comparisons to the Scratch Baseline.}
Since the impact of the forget set can be perfectly removed by retraining from scratch, any relevant unlearning method must be faster than simple retraining.
This naturally raises the question of how unlearning methods compare with retraining: what computational savings can be achieved, and at what utility cost. While essential, this question is generally challenging to answer due to the complexity of deep-learning models, and the lack of both unified set of hypotheses and unlearning definition. As a result, while it is common to provide an empirical comparison between unlearning and retraining from scratch, this comparison is rarely backed by theoretical arguments. \citet{izzo2021approximate} provided a comparison of the computational costs of hessian-based MU methods for linear models. Other relevant literature use first-order methods to perform MU. \citet{huang2023tight} proved that when the retain set is not accessible during unlearning, one cannot outperform DP on the entire dataset. Two recent papers \cite{chourasia2023forget_unlearning, allouah2024utility} have identified upper-bounds on the number of gradient steps needed by their specific MU algorithm to achieve unlearning, and compare these bounds with the ones on full retraining and other methods. Specifically, \citet{chourasia2023forget_unlearning} consider a stronger definition of unlearning based on adversarial requests and has to rely on the application of DP on the entire training set. \cite{allouah2024utility} is a recent work that considers a setting closer to ours, but focusing however on non-stochastic gradient descent. The authors provide bounds on the number of samples that their two algorithms can delete at a certain utility or computing cost, in contrast to our study of the performance of the best possible algorithms over a wide class.