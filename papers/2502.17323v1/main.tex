\documentclass{article}


\usepackage[round]{natbib}
\let\cite=\citet
\renewcommand{\bibname}{References}
\renewcommand{\bibsection}{\subsubsection*{\bibname}}


\bibliographystyle{apalike}



\usepackage{microtype}
\usepackage{graphicx}
\usepackage{booktabs} % for professional tables
\usepackage{stfloats}
\usepackage{float}





\usepackage{hyperref}



\newcommand{\theHalgorithm}{\arabic{algorithm}}


\usepackage[accepted]{arxiv2025}





\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}


\usepackage[capitalize,noabbrev]{cleveref}

\usepackage{bm}
\usepackage{tikzsymbols}
\usepackage{fancyhdr}
\usepackage{natbib}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{tabularx}
\usepackage{multirow}
\usepackage{array}
\usepackage{makecell}
\usepackage{arydshln}
\usepackage{amssymb}
\usepackage{pifont}
\usepackage{bbding}
\usepackage{stmaryrd}




\usepackage{hyperref}
\newtheorem{assumption}{Assumption}
\newtheorem{definition}{Definition}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{property}{Property}

\usepackage{algorithm}
\usepackage{algorithmic}

\usepackage{tikz}
\usetikzlibrary{arrows}
\usepackage{blindtext}
\usepackage{mathrsfs}


\newcommand{\kevin}[1]{{\textbf{\color{red}KS: #1}}}

\hypersetup{
  colorlinks   = true, %Colours links instead of ugly boxes
  urlcolor     = cyan, %Colour for external hyperlinks
  linkcolor    = blue, %Colour of internal links
  citecolor    = blue, %Colour of citations
}

\newcommand{\figref}[1]{Fig.~\ref{#1}}
\newcommand{\tabref}[1]{Tab.~\ref{#1}}
\newcommand{\eqnref}[1]{Eq.~(\ref{#1})}
\newcommand{\algref}[1]{Alg.~(\ref{#1})}

\newcommand{\tbc}[0]{\textbf{> > > TO BE CHECKED < < <}}
\newcommand{\tbm}[0]{\textbf{> > > TO BE MODIFIED < < <}}
\newcommand{\tbe}[0]{\textbf{> > > TO BE EXPANDED < < <}}
\newcommand{\tba}[0]{\textbf{[To Be Added]:}}

\newcommand{\cf}[0]{\textit{cf. }}
\newcommand{\eg}[0]{\textit{e.g., }}
\newcommand{\ibid}[0]{\textit{ibid.}}
\newcommand{\ie}[0]{\textit{i.e., }}

\newcommand{\suppmat}[0]{\hyperref[sec:supmat]{\textit{Supplementary Material}} }
\newcommand{\asuppmat}[0]{\hyperref[sec:supmat]{\textit{Sup. Mat.}} }
\newcommand{\Eq}[1]{Eq.~(\ref{eq:#1})}

\newcommand{\kdp}[0]{\kappa_{\eps,\delta}}
\newcommand{\abs}[1]{\left| #1 \right|}

\newcommand{\D}[0]{\Dcal}
\newcommand{\Df}[0]{\Dcal_f}
\newcommand{\Dr}[0]{\Dcal_r}
\newcommand{\thethat}[0]{\hat{\vtheta}}
\newcommand{\thetilde}[0]{\widetilde{\vtheta}}

\newcommand{\dN}[1]{d_{\mbox{\rm\tiny #1}}}
\definecolor{darkgreen}{RGB}{0,180,0}
\definecolor{purple}{RGB}{200,0, 200}
\definecolor{royal_blue}{RGB}{90, 120, 250}

\newcommand{\martin}[1]{\textbf{\textcolor{darkgreen}{Martin}} \textcolor{darkgreen}{#1}}
\newcommand{\gio}[1]{\textbf{\textcolor{royal_blue}{GN: #1}}}
\newcommand{\marco}[1]{\textbf{\textcolor{purple}{ML: #1}}}


% Abbreviations
\newcommand{\rhs}[0]{right hand side}  % right hand side
\newcommand{\lhs}[0]{left hand side}  % left hand side
\newcommand{\snr}[0]{snr}  % signal-to-noise ratio
% Definitions
\newcommand{\realnumbers}[0]{\mathbb{R}}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
% boldface symbols
\newcommand{\ab}[0]{\mathbf{a}}
\newcommand{\bb}[0]{\mathbf{b}}
\newcommand{\xb}[0]{\mathbf{x}}
\newcommand{\setx}[0]{\mathbf{x}_1,\ldots,\mathbf{x}_C}
\newcommand{\setxn}[0]{\mathbf{x}_1^{(n)},\ldots,\mathbf{x}_C^{(n)}}
\newcommand{\yb}[0]{\mathbf{y}}
\newcommand{\zb}[0]{\mathbf{z}}
\newcommand{\dz}[0]{d\zb}
% colored symbols
\newcommand{\xc}[0]{\color{blue}x\color{black}}
\newcommand{\xbc}[0]{\color{blue}\xb\color{black}}
\newcommand{\yc}[0]{\color{olive}y\color{black}}
\newcommand{\ybc}[0]{\color{olive}\yb\color{black}}
% bold symbols
\newcommand{\alphab}[0]{\boldsymbol{\alpha}}
\newcommand{\thetab}[0]{\boldsymbol{\theta}}
\newcommand{\thetaset}[0]{\thetab_1,\ldots,\thetab_C}
\newcommand{\phib}[0]{\boldsymbol{\phi}}
\newcommand{\phiset}[0]{\phib_1,\ldots,\phib_C}
\newcommand{\etab}[0]{\boldsymbol{\eta}}
\newcommand{\epsilonb}[0]{\boldsymbol{\epsilon}}
\newcommand{\x}[0]{\xb}
\newcommand{\z}[0]{\zb}
\newcommand{\Lin}[0]{\mathbf{L}}
\newcommand{\mub}[0]{\boldsymbol{\mu}}
\newcommand{\Sigmab}[0]{\boldsymbol{\Sigma}}
\newcommand{\sigmab}[0]{\boldsymbol{\sigma}}
% cal symbols
\newcommand{\Acal}{\mathcal{A}}
\newcommand{\Bcal}{\mathcal{B}}
\newcommand{\Ccal}{\mathcal{C}}
\newcommand{\Dcal}{\mathcal{D}}
\newcommand{\Ecal}{\mathcal{E}}
\newcommand{\Fcal}{\mathcal{F}}
\newcommand{\Gcal}{\mathcal{G}}
\newcommand{\Hcal}{\mathcal{H}}
\newcommand{\Ical}{\mathcal{I}}
\newcommand{\Jcal}{\mathcal{J}}
\newcommand{\Kcal}{\mathcal{K}}
\newcommand{\Lcal}{\mathcal{L}}
\newcommand{\Mcal}{\mathcal{M}}
\newcommand{\Ncal}{\mathcal{N}}
\newcommand{\Ocal}{\mathcal{O}}
\newcommand{\Pcal}{\mathcal{P}}
\newcommand{\Qcal}{\mathcal{Q}}
\newcommand{\Rcal}{\mathcal{R}}
\newcommand{\Scal}{\mathcal{S}}
\newcommand{\Tcal}{\mathcal{T}}
\newcommand{\Ucal}{\mathcal{U}}
\newcommand{\Vcal}{\mathcal{V}}
\newcommand{\Wcal}{\mathcal{W}}
\newcommand{\Xcal}{\mathcal{X}}
\newcommand{\Ycal}{\mathcal{Y}}
\newcommand{\Zcal}{\mathcal{Z}}
% Gaussian
\newcommand{\Gauss}[2]{\mathcal{N}\left(#1;#2\right)}
\newcommand{\Gaussof}[1]{\mathcal{N}\left(#1\right)}
\newcommand{\Gausstd}[0]{\Gauss{\mathbf{0}}{\mathbf{I}}}
\newcommand{\GaussStdDim}[1]{\Gauss{\mathbf{0}}{\mathbf{I}_{#1}}}
% Kullback-Leibler
\newcommand{\kl}[0]{\operatorname{\mathcal{D}_{KL}}}
\newcommand{\KL}[2]{\kl\bigg(#1\bigg\vert\bigg\vert#2\bigg)}
% Expected Value
\newcommand{\expect}{\operatorname{\mathbb{E}}}
\newcommand{\Var}{\operatorname{Var}}
\newcommand{\Cov}{\operatorname{Cov}}
\newcommand{\VAR}[1]{\Var\left[#1\right]}
\newcommand{\VARR}[2]{\Var_{#1}\left[#2\right]}
\newcommand{\COV}[2]{\Cov\left[#1 , #2\right]}
\newcommand{\E}[1]{\expect\left[#1\right]}
\newcommand{\EE}[2]{\expect_{#1}\left[#2\right]}
% Marginal / Joint probabilities
\newcommand{\q}[1]{q\left(#1\right)}
\newcommand{\p}[1]{p\left(#1\right)}
\newcommand{\px}[0]{\p{\x}}
\newcommand{\pz}[0]{\p{\z}}
\newcommand{\pxz}[0]{\p{\x,\z}}
\newcommand{\psetx}[0]{\p{\setx}}
\newcommand{\psetxz}[0]{\p{\setx,\z}}
% Conditional probabilities
\newcommand{\pxgz}[0]{\p{\x|\z}} % x Given z
\newcommand{\psetxgz}[0]{\p{\setx|\z}}
\newcommand{\pzgsetx}[0]{\p{\z|\setx}}
\newcommand{\pzgx}[0]{\p{\z|\x}}
% Posterior
\newcommand{\pzsetx}[0]{\p{\z|\setx}}
% Variational approximation
\newcommand{\qz}[0]{\q{\z}}
\newcommand{\qzgx}[0]{\q{\z|\x}}
\newcommand{\qzgsetx}[0]{\q{\z|\setx}}
\newcommand{\qzg}[1]{\q{\z|#1}}
\newcommand{\qzx}[1]{\q{\z|\x_{#1}}}
% Lower Bound
\newcommand{\LB}[0]{\mathcal{L}\left(\thetab, \phib, \x \right)}
% EXPECTATION PROPAGATION
\newcommand{\qi}[0]{q_i(\thetab)}
\newcommand{\qj}[0]{q_j(\thetab)}
\newcommand{\cavity}[0]{Q_{\backslash i}(\thetab)}
%Innner Product
\newcommand{\inner}[2]{\langle #1, #2 \rangle}
%Indistin
\newcommand\indistin{\stackrel{\mathclap{\normalfont\mbox{$\epsilon, \delta$}}}{\approx}}
%Lemmas
\newtheorem{lemma}{Lemma}[section]


%%%%% NEW MATH DEFINITIONS %%%%%

\usepackage{amsmath,amsfonts,bm}

% Mark sections of captions for referring to divisions of figures
\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

% Highlight a newly defined term
\newcommand{\newterm}[1]{{\bf #1}}


% Figure reference, lower-case.
\def\figref#1{figure~\ref{#1}}
% Figure reference, capital. For start of sentence
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
% Section reference, lower-case.
\def\secref#1{section~\ref{#1}}
% Section reference, capital.
\def\Secref#1{Section~\ref{#1}}
% Reference to two sections.
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
% Reference to three sections.
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
% Reference to an equation, lower-case.
\def\eqref#1{equation~\ref{#1}}
% Reference to an equation, upper case
\def\Eqref#1{Equation~\ref{#1}}
% A raw reference to an equation---avoid using if possible
\def\plaineqref#1{\ref{#1}}
% Reference to a chapter, lower-case.
\def\chapref#1{chapter~\ref{#1}}
% Reference to an equation, upper case.
\def\Chapref#1{Chapter~\ref{#1}}
% Reference to a range of chapters
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
% Reference to an algorithm, lower-case.
\def\algref#1{algorithm~\ref{#1}}
% Reference to an algorithm, upper case.
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
% Reference to a part, lower case
\def\partref#1{part~\ref{#1}}
% Reference to a part, upper case
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\left \lceil #1 \right \rceil}
\def\floor#1{\left \lfloor #1 \right \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


% Random variables
\def\reta{{\textnormal{$\eta$}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
% rm is already a command, just don't name any random variables m
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

% Random vectors
\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

% Elements of random vectors
\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

% Random matrices
\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

% Elements of random matrices
\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

% Vectors
\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vDelta{{\bm{\Delta}}}
\def\vtheta{{\bm{\theta}}}
\def\vepsilon{{\bm{\epsilon}}}
\def\vnu{{\bm{\nu}}}
\def\vphi{{\bm{\phi}}}
\def\vpsi{{\bm{\psi}}}
\def\vxi{{\bm{\xi}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}
\def\vR{{\bm{R}}}

% Elements of vectors
\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

% Matrix
\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}
\def\mPhi{{\bm{\Phi}}}

% Tensor
\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


% Graph
\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

% Sets
\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sE{{\mathbb{E}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

% Entries of a matrix
\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

% entries of a tensor
% Same font as tensor, without \bm wrapper
\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

% The true underlying data generating distribution
\newcommand{\pdata}{p_{\rm{data}}}
% The empirical distribution defined by the training set
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
% The model distribution
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
% Stochastic autoencoder distributions
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} % Laplace distribution

%\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
%\newcommand{\KL}{D_{\mathrm{KL}}}
%\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
%\newcommand{\Cov}{\mathrm{Cov}}
% Wolfram Mathworld says $L^2$ is for function spaces and $\ell^2$ is for vectors
% But then they seem to use $L^2$ for vectors throughout the site, and so does
% wikipedia.
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} % See usage in notation.tex. Chosen to match Daphne's book.

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak

\newcommand{\set}[1]{\llbracket#1\rrbracket}
\newcommand{\thetat}{\hat{\vtheta}}

\icmltitlerunning{Complexity Trade-offs in Machine Unlearning}
\begin{document}


\twocolumn[
\icmltitle{When to Forget? Complexity Trade-offs in Machine Unlearning}

\begin{icmlauthorlist}
\icmlauthor{Martin Van Waerebeke}{yyy}
\icmlauthor{Marco Lorenzi}{zzz}
\icmlauthor{Giovanni Neglia}{zzz}
\icmlauthor{Kevin Scaman}{yyy}
\end{icmlauthorlist}

\icmlaffiliation{yyy}{INRIA Paris}
\icmlaffiliation{zzz}{INRIA Sophia-Antipolis}

\icmlcorrespondingauthor{Martin Van Waerebeke}{martin.van-waerebeke@inria.fr}




\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

\printAffiliationsAndNotice{}

\begin{abstract}

Machine Unlearning (MU) aims at removing the influence of specific data points from a trained model, striving to achieve this at a fraction of the cost of full model retraining. In this paper, we analyze the efficiency of unlearning methods and establish the first upper and lower bounds on minimax computation times for this problem, characterizing the performance of the most efficient algorithm against the most difficult objective function. Specifically, for strongly convex objective functions and under the assumption that the forget data is inaccessible to the unlearning method, we provide a phase diagram for the unlearning complexity ratio---a novel metric that compares the computational cost of the best unlearning method to full model retraining. The phase diagram reveals three distinct regimes: one where unlearning at a reduced cost is infeasible, another where unlearning is trivial because adding noise suffices, and a third where unlearning achieves significant computational advantages over retraining. These findings highlight the critical role of factors such as data dimensionality, the number of samples to forget, and privacy constraints in determining the practical feasibility of unlearning.




\end{abstract}
\section{Introduction}


\begin{figure}[t]
    \label{fig:theo_diagram}
    \centering
    \includegraphics[width=\textwidth/2-15pt]{theo_diagram_2.5_2025-01-31.pdf}

    \caption{Schematic representation of the phase diagram for unlearning, where $e$ (resp. $e_0$) is the target (resp. initial) excess risk, and $\kdp$ the strength of the privacy constraint (see Section \ref{sec:results}). We describe the existence of three regimes of unlearning (IR, ER, TR).}

\end{figure}



With the widespread collection of personal data, privacy concerns have become more important than ever \cite{jha2024privacy, kiryati2021dataset}. As vast amounts of data are gathered, the risk of errors or damaging information being recorded grows, potentially leading to severe consequences in the training of machine learning models. This, in turn, can result in inaccurate predictions, such as incorrect medical treatment plans and prognosis \cite{navarro2021risk, lawrence2024opportunities}.
Regulations such as the GDPR \cite{gdpr} and CCPA \cite{ccpa} attempt to limit these risks and give individuals the right to ask for erasure of their data, but modern machine learning models rely on vast corpora, making it impractical and costly to honor such a request by retraining the model completely \cite{cottier2024rising}.

Machine Unlearning (MU) aims to solve this issue by efficiently removing the influence of specific training data (the 'forget set') without the need for full model retraining. MU focuses on developing algorithms that can replicate the effects of retraining from scratch at a ``fraction of its cost'' \cite{kong2023data, georgiev2024attribute}.






While we want to retrain the model \textit{“at a fraction of the cost”}, the exact \textit{“fraction”} is currently not quantified in the literature. Specifically, in a general machine learning setting and for any accepted excess risk  $e$, let us define $r_f$ as the fraction of data in the forget set. We denote by $T^U_e$ (resp. $T^S_e$) the number of stochastic gradient steps required to unlearn the forget set (resp. to retrain from scratch without the forget set) until the excess risk falls below  $e$.
The efficacy of MU can thus be quantified through the ratio of time required by unlearning over  retraining,  $T^U_e/T^S_e$, which we call the \textit{unlearning complexity ratio}. We currently lack general bounds on this ratio, making it difficult to rigorously assess the efficiency of MU methods relative to simple retraining.

Our first goal consists in characterizing the regime in which the unlearning complexity ratio is provably lower than one, \ie when unlearning is possible at a smaller cost than full retraining. The second goal of  our work consists in studying the relationship between the unlearning complexity ratio and the amount of data to forget quantified by $r_f$, in order to establish a link between the size of the forget data and the unlearning cost. %, as no such link has currently been made.






\textbf{Contributions.}






In this paper, we provide a detailed analysis of when unlearning algorithms can lead to efficient data removal. To do so, we provide the first upper and lower bounds on the \emph{unlearning complexity ratio}, and thus characterize the relative performance of unlearning with respect to retraining.




In particular:






\begin{itemize}
    \item We introduce the \textit{unlearning complexity ratio} (unlearning time over retraining time), leveraging minimax optimization complexity in MU for the first time. This allows us to identify regimes where unlearning is impossible $(\text{\text{IR}})$, efficient $(\text{ER}$), or trivial $(\text{\text{TR}})$.
    \item We provide the first lower bound for the unlearning complexity ratio, answering an open problem in the literature \cite{allouah2024utility} and showing that there is a regime in which gradient-based unlearning cannot asymptotically beat simple retraining $(\text{IR})$.
    \item We derive the first upper bound for this ratio, exhibiting a regime in which unlearning is provably faster than retraining $(\text{ER})$. The bound scales with  $r_f^2$, the square of the portion of data to forget, demonstrating that a small fraction of data can be unlearned at a small cost.
    \item We identify a last regime where unlearning is ``trivial,'' as simply adding noise to the parameters suffices, and is thus much more efficient than retraining $(\text{TR})$.
\end{itemize}





\section{Related Work}

Machine Unlearning is a fast-growing but still relatively new field. Several research directions have emerged in the last years, providing the literature with innovative unlearning methods \cite{jin2023ntk, eldan2023harry_potter} %fosteER024selective_synapse} 
and clever ways of evaluating them \cite{lynch2024eight, hong2024intrinsic}. 
One part of the literature has focused on efficient methods without certified guarantees  \cite{kurmanji2024towards_unbounded, eldan2023harry_potter} %, fosteER024selective_synapse},
whose unlearning performance are verified empirically by assessing various metrics such as the accuracy of Membership Inference Attacks (MIAs).

In the case of unlearning, the assumption is that the worse the performance of MIAs over the forget set, the better the quality of unlearning. Recently, the efficacy of these methods has been questioned \cite{aubinais2023fundamental, hayes2024false_sense}.
At the same time, the theoretical analysis of MU has known important advances, with new certified unlearning methods \cite{chourasia2023forget_unlearning, ullah2023adaptive, georgiev2024attribute}, which allow for provable robustness against any MIA. 

\textbf{Certified Unlearning.}
Studies offering certified unlearning guarantees are based on either exact or approximate schemes. While the former offers stronger guarantees by generating models corresponding to exact retraining, they need to modify the training process, usually through some form of sharding \cite{sisa, yan2022arcane, wang2023fedcsa}, or tree-based approaches \cite{ullah2021machine, ullah2023adaptive}.
For this reason, in this work we choose to focus on certified approximate unlearning methods,
a less explored approach to MU that is steadily gaining attention in the field. Some approximate methods rely on KL-based metrics \cite{Golatkar_2020_CVPR, jin2023ntk, georgiev2024attribute}, or  to linear modeling methods \cite{izzo2021approximate} to ensure unlearning.

We note that the majority of these approaches provide unlearning certification relying on the theory of Differential Privacy \cite{guo2020certified, DescentToDelete, gupta2021adaptive, chourasia2023forget_unlearning, allouah2024utility}.

\textbf{Machine Unlearning and Differential Privacy.}
 Differential Privacy (DP \cite{DP_book} ensures that the protected data (the forget set in our case) has a small statistical impact on the final model. One notable advantage of DP-based unlearning certification is its provable robustness against any MIA. 
 Protecting every sample through DP would ensures the privacy of the entire dataset, and thus eliminate the need for unlearning, but would come at the cost of a significant decrease in model utility \cite{chaudhuri2011DPERM, abadi2016deep}. In contrast, in the most common definition of unlearning \cite{ginart2019making}, MU only needs to guarantee the privacy of a small subset of samples, making it a far less restrictive approach. This is emphasized by the fact that several papers in both centralized and decentralized MU offer significant gains in performance and running time as compared to the systematic application of DP \cite{fraboni2024sifu, allouah2024utility}. The literature of DP-based MU relies on starting from the original optimum, retraining on the retain set (the full dataset minus the forget set) and adding noise to the parameters in order to achieve unlearning. The forget set is thus ignored in order to remove its impact on the weights of the model.

\textbf{Comparisons to the Scratch Baseline.}
Since the impact of the forget set can be perfectly removed by retraining from scratch, any relevant unlearning method must be faster than simple retraining.
This naturally raises the question of how unlearning methods compare with retraining: what computational savings can be achieved, and at what utility cost. While essential, this question is generally challenging to answer due to the complexity of deep-learning models, and the lack of both unified set of hypotheses and unlearning definition. As a result, while it is common to provide an empirical comparison between unlearning and retraining from scratch, this comparison is rarely backed by theoretical arguments. \citet{izzo2021approximate} provided a comparison of the computational costs of hessian-based MU methods for linear models. Other relevant literature use first-order methods to perform MU. \citet{huang2023tight} proved that when the retain set is not accessible during unlearning, one cannot outperform DP on the entire dataset. Two recent papers \cite{chourasia2023forget_unlearning, allouah2024utility} have identified upper-bounds on the number of gradient steps needed by their specific MU algorithm to achieve unlearning, and compare these bounds with the ones on full retraining and other methods. Specifically, \citet{chourasia2023forget_unlearning} consider a stronger definition of unlearning based on adversarial requests and has to rely on the application of DP on the entire training set. \cite{allouah2024utility} is a recent work that considers a setting closer to ours, but focusing however on non-stochastic gradient descent. The authors provide bounds on the number of samples that their two algorithms can delete at a certain utility or computing cost, in contrast to our study of the performance of the best possible algorithms over a wide class.


\section{Problem Setup}
In this section, we describe our learning setup, including the class of iterative learning and unlearning algorithms we consider, and the definition of differential privacy-based unlearning used throughout the paper.


\textbf{Notations.} Let $L,\mu\geq 0$, and $\ell:\R^d\to\R$ a differentiable function. We say that $\ell$ is $L$-Lipschitz if, $\forall\vtheta, \vtheta' \in \sR^d$, 
\(
\bigl|\ell(\vtheta) - \ell(\vtheta')\bigr|\,\le\, 
L \,\|\vtheta - \vtheta'\|,
\)
and $\mu$-strongly convex if
\(
\ell(\vtheta') 
\,\ge\, 
\ell(\vtheta) 
\;+\;
\inner{\nabla \ell(\vtheta)}{\vtheta' - \vtheta}
\;+\;
\frac{\mu}{2}\,\|\vtheta' - \vtheta\|^2 \,.
\)
Moreover, we denote as $\|\vtheta\| = \sqrt{\sum_i \vtheta_i^2}$ the $L_2$-norm and as $\sB(0,R)=\{\vtheta\in\R^d~:~\norm{\vtheta}\leq R\}$ the ball of radius $R$ in $\R^d$.
Finally, we will use the notation $\text{supp}(\D)$ for the support of a probability distribution $\D$.

























\subsection{Learning and Unlearning Setups}
Consider a supervised learning setting in which our objective is to minimize the objective function
\begin{equation}\label{eq:opt_prob}
\min_{\vtheta\in\R^d} \Lcal(\vtheta) \coloneqq \E{\ell(\vtheta,\xi)}\,,
\end{equation}
where $\ell:\R^d\times\R^s\to\R$ is a loss function, $\vtheta\in\R^d$ the vector of model parameters, and $\xi\sim\D$ a random data point in $\R^s$ drawn according to a data distribution $\D$. %, where $\sD$ is a set of data distributions over $\sR^s$.
In machine unlearning, our goal is to remove the impact of a subset of the distribution $\D$ from an already learnt model.

More precisely, for a fraction $r_f \in [0,1]$, we decompose our distribution in two parts,
\begin{equation}
\D \coloneqq r_f \,\D_f \;+\; \bigl(1-r_f\bigr)\,\D_r\,,
\end{equation}
where $\D_f$ is the distribution one wishes to remove (\ie, the \emph{forget} distribution), while $\D_r$ is the distribution that is unaffected by the removal (\ie, the \emph{retain} distribution).
For example, in order to recover a more common setting over a discrete data set (see \eg \cite{ginart2019making}), we may take $\D$ as the uniform distribution over the training dataset $\Xi_N = \{\xi_1,\dots,\xi_N\}$, $\Df$ as the uniform distribution over the forget set $\{\xi_1,\dots,\xi_k\}\subset\Xi_N$, and $r_f=k/N$.
Finally, from a model trained on $\D$ (\ie achieving a small loss $\Lcal(\vtheta)$), our goal is now to recover a good model for $\Dr$ in as little computation time as possible, \ie a model $\vtheta'$ whose loss over $\Dr$, $\Lcal_r(\vtheta') \coloneqq \sE_{\xi\sim\Dr}\left[\ell(\vtheta', \xi)\right]$, is small.



In order to derive upper and lower bounds for the time complexity of unlearning, we will focus on strongly-convex and Lipschitz optimization problems.

\begin{assumption}[loss regularity]
Let $\mu, L > 0$, and $R=L/2\mu$. For any $\xi\in\R^s$, the loss function $\ell(\cdot,\xi)$ of \eqnref{eq:opt_prob} is $L$-Lipschitz and $\mu$-strongly convex on $\sB(0,R)$.
\end{assumption}
Although not directly applicable to most neural networks, these assumptions have already been used in the literature \cite{sekhari2021remember, huang2023tight} and are less restrictive than other recent studies that also rely on smoothness \cite{chourasia2023forget_unlearning, allouah2024utility}.
We denote as $\Fcal_{sc}(\mu, L)$ the class of such loss functions, abbreviated to $\Fcal_{sc}$ when there is no ambiguity. Moreover, for any function $\ell\in\Fcal_{sc}$, we denote as $\vtheta^*$ (resp. $\vtheta_r^*$) the unique minimizer of $\Lcal(\theta) = \sE_{\xi\sim\D}\left[\ell(\vtheta,\xi)\right]$ (resp. $\Lcal_r(\theta) =\sE_{\xi\sim\Dr}\left[\ell(\vtheta,\xi)\right]$), and it corresponding loss value $\Lcal^*$ (resp. $\Lcal_r^*$).

Note that the choice of radius $R=L/2\mu$ is relatively standard for theoretical analyses (see \eg \citealt{bubeck2015convex}), as it allows to focus on the regime in which strongly-convex functions offer significantly faster convergence times than their convex counterparts. %












Finally, following standard terminology in the stochastic optimization literature, we will denote as \emph{computing time} the number of stochastic gradient accesses, or equivalently the number of data samples used throughout the (un)learning procedure. %A stochastic gradient access refers to an instance where the learning algorithm is permitted to access use the gradient value computed on a single data sample.














\subsection{Iterative First-Order Algorithms}
In this, section, we provide precise definitions for learning and unlearning algorithms. More precisely, we will consider that both types of algorithms are \emph{non-deterministic}, \emph{iterative} and \emph{first-order}, \ie that model parameters are updated through a stochastic iterative procedure that accesses a stochastic gradient of the loss function at each iteration (see Algorithm \ref{alg:learn}). This class of algorithms, defined by their \emph{update rule} $A\in\boldsymbol{A}$, is very general and contains most standard optimization algorithms used in machine learning. More precisely, an update rule is a measurable function





\begin{equation}
A\left(\vtheta_t,  \nabla_t, m_t,\omega\right) = (\vtheta_{t+1}, m_{t+1})\,,
\end{equation}





where $\vtheta_t\in\R^d$ is the current model, $\nabla_t\in\R^d$ a stochastic gradient, $m_t\in M$ a memory state, and $\omega\in\Omega$ a seed used for adding randomness into the algorithm. The memory serves as a storage mechanism for essential information about past iterates, enabling the computation of quantities such as momentum, moving averages, or adaptive step-sizes.

\begin{algorithm}[t]
\caption{Iterative (Un)Learning Algorithm}
\label{alg:learn}
\begin{algorithmic}[1]
\REQUIRE Update rule $A \in \boldsymbol{A}$, number of iterations $T$, initial model $\vtheta_0$, loss function $\ell$, dataset $\D$. %Randomness $w$ and data points $\{\xi_t\}_{t=1}^T\in \D^T$ are pre-sampled and identical across algorithms.

\STATE Initialize memory: $m_0=\emptyset$

\FOR{$t = 0$ to $T-1$}
    \STATE Sample data point: $\xi_t \sim \D$
    \STATE Compute gradient: $\nabla \ell(\vtheta_t, \xi_t)$
    \STATE Update: $(\vtheta_{t+1}, m_{t+1}) = A(\vtheta_t, \nabla \ell(\vtheta_t, \xi_t), m_t, \omega)$
\ENDFOR

\STATE \textbf{return} Final model $\vtheta_T$
\end{algorithmic}
\end{algorithm}






For a given update rule $A\in\boldsymbol{A}$, we denote as $\vtheta^A_T(\vtheta_0, \ell, \Dr)$ the output of Algorithm \ref{alg:learn}, which applies the update rule $A$ successively $T$ times, starting at $\vtheta_0\in\R^d$.




\textbf{Learning Algorithms.} For any update rule $A\in\boldsymbol{A}$, we define the associated \textit{learning algorithm} as the function $\Acal$ mapping the number of iterations, loss function, and dataset to the output of $A$ initialized at $\vtheta_0=0$, \ie

\begin{equation}
\Acal(T, \ell, \D_r) = \vtheta^A_T(0, \ell, \Dr)\,.
\end{equation}







In what follows, we will denote by $\sA$ the class of such learning algorithms, and write $\Acal(\Dr)$ when there is no ambiguity on the values of $T$ and $\ell$.


\textbf{Unlearning Algorithms.} While learning algorithms try to estimate the optimum of the objective function $\Lcal_r$ from scratch, unlearning algorithms have the advantage of starting from a pre-trained model with low excess risk (\ie error of the model minus error of the optimal model) on the whole dataset. More precisely, we will assume that such model was trained for a sufficiently large amount of time, and reached the unique minimizer $\vtheta^*$ of the objective function $\Lcal$.




Therefore, for any update rule $A\in\boldsymbol{A}$, we define the associated \textit{unlearning algorithm} as the function $\Ucal$ mapping the number of iterations, loss function, retain dataset and forget dataset to the output of $A$ initialized at $\vtheta_0=\vtheta^*$, \ie

\begin{equation}
\Ucal(T, \ell, \D_r, \Df) = \vtheta^A_T(\vtheta^*, \ell, \Dr)\,.
\end{equation}







Again, we will denote as $\sU$ the class of such unlearning algorithms, and simply write $\Ucal(\Dr,\Df)$ when there is no ambiguity on the values of $T$ and $\ell$. 
Note that these unlearning algorithms can only sample from the retain set to perform unlearning. This is relatively common in the literature of DP-based MU \cite{DescentToDelete, fraboni2024sifu, huang2023tight, allouah2024utility}, although more efficient unlearning methods might exist in scenarios in which the forget dataset is also available during unlearning.


Finally, while we allow stateful algorithms in our framework, the algorithm used to achieve our upper bound in Section \ref{sec:results} only uses the state to remember the weighted average of previous iterations rather than all iterations, alleviating some privacy issues for adaptive unlearning requests \cite{izzo2021approximate}.




\begin{algorithm}[t]
\caption{``Noise and Fine-Tune'' Unlearning Algorithm}
\label{alg:noise+ft}
\begin{algorithmic}[1]
\REQUIRE number of iterations $T$, initial model $\vtheta^*$, loss function $\ell$, dataset $\D_r$.


\STATE Sample noise $g \sim \Ncal\left(0,\left(\kdp r_f \frac{L}{\mu}\right)^2 I_d\right)$
\STATE Initialize model: $\vtheta_0 = \vtheta^* + g$
\STATE Initialize memory: $m_0=\vtheta_0$

\FOR{$t = 1$ to $T$}
    \STATE Sample data point: $\xi_t \sim \Dr$
    \STATE Compute gradient: $\nabla \ell(\vtheta_t, \xi_t)$
    \STATE Update: $\vtheta_{t+1} = \vtheta_{t} - \frac{2}{\mu(t+1)}\nabla \ell(\vtheta_t, \xi_t)$
    \STATE Update: $m_{t+1} = m_t + (t+1)\vtheta_{t+1}$
\ENDFOR

\STATE \textbf{return} Final model $\thetat = \frac{2 m_T}{(T+1)(T+2)}$
\end{algorithmic}
\end{algorithm}


\subsection{Unlearning Guarantees}

Unlearning aims at removing the impact of the forget set on the trained model. The way it is achieved in DP-based unlearning is by making sure that the output of the unlearning algorithm is statistically indistinguishable from the output of another algorithm independent from the retain set. 

In the literature of DP-based MU, most papers rely on the following definition, first introduced by \citet{ginart2019making}. %, or a close variation \martin{citer}. %We provide it here:
\begin{definition}[$(\epsilon, \delta)$-Reference Unlearning]
 An unlearning algorithm $\Ucal\in\sU$ satisfies $(\epsilon, \delta)$-Reference Unlearning if there is a reference algorithm $\Acal\in\sA$ such that, for any couple of distributions $(\Dr, \Df)$ and subset $S\subset\sR^d$,
 \begin{align*}
\sP[\Ucal(\Dr, \Df) \in S] &\leq e^{\epsilon} \cdot \sP[\Acal(\Dr) \in S] + \delta, \\
\sP[\Acal(\Dr) \in S] &\leq e^{\epsilon} \cdot \sP[\Ucal(\Dr, \Df) \in S] + \delta.
\end{align*}
\end{definition}
We refer to it as ``reference unlearning" in the sense that an algorithm $\Ucal$ is said to achieve unlearning if another algorithm $\Acal$ achieves a similar output while being independent from the forget set. However, as mentioned in recent research \cite{georgiev2024attribute}, this is not satisfying in the sense that it makes the unlearning definition rely on the algorithm $\Acal$ which is generally simple retraining on the retain set. We propose another -slightly stronger (see Lemma \ref{lemma:MU_def})- definition that does not rely on a reference algorithm but rather only on the unlearning algorithm itself.

\begin{definition}[$(\epsilon, \delta)$-Unlearning]
\label{def:unlearning}

An unlearning algorithm $\Ucal\in\sU$ satisfies $(\epsilon, \delta)$-Unlearning, if, for any triplet of distributions $(\Dr, \Df, \Df')$, loss function $\ell$, and for any subset of outputs $S \subset \sR^d$, the following holds,
\begin{align*}
\label{lemma:def_eq}
\sP[\Ucal(\Dr, \Df) \in S] &\leq e^{\epsilon} \cdot \sP[\Ucal(\Dr, \Df') \in S] + \delta.
\end{align*}
\end{definition}

The values \((\eps, \delta)\) are called the unlearning budget. The lower the budget, the harder unlearning is to achieve.

This definition is very close to the notion of Differential Privacy. The only difference is that this one only ensures privacy of the forget set rather than the entire training set. This definition is very convenient because it does not depend on any other learning algorithm. We will refer to this definition going forward, but note the two are closely related, as proven next.

\begin{lemma}[Equivalence of definitions]\label{lemma:MU_def}
Any algorithm achieving $(\epsilon, \delta)$-Unlearning also achieves $(\epsilon, \delta)$-Reference Unlearning.
Furthermore, any algorithm achieving $(\epsilon, \delta)$-Reference Unlearning also achieves $(2\epsilon, (1+e^{\epsilon})\delta)$-Unlearning.
\end{lemma}



Note that the way $\sU$ is defined allows non-private algorithms to belong in it. For any unlearning budget $(\eps, \delta)$, we thus define \(\sU_{(\eps, \delta)} \) as the algorithms in $\sU$ achieving $\eps, \delta$-Unlearning.



\subsection{Minimax Computation Times}\label{sec:unlearn_time}





In order to quantify whether or not it is worth it to perform unlearning, let us start by introducing some key elements.  First, we define the time required to re-learn from scratch and to unlearn. For a given excess risk threshold $e$, loss $\ell$ and learning algorithm $\Acal\in\sA$, one can define the time needed to retrain from scratch until we get an excess risk smaller than $e$ as
\begin{equation}
    T^S_e(\ell, \Acal) \coloneqq \:\min_{T\in \mathbb{N}} \{T ; \; \mathbb{E}[\Lcal_r(\Acal(T, \ell, \Dr)) - \Lcal^*_{r}] \le e \}\:,
\end{equation}
where $\Lcal_r(\vtheta)=\sE_{\xi\sim\Dr}\left[\ell(\vtheta, \xi)\right]$.

In the same way, we can define the minimal number of optimization steps required by an algorithm $\Ucal$ to $(\epsilon, \delta)$-Unlearn the forget set and achieve an expected excess risk under the threshold $e$ starting from the optimum $\vtheta^*$, \ie
\begin{equation}
    T^U_e(\ell, \Ucal) \coloneqq \:\min_{T\in \mathbb{N}} \{T ; \; \mathbb{E}[\Lcal_r(\Ucal(T, \ell, \Dr, \Df)) - \Lcal^*_{r}] \le e \}\:.
\end{equation}
When studying the performance of an algorithm over a function class, one wants to study the worst-case performance of any algorithm $\Acal$ and to find the algorithm minimizing this worst case. Therefore, one can define the minimax retraining time of algorithms in $\sA$ over the function class $\Fcal_{sc}$ as
\begin{equation}
    T^S_e \coloneqq \inf_{\Acal \in \sA} \; \sup_{\ell \in \Fcal_{sc}} \; T^S_e(\ell, \Acal).
\end{equation}
In the same way, we define the minimax forget time of unlearning algorithms in $\sU{\eps, \delta}$ over the function class $\Fcal_{sc}$ as
\begin{equation}
    T^U_e \coloneqq \: \inf_{\Ucal \in \sU_{\eps, \delta}} \; \sup_{\ell \in \Fcal_{sc}} \; T^U_e(\ell, \Ucal).
\end{equation}







\section{Regimes of Unlearning Complexity}
\label{sec:results}

In this section, we provide lower and upper bounds for the \emph{unlearning complexity ratio} $T^U_e/T^S_e$ (see Section \ref{sec:unlearn_time}) over a wide class of unlearning algorithms $\sU$. By doing so, we identify regimes in which unlearning methods are significantly faster than retraining, and regimes in which they are not. In particular, we identify values of the target excess risk $e$ and strength of the privacy constraint $\kdp \coloneqq \epsilon^{-1}\sqrt{2 \;\text{ln}(1.25/\delta)}$ (see the Gaussian mechanismin \citealt{DP_book}) in which the unlearning complexity ratio is small when forget ratio $r_f$ is small. An illustration of this phase diagram is available in Figure \ref{fig:theo_diagram}, highlighting the three regimes observed in the analysis. All the proofs are deferred to the Appendix.





\subsection{Speed of Retraining from Scratch}
First, we recall the optimal convergence rate for strongly convex and Lipschitz functions, and adapt its proof to our setting.
In addition, we show that learning is trivial (\ie $T^S_e = 0$) if $e\geq e_0 \coloneqq \frac{L^2}{8\mu}$, as $\vtheta_0=0$ already satisfies the target excess risk.
\begin{lemma}
\label{lemma:scratch_e0}
If $e\geq e_0$, then \[T^S_e = T^U_e = 0.\]
\end{lemma}

In order for our lower bounds to hold, we need a technical assumption that allows flexibility on the choice of forget distribution and objective function, as well as a clear separation between forget and retain distributions.

\begin{assumption}(Flexible distributions)\label{ass:flex}
For any $p\in[0,1]$, $\exists A\subset\R^s$ s.t. $\sP(\xi_r\in A)=p$, where $\xi_r\sim\Dr$.
Moreover, there exists a distribution $\Df'$ such that $\text{supp}(\Dr),\text{supp}(\Df)$ and $\text{supp}(\Df')$ are two-by-two disjoint.
\end{assumption}
This assumption is relatively weak, and usually verified for continuous distributions, as long as $\text{supp}(\Dr)$ and $\text{supp}(\Df)$ do not cover the whole space $\R^s$.



\begin{lemma}
\label{lemma:scratch_speed}
Under Assumption \ref{ass:flex}, and if $e<e_0$, we have
\begin{equation}
T^S_e = \Theta\left( \frac{e_0}{e} \right)\,. % \Theta\left( \frac{L^2}{\mu e} \right)\,= 
\end{equation}
\end{lemma}

To claim that an unlearning method is efficient will thus require for its computation time to be significantly smaller than $O(e_0/e)$. We will show that such unlearning algorithms do exist in Section \ref{sec:eff_unlearn}.

\subsection{Trivial Unlearning Regime}

We now start with the simplest case: for a high target excess risk $e$ and low privacy constraint $\kdp$, simply adding Gaussian noise to the parameters of the model is sufficient.
\begin{theorem}[Trivial regime]
\label{theo:trivial_regime}
If the target excess risk verifies $e\in\left[\frac{r_f}{1-r_f} \left( \frac{r_f}{1-r_f} + \sqrt{d}\kdp \right) e_0, e_0\right)$, then
\begin{equation}
\frac{T^U_e}{T^S_e} = 0\,. %\quad \: \text{and thus} \quad \: \frac{T^U_e}{T^S_e} = 0\,.
\label{eq:impossible_regime}
\end{equation}
\end{theorem}

This first regime corresponds to the dark blue area in Figure~\ref{fig:theo_diagram}. In this regime, unlearning can be performed with zero gradient access, provided that the combination of \( e \) and \( \kdp \) is sufficiently permissive.  
The boundary of this region, as described in the previous theorem, is an affine function of \( e \), as represented in Fig. \ref{fig:theo_diagram}.
For low forget ratios $r_f\ll 1$, the regime begins at $e \approx r_f^2 e_0$ and $\kdp = 0$, and ends at $e = e_0$ and $\kdp \approx 1/(r_f\sqrt{d})$. As a direct consequence, for fixed target excess risks $e$ and privacy constraints $\kdp$, unlearning eventually becomes trivial as the forget ratio $r_f$ tends to $0$. This result is expected, as the distance between the two optima $\norm{\vtheta^* - \vtheta_r^*}$ is proportional to $r_f$, and thus unlearning algorithms start directly from the optimum in this regime (see Lemma \ref{lemma:opt_dist}).


















\subsection{Impossible Unlearning Regime}\label{sec:imp_unlear}

Conversely, we now show the existence of a regime in which unlearning cannot asymptotically outperform retraining. 
\begin{theorem}[Impossible regime]
\label{theo:lbunlearn}
Let $\delta \in[10^{-8},\epsilon]$. Under Assumption \ref{ass:flex}, there exists a universal constant $c > 0$ such that, if $e < \min\left\{1,\,c \left(\frac{r_f}{1-r_f}\right)^2 \left(1+\kdp^2\right)\right\} e_0$, then
\begin{equation}
\frac{T^U_e}{T^S_e} = \Omega(1)\,.
\end{equation}
\end{theorem}







The proof of Theorem \ref{theo:lbunlearn} relies on three steps: 1) defining a class of objective functions $\Lcal^g$ for $g:\R^s\to\{-1,1\}$ such that their optimum over $\D$ does not provide any information on the dataset $\Dr$, 2) showing that two such functions $\Lcal^g$ and $\Lcal^{-g}$ have optimums over $\Dr$ distant from one another, and 3) showing that any algorithm's output will behave nearly identically on both $\Lcal^g$ and $\Lcal^{-g}$, thus leading to the impossibility of having both functions efficiently optimized by the same algorithm. Overall, the approach is similar to Le Cam's two point method (see for instance \citealt{Polyanskiy_Wu_2025}), but requires combining this classical method with the $(\epsilon, \delta)$-Unlearning constraint in order to derive proximity between the two algorithm's outputs.

Theorem \ref{theo:lbunlearn} provides a regime in which first-order unlearning methods cannot asymptotically outperform retraining. This regime is delimited by a curve of type $\kdp\ge\alpha\sqrt{e}$, with $\alpha$ a constant, explaining our choice of representation in Fig. \ref{fig:theo_diagram}.
For low forget ratios $r_f\ll 1$, the unlearning complexity ratio is lower bounded by a constant when $e$ is below a quantity proportional to $r_f^2(1+\kdp^2)e_0$. In this regime, removing even minimal parts of a dataset requires a non-negligible retraining time. This may be an issue when numerous small removals must be made to a model, as each of these removals will incur a cost proportional to that of its full retraining. Fortunately, $r_f^2 e_0$ is often extremely small in practical scenarios when the forget ratio is proportional to $1/n$ where $n$ is the size of the training dataset (see Section \ref{sec:discussion}), and this regime only appears for very high privacy constraints $\kdp$ of the order of $\frac{1}{r_f}\sqrt{\frac{e}{e_0}}$.










Finally, when the target excess risk $e$ tends to $0$, a direct corollary of Theorem \ref{theo:lbunlearn} is that unlearning cannot asymptotically outperform retraining, regardless of the strenght of the privacy constraint $\kdp$.

\begin{corollary}
Within the hypothesis of Theorem \ref{theo:lbunlearn}, and for $r_f\in(0,1)$ and $\kdp\geq 0$ fixed, we have
\begin{equation}
\liminf_{e\to 0} \frac{T^U_e}{T^S_e} > 0 \,.
\end{equation}
\end{corollary}
In other words, the advantage of starting from a pre-trained model $\vtheta^*$ instead of retraining from scratch reduces as the target excess risk decreases, and, below a certain threshold, no significant asymptotic complexity improvement can be achieved by unlearning methods over retraining.

\subsection{Efficient Unlearning Regime}\label{sec:eff_unlearn}

We have now identified that, on one end of the spectrum, unlearning is trivial, while on another end, unlearning is inefficient. We now characterize what happens between those two extremes by showing that a simple unlearning mechanism achieves a good unlearning complexity ratio in this intermediate regime.


To do so, we derive an upper bound on unlearning time using the unlearning algorithm ``noise and fine-tune'' (see Algorithm \ref{alg:noise+ft}), which is an adapted version of \citet{DescentToDelete}'s perturbed gradient descent. Using Lemma \ref{lemma:scratch_speed}, such a bound immediately implies an upper bound on the unlearning complexity ratio.

\begin{theorem}[Efficient regime]\label{theo:up_unlearn}

For any $e<e_0$, we have
\begin{equation}
    \frac{T^U_e}{T^S_e} = \Ocal\left(\left(\frac{r_f}{1-r_f}\right)^2 \left(1+d\kdp^2\right) \frac{e_0}{e}\right)\,.
\end{equation}



\end{theorem}
The proof of Theorem \ref{theo:up_unlearn} relies on controlling the distance between the two optima $\norm{\vtheta^* - \vtheta_r^*}$ by a factor proportional to $r_f$, and using the classical convergence rate in $\Ocal(LR/\sqrt{T})$ for convex stochastic optimization when the optimum is within a distance $R$ from the initialization. Quite surprisingly, this convergence rate proportional to $r_f$ is obtained with the use of convergence rates from the convex literature, despite being in a strongly convex regime where faster rates in $\Ocal(L^2/\mu T)$ are usually favored.

Theorem \ref{theo:up_unlearn} shows that efficient unlearning, with an unlearning complexity ratio proportional to $r_f^2$, is possible.
For low forget ratios $r_f \ll 1$, the ``noise and fine-tune'' method outperforms retraining (\ie $T^U_e < T^S_e$) when the target excess risk is above a quantity proportional to $r_f^2 \left(1+d\kdp^2\right)e_0$, and we recover, up to a constant and for a fixed dimension $d$, the regime in which unlearning becomes possible in Theorem \ref{theo:lbunlearn} (see Section \ref{sec:imp_unlear}).

The combination of both Theorem \ref{theo:lbunlearn} and Theorem \ref{theo:up_unlearn} thus shows that $r_f^2 \left(1+d\kdp^2\right)e_0$ acts as a threshold for the target excess risk before which efficient unlearning is impossible, and above which unlearning becomes efficient (and even trivial beyond $r_f(r_f+\sqrt{d}\kdp)e_0$).


Moreover, while a large value of $\kdp$ negatively impacts the unlearning time, the converse is only true to a certain extent since a low value will not necessarily allow for immediate unlearning. This is due to the fact that, even with very large privacy constraints, the unlearning algorithm still has to cover the distance between the full optimum and the retain optimum.

\subsection{Discussion}\label{sec:discussion}

Overall, our analysis shows that there are three main regimes—Trivial, Impossible, and Efficient—that describe how unlearning time compares to retraining-from-scratch time, based on the target excess risk  $e$, the strength of the privacy constraint \(\kdp\) and the forget ratio  $r_f$. Figure \ref{fig:theo_diagram} illustrates these regimes and their boundaries.

Since we rely on noising the model parameters to ensure unlearning, our bound scales with $\sqrt{d}$, as is common in differentially-private optimization \cite{bassily2014private}. While natural, this dependence is not matched by our lower bound in Theorem \ref{theo:lbunlearn}. We leave the exploration of this discrepancy to future work.

In a realistic machine learning setting, one is usually interested in achieving an excess risk on the training set of the same order as the generalization error, which value if often around $1/n$, where $n$ is the size of the dataset \cite{journals/jmlr/BousquetE02}. In this context, forgetting one sample with an average unlearning budget (\ie $\kdp\approx1$) can be done fairly cheaply, since applying the bound from Theorem \ref{theo:lbunlearn} yields $T^U_e/T^S_e$ of the order of $r_f^2 d \frac{e_0}{e} \approx r_f^2 d n$, and one can thus remove a number of data points at most of the order $\sqrt{\frac{n}{d}}$. Moreover, if $r_f\propto 1/n$, then the unlearning complexity ratio is of the order $d/n$ and unlearning can be very efficient in under-parameterized settings.


\section{Experiments}

We investigate the global landscape of the unlearning complexity ratio as a function of key factors, including the accepted excess risk threshold $e$ and the unlearning budget $(\epsilon, \delta)$, which are jointly quantified by the constant $\kdp$.

\begin{figure}[t]
    \includegraphics[width=\textwidth/2]{result_6.pdf}
    \caption{Experimental phase diagram of the unlearning complexity ratio. We give estimates for $T^U_e$ and $T^S_e$ using the ``noise and fine-tune" (Algorithm \ref{alg:noise+ft}) and SGD algorithms, respectively. We display the value of their ratio as a function of $\kdp$ and $e$ in log-log scale. We notice the three regimes described in our theoretical analysis: impossible (\text{IR}), efficient (\text{\text{ER}}),  and trivial (\text{TR}). \label{fig:exp_diagram}}
\end{figure}




\subsection{Experimental Setting}
The goal of the experiment section is to validate the theoretical analysis presented in Section \ref{sec:results} by comparing the performance of unlearning and retraining on synthetic functions and datasets. 

In order to give an estimate of the unlearning complexity ratio, we need to choose specific algorithms to represent the learning algorithm class $\sA$ as well as the unlearning algorithm class $\sU$. For the learning algorithm, we choose stochastic gradient descent, as defined in \cite{garrigos2023handbook}, since it is known to achieve the optimal asymptotic speed of $\Theta(\frac{e_0}{e})$ (see proof of Lemma \ref{lemma:scratch_speed}). 

For the unlearning algorithm, we choose the ``noise and fine-tune'' algorithm (see Alg. \ref{alg:noise+ft}) since it is the one used to derive the bound in Theorem $\ref{theo:up_unlearn}$. 

We aim to learn linear regression models in $\sR^d$ (with even $d$). The expected loss function is
\[
\Lcal(\vtheta)
\;=\;
\frac{\mu}{2}\,\norm{\vtheta}^{2}
-
\frac{L}{4}\,\sE_{\xi\sim \D}[g(\xi)] \sum_{i=1}^{d/2} \theta_{i}
+
\frac{L}{4}\,\sum_{i=1+d/2}^{d}\abs{\theta_{i}},
\]
where $g:\sR^s\mapsto\{-1,1\}$ and $\theta_{i}$ represents the $i$-th coordinate of $\vtheta$. These functions are inspired from the ``worst-case'' functions used in our proofs to lower bound the convergence speed of algorithms. 
The dataset distribution $\D$ influences the loss solely through the resulting distribution of $g(\xi)$, where $g(\xi)$ is a Rademacher random variable with an expected value of $\sE[g] = \frac{1}{2\sqrt{T}}$, where $T$ denotes the number of time steps for which the experiment will be conducted. For this experiment, we set $L = 25$ and $\mu = 1$.





The retain and forget are obtained through the random splitting of the dataset into two parts of respective sizes $n-\floor{r_f*n}$ and $\floor{r_f*n}$. We consider $r_f= 10^{-2}$.
















The unlearning and retraining algorithms are performed for a fixed time $T$.
To produce the phase diagram of Fig.~\ref{fig:exp_diagram}, we choose $36$ values of $T$ logarithmically spread between $1$ and $10^6$. We also choose $36$ values of $e$ and $\kdp$ logarithmically spread between $10^{-2}$ and $10^2$.
For each value of  $T$, we generate a corresponding function $g$ and loss function $\Lcal$. We run our learning algorithms for $T$ stochastic gradient steps, recording the corresponding time every error threshold $e$ is passed. For each value of $\kdp$, we run our unlearning algorithm  for $T$ stochastic gradient steps, recording the corresponding time every error threshold $e$ is passed. We repeat this process $50$ times to get better estimations of the expected unlearning and retraining times.
Then, for each value of $e$, we average all of the obtained learning times. For each pair of value $(e, \kdp)$, we average all of the obtained unlearning times. The ratio of the average  unlearning time and of the average learning time is what we refer to as the empirical unlearning complexity ratio. By definition, this is set to $0$ if the average unlearning time is $0$. 





\subsection{Experimental Results}

Figure~\ref{fig:exp_diagram} illustrates the empirical complexity ratio.


The figure empirically demonstrates the existence of the different regimes theorized in Sec.~\ref{sec:results}.
More specifically, Theorem~\ref{theo:trivial_regime} predicts an affine linear dependency between $e$ and $\kdp$ at the frontier of the efficient regime and the trivial regime. In the log-log plot, this would appear as a line with slope $1$, which we can recognize in the level-curve corresponding to an empirical complexity ratio equal to $0.1$.\footnote{The fact that the level line  stops around $e=1$ is an artifact due to the discretization of the experimental setting.
}









The existence of the efficient regime~\text{ER} can also be noticed in Fig.~\ref{fig:exp_diagram}. A large portion of the figure shows values of the experimental unlearning complexity ratio well below~$1$. 
We observe that, while our experiments considered an optimal learning algorithm, the ``fine and tune'' unlearning algorithm may be suboptimal. One can then aspect that better unlearning algorithms can achieve even smaller complexity ratios.


Finally, we also observe the $\text{IR}$ regime at the top of the figure. Infact, the low slope of the level line at $0.5$ indicates a behaviour similar to what is theoretically predicted by Theorem~\ref{theo:up_unlearn} as a square root function has a slope of $0.5$ in log-log scale.




\section{Conclusion}

In this paper, we study the efficiency of machine unlearning through the lens of a novel metric —the \emph{unlearning complexity ratio}— which compares the worst-case convergence speeds of the best unlearning and retraining algorithms. Our analysis reveals three distinct regimes. In one $(\text{TR})$, we show that unlearning can be done ``for free" because perturbation the parameters is enough to achieve unlearning while keeping the excess risk low enough (Theorem \ref{theo:trivial_regime}). In another $(\text{IR})$, described by our lower bound on the unlearning complexity ratio (Theorem \ref{theo:lbunlearn}), unlearning cannot asymptotically beat retraining through gradient-based methods. In the last regime $(\text{ER})$, in between the other two, our upper bound on the unlearning complexity ratio shows that unlearning is possible at a small fraction of the cost of retraining, a cost that scales with the square of the fraction of forgotten samples (Theorem \ref{theo:up_unlearn}).

Empirical validation confirms these insights, showing the utility of analysing unlearning through the minimax complexity framework. Beyond unveiling fundamental limits and opportunities, our results address an open question on whether unlearning can outperform retraining—and under what circumstances. We introduce the first bounds on the unlearning complexity ratio, as well as the first lower bound on unlearning time.

We hope that the framework and findings presented here will stimulate further studies on machine unlearning in broader contexts, and allow further analysis of a wider class of algorithms and objective functions, as well as data distributions.
Specifically, lower-bounding the unlearning complexity ratio for methods beyond the first order, not verifying Assumption \ref{ass:flex}, or methods relying on the forget set, remains an open challenge. 

\section*{Impact Statement}

This paper presents work whose goal is to advance the field of 
Machine Learning and Unlearning. There are many potential societal consequences 
of our work, none which we feel must be specifically highlighted here.


\bibliography{main.bib}

\newpage
\appendix
\onecolumn
\section{Upper bounds}

\begin{proof}[Proof of Theorem \ref{theo:trivial_regime}]
Let $e \geq \frac{r_f}{1-r_f} \left( \frac{r_f}{1-r_f} + \sqrt{d}\kdp \right) e_0$.

Let the unlearning algorithm consist in simply adding Gaussian noise to the previous optimum $\vtheta^*$. By application of the Gaussian mechanism \cite{DP_book}, adding i.i.d. Gaussian noise with standard deviation $\kdp \norm{\vtheta^*_r-\vtheta^*}$ ensure $(\eps,\delta)$-Unlearning of the forget set. Using the bound from Lemma \ref{lemma:opt_dist}, we sample:
\begin{equation}
g \sim \Ncal(0, (\kdp \frac{r_f}{1-r_f} \frac{L}{\mu})^2 I_d),
\end{equation}
where $I_d$ is the identity matrix in $\sR^d$.

Let $\hat{\vtheta} \coloneqq \vtheta^* + g $. We can then bound the expected loss of $\hat{\vtheta}$:
\begin{align}
    \sE\left[\Lcal_r(\thetat)-\Lcal_r^*\right] &\le \sE\left[\Lcal_r(\thetat)-\Lcal_r(\vtheta^*)\right] + \Lcal_r(\vtheta^*)-\Lcal_r^* \\
    &\le \sqrt{d}\kdp \left(\frac{r_f}{1-r_f}\right)^2 \frac{L^2}{\mu} + \frac{r_f}{1-r_f} \frac{L^2}{\mu} \\
    &\le r_f \left( r_f + \sqrt{d}\kdp \right) e_0\\
    &\le e\,.
\end{align}
\end{proof}



\begin{proof}[Proof of Theorem \ref{theo:up_unlearn}]


According to Lemma \ref{lemma:opt_dist}, we have
\begin{equation}
    \norm{\vtheta^*-\vtheta^*_r} \le \frac{r_f}{1-r_f}\frac{L}{\mu} \eqqcolon R_1\,.
\end{equation}


To perform the unlearning, we use the noise + fine-tune method as introduced in Algorithm \ref{alg:noise+ft}. For the noising part, the standard deviation of the noise that needs to be added to ensure $(\eps, \delta)$-Unlearning of $\Df$ is $\kdp \norm{\vtheta^*-\vtheta^*_r}$. Thus, we set $\sigma = \kdp R_1$ and define $\widetilde{\vtheta} \coloneqq \vtheta^* + g$, where $g \sim \mathcal{N}(0, \sigma^2)$. %When needed, we will write $g \coloneqq \norm{\frac{g}{\kdp R_1}}_2 \sim \sqrt{d} \cdot \Ncal(0,1)$.

Now, one can notice that
\begin{align}
    \sE\left[\norm{\thetilde-\vtheta^*_r}^2\right] = \sE\left[\norm{g}^2\right] + \norm{\vtheta^*-\vtheta^*_r}^2 \le (1+ d\kdp^2) R_1^2\,.
\end{align}

While we do not know the exact distance $\norm{\thetilde-\vtheta^*_r}$ our SGD will need to cover, we have its expectation. Thus, we set the learning rate $\gamma$ to be optimal for a the expectation of the distance, \ie: \( \gamma = \sqrt{\frac{(1+d \kdp^2) R_1^2 }{T^U L^2}}\).

Let $\Acal_\gamma$ be the SSD algorithm with learning rate $\gamma$, as defined in Section 3 of \citet{garrigos2023handbook}.
Using Theorem 9.7 from \citet{garrigos2023handbook}, we get 

\begin{align}
        \sE \left[\Lcal_r(\Acal_\gamma(\thetilde, \Dr, T))-\Lcal^*_{r}\right] &\le \sE\left[\frac{\norm{\thetilde-\vtheta^*_r}^2}{2 \gamma T} + \frac{\gamma L^2}{2}\right] \\
        &\le \frac{(1 + d\kdp^2) R_1^2}{2 \gamma T} + \frac{\gamma L^2}{2} \\
        &\le \frac{L R_1}{\sqrt{T}} \sqrt{1+d\kdp^2}\,.
\end{align}

For a given excess risk threshold $e$, the unlearning time can then be upper-bounded as

\begin{equation}
    T^U_e \le \frac{L^2 R_1^2}{e^2} (1 + d\kdp^2) = \left(\frac{r_f}{1-r_f}\right)^2 (1 + d\kdp^2) \left(\frac{e_0}{e}\right)^2\,.
\end{equation}



\end{proof}


\section{Lower bounds}\label{app:lower_bounds}
The lower bounds of Section \ref{sec:results} rely on three steps: 1) defining a class of objective functions $\Lcal^g$ for $g:\R^s\to\{-1,1\}$ such that their optimum over $\D$ does not provide any information on the dataset $\Dr$, 2) showing that two such functions $\Lcal^g$ and $\Lcal^{-g}$ have optimums over $\Dr$ distant from one another, and 3) showing that any algorithm's output will behave nearly identically on both $\Lcal^g$ and $\Lcal^{-g}$, thus leading to the impossibility of having both functions efficiently optimized by the same algorithm.

In what follows, for any function $g:\R^s\to[-1,1]$, we denote as $\Lcal^g(\vtheta) = \E{\ell^g(\vtheta,\xi)}$ where $\ell^g$ is a loss function such that
\begin{equation}
\ell^g(\vtheta,\xi) = \frac{\mu}{2}\|\vtheta\|^2 - \frac{L}{2}g(\xi)\vtheta_1\,,
\end{equation}
where $\vtheta_1$ is the first coordinate of $\vtheta$ in the canonical basis of $\sR^d$.
By definition, $\nabla_\vtheta\ell^g(\vtheta,\xi) = \mu\vtheta - Lg(\xi)e_1/2$ where $e_1$ is the first vector of the canonical basis of $\R^d$, and $\ell^g$ is $L$-Lipschitz and $\mu$-strongly convex.
Moreover, the objective function on $\Dr$ is $\Lcal^g_r(\vtheta) = \frac{\mu}{2}\|\vtheta\|^2 - \frac{L}{2}\E{g(\xi')}\vtheta_1$, where $\xi'\sim\Dr$, and thus the minimizer of $\Lcal_r^g$ is $\vtheta^*_{g,r} = \frac{L}{2\mu}\E{g(\xi')}e_1$ (note that $\|\vtheta^*_{g,r}\| = \frac{L}{2\mu}|\E{g(\xi')}|\leq R$).
We now show that, provided we find two functions $g,g'$ such that the output of any algorithm is (statistically) almost indistinguishable, then minimizing both $\Lcal^g$ and $\Lcal^{g'}$ beyond a certain quantity is impossible.
To properly define this \emph{indistinguishability}, we will use the \emph{total variation} distance $\dN{TV}(P,Q) = \sup_{A\subset\R^s}\left| P(A) - Q(A) \right|$ for two probability distributions $P$ and $Q$.

\begin{lemma}
\label{lemma:cvLg}
Let $g,g':\R^s\to[-1,1]$ two functions, $\theta_0, \theta_0'\in\R^d$ two initial parameters, and $A\in\boldsymbol{A}$ an algorithm. Then
\begin{equation}
\sup_{g''\in\{g,g'\}}\E{\Lcal_r^{g''}(\vtheta^A_T(\vtheta_0, \ell^{g''}, \Dr)) - {\Lcal_r^{g''}}^*} \geq \frac{L^2 (\E{g(\xi')}-\E{g'(\xi')})^2}{32 \, \mu} (1-\dN{TV}(P_g,P_{g'}))\,,
\end{equation}
where $\xi'\sim\Dr$, ${\Lcal_r^{g''}}^* = \min_{\vtheta\in\R^d}\Lcal_r^{g''}(\vtheta)$ and $P_g$ (resp. $P_{g'}$) is the probability distribution of $\vtheta^A_T(\vtheta_0, \ell^g, \Dr)$ (resp. $\vtheta^A_T(\vtheta_0', \ell^{g'}, \Dr)$).
\end{lemma}
\begin{proof}
First, note that $\vtheta^*_{g,r} = \frac{L}{2\mu}\E{g(\xi')}e_1$ and thus $\Lcal_r^g(\vtheta) - \Lcal_r^{g^*} = \frac{\mu}{2}\|\vtheta^*_{g,r} - \vtheta\|^2$. Using the optimal transport definition of total variation (see \eg \citealt{villani2009}), $\dN{TV}(P,Q) = \inf_{(X,Y)} \sP(X\neq Y)$ where the infimum is taken over all couplings of $P$ and $Q$. As a consequence, there exists two random variables $\vtheta_1\sim P_g$ and $\vtheta_2\sim P_{g'}$, and such that $\sP(\vtheta_1 \neq \vtheta_2) = \dN{TV}(P_g,P_{g'})$, leading to

\begin{equation}
\begin{array}{lll}
\sup_{g''\in\{g,g'\}} \E{\norm{\vtheta^*_{g'',r}-\vtheta^A_T(\vtheta_0, \ell^{g''}, \Dr)}^2} &=& \max\left\{ \E{\norm{\vtheta^*_{g,r} - \vtheta_1}^2},\, \E{\norm{\vtheta^*_{g',r} - \vtheta_2}^2} \right\}\\
&\geq& (1-\dN{TV}(P_g,P_{g'}))\max\left\{ \norm{\vtheta^*_{g,r} - \tilde{\vtheta}}^2,\, \norm{\vtheta^*_{g',r} - \tilde{\vtheta}}^2\right\}\\
&\geq& (1-\dN{TV}(P_g,P_{g'}))\frac{\norm{\vtheta^*_{g,r} - \vtheta^*_{g',r}}^2}{4}\,,
\end{array}
\end{equation}
where $\tilde{\vtheta} = \E{\vtheta_1 ~|~ \vtheta_1=\vtheta_2} = \E{\vtheta_2 ~|~ \vtheta_1=\vtheta_2}$. Finally, using the formula for $\vtheta^*_{g,r} = \frac{L}{2\mu}\E{g(\xi')}e_1$ and $\Lcal_r^g(\vtheta) - \Lcal_r^{g^*} = \frac{\mu}{2}\|\vtheta^*_{g,r} - \vtheta\|^2$ gives the desired result.
\end{proof}

We now show that a particular choice of functions $g,g'$ leads to almost indistinguishable outputs.

\begin{lemma}
\label{lemma:tvbound}
Assume that $\forall\gamma\in[0,1]$, there exists $A^\gamma\subset\text{supp}(\Dr)$ such that $\sP_{\Dr}(A)=(1+\gamma)/2$. Let $g^\gamma(\xi) = 2\1\{\xi\in A^{\gamma}\} - 1$ if $\xi\in\text{supp}(\Dr)$, and $g^\gamma(\xi) = -\min\{1, \frac{(1-r_f)\gamma}{r_f}\}$ otherwise. Then, we have
\begin{equation}
\dN{TV}\left(\Ucal(T, \ell^{g^\gamma}, \Dr, \Df),\, \Ucal(T, \ell^{g^0}, \Dr, \Df)\right) \leq \frac{\pi\gamma \sqrt{T}}{4} + \frac{((1-r_f)\gamma - r_f)_+}{2r_f}(e^\epsilon - 1 + \delta)\,.
\end{equation}
\end{lemma}
\begin{proof}
First, note that the minimizer of $\Lcal^\gamma$ is $\vtheta^*_{\gamma} = \frac{L}{2\mu}((1-r_f)\gamma - r_f)_+ e_1$, and we thus have $\Ucal(T, \ell^{g^\gamma}, \Dr, \Df) = \vtheta^A_T(\vtheta^*_\gamma, \ell^{g^\gamma}, \Dr)$ and $\Ucal(T, \ell^{-g^\gamma}, \Dr, \Df) = \vtheta^A_T(-\vtheta^*_\gamma, \ell^{-g^\gamma}, \Dr)$.

To ease the notations, we denote by $\vtheta_{k,l} = \vtheta^A_T(\frac{(1-r_f)L\gamma_{2k+1}}{2\mu} e_1, \ell^{g^{\gamma_{2l}}}, \Dr)$ the output of algorithm $A$ on the function $\ell^{g^{\gamma_{2l}}}$ starting at $\vtheta_0=\frac{(1-r_f)L\gamma_{2k+1}}{2\mu} e_1$, where $\gamma_k = \left(\gamma - \frac{k r_f}{1-r_f}\right)_+$. Let $K = \left\lceil \frac{(1-r_f)\gamma}{2r_f}\right\rceil$, then we have, by triangular inequality,
\begin{equation}
\dN{TV}(\vtheta_{0, 0}, \vtheta_{K, K}) \leq \sum_{k=0}^{K-1} \dN{TV}(\vtheta_{k, k}, \vtheta_{k, k+1}) + \sum_{k=0}^{K-1} \dN{TV}(\vtheta_{k, k+1}, \vtheta_{k+1, k+1})\,.
\end{equation}

By construction, we have $\vtheta_{0,0} = \Ucal(T, \ell^{g^\gamma}, \Dr, \Df)$ and $\vtheta_{K, K} = \Ucal(T, \ell^{g^0}, \Dr, \Df)$. We now show that both sums can be bounded: the first using the fact that the $T$ gradients $g^\gamma(\xi_t)$ for $t\in\set{0,T-1}$ are close in total variation distance (\ie Lemma \ref{lemma:binvsbin}), and the second using the $(\epsilon, \delta)$-Unlearning constraint on $\Ucal$.






By Lemma \ref{lemma:phiA_measurable}, there is a measurable function $\varphi_A$ such that $\vtheta_{k,l} = \varphi_A\left(\frac{(1-r_f)L\gamma_{2k+1}}{2\mu}, Z^l_0,\dots, Z^l_{T-1}, \omega\right)$ where $Z^l_t = (1+g^{\gamma_{2l}}(\xi_t))/2$ are i.i.d. Bernoulli random variables of parameter $\frac{1+\gamma_{2l}}{2}$. As $\vtheta_{k,k}$ and $\vtheta_{k,k+1}$ are outputs of the same algorithm initialized at the same starting position, we have
\begin{equation}
\begin{array}{lll}
\sum_{k=0}^{K-1} \dN{TV}(\vtheta_{k, k}, \vtheta_{k, k+1}) &=& \sum_{k=0}^{K-1} \dN{TV}\left((Z^{k}_0,\dots,Z^{k}_{T-1}),\, (Z^{k+1}_0,\dots,Z^{k+1}_{T-1})\right)\\
&=& \dN{TV}\left(\mbox{Bin}\left(T, \frac{1+\gamma_{2k}}{2}\right), \mbox{Bin}\left(T, \frac{1+\gamma_{2k+2}}{2}\right)\right)\\
&=& \sum_{k=0}^{K-1} \frac{\sqrt{T}}{2} \left|\tan^{-1}\left(\frac{\gamma_{2k+2}}{\sqrt{1-{\gamma_{2k+2}}^2}}\right) - \tan^{-1}\left(\frac{\gamma_{2k}}{\sqrt{1-\gamma_{2k}^2}}\right)\right|\\
&=& \frac{\sqrt{T}}{2} \left|\tan^{-1}\left(\frac{\gamma_{2K}}{\sqrt{1-{\gamma_{2K}}^2}}\right) - \tan^{-1}\left(\frac{\gamma_0}{\sqrt{1-\gamma_0^2}}\right)\right|\\
&=& \frac{\sqrt{T}}{2} \tan^{-1}\left(\frac{\gamma_{2K}}{\sqrt{1-{\gamma_{2K}}^2}}\right)\\
&\leq &\frac{\pi\gamma \sqrt{T}}{4}\,,
\end{array}
\end{equation}
using the fact that $f:x\mapsto \tan^{-1}\left(\frac{x}{\sqrt{1-x^2}}\right)$ is increasing and convex on $x\in[0,1]$, and $f(0) = 0$ and $f(1)=\pi/2$.





















Finally, let $\Df'$ be a probability distribution on $\R^s$ such that $\text{supp}(\Df') \cap (\text{supp}(\Dr) \cup \text{supp}(\Df)) = \emptyset$, for any $\gamma\in[0,1]$, let $\tilde{g}^\gamma(\xi) = g^\gamma(\xi)$ if $\xi\in\text{supp}(\Dr)\cup\text{supp}(\Df)$, and $\tilde{g}^\gamma(\xi) = 1$ otherwise. Then, we have $\Ucal(T, \ell^{\tilde{g}^{\gamma_{2k}}}, \Dr, \Df) = \vtheta_{k,k}$ and $\Ucal(T, \ell^{\tilde{g}^{\gamma_{2k}}}, \Dr, \Df') = \vtheta_{k-1,k}$. Thus, we have
\begin{equation}
\begin{array}{lll}
\sum_{k=0}^{K-1} \dN{TV}(\vtheta_{k, k+1}, \vtheta_{k+1, k+1}) &=& \sum_{k=0}^{K'-1} \dN{TV}\left(\Ucal(T, \ell^{\tilde{g}^{\gamma_{2k+2}}}, \Dr, \Df),\,\Ucal(T, \ell^{\tilde{g}^{\gamma_{2k+2}}}, \Dr, \Df')\right)\\
&\leq& \sum_{k=0}^{K'-1} (e^\epsilon - 1 + \delta)\\
&=& K'(e^\epsilon - 1 + \delta)\,,
\end{array}
\end{equation}
where $K' = \left\lceil \frac{((1-r_f)\gamma - r_f)_+}{2r_f}\right\rceil$. Combining the two inequalities concludes the proof.
\end{proof}

We are now in position to prove Theorem \ref{theo:lbunlearn}.
\begin{proof}[Proof of Theorem \ref{theo:lbunlearn}]
Combining Lemma \ref{lemma:cvLg} (with $g=g^\gamma$ and $g'=-g^\gamma$) and Lemma \ref{lemma:tvbound}, we have, for any $\gamma\in[0,1]$,
\begin{equation}
\min_{\Ucal \in \sU} \; \max_{\Lcal \in \Fcal_{sc}} \E{\Lcal_r(\Ucal(T, \ell, \Dr, \Df)) - \Lcal_r^*} \geq \frac{L^2 \gamma^2}{8 \, \mu} (1-\dN{TV}(P_{g^\gamma},P_{-g^\gamma}))\,,
\end{equation}
where $\dN{TV}(P_{g^\gamma},P_{-g^\gamma}) \leq \dN{TV}(P_{g^\gamma},P_{g^0}) + \dN{TV}(P_{g^0},P_{-g^\gamma}) \leq \frac{\pi\gamma \sqrt{T}}{2} + \frac{((1-r_f)\gamma - r_f)_+}{r_f}(e^\epsilon - 1 + \delta)$.












Let $c_1,c_2\in[0,1]$ and $\gamma = c_1/\sqrt{T}$. If $\left(\frac{(1-r_f)c_1}{r_f\sqrt{T}} - 1\right)_+(e^\epsilon - 1 + \delta) \leq c_2$, then
\begin{equation}
\min_{\Ucal \in \sU} \; \max_{\Lcal \in \Fcal_{sc}} \E{\Lcal_r(\Ucal(T, \ell, \Dr, \Df)) - \Lcal_r^*} \geq \frac{L^2 c_1^2}{8 \, \mu T} (1-\frac{\pi c_1}{2} - c_2)\,,
\end{equation}
and thus, if $\left(\frac{(1-r_f)\sqrt{8\mu e}}{r_f L\sqrt{1 - \frac{\pi c_1}{2} - c_2}} - 1\right)_+(e^\epsilon - 1 + \delta) \leq c_2$,
\begin{equation}
T^U_e \geq \frac{L^2 c_1^2}{8 \, \mu e} (1-\frac{\pi c_1}{2} - c_2)\,.
\end{equation}
Finally, we take $c_2 = 1/2$, $c_1$ such that $1-\frac{\pi c_1}{2} - c_2 = 1/3$, and rewrite the condition as $e \leq \frac{r_f^2 L^2}{8 \, (1-r_f)^2\mu} (1-\frac{\pi c_1}{2}-c_2)\left(1+\frac{c_2}{e^\epsilon - 1+\delta}\right)^2$. A simple functional analysis gives that, for $10^{-8} \leq \delta \leq \epsilon$, we have 
\begin{equation}
1+\frac{1}{2(e^\epsilon - 1+\delta)} \geq 1+\frac{1}{2(e^\epsilon - 1+\epsilon)} \geq c_3 \left(1+\frac{\sqrt{2\ln(1.25\cdot 10^8)}}{\epsilon}\right) \geq c_3 \left(1+\frac{\sqrt{2\ln(1.25/\delta)}}{\epsilon}\right)\,,
\end{equation}
where $c_3 = 1/\sqrt{32\ln(1.25\cdot 10^8)}$ and the desired result.
\end{proof}

Using the same approach, a lower bound on the time complexity of scratch can also be derived.

\begin{proof}[Proof of Lemma \ref{lemma:scratch_speed}]
First, by strong convexity, we have
\begin{equation}
\Lcal_r(0) - \Lcal_r^* \leq \inner{\nabla\Lcal(0)}{\vtheta_r^*} - \frac{\mu}{2}\norm{\vtheta_r^*}^2\,.
\end{equation}
Moreover, the convexity of $\vtheta\mapsto\Lcal_r(\vtheta) - \frac{\mu}{2}\norm{\vtheta}^2$ implies that $\inner{\nabla\Lcal(-\vtheta_r^*) + \mu\vtheta_r^* - \nabla\Lcal(0)}{-\vtheta_r^*} \geq 0$ and thus
\begin{equation}
\Lcal_r(0) - \Lcal_r^* \leq \norm{\nabla\Lcal(-\vtheta_r^*)}\norm{\vtheta_r^*} - \frac{3\mu}{2}\norm{\vtheta_r^*}^2 \leq LR - \frac{3\mu}{2}R^2 = \frac{L^2}{8\mu}\,.
\end{equation}
As a consequence, if $e\geq e_0$, then $T^S_e=0$ (and $T^U_e=0$).

Let us now assume that $e< e_0$. First, note that this convergence rate is achieved by stochastic gradient descent. For example, a direct extension of Theorem 6.2 from \citet{bubeck2015convex} gives, after $T$ iterations of (stochastic) gradient descent $\theta_{t+1} = \theta_t - \eta_t \nabla_\theta\ell(\theta_t, \xi_t)$ with decreasing step-size $\eta_t = \frac{2}{\mu(t+2)}$.
\begin{equation}
\E{\Lcal\left(\tilde{\theta}_T\right) - \min_{\theta\in\R^d}\Lcal(\theta)} \leq \frac{2L^2}{\mu (T+2)}\,,
\end{equation}
where $\tilde{\theta}_T = \sum_{t=0}^{T-1} \frac{2(1+t)}{(T+1)(T+2)}\theta_t$, and thus
\begin{equation}
T^S_e \leq \frac{2L^2}{\mu e}\,.
\end{equation}

The lower bound is a consequence of Theorem \ref{theo:lbunlearn} with $\kdp=0$, as an algorithm retraining from scratch would not depend on the forget dataset, and thus have absolute privacy. In particular, if $e \leq \frac{L^2}{8 \, \mu} (1-\frac{\pi c_1}{2})$, we have
\begin{equation}
T^S_e \geq \frac{L^2 c_1^2}{8 \, \mu e} (1-\frac{\pi c_1}{2})\,,
\end{equation}
and as soon as $e/e_0 \leq 1-\eta$ for $\eta > 0$, there exists a constant $c_2>0$ such that $T^S_e \geq c_2 e_0/e$.
\end{proof}


\section{Useful lemmas}
In this section, we provide five lemmas that will be necessary to prove our upper and lower bounds (see sections above), as well as the proof for unlearning definition equivalence.


\begin{lemma}
\label{lemma:opt_dist}
Let $\vtheta^*_r = \arg\min_{\vtheta} \Lcal_r(\vtheta)$. Then, we have:
\begin{equation}
    \norm{\vtheta^*-\vtheta^*_r} \le \frac{r_f}{1-r_f}\cdot\frac{L}{\mu}\,.
\end{equation}
\end{lemma}
\begin{proof}
By strong convexity of $\Lcal_r$, we have $\norm{\vtheta^*-\vtheta^*_r} \le \frac{\norm{\nabla\Lcal_r(\vtheta^*)}}{\mu}$. Moreover, $\norm{\nabla\Lcal_r(\vtheta^*)} = \norm{\E{\nabla\ell(\vtheta^*,\xi_r)}} = \norm{-\frac{r_f}{1-r_f}\E{\nabla\ell(\vtheta^*,\xi_r)}} \leq \frac{r_f}{1-r_f}L$ where $\xi_r\sim\Dr$ and $\xi_f\sim\Df$, as $\nabla\Lcal(\vtheta^*) = 0$. Combining the two inequalities gives the desired result.
\end{proof}

\begin{lemma}
\label{lemmaopt_loss}
\begin{equation}
    \Lcal_r(\vtheta^*) - \Lcal_r^* \le \left(\frac{r_f}{1-r_f}\right)^2\frac{L^2}{\mu}
\end{equation}
\end{lemma}
\begin{proof}
    Let $\vtheta^*_r = \arg\min_{\vtheta} \Lcal_r(\vtheta)$. Then,
    \begin{align}
        \sE_{\Dr}\left(\ell(\vtheta^*) - \ell(\vtheta^*_r)\right) &= \sE_{\D}\left(\ell(\vtheta^*) - \ell(\vtheta^*_r)\right) - \frac{r_f}{1-r_f}\sE_{\Df}\left(\ell(\vtheta^*) - \ell(\vtheta^*_r)\right) \\
    &\le - \frac{r_f}{1-r_f}\sE_{\Df}\left(\ell(\vtheta^*) - \ell(\vtheta^*_r)\right) \\
    &\le \frac{r_f}{1-r_f}L\norm{\vtheta^*-\vtheta^*_r} \\
    &\le \left(\frac{r_f}{1-r_f}\right)^2\frac{L^2}{\mu},
    \end{align}
where the last inequality is given by Lemma \ref{lemma:opt_dist}.
\end{proof}




\begin{lemma}
\label{lemma:tv_dp}
If the unlearning algorithm $\Ucal$ verifies $(\epsilon,\delta)$-Unlearning, then, for any triplet of distributions $(\Dr,\Df,\Df')$, we have
\begin{equation}
\dN{TV}\left( \Ucal(\Dr,\Df),\, \Ucal(\Dr,\Df') \right) \leq e^\epsilon - 1 + \delta\,.
\end{equation}
\end{lemma}
\begin{proof}
By $(\epsilon,\delta)$-Unlearning, we have, for any $S\subset\R^s$, $\sP[\Ucal(\Dr, \Df) \in S] \leq e^{\epsilon} \cdot \sP[\Ucal(\Dr, \Df') \in S] + \delta$, and thus
\begin{equation}
\sP[\Ucal(\Dr, \Df) \in S] - \sP[\Ucal(\Dr, \Df') \in S] \leq (e^{\epsilon}-1) \cdot \sP[\Ucal(\Dr, \Df') \in S] + \delta \leq e^\epsilon - 1 + \delta\,.
\end{equation}
The converse relation with $\Df$ and $\Df'$ exchanged leads to a bound on the absolute value, and thus the desired result.
\end{proof}

\begin{proof}[Proof of Lemma \ref{lemma:MU_def}]
Let $\Dcal_0$ be an arbitrary distribution, \eg the uniform distribution on the $R/2$-ball.
Let $\Ucal\in\sU$ be an \((\eps,\delta)\)-Unlearning algorithm. Let $\Ucal \in \sU$ be an \((\eps,\delta)\)-Unlearning algorithm.
Then, the algorithm $\Acal_0: (T, l, \Dr) \longmapsto \Ucal(T, l, \Dr, \D_0)$ is such that for any couple of distributions $(\Dr, \Df)$ over $\sB(0,R)$ and subset $S \subset \sR^d$,
 \begin{align*}
\sP[\Ucal(\Dr, \Df) \in S] &\leq e^{\epsilon} \cdot \sP[\Ucal(\Dr, \D_0) \in S] + \delta = e^{\epsilon} \cdot \sP[\Acal_0(\Dr) \in S] + \delta, \\
\sP[\Acal_0(\Dr) \in S] = \sP[\Ucal(\Dr, \D_0) \in S] &\leq e^{\epsilon} \cdot \sP[\Ucal(\Dr, \Df) \in S] + \delta.
\end{align*}
$\Ucal$ is thus an \((\eps,\delta)\)-Reference Unlearning algorithm.
This proves the first implication.

Let $\Ucal\in\sU$ be an \((\eps,\delta)\)-Reference Unlearning algorithm and $\Acal \in \sA$ its reference algorithm.
Let $\Dr, \Df, \Df' $ be three distributions over $\sB(0,R)$.
Then,
\begin{align}
    \sP[\Ucal(\Dr, \Df) \in S] &\leq e^{\epsilon} \cdot \sP[\Acal(\Dr) \in S] + \delta \leq e^{\epsilon}\left(e^{\epsilon} \cdot \sP[\Ucal(\Dr, \Df') \in S] + \delta\right) + \delta
\end{align}
$\Ucal$ is thus an \((2\eps,(1+\exp(\eps))\delta)\)-Reference Unlearning algorithm.
This concludes the proof.
\end{proof}

\begin{lemma}
\label{lemma:phiA_measurable}
Let $A\in\boldsymbol{A}$ be an iterative algorithm as defined in Algorithm \ref{alg:learn}. Then, there exists $T$ i.i.d. random variables $\xi_t\sim\Dr$ and a measurable function $\varphi_\Acal$ such that, for any $T>0$, $\theta_0\in\R^d$ and $g:\R^s\to\{-1,1\}$, we have 
\begin{equation}
\vtheta^A_T(\vtheta_0, \ell^g, \Dr) = \varphi_A(\vtheta_0, g(\xi_0), \dots, g(\xi_{T-1}), \omega)\,.
\end{equation}
\end{lemma}
\begin{proof}
Let $(m_t, \vtheta_t)$ be the memory state and current parameter of algorithm $A$ at iteration $t$ (see \algref{alg:learn}). First, for $T=0$, $m_0 = \emptyset$ and $\vtheta_0$ are both (trivially) measurable functions of $\vtheta_0$. Then, by recursion, if both $m_{T-1}$ and $\vtheta_{T-1}$ are measurable functions of $\vtheta_0,g(\xi_0), \dots, g(\xi_{T-2}), \omega$, then
\begin{equation}
(m_T, \vtheta_T) = A(m_t, \nabla\ell^g(\vtheta_{T-1},\xi_{T-1}),\omega) = A(m_{T-1},\, \mu\vtheta_{T-1} - Lg(\xi_{T-1})e_1/2,\, \omega)\,,
\end{equation}
which is a measurable function of $\vtheta_0,g(\xi_0), \dots, g(\xi_{T-1}), \omega$. This concludes the proof.
\end{proof}



\begin{lemma}
\label{lemma:binvsbin}
Let $T\geq 0$ and $\gamma, \gamma'\in[-1,1]$. Then, we have
\begin{equation}
\dN{TV}\left(\mbox{Bin}\left(T, \frac{1+\gamma}{2}\right), \mbox{Bin}\left(T, \frac{1+\gamma'}{2}\right)\right) \leq \frac{\sqrt{T}}{2} \left|\tan^{-1}\left(\frac{\gamma'}{\sqrt{1-{\gamma'}^2}}\right) - \tan^{-1}\left(\frac{\gamma}{\sqrt{1-\gamma^2}}\right)\right|\,.
\end{equation}
\end{lemma}
\begin{proof}
Assume that $\gamma' \geq \gamma$, and let $\varphi(\gamma,\gamma') = \dN{TV}\left(\mbox{Bin}\left(T, \frac{1+\gamma}{2}\right), \mbox{Bin}\left(T, \frac{1+\gamma'}{2}\right)\right)$. The proof relies on bounding the derivative of $\varphi$ with respect to its second variable. Let $\gamma' = \gamma + \varepsilon$ where $\varepsilon > 0$, then
\begin{equation}
\begin{array}{lll}
\varphi(\gamma,\gamma + \varepsilon) &=& \frac{1}{2}\E{\left|1 - \frac{P_{\gamma+\varepsilon}(X)}{P_{\gamma(X)}}\right|}\\
&=& \frac{1}{2}\E{\left|1 - \frac{(1+\gamma+\varepsilon)^X (1-\gamma-\varepsilon)^{T-X}}{(1+\gamma)^X (1-\gamma)^{T-X}}\right|}\\
&=& \frac{1}{2}\E{\left|1 - (1+X\frac{\varepsilon}{1+\gamma}) (1-(T-X)\frac{\varepsilon}{1-\gamma}) + O(\varepsilon^2)\right|}\\
&=& \frac{\varepsilon}{2(1+\gamma)}\E{\left|X - (T-X)\frac{1+\gamma}{1-\gamma}\right|} + O(\varepsilon^2)\\
&\leq& \frac{\varepsilon}{(1+\gamma)}\sqrt{\frac{\mbox{Var}(X)}{(1-\gamma)^2}} + O(\varepsilon^2)\\
&=& \frac{\varepsilon}{2}\sqrt{\frac{T}{1-\gamma^2}} + O(\varepsilon^2)\,,
\end{array}
\end{equation}
where $P_\gamma$ is the density of the binomial distribution $\mbox{Bin}\left(T, \frac{1+\gamma}{2}\right)$, and $X\sim\mbox{Bin}\left(T, \frac{1+\gamma}{2}\right)$. As the total variation distance verifies the triangular inequality, we have
\begin{equation}
\dN{TV}\left(\mbox{Bin}\left(T, \frac{1+\gamma}{2}\right), \mbox{Bin}\left(T, \frac{1+\gamma'}{2}\right)\right) \leq \int_{u=\gamma}^{\gamma'} \varphi(u, u+du) \leq \frac{\sqrt{T}}{2} \left(\tan^{-1}\left(\frac{\gamma'}{\sqrt{1-{\gamma'}^2}}\right) - \tan^{-1}\left(\frac{\gamma}{\sqrt{1-\gamma^2}}\right)\right)\,.
\end{equation}
\end{proof}











































































































































































\end{document}

\typeout{get arXiv to do 4 passes: Label(s) may have changed. Rerun}

