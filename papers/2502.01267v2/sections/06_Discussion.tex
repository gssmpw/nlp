% 
With CST we move away from the standard, idealized comparison used in ST and other discrimination testing tools by operationalizing fairness given the difference. 
The results are promising but there are limitations to CST worth considering for future work.
In this section, we discuss the main ones.

\paragraph{The reliability of $\mathcal{D}^{CF}$.}
In this work, we do not constrain the generation of the counterfactual dataset. 
In principle, it is not an issue as the counterfactual distribution that results from intervening a given SCM $\mathcal{M}$ is unique and is the closest possible parallel world to the factual world \parencite{KarimiKSV2020_AlgoRecourseImperfectInfo, Woodward2005MakingThigsHappen}.
The resulting $\mathcal{D}^{CF}$ is reliable conditional on what is assumed by the SCM $\mathcal{M}$. 
Therefore, if the auxiliary causal knowledge is biased, then so will the counterfactual distribution and, in turn, $\mathcal{D}^{CF}$. 
What amounts to an unbiased SCM $\mathcal{M}$ is debatable when modeling humans behavior.
We come back to this point in the next theme.

Even if we do have access to an unbiased SCM $\mathcal{M}$, however, it is still difficult to judge what amounts to a reliable $\mathcal{D}^{CF}$ when our point of reference is the factual $\mathcal{D}$.
The issue here is accepting what $\mathcal{D}^{CF}$ represents based on our believes about $\mathcal{D}$. 
In Section~\ref{sec:Experiments.IllustrativeExample}, e.g.,~we find ourselves in a setting in which we have access to the data generating model of $\mathcal{D}$. 
The $\mathcal{D}^{CF}$, thus, represents the what would have been of the female applicants.
In Table~\ref{table:k-results} we observe the impact of considering the complainant and its counterfactual, observing how using CST increases the number of cases detected relative to ST and how such cases are penalized once we consider the statistical significance of $\Delta p$.
In Table~\ref{table:k-results}, one could argue that these counterfactuals are not reliable as they lead to non-statistically significant cases. 
One could also, though, argue that it is counterintuitive and even counterproductive to measure the reliability of the counterfactual world using the factual world.
%
In particular,
when it comes to indirect discrimination testing and its goal of achieving substantive equality \parencite{Wachter2020BiasPreserving}, 
we argue that it is conceivable to generate counterfactual distributions that cannot be other than non-representative of the current, non-neutral status quo behind the factual world. 
This is the case in Section~\ref{sec:Experiments.IllustrativeExample}.
For this reason alone, we expect \textit{ceteris paribus} over \textit{mutatis mutandis} to remain the preferred manipulation in discrimination testing as the idealized comparison is easier to motivate.
Further research is needed for defining what we mean, or want to mean, when speaking of a reliable $\mathcal{D}^{CF}$ when testing for discrimination, especially, if we wish to implement the \textit{mutatis mutandis} manipulation.

\paragraph{Auxiliary causal knowledge as groundtruth.}
Similarly, in this work we take as a given the derivation of the SCM $\mathcal{M}$, emphasizing it as a product of stakeholder engagement.
As with any model, the closer $\mathcal{M}$ is to the groundtruth, the better and more reliable is $\mathcal{D}^{CF}$ and, in turn, the results from CST.
This problem is often viewed in terms of missing confounders (see, e.g.,~\textcite{DBLP:conf/uai/KilbertusBKWS19}), such that what is missing are variables according to a theoretical model or domain expert. 
Given our focus on defining CST, we take a practical approach to the potential biasedness of the SCM $\mathcal{M}$, viewing it as evidence that is required from and that requires agreement among the stakeholders involved. 
$\mathcal{D}$ delimits the worldview of the discrimination context. Missing information, such as a confounder, should be addressed by the stakeholders when drawing $\mathcal{M}$ given $\mathcal{D}$.

Coming back to the first theme, the idea of groundtruth for describing human behavior is an open discussion within the fair ML community (see, e.g.,~\textcite{Hu_facct_sex_20, Kasirzadeh2021UseMisuse}). 
We agree that to speak of groundtruth in discrimination testing can be misleading, but that does not diminish the usefulness of using the SCM $\mathcal{M}$ for answering counterfactual questions.
As long as the SCM $\mathcal{M}$ is agreed upon by the stakeholders, we argue that the question on whether it amounts to groundtruth or not is secondary.\footnote{For instance, consider that there is already a similar and still unresolved discussion on what it means for individuals to be measurably similar between each other \parencite{Westen1982EmptyEquality}.}
%
That said, even if the stakeholders agree on a SCM $\mathcal{M}$, it is still possible for other SCMs to describe $\mathcal{D}$.
In this work, by always considering a single SCM $\mathcal{M}$, we implicitly work with faithful auxiliary causal knowledge \parencite{Peters2017_CausalInference}.
%
What happens when, e.g., the stakeholders agree on or are open to multiple SCMs?
%
We would find ourselves in a competing worlds setting for CST in which the sensitivity of the results depend on the faithfulness of the SCM $\mathcal{M}$ \parencite{DBLP:conf/nips/RussellKLS17}.
Future work should explore this line of work as a possible extension to CST.

\paragraph{A non-discriminatory ADM.}
This work positions CST within indirect discrimination testing for the ADM $b(X)=\hat{Y}$.
%
Recall that we express this type of discrimination using the DAG $A \rightarrow X \rightarrow Y$. 
Such DAG $\mathcal{G}$ implies the causal relation $X \leftarrow f(A, U)$ in the corresponding SCM $\mathcal{M}$.
We use this set up throughout Section~\ref{sec:Experiments}.
As emphasized in the previous two themes, $\mathcal{M}$ and $\mathcal{G}$ describe the dataset $\mathcal{D}$ on which $b()$ is used upon and condition the results of CST.
%
Given this setup, it is reasonable to ask: when does CST detect individual discrimination?
Broadly, the answer is when $A$ influences $X$ enough for $b()$ to make decisions that would have been different based on $X$ under a different $A$.
It follows, thus, that CST tests for whether the ADM relies or not on non-neutral information in $\mathcal{D}$. 
We stress, however, that detecting discrimination is not granted under CST only by having a biased $\mathcal{D}$.
It depends, for instance, on how much weight $b()$ gives to elements of $X$ and how these same elements relate to $A$.
Still,
% From the fair ML literature , 
since an unbiased $\mathcal{D}$ for high-stake decision-making context is rare to find \parencite{DBLP:journals/ethicsit/AlvarezCEFFFGMPLRSSZR24},
% Thus, 
it is also reasonable to further ask: how often will CST detect individual discrimination or, put differently, what amounts to a non-discriminatory $b()$ given this setup? The answer to this follow up question is multidisciplinary and more complex.

When testing for indirect discrimination, we essentially suspect the presence of systematic biases---in the form of $A \rightarrow X$---that hinder $b()$ through $X$ and we test wether we are wrong given $\mathcal{D}$.
%
For high-stake settings, like those studied in Section~\ref{sec:Experiments}, we acknowledge that testing for indirect discrimination using CST sounds like a self-fulfilling prophecy.
However, two points are important here.
%
First, even if we do detect indirect discrimination in a context known to be biased, the evidence is one aspect of the discrimination testing pipeline \parencite{DBLP:conf/fat/WeertsXTOP23}. 
The decision-maker still can justify the use of $X$ in $b()$ as a business requirement.
We must keep in mind that CST detects \textit{prima facie} evidence.
%
Second, as argued by legal scholars \parencite{Hacker2018TeachingFairness, Xenidis2020_TunningEULaw, Wachter2020BiasPreserving}, the role of indirect non-discrimination law is to address this kind of setting. 
Methods like CST, at a minimum, raise awareness around the use of $X$ by $b()$ and motivate policies around the business requirements of the decision-maker. 

Raising awareness is an important byproduct of discrimination testing methods.
As shown in Section~\ref{sec:Experiments.IllustrativeExample}, CST detects the lack of neutrality of $X$ and its impact on $b()$ better than ST.
In that example, the bank could successfully justify to a court its use of annual income and account balance despite the results.
Further, beyond the business requirements, arguably, there could be a shared societal interest for the bank to make informed decisions, meaning we might want for the bank's $b()$ to be fair but not at the expense of the bank becoming insolvent or applicants going bankrupt \parencite{DBLP:journals/eor/KozodoiJL22, DBLP:conf/fat/DAmourSABSH20, DBLP:conf/fat/SchwobelR22}.
What matters, regardless, is showing through CST that the bank uses a non-neutral $X$, even if it does not translate into validating the complainant's discrimination claim.
%
It helps to acknowledge, through evidence, that we are not in the best possible circumstances as a society.
This ties back to the calls made by legal scholars on the role of indirect non-discrimination law and its aim for correcting the present non-neutral status quo. 
See, e.g., the discussion by \textcite{Wachter2020BiasPreserving} on substantive equality over formal equality in indirect non-discrimination law.
%
It also ties with calls within fair ML to acknowledge the present non-neutral status quo as a starting point and view the fair ML tools as corrective measures for achieving a better one.
See, e.g., work on revisiting the fairness-accuracy trade-off \parencite{DBLP:conf/nips/WickpT19} and on non-ideal over ideal fairness \parencite{Otto2024_WhatisImpossibleAboutAlgoFairness}.
%
Future work should formalize CST and its use of counterfactuals for envisioning and testing for a desired status quo. 

%
% EOS
%
