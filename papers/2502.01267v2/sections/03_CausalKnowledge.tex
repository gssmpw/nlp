%
In this section, we discuss the role of auxiliary causal knowledge within CST. 
We formulate causality using structural causal models (SCM). 
%
CST requires access to the dataset of decisions, $\mathcal{D}$, and the ML model that produced it, $b()$.
CST also requires a SCM describing the data generating model behind $\mathcal{D}$.
%
We view this last requirement as an input space for stakeholders and domain experts. 
SCM are a convenient way for organizing assumptions on the source of the discrimination, facilitating stakeholder participation and supporting collaborative reasoning about contested concepts \parencite{Mulligan2022_AFCP}. 
There is no ground-truth concerning the SCM for $\mathcal{D}$.
The SCM describes an agreed view on the discrimination problem, though, not necessarily the only nor correct view of it.

Let $\mathcal{D}$ contain the set of relevant attributes $X$, the set of protected attributes $A$, and the decision outcome $\hat{Y}$ such that $\hat{Y}=b(X)$. 
We describe $\mathcal{D}$ as a collection of $n$ tuples, each $(x_i, a_i, \widehat{y}_i)$ representing the $i^{th}$ individual profile, with $i \in [1, n]$. $\hat{Y}$ is binary with $\hat{Y} = 1$ denoting the positive outcome (e.g.,~loan granted). 
%
For illustrative purposes, we assume a single binary $A$ with $A=1$ denoting the protected status (e.g., female gender). 
We relax this assumption in Section~\ref{sec:CST.Multi} when formalizing multidimensional discrimination.
 
\subsection{Structural Causal Models and Counterfactuals}
\label{sec:CausalKnowledge.SCM}

A \textit{structural causal model} (SCM) \parencite{PearlCausality2009} $\mathcal{M}=\{ \mathcal{S}, \mathcal{P}_{\mathbf{U}} \}$ describes how the set of $p$ variables $W = X \cup A$ is determined based on corresponding sets of structural equations $\mathcal{S}$,
and $p$ latent variables $U$ with prior distribution $\mathcal{P}_{\mathbf{U}}$. Each $W_j \in W$ is assigned a value through a deterministic function $f_j \in \mathcal{S}$ of its causal parents $W_{pa(j)} \subseteq W \setminus \{ W_j \}$ and latent variable $U_j$ with distribution $P(U_j) \in \mathcal{P}_{\mathbf{U}}$. Formally, for $W_j \in W$ we have that: 
%
\begin{equation}
\label{eq:SCM}
    W_j \leftarrow f_j(W_{pa(j)}, U_j)
\end{equation}
%
indicating the flow of information in terms of child-parent or cause-effect pairs. We consider the associated \textit{causal graph} $\mathcal{G} = (\mathcal{V}, \mathcal{E})$, where a node $V_j \in \mathcal{V}$ represents a $W_j$ variable and a directed edge $E_{(j, j')} \in \mathcal{E}$ a causal relation.
We can use $\mathcal{M}$ to derive $\mathcal{G}$.\footnote{This is based on the global Markov and faithfulness properties, summarized in the notion of the d-separation. We skip d-separation as we do not use it in this paper. See \textcite{Peters2017_CausalInference}.}

We make three assumptions for the SCM $\mathcal{M}$ that are common within the causal fairness literature \parencite{DBLP:journals/jlap/MakhloufZP24}. 
First, we assume \textit{causal sufficiency}, meaning there are no hidden common causes in $\mathcal{M}$, or confounders. 
Second, we assume $\mathcal{G}$ to be \textit{acyclical}, which turns $\mathcal{G}$ into a directed acyclical graph (DAG), allowing for no feedback loops.
Third, we assume \textit{additive noise models} (ANM) to insure an invertible class of SCM \parencite{Hoyer2008_ANM}.
%
The ANM assumption implies $\mathcal{S} = \{W_j \leftarrow f_j(W_{pa(j)}) + U_j \}_{j=1}^p$ in \eqref{eq:SCM}.
%
These assumptions are not necessary for generating counterfactuals, but do simplify the process \parencite{Pearl2016_CausalInference}. 
% Conceptually, 
The CST framework is not tied to any of these assumptions as long as the generated counterfactuals are reliable.

The causal sufficiency assumption is particularly deceitful as it is difficult to both test and account for a hidden confounder \parencite{DBLP:journals/corr/abs-1902-10286, mccandless2007bayesian, DBLP:conf/nips/LouizosSMSZW17}. 
The risk of a hidden confounder is a general modeling problem. Here, the dataset $\mathcal{D}$ delimits our context. By this we mean that we expect it to contain all relevant information used by $b()$.
Causal sufficiency implies independence among the random variables in $U$, which allows to factorize $P_{\mathbf{U}}$ into its individual components:
%
\begin{equation}
\label{eq:SCM.Causal.Sufficiency}
    P(U_1, \dots, U_j) = P(U_1) \times \dots \times P(U_j).
\end{equation}
%

For a given SCM $\mathcal{M}$ we want to run \textit{counterfactual queries} to build the test group for a complainant. Counterfactual queries answer to \textit{what would have been if} questions. 
In CST, we ask such questions around the protected attribute $A$. 
By setting $A$ to the non-protected status $\alpha$ using the \textit{do-operator} $do(A := \alpha)$ \parencite{PearlCausality2009}, we capture the individual-level effects $A$ has on $X$ according to the SCM $\mathcal{M}$. 
Let $X^{CF}$ denote \textit{the set of counterfactual variables} obtained via the three steps: abduction, action, and prediction \parencite{Pearl2016_CausalInference}. 
Further, 
let $P(X^{CF}_{A \leftarrow \alpha}(U) \; | \; X, A)$ denote \textit{counterfactual distribution}.
%
We now describe each step.
%
\textit{Abduction}: for each prior distribution $P(U_i)$ that describes $U_i$, we compute its posterior distribution given the evidence, or $P(U_i \;| \; X, A)$. \textit{Action}: we intervene $A$ by changing its structural equation to $A := \alpha$, which gives way to a new SCM $\mathcal{M}'$. \textit{Prediction}: we generate the \textit{counterfactual distribution} $P(X^{CF}_{A \leftarrow \alpha}(U) \; | \; X, A)$ by propagating the abducted $P(U_i \;| \; X, A)$ through the revised structural equations in $\mathcal{M}'$.
%
% Notably, 
Unlike counterfactual explanations \parencite{Wachter2017Counterfactual}, generating counterfactuals and, thus, CST,  does not require a change in the individual decision outcome. 
% It is possible for $\widehat{Y} = \widehat{Y}^{CF}$ after manipulating $A$.

\subsection{On Conceiving Discrimination}
\label{sec:CausalKnowledge.IndDisc}

Discrimination is a comparative process \parencite{Lippert2006BadnessOfDiscrimination}. 
Non-discrimination law is centered on Aristotle's maxim of treating similar (or similarly situated) individuals similarly \parencite{Westen1982EmptyEquality}.
Granted that we can agree on what similar (or similarly situated) individuals are,\footnote{We briefly discuss this issue in the next section, though similarity in itself is a complex, ongoing, legal discussion. We recommend \textcite{Westen1982EmptyEquality} for further reading.} in practice, testing for discrimination reduces to comparing similar protected and non-protected by non-discrimination law individuals to see if their outcomes differ within the context of interest \parencite{DBLP:conf/fat/WeertsXTOP23}.
Most, if not all, discrimination tools operationalize this comparative process \parencite{Kohler2018CausalEddie}.

The legal setting of interest in this work is indirect discrimination under EU non-discrimination law. 
Indirect discrimination occurs when an apparently neutral practice disadvantages individuals that belong to a protected group. Following \textcite{Hacker2018TeachingFairness}, we focus on indirect discrimination for three reasons. 
First, unlike disparate impact under US law \parencite{Barocas2016_BigDataImpact}, the decision-maker can still be liable for it despite lack of premeditation and, thus, all practices need to consider potential indirect discrimination implications. 
Second, many ML models are not allowed to use the protected attribute as input, making it difficult for regulators to use the direct discrimination setting.\footnote{This point, though, has been contested recently by \textcite{Adams2022DirectlyDiscriminatoryAl}.}
Third, we conceive discrimination as a product of a biased society where $b()$ continues to perpetuate the bias reflected in $\mathcal{D}$ because it cannot escape making deriving $\hat{Y}$ based on $X$.
The indirect setting best describes how biased information can still be an issue for a ML model that never uses the protected attribute.
%
That said, it does not mean CST cannot be implemented in other legal contexts. 
We simply acknowledge that it was developed with the EU non-discrimination legal framework in mind due to the previous reasons.

Causality is often used for formalizing the problem of discrimination testing.
This is because of the legal framing of discrimination in which we are interested in the protected attribute as a direct or indirect cause of the decision outcome \parencite{Heckman1998_DetectingDiscrimination, Kohler2018CausalEddie}.
Previous works \parencite{Kilbertus2017AvoidDiscCau, Chiappa2019_PathCF, Plecko2022_CFA, Tschantz2022_ProxyDisc} focus more on whether the paths between $A$ and $\hat{Y}$ are direct or indirect, leading to the two kinds of discrimination prescribed under EU non-discrimination law. 
The causal setting here is much simpler. 
We know that $b()$ only uses $X$, and are interested in how information from $A$ is carried by $X$ and how we account for these links when testing for discrimination by using the auxiliary causal knowledge. 

\subsection{Fairness Given the Difference}
\label{sec:CausalKnowledge.KHC}

The SCM required by CST allows to operationalize the notion of \textit{fairness given the difference}, FGD for short, and depart from the standard idealized comparison.
Access to a SCM enables the generation of a counterfactual instance for the complainant, allowing to represent how the protected attribute influences the non-protected attributes used by $b()$.
We come back to this point in the next section; here, we motivate FGD.
% 
The reference work is \textcite{Kohler2018CausalEddie}.
FGD captures that work's overall criticism toward the counterfactual causal model of discrimination (CMD) introduced in Section~\ref{sec:Introduction}.\footnote{We attribute this phrase to Kohler-Hausmann as she used it during a panel discussion at the NeurIPS'21 Workshop on Algorithmic Fairness through the Lens of Causality and Robustness (\href{https://www.afciworkshop.org/afcr2021}{AFCR}). It is not, however, present in her paper. 
The phrase first appears in \textcite{DBLP:conf/eaamo/AlvarezR23}.}

As argued by \textcite{Kohler2018CausalEddie} and others before her \parencite{Bonilla1997_RethinkingRace, Sen2016_RaceABundle}, it is difficult to deny that most protected attributes, if not all of them, are \textit{social constructs}. 
% That is, 
These are attributes that were used to classify \textit{and} divide groups of people in a systematic way that conditioned the material opportunities of multiple generations \parencite{Mallon2007SocialConstruction, rose_constructivist_2022}. 
Recognizing $A$ as a social constructs means recognizing that its effects can be reflected in seemingly neutral variables in $X$. It is recognizing that $A$, the attribute, cannot capture alone the meaning of belonging to $A$ and that we might, as a minimum, have to link it with other attributes to better capture this, such as $A \rightarrow X$ where $A$ and $X$ change in unison. Protected attributes summarize the historical processes that fairness researchers are trying to address today and should not be treated lightly.\footnote{An example is the use of race by US policy makers after WWII. See, e.g.,~the historical evidence provided by \textcite{Rothstein2017Color} (for housing), \textcite{Schneider2008Smack} (for narcotics), and \textcite{Adler2019MurderNewOrleansJimCrow} (for policing).}

FGD centers on how $A$ is treated in the CMD. 
It goes beyond the standard manipulation concern in which $A$ is an inmutable attribute \parencite{Angrist2008MostlyHarmless}. 
Instead, granted that we \textit{can} or, more precisely, \textit{have to} manipulate $A$ for running a discrimination analysis, FGD puts into question how a testing framework operationalize such manipulation.
If $A$ is a social construct with clear influence on $X$, then \textit{when $A$ changes, $X$ should change as well}.
This is precisely what FGD entails.
As discussed in Section~\ref{sec:CST},
within CST it manifests by building the test group on the complainant's counterfactual, letting $X^{CF}$ reflect the effects of changing $A$ instead of assuming $X = X^{CF}$. 
This is because we view the test group as a representation of the hypothetical counterfactual world of the complainant.

Based on FGD we consider two types of manipulations that summarize existing discrimination testing frameworks. 
The \textit{ceteris paribus} (CP), or all else equal, manipulation in which $A$ changes but $X$ remains the same. 
Examples of it include situation testing \parencite{Thanh_KnnSituationTesting2011, Zhang_CausalSituationTesting_2016} and the famous correspondence study by \textcite{Bertrand2004_EmilyAndGreg}. 
The \textit{mutatis mutandis} (MM), or changing what needs to be changed, manipulation in which $X$ changes when we manipulate $A$ based on some additional knowledge, like a structural causal model, that explicitly links $A$ to $X$. Counterfactual fairness \parencite{Kusner2017CF} uses this manipulation. The MM is clearly preferred over the CP manipulation when we view $A$ as a social construct.
See \textcite{MutatisMutandis} for a detailed discussion on the CP and MM manipulations.

%
% EOS
%
