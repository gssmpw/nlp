%
The goal of CST is to construct and compare a control and test group for each protected individual (read, \textit{complainant}) $c$ in the dataset in a meaningful and actionable way. 
The focus is on the tuple $(x_c, a_c, \widehat{y}_c) \in \mathcal{D}$, with $c \in [1,n]$, that motivates the individual discrimination claim.
CST requires access to the ADM $b()$, the dataset $\mathcal{D}$, and the auxiliary causal knowledge SCM $\mathcal{M}$ and DAG $\mathcal{G}$.

Three additional inputs are central to CST:
the number of instances, $k$; 
the similarity function, $d$; 
and the strength of the evidence for the discrimination claim, $\alpha$.
Here, 
$k$ determines the size of the control and test groups for $c$; 
$d$ determines how much these two groups resemble $c$; 
and $\alpha$ determines the statistical significance required when comparing these two groups to trust the claim around $c$.
%
We must also define a search algorithm for implementing CST.
We use the k-nearest neighbors, or k-NN, algorithm \parencite{Hastie2009_ElementsSL}, resulting in the present k-NN CST.
The k-NN is intuitive, easy to implement, and commonly used by other frameworks.
The k-NN implementation is straightforward. 
We provide the relevant algorithms in Appendix~\ref{Appendix.Supplements}.
Other implementations are possible as long as the following definitions are adjusted.

\subsection{Measuring Individual Similarity}
\label{sec:CST.Distance}

We start by defining the \textit{similarity measure} $d$.
We use the same $d$ as the one used by \textcite{Thanh_KnnSituationTesting2011} to compare our implementation against its standard situation testing counterpart.
%
Let us define the \textit{between tuple distance} $d(x_1, x_2)$ as:
%
\begin{equation}
\label{eq:Distance}
    d(x_1, x_2) = \frac{\sum_{i=1}^{|X|} d_i(x_{1, i}, x_{2, i})}{|X|}
\end{equation}
%
such that $d(x_1, x_2)$ averages the sum of the \textit{per-attribute distances} $d_i(x_{1,i}, x_{2, i})$ across all attributes in $X$. 
It does not use the protected attribute(s) $A$.
% In \eqref{eq:Distance}, 
A lower $d$ implies a higher similarity between the tuples $x$ and $x'$ and further implies two similar individuals.
The k-NN CST handles non-normalized attributes but, as default, we normalize them to insure comparable per-attribute distances.

The specific $d_i$ used depends on the type of the \textit{i-th} attribute.
It equals the \textit{overlap measurement} ($ol$) if the attribute $X_i$ is categorical; otherwise, it equals the \textit{normalized Manhattan distance} ($md$) if the attribute $X_i$ is continuous, ordinal, or interval.
Under this conception, $d$ amounts to Gower's distance \parencite{Gower1971}.
%
For illustrative purposes, we recall both $md$ and $ol$ distances below. We define $md$ as:
%
\begin{equation}
    md(x_{1,i}, x_{2, i}) = \frac{| x_{1,i} - x_{2, i} |}{(\max(X_i) - \min(X_i))}
\end{equation}
%
and we define $ol$ as:
%
\begin{equation}
    ol(x_{1,i}, x_{2, i}) = 
    \begin{cases}
    1 & \text{if } x_{1, i} \neq x_{2, i} \\
    0 & \text{otherwise}.
\end{cases}
\end{equation}
%
The choices of $d_i$ and, in turn, of $d$ are not restrictive.
We plan to explore other formulations in subsequent works, like heterogeneous distance functions \parencite{WilsonM97_HeteroDistanceFunctions} and propensity score matching \parencite{DBLP:journals/jiis/QureshiKKRP20}.
Hence, why we view $d$ as an input to rather than a characteristic of k-NN CST.

\subsection{Building the Control and Test Groups}
\label{sec:CST_ControlTest}
 
For a complainant $c$, the control and test groups are built on the search spaces and search centers for each group. 
The search spaces are derived from $\mathcal{D}$.
The search centers, however, are derived separately. The one for the control group comes from $\mathcal{D}$ while the one for the test group comes from its corresponding generated counterfactual dataset $\mathcal{D}^{CF}$. 

%
\begin{definition}[Search Spaces]
\label{def:SearchSpaces}
    Under a binary $A$, with $A=1$ denoting the protected status, we partition $\mathcal{D}$ into the \textit{control search space} $\mathcal{D}_c=\{(x_i, a_i, \widehat{y}_i) \in \mathcal{D}: a_i=1\}$ and the \textit{test search space} $\mathcal{D}_t=\{(x_i, a_i, \widehat{y}_i) \in \mathcal{D}: a_i=0\}$.
\end{definition}
%

%
\begin{definition}[Counterfactual Dataset]
\label{def:CounterDataset}
    Given the ADM $b()$ and SCM $\mathcal{M}$, the \textit{counterfactual dataset} $\mathcal{D}^{CF}$ includes the counterfactual mapping of each instance with a protected status in $\mathcal{D}$ via the abduction, action, and prediction steps when setting a binary $A$ to the non-protected status, or \textit{do}($A:=0$).
\end{definition}
%

%
\begin{definition}[Search Centers]
\label{def:SearchCenters}
    We use $x_c$ from the tuple of interest $(x_c, a_c, \hat{y}_c) \in \mathcal{D}$ as the \textit{control search center} for exploring $\mathcal{D}_c \subset \mathcal{D}$, and use $x_c^{CF}$ from the tuple of interest's counterfactual $(x_c^{CF}, a_c^{CF}, \hat{y}_c^{CF}) \in \mathcal{D}^{CF}$ as the \textit{test search center} for exploring $\mathcal{D}_t \subset \mathcal{D}$.
\end{definition}
%

We extend these definitions for $|A| > 1$ in Section~\ref{sec:CST.Multi}.
Importantly, to obtain $\mathcal{D}^{CF}$ we consider a SCM $\mathcal{M}$ in which $A$ has no causal parents, $A$ affects only elements of $X$, and $\hat{Y}=b(X)$.
See, e.g.,~Figures~\ref{fig:KarimiV2} and \ref{fig:LawSchool}.
Under such auxiliary causal knowledge, if $A$ changes then $X$ changes too. 
% When generating the counterfactuals on $A$ (Section~\ref{sec:CausalKnowledge.SCM}), under the indirect discrimination setting (Section~\ref{sec:CausalKnowledge.IndDisc}), the resulting $X^{CF}$ in $\mathcal{D}^{CF}$ reflects a MM manipulation (Section~\ref{sec:CausalKnowledge.KHC}).
% It follows that 
Here, $\mathcal{D}^{CF}$
% , \textit{given our worldview} by $\mathcal{M}$, 
represents the world that the complainants would have experienced had they belonged to the non-protected group. 
This is why we draw the test search center from $\mathcal{D}^{CF}$ and not from $\mathcal{D}$. 

Given the $\mathcal{D}$ and $\mathcal{D}^{CF}$, we construct the control and test groups for $c$ using the k-NN algorithm with distance function $d$ \eqref{eq:Distance}.
We want each group (read, neighborhood) to have size $k$. 
%
For \textbf{the control group} (\textit{k-ctr}) we use the factual tuple of interest $(x_c, a_c, \hat{y}_c) \in \mathcal{D}$ as the search center to explore $\mathcal{D}_c$:
%
\begin{equation}
\label{eq:kctr}
    \text{\textit{k-ctr}} =
    \{ (x_i, a_i, \widehat{y}_i) \in \mathcal{D}_c: rank_{d}( x_c, x_i) \leq k \}
\end{equation}
%
where $rank_{d}(x_c, x_i)$ is the rank position of $x_i$ among tuples in $\mathcal{D}_c$ with respect to the ascending distance $d$ from $x_c$. 
%
For \textbf{the test group} (\textit{k-tst}) we use the counterfactual tuple of interest $(x^{CF}_c, a^{CF}_c, \widehat{y}^{CF}_c) \in \mathcal{D}^{CF}$ as the search center to explore $\mathcal{D}_t$:
%
\begin{equation}
\label{eq:ktst}
    \text{\textit{k-tst}} = \{ (x_i, a_i, \widehat{y}_i) \in \mathcal{D}_t: rank_{d}( x^{CF}_c, x_i) \leq k \}
\end{equation}
%
where $rank_{d}(x_c^{CF}, x_i)$ is the rank position of $x_i$ among tuples in $\mathcal{D}_t$ with respect to the ascending distance $d$ from $x_c^{CF}$. 
%
We use the same $d$ for each group. 
Neither $A$ nor $\hat{Y}$ are used for constructing the groups.
% \footnote{
Further, we can always expand \eqref{eq:kctr} and \eqref{eq:ktst} by adding constraints such as, for instance, a maximum distance $\epsilon > 0$: $\text{\textit{k-ctr}} = \{ x_i \in \mathcal{D}_c: rank_{d}( \mathbf{x}_c, x_i) \leq k \land d(x_c, x_i) \leq \epsilon \}$ and $\text{\textit{k-tst}} = \{ x_i \in \mathcal{D}_t: rank_{d}( x_c^{CF}, x_i) \leq k \land d(x_c^{CF}, x_i) \leq \epsilon \}$.
% }

%
\begin{remark}(Meaningfulness)
\label{rem:meaningfulness}
    The choice of search centers for \eqref{eq:kctr} and \eqref{eq:ktst} operationalizes \textit{fairness given the difference} in CST, making it a meaningful framework for testing individual discrimination.
    % In particular, 
    Using $x_c$ and $x_c^{CF}$ to search for, respectively, protected and non-protected individuals in $\mathcal{D}_c$ and $\mathcal{D}_t$ is a statement on how we view the role of \textit{within group ordering} as imposed by the protected attribute $A$.
    % on $\mathcal{D}_c$ and $\mathcal{D}_t$. 
    Each search center reflects the $A$-specific ordering imposed on the search space it targets.
\end{remark}
%

To illustrate Remark~\ref{rem:meaningfulness}, let us consider Example~\ref{ex:IllustrativeExample}.
If being a female imposes certain systematic limitations that hinder $x_c$, then comparing $c$ to other females preserves the group ordering prescribed by $X|A=1$ as all instances involved experience $A$ in the same way.
Similarly, the generated counterfactual male instance for $c$ should reflect the group ordering prescribed by $X|A=0$. 
Here, in particular, we would expect $x_c \leq x_c^{CF}$ given what we know about the effects of $A$ on $X$.
A way to reason about this remark is through the notion of effort. 
If being female requires a higher individual effort than being male to achieve the same $x_c$, then it is fair to compare $c$ to other female instances. 
However, it is unfair to compare $c$ to other male instances without adjusting for the extra effort undertaken by $c$ to be comparable to these male instances. 
The counterfactual $x_c^{CF}$ reflects said adjustment for $c$.
For a formal discussion on effort and its role on individual fairness \parencite{DworkHPRZ12}, see \textcite{Chzhen2020WassersteinBarycenters, Chzhen2022MiniMax}.

\subsection{Detecting Discrimination}
\label{sec:CST_Disc}

For a complainant $c$, we compare the control and test groups by looking at the \textit{difference in proportion of negative decision outcomes}:
%
\begin{equation}
\label{eq:delta}
    \Delta p = p_c - p_t
\end{equation}
%
such that $p_c$ and $p_t$ represent the count of tuples with a negative decision outcome, respectively, in the control group \eqref{eq:kctr} and test group \eqref{eq:ktst}. Formally:
%
\begin{equation}
\label{eq:p1_and_p2}
\begin{aligned}
    p_c & = \frac{|\{ (x_i, a_i, \widehat{y}_i) \in \text{\textit{k-ctr}}: \hat{y}_i = 0 \}|}{k} \\
    p_t & = \frac{|\{ (x_i, a_i, \widehat{y}_i) \in \text{\textit{k-tst}}: \hat{y}_i = 0 \}|}{k}
\end{aligned}
\end{equation}
% 
where only $\hat{Y}$ is used for deriving the proportions.
It follows that $p_c, p_t \in [0, 1]$ and $\Delta p \in [-1, 1]$.
We compute $\Delta p$ for all complainants in $\mathcal{D}$ regardless of their decision outcome.

CST uses $\Delta p$ to test for the complainant's individual discrimination claim. 
Implicit to this task is the \textit{accepted deviation} $\tau \in [-1, 1]$ for $\Delta p$.
%
It represents the maximum acceptable difference between $p_c$ and $p_t$, such that any deviation from it amounts to discrimination: i.e., $\Delta p > \tau$. 
The $\tau$ is often implied with the default choice of $\tau=0$, as we wish for protected and non-protected individuals to be rejected at equal rates.
%
As $\Delta p$ is a proportion comparison, $\Delta p$ is asymptotically normally distributed, which allows to build \textit{Wald confidence intervals} (CI) around it. For other confidence interval methods, such as exact methods for small samples, see \textcite{Newcombe1998}.
With the CI we equip the complainant's claim with a measure of certainty and answer whether the claim, meaning the deviation from $\tau$, is or not statistically significant.
If $\tau$ falls within the \textit{one-sided CI}, then we cannot say that the complainant's claim is statistically significant.
We write such CI for $\Delta p$ as:
%
\begin{align}
\label{eq:CIs}
    [\Delta p - w_{\alpha}, + \infty),
    % [\Delta p - w_{\alpha}, \Delta p +  w_{\alpha}],
    & \; \; \; \text{with} \; \; \;
    w_{\alpha} =  z_{\alpha} \sqrt{\frac{p_c(1 - p_c) + p_t(1 - p_t)}{k}}.
\end{align}
%
where $z_{\alpha} = \Phi^{-1}(1-\alpha)$ is the $1-\alpha$ quantile of the standard normal distribution $\mathcal{N}$ under a \textit{significance level} of $\alpha$ or, equivalently, a \textit{confidence level} $(1 - \alpha) \cdot 100$\%.\footnote{\textcite{DBLP:conf/eaamo/AlvarezR23} contains a typo in the numerator of $w_{\alpha}$: we wrote a minus instead of a plus sign. In the code, however, it was implemented correctly. It also discusses a two-sided CI.}
The $+ \infty$ represents that there is no upper bound, as we are interested in values greater than $\tau$.
%
The choice of $\alpha$ and $\tau$, as with $k$, depends on the context of the discrimination claim. 
These parameters are motivated by legal requirements (set, e.g.,~by the court \parencite{Thanh_KnnSituationTesting2011}), or technical requirements (set, e.g.,~via power analysis \parencite{Cohen2013StatisticalPower}), or both.
A common choice for $\alpha$ is 0.05, though common alternatives are also 0.01 and 0.10. 

The CI represents a one-sided statistical test based on the hypothesis that there is individual discrimination, providing a measure of certainty on $\Delta p$ through a range of possible values.
Formally, 
let $\pi$ be the true difference in proportion of negative decision outcomes between the control and test groups. Then the \textit{null hypothesis} is $H_0: \pi = \tau$, while the \textit{alternative hypothesis} $H_1: \pi > \tau$.
When $\tau$ falls within the range of probable values in CI, we fail to reject $H_0$ with $\alpha$ significance level.
Given $\Delta p$ \eqref{eq:delta} and its CI \eqref{eq:CIs}, we can now proceed to define individual discrimination under CST.

%
\begin{remark}(Two Versions of CST)
\label{rem:SearchCenters}
    CST can include or exclude the search centers in \eqref{eq:delta} and \eqref{eq:CIs}. 
    If we exclude them, then both remain as is; 
    if we include them, then $\hat{y}_c$ and $\hat{y}_c^{CF}$ are counted in $p_c$ and $p_t$, leading to a denominator of $k + 1$ in both as well as in the $w_{\alpha}$ calculation.
    To distinguish between the two versions of CST, we will use CST w/o when excluding and CST w/ when including the search centers.
    We add this option for comparing CST against situation testing \parencite{Thanh_KnnSituationTesting2011}, which excludes the search centers, and counterfactual fairness \parencite{Kusner2017CF}, which only uses the search centers.
    We use this option extensively in Section~\ref{sec:Experiments}. 
\end{remark}
%

%
\begin{definition}[Individual Discrimination]
\label{def:IndDisc}
    There is (potential) individual discrimination toward the complainant $c$ if $\Delta p > \tau$, meaning the negative decision outcomes rate for the control group is greater than for the test group by some accepted deviation $\tau \in [-1, 1]$.
\end{definition}
%

%
\begin{definition}[Confidence on the Individual Discrimination Claim]
\label{def:CIs}
    A detected (potential) discrimination claim for the complainant $c$ by Definition~\ref{def:IndDisc}
    % by $\Delta p$ \eqref{eq:delta}  
    is statistically significant with significance level $\alpha$ if the CI
    % Wald confidence interval \eqref{eq:CIs} 
    excludes $\tau$.
\end{definition}
%

We highlight the use of the word \textit{potential} in both definitions. 
It implies, formally, that under CST, as with any individual or group discrimination testing framework \parencite{Romei2014MultiSurveyDiscrimination}, we test for \textit{prima facie} discrimination.
Uncovering discrimination amounts to a series of steps among which there is the need to provide evidence of the discrimination claim. 
Even if said evidence is found, it still needs to be argued for in court.
For a discussion on the EU discrimination testing pipeline, see \textcite{DBLP:conf/fat/WeertsXTOP23}.

%
\begin{remark}(Actionability)
\label{rem:actionability}
    The many-to-many comparison behind $\Delta p$ is what makes CST an actionable framework for testing individual discrimination. 
    The single comparison is not enough when proving \textit{prima facie} discrimination 
    % \parencite[Sec. 6.3]{EU2018_NonDiscriminationLaw} 
    as we want to ensure, one, that the individual claim is representative of the population, and two, be certain about the individual claim.
    Implicit to both concerns is finding a pattern of unfavorable decisions against the protected group of the complainant on which we are confident enough.
\end{remark}
%

The notion of repetition is important in Remark~\ref{rem:actionability}.
%
Similar to flipping a coin multiple times to uncover its (un)fairness, we expect a significant pattern of unfavorable decisions (read, discrimination) to emerge through ``repeating'' the decision-making process in question.
Such repetition is often not possible in practice. 
In a non-ADM setting we cannot ask the same female complainant in Example~\ref{ex:IllustrativeExample} to apply multiple times to the same bank;
we can, instead, look at other similar instances that underwent the same decision process.
Similarly, even when repetition is deterministic, such as entering the same input multiple times into the ADM, it is non-trivial to generalize that the individual case represents a group-wise pattern.
What rules out that the $\Delta p$ for complainant $c$ is an exception rather than a systematic effect?
We can, for instance, assume a theoretical distribution of comparisons with $\pi$ to account for potential randomness in what we detect from the single point estimate that is $\Delta p$.
The $p_c$ and $p_t$ help tackle these concerns.

\paragraph{On positive discrimination.}
Positive individual discrimination, or affirmative action, refers to the setting in which complainant $c$ is shown to be favored in the decision-making process \parencite{Romei2014MultiSurveyDiscrimination}.
Policies like diversity quotas are an example of positive discrimination.
We can operationalize positive discrimination easily under the current k-NN CST implementation: we would rewrite Definitions \ref{def:IndDisc} and \ref{def:CIs} by looking at the same complainant $c$ but focusing on the opposite effect.
Formally, we would consider $\Delta p < \tau$ (where now $\tau < 0$) and, in turn, the CI $(- \infty, \Delta p + w_\alpha]$ as we would test for the alternative hypothesis $H_1: \pi < \tau$. The rest of the k-NN CST pipeline would apply the same.

Positive discrimination remains understudied within algorithmic discrimination. 
We believe this is due to standard discrimination being more prevalent as a societal and research problem.
We also believe positive discrimination poses a different set of conceptual challenges over traditional discrimination, making it harder to justify by those in favor of it.\footnote{Take, e.g., the US Supreme Court's overturn of affirmative action \parencite{NPR2023AffirmativeAction}.}
This is, at least, our reading from the lack of discussion positive discrimination enjoys by the legal works we cite.
In short, although testing for positive discrimination under CST is straightforward, a clear legal narrative is lacking for us to comfortably operationalize it.
We formalize and test for positive discrimination in Appendices~\ref{Appendix.Supplements} and \ref{Appendix.AddExperiments}, respectively.
We do so mainly for illustrative purposes since we are focused on understanding traditional discrimination in its single and multidimensional forms.

\subsection{Connection to Counterfactual Fairness}
\label{sec:CST.OnCF}

There is a clear link between CST and counterfactual fairness (CF) of \textcite{Kusner2017CF}.
Recall that the ADM $b()$ is counterfactually fair if it outputs the same outcome for the factual tuple as for its counterfactual tuple, where the latter is generated through the abduction, action, and prediction steps when intervening the protected attribute $A$.\footnote{Formally, $P(\hat{Y}_{A \leftarrow a}(U)=y \, | \, X, A) = P(\hat{Y}_{A \leftarrow a'}(U)=y \, | \, X, A)$, where the left side is the factual $A=a$ and the right side the counterfactual $A=a'$. Similar with $\tau$ in $\Delta p$, the equality can be relaxed given some permissible difference threshold $\epsilon > 0$ between the factual and counterfactual quantities.} 
Hence, the factual $(x_c, a_c, \hat{y}_c)$ and counterfactual $(x_c^{CF}, a_c^{CF}, \hat{y}_c^{CF})$ tuples used in CST are also the ones used in CF for evaluating the counterfactual fairness for complainant $c$.

We view CST, when including the search centers, as an actionable extension of CF.
CST equips CF with CI \eqref{eq:CIs}, providing a certainty measure on the counterfactual fairness of $b()$. 
Previous works on CF have addressed uncertainty concerns \parencite{DBLP:conf/nips/RussellKLS17, DBLP:conf/uai/KilbertusBKWS19}, but with a focus on the structure of the SCM $\mathcal{M}$ and how that affects the measured unfairness of $b()$. 
We instead address certainty on the literal comparison that motivates the CF definition.
A key consequence of this link between CST and CF is that we can have an ADM $b()$ that is counterfactually fair but discriminatory. 
We summarize this point in Proposition~\ref{prop:ActioanbleCF}. 
We also present a sketch of proof for it.

%
\begin{proposition}[Actionable Counterfactual Fairness]
\label{prop:ActioanbleCF} 
    Counterfactual fairness does not imply nor it is implied by individual discrimination as conceived in Definition~\ref{def:IndDisc}.
\end{proposition}
%

We now present a sketch of proof to Proposition~\ref{prop:ActioanbleCF}.
Consider the factual tuple $(x_c, a_c=1, \widehat{y}_c=0)$ and assume the generated counterfactual is $(x_c^{CF}, a_c^{CF}=0, \widehat{y}_c^{CF}=0)$. 
Since $\widehat{y}_c = \widehat{y}_c^{CF}$, this is a case in which CF holds. However, the decision boundary of the ADM $b()$ can be purposely set such that the $k$-nearest neighbors of $x_c$ are all within the decision $\hat{Y}=0$, and less than $1-\tau$ fraction of the $k$-nearest neighbors of $x_c^{CF}$ are within the decision $\hat{Y}=0$. 
This leads to a $\Delta p > 1-(1-\tau) = \tau$, showing that there is individual discrimination. 
Similarly, the other way can be shown by assuming $\widehat{y}_c \neq \widehat{y}_c^{CF}$ but the sets of $k$-nearest neighbors have rates of negative decisions whose difference is smaller than $\tau$.

Proposition~\ref{prop:ActioanbleCF} alludes to the scenario in which $b()$ is counterfactually fair yet discriminatory. 
Intuitively, it is possible to handle \textit{borderline cases} where the tuple of interest and its counterfactual both get rejected by $b()$, though the latter is closer to the decision boundary than the former. 
The model $b()$ would be considered counterfactually fair, but would that disprove the individual discrimination claim? 
CST, by constructing the control and test groups around this single comparison, accounts for this actionability concern.

Importantly, for the purposes of Proposition~\ref{prop:ActioanbleCF} we consider the \textit{two-sided CI} over the previous one-sided CI \eqref{eq:CIs}.
We are interested in addressing the statistical significance of the ``counterfactual fairness claim'' using the neighborhoods built by the k-NN---note via the CST w/ version, cfr. Remark~\ref{rem:SearchCenters}---algorithm around the factual and counterfactual instances of CF.
We write such CI for $\Delta p$ as:
%
\begin{align}
\label{eq:CIsforCF}
    % [\Delta p - w_{\alpha}, + \infty),
    [\Delta p - w_{\alpha/2}, \Delta p +  w_{\alpha/2}],
    & \; \; \; \text{with} \; \; \;
    w_{\alpha / 2} =  z_{\alpha / 2} \sqrt{\frac{p_c(1 - p_c) + p_t(1 - p_t)}{k + 1}}
\end{align}
%
where $z_{\alpha / 2} = \Phi^{-1}(1-\alpha / 2)$ be the $1-\alpha / 2$ quantile of $\mathcal{N}$ under a \textit{significance level} of $\alpha$ or, equivalently, a \textit{confidence level} $(1 - \alpha) \cdot 100$\%.
We use \eqref{eq:CIsforCF} when addressing CF as a whole, and use \eqref{eq:CIs} when addressing the discrimination claim through CF.

% %
% \begin{remark}(CST w/ for Actionable CF)
%     Following Remark~\ref{rem:SearchCenters}, we always use the CST version that includes the search centers, CST w/, to study CF since these are the factual and counterfactual instances behind CF. In practice, it implies neighborhoods of size $k+1$ to equip CF, either for discrimination claims or as a whole, with confidence intervals. 
%     For consistency in the notation between \eqref{eq:CIs} and \eqref{eq:CIsforCF} we keep the denominator as $k$ in the latter. This is because we also could run a two-sided CI with CST w/o for queries not related to CF if needed. The CST w/ link to CF is explored in Section~\ref{sec:Experiments}.  
% \end{remark}
% %

\subsection{The Multidimensional Setting}
\label{sec:CST.Multi}

Let us revisit the previous definitions under multidimensional discrimination. 
It occurs when $|A| > 1$.
Following the legal literature \parencite{Xenidis2020_TunningEULaw}, we distinguish two forms of multidimensional discrimination: multiple and intersectional.

%
\begin{definition}[Multiple Discrimination]
\label{def:MultipleDisc}
    Under Definition~\ref{def:IndDisc}, there is (potential) multiple individual discrimination toward the complaint $c$ with the set of $|A| = q > 1$ protected attributes, if $\Delta p > \tau$ for each $\{A_i\}_{i=1}^{q}$ protected attribute.
\end{definition}
%

%
\begin{definition}[Intersectional Discrimination]
\label{def:IntersectionaleDisc}
    Under Definition~\ref{def:IndDisc}, there is (potential) intersectional individual discrimination toward the complaint $c$ with the set of $|A| = q > 1$ protected attributes, if $\Delta p > \tau$ for the attribute $A^* = \mathbbm{1}\{ A_1=1 \wedge A_2=1 \wedge \dots \wedge A_q=1 \}$ obtained by the intersection of the protected attributes.
\end{definition}
%

Regarding Definition~\ref{def:CIs}, meaning the confidence on the individual claim under multiple and intersectional discrimination, notice that both Definitions \ref{def:MultipleDisc} and \ref{def:IntersectionaleDisc} work with $\Delta p$ point estimates. Definition~\ref{def:MultipleDisc} looks at $q$ deltas for $c$ given the $q$ protected attributes, while Definition~\ref{def:IntersectionaleDisc} looks at a single delta for the attribute obtained by the intersection for $c$ of all the protected attributes. For intersectional discrimination, the single intersection delta must be statistically significant given $\alpha$. For multiple discrimination, instead, all the multiple deltas must be statistically significant given $\alpha/q$. The \textit{Bonferroni correction factor} $1/q$ counteracts the well-known multiple comparisons problem, allowing to test for a family-wise error rate of $\alpha$ in a set of $q$ (possibly, dependent) statistical tests.
%
We assume the same $\tau$ in both cases, in particular, with $\tau$ being the same irrespective of the protected attribute considered under multiple discrimination.

The difference between Definition~\ref{def:MultipleDisc} and Definition~\ref{def:IntersectionaleDisc} is subtle but central to detecting \textit{prima facie} individual discrimination.
In multiple discrimination, we require $c$ to be discriminated \textit{separately} $q$ times as a member of each protected attribute it belongs to: e.g.,~as a female and as a non-white individual.
In intersectional discrimination, instead, we require $c$ to be discriminated \textit{simultaneously} as a member of all the $q$ protected attributes it belongs to: e.g.,~as a female-non-white individual.
As we discuss below, this distinction has clear modeling implications.
%
The tension between these types of multidimensional discrimination occurs as it is possible for $c$ not to suffer multiple discrimination while suffering intersectional discrimination \parencite{Crenshaw1989_DemarginalizingTheIntersection}. 
This is, in particular, troubling as only the former is recognized under EU non-discrimination law \parencite{Xenidis2020_TunningEULaw}. 

For the present k-NN CST implementation we operationalize the two forms of multidimensional discrimination as follows:
%
\begin{itemize}
    \item For multiple discrimination we run CST separately for each $A_i$, including the generation of the corresponding counterfactual datasets via each $do(A_i := 0)$; and look for individual cases in which discrimination is detected across all runs.
    %
    \item For intersectional discrimination we create the \textit{intersectional protected attribute} $A^*$ as in Definition~\ref{def:IntersectionaleDisc}; generate the corresponding counterfactual dataset via $do(A^* := 0)$; and run a single CST as we would for $|A|=1$.
\end{itemize}
%
Beyond using Definitions \ref{def:MultipleDisc} and \ref{def:IntersectionaleDisc}, respectively, both procedures have implications on Section~\ref{sec:CST_ControlTest}.
For multiple discrimination, we repeat Definitions~\ref{def:SearchSpaces}, \ref{def:CounterDataset}, and \ref{def:SearchCenters} for each of the $q$ protected attributes.
For intersectional discrimination, once we have generated $A^{*}$, we apply only once Definitions~\ref{def:SearchSpaces}, \ref{def:CounterDataset}, and \ref{def:SearchCenters} for this ``new'' protected attribute.
Section~\ref{sec:Experiments.Real} showcases both of these procedures for testing multidimensional discrimination.
%
Additionally, under these two producers we can also explore positive discrimination for multiple and intersectional discrimination using k-NN CST. We would simple revisit, respectively, Definitions \ref{def:MultipleDisc} and \ref{def:IntersectionaleDisc} by considering the opposite effect, meaning the relevant delta(s) being less than $\tau$.
We leave this for future work.
% See Appendix~\ref{Appendix.Supplements} for a discussion.

%
% EOS
%
