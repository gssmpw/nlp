%
We position CST with respect to current frameworks for discrimination testing along the goals of actionability and meaningfulness.
Later in Section~\ref{sec:CausalKnowledge} we discuss the role of causality for conceiving discrimination.
For a broader, multidisciplinary view on discrimination testing, we refer to the survey by~\textcite{Romei2014MultiSurveyDiscrimination}. 
For a recent survey of the fair ML testing literature, see \textcite{DBLP:journals/tosem/ChenZHHS24}.

Regarding actionability, it is important when proving discrimination to insure that the framework accounts for sources of randomness in the decision-making process. Popular non-algorithmic frameworks---such as natural \parencite{Godin2000Orchestra} and field \parencite{Bertrand2017_FieldExperimentDiscrimination} experiments, audit \parencite{Fix&Struyk1993_ClearConvincingEvidence} and correspondence \parencite{Bertrand2004_EmilyAndGreg, Rooth2021} studies---address this issue by using multiple observations to build inferential statistics. Similar statistics are asked in court for proving discrimination \parencite[Section 6.3]{EU2018_NonDiscriminationLaw}. 
Few algorithmic frameworks address this issue due to model complexity preventing formal inference \parencite{Athey2019MachineLearningForEconomists}. An exception are data mining frameworks for discrimination discovery \parencite{DBLP:conf/kdd/PedreschiRT08, DBLP:journals/tkdd/RuggieriPT10} that operationalize the non-algorithmic notions, including situation testing \parencite{Thanh_KnnSituationTesting2011, Zhang_CausalSituationTesting_2016}.
These frameworks \parencite{TR-DBLP:conf/sigsoft/GalhotraBM17, TR-DBLP:journals/corr/abs-1809-03260, DBLP:journals/jiis/QureshiKKRP20} keep the focus on comparing multiple control-test instances for making individual claims, providing evidence similar to that produced by the quantitative tools used in court.
It remains unclear if the same can be said about existing causal fair machine learning methods
% \parencite{DBLP:journals/jlap/MakhloufZP24} 
as these have yet to be used beyond academic circles.
The suitability of algorithmic fairness methods for testing discrimination, be it or not ADM, remains an ongoing discussion \parencite{DBLP:conf/fat/WeertsXTOP23}.

Regarding meaningfulness, situation testing and the other methods previously mentioned have been criticized for their handling of the counterfactual question behind the causal model of discrimination \parencite{Kohler2018CausalEddie, Hu_facct_sex_20, Kasirzadeh2021UseMisuse}. In particular, these actionable methods take for granted the influence of the protected attribute on all other attributes. This point can be seen, e.g.,~in how situation testing constructs the test group, which is equivalent to changing the protected attribute while keeping everything else equal. Such an approach goes against how most social scientists interpret the protected attribute and its role as a social construct when proving discrimination \parencite{Bonilla1997_RethinkingRace, rose_constructivist_2022, Sen2016_RaceABundle, Hanna2020_CriticalRace}. 
It is in that regard where structural causal models \parencite{PearlCausality2009} and their ability to generate counterfactuals via the abduction, action, and prediction steps (e.g.,~\textcite{Chiappa2019_PathCF, Yang2021_CausalIntersectionality}), including counterfactual fairness \parencite{Kusner2017CF}, have an advantage.
This advantage is overlooked by critics of counterfactual reasoning \parencite{Kasirzadeh2021UseMisuse, Hu_facct_sex_20}: generating counterfactuals, as long as the structural causal model is properly specified, accounts for the effects of changing the protected attribute on all other attributes. 
Hence, a framework like counterfactual fairness, relative to situation testing and other discrimination discovery methods, is more meaningful in its handling of protected attributes. 

CST bridges these two lines of work, borrowing the actionability aspects from frameworks like situation testing and meaningful aspects from frameworks like counterfactual fairness. 
% Intuitively, 
Counterfactual generation allows to create a comparator for the complainant that accounts for the influence of the protected attribute on the other attributes, departing from the idealized comparison.
It is not far, conceptually, from the broader ML problem of learning fair representations \parencite{Zemel2013LearningFairRepresentations} since we wish to learn (read, map) a new representation of the complainant that reflects where it would have been had it belonged to the non-protected group. 
It is a normative claim on what a non-protected instance similar to the complainant looks like.
Beyond counterfactual fairness and derivatives (e.g., \textcite{Chiappa2019_PathCF}), other works address this problem of deriving such a pair for the complainant.
For instance, \textcite{Plevcko2020FairDataAdaptation} use a quantile regression approach while \textcite{DBLP:journals/corr/abs-2307-12797} use a residual-based approach for generating the pair.
Both works rely on having access to a structural causal model, but do not exploit the abduction, action, and prediction steps for generating counterfactual distributions.
\textcite{BlackYF20_FlipTest}, instead, propose the FlipTest, a non-causal approach that uses an optimal transport mapping to derive the pair for the complainant.
These three works exemplify ML methods that use counterfactual reasoning to operationalize different interpretations of individual similarity. 
With CST we align with these and similar efforts to propose an alternative to the idealized comparison often used in discrimination testing.
% \sr{Isn't propensity score another form of representation learning?}

%
% EOS
%
