\section{EvoP: An Evolutionary Pruning Framework}
\label{sec:dyna}

Figure~\ref{fig:overview} shows the overview of EvoP, which consists of two key steps: 
(1) Build a robust calibration dataset based on the Cluster-based Calibration Data Sampling (CCDS), 
%Cluster the raw dataset based on sentence semantics and sample representative data as the calibration dataset
and (2) Determine the optimal pruning pattern with the Evolutionary Pruning Pattern Search (EPPS).
Below, we show the details of each step.%, and the algorithm is shown in Algorithm \ref{alg:algo}.

%preprocess the dataset and divide the data into semantically preserved chunks, clustering the dataset into $k$ groups based on input characteristics, selecting representative samples from each cluster, 
%and (2) using an Evolutionary Pruning Pattern Search (EPPS) to determine the optimal pruning pattern for each cluster. 

\subsection{CCDS: Cluster-based Calibration Dataset Sampling}
To achieve a more robust pruning pattern, EvoP first partition the raw dataset (e.g., WikiText-2~\cite{wikitext2} or C4~\cite{c4}) into $k$ clusters based on input semantic similarity. 
To ensure the clustering effect and semantic integrity, we first divide the dataset into chunks with the same number of sentences and then cluster the data at the chunk level.
BERT~\cite{BERT} is applied to generate the embedding of each chunk since it can get information-rich representations compared to unidirectional pre-trained language models.
Then, clustering algorithms such as KMeans are used to cluster these embeddings.
%This is achieved using a clustering algorithm such as k-means or hierarchical clustering, where the feature representation for each input is derived from the representation generated by the BERT~\cite{BERT}. 
%As we know, BERT can get information-rich representations compared to unidirectional pre-trained language models.

%The resulting clusters group inputs with similar computational requirements, 
%, enabling cluster-specific pruning strategies.
The generated clusters separate inputs with different computational requirements, allowing for more diverse and representative data sampling.
For each cluster $C_j$ (where j = 1,2,$\dots$,k), We randomly select $n$ representative samples, each of which may contain multiple chunks.
These samples are used to evaluate the performance of different pruning patterns during the search process. 
%The randomness in sampling ensures diversity within each cluster, 
Sampling across all clusters ensures data diversity, 
improving the robustness of the pruning patterns.
The detailed algorithm is shown in Algorithm~\ref{alg:algo}.
% \subsection{Sampling}
% For each cluster $C_j$ (where j = 1,2,$\dots$,k), we randomly select $n$ representative samples. 
% These samples are used to evaluate the performance of different pruning patterns during the optimization. 
% The randomness in sampling ensures diversity within each cluster, improving the robustness of the pruning patterns.

\begin{algorithm}[t]
\caption{Cluster-based Calibration Data Sampling (CCDS)}
\label{alg:algo}
\begin{algorithmic}[1]

\REQUIRE Raw Dataset \( \mathcal{D} \), number of clusters \( k \), number of samples per cluster \( n \), the maximum length of each samples \(len\)
\ENSURE Calibration dataset \( \mathcal{X}\)

% Cluster the Dataset:
\STATE Divide \( \mathcal{D} \) into chunks with the same number of sentences \(\{c_1,c_2,\dots,c_{max}\}\)

\FOR{$i = 1$ \textbf{to} $max$}
    \STATE \(e_i = \text{BERT\_embedding}(c_i) \)
\ENDFOR

\STATE \( \{\mathcal{C}_1, \mathcal{C}_2, \dots, \mathcal{C}_k\} = \text{KMeans}(E,k)\), where \(E=\{e_1,e_2,\dots,e_{max}\}\)

%\STATE Partition embeddings \( E \) into \( k \) clusters \( \{\mathcal{C}_1, \mathcal{C}_2, \dots, \mathcal{C}_k\} \) 

% **Sample Representative Inputs**:
\FOR {each cluster \( \mathcal{C}_j \)} 
     \FOR{$i = 1$ \textbf{to} $n$}
    \WHILE {\(|x_i| < len\)}
        \STATE randomly select a chunk from \( \mathcal{C}_j \) and append to \(x_i\).
    \ENDWHILE
    \STATE Add \(x_i[0:len]\) to \(\mathcal{X}_j\)
    \ENDFOR
\ENDFOR
\RETURN \( \mathcal{X} = \{\mathcal{X}_1,\mathcal{X}_2,\dots,\mathcal{X}_k\}\).

\end{algorithmic}
\end{algorithm}

\begin{algorithm}[t]
\caption{Evolutionary Pruning Pattern Search (EPPS)}
\label{alg:algo2}
\begin{algorithmic}[1]

\REQUIRE Calibration dataset \( \mathcal{X}\), sparsity level \( \theta\), number of generations \( G \), population size \( S \), mutation rate \( \mu \)

\ENSURE Pruning patterns \( \mathbf{p^*} \)
% Cluster the Dataset:
% **Evolutionary Pruning Pattern Search (EPPS)**:
%    - For each cluster \( \mathcal{C}_j \):
%      a. **Initialize Population**:
\STATE  Generate \( S \) random pruning patterns \(  \mathcal{P} =\{\mathbf{p}_1, \mathbf{p}_2, \dots, \mathbf{p}_S\} \) that satisfies sparsity \( \theta\), where \( \mathbf{p}_i \in \{0, 1\}^m \).
\FOR {each generation \( g = 1 \) \textbf{to} \( G \)}
%         i. **Fitness Evaluation**:
    \FOR {each \( \mathbf{p}_i \) in \(\mathcal{P}\)}
        \STATE compute the average loss \( \mathcal{L}(\mathbf{p}_i) \) on \( \mathcal{X}\).
    \ENDFOR
%         ii. **Selection**:
    \STATE  Select the top part of pruning patterns with the lowest loss.
%         iii. **Crossover**:
    \STATE Generate new patterns by combining pairs of selected patterns.
%         iv. **Mutation**:
    \STATE Randomly flip bits in the new patterns with probability \( \mu \).
%         v. **Update Population**:
    \STATE Adjust the new patterns according to sparsity \(\theta\) and replace \(\mathcal{P}\).
%      c. **Select Final Pattern**:
\ENDFOR
\STATE Choose the pruning pattern \( \mathbf{p^*} \) with the lowest loss.
\RETURN The pruning pattern \( \mathbf{p^*}\).

\end{algorithmic}
\end{algorithm}

\subsection{EPPS: Evolutionary Pruning Pattern Search}
Unlike heuristic methods in SLEB~\cite{sleb}, which rely on predefined rules (e.g., magnitude-based pruning), we proposed a revolutionary pruning pattern search algorithm (Algorithm~\ref{alg:algo2}) to explore search space of possible pruning patterns systematically and adaptively. 
First, a population of pruning patterns is initialized, where each pattern is represented as a binary vector $p \in \{0,1\}^{m}$, with 
$m$ is the number of layers in the LLM.
A value of 1 indicates that the layer is pruned, while 0 means it is retained. 
Then, for each pruning pattern $p$, the fitness is computed as the average loss on the $k\times n$ representative samples.
The loss is measured using the task-specific objective (e.g., cross-entropy loss for language modeling). 
EvoP uses perplexity to measure the loss and select the top patterns with the lowest loss (highest fitness) to generate the next generation\footnote{We select top 30\% in the implementation.}.
%Next, pruning patterns with the lowest loss (highest fitness) are selected to form the next generation. 
This mimics natural selection, where the fittest individuals survive. 
New pruning patterns are generated by combining (crossover) and randomly altering (mutation) the selected patterns. 
This introduces diversity into the population, enabling the exploration of new pruning configurations. Note that the newly pruning patterns may not meet the sparsity requirements, so we randomly select several corresponding positions to flip until the required sparsity is met (double mutation).
Last, the process repeats for several generations or until convergence. 
The final pruning pattern for the cluster is the one with the lowest loss.

% \subsection{Build pruning pattern database}
% Before inference, EvoP needs to build a pruning database for all datasets.
% This process involves two key steps: 
% (1) labeling the dataset $D$ with cluster-specific pruning patterns,
% and (2) building a retriever database to map inputs of $D$ to pruning patterns.

%  \textbf{Labeling the Dataset with Pruning Patterns} 
%  After obtaining the cluster-specific pruning patterns $\{m_1, m_2, \dots, m_k \}$ during the training phase, we assign the corresponding pruning pattern to all samples within each cluster. 
%  Specifically, for each cluster $C_j$, every sample $x \in C_j$ is labeled with the pruning pattern $m_j$.
%  This ensures the entire dataset is annotated with the appropriate pruning strategy for efficient inference. 
 
%  \textbf{Building the Retriever Database} 
%  To enable dynamic and efficient pruning at inference time, we construct a retriever database $R$ that stores each sample representation and its corresponding pruning pattern.
% For each sample $x$ in the dataset, we extract its feature representation $f(x)$ using the embedding layer of the BERT~\cite{BERT}. 
% This representation captures the semantic information of the input.
% Then, the retriever database $R$ is constructed as a key-value store, where the key is the feature representation $f(x)$ of the sample. 
% The value is the pruning pattern $m_j$ associated with the cluster $C_j$ to which $x$ belongs.
% The database is indexed using an efficient nearest-neighbor search algorithm (e.g., FAISS~\cite{faiss}) to enable fast retrieval during inference.

% \subsection{Inference Stage}
% At the inference stage, EvoP dynamically adapts the pruning strategy for each input based on its semantic characteristics, retrieving and ensembling pruning patterns for test samples. 
% When a test sample $x_{test}$ arrives, EvoP dynamically determines its pruning pattern by retrieving existing patterns of the most semantically similar samples from the retriever database. 
% $x_{test}$  queries the retriever database $R$ to retrieve the top $t$ samples with the highest semantic similarity to $x_{test}$. 
% This is achieved by performing a nearest-neighbor search on the feature representations. 
% Let  $\{m_1, m_2, \dots, m_t \}$ be the pruning patterns of the retrieved samples. 
% The fisdnal pruning pattern $m_{test}$ for the test sample is obtained by assembling these patterns.
% Spefcaifically, for each layer $l$ in the LLM, compute the fraction of retrieved patterns that retain the layer:

% \begin{equation}
%     ensemble algorithm
% \end{equation}

% Last, apply the ensembled pruning pattern $m_{test}$ to the LLM and perform inference on $x_{test}$. This ensures the model dynamically adapts its computational load based on the input's semantic characteristics.

% \begin{algorithm}[t]
% \caption{\small EvoP-Dynamic Pruning via Evolutionary Pruning Pattern Search (EPPS)}
% \label{alg:algo}
% \begin{algorithmic}[1]

% \REQUIRE Dataset \( \mathcal{D} \) (e.g., Wikitext or C4), number of clusters \( k \), number of samples per cluster \( n \), number of generations \( G \), population size \( P \), mutation rate \( \mu \)

% \ENSURE Cluster-specific pruning patterns \( \{\mathbf{m}_1, \mathbf{m}_2, \dots, \mathbf{m}_k\} \)
% \STATE  Cluster the Dataset:
%    - Compute input embeddings using the LLM.
%    - Partition \( \mathcal{D} \) into \( k \) clusters \( \{\mathcal{C}_1, \mathcal{C}_2, \dots, \mathcal{C}_k\} \) using \( k \)-means or hierarchical clustering.

% \STATE **Sample Representative Inputs**:
%    - For each cluster \( \mathcal{C}_j \), randomly select \( n \) samples \( \mathcal{X}_j = \{x_1, x_2, \dots, x_n\} \).

% \STATE **Evolutionary Pruning Pattern Search (EPPS)**:
%    - For each cluster \( \mathcal{C}_j \):
%      a. **Initialize Population**:
%         - Generate \( P \) random pruning patterns \( \{\mathbf{m}_1, \mathbf{m}_2, \dots, \mathbf{m}_P\} \), where \( \mathbf{m}_i \in \{0, 1\}^L \).
%      b. **For each generation \( g = 1 \) to \( G \)**:
%         i. **Fitness Evaluation**:
%            - For each \( \mathbf{m}_i \), compute the average loss \( \mathcal{L}(\mathbf{m}_i) \) on \( \mathcal{X}_j \).
%         ii. **Selection**:
%             - Select the top \( P/2 \) pruning patterns with the lowest loss.
%         iii. **Crossover**:
%             - Generate new patterns by combining pairs of selected patterns.
%         iv. **Mutation**:
%             - Randomly flip bits in the new patterns with probability \( \mu \).
%         v. **Update Population**:
%            - Replace the current population with the new patterns.
%      c. **Select Final Pattern**:
%         - Choose the pruning pattern \( \mathbf{m}_j \) with the lowest loss.

% \RETURN The set of cluster-specific pruning patterns \( \{\mathbf{m}_1, \mathbf{m}_2, \dots, \mathbf{m}_k\} \).

% \end{algorithmic}
% \end{algorithm}