\section{Experiments}
In this section, we first introduce the dataset and the experimental setup.
Then, we present the results of our EvoP and different pruning techniques on five downstream tasks.
We also conduct analysis experiments to show our EvoP and those pruning techniques' in-domain and out-of-domain performance.
Finally, we show the ablation studies.


\begin{table*}[t]
\centering
\begin{tabular}{lccccccccc}
\toprule
\multicolumn{2}{c}{Methods} & Sparsity & SpeedUp & ARC\_E & ARC\_C & HellaSwag & PIQA & Winogrande & Avg \\
\midrule
\multirow{8}{*}{\rotatebox{90}{Llama-2-13b}} 
& Dense & 0\% & 1.00x & 0.82 & 0.53 & 0.61 & 0.80 & 0.75 & 0.70 \\ \cline{2-10}
& Wanda (2:4) & 50\% & 1.00x & 0.71 & 0.35 & 0.43 & 0.72 & 0.62 & 0.57 \\ 
& \multirow{2}{*}{SliceGPT} & 10\% & 0.99x & 0.65 & 0.34 & 0.44 & 0.69 & 0.70 & 0.56 \\
& & 20\% & 1.10x & 0.44 & 0.24 & 0.34 & 0.60 & 0.62 & 0.45 \\
& \multirow{2}{*}{SLEB} & 10\% & 1.10x & \textbf{0.77} & 0.43 & 0.56 & 0.78 & 0.69 & 0.65 \\
& & 20\% & 1.23x & \underline{0.73} & 0.39 & 0.50 & \underline{0.77} & 0.65 & 0.61 \\ \cline{2-10}
& \multirow{2}{*}{EvoP} & 10\% & 1.10x & \textbf{0.77} & \textbf{0.46} & \textbf{0.57} & \textbf{0.79} & \textbf{0.71} & \textbf{0.66} \\
& & 20\% & 1.23x & \underline{0.73} & \underline{0.40} & \underline{0.52} & \underline{0.77} & \underline{0.68} & \underline{0.62} \\ 
\midrule
\multirow{8}{*}{\rotatebox{90}{OPT-13b}}
& Dense & 0\% & 1.00x & 0.71 & 0.35 & 0.52 & 0.76 & 0.67 & 0.60 \\ \cline{2-10}
& Wanda (2:4) & 50\% & 1.00x &  0.63 & 0.28 & 0.42 & 0.71 & 0.60 & 0.53 \\
& \multirow{2}{*}{SliceGPT} & 10\% & 0.93x & 0.67 & 0.32 & 0.49 & 0.74 & 0.65 & 0.57 \\
& & 20\% & 1.01x & 0.60 & 0.31 & 0.44 & 0.69 & 0.63 & 0.53 \\
& \multirow{2}{*}{SLEB} & 10\% & 1.11x & \textbf{0.70} & 0.34 & \textbf{0.52} & \textbf{0.76} & \textbf{0.67} & \textbf{0.60} \\
& & 20\% & 1.24x & 0.65 & 0.30 & 0.47 & 0.74 & 0.64 & 0.56 \\ \cline{2-10}
& \multirow{2}{*}{EvoP} & 10\% & 1.11x & \textbf{0.70} & \textbf{0.35} & \textbf{0.52} & \textbf{0.76} & \textbf{0.67} & \textbf{0.60} \\
& & 20\% & 1.24x & \underline{0.67} & \underline{0.33} & \underline{0.50} & \underline{0.75} & \underline{0.66} & \underline{0.58} \\ 
\midrule
\multirow{8}{*}{\rotatebox{90}{OPT-30b}}
& Dense & 0\% & 1.00x & 0.72 & 0.37 & 0.55 & 0.77 & 0.69 & 0.62 \\ \cline{2-10}
& Wanda (2:4) & 50\% & 1.00x & 0.64 & 0.29 & 0.42 & 0.70 & 0.61 & 0.53 \\
& \multirow{2}{*}{SliceGPT} & 10\% & 0.95x & 0.69 & 0.34 & 0.51 & 0.75 & 0.68 & 0.59 \\
& & 20\% & 1.01x & 0.63 & 0.32 & 0.47 & 0.72 & 0.65 & 0.56 \\
& \multirow{2}{*}{SLEB} & 10\% & 1.12x & 0.71 & 0.36 & 0.53 & \textbf{0.77} & 0.68 & 0.61 \\
& & 20\% & 1.26x & 0.69 & \underline{0.35} & 0.50 & 0.75 & 0.65 & \underline{0.59} \\ \cline{2-10}
& \multirow{2}{*}{EvoP} & 10\% & 1.12x & \textbf{0.72} & \textbf{0.38} & \textbf{0.55} & \textbf{0.77} & \textbf{0.69} & \textbf{0.62} \\
& & 20\% & 1.26x & \underline{0.70} & 0.34 & \underline{0.51} & \underline{0.76} & \underline{0.66} & \underline{0.59} \\
\bottomrule
\end{tabular}
\caption{Performance on five downstream tasks. Results in \textbf{bold} are the best performance with 10\% sparsity, and results in \underline{underline} are the best performance with 20\% sparsity. Since Wanda only supports 50\%, we compare it to models with 10\% and 20\%, respectively. `ARC\_C' and `ARC\_E' represent the tasks ARC\_Easy and ARC\_Challenge.}
\label{tab-down}
\end{table*}

\subsection{Datasets and Experiments Setups}

\textbf{Datasets.} Following SLEB~\cite{sleb}, we also use the library LM-Harness~\cite{eval-harness} to evaluate our EvoP and other pruning techniques on the same five representative datasets.
These datasets cover a wide range of downstream tasks, including reasoning, commonsense understanding, and question answering, making them suitable for evaluating the generalization ability of pruned models.

\noindent\textbf{Models.} We conducted experiments on three large language models, i.e., Llama-2-13b~\cite{llama2}, OPT-13b, and OPT-30b~\cite{opt}.
These models can serve as representative benchmarks to analyze the effectiveness of our EvoP across different architectures and scales.

\noindent\textbf{Baselines.} We choose the state-of-the-art techniques of each kind of network pruning,
\begin{itemize}[nosep]
    \item \textbf{Wanda~\cite{wanda}:} A superior element-wise pruning that prunes weights with the smallest magnitudes multiplied by the corresponding input activations.
    \item \textbf{SliceGPT~\cite{slicegpt}:} An advanced channel-wise pruning that prunes the columns or rows of weights.
    \item \textbf{SLEB~\cite{sleb}:} A surpassing layer-wise pruning that removes some layers or blocks of the model.
\end{itemize}
Noted, all baselines, including our EvoP, require no retraining or weight update, and the pruned LLM can be used as is.

\noindent\textbf{Implementation Details.} The proposed EvoP was implemented using the PyTorch and Transformer frameworks.
We evaluated the baseline with Llama-2-13b and OPT-13b on one NVIDIA A100 GPU with 40GB memory and 
the baseline with OPT-30b on one NVIDIA A800 GPU with 80GB memory.
The size of the calibration dataset includes 5 samples with 2048 tokens each, which is equivalent to the dataset used in SLEB~\cite{sleb}.
The number of clusters in EvoP is set to 5, and the max number of iterations of EPPS is set to 100 for models with 13 billion parameters and 200 for models with 30 billion parameters.



\begin{table*}[t]
\centering
\begin{tabular}{lccccccc}
\toprule
\multirow{2}{*}{Methods} & \multirow{2}{*}{Sparsity} & \multicolumn{3}{c}{WikiText2-WikiText2} & \multicolumn{3}{c}{C4-C4} \\
& & Llama-2-13b & OPT-13b & OPT-30b & Llama-2-13b & OPT-13b & OPT-30b \\
\midrule
Dense & 0\% & 4.57 & 10.13 & 9.56 & 6.52 & 12.07 & 11.46 \\ \hline
Wanda & 50\% & 7.93 & 15.17 & 14.93 & 12.28 & 16.97 & 17.50 \\ 
SliceGPT & 10\% & 6.57 & 11.08 & 10.51 & 10.49 & 13.73 & 12.73 \\
SliceGPT & 20\% & 10.26 & 13.19 & 12.09 & 16.93 & 17.50 & 15.88 \\ 
SLEB & 10\% & 5.37 & 10.30 & 10.19 & 7.57 & 12.45 & 11.57 \\
SLEB & 20\% & 6.68 & 12.82 & 11.07 &  \underline{9.00} & 13.97 & 12.17\\
\midrule
EvoP & 10\% & \textbf{5.24} &  \textbf{10.16} &  \textbf{9.75} &  \textbf{7.51} &  \textbf{12.35} &  \textbf{11.55} \\
EvoP & 20\% &  \underline{6.30} &  \underline{11.56} &  \underline{11.01} & 9.01 &  \underline{13.63} &  \underline{12.10} \\
\bottomrule
\end{tabular}
\caption{In-domain perplexity results. Results in \textbf{bold} are the best performance with 10\% sparsity, and results in \underline{underline} are the best performance with 20\% sparsity.}
\label{tab-id-ppl}
\end{table*}

\begin{table*}[t]
\centering
\begin{tabular}{lccccccc}
\toprule
\multirow{2}{*}{Methods} & \multirow{2}{*}{Sparsity} & \multicolumn{3}{c}{WikiText2-C4} & \multicolumn{3}{c}{C4-WikiText2} \\
& & Llama-2-13b & OPT-13b & OPT-30b & Llama-2-13b & OPT-13b & OPT-30b \\
\midrule
Dense & 0\% & 6.52 & 12.07 & 11.46 & 4.57 & 10.13 & 9.56 \\
\midrule
Wanda & 50\% & 12.53 & 18.94 & 22.08 & 8.58 & 17.51 & 16.42 \\ 
SliceGPT & 10\% & 18.42 & 14.04 & 13.09 & 9.69 & 11.91 & 11.26 \\
SliceGPT & 20\% & 43.77 & 19.24 & 16.74 & 22.72 & 17.16 & 15.03 \\ 
SLEB & 10\% & 7.62 & 12.44 & 12.36 & 5.33 & 10.56 & \textbf{9.65} \\
SLEB & 20\% & 9.38 & 15.22 & 13.76 & \underline{6.31} & 12.40 & 10.22 \\
\midrule
EvoP & 10\% & \textbf{7.55} & \textbf{12.37} & \textbf{11.68} & \textbf{5.26} & \textbf{10.38} & \textbf{9.65} \\
EvoP & 20\% & \underline{8.97} & \underline{13.69} & \underline{13.14} & 6.33 & \underline{11.42} & \underline{10.15} \\
\bottomrule
\end{tabular}
\caption{Out-of-domain perplexity results. Results in \textbf{bold} are the best performance with 10\% sparsity, and results in \underline{underline} are the best performance with 20\% sparsity.}
\label{tab-ood-ppl}
\end{table*}

\subsection{Results on Downstream Tasks}

Table~\ref{tab-down} shows the performance on six downstream tasks.
Overall, experimental results demonstrate that the proposed EvoP outperforms existing state-of-the-art pruning techniques on the representative downstream tasks across different sparsity levels.
Compared to the lower sparsity (10\%), the higher sparsity (20\%) represents a larger pruning pattern space, which increases the difficulty of the network pruning problem. 
From the results, our EvoP achieves a better performance on higher-sparsity cases, consistently achieving better performance compared to baselines.
The above results indicate EvoP's robustness in balancing sparsity and performance.

Table~\ref{tab-down} also presents the inference speedup of the pruned model with different pruning techniques compared to the dense model.
Wanda achieves the same inference speed as that of the dense model.
This is due to the fact that Wanda requires customized implementations using hardware-friendly codes.
Otherwise, running Wanda's pruned model is like running the dense model with half-zero weights.
SliceGPT with lower sparsity runs a bit slower than the dense model but a bit faster when with high sparsity.
This might lie in that the extra overheads of SliceGPT are quite large; only when applying large sparsity would the efficiency benefits be dominant.
For SLEB and our EvoP, these two are all layer-wise pruning, which only removes some layers.
The pruned model would be quite hardware-friendly; thus, the speedup gains as much as the given sparsity.
Our EvoP only removes layers according to the searched pruning pattern. 
Thus, its speedup is exactly the same as that of SLEB.

\subsection{Results on Language Modeling}
Table~\ref{tab-id-ppl} and Table~\ref{tab-ood-ppl} show the in-domain and out-of-domain perplexity results on the wikitext-2 and C4 datasets.
Experiments in Table~\ref{tab-id-ppl} show that our EvoP consistently outperforms other pruning techniques, especially on the sparsity of 10\%, demonstrating its effectiveness in maintaining in-domain performance.
Similar conclusions can also be drawn from out-of-domain results. 


\subsection{Ablation Study}

% \begin{table}[t]
% \centering
% \begin{tabular}{lcccc}
% \toprule
% Methods & Sparsity & L-13 & O-13 & O-30 \\
% \midrule
% Wanda & 50\% & 12.53 & 18.94 & 22.08 \\
% \quad + CCDS & 50\% & 12.38 & 21.43 & 25.37 \\
% SliceGPT & 10\% & 18.42 & 14.04 & 13.09 \\
% \quad + CCDS & 10\% & 18.62 & 14.55 & 13.32 \\
% SliceGPT & 20\% & 43.77 & 19.24 & 16.74 \\
% \quad + CCDS & 20\% & 38.50 & 21.34 & 17.82 \\
% SLEB & 10\% & 7.62 & 12.44 & 12.36 \\
% \quad + CCDS & 10\% & 7.55 & 12.37 & 11.63 \\
% SLEB & 20\% & 9.38 & 15.22 & 13.76 \\
% \quad + CCDS & 20\% & 9.31 & 13.78 & 12.79 \\
% \bottomrule
% \end{tabular}
% \caption{Perplexity results on C4 datasets using the calibration dataset sampled from WikiText-2.}
% \label{tab-ablation}
% \end{table}

\begin{table}[t]
\centering
\begin{tabular}{lccc}
\toprule
Methods & Sparsity & L-13 & O-13 \\
\midrule
EvoP & 10\% & 7.51 & 12.35 \\
\quad w/o CCDS & 10\% & 7.62 & 12.44  \\
\quad w/o EPPS & 10\% & 7.55 & 12.37   \\
EvoP & 20\% & 9.01 & 13.63 \\
\quad w/o CCDS & 20\% & 9.21 & 13.76 \\
\quad w/o EPPS & 20\% & 9.31 & 13.78    \\
\bottomrule
\end{tabular}
\caption{Perplexity results on C4 datasets using the calibration dataset sampled from WikiText-2. The heuristic algorithm used in SLEB is adopted for EvoP w/o EPPS. `L-13' and `O-13' represents Llama-2-13b model and OPT-13b model.}
\label{tab-ablation}
\end{table}

We conducted the ablation study of different components with different sparsity on the C4 calibration dataset.
As shown in Table~\ref{tab-ablation}, CCDS can help find a better pruning pattern on the low sparsity level, while EPPS contributes more to the performance improvements on the high sparsity level.
This may be due to the fact that at the low sparsity level, the pruning pattern space is not large enough, and heuristic algorithms can easily find the same optimal pruning pattern as the EPPS.
However, at the high sparsity level, a large pruning pattern space is better for demonstrating the effectiveness of the EPPS, thus achieving more performance improvements.
% The EPPS consistently brings more performance improvements compared to that of the CCDS, especially in high-sparsity scenarios (large pruning pattern space).
% This is because the performance improvements gained through the pruning pattern of EPPS search are usually easier to achieve than by sampling a better dataset that includes new global optimal solutions.

% As shown in Table~\ref{tab-ablation}, SLEB can further improve the performance due to more diversity introduced by our proposed CCDS, especially on higher sparsity levels.
% On the contrary, SliceGPT benefits little from the CCDS's data and only improves on high-sparsity llama models.
% The reasons may lie in the fact that channel-wise pruning requires determining which channel should be pruned based on the calibration dataset, but CCDS improves the diversity of the calibration dataset, increasing the difficulties of each sample for pruning.
% Unfortunately, Wanda also degrades a lot as CCDS's global sampling may not provide sufficient local information to preserve crucial weights.
