\section{Related Work}

\textbf{Language Models Compression.} There are several main techniques for language model compression: knowledge distillation~\cite{kd1, kd2, Meta_kd}, quantization~\cite{ZeroQuant, quantization_survey, SmoothQuant}, network pruning or sparsity,~\cite{chess, sparcegpt, slicegpt, wanda, dsnot, sleb, DejaVu} and early exit~\cite{24arxiv-raee, SEENN, deebert}. Knowledge distillation methods transfer knowledge from a large, complex model (called the teacher model) to a smaller, simpler model (called the student model). They must either learn the output distribution of teacher models~\cite{kd3} or design multi-task approaches~\cite{kd4} to ensure that student models retain the knowledge and generative capabilities of the teacher models.  Quantization methods compress language models by reducing the precision of the numerical values representing the model's parameters (e.g., weights and activations). For example, OneBit quantized the weight matrices of LLMs to 1-bit~\cite{1bit}. Early exit methods allow a model to terminate its processing early during inference if it has already made a confident prediction, avoiding the need for additional layers of computation~\cite{ee_llm}. Network pruning, also known as sparsity techniques, refers to methods employed to compress language models by eliminating less significant structures within the network, such as individual weights, neurons, or layers~\cite{sparcegpt, slicegpt, sleb}. The primary objectives are to reduce the modelâ€™s size, enhance inference speed, and decrease memory consumption while preserving or only marginally affecting its performance. These works are orthogonal to each other, and we focus on the pruning methods.

\textbf{Different Granularity of Pruning.} Recent studies have investigated pruning at various granularities, ranging from coarse to fine. At the coarse-grained level, pruning methods include layer-wise~\cite{sleb}, attention heads~\cite{att_prun}, channel-wise pruning~\cite{slicegpt,llmpruner}, and neurons~\cite{neur_prun}. At the fine-grained level, techniques such as N:M sparsity~\cite{sparcegpt, wanda, dsnot, nm_sparse1, nm_sparse2} and individual weight pruning~\cite{sparsert} have been explored.
This paper mainly explored but was not limited to layer-wise network pruning.
