\section{Background}
\subsection{Network Pruning}
Network pruning is a key technique in the LLM era, which aims to reduce neural networks' size and computational complexity by removing unnecessary or redundant structures, such as elements~\cite{wanda}, channels~\cite{slicegpt}, or layers~\cite{sleb}. 
Existing methods target different levels of pruning granularity, from fine-grained element-wise pruning to coarse-grained layer-wise removal, offering a trade-off between model compression, hardware efficiency, and task performance. 

This paper formally defines the network pruning problem to understand and unify these approaches better.
Given a sparsity hyperparameter $\theta$, a pre-trained backbone model $\mathcal{M}$ with parameters $W$, and a calibration dataset $\mathcal{D}$ with $n$ data, the goal of network pruning is to find a pruning pattern $p$ in the Pruning Pattern Space $P$ (PPS) that minimizes the loss of the pruned model $\mathcal{M}^\theta$ over the calibration dataset $\mathcal{D}$,
\begin{equation}
    \min_{p\in P}\mathcal{L}(f(\mathcal{M}, p), \mathcal{D}), \text{s.t.} \|p\|_0 \leq \theta \cdot \|W\|_0
\end{equation}
, where $\mathcal{L}(\cdot)$ is the loss function, $p\in \{0, 1\}^m$ is a binary vector of zeros and ones, indicating a pruning pattern over $m$ pruning components, which can be weight elements, channels, or layers.
$p[i] = 1$ represents the $i$-th components in the model $\mathcal{M}$ is pruned, and vice versa.
Moreover, $f(, )$ is the function that applies the pruning pattern $p$ to the pre-trained model $\mathcal{M}$ with different pruning techniques.
$\|p\|_0$ and $\|W\|_0$ are the number of non-zeros values in $p$ and the total number of parameters in $W$.

\begin{figure}[t]
\centering
\includegraphics[]{fig/fig1-motivation.pdf}
\caption{Perplexity loss of the Tinyllama model on the calibration dataset across different sparsity. The Ideal is the globally optimal solution in each pruning pattern space. The Gene is the solution using the genetic algorithm. \textbf{SLEB fails to find optimal when the pruning pattern space becomes larger (large sparsity)}.}
\label{fig:ideal}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[width=1\linewidth]{fig/fig2-cluster.png}
\caption{Visualization of the semantic distribution of the calibration dataset. Each point represents an input sentence. Points with the same color are clustered together by KMeans.}
\label{fig:cluster}
\end{figure}


\subsection{Challenges of Different Pruning Techniques}
The above formulation provides a unified framework for understanding and comparing different pruning methods. 
However, each pruning granularity introduces unique challenges.
Element-wise pruning is a kind of unstructured pruning, which is not a hardware-friendly technique, requiring specific computation kernel designs and implementations.
For example, 2:4 pruning~\cite{sparcegpt, wanda, dsnot, nm_sparse1, nm_sparse2} relies on specific sparse matrix multiplication operators and is currently only supported by GPUs with NVIDIA Ampere architecture.
Besides, the pruning pattern space $P$ of the element-wise pruning can be enormous, resulting in the proposed solutions being generally heuristic algorithms on each weight matrix~\citep{sparsert, neur_prun}.

Channel-wise pruning removes the entire channel for better hardware efficiency~\citep{slicegpt,llmpruner}. 
However, complex dependencies between channels often make it challenging to determine which channel should be pruned, thus leading to substantial performance drops~\cite{att_prun}. 

Recent works~\cite {24arxiv-raee, sleb} pay more attention to coarse-grained pruning, i.e., layer-wise pruning, due to its greater hardware efficiency and better model performance.
Those works can be categorized into two branches: early exit and layer dropping.
The former introduces intermediate exit points within the model and terminates the inference at the predicted exit point~\citep{24arxiv-raee, deebert}, while the latter drops some layers based on the calibration dataset and leaves the pruned model for the inference~\citep{sleb}.
Compared to layer dropping, although early exit seems an easier-to-solve approach to pruning the model, it isn't easy to accelerate the inference in batches and reduce memory overheads in deployment~\cite{sleb}.
Considering the above challenges of different pruning techniques, this paper mainly focuses on optimizing the layer dropping in layer-wise pruning.

\section{Observations}

This section presents two observations regarding algorithms and data, which motivate us to propose corresponding methods.

\begin{figure}[t]
\centering
\includegraphics[]{fig/fig3-diff-optimal.pdf}
\caption{Perplexity of SLEB and Gene using different calibration datasets. The sparsity of Tinyllama is set to 50\% for large enough PPS. %`RA' is randomly sampling from the whole dataset, `RC' is randomly sampling data across clusters, 
`C0', `C1', ..., and `C4' are randomly sampled within each cluster.}
\label{fig:diff-optimal}
\end{figure}

\begin{figure*}[t]
\centering
\includegraphics[width=0.8\linewidth]{fig/fig4-overview.pdf}
\caption{The overview of EvoP. The left part shows the cluster-based calibration dataset sampling. The right part presents the evolutionary pruning pattern search.}
\label{fig:overview}
\end{figure*}

\subsection{Observation 1: Sub-optimal Pruning Pattern of Heuristics in Large PPS}

Existing state-of-the-art layer-dropping techniques adopt heuristic algorithms to select layers to drop.
For example, SLEB~\cite{sleb} enumerates all unpruned layers and removes the one where the model without the layer achieves the minimum perplexity loss on the calibration dataset.
Then, SLEB repeats this process until the given sparsity is reached.
This simple yet effective technique is successful because the SLEB algorithms can achieve as optimal as ideal solutions when the pruning pattern space is not large enough (small sparsity).

To better support the above claims, we conducted an analysis experiment on the Tinyllama model~\cite{tinyllama} over the calibration dataset in Figure~\ref{fig:ideal}.
We follow the data selection of SLEB to build the calibration dataset from WikiText-2~\cite{wikitext2}.
We collect three sets of perplexity losses over different sparsities: the SLEB, the Ideal that enumerates the whole pruning pattern space, and the Gene using the genetic algorithm to search.

As expected, when the sparsity is below around 35\%, SLEB can easily find the global optimal %since the amount of the pruning pattern space is only about 0.32 million.
since the pruning pattern space is small, only seven layers need to be selected at most.
However, when the size of the pruning pattern space increases (larger sparsity), SLEB gradually gets stuck in the local optimal.
Therefore, this motivates us to propose a new algorithm to further optimize the pruning in the scenarios with larger pruning pattern space. 
Noted, the size of the pruning pattern space is not only related to the given sparsity but also depends on the original model size, which can be calculated as the combinatorial number~\footnote{The number of model layers is the total number of $N$, and the number of pruned layers is the degree $k$, thus the size of the pruning pattern space is $\binom{N}{k}$.}. 
Therefore, even a smaller sparsity (e.g., 10\%-20\%) can lead to an ample pruning pattern space for larger models.

\subsection{Observation 2: Low Data Diversity of Calibration Dataset}

%The above analysis shows the potential to approach the global optimal.
The above analysis shows the potential to approach the global optimal.
However, the global optimal might differ when pruning on different calibration datasets.
First, we analyze the semantic distribution of WikiText-2 and visualize the distribution in Figure~\ref{fig:cluster}.
We use the BERT-base~\cite{BERT} model to encode each sentence and TSNE to reduce the embedding dimensions.
Figure~\ref{fig:cluster} demonstrates that some sentences are semantically similar and naturally clustered.
We also use KMeans to cluster those sentences and draw the colors, proving this claim.

Based on the data characteristics, we further show the impact of different calibration datasets on the global optimal pruning patterns.
Since Figure~\ref{fig:ideal} has demonstrated the Gene can achieve comparable performance to the Ideal, we would regard the Gene's results as the Ideal's.
We collect the perplexity of SLEB and Gene using the samples from each cluster in Figure~\ref{fig:diff-optimal}.
Experimental results show that different calibration datasets may lead to different optimal.
This motivates us to propose a new data sampling method to help the pruning algorithms reach a better optimal.
