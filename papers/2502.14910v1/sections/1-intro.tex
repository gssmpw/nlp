\section{Introduction}

Large Language Models (LLMs), such as Llama~\cite{llama2} and OPT~\cite{opt}, have revolutionized natural language processing (NLP) by achieving state-of-the-art performance across a wide range of tasks~\cite{llama, opt, bloom, palm, lamda, glm, palm}.
However, the success of these models comes at a significant computational cost. 
LLM consists of billions of parameters, requiring substantial memory, storage, and energy resources. 
This computational burden limits their deployment in resource-constrained environments. 

To address these challenges, network pruning~\cite{lecun_prun} has emerged as a critical technique for optimizing LLMs by reducing their size while preserving their performance.
Pruning involves selectively removing redundant or less important parameters (e.g., elements~\cite{wanda}, channels~\cite{slicegpt}, or layers~\cite{sleb}) from a pre-trained model, which enables faster inference and lower energy consumption. 
Importantly, pruning is feasible in LLMs because these models are often over-parameterized, containing more parameters than necessary to achieve their performance.

% Traditional pruning methods often rely on iterative training and fine-tuning to recover accuracy after removing redundant weights or neurons~\cite{itea_prun, iclr_prun}. 
% However, in scenarios where training is infeasible or computationally prohibitive, training-free pruning methods have gained attention as a practical alternative~\cite{wanda,sleb,slicegpt,dsnot}. 
% These methods aim to identify and remove less important structures of a model from different granularity while maintaining its performance, all without the need for retraining.
% Despite their efficiency, training-free pruning approaches face significant challenges, particularly in achieving optimal pruning structures and ensuring generalization across diverse downstream tasks.

Unfortunately, existing pruning techniques still face different challenges. 
Element-wise pruning achieves the finest-grained pruning to obtain a quite high sparsity, e.g., 50\%, but requires specific computation kernels and hardware support to accelerate the inference.
Channel-wise pruning trades off the hardware efficiency and sparsity. 
However, the complex channel dependency increases the difficulty of determining the pruning channel, thus resulting in performance drops.
Layer-wise pruning adopts heuristic algorithms to search the pruning pattern and maintain the hardware friendliness for faster inference.
But it often fails to find the optimal pruning patterns.
% Recent methods for training-free pruning often employ heuristic strategies~\cite{sleb}, such as greedy search, to select pruned structures.
% While these approaches are computationally efficient, they are prone to sub-optimal solutions, as they may prematurely converge to local optima.
% Furthermore, the effectiveness of pruning is highly dependent on the data used to guide the process.
% Different datasets exhibit distinct pruning patterns, and relying on a narrow or homogeneous dataset can lead to pruned models that fail to generalize well to unseen tasks.
% These observations highlight the need for a more systematic and data-aware approach to training-free pruning.
% In this work, we formulate the problem of training-free pruning as a state space search problem, where the goal is to identify high-quality pruning structures that minimize the model's loss.

To address the challenges, we follow layer-wise pruning and introduce a novel evolutionary pruning framework, EvoP.
EvoP first presents a cluster-based calibration dataset sampling strategy for collecting a more diverse calibration dataset.
EvoP then combines the diverse calibration dataset with an evolutionary pruning pattern search method to identify the optimal pruning patterns that generalize well across tasks.
Extensive experiments demonstrate that the proposed EvoP outperforms existing state-of-the-art pruning methods of different granularity.
The proposed EvoP also performs well on in-domain and out-of-domain perplexity datasets, showing its better generalization capabilities. 
Codes are available at~\footnote{https://anonymous.4open.science/r/EvoP-D4DA}.


The main contributions of this paper are:
\begin{itemize}
    \item We formulate the network pruning problem and present two key observations, including suboptimal solutions in the algorithm aspect and low data diversity in the dataset aspect.
    \item Based on the observations, we propose the evolutionary pruning framework, EvoP, which includes a cluster-based calibration dataset sampling and an evolutionary pruning pattern search.
    \item Experimental results show that the proposed EvoP can achieve the best performance across five downstream tasks and in-/out-of-domain datasets.
\end{itemize}
