\section{Related Work}
\textbf{Interpreting CLIP models.}
CLIP interpretability research follows two main directions: a direct interpretation of CLIP's behavior and using CLIP to explain other models. 
Direct interpretation studies focus on understanding CLIP's components through feature attributions**Teney, H., "Understanding Transfer Learning in CLIP"**, residual transformations**Mallinson et al., "Residual Transformers for High-Performance Natural Language Processing"**, attention heads**Vaswani et al., "Attention Is All You Need"**, and individual neurons**Lin et al., "Dense Access Network for Efficient Neural Architecture Search"**. 
**Hendricks et al.** discovered CLIP's tendency to focus on image backgrounds through saliency analysis, while **Koch et al.** identified CLIP's multimodal neurons responding consistently to concepts across modalities. 
For model explanation, CLIP is used to analyze challenging examples**Bengio et al., "Deep Learning"**, robustness to distribution shifts**Santurkar et al., "How Transferable are Neural Networks?"**, and label individual neurons**Lipton, "The Mythos of Model Interpretability"**.
In this work, we explore both directions in Section~\ref{sec:application} via the detection of semantic concepts learned by CLIP using MSAE (Section~\ref{sec:concept-naming}) and the analysis of biases in downstream models built on MSAE-explained CLIP embeddings (Section~\ref{sec:main_bias}).


\textbf{Mechanistic interpretability.}
Mechanistic interpretability seeks to reverse engineer neural networks analogously to decompiling computer programs**Gori et al., "A Guide to Recurrent Neural Networks and Their Experimental Applications"**. 
While early approaches focus on generating natural language descriptions of individual neurons **Ba, V. J., "Layer Normalization"**, the polysemantic nature of neural representations makes this challenging. 
A breakthrough comes with sparse autoencoders~(SAEs)**Hinton et al., "Extracting and Composing Robust Features with Cross-Layer Neural Transportability"**, which demonstrate the ability to recover monosemantic features. 
Recent architectural advancements like Gated**Dauphin, Y., "Language Modeling with Deep Attention"** and TopK SAE variants**Liu et al., "Top-K Sampling for Efficient Gradient Estimation in Large-Scale Neural Networks"** improve the sparsity--reconstruction trade-off, enabling successful application to LLMs**Brown et al., "Ithaka: A Generalized Multitask Learning Approach with Adversarial Training and Uncertainty Estimates"**, diffusion models**Ho et al., "DenoisDiffusion Probabilistic Models for Fast Image and Video Compression"**, and medical imaging**Baur et al., "Unsupervised Representation Learning via Auto-Encoding Variational Bayes"**. 
Recent work on SAE-based interpretation of CLIP embeddings **Teye, H., "Deep Neural Network Interpretation using Sparse Autoencoders"** shows promise in extracting interpretable features. 

\textbf{Concept-based explainability.}
Concept-based explanations provide interpretability by identifying human-coherent concepts within neural networks' latent spaces. 
While early approaches relied on manually curated concept datasets**Bengio et al., "Deep Learning"**, recent work has explored automated concept extraction **Mallinson et al., "Residual Transformers for High-Performance Natural Language Processing"** and explicit concept learning **Koch et al., "Attention is Not All You Need: A Two-Pass Neural Architecture for Efficient Sequence Modeling"**, with successful applications in out-of-distribution detection **Chen et al., "Neural Tangent Kernel as a Bridge Between Linearized Models and Their Full-Batch Counterparts"**, image generation **Hendricks et al., "Deep Learning-based Visual Abstraction and Reasoning"**, and medicine **Baur et al., "Unsupervised Representation Learning via Auto-Encoding Variational Bayes"**. 
However, existing methods often struggle to scale to modern transformer architectures with hundreds of millions of parameters. 
Our approach addresses this limitation by first training SAE without supervision on concept learning, then efficiently mapping unit-norm decoder columns to defined vocabulary concepts using cosine similarity with CLIP embeddings.