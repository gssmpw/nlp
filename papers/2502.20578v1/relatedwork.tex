\section{Related Work}
\textbf{Interpreting CLIP models.}
CLIP interpretability research follows two main directions: a direct interpretation of CLIP's behavior and using CLIP to explain other models. 
Direct interpretation studies focus on understanding CLIP's components through feature attributions~\citep{joukovsky2023model,sammani2024visualizing,zhao2024gradientbased}, residual transformations~\citep{balasubramanian2024decomposing}, attention heads \citep{gandelsman2024interpreting}, and individual neurons~\citep{goh2021multimodal,li2022exploring}. 
\citet{li2022exploring} discovered CLIP's tendency to focus on image backgrounds through saliency analysis, while \citet{goh2021multimodal} identified CLIP's multimodal neurons responding consistently to concepts across modalities. 
For model explanation, CLIP is used to analyze challenging examples~\citep{jain2022distilling}, robustness to distribution shifts~\citep{crabbeinterpreting}, and label individual neurons~\citep{oikarinen2023clip}.
In this work, we explore both directions in Section~\ref{sec:application} via the detection of semantic concepts learned by CLIP using MSAE (Section~\ref{sec:concept-naming}) and the analysis of biases in downstream models built on MSAE-explained CLIP embeddings (Section~\ref{sec:main_bias}).


\textbf{Mechanistic interpretability.}
Mechanistic interpretability seeks to reverse engineer neural networks analogously to decompiling computer programs~\citep{conmy2023towards,bereska2024mechanistic}. 
While early approaches focus on generating natural language descriptions of individual neurons \citep{hernandez2021natural,bills2023language}, the polysemantic nature of neural representations makes this challenging. 
A breakthrough comes with sparse autoencoders~(SAEs)~\citep{bricken2023monosemanticity,cunningham2024sparse}, which demonstrate the ability to recover monosemantic features. 
Recent architectural advancements like Gated~\citep{rajamanoharan2024improving} and TopK SAE variants~\citep{gao2024scaling} improve the sparsity--reconstruction trade-off, enabling successful application to LLMs~\citep{adly2024scaling}, diffusion models~\citep{surkov2024unpacking}, and medical imaging~\citep{abdulaal2024x}. 
Recent work on SAE-based interpretation of CLIP embeddings \citep{rao2024discover} shows promise in extracting interpretable features. 

\textbf{Concept-based explainability.}
Concept-based explanations provide interpretability by identifying human-coherent concepts within neural networks' latent spaces. 
While early approaches relied on manually curated concept datasets~\citep{kim2018interpretability,zhou2018interpretable}, recent work has explored automated concept extraction \citep{ghorbani2019towards} and explicit concept learning \citep{liu2020part,koh2020concept,espinosa2022concept}, with successful applications in out-of-distribution detection \citep{madeira2023zebra}, image generation \citep{misino2022vael}, and medicine \citep{lucieri2020interpretability}. 
However, existing methods often struggle to scale to modern transformer architectures with hundreds of millions of parameters. 
Our approach addresses this limitation by first training SAE without supervision on concept learning, then efficiently mapping unit-norm decoder columns to defined vocabulary concepts using cosine similarity with CLIP embeddings.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%