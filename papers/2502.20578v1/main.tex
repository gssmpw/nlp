\documentclass{article}

\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs}
\usepackage[hyphens]{url}
\usepackage{hyperref}
\usepackage{multirow}
\usepackage{xspace}
\usepackage{titletoc}
\newcommand{\mdoll}{\raisebox{-2pt}{\includegraphics[height=11pt]{plots/doll.pdf}}}
\newcommand{\mdollbig}{\raisebox{-2pt}{\includegraphics[height=13pt]{plots/doll.pdf}}}
\newcommand{\theHalgorithm}{\arabic{algorithm}}

\usepackage[preprint]{icml2025}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{siunitx} 

\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

\icmltitlerunning{Matryoshka SAE}

\begin{document}

\twocolumn[
\icmltitle{Interpreting CLIP with Hierarchical Sparse Autoencoders}

\begin{icmlauthorlist}
\icmlauthor{Vladimir Zaigrajew}{pw}
\icmlauthor{Hubert Baniecki}{pw,uw}
\icmlauthor{Przemyslaw Biecek}{pw,uw}
\end{icmlauthorlist}

\icmlaffiliation{pw}{Warsaw University of Technology, Warsaw, Poland}
\icmlaffiliation{uw}{University of Warsaw, Warsaw, Poland}

\icmlcorrespondingauthor{Vladimir Zaigrajew}{vladimir.zaigrajew.dokt@pw.edu.pl}

\icmlkeywords{sparse autoencoders, Matryoshka representation learning, interpretability, explainable AI, CLIP, SAE, MRL}

\vskip 0.3in
]

\printAffiliationsAndNotice{}
 
\begin{abstract}
Sparse autoencoders (SAEs) are useful for detecting and steering interpretable features in neural networks, with particular potential for understanding complex multimodal representations. 
Given their ability to uncover interpretable features, SAEs are particularly valuable for analyzing large-scale vision-language models (e.g., CLIP and SigLIP), which are fundamental building blocks in modern systems yet remain challenging to interpret and control. 
However, current SAE methods are limited by optimizing both reconstruction quality and sparsity simultaneously, as they rely on either activation suppression or rigid sparsity constraints. 
To this end, we introduce Matryoshka SAE (MSAE), a new architecture that learns hierarchical representations at multiple granularities simultaneously, enabling a direct optimization of both metrics without compromise. 
MSAE establishes a new state-of-the-art Pareto frontier between reconstruction quality and sparsity for CLIP, achieving 0.99 cosine similarity and less than 0.1 fraction of variance unexplained while maintaining ~80\% sparsity. 
Finally, we demonstrate the utility of MSAE as a tool for interpreting and controlling CLIP by extracting over 120 semantic concepts from its representation to perform concept-based similarity search and bias analysis in downstream tasks like CelebA.
\end{abstract}

\begin{figure}[ht]
\centering
\includegraphics[width=\columnwidth]{plots/SAE_Figure.png}
\caption{
\mdoll\xspace \textbf{Matryoshka Sparse Autoencoder (MSAE)} enables learning hierarchical concept representations from coarse to fine-grained features while avoiding rigid sparsity constraints in TopK and the activation shrinkage problem in ReLU SAE. 
(B)~At training, MSAE uses multiple top-$k$ values up to dimension $d$ instead of a single $k$ like in TopK SAE, combining losses across different granularities. 
(C)~At inference our method uses the whole $d$-dimensional representation. 
(D)~MSAE allows for more precise editing and manipulation in the concept space.}
\label{fig:MatryoshkaSAE}
\end{figure}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
Vision-language models, particularly contrastive language-image pre-training~\citep[CLIP,][]{radford2021learning,cherti2023reproducible}, revolutionize multimodal understanding by learning robust representations that bridge visual and textual information. 
Through contrastive learning on massive datasets, CLIP and its less adopted successor SigLIP~\citep{zhai2023sigmoid} demonstrate remarkable capabilities that extend far beyond their primary objective of cross-modal similarity search.
Its representation is a foundational component in text-to-image generation models like Stable Diffusion~\citep{podell2024sdxl} and serves as a powerful feature extractor for numerous downstream vision and language tasks \citep{shen2022much}, establishing CLIP as a crucial building block in modern VLMs~\citep{liu2023llava,wang2023cogvlm}.

Despite CLIP's widespread adoption, understanding how it processes and represents information remains a challenge. 
The distributed nature of its learned representations and the complexity of the optimized loss function make it particularly difficult to interpret. Traditional explainability approaches have limited success in addressing this challenge: gradient-based feature attributions~\citep{simonyan2013deep,shrikumar2017not,selvaraju2017grad,sundararajan2017axiomatic,abnar2020quantifying} struggle to provide human-interpretable explanations, perturbation-based approaches~\citep{zeiler2014visualizing,ribeiro2016should,lundberg2017unified,adebayo2018sanity} yield inconsistent results, and concept-based methods~\citep{ramaswamy2023overlooked,oikarinen2023labelfree} are constrained by their reliance on manually curated concept datasets. 
This interpretability gap hinders our ability to identify and mitigate potential biases or failure modes of CLIP in downstream applications.
Recent advances in mechanical interpretability~\citep{conmy2023towards,bereska2024mechanistic} use sparse autoencoders (SAEs) as a tool for disentangling interpretable features in neural networks \citep{cunningham2024sparse}. 
When applied to CLIP's representation space, SAEs offer the potential to decompose complex, distributed representations into human-interpretable components through self-supervised learning.
It eliminates the need for concept datasets and limits predefined concept sets in favor of natural concept emergence.

However, training effective SAEs poses unique challenges.
The richness of data distribution and high dimensionality of CLIP's multimodal embedding space requires tuning the sparsity-reconstruction trade-off~\citep{bricken2023monosemanticity,gao2024scaling}.
Furthermore, evaluating SAE effectiveness extends beyond traditional metrics, requiring discovering interpretable features that maintain their semantic meaning across both visual and textual modalities.
Current approaches for enforcing sparsity in autoencoders use either $L_1$~\citep{bricken2023monosemanticity} or TopK~\citep{gao2024scaling} proxy functions, each with significant drawbacks. 
$L_1$ regularization results in activation shrinkage, systematically underestimating feature activations and potentially missing subtle but important concepts. 
TopK enforces a fixed number of active neurons, imposing rigid constraints that may not align with the natural concept density in different regions of CLIP's embedding space~\citep{gao2024scaling,bussmann2024batchtopk}.

To this end, we propose a hierarchical approach to sparse autoencoders, a new architecture inspired by Matryoshka representation learning~\citep[\mdoll,][]{kusupati2022matryoshka}, as illustrated in Figure \ref{fig:MatryoshkaSAE}. 
While Matryoshka SAE (MSAE) can be applied to interpret any neural network representation, we demonstrate its utility in the CLIP's complex multimodal embedding space. 
At its core, MSAE applies TopK operations $h$-times with progressively increasing numbers of $k$ neurons, learning representations at $h$ granularities simultaneously -- from coarse concepts to fine-grained features. 
By combining reconstruction losses across all granularity levels, MSAE achieves a more flexible and adaptive sparsity pattern. 
We effectively remove the rigid constraints of simple TopK while avoiding the activation shrinkage problems associated with $L_1$ regularization, resulting in the state-of-the-art Pareto frontier between the reconstruction quality and sparsity.

\paragraph{Contributions.}
We introduce a hierarchical SAE architecture that establishes a new leading Pareto frontier between reconstruction quality ($0.99$ cosine similarity and $<0.1$~FVU) and sparsity ($\sim80\%$), while maintaining computational efficiency comparable to standard SAEs at inference time.
We develop a robust methodology for validating discovered concepts in CLIP's multimodal embedding space, successfully identifying and verifying over 120 interpretable concepts across both image and text domains. 
Through extensive empirical evaluation on CC3M and ImageNet
datasets, we demonstrate progressive recovery capabilities and the effectiveness of hierarchical sparsity thresholds compared to existing approaches. 
We showcase the practical utility of MSAE in two key applications: concept-based similarity search with controllable concept strength and systematic analysis of gender biases in downstream classification models through SAE activations and concept-level interventions on the CelebA dataset.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Related Work}


\textbf{Interpreting CLIP models.}
CLIP interpretability research follows two main directions: a direct interpretation of CLIP's behavior and using CLIP to explain other models. 
Direct interpretation studies focus on understanding CLIP's components through feature attributions~\citep{joukovsky2023model,sammani2024visualizing,zhao2024gradientbased}, residual transformations~\citep{balasubramanian2024decomposing}, attention heads \citep{gandelsman2024interpreting}, and individual neurons~\citep{goh2021multimodal,li2022exploring}. 
\citet{li2022exploring} discovered CLIP's tendency to focus on image backgrounds through saliency analysis, while \citet{goh2021multimodal} identified CLIP's multimodal neurons responding consistently to concepts across modalities. 
For model explanation, CLIP is used to analyze challenging examples~\citep{jain2022distilling}, robustness to distribution shifts~\citep{crabbeinterpreting}, and label individual neurons~\citep{oikarinen2023clip}.
In this work, we explore both directions in Section~\ref{sec:application} via the detection of semantic concepts learned by CLIP using MSAE (Section~\ref{sec:concept-naming}) and the analysis of biases in downstream models built on MSAE-explained CLIP embeddings (Section~\ref{sec:main_bias}).


\textbf{Mechanistic interpretability.}
Mechanistic interpretability seeks to reverse engineer neural networks analogously to decompiling computer programs~\citep{conmy2023towards,bereska2024mechanistic}. 
While early approaches focus on generating natural language descriptions of individual neurons \citep{hernandez2021natural,bills2023language}, the polysemantic nature of neural representations makes this challenging. 
A breakthrough comes with sparse autoencoders~(SAEs)~\citep{bricken2023monosemanticity,cunningham2024sparse}, which demonstrate the ability to recover monosemantic features. 
Recent architectural advancements like Gated~\citep{rajamanoharan2024improving} and TopK SAE variants~\citep{gao2024scaling} improve the sparsity--reconstruction trade-off, enabling successful application to LLMs~\citep{adly2024scaling}, diffusion models~\citep{surkov2024unpacking}, and medical imaging~\citep{abdulaal2024x}. 
Recent work on SAE-based interpretation of CLIP embeddings \citep{rao2024discover} shows promise in extracting interpretable features. 

\textbf{Concept-based explainability.}
Concept-based explanations provide interpretability by identifying human-coherent concepts within neural networks' latent spaces. 
While early approaches relied on manually curated concept datasets~\citep{kim2018interpretability,zhou2018interpretable}, recent work has explored automated concept extraction \citep{ghorbani2019towards} and explicit concept learning \citep{liu2020part,koh2020concept,espinosa2022concept}, with successful applications in out-of-distribution detection \citep{madeira2023zebra}, image generation \citep{misino2022vael}, and medicine \citep{lucieri2020interpretability}. 
However, existing methods often struggle to scale to modern transformer architectures with hundreds of millions of parameters. 
Our approach addresses this limitation by first training SAE without supervision on concept learning, then efficiently mapping unit-norm decoder columns to defined vocabulary concepts using cosine similarity with CLIP embeddings.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{\mdollbig\xspace Matryoshka Sparse Autoencoder} \label{sec:matryoshka}
\subsection{Preliminaries}
Sparse autoencoders (SAEs) decompose model activations $x \in \mathbb{R}^n$ into sparse linear combinations of learned directions, aiming for interpretability and monosemanticity. 
The standard SAE architecture consists of:
\begin{equation}
\begin{gathered}
z = \mathrm{ReLU}\big(W_{\mathrm{enc}}\left(x - b_{\mathrm{pre}}\right) + b_{\mathrm{enc}}\big), \\
\hat{x} = W_{\mathrm{dec}} z + b_{\mathrm{pre}},
\end{gathered}
\end{equation}
where encoder matrix $W_{\mathrm{enc}} \in \mathbb{R}^{n\times d}$, encoder bias $b_{\mathrm{enc}} \in \mathbb{R}^d$, decoder matrix $W_{\mathrm{dec}} \in \mathbb{R}^{d\times n}$, and preprocessing bias $b_{\mathrm{pre}} \in \mathbb{R}^n$ are the learnable parameters, with $d$ being the dimension of the latent space. 
The basic reconstruction objective is $\mathcal{L}(x) := \|x - \hat{x}\|_2^2$.

Existing approaches established two primary sparsity mechanisms. ReLU SAE \citep{bricken2023monosemanticity} uses $L_1$ regularization with the objective $\mathcal{L}(x) := \|x - \hat{x}\|_2^2  + \lambda \|z\|_1$, while TopK SAE \citep{gao2024scaling} enforces fixed sparsity through $z = \mathrm{ReLU}\left(\mathrm{TopK}\left(W_{\mathrm{enc}}\left(x - b_{\mathrm{pre}}\right) + b_{\mathrm{enc}}\right)\right)$. 
However, each approach faces distinct limitations: $L_1$ regularization causes activation shrinkage \citep{rajamanoharan2024improving}, while TopK imposes rigid sparsity constraints \citep{bussmann2024batchtopk}.

\subsection{Matryoshka SAE Architecture}
Following Matryoshka representation learning \citep[\mdoll,][]{kusupati2022matryoshka}, we propose a SAE architecture that learns representations at multiple granularities simultaneously. 
Instead of enforcing a single sparsity threshold $k$ or using $L_1$ regularization, our approach applies multiple TopK operations with increasing $k$ values, optimizing across all granularity levels. 
We set $k$ values as powers of 2, i.e. $k_i = 2^i$ up to dimension $d$, which provides effective coverage of the representation space while maintaining reasonable computational costs.
For a given input $x$, MSAE computes $h$ latent representations during training using a sequence of increasing k values $\{k_1, k_2, \ldots, k_h\}$ with $k_1 < k_2 < \ldots < k_h \leq d$:
\begin{equation}
\begin{gathered}
z_i = \mathrm{ReLU}(\mathrm{TopK}_i(W_{\mathrm{enc}}(x - b_{\mathrm{pre}}) + b_{\mathrm{enc}})), \\
\hat{x}_i = W_{\mathrm{dec}} z_i + b_{\mathrm{pre}}, \\ 
\mathcal{L}(x) := \sum_{i=1}^{h} \alpha_i\| x - \hat{x}_i \|_2^2,
\end{gathered}
\end{equation}
where $\alpha_i$ are weighting coefficients for each granularity level. 
At inference time, we can either apply TopK with any desired granularity or discard it entirely, leaving only ReLU, which allows the model to utilize all neurons it deems essential for reconstruction.


\textbf{Hierarchical learning.} 
The key insight of our approach is that different samples require different levels of sparsity (numbers of concepts) for an optimal representation. 
By simultaneously optimizing across multiple $k$ values, MSAE learns a natural hierarchy of features. 
Our TopK operations maintain a nested structure where features selected at each level form a subset of those selected at higher $k$ values, i.e. $\mathrm{TopK}_1 \subseteq \mathrm{TopK}_2 \subseteq \ldots \subseteq \mathrm{TopK}_h$. 
Such a hierarchical structure ensures coherence between granularity levels, where low $k$ values capture coarse, high-level concepts while higher $k$ values progressively enable fine-grained feature representation.

\textbf{Sparsity coefficient weighting.} 
We propose and evaluate two strategies for setting the weighting coefficients $\alpha_i$. 
The \emph{uniform weighting}~(UW) approach sets $\alpha_i = 1$ for all $i$, while the \emph{reverse weighting}~(RW) strategy uses $\alpha_i = h-i+1$, giving higher weights to lower $k$ values. 
RW places greater emphasis on the sparsity of the representation as compared to UW.
RW still maintains the quality of the reconstruction, as evidenced by our empirical findings of improved sparsity without significant performance degradation (Table \ref{tab:metrics}).



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Evaluating MSAE}\label{sec:eval_msae}

In this section, we conduct extensive experiments to evaluate MSAE against ReLU and TopK SAEs. We compare the sparsity--fidelity trade-off (Section~\ref{sec:pareto}), at multiple granularity levels ~(Section~\ref{sec:granularities}).
We follow with evaluating the semantic quality of learned representations beyond traditional distance metrics (Section~\ref{sec:metrics}), analyzing decoder orthogonality (Section~\ref{sec:ortho}), and examining the statistical properties of SAE activation magnitudes (Section~\ref{sec:evalcapping}). 
To verify that MSAE successfully learns hierarchical features, we conduct experiments on the progressive recovery task~(Section~\ref{sec:progressive}). 
We conclude with an ablation study comparing the influence of different training modalities in Section~\ref{sec:textvsimage}.

\textbf{Setup.} 
All SAE models are trained on the CC3M \citep{sharma2018conceptual} training set with features (post-pooled) from the CLIP ViT-L/14 or ViT-B/16 model. 
Image modality is evaluated on ImageNet-1k training set \citep{russakovsky2015imagenet}, while text modality is evaluated on the CC3M validation set. 
Each SAE is trained with expansion rates of $8\times$, $16\times$ and $32\times$, effectively scaling the latent layer from 768 to \{6144, 12288, 24576\} neurons for ViT-L/14, and from 512 to \{4096, 8192, 16384\} neurons for ViT-B/16. 
We provide further details on the implementation and hyperparameter settings in Appendix~\ref{sec:implementation}. 


\begin{table*}[t!]
\caption{
\textbf{Quantitative comparison of SAE models on ImageNet-1k.} 
We compare the following SAEs with expansion rate 8: ReLU with varying sparsity regularization~($\lambda$), TopK with 64 or 256 active neurons, and Matryoshka using uniform~(UW) or reverse weighting~(RW) $\alpha$ coefficients.
Arrows ($\uparrow$/$\downarrow$) indicate the preferred direction of metrics. 
NDN values in parentheses show the dead neuron count on the training set. 
LP (KL) values are scaled by $10^6$ for readability.
Extended results for higher expansion rates and the text modality are reported in Appendix~\ref{sec:appendix_architectures}.}
\label{tab:metrics}
\vspace*{0.1in}
\centering
\begin{small}
\begin{tabular}{lccccccll}
\toprule
\textbf{Model} & \textbf{$L_0$ $\uparrow$} & \textbf{FVU $\downarrow$} & \textbf{CS $\uparrow$} & \textbf{LP (KL) $\downarrow$} & \textbf{LP (Acc) $\uparrow$} & \textbf{CKNNA $\uparrow$} & \textbf{DO $\downarrow$} & \textbf{NDN $\downarrow$} \\ 
\midrule
ReLU ($\lambda = 0.003$) & $.649_{\pm.007}$ & $.004_{\pm.000}$ & $.998_{\pm.000}$ & $0.66_{\pm1.03}$ & $.994_{\pm.083}$ & $.781_{\pm.004}$ & $.003$ & $0(0)$ \\
ReLU ($\lambda = 0.001$) & $.553_{\pm.006}$ & $.002_{\pm.001}$ & $.999_{\pm.000}$ & $0.36_{\pm0.65}$ & $.995_{\pm.073}$ & $.822_{\pm.004}$ & $.002$ & $0(0)$ \\
TopK ($k = 64$) & $.950_{\pm.009}$ & $.172_{\pm.026}$ & $.912_{\pm.013}$ & $60.1_{\pm90.8}$ & $.930_{\pm.255}$ & $.762_{\pm.004}$ & $.002$ & $0(335)$ \\
TopK ($k = 256$) & $.900_{\pm.004}$ & $.011_{\pm.003}$ & $.994_{\pm.002}$ & $2.71_{\pm5.40}$ & $.987_{\pm.114}$ & $.874_{\pm.003}$ & $.003$ & $0(296)$ \\
\midrule
Matryoshka (RW) & $.829_{\pm.008}$ & $.007_{\pm.003}$ & $.997_{\pm.002}$ & $3.13_{\pm7.08}$ & $.987_{\pm.115}$ & $.809_{\pm.002}$ & $.002$ & $2(4)$ \\
Matryoshka (UW) & $.748_{\pm.006}$ & $.002_{\pm.001}$ & $.999_{\pm.000}$ & $0.35_{\pm0.82}$ & $.995_{\pm.070}$ & $.848_{\pm.003}$ & $.002$ & $0(22)$ \\ 
\bottomrule
\end{tabular}
\end{small}
\end{table*}

\subsection{Evaluation Metrics} \label{sec:metrics-def}
Here, we briefly define each metric used to evaluate SAE.

\textbf{$L_0$} denotes the mean proportion of zero elements in SAE activations.
\textbf{Fraction of variance unexplained (FVU)}, also known as Normalized MSE \citep{gao2024scaling}, measures reconstruction fidelity by normalizing the mean squared reconstruction error $\mathcal{L}(x)$ by the mean squared value of the (mean-centered) input.
\textbf{Explained variance ratio (EVR)} is FVU's complement metric, defined as $1-\text{FVU}$.
\textbf{Linear probing~(LP)} assesses how well SAE preserves semantic information in the reconstructed embeddings on the downstream task. To evaluate this, we train a linear probe model on ImageNet-1k using CLIP embeddings as a backbone, with AdamW optimizer ($\mathrm{lr}=1e{-}3$), ReduceLROnPlateau scheduler, and batch\_size=256. We measure performance by comparing predictions from original versus reconstructed embeddings using two metrics: Kullback-Leibler divergence (KL) between predicted class distributions and classification accuracy (Acc), where accuracy uses $\mathrm{argmax}$ predictions from original embeddings as targets.
\textbf{Centered kernel nearest neighbor alignment (CKNNA)} \citep{huh2024platonic} measures kernel alignment based on mutual nearest neighbors, providing a quantitative assessment of alignment between SAE activations and input embeddings. A detailed explanation is provided in Appendix~\ref{sec:cknna}.
\textbf{Decoder orthogonality~(DO)} calculates the mean cosine similarity of the lower triangular portion of the SAE decoder, where 0 indicates perfect orthogonality. This metric assesses how orthogonal the monosemantic feature directions are in the decoder.
\textbf{Number of dead neurons (NDN)} is a metric that measures how many neurons remain consistently inactive (zero in the SAE activations layer) across all inputs during training or evaluation, indicating the network's inability to fully utilize its capacity for learning semantic features.

\begin{figure}[h]
\centering
\includegraphics[width=0.99\columnwidth]{plots/pareto-evr-main.pdf}
\caption{\textbf{Comparison of sparsity--fidelity trade-offs across SAE architectures on ImageNet-1k.} Each model presents results from all 3 expansion rates, comparing ReLU SAE ($\lambda=\{0.001, 0.003, 0.03\}$), TopK SAE ($k=\{32, 64, 128, 256\}$), and MSAE (UW, RW). The optimal SAE would occupy the upper right corner, achieving both high sparsity and reconstruction fidelity. For extended results across both modalities, refer to Figure \ref{fig:appendix_pareto}.
}
\label{fig:pareto-mae}
\end{figure}

%%%
\subsection{Sparsity--Fidelity Trade-off}\label{sec:pareto}

We assess SAE performance using sparsity--fidelity trade-off, measuring sparsity with $L_0$ and reconstruction quality with EVR, following previous work. Figure \ref{fig:pareto-mae} reveals that ReLU SAE shows difficulty balancing performance, achieving either high fidelity with low $L_0$ or the opposite, with expansion rate primarily improving sparsity. TopK SAE with higher k values achieves better but not ReLU-level fidelity while offering improved sparsity yet consistently suffering from at least 5\% of dead neurons (Table \ref{tab:metrics}). Both variants of MSAE achieve better sparsity than ReLU and better fidelity than TopK, establishing a superior Pareto frontier while maintaining less than 1\% of dead neurons. The RW variant further improves sparsity as expected, with only minor fidelity degradation. Notably, only Matryoshka consistently improves on both metrics with higher expansion rates, while TopK struggles with increased expansion rate, and ReLU shows improvements only in the highest $\lambda$.

As an ablation, we evaluate Cosine Similarity as an alternative reconstruction metric, motivated by observations that SAEs primarily struggle with embedding magnitude reconstruction and CLIP embeddings are commonly $L_2$-normalized. Results in Appendix~\ref{sec:appendix_pareto} show consistent findings, with MSAE showing even clearer advantages through stable, low-variance performance across both modalities.

\subsection{Ablation: Matryoshka at Lower Granularity Levels} \label{sec:granularities}

\begin{figure}[t]
\centering
\includegraphics[width=0.99\columnwidth]{plots/matryoshkavstopk.pdf}
\caption{
\textbf{Low Granularity Level Matroshka vs. TopK SAE on ImageNet-1k.} 
We report FVU (left) and CKNNA (right) metrics for two TopK variants ($k = 128,256$), and Matryoshka trained on these granularities in RW and UW variants at expansion rates 8 and 16. 
Even at this small granularity, MSAE improves the Pareto frontier relative to both TopK variants, pushing it as the expansion rate grows from 8 to 16. 
For extended results across other metrics, refer to Figure~\ref{fig:appendix_matryoshkavstopk}.
}
\label{fig:matryoshkavstopk}
\end{figure}

We train both MSAE variants (RW and UW) on two granularities $[128,256]$ and compare them against TopK with $k = 128$ and $k = 256$ to analyze MSAE behavior at lower granularity levels. Figure \ref{fig:matryoshkavstopk} shows that Matryoshka achieves similar sparsity to at least the lower TopK variant while maintaining CKNNA and EVR performance comparable to the best TopK variant, and even better with MSAE RW.
This demonstrates that even at small granularity, MSAE maintains or improves the Pareto frontier over TopK across various metrics, with RW achieving better trade-offs. 
As observed also in Section \ref{sec:pareto}, MSAE's performance advantages over TopK increase at higher expansion rates.

%%%
\subsection{Semantic Preservation Analysis} \label{sec:metrics}
In Section~\ref{sec:pareto}, we only evaluated SAEs using $L_0$ for activation sparsity and EVR for reconstruction fidelity, however these metrics have limitations. $L_0$ only counts active neurons without assessing how well SAE representations align with original embeddings, and EVR focuses solely on distance reconstruction rather than semantic preservation. To address these limitations, we introduce additional metrics. Following \citep{yu2025repa}, we adopt the CKNNA metric to assess how well SAE activations preserve the neighborhood structure of CLIP embeddings. We also evaluate semantic preservation through linear probing metrics; following \citep{gao2024scaling,lieberum2024gemma}, we use LP (KL) to measure prediction distribution alignment and LP (Acc) to compare classification accuracy. All metrics are defined in Section~\ref{sec:metrics-def} and presented in Table~\ref{tab:metrics}. 
Our analysis reveals that while cosine similarity and FVU correlate well with linear probing metrics, the alignment metric demonstrates Matryoshka's strength in preserving semantic structures.

%%%
\subsection{Orthogonality of SAE Features} \label{sec:ortho}

SAEs can disentangle polysemantic representations into monosemantic features, as shown and explained by \citep{bricken2023monosemanticity}. 
To evaluate feature monosemanticity, we measure decoder orthogonality using the DO metric, with results reported in Table~\ref{tab:metrics}. 
While all methods achieve high orthogonality as indicated by low DO values, none reach perfect orthogonality. This might stem from multiple factors, including feature absorption as noted in~\citep{chanin2024absorption}, or just learning similar concepts (such as different numbers). We argue that understanding these sources of non-orthogonality is crucial for advancing the development of more effective monosemantic feature learning in SAEs.

\begin{figure}[t]
\centering
\includegraphics[width=0.79\columnwidth]{plots/statistic-main.pdf}
\caption{\textbf{Distribution of non-zero SAE activations on ImageNet-1k validation set.} Frequency histograms for ReLU~($\lambda = 0.003$), TopK~($k = 32$), and Matryoshka (RW) models at expansion rate 8. 
Matryoshka models exhibit a double-curvature distribution similar to ReLU models but without activation shrinkage, while TopK shows this pattern only at higher $k$ values, as can be seen in an extended Figure~\ref{fig:statistic_8}.
Extended results for higher expansion rates are reported in Figure~\ref{fig:statistic_more}.}
\label{fig:statistic-main}
\end{figure}

%%%
\subsection{Activations Magnitudes Analysis}\label{sec:evalcapping}

To analyze the impact of sparsity proxies on SAE, we examine non-zero activation distributions across ViT-L with expansion rate 8 in Figure \ref{fig:statistic-main}. 
Matryoshka models display a distinctive double-curvature distribution similar to ReLU-based models, with values between $5$ to $10$ appearing almost linear in $\log_{10}$ space. 
Following \citep{adly2024scaling}, we attribute low activations to reconstruction purposes rather than semantic meaning. 
The second curvature reflects natural images' complexity, which requires multiple concept reconstructions rather than single dominant features, as evidenced by the small number of very high values corresponding to rare, nearly singular concept images (Figure~\ref{fig:max_active}). 
As the sparsity parameter $k$ in TopK methods increases (Figure~\ref{fig:statistic_8}), the transition from one to double-curvature behavior suggests that stronger sparsity constraints create composite features, supported by Appendix~\ref{sec:highest} showing that high-activation features~($>15$) in TopK methods have a lower ratio of valid named features compared to Matryoshka.

\begin{table*}[t]
\caption{\textbf{Training modality influence on MSAE performance.} 
We train MSAE on the text version of the CC3M train set and compare it to models trained on its original image version, evaluating across both domains using the CC3M validation text set and ImageNet-1k. 
While models perform best on their training modality, text-trained variants show better cross-domain generalization. 
\textbf{Bold} values indicate the best performance per metric, with NDN showing dead neuron count from the final checkpoint.
}
\label{tab:matryoshka-comparison}
\vspace*{0.1in}
\centering
\begin{small}
\begin{tabular}{l|cccc|cccc|c}
\toprule
\multirow{2}{*}{\begin{tabular}[c]{@{}l@{}}\textbf{Matryoshka}\\ \textbf{SAE variant}\end{tabular}} & \multicolumn{4}{c|}{\textbf{Language metrics on CC3M}} & \multicolumn{4}{c|}{\textbf{Vision metrics on ImageNet-1k}} & \multirow{2}{*}{NDN $\downarrow$} \\
& $L_0$ $\uparrow$ & FVU $\downarrow$ & CS $\uparrow$ & CKNNA $\uparrow$ & $L_0$ $\uparrow$ & FVU $\downarrow$ & CS $\uparrow$ & CKNNA $\uparrow$ & \\
\midrule
Image (RW) &  $.824_{\pm.029}$ & $.060_{\pm.052}$ & $.971_{\pm.026}$ & $.775_{\pm.001}$  & $.829_{\pm.008}$ & $.007_{\pm.003}$ & $.997_{\pm.002}$ & $.809_{\pm.002}$ & $4$ \\
Image (UW) & $.755_{\pm.024}$ & $.026_{\pm.027}$ & $.988_{\pm.012}$ & $\textbf{.790}_{\pm.002}$ & $.748_{\pm.006}$ & $\textbf{.002}_{\pm.001}$ & $\textbf{.999}_{\pm.000}$ & $.848_{\pm.003}$ & $22$ \\
Text (RW) & $\textbf{.841}_{\pm.014}$ & $.008_{\pm.003}$ & $.996_{\pm.002}$ & $.782_{\pm.008}$ & $\textbf{.841}_{\pm.014}$ & $.008_{\pm.003}$ & $.996_{\pm.002}$ & $.782_{\pm.008}$ & $0$ \\
Text (UW) & $.791_{\pm.010}$ & $\textbf{.001}_{\pm.001}$ & $\textbf{.999}_{\pm.000}$ & $.784_{\pm.007}$ & $.799_{\pm.012}$ & $.015_{\pm.013}$ & $.993_{\pm.006}$ & $\textbf{.877}_{\pm.003}$ & $0$ \\
\bottomrule
\end{tabular}
\end{small}
\end{table*}

%%%
\subsection{Progressive Recovery} \label{sec:progressive}

To verify that our method learns hierarchical structure, we perform a progressive reconstruction task by using an increasing number of SAE activations, ordered by magnitude, to recover the original vector. 
Figure~\ref{fig:progressive_main} shows that reconstruction quality improves with decreasing sparsity thresholds (increasing $k$) during inference. 
TopK variants exhibit performance plateaus shortly after their training thresholds ($k=\{32, 64\}$), while ReLU-based models show continued improvement but with inferior performance at higher sparsity. 
MSAE demonstrates a better hierarchical structure that combines TopK's efficient high-sparsity performance with ReLU's scaling capabilities. While our method performs slightly below TopK ($k = 32$) at the highest sparsity, it quickly surpasses TopK's plateau at lower sparsity, achieving performance levels above ReLU models. 
We observe similar patterns in the CKNNA alignment metric, with MSAE outperforming both TopK ($k = 32$) and ReLU models beyond $k=10$ while performing only slightly below TopK ($k = 256$) at the lowest sparsity.
Evidence of improved hierarchical feature learning across metrics and modalities is presented in Appendix~\ref{sec:appendix_progressive}.

\begin{figure}[t]
\centering
\includegraphics[width=0.99\columnwidth]{plots/progressive_main.pdf}
\caption{\textbf{Progressive recovery performance on ImageNet-1k.} 
We report FVU (left) and CKNNA (right) metrics for different SAE architectures with expansion rate 8 as functions of increasing top $k$ values by magnitudes of SAE activations during inference. SAE trained with
TopK variants ($k=32, 64$) show performance plateaus beyond their training thresholds, while ReLU-based models ($\lambda=0.001, 0.003$) and Matryoshka variants (UW and RW) demonstrate continuous improvement. 
Extended results for higher expansion rates and across other metrics are reported in~Figures~\ref{fig:progressive_8}~\&~\ref{fig:progressive_more}.
}
\label{fig:progressive_main}
\end{figure}

%%%
\subsection{Training Modality: Language and Vision} \label{sec:textvsimage}

We evaluate how training modality affects MSAE performance by comparing models trained on text with the original image-trained models, validating both modality models across text and image domains in Table~\ref{tab:matryoshka-comparison}. 
While both variants perform best in their training domains, text-trained models achieve superior cross-modal performance, demonstrating stronger generalization capabilities. 
Moreover, text-trained models achieve higher sparsity on both modalities with no dead neurons, showing better utilization of learned features. 
These findings position text training as a preferred approach for multi-modal applications where balanced performance is desired. 
Future research could explore training SAEs on varying ratios of text and image data to optimize cross-modal performance or try to train \emph{crosscoders} \citep{jack2024sparse} on both modalities simultaneously.

We defer extended MSAE evaluations, also for ViT-B/16, to Appendix~\ref{sec:append_qaq}.


%%%
\section{Interpreting CLIP with MSAE} \label{sec:application}
In this section, we demonstrate how MSAE can enhance interpretability and control interpretable features in CLIP-based applications. 
We first establish neuron-concept mappings in the activation layer through an automated technique described in Section~\ref{sec:concept-naming}. 
Then, we show its effectiveness in concept-based similarity search across the ImageNet validation set, enabling retrieval of images with varying degrees of explicit concept presence. 
Moreover, we leverage MSAE to study potential conceptual biases in a gender classification model trained on the CelebA dataset~\citep{liu2015faceattributes}.


%%%
\subsection{Concept Naming} \label{sec:concept-naming}
While self-supervised training of SAE enables learning up to $d$ monosemantic concepts, mapping these concepts to specific neurons remains non-trivial. 
Previous work used LLMs for identifying neuron-encoded concepts \citep{bills2023language}, but we adopt the more efficient method for CLIP-trained SAE proposed in~\citep{rao2024discover}, which leverages CLIP's representation space. 
Our concept detection and validation methodology is detailed in Appendix~\ref{sec:concept_validation}, with comprehensive results on valid concept counts across SAE models presented in Table \ref{tab:concepts}. 
Figure \ref{fig:concept-main} demonstrates the concept 'smile' from MSAE RW (with 140 valid concepts detected) through its highest-activating text and image examples, confirming consistent concept presence across diverse inputs.
Supplementary analysis of highly activated concept examples in Appendix~\ref{sec:append_visconcept} showcase SAE's ability to learn a wide range of concepts, from simple textures and colors to more complex ones like \textit{light} (lights in darkness), countable concepts like \textit{trio} (groups of three), and even nationality-related concepts like \textit{ireland} or \textit{germany}.


\begin{figure}[t]
\centering
\includegraphics[width=0.99\columnwidth]{plots/neuron_interpretation_smile_main.pdf}
\caption{
\textbf{Visualization of the `smile' concept through top-activating examples.} 
Analysis of neuron no. 3552 in MSAE~(RW) identified through automated interpretability as the concept \textit{smile} with a similarity score of 0.59. 
Both text and image examples with the highest activation values strongly confirm the concept's presence. 
We show additional such examples of valid concepts with their top-activating examples in Figure \ref{fig:vis_concept}.
}
\label{fig:concept-main}
\end{figure}


%%%
Naming SAE features enables using SAE to conduct diverse interpretability analyses related to CLIP. 
We present two use cases where we apply the MSAE RW variant with an expansion rate of~8.

\begin{figure*}[th]
\centering
\includegraphics[width=0.82\linewidth]{plots/celeba_neuron_woman.pdf}
\caption{\textbf{Impact of concept manipulation on gender classification.} By increasing concept magnitudes (\textit{bearded}, \textit{glasses}, \textit{blonde}) in SAE space and mapping back to CLIP space, we observe changes in gender classification probabilities. Results reveal the model's learned gender associations through plateauing effects: \textit{bearded} and \textit{glasses} bias toward male classification, while \textit{blonde} bias toward female.}
\label{fig:bias_main}
\end{figure*}

\subsection{Similarity Search}
CLIP embeddings are widely used for cross-modal similarity search between images and text through cosine similarity metric, primarily for retrieval engines. We extend this capability using SAE in three ways.

First, SAE provides interpretable insights into nearest neighbor (NN) image retrievals. 
Figure \ref{fig:similar_search} shows the top~8 concepts for the two closest retrieved images, revealing shared semantic concept patterns and explaining why both NNs match the query image of an Irish police vehicle, with the first NN (Irish police vehicle) being closer than the second (British police vehicle).
Second, we compare similarity search in CLIP embedding space against SAE activation space using Manhattan distance (detailed and visualized in Appendix~\ref{sec:append_similarity}). 
While the first NN remains consistent across both spaces, the second NN in the SAE space shows the same vehicle type from a different angle, demonstrating that similarity searches can be done in both spaces while SAE enables additional concept-based interpretability.
Finally, we demonstrate a controlled similarity search by manipulating concept magnitudes. 
In Figure \ref{fig:similarity_manipulation}, increasing the \textit{germany} concept strength preserves the original image as the top match but shifts the second NN from an Irish to a German police vehicle, while preserving the overall input image structure. 
The increasing distances from the original image embedding show how larger magnitude adjustments affect embedding coherence.

\begin{figure}[t]
\centering
\includegraphics[width=0.8\columnwidth]{plots/neuron_manipulation.pdf}
\caption{
\textbf{Nearest neighbor analysis with enhanced \textit{germany} concept.} 
By increasing the magnitude of the \textit{germany} concept in SAE space (from 0.3 to 20, then 30) and mapping back to CLIP space, we observe shifts in nearest neighbors. 
While the input image remains the top match (with increasing distance), the second-nearest neighbor changes from a British police vehicle (shown in Figure \ref{fig:similar_search}) to a German one.
}
\label{fig:similarity_manipulation}
\end{figure}

\subsection{Bias Validation on a Downstream Task} \label{sec:main_bias}
CLIP models are commonly used as feature extractors for downstream tasks, enabling efficient fine-tuning with limited data. 
With MSAE, we can investigate whether downstream models learn to associate specific concepts with classes. 
To demonstrate this, we train a single-layer classifier on CLIP embeddings from the CelebA dataset to perform binary gender classification (1 for female, 0 for male), achieving an F1 score of approximately 0.99.
Through statistical analysis in Appendix~\ref{sec:biasvalidationtests}, we uncover several concept-gender associations: \textit{bearded} biases toward male classification, \textit{blonde} toward female, and \textit{glasses} showing modest male bias. 
To validate these findings, Figure \ref{fig:bias_main} demonstrates an example of how increasing these concepts' magnitudes affects classification scores for a female example (see Figure \ref{fig:celeba_neuron_man} for a male example). 
The results confirm our statistical analysis, and the plateaus in classification probabilities as concept magnitudes increase help quantify the strength of concept-gender associations in the model.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusion}

We propose Matryoshka SAE to advance our understanding of CLIP embeddings through hierarchical sparse autoencoders. 
MSAE improves upon both TopK and ReLU approaches, achieving superior sparsity--fidelity trade-off while providing flexible sparsity control via the $\alpha$ coefficient. 
Our experiments demonstrate MSAE's effectiveness through near-optimal metrics, progressive feature recovery, and extraction of over 120 validated concepts, enabling new applications in concept-based similarity search and bias detection in downstream tasks.

\textbf{Limitations and future work.} 
MSAE faces three limitations with clear paths for future improvement. 
The current implementation's use of multiple decoder passes with different TopK activations introduces computational overhead, which could be addressed through optimized CUDA kernels enabling parallel processing of multiple granularities. 
While we demonstrated MSAE's effectiveness using CLIP embeddings, it has great potential to explain hierarchical representations in other embedding spaces, such as SigLIP~\citep{zhai2023sigmoid} or modality-specific representations. 
Finally, since not all neurons correspond to simple concepts in our vocabulary, investigating complex semantic features through LLM-based interpretability methods could provide deeper insights into the learned hierarchical representations.
MSAE has been proposed independently in concurrent work~\citep{nabeshima2024matryoshka,bussman2024learning}; however, we conduct comprehensive evaluations and focus on its concrete application to interpreting CLIP.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \newpage
\section*{Impact Statement}

This paper presents work whose goal is to advance the field of Machine Learning Interpretability. 
There are some potential societal consequences of our work, none which we feel must be specifically highlighted here.

\bibliographystyle{icml2025}
\bibliography{references}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn

\section*{Appendix for ``Interpreting CLIP with Hierarchical Sparse Autoencoders''}

\startcontents[sections]
\printcontents[sections]{l}{1}{\setcounter{tocdepth}{2}}


\clearpage
\section{Concept Discovery and Validation} \label{sec:concept_validation}
Here, we describe our approach for detection, which concepts SAE learned, and how we validated the mappings of these concepts to specific neurons. While LLMs are commonly used to identify neuron-encoded concepts \citep{bereska2024mechanistic,conmy2023towards}, we follow \citet{rao2024discover} in implementing a more computationally efficient approach, which is more tailored to the CLIP-based SAE.

\textbf{CLIP-Based concept matching.}
The method uses predefined vocabulary of concepts (e.g., `hair', `pink') to compute cosine similarity between CLIP embeddings and SAE decoder columns. After mapping concepts to CLIP's embedding space and applying the same preprocessing as during SAE training, we remove $b_{pre}$ from the preprocessed CLIP embeddings for comparison with the decoder. For the feature columns in the SAE decoder, which are unit-magnitude by definition, the best matching concept to the neuron is determined by maximizing cosine similarity, where a value of 1 indicates perfect alignment. Thus, the optimal concept $s_c$ for neuron $p_c$ is defined as:
\begin{equation}
s_c = \underset{v \in V}{\arg\max} \left[\cos(p_c, \text{CLIP}(v))\right] = \underset{v \in V}{\arg\max} \left[\frac{p_c \cdot \text{CLIP}(v)}{|p_c| |\text{CLIP}(v)|}\right]. \label{eq:cosine}
\end{equation}

\textbf{Pre-activation bias in similarity calculations.}
While the method above suggests removing $b_{pre}$ from CLIP embeddings, our empirical analysis revealed this significantly masks neuron-concept relationships. Without $b_{pre}$, in Figure~\ref{fig:similarity_stats}, we show that similarities cluster around 0.1 (mean) with maxima around $0.15-0.2$, whereas retaining $b_{pre}$ yields higher similarity scores ($>$0.42) that correspond to correct concepts. Importantly, both approaches preserve neuron rankings, with over 95\% of concepts sharing identical highest matching neurons, so not removing bias doesn't destroy ranking. Manual evaluation confirmed that neurons with bias-removed similarities ($\sim$0.2) are under-estimated compared to their bias-inclusive counterparts ($\sim$0.5). Based on these findings, \underline{we retain $b_{pre}$ in our calculations}.


\begin{figure}[h]
\centering
    \subfigure[]{\includegraphics[width=0.87\textwidth]{plots/neuron_similarit_stats_matryoshka_bias.pdf}}
    \subfigure[]{\includegraphics[width=0.87\textwidth]{plots/neuron_similarit_stats_matryoshka.pdf}}
\caption{
    \textbf{Impact of pre-activation bias on concept similarities.} We take the highest neuron similarities per concept across expansion rates for (RW) and (UW) MSAE variants, (a) with and (b) without $b_{pre}$. Not removing $b_{pre}$ yields a better distribution with higher similarities that better reflect neuron interpretability.
}
\label{fig:similarity_stats}
\end{figure}

\textbf{Limitations of the current approach.}
We identify several limitations in the current concept mapping approach. First, the method assigns concepts to neurons based on the highest similarity regardless of the absolute matching quality, potentially leading to poor concept assignments when no good matches exist. Second, hierarchical concepts pose a challenge when matching with more specific neuronal features. For example, a high-level concept like `mammal' may show strong similarity to both `cat' and `dog' neurons, resulting in imprecise assignments. This issue stems from either semantic feature vectors that aren't perfectly orthogonal or incomplete vocabulary coverage.

\textbf{Threshold-based validation.}
To address the challenges identified above, we propose three validations to remove weak assignments. Before applying the validations, we switch the mapping from concepts to neurons, to a mapping of neurons to concepts to reduce spurious assignments. Based on this, we threshold results by either:
\begin{enumerate}
    \item Cosine similarity $> 0.42$, which ensures that the neurons exhibit strong alignment with their assigned concepts, preventing weak or ambiguous concept mappings
    \item Concept similarity ratio $\frac{\text{Top similarity}}{\text{Second-highest similarity}} > 2.0$, confirms concept uniqueness by requiring the best match to be at least twice as strong as the second-best concept, avoiding distributed representations
    \item One concept per neuron (with the highest similarity) enforces monosemanticity by assigning only the most strongly aligned concept to each neuron, which is needed due to the vocabulary structure containing multiple variations of the same concept (e.g., `bird' and `birdie')
\end{enumerate}

\textbf{Vocab data.}
Following \citep{ramaswamy2023overlooked} principle that vocabulary concepts should be simple, we adopted the vocabulary from \citep{bhalla2402interpreting}. This vocabulary comprises the most frequent unigrams from LAION-400m captions dataset \citep{schuhmann2021laion}. To account for semantic relationships between concepts, we perform manual validation of top concepts for each discovered neuron.

\textbf{Semantic consistency.}
Manual evaluation of top concepts per neuron verifies concept consistency and identifies hierarchical relationships, where top vocab similarities (such as dog breeds) can indicate broader categorical concepts (such as `dog').

\textbf{Results across SAE architectures.}
We evaluate concept neurons across architectures in Table~\ref{tab:concepts}. From 37,445 neurons at expansion rate 8, only a small fraction passed similarity validation: $\sim$10\% for TopK and 1--3\% for ReLU and Matryoshka architectures. While higher expansion rates typically reduce valid neurons, both TopK variants and ReLU ($\lambda = 0.001$) exhibit increased valid mappings under best vector validation. Although these results suggest limited concept learning or concept distribution across neurons, the vocabulary structure prevents definitive conclusions, due to the dominance of non-semantic unigrams, and many semantically similar concepts appear across vocabulary (e.g., `blue', `blau' and `bleu'). The validation results from the table demonstrate that sparser architectures (TopK) yield 3--8 times more interpretable concept neurons compared to more dense ones (ReLU), with Matryoshka being between the two, supporting the hypothesis that sparsity promotes concept specialization.

\begin{table}[ht]
\caption{\textbf{Comparison of valid concept neurons detected across different SAEs and validation methods.} The validation methods include a cosine similarity threshold above 0.42, selecting the best matching neuron, combining both criteria, applying the concept similarity ratio threshold between the first and second best vocab concept for the neuron, and enforcing all conditions simultaneously. Measurements were made for each model at three expansion rates ($\times8\mid\times16\mid\times32$).}
\label{tab:concepts}
\vspace*{0.1in}
\centering
\begin{small}
\begin{tabular}{lccccc}
\toprule
\textbf{Model} & \textbf{Similarity above 0.42} & \textbf{Best vector} & \textbf{Above and best} & \textbf{Ratio threshold} & \textbf{All conditions}  \\
\midrule
ReLU ($\lambda = 0.03$) & $3308\mid3765\mid4181$ & $2740\mid3608\mid5129$ & $874\mid1046\mid1304$ & $380\mid175\mid45$ & $97\mid31\mid16$ \\
ReLU ($\lambda = 0.003$) & $896\mid781\mid799$ & $2372\mid3305\mid5129$ & $217\mid196\mid188$ & $395\mid251\mid194$ & $29\mid19\mid7$ \\
ReLU ($\lambda = 0.001$) & $351\mid247\mid128$ & $4116\mid6793\mid11417$ & $77\mid63\mid32$ & $169\mid47\mid3$ & $8\mid2\mid0$ \\
\midrule
TopK ($k = 32$) & $4081\mid4719\mid5027$ & $2755\mid3415\mid3827$ & $1021\mid1259\mid1411$ & $999\mid857\mid858$ & $216\mid197\mid203$ \\
TopK ($k = 64$) & $3797\mid4504\mid4915$ & $2557\mid3272\mid167$ & $873\mid1080\mid1238$ & $1322\mid1151\mid1167$ & $238\mid232\mid238$ \\
TopK ($k = 128$) & $2141\mid2590\mid3059$ & $2167\mid2670\mid3306$ & $455\mid565\mid745$ & $1508\mid1383\mid1379$ & $211\mid226\mid231$ \\
TopK ($k = 256$) & $943\mid888\mid962$ & $1883\mid2191\mid2631$ & $168\mid167\mid171$ & $1579\mid1523\mid1554$ & $134\mid126\mid127$ \\
\midrule
Matryoshka (RW) & $1136\mid1109\mid1038$ & $1628\mid2213\mid2541$ & $237\mid257\mid259$ & $1429\mid1135\mid1059$ & $140\mid132\mid121$ \\
Matryoshka (UW) & $907\mid894\mid748$ & $1517\mid1908\mid2396$ & $195\mid191\mid167$ & $1254\mid1169\mid1069$ & $125\mid128\mid98$ \\
\bottomrule
\end{tabular}
\end{small}
\end{table}



%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\section{Implementation Details} \label{sec:implementation}
We conducted experiments using CLIP ViT-L/14 (and ViT-B/16, reported later in the appendix) pre-trained on the CC3M dataset image training subset. Following \citep{bricken2023monosemanticity,gao2024scaling} and this blog\footnote{\url{https://transformer-circuits.pub/2024/april-update/index.html\#training-saes}}, our SAE implementation uses unit-norm constraint on the decoder columns with untied encoder and decoder. We initialized $b_{\mathrm{pre}}$ as the geometric median of CLIP embeddings, $b_{\mathrm{enc}}$ with zeros, decoder with uniform Kaiming initialization (scaled $L_2$ norm to 0.1), and encoder as the decoder's transpose. Gradient clipping was set to 1. For data preprocessing, we centralized embeddings per modality \citep{bhalla2402interpreting} and scaled by a constant to achieve $\mathbb{E}_{x \in \mathcal{X}} [\|x\|_2] = \sqrt{n}$. All models were trained for 30 epochs on a single NVIDIA A100 GPU with batch size 4096, except for the model with an expansion rate of 32, which was trained for 20 epochs. While MSAE and TopK showed dead neurons, we omitted revival strategies as only in TopK the number of dead neurons exceeded 1\%.

\subsection{Hyperparameters}
We first conducted experiments on CLIP RN50 using hyperparameters from \citep{rao2024discover}, later validating them on ViT-L/14. For ViT-L/14, we explored parameters near RN50-optimal values to ensure cross-architecture consistency. With expansion factor 8 (768  6144), we explore:
\begin{itemize}
    \item Learning rates per method: {$1\cdot10^{-5},5\cdot10^{-5},1\cdot10^{-4},5\cdot10^{-4},1\cdot10^{-3}$}
    \item ReLU $L_1$ coefficients ($\lambda$): {$1\cdot10^{-4},3\cdot10^{-3},1\cdot10^{-3},3\cdot10^{-2}$}
    \item TopK values: k $\in$ \{32, 64, 128, 256\}, up to 256 as \citep{gao2024scaling} suggests higher values do not learn interpretable features
    \item Matryoshka K-lists: \{32\ldots6144\} and \{64\ldots6144\}, for higher expansion rates we adjust the upper limit
    \item $\alpha$ coefficients: uniform weighting (UW) \{1,1,1,1,1,1,1\} and reverse weighting (RW) \{7,6,5,4,3,2,1\}
\end{itemize}
The optimal parameters from these experiments were applied to larger expansion factors of 16 ($768 \rightarrow 12288$), 32 ($768 \rightarrow 24576$), and all expansion rates of VIT-B/16.


\subsection{Optimal Parameters}
Based on RN50 experiments and subsequent adjustment to VIT-L/14 with expansion factor 8, we selected the following optimal configurations: ReLU with learning rate $5\cdot10^{-5}$ and $\lambda$ values of {$1\cdot10^{-3}$, $3\cdot10^{-3}$, $3\cdot10^{-2}$}; TopK with learning rate $5\cdot10^{-4}$ and k values of 32, 64, 128, and 256; MSAE with learning rate $1\cdot10^{-4}$, K-list \{64\ldots6144\}, for both uniform (UW) and reverse weighting (RW) $\alpha$ strategies.



%%%%%%%%%%%%%%%%
\clearpage
\section{Highest Neuron Magnitudes} \label{sec:highest}
Based on results from Figure~\ref{fig:statistic-main}, we analyze images from ImageNet-1k validation set that produced the highest neuron magnitudes for TopK and MSAE architectures. 
In Table \ref{tab:max_active}, we show that more constrained SAEs (TopK ($k \leq 128$)) produce a higher number of samples with neurons above 15; however, the percentage of valid neurons is lower than in MSAE and TopK ($k = 256$) which have significantly less high magnitude samples. This indicates that high-magnitude neurons in highly constrained TopK may presumably learn complex features. Figure~\ref{fig:max_active} presents the top 6 valid highest neuron magnitude images per model, demonstrating that very high magnitudes often correspond to images with almost singular concepts.

\begin{table}[h]
\caption{\textbf{Analysis of high-magnitude neurons across architectures.} We analyze samples with magnitude $>15$ in the ImageNet-1k validation set, showing the number of total occurrences, the proportion of valid concepts among high-magnitude concepts, and the rate of high-magnitude valid concepts relative to all valid concepts in the model from Table~\ref{tab:concepts}.}
\label{tab:max_active}
\vspace*{0.1in}
\centering
\begin{tabular}{lccc}
\toprule
\textbf{Model} & \textbf{High-Magnitude Samples} & \textbf{Valid Concept Rate} & \textbf{High-Magnitude Concept Rate} \\
\midrule
TopK ($k = 32$) & $113$ & $6$ ($5\%$) & $216$ ($3\%$) \\
TopK ($k = 64$) & $18$ & $0$ ($0\%$) & $238$ ($0\%$) \\
TopK ($k = 128$) & $3$ & $0$ ($0\%$) & $211$ ($0\%$) \\
TopK ($k = 256$) & $12$ & $8$ ($67\%$) & $134$ ($6\%$) \\
\midrule
MSAE (RW) & $21$ & $8$ ($38\%$) & $140$ ($6\%$) \\
MSAE (UW) & $22$ & $7$ ($32\%$) & $125$ ($6\%$) \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[h]
\centering
    \subfigure[]{\includegraphics[width=0.49\textwidth]{plots/topk_32_max_images.pdf}}
    \subfigure[]{\includegraphics[width=0.49\textwidth]{plots/topk_256_max_images.pdf}}
    \subfigure[]{\includegraphics[width=0.49\textwidth]{plots/matryoshka_rw_max_images.pdf}}
    \subfigure[]{\includegraphics[width=0.49\textwidth]{plots/matryoshka_uw_max_images.pdf}}
\caption{\textbf{Images with the highest valid concept neuron magnitudes.} We took 6 images from ImageNet-1k validation set per model based on results from Table~\ref{tab:max_active} with (a) TopK $k = 32$, (b) TopK $k = 256$, (c) Matryoshka RW, and (d) Matryoshka UW.}
\label{fig:max_active}
\end{figure}



%%%%%%%%%%%%
\clearpage
\section{Activation Soft-capping}\label{sec:soft-capping}
Analysis of MSAE in Figure \ref{fig:statistic-main} reveals that despite effective handling of multi-granular sparsity, the model learns to encode concepts using extremely large activation values ($>$15). This can lead to more composite rather than atomic features, as it was in the case of TopK ($k \leq 128$) revealed in Appendix~\ref{sec:highest}.

\subsection{Definition of Soft-capping.}
To address this, we introduce activation soft-capping (SC), adapting the logit soft-capping concept from language models~\citep{team2024gemma}. This technique prevents too high activation magnitude and circumvention of sparsity constraints via activation magnitude manipulation:
\begin{equation}\label{eq:softcap}
\hat{z} = \mathrm{softcap} \cdot \tanh(z/\mathrm{softcap}), \quad \hat{x} = W_{\mathrm{dec}} \hat{z} + b_{\mathrm{pre}},
\end{equation}
where $\mathrm{softcap}$ hyperparameter controls maximum activation magnitude. Combined with ReLU, this bounds SAE activations to $(0, \mathrm{softcap})$.

\subsection{Results.}
In Table \ref{tab:soft-cap}, we show soft-capping's impact on MSAE performance across key metrics using the ImageNet-1k training set. Our analysis reveals two key benefits of applying soft-capping on MSAE. First, it consistently improves $L_0$ sparsity, with MSAE RW (SC) achieving values of 0.830 and 0.889 for 6144 and 12288 sizes, respectively. Second, while base MSAE UW maintains better FVU and CS scores, the soft-capped MSAE RW significantly reduces the number of dead neurons. With a latent size of 12288, MSAE RW exhibits only 66 dead neurons, compared to 491 in the model with a latent size of 6144.
These findings show that soft-capping is particularly beneficial for large-scale SAEs with wider sparse layers, where neuron utilization becomes more challenging. The technique provides a practical approach to reducing dead neurons while maintaining high $L_0$ sparsity, with only minimal impact on reconstruction fidelity.

\begin{table}[h]
\caption{\textbf{Impact of soft-capping (SC) on MSAE performance.}  We evaluate soft-capping across different expansion rates (8 and 16) on the ImageNet-1k validation set, comparing UW and RW variants. While base MSAE maintains better FVU and CS scores, soft-capped variants show improved $L_0$ sparsity and reduced number of dead neurons, particularly at larger sizes. \textbf{Bold} values indicate the best performance per metric and size, with NDN in parentheses showing dead neuron counts from the final checkpoint.}
\label{tab:soft-cap}
\vspace*{0.1in}
\centering
\begin{small}
\begin{tabular}{llccccl}
\toprule
Size & Model & $L_0$ $\uparrow$ & FVU $\downarrow$ & CS $\uparrow$ & CKNNA $\uparrow$ & NDN $\downarrow$ \\
\midrule
\multirow{4}{*}{6144} & Matryoshka (RW) & $0.829_{\pm.008}$ & $0.007_{\pm.003}$ & $0.997_{\pm.002}$ & $0.809_{\pm.002}$ & $2(4)$ \\
& Matryoshka (UW) & $0.748_{\pm.006}$ & $\textbf{0.001}_{\pm.002}$ & $\textbf{0.999}_{\pm.000}$ & $0.848_{\pm.003}$ & $0(22)$ \\
& Matryoshka (RW, SC) & $\textbf{0.830}_{\pm.007}$ & $0.010_{\pm.003}$ & $0.995_{\pm.002}$ & $0.839_{\pm.004}$ & $\textbf{1(2)}$ \\
& Matryoshka (UW, SC) & $0.774_{\pm.006}$ & $0.004_{\pm.001}$ & $0.998_{\pm.001}$ & $\textbf{0.856}_{\pm.003}$ & $1(3)$ \\
\midrule
\multirow{4}{*}{12288} & Matryoshka (RW) & $0.884_{\pm.006}$ & $0.005_{\pm.003}$ & $0.998_{\pm.001}$ & $0.801_{\pm .003}$ & $32(124)$ \\
& Matryoshka (UW) & $0.830_{\pm.003}$ & $\textbf{0.000}_{\pm.000}$ & $\textbf{1.000}_{\pm.000}$ & $\textbf{0.853}_{\pm .002}$ & $22(491)$ \\
& Matryoshka (RW, SC) & $\textbf{0.889}_{\pm.005}$ & $0.007_{\pm.003}$ & $0.997_{\pm.001}$ & $0.833_{\pm .002}$ & $\textbf{11(66)}$ \\
& Matryoshka (UW, SC) & $0.842_{\pm.005}$ & $0.001_{\pm.001}$ & $0.999_{\pm.000}$ & $0.849_{\pm .002}$ & $87(172)$ \\
\bottomrule
\end{tabular}
\end{small}
\end{table}



%%%%%%%%%%%%%
\clearpage
\section{CKNNA Alignment Metric}\label{sec:cknna}
Introduced in Section~\ref{sec:metrics}, CKNNA (Centered Kernel Nearest-Neighbor Alignment) measures representation similarity between networks while focusing on local neighborhood structures. Unlike its predecessor CKA \citep{kornblith2019similarity}, CKNNA refines the alignment computation by considering only $k$-nearest neighbors, making it more sensitive to local geometric relationships. The alignment score between two networks' representations in our case CLIP embeddings and SAE activation is computed as:

\begin{equation}
\begin{gathered}
    \text{CKNNA}(\mathbf{K}, \mathbf{L}) = \frac{\text{Align}(\mathbf{K}, \mathbf{L})}{\sqrt{\text{HSIC}(\mathbf{K}, \mathbf{K})\text{HSIC}(\mathbf{L}, \mathbf{L})}}, \\
    \text{HSIC}(\mathbf{K}, \mathbf{L}) = \frac{1}{(n-1)^2}\left(\sum_i\sum_j (\langle\phi_i, \phi_j\rangle - \mathbb{E}_l[\langle\phi_i, \phi_l\rangle])(\langle\psi_i, \psi_j\rangle - \mathbb{E}_l[\langle\psi_i, \psi_l\rangle])\right), \\
    \text{Align}(\mathbf{K}, \mathbf{L}) = \frac{1}{(n-1)^2}\left(\sum_i\sum_j \alpha(i,j)(\langle\phi_i, \phi_j\rangle - \mathbb{E}_l[\langle\phi_i, \phi_l\rangle])(\langle\psi_i, \psi_j\rangle - \mathbb{E}_l[\langle\psi_i, \psi_l\rangle])\right), \\
    \alpha(i,j;k) = \mathbf{1}[i \neq j \text{ and } \phi_j \in \text{knn}(\phi_i; k) \text{ and } \psi_j \in \text{knn}(\psi_i; k)],
\end{gathered}
\end{equation}

where $\text{HSIC}$ measures the global similarity between kernel matrices, and $\text{Align}$ introduces the neighborhood constraint through $\alpha(i,j;k)$. The indicator function $\alpha(i,j;k)$ ensures that only pairs of points that are $k$-nearest neighbors in both representation spaces contribute to the alignment score. Here, $\phi_i, \phi_j$ represent CLIP embeddings and $\psi_i, \psi_j$ represent SAE activations for corresponding input data points $i$ and $j$. Following \citep{yu2025repa}, we set $k=10$ as it provides better alignment sensitivity and calculate CKNNA over randomly sampled (batch size) 10,000 representations when evaluating. Higher CKNNA scores indicate stronger similarity between the CLIP and SAE learned representations.



%%%%%%%%%%%
\clearpage
\section{Evaluating MSAE: Additional Results}\label{sec:append_qaq}
We extend the results from Section~\ref{sec:eval_msae} by analyzing multiple expansion rates, SAE variants, input modalities, and CLIP architectures. Unless otherwise specified, experiments use CLIP ViT-L/14 with an expansion rate of 8 on image modality. For text modality evaluations, we use the CC3M validation subset, while image modality evaluations are performed on the ImageNet-1k training subset.

\subsection{SparsityFidelity Trade-off} \label{sec:appendix_pareto}
Figure~\ref{fig:appendix_pareto} presents an extended analysis of sparsity-fidelity trade-offs, including standard deviations and alternative reconstruction metric more tailored for the CLIP embeddings (cosine similarity). The results demonstrate MSAE's superior stability across both modalities, particularly in text representations where only MSAE show stable and elevated results. Figure~\ref{fig:appendix_pareto_vitb} strengthens our findings by showing MSAE superiority on different CLIP architecture (ViT-B/16).

\begin{figure}[h]
\centering
\includegraphics[width=0.9\textwidth]{plots/pareto.pdf}
\caption{\textbf{Extended sparsity-fidelity trade-off analysis across modalities.} Expanding on Figure~\ref{fig:pareto-mae}, we compare ReLU SAE ($\lambda = {0.001, 0.003, 0.03}$), TopK SAE ($k = {32, 64, 128, 256}$), and MSAE (UW, RW) using two reconstruction metrics: mean EVR fidelity (top) and mean cosine similarity (bottom). Results are shown for both image (left) and text (right) modalities, with standard deviation also reported for each metric, demonstrating MSAE's consistent performance across modalities and metrics.}
\label{fig:appendix_pareto}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[width=0.9\textwidth]{plots/pareto-vitb.pdf}
\caption{\textbf{ViT-B/16 sparsity-fidelity trade-off analysis across modalities.} Parallel analysis to ViT-L/14 (Figure~\ref{fig:appendix_pareto}), demonstrating that MSAE's superior performance and stability generalizes across CLIP architectures.}
\label{fig:appendix_pareto_vitb}
\end{figure}


\clearpage
\subsection{Ablation: Matryoshka at Lower Granularity Levels}\label{sec:appendix_comparison}
Figure~\ref{fig:appendix_matryoshkavstopk} extends the analysis from Figure~\ref{fig:matryoshkavstopk} by evaluating four key metrics: reconstruction fidelity (EVR), reconstruction error (CS), alignment (CKNNA), and neuron utilization (NDN). Our expanded comparison reinforces MSAE's competitive performance against TopK SAE, demonstrating that MSAE (RW) achieves similar or better results across most metrics except for NDN, where (UW) version of MSAE performs better.

\begin{figure}[h]
\centering
\includegraphics[width=\textwidth]{plots/matryoshkavstopk-appendix.pdf}
\caption{\textbf{Comprehensive comparison of Matryoshka and TopK SAE on ImageNet-1k.} Extension of Figure~\ref{fig:matryoshkavstopk} comparing model performance on reconstruction fidelity (EVR), reconstruction error (CS), concept alignment (CKNNA), and neuron utilization (NDN) against sparsity (L0). MSAE (RW) demonstrates competitive performance across most metrics, while MSAE (UW) achieves better results in neuron utilization.}
\label{fig:appendix_matryoshkavstopk}
\end{figure}


\clearpage
\subsection{Activation Magnitudes Analysis}\label{sec:appendix_activation}
We extend the activation magnitude analysis from Figure~\ref{fig:statistic-main} by including varying versions of model sparsity from each evaluated SAE architecture. Figure~\ref{fig:statistic_8} shows non-zero and maximum SAE activations for expansion rate 8, revealing that less constrained TopK models exhibit double-curvature distributions similar to MSAE and ReLU. Maximum activation analysis highlights ReLU's shrinkage effect, while TopK and MSAE maintain distributions closer to normal. These patterns persist at higher expansion rates (16 and 32) as shown in Figure~\ref{fig:statistic_more}.

\begin{figure}[h]
\centering
\includegraphics[width=0.47\textwidth]{plots/statistic-8.pdf}
\caption{\textbf{Activation distributions at expansion rate 8.} Extended analysis of Figure~\ref{fig:statistic-main} showing: (left) non-zero activation distributions, revealing TopK's convergence to double-curvature patterns at lower constraints (higher $k$), (right) maximum activation distributions, demonstrating ReLU shrinkage problem compared to TopK and MSAE behavior which resembles a normal distribution.}
\label{fig:statistic_8}
\end{figure}

\begin{figure}[h]
\centering
   \subfigure[Expansion rate 16]{\includegraphics[width=0.49\textwidth]{plots/statistic-16.pdf}}
   \subfigure[Expansion rate 32]{\includegraphics[width=0.49\textwidth]{plots/statistic-32.pdf}} 
\caption{\textbf{Activation distributions at higher expansion rates.} Extended analysis of Figure~\ref{fig:statistic-main} for expansion rates 16 (a) and 32 (b), showing consistency of distribution patterns across scales.}
\label{fig:statistic_more}
\end{figure}


\clearpage
\subsection{Progressive Recovery}\label{sec:appendix_progressive}
We extend the analysis from Figure~\ref{fig:pareto-mae} by examining progressive reconstruction performance across additional metrics and modalities. Figure~\ref{fig:progressive_8} demonstrates performance at expansion rate 8 for reconstruction quality (EVR, CS) and neuron utilization (NDN) across both modalities, while Figure~\ref{fig:progressive_more} extends this analysis to expansion rates 16 and 32.

\begin{figure}[h]
\centering
\includegraphics[width=\textwidth]{plots/progressive_8.pdf}
\caption{\textbf{Progressive recovery analysis at expansion rate 8.} Extension of Figure~\ref{fig:pareto-mae} showing reconstruction (EVR, CS), alignment (CKNNA), and neuron utilization (NDN) metrics against an increasing number of utilized top magnitude SAE neurons for image and text modalities. MSAE demonstrates comparable performance to TopK ($k = 256$) on image modality and superior performance on text.}
\label{fig:progressive_8}
\end{figure}

\begin{figure}[h]
\centering
   \subfigure[Expansion rate 16]{\includegraphics[width=\textwidth]{plots/progressive_16.pdf}} \\
   \subfigure[Expansion rate 32]{\includegraphics[width=\textwidth]{plots/progressive_32.pdf}} 
\caption{\textbf{Progressive recovery analysis at higher expansion rates.} Analysis parallel to Figure~\ref{fig:progressive_8} for expansion rates 16 (a) and 32 (b), demonstrating the stability of the results over higher expansion rates.}
\label{fig:progressive_more}
\end{figure}



\clearpage
\subsection{Comprehensive Evaluation with CLIP ViT-L/14 and ViT-B/16 Architectures}\label{sec:appendix_architectures}
We present an extensive quantitative comparison of SAE variants across CLIP ViT-L/14 and ViT-B/16 architectures. Our evaluation encompasses ReLU ($\lambda = {0.03, 0.003, 0.001}$), TopK ($k = {64, 128, 256}$), and MSAE (RW, UW) models, tested across three expansion rates (8, 16, 32) for both image and text modalities.  For text modality and ViT-B/16 architecture, we omit LP (Acc) and LP (KL) metrics based on our findings in Section~\ref{sec:metrics} that CS and FVU correlate strongly with linear probing metrics.


\subsubsection{Results for Image Modality}
For image modality, Tables~\ref{tab:metrics_8}--\ref{tab:metrics_32} present detailed results for ViT-L/14 across expansion rates 8, 16, and 32, while Tables~\ref{tab:metrics_vitb_8}--\ref{tab:metrics_vitb_32} show parallel performance metrics for ViT-B/16. These tables extend the analysis from Table~\ref{tab:metrics}, providing comprehensive measurements across different metrics and model configurations.


\begin{table}[h]
\caption{\textbf{CLIP ViT-L/14 SAE comparison at expansion rate 8.} Extended evaluation from Table~\ref{tab:metrics} with additional TopK ($k = 128$) and ReLU ($\lambda = 0.001$) variants on ImageNet-1k. Arrows indicate preferred metric direction, NDN values show training set dead neurons in parentheses.}
\label{tab:metrics_8}
\vspace*{0.1in}
\centering
\begin{small}
\begin{tabular}{lcccccccc}
\toprule
\textbf{Model} & \textbf{$L_0$ $\uparrow$} & \textbf{FVU $\downarrow$} & \textbf{CS $\uparrow$} & \textbf{LP (KL) $\downarrow$} & \textbf{LP (Acc) $\uparrow$} & \textbf{CKNNA $\uparrow$} & \textbf{DO $\downarrow$} & \textbf{NDN $\downarrow$} \\ 
\midrule
ReLU ($\lambda = 0.03$) & $.920_{\pm.008}$ & $.185_{\pm.031}$ & $.928_{\pm.009}$ & $50.5_{\pm77.1}$ & $.936_{\pm.244}$ & $.727_{\pm.004}$ & $.003$ & $0(0)$ \\
ReLU ($\lambda = 0.003$) & $.649_{\pm.007}$ & $.004_{\pm.000}$ & $.998_{\pm.000}$ & $0.66_{\pm1.03}$ & $.994_{\pm.083}$ & $.781_{\pm.004}$ & $.003$ & $0(0)$ \\
ReLU ($\lambda = 0.001$) & $.553_{\pm.006}$ & $.002_{\pm.001}$ & $.999_{\pm.000}$ & $0.36_{\pm0.65}$ & $.995_{\pm.073}$ & $.822_{\pm.004}$ & $.002$ & $0(0)$ \\
TopK ($k = 64$) & $.950_{\pm.009}$ & $.172_{\pm.026}$ & $.912_{\pm.013}$ & $60.1_{\pm90.8}$ & $.930_{\pm.255}$ & $.762_{\pm.004}$ & $.002$ & $0(335)$ \\
TopK ($k = 128$) & $.928_{\pm.008}$ & $.098_{\pm.015}$ & $.951_{\pm.007}$ & $2.71_{\pm5.40}$ & $.987_{\pm.114}$ & $.811_{\pm.004}$ & $.003$ & $0(117)$ \\
TopK ($k = 256$) & $.900_{\pm.004}$ & $.011_{\pm.003}$ & $.994_{\pm.002}$ & $2.71_{\pm5.40}$ & $.987_{\pm.114}$ & $.874_{\pm.003}$ & $.003$ & $0(296)$ \\
\midrule
Matryoshka (RW) & $.829_{\pm.008}$ & $.007_{\pm.003}$ & $.997_{\pm.002}$ & $3.13_{\pm7.08}$ & $.987_{\pm.115}$ & $.809_{\pm.002}$ & $.002$ & $2(4)$ \\
Matryoshka (UW) & $.748_{\pm.006}$ & $.002_{\pm.001}$ & $.999_{\pm.000}$ & $0.35_{\pm0.82}$ & $.995_{\pm.070}$ & $.848_{\pm.003}$ & $.001$ & $0(22)$ \\
\bottomrule
\end{tabular}
\end{small}
\end{table}

\begin{table}[h]
\caption{\textbf{CLIP ViT-L/14 SAE comparison at expansion rate 16.} Results parallel to Table~\ref{tab:metrics_8} showing performance scaling at higher expansion rate on ImageNet-1k.}
\label{tab:metrics_16}
\vspace*{0.1in}
\centering
\begin{small}
\begin{tabular}{lcccccccc}
\toprule
\textbf{Model} & \textbf{$L_0$ $\uparrow$} & \textbf{FVU $\downarrow$} & \textbf{CS $\uparrow$} & \textbf{LP (KL) $\downarrow$} & \textbf{LP (Acc) $\uparrow$} & \textbf{CKNNA $\uparrow$} & \textbf{DO $\downarrow$} & \textbf{NDN $\downarrow$} \\ 
\midrule
ReLU ($\lambda = 0.03$) & $.945_{\pm.006}$ & $.147_{\pm.033}$ & $.939_{\pm.008}$ & $41.1_{\pm64.8}$ & $.945_{\pm.229}$ & $.714_{\pm.004}$ & $.003$ & $0(0)$ \\
ReLU ($\lambda = 0.003$) & $.716_{\pm.009}$ & $.006_{\pm.001}$ & $.997_{\pm.000}$ & $1.08_{\pm1.74}$ & $.991_{\pm.093}$ & $.695_{\pm.003}$ & $.002$ & $0(0)$ \\
ReLU ($\lambda = 0.001$) & $.664_{\pm.007}$ & $.001_{\pm.000}$ & $.999_{\pm.000}$ & $0.14_{\pm0.22}$ & $.997_{\pm.056}$ & $.789_{\pm.004}$ & $.002$ & $0(0)$ \\
TopK ($k = 64$) & $.973_{\pm.006}$ & $.174_{\pm.028}$ & $.911_{\pm.014}$ & $61.7_{\pm96.8}$ & $.927_{\pm.260}$ & $.745_{\pm.003}$ & $.002$ & $0(2079)$ \\
TopK ($k = 128$) & $.960_{\pm.006}$ & $.104_{\pm.017}$ & $.948_{\pm.008}$ & $30.7_{\pm49.0}$ & $.951_{\pm.215}$ & $.801_{\pm.003}$ & $.002$ & $1(897)$ \\
TopK ($k = 256$) & $.937_{\pm.006}$ & $.019_{\pm.004}$ & $.991_{\pm.002}$ & $3.76_{\pm6.85}$ & $.984_{\pm.127}$ & $.871_{\pm.002}$ & $.004$ & $15(1383)$ \\
\midrule
Matryoshka (RW) & $.884_{\pm.006}$ & $.005_{\pm.003}$ & $.998_{\pm.001}$ & $2.08_{\pm4.68}$ & $.989_{\pm.103}$ & $.801_{\pm.003}$ & $.002$ & $32(124)$ \\
Matryoshka (UW) & $.830_{\pm.003}$ & $.000_{\pm.000}$ & $.999_{\pm.000}$ & $0.12_{\pm0.41}$ & $.998_{\pm.050}$ & $.853_{\pm.002}$ & $.002$ & $22(491)$ \\
\bottomrule
\end{tabular}
\end{small}
\end{table}

\begin{table}[h]
\caption{\textbf{CLIP ViT-L/14 SAE comparison at expansion rate 32.} Analysis at maximum tested expansion rate on ImageNet-1k, completing the scaling study from Tables~\ref{tab:metrics_8} and~\ref{tab:metrics_16}.}
\label{tab:metrics_32}
\vspace*{0.1in}
\centering
\begin{small}
\begin{tabular}{lcccccccc}
\toprule
\textbf{Model} & \textbf{$L_0$ $\uparrow$} & \textbf{FVU $\downarrow$} & \textbf{CS $\uparrow$} & \textbf{LP (KL) $\downarrow$} & \textbf{LP (Acc) $\uparrow$} & \textbf{CKNNA $\uparrow$} & \textbf{DO $\downarrow$} & \textbf{NDN $\downarrow$} \\ 
\midrule
ReLU ($\lambda = 0.03$) & $.964_{\pm.004}$ & $.120_{\pm.029}$ & $.948_{\pm.007}$ & $36.1_{\pm60.2}$ & $.949_{\pm.221}$ & $.707_{\pm.005}$ & $.003$ & $0(0)$ \\
ReLU ($\lambda = 0.003$) & $.781_{\pm.007}$ & $.011_{\pm.002}$ & $.995_{\pm.001}$ & $2.06_{\pm3.40}$ & $.988_{\pm.111}$ & $.619_{\pm.007}$ & $.002$ & $0(0)$ \\
ReLU ($\lambda = 0.001$) & $.653_{\pm.005}$ & $.004_{\pm.001}$ & $.998_{\pm.000}$ & $0.77_{\pm1.25}$ & $.993_{\pm.085}$ & $.493_{\pm.007}$ & $.002$ & $0(0)$ \\
TopK ($k = 64$) & $.973_{\pm.006}$ & $.174_{\pm.028}$ & $.911_{\pm.014}$ & $61.7_{\pm96.8}$ & $.927_{\pm.260}$ & $.745_{\pm.003}$ & $.002$ & $0(9347)$ \\
TopK ($k = 128$) & $.964_{\pm.009}$ & $.110_{\pm.021}$ & $.947_{\pm.009}$ & $31.4_{\pm51.2}$ & $.952_{\pm.213}$ & $.794_{\pm.005}$ & $.002$ & $10(5604)$ \\
TopK ($k = 256$) & $.942_{\pm.012}$ & $.032_{\pm.008}$ & $.986_{\pm.003}$ & $5.39_{\pm8.80}$ & $.980_{\pm.139}$ & $.864_{\pm.003}$ & $.003$ & $91(6590)$ \\
\midrule
Matryoshka (RW) & $.927_{\pm.004}$ & $.003_{\pm.001}$ & $.999_{\pm.001}$ & $1.00_{\pm2.15}$ & $.992_{\pm.090}$ & $.810_{\pm.002}$ & $.002$ & $79(142)$ \\
Matryoshka (UW) & $.908_{\pm.002}$ & $.000_{\pm.000}$ & $.999_{\pm.000}$ & $0.09_{\pm0.35}$ & $.998_{\pm.047}$ & $.850_{\pm.003}$ & $.002$ & $297(162)$ \\
\bottomrule
\end{tabular}
\end{small}
\end{table}

\begin{table}[h]
\caption{\textbf{CLIP ViT-B/16 SAE comparison at expansion rate 8.} Parallel analysis to ViT-L/14 (Table~\ref{tab:metrics_8}) using smaller CLIP architecture on ImageNet-1k.}
\label{tab:metrics_vitb_8}
\vspace*{0.1in}
\centering
\begin{small}
\begin{tabular}{lcccccc}
\toprule
\textbf{Model} & \textbf{$L_0$ $\uparrow$} & \textbf{FVU $\downarrow$} & \textbf{CS $\uparrow$} & \textbf{CKNNA $\uparrow$} & \textbf{DO $\downarrow$} & \textbf{NDN $\downarrow$} \\ 
\midrule
ReLU ($\lambda = 0.03$) & $.908_{\pm.010}$ & $.154_{\pm.036}$ & $.936_{\pm.010}$ & $.671_{\pm.004}$ & $.004$ & $0(0)$ \\
ReLU ($\lambda = 0.003$) & $.629_{\pm.008}$ & $.003_{\pm.000}$ & $.999_{\pm.000}$ & $.737_{\pm.003}$ & $.003$ & $0(0)$ \\
ReLU ($\lambda = 0.001$) & $.366_{\pm.012}$ & $.009_{\pm.003}$ & $.996_{\pm.001}$ & $.695_{\pm.003}$ & $.002$ & $0(0)$ \\
TopK ($k = 64$) & $.935_{\pm.011}$ & $.138_{\pm.024}$ & $.930_{\pm.012}$ & $.730_{\pm.003}$ & $.003$ & $0(196)$ \\
TopK ($k = 128$) & $.843_{\pm.020}$ & $.116_{\pm.026}$ & $.948_{\pm.010}$ & $.735_{\pm.003}$ & $.003$ & $0(95)$ \\
TopK ($k = 256$) & $.859_{\pm.008}$ & $.024_{\pm.007}$ & $.988_{\pm.003}$ & $.787_{\pm.002}$ & $.004$ & $0(8)$ \\
\midrule
Matryoshka (RW) & $.783_{\pm.009}$ & $.003_{\pm.001}$ & $.999_{\pm.001}$ & $.792_{\pm.003}$ & $.003$ & $0(0)$ \\
Matryoshka (UW) & $.711_{\pm.004}$ & $.000_{\pm.000}$ & $.999_{\pm.000}$ & $.814_{\pm.002}$ & $.003$ & $0(1)$ \\
\bottomrule
\end{tabular}
\end{small}
\end{table}

\begin{table}[h]
\caption{\textbf{CLIP ViT-B/16 SAE comparison at expansion rate 16.} Extension of Table~\ref{tab:metrics_vitb_8} to expansion rate 16 on ImageNet-1k.}
\label{tab:metrics_vitb_16}
\vspace*{0.1in}
\centering
\begin{small}
\begin{tabular}{lcccccc}
\toprule
\textbf{Model} & \textbf{$L_0$ $\uparrow$} & \textbf{FVU $\downarrow$} & \textbf{CS $\uparrow$} & \textbf{CKNNA $\uparrow$} & \textbf{DO $\downarrow$} & \textbf{NDN $\downarrow$} \\ 
\midrule
ReLU ($\lambda = 0.003$) & $.695_{\pm.010}$ & $.004_{\pm.008}$ & $.998_{\pm.000}$ & $.635_{\pm.005}$ & $.003$ & $0(0)$ \\
ReLU ($\lambda = 0.001$) & $.646_{\pm.008}$ & $.001_{\pm.000}$ & $.999_{\pm.000}$ & $.742_{\pm.003}$ & $.003$ & $0(0)$ \\
TopK ($k = 64$) & $.962_{\pm.008}$ & $.141_{\pm.026}$ & $.930_{\pm.012}$ & $.710_{\pm.003}$ & $.003$ & $0(1268)$ \\
TopK ($k = 128$) & $.950_{\pm.007}$ & $.072_{\pm.016}$ & $.965_{\pm.007}$ & $.782_{\pm.003}$ & $.004$ & $0(597)$ \\
TopK ($k = 256$) & $.935_{\pm.003}$ & $.003_{\pm.002}$ & $.998_{\pm.001}$ & $.839_{\pm.002}$ & $.003$ & $2(4686)$ \\
\midrule
Matryoshka (RW) & $.861_{\pm.005}$ & $.002_{\pm.001}$ & $.999_{\pm.001}$ & $.778_{\pm.003}$ & $.003$ & $8(63)$ \\
Matryoshka (UW) & $.805_{\pm.004}$ & $.000_{\pm.000}$ & $.999_{\pm.000}$ & $.813_{\pm.003}$ & $.003$ & $44(275)$ \\
\bottomrule
\end{tabular}
\end{small}
\end{table}

\begin{table}[h]
\caption{\textbf{CLIP ViT-B/16 SAE comparison at expansion rate 32.} Completion of ViT-B/16 scaling analysis on ImageNet-1k at maximum tested expansion rate.}
\label{tab:metrics_vitb_32}
\vspace*{0.1in}
\centering
\begin{small}
\begin{tabular}{lcccccc}
\toprule
\textbf{Model} & \textbf{$L_0$ $\uparrow$} & \textbf{FVU $\downarrow$} & \textbf{CS $\uparrow$} & \textbf{CKNNA $\uparrow$} & \textbf{DO $\downarrow$} & \textbf{NDN $\downarrow$} \\ 
\midrule
ReLU ($\lambda = 0.03$) & $.956_{\pm.005}$ & $.104_{\pm.025}$ & $.953_{\pm.007}$ & $.656_{\pm.004}$ & $.003$ & $0(0)$ \\
ReLU ($\lambda = 0.003$) & $.757_{\pm.009}$ & $.010_{\pm.002}$ & $.996_{\pm.001}$ & $.568_{\pm.005}$ & $.003$ & $0(0)$ \\
ReLU ($\lambda = 0.001$) & $.625_{\pm.006}$ & $.004_{\pm.001}$ & $.998_{\pm.000}$ & $.516_{\pm.005}$ & $.003$ & $0(0)$ \\
TopK ($k = 64$) & $.967_{\pm.012}$ & $.152_{\pm.032}$ & $.927_{\pm.014}$ & $.698_{\pm.003}$ & $.003$ & $0(5643)$ \\
TopK ($k = 128$) & $.960_{\pm.010}$ & $.085_{\pm.023}$ & $.961_{\pm.009}$ & $.772_{\pm.002}$ & $.003$ & $2(3321)$ \\
TopK ($k = 256$) & $.922_{\pm.014}$ & $.015_{\pm.006}$ & $.995_{\pm.002}$ & $.822_{\pm.002}$ & $.003$ & $1(10480)$ \\
\midrule
Matryoshka (RW) & $.915_{\pm.003}$ & $.001_{\pm.001}$ & $.999_{\pm.000}$ & $.794_{\pm.003}$ & $.003$ & $6(23)$ \\
Matryoshka (UW) & $.880_{\pm.003}$ & $.000_{\pm.000}$ & $.999_{\pm.000}$ & $.804_{\pm.002}$ & $.002$ & $15(26)$ \\
\bottomrule
\end{tabular}
\end{small}
\end{table}


\subsubsection{Results for Text Modality}
For text modality, Tables~\ref{tab:metrics_8_text}--\ref{tab:metrics_32_text} present results for ViT-L/14, while Tables~\ref{tab:metrics_vitb_8_text}--\ref{tab:metrics_vitb_32_text} show ViT-B/16 performance on CC3M validation text data, enabling cross-modal and cross-architecture comparisons.

\begin{table}[h]
\caption{\textbf{CLIP ViT-L/14 SAE text analysis at expansion rate 8.} Evaluation on CC3M text validation set parallel to image results in Table~\ref{tab:metrics_8}, highlighting cross-modal performance differences.}
\label{tab:metrics_8_text}
\vspace*{0.1in}
\centering
\begin{small}
\begin{tabular}{lcccccc}
\toprule
\textbf{Model} & \textbf{$L_0$ $\uparrow$} & \textbf{FVU $\downarrow$} & \textbf{CS $\uparrow$} & \textbf{CKNNA $\uparrow$} & \textbf{DO $\downarrow$} & \textbf{NDN $\downarrow$} \\ 
\midrule
ReLU ($\lambda = 0.03$) & $.901_{\pm.010}$ & $.427_{\pm.174}$ & $.802_{\pm.049}$ & $.622_{\pm.003}$ & $.003$ & $0(0)$ \\
ReLU ($\lambda = 0.003$) & $.609_{\pm.027}$ & $.041_{\pm.039}$ & $.981_{\pm.018}$ & $.744_{\pm.002}$ & $.003$ & $0(0)$ \\
ReLU ($\lambda = 0.001$) & $.522_{\pm.045}$ & $.035_{\pm.038}$ & $.984_{\pm.014}$ & $.706_{\pm.005}$ & $.002$ & $0(0)$ \\
TopK ($k = 64$) & $.715_{\pm.295}$ & $.760_{\pm.249}$ & $.585_{\pm.209}$ & $.025_{\pm.020}$ & $.002$ & $0(335)$ \\
TopK ($k = 128$) & $.781_{\pm.190}$ & $.537_{\pm.252}$ & $.708_{\pm.190}$ & $.042_{\pm.011}$ & $.003$ & $0(117)$ \\
TopK ($k = 256$) & $.783_{\pm.180}$ & $.366_{\pm.366}$ & $.742_{\pm.301}$ & $.088_{\pm.007}$ & $.003$ & $0(296)$ \\
\midrule
Matryoshka (RW) & $.824_{\pm.029}$ & $.060_{\pm.052}$ & $.971_{\pm.026}$ & $.775_{\pm.001}$ & $.002$ & $0(4)$ \\
Matryoshka (UW) & $.755_{\pm.024}$ & $.026_{\pm.027}$ & $.988_{\pm.012}$ & $.790_{\pm.002}$ & $.001$ & $0(22)$ \\
\bottomrule
\end{tabular}
\end{small}
\end{table}

\begin{table}[h]
\caption{\textbf{CLIP ViT-L/14 SAE text analysis at expansion rate 16.} Extended CC3M text evaluation showing scaling effects at expansion rate 16, complementing image results from Table~\ref{tab:metrics_16}.}
\label{tab:metrics_16_text}
\vspace*{0.1in}
\centering
\begin{small}
\begin{tabular}{lcccccc}
\toprule
\textbf{Model} & \textbf{$L_0$ $\uparrow$} & \textbf{FVU $\downarrow$} & \textbf{CS $\uparrow$} & \textbf{CKNNA $\uparrow$} & \textbf{DO $\downarrow$} & \textbf{NDN $\downarrow$} \\ 
\midrule
ReLU ($\lambda = 0.03$) & $.930_{\pm.027}$ & $.510_{\pm.500}$ & $.812_{\pm.052}$ & $.581_{\pm.006}$ & $.003$ & $0(0)$ \\
ReLU ($\lambda = 0.003$) & $.675_{\pm.067}$ & $.070_{\pm.060}$ & $.973_{\pm.023}$ & $.654_{\pm.009}$ & $.002$ & $0(0)$ \\
ReLU ($\lambda = 0.001$) & $.599_{\pm.028}$ & $.021_{\pm.019}$ & $.990_{\pm.010}$ & $.781_{\pm.002}$ & $.002$ & $0(0)$ \\
TopK ($k = 64$) & $.774_{\pm.280}$ & $.790_{\pm.258}$ & $.583_{\pm.203}$ & $.023_{\pm.008}$ & $.002$ & $0(2079)$ \\
TopK ($k = 128$) & $.813_{\pm.212}$ & $.604_{\pm.255}$ & $.690_{\pm.190}$ & $.029_{\pm.010}$ & $.002$ & $0(897)$ \\
TopK ($k = 256$) & $.848_{\pm.156}$ & $.390_{\pm.357}$ & $.740_{\pm.280}$ & $.093_{\pm.004}$ & $.004$ & $0(1383)$ \\
\midrule
Matryoshka (RW) & $.880_{\pm.021}$ & $.043_{\pm.038}$ & $.980_{\pm.019}$ & $.783_{\pm.006}$ & $.002$ & $4(124)$ \\
Matryoshka (UW) & $.832_{\pm.028}$ & $.017_{\pm.017}$ & $.992_{\pm.008}$ & $.788_{\pm.001}$ & $.002$ & $0(491)$ \\
\bottomrule
\end{tabular}
\end{small}
\end{table}

\begin{table}[h]
\caption{\textbf{CLIP ViT-L/14 SAE text analysis at expansion rate 32.} Maximum expansion rate analysis on CC3M text.}
\label{tab:metrics_32_text}
\vspace*{0.1in}
\centering
\begin{small}
\begin{tabular}{lcccccc}
\toprule
\textbf{Model} & \textbf{$L_0$ $\uparrow$} & \textbf{FVU $\downarrow$} & \textbf{CS $\uparrow$} & \textbf{CKNNA $\uparrow$} & \textbf{DO $\downarrow$} & \textbf{NDN $\downarrow$} \\ 
\midrule
ReLU ($\lambda = 0.03$) & $.951_{\pm.023}$ & $.540_{\pm.667}$ & $.822_{\pm.053}$ & $.557_{\pm.001}$ & $.003$ & $1(0)$ \\
ReLU ($\lambda = 0.003$) & $.749_{\pm.090}$ & $.172_{\pm.200}$ & $.966_{\pm.027}$ & $.336_{\pm.008}$ & $.002$ & $0(0)$ \\
ReLU ($\lambda = 0.001$) & $.631_{\pm.054}$ & $.052_{\pm.045}$ & $.983_{\pm.014}$ & $.376_{\pm.007}$ & $.002$ & $0(0)$ \\
TopK ($k = 64$) & $.888_{\pm.156}$ & $.795_{\pm.337}$ & $.630_{\pm.157}$ & $.049_{\pm.026}$ & $.002$ & $0(2079)$ \\
TopK ($k = 128$) & $.871_{\pm.156}$ & $.612_{\pm.252}$ & $.717_{\pm.161}$ & $.053_{\pm.024}$ & $.002$ & $0(5604)$ \\
TopK ($k = 256$) & $.869_{\pm.134}$ & $.435_{\pm.342}$ & $.740_{\pm.240}$ & $.130_{\pm.006}$ & $.003$ & $0(6590)$ \\
\midrule
Matryoshka (RW) & $.925_{\pm.014}$ & $.030_{\pm.026}$ & $.986_{\pm.013}$ & $.774_{\pm.000}$ & $.002$ & $32(142)$ \\
Matryoshka (UW) & $.901_{\pm.026}$ & $.013_{\pm.013}$ & $.994_{\pm.006}$ & $.784_{\pm.000}$ & $.002$ & $126(162)$ \\
\bottomrule
\end{tabular}
\end{small}
\end{table}

\begin{table}[h]
\caption{\textbf{CLIP ViT-B/16 SAE text analysis at expansion rate 8.} CC3M text evaluation using smaller CLIP architecture, enabling cross-modal and cross-architecture comparisons.}
\label{tab:metrics_vitb_8_text}
\vspace*{0.1in}
\centering
\begin{small}
\begin{tabular}{lcccccc}
\toprule
\textbf{Model} & \textbf{$L_0$ $\uparrow$} & \textbf{FVU $\downarrow$} & \textbf{CS $\uparrow$} & \textbf{CKNNA $\uparrow$} & \textbf{DO $\downarrow$} & \textbf{NDN $\downarrow$} \\ 
\midrule
ReLU ($\lambda = 0.03$) & $.870_{\pm.021}$ & $.472_{\pm.166}$ & $.761_{\pm.063}$ & $.661_{\pm.008}$ & $.004$ & $0(0)$ \\
ReLU ($\lambda = 0.003$) & $.580_{\pm.028}$ & $.030_{\pm.028}$ & $.986_{\pm.013}$ & $.764_{\pm.000}$ & $.003$ & $0(0)$ \\
ReLU ($\lambda = 0.001$) & $.393_{\pm.122}$ & $.118_{\pm.177}$ & $.975_{\pm.022}$ & $.129_{\pm.021}$ & $.002$ & $0(0)$ \\
TopK ($k = 64$) & $.766_{\pm.223}$ & $.733_{\pm.353}$ & $.644_{\pm.155}$ & $.033_{\pm.001}$ & $.003$ & $0(196)$ \\
TopK ($k = 128$) & $.747_{\pm.164}$ & $.515_{\pm.343}$ & $.782_{\pm.106}$ & $.275_{\pm.016}$ & $.003$ & $0(95)$ \\
TopK ($k = 256$) &$.783_{\pm.095}$ & $.229_{\pm.152}$ & $.888_{\pm.081}$ & $.759_{\pm.000}$ & $.004$ & $0(8)$ \\
\midrule
Matryoshka (RW) & $.762_{\pm.063}$ & $.044_{\pm.047}$ & $.979_{\pm.023}$ & $.799_{\pm.001}$ & $.003$ & $0(0)$ \\
Matryoshka (UW) & $.709_{\pm.043}$ & $.021_{\pm.025}$ & $.990_{\pm.012}$ & $.812_{\pm.003}$ & $.003$ & $0(1)$ \\
\bottomrule
\end{tabular}
\end{small}
\end{table}

\begin{table}[h]
\caption{\textbf{CLIP ViT-B/16 SAE text analysis at expansion rate 16.} Results for expansion rate 16 with ViT-B/16 on CC3M text data.}
\label{tab:metrics_vitb_16_text}
\vspace*{0.1in}
\centering
\begin{small}
\begin{tabular}{lcccccc}
\toprule
\textbf{Model} & \textbf{$L_0$ $\uparrow$} & \textbf{FVU $\downarrow$} & \textbf{CS $\uparrow$} & \textbf{CKNNA $\uparrow$} & \textbf{DO $\downarrow$} & \textbf{NDN $\downarrow$} \\ 
\midrule
ReLU ($\lambda = 0.003$) & $.641_{\pm.055}$ & $.052_{\pm.038}$ & $.994_{\pm.005}$ & $.767_{\pm.001}$ & $.003$ & $0(0)$ \\
ReLU ($\lambda = 0.001$) & $.577_{\pm.037}$ & $.013_{\pm.012}$ & $.979_{\pm.016}$ & $.678_{\pm.002}$ & $.003$ & $0(0)$ \\
TopK ($k = 64$) & $.801_{\pm.231}$ & $.803_{\pm.424}$ & $.618_{\pm.170}$ & $.028_{\pm.006}$ & $.003$ & $0(1268)$ \\
TopK ($k = 128$) & $.781_{\pm.217}$ & $.601_{\pm.252}$ & $.696_{\pm.177}$ & $.045_{\pm.002}$ & $.004$ & $0(597)$ \\
TopK ($k = 256$) & $.787_{\pm.224}$ & $.430_{\pm.401}$ & $.688_{\pm.326}$ & $.098_{\pm.007}$ & $.003$ & $0(4686)$ \\
\midrule
Matryoshka (RW) & $.847_{\pm.040}$ & $.033_{\pm.036}$ & $.984_{\pm.018}$ & $.800_{\pm.002}$ & $.003$ & $0(63)$ \\
Matryoshka (UW) & $.801_{\pm.043}$ & $.017_{\pm.021}$ & $.992_{\pm.010}$ & $.803_{\pm.001}$ & $.003$ & $0(275)$ \\
\bottomrule
\end{tabular}
\end{small}
\end{table}

\begin{table}[h]
\caption{\textbf{CLIP ViT-B/16 SAE text analysis at expansion rate 32.} Final expansion rate evaluation for ViT-B/16 on CC3M text.}
\label{tab:metrics_vitb_32_text}
\vspace*{0.1in}
\centering
\begin{small}
\begin{tabular}{lcccccc}
\toprule
\textbf{Model} & \textbf{$L_0$ $\uparrow$} & \textbf{FVU $\downarrow$} & \textbf{CS $\uparrow$} & \textbf{CKNNA $\uparrow$} & \textbf{DO $\downarrow$} & \textbf{NDN $\downarrow$} \\ 
\midrule
ReLU ($\lambda = 0.03$) & $.934_{\pm.019}$ & $.472_{\pm.356}$ & $.790_{\pm.058}$ & $.610_{\pm.008}$ & $.003$ & $0(0)$ \\
ReLU ($\lambda = 0.003$) & $.713_{\pm.103}$ & $.183_{\pm.201}$ & $.963_{\pm.028}$ & $.304_{\pm.004}$ & $.003$ & $0(0)$ \\
ReLU ($\lambda = 0.001$) & $.600_{\pm.051}$ & $.041_{\pm.033}$ & $.987_{\pm.009}$ & $.422_{\pm.002}$ & $.003$ & $0(0)$ \\
TopK ($k = 64$) & $.879_{\pm.146}$ & $.832_{\pm.622}$ & $.645_{\pm.138}$ & $.075_{\pm.028}$ & $.003$ & $0(5643)$ \\
TopK ($k = 128$) & $.863_{\pm.145}$ & $.590_{\pm.247}$ & $.740_{\pm.126}$ & $.150_{\pm.029}$ & $.003$ & $0(3321)$ \\
TopK ($k = 256$) & $.793_{\pm.215}$ & $.365_{\pm.359}$ & $.776_{\pm.253}$ & $.197_{\pm.024}$ & $.003$ & $0(10480)$ \\
\midrule
Matryoshka (RW) & $.897_{\pm.035}$ & $.022_{\pm.026}$ & $.990_{\pm.013}$ & $.807_{\pm.002}$ & $.003$ & $1(23)$ \\
Matryoshka (UW) & $.873_{\pm.030}$ & $.010_{\pm.014}$ & $.995_{\pm.007}$ & $.806_{\pm.003}$ & $.002$ & $0(26)$ \\
\bottomrule
\end{tabular}
\end{small}
\end{table}



%%%%%%%%%
\clearpage
\section{Interpreting CLIP with MSAE: Additional Results}\label{sec:append_msae}

In this appendix section, we provide additional analysis supporting Section~\ref{sec:application}. Section~\ref{sec:append_visconcept} presents high-magnitude activation samples across modalities from MSAE (RW) with an expansion rate of 8. Section~\ref{sec:append_similarity} demonstrates how SAE enhances similarity search with interpretable results. Section~\ref{sec:biasvalidationtests} presents statistical gender bias analysis on CelebA dataset, supported by concept manipulation visualizations that reinforce the statistical findings. These analyses strengthen our findings from Section \ref{sec:application} while providing deeper insights into MSAE's interpretability capabilities.

\subsection{Concept Visualization Analysis}\label{sec:append_visconcept}
Figures \ref{fig:neuron_max} and \ref{fig:vis_concept} showcase six valid concepts through their highest-activating images and texts, confirming concept validity. Conversely, Figure \ref{fig:not_valid_concepts} demonstrates two invalid concepts, highlighting the importance of validation methods from Section~\ref{sec:concept_validation}.

\begin{figure}[h]
\centering
\includegraphics[width=\textwidth]{plots/neuron_max.pdf}
\caption{\textbf{High-magnitude image activations for valid concepts.} We gather top activating ImageNet-1k images for six valid MSAE~(RW) concept neurons.}
\label{fig:neuron_max}
\end{figure}

\begin{figure}[h]
\centering
   \begin{tabular}{cc}
       \subfigure[]{\includegraphics[width=0.37\textwidth]{plots/neuron_interpretation_smile.pdf}} &
       \subfigure[]{\includegraphics[width=0.37\textwidth]{plots/neuron_interpretation_alcoholic.pdf}} \\[1ex]
       \subfigure[]{\includegraphics[width=0.37\textwidth]{plots/neuron_interpretation_trio.pdf}} &
       \subfigure[]{\includegraphics[width=0.37\textwidth]{plots/neuron_interpretation_heart.pdf}} \\[1ex]
       \subfigure[]{\includegraphics[width=0.40\textwidth]{plots/neuron_interpretation_runnin.pdf}} &
       \subfigure[]{\includegraphics[width=0.37\textwidth]{plots/neuron_interpretation_questions.pdf}}
   \end{tabular}
\caption{\textbf{Cross-modal highest valid concept activation samples.} Extending Figure~\ref{fig:concept-main}, we show the highest-activating ImageNet-1k images and CC3M texts from valid MSAE (RW) concepts: \textit{smile}, \textit{alcoholic}, \textit{trio}, \textit{heart}, \textit{running}, and \textit{questions}.}
\label{fig:vis_concept}
\end{figure}

\begin{figure}[h]
\centering
   \subfigure[]{\includegraphics[width=0.49\textwidth]{plots/neuron_interpretation_6.pdf}}
   \subfigure[]{\includegraphics[width=0.49\textwidth]{plots/neuron_interpretation_hl.pdf}}
\caption{\textbf{Analysis of invalid concept neurons in MSAE (RW).} In (a), we showcase the invalid concept '6' with a low similarity score ($<0.42$), which shows inconsistent presence of the number six in the top active samples. In (b), we present how a low ratio threshold ($0.45/0.44 < 2$) can indicate a broader 'h' concept rather than a specific 'hl'/'hri' from the vocabulary.}
\label{fig:not_valid_concepts}
\end{figure}


\clearpage
\subsection{SAE-Enhanced Similarity Search}\label{sec:append_similarity}
Building upon Section~\ref{sec:application}, we demonstrate how SAE enhances nearest neighbor (NN) search by revealing shared semantic concepts between query and retrieved images. Figure~\ref{fig:similar_search} illustrates how SAE uncovers interpretable features that drive CLIP's similarity assessments. Furthermore, we show that conducting similarity search directly in the SAE activation space produces comparable results to CLIP-based search while providing more semantically meaningful matches.

\begin{figure}[h]
\centering
\includegraphics[width=\textwidth]{plots/neuron_reconstruction.pdf}
\caption{\textbf{SAE-enhanced similarity search.} Examples demonstrating how SAE reveals shared semantic concepts (bottom row) between query images and their CLIP nearest neighbors (top row), providing interpretable explanations for similarity matches. Additionally, the two rightmost examples show nearest neighbors retrieved based on SAE activation similarity, demonstrating how searching in the SAE space yields similar results to CLIP-based search while making the retrieval process more semantically interpretable.}
\label{fig:similar_search}
\end{figure}

\subsection{Gender Bias Analysis in CelebA}\label{sec:biasvalidationtests}
We analyze gender biases in a CLIP-based classification model using the CelebA dataset, which forms the foundation for our analysis in Section~\ref{sec:main_bias}. Through statistical analysis of concept magnitude distribution against the model gender predictions in Figure~\ref{fig:celeba_neuron_analysis}, we identify significant gender associations for concepts \textit{bearded}, \textit{blondes}, and \textit{glasses} in the classification model. To verify that these concepts align with the true features in the CelebA dataset, we visualize highest-activation images for each concept in Figure~\ref{fig:top_celeba}. Further concept manipulation experiments on both female (Figure~\ref{fig:similarity_manipulation}) and male (Figure~\ref{fig:celeba_neuron_man}) examples confirm and strengthen these statistical findings, providing even greater insight into the relationship between gender classification and the chosen concepts.


\begin{figure}[h]
\centering
\includegraphics[width=\textwidth]{plots/celeba_neuron_max.pdf}
\caption{\textbf{Highest-activating CelebA images for gender-associated concepts.} We visualize images from the CelebA test set that produce the highest activations for the concepts \textit{bearded}, \textit{blondes}, and \textit{glasses}, validating their alignment with the concept.}
\label{fig:top_celeba}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[width=\textwidth]{plots/celeba_neuron_man.pdf}
\caption{\textbf{Impact of concept manipulation for the male example.} Complementing Figure~\ref{fig:similarity_manipulation}, we further strengthen our findings of male association for \textit{bearded}, moderate for \textit{glasses}, and female bias for \textit{blondes} concept.}
\label{fig:celeba_neuron_man}
\end{figure}


\begin{figure}[h]
\centering
\includegraphics[width=\textwidth]{plots/celeba_neuron_analysis.pdf}
\caption{\textbf{Statistical analysis of concept-gender associations.} We analyze six concepts: \textit{bearded}, \textit{blondes}, \textit{black}, \textit{hair}, \textit{glasses}, and \textit{ginger}. For each concept, we show its density distribution of concept magnitude against gender prediction alongside corresponding boxplots. Results reveal that \textit{bearded}, \textit{blondes}, and \textit{glasses} exhibit significant gender-specific associations.}
\label{fig:celeba_neuron_analysis}
\end{figure}



\end{document}