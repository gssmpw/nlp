% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").

@book{Aho:72,
    author  = {Alfred V. Aho and Jeffrey D. Ullman},
    title   = {The Theory of Parsing, Translation and Compiling},
    year    = "1972",
    volume  = "1",
    publisher = {Prentice-Hall},
    address = {Englewood Cliffs, NJ}
}

@book{APA:83,
    author  = {{American Psychological Association}},
    title   = {Publications Manual},
    year    = "1983",
    publisher = {American Psychological Association},
    address = {Washington, DC}
}

@article{Chandra:81,
	author = {Ashok K. Chandra and Dexter C. Kozen and Larry J. Stockmeyer},
	year = "1981",
	title = {Alternation},
	journal = {Journal of the Association for Computing Machinery},
	volume = "28",
	number = "1",
	pages = "114--133",
	doi = "10.1145/322234.322243",
}

@inproceedings{andrew2007scalable,
  title={Scalable training of {L1}-regularized log-linear models},
  author={Andrew, Galen and Gao, Jianfeng},
  booktitle={Proceedings of the 24th International Conference on Machine Learning},
  pages={33--40},
  year={2007},
}

@book{Gusfield:97,
    author  = {Dan Gusfield},
    title   = {Algorithms on Strings, Trees and Sequences},
    year    = "1997",
    publisher = {Cambridge University Press},
    address = {Cambridge, UK}
}

@article{rasooli-tetrault-2015,
    author    = {Mohammad Sadegh Rasooli and Joel R. Tetreault},
    title     = {Yara Parser: {A} Fast and Accurate Dependency Parser},
    journal   = {Computing Research Repository},
    volume    = {arXiv:1503.06733},
    year      = {2015},
    url       = {http://arxiv.org/abs/1503.06733},
    note    = {version 2}
}

@article{Ando2005,
	Acmid = {1194905},
	Author = {Ando, Rie Kubota and Zhang, Tong},
	Issn = {1532-4435},
	Issue_Date = {12/1/2005},
	Journal = {Journal of Machine Learning Research},
	Month = dec,
	Numpages = {37},
	Pages = {1817--1853},
	Publisher = {JMLR.org},
	Title = {A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data},
	Volume = {6},
	Year = {2005}
}

@article{bandarkar2024layer,
  title={Layer swapping for zero-shot cross-lingual transfer in large language models},
  author={Bandarkar, Lucas and Muller, Benjamin and Yuvraj, Pritish and Hou, Rui and Singhal, Nayan and Lv, Hongjiang and Liu, Bing},
  journal={arXiv preprint arXiv:2410.01335},
  year={2024}
}

@article{meng2022locating,
  title={Locating and editing factual associations in GPT},
  author={Meng, Kevin and Bau, David and Andonian, Alex and Belinkov, Yonatan},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={17359--17372},
  year={2022}
}


@article{grattafiori2024llama,
  title={The llama 3 herd of models},
  author={Grattafiori, Aaron and Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Vaughan, Alex and others},
  journal={arXiv e-prints},
  pages={arXiv--2407},
  year={2024}
}

@article{li2024exploring,
  title={Exploring multilingual probing in large language models: A cross-language analysis},
  author={Li, Daoyang and Jin, Mingyu and Zeng, Qingcheng and Zhao, Haiyan and Du, Mengnan},
  journal={arXiv preprint arXiv:2409.14459},
  year={2024}
}

@article{wang2024probing,
  title={Probing the Emergence of Cross-lingual Alignment during LLM Training},
  author={Wang, Hetong and Minervini, Pasquale and Ponti, Edoardo M},
  journal={arXiv preprint arXiv:2406.13229},
  year={2024}
}


@article{le2023bloom,
  title={Bloom: A 176b-parameter open-access multilingual language model},
  author={Le Scao, Teven and Fan, Angela and Akiki, Christopher and Pavlick, Ellie and Ili{\'c}, Suzana and Hesslow, Daniel and Castagn{\'e}, Roman and Luccioni, Alexandra Sasha and Yvon, Fran{\c{c}}ois and Gall{\'e}, Matthias and others},
  year={2023}
}

@inproceedings{zhao2024large,
  title={How do Large Language Models Handle Multilingualism?},
  author={Zhao, Yiran and Zhang, Wenxuan and Chen, Guizhen and Kawaguchi, Kenji and Bing, Lidong},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  year={2024}
}

@article{zhao2024adamergex,
  title={AdaMergeX: Cross-Lingual Transfer with Large Language Models via Adaptive Adapter Merging},
  author={Zhao, Yiran and Zhang, Wenxuan and Wang, Huiming and Kawaguchi, Kenji and Bing, Lidong},
  journal={arXiv preprint arXiv:2402.18913},
  year={2024}
}

@misc{tian2024scicode,
    title={SciCode: A Research Coding Benchmark Curated by Scientists},
    author={Minyang Tian and Luyu Gao and Shizhuo Dylan Zhang and Xinan Chen and Cunwei Fan and Xuefei Guo and Roland Haas and Pan Ji and Kittithat Krongchon and Yao Li and Shengyan Liu and Di Luo and Yutao Ma and Hao Tong and Kha Trinh and Chenyu Tian and Zihan Wang and Bohao Wu and Yanyu Xiong and Shengzhu Yin and Minhui Zhu and Kilian Lieret and Yanxin Lu and Genglin Liu and Yufeng Du and Tianhua Tao and Ofir Press and Jamie Callan and Eliu Huerta and Hao Peng},
    year={2024},
    eprint={2407.13168},
    archivePrefix={arXiv},
    primaryClass={cs.AI}
}

@article{aleithan2024swe,
  title={Swe-bench+: Enhanced coding benchmark for llms},
  author={Aleithan, Reem and Xue, Haoran and Mohajer, Mohammad Mahdi and Nnorom, Elijah and Uddin, Gias and Wang, Song},
  journal={arXiv preprint arXiv:2410.06992},
  year={2024}
}

@article{hendrycksmath2021,
  title={Measuring Mathematical Problem Solving With the MATH Dataset},
  author={Dan Hendrycks and Collin Burns and Saurav Kadavath and Akul Arora and Steven Basart and Eric Tang and Dawn Song and Jacob Steinhardt},
  journal={NeurIPS},
  year={2021}
}

@article{glazer2024frontiermath,
  title={Frontiermath: A benchmark for evaluating advanced mathematical reasoning in ai},
  author={Glazer, Elliot and Erdil, Ege and Besiroglu, Tamay and Chicharro, Diego and Chen, Evan and Gunning, Alex and Olsson, Caroline Falkman and Denain, Jean-Stanislas and Ho, Anson and Santos, Emily de Oliveira and others},
  journal={arXiv preprint arXiv:2411.04872},
  year={2024}
}

@article{davis2023benchmarks,
  title={Benchmarks for automated commonsense reasoning: A survey},
  author={Davis, Ernest},
  journal={ACM Computing Surveys},
  volume={56},
  number={4},
  pages={1--41},
  year={2023},
  publisher={ACM New York, NY, USA}
}

@article{orgad2024llms,
  title={Llms know more than they show: On the intrinsic representation of llm hallucinations},
  author={Orgad, Hadas and Toker, Michael and Gekhman, Zorik and Reichart, Roi and Szpektor, Idan and Kotek, Hadas and Belinkov, Yonatan},
  journal={arXiv preprint arXiv:2410.02707},
  year={2024}
}

@article{ghandeharioun2024patchscope,
  title={Patchscope: A unifying framework for inspecting hidden representations of language models},
  author={Ghandeharioun, Asma and Caciularu, Avi and Pearce, Adam and Dixon, Lucas and Geva, Mor},
  journal={arXiv preprint arXiv:2401.06102},
  year={2024}
}

@article{garcia2024extracting,
  title={Extracting Interpretable Task-Specific Circuits from Large Language Models for Faster Inference},
  author={Garc{\'\i}a-Carrasco, Jorge and Mat{\'e}, Alejandro and Trujillo, Juan},
  journal={arXiv preprint arXiv:2412.15750},
  year={2024}
}

@article{fan2024not,
  title={Not all layers of llms are necessary during inference},
  author={Fan, Siqi and Jiang, Xin and Li, Xiang and Meng, Xuying and Han, Peng and Shang, Shuo and Sun, Aixin and Wang, Yequan and Wang, Zhongyuan},
  journal={arXiv preprint arXiv:2403.02181},
  year={2024}
}

@article{touvron2023llama,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}

@article{jiang2023mistral,
  title={Mistral 7B},
  author={Jiang, Albert Q and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lucile and others},
  journal={arXiv preprint arXiv:2310.06825},
  year={2023}
}

@article{team2024gemma,
  title={Gemma: Open models based on gemini research and technology},
  author={Team, Gemma and Mesnard, Thomas and Hardin, Cassidy and Dadashi, Robert and Bhupatiraju, Surya and Pathak, Shreya and Sifre, Laurent and Rivi{\`e}re, Morgane and Kale, Mihir Sanjay and Love, Juliette and others},
  journal={arXiv preprint arXiv:2403.08295},
  year={2024}
}

@article{abdin2024phi,
  title={Phi-3 technical report: A highly capable language model locally on your phone},
  author={Abdin, Marah and Aneja, Jyoti and Awadalla, Hany and Awadallah, Ahmed and Awan, Ammar Ahmad and Bach, Nguyen and Bahree, Amit and Bakhtiari, Arash and Bao, Jianmin and Behl, Harkirat and others},
  journal={arXiv preprint arXiv:2404.14219},
  year={2024}
}

@article{soykan2024linguistically,
  title={Linguistically-Informed Multilingual Instruction Tuning: Is There an Optimal Set of Languages to Tune?},
  author={Soykan, G{\"u}rkan and {\c{S}}ahin, G{\"o}zde G{\"u}l},
  journal={arXiv preprint arXiv:2410.07809},
  year={2024}
}

@article{aryabumi2024aya,
  title={Aya 23: Open weight releases to further multilingual progress},
  author={Aryabumi, Viraat and Dang, John and Talupuru, Dwarak and Dash, Saurabh and Cairuz, David and Lin, Hangyu and Venkitesh, Bharat and Smith, Madeline and Campos, Jon Ander and Tan, Yi Chern and others},
  journal={arXiv preprint arXiv:2405.15032},
  year={2024}
}

@article{ustun2024aya,
  title={Aya model: An instruction finetuned open-access multilingual language model},
  author={{\"U}st{\"u}n, Ahmet and Aryabumi, Viraat and Yong, Zheng-Xin and Ko, Wei-Yin and D'souza, Daniel and Onilude, Gbemileke and Bhandari, Neel and Singh, Shivalika and Ooi, Hui-Lee and Kayid, Amr and others},
  journal={arXiv preprint arXiv:2402.07827},
  year={2024}
}

@article{nikankin2024arithmetic,
  title={Arithmetic without algorithms: Language models solve math with a bag of heuristics},
  author={Nikankin, Yaniv and Reusch, Anja and Mueller, Aaron and Belinkov, Yonatan},
  journal={arXiv preprint arXiv:2410.21272},
  year={2024}
}

@article{zhou2024pre,
  title={Pre-trained Large Language Models Use Fourier Features to Compute Addition},
  author={Zhou, Tianyi and Fu, Deqing and Sharan, Vatsal and Jia, Robin},
  journal={arXiv preprint arXiv:2406.03445},
  year={2024}
}

@article{burns2022discovering,
  title={Discovering latent knowledge in language models without supervision},
  author={Burns, Collin and Ye, Haotian and Klein, Dan and Steinhardt, Jacob},
  journal={arXiv preprint arXiv:2212.03827},
  year={2022}
}

@article{singh2024rethinking,
  title={Rethinking interpretability in the era of large language models},
  author={Singh, Chandan and Inala, Jeevana Priya and Galley, Michel and Caruana, Rich and Gao, Jianfeng},
  journal={arXiv preprint arXiv:2402.01761},
  year={2024}
}

@inproceedings{golgoon2024mechanistic,
  title={Mechanistic interpretability of large language models with applications to the financial services industry},
  author={Golgoon, Ashkan and Filom, Khashayar and Ravi Kannan, Arjun},
  booktitle={Proceedings of the 5th ACM International Conference on AI in Finance},
  pages={660--668},
  year={2024}
}

@article{chowdhury2024probing,
  title={Probing Ranking LLMs: Mechanistic Interpretability in Information Retrieval},
  author={Chowdhury, Tanya and Allan, James},
  journal={arXiv preprint arXiv:2410.18527},
  year={2024}
}

@article{rai2024practical,
  title={A practical review of mechanistic interpretability for transformer-based language models},
  author={Rai, Daking and Zhou, Yilun and Feng, Shi and Saparov, Abulhair and Yao, Ziyu},
  journal={arXiv preprint arXiv:2407.02646},
  year={2024}
}

@article{dubey2024llama,
  title={The llama 3 herd of models},
  author={Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024}
}