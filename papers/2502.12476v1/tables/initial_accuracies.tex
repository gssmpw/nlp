% \begin{table}[t]
% \resizebox{\columnwidth}{!}{ 
% \centering
% \begin{tabular}{lccc}
% \toprule
% \textbf{Language} & \textbf{SFT} & \textbf{Pretrained} & \(\Delta\) \\
% \midrule
% English & 38.44 & 13.27 & +25.17 \\
% German & 40.34 & 7.16 & +33.18 \\
% French & 40.27 & 11.3 & +28.97 \\
% Italian & 41.58 & 7.06 & +34.52 \\
% Portuguese & 38.23 & 5.38 & +32.85 \\ 
% Hindi & 21.18 & 5.27 & +15.91 \\ 
% Spanish & 41.71 & 6.13 & +35.58 \\
% \bottomrule
% \end{tabular}
% }
% \caption{Accuracy improvement (\(\Delta\)) of the finetuned model over the pretrained model across different languages.}
% \label{tab:initial_acc}
% \end{table}

% \begin{table*}[t]
% \centering
% \resizebox{2\columnwidth}{!}{ 
% \begin{tabular}{lccccccc}
% \toprule
% \textbf{Model} & \textbf{English} & \textbf{German} & \textbf{French} & \textbf{Italian} & \textbf{Portuguese} & \textbf{Hindi} & \textbf{Spanish} \\
% \midrule
% Llama-1B (Pretrained) & 13.27 & 7.16 & 11.30 & 7.06 & 5.38 & 5.27 & 6.13 \\
% Llama-1B (SFT) & 38.44 & 40.34 & 40.27 & 41.58 & 38.23 & 21.18 & 41.71 \\
% Llama-1B (\(\Delta\)) & +25.17 & +33.18 & +28.97 & +34.52 & +32.85 & +15.91 & +35.58 \\
% \midrule
% Llama-3B (Pretrained) & 32.85 & 23.79 & 22.90 & 21.87 & 20.06 & 7.33 & 22.01 \\
% Llama-3B (SFT) & 53.09 & 48.10 & 43.80 & 42.73 & 37.04 & 30.39 & 45.69 \\
% Llama-3B (\(\Delta\)) & +20.24 & +24.31 & +20.90 & +20.86 & +16.98 & +23.06 & +23.68 \\
% \bottomrule
% \end{tabular}
% }
% \caption{Accuracy improvement (\(\Delta\)) of the finetuned models (SFT) over the pretrained models across different languages for Llama-1B and Llama-3B.}
% \label{tab:initial_acc}
% \end{table*}


% \begin{table*}[t]
% \centering
% % \renewcommand{\arraystretch}{1.5} % Improve row spacing
% \resizebox{2\columnwidth}{!}{ 
% \begin{tabular}{clccccccc}
% \toprule
% \rotatebox{90}{} & \textbf{Model} & \textbf{English} & \textbf{German} & \textbf{French} & \textbf{Italian} & \textbf{Portuguese} & \textbf{Hindi} & \textbf{Spanish} \\ 
% \midrule
% \multirow{3}{*}{\rotatebox{90}{1B}} & Pretrained & 13.27 & 7.16 & 11.30 & 7.06 & 5.38 & 5.27 & 6.13 \\
%                                     & SFT        & 38.44 & 40.34 & 40.27 & 41.58 & 38.23 & 21.18 & 41.71 \\
%                                     & (\(\Delta\)) & +25.17 & +33.18 & +28.97 & +34.52 & +32.85 & +15.91 & +35.58 \\
% \midrule
% \multirow{3}{*}{\rotatebox{90}{3B}} & Pretrained & 32.85 & 23.79 & 22.90 & 21.87 & 20.06 & 7.33 & 22.01 \\
%                                     & SFT        & 53.09 & 48.10 & 43.80 & 42.73 & 37.04 & 30.39 & 45.69 \\
%                                     & (\(\Delta\)) & +20.24 & +24.31 & +20.90 & +20.86 & +16.98 & +23.06 & +23.68 \\
% \bottomrule
% \end{tabular}
% }
% \caption{Accuracy improvement (\(\Delta\)) of the finetuned models (SFT) over the pretrained models across different languages for Llama-1B and Llama-3B.}
% \label{tab:initial_acc}
% \end{table*}


% \begin{table}[t]
% \centering
% \resizebox{0.95\columnwidth}{!}{ 
% \begin{tabular}{lccccccccc}
% \toprule
%  & \multicolumn{2}{c}{\textbf{1B}} & \multicolumn{2}{c}{\textbf{3B}} \\ 
% \cmidrule(lr){2-3} \cmidrule(lr){4-5}
% \textbf{Language} & PLM & SFT  & PLM & SFT \\ 
% \midrule
% English     & 13.27 & 38.44 & 32.85 & 53.09 \\
% German      & 7.16  & 40.34  & 23.79 & 48.10  \\
% French      & 11.30 & 40.27  & 22.90 & 43.80  \\
% Italian     & 7.06  & 41.58  & 21.87 & 42.73  \\
% Portuguese  & 5.38  & 38.23  & 20.06 & 37.04  \\
% Hindi       & 5.27  & 21.18  & 7.33  & 30.39  \\
% Spanish     & 6.13  & 41.71  & 22.01 & 45.69  \\
% \bottomrule
% \end{tabular}
% }
% \caption{Accuracy improvement (\(\Delta\)) of the finetuned models (SFT) over the pretrained models across different languages for Llama-1B and Llama-3B.}
% \label{tab:initial_acc}
% \end{table}

% \begin{table*}[t]
% \centering
% \resizebox{0.8\textwidth}{!}{ 
% \begin{tabular}{lccccccccc}
% \toprule
%  & \multicolumn{3}{c}{\textbf{1B}} & \multicolumn{3}{c}{\textbf{3B}} & \multicolumn{3}{c}{\textbf{8B}} \\ 
% \cmidrule(lr){2-4} \cmidrule(lr){5-7} \cmidrule(lr){8-10}
% \textbf{Language} & PLM & SFT & $\Delta$ & PLM & SFT & $\Delta$ & PLM & SFT & $\Delta$ \\ 
% \midrule
% English     & 13.27 & 38.44 & +25.17 & 32.85 & 53.09 & +20.24 & 12.92 & 50.98 & +38.06 \\
% French      & 11.30 & 40.27 & +28.97 & 22.90 & 43.80 & +20.90 & 18.53 & 50.85 & +32.32 \\
% German      & 7.16  & 40.34 & +33.18 & 23.79 & 48.10 & +24.31 & 11.04 & 44.35 & +33.31 \\
% Hindi       & 5.27  & 21.18 & +15.91 & 7.33  & 30.39 & +23.06 & 6.21  & 35.29 & +29.08 \\
% Italian     & 7.06  & 41.58 & +34.52 & 21.87 & 42.73 & +20.86 & 16.48 & 43.22 & +26.74 \\
% Portuguese  & 5.38  & 38.23 & +32.85 & 20.06 & 37.04 & +16.98 & 18.38 & 31.11 & +12.73 \\
% Spanish     & 6.13  & 41.71 & +35.58 & 22.01 & 45.69 & +23.68 & 16.60 & 45.46 & +28.86 \\
% \bottomrule
% \end{tabular}
% }
% \caption{Accuracy improvement ($\Delta$) of the fine-tuned models (SFT) over the pretrained models across different languages for Llama-1B, Llama-3B, and Llama-8B.}
% \label{tab:initial_acc}
% \end{table*}

\begin{table}[t]
\centering
\resizebox{\columnwidth}{!}{ 
\begin{tabular}{lcccccc}
\toprule
 & \multicolumn{2}{c}{\textbf{Llama-1B}} & \multicolumn{2}{c}{\textbf{Llama-3B}} & \multicolumn{2}{c}{\textbf{Llama-8B}} \\ 
\cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7}
\textbf{Language} & PLM & SFT & PLM & SFT & PLM & SFT \\ 
\midrule
English     & 13.27 & 38.44 & 32.85 & 53.09 & 12.92 & 50.98 \\
French      & 11.30 & 40.27 & 22.90 & 43.80 & 18.53 & 50.85 \\
German      & 7.16  & 40.34 & 23.79 & 48.10 & 11.04 & 44.35 \\
Hindi       & 5.27  & 21.18 & 7.33  & 30.39 & 6.21  & 35.29 \\
Italian     & 7.06  & 41.58 & 21.87 & 42.73 & 16.48 & 43.22 \\
Portuguese  & 5.38  & 38.23 & 20.06 & 37.04 & 18.38 & 31.11 \\
Spanish     & 6.13  & 41.71 & 22.01 & 45.69 & 16.60 & 45.46 \\
\bottomrule
\end{tabular}
}
\caption{Performance of pre-trained (PLM) and fine-tuned (SFT) models across different languages.}
\label{tab:initial_acc}
\end{table}
