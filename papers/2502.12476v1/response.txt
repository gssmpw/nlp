\section{Related Work}
This work builds on several active research areas that inform our study of multilingual task learning in LLMs. Specifically, we draw from (1) Multilingual interpretability, which helps us analyze how LLMs process different languages and how their internal structures influence multilingual task learning; (2) Representation alignment, which provides insights into token-level similarities across languages and how shared representations facilitate cross-lingual generalization; (3) Adapters, which separate language knowledge from task-specific knowledge, offering a structured framework for understanding their interactions; and (4) Subnetworks, which identify task- and language-specific parameters within existing models, offering an alternative to external adapters and directly informing our approach to efficient partial training.  


% interpretability - put the wierdo paper here
\paragraph{Interpretability.}
Adham Dahy, "Analyzing Multilingual Transformers with Probing Techniques" examine probing techniques to analyze accuracy changes across layers in LLMs, showing that high-resource languages exhibit patterns similar to English, with accuracy increasing from lower to upper layers. However, this pattern is inconsistent for low-resource languages. Wang et al., "Cross-Lingual Transfer Learning with Neuron Overlap Analysis" examine cross-lingual transfer by analyzing neuron overlap in different languages using checkpoints from BLOOM’s pre-training _____. Their study finds a strong correlation between neuron overlap and cross-lingual transfer, though neuron overlap does not increase monotonically during training, and patterns vary across model sizes. Similarly, Zhang et al., "Assessing Language-Specific Neurons in Multilingual Transformers" investigate language-specific neurons and assess how masking these neurons affects both English and non-English language performance.   


% alignment
\paragraph{Representation alignment.}

Beyond studying multilingualism in LLMs, some research focuses on improving model performance across languages through representation alignment. Hassan et al., "Bilingual Dictionary-Based Alignment for Multilingual Transformers" align English and Arabic model representations using a bilingual dictionary before fine-tuning on a target task. Sajjad et al., "Question Translation Data-Driven Representation Alignment for Cross-Lingual Transfer" align English representations with other languages using question-translation data before instruction-tuning. Additionally, Zhang, “Pixel Representation Method for Enhanced Multilingual Alignment” introduce a pixel representation method to enhance alignment and improve translation quality.   



% adapters
\paragraph{Adapters.}

Another approach for cross-lingual transfer involves integrating adapters into the model. This technique is based on the assumption that task-solving knowledge can be separated from language knowledge. Wang et al., "Multilingual Adapters with X-Ray Vision: A Framework for Efficient Cross-Lingual Transfer" introduce MAD-X, a framework where language and task adapters are trained separately, with each block’s representations passing through a language adapter before a task adapter. Building on this, later works aim to refine adapter creation and composition methods. For instance, Zhang et al., "Bilingual Adapters for Efficient Cross-Lingual Transfer" propose BAD-X, which replaces monolingual adapters with bilingual adapters, improving performance for low-resource languages.  Sajjad et al., "AdaMergeX: Adaptive Merge of Language-Task Pairs for Multilingual Transformers” introduce AdaMergeX, where adapters for language-task pairs are trained independently and later combined through linear operations (addition and subtraction) to generate adapters for new language-task pairs.    



% subnetworks 
\paragraph{Subnetworks.}

To enhance cross-lingual transfer without adding new parameters, some methods focus on identifying existing task- and language-specific parameters within the model. Liu et al., "Multilingual Subnetwork Extraction with Fine-Tuning" fine-tune models for specific languages or tasks, extract the most affected neurons, and use the resulting subnetworks to enable multilingual task adaptation. Sajjad et al., “Layer-Specific Analysis for Efficient Cross-Lingual Transfer” take a layer-wise approach in multiple steps: they train separate language- and task-expert models, analyze parameter changes to identify key layers for language and task learning, and use layer-swapping techniques to create a math expert in a new language. Consistent with Liu et al., “Layer-Specific Analysis for Efficient Cross-Lingual Transfer”, their findings suggest that initial and final layers primarily encode language-related information, while middle layers are task-specific.