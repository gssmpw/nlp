\section{Related Work}
This work builds on several active research areas that inform our study of multilingual task learning in LLMs. Specifically, we draw from (1) Multilingual interpretability, which helps us analyze how LLMs process different languages and how their internal structures influence multilingual task learning; (2) Representation alignment, which provides insights into token-level similarities across languages and how shared representations facilitate cross-lingual generalization; (3) Adapters, which separate language knowledge from task-specific knowledge, offering a structured framework for understanding their interactions; and (4) Subnetworks, which identify task- and language-specific parameters within existing models, offering an alternative to external adapters and directly informing our approach to efficient partial training.  


% interpretability - put the wierdo paper here
\paragraph{Interpretability.}
____ use probing techniques to analyze accuracy changes across layers in LLMs, showing that high-resource languages exhibit patterns similar to English, with accuracy increasing from lower to upper layers. However, this pattern is inconsistent for low-resource languages. ____ examine cross-lingual transfer by analyzing neuron overlap in different languages using checkpoints from BLOOM’s pre-training ____. Their study finds a strong correlation between neuron overlap and cross-lingual transfer, though neuron overlap does not increase monotonically during training, and patterns vary across model sizes. Similarly, ____ investigate language-specific neurons and assess how masking these neurons affects both English and non-English language performance.   


% alignment
\paragraph{Representation alignment.}

Beyond studying multilingualism in LLMs, some research focuses on improving model performance across languages through representation alignment. ____ align English and Arabic model representations using a bilingual dictionary before fine-tuning on a target task. ____ align English representations with other languages using question-translation data before instruction-tuning. Additionally, ____ introduce a pixel representation method to enhance alignment and improve translation quality.   



% adapters
\paragraph{Adapters.}

Another approach for cross-lingual transfer involves integrating adapters into the model. This technique is based on the assumption that task-solving knowledge can be separated from language knowledge. ____ introduce MAD-X, a framework where language and task adapters are trained separately, with each block’s representations passing through a language adapter before a task adapter. Building on this, later works aim to refine adapter creation and composition methods. For instance, ____ propose BAD-X, which replaces monolingual adapters with bilingual adapters, improving performance for low-resource languages. ____ introduce AdaMergeX, where adapters for language-task pairs are trained independently and later combined through linear operations (addition and subtraction) to generate adapters for new language-task pairs.    



% subnetworks 
\paragraph{Subnetworks.}

To enhance cross-lingual transfer without adding new parameters, some methods focus on identifying existing task- and language-specific parameters within the model. ____ fine-tune models for specific languages or tasks, extract the most affected neurons, and use the resulting subnetworks to enable multilingual task adaptation. ____ take a layer-wise approach in multiple steps: they train separate language- and task-expert models, analyze parameter changes to identify key layers for language and task learning, and use layer-swapping techniques to create a math expert in a new language. Consistent with ____, their findings suggest that initial and final layers primarily encode language-related information, while middle layers are task-specific.