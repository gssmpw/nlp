% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[preprint]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.
\usepackage{color,soul}
\usepackage{booktabs}
\usepackage{lipsum}  
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{colortbl}
\usepackage{xcolor}
\usepackage{textcomp}  % Required for encoding \textbigcircle
\usepackage{scalerel}  % Required for emoji \scalerel
% \def\cocacolaemoji{\scalerel*{\includegraphics{figures/soda-bottle-2.pdf}}{\textrm{\textbigcircle}}}
\usepackage{todonotes} 
\usepackage{cleveref}
\usepackage{arydshln}

\newcommand{\atodo}[2][]{\todo[color=cyan!20,size=\scriptsize,#1]{old: #2}}
\newcommand{\etodo}[2][]{\todo[color=yellow!20,size=\scriptsize,#1]{Elnaz: #2}}

\def\cocacolaemoji{\raisebox{-0.13em}{\includegraphics[height=1.4em]{figures/soda-bottle-2.pdf}}}

\title{\cocacolaemoji CoCo-CoLa: Evaluating Language Adherence in Multilingual LLMs}



\author{Elnaz Rahmati\thanks{Equal contribution.} \And Alireza S. Ziabari\footnotemark[1] \And Morteza Dehghani \AND
        \normalfont \small University of Southern California \\
        \texttt{\small\{erahmati, salkhord, mdehghan\}@usc.edu}}


\begin{document}
\maketitle
\begin{abstract}
Multilingual Large Language Models (LLMs) develop cross-lingual abilities despite being trained on limited parallel data. However, they often struggle to generate responses in the intended language, favoring high-resource languages such as English. In this work, we introduce \emph{CoCo-CoLa} (Correct Concept - Correct Language), a novel metric to evaluate language adherence in multilingual LLMs. Using fine-tuning experiments on a closed-book QA task across seven languages, we analyze how training in one language affects others' performance. Our findings reveal that multilingual models share task knowledge across languages but exhibit biases in the selection of output language. We identify language-specific layers, showing that final layers play a crucial role in determining output language. Accordingly, we propose a partial training strategy that selectively fine-tunes key layers, improving language adherence while significantly reducing computational cost. Our method achieves comparable or superior performance to full fine-tuning, particularly for low-resource languages, offering a more efficient multilingual adaptation.\footnote{Our code is available at \href{https://github.com/elnazrahmati/CoCo-CoLa/}{https://github.com/elnaz rahmati/CoCo-CoLa/}}

\end{abstract}

\section{Introduction}
% LLMs show signs of cross-lingual transfer and translation abilities without explicit training on parallel data but there is little work on how knowledge aquisition happens accross languages during training.

% sparse training and adapter training is used for cross-lingual transfer, and better cross-lingual continual learning. 

% we aim to understand how knowledge aquisistion works for LLMs during supervised finetuning accross languages, the insight can be used for better cross-lingual transfer specially for low resource languages. 

% What are you trying to do? Articulate your objectives using absolutely no jargon. 
% How is it done today, and what are the limits of current practice? 
% What is new in your approach and why do you think it will be successful? 
% Who cares? If you are successful, what difference will it make? 
Multilingual LLMs are pre-trained on raw text from multiple languages, typically consisting of separate corpora for each language. Remarkably, despite this lack of explicit parallel data to facilitate cross-lingual associations, these models develop an implicit understanding of inter-language relations and cross-lingual word associations  \citep{wen-yi-mimno-2023-hyperpolyglot}. Instruction tuning further enhances their ability to follow prompts, and models trained on multilingual data often exhibit zero-shot cross-lingual transfer of instruction-following capabilities \citep{chirkova-nikoulina-2024-zero-shot}. However, this generalization is uneven: while high-resource languages in pretraining benefit significantly from instruction tuning, lower-resource or unseen languages often struggle to follow instructions reliably, frequently exhibiting degraded performance or defaulting to generating output in a preferred language \citep{nguyen-etal-2024-democratizing,chirkova-nikoulina-2024-zero-shot}.  To address these issues, we investigate how multilingual LLMs learn the same task across different languages.

\begin{figure}
    \centering
    \includegraphics[width=0.8\linewidth]{figures/main_plot.png}
    \caption{Evaluation of correctness and language adherence on French input. Soda level represents the CoCo-CoLa ratio. Higher level indicates better adherence.}
    %The amount of soda represents the CoCo-CoLa ratio, with a higher ratio indicating better language adherence.
    \label{fig:enter-label}
\end{figure}

% first interepretability in general
A crucial step toward addressing the limitations of multilingual LLMs is understanding how they internally process and encode multilingual knowledge. Interpretability research has traditionally focused on monolingual models, leveraging techniques such as representation probing \citep{orgad2024llms,saphra-lopez-2019-understanding} and model patching \citep{ghandeharioun2024patchscope,garcia2024extracting}. These methods have been widely used to examine LLMs’ performance across tasks such as mathematics \citep{nikankin2024arithmetic,zhou2024pre}, and general knowledge \citep{jiang-etal-2024-large,burns2022discovering,singh2024rethinking,golgoon2024mechanistic,chowdhury2024probing,rai2024practical}. Studies on model internals suggest that Multi-Layer Perceptrons (MLPs) retrieve task-relevant information, while attention layers refine and promote the correct response \citep{geva-etal-2021-transformer, meng2022locating}. Furthermore, knowledge is often identified in earlier layers and reinforced in later layers \citep{fan2024not}.


% multilinguality in LLMs
However, these interpretability techniques have primarily been applied to monolingual models, which were initially dominant due to the early focus on English-language pertaining \cite{touvron2023llama,jiang2023mistral,team2024gemma,abdin2024phi}. The rise of multilingual LLMs trained on diverse languages \citep{gao-etal-2024-multilingual, shaham-etal-2024-multilingual, soykan2024linguistically}, necessitates extending interpretability research beyond English. Multilingual LLMs present additional challenges: representations of different languages are intertwined within a shared space; cross-lingual alignment varies across languages; and shared tokens between languages impact their process. These complexities make it difficult to isolate language-specific knowledge, benchmark cross-lingual generalization, and interpret how multilingual LLMs acquire and apply linguistic information. Given the prevalence of mid- and low-resource languages, understanding these mechanisms is crucial not only for improving cross-lingual transfer but also for mitigating the ``curse of multilinguality'' — the performance degradation observed as the number of supported languages increases.


% multilingualism 
Recent efforts have begun addressing these challenges by probing internal multilingual representations \citep{li2024exploring}, analyzing the emergence of cross-lingual transfer \citep{wang-etal-2024-probing-emergence}, and studying token representation alignment on cross-lingual transfer \citep{gaschi-etal-2023-exploring}. Furthermore, researchers attempt to separate the linguistic abilities from task-specific abilities by developing language- and task-specific adapters \citep{pfeiffer-etal-2020-mad,parovic-etal-2023-cross}, subnetworks \citep{choenni-etal-2023-cross}, or layers \citep{bandarkar2024layer}. However, despite this progress, most prior works treat multilinguality as a monolithic phenomenon, focusing on general cross-lingual transfer or aggregating all languages into a single block of linguistic knowledge. Less attention has been given to understanding how LLMs process individual languages at a more granular level, particularly within the context of task learning.

In this work, our goal is to identify both shared and distinct patterns in cross-lingual task acquisition, revealing how multilingual models internalize and apply linguistic knowledge (\Cref{sec:preliminary}). We find that training on a task in one language improves performance in other languages. However, this benefit is not always directly observable due to an inherent model bias towards generating output in a preferred language, rather than strictly adhering to the input language (\Cref{sec:coca-cola-intro}). To quantify this bias, we introduce \emph{CoCo-CoLa} (Correct Concept, Correct Language), a novel metric designed to assess a model’s ability to generate responses in the intended input language, particularly for languages not included in supervised finetuning (SFT). Furthermore, we propose a \emph{partial training method} that selectively fine-tunes specific model layers to enhance language adherence (\Cref{sec:partial-training}). This approach enables more efficient multilingual adaptation, achieving comparable or even superior performance, especially for low-resource languages, without the need for full model retraining.


\section{Related Work}

This work builds on several active research areas that inform our study of multilingual task learning in LLMs. Specifically, we draw from (1) Multilingual interpretability, which helps us analyze how LLMs process different languages and how their internal structures influence multilingual task learning; (2) Representation alignment, which provides insights into token-level similarities across languages and how shared representations facilitate cross-lingual generalization; (3) Adapters, which separate language knowledge from task-specific knowledge, offering a structured framework for understanding their interactions; and (4) Subnetworks, which identify task- and language-specific parameters within existing models, offering an alternative to external adapters and directly informing our approach to efficient partial training.  


% interpretability - put the wierdo paper here
\paragraph{Interpretability.}
\citet{li2024exploring} use probing techniques to analyze accuracy changes across layers in LLMs, showing that high-resource languages exhibit patterns similar to English, with accuracy increasing from lower to upper layers. However, this pattern is inconsistent for low-resource languages. \citet{wang2024probing} examine cross-lingual transfer by analyzing neuron overlap in different languages using checkpoints from BLOOM’s pre-training \citep{le2023bloom}. Their study finds a strong correlation between neuron overlap and cross-lingual transfer, though neuron overlap does not increase monotonically during training, and patterns vary across model sizes. Similarly, \citet{zhao2024large} investigate language-specific neurons and assess how masking these neurons affects both English and non-English language performance.   


% alignment
\paragraph{Representation alignment.}

Beyond studying multilingualism in LLMs, some research focuses on improving model performance across languages through representation alignment. \citet{gaschi-etal-2023-exploring} align English and Arabic model representations using a bilingual dictionary before fine-tuning on a target task. \citet{zhang-etal-2024-getting} align English representations with other languages using question-translation data before instruction-tuning. Additionally, \citet{salesky-etal-2023-multilingual} introduce a pixel representation method to enhance alignment and improve translation quality.   



% adapters
\paragraph{Adapters.}

Another approach for cross-lingual transfer involves integrating adapters into the model. This technique is based on the assumption that task-solving knowledge can be separated from language knowledge. \citet{pfeiffer-etal-2020-mad} introduce MAD-X, a framework where language and task adapters are trained separately, with each block’s representations passing through a language adapter before a task adapter. Building on this, later works aim to refine adapter creation and composition methods. For instance, \citet{parovic-etal-2022-bad} propose BAD-X, which replaces monolingual adapters with bilingual adapters, improving performance for low-resource languages. \citet{zhao2024adamergex} introduce AdaMergeX, where adapters for language-task pairs are trained independently and later combined through linear operations (addition and subtraction) to generate adapters for new language-task pairs.    



% subnetworks 
\paragraph{Subnetworks.}

To enhance cross-lingual transfer without adding new parameters, some methods focus on identifying existing task- and language-specific parameters within the model. \citet{choenni-etal-2023-cross} fine-tune models for specific languages or tasks, extract the most affected neurons, and use the resulting subnetworks to enable multilingual task adaptation. \citet{bandarkar2024layer} take a layer-wise approach in multiple steps: they train separate language- and task-expert models, analyze parameter changes to identify key layers for language and task learning, and use layer-swapping techniques to create a math expert in a new language. Consistent with \citet{zhao2024large}, their findings suggest that initial and final layers primarily encode language-related information, while middle layers are task-specific.   



\section{Preliminary Analysis} \label{sec:preliminary}
In the preliminary section of this paper, we first isolate language effects from task learning by choosing multi-lingual parallel  QA data (\Cref{sec:preliminary-setup}), examining fine-tuning performance across multiple languages (\Cref{sec:preliminary-performance}), exploring how well LLMs generalize knowledge across languages (\Cref{sec:preliminary-shared-knowledge}), and which model components are most affected during training (\Cref{sec:parameter_update}). Then, in \Cref{sec:coca-cola-intro}, we introduce \textbf{CoCo-CoLa} metric to measure language adherence in multilingual LLMs followed by an efficient partial training method to increase the model adherence (\Cref{sec:partial-training}). 

\subsection{Setup} \label{sec:preliminary-setup}
To investigate how multilingual LLMs learn a new task in a monolingual setting, we train three different model sizes on a Closed-Book Question-Answering (CBQA) task. We include two sizes of the Llama-3.2 series \citep{dubey2024llama} to analyze the effect of model size on multilingual performance and behavior, given that these models are specifically optimized for multilingual dialogue. We also include Llama-3.1-8B as a point of comparison, as it, while not explicitly optimized for multilingualism, was trained on a small amount of multilingual data.  

We select CBQA because it is inherently language-dependent and demonstrates a model's ability to act as a knowledge base \citep{wang-etal-2021-generative}. To isolate the impact of language differences from the effects of learning a new task or acquiring new knowledge, we use the Mintaka CBQA dataset \citep{sen-etal-2022-mintaka}.  Mintaka provides identical question-answer pairs in nine languages, allowing us to keep the question content consistent and thus isolate the influence of language itself. The dataset was originally created in English and later translated into Arabic, French, German, Hindi, Italian, Japanese, Portuguese, and Spanish.  

One challenge with Mintaka is that some answer types are not translated across languages. To keep question-answer pairs within the same language, we use Google Translate to convert these answers into the language of their respective questions and apply back-translation for accuracy checks. Additionally, since our goal is to study how models learn new tasks in languages they have been exposed to before, we exclude Arabic and Japanese.

\subsection{SFT Performance}
\label{sec:preliminary-performance}
Our initial step is to assess the model's ability to learn the task in each individual language, effectively measuring how learning difficulty varies across languages. To do this, we perform SFT for Llama-3 models on each language of the CBQA dataset for three epochs and generate answers for given questions. Next, we select the best model based on the validation loss. Further implementation details are provided in \Cref{sec:implementation-detail}.
% , which occurs at the same training step for all languages except Hindi, where it requires 100 additional steps. 

\autoref{tab:initial_acc} shows a comparison of accuracy between the pre-trained model and the best checkpoint of the language-specific SFT model across different languages. SFT significantly improves performance for all languages with relatively consistent accuracy levels, except for Hindi in all model sizes and Portuguese for Llama-3.1-8B, which exhibit notably lower accuracy. This discrepancy is likely due to undertraining. Among the SFT models, English achieves the highest accuracy in the 3B (53.09\%) and 8B (50.98\%) models, while Spanish performs best (41.71\%) in the 1B model. The largest accuracy gains are observed in English (+38.06\%) for the 8B model, German (+24.31\%) for the 3B model, and Spanish (+35.58\%) for the 1B model, indicating that these languages benefited the most from fine-tuning. The similar accuracy across languages indicates comparable knowledge acquisition.  

However, two critical questions remain: (1) Do models share learned knowledge uniformly across languages, or do they correctly answer distinct subsets of questions depending on the language? (2) Are there specific parts of the model that are responsible for encoding language-specific information?

To address these questions, we first analyze the overlap in correct answers across languages using the Jaccard Index, followed by an investigation of parameter updates to determine whether certain components of the model specialize in handling linguistic differences.
\input{tables/initial_accuracies}

\input{figures/jaccard_index}

\subsection{Cross-lingual Task Knowledge} \label{sec:preliminary-shared-knowledge}
To further investigate the extent of cross-lingual task knowledge transfer within the model, we analyze the overlap in correct answers across languages. Specifically, we measure how consistently the model arrives at the same correct answers in different languages, providing insight into whether knowledge is shared across languages.

It is important to note that there is no overlap between the knowledge present in the training and evaluation data. This ensures that any correct answers during evaluation are derived from knowledge acquired during pretraining rather than memorization. Consequently, the model’s ability to generate correct responses across languages indicates that it has internalized the underlying task knowledge from the training data, rather than relying solely on language-specific cues. Let \( L_A \) and \( L_B \) represent two languages, and let \( C_{L_A} \) denote the set of correct answers for \( L_A \). To quantify the degree of shared task knowledge between languages, we compute the Jaccard Index, also known as Intersection over Union (IoU), between \( C_{L_A} \) and \( C_{L_B} \) (see \autoref{eq:overlap}). The Jaccard Index is a natural choice for this analysis as it directly measures the proportion of overlapping correct answers relative to the total distinct answers across languages. This allows us to assess knowledge consistency and cross-lingual transfer within the model.
\vspace{-1mm}
\begin{equation}
    IoU(A, B) = \frac{|C_{L_A} \cap C_{L_B}|}{|C_{L_A} \cup C_{L_B}|}
    \label{eq:overlap}
\end{equation}

The results, shown in \autoref{fig:overlap}, indicate that on average approximately 60\% of correctly answered questions are shared across languages for all models, suggesting a strong degree of shared knowledge among languages. However, Hindi exhibits significantly lower overlap with other languages in Llama-3.2 models, suggesting weaker generalization for this language. Interestingly, in Llama-3.1-8B, Hindi shows higher overlap compared to Llama-3.2 models, but Portuguese experiences a notable drop in overlap. Additionally, Llama-3.2-3B demonstrates a higher rate of shared knowledge compared to Llama-3.1-8B, despite both models achieving comparable accuracy across languages (see \autoref{tab:initial_acc}). This highlights the importance of multilingual optimization in enhancing cross-lingual transfer among languages.   


\subsection{Parameter Updates}
\label{sec:parameter_update}
To investigate language-specific encoding in LLMs, we analyze parameter updates during fine-tuning and compare them across languages to determine whether certain components of the model specialize in processing linguistic information.  \citet{meng2022locating} suggest that MLP modules primarily store knowledge, while attention modules control information retrieval and selection. SFT models correctly answer approximately 40\% of evaluation questions in all languages. However, they require fine-tuning to improve their ability to select and output the correct information. As a result, we expect substantial modifications in the attention modules, particularly in the final layers, while changes in the MLP modules remain limited. Since these datasets differ only in language, not in task or knowledge, analyzing the model updates allows us to pinpoint which layers or components are most crucial for learning language-specific representations. 

To compute parameter update,  we follow \citet{bandarkar2024layer} and calculate the average parameter modifications for each module in each layer. Denoting the pre-trained weight matrix as \( W_p \) and the fine-tuned weight matrix as \( W_f \), the average magnitude of differences is given by: 
\vspace{-2mm}
\begin{equation}
\Delta W = \frac{1}{n} \sum_{i=1}^{n} | W_p^{(i)} - W_f^{(i)} |
\label{eq:model-diff}
\vspace{-2mm}
\end{equation}

\input{figures/heatmap}

The results for four languages are shown in \autoref{fig:model-diff}, with the remaining three languages in \autoref{fig:model-diff-appendix}. As expected, significant modifications occur in the attention modules of the final six layers for Llama-3.2-1B and the final 14 layers for Llama-3.2-3B and Llama-3.1-8B models across all languages. However, in Llama-3.2 models, we observe substantial changes in the MLP modules in these layers for all languages except English, suggesting that these variations are tied to language-specific processing rather than task-related learning. Surprisingly, for Llama-3.1-8B, even the model fine-tuned on English shows a high rate of change similar to other languages. Considering the unexpectedly low accuracy of the Llama-3.1-8B pre-trained model across all languages compared to Llama-3.2-3B, this larger modification could be related to learning the task or acquiring new knowledge rather than just language adaptation.
%remains unclear why the MLP modules change in non-English languages but not in English. 

\section{Approach}
Our previous analysis suggests that while task knowledge is largely shared across languages, the \emph{way} this knowledge is processed and accessed differs. Although a Jaccard Index analysis revealed substantial overlap in correct answers, our investigation of parameter updates showed that models trained on non-English languages required more substantial modifications in their MLP modules compared to English, even when achieving comparable accuracy. This raises an important question: Do these modifications reflect deviations in knowledge acquisition, or are they more related to language generation? In this section, we first  introduce a metric to analyze linguistic bias in multilingual LLM outputs. Then, we propose a partial training strategy aimed at reducing this bias by selectively fine-tuning specific model components.

\subsection{Correct Concept in Correct Language} \label{sec:coca-cola-intro}
According to \citet{dubey2024llama}, only 8\% of the pre-training data used for Llama-3 models is multilingual, while the rest is dominated by English general knowledge, mathematics, and code. This suggests a strong bias toward English. Given this imbalance, we hypothesize that the observed MLP module changes in non-English languages may not indicate new knowledge acquisition but rather adjustment in language selection during response generation. Supporting this, \citet{chirkova-nikoulina-2024-zero-shot} found that when Llama-2-13B is instruction-tuned in English and tested in other languages, it generates responses in a different language from input language in over 30\% of cases, with this behavior influenced by training hyperparameters.  

To investigate this further, we introduce \textbf{CoCo-CoLa} (\textbf{Co}rrect \textbf{Co}ncept - \textbf{Co}rrect \textbf{La}nguage), a metric designed to measure how well the model adheres to the input language while generating correct responses. Let \( L_{i} \) denote the input language, \( C_{L_{i} \rightarrow L_{o}} \) the set of correct output in language \( L_{o} \) when passing language \( L_{i} \) as input. We define the CoCo-CoLa ratio as follows:  

% \vspace{-2mm}
\begin{equation}
\begin{split}
    \text{CoCo-CoLa}(L_{i}) & \\
    & \hspace{-2em} = \frac{|C_{L_{i}\rightarrow L_{i}} - \bigcup\limits_{L_{o}\neq L_{i}}C_{L_{i}\rightarrow L_{o}}|}
    {|C_{L_{i}\rightarrow L_{i}} ~ \Delta  \bigcup\limits_{L_{o}\neq L_{i}}C_{L_{i}\rightarrow L_{o}}|}
    \label{eq:coca-cola}
\end{split}
\end{equation}

\input{tables/coca-cola-ratios}

The denominator uses the symmetric difference between \( C_{L_{i}\rightarrow L_{i}} \) and correct answers in other languages because many answers involve named entities, such as well-known places, books, and individuals. Since most of the languages, use similar scripts, named entities often appear in identical forms across multiple languages. This redundancy leads to overlap between \( C_{L_{i} \rightarrow L_{i}} \) and \( \bigcup_{L_{o}\neq L_{i}}C_{L_{i}\rightarrow L_{o}} \), which the symmetric difference helps mitigate by ensuring that shared named entities do not artificially inflate the metric.

Given that these models are primarily trained on English, when the input is in \( L_{i} \) the output is usually either \( L_{i} \) or English. Thus, \( \bigcup_{L_{o}\neq L_{i}}C_{L_{i}\rightarrow L_{o}} \) is largely dominated by \(C_{L_{i}\rightarrow en}\), meaning that most language switching occurs between the input language and English rather than other languages.

To further simplify the calculation, we filter the data to include only questions where the correct answers in \( L_{i} \) and English are different. Under this condition, \( C_{L_{i} \rightarrow L_{i}} \cap C_{L_{i} \rightarrow en} = \emptyset \), allowing the CoCo-CoLa ratio to reduce to:  

\vspace{-1mm}
\begin{equation}
    \text{CoCo-CoLa}(L_{i}) = \frac{|C_{L_{i}\rightarrow L_{i}}|}{|C_{L_{i}\rightarrow L_{i}}| + |C_{L_{i}\rightarrow en}|}
    \label{eq:coca-cola-simplified}
\end{equation}

To evaluate language adherence and accuracy, we pass the input in $L_{i}$ to pre-trained, \textit{en-tuned}, and  $L_{i}$\textit{-tuned} models. We then compute the CoCo-CoLa ratio and the cumulative accuracy, defined as the proportion of correct answers either in \( L_{i} \) or English. The results, presented in \autoref{tab:model_performance}, show that while the \textit{en-tuned} models and the $L_{i}$\textit{-tuned} models achieve similar cumulative accuracy on $L_{i}$ input, the CoCo-CoLa ratio is significantly lower for the \textit{en-tuned} model. This suggests that although the \textit{en-tuned} model can correctly process the question in $L_{i}$ and retrieve the correct answer at the same rate as the $L_{i}$\textit{-tuned} model, it frequently generates the answer in English instead of \( L_{i} \). Furthermore, analyzing the CoCo-CoLa ratio of the pre-trained model reveals that the model already exhibits a bias toward generating English responses, though this bias is less pronounced than in the \textit{en-tuned} model. These findings support our hypothesis that the varying rate of parameter updates across languages is related to output language preference. Since the model is already inherently biased toward English, \textit{en-tuned} results in the least change.

\input{figures/cocacola_barplot}

\subsection{Partial Training for Efficient Adaptation} \label{sec:partial-training}
In this section, we aim to disentangle task learning from output generation in language $L_{i}$. Our previous results reveal two key observations. First, as shown in \Cref{sec:coca-cola-intro}, both the \textit{en-tuned} model and the $L_{i}$\textit{-tuned} model achieve the same cumulative accuracy on $L_{i}$, indicating that they learn the task equally well. The only difference is their CoCo-CoLa score, meaning that while both models understand the task to the same degree, they generate outputs in different languages. Second, from \Cref{sec:parameter_update}, we observed that the \textit{en-tuned} and  $L_{i}$\textit{-tuned} models undergo different parameter updates. Some of these updates are necessary for learning the task itself, while others are specifically related to generating responses in the correct language.

Based on these observations, we hypothesize that fine-tuning specific layers of an \textit{en-tuned} model on $L_{i}$ can enable it to generate responses in  $L_{i}$  without requiring full model updates. Specifically, these layers correspond to the parameters that were updated in the  $L_{i}$\textit{-tuned} model but not in the \textit{en-tuned} model. To test this hypothesis, we first identify the layers that undergo language-specific updates. We then fine-tune only these layers in the \textit{en-tuned} model and compare the results to fine-tuning non-specific layers. This comparison allows us to isolate the parameters responsible for output language. 

\paragraph{Identifying language layers.} We select layers for partial training based on the variation in parameter update rates observed in \Cref{sec:parameter_update}. For the Llama-3.2-1B model, we train three variants by unfreezing different sets of layers: (1) layers 11-16, (2) layers 1-5 (chosen to match the parameter count of the final six layers), and layers 1-10 (including all parameters except the final six). We expect the first variant, which targets the final six layers, to be the most language-related and to result in the largest improvement in the CoCo-CoLa ratio, while the other two should have a smaller effect. Similarly, for Llama-3.2-3B, we train two variants by unfreezing layers 15–27 and layers 1–14, again expecting the first variant to be more strongly related language generation. For Llama-3.1-8B, which does not show clear variations in update rates across languages (as noted in \Cref{sec:parameter_update}), we instead select layers based on the most updated MLP modules. Specifically, we choose layers 16–31 and layers 1–15 for partial training to determine which part of the model is more responsible for language generation. Through this analysis, we aim to verify whether the final layers play a greater role in controlling the output language

\paragraph{Partial training evaluation.} To evaluate the effectiveness of partial training, we compare all partially trained models to both their fully \textit{en-tuned} and fully \(L_{i}\)\textit{-tuned} models. \autoref{fig:partial-training-full} presents cumulative accuracy and \(L_{i}\) accuracy across three languages, while results for the remaining three languages are included in \autoref{fig:partial-training-appendix}. In addition, CoCo-CoLa ratios for partially trained models are also available in \Cref{sec:partial-appendix}, providing further insight into the extent to which partial fine-tuning improves output language consistency. 

As shown in \autoref{fig:partial-training-full}, among the partially trained models, training the final layers yields the highest accuracy and CoCo-CoLa ratio for Llama-3.2 models, indicating that these layers play a crucial role in determining output language. Moreover, the accuracy of this partially trained model approaches that of the fully \(L_{i}\)\textit{-tuned} model, suggesting that the earlier layers already provide sufficient information for answering questions, even without exposure to \(L_{i}\) during training. Interestingly, Hindi, which initially exhibited lower performance than other languages, benefits from cross-lingual transfer, achieving better results with partial training than full training in both Llama-3.2 models. Llama-3.2-3B shows even stronger cross-lingual transfer, improving accuracy for Italian and Portuguese as well. For Llama-3.1-8B, training the second half of the model leads to the best CoCo-CoLa ratio, but the difference in \(L_{i}\) accuracy across partial training configurations is less pronounced than in Llama-3.2 models. This model also shows improved accuracy in partial training over full training for German, Italian, and Portuguese.  

These findings confirm the hypothesis that the final layers are strongly linked to output language selection. Additionally, for low-resource languages, partially training only the final layers of an \textit{en-tuned} model can achieve similar or even better accuracy compared to full fine-tuning in the target language. Beyond its effectiveness, partial training is significantly more efficient, reducing training time to half and memory usage to 65\% of full training. Furthermore, the model achieves higher accuracy in fewer training steps, requiring less than one epoch, meaning it is trained on fewer data points. 


\section{Conclusion}
In this work, we first analyzed shared knowledge across seven languages and identified key differences in the parameters most affected when training models for each language. Building on these insights, we proposed the CoCo-CoLa ratio, a metric for evaluating language adherence in multilingual LLMs, and used it to evaluate both pre-trained and fine-tuned LLMs. Our findings show that pre-trained models tend to generate English outputs regardless of the input language and that fine-tuning on English further amplifies this bias.

To address this problem, we leveraged insights from parameter updates and CoCo-CoLa results to develop a partial training method that improves language adherence in English-trained models. Our analysis demonstrated a more efficient alternative to full fine-tuning, achieving comparable or even superior performance while significantly reducing the number of updated parameters. In addition, given the widespread availability of instruction-tuned and task-specific English models, partial training of final layers presents a fast and efficient approach to adapting LLMs to new languages.

\section*{Limitations}
We acknowledge that training hyperparameters can significantly influence the linguistic bias of fine-tuned models, as highlighted by \citet{chirkova-nikoulina-2024-zero-shot}. For instance, while smaller learning rates may reduce bias, they can also lead to degraded task performance. Due to resource constraints, we used a single set of hyperparameters optimized for task performance rather than systematically exploring bias mitigation strategies. Additionally, we applied the same hyperparameter settings across all languages and model sizes, though fine-tuning them individually for each model-language pair could potentially yield better results.

Moreover, linguistic bias in pre-trained models and the observed trends in parameter updates across languages are influenced by factors such as model architecture, training procedures, data proportions, and even the order in which the model encounters training data. As a result, the specific layers we identified for each model size may differ when tested on other LLMs. Additionally, our observations suggest that certain languages are undertrained in Llama models. However, due to the lack of publicly available information on training data and procedures, we cannot make definitive claims regarding language-specific training discrepancies.

Another key limitation is that our study focuses on languages from the same language family, which are relatively close to each other and exhibit significant token overlap, facilitating cross-lingual transfer. The models we evaluated were also trained on a limited set of languages with similar characteristics. None of the studied languages fall into the low-resource category, meaning our findings may not generalize to massively multilingual models trained on a more diverse set of languages. Finally, our method is restricted to languages present in the model’s pretraining data and cannot be applied to completely new languages.

\section*{Ethical Statement}
This research investigates language adherence in multilingual large language models (LLMs) and proposes partial training methods for efficient adaptation. Our work aims to enhance linguistic fairness and accessibility by mitigating biases that favor high-resource languages. We acknowledge that training data composition and fine-tuning decisions can introduce unintended biases, which may disproportionately affect underrepresented languages. While our findings contribute to more equitable multilingual model adaptation, they are limited to languages present in the model's pretraining data and may not generalize to unseen languages.

Additionally, we use machine translation tools (e.g., Google Translate) to ensure consistency in question-answer pairs. While back-translation was employed to verify accuracy, potential translation artifacts could impact evaluation. We encourage further work to assess our method’s applicability to a broader set of languages, particularly low-resource and non-Indo-European languages.

This study does not involve human subjects, personal data, or user interactions, and we adhere to ethical guidelines for computational research. Our experiments were conducted using publicly available models and datasets, ensuring transparency and reproducibility.

% \section*{Acknowledgments}


\bibliography{anthology, custom}
\appendix

\section{Appendix}
\subsection{Implementation details} \label{sec:implementation-detail}
We experimented with dropout rates of 0.1 and 0.05, and learning rates of 5e-5, 1e-5, 5e-6, 1e-6, 5e-7, and 1e-7 for training on the English CBQA task. The best setting (dropout = 0.1, learning rate = 5e-6) was selected based on the minimum validation loss. These hyperparameters were used consistently across all languages and models throughout the paper.

For all training runs in our experiments, we used the hyperparameters listed in \autoref{tab:training-hyperparameter}. All experiments were conducted with a fixed random seed of 42. We implemented our models using Transformers 4.46.3 and Torch 2.5.1, with Accelerate 1.1.0 and DeepSpeed 0.16.1 for multi-GPU training. All experiments were run on NVIDIA RTX A6000 GPUs, with all experiments taking approximately 48 hours on eight GPUs. 
\input{tables/training_hyperparameters}

\label{sec:appendix}

\subsection{Language specific knowledge}
Beyond measuring similarities between languages using the Jaccard Index, we also analyze differences by identifying answers that are known in language A but unknown in language B. This allows us to examine the distribution of languages within the 40\% of answers that are not correctly predicted by both languages. The results, presented in \autoref{fig:correct-diff}, reveal an almost symmetrical distribution of known and unknown answers across most language pairs. However, notable deviations occur for languages with significantly lower overall accuracy. Specifically, Hindi shows a greater disparity in the Llama-3.2 models, while both Hindi and Portuguese exhibit this trend in the Llama-3.1-8B model.

\begin{figure*}
    \centering
    \includegraphics[width=0.9\textwidth]{figures/diffs_all.png}
    \caption{Difference in known knowledge between each pair of languages across different model sizes.}
    \label{fig:correct-diff}
\end{figure*}

\subsection{Parameter update} \label{appendix:heatmap}
Due to space constraints, the main text presents results for only four languages. However, the analysis of model updates for Italian, Spanish, and Portuguese follows similar trends and can be found in \autoref{fig:model-diff-appendix}. These additional results confirm the patterns observed in other languages, reinforcing our findings on language-specific parameter updates.

\begin{figure*}
    \centering
    \includegraphics[width=0.8\linewidth]{figures/heatmaps_all_appendix.png}
    \caption{Average magnitude of difference between pretrained and monolingually fine-tuned models for Llama-1B, Llama-3B, and Llama-8B.}
    \label{fig:model-diff-appendix}
\end{figure*}

\subsection{Partial Training} \label{sec:partial-appendix}
Due to space limitations, the results of partial training on Italian, Portuguese, and Spanish are provided in \autoref{fig:partial-training-appendix}. Additionally, the CoCo-CoLa ratios for both partially trained and fully trained models are shown in \autoref{tab:coca-ratio-partial-1b} for Llama-3.2-1B, \autoref{tab:coca-ratio-partial-3b} for Llama-3.2-3B, and \autoref{tab:coca-ratio-partial-8b} for Llama-3.1-8B. These comparisons highlight the consistently superior CoCo-CoLa ratio in the partial training of final layers.
\begin{figure*}[h]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/appendix_bar_plot.png}
    \caption{Cumulative accuracy and \( L_i \) accuracy on \textit{en-tuned} (\( SFT_{en} \)) and \( L_i \)-tuned models (\( SFT_{L_i} \)), along with partially trained models, across all Llama model sizes.}
    \label{fig:partial-training-appendix}
\end{figure*}

\input{tables/coca_ratio_partial_1b}
\input{tables/coca_ratio_partial_3b}
\input{tables/coca_ratio_partial_8b}

\end{document}
