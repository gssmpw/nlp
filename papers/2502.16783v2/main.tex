\pdfoutput=1
\documentclass[manyauthors]{fundam}

%%%%%%%  parameters to be filled in by copy-editor  %%%%%%%%%%

% \setcounter{page}{1}
% \publyear{22}
% \papernumber{2102}
% \volume{185}
% \issue{1}
%
% \finalVersionForARXIV
% \finalVersionForIOS

\usepackage{geometry}
\usepackage{core}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{hcalculation} % Cooler calculational proofs

% Figures
\usepackage{tikz/tikzit}
\usepackage{tikz/gla}
\input{tikz/gla.tikzstyles}

% Make the semicolon act as operator (instead of punctuation) in math mode
\mathcode`;=\numexpr\mathcode`;-"6000

% useful macros
\DeclareMathOperator{\im}{Im}
\let\ker\relax
\DeclareMathOperator{\ker}{Ker}
\def\by{{\times}}
\def\K{\mathbb{K}}

%% Precompile tikz figures
\usetikzlibrary{external}
\tikzset{external/only named=true}
\tikzexternalize[mode=graphics if exists,prefix=compiled_figures/]


\title{Generalizing the Invertible Matrix Theorem with Linear Relations using Graphical Linear Algebra}
\date{\today}

\author{%
Iago Leal de Freitas%
\thanks{The authors would like to thank Gustavo Freire and David E. Bernal Neira for their help reviewing this manuscript.}%
\\ Purdue University \\ West Lafayette, IN 47907, USA \\
\and
Júlia Mota\thanksas{1}%
\thanks{This study was financed in part by the Coordenação de Aperfeiçoamento de Pessoal de Nível Superior - Brasil (CAPES) - Finance Code 001.}%
\\ Universidade Federal do Rio de Janeiro \\ Rio de Janeiro, RJ, Brazil
\and
João Paixão\thanksas{1}%
\\ Universidade Federal do Rio de Janeiro \\ Rio de Janeiro, RJ, Brazil
\and
Lucas Rufino\thanksas{1}\thanksas{2}\corresponding%
\\ Instituto Nacional de Matemática Pura e Aplicada \\ Rio de Janeiro, RJ, Brazil
}

% \address{Address for correspondence goes here}
\runninghead{I. Leal de Freitas, J. Mota, J. Paixão, L. Rufino}{A Generalized IMT via Relational Decompositions}

\begin{document}

\maketitle

\begin{abstract}
Linear algebra's main concerns are sets of vectors, linear functions, subspaces, linear systems, matrices and concepts about those, such as whether the solution of linear system exists or is unique; a set of vectors is linearly independent or spans the whole space; a linear function has a right or a left inverse; a linear function is surjective or injective; and the kernel of a matrix is trivial or the its image is full.

The Invertible Matrix Theorem ties all these ideas and many others together.
Many modern linear algebra books use this theorem as a guiding principle to explain many connections in linear algebra. The main idea is to separately characterize whether the linear function is surjective or injective. The proof usually uses a matrix decomposition as the key step.
However, the invertible matrix theorem deals with a single linear function, a single set of vectors, a single subspace, and a single matrix.

In this work, we generalize part of the invertible matrix theorem to results about
a pair of linear functions, a pair of sets of vectors, a pair of subspaces, and a single linear relation.
The main idea is to separately characterize the linear relation's fundamental properties---whether it is surjective, injective, deterministic and total.
Our proof uses a decomposition of a linear relation as the key step.

Unfortunately, reasoning with linear relations in classical notation requires
applying many rules besides shuffling quantifiers and variables around,
which can obscure the symmetries in the results.
Therefore,  this work employs graphical linear algebra,
a two-dimensional diagrammatic syntax with the fundamental rules of linear relations built-in.
\end{abstract}

\begin{keywords}
Linear Relations, Matrix Decomposition, Graphical Linear Algebra, Invertible Matrix Theorem, String Diagrams
\end{keywords}

\tableofcontents
\newpage

\section{Introduction}
\label{sec:introduction}

In linear algebra, there are many ways of stating that a matrix is invertible. For example, a $n$-by-$n$ matrix $A$ is invertible when either its kernel is zero, its image is full, or it admits an inverse $AB = I = BA$. The central result that establishes all these notions of invertibility as equivalent is the Invertible Matrix Theorem~\cite{weisstein2014invertible}. Its only underlying hypothesis is that the matrix A must be square. As illustrated in Figure~\ref{fig:intro_invertible_matrix_theorem}, by dropping this hypothesis, the characterization breaks down into two: surjectivity and injectivity.

\begin{figure}[h]
  \centering
  \tikzfig{introduction_figures/invertible_matrix_theorem}
  \caption{For a general $n \by m$ matrix $A$, surjectivity and injectivity are separate properties.
  However, when $A$ is square, the two characterizations merge into one.}
    \label{fig:intro_invertible_matrix_theorem}
\end{figure}

The Invertible Matrix Theorem may be viewed as a gluing theorem, connecting two characterizations into a single one. The key to this connection is knowing how the properties of surjectivity (SUR), injectivity (INJ), and the dimensional properties (i.e. $m \leq n$ and $m \geq n$) relate to each other. Let's call them the \emph{fundamental properties} of a matrix. A good way to illustrate their relation is through a graph, shown in Figure~\ref{fig:intro_poset_of_4_fundamental_properties}.

\begin{figure}[h]
  \centering
  \tikzfig{poset_of_fundamental_properties/little_poset}
  \caption{Graph of the $4$ fundamental properties of a $n \by m$ matrix.
  To read this graph, start at a node $Y$ at the top and let $X_1, X_2,\ldots, X_k$ be all the nodes at the bottom reachable through the paths starting at $Y$.
  Then, $X_1,\ldots, X_k \implies Y$.
  For example, if we start at the node SUR at the top and follow the edges downwards, we reach the nodes INJ and $(m \leq n)$ at the bottom.
  }
  \label{fig:intro_poset_of_4_fundamental_properties}
\end{figure}
The graph shows that an injective matrix with more rows than columns must also be surjective. Consequently, for square matrices injectivity and surjectivity are equivalent notions, thus characterizing a single concept: invertibility. Under this perspective, the Invertible Matrix Theorem is a consequence of two results: \emph{a characterization of the fundamental properties} and \emph{a graph showing how they relate}.

Everything so far concerns theorems about a single matrix.
Linear algebra, however, also has various theorems about a more general class of objects, namely the solutions of the linear system $\{(x,y) \mid Ax = By\}$
(Notice that by setting $B = I$, this becomes the graph for the function $A$. Thus, this generalizes the ``single matrix'' case).
In this context, there are natural generalizations of the aforementioned fundamental properties SUR, INJ, $m \leq n$ and $m \geq n$, and also two new ones called determinism (DET) and totality (TOT).
The main goal of this paper is to generalize the Invertible Matrix Theorem to this setting.
More concretely, Theorems~\ref{thm:cospan_dict} and~\ref{thm:poset_of_fundamental_properties} provide characterizations for each fundamental property and a gluing theorem showing how they relate.
This yields a more general version of the graph in Figure~\ref{fig:intro_poset_of_4_fundamental_properties}, shown in Figure~\ref{fig:intro_poset_of_6_fundamental_properties}.

\begin{figure}[h]
  \centering
  \tikzfig{poset_of_fundamental_properties/poset}
  \caption{Graph of the $6$ fundamental properties of a pair of $k \by m$ and $k \by n$ matrices.
  To read this graph, use the same rules as Figure~\ref{fig:intro_poset_of_4_fundamental_properties}.}
  \label{fig:intro_poset_of_6_fundamental_properties}
\end{figure}
We also show this is a generalization: by taking $B = I$ the identity matrix, the characterizations reduce to the usual Injective and Surjective Matrix Theorems.
Furthermore, assuming $m = n$ recovers the original Invertible Matrix Theorem.
To better formulate that generalization, the language of \emph{linear relations} proves useful.
Thus, in what follows, we briefly introduce the topic.

\subsection{Linear Relations}
\label{sec:pigeonhole}

One of the simplest ways to motivate the use of relations in linear algebra involves the pigeonhole principle and drawing dots. Given two finite sets $A$ and $B$, we call any subset $R \subseteq A \times B$ a \emph{relation} between $A$ and $B$.
By representing these sets as dots on a sheet of paper, a relation $R$ becomes a graph connecting those dots (we draw a line connecting $a \in A$ to $b \in B$ if and only if $(a, b) \in R$), as seen in Figure~\ref{fig:fin-rel}.

\begin{figure}[h]
    \centering
    \tikzfig{introduction_figures/pigeonhole_0}
    \caption{A relation between two finite sets $A$ and $B$ represented as a bipartite graph.}
    \label{fig:fin-rel}
\end{figure}

\noindent It is always possible to represent a function $f : A \to B$ by its graph $\text{Gr}(f) \coloneq \{(a,b) \mid f(a) = b\} \subseteq A \times B$, making functions a special case of relations.
The pigeonhole principle states that if $f$ is injective, then $\# A \leq \# B$, where $\#A$ stands for its cardinality.
Figure~\ref{tab:pigeonhole_2} illustrates the graph of an injective function.
Notice how the pigeonhole principle forces
the set on the right to be at the as large as the set on the left.

\begin{figure}[h]
    \centering
    \begin{tabular}{l l}
    \tikzfig{introduction_figures/pigeonhole_1} & \makecell{Function: Any dot on the left connects to a unique dot on the right. \\
    INJ: Any dot on the right connects to at most one dot on the left.}
    \end{tabular}
    \caption{The graph of an injective function.}
    \label{tab:pigeonhole_2}
\end{figure}

Since every dot on the left connects to a different unique dot on the right, there must be at least as many right-dots as left-dots. The not-so-immediate observation is that we do not need uniqueness in the function property for the pigeonhole principle to hold. More concretely, we only need that every dot on the left connects to \emph{at least} one dot on the right.

\begin{figure}[h]
    \centering
    \begin{tabular}{l l}
    \tikzfig{introduction_figures/pigeonhole_2} & \makecell{TOT: Any dot on the left connects to at least one dot on the right. \\
    INJ: Any dot on the right connects to at most one dot on the left.}
    \end{tabular}
    \caption{The pigeonhole principle still works for the graph of a relation satisfying TOT+INJ.}
    \label{tab:pigeonhole_3}
\end{figure}

As seen in Figure~\ref{tab:pigeonhole_3}, the pigeonhole principle holds for some relations that are not functions (i.e. do not satisfy the function property). This tells us that the pigeonhole principle only requires relations and that its usual formulation about injective functions contains an unnecessary hypothesis.

These observations about functions of finite sets generalize to linear transformations. A \emph{linear relation} between vector spaces $V$ and $W$ is a subspace $R \subseteq V \times W$ of their Cartesian product.
By replacing the cardinality $\# X$ with the dimension $\text{dim}(X)$, the pigeonhole principle works the same, while the previous weakening procedure constructs two linear algebraic results shown in Table~\ref{tab:pigeonhole_4}.

\begin{table}[h]
    \centering
    \bgroup
      \renewcommand{\arraystretch}{1}
      \setlength\tabcolsep{1cm}
      \begin{tabular}{l l}
        \textbf{Pigeonhole for functions} & \textbf{Pigeonhole for linear transformations}  \\[1mm]
          If $f : A \to B$ is INJ, & If $f : V \to W$ is INJ, \\
          then $\# A \leq \# B$ & then $\text{dim}(V) \leq \text{dim}(W)$ \\[5mm]
          \textbf{Pigeonhole for relations} & \textbf{Pigeonhole for linear relations} \\[1mm]
          If $R \subseteq A \times B$ is TOT+INJ, & If $R \subseteq V \times W$ is TOT+INJ, \\
          then $\# A \leq \# B$ & then $\text{dim}(V) \leq \text{dim}(W)$
      \end{tabular}
    \egroup
    \caption{The usual pigeonhole principle can be weakened to relations and generalized to the context of linear algebra.}
    \label{tab:pigeonhole_4}
\end{table}

The usual pigeonhole principle of linear transformations is a standard result in linear algebra: injective matrices are tall.
Given how well-understood linear algebra is, the modified, relational pigeonhole principle is expected to also be equivalent to a well-known result.
Perhaps surprisingly, it is equivalent to the Exchange Lemma~\cite[Page 35; 2.22]{axler2024linear}, a fundamental result in linear algebra implying that all bases for a given vector space must have the same size. The precise translation from one to the other are explained later in Section~\ref{sec:aplications_poset_of_fundamental_properties}).
However, it already serves as a good example of how the language of relations produces unexpected connections between seemingly unrelated results. This could not be more appropriate for our goals: as discussed earlier, the Invertible Matrix Theorem is essentially a \emph{connecting} (or gluing) theorem for different fundamental properties.

To close this section, we properly define linear relations.
Also, from now on, we always work over a fixed field $\K$
and, since finite-dimensional vector spaces are completely determined
(up to isomorphism) by their dimension,
we restrict ourselves to subspaces $R \subseteq \K^m \times \K^n$.

\begin{definition}
\label{def:relation}
A \emph{linear relation} $R$ between finite-dimensional vector spaces $V, W$ is a subspace $R \subseteq V \times W$.
\end{definition}

\begin{definition}
\label{def:relation_operations}
The main operations on linear relations are the following.
\begin{enumerate}
\item[(i)] Given $R \subseteq \K^{m} \times \K^k$, $S \subseteq \K^k \times \K^n$, define their \emph{relational composition} $R \,;\, S \subseteq \K^m \times \K^n$ as
$$R \,;\, S \coloneq \{(x, z) \mid \exists y ((x, y) \in R, (y, z) \in S)\}.$$
\item[(ii)] Given $R \subseteq \K^m \times \K^n$, $S \subseteq \K^s \times \K^r$, define their \emph{Cartesian product}
$R \times S \subseteq \K^{m+s} \times \K^{n+r}$ as
$$R \times S \coloneq \{((x, w), (y, z)) \mid (x, y) \in R, (w, z) \in S\}.$$
\item[(iii)] Given $R \subseteq \K^m \times \K^n$, define its \emph{opposite} $R^{op} \subseteq \K^n \times \K^m$ as
$$R^{op} \coloneq \{(x,y) \mid (y, x) \in R\}.$$
\end{enumerate}
\end{definition}

\begin{definition}
\label{def:relation_poset}
Linear relations have a natural poset structure $\subseteq$ induced by the usual set-theoretic inclusion. For linear relations $R, S \in \K^n \times \K^m$,

\[
  R \subseteq S
  \iff
  \forall x \in \K^n,\, y \in \K^m,\; (x,y) \in R \implies (x, y) \in S.\]

\end{definition}

\noindent Linear relations are generalizations of linear maps and linear subspaces, in the sense that any subspace $V \subseteq \K^n$ can be seen as a $0$-by-$n$ linear relation $\{(0, x) \mid x \in v\} \subseteq \K^{0} \times \K^n$, where $\K^0 = \{0\}$ is the zero-dimensional vector-space, and any linear map $f : \K^m \to \K^n$ can be seen as a $m$-by-$n$ linear relation via its graph $\text{Gr}(f) \coloneq \{(x, f(x)) \mid x \in \K^m\}$. When restricted to linear maps, the operations (i) and (ii) above become the usual composition and Cartesian product of maps. Moreover, the composition $V \,;\, f$ is the subspace $f(V)$.

Relational composition and Cartesian product are associative operations, so they satisfy the laws $(R \,;\, S) \,;\, H = R \,;\, (S \,;\, H)$ and $(R \times S) \times H = R \times (S \times H)$. There is also a law describing how these two operations interact:
\begin{equation}\label{eq:2d_associativity_in_symbolic_syntax}
    (A \,;\, B) \times (C \,;\, D) = (A \times C) \,;\, (B \times D).
\end{equation}
As hinted by the discussion about the Pigeonhole Principle, the notions of surjectiveness and injectiveness extend to linear relations, together with two dual properties named \emph{totality} and \emph{determinism}.
\begin{definition}\label{def:fundamental_properties_classical} A linear relation $R \subseteq V \times W$ is \\
\begin{tabular}{r r l}
  (i) & injective if & $\forall y \in W \text{ there is at most one $x \in V$ such that } (x,y) \in R;$ \\
  (ii) & surjective if & $\forall y \in W \text{ there is at least one $x \in V$ such that } (x,y) \in R;$ \\
  (iii) & deterministic if & $\forall x \in V \text{ there is at most one $y \in W$ such that } (x,y) \in R;$ \\
  (iv) & total if & $\forall x \in V \text{ there is at least one $y \in W$ such that } (x,y) \in R.$
\end{tabular}
\end{definition}

\subsection{Graphical Notation}
\label{sec:graphical_notation}

In this paper, linear relations are represented using string diagrams.
That is, instead of the usual symbolic syntax,
relations are written as two-dimensional diagrams and composed similarly to logical gates.
This is known as \emph{graphical notation} and is widely used among category theorists.
According to \cite{hinze2023introducing},
diagrams enable equational reasoning without loss of type information.
The use of diagrams in linear algebra as seen here was developed by Zanasi in his thesis~\cite{ZanasiThesis},
leading to what is now called Graphical Linear Algebra (GLA)~\cite{bonchi2014categorical,Bonchi2015,bonchi2017refinement,Bonchi2019a, PAIXAO2022}.

In what follows we give a brief introduction
to enable the reader to read proofs written with diagrams.
For a more thorough account of Graphical Linear Algebra,
we recommend the blog series by Pawe{\l} Soboci\'{n}ski~\cite{blogpawel}.

A $m$-by-$n$ linear relation can be written as a two-dimensional diagram with $m$ dangling wires on the left and $n$ on the right.
\begin{equation}
R \subset \K^m \times \K^n \mapsto \quad \tikzfig{introduction_figures/n_wired_dots_relation}.
\end{equation}
For example, a $1$-by-$2$ linear relation $R$ can be written as a diagram \tikzfig{introduction_figures/1_by_2_relation} with one wire on the left and two wires on the right. Generally, when $R$ is a $m$-by-$n$ linear relation, it is more convenient to write \tikzfig{introduction_figures/m_by_n_relation}, and when there is no confusion about the type information, we may omit the numbers $m, n$ and write it as \Rel{R}. Lastly, when one of the numbers is zero, say, $m = 0$,  then $R$ is called a linear subspace and we omit the dangling wire on the left, writing \tikzfig{introduction_figures/0_by_n_relation}.

We also reserve different symbols based on how specialized the linear relation is:
while the box symbol $\Rel{R}$ denotes a general linear relation,
the curvy symbol $\Map{A}$ is used when we know $A$ is a linear map.
Moreover, the blue curvy symbol $\Inv{A}$ is used when $A$ is known to have an inverse,
usually denoted $A^{-1}$.
In the diagrammatic notation, operations~(i) and~(ii) of Definition~\ref{def:relation_operations} become, respectively, the action of connecting diagrams horizontally and stacking them vertically, and the action of flipping the diagram horizontally.
Table~\ref{tab:graphical_notation} summarizes what we have said so far.

\begin{table}[htb]
    \centering
    \tikzfig{introduction_figures/graphical_notation}
    \caption{Graphical notation.}
    \label{tab:graphical_notation}
\end{table}

Under the graphical syntax, both sides in Equation~\eqref{eq:2d_associativity_in_symbolic_syntax} become the two ways we can put parenthesis in the diagram composition
\begin{equation}
\tikzfig{introduction_figures/2d_associativity}
\end{equation}
which is very similar to an associativity law. Informally speaking, the difference is that the parentheses move in two dimensions, instead of simply from left-to-right. Due to this law, we will always omit the dashed lines when composing diagrams.

There are specific symbols for some commonly occurring linear relations called identity, twist, zero, sum, copy, and discard. These are all graphs of linear maps. Table~\ref{tab:generators} shows their corresponding diagrammatic symbols and linear maps.

\begin{table}[htb]
  \centering
  \renewcommand{\arraystretch}{2.5}
  \begin{tabular}{r l l l}
    (Identity) & \TypedId{n} & $x \mapsto x$ & $: \K^n \to \K^n$  \\
    (Twist) & \tikzfig{introduction_figures/n_wired_twist} & $(y \in \K^m, x \in \K^n) \mapsto (y, x)$ & $: \K^{m+n} \to \K^{m+n}$ \\
    (Zero) & \TypedZero{n} & $x \mapsto 0$ & $: \K^0 \to \K^n$ \\
    (Sum) & \tikzfig{introduction_figures/n_wired_sum} & $(x \in \K^n, y \in \K^n) \mapsto x + y$ & $: \K^{2n} \to \K^n$  \\
    (Discard) & \TypedDiscard{n} & $x \mapsto 0$ & $: \K^n \to \K^0$ \\
    (Copy) & \tikzfig{introduction_figures/n_wired_copy} & $x \mapsto (x, x)$ & $: \K^n \to \K^{2n}$ \\
  \end{tabular}
  \caption{Special relations together with their names and graphical notation.}
  \label{tab:generators}
\end{table}

In the same way as before, we will often omit the numbers $m,n$ when there is no potential for confusion. The zero diagram \TypedZero{n} can be thought of as the zero subspace $\{0\} \subseteq \K^n$, whereas the opposite of the discard, \TypedCoDiscard{n}, can be thought of as the full space $\K^n \subseteq \K^n$.
Additionally, any linear map $A$ satisfies
\begin{equation}\label{eq:map_equations}
\begin{tabular}{l l}
    $\Image{A} = \; \tikzfig{introduction_figures/image_of_A}$, & $\Kernel{A} = \; \tikzfig{introduction_figures/kernel_of_A}$, \\
    $\CoImage{A} = \; \CoDiscard$, & $\CoKernel{A} = \; \Zero$.
\end{tabular}
\end{equation}
From this, we see that a map $A$ is surjective if and only if \Image{A} = \CoDiscard and injective if and only if \Kernel{A} = \Zero. This is still true for linear relations in general. More concretely, all four properties in Definition~\ref{def:fundamental_properties_classical} can be written in terms of the white and black structures, as stated below.
\begin{proposition}[Fundamental Properties]
  \label{def:fundamental_properties_and_invertibility}
  A linear relation \Rel{R} is said to be
\begin{center}
\renewcommand{\arraystretch}{2.0}
\resizebox{\textwidth}{!}{\begin{tabular}{l l l l}
    total (TOT) if &\RelCoImage{R} \!\!= \CoDiscard \!\!\!, & deterministic (DET) if &\RelCoKernel{R} \!\!= \Zero \!\!\!, \\
    surjective (SUR) if &\RelImage{R} \!\!= \CoDiscard \!\!\!, & injective (INJ) if &\RelKernel{R} \!\!= \Zero \!\!\!, \\
    a map if it is &TOT and DET, &  bijective if it is & TOT, DET, INJ, and SUR.
\end{tabular}}
\end{center}
\end{proposition}

\noindent By our previous considerations, this definition coincides with the usual definition of surjectiveness and injectiveness when \Rel{R} is a linear map.
Also, notice that the two inequalities
\begin{equation}
    \RelImage{R} \subseteq \; \CoDiscard, \quad \RelKernel{R} \supseteq \; \Zero
\end{equation}
hold for any relation.
Therefore, throughout the paper, when showing that a relation is TOT, DET, INJ or SUR, we often do not prove the equality required by Definition~\ref{def:fundamental_properties_and_invertibility}, and instead only show the corresponding non-trivial inequality. For example, to show that a linear relation $\Rel{R}$ is total, it is enough to show that $\RelImage{R} \supseteq \; \CoDiscard$.
See Figure~\ref{fig:Inequality} for which inequalities hold for every relation.

As this paper deals with the Invertible Matrix Theorem in a relational context,
we define below what it means for a map to have an inverse.
Notice that we do not assume a bijective map has an inverse,
as this will result from the Invertible Matrix Theorem, proven later in the text.
\begin{definition}
\label{def:inverse}
  Given a map $\Map{A}$, a map $\Map{B}$ is called
  \begin{enumerate}
      \item[(i)] a left-inverse\footnote{It is worth noting that the left-inverse $\Map{B}$ appears at the \emph{right} of $\Map{A}$, and the right-inverse appears at the \emph{left}. This is because the relational composition $A ; B$ has its arguments flipped with respect to the standard composition (i.e. the composition of matrices $BA$ is written with diagrams as $\Comp{A/Map, B/Map}$). We maintain this confusing terminology in order to stay consistent with the standard literature.} if $\Comp{A/Map, B/Map} = \Id$,
      \item[(ii)] a right-inverse if $\Comp{B/Map, A/Map} = \Id$,
      \item[(iii)] an inverse if it is a right-inverse and a left-inverse.
  \end{enumerate}
\end{definition}
\noindent Also, notice that an inverse is unique
and that if a map has both a left and a right-inverse, they must be equal
and the map has an inverse.

It is important to note that the poset structure $\subseteq$
interacts well with the two composition operations~\cite{PAIXAO2022}.
More concretely, if $\Rel{X} \subseteq \Rel{Y}$, then for any relations $\Rel{A}, \Rel{B}$ we have
\begin{align*}
  \tikzfig{introduction_figures/poset_vs_composition_1}
  &,&
  \tikzfig{introduction_figures/poset_vs_composition_2}.
\end{align*}
We will often use this to produce inequalities between complicated diagrams. For example, we know $\Zero \subseteq \CoDiscard$, therefore, the following inequality must hold, regardless of what $A$ and $B$ are:
\[\tikzfig{introduction_figures/poset_vs_composition_3}.\]

All proofs in this paper consist of sequences of diagram manipulations.
Appendix~\ref{sec:gla_thms} contains all necessary rules in their diagrammatic form.
The proof style follows a calculational format~\cite{Dijkstra1989PredicateCA},
with a ``proof hint'' in square brackets to the right of a diagram or statement
explaining which result was used  to conclude it from the previous one.
To exemplify, here is the proof that any map $\Map{A}$ with a left-inverse $\Map{B}$
is injective:
\begin{hcalculation}[\subseteq]{\Kernel{A}}
  \hstep{\CompTwo{White, CoMap, B, CoMap, A, .}}{Fig.\ref{fig:Minimum_and_maximum}: $\Zero \subseteq \Kernel{}$}
  \hstep[=]{\Zero}{Hyp: $\Comp{A/Map, B/Map} = \Id$}
\end{hcalculation}
On the right, the manipulation rules in Figure~\ref{fig:Minimum_and_maximum}
and the theorem's hypothesis
are noted as the steps for going from the first diagram to the last one.

The propositions in Figure~\ref{fig:Symmetry},
reproduced below,
\[
  \begin{aligned}
    \Rel{R} & \subseteq \Rel{S} &\iff \CoRel{R} &\subseteq \CoRel{S}, \\
    \RelGray{R} & \subseteq \Rel{S} &\iff \Rel{R} &\supseteq \RelGray{S}.
  \end{aligned}
\]
imply that any inequality between relations gives rise to other two inequalities
by either mirroring all relations to their opposites or swapping their colors.
This way, any theorem in diagrammatic form actually summarizes 4 theorems.
As a corollary, the opposites and color-swapped versions of an \emph{equality} are also equalities.
This will be used thoroughly in this paper without further comments.

Finally, we note that the advantages of diagram manipulation were the main reason for choosing graphical notation as a tool in this work.
However, it is important to emphasize that the results obtained do not depend exclusively on this notation;
all proofs can be equivalently written in the usual syntax.


\subsection{Main Contributions}

This paper main contributions are the characterization of fundamental properties
in Theorem~\ref{thm:cospan_dict} generalizing the Invertible Matrix Theorem for linear relations
and the graph of fundamental properties implications in Theorem~\ref{thm:poset_of_fundamental_properties}.
To tackle those results, in Theorem~\ref{thm:rcd_for_cospan},
we develop a decomposition for linear relations in cospan form
\[ \Rel{R} = \CoSpan{A}{B} = \GRR,\]
which acts as a generalization of the Canonical Decomposition for matrices.

Furthermore, this decomposition induces a \emph{generalized matrix decomposition}
simultaneously decomposing the cospan's factors as
\[\tikzfig{rcd_for_pairs_of_matrices/stat}.\]
The linking matrix $\Map{H}$ is constructed in Theorems~\ref{thm:abstract} and~\ref{thm:rcd_for_pairs_of_matrices},
and, as shown in Theorem~\ref{thm:subspaces},
can be used to calculate all subspaces related to the images $\im(A)$ and $\im(B)$.
It is also worth noticing that this decomposition is analogous to the Generalized Singular Value Decomposition (GSVD)~\cite{gsvd1976,gsvd1981},
but defined without assumptions on the underlying scalar field.


\section{Fundamental Properties}\label{sec:fundamental_properties}

We start our discussion by dissecting the proof of
the original Invertible Matrix Theorem (IMT),
as a motivation for the decisions taken later in the text regarding its relational version.
As mentioned in Section~\ref{sec:introduction},
IMT is a two-part theorem, starting with characterizing surjectivity and injectivity
i.e., the fundamental properties of a linear map.
For an $m$-by-$n$ linear map $A$,
such a characterization can be split into two blocks,
as illustrated in Figure~\ref{fig:imt-graph},
\begin{enumerate}
  \item Statements regarding the existence of a right or left-inverse $B$ to $A$ or, equivalently, solutions to linear systems.
  \item Statements concerning only the map $A$ and its properties;
\end{enumerate}

\begin{figure}[h]
    \centering
    \tikzfig{thm_cospan_dict/blocks}
    \caption{Statements in the same block can easily be shown to be equivalent. However, going from a right-statement to a left-statement requires a matrix decomposition or an equivalent algorithm such as Gaussian elimination to construct the linear map $B$.}
    \label{fig:imt-graph}
\end{figure}

Statements in the same block are straightforwardly shown to be equivalent.
Also, the existence of an inverse $B$ directly implies the properties of interest,
such as injectivity or trivial kernel.
The hard part of proving the IMT is showing that those properties are enough to to assure the existence of an inverse.
This process involves finding an algorithm (e.g. Gaussian elimination)
to construct said linear map $B$.
Instead of reproducing the algorithm inside the proof,
this is usually appears as a rank-revealing matrix decomposition
\[
  \label{eq:functional_canonical_decomposition}
  A = P^{-1}\begin{bmatrix} I & 0 \\ 0 & 0 \\\end{bmatrix} Q^{-1}.
\]

Once the characterizations are proven, the decomposition in Equation~\eqref{eq:functional_canonical_decomposition}
can be used to derive a graph connecting the properties SUR and INJ with the dimensionality properties $m \leq n$ and $m \geq n$ shown in Figure~\ref{fig:intro_poset_of_4_fundamental_properties}.
This gives a general guideline of how to prove the IMT:
\begin{enumerate}
    \item[(i)] For each of the properties SUR, INJ, prove the equivalencies in both blocks separately;
    \item[(ii)] Connect the two blocks via a canonical decomposition;
    \item[(iii)] Use the decomposition once more to build a graph connecting the fundamental properties SUR and INJ to the dimensionality properties $m \leq n$ and $m \geq n$.
\end{enumerate}

\subsection{Preliminary Characterizations}

We now follow these general guidelines to generalize the IMT to the context of linear relations.
First, we state a classical result,
asserting that any linear relation $R$ can be realized as the solution set $\{(x, y) \mid Ax = By\}$
for some pair of linear maps $A, B$.
This is known as the \emph{cospan form} of $R$.

\begin{proposition}\label{prop:cospan}
For every linear relation $\TypedRel{R}{m}{n}\!$, there exist $\TypedMap{A}{m}{k}\!\!$, $\TypedMap{B}{n}{k}\!\!$ linear maps such that
\[\Rel{R} = \CoSpan{A}{B}.\]
\end{proposition}
\begin{proof}
   See Lemma \ref{lem:relation_in_cospan_form_complete}.
\end{proof}

Due to this result, we express the following theorems in terms of the maps $\Map{A}$ and $\Map{B}$ instead of the linear relation $\Rel{R}$.
As in Definition~\ref{def:fundamental_properties_and_invertibility}, there are two additional fundamental properties called determinism and totality.
The first step is to prove a characterization theorem for each fundamental property
without involving invertibility or the existence of solutions.

\begin{proposition}
  \label{prop:r_block}
  Given linear maps $\TypedMap{A}{m}{k}\!\!$, $\TypedMap{B}{n}{k}\!\!$, each column of the following table consists of a series of equivalent statements.
\[\resizebox{\textwidth}{!}{\tikzfig{thm_cospan_dict/R_block}}\]
\end{proposition}
\begin{proof}
We first show the TOT case, then the DET case.
The other two follow similarly.
The strategy is Row 1 $\Rightarrow$ Row 2 $\Rightarrow$ Row 3 $\Rightarrow$ Row 1.

(Row 1 $\implies$ Row 2)
\begin{hcalculation}[\subseteq]{\Image{A}}
\hstep[=]{\CompThree{Black, Map, B, CoMap, A, Map, A, .}}{Hyp: $\CompTwo{Black, Map, B, CoMap, A, .} = \CoDiscard$}
\hstep{\Image{B}}{Fig.\ref{fig:typerelations}-DET: $\CoSpan{\;}{\;} \subseteq \Id$}
\end{hcalculation}

(Row 2 $\implies$ Row 3)
\begin{hcalculation}[=]{\Span{A}{A}}
\hstep{\tikzfig{thm_cospan_dict/row_2_implies_row_3_step_2}}{Fig.\ref{fig:Commutative_Comonoid}: \tikzfig{theorems/unitblack}}
\hstep{\tikzfig{thm_cospan_dict/row_2_implies_row_3_step_3}}{Fig.\ref{fig:Flip}: \tikzfig{theorems/flipblack}}
\hstep[\subseteq]{\tikzfig{thm_cospan_dict/row_2_implies_row_3_step_4}}{Hyp: $\Image{A} \subseteq \Image{B}$}
\hstep{\tikzfig{thm_cospan_dict/row_2_implies_row_3_step_5}}{Fig.\ref{fig:Flip}: \tikzfig{theorems/flipblack}}
\hstep{\Span{B}{B}}{Fig.\ref{fig:Commutative_Comonoid}: \tikzfig{theorems/unitblack}}
\end{hcalculation}

(Row 3 $\implies$ Row 1)
\begin{hcalculation}[=]{\CompTwo{., Map, A, CoMap, B, Black}}
\hstep{\CompThree{., Map, A, CoMap, B, Map, B, Black}}{Fig.\ref{fig:typerelations}-TOT: $\Diagram{Map}{}{.}{Black} = \Discard$}
\hstep[\supseteq]{\CompThree{., Map, A, CoMap, A, Map, A, Black}}{Hyp: $\Span{B}{B} \supseteq \Span{A}{A}$}
\hstep[\supseteq]{\Diagram{Map}{A}{.}{Black}}{Fig.\ref{fig:typerelations}-TOT: $\CoSpan{\;}{\;} \supseteq \Id$}
\hstep{\Discard}{Fig.\ref{fig:typerelations}-TOT: $\Diagram{Map}{}{.}{Black} = \Discard$}
\end{hcalculation}

\noindent Now we show the DET column. The proof is analogous, but we include it for the sake of clarity.

(Row 1 $\implies$ Row 2)
\begin{hcalculation}[\subseteq]{\Kernel{B}}
\hstep[=]{\CompTwo{White, Map, A, CoMap, B, .}}{$\CoKernel{A} = \Zero$}
\hstep[\subseteq]{\Zero}{Hyp: $\CompTwo{White, Map, A, CoMap, B, .} \subseteq \Zero$}
\end{hcalculation}

(Row 2 $\implies$ Row 3)
\begin{hcalculation}[=]{\CoSpan{B}{B}}
\hstep{\tikzfig{thm_cospan_dict/row_2_implies_row_3_step_2_DET}}{Fig.\ref{fig:Commutative_Comonoid}: \tikzfig{theorems/unit}}
\hstep{\tikzfig{thm_cospan_dict/row_2_implies_row_3_step_3_DET}}{Fig.\ref{fig:Flip}: \tikzfig{theorems/flip}}
\hstep[\subseteq]{\tikzfig{thm_cospan_dict/row_2_implies_row_3_step_4_DET}}{Hyp: $\Kernel{B} \subseteq \Zero$}
\hstep{\Zero}{Fig.\ref{fig:Flip}: \tikzfig{theorems/unit}}
\end{hcalculation}

(Row 3 $\implies$ Row 1)
\begin{hcalculation}[=]{\CompTwo{White, Map, A, CoMap, B, .}}
\hstep{\Kernel{B}}{$\CoKernel{A} = \Zero$}
\hstep{\CompTwo{White, Map, B, CoMap, B, .}}{$\CoKernel{B} = \Zero$}
\hstep[\subseteq]{\Zero}{Hyp: $\CompTwo{White, Map, B, CoMap, B, .} \subseteq \Zero$}
\end{hcalculation}
\end{proof}

Now we move our focus to an equivalence for existence theorem
about the constituent parts of a cospan.
These results are related to invertibility, as we will shortly see.

\begin{proposition}
  \label{prop:s_block}
  For maps $\TypedMap{A}{m}{k}\!\!$, $\TypedMap{B}{n}{k}\!\!$,
\begin{enumerate}
  \item[(i)] $\exists \Map{S_1}, \Map{S_1} \subseteq \CoSpan{A}{B}
      \iff
      \exists \Map{S_1}, \Comp{S_1/Map, B/Map} = \Map{A} $,
  \item[(ii)] $\exists \Map{S_2}, \CoMap{S_2} \subseteq \CoSpan{A}{B}
      \iff
      \exists \Map{S_2}, \Comp{S_2/Map, A/Map} = \Map{B} $.
\end{enumerate}
\end{proposition}

\begin{proof}
  We prove item (i). The other is analogous.
  \begin{enumerate}
    \item[$(\Rightarrow)$]
    \begin{hcalculation}[=]{\Comp{S_1/Map, B/Map}}
      \hstep[\subseteq]{\Comp{A/Map, B/CoMap, B/Map}}{Hyp: $\Map{S_1} \subseteq \Span{A}{B}$}
      \hstep[\subseteq]{\Map{A}}{Fig.\ref{fig:typerelations}-DET: $\Span{\;}{\;} \subseteq \Id$}
    \end{hcalculation}
    The equality follows by Lemma~\ref{lem:pair_of_matrices_1}.
    \item[$(\Leftarrow)$]
    \begin{hcalculation}[=]{\Map{S_1}}
      \hstep[\subseteq]{\Comp{S_1/Map, B/Map, B/CoMap}}{Fig.\ref{fig:typerelations}-TOT: $\Id \subseteq \CoSpan{\;}{\;}$}
      \hstep{\Comp{A/Map, B/CoMap}}{Hyp: $\Comp{S_1/Map, B/Map} = \Map{A}$}
    \end{hcalculation}
  \end{enumerate}
\end{proof}

\begin{remark}
Notice that although the statements in the previous theorem
are in terms of existential propositions,
the proofs guarantee that the quantified maps
on both sides of the ``if and only if'' are the same.
\end{remark}

Recall Definition~\ref{def:inverse} about left and right-inverses.
From Proposition~\ref{prop:s_block}, we can derive some properties about a map and its inverse.
\begin{proposition}
\label{thm:prop_linverse}
If a linear map $\Map{A}$ has a left-inverse $\Map{B}$, the following hold for all linear relations $\Rel{R}, \Rel{S}$.
\begin{enumerate}
    \item[(i)] \Map{A} is INJ,
    \item[(ii)] $\Map{A} \subseteq \CoMap{B}$,
    \item[(iii)] $\Comp{R/Rel, A/Map} = \Rel{S} \implies \Rel{R} = \Comp{S/Rel, B/Map}$.
    \item[(iv)] $\Rel{R} = \Rel{S} \iff \Comp{R/Rel, A/Map} = \Comp{S/Rel, A/Map}$.
\end{enumerate}
\end{proposition}
\begin{proof} \quad

(i)
\begin{hcalculation}[\subseteq]{\Kernel{A}}
  \hstep{\CompTwo{White, CoMap, B, CoMap, A, .}}{Fig.\ref{fig:Minimum_and_maximum}: $\Zero \subseteq \Kernel{}$}
  \hstep[=]{\Zero}{Hyp: $\Comp{A/Map, B/Map} = \Id$}
\end{hcalculation}

(ii) Proposition~\ref{prop:s_block} by taking $\Map{S} \coloneq \Map{B}$.

(iii) Here,
\begin{hcalculation}{\Rel{R}}
\hstep{\Comp{R/Rel, A/Map, B/Map}}{Hyp: $\Comp{A/Map, B/Map} = \Id$}
\hstep{\Comp{S/Rel, B/Map}}{Hyp: $\Comp{R/Rel, A/Map} = \Rel{S}$}
\end{hcalculation}

(iv) The $(\Rightarrow)$ direction is straightforward,
\begin{hcalculation}[\implies]{\Rel{R} = \Rel{S}}
\hstep{\Comp{R/Rel, A/Map} = \Comp{S/Rel, A/Map}}{Compose with $\Map{A}$}
\end{hcalculation}

For $(\Leftarrow)$,
\begin{hcalculation}[\implies]{\Comp{R/Rel, A/Map} = \Comp{S/Rel, A/Map}}
\hstep{\Comp{R/Rel, A/Map, B/Map} = \Comp{S/Rel, A/Map, B/Map}}{Compose with $\Map{B}$}
\hstep{\Rel{R} = \Rel{S}}{Hyp: $\Comp{A/Map, B/Map} = \Id$}
\end{hcalculation}


The $(\Rightarrow)$ direction is relation composition.
\end{proof}

Analogously, right-inverses satisfy a dual version of this result.
\begin{proposition}
\label{thm:prop_rinverse}
If a linear map $\Map{A}$ has a right-inverse $\Map{B}$, the following hold for all linear relations $\Rel{R}, \Rel{S}$.
\begin{enumerate}
    \item[(i)] \Map{A} is SUR,
    \item[(ii)] $\Map{A} \supseteq \CoMap{B}$,
    \item[(iii)] $\Comp{A/Map, R/Rel} = \Rel{S} \implies \Rel{R} = \Comp{B/Map, S/Rel}$.
    \item[(iv)] $\Rel{R} = \Rel{S} \iff \Comp{A/Map, R/Rel} = \Comp{A/Map, S/Rel}$.
\end{enumerate}
\end{proposition}

By joining these two results, we further establish properties for maps with inverses.
\begin{proposition}
\label{thm:prop_inverse}
If a linear map $\Map{A}$ has an inverse $\Map{B}$, the following hold for all linear relations $\Rel{R}, \Rel{S}$.
\begin{enumerate}
    \item[(i)] \Map{A} is bijective,
    \item[(ii)] $\Map{A} = \CoMap{B}$,
    \item[(iii)] $\Comp{R/Rel, A/Map} = \Rel{S} \iff \Rel{R} = \Comp{S/Rel, B/Map}$.
    \item[(iv)] $\tikzfig{thm_cospan_dict/inv_cancellation_law}$
\end{enumerate}
\end{proposition}

The next step is to connect Propositions~\ref{prop:r_block} and~\ref{prop:s_block}
through a canonical decomposition.
The usual canonical decomposition of linear maps is not sufficient in this case.
Thus, the need arises for a relational version of Equation~\eqref{eq:functional_canonical_decomposition}.
The goal of the next section is to construct such a decomposition.


\subsection{Decomposing Linear Relations}
\label{sec:decomposition_relations}

Gaussian elimination is among the most widely used tools in linear algebra.
Although it is most commonly associated with the LU matrix decomposition, Gaussian elimination also appears in other matrix factorization methods.
Particularly useful for linear relations is the \emph{Canonical Decomposition} from Equation~\eqref{eq:functional_canonical_decomposition},
which factorizes any linear map $\Map{A}$ in terms of matrices with inverses, linked by $r$ simple wires, where $r$ is the rank of $\Map{A}$.

\begin{theorem}[Canonical Decomposition]\label{thm:gaussian_elimination}
    For any linear map $\TypedMap{A}{m}{n}$ there are linear maps $\TypedInv{P}{m}{m}, \TypedInv{Q}{n}{n}$ with inverses $\Inv{P^{-1}}, \Inv{Q^{-1}}$ and a number $r \in \mathbb{N}$ such that
    \[\TypedMap{A}{m}{n} = \GaussianElim.\]
\end{theorem}
\begin{proof}
When transforming a matrix $\Map{A}$ into reduced row-echelon form (RREF) we obtain
\begin{equation}\label{eq:RREF}
    \Map{A} = \tikzfig{rcd_for_cospan/cospan/canonicaldecomposition/step1}
\end{equation}
where $\Inv{Q^{-1}}$ is an elementary matrix that has inverse and $\Inv{P_{er}}$ is a permutation matrix.
By rearranging the diagrams, we get

\begin{hcalculation}[=]{\Map{A}}
\hstep{\tikzfig{rcd_for_cospan/cospan/canonicaldecomposition/step1}}{Eq.\eqref{eq:RREF}: \Map{A} = \tikzfig{rcd_for_cospan/cospan/canonicaldecomposition/step1}}
\hstep{\tikzfig{rcd_for_cospan/cospan/canonicaldecomposition/step2}}{Fig.\ref{fig:Commutative_Comonoid}: \tikzfig{theorems/unit}}
\hstep{\tikzfig{rcd_for_cospan/cospan/canonicaldecomposition/step3}}{\tikzfig{rcd_for_cospan/cospan/canonicaldecomposition/defP}}
\hstep{\tikzfig{rcd_for_cospan/cospan/canonicaldecomposition/step4}}{Prop.\ref{thm:prop_inverse}: $\Inv{P}$ and $\Inv{Q}$ have inverses}
\end{hcalculation}
\end{proof}

The previous theorem applies only to a linear map.
To generalize it to any linear relation,
it is also necessary to reveal the information for totality and determinism.
This gives rise to a decomposition with additional associated numbers besides $r$.

This decomposition is the main tool developed in this paper and,
similarly to the Canonical Decomposition or Gaussian elimination for matrices,
is what allows us to explicitly construct solutions to relational linear equations.
Also, notice that this decomposition was already independently developed in~\cite[Lemma~48]{booth2024completeequationaltheoriesclassical}.

\begin{theorem}[Cospan Decomposition]\label{thm:rcd_for_cospan}
For all linear relations $\Rel{R}$ there are linear maps $\TypedInv{P}{m}{m}, \TypedInv{Q}{n}{n}$ with inverses $\Inv{P^{-1}}$ and $\Inv{Q^{-1}}$ and numbers $k_I, k_S, k_T, k_D, r \in \mathbb{N}$ such that
\[ \Rel{R} = \GRR.\]
\end{theorem}

\begin{proof}
\begin{hcalculation}[=]{\Rel{R}}
  \hstep{\CoSpan{A}{B}}{Prop.\ref{prop:cospan}:$\Rel{R} = \CoSpan{A}{B}$}
  \hstep{\tikzfig{rcd_for_cospan/step1}}{Thm.\ref{thm:gaussian_elimination}: Canonical Decomposition}
  \hstep{\tikzfig{rcd_for_cospan/step2}}{\tikzfig{rcd_for_cospan/defAtil} and $k_D \coloneq n - r_1$}
  \hstep{\tikzfig{rcd_for_cospan/step3}}{Fig.\ref{fig:Split}: \tikzfig{theorems/splitblack2}}
  \hstep{\tikzfig{rcd_for_cospan/step4}}{Thm.\ref{thm:gaussian_elimination}: Canonical Decomposition}
  \hstep{\tikzfig{rcd_for_cospan/step41}}{Fig.\ref{fig:typerelations}-DET: \tikzfig{theorems/whiteball}}
  \hstep{\tikzfig{rcd_for_cospan/step5}}{Fig.\ref{fig:Frobenius_Algebra}: \tikzfig{theorems/bone-white}}
  \hstep{\tikzfig{rcd_for_cospan/step6}}{Fig.\ref{fig:Flip}: \tikzfig{theorems/flipblack}}
  \hstep{\tikzfig{rcd_for_cospan/step7}}{\tikzfig{rcd_for_cospan/defAtiltil}}
  \hstep{\tikzfig{rcd_for_cospan/step71}}{Fig.\ref{fig:Commutative_Comonoid}: \tikzfig{theorems/unitblack}}
  \hstep{\tikzfig{rcd_for_cospan/step8}}{Fig.\ref{fig:Commutative_Comonoid}: \tikzfig{theorems/whitedestroyer2}  and $k_T \coloneq r_2$}
  \hstep{\tikzfig{rcd_for_cospan/step9}}{Lem.~\ref{lem:break_a}: introduce $\Map{\tilde{\tilde{A}}_{12}}$}
  \hstep{\tikzfig{rcd_for_cospan/step10}}{Thm.\ref{thm:gaussian_elimination}: Canonical Decomposition}
  \hstep{\tikzfig{rcd_for_cospan/step101}}{$k_I \coloneq m-r_2-r_3, k_S \coloneq r_1 - r_3, r \coloneq r_3$}
  \hstep{\tikzfig{rcd_for_cospan/step11}}{Prop.~\ref{thm:prop_inverse}: $\Inv{P_2}, \Inv{P_3}$ have inverses}
    \hstep{\tikzfig{rcd_for_cospan/step112}}{\tikzfig{rcd_for_cospan/defPtil}}
    \hstep{\tikzfig{rcd_for_cospan/step113}}{Prop.~\ref{thm:prop_inverse}: $\Inv{P_1}$ has inverse}
  \hstep{\tikzfig{rcd_for_cospan/step12}}{\tikzfig{rcd_for_cospan/defQtil}}
  \hstep{\tikzfig{rcd_for_cospan/step13}}{\hyperref[fig:lawsSMC]{SSM}: Rearrange wires}
  \hstep{\GRR}{\tikzfig{rcd_for_cospan/defP} \\ \tikzfig{rcd_for_cospan/defQ}}
\end{hcalculation}
\end{proof}

Notice that whenever $\Rel{R}$ is a map,
this decomposition specializes to the Canonical Decomposition from Theorem~\ref{thm:gaussian_elimination}.
Therefore, we can view it as its relational generalization.

\subsection{Characterization of Fundamental Properties in Cospan Form}
\label{sec:applications_characterization}

With a relational decomposition, we can complete the characterization of fundamental properties mentioned in Section~\ref{sec:fundamental_properties}.
More concretely, the decomposition unveils the equivalence between Propositions~\ref{prop:r_block} and~\ref{prop:s_block}.
We then build a graph in Section~\ref{sec:aplications_poset_of_fundamental_properties} that relates the fundamental properties to each other,
followed by Section~\ref{sec:applications_characterization} on applications, i.e., the Rank Lemma and the connection between the pigeonhole principle and the Exchange Lemma.
This completes the generalization of the Invertible Matrix Theorem.
Section~\ref{sec:applications_invertible_matrix_theorem} shows how the usual IMT is obtained as a particular case.

The next step in the characterization is connecting the abstract notions of injectivity, surjectivity, determinism, and totality to a structural property: the vanishing of wires after decomposing the relation.

\begin{lemma}
\label{lem:no_lolipop}
Let \Rel{R} be decomposed as in Theorem~\ref{thm:rcd_for_cospan}, i.e.,
\[ \Rel{R} = \GRR.\]
The vanishing of central wires determines its fundamental properties.
\begin{center}
\renewcommand{\arraystretch}{2.0}
\begin{tabular}{lcr|lcr}
        \Rel{R} is INJ   & $\iff$ & $k_I = 0$
        & \Rel{R} is SUR & $\iff$ & $k_S = 0$  \\
        \Rel{R} is TOT   & $\iff$ & $k_T = 0$
        & \Rel{R} is DET & $\iff$ & $k_D = 0$
\end{tabular}
\end{center}
\end{lemma}

\begin{proof} We prove the TOT case. The other three cases are proven similarly.
\begin{hcalculation}[\iff]{\Discard \subseteq \Diagram{Rel}{R}{.}{Black}}
\hstep{\tikzfig{no_lolipop/step2}}{Thm.~\ref{thm:rcd_for_cospan}: Cospan Decomposition}
\hstep{\tikzfig{no_lolipop/step3}}{Fig.\ref{fig:typerelations}-SUR: \tikzfig{theorems/blackballinv}}
\hstep{\tikzfig{no_lolipop/step4}}{Fig.\ref{fig:Frobenius_Algebra}: \tikzfig{theorems/bone} $+$ \ref{fig:Bialgebra}: \tikzfig{theorems/bonecolor}}
\hstep{\tikzfig{no_lolipop/step5}}{Prop.~\ref{thm:prop_inverse}: $\Inv{P}$ has inverse}
\hstep{\tikzfig{no_lolipop/step6}}{Fig.\ref{fig:typerelations}-SUR: \tikzfig{theorems/blackballinv}}
\hstep{\TypedDiscard{k_3} \subseteq \TypedCoZero{k_3}}{Remove \Discard from both sides}
\hstep{k_T = 0}{Fig.\ref{fig:Inequality}: $\TypedDiscard{n} \subseteq\TypedCoZero{n} \iff n=0$}
\end{hcalculation}
\end{proof}

Now we are ready to prove that all statements in Lemma~\ref{lem:no_lolipop}
and Propositions~\ref{prop:r_block} and~\ref{prop:s_block} are equivalent.
This is a generalization of the Invertible Matrix Theorem
giving a series of alternative characterization for each fundamental property.

\begin{theorem}[Characterization of fundamental properties]\label{thm:cospan_dict}
  Let \CoSpan{A}{B} be decomposed as in Theorem~\ref{thm:rcd_for_cospan}, i.e.,
  \[ \CoSpan{A}{B} = \GRR.\]
  Each column of the following table is a series of equivalent statements.
  \begin{center}
    \resizebox{\textwidth}{!}{\tikzfig{thm_cospan_dict/stat_1}}
  \end{center}
\end{theorem}
\begin{proof}
  We only prove the TOT case (second column), the other can be proven similarly.
  The equivalence between rows~1,~5, and~6
  is the subject of Proposition~\ref{prop:r_block},
  whereas rows~3 and~4 are equivalent by Proposition~\ref{prop:s_block}.
  Therefore, it only remains to show that rows~1,~2 and~3 are equivalent.
  The strategy is Row 1 $\Rightarrow$ Row 2 $\Rightarrow$ Row 3 $\Rightarrow$ Row 1.

  (Row 1 $\implies$ Row 2) Lemma~\ref{lem:no_lolipop}.

  (Row 2 $\implies$ Row 3)
    \begin{hcalculation}[=]{\CoSpan{A}{B}}
      \hstep{\GRR}{Thm.~\ref{thm:rcd_for_cospan}: Cospan Decomposition}
      \hstep{\tikzfig{thm_cospan_dict/2_implies_3_step_3}}{Hyp: $k_T = 0$}
      \hstep[\supseteq]{\tikzfig{thm_cospan_dict/2_implies_3_step_4}}{Fig.\ref{fig:Inequality}: $\Zero \subseteq \CoDiscard$}
      \hstep[\supseteq]{\tikzfig{thm_cospan_dict/2_implies_3_step_5}}{Prop.~\ref{thm:prop_inverse}: $\Inv{Q}$ has inverse}
      \hstep{\Map{S}}{$\Map{S} \coloneq
      \tikzfig{thm_cospan_dict/2_implies_3_step_4}$}
    \end{hcalculation}

  (Row 3 $\implies$ Row 1)
    \begin{hcalculation}[\supseteq]{\CompTwo{., Map, A, CoMap, B, Black}}
      \hstep{\Diagram{Map}{S}{.}{Black}}{Hyp: $\CoSpan{A}{B} \supseteq \Map{S}$}
      \hstep[=]{\Discard}{Fig.\ref{fig:typerelations}-TOT: $\Diagram{Map}{}{.}{Black} = \Discard$}
    \end{hcalculation}
\end{proof}

\begin{remark}
  Theorem~\ref{prop:s_block} only proved existence results,
  without a procedure to construct the maps $\Map{S}$ relating the cospan terms.
  In contrast to that, the previous theorem uses the relational decomposition
  to explicitly construct the maps.
\end{remark}


\subsection{Graph of Fundamental Properties}
\label{sec:aplications_poset_of_fundamental_properties}

We now discuss how the fundamental properties relate to each other.
This result can also be visualized as the graph in Figure~\ref{fig:poset_of_fundamental_properties}.

\begin{figure}[thb]
  \centering
  \tikzfig{poset_of_fundamental_properties/poset}
  \caption{Graph of fundamental properties.
   The way to read the graph is as follows: given a node $Y$ at the top, let $X_1, X_2, ..., X_k$ be all the nodes at the bottom you can reach by following the paths starting at $Y$. Then, it holds that $X_1, ..., X_k \implies Y$. For example, if we start at the node $(m \leq n)$ at the top and follow the edges downwards, we reach the nodes TOT and INJ at the bottom, so it holds that TOT + INJ $\implies m \leq n$, which is statement (v) of Theorem~\ref{thm:poset_of_fundamental_properties}.}
  \label{fig:poset_of_fundamental_properties}
\end{figure}

\begin{theorem}
  \label{thm:poset_of_fundamental_properties}
  For a relation $\Rel{R}$, the following hold\newline
\begin{tabular}{rll}
(i)   & $R$ is TOT, INJ, SUR and $m \geq n$ & $\implies R$ is DET; \\
(ii)  & $R$ is DET, INJ, SUR and $m \leq n$ & $\implies R$ is TOT; \\
(iii) & $R$ is DET, TOT, INJ and $m \geq n$ & $\implies R$ is SUR; \\
(iv)  & $R$ is DET, TOT, SUR and $m \leq n$ & $\implies R$ is INJ; \\
(v)   & $R$ is TOT and INJ                  & $\implies m \leq n$; \\
(vi)  & $R$ is DET and SUR                  & $\implies m \geq n$.
\end{tabular}
\end{theorem}
\begin{proof} We prove (i) and (iii). The other arguments are analogous.

(i)
\begin{hcalculation}[=]{\TypedRel{R}{m}{n}}
\hstep{\GRR}{Thm.~\ref{thm:rcd_for_cospan}: Cospan Decomposition}
\hstep{\tikzfig{poset_of_fundamental_properties/step3}}{Lem.~\ref{lem:no_lolipop}: $k_S = k_T = k_D = 0$}
\hstep[\implies]{n + k_I = m}{$\Inv{P}$ and $\Inv{Q}$ are square}
\hstep[\implies]{k_I = 0}{$m \leq n$ and $k_I \geq 0$}
\hstep[\implies]{\Rel{R} \text{ is INJ}}{Lem.~\ref{lem:no_lolipop}}
\end{hcalculation}

(iii)
\begin{hcalculation}[=]{\TypedRel{R}{m}{n}}
\hstep{\GRR}{Thm.~\ref{thm:rcd_for_cospan}: Cospan Decomposition}
\hstep{\tikzfig{poset_of_fundamental_properties/step8}}{Lem.~\ref{lem:no_lolipop}: $k_I = k_T = 0$}
\hstep[\implies]{m = r \text{ and } r + k_T + k_D = n}{$\Inv{P}$ and $\Inv{Q}$ are square}
\hstep[\implies]{m + k_T + k_D = n}{Substitute $m = r$}
\hstep[\implies]{m \leq n}{$k_T, k_D \geq 0$}
\end{hcalculation}
\end{proof}

We are ready to state the equivalence between the Pigeonhole Principle and the Exchange Lemma discussed in Section~\ref{sec:pigeonhole}. As it is usually formulated, the Exchange Lemma states that if $A \subseteq \K^k$ is a set of $m$ linearly independent vectors and $B \subseteq \K^k$ is a set of $n$ spanning vectors, then $m \leq n$. As a slightly cleaner way of saying this, we may arrange the vectors in $A$ (resp. $B$) as columns of a matrix. We arrive at the following.
\begin{center}
\renewcommand{\arraystretch}{2.0}
\begin{tabular}{r l}
    (Exchange Lemma) &$\TypedMap{A}{m}{k}$ is INJ and $\;\TypedImage{B}{n}{k}\!\!\! \supseteq \Image{A} \implies m \leq n$. \\
    (Pigeonhole Principle) &$\TypedCoSpan{A}{B}{m}{k}{n}$ is TOT and INJ $ \implies m \leq n$.
\end{tabular}
\end{center}
We proceed to prove that their hypotheses are equivalent.
\begin{proposition}
Let $\TypedMap{A}{m}{k}, \TypedMap{B}{n}{k}$ be maps, then, the following are equivalent.
\begin{enumerate}
    \item[(i)] $\Map{A}$ is INJ and $\;\Image{B} \!\!\!\supseteq \Image{A}$,
    \item[(ii)] $\CoSpan{A}{B}$ is TOT and INJ.
\end{enumerate}
\end{proposition}
\begin{proof} This is a simple application of the characterization in Theorem~\ref{thm:cospan_dict}.
\end{proof}


\subsection{Invertible Matrix Theorem}
\label{sec:applications_invertible_matrix_theorem}

The \emph{Invertible Matrix Theorem} (IMT) is a central result in linear algebra
enumerating equivalent characterizations for a square matrix to have an inverse.
Theorem~\ref{thm:cospan_dict} characterizes a relation according to its fundamental properties,
and functions as a relational generalization of the IMT.
This section recovers a characterization of matrices with inverse
from Theorem~\ref{thm:cospan_dict} by applying it to the map $\Map{A}$
viewed as a cospan, i.e., with $\CoMap{B} = \Id$.
This way, the theorem specializes to a characterization of left and right-invertibility.
Following Section~\ref{sec:introduction},
we first characterize injective and surjective maps separately
and then link both results for square maps.

\begin{theorem}[Surjective/Injective Matrix Theorem]
\label{thm:ivm_sur_inj}
For a linear map \Map{A},
each column of the following table is a series of equivalent statements.
\begin{center}
\tikzfig{thm_surjective_injective_matrix/stat_1}
\end{center}
\end{theorem}
\begin{proof}
  Apply Theorem~\ref{thm:cospan_dict} to the cospan \Map{A}.
\end{proof}

For square matrices, all these statements are, in fact, equivalent.
This next result is the Invertible Matrix Theorem,
and comes as a consequence of the graph of fundamental properties.

\begin{theorem}[Invertible Matrix Theorem]
\label{thm:ivm_inv}
For a square map $\TypedMap{A}{n}{n}$,
all statements in Theorem~\ref{thm:ivm_sur_inj} are equivalent.
\end{theorem}
\begin{proof}
  From Theorem~\ref{thm:poset_of_fundamental_properties}, $\Map{A}$ is INJ if and only if it is SUR.
\end{proof}

Finally, it is worth noticing that, as corollary,
the IMT provides converse to Propositions~\ref{thm:prop_linverse},~\ref{thm:prop_rinverse} and~\ref{thm:prop_inverse}.
While those propositions conclude a fundamental property from the maps' invertibility,
we can now assure the existence of an inverse from these properties.

\begin{corollary}[Existence of Inverse]
  Consider a map $\Map{A}$. From the IMT, it is\newline
    \begin{tabular}{rrcl}
      (i)   & injective  &$\iff$ &it has a left-inverse; \\
      (ii)  & surjective &$\iff$ &it has a right-inverse; \\
      (iii) & bijective  &$\iff$ &it has an inverse.
    \end{tabular}
\end{corollary}

\section{Pair of Matrices and their Relevant Subspaces}

The Generalized Singular Value Decomposition (GSVD)~\cite{gsvd1976,gsvd1981}
is a powerful tool for simultaneously decomposing a pair of matrices.
Unfortunately, similarly to the usual SVD, it only works for the fields of real and complex numbers.
In this section, we show that the relational decomposition from Theorem~\ref{thm:rcd_for_cospan}
induces a similar simultaneous matrix decomposition that works without assumptions on the underlying scalar field.
We finish this section by showing how this decomposition can compute the main subspaces involving any pair of linear maps.

\subsection{Decomposing Pairs of Matrices}
\label{sec:decomposition_matrices}

Theorem~\ref{thm:rcd_for_cospan} provides a decomposition for any cospan $\CoSpan{A}{B}$.
Since these are formed by two linear maps, a question arises: can we turn it into mutual---but separate---decompositions for its constituent factors $\Map{A}$ and $\Map{B}$?
The answer is positive, requiring the introduction of another linear map $\Map{H}$ connecting both factorizations. We construct the needed map in Theorem~\ref{thm:abstract} and associate it with the linear decomposition in Theorem~\ref{thm:rcd_for_pairs_of_matrices}.

The next theorem is this section's workhorse
in that, it constructs a linking relation with all the properties needed for the main result. It presents well-known hypothesis when dealing with spans and cospans~\cite{freyd1990categories}.

\begin{theorem}\label{thm:abstract}
For all $\CoSpan{A}{B}=\CoSpan{D_1}{D_2}$
where \tikzfig{rcd_for_pairs_of_matrices/TheoremAbstract/D1D2} is surjective,
let
\begin{equation}
\Rel{H}=\tikzfig{rcd_for_pairs_of_matrices/TheoremAbstract/defH}
\end{equation}
The relation just defined satisfies
\begin{enumerate}
    \item[(i)] \Rel{H} is deterministic;
    \item[(ii)] \Rel{H} is total;
    \item[(iii)] \Rel{H} is injective;
    \item[(iv)] \Rel{H} is surjective if \tikzfig{rcd_for_pairs_of_matrices/TheoremAbstract/AB} is surjective;
    \item[(v)] \tikzfig{rcd_for_pairs_of_matrices/TheoremAbstract/defAeB_Rel};
    \item[(vi)] \Rel{H} is the unique linear relation satisfying (i), (ii) and (v).
\end{enumerate}
\end{theorem}

\begin{proof}
(i) \Rel{H} is deterministic.
\begin{hcalculation}[=]{\tikzfig{rcd_for_pairs_of_matrices/TheoremAbstract/item1/hSV_step1}}
\hstep{\tikzfig{rcd_for_pairs_of_matrices/TheoremAbstract/item1/hSV_step2}}{Def. $\Rel{H}$}
\hstep{\tikzfig{rcd_for_pairs_of_matrices/TheoremAbstract/item1/hSV_step3}}{Lemma~\ref{lemma:abstract}}
\hstep[\subseteq]{\tikzfig{rcd_for_pairs_of_matrices/TheoremAbstract/item1/hSV_step4}}{Fig.\ref{fig:typerelations}-DET: \tikzfig{theorems/detid}}
\hstep{\Zero}{Fig.\ref{fig:Frobenius_Algebra}: \tikzfig{theorems/frobenius}}
\end{hcalculation}

(ii) \Rel{H} is total.
\begin{hcalculation}[=]{\tikzfig{rcd_for_pairs_of_matrices/TheoremAbstract/item1/hTOT_step1}}
\hstep{\tikzfig{rcd_for_pairs_of_matrices/TheoremAbstract/item1/hTOT_step2}}{Def. $\Rel{H}$}
\hstep{\tikzfig{rcd_for_pairs_of_matrices/TheoremAbstract/item1/hTOT_step3}}{Fig.\ref{fig:Bialgebra}: \tikzfig{theorems/blackdestroyer}}
\hstep{\tikzfig{rcd_for_pairs_of_matrices/TheoremAbstract/item1/hTOT_step4}}{Fig.\ref{fig:typerelations}-TOT: \tikzfig{theorems/blackball}}
\hstep[\supseteq ]{\tikzfig{rcd_for_pairs_of_matrices/TheoremAbstract/item1/hTOT_step5}}{Fig.\ref{fig:typerelations}-SUR: \tikzfig{theorems/sur}}
\end{hcalculation}

(iii) \Rel{H} is injective.
\begin{hcalculation}[=]{\tikzfig{rcd_for_pairs_of_matrices/TheoremAbstract/item4/step1}}
\hstep{\tikzfig{rcd_for_pairs_of_matrices/TheoremAbstract/item4/step2}}{Def. $\Rel{H}$}
\hstep{\tikzfig{rcd_for_pairs_of_matrices/TheoremAbstract/item4/step3}}{Lemma~\ref{lemma:abstract}}
\hstep{\tikzfig{rcd_for_pairs_of_matrices/TheoremAbstract/item4/step4}}{Fig.\ref{fig:typerelations}-SUR: \tikzfig{theorems/surideq}}
\hstep{\CoZero}{Fig.\ref{fig:Frobenius_Algebra}: \tikzfig{theorems/frobenius}}
\end{hcalculation}

(iv) If \tikzfig{rcd_for_pairs_of_matrices/TheoremAbstract/AB} is surjective, then so is \Rel{H}.
\begin{hcalculation}[=]{\tikzfig{rcd_for_pairs_of_matrices/TheoremAbstract/item5/step1}}
\hstep{\tikzfig{rcd_for_pairs_of_matrices/TheoremAbstract/item5/step2}}{Def. $\Rel{H}$}
\hstep{\tikzfig{rcd_for_pairs_of_matrices/TheoremAbstract/item5/step3}}{Fig.\ref{fig:Bialgebra}: \tikzfig{theorems/blackdestroyer}}
\hstep{\tikzfig{rcd_for_pairs_of_matrices/TheoremAbstract/item5/step4}}{Fig.\ref{fig:typerelations}-TOT: \tikzfig{theorems/blackball}}
\hstep{\CoDiscard}{Hyp: \ref{fig:typerelations}-SUR: \tikzfig{theorems/sureq}}
\end{hcalculation}

(v) We show $\Map{A} \subseteq \Comp{D_1/Map, H/Rel}$.
The proof for $\Map{B}$ is similar.
The equalities follow from Lemma~\ref{lem:pair_of_matrices_1} and items~(i) and~(ii) of this theorem.
\begin{hcalculation}[=]{\tikzfig{rcd_for_pairs_of_matrices/TheoremAbstract/item2/step1}}
\hstep{\tikzfig{rcd_for_pairs_of_matrices/TheoremAbstract/item2/step2}}{Def. $\Rel{H}$}
\hstep[\supseteq]{\tikzfig{rcd_for_pairs_of_matrices/TheoremAbstract/item2/step3}}{Fig.\ref{fig:Minimum_and_maximum}: \tikzfig{theorems/zero}}
\hstep{\tikzfig{rcd_for_pairs_of_matrices/TheoremAbstract/item2/step4}}{Fig.\ref{fig:Commutative_Comonoid}: \tikzfig{theorems/unit}}
\hstep[\supseteq]{\Map{A}}{Fig.\ref{fig:typerelations}-TOT: \tikzfig{theorems/totalid1}}
\end{hcalculation}

(vi) Lastly, we show $\Rel{H}$ is the unique map satisfying (v).
Suppose we have two maps $\Map{H_1}, \Map{H_2}$ satisfying (v). Then,
\begin{hcalculation}[\iff]{\tikzfig{rcd_for_pairs_of_matrices/TheoremAbstract/item3/step1}}
\hstep{\tikzfig{rcd_for_pairs_of_matrices/TheoremAbstract/item3/step12}}{Fig.\ref{fig:Connect_sum}: \tikzfig{theorems/connect}}
\hstep{\tikzfig{rcd_for_pairs_of_matrices/TheoremAbstract/item3/step2}}{Fig.\ref{fig:Flip}: \tikzfig{theorems/flip2}}
\hstep[\implies]{\tikzfig{rcd_for_pairs_of_matrices/TheoremAbstract/item3/step3}}{Multiply \tikzfig{rcd_for_pairs_of_matrices/TheoremAbstract/D1D21}}
\hstep{\tikzfig{rcd_for_pairs_of_matrices/TheoremAbstract/item3/step5}}{Fig.\ref{fig:typerelations}-SUR: \tikzfig{theorems/surideq}}
\hstep{\tikzfig{rcd_for_pairs_of_matrices/TheoremAbstract/item3/step4}}{Fig.\ref{fig:Frobenius_Algebra}: \tikzfig{theorems/frobenius}}
\end{hcalculation}
This concludes the proof.
\end{proof}

\begin{remark}
  Theorem~\ref{thm:abstract} implies that the relation $\Rel{H}$ is a linear map.
  Therefore, we from now on denote it as $\Map{H}$.
\end{remark}

We proceed to show how to attain a decomposition for a pair $\Map{A}$ and $\Map{B}$ given the relational decomposition of the cospan form $\CoSpan{A}{B}$. For this, we need Definition~\ref{def:D1_e_D2} and Lemma~\ref{lem:D1_D2sur} below.

\begin{definition}
  \label{def:D1_e_D2}
  For maps $\Map{A}$, $\Map{B}$ decomposed as in Theorem~\ref{thm:rcd_for_cospan},
  \[\TypedRel{R}{m}{n} = \GRR,\]
  define the auxiliary maps $\Map{D_1}$ and $\Map{D_2}$ as
  \[
    \begin{aligned}
      \tikzfig{rcd_for_pairs_of_matrices/defD1} & &
      \tikzfig{rcd_for_pairs_of_matrices/defD2}
    \end{aligned}
  \]
\end{definition}

\begin{lemma}\label{lem:D1_D2sur} The maps \Map{D_1} and \Map{D_2} of Definition~\ref{def:D1_e_D2} are such that $\tikzfig{rcd_for_pairs_of_matrices/step22}$ is surjective.
\end{lemma}
\begin{proof}
\begin{hcalculation}[=]{\tikzfig{rcd_for_pairs_of_matrices/step2}}
\hstep{\tikzfig{rcd_for_pairs_of_matrices/step3}}{Def.\ref{def:D1_e_D2}: $\Map{D_1}$ and $\Map{D_2}$}
%\hstep{\tikzfig{rcd_for_pairs_of_matrices/step4}}{\tikzfig{theorems/bone} $+$ \tikzfig{theorems/unit}}
\hstep{\CoDiscard}{Fig.\ref{fig:Frobenius_Algebra}: \tikzfig{theorems/bone} $+$ \ref{fig:Commutative_Comonoid}: \tikzfig{theorems/unit}}
\end{hcalculation}
\end{proof}

We have all the necessary tools for simultaneously decomposing a matrix pair.
The next results puts together Theorems~\ref{thm:rcd_for_cospan} and~\ref{thm:abstract}
to construct the desired matrix decompositions.


\begin{theorem}[Pair Decomposition]
  \label{thm:rcd_for_pairs_of_matrices}
Let $\Rel{R}=\CoSpan{A}{B}$ be decomposed as in Theorem~\ref{thm:rcd_for_cospan}, i.e.,
\[\TypedRel{R}{m}{n} = \GRR.\]
Then, there exists a unique injective linear map $\Map{H}$ such that
\[\tikzfig{rcd_for_pairs_of_matrices/stat}\]
\end{theorem}

\begin{proof} Note that
\begin{hcalculation}[\iff]{\TypedCoSpan{A}{B}{m}{}{n} = \GRR}
\hstep{\CoSpan{A}{B} = \tikzfig{rcd_for_pairs_of_matrices/step1}}{\hyperref[fig:lawsSMC]{SSM}: Rearrange wires}
\hstep{\CoSpan{A}{B}=\tikzfig{rcd_for_pairs_of_matrices/step12}}{Def.\ref{def:D1_e_D2}: $\Map{D_1}$ and $\Map{D_2}$}
\hstep{\tikzfig{rcd_for_pairs_of_matrices/step11} = \CoSpan{D_1}{D_2}}{Prop.~\ref{thm:prop_inverse}: $\Inv{P}$, $\Inv{Q}$ have inverses.}
\hstep{\CoSpan{\tilde{A}}{\tilde{B}} = \CoSpan{D_1}{D_2}}{$\tikzfig{rcd_for_pairs_of_matrices/defAtil}$ \\ $\tikzfig{rcd_for_pairs_of_matrices/defBtil}$}
\end{hcalculation}

By Lemma~\ref{lem:D1_D2sur} and Theorem~\ref{thm:abstract},
there exists a unique injective linear map
\[\Map{H}=\tikzfig{rcd_for_pairs_of_matrices/defH},\]
and, from Theorem~\ref{thm:abstract} item (v),
\begin{hcalculation}[\iff]{\tikzfig{rcd_for_pairs_of_matrices/step5}}
\hstep{\tikzfig{rcd_for_pairs_of_matrices/step6}}{Def. $\Map{\tilde{A}}$ and $\Map{\tilde{B}}$}
\hstep{\tikzfig{rcd_for_pairs_of_matrices/step7}}{Multiply $\tikzfig{rcd_for_pairs_of_matrices/step9}$}
\hstep{\tikzfig{rcd_for_pairs_of_matrices/step8}}{Prop.~\ref{thm:prop_inverse}: $\Inv{P}$ and $\Inv{Q}$ have inverses}
\hstep{\tikzfig{rcd_for_pairs_of_matrices/stat}}{Def. $\Map{D_1}$ and $\Map{D_2}$}
\end{hcalculation}
\end{proof}


\subsection{Calculating Subspaces}
\label{sec:applications_subspaces}
For a pair of matrices $A$ and $B$ with the same amount of rows,
a common problem in linear algebra is to find bases for the sum $\im(A) + \im(B)$ and the intersection $\im(A) \cap \im(B)$.
One method to calculate them is the Zassenhaus Algorithm~\cite{Fischer2012}. It consists of putting the matrix $\begin{bsmallmatrix} A & A \\ B & 0 \end{bsmallmatrix}$ in row-echelon form. The bases are then constructed from the coefficients of the resulting matrix. In this section, we show that the matrix $H$ obtained in Theorem~\ref{thm:abstract} is an alternative method for calculating such bases. Its columns contain not only bases for the sum and intersection but also bases for other relevant subspaces obtained from $\im(A)$ and $\im(B)$ using the standard operations on vector spaces---sum, intersection, and complement, as defined below.

\begin{definition}
    Given subspaces $\Subspace{A}$, $\Subspace{B}$,
    define their
    \begin{enumerate}
      \item[(i)] \textbf{Sum:}
          $\tikzfig{subspaces/AB_sum}      = \left\{a+b \mid a \in A \;\text{and}\; b \in B \right\}$,
        \item[(ii)] \textbf{Intersection:}
          $\tikzfig{subspaces/AB_intersec} = \left\{x \mid x \in A \;\text{and}\; x \in B \right\}$,
        \item[(iii)] \textbf{Complement:}
          \emph{$\Subspace{A}$ complements $\Subspace{B}$ wrt a subspace $\Subspace{C}$} whenever
        their sum is $\Subspace{C}$ and their intersection is zero, i.e.,
        \[
          \Subspace{A + B} = \Subspace{C} \;\text{ and }\; \Subspace{A \cap B} = \Zero.
        \]
    \end{enumerate}
\end{definition}

\begin{theorem}\label{thm:subspaces}
Let $A, B$ be matrices and $\mathcal{A} \coloneq \im(A)$,
$\mathcal{B} \coloneq \im(B)$.
Then, we have the following equalities,
where the complements are with respect to $\mathcal{A} + \mathcal{B}$.
\begin{center}
\bgroup
\renewcommand*{\arraystretch}{2.0}
\begin{tabular}{crcl crcl}
$(i)$   & $\mathcal{A}$                  & $=$ & \tikzfig{subspaces/imAH}        & $(v)$         & $\mathcal{A}$ is complemented by               & \; & \tikzfig{subspaces/b-a} \\
$(ii)$  & $\mathcal{B}$                  & $=$ & \tikzfig{subspaces/imBH}        & $(vi)$        & $\mathcal{B}$ is complemented by  & \;           & \tikzfig{subspaces/a-b} \\
$(iii)$ & $\mathcal{A} \cap \mathcal{B}$ & $=$ & \tikzfig{subspaces/intersecH}   & $(vii)$       & $\mathcal{A} \cap \mathcal{B}$ is complemented by   & \; & \tikzfig{subspaces/ab-a-b} \\
$(iv)$  & $\mathcal{A}+\mathcal{B}$      & $=$ & \tikzfig{subspaces/sumH} & $(viii)$ & $\left \{ 0 \right \}$    & $=$ & \tikzfig{subspaces/empty}
\end{tabular}
\egroup
\end{center}
\end{theorem}

\begin{proof}
\begin{itemize}
\item[(i)]\textbf{[$\mathcal{A}$] }
\begin{hcalculation}[=]{\Image{A}}
\hstep{\tikzfig{subspaces/imAstep1}}{Thm.~\ref{thm:rcd_for_pairs_of_matrices}}
\hstep{\tikzfig{subspaces/imAH}}{Fig.\ref{fig:typerelations}-SUR: \tikzfig{theorems/blackballinv}}\end{hcalculation}

\item[(ii)] [\textbf{$\mathcal{B}$}] Analogous to item~(i).

\item[(iii)]\textbf{[$\mathcal{A} \cap \mathcal{B}$]}
\begin{hcalculation}[=]{\tikzfig{subspaces/intersec}}
\hstep{\tikzfig{subspaces/intersecstep2}}{Items (i) and $(ii)$}
\hstep{\tikzfig{subspaces/intersecstep4}}{Fig.\ref{fig:typerelations}-INJ: \tikzfig{theorems/flip3}}
\hstep{\tikzfig{subspaces/intersecstep5}}{Fig.\ref{fig:Commutative_Comonoid}: \tikzfig{theorems/unitblack} $+$ \ref{fig:Bialgebra}: \tikzfig{theorems/whitedestroyer} $+$ \ref{fig:Bialgebra}: \tikzfig{theorems/bonecolor}}
\end{hcalculation}

\item[(iv)]\textbf{[$\mathcal{A} + \mathcal{B}$]}
\begin{hcalculation}[=]{\tikzfig{subspaces/add_1}}
\hstep{\tikzfig{subspaces/add_2}}{Items (i) and $(ii)$}
\hstep{\tikzfig{subspaces/add_3}}{\ref{fig:Flip}: \tikzfig{theorems/flip2}}
\hstep{\tikzfig{subspaces/add_4}}{\ref{fig:Commutative_Comonoid}: \tikzfig{theorems/unit} $+$ \ref{fig:Bialgebra}: \tikzfig{theorems/blackdestroyer} $+$ \ref{fig:Bialgebra}: \tikzfig{theorems/bonecolor}}
\end{hcalculation}

\item[(v)]\textbf{[Complement of $\mathcal{A}$ w.r.t. $\mathcal{A} + \mathcal{B}$]}

    \begin{hcalculation}[=]{\tikzfig{subspaces/a-intersecstep1}}
\hstep{\tikzfig{subspaces/a-intersecstep2}}{\ref{fig:Flip}: \tikzfig{theorems/whitefront}}
\hstep{\tikzfig{subspaces/sumH}}{Fig.\ref{fig:Commutative_Comonoid}:\tikzfig{theorems/unit}}
\end{hcalculation}

\noindent A similar calculation demonstrates \[\tikzfig{subspaces/a-intersec2step1} = \tikzfig{subspaces/empty},\] proving that $\tikzfig{subspaces/b-a}$ is the complement of $\mathcal{A}$ with respect to $\mathcal{A} + \mathcal{B}$.

\item[(vi)]\textbf{[Complement of $\mathcal{B} $ w.r.t. $\mathcal{A} + \mathcal{B}$]}  Analogous to item~(v).

\item[(vii)]\textbf{[Complement of $\mathcal{A}\cap \mathcal{B}$ w.r.t. $\mathcal{A} + \mathcal{B}$]} Analogous to item~(v).

\item [(vii)] \textbf{[$\left \{ 0 \right \}$]}
\begin{hcalculation}[=]{\tikzfig{subspaces/empty}}
\hstep{\Zero}{Fig.\ref{fig:typerelations}-DET: \tikzfig{theorems/whiteball}}
\end{hcalculation}
\end{itemize}
\end{proof}

From Theorem \ref{thm:subspaces}, it is possible to obtain all relevant subspaces of $\im(A)$ and $\im(B)$ through $H$.


\appendix

\section{Axioms and Theorems in Graphical Notation}
\label{sec:gla_thms}

This appendix presents the linear relations theorems in the graphical notation used throughout the paper. Only the theorems necessary for the presented proofs are recorded here. The complete presentation can be found in~\cite{PAIXAO2022}. In order to maintain the focus of this paper, most of the proofs in this section will be omitted, as they can be found in the same reference.

\paragraph*{Symmetric Strict Monoidal Categories}
\label{sec:SSM}
Raw terms are quotiented with respect to the laws of symmetric strict monoidal (SSM) categories, summarized in Fig.~\ref{fig:lawsSMC}. We omit the (well-known) details~\cite{selinger2010survey} here and mention only that this amounts to eschewing the need for ``dotted line boxes''
and ensuring that diagrams with the same topological connectivity are equated.

\begin{figure}[h]
\begin{center}
\[
 \tikzfig{SSM/sequential-associativity} = \tikzfig{SSM/sequential-associativity-1}
\]
%
\[
\tikzfig{SSM/interchange-law} = \tikzfig{SSM/interchange-law-1}
 \qquad
 \tikzfig{SSM/parallel-associativity} = \tikzfig{SSM/parallel-associativity-1}
\]
\[
\tikzfig{SSM/unit-right2} = \tikzfig{SSM/c}
%\smallubox{c}
= \tikzfig{SSM/unit-left2}
\quad\quad
  \tikzfig{SSM/parallel-unit-above} = \tikzfig{SSM/c} =  \tikzfig{SSM/parallel-unit-below}
\]
\[
\tikzfig{SSM/sym-natural} = \tikzfig{SSM/sym-natural-1}
\qquad
\tikzfig{SSM/sym-iso} = \tikzfig{SSM/id2}
\]
%\end{eqnarray*}
\caption{Laws of Symmetric Strict Monoidal (SSM) Categories.}
\label{fig:lawsSMC}
\end{center}
\end{figure}

In graphical notation, the diagrams are closed under two symmetries: \emph{Mirror-Image} and \emph{Color-Swap}.
The theorems and their corresponding symmetries are summarized in Figures~\ref{fig:Commutative_Comonoid} to~\ref{fig:Minimum_and_maximum}.
Finally, Figure~\ref{fig:typerelations} outlines statements determining whether a relation is classified as total (TOT), deterministic (DET), surjective (SUR), or injective (INJ).

\begin{lemma}
  \label{lem:pair_of_matrices_1}
  Let $\Map{A}$ and $\Map{B}$ be two linear maps, then
  \[\tikzfig{rcd_for_pairs_of_matrices/Lemma1/lemma}.\]
\end{lemma}
\begin{proof}
  \begin{hcalculation}[\subseteq]{\Map{B}}
    \hstep{\tikzfig{rcd_for_pairs_of_matrices/Lemma1/proof1}}{Fig.\ref{fig:typerelations}-TOT: \tikzfig{theorems/totalid}}
    \hstep{\tikzfig{rcd_for_pairs_of_matrices/Lemma1/proof2}}{Hyp: $\Map{A}\subseteq \Map{B}$}
    \hstep{\Map{A}}{Fig.\ref{fig:typerelations}-DET: \tikzfig{theorems/detid}}
  \end{hcalculation}
\end{proof}

\begin{lemma}
\label{lem:break_a}
For all maps \Map{A}, there exists a map \Map{A_2} such that
\[ \tikzfig{rcd_for_cospan/lemma_break/lemma_hyp1} = \tikzfig{rcd_for_cospan/lemma_break/lemma_hyp2}.\]
\end{lemma}
\begin{proof}
\begin{hcalculation}[=]{\tikzfig{rcd_for_cospan/lemma_break/lemma_hyp1}}
    \hstep{\tikzfig{rcd_for_cospan/lemma_break/step1}}{Fig.\ref{fig:Split}: \tikzfig{theorems/split}}
    \hstep{\tikzfig{rcd_for_cospan/lemma_break/step2}}{Fig.\ref{fig:typerelations}-DET: \tikzfig{theorems/deteq}}
    \hstep{\tikzfig{rcd_for_cospan/lemma_break/lemma_hyp2}}{Fig.\ref{fig:Commutative_Comonoid}: \tikzfig{theorems/unit}}
  \end{hcalculation}
\end{proof}

\begin{lemma}
\label{thm:subspace_image}
    For every subspace $\tikzfig{rcd_for_cospan/cospan/subspace}$, there exist linear maps $\Map{X_1}$ and $\Map{X_2}$ such that
\begin{enumerate}
    \item[(i)] \tikzfig{rcd_for_cospan/cospan/subspace-image},
    \item[(ii)] \tikzfig{rcd_for_cospan/cospan/subspace-kernel}.
  \end{enumerate}
\end{lemma}
\begin{proof}
  \begin{itemize}
  \item[$(i)$] \citep[Proposition~2.7]{axler2024linear}.
  \item[$(ii)$] Analogous by symmetry.
  \end{itemize}
\end{proof}


\begin{lemma}[Cospan and Span]\label{lem:relation_in_cospan_form_complete} For all relations $\Rel{R}$,
  \begin{enumerate}
    \item[(i)] $\exists \Map{A}, \exists \Map{B}$, such that $\Rel{R} = \CoSpan{A}{B} $,
    \item[(ii)] $\exists \Map{C}, \exists \Map{D}$, such that $\Rel{R} = \Span{C}{D} $.
  \end{enumerate}
\end{lemma}
\begin{proof}
We prove item~(i). The other is analogous.
  \begin{hcalculation}[=]{\Rel{R}}
    \hstep{\tikzfig{rcd_for_cospan/cospan/lemmacospan/step1}}{Fig.\ref{fig:Snake}: \tikzfig{theorems/snake}}
    \hstep{\tikzfig{rcd_for_cospan/cospan/lemmacospan/step2}}{Lem.\ref{thm:subspace_image}:\tikzfig{theorems/subspace-kernel}}
    \hstep{\tikzfig{rcd_for_cospan/cospan/lemmacospan/step3}}{Fig.\ref{fig:Split}: \tikzfig{theorems/split}}
    \hstep{\tikzfig{rcd_for_cospan/cospan/lemmacospan/step3-1}}{Fig.\ref{fig:typerelations}-DET: \tikzfig{theorems/whiteball}}
    \hstep{\tikzfig{rcd_for_cospan/cospan/lemmacospan/step4}}{Fig.\ref{fig:Flip}: \tikzfig{theorems/flip}}
    \hstep{\tikzfig{rcd_for_cospan/cospan/lemmacospan/step5}}{Fig.\ref{fig:Snake}: \tikzfig{theorems/snake}}
  \end{hcalculation}
\end{proof}

\begin{lemma}\label{lemma:abstract}
  For all linear maps $\Map{A}, \Map{B}, \Map{C}, \Map{D}$,
  \[\CoSpan{A}{B}=\CoSpan{C}{D} \iff \tikzfig{rcd_for_pairs_of_matrices/TheoremAbstract/Lemma/lemma}\]
\end{lemma}
\begin{proof}
  \begin{hcalculation}[\iff]{\tikzfig{rcd_for_pairs_of_matrices/TheoremAbstract/Lemma/lemma}}
    \hstep{\tikzfig{rcd_for_pairs_of_matrices/TheoremAbstract/Lemma/step7}}{Fig.\ref{fig:typerelations}-DET: \tikzfig{theorems/whiteball}}
    \hstep{\tikzfig{rcd_for_pairs_of_matrices/TheoremAbstract/Lemma/step8}}{Fig.\ref{fig:Flip}: \tikzfig{theorems/flip}}
    \hstep{\CoSpan{A}{B}=\CoSpan{C}{D}}{Fig.\ref{fig:Compare}: \tikzfig{theorems/comparison}}
  \end{hcalculation}
\end{proof}

\begin{figure}[htb]
    \centering
    \resizebox{\textwidth}{!}{\tikzfig{Appendix/Commutative_Comonoid}}
    \caption{Commutative Comonoid}
    \label{fig:Commutative_Comonoid}
\end{figure}

\begin{figure}[htb]
    \centering
    \resizebox{\textwidth}{!}{\tikzfig{Appendix/Symmetry}}
    \caption{The Symmetry Theorems}
    \label{fig:Symmetry}
\end{figure}

\begin{figure}[htb]
    \centering
    \tikzfig{Appendix/Inequality}
    \caption{The Inequality Theorems}
    \label{fig:Inequality}
\end{figure}

\begin{figure}[htb]
    \centering
    \resizebox{\textwidth}{!}{\tikzfig{Appendix/Frobenius_Algebra}}
    \caption{Frobenius Algebra}
    \label{fig:Frobenius_Algebra}
\end{figure}

\begin{figure}[htb]
    \centering
    \tikzfig{Appendix/Bialgebra}
    \caption{Bialgebra}
    \label{fig:Bialgebra}
\end{figure}

\begin{figure}[htb]
    \centering
    \resizebox{\textwidth}{!}{\tikzfig{Appendix/Flip}}
    \caption{Flip}
    \label{fig:Flip}
\end{figure}

\begin{figure}[htb]
    \centering
    \tikzfig{Appendix/Snake}
    \caption{Snake}
    \label{fig:Snake}
\end{figure}

\begin{figure}[htb]
    \centering
    \begin{minipage}[t]{.6\textwidth}
      \centering
      \tikzfig{Appendix/Connect_sum}
      \captionof{figure}{Connect sum}
      \label{fig:Connect_sum}
    \end{minipage}%
    \hfill\begin{minipage}[t]{.4\textwidth}
      \centering
      \tikzfig{Appendix/Split}
      \captionof{figure}{Split}
      \label{fig:Split}
    \end{minipage}\\
    \begin{minipage}[t]{.6\textwidth}
      \centering
      \tikzfig{Appendix/Compare}
      \captionof{figure}{Compare}
      \label{fig:Compare}
    \end{minipage}%
    \hfill\begin{minipage}[t]{.4\textwidth}
      \centering
      \tikzfig{Appendix/Minimum_and_maximum}
      \captionof{figure}{Minimum and maximum relations}
      \label{fig:Minimum_and_maximum}
    \end{minipage}\\
\end{figure}

\begin{figure}[htb]
    \centering
    \resizebox{\textwidth}{!}{\tikzfig{Appendix/typesofrelations}}
    \caption{Each column of the table is a series of equivalent statements.
      Furthermore, the opposite inequalities for the second and third row hold for any relation (See Figure~\ref{fig:Inequality}).
      We are, thus, allowed to treat these characterizations as equalities.
    }
    \label{fig:typerelations}
\end{figure}

\FloatBarrier

\bibliographystyle{fundam}
\bibliography{bibliography}

\end{document}
