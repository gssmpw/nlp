\section{Preliminaries}
\label{sec:prel}
In user-level DP-SCO, we are given a dataset $\calD=\{Z_i\}_{i\in[n]}$ of $n$ users, where $Z_i \in \calZ^m$ is the user $i$'s data which consists of $m$ datapoints drawn i.i.d. from an (unknown) underlying distribution $\calP$.
%each user $Z_i\sim \calP^m$, and 
The objective is to minimize the following population function under user-level DP constraint:
\begin{align*}
    F(x):=\E_{z\sim \calP}f(x;z).
\end{align*}

In this section, we present the key definitions and assumptions.  
Discussions regarding the limitations of these assumptions can be found in Section~\ref{sec:discussion}. Additional tools, including  
those from differential privacy, are deferred to Appendix~\ref{subsec:prel}.

\vspace{-2mm}
\begin{definition}[Lipschitz]
\label{def:lip}
    We say a function $f:\calX\to\R$ is \emph{$G$-Lipschitz}  with respect to $\ell_p$-norm , if for any $x,y\in\calX$, we have
    $
        |f(x)-f(y)|\le G\|x-y\|_p.$
    This means $\|\nabla f(x)\|_q\le G$ for any $x \in \calX$,
    where $1/p+1/q=1$.
\end{definition}
\vspace{-5mm}
\begin{definition}[Smooth]
\label{def:smooth}
    In this work, we say a function $f$ is \emph{$\beta$-smooth}, if $\|\nabla^2 f(x)\|_\infty\le \beta,\forall x\in\calX$, where $\|A\|_\infty:=\max_i\sum_{j}|A_{i,j}|$ for a symmetric matrix $A$.
    This implies that $\|\nabla f(x)-\nabla f(y)\|_\infty\le \beta \|x-y\|_\infty$ for any $x,y\in\calX$.
\end{definition}
\vspace{-5mm}
\begin{definition}[Diagonal Dominance]
    A matrix $A\in\R^{d\times d}$ is \emph{diagonally dominant} if 
    \begin{align*}
        |A_{i,i}|\ge \sum_{j\neq i}|A_{i,j}|,& &\forall i \in [d].
    \end{align*}
\end{definition}
\vspace{-5mm}
\begin{assumption}
\label{assum:lispchitz_smooth}
    Each function $f(;z):\calX\to \R$ in the universe is convex, $G$-Lipschitz with respect to $\ell_1$-norm (Definition~\ref{def:lip}) and $\beta$-smooth (Definition~\ref{def:smooth}).
    $\calX$ is a ball of radius $D$ in $\ell_\infty$-norm.
\end{assumption}
\vspace{-5mm}
\begin{assumption}
\label{assump:dia_dominant}
    The Hessian of each function $f(\cdot;z)$ in the universe is diagonally dominant.
\end{assumption}

Diagonal dominance, although somewhat restrictive, is a commonly discussed assumption in the literature. For example, \cite{wang2021convergence}  demonstrated the convergence rate of SGD in heavy-tailed settings under  
the assumption of diagonal dominance. Similarly, \cite{das2024towards}  studied Adamâ€™s preconditioning effect for quadratic functions with diagonally dominant Hessians.  
In the case of 1-hidden layer neural networks (a common focus of the NTK line of work), the Hessian is diagonal (see Section 3 in \cite{liu2020linearity}). Additionally, it has been shown that in  
practice, Hessians are typically block diagonal dominant  
\cite{martens2015optimizing, botev2017practical}.  
We also discuss potential ways to avoid this assumption in  
Section~\ref{sec:discussion}.


% \begin{lemma}[\cite{jin2019short}]
% \label{lm:inner_product_nSG}
% There exists an absolute constant $c$, such that if $X$ is $\nSG(\zeta)$, then for any fixed unit vector $v\in \R^d$, 
% $\langle v,X\rangle$ is $c\zeta$ norm-SubGaussian.
% \end{lemma}

\paragraph{Notation}
For $X\in\R^d$, we use $X[i]$ to denote its $i$th coordinate.
For a vector $X\in\R^d$ and a convex set $\calX\subset\R^d$, we use $\Pi_{\calX}(X):=\arg\min_{Y\in\calX}\|Y-X\|_2$.
For $X\in\R^d$ and $r\in\R_{\ge0}$, we use $B_\infty(X,r)$ to denote the $\ell_\infty$-ball centered at $X$ of radius $r$.