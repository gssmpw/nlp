\section{Lower Bound}
This section presents our main lower bound. As stated above, the lower bound is nearly tight, apart from lower-order terms and the dependency on $\eps$.

\begin{theorem}
\label{thm:lb}
There exists a distribution $\calP$ and a loss function $f$  satisfying Assumption~\ref{assum:lispchitz_smooth} and Assumption~\ref{assump:dia_dominant}, such that for any $(\epsilon,\delta)$-User-level-DP algorithm $\calM$, given i.i.d. dataset $\calD$ drawn from $\calP$, the output of $\calM$ satisfies
\begin{align*}
    \E[F(\calM(\calD))-F(x^*)]\ge GD\cdot \Tilde{\Omega}\Big(\min\Big\{d,\frac{d}{\sqrt{mn}}+\frac{d^{3/2}}{n\epsilon\sqrt{m}}\Big\}\Big).
\end{align*}
%where $F(x):=\E_{z\sim \calP}f(x;z)$ and $x^*=\arg\min_{x\in\calX}f(x)$.
\end{theorem}

The non-private term $GD\frac{d}{\sqrt{mn}}$ represents the information-theoretic lower bound for SCO under these assumptions (see, e.g., Theorem 1 in \cite{agarwal2009information}).  


We construct the hard instance as follows:
let $\calX=[-1,1]^d$ be unit $\ell_\infty$-ball and let $f(x;z)=-\langle x,z\rangle$ for any $x\in \calX$ be the linear function.
Let $z\in[-\sqrt{m},\sqrt{m}]^d$  with $\E_{z\sim\calP}[z]=\mu$.
Then one can easily verify that $f$ satisfies Assumptions~\ref{assum:lispchitz_smooth} and~\ref{assump:dia_dominant} with $G=\sqrt{m},D=1$ and $\beta=0$.
We have
\begin{align}
    F(\calM(\calD))-F(x^*) &= \sum_{i=1}^{d} (\sign(\mu[i])-\calM(\calD)[i])\cdot\mu[i]
   \nonumber \\ &\ge \sum_{i=1}^{d}  |\mu[i]|.\ind\big(\sign(\mu[i])\neq\sign(\calM(\calD)[i])\big). \label{eq:opt_error_to_sign_error}
\end{align}

By~\eqref{eq:opt_error_to_sign_error}, we reduce the optimization error to the weighted sign estimation error.  
Most existing lower bounds rely on the $\ell^2_2$-error of mean estimation.  
We adapt their techniques, especially the fingerprinting lemma, and provide the proof in the Appendix~\ref{sec:lbproof}.