\newpage
\section{Proof of Section~\ref{sec:main_alg}}
\begin{algorithm2e}
    \caption{Localization for user-level DP-SCO}
    \label{alg:loacalizatioin}
    \textbf{Input:} Dataset $\calD$, parameters $\epsilon,\delta,B$, initial point $x_0$\;
    \textbf{Process:}
    Set $S=\lceil \log n/B\rceil $\;
    \For{Phase $s=1,\ldots,S$}
    {
    Set $n_s=n/2^s$ and $\eta_s=(\log^{-s}m)\eta$\;
    Draw a dataset $\calD_s$ of size $n_s$ from the unused users\;
    Run Algorithm~\ref{alg:dpsgd} with inputs $\calD_s,\epsilon,\delta,\eta_s,\tau,\upsilon,B,x_{s-1}$\;
    % at $x_{s-1}$ with parameters $\calD_s,\epsilon,\delta$ ...\;
    set $x_s=\bar{x}_s+\zeta_s$, where $\zeta_s\sim \calN(0,\sigma_s^2I_d)$ with $\sigma_s=O(\frac{\eta_s G\sqrt{d\log(\exp(\epsilon)/\delta)\log(nmd)}}{\sqrt{m}\epsilon})$\;
    }
     \textbf{ Return:} the final solution $x_s$
\end{algorithm2e}

\subsection{Proof of Lemma~\ref{lm:contractivity}}
\contractivity*
\begin{proof}
By the diagonal dominance assumption and precondition that $\eta\le 2/\beta$, we know
\begin{align*}
    \|I-\eta\nabla^2f(z)\|_\infty=\max_{j}\left\{|1-\eta\nabla^2 f(z)_{j,j}|+\sum_{i\neq j}\eta |\nabla^2 f(z)_{j,i}|\right\}\le 1.
\end{align*}

Note that by the mean-value theorem, 
\begin{align*}
    (x-\eta \nabla f(x)) - (y-\nabla f(y))= x-y+\eta (x-y)^\top \nabla^2 f(z) = (x-y)(I-\eta \nabla^2 f(z)),
\end{align*}
where $z$ is in the segment between $x$ and $y$.
Hence we have
\[
\| (x-\eta \nabla f(x)) - (y-\nabla f(y)) \|_\infty  
\le \|x-y\|_\infty \cdot \|I-\eta \nabla^2 f(z) \|_\infty\cdot \\
\le \|x-y\|_\infty.
\]
\end{proof}

\subsection{Proof of Lemma~\ref{lm:proj_mean_to_rs}}
\projmeantors*
\begin{proof}
    Without loss of generality, we assume $a\le b$.
    We do case analysis.

    Case (I): if no projection happens. Then it is straightforward that $c'=c$ and $d'=d$ and hence $|c'-d'|=|c-d|\le 1$.

    Case (II): if one projection happens. Without loss of generality, assume we project $c$ and get $c'$. We analyze the following sub-cases:\\ 
    (i): $c'=a-r$. In this case we know $c\le c'$.
    If $d\ge a-r=c'$, then we know $|c'-d'|\le |c-d|\le 1$.
    If $d<a-r$, then $d-b<-r$ which is impossible.\\    
    (ii): $c'=a+r$. If $a+r\le b$, then we know $|c'-d'|\le |a-b|\le 1$.
    Consider the subsubcase when $a+r>b$. If $d\le a+r$, then $|c'-d'|\le |c-d|\le 1$.
    Else if $d\ge a+r$, as $b+r\ge d\ge c'=a+r$, we have $|c'-d'|\le |a-b|\le 1$.
    
    Case (III): if two projections happen.\\
    (i): $c'=a-r,d'=b-r$. This is a trivial case.\\
    (ii): $c'=a+r,d'=b+r$. This case is also trivial.\\
    (iii): $c'=a-r,d'=b+r$. We can show that $|c'-d'|\le |c-d|\le 1$.\\
    (vi): $c'=a+r,d'=b-r$. If $a+r\le b$, then we know $|c'-d'|\le |a-b|\le 1$.
    Else, if $a+r>b$, then we know $|c'-d'|\le |c-d|\le 1$.
\end{proof}

\subsection{Proof of Lemma~\ref{lm:ite_sensitivity_base}}
\itesensitivitybase*
\begin{proof}
It suffices to show that $\|g_0-g_0'\|_\infty\le 4\rho+2\varsigma$.
By the first property of Assumption~\ref{assum:prop_geo_median} and the preconditions in the statement, we know $B_\infty(X',\rho)\cap B_\infty(Y',\rho)\neq \emptyset$,
which leads to that
\begin{align*}
    \|X'-Y'\|\le 2\rho.
\end{align*}

Moreover, we have that the robust statistic $\|X_{\rs}-Y_{\rs}\|_\infty\le 4\rho$ by the triangle inequality as $\|X_{\rs}-X'\|_\infty\le \rho$ and $\|Y_{\rs}-Y'\|_\infty\le \rho$.

By the projection operation in Algorithm~\ref{alg:robust_gradient_est}, we know $g_0\in B_\infty(X_{\rs},\varsigma)$ and $g_0'\in B_\infty(Y_{\rs},\varsigma)$, and hence we know $\|g_0-g_0'\|_\infty\le 4\rho+2\varsigma$.
This completes the proof.
\end{proof}


\subsection{Proof of Lemma~\ref{lm:iteration_sensitivity}}
\iterationsensitivity*
\begin{proof}
Recall that we assume all users $Z_{i,t}$ are equal to $Z_{i,t}'$ but $Z_{1,1}$.

% Now we show that $\|x_t-y_t\|_\infty\le \|x_{t-1}-y_{t-1}\|_\infty$.
We actually show that 
\begin{align*}
    \|(x_{t-1}-\eta g_{t-1}) - (y_{t-1}-\eta g_{t-1}')\|_\infty\le \|x_{t-1}-y_{t-1}\|_\infty,
\end{align*}
as the projection operator to the same convex set is contractive in $\ell_2$- and $\ell_\infty$-norm in our case.

Let $X_{i,t}=x_{t-1}-\eta q_t(Z_{i,t})$ and $Y_{i,t}=y_{t-1}-\eta q_t'(Z_{i,t})$. Note that the users used in computing the gradients are the same.
Let $X_{\est}$ be the output of Algorithm~\ref{alg:robust_gradient_est} with $\{X_{i,t}\}_{i\in[B]}$ as inputs, and $Y_{\est}$ be the corresponding output of $\{Y_{i,t}\}_{i\in[B]}$.
By the third property of Assumption~\ref{assum:prop_geo_median}, it suffices to show that 
\begin{equation}
\label{eq:contractive_est}
    \|X_{\est}-Y_{\est}\|_\infty\le \|x_{t-1}-y_{t-1}\|_\infty.
\end{equation}

By Lemma~\ref{lm:contractivity}, we know 
\begin{align*}
    \|X_{i,t}-Y_{i,t}\|_\infty\le \|x_{t-1}-y_{t-1}\|_\infty.
\end{align*}

Then by the second property in Assumption~\ref{assum:prop_geo_median}, we know that $\|X_{\rs}-Y_{\rs}\|_\infty\le \|x_{t-1}-y_{t-1}\|_\infty$.
Similarly, by Lemma~\ref{lm:contractivity}, we have $\|\bx-\by\|_\infty\le \|x_{t-1}-y_{t-1}\|_\infty$.
Then~\eqref{eq:contractive_est} follows from Lemma~\ref{lm:proj_mean_to_rs}.
This completes the proof.
\end{proof}


\subsection{Proof of Lemma~\ref{lm:sensitivity_base}}
\sensitivitybase*
\begin{proof}
% We show if $\AboTh$ outputs $\top$ when $t=1$, then with probability at least $1-\delta/T$, there exists a point $Y$ such that $3B/4$ points in $\{q_1(Z_{i,1})\}_{i\in[B]}$ live in $B_{\infty}(Y,1/\tau)$.
The main randomness comes from the Laplacian noise we add to the query and the threshold.
% With probability at least $1-\delta/T$, we know 
% \begin{align}
% \label{eq:large_score}
%     \qcon_1(\calD,\tau)\ge\zeta-2\log(T/\delta)/\epsilon\ge 0.9B.
% \end{align}
% Suppose that this is false for contravention, that is, for any $Y$, there are more than $B/3$ points living outside $B_{\infty}(Y,\tau)$.
Under the precondition that $|X_{\good}|<B/3$ for any $Y$, 
then we know the concentration score 
\begin{align*}
    \qcon_1(\calD,\tau)=\sum_{Z_{i,1}}\sum_{Z_{j,1}}\exp(-\tau \|q_1(Z_{i,1})-q_1(Z_{j,1}\|)
    \le 2B/3+\exp(-1)\cdot B/3<0.8B.
\end{align*}
Then by Lemma~\ref{thm:Above_Threshold} with a probability of at least $1-\delta/T\exp(\epsilon)$, we have
\begin{align*}
    \qcon_1(\calD,\tau)+\Lap(6/\epsilon)\le \upsilon,
\end{align*}
which means $a_1=\bot$.
\end{proof}



\subsection{Proof of Lemma~\ref{lm:query_sensitivity}}
\querysensitivity*
\begin{proof}
Consider the difference between $\|q_t(Z_{j,t})-q_t(Z_{i,t})\|_\infty-\|q_t'(Z_{i,t})-q_t'(Z_{j,t})\|_\infty$.

By Lemma~\ref{lm:ite_sensitivity_base} and Lemma~\ref{lm:iteration_sensitivity}, we know $\|x_t-y_t\|_\infty\le 6\eta/\tau$. 
Then by the smoothness assumption, 
we have
\begin{align*}
& ~\|q_t(Z_{j,t})-q_t(Z_{i,t})\|_\infty-\|q_t'(Z_{i,t})-q_t'(Z_{j,t})\|_\infty\\
\le &~ \|q_t(Z_{i,t})-q_t'(Z_{i,t})-(q_t(Z_{j,t})-q_t'(Z_{j,t}))\|_\infty\\
\le &~ \|q_t(Z_{i,t}-q_t'(Z_{i,t})\|_\infty+\|q_t(Z_{j,t})-q_t'(Z_{j,t})\|_\infty\\
\le &~ 2\beta\|x_t-y_t\|_\infty.
\end{align*}


Hence we have
\begin{align*}
    \qcon_i(\calD,\tau)=&\frac{1}{B}\sum_{Z,Z'\in\calD}\exp(-\tau\|q_i(Z)-q_i(Z')\|_\infty)\\
    \ge & \frac{1}{B}\sum_{Z,Z'\in\calD'}\exp(-\tau\|q_i'(Z)-q_i'(Z')\|_\infty)\exp(-12\beta\eta)\\
    \ge & \exp(-12\beta\eta)\qcon_i(\calD',\tau).
\end{align*}
As both $\qcon_i(\calD,\tau)$ and $\qcon_i(\calD',\tau)$ are in the range $[0,B]$, we know that
\begin{align*}
    \qcon_i(\calD',\tau)-\qcon_i(\calD,\tau)\le (1-\exp(-12\beta\eta))B\le 12\beta\eta B,
\end{align*}
where we use the fact $1-\exp(-x)\le x$ for any $x\ge 0$.
    
Similarly, we can bound $\qcon_i(\calD,\tau)-\qcon_i(\calD',\tau)\le 12\beta\eta B$, and complete the proof.
\end{proof}

\subsection{Proof of Lemma~\ref{lm:privacy_guarantee}}
\label{app:privacy-proof}
\privacyguarantee*
\begin{proof}
Consider the implementation over two neighboring datasets $\calD$ and $\calD'$, and we use the prime notation to denote the corresponding variables with respect to $\calD'$.
Without loss of generality, we assume the different user is used in the first phase.

To avoid confusion, let $\bx_1$ and $\bx_1'$ be the outputs of Algorithm~\ref{alg:dpsgd} with neighboring inputs $\calD_1$ and $\calD_1'$, 
$x_1$ and $x_1'$ be the outputs of Algorithm~\ref{alg:loacalizatioin} by privatizing $\bx_1$ and $\bx_1'$ with Gaussian noise respectively.

% To prove the privacy guarantee, we argue about the output $\bx_1$ and $\bx_1'$ of Algorithm~\ref{alg:dpsgd} with neighboring inputs $\calD_1$ and $\calD_1'$.
The outputs $\bx_1$ and $\bx_1'$ of  Algorithm~\ref{alg:dpsgd} depend on the intermediate random variables $\{a_t\}_{t\in [T]}$ and $\{a_t'\}_{t\in[T]}$.

As the query sensitivity for $t=1$ is always bounded, by our parameter setting and property of $\AboTh$, we know $a_1\approx_{\epsilon/2,0}a_1'$.

We do case analysis.

(i) $(\calD_1,\calD_1')$ is not $(1/\tau)$-aligned.
Then by Lemma~\ref{lm:sensitivity_base}, either $\Pr[a_1=\bot]\ge 1-\delta/2$ or $\Pr[a_1'=\bot]\ge 1-\delta/2e^{\epsilon}$.
Then by union bound and $a_1\approx_{\epsilon/2,0}a_1'$, we have
\begin{align*}
    \Pr[a_1=a_1'=\bot]\ge 1-(1+\exp(\epsilon))\delta/2e^{\epsilon}.
\end{align*}
If $a_1=a_1'=\bot$, then $\bx_1=\bx_1'$ is the initial point.
We have for any event range $\calO$,
\begin{align*}
    \Pr[x_1\in \calO] =&\Pr[x_1\in \calO\mid a_1=\bot]\Pr[a_1=\bot]+\Pr[x_1\in\calO\mid a_1=\top]\Pr[a_1=\top]\\
    \le & \Pr[x_1'\in\calO\mid a_1'=\bot] \exp(\epsilon/2)\Pr[a_1'=\bot]+ e^{\epsilon}(\delta/2\exp(\epsilon))\\
    \le & e^{\epsilon/2}\Pr[x_1'\in\calO]+ \delta/2.
\end{align*}
This completes the privacy guarantee for case(i).

(ii) $(\calD_1,\calD_1')$ is $(1/\tau)$-aligned.
In this case, by Lemma~\ref{lm:ite_sensitivity_base} and Lemma~\ref{lm:iteration_sensitivity}, we know $\|x_t-y_t\|_\infty\le 6\eta/\tau$.
Moreover, Lemma~\ref{lm:query_sensitivity} suggests that the query sensitivity is always bounded by 1.
Then by the property of $\AboTh$ (Lemma~\ref{thm:Above_Threshold}), we have $\{a_t\}_{t\in[T]}\approx_{\epsilon/2,0}\{a_t'\}_{t\in[T]}$.
We have for any event range $\calO$,
\begin{align*}
\Pr[x_1\in\calO]=& \Pr[x_1\in\calO \mid \exists t,a_t=\bot]\Pr[\exists t,a_t=\bot]  +\Pr[x_1\in\calO\mid a_t=\top,\forall t]\Pr[a_t=\top,\forall t]\\
 \le & \Pr[x_1'\in\calO \mid \exists t,a_t'=\bot]\exp(\epsilon/2)\Pr[\exists t,a_t'=\bot]+\Pr[x_1\in\calO\mid a_t=\top,\forall t]\Pr[a_t=\top,\forall t]\\
 \le& \exp(\epsilon/2)\Pr[x_1'\in\calO \mid \exists t,a_t'=\bot]\Pr[\exists t,a_t'=\bot]\\
 &~~~+(\exp(\epsilon/2)\Pr[x_1'\in\calO\mid a_t'=\top,\forall t]+\delta/2\exp(\epsilon))\exp(\epsilon/2)\Pr[a_t'=\top,\forall t]\\
 \le& \exp(\epsilon)\Pr[x_1'\in\calO]+\delta,
\end{align*}
where we use the Gaussian Mechanism (Proposition~\ref{prop:GM}) to bound $\Pr[x_1\in\calO\mid a_t=\top,\forall t]$ by $\Pr[x_1'\in\calO\mid a_t'=\top,\forall t]$.
This completes the proof.

% Recall that event $E_g$ means there exists a point $Y$ such that at least $3B/4$ points in $\{q_1(Z_{i,1})\}_{i\in[B]}$ live in $B_{\infty}(Y,1/\tau)$.
% If $a_1=a_1'=\bot$, then the $\bx_1=\bx_1'=x_0$ without additional privacy loss.
% Consider the case when $a_1=\top$. 
% By Lemma~\ref{lm:sensitivity_base}, we know
% \begin{align*}
%     \Pr[E_g \mid a_1=\top]\ge 1-\delta/T,
% \end{align*}
% which holds similarly for $E_g'$ and $a_1'$.
% % Denote this event by $E_g$, then we have
% % \begin{align*}
% %     \Pr[E_g\mid a_1=a_1'=\top]\ge 1-\delta/2.
% % \end{align*}


% Conditional on $E_g\cap E_g'$, 
% then Lemma~\ref{lm:query_sensitivity} shows the query sensitivity is always bounded by 1, and hence $\{a_t\}_{t\ge 2}\approx_{\epsilon/2,0}\{a_t'\}_{t\ge 2}$ by the property of $\AboTh$ (Lemma~\ref{thm:Above_Threshold}).
% This leads to that for any $\calO$,
% \begin{align*}
%     \Pr[\{a_t\}_{t\ge 2}\in\calO\mid a_1=a_1',E_g\cap E_g']\le \exp(\epsilon/2) \Pr[\{a_t'\}_{t\ge 2}\in\calO\mid a_1=a_1',E_g\cap E_g'].
% \end{align*}

% Moreover, conditional on $E_g\cap E_g'$, we have
% $\|\bx_1-\bx_1'\|_\infty\le 2\eta/\tau$.
% % The privacy guarantee follows from the sensitivity between $\bar{x}_1$ and $\bar{x}_1'$, and the Gaussian mechanism.
% % To argue about the sensitivity, we also need to argue about the intermediate random variables $\{a_i\}_{i\in[T]}$.
% % By Lemma~\ref{lm:query_sensitivity}, we know the query sensitivity is always bounded by $O(1)$, and hence by the guarantee of $\AboTh$, we know $\{a_i\}_{i\in[T]}\approx_{\epsilon/2,\delta/2}\{a_i'\}_{i\in[T]}$.
% % Moreover, conditional on $\{a_i\}_{i\in[T]}=\{a_i'\}_{i\in[T]}$, we know $\|x_t-x_t'\|_{\infty}\le \eta/\tau$.
% The privacy guarantee between $\{a_t\}_{t\in T}$ and $\{a_t'\}_{t\in[T]}$ has been established.
% It remains to consider the case when $a_t=a_t'=\top,\forall t\in[T]$, as
% \begin{align*}
%     \Pr[x_1\in\calO\mid \exists t, a_t=\bot]=\Pr[x_1'\in\calO\mid \exists t, a_t'=\bot].
% \end{align*}

% By the Gaussian Mechanism (Proposition~\ref{prop:GM}), we have $\forall \calO$,
% \begin{align*}
%     \Pr[x_1\in \calO\mid a_t=\top,\forall t\in [T] \text{ and } E_g]\le \exp^{\epsilon}\Pr[x_1'\in \calO\mid a_t'=\top,\forall t\in [T] \text{ and }  E_g']+\delta.
% \end{align*}
% Then we have $\forall\calO$,
% \begin{align*}
%  \Pr[x_1\in\calO]=& \Pr[x_1\in\calO \mid \exists t,a_t=\bot]\Pr[\exists t,a_t=\bot]  \\
%  &~~~+\Pr[x_1\in\calO\mid a_t=\top,\forall t]\Pr[a_t=\top,\forall t]\\
%     \Pr[x_1\in\calO]=& \Pr[x_1\in\calO \mid a_1=\bot]\Pr[a_1=\bot]+\Pr[x_1\in\calO\mid a_1=\top]\Pr[a_1=\top]\\
%     \le& \Pr[x_1'\in\calO\mid a_1'=\bot] (\exp(\epsilon/3)\Pr[a_1'=\bot]\\
%     &~~+\Pr[x_1\in\calO\mid a_1=\top,E_g]\Pr[a_1=\top,E_g]+\Pr[E_g^c,a_1=\top]\\
%     \le & \exp(\epsilon/3)\Pr[x_1'\in\calO\mid a_1'=\bot] \Pr[a_1'=\bot]+\delta/2\\
%     &~~~+\Pr[x_1\in\calO\mid a_1=\top,E_g,a_1'=\top,E_g']\Pr[a_1=\top,E_g]\\
%     \le&~ \exp(\epsilon/3)\Pr[x_1'\in\calO\mid a_1'=\bot] \Pr[a_1'=\bot]+\delta/2\\
%     &~~~~+
% \end{align*}
% Hence, the final privacy guarantee of  follows from the Gaussian Mechanism (Proposition~\ref{prop:GM}) and composition.
\end{proof}

\subsection{Proof of Lemma~\ref{lm:dpsgd_utility}}
\dgsgdutility*
\begin{proof}
By the Hoeffding inequality for norm-subGaussian vectors (Theorem~\ref{thm:hoeffding_nSG}), for each $t\in[T]$ and each $i\in[B]$, we have
\begin{align*}
    \Pr\Big[\|q_t(Z_{i,t})-\nabla F(x_{t-1})\|_\infty \ge G\log(ndm/\omega)/\sqrt{m}\Big]\le 1-\omega/nm.
\end{align*}


Then conditional on the event that $\|q_t(Z_{i,t})-\nabla F(x_{t-1})\|_\infty\le \tau$ for all $i\in[B]$ and $t\in[T]$,
by our setting of parameters, we know that we pass all the concentration tests with $a_t=\top,\forall t\in[T]$, and that
\begin{align*}
    g_{t-1}=\frac{1}{B}\sum_{i\in[B]}q_t(Z_{i,t}),
\end{align*}
which means $d_{\TV}\Big(\{g_{t-1}\}_{t\in[T]},\{\frac{1}{B}\sum_{i\in[B]}q_t(Z_{i,t})\}_{t\in[T]}\Big)\le \omega$.
Note that $\E[\sum_{i\in[B]}q_t(Z_{i,t})]=\nabla F(x_{t-1})$ and $\E[\|\frac{1}{B}\sum_{i\in[B]}q_t(Z_{i,t})-\nabla F(x_{t-1})\|_2^2 ]\le G^2d/Bm$ when all functions are drawn i.i.d. from the distribution.
By the small TV distance between $g_{t-1}$ and the good gradient estimation $\frac{1}{B}\sum_{i\in[B]}q_t(Z_{i,t})$, 
it follows from Lemma~\ref{lm:sgd_smooth} that
\begin{align*}
    \E[F(\bx)-F(x)]\lesssim (\beta+\frac{1}{\eta})\frac{\E[\|x_0-x\|^2_2]}{T}+\frac{\eta G^2d}{Bm}+GDd\omega,
\end{align*}
where the last term comes from the worst value $GDd$, and the small failure probability $\omega$.
\end{proof}

\subsection{Proof of Lemma~\ref{lm:localization}}
\Localization*
\begin{proof}
Let $\bx_0=x^*$ and $\zeta_0=x_0-x^*$.
Lemma~\ref{lm:dpsgd_utility} can be used to analyze the utility concerning $\bx_s$.
As we add Gaussian noise $\zeta_s$ to $\bx_s$ in each phase, we analyze the influence of $\zeta_s$ first.

By the assumption, we know $\|\zeta_0\|_2\le D\sqrt{d}$.
Recall that by the setting that $\eta\le \frac{D}{G}\cdot\frac{\sqrt{m}\epsilon}{d\log(1/\delta)\log(nmd)}$, for all $s\ge 0$,
\begin{align*}
\E[\|\zeta_s\|_2^2]=d\sigma_s^2=\eta_s^2d\frac{Gd\log(1/\delta)\log(nmd)}{m\epsilon^2}\le D^2d\cdot \log^{-s}m.
\end{align*}
Then by Lemma~\ref{lm:dpsgd_utility}, we have
\begin{align*}
    \E[F(x_S)]-F(x^*)=&\sum_{s=1}^{S}\E[F(\bx_{s}-\bx_{s-1})]+\E[F(x_s)-F(\bx_s)]\\
    \le & \sum_{s=1}^{S} \left(\frac{\E[\|\zeta_{s-1}\|_2^2]}{\eta_sT_s}+\frac{\eta_sG^2d}{2Bm} \right)+G\E[\|\zeta_S\|_2]\\
    \le& \sum_{s=1}^{S} \left(\frac{\log m}{2} \right)^{-(i-2)}\left(\frac{D^2d}{\eta n/B}+\frac{\eta G^2d}{2Bm} \right)+\frac{GDd}{(\log m)^{\log n}}\\
    \le & \Tilde{O} \Big( GD(\frac{d}{\sqrt{nm}}+\frac{d^{3/2}}{n\sqrt{m}\epsilon^2})\Big),
\end{align*}
where we use the fact that $\frac{1}{(\log m)^{\log n}}\le 1/nm$ when $m\le n^{\log \log n}$.
\end{proof}

\subsection{Counterexample of the 1-Lipschitz of Geometric Median}
\label{sec:counter_example}
We use the counterexample from \cite{durocher2009projection} to show the geometric median is unstable in 2-dimension space.
Recall give a set of points $P$ in $\R^d$, the geometric median of $P$, denoted by $M(P)$, is
\begin{align*}
    M(P):=\arg\min_{x}\sum_{p\in P}\|x- p\|_2.
\end{align*}

Let $P=\{(0,0),(0,0),(1,0),(1,\alpha)\}$ and $P'=\{(0,0),(0,\alpha),(1,0),(1,0)\}$ with $\alpha>0$ as a very small perturbation.
But we know $M(P)=(0,0)$ and $M(P')=(1,0)$.