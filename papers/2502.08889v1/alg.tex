\section{Main Algorithm}
\label{sec:main_alg}
We present our main result in this section and explain the algorithm in a top-down manner.  The algorithm is based on the localization framework of  
\cite{FKT20}; see Algorithm~\ref{alg:loacalizatioin} in the Appendix for details. The main result is stated formally below:
\begin{theorem}
\label{thm:main_result}
Under Assumptions~\ref{assum:lispchitz_smooth} and \ref{assump:dia_dominant}, suppose $\beta\le\frac{G}{D}(\frac{\sqrt{n}\epsilon}{\sqrt{m}\log(nmd/\delta)}+\frac{\sqrt{d\log(1/\delta)\log(nmd)}}{\sqrt{m}\epsilon})$, $\epsilon\le O(1),n\ge \log^2(nd/\delta)/\epsilon$ and $ m\le n^{O(\log\log n)}$.
Setting $\eta\le\frac{D}{G}\cdot \min\{ \frac{B\sqrt{m}}{\sqrt{n}} ,   \frac{\sqrt{m}\epsilon}{\sqrt{d\log(1/\delta)\log( nmd)}}\}$, $B=100\log(mnd/\delta)/\epsilon$, $\tau=O(G\log(nmd)/\sqrt{m})$ and $\upsilon=0.9B+\frac{2\log(T/\delta)}{\epsilon}$, Algorithm~\ref{alg:loacalizatioin}  is $(\epsilon,\delta)$-user-level-DP. 
When the $nm$ functions in dataset $\calD$ are i.i.d. drawn from the underlying distribution $\calP$, it takes $mn$ gradient computations and outputs $x_S$ such that
    \begin{align*}
        \E[F(x_S)-F(x^*)]\le \Tilde{O} \left(\frac{d}{\sqrt{nm}}+\frac{d^{3/2}}{n\epsilon^2\sqrt{m}} \right).
    \end{align*}
\end{theorem}

We briefly describe the localization framework.  
In the first phase, it runs (non-private) SGD using half of the dataset, and averages the iterates to get $\bx_1$.
Roughly speaking, the solution $\bx_1$ already provides a good approximation with a small population loss when the datasets are  drawn i.i.d. from the underlying distribution. However, to ensure privacy, we require a  
sensitivity bound on $\|\bx_1\|$ and add noise $\zeta_1$ correspondingly to privatize $\bx_1$, yielding the private solution $x_1 \leftarrow \bx_1 + \zeta_1$.  

A naive bound on the excess loss due to the privatization is given by  
\[
\E[F(x_1) - F(\bx_1)] \leq G\|\zeta_1\|_2,
\]  
but the magnitude of the noise $\|\zeta_1\|_2$ is typically too large  
to achieve a good utility guarantee. Nevertheless, this process yields  
a much better initial point $x_1$ compared to the original starting  
point $x_0$. 
As a result, a smaller dataset and a smaller step size are sufficient  
to find the next good solution $\bx_2$ in expectation, with smaller noise $\|\zeta_2\|_2$ added to privatize $\bx_2$.  

This process is repeated over $O(\log n)$ phases, where each subsequent solution $\bx_S$ is progressively refined, and the Gaussian noise  
$\|\zeta_S\|_2$ becomes negligible. Ultimately, this iterative refinement  
balances privacy and utility, as established in Theorem~\ref{thm:main_result}.  
The formal argument about the utility guarantee and proof can be found in Lemma~\ref{lm:localization}.  

Our main contribution is in Algorithm~\ref{alg:dpsgd},  
which uses a novel gradient estimation sub-procedure.

% Given some initial point $x$, we define the Projected Gradient Descent sequences $\{x_{t}^Z\}_{t\in[m/K]}$, denoted by $\PGD(x,\eta,Z,K)$ for each user $Z$ with batch size $K$ as follows:
% \begin{align}
% \label{eq:PGD_each_user}
%     x_t^Z= \Pi_{\calX}(x^Z_{t-1}-\eta\frac{1}{K}\sum_{j\in[K]}\nabla f(x^Z_{t-1};z_{t,j})),
% \end{align}
% where $\{z_{t,j}\}_{j\in[K]}$ is a set of unused item functions of size $K$.
% This is simply running GD for each user for $m/K$ iterations, with batch size $K$ for each iteration.

\begin{algorithm2e}
\caption{SGD for User-level DP-SCO}
\label{alg:dpsgd}
\textbf{ Input:} dataset $\calD$, privacy parameters $\epsilon,\delta$, other parameters $\eta,\tau,\upsilon,B$, initial point $x_0$\;
%\textbf{ Process:} \\
Divide $\calD$ into {B} disjoint subsets of equal size, denoted by $\{\calD_i\}_{i\in[B]}$,
$\calD_i=\{Z_{i,t}\}_{t\in[|\calD|/B]}$\; 
Set $T=|\calD|/B$\;
\For{Step $t=1,\ldots,T$}
{
For each $i\in[B]$, get $q_t(Z_{i,t}):=\frac{1}{m}\sum_{z\in Z_{i,t}}\nabla f(x_{t-1};z)$\;
Let $g_{t-1}$ be the output of Algorithm~\ref{alg:robust_gradient_est} with inputs $\{q_t(Z_{i,t})\}_{i\in[B]}$ and threshold $1/\tau$\;
$x_{t}=\Pi_\calX(x_{t-1}-\eta g_{t-1})$
}
\tcc{Concentration Test}
\tcc{Recall the query $q_t(Z_{i,t})$ for each $t\in[T], i\in[B]$ from above}
Run Algorithm~\ref{alg:out_rem} with query $\{q_t\}_{t\in[T]}$ and parameters $\calD_t,\epsilon,\frac{\delta}{2Tmnd},\tau,\upsilon$ to get answers $\{a_t\}_{t\in [T]}$ \;
\If{$a_t=\top,\forall t\in[T]$}
{
\textbf{ Return:} Average iterate $\bar{x}=\frac{1}{T}\sum_{t\in[T]}x_t$\;
}
\Else
{
\textbf{ Output:} Initial point $x_0$\;
}
\end{algorithm2e}

\paragraph{ Iteration Sensitivity of Algorithm~\ref{alg:dpsgd}:}
The contractivity of gradient descent plays a crucial role in the sensitivity analysis, for which we need the Hessians to be diagonally  dominant
(Assumption~\ref{assump:dia_dominant}). 

\begin{restatable}{lem}{contractivity}[Contractivity]
    \label{lm:contractivity}
Suppose $f:\calX\to\R$ is a convex and $\beta$-smooth function satisfying Assumption~\ref{assump:dia_dominant}, then for any two points $x,y\in \calX$, with step size $\eta\le 2/\beta$, we have
    \begin{align*}
        \|(x-\eta \nabla f(x)) - (y-\eta \nabla f(y))\|_\infty\le \|x-y\|_\infty.
    \end{align*}
\end{restatable}

Now, we discuss Algorithm~\ref{alg:dpsgd}.  
Given the dataset $\calD$, we proceed in $T = |\calD|/B$ steps.  
At the $t$th step, we draw $B$ users $\{Z_{i,t}\}_{i \in [B]}$ and compute the average gradient of each user. 
We then apply our gradient estimation algorithm (Algorithm~\ref{alg:robust_gradient_est}) and perform normal gradient descent for $T$ steps.  

In the second phase of Algorithm~\ref{alg:dpsgd}, we perform the concentration test  
(Algorithm~\ref{alg:out_rem}) on the $B$ gradients at each step based on $\AboTh$ (Algorithm~\ref{alg:mean_est_with_AT}).  
If the concentration test passes for all steps (i.e., $a_t = \top$  
for all $t \in [T]$), we output the average iterate. Otherwise, the  
algorithm fails and returns the initial point.  
As mentioned in the Introduction, the crucial novelty of Algorithm~\ref{alg:dpsgd}  
and Algorithm~\ref{alg:robust_gradient_est} lies in bounding the sensitivity  
of each solution $x_t$ by incorporating the (coordinate-wise) robust  
statistics into SGD.

% As discussed before, we apply the (coordinate-wise) geometric median into the SGD algorithm, and show that the iteration-sensitivity can always be bounded in terms of the $\ell_\infty$ norm when the number of ``bad'' users does not exceed the ``break point''.

% Our algorithm framework is based on SGD.
% For the $t$-th phase, we get solution $x_t$ and then take a batch of $B$ users, denoted by $\{Z_{i,t}\}_{i\in[B]}$.
% Each user owns $m$ functions and can run their own gradient descent freely with batch size $K$ from $[1,m]$.
% We take $K=m$ for simplicity; that is, each user takes the average of the $m$ gradients and does one descent step, and we get $\{x_1^{Z_{i,t}}\}_{i\in[B]}$.
% Then we let $x_{t+1}:=\arg\min_{x}\sum_{i\in[B]}\|x-x_1^{Z_{i,t}}\|_\infty$ be the geometric median over the $B$ points. 

\begin{algorithm2e}
\caption{Gradient Estimation based on Robust Statistics}
\label{alg:robust_gradient_est}
\textbf{ Input:} a set of $d$-dimensional vectors $\{X_i\}_{i\in[B]}$, threshold parameter $\varsigma>0$\;
%Initialize a zero vector $X_{est}=\mathbf{0}$\;
\For{Each dimension $j=1,\ldots,d$}
{
Compute the robust statistics $X_{\rs}[j]$, and the mean $\bx[j]$ over $\{X_{i}[j]\}_{i\in[B]}$\;
\If{$|X_{\rs}[j]-\bx[j]|\ge \varsigma$}
{
Set $X_{est}[j]=\Pi_{B(Y_j,\varsigma)}(\bx[j])$\;
}
\Else
{
Set $X_{est}[j]=\bx[j]$\;
}
}
\textbf{ Return $X_{est}$}
\end{algorithm2e}


We utilize robust statistics in the  
gradient estimation sub-procedure. 
We make the following assumptions regarding the robust statistics used:

\begin{assumption}
\label{assum:prop_geo_median}
    Given a set $\{X_i\}_{i \in [B]}$ of vectors, let $X_{\rs}$ be  
    any robust statistic satisfying the following properties:
    
    (i) For any $\rho \ge 0$, if there exists a point $X'$ such  
        that more than $B/2$ points lie within $B_\infty(X', \rho)$,  
        then $X_{\rs} \in B_\infty(X', \rho)$.
        
(ii) If we perturb each point $Y_i = X_i + \iota_i$ such that  
        $\|\iota_i\|_\infty \le \Delta$ for any $\Delta \ge 0$, and let  
        $Y_{\rs}$ be the robust statistic of $\{Y_i\}$, then  
        $\|X_{\rs} - Y_{\rs}\|_\infty \le \Delta$.
        
    (iii) For any real numbers $a$ and $b$, if $Z_i = aX_i + b$ for  
        each $i \in [B]$, and $Z_{\rs}$ is the corresponding robust  
        statistic of $\{Z_i\}_{i \in [B]}$, then $Z_{\rs} = aX_{\rs} + b$.  
\end{assumption}

\begin{remark}
    Common robust statistics, such as the (coordinate-wise) median and trimmed mean,  
    satisfy Assumption~\ref{assum:prop_geo_median}.
    %Pasin: I'm commenting the following out since I don't think all robust statistics are computed in coordinate-wise manner.
    %One can verify  
    %whether the robust statistic satisfies Assumption~\ref{assum:prop_geo_median}  
    %in one dimension, as robust statistics can be computed in a  
    %coordinate-wise manner.
\end{remark}
\vspace{-2mm}
In Algorithm~\ref{alg:robust_gradient_est}, we output means if they are close to the robust statistics to control the bias, and project the means onto the sphere around the robust statistics in a coordinate-wise manner when they are far apart.  
However, we still need to ensure that the sensitivity remains bounded when the projection is operated.  
The following technical lemma plays a crucial role in establishing iteration sensitivity to deal with the sensitivity with potential projection operations.
% Its proof can be found in the Appendix:
\vspace{-1mm}

\begin{restatable}{lem}{projmeantors}
\label{lm:proj_mean_to_rs}
Consider four real numbers $a,b,c,d$, such that $|a-b|\le 1$, and $|c-d|\le 1$.
Let $c'=\Pi_{B(a,r)}(c)$ and $d'=\Pi_{B(b,r)}(d)$ for any $r\ge 0$.
Then, we have $|c'-d'|\le 1.$
\end{restatable} 


Unfortunately, we are unaware of any robust statistic satisfying  
Assumption~\ref{assum:prop_geo_median} in high dimensions under the  
$\ell_2$-norm, and Lemma~\ref{lm:proj_mean_to_rs} does not hold in high  
dimensions either. These limitations restrict the applicability of our  
techniques in high-dimensional Euclidean spaces; see Section~\ref{sec:discussion}.  

Let $\{x_t\}_{t \in [T]}$ and $\{y_t\}_{t \in [T]}$ be two trajectories  
corresponding to neighboring datasets that differ by one user. The  
crucial technical novelty is that, for any $t \in [T]$, we can control  
$\|x_t - y_t\|_{\infty}$ as long as the number of ``bad'' users in each  
phase ($B$ in total) does not exceed the ``break point'', say $2B/3$.  
Without loss of generality, assume that $Z_{1,1} \neq Z_{1,1}'$ is the  
differing user in the neighboring dataset pairs $(\calD, \calD')$  
considered in the following proof.  

The first property of Assumption~\ref{assum:prop_geo_median} ensures that when the number of ``bad'' users in each phase does not exceed the  ``break point'' $2B/3$, the robust statistic remains close to most of the gradients, allowing us to control $\|x_1 - y_1\|_\infty$.  
To formalize this, we say that the neighboring dataset pair 
$(\calD, \calD')$ is $\rho$-\textit{aligned} if there exist points  
$X'$ and $Y'$ such that $|X_{\good}| \ge 2B/3$ and  
$|Y_{\good}| \ge 2B/3$, where  
\[
    X_{\good} = \{q_1(Z_{i,1}) : q_1(Z_{i,1}) \in B_{\infty}(X', \rho),  
    i \in [B]\},  \text{ and }
\]  
\[
    Y_{\good} = \{q_1'(Z_{i,1}') : q_1'(Z_{i,1}') \in B_{\infty}(Y', \rho),  
    i \in [B]\}.  
\]  
This definition essentially states that the number of ``bad'' users does  
not exceed the ``break point'' in either $\calD$ or $\calD'$, ensuring  
that most gradients remain well-aligned within a bounded region.

\begin{restatable}{lem}{itesensitivitybase}
    \label{lm:ite_sensitivity_base}
    For some (unknown) parameter $\rho > 0$, suppose $(\calD, \calD')$  
    is $\rho$-aligned. Then, by running Algorithm~\ref{alg:robust_gradient_est}  
    with threshold parameter $\varsigma \ge 0$, we have $\|x_1 - y_1\|_\infty \le \eta(4\rho + 2\varsigma)$.
\end{restatable}


% Now the sequential sensitivity can be bounded by induction, for which the base case, $\|x_1-y_1\|_\infty$ is already bounded.
% Say $\|x_{t-1}-y_{t-1}\|_\infty$ is bounded,
% then by Lemma~\ref{lm:contractivity}, we can show that $\|x_{j}^{Z_{i,t}}-y_{j}^{Z_{i,t}}\|_\infty\le \|x_{t-1}-y_{t-1}\|_\infty$.
% We then treat $x_{j}^{Z_{i,t}}-y_{j}^{Z_{i,t}}$ as the perturbation and apply the second property in Assumption~\ref{assum:prop_geo_median}, which leads to that $\|x_t-y_t\|_\infty\le \|x_{t-1}-y_{t-1}\|_\infty$.
% The formal statements can be found in Lemma~\ref{lm:iteration_sensitivity}.
The sequential sensitivity can be bounded using induction, with the base  
case $\|x_1 - y_1\|_\infty$ already established. The formal statement  
is provided in Lemma~\ref{lm:iteration_sensitivity}.  

\begin{algorithm2e}
\caption{Concentration Test}
\label{alg:out_rem}
\textbf{ Input:} Dataset $\calD=(Z_1,\ldots,Z_B)$, privacy parameters $\epsilon,\delta$, parameters $\tau,\upsilon$\;
\For{$t=1,\ldots,T$}
{ 
Receive a new concentration query $q_t:\calZ\to\R^d$\;
Define the concentration score
\begin{align}
\label{eq:concentration_score_def}
    \qcon_t(\calD,\tau):=\frac{1}{B}\sum_{Z\in\calD}\sum_{Z'\in \calD}\exp(-\tau\|q_t(Z)-q_t(Z')\|_\infty)\;
\end{align}
\textbf{ Return }$\AboTh(\qcon_t, \epsilon/2, \upsilon)$
}
\end{algorithm2e}


\begin{restatable}[Iteration Sensitivity]{lem}{iterationsensitivity}
\label{lm:iteration_sensitivity}  
    If we use a robust statistic satisfying Assumption~\ref{assum:prop_geo_median}  
    in Algorithm~\ref{alg:robust_gradient_est}, then for all $t \in [T]$, we have  $\|x_t - y_t\|_\infty \le \|x_1 - y_1\|_\infty$.
\end{restatable}

Lemmas~\ref{lm:ite_sensitivity_base} and \ref{lm:iteration_sensitivity}  
together establish the iteration sensitivity of Algorithm~\ref{alg:dpsgd}.

\paragraph{ Query Sensitivity of Concentration Test (Algorithm~\ref{alg:out_rem}):}
We have established iteration sensitivity for any aligned neighboring  
dataset pair $(\calD, \calD')$. Next, we analyze the influence of the  
concentration test, which we use to check if the number of ``bad'' users exceed the ``break point''.

To apply the privacy guarantee of $\AboTh$  
(Lemma~\ref{thm:Above_Threshold}), it suffices to bound the sensitivity  
of each query in the concentration test.  
Recall that we assume $Z_{1,1} \neq Z_{1,1}'$ in the neighboring datasets.  
Thus, by the definition (Equation~\eqref{eq:concentration_score_def}), it is straightforward to observe that  
\begin{align}
\label{eq:query_sensitivity_qcon_one}
    |\qcon_1(\calD, \tau) - \qcon_1(\calD', \tau)| \le 2.  
\end{align}  

Next, we consider the sensitivity of $\qcon_t$ for $t \ge 2$.  
The sensitivity is proportional to $\|x_t - y_t\|_\infty$, which we have  
already bounded by $\|x_1 - y_1\|_\infty$.  
Note that we can bound the iteration sensitivity if the neighboring  
datasets are aligned, meaning the number of ``bad'' users does not  
exceed the ``break point''. We first show that if the number of ``bad''  
users exceeds the ``break point'', the algorithm is likely to halt  
after the first step by failing the first test.


\begin{restatable}{lem}{sensitivitybase}
    \label{lm:sensitivity_base}
Suppose $B\ge \frac{100\log(T/\delta)}{\epsilon}, \epsilon\le O(1)$ and we set $\upsilon=0.9B+\frac{2\log(T/\delta)}{\epsilon}$.
Suppose for any point $Y$, we get $|X_{\good}|<B/3$ where $X_{\good}=\{q_1(Z_{i,1}):q_1(Z_{i,1})\in B_{\infty}(Y,1/\tau),i\in[B]\}$.
Then with probability at least $1-\delta/T\exp(\epsilon)$, the $\AboTh$ returns $a_1=\bot$.
\end{restatable}






% \begin{lemma}[Query Sensitivity: Part One]
% \label{lm:query_diff}
% Consider two initial points $x$ and $y$ such that $\|x-y\|_\infty \le \nu$, and get $\{x_{t}^Z\}_{t\in[m/K]}$ and $\{y_{t}^Z\}_{t\in[m/K]}$ respectively from running $\PGD(x,\eta,Z,K)$ and $\PGD(y,\eta,Z,K)$, defined from Equation~\eqref{eq:PGD_each_user}.
% Similarly, we get $\{x_t^{Z'}\}_{t\in[m/K]}$ and $\{y_t^{Z'}\}_{t\in[m/K]}$ for another user $Z'$.
% Then under Assumptions~\ref{assum:lispchitz_smooth}, with $\eta\beta\le1$, for any $t\in[m/K]$, we have
% \begin{align*}
%     \Bigg|\|x_t^Z-x_t^{Z'}\|_\infty-\|y_t^{Z}-y_t^{Z'}\|_\infty\Bigg|\le 2\nu \eta\beta.
% \end{align*}
% \end{lemma}

% \begin{proof}
%     % We prove the statement by induction.
%     % As for the basic case when $t=1$, we have
%     % \begin{align*}
%     %    & \|x_t^Z-x_t^{Z'}\|_\infty-\|y_t^Z-y_t^{Z'}\|_\infty\\
%     %    =& \|x_t^Z-x-(x_t^{Z'}-x)\|_\infty-\|y_t^Z-y-(y_t^{Z'}-y)\|_\infty\\
%     %    \le & \|x_t^Z-x-(y_t^Z-y)+(y_t^{Z'}-y)-(x_t^{Z'}-x)\|_\infty\\
%     %    \le & \|x_t^Z-x-(y_t^Z-y)\|_\infty+\|(y_t^{Z'}-y)-(x_t^{Z'}-x)\|_\infty\\
%     %    \le & 2\eta\nu\beta,
%     % \end{align*}
%     % where the last inequality comes from the assumption on smoothness.

%     % Now suppose the condition holds for any $t\le t'$, consider the case when $t=t'+1$.
%     Letting $x_0^Z=x_0^{Z'}=x$ and $y_0^Z=y_0^{Z'}$, notice that
%     \begin{align*}
%         &\|(x_t^Z-x_{t-1}^Z)-(y_t^Z-y_{t-1}^Z)\|_\infty \\
%         \le&  \eta/K\|\sum_{j\in[K]}\nabla f(x_{t-1}^Z;z_{t,j})-\nabla f(y_{t-1}^Z;z_{t,z})\|_\infty \\
%         \le & 2\eta\beta \|x_{t-1}-y_{t-1}^Z\|_\infty\\
%         \le & 2\eta\beta\nu.
%     \end{align*}
    
%  Hence, we have
%     \begin{align*}
%         & \|x_t^Z-x_t^{Z'}\|_\infty-\|y_t^Z-y_t^{Z'}\|_\infty\\
%        =& \|\sum_{i=1}^{t}(x_i^Z-x_{i-1}^Z)-\sum_{i=1}^t(x_i^{Z'}-x_{i-1}^{Z'})\|_\infty-\|\sum_{i=1}^{t}(y_i^Z-y_{i-1}^Z)-\sum_{i=1}^t(y_i^{Z'}-y_{i-1}^{Z'})\|_\infty\\
%        \le & \|\sum_{i=1}^{t} (x_i^Z-x_{i-1}^Z)-(y_i^Z-y_{i-1}^Z)\|_\infty+\|\sum_{i=1}^t(y_i^{Z'}-y_{i-1}^{Z'})-(x_i^{Z'}-x_{i-1}^{Z'})\|_\infty\\
%        \le & 2t\nu\eta\beta.
%     \end{align*}
%     This completes the proof.
% \end{proof}

We now analyze the query sensitivity between the aligned neighboring  
datasets.

\begin{restatable}[Query Sensitivity]{lem}{querysensitivity}
    \label{lm:query_sensitivity}
Suppose $6\beta\eta B\le1$.
Suppose $(\calD,\calD')$ is $(1/\tau)$-aligned and set threshold parameter $\varsigma=1/\tau$ in Algorithm~\ref{alg:mean_est_with_AT}, the sensitivity of the query is bounded by at most $2$.
That is,
\begin{align*}
    |\qcon_t(\calD,\tau)-\qcon_1(\calD',\tau)|\le 2, & & \forall t\ge 2.
\end{align*}
\end{restatable}



Equation~\eqref{eq:query_sensitivity_qcon_one} shows that the sensitivity is always bounded for $\qcon_1$.  
Lemma~\ref{lm:sensitivity_base} shows that if the number of ``bad''  
users exceeds the ``break point'', we obtain $a_1 = \bot$, and  
the query sensitivities of the subsequent queries do not need to be considered.  
Lemma~\ref{lm:query_sensitivity} establishes the query sensitivity  
in the concentration test when the neighboring datasets are aligned,  
and the number of "bad" users is below the threshold.

\paragraph{Privacy proof.}
% Consider the implementation on two neighboring datasets $\calD$ and $\calD'$.
% Without loss of generality, we assume that the different users appeared in the first batch, that is, $t=1$.

%Now, we can complete the proof of the privacy guarantee.

%The final privacy guarantee is stated below. 
The final privacy guarantee--stated formally below--now easily follows from the previous lemmas.
% Due to space constraint, 
The full proof is deferred to Appendix~\ref{app:privacy-proof}.

\begin{restatable}[Privacy Guarantee]{lem}{privacyguarantee}
    \label{lm:privacy_guarantee}
    Under Assumption~\ref{assum:lispchitz_smooth} and Assumption~\ref{assump:dia_dominant}, suppose $\epsilon\le O(1), B\ge\frac{100\log(T/\delta)}{\epsilon}$, then Algorithm~\ref{alg:loacalizatioin} is $(\epsilon,\delta)$-user-level-DP.
\end{restatable}





\paragraph{Utility proof.}
We apply the localization framework in private optimization to finish the utility argument.
We analyze the utility guarantee of Algorithm~\ref{alg:dpsgd} based on the classic convergence rate of SGD on smooth convex functions (Lemma~\ref{lm:sgd_smooth}) as follows:
% The following classic result states the convergence rate of SGD for smooth convex functions.



% \Daogao{Clean the notations..}
% We have the following lemma:

% \begin{lemma}[\cite{LLA24}]
% \label{lemma_tech_core}
% Assume $f(\cdot, z)$ is convex, $G$-Lipschitz, and $\beta$-smooth on $\calX$ with $\eta \leq 1/\beta$. Let $\tilde{x} \gets SGD(D, \eta, T, x_0,1)$ and $\tilde{y} \gets SGD(D', \eta, T, x_0,1)$ be two independent runs of projected SGD, where
% $D, D' \sim \calP^T$ are i.i.d. Then, with probability at least $1 - \zeta$, we have \[
% \|\tilde{x} - \tilde{y}\|_2 \lesssim \eta G\sqrt{T \log(dT/\zeta)}.
% \]
% \end{lemma}

% As may be noticed, the naive bound we can get is $\|\title{x}-\title{y}\|_2\le 2\eta LT$.
% Hence, the distributional assumption on $\calD$ and $\calD'$ improves the stability from $\Tilde{O}(\eta LT)$ to $\Tilde{O}(\eta L\sqrt{T})$, which is crucial in getting improved results in user-level setting.

% We generalize it into a batched version of SGD, that is the batch size of each iterate is captured by a parameter $K\ge 1$:
% \Daogao{Overuse notation $T$...}

% \begin{lemma}
% \label{lm:batched_tech_core}
%     Assume $f(\cdot, z)$ is convex, $G$-Lipschitz, and $\beta$-smooth on $\calX$ with $\eta \leq 1/\beta$. Let $\tilde{x} \gets SGD(D, \eta, T, x_0,K)$ and $\tilde{y} \gets SGD(D', \eta, T, x_0,K)$ be two independent runs of projected SGD, where
% $D, D' \sim \calP^{TK}$ are i.i.d. Then, with probability at least $1 - \zeta$, we have \[
% \|\tilde{x} - \tilde{y}\|_2 \lesssim \eta G\sqrt{T \log(dT/\zeta)/K}.
% \]
% \end{lemma}

% \begin{proof}
% Let $g_t:= \frac{1}{K}\sum_{i\in[K]}\nabla f(x_t, z_{t,i})$ for $\{z_{t,i}\}_{i\in[K]}$ drawn uniformly from $D$ without replacement and $g_t':=  \frac{1}{K}\sum_{i\in[K]}\nabla f(y_t, z'_{t,i})$ for $z'_{t,i}$ drawn uniformly from $D'$ without replacement. Let $F(x) := \E_{z \sim \calP}[f(x,z)]$. 

% We will prove that $\|x_t - y_t\| \lesssim \eta L\sqrt{T \log(dT/\zeta)/K}$ with probability at least $1 - \zeta/t$ for all $t \in [T]$. Note that this implies the lemma. We proceed by induction. The base case, when $t=0$, is trivially true since $x_0 = y_0$. For the inductive hypothesis, suppose there is an absolute constant $c > 0$ such that with probability at least $1-t\zeta/T$, we have 
% \begin{align*}
%     \|x_{i}-y_i\|\le  c  \eta L\sqrt{i\cdot \log(dT/\zeta)/K},
% \end{align*}
% $\forall i \le t$. Then, for the inductive step, we have by non-expansiveness of projection onto convex sets, that
% \begin{align}
% \label{eq: thingy}
%     \|x_{t+1}-y_{t+1}\|^2 \le &~ \|x_t-\eta g_t-(y_t-\eta g_t')\|^2 \nonumber \\
%     =& ~ \|x_t-\eta \nabla F(x_t)-(y_t-\eta \nabla F(y_t))-\eta (g_t- \nabla F(x_t)-g_t'+\nabla F(y_t)   )\|^2 \nonumber\\
%     =& ~ \|x_t-\eta\nabla F(x_t)-(y_t-\eta \nabla F(y_t))\|^2 \nonumber\\ 
%     &~-2\eta \langle  x_t-\eta\nabla F(x_t)-(y_t-\eta \nabla F(y_t)),g_t- \nabla F(x_t)-g_t'+\nabla F(y_t)\rangle \nonumber\\
%     &~+ \eta^2 \|g_t-\nabla F(x_t)-g_t'+\nabla F(y_t)\|^2 \nonumber \\
%     \stackrel{(i)}{\le} & ~ \|x_t-y_t\|^2-2\eta \langle  x_t-\eta\nabla F(x_t)-(y_t-\eta \nabla F(y_t)),g_t- \nabla F(x_t)-g_t'+\nabla F(y_t)\rangle \nonumber \\
%     &~+ \eta^2 \|g_t-\nabla F(x_t)-g_t'+\nabla F(y_t)\|^2, 
% \end{align}
% where $(i)$ follows from the non-expansive property of gradient descent on smooth convex function for $\eta \le 1/\beta$~\cite{hardt16}.

% For any $t\in T$, we have
% \begin{align*}
%  \Pr\Big[\eta^2 \|g_t-\nabla F(x_t)-g_t'+\nabla F(y_t)\|^2\ge 4\log(Td/\zeta)\eta^2L^2/K\Big]\le \zeta/T .  
% \end{align*}
% Conditional on this event in the following argument.

% Define $a_t:=-2\eta \langle  x_t-\eta\nabla F(x_t)-(y_t-\eta \nabla F(y_t)),g_t- \nabla F(x_t)-g_t'+\nabla F(y_t)\rangle$.
% By Inequality~\eqref{eq: thingy} and the inductive hypothesis, we obtain
% \begin{align*}
%     \|x_{t+1}-y_{t+1}\|^2\le 4t\log(Td/\zeta)\eta^2L^2/K+\sum_{i=1}^{t}a_t.
% \end{align*}
% It remains to bound $\sum_{i=1}^t a_i$.
% Note that $\E[a_i \mid a_1,\cdots,a_{i-1}]=0$ and $g_t-\nabla F(x_t)$ is $\nSG(\log(d/\zeta)/\sqrt{K})$. 
% By Lemma~\ref{lm:inner_product_nSG}, we know there is a constant $c' > 0$ such that $a_i$ is $\nSG(c' \eta L \|x_i-y_i\|/\sqrt{K})$ for all $i$.
% Hence by Theorem~\ref{thm:hoeffding_nSG}, we know
% \begin{align*}
% \mathbb{P}\left[\left|\sum_{i=1}^{t}a_i\right|\ge c'\eta L\sqrt{\log(dT/\zeta)\sum_{i\le t}\|x_i-y_i\|^2/K}\right]\le 1-\zeta/T.
% \end{align*}

% Conditional on the event that $\|x_{i}-y_{i}\|\le c\sqrt{\log(dT/\zeta)}\eta L\sqrt{i/K}$ for all $i \leq t$ (which happens with probability $1-t\zeta/T$ by the  inductive hypothesis), we know
% \begin{align*}
% \mathbb{P}\left[\left|\sum_{i=1}^{t}a_i\right|\ge (cc')tL^2\eta^2\log(dT/\zeta)/K \middle| \|x_i-y_i\|\le c\log(dT/\zeta)\eta L\sqrt{i/K},\forall i\le t\right]\le 1-\zeta/T.
% \end{align*}
% Hence, as long as $4t+cc't\le c^2(t+1)$, we know
% \begin{align*}
%     \mathbb{P}\left[\|x_{t+1}-y_{t+1}\|^2\ge c^2\log(dT/\zeta)\eta^2L^2(t+1)/K \middle|  \|x_i-y_i\|\le c\log(dT/\zeta)\eta L\sqrt{i/K},\forall i\le t \right]\le 1-\zeta/T.
% \end{align*}
% Combining the above elements completes the inductive step, showing that 
% \[\|x_{t+1}-y_{t+1}\|\le c\sqrt{(t+1)\log(dT/\zeta)/K}\eta L\]
% with probability at least $1-(t+1)\zeta/T$.
% This completes the proof.
% \end{proof}


% \begin{lemma}
% \label{lem:concentrated_grd}
% Under Assumption~\ref{assum:lispchitz_smooth},
% for any fixed $x$ and for each $Z_i$, if each item in $Z_i$ is drawn i.i.d. from $\calP$, with probability at least $1-\gamma/n$, we have
% \begin{align*}
%     \|\nabla F(x;Z_i)-\nabla F_\calP(x)\|_2\lesssim \frac{G\log(nd/\gamma)}{\sqrt{m}},
% \end{align*}
% \end{lemma}



% \subsection{Further Potential Improvements}
% It is interesting to see if we can generalize the results to the $\ell_2$ norm by utilizing other estimators.
% In the $\ell_2$ norm, we may use and generalize the following stability lemma from \cite{LLA24}.



\begin{restatable}{lem}{dgsgdutility}
    \label{lm:dpsgd_utility}
    Let $x\in\calX$ be any point in the domain.
    Suppose the data set $\calD$ of the users, whose size $|\calD|$ is larger than $\frac{100\log(T/\delta)}{\epsilon}$, is drawn i.i.d. from the distribution $\calP$.
    Setting $\tau=G\log(nmd/\omega)/\sqrt{m}$
    then the final output $\bar{x}$ of Algorithm~\ref{alg:dpsgd} satisfies that
    \begin{align*}
        \E[F(\bar{x})-F(x)]\lesssim \left(\beta+\frac{1}{\eta} \right)\frac{\E[\|x_0-x\|^2]}{T}+\frac{\eta G^2d}{Bm}+GDd\omega.
    \end{align*}
\end{restatable}



% \begin{proof}
% We reindex the iterates by $y_{i,(t-1)m/K+j}=x_{j}^{Z_{i,t}}$ when $j\neq m/K$, ane define $y_{i,tm/K}=x_{t}$ for $0\le t\le T$.

% Then the average iterate $\bar{x}=\frac{K}{mTB}\sum_{i\in[B],j\in[m/K],t\in[T]}x_j^{Z_{0,t}}=\frac{K}{mTB}\sum_{i\in[B],j\in[Tm/K]}y_{i,j}$.

% To prove the statement, without loss of generality, it suffices to show
% \begin{align*}
%     \E[F(\frac{K}{Tm}\sum_{j\in[Tm/K]}y_{1,j})-F(x)]\lesssim (\beta+\frac{1}{\eta})\frac{\E[\|x_0-x\|^2]}{Tm/K}+\frac{\eta(TG^2d m/K^2) }{Tm/K}.
% \end{align*}

% Let $g_j$ be the gradient estimate, that is
% \begin{align*}
%     y_{1,j}=\Pi_\calX(y_{1,j-1}-\eta g_j).
% \end{align*}

% By Lemma~\ref{lm:sgd_smooth}, 
% it suffices to bound $\sum_{j\in[Tm/K]}\E\|g_j-\nabla F(y_{1,j})\|_2^2\le \eta TG^2dm/K^2$.

% For any $j\mod (m/K)\neq 0$, we know $\E g_j=\nabla F(y_{1,j})$ and $\E\|g_j-\nabla F(1,j)\|^2_2\le G^2/K$.
% For the other case when $j\mod (m/K)\equiv 0$, define $\Tilde{g}_j$ be the gradient estimator for which 
% \begin{align*}
%     x_{m/K}^{Z_{1,jK/m}}=\Pi_{\calX}(x_{m/K-1}^{Z_{1,jK/m}}-\eta \Tilde{g}_j).
% \end{align*}

% We have
% \begin{align*}
%     \E\|g_j- \nabla F(y_{1,j})\|_2^2\le & 2\E\|g_j-\Tilde{g}_j\|_2^2+2\E\|\Tilde{g}_j-\nabla F(y_{1,j})\|_2^2.
% \end{align*}
% Similarly, we can bound $\E\|\Tilde{g}_j-\nabla F(y_{1,j})\|_2^2\le G^2/K$ by the assumptions on i.i.d. and Lipschitz.
% It remains to bound $\E\|g_j-\tg_j\|_2^2$.

% By Lemma~\ref{lm:batched_tech_core} and Lemma~\ref{lm:prop_geo_median}, we know that 
% $
% \|y_{1,j}-x_{m/K}^{Z_{1,jK/m}}\|_\infty\lesssim \eta G\sqrt{m\log(dm)/K^2}$,
% and hence we can bound 
% $\E\|g_j-\tg_j\|_2^2\le\|y_{1,j}-x_{m/K}^{Z_{1,jK/m}}\|_2^2/\eta^2+G^2\lesssim G^2md\log(dm)/K^2$.
% This completes the proof.
% \Daogao{We need a new lemma to handle the shift for the utility when $K$ is large...}
% \end{proof}

Now we apply the localization framework.
We set $\omega=1/(nmd)^3$ to make the term depending on it negligible.
The proof of the following lemma mostly follows from \cite{FKT20}.

\begin{restatable}[Localization]{lem}{Localization}
    \label{lm:localization}
Under Assumption~\ref{assum:lispchitz_smooth} and Assumption~\ref{assump:dia_dominant}, suppose $\beta\le\frac{G}{D}(\frac{\sqrt{n}\epsilon}{\sqrt{m}\log(nmd/\delta})$, $n\ge \log^2(nd/\delta)/\epsilon, \epsilon\le O(1)$ and $ m\le n^{O(\log\log n)}$.
Set $\eta\le\frac{D}{G}\cdot \min\{ \frac{B\sqrt{m}}{\sqrt{n}} ,  \frac{\sqrt{m}\epsilon}{\sqrt{d\log(1/\delta)\log( nmd)}}\}$, $B=100\log(mnd/\delta)/\epsilon$, $\tau=O(G\log(nmd)/\sqrt{m})$ and $\upsilon=0.9B+\frac{2\log(T/\delta)}{\epsilon}$.
If the dataset is drawn i.i.d. from the distribution $\calP$,
the final output $x_S$ for Algorithm~\ref{alg:loacalizatioin} satisfies
\begin{align*}
    \E[F(x_S)-F(x^*)]\le \Tilde{O}\Big(GD\Big(\frac{d}{\sqrt{mn}}+\frac{d^{3/2}}{n\epsilon^2\sqrt{m}}\Big)\Big).
\end{align*}
\end{restatable}

\noindent\textbf{ Main Result:}
Theorem~\ref{thm:main_result} directly follows from Lemma~\ref{lm:localization} and Lemma~\ref{lm:privacy_guarantee}.


% \begin{lemma}[Localization]
% \label{lm:localization}
% Under Assumption~\ref{assum:lispchitz_smooth} and Assumption~\ref{assump:dia_dominant},
% the final output $x_k$ for Algorithm~\ref{alg:loacalizatioin} satisfies that
% \begin{align*}
%     \E[F(x_k)-F(x^*)]\le \Tilde{O}(\frac{\sqrt{d}}{\sqrt{mn\epsilon}}+\frac{d}{n\epsilon\sqrt{m}}).
% \end{align*}
% \end{lemma}

% \begin{proof}
% We need $\eta\le\min\{ \frac{K}{\epsilon\sqrt{nmd}} ,   \frac{K\epsilon}{d\sqrt{m}}\}$.
% Let $\bx_0=x^*$ and $\zeta_0=x_0-x^*$.
% By the assumption, we know $\|\zeta_0\|_2\le D\sqrt{d}$.
% Recall that by definition $\eta\le \frac{D}{G}\cdot\frac{K\epsilon}{d\sqrt{m}}$, for all $t\ge 0$,
% \begin{align*}
%     \E[\|\zeta_t\|_2^2]=d\sigma_t^2=\frac{G^2d^2m}{K^2\epsilon^2}\cdot\frac{D^2K^2\epsilon^2}{mdG^2(\log m)^{2t}}\le (\frac{D}{\log^{-t} m})^2.
% \end{align*}
% Then by Lemma~\ref{lm:dpsgd_utility}, we have
% \begin{align*}
%     \E[F(x_k)]-F(x^*)=&\sum_{t=1}^{k}\E[F(\bx_{t}-\bx_{t-1})]+\E[F(x_k)-F(\bx_k)]\\
%     \le & \sum_{t=1}^{k}(\frac{\E[\|\zeta_{i-1}\|_2^2]}{\eta_i(T_im/K)}+\frac{\eta_iG^2d}{2K})+G\E[\|\zeta_k\|_2]\\
%     \le& \sum_{i=1}^{k}(\frac{\log m}{2})^{-i}(\frac{D^2}{\eta nm\epsilon/K}+\frac{\eta G^2d}{2K})+\frac{GD}{(\log m)^{\log n}}\\
%     \lesssim &   GD(\frac{\sqrt{d}}{\sqrt{nm}}+\frac{d}{n\sqrt{m}\epsilon}).
% \end{align*}
% \end{proof}


