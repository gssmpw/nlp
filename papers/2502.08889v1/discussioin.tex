\section{Conclusion}
\label{sec:discussion}

We present a linear-time algorithm for user-level DP-SCO that leverages (coordinate-wise) robust statistics in the gradient estimation subprocedure and provide a lower bound that nearly matches the upper bound up to logarithmic terms and an additional dependence on $\epsilon$. Despite this progress, several limitations and open problems remain, some of which we highlight below.
%, which are discussed in Appendix~\ref{sec:limitation}.

\begin{itemize}
    \item \textbf{Generalization to Euclidean and Other Spaces.}  
    Extending our results to Euclidean and other spaces is an interesting but technically challenging problem. One key challenge is the lack of robust statistics that are $1$-Lipschitz under perturbations in the high-dimensional $\ell_2$-norm (see the second item in Assumption~\ref{assum:prop_geo_median}).
    One may wonder whether commonly used robust statistics, such as the geometric median, are stable in this sense. However, we provide a simple counterexample involving the geometric median in the Appendix (Section~\ref{sec:counter_example}).  
    
    Another challenge arises from our approach of projecting the mean towards the robust statistic to ensure unbiased gradient estimation. This projection is $1$-Lipschitz under perturbations in one dimension (Lemma~\ref{lm:proj_mean_to_rs}), but there are known counterexamples in higher dimensions \cite[Lemma 16]{alt24}. Overcoming these issues is crucial for extending our method to general spaces.

    \item \textbf{ Additional Assumption on Diagonal Dominance.}  
    Our analysis assumes that the Hessians of functions in the universe are diagonally dominant, which is primarily used to show that gradient descent is contractive in the $\ell_\infty$-norm (Lemma~\ref{lm:contractivity}). This assumption is somewhat restrictive compared to the $\ell_2$-norm setting, where it is sufficient to assume that the operator norm of the Hessians is bounded (i.e., smoothness). Addressing the aforementioned challenges and generalizing our results to the Euclidean space could potentially eliminate this additional assumption.

    \item \textbf{ Suboptimal Dependence on $\epsilon$.}  
    Although our upper bound nearly matches the lower bound, it has a suboptimal dependence on $\epsilon$. This issue arises from the loose dependence on sensitivity. Specifically, we draw $B$ users at each step and compute their average gradient, with $B = \Tilde{O}(1/\epsilon)$. However, the final sensitivity remains roughly $\Tilde{O}(1/\sqrt{m})$ and does not improve with larger $B$. An improvement in the dependence on $\epsilon$ could be achieved if the sensitivity of the robust statistic could benefit from larger $B$.
\end{itemize}

Finally, it would be interesting to explore the use of robust statistics, such as the median used in this work, to address other private optimization problems.



   % \item {\bf Worse dependence on dimension $d$ and additional assumptions.} Our results are off the lower bound by a factor of $\sqrt{d}$.
    % This is due to the usage of the coordinate-wise geometric median.
    % If we can make the first item of Assumption~\ref{assum:prop_geo_median} work in $\ell_2$ norm, then we may be able to save the additional assumptions and achieve right dependence on the dimension.
    % \item {\bf Bias term from robust statistics}
    % \item {\bf Dependence on $\epsilon$.}
    % Even if we treat the dimension $d$ as constant, 
    % neither our result nor the previous works \cite{bassily2023user,LLA24} can achieve the optimal bound $O(1/\sqrt{mn})$ when $\epsilon=o(1)$.
    % However, in the classic item level (when $m=1$), we can achieve privacy free and match the infomation-theoretical lower bound $O(1/\sqrt{n})$ as long as $\epsilon\ge \Omega(1/\sqrt{n})$. 
    % This is potentially because we need to partition the users into $\Tilde{O}(1/\epsilon)$ groups for the outlier removal step. 
    % How to avoid the additional suboptimal dependence on $\epsilon$ is also a very interesting problem.

% \newpage 
% \section{Overview}

% There are four linear-time algorithms under item-level in my mind:\\
% 1. \cite{FKT20}: Requires smoothness $\beta\le\sqrt{n}+\sqrt{d}/\epsilon$.\\
% 2. \cite{zhang2022differentially,choquetteoptimal}: Require smoothness $\beta\le O(1)$.\\
% 3. \cite{carmon2023resqueing}. For non-smooth functions. But it is only (nearly) linear when $d\le \sqrt{n}$.

% It might be easier to generalize \cite{zhang2022differentially,choquetteoptimal,carmon2023resqueing} to user-level as all intermediate iterates are private, and hence it is easier to do outlier removal for those steps.
% But I guess privatizing \cite{zhang2022differentially,choquetteoptimal} might require $\beta\le O(1/\sqrt{m})$ or something similar to achieve optimal rates, which is good to check.
% It is more challenging and interesting to privatize algorithms from \cite{FKT20}, which is the stuff in the overleaf.


% \begin{algorithm2e}
% \caption{DP-SGD for demonstration purpose}
% {\bf Input:} dataset $\calD$, initial point $x_0$\;
% \For{$t=1,\cdots, n$}
% {
% $x_{t}\leftarrow  \SP(x_{t-1},\{g_{t,i}\}_{i\in[B]}) $
% }
% \label{alg:sgd_demo}
% \end{algorithm2e}

% In item-level, \cite{FKT20} show for simple SGD (where $B=1$ and we simply run GD: $x_t\leftarrow x_{t-1}-\eta g_t$), for neighboring datasets $\calD,\calD'$, we have $\|x_t-x_t'\|_2\le \eta G,\forall t\in[n]$ for $G$-Lipschitz functions.

% Let us simply assume that $g_{t,i}$ is the average of the $m$ gradients of user $Z_{t,i}$.
% The ideal (non-private) $\SP$ we want at the user level is that:
% we can show $\|x_t-x_t'\|_2\le O(\eta G/\sqrt{m})$ when the number of 'bad' users are smaller than some 'break point';

% Some approaches:\\
% 1. if we simply compute the average of $\{g_{t_i}\}_{i\in[B]}$, we can only guarantee $\|x_t-x_t'\|_2\le \eta G$;\\
% 2. if we run geometric median, it is unclear whether $\|x_t-x_t'\|_2\le O(\eta G/\sqrt{m})$. It might be false;\\
% 3. if we compute the concentration score and filter out the outliers before averaging the remaining users in the batch, we do not know how to argue about the sensitivity $\|x_t-x_t'\|_2$.

% In the Overleaf project, we demonstrate that $|x_t - x_t'|_\infty \leq \eta G / \sqrt{m}$ using a geometric median in coordinates. Additionally, we smooth the concentration score and utilize $\AboTh$ to privately verify whether the number of 'bad' users in each batch exceeds the 'break point.'

% In terms of results, we achieve a rate of $O\left(\frac{\sqrt{d}}{\sqrt{nm\epsilon}} + \frac{d}{n\sqrt{m}\epsilon}\right)$ with $\beta = O(\sqrt{n}m^{\Omega(1)})$, which improves upon the best known rate of $O\left(\frac{\sqrt{d}}{\sqrt{nm\epsilon^2}}\right)$ when $n \geq d$.

% Although the improvements are not particularly significant, the algorithmic challenges we addressed are quite interesting. I agree that exploring other potential applications of these new algorithmic techniques could strengthen the results further. For instance, could we recover the best-known results in the heavy-tailed setting by simply applying the geometric median over the gradients?

% Also, is there a better $\SP$ that can give us what we want and can achieve the optimal rate?
