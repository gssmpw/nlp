\newpage
\section{Details of Lower Bound}
\label{sec:lbproof}
Now we construct a lower bound for the weighted sign estimation error.

\paragraph{Weighted sign estimation error.}
We construct a distribution $\calP_1$ as follows: for each coordinate $k\in[d]$, we draw $\mu[k]$ uniformly random from $[-1,1]$, and $z_{i,j}[k]\sim\calN(\mu[k],m)$ i.i.d., for $i\in[n],j\in[m]$.
The objective is to minimize weighted sign estimation error with respect to $\mu$.

\begin{lemma}
\label{lm:sign_error_lb}
    Let $\epsilon\le 0.1, \delta\le 1/(dnm)$.
    For any $(\epsilon,\delta)$-user-level-DP algorithm $\calM$, 
    there exists a distribution $\calP_2$ such that $\|\E_{z\sim \calP_2}[z]\|_\infty\le 1$ and $\|z\|_\infty\le\Tilde{O}(\sqrt{m})$ almost surely, and,
    given dataset $\calD$ i.i.d. drawn from $\calP_2$, we have
    \begin{align*}
    \E_{\calD,\calM,\mu}\sum_{i=1}^{d} |\mu[i]|\cdot \ind\big(\sign(\mu[i])\neq\sign(\calM(\calD)[i])\big)\ge \Tilde{\Omega}(\frac{d^{3/2}}{n\epsilon}).
    \end{align*}
\end{lemma}

First, by the previous result, we can reduce the user-level to item-level setting.

\begin{lemma}[\cite{levy2021learning}]
\label{lm:reduction_to_item_level}
Suppose each user $Z_i,i\in[n]$ observes $z_{i,1},\ldots,z_{i,m} \stackrel{\text{i.i.d.}}{\sim}\calN(\mu,\sigma^2I_d)$ with $\sigma$ known.
For any $(\epsilon,\delta)$-User-level-DP algorithm $\calM_{\user}$, there exists an $(\epsilon,\delta)$-Item-level-DP algorithm $\calM_{\rmitem}$ that takes inputs $(\bZ_1,\ldots,\bZ_n)$ where $\bZ_i=\frac{1}{m}\sum_{j\in[m]}z_{i,j}$ and has the same performance as $\calM_{\user}$.
\end{lemma}

Hence by Lemma~\ref{lm:reduction_to_item_level}, it suffices to consider the item-level lower bound.
We prove the following lemma:

\begin{lemma}
\label{lm:sample_compelexity_lb}
Let $\{\mu[k]\}_{k\in[d]}\stackrel{i.i.d.}{\sim}[-\sigma,\sigma]$.
    Let $\{\bZ_1,\ldots,\bZ_n\}$ be i.i.d. drawn from $\calN(\mu,\sigma^2I_d)$.
    If $\calM: \R^{n\times d}\to\{-1,1\}^d$ is $(\epsilon,\delta)$-DP, and
    \begin{align*}
        \E_{\mu,\calD,\calM}\sum_{i=1}^{d} |\mu[i]|\cdot \ind\big(\sign(\mu[i])\neq\sign(\calM(\calD)[i])\big)\le \alpha\le  \frac{d\sigma}{8},
    \end{align*}
    then $n\ge \frac{\sqrt{d}}{32\epsilon}$.
\end{lemma}

By the invariant scaling, it suffices to consider the case when $\sigma=1$.
To prove Lemma~\ref{lm:sample_compelexity_lb}, we need the fingerprinting lemma:

\begin{lemma}[Lemma 6.8 in \cite{kamath2019privately}]
\label{lm:fingerprinting}
For every fixed number $p$ and every $f:\R^n\to[-1,1]$, define $g(p):=\E_{X_{1,\ldots n}\sim\calN(p,1)}[f(X)]$.
We have
    \begin{align}
    \label{eq:finger_printing_lem}
        \E_{X_{1,\ldots n}\sim\calN(p,1)}[(1-p^2)(f(X)-p)\sum_{i\in[n]}(X_i-p)]=(1-p^2)\frac{\d}{\d p}g(p).
    \end{align}
\end{lemma}

By choosing $p$ uniformly from $[-1,1]$, we have the following observation over the expectation on the RHS of Equation~\eqref{eq:finger_printing_lem}.

\begin{lemma}
\label{lm:expec_derivate}
We have
\begin{align*}
    \E_{p\sim[-1,1]}[(1-p^2)\frac{\d}{\d p}g(p)]= \E_{p}[g(p)\cdot p]. 
\end{align*}
\end{lemma}

\begin{proof}
Using integration by parts, we have
\begin{align*}
    \E_{p\sim[-1,1]}[(1-p^2)\frac{\d}{\d p}g(p)]= & \frac{1}{2}\int_{-1}^{1}(1-p^2)\frac{\d}{\d p}g(p)  \d p\\
    =&\frac{1}{2}\int_{-1}^{1}\Big(\frac{\d}{\d p}\big((1-p^2)g(p)\big)-g(p)\frac{\d}{\d p}\big(1-p^2\big)     \Big)\d p\\
    =&\int_{-1}^1 g(p)p \d p\\
    =&\E[g(p)\cdot p].
\end{align*}
\end{proof}

Now we use the fingerprinting lemma.
\begin{lemma}
One has
\begin{align*}
    \E\left[\sum_{i\in[n],k\in[d]} (1-\mu[k]^2) (\calM(\calD)[k]-\mu[k])\cdot(\bZ_i[k]-\mu[k])\right]= \E\left[ \sum_{k\in[d]}\calM(\calD)[k]\cdot\mu[k] \right].
\end{align*}

\end{lemma}
\begin{proof}
 Fix a column $k\in[d]$.
 
Construct the $f$ for our purpose. 
Define $f:\R^n\to [-1,1]$ to be 
\begin{align*}
    f(X):=\E_{\calD,\calM}[\calM(\calD^{-k}\| X)[k]].
\end{align*}
That is, $f(X)$ is the expectation of $\calM(\calD)[k]$ conditioned on $\bZ_i[k]=X_i,\forall i\in[n]$.
And define $g:[-1,1]\to[-1,1]$ to be
\begin{align*}
    g(p):=\E_{\mu^{-k},X_{1,\ldots n}\sim \calN(p,1)}[f(X)].
\end{align*}
That is $g(p)$ is the expectation of $\calM(\calD)[k]$ conditional on $\mu[k]=p$.

Now we can calculate
\begin{align*}
    &\E\Big[(1-\mu[k]^2)(\calM(\calD)[k]-\mu[k])\sum_{i\in[n]}(\bZ_i[k]-\mu[k])\Big]\\
    =& \E_{\mu[k]\sim[-1,1],X_{1,\ldots n}\sim\calN(\mu[k],1)}\Big[ (1-\mu[k]^2) (f(X)-\mu[k])\sum_{i\in[n]}(\bZ_i[k]-\mu[k])   \Big] \\
    \stackrel{(i)}{=}& \E_{\mu[k]} [g(\mu[k])\cdot \mu[k]]\\
    =& \E[\calM(\calD)[k]\mu[k]],
\end{align*}
where $(i)$ follows from Lemma~\ref{lm:fingerprinting} and Lemma~\ref{lm:expec_derivate}.
The statement follows by summation over $k\in[d]$.
 \end{proof}


A small weighted sign error means a large $\E\left[ \sum_{k\in[d]}\calM(\calD)[k]\cdot\mu[k] \right]$, as demonstrated by the following lemma:

\begin{lemma}
Let $\calM:\R^{n\times d}\to [-1,1]^d$.
    Suppose $\{\mu[k]\}\stackrel{i.i.d.}{\sim}[-1,1]$, and
    \begin{align*}
        \E_{\mu,\calD,\calM}\left[\sum_{k=1}^{d} |\mu[k]|\cdot \ind\big(\sign(\mu[k])\neq\sign(\calM(\calD)[k])\big)\right]\le \alpha,
    \end{align*}
    then we have 
    \begin{align*}
        \E\left[ \sum_{k\in[d]}\calM(\calD)[k]\cdot\mu[k] \right]\ge \frac{d}{2}-2\alpha.
    \end{align*}
\end{lemma}

\begin{proof}
Note that
\begin{align*}
    &\E\left[\sum_{k=1}^{d}|\mu[k]|\cdot \Big(\ind(\sign(\mu[k])\neq\sign(\calM(\calD)[k])) + \ind(\sign(\mu[k])=\sign(\calM(\calD)[k]) )\Big)\right] \\
    &=\E\left[\sum_{k=1}^d|\mu[k]|\right]=d/2.
\end{align*}

Moreover, one has that
\begin{align*}
    &\E\left[ \sum_{k\in[d]}\calM(\calD)[k]\cdot\mu[k] \right]\\
    =& \E\left[\sum_{k\in[d]}|\mu[k]|\cdot \Big(\ind(\sign(\mu[k])=\sign(\calM(\calD)[k]))-\ind(\sign(\mu[k])\neq \sign(\calM(\calD)[k]))\Big)\right]\\
    \ge& d/2-2\alpha.
\end{align*}
This completes the proof.
\end{proof}

It remains to show the sample complexity to achieve a large value of 
\begin{align}
\label{eq:large_correlation}
\E\left[\sum_{i\in[n],k\in[d]} (1-\mu[k]^2) (\calM(\calD)[k]-\mu[k])\cdot(\bZ_i[k]-\mu[k])\right]\ge d/2-2\alpha.    
\end{align}


% Roughly we can bound it as $n\sqrt{d}\epsilon/\sigma$, hence $n\ge \sqrt{d}\sigma/\epsilon$.
Now we complete the proof of Lemma~\ref{lm:sample_compelexity_lb}.

\begin{proof}[Proof of Lemma~\ref{lm:sample_compelexity_lb}]

Let $A_i=\sum_{k\in[d]}(1-\mu[k]^2)(\calM(\calD)[k]-\mu[k])(\bZ_i[k]-\mu[k])$.
We use the DP constraint of $\calM$ to upper bound $\E[A_i]$.

Let $\calD_{\sim i}$ denote $\calD$ with $\bZ_i$ replaced with an independent draw from $\calP_1$.
Define 
\begin{align*}
    \TA_i=\sum_{k\in[d]}(1-\mu[k]^2)(\calM(\calD_{\sim i})[k]-\mu[k])(\bZ_i[k]-\mu[k]).
\end{align*}

Due to the independence between $\calM(\calD_{\sim i})$ and $\bZ_i$, we have $\E[\TA_i]=0$.
Moreover, as $|1-\mu[k]^2|\le 1$ and $|\calM(\calD_{\sim i})-\mu[k]|\le 2$, 
we have $\E[|\TA_i|]\le 2 \sqrt{\sum_{k \in [d]} \Var(\bZ_i[k])}\le 2\sqrt{d}$.

Split $A_i$ with $A_{i,+}=\max\{0,A_i\}$ and $A_{i,-}=\min\{0,A_i\}$ and split $\TA_{i}$ similarly.
By the property of DP, we know 
\begin{align*}
    \Pr[|A_{i,+}|\ge t]\le \exp(\epsilon)\Pr[|\TA_{i,+}|\ge t]+\delta,\forall t\ge 0,\\
    \Pr[|A_{i,-}|\ge t]\ge \exp(-\epsilon)\Pr[|\TA_{i,1}|\ge t]-\delta, \forall t\ge 0.
\end{align*}
Then we have
\begin{align*}
    \E[A_i]=& \int_{0}^{\infty}\Pr[|A_{i,+}|\ge t] \d t- \int_{0}^{\infty} \Pr[|A_{i,-}|\le t]\d t\\
    \le & \exp(\epsilon) \E[|\TA_{i,+}|]-\exp(-\epsilon)\E[|\TA_{i,-}|]+2\delta T+ \int_{T}^{\infty}\Pr[|A_i|\ge t]\d t\\
    \le & \E[\TA_i]+(\exp(\epsilon)-1)\E[|\TA_{i,+}|]+ (1-\exp(-\epsilon))\E[|\TA_{i,-}|]+2\delta T+\int_{T}^{\infty}\Pr[|A_i|\ge t]\d t\\
    \le & \E[\TA_i]+2\epsilon \E[|\TA_i|]+2\delta T+\int_{T}^{\infty}\Pr[|A_i|\ge t]\d t,
\end{align*}
where the last inequality used the fact that $\exp(\epsilon)-1\le 2\epsilon$ when $\epsilon\le 1/10$.

When $\delta\le 1/dn^2$ and set $T=O(\sqrt{d\log(1/\delta)})$, we have
\begin{align*}
    \E[A_i]\le 4\epsilon \sqrt{d}+ d/8n.
\end{align*}

When $\alpha\le d/8$, Equation~\eqref{eq:large_correlation} implies that
\begin{align*}
    n(4\epsilon \sqrt{d}+ d/8n)\ge d/4,
\end{align*}
which leads to
\begin{align*}
    n\ge \frac{\sqrt{d}}{32 \epsilon}.
\end{align*}
This completes the proof.
\end{proof}

It is standard to translate the sample complexity lower bound (Lemma~\ref{lm:sample_compelexity_lb}) to the error lower bound (Lemma~\ref{lm:sign_error_lb}).
We present a proof below.


\begin{proof}[Proof of Lemma~\ref{lm:sign_error_lb}]

Let $\PAitem$ be the set of item-level DP mechanisms, and let $\PAuser$ be the set of user-level DP mechanisms.

Define the error term:
\begin{align*}
\Error[\calP,\calM,n]=\E_{\calD\sim\calP^n,\calM}\sum_{i=1}^d|\mu[i]|\ind(\sign(\mu[i])\neq\sign(\calM(\calD)[i])),
\end{align*}
where $\mu=\E_{z\sim\calP}z$.

Recall that we construct the distribution $\calP_1$ as follows: for each coordinate $k\in[d]$, we draw $\mu[k]$ uniformly random from $[-1,1]$, and $z_{i,j}[k]\sim\calN(\mu[k],m)$ i.i.d., for $i\in[n],j\in[m]$.

Let $\bar{\calP}_1$ be the following:
for each coordinate $k\in[d]$, we draw $\mu[k]$ uniformly random from $[-1,1]$, and $\bZ_{i}[k]\sim\calN(\mu[k],1)$ i.i.d., for $i\in[n]$.
$\bar{\calP}_1$ is corresponding to averaging the $m$ samples for each user.
By Lemma~\ref{lm:reduction_to_item_level}, we have
\begin{align*}
    \inf_{\calM\in\PAuser}\Error[\calP_1,\calM,nm]\ge \inf_{\calM\in\PAitem}\Error[\bar{\calP}_1,\calM,n].
\end{align*}
By Lemma~\ref{lm:sample_compelexity_lb},
\begin{align*}
\inf_{\calM\in\PAitem}\Error[\bar{\calP}_1,\calM,\sqrt{d}/32\epsilon]\ge \Omega(d).
\end{align*}

Let $n^*=\Tilde{O}(\sqrt{d}/\epsilon)$.
When we have a large data set of size $n\gg n^*$, construct $\bar{\calP}_2=\frac{n^*}{n}\bar{\calP}_1+(1-\frac{n^*}{n})\calP_3$, where $\calP_3$ is a Dirac distribution at $\mathbf{0}\in\R^d$.

Hence, by a Chernoff bound, with high probability, the number of samples drawn from $\bar{\calP}_1$ in the dataset $\calD$ is no more than $O(n^*\cdot\log(nd))=\frac{\sqrt{d}}{32\epsilon}$.
Then we have
\begin{align*}
    \inf_{\calM\in\PAuser}\Error[\calP_1,\calM,nm]\ge &\inf_{\calM\in\PAitem}\Error[\bar{\calP}_2,\calM,n]\\
    \ge &\frac{n^*}{n}\inf_{\calM\in\PAitem}\Error[\bar{\calP}_1,\calM, \sqrt{d}/32\epsilon]\ge \Tilde{\Omega}(\frac{d^{3/2}}{\epsilon n}).
\end{align*}

In the precondition of $\calP_2$, we need $\|z\|_\infty\le \Tilde{O}(\sqrt{m})$ almost surely for $z\sim\calP_2$.
But $\calP_1$ involves some Gaussian distributions.
We construct $\calP_2$ by truncating the Gaussian distributions.

More specifically, for each item $z_{i,j}$ drawn from $\calN(\mu,m I_d)$, we set $z'_{i,j}[k]=\frac{z_{i,j}[k]}{\max\{1,|z_{i,j}[k]|/G \}}$ with $G=\Theta(\sqrt{m\log(mnd)})$.
In other words, we get $z'_{i,j}$ by projecting $z_{i,j}$ to $B_\infty(0,G)$.
Fixing $\mu$, we first show  
\begin{align}
\label{eq:small_mean_shift}
    \|\E_{z\sim\calP_2}[z]-\mu\|_\infty\le O(1/dn^2).
\end{align}

It suffices to consider any coordinate $k\in[d]$, and prove
\begin{align*}
    |\E_{z\sim\calP_2}[z[k]]-\mu[k]|\le O(1/dn^2).
\end{align*}
Letting $\beta=\frac{-\mu[k]+G}{\sqrt{m}}$ and $\alpha=\frac{-\mu[k]-G}{\sqrt{m}}$, we know
\begin{align*}
   |\E_{z\sim\calP_2}[z[k]]-\mu[k]|\le \frac{n^*}{n}\cdot \sqrt{m} \cdot \frac{\phi(\alpha)-\phi(\beta)}{\int_{\alpha}^{\beta}\phi(x) \d x},
\end{align*}
where $\phi$ is density function of standard Gaussian $\calN(0,1)$.
As $\mu[k]\in[-1,1]$ and $G=\Theta(\sqrt{m\log(mnd)})$, we establish Equation~\eqref{eq:small_mean_shift}.
Denote $\mu'=\E_{z\sim\calP_2}[z\mid \mu]$, that is the mean of $\calP_2$ conditional on $\mu$.

Moreover, we have
\begin{align*}
&~\inf_{\calM\in\PAuser}\E_{\mu,\calD\sim\calP_2^{mn},\calM}\sum_{i=1}^{d}|\mu'[i]|\ind(\sign(\mu'[i])\neq \sign(\calM(\calD)))\\
\ge &  \inf_{\calM\in\PAuser}\E_{\mu,\calD\sim\calP_2^{mn},\calM}\Big(\sum_{i=1}^{d}|\mu[i]|\ind(\sign(\mu[i])\neq \sign(\calM(\calD)))-d\|\mu-\mu'\|_\infty\Big)\\
\ge & \inf_{\calM\in\PAuser}\E_{\mu,\calD\sim\calP_2^{mn},\calM}\sum_{i=1}^{d}|\mu[i]|\ind(\sign(\mu[i])\neq \sign(\calM(\calD)))- O(1/n^2)\\
\stackrel{(i)}{\ge} & \inf_{\calM\in\PAuser}\E_{\mu,\calD\sim\calP_1^{mn},\calM}\sum_{i=1}^{d}|\mu[i]|\ind(\sign(\mu[i])\neq \sign(\calM(\calD)))- O(1/n^2)\\
\ge & \inf_{\calM\in\PAuser}\Error(\calP_1,\calM,nm)-O(1/n^2)\\
\ge & \Tilde{\Omega}(\frac{d^{3/2}}{\epsilon n}),
\end{align*}
where the inequality (i) follows from that we can always sample from $\calP_2$ with samples from $\calP_1$, which means  the problem over $\calP_2$ is harder than $\calP_1$.
This completes the proof.
% This can be done by truncating the Gaussian distributions, which leads to a negligible total variation distance between $\calP_1$ and $\calP_2$.
% Similar to Proposition 5 in \cite{levy2021learning}, we can show 
% \begin{align*}
%     \Error[\calP_2,\calM,nm]\ge \Error[\calP_1,\calM,nm]-\Tilde{O}(\sqrt{m}d_{TV}(\calP_2,\calP_1)),
% \end{align*}
% where $d_{TV}(\calP_2,\calP_1)$ is the TV distance between $\calP_2$ and $\calP_1$.
% Then we complete the proof.
\end{proof}

The lower bound Theorem~\ref{thm:lb} follows from Lemma~\ref{lm:sign_error_lb} and our construction of the function class, that is
\begin{align*}
    \E[F(\calM(\calD))-F(x^*)] 
    & \ge \E_{\calD,\calM,\mu}\sum_{i=1}^{d} |\mu[i]|\cdot \ind\big(\sign(\mu[i])\neq\sign(\calM(\calD)[i])\big) \\
    & \ge \Tilde{\Omega} \left(\frac{d^{3/2}}{n\epsilon} \right)= GD \cdot \Tilde{\Omega} \left(\frac{d^{3/2}}{n\epsilon \sqrt{m}} \right),
\end{align*}
given $G=\Tilde{O}(\sqrt{m})$.