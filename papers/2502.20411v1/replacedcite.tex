\section{Related Work and Background}
\label{sec2}
This section presents an overview of relevant literature for this work, including Learning Approaches for SNNs, Forward-Forward Algorithm, and Forward-Forward Algorithm in SNNs.

\subsection{Learning Approaches for SNNs}
Since the emergence of spiking neural networks, various ways have been used to train them. These learning approaches are generally divided into three categories: STDP, ANN-SNN conversion, and Backpropagation.
STDP____ is an unsupervised learning algorithm inspired by biological neural systems, which adjusts synaptic weights based on the relative timing of presynaptic and postsynaptic spikes. The fundamental principle is that the order of these spikes determines whether the synapse is strengthened or weakened. In Hebbian STDP, a presynaptic spike occurring before a postsynaptic spike leads to synaptic strengthening, reinforcing the association between neurons. Conversely, in anti-Hebbian STDP, the same spike timing results in synaptic weakening, promoting different forms of neural plasticity____. Some variants of the STDP approach include Reward-modulated STDP____, Mirrored STDP____, and Probabilistic STDP____, each of which can be utilized for different purposes.
%STDP____ is an unsupervised learning algorithm inspired by biological neural systems. It adjusts the synaptic weight based on the relative timing of presynaptic and postsynaptic spikes. The basic idea is that if a presynaptic spike occurs before a postsynaptic spike, the synapse may be strengthened or weakened depending on the variant of STDP. Hebbian STDP strengthens synapses in this scenario, while anti-Hebbian STDP weakens them____.
ANN-to-SNN conversion approach is a widely used method for leveraging the strengths of both networks____. It involves training a standard ANN using traditional methods and then adapting it into an SNN by mapping its weights and neuron parameters to spiking equivalents. This approach combines the efficiency of ANN training with the benefits of SNN operation, allowing SNNs to achieve performance comparable to ANNs on some benchmarks.

Backpropagation in SNNs focused on understanding and modeling complex spatiotemporal relationships between spikes. This method prioritizes computational effectiveness rather than adhering to biological plausibility, making it distinct from approaches like STDP. A primary challenge with backpropagation is the nondifferentiable nature of spiking neuron equations, which prevents the direct application of standard gradient-based optimization methods. Various approximation techniques like SpikeProp____, SuperSpike____, SSTDP____, and SLAYER____ are employed to overcome this, enabling backpropagation to function effectively in the spiking domain. 

\subsection{Forward-Forward Algorithm}

The Forward-Forward Algorithm is a novel learning approach for neural networks that forgoes traditional backpropagation by using two forward passes: one with real or "positive" data and another with "negative" or contrasting data. First, the algorithm prepares data by creating positive samples, which are real labeled data, and generating negative samples. These negative samples are carefully crafted, either by corrupting positive samples or using synthetic data, to differ structurally from positive data. In supervised learning, in order to use labels in the training process, positive and negative samples can be generated from the original data in various ways. One common way is to embed the corresponding labels on each sample. When embedding, it should be noted that if we are creating positive samples, we must put the correct label on the data, and if we are looking to create negative samples, we must put the wrong label on the data. 
Fig.~\ref{fig:label} shows how to embed labels and create positive and negative samples corresponding to a sample from class one from a dataset that has 10 classes. 
It can be seen that in the positive sample corresponding to the selected sample from class one, the second pixel is lit out of the first ten pixels. Similarly, in general, in creating its corresponding negative sample, any pixel except the second pixel can be lit out of the first ten pixels, which, for example, class zero is considered as the class of the negative sample.

The FF algorithm then processes each sample in two separate forward passes through the network, each with distinct objectives.
In the first forward pass, positive data is fed through the network, and the network’s goal is to maximize a metric called “goodness” in each layer. Goodness typically represents the sum of squared neuron activations within a layer, measuring that input's “energy” or response level. During the second forward pass, the same network processes the negative samples, but this time it seeks to minimize the goodness measure, reducing neuron activations in response to negative samples. Each layer thus learns independently by adjusting its weights based on local goodness functions, making the FF algorithm highly adaptable without needing backward error propagation.

To ensure the network learns robust representations, FF applies layer normalization. This step forces layers to focus on the orientation of activations rather than just their magnitude, preventing lower layers from overpowering the classification and encouraging meaningful feature extraction across all layers. In a deep network, this allows each layer to contribute to classification rather than relying solely on early layers.
Training proceeds sequentially across layers, following a greedy layer-wise approach that simplifies learning since each layer has its objective and does not depend on global backpropagation. For different tasks, FF adapts to both unsupervised and supervised learning. Unsupervised learning uses synthetic negative samples to differentiate representations, while in supervised learning, correct labels in the input indicate positive data, and incorrect labels create negative samples. 
%After training, the classification of new data is achieved with a single forward pass, and a softmax classifier on accumulated goodness scores can be used for final predictions.
The FF algorithm has practical advantages, notably eliminating the need for backpropagation and reducing computational demands____. This makes it promising for online learning and analog hardware applications where backpropagation is challenging____. 
%Additionally, the FF algorithm resembles learning mechanisms observed in biological systems, mirroring wake-sleep cycles and offering a contrastive approach suited for neuromorphic and biologically inspired models.

\subsection{Forward-Forward Algorithm in SNNs}
Despite the widespread use of the BP method in learning spiking neural networks, problems such as weight transport, update locking, and black-box handling still persist. Using the Forward-Forward algorithm instead of backpropagation and replacing its steps in the training process of spiking neural networks can solve these problems to a good extent. Studies have also been conducted using the FF algorithm's learning idea to spiking neural networks. Ororbia____ introduces a novel neuro-mimetic architecture of SNNs designed to operate with layer-wise parallelism and synaptic plasticity. The proposed model, termed contrastive-signal-dependent plasticity (CSDP), generalizes forward-forward learning for spiking systems by iteratively processing sensory inputs within dynamic, recurrent circuits. This system uniquely avoids feedback pathways, offering biologically inspired and computationally efficient mechanisms for classification and reconstruction tasks. The architecture integrates a layer-wise parallel design incorporating bottom-up, top-down, and lateral signals for dynamic computation without forward-locking and introduces CSDP as an online, biologically plausible plasticity mechanism for adapting synapses using local spike-based activity traces. Terres et al.____ explore using the Forward-Forward algorithm as a biologically plausible alternative to backpropagation, specifically adapting it for SNNs to enhance energy efficiency and robustness. The authors present two spiking-specific goodness functions—bounded and unbounded—tailored to the discrete spiking activity of neurons, enabling the effective integration of the FF algorithm into SNNs. Also, building on these properties, they introduce the FF-SCP algorithm, a novel approach for out-of-distribution detection. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%