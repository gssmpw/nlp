%% 
%% Copyright 2007-2025 Elsevier Ltd
%% 
%% This file is part of the 'Elsarticle Bundle'.
%% ---------------------------------------------
%% 
%% It may be distributed under the conditions of the LaTeX Project Public
%% License, either version 1.3 of this license or (at your option) any
%% later version.  The latest version of this license is in
%%    http://www.latex-project.org/lppl.txt
%% and version 1.3 or later is part of all distributions of LaTeX
%% version 1999/12/01 or later.
%% 
%% The list of all files belonging to the 'Elsarticle Bundle' is
%% given in the file `manifest.txt'.
%% 
%% Template article for Elsevier's document class `elsarticle'
%% with harvard style bibliographic references

\documentclass[preprint,3p,twocolumn]{elsarticle}

%% Use the option review to obtain double line spacing
%% \documentclass[authoryear,preprint,review,12pt]{elsarticle}

%% Use the options 1p,twocolumn; 3p; 3p,twocolumn; 5p; or 5p,twocolumn
%% for a journal layout:
%% \documentclass[final,1p,times,authoryear]{elsarticle}
%% \documentclass[final,1p,times,twocolumn,authoryear]{elsarticle}
%% \documentclass[final,3p,times,authoryear]{elsarticle}
%% \documentclass[final,3p,times,twocolumn,authoryear]{elsarticle}
%% \documentclass[final,5p,times,authoryear]{elsarticle}
%% \documentclass[final,5p,times,twocolumn,authoryear]{elsarticle}

%% For including figures, graphicx.sty has been loaded in
%% elsarticle.cls. If you prefer to use the old commands
%% please give \usepackage{epsfig}

\usepackage{amsmath,amssymb,amsfonts}
\usepackage{graphicx}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{enumitem}
\usepackage{balance} 
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{lipsum}
\usepackage{booktabs}
\usepackage{subcaption}
\usepackage{mwe}
\usepackage{adjustbox}
\usepackage{breqn}
\usepackage{fancyhdr}
\usepackage{multirow}
\usepackage{caption}
\usepackage{csquotes}
\usepackage{colortbl}
\usepackage{comment}
% \usepackage{cite}

\begin{document}

\begin{frontmatter}

%% Title, authors and addresses

%% use the tnoteref command within \title for footnotes;
%% use the tnotetext command for theassociated footnote;
%% use the fnref command within \author or \affiliation for footnotes;
%% use the fntext command for theassociated footnote;
%% use the corref command within \author for corresponding author footnotes;
%% use the cortext command for theassociated footnote;
%% use the ead command for the email address,
%% and the form \ead[url] for the home page:
%% \title{Title\tnoteref{label1}}
%% \tnotetext[label1]{}
%% \author{Name\corref{cor1}\fnref{label2}}
%% \ead{email address}
%% \ead[url]{home page}
%% \fntext[label2]{}
%% \cortext[cor1]{}
%% \affiliation{organization={},
%%            addressline={}, 
%%            city={},
%%            postcode={}, 
%%            state={},
%%            country={}}
%% \fntext[label3]{}

\title{Backpropagation-free Spiking Neural Networks with the Forward-Forward Algorithm} %% Article title

%% use optional labels to link authors explicitly to addresses:
%% \author[label1,label2]{}
%% \affiliation[label1]{organization={},
%%             addressline={},
%%             city={},
%%             postcode={},
%%             state={},
%%             country={}}
%%
%% \affiliation[label2]{organization={},
%%             addressline={},
%%             city={},
%%             postcode={},
%%             state={},
%%             country={}}

\author[cssbu]{Mohammadnavid Ghader} %% Author name
\ead{m\_ghader@sbu.ac.ir}
\author[cssbu]{Saeed Reza Kheradpisheh\corref{cor1}} %% Author name
\ead{s\_kheradpisheh@sbu.ac.ir}

\author[cybsbu]{Bahar Farahani} %% Author name
\ead{b\_farahani@sbu.ac.ir}
\author[Hertfordshire]{Mahmood Fazlali} %% Author name
\ead{m.fazlali@herts.ac.uk}

%% Author affiliation
\affiliation[cssbu]{Department of Computer and Data Science, Faculty of Mathematical Sciences, Shahid Beheshti University, Tehran, Iran }
\affiliation[cybsbu]{Cyberspace Research Institute, Shahid Beheshti University, Tehran, Iran }
\affiliation[Hertfordshire]{Cybersecurity and Computing Systems Research Group, University of Hertfordshire, Hatfield, United Kingdom}
\cortext[cor1]{Corresponding authors}


%% Abstract
\begin{abstract}
%% Text of abstract
Spiking Neural Networks (SNNs) offer a biologically inspired computational paradigm that emulates neuronal activity through discrete spike-based processing. Despite their advantages, training SNNs with traditional backpropagation (BP) remains challenging due to computational inefficiencies and a lack of biological plausibility. This study explores the Forward-Forward (FF) algorithm as an alternative learning framework for SNNs. Unlike backpropagation, which relies on forward and backward passes, the FF algorithm employs two forward passes, enabling localized learning, enhanced computational efficiency, and improved compatibility with neuromorphic hardware. We introduce an FF-based SNN training framework and evaluate its performance across both non-spiking (MNIST, Fashion-MNIST, CIFAR-10) and spiking (Neuro-MNIST, SHD) datasets. Experimental results demonstrate that our model surpasses existing FF-based SNNs by over 5\% on MNIST and Fashion-MNIST while achieving accuracy comparable to state-of-the-art backpropagation-trained SNNs. On more complex tasks such as CIFAR-10 and SHD, our approach outperforms other SNN models by up to 6\% and remains competitive with leading backpropagation-trained SNNs. These findings highlight the FF algorithm’s potential to advance SNN training methodologies and neuromorphic computing by addressing key limitations of backpropagation.
\end{abstract}

%%Graphical abstract
% \begin{graphicalabstract}
% %\includegraphics{grabs}
% \end{graphicalabstract}

%%Research highlights
% \begin{highlights}
% \item Research highlight 1
% \item Research highlight 2
% \end{highlights}

%% Keywords
\begin{keyword}
Spiking Neural Network\sep Forward-Forward Algorithm\sep Forward-Only Learning\sep Backpropagation-free
%% keywords here, in the form: keyword \sep keyword

%% PACS codes here, in the form: \PACS code \sep code

%% MSC codes here, in the form: \MSC code \sep code
%% or \MSC[2008] code \sep code (2000 is the default)

\end{keyword}

\end{frontmatter}

%% Add \usepackage{lineno} before \begin{document} and uncomment 
%% following line to enable line numbers
%% \linenumbers

%% main text
%%

%% Use \section commands to start a section
\section{Introduction}
\label{sec1}
Spiking Neural Networks (SNNs), as a biologically inspired model of neural computation, are designed to process information using discrete spike events~\cite{b17}, closely mimicking the dynamics of real neurons. However, training SNNs remains effectively a significant challenge, primarily due to the credit assignment problem~\cite{b20} and the difficulty attributing individual neuron contributions to network output errors. Unlike traditional artificial neural networks, SNNs operate on non-differentiable spike dynamics, complicating the application of gradient-based training methods like backpropagation~\cite{snnbp}. To address this, surrogate gradient~\cite{neftci2019} methods are introduced, allowing approximate gradients for effective training while preserving the spike-based behavior of SNNs. The surrogate gradient method replaces the non-differentiable spike activation function with a smooth, differentiable approximation during the backward pass of training. This allows gradients to propagate through the network, enabling optimization using gradient descent.

Despite many successes in training spiking neural networks, backpropagation has faced persistent criticism for its lack of alignment with the mechanisms of the brain~\cite{bpbrain1}, raising doubts about its viability as a model for credit assignment in neural systems~\cite{bpandbrain}. 
One of the major limitations lies in its requirement to explicitly store all neural activity for later use during synaptic adjustments. This explicit storage mechanism has no direct counterpart in biological neural circuits, where memory and computation are more integrated~\cite{bpweakness}.
Another challenge is the need for error derivatives to travel along a dedicated global feedback pathway to create teaching signals. This kind of structured, system-wide error communication contrasts sharply with the distributed and localized nature of information processing in the brain~\cite{bpweakness2}.
Furthermore, backpropagation assumes that error signals can move backward through the same neural pathways that forward-propagate information, a phenomenon referred to as the weight transport problem~\cite{wtproblem}. This assumption is biologically implausible, as neural pathways in the brain are not designed to facilitate such exact bidirectional transport~\cite{wtproblem2}.
The sequential nature of backpropagation adds another layer of incompatibility. Unlike the brain’s ability to perform massively parallel computations, backpropagation enforces a process where inference and learning must occur largely step-by-step. This sequentiality significantly diverges from biological neural networks' highly dynamic and concurrent operations~\cite{bpparallel}. 
These challenges highlight why backpropagation is unlikely to serve as a biologically plausible brain learning model. However, recent advancements have been exploring alternative approaches~\cite{b21} that address these limitations and offer solutions more in line with biological principles.

The Forward-Forward algorithm~\cite{hinton2022}, by inspiration from contrastive methods like Boltzmann Machines~\cite{boltzman} and Noise Contrastive Estimation~\cite{ncl}, is a novel alternative to the traditional backpropagation method for training neural networks, which Geoffrey Hinton introduced. This algorithm replaces the forward and backward passes of backpropagation with two forward passes. Each layer of the network has its own objective function to optimize the corresponding layer's weights. FF does not require storing all intermediate activities for backpropagation, nor does it assume symmetrical weight connections. This makes it a more plausible model for cortical learning and suitable for low-power analog hardware. Also, FF can process data sequentially, enabling on-the-fly learning without halting the data pipeline for all the weights' updating. Another feature of FF is that, unlike backpropagation, it does not require direct connectivity of all consecutive layers through the forward pass, allowing for the inclusion of black-box components within the network.


Motivated by the advantages of the FF algorithm and its potential for SNN training, this work develops an FF-based SNN learning approach and investigates its performance on established datasets. We summarize our contributions as follows:

\begin{itemize}
    \item We present a novel learning approach for training spiking neural networks using gradient descent inspired by the Forward-Forward algorithm, which is effective across different types of datasets.
    
    \item We propose a structured training approach for spiking neural networks that leverages key features of the Forward-Forward algorithm, such as layer-level local learning, black-box handling, and more.

    \item For the first time, we present the results of an SNN model trained with the FF algorithm on spiking datasets such as N-MNIST and SHD, demonstrating competitive performance against BP-based models and establishing the FF approach as a viable alternative for SNN training.
   
\end{itemize}

The rest of this paper is organized as follows: Section~\ref{sec2} overviews related work and backgrounds of SNNs and FF algorithm. Section~\ref{sec3}  presents the proposed method. Section~\ref{sec4}  elaborates on the experimental setup and results. Finally, Section~\ref{sec5} concludes the paper and suggests a direction for future work. 


\begin{figure*}[th]
\centering
%\includegraphics[totalheight=5cm]{SNNFF-Figure 2.png}
%\includegraphics[width=1.0\textwidth]{SNNFF-Figure 2.png}
%\includegraphics[width=0.7\textwidth]{FF Labeling 7.png}
%\includegraphics[width=0.9\textwidth]{FIG1-NEW.png}
\includegraphics[width=0.9\textwidth]{FIG1-NEW11.png}
%\caption{Different options for overlaying a label on an image of number one from the MNIST dataset for positive and negative sample generation.}
\caption{Different options for overlaying a label on a sample of a 10-class dataset for positive and negative sample generation.}
\label{fig:label}
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Related Work and Background}
\label{sec2}
This section presents an overview of relevant literature for this work, including Learning Approaches for SNNs, Forward-Forward Algorithm, and Forward-Forward Algorithm in SNNs.

\subsection{Learning Approaches for SNNs}
Since the emergence of spiking neural networks, various ways have been used to train them. These learning approaches are generally divided into three categories: STDP, ANN-SNN conversion, and Backpropagation.
STDP~\cite{stdp} is an unsupervised learning algorithm inspired by biological neural systems, which adjusts synaptic weights based on the relative timing of presynaptic and postsynaptic spikes. The fundamental principle is that the order of these spikes determines whether the synapse is strengthened or weakened. In Hebbian STDP, a presynaptic spike occurring before a postsynaptic spike leads to synaptic strengthening, reinforcing the association between neurons. Conversely, in anti-Hebbian STDP, the same spike timing results in synaptic weakening, promoting different forms of neural plasticity~\cite{hebbian}. Some variants of the STDP approach include Reward-modulated STDP~\cite{rstdp}, Mirrored STDP~\cite{mstdp}, and Probabilistic STDP~\cite{pstdp}, each of which can be utilized for different purposes.
%STDP~\cite{stdp} is an unsupervised learning algorithm inspired by biological neural systems. It adjusts the synaptic weight based on the relative timing of presynaptic and postsynaptic spikes. The basic idea is that if a presynaptic spike occurs before a postsynaptic spike, the synapse may be strengthened or weakened depending on the variant of STDP. Hebbian STDP strengthens synapses in this scenario, while anti-Hebbian STDP weakens them~\cite{hebbian}.
ANN-to-SNN conversion approach is a widely used method for leveraging the strengths of both networks~\cite{ann2snn}. It involves training a standard ANN using traditional methods and then adapting it into an SNN by mapping its weights and neuron parameters to spiking equivalents. This approach combines the efficiency of ANN training with the benefits of SNN operation, allowing SNNs to achieve performance comparable to ANNs on some benchmarks.

Backpropagation in SNNs focused on understanding and modeling complex spatiotemporal relationships between spikes. This method prioritizes computational effectiveness rather than adhering to biological plausibility, making it distinct from approaches like STDP. A primary challenge with backpropagation is the nondifferentiable nature of spiking neuron equations, which prevents the direct application of standard gradient-based optimization methods. Various approximation techniques like SpikeProp~\cite{SpikeProp}, SuperSpike~\cite{superspike}, SSTDP~\cite{sstdp}, and SLAYER~\cite{slayer} are employed to overcome this, enabling backpropagation to function effectively in the spiking domain. 

\subsection{Forward-Forward Algorithm}

The Forward-Forward Algorithm is a novel learning approach for neural networks that forgoes traditional backpropagation by using two forward passes: one with real or "positive" data and another with "negative" or contrasting data. First, the algorithm prepares data by creating positive samples, which are real labeled data, and generating negative samples. These negative samples are carefully crafted, either by corrupting positive samples or using synthetic data, to differ structurally from positive data. In supervised learning, in order to use labels in the training process, positive and negative samples can be generated from the original data in various ways. One common way is to embed the corresponding labels on each sample. When embedding, it should be noted that if we are creating positive samples, we must put the correct label on the data, and if we are looking to create negative samples, we must put the wrong label on the data. 
Fig.~\ref{fig:label} shows how to embed labels and create positive and negative samples corresponding to a sample from class one from a dataset that has 10 classes. 
It can be seen that in the positive sample corresponding to the selected sample from class one, the second pixel is lit out of the first ten pixels. Similarly, in general, in creating its corresponding negative sample, any pixel except the second pixel can be lit out of the first ten pixels, which, for example, class zero is considered as the class of the negative sample.

The FF algorithm then processes each sample in two separate forward passes through the network, each with distinct objectives.
In the first forward pass, positive data is fed through the network, and the network’s goal is to maximize a metric called “goodness” in each layer. Goodness typically represents the sum of squared neuron activations within a layer, measuring that input's “energy” or response level. During the second forward pass, the same network processes the negative samples, but this time it seeks to minimize the goodness measure, reducing neuron activations in response to negative samples. Each layer thus learns independently by adjusting its weights based on local goodness functions, making the FF algorithm highly adaptable without needing backward error propagation.

To ensure the network learns robust representations, FF applies layer normalization. This step forces layers to focus on the orientation of activations rather than just their magnitude, preventing lower layers from overpowering the classification and encouraging meaningful feature extraction across all layers. In a deep network, this allows each layer to contribute to classification rather than relying solely on early layers.
Training proceeds sequentially across layers, following a greedy layer-wise approach that simplifies learning since each layer has its objective and does not depend on global backpropagation. For different tasks, FF adapts to both unsupervised and supervised learning. Unsupervised learning uses synthetic negative samples to differentiate representations, while in supervised learning, correct labels in the input indicate positive data, and incorrect labels create negative samples. 
%After training, the classification of new data is achieved with a single forward pass, and a softmax classifier on accumulated goodness scores can be used for final predictions.
The FF algorithm has practical advantages, notably eliminating the need for backpropagation and reducing computational demands~\cite{danilo}. This makes it promising for online learning and analog hardware applications where backpropagation is challenging~\cite{hinton2022}. 
%Additionally, the FF algorithm resembles learning mechanisms observed in biological systems, mirroring wake-sleep cycles and offering a contrastive approach suited for neuromorphic and biologically inspired models.

\subsection{Forward-Forward Algorithm in SNNs}
Despite the widespread use of the BP method in learning spiking neural networks, problems such as weight transport, update locking, and black-box handling still persist. Using the Forward-Forward algorithm instead of backpropagation and replacing its steps in the training process of spiking neural networks can solve these problems to a good extent. Studies have also been conducted using the FF algorithm's learning idea to spiking neural networks. Ororbia~\cite{csdp2024} introduces a novel neuro-mimetic architecture of SNNs designed to operate with layer-wise parallelism and synaptic plasticity. The proposed model, termed contrastive-signal-dependent plasticity (CSDP), generalizes forward-forward learning for spiking systems by iteratively processing sensory inputs within dynamic, recurrent circuits. This system uniquely avoids feedback pathways, offering biologically inspired and computationally efficient mechanisms for classification and reconstruction tasks. The architecture integrates a layer-wise parallel design incorporating bottom-up, top-down, and lateral signals for dynamic computation without forward-locking and introduces CSDP as an online, biologically plausible plasticity mechanism for adapting synapses using local spike-based activity traces. Terres et al.~\cite{snnffrobust} explore using the Forward-Forward algorithm as a biologically plausible alternative to backpropagation, specifically adapting it for SNNs to enhance energy efficiency and robustness. The authors present two spiking-specific goodness functions—bounded and unbounded—tailored to the discrete spiking activity of neurons, enabling the effective integration of the FF algorithm into SNNs. Also, building on these properties, they introduce the FF-SCP algorithm, a novel approach for out-of-distribution detection. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section {Proposed Method}
\label{sec3}
\begin{figure*}[th]
\centering
%\includegraphics[totalheight=5cm]{SNNFF-Figure 2.png}
%\includegraphics[width=1.0\textwidth]{SNNFF-Figure 2.png}
%\includegraphics[width=0.9\textwidth]{SNNFF Main Fig 14.png}
%\includegraphics[width=0.9\textwidth]{SNNFF_new2.png}
%\includegraphics[width=0.97\textwidth]{Picture1122.png}
%\includegraphics[width=0.97\textwidth]{SNN-FF-FIG1.png}
%\includegraphics[width=0.9\textwidth]{FF-FIG M3.png}
%\includegraphics[width=0.97\textwidth]{SNN-FF-FIG5.png}
\includegraphics[width=0.97\textwidth]{FF-NEW12.png}
%\caption{The architecture of the proposed spiking forward-forward algorithm for MNIST dataset.}
\caption{The architecture of the proposed spiking forward-forward algorithm. The diagram highlights the input forward path (solid arrows) and the error propagation path (dotted arrows).}
\label{fig:model}
\end{figure*}

The training procedure for SNNs using the forward-forward algorithm involves several crucial steps to ensure effective learning and optimal performance.
To create training examples, we superimpose the labels onto the input images. This process involves setting the first $n$ elements of the flattened input vector to zero which $n$ is the number of classes. Then assign the maximum pixel value of the image to the index corresponding to the true label. This labeling aids in distinguishing between different digit classes during the training. We use hard labeling to assign the labels of negative samples.  For each input image, the model computes a "goodness" score for all possible class labels. The goodness score measures the mean squared activation of the network layers in response to the input. The goodness score for the true class label of each input image is set to zero. This ensures that the true label is not considered when selecting a hard label. The remaining goodness scores (excluding the true label) are transformed using a square root function. This transformation flattens the distribution, making it less peaked and more uniform, thus avoiding the dominance of any single class label. Using the transformed goodness scores as a probability distribution, a hard label is sampled for each input image. This label is chosen from the set of non-true labels and represents a class that the network finds relatively difficult to distinguish from the true label. Hard labeling aims to create challenging negative examples for the network.

%These negative examples are generated by selecting labels (other than the true label) that the network finds difficult to differentiate. This is achieved by analyzing the goodness metric for each class and selecting a label with a high goodness value, excluding the true label. The goodness metric is defined as the mean squared activation of a layer.

%To carry out the training process and how to transfer information forward, we examined two different methods. The first method is more consistent with the steps of the Forward-Forward algorithm and the second method is based on SNN's mechanism.

%    \subsection{First Method}
%    In the first method,

%The second layer includes A normalization sub-layer, which normalizes its inputs to unit-length vectors. 
%According to Fig.~\ref{fig:model}, 


After generating positive and negative samples, it's time to feed them into the network. The network comprises an input layer and hidden layers that are fully connected. Notably, there is no output layer, but the number of hidden layers can be increased according to the problem's needs. For the non-spiking datasets, the training data vectors are directly encoded into the first layer, whereas for the spiking datasets, the data is entered with their respective coding. After a linear transformation of the input data, the desired neuron model, like leaky integrate-and-fire (LIF), is performed.
In the Forward-Forward algorithm, the primary objective is to enhance neuronal activation in response to positive data while reducing activation in response to negative data. However, if the output of the neurons is directly propagated to the subsequent hidden layer, it may hinder effective learning in deeper layers, particularly when processing positive data. This issue arises due to the influence of large values carried over from the preceding layer. To mitigate this effect and ensure that deeper hidden layers learn independently of prior layers, a normalization step is applied before activating the mechanism in the Leaky Integrate-and-Fire model.
LIF neurons accumulate input current over time and emit a spike when the membrane potential exceeds a predefined threshold. The membrane potential then resets, mimicking the functions observed in biological neurons. The neurons are initialized with parameters like leakage term and spiking threshold. The recursive representation of a LIF neuron is as follows:

\begin{equation}
\begin{aligned}
U[t+1] = \beta U[t] + WX[t+1] - R[t] ,  \\
\end{aligned}
\end{equation}
where $U$, $W$, $X$, $R$, and $\beta$ are membrane potential, weights, inputs, reset term, and decay rate, respectively. In each time step, spike emission occurs if the membrane potential exceeds the membrane potential threshold $U_{thr}$. The following equation represents this:
%In the first method, We wait until the states of all spikes are determined during all the time steps in the first layer. Then we consider the total number of spikes occurring in each neuron as the activation value of that neuron. Therefore, for each neuron in the first hidden layer, we have a specific activation value.
\begin{equation}
\begin{aligned}
S[t] = 
\begin{cases}
      1 & U_{thr}\leq U[t], \\
      0 & otherwise .  
\end{cases}
\end{aligned}
\label{spike function}
\end{equation}

Our method has two forward passes called positive and negative passes. In the positive pass, positive data are entered into the network, and for each step, the condition of the occurrence of spikes is determined. We calculate the goodness value of each hidden layer for positive data by considering the spikes' count of each neuron as the input of the goodness function. After that, the negative pass starts, and negative samples are entered into the neural network to calculate the goodness of the negative samples by going through the same steps as the positive data. During the execution of positive and negative forward passes, for each neuron, the spike count value is equal to the sum of all spikes through all time steps and calculated as follows:

\begin{equation}
    C_{pos} = \sum\limits_{t=1}^T S_{pos}[t] ,  \\
\end{equation}
\begin{equation}
    C_{neg} = \sum\limits_{t=1}^T S_{neg}[t] ,  \\
\end{equation}
where $T$ is the number of time steps. Also, $S_{pos}$ and $S_{neg}$ are spike occurrences of neurons in the face of positive and negative samples, respectively. In each layer, the goodness function $G$ for positive and negative samples is defined as follows:

\begin{equation}
    G_{pos} = \frac{1}{N}\sum\limits_{i=1}^N C_{pos, i}^2 ,  \\
\end{equation}
\begin{equation}
    G_{neg} = \frac{1}{N}\sum\limits_{i=1}^N C_{neg, i}^2 ,  \\
\end{equation}
where N is the number of neurons in the desired layer, and $C_{pos, i}$ and $C_{neg, i}$ are the spike counts of neuron $i$ for positive and negative data, respectively. Using the goodness values obtained from positive and negative samples, the loss of the layer is calculated with a formula which is inspired by the swish function~\cite{swish} and described as follows:

%where N is the number of neurons in the desired hidden layer and $s_{pos, i}$ and $s_{neg, i}$ are the spike counts of neuron $i$ for positive and negative data, respectively. Using the goodness values obtained from positive and negative data, the loss of the layer is calculated with a formula which is inspired by the swish function~\cite{swish} and described as follows:
 %   \begin{figure}[t]
 %   \centering
 %   %\includegraphics[totalheight=5cm]{SNNFF-Figure 2.png}
 %   %\includegraphics[width=1.0\textwidth]{SNNFF-Figure 2.png}
 %   %\includegraphics[width=0.9\textwidth]{SNNFF Main Fig 14.png}
 %   %\includegraphics[width=0.9\textwidth]{SNNFF_new2.png}
 %   \includegraphics[width=0.37\textwidth]{desmos-graph (10)b1.png}
 %   %\caption{The architecture of the proposed spiking forward-forward algorithm for MNIST dataset.}
 %   \caption{Plotting of the utilized loss function concerning the variable $\Delta$.}
 %   \label{fig:model}
 %   \end{figure}

\begin{equation}
\begin{aligned}
&\Delta = G_{\text{pos}} - G_{\text{neg}}  ,\\
%&Loss_{\text{Swish}} = \frac{-\alpha\Delta}{1 + e^{\alpha\Delta}}
&Loss = \frac{-\alpha\Delta}{1 + e^{\alpha\Delta}} ,
\end{aligned}
\end{equation}
where $\alpha$ is a scaling factor. Training of spiking neural networks is challenging due to the non-differentiability of the spike generation process. To overcome this, surrogate gradient learning can be utilized. Surrogate gradients provide a differentiable approximation of the spike function, allowing gradient-based optimization methods to be used. During layer-wise weight updating, these surrogate gradients approximate the true gradients of the spike function, enabling the network to update its parameters effectively. Various functions can be considered for gradient approximation, the most important of which are sigmoid, fast sigmoid, and shifted arc-tan. Refer to equation (\ref{spike function}), shifted arc-tangent surrogate function and its gradient w.r.t. the membrane potential is defined as follows:

\begin{equation}
\begin{aligned}
%S &\approx \frac{U}{1+k|U|} \\
%\frac{\partial S}{\partial U} &= \frac{1}{(1+k|U|)^2} \\
S &\approx \frac{1}{\pi}arctan(\pi U\frac{\alpha}{2}) \\
\frac{\partial S}{\partial U} &= \frac{1}{\pi}\frac{1}{(1+(\pi U\frac{\alpha}{2})^2)}
\end{aligned}
\end{equation}
where $U$ is the membrane potential and $k$ is the slope value, a hyperparameter. Obviously, other proposed approximation functions can also be used to calculate the surrogate gradient. 
After updating the weights of each layer, the next epoch of training is implemented.

Fig.~\ref{fig:model} illustrates the learning procedure of the proposed method. It is assumed that the training operation is to be performed on an original sample. By applying the label embedding operation, the sample is converted to a positive and negative sample, and before entering the fully connected neural network, each positive and negative sample must be flattened and converted to a vector. In the positive forward pass, the positive sample enters the neural network, and spikes are generated in the neurons during the time steps. In each neuron, the number of spikes is counted independently, and after that, their spike count passes to goodness function $G$ to calculate the goodness value of the positive sample. For the negative sample, the same steps are followed, and at the end, the goodness of the negative sample is calculated. After finding the goodness values for positive and negative samples, the amount of loss is calculated with the loss function $L$, and then the weights of the corresponding layer are updated locally.


\begin{table*}[h]\label{table:hyperparameters}
\caption{Datasets and corresponding training parameters.}
\label{table:hyperparams}
\begin{center}
\resizebox{0.75\textwidth}{!}{
\renewcommand{\arraystretch}{1.3}
\begin{tabular}{cccccccccc}
\hline
&\multicolumn{1}{c}{\bf MNIST} &\multicolumn{1}{c}{\bf F-MNIST} &\multicolumn{1}{c}{\bf CIFAR-10} &\multicolumn{1}{c}{\bf N-MNIST} &\multicolumn{1}{c}{\bf SHD}\\
\hline
\multicolumn{1}{c}{\multirow{1}{*}{Dataset (train/test)}} &$60\text{k}/10\text{k}$ &$60\text{k}/10\text{k}$ &$50\text{k}/10\text{k}$  &$60\text{k}/10\text{k}$&$8156/2264$ \\
\multicolumn{1}{c}{\multirow{1}{*}{Input neurons}} &$784$ &$784$ &3072 &$2312$ &$700$ \\
\multicolumn{1}{c}{\multirow{1}{*}{Dataset classes}} &$10$ &$10$ &$10$ &$10$ &$20$ \\
\multicolumn{1}{c}{\multirow{1}{*}{Epochs}} &$300$ &$300$ &$300$ &$300$ &$500$ \\
\multicolumn{1}{c}{\multirow{1}{*}{Learning rate}} &$0.001$ &$0.001$ &$0.001$ &$0.001$ &$0.001$  \\
\multicolumn{1}{c}{\multirow{1}{*}{Batch size}} &$4096$ &$4096$ &$4096$ &$4096$ &$4096$  \\
\multicolumn{1}{c}{\multirow{1}{*}{Time steps $T$}} &$10$ &$10$ &$10$ &$10$ &$10$ \\
\multicolumn{1}{c}{\multirow{1}{*}{$\alpha$ (in loss)}} &$0.6$ &$0.6$ &$0.6$ &$0.6$ &$0.6$ \\
\multicolumn{1}{c}{\multirow{1}{*}{Neurons' threshold}} &$1.0$ &$1.0$ &$1.2$ &$1.0$ &$5.0$ \\
\multicolumn{1}{c}{\multirow{1}{*}{Neurons' decay rate}} &$0.99$ &$0.99$ &$0.8$ &$0.9$ &$0.9$ \\
%\multicolumn{1}{c}{\multirow{1}{*}{Time resolution $\Delta t$} (ms)} &$1$ &$1$ &$1$ &$1$ &$1$ &$1$ &$2$ \\
%\multicolumn{1}{c}{\multirow{1}{*}{Milestones}} &$(50, 100)$ &$(15, 90, 120)$ &$(30, 60, 90)$ &$(15, 90, 120)$ &$(30, 60, 90)$ \\
\hline
\end{tabular}
}
\end{center}
\end{table*}



In the inference phase, due to the absence of an output layer, implementing conventional classification is not possible. Hence, a method should be adopted that uses layer information to classify test data. For each test sample, all possible labels in the dataset are applied to the sample once individually, and each of them passes to the trained neural network. After that, the sum of all goodness values of all layers is calculated, and the label of the sample that produces the highest sum of goodness values is considered the sample class. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table*}[tbh]
\caption{Performance of our SNN model comparison to existing SNN models (* denotes self-implementation, $^\dagger$ denotes data augmentation, and $^\beta$ denotes learnable membrane time constants).}
%\caption{Performance of our SNN model comparison to existing SNN models (* denotes self-implementation and $^\beta$ denotes trainable time constants).}
\label{table:results}
\begin{center}
\resizebox{\textwidth}{!}{
\renewcommand{\arraystretch}{1.3}
\begin{tabular}{cccccccccc}
\hline
\multicolumn{1}{c}{\bf Dataset}  &\multicolumn{1}{c}{\bf Model} &\multicolumn{1}{c}{\bf Architecture} &\multicolumn{1}{c}{\bf Neuron model} &\multicolumn{1}{c}{\bf Learning Approach} &\multicolumn{1}{c}{\bf Accuracy (\%)}  \\
% &\multicolumn{1}{c}{\bf \thead{Sequential \\complexity}}
\hline

% MNIST
\multicolumn{1}{c}{\multirow{2}{*}{MNIST}}
	& \textbf{Proposed model}  &784-500-500 &LIF$^\beta$ &FF  &$98.34$ \\
	%& Ororbia.~\cite{csdp2024}  &784-700-700 &LIF &FF  &$97.46\pm0.03$ \\
	& Ororbia.~\cite{csdp2024}  &784-700-700 &LIF &FF  &97.46 \\
	& Terres et al.~\cite{snnffrobust}  &784-1400-1400 &LIF &FF &93.27 \\
	& Comsa et al.~\cite{comsa2020}  &784-340-10 &IF (alpha-PSP) &BP &97.90 \\
	%& Taylor et al.~\cite{oxford2023} $^\dagger$  &784-1000-10 &LIF$^\beta$ &BP  &$97.91\pm0.10$ \\
	& Taylor et al.~\cite{oxford2023} $^\dagger$  &784-1000-10 &LIF$^\beta$ &BP  &97.91 \\
	%& Neftci et al.~\cite{neftci2019}$^\dagger$  &784-1000-10 &LIF$^\beta$ &BP &$97.87\pm0.09$ \\
	& Neftci et al.~\cite{neftci2019}$^\dagger$  &784-1000-10 &LIF$^\beta$ &BP &97.87 \\
	
    \hline
   
% F-MNIST
\multicolumn{1}{c}{\multirow{2}{*}{F-MNIST}}
    &\textbf{Proposed model}  &784-1000-1000 &LIF$^\beta$ &FF &89.82 \\
    %&\textbf{Proposed model}  &784-500-500 &LIF$^\beta$ &FF &89.36 \\
    & Terres et al.~\cite{snnffrobust}  &784-1400-1400 &LIF &FF &85.68 \\
    & Perez-Nieves et al.~\cite{sparse2021}  &784-200-10 &LIF &BP &82.20 \\
	& Zhang et al.~\cite{zhang2022}$^\dagger$  &784-1000-10 &IF(ReL-PSP) &BP &88.1 \\
	& Kheradpisheh et al.~\cite{kheradpisheh2020}$^\dagger$  &784-1000-10 &IF &BP &88.0 \\
	%& Taylor et al.~\cite{oxford2023}$^\dagger$  &784-1000-10 &LIF$^\beta$ &BP &$89.05\pm0.27$ \\
	& Taylor et al.~\cite{oxford2023}$^\dagger$  &784-1000-10 &LIF$^\beta$ &BP &89.05 \\
	%& Neftci et al.~\cite{neftci2019}$^\dagger$  &784-1000-10 &LIF$^\beta$ &BP &$89.93\pm0.30$ \\
	& Neftci et al.~\cite{neftci2019}$^\dagger$  &784-1000-10 &LIF$^\beta$ &BP &89.93 \\
    

    \hline

% CIFAR10
\multicolumn{1}{c}{\multirow{2}{*}{CIFAR-10}}
&\multicolumn{1}{c}{\multirow{1}{*}{\textbf{Proposed model}}}  &3072-2000-2000 &LIF &FF &$51.05$ \\

	& Neftci et al.~\cite{neftci2019}*  &3072-2000-2000-10 &LIF &BP &47.82 \\
\hline

    
% N-MNIST
\multicolumn{1}{c}{\multirow{2}{*}{N-MNIST}}
&\multicolumn{1}{c}{\multirow{1}{*}{\textbf{Proposed model}}}  &2312-500-500 &LIF$^\beta$ &FF &97.26 \\
        & Perez-Nieves et al.~\cite{sparse2021}  &2312-200-10 &LIF &BP &92.70 \\
        %& Taylor et al.~\cite{oxford2023} &2312-300-10 &LIF &BP &$95.91\pm0.1$ \\
        & Taylor et al.~\cite{oxford2023} &2312-300-10 &LIF &BP &95.91 \\
	  %& Neftci et al.~\cite{neftci2019}  &2312-300-10 &LIF$^\beta$ &BP &$97.47\pm0.25$ \\
	  & Neftci et al.~\cite{neftci2019}  &2312-300-10 &LIF$^\beta$ &BP &97.47 \\
\hline

% SHD
\multicolumn{1}{c}{\multirow{2}{*}{SHD}}
        & \textbf{Proposed model}  &700-500-500 &recurrent LIF &FF &77.87 \\
        %& Cramer et al.~\cite{cramerSHD}  &700-128-20 &LIF &BP &$48.1\pm1.6$ \\
        & Cramer et al.~\cite{cramerSHD}  &700-128-20 &LIF &BP &48.1 \\
        %& Neftci et al.~\cite{neftci2019}  &700-300-20 &LIF$^\beta$ &BP &$70.81\pm2.05$ \\
        & Neftci et al.~\cite{neftci2019}  &700-300-20 &LIF$^\beta$ &BP &70.81\\
        %& Cramer et al.~\cite{cramerSHD}  &700-128-20 &recurrent LIF &BP &$71.4 \pm 1.9$ \\
        & Cramer et al.~\cite{cramerSHD}  &700-128-20 &recurrent LIF &BP &71.40 \\
        %& perez (2021) &700-500-500 &recurrent LIF$^\beta $&$82.7\pm0.8$\\

\hline
\end{tabular}}
\end{center}
\end{table*}
    




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{table*}[tbh]
\caption{Performance of our SNN model comparison to existing ANN implementation of the Forward-Forward algorithm}
\label{table:results2}
\begin{center}
\resizebox{0.85\textwidth}{!}{
\renewcommand{\arraystretch}{1.3}
\begin{tabular}{cccccccccc}
\hline
\multicolumn{1}{c}{\bf Dataset}  &\multicolumn{1}{c}{\bf Model} &\multicolumn{1}{c}{\bf Architecture} &\multicolumn{1}{c}{\bf Network Type} &\multicolumn{1}{c}{\bf Accuracy (\%)} \\
% &\multicolumn{1}{c}{\bf \thead{Sequential \\complexity}}
\hline

% MNIST
\multicolumn{1}{c}{\multirow{2}{*}{MNIST}}
	%& \textbf{Proposed model} &784-500-500 &SNN &$98.34$ \\
	& \textbf{Proposed model} &784-2000-2000 &SNN &$98.69$ \\
        & Ororbia et al.~\cite{Ororbia2023}  &784-2000-2000 & ANN  &98.66 \\
        & Hinton~\cite{hinton2022}  &784-2000-2000-2000-2000 & ANN  &98.63 \\
	& Gandhi et al.~\cite{gandhi2023} &784-2000-2000-2000-2000 & ANN  &98.63 \\
        & Aminifar et al.~\cite{Aminifar2024}  &784-2000-2000-2000-2000 & ANN  &98.60 \\
        & Ghader et al.~\cite{ghader2024}  &784-500-500 & ANN  &98.58 \\
        & Lee et al.~\cite{symba2023}  &784-2000-2000-2000 & ANN  &98.58 \\
	& Brenig et al.~\cite{brenig2023} &784-2000-2000-2000-2000 & ANN &98.03 \\
	& Tang.~\cite{tangff2023}  &784-200-200-200-50-50 &ANN &$98.00$ \\
        & Tosato et al.~\cite{Tosato2023}  &784-784-784-784 & ANN  &94.00 \\
	
    \hline
   
% F-MNIST
\multicolumn{1}{c}{\multirow{2}{*}{F-MNIST}}
	%&\textbf{Proposed model}  &784-500-500 &SNN &89.13 \\
	&\textbf{Proposed model}  &784-2000-2000 &SNN &90.27 \\
        & Ghader et al.~\cite{ghader2024}  &784-500-500-500 & ANN  &90.22 \\
	& Ororbia et al.~\cite{Ororbia2023}  &784-2000-2000 & ANN  &89.60 \\
	& Brenig et al.~\cite{brenig2023}  &784-2000-2000-2000-2000 & ANN &87.31 \\
	& Tosato et al.~\cite{Tosato2023}  &784-784-784-784 & ANN  &82.60 \\

    \hline

% CIFAR10
\multicolumn{1}{c}{\multirow{2}{*}{CIFAR-10}}
&\multicolumn{1}{c}{\multirow{1}{*}{\textbf{Proposed model}}}  &3072-3072-3072 &SNN & 54.04 \\

    & Aminifar et al.~\cite{Aminifar2024}  &3072-2000-2000-2000-2000 & ANN  &53.95 \\
    & Ghader et al.~\cite{ghader2024}  &3072-500-500-500 & ANN  &53.31 \\
	& Brenig et al.~\cite{brenig2023}  &3072-2000-2000-2000-2000 & ANN & 47.60 \\
    %& Lee et al.~\cite{symba2023}  &3072-3072-3072-3072 & ANN  &59.09 \\
    %& Hinton.~\cite{hinton2022} &3072-3072-3072-3072 & ANN  &59.00 \\
\hline

    
\end{tabular}}
\end{center}
\end{table*}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Experiments}
\label{sec4}


\subsection{Experimental Setup}

The experiments were conducted using a PyTorch-based implementation~\cite{pytorch}, leveraging the snntorch library~\cite{jason2023} for spiking neuron models and surrogate gradients. To evaluate the proposed method, five datasets were utilized: MNIST~\cite{MNIST}, Fashion-MNIST (F-MNIST)\cite{FMNIST}, CIFAR-10~\cite{CIFAR10}, Neuromorphic-MNIST (N-MNIST)~\cite{NMNIST}, and Spiking Heidelberg Digits (SHD)~\cite{cramerSHD}.
The MNIST dataset comprises 70,000 grayscale images of hand-written digits, each sized 28×28 pixels. The dataset is divided into 60,000 samples for training and 10,000 for testing, covering digit classes from 0 to 9. Its variation, the F-MNIST dataset, retains the same number of samples and classes but replaces hand-written digits with images of fashion items such as T-shirts and shoes, providing more complex features for classification tasks.
The CIFAR-10 dataset includes 60,000 color images, each sized 32×32 pixels with three color channels. It spans 10 object classes, with 50,000 samples allocated for training and 10,000 for testing. Unlike MNIST and Fashion-MNIST, which use grayscale images, CIFAR-10 focuses on more visually intricate static 2D color images.
The N-MNIST dataset is derived from the MNIST dataset by applying a spiking conversion. This process involves recording digit samples displayed on an LCD monitor using an ATIS sensor mounted on a motorized pan-tilt unit, which introduces motion-based neuromorphic features while retaining the same classes and sample count as MNIST.
Lastly, the SHD dataset is used, which includes high-quality studio recordings of spoken digits in both German and English. The original dataset contains 10,420 samples which are divided into 8,156 training and 2,264 testing samples spanning 20 classes.

The network parameters are initialized, and the Adam optimizer is configured with an initial learning rate of 0.001. The training is conducted for 300 epochs, with adjustments to the learning rate based on epoch milestones to fine-tune the learning process. The training data is split into mini-batches of 4096 samples each. For each mini-batch, positive and negative examples are generated by superimposing true and hard labels onto the input vectors. The hyperparameter values used to train the model on each dataset are summarized in Table~\ref{table:hyperparams}.



\subsection{Experimental Results and Analysis}





Table~\ref{table:results} comprehensively compares spiking neural network models across five datasets: MNIST, FMNIST, CIFAR-10, N-MNIST, and SHD. The key columns detail the model name, architecture, neuron model type, and corresponding accuracy percentage, highlighting both our proposed model and various state-of-the-art models with fully connected architecture. 
In the MNIST dataset, our model achieves an impressive 98.69\% accuracy using a LIF$^{\beta}$\cite{PLIF} neuron model and two hidden layers of 500 neurons architecture, outperforming other models. Compared to the FF-based models, Ororbia et al. use a deeper model with two hidden layers of 700 neurons, achieving 97.46\% with LIF neurons, and Terres et al. achieve lower accuracy with 93.27\%.
Competing BP-based models from Neftci et al.~\cite{neftci2019}, Comsa et al.~\cite{comsa2020}, and Taylor et al.~\cite{oxford2023} use slightly different architectures and neuron models like IF or LIF$^{\beta}$, achieving comparable accuracies 97.87\%, 97.90\%, and 97.91\%, respectively. However, our model outperforms these models, showcasing an improvement, potentially due to the specific architecture and neuron model used.

In the F-MNIST, our model achieves an accuracy of 89.82\% with the same two hidden layers of 1000 neurons architecture and LIF$^{\beta}$ neuron model. Compared to an FF-based model, Terres et al. use two hidden layers of 1400 neurons, achieving 85.68\% that our model achieves higher accuracy despite a lighter network.
Compared to BP-based models, Zhang et al.~\cite{zhang2022} and Kheradpisheh et al.~\cite{kheradpisheh2020} report slightly lower accuracies (88.1\% and 88.0\%) using IF-type neuron models with a 1000-neuron hidden layer architecture. The other competing models, Taylor et al.~\cite{oxford2023} achieve closer performance to our model employing LIF$^{\beta}$ with 89.05\% and Neftci et al.~\cite{neftci2019}, performs slightly better with 89.93\% accuracy and our model is very close to it. 
%While our model is close in performance, the results suggest the need for further refinement to achieve state-of-the-art results.

In the CIFAR-10, our model, with a significantly larger 3072-2000-2000 architecture, achieves 51.05\% accuracy using the LIF neuron model. We implemented an experiment with a similar architecture of our model based on Neftci et al.~\cite{neftci2019}, utilizing the LIF neuron model, and we got a lower accuracy of 47.82\%. This result suggests that while our model shows progress, the CIFAR-10 dataset remains difficult for fully connected SNNs due to training on complex, high-dimension images.

In the N-MNIST dataset, our model achieves competitive results with 96.13\% accuracy using the LIF neuron model in the two hidden layers of 500 neurons architecture. 
Perez-Nieves et al.~\cite{sparse2021} use a single hidden layer of 200 neurons, achieving 92.70\% and Taylor et al.~\cite{oxford2023} achieves 95.91\% accuracy, showcasing the adaptability of our model. Neftci et al.~\cite{neftci2019}, using LIF$^{\beta}$ neurons and a similar architecture, achieves the highest accuracy at 97.47\%. These results highlight strong performance for our model, although it slightly underperforms relative to Neftci et al.’s model.

%In the SHD dataset, Our model, with a 700-500-500 recurrent LIF architecture, achieves a solid 64.31\% accuracy. Competing models such as Cramer et al.~\cite{cramerSHD} (700-128-20 recurrent LIF) and Neftci et al. (2019)~\cite{neftci2019} report varying performance, with Neftci et al.’s LIF$^{\beta}$ model achieving the highest accuracy of 70.81\%. Cramer et al.’s recurrent LIF model also performs well at 71.4\%, emphasizing the importance of recurrent architectures in processing temporal data. While our model is competitive, further improvements in recurrent processing mechanisms could enhance its performance on SHD.

In the SHD dataset, our model, with two hidden layers of 500 neurons and recurrent LIF architecture, achieves a solid 77.87\% accuracy. Cramer et al.~\cite{cramerSHD} and Neftci et al.~\cite{neftci2019} use recurrent LIF neurons with smaller architectures, each employing one hidden layer of 128 and 300 neurons, respectively, reaching 71.4\% and 70.81\%. The higher performance of our model demonstrates its strength in handling time-series data.

%Competing models such as Cramer et al.~\cite{cramer2022} (700-128-20 recurrent LIF) and Neftci et al. (2019)~\cite{neftci2019} report varying performance, with Cramer et al.’s recurrent LIF model achieving the highest accuracy of 71.40\%. The results emphasize the importance of recurrent architectures in processing temporal data. While our model is competitive, further improvements in recurrent processing mechanisms could enhance its performance on SHD.

The table highlights that our model demonstrates strong performance across multiple datasets, particularly excelling on MNIST with state-of-the-art accuracy while also performing very competitively on FMNIST and N-MNIST. However, for more complex datasets like CIFAR-10, where the accuracy remains relatively low at 51.05\%, architectural enhancements or improved learning mechanisms are needed to address challenges posed by high-dimensional inputs. On temporal datasets like SHD, our model achieves remarkable results using a recurrent LIF architecture and outperforms other BP-based models. 
%Overall, trends suggest that models incorporating advanced neuron dynamics (e.g., LIF$^{\beta}$) and deeper architectures tend to achieve higher accuracies, particularly for more challenging or neuromorphic tasks, indicating key areas for future improvements in our model.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}[t]
\centering
   \begin{subfigure}{0.45\linewidth}
   \centering
   %\includegraphics[width=\linewidth]{Loss_plot_1l.png}
   \includegraphics[width=\linewidth]{Loss3.png}
   \caption{}
   \label{fig:loss1} 
\end{subfigure}
%\hfill
\begin{subfigure}{0.45\linewidth}
   \centering
   %\includegraphics[width=\linewidth]{Loss_plot_1r.png}
   \includegraphics[width=\linewidth]{Loss3-S.png}
   \caption{}
   \label{fig:loss2}
\end{subfigure}

\caption{Loss trend during training of our proposed FF-based spiking neural network model through epochs. (a) Loss changes for non-spiking datasets such as MNIST, F-MNIST, and CIFAR-10. (b) Loss changes for spiking datasets such as N-MNIST and SHD.}
\label{fig:losses}
\end{figure*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
After comparing the performance of our model with other models based on spiking neural networks, we also conducted a comparison with some models based on the Forward-Forward algorithm that were based on artificial neural networks.
Table~\ref{table:results2} compares the performance of various FF-based models across three datasets—MNIST, F-MNIST, and CIFAR-10—using different architectures and network types. The datasets are organized into sections, and for each, the accuracy of our SNN model is compared against other referenced works that use ANN models.

In the MNIST dataset, our model, an SNN with two hidden layers containing 2000 neurons each, achieves an impressive 98.69\% accuracy, outperforming or matching most ANN implementations. Ororbia et al.~\cite{Ororbia2023} use an ANN with the same architecture and achieve 98.66\%, closely followed by Hinton's ANN~\cite{hinton2022} with 98.63\% accuracy. Gandhi et al.~\cite{gandhi2023}, Aminifar et al.~\cite{Aminifar2024}, and other models with similar architectures yield accuracies ranging from 98.63\% to 98.58\%. Tang~\cite{tangff2023} employs a more compact ANN with two hidden layers of 200 neurons followed by a layer of 50 neurons, obtaining 98.00\% accuracy. Tosato et al.~\cite{Tosato2023} use a different ANN with three hidden layers of 784 neurons each, reaching a significantly lower 94.00\%, demonstrating wider networks' importance for this task.

\begin{figure*}[!ht]
\centering
   \begin{subfigure}{\linewidth}
   \centering
   \includegraphics[width=\linewidth]{ACC_radar4}
   \caption{}
   \label{fig:dragratio} 
\end{subfigure}
\\[\baselineskip]
\begin{subfigure}[H]{\linewidth}
   \centering
   \includegraphics[width=\linewidth]{PARAM_radar4}
   \caption{}
   \label{fig:dragratio3}
\end{subfigure}
\centering
\caption{Comparison of the accuracy and parameter count between our proposed SNN model and other ANN models based on the Forward-Forward algorithm. (a) Accuracy of our proposed model compared to other FF-based models trained with ANN. (b) Parameters count of our proposed model compared to other FF-based models trained with ANN.}
\label{fig:radar}
\end{figure*}

In the Fashion-MNIST dataset, our SNN-based model with two hidden layers of 2000 neurons achieves the highest accuracy of 90.27\%. Ghader et al.~\cite{ghader2024} use an ANN with two hidden layers of 500 neurons, reaching a comparable 90.22\%. Ororbia et al. achieve 89.60\% accuracy using an ANN with the same architecture as "our model." Brenig et al. score 87.31\%, while Tosato et al.~\cite{Tosato2023} attain only 82.60\% using three hidden layers of 784 neurons each. The results demonstrate the superior performance of SNNs in handling moderately complex image data.

In the more challenging CIFAR-10 dataset, our proposed SNN model with two hidden layers of 3072 neurons achieves 54.04\% accuracy, and Ghader et al.’s model~\cite{ghader2024}, which is our previous ANN FF-based model with two hidden layers of 500 neurons achieves 53.31\%. In comparison, Brenig et al.~\cite{brenig2023} and Aminifar et al.~\cite{Aminifar2024} proposed the models with four hidden layers of 2000 neurons reach 47.60\%, 53.95\%, respectively. In general, Table~\ref{table:results2} demonstrates that our SNN model outperforms other ANN models on MNIST and F-MNIST datasets despite its use of SNNs and relatively smaller architectures than other ANN-based approaches. 
%The lower performance of SNNs on CIFAR-10 highlights the need for further advancements to handle complex visual tasks effectively.
%Lee et al. use an ANN matching our proposed model's architecture and obtain a better accuracy of 59.09\%.

%While ANNs still dominate the CIFAR-10 dataset, the results suggest the potential for SNNs to perform comparably in simpler tasks, highlighting their efficiency and promise for further research and optimization.



One of the points that is considered in training deep neural network models is analyzing the training process. Fig.~\ref{fig:losses} shows the loss changes during the training of our FF-based spiking neural network model on both non-spiking and spiking datasets. The x-axis represents the epochs, while the y-axis represents the loss value.
For the non-spiking datasets, The loss for MNIST starts at a higher value and gradually decreases, stabilizing after around 100 epochs. The Fashion-MNIST dataset follows a similar pattern but reaches a slightly lower final loss than MNIST. The CIFAR-10 dataset shows more fluctuations, indicating that the model struggles more with this dataset compared to the others.
For the spiking datasets, The loss for N-MNIST starts at a moderately low value and decreases gradually, stabilizing around -0.3. The SHD dataset exhibits a much steeper initial loss decrease and stabilizes at a lower loss value compared to N-MNIST. By evaluating the overall changes in the loss values obtained in model training, it appears that the presented model has a good learning ability and has been able to converge after performing certain epochs. There is this point that when faced with more complex datasets such as CIFAR-10, the process of large changes is not always smooth and decreasing, which is expected to be improved by using different architectures.



Considering the architectures and results obtained in Table~\ref{table:results2}, a relative comparison can be made between the number of parameters used in the models and their performance. Fig.~\ref{fig:radar} compares our SNN model and other ANN models, both based on the Forward-Forward algorithm, across the MNIST, F-MNIST, and CIFAR-10 datasets. In the MNIST dataset, our proposed SNN model, with 5.57 million parameters, ranks third in terms of the number of network parameters and has the best performance compared to other models. The ANN model that we presented earlier in the study~\cite{ghader2024} had 0.64 million parameters, which could be among the selected models if the goal is to select the best model with a smaller parameter volume. A similar situation occurs in the F-MNIST dataset, where our SNN model achieves the highest accuracy while ranking third in parameter count. Therefore, if reducing the number of parameters is a priority and a slight trade-off in accuracy is acceptable, the model used in~\cite{ghader2024} could be a viable alternative.
In the CIFAR-10 dataset, our SNN model outperforms other ANN models in accuracy, and relative to the number of parameters used in the models, the ANN model that we presented earlier in the study~\cite{ghader2024} has meaningfully good conditions, but the size of the parameters of our SNN model and other models does not differ significantly. These comparisons demonstrate that our proposed model achieves the best performance among the evaluated models while maintaining a highly competitive parameter count. Notably, this superiority is attained without incurring excessive computational costs during the network training process.
%These comparisons show that our proposed model in this study has been able to achieve the best performance compared to other models with a very reasonable parameter volume and competitiveness, and this superiority has been achieved without imposing unreasonable computational costs on the network training process.
%The x-axis represents the number of parameters (in millions), and the y-axis represents accuracy (in percentage). In the MNIST plot, our model achieves the highest accuracy with approximately 5.57M parameters. Another model that ranks next in terms of accuracy also has the same parameter amount as ours, which shows that the architecture used can be optimal. 
%In the Fashion-MNIST plot, our model achieves the highest performance with the same architecture as the MNSIT dataset, which has approximately 5.57M parameters. The model with second-ranked accuracy values is from our previous work, which used a loss function similar to this research. Despite a lighter architecture, it achieved significant accuracy, showing that the loss function used can have interesting capabilities.
%In the CIFAR-10 plot, our model achieves third-ranked accuracy with approximately 18.87M parameters. The first and second-ranked models have an additional hidden layer with approximately 28.31M parameters. Due to the complexity of the CIFAR-10 dataset, our model with an extra hidden layer is predicted to be very close to the top models.


%The spiking datasets converge faster and stabilize sooner than the non-spiking datasets. The CIFAR-10 dataset shows higher variability in its loss curve, suggesting that the model has difficulty generalizing to more complex non-spiking datasets. The SHD dataset reaches the lowest final loss, indicating better learning efficiency for this dataset.

Another point commonly considered in spiking neural networks is the number of time steps required to train the network, as it directly affects both the computational cost and the inference latency. Longer time steps often provide more opportunities for the network to accumulate information and stabilize spike-based learning, potentially leading to higher accuracy, but this comes at the expense of increased energy consumption and longer training durations. Conversely, reducing the number of time steps can improve efficiency and speed. Still, it may compromise the quality of learning if the network does not have sufficient temporal resolution to capture important features. 
Inference within SNNs involves multiple feedforward computations, with each pass corresponding to a time step. These computations rely on sparse spikes rather than continuous activations. However, when using backpropagation to train SNNs, there is a tendency for spike signals to weaken over time, similar to the vanishing gradient problem in ANNs. To avoid losing important information, SNNs need a sufficient number of time steps, which in many cases, the number of time steps required is in the triple digits~\cite{sstdp}.
Nevertheless, our model utilizes 10-time steps in the training process on all mentioned datasets, which is a relatively small number compared to other SNN models, which shows that our proposed algorithm can have more optimal performance than other previously presented algorithms.

%Therefore, finding an optimal balance between time steps, performance, and computational resource usage is a critical consideration when designing and optimizing spiking neural network models.

\section{Conclusion}
\label{sec5}
%The backpropagation algorithm has long been the prevailing method for training deep learning models. 
This paper presented a novel approach to training spiking neural networks using the Forward-Forward algorithm. Unlike traditional backpropagation, the FF algorithm offers a more biologically plausible and computationally efficient training method relative to BP by utilizing two forward passes instead of the typical forward and backward pass. This approach addresses key limitations of backpropagation, such as the need for global feedback pathways, explicit storage of all intermediate activities, and weight symmetry, all of which are biologically implausible.

%Our experimental results demonstrate the efficacy of the FF algorithm across various datasets, including spiking (Neuro-MNIST, SHD) and non-spiking datasets (MNIST, Fashion-MNIST, CIFAR-10). The FF-based SNNs achieved competitive accuracy, often outperforming traditional BP-based models on simpler datasets like MNIST and Fashion-MNIST. They also showed promising results on more complex datasets such as CIFAR-10.
Our experimental results demonstrated the effectiveness of the FF algorithm in training SNNs across both spiking (Neuro-MNIST, SHD) and non-spiking (MNIST, Fashion-MNIST, CIFAR-10) datasets. The proposed FF-based SNN model achieved competitive accuracy, often outperforming traditional backpropagation-based SNNs, particularly on MNIST and Fashion-MNIST. Additionally, on complex datasets such as CIFAR-10, our model showed promising performance despite the inherent challenges posed by high-dimensional data in fully connected architectures.

The FF algorithm's greater adaptability to neuromorphic hardware architectures further underscores its potential for real-world applications, particularly in scenarios requiring low power consumption and on-the-fly learning. Additionally, the algorithm's inherent advantages, such as local learning and the ability to handle black-box components, make it a versatile tool for future advancements in neuromorphic computing. Future work could explore further optimization of the FF algorithm, its integration with other neuromorphic hardware, and its application to more complex and diverse tasks, paving the way for more robust and scalable spiking neural network solutions.

%In conclusion, the Forward-Forward algorithm represents a significant step forward in the training of Spiking Neural Networks. Addressing the challenges of biological plausibility and computational efficiency opens new avenues for research and application in artificial intelligence and neuromorphic computing.














%% Use \subsection commands to start a subsection.
\begin{thebibliography}{00}

\bibitem{b17} B. Han and K. Roy, ‘Deep Spiking Neural Network: Energy Efficiency Through Time Based Coding’, in European Conference on Computer Vision, 2020.

%\bibitem{b18} B. A. Richards and T. P. Lillicrap, ‘Dendritic solutions to the credit assignment problem’, Current Opinion in Neurobiology, vol. 54, pp. 28–36, 2019.

%\bibitem{b19} M. Pfeiffer and T. Pfeil, ‘Deep Learning With Spiking Neurons: Opportunities and Challenges’, Frontiers in Neuroscience, vol. 12, 2018.

\bibitem{b20} J. Guerguiev, T. P. Lillicrap, and B. A. Richards, ‘Towards deep learning with segregated dendrites’, eLife, vol. 6, p. e22901, Dec. 2017.
\bibitem{snnbp} J. H. Lee, T. Delbruck, and M. Pfeiffer, ‘Training Deep Spiking Neural Networks Using Backpropagation’, Frontiers in Neuroscience, vol. 10, 2016.
\bibitem{neftci2019} E. O. Neftci, H. Mostafa, and F. Zenke, ‘Surrogate Gradient Learning in Spiking Neural Networks: Bringing the Power of Gradient-based optimization to spiking neural networks’, IEEE Signal Processing Magazine, vol. 36, pp. 51–63, 2019.
\bibitem{bpbrain1} Y. Song, T. Lukasiewicz, Z. Xu, and R. Bogacz, ‘Can the brain do backpropagation?---exact implementation of backpropagation in predictive coding networks’, Advances in neural information processing systems, vol. 33, pp. 22566–22579, 2020.
\bibitem{bpandbrain} T. P. Lillicrap, A. Santoro, L. Marris, C. J. Akerman, and G. Hinton, ‘Backpropagation and the brain’, Nat Rev Neurosci, vol. 21, no. 6, pp. 335–346, Apr. 2020.
\bibitem{bpweakness} Y. Bengio, D.-H. Lee, J. Bornschein, and Z. Lin, ‘Towards Biologically Plausible Deep Learning’, ArXiv, vol. abs/1502.04156, 2015.
\bibitem{bpweakness2} M. Konishi, K. M. Igarashi, and K. Miura, ‘Biologically plausible local synaptic learning rules robustly implement deep supervised learning’, Frontiers in Neuroscience, vol. 17, 2023.
\bibitem{wtproblem} C. Frenkel, M. Lefebvre, and D. Bol, ‘Learning Without Feedback: Fixed Random Learning Signals Allow for Feedforward Training of Deep Neural Networks’, Frontiers in Neuroscience, vol. 15, 2021.
\bibitem{wtproblem2} Q. Liao, J. Z. Leibo, and T. A. Poggio, ‘How Important Is Weight Symmetry in Backpropagation?’, ArXiv, vol. abs/1510.05067, 2015.
\bibitem{bpparallel} F. Tang, J. Zhang, C. Zhang, and L. Liu, ‘Brain-Inspired Architecture for Spiking Neural Networks’, Biomimetics, vol. 9, no. 10, 2024.
\bibitem{b21} G. Dellaferrera and G. Kreiman, ‘Error-driven input modulation: Solving the credit assignment problem without a backward pass’, in International Conference on Machine Learning, 2022.
\bibitem{hinton2022} G. E. Hinton, ‘The Forward-Forward Algorithm: Some Preliminary Investigations’, ArXiv, vol. abs/2212.13345, 2022.
\bibitem{boltzman} G. E. Hinton and T. J. Sejnowski, ‘Learning and Relearning in Boltzmann Machines’, in Parallel Distributed Processing: Explorations in the Microstructure of Cognition, Volume 1: Foundations, D. E. Rumelhart and J. L. Mcclelland, Eds. Cambridge, MA: MIT Press, 1986, pp. 282–317.
\bibitem{ncl} M. Gutmann and A. Hyvärinen, ‘Noise-contrastive estimation: A new estimation principle for unnormalized statistical models’, in Proceedings of the thirteenth international conference on artificial intelligence and statistics, 2010.
\bibitem{stdp} G. Q. Bi and M. M. Poo, ‘Synaptic modifications in cultured hippocampal neurons: dependence on spike timing, synaptic strength, and postsynaptic cell type’, J Neurosci, vol. 18, no. 24, pp. 10464–10472, Dec. 1998.

\bibitem{hebbian} S. Song, K. D. Miller, and L. F. Abbott, ‘Competitive Hebbian learning through spike-timing-dependent synaptic plasticity’, Nat Neurosci, vol. 3, no. 9, pp. 919–926, Sep. 2000.

\bibitem{rstdp} A. Juarez-Lora, V. H. Ponce-Ponce, H. Sossa, and E. Rubio-Espino, ‘R-STDP Spiking Neural Network Architecture for Motion Control on a Changing Friction Joint Robotic Arm’, Front Neurorobot, vol. 16, p. 904017, May 2022.
\bibitem{mstdp} K. S. Burbank, ‘Mirrored STDP Implements Autoencoder Learning in a Network of Spiking Neurons’, PLoS Comput Biol, vol. 11, no. 12, p. e1004566, Dec. 2015.
\bibitem{pstdp} T. Masquelier and S. J. Thorpe, ‘Unsupervised learning of visual features through spike timing dependent plasticity’, PLoS Comput. Biol., vol. 3, no. 2, p. e31, Feb. 2007.
\bibitem{ann2snn} W. Fang, Z. Yu, Y. Chen, T. Huang, T. Masquelier, and Y. Tian, ‘Deep Residual Learning in Spiking Neural Networks’, in Advances in Neural Information Processing Systems, 2021, vol. 34, pp. 21056–21069.
\bibitem{SpikeProp} S. M. Bohté, J. N. Kok, and H. L. Poutré, ‘SpikeProp: backpropagation for networks of spiking neurons’, in The European Symposium on Artificial Neural Networks, 2000.
\bibitem{superspike} F. Zenke and S. Ganguli, ‘SuperSpike: Supervised Learning in Multilayer Spiking Neural Networks’, Neural Comput, vol. 30, no. 6, pp. 1514–1541, Apr. 2018.

%\bibitem{b6} H.-C. Lee and J. Song, ‘SymBa: Symmetric Backpropagation-Free Contrastive Learning with Forward-Forward Algorithm for Optimizing Convergence’, 2023. 
%\bibitem{b8} N. Tosato, L. Basile, E. Ballarin, G. de Alteriis, A. Cazzaniga, and A. Ansuini, ‘Emergent representations in networks trained with the Forward-Forward algorithm’, ArXiv, vol. abs/2305.18353, 2023.
%\bibitem{b9} S. Gandhi, R. Gala, J. Kornberg, and A. Sridhar, ‘Extending the Forward Forward Algorithm’, ArXiv, vol. abs/2307.04205, 2023.
%\bibitem{b11} D. P. Pau and F. M. Aymone, ‘Suitability of Forward-Forward and PEPITA Learning to MLCommons-Tiny benchmarks’, in 2023 IEEE International Conference on Omni-layer Intelligent Systems (COINS), 2023, pp. 1–6.
%\bibitem{b7} S. Park, D. Shin, J. Chung, and N. Lee, ‘FedFwd: Federated Learning without Backpropagation’, arXiv [cs.LG]. 2023.


\bibitem{sstdp} F. Liu, W. Zhao, Y. Chen, Z. Wang, T. Yang, and L. Jiang, ‘SSTDP: Supervised Spike Timing Dependent Plasticity for Efficient Spiking Neural Network Training’, Frontiers in Neuroscience, vol. 15, 2021.

\bibitem{slayer} S. B. Shrestha and G. Orchard, ‘Slayer: Spike layer error reassignment in time’, Advances in neural information processing systems, vol. 31, 2018.

\bibitem{danilo} D. P. Pau and F. M. Aymone, ‘Suitability of Forward-Forward and PEPITA Learning to MLCommons-Tiny benchmarks’, in 2023 IEEE International Conference on Omni-layer Intelligent Systems (COINS), 2023.

\bibitem{csdp2024} A. G. Ororbia, ‘Contrastive signal-dependent plasticity: Self-supervised learning in spiking neural circuits’, Science Advances, vol. 10, no. 43, p. eadn6076, Oct. 2024.
\bibitem{snnffrobust} E. B. Terres-Escudero, J. D. Ser, A. Martínez-Seras, and P. G. Bringas, ‘On the Robustness of Fully-Spiking Neural Networks in Open-World Scenarios using Forward-Only Learning Algorithms’, ArXiv, vol. abs/2407.14097, 2024.



\bibitem{swish} P. Ramachandran, B. Zoph, and Q. Le, ‘Swish: a Self-Gated Activation Function’, 10 2017.

\bibitem{pytorch} A. Paszke et al., ‘PyTorch: An Imperative Style, High-Performance Deep Learning Library’, in Advances in Neural Information Processing Systems, 2019, vol. 32.

\bibitem{jason2023} J. K. Eshraghian et al., ‘Training Spiking Neural Networks Using Lessons From Deep Learning’, Proceedings of the IEEE, vol. 111, no. 9, pp. 1016–1054, 2023.

\bibitem{MNIST} Y. LeCun and C. Cortes, ‘The mnist database of handwritten digits’, 2005.

\bibitem{FMNIST} H. Xiao, K. Rasul, and R. Vollgraf, ‘Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms’, ArXiv, vol. abs/1708.07747, 2017.

\bibitem{CIFAR10} A. Krizhevsky, V. Nair, and G. Hinton, ‘Cifar-10 (canadian institute for advanced research)’, URL http://www. cs. toronto. edu/kriz/cifar. html, vol. 5, no. 4, p. 1, 2010.

\bibitem{NMNIST} G. Orchard, A. Jayawant, G. K. Cohen, and N. Thakor, ‘Converting Static Image Datasets to Spiking Neuromorphic Datasets Using Saccades’, Frontiers in Neuroscience, vol. 9, 2015.

%\bibitem{b12} D. Chen, P. Peng, T. Huang, and Y. Tian, ‘Deep reinforcement learning with spiking q-learning’, arXiv preprint arXiv:2201. 09754, 2022.

%\bibitem{b14} S. Sheik et al., ‘Synaptic sampling in hardware spiking neural networks’, in 2016 IEEE International Symposium on Circuits and Systems (ISCAS), 2016, pp. 2090–2093.

%\bibitem{b15} M. Ghader, S. Reza Kheradpisheh, B. Farahani, and M. Fazlali, ‘Enabling Privacy-Preserving Edge AI: Federated Learning Enhanced with Forward-Forward Algorithm’, in 2024 IEEE International Conference on Omni-layer Intelligent Systems (COINS), 2024.


%\bibitem{b22} D. O. Hebb, The Organization of Behavior: A Neuropsychological Theory. New York, NY, USA: Science Editions, 1949.

%\bibitem{b23} D. E. Rumelhart, G. E. Hinton, and R. J. Williams, ‘Learning representations by back-propagating errors’, Nature, vol. 323, no. 6088, pp. 533–536, Oct. 1986.

%\bibitem{b24} A. Nøkland, ‘Direct feedback alignment provides learning in deep neural networks’, Advances in neural information processing systems, vol. 29, 2016.

%\bibitem{b25} M. W. Spratling, ‘A review of predictive coding algorithms’, Brain and cognition, vol. 112, pp. 92–97, 2017.

%\bibitem{b26} E. Imani, W. Hu, and M. White, ‘Representation alignment in neural networks’, arXiv preprint arXiv:2112. 07806, 2021.

%\bibitem{b27} D.-H. Lee, S. Zhang, A. Fischer, and Y. Bengio, ‘Difference target propagation’, in Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2015, Porto, Portugal, September 7-11, 2015, Proceedings, Part I 15, 2015, pp. 498–515.

\bibitem{cramerSHD} B. Cramer, Y. Stradmann, J. Schemmel, and F. Zenke, ‘The Heidelberg Spiking Data Sets for the Systematic Evaluation of Spiking Neural Networks’, IEEE Transactions on Neural Networks and Learning Systems, vol. 33, pp. 2744–2757, 2020.

\bibitem{PLIF} W. Fang, Z. Yu, Y. Chen, T. Masquelier, T. Huang, and Y. Tian, ‘Incorporating Learnable Membrane Time Constant to Enhance Learning of Spiking Neural Networks’, 2021 IEEE/CVF International Conference on Computer Vision (ICCV), pp. 2641–2651, 2020.

\bibitem{comsa2020} 
I. M. Comsa, K. Potempa, L. Versari, T. Fischbacher, A. Gesmundo, and J. Alakuijala, ‘Temporal Coding in Spiking Neural Networks with Alpha Synaptic Function’, in ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2020.

\bibitem{oxford2023} L. Taylor, A. King, and N. S. Harper, ‘Robust and accelerated single-spike spiking neural network training with applicability to challenging temporal tasks’, 2023.

\bibitem{kheradpisheh2020} S. R. Kheradpisheh and T. Masquelier, ‘Temporal backpropagation for spiking neural networks with one spike per neuron’, Int. J. Neural Syst., vol. 30, no. 6, p. 2050027, Jun. 2020.

\bibitem{sparse2021} N. Perez-Nieves and D. Goodman, ‘Sparse spiking gradient descent’, Advances in Neural Information Processing Systems, vol. 34, pp. 11795–11808, 2021.

\bibitem{zhang2022} M. Zhang et al., ‘Rectified linear postsynaptic potential function for backpropagation in deep spiking neural networks’, IEEE Trans. Neural Netw. Learn. Syst., vol. 33, no. 5, pp. 1947–1958, May 2022.

\bibitem{tangff2023} D. Tang, ‘The Integrated Forward-Forward Algorithm: Integrating Forward-Forward and Shallow Backpropagation With Local Losses’, ArXiv, vol. abs/2305.12960, 2023.

\bibitem{brenig2023} J. Brenig and R. Timofte, ‘A Study of Forward-Forward Algorithm for Self-Supervised Learning’, ArXiv, vol. abs/2309.11955, 2023.

\bibitem{gandhi2023} S. Gandhi, R. Gala, J. Kornberg, and A. Sridhar, ‘Extending the Forward Forward Algorithm’, ArXiv, vol. abs/2307.04205, 2023.

\bibitem{Ororbia2023} A. Ororbia and A. A. Mali, ‘The Predictive Forward-Forward Algorithm’, ArXiv, vol. abs/2301.01452, 2023.

\bibitem{Tosato2023} N. Tosato, L. Basile, E. Ballarin, G. de Alteriis, A. Cazzaniga, and A. Ansuini, ‘Emergent representations in networks trained with the Forward-Forward algorithm’, ArXiv, vol. abs/2305.18353, 2023.

\bibitem{symba2023} H.-C. Lee and J. Song, ‘SymBa: Symmetric Backpropagation-Free Contrastive Learning with Forward-Forward Algorithm for Optimizing Convergence’, ArXiv, vol. abs/2303.08418, 2023.

\bibitem{Aminifar2024}  A. Aminifar, B. Huang, A. Abtahi, and A. Aminifar, ‘Lightweight Inference for Forward-Forward Algorithm’, in European Conference on Artificial Intelligence, 2024.

\bibitem{ghader2024} M. Ghader, S. Reza Kheradpisheh, B. Farahani, and M. Fazlali, ‘Enabling Privacy-Preserving Edge AI: Federated Learning Enhanced with Forward-Forward Algorithm’, in 2024 IEEE International Conference on Omni-layer Intelligent Systems (COINS), 2024.

\end{thebibliography}
\end{document}

\endinput
%%
%% End of file `elsarticle-template-harv.tex'.


