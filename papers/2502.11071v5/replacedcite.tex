\section{Related work\label{Section related work}
}

The Gibbs algorithm traces its origin to the work of \cite%
{boltzmann1877beziehung} and ____ on statistical
mechanics, and its relevance to machine learning was recognized by \cite%
{levin1990statistical} and ____. \cite%
{mcallester1999pac} realized that the minimizers of the PAC-Bayesian bound
are Gibbs distributions. The fact that they are limiting distributions of
stochastic gradient Langevin dynamics (____), raises the
question about the generalization properties of individual hypotheses as
addressed in this paper. Average generalization of the Gibbs posterior was
further studied notably by ____ and \cite%
{aminian2023information}, where there are also investigations into the
limiting behavior as $\beta \rightarrow \infty $.

Theorem \textbf{\ref{Theorem Main}} is part of the circle of information
theoretic ideas in machine learning, ranging from the PAC-Bayesian theorem (%
____, ____, ____, 
____) to generalization bounds in terms of mutual
information (____ and ____). It
is inspired by and indebted to the disintegrated PAC-Bayesian bounds as in 
____, ____ and \cite%
{viallard2024general}.

The benefit of wide minima was noted by ____, where
also a variant of the Gibbs algorithm was discussed. The idea was promoted
by ____ and others ____, \cite%
{iyer2023wide}. It was soon objected by ____ that there are
narrow reparametrizations of wide minima which compute the same function.
Several authors then searched for reparametrization-invariant measures of
"width" (____, ____).
Nevertheless it was early conjectured (____), that
the relevant property is average width, which is also the position of the
paper at hand.