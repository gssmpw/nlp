\section{Results}
\label{sec:results}
\Cref{fig:teaser,fig:qualitative,fig:qualitative_artists,fig:qualitative3} present qualitative results of our method across a diverse range of sketches. These include various object categories, both abstract and detailed scenes, different styles, and sketches from our new dataset featuring stroke variations and challenging abstractions. Our method effectively handles object categories beyond those used in our fine-tuning from the SketchyScene dataset, such as toys, furniture, and food items.
Our approach successfully addresses challenging scenarios, such as detailed scenes with numerous objects and occluded objects, as seen in \Cref{fig:teaser} and the first row of \Cref{fig:qualitative}. More results are provided in the supplementary material.
% Additionally, it demonstrates robustness to occluded objects, as illustrated in \Cref{fig:qualitative_artists}, and performs effectively on uncommon categories, such as exotic plants and abstract shapes, shown in \Cref{fig:qualitative3}. These results highlight the versatility and adaptability of our method to diverse sketching styles and complex object arrangements.


\begin{figure}
    \centering
    \setlength{\tabcolsep}{2pt}
    % \addtolength{\belowcaptionskip}{-7.5pt}
    {\small
    \begin{tabular}{c c c}
        \frame{\includegraphics[width=0.3\linewidth]{figs/ours/a.png}} &
        \frame{\includegraphics[width=0.3\linewidth]{figs/ours/market_kid_balloon.png}} &
        
        \frame{\includegraphics[width=0.3\linewidth]{figs/ours/bunny_blackboard.png}} \\
        \frame{\includegraphics[width=0.3\linewidth]{figs/ours/c.png}} &
        \frame{\includegraphics[width=0.3\linewidth]{figs/ours/b.png}} &
        
        \frame{\includegraphics[width=0.3\linewidth]{figs/ours/1_pour_drink.png}} \\
        \frame{\includegraphics[width=0.3\linewidth]{figs/ours/SketchAgent/1071.png}} &
        \frame{\includegraphics[width=0.3\linewidth]{figs/ours/SketchyScene/L0_sample1083.png}} &
        \frame{\includegraphics[width=0.3\linewidth]{figs/ours/CLIPasso_3/0249.png}} \\
    \end{tabular}
    }
    % \vspace{-0.2cm}
    \caption{Sketch segmentation results obtained by our method for a diverse set of sketch styles and levels of complexity.}
    % \vspace{-0.2cm}
    \label{fig:qualitative}
\end{figure}

% \input{tables/object_detection_metrics}
\input{tables/object_detection_met2_arxiv}
\begin{table}
\small
\setlength{\tabcolsep}{3pt}
% \addtolength{\belowcaptionskip}{-5pt}
\centering
\caption{Quantitative comparisons from image segmentation. We report Accuracy and IoU metrics across seven datasets, along with the mean and standard deviation for each method. Our method consistently outperforms baselines across all datasets.}
\label{tb:segmentation1}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{l | c c c | c c c}
    \toprule
    \multirow{2}{*}{\diagbox{Dataset}{Metric}} &
    \multicolumn{3}{c|}{Acc $\uparrow$} & \multicolumn{3}{c}{IoU $\uparrow$} \\
    & SketchyS & G-SAM & \textbf{Ours} & SketchyS & G-SAM & \textbf{Ours} \\
    \midrule
    SketchyScene & $0.79$ & $0.53$ & $\mathbf{0.92}$ & $0.72$ & $0.26$ & $\mathbf{0.88}$ \\
    SketchAgent & $0.39$ & $0.35$ & $\mathbf{0.88}$ & $0.38$ & $0.16$ & $\mathbf{0.84}$ \\
    C-Base & $0.70$ & $0.54$ & $\mathbf{0.91}$ & $0.64$ & $0.32$ & $\mathbf{0.88}$ \\
    C-Calligraphic  & $0.66$ & $0.50$ & $\mathbf{0.87}$ & $0.63$ & $0.30$ & $\mathbf{0.86}$ \\
    C-Charcoal  & $0.59$ & $0.47$ & $\mathbf{0.85}$ & $0.43$ & $0.26$ & $\mathbf{0.84}$ \\
    C-BrushPen & $0.66$ & $0.51$ & $\mathbf{0.89}$ & $0.54$ & $0.29$ & $\mathbf{0.85}$ \\
    InstantStyle & $0.43$ & $0.65$ & $\mathbf{0.70}$ & $0.32$ & $0.44$ & $ \mathbf{0.78}$ \\
    \midrule
    All & $0.60$ & $0.50$ & $\mathbf{0.86}$ & $0.52$ & $0.29$ & $\mathbf{0.78}$ \\
    & \footnotesize{$\pm 0.14 $} & \footnotesize{$\pm 0.09$} & \footnotesize{$\pm 0.07$} & \footnotesize{$\pm 0.14$} & \footnotesize{$\pm 0.08$} & \footnotesize{$\pm 0.03$}  \\
    \bottomrule
\end{tabular}
}
\end{table}

\begin{table}[h]
\small
% \setlength{\tabcolsep}{2pt}
% \addtolength{\belowcaptionskip}{-5pt}
\centering
\caption{Quantitative comparisons from image segmentation on the filtered datasets. We report Accuracy and IoU metrics across seven datasets, along with the mean and standard deviation for each method. OpenVocab performs semantic segmentation, and requires an input text prompt. We provide ground truth class labels as input prompts to generate segmentations. } 
\label{tb:bbox}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{l | c c | c c}
    \toprule
    \multirow{2}{*}{\diagbox{Dataset}{Metric}} &
    \multicolumn{2}{c|}{Acc $\uparrow$} & \multicolumn{2}{c}{IoU $\uparrow$} \\
    & OpenVocab & \textbf{Ours} & OpenVocab & \textbf{Ours} \\
    \midrule
    SketchyScene&  $0.25$ & $\mathbf{0.93}$ & $0.16$ & $\mathbf{0.90}$ \\
    SketchAgent &  $0.23$ & $\mathbf{0.93}$ & $0.16$ & $\mathbf{0.88}$ \\
    C-Base  &  $0.29$ & $\mathbf{0.94}$ & $0.19$ & $\mathbf{0.92}$ \\
    C-Calligraphic &  $0.32$ & $\mathbf{0.88}$ & $0.22$ & $\mathbf{0.88}$ \\
    C-Charcoal  &  $0.55$ & $\mathbf{0.85}$ & $0.43$ & $\mathbf{0.85}$ \\
    C-BrushPen &  $0.26$ & $\mathbf{0.91}$ & $0.12$ & $\mathbf{0.88}$ \\
    InstantStyle &  $0.26$ & $\mathbf{0.79}$ & $0.16$ & $\mathbf{0.73}$ \\
     \midrule
    All & $0.30$ & $\mathbf{0.89}$ & $0.20$ & $\mathbf{0.86}$ \\
    & \footnotesize{$\pm 0.11 $} & \footnotesize{$\pm 0.05$} & \footnotesize{$\pm 0.10$} & \footnotesize{$\pm 0.06$} \\
    \bottomrule
\end{tabular}}
\end{table}

\subsection{Comparisons}
We evaluate our method alongside existing scene sketch segmentation approaches, including SketchyScene \cite{Zou18SketchyScene} and the method proposed by Bourouis \etal \shortcite{bourouis2024open}. Additionally, we include Grounding DINO \cite{liu2023grounding} as a baseline, applying it directly to sketches, using Recognize-Anything Model (RAM) \cite{zhang2023recognize} for automatic labeling.
Our evaluation dataset consists of 7746 samples: 1,113 test samples from the SketchyScene dataset, 1,113 samples from each of the CLIPasso brush styles: Calligraphic Pen, Charcoal, and Brush Pen, 1,113 samples from the SketchAgent style, and 1068 samples from the InstantStyle sketch dataset, spanning 99 object categories.
Each method was applied to these datasets following their recommended best practices. Note that the SketchyScene mask generation implementation relies on legacy dependencies that are no longer executable. Therefore, we used SAM for mask generation based on their detected bounding boxes, which yielded better performance than reported in the original paper. \Cref{fig:comparison} illustrates selected segmentation results for all instance segmentation methods, with additional results provided in the supplementary material. Since the method by Bourouis \etal \shortcite{bourouis2024open} is designed for semantic segmentation rather than instance segmentation, it was evaluated separately on a filtered subset of our dataset, where each scene contained only one instance per class. 

\begin{figure}
    \centering
    \setlength{\tabcolsep}{2pt}
    {\small
    \begin{tabular}{c c c c c}
        Input & SketchyScene & Grounded SAM & Ours \\
        
         \frame{\includegraphics[width=0.23\linewidth]{figs/complete_scene_comparisons/L0_sample376_input.png}} &
        \frame{\includegraphics[width=0.23\linewidth]{figs/complete_scene_comparisons/L0_sample376_ss.png}} &
        \frame{\includegraphics[width=0.23\linewidth]{figs/complete_scene_comparisons/L0_sample376_gs.png}} &
        \frame{\includegraphics[width=0.23\linewidth]{figs/complete_scene_comparisons/L0_sample376.png}} \\
        
        % \frame{\includegraphics[width=0.23\linewidth]{figs/complete_scene_comparisons/0002_base.png}} &
        % \frame{\includegraphics[width=0.23\linewidth]{figs/complete_scene_comparisons/0002_SketchyScene.png}} &
        % \frame{\includegraphics[width=0.23\linewidth]{figs/complete_scene_comparisons/0002_grounded_SAM.png}} &
        % \frame{\includegraphics[width=0.23\linewidth]{figs/complete_scene_comparisons/0002_ours.png}} \\

        
        \frame{\includegraphics[width=0.23\linewidth]{figs/complete_scene_comparisons/0017_04_input.png}} &
        \frame{\includegraphics[width=0.23\linewidth]{figs/complete_scene_comparisons/0017_04_ss.png}} &
        \frame{\includegraphics[width=0.23\linewidth]{figs/complete_scene_comparisons/0017_04_gs.png}} &
        \frame{\includegraphics[width=0.23\linewidth]{figs/complete_scene_comparisons/0017_04.png}} \\

        \frame{\includegraphics[width=0.23\linewidth]{figs/complete_scene_comparisons/0083_11_input.png}} &
        \frame{\includegraphics[width=0.23\linewidth]{figs/complete_scene_comparisons/0083_11_ss.png}} &
        \frame{\includegraphics[width=0.23\linewidth]{figs/complete_scene_comparisons/0083_11_gs.png}} &
        \frame{\includegraphics[width=0.23\linewidth]{figs/complete_scene_comparisons/0083_11.png}} \\

        
        \frame{\includegraphics[width=0.23\linewidth]{figs/complete_scene_comparisons/0919_input.png}} &
        \frame{\includegraphics[width=0.23\linewidth]{figs/complete_scene_comparisons/0919_ss.png}} &
        \frame{\includegraphics[width=0.23\linewidth]{figs/complete_scene_comparisons/0919_gs.png}} &
        \frame{\includegraphics[width=0.23\linewidth]{figs/complete_scene_comparisons/0919.png}} \\

        \frame{\includegraphics[width=0.23\linewidth]{figs_supp/InstantStyle/50960.png}} &
        \frame{\includegraphics[width=0.23\linewidth]{figs_supp/InstantStyle/50960_SketchyScene.png}} &
        \frame{\includegraphics[width=0.23\linewidth]{figs_supp/InstantStyle/50960_grounded_SAM.png}} &
        \frame{\includegraphics[width=0.23\linewidth]{figs_supp/InstantStyle/50960_ours.png}} \\
    \end{tabular}
    }
    \vspace{-0.3cm}
    \caption{Qualitative comparison of instance segmentation methods. Each row corresponds to a different sketch style, with the first row illustrating a sample from SketchyScene and the remaining rows showcasing samples from our dataset. Black pixels indicate regions where segmentation was not applied. Our method effectively segments sketch pixels into distinct instances without introducing artifacts, outperforming alternative approaches.}
    \label{fig:comparison}
\end{figure}


\paragraph{Object Detection Evaluation}
For object detection, we report Intersection over Union (IoU), which measures the overlap between predicted and ground-truth bounding boxes, Average Recall (AR), which evaluates the ability to detect all relevant objects, and Average Precision (AP), which combines precision and recall across various IoU thresholds. Our goal is to assess the ability to precisely detect any object in the sketch, regardless of its class. To achieve this, we calculate the mean of these metrics across object instances rather than across classes. 
% This approach allows for flexibility in class label predictions, meaning that incorrect class labels are not penalized, and it accommodates methods that do not produce class labels.
The results for instance segmentation methods are summarized in \cref{tb:bbox_instance}. 
% The datasets are listed in the leftmost column, and each set of three columns corresponds to a specific metric (shown at the top) for each method. \maneesh{Don't need to describe things that visually obvious in the table or things taht will go in the table caption. Here I would just focus on saying the amin takeaways -- we perform better than the others. Even better if we can point to sharp increases in performance and explain why we see those increases.}
The SketchyScene method performs well on the SketchyScene data, while its performance significantly declines across all metrics when applied to other datasets, particularly for the challenging styles of the SketchAgent samples. 
In contrast, our method demonstrates consistent performance across all datasets, achieving an average AR score of 0.8, as shown in the last row of the table. Notably, our method outperforms SketchyScene even on its native dataset, demonstrating the effectiveness of leveraging priors from pretrained models on natural images and adapting them for sketch segmentation.
The scores obtained for our baseline method, Grounding DINO, support our claim that this model struggles to generalize to the domain of sketches without adaptation, despite its strong performance on natural images. This is evident from the large margin in scores between our method and Grounding DINO, seeing an increase of 44\% in IoU, 51\% increase in AR, and 51\% increase in AP.  Furthermore, while we fine-tuned Grounding DINO exclusively on the SketchyScene dataset, the results in the table confirm that this approach surprisingly generalizes well to very different types of sketches and object categories.

\paragraph{Segmentation Evaluation}
We evaluate the final segmentation results using two common metrics: Pixel Accuracy (Acc), which measures the ratio of correctly labeled pixels to the total pixel count in a sketch, and  Intersection over Union (IoU), which evaluates the overlap between the predicted and ground-truth segmentation masks. The results for instance segmentation methods are presented in \cref{tb:segmentation1}, while the results for Bourouis \etal \shortcite{bourouis2024open} are shown separately in \cref{tb:bbox} since it performs semantic segmentation and requires dataset filtering. 
As shown, our method outperforms alternative approaches across both metrics, with a particularly notable advantage over Grounded SAM. Additionally, our method demonstrates robustness to various styles, especially excelling on the challenging SketchAgent style compared to other methods.


% We also evaluate Precision, defined as the ratio of correctly detected objects to the total number of detections, and Recall, the ratio of correctly detected objects to the total number of actual objects. Additionally, we report AP at specific IoU thresholds, such as AP50 and AP75, which provide insights into the model's performance at different levels of localization accuracy. 
% \yael{as can be seen .... describe once we have}







% \begin{figure}
%     \centering
%     \setlength{\tabcolsep}{0pt}
%     {\small
%     \begin{tabular}{c@{\hspace{1pt}}|@{\hspace{1pt}}c c c c}
%         Input & LDP & OpenVocab & Ours \\
%         \includegraphics[width=0.2\linewidth]{figs/placeholder2.png} &
%         \includegraphics[width=0.2\linewidth]{figs/placeholder2.png} &
%         \includegraphics[width=0.2\linewidth]{figs/placeholder2.png} &
%         \includegraphics[width=0.2\linewidth]{figs/placeholder2.png} 
%         \\
%         \includegraphics[width=0.2\linewidth]{figs/placeholder2.png} &
%         \includegraphics[width=0.2\linewidth]{figs/placeholder2.png} &
%         \includegraphics[width=0.2\linewidth]{figs/placeholder2.png} &
%         \includegraphics[width=0.2\linewidth]{figs/placeholder2.png}  \\
%         \includegraphics[width=0.2\linewidth]{figs/placeholder2.png} &
%         \includegraphics[width=0.2\linewidth]{figs/placeholder2.png} &
%         \includegraphics[width=0.2\linewidth]{figs/placeholder2.png} &
%         \includegraphics[width=0.2\linewidth]{figs/placeholder2.png}  \\
%         \includegraphics[width=0.2\linewidth]{figs/placeholder2.png} &
%         \includegraphics[width=0.2\linewidth]{figs/placeholder2.png} &
%         \includegraphics[width=0.2\linewidth]{figs/placeholder2.png} &
%         \includegraphics[width=0.2\linewidth]{figs/placeholder2.png}  \\
%     \end{tabular}
%     }
%     \caption{Qualitative results - comparison (Partial Scenes)}
%     \label{fig:comparison_partial}
% \end{figure}