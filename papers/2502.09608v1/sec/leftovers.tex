% on sketches from the SketchyScene dataset.

% utilize the SketchyScene dataset, which contains 30K segmented scene sketches, and use it to finetune Grounding DINO.
% Note that Grounding DINO is designed to detect objects based on a given input text prompt describing the image. 

% For our goal of localizing arbitrary object instances in sketches, we randomly select 5,000 scene sketches from the SketchyScene~\cite{Zou18SketchyScene} dataset, which provides ground truth instance segmentation data, and compute the corresponding bounding boxes. We assign each object bounding box the same class label, ``object,'' enabling the model to focus on identifying individual objects independent of their semantic categories.

% We fine-tune a sketch-aware Grounding DINO model initialized with a pretrained Swin Transformer backbone. The backbone is fine-tuned with a reduced learning rate to preserve its general feature extraction capabilities while adapting to the sketch domain. Since the dataset contains only a single class and does not require linguistic context, the language model component is kept frozen. The detection head, responsible for classification and bounding box regression, is fully trained using Focal Loss, L1 Loss, and GIoU Loss. \mia{Write out the losses formulations?} 
% At inference time, we prompt the model with the input image and the word ``object'' to detect all potential object instances. The predicted bounding boxes are filtered based on a confidence threshold, but no additional non-maximum suppression (NMS) is applied. We forgo NMS because we observe that it often eliminates bounding boxes corresponding to small, intricate sketch objects, which are ought to be included in the final sketch parsed layers.


\subsection{Bounding Box and Masks Refinement \yael{on hold. not clear if this is needed}}
\begin{itemize}
    \item scene hierarchy and correlation to remove redundant bounding boxes 
    \item sorting
    \item inpainting
    \item ambiguity
\end{itemize}

Based on this set of bounding boxes, we apply SAM on the sketch image to produce the initial object masks $M_{init} = \{M_i\}_{i=1}^N$.
While the output of our finetuned model provide superior results to those of the original model, there are still many redundancies in the provided bounding boxes and overlapping regions in the masks.
We therefore turn into a filtering and refinement stage.

We model the relationships between the bounding boxes with a directed graph:
\begin{equation}
    G=(V, E), \quad V = \{i\}_{i=1}^N, \quad E=\{(i,j) | B_j \subset B_i\},
\end{equation}
where the nodes $V$ are the set of bounding boxes' indices, and the edges $E$ formulate the spatial relationship between the bounding boxes, such that there is a directed edge $(i,j)$ (from node $i$ to $j$) if $B_j$ is within $B_i$.
Then we perform a bottom-up traversal in which we remove redundant nodes from the graph. 
\yael{I feel like this algorithm is a heuristic to cover the class-agnostic approach.. before I write this I want to make sure class-agnostic is actually needed here}



\paragraph{Mask sorting}
Since some bounding boxes may partially overlap, the resulting masks can also overlap. To address this, we post-process the masks into disjoint regions, ensuring each pixel is uniquely assigned to a single object instance.
We sort the segments by their corresponding depth. We compute the depth map of the input sketch with DepthAnything, and sample the sketch at equally spaced points to compute the average depth score of each masked object in the scene.

% \subsection{Mask Generation and Completion}
% We use Segment Anything~\citep{kirillov2023segany} to generate initial segmentation masks, using the detected object bounding boxes as prompts. Since some bounding boxes may partially overlap, the resulting masks can also overlap. To address this, we post-process the masks into disjoint regions, ensuring each pixel is uniquely assigned to a single object instance.
\textbf{\textit{Watershed-based expansion. }}In the first stage, we use a watershed-based approach to expand masks and improve coverage of unlabeled regions, particularly in large, continuous black areas of the sketches. The sketch is first binarized, marking black pixels as potential object boundaries. Initial masks are used as markers, while unlabeled regions are preprocessed using morphological operations, such as binary closing, to improve continuity. A distance transform is applied to enhance separation between regions, giving higher weights to large areas. Finally, the watershed algorithm propagates labels from the markers into the unlabeled areas, effectively capturing intricate details and resolving gaps in the masks.


% \subsection{Mask Refinement}
% To refine the initial segmentation masks generated by Segment Anything, we employ a two-stage post-processing pipeline: watershed-based expansion and bounding box refinement. 


% \textbf{\textit{Bounding box refinement. }}In the second stage, we refine the segmentation masks by leveraging bounding box constraints to resolve ambiguities and improve mask coverage. We identify any unlabeled black pixels within the sketch that are not covered by the any masks. For each such pixel, we determine which bounding boxes contain it and assign the pixel to the mask associated with the most relevant box. If multiple masks overlap with the pixel, we assign it to the nearest mask based on the Euclidean distance to existing labeled pixels, ensuring consistency with the mask structure.



% followed by a refinement step using a
% fine-tuned Stable Diffusion model. This refinement process takes a
% color-coded sketch (where colors indicate different instances) and
% focuses on improving segmentation quality, especially in areas where
% bounding boxes overlap.

% We frame the segmentation refinement as a coloring task: the input
% consists of a coloring suggestion for sketch lines and a mask
% indicating the region to be fixed (focused on overlapping objects),
% and the output is the corrected coloring, which is used to attribute
% ambiguous pixels to their respective groups.

% To solve this task, we leverage
% MagicFixup~\cite{alzayer2024magicfixup}, a pre-trained image editing
% diffusion model designed to transform coarse user edits into
% photorealistic results. MagicFixup proves highly robust to sketches,
% likely due to its training on video sequences, which include objects
% and scenes under varying lighting and perspective conditions.

\maneesh{The pipeline figure should label each of the pieces we are using from previous work. -- SAM, Grounding DINO and Magic Fixup.}


\section{related work}

% \textbf{\textit{Layered representations of scenes.}} Our goal of providing layered representations for
% sketch editing is inspired by real-world creative
% workflows and prior research on decomposing images into semantically
% meaningful components. Layered representations have been applied in
% diverse domains, including painterly
% rendering~\cite{painterlyrendering}, image
% vectorization~\cite{layerwisevectorization}, dynamic motion
% visualizations~\cite{movingimages}, and VR
% painting~\cite{yu2024_3dlayer}. We adapt these principles to the
% domain of sketches, enabling intuitive tools for creative workflows.
% By
% leveraging robust instance segmentation outputs, our method aims to
% generate artistically meaningful layers that enhance sketch editing
% and animation.


% \clearpage


% vector sketches~\cite{wang2024contextseg, sketchGNN2021, ENDE-GNN2022, sketchSegNet2019, segFormer2023, dataDrivenSeg2014, oneshot2021, Zhu2018PartLevelSS} or raster sketches~\cite{fastsketchseg2019, freehandseg2012, SPFusionNet2019, wang2020multicolumnseg}.
% can be broadly categorized by input type: vector sketch strokes~\cite{wang2024contextseg, sketchGNN2021, ENDE-GNN2022, sketchSegNet2019, segFormer2023, dataDrivenSeg2014, oneshot2021, Zhu2018PartLevelSS} or raster sketch pixels~\cite{fastsketchseg2019, freehandseg2012, SPFusionNet2019, wang2020multicolumnseg}.

% LocalWords:  SPG CreativeSketch SketchSeg RNN GNN CRF Zou et al pre
% LocalWords:  SketchyScene Kutuk ClassAgnosticVS li vectorize Chuan
% LocalWords:  universalperceptualgrouping vectorization Alla VR prev
% LocalWords:  Scheffer promptable vectorizing

% Efforts in this
% area include both vector~\cite{zhang2023strokeSeg,
%   Kutuk2024ClassAgnosticVS} and raster scene
% sketches~\cite{GeLocalDetailPerception, bourouis2024open,
%   Zou18SketchyScene, yang2023sceneHierTransformer}.  
% \yael{Mia, please verify that the following claim is valid w.r.t the works we cite in the paragraph above:}
% However, these works focus on semantic segmentation, which does not differentiate between instances of the same class (e.g. trees in a forest scene are grouped into a single ``tree'' segment rather than each tree being labeled as an individual instance) â€” a crucial capability for sketch editing.
% \yael{I think the following is incorrect?}\mia{done}
% \yael{where is Judy's work? we need to group the most relevant works here and elaborate on their advantages and disadvantages} \mia{done}
% Four works are
% most relevant to us for showcasing capabilities in instance
% segmentation and open-vocabulary segmentation. 


% \yael{we need to describe a vector scene segmentaiton work, then write this: and then sketchyscene. Mia please fix the wrong naming above and then I'll fix the order} \mia{done, fixed naming, Kutuk's work is vector scene segmentation}
% Vector-based approaches, while powerful
% for certain applications, are inadequate for our task. During the
% exploratory phase of drawing, artists naturally work in pixel space,
% making quick strokes and iterative adjustments. While one might
% consider vectorizing these raster sketches as a pre-processing step,
% this introduces new challenges: vectorization algorithms often merge
% overlapping strokes into single paths, particularly at object
% intersections. These merged strokes can span multiple objects, making
% it impossible to determine accurate object boundaries - precisely the
% regions where we need the most clarity for segmentation.
% \maneesh{Might want to cite vectorization work from Chuan and perhaps also from Alla Scheffer}

% SketchyScene \shortcite{Zou18SketchyScene} stands out as one of the most representative resources for real-world sketch complexity. It provides meaningful layouts of object interactions by referencing existing images, with reasonable occlusions between objects that make it valuable for studying complex scene sketches. 
% However, its main limitation lies in the lack of diversity in artistic styles among the object components, which are predominantly clipart-like depictions.





% In contrast, other scene sketch datasets are less complex. FSCOCO \shortcite{fscoco} introduces symbolic, novice-style scene sketches but lacks instance segmentation ground truth data, limiting its utility for instance-level tasks. Similarly, SketchyCOCO \shortcite{gao2020sketchycocoimagegenerationfreehand}, derived from the MS COCO Stuff dataset \cite{Caesar2016COCOStuffTA}, contains sparse layouts focused primarily on nature scenes, with object components drawn in a simple, freehand style. 

ketchSeger \cite{yang2023sceneHierTransformer} proposes a hierarchical Transformer-based model for semantic segmentation, with a decoder that fuses multi-scale feature maps. These fused maps are subsequently mapped to a fixed number of semantic categories via a channel-wise transformation, inherently restricting the approach to the predefined set of classes used during training.