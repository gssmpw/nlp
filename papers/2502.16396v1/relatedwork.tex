\section{Related Works}
\label{sec:related_work}
The literature on FL robustness against poisoning attacks can be categorized into two main streams: server-side and client-side defense \cite{pmlr-v202-zhu23j, NEURIPS2021_692baebe, pmlr-v139-xie21a, NIPS2017_f4b9ec30, 291249}. These two can be used simultaneously, as they secure different ends of a FL network. Server-side defense, which is the topic of interest in this work, is often carried out using anomaly detection or tailored aggregation rules that make the central aggregator robust against poisoning attacks. Here, we refer to such methods as robust aggregation. Moreover, a number of studies show that partial robustness against poisoning attacks can be obtained using regularization techniques and Differential Privacy (DP) \cite{sun2019really, naseri2022local}.
%those that assess the security of FL by testing its robustness and identifying potential vulnerabilities, and those that propose methods to fortify FL against adversarial threats. 
In the following overview, we will concisely go through both domains and underscore their relevance to our research.


% \subsection{Poisoning Attacks}
% Poisoning attacks generally fall under model poisoning and data poisoning categories \cite{10420449, 9945997}. In data poisoning, the attacker trains a local model using corrupted data samples to satisfy a malicious objective. A common objective in data poisoning is increasing the overall classification error (i.e., untargeted setting) or deteriorating the classification performance on specific classes (i.e., targeted setting). Data poisoning can be accomplished by corrupting the samples or their labels. When corrupting the label, these attacks are referred to as label flipping (i.e., also known as label poisoning). Data poisoning can be also in the form of backdoor attacks, where specific trigger patterns are injected into the local data \cite{9802938}. This targeted attack ideally does not affect the classification performance and only misclassifies samples that contain the specified trigger.


% In model poisoning, the attacker controls the global model by manipulating the objective function. If the local model is trained using a malicious objective on valid data, the obtained parameters can poison the global model \cite{9464278, 10420449}. As an example, the malicious objective can include an additional term to the global objective function that penalizes sensitivity to the corrupted data \cite{pmlr-v97-bhagoji19a}. The same strategy can be employed to decrease the global classification performance in an untargeted manner \cite{9210138}. Another study in \cite{9293091} uses pointwise Gaussian noise to poison gradients. Although model poisoning can be performed by directly manipulating the gradients, data poisoning is also used as a tool to perform model poisoning \cite{DBLP:journals/corr/abs-2202-02817}. This further signifies the importance of mitigating data poisoning attacks in FL.

% The aforementioned poisoning attack can be orchestrated using multiple malicious clients. This setting is often referred to as a Byzantine attack, where multiple nodes collaboratively launch an attack in a FL network. When a single client creates several fake nodes to satisfy the same objective, a Sybil attack takes place \cite{259745}.



\paragraph{Robust Aggregation}
Several robust aggregation techniques have been introduced to tackle data poisoning in FL, while aiming at maintaining the FL performance. A group of these algorithms detects suspicious updates and reduces their contribution weight to the aggregation process on the server. For instance, the aggregator in \cite{pmlr-v80-yin18a} calculates the median or coordinate trimmed mean of local updates prior to generating the global update. Another group of algorithms finds clusters of clients and sets malicious clients apart from benign users so that the suspicious users do not participate in the aggregation process. As an example, FoolsGold \cite{259745} combats Sybil attacks by adjusting the learning rates of local models based on a contribution similarity. This method effectively identifies Sybil groups, when they are present. Nonetheless, it is prone to mistakenly flag benign participants and deteriorate the training performance. While this method relies on the similarity of malicious users, other approaches such as \cite{NIPS2017_f4b9ec30, pmlr-v80-mhamdi18a} take the correlation of benign users into account. In addition, statistical methods such as taking the median of updates have been shown effective in enhancing attack robustness \cite{9721118}. Robust aggregation with adaptive clipping (\verb+AdaClip+) is performed by zeroing out extremely large values for robustness to data corruption on clients, and adaptively clipping in the L2 norm to the moderately high norm for robustness to outliers \cite{NEURIPS2021_91cff01a}.

\paragraph{Differential Privacy}
While DP has been primarily considered as a defense against inference attacks, several studies show that it can also be effective in making the FL model more robust against poisoning attacks \cite{10.5555/3489212.3489304, sun2019really, naseri2022local, NEURIPS2020_fc4ddc15, ijcai2019p657}. An adaptive version of DP for FL is presented in \cite{NEURIPS2021_91cff01a} (\verb+AdaDP+) that clips the norm at a specified quantile of the update norm distribution similar to \verb+AdaClip+. The value at the quantile is calculated in real-time with DP.


% \subsection{Anomaly Detection}
% Anomaly detection techniques are actively employed to identify and prevent malicious updates from compromising systems. In FL, these methods can be employed on the server side to detect potential threats \cite{Hallaji2023}. FL-based anomaly detectors often utilize a central test data to evaluate updates and filter out those that are detrimental or neutral to the global model.

% For instance, a detection method described in \cite{10.1145/2991079.2991125} groups users into benign and dangerous categories by clustering clients based on the information received to identify malicious updates. Another technique uses a distance metric for several participants to track updates' drift \cite{NIPS2017_f4b9ec30}. To find anomalous updates from participants, a different method creates low-dimensional model weight surrogates \cite{DBLP:journals/corr/abs-1910-09933}. Among other updates, updates that support the objective function are chosen using an outlier detection strategy. Examples of anomaly detection in FL include several methods outlined in recent studies \cite{247652, li2020learning}.



% Secure aggregator \cite{10.1145/3133956.3133982} zeroes out extremely large values for robustness to data corruption on clients, clips to moderately high norm for robustness to outliers. After weighting in mean, the weighted values are summed using cryptographic protocol ensuring that the server cannot see individual updates until sufficient number of updates have been added together.



% \paragraph{Regularization}



%[13â€“16], thus the attack effects on training performance can be diminished. Meanwhile, some heuristic-based aggregation rules [20, 21, 3, 22, 23] have been proposed to cluster participating clients into a benign group and a malicious group, and then perform aggregation on the benign group only. 

%In addition, [7, 24] show that applying differential privacy to the aggregated global model can improve the robustness against model poisoning attacks. All these defensive methods are deployed at the server side and their goals are to mitigate model poisoning attacks during aggregation. Unfortunately, often in extreme cases (e.g. attackers occupy a large proportion of total clients), existing robust aggregation methods fail to prevent the aggregation from being polluted by the malicious local updates showing that it is not sufficient to offer defense via aggregation solely. Thus, there is an urgent necessity to design a novel local training method in FL to enhance its robustness against model poisoning attacks at the client side, which is complementary to existing robust aggregation approaches.