\documentclass{article}

% \usepackage{microtype}
% \usepackage{subfigure}

% \usepackage{hyperref}       % hyperlinks
% \usepackage{url}            % simple URL typesetting
% \usepackage{booktabs}       % professional-quality tables
% \usepackage{amsfonts}       % blackboard math symbols
% \usepackage{amsmath}
% \usepackage{amssymb}
% \usepackage{xcolor}         % colors
% \usepackage[ruled, lined, longend, linesnumbered]{algorithm2e}
% \usepackage{graphicx}
% \usepackage{multirow}
% \usepackage{flushend}
% \usepackage{textcomp}

\usepackage{microtype}
\usepackage{graphicx}
%\usepackage{subfigure}
\usepackage{subcaption}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
\usepackage[accepted]{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
%\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}



% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{FedNIA: Noise-Induced Activation Analysis for Mitigating Data Poisoning in FL}

\begin{document}

\twocolumn[
\icmltitle{FedNIA: Noise-Induced Activation Analysis for Mitigating Data Poisoning in Federated Learning}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Ehsan Hallaji}{a}
\icmlauthor{Roozbeh Razavi-Far}{b}
\icmlauthor{Mehrdad Saif}{a}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{a}{Department of Electrical and Computer Engineering, University of Windsor, Windsor ON, Canada}
\icmlaffiliation{b}{Faculty of Computer Science, University of New Brunswick, Fredericton NB, Canada}

\icmlcorrespondingauthor{Ehsan Hallaji}{hallaji@uwindsor.ca}
%\icmlcorrespondingauthor{Firstname2 Lastname2}{first2.last2@www.uk}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Federated learning, Data poisoning, Robust aggregation, Security, Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
%\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.


\begin{abstract}
%Federated learning systems are vulnerable to data poisoning attacks, where malicious clients manipulate the global model by training local models on tampered data. This paper presents a novel evaluation mechanism designed to identify and exclude such adversaries prior to aggregation. Our approach is distinct from existing defense mechanisms as it addresses a wider array of data poisoning attacks, including sample poisoning, label flipping, and backdoor attacks. The proposed method involves the injection of random noise inputs into the reconstruction of client models based on the received updates, followed by an analysis of layer activations for participating clients in the current round. An autoencoder network is trained on the activation outputs of the global model using the generated random noise, and subsequently used to identify abnormal activation patterns in the reconstructed clients' models when subjected to the noise pattern. This strategy eliminates the requirement for a central test dataset for client evaluation, a common yet often impractical assumption. Furthermore, our mechanism strengthens federated learning against Byzantine nodes, even when the ratio of malicious to benign clients is high. We demonstrate the effectiveness of our approach using non-iid federated data, underscoring its applicability in real-world federated learning scenarios.
Federated learning systems are increasingly threatened by data poisoning attacks, where malicious clients compromise global models by contributing tampered updates. Existing defenses often rely on impractical assumptions, such as access to a central test dataset, or fail to generalize across diverse attack types, particularly those involving multiple malicious clients working collaboratively. To address this, we propose Federated Noise-Induced Activation Analysis (\texttt{FedNIA}), a novel defense framework to identify and exclude adversarial clients without relying on any central test dataset. \texttt{FedNIA} injects random noise inputs to analyze the layerwise activation patterns in client models leveraging an autoencoder that detects abnormal behaviors indicative of data poisoning. \texttt{FedNIA} can defend against diverse attack types, including sample poisoning, label flipping, and backdoors, even in scenarios with multiple attacking nodes. Experimental results on non-iid federated datasets demonstrate its effectiveness and robustness, underscoring its potential as a foundational approach for enhancing the security of federated learning systems.
\end{abstract}



\section{Introduction}
\label{sec:intro}
Federated Learning (FL) redefines conventional centralized data processing by enabling training across decentralized devices \cite{pmlr-v54-mcmahan17a}. This collaborative methodology aggregates model parameters derived from local training at the network's edge, emphasizing the advantages of local computation. By doing so, FL ensures user privacy by restricting the server's access to local data. The decentralized nature of FL yields significant benefits, including diminished data transfer requirements, enhanced privacy preservation, and the capability to harness diverse datasets from various clients. Despite these advantages, FL is not immune to security threats. Particularly vulnerable to poisoning attacks, FL faces challenges in maintaining the integrity of the collaborative learning process \cite{NEURIPS2020_b8ffa41d, pmlr-v97-bhagoji19a, 247652}. In light of the pressing importance of this threat, this paper focuses on fortifying FL against the subtle threat of data poisoning attacks, thereby ensuring the reliability and robustness of the decentralized learning framework.

Data poisoning attacks in FL primarily aim to manipulate the sample distribution and label distribution in both targeted and untargeted manners. These attacks occur on the malicious client side, where adversaries alter the training data before contributing updates to the global model. If undetected or not properly addressed, a malicious update has the potential to contaminate the global model, leading to the spread of the attack into other clients' models.

In untargeted schemes, the goal is to generally degrade the overall performance of the global model \cite{9833647, NEURIPS2022_e2ef0cae}. This type of attack introduces changes to samples or their labels and compromises the model's accuracy across various classes, making it less reliable in its predictions. In targeted data poisoning attacks, the focus is on specific classes or injecting triggers to produce forged predictions upon triggering the system \cite{pmlr-v108-bagdasaryan20a, NEURIPS2023_6ad5d39b, Cao_2022_CVPR}. The aim here is to manipulate the model's behavior strategically, introducing biases or vulnerabilities that can be exploited to compromise the integrity of predictions related to particular classes or conditions. Addressing and thwarting these data poisoning techniques is of crucial importance to maintain the trustworthiness and effectiveness of FL in decentralized environments.

Addressing data poisoning challenges in FL involves exploring various solutions, mainly categorized into anomaly detection, adversarial training, robust aggregation, and the utilization of regularization techniques. Anomaly detection methods have been proven effective against untargeted attacks but may fall short in eliminating targeted ones, often requiring a test dataset on the server for comparison, which may not always be available \cite{li2019abnormal, li2020learning, 247652, 291249, cao2022fltrust}. Adversarial training assumes knowledge of the attack distribution and may struggle when confronted with new attack patterns. Regularization techniques, such as pruning and dropout, provide partial robustness but do not offer comprehensive protection against attacks. Robust aggregation, while enhancing model resilience, often results in lower performance compared to federated averaging (\verb+FedAvg+) in the absence of attacks. Moreover, it is important to note that the majority of the available security solutions are typically effective against specific groups of attack. For instance, several research efforts are dedicated to addressing backdoors in FL \cite{pmlr-v139-xie21a, NEURIPS2023_2376f25e, 280048}. A framework that offers robust protection against all types of data poisoning attacks and can handle several attacking nodes remains rare. Available security solutions, however, are not limited to the mentioned groups, as the landscape of FL security is continually evolving, and novel approaches may emerge to counter emerging threats. The key lies in understanding the strengths and limitations of each approach and striving for a balanced and comprehensive defense strategy.

\paragraph{Contributions} In pursuit of a comprehensive approach to mitigate data poisoning in FL, a novel client update verification mechanism, called \verb+FedNIA+, is proposed that can filter out potentially malicious updates from the aggregation process. This mechanism, combined with any aggregators such as \verb+FedAvg+ enables robust aggregation. This approach eliminates the need for having a private test dataset on the server and analyzes changes in the activation outputs of network layers. Unlike other robust aggregators, the proposed method maintains a performance level similar to the baseline when no attack is present, striking a balance between resilience and performance in the absence of threats. Moreover, this mechanism is robust against various mechanisms of sample poisoning, label flipping, and backdoor attacks, covering the primary categories of data poisoning attacks. Notably, our approach is able to mitigate collaborative poisoning attacks even with high ratios of malicious to benign clients.


%The remainder of this paper is organized as follows. Section \ref{sec:related_work} reviews the related works. Section \ref{sec:threat_model} formulates the threat models used in this work. Section \ref{sec:methodology} presents the proposed methodology. Experimental results are reported and analyzed in Section \ref{sec:results}. Finally, the paper is concluded in Section \ref{sec:conclusion}.

    % \item Thorough Evaluation: We systematically evaluate our proposed model against a wide range of attacks under different settings. This includes variations in the number of clients, severity levels of attacks, and considerations for the heterogeneity of clients' data. Our experiments demonstrate the effectiveness of our approach across diverse scenarios.

    % \item Practical Applicability: We showcase that the time complexity of our designed model is comparable to computationally efficient rivals that do not address as wide a spectrum of attacks as considered in this work. This emphasizes the practicality of our approach in real-world FL settings.


\section{Related Works}
\label{sec:related_work}
The literature on FL robustness against poisoning attacks can be categorized into two main streams: server-side and client-side defense \cite{pmlr-v202-zhu23j, NEURIPS2021_692baebe, pmlr-v139-xie21a, NIPS2017_f4b9ec30, 291249}. These two can be used simultaneously, as they secure different ends of a FL network. Server-side defense, which is the topic of interest in this work, is often carried out using anomaly detection or tailored aggregation rules that make the central aggregator robust against poisoning attacks. Here, we refer to such methods as robust aggregation. Moreover, a number of studies show that partial robustness against poisoning attacks can be obtained using regularization techniques and Differential Privacy (DP) \cite{sun2019really, naseri2022local}.
%those that assess the security of FL by testing its robustness and identifying potential vulnerabilities, and those that propose methods to fortify FL against adversarial threats. 
In the following overview, we will concisely go through both domains and underscore their relevance to our research.


% \subsection{Poisoning Attacks}
% Poisoning attacks generally fall under model poisoning and data poisoning categories \cite{10420449, 9945997}. In data poisoning, the attacker trains a local model using corrupted data samples to satisfy a malicious objective. A common objective in data poisoning is increasing the overall classification error (i.e., untargeted setting) or deteriorating the classification performance on specific classes (i.e., targeted setting). Data poisoning can be accomplished by corrupting the samples or their labels. When corrupting the label, these attacks are referred to as label flipping (i.e., also known as label poisoning). Data poisoning can be also in the form of backdoor attacks, where specific trigger patterns are injected into the local data \cite{9802938}. This targeted attack ideally does not affect the classification performance and only misclassifies samples that contain the specified trigger.


% In model poisoning, the attacker controls the global model by manipulating the objective function. If the local model is trained using a malicious objective on valid data, the obtained parameters can poison the global model \cite{9464278, 10420449}. As an example, the malicious objective can include an additional term to the global objective function that penalizes sensitivity to the corrupted data \cite{pmlr-v97-bhagoji19a}. The same strategy can be employed to decrease the global classification performance in an untargeted manner \cite{9210138}. Another study in \cite{9293091} uses pointwise Gaussian noise to poison gradients. Although model poisoning can be performed by directly manipulating the gradients, data poisoning is also used as a tool to perform model poisoning \cite{DBLP:journals/corr/abs-2202-02817}. This further signifies the importance of mitigating data poisoning attacks in FL.

% The aforementioned poisoning attack can be orchestrated using multiple malicious clients. This setting is often referred to as a Byzantine attack, where multiple nodes collaboratively launch an attack in a FL network. When a single client creates several fake nodes to satisfy the same objective, a Sybil attack takes place \cite{259745}.



\paragraph{Robust Aggregation}
Several robust aggregation techniques have been introduced to tackle data poisoning in FL, while aiming at maintaining the FL performance. A group of these algorithms detects suspicious updates and reduces their contribution weight to the aggregation process on the server. For instance, the aggregator in \cite{pmlr-v80-yin18a} calculates the median or coordinate trimmed mean of local updates prior to generating the global update. Another group of algorithms finds clusters of clients and sets malicious clients apart from benign users so that the suspicious users do not participate in the aggregation process. As an example, FoolsGold \cite{259745} combats Sybil attacks by adjusting the learning rates of local models based on a contribution similarity. This method effectively identifies Sybil groups, when they are present. Nonetheless, it is prone to mistakenly flag benign participants and deteriorate the training performance. While this method relies on the similarity of malicious users, other approaches such as \cite{NIPS2017_f4b9ec30, pmlr-v80-mhamdi18a} take the correlation of benign users into account. In addition, statistical methods such as taking the median of updates have been shown effective in enhancing attack robustness \cite{9721118}. Robust aggregation with adaptive clipping (\verb+AdaClip+) is performed by zeroing out extremely large values for robustness to data corruption on clients, and adaptively clipping in the L2 norm to the moderately high norm for robustness to outliers \cite{NEURIPS2021_91cff01a}.

\paragraph{Differential Privacy}
While DP has been primarily considered as a defense against inference attacks, several studies show that it can also be effective in making the FL model more robust against poisoning attacks \cite{10.5555/3489212.3489304, sun2019really, naseri2022local, NEURIPS2020_fc4ddc15, ijcai2019p657}. An adaptive version of DP for FL is presented in \cite{NEURIPS2021_91cff01a} (\verb+AdaDP+) that clips the norm at a specified quantile of the update norm distribution similar to \verb+AdaClip+. The value at the quantile is calculated in real-time with DP.


% \subsection{Anomaly Detection}
% Anomaly detection techniques are actively employed to identify and prevent malicious updates from compromising systems. In FL, these methods can be employed on the server side to detect potential threats \cite{Hallaji2023}. FL-based anomaly detectors often utilize a central test data to evaluate updates and filter out those that are detrimental or neutral to the global model.

% For instance, a detection method described in \cite{10.1145/2991079.2991125} groups users into benign and dangerous categories by clustering clients based on the information received to identify malicious updates. Another technique uses a distance metric for several participants to track updates' drift \cite{NIPS2017_f4b9ec30}. To find anomalous updates from participants, a different method creates low-dimensional model weight surrogates \cite{DBLP:journals/corr/abs-1910-09933}. Among other updates, updates that support the objective function are chosen using an outlier detection strategy. Examples of anomaly detection in FL include several methods outlined in recent studies \cite{247652, li2020learning}.



% Secure aggregator \cite{10.1145/3133956.3133982} zeroes out extremely large values for robustness to data corruption on clients, clips to moderately high norm for robustness to outliers. After weighting in mean, the weighted values are summed using cryptographic protocol ensuring that the server cannot see individual updates until sufficient number of updates have been added together.



% \paragraph{Regularization}



%[13–16], thus the attack effects on training performance can be diminished. Meanwhile, some heuristic-based aggregation rules [20, 21, 3, 22, 23] have been proposed to cluster participating clients into a benign group and a malicious group, and then perform aggregation on the benign group only. 

%In addition, [7, 24] show that applying differential privacy to the aggregated global model can improve the robustness against model poisoning attacks. All these defensive methods are deployed at the server side and their goals are to mitigate model poisoning attacks during aggregation. Unfortunately, often in extreme cases (e.g. attackers occupy a large proportion of total clients), existing robust aggregation methods fail to prevent the aggregation from being polluted by the malicious local updates showing that it is not sufficient to offer defense via aggregation solely. Thus, there is an urgent necessity to design a novel local training method in FL to enhance its robustness against model poisoning attacks at the client side, which is complementary to existing robust aggregation approaches.




\section{Threat Models}
\label{sec:threat_model}
Poisoning attacks can be conducted in various settings with different objectives. In this work, we consider three main categories of these attacks, namely sample poisoning, label flipping, and backdoors.

\paragraph{Sample Poisoning}
Let $X = \{x_1, x_2, \dots , x_m\}$ and $Y = \{y_1, y_2, \dots, y_m\}$ represent benign training samples and labels at time $t$, where $X \in \mathbb{R}^{n}$ and $Y \in \mathbb{N}$. $m$ and $n$ denote the number of samples and dimensions, respectively. A dataset $\mathcal{D} = \{(x_i, y_i)\}$ is defined as a set of tuples drawn from $X$ and $Y$, where $1\leq i\leq m$. The attacker aims to obtain a poisoned dataset $\tilde{\mathcal{D}}$ by injecting a set of malicious samples $\Delta x$ into $\tilde{\mathcal{D}}$ to minimize the model's performance. Under an untargeted setting, sample poisoning is carried out as:
\begin{equation}
    %\min_{\Delta X} L(\theta, X_t + \Delta X, Y_t),
    % \min_{\Delta x} L(\theta, \{x_i + \Delta x \mid P(\Delta x \neq 0) = \gamma\}_{i=1}^m, Y),
    \tilde{\mathcal{D}} = \{(x_i + \Delta x, y_i) \mid P(\Delta x \neq 0) = \gamma \}_{i=1}^m,
\end{equation}
where $\gamma$ is a parameter determining the ratio of poisoned samples in $\tilde{\mathcal{D}}$. Under a targeted setting, the attacker distorts the sample data for a specific class $c$:
\begin{equation}
    % \min_{\Delta x} L(\theta, \{x_i + \Delta x \mid P(\Delta x \neq 0 \mid y_i = c) = \gamma\}_{i=1}^m, Y)
    \tilde{\mathcal{D}} = \{(x_i + \Delta x, y_i) \mid P(\Delta x \neq 0 \mid y_i = c) = \gamma \}_{i=1}^m
\end{equation}

\paragraph{Label Flipping}
Let $y_i$ represent the true label of a sample $x_i$, and $\hat{y}_i$ denotes the label assigned by the model. The attacker aims to modify the labels of a subset of the training data to minimize the model's performance. In the untargeted scheme, the attacker randomly flips labels:
\begin{equation}
    % \min_{\hat{y}_i} L(\theta, X, \{\hat{y}_i \mid P(\hat{y}_i \neq y_i) = \gamma\}_{i=1}^m)
    \tilde{\mathcal{D}} = \{( x_i, \hat{y}_i) \mid P(\hat{y}_i \neq y_i) = \gamma\}_{i=1}^m
\end{equation}
In the targeted scheme, the attacker specifically targets a certain class $c$, switching its labels to other classes:
\begin{equation}
    % \min_{\hat{y}_i} L(\theta, X, \{\hat{y}_i \mid P(\hat{y}_i \neq y_i) = \gamma \land y_i = c\}_{i=1}^m)
    \tilde{\mathcal{D}} = \{(x_i, \hat{y}_i) \mid P(\hat{y}_i \neq y_i) = \gamma \land y_i = c\}_{i=1}^m
\end{equation}

\paragraph{Backdoor}
The attacker aims to modify the training data distribution to include backdoor triggers $\epsilon$ to the samples of class $c$ so that when the trigger is activated or observed in samples, it will be predicted as $y_{backdoor}$ to manipulate the model's behavior:
\begin{equation}
    %\min_{\Delta x, y_{backdoor}} L(\theta, X_t \cup \Delta X , y_{backdoor}),
    % \min_{\epsilon, y_{backdoor}} L(\theta, \{x_i + \epsilon \mid y_i = c\}_{i=1}^m, y_{backdoor})
    \tilde{\mathcal{D}} = \{(x_i + \epsilon, y_{backdoor}) \mid y_i = c\}_{i=1}^m
\end{equation}
$\epsilon \in \mathbb{R}^n$, and $y_{backdoor} \neq c$ in the above formulation. Backdoors are targeted attacks by nature. We consider two backdoor mechanisms: pixel backdoor and injecting specific noise patterns into the data.

In our simulations, attackers have white-box access to local models, meaning they can see the architecture of the local model but have no knowledge of the aggregation structure on the server. This implies that the attackers can inspect the internal workings of individual models but do not have information about how these models are combined or aggregated at a higher level.




\section{Noise Induced Activation Analysis}
\label{sec:methodology}
In this section, the design of the proposed method, \verb+FedNIA+, and the motivation behind it are explained. Moreover, we explain how other FL system components interact with the proposed approach.

\paragraph{Client Models} Considering a set of $k$ clients $C=\{c_1, c_2, \dots, c_k\}$, a model $M$ is formally defined as:
\begin{equation}
M(W_i, x):  x, h_1^M, h_2^M, \dots, h_L^M, \hat{y}
\end{equation}
for client $c_i$ with network weights $W_i$ and $L$ hidden layers $h_l$. Indicating the FL time-steps with $t$, we denote trained model weights and the local dataset at each iteration with $W_i^t$ and $\mathcal{D}_i^t$, respectively. At each time step, $M$ is first initialized with weights of the global model $W_G^t$, which we refer to as the global state. Then, $M$ is trained on the local data $\mathcal{D}_i^t$ to obtain $W_i^t$ as $M(W_G^t, \mathcal{D}_i^t)\rightarrow W_i^t$. Afterward, $W_i^t$ will be communicated to the server as an update. This process is also shown in Algorithm \ref{alg:client}.

\begin{algorithm}[t]
\caption{Clients}
\label{alg:client}
%\vspace{0.1cm}
\begin{algorithmic}
%\SetAlgoLined
\STATE{\hspace{-0.3cm}{\bfseries Option:} Benign or malicious}
\STATE{\hspace{-0.3cm}{\bfseries Input:} Local datasets $\mathcal{D}$ (or malicious dataset $\tilde{\mathcal{D}}$),\\ global state $W_G$}
\STATE{\hspace{-0.3cm}{\bfseries Output:} Local updates $W$ (or malicious updates $\tilde{W}$)}
%etKwInput{Initialization}{Initialization}
 %\Initialization{Initialize $M$ with $W_G^0$}
 \FOR{$t \in [0, T]$}
 \IF{Malicious}
  \FORALL{client $i \in [1, r]$ in parallel}
  \STATE Train $M(W_G^t, \mathcal{\tilde{D}}_i^t)$\\
  \STATE Create update $\tilde{W}_i^t \gets M(W_G^t, \mathcal{\tilde{D}}_i^t)$ \\
  \ENDFOR
  \ELSE
\FORALL{client $i \in [1, k]$ in parallel}
  \STATE Train $M(W_G^t, \mathcal{D}_i^t)$\\
  \STATE Create update $W_i^t \gets M(W_G^t, \mathcal{D}_i^t)$ \\
  \ENDFOR
  \ENDIF
 \ENDFOR
\end{algorithmic}
%\vspace{0.1cm}
\end{algorithm}



% \begin{algorithm}[t]
% \caption{Malicious Client}
% \label{alg:attacker}
% \begin{algorithmic}
% %\SetAlgoLined
% \STATE{\hspace{-0.3cm}{\bfseries Input:} Malicious dataset $\tilde{\mathcal{D}}$, global state $W_G$}
% \STATE{\hspace{-0.3cm}{\bfseries Output:} Malicious updates $\tilde{W}$}
% %\SetKwInput{Initialization}{Initialization}
%  %\Initialization{Initialize $M$ with $W_G^0$}
%  \FOR{$t \in [0, T]$}
%   \FORALL{client $i \in [1, r]$ in parallel}
%   \STATE Train $M(W_G^t, \mathcal{\tilde{D}}_i^t)$\\
%   \STATE Create update $\tilde{W}_i^t \gets M(W_G^t, \mathcal{\tilde{D}}_i^t)$ \\
%   \ENDFOR
%  \ENDFOR
%  \end{algorithmic}
% \end{algorithm}


\paragraph{Malicious Clients} 
Algorithm \ref{alg:client} also details the condition where malicious clients $\tilde{C}=\{\tilde{c}_1, \tilde{c}_2, \dots, \tilde{c}_r\}$ can initiate a poisoning attack on the FL process by training the model on poisoned data $\tilde{\mathcal{D}}_i^t$ and obtain $M(W_G^t, \tilde{\mathcal{D}}_i^t) \rightarrow \tilde{W}_i^t$. If not eliminated by the aggregator, sending $\tilde{W}_i^t$ to the server will poison the next global state $W_G^{t+1}$ and corrupt all $W_i^{t+1}$ as a result. Here, the number of malicious clients is assumed to be $1 \leq r \leq \frac{k}{2}$. This is because $r\geq \frac{k}{2}$ indicates a $51\%$ attack, where the population of malicious clients is higher than benign clients, which enables them to control the FL network.

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.93\textwidth, trim={0.6cm 0.5cm 0.6cm 0.5cm}, clip]{nips.pdf}
    \caption{Block diagram of \texttt{FedNIA} on the server. Dashed red lines indicate steps of the algorithm that should be undertaken once all steps specified with black solid lines are completed.}
    \label{fig:diagram}
\end{figure*}

\paragraph{Server} The FL server will receive a number of updates $W_i^t$ from clients. For the sake of simplicity, we assume all clients are available at each iteration, that is $1 \leq i \leq k + r$. At this step, the server aims to evaluate $W_i^t$ to filter out potentially malicious updates. Since different mechanisms of poisoning attack affect $M$ differently, designing a detection model that handles a spectrum of attacks requires careful analysis of $W_i^t$ at each layer for all incoming updates. Nevertheless, $W_i^t$ is often high-dimensional with millions of values included in weight matrices. Therefore, resorting to anomaly detection principles to directly process $W_i^t$ may not be efficient. A better alternative would be reconstructing $M(W_i^t, \mathcal{D}_{test})$ on the server so that the model could be evaluated on a test dataset $\mathcal{D}_{test}$ based on the resulting outputs. Nonetheless, access to client data is prohibited, and considering central $\mathcal{D}_{test}$ requires the assumption that the data distribution remains unchanged in time, which is unrealistic in most cases. Server-side operations are explained in Algorithm \ref{alg:server}.

%\setlength{\algomargin}{1em}
\begin{algorithm}[t]
\caption{\texttt{FedNIA}}
\label{alg:server}
\begin{algorithmic}
\STATE{\hspace{-0.3cm}{\bfseries Input:} Client weights $W_i^t$}
\STATE{\hspace{-0.3cm}{\bfseries Output:} Next global State $W_G^{t+1}$}
 \STATE{\hspace{-0.3cm}{\bfseries Initialization:} Initialize the global model $W_G^0$}
 \FOR{$t \in [0, T]$}
  \STATE Generate random inputs $Z^t$\;
  \FORALL{$W_i^t$ in parallel}
   \STATE Get layer activations $A_i^t \gets M(W_i^t, Z^t)$\\
   \STATE Estimate the average activations for each $c_i$: $\bar{A}_i^t = \frac{1}{\nu}\sum_{j=1}^\nu A_{i(j)}^t$
  \ENDFOR
  \STATE Get layer activations $A_G^t \gets M(W_G^t, Z^t)$\\
  \STATE Compute averaged layer activations $\bar{A}_G^t = \frac{1}{\nu}\sum_{j=1}^{\nu}A_{G(j)}^t$\\
  \STATE Train autoencoder $AE$ on $\bar{A}_G^t$ w.r.t. (\ref{eq:loss})\\
  \FORALL{$W^t_i$}
   \STATE Feed $\bar{A}_i^t$ to the detector:  $\hat{A}_i^t \gets AE(\bar{A}_i^t)$\\
   \STATE Compute reconstruction error $e_i = \sqrt{\frac{1}{k+r}\parallel \bar{A}_i^t - \hat{A}_i^t \parallel^2}$\\
  \ENDFOR
  \STATE Calculate threshold $\tau$ using (\ref{eq:tau})\\
  \STATE Filter updates $\Omega = \{W^t_1, W^t_2, \dots, W_k^t\}$, where $W^t_i = \varnothing$ if $e_i < \tau$\\
  \STATE $W_G^{t+1} \gets$ Aggregate updates FedAvg$(\Omega)$
 \ENDFOR
\end{algorithmic}
\end{algorithm}




\paragraph{Detection Model}
Each layer $l$ of the model $M$ produces a vector of activation values, denoted as $\alpha_l$, where the length of $\alpha_l$ equals the number of neurons in that layer. For a network with $L$ layers, $A_i^t$ represents the concatenation of these vectors $\alpha_1, \alpha_2, \dots, \alpha_L$ for client $i$ at time $t$. To enable the comparison between the average of noise-induced client activations $\bar{A}^t_i$, $1\leq i \leq k + r$, and the activations of the global state, $\bar{A}^t_G$, $Z^t$ is also passed to $M(W_G^t, Z^t)$. At this stage, $\bar{A}_G^t$ can be used as a reference for comparison with $\bar{A}_i^t$, where $1\leq i \leq k + r$. To enable anomaly detection on each layer of $M$, we tailor an autoencoder model with sub-networks that share input and output layers so that each sub-network can concentrate on encoding and decoding activation values corresponding to a specific layer. Each sub-encoder $E(\alpha_l)$ takes $\alpha_l$ portion of $\bar{A}_i^t$ that corresponds to layer $l$ of $M(W_i^t, Z^t)$ as in the following:
\begin{equation}
    E(\alpha_l): \alpha_l, h^E_{l(1)}, h^E_{l(2)}, \dots, h^*_l
\end{equation}
where $h_{l(j)}^E$ denotes the $j$-th hidden layer of the sub-network $E(\alpha_l)$, and $h_l^*$ is the code layer of $E(\alpha_l)$. The encoder network is then formally defined as:
\begin{equation}
    \operatorname{Encoder}(\bar{A}): \bar{A}, \bigcup_{l=1}^L E(\alpha_l) \mid \alpha_l \in \bar{A}
\end{equation}
Correspondingly, sub-decoders $D_l$ and the decoder are formulated as:
\begin{equation}
    D(h^*_l): h^*_l, h^D_{l(1)}, h^D_{l(2)}, \dots, \hat{\alpha}_l
\end{equation}
\begin{equation}
    \operatorname{Decoder}(h^*): \bigcup_{l=1}^L D(h^*_l), \hat{A} \mid h^*_l  \in h^*
\end{equation}
where $\hat{\alpha}$ is the reconstructed activation values of $h_l^M$, and $\hat{A}$ is the estimated vector of all activation values. Also, $h^* = \{h_1^*, h_2^*, \dots h_L^*\}$ is the code layer connected to the sub-networks in the encoder and decoder. The autoencoder is formulated as $AE(\bar{A}) = \operatorname{Decoder}\left(\operatorname{Encoder}(\bar{A})\right)$. The architecture of the trained $AE$ and other components of the server are depicted in Figure \ref{fig:diagram}.



\paragraph{Training Loss} Minimizing the reconstruction error on the whole $\hat{A}^t$ vector may reduce the success rate in some cases. Given that $|h_l^M|$ is different for $1\leq l \leq L$, the reconstruction error associated with each $\hat{\alpha}_l$ does not equally contribute to the training loss. For example, in a binary classification problem $|\alpha_L|=2$; however, other hidden layers may have hundreds of values. Therefore, the training loss $J$ of the $AE$ network is defined as the average of root mean squared errors of separate layers as follows:
\begin{equation}
    J(\bar{A}^t_i, \hat{A}^t
    _i) = \frac{1}{L}\sum_{l=1}^L \sqrt{\frac{\parallel\alpha_l - \hat{\alpha}_l\parallel^2}{|\alpha_l|}}
    \label{eq:loss}
\end{equation}

\paragraph{Filtering Updates}
At each FL round, $AE$ is first trained on $\bar{A}_G^t$. Then, $AE$ encodes and reconstructs $\bar{A}_i^t$ and receives $\bigcup_{i=1}^{k+r} \hat{A}_i^t = AE(\bar{A}_i^t)$. Then, the reconstruction error for each $\bar{A}_i^t$, $1 \leq i \leq k + r$, will be computed as $e_i = \sqrt{\frac{1}{k+r}\parallel \bar{A}_i^t - \hat{A}_i^t \parallel^2}$. Once the errors are estimated, $c_i$ corresponding to $e_i$ will be filtered based on the threshold $\tau$:
\begin{equation}
    \tau = \left(\frac{1}{k + r}\sum_{i=1}^{k+r} e_i\right) + \lambda \sigma
    \label{eq:tau}
\end{equation}
where $\lambda$ is the scaling factor, and $\sigma$ denotes the standard deviation of obtained errors. Benign updates are then sampled into $\Omega = \{W^t_1, W^t_2, \dots, W_k^t\}$ to prevent $\tilde{c}_i \in \tilde{C}$ contribute in the aggregation step:
\begin{equation}
\bigcup_{i=1}^{k+r}  
    \begin{cases}
        \varnothing & \text{if $e_i < \tau$}  \\
        W_i^t  & \text{otherwise}
    \end{cases}
\end{equation}
The aggregation of $W_i^t$ is performed using the \verb+FedAvg+ algorithm. At the end of round $t$, \verb+FedAvg+$(\Omega) = W_G^{t+1}$ estimates the global state for $t+1$, when the explained process will be repeated.

\paragraph{Time Complexity}
As detailed in Appendix \ref{sec:complexity}, the time complexity of \texttt{FedNIA} is approximately $O(k |W| + \beta \eta |\theta|)$, which roughly equals a complexity of $O(\beta \eta |\theta|)$ added to that of the \verb+FedAvg+.

\begin{figure*}[t]
    \centering
    \includegraphics[trim={0.2cm 0.5cm 0.2cm 0},clip,width=0.75\textwidth]{benign.eps}
    \caption{Performance of different aggregation techniques trained on benign data. 100 clients participated in the training process. $t$ denotes the FL iteration number.}
    \label{fig:bengin}
\end{figure*}

% \subsection{Complexity Analysis}
% For the sake of simplicity, we assume all clients are benign and $r=0$. In each FL iteration $t$, \verb+FedNIA+ initially generates $\nu$ random inputs, which has a complexity of $O(\nu)$. Next, obtaining layer activations for each client model involves a forward pass, leading to a complexity of $O(k \nu |W|)$, where $k$ is the number of updates received from clients, and $|W|$ is the total number of weights in the model. Averaging these activations across all clients adds a complexity of $O(k \nu  \eta)$, where $\eta$ is the total number of activations across all layers. Training the autoencoder on the averaged activations $\bar{A}_G^t$ involves $\beta$ epochs with each epoch having a complexity of $O(\eta |\theta|)$, where \(|\theta|\) is the number of parameters in the autoencoder, leading to a total complexity of $O(\beta \eta |\theta|)$ for the training phase. The inference phase, which includes encoding and reconstructing activations, adds $O(k \eta |\theta|)$. Calculating the reconstruction error for each client is $O(k \eta)$, and computing the threshold and filtering updates both contribute a complexity of $O(k)$. Finally, aggregating the filtered updates, akin to \verb+FedAvg+, has a complexity of $ O(k |W|)$. Assuming that $\nu, \eta \ll |W|$ and $|\theta| < |W|$, the dominating terms in the combination of the aforementioned terms, simplify to $O(k |W| + \beta \eta |\theta|)$, indicating that time complexity of $O(\beta \eta |\theta|)$ is added to that of the \verb+FedAvg+.

\section{Experimental Results}
\label{sec:results}



This section evaluates the proposed method under several scenarios. The evaluation process involves comparing \verb+FedNIA+ with other defense mechanisms usable in the selected attack scenarios. The experimental setup and attack scenarios are reported and analyzed accordingly. 

\subsection{Experimental Setup}


\paragraph{Dataset}
The proposed method is evaluated on the Extended MNIST (EMNIST) \cite{7966217} and Fashion-MNIST \cite{xiao2017fashionmnist} datasets. The original dataset is shuffled with a fixed random seed to ensure consistency across multiple runs. To create local datasets, $\frac{m}{k+r}$ samples are randomly drawn without replacement from the original dataset and batched with a size of 20 samples. This causes local datasets to be highly imbalanced and makes the datasets non-i.i.d. across the federated network.



\paragraph{Attacks} Attacks are implemented with different ratios of $\delta = \frac{r}{k+r}$ ratios, where $r$ and $k$ are the numbers of malicious and benign clients, respectively. In our simulations, we consider $0.02\leq \delta \leq 0.2$. Furthermore, the total number of clients is fixed to $k+r=50$ when $\delta$ changes. To maximize the effectiveness of attacks, sample poisoning, and label flipping attacks are implemented with $\gamma=1$. Untargeted attacks are implemented using sample poisoning and label flipping. For targeted attacks, targeted label poisoning and backdoors are simulated. Targeted label flipping is carried out in both targeted and untargeted fashion, where labels $1$ and $2$ are changed to $7$ and $5$, respectively. Backdoors are injected into samples of class $c=1$ using a trigger $\epsilon$ resembling a pixel backdoor attack.


\paragraph{Federated Learning Structure}
The model structure used for FL is a feedforward neural network with three hidden layers with ReLU activations and sizes 256, 256, and 128. The final layer uses a Softmax activation. Local models undergo five training epochs in each FL round, both clients and the server use Stochastic Gradient Decent (SGD) optimizers. The number of FL training rounds is set to $T=500$, and global and local learning rates are set to $1$ and $0.02$, respectively. 

\paragraph{Detector} The detector uses ReLU activations and is optimized using SGD with a learning rate of $0.02$. Encoder layers are set to half of the corresponding layer size of the global model in the first hidden layer, and divided by two for each subsequent layer. The decoder part mirrors this structure with two hidden layers on each side and connects to the encoder via a code layer. The number of generated noise inputs is set to $\nu=100$, and the detector is trained for $\beta = 50$ epochs with a batch size of 10 in each FL round.

\begin{figure*}[t]
    \centering
    \includegraphics[trim={0.4cm 0.4cm 0.2cm 0cm},clip,width=0.94\textwidth]{attacks.eps}
    \caption{Evaluation of aggregation mechanisms when the FL system is subjected to poisoning attacks. $\delta$ denotes the ratio of attackers to the total number of clients. Subplot (i) depicts the overall accuracy, averaged across all experiments. ASR is converted to accuracy for this subplot.}
    \label{fig:attacks}
\end{figure*}


\subsection{Aggregation Performance}
Before evaluating the selected algorithms under poisoning attacks, we assess their performance when the FL network is not under attack and $\delta=0$. Figure \ref{fig:bengin} depicts the classification performance using various aggregation techniques when the FL model is trained by 100 benign clients. Comparing the training loss of these algorithms in Figure \ref{fig:bengin}(a), it can be observed that \verb+FedNIA+ initially exhibits a slower convergence speed compared to \verb+FedAvg+; however, given enough time, it outperforms all selected aggregators. This delay in the convergence is due to the fact that the autoencoder network inside \verb+FedNIA+ itself requires convergence and training. Nonetheless, when the autoencoder is finally trained, \verb+FedNIA+ will boost the convergence speed faster than others. Therefore, when the model is not under attack, \verb+FedNIA+ can maintain the model performance and improve the training loss. Similarly, the results in Figure \ref{fig:bengin}(c) show that \verb+FedNIA+ converges at a speed similar to that of \verb+FedAvg+. However, the convergence speeds in experiments using the Fashion MNIST dataset are mostly on par with each other.



The recorded test accuracy during the FL training verifies the previous analysis, as shown in Figure \ref{fig:bengin}(b). Although the test accuracy of \verb+FedNIA+ initially lags behind \verb+FedAvg+, given enough training time, it reaches an accuracy on par with that of \verb+FedAvg+. This is while the proposed method outperforms the rest of the aggregators in terms of the test accuracy, regardless of the number of iterations. This analysis is also confirmed in Figure \ref{fig:bengin}(d), which depicts the test accuracy on the Fashion MNIST dataset.


\subsection{Resilience to Attacks}
The goal of attackers in untargeted attacks is generally to deteriorate the overall accuracy of the FL model. Therefore, sample poisoning and untargeted label flipping are evaluated based on the test accuracy. On the other hand, backdoor and targeted label flipping aim at specific classes. Thus, a test subset is created by drawing the test samples associated with the targeted class and used for calculating the test accuracy. For backdoor attacks, we create a test subset containing samples with triggers and the expected labels the attacker selects so that the Attack Success Rate (ASR) can be estimated.



\paragraph{Targeted Data Poisoning} Figure \ref{fig:attacks} reports the aggregation results for different ratios of $\delta$ and attack mechanisms using EMNIST and Fashion MNIST datasets. As expected, increasing $\delta$ significantly affects the model performance using other aggregators. Nonetheless, \verb+FedNIA+ maintains its robustness to a large extent as $\delta$ varies. The results indicate that \verb+FedNIA+ specifically sets itself apart when it comes to targeted attacks. As shown in Figure \ref{fig:attacks}(a, b), targeted attacks (i.e., targeted label poisoning and backdoor) are properly mitigated by the proposed aggregation method. In contrast to other methods, this attack almost changes the \verb+FedNIA+ performance with a linear pattern, while slightly changing its accuracy. Moreover, the results of backdoor attacks indicate even DP-based methods that induce noise in the network weights, still fail to mitigate backdoors when they are initiated by several malicious nodes. Nonetheless, \verb+FedNIA+ sets itself apart in detecting backdoors and significantly outperforms the rest of the methods. It is worth mentioning that the designed backdoors are so subtle that launching them by a single attacker node hardly has any effect on the global state, even on \verb+FedAvg+. Figure \ref{fig:attacks}(e, f) depicts the results of targeted attacks for Fashion MNIST dataset. Similar to the previous analysis, \verb+FedNIA+ outperforms the rest of the methods in terms of accuracy. In addition, this method exhibits more robustness against different ratios of $\delta$. While the comparison between different methods is mostly similar for this case, what stands out is the bigger difference between the baseline performance (\verb+FedAvg+) and the rest of the methods. Furthermore, the difference in the performance is more noticeable for the targeted label poisoning compared to that of the EMNIST experiment.

% \begin{figure*}[h]
%     \centering
%     \includegraphics[width=\textwidth, trim={2.9cm 0.6cm 3.1cm 0.6cm}, clip]{targeted.eps}
%     \caption{Evaluation of aggregation mechanisms when the FL system is subjected to targeted poisoning attacks. $\delta$ denotes the ratio of attackers to the total number of clients.}
%     \label{fig:attacks}
% \end{figure*}



\paragraph{Untargeted Data Poisoning}
The evaluation results for untargeted poisoning attacks are illustrated in Figure \ref{fig:attacks}(c, d). \verb+FedNIA+ demonstrates high robustness against sample poisoning compared to the other algorithms using EMNIST dataset. However, the difference in the accuracy among the selected aggregators is less pronounced for this attack. As the ratio of malicious clients 
($\delta$) increases, the accuracy of \verb+FedNIA+ remains relatively stable, showcasing its effectiveness in mitigating the adverse effects of data poisoning. Furthermore, \verb+FedNIA+ effectively eliminates various ratios of the label flipping attack, maintaining relatively high performance. In contrast, the accuracy of the other methods drastically deteriorates as $\delta$ increases. Looking at Figure \ref{fig:attacks}(g, h), it can be observed that \verb+FedNIA+ also outperforms the rest of the methods when tested on Fashion MNIST dataset. These results also confirm the findings from EMNIST experiments. The most noticeable difference in this set of results is the smaller difference between the performance of \verb+FedNIA+ and the rest, albeit still significant.



\paragraph{Overall Performance}
Figure \ref{fig:attacks}(i) shows the overall accuracy of robust aggregators over all experiments. The results are averaged for all attacks and datasets. For Backdoor attacks, we converted ASR to accuracy by calculating $1-\operatorname{ASR}$. As indicated in this figure, \verb+FedNIA+ outperforms all when considering the overall performance. Moreover, \verb+FedAvg+ results in the lowest performance when under attack, as expected. The rest of the methods exhibit a somewhat similar performance, especially in higher $\delta$ ratios. Nonetheless, as explained before, this behavior may not be always the case when studying certain attacks and datasets, as these results only reflect the overall performance of each method throughout the experiments. Moreover, Appendix \ref{sec:ttest} confirms that $\texttt{FedNIA}$ exhibits significant improvement in handling various types of poisoning attacks.




\subsection{Runtime Analysis}
Figure \ref{fig:time} presents a comparison of the average runtime for a single FL training iteration using each of the selected aggregators. The recorded run time is averaged for both datasets and all attacks and ratios. These measurements were obtained on a computer equipped with an NVIDIA RTX 3080 GPU, an Intel Core i7-12700 CPU, and 32 GB of RAM. The experiments were conducted using TensorFlow Federated on an Ubuntu kernel, accessed via the Windows Subsystem for Linux. The results suggest that the superior balance between accuracy and security offered by \verb+FedNIA+ comes with a higher computational cost, a common trait among robust aggregators. However, similar to \verb+FedAvg+, the runtime of \verb+FedNIA+ is linearly correlated with the total number of clients. This finding aligns with the previous complexity analysis of the proposed algorithm. While we consider the enhanced robustness worth the increased computational cost, future research could explore ways to improve the algorithm's efficiency.


\begin{figure}
    \centering
    \includegraphics[trim={0.3cm 0.5cm 0.3cm 0},clip,width=0.81\columnwidth]{time.eps}
    \caption{Averaged runtime comparison between the proposed method and the baseline, FedAvg. The time is recorded for one FL training round and averaged for EMNIST and Fashion MNIST experiments.}
    \label{fig:time}
\end{figure}

% \begin{table}[h]
%     \centering
%     \tabcolsep 8pt
%     \caption{Run time per FL training ground using different aggregators. Results are reported in terms of seconds.}
%     \begin{tabular}{lcc}
%     \toprule
%         \multirow{2}{*}{Aggregator} & \multicolumn{2}{c}{Time (s)}\\
%         \cline{2-3}
%         & $r=50$ & $r=100$ \\
%         \midrule
%         FedAvg & 0.65 & 1.50 \\
%         DP & 0.70 & 1.61\\
%         AdaDP & 0.83 & 1.82 \\
%         AdaClip & 0.76 & 1.75 \\
%         FedNIA & 1.32 & 2.15 \\
%         \bottomrule
%     \end{tabular}
%     \label{tab:time}
% \end{table}
 
% \begin{figure*}[h]
%     \centering
%     \includegraphics[width=\textwidth, trim={2.9cm 0.6cm 3.1cm 0.6cm}, clip]{untargeted.eps}
%     \caption{Performance evaluation of aggregators when FL model is under untargeted poisoning attacks.}
%     \label{fig:untargeted}
% \end{figure*}

%\subsection{Ablation Study}

\section{Conclusion}
\label{sec:conclusion} 
This paper contributes to the ongoing efforts to secure FL systems against adversaries by presenting \verb+FedNIA+, a novel mechanism to fortify FL systems against a wide array of data poisoning attacks. This method is unique in its ability to address both targeted and untargeted attacks, including sample poisoning, label flipping, and backdoor attacks that are launched by several malicious nodes. By injecting random noise inputs into the reconstruction of client models and analyzing the layer activations, \verb+FedNIA+ was able to identify and exclude malicious clients before the aggregation process. In addition, \verb+FedNIA+ eliminates the need for a central test dataset for client evaluation, a common yet often impractical assumption in existing defense mechanisms. Experimental results demonstrate the effectiveness of our approach. The proposed method not only successfully identifies and excludes malicious updates, but also maintains a performance level similar to the baseline when no attack is present. This balance between resilience and performance in the absence of threats is a significant advantage of the proposed approach. Moreover, \verb+FedNIA+ has shown robustness against various mechanisms of sample poisoning, label flipping, and backdoor attacks, covering the primary categories of data poisoning attacks when federated data is non-i.i.d. Future works will focus on further refining the computational efficiency of our method and exploring its applicability to other types of attacks and FL scenarios.


\section*{Acknowledgments}
This work was supported by the Natural Sciences and Engineering Research Council of Canada (NSERC) under funding reference numbers CGSD3-569341-2022 and RGPIN-2021-02968.

%\section*{Impact Statement}
%This paper presents work whose goal is to advance the field of Federated Learning security. There are many potential societal consequences of our work, none of which we feel must be specifically highlighted here.
%This paper proposes \texttt{FedNIA}, a method to enhance the robustness of federated learning systems against data poisoning attacks, crucial for privacy-preserving machine learning. By detecting and mitigating adversarial behaviors without the need for centralized test datasets, FedNIA is applicable in privacy-sensitive and decentralized environments, benefiting sectors like healthcare, finance, and smart infrastructure. Ethically, it promotes fairness and trust by reducing adversarial manipulation, though its potential misuse for creating advanced attacks highlights the need for responsible use. Future research could also address the environmental impact of its computational costs, aiming to balance robustness, efficiency, and ethical concerns in machine learning.
%This paper addresses the critical issue of securing federated learning systems against data poisoning attacks, a significant concern in machine learning. The proposed method, enhances the robustness of federated learning by detecting and mitigating adversarial behaviors without requiring centralized test datasets, making it suitable for privacy-sensitive and decentralized environments. This work has potential societal benefits in domains such as healthcare, finance, and smart infrastructure, where secure and distributed data processing is critical. Ethically, FedNIA promotes fairness and trust by reducing adversarial manipulation risks, but it also poses a potential risk of misuse for developing advanced adversarial attacks, emphasizing the need for responsible dissemination. Additionally, its computational costs call for future work to explore environmentally conscious optimizations, balancing robustness, efficiency, and ethical responsibility in advancing machine learning.



%This paper addresses the pressing issue of securing FL systems against data poisoning attacks, a growing concern in machine learning. The proposed method enhances the robustness of FL by detecting and mitigating adversarial behaviors without requiring centralized test datasets, thus making it applicable in privacy-sensitive and decentralized environments. Enhancing the reliability of FL systems, this work can therefore have potential societal benefits in domains such as healthcare, finance, and smart infrastructure wherein secure and distributed data processing is critically needed. Our methodology, from an ethical point of view, promotes trust in machine learning by reducing the risk of adversarial manipulation. This work contributes to the bigger goal of enabling secure, fair, and reliable machine learning and we believe our approach will foster risk mitigation in real-world FL systems.


\bibliography{main.bib}
\bibliographystyle{icml2025}

\newpage
\appendix
\onecolumn

\section{Appendix}
\subsection{Complexity Analysis}
\label{sec:complexity}
For the sake of simplicity, we assume all clients are benign and $r=0$. In each FL iteration $t$, \verb+FedNIA+ initially generates $\nu$ random inputs, which has a complexity of $O(\nu)$. Next, obtaining layer activations for each client model involves a forward pass, leading to a complexity of $O(k \nu |W|)$, where $k$ is the number of updates received from clients, and $|W|$ is the total number of weights in the model. Averaging these activations across all clients adds a complexity of $O(k \nu  \eta)$, where $\eta$ is the total number of activations across all layers. Training the autoencoder on the averaged activations $\bar{A}_G^t$ involves $\beta$ epochs with each epoch having a complexity of $O(\eta |\theta|)$, where \(|\theta|\) is the number of parameters in the autoencoder, leading to a total complexity of $O(\beta \eta |\theta|)$ for the training phase. The inference phase, which includes encoding and reconstructing activations, adds $O(k \eta |\theta|)$. Calculating the reconstruction error for each client is $O(k \eta)$, and computing the threshold and filtering updates both contribute a complexity of $O(k)$. Finally, aggregating the filtered updates, akin to \verb+FedAvg+, has a complexity of $ O(k |W|)$. Assuming that $\nu, \eta \ll |W|$ and $|\theta| < |W|$, the dominating terms in the combination of the aforementioned terms, simplify to $O(k |W| + \beta \eta |\theta|)$, indicating that time complexity of $O(\beta \eta |\theta|)$ is added to that of the \verb+FedAvg+.

\subsection{Significance Test}
\label{sec:ttest}
Figure \ref{fig:ttest} shows Critical Difference (CD) diagrams obtained from the post-hoc Friedman test. The significance level (i.e., parameter $\alpha$) is set to $0.05$ in this test. This test estimates the significance of differences among the results obtained from each method in different experiments. Based on the determined CD level of this test, methods that are not significantly different in terms of accuracy are connected and grouped using colored lines. For instance, in Fig, \ref{fig:ttest}(b, d), the results obtained from \verb+FedAvg+, \verb+AdaClip+, and DP are not significantly different when used against backdoor and label flipping attacks. In panels (c, e), on the other hand, the results indicate that \verb+AdaDP+ and DP statistically result in somewhat similar accuracy when dealing with sample poisoning, and when the overall performance is considered. The same observation can be made for \verb+AdaClip+ and \verb+AdaDP+ in the same scenarios. Nevertheless, in all conducted experiments, the performance of \verb+FedNIA+ is significantly better than the rest of the methods. This indicates the effectiveness and generalizability of the proposed method against data poisoning attacks in FL.

\begin{figure}[h]
    \centering
    \begin{subfigure}{0.49\textwidth}
        \centering
        \includegraphics[trim={0.3cm 13cm 0.3cm 0.2cm},clip,width=\textwidth]{cd_plot.eps}
    \end{subfigure}
    \begin{subfigure}{0.49\textwidth}
        \centering
        \vspace{1.2cm}
        \includegraphics[trim={0.3cm 0.5cm 0.3cm 19cm},clip,width=\textwidth]{cd_plot.eps}
            \vspace{1.2cm}
    \end{subfigure}
    \caption{Critical difference diagram obtained from the post-hoc Friedman test. The significance level ($\alpha$) is set to $0.05$.}
    \label{fig:ttest}
\end{figure}




% \section{Mathematical Analysis}

% \subsection{Effectiveness of Threshold-Based Filtering in FedNIA}

% FedNIA filters out potential malicious client updates based on the reconstruction error of their model activations. The filtering threshold is computed as:
% \begin{equation}
%     \tau = \mu + \lambda\sigma,
% \end{equation}
% where $\mu$ is the mean reconstruction error:
%     \begin{equation}
%         \mu = \frac{1}{k + r} \sum_{i=1}^{k+r} e_i.
%     \end{equation}
%  $e_i$ is the reconstruction error for client $i$, defined as:
%     \begin{equation}
%         e_i = \sqrt{\frac{1}{L} \sum_{l=1}^{L} \|\alpha_l - \hat{\alpha}_l\|^2},
%     \end{equation}
%     where $\alpha_l$ and $\hat{\alpha}_l$ are the activation vectors and their reconstructions at layer $l$.
%  $\sigma$ is the standard deviation of the reconstruction errors:
%     \begin{equation}
%         \sigma = \sqrt{\frac{1}{k+r} \sum_{i=1}^{k+r} (e_i - \mu)^2}.
%     \end{equation}
%  $\lambda$ is a tunable parameter controlling the strictness of filtering.


% Our goal is to analyze the effectiveness of this threshold in separating benign and malicious clients while minimizing false positives (incorrectly filtering benign clients) and false negatives (failing to filter malicious clients). We also derive an optimal choice of $\lambda$ to achieve this balance.

% \subsubsection{Modeling the Reconstruction Errors}

% We assume the following statistical distributions for the reconstruction errors:

% \begin{assumption}
% Reconstruction errors for benign clients follow a normal distribution:
% \begin{equation}
%     e_i^{\text{(benign)}} \sim \mathcal{N}(\mu_b, \sigma_b^2).
% \end{equation}
% Reconstruction errors for malicious clients follow:
% \begin{equation}
%     e_i^{\text{(malicious)}} \sim \mathcal{N}(\mu_m, \sigma_m^2),
% \end{equation}
% where $\mu_m > \mu_b$, meaning that malicious activations deviate significantly from the normal behavior, leading to higher reconstruction errors.
% \end{assumption}

% \subsubsection{Bounding the False Positive Rate (Benign Clients Mistakenly Filtered)}

% \begin{lemma}
% The probability that a benign client is mistakenly filtered (false positive) is given by:
% \begin{equation}
%     P(e_i^{\text{(benign)}} > \tau) = 1 - \Phi\left(\frac{\tau - \mu_b}{\sigma_b}\right),
% \end{equation}
% where $\Phi(\cdot)$ is the cumulative distribution function (CDF) of the standard normal distribution.
% \end{lemma}

% \begin{proof}
% A benign client is incorrectly filtered if its reconstruction error exceeds the threshold:
% \begin{equation}
%     P(e_i^{\text{(benign)}} > \tau) = P\left(e_i^{\text{(benign)}} > \mu + \lambda\sigma\right).
% \end{equation}

% Using the normal CDF definition, we standardize the variable:
% \begin{equation}
%     P\left(e_i^{\text{(benign)}} > \mu + \lambda\sigma\right) = 1 - \Phi\left(\frac{\mu + \lambda\sigma - \mu_b}{\sigma_b}\right).
% \end{equation}

% Rewriting:
% \begin{equation}
%     P(e_i^{\text{(benign)}} > \tau) = 1 - \Phi\left(\frac{\lambda\sigma + (\mu - \mu_b)}{\sigma_b}\right).
% \end{equation}

% \end{proof}

% \subsubsection{Bounding the False Negative Rate (Malicious Clients Accepted)}

% \begin{lemma}
% The probability that a malicious client is mistakenly accepted (false negative) is given by:
% \begin{equation}
%     P(e_i^{\text{(malicious)}} < \tau) = \Phi\left(\frac{\tau - \mu_m}{\sigma_m}\right).
% \end{equation}
% \end{lemma}

% \begin{proof}
% A malicious client is incorrectly accepted if:
% \begin{equation}
%     P(e_i^{\text{(malicious)}} < \tau) = P\left(e_i^{\text{(malicious)}} < \mu + \lambda\sigma\right).
% \end{equation}

% Standardizing:
% \begin{equation}
%     P(e_i^{\text{(malicious)}} < \tau) = \Phi\left(\frac{\lambda\sigma + (\mu - \mu_m)}{\sigma_m}\right).
% \end{equation}

% \end{proof}

% \subsubsection{Choosing an Optimal $\lambda$ for Robust Filtering}

% \begin{proposition}
% Setting $\lambda = \sqrt{-2 \log \delta}$ ensures that the probability of false positives or false negatives is at most $\delta$.
% \end{proposition}

% \begin{proof}
% From Lemma 1, we solve:
% \begin{equation}
%     e^{-\lambda^2 / 2} = \delta.
% \end{equation}
% Taking the natural logarithm:
% \begin{equation}
%     \lambda = \sqrt{-2 \log \delta}.
% \end{equation}

% This guarantees that the probability of either a benign or a malicious client being misclassified is kept below $\delta$. Therefore, $\lambda$ controls the trade-off between false positives and false negatives, ensuring effective filtering.
% \end{proof}

% \subsection{Convergence Analysis of FedNIA}

% \begin{theorem}
% FedNIA has the same asymptotic convergence rate as FedAvg but with lower variance:
% \begin{equation}
%     \mathbb{E}[F_{\text{FedNIA}}(w^t) - F(w^*)] = \mathcal{O} \left( \frac{1}{t} \right).
% \end{equation}
% \end{theorem}

% \begin{proof}
% Filtering out high-error updates reduces variance. If $\gamma$ is the fraction of malicious clients removed, the variance of updates satisfies:
% \begin{equation}
%     \sigma_{\text{FedNIA}}^2 = (1 - \gamma) \sigma_{\text{FedAvg}}^2.
% \end{equation}
% Since lower variance improves convergence speed, we obtain:
% \begin{equation}
%     \mathbb{E}[F_{\text{FedNIA}}(w^t) - F(w^*)] \leq (1 - \eta \mu) \mathbb{E}[F(w^t) - F(w^*)] + \mathcal{O}((1 - \gamma) \eta^2).
% \end{equation}
% \end{proof}

\end{document}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}