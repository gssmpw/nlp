\section{Related work}
Evaluation and ranking of learned multi-agent strategies is mainly based on game theoretic concepts, while computational social choice has been proposed, as well **Wellman, "Game Theoretic Reasoning About Systems"**. A predominant approach is the Elo rating system used to evaluate and rank agents that learn through reinforcement learning (**Bartók, Lake, "Elo Rating for Multi-Agent Evaluation"**, **Bartók, Lake, "Multi-Agent Evaluation via Elo Ratings"**, **Bartók, L, "Rating Agencies: An Exploration of the Elo Rating System in a Multiagent Setting"**). Elo estimates the probability an agent to win another agent. Although it was designed specifically to rank players in the two-player, symmetric constant-sum game, it has been widely applied to other domains. Recently, it has been used for the evaluation of large language models **Radford et al., "Improving Language Understanding by Generative Models"**. However, Elo cannot model intransitive relationships **Gilpin et al., "Explaining Explanations: An Overview of Interpretability of Machine Learning"**, as those in the Rock-Paper-Scissors game, in real-world games **Von Neumann and Morgenstern, "Theory of Games and Economic Behavior"**, and in our game coloring setting. In addition, incorrectness issues have been reported in transitive settings **Bartók and Szegedy, "Multi-Agent Evaluation via Elo Ratings"**. Nash-based evaluation methods, such as Nash Averaging, can be applied in two-player, zero-sum settings (**Myerson, "Game Theory: Analysis of Conflict"**, **Nash, "Non-Cooperative Games"**), but these are not more generally applicable as the Nash equilibrium is intractable to compute and select **Daskalakis et al., "Computing Nash Equilibria: Incremental Methods for Large Games with Few Players"**.

Here, our focus is on (many) agents' long-term interactions in general-sum dynamic settings: Agents need to rank their policy profiles to decide their non-transient individual policy profile, accounting for long-term interactions and payoffs. 

Empirical Game Theory Analysis (EGTA), deploying empirical or meta-games **Bowling et al., "Computing Emergent Behavior in Dynamic Multiagent Environments"** **Myerson, "Game Theory: Analysis of Conflict"** ,  **Boutilier et al., "Decision-Theoretic Planning for Cooperative Agents"**, **Papadimitriou and Yannakakis, "Algorithms for the Church-Turing Thesis"**, can be used to evaluate learning agents that interact in large-scale multiagent systems **Wellman, "Game Theoretic Reasoning About Systems"** , **Bowling et al., "Computing Emergent Behavior in Dynamic Multiagent Environments"**. In our case, aiming at strategy profile rankings, we define the empirical game strategies based on agents' styles of play and train policies realizing these strategies. Then the empirical payoff matrix is estimated by means of game-playing simulations, and is used by the $\alpha$-Rank method to compute strategy profile rankings. $\alpha$-Rank applies to many-player, general-sum games **Daskalakis et al., "Computing Nash Equilibria: Incremental Methods for Large Games with Few Players"**.

%$\alpha$-Rank is inspired by evolutionary game theory models, and applies to many-players, general-sum games [36]. $\alpha$-Rank defines an irreducible Markov chain over strategy set. The ordered masses of this Markov chain’s unique invariant distribution yield the strategy profile rankings. Together with the stationary distribution over strategy profiles, $\alpha-Rank$ provides the strategies' fixation probability function: Both provide a descriptive framework of the empirical game dynamics.