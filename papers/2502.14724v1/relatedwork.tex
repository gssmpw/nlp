\section{Related work}
Evaluation and ranking of learned multi-agent strategies is mainly based on game theoretic concepts, while computational social choice has been proposed, as well \cite{lanctot2023evaluating}.
A predominant approach is the Elo rating system used to evaluate and rank agents that learn through reinforcement learning (\cite{elo1978rating}, \cite{silver2016mastering}, \cite{doi:10.1126/science.aar6404}, \cite{mnih2015human}). Elo estimates the probability an agent to win another agent. Although it was designed specifically to rank players in the two-player, symmetric constant-sum game, it has been widely applied to other domains. Recently, it has been used for the evaluation of large language models \cite{10.5555/3666122.3668142}. However, Elo cannot model intransitive relationships \cite{DBLP:conf/nips/BalduzziTPG18}, as those in the Rock-Paper-Scissors game, in real-world games \cite{NEURIPS2020_ca172e96}, and in our game coloring setting. In addition, incorrectness issues have been reported in transitive settings \cite{pmlr-v206-bertrand23a}. Nash-based evaluation methods, such as Nash Averaging, can be applied in two-player, zero-sum settings (\cite{DBLP:conf/nips/BalduzziTPG18} \cite{10.5555/3237383.3237402}), but these are not more generally applicable as the Nash equilibrium is intractable to compute and select \cite{10.1145/1132516.1132527}.

Here, our focus is on (many) agents' long-term interactions in general-sum dynamic settings: Agents need to rank their policy profiles to decide their non-transient individual policy profile, accounting for long-term interactions and payoffs. 

Empirical Game Theory Analysis (EGTA), deploying empirical or meta-games \cite{10.1007/11575726_8} \cite{tuyls2007evolutionary},  \cite{gerald2002analyzing}, \cite{wellman2006methods}, can be used to evaluate learning agents that interact in large-scale multiagent systems \cite{omidshafiei2019alpharank}, \cite{tuyls2018generalised} \cite{tuyls2018symmetric}. In our case, aiming at strategy profile rankings, we define the empirical game strategies based on agents' styles of play and train policies realizing these strategies. Then the empirical payoff matrix is estimated by means of game-playing simulations, and is used by the $\alpha$-Rank method to compute strategy profile rankings. $\alpha$-Rank applies to many-player, general-sum games \cite{omidshafiei2019alpharank}.

%$\alpha$-Rank is inspired by evolutionary game theory models, and applies to many-players, general-sum games [36]. $\alpha$-Rank defines an irreducible Markov chain over strategy set. The ordered masses of this Markov chainâ€™s unique invariant distribution yield the strategy profile rankings. Together with the stationary distribution over strategy profiles, $\alpha-Rank$ provides the strategies' fixation probability function: Both provide a descriptive framework of the empirical game dynamics.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%