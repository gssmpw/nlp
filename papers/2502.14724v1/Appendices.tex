%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% LaTeX Template for AAMAS-2025 (based on sample-sigconf.tex)
%%% Prepared by the AAMAS-2025 Program Chairs based on the version from AAMAS-2025. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% Start your document with the \documentclass command.


%%% == IMPORTANT ==
%%% Use the first variant below for the final paper (including auithor information).
%%% Use the second variant below to anonymize your submission (no authoir information shown).
%%% For further information on anonymity and double-blind reviewing, 
%%% please consult the call for paper information
%%% https://aamas2025.org/index.php/conference/calls/submission-instructions-main-technical-track/

%%%% For anonymized submission, use this
%\documentclass[sigconf,anonymous]{aamas} 

%%%% For camera-ready, use this
\documentclass[sigconf]{aamas} 


%%% Load required packages here (note that many are included already).

\usepackage{balance} % for balancing columns on the final page
\usepackage{subcaption}
\usepackage{newfloat}
\usepackage{tabularx}
\usepackage{graphicx}

\newcommand{\ak}[1]{\textcolor{red}{AK: [#1]}}

\usepackage{xcolor} 
\usepackage[table]{xcolor}
\usepackage{colortbl}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{amsmath}
\usepackage{enumitem}

\usepackage{amsfonts}

\usepackage{listings}
\DeclareCaptionStyle{ruled}{labelfont=normalfont,labelsep=colon,strut=off}
\lstset{%
    basicstyle={\footnotesize\ttfamily},% Basic style
    numbers=left, 
    numberstyle=\footnotesize, 
    xleftmargin=2em,
    aboveskip=0pt,
    belowskip=0pt,
    showstringspaces=false,
    tabsize=2,
    breaklines=true,
    keywordstyle=\color{blue},         % Set keyword color
    commentstyle=\color{gray}\ttfamily, % Set comment color
    stringstyle=\color{red}            % Set string color
}
\floatstyle{ruled}
\newfloat{listing}{tb}{lst}{}
\floatname{listing}{Listing}

% For table results
\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{float}  % Allows for [H] placement specifier

% For table of figures
\usepackage{graphicx}  % Required for including images

\usepackage{xurl}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% AAMAS-2025 copyright block (do not change!)

\makeatletter
\gdef\@copyrightpermission{
  \begin{minipage}{0.2\columnwidth}
   \href{https://creativecommons.org/licenses/by/4.0/}{\includegraphics[width=0.90\textwidth]{by}}
  \end{minipage}\hfill
  \begin{minipage}{0.8\columnwidth}
   \href{https://creativecommons.org/licenses/by/4.0/}{This work is licensed under a Creative Commons Attribution International 4.0 License.}
  \end{minipage}
  \vspace{5pt}
}
\makeatother

\setcopyright{ifaamas}
\acmConference[AAMAS '25]{Proc.\@ of the 24th International Conference
on Autonomous Agents and Multiagent Systems (AAMAS 2025)}{May 19 -- 23, 2025}
{Detroit, Michigan, USA}{Y.~Vorobeychik, S.~Das, A.~Now√©  (eds.)}
\copyrightyear{2025}
\acmYear{2025}
\acmDOI{}
\acmPrice{}
\acmISBN{}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% == IMPORTANT ==
%%% Use this command to specify your OpenReview submission number.
%%% In anonymous mode, it will be printed on the first page.

\acmSubmissionID{<<845>>}

%%% Use this command to specify the title of your paper.

\title[AAMAS-2025 Formatting Instructions]{Ranking Joint Policies in Dynamic Games using \\Evolutionary Dynamics \\SUPPLEMENTARY MATERIAL}

% Add the subtitle below for an extended abstract
%\subtitle{Extended Abstract}

%%% Provide names, affiliations, and email addresses for all authors.

\author{Natalia Koliou}
\affiliation{
  \institution{University of Piraeus}
  \city{Piraeus}
  \country{Greece}}
\email{nataliakoliou@iit.demokritos.gr}
\orcid{https://orcid.org/0009-0004-3920-9992}

\author{George Vouros}
\affiliation{
  \institution{University of Piraeus}
  \city{Piraeus}
  \country{Greece}}
\email{georgev@unipi.gr}
\orcid{https://orcid.org/0000-0001-5451-622X}

%%% Use this environment to specify a short abstract for your paper.

%\begin{abstract}

%Game-theoretic solution concepts, such as the \emph{Nash equilibrium}, have been key to finding stable joint actions in multi-player games. However, it has been shown that the dynamics of agents' interactions, even in simple two-player games with few strategies, are incapable of reaching \emph{Nash equilibria}, exhibiting complex and unpredictable behavior. Instead, evolutionary approaches can describe the long-term persistence of strategies and filter out transient ones, accounting for the long-term dynamics of agents' interactions. Our goal is to identify agents' joint strategies that result in stable behavior, being resistant to changes, while also accounting for agents' payoffs, in dynamic games. Towards this goal, and building on previous results, this paper proposes transforming dynamic games into their empirical forms by considering agents' strategies instead of agents' actions, and applying the evolutionary methodology \emph{$\alpha$-Rank} to evaluate and rank strategy profiles according to their long-term dynamics. This methodology not only allows us to identify joint strategies that are strong through agents' long-term interactions, but also provides a descriptive, transparent framework regarding the high ranking of these strategies. Experiments report on agents that aim to collaboratively solve a stochastic version of the graph coloring problem. We consider different styles of play as strategies to define the empirical game, and train policies realizing these strategies, using the DQN algorithm. Then we run simulations to generate the payoff matrix required by \emph{$\alpha$-Rank} to rank joint strategies.

%\end{abstract}

%%% The code below was generated by the tool at http://dl.acm.org/ccs.cfm.
%%% Please replace this example with code appropriate for your own paper.


%%% Use this command to specify a few keywords describing your work.
%%% Keywords should be separated by commas.

\keywords{Evolutionary Dynamics, Empirical Games, Stochastic Games, Deep Reinforcement Learning, Ranking Strategy Profiles}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% Include any author-defined commands here.
         
\newcommand{\BibTeX}{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em\TeX}

\newcommand{\natalia}[1]{\textcolor{red}{[NK]: #1}}
\newcommand{\george}[1]{\textcolor{blue}{[GV]: #1}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

%%% The following commands remove the headers in your paper. For final 
%%% papers, these will be inserted during the pagination process.

\pagestyle{fancy}
\fancyhead{}

%%% The next command prints the information defined in the preamble.

\maketitle 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\appendix

\onecolumn

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Empirical Payoff Matrix}
\label{appendix:A}

This is the empirical payoff matrix derived from simulations of the \emph{Graph Coloring Game} using policies trained to adhere to specific styles of play. Each entry in the matrix represents the payoffs of strategies in the corresponding profile, with the first value indicating the payoff of the row strategy and the second value of the column player. The Nash equilibria are highlighted in bold, while nine of the top-ranked strategy profiles in the MCC are shaded in gray.
%
\begin{table*}[h!]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{lccccccccccc}
    \hline
        & A & AE & C & CA & E & I & L & LE & M & W & WL \\
    \hline
    A  & (3.12, 3.11) & (3.15, 3.16) & (3.17, 3.17) & (3.14, 3.17) & (3.16, 3.17) & (3.16, 3.15) & (3.22, 3.13) & (3.19, 3.16) & (3.15, 3.18) & (3.16, 3.17) & (3.21, 3.18) \\
    
    AE & (3.17, 3.17) & (3.11, 3.11) & (3.18, 3.17) & \cellcolor{gray!16}(3.15, 3.17) & (3.17, 3.16) & (3.19, 3.16) & (3.23, 3.12) & (3.19, 3.16) & \cellcolor{gray!16}(3.15, 3.18) & (3.17, 3.17) & (3.20, 3.16) \\
    
    C  & (3.17, 3.16) & (3.16, 3.17) & (3.10, 3.10) & (3.14, 3.17) & (3.15, 3.15) & (3.18, 3.15) & (3.22, 3.12) & (3.17, 3.14) & (3.14, 3.17) & (3.17, 3.16) & (3.20, 3.17) \\
    
    CA & (3.17, 3.15) & (3.17, 3.15) & (3.17, 3.14) & (3.11, 3.11) & (3.18, 3.15) & (3.18, 3.14) & (3.24, 3.13) & \cellcolor{gray!16}(3.21, 3.16) & \cellcolor{gray!16}(3.16, 3.16) & (3.19, 3.16) & \cellcolor{gray!16}(3.22, 3.15) \\
    
    E  & (3.15, 3.16) & (3.16, 3.16) & (3.15, 3.16) & \cellcolor{gray!16}(3.15, 3.17) & (3.10, 3.10) & (3.18, 3.16) & (3.22, 3.12) & (3.19, 3.14) & (3.15, 3.17) & (3.16, 3.17) & (3.19, 3.17) \\
    
    I  & (3.14, 3.16) & (3.16, 3.18) & (3.16, 3.18) & (3.15, 3.19) & (3.16, 3.17) & (3.12, 3.12) & (3.22, 3.14) & (3.18, 3.16) & (3.14, 3.19) & (3.16, 3.18) & (3.19, 3.18) \\

    L  & (3.14, 3.22) & (3.11, 3.22) & (3.12, 3.22) & (3.13, 3.23) & (3.12, 3.22) & (3.13, 3.22) & (3.12, 3.12) & (3.14, 3.20) & (3.11, 3.21) & (3.14, 3.23) & \textbf{(3.15, 3.21)} \\
    
    LE & (3.15, 3.19) & (3.14, 3.18) & (3.14, 3.18) & (3.15, 3.21) & (3.15, 3.19) & (3.16, 3.17) & (3.20, 3.14) & (3.11, 3.11) & (3.14, 3.22) & (3.15, 3.18) & (3.18, 3.19) \\
    
    M  & (3.17, 3.14) & (3.17, 3.15) & (3.17, 3.15) & \cellcolor{gray!16}(3.16, 3.17) & (3.16, 3.14) & (3.18, 3.14) & (3.23, 3.11) & (3.20, 3.14) & (3.06, 3.08) & (3.18, 3.15) & (3.20, 3.16) \\
    
    W  & (3.17, 3.17) & (3.17, 3.18) & (3.16, 3.18) & \cellcolor{gray!16}(3.16, 3.20) & (3.17, 3.17) & (3.18, 3.16) & (3.21, 3.13) & (3.18, 3.15) & (3.15, 3.18) & (3.08, 3.09) & (3.19, 3.15) \\
 
    WL & (3.17, 3.20) & (3.17, 3.19) & (3.17, 3.19) & (3.17, 3.22) & (3.17, 3.19) & (3.18, 3.19) & \textbf{(3.21, 3.15)} & (3.19, 3.17) & \cellcolor{gray!16}(3.16, 3.20) & (3.16, 3.19) & (3.13, 3.13) \\
    \hline
\end{tabular}
}
\vspace{0.5em}
\caption{Empirical Payoff Matrix for the Graph Coloring Game}
\label{tab:gcg_payoff_matrix}
\end{table*}
%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\section{Response Graph}
\label{appendix:B}

These are four response graphs that illustrate the dynamics of strategy profiles in the empirical \emph{Graph Coloring Game} for different $\alpha$ values. Each node in the graph represents a unique strategy profile in the MCC, while the edges indicate transitions between them. The values on the edges show the fixation probabilities normalized by the neutral fixation probability, denoted as $\rho_m$. The nodes and edges are color-coded. Darker blue nodes represent more strong joint profiles, while lighter blue nodes represent transient ones. Similarly, bold arrows suggest a strong advantage in shifting between the nodes, whereas faint ones suggest less of an advantage. 
%
\begin{figure*}[h]
  \centering
  \begin{subfigure}[b]{0.48\linewidth}
    \includegraphics[width=\linewidth]{AAMAS-2025/static/rg_0.4.png}
    \caption{$alpha=0.4$}
    \label{fig:response_graph_0.4}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.48\linewidth}
    \includegraphics[width=\linewidth]{AAMAS-2025/static/rg_1.3.png}
    \caption{$alpha=1.3$}
  \end{subfigure}

  \begin{subfigure}[b]{0.48\linewidth}
    \includegraphics[width=\linewidth]{AAMAS-2025/static/rg_1.9.png}
   \caption{$alpha=1.9$}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.48\linewidth}
    \includegraphics[width=\linewidth]{AAMAS-2025/static/rg_6.4.png}
    \caption{$alpha=6.4$}
  \end{subfigure}

  \caption{Response graphs of strategy profiles' dynamics.}
  \label{fig:response_graphs}
\end{figure*}
%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\section{Experimenting and aggregating rankings using different configurations}
\subsection{Experiments in additional game configurations}
\label{appendix:C1}

Here we show experiments and results with two additional $4 \times 5$ with 10 blocks game configurations, shown in Figure~\ref{fig:grids}. The strategies, the underlying policy model architecture and the setup for training the policy models realizing these strategies are as described in the article. According to the proposed method, for each configuration, we generate the empirical payoff matrix by simulating each strategy profile across multiple games. Finally, we apply the $\alpha$-Rank method to evaluate the performance of strategy profiles.

 %
 \begin{figure}[h]
  \centering
   \begin{subfigure}{0.40\textwidth}
      \centering
      \includegraphics[width=\textwidth]{static/grid1}
      \caption{}
      \label{fig:grid_1}
   \end{subfigure}
   \hspace{0.05\textwidth}
   \begin{subfigure}{0.40\textwidth}
      \centering
      \includegraphics[width=\textwidth]{static/grid2}
      \caption{}
      \label{fig:grid_2}
   \end{subfigure}
   \caption{The $4 \times 5$ grid environments used for the two additional experiments on the graph coloring game.}
   \label{fig:grids}
 \end{figure}
 %

 As observed from the rankings in Tables~\ref{tab:ranking_table_1} and \ref{tab:ranking_table_2}, the strategy profiles that prevail vary between the two configurations. For the first configuration, the top-ranked profile is (C, W) with a score of 0.18 (Table~\ref{tab:ranking_table_1}), while for the second configuration the best-performing profile is (W, CA) with a score of 0.34 (Table~\ref{tab:ranking_table_2}). First, while the top-ranked profiles differ between configurations, both include strategies with cool (C) and warm (W) preferences. This is reasonable, as these preferences inherently guarantee minimal overlap in color selection, reducing conflicts and improving overall performance. The difference in highly ranked profiles can be explained by the blocks with the largest number of neighbors in the different game configurations. Having many blocks with many neighbors, the configuration results to a dense graph: a strategy preferring a large block coloring difficulty (i.e., ``A") is rewarded proportionally to the number of chosen block neighbors, being indifferent in colors chosen. Combined with ``C" (i.e., ``CA") it prefers to color the chosen blocks with cool colors. Therefore, in dense graph configurations, ``CA" will chose blocks that constrain the choice of colors for many other blocks, and will color them with cool colors: This allows ``W" strategy to be more rewarding when coloring the blocks not chosen by ``CA".
 
%  a minimalistic strategy is rewarded by choosing colors already in use, proportionally to their frequency of use, This also explains the second rank of the profile (M,CA) in Figure~\ref{fig:grid_1}, since the extravagant strategy is rewarded when adding new colors, while the minimalistic is rewarded when using any of the colors already used.

 Another pair of strategies that appears among the top 6 best-performing profiles in both configurations are the lazy (L) and ambitious (A) preferences. These strategies complement each other naturally, as a lazy player tends to choose blocks with fewer neighbors, while an ambitious player does not prefer the coloring of such blocks. This combination minimizes the likelihood of selecting the same block, thus preventing penalties due to overlapping choices.

 %
 \begin{table}[h]
     \centering
     \begin{subtable}{0.45\textwidth}
         \centering
         \begin{tabular}{lcc}
             \hline
             \textbf{Profile} & \textbf{Rank} & \textbf{Score} \\
             \hline
             (C, W) & 1 & 0.18 \\
             (M, CA) & 2 & 0.11 \\
             (M, AE) & 3 & 0.10 \\
             (M, C) & 4 & 0.06 \\
             (WL, AE) & 5 & 0.05 \\
             (CA, M) & 6 & 0.05 \\
             \hline
         \end{tabular}
         \caption{}
         \label{tab:ranking_table_1}
     \end{subtable}
     \hspace*{0pt}
     \begin{subtable}{0.45\textwidth}
         \centering
         \begin{tabular}{lcc}
             \hline
             \textbf{Profile} & \textbf{Rank} & \textbf{Score} \\
             \hline
             (W, CA) & 1 & 0.34 \\
             (W, E) & 2 & 0.07 \\
             (M, W) & 3 & 0.05 \\
             (WL, CA) & 4 & 0.04 \\
             (E, W) & 5 & 0.04 \\
             (W, M) & 6 & 0.03 \\
             \hline
         \end{tabular}
         \caption{}
         \label{tab:ranking_table_2}
     \end{subtable}
     \caption{Strategy profiles' rankings for $\alpha=2$}
     \label{tab:ranking_tables}
 \end{table}
 %

 The response graphs for the two configurations, as shown in Figure~\ref{fig:response_graphs}, illustrate the dynamics of strategy profiles in the empirical game. In both cases, the response graphs reveal two strongly connected components, and the top-ranked profiles form the primary components of the MCC: (C, W) in Figure~\ref{fig:response_graph_1} and (W, CA) in Figure~\ref{fig:response_graph_2}. 
 %
 \begin{figure}[h]
  \centering
   \begin{subfigure}{0.47\textwidth}
      \centering
       \includegraphics[width=1\linewidth]{AAMAS-2025/static/rg_6.4_1.png}
       \caption{}
       \label{fig:response_graph_1}
   \end{subfigure}
   \hspace{0.05\textwidth}
   \begin{subfigure}{0.47\textwidth}
      \centering
       \includegraphics[width=1\linewidth]{AAMAS-2025/static/rg_6.4_2.png}
       \caption{}
       \label{fig:response_graph_2}
   \end{subfigure}
   \caption{Response graphs for $\alpha=6.4$.}
   \label{fig:response_graphs}
 \end{figure}
 %

 To further investigate the effect of $\alpha$ on the ranking of profiles, we plot the stationary distribution $\pi$ across all $\alpha$ values, ranging from 0.1 to 3, per experiment (Figure~\ref{fig:alpha_x_pis}).

 \begin{figure*}[h]
   \centering
   \begin{subfigure}[b]{0.49\linewidth}
     \includegraphics[width=\linewidth]{AAMAS-2025/static/alpha_x_pi_3_1.png}
     \caption{}
     \label{fig:alpha_x_pi_3_1}
   \end{subfigure}
   \hfill
   \begin{subfigure}[b]{0.49\linewidth}
     \includegraphics[width=\linewidth]{AAMAS-2025/static/alpha_x_pi_3_2.png}
     \caption{}
     \label{fig:alpha_x_pi_3_2}
   \end{subfigure}
   \caption{Effect of ranking intensity $\alpha \in [0.1, 3]$ on strategy profile mass in the stationary distribution $\pi$.}
   \label{fig:alpha_x_pis}
 \end{figure*}

 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage
\subsection{Cross-configurations policy rankings}
\label{appendix:C2}
%
To determine which strategy profiles perform best across different graph configurations, we aggregate data from all three experiments (the graph configuration shown in Figure \ref{fig:grid_and_graph} and the two configurations in Figure \ref{fig:grids}) and applied the $\alpha$-Rank method. 
Data aggregation here involves taking the empirical payoff matrices produced in the different configurations and computing cell-wise average values. This results into a new ``average" empirical payoff matrix. 
Ranking policy profiles in this way reduces the risk of overfitting to any specific configuration and provides more generalized and reliable rankings. Having done so, the rankings for the top 6 strategy profiles are presented in Table~\ref{tab:ranking_table_3}. 
However, aggregating empirical payoff matrices for different configurations may not be of value in cases these configurations differ largely. This is an aspect that needs more study.

 %
 \begin{table}[h]
   \centering
   \begin{tabular}{lcc}
       \hline
       \textbf{Profile} & \textbf{Rank} & \textbf{Score} \\
       \hline
       (M, CA) & 1 & 0.31 \\
       (W, CA) & 2 & 0.09 \\
       (CA, M) & 3 & 0.09 \\
       (C, W) & 4 & 0.05 \\
       (CA, W) & 5 & 0.04 \\
       (WL, CA) & 6 & 0.03 \\
       \hline
   \end{tabular}
   \vspace{0.5em}
   \caption{Averaged strategy profiles' rankings for $\alpha=2$}
   \label{tab:ranking_table_3}
 \end{table}
%

 The aggregated results highlight that (M, CA) ranks first with a significant score of 0.31. This agrees with the arguments for the rankings in the different configurations discussed in \hyperref[appendix:C1]{Appendix C1}, given also the fact that a minimalistic strategy is rewarded by choosing colors already in use, proportionally to their frequency of use.  We conjecture that this ranking shows that the minimalistic strategy learns to choose blocks that are not adjacent to those chosen by the CA strategy, using colors similar to those used by the CA strategy.
 It is interesting to note that profiles (W, CA), in Table~\ref{tab:ranking_table_2}, (C, W) in Table~\ref{tab:ranking_table_1}, and (WL, CA) in Table~\ref{tab:ranking_table}, are ranked second, fourth, and sixth, respectively, in the aggregated data. This suggests that their performance remains strong across all three configurations, unlike many other strategy profiles. 

 %
 \begin{figure*}[h]
   \centering
   \includegraphics[width=0.65\linewidth]{static/rg_6.4_3.png}
   \caption{Response graph for $\alpha=6.4$}
   \label{fig:response_graph_6.4_3}
 \end{figure*}
 %

 The response graph for the aggregated results (Figure~\ref{fig:response_graph_6.4_3}) illustrates the overall dynamics of the strategy profiles. The top-ranked profile, (M, CA), is highlighted in dark blue, signifying that once the game reaches this MCC, it is unlikely to transition to any other profile. The graph also shows that all 8 profiles form a large connected cluster, with varying degrees of connection between them. 
% 
To further investigate the effect of $\alpha$ on profile dominance, we plotted the stationary distribution $\pi$ across all $\alpha$ values, ranging from 0.1 to 3 (Figure~\ref{fig:alpha_x_pi_3_3}).

 %
 \begin{figure*}[h]
   \centering
   \includegraphics[width=0.85\linewidth]{static/alpha_x_pi_3_3.png}
   \caption{Effect of ranking intensity $\alpha \in [0.1, 3]$ on strategy profile mass in the stationary distribution $\pi$.}
   \label{fig:alpha_x_pi_3_3}
 \end{figure*}
 %

 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 \clearpage
 \section{The Role of Convolutions in Policy Training}
 \label{appendix:D}

We conjecture that the grid-like structure of the environment makes convolutions a natural choice for the policy model architecture: Since the grid includes spatial relationships between blocks, local dependencies, and configurations with multiple blocks, convolutions enable the policy model to efficiently capture these spatial patterns. Thus, incorporating convolutional layers, policy models are enabled to converge more effectively. To validate this claim, we performed an experiment that compares the training of policies with and without convolutional layers. The results are presented in Figure~\ref{fig:conv_vs_noconv}.

Specifically, we trained a policy for the ``warm" strategy using the model architecture with convolutional layers (W), and compared its performance to a ``warm" policy trained with a model of fully connected layers (Wfc). Both policies are trained on the same graph configuration, and their performance is evaluated based on the loss curves. The results show that the policy with convolutional layers converges more quickly, suggesting that convolutions enable the model to learn the spatial relationships between blocks more efficiently. It must be noted that the Wfc curve picks at 2.27, while the W at 0.89. While both policies eventually reach the same level of performance (with the loss after 10,000 epochs converging to the same low point), the model with the convolutional layers achieves this more effectively.

 %
 \begin{figure}[h]
   \centering
   \includegraphics[width=0.6\linewidth]{static/loss_WWfc.png}
   \caption{Training efficiency comparison: W uses convolutional layers (orange) and Wfc fully connected layers (purple).}
   \label{fig:conv_vs_noconv}
 \end{figure}
 

\end{document}