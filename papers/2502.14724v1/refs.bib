@article{omidshafiei2019alpharank,
  title={$\alpha$-rank: Multi-agent evaluation by evolution},
  author={Omidshafiei, Shayegan and Papadimitriou, Christos and Piliouras, Georgios and Tuyls, Karl and Rowland, Mark and Lespiau, Jean-Baptiste and Czarnecki, Wojciech M and Lanctot, Marc and Perolat, Julien and Munos, Remi},
  journal={Scientific reports},
  volume={9},
  number={1},
  pages={9937},
  year={2019},
  publisher={Nature Publishing Group UK London}
}


@article{silver2016mastering,
author = {Silver, David and Huang, Aja and Maddison, Christopher and Guez, Arthur and Sifre, Laurent and Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and Dieleman, Sander and Grewe, Dominik and Nham, John and Kalchbrenner, Nal and Sutskever, Ilya and Lillicrap, Timothy and Leach, Madeleine and Kavukcuoglu, Koray and Graepel, Thore and Hassabis, Demis},
year = {2016},
month = {01},
pages = {484-489},
title = {Mastering the game of Go with deep neural networks and tree search},
volume = {529},
journal = {Nature},
doi = {10.1038/nature16961}
}

@inproceedings{10.1609/aaai.v33i01.33014561,
author = {Annasamy, Raghuram Mandyam and Sycara, Katia},
title = {Towards better interpretability in deep Q-networks},
year = {2019},
isbn = {978-1-57735-809-1},
publisher = {AAAI Press},
url = {https://doi.org/10.1609/aaai.v33i01.33014561},
doi = {10.1609/aaai.v33i01.33014561},
abstract = {Deep reinforcement learning techniques have demonstrated superior performance in a wide variety of environments. As improvements in training algorithms continue at a brisk pace, theoretical or empirical studies on understanding what these networks seem to learn, are far behind. In this paper we propose an interpretable neural network architecture for Q-learning which provides a global explanation of the model's behavior using key-value memories, attention and reconstructible em-beddings. With a directed exploration strategy, our model can reach training rewards comparable to the state-of-the-art deep Q-learning models. However, results suggest that the features extracted by the neural network are extremely shallow and subsequent testing using out-of-sample examples shows that the agent can easily overfit to trajectories seen during training.},
booktitle = {Proceedings of the Thirty-Third AAAI Conference on Artificial Intelligence and Thirty-First Innovative Applications of Artificial Intelligence Conference and Ninth AAAI Symposium on Educational Advances in Artificial Intelligence},
articleno = {560},
numpages = {9},
location = {Honolulu, Hawaii, USA},
series = {AAAI'19/IAAI'19/EAAI'19}
}

@article{LIU2019272,
title = {Deep Learning-based Human Motion Prediction considering Context Awareness for Human-Robot Collaboration in Manufacturing},
journal = {Procedia CIRP},
volume = {83},
pages = {272-278},
year = {2019},
note = {11th CIRP Conference on Industrial Product-Service Systems},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2019.04.080},
url = {https://www.sciencedirect.com/science/article/pii/S2212827119306948},
author = {Zitong Liu and Quan Liu and Wenjun Xu and Zhihao Liu and Zude Zhou and Jie Chen},
keywords = {Disassembly, Vision System, Action Prediction, Machine Learning},
abstract = {The interest of human-robot collaboration (HRC) for intelligent manufacturing service system is gradually increasing. Fluent human-robot coexistence in manufacturing requires accurate estimation of the human motion intention so that the efficiency and safety of HRC can be guaranteed. Human motion is mainly defined as the sequential positions of the joints of human skeletons among traditional motion prediction solutions, which lead to a deficiency of tools or product components holding in hand. Context awareness based temporal processing is the key to evaluating human motion before the accomplishment of it, so as to save time as well as recognize the intention of the human. In this paper, a deep learning system combing convolutional neural network (CNN) and long short-term memory network (LSTM) towards vision signals is explored to predict human motion accurately. Creatively, this paper utilizes LSTM to extract temporal patterns of human motion automatically outputting the prediction result before motion takes place. Not only does it avoid complex feature extraction due to its end-to-end characteristic, but provide a natural interaction between human and robot without wearable devices or tags that may become a burden for the former. A case study of desktop computer product disassembly is executed to demonstrate the feasibility of the recommended method. Experimental performance proves that our method outperforms the other three optimization algorithms on the prediction accuracy.}
}

@misc{zhang2024opensourcereproduciblechessrobot,
      title={An Open-Source Reproducible Chess Robot for Human-Robot Interaction Research}, 
      author={Renchi Zhang and Joost de Winter and Dimitra Dodou and Harleigh Seyffert and Yke Bauke Eisma},
      year={2024},
      eprint={2405.18170},
      archivePrefix={arXiv},
      primaryClass={cs.RO},
      url={https://arxiv.org/abs/2405.18170}, 
}

@misc{shafti2020realworldhumanrobotcollaborativereinforcement,
      title={Real-World Human-Robot Collaborative Reinforcement Learning}, 
      author={Ali Shafti and Jonas Tjomsland and William Dudley and A. Aldo Faisal},
      year={2020},
      eprint={2003.01156},
      archivePrefix={arXiv},
      primaryClass={cs.RO},
      url={https://arxiv.org/abs/2003.01156}, 
}

@inproceedings{Dagioglou2021TheSO,
  title={The sense of agency during Human-Agent Collaboration},
  author={Maria Dagioglou and Vangelis Karkaletsis},
  year={2021},
  url={https://api.semanticscholar.org/CorpusID:244770831}
}

@inproceedings{10.1145/3434074.3447168,
author = {Yang, Boling and Xie, Xiangyu and Habibi, Golnaz and Smith, Joshua R.},
title = {Competitive Physical Human-Robot Game Play},
year = {2021},
isbn = {9781450382908},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3434074.3447168},
doi = {10.1145/3434074.3447168},
abstract = {While competitive games have been studied extensively in the AI community for benchmarking purposes, there has only been limited discussion of human interaction with embodied agents under competitive settings. In this work, we aim to motivate research in competitive human-robot interaction (competitive-HRI) by discussing how human users can benefit from robot competitors. We then examine the concepts from game AI that we can adopt for competitive-HRI. Based on these discussions, we propose a robotic system that is designed to support future competitive-HRI research. A human-robot fencing game is also proposed to evaluate a robot's capability in competitive-HRI scenarios. Finally, we present the initial experimental results and discuss possible future research directions.},
booktitle = {Companion of the 2021 ACM/IEEE International Conference on Human-Robot Interaction},
pages = {242–246},
numpages = {5},
keywords = {artificial intelligence for games, competitive-HRI, human-robot interaction, reinforcement learning},
location = {Boulder, CO, USA},
series = {HRI '21 Companion}
}

@misc{shafti2019gazebasedcontextawareroboticassisted,
      title={Gaze-based, Context-aware Robotic System for Assisted Reaching and Grasping}, 
      author={Ali Shafti and Pavel Orlov and A. Aldo Faisal},
      year={2019},
      eprint={1809.08095},
      archivePrefix={arXiv},
      primaryClass={cs.RO},
      url={https://arxiv.org/abs/1809.08095}, 
}

@article{10.5555/2831071.2831085,
author = {Bloembergen, Daan and Tuyls, Karl and Hennes, Daniel and Kaisers, Michael},
title = {Evolutionary dynamics of multi-agent learning: a survey},
year = {2015},
issue_date = {May 2015},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {53},
number = {1},
issn = {1076-9757},
abstract = {The interaction of multiple autonomous agents gives rise to highly dynamic and nondeterministic environments, contributing to the complexity in applications such as automated financial markets, smart grids, or robotics. Due to the sheer number of situations that may arise, it is not possible to foresee and program the optimal behaviour for all agents beforehand. Consequently, it becomes essential for the success of the system that the agents can learn their optimal behaviour and adapt to new situations or circumstances. The past two decades have seen the emergence of reinforcement learning, both in single and multi-agent settings, as a strong, robust and adaptive learning paradigm. Progress has been substantial, and a wide range of algorithms are now available. An important challenge in the domain of multi-agent learning is to gain qualitative insights into the resulting system dynamics. In the past decade, tools and methods from evolutionary game theory have been successfully employed to study multi-agent learning dynamics formally in strategic interactions. This article surveys the dynamical models that have been derived for various multi-agent reinforcement learning algorithms, making it possible to study and compare them qualitatively. Furthermore, new learning algorithms that have been introduced using these evolutionary game theoretic tools are reviewed. The evolutionary models can be used to study complex strategic interactions. Examples of such analysis are given for the domains of automated trading in stock markets and collision avoidance in multi-robot systems. The paper provides a roadmap on the progress that has been achieved in analysing the evolutionary dynamics of multi-agent learning by highlighting the main results and accomplishments.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {659–697},
numpages = {39}
}

@misc{paul2022multiagentpathfinding,
      title={Multi Agent Path Finding using Evolutionary Game Theory}, 
      author={Sheryl Paul and Jyotirmoy V. Deshmukh},
      year={2022},
      eprint={2212.02010},
      archivePrefix={arXiv},
      primaryClass={cs.MA},
      url={https://arxiv.org/abs/2212.02010}, 
}

@article{David_2014,
   title={Genetic Algorithms for Evolving Computer Chess Programs},
   volume={18},
   ISSN={1941-0026},
   url={http://dx.doi.org/10.1109/TEVC.2013.2285111},
   DOI={10.1109/tevc.2013.2285111},
   number={5},
   journal={IEEE Transactions on Evolutionary Computation},
   publisher={Institute of Electrical and Electronics Engineers (IEEE)},
   author={David, Omid E. and van den Herik, H. Jaap and Koppel, Moshe and Netanyahu, Nathan S.},
   year={2014},
   month=oct, pages={779–789} }

@inproceedings{10.1145/3434074.3447220,
author = {Teh, Nicholas and Hu, Shuyue and Soh, Harold},
title = {A Theoretical Framework for Large-Scale Human-Robot Interaction with Groups of Learning Agents},
year = {2021},
isbn = {9781450382908},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3434074.3447220},
doi = {10.1145/3434074.3447220},
abstract = {Recent advances in robot capabilities have led to a growing consensus that robots will eventually be deployed at scale across numerous application domains. An important open question is how humans and robots will adapt to one another over time. In this paper, we introduce the model-based Theoretical Human-Robot Scenarios (THuS) framework, capable of elucidating the interactions between large groups of humans and learning robots. We formally establish THuS, and consider its application to a human-robot variant of the n-player coordination game, demonstrating the power of the theoretical framework as a tool to qualitatively understand and quantitatively compare HRI scenarios that involve different agent types. We also discuss the framework's limitations and potential. Our work provides the HRI community with a versatile tool that permits first-cut insights into large-scale HRI scenarios that are too costly or challenging to carry out in simulations or in the real-world.},
booktitle = {Companion of the 2021 ACM/IEEE International Conference on Human-Robot Interaction},
pages = {489–493},
numpages = {5},
keywords = {multi-agent learning, human-robot interaction, game theory},
location = {Boulder, CO, USA},
series = {HRI '21 Companion}
}

@misc{watkins2023generating,
    title={Generating a Graph Colouring Heuristic with Deep Q-Learning and Graph Neural Networks}, 
    author={George Watkins and Giovanni Montana and Juergen Branke},
    year={2023},
    eprint={2304.04051},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@book{Shoham_Leyton-Brown_2008,
    place={Cambridge}, 
    title={Multiagent Systems: Algorithmic, Game-Theoretic, and Logical Foundations},
    publisher={Cambridge University Press}, 
    author={Shoham, Yoav and Leyton-Brown, Kevin}, 
    year={2008}
}

@misc{vanhasselt2015deep,
    title={Deep Reinforcement Learning with Double Q-learning}, 
    author={Hado van Hasselt and Arthur Guez and David Silver},
    year={2015},
    eprint={1509.06461},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@article{10.1007/BF00992699,
author = {Lin, Long-Ji},
title = {Self-Improving Reactive Agents Based on Reinforcement Learning, Planning and Teaching},
year = {1992},
issue_date = {May 1992},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {8},
number = {3–4},
issn = {0885-6125},
url = {https://doi.org/10.1007/BF00992699},
doi = {10.1007/BF00992699},
journal = {Mach. Learn.},
month = {may},
pages = {293–321},
numpages = {29},
keywords = {Reinforcement learning, connectionist networks, planning, teaching}
}

@inproceedings{Levet2016GameT,
  title={Game Theory : Normal Form Games},
  author={Michael Levet},
  year={2016},
  url={https://api.semanticscholar.org/CorpusID:131771375}
}

@article{Shapley1953StochasticG,
  title={Stochastic Games*},
  author={Lloyd S. Shapley},
  journal={Proceedings of the National Academy of Sciences},
  year={1953},
  volume={39},
  pages={1095 - 1100},
  url={https://api.semanticscholar.org/CorpusID:263414073}
}

@misc{wellman2024empiricalgametheoreticanalysissurvey,
      title={Empirical Game-Theoretic Analysis: A Survey}, 
      author={Michael P. Wellman and Karl Tuyls and Amy Greenwald},
      year={2024},
      eprint={2403.04018},
      archivePrefix={arXiv},
      primaryClass={cs.GT},
      url={https://arxiv.org/abs/2403.04018}, 
}

####################################################################
@article{Vouros_2022,
   title={Explainable Deep Reinforcement Learning: State of the Art and Challenges},
   volume={55},
   ISSN={1557-7341},
   url={http://dx.doi.org/10.1145/3527448},
   DOI={10.1145/3527448},
   number={5},
   journal={ACM Computing Surveys},
   publisher={Association for Computing Machinery (ACM)},
   author={Vouros, George A.},
   year={2022},
   month=dec, pages={1–39} 
}

@article{BORGERS19971,
title = {Learning Through Reinforcement and Replicator Dynamics},
journal = {Journal of Economic Theory},
volume = {77},
number = {1},
pages = {1-14},
year = {1997},
issn = {0022-0531},
doi = {https://doi.org/10.1006/jeth.1997.2319},
url = {https://www.sciencedirect.com/science/article/pii/S002205319792319X},
author = {Tilman Börgers and Rajiv Sarin},
abstract = {This paper considers a version of R. R. Bush and F. Mosteller's (1951,Psych. Rev.58, 313–323; 1955, “Stochastic Models for Learning,” Wiley, New York) stochastic learning theory in the context of games. We show that in a continuous time limit the learning model converges to the replicator dynamics of evolutionary game theory. Thus we provide a non-biological interpretation of evolutionary game theory.Journal of Economic LiteratureClassification Numbers: C72, D83.}
}

@article{Szab__2007,
   title={Evolutionary games on graphs},
   volume={446},
   ISSN={0370-1573},
   url={http://dx.doi.org/10.1016/j.physrep.2007.04.004},
   DOI={10.1016/j.physrep.2007.04.004},
   number={4–6},
   journal={Physics Reports},
   publisher={Elsevier BV},
   author={Szabó, György and Fáth, Gábor},
   year={2007},
   month=jul, pages={97–216} 
}


@book{elo1978rating,
  title={The Rating of Chessplayers, Past and Present},
  author={Elo, A.E.},
  isbn={9780668047210},
  lccn={78024077},
  url={https://books.google.gr/books?id=8pMnAQAAMAAJ},
  year={1978},
  publisher={Arco Pub.}
}

@article{
doi:10.1126/science.aar6404,
author = {David Silver  and Thomas Hubert  and Julian Schrittwieser  and Ioannis Antonoglou  and Matthew Lai  and Arthur Guez  and Marc Lanctot  and Laurent Sifre  and Dharshan Kumaran  and Thore Graepel  and Timothy Lillicrap  and Karen Simonyan  and Demis Hassabis },
title = {A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play},
journal = {Science},
volume = {362},
number = {6419},
pages = {1140-1144},
year = {2018},
doi = {10.1126/science.aar6404},
URL = {https://www.science.org/doi/abs/10.1126/science.aar6404},
eprint = {https://www.science.org/doi/pdf/10.1126/science.aar6404},
abstract = {Computers can beat humans at increasingly complex games, including chess and Go. However, these programs are typically constructed for a particular game, exploiting its properties, such as the symmetries of the board on which it is played. Silver et al. developed a program called AlphaZero, which taught itself to play Go, chess, and shogi (a Japanese version of chess) (see the Editorial, and the Perspective by Campbell). AlphaZero managed to beat state-of-the-art programs specializing in these three games. The ability of AlphaZero to adapt to various game rules is a notable step toward achieving a general game-playing system. Science, this issue p. 1140; see also pp. 1087 and 1118 AlphaZero teaches itself to play three different board games and beats state-of-the-art programs in each. The game of chess is the longest-studied domain in the history of artificial intelligence. The strongest programs are based on a combination of sophisticated search techniques, domain-specific adaptations, and handcrafted evaluation functions that have been refined by human experts over several decades. By contrast, the AlphaGo Zero program recently achieved superhuman performance in the game of Go by reinforcement learning from self-play. In this paper, we generalize this approach into a single AlphaZero algorithm that can achieve superhuman performance in many challenging games. Starting from random play and given no domain knowledge except the game rules, AlphaZero convincingly defeated a world champion program in the games of chess and shogi (Japanese chess), as well as Go.}}

@inproceedings{DBLP:conf/nips/BalduzziTPG18,
  author       = {David Balduzzi and
                  Karl Tuyls and
                  Julien P{\'{e}}rolat and
                  Thore Graepel},
  editor       = {Samy Bengio and
                  Hanna M. Wallach and
                  Hugo Larochelle and
                  Kristen Grauman and
                  Nicol{\`{o}} Cesa{-}Bianchi and
                  Roman Garnett},
  title        = {Re-evaluating evaluation},
  booktitle    = {Advances in Neural Information Processing Systems 31: Annual Conference
                  on Neural Information Processing Systems 2018, NeurIPS 2018, December
                  3-8, 2018, Montr{\'{e}}al, Canada},
  pages        = {3272--3283},
  year         = {2018},
  url          = {https://proceedings.neurips.cc/paper/2018/hash/cdf1035c34ec380218a8cc9a43d438f9-Abstract.html},
  timestamp    = {Mon, 16 May 2022 15:41:51 +0200},
  biburl       = {https://dblp.org/rec/conf/nips/BalduzziTPG18.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{10.5555/3237383.3237402,
author = {Tuyls, Karl and Perolat, Julien and Lanctot, Marc and Leibo, Joel Z. and Graepel, Thore},
title = {A Generalised Method for Empirical Game Theoretic Analysis},
year = {2018},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {This paper provides theoretical bounds for empirical game theoretical analysis of complex multi-agent interactions. We provide insights in the empirical meta game showing that a Nash equilibrium of the meta-game is an approximate Nash equilibrium of the true underlying game. We investigate and show how many data samples are required to obtain a close enough approximation of the underlying game. Additionally, we extend the meta-game analysis methodology to asymmetric games. The state-of-the-art has only considered empirical games in which agents have access to the same strategy sets and the payoff structure is symmetric, implying that agents are interchangeable. Finally, we carry out an empirical illustration of the generalised method in several domains, illustrating the theory and evolutionary dynamics of several versions of the AlphaGo algorithm (symmetric), the dynamics of the Colonel Blotto game played by human players on Facebook (symmetric), and an example of a meta-game in Leduc Poker (asymmetric), generated by the PSRO multi-agent learning algorithm.},
booktitle = {Proceedings of the 17th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {77–85},
numpages = {9},
keywords = {replicator dynamics, empirical games, asymmetric games},
location = {Stockholm, Sweden},
series = {AAMAS '18}
}

@inproceedings{10.1145/1132516.1132527,
author = {Daskalakis, Constantinos and Goldberg, Paul W. and Papadimitriou, Christos H.},
title = {The complexity of computing a Nash equilibrium},
year = {2006},
isbn = {1595931341},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1132516.1132527},
doi = {10.1145/1132516.1132527},
abstract = {We resolve the question of the complexity of Nash equilibrium by showing that the problem of computing a Nash equilibrium in a game with 4 or more players is complete for the complexity class PPAD. Our proof uses ideas from the recently-established equivalence between polynomial time solvability of normal form games and graphical games, establishing that these kinds of games can simulate a PPAD-complete class of Brouwer functions.},
booktitle = {Proceedings of the Thirty-Eighth Annual ACM Symposium on Theory of Computing},
pages = {71–78},
numpages = {8},
keywords = {Nash equilibrium, PPAD-completeness, complexity, game theory},
location = {Seattle, WA, USA},
series = {STOC '06}
}


@inproceedings{10.5555/3666122.3668142,
author = {Zheng, Lianmin and Chiang, Wei-Lin and Sheng, Ying and Zhuang, Siyuan and Wu, Zhanghao and Zhuang, Yonghao and Lin, Zi and Li, Zhuohan and Li, Dacheng and Xing, Eric P. and Zhang, Hao and Gonzalez, Joseph E. and Stoica, Ion},
title = {Judging LLM-as-a-judge with MT-bench and Chatbot Arena},
year = {2023},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Evaluating large language model (LLM) based chat assistants is challenging due to their broad capabilities and the inadequacy of existing benchmarks in measuring human preferences. To address this, we explore using strong LLMs as judges to evaluate these models on more open-ended questions. We examine the usage and limitations of LLM-as-a-judge, including position, verbosity, and self-enhancement biases, as well as limited reasoning ability, and propose solutions to mitigate some of them. We then verify the agreement between LLM judges and human preferences by introducing two benchmarks: MT-bench, a multi-turn question set; and Chatbot Arena, a crowdsourced battle platform. Our results reveal that strong LLM judges like GPT-4 can match both controlled and crowdsourced human preferences well, achieving over 80\% agreement, the same level of agreement between humans. Hence, LLM-as-a-judge is a scalable and explainable way to approximate human preferences, which are otherwise very expensive to obtain. Additionally, we show our benchmark and traditional benchmarks complement each other by evaluating several variants of LLaMA and Vicuna. The MT-bench questions, 3K expert votes, and 30K conversations with human preferences are publicly available at https://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge.},
booktitle = {Proceedings of the 37th International Conference on Neural Information Processing Systems},
articleno = {2020},
numpages = {29},
location = {New Orleans, LA, USA},
series = {NIPS '23}
}

@inproceedings{NEURIPS2020_ca172e96,
 author = {Czarnecki, Wojciech M. and Gidel, Gauthier and Tracey, Brendan and Tuyls, Karl and Omidshafiei, Shayegan and Balduzzi, David and Jaderberg, Max},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {17443--17454},
 publisher = {Curran Associates, Inc.},
 title = {Real World Games Look Like Spinning Tops},
 url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/ca172e964907a97d5ebd876bfdd4adbd-Paper.pdf},
 volume = {33},
 year = {2020}
}


@article{mnih2015human,
  title={Human-level control through deep reinforcement learning},
  author={Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A and Veness, Joel and Bellemare, Marc G and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K and Ostrovski, Georg and others},
  journal={nature},
  volume={518},
  number={7540},
  pages={529--533},
  year={2015},
  publisher={Nature Publishing Group UK London}
}


@InProceedings{pmlr-v206-bertrand23a,
  title = 	 {On the Limitations of the Elo, Real-World Games are Transitive, not Additive},
  author =       {Bertrand, Quentin and Czarnecki, Wojciech Marian and Gidel, Gauthier},
  booktitle = 	 {Proceedings of The 26th International Conference on Artificial Intelligence and Statistics},
  pages = 	 {2905--2921},
  year = 	 {2023},
  editor = 	 {Ruiz, Francisco and Dy, Jennifer and van de Meent, Jan-Willem},
  volume = 	 {206},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {25--27 Apr},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v206/bertrand23a/bertrand23a.pdf},
  url = 	 {https://proceedings.mlr.press/v206/bertrand23a.html},
  abstract = 	 {The Elo score has been extensively used to rank players by their skill or strength in competitive games such as chess, go, or StarCraft II. The Elo score implicitly assumes games have a strong additive—hence transitive—component. In this paper, we investigate the challenge of identifying transitive components in games. As a starting point, we show that the Elo score provably fails to extract the transitive component of some elementary transitive games. Based on this observation, we propose an alternative ranking system which properly extracts the transitive components in these games. Finally, we conduct an in-depth empirical validation on real-world game payoff matrices: it shows significant prediction performance improvements compared to the Elo score.}
}

@inproceedings{10.1007/11575726_8,
author = {Phelps, Steve and Parsons, Simon and McBurney, Peter},
title = {An evolutionary game-theoretic comparison of two double-auction market designs},
year = {2004},
isbn = {3540297375},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11575726_8},
doi = {10.1007/11575726_8},
abstract = {In this paper we describe an analysis of two double auction markets—the clearing house auction and the continuous double auction. The complexity of these institutions is such that they defy analysis using traditional game-theoretic techniques, and so we use heuristic-strategy approximation to provide an approximated game-theoretic analysis. As well as finding heuristic-strategy equilibria for these mechanisms, we subject them to an evolutionary game-theoretic analysis which allows us to quantify which equilibria are more likely to occur. We then weight the design objectives for each mechanism according to the probability distribution over equilibria, which allows us to provide more realistic estimates for the efficiency of each mechanism.},
booktitle = {Proceedings of the 6th AAMAS International Conference on Agent-Mediated Electronic Commerce: Theories for and Engineering of Distributed Mechanisms and Systems},
pages = {101–114},
numpages = {14},
location = {New York, NY},
series = {AAMAS'04}
}

@article{tuyls2007evolutionary,
  title={What evolutionary game theory tells us about multiagent learning},
  author={Tuyls, Karl and Parsons, Simon},
  journal={Artificial Intelligence},
  volume={171},
  number={7},
  pages={406--416},
  year={2007},
  publisher={Elsevier}
}

@article{gerald2002analyzing,
  title={Analyzing Complex Strategic Interactions in Multi-Agent Systems},
  author={Gerald, William E Walsh Rajarshi Das and Kephart, Tesauro Jeffrey O},
  year={2002}
}

@inproceedings{wellman2006methods,
  title={Methods for empirical game-theoretic analysis},
  author={Wellman, Michael P},
  booktitle={AAAI},
  volume={980},
  pages={1552--1556},
  year={2006}
}

@article{tuyls2018generalised,
  title={A generalised method for empirical game theoretic analysis},
  author={Tuyls, Karl and Perolat, Julien and Lanctot, Marc and Leibo, Joel Z and Graepel, Thore},
  journal={arXiv preprint arXiv:1803.06376},
  year={2018}
}

@article{tuyls2018symmetric,
  title={Symmetric decomposition of asymmetric games},
  author={Tuyls, Karl and P{\'e}rolat, Julien and Lanctot, Marc and Ostrovski, Georg and Savani, Rahul and Leibo, Joel Z and Ord, Toby and Graepel, Thore and Legg, Shane},
  journal={Scientific reports},
  volume={8},
  number={1},
  pages={1015},
  year={2018},
  publisher={Nature Publishing Group UK London}
}

@article{lanctot2023evaluating,
  title={Evaluating Agents using Social Choice Theory},
  author={Lanctot, Marc and Larson, Kate and Bachrach, Yoram and Marris, Luke and Li, Zun and Bhoopchand, Avishkar and Anthony, Thomas and Tanner, Brian and Koop, Anna},
  journal={arXiv preprint arXiv:2312.03121},
  year={2023}
}