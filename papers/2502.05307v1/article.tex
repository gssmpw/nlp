%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
%\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
  
% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
%\usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025} %PREPRINT

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Training Set Reconstruction from Differentially Private Forests: How Effective is DP?}

% AUTHOR IMPORTS AND MACROS
\usepackage{xcolor}
\usepackage{amsfonts}
\usepackage{dsfont}
\usepackage{subcaption}
\usepackage{multirow}
\usepackage{stmaryrd}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{soul}
%\usepackage{enumitem}
\begin{document}
\include{macros}
\allowdisplaybreaks

\twocolumn[

\icmltitle{Training Set Reconstruction from Differentially Private Forests:\\ How Effective is DP?}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Alice Gorgé}{xxx}
\icmlauthor{Julien Ferry}{yyy}
\icmlauthor{Sébastien Gambs}{zzz}
\icmlauthor{Thibaut Vidal}{yyy}
\end{icmlauthorlist}

\icmlaffiliation{yyy}{CIRRELT \& SCALE-AI Chair in Data-Driven Supply Chains, Department of Mathematics and Industrial Engineering, Polytechnique Montréal, Canada}
\icmlaffiliation{xxx}{École Polytechnique, Paris, France}
\icmlaffiliation{zzz}{Université du Québec à Montréal, Canada}
%\icmlaffiliation{ttt}{Department of XXX, University of YYY, Location, Country}

\icmlcorrespondingauthor{Thibaut Vidal}{thibaut.vidal@polymtl.ca}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%PREPRINT
\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution

\begin{abstract}
Recent research has shown that machine learning models are vulnerable to privacy attacks targeting their training data. 
Differential privacy (DP) has become a widely adopted countermeasure, as it offers rigorous privacy protections.  
In this paper, we introduce a reconstruction attack targeting state-of-the-art $\varepsilon$-DP random forests. 
By leveraging a constraint programming model that incorporates knowledge of the forest's structure and DP mechanism characteristics, our approach formally reconstructs the \emph{most likely} dataset that could have produced a given forest.
Through extensive computational experiments, we examine the interplay between model utility, privacy guarantees, and reconstruction accuracy across various configurations.
Our results reveal that random forests trained with meaningful DP guarantees can still leak substantial portions of their training data. 
Specifically, while DP reduces the success of reconstruction attacks, the only forests fully robust to our attack exhibit predictive performance no better than a constant classifier.
Building on these insights, we provide practical recommendations for the construction of DP random forests that are more resilient to reconstruction attacks and maintain non-trivial predictive performance.
\end{abstract}


\section{Introduction}

The success of machine learning algorithms in critical applications, such as medicine, kidney attribution and credit scoring \citep{dildar2021skin,DBLP:conf/aaai/0001CDM21,DBLP:journals/asc/DastileCP20}, hinges on access to large volumes of sensitive data, including medical records, personal histories and financial information. 
This extensive data is crucial for training models capable of achieving high accuracy. 
At the same time, trustworthiness in these applications demands transparency, as emphasized by regulations like Article 13 of the EU AI Act\footnote{\href{https://artificialintelligenceact.eu/article/13/}{https://artificialintelligenceact.eu/article/13/}}. 
Transparency involves the ability to audit models and explain their outputs and inner workings, which fosters public confidence and ensures compliance with legal requirements.

However, transparency can inadvertently create vulnerabilities. When models are made public, even via black-box APIs, they become susceptible to privacy attacks that can infer sensitive information about their training data~\citep{rigaki2023}. 
Recent studies on reconstruction attacks~\citep{dwork2017exposed} demonstrated that large language models~\citep{carlini2021}, diffusion models~\citep{DBLP:conf/uss/CarliniHNJSTBIW23}, neural networks~\citep{DBLP:conf/nips/HaimVYSI22}, random forests (RFs) and a wide range of interpretable models~\citep{ferry2024probabilistic,ferry2024trained} inherently leak part of their training data through their structure or outputs.

To mitigate such privacy risks, \emph{differential privacy} (DP)~\cite{dwork2014algorithmic} has emerged as the \emph{de facto} standard, as it limits the amount of information an adversary can gain regarding any single individual in the dataset. 
DP theoretically bounds the success of membership inference attacks (which aim at inferring the absence or presence of a given individual in a model's training data), but also empirically provides a form of protection against other attacks. 
For instance, recent work on deep learning for medical imaging~\cite{ziller2024reconciling} has shown that DP mitigates the success of some reconstruction attacks, even for loose privacy guarantees. 
However, DP imposes significant trade-offs as ensuring privacy involves adding noise or other modifications that degrade a model's predictive performance. 
Consequently, practitioners must carefully balance privacy guarantees and model utility in sensitive domains.

In this paper, we focus on reconstruction attacks targeting tree ensembles, such as RFs. 
These models frequently achieve state-of-the-art performance on tabular data, but recent research~\citep{ferry2024trained} has revealed that their structure can inherently expose (nearly) all of their training data. 
While several DP tree-based models have been designed~\citep{fletcher2019decision}, it remains unclear whether DP can effectively prevent such privacy leaks. 
To address this gap, the main contributions of our study are as follows:
\begin{itemize}
    \item We introduce an effective reconstruction attack against state-of-the-art DP RFs.
    Building on the attack paradigm pioneered by~\citep{ferry2024trained}, our approach frames the reconstruction attack as the solution of a Constraint Programming (CP) model, which identifies a \emph{most likely} dataset capable of generating the observed forest and noisy counts.
    We demonstrate that this method can retrieve significant portions of RFs' training data, even under tight privacy budgets. 
    \item Through extensive experiments, we show that DP RFs, while formally satisfying privacy guarantees, can still leak substantial portions of their training data. 
    In particular, our findings reveal that \textbf{all the considered DP RFs with non-trivial predictive performance remain vulnerable to our reconstruction attack}. 
    The source code of our method, permitting the reproducibility of all the experiments, is openly available at \url{https://github.com/vidalt/DRAFT-DP}, under a MIT license.
    \item We provide key insights and practical recommendations regarding the factors influencing the trade-offs between DP RFs' predictive performance and their vulnerability to reconstruction attacks. 
    These insights aim to guide practitioners in effectively deploying DP RFs in real-world scenarios. 
\end{itemize}

\section{Technical Background}
\label{sec:preliminaries}

\subsection{Differential Privacy}\label{sec:dp}

Originally formalized in~\citet{dwork2006differential}, DP enables the computation of useful statistics on private data while strictly limiting the information that can be inferred about any single individual.  
A randomized mechanism \(\mathcal{K}\) satisfies \( \varepsilon \)-DP if, for any pair of datasets \( \dataset_1 \) and \( \dataset_2 \) differing by at most one record, and all \( U \subseteq \text{Range}(\mathcal{K}) \):
\[
\mathbb{P}(\mathcal{K}(\dataset_1) \in U) \leq e^\varepsilon \times \mathbb{P}(\mathcal{K}(\dataset_2) \in U).
\]
The parameter \( \varepsilon \), called the privacy budget, quantifies the trade-off between privacy and utility. 
Smaller values of \( \varepsilon \) ensure stronger privacy at the expense of reduced utility.

Let \(\query\) represent a deterministic query such as counts, sums, averages, or, in the context of tree ensembles, the number of samples of a particular class in a data subset. 
DP can give protected access to this query through a probabilistic mechanism \(\mathcal{K}\), which outputs \(\query\) with additional noise. 
As detailed in~\citet{dwork2006calibrating}, the amount of noise needed to protect the data depends on the \emph{global sensitivity} of the query, defined as
\(
\sensitivity{\query} = \max ||\query(\dataset_1) - \query(\dataset_2)||_1,
\)
in which the maximum is taken over all pairs of datasets $\dataset_1$ and $\dataset_2$ differing in at most one element. 
Thus, \(\sensitivity{\query}\) quantifies the largest possible change in the query output due to a single record's addition or removal (\emph{e.g.}, $\sensitivity{\query} = 1$ for counts).
For instance, the Laplace mechanism, a widely used DP method, adds noise sampled from a Laplace distribution:
\[
\mathcal{K}(\dataset) = \query(\dataset) + (\noisevar_1, \ldots, \noisevar_k),
\]
in which \( \noisevar_i \) are random variables i.i.d. following the Laplace distribution \( \text{Lap}(\sensitivity{\query} / \varepsilon) \) with $\varepsilon$ the privacy budget granted to the mechanism.

When multiple DP mechanisms are combined, the overall privacy budget is governed by two fundamental composition theorems~\cite{mcsherry2007mechanism}.
First, \emph{sequential composition} states that applying several \( \varepsilon_i \)-DP mechanisms to the same dataset results in \( \sum_i \varepsilon_i \)-DP.
In contrast, \emph{parallel composition} states that applying several \( \varepsilon_i \)-DP mechanisms to disjoint subsets of data results in  \( \max_i \varepsilon_i \)-DP.
Finally, the \emph{postprocessing property} ensures that any function applied to the output of \(\mathcal{K}\) does not affect the DP guarantee, providing flexibility for downstream analysis.

DP mechanisms have been widely adopted to protect privacy when releasing machine learning models.
For example, noise can be added to gradients during neural network training~\citep{Abadi_2016}, or split thresholds can be perturbed in decision tree predictors~\citep{fletcher2019decision}. 
In particular, DP can mitigate privacy risks such as membership inference or model inversion~\citep{DBLP:journals/tifs/YeSZLZ22}, 
and implementations of DP mechanisms have been integrated within libraries, such as IBM's \texttt{diffprivlib}~\citep{DBLP:journals/corr/abs-1907-02444} or Google's \texttt{differential-privacy}\footnote{\href{https://github.com/google/differential-privacy/}{https://github.com/google/differential-privacy/}}.

\subsection{Differentially-Private Random Forests}
\label{sec:dp_rf}

We consider a training set $\dataset = {\{\attributes[\example];\class_{\example}\}}^{\nexamples}_{\example=1}$ in which each example $\example$ is characterized by a vector $\attributes[\example] \in \{0,1\}^{\nattributes}$ of $\nattributes$ binary attributes and a class $\class_{\example} \in \classes$. 
Categorical features can be transformed into this format using one-hot encoding, as is standard practice in tree ensemble training, while continuous features can be discretized through binning.
Based on this dataset, a random forest \(\forest\) is built as an ensemble of decision trees \(\tree \in \forest\). 
Each tree consists of a set of internal nodes \(\internalnodes[\tree]\), in which each node $\node \in \internalnodes[\tree]$ contains a binary condition on the value of an attribute, and a set of leaves \(\leaves[\tree]\). 
When an example is evaluated, it traverses the tree by descending to the left child \(\leftchild[\node]\) if the condition is satisfied, or to the right child \(\rightchild[\node]\) otherwise. 
Once a leaf \(\node \in \leaves[\tree]\) (terminal node) is reached, it contributes to the class distribution associated with that leaf, which is determined by the per-class counts of training examples reaching the leaf.

In most random forest implementations, including popular libraries such as \texttt{scikit-learn}~\citep{scikit-learn}, predictions are made using \emph{soft voting}: each tree outputs class probabilities based on the normalized class counts in its leaves and the ensemble prediction averages these probabilities.
Specifically, for each leaf~\(\node \in \leaves[\tree]\), the model stores~\(\nodesupport[\tree,\node,\class]\), the number of training examples of class~\(\class\) that reach~\(\node\). 
This information is essential for on-device inference and audits but also introduces risks, as it exposes model parameters to potential  adversaries~\citep{ferry2024trained}.

To address these risks, an extensive body of research has focused on constructing DP RFs to conciliate model accessibility and privacy protection with many of these seminal methods being surveyed by
\citet{fletcher2019decision}.
Among these, the method proposed by~\citet{jagannathan2012enabling}, which combines randomized construction and noisy counts publication, was the first to achieve strong empirical predictive performance and formal DP guarantees. 
Consequently, we use it a foundation of our experimental study.

In this approach, each tree is trained on the complete dataset~$\dataset$.
Each tree of a chosen depth $\depth$ is grown by randomly selecting attributes and split values for all internal nodes \(\node \in \internalnodes[\tree]\), avoiding direct access to $\dataset$ during this phase to preserve the DP budget.
Subsequently, the class counts~\(\nodesupport[\tree,\node,\class]\) at each leaf \(\node \in \leaves[\tree]\) are obfuscated using the Laplace mechanism~\citep{dwork2006calibrating} and cast to integer values, as illustrated in Figure~\ref{fig:comparaison}.

\input{trees-examples-single}

Let $\varepsilon$ denote the total privacy budget. 
Since all trees $\tree \in \forest$ are trained on the same dataset $\dataset$, sequential composition applies, dividing the budget across trees such that each tree has a budget of $\varepsilon_{\tree} = \varepsilon / |\forest|$.
Since the attributes and split values within each tree are chosen randomly without accessing the training data, the noisy computation of class counts is the sole operation consuming the DP budget. 
Within each tree, \emph{parallel composition} applies since the leaves have disjoint supports.
Similarly, within a leaf, the counting operations for each class are independent, also satisfying parallel composition. 
Accordingly, the DP budget allocated to publish each class count $ \nodesupport[\tree,\node,\class]$  for $\tree \in \forest, \node \in \leaves[\tree], \class \in \classes$ is $\varepsilon_\node = \varepsilon_\tree = \varepsilon / \vert \forest \rvert$. 
Since class counts have a global sensitivity $\sensitivity{\query}=1$, the noisy class counts are computed~as:
\[
\nodesupport[\tree,\node,\class]^* = \textrm{int}(\nodesupport[\tree,\node,\class] + \noisevar_{\tree\node\class}) = \nodesupport[\tree,\node,\class]  +  \textrm{int}( \noisevar_{\tree\node\class}) ,
\]
in which $\noisevar_{\tree\node\class}$ is a random variable sampled from a Laplace distribution \( \text{Lap}(1 / \varepsilon_\node)\) and
$\textrm{int}(.)$ is the integer part function.

\section{Reconstruction Attack against DP RFs}
\label{sec:attack}

\paragraph{Attack Model.} We assume \emph{white-box} access to an $\varepsilon$-DP RF constructed using the algorithm of~\citet{jagannathan2012enabling}, as described in Section~\ref{sec:dp_rf}. 
This includes access to the structure of the forest (split attributes and values) and the (noisy) counts at the leaves. 
Consistent with the literature on reconstruction attacks~\citep[\emph{e.g.},][]{dwork2017exposed,ferry2024trained}, we also assume knowledge of the number of attributes $\nattributes$ and training examples $\nexamples$.
While the knowledge of $\nattributes$ is necessary for inference, the knowledge of $\nexamples$ simplifies the attack but is not strictly required. 
In particular, in Appendix~\ref{appendix:cp_model_n_unknown}, we demonstrate that our attack can reconstruct the dataset without prior knowledge of $\nexamples$, making it the first method to target full dataset reconstruction without this assumption.
We also assume the privacy budget~$\varepsilon$ is known. This does not degrade the DP guarantees, is generally disclosed by practitioners\footnote{For example, \citet{desfontainesblog20211001}'s website documents real-world applications of DP and their associated budgets.}, and researchers have advocated for its transparency~\citep{DBLP:journals/jpc/DworkKM19}.

\paragraph{Reconstruction Approach.} Our approach is grounded on the CP formulation introduced in~\citet{ferry2024trained}, with additional constraints, variables and an objective specifically designed to model the (observed) noisy counts, the underlying (guessed) true counts and their likelihood. 
Solving the original problem formulation (called DRAFT) was shown to be NP-Hard, though practical at scale with state-of-the-art CP solvers~\citep{cpsatlp}. 
As our reconstruction problem for DP RFs generalizes it (\emph{i.e.}, DRAFT is a special case with no noise), solving it is also NP-Hard.

Our formulation revolves around three main groups of \emph{decision variables}. 
The objective of a CP solver is to find compatible variable values that satisfy a given set of \emph{constraints}, yielding a feasible solution. Moreover, as explained later in this section, an \emph{objective function} steering the search towards likely solutions (reconstructions) will be included.
First, for each tree $\tree \in \forest$, the (guessed) true number of examples of class $\class \in \mathcal{\classes}$ in each leaf $\node \in \leaves[\tree]$ are encoded using a variable $\nodesupport[\tree,\node,\class] \in \mathbb{N}$. Next, a set of variables encodes each reconstructed example $\example \in \{1,\dots,\nexamples\}$: $\varx[\example,\feature] \in \{ 0,1 \}$ encodes the value of its $i^{\text{th}}$ attribute $\feature \in \{1,\dots,\nattributes\}$, while $\varz[\example,\class] \in \{ 0,1 \}$ indicates if it belongs to class $\class \in \classes$.
Third, the path of each example $\example \in \{1,\dots,\nexamples\}$ through each tree $\tree \in \forest$ is modelled through a binary variable $\varyb[\tree,\node,\example,\class] \in \{0,1\}$, which is $1$ if it is classified by leaf $\node \in \leaves[\tree]$ as class $\class \in \classes$, and $0$ otherwise.
The following constraints linking these variables are then enforced:
\begin{align}
    &\sum\limits_{\node \in \leaves[\tree]} \sum\limits_{\class \in \classes} \nodesupport[\tree,\node,\class] = \nexamples &\tree \in \forest \label{constr_count_sum_N}\\
    &\sum\limits_{\class \in \classes}\varz[\example,\class]=1 &\example \in \{1,\dots,\nexamples\}\label{constr_exactly_one_class}\\
    &\varz[\example,\class] =  \sum\limits_{\smash{\mathclap{\node \in \leaves[\tree]}}} \varyb[\tree,\node,\example,\class]  &   \example \in \{1,\dots,\nexamples\}, \tree \in \forest, \class \in \classes\label{constr_not_in_class_counts}\\
    & \sum_{\example=1}^{\smash{\nexamples}} \varyb[\tree,\node,\example,\class] =\nodesupport[\tree,\node,\class] &\tree \in \forest,~\node \in \leaves[\tree],~\class \in \classes\label{constr_counts_flow}
\end{align}
In a nutshell, Constraint~\ref{constr_count_sum_N} ensures that the example counts sum up to $\nexamples$ within each tree.
Constraint~\ref{constr_exactly_one_class} guarantees that each example is associated to exactly one class.
Constraint~\ref{constr_not_in_class_counts} enforces that each example contributes only to the counts of its assigned class. 
Moreover, Constraint~\ref{constr_counts_flow} ensures consistency between the counts at each leaf and the examples assigned to it. 
Finally, for each leaf $\node \in \leaves[\tree]$ of each tree $\tree \in \forest$, let $\Phi_\node^+$ (respectively $\Phi_\node^-$) be the set of binary attributes that must equal $1$ (respectively $0$) for an example to fall into $\node$. 
Then, the following constraint enforces consistency between the examples' assignments to the leaves, and the value of their attributes:
\begin{align}
\sum\limits_{\class \in \classes} \varyb[\tree,\node,\example,\class] = 1 \Rightarrow &\left(\displaystyle\bigwedge_{i\in\Phi_v^+} \varx[k,i] = 1 \right) \wedge
\left( \displaystyle\bigwedge_{i\in \Phi_v^-} \varx[k,i] = 0 \right)\nonumber\\
&\quad \tree \in \forest,~\node \in \leaves[\tree],~\example \in \{1,\dots,\nexamples\}.\label{constr_flow_attr_values}
\end{align}
Optionally, if additional structural knowledge about the features is available, \emph{e.g.}, one-hot encoding or logical/linear relationships between their values, this information can be incorporated by adding the corresponding constraints on the~$\varx[k,i]$ values for each sample $k$.

The aforementioned variables and constraints define the \textbf{domain of all possible datasets and associated counts~$\nodesupport[\tree,\node,\class]$ compatible with the structure and split values of an observed forest}. 
Among all solutions in this domain, we aim at retrieving the one that is the \emph{most likely} to have generated the observed noisy counts $\nodesupport[\tree,\node,\class]^{*}$ through the DP mechanism.
To this end, we formulate through Constraint~\ref{constr_count_link_noise} the noise $\Delta_{\tree\node\class}$ that must have been added to each count to achieve the observed values.
\begin{align}
&\Delta_{\tree\node\class} = \nodesupport[\tree,\node,\class]^{*} - \nodesupport[\tree,\node,\class] & \tree \in \forest,~\node \in \leaves[\tree],~\class \in \classes \label{constr_count_link_noise}
\end{align}
We know that these noise values correspond to i.i.d realizations of random variables $\noisevar_{\tree\node\class}$ from a Laplace distribution \( \text{Lap}(1 / \varepsilon_\node)\) cast to integer. 
Given this information, the \emph{likelihood} of a complete solution associated to a set of $\Delta_{\tree\node\class}$ values is:
\begin{align}
    \mathcal{P}_{\boldsymbol\Delta} = \prod\limits_{\tree,\node,\class} \mathbb{P}(\textrm{int}(\noisevar_{\tree\node\class}) = \Delta_{\tree\node\class}).
\end{align} 
Maximizing this likelihood is equivalent to maximizing its logarithm, leading to the following objective function:
\begin{align}
\log(\mathcal{P}_{\boldsymbol\Delta})
    &= \max \sum\limits_{\tree,\node,\class} \log(\mathbb{P}(\textrm{int}(\noisevar_{\tree\node\class}) = \Delta_{\tree\node\class})).\label{eq:obj_f_th}
\end{align} 
The Laplace distribution density with parameter \((1 / \varepsilon_\node)\) is given by $\Laplace(u) = \frac{\varepsilon_\node}{2}e^{(-|u|\varepsilon_\node)}$. 
Since the noisy counts are cast to integer values, the probability of a specific (integer) noise value $\noiseval$ is:
\begin{equation*}
p_\noiseval = \mathbb{P}(\textrm{int}(\noisevar_{\tree\node\class}) = l) = 
\begin{cases}
\int_{\noiseval}^{\noiseval+1} \frac{\varepsilon_\node}{2} e^{-\lvert u \rvert \varepsilon_\node} \, du & \text{if } \noiseval > 0, \\
\int_{-1}^{1} \frac{\varepsilon_\node}{2} e^{-\lvert u \rvert \varepsilon_\node} \, du & \text{if } \noiseval = 0, \\
\int_{l-1}^{l} \frac{\varepsilon_\node}{2} e^{-\lvert u \rvert \varepsilon_\node} \, du & \text{if } \noiseval < 0.
\end{cases}
\end{equation*}

In practice, we pre-compute constants $p_\noiseval$ for all values $\noiseval \in \{ -\delta,\dots, \delta \}$ such that $\mathbb{P}(\textrm{int}(\noisevar_{\tree\node\class}) \in \{ -\delta,\dots, \delta \} ) \geq 0.999$. 
Details on the computation of $\delta$ are provided in Appendix~\ref{appendix:width_delta_search_interval}, but typically, $\delta = \lceil 12 / \varepsilon_\node \rceil$ is sufficient.

With this, we can additionally restrict the feasible domain of the variables to $\nodesupport[\tree,\node,\class] \in \{\max(0,\nodesupport[\tree,\node,\class]^*-\delta),\dots,\nodesupport[\tree,\node,\class]^*+\delta\}$ and $\Delta_{\tree\node\class} \in \{ -\delta,\dots, \delta \}$ and formulate the objective as:
\begin{align}
\max \ \sum\limits_{\tree \in \forest}\sum\limits_{\node \in \leaves[\tree]} \sum\limits_{\class \in \classes} \sum\limits_{\noiseval = -\delta}^{\delta} \log(p_\noiseval) \mathds{1}_{\Delta_{\tree\node\class} = \noiseval},
\end{align}
in which the indicator $\mathds{1}_{\Delta_{\tree\node\class} = \noiseval}$ can efficiently be expressed in CP using \emph{domain mapping} constraints based on the values of the $\Delta_{\tree\node\class}$ variables.
 
\section{Experimental Study}
\label{sec:experiments}

We now empirically evaluate our proposed reconstruction attack, and investigate whether it succeeds in retrieving training examples despite the DP protection.

The source code and data needed to reproduce all our experiments and figures are available on our repository\footnote{\href{https://github.com/vidalt/DRAFT-DP}{https://github.com/vidalt/DRAFT-DP}}.

\subsection{Experimental Setup}
\label{subsec:setup}

\paragraph{Datasets and target models.} We rely on three datasets, using the same data preprocessing as in~\citet{ferry2024trained}\footnote{\footnotesize \href{https://github.com/vidalt/DRAFT/tree/main/data}{https://github.com/vidalt/DRAFT/tree/main/data}}. The COMPAS dataset~\citep{angwin2016machine} contains records of $7{,}206$ criminal offenders from Florida, each characterized by $15$ binary attributes, and is used for recidivism prediction.
The UCI Adult Income dataset~\citep{Dua:2019} gathers data on $48{,}842$ individuals from the 1994 U.S. census. 
Each individual is described by $20$ binary features, with the objective of predicting whether an individual earns more than \$50K/year. 
Finally, the Default Credit Card dataset~\citep{yeh2009comparisons} includes $29{,}986$ customer records from Taiwan, with $22$ binary attributes, to predict payment defaults.

The $\varepsilon$-DP RFs are trained as described in Section~\ref{sec:dp_rf}. In practice, we build upon the efficient and modular implementation of IBM's \texttt{diffprivlib} library~\citep{DBLP:journals/corr/abs-1907-02444}, which we slightly modify for our experiments.
Following~\citet{ferry2024trained}, each experiment uses a randomly sampled training set of $100$ examples per dataset. Beyond this sample size, the generalization capabilities of the RFs did not improve further, and this smaller sample size allows all experiments to be conducted in a moderate time frame. We build DP RFs of size $\lvert \forest \rvert \in \{1, 5, 10, 20, 30\}$ and of maximum depth $\depth \in \{3,5,7\}$.  
Empirical evidence from~\citet{fan2003is} suggests that the predictive accuracy of a random forest typically plateaus beyond $30$ trees, with $10$ to $30$ trees being sufficient for robust performance.
This is particularly true in the DP setting, where adding trees consumes additional privacy budget~\citep{fletcher2019decision}. Finally, we evaluate privacy parameters $\varepsilon \in \{0.1, 1, 5, 10, 20, 30\}$, ranging from very tight to large privacy budgets, which reflect commonly used values in practice~\citep{desfontainesblog20211001}. 

\paragraph{Reconstruction attack.} We use the \texttt{OR-Tools} CP-SAT solver~\citep{cpsatlp} (v9) to solve the CP formulation introduced in Section~\ref{sec:attack}. 
Each model resolution is limited to a maximum of $2$ hours of CPU time using $16$ threads with up to $7$GB of RAM per thread. 
All experiments are run on a computing cluster over a set of homogeneous nodes with AMD EPYC 7532 (Zen 2) @ 2.40 GHz CPU.

We consider two baselines in our experiments. 
As in~\citet{ferry2024trained}, a random reconstruction baseline is used to quantify the amount of information that can be inferred without knowledge of the RF (\emph{i.e.}, only based on the number of examples $\nexamples$, the different attributes $\nattributes$, their domains and one-hot encoding). 
This baseline randomly guesses the value of each attribute of each example while remaining consistent with one-hot encoding when applicable. Its performance is averaged over $100$ runs. 
To quantify the effect of DP on the reconstruction error, we also run the state-of-the-art DRAFT attack against the trained RFs without the DP protection (\emph{i.e.}, with the true leaves' counts). 

Once a reconstructed dataset is inferred, first it is aligned with the original training set using a minimum cost matching with the Manhattan distance, i.e., this process does a best pairwise association between the reconstructed examples and the original ones.
Then, the proportion of binary attributes that differ is averaged over the entire dataset to quantify the reconstruction error. 
Each experiment is repeated using five different random seeds and the results are averaged. 

\input{summary_table} 

\subsection{Results}

We now highlight our key empirical findings and illustrate each of them with a subset of representative results.

\paragraph{Result 1. Reconstruction attacks succeed against DP RFs, even with meaningful privacy guarantees.} 
Table~\ref{tab:error-values} reports the reconstruction error for all our experiments. 
For commonly protective privacy budget (\emph{e.g.}, $\varepsilon = 1$ or $5$), our attack achieves significantly lower reconstruction error compared to the random baseline. 
This indicates that the adversary was able to exploit the RF to reconstruct parts of the training data, highlighting a privacy breach.
Nonetheless, the reconstruction error consistently increases when reducing the privacy budget $\varepsilon$, which suggests that DP partially mitigates the reconstruction success. 
To assess this, Figure~\ref{fig:DRAFT_baseline} compares the efficiency of our reconstruction to that of DRAFT~\citep{ferry2024trained} applied on the same RF (without the DP protection).
Intuitively, for a given RF's size $\lvert \forest \rvert$ and privacy budget $\varepsilon$, any difference in the reconstruction error compared to DRAFT can be associated to the DP protection.
For very tight privacy budgets (\emph{e.g.}, small~$\varepsilon$), DP significantly increases the reconstruction error compared to the DRAFT baseline. However, as $\varepsilon$ increases, the protective effect of DP diminishes, and the difference in reconstruction error between our attack and DRAFT becomes much smaller, even for $\varepsilon$ values commonly considered protective.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\linewidth]{reconstruction_error_vs_epsilon_compas.pdf}
    \includegraphics[width=\linewidth]{legend.pdf}
    \caption{Average reconstruction error as a function of the privacy budget $\varepsilon$ used to fit the target forest $\forest$, for different numbers of depth-5 trees $\vert \forest \rvert$ on the COMPAS dataset. 
    For comparison, we also report the reconstruction error of DRAFT applied to the same RFs without DP protection. 
    }
    \label{fig:DRAFT_baseline}
\end{figure}

\paragraph{Result 2. DP leads to a complex trade-off between the RFs' size and the reconstruction attack success.} 
As can be seen in Figure~\ref{fig:DRAFT_baseline}, larger DP RFs do not necessarily provide more information to the attacker, unlike non-DP RFs. 
A threshold effect is observed: the reconstruction error decreases as the number of trees increases, up to a point (typically around 5-10 trees), before increasing again. 
This behavior is consistent across all privacy budgets and is likely due to the privacy budget being distributed across more trees, resulting in noisier counts for individual leaves. Thus, while deeper trees in the non-DP setting provide more information to the attacker, this is not always the case for DP RFs (see also, Table~\ref{tab:error-values}) since smaller leaves' counts are proportionally more affected by the noise.

\paragraph{Result 3. Protection against reconstruction attacks comes at the expense of predictive accuracy.} 
As shown in Table~\ref{tab:error-values}, the considered DP mechanism effectively mitigates the success of reconstruction attacks at very low $\varepsilon$ values (\emph{e.g.}, $\varepsilon = 0.1$). However, at these privacy levels, the resulting RFs have poor predictive accuracy, often performing no better than a majority prediction classifier. Figure~\ref{fig:heatmap} illustrates the trade-off between the enforced DP budget and the resulting RF accuracy. Notably, only RFs with trivial predictive performance provide no advantage to the reconstruction adversary, as also illustrated in Figure~\ref{fig:acc_vs_error_compas}, in which most RFs either perform better than a majority classifier but provide a significant advantage to the reconstruction attacker (upper left corner), or provide no advantage to the attacker but perform worse than a majority classifier (lower right corner).

    \begin{figure}[ht]
        \centering
        \includegraphics[width=0.95\linewidth]{N_fixed_compas_heatmap_depth7.pdf}
     \caption{Average training accuracy of $\varepsilon$-DP RFs with depth-7 trees as a function of the privacy budget $\varepsilon$, for different forest sizes $\lvert \forest \rvert$ on the COMPAS dataset}
    \label{fig:heatmap}
\end{figure}

\begin{figure}[htb]
    \centering
    \includegraphics[width=\linewidth]{N_fixed_compas_train_accuracy_vs_error_depth_5.pdf}
    \includegraphics[width=\linewidth]{legend_tradeoffs.pdf}
    \caption{Average training accuracy of $\varepsilon$-DP RFs with depth-5 trees as a function of the reconstruction error, for different privacy budgets $\varepsilon$ and forest sizes $\lvert \forest \rvert$ on the COMPAS dataset}
    \label{fig:acc_vs_error_compas}
\end{figure}

\paragraph{Result 4. Trade-offs between the RF predictive performance and the reconstruction error are complex.}
Detailed trade-offs between the RFs accuracy and reconstruction error for all considered datasets and maximum depths are provided in  Appendix~\ref{appendix:complete_results_tradeoffs_perf_reconstr_error}. Consistent with Figure~\ref{fig:acc_vs_error_compas}, the only configurations yielding both non-trivial predictive performance and offering no advantage to the reconstruction adversary (upper right corner) are typically the smallest forests ($1$ tree) or the largest forests ($30$ trees).
A plausible explanation is that small RFs inherently limit the amount of available information, providing natural protection against reconstruction. Conversely, in large RFs, the increased noise is offset by greater diversity among the trees, enhancing empirical privacy while preserving predictive performance. In contrast, medium-sized RFs often strike a poor balance: they provide enough diversity to aid reconstruction attacks but insufficient noise to effectively counteract them.
These findings emphasize the importance of carefully selecting parameters such as forest size, tree depth, and privacy budget when training DP RFs for sensitive applications.

\paragraph{Result 5. Knowledge of the training set size $\nexamples$ is unnecessary.} 
While prior work often assumes knowledge of the training set size $\nexamples$, Appendix~\ref{appendix:cp_model_n_unknown} presents a modified version of our CP model that enables reconstruction without this information. Our experiments demonstrate that accurate reconstruction remains feasible under this assumption. 
Although the computational cost and reconstruction error increase slightly compared to the traditional setup where~$\nexamples$ is known, the attack remains highly effective for most practically relevant $\varepsilon$ values. 

\section{Related Work}
\label{sec:related_work}

\noindent
\textbf{Reconstruction Attacks.}
Reconstruction attacks were first introduced against database queries by~\citet{DBLP:conf/pods/DinurN03}. In their setup, all database columns are public except for a binary one, which the adversary attempts to recover using (noisy) responses to counting queries. By modeling the noise within a linear programming framework, their attack successfully retrieves the private bit for all individuals after a significant number of queries. Over the following decade, several works extended this approach~\mbox{\citep{dwork2017exposed}}.


More recently, reconstruction attacks have been adapted to machine learning models. Unlike database-focused attacks, these methods often aim to infer specific attributes or recover information about individual samples rather than reconstructing the entire training set~\citep{fredrikson2015model}. Many of these attacks also rely on auxiliary information beyond the trained model, such as intermediate gradients during training~\citep{DBLP:conf/atis/PhongA0WM17,DBLP:conf/uss/0001B0F020}, stationary points reached during optimization~\citep{DBLP:conf/nips/HaimVYSI22}, or fairness-related constraints~\citep{hu2020inference,hamman2022can,ferry2023exploiting}.

In contrast, recent studies have demonstrated that tree-based models are particularly vulnerable to reconstruction attacks due to their combinatorial structure. For example, \citet{DBLP:conf/dbsec/GambsGH12} and \citet{ferry2024probabilistic} developed algorithms leveraging white-box access to a single decision tree to reconstruct a probabilistic representation of its dataset, encoding all reconstructions consistent with the tree's structure. More recently, \citet{ferry2024trained} proposed DRAFT, a CP-based reconstruction attack against RFs, showing that the structure of an RF can be exploited to reconstruct most of its training data efficiently.

DP~\citep{dwork2014algorithmic} has been widely adopted as a protection mechanism as it provides strong theoretical guarantees. 
However, while it inherently bounds the risk of membership inference attacks, the effectiveness of its protection against other types of attacks varies. 
For instance, DP has been shown to mitigate reconstruction attacks on deep neural networks~\citep{ziller2024reconciling} and language models~\citep{DBLP:journals/corr/abs-2202-07623}, even for large privacy budgets.
However, these defenses often come at a significant cost to model performance, and the considered attacks typically do not exploit the DP mechanism explicitly.
Our work advances this field by explicitly accounting for the DP mechanism within the attack formulation. We analyze the trade-offs between the enforced DP budget, the empirical reconstruction attack success, and the DP model's predictive performances in the context of tree ensembles.

\textbf{Building DP Random Forests.}
Traditional tree induction algorithms iteratively perform a greedy selection of the attributes and values to split on at each level of the considered tree, based on some information gain criterion. 
Some works on DP RFs proceed similarly~\citep{Singh,DBLP:conf/ausdm/Fletcher015, rana}. 
However, since the training data need to be accessed at each level of the tree to greedily determine the splits, the DP budget $\varepsilon$ must be divided between them, resulting in the addition of a significant amount of noise. 
Thus, other approaches proposed to randomly determine all splitting attributes and values~\citep{jagannathan2012enabling,DBLP:conf/ausai/FletcherI15,fletcher2016smooth}, and only access the data to populate the leaves. 
Even if random splits may partition the data imperfectly, this reduction in the amount of noise required to ensure DP generally allows for better predictive performance. 
Note that this specific aspect makes no difference in our attack's design, since the reconstruction model does not directly exploit the way the splits are determined.

In non-DP RFs, bootstrap sampling is commonly used to generate each tree's training set, by performing sampling with replacement from $\nexamples$ to $\nexamples$. 
While some DP RF approaches~\citep{rana} have adopted this strategy, providing tight and thorough DP guarantees remains challenging, as bootstrapped subsets are not disjoint and often contain multiple occurrences of certain examples. An alternative is to divide the original dataset into disjoint subsets to independently train each tree, enabling parallel composition of the privacy budget~\citep{DBLP:conf/ausai/FletcherI15,fletcher2016smooth}. However, this approach may harm predictive performance as each tree is trained on significantly less data. Moreover, this also results in smaller leaves' class counts, which in turn are proportionally more affected by the noise added to enforce DP. 
A standard approach is to learn all trees on the full training dataset, even though it requires dividing the privacy budget across trees~\citep{jagannathan2012enabling,DBLP:conf/ausai/FletcherI15,fletcher2016smooth,Singh,DBLP:conf/ausdm/Fletcher015}. 
While our attack could handle bootstrap sampling as in~\citet{ferry2024trained}, if the trees are trained on completely disjoint subsets, a per-tree probabilistic reconstruction approach, such as the one proposed by~\citet{ferry2024probabilistic}, suffices, since nothing can be gained by relating the information discovered on examples across different~trees.

Finally, some approaches such as IBM's \texttt{diffprivlib} implementation~\citep{DBLP:journals/corr/abs-1907-02444}, release only (noisy) majority labels within the leaves of DP RFs to conserve the privacy budget instead of providing full class counts. However, this strategy prevents the use of the traditional soft voting scheme for inference, relying instead on simple majority voting, which may degrade predictive performance. Furthermore, access to class counts is often necessary for auditing purposes. Adapting the attack developed in this study to handle this setup would require modifying the objective function and introducing additional variables, to compute the likelihood of generating an observed (noisy) majority label for each leaf based on the guessed class counts.

\section{Conclusion}
\label{sec:conclusion}

This work has demonstrated that even with a low privacy budget, accurate reconstruction of the training examples of an $\varepsilon$-DP RF remains possible. 
Crucially, it shows that a model can comply with rigorous DP guarantees while still leaking a significant portion of its training data, highlighting that privacy protection cannot be solely reduced to choosing a sufficiently small $\varepsilon$. 
Rather, the design of the DP mechanism itself plays a fundamental role in mitigating reconstruction risks. 
This aligns with recent findings on privacy attacks  (\emph{i.e.}, re-identification and attribute inference) against local-differentially private protocols, in which the attack success depends not only on $\varepsilon$ but also on the mechanism's structure~\cite{DBLP:journals/pvldb/ArcoleziGCP23}. 
Similarly, our results suggest that a careful selection of the hyperparameters used to tune the RF and the DP mechanism allows for better trade-offs between predictive performance and empirical protection against reconstruction attacks.

The research avenues connected to this work are numerous.
One first direction involves extending the proposed reconstruction framework to DP RFs trained with alternative mechanisms ---\emph{e.g.}, adding noise from different distributions or distributing it adaptively across the different algorithm components--- or revealing different information, as discussed in Section~\ref{sec:related_work}. 
Notably, we observe that any known \emph{prior} on the dataset distribution can be embedded into the likelihood objective. 
Next, our approach lays the foundation for a more systematic privacy evaluation methodology grounded on mathematical programming, which integrates available knowledge from the training model as constraints and guides the search towards a most likely solution according to the underlying DP mechanism specifications. 
This framework can serve as a benchmark for DP strategies, helping to analyze how different DP configurations influence the trade-off between privacy and model utility. 
Understanding this interplay is crucial for developing machine learning models that are not only privacy-preserving in theory but also practically usable and robust against real-world privacy attacks. 

\section*{Impact Statement}

Machine learning models are increasingly trained on sensitive private data, such as medical records or personal histories. 
The need to audit their decision-making processes (\emph{e.g.}, to prevent discriminatory practices) has led to regulations advocating for greater transparency.
To enable the safe release of statistics, model predictions or trained models, DP has become a widely adopted standard due to its appealing theoretical properties. 

However, DP does not (at least directly) provide theoretical protection against dataset reconstruction attacks. 
Our work demonstrates that, in practice, machine learning models can comply with rigorous DP guarantees while still leaking a significant portion of their training data.
This critical finding highlights the need for developing new defense mechanisms, and emphasizes the importance of the design of the DP mechanism itself, beyond the sole privacy budget.
Going in this direction, our study provides insights into the impact of different DP design choices and advocates for empirically evaluating a model’s resilience against a variety of reconstruction attacks ---including the one introduced in this work--- before disclosure. 
Our findings emphasize that achieving privacy-preserving machine learning requires a holistic approach that carefully balances transparency, model utility, and empirical robustness against privacy threats. 
Ultimately, DP guarantees alone are not a silver bullet for privacy.

\bibliography{sample}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
\input{supplementary_material}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}

%\section{You \emph{can} have an appendix here.}

%You can have as much text here as you want. The main body must be at most $8$ pages long.
%For the final version, one more page can be added.
%If you want, you can use an appendix like this one.  

%The $\mathtt{\backslash onecolumn}$ command above can be kept in place if you prefer a one-column appendix, or can be removed if you prefer a two-column appendix.  Apart from this possible change, the style (font size, spacing, margins, page numbering, etc.) should be kept the same as the main body.

% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
