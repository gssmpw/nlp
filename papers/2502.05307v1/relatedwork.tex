\section{Related Work}
\label{sec:related_work}

\noindent
\textbf{Reconstruction Attacks.}
Reconstruction attacks were first introduced against database queries by~\citet{DBLP:conf/pods/DinurN03}. In their setup, all database columns are public except for a binary one, which the adversary attempts to recover using (noisy) responses to counting queries. By modeling the noise within a linear programming framework, their attack successfully retrieves the private bit for all individuals after a significant number of queries. Over the following decade, several works extended this approach~\mbox{\citep{dwork2017exposed}}.


More recently, reconstruction attacks have been adapted to machine learning models. Unlike database-focused attacks, these methods often aim to infer specific attributes or recover information about individual samples rather than reconstructing the entire training set~\citep{fredrikson2015model}. Many of these attacks also rely on auxiliary information beyond the trained model, such as intermediate gradients during training~\citep{DBLP:conf/atis/PhongA0WM17,DBLP:conf/uss/0001B0F020}, stationary points reached during optimization~\citep{DBLP:conf/nips/HaimVYSI22}, or fairness-related constraints~\citep{hu2020inference,hamman2022can,ferry2023exploiting}.

In contrast, recent studies have demonstrated that tree-based models are particularly vulnerable to reconstruction attacks due to their combinatorial structure. For example, \citet{DBLP:conf/dbsec/GambsGH12} and \citet{ferry2024probabilistic} developed algorithms leveraging white-box access to a single decision tree to reconstruct a probabilistic representation of its dataset, encoding all reconstructions consistent with the tree's structure. More recently, \citet{ferry2024trained} proposed DRAFT, a CP-based reconstruction attack against RFs, showing that the structure of an RF can be exploited to reconstruct most of its training data efficiently.

DP~\citep{dwork2014algorithmic} has been widely adopted as a protection mechanism as it provides strong theoretical guarantees. 
However, while it inherently bounds the risk of membership inference attacks, the effectiveness of its protection against other types of attacks varies. 
For instance, DP has been shown to mitigate reconstruction attacks on deep neural networks~\citep{ziller2024reconciling} and language models~\citep{DBLP:journals/corr/abs-2202-07623}, even for large privacy budgets.
However, these defenses often come at a significant cost to model performance, and the considered attacks typically do not exploit the DP mechanism explicitly.
Our work advances this field by explicitly accounting for the DP mechanism within the attack formulation. We analyze the trade-offs between the enforced DP budget, the empirical reconstruction attack success, and the DP model's predictive performances in the context of tree ensembles.

\textbf{Building DP Random Forests.}
Traditional tree induction algorithms iteratively perform a greedy selection of the attributes and values to split on at each level of the considered tree, based on some information gain criterion. 
Some works on DP RFs proceed similarly~\citep{Singh,DBLP:conf/ausdm/Fletcher015, rana}. 
However, since the training data need to be accessed at each level of the tree to greedily determine the splits, the DP budget $\varepsilon$ must be divided between them, resulting in the addition of a significant amount of noise. 
Thus, other approaches proposed to randomly determine all splitting attributes and values~\citep{jagannathan2012enabling,DBLP:conf/ausai/FletcherI15,fletcher2016smooth}, and only access the data to populate the leaves. 
Even if random splits may partition the data imperfectly, this reduction in the amount of noise required to ensure DP generally allows for better predictive performance. 
Note that this specific aspect makes no difference in our attack's design, since the reconstruction model does not directly exploit the way the splits are determined.

In non-DP RFs, bootstrap sampling is commonly used to generate each tree's training set, by performing sampling with replacement from $\nexamples$ to $\nexamples$. 
While some DP RF approaches~\citep{rana} have adopted this strategy, providing tight and thorough DP guarantees remains challenging, as bootstrapped subsets are not disjoint and often contain multiple occurrences of certain examples. An alternative is to divide the original dataset into disjoint subsets to independently train each tree, enabling parallel composition of the privacy budget~\citep{DBLP:conf/ausai/FletcherI15,fletcher2016smooth}. However, this approach may harm predictive performance as each tree is trained on significantly less data. Moreover, this also results in smaller leaves' class counts, which in turn are proportionally more affected by the noise added to enforce DP. 
A standard approach is to learn all trees on the full training dataset, even though it requires dividing the privacy budget across trees~\citep{jagannathan2012enabling,DBLP:conf/ausai/FletcherI15,fletcher2016smooth,Singh,DBLP:conf/ausdm/Fletcher015}. 
While our attack could handle bootstrap sampling as in~\citet{ferry2024trained}, if the trees are trained on completely disjoint subsets, a per-tree probabilistic reconstruction approach, such as the one proposed by~\citet{ferry2024probabilistic}, suffices, since nothing can be gained by relating the information discovered on examples across different~trees.

Finally, some approaches such as IBM's \texttt{diffprivlib} implementation~\citep{DBLP:journals/corr/abs-1907-02444}, release only (noisy) majority labels within the leaves of DP RFs to conserve the privacy budget instead of providing full class counts. However, this strategy prevents the use of the traditional soft voting scheme for inference, relying instead on simple majority voting, which may degrade predictive performance. Furthermore, access to class counts is often necessary for auditing purposes. Adapting the attack developed in this study to handle this setup would require modifying the objective function and introducing additional variables, to compute the likelihood of generating an observed (noisy) majority label for each leaf based on the guessed class counts.