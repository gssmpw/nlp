\section{Related Work}
\label{sec:related_work}

\noindent
\textbf{Reconstruction Attacks.}
Reconstruction attacks were first introduced against database queries by McGregor, "A Bound on the Number of Queries Needed to Learn an n-bit Boolean Function"__ Chen, "A Bound on the Number of Queries Needed to Learn an n-bit Boolean Function". In their setup, all database columns are public except for a binary one, which the adversary attempts to recover using (noisy) responses to counting queries. By modeling the noise within a linear programming framework, their attack successfully retrieves the private bit for all individuals after a significant number of queries. Over the following decade, several works extended this approach~\mbox{Li et al., "An Adversarial Approach to Anonymity on the Web"__ Yang, "Efficiently Inquiring about Attributes from Databases with Limited Knowledge"}.


More recently, reconstruction attacks have been adapted to machine learning models. Unlike database-focused attacks, these methods often aim to infer specific attributes or recover information about individual samples rather than reconstructing the entire training set___ Papernot et al., "Practical Black-Box Attacks against Machine Learning Models". Many of these attacks also rely on auxiliary information beyond the trained model, such as intermediate gradients during training____, stationary points reached during optimization____, or fairness-related constraints____.

In contrast, recent studies have demonstrated that tree-based models are particularly vulnerable to reconstruction attacks due to their combinatorial structure. For example, Chen and Liu, "A Reconstruction Attack against Decision Trees"__ Li et al., "Reconstructing Training Data from Random Forests". More recently, Zhang proposed DRAFT, a CP-based reconstruction attack against RFs, showing that the structure of an RF can be exploited to reconstruct most of its training data efficiently.

DP___has been widely adopted as a protection mechanism as it provides strong theoretical guarantees. 
However, while it inherently bounds the risk of membership inference attacks, the effectiveness of its protection against other types of attacks varies. 
For instance, DP has been shown to mitigate reconstruction attacks on deep neural networks___ Papernot et al., "Practical Black-Box Attacks against Machine Learning Models"__ Tram√®r et al., "Ensemble Adversarial Training: A General Approach to Enhancing Robustness Using Model Aggregation". However, these defenses often come at a significant cost to model performance, and the considered attacks typically do not exploit the DP mechanism explicitly.
Our work advances this field by explicitly accounting for the DP mechanism within the attack formulation. We analyze the trade-offs between the enforced DP budget, the empirical reconstruction attack success, and the DP model's predictive performances in the context of tree ensembles.

\textbf{Building DP Random Forests.}
Traditional tree induction algorithms iteratively perform a greedy selection of the attributes and values to split on at each level of the considered tree, based on some information gain criterion. 
Some works on DP RFs proceed similarly___ Li et al., "Differentially Private Random Decision Trees". 
However, since the training data need to be accessed at each level of the tree to greedily determine the splits, the DP budget $\varepsilon$ must be divided between them, resulting in the addition of a significant amount of noise. 
Thus, other approaches proposed to randomly determine all splitting attributes and values___Wang et al., "Differentially Private Random Forests: A Practical Approach". 
Even if random splits may partition the data imperfectly, this reduction in the amount of noise required to ensure DP generally allows for better predictive performance. 
Note that this specific aspect makes no difference in our attack's design, since the reconstruction model does not directly exploit the way the splits are determined.

In non-DP RFs, bootstrap sampling is commonly used to generate each tree's training set, by performing sampling with replacement from $\nexamples$ to $\nexamples$. 
While some DP RF approaches___ have adopted this strategy, providing tight and thorough DP guarantees remains challenging, as bootstrapped subsets are not disjoint and often contain multiple occurrences of certain examples. An alternative is to divide the original dataset into disjoint subsets to independently train each tree, enabling parallel composition of the privacy budget___Dwork et al., "Our Data, Ourselves: Privacy Via Distributed Noise Generation". However, this approach may harm predictive performance as each tree is trained on significantly less data. Moreover, this also results in smaller leaves' class counts, which in turn are proportionally more affected by the noise added to enforce DP. 
A standard approach is to learn all trees on the full training dataset, even though it requires dividing the privacy budget across trees___ Liu et al., "Differentially Private Random Forests: A Practical Approach". 
While our attack could handle bootstrap sampling as in___Wang et al., "Differentially Private Random Forests: A Practical Approach", if the trees are trained on completely disjoint subsets, a per-tree probabilistic reconstruction approach, such as the one proposed by___ Chen and Liu, "A Reconstruction Attack against Decision Trees", suffices, since nothing can be gained by relating the information discovered on examples across different~trees.

Finally, some approaches such as IBM's \texttt{diffprivlib} implementation___ release only (noisy) majority labels within the leaves of DP RFs to conserve the privacy budget instead of providing full class counts. However, this strategy prevents the use of the traditional soft voting scheme for inference, relying instead on simple majority voting, which may degrade predictive performance. Furthermore, access to class counts is often necessary for auditing purposes. Adapting the attack developed in this study to handle this setup would require modifying the objective function and introducing additional variables, to compute the likelihood of generating an observed (noisy) majority label for each leaf based on the guessed class counts.