% In this section, we propose approaches to making explanations quickly understandable at a glance from both spatial and temporal perspectives: \textit{what} information needs to be displayed and \textit{when} the explanation needs to be displayed. 
% In this section, we introduce the dataset and LLM pipeline used for generating recommendations and then we describe our approaches to format the display of LLM explanations.

In this section, we introduce the dataset and LLM pipeline used for generating recommendations, then outline our approaches to present spatially and temporally glanceable LLM explanations on ultra-small devices. The goal of this pipeline was to imitate a human-AI interface that empowered users to accomplish their daily tasks via a smooth transition to digital actions. The system used observations of longitudinal context as input, which could be any combination of sensors a device had access to, such as egocentric video, biometric data, or digital state information. After predicting the user's goal for their next action based on the longitudinal context, the system recommended a digital action to fulfill this goal and disassembled it into device-executable instructions. Along with the recommendation, an explanation was provided for users to ensure transparency and trustworthiness with the system.


\subsection{Dataset}
We used the Ego4D \cite{grauman2022ego4d} dataset as the testbed to evaluate the LLM pipeline in pre-recorded real-world scenarios. The Ego4D dataset consists of over 3,670 hours of egocentric videos of people’s daily activities. The videos were captured using head-mounted cameras to naturally approximate first-person visual perception. 
% The Ego4D-DI data set comprises digital interaction annotations on existing Ego4D videos. The annotations include timepoint box around the start and end of the digital interaction, high-level human goals, situational context, and device and application used. 

We leveraged the annotated video narrations included in the dataset to manually curate a subset of videos that were comprised of interactions with digital devices (e.g., ``phone’’ or ``laptop’’). We trimmed the videos to start 30 seconds prior to the digital action as context for the action and ensured that the video content prior to the digital action contained primarily physical activities, such as cooking or hiking. 
Following this, we were left with 1101 videos 
% that aligned with our study scenarios 
that focused on everyday goal-oriented, contextual scenarios that the LLM could take as input to generate action recommendations. The daily activities covered all 27 categories of common everyday activities identified by a taxonomist who is one of the authors, such as browsing the internet, chores and cleaning, cooking and eating, exercise, work, gaming, messaging and communication, shopping, and traveling.




\subsection{LLM Recommendation Pipeline}

\begin{figure*}[t]
  \centering
\includegraphics[width=\linewidth]{figures/LLM_pipeline.png}
  \caption{Our LLM pipeline, which was based on Socratic Models, used pre-trained vision-language models 
  % (DETR and LaViLa) 
  to generate a linguistic summary of a video input (detected objects and user physical actions) for downstream processing with an LLM (GPT-4). To obtain the LLM self-explanations, the LLM first summarized all possible contexts (i.e., the [activity] the user is doing, the [object] the user is interacting with, and the [location] the user is in) and then inferred the short-term [goal] that the user may want to achieve. Based on the inferred goal, the LLM then provided a digital action recommendation. Throughout the process, we calculated confidence levels of the output from pre-trained vision-language models. The LLM was then prompted for confidence levels for each contextual component (i.e., [activity], [object], [location], and [goal]) and the recommendation.}
  \label{fig:LLM_pipeline}
\end{figure*}



The pipeline was based on Socratic Models \cite{zeng2022socratic}, where pre-trained vision-language models generated a linguistic summary of a video for downstream processing with an LLM (Figure \ref{fig:LLM_pipeline}). Specifically, we employed 
% the LaViLa model \cite{zhao2023learning} 
a pre-trained transformer-based model designed for video understanding 
to extract physical actions (i.e., narrations) present in the video by dividing each video into 2-second clips. We used 
% the DETR model \cite{carion2020end} 
a pre-trained transformer-based object detection model 
to detect objects appearing in the video within a 5-second window before the digital action\footnote{Note that some of the Ego4D videos did not have audio, so only video frames were used as input.}. With these two pieces of contextual information about the video input, GPT-4 was prompted to infer the user's intent and output its recommendation for the user's next digital action.

We prompted the LLM to produce natural language explanations along with its recommendations following step-by-step thinking based on the Chain-of-Thought method \cite{wei2022chain, kojima2022large}. 
As text-based responses from LLMs can be too verbose to be displayed on an ultra-small device's screen, spatially, we structured the textual responses using defined contextual components to create a visual experience that required minimal cognitive effort to comprehend. Temporally, we controlled the presentation timing of the explanations to avoid unnecessary interruptions to the user. We addressed situations where the AI system showed uncertainty, meaning it might make errors and require user confirmation. The prompt template is shown in Figure \ref{fig:prompt_template}.



\begin{figure*}[t]
  \centering
\includegraphics[width=0.95\linewidth]{figures/prompt_template-v2.png}
  \caption{The prompt template included an input description, explanation instructions with few-shot in-context examples (\textcolor{myblue}{\textit{blue text in italics}}), output formatting instructions, and the target input (\textcolor{mypink}{\textit{pink text in italics}}, changed for every query).
  }
  \label{fig:prompt_template}
  \vspace{-10pt}
\end{figure*}




\subsubsection{Structuring Explanation Text Using Defined Contextual Components}

\label{sec:q1_what}
The inference process was divided into several stages. The LLM first extracted and summarized the relevant contextual information, i.e., the [\textit{activity}] the user was performing, the [\textit{object}] the user was interacting with, and the [\textit{location}] of the user, from the output generated by pre-trained vision-language models. Then, the LLM inferred the user's short-term [\textit{goal}] based on these contextual cues. Finally, using the inferred goal, the LLM provided a digital action recommendation.
We chose ``object'', ``activity'', ``location'', and ``goal'' as the contextual information as these entities were the core contextual components necessary to generate relevant recommendations and explanations in an everyday mixed-reality scenario, as provided by the aforementioned taxonomist.
We also referenced definitions and few-shot examples in the prompt for each component. The LLM was instructed to provide its output in a JSON format, which was later used to create the structured representations.


\subsubsection{Adaptively Presenting 
Explanations Based on Confidence Levels}
% \begin{figure*}[t]
%   \centering
% \includegraphics[width=0.8\linewidth]{figures/hybrid_confidence.png}
%   \caption{A hybrid approach that combines consistency-based methods and verbalized confidence to calculate the confidence level for each recommendation.}
%   \label{fig:hybrid_confidence}
% \end{figure*}

Following Chen and Muelle \cite{chen2023quantifying} and Xiong et al. \cite{xiong2024can}, we employed a hybrid approach that combined consistency-based methods and verbalized confidence to extract a calibrated confidence level for each recommendation. We mapped the confidence score for each detected object from the object detection
% DETR 
model and the perplexity score for each detected physical action from the 
% LaViLa 
video narration model into one of five textual confidence levels (i.e., ``very low'', ``low'', ``medium'', ``high'', ``very high''), based on the quantile position of the score. We opted for a textual representation of confidence to ensure a unified representation of “confidence”, as well as to maximize the LLMs’ abilities to comprehend natural language. We included the confidence levels for raw contextual information in the prompt, then prompted the LLM to output confidence levels along with the recommendation. 

We then generated a reference response $y_0$ and confidence $c_0$ with temperature sampling set at 0. Using the same prompt, we produced $K=5$ candidate responses $\{y_1, y_2, ..., y_K\}$ and their verbalized confidences $\{c_1, c_2, ..., c_K\}$ by increasing the temperature value to 0.7 \cite{xiong2024can}. Each candidate confidence $c_i$ ($i \in \{1,2,...,K\}$) was transformed back into a numerical value and was then updated by incorporating the reference answer and the similarity to the reference answer, denoted as $s_i$: $\bar {c_i} = \frac{(c_0+c_i)}{2}s_i$ ($i \in \{1,2,...,K\}$). The similarity scores were obtained using the BERTScore model \cite{Zhang2020BERTScore:}, which measured the semantic similarity between two texts. Finally, we computed the average contribution from all candidate answers as the final confidence score $c_{hybrid} = \sum_{i=1}^{i=K}\bar {c_i}/K$, and then converted it into one of the five confidence levels based on the quantile position of the score.
% (see Figure \ref{fig:hybrid_confidence}).



\subsection{Technical Evaluation}
\label{sec:tech_eval}
To validate the quality of structured explanation components and confidence levels that the system generated, we conducted a small-scale technical evaluation. We randomly sampled one video from each of the 27 activity context categories. Three coders then coded how likely it was that the AI recommendation was correct, how likely it was that the inference about each explanation components (i.e., activity, object, location, and goal) was correct (i.e., plausibility), and to what extent each explanation component supported the AI recommendation (i.e., faithfulness) \cite{mathew2021hatexplain, el2022evaluation}, for each of the 27 videos using a 7-point Likert scale. 

We then calculated the inter-rater agreement across the coders using Krippendorff's $\alpha$ and obtained a score of 0.553, which is considered acceptable for exploratory research involving subjective ratings \cite{krippendorff2018content}. We found that all explanation components were rated as plausible and faithful to the recommendation, with the median of all ratings being larger than 4 (one-sample Wilcoxon signed-rank test showed $p<0.05$ for all explanation components for the plausibility and faithfulness measures; Table \ref{tab:tech_explanation}). 
To assess the calibration of the recommendation confidence, we computed the Pearson’s $r$ correlation coefficient between the coders' estimations of the recommendation correctness likelihood (as ground truth) and the hybrid confidence $c_{hybrid}$, obtaining a significant positive coefficient of $r = 0.559$ ($p<0.01$). 
%p = 0.002$
Meanwhile, the verbalized confidence levels $c_0$ did not correlate with the coders' estimations ($r=0.171$, $p=0.393$), indicating that the hybrid approach could lead to more calibrated confidence.
% In short, the technical evaluation results suggest that our method for generating glanceable explanations is reliable. 


\begin{table}[t]
\caption{Ratings for the plausibility and faithfulness of each explanation component. 
% \todo{change to median (IQR) e.g. 45,000 (12,000 - 60,000)}
}
\label{tab:tech_explanation}
% \vskip 0.15in
\begin{center}
% \begin{small}
% \begin{sc}
\begin{tabular}{lcc}
\toprule
\begin{tabular}[c]{@{}l@{}}\textbf{Explanation}\\\textbf{Component}\end{tabular} & \begin{tabular}[c]{@{}l@{}}\textbf{Plausibility}\\median (IQR)\end{tabular} & \begin{tabular}[c]{@{}l@{}}\textbf{Faithfulness}\\median (IQR)\end{tabular} \\
\midrule
Goal  & 5.67 (3.50 - 6.67)   & 6.67 (6.50 - 7.00)\\
Activity  & 6.67 (6.67 - 7.00)  & 6.333 (5.50 - 6.67)\\
Object   & 7.00 (6.67 - 7.00)  & 6.333 (5.00 - 7.00)\\
Location  & 6.67 (3.00 - 7.00) & 5.000 (3.33 - 6.67) \\
\bottomrule
% \toprule
% \begin{tabular}[c]{@{}l@{}}\textbf{Explanation}\\\textbf{Component}\end{tabular} & \begin{tabular}[c]{@{}l@{}}\textbf{Plausibility}\\(mean $\pm$ std)\end{tabular} & \begin{tabular}[c]{@{}l@{}}\textbf{Faithfulness}\\(mean $\pm$ std)\end{tabular} \\
% \midrule
% Goal  & 4.988 $\pm$ 1.951   & 6.568 $\pm$ 0.513\\
% Activity  & 6.457 $\pm$ 0.925  & 5.975  $\pm$ 1.162\\
% Object   & 6.420 $\pm$ 1.313  & 5.778 $\pm$ 1.343\\
% Location  & 4.963 $\pm$ 2.397 & 4.938 $\pm$ 1.677    \\
% \bottomrule
\end{tabular}
% \end{sc}
% \end{small}
\end{center}
% \vskip -0.1in
\vspace{-10pt}
\end{table}