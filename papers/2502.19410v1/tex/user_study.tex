To gain insights into how LLM explanations that differ in their structure and adaptivity influence user experiences, we conducted a lab-based user study. The study measured participants' perceptions of LLM recommendations and explanations while they watched videos of everyday interactions and received recommendations on a simulated smartwatch UI.

\subsection{Participants}
Forty-four participants (Male = 20, Female = 23, prefer not to say = 1; mean age = 38 years, std = 12.3 years) were recruited to participate in our in-person user study, a sample size comparable to prior work \cite{caine2016local}. 
Study participants were recruited through email invitations and social media platforms from an existing participant pool. Twenty participants had a graduate degree, 18 had a Bachelor's degree, 4 had some college experience, and 2 did not have any education past high school. Nine participants worked on AI-powered devices or products, 11 used AI-powered devices or services at least once a week, 9 used them at least once a month, 9 had used them a few times, and 6 had never interacted with AI-based devices or services. The entire study lasted about one hour, and each participant was compensated \$75 upon completion of the study.


\subsection{Task}
During the study, participants were asked to watch a 30-second Ego4D video on a standard desktop computer and were told that the videos they were watching were captured by a camera embedded in a pair of smart glasses that were worn on a person’s head and naturally approximated the visual field of the wearer of the glasses. We instructed participants to imagine that they were the one wearing these smartglasses and they were performing the activities shown in the video. They were also told that while they were imagining performing these activities, their personal AI assistant would try to predict their next intent and recommend a digital action, which was pushed to their smartwatch. The smartwatch UI was displayed next to the video on the desktop computer. Participants were asked to evaluate whether the recommendation met their needs (i.e., as the wearer of the glasses in the video) and then choose to accept or dismiss the AI's recommendation on the smartwatch UI by clicking on it (Figure \ref{study_interface}).

We opted to build a web-based interface that simulated smartwatch UI for two reasons. 
First, replicating a wide variety of real-life scenarios in a lab environment would be difficult. Instead, the existing Ego4D dataset provided a diverse set of pre-recorded situations.
Second, our recommendation pipeline, which employed large transformer-based models (i.e., 
% LaViLa, DETR
vision-language models, GPT-4), had slow response times. Deploying it on a smartwatch for real-time video processing would cause significant latency (\textasciitilde tens of seconds delay), which would have influenced participant satisfaction.



\begin{figure*}[t]
  \centering
\includegraphics[width=0.65\linewidth]{figures/interface.png}
  \caption{The interface that participants saw on the desktop computer. On the left, they could watch the 30-second Ego4D video. They could chose whether to accept or dismiss the AI’s recommendation on the smartwatch UI on the right.}
  \label{study_interface}
\end{figure*}



\subsection{Conditions}
A within-subject design was used with four conditions that manipulated whether and how the explanations were displayed (Figure \ref{fig:watch}). A balanced 4 $\times$ 4 Latin Squares design was used for condition counterbalancing \cite{mackenzie2013human}. A UX designer who is one of the authors standardized the smartwatch UI design 
%for optimal space usage 
using Figma.

\begin{itemize}
    \item In the \textbf{no explanation} condition (Figure \ref{fig:T0}), participants received recommendations without any explanation on the smartwatch UI. 
    \item In the \textbf{always-on unstructured explanation} condition (Figure \ref{fig:T1}), participants received recommendations along with an unstructured explanation on the smartwatch UI. The number of words in the unstructured explanation were limited to ensure they occupied the same amount of screen space as the structured explanations (i.e., the following two conditions), while allowing participants to scroll up and down to view more text. The prompt used to generate the unstructured explanations can be found in Appendix \ref{app:prompt_baseline}. 
    \item In the \textbf{always-on structured explanation} condition (Figure \ref{fig:T2}), participants received a recommendation along with a structured explanation that was comprised of four components: a goal, an activity, an object, and a location. 
    \item In the \textbf{adaptive structured explanation} condition, structured explanations were provided adaptively depending on the recommendation's confidence level. The explanation was displayed automatically when the recommendation confidence was low (Figure \ref{fig:T3}), otherwise, it was hidden by default (Figure \ref{fig:T3_high0}) and a toggle button enabled participants to see it ((Figure \ref{fig:T3_high1})). 
\end{itemize}




\begin{figure}[t]
  \centering
  \subfigure[No explanation]{
\includegraphics[width=0.28\linewidth]{figures/T0.png}
\label{fig:T0}
    }
    \hspace{6pt}
    \subfigure[Always-on unstructured explanation]{\includegraphics[width=0.28\linewidth]{figures/T1.png}
    \label{fig:T1}
    }
    \hspace{6pt}
    \subfigure[Always-on structured explanation]{
\includegraphics[width=0.28\linewidth]{figures/T2.png}
\label{fig:T2}
    }\\
    \subfigure[Adaptive structured explanation (low confidence)]{
    \includegraphics[width=0.28\linewidth]{figures/T3.png}
\label{fig:T3}
    }
    \hspace{6pt}
    \subfigure[Adaptive structured explanation (high confidence - default)]{
    \includegraphics[width=0.28\linewidth]{figures/T3_Confidence_High0.png}
\label{fig:T3_high0}
    }
    \hspace{6pt}
    \subfigure[Adaptive structured explanation (high confidence - toggled)]{
    \includegraphics[width=0.28\linewidth]{figures/T3_Confidence_High1.png}
\label{fig:T3_high1}
    }
  \caption{The Smartwatch UI designs for the four conditions in the user study. (d)-(f) illustrate different variations of the adaptive structured explanation condition.}
  \label{fig:watch}
  % \vspace{-10pt}
\end{figure}


For the adaptive structured explanation condition, we categorized recommendation confidence levels into a binary classification as high or low, depending on whether the $c_{hybrid}$ level was “low”/“very low” or “high”/“very high". These categorizations were based on the findings from our technical evaluation (Section \ref{sec:tech_eval}), where we observed a clear gap in the human estimation of the recommendation correctness likelihood based on the confidence levels ($c_{hybrid}$). When $c_{hybrid}$ was ``low'' (median correctness rating = 3.5) or ``very low'' (median correctness rating = 3.0), coders rated the correctness likelihood much lower compared to recommendations with $c_{hybrid}$ being ``high'' (median rating = 6.0) or ``very high'' confidence levels (median rating = 5.5). \rv{From the same pool of task instances used in the technical evaluation described in Section \ref{sec:tech_eval}}, we chose 20 task instances where the recommendation confidence $c_{hybrid}$ was “low”/“very low” and 20 instances where the confidence $c_{hybrid}$ was “high”/“very high.” To ensure the representativeness of the selected instances, we sampled all task instances based on the data distributions across the 27 activity context categories in the video dataset.





\subsection{Study Procedure}
Upon arrival at our lab, participants were asked to complete a consent form and an initial survey about their demographics and experience with AI. 

Participants then completed 40 trials, which were divided into four blocks, one block for each condition. Before beginning each block, there were two tutorials that were identical to the actual trials but during which no user data was collected. Each block then consisted of 10 trials, with half of them having low confidence recommendations and half having high confidence recommendations. The order of all 10 trials within each block was randomized across participants.  During each trial, participants watched a 30-second video on the desktop computer. At the end of the video, a recommendation for a digital action and an explanation (depending on the assigned condition) was displayed on the simulated smartwatch UI. Participants then chose whether to accept or dismiss the AI’s recommendation. After each block, participants completed a survey about their experience with the AI's recommendations and explanations.

Upon completion of all four blocks, participants completed a final survey and answered interview questions to understand what they liked or disliked about each AI explanation condition, when they would like to see an explanation, what information they would like to see in the explanation, and how the AI explanation could be improved to better meet their needs. 

\subsection{Metrics}
% Several metrics were computed to understand participants’ experiences and perceptions when presented with the AI explanation conditions. First, we computed the following the time to action (i.e., the time taken by a participant from the end of the video until they accepted or dismissed the AI recommendation) and the acceptance rate (i.e., the percentage of tasks where the participant chose to accept the AI recommendation). At the end of each block, we also asked participants about their mental load (while making the decision, understanding the recommendation, and reading the explanation), trust, satisfaction with the AI, their understanding of the AI, and their satisfaction with the AI explanations, using 7-point Likert scales. At the end of all four blocks, participants also ranked the four conditions based on their overall preferences for them. 

Several metrics were computed to understand participants’ experiences and perceptions when presented with the AI explanation conditions. First, we computed the following objective measures:
\begin{itemize}
    \item \textbf{Time to action}: The time taken by a participant from the end of the video until they accepted or dismissed the AI recommendation.
    \item \textbf{Acceptance rate}: The percentage of tasks where the participant chose to accept the AI recommendation
\end{itemize}

At the end of each block, we asked participants about the following subjective measures using 7-point Likert scales:

\begin{itemize}
    \item \textbf{Mental load}: We measured participants' mental load while making the decision, understanding the recommendation, and reading the explanation via three questions: 
    \begin{itemize}
        \item ``\textit{Overall, how much mental effort did you spend on deciding whether to accept or dismiss the AI recommendation?}'' 
        \item ``\textit{Overall, how much mental effort did you spend on understanding how the AI model makes recommendations based on the context in the video?}'' 
        \item ``\textit{How much mental effort did you spend on reading the AI model’s explanation of the recommendation?}''
    \end{itemize}
    \item \textbf{Trust in AI}: Participants' trust in AI was calculated by the average rating of the following three questions: \begin{itemize}
        \item ``\textit{I have faith that the AI model would be able to cope with all different situations.}'' 
        \item ``\textit{I am confident that the AI model can make good recommendations.}'' 
        \item ``\textit{Recommendations made by the AI model are likely to be reliable.}''
    \end{itemize}
    \item \textbf{Understanding of AI.}: This was also computed by averaging participants' responses to three questions:
    \begin{itemize}
        \item ``\textit{I understand how the AI model works to predict my next digital action based on the context in the video.}'' 
        \item ``\textit{I can predict how the AI model will behave based on the context in the video.}'' 
        \item ``\textit{I feel the AI model is transparent in communicating how the recommendations were made based on the context in the video.}''
    \end{itemize}
    \item \textbf{Satisfaction with AI}: ``\textit{I’m satisfied with the AI model’s recommendation.}''
    \item \textbf{Satisfaction with AI explanations.}: This metric differed from ``satisfaction with AI'' as it specifically measured how participants felt about the \textit{explanation} provided by the AI for its recommendations, i.e.,  ``\textit{I’m satisfied with the AI model’s explanation of  the recommendation.}''
\end{itemize}

At the end of all four blocks, participants also ranked the four conditions based on their \textbf{overall preferences} for them. 

% For objective measurements, we further categorized each AI recommendation as either correct or incorrect, to see whether the correctness of each AI recommendation influences the users' response time and acceptance rate. We manually labeled the correctness of the AI recommendations. This was necessary because the annotations of the Ego4D-DI dataset were made by the actual person in the video, yet in our experiment, we asked our participants to "imagine" themselves as the person in the video, thus they lacked the rich context to replicate the exact actions of the person in the video.


\subsection{Analysis Methods}

Mixed-effect regression models were used to analyze the aforementioned metrics. For the time to action and acceptance rate, each participant and trial were random effects, while the conditions were fixed effects. For the mental load, trust, satisfaction with the AI, understanding of the AI, and the satisfaction of the AI explanations, each participant was a random effect. For the analysis of participants' rankings of the four conditions, a cumulative link mixed-effects model was used to analyze the ordinal response variable. The results of these models were interpreted through the estimated coefficient values for the fixed effect variables. 

The interview data was analyzed by two coders using thematic analysis \cite{braun2006using}. Two authors iteratively discussed and developed a codebook. The inter-rater reliability was calculated using Cohen's kappa \cite{mchugh2012interrater}, where $\kappa=0.82$ ($\text{SE}=0.055, \text{95\% CI}=[0.71, 0.93]$). The final themes were developed and refined over multiple iterations of discussion. 
