We analyzed participants' time to action, acceptance rate, subjective perceptions, and interview data to better understand their experiences with the different AI explanation conditions. 
% For each objective metric (i.e., time to action, acceptance rate), we manually annotated the correctness of each AI recommendation and conducted a deeper analysis based on AI correctness in case the recommendation quality moderated users' behavior.


\subsection{Quantitative Results}

\subsubsection{Time to Action}

\begin{figure}[t]
  \centering
% \subfigure[Time to Action]{
    \includegraphics[width=0.8\linewidth]{figures/toa+ar.png}
% \label{fig:time_to_action}
%     }
%     \subfigure[Acceptance Rate]{
%     \includegraphics[width=0.48\linewidth]{figures/result_obj_acceptance.png}
  \caption{Participants’ time to action and acceptance rate for each AI explanation condition. The error bars represent standard errors. (*: $p$ < 0.05; **: $p$  < 0.01; ***: $p$  < 0.001)}
  \label{fig:time+acceptance}
\end{figure}


Our results showed the presentation of the explanations impacted user's time to action. Compared to the no explanation condition, providing explanations significantly increased the time participants took to select an AI recommendation (always-on unstructured: $\beta=4.6467, p<0.001$, always-on structured: $\beta=1.9341, p<0.05$, always-on unstructured: $\beta=2.7492, p<0.01$). But among the three conditions with explanation, structured explanations---both always-on structured explanations ($\beta=-2.7126, p<0.01$) and adaptive structured explanations ($\beta=-1.8975, p<0.05$)---significantly reduced participants' time to action compared to unstructured explanations (Figure \ref{fig:time+acceptance}). 





% After dividing the AI recommendations by their correctness, we observed that the same pattern held for correct recommendations, i.e., the time to action for the always-on structured explanations ($\beta=-3.013, p=0.007$) and adaptive structured explanations ($\beta=-1.897, p=0.096$) was faster than the unstructured explanations. However, for incorrect recommendations, a faster time to action was only marginally observed with always-on structured explanation ($\beta=-2.266, p=0.073$) compared to the unstructured condition.


\subsubsection{Acceptance Rate}
Our results found that always-on structured explanations resulted in a higher acceptance rate compared to the other three conditions (compared with no explanation: $\beta=0.0675, p<0.05$, with unstructured explanations: $\beta=0.0712, p<0.05$, and with adaptive structured explanations: $\beta=0.0584, p<0.05$; Figure \ref{fig:time+acceptance}). 

% After categorizing AI recommendations based on their correctness, there was a significant main effect of recommendation correctness on the acceptance rate ($\beta=0.290, p<0.001$). We found that always-on structured explanations led to a higher acceptance rate compared to unstructured
% explanations ($\beta=0.104, p=0.006$), but only for correct recommendations. When the recommendations were incorrect, there was no significant difference in the acceptance rate. 
% This suggests that users may prefer more detailed information, such as text, when the recommendations are incorrect.




 \subsubsection{Mental Load}
 We found no significant differences in participants' mental load when deciding or understanding recommendations (Figure \ref{fig:result_sub}), however, participants rated their mental load as significantly lower when reading structured explanations (i.e., always-on structured: $\beta=-0.8182, p<0.01$, adaptive structured: $\beta=-0.5909, p<0.05$) than when reading unstructured explanations, which aligns with the time to action findings.

 \begin{figure*}[t]
  \centering
\includegraphics[width=\linewidth]{figures/result_sub.png}
    
% \subfigure[Mental Load]{
%     \includegraphics[width=0.4\linewidth]{figures/mental_load_sub.png}
%     \label{fig:mental_load_sub}
% }
% \subfigure[Trust, Satisfaction, Understanding, Satisfaction with the AI Explanation]{
%     \includegraphics[width=0.4\linewidth]{figures/AI_sub.png}
%     \label{fig:AI_sub}
% }
  \caption{Participants’ reported mental load, trust, satisfaction, understanding, and satisfaction with the AI Explanations. The error bars represent standard errors. (*: $p$ < 0.05; **: $p$  < 0.01; ***: $p$  < 0.001)}
  \label{fig:result_sub}
\end{figure*}

\subsubsection{Trust in AI}
We found that both the always-on unstructured explanations ($\beta=0.4091, p<0.05$) and always-on structured explanations ($\beta=0.4545, p<0.01$) led to higher levels of trust when compared to the no explanation condition (Figure \ref{fig:result_sub}). There was no difference between the always-on unstructured, always-on structured, and adaptive structured explanation conditions.

\subsubsection{Satisfaction with AI}
When presented with always-on unstructured and always-on structured explanations, participants were both more satisfied with the recommendations given by the AI compared to when no explanation was provided (always-on unstructured: $\beta=0.4773, p<0.05$; always-on structured: $\beta=0.7045, p<0.001$; Figure \ref{fig:result_sub}). The use of adaptive structured explanations did not result in participants being more satisfied with the AI's recommendations compared to when no explanation was provided, and even led to significantly lower satisfaction compared to the always-on structured explanation condition ($\beta=-0.4318, p<0.05$).

\subsubsection{Understanding of AI}
When participants were provided with an explanation, they had a better understanding of the rationale behind the AI recommendation than when they were not (e.g., always-on unstructured explanations: $\beta=1.1136, p<0.001$, always-on structured explanations: $\beta=1.0303, p<0.001$, always-on structured explanations: $\beta=0.8258, p<0.001$; Figure \ref{fig:result_sub}). There was no significant difference between the always-on unstructured, always-on structured, and adaptive structured explanation conditions, suggesting that structuring and adaptively presenting the unstructured explanation did not negatively impact participants' understanding of the AI. 


\subsubsection{Satisfaction with AI explanations} 
We also found that adaptive structured explanations were less satisfying compared to always-on unstructured explanations ($\beta=-0.5227, p<=0.05$; Figure \ref{fig:result_sub}). Though not statistically significant, always-on structured explanations were rated as marginally less satisfying than always-on unstructured explanations ($\beta=-0.5000, p=0.053$). 

\subsubsection{Overall Preferences}
In terms of participants' overall preferences for the four conditions (Figure \ref{fig:overall_pref}), both the always-on unstructured explanations ($\beta=-1.1543, p<0.01$) and the adaptive structured explanations ($\beta=-0.9242, p<0.05$) were ranked higher than the no-explanation condition. 
However, the difference in ranks between the adaptive structured explanation condition and the no explanation condition was not statistically significant ($\beta=-0.6752, p=0.074$).
Additionally, no significant difference was found between the three explanation conditions. 
% While unstructured explanations were most frequently ranked as most preferred (19 out of 44).
% summing the two structured explanations resulted in them being the most preferred by 20 out of 44 participants, suggesting that structured explanations were preferred at a similar level to that of unstructured explanations.

\begin{figure}[t]
  \centering
\includegraphics[width=\linewidth]{figures/result_rank.png}
\vspace{-10pt}
  \caption{Participants’ preference rankings for the four AI explanation conditions.}
  \label{fig:overall_pref}
  % \vspace{-10pt}
\end{figure}





\subsection{Qualitative Results}

Seven themes were identified within the interview feedback. Some of the themes helped explain why participants preferred some conditions over others, whereas others speculated about other ways that explanations could be improved. Each theme is elaborated in the following sections.

\subsubsection{Usability} 
Usability referred to the amount of effort or time to interact with interface, i.e., whether the interface was easy and quick to use, or required more effort. Participants generally wanted to perform minimal actions and reading on the interface, with easy viewing and decision-making. For example, some participants noted that they would only want explanations when they had enough cognitive capacity, e.g., ``\textit{If it were detecting locations or the amount of activity I was doing, then I'd probably prefer the simpler interface. If I were driving or doing something active, it wouldn’t be helpful for me to stop and read that explanation, so I’d dismiss it.}'' (P13) 

Twenty participants valued the high usability of the no explanation condition, saying the interface was easy and fast to use, e.g., ``\textit{It allowed me to react fastest}'' (P10). In conditions with explanations, eight participants mentioned that the information provided by always-on structured explanations was easy to access, e.g., ``\textit{The explanations were automatically expanded}'' (P13). The adaptive structured explanations received mixed feedback. Fifteen participants found them easy and fast to use (e.g.,``\textit{[They] were very simple and to the point}'' (P2)). On the other hand, usability concerns were raised by 30 participants who disliked the extra effort needed to interact with the interface (e.g.,``\textit{You have to hit an extra button, which just seems like an unnecessary step}'' (P29)).


\subsubsection{Sufficiency}
Participants also noted that sufficiency, or the amount of information required to understand AI-generated recommendations or explanations, was important. This included both wanting more information and removing unnecessary details. Thirty participants mentioned their desire for more transparency in the AI decision process in the no explanation condition to help them understand AI decision better (e.g., ``\textit{it doesn't have any explanation}'' (P37)). Twenty-four participants favored the comprehensive details provided by the unstructured explanations, e.g., ``\textit{it gives you a more comprehensive explanation as to what's happened}'' (P9). Regarding the structured explanations, 18 participants thought the four icons contained sufficient information, whereas 15 participants suggested that the icons could have been less redundant and location information was unnecessary. 
For example, ``\textit{the feedback I got just like seemed redundant. It just kind of felt like a waste of time to read down a list of four points that often had the same information in them}'' (P33).


\subsubsection{Readability}
Readability referred to the ease of reading and viewing text or icons presented on the interface (e.g., ease of viewing information, too much text, low cognitive load). The comparison primarily focused on unstructured versus structured explanations. Thirty-one participants noted that the unstructured, free-text explanations resembled human speech and were easy to understand (e.g., ``\textit{it would be if like a human was just explaining it to me, like, oh because I saw that this and so I predicted your in this environment trying to do this ... I feel that's easy to understand}'' (P40). The downside, as complained by 30 participants, was that there was too much text making the explanations difficult to read (e.g., ``\textit{It was hard to read this block of text}'' (P7).

On the other hand, 8 participants mentioned that the structured explanations were easy to view (e.g., ``\textit{it was just really nicely laid out, organized}'' (P22). Twenty-three participants, however, found the icons and the logic between them difficult to understand (e.g., ``\textit{I had to like keep reminding myself what they meant}'' (P1) and ``\textit{those four icons didn't always connect}'' (P12)).




\subsubsection{User Control}

Participants also highlighted the importance of the amount of control user they had over the interface and their choices (e.g., giving users more options, allowing the user to decide when to view information). They believed that the AI ``\textit{should give you that option ... ask you, would you like more suggestions? }'' (P21). Twenty-eight participants expressed a desire for more control over explanation length, appreciating the greater user control offered in the adaptive structured explanation condition to toggle explanations on or off (e.g., ``\textit{[this condition] was good in a way that they have an option, have it on or have it off}'' (P9)).


\subsubsection{Accuracy}
Accuracy concerns related to the contextual relevance of AI-generated recommendations and information. 
Twenty-four participants wanted more accurate recommendations for certain situations and sometimes found that recommendations or explanations didn’t match the context (e.g., ``\textit{The context of the video was totally different from the action that I'm kind of like being asked to perform}'' (P30)).


\subsubsection{Multi-Modality}

 Participants thought that explanations should support options for other modes of accessibility, such as information presented in multiple languages, audio input/output, or visual display. This was mentioned by 19 participants as a potential way to improve the experience. For instance, one of the videos shown in the study contained dialogue spoken in Russian, which provided direct information on the next recommended action, but the AI assistant failed to capture it. 
Another example came from P37, who explained, ``\textit{if I don't need to read but just show me some pictures or images like for example, for objects you don't need to mention like what object it is, you just put what I'm looking at the picture here.}''
 
\subsubsection{Personalization}

Lastly, 43 participants showed interest in personalization. They thought that explanations should contain information customized for personal preferences, such as providing recommendations based on daily routine and previous actions, displaying explanations for unfamiliar tasks, and being less frequent for tasks users are already familiar with. As P12 suggested, ``\textit{one thing I think would be helpful to add [to improve the explanation] could be like historical action.}''


\subsection{Summary}

In this section, we bridge the qualitative and quantitative results by showing how the qualitative feedback from participants provides context for interpreting the quantitative results. 



\textbf{Structured explanations are quick and easy to use, but fall short in providing sufficient details and the naturalness of human-like language.}
%\textbf{The high usability of structured explanations lead to quicker time to action and higher acceptance rates.}
The quantitative results showed that both always-on structured explanations and adaptive structured explanations reduced participants' time to action and mental load when reading the explanations, compared to unstructured explanations. Interview feedback supported these findings, as participants mentioned that structured explanations were clear to view and access (i.e., high usability), allowing them to quickly glance at the information, which contributed to shorter time to action. This may also explain why always-on structured explanations also resulted in a higher acceptance rate of AI recommendations compared to all other conditions, i.e., they had less time to overthink and instead relied on the easily-accessible icons, which reinforced their belief that the recommendation seemed trustworthy.


Despite these advantages, participants rated the always-on structured explanations (marginally) and adaptive structured explanations as less satisfying than the always-on unstructured explanation. During the interviews, participants mentioned that structured explanations were clear to view, but they struggled to remember the meaning of the icons and their logical connection. Unstructured explanations, on the other hand, offered more comprehensive and human-like information. 
\rv{The process of mapping abstract icons to their semantic meanings likely imposed an extraneous cognitive burden, which may have led participants to value ``sufficiency'' and ``readability'' more when evaluating whether an explanation was satisfactory, thus leading to higher satisfaction ratings for unstructured explanations.}


\textbf{Adaptive structured explanations offer users control over the interface, but the (inappropriate) extra toggling hindered effective human-AI interaction.}
In comparison to the no explanation condition, all three explanation conditions led to a higher understanding of the AI. However, the adaptive structured explanation condition---where participants needed to proactively interact with the interface to view details---was less effective in showing the benefits of providing AI explanations. Specifically, both always-on unstructured and always-on structured explanations increased participants' trust and satisfaction in the AI's recommendations and were preferred over the no explanation condition, but this was not observed for the adaptive structured explanations. The qualitative feedback revealed that while participants appreciated having control over when to view explanations, the additional toggle action felt like a redundant barrier that impaired the smoothness of  their interaction with the interface, thus \rv{shifting their focus from quickly grasping the information to managing the interface.}
% disrupting their ability to quickly grasp the information. 


