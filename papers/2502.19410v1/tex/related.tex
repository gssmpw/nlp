Our work builds upon three areas of relevant literature, i.e., existing XAI techniques to explain LLMs, empirical studies on AI explanations, and user interface design challenges for ultra-small devices.

\subsection{Techniques to Explain Large Language Models}

A variety of XAI techniques have been developed to increase the interpretability of AI models. Conventional XAI methods can be categorized into model-specific methods tailored to particular model types, and model-agnostic methods applicable across various model types. Training interpretable models such as rule-based models and generalized additive models \cite{jung2017simple, lakkaraju2016interpretable, wang2015falling} are examples of model-specific methods. On the other hand, examples of model-agnostic methods include generating feature contributions \cite{lundberg2017unified, ribeiro2016should, simonyan2013deep} or searching for prototypes or counterfactual instances \cite{kim2016examples, wachter2017counterfactual}.


The emergence of LLMs raised unique challenges when applying traditional XAI methods to explain them due to the size of their training datasets and non-deterministic outputs \cite{liao2023ai}. 
Open-source LLMs like Meta’s LLaMA models \cite{touvron2023llama} provide broader transparency for AI researchers to uncover their internal states. For instance, \cite{voita2023neurons} identified when neurons within LLMs were activated, while \cite{zou2023representation} analyzed how high-level cognitive processes were represented in LLM neurons. \cite{zhu2023physics} explored how LLMs encoded knowledge in their embeddings. 
On the other hand, closed-source LLMs, such as GPT \cite{achiam2023gpt4} and Google Gemini \cite{team2023gemini}, pose more challenges due to the restricted access to their internals. Regardless of the closed or open nature of LLMs, one common approach to generate explanations is to prompt the model to provide “self-rationalizations” alongside predictions \cite{wei2022chain, kojima2022large, lampinen2022can, yao2023beyond}. As these LLMs are based on transformer architectures, they can capture complex relationships between input data and generate output through the attention mechanism, to self-rationalize their recommendations in a way that appears logical to humans. Yet, some work has criticized these self-rationalized explanations as being unreliable and unfaithful to the actual reasoning process \cite{turpin2024language, ye2022unreliability}. To address these limitations, researchers have pursued approaches including iteratively prompting the LLM for self-feedback and refinement \cite{creswell2022selection, madaan2024self}, and grounding an LLM in external knowledge bases \cite{zi2023ierl, chen2023lmexplainer}.


While the natural language modality of LLMs' responses opens up opportunities for explanations that are understandable to non-experts, they are often too verbose to be grasped in a quickly digestible manner. This issue is particularly pronounced on ultra-small devices, and our work aims to address it.


\subsection{User-Centered Design and Evaluation of AI Explanations}

Empirical research has focused on understanding how users understand AI explanations \cite{wang2021explanations, wang2022effects} and trust AI models \cite{zhang2020effect, wang2022effects, wang2023watch}, as well as users' overall task performance \cite{bansal2021does, lai2020why, liu2021understanding, wang2023effects, wang2024human}. This research has also spurred novel design processes that identify users' explainability needs to inform XAI design (e.g., identifying the questions that users commonly ask AI systems \cite{rebanal2021xaigo}). In addition to explaining traditional machine learning models, more recently, user-centered design approaches have been applied to explain large generative AI models. For example, Sun et al. \cite{sun2022investigating} identified explainability needs unique to generative AI for code compared to traditional discriminative machine learning models, and proposed four XAI features: AI documentation, uncertainty indicators, attention visualizations, and social transparency. 

On the other hand, research on designing and evaluating XAI methods for particular digital devices is lacking. \rv{Developing AI explanations for specific interfaces often involves trade-offs between usability and transparency. Springer and Whitaker \cite{springer2019progressive} highlighted that transparency can be distracting and that users might benefit from initially simplified explanations.} Chromik and Butz \cite{10.1007/978-3-030-85616-8_36} surveyed a list of XAI publications and identified design principles for human interaction with explanation user interfaces, including naturalness, responsiveness, flexibility, and sensitivity. Xu et al. \cite{xu2023xair} proposed design principles for displaying XAI information in floating windows on AR devices. \rv{They recommended simplifying explanations by selecting the appropriate content, and triggering explanations only when users have enough capacity, are unfamiliar with the outcome, or when the model is uncertain.}
Our work builds on this prior research but focus on ultra-small devices, where there is an urgent need for highly glanceable explanations.


\subsection{User Interface Design for Ultra-Small Devices}

The ultra-small screen sizes on devices such as smartwatches raises key usability issues for the layout of visually rich content such as verbose text \cite{10.1007/11555261_24, spalink-2002-small, https://doi.org/10.1002/hfm.20733}. To address these challenges, researchers have recommended design guidelines to condense information while maintaining clear user understanding \cite{8b67e137-384d-3dd3-a12e-1ab945cfa108, albers2020information}. For example, a navigation interface should only display an upcoming route as an arrow \cite{10.1145/506443.506514}. Others have explored adaptive menus and have found that users preferred a cloud menu, where predicted items were arranged in a circular tag cloud \cite{10.1145/3237190}. Work by Rahman and Muter showed that presenting continuous text sentence-by-sentence within small display windows offered users the ability to reread one sentence at a time and was found to be as efficient as conventional reading \cite{doi:10.1518/001872099779577264}. In summary, these common approaches included converting lengthy text into graphical diagrams or icons \cite{doi:10.1177/154193120605000508, graphologue}, adaptively prioritizing highly-relevant content \cite{10.1145/1357054.1357249, doi:10.1177/154193120104500602}, progressively displaying content \cite{10.1145/238386.238582, doi:10.1518/001872099779577264}, or utilizing multiple modalities such as voice and gestures \cite{10.1145/2807442.2807500, 10.1145/3010915.3010983, Lee2017}. Guided by these existing design ideas, we selected two major dimensions to explore fitting verbose content on ultra-small screens, i.e., \textit{spatially}, by structuring it into easily viewable formats, and \textit{temporally}, by adaptively displaying the content.