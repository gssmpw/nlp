Findings from our user study drive the design implications for creating glanceable AI explanations on ultra-small devices that meet users' expectations and needs more effectively.  In this section, we discuss these design implications as well as the limitations of our work.


\subsection{Design Implication on Structuring LLM Explanation Text into Components}


The trust gain resulted from presenting always-on structured explanations may be superficial trust based on simplicity rather than understanding. It’s therefore crucial to build genuine trust and avoid blind reliance on oversimplified icons. Coupled with participants’ desire for sufficient and readable details, accurate summarization and iconification play a critical role in ensuring appropriate understanding of AI recommendations. Designers should carefully balance brevity with clarity when creating icons and summaries for glanceability. As an example, Zender and Mejía \cite{zender2013improving} suggest that adding more symbols can enhance comprehension by providing context and narrow the focus. For instance, adding a ``bookshelf’’ icon to a ``library’’ icon can improve understanding compared to using just a ``man reading’’ icon. They also encourage designers to test icons through multiple iterations and learn from user feedback for improvement. 

There is also likely a continuum of text verbosity beyond simply hiding and displaying explanations that adapts to different tasks, e.g., ``progressively disclosing’’ a continuum of explanation levels \cite{springer2019progressive}. Computational models could be developed to predict the appropriate level of detail based on user preferences, current contexts, and interaction histories. Alternatively, from a design perspective, users should be given control over the amount of explanation detail they receive, ranging from icons and diagrams to unstructured text. 




\subsection{Design Implication on Adaptively Presenting Structured Explanations}

% Our analysis also showed that the quicker time to action and higher acceptance rates found with always-on structured explanations mainly held when the AI was correct. When the AI made incorrect recommendations, users may become confused and engage in more careful thinking processes. More information should have been provided to assist them in making decisions. When the AI was wrong but overly confident, however, the adaptive explanation based on confidence level estimation failed to provide sufficient detail.


Our analysis highlighted that participants preferred AI to be as transparent as possible and found that inefficient adaptive structured explanations were less favored compared to always-on explanations. Thus if adaptive explanations are to be used, it is crucial to carefully select the optimal timing to present and hide explanations. Based on prior literature \cite{xu2023xair}, explanations should be displayed automatically when there is a need to manage AI errors or to resolve user surprise/confusion. More sophisticated methods need to be developed to implement adaptive explanations effectively. 
Model developers should explore more accurate indicators of AI correctness to detect when the AI makes mistakes.
For situations require the understanding of user states, building a personalized user mental model based on interaction logs to predict when users are confused, unfamiliar with the outcome, or uncertain about their own choices \cite{ma2023should, ma2024you, wang2022will}, could further improve the effectiveness of adaptive explanations.




\subsection{Limitations and Future Work} 

This research has several limitations. The results of our study are limited by the specific type of AI assistance scenario (i.e., goal-oriented contextual action recommendation)
% , \rv{the participant sample primarily consisting of highly-educated individuals}, 
and the fact that participants watched third-party videos rather than interacting with an actual working system on a smartwatch. The controlled in-lab setting may have also made participants deliberately curious about seeing more information, \rv{whereas in a longitudinal study conducted in real-world settings, they may spend less time deciding what to do next, and their user preferences may evolve over time}.
% We advise caution in generalizing these results to other contexts.caution readers do not over-generalize our findings to other contexts.
Moreover, our LLM recommendation pipeline used only video frames as input. In practice, other signals such as digital states, audio, eye tracking, text recognition, EMG, IMU could all be utilized as input for the modality encoder models and mapped into the LLM embedding space \cite{moon2023anymal} to enable more accurate action inference. 
\rv{Furthermore, the generated LLM recommendations carry potential risks including biases against underrepresented groups and accessibility challenges for populations with disabilities or limited technological proficiency.}
% Last but not least, the collection and analysis of egocentric video data for LLM-based recommendations in real-world applications may raise privacy concerns. To enhance the security and trustworthiness of LLMs, the use of such sensitive user data necessitates robust solutions that ensure user consent, data anonymization, and secure data storage \cite{li2024llm}.


A promising future direction to explore for effectively presenting AI explanations on ultra-small devices is to flexibly leverage multiple modalities and selectively summarize information from various input signals \cite{miller2019explanation}.
With more input signals available such as audio and eye tracking, explanations could incorporate much richer information than the four contextual concepts currently used (i.e., object, activity, goal, location). For example, the AI explanations could include the user's historical preferences, dialogues with friends, or current health state.
In addition to text and icons, there are alternative modalities for displaying explanations that can reduce mental demand while improving the naturalness of information delivery. For example, on interfaces like AR smartglasses, which support overlay displays, real-world objects can be highlighted and overlaid onto the physical environment to enhance understanding \cite{du2020depthlab}. 
