In this research, we explored how to effectively present glanceable LLM explanations on ultra-small devices. We proposed that LLM explanations should be \textit{structured} using defined contextual components during prompting to achieve spacial glanceability, and \textit{adaptively} presented based on confidence levels to achieve temporal glanceability. 
We conducted a user study to evaluate the effectiveness of structuring and adaptively presenting explanations on user experiences and perceptions of an AI assistant system using a simulated smartwatch UI. We found that structured explanations reduced users' time to action and lowered perceived mental load when reading the explanations. Always-on structured explanations led to higher acceptance rates of the AI recommendations.  But structured explanations also lacked sufficient and readable details and were thus less satisfying than unstructured explanations. Additionally, while adaptive structured explanations offered participants more control over the interface, they introduced a redundant interaction step and generally hindered effective interaction with the AI, highlighting the need for improvements in the timing for adaptive presentation of AI explanations.
Our study should thus serve as a foundational starting point to understand how users prefer to view AI explanations on ultra-small devices and encourage future research and technical developments that promote responsible AI for users of AI-based personal assistants.