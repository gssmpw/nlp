Artificial Intelligence (AI) advancements over the past decades have led to the development of AI-based personal assistants, which have become a popular way for human users to interact with digital devices and data. 
More recently, Large Language Models (LLMs) such as the GPT \cite{achiam2023gpt4} and LLaMa \cite{touvron2023llama} series have shown remarkable performance in generation tasks, which has led to their growing use as a method to make everyday contextual recommendations for AI-based personal assistants \cite{li2024llm, cosentino2024towards}. 
In addition, to ensure transparency and control during human-AI interaction, there has been a growing interest in Explainable AI (XAI) techniques that can help users understand and trust the AI assistance they receive \cite{ribeiro2016should, zhang2020effect, wang2021explanations, rebanal2021xaigo, xu2023xair}. In the context of LLMs, those models based on transformer architectures are capable of generating explanations that rationalize their reasoning process %alongside their recommendations 
using natural language \cite{wei2022chain, kojima2022large}.




An emerging manifestation of AI-based personal assistant has been on ultra-small devices, such as smartwatches, smartglasses \cite{Meta_2024_orion, google_glass}, and on-cloth gadgets \cite{ai_pin}, to offer low-friction assistance that is seamlessly integrated into the physical world. These devices rely on vast amount of user contextual data gathered from their sensors (e.g., egocentric video, gaze tracking, and audio) to predict the goal of the user's next action and generate action recommendations \cite{jonker2020role}, such as launching an application, to help users efficiently complete daily tasks.

Interacting with LLM recommendations and explanations on such ultra-small devices can, however, be challenging for several reasons.
First, LLM self-explanations are often verbose and time-consuming to comprehend \cite{graphologue}, while the constrained screen space on ultra-small devices exacerbates the challenge of displaying verbose explanations. Real-world examples include smartwatches or Augmented Reality (AR) glasses, which have limited screen real estate that is available, and struggle to display long messages \cite{duchnicky1983readability}. 
% For AI explanations, incomplete information may even lead to misunderstandings or the misuse of an AI system’s recommendations.
Designers need to carefully condense the \textit{content} of an explanation to ensure it remains relevant and informative while not overloading the limited screen space.
Additionally, some tasks, such as providing route adjustment recommendations while driving, are inherently time-sensitive and consume a lot of the user's cognitive resources. Thus, it is crucial to optimize the \textit{timing} of explanations by presenting them only at moments when they would be most useful or when a user requests them to prevent unnecessary user interruptions \cite{xu2023xair}.



To overcome these challenges, this work aims to design glanceable explanations for a goal-oriented, contextual LLM action recommendation system for end-users using ultra-small devices.
We focused on improving \textit{spatial} and \textit{temporal} glanceability via two research questions:



\begin{itemize}
    \item \textbf{RQ1:} To improve the spatial glanceability of LLM explanations on ultra-small devices, how should LLM explanation text be \textit{structured}? 
    
    \item \textbf{RQ2:} To take a step beyond and further improve the temporal glanceability of LLM explanations on ultra-small devices, how should LLM explanations be \textit{adaptively} presented?
\end{itemize}



To answer these questions, we designed a web-based interface that enabled us to simulate an LLM recommending and explaining actions to a user when they were using an AI-based personal assistant embedded in a smartwatch, and we recruited participants to interact with this interface.
A subset of Ego4D \cite{grauman2022ego4d} videos featuring digital interactions in people’s daily activities was selected for evaluation. 
We used a learning approach based on Socratic Models \cite{zeng2022socratic} with the dataset, where pre-trained vision-language models 
% (e.g., LaViLa \cite{zhao2023learning} and DETR \cite{carion2020end}) 
were used to generate a linguistic summarization (e.g., user physical actions and detected objects) of a video input for downstream processing with an LLM, 
which then made inferences and generated action recommendations.



To improve spatial glanceability (\textbf{RQ1}), we converted an LLM’s verbose explanation text into concise concepts and intuitive icons that users could grasp at a glance. We adopted a Chain-of-Thought prompt strategy \cite{wei2022chain, kojima2022large} and broke down the inference process into steps. An LLM first summarized all possible contexts (i.e., the \textit{[activity]} the user is doing, the \textit{[object]} the user is interacting with, and the \textit{[location]} the user is in) from the output of the vision-language models. Then, the LLM inferred the short-term \textit{[goal]} that the user wanted to achieve and provided a digital action recommendation that the user could take as a next step. Instead of displaying lengthy text, the explanation only included the four structured components.


To improve temporal glanceability (\textbf{RQ2}), we focused on situations when the intelligent system became uncertain and needed user confirmation \cite{xu2023xair}. 
We employed a hybrid method \cite{xiong2024can, chen2023quantifying} that combined the consistency among multiple responses and the textual confidence generated by the model to extract the confidence level for each recommendation. Explanations were automatically presented whenever the recommendation confidence was low, otherwise they were only displayed upon the user’s request.




We then conducted a user study with 44 participants to understand users’ experiences when provided with  structured and adaptively presented explanations for LLM-driven contextual recommendations. The study was designed with four within-subject conditions: no explanations, always-on unstructured explanations, always-on structured explanations, and adaptive structured explanations whose presentation was dependant on the recommendation confidence level. We found that structured explanations effectively reduced participants’ time to select an AI-recommended action and lowered their cognitive load when reading the explanation. Participants were also more likely to accept the recommendations when presented with always-on structured explanations.
Participants were, however, less satisfied with structured explanations compared to unstructured explanations. When presented adaptively, the structured explanations were less effective at improving user perceptions of the AI (e.g., trust, satisfaction) compared to the always-on structured explanations.
Our analysis of participants’ feedback from a semi-structured interview showed that they valued the ease of reading and viewing text in the structured explanations, but also desired the level of detail and naturalness as provided by the unstructured explanations. Additionally, while participants appreciated the control over interface offered by adaptively presented explanations, it introduced extra efforts to interact with the interface.

%We conclude by discussing potential future directions, such as leveraging additional modalities to enrich the  information in explanations, as well as considering more sophisticated methods for implementing adaptive explanations.

Taken together, we make the following contributions in this work: 

\begin{itemize}
    \item A prompting strategy that structures an LLM’s explanation text into defined contextual components to create spatially glanceable LLM explanations on ultra-small devices.
    \item The use of confidence level estimations to adaptively present explanations to create temporally glanceable LLM explanations on ultra-small devices.
    \item A user study to analyze the impact of structuring and adaptively presenting explanations on user experience with AI using ultra-small devices. We conclude by discussing design implications for optimizing the content and timing of glanceable LLM explanations.
    % structured explanations are quick and easy to view, but lack sufficient detail and natural readability.
    % \item The study further shows that while adaptive structured explanations offer user control over the interface, the additional interaction step can feel redundant and hinder the user experience.

\end{itemize}