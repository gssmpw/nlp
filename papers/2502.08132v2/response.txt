\section{Related Work}
\subsection{Sequential Recommendation}
Sequential recommendation has been extensively studied, with early approaches primarily relying on Markov chain. For example, **Rendle et al., "Factorizing Personalized Markov Chains"** combines the factorization machine and Markov chain techniques to construct a personalized item-item relationship transition matrix for each user, enabling the next-item prediction based on users' most recent interactions.

In recent years, deep neural networks (DNNs) have gained significant attention due to their strong expressive capabilities, leading to the emergence of DNN-based models in the field of sequential recommendation. **Hidasi et al., "GRU4Rec"** utilizes Gate Recurrent Unit (GRU)** Rendle et al., "Deep Session Interest Network"** network with ranking loss to capture the user interests within the sequence. **Kang and McAuley, "NARM: Negative Adaptive Memory Networks for Session-based Recommendation"** applies GRU to extract the users' general interests and intentions through a global and local encoder, respectively. **Liu et al., "SASRec: Association-aware Self-Attentive Model for Sequential Recommendation"** employs the Transformer-based architecture to model user interaction sequences, capturing long-term dependencies while adaptively adjusting attention weights of items in the sequence, thereby focusing on important information. **Zhang and Lan, "BERT4Rec: BERT Based Sequence Modeling for Recommender Systems"** introduces a Cloze task and utilizes a bidirectional attention mechanism to learn contextual information from the interaction sequences. **Wu et al., "DCR: Deep Collaborative Ranking Model for Top-N Recommendation"** and **Ye et al., "GCL4SR: Graph Convolutional Layer for Sequential Recommendation"** leverage self-supervised learning techniques to capture complex relationships within sequential data and enhance recommendation performance. All the aforementioned approaches are devoted to modeling sequential dependencies in historical user behaviors but ignore the interaction timestamps, limiting their ability to capture the dynamic evolution of user interests.

\subsection{Time-Aware Recommendation}
% Temporal information is a crucial factor in accurately predicting user interactions. 
Several time-aware sequential recommendation approaches have been proposed to incorporate temporal information in modeling user preferences. **Xie et al., "TiSASRec: Time-aware Self-attentive Model for Sequential Recommendation"** adopts a time-aware self-attentive model to learn the weights of different items in a sequence, integrating both absolute positions and relative time intervals between items to predict future items. **Liu et al., "TGSRec: Temporal Graph-based Sequential Recommendation with Transformer"** combines sequential patterns and temporal collaborative signals by introducing a Temporal Collaborative Transformer (TCT) layer, capturing both user-item interactions and temporal dynamics for continuous-time recommendation. **Wu et al., "TAT4SRec: Time-aware Temporal Attentive Model for Sequential Recommendation"** processes item sequences and timestamp sequences separately, using a window function to maintain continuity in timestamps. **Zhang et al., "TASER: Temporal Attention-based Self-attentive Embedding Recommender System"** introduces a relative temporal attention network to capture the pairwise dependencies using absolute temporal vectors. The work of **Liu et al., "TiCoSeRec: Time-aware Collaborative Sequence Recommendation with Uniformly Distributed Time Intervals"** demonstrates that uniformly distributed time intervals can improve performance. Different from these time-discretized approaches, we explore the continuous-time recommender systems, aiming to extract dynamic evolving information underlying continuous systems.
% CTA, based on self-attention mechanism, manages several parameterized kernels to model the temporal information influence and employ it with contextual information for final determination. Qin et al. present **GDERec that designs a graph ordinary differential equation based method to capture the collaborative signals and model the temporal evolution of them. 

\subsection{State Space Models}
State space models have emerged as a promising class of architectures, demonstrating exceptional performance in sequence modeling tasks** Raghunathan et al., "Structured State Space Sequence Model with Diagonal State Matrix"**. The first structured state space sequence model, **S4: Structured State Space Sequence Model with Parallel Scans and Hierarchical State Initialization"**, addresses the challenge of capturing long-range dependencies through the state matrix initialization based on HiPPO framework** Yuan et al., "HiPPO: Hierarchical Importance Position-aware Positional Encoding for Hierarchical Graphs"**. Different from the single-input, single-output scheme of S4, **S5: Structured State Space Sequence Model with Multi-Input and Multi-Output"**, adopts multi-input, multi-output (MIMO) SSM. By utilizing a diagonal state matrix, S5 achieves linear computational complexity and time-varying parameterization via parallel scans. The latest work of SSM, **Mamba: Memory-Augmented State Space Sequence Model with Selective Mechanism"**, incorporates a selection mechanism to filter irrelevant information while retaining relevant information, and designs a hardware-aware algorithm for both efficient training and inference. **Mamba2: Mamba-based Attention Variants for Large-Scale Language Modeling"** connects attention variants with selective SSM directly for large-scale language modeling. **Jamba: Joint Model of Hierarchical and Associative Memory for Large-Scale Sequence Modeling"** is a hybrid architecture for large-scale modeling that combines Mamba with Transformer and mixture-of-experts module to take full advantage of each architecture.

Recent works have introduced Mamba into recommender systems due to its impressive performance in sequential tasks. **Mamba4Rec: Mamba-based State Space Model for Sequential Recommendation"**, is the first to apply the Mamba block for sequential recommendation with improving performance and fast inference. **RecMamba: Recommender System using Mamba-based Attention Mechanism for Lifelong Sequences"**, replaces the Transformer block in SASRec with Mamba for lifelong sequences. **MaTrRec: Memory-Augmented Transformer-based Model for Sequential Recommendation"**, combines Mamba with Transformer, leveraging their strengths of capturing both long-range and short-range dependencies. **SSD4Rec: Structured State-Space Dual Attention Block for Sequential Recommendation"**, introduces a bi-directional structured state-space duality block to capture contextual information, similar to Bert4Rec.
Despite these advancements, the challenge of time-specific personalized recommendations with irregular intervals remains largely unexplored. By formulating this problem as a continuous time-varying system and adopting hybrid state space models to capture both temporal and sequential information of user interactions, our work sheds light on the continuous-time sequential recommendation.