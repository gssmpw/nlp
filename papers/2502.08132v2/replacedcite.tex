\section{Related Work}
\subsection{Sequential Recommendation}
Sequential recommendation has been extensively studied, with early approaches primarily relying on Markov chain. For example, FPMC (Factorizing Personalized Markov Chains)____ combines the factorization machine and Markov chain techniques to construct a personalized item-item relationship transition matrix for each user, enabling the next-item prediction based on users' most recent interactions.

In recent years, deep neural networks (DNNs) have gained significant attention due to their strong expressive capabilities, leading to the emergence of DNN-based models in the field of sequential recommendation. GRU4Rec____ utilizes Gate Recurrent Unit (GRU)____ network with ranking loss to capture the user interests within the sequence. NARM____ applies GRU to extract the users' general interests and intentions through a global and local encoder, respectively. SASRec____ employs the Transformer-based architecture to model user interaction sequences, capturing long-term dependencies while adaptively adjusting attention weights of items in the sequence, thereby focusing on important information. Bert4Rec____ introduces a Cloze task and utilizes a bidirectional attention mechanism to learn contextual information from the interaction sequences. DCRec____ and GCL4SR____ leverage self-supervised learning techniques to capture complex relationships within sequential data and enhance recommendation performance. All the aforementioned approaches are devoted to modeling sequential dependencies in historical user behaviors but ignore the interaction timestamps, limiting their ability to capture the dynamic evolution of user interests.

\subsection{Time-Aware Recommendation}
% Temporal information is a crucial factor in accurately predicting user interactions. 
Several time-aware sequential recommendation approaches have been proposed to incorporate temporal information in modeling user preferences. TiSASRec____ adopts a time-aware self-attentive model to learn the weights of different items in a sequence, integrating both absolute positions and relative time intervals between items to predict future items. TGSRec____ combines sequential patterns and temporal collaborative signals by introducing a Temporal Collaborative Transformer (TCT) layer, capturing both user-item interactions and temporal dynamics for continuous-time recommendation. TAT4SRec____ processes item sequences and timestamp sequences separately, using a window function to maintain continuity in timestamps. TASER____ introduces a relative temporal attention network to capture the pairwise dependencies using absolute temporal vectors. The work of TiCoSeRec____ demonstrates that uniformly distributed time intervals can improve performance. Different from these time-discretized approaches, we explore the continuous-time recommender systems, aiming to extract dynamic evolving information underlying continuous systems.
% CTA, based on self-attention mechanism, manages several parameterized kernels to model the temporal information influence and employ it with contextual information for final determination. Qin et al. present GDERec that designs a graph ordinary differential equation based method to capture the collaborative signals and model the temporal evolution of them. 

\subsection{State Space Models}
State space models have emerged as a promising class of architectures, demonstrating exceptional performance in sequence modeling tasks____. The first structured state space sequence model, S4____, addresses the challenge of capturing long-range dependencies through the state matrix initialization based on HiPPO framework____. Different from the single-input, single-output scheme of S4, S5____ adopts multi-input, multi-output (MIMO) SSM. By utilizing a diagonal state matrix, S5 achieves linear computational complexity and time-varying parameterization via parallel scans. The latest work of SSM, Mamba____, incorporates a selection mechanism to filter irrelevant information while retaining relevant information, and designs a hardware-aware algorithm for both efficient training and inference. Mamba2____ connects attention variants with selective SSM directly for large-scale language modeling. Jamba____ is a hybrid architecture for large-scale modeling that combines Mamba with Transformer and mixture-of-experts module to take full advantage of each architecture.

Recent works have introduced Mamba into recommender systems due to its impressive performance in sequential tasks. Mamba4Rec____ is the first to apply the Mamba block for sequential recommendation with improving performance and fast inference. RecMamba____ replaces the Transformer block in SASRec with Mamba for lifelong sequences. MaTrRec____ combines Mamba with Transformer, leveraging their strengths of capturing both long-range and short-range dependencies. SSD4Rec____ introduces a bi-directional structured state-space duality block to capture contextual information, similar to Bert4Rec.
Despite these advancements, the challenge of time-specific personalized recommendations with irregular intervals remains largely unexplored. By formulating this problem as a continuous time-varying system and adopting hybrid state space models to capture both temporal and sequential information of user interactions, our work sheds light on the continuous-time sequential recommendation.