\section{Background} \label{sec:background}
% Paragraph 1: Formal Language Background
%\shahaf{I commented this since I think its well known}
%For any integer $n > 0$, we define $[n]$ as the set of integers from 1 to $n$.


\subsection{Model Types}

%Our work will examine the computational complexity of calculating Shapley values across a variety of model types, covering both regression and classification models. Specifically, 
We examine the following types of models: \begin{inparaenum}[(i)] \item linear regression models ($\texttt{LIN}_{\texttt{R}}$); \item decision trees (\texttt{DT}); \item tree ensembles (including both majority voting ensembles such as Random Forests and weighted voting ensembles like XGBoost) used for regression or classification ($\texttt{ENS-DT}_{\texttt{R}}$, $\texttt{ENS-DT}_{\texttt{C}}$); \item (feed-forward/recurrent) neural networks with ReLU/Sigmoid activations (\texttt{NN-SIGMOID}, \texttt{RNN-ReLU}); and \item Weighted Automata (\texttt{WA}). \end{inparaenum} A complete formalization of all models is provided in Appendix~\ref{app:sec:terminology}.
%\textcolor{red}{guy: same comment as before, for all mentioning of the appendix}

Although WAs may be considered a niche model family within the broader ML community, a significant portion of our paper focuses on establishing tractability results for them. From the perspective of Explainable AI, our interest in WAs stems from two factors:  Firstly, WAs have been proposed as abstractions of neural networks ~\citep{okudono2020weighted, eyraud2024distillation, weiss2019learning, lacroce2021extracting}, offering enhanced transparency. Secondly, and importantly, WAs can be reduced to various other popular ML models like decision trees, linear regression, and tree ensembles, making tractability results for WAs applicable to a wide range of models. We begin by defining N-Alphabet WAs, a generalization of WAs: %Formally:








%While WA are less commonly utilized in the broader ML community, a significant portion of our paper will focus on establishing tractability results for this model. Our interest in WAs stems from two key reasons. First, WAs offer a robust framework for modeling sequential tasks and have recently been proposed for interpretable modeling of neural networks~\citep{okudono2020weighted, eyraud2024distillation, weiss2019learning, lacroce2021extracting}. The second, and primary, advantage of analyzing WAs is that they can be reduced to a variety of other popular ML models — including decision trees, linear regression models, and tree ensembles used for regression. Therefore, proving tractability for WAs also enables proving tractability for a broad range of models. We will start by defining N-Alphabet WAs, of which WAs are a specific instance. Formally:

\begin{definition}[N-Alphabet Weighted Automata] \label{def:nletterwa}

For $n,N \in \mathbb{N}$, and $\{\Sigma_{i}\}_{i \in [N]}$, a collection of finite alphabets, an N-Alphabet Weighted Automaton $A$ over the product $\Sigma_{1} \times \ldots \times \Sigma_{N}$ is defined by the tuple $\langle\alpha, {A_{\sigma_{1}, \ldots ,\sigma_{N}}}, \beta\rangle$, where $(\alpha, \beta) \in \mathbb{R}^{n}\times \mathbb{R}^{n}$ are the initial and final state vectors, and $A_{\sigma_{1}, \ldots ,\sigma_{N}} \in \mathbb{R}^{n \times n}$ are transition matrices. The N-Alphabet WA $A$ computes a function over $\Sigma_{1}^{} \times \ldots \times \Sigma_{N}^{}$ as: 
$$f_{A}(w^{(1)}, \ldots , w^{(N)}) \myeq \alpha^{T} \cdot \prod\limits_{i=1}^{L}
   A_{w_{i}^{(1)} \ldots w_{i}^{(N)}} \cdot \beta$$ where $(w^{(1)}, \ldots , w^{(N)}) \in \Sigma_{1}^{} \times \ldots \times \Sigma_{N}^{}$ and $|w^{(1)}| = \ldots = |w^{(N)}| = L$.



    %Let $N > 0$ be an integer, and $\{\Sigma_{i}\}_{i \in [N]}$ a collection of finite alphabets. For an interger $n > 0$, a N-Alphabet Weighted Automaton $A$ defined over the cartesian product $\Sigma_{1} \times \ldots \times \Sigma_{N}$ is given by the tuple $<\alpha, \{A_{\sigma_{1}, \times ,\sigma_{N}}\}_{(\sigma_{1} , \ldots ,\sigma_{N}) \in \Sigma_{1} \times \ldots \times \Sigma_{N}}, \beta>$ where $(\alpha, \beta) \in \mathbb{R}^{n}\times \mathbb{R}^{n}$ called the initial and the final state vector respectively, and
    %for $(\sigma_{1},\ldots, \sigma_{N}) \in \Sigma_{1} \times \ldots \times \Sigma_{N}$, $A_{\sigma_{1}, \ldots ,\sigma_{N}} \in \mathbb{R}^{n \times n}$  are called transition matrices.    The N-Alphabet WA $A$ computes a function over $\Sigma_{1}^{*} \times \ldots \times \Sigma_{N}^{*}$ given as:
   %$$f_{A}(w^{(1)}, \ldots , w^{(N)}) \myeq \alpha^{T} \cdot \prod\limits_{i=1}^{L}
   %A_{w_{i}^{(1)} \ldots w_{i}^{(N)}} \cdot \beta$$

%where $(w^{(1)}, \ldots , w^{N}) \in \Sigma_{1}^{*} \times \ldots \times \Sigma_{N}^{*}$ such that $|w^{(1)}| = \ldots = |w^{(N)}| = L$.    
\end{definition}

The integer $n$ denotes the size of the WA $A$, denoted as $\texttt{size}(A)$. For $N = 1$, 1-Alphabet WAs match the classical definition of WA~\citep{droste10}, so we use ``WA'' and ``1-letter WA'' interchangeably.



%where the integer $n$ is the size of the WA $A$ and shall be denoted $\texttt{size}(A)$. When $N = 1$, the class of $1$-Alphabet WAs aligns with the classical definition of WAs (\cite{droste10}). Therefore, in the sequel, we shall use the terminology Weighted Automata and 1-letter Weighted Automata interchangeably to refer to this class of models.

\subsection{Distributions}

Our analysis briefly touches on several types of distributions, primarily for comparison purposes. These include \begin{inparaenum}[(i)]
\item \emph{independent distributions} (\texttt{IND}), where all features are assumed to be independent of one another; \item \emph{empirical distributions} (\texttt{EMP}), i.e., the family of distributions induced from finite datasets; and 
\item \emph{Markovian distributions} (\texttt{MARKOV}), i.e., distributions where the future state depends only on the current state, independent of past states\end{inparaenum}. A full formalization of these distribution families is in Appendix~\ref{app:sec:terminology}.
%\item \emph{Markovian distributions} (\texttt{MARKOV}), i.e  distributions that describe probabilistic systems where the future state depends only on the current state, independent from past states \end{inparaenum}. A full formalization appears in Appendix~\ref{app:sec:terminology}.

Previous studies explored the complexity of these three distributions for the conditional SHAP variant~\citep{arenas23, vander21, marzouk24a, huangupdates}. However, the tractability results here apply to a broader class of distributions, specifically those modeled by \emph{Hidden Markov Models} (\texttt{HMMs}). HMMs are more expressive than standard Markovian distributions, as they incorporate hidden states to model sequences influenced by latent variables. In Appendix~\ref{app:reductiontree}, we prove that HMMs include independent, Markovian, and empirical distributions.


%Previous studies have examined the complexity of these distributions within the context of the conditional SHAP variant~\citep{arenas23, vander21, marzouk24a}. The main tractability results in this paper, however, apply to a much broader family of distributions—specifically, any distribution that can be modeled by a \emph{Hidden Markov Model} (\texttt{HMM}). HMMs are significantly more expressive than standard Markovian distributions because they incorporate hidden states, enabling more sophisticated modeling of sequences where observations are influenced by latent or unobserved variables. In the appendix, we prove that the family of HMM distributions includes independent distributions, Markovian distributions, and empirical distributions.








%All of these distributions have been analyzed in previous works in terms of their complexity, but only in the context of the conditional SHAP variant~\citep{arenas23, vander21, marzouk24a}. Our main tractability result of this paper will hold for a much more general family of distributions, which is any distribution that can be modeled by a Hidden Markov Model (HMM). HMMs are much more expressive than standard Markovian distributions because they incorporate hidden states, allowing for richer modeling of sequences where observations are influenced by unobserved or latent variables. We prove in the appendix that the family of HMM distributions encompasses independent distributions, markovian distributions, as well as empirical distributions. 

%Let $\Sigma$ represent a finite alphabet, with its elements referred to as symbols. The notation $\Sigma^{*}$ (and, $\Sigma^{\infty}$)  is used to denote the set of all finite (and infinite) sequences constructed from symbols in $\Sigma$. For an integer $n > 0$, $\Sigma^{n}$ refers to the set of sequences of length $n$. Given sequence $w \in \Sigma^{*}$, we define $|w|$ as its length, $w_{i:j}$ as the subsequence of $w$ starting from the $i$-th to the $j$-th symbol, and $w_{i}$ to indicate its $i$-th symbol. 

%For a given model $M$, like a weighted automaton (\texttt{WA}) or a tree-based model (\texttt{TREE}), the function computed by $M$ will be denoted as $f_{M}$. If the model $M$ represents a probability distribution, such as in Hidden Markov Models (\texttt{HMM}), we use the notation $P_{M}$ instead.

%For a specific model $M$, such as a weighted automaton (\texttt{WA}) or a Tree-based model (\texttt{TREE}), we will denote the function computed by $M$ as $f_{M}$. When the model $M$ implements a probability distribution, such as Hidden Markov Models (\texttt{HMM}), we use the symbol $P_{M}$ instead.

% Paragraph 2: Pattern
 


% Paragraph 3: N-Alphabet Weighted Automata
%\subsection{N-Alphabet Weighted Automata} \label{subsec:nwa}
%One of the primary contributions of this article is a proof of the tractability of computing different SHAP score variants for the family of Weighted Automata (WAs) under different distributional assumptions (Section \ref{sec:tractable}
%). Algorithms developed for this purpose leverages a class of models known as N-Alphabet WAs, of which WAs are a specific instance. Formally, N-Alphabet Weighted Automata are defined as follows:
%\begin{definition}[N-Alphabet Weighted Automata] \label{def:nletterwa}
%    Let $N > 0$ be an integer, and $\{\Sigma_{i}\}_{i \in [N]}$ a collection of finite alphabets. For an interger $n > 0$, a N-Alphabet Weighted Automaton $A$ defined over the cartesian product $\Sigma_{1} \times \ldots \times \Sigma_{N}$ is given by the tuple $<\alpha, \{A_{\sigma_{1}, \times ,\sigma_{N}}\}_{(\sigma_{1} , \ldots ,\sigma_{N}) \in \Sigma_{1} \times \ldots \times \Sigma_{N}}, \beta>$ where 
%    \begin{itemize}
%    \item $(\alpha, \beta) \in \mathbb{R}^{n}\times \mathbb{R}^{n}$ called the initial and the final state vector respectively.
%    \item For $(\sigma_{1},\ldots, \sigma_{N}) \in \Sigma_{1} \times \ldots \times \Sigma_{N}$, $A_{\sigma_{1}, \ldots ,\sigma_{N}} \in \mathbb{R}^{n \times n}$  are called transition matrices. 
%    \end{itemize}
%    The N-Alphabet WA $A$ computes a function over $\Sigma_{1}^{*} \times \ldots \times \Sigma_{N}^{*}$ given as:
%   $$f_{A}(w^{(1)}, \ldots , w^{(N)}) \myeq \alpha^{T} \cdot \prod\limits_{i=1}^{L}
%   A_{w_{i}^{(1)} \ldots w_{i}^{(N)}} \cdot \beta$$

%where $(w^{(1)}, \ldots , w^{N}) \in \Sigma_{1}^{*} \times \ldots \times \Sigma_{N}^{*}$ such that $|w^{(1)}| = \ldots = |w^{(N)}| = L$,    
%\end{definition}
%The integer $n$ is called the size of the WA $A$ and shall be denoted $\texttt{size}(A)$.

%When $N = 1$, the class of 1-Alphabet WAs aligns with the classical definition of WAs (\cite{droste10}). Therefore, in the sequel, we shall use the terminology Weighted Automata and 1-letter Weighted Automata to refer to this class of models.
 %\paragraph{HMM distributions.} 
\textbf{HMM distributions.} HMMs~\citep{rabiner1986introduction} are a popular class of sequential latent probabilistic models used in various applications~\citep{knill1997hidden, de2007hidden}. For an alphabet $\Sigma$ (also referred to as the observation space), an HMM defines a probabilistic function over $\Sigma^{\infty}$. Formally, an HMM of size $n$ over $\Sigma$ is a tuple $\langle\alpha, T, O\rangle$, where: \begin{inparaenum}[(i)] \item $\alpha \in \mathbb{R}^{n}$, the initial state vector, represents a probability distribution over $[n]$; and \item $T \in \mathbb{R}^{n \times n}$, $O \in \mathbb{R}^{n \times |\Sigma|}$ are stochastic matrices, with each row encoding a probability distribution.\end{inparaenum}

%HMMs~\citep{rabiner1986introduction} are a widely popular class of sequential latent probabilistic models employed in various applications~\citep{knill1997hidden, de2007hidden}. For an alphabet $\Sigma$ (also, referred to as the observation space in the classical theory of HMMs), an HMM implements a probabilistic function over $\Sigma^{\infty}$. Formally, for an integer $n$, an HMM of size $n$ over $\Sigma$ is given by the tuple $<\alpha, T, O>$, where: \begin{inparaenum}[(i)] \item $\alpha \in \mathbb{R}^{n}$, referred to as the initial state vector, encodes a probability distribution over the set $[n]$, and \item $T \in \mathbb{R}^{n \times n}$, $O \in \mathbb{R}^{n \times \Sigma}$ are stochastic matrices (i.e., each of its row vectors encodes a probability distribution).\end{inparaenum}
    
\textbf{HMMs and WAs}. The WA formalism in Definition \ref{def:nletterwa}  suffices to cover HMMs, up to reparametrization.  Indeed, it has been proven that the probability that an HMM $M=\langle\alpha, T, O\rangle$ generates a prefix $w \in \Sigma^{*}$ is: $\mathbf{1}^{T} \cdot \prod\limits_{i=1}^{|w|} A_{w_{i}} \cdot \alpha$~\citep{hsu12}, where $\mathbf{1}$ is a row vector with all $1$'s, and for any $\sigma \in \Sigma$, $A_{\sigma} \myeq T \cdot \text{Diag}(O[:,\sigma])$. The matrix $\text{Diag}(O[:, \sigma])$ is the diagonal matrix formed from the column vector in $O$ indexed by $\sigma$. We follow this parameterization of HMMs and assume they are parameterized by the 1-Alphabet WA formalism in Definition~\ref{def:nletterwa}. For \emph{non-sequential models}, we assume the family of HMMs, denoted $\overrightarrow{\text{HMM}}$, represents latent variable models describing probability distributions over random vectors in a finite domain.
%HMMs admit an equivalent reparametrization using the WA formalism~\citep{hsu12}. Indeed, it can be proven that the probability of generating a prefix $w \in \Sigma^{*}$ by an HMM $M=<\alpha, T, O>$ is given as:
  %$P_{M}(w) = \mathbf{1}^{T} \cdot \prod\limits_{i=1}^{|w|} A_{w_{i}} \cdot \alpha$ 




  
  %where $\mathbf{1}$ refers to the row vector with all elements are equal to 1, and for any $\sigma \in \Sigma$, $A_{\sigma} \myeq T \cdot \text{Diag}(O[:,\sigma])$. The matrix $\text{Diag}(O[:, \sigma])$ denotes the diagonal matrix  formed from the elements of the column vector in $O$ indexed by $\sigma$. Leveraging this connection between HMMs and WAs, HMMs, as treated in this article, are implicitly assumed to be parameterized using the 1-Alphabet Weighted Automata formalism outlined in Definition \ref{def:nletterwa}. 

%\textbf{HMMs for Modeling Non-sequential data.} %While HMMs are well-known for modeling sequential data in the ML community, they are less common for non-sequential data. 
%Here, the family of HMMs, denoted $\overrightarrow{\text{HMM}}$ to distinguish from its sequential form, represents Latent Variable Models that describe probability distributions over random vectors with values in a finite domain.
%While the usage of HMMs for modeling sequential data is widely known within ML community, it is less popular for the case of non-sequential data. In this setting, the family of HMMs, denoted $\overrightarrow{\text{HMM}}$ to avoid confusion with its sequential counterpart, is a family of Latent Variable Models describing probability distributions over random vectors taking values in a finite domain: 

\begin{definition} {($\overrightarrow{\emph{HMM}}$)}\label{def:hmmnonsequentialdata}
Let $(n,N) \in \mathbb{N}^{2}$ be two integers, and $\mathbb{D}$ a finite set. An $\overrightarrow{\emph{HMM}}$ over $\mathbb{D}^{n}$ is parameterized by the tuple $\langle\pi, \alpha, \{T_{i}\}_{i \in [n]}, \{O_{i}\}_{i \in [n]}\rangle$, where $\pi$ is a permutation on $[n]$, and for each $i \in [n]$, $T_{i}$ and $O_{i}$ are stochastic matrices in $\mathbb{R}^{N}$ and $\mathbb{R}^{N \times |\mathbb{D}|}$, respectively. A model $M$ in $\overrightarrow{\emph{HMM}}$ computes the following probability distribution over $\mathbb{D}^{n}$:

%Let $(n,N) \in \mathbb{N}^{2}$ be a pair of integers, and $\mathbb{D}$ be a finite set. A $\overrightarrow{\emph{HMM}}$ over $\mathbb{D}^{n}$ is parameterized by the tuple $<\pi, \alpha, \{T_{i}\}_{i \in [n]}, \{O_{i}\}_{i \in [n]}>$, where $\pi$ is a permutation from $[n]$ to $[n]$ and for any $i \in [n]$, $T_{i}$ (resp. $O_{i}$) are stochastic matrices in $\mathbb{R}^{N}$ (resp. $\mathbb{R}^{N \times |\mathbb{D}|}$). A model $M$ in $\overrightarrow{\emph{HMM}}$ computes the following probability distribution over $\mathbb{D}^{n}$:
$$
      P_{M}(x_{1}, \ldots, x_{n}) := \mathbf{1}^{T} \cdot \prod\limits_{i=1}^{n} A_{i,x_{\pi(i)}} \cdot \alpha
      $$
  where: 
  $A_{i,x} \myeq T_{i} \cdot \text{Diag}(O_{i}[:,x])$.
\end{definition}

In essence, models in the family $\overrightarrow{\text{HMM}}$ are non-stationary HMMs where observations are ordered by a permutation $\pi$. They include a stopping probability mechanism, terminating after the $n$-th symbol with probability 1. Like HMMs, $\overrightarrow{\text{HMM}}$ includes independent, empirical, and Markovian distributions (see proof in  Appendix~\ref{app:reductiontree}).

%In essence, models within the family $\overrightarrow{\text{HMM}}$ can be characterized as non-stationary HMMs in which observations are arranged according to the variable ordering specified by a permutation $\pi$. In addition, these models incorporate a stopping probability mechanism, whereby the probability of terminating after emitting the $n$-th symbol is equal to $1$. The class $\overrightarrow{\text{HMM}}$, similarly to HMM, comprises independent, empirical, and markovian distributions (see proof in Appendix).

% subsection: SHAP score
\subsection{The (Many) Shapley Values}
% Paragraph 1: Brief Presentation of the SHAP formalism 
%In the field of Explainable AI, the SHAP framework offers an elegant analogy between cooperative games  and feature-attribution ML explainability. In this framework, the features of a model are likened to players in a cooperative game, with the model's output representing the total payoff to be distributed among them. Drawing from the concept of Shapley values in cooperative game theory, SHAP scores Tassigned to features are interpreted as the equitable payouts that reflect each feature's contribution to the overall model outcome. This approach provides a systematic method for understanding feature importance and enhancing interpretability in machine learning models.

% Paragraph 2: Formulation of the SHAP formalism

\textbf{Local Shapley values.} %A typical method for assigning feature attributions for ML models is through Shapley values~\citep{lundberg2017, sundararajan20b}, which draws on cooperative game theory principles. 
%Let there be an input space $\mathcal{X} = \mathcal{X}_{1} \times \ldots \times \mathcal{X}_{n}$ a model $f$, which may be either a regression model $f:\mathcal{X}\to \mathbb{R}$ or a classification model $f:\mathcal{X}\to[c]$ for some set of classes $[c]$, and a specific \emph{local} instance $\x \in \mathcal{X}$.
Let there be a discrete input space $\mathcal{X} = \mathcal{X}_{1} \times \ldots \times \mathcal{X}_{n}$ and a model $f$, which can be either a regression model $f:\mathcal{X}\to \mathbb{R}$ or a classification model $f:\mathcal{X}\to[c]$ for a certain set of classes $[c]$ ($c\in \mathbb{N}$), along with a specific \emph{local} instance $\x \in \mathcal{X}$. Then, the (local) Shapley value attribution for a feature $i \in [n]$ with respect to $\langle f,\x\rangle$ is defined as:

%\begin{align}
\begin{equation}
\begin{aligned}
\phi(f,\x,i)\myeq\sum_{S \subseteq [n] \setminus \{i\}} \frac{|S|!\cdot (n - |S| - 1)!}{n!} \nonumber \cdot \\
\!\!\!\!\!\! \left[ v(f,\x, S \cup \{i\}) - v(f,\x,S) \right] \label{eq:genericshap}
\end{aligned}
\end{equation}
%\end{align}

where $v$ is referred to as the \emph{value function} of $\phi$. The primary versatility of Shapley values lies in the various ways $v$ can be defined. Typically, Shapley values are computed with respect to a distribution $\mathcal{D}_p$ over $\mathcal{X}$, meaning $v$ is determined by this distribution, i.e., $v(f,\x,S,\mathcal{D}_p)$. A common definition of $v$ is through conditional expectation, referred to here as \emph{Conditional SHAP}, also known as Conditional Expectation SHAP (CES)~\citep{sundararajan20b}:

%where $v$ is referred to as the \emph{value function} of $\phi$. The primary versatility of Shapley values stems from the different ways in which $v$ can be defined. Shapley values are often calculated with respect to a distribution $\mathcal{D}_p$ over $\mathcal{X}$, meaning that the value function $v$ is determined based on this distribution, i.e., $v(f,\x,S,\mathcal{D}_p)$. One classic way to define $v$ is via the conditional expectation. We will refer to this variant as \emph{Conditional SHAP}, although it is also known by other terms, such as Conditional Expectation SHAP (CES)~\citep{sundararajan20b}.


\begin{equation}
    v_c(f,\x,S, \mathcal{D}_p) \myeq \mathbb{E}_{\z \sim \mathcal{D}_p} \left[ f(\z) | \z_S=\x_S \right]
\end{equation}

where $\z_S=\x_S$ indicates that the values of the features $S$ in $\z$ are set to those in $\x$. Another approach for computing the value function is \emph{Interventional SHAP}~\citep{janzing20a}, also known as Random-Baseline SHAP~\citep{sundararajan20b}, used in practical algorithms like KernelSHAP, LinearSHAP, and TreeSHAP~\citep{lundberg2017, lundbergnature}. In interventional SHAP, when a feature $j \in \overline{S}$ is missing, it is replaced with a reference value independently drawn from a predefined distribution, breaking dependencies with other features. Formally:

%where $\z_S=\x_S$ indicates that the values of the features in $S$ within $\z$ are fixed to those in $\x$. Another method for computing the value function is through \emph{interventional SHAP}~\citep{janzing20a}, also known as Random-Baseline SHAP~\citep{sundararajan20b}, which is widely used in many practical algorithms like KernelSHAP, LinearSHAP, and TreeSHAP~\citep{lundberg2017, lundbergnature}. In interventional SHAP, When a feature $j \in \overline{S}$ is missing, its value is replaced with a reference value independently drawn from a predefined distribution, thereby breaking any dependencies between the missing feature and the others. Formally:

\begin{equation}
    v_i(f,\x,S, \mathcal{D}_p) \myeq \mathbb{E}_{\z \sim \mathcal{D}_p} \left[ f(\x_{S}; \z_{\Bar{S}}) \right]
\end{equation}

where $(\x_{S}; \z_{\Bar{S}})$ represents a vector in which the features in $S$ are fixed to the values in $\x$, and the features in $\overline{S}$ are fixed to the values in $\z$. When the distribution $\mathcal{D}_p$ assumes feature independence, interventional and conditional SHAP coincide, i.e., $v_i(f,\x,S, \mathcal{D}_p)=v_c(f,\x,S, \mathcal{D}_p)$~\citep{sundararajan20b}. However, this alignment does not typically hold in many real-world distributions.

Finally, instead of defining Shapley values with respect to a distribution $\mathcal{D}_p$, they can be defined using an auxiliary baseline $\z^{\text{ref}}$ that captures the ``missingness'' of features in $\overline{S}$. \emph{Baseline SHAP}~\citep{sundararajan20b} is defined as follows:

%Finally, instead of defining Shapley values with respect to a distribution $\mathcal{D}_p$, they can be defined with reference to an auxiliary baseline $\z^{\text{ref}}$, which is intended to capture the ``missingness'' of the features in $\overline{S}$. \emph{Baseline SHAP}~\citep{sundararajan20b} is defined as follows:

\begin{equation}
    v_b(f,\x,S, \z^{\text{ref}}) \myeq f(\x_{S}; \z^{\text{ref}}_{\Bar{S}}) 
\end{equation}


%In the context of this work, our interest lies in two SHAP variants: Interventional SHAP (I-SHAP), and Baseline SHAP (B-SHAP). For a coalition $S \subseteq [n]$, a probability distribution $P$ over $\mathcal{X}$ and $x^{(ref)} \in \mathcal{X}$,  
%the value functions associated to each of these two variants are given as follows: 
%    \begin{equation} \label{eq:VIshap}
%    V_{I}(f,x,S,P) = \mathbb{E}_{X \sim P} \left[ f(x_{S}, X_{[n] \setminus S}) \right]
%    \end{equation}
%  \begin{equation}
%  \label{eq:VBshap}
%   V_{B}(f,x,S,x^{(ref)}) = f(x_{S}, x^{(ref)}_{[n] \setminus S})
%   \end{equation}


By substituting these value function definitions into the generic Shapley value formula (Equation~\ref{eq:genericshap}), we obtain $\phi_c(f,i,\mathcal{D}_p)$, $\phi_i(f,i, \mathcal{D}_p)$, and $\phi_b(f,i, \z^{\text{ref}})$, corresponding to the (local) conditional, interventional, and baseline Shapley values, respectively.

%Plugging these definitions of value functions into the generic Shapley value formula (Equation \ref{eq:genericshap}) gives us $\phi_c(f,i,\mathcal{D}_p)$, $\phi_i(f,i, \mathcal{D}_p)$, and $\phi_b(f,i, \z^{\text{reff}})$, for (local) conditional, interventional, and baseline Shapley values, respectively.

\textbf{Global Shapley values.} Shapley values $\phi(f,\x,i)$ offer \emph{local} explainability for the model's prediction on a specific data point $\x$. One approach to deriving a global importance indicator for input features from their local Shapley values consists at aggregating these values over the input space, weighted by the target data generating distribution~\citep{frye20}:\footnote{Several other methods exist for computing global feature importance using SHAP~\citep{covert2020understanding, lundberg2017, lundbergnature}, but they fall outside the scope of this work.}

\begin{equation}
  \label{eq:VBshap}
   \Phi(f,i, \mathcal{D}_p) \myeq \mathbb{E}_{\x \sim \mathcal{D}_p}[\phi(f,\x,i)]
\end{equation}

%To capture the model's global behavior, these $\phi(f,\x,i)$ values are aggregated into \emph{global} Shapley values~\citep{covert2020understanding} \footnote{Another popular approach for global feature importance is the SAGE framework~\citep{covert2020understanding}, which is beyond the scope of this article.}:

%\begin{equation}
%  \label{eq:VBshap}
%   \Phi(f,i, \mathcal{D}_p) \myeq \mathbb{E}_{\x \sim \mathcal{D}_p}[\phi(f,\x,i)]
%\end{equation}

Note that the global Shapley value is always computed with respect to a distribution $\mathcal{D}_p$, as the inputs $\x$ are aggregated over it. This gives rise to $\Phi_c(f,i,\mathcal{D}_p)$, $\Phi_i(f,i,\mathcal{D}_p)$, and $\Phi_b(f,i,\mathcal{D}_p,\z^{\text{ref}})$, representing the (global) conditional, interventional, and baseline Shapley values, respectively. For \emph{sequential models}, $f$ accepts input vectors of arbitrary length $n$, i.e., $|\x|=n$. While local Shapley values are computed for a specific input $\x$, global Shapley values pose a challenge as they are input-independent. One approach is to fix the feature space length $n$ and compute the global Shapley value for an input of that size. In this case, the global Shapley value $\Phi$ incorporates $n$ as part of its input: $\Phi(f,i,\mathcal{D}_p,n) \myeq \mathbb{E}_{\x \sim \mathcal{D}_p^{(n)}}[\phi(f,\x,i)]$, where $i \in [n]$, $\mathcal{D}_p$ is a probability distribution over an infinite alphabet $\Sigma^{\infty}$, and $\mathcal{D}_p^{(n)}$ is the probability of generating an infinite sequence prefixed by $\x \in \Sigma^{n}$.



%Note that the global Shapley value is always calculated with respect to a distribution $\mathcal{D}_p$, as the inputs $\x$ must be aggregated over this distribution. From these definitions arise $\Phi_c(f,i,\mathcal{D}_p)$, $\Phi_i(f,i, \mathcal{D}_p)$, and $\Phi_b(f,i,\mathcal{D}_p, \z^{\text{reff}})$, representing the (global) conditional, interventional, and baseline Shapley values, respectively.



%The local version of SHAP offers explanations for the decisions made by a model based on a specific input instance. In contrast, the global version of SHAP addresses the explainability problem at the feature level by assigning a weight to each input feature, which reflects its importance in the model's decision-making process. The relationship between the local and global versions of I-SHAP and B-SHAP is expressed in the following equations:

%\begin{equation}
%  \label{eq:VBshap}
%   \Phi_i(f,x) = \mathbb{E}_{x \sim P}[\phi_i(f,x)]
%   \end{equation}



%{\small 
%\begin{equation*}
%\texttt{GLO-I-SHAP}(f,i,P) = \mathbb{E}_{x \sim P} \left[ \texttt{LOC-I-SHAP}(f,x,i,P) \right]
%\end{equation*}
%}
%{\small 
%\begin{equation*} \texttt{GLO-B-SHAP}(f,i,x^{(ref)},P) = \mathbb{E}_{x \sim P} \left[\texttt{LOC-B-SHAP}(f,x,i,x^{(ref)}) \right]
%\end{equation*}
%}









%In the case of \emph{sequential models}, $f$ can accept input vectors of arbitrary length $n$, i.e., $|\x|=n$. While local Shapley values are computed for a specific input $\x$, global Shapley values pose a challenge as they do not depend on a particular input. One way to define the global Shapley value is by fixing the feature space length $n$ and calculating it for an input of that size. In this scenario, the global Shapley value $\Phi$ incorporates the sequence length $n$ as part of its input. In other words:     $\Phi(f,i,\mathcal{D}_p,n) \myeq \mathbb{E}_{\x \sim \mathcal{D}_p^{(n)}}[\phi(f,\x,i)]$, where $i \in [n]$, $\mathcal{D}_p$ is a probability distribution over an infinite alphabet $\Sigma^{\infty}$ (for instance, modeled by HMMs), and $\mathcal{D}_p^{(n)}$ refers to the probability of generating an infinite sequence prefixed by $\x \in \Sigma^{n}$.




%We note that in the case of \emph{sequential models}, $f$ can accept input vectors of arbitrary length $n$, i.e., $|\x|=n$. When defining the local Shapley value with respect to a specific input $\x$, it can be computed accordingly. However, when it comes to computing global Shapley values, a challenge arises since they do not depend on a particular input $\x$. One approach to define the global Shapley value is by fixing the length of the feature space $n$ and computing it for a fixed input of size $n$. Thus, in this specific scenario, the global Shapley value $\Phi$ incorporates the sequence length $n$ as part of its input. In other words:

%\begin{equation}
%    \Phi(f,i,\mathcal{D}_p,n) \myeq \mathbb{E}_{\x \sim \mathcal{D}_p^{(n)}}[\phi(f,\x,i)]
%\end{equation}




%We note that in the specific case of sequential models, $f$ can take in vectors of an arbitrary length $n$, i.e., $|\x|=n$. When defining the local Shapley value which is computed with respect to $\x$ it can be computed accordingly. However, when computing global Shapley values, this raises a question on their computation since they do not rely on a specific input $\x$. A possible way to define global Shapley value is by fixing the length of the feature space $n$ and computing the global Shapley value with respect to a fixed input of size $n$. Hence in these particular case, the global Shapley value $\Phi$, will take the sequence length $n$ as part of its input. In other words: 

%These formulas are relevant for models with a fixed input feature space dimension, such as sigmoidal neural networks (\texttt{SIGMOID}) and tree-based models (\texttt{TREE}). For sequential models, like RNNs and WAs, a more appropriate formulation can be derived by fixing the length of the input sequences. In this case, the Global I-SHAP becomes dependent on this parameter, as follows:
%$$\texttt{GLO-I-SHAP}(f,i,n,P) = \mathbb{E}_{x \sim P^{(n)}} \left[ \texttt{LOC-I-SHAP}(f,x,i,P) \right]$$
%where $i \in [n]$, $P$ a probability distribution over $\Sigma^{\infty}$ (for instance, modeled by HMMs), and $P^{(n)}(x)$ refers to the probability of generating an infinite sequence prefixed by $x \in \Sigma^{n}$. 

\textbf{Shapley values for sequential models.} For complexity results on sequential models (WAs and RNNs), we build on prior work~\citep{marzouk24a}, which uses the \emph{pattern} formalism to analyze the complexity of obtaining Shapley values for these models. Formally, let $\Sigma$ be a finite alphabet, with its elements called symbols. The set of all finite (and infinite) sequences from $\Sigma$ is denoted by $\Sigma^{}$ (and $\Sigma^{\infty}$). For any integer $n > 0$, $\Sigma^{n}$ represents sequences of length $n$. For a sequence $w \in \Sigma^{}$, $|w|$ is its length, $w_{i:j}$ is the subsequence from the $i$-th to the $j$-th symbol, and $w_{i}$ is its $i$-th symbol. A pattern $p$ over $\Sigma$ is a regular expression in the form $\#^{i_{1}}w_{1} \ldots \#^{i_{n}}w_{n}\#^{i_{n+1}}$, where $\#$ is a placeholder symbol (i.e., $\# = \Sigma$), $\{i_{k}\}_{k \in [n+1]}$ are integers, and $\{w_{k}\}_{k \in [n+1]}$ are sequences over $\Sigma^{*}$. The extended alphabet $\Sigma \cup \#$ is $\Sigma{\#}$. The language of a pattern $p$ is $L{p}$, with $|p|$ as its length and $|p|_{\#}$ indicating the number of $\#$ symbols. 

We define two operators on patterns: \begin{inparaenum}[(i)] \item The \emph{swap} operator, which takes a tuple $(p,\sigma,i) \in \Sigma_{\#}^{*} \times \Sigma \times \mathbb{N}$ with $i \leq |p|$, and returns a pattern where the i-th symbol of $p$ is replaced by $\sigma$. For example, with $\Sigma = \{0,1\}$: $\texttt{swap}(0\#0\#, 1,2) = 0\mathbf{1}0\#$; \item The \emph{do} operator, which takes a tuple $(p,w',w) \in \Sigma_{\#}^{*} \times \Sigma^{*} \times \Sigma^{*}$ with $|w|$ = $|w'|$ = $|p|$, and returns a sequence $u$ where $u_{i}$ = $w'_{i}$ if $p_{i}$=$\#$, and $u_{i}$ = $w_{i}$ otherwise. For example, with $\Sigma = \{0,1\}$: $\texttt{do}(0\#0\#,1100,1111) = 1110$ \end{inparaenum}. We represent a coalition $S$ in the SHAP formulation using patterns. For instance, with the alphabet $\Sigma = \{0,1\}$ and the sequence $w = 0011$, the coalition of the first and third symbols is represented by the pattern $0\#1\#$.



%Two operators on patterns are defined as follows: \begin{inparaenum}[(i)] \item \emph{The swap operator}, which for any tuple $(p,\sigma,i) \in \Sigma_{\#}^{*} \times \Sigma \times \mathbb{N}$ such that $i \leq |p|$,  $\texttt{swap}(p,\sigma,i)$ returns a pattern equal to $p$ except for the i-th symbol being replaced by $\sigma$. For instance, by fixing the alphabet $\Sigma = \{0,1\}$, we have: $\texttt{swap}(0\#0\#, 1,2) = 0\mathbf{1}0\#$. \item \emph{The do operator}, which for $(p,w',w) \in \Sigma_{\#}^{*} \times \Sigma^{*} \times \Sigma^{*}$ such that $|w| = |w'| = |p|$, $\texttt{do}(p,w',w) \in \Sigma^{*}$ returns a sequence $u \in \Sigma^{*}$ such that $|u| = |p|$, and for any $i \in [|p|]$, $u_{i} = w'_{i}$ if $p_{i} = \#$, and equals to $w_{i}$ otherwise. For instance, for the alphabet $\Sigma = \{0,1\}$, we have:$\texttt{do}(0\#0\#,1100,1111) = 1110$
%\end{inparaenum}.

%Let $\Sigma$ denote a finite alphabet, with its elements referred to as symbols. The set of all finite (and infinite) sequences formed from symbols in $\Sigma$ is represented by $\Sigma^{}$ (and $\Sigma^{\infty}$, respectively). For any integer $n > 0$, $\Sigma^{n}$ designates the set of sequences of length $n$. For a sequence $w \in \Sigma^{}$, we define $|w|$ as its length, $w_{i:j
%}$ as the subsequence of $w$ from the $i$-th symbol to the $j$-th, and $w_{i}$ as its $i$-th symbol. For an alphabet $\Sigma$, a pattern $p$ is defined as a regular expression in the form $\#^{i_{1}}w_{1} \ldots \#^{i_{n}}w_{n}\#^{i_{n+1}}$, where $\#$ is a special symbol interpreted as a placeholder (i.e., $\# = \Sigma$), $\{i_{k}\}_{k \in [n+1]}$ is a set of integers, and $\{w_{k}\}_{k \in [n+1]}$ is a collection of sequences over $\Sigma^{*}$. The extended alphabet $\Sigma \cup \#$ is referred to as $\Sigma{\#}$. Additionally, the language described by a pattern $p$ is denoted as $L_{p}$. Like sequences, the notation $|p|$ represents the length of the pattern, while $|p|_{\#}$ indicates the number of occurrences of the symbol $\#$ in $p$. We define two operators on patterns:



%For a given model $M$, the function computed by $M$ will be denoted as $f_{M}$. If the model $M$ represents a probability distribution, we use the notation $P_{M}$ instead.

%\paragraph{Patterns.} 






 



%For an alphabet $\Sigma$, a pattern $p$ is defined as a regular expression of the form $\#^{i_{1}}w_{1} \ldots \#^{i_{n}}w_{n}\#^{i_{n+1}}$, where $\#$ is a special symbol interpreted as a placeholder (i.e. $\# = \Sigma$), and $\{i_{k}\}_{k \in [n+1]}$ is a set of integers, and $\{w_{k}\}_{k \in [n+1]}$ is a collection of sequences over $\Sigma^{*}$. We shall adopt the terminology employed in \cite{marzouk24a} to refer to concepts revolving around the notion patterns: The extended alphabet $\Sigma \cup \#$ will be denoted by $\Sigma_{\#}$. In addition, the language described by a pattern $p$ is denoted by $L_{p}$. Similar to sequences, the notation $|p|$ represents the length of the pattern. Additionally, $|p|_{\#}$ indicates the number of times the symbol $\#$ appears in $p$. Two operations on patterns will be particularly useful in this article:
%\begin{itemize}
%    \item \textbf{The swap operator:} For any tuple $(p,\sigma,i) \in \Sigma_{\#}^{*} \times \Sigma \times \mathbb{N}$ such that $i \leq |p|$,  $\texttt{swap}(p,\sigma,i)$ returns a pattern equal to $p$ except for the i-th symbol being replaced by $\sigma$. For instance, by fixing the alphabet $\Sigma = \{0,1\}$, we have:
%    $$\texttt{swap}(0\#0\#, 1,2) = 0\mathbf{1}0\#$$
%    \item \textbf{The do operator:} For $(p,w',w) \in \Sigma_{\#}^{*} \times \Sigma^{*} \times \Sigma^{*}$ such that $|w| = |w'| = |p|$, $\texttt{do}(p,w',w) \in \Sigma^{*}$ returns a sequence $u \in \Sigma^{*}$ such that $|u| = |p|$, and for any $i \in [|p|]$, $u_{i} = w'_{i}$ if $p_{i} = \#$, and equals to $w_{i}$ otherwise. For instance, for the alphabet $\Sigma = \{0,1\}$, we have:

%    $$\texttt{do}(0\#0\#,1100,1111) = 1110$$
%\end{itemize}


%In the reformulation of the I-SHAP and B-SHAP scores using patterns conducted later in this section, some subsets and functions over patterns will be of particular interest. Let $w \in \Sigma^{*}$, and $(i,k) \in [|w|]^{2}$, we define the following subset of patterns:
%$$\mathcal{L}_{i,k}^{w} \myeq \{p \in \Sigma_{\#}^{|w|}:  ~ w \in L_{p} \land |p|_{\#} = k \land p_{i} = \#
%\}$$

%By defining the uniform distribution over $\mathcal{L}_{i,k}^{w}$ as $\mathcal{P}_{i,k}^{w}$, we introduce the following probability distribution over patterns:
 
%\begin{align}
%\mathcal{P}_{i}^{w}(p) & \myeq \frac{1}{|w|} \cdot \sum\limits_{k=1}^{|w|} \mathcal{P}_{i,k}^{w}(p) 
%\nonumber \\
%&= \begin{cases}
%\frac{(|p|_{\#}-1)! \cdot (|w| - |p|_{\#})!}{|w|!} & \text{if}~~ w \in L_{p} \\ 
%0 & \text{otherwise}
%\end{cases}
%\label{eq:Piw}
%\end{align}

%where the second inequality is obtained by observing that: $|\mathcal{L}_{i,k}^{w}| = \binom{|w|-1}{|k| - 1}$.

%When computing SHAP scores for sequential models, we will use an alternative reformulation of the I-SHAP and B-SHAP variants through pattern languages, recently proposed in \cite{marzouk24a} for the case of C-SHAP. In this reformulation, coalitions of features are represented by patterns:


%We are now ready to establish the first connection between our pattern operators and (interventional) Shapley values. Due to space limitations, we have included the corresponding relations for baseline SHAP in the appendix.

% Write the value function using patterns
%Leveraging this connection, one can rewrite the value function associated to I-SHAP (Equation \eqref{eq:VIshap}) for a sequential model as: 
%$$V_{I}(f,w,p,P) = \mathbb{E}_{w' \sim P^{(|w|)}} \left[ f(\texttt{do}(p,w',w)) \right]$$
%where $w \in \Sigma^{*}$ is a sequence and $p \in \Sigma_{\#}^{*}$ such that $|p| = |w|$.



%\begin{equation}
%    v_{i}(f,\x,S,\mathcal{D}_p) = \mathbb{E}_{\z \sim \mathcal{D}_p^{|S|}} \left[ f(\texttt{do}(S,\z,\x)) \right]
%\end{equation}


%\begin{equation}
%\begin{aligned}
%\phi_i(f,\x,i,\mathcal{D}_p) = \mathbb{E}_{\z \sim \mathcal{P}_i^{\x}} [v_{i}(f,\x,\texttt{swap}(\z,\x_{i},i),\mathcal{D}_p) \nonumber 
% \\ - v_{i}(f,\x,\z,\mathcal{D}_p)] \quad\quad\quad\quad\quad\quad
%\end{aligned}
%\end{equation}

%With this new reformulation of the Value function, and by reinterpreting the weighted sum in the I-SHAP formula in probabilistic terms (Equation \eqref{eq:Piw}), the Local I-SHAP formula (for sequential models) can be rewritten as:
%{\small 
%\begin{align}
%\texttt{LOC-I-SHAP}(f,w,i,P) = &\mathbb{E}_{p \sim \mathcal{P}_{i}^{(w)}} [V_{I}(f,w,\texttt{swap}(p,w_{i},i),P) \nonumber \\
%&  - V_{I}(f,w,p,P)] \label{eq:shapusingpattern}
%\end{align}
%}

%Same reasoning steps leads to an analogous formula of Local B-SHAP. 

\textbf{SHAP as a computational problem.} As outlined in the introduction, this work aims to provide a comprehensive computational analysis of the SHAP problem across several dimensions: \begin{inparaenum}[(i)] \item the class of models being interpreted; \item the underlying data-generating distributions; \item the specific SHAP variant; and \item its scope (global or local)\end{inparaenum}. Each combination of these dimensions gives rise to a distinct formal computational problem. To navigate this multi-dimensional landscape of computational problems, we adopt the following notation:
A (variant) of the SHAP computational problem shall be denoted as \texttt{(LOC|GLOB)-(I|B|C)-SHAP}($\mathcal{M}$,$\mathcal{P}$), where \texttt{LOC} and \texttt{GLOB} refer to local and global SHAP, respectively, while \texttt{I}, \texttt{B}, and \texttt{C} correspond to the interventional, baseline, and conditional SHAP variants. The symbols $\mathcal{M}$ and $\mathcal{P}$ represent the class of models and the class of feature distributions, respectively. Under this notation, \texttt{LOC-I-SHAP}(\texttt{WA}, \texttt{HMM}) refers to the problem of computing local interventional SHAP for the family of weighted automata under Hidden Markov Model distributions.


 A variant of the SHAP computational problem takes as input instance a model $M \in \mathcal{M}$, a data-generating distribution 
$\text{P} \in \mathcal{P}$ \footnote{Note that in the formulation of Local Baseline SHAP variants of the SHAP problem, the data-generating distribution $P$ is replaced by a reference input instance.}, an index specifying the input feature of interest, and, in the case of local SHAP variants, the  model's input undergoing explanatory analysis. The computational complexity of the problem is measured with respect to the size of $M$, the size of $P$, and the dimensionality of the input space\footnote{For sequential models, where inputs are sequences, we assume the input space's dimensionality equals the sequence length under analysis.}. A variant is considered tractable if it can be solved in polynomial time with respect to these parameters.

For completeness, a summary of all complexity classes discussed in this article (PTIME, NP, coNP, NTIME, and \#P) is provided in Appendix~\ref{app:sec:terminology}.


%This paper examines two types of computational problems: functional and decision problems. The notation for obtaining a Shapley value functional follows: \texttt{(LOC|GLOB)-(I|B|C)-SHAP}($\mathcal{M}$,$\mathcal{P}$), where \texttt{(Loc|Glob)} indicates local/global SHAP, \texttt{(I-B-C)} represents interventional, baseline, or conditional variants, $\mathcal{M}$ is the model class, and $\mathcal{P}$ denotes the feature distribution class $\mathcal{D}_p$. For example, \texttt{LOC-I-SHAP}(\texttt{WA}, \texttt{HMM}) refers to computing local interventional SHAP for WAs under HMM distributions. Each functional problem $\mathcal{F}$ has a corresponding decision problem, which, given an instance $I$ and scalar $c \in \mathbb{R}$, outputs \textit{Yes} if $\mathcal{F}(I) > c$, and \textit{No} otherwise. In this work, we discuss standard complexity classes (PTIME, NP, coNP, NTIME, and $\#$P) and formalize them in Appendix~\ref{app:sec:terminology}. It is widely believed that PTIME $\subsetneq$ NP, coNP, NTIME, and $\#$P~\citep{arora2009computational}.





%This paper explores two types of computational problems: functional problems and decision problems. The notation for a computational problem of obtaining a Shapley value functional problem follows the format: \texttt{(LOC|GLOB)-(I|B|C)-SHAP}($\mathcal{M}$,$\mathcal{P}$), where \texttt{(Loc|Glob)} represents local/global SHAP, \texttt{(I-B-C)} represents interventional, baseline or conditional variants, $\mathcal{M}$ represents the class of models, and $\mathcal{P}$ denotes the class of feature distributions $\mathcal{D}_p$. For instance, \texttt{LOC-I-SHAP}(\texttt{WA}, \texttt{HMM}) refers to the task of computing local interventional SHAP for WAs under distributions modeled by HMMs. Each functional problem $\mathcal{F}$ has a corresponding decision problem, which takes as input an instance $I$, and a scalar $c \in \mathbb{R}$. The output is \textit{Yes} if $\mathcal{F}(I) > c$, and \textit{No} otherwise.

%This article examines two types of computational problems: functional problems and decision problems. The notation used to denote specific configurations of SHAP functional problems adheres to the following syntax: \texttt{(LOC|GLOB)-(I|B)-SHAP}($\mathcal{M}$, $\mathcal{P}$), where $\mathcal{M}$ represents the class of models to be explained and $\mathcal{P}$ signifies the class of data-generating distributions. For example, \texttt{LOC-I-SHAP}(\texttt{WA}, \texttt{HMM}) refers to the problem of computing the local I-SHAP for WAs under distributions modeled by HMMs. Each functional problem $F$ has an associated decision problem that takes as input an input instance 
%$I$ of $F$ and a scalar $C \in \mathbb{R}$. The output is YES if $F(I) > C$ and NO otherwise.  
 