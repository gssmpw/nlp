%\subsection{The problem \texttt{LOC-I-SHAP}(\texttt{RNN-ReLu}, \texttt{IND}) is NP-Hard.}
\subsection{When Interventional SHAP is Hard}
\label{subsec:rnnreluhard}
In this subsection, we present intractability results for interventional SHAP.
%The main result of this section is given in the following: 

\begin{theorem} \label{thm:relushap}
    The decision version of the problem \emph{\texttt{LOC-I-SHAP}}\emph{\texttt{(RNN-ReLu}, \texttt{IND)}} is \emph{NP-Hard}.
\end{theorem}

Recall that an RNN-ReLu model $R$ over $\Sigma$ is parametrized as: $\langle h_{init}, W, \{v_{\sigma}\}_{\sigma \in \Sigma}, O\rangle$ and processes a sequence sequentially from left-to-right such that $h_{\epsilon} = h_{init}$, $h_{w'\sigma} = \emph{ReLu}(W \cdot h_{w'} + v_{\sigma} )$, and $f_{R}(w) = I(O^{T} \cdot h_{w} \geq 0)$. The proof of Theorem~\ref{thm:relushap} leverages the efficiency axiom of Shapley values. Specifically, for a sequential binary classifier $f$ over alphabet $\Sigma$, an integer $n$, and $i \in [n]$, the efficiency property can be expressed as~\citep{arenas23}:
\begin{equation} \label{eq:efficiency}
\sum\limits_{i=1}^{n} \phi_i(f,\x,i,P_{unif}) =  f(\x) - 
 P_{unif}^{(n)} (f(\x) = 1) \end{equation}
%$$
%\begin{cases}
%    h_{\epsilon} = h_{init} \\ 
%    h_{w'\sigma} = \emph{ReLu}(W \cdot h_{w'} + v_{\sigma} ) \\ 
%    f_{R}(w) = I(O^{T} \cdot h_{w} \geq 0)
%\end{cases}
%$$



%The proof of Theorem \ref{thm:relushap} leverages the efficiency axiom of Shapley values. Formally, given a sequential binary classifier $f$ over the alphabet $\Sigma$, an integer $n$, and $i \in [n]$, the efficiency property can be written as~\citep{arenas23}:
%\begin{equation}
%\label{eq:efficiency}
%\sum\limits_{i=1}^{n} \phi_i(f,\x,i,P_{unif}) =  f(\x) - 
% \nonumber 
% P_{unif}^{(n)} (f(\x) = 1) 
%\end{equation} 


where $P_{unif}$ denotes the uniform distribution over $\Sigma^{\infty}$. This property will be used to reduce the $\texttt{EMPTY}$ problem to the computation of interventional SHAP in our context. The $\texttt{EMPTY}$ problem is defined as follows: Given a set of models $\mathcal{F}$, $\texttt{EMPTY}$ takes as input some $f \in \mathcal{F}$ and an integer $n > 0$ and asks if $f$ is empty on the support $\Sigma^{n}$ (i.e., is the set $\{ \x \in \Sigma^{n}:~f(\x) = 1 \}$ empty?). The connection between local interventional SHAP and the emptiness problem is outlined in the following proposition:

%where $P_{unif}$ denotes the uniform distribution over $\Sigma^{\infty}$. This property will be utilized to reduce the $\texttt{EMPTY}$ problem to the computation of interventional SHAP in our context. The $\texttt{EMPTY}$ problem is defined as follows: Given a set of models $\mathcal{F}$, $\texttt{EMPTY}$ takes as input some $f\in \mathcal{F}$ and asks whether $f$ on the support $\Sigma^{n}$ empty? (i.e. Is the set $\{ \x \in \Sigma^{n}:~f(\x) = 1 \}$ empty?). The connection between the Local I-SHAP problem and the emptiness problem is highlighted in the following proposition:

%As noted by \cite{arenas23}, the efficiency property of the SHAP score reveals an intresting connection between the SHAP computational problem and the model counting problem [\textbf{citation needed of the model counting problem}]. Specifically, Equation \ref{eq:efficiency} highlights the fact that computing I-SHAP scores for binary classifiers is at least as hard as counting the number of examples classified as positive by the model being explained. To formalize this insight, we define the emptiness problem, which is the decision problem related to model counting.  Let $\mathcal{M}$ be a class of sequential binary classifiers over $\Sigma$,  we have:

%\textbf{Problem:} \texttt{EMPTY}($\mathcal{M}$) \\
%\textbf{Instance:} a model $M$, an integer $n > 0$  \\
%\textbf{Output:} Is $M$ on the support $\Sigma^{n}$ empty? (i.e. Is the set $\{ x \in \Sigma^{n}:~f_{M}(x) = 1 \}$ empty?) 


\begin{proposition} \label{prop:modelcountingshap}
Let $\mathcal{F}$ be a class of sequential binary classifiers. Then, \emph{\texttt{EMPTY($\mathcal{F}$)}} can be reduced in polynomial time to the problem $\emph{\texttt{LOC-I-SHAP}}(\mathcal{F}, \emph{\texttt{IND}})$.
\end{proposition}
%\emph{Proof.}
%    Let $\mathcal{F}$ be a class of binary classifiers, $f \in \mathcal{F}$, and $n > 0$. Choose an arbitrary sequence $\x \in \Sigma^{n}$. Without loss of generality, we assume $f(\x) = 0$ (otherwise, \texttt{EMPTY}($\mathcal{F}$) for the instance $<F,n>$ is already solved). By Equation~\ref{eq:efficiency}, we have that the set $\{\x \in \Sigma^{n}:f(\x) = 1\}$ is empty if and only if for any $i \in [n]$, the input instance $<f,\x,i,P_{unif},0>$ of the decision problem associated to $\texttt{LOC-I-SHAP}(\texttt{RNN-ReLu}, \texttt{IND})$ yields a \textit{No} answer. \quad \quad \quad \quad\quad \quad\quad \quad\quad \quad \quad \quad\quad \quad\quad \quad\quad \quad \quad  $\qedsymbol{}$


Proposition \ref{prop:modelcountingshap} suggests that proving the NP-Hardness of the problem $\texttt{EMPTY}(\texttt{RNN-ReLu})$ is a sufficient condition to yield the result of Theorem \ref{thm:relushap}. This condition is asserted in the following lemma:
\begin{lemma} \label{lemma:emptyrnnrelu}
\emph{\texttt{EMPTY}}\emph{\texttt{(RNN-ReLu)}} is NP-Hard. 
\end{lemma}



The proof of Lemma~\ref{lemma:emptyrnnrelu} is done via a reduction from the \emph{closest string problem} (CSP), which is NP-Hard~\citep{li2000closest}. CSP takes as input a set of strings $S := \{w_{i}\}_{i \in [m]}$ of length $n$ and an integer $k > 0$. The goal is to determine if there exists a string $w' \in \Sigma^{n}$ such that for all $w_{i} \in S$, $d_{H}(w_{i}, w') \leq k$, where $d_{H}(.,.)$ is the Hamming distance: $d_{H}(w,w') := \sum\limits_{i=1}^{ [|w|]} \mathrm{1}_{w_{j}}(w'_{j})$. We conclude our reduction by proving the following proposition, with the proof in Appendix~\ref{app:ISHAPRNN}:


%The proof of lemma \ref{lemma:emptyrnnrelu} will be carried out by a reduction from the \emph{closest string problem} (CSP), which is known to be NP-Hard~\citep{li2000closest}. The CSP problem takes as input a collection of strings $S = \{w_{i}\}_{i \in [m]}$ whose length is equal to $n$, and an integer $k > 0$. The problem outputs yes whether there exist a string $w' \in \Sigma^{n}$ such that for any $w_{i} \in S$, we have $d_{H}(w_{i}, w') \leq k$,    where $d_{H}(.,.)$ is the Hamming distance given as: $d_{H}(w,w') = \sum\limits_{i=1}^{ [|w|]} \mathrm{1}_{w_{j}}(w'_{j})$. Thus, we conclude our reduction by proving the following proposition, whose proof appears in the appendix:

\begin{proposition}
    \emph{\texttt{CSP}} can be reduced in polynomial time to \emph{\texttt{EMPTY}}\emph{\texttt{(RNN-ReLu)}}.
\end{proposition}
    
    %$\bullet$ \textbf{Problem:} \texttt{CSP} \\
    %\textbf{Instance:} A collection of strings $S = \{w_{i}\}_{i \in [m]}$ whose length is equal to $n$, an integer $k > 0$, \\ 
    %  \textbf{Output} Does there exist a string $w' \in \Sigma^{n}$ such that for any $w_{i} \in S$, we have $d_{H}(w_{i}, w') \leq k$?  \\ 
     %   where $d_{H}(.,.)$ is the Hamming distance given as: $d_{H}(w,w') = \sum\limits_{i=1}^{ [|w|]} \mathrm{1}_{w_{j}}(w'_{j})$. \\

     %\textcolor{blue}{@Shahaf (Shortening suggestion): You can omit the rest of this subsection by delegating the details of the reduction from \texttt{CSP} to \texttt{LOC-I-SHAP}(\texttt{RNN-ReLu}, \texttt{IND}) to the appendix (see section \ref{app:ISHAPRNN} of the appendix). One possible way to conclude this subsection is by introducing a mathematical result stating the existence of a polynomial-time algorithm that performs the reduction e.g:
      %\begin{proposition}
      %    There exists a polynomial-time algorithm that takes as input an input instance $I = <S, k>$ of the \texttt{CSP} and outputs a RNN-ReLu $R_{I}$ such that:
      %    $$ \text{There exists no $k$-closest string of $S$ }  \iff R_{I} \text{ is empty}$$ 
      %\end{proposition} 
     %} 
     
     %The \texttt{CSP} problem is known to be NP-Hard \cite{li2000closest}. The reduction strategy consists at constructing in polynomial time a RNN that accepts only closest string strings for a given input instance $(S,k)$ of the \texttt{CSP} problem. Thus, the \texttt{CSP} outputs a YES answer on the given input instance if and only if the constructed RNN-ReLu is empty on the support $\Sigma^{n}$,  yielding straightforwardly the result of lemma \ref{lemma:emptyrnnrelu}. 
     %)
     %We structure the proof of this reduction strategy into two parts:
     %\begin{itemize}
     %    \item Given an arbitrary string $w$ and an integer $k > 0$, a construction of a RNN-ReLu that simulates the computation of the Hamming distance for $w$ such that the information of whether the hamming distance exceeds the threshold $k$ is encoded in the activation of a particular neuron withinin the constructed RNN-ReLu. We shall refer to this procedure under the name $\texttt{CONSTRUCT}(w,k)$.
    %     \item Concatenate the RNN-ReLus outputted by the procedure $\texttt{CONSTRUCT}(.,.)$ into a unifying RNN cell. Afterwards, we set a convenient output matrix that achieves the target objective of the construction.
    % \end{itemize}
     
    % \paragraph{The procedure \texttt{CONSTRUCT}($w,k$).}   Fix a string of reference $w \in \{0,1\}^{n}$, and an integer $k > 0$. The procedure $\texttt{CONSTRUCT}(w,k)$ returns  a RNN cell of dimension $|w| + 1$ that meets the satisfies the properties stated above discussion. The parametrization of $\texttt{CONSTRUCT}(w,k)$ is given as follows: 
%\begin{itemize}
%\item \textbf{The initial state vector.} $h_{init} = [1, 0 \ldots, 0 ]^{T} \in \mathbb{R}^{n+1}$
% \item \textbf{The transition matrix.} The transition matrix depends on $k$, and is given as
%\begin{equation}\label{transition}
%W_{k} = \begin{pmatrix}
%   0 & 0 & .& . & . & 0 & 0\\
%   1 & 0 & . & . & . & 0  & 0\\ 
%   0 & 1 & . & . & . & 0  & 0\\ 
%    . & . & . & . & . & .  & .\\
%   . & . & . & . & . & .  & . \\
%   0 & 0 & .& . & . & 0 & -k \\ 
%   0 & 0 & .. & . & . & 0 &  1 
%\end{pmatrix} \in \mathbb{R}^{(n+1) \times (n+1)}
%\end{equation}
%\item \textbf{The embedding vectors.} Embedding vectors depend on the string of reference $w$. In order to avoid confusion, we will index them with the superscript $w$. The construction of the embedding vectors is given as follows: \\ 
%For a symbol $\sigma \in \{0,1\}$ and $l \in [n]$, we have $v_{\sigma}^{w}[l] = 1 $ if $w_{l} \neq \sigma$. All the other elements of $v_{\sigma}^{w}$ are set to $0$. 
%\end{itemize}
%The following proposition highlights formally that this construction satisfies the sought property:
%\begin{proposition}\label{prop:reluproperties}
%Let $w \in \{0,1\}^{n}$ be an arbitrary string, and $k > 0$ be an arbitrary integer. The procedure $\emph{\texttt{CONSTRUCT}}(w,k)$ outputs a RNN cell $<h_{init}, W_{k}, \{v_{\sigma}^{(w)}\}_{\sigma \in \{0,1\}}>$ satisfies the following properties: 
%\begin{enumerate}
%    \item For any string $w' \in \{0,1\}^{s}$ where $ s < n$, we have $h_{w}[s] = d_{H}(w,w')$,
%    \item For a string $w' \in \{0,1\}^{n}$, we have 
    %\begin{equation}\label{eq:levsimulate}
     %   h_{w'}[n] = \text{ReLu}(d_{H}(w, w') - k)
    %\end{equation}
%\end{enumerate}
%\end{proposition}
%The most important property highlighted by Proposition \ref{prop:reluproperties} is given in Equation \ref{eq:levsimulate}. It shows that the $n$-th neuron of the constructed RNN Cell encodes the Hamming distance between the string of reference $w$ and the input string $w'$. Note that the activation value of this neuron is equal to $0$ if and only if $d_{H}(w,w') \leq k$. Otherwise, it is greater or equal to $1$.  

%\paragraph{Concatenation and Output Matrix instantiation.} 
%Let $<S, k>$ be an input instance of the closest string problem.The final RNN cell will be constructed as a concatenation of the RNN cells $<h_{init}, W_{k}, \{v_{\sigma}^{w^{(i)}}\}_{i \in [|S|]}$ where the set $\{w^{(i)}\}_{i \in |S|}$ are elements of $S$. \\ 
%The concatenation operation of two RNN cells, say $<h_{1}, W_{1}, v_{\sigma}^{(1)}>$ and $<h_{1}, W_{2}, v_{\sigma}^{(2)}>$, result in a new cell given as $<\begin{pmatrix}
%    h_{1} \\h_{2}
%\end{pmatrix}, \begin{pmatrix}
%    W_{1} & 0 \\ 
%    0 & W_{2}
%\end{pmatrix}$, $\{ \begin{pmatrix} v_{\sigma}^{1} \\ 
%v_{\sigma}^{2}
%\end{pmatrix}
%\}_{\sigma \in \Sigma}>$. %\footnote{Although the concatenation operation is non-commutative, the order according to which we perform the concatenation operator does't affect the result of the reduction.} \\
%We concatenate all RNN-ReLus resulting from the procedure \texttt{CONSTRUCT}(.,.) on the instances $\{(w,k)\}_{w \in S}$. 
%The output matrix of the resulting RNN-ReLu $O \in \mathbb{R}^{(n+1) \cdot |S|}$ is chosen such that, for any collection of vectors $\{h_{i}\}_{i \in [|S|]}$ in $\mathbb{R}^{n+1}$, we have:
%$$O^{T} \cdot \begin{bmatrix}
%    h_{1} & \ldots & h_{s}
%\end{bmatrix} = \sum\limits_{i \in [|S|]} - h_{i}[n] + \frac{1}{2} \cdot h_{1}[n+1]$$

%Under this setting, it is to be noted, in light of the properties of the RNN-ReLu cell outputted by the procedure $\texttt{CONSTRUCT}(.,.)$ (Proposition \ref{prop:reluproperties}), and the fact that the activation value of the $(n+1)$-neuron is always equal to $1$ by construction, the output of the constructed RNN-ReLu on an input sequence $w'$ is equal to $1$ if only if $w$ for all $w \in S$, we have $d_{H}(w,w') \leq k$. Consequently, the constructed RNN-ReLu is empty on the support $n$ if and only if there exists no string $w'$ such that for all $d_{H}(w,w') \leq k$. 