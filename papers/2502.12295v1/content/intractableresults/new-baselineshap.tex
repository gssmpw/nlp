\subsection{When Baseline SHAP is Hard} \label{subsec:baseline}
% Paragraph 1: Explain Baseline SHAP
In this subsection, we turn our focus to Baseline SHAP, which can be seen as a special case of Interventional SHAP when confined to empirical distributions induced by a reference input $\x^{\text{reff}}$. Hence, it is expected to be computationally simpler to compute than Interventional SHAP. However, we identify specific scenarios where calculating Baseline SHAP remains computationally challenging:
%This section aims to examine these scenarios and analyze the associated complexity hurdles. Specifically, we prove that:
\begin{theorem} \label{thm:intractable} \begin{inparaenum}[(i)] \item Unless P=co-NP, the problem \texttt{\emph{LOC-B-SHAP}}\texttt{\emph{(NN-SIGMOID)}} can not be solved in polynomial time; \item The decision versions of the problems $\emph{\texttt{LOC-B-SHAP}\texttt{(RNN-ReLu)}}$ and \emph{\texttt{LOC-B-SHAP}$\texttt{(ENS-DT}_{\texttt{C}}\texttt{)}$} %\footnote{Random Forests in the context of this theorem is the binary classifier that classifies an instance according to the majority vote mechanism.}   
are co-NP-Hard and NP-Hard respectively.
    \end{inparaenum}
\end{theorem}


% Introduce what follows in this section
\begin{comment}
In the sequel, we shall provide the proof sketch of the reduction made to prove  the Hardness of the problems \texttt{LOC-B-SHAP}(\texttt{SIGMOID}). Due to space constraints and the similarities between the reduction strategies employed for \texttt{LOC-B-SHAP}(\texttt{SIGMOID}) and \texttt{LOC-B-SHAP}(\texttt{RNN-ReLu}), we won't discuss this latter in the main article.
\end{comment}

We begin with results for \texttt{LOC-B-SHAP}(\texttt{NN-SIGMOID}) and \texttt{LOC-B-SHAP}(\texttt{RNN-ReLu}). Our proofs reduce from the Dummy Player problem of Weighted Majority Games~\citep{freixas2011complexity}:  %On the other hand, the reduction performed to show the NP-Hardness of the problem \texttt{B-SHAP}(\texttt{RF}) is made by reduction from the classical SAT problem.

%\subsubsection{On the Hardness of \texttt{LOC-B-SHAP}(\texttt{SIGMOID}) and  \texttt{LOC-B-SHAP}(\texttt{RNN-ReLu}).}
%As mentioned earlier, the reduction strategy to prove results regarding the problems \texttt{LOC-B-SHAP}(\texttt{SIGMOID}) and  \texttt{LOC-B-SHAP}(\texttt{RNN-ReLu}) in Theorem \ref{thm:intractable} is based on the dummy player problem of WMGs. Next, we provide a technical background of WMGs and the dummy problem. Afterwards, we provide a proof sketch of the reduction. The 
%full details of the reduction can be found in section \ref{app:BSHAPSIGMOID} of the Appendix. 
 
%\textcolor{blue}{@Shahaf (Shortening Hint): Is there any way to shorten the following background of WMGs?}
%\paragraph{Weighted Majority Games.} A weighted coalitional game is formally defined by a pair $G = (N,v)$, where $N$ is an integer referring to the number of players in the game, and $v$ is the value function of the game that maps each subset of players, referred to as a coalition, to the value generated by the coalition if they accept to cooperate in the game. In weighted games, players have different voting powers. A coalition is winning if the total weight of its members exceeds a predetermined threshold:
\begin{definition} \label{def:wmg}
A Weighted Majority Game (WMG) $G$ is a coalitional game defined by the tuple $  \langle N,\{n_{i}\}_{i \in [N]}, q\rangle$, where: \begin{inparaenum}[(i)] \item $N$ is the number of players; \item $n_{i}$ is the voting power of player $i$, for $i \in [N]$; \item $q$ is the winning quota. \end{inparaenum} The value function $v_{G}(S)$ is 1 if $\sum_{i \in S} n_{i} \geq q$, and 0 otherwise.

    %A Weighted majority game (WMG) $G$ is a coalitional game parametrized by the tuple $  <N,\{n_{i}\}_{i \in [N]}, q>$, where:
    %\begin{inparaenum}[(i)]
    %    \item $N$: The number of players,
    %    \item For $i \in [N]$, $n_{i}$ is an integer corresponding to the voting power of player $i$, 
    %    \item $q$ is an integer corresponding to the winning quota in the game  
    %\end{inparaenum}
    %The value function associated to the WMG $G$ is denoted as $v_{G}(S)$, and equals $1$ if $\sum\limits_{i \in S} n_{i} \geq q$, and $0$, otherwise.
    %$$v_{G}(S) = \begin{cases}
    %    1 & \text{if}~~ \sum\limits_{i \in S} n_{i} \geq q \\
    %    0 & \text{otherwise}
    %\end{cases}$$
\end{definition}

% The dummy player: Definition and associated computational problem
%\paragraph{Dummy players in WMGs.}

A \emph{dummy player} in a WMG $G$ is one whose voting power adds no value to any coalition. Formally, $i$ is a dummy in $G$ if $\forall S \subseteq [N] \setminus \{i\}$, it holds that $v_{G}(S \cup \{i\}) = v_{G}(S)$. Determining if a player $i$ is a dummy in $G$ is co-NP-Complete \citep{freixas2011complexity}.

%A \emph{dummy player} in a WMG $G$ is a player whose voting power brings no value to any coalition of players in the game. Formally, $i$ is dummy for $G$ iff $\forall S \subseteq N \setminus \{i\}$ it holds that $v_{G}(S \cup \{i\}) = v_{G}(S)$. Given a WMG $G$ and a player $i$ in $G$, deciding if the player $i$ is dummy is known to be co-NP-Complete \cite{freixas2011complexity}.

%$$\forall S \subseteq N \setminus \{i\}:~~v_{G}(S \cup \{i\}) = v_{G}(S)$$


\begin{comment}
An interesting characterization of dummy players in WMGs highlighted in the following proposition. This characterization will prove useful in our reduction proof to our target problems that will be detailed:
\begin{proposition}\label{prop:characterization_dummyplayers}
      Let $G = (N,\{n_{i}\}_{i \in [N]}, q)$ be a WMG. A player $i \in [N]$ is dummy if and only if:
     \begin{equation} \label{eq:expression}
     \forall S \subseteq [N] \setminus \{i\}:~~ \sum\limits_{j \in S} n_{j}  \notin [q - n_{i}, q]
     \end{equation}
\end{proposition}
The proof of this proposition can be found in Appendix. 
\end{comment}

\textbf{Reducing the dummy problem in WMG to both of the problems \texttt{LOC-B-SHAP}(\texttt{NN-SIGMOID}) and \texttt{LOC-B-SHAP}(\texttt{RNN-ReLu}).} 
Informally, given a WMG $G := \langle N, \{n_{j}\}_{j \in [N]}, q\rangle$, the reduction constructs a model $f_{G}$ over the input set $\{0,1\}^{N}$ from the target model family to simulate $G$'s value function using chosen input instances $\x,\x^{\text{reff}} \in \{0,1\}^{n}$, i.e., $f_{G}(\x_{S}; \x_{\bar{S}}^{\text{reff}}) \approx v(S)$. The properties of this reduction for both model families are formally stated as follows:


%Informally, given a WMG $G = <N, \{n_{j}\}_{j \in [N]}, q>$, the reduction strategy consists at constructing a model $f_{G}$  over the input set $\{0,1\}^{N}$ belonging to the target family of models to explain that simulates the value function of $G$ by a suitable choice of the input instances $(\x,\x^{ref}) \in \{0,1\}^{n}$, i.e.:     
%$f_{G}(\x_{S}; \x_{\bar{S}}^{(ref)}) \approx v(S)$. The properties of the algorithmic constructions for the reduction of both these families of models is formally stated as follows:
\begin{proposition} \label{prop:reductionsigmoid} 

There are poly-time algorithms that: \begin{inparaenum}[(i)] \item Given a WMG $G$ and player $i$, return a sigmoidal neural network $f_{G}$ over $\{0,1\}^{N}$, $\x,\x^{\text{reff}} \in \{0,1\}^{N}$ and $\epsilon \in \mathbb{R}$ such that $i$ is not dummy iff $\phi_{b}(f_{G},i,\x,\x^{\text{reff}}) > \epsilon$; \item Given a WMG $G$ and player $i$, return an RNN-ReLU $f_{G}$ over $\{0,1\}^{N}$, $\x,\x^{\textbf{reff}} \in \{0,1\}^{N}$ such that $i$ is not dummy iff $\phi_{b}(f_{G},i,\x,\x^{\text{reff}}) > 0$. \end{inparaenum}









%There exist polynomial-time algorithms that:
%    \begin{inparaenum}[(i)]
%    \item Given a WMG $G$ and a player $i$ in $G$, returns a sigmoidal neural network $f_{G}$ over $\{0,1\}^{N}$, $(\x,\x^{ref}) \in \{0,1\}^{N}$ and $\epsilon \in \mathbb{R}$ such that player $i$ is not dummy iff $\phi_{b}(f_{G},i,\x,\x^{ref}) > \epsilon$.
        %\textcolor{blue}{@Shahaf: The technical obstacle I found in the proof to upgrade this complexity result to co-NP-Hardness is that the parameter $\epsilon$ decreases exponentially with $N$ in my construction. Imagine $\epsilon = 2^{-N}$, a polynomial algorithm for the decision problem of LOC-B-SHAP(SIGMOID) is allowed to run polynomially with respect to $\frac{1}{\epsilon} = 2^{N}$. This technical subtelty prevents from obtaining the co-NP-Hardness of LOC-B-SHAP(SIGMOID).}
%        \item Given a WMG $G$ and a player $i$ in $G$, returns an RNN-ReLu $f_{G}$ over $\{0,1\}^{N}$, $(\x,\x^{ref}) \in \{0,1\}^{N}$ such that player $i$ is not dummy iff $\phi_{b}(f_{G},i,x,x^{ref}) > 0$
%    \end{inparaenum}
\end{proposition}

The complexity of computing \texttt{LOC-B-SHAP}(\texttt{SIGMOID}) and \texttt{LOC-B-SHAP}(\texttt{RNN-ReLu}) from Theorem \ref{thm:intractable} are corollaries of Proposition \ref{prop:reductionsigmoid}.

%\textcolor{blue}{@Shahaf: I write down the proof of the results of Theorem \ref{thm:intractable} for your understanding. You can decide if you want to refine it and keep it in the main text or delegate it to the appendix ..}
%\begin{proof}{(Theorem \ref{thm:intractable})}
%1. \texttt{SIGMOID}: Suppose there exists an algorithm $\mathcal{A}$ that computes exactly the problem \texttt{LOC-B-SHAP}(\texttt{SIGMOID}). We prove that we can decide the dummy problem in polynomial time using $\mathcal{A}$. The algorithm for deciding the dummy problem runs as follows: First, Run the polynomial-time algorithm whose existence is shown in point 1 of proposition \ref{prop:reductionsigmoid}, yielding the output $f_{G}, x,x^{ref}, \epsilon$. Second, run $\mathcal{A}$ on $f_{G}, i , x, x^{ref}$ yielding the $\phi_{b}(f_{G}, i, x, x^{ref})$. Finally, test if $\phi_{b}(f_{G}, i , x, x^{ref})$ is greater than $\epsilon$. By proposition \ref{prop:reductionsigmoid}, this algorithm decides in polynomial time if the player $i$ is dummy. Since the dummy problem is co-NP-Hard, the algorithm $\mathcal{A}$ doesn't exist unless \text{P=co-NP}.

%2. \texttt{RNN-ReLu}: Let $<G,i>$ be an input instance of the dummy player $i$. This input instance is reduced to the input instance $<f_{G}, i , x, x^{ref}, 0>$ of the decision problem associated to \texttt{LOC-B-SHAP}(\texttt{RNN-ReLu}) (using the polynomial-time algorithm whose existence is stated in point 2 of proposition \ref{prop:reductionsigmoid}). By this proposition, the player $i$ is dummy if and only $\phi_{b}(f_{G},i , x, x^{ref}) > 0$.
%    \end{proof}

\textbf{The problem \texttt{LOC-B-SHAP}($\texttt{ENS-DT}_{\texttt{C}}$) is NP-Hard.} 
This segment is dedicated to proving the remaining point of Theorem \ref{thm:intractable} stating the NP-Hardness of \texttt{LOC-B-SHAP}($\texttt{ENS-DT}_{\texttt{C}}$). We prove this by reducing from the classical NP-Complete 3-SAT problem. The reduction strategy is illustrated in Algorithm \ref{alg:SAT2b-SHAP}.

%\textcolor{blue}{@Shahaf: The reduction strategy detailed below is entirely copied and pasted to subsection \ref{app:subsec:bshaprf} (section \ref{app:BSHAPSIGMOID} of the appendix). If you want to delagate anything in the appendix from this segment, there where it should go ..}
%\paragraph{Reduction strategy.} The reduction strategy is illustrated in Algorithm \ref{alg:SAT2b-SHAP} (\textcolor{blue}{This algorithm is added to the appendix. It takes too much space. You can settle for describing the details of the construction and refer the reader to the appendix for the full steps of the algorithm to save space.} . For a given input CNF formula $\Phi$ over $n$ boolean variables $X = \{X_{1}, \ldots, X_{n}\}$ and $m$ clauses, the constructed random forest is a model whose set of input features contains the set $X$, with an additional feature denoted $X_{n+1}$ added for the sake of the reduction. The resulting random forest comprises a collection of $2m - 1$ decision trees, which can be categorized into two distinct groups:
%\begin{itemize}
%    \item $\mathcal{T}_{\Phi}$: A set of $m$ decision trees, each corresponding to a distinct clause in the input CNF formula. For a given clause $C$ in $\Phi$, the associated decision tree is constructed to assign a label 1 to all variable assignments that satisfy the clause $C$  while also ensuring that $x_{n+1} = 1$. It is easy to verify that such a decision tree can be constructed in polynomial time relative to the size of the input CNF formula
%
 %   \item \( \mathcal{T}_{null} \): This set consists of \( m - 1 \) copies of a trivial null decision tree. A null decision tree assigns a label of 0 to all input instances. 
%\end{itemize}
\begin{algorithm}
\caption{\texttt{3-SAT} to \texttt{LOC-B-SHAP}($\texttt{ENS-DT}_{\texttt{C}}$)}
\label{alg:SAT2b-SHAP}
\begin{algorithmic}[1]
\REQUIRE A CNF Formula $\Psi$ of $m$ clauses %$X = \{X_{1}, \ldots, X_{n} \}$
\ENSURE An instance of \texttt{LOC-B-SHAP}($\texttt{ENS-DT}_{\texttt{C}}$)
\STATE $\x \leftarrow [1, \ldots , 1]$
\STATE $\x^{\text{reff}} \leftarrow [0, \ldots , 0]$
\STATE $i \leftarrow n+1$
\STATE $\mathcal{T} \leftarrow \emptyset$
\FOR{$j \in [1,m]$}
 \STATE Construct a DT $T_{j}$ that assigns $1$ to variable assignments satisfying the formula: $C_{j} \land \x_{n+1}$.
 \STATE $\mathcal{T} \leftarrow \mathcal{T} \cup \{T\}_{j}$
\ENDFOR
 \STATE Construct a null decision tree $T_{\text{null}}$ that assigns a label $0$ to all variable assignments
 \STATE Add $m-1$ copies of $T_{\text{null}}$ to $\mathcal{T}$
\RETURN $\langle\mathcal{T},i,\x,\x^{\text{reff}}\rangle$
\end{algorithmic}
\end{algorithm}


The next proposition establishes a property of $\texttt{ENS-DT}_{\texttt{C}}$ used in Lemma~\ref{lemma:sat2bshap} to derive our complexity results:

%The next proposition establishes a property of $\texttt{ENS-DT}_{\texttt{C}}$ from the construction, which will be used in Lemma~\ref{lemma:sat2bshap} to derive the main result of this subsection:
%The next proposition provides a property of $\texttt{ENS-DT}_{\texttt{C}}$ resulting from the construction. This property shall be leveraged in Lemma \ref{lemma:sat2bshap} to yield the main result of this section:
\begin{proposition} \label{prop:dnf2bshaprf}
    Let $\Psi$ be an arbitrary \emph{CNF} formula over $n$ boolean variables, and  $\mathcal{T}$ be the ensemble of decision trees outputted by Algorithm \ref{alg:SAT2b-SHAP} for the input $\Psi$. 
    We have that $f_{\mathcal{T}}(\x_{1}, \ldots, \x_{n}, \x_{n+1}) = 1$ if $\x_{n+1} = 1 \land \x \models \Psi
          $, and $f_{\mathcal{T}}(\x_{1}, \ldots, \x_{n}, \x_{n+1})=0$, otherwise.
\end{proposition}
    %$$f_{\mathcal{T}}(x_{1}, \ldots, x_{n}, x_{n+1}) = \begin{cases}
    %      1 & \text{if} ~~ x_{n+1} = 1 \land x \models \Psi \\
    %      0 & \text{otherwise}
    %\end{cases}$$   
%\end{proposition}
%\textcolor{blue}{@Shahaf: The proof is placed here only for your understanding. It is also placed in the appendix. You can omit it from the main article once you understand it.}
%\begin{proof}
%    Fix an arbitrary CNF formula over $n$ boolean variables, and $(x_{1}, \ldots, x_{n})$ be an arbitrary variable assignment. If $x_{n+1} = 0$, decision trees in the set $\mathcal{T}$ assigns the label $0$, by construction. Consequently, $f_{\mathcal{T}}(x) = 0$.

%    For now, we assume that $x_{n+1} = 1$, if $x \models \Phi$, then $x$ is assigned the label 1 for all decision trees in $\mathcal{T}$. Consequently, $f_{\mathcal{T}}(x) = 1$. On the other hand, if $x$ is not satisfied by $\Phi$, then there exists at least one decision tree in $\mathcal{T}$, say $T_{j}$, that assigns a label 0 to $x$. Consequently, $x$ is assigned a label $0$ by at least $m$ decision trees, i.e. for all decision trees in $\mathcal{T}_{null}$ plus $T_{j}$ 
%\end{proof}
%\label{prop:dnf2bshaprf}


Using the result of Proposition~\ref{prop:dnf2bshaprf}, the following lemma directly establishes the NP-hardness of the decision problem for \texttt{LOC-B-SHAP}($\texttt{ENS-DT}_{\texttt{C}}$):

%Leveraging the result of proposition \ref{prop:dnf2bshaprf}, the following Lemma yields immediately the result of NP-Hardness of the decision problem associated to \texttt{LOC-B-SHAP}($\texttt{ENS-DT}_{\texttt{C}}$):
\begin{lemma} \label{lemma:sat2bshap}
    Let $\Psi$ be an arbitrary \emph{CNF} formula of $n$ variables, and  $\langle\mathcal{T},n+1,\x,\x^{\text{reff}}\rangle$ be the output of Algorithm \ref{alg:SAT2b-SHAP} for the input $\Psi$. We have that 
    $\phi_{b}(f_{\mathcal{T}}, n+1, \x, \x^{\text{reff}}) > 0$ iff $\exists \x \in \{0,1\}^{n}: ~ \x \models \Psi$. 
\end{lemma}


\section{Generalized Complexity Relations of SHAP Variants} \label{sec:generalized}


%\subsubsection{The Hardness of \texttt{LOC-B-SHAP}(.) implies the Hardness of \texttt{GLO-B-SHAP}(.,.) and \texttt{LOC-I-SHAP}(.,.).} 
%\textcolor{blue}{@Shahaf:I am not fully satisfied with my writing of this subsection (how to make it shine in the overall narrative of the article). If you can write a better pitch to introduce the result of proposition \ref{prop:hardnessrelation}, that would be good.}

While the previous sections presented specific results on the complexity of generating various SHAP variants for different models and distributions, this section aims to establish \emph{general} relationships concerning the complexity of different SHAP variants. We will then leverage these insights to derive corollaries for other SHAP contexts not explicitly covered in the paper.

%The objective of this final segment of this section is to complement negative complexity results provided in Theorem \ref{thm:intractable} with other results based on the hardness relations between the computational problems associated to different variants of SHAP scores, thus complementing the complexity results highlighted in Table \ref{fig:summaryresults}. Indeed, the following proposition highlights the existence of configurations where computing the Global B-SHAP and Local I-SHAP are at least as hard as computing Local B-SHAP:
\begin{proposition} \label{prop:hardnessrelation}
  \begin{inparaenum}[(i)]
    Let $\mathcal{M}$ be a class of models and $\mathcal{P}$ a class of probability distributions such that $\texttt{\emph{EMP}} \preceq_{P}  \mathcal{P}$. %\footnote{The symbol $\preceq_{P}$ signifies "polynomially reducible to"}. Then, 
    Then, \texttt{\emph{LOC-B-SHAP}}($\mathcal{M}$)  $\preceq_{P}$ \texttt{\emph{GLO-B-SHAP}}($\mathcal{M}$,$\mathcal{P}$) and \texttt{\emph{LOC-B-SHAP}}($\mathcal{M}$)  $\preceq_{P}$   \texttt{\emph{LOC-I-SHAP}}($\mathcal{M},\mathcal{P}$).
    \end{inparaenum}
\end{proposition}


In other words, assume a class of probability distributions $\mathcal{P}$ is "harder" (under polynomial reductions) than the class of empirical distributions \texttt{EMP}, i.e., any $P \in \mathcal{P}$ can be reduced in poly-time to some $P' \in$ \texttt{EMP}. Then, global baseline SHAP is at least as hard to compute as local baseline SHAP and local interventional SHAP is at least as hard to compute as local baseline SHAP (both under polynomial reductions). Since $\texttt{EMP} \preceq_{P} \texttt{HMM}$ (proof in Appendix~\ref{app:reductiontree}), these corollaries follow from Theorem~\ref{thm:intractable} and proposition~\ref{prop:hardnessrelation}:


%In other words, let us assume that a class of probability distributions $\mathcal{P}$ is ``harder'' (under polynomial reductions) than the class of empirical distributions \texttt{EMP}, i.e., any probability distribution $P\in\mathcal{P}$ can be reduced in polynomial time to a probability distribution $P'\in$\texttt{EMP}. Then, it satisfies that global baseline SHAP is at least as hard as local baseline SHAP (under polynomial reductions), and local interventional SHAP is at least as hard as local baseline SHAP (under polynomial reductions). Since $\texttt{\text{EMP}} \preceq_{P} \texttt{\text{HMM}}$ (proof in the appendix), we derive these corollaries from Theorem \ref{thm:intractable} and proposition~\ref{prop:hardnessrelation}:

%In other words, for any distribution class of probability distributions that encompasses the class \texttt{EMP} it holds that local baseline SHAP is at least as hard as global baseline SHAP (under polynomial reductions) and that local interventional SHAP is at least as hard as local baseline SHAP (under polynomial reductions. Since $\texttt{\text{EMP}} \preceq_{P} \texttt{\text{HMM}}$, (See proof \ref{app:reductiontree} of the supplementary material), we obtain these corollaries from Theorem \ref{thm:intractable} and proposition~\ref{prop:hardnessrelation}:

%\ref{prop:hardnessrelation}:
\begin{corollary} \label{cor:globshaphard}
    \begin{inparaenum}[(i)]
    Let there be some $\texttt{\emph{P}} \in \{ \texttt{\emph{EMP}}, \texttt{\emph{HMM}}\}$, and $\overrightarrow{\texttt{\emph{P}}} \in \{\texttt{\emph{EMP}}, \overrightarrow{\texttt{\emph{HMM}}} \}$. Then it holds that:
        \item%\texttt{\emph{SIGMOID}}
        Unless P=co-NP, the problems \texttt{\emph{GLO-B-SHAP}}\texttt{\emph{(SIGMOID}},$\overrightarrow{\texttt{\emph{P}}}$\texttt{\emph{)}} and \texttt{\emph{LOC-I-SHAP}}\texttt{\emph{(NN-SIGMOID}},$\overrightarrow{\texttt{\emph{P}}}$\texttt{\emph{)}} can not be computed exactly in polynomial time;
        \item %\texttt{\emph{RNN-ReLu}}:
        The decision version of the problems $\texttt{\emph{GLO-B-SHAP}}(\texttt{\emph{RNN-ReLu}}, \texttt{\emph{P}})$ and $\texttt{\emph{LOC-I-SHAP}}(\texttt{\emph{RNN-ReLu}}, \texttt{\emph{P}})$ are co-NP-Hard;
        \item %\texttt{\emph{RF}}:
        The decision version of the problems $\texttt{\emph{GLO-B-SHAP}\texttt{\emph{(M}},\texttt{\emph{P}}})$ and $\texttt{\emph{LOC-I-SHAP}}\texttt{\emph{(ENS-DT}}_{\texttt{\emph{C}}},\texttt{\emph{P)}}$ are NP-Hard. 
    \end{inparaenum} 
\end{corollary}


