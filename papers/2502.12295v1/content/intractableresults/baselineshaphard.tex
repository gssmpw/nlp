\subsection{When Baseline SHAP is Hard} \label{subsec:baseline}
The main result of this section is given in the following theorem:
\begin{theorem} \label{thm:intractable}
\begin{enumerate}
    \item \textcolor{green}{Unless P = NP, the problem \texttt{LOC-B-SHAP}(\texttt{SIGMOID}) can't be computed exactly in polynomial time.}
    \item The decision versions of the problem  and $\emph{\texttt{LOC-B-SHAP}(\texttt{RNN-ReLu})}$ is \emph{co-NP-Hard},
    \item The decision version of the problem $\emph{\texttt{LOC-B-SHAP}(\texttt{RF})}$ \footnote{Random Forests in the context of this theorem is the binary classifier that classifies an instance according to the majority vote mechanism.} is \emph{NP-Hard}.
    \end{enumerate}
\end{theorem}

The full proof of Theorem \ref{thm:intractable} can be found in Appendix. 

% Introduce what follows in this section
In the sequel, we shall provide the proof sketch of the reduction made to prove  the Hardness of the problems \texttt{LOC-B-SHAP}(\texttt{SIGMOID}). Due to space constraints and the similarities between the reduction strategies employed for \texttt{LOC-B-SHAP}(\texttt{SIGMOID}) and \texttt{LOC-B-SHAP}(\texttt{RNN-ReLu}), we won't discuss this latter in the main article.

The reduction strategy to prove results of Theorem \ref{thm:intractable} is based on top of the Dummy Player problem of Weighted Majority Games (WMGs) \cite{freixas2011complexity}, the reduction performed to show the NP-Hardness of the problem \texttt{B-SHAP}(\texttt{RF}) is made by reduction from the classical SAT problem.
\subsubsection{The problems \texttt{LOC-B-SHAP}(\texttt{SIGMOID}) and  \texttt{LOC-B-SHAP}(\texttt{RNN-ReLu}) are co-NP-Hard.}
As mentionned earlier, the reduction strategy to prove statement 1 of Theorem \ref{thm:intractable} is based on the dummy player problem of WMGs. The steps of this reduction strategy are similar for both these problems. Thus, we shall focus in the main text on the case of $\texttt{B-SHAP}(\texttt{SIGMOID})$. The case of RNN-ReLus will be fully examined in the appendix. 

Before outlining the steps of the reduction, we'll first introduce the required background of WMGs. 

\paragraph{Weighted Majority Games.} A coalitional game is formally defined by a pair $G = (N,v)$, where $N$ is an integer referring to the number of players in the game, and $v$ is the value function of the game that maps each subset of players, referred to as a coalition, to the value generated by the coalition if they accept to cooperate in the game.  

% What is a Weighted Majority game (formal definition, parametrization etc.)
A Weighted Majority Game (WMG) is a particular class of coalitional games suitable to model scenarios where players have different voting powers. A coalition is winning if the total weight of its members exceeds a predetermined threshold:
\begin{definition} \label{def:wmg}
    A WMG $G$ is a coalitional game parametrized by the tuple $  <N,\{n_{i}\}_{i \in [N]}, q>$, where:
    \begin{itemize}
        \item $N$: The number of players,
        \item For $i \in [N]$, $n_{i}$ is an integer corresponding to the voting power of player $i$, 
        \item $q$ is an integer corresponding to the winning quota in the game  
    \end{itemize}
    The value function associtated to the WMG $G$ is given as:
    $$v_{G}(S) = \begin{cases}
        1 & \text{if}~~ \sum\limits_{i \in S} n_{i} \geq q \\
        0 & \text{otherwise}
    \end{cases}$$
\end{definition}

% The dummy player: Definition and associated computational problem
%\paragraph{Dummy players in WMGs.} 

A \emph{dummy player} in a WMG $G$ is a player whose voting power brings no value to any coalition of players in the game, i.e a player $i$ is \textit{dummy} if: :
$$\forall S \subseteq N \setminus \{i\}:~~v_{G}(S \cup \{i\}) = v_{G}(S)$$

Deciding if a player is dummy in a WMG is known to be co-NP-Complete \cite{freixas2011complexity}.

\begin{comment}
An interesting characterization of dummy players in WMGs highlighted in the following proposition. This characterization will prove useful in our reduction proof to our target problems that will be detailed:
\begin{proposition}\label{prop:characterization_dummyplayers}
      Let $G = (N,\{n_{i}\}_{i \in [N]}, q)$ be a WMG. A player $i \in [N]$ is dummy if and only if:
     \begin{equation} \label{eq:expression}
     \forall S \subseteq [N] \setminus \{i\}:~~ \sum\limits_{j \in S} n_{j}  \notin [q - n_{i}, q]
     \end{equation}
\end{proposition}
The proof of this proposition can be found in Appendix. 
\end{comment}

\paragraph{Reduction of \texttt{DUMMY} to \texttt{LOC-B-SHAP}(\texttt{SIGMOID}).} Recall that a sigmoidal neural network $M$ of $N$ input features computes a function from $\mathbb{R}^{n}$ to $[0,1]$ given as:
$$f_{M}(x; \{w_{i}\}_{i \in [n]}, b) = \sigma(\sum\limits_{i = 1}^{N} w_{i} \cdot x_{i} + b)$$
where $\sigma(.)$ is the sigmoidal function. 

The strategy of the reduction consists at constructing a sigmoidal neural network $f_{G}$ that simulates a given WMG $G= N, \{n_{i}\}_{i \in [N]}, q$, such that: 
$$f_{G}(x_{S}; x_{\bar{S}}^{(ref)}) \approx v(S)$$
where the instance to explain $x$ is the vector $[1, \ldots, 1]$, and $x^{(ref)}$ is the vector $[0, \ldots, 0]$. 

If a player $i$ in the WMG $G$ is dummy, then, for any $S \subseteq N \setminus \{i\}$, we have:
$$f_{G}(x_{S \setminus \{i\}}; x_{\bar{S \setminus \{i\}}}^{(ref)}) - f_{G}(x_{S}; x_{\bar{S}}^{(ref)}) \approx 0$$
which implies that $\texttt{LOC-B-SHAP}(f_{G}, i, x, x^{(ref)}) \approx 0$. 

On the other hand, if the player $i$ is not dummy, then there must exist a coalition $S$ of size $N-1$ \footnote{By the monotonicity of the value function of a WMG, if $S$ is a winning coalition, then for any $S'$ such that $S \subseteq S'$, $S'$ is also a winning coalition.} in $N \setminus \{i\}$ such that: 
\begin{align*}
    f_{G}(x_{S \setminus \{i\}}; x_{\bar{S\setminus \{i\}}}^{ref}) - f_{G}(x_{S}; x_{\bar{S}}) \approx 1
\end{align*}
which implies that:
{\small 
\begin{align*}
\texttt{LOC-B-SHAP}(f_{G},i,x,x^{(ref)}) & \geq \frac{1}{N} [f_{G}(x_{S \setminus \{i\}}; x_{\bar{S\setminus \{i\}}}^{ref}) - f_{G}(x_{S}; x_{\bar{S}})] \\
    &\gtrapprox \frac{1}{N}
\end{align*}
}

{\small
\begin{algorithm}
\caption{Reduction of the \texttt{Dummy} problem to \texttt{LOC-B-SHAP}(\texttt{SIGMOID})}
\label{alg:WMG2b-SHAP}
\begin{algorithmic}[1]
\REQUIRE A WMG $G = <N, \{n_{j}\}_{j \in [N]},q>$, $i \in [N]$
\ENSURE A Sigmoidal Neural Network $\sigma$ over $\mathbb{R}^{N}$, an integer $i \in [N]$, $(x,x^{ref}) \in \mathbb{R}^{2},~\epsilon \geq 0$ 
\STATE $x \leftarrow [1 , \ldots , 1]$
\STATE $x^{ref} \leftarrow [0, \ldots , 0]$
\STATE $\epsilon \leftarrow \frac{1}{N + 1}$
\STATE $C \leftarrow 2 \cdot \log(N)$
\STATE Construct the Sigmoidal Neural Network $f$ given as:
$$f_{G}(x) = \sigma(C \cdot (x - q + \frac{1}{2}))$$
\RETURN $f_{G},~i,~x,~x^{(ref)},~\epsilon$
\end{algorithmic}
\end{algorithm}
}

Algorithm \ref{alg:WMG2b-SHAP} provides the exact steps of the reduction. 

The following theorem provides a proof of the correctness of the reduction outlined in algorithm \ref{alg:WMG2b-SHAP}, formalizing the intuition behind the reduction discussed above:
\begin{proposition} \label{prop:reductionsigmoid}
    Let $G = (N, \{n_{i}\}_{i \in [N]}, b)$ be a WMG, and $<f_{G}, i, x,x^{(ref)}, \epsilon>$ be the output of Algorithm \ref{alg:WMG2b-SHAP} on the input $G$. 

    The player $i$ is dummy if and only if \emph{\texttt{LOC-B-SHAP}}$(f_{G}, i , x, x^{(ref)}) \leq \epsilon$.
\end{proposition}

The complete proof of Proposition \ref{prop:reductionsigmoid} can be found in section \ref{app:BSHAPSIGMOID} of the supplementary material.

\paragraph{The Hardness of \texttt{LOC-B-SHAP}(.) implies the Hardness of \texttt{GLO-B-SHAP}(.,.) and \texttt{LOC-I-SHAP}(.,.).} The objective of this final segment of this section is to complement negative complexity results provided in Theorem \ref{thm:intractable} with other results based on the hardness relations between the computational problems associated to different variants of SHAP scores, thus completing the results illustrated in Table \ref{fig:summaryresults}. Indeed, the following proposition highlights the existence of configurations where computing the Global B-SHAP and Local I-SHAP are at least as hard as computing Local B-SHAP:
\begin{proposition} \label{prop:hardnessrelation}
    Let $\mathcal{M}$ be a class of models and $\mathcal{P}$ a class of probability distributions such that $\texttt{\emph{EMP}} \preceq_{P}  \mathcal{P}$ \footnote{The symbol $\preceq_{P}$ signifies "polynomially reducible to"}. Then, 
    \texttt{\emph{LOC-B-SHAP}}($\mathcal{M}$)  $\preceq_{P}$ \texttt{\emph{GLO-B-SHAP}}($\mathcal{M}$, $\mathcal{P}$) and \texttt{\emph{LOC-B-SHAP}}($\mathcal{M}$)  $\preceq_{P}$   \texttt{\emph{LOC-I-SHAP}}( $\mathcal{M}, \mathcal{P}$) .
\end{proposition}

Given that $\texttt{\text{EMP}} \preceq \texttt{\text{HMM}}$ (See section \ref{app:reductiontree} of the supplementary material), one can obtain immediate corollaries from Theorem \ref{thm:intractable} and proposition \ref{prop:hardnessrelation}:
\begin{corollary} \label{cor:globshaphard}
    \begin{enumerate}
        Let $\texttt{\emph{P}} \in \{ \texttt{\emph{EMP}}, \texttt{\emph{HMM}}\}$.
        \item For any $\texttt{\emph{M}} \in \{\texttt{\emph{SIGMOID}}, \texttt{\emph{RNN-ReLu}} \}$, the decision version of the problems $\texttt{\emph{GLO-B-SHAP}}(\texttt{\emph{M}}, \texttt{\emph{P}})$ and $\texttt{\emph{LOC-I-SHAP}}(\texttt{\emph{P}})$ are \emph{co-NP-Hard}.
        \item The decision version of the problems  $\texttt{\emph{GLO-B-SHAP}(\texttt{\emph{M}}, \texttt{\emph{P}}})$ and $\texttt{\emph{LOC-I-SHAP}}(\texttt{\emph{P}})$ are \emph{NP-Hard}. 
    \end{enumerate} 
\end{corollary}


