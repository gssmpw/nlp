%\subsection{Local and Global I-SHAP for the family of Weighted Automata} 
\subsection{Tractability for WAs} \label{subsec:shapwa}
This main result of this subsection is given in the following theorem:
\begin{theorem} \label{thm:shapwa}
    The following computational problems, which include \emph{\texttt{LOC-I-SHAP}}\emph{\texttt{(WA}},\emph{\texttt{HMM)}}, \emph{\texttt{GLO-I-SHAP}}\emph{\texttt{(WA}},\emph{\texttt{HMM)}}, \emph{\texttt{LOC-B-SHAP}}\emph{\texttt{(WA)}}, as well as \emph{\texttt{GLO-B-SHAP}}\emph{\texttt{(WA}},\emph{\texttt{HMM)}} are poly-time computable with respect to the size of the \text{WA}, the size of the \text{HMM}, the sequence length and the size of the alphabet.
\end{theorem}

The remainder of this section provides a proof sketch for Theorem \ref{thm:shapwa}, with the complete proof available in Appendix~\ref{app:shapwa}. The proof is constructive and draws heavily on techniques from the theory of rational languages \citep{berstel88}. We begin by linking the previously defined swap and do operators for patterns to the computation of (interventional) SHAP values. The corresponding relations for baseline SHAP are provided in Appendix~\ref{app:shapwa} due to space constraints.

\begin{lemma}
For a sequential model $f$, a string $w$ (representing an input $\x\in\mathcal{X}$), a pattern $p$ (representing a coalition $S\subseteq [n]$), and a distribution $\mathcal{D}_p$ over $\mathcal{X}$, then the following relations hold:
\begin{equation}
\begin{aligned}
        v_{i}(f,w,p,\mathcal{D}_p) = \mathbb{E}_{\z \sim \mathcal{D}_p^{|w|}} \left[ f(\texttt{do}(p,\z,w)) \right]; \\
        \phi_i(f,w,i,\mathcal{D}_p) = \mathbb{E}_{p \sim \mathcal{P}_i^{\x}} [v_{i}(f,w,\texttt{swap}(p,w_{i},i),\mathcal{D}_p) \nonumber 
 \\ - v_{i}(f,w,p,\mathcal{D}_p)] \quad\quad\quad\quad\quad\quad
\end{aligned}
\end{equation}
where:
\begin{equation*}
\begin{aligned}
\mathcal{P}_{i}^{w}(p)  \myeq \begin{cases}
\frac{(|p|_{\#}-1)! \cdot (|w| - |p|_{\#})!}{|w|!} & \text{if}~~ w \in L_{p} \\ 
0 & \text{otherwise}
\end{cases}
\end{aligned}
\end{equation*}
\end{lemma}

To develop an algorithm for computing $v_i$ and $\phi_i$ in polynomial time for the class of WA under HMM distributions, we utilize two operations on N-Alphabet WA, parameterized by the input instance:

\begin{definition} \label{def:projectionoperation}
  Let $N > 0$ be an integer and $\{\Sigma_{i}\}_{i \in [N]}$ be a collection of $N$ alphabets.
  \begin{enumerate}  
  \item \textbf{The Kronecker product operation} of two $N$-Alphabet WAs $A$ and $B$ over $\Sigma_{1} \times \ldots \times \Sigma_{N}$ at index $i \in [N]$ returns an $N$-Alphabet WA over $\Sigma_{1} \times \ldots \times \Sigma_{N}$, denoted $A \otimes B$ 
  implementing the function:
  \begin{align*} 
  f_{A \otimes B}(w^{(1)},\ldots, w^{(n)}) :=& f_{A}(w^{(1)},\ldots, w^{(n)}) \\
  & \cdot f_{B}(w^{(1)},\ldots, w^{(n)})
  \end{align*}
  \item \textbf{The projection operation} of a 1-Alphabet WA $A$ over an N-Alphabet WA $T$ over $\Sigma_{1} \times \ldots \times \Sigma_{N}$ at index $i \in [N]$, returns an (N-1)-Alphabet WA over $\Sigma_{1} \times \ldots \times \Sigma_{i-1} \times \Sigma_{i+1} \times \ldots \times \Sigma_{N}$, denoted $\Pi_{i}(A,T)$, implementing the following function:
       \begin{align*}
           g(w^{(1)}, \ldots, w^{(i-1)}, w^{(i+1)}, w^{(N)}) := ~~~~~~~~~~~~  \\ 
           \sum\limits_{w \in \Sigma_{i}^{L}} f_{A}(w) \cdot f_{T}(w^{(1)}, \ldots w^{(i)}, w, w^{(i+1)}, \ldots, w^{(N)})
        \end{align*}
       where $(w^{(1)}, \ldots, w^{(i-1)}, w^{(i+1)}, \ldots w^{(N)}) \in \Sigma_{1}^{*} \times \ldots \times \Sigma_{i-1}^{*} \times \Sigma_{i+1}^{*} \ldots \times \Sigma_{N}^{*}$ such that $|w^{(1)}| = \ldots =|w^{(i-1)}| = |w^{(i+1)}| = \ldots = |w^{(N)}| = L$.
       \end{enumerate}
\end{definition}

Note that if $T$ is a 1-Alphabet WA, then the returned model $\Pi_{1}(A,T)$ is a 0-Alphabet WA. For simplicity, we denote: $\Pi_{1}(A,T) \myeq \sum\limits_{w \in \Sigma_{1}^{}}$ $f_{A}(w) \cdot f_{T}(w)$, yielding a scalar. Additionally, we define $\Pi_{0}(A) \myeq \Pi_{1}(A,\mathbf{1})$, where $\mathbf{1}$ is a WA that assigns $1$ to all sequences in $\Sigma^{}$. The next proposition provides a useful intermediary result, with its proof in Appendix~\ref{app:shapwa}:

%An intermediary computational result useful for the following sections is presented in the next proposition, with its proof in Appendix~\ref{app:shapwa}:

%An intermediary computational result  regarding these operations that will be useful in the sequel is stated in the following proposition whose proof  appears in Appendix~\ref{app:shapwa}:








%Note that if $T$ is a 1-Alphabet WA, the returned model $\Pi_{1}(A,T)$ is a 0-Alphabet WA. To ease exposition, we shall abuse notation and adopt the following convention for this case: $\Pi_{1}(A,T) \myeq \sum\limits_{w \in \Sigma_{1}^{*}}$ $f_{A}(w) \cdot f_{T}(w)$, thus returning a scalar. In addition, $\Pi_{0}(A) \myeq \Pi_{1}(A,\mathbf{1})$ where $\mathbf{1}$ refers to the trivial (single state) WA that assigns a label $1$ to all sequences in $\Sigma^{*}$.  
 
 %that they can be implemented in polynomial time (provided that the number of alphabets $N$ is constant): 
\begin{proposition}\label{prop:efficentoperations}
If $N=O(1)$, the projection and Kronecker product operations are poly-time computable.
\end{proposition}

\begin{comment}
 The proof can be found in the appendix. The running time complexity of the projection  (resp. the Kronecker product) operation is equal to $O\left(|\Sigma_{i}|^{N} \cdot \texttt{size}(A) \cdot \texttt{size}(T)\right)$ (resp. $O\left((\max\limits_{i \in [N]} 
 |\Sigma_{i}|)^{N} \cdot \texttt{size}(A) \cdot \texttt{size}(B) \right)$. The size of their output N-Alphabet WAs is equal to the product of the size of their respective input N-Alphabet WAs.
\end{comment}

\textbf{Interventional and Baseline SHAP of WAs in terms of N-Alphabet WA operators.}
As mentioned earlier, the algorithmic construction for $\texttt{LOC-I-SHAP}(\texttt{WA}, \texttt{HMM})$, and $\texttt{GLO-I-SHAP}(\texttt{WA}, \texttt{HMM})$ will take the form of efficiencly computable operations over N-Alphabet WAs parameterized by the input instance of the corresponding problem. The main lemma formalizing this fact is given as follows:
\begin{lemma} \label{lemma:shapasoperations}
Fix a finite alphabet $\Sigma$. Let $f$ be a WA over $\Sigma$, and consider a sequence $(w, w^{\text{reff}}) \in \Sigma^{*} \times \Sigma^{}$ (representing an input and a basline $\x, \x^{\text{reff}} \in \mathcal{X}$) such that $|w| = |w^{\text{reff}}|$. Let $i \in [|w|]$ be an integer, and $\mathcal{D}_P$ be a distribution modeled by an HMM over $\Sigma$. Then:
%Fix a finite alphabet $\Sigma$. Let $f$ be a WA over $\Sigma$, and consider a sequence $(w, w^{\text{reff}}) \in \Sigma^{*} \times \Sigma^{*}$ (representing an input $\x \in \mathcal{X}$ and an auxiliary baseline $\x^{\text{reff}} \in \mathcal{X}$), such that $|w| = |w^{\text{reff}}|$. Additionally, let $i \in [|w|]$ be an integer, and $\mathcal{D}_P$ be a distribution modeled by an HMM over $\Sigma$. We have that:
 %Fix a finite alphabet $\Sigma$. Let $f$ be a WA over $\Sigma$, a sequence $(w,w^{\text{ref}}) \in \Sigma^{*} \times \Sigma^{*}$ (representing an input $\x\in\mathcal{X}$ and an auxialry basline $\x^{\text{ref}}\in\mathcal{X}$) such that $|w| = |w^{\text{ref}}|$, an integer $i \in [|w|]$, and a distribution $\mathcal{D}_P$ modeled by an HMM over $\Sigma$. We have:
         {\small 
            \begin{align*}
         %\emph{\texttt{LOC-I-SHAP}}
         \phi_i
         (f,w,i,\mathcal{D}_P) = \quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad \\ \Pi_{1} (A_{w,i}, \Pi_{2}(\mathcal{D}_P, \Pi_{3}(f,T_{w,i}) 
          - \Pi_{3}(f,T_{w}) ) ); \quad\quad \\
            %\end{align*}
            %}
            %{\small 
            %\begin{align*}  
            \Phi_i(f,i,n,\mathcal{D}_P) = \quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad \\ 
            \Pi_{0} ( \Pi_{2}(\mathcal{D}_P, A_{i,n} \otimes \Pi_{2}(\mathcal{D}_P, 
             \Pi_{3}(f,T_{i}) - \Pi_{3}(f,T)))); \quad\\
            %\end{align*}
            %}
            %{\small 
            %\begin{align*}
            \phi_b(f,w,i,w^{\text{reff}}) = \quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad \\ \Pi_{1} (A_{w,i}, \Pi_{2}(f_{w^{\text{reff}}}, \Pi_{3}(f,T_{w,i}) 
          - \Pi_{3}(f,T_{w})));\quad \\
            %\end{align*}    
            %}
            % {\small 
            %  \begin{align*}
            \Phi_b(f,i,n,w^{\text{reff}},\mathcal{D}_P) = \quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad \\ \Pi_{0} ( \Pi_{2}(\mathcal{D}_P, A_{i,n} \otimes \Pi_{2}(f_{w^{\text{reff}}} ,
            \Pi_{3}(f,T_{i}) - \Pi_{3}(f,T)) ))
               \end{align*}
             } where:
    \begin{itemize}
        \item $A_{w,i}$ is a 1-Alphabet WA over $\Sigma_{\#}$ implementing the uniform distribution over coalitions excluding the feature $i$ (i.e., $f_{A_{w,i}} = \mathcal{P}_{i}^{w}$);
        \item $T_{w}$ is a 3-Alphabet \emph{WA} over $\Sigma_{\#} \times \Sigma \times \Sigma$ implementing the function: $ g_{w}(p,w',u) := I(\texttt{do}(p,w',w) = u)$.
        %\begin{equation} \label{eq:Tw}
        %    g_{w}(p,w',u) = I(\texttt{do}(p,w',w) = u)
        %\end{equation}
        \item $T_{w,i}$ is a 3-Alphabet \emph{WA} over $\Sigma_{\#} \times \Sigma \times \Sigma$ implementing the function: $            g_{\x,i}(p,w',u) := I(\texttt{do}(\texttt{swap}(p,w_{i},i),w',w) = u)$.
        %\begin{equation} \label{eq:Twi}
        %    g_{w,i}(p,w',u) = I(\texttt{do}(\texttt{swap}(p,w_{i},i),w',w) = u)
        %\end{equation}
        \item $T$ is a 4-Alphabet \emph{WA} over $\Sigma_{\#} \times \Sigma \times \Sigma \times \Sigma$ given as: $g(p,w',u,w) := g_{w}(p,w',u)$.
        %\begin{equation} \label{eq:T}
        %    g(p,w',u,w) = g_{w}(p,w',u)
        %\end{equation}
        \item $T_{i}$ is a 4-Alphabet \emph{WA} over $\Sigma_{\#} \times \Sigma \times \Sigma \times \Sigma$ given as: $ g_{i}(p,w',u,w) := g_{w,i}(p,w',u)$.
        %\begin{equation} \label{eq:Ti}
        %    g_{i}(p,w',u,w) = g_{w,i}(p,w',u)
        %\end{equation}
        \item $A_{i,n}$ is a 2-Alphabet \emph{WA} over $\Sigma_{\#} \times \Sigma$ implementing the function:
         $g_{i,n}(p,w) := I(p \in \mathcal{L}_{i}^{w}) \cdot \mathcal{P}_{i}^{w}(p)$,
         where $|w| = |p| = n$.
       \item $f_{w^{\text{reff}}}$ is an \emph{HMM} such that the probability of generating $w^{\text{reff}}$ as a prefix is equal to $1$.
    \end{itemize}
\end{lemma}



The proof of Lemma~\ref{lemma:shapasoperations} is in Appendix~\ref{app:shapwa}. In essense, it reformulates the computation of both local and global interventional and baseline SHAP using operations on N-Alphabet WAs, depending on the input instance, particularly involving $A_{w,i}$, $T_{w}$, $T_{w,i}$, $T$, $T_{i}$, $A_{i,n}$, and $f_{w^{\text{ref}}}$. The final step to complete the proof of Theorem~\ref{thm:shapwa} is to show that these WAs can be constructed in polynomial time relative to the input size.


%The proof of Lemma~\ref{lemma:shapasoperations} is provided Appendix~\ref{}. In essense, this lemma reformulates the computation of both Local and Global Interventional SHAP scores using operations on N-Letter WAs, which depend on the specific input instance of the problem, particularly involving $A_{w,i},~T_{w},~T_{w,i},~T,~T_{i},~A_{i,n}$, and $M_{w^{(ref)}}$. The final step needed to complete the proof of Theorem~\ref{thm:shapwa} is to show that these N-Alphabet WAs can be constructed in polynomial time relative to the size of the input instance for these problems.

%The proof of Lemma \ref{lemma:shapasoperations} can be found in the appendix. 

%Lemma \ref{lemma:shapasoperations} reformulates the computation of Local and Global Interventional SHAP score in terms of operations over N-Letter WAs that depend on the input instance of the problem (specifically, $A_{w,i},~T_{w},~T_{w,i},~T,~T_{i},~A_{i,n}$ and $M_{w^{(ref)}}$). 

%The missing link to complete the proof of Theorem \ref{thm:shapwa} is to demonstrate that these N-Alphabet WAs can be constructed in polynomial time with respect to the size of the input instance of these problems:

\begin{proposition} \label{prop:nletterwaconstruction}
   The N-Alphabet WAs $A_{w,i}$, $T_{w}$, $T_{w,i}$, $T$, $T_{i}$, $A_{i,n}$ (defined in Lemma \ref{lemma:shapasoperations}) and the HMM $f_{w^{\text{reff}}}$ can be constructed in polynomial time with respect to $|w|$ and $|\Sigma|$.
\end{proposition}

The full proof of Proposition \ref{prop:nletterwaconstruction} can be found in Appendix~\ref{app:shapwa}. Theorem \ref{thm:shapwa} is a direct corollary of Lemma \ref{lemma:shapasoperations}, Proposition \ref{prop:efficentoperations}, and Proposition \ref{prop:nletterwaconstruction}.

%\paragraph{Complexity.} [\textcolor{blue}{\textbf{@SHAHAF: Wouldn't it be better to include a small table (in a one-column) page instead of a paragraph to provide the complexity results? The rows are: (LOC and GLO) and the columns are (I and B) for instance..}}]The algorithmic construction for solving the problems (\texttt{LOC} | \texttt{GLO})-(\texttt{I}|\texttt{B})-\texttt{SHAP}(\texttt{WA}, \texttt{HMM}), as highlighted in Lemma \ref{lemma:shapasoperations} involves the construction of N-Letter WAs that depend on the input instance and performing tractable operations over them to yield the solution of these problems. The complexity of $\texttt{LOC-I-SHAP}(\texttt{WA}, \texttt{})$ is .. 

\subsection{Tractability for other ML models.} \label{subsec:tree2WA}

%\paragraph{From WAs to Tree-based models.} 
Beyond the proper interest of the result in Theorem \ref{thm:shapwa} regarding WAs, it also yields interesting results about the computational complexity of obtaining interventional and baseline SHAP variants for other popular ML models which include decision trees, tree ensembles for regression tasks (e.g. Random forests or XGBoost), and linear regression models:

%$\texttt{DT}$,
%$\texttt{ENS-DT}_{\texttt{C}}$, %$\texttt{ENS-DT}_{\texttt{R}}$, 
%$\texttt{ENS-DT}_{\texttt{C}}$

\begin{theorem} \label{cor:reductions}
    Let $\mathbb{S}:= \{ \emph{\texttt{LOC}}, \emph{\texttt{GLO}} \}$, $\mathbb{V} := \{\texttt{\emph{B}}, \texttt{\emph{I}}\}$, $\mathbb{P}:= \{ \texttt{\emph{EMP}}, \overrightarrow{\emph{\texttt{HMM}}} \} $, and $\mathbb{F}:= \{\emph{\texttt{DT}}, \emph{\texttt{ENS-DT}}_{\emph{\texttt{R}}}, \emph{\texttt{Lin}}_{\emph{\texttt{R}}}\}$. Then, for any \emph{\texttt{S}} $\in \mathbb{S}$, $\texttt{\emph{V}} \in \mathbb{V}$, $\texttt{\emph{P}} \in \mathbb{P}$, and $\emph{\texttt{F}}\in \mathbb{F}$ the problem  $\emph{\texttt{S-V-SHAP}}(\emph{\texttt{F}}, \texttt{\emph{P}})$ can be solved in polynomial time.
\end{theorem}

The proof of Theorem~\ref{cor:reductions} is provided in Appendix~\ref{app:reductiontree}, where a poly-time construction of either a decision tree, an ensemble of decision trees for regression, or a linear regression model into a WA is detailed. This result brings forward two interesting outcomes. First, it expands the distributional assumptions of some popular SHAP algorithms such as LinearSHAP and TreeSHAP. Let us denote $\texttt{TREE}$ as the family of all decision trees $\texttt{DT}$ and regression tree ensembles $\texttt{ENS-DT}_{\texttt{R}}$. Then:

\begin{corollary}
\label{treeshap_corollary}
    While the \emph{TreeSHAP}~\citep{lundbergnature} algorithm solves \texttt{\emph{LOC-I-SHAP}}\texttt{\emph{(TREE}},\texttt{\emph{EMP)}} and \texttt{\emph{GLO-I-SHAP}}\texttt{\emph{(TREE}},\texttt{\emph{EMP)}} in poly-time, Theorem~\ref{cor:reductions} establishes that \texttt{\emph{LOC-I-SHAP}}\texttt{\emph{(TREE}},$\overrightarrow{\emph{\texttt{\text{HMM}}}}$\texttt{\emph{)}} and \texttt{\emph{GLO-I-SHAP}}\texttt{\emph{(TREE}},$\overrightarrow{\emph{\texttt{\text{HMM}}}}$\texttt{\emph{)}} can be solved in poly-time.
\end{corollary}

Since empirical distributions are strictly contained within HMMs, i.e., $\texttt{EMP}\subsetneq \texttt{HMM}$ and $\texttt{EMP}\subsetneq \overrightarrow{\texttt{HMM}}$ (see proof in Appendix~\ref{app:reductiontree}), Corollary~\ref{treeshap_corollary} significantly broadens the distributional assumption of the TreeSHAP algorithm beyond just empirical distributions. Similar conclusions can be drawn for LinearSHAP:

\begin{corollary}
    While the \emph{LinearSHAP}~\citep{lundberg2017} algorithm solves \texttt{\emph{LOC-I-SHAP}}$\texttt{\emph{(LIN}}_\texttt{\emph{R}}$,\texttt{\emph{IND}}) in polynomial time, Theorem~\ref{cor:reductions} establishes that \texttt{\emph{LOC-I-SHAP}}$\texttt{\emph{(LIN}}_\texttt{\emph{R}}$,$\overrightarrow{\emph{\texttt{\text{HMM}}}}$\texttt{\emph{)}} and \texttt{\emph{GLO-I-SHAP}}$\texttt{\emph{(LIN}}_\texttt{\emph{R}}$,$\overrightarrow{\emph{\texttt{\text{HMM}}}}$\texttt{\emph{)}} can be solved in polynomial time.
\end{corollary}

Which, again, demonstrates the expansion of the distributional assumption of LinearSHAP, as $\texttt{IND}\subsetneq \overrightarrow{\texttt{HMM}}$ (see Appendix~\ref{app:reductiontree}). 
Lastly, Theorem~\ref{cor:reductions} establishes a \emph{strict computational complexity gap} between computing \emph{conditional} SHAP, which remains intractable even for simple models like decision trees under HMM distributions (and even under empirical distributions~\citep{vander21}), whereas computing both local interventional and baseline SHAP values are shown to be tractable. This suggests that interventional and baseline SHAP are strictly more efficient to compute in these settings.

%Lastly,~\ref{cor:reductions} also establishes a \emph{strict computational complexity gap} between the computation of \emph{conditional} SHAP, which is intractable to compute for simple models such as decision trees even under HMM distributions (and even under empirical distributions~\citep{vander21}), while both interventional and baseline SHAP prove to be tractable. This provides evidence that interventional and baseline SHAP are strictly more efficient in these settings:

\begin{corollary}
    If $f\in\{\emph{\texttt{WA}},\emph{\texttt{DT}},\emph{\texttt{ENS-DT}}_{\emph{\texttt{R}}},\emph{\texttt{Lin}}_{\emph{\texttt{R}}}\}$, $\mathcal{D}_p:=\overrightarrow{\emph{\texttt{\text{HMM}}}}$ (or $\mathcal{D}_p:=\emph{\texttt{HMM}}$ for \emph{\texttt{WA}}), and assuming P$\neq$NP, then computing local interventional SHAP or local baseline SHAP for $f$ \emph{is strictly more computationally tractable} than computing local conditional SHAP for $f$.
\end{corollary}

%The proof of corollary \ref{cor:reductions} can be found in the appendix, where a polynomial-time construction either a decision tree, an ensemble of decision trees for regression, or a linear regression model to a WA is provided.

%The most interesting result in corollary \ref{cor:reductions} is the one establishing the tractability of computing both local and global I-SHAP scores variants for Tree-based models under the distribution class $\overrightarrow{\texttt{\text{HMM}}}$. Notably, this extends the popular TreeSHAP algorithm \cite{lundbergnature} in the literature of Explainable AI: While the TreeSHAP algorithm efficiently solves the problems \texttt{LOC-I-SHAP}(\texttt{TREE}, \texttt{EMP}) and \texttt{GLO-I-SHAP}(\texttt{TREE}, \texttt{EMP}), corollary \ref{cor:reductions} establishes the existence of  polynomial-time algorithms to compute local and global I-SHAP scores of Tree-based models under the broader distribution class $\overrightarrow{\texttt{\text{HMM}}}$. The fact that $\texttt{\text{EMP}} \subset \overrightarrow{\texttt{\text{HMM}}}$ is proven in section \ref{app:reductiontree} of the supplementary material.       
