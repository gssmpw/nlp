\documentclass[twoside]{article}
%\usepackage{aistats2025}
\usepackage[round]{natbib}
\bibliographystyle{abbrvnat}
\usepackage{amssymb}
%\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{url}
\usepackage{amsthm}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{comment}
\usepackage{xr}
\usepackage{adjustbox}
\usepackage{caption}
\usepackage{multirow}
\usepackage{tabularx}
\usepackage{subcaption}
\usepackage{paralist}
\usepackage[table]{xcolor}
\usepackage{booktabs}
\usepackage{tikz}
\usetikzlibrary{tikzmark}


\newcommand{\z}{\textbf{z}}
\newcommand{\x}{\textbf{x}}

\newcommand{\shahaf}[1]{\marginpar{\textcolor{green}{Shahaf: #1}}}

\newcommand{\reda}[1]{\marginpar{\textcolor{blue}{Reda: #1}}}

\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\usepackage{enumitem}
\setlist[enumerate]{itemsep=0pt, topsep=0pt} 
\setlist[itemize]{itemsep=0pt, topsep=0pt}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}
\newtheorem{proposition}{Proposition}
\newtheorem{definition}{Definition}
\newcommand{\ar}[1]{\overrightarrow{#1}}
\newcommand{\ttt}[1]{\texttt{#1}}

\makeatletter

\newcommand*{\addFileDependency}[1]{% argument=file name and extension
\typeout{(#1)}% latexmk will find this if $recorder=0
% however, in that case, it will ignore #1 if it is a .aux or 
% .pdf file etc and it exists! If it doesn't exist, it will appear 
% in the list of dependents regardless)
%
% Write the following if you want it to appear in \listfiles 
% --- although not really necessary and latexmk doesn't use this
%
\@addtofilelist{#1}
%
% latexmk will find this message if #1 doesn't exist (yet)
\IfFileExists{#1}{}{\typeout{No file #1.}}
}\makeatother

\newcommand*{\myexternaldocument}[1]{%
\externaldocument{#1}%
\addFileDependency{#1.tex}%
\addFileDependency{#1.aux}%
}

\myexternaldocument{supplement}

\newcommand\myeq{\mathrel{\stackrel{\makebox[0pt]{\mbox{\normalfont\tiny def}}}{=}}}

% If your paper is accepted, change the options for the package
% aistats2025 as follows:
%
\usepackage[accepted]{aistats2025}
%
% This option will print headings for the title of your paper and
% headings for the authors names, plus a copyright note at the end of
% the first column of the first page.

% If you set papersize explicitly, activate the following three lines:
%\special{papersize = 8.5in, 11in}
%\setlength{\pdfpageheight}{11in}
%\setlength{\pdfpagewidth}{8.5in}

% If you use natbib package, activate the following three lines:
%\usepackage[round]{natbib}
%\renewcommand{\bibname}{References}
%\renewcommand{\bibsection}{\subsubsection*{\bibname}}

% If you use BibTeX in apalike style, activate the following line:
%\bibliographystyle{apalike}











\usepackage{tikz}
\usetikzlibrary{automata, positioning}
\usepackage{comment}
% If your paper is accepted, change the options for the package
% aistats2025 as follows:
%
%\usepackage[accepted]{aistats2025}
%
% This option will print headings for the title of your paper and
% headings for the authors names, plus a copyright note at the end of
% the first column of the first page.

% If you set papersize explicitly, activate the following three lines:
%\special{papersize = 8.5in, 11in}
%\setlength{\pdfpageheight}{11in}
%\setlength{\pdfpagewidth}{8.5in}

% If you use natbib package, activate the following three lines:
%\usepackage[round]{natbib}
%\renewcommand{\bibname}{References}
%\renewcommand{\bibsection}{\subsubsection*{\bibname}}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{url}
\usepackage{xr}
\usepackage{amsthm}
\usepackage{subfiles}
 \usepackage{natbib}
\usepackage{algorithm}
\usepackage{algorithmic}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}

\newtheorem{claim}{Claim}
\newtheorem{property}{Property}


\myexternaldocument{sample_paper}


\newtheorem*{unumberedtheorem}{Theorem}
\newtheorem*{unumberedproposition}{Proposition}
\newtheorem*{unumberedlemma}{Lemma}
\newtheorem*{unumberedclaim}{Claim}

\usepackage{xr} % Use 'xr-hyper' if you are using hyperref in the main document
\externaldocument{output.aux}




\newcommand{\one}[1]{\textcolor{orange}{#1}}

\newcommand{\two}[1]{\textcolor{red}{#1}}

\newcommand{\three}[1]{\textcolor{green}{#1}}

\newcommand{\four}[1]{\textcolor{blue}{#1}}


\begin{document}

% If your paper is accepted and the title of your paper is very long,
% the style will print as headings an error message. Use the following
% command to supply a shorter title of your paper so that it can be
% used as headings.
%
%\runningtitle{I use this title instead because the last one was very long}

% If your paper is accepted and the number of authors is large, the
% style will print as headings an error message. Use the following
% command to supply a shorter version of the authors names so that
% they can be used as headings (for example, use only the surnames)
%
%\runningauthor{Surname 1, Surname 2, Surname 3, ...., Surname n}

\twocolumn[

\aistatstitle{On the Computational Tractability of the (Many) Shapley Values}

%\aistatstitle{On the Computational Complexity of the Many Shapley Values for Model Explanation}\shahaf{What do you think of the title: ''On the Tractability of the (Many) Shapley Values''}


\aistatsauthor{Reda Marzouk*\textsuperscript{1} \And Shahaf Bassan*\textsuperscript{2} \And Guy Katz$^\dagger$\textsuperscript{2} \And Colin de la Higuera$^\dagger$\textsuperscript{1}}\begingroup
\renewcommand\thefootnote{}
%\addtocounter{footnote}{-1}% Optional: Prevents incrementing the footnote counter
\endgroup

\aistatsaddress{\textsuperscript{1} LS2N, Universit√© de Nantes, France \And \textsuperscript{2} The Hebrew University of Jerusalem, Israel} ]

\begin{abstract}
Recent studies have examined the computational complexity of computing Shapley additive explanations (also known as SHAP) across various models and distributions, revealing their tractability or intractability in different settings. However, these studies primarily focused on a specific variant called Conditional SHAP, though many other variants exist and address different limitations. In this work, we analyze the complexity of computing a much broader range of such variants, including Conditional, Interventional, and Baseline SHAP, while exploring both local and global computations. We show that both local and global Interventional and Baseline SHAP can be computed in polynomial time for various ML models under Hidden Markov Model distributions, extending popular algorithms such as TreeSHAP beyond empirical distributions. On the downside, we prove intractability results for these variants over a wide range of neural networks and tree ensembles. We believe that our results emphasize the intricate diversity of computing Shapley values, demonstrating how their complexity is substantially shaped by both the specific SHAP variant, the model type, and the distribution.
\end{abstract}

%This article investigates the computational complexity of some well-known SHAP variants proposed in the litterature, particularly Interventional SHAP (I-SHAP) and Baseline SHAP (B-SHAP). We show that both Local and Global I-SHAP and B-SHAP can be computed in polynomial time for the family of Weighted Automata and Tree-based models under Hidden Markov Model distributions, thus extending the distributional assumption of the popular TreeSHAP algorithm limited to the class of empirical distributions. On the other hand, we identify novel negative complexity results, including the NP-hardness of computing Local I-SHAP forthe class of RNN-ReLus and co-NP-hardness for Local B-SHAP in sigmoidal neural networks and Random Forests. These findings provide insights into the nuanced landscape of SHAP variant computations, informing future research in explainable AI through the lens of formal computational theory.



\input{content/introduction}
\input{content/background}
\input{content/tractableresults/entry}
\input{content/intractableresults/entry}
\input{content/conclusion}

\section*{Acknowledgments}
This work was partially funded by the European Union (ERC,
VeriDeL, 101112713). Views and opinions expressed are however
those of the author(s) only and do not necessarily reflect those of the
European Union or the European Research Council Executive Agency.
Neither the European Union nor the granting authority can be held
responsible for them.

%\subsubsection*{Acknowledgements}
%All acknowledgments go at the end of the paper, including thanks to reviewers who gave useful comments, to colleagues who contributed to the ideas, and to funding agencies and corporate sponsors that provided financial support. 
%To preserve the anonymity, please include acknowledgments \emph{only} in the camera-ready papers.


\bibliography{bibliography}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Checklist}


% %%% BEGIN INSTRUCTIONS %%%
%The checklist follows the references. For each question, choose your answer from the three possible options: Yes, No, Not Applicable.  You are encouraged to include a justification to your answer, either by referencing the appropriate section of your paper or providing a brief inline description (1-2 sentences). 
%Please do not modify the questions.  Note that the Checklist section does not count towards the page limit. Not including the checklist in the first submission won't result in desk rejection, although in such case we will ask you to upload it during the author response period and include it in camera ready (if accepted).

%\textbf{In your paper, please delete this instructions block and only keep the Checklist section heading above along with the questions/answers below.}
% %%% END INSTRUCTIONS %%%


 \begin{enumerate}


 \item For all models and algorithms presented, check if you include:
 \begin{enumerate}
   \item A clear description of the mathematical setting, assumptions, algorithm, and/or model. [Yes] Mainly in Section 1 and the appendix.
   \item An analysis of the properties and complexity (time, space, sample size) of any algorithm. [Yes] See Section 2, Section 3, Section 4, and the appendix.
   \item (Optional) Anonymized source code, with specification of all dependencies, including external libraries. [Not Applicable]
 \end{enumerate}


 \item For any theoretical claim, check if you include:
 \begin{enumerate}
   \item Statements of the full set of assumptions of all theoretical results. [Yes] Mainly in Section 1 and the appendix.
   \item Complete proofs of all theoretical results. [Yes] Because of space constraints, we provide only a brief summary of the proofs for our claims in the paper, with the complete detailed proofs available in the appendix.
   \item Clear explanations of any assumptions. [Yes]     
 \end{enumerate}


 \item For all figures and tables that present empirical results, check if you include:
 \begin{enumerate}
   \item The code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL). [Not Applicable]
   \item All the training details (e.g., data splits, hyperparameters, how they were chosen). [Not Applicable]
         \item A clear definition of the specific measure or statistics and error bars (e.g., with respect to the random seed after running experiments multiple times). [Not Applicable]
         \item A description of the computing infrastructure used. (e.g., type of GPUs, internal cluster, or cloud provider). [Not Applicable]
 \end{enumerate}

 \item If you are using existing assets (e.g., code, data, models) or curating/releasing new assets, check if you include:
 \begin{enumerate}
   \item Citations of the creator If your work uses existing assets. [Not Applicable]
   \item The license information of the assets, if applicable. [Not Applicable]
   \item New assets either in the supplemental material or as a URL, if applicable. [Not Applicable]
   \item Information about consent from data providers/curators. [Not Applicable]
   \item Discussion of sensible content if applicable, e.g., personally identifiable information or offensive content. [Not Applicable]
 \end{enumerate}

 \item If you used crowdsourcing or conducted research with human subjects, check if you include:
 \begin{enumerate}
   \item The full text of instructions given to participants and screenshots. [Not Applicable]
   \item Descriptions of potential participant risks, with links to Institutional Review Board (IRB) approvals if applicable. [Not Applicable]
   \item The estimated hourly wage paid to participants and the total amount spent on participant compensation. [Not Applicable]
 \end{enumerate}

 \end{enumerate}

\setcounter{definition}{0}
\setcounter{section}{0}
\setcounter{proposition}{0}
\setcounter{theorem}{0}
\setcounter{lemma}{0}

\onecolumn
\aistatstitle{
Appendix}

\vspace{-2.5em}

% Paragrah that describes the structure of the appendix 
The appendix provides formalizations, supplementary background, and gathers the proofs of several mathematical statements referenced either implicitly or explicitly throughout the main article. It is structured as follows:


\begin{itemize} 
\item Appendix~\ref{app:limitations} discusses the limitations of this work and outlines potential directions for future research.
\item Appendix \ref{app:sec:terminology} provides the technical background and preliminary results that will be referenced throughout the appendix.
\item Appendix \ref{app:shapwa} includes the proofs of intermediate mathematical statements that demonstrate the tractability of computing both local and global interventional SHAP and baseline SHAP variants (Theorem \ref{thm:shapwa}). 
\item %\textbf{@reda: TO FIX]}
Appendix~\ref{app:reductiontree} elaborates on the reduction strategy from WAs to decision trees, linear regression models, and tree ensembles employed for regression (Theorem~\ref{cor:reductions}). Moreover, it presents a polynomial-time reduction between the distribution families discussed throughout the main paper, along with corollaries that can be derived from these relationships.


%Appendix~\ref{app:reductiontree} details the reduction strategy from WAs to decision trees, linear regression models, and tree ensembles used for regression (Corollary \ref{cor:reductions}). Additionally, it provides polynomial time reduction between distribution families mentioned throught the main paper, and corrolaries that can be extracted from these relations.
\item Appendix~\ref{app:ISHAPRNN} provides proofs of the intermediary results that validate the reduction strategy from the closest string problem to $\texttt{LOC-I-SHAP}(\texttt{RNN-ReLu}, \texttt{IND})$ (Theorem \ref{thm:relushap}).
\item Appendix~\ref{app:BSHAPSIGMOID} is focused on providing the complete proofs of intermediate results related to the complexity of computing local Baseline SHAP for Sigmoidal Neural Networks, RNN-ReLUs, and tree ensemble classifiers (Theorem \ref{thm:intractable}).
\item Appendix~\ref{app:sec:generalized},  presents the proof for the main result discussed in the section on generalized relations of SHAP variants (Section~\ref{sec:generalized}) of the main article, specifically Proposition~\ref{prop:hardnessrelation}.
\end{itemize}

\input{content-supplement/limitations}
\input{content-supplement/extended_related_work}
\input{content-supplement/terminology_preliminaryresults}
\input{content-supplement/SHAPWA}
\input{content-supplement/SHAPTREE}
\input{content-supplement/ISHAPRNNReLu}
\input{content-supplement/BSHAPHard}
\input{content-supplement/generalized-complexity-relations}
\vfill



\end{document}
