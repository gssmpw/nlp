 \section{From Sequential Models to Non-Sequential Models: Reductions and Inter-inclusions} \label{app:reductiontree}
%\textcolor{red}{This section requires some text editing, notation harmonisation etc. 
%I believe it's well-structured and the technical stuff is fairly correct but it requires some editing refinements, clarifications etc.
%Note that this section relies heavily on background of section \ref{app:sec:terminology}(Models and Distributions).
%}


In Section~\ref{subsec:tree2WA} of the main paper, we demonstrated how the tractability result for computing various SHAP variants for WAs extends to several non-sequential models, including Ensemble Trees for Regression and Linear Regression Models (Theorem~\ref{cor:reductions}). In this section, we will present a complete and rigorous proof of this connection, along with additional theoretical insights into the relationships between these models and distributions. Specifically:



%In section \ref{subsec:tree2WA} of the main article, we showed how the tractability result of computing different SHAP variants for WAs is projected on several non-sequential models such as Ensemble Trees for Regression and Linear Regression Models (Theorem \ref{cor:reductions}). This section is dedicated to provide theoretical insights into this connection: 

\begin{enumerate}
\item 
In the first subsection, we establish the proof of Theorem~\ref{cor:reductions}. This proof is based on several reductions, including those from linear regression models and ensemble trees to weighted automata, as well as from empirical distributions to the family $\overrightarrow{\texttt{HMM}}$.


%In the first subsection, we prove the result of Theorem \ref{cor:reductions}. This result will rely on several reductions from Linear regression models and ensemble trees to Weighted Automata and from empirical distributions to the family $\overrightarrow{\texttt{\text{HMM}}}$. 
\item In the second subsection, we present additional reductions that, while not explicitly used to prove the complexity results stated in the article, showcase the expressive power of the HMM class in modeling various families of distributions relevant to SHAP computations. These include distributions represented by Naive Bayes models and Markovian distributions. Moreover, we will highlight certain complexity results, listed in Table~\ref{fig:summaryresults}, that were not directly mentioned in the main text but are illuminated by these reduction findings.

%In the second subsection, we prove other reductions which weren't used explicitly to prove complexity results stated in the article, but highlights the expressiveness power of the class of HMMs to model various family of distributions encountered in works related to SHAP computations, including distributions modeled by Naive Bayes Models and Markovian Distributions. Additionally, we will highlight some complexity results presented in Table \ref{fig:summaryresults} but weren't explicitly mentionned in the main article in light of these reduction results.  

\end{enumerate} 

\subsection{Proof of Theorem \ref{cor:reductions}}

Recall the statement of Theorem \ref{cor:reductions}:

\begin{unumberedtheorem}
        Let $\mathbb{S}:= \{ \emph{\texttt{LOC}}, \emph{\texttt{GLO}} \}$, $\mathbb{V} := \{\texttt{\emph{B}}, \texttt{\emph{I}}\}$, $\mathbb{P}:= \{ \texttt{\emph{EMP}}, \overrightarrow{\emph{\texttt{HMM}}} \} $, and $\mathbb{F}:= \{\emph{\texttt{DT}}, \emph{\texttt{ENS-DT}}_{\emph{\texttt{R}}}, \emph{\texttt{Lin}}_{\emph{\texttt{R}}}\}$. Then, for any \emph{\texttt{S}} $\in \mathbb{S}$, $\texttt{\emph{V}} \in \mathbb{V}$, $\texttt{\emph{P}} \in \mathbb{P}$, and $\emph{\texttt{F}}\in \mathbb{F}$ the problem  $\emph{\texttt{S-V-SHAP}}(\emph{\texttt{F}}, \texttt{\emph{P}})$ can be solved in polynomial time.
\end{unumberedtheorem}

The proof of this theorem will rely on four inter-model polynomial reductions:

\begin{unumberedclaim}
The following statements hold true:
\begin{enumerate}
    \item $\overrightarrow{\texttt{\emph{HMM}}} \preceq_{\emph{P}} 
\texttt{\emph{HMM}}$
   \item $\texttt{\emph{EMP}} \preceq_{\emph{P}} \overrightarrow{\texttt{\emph{HMM}}}$ 
   \item $\texttt{\emph{ENS-DT}}_{\texttt{\emph{R}}}\preceq_{\emph{P}} \texttt{\emph{WA}}$
   \item $\texttt{\emph{LIN}}_{\texttt{\emph{R}}} \preceq_{\emph{P}} \texttt{\emph{WA}}$
\end{enumerate}
\end{unumberedclaim}

Theorem \ref{cor:reductions} is a direct consequence of the following four claims. This section is structured into four parts, each dedicated to proving one of these claims. Without loss of generality and to simplify the notation, we will assume throughout that all models operate on binary inputs. However, it is important to note that all of our results can be extended to the setting where each input is defined over $k$ discrete values.

%follows directly from these four claims. The rest of this section is divided into four parts, with each part proving one of these claims. Without a loss of generality, we assume in the sequel that all models operate on binary inputs. 

\subsubsection{Claim 1: $\overrightarrow{\texttt{\text{HMM}}}$ is polynomially reducible to $\texttt{\text{HMM}}$}

Consider a model $\ar{\tt{\text{M}}} = \langle \pi, \alpha, \{T_{i}\}_{i \in [n]}, \{O_{i}\}_{i \in [n]}\rangle$, where the size of $\ar{\text{M}}$ is denoted as $m$. This model $\ar{\text{M}}$ encodes a probability distribution over the hypercube $\{0,1\}^{n}$. The goal is to prove that a model $M \in \ttt{\text{HMM}}$, which satisfies the following condition:

\begin{equation} \label{app:eq
} P_{\ar{\text{M}}}(x_{1}, \ldots, x_{n}) = P_{\text{M}}^{(n)}(x_{1}, \ldots, x_{n}), \end{equation}

can be constructed in polynomial time with respect to the size of $\ar{\text{M}}$. Intuitively, this condition asserts that the probability of generating a sequence $x = x_{1}, \ldots, x_{n}$ of length $n$ using $M$ is the same as the probability of generating $(x_{1}, \ldots, x_{n})$ using $\ar{\text{M}}$.



%Fix a model $\ar{\tt{\text{M}}} = \langle \pi, \alpha, \{T_{i}\}_{i \in [n]}, \{O_{i}\}_{i \in [n]}\rangle$ where $\texttt{size}(\ar{\text{M}}) = m$. The model $\ar{\text{M}}$ that encodes a probability distribution over the hypercube $\{0,1\}^{n}$. The objective is to prove that a model $M \in \ttt{\text{HMM}}$ satisfying the following condition:

%\begin{equation} \label{app:eq:hmm2hmm}
%P_{\ar{\text{M}}}(x_{1}, \ldots, x_{n}) = P_{\text{M}}^{(n)}(x_{1} \ldots x_{n} )
%\end{equation}

%can be constructed in polynomial-time with respect to the size of $\ar{\text{M}}$.


%Intuitively, this condition means that the probability of generating a prefix $x = x_{1} \ldots x_{n}$ of length $n$ by $M$ is equal to the probability of generating $(x_{1}, \ldots, x_{n})$ by $\ar{\text{M}}$.

%\textcolor{blue}{@Shahaf: Basically, this condition means that the probability of generating a prefix $x = x_{1} \ldots x_{n}$ of length $n$ by $M$ is equal to the probability of generating $(x_{1}, \ldots, x_{n})$ by $\ar{\text{M}}$} 

%can be constructed in polynomial-time with respect to the size of $\ar{\text{M}}$.

The construction aims to build an HMM $M$ that simulates $\ar{\text{M}}$ up to position $n$. Afterward, $M$ transitions into a dummy hidden state where it stays stuck permanently, emitting symbols uniformly at random. To simulate $\ar{\text{M}}$ within the support $\{0,1\}^{M}$, $M$ must keep track of both the state reached by $\ar{\text{M}}$ after emitting a prefix $x_{\pi{1}}, \ldots x_{\pi{j}}$ for a given $j \in [n]$, and the position reached in the sequence. Once the $n$-th symbol has been emitted, the HMM $\text{M}$ transitions into a dummy state, which emits symbols uniformly at random and remains in that state indefinitely. The detailed construction is provided as follows:


%The idea of the construction is to create an HMM $M$ that simulates $\ar{\text{M}}$ up to the position $n$. Afterwards, $M$ transitions to a dummy hidden state where it remains stuck forever, emitting symbols uniformly at random. To simulate $\ar{\text{M}}$ inside the support $\{0,1\}^{M}$, $M$ needs to keeps track of both the state reached by $\ar{\text{M}}$ after emitting a prefix $x_{\pi{1}}, \ldots x_{\pi{j}}$ for a given $j \in [n]$ as well as the reached position in the sequence. After emitting the $n$-th symbol, the HMM $\text{M}$ transitions to a dummy state that emits a symbol uniformly at random and remains stuck in this state forever. The details of the construction are given as follows:

\begin{itemize}
    \item \textbf{The state space:} $Q = [M] \times [n+1]$
    \item \textbf{State initialization:} The HMM $M$ begins generating from the state $(i,1)$ for $i \in [M]$ with a probability of $\alpha(i)$.
    
    %the HMM $M$ starts generating from the state $(i,1)$ for $i \in [M]$ with probability equal to $\alpha(i)$
    \item \textbf{The transition dynamics:} For $(i,k,j) \in [M]^{2} \times [N]$, the HMM $M$ transitions from state $(i,j)$ to state $(k,j+1)$ with a probability of $T_{j}[i,k]$. Additionally, for any $i \in [M]$, we have $T[(i,n+1), (i,n+1)] = 1$ (meaning the HMM remains in the dummy state $n+1$ indefinitely).
    %For $(i,k,j) \in [M]^{2} \times [N]$, the HMM $M$ transitions from the state $(i,j)$ to the state $(k,j+1)$ with probability equal to $T_{j}[i,k]$. On the other hand, for any $i \in [M]$, we have $T[(i,n+1), (i,n+1)] = 1$ (i.e. the HMM stays in the dummy state $n+1$ forever).
    \item \textbf{Symbol emission:} At a state $(i,j) \in [M] \times [n]$, the probability of emitting the symbol $\sigma$ is equal to $O_{\pi(i)}[i,\sigma]$. On the other, for any state $(i,n+1)$ where $i \in [M]$, the HMM $M$ generates the symbol $\sigma$ uniformly at random. 
\end{itemize}

\subsubsection{Claim 2: \texttt{EMP} is polynomially reducible to $\overrightarrow{\texttt{HMM}}$}
% Introduction of the content of this section

The purpose of this subsection is to demonstrate that the class of Empirical distributions can be polynomially reduced to the \texttt{HMM} family. This claim has been referenced multiple times throughout the main article, where it played a key role in deriving complexity results for computing SHAP variants across various ML models, including decision trees, linear regression models, and tree ensemble regression models. Specifically, it shows that computing these variants under distributions modeled by HMMs is at least as difficult as computing them under empirical distributions. %We begin by revisiting the concept of empirical distributions (in the context of models with boolean input features):

%The goal of this subsection is to show that the class of Empirical distributions is polynomially reducible to the family \texttt{HMM}. This claim has been used in several parts of the main article, where it was useful to deduce compelxity results of computing SHAP variants of different ML models such as decision trees, linear regression models, and tree ensemble regression models, under distributions modeled by HMMs are at least as hard as computing these variants under empirical distributions. We first recall what is meant by empirical distributions (in the context of models with boolean input features): 

% Introduction of the empirical distribution 

% The strategy consists of 2 steps: Shalllow description of these steps 
%The strategy of reducing empirical distributions into distributions modeled by the family $\overrightarrow{\text{HMM}}$ comprises two steps:

The strategy for reducing empirical distributions to those modeled by the $\overrightarrow{\text{HMM}}$ family involves two steps:

\begin{enumerate}
    \item The sequentialization step, which aims to transform the vectors in $\mathcal{D}$ into a dataset of binary sequences, referred to as $\texttt{SEQ}(\mathcal{D})$.
    %The sequentialization step: The objective of this step is to convert vectors in $\mathcal{D}$ into a dataset of binary sequences, denoted $\texttt{SEQ}(\mathcal{D})$.
    \item The construction of a  model in $\overrightarrow{\text{HMM}}$ that encodes the empirical distribution induced by $\texttt{SEQ}(\mathcal{D})$.  
\end{enumerate}

We will now provide a detailed explanation of each step:

% The first step (Sequentialization) is relatively easy .. 
\emph{Step 1 (The sequentialization step):} The sequentialization step is fairly straightforward and has been utilized in \cite{marzouk24a} to transform decision trees into equivalent WAs. The sequentialization operation, denoted as $\texttt{SEQ}(.,.)$, is defined as follows:
%The sequentialization step is relatively easy and has been used in the work of \cite{marzouk24a} to reduce decision trees into equivalent WAs. Define the sequentialization operation $\texttt{SEQ}(.,.)$ as: 
$$\texttt{SEQ}(\overrightarrow{x}, \pi) = x_{\pi(1)} \ldots x_{\pi(N)}$$

where $\pi$ is a permutation. Essentially, the $\texttt{SEQ}(.,.)$ operation transforms a given vector in $\{0,1\}^{N}$ into a binary string, with the order of the feature variables determined by the permutation $\pi$. From here on, without loss of generality, we assume $\pi$ to be the identity permutation.

%where $\pi$ is a permutation. Basically, the $\texttt{SEQ}(.,.)$ operation converts a given vector in $\{0,1\}^{N}$ into a binary string where the ordering of feature variables is dictated by the permutation $\pi$. In the sequel, without a loss of generality, we assume $\pi$ to be the identity permutation. 

% The second step : Description of PTAs, Conversion of EMP to PTAs  
\emph{Step 2 (The construction of the HMM):} 
Applying the sequentialization step to the dataset $\mathcal{D}$ results in a \emph{sequentialized} dataset consisting of $M$ binary strings, denoted as $\texttt{SEQ}(\mathcal{D})$. The goal of the second step in the reduction is to construct a model in $\overrightarrow{HMM}$ that models the empirical distribution induced by $\texttt{SEQ}(\mathcal{D})$. While this construction is well-known in the literature of Grammatical Inference (refer to
La Higuera book on Grammatical Inference), we will provide the details of this construction below. 

%Applying the sequentialization step on the dataset $\mathcal{D}$ yields a \emph{sequentialized} dataset comprised of $M$ binary strings denoted $\texttt{SEQ}(\mathcal{D})$. The objective of the second step of the reduction consists at constructing a PPTA that models the empirical distribution induced by $\texttt{SEQ}(\mathcal{D})$. Although this construction is folklore in the litterature of Grammatical Inference [ref:de La Higuera book Grammatical Inference], we provide next the details of this construction.

For a given sequence $w$, we denote the number of occurences of $w$ as a prefix in the dataset $P(\mathcal{D})$ as:
$$N(w) \myeq \#|\{x \in \texttt{SEQ}(\mathcal{D}): ~ \exists (s) \in \{0,1\}^{*}: x = ws\}|$$

We also define the set of prefixes appearing in $\texttt{SEQ}(\mathcal{D})$ as the set of all sequences $w \in \{0,1\}^{*}$ such that $N(w) \neq 0$. This set shall be denoted as $\text{P}(\mathcal{D})$. Note that the set $\text{P}$ is prefix-closed \footnote{A set of sequences $S$ is prefix-closed if the set of prefixes of any sequence in $S$ is also in $S$}.

A (stationary) model in $\overrightarrow{\text{HMM}}$ that encodes the empirical distribution induced by the dataset $\texttt{SEQ}(\mathcal{D})$ is a probabilistic finite state automaton parametrized as follows: 
\begin{enumerate}
    \item \textbf{The permutation:} The identity permutation.
    \item \textbf{The state space:} $Q = \text{P}(\mathcal{D})$.
    \item \textbf{The initial state vector:} Given $\sigma \in \{0,1\}$, the probability of starting from the state $\sigma \in \text{P}(\mathcal{D})$ is equal to $\frac{N(\sigma)}{|\mathcal{D}|}$.
    \item \textbf{The transition dynamics:} For a state $w \sigma \in Q$, where $w \in Q$ \footnote{Thanks to the prefix-closedness property of the set $\text{P}(\mathcal{D})$, if $w\sigma$ is in $Q$, then $w$ is also in $Q$.} and $\sigma \in \Sigma$,  the probability of transitioning from the state $w$ to the state $w\sigma$ is equal to $\frac{N(w \sigma)}{N(w)}$. 
    \item \textbf{The symbol emission:} For any state $w \sigma \in Q$, where $w \in Q$ and $\sigma \in \{0,1\}$, the probability of generating the symbol $\sigma$ from the state $w\sigma$ is equal to $1$.
 \end{enumerate}

One can readily prove that the resulting model computes the empirical distribution induced by $\text{P}(\mathcal{D})$. Indeed, by construction, the probability of generating a binary sequence $w$ by the model is equal to the probability of generating the sequence of states $w_{1}, w_{1:2}, \ldots, w_{1:n-1} w$. The probability of generating this state sequence is equal to:
$$\frac{N(w_{1})}{|\mathcal{D}|} \prod\limits_{i=1}^{|w|} \frac{N(w_{1:i+1})}{N(w_{1:i})} = \frac{N(w)}{|\mathcal{D}|}$$

\subsubsection{Claim 3: \texttt{DT} and $\texttt{\text{ENS-DT}}_{\texttt{\text{R}}}$ are polynomially reducible to \texttt{WA}}
%\textcolor{blue}{@Shahaf: In my old writing of this section, Tree-based models designate both decision trees and ensemble trees for regression in your new formulation.}

The construction of an equivalent WA for either a decision tree or a tree ensemble used in regression tasks (i.e., a model in \texttt{DT} or a model in $\texttt{\text{ENS-DT}}_{\text{R}}$) is built upon the following result provided in \cite{marzouk24a} for the \texttt{DT} family:

%The construction of an equivalent WA to either a decision tree or a tree ensemble used for regression tasks (i.e. a model in \texttt{DT} or a model in $\texttt{\text{ENS-DT}}_{\text{R}}$) is based on top of the following result provided in \cite{marzouk24a} for the family of \texttt{DT}:

\begin{proposition} \label{app:prop:DT2WA}
    There exists a polynomial-time algorithm that takes as input a decision Tree $T \in \texttt{\emph{DT}}$ over the binary feature set $X = \{X_{1}, \ldots, X_{n} \}$ and outputs a WA $A$ such that:
     $$f_{A}(x_{1}\ldots x_{n}) = f_{T}(x_{1}, \ldots, x_{n} )$$
\end{proposition}


Since an ensemble of decision trees used for regression tasks is a linear combination of decision trees, and the family of WAs is closed under linear combination operations, with these operations being implementable in polynomial time as demonstrated in section~\ref{app:sec:ter}, Proposition~\ref{app:prop:DT2WA} consequently implies the existence of a polynomial-time algorithm that produces a WA equivalent to a given tree ensemble model used for regression.

%Given that an ensemble of deicision trees used for regression taks is a linear combination of a collection of decision trees, and that the family of WAs is closed under linear combination operations and implementing these linear operations can be performed in polynomial time as shown in section \ref{app:sec:ter}, Proposition \ref{app:prop:DT2WA} implies as a corollary the existence of a polynomial-time algorithm that returns a WA equivalent to a given tree ensemble model used for regression.

\paragraph{Complexity.} The algorithm for constructing a WA equivalent to a given DT $T$ runs in $O(|T|)$, where $|T|$ denotes the number of edges in the DT $T$. The size of the resulting WA is $O(|T|)$ (See \cite{marzouk24a} for more details). Consequently, the overall algorithm for constructing an equivalent regression tree ensemble consisting of $m$ DTs $\{T_{i}\}_{i \in [m]}$ operates in $O(m \cdot \max\limits_{i \in [m]} |T_{i}|)$. The size of the ensemble is also $O(m \cdot \max\limits_{i \in [m]} |T_{i}|)$.

%The algorithm for constructing an equivalent WA to a given DT $T$ runs in $O(|T|)$ where $|T|$ refers to the number of edges of the DT $T$. The size of the resulting WA is $O(|T|)$ (See \cite{marzouk24a} for full details). Consequently, the overall algorithm of constructing an equivalent regression tree ensemble comprised of an ensemble of $m$ DTs $\{T_{i}\}_{i \in [m]}$ is given as $O(m \cdot \max\limits_{i \in [m]} |T_{i}|)$. Its size is likewise equal to $O(m \cdot \max\limits_{i \in [m]} |T_{i}|)$. 



\subsubsection{Claim 4: $\texttt{\text{Lin}}_{\texttt{\text{R}}}$ is polynomially reducible to \texttt{WA}.}

Let $M := \langle \{w_{i,d}\}_{i \in [n], d \in \mathbb{D}_{i}}, b\rangle$ be a linear regression model over the finite set $\mathbb{D} = [m_{1}] \times \ldots [m_{n}]$ where $\{m_{i}\}_{i \in [n]}$, computing the function (as defined earlier):

%Let $M := \langle \{w_{i,d}\}_{i \in [n], d \in \mathbb{D}_{i}}, b\rangle$ be linear regression model over the finite set $\mathbb{D} = [m_{1}] \times \ldots [m_{n}]$ where $\{m_{i}\}_{i \in [n]}$ computing the function (as was defined earlier):

$$f(x_{1}, \ldots , x_{n}) = \sum\limits_{i=1}^{n} \sum\limits_{d \in \mathbb{D}_{i}} w_{i,d} \cdot I(x_{i}=d) + b$$


For the purposes of our reduction to WAs, we will assume that: $m_{1} = \ldots = m_{n} = m$.
The conversion of a linear regression model $M$ over the input space $\mathbb{D} = [m_{1}] \times \ldots [m_{n}]$ to an equivalent one over $[m]^{n}$ can be done as follows: We set $m$ to be $\max\limits_{i\in [n]} m_{i}$, and the parameters $\{\tilde{w}_{i,d}\}_{i \in [n], d \in [m]}$ for the new model are set as:

%For the sake of our reduction to WAs, we shall assume that: $m_{1} = \ldots = m_{n} = m$.
%The conversion of a linear regression model $M$ over the input space $\mathbb{D} = [m_{1}] \times \ldots [m_{n}]$ to an equivalent one over $[m]^{n}$ can be performed as follows: We set $m$ as $\max\limits_{i=1}^{n} m_{i}$ and we set the parameters $\{\tilde{w}_{i,d}\}_{i \in [n], d \in [m]}$ for the new model as:   

$$\tilde{w}_{i,d} = \begin{cases}
    w_{i,d} & \text{if} ~~ d \leq m_{i} \\
    0 & \text{otherwise}
\end{cases}$$


Observe that this procedure operates in $O(\max\limits_{i \in [n]} m_{i})$ time.

\paragraph{Reduction strategy.} For a given linear regression model $M := \langle\{w_{i,d}\}_{i \in [n], d \in [m]},b\rangle$ over $[m]^{n}$, we construct a WA over the alphabet $\Sigma = [m]$ with $n+1$ states. The initial state vector has value $1$ for state $1$, and $0$ for all other states. For $i \in [n]$, the transition from state $i$ to state $i+1$ is assigned a weight of $w_{i,\sigma}$ when processing the symbol $\sigma$ from this state. Figure~\ref{app:fig:lin2wa} illustrates an example of the graphical representation of the resulting WA from a given linear regression tree.

%For a given linear regression model $M = \langle\{w_{i,d}\}_{i \in [n], d \in [m]},b\rangle$ over $[m]^{n}$, we construct a WA over the alphabet $\Sigma = [m]$ of $n+1$ states. The initial state vector is equal to $1$ for the state $1$, and $0$ for all the others. For $i \in [n]$, a transitions from the state $i$ to the state $i+1$ is assigned a weight equal to $w_{i,\sigma}$ when processing the symbol $\sigma$ from this state. Figure \ref{app:fig:lin2wa} illustrates an example of the graphical representation of the resulting WA from a given linear regression tree.



The additive intercept parameter $b$ can be incorporated into this constructed model by simply adding the resulting WA from the procedure described above to a trivial single-state WA that outputs the value $b$ for all binary strings (see Section~\ref{app:sec:ter} for more details on the addition operation between two WAs).

%The incorporation of the additive intercept parameter $b$ in this constructed model can be straightforwardly obtained by performing an addition operation over this resulting WA of the procedure outlined above and the trivial single state WA that output the value $b$ to all binary strings (see Section \ref{app:sec:ter} for further details of the addition operation over 2 WAs). 
\begin{figure}[h]
    \centering
    \begin{tikzpicture}[shorten >=1pt, node distance=3cm, on grid, auto]

      % States
      \node[state, initial] (q_1) {$1/0$};
      \node[state] (q_2) [right=of q_1] {$0/0$};
      \node[state] (q_3) [right=of q_2] {$0/1$};

      % Transitions
      \path[->] (q_1) edge node[above] {0 / $1$} 
                         node[below] {1 / $0.5$} (q_2);
      \path[->] (q_2) edge node[above] {0 / $-1$} 
                         node[below] {1 / $0$} (q_3);

    \end{tikzpicture}
    \caption{A construction of a WA equivalent to a linear regression over $2$ binary features with the following weights: $w_{1,0} = 1,~w_{1,1}=0.5,~w_{2,0}=-1,~w_{2,1}=0$. The notation $x/y$ within the state nodes represents the initial weight of the state ($x$) and the final weight of the state ($y$).
    %A construction of a WA equivalent to a linear regression over $2$ binary features with the weights: $w_{1,0} = 1,~w_{1,1}=0.5,~w_{2,0}=-1,~w_{2,1}=0$. The notation $x/y$ inside the state nodes corresponds to the initial weight of the state ($x$) and the final weight of the state ($y$).
    }
    \label{app:fig:lin2wa}
\end{figure}

\subsection{Additional reduction results and implications on SHAP computation}
Some of the complexity results presented in Table~\ref{fig:summaryresults} were not explicitly discussed in the main article. Instead, additional polynomial-time reductions between Hidden Markov Models and other families of distributions are needed to derive them. For completeness, we include in the first part of this subsection proofs of these reduction relationships. Following that, we outline the implications of these relationships on the complexity of certain SHAP configurations shown in Table~\ref{fig:summaryresults}.


%Some complexity results highlighted in Table \ref{fig:summaryresults} weren't explicitly stated in the main article. Instead, additional polynomial-time reductions  between Hidden Markov Models and other families of distributions are required to derive them.  For the sake of completeness, we provide in the first segment of this section proofs of these reduction relationships. Afterwards, we highlight the implications of these relations on the complexity of some SHAP configurations present in Table \ref{fig:summaryresults}. 

\subsubsection{Additional reduction results}

An interesting aspect of Hidden Markov Models is that they provide a unifying probabilistic modeling framework encompassing several families of distributions discussed in the literature on SHAP computation. Specifically, the class \texttt{MARKOV} (and, \textit{de facto}, the family \texttt{IND}, which is trivially a subclass of \texttt{MARKOV}) as well as \texttt{NB} (more formally, $\texttt{NB} \preceq_{P} \ar{\texttt{\text{HMM}}}$) can be polynomially reduced to the class of Hidden Markov Models.

%An interesting feature of Hidden Markov Models  offer a unifying probabilistic modeling framework that covers several families of distributions covered in the literature regarding SHAP computation. In particular, the class \texttt{MARKOV}, (and, \textit{de facto} the family \texttt{IND}, this latter being trivially a sub-class of \texttt{MARKOV}) and \texttt{NB} (more rigorously, $\texttt{NB} \preceq_{P} \ar{\texttt{\text{HMM}}}$) can be polynomially reduced to the class of Hidden Markov Models. 

\paragraph{Claim 1: \texttt{NB} is  polynomially reducible to $\overrightarrow{\texttt{HMM}}$.}

Let $M \in \texttt{NB}$ be a model over $n$ binary features with a hidden variable $Y$ that takes values from the discrete set $[m]$. The goal is to construct, in polynomial time, a model $M' \in \ar{\texttt{\text{HMM}}}$ that is equivalent to $M$, i.e.:

%Let there be some $M \in \texttt{NB}$ over $n$ binary features with a hidden variable $Y$ taking values from the discrete set $[m]$. The objective is to construct in polynomial time a  model $M' \in \ar{\texttt{\text{HMM}}}$ equivalent to $M'$, i.e.:

$$\forall (x_{1}, \ldots, x_{n}) \in \{0,1\}^{n}:~~P_{M}(x_{1}, \ldots, x_{n}) = P_{M'}(x_{1}, \ldots , x_{n})$$

A key observation underlying the reduction is that a naive Bayes Model can be viewed as a specific instance of a model in the family $\ar{\texttt{\text{HMM}}}$, where the state space coincides with the domain of the hidden variable in the naive Bayes model. The distinct feature of this model is that it remains in the same state throughout the entire generation process, with this state being determined at the initialization phase according to the probability distribution.

%A simple observation that lies at the heart of the reduction is that a naive Bayes Model can be seen a particular case of a model in the family $\ar{\texttt{\text{HMM}}}$ whose state space coincide with the domain of the hidden variable of the naive bayes model. The peculiarity of this model is that the model keeps transitioning to the same state during  the whole generation procedure, where this state is the one generated at the intialization phase according to the probability dist 

Formally, let $M = \langle\pi, \{P_{i}\}_{i \in [n]}\rangle$ be a naive bayes model over $n$ observed RVs such that the domain value of its hidden state variable $Y$ is equal to $[m]$.  The construction of a model $M' \in \ar{\texttt{\text{HMM}}} $ is given as follows:
\begin{itemize}
    \item \textbf{The permutation:} The identity permutation,
    \item \textbf{The state space:} $[m]$.
    \item \textbf{The state initialization:} $M'$ starts generating from the state $i \in [m]$ with probability equal to $\pi[m]$.
    \item \textbf{The transition dynamics:} $M'$ transitions from a given state $j$ to the same state $j$ with probability equal to $1$.
    \item \textbf{The symbol emission:} At position $i$, the model $M'$ emits the symbol $\sigma \in \{0,1\}$ from the state $j$ with probability equal to $P_{i}$. 
\end{itemize}

\begin{comment}
    Corollary:
      *-*-SHAP(M,NB) < *-*SHAP(M,NB)
     From Van den Broeck: 
      LOC-C-SHAP(SIGMOID,HMM) Hard
\end{comment}

\paragraph{Claim 2: \texttt{MARKOV} (resp. $\ar{\texttt{\text{MARKOV}}}$) is polynomially reducible to \texttt{HMM} (resp. $\ar{\texttt{\text{HMM}}}$).} The class \texttt{HMM} (resp. $\ar{\texttt{\text{HMM}}}$) can be viewed as a generalization of the class \texttt{MARKOV} (resp. $\ar{\texttt{\text{MARKOV}}}$) for handling partially observable stochastic processes: Markovian distributions represent fully observable processes, whereas HMMs represent partially observable ones. Converting a Markovian distribution into a distribution modeled by an HMM involves encoding the Markovian dynamics into the hidden state dynamics of the HMM. The HMM then trivially emits the corresponding symbol of the reached hidden state at each position.

%The class \texttt{HMM} (resp. $\ar{\texttt{\text{HMM}}}$) can be seen as a generalization of the class \texttt{MARKOV} (resp. $\ar{\texttt{\text{MARKOV}}}$) to handle partially observable stochastic processes: Markovian Distributions represent Fully-observable processes, while HMMs represent Partially-observable ones. The conversion of a Markovian distribution to a distribution modeled by a HMM consists at encoding the markovian dynamics into the hidden state dynamics of the  HMM. The HMM trivially emits the corresponding symbol of the reached hidden state at each position.

For completeness, the following provides a formal description of how \texttt{MARKOV} is polynomially reducible to \texttt{HMM}. The derivation of a polynomial-time reduction from $\ar{\texttt{\text{MARKOV}}}$ to $\ar{\texttt{\text{HMM}}}$ can be achieved using a similar construction. Let $M := \langle \pi, T\rangle $ be a model in \texttt{MARKOV} over the alphabet $\Sigma$. The description of a model $M' \in \texttt{\text{HMM}}$, equivalent to $M$, is obtained as follows:

%For the sake of completeness, the following provides a formal description of how \texttt{MARKOV} are polynomially reducible to \texttt{HMM}. The derivation of a polynomial-time reduction  of $\ar{\texttt{\text{MARKOV}}}$ to $\ar{\texttt{\text{HMM}}}$ can be derived using a similar construction. Let $M := \langle \pi, T\rangle $ a model in \texttt{MARKOV} over the alphabet $\Sigma$. The description of a model $M' \in \texttt{\text{HMM}}$ equivalent to $M$ is obtained as follows:

\begin{itemize}
    \item \textbf{The state space:} The alphabet $\Sigma$
    \item \textbf{The state initialization:} $M'$ starts by generating a state $\sigma \in \Sigma$ with probability equal to $\pi(\sigma)$.
    \item \textbf{The transition dynamics:} From a hidden state $\sigma \in \Sigma$, the probability of transitioning to the state $\sigma' \in \Sigma$ is equal to $T[\sigma, \sigma']$. 
    \item \textbf{The symbol emission:} For any symbol $\sigma \in \Sigma$, $M'$ emits a symbol $\sigma \in \Sigma$ from the hidden state $\sigma$ with probability equal to $1$.
\end{itemize}

\subsubsection{Corollaries on SHAP computational problems} 
%\textcolor{red}{@Shahaf: This short segment is only partially written. In particular the second corollary requires clarifications to the reader. You can rely on notes highlighted in red for each of the points of this corollary.}

The reduction relationships among independent distributions, Markovian distributions, and those modeled by HMMs (i.e., \texttt{IND} $\preceq_{P}$ \texttt{MARKOV} $\preceq_{P}$ \texttt{HMM} and $\ar{\texttt{IND}}$ $\preceq_{P}$ $\ar{\texttt{MARKOV}}$ $\preceq_{P}$ $\ar{\texttt{HMM}}$), as well as the relationship between empirical and HMM-modeled distributions (i.e., \texttt{EMP} $\preceq_{P}$ $\ar{\texttt{HMM}}$ $\preceq_{P}$ \texttt{HMM}), naturally leads to a corollary regarding the relationships between different SHAP variants:

%The reduction relationships between Indedepent distributions, Markovian distributions and those modeled by HMMs (i.e., \texttt{IND} $\preceq_{P}$ \texttt{MARKOV} $\preceq_{P}$ \texttt{HMM} and $\ar{\texttt{IND}}$ $\preceq_{P}$ $\ar{\texttt{MARKOV}}$ $\preceq_{P}$ $\ar{\texttt{HMM}}$) as well as the relationship between empirical and HMM-modeled distributions (i.e., \texttt{EMP} $\preceq_{P}$ $\ar{\texttt{HMM}}$ $\preceq_{P}$ $\texttt{HMM}$ )implies straightforwadly a corollary regarding the following relation between different SHAP variants:
\begin{corollary}
    For any family of sequential (resp. non-sequential) models $\mathbb{M}$ (resp. $\ar{\mathbb{M}}$), $\mathbb{S} = \{ \texttt{\emph{LOC}}, \texttt{\emph{GLO}} \}$, we have:
    \begin{equation}\label{app:eq:markov2hmm1}
        \mathbb{S}-\mathbb{V}-\texttt{\emph{SHAP}}(\mathbb{M}, \texttt{\emph{IND}}) \preceq_{P} \mathbb{S}-\mathbb{V}-\texttt{\emph{SHAP}}(\mathbb{M}, \texttt{\emph{MARKOV}}) \preceq_{P} \mathbb{S}-\mathbb{V}-\texttt{\emph{SHAP}}(\mathbb{M}, \texttt{\emph{HMM}})
    \end{equation}
    \begin{equation}\label{app:eq:markov2hmm2}
        \mathbb{S}-\mathbb{V}-\texttt{\emph{SHAP}}(\ar{\mathbb{M}},
    \ar{\texttt{\emph{IND}}}) \preceq_{P} \mathbb{S}-\mathbb{V}-\texttt{\emph{SHAP}}(\ar{\mathbb{M}}, \ar{\texttt{\emph{MARKOV}}}) \preceq_{P} \mathbb{S}-\mathbb{V}-\texttt{\emph{SHAP}}(\ar{\mathbb{M}}, \ar{\texttt{\emph{HMM}}})
    \end{equation}
    \begin{equation}\label{app:eq:markov2hmm1}
        \mathbb{S}-\mathbb{V}-\texttt{\emph{SHAP}}(\mathbb{M}, \texttt{\emph{EMP}}) \preceq_{P} \mathbb{S}-\mathbb{V}-\texttt{\emph{SHAP}}(\mathbb{M}, \texttt{\emph{HMM}}) \ \ ; \      \    \mathbb{S}-\mathbb{V}-\texttt{\emph{SHAP}}(\ar{\mathbb{M}}, \texttt{\emph{EMP}}) \preceq_{P} \mathbb{S}-\mathbb{V}-\texttt{\emph{SHAP}}(\ar{\mathbb{M}}, \ar{\texttt{\emph{HMM}}})
    \end{equation}
\end{corollary}

This corollary, along with previous complexity results for computing conditional SHAP values~\citep{vander21, huangupdates, arenas23}, and the fact that conditional and interventional SHAP variants coincide under independent distributions~\citep{sundararajan20b}, presents a wide range of complexity results on SHAP computational problems that can be derived as corollaries:

\begin{corollary} All of the following complexity results hold:
\begin{enumerate} 
    \item For the class of models $\{\texttt{\emph{DT}}, \texttt{\emph{ENS-DT}}_{\texttt{\emph{R}}}, \texttt{\emph{LIN}}_{\texttt{\emph{R}}}\}$: The computational problems $\mathbb{S}-\mathbb{V}-\texttt{\emph{SHAP}}(\ar{\mathbb{M}},
    \ar{\texttt{\emph{IND}}})$ for  $\mathbb{V}\in\texttt{\emph{\{B,V\}}}$ can be solved in polynomial time. The computational problem $\texttt{\emph{GLO}}-\texttt{\emph{C}}-\texttt{\emph{SHAP}}(\ar{\mathbb{M}}, 
    \ar{\texttt{\emph{IND}}})$ can be solved in polynomial time.
    The computational problem $\texttt{\emph{GLO}}-\texttt{\emph{C}}-\texttt{\emph{SHAP}}(\ar{\mathbb{M}}, 
    \ar{\texttt{\emph{HMM}}})$ is $\#$P-Hard.
    
    \item For \texttt{\emph{WA}}:  The computational problems         $\mathbb{S}-\mathbb{V}-$\texttt{\emph{SHAP(WA,}}$\mathbb{P}$\texttt{\emph{)}} for $\mathbb{S}\in\texttt{\emph{\{LOC,GLO\}}}$, $\mathbb{P}\in\texttt{\emph{\{IND,EMP\}}}$, and $\mathbb{V}\in\texttt{\emph{\{B,V\}}}$ can be solved in polynomial time. The computational problem $\texttt{\emph{GLO}}-\texttt{\emph{C}}-\texttt{\emph{SHAP}}(\texttt{\emph{WA}}, 
    \texttt{\emph{IND}})$ can be solved in polynomial time. The computational problem $\texttt{\emph{LOC}}-\texttt{\emph{C}}-\texttt{\emph{SHAP}}(\texttt{\emph{WA}}, 
    \texttt{\emph{EMP}})$ is NP-Hard. The computational problem $\texttt{\emph{LOC}}-\texttt{\emph{C}}-\texttt{\emph{SHAP}}(\texttt{\emph{WA}}, 
    \texttt{\emph{HMM}})$ is $\#$P-Hard.
    \item For $\texttt{\emph{ENS-DT}}_{\texttt{\emph{C}}}$: The computational problem $\texttt{\emph{LOC}}-\texttt{\emph{C}}-\texttt{\emph{SHAP}}\texttt{\emph{(ENS-DT}}_{\texttt{\emph{C}}}, \texttt{\emph{EMP}}$\texttt{\emph{)}} is NP-Hard. The computational problem $\texttt{\emph{LOC}}-\texttt{\emph{C}}-\texttt{\emph{SHAP}}\texttt{\emph{(ENS-DT}}_{\texttt{\emph{C}}},\ar{\texttt{\emph{HMM}}}$\texttt{\emph{)}} is $\#$P-Hard.

    \item For \texttt{\emph{NN-SIGMOID}}: The computational problem $\texttt{\emph{LOC}}-\texttt{\emph{C}}-\texttt{\emph{SHAP}}\texttt{\emph{(NN-SIGMOID,}}$$\ar{\texttt{\emph{HMM}}}$\texttt{\emph{)}} is NP-Hard.

    \item For \texttt{\emph{RNN-ReLU}}: The computational problem $\texttt{\emph{LOC}}-\texttt{\emph{C}}-\texttt{\emph{SHAP}}\texttt{\emph{(RNN-ReLU,}}$$\ar{\texttt{\emph{IND}}}$\texttt{\emph{)}} is NP-Hard.

    %\item The computational problem $\texttt{\emph{LOC}}-\texttt{\emph{C}}-\texttt{\emph{SHAP}}\texttt{\emph{(NN-SIGMOID)}}$ is NP-Hard.

    %\item The computational problem $\texttt{\emph{LOC}}-\texttt{\emph{C}}-\texttt{\emph{SHAP}}\texttt{\emph{(RNN-ReLU)}}$ is NP-Hard.
    
    %\item The computational problem \texttt{\emph{GLO-C-SHAP}}\texttt{\emph{(WA}}, \texttt{\emph{IND)}} can be solved in polynomial time.
    %\item The computational problem \texttt{\emph{LOC-C-SHAP}}(\texttt{\emph{WA}}, \texttt{\emph{EMP}}) is NP-Hard.
    %\item The computational problem \texttt{\emph{LOC-C-SHAP}}(\texttt{\emph{WA}}, \texttt{\emph{HMM}}) is $\#$P-Hard.
    %\item The computational problem \texttt{\emph{LOC-I-SHAP}}(\texttt{\emph{WA}}, \texttt{\emph{HMM}}) 
    %\item The problem \texttt{\emph{LOC-I-SHAP}}($\texttt{\emph{ENS-DT}}_{c}$, $\ar{\texttt{\emph{HMM}}}$), \texttt{\emph{LOC-I-SHAP}}($\texttt{\emph{ENS-DT}}_{c}$, $\ar{\texttt{\emph{HMM}}}$) are \#P-Hard \textcolor{red}{@Shahaf: (Corollary of the result LOC-C-SHAP($ENS-DT_{c}$ , IND) is \#P-Hard in the article "Update on the Complexity of SHAP scores" and $IND$ is in $\ar{HMM}$)} 
    %\item The family \texttt{\emph{RNN-ReLu}}: The problems \texttt{\emph{LOC-C-SHAP}}(\texttt{\emph{RNN-ReLu}}, \texttt{\emph{HMM}})are \texttt{\emph{LOC-I-SHAP}}(\texttt{\emph{RNN-ReLu}}, \texttt{\emph{HMM}}) are NP-Hard. \textcolor{red}{@Shahaf: Corollary of our result LOC-I-SHAP(RNN,IND) is NP-Hard and IND is in HMM}
    %\item The family \texttt{\emph{NN-SIGMOID}}: The family $\texttt{\emph{ENS-DT}}_{c}$: The problems \texttt{\emph{LOC-I-SHAP}}(\texttt{\emph{NN-SIGMOID}}, $\ar{\texttt{\emph{HMM}}}$), \texttt{\emph{LOC-I-SHAP}}(\texttt{\emph{NN-SIGMOID}}, $\ar{\texttt{\emph{HMM}}}$) are NP-Hard \textcolor{red}{@Shahaf: Corollary of Van den Broeck's result LOC-C-SHAP(SIGMOID,IND) is NP-Hard and IND is in HMM}
\end{enumerate}
\end{corollary}

\begin{proof}

We will explain the results of each part of the corollaries separately: \begin{enumerate}
 \item For the class of models $\{\texttt{DT}, \texttt{ENS-DT}_{\texttt{R}}, \texttt{LIN}_{\texttt{R}}\}$: The complexity results for these model families under independent distributions, for both baseline and interventional SHAP, are derived from our findings on the tractability of these models under HMM-modeled distributions and our proof that \texttt{EMP} $\preceq_{P}$ $\ar{\texttt{HMM}}$. The complexity results for the global and conditional forms under independent distributions stem from our tractability results for interventional SHAP in this setup, as well as the fact that interventional and conditional SHAP coincide under independent distributions~\citep{sundararajan20b}. Lastly, the $\#$P-Hardness of the conditional variant under HMM-modeled distributions follows from the hardness of generating this form of explanation under Naive Bayes modeled distributions~\citep{vander21}, and our proof that $\ar{\texttt{NB}}$ $\preceq_{P}$ $\ar{\texttt{HMM}}$.
\item For \texttt{WA}, the tractability results for local and global baseline, as well as interventional SHAP under independent and empirical distributions, follow from our primary complexity findings for this family of models, which demonstrate tractability over HMM-modeled distributions, and our proof that both \texttt{EMP} $\preceq_{P}$ \texttt{HMM} and \texttt{IND} $\preceq_{P}$ \texttt{HMM}. The tractability of global interventional SHAP under independent distributions also follows from these results, along with the fact that interventional and conditional SHAP coincide under independent distributions~\citep{sundararajan20b}. The NP-hardness of conditional SHAP under empirical distributions is derived from the complexity of this setting for decision tree classifiers~\citep{vander21} and our proof that \texttt{DT} $\preceq_{P}$ \texttt{WA}. Finally, the \#P-hardness of conditional SHAP under HMM-modeled distributions results from the hardness of this setting for decision trees, as discussed earlier, and our proof that \texttt{DT} $\preceq_{P}$ \texttt{WA}.

\item For $\texttt{ENS-DT}_{\texttt{C}}$: The complexity results for local conditional SHAP under empirical distributions are derived from the hardness results provided for decision trees under Naive Bayes-modeled distributions~\citep{vander21}, along with the facts that \texttt{DT} $\preceq_{P}$ $\texttt{ENS-DT}_{\texttt{C}}$ and $\ar{\texttt{NB}}$ $\preceq_{P}$ $\texttt{EMP}$. The result on the complexity of conditional SHAP under hidden Markov distributions follows as a corollary of the results in~\cite{huangupdates} regarding the hardness of computing conditional SHAP under independent distributions, combined with our result showing that $\texttt{IND}$ $\preceq_{P}$ $\texttt{EMP}$.

\item For \texttt{NN-SIGMOID}: The complexity results for conditional SHAP under HMM distributions follow as a corollary of the hardness results presented in~\citep{vander21} for computing conditional SHAP values for sigmoidal neural networks under independent distributions, along with our proof that $\texttt{IND}$ $\preceq_{P}$ $\ar{\texttt{HMM}}$.
\item For \texttt{RNN-ReLU}: The complexity results for conditional SHAP under independent distributions stem from our main complexity result for this family of models, which established the hardness for interventional SHAP. Additionally, interventional and conditional SHAP coincide under independent distributions~\citep{sundararajan20b}.





%For \texttt{NN-SIGMOID}: The complexity results for conditional SHAP under HMM distributions is a corrollary of the hardness results provided in~\citep{vander21} for computing conditional SHAP values for sigmoidal neural networks under independent distributions, and our proof that $\texttt{IND}$ $\preceq_{P}$ $\ar{\texttt{HMM}}$.

%For $\texttt{ENS-DT}_{\texttt{C}}$: The complexity results for local conditional SHAP under empirical distributions is derived from hardness for decision trees under naive bayes modeled distributions~\citep{vander21}, as well as the fact that \texttt{DT} $\preceq_{P}$ $\texttt{ENS-DT}_{\texttt{C}}$ and that $\ar{\texttt{NB}}$ $\preceq_{P}$ $\texttt{EMP}$. The result on the complexity of conditional SHAP under hidden markov distributions is a corrolarry of the results provided by~\cite{huangupdates} regarding hardness of computing conditional SHAP under independent distributions and our result which shows that $\texttt{IND}$ $\preceq_{P}$ $\texttt{EMP}$.

%For \texttt{WA}: The tractability results for local and global baseline and interventional SHAP under independent and empirical distributions hold from our main complexity results for this family of models which proved tractability over HMM modeled distributions and our proofs that both \texttt{EMP} $\preceq_{P}$ \texttt{HMM} and \texttt{IND} $\preceq_{P}$ \texttt{HMM}. The tractability results for global interventional SHAP under independent distributions holds again from our tractability results for global interventional SHAP, and the fact that interventional and conditional SHAP concide under independent distributions~\citep{sundararajan20b}. The NP-Hardness resuts for conditional SHAP under empirical distributions holds from the hardness of this setting for decision tree classifiers~\citep{vander21} and our proof that \texttt{DT} $\preceq_{P}$ \texttt{WA}. Lastly, the $\#$P-Hardness of conditional SHAP under HMM modeled distributions is a result of the hardness of this setting for decision trees (which we explained in the previous section, and our proof that \texttt{DT} $\preceq_{P}$ \texttt{WA}.
\end{enumerate}
%The polynomial-time complexity result for the \texttt{LOC-C-SHAP}( problem follows directly from the complexity result in~\cite{marzouk24a}, which demonstrated that computing conditional SHAP under Markovian distributions can be done in polynomial time, along with the fact that \texttt{IND} is a subset of \texttt{Markov}. The $\#$P-Hard complexity of \texttt{LOC-I-SHAP}($\texttt{ENS-DT}{C}$, $\ar{\texttt{HMM}}$) is derived from the results in~\citep{huangupdates}, which showed that \texttt{LOC-I-SHAP}($\texttt{ENS-DT}{\texttt{C}}$, \texttt{IND}) is $\#$P-Hard and that \texttt{IND} is a subset of $\ar{\texttt{HMM}}$. The NP-Hardness of \texttt{LOC-C-SHAP}(\texttt{RNN-ReLU}, \texttt{HMM}) is a corollary of our result that \texttt{LOC-C-SHAP}(\texttt{RNN-ReLU}, \texttt{IND}) is NP-Hard, combined with the fact that \texttt{IND} is a subset of \texttt{HMM}. Finally, the NP-Hardness of \texttt{LOC-I-SHAP}(\texttt{NN-SIGMOID}, $\ar{\texttt{HMM}}$) is a corollary of the result from~\cite{vander21}, which proved that \texttt{LOC-C-SHAP}(\texttt{NN-SIGMOID}, \texttt{IND}) is NP-Hard, given that \texttt{IND} is a subset of $\ar{\texttt{HMM}}$, and that conditional and interventional SHAP coincide under independent (\texttt{IND}) distributions~\citep{sundararajan20b}.



%The polynomial-time complexity result for the \texttt{LOC-C-SHAP} problem follows directly from the complexity result in~\cite{marzouk24a}, which demonstrated that computing conditional SHAP under Markovian distributions can be done in polynomial time, combined with the fact that \texttt{IND} $\subseteq$ \texttt{Markov}. The fact that the problem 
 %\texttt{LOC-I-SHAP}($\texttt{ENS-DT}_{C}$, $\ar{\texttt{HMM}}$) is $\#$P-Hard is derived from the complexity results provided~\citep{huangupdates} which showed that texttt{LOC-I-SHAP}($\texttt{ENS-DT}_{\texttt{C}}$, \texttt{IND}) and the fact that \texttt{IND} $\subseteq$ $\ar{\texttt{HMM}}$. The complexity result that shows that \texttt{LOC-C-SHAP}(\texttt{RNN-ReLU}, \texttt{HMM}) is NP-Hard is crollary of our result that \texttt{LOC-C-SHAP}(\texttt{RNN-ReLU}, \texttt{IND}) is NP-Hard and that \texttt{IND} $\subseteq$ \texttt{HMM}. Finally, the fact that  \texttt{LOC-I-SHAP}(\texttt{NN-SIGMOID}, $\ar{\texttt{HMM}}$) is NP-Hard is a corollary of the result provided in~\cite{vander21} which showed that \texttt{LOC-C-SHAP}(\texttt{NN-SIGMOID}, $\texttt{IND}$), that \texttt{IND} $\subseteq$ $\ar{\texttt{HMM}}$, and that conditional and interventional SHAP intersect under independent (\texttt{IND}) distrbutions~\citep{sundararajan20b}.

\end{proof}
 
%\textcolor{orange}{Your tasks on the appendix are finished :) :) Bravo. I'll manage the remaining sections (Ignore the comments highlighted in red and blue in the following sections as they are not up to date anymore.}



\begin{comment}
    Corollary: 
     For P \in {IND, EMP}. 
     *-*-SHAP(M,IND) is also hard *-*-SHAP(M,HMM)
        
\end{comment}