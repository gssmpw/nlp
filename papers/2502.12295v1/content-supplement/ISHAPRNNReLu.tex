\section{On the NP-Hardness of computing local Interventional SHAP for RNN-ReLus under independent distributions} 
\label{app:ISHAPRNN}
The objective of this segment is to prove Lemma \ref{lemma:emptyrnnrelu} from section \ref{subsec:rnnreluhard} of the main paper. This lemma played a crucial role in proving the primary result of this section, which demonstrates the intractability of the problem $\texttt{LOC-I-SHAP}(\texttt{RNN-ReLu}, \texttt{IND})$.  The lemma is stated as follows:

\begin{unumberedlemma}
    The problem \texttt{\emph{EMPTY}}\texttt{\emph{(RNN-ReLu)}} is NP-Hard. 
\end{unumberedlemma}

The proof is performed by reduction from the closest string problem (\texttt{CSP}). Formally, the \texttt{CSP} problem is given as follows:

$\bullet$ \textbf{Problem:} \texttt{CSP} \\
    \textbf{Instance:} A collection of strings $S = \{w_{i}\}_{i \in [m]}$, whose length is equal to $n$, and an integer $k > 0$. \\ 
      \textbf{Output:} Does there exist a string $w' \in \Sigma^{n}$, such that for any $w_{i} \in S$, we have $d_{H}(w_{i}, w') \leq k$?  \\ 
        where $d_{H}(.,.)$ is the Hamming distance given as: $d_{H}(w,w') := \sum\limits_{i=1}^{ [|w|]} \mathrm{1}_{w_{j}}(w'_{j})$. \\
     
The \texttt{CSP} problem is known to be NP-Hard \citep{li2000closest}. Our reduction approach involves constructing, in polynomial time, an RNN that accepts only closest string solutions for a given input instance $(S,k)$ of the \texttt{CSP} problem. Consequently, the \texttt{CSP} produces a \textit{Yes} answer for the input instance if and only if the constructed RNN-ReLU is empty on the support $\Sigma^{n}$, which directly leads to the result of Lemma~\ref{lemma:emptyrnnrelu}. We divide the proof of this reduction strategy into two parts:
     
     
     %The reduction strategy for our problem consists of constructing in polynomial time an RNN that accepts only closest string strings for a given input instance $(S,k)$ of the \texttt{CSP} problem. Thus, the \texttt{CSP} outputs a YES answer on the given input instance if and only if the constructed RNN-ReLu is empty on the support $\Sigma^{n}$,  yielding straightforwardly the result of Lemma \ref{lemma:emptyrnnrelu}. 
     %)
     %We structure the proof of this reduction strategy into two parts:
     \begin{itemize}
         \item Given an arbitrary string $w$ and an integer $k > 0$, we present a construction of an RNN-ReLu that simulates the computation of the Hamming distance for $w$. The outcome, indicating whether the Hamming distance exceeds the threshold $k$, is encoded in the activation of a specific neuron within the constructed RNN-ReLu. This procedure will be referred to as $\texttt{CONSTRUCT}(w,k)$.
         \item Concatenate the RNN-ReLUs generated by the procedure $\texttt{CONSTRUCT}(.,.)$ into a single unified RNN cell. Then, assign an appropriate output matrix that accomplishes the desired objective of the construction.
     \end{itemize}
     
     \paragraph{The procedure \texttt{CONSTRUCT}($w,k$).}  Let $w \in \{0,1\}^{n}$ be a fixed reference string and $k > 0$ be an integer. The procedure $\texttt{CONSTRUCT}(w,k)$ returns an RNN cell of dimension $|w| + 1$ that satisfies the properties discussed earlier. The parametrization of $\texttt{CONSTRUCT}(w,k)$ is defined as follows:









 %Fix a string of reference $w \in \{0,1\}^{n}$, and an integer $k > 0$. The procedure $\texttt{CONSTRUCT}(w,k)$ returns an RNN cell of dimension $|w| + 1$ that meets the satisfies the properties stated above discussion. The parametrization of $\texttt{CONSTRUCT}(w,k)$ is given as follows: 
\begin{itemize}
\item \textbf{The initial state vector.} $h_{init} = [1, 0 \ldots, 0 ]^{T} \in \mathbb{R}^{n+1}$.
 \item \textbf{The transition matrix.} he transition matrix is dependent on $k$ and is expressed as:
\begin{equation}\label{transition}
W_{k} = \begin{pmatrix}
   0 & 0 & .& . & . & 0 & 0\\
   1 & 0 & . & . & . & 0  & 0\\ 
   0 & 1 & . & . & . & 0  & 0\\ 
    . & . & . & . & . & .  & .\\
   . & . & . & . & . & .  & . \\
   0 & 0 & .& . & . & 0 & -k \\ 
   0 & 0 & .. & . & . & 0 &  1 
\end{pmatrix} \in \mathbb{R}^{(n+1) \times (n+1)}
\end{equation}
\item \textbf{The embedding vectors.} Embedding vectors are dependent on the reference string $w$. To prevent any confusion, we will use the superscript $w$ for indexing. The embedding vectors are constructed as follows: \\ For a symbol $\sigma \in \{0,1\}$  and $l \in [n]$, we define $v_{\sigma}^{w_{i}}[l] = 1 $ if $w_{l} \neq \sigma$. All other elements of $v_{\sigma}^{w}$ are set to $0$.
%Embedding vectors depend on the string of reference $w$. In order to avoid confusion, we will index them with the superscript $w$. The construction of the embedding vectors is given as follows: \\ 
%For a symbol $\sigma \in \{0,1\}$ and $l \in [n]$, we have $v_{\sigma}^{w_{i}}[l] = 1 $ if $w_{l} \neq \sigma$. All the other elements of $v_{\sigma}^{w}$ are set to $0$. 
\end{itemize}
The following proposition formally demonstrates that this construction possesses the desired property:
\begin{proposition}\label{app:prop:reluproperties}
Let $w \in \{0,1\}^{n}$ be an arbitrary string, and $k > 0$ be an arbitrary integer. The procedure $\emph{\texttt{CONSTRUCT}}(w,k)$ outputs an RNN cell $\langle h_{init}, W_{k}, \{v_{\sigma}^{(w)}\}_{\sigma \in \{0,1\}}\rangle$, which satisfies the following properties: 
\begin{enumerate}
    \item For any string $w' \in \{0,1\}^{s}$, where $ s < n$, we have that $h_{w}[s] = d_{H}(w,w')$,
    \item For a string $w' \in \{0,1\}^{n}$, it holds that: 
\begin{equation}\label{eq:levsimulate}        
h_{w'}[n] = \emph{\text{ReLu}}(d_{H}(w, w') - k)
    \end{equation}
\end{enumerate}
\end{proposition}

\begin{proof} We individually prove the two defined requirements as follows:
\begin{enumerate}
    \item The proof proceeds by induction on $s$. For the base case, when $s = 1$, we observe that for any symbol $\sigma \in \Sigma$, it holds that $h_{\sigma} = v_{\sigma}^{(w)}$. By construction, $v_{\sigma}^{(w)}[1] = \mathrm{1}_{w_{1}}(\sigma) = d_{H}(\sigma,w_{1})$.
    %The proof is by induction on $s$. For $s = 1$, we have for any symbol $\sigma \in \Sigma$: $h_{\sigma} = v_{\sigma}^{(w)}$. By construction, $v_{\sigma}^{(w)}[1] = \mathrm{1}_{w_{1}}(\sigma) = d_{H}(\sigma,w_{1})$. \\ 
    Assume the proposition holds for $s < n-2$. We now prove it for $s+1$. Let $w' \in \Sigma^{s}$ and $\sigma \in \Sigma$. Then, we have
    %Assume the statement of the proposition is true for $s < n-2$, we prove it's the case for $s+1$. Let $w' \in \Sigma^{s}$ and $\sigma \in \Sigma$, we have
 
    \begin{align*}
    h_{w' \sigma}[s+1] &= \text{ReLu}(W[:,s+1]^{T} \cdot h_{w'} + v_{\sigma}[s+1]) \\
    &= \text{ReLu}(h_{w'}[s] + v_{\sigma}[s+1]) \\
    &= d_{H}(w_{1:s}, w') + \mathrm{1}_{w_{s+1}}( \sigma) \\
    &= d_{H}(w'\sigma, w_{1:s+1})
    \end{align*}
 
    \item Let $w" = w'\sigma$ be a string of length $n$. From the first part of the proposition, we have 
    $h_{w'}[n-1] = d(w', w_{1:(n-1)})$. Consequently,
    \begin{align*} 
    h_{w"}[n] &= \text{ReLu}(W[:,n]^{T} \cdot h_{w'} + v_{\sigma}[n]) \\
    &= \text{ReLu}(h_{w'}[n-1]  - k + \mathrm{1}_{w_{n} } (\sigma)) \\ 
    &= \text{ReLu}(d_{H}(w, w^{i}) - k)
    \end{align*}
    \end{enumerate}
\end{proof}


The key property emphasized by Proposition~\ref{app:prop:reluproperties} is stated in Equation~\ref{eq:levsimulate}. It demonstrates that the $n$-th neuron of the constructed RNN cell encodes the Hamming distance between the reference string $w$ and the input string $w'$. Notably, the activation value of this neuron is $0$ if and only if $d_{H}(w,w') \leq k$; otherwise, it is at least $1$.

%The most important property highlighted by Proposition \ref{prop:reluproperties} is given in Equation \ref{eq:levsimulate}. It shows that the $n$-th neuron of the constructed RNN cell encodes the Hamming distance between the string of reference $w$ and the input string $w'$. Note that the activation value of this neuron is equal to $0$ if and only if $d_{H}(w,w') \leq k$. Otherwise, it is greater or equal to $1$.  

\paragraph{Concatenation and Output Matrix instantiation.} 

Let $(S, k)$ be an instance of the closest string problem. The final RNN cell will be formed by concatenating the RNN cells $\langle h_{init}, W_{k}, \{v_{\sigma}^{w^{(i)}}\}_{i \in [|S|]}\rangle$, where the set $\{w^{(i)}\}_{i \in |S|}$ consists of elements from $S$. The concatenation of two RNN cells, such as $\langle h_{1}, W_{1}, v_{\sigma}^{(1)}\rangle$ and $\langle h_{1}, W_{2}, v_{\sigma}^{(2)}\rangle$, produces a new cell defined as
%Let $(S, k)$ be an input instance of the closest string problem.The final RNN cell will be constructed as a concatenation of the RNN cells $\langle h_{init}, W_{k}, \{v_{\sigma}^{w^{(i)}}\}_{i \in [|S|]}\rangle$ where the set $\{w^{(i)}\}_{i \in |S|}$ are elements of $S$. \\ 
%The concatenation operation of two RNN cells, say $\langle h_{1}, W_{1}, v_{\sigma}^{(1)}\rangle$ and $\langle h_{1}, W_{2}, v_{\sigma}^{(2)}\rangle$, result in a new cell given as 
$\Big\langle \begin{pmatrix}
    h_{1} \\h_{2}
\end{pmatrix}, \begin{pmatrix}
    W_{1} & 0 \\ 
    0 & W_{2}
\end{pmatrix}$, $\{ \begin{pmatrix} v_{\sigma}^{1} \\ 
v_{\sigma}^{2}
\end{pmatrix}
\}_{\sigma \in \Sigma}\Big\rangle$. \footnote{Although the concatenation operation is non-commutative, the order according to which we perform the concatenation operator does not affect the result of the reduction.} \\

We concatenate all RNN-ReLUs produced by the \texttt{CONSTRUCT}(.,.) procedure on the instances $\{(w,k)\}_{w \in S}$. The output matrix of the resulting RNN-ReLU, denoted as $O \in \mathbb{R}^{(n+1) \cdot |S|}$, is selected such that for any set of vectors $\{h_{i}\}_{i \in [|S|]}$ in $\mathbb{R}^{n+1}$, the following holds:

%We concatenate all RNN-ReLus resulting from the procedure \texttt{CONSTRUCT}(.,.) on the instances $\{(w,k)\}_{w \in S}$. 
%The output matrix of the resulting RNN-ReLu $O \in \mathbb{R}^{(n+1) \cdot |S|}$ is chosen such that, for any collection of vectors $\{h_{i}\}_{i \in [|S|]}$ in $\mathbb{R}^{n+1}$, we have:
$$O^{T} \cdot \begin{bmatrix}
    h_{1} & \ldots & h_{s}
\end{bmatrix} = \sum\limits_{i \in [|S|]} - h_{i}[n] + \frac{1}{2} \cdot h_{1}[n+1]$$



Under this setting, it is important to note that, given the properties of the RNN-ReLu cell produced by the procedure $\texttt{CONSTRUCT}(.,.)$ (Proposition \ref{app:prop:reluproperties}), and the fact that the activation value of the $(n+1)$-neuron is always equal to 1 by design, the output of the constructed RNN-ReLu on an input sequence $w'$ is 1 if and only if for all $w \in S$, we have $d_{H}(w,w') \leq k$. As a result, the constructed RNN-ReLu is empty on the support $n$ if and only if there is no string $w'$ such that $d_{H}(w,w') \leq k$ for all $w$.

%Under this setting, it is to be noted, in light of the properties of the RNN-ReLu cell outputted by the procedure $\texttt{CONSTRUCT}(.,.)$ (Proposition \ref{prop:reluproperties}), and the fact that the activation value of the $(n+1)$-neuron is always equal to $1$ by construction, the output of the constructed RNN-ReLu on an input sequence $w'$ is equal to $1$ if only if $w$ for all $w \in S$, we have $d_{H}(w,w') \leq k$. Consequently, the constructed RNN-ReLu is empty on the support $n$ if and only if there exists no string $w'$ such that for all $d_{H}(w,w') \leq k$. 
\begin{comment}
This segment provides a formal proof of Proposition 4 of the main article. This proposition highlights some properties satisfied by the RNN-ReLu cell resulting from the procedure \texttt{CONSTRUCT}(.,.):
\begin{proposition}
Let $w \in \{0,1\}^{n}$ be an arbitrary string, and $k > 0$ be an arbitrary integer. The procedure $\emph{\texttt{CONSTRUCT}}(w,k)$ outputs a RNN cell $<h_{init}, W_{k}, \{v_{\sigma}^{(w)}\}_{\sigma \in \{0,1\}}>$ satisfies the following properties: 
\begin{enumerate}
    \item For any string $w' \in \{0,1\}^{s}$ where $ s < n$, we have $h_{w}[s] = d_{H}(w,w')$,
    \item For a string $w' \in \{0,1\}^{n}$, we have 
    \begin{equation}\label{eq:levsimulate}
        h_{w'}[n] = \text{ReLu}(d_{H}(w, w') - k)
    \end{equation}
\end{enumerate}
\end{proposition}
\begin{proof}
    \textit{(1)} The proof is by induction on $s$. For $s = 1$, we have for any symbol $\sigma \in \Sigma$: $h_{\sigma} = v_{\sigma}^{(w)}$. By construction, $v_{\sigma}^{(w)}[1] = \mathrm{1}_{w_{1}}(\sigma) = d_{H}(\sigma,w_{1})$. \\ 
    Assume the statement of the proposition is true for $s < n-2$, we prove it's the case for $s+1$. Let $w' \in \Sigma^{s}$ and $\sigma \in \Sigma$, we have
 
    \begin{align*}
    h_{w' \sigma}[s+1] &= ReLu(W[:,s+1]^{T} \cdot h_{w'} + v_{\sigma}[s+1]) \\
    &= ReLu(h_{w'}[s] + v_{\sigma}[s+1]) \\
    &= d_{H}(w_{1:s}, w') + \mathrm{1}_{w_{s+1}}( \sigma) \\
    &= d_{H}(w'\sigma, w_{1:s+1})
    \end{align*}
 
    \textit{(2)} Let $w" = w'\sigma$ be a string of length $n$. According to the first part of the proposition, we have 
    $h_{w'}[n-1] = d(w', w_{1:(n-1)})$. Consequently,
    \begin{align*} 
    h_{w"}[n] &= \text{ReLu}(W[:,n]^{T} \cdot h_{w'} + v_{\sigma}[n]) \\
    &= \text{ReLu}(h_{w'}[n-1]  - k + \mathrm{1}_{w_{n} } (\sigma)) \\ 
    &= \text{ReLu}(d_{H}(w, w^{i}) - k)
    \end{align*}
\end{proof}
\end{comment}