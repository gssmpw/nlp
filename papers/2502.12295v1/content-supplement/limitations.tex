\section{Limitations and Future Work} \label{app:limitations}
First, while our work presents novel complexity results for the computation of various Shapley value variants, there are many other variants that we did not address, such as asymmetric Shapley values~\citep{frye20}, counterfactual Shapley values~\citep{albini22}, and others. Additionally, in Section~\ref{sec:generalized}, we provided new complexity relationships between SHAP variants, but many more connections remain unexplored. Extensions of our work could also consider different model types and distributional assumptions beyond those discussed here.

A second limitation is that, while we thoroughly examine theoretical strict complexity gaps between polynomial-time and non-polynomial-time (i.e., NP-Hard) computations of SHAP variants, we did not focus on techincal optimizations of the specific poly-time algorithms proposed in this paper. Some of the polynomial factors of these algorithms, though constant, may be improved (see Section~\ref{sec:generalized} for more details), and we believe improving them, as well as exploring potential parallelization techniques for solving them presents an exciting direction for future research.

Finally, we adopt the common convention used in all previous works on the computational complexity of computing Shapley values~\citep{arenas23, vander21, marzouk24a, huangupdates}, assuming a discrete input space to simplify the technical aspects of the proofs. However, we emphasize that many of our findings also extend to continuous domains. Specifically, for both decision trees and tree ensembles, the complexity results remain consistent across both discrete and continuous domains, as the tractability of the underlying models is unaffected. On the other hand, while this assumption does not generally apply to linear models and neural networks, we stress that the computational \emph{hardness} results we present for these models continue to hold in continuous settings. The same, however, cannot be said for membership proofs. Extending the membership proofs for linear models and neural networks, as presented in this work, to continuous domains and exploring other computational complexity frameworks for these models offers a promising direction for future research.

%Finally, we follow the common convention of all previous works that studied the computational complexity of computing Shapley values~\citep{arenas23, vander21, marzouk24a, huangupdates}, and assume the input space is discrete. However, we highlight that many of our results hold for continous domains as well. FIrstly, for both decision trees and tree ensembles, all complexity results remain consistent, whether under discrete or continous domains since the tractability of the underlynig models remains. Secondly, while this assumption does not generally hold for linear models and neural networks, we highlight that the computational \emph{hardness} results we present for these models continue to apply in continuous settings, while the same does not necessarily hold for membership proofs. Expanding the membership proofs presented in this work for linear models and neural networks, along with exploring other computational complexity frameworks for these models in continuous domains, presents a valuable direction for future research.








%First, while our work provides novel complexity results for the computation of a wide range of Shapley value vraiants, there exist many other variants that were not addressed in this work, such as Assymetic Shapley values~\citep{frye20}, counterfactual Shapley values~\citep{albini22}, and more. Moreover, while in Section~\ref{sec:generalized} we indeed provided novel complexity relations between SHAP variants, many additional relations can be explored. Other extensions of our work can include other model types and distributional assumptions than the ones mentioned here. A second limitation of our work is that while this work provides a comprehensive investigation of strict complexity gaps between polynomial-time and non-polynomial time (i.e., NP-Hard) computations of SHAP values, we do not focus on optimizing the specific algorithms that were proposed in this work. While some of the algorithms require relatively high (but up to a constant) polynomial factors (See section~\ref{app:shapwa}), we believe that improving these algorithms as well as offering paralleisation techniques for performing them are compelling avenues for future research. Lastly, similarly to all previous works which investigated the computational complexity of obtaining Shapley values~\citep{arenas23, vander21, marzouk24a, huangupdates}, we assumed that the input space is in discrete form, which alligns with many ML settings, such as NLP domains. While in some models such as in decision trees and tree ensembles, this distinction does not possess a difference in complexity, this distinction can cause differences in lilnear models and neural networks. We note that the \emph{hardness} results we provide here maintain for the continous setting, but this is not necessairily the case for membership proofs. Expanding the membership results we obtained here, as well as other computational complexity frameworks for Shapley values represents a compelling avenue for further research.

%\textcolor{red}{ This section is to be completed. If you are thinking about raising a limitation regarding the running time complexities we discussed in our previous meeting, let me know so i include the table of running time complexities in section \ref{app:shapwa}. 
%}