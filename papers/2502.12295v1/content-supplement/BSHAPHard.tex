\section{The problem \texttt{LOC-B-SHAP}(\texttt{NN-SIGMOID}), \texttt{LOC-B-SHAP}(\texttt{RNN-ReLu}) and \texttt{LOC-B-SHAP}($\texttt{ENS-DT}_{\texttt{C}}$) are Hard: Proofs of Intermediary Results} \label{app:BSHAPSIGMOID}

This section of the appendix is devoted to presenting the proofs of the mathematical results discussed in subsection~\ref{subsec:baseline} regarding the local Baseline SHAP problem for different model families. The structure is as follows:
%This section of the appendix is dedicated to provide proofs of mathematical results provided in subsection \ref{subsec:baseline} concerning the local Baseline SHAP problem for various model families. It is organized as follows: 

\begin{enumerate}
    \item 
The first subsection (Subsection~\ref{app:subsec:locbshap}) presents the proof of the intractability of the \texttt{LOC-B-SHAP}(\texttt{NN-SIGMOID}) problem, established through a reduction from the dummy player problem in WMGs.
    \item The second subsection (Subsection~\ref{app:subsec:bshaprf}) presents the proof of the NP-hardness of computing Local B-SHAP for the tree ensemble classifiers, achieved via a reduction from the 3SAT problem.
    %The second subsection (subsection \ref{app:subsec:bshaprf}) provides the proof of the NP-Hardness of computing the Local B-SHAP for the family of Random Forests by reduction from the 3SAT problem.
\end{enumerate}

 
\subsection{Reducing the dummy player problem of WMGs to \texttt{LOC-B-SHAP}(\texttt{NN-SIGMOID}) and \texttt{LOC-B-SHAP}(\texttt{RNN-ReLu})} \label{app:subsec:locbshap}


The purpose of this segment is to establish Proposition~\ref{prop:reductionsigmoid} from the main paper. Let us first restate the proposition:

\begin{unumberedproposition} There exist two polynomial-time algorithms, which are defined as follows:
 \begin{enumerate}
        \item For \texttt{\emph{NN-SIGMOID}}, there exists a polynomial-time algorithm that takes as input a WMG $G$ and a player $i$ in $G$ and returns a sigmoidal neural network $f_{G}$ over $\{0,1\}^{N}$, $(x,x^{ref}) \in \{0,1\}^{N}$ and $\epsilon \in \mathbb{R}$ such that:
        $$\text{The player i is not dummy} \iff \phi_{b}(f_{G},i,x,x^{ref}) > \epsilon$$
        \item For \texttt{\emph{RNN-ReLu}}, there exists a polynomial-time algorithm that takes as input a WMG $G$ and a player $i$ in $G$ and returns a sigmoidal neural network $f_{G}$ over $\{0,1\}^{N}$, $(x,x^{ref}) \in \{0,1\}^{N}$ and $\epsilon \in \mathbb{R}$ such that:
         $$\text{The player i is not dummy} \iff \phi_{b}(f_{G},i,x,x^{ref}) > 0$$
    \end{enumerate}
\end{unumberedproposition}

We divide the remainder of this segment into two parts. The first part focuses on the family of sigmoidal neural networks (\texttt{NN-SIGMOID}), and the second part addresses the class of RNN-ReLus (\texttt{RNN-ReLu}).

\subsubsection{The case of \texttt{LOC-B-SHAP}(\texttt{NN-SIGMOID})}
The intractability of the problem \texttt{LOC-B-SHAP}(\texttt{NN-SIGMOID}) is obtained by reduction from the dummy player problem of WMGs. The remainder of this segment will be dedicated to prove the following:
\begin{proposition} \label{app:prop:bshapsigmoid}
    There exists a polynomial-time algorithm that takes as input a WMG $G = \langle N, \{n_{j}\}_{j \in [N]}, q \rangle$, and a player $i$ in $G$, and returns a sigmoidal neural network $f_{G}$ over $\mathbb{R}^{N}$, two vectors $(x, x^{ref}) \in \mathbb{R}^{N} \times \mathbb{R}^{N}$, and a scalar $\epsilon > 0$ such that:
    \begin{equation*}
     \text{The player i is dummy} \iff \texttt{\emph{LOC-B-SHAP}}(f_{G},x,i,x^{ref}) \leq \epsilon   
    \end{equation*}
     
\end{proposition}

\begin{algorithm}
\caption{Reduction of the \texttt{DUMMY} problem to \texttt{LOC-B-SHAP}(\texttt{NN-SIGMOID})}
\label{app:alg:WMG2b-SHAP}
\begin{algorithmic}[1]
\REQUIRE A WMG $G = \langle N, \{n_{j}\}_{j \in [N]},q\rangle$, $i \in [N]$
\ENSURE A Sigmoidal Neural Network $\sigma$ over $\mathbb{R}^{N}$, an integer $i \in [N]$, $(x,x^{ref}) \in \mathbb{R}^{2}$, and $\epsilon \geq 0$ 
\STATE $x \leftarrow [1 , \ldots , 1]$
\STATE $x^{ref} \leftarrow [0, \ldots , 0]$
\STATE $C_{N} \leftarrow N \cdot \binom{N-1}{\lfloor \frac{N-1}{2} \rfloor}!$
\STATE $\epsilon \leftarrow \frac{1}{1 + C_{N}}$
\STATE Construct the Sigmoidal Neural Network $f$, parameterized by:
$$f_{G}(x) = \sigma(2 \cdot \log(\frac{1-\epsilon}{\epsilon}) \cdot (x - q + \frac{1}{2}))$$
\RETURN $\langle f_{G},~i,~x,~x^{(ref)},~\epsilon\rangle$
\end{algorithmic}
\end{algorithm}

The pseudo-code of the (polynomial-time) algorithm whose existence is implicitly stated in proposition \ref{app:prop:bshapsigmoid} is provided in Algorithm \ref{app:alg:WMG2b-SHAP}. In the following, we provide the proof of the correctness of this reduction. In the remainder of this segment, we fix an input instance $I = \langle G,i\rangle $ of the \texttt{DUMMY} problem where $G = \langle N, \{n_{j}\}_{j \in [N]}, q\rangle$ is a WMG, and $i \in [N]$ is a player in $G$. And, we use the notation $\langle f_{G}, i, x,x^{ref}, \epsilon\rangle$ to represent the output of Algorithm~\ref{app:alg:WMG2b-SHAP}. To ease exposition, we also introduce the following parameter which depends on the number of players $N$ (appearing in Line 3 of Algorithm \ref{app:alg:WMG2b-SHAP}): 
$$C_{N} \myeq N \cdot \binom{\lfloor \frac{N-1}{2} \rfloor}{N-1}!$$

Note that by setting the instance to explain $x$ to $\begin{pmatrix} 1 \\ \ldots \\ 1\end{pmatrix}$, and the reference instance $x^{ref}$ to $\begin{pmatrix}
    0 \\ \ldots \\ 0
\end{pmatrix}$, the constructed function $f_{G}$ (Line 5 of Algorithm \ref{app:alg:WMG2b-SHAP}) computes the following quantity for any coalition of players $S \subseteq [N] \setminus \{i\}$:

$$f_{G}(x_{S};x_{\bar{S}}^{ref}) = \sigma \left(2 \log(\frac{1 - \epsilon}{\epsilon})\cdot \log(N) \cdot (\sum\limits_{j \in S} n_{j} - q + \frac{1}{2})\right)$$

where $\mathbf{x}_{S} = (x_{S}; x_{\bar{S}}^{ref}) \in \{0,1\}^{n}$ is such that $x_{i} = 1$ if $i \in S$, $0$ otherwise.

The correctness of the reduction proposed in  Algorithm~\ref{app:alg:WMG2b-SHAP} is a result of the following two claims:

\begin{claim} \label{app:claim:one} 
If the player $i$ is dummy then for any $S \subseteq [N] \setminus \{i\}$ it holds that: 
    \begin{equation} \label{app:eq:claim1}
    f_{G}(x_{S \cup \{i\}};x_{\bar{S \cup \{i\}}}^{ref}) - f_{G}(x_{S}; x_{\bar{S}^{ref}}) \leq \epsilon
    \end{equation}
    \end{claim}

  \begin{claim} \label{app:claim:two}
  If the player $i$ is not dummy then there must exist some $S_{d} \subseteq [N] \setminus \{i\}$, such that: 
  \begin{equation} \label{app:eq:claim2}
    f_{G}(x_{S_{d} \cup \{i\}};x_{\bar{S_{d} \cup \{i\}}}^{ref}) - f_{G}(x_{S_{d}}; x_{\bar{S_{d}}}) > 1 - \epsilon
    \end{equation}
 \end{claim}
The result of proposition \ref{app:prop:bshapsigmoid} is a corollary of claims \ref{app:claim:one} and \ref{app:claim:two}. Before proving these two claims, we will first prove that, provided claims \ref{app:claim:one} and  \ref{app:claim:two} are true, then the proposition \ref{app:prop:bshapsigmoid} holds.  To prove this, we will consider two separate cases:
\begin{itemize}
    \item \textbf{Case 1 (The player $i$ is dummy):} In this case, we show that if the player $i$ is dummy and claim \ref{app:claim:one} holds, then: 
      $$\phi_{b}(f_{G}, i , x, x^{ref}) \leq \epsilon$$. 
    
Assume claim \ref{app:claim:one} holds, we have:
    \begin{align*} 
    \phi_b(f_{G},i,x,x^{ref}) &= \sum_{S \subseteq [n] \setminus \{i\}} \frac{|S|!\cdot (N - |S| - 1)!}{N!} \cdot \left[ f_{G}(x_{S \cup \{i\}};x_{\bar{S \cup \{i\}}}^{ref}) - f_{G}(x_{S}; x_{\bar{S}^{ref}})  \right] \\
    & \leq \sum_{S \subseteq [n] \setminus \{i\}} \frac{|S|!\cdot (N - |S| - 1)!}{N!} \cdot \epsilon \\
    & = \epsilon
    \end{align*}

\item \textbf{Case 2 (The player $i$ is not dummy):} In this case, we show that if the player $i$ is not dummy and Claim \ref{app:claim:two} holds, then: 
$$\phi_{b}(f_{G},i,x,x^{ref}) > \epsilon$$

Assume claim \ref{app:claim:two} holds, we have:
\begin{align*}
    \phi_b(f_{G}, i , x, x^{ref}) &= \sum_{S \subseteq [n] \setminus \{i\}} \frac{|S|!\cdot (n - |S| - 1)!}{n!} \cdot \left[ f_{G}(x_{S \cup \{i\}};x_{\bar{S \cup \{i\}}}^{ref}) - f_{G}(x_{S}; x_{\bar{S}^{ref}})  \right]  \\
    &> \frac{|S_{d}|! \cdot (N - |S_{d}|-1)!}{N!} \cdot (1 - \epsilon) \\ 
    & = \frac{1}{N \cdot  \binom{|S_{d}|}{N-1}!}  \cdot (1 - \epsilon) \\
    &> \frac{1}{N \cdot \binom{\lfloor \frac{N-1}{2} \rfloor}{N-1}!} \cdot (1 - \epsilon) \ \\ 
    &> \frac{1}{C_{N}}  \cdot (1 - \epsilon) = \frac{1}{C_{N}} \cdot (1 - \frac{1}{1 + C_{N}}) = \frac{1}{1+C_{N}} = \epsilon
\end{align*}
where the third inequality follows from the fact that for any $N \in \mathbb{N}$, $k \in [N]$, we have $\binom{N}{k} ! \leq \binom{N}{\lfloor \frac{N}{2}! \rfloor}$.
\end{itemize}

% Properties of the sigmoid:


The above argument indicates that proving claims \ref{app:claim:one} and \ref{app:claim:two} is sufficient to establish the desired result in this section (Proposition~\ref{app:prop:bshapsigmoid}). What remains is to show that claims \ref{app:claim:one} and \ref{app:claim:two} indeed hold. 

The following simple technical lemma leverages some properties of the sigmoidal function to prove that the constructed sigmoidal function $f_{G}$ in ALgorithm \ref{app:alg:WMG2b-SHAP} verifies the desired properties of both these claims: 
%The above argument suggests that proving the truth of Claims 1 and 2 is a sufficient condition to prove the sought result of this segment (Proposition \ref{app:prop:bshapsigmoid}). In the following we suggest to prove that Claims 1 and 2 indeed hold. 

%Before delving into the technical details of the reduction, we present first a technical lemma providing some properties of the sigmoidal neural network that will be useful in the proof of these two claims:
\begin{lemma} \label{app:lemma:technicallemma}
 The constructed function $f_{G}$ in Algorithm \ref{app:alg:WMG2b-SHAP}
  satisfies the following conditions:
  \begin{enumerate}
      \item If $x \leq q-1$, we have: $f_G(x; N,q) \leq \epsilon$
      \item If $x > q$, we have: $f_G(x;N,q) \geq 1 - \epsilon$
  \end{enumerate}
\end{lemma}
\begin{proof}
   By the property of monotonicity of the sigmoidal function and its symmertry around $0$ (thus, assuming $q = 0$ w.l.o.g), it's sufficient to prove that for $x = -1$, we have that: 
   $f_{G}(-1;N,0) \leq \epsilon$. By simple calculation of $f_{G}(-1,N,0)$ using the parametrization of the function $f_{G}$, one can obtain the result.   
\end{proof}

Now we are ready to prove claims \ref{app:claim:one} and \ref{app:claim:two}.

\paragraph{Proof of Claim \ref{app:claim:one}.}
Assume that player $i$ is a dummy. Fix an arbitrary coalition $S \subseteq [N] \setminus \{i\}$. We now need to demonstrate that condition~\eqref{app:eq:claim1} holds for $S$. Since player $i$ is a dummy, there are two possible cases: either both $S$ and $S \cup \{i\}$ are winning, or neither of them are.

%Assume that player $i$ is dummy. Fix an arbitrary coalition $S \subseteq [N] \setminus \{i\}$. We now need to prove that condition \eqref{app:eq:claim1} holds for $S$. Since player $i$ is dummy, there exists between two cases: The case where both $S$ and $S \cup \{i\}$ are winning, and the case where they are not. 

$\bullet$ \textbf{Case 1 (The winning case: $v_{G}(S) = 1$ and $v_{G}(S \cup \{i\}) = 1$):} In this case, we have both $\sum\limits_{j \in S} n_{j} \geq q$ and $\sum\limits_{j \in S} n_{j} + n_{i} \geq q$. Consequently, by Lemma \ref{app:lemma:technicallemma}, both
$f_G(\sum\limits_{j \in S \cup \{i\}} n_{j}; N,q)$ and $f_G(\sum\limits_{j \in S} n_{j}; N,q)$ lie 
in the interval $(1 - \epsilon,1)$. Thus, we have 
 that:
$$f_{G}(x_{S \cup \{i\}};x_{\bar{S \cup \{i\}}}^{ref}) - f_{G}(x_{S}; x_{\bar{S}}^{ref}) \leq 1 - (1 - \epsilon) = \epsilon$$

$\bullet$ \textbf{Case 2 (The non-winning case: $v_{G}(S) = 0$ and $v_{G}(S \cup \{i\}) = 0$):} The proof for this case mimicks the one of the former case. In this case, we have both $\sum\limits_{j \in S} n_{j} \leq q-1$ and $\sum\limits_{j \in S} n_{j} + n_{i} \leq q-1$. Consequently, by Lemma \ref{app:lemma:technicallemma}, both
$g(\sum\limits_{j \in S \cup \{i\}} n_{j}; N,q)$ and $g(\sum\limits_{j \in S} n_{j}; N,q)$ lies 
in the interval $[0, \epsilon)$. Thus, we have:
$$f_{G}(x_{S \cup \{i\}};x_{\bar{S \cup \{i\}}}^{ref}) - f_{G}(x_{S}; x_{\bar{S}^{ref}}) \leq \epsilon$$

\begin{comment}
($\impliedby$) Assume that for any coalition $S \subseteq [N] \setminus \{i\}$, the condition \eqref{app:eq:claim1} holds. We need to prove that the player $i$ is dummy. Assume that the player $i$ is not dummy. Then, there must exist a coalition $S' \subseteq \{i\}$ such that $\sum\limits_{j \in S'} n_{j} \leq q-1$ and $\sum\limits_{j \in S'} n_{j} + n_{i} \geq q$. Then, we have:
$$f_{G}(x_{S \cup \{i\}};x_{\bar{S \cup \{i\}}}^{ref}) - f_{G}(x_{S}; x_{\bar{S}^{ref}}) = g(\sum\limits_{j \in S \cup \{i\}} n_{j}; N,q) - g(\sum\limits_{j \in S} n_{j}; N,q) \geq \frac{N}{N+1} - \frac{1}{N+1} = \frac{N-1}{N+1} \geq \frac{1}{N+1}$$
violating the condition \eqref{app:eq:claim1}, which leads to a contradiction. Consequently, the player $i$ is dummy.
\end{comment}

\paragraph{Proof of Claim \ref{app:claim:two}.} Assume that the player $i$ is not dummy. Then, there must exist a coalition $S_{d} \subset [N] \setminus \{i\}$ such that $\sum\limits_{j \in S \cup \{i\}} n_{j} \geq q$ (a winning coalition), and $\sum\limits_{j \in S } n_{j} \leq q-1$ (A losing coalition). By proposition \ref{app:lemma:technicallemma}, we have then: $f_{G}(x_{S \cup \{i\}}; x_{\bar{S_{d} \cup \{i\}}}) \in (1 - \epsilon,1)$ and $f_{G}(x_{S_{d}}; x_{\bar{S_{d}}}) \in (0,\epsilon)$. Consequently, we have:
   $$f_{G}(x_{S \cup \{i\}}; x_{\bar{S_{d} \cup \{i\}}}) - f_{G}(x_{S_{d}}; x_{\bar{S_{d}}}) \geq 1 - \epsilon$$
\subsubsection{The case \texttt{LOC-B-SHAP}(\texttt{RNN-ReLu})} \label{app;subsec:bshaprnnrelu}
In this subsection, we shall provide the details of the construction of a polynomial-time algorithm that takes as input an instance $\langle G,i\rangle$ of the Dummy problem of WMGs and outputs an input instance of $\texttt{LOC-B-SHAP}(\texttt{RNN-ReLu})$ $\langle f_{G},i,x,x^{ref}\rangle$ such that:
$$\text{The player i is dummy} \iff \phi_{b}(f_{G},i,x,x^{ref}) > 0$$
where $f_{G}$ is a RNN-ReLu. 

Similar to the case of Sigmoidal neural networks, the main idea is to construct a RNN-ReLu that simulates a given WMG. However, contrary to  sigmoidal neural networks, the constructed RNN-ReLu perfectly simulates a WMG:

\begin{proposition} \label{app:prop:WMG2relu}
There exists a polynomial time algorithm that takes as input a WMG $G = \langle N, \{n_{j}\}_{j \in [N]}, q\rangle$ and outputs an RNN-ReLu such that: 
$$\forall x \in \{0,1\}^{N}: f_{G}(x) = v_{G}(S_{x})$$
where: $S_{x} \myeq \{j \in [N]: x_{j} = 1\}$ 
\end{proposition}

\begin{proof}
   Fix a WMG $G = <N,\{n_{j}\}_{j \in [N]}, q>$. The idea of constructing a RNN-ReLu that satisfies the property implicitly stated in Proposition \ref{app:prop:WMG2relu} consists at maintaining the sum of votes of players participating in the coalition in the hidden state vector of the RNN during the forward run of the RNN.
      
    The dynamics of a RNN-ReLu of size $N+2$ that performs this operation is given as follows:
    \begin{enumerate}
        \item \textbf{Initial state:} $h_{init} = \begin{pmatrix} 0 \\ \vdots \\ 0 \\ 1 \end{pmatrix}$
        \item \textbf{Transition function:} for $j \in \{1, \ldots, N-1\}$
        $$h[j+1] = \begin{cases}
        h[j] + n_{j} & \text{if}~~ x_{j} = 1 ~~ \text{(Add the vote of the player $j$ and store it in neuron j+1 (i.e. $h[j+1]$))}\\
        h[j] & \text{if}~~x_{j} = 0 ~~\text{(Ignore the vote of player j as he/she is not part of the coalition)}
        \end{cases}
        $$ 
        $$h[N+2] = h[N+2]$$
                \item \textbf{The output layer:} For a given hidden state vector $\mathbb{R}^{N+1}$ 
        $$y = I(h[N+1] - q \cdot h[N+2] \geq 0)$$
        In other words, the output vector $O = \begin{pmatrix}
            0 \\ \vdots \\ 0 \\ 1 \\ -q
        \end{pmatrix}$
    \end{enumerate}
    The dynamics of this RNN-ReLu is designed in such a way that for any $j \in [N]$ the value of the element $h[j+1]$ of the hidden state vector stores the cumulative votes of participants represented by the input sequence $x_{1:j}$ (The first $j$ symbols of the input sequence corresponding tp the players $\{1, \ldots, j\}$ in the game). Consequently, when $j=N+1$, the voting power of all players in the coalition is stored in $h[N+1]$. The output layer then outputs $1$ if the $h[N+1] \geq q$, otherwise it's equal to $0$.
    \end{proof}

\subsection{The problem \texttt{LOC-B-SHAP}($\texttt{ENS-DT}_{\texttt{C}}$) is NP-Hard} \label{app:subsec:bshaprf}
This segment is dedicated to proving the remaining point of Theorem \ref{thm:intractable}, which states the NP-Hardness of \texttt{LOC-B-SHAP}($\texttt{ENS-DT}_{\texttt{C}}$). As noted in the definition of $\texttt{ENS-DT}{\texttt{C}}$ in Appendix~\ref{app:sec:terminology}, since we are focusing on a \emph{hardness} proof, we can, for simplicity, assume that the weights associated with each tree are equal, giving us a classic majority voting setting. Additionally, we will assume that the number of classes $c:=2$, meaning we have a binary classifier. Clearly, proving hardness for this setting will establish hardness for the more general setting as well. Essentially, these assumptions provide us with a simple random forest classifier for boolean classification. For simplicity, we will henceforth denote this problem as \texttt{LOC-B-SHAP}($\texttt{ENS-DT}_{c}$) rather than the more general \texttt{LOC-B-SHAP}($\texttt{ENS-DT}_{\texttt{C}}$). Proving NP-Hardness for the former will also establish it for the latter. As mentioned earlier, this problem is reduced from the classical 3-SAT problem, a widely known NP-Hard problem.

%This segment is dedicated to prove the remaining point of Theorem \ref{thm:intractable} stating the NP-Hardness of \texttt{LOC-B-SHAP}($\texttt{ENS-DT}_{C}$). As noted in the definition of $\texttt{ENS-DT}_{C}$ within Appendix~\ref{app:sec:terminology}, since in this case we are focused on a \emph{hardness} proof, we can assume for simplicity, that the weights associated which each tree is equal, hence giving us a classic majority voting setting. Moreover, we will assume that the number of classes $c:=2$, meaning that we have a binary classifier. Clearly, proving hardness for this setting, will prove hardness for our more general setting as well. In essense, these assumptions give us a simple random forest classifier for boolean classification. We will, hence for simplicity denote this problem as \texttt{LOC-B-SHAP}(\texttt{ENS-DT}_{c}) and not the more general: \texttt{LOC-B-SHAP}($\texttt{ENS-DT}_{\texttt{C}}$. Obtaining NP-Hardness for the former will derive it for the latter as well. As mentioned earlier, this problem is reduced from the classical 3-SAT problem, a widely known NP-Hard problem.

\paragraph{Reduction strategy.} The reduction strategy is illustrated in Algorithm \ref{alg:SAT2b-SHAP}. For a given input CNF formula $\Psi$ over $n$ boolean variables $X = \{X_{1}, \ldots, X_{n}\}$ and $m$ clauses, the constructed random forest is a model whose set of input features contains the set $X$, with an additional feature denoted $X_{n+1}$ added for the sake of the reduction. The resulting random forest comprises a collection of $2m - 1$ decision trees, which can be categorized into two distinct groups:
\begin{itemize}
    \item $\mathcal{T}_{\Psi}$: A set of $m$ decision trees, each corresponding to a distinct clause in the input CNF formula. For a given clause $C$ in $\Psi$, the associated decision tree is constructed to assign a label 1 to all variable assignments that satisfy the clause $C$  while also ensuring that $x_{n+1} = 1$. It is simple to verify that such a decision tree can be constructed in polynomial time relative to the size of the input CNF formula
    \item \( \mathcal{T}_{null} \): This set consists of \( m - 1 \) copies of a trivial null decision tree. A null decision tree assigns a label of 0 to all input instances. 
\end{itemize}
\begin{algorithm}
\caption{Reduction of the \texttt{SAT} problem to \texttt{LOC-B-SHAP}($\texttt{ENS-DT}_{c})$}
\label{alg:SAT2b-SHAP}
\begin{algorithmic}[1]
\REQUIRE A CNF Formula $\Phi$ of $m$ clauses over $X = \{X_{1}, \ldots, X_{n} \}$
\ENSURE An input instance of \texttt{LOC-B-SHAP}($\texttt{ENS-DT}_{c}$): $\langle\mathcal{T}$, $i$, $x$, $x^{ref}\rangle$
\STATE $x \leftarrow [1 , \ldots , 1]$
\STATE $x^{ref} \leftarrow [0, \ldots , 0]$
\STATE $i \leftarrow n+1$
\STATE $\mathcal{T} \leftarrow \emptyset$
\FOR{$j \in [1,m]$}
 \STATE Construct a Decision Tree $T_{j}$ that assigns a label $1$ to variable assignments satisfying the formula: $C_{j} \land x_{n+1}$
 \STATE $\mathcal{T} \leftarrow \mathcal{T} \cup \{T\}_{j}$
\ENDFOR
 \STATE Construct a null decision tree $T_{null}$ that assigns a label $0$ to all variable assignments
 \STATE Add $m-1$ copies of $T_{null}$ to $\mathcal{T}$
\RETURN $\langle\mathcal{T},~i,~,x,~x^{ref}\rangle$
\end{algorithmic}
\end{algorithm}

The next proposition provides a property of the Random Forest classifier resulting from the construction. This property shall be leveraged in Lemma \ref{lemma:sat2bshap} to yield the main result of this section:

\begin{proposition} \label{app:prop:dnf2bshaprf}
    Let $\Psi$ be an arbitrary \emph{CNF} formula over $n$ boolean variables, and let $\mathcal{T}$ be the ensemble of decision trees outputted by Algorithm \ref{alg:SAT2b-SHAP} for the input $\Psi$. 
    We have: 
    $$f_{\mathcal{T}}(x_{1}, \ldots, x_{n}, x_{n+1}) = \begin{cases}
          1 & \text{if} ~~ x_{n+1} = 1 \land (x_1 , \ldots, x_n) \models \Psi \\
          0 & \text{otherwise}
    \end{cases}$$   
\end{proposition}
\begin{proof}
  Fix an arbitrary CNF formula over $n$ boolean variables, and let $(x_{1}, \ldots, x_{n})$ be an arbitrary variable assignment. If $x_{n+1} = 0$, the decision trees in the set $\mathcal{T}$ assign the label $0$, by construction. Consequently, $f_{\mathcal{T}}(x) = 0$.

    For now, we assume that $x_{n+1} = 1$, if $x \models \Psi$, then $x$ is assigned the label 1 for all decision trees in $\mathcal{T}$. Consequently, $f_{\mathcal{T}}(x) = 1$. On the other hand, if $x$ is not satisfied by $\Psi$, then there exists at least one decision tree in $\mathcal{T}$, say $T_{j}$, that assigns a label 0 to $x$. Consequently, $x$ is assigned a label $0$ by at least $m$ decision trees, i.e., for all decision trees in $\mathcal{T}_{null}\cup\{T_{j}\}$.
\end{proof}

Leveraging the result of Proposition \ref{app:prop:dnf2bshaprf}, the following lemma yields immediately the result of NP-Hardness of the decision problem associated to \texttt{LOC-B-SHAP}($\texttt{ENS-DT}_{c}$):
\begin{lemma} \label{app:lemma:sat2bshap}
    Let $\Psi$ be an arbitrary \emph{CNF} formula of $n$ variables, and  $\langle\mathcal{T},~n+1,~x,x^{ref}\rangle$ be the output of Algorithm \ref{alg:SAT2b-SHAP} for the input $\Psi$. We have:
    $$\phi_{b}(f_{\mathcal{T}}, n+1, x, x^{ref}) > 0 \iff \exists x \in \{0,1\}^{n}: ~ x \models \Psi$$  
\end{lemma}

\begin{proof}
            Let $\Phi$ be an arbitrary CNF formula of $n$ variables, and $\langle \mathcal{T}, n+1, x, x^{ref} \rangle$ be the output of Algorithm \ref{alg:SAT2b-SHAP}. 

    By proposition \ref{app:prop:dnf2bshaprf}, we note that: 
    $$\forall  \mathbf{x}_{n} = (x_1, \ldots, x_n) \in \{0,1\}^{n}: ~ \left[ f_{\mathcal{T}}(\mathbf{x}_{n}, 1) - f_{\mathcal{T}}(\mathbf{x}_{n}, 0) = 0 \iff \mathbf{x}_{n} \text{ doesn't satisfy } \Phi \right] $$
    Combining this fact with the following two facts yiels the result of the lemma:
    \begin{enumerate}
     \item The Baseline SHAP score $\phi_{b}(f_{\mathcal{T}}, n+1, x, x^{ref})$ is expressed as a linear combination of positive weights of the terms $\{f_{\mathcal{T}}(\mathbf{x}_{n},1) - f_{\mathcal{T}}(\mathbf{x}_{n},1) \}_{\mathbf{x}_{n} \in \{0,1\}^{n} }$
     \item According to Proposition\ref{app:prop:dnf2bshaprf}, we obtain:
     $$\forall \mathbf{x}_{n} \in \{0,1 \}^{n} :~f_{\mathcal{T}}(\mathbf{x}_{n}, 1) - f_{\mathcal{T}}(\mathbf{x}_{n}, 0) = f_{\mathcal{T}}(\mathbf{x}_{n}, 1) \geq 0$$
     \end{enumerate}
     
\end{proof}
