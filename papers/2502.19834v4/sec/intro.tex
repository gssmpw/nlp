
%\begin{quote}
%\vspace{-5pt}
%    \textcolor{gray}{``Knowledge itself is power.''  \emph{— Francis Bacon.}}
%\end{quote}
%\vspace{-6pt}

\section{Introduction}
\label{sec:intro}

Multimodal learning \cite{ngiam2011multimodal, jiang2024delving} is a foundational task in artificial intelligence that enables models to integrate diverse data types—such as text, images, and audio—to enhance their representational and reasoning capabilities \cite{ren2021learning, lv2021differentiated, liu2025unsupervised}. However, real-world applications often encounter incomplete or missing modalities due to factors like noise, sensor failures, or privacy constraints, which can compromise a model’s robustness and generalizability. Missing modality completion (MMC) addresses this issue by imputing or reconstructing absent modalities based on available data, allowing models to fully leverage multimodal information. Recent MMC methods \cite{guo-etal-2024-multimodal, wang2023distribution, lee2023multimodal, cai2018deep} have demonstrated promising results across various applications. For instance, in medical diagnostics \cite{zhang2022m3care, cohen2023joint}, MMC models can reconstruct absent diagnostic data (e.g., MRI, CT, and X-rays) by leveraging existing medical reports or related images.

Despite these advancements, MMC methods often have limited out-of-domain (OOD) transferability, typically requiring retraining on new domain data to maintain performance. For example, some MMC models \cite{zhao2021missing, ma2021smil} developed for sentiment analysis need substantial adaptations to be effective in applications like medical imaging or autonomous driving, leading to significant human and computational costs. To address these challenges, recent works have proposed domain-agnostic solutions, such as missing modality tags \cite{zeng2022tag} to aid in predicting absent modalities or prompt-learning techniques \cite{guo-etal-2024-multimodal, lee2023multimodal} that dynamically adjust fusion strategies. While these methods lower domain adaptation costs, they still require extensive training data, limiting their effectiveness in data-scarce domains, such as rare disease analysis. This raises a crucial question: \textit{Can we develop an MMC model that achieves both low-resource dependency (e.g., minimal computational and domain-specific data requirements) and strong OOD capability?}

Recent advancements \cite{wang2024qwen2, liu2024visual, team2024chameleon, hurst2024gpt} in large multimodal models have highlighted their strong OOD capabilities and adaptability to new tasks with minimal resources \cite{kojima2022large}. In this work, we explore the potential of leveraging LMM to effectively address the MMC challenge. Before progressing, it is essential to distinguish between modality generation and missing modality completion. Modality generation typically focuses on creating new modalities from available information, prioritizing \textbf{diversity and creativity} in the generated content. In contrast, missing modality completion emphasizes \textbf{accuracy} over diversity, reconstructing absent modalities to maintain semantic coherence and improve task performance. 

Addressing MMC with LMM presents two primary challenges: \textit{1) \textbf{generation}: applying constraints on modality generation to ensure fidelity to the missing content}, and \textit{2) \textbf{ranking}: selecting the most appropriate completion from generated candidates}. Effective use of LMM for these tasks requires embedding sufficient domain knowledge to guide accurate interpretation and reconstruction of missing content. For example, generating an accurate image in a vision-language task can be challenging with only brief textual descriptions; however, incorporating detailed descriptions of entities and their interactions allows LMM to more accurately reconstruct missing data. Additionally, such prior knowledge aids LMM in ranking candidate completions by identifying the most semantically plausible options. Leveraging LMM’s in-context learning capability \cite{wei2022emergent}, we can non-intrusively embed prior knowledge, such as using the Chain-of-Thought (CoT) approach \cite{wei2022chain}, to enhance the model's understanding of missing modalities.

Based on these insights, we propose a novel, training-free MMC approach, termed ``Knowledge Bridger'', which autonomously mines multimodal knowledge, generates missing modalities, and ranks the best completions. This method comprises three main modules: a knowledge modeling module, a knowledge-driven modality generation module, and a ranking module. In the knowledge modeling stage, we employ LMM to analyze available modalities and extract key elements such as objects, interactions, and attributes using the CoT approach. For specialized fields like medical imaging, we use in-context learning to integrate domain-specific knowledge, reducing reliance on extensive target domain data. Following knowledge extraction, we construct a knowledge graph to represent the relationships and attributes of available and missing modalities, providing LMM with a structured reference for generating missing content. To ensure high-quality completion, the knowledge-driven generation module utilizes this graph to guide LMM in generating specific content with precise details, such as object locations and observable attributes. Finally, the ranking module, informed by expert knowledge, assesses each generated completion by computing graph and representational similarity scores. Specifically, a graph similarity score is derived from comparing the knowledge graphs of the available and generated modalities, while a representational similarity score is calculated using models like BLIP \cite{li2023blip} and CLIP \cite{radford2021learning}. A weighted average of these scores offers the final assessment. 

Our extensive experiments indicate that our method markedly improves MMC performance in both general and OOD scenarios. We also find that our approach scales effectively with LMM, with larger models yielding higher-quality completions. For instance, using OpenAI's GPT-4o \cite{hurst2024gpt} results in a significant performance boost across metrics compared to models with 72B or 7B parameters. The visualization results show that our method outperforms the conditional generation variant in generating missing modalities. Our contributions are summarized as follows:

\begin{itemize}
\item We introduce a training-free pipeline to address missing modality completion, leveraging LMM to automatically extract multimodal knowledge, generate missing modalities, and rank completions. To our knowledge, this is the first work to apply LMM to MMC tasks.
\item We delve into a modality-agnostic unified strategy for both missing modality completion and ranking. This approach allows us to focus on defining domain-specific knowledge without the necessity of intricate fusion methods or implementing a specialized training pipeline. 
\item We present extensive experimental evidence demonstrating that our method facilitates domain transfer, outperforming other MMC methods in both general and OOD scenarios. Additionally, our generated completion data improves the performance of other MMC models.
\end{itemize}
