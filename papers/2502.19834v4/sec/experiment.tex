\section{Experiments}

\subsection{Setup}
%  SMIL \cite{ma2021smil}, ShaSpec \cite{wang2023multi},
\noindent \textbf{Baselines.} To validate the effectiveness of our proposed method, we selected several baseline methods, categorized into two types: imputation-based and non-imputation-based. Imputation-based methods aim to restore missing modalities by learning relationships among the representations within each modality; for this category, we choose MMIN \cite{zhao2021missing} and DiCMoR \cite{wang2023distribution} as baselines. In contrast, non-imputation-based methods bypass the need to restore missing modalities or predict their representations, allowing downstream tasks to be completed without these steps. We choose MPLMM \cite{guo-etal-2024-multimodal} and MPMM \cite{lee2023multimodal} as baselines for this category. Additionally, we introduce a simple baseline that completely removing missing data. This baseline utilizes the pre-trained CLIP model as multimodal backbones. Subsequently, the modality features are concatenated and fed into a classification head (a single-layer MLP) to ultimately obtain the probability for each category.

\noindent \textbf{Dataset.} To evaluate the domain transferability of our method, we employ two general domain datasets and one OOD dataset. The general domain datasets include COCO-2014 \cite{lin2014microsoft} and MM-IMDb \cite{arevalo2017gated}. COCO-2014 \cite{lin2014microsoft} is a large-scale vision-language dataset containing approximately 81$K$ training samples and 41$K$ validation and test samples, with objects classified into 80 categories. In our setup, we treat it as a multimodal multi-label classification dataset, using the validation set as the test set. MM-IMDb \cite{arevalo2017gated} is a large-scale movie genre classification dataset with around 25,000 movies from IMDb, each represented by its movie poster and plot summary, and annotated with 27 multi-label genre tags. In our setup, we divide the dataset into 18,160 samples for the training set and 7,799 samples for the test set. On the other hand, we employ the IU X-ray \cite{demner2016preparing} dataset as our OOD dataset. This dataset includes 6,674 training and 756 testing samples. We further expanded the original 14 observations into 105 finer-grained categories based on location and severity, creating a long-tail dataset.

\noindent \textbf{Evaluation Metrics.} For multimodal multi-label classification on three datasets, we report the performance of comparison methods using the macro F1-score and Average Precision (AP). We report the average result of five different random seeds. Additionally, we introduce the mean similarity score (denoted \textbf{SS}) to evaluate the generation quality of our method and other imputation-based methods. Specifically, we utilize the vision and text backbones of the pre-trained CLIP \cite{radford2021learning} to compute embeddings for all ground-truth and generated missing modalities. The cosine similarity between each pair of embeddings is then computed and averaged, with scores ranging from 0 to 100. A higher score indicates better generation quality.

\noindent \textbf{Implementation Details.} We employ Qwen2-VL-7B \cite{wang2024qwen2} as our default large multimodal model\footnote{More implementation details are presented in the appendix.}. It is specifically designed to handle both visual and textual inputs, enabling it to process and interpret complex multimodal content. For image reconstruction, we apply Stable Diffusion XL (SDXL) 1.0 \cite{podell2023sdxl} as the restoration module for general domains. SDXL 1.0 is an advanced text-to-image diffusion model that can generate images according to a given prompt. Additionally, for the restoration of chest X-ray modality, we use Cheff \cite{weber2023cascaded}, a cascaded chest X-ray latent diffusion model. By default, we generate 5 candidates for the missing modality during the generation process. Since the chosen LMM has already been pre-trained on the COCO dataset, to ensure a fair comparison, we employ pre-trained CLIP \cite{radford2021learning} vision and text encoders to replace the modality backbones of the comparison methods. We conduct all experiments on the PyTorch 2.4.0 platform, running on Ubuntu 20.04 LTS utilizing 4 GPUs (NVIDIA GeForce RTX 4090 with 24 GB of memory). In our setting, we conduct the missing rate $\eta = \{0.3, 0.5, 0.7\}$ to simulate the missing modality scenario during training.


% For tables use
\begin{table*}[ht]
% \renewcommand{\arraystretch}{1.3}
\setlength{\abovecaptionskip}{0cm} 
\setlength{\belowcaptionskip}{-0.2cm} 
\setlength\tabcolsep{2pt}
% table caption is above the table
% For LaTeX tables use
\begin{center}
\scalebox{0.94}{
\begin{tabular}{l|lc|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c}
\toprule
 \multicolumn{2}{c}{} & \multicolumn{9}{c}{COCO-2014 \cite{lin2014microsoft}} & \multicolumn{9}{c}{MM-IMDb \cite{demner2016preparing}} \\
 \cmidrule(lr){3-11} \cmidrule(lr){12-20} 
 \multicolumn{2}{c}{Missing Rate $\eta$}   & \multicolumn{3}{c}{0.3} &  \multicolumn{3}{c}{0.5} & \multicolumn{3}{c}{0.7} & \multicolumn{3}{c}{0.3}  & \multicolumn{3}{c}{0.5} & \multicolumn{3}{c}{0.7}  \\
\cmidrule(lr){3-5} \cmidrule(lr){6-8} \cmidrule(lr){9-11}  \cmidrule(lr){12-14} \cmidrule(lr){15-17} \cmidrule(lr){18-20} 
 \multicolumn{2}{c}{Method} & F1 & AP & SS &  F1  & AP & SS &  F1  & AP & SS &  F1  & AP & SS &  F1  & AP & SS &  F1  & AP & SS  \\
\midrule

\multicolumn{2}{c}{\textcolor{gray}{Baseline (complete)}} & \multicolumn{8}{c}{\textcolor{gray}{F1: 78.3  $|$  AP: 84.6 $|$ SS: -}} & & \multicolumn{8}{c}{\textcolor{gray}{F1: 56.2  $|$  AP: 62.7 $|$ SS: -}} \\
\multicolumn{2}{c}{Baseline (remove missing)}   & 75.8 & 80.8 & - & 73.1 & 79.3 & - & 70.3 & 77.5 & - & 51.8 & 58.5 & - & 50.3 & 56.1 & - & 47.2 & 53.9 & - \\

\midrule
% \multirow{2}{*}{\makecell{Non- \\ imputation}} 

\multicolumn{2}{c}{MPMM \cite{lee2023multimodal} (CVPR'23)}& 76.2 & 82.0 & - & 74.7 & 80.5 & - & 71.0 & 78.8 & - & 53.6 & 59.8 & - & 51.1 & 56.9 & - & 48.5 & 55.7 & - \\
\multicolumn{2}{c}{MPLMM \cite{guo-etal-2024-multimodal} (ACL'24)} & \underline{77.1} & \underline{82.6} & - & \underline{75.2} & \underline{81.3} & - & \underline{72.3} &   \underline{80.1} & - &  \underline{53.9}  & \underline{60.3}  & - &  \underline{52.8} &  \underline{57.3}  & - &  \underline{49.1}  &  \underline{56.2}  & - \\

% \midrule
% \multirow{2}{*}{\makecell[c]{Imputation}} 
\multicolumn{2}{c}{ MMIN \cite{zhao2021missing} (ACL'21)}& 73.2 & 78.3 & \underline{37.1} & 71.4 & 77.5 & \underline{36.7} & 70.5 & 76.4 & \underline{36.0} & 50.1 & 53.8 & 26.7 & 49.5 & 51.7 & \underline{26.3} & 44.6 & 50.8 & \underline{24.4} \\
\multicolumn{2}{c}{DiCMoR \cite{wang2023distribution} (CVPR'23)}& 65.3 & 74.4 & 34.3 & 59.7 & 67.1 & 33.5 & 55.3 & 64.0 & 31.9 & 49.2 & 54.7 & \underline{26.9} & 43.7 & 50.8 & 25.9 & 30.5 & 41.7 & 23.1 \\


\midrule

\multicolumn{2}{c}{Ours (Qwen-VL-2B)} & 76.1 & 81.8 & 39.8 & 74.3 & 79.9 & 37.1 & 70.7 & 78.2 & 36.3 & 52.4 & 58.9 & 28.8 & 51.9 & 57.1 & 28.3 & 50.3 & 56.5 & 28.1 \\
\multicolumn{2}{c}{Ours (Qwen-VL-7B)} & \textbf{77.5} & \textbf{82.9} & \textbf{40.4} & \textbf{77.5} & \textbf{82.8} & \textbf{40.2} & \textbf{77.9} & \textbf{83.5} & \textbf{38.2} & \textbf{54.7} & \textbf{60.9} & \textbf{33.5} & \textbf{54.9} & \textbf{61.3} & \textbf{32.7} & \textbf{55.2} & \textbf{61.8} & \textbf{32.3} \\

\midrule
% array([-2.8, -1.7, 40.4, -0.8, -1.8, 40.4, -0.4, -1.1, 38.5])
% array([-1.5, -1.8, 33.5, -1.3, -1.4, 32.7, -1. , -0.9, 32.3])
\multicolumn{2}{c}{\textcolor{gray}{$\Delta$ Complete Baseline}} & \color{gray} -2.8 & \color{gray} -1.7 & -  & \color{gray} -0.8 & \color{gray} -1.8 & - & \color{gray} -0.4 & \color{gray} -1.1 & - & \color{gray} -1.5 & \color{gray} -1.8 & - & \color{gray} -1.3 & \color{gray} -1.4 & - & \color{gray} -1.0 & \color{gray} -0.9 & - \\

% array([0.4, 0.3, 3.3, 2.3, 1.5, 3.7, 5.6, 3.4, 2.5])
\multicolumn{2}{c}{$\Delta$ SOTA} & \color{red} +0.4 & \color{red} +0.3 & \color{red} +3.3 & \color{red} +2.3 & \color{red} +1.5 & \color{red} +3.7 & \color{red} +5.6 & \color{red} +3.4 & \color{red} +2.5 & \color{red} +0.8 & \color{red} +0.6 & \color{red} +6.6 & \color{red} +2.1 & \color{red} +4.0 & \color{red} +6.4 & \color{red} +6.1 & \color{red} +5.6 & \color{red} +7.9 \\

% $\Delta$ SOTA  & \textcolor{commentcolor}{$+$4.16} & \textcolor{commentcolor}{$+$4.25} & \textcolor{commentcolor}{$+$4.44} & \textcolor{commentcolor}{$+$4.12} & \textcolor{commentcolor}{$+$4.18} & \textcolor{commentcolor}{$+$5.2}\\

\bottomrule
\end{tabular}
}
\end{center}
\caption{\textbf{Quantitative analysis results (\%) on COCO-2014 and MM-IMDb datasets.} \textbf{Bold} denotes the best results and \underline{underline} denotes the second-best. SS (\%) refers to the average similarity score, which is used to assess the generation quality of imputation-based methods. A higher score indicates better quality. `-' indicates that the metric is not applicable. All results are reproduced using the officially released code.}
\label{tab:quantitative-result}       % Give a unique label
% \vspace{-10pt}
\end{table*}




% For tables use
\begin{table}[h]
% \renewcommand{\arraystretch}{1.3}
\setlength{\abovecaptionskip}{0cm} 
\setlength{\belowcaptionskip}{-0.2cm} 
\setlength\tabcolsep{2pt}
% table caption is above the table
% For LaTeX tables use
\begin{center}
\scalebox{0.9}{
\begin{tabular}{lc|c|c|c|c|c}
\toprule

Missing Rate $\eta$ & \multicolumn{3}{c}{0.3} & \multicolumn{3}{c}{0.7}\\
\cmidrule(lr){2-4} \cmidrule(lr){5-7}
Method & F1 & AP & SS & F1 & AP & SS \\
\midrule
\textcolor{gray}{Baseline (complete)} & \multicolumn{6}{c}{\textcolor{gray}{F1: 57.0 $|$ AP: 75.7$|$ SS: -}} \\
Baseline (remove missing) & 49.1 & 71.4 & -  & 31.5 & 56.2 & - \\
\midrule
MPMM \cite{lee2023multimodal} (CVPR'23) & \underline{49.9} & 71.8 & - & \underline{36.8} & 61.4 & - \\
MPLMM \cite{guo-etal-2024-multimodal} (ACL'24) & 49.3 & \underline{72.7} & - & 35.2 & \underline{61.9} & -\\
% \midrule
MMIN \cite{zhao2021missing} (ACL'21) & 37.3 & 64.2 & 17.3 & 26.7 & 50.1 & 10.2  \\
DiCMoR \cite{wang2023distribution} (CVPR'23) & 40.5 & 69.1 & \underline{18.1} & 29.8 & 53.6 & \underline{13.3} \\
\midrule
Ours (Qwen-VL-2B)  & 51.4 & 72.0 & 21.9 & 41.1 & 68.9 & 17.7 \\
Ours (Qwen-VL-7B)  & \textbf{53.6} & \textbf{73.9} & \textbf{22.6} & \textbf{46.3} & \textbf{70.5} & \textbf{19.8} \\
\midrule
\textcolor{gray}{$\Delta$ Complete Baseline} &  \color{gray} -3.4 & \color{gray} -1.8 & \color{gray} - & \color{gray} -10.7 & \color{gray} -5.2 & \color{gray} - \\
$\Delta$ SOTA & \color{red} +3.7 & \color{red} +1.2 & \color{red} +4.5 & \color{red} +9.5 & \color{red} +8.6 & \color{red} +6.5 \\
\bottomrule
\end{tabular}}
\end{center}
\caption{\textbf{Quantitative results (\%) on IU X-ray datasets.} \textbf{Bold} denotes the best results and \underline{underline} denotes the second-best. SS (\%) refers to the average similarity score, which is used to assess the generation quality of imputation-based methods. A higher score indicates better quality. `-' indicates that the metric is not applicable. All results are reproduced using the officially released code.}
\label{tab:classification-result}       % Give a unique label
\vspace{-10pt}
\end{table}


\subsection{Quantitative Analysis}

In Tables \ref{tab:quantitative-result} and \ref{tab:classification-result}, we present the results of our method compared to others across different missing data ratios, as measured by F1, AP, and mean similarity score (SS) on three datasets \footnote{More results (more modalities) are presented in the appendix.}. To comprehensively evaluate the performance of various approaches, we introduce two baselines: one trained on complete data and another trained on data with missing entries removed. Additionally, to fair comparison regarding the scale of model parameters, the proposed method based on the Qwen-VL-2B were incorporated.

Results from general domain datasets (COCO-2014 and MM-IMDb) indicate that most MMC methods outperform the baseline where missing data is simply removed. Our method demonstrates superior performance across different missing rates, particularly at a missing rate of 0.7, where it shows significant improvements in both F1 and AP metrics. We attribute these enhancements to the synthetic data generated by our method, which appears to bolster downstream task performance. This phenomenon is also observed in \cite{gu2024infinity, wang2023see}. However, it should be noted that the synthetic data may lead to a slight decline in mean similarity score.

On the other hand, results from the OOD dataset highlight our method's superior domain adaptation capability compared to other methods. Our approach not only excels in classification metrics but also significantly surpasses other imputation-based methods, such as MMIN and DiCMoR, in missing data generation metrics (SS). This advantage can be credited to our method's ability to harness the few-shot learning and in-context learning capabilities of LMM, effectively mitigating the impact of varying degrees of data incompleteness.


\subsection{Ablation Study}
% 1. different model.
% 2. different scale.
% 3. different missing modality.
% 4. Qualitative analysis.

\noindent \textbf{Q1: Does `knowledge' really help MMC?} We study the impact of different components under general and OOD scenarios with the same missing rate.  As shown in Table \ref{tab:ab-study-knowledge}, we report the results of various components. For the generation process, we analyze the variant without knowledge modeling\footnote{We directly use LMM to generate descriptions of available modalities.} (row 1) and the variant with random ranking (row 2). Additionally, in examining the ranking module, we assess the variant with random ranking (row 3), the variant without knowledge graph ranking (row 4), and the variant without using semantic similarity score ranking (row 5). The results demonstrate that the variant without knowledge modeling shows a significant decline across all metrics under OOD scenarios. Furthermore, the semantic similarity ranking strategy in the ranking module is crucial for the effectiveness of the MMC. In general, the study reveals that knowledge modeling is the most critical component of our method.
% For tables use
\begin{table}[h]
% \renewcommand{\arraystretch}{1.3}
\setlength{\abovecaptionskip}{0cm} 
\setlength{\belowcaptionskip}{-0.2cm} 
\setlength\tabcolsep{2pt}
% table caption is above the table
% For LaTeX tables use
\begin{center}
\scalebox{0.85}{
\begin{tabular}{cllc|c|c|c|c|c}
\toprule
\multicolumn{3}{c}{} & \multicolumn{3}{c}{MM-IMDb} & \multicolumn{3}{c}{IU X-ray} \\

\multicolumn{3}{c}{Missing Rate $\eta$} & \multicolumn{3}{c}{0.7} & \multicolumn{3}{c}{0.7}\\
\cmidrule(lr){4-6} \cmidrule(lr){7-9}
\multicolumn{3}{c}{Variants} & F1 & AP & SS & F1 & AP & SS \\
\midrule
0 & \multicolumn{2}{c}{Baseline (Qwen-VL-7B)} & 55.2 & 61.8 & 32.3 & 46.3 & 70.5 & 19.8 \\

\midrule
1 & \multicolumn{2}{c}{ \textit{w/o} Knowledge Modeling} & -1.3 & -3.6 & -8.8 & \color{red}-17.5 & \color{red}-29.2 & \color{red}-13.7 \\
2 & \multicolumn{2}{c}{+ Random Ranking} & -1.6 & -4.1 & -9.9 & \color{red}-19.3 & \color{red}-31.8 & \color{red}-15.0 \\
\midrule
3 & \multicolumn{2}{c}{Random Ranking} & -0.5 & -2.7 & -0.6 & -3.8 & -7.1 & -4.7 \\
4 & \multicolumn{2}{c}{\textit{w/o} Knowledge Ranking} & -0.2 & -0.8 & -0.1 & -1.9 & -2.7 & -1.1  \\
5 & \multicolumn{2}{c}{\textit{w/o} Semantic Ranking} & -0.2 & -1.0 & -0.3 & -2.4 & -3.3 & -1.6 \\
\bottomrule
\end{tabular}}
\end{center}
\caption{\textbf{The impact of various components.} We report the comparison results between different combinations and the baseline.}
\label{tab:ab-study-knowledge}       % Give a unique label
% \vspace{-10pt}
\end{table}



\noindent \textbf{Q2: Does the scale of model parameters affect MMC?} We evaluate the performance of our approach using different model scales under at various missing rates. Specifically, we conduct the 2B, 7B, and 72B variants of the open-source large model Qwen-VL \cite{wang2024qwen2}, as well as OpenAI's GPT-4o \cite{hurst2024gpt} on the IU X-ray dataset. For a fair comparison, we leverage these models solely for handling the knowledge modeling and integration in steps 1 and 2 of our method, keeping the modality generators unchanged. As illustrated in Fig. \ref{fig:ab-model-scale}, GPT-4o demonstrates significant superiority and robustness. These results demonstrate that as the scale of model parameters increases, the quality of knowledge modeling in our approach also improves.


% \begin{figure}[t]
%     \centering
%     % \fbox{\rule{0pt}{1in} \rule{.9\linewidth}{0pt}}
%     \includegraphics[width=0.90\linewidth]{pic/ab-scale.pdf}
%     \caption{The impact of model various scales and missing rates.}
%     \label{fig:ab-model-scale}
% \vspace{-10pt}
% \end{figure}


\begin{figure}[h]
    \centering
    \begin{subfigure}[b]{0.41\textwidth}
        \centering
        \includegraphics[width=\textwidth]{pic/different_missing_rate_f1.pdf}
        \caption{F1 score at different missing rates.}
        \label{fig:ab-model-scale-a}
    \end{subfigure}
    \begin{subfigure}[b]{0.4\textwidth}
        \centering
        \includegraphics[width=\textwidth]{pic/different_missing_rate_ss.pdf}
        \caption{Average similarity score at different missing rates. }
        \label{fig:ab-model-scale-b}
    \end{subfigure}
    \caption{The impact of different model's parameters scales and missing rates.}
    \label{fig:ab-model-scale}
\vspace{-10pt}
\end{figure}





\noindent \textbf{Q3: Does generating more missing data improve performance?} We evaluate the impact of different numbers of generation candidates in step 2 of our method on the MM-IMDb and IU X-ray datasets. Specifically, we conduct the settings with generation candidates \( n = \{1, 5, 10, 15\} \), and the results are presented in Table \ref{tab:ab-study-generation}. The findings indicate that generating only one candidate significantly reduces the missing generation quality. While using a higher number of generation candidates can lead to slight improvements across all metrics, it also substantially increases inference time. Therefore, setting \( n = 5 \) provides a balance between inference time and performance.


% For tables use
\begin{table}[t]
% \renewcommand{\arraystretch}{1.3}
\setlength{\abovecaptionskip}{0cm} 
\setlength{\belowcaptionskip}{-0.2cm} 
\setlength\tabcolsep{2pt}
% table caption is above the table
% For LaTeX tables use
\begin{center}
\scalebox{0.9}{
\begin{tabular}{lcc|c|c|c|c|c}
\toprule

 & & \multicolumn{3}{c}{MM-IMDb} & \multicolumn{3}{c}{IU X-ray} \\

Missing Rate $\eta$ &  & \multicolumn{3}{c}{0.7} & \multicolumn{3}{c}{0.7}\\
\cmidrule(lr){3-5} \cmidrule(lr){6-8}
Candidates $n$ & time & F1 & AP & SS & F1 & AP & SS \\
\midrule
\color{gray}$n = 5$ (default) & 40s &  \color{gray} 55.2 & \color{gray} 61.8 & \color{gray} 32.3 & \color{gray} 46.3 & \color{gray} 70.5 & \color{gray} 19.8 \\
\midrule
$n = 1$ & 7s &  -0.7 & -2.8 & -7.3 & -8.5 & -16.4 & -14.7 \\
$n = 10$ & 83s &  +0.4 & +0.9 & +0.1 & +0.0 & +0.1 & +0.0 \\
$n = 15$ & 132s & +0.4 & +0.9 & +0.1 & +0.1 & +1.3 & +0.3 \\
\bottomrule
\end{tabular}}
\end{center}
\caption{The impact of different generation candidates.}
\label{tab:ab-study-generation}       % Give a unique label
\vspace{-10pt}
\end{table}

% \noindent \textbf{Q4: What is the impact of different missing scenarios?}
% ratio and type

% For tables use
% \begin{table}[h]
% % \renewcommand{\arraystretch}{1.3}
% \setlength{\abovecaptionskip}{0cm} 
% \setlength{\belowcaptionskip}{-0.2cm} 
% \setlength\tabcolsep{2pt}
% % table caption is above the table
% % For LaTeX tables use
% \begin{center}
% \scalebox{0.9}{
% \begin{tabular}{ccc|c|c|c|c|c}
% \toprule

%  & & \multicolumn{3}{c}{MM-IMDb} & \multicolumn{3}{c}{IU X-ray}\\
% \cmidrule(lr){3-5} \cmidrule(lr){6-8}
% Rate $\eta$ & Type & F1 & AP & SS & F1 & AP & SS \\
% \midrule

% \multirow{3}{*}{0.3} 
% & Text & \\
% & Image & \\
% & Mixed & 54.7 & 60.9 & 33.5 & 53.6 & 73.9 & 22.6 \\

% \midrule

% \multirow{3}{*}{0.7} 
% & Text & \\
% & Image & \\
% & Mixed &  55.2 & 61.8 & 32.3 & 46.3 & 70.5 & 19.8 \\

% \bottomrule
% \end{tabular}}
% \end{center}
% \caption{Missing rate and type}
% \label{tab:ab-missing-type}       % Give a unique label
% % \vspace{-10pt}
% \end{table}



\subsection{Visualization Analysis}
To better understanding the differences between our approach and conditional generation, we present completion results produced by our method, as shown in Fig. \ref{fig:vis-1}. We provide visualization results across different missing modalities for three datasets\footnote{More visualization are presented in appendix.}. In the general domain, our knowledge modeling module focuses on understanding the quantity of objects, their attributes, and the contextual environment. The results in Fig. \ref{fig:vis-1}a and Fig. \ref{fig:vis-1}b indicate that our method is more similar with the original missing modality compared to direct generation approaches. In the medical domain, incorporating knowledge of different lesions enables the LMM to understand the relationships between various regions in chest X-rays and the content described by the modality. Fig. \ref{fig:vis-1}c and Fig. \ref{fig:vis-1}d show that our method provides a more reliable strategy for missing data completion than direct generation.

\begin{figure*}[ht]
  \centering
  \includegraphics[width=\linewidth]{pic/vis-1.pdf}
  \caption{\textbf{Visualization analysis.} We present the results of multi-modality completion across different datasets. These results include visualizations related to the number of objects and specific people cases in general domain, as well as lesion completion in the medical domain. The green box represents the part that is close to the ground-truth, and the red box represents the wrong part.}
  \label{fig:vis-1}
\vspace{-10pt}
\end{figure*}


