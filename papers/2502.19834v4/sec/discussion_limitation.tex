\section{Discussion and Limitations}


\noindent \textbf{Why we need the training-free MMC?} Previous approaches for handling missing modalities have demonstrated limited generalizability across different domains and constrained data flexibility. For example, recent prompt-based methods such as MPMM \cite{lee2023multimodal} and MPLMM \cite{guo-etal-2024-multimodal} have shown significantly lower performance in general domains compared to their effectiveness in the medical domain, as reflected in our experimental results. Additionally, these methods depend on pre-trained models that are highly tailored to the target domain. In data-hungry scenarios, such as the medical field where data often contain missing elements, these constraints greatly impact the reliability of model decisions. In recent years, LMMs have exhibited remarkable zero-shot capabilities across various domains. Consequently, leveraging these models in a training-free manner to address the MMC problem presents a promising and cost-effective research direction. Our experimental results corroborate this perspective, and we hope these findings will encourage the community to further explore the potential of LMMs for addressing MMC challenges.




\noindent \textbf{Why `knowledge bridger' can help MMC?} We believe that the key to addressing the MMC problem using LMM lies in accurately generating missing data and effectively ranking the generation candidates. Our experimental results show that directly employing LMM to generate the missing modality does not ensure the accuracy required for MMC. Our proposed approach first involves mining internal knowledge from the available modalities, enabling LMM to understand intra-modal interactions. This structured knowledge is then used to address challenges in both generation and ranking. However, using LMM inherently leads to hallucinations. In future work, we plan to employ more robust knowledge extraction methods, such as retrieval-augmented generation \cite{guo2024lightrag}, to mitigate hallucinations\footnote{Potential hallucinations of our method are presented in appendix.} and further enhance the accuracy of missing data generation.


\noindent \textbf{Limitations.} Our method focuses exclusively on image and text modalities, leaving its performance on other modalities, such as speech and depth, yet to be explored. The approach emphasizes the automatic extraction of inter-modal knowledge and the completion of missing modalities through domain knowledge. Thus, in the future, adaptation to other modalities is possible by defining a more comprehensive modality knowledge. Some promising works \cite{girdhar2023imagebind, lyu2024unibind} show that one modality, such as image or text, can be connected to any other modality. Additionally, we observe that while our method enhances classification performance under a high missing rate (e.g., 0.7), it paradoxically results in a decrease in the similarity scores of the completed modalities. Therefore, there remains substantial potential for further exploration to develop more robust generation and ranking strategies in the future.
