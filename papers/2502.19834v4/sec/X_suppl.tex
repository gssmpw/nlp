\newpage
\setcounter{page}{1}
\maketitlesupplementary

\renewcommand{\thesection}{\Alph{section}}
\renewcommand{\contentsname}{\centering \Large\bfseries Table of Contents}
\newenvironment{revcomment}[1][]{\refstepcounter{revcomment}
	\begin{tcolorbox}[adjusted title={}, fonttitle={\bfseries}, colback={colorcommentbg}, colframe={colorcommentframe},coltitle={colorcommentbg},#1]
	}{\end{tcolorbox}}
\newcounter{revcomment}
\setcounter{revcomment}{0}

% \noindent{\large\textit{Knowledge Bridger: Towards Training-Free Missing Modality Completion }} 
% \begin{center}
% {\large\textit{-- -- (ID: 4149) Supplementary Materials -- --}}
% \end{center}

%\tableofcontents
%\newpage


%\twocolumn

\section{Prior Knowledge Rules and Prompts}

We report the knowledge extraction rules used in our method and the prompts used in the knowledge-driven generation respectively.

\subsection{Prior Knowledge}

To simplify the research problem, we utilize only a set of basic instructions to direct the large language model (LMM) focus toward the desired multimodal content extraction. For the general domain, we predefine the extracted knowledge to include major objects, the quantity of each object, and their corresponding attributes and styles. Given the strong reasoning capabilities of LMM in the general domain, it is not necessary to differentiate between input modalities. However, to mitigate hallucinations generated by the LMM, we limit the number of major objects. In our experiments, extracting between 5 to 7 objects show optimal.

\begin{revcomment}[title=General Domain Rules]
Understand the given \textit{[input-format]} to extract the following information: \\
- Identify the top \textit{[object-numbers]} objects by their specific names (e.g., `man', `woman' instead of `person').\\
- Specify the count of each identified object.\\
- Describe attributes for each object in detail.\\
- Summarize the style of the \textit{[input-format]}.
\end{revcomment}

In the medical domain, it is necessary to pay attention to the distinctions between different modalities. This is because LMM may not inherently comprehend the knowledge required for out-of-domain (OOD) scenarios, necessitating clear identification of the modality being processed and the content to be understood. In our approach, for X-rays, we instruct the LMM to focus on anatomical structures, clinical significance, abnormal findings, and report generation. 

\begin{revcomment}[title=Medical Domain Rules (X-ray)]
This is a chest X-ray image. Please follow these steps for a comprehensive analysis: \\ 
- Describe the main anatomical structures visible in the image, such as the lungs, heart, and trachea. \\ 
- Identify any abnormalities present, such as opacities, nodules, or effusions, and describe their characteristics. \\ 
- Explain the potential clinical significance of any abnormalities noted. \\
- Summarize the findings and draft a detailed clinical report based on your observations. 
\end{revcomment}

For reports, in addition to the information extracted from X-rays, we direct the LMM to consider the locations of the anatomical structures mentioned and any additional characteristic details present in the report.

\begin{revcomment}[title=Medical Domain Rules (Report)]
Given the following clinical report, analyze and identify specific visual details that would correspond to the described findings on a chest X-ray. Follow these steps: \\
- Identify the main anatomical structures mentioned in the report and locate them on a chest X-ray. \\
- Highlight the abnormalities or specific findings described in the report. \\
- Describe the characteristics (e.g., size, shape, density) of these abnormalities. \\
- Relate these characteristics to potential clinical conditions. \\
- Summarize your analysis with a list of visual features expected in the X-ray.

\end{revcomment}

\subsection{Knowledge Extraction with Chain-of-Thought}

We employ an LMM with Chain-of-Thought (CoT) \cite{wei2022chain} reasoning to extract knowledge according to the aforementioned rules. This approach helps reduce the computational strain associated with long-problem reasoning and enhances the overall accuracy of problem-solving. For the general domain, our final instruction prompt is designed as follows:

\begin{revcomment}[title=General Domain Knowledge Extraction]
Role: SYSTEM \\
Content: You are a helpful assistant in understanding images and texts and you can extract very important and accurate information from them. \\
\\
Role: USER \\
Content: \\
\# Instruction \\
Your task is to understand the user's inputs and extract the related information following the instruction: \\ 
\{ RULES \} \\

\{ User Input \} \\
Please process each point step by step. \\

Role: ASSISTANT \\
Content: ... \\
\end{revcomment}

For medical domain, we have:

\begin{revcomment}[title=Medical Domain Knowledge Extraction]
Role: SYSTEM \\
Content: You are a very experienced radiologist. \\
\\
Role: USER \\
Content: \\
\# Instruction \\
The following are some chest x-ray image and report examples. Your task is to understand the images and reports, and extract the important information based on the following questions. \\
\# Examples \\
Example 1: \\
Chest X-ray Image: \textit{[Image 1]}. \\
Clinical report: \textit{[Report 1]}. \\

Example 2: \\
Chest X-ray Image: \textit{[Image 2]}. \\
Clinical report: \textit{[Report 2]}. \\

\# Query \\
\{ RULES \} \\

\{ User Input \} \\
Please process each point step by step. \\

Role: ASSISTANT \\
Content: ... \\
\end{revcomment}

Next, we require the LMM to integrate the CoT results according to the following instructions:

\begin{revcomment}[title=Integrating the CoT Results]

Role: USER \\
Content: \\
\# Instruction \\
- You have to integrate the previous result into a structure format. \\
- Use precise nouns and avoid general terms; each object should be accurately named. \\

\# Return Format \\
The output must be in JSON format as follows: \\
\{ return-format \} \\

Role: ASSISTANT \\
Content: ... \\

\end{revcomment}

We strictly require the LMM to return structured information in the following format:

\begin{revcomment}[title=General Domain Return Format]
\{\\
    ``objects'': [``Obj. 1'', ``Obj. 2'', ...], \\
    ``numbers'': \{ \\
        ``Obj. 1'': 2, \\
        ``Obj. 2'': 1, \\
       ... \\
    \}, \\
     ``attributes'': \{ \\
        ``Obj. 1": ``Description of attributes here.", \\
       ... \\
    \}, \\
     ``style'': ``Description of style here.'' \\
\}

\end{revcomment}

\begin{revcomment}[title=Medical Domain Return Format]
\# Structured Analysis \\
1. **Anatomical Structures**: \\
   - Lungs: [Left Upper Lobe: Normal/Abnormal], [Right Lower Lobe: Normal/Abnormal] \\
   - Heart: [Normal/Abnormal] \\
   - Trachea: [Normal/Abnormal] \\

2. **Type of Abnormality**: \\
   - Identified Abnormality: [e.g., opacity, nodule, effusion] \\
   - Characteristics: [e.g., size: 2 cm, shape: round, border: well-defined/ill-defined, density: high] \\

3. **Distribution and Location**: \\
   - Side: [Unilateral/Bilateral] \\
   - Location: [Upper/Lower/Middle lobe] \\
   - Extent: [Localized/Diffuse] \\

4. **Clinical Implication**: \\
   - Possible Diagnosis: ['No Finding', 'Enlarged Cardiomediastinum', 'Cardiomegaly', \\
    'Lung Opacity', 'Lung Lesion', 'Edema', 'Consolidation',  \\
    'Pneumonia', 'Atelectasis', 'Pneumothorax', 'Pleural Effusion', \\
    'Pleural Other', 'Fracture', 'Support Devices'] \\
   - Recommended Action: [Further imaging, clinical follow-up, etc.] \\
\end{revcomment}

After extracting the aforementioned structured information, we employ LMM to transform these knowledge into the form of a knowledge graph. To simplify the process, we represent relationships on the graph using a triplet structure (nodes and edges):

\begin{revcomment}[title=Building Knowledge Graphs]
\# Instruction \\
Your task is to analyze the provided \textit{[input-type]} and extract **exactly \textit{[numbers-of-relationships]} distinct relationships** to build a knowledge graph. Each relationship should be structured as (Head, Relation, Tail), focusing on **clear, direct relations** (e.g., "causes," "is a part of," "describes," etc.).  \\
\{ User Input \} \\
\# Return Format \\
The output must be in JSON format as follows: \\

[\\
    \{\\
        "head": ...,  \\
        "relation": ...,\\
        "tail": ...\\
    \},\\
    ...\\
] \\
Please process each point step by step. 
\end{revcomment}


\subsection{Knowledge-driven Generation}

We employ the knowledge graphs extracted by the aforementioned process as input and employ the LMM to process this information to generate meaningful descriptions of missing modalities. Various modality generators are then utilized to produce the missing information based on these descriptions. For missing image, predictions are based on observable text:

\begin{revcomment}[title=General Domain Image Generation]
\# Instruction \\
- Expand the basic sentence to \textit{[num-prompts]} high-quality description based on previous analysis and structured data. \\
- Each new prompt should emphasize different object attributes or scene details. \\
- **Basic Sentence**: \textit{[text-content]} \\
\textit{[Knowledge Graphs]} \\
\# Output Format \\
Output the prompt format must in JSON: \\

[ \\
    "description 1", \\
    ...., \\
    "description K" \\
] \\
\end{revcomment}

\begin{revcomment}[title=Medical Domain Image Generation]
\# Instruction \\
Using the following structured analysis, this information is organized to generate \textit{[num-prompts]} meaningful clinical description: \\

\textit{[Knowledge Graphs]} \\

\{ User Input \} \\

\# Output Format \\
Output the prompt format must in JSON: \\

[ \\
    "description 1", \\
    ...., \\
    "description K" \\
] \\

\end{revcomment}

For missing text, the same method is applied to generate descriptions of the missing content, which are then refined by the LMM to produce the required missing text.

\subsection{Knowledge-based Ranking Pseudo-code}

The pseudo-code for the ranking process is shown in Alg. \ref{algo:ranking}.
\begin{algorithm}[h]
\SetAlgoLined
    \PyComment{$f_a(\cdot)$, $f_c(\cdot)$, $f_b(\cdot)$: the adjacency matrix, CLIP’s embedding, and BLIP’s embedding of the given modality, respectively.} \\
    \PyComment{$cos_{graph}(\cdot, \cdot)$, $cos(\cdot, \cdot)$: Graph similarity and embedding similarity.} \\
    \PyComment{$C$: Missing generation candidates.} \\
    \PyComment{$A$: Available modality.} \\
    \texttt{\\}
    \PyComment{Quality Scores} \\
    QS = [] \\
    \PyKey{for} c \PyKey{in} $C$: \PyComment{load a candidate.} \\
    \Indp   % start indent
%        x1, x2 = aug(x), aug(x) \PyComment{random augmentation.} \\ 
    \PyComment{Computing the graph similarity} \\
    graph\_simi =  $cos_{graph}(f_a(A), f_a(c))$ \\    
    \PyComment{Computing the embedding similarity by CLIP} \\
    clip\_simi =  $cos(f_c(A), f_c(c))$ \\
    \PyComment{Computing the embedding similarity by BLIP} \\
    blip\_simi =  $cos(f_b(A), f_b(c))$ \\ 
    score = graph\_simi + clip\_simi + blip\_simi \\
    QS.append(score) \\
\Indm
\PyComment{Ranking.} \\
max\_c = QS.index(max(QS)) \\

return $C$[max\_c]

    \texttt{\\}
\caption{Ranking Module. python-style pseudocode}
\label{algo:ranking}
\end{algorithm}


\section{Implementation Details}

\noindent \textbf{GPUs Details.} We conduct all experiments on the PyTorch 2.4.0 \cite{paszke2019pytorch} platform, running on Ubuntu 20.04 LTS utilizing 4 GPUs (NVIDIA GeForce RTX 4090 with 24 GB of memory).

\noindent \textbf{Deploy Efficient Large Multimodal Model.} We deploy the Qwen-VL\cite{wang2024qwen2} large model using vLLM \cite{kwon2023efficient}. vLLM is an LLM serving system that achieves (1) near-zero waste in KV cache memory and (2) flexible sharing of KV cache within and across requests to further minimize memory usage. We deploy versions with 2B\footnote{https://huggingface.co/Qwen/Qwen2-VL-2B-Instruct}, 7B\footnote{https://huggingface.co/Qwen/Qwen2-VL-7B-Instruct}, and 72B parameters. Specifically, due to hardware constraints, we utilize the 72B quantized version with Int-8 precision available on Hugging Face: Qwen2-VL-72B-Instruct-GPTQ-Int8\footnote{https://huggingface.co/Qwen/Qwen2-VL-72B-Instruct-GPTQ-Int8}. For all versions, we maintain an 8K context window length and support a maximum of four image queries. For each query, we set the maximum number of tokens to 512 and use a temperature of 0.1.

\noindent \textbf{Generators Settings.} For image reconstruction, we apply Stable Diffusion XL (SDXL) 1.0 \cite{podell2023sdxl} as the restoration module for general domains. SDXL 1.0 is an advanced text-to-image diffusion model that can generate images according to a given prompt. Additionally, for the restoration of chest X-ray modality, we use Cheff \cite{weber2023cascaded}, a cascaded chest X-ray latent diffusion model. By default, we generate 5 candidates for the missing modality during the generation process. 

\noindent \textbf{Missing Modality Simulation} In our setting, we conduct the missing rate $\eta = \{0.3, 0.5, 0.7\}$ to simulate the missing modality scenario during training. Specifically, we calculate the number of missing samples in different datasets under a given missing rate and then randomly mark the text and image modalities of these samples as missing with a probability of 0.5. To ensure the reproducibility of the experiment, we perform multiple simulations using the same set of missing samples. Finally, we retrain the baseline model, which was initially trained on complete modalities, using the data with imputed missing modality and report the performance across various metrics.



\section{More Experimental Results}

\subsection{Table of Quantitative Results}

We present additional quantitative analysis results, as shown in Table \ref{tab:classification-result-1}.

% For tables use
\begin{table}[h]
% \renewcommand{\arraystretch}{1.3}
% \setlength{\abovecaptionskip}{0cm} 
% \setlength{\belowcaptionskip}{-0.2cm} 
% \setlength\tabcolsep{2pt}
% table caption is above the table
% For LaTeX tables use
\begin{center}
\scalebox{0.9}{
\begin{tabular}{lc|c|c}
\toprule

Missing Rate $\eta$ & \multicolumn{3}{c}{0.5} \\
\cmidrule(lr){2-4} 
Method & F1 & AP & SS  \\
\midrule
\textcolor{gray}{Baseline (complete)} & \multicolumn{3}{c}{\textcolor{gray}{F1: 57.0 $|$ AP: 75.7$|$ SS: -}} \\
Baseline (remove missing) & 42.1 & 63.9 & - \\
\midrule
MPMM \cite{lee2023multimodal} (CVPR'23) & 42.8 & 64.2 & -  \\
MPLMM \cite{guo-etal-2024-multimodal} (ACL'24) & \underline{43.3} & \underline{65.9} & - \\
% \midrule
MMIN \cite{zhao2021missing} (ACL'21) & 34.8 & 60.1 & 17.0   \\
DiCMoR \cite{wang2023distribution} (CVPR'23) & 37.6 & 63.8 & \underline{17.7} \\
\midrule
Ours (Qwen-VL-2B)  & 46.6 & 67.1 & 21.1  \\
Ours (Qwen-VL-7B)  & \textbf{50.9} & \textbf{70.2} & \textbf{21.5}  \\
\midrule
\textcolor{gray}{$\Delta$ Complete Baseline} &  \color{gray} -6.1 & \color{gray} -5.5 & \color{gray} -  \\
$\Delta$ SOTA & \color{red} +7.6 & \color{red} +4.3 & \color{red} +3.8  \\
\bottomrule
\end{tabular}}
\end{center}
\caption{\textbf{Quantitative results (\%) on IU X-ray datasets.} \textbf{Bold} denotes the best results and \underline{underline} denotes the second-best. SS (\%) refers to the average similarity score, which is used to assess the generation quality of imputation-based methods. A higher score indicates better quality. `-' indicates that the metric is not applicable. All results are reproduced using the officially released code.}
\label{tab:classification-result-1}       % Give a unique label
\end{table}

\subsection{Table of Ablation Study}

We present complete results of the ablation study in Table \ref{tab:ab-result}.

\begin{table*}[ht]
% \renewcommand{\arraystretch}{1.3}
\setlength{\abovecaptionskip}{0cm} 
\setlength{\belowcaptionskip}{-0.2cm} 
\setlength\tabcolsep{2pt}
% table caption is above the table
% For LaTeX tables use
\begin{center}
\scalebox{0.9}{
\begin{tabular}{l|lc|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c}
\toprule
 \multicolumn{2}{c}{} & \multicolumn{9}{c}{MM-IMDb} & \multicolumn{9}{c}{IU X-ray} \\
 \cmidrule(lr){3-11} \cmidrule(lr){12-20} 
 \multicolumn{2}{c}{Missing Rate $\eta$}   & \multicolumn{3}{c}{0.3} &  \multicolumn{3}{c}{0.5} & \multicolumn{3}{c}{0.7} & \multicolumn{3}{c}{0.3}  & \multicolumn{3}{c}{0.5} & \multicolumn{3}{c}{0.7}  \\
\cmidrule(lr){3-5} \cmidrule(lr){6-8} \cmidrule(lr){9-11}  \cmidrule(lr){12-14} \cmidrule(lr){15-17} \cmidrule(lr){18-20} 
 \multicolumn{2}{c}{Variants} & F1 & AP & SS &  F1  & AP & SS &  F1  & AP & SS &  F1  & AP & SS &  F1  & AP & SS &  F1  & AP & SS  \\
\midrule


\multicolumn{2}{c}{Baseline (Qwen-VL-7B)}  & 54.7 & 60.9 & 33.5 & 54.9 & 61.3 & 32.7 & 55.2 & 61.8 & 32.3 & 53.6 & 73.9 & 22.6 & 50.9 & 70.2 & 21.5 & 46.3 & 70.5 & 19.8 \\

\midrule

 \multicolumn{2}{c}{ \textit{w/o} Knowledge Modeling} & -1.2 & -3.3 & -7.0 & -1.5 & -4.1 & -8.8 & -1.3 & -3.6 & -8.8 & -12.1 & -21.6 & -10.7 & -13.3 & 26.8 & -11.6 & \color{red}-17.5 & \color{red}-29.2 & \color{red}-13.7 \\
 \multicolumn{2}{c}{+ Random Ranking} & -1.7 & -3.5 & -7.2 & -1.6 & -4.3 & -9.0 & -1.6 & -4.1 & -9.9 & -13.9 & -26.8 & -11.4 & -14.7 & -28.1 & -12.4 & \color{red}-19.3 & \color{red}-31.8 & \color{red}-15.0 \\
\midrule
 \multicolumn{2}{c}{Random Ranking} & -0.5 & -2.6 & -0.6 & -0.4 & -2.8 & -0.7 & -0.5 & -2.7 & -0.6 & -2.9 & -6.3 & -3.8 & -3.1 & -6.9 & -4.3 & -3.8 & -7.1 & -4.7 \\
 \multicolumn{2}{c}{\textit{w/o} Knowledge Ranking} & -0.1 & -0.4 & -0.2 & -0.2 & -1.9 & -0.4 & -0.2 & -0.8 & -0.1 & -1.3 & -3.3 & -1.5 & -2.1 & -5.4 & -3.6 & -1.9 & -5.7 & -2.1  \\
 \multicolumn{2}{c}{\textit{w/o} Semantic Ranking} & -0.3 & -1.4 & -0.2 & -1.4 & -0.2 & -0.1 & -0.2 & -1.0 & -0.3 & -0.9 & -0.4 & -0.7 & -1.3 & -2.3 & -1.1 & -2.4 & -3.3 & -1.6 \\

\bottomrule
\end{tabular}
}
\end{center}
\caption{\textbf{The impact of various components.} We report the comparison results between different combinations and the baseline.}
\label{tab:ab-result}       % Give a unique label
% \vspace{-10pt}
\end{table*}


\subsection{More Modalities Results}
The proposed method primarily focuses on image and text modalities to facilitate the evaluation of the proposed method, as these modalities are well-supported by the community and computationally efficient when using large vision-language models. However, our approach imposes no constraints on the modality encoders, allowing it to be easily generalized to other modalities. To validate this, we conducted experiment on a multimodal sarcasm detection dataset \cite{castro-etal-2019-towards}, which includes audio, vision, and text modalities. Using Unified-IO \cite{lu2024unified} as the backbone and keeping other configurations unchanged, our method demonstrated effectiveness even when extended to these modalities, as shown in Table ~\ref{tab:more-modalities-classification-result}. 

\begin{table}[ht]
% \renewcommand{\arraystretch}{1.3}
\setlength{\abovecaptionskip}{0cm} 
\setlength{\belowcaptionskip}{-0.2cm} 
\setlength\tabcolsep{2pt}
% table caption is above the table
% For LaTeX tables use
\begin{center}
\scalebox{0.9}{
\begin{tabular}{lc|c|c|c|c|c}
\toprule
Missing Rate $\eta$ & \multicolumn{3}{c}{0.3} & \multicolumn{3}{c}{0.5}\\
\cmidrule(lr){2-4} \cmidrule(lr){5-7}
Method & F1 & AP & SS & F1 & AP & SS \\
\midrule
\textcolor{gray}{Baseline (complete)} & \multicolumn{6}{c}{\textcolor{gray}{F1: 62.4 $|$ mAP: 64.7$|$ SS: -}} \\
Baseline (remove missing) & 57.1 & 59.3 & -  & 53.8 & 54.6 & - \\
DiCMoR (CVPR'23) & 55.5 & 57.6 & 11.7 & 51.3 & 52.9 & 10.2 \\
Ours (Unified-IO 7B)  & 58.3 & 60.1 & 13.3 & 54.7 & 55.8 & 11.4 \\
% \midrule
% \textcolor{gray}{$\Delta$ Complete Baseline} &  \color{gray} -3.4 & \color{gray} -1.8 & \color{gray} - & \color{gray} -10.7 & \color{gray} -5.2 & \color{gray} - \\
% $\Delta$ SOTA & \color{red} +3.7 & \color{red} +1.2 & \color{red} +4.5 & \color{red} +9.5 & \color{red} +8.6 & \color{red} +6.5 \\
\bottomrule
\end{tabular}}
\end{center}
\caption{Quantitative results (\%) on sarcasm datasets.}
\label{tab:more-modalities-classification-result}       % Give a unique label
\end{table}

\section{Visualization Analysis}

\subsection{Completion Results}

We present additional completion results, as shown in Fig. \ref{fig:vis-2}, \ref{fig:case-1}, and \ref{fig:case-2}. In the general domain, our knowledge modeling module emphasizes understanding the quantity of objects, their attributes, and the contextual environment. The results in Fig. \ref{fig:vis-2} indicate that our method more closely resembles the original missing modality compared to direct generation approaches. In the medical domain, incorporating knowledge of different lesions allows the LMM to comprehend the relationships between various regions in chest X-rays and the content described by the modality. Figs. \ref{fig:case-1} and \ref{fig:case-2} demonstrate that our method offers a more reliable strategy for missing data completion than direct generation.


\begin{figure*}[h]
  \centering
  \includegraphics[width=\linewidth]{pic/vis-2.pdf}
  \caption{\textbf{Visualization analysis.} We present the results of image completion on the COCO dataset. The first and fifth columns display the ground truth images, while the fourth and eighth columns show images generated directly by LMM using the available textual modality. The remaining columns illustrate the outcomes produced by our method, which employs LMM of varying scales.}
  \label{fig:vis-2}
\end{figure*}


\begin{figure*}[h]
  \centering
  \includegraphics[width=\linewidth]{pic/case1.pdf}
  \caption{\textbf{Visualization analysis.} We present the results of completing missing reports based on the X-ray modality from the IU X-ray dataset.}
  \label{fig:case-1}
\end{figure*}


\begin{figure*}[h]
  \centering
  \includegraphics[width=\linewidth]{pic/case2.pdf}
  \caption{\textbf{Visualization analysis.} We present the results of completing missing reports based on the X-ray modality from the IU X-ray dataset.}
  \label{fig:case-2}
\end{figure*}


\subsection{Intermediate Results}

We present some intermediate results as shown in Figs. \ref{fig:knowledge-1} and \ref{fig:knowledge-2}.


\subsection{Knowledge-based Ranking Results}

We present partial results of knowledge-based ranking as shown in Fig. \ref{fig:ranking}. Here, ``available [modality]'' indicates that the modality is visible, and ``QS'' represents the quality score of the completed missing modality combined with the observed modality. The results demonstrate that our proposed knowledge-based ranking module can effectively select relatively reasonable generated outcomes.

\begin{figure*}[h]
  \centering
  \includegraphics[width=\linewidth]{pic/ranking-score.pdf}
  \caption{\textbf{Visualization of Knowledge-based Ranking.} We present the results of knowledge-based ranking.}
  \label{fig:ranking}
\end{figure*}

\begin{figure*}[h]
  \centering
  \includegraphics[width=\linewidth]{pic/knowledge1.png}
  \caption{\textbf{Intermediate results.} We present the intermediate results extracted by our method, referred to as knowledge.}
  \label{fig:knowledge-1}
\end{figure*}

\begin{figure*}[h]
  \centering
  \includegraphics[width=\linewidth]{pic/knowledge2.png}
  \caption{\textbf{Intermediate results.} We present the intermediate results extracted by our method, referred to as knowledge.}
  \label{fig:knowledge-2}
\end{figure*}


\section{Hallucinations}

The hallucinations of LMM \cite{farquhar2024detecting} refer to instances when an AI model generates content that is factually incorrect, misleading, or unsubstantiated. In our approach, hallucinations primarily stem from the limitations of the training data. We advocate for a training-free method to achieve MMC, which has the advantage of being easily deployable across various domains with minimal input of relevant domain knowledge. However, the drawback is that the lack of task-specific training can result in the model having less ``common sense'' compared to models trained on extensive datasets. As illustrated in Fig. \ref{fig:vis-3}, the absence of common sense in our approach may lead to results that deviate from expected cognitive outcomes. In recent years, RAG (Retrieval-Augmented Generation) \cite{lewis2020retrieval, guo2024lightrag} has been regarded as an effective technique for mitigating hallucinations in large models. This technique provides the model with external truths, thereby reducing hallucinations during the reasoning process. In future work, we plan to incorporate RAG to enhance the robustness of our approach.


\begin{figure*}[h]
  \centering
  \includegraphics[width=\linewidth]{pic/vis-3.pdf}
  \caption{Hallucinations analysis.}
  \label{fig:vis-3}
\end{figure*}

\section{Limitations}

\subsection{More Modalities}

Our method focuses exclusively on image and text modalities, leaving its performance on other modalities, such as speech and depth, yet to be explored. The approach emphasizes the automatic extraction of inter-modal knowledge and the completion of missing modalities through domain knowledge. However, this focus on a limited set of modalities limits its generalizability and adaptability in real-world applications where multi-modal data often involves various types of sensory inputs. Thus, in the future, adaptation to other modalities is possible by defining a more comprehensive modality knowledge and expanding the learning framework to accommodate these new modalities. Some promising works \cite{girdhar2023imagebind, lyu2024unibind} show that one modality, such as image or text, can be connected to any other modality, paving the way for more inclusive and versatile multi-modal systems that handle diverse data types with high efficacy.

\subsection{More Tasks and Metrics}

Additionally, we observe that while our method enhances classification performance under a high missing rate (e.g., 0.7), it paradoxically results in a decrease in the similarity scores of the completed modalities. This suggests that although the model performs well in reconstructing missing data for classification tasks, the semantic alignment and quality of the generated modalities may still require significant refinement. Addressing these limitations presents an opportunity to improve the balance between classification accuracy and modality similarity. Therefore, there remains substantial potential for further exploration to develop more robust generation and ranking strategies in the future. These improvements could include incorporating advanced similarity-preserving techniques and exploring diverse evaluation metrics to assess the completeness and coherence of generated data across different tasks.


