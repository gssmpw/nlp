\section{Related Work}
\label{sec:related-work}


\subsection{Missing Multimodal Learning}

% Missing multimodal learning enables effective training and inference despite absent modalities. Approaches fall into imputation-based and non-imputation-based categories. Imputation-based methods predict missing data by modeling cross-modal interactions. Simple strategies use fixed values like zero \cite{parthasarathy2020training} or averages \cite{zhang2020deep}, which struggle with high-dimensional data. Advanced methods, such as SMIL \cite{ma2021smil}, apply meta-learning to simulate missing modalities. Wang \textit{et al.} \cite{wang2023multi} use a multitask framework to learn shared features for reconstruction, while Lian \textit{et al.} \cite{lian2023gcnet} employ graph neural networks. Zhao \textit{et al.} \cite{zhao2021missing} and Pham \textit{et al.} \cite{pham2019found} leverage cycle consistency for cross-modal predictions. Non-imputation-based methods focus on fusion strategies or missing indicators instead of predicting missing data. Wang \textit{et al.} \cite{wang2020transmodality} and Han \textit{et al.} \cite{han2019implicit} use translational semantics for implicit fusion. Ma \textit{et al.} \cite{ma2022multimodal} leverage transformers' variable input capability. Lee \textit{et al.} \cite{lee2023multimodal} and Guo \textit{et al.} \cite{guo-etal-2024-multimodal} introduce learnable missing semantic tokens, while Zeng \textit{et al.} \cite{zeng2022tag} use fixed tags to identify missing inputs.

% Current methods often require pre-training with complete data and show limited domain transfer. Our approach leverages LMM to predict missing modalities using domain knowledge without extra pre-training, employing few-shot learning for flexible adaptation.

Missing multimodal learning enables models to train and inference effectively even when some modalities are absent. The missing multimodal learning methods can be categorized into imputation-based and non-imputation-based approaches. Imputation-based methods predict or recover missing data by learning interactions between modalities. Typical strategies include filling with special values such as zero or the average value, which are unsuitable for high-dimensional data \cite{parthasarathy2020training} . Advanced methods like SMIL \cite{ma2021smil} use meta-learning to simulate missing modalities. Wang \textit{et al.} \cite{wang2023multi} propose a multitask framework for missing modality learning, which learns shared features across different tasks and uses these shared features to reconstruct missing modalities. Lian \textit{et al.} \cite{lian2023gcnet} use graph neural networks to explore relationships between modalities and recover missing ones, while Zhao \textit{et al.} \cite{zhao2021missing} and Pham \textit{et al.} \cite{pham2019found} leverage cycle consistency to capture cross-modal interactions for predicting missing modalities. 

% On the other hand, the goal of non-imputation-based methods is to carefully design fusion strategies or missing indicators instead of predicting missing data. Wang \textit{et al.} \cite{wang2020transmodality} and Han \textit{et al.} \cite{han2019implicit} explore translational semantics for implicit fusion. Ma \textit{et al.} \cite{ma2022multimodal} leverage the transformer's variable input capability for handling missing data. Lee \textit{et al.} \cite{lee2023multimodal} and Guo \textit{et al.} \cite{guo-etal-2024-multimodal} employ learnable special missing semantic tokens, allowing the model to focus on the parts of the fusion corresponding to the missing modalities. Zeng \textit{et al.} \cite{zeng2022tag} use fixed tags for identifying missing inputs. 

On the other hand, non-imputation-based methods focus on fusion strategies or missing indicators instead of predicting missing data \cite{ke2023clustering}. Wang \textit{et al.} \cite{wang2020transmodality} and Han \textit{et al.} \cite{han2019implicit} use translational semantics for implicit fusion. Ma \textit{et al.} \cite{ma2022multimodal} leverage transformers' variable input capability. Lee \textit{et al.} \cite{lee2023multimodal} and Guo \textit{et al.} \cite{guo-etal-2024-multimodal} introduce learnable missing semantic tokens, while Zeng \textit{et al.} \cite{zeng2022tag} use fixed tags to identify missing inputs. Current methods often require pre-training with complete data and show limited domain transfer. Our approach leverages LMM to predict missing modalities using domain knowledge without extra pre-training, employing few-shot learning for flexible adaptation.


\subsection{Large Multimodal Models}

Large multimodal models (LMMs) integrate diverse data types, such as text, images, and audio, to enhance understanding and interaction capabilities. Early works focused on fusion techniques to combine these modalities, like early \cite{chen2020uniter}, late \cite{evangelopoulos2013multimodal}, or hybrid fusion \cite{prabu2019multimodal}. Recent advancements \cite{team2024chameleon, team2024gemini}, such as OpenAI's GPT-4o \cite{hurst2024gpt} and Google's PaLM-E \cite{driess2023palm}, have achieved remarkable success by increasing the scale of both pre-training data and model parameters. These private ultra-models (greater than 100 billion parameters) exhibit robust generalization across various domains, which enables applications in image captioning, visual question answering, and multimodal dialogue. On the other hand, some open-source, medium- to small-scale (less than 100 billion parameters) models, such as LLaVA \cite{liu2024visual}, Qwen-VL \cite{wang2024qwen2}, and InternVL \cite{chen2024internvl}, have matched the performance of these ultra-large models. The common feature among them is their rich prior knowledge, understanding, and OOD capabilities, which form the foundation of training-free missing multi-modality completion.


\subsection{Conditional Generation}

Conditional generation creates new data samples based on specific inputs, enabling models to generate tailored outputs. Early foundational work in this area includes Generative Adversarial Networks (GANs) \cite{goodfellow2014generative} and Variational Autoencoders (VAEs) \cite{kingma2013auto}, with notable advancements such as conditional GANs \cite{mirza2014conditional} and Conditional VAEs (CVAEs) \cite{sohn2015learning}. Recent progress has introduced diffusion models \cite{song2020denoising, li2022diffusion, ruiz2023dreambooth}, which have demonstrated the ability to generate more realistic content, as exemplified by systems like DALL-E \cite{ramesh2021zero} and Stable Diffusion \cite{podell2023sdxl}. ControlNet \cite{zhang2023adding} further improves user control. The key distinction between these approaches and our method lies in their respective objectives. Unlike these approaches, which focus on diverse content generation, our method emphasizes understanding modality interactions to \textbf{accurately} complete \textbf{missing content}. 

% While the former focuses on generating \textbf{diverse content} in response to prompts, Our method goes beyond this goal by requiring the model to understand interactions between modalities to \textbf{accurately} complete \textbf{missing content}.

% Conditional generation creates new data samples based on specific inputs, enabling models to generate tailored outputs. Early foundational work in this area includes Generative Adversarial Networks (GANs) \cite{goodfellow2014generative} and Variational Autoencoders (VAEs) \cite{kingma2013auto}, with notable advancements such as conditional GANs \cite{mirza2014conditional} and Conditional VAEs (CVAEs) \cite{sohn2015learning}. Recent advances, such as diffusion models \cite{song2020denoising, li2022diffusion, ruiz2023dreambooth}, enhance realism, exemplified by DALL-E \cite{ramesh2021zero} and Stable Diffusion \cite{podell2023sdxl}. ControlNet \cite{zhang2023adding} further improves user control. Unlike these approaches, which focus on diverse content generation, our method emphasizes understanding modality interactions to \textbf{accurately} complete \textbf{missing content}.