%% 
%% Copyright 2007-2024 Elsevier Ltd
%% 
%% This file is part of the 'Elsarticle Bundle'.
%% ---------------------------------------------
%% 
%% It may be distributed under the conditions of the LaTeX Project Public
%% License, either version 1.3 of this license or (at your option) any
%% later version.  The latest version of this license is in
%%    http://www.latex-project.org/lppl.txt
%% and version 1.3 or later is part of all distributions of LaTeX
%% version 1999/12/01 or later.
%% 
%% The list of all files belonging to the 'Elsarticle Bundle' is
%% given in the file `manifest.txt'.
%% 
%% Template article for Elsevier's document class `elsarticle'
%% with numbered style bibliographic references
%% SP 2008/03/01
%% $Id: elsarticle-template-num.tex 249 2024-04-06 10:51:24Z rishi $
%%
\documentclass[preprint,12pt]{elsarticle}

%% Use the option review to obtain double line spacing
%% \documentclass[authoryear,preprint,review,12pt]{elsarticle}

%% Use the options 1p,twocolumn; 3p; 3p,twocolumn; 5p; or 5p,twocolumn
%% for a journal layout:
%% \documentclass[final,1p,times]{elsarticle}
%% \documentclass[final,1p,times,twocolumn]{elsarticle}
%% \documentclass[final,3p,times]{elsarticle}
%% \documentclass[final,3p,times,twocolumn]{elsarticle}
%% \documentclass[final,5p,times]{elsarticle}
%% \documentclass[final,5p,times,twocolumn]{elsarticle}

%% For including figures, graphicx.sty has been loaded in
%% elsarticle.cls. If you prefer to use the old commands
%% please give \usepackage{epsfig}

%% The amssymb package provides various useful mathematical symbols
\usepackage{amssymb}
%% The amsmath package provides various useful equation environments.
\usepackage{amsmath}
%% The amsthm package provides extended theorem environments
%% \usepackage{amsthm}
\usepackage{caption}
\usepackage{rotating}
\usepackage{tabularx}
\usepackage{booktabs}
\usepackage{lscape}
\usepackage{longtable}
\usepackage{hyperref}

\usepackage{float} % for the H specifier
\usepackage{graphicx}
\usepackage{adjustbox} 
\usepackage{tabulary} 
\usepackage{natbib}
\usepackage{subcaption}
\usepackage{tikz}
 \usepackage{multirow}

%% The lineno packages adds line numbers. Start line numbering with
%% \begin{linenumbers}, end it with \end{linenumbers}. Or switch it on
%% for the whole article with \linenumbers.
%% \usepackage{lineno}

\journal{Natural Language Processing}

\begin{document}

\begin{frontmatter}

%% Title, authors and addresses

%% use the tnoteref command within \title for footnotes;
%% use the tnotetext command for theassociated footnote;
%% use the fnref command within \author or \affiliation for footnotes;
%% use the fntext command for theassociated footnote;
%% use the corref command within \author for corresponding author footnotes;
%% use the cortext command for theassociated footnote;
%% use the ead command for the email address,
%% and the form \ead[url] for the home page:
%% \title{Title\tnoteref{label1}}
%% \tnotetext[label1]{}
%% \author{Name\corref{cor1}\fnref{label2}}
%% \ead{email address}
%% \ead[url]{home page}
%% \fntext[label2]{}
%% \cortext[cor1]{}
%% \affiliation{organization={},
%%             addressline={},
%%             city={},
%%             postcode={},
%%             state={},
%%             country={}}
%% \fntext[label3]{}

\title{Explainability in Practice: A Survey of Explainable NLP Across Various Domains}


%% AUTHOR BLOCK
%% -----------------------------------------------------------
\author[1]{\textit{Hadi Mohammadi}\corref{cor1}}
%\ead{h.mohammadi@uu.nl}
%\ead[url]{https://orcid.org/0000-0003-0860-9200}

\cortext[cor1]{Corresponding author: Hadi Mohammadi (h.mohammadi@uu.nl)}

\author[1]{\textit{Ayoub Bagheri}}
\ead{a.bagheri@uu.nl}
%\ead[url]{https://orcid.org/0000-0001-6366-2173}

\author[1]{\textit{Anastasia Giachanou}}
\ead{a.giachanou@uu.nl}
%\ead[url]{https://orcid.org/0000-0002-7601-8667}

\author[1]{\textit{Daniel L. Oberski}}
\ead{d.l.oberski@uu.nl}
%\ead[url]{https://orcid.org/0000-0001-7467-2297}


%% AFFILIATIONS
%% -----------------------------------------------------------
\affiliation[1]{
  organization={Department of Methodology and Statistics, Utrecht University},
  addressline={Padualaan 14},
  city={Utrecht},
  postcode={3584 CH},
  state={Utrecht},
  country={The Netherlands}
}
            
%% Abstract
{
\begin{abstract}
\small
%% Text of abstract
\textbf{Natural Language Processing (NLP)} has become a cornerstone in many critical sectors, including healthcare, finance, customer relationship management, etc. This is particularly true with the development and use of advanced models like GPT-4o, Gemini, and BERT, which are now widely used for decision-making processes. However, the black-box nature of these advanced NLP models has created an urgent need for transparency and explainability. This review provides an exploration of \textbf{explainable NLP (XNLP)} with a focus on its practical deployment and real-world applications, examining how these can be applied and what the challenges are in domain-specific contexts. The paper underscores the importance of explainability in NLP and provides a comprehensive perspective on how XNLP can be designed to meet the unique demands of various sectors, from healthcare's need for clear insights to finance's focus on fraud detection and risk assessment. Additionally, the review aims to bridge the knowledge gap in XNLP literature by offering a domain-specific exploration and discussing underrepresented areas such as real-world applicability, metric evaluation, and the role of human interaction in model evaluation. The paper concludes by suggesting future research directions that could lead to a better understanding and broader application of XNLP.
\end{abstract}
}

%% Keywords
{

\begin{keyword}
\footnotesize
\textit{Natural Language Processing (NLP)\sep Explainable Natural Language Processing (XNLP \sep Transparency \sep Interpretability \sep Ethical AI.}
\end{keyword}
}
\end{frontmatter}

%% Add \usepackage{lineno} before \begin{document} and uncomment 
%% following line to enable line numbers
%% \linenumbers

%% main text

\section{Introduction}
\label{sec:introduction}

The advent of Natural Language Processing (NLP) and Large Language Models (LLMs) has mainly improved machine-human interaction by enabling machines to better understand human language, making these systems more accessible. Advanced models like OpenAI’s GPT-4o, Google’s Gemini, Meta's LLaMA 3.1, and BERT have improved the capabilities of machines in understanding and processing human language. This has enabled their applications across diverse domains such as healthcare, finance, and customer relationship management (CRM)~\cite{shrivastava_innovation_2022}. These advancements have reduced traditional processing times, enabling rapid content generation and complex question answers~\cite{ribeiro_why_2016, guidotti2018survey}. For instance, a study by~\citet{oniani2024enhancinglargelanguagemodels} showed that LLMs could improve clinical decision support (CDS) by incorporating Clinical Practice Guidelines (CPGs). By using CPGs, LLMs can provide more precise and contextually relevant treatment recommendations, thus enhancing patient care and assisting healthcare professionals in making more informed decisions~\cite{Li2023MedDMLC}.  These advanced LLMs techniques for decision-making offer many advantages, including timely and accurate analysis, user-friendly interfaces, and the ability to handle vast amounts of data efficiently~\cite{zhao_survey_2023}.
Advanced models have achieved high prediction performance in several tasks, including image classification~\cite{krizhevsky2017imagenet, JAHROMI2024}, NLP tasks such as machine translation~\cite{vaswaniattention2023}, speech recognition~\cite{hinton2012deep}, and game playing~\cite{silver2016mastering}. However, it is not possible to understand their predictions due to the number of parameters. These models, known as ``black boxes" in machine learning, are often developed directly from data by complex algorithms. Incorporating billions of parameters, these models are so opaque that even their creators cannot fully understand how variables integrate to generate predictions, resulting in a reduction in transparency despite their advanced capabilities~\cite{rudin2019we}. Furthermore, those models have been trained on large amounts of data, which often contain biases\footnote{\scriptsize\texttt{Biases are systematic errors in decision-making that unfairly favor certain groups over others.}}. The core issue is that the lack of clarity in the decision-making process can not only hinder the detection of biases embedded within the model but also potentially lead to further biases. For instance, biases can arise from various sources, such as historical imbalances and stereotypes present in training data \cite{caliskan2017semantics}, model assumptions that reinforce existing prejudices~\cite{bolukbasi2016man}, and feedback loops that perpetuate discrimination~\cite{barocas2016big}. These biases can go unnoticed and unaddressed, leading to biased outcomes. This phenomenon has been observed in various applications, such as in hiring algorithms displaying biases against specific demographic groups, as was notably seen in Amazon's recruitment tool~\cite{dastin2018amazon}. This issue is not limited to hiring practices as different tools are applied diversely across various fields due to their tendency to have different goals. In healthcare, for instance, there have been concerns about biases in diagnostic algorithms that could negatively affect certain patient groups~\cite{igoe2021algorithmic}. Similarly, in the financial sector, credit-scoring algorithms might unintentionally discriminate against minority communities. For example,~\citet{andrews2021flawed} found that predictive tools used in credit scoring are between 5 and 10 percent less accurate for lower-income families and minority borrowers compared to higher-income and non-minority groups. These examples highlight the urgent need for transparency and a deeper understanding of the decision-making mechanisms in NLP applications. This is particularly crucial in domains like healthcare and finance, where decisions can have lasting impacts on individuals and communities. Therefore, while the bias is indeed a product of the training data or model's training process, the lack of explainability worsens the issue caused by these biases, making them more challenging to identify and correct. 
Explainable AI (XAI) has emerged as a way to better understand the ``black-box" nature of deep learning models. Research activity around XAI has identified various goals for achieving an explainable model, including enhancing trustworthiness, interaction, compliance, fairness, privacy awareness, and ethical usage~\cite{arrieta2020explainable, bolukbasi2016man, tenney_language_2020}. For example, techniques such as Local Interpretable Model-agnostic Explanations (LIME) have been beneficial in clarifying model decisions in sentiment analysis by highlighting influential words, while SHapley Additive exPlanations (SHAP) have played a role in feature importance analysis, particularly in financial risk assessment~\cite{jain2023explaining, gramegna2021shap}. Although these different goals help discriminate the purposes for which ML explainability is performed, it is crucial to look at specific XAI applications because different fields have unique requirements for transparency and interpretability. These requirements directly impact their decision-making processes and regulatory compliance, as emphasized by a recent review~\citet{islam2022systematic}, which highlighted the importance of domain-specific approaches to enhance transparency, interpretability, and user trust, particularly in sensitive domains such as finance and the healthcare system.

Existing research in explainable NLP (XNLP), a specific area within XAI, has identified several open questions, particularly concerning its real-world applications, evaluation metrics, and the role of human interaction in model assessment. While XNLP has been a topic of interest and recent reviews provide insights into explainability models, they do not thoroughly discuss the specifics of XNLP applications. Although insightful research by~\citet{gurrapu2023rationalization, qian2021xnlp, ali2023explainable, tjoa2020survey} and,~\citet{arrieta2020explainable} has advanced our understanding of XNLP, they often leave the practical integration of XNLP applications less explored. Despite~\citet{islam2022systematic} emphasized the importance of domain-specific approaches and the need for more focused research in sensitive XAI domains, they did not extensively cover XNLP specifics. Similarly, while~\citet{tjoa2020survey} and~\citet{Zini2022OnTE} provide useful information on deep learning models, they do not emphasize XNLP specifics. Previous surveys like~\cite{puiutta2020explainable, danilevsky_survey_2020, ali2023explainable, wiegreffe2021teach, vilone2020explainable} offer a more generalized perspective on XAI. For example,~\citet{ali2023explainable} explores different explainability methods but does not cover their application in practical NLP settings, such as healthcare or finance, where not only the deployment of XNLP could play a crucial role, but also its direct impact on end-users is more important. For instance, in the medical field, the integration of XNLP needs to align with doctors' specific workflows and professional backgrounds. Unlike general applications, XNLP in healthcare must provide explanations that are not only clear but also medically relevant, as they directly impact patient care. In the financial sector, there are specific applications like fraud detection and risk assessment. Here, XNLP must take into account the complex language used in financial reports and transactions. For instance, if XNLP is used to estimate the value of companies based on their reports, there's a risk of these reports being manipulated to influence the model's output. This presents a distinct XNLP challenge: ensuring that the system can recognize real insights from possibly manipulated input. The evaluation of explainability models is not extensively discussed in papers like~\cite{arrieta2020explainable,deyoung2019eraser,liu_towards_2019, islam2022systematic}, which lack an exploration of XNLP evaluation. To the best of our knowledge, none of the reviewed papers completely explore how an XNLP model should comply with domain-specific approaches. This gap underscores the need for more focused research on the application, limitations, and challenges of XNLP in specific sectors to improve our understanding of it.

In this paper, we first introduce key modeling techniques for XNLP, aimed at readers who are either new to the field or looking to deepen their expertise, providing a foundation for the discussions that follow. We then delve into the practical deployment of XNLP and its direct impact on end-users. We provide detailed discussions on how XNLP systems interact in practice, a topic identified as a remaining open question by both prior research and industry insights. This focus not only addresses a critical gap in the literature but also offers novel views on how XNLP can be applied across various domains to improve user understanding and trust in machine learning models. We emphasize the importance of using XNLP approaches to meet the needs of specific domains. Through this paper, we aim to discuss the challenges and opportunities presented by each domain in XNLP applications, showing the necessity of domain-specific approaches in the development and implementation of XNLP solutions. Our review tries to bridge this knowledge gap by exploring XNLP with a focus on domain-specific views. We discuss critical aspects like metrics evaluation, real-world applicability, rationalization methods, and the role of human interaction in model evaluation. Also, we discuss future extensions and some insights gained from the applied perspective.

The remainder of the paper is organized as follows: Section~\ref{sec:modeling} talks about NLP modeling and techniques, starting with TF-IDF-based approaches and embedding models such as Word2Vec and GloVe, leading to transformer models. Section~\ref{sec:application} discusses the specific issues encountered in a range of applications, including medicine, finance, systematic reviews, customer relationship management (CRM), chatbots and conversational AI, social and behavioural Science, and Human Resources (HR). Section~\ref{sec:critical} examines the critical aspects of XNLP, focusing on trade-offs fundamental to model deployment and discussing future research directions in XNLP. Finally, The paper concludes in Section~\ref{sec:Conclusion}.

%ُSection 2
\section{Modeling Techniques for XNLP}
\label{sec:modeling}

\subsection{Explainability of Traditional NLP models}

Traditional NLP models, such as Bag of Words (BoW) and its variants like term frequency (TF) and term frequency-inverse document frequency (TF-IDF), offer a foundational representation of textual data. The BoW, in particular, represents a text document as a collection of words without highlighting their order or the context in which those words are used. When paired with transparent classifiers like logistic regression, this representation emphasizes interpretability. Specifically, the logistic regression \textbf{model's coefficients} allow us to discern each word's influence on the prediction outcome. Metrics like semantic relationships highlight the importance of specific words for a given prediction~\cite{heap_word_2017, liu_representation_2020}.

While the TF-IDF model improves upon TF by assigning significance to words based on their relative importance in the corpus~\cite{shimomoto_text_2018, zubiaga_tf-cr_2020}, these models may struggle with limitations such as contextual relationships and syntactic structures~\cite{dagan_contextual_1995, jurafsky_vector_2019, zubiaga_tf-cr_2020}. 
%Other key drawbacks of the BoW model are its challenges with word order, polysemy, homonymy, and inadequate capturing of semantic similarity~\cite{navigli_word_2009, rossiello2017centroid, pilehvar2020embeddings}.
In TF and TF-IDF BoW models interpretability can be an issue; for example, if multiple words often co-occur and collectively predict an outcome (like a diease code from discharge letters), each word might have a small model coefficient, potentially smaller than less important words. Although grounded in early NLP, the effort to process complex linguistic tasks emphasizes the importance of advanced models that combine the simplicity and interpretability of traditional techniques with high performance.

%\subsection{Embedding Models}

Embedding models have revolutionized the field of NLP by bridging the semantic and syntactic gap that models such as TF and TF-IDF tried to pass. By mapping words and sentences into continuous vector spaces, these models capture semantic relationships and similarities between words, phrases, and sentences, enhancing their performance on various NLP tasks~\cite{song_information_2020}. Word embeddings such as Word2Vec~\cite{mikolov_distributed_2013} and GloVe~\cite{pennington_glove_2014} produce dense vector representations for words, where semantically similar words tend to be closer together in the vector space. Due to their semantic similarity, words such as ``king" and ``queen" have embedding vectors that lie close to one another. This provides an intuitive way to comprehend word relationships, which simpler models such as BoW could not capture~\cite{sileo_analysis_2021}. However, word embeddings cannot capture the meaning of longer texts, such as complete sentences or paragraphs. Sentence embeddings, such as the Universal Sentence Encoder~\cite{cer_universal_2018}, were developed to address this limitation. These models encode entire sentences or paragraphs into fixed-dimensional vectors, capturing the text's semantic content.

%\subsubsection{Explainability Techniques for Embeddings}
Despite the fact that embedding models improved the performance of NLP tasks, 
%for instance, in medicine, these models have improved the accuracy of electronic health record analysis, aiding in more precise diagnosis and treatment predictions~\cite{rasmy2021med}. 
they introduced a lack of clarity level compared to the previous models. Therefore, various explainability techniques have been proposed to understand the underlying structure of the embedding models~\cite{wallace2019nlp}. Explainability techniques for embeddings in NLP  models are mainly achieved through \textbf{visualization and the identification of influential vector space dimensions}. Visualization tools, such as ``TensorBoard"~\cite{vogelsang2020magician} and ``EmbeddingVis"~\cite{li2018embeddingvis}, allow researchers to understand and compare different embedding models by revealing the relationships between words and their structural positioning in language, highlighting semantically similar groups, and tracking changes in word meanings over time~\cite{wallace2019nlp}. Recent studies, such as those by~\citet{bracsoveanu2022visualizing}, highlight the critical role of visualization in enhancing the interpretability of language models. They suggest that visualization aids not only in debugging by revealing areas of overfitting or underfitting but also in clarifying how specific inputs lead to particular outputs. Similarly, \textbf{dimensionality reduction techniques} like t-distributed Stochastic Neighbor Embedding (t-SNE)~\cite{van_der_maaten_visualizing_2008} and Principal Component Analysis (PCA)~\cite{jolliffe_principal_2016} enable the projection of high-dimensional vectors into 2D or 3D spaces, which can then be visualized to identify clusters and correlations. Additionally, research by ~\citet{wang2019single} showes the effectiveness of PCA~\cite{jolliffe_principal_2016} in optimizing word embeddings by selecting the most informative dimensions.  This process involves incrementally removing less informative dimensions and assessing the impact on performance across various language tasks, aligning with the goal of identifying the most critical dimensions in the embedding space for language understanding and predictions.

Furthermore within the domain of explainability, \textbf{gradient-based methods} and \textbf{attention mechanisms} represent different approaches. Gradient-based techniques, such as saliency maps, operate by computing the gradients of the output with respect to the input embeddings~\cite{simonyan2013deep}. This process essentially performs a backward pass through the model to identify which input nodes (i.e., words or phrases) are most effective in determining a particular output. Such methods have been widely used in areas like image classification~\cite{simonyan2013deep,krizhevsky2017imagenet} and are increasingly being adapted for NLP tasks. They offer insights into the internal workings of a model by highlighting the parts of the input text that have the most impact on the model's predictions.

On the other hand, attention mechanisms, which are integral to models like those described in~\cite{bahdanau2014neural} and~\cite{vaswaniattention2023}, function differently. Attention is a feature built into the architecture of certain NLP models, particularly in tasks such as machine translation and text summarization. Unlike gradient-based methods that require a backward pass, attention mechanisms are part of the forward pass of the model. They provide an inherent method of highlighting or focusing on specific parts of the input sequence, determining how much attention or importance the model should give to each part of the input during the prediction process.

Despite advancements in the explainability of embedding models, several challenges remain. First, the growing complexity of the models often leads to decreased interpretability. As models become more complex, understanding the reasoning behind their decisions becomes more complex, even with advanced explainability techniques~\cite{sonkar_attention_2020}. A second challenge is that what counts an ``explanation" will depend on the application and audience. In some contexts, simpler methods, such as visualization through dimensionality reduction, are appropriate and beneficial. While these methods might risk oversimplifying data relationships and making incomplete interpretations, they can be quite effective in making the model's workings accessible to non-expert audiences. Techniques like PCA or t-SNE, although they reduce high-dimensional data to two or three dimensions, can provide valuable insights in a more digestible format~\cite{wattenberg2016use, conklin2021meta}. On the other hand, for more technical applications where understanding the complex details of model behavior is crucial, such simplifications might not suffice. In these cases, more complex and detailed methods are necessary to capture the full scope of the relationships within the data. The goal should be to provide a balance between high performance, good interpretability, and the proper representation of the specific needs of each application. How this balance should be struck will often require additional human factors research specific to the application area.
 
\subsection{Explainability of Transformer-based models}

Transformers, foundational to models like BERT~\cite{devlin2019bert}, have transformed NLP by enabling parallel processing and handling long-range dependencies effectively, setting new performance benchmarks~\cite{kalyan_ammus_2021}. By using self-attention mechanisms, Transformers assign varying weights to input components, capturing relationships and dependencies even in distant parts of the input, which is crucial for tasks like machine translation and sentence summarization~\cite{vaswaniattention2023}. BERT, a model using attention, can be pre-trained on extensive text and fine-tuned for specific tasks, with attention mechanisms highlighting salient input parts~\cite{vaswaniattention2023}. Advanced variants such as RoBERTa~\cite{liu_roberta_2019} and ALBERT~\cite{lan_albert_2020} have further improved performance through refined training and model configurations, excelling in tasks like sentiment analysis, question answering, and named entity recognition, often surpassing human performance on benchmarks~\cite{mccann2018natural}.

However, despite the Transformer models' impressive performance and capabilities, they face obstacles. mainly, their complexity makes interpretability and explainability more difficult to illuminate their decision-making processes and identify the linguistic features they rely on. One of the primary explainability approaches for transformers has been the \textbf{visualization of attention weights}, which allows researchers and practitioners to see which parts of an input sequence the model is ``attending" to when producing an output~\cite{vaswaniattention2023}. Another technique involves \textbf{probing tasks}, which are designed to ascertain what linguistic properties a pre-trained model has learned by evaluating it on simplified tasks~\cite{hewitt2019designing}. \textbf{Feature visualization}, where the activations and features within the model are visualized, is another approach to gaining insights into model behavior~\cite{olah2017feature}. There is also the use of \textbf{attribution methods}, such as Integrated Gradients, to identify the importance of each word in a given input for the model's final prediction~\cite{sundararajan2017axiomatic}. 

Other techniques aimed at addressing the interpretability issues of these models include \textbf{model simplification}, \textbf{local interpretation methods}, and \textbf{global interpretation methods}. Model simplification techniques, such as distillation, involve training a simpler model to mimic the behavior of a complex model, thereby making the decision-making process more transparent~\cite{hinton2015distilling}. Local interpretation methods offer explanations for individual predictions, with LIME (Local Interpretable Model-agnostic Explanations) perturbing input data to observe changes in predictions, thus creating local explanations around specific instances~\cite{ribeiro2016model}. Global interpretation methods aim to provide an overall understanding of the model’s behavior, with SHAP (SHapley Additive exPlanations) using a game-theoretic approach to offer global explanations by evaluating the importance of features across all possible combinations~\cite{lundberg2017unified}. In addition, \textbf{input perturbation }techniques, such as the one proposed by~\citet{liang_deep_2018}, can identify the most influential words or phrases for a particular prediction. By systematically altering the input text and observing the resulting output, these techniques help to comprehend the model's decision-making process. Similarly, \textbf{contextual decomposition}~\cite{murdoch_beyond_2018} aims to decompose the model's output into contributions from individual words or phrases to provide a more granular view of the model's decision-making process.

Although various techniques have been developed to make these models more comprehensible and reliable for human users~\cite{neerudu_robustness_2023}, these tools can sometimes provide insufficient or misleading interpretations. Visualization tools like attention maps, which highlight important words or phrases in the model's decisions and enhance interpretability and transparency, may not accurately reflect the model's true decision-making process~\cite{wiegreffe_attention_2019, sonkar_attention_2020}.For instance, in a sentiment analysis task, an attention map might heavily weigh a neutral word like ``the" while downplaying the importance of a more sentiment-revealing word like ``hate"~\cite{jain_attention_2019}. Such instances underscore the potential pitfalls and lead to erroneous conclusions regarding the model's reasoning. Furthermore, there are numerous techniques and ideas for interpreting these models, yet no clear guidance exists on which to use in specific situations. Ultimately, it comes down to how the explanations are used in practice and whether users genuinely understand them. The evaluation and implementation of these techniques will be the focus of the remainder of this paper~\cite{belinkov2020interpretability}. Transformer models including prominent examples like BERT~\cite{devlin2019bert} and GPT-x~\cite{radford2019language}, have considerably advanced the field of NLP, their intricacy, extensive resource demands, and interpretability and explainability challenges continue to drive intensive research. . While we aim to mention most of the explainability techniques, our main focus is on their application. For more detailed information about these techniques, please refer to the bibliography.

%Secton 3
\section{Applications and Domains of XNLP}
\label{sec:application}

AI-based applications have recently been applied to various facets of human life, including social networking, medicine, commerce, customer service, and finance. General NLP applications, such as machine translation, text summarization, and sentiment analysis, are commonly used to automate routine tasks and enhance user experiences~\cite{vaswaniattention2023, covington2016deep}. In these cases, the focus is often on performance rather than the need for explanations of how the models reach their conclusions. However, in specific fields such as medicine, NLP must provide explanations that are not only clear but also medically relevant. For example, it is important to understand the predictions of systems trained on electronic health records (EHRs), as these predictions can influence decisions about which patients are flagged for follow-up by a specialist or for additional treatment, consequently impacting patient care. Fortunately, different from traditional AI, which can answer ``Yes" and ``No" type questions without explaining how they did so, XNLP can answer ``Wh-questions" such as ``Why", ``When", and ``Where". However, XNLP should produce explanations that humans can understand, ensuring that all beneficiaries can trust how the models work. On one hand, confidence and trust in explanations of how answers were obtained are critical, and on the other hand, produced explanations should be expressive enough for humans to comprehend.

%Figure 2. AI vs. XAI.
%\begin{figure}[H] % H specifier enforces the position
%\centering
%\includegraphics[width=0.8\textwidth]{AI vs XAI .pdf} 
%\caption{AI vs XAI as shown in %\cite{gohel2021explainable}} 
%\label{fig:AIvsXAI}
%\end{figure}

On top of that, XNLP should ensure that AI/ML models make fair and equitable decisions. Like the EHRs, understanding how a patient is selected for additional treatment shows how the system is fair~\cite{ledro2022artificial}. An explanation of how an answer was obtained is critical for confidence and fairness in many applications. For example, advancements in AI technologies enable CRM systems to offer personalized marketing responses and insights, impacting customer engagement and trust. These AI-enhanced features aid in better anticipating customer needs and preferences, thereby boosting confidence in the system's recommendations and decisions~\cite{ledro2022artificial}. As explored by~\citet{brandl-etal-2024-interplay}, balancing fairness and explainability is essential to creating more trustworthy models, particularly in sensitive applications. To a great extent, XNLP should ensure that the decisions can be justified and audited since when logical and scientific arguments support existing beliefs, humans tend to accept predictions and trust conclusions given by AI/ML systems~\cite{islam2021local}. 

Transparent explanations are helpful in many cases to enable cooperation between AI and humans. For instance, if an AI outperforms humans at a specific task (e.g., AlphaGo~\cite{silver2016mastering}), people can learn and derive information from the explanations provided. Moreover, if an AI performs similarly to human intelligence, transparent explanations can enhance people's trust in the AI~\cite{glikson2020human}. For example, in a model trained to predict firm valuation from yearly reports, transparency is crucial so that prospective and actual shareholders can audit the system and understand why one company gets a higher valuation than another. If the model's training isn't transparent, it could be intentionally manipulated to unfairly benefit certain individuals. In another example, consider chatbots in customer service. In this context, XNLP must not only understand and respond to customer questions but also explain its responses transparently to build trust and clarify the reasoning behind them. This is important in domains like customer service, where the clarity and rationale of responses directly impact user satisfaction and trust.

To sum up, XNLP has an important role in multiple domains where interpretability, transparency, and accountability are crucial. As the demand for more understandable AI models continues to rise, various industries are using the power of XNLP to uncover new insights and improve decision-making processes. This section explores key fields in which XNLP has proven to be a game-changer. Table~\ref{tab:applications} illustrates these applications and their respective studies.

{%Table 1.Overview of XNLP Applications, Subcategories, and Case Studies
\begin{scriptsize} 
\centering
\renewcommand{\arraystretch}{1}
\begin{longtable}{p{3.5cm} p{5.5cm} p{3.2cm}}
\caption{Overview of XNLP Applications, Subcategories, and Case Studies} \label{tab:applications} \\
\toprule
\textbf{XNLP Applications} & \textbf{Subcategories} & \textbf{Studies} \\
\midrule
\endfirsthead
\toprule
\textbf{XNLP Applications} & \textbf{Subcategories} & \textbf{Studies} \\
\midrule
\endhead

Medicine & Electronic Health Records &\cite{choi_doctor_2016, alabi2023machine} \\
 & Medical Documents Analysis &\cite{li_neural_2022, moradi_deep_2022, kang_eliie_2017, weng_medical_2017} \\
\addlinespace

Finance & Risk Assessment &\cite{fritz-morgenthal_financial_2022, wallace2019nlp}\\
 & Fraud Detection&\cite{psychoula_explainable_2021, fritz-morgenthal_financial_2022, varshney_safety_2017, cirqueira_towards_2021}\\
 & Firm valuation&\cite{rizinski2024sentiment, bagga2023towards}  \\
\addlinespace

Systematic Reviews& Review Automation&\cite{rosemblat_towards_2019}\\
 & Text Summarization&\cite{marshall_robotreviewer_2016} \\
\addlinespace

Customer Relationship Management & Sentiment Analysis&\cite{capuano2021sentiment, du_techniques_2019, bacco_explainable_2021} \\
 & Customer Support Automation&\cite{bar-haim_arguments_2020, jenneboer2022impact}\\
\addlinespace

Chatbots and Conversational AI & Conversational Agents &\cite{bar-haim_arguments_2020, coppo_artificial_2019}\\
 & Context-Aware Recommendations &\cite{zhang_deep_2019} \\
\addlinespace

Social and Behavioral Science & Sexism and Hate Speech Detection &\cite{mozafari2020hate, saleh2023detection, plaza2023overview, kirk2023semeval, mohammadi2023towards, mohammadi2024transparent, anjum2023hate}\\
 & Fake News and AI Generative Detection &\cite{capuano2023content, mohammadi_2023_10079010} \\
\addlinespace

Human Resources & Talent Acquisition and Recruitment &\cite{patwardhan2023transformers, gurrapu2023rationalization} \\
 & Employee Sentiment Analysis &\cite{jim2024recent, plucinski_overview_2022} \\
 & Performance Evaluation &\cite{chang2023natural, herrewijnen2023human} \\
 & Diversity and Inclusion &\cite{longo2023explainable} \\
\bottomrule
\end{longtable}
\end{scriptsize} 
}

Table~\ref{tab:applications} highlights some of the domains to which XNLP has been applied, including medicine, finance, systematic reviews, CRM, chatbots, conversational AI, and studies within these application domains. Each domain is subdivided into subdomains or specific applications, and each application is associated with relevant studies. 


\subsection{Medicine}

XNLP has emerged as a valuable tool in the medical domain, particularly in managing the vast amounts of unstructured data contained in EHRs. EHR data comprises textual information like medical histories, physician's notes, and medical narratives that are vital for various medical applications like heart failure prediction~\cite{choi_doctor_2016}. The interpretability of predictions from EHR data is crucial due to the life-impacting decisions that may be based on these predictions. Through XNLP, the logic behind diagnoses or treatment recommendations can be elucidated, aiding medical practitioners in understanding the model's predictions, which in turn supports informed and reliable decision-making in patient care~\cite{li_neural_2022}.

For instance,~\citet{choi_doctor_2016} used an interpretable predictive model to predict heart failures from EHR data, assigning significance to various features like medical codes to provide explainable risk factors~\cite{choi_doctor_2016}. Furthermore,~\citet{kang_eliie_2017} and, ~\citet{weng_medical_2017}used rule-based NLP models to extract clinical evidence from randomized controlled trials, making the extraction process transparent and traceable. Text mining and NLP techniques have been applied to numerous health applications involving text de-identification tools, clinical decision support systems, patient identification, disease classification, disease history, ICD10 classification, hospital readmission prediction, and chronic disease prediction~\cite{bagheri2021text}. Table~\ref{tab:summary1} provides a summary of XNLP applications in the medicine domain, outlining the models, explainability methods, datasets, and metrics used.

Moreover, XNLP facilitates the analysis of medical literature and research papers, enabling healthcare professionals to stay updated with recent developments and evidence-based practices. By explicating findings or recommendations in medical literature, XNLP bridges the gap between research and clinical practice, empowering practitioners to make informed decisions~\cite{moradi_deep_2022}.

{
%Table 2. Summary of XNLP Applications in Medicine
\begin{scriptsize}
\centering
\renewcommand{\arraystretch}{0.9}
\begin{longtable}{p{0.2cm} p{0.2cm} p{2cm} p{2.2cm} p{2.8cm} p{2cm} p{1.5cm}}
\caption{Summary of XNLP Applications in Medicine} \label{tab:summary1} \\
\toprule
\textbf{Paper} & \textbf{}  & \textbf{Application} & \textbf{Model} & \textbf{Explainability Method} & \textbf{Dataset} & \textbf{Metrics} \\ 
\midrule
\endfirsthead
\toprule
\textbf{Paper} & \textbf{} & \textbf{Application} & \textbf{Model} & \textbf{Explainability Method} & \textbf{Dataset} & \textbf{Metrics} \\ 
\midrule
\endhead

\cite{choi_doctor_2016} &  & Heart Failure Prediction & RNN & Feature Importance & EHRs & Accuracy,

AUC-ROC \\
\addlinespace
\cite{mullenbach2018explainable} &  & EHRs Classification & CNN & Rationale-based explanations & MIMIC-III & Precision, 

Recall, 

F1-score \\
\addlinespace
\cite{kang_eliie_2017} &   & RCT Analysis \& Extraction & Rule-based NLP & Transparent Rule Extraction & Clinical Trials & Extraction Accuracy \\
\addlinespace
\cite{zhang2019biowordvec} & & Biomedical Word Embeddings & MeSH term graph and fastText & Incorporation of MeSH & UMNSRS-Sim,

UMNSRS-Rel & Embedding Quality, Nearest Neighbors \\
\addlinespace
\cite{yang2021deep} &  & Disease Progression Prediction & Transformer & Attention Mechanism & Public EHRs & RMSE, MAE \\
\addlinespace
\cite{alabi2023machine} &  & Cancer Diagnosis & BERT & LIME and SHAP & Cancer Registry & F1-score, Precision, Recall \\

\bottomrule
\end{longtable}
\end{scriptsize}
}

When it comes to text data and NLP in the medical domain, though historically inclined towards BoW and rule-based models, there has been a shift towards deep learning-based approaches, including word embeddings and transformer-based methods. NLP and text mining techniques can be applied to create a more structured representation of text, making its content more accessible for data science, machine learning, and statistics, and for medical prediction models~\cite{Bagheri2023}. Our review of these studies reveals a substantial gap in the progress toward XNLP within the medical domain compared to other fields. Nonetheless, we suggest that researchers focus on bridging this gap by intensifying efforts toward integrating XNLP methods. This includes prioritizing the development of models that not only deliver high accuracy but also provide interpretable and transparent decision-making processes. Such an approach is crucial in healthcare, where understanding the rationale behind model predictions is as important as the predictions themselves. A multidisciplinary approach involving collaboration between medical experts, data scientists, and developers can ensure that these models are developed in a manner that is interpretable and clinically relevant. Recent studies have shown promise in using LLMs for mental health analysis, indicating how explainability in model outputs can enhance reliability in sensitive areas like healthcare~\cite{yang-etal-2023-towards}. This integration of XNLP methods can lead to applications such as clinical decision support systems, which enhance healthcare providers' ability to deliver safer care, reduce medical errors, and improve productivity, quality, and efficiency. Addressing issues like text de-identification and bias in AI systems is also crucial for ensuring the reliable and ethical application of XNLP in clinical settings~\cite{Bagheri2023}.

\subsection{Finance}

Recent research indicates that NLP applications in finance, such as analyzing financial reports and news articles, can identify potential risk factors and provide timely warnings and decision support. For instance, an NLP-based financial risk detection model can perform excellently in risk identification and prediction, offering effective tools for financial~\cite{wang2024application}. XNLP holds promise for risk assessment and fraud detection in the financial sector. It can assist in detecting adversarial behaviors in fraud detection models, providing companies and end users with more trust in the machine learning inferences~\cite{psychoula_explainable_2021}. Furthermore, user-centric XNLP methods have been developed to align fraud experts' tasks with explanation methods, enhancing the overall effectiveness of fraud detection systems~\cite{cirqueira_towards_2021}. So users, in turn, can understand the factors impacting risk assessment and take necessary actions to reduce risks.


%\subsubsection{Risk Assessment}
Risk assessment is a crucial facet of the financial sector, benefiting from XNLP. Financial institutions depend on comprehensive risk assessments to make critical decisions like granting loans or determining insurance premiums. Given the potential implications, these evaluations must be accurate, trustworthy, and, importantly, understandable~\cite{fritz-morgenthal_financial_2022, wallace2019nlp}. XNLP can assist by providing clear explanations for generated risk scores or credit evaluations, enabling users to understand the rationale behind their assessments and take corrective actions to reduce risk and enhance their credit scores. Thus, XNLP can boost client confidence in financial institutions~\cite{rudin2022interpretable}. For instance,~\citet{wallace2019nlp} introduced a graph-based attention model for credit risk assessment, shedding light on interactions and transactions impacting the final risk assessment and facilitating the understanding of credit decisions.

%\subsubsection{Fraud Detection}
Fraud detection is another domain within the financial sector that can benefit from XNLP. Conventionally, fraud detection systems have been opaque, leaving users clueless about why a particular transaction was flagged as potentially fraudulent. XNLP can offer clear insights into the features or patterns that triggered the fraud alert, making the process more transparent and reliable~\cite{psychoula_explainable_2021, fritz-morgenthal_financial_2022, varshney_safety_2017, cirqueira_towards_2021}. By enhancing the explainability of these systems, financial institutions can refine the accuracy of their fraud detection models and provide users with a better understanding of flagged transactions. This transparency can elevate user trust and engagement, and diminish the likelihood of false fraud alerts~\cite{fritz-morgenthal_financial_2022}.For instance, in a study by~\citet{varshney_safety_2017}, interpretable ML models were employed for financial fraud detection, attributing importance to different transaction features to explain fraud alerts. Similarly,~\citet{cirqueira_towards_2021} introduced design principles to align fraud experts' tasks with explanation methods for XAI decision support in fraud detection, enhancing the transparency and accountability of financial institutions.

%\subsubsection{Firm Valuation}
Additionally, the use of XNLP models in the firm valuation, which is another critical area in finance, can help stakeholders understand the factors driving valuation metrics, such as revenue projections, market trends, and financial ratios~\cite{bagga2023towards}. NLP models encounter challenges when used for firm valuation, particularly in distinguishing between relevant and irrelevant information. Recent literature emphasizes the crucial role of context in understanding human language, highlighting that the meaning of words and phrases can change based on their usage~\cite{abro2023natural}. This contextual ambiguity poses a major obstacle for NLP systems, which often struggle to understand the nuances of language, such as sarcasm or humor\footnote{\scriptsize\href{https://spectur.com.au/the-10-biggest-issues-in-natural-language-processing-nlp/}{\texttt {https://spectur.com.au/the-10-biggest-issues-nlp/}}}. In the field of financial reporting, this limitation could cause NLP models to misinterpret or overemphasize certain words or phrases in annual reports that human shareholders would easily overlook. The challenge is further complicated by long-range dependencies in text, where subtle relationships between words or concepts separated by large distances can lead to errors in comprehension~\cite{abro2023natural}. 

These issues create a potential vulnerability, where firms could theoretically ``game the system" by strategically inserting certain words into their reports, knowing that the NLP model might pick up on these irrelevant terms and alter its valuation accordingly. To tackle this, more advanced NLP techniques are required, like XNLP, which could better capture the delicate, context-dependent information that human shareholders rely on, as accurate firm valuation is essential for investment decisions, mergers and acquisitions, and financial reporting~\cite{bagga2023towards}. For this purpose,~\citet{bagga2023towards} proposed an XNLP system using transformers to analyze and interpret complex financial documents and valuation models, providing clear explanations for valuation outcomes. This approach enhances transparency and confidence in valuation processes by elucidating the underlying drivers and assumptions. Similarly, the use of explainable lexicons in sentiment analysis for financial texts has been shown to improve the accuracy and interpretability of sentiment-based valuation models. These models can identify the sentiment in financial news, reports, and market analyses, which impact firm valuation~\cite{rizinski2024sentiment}. In summary, institutions can fine-tune their AI systems by integrating explainability into these core functions within the finance sector, ensuring more precise and reliable results. Table~\ref{tab:summary2} showcases the details of applications in the finance domain, including the model, explainability method, dataset, and metrics used.

{
%Table 3. Summary of XNLP Applications in Finance

\begin{scriptsize} 
\centering
\renewcommand{\arraystretch}{0.9}
\begin{longtable}{p{0.1cm} p{0.2cm} p{2cm} p{2cm} p{2cm} p{2cm} p{2.5cm}}
\caption{Summary of XNLP Applications in Finance} \label{tab:summary2} \\
\toprule
\textbf{Paper} & \textbf{} & \textbf{Application} & \textbf{Model} & \textbf{Explainability Method} & \textbf{Dataset} & \textbf{Metrics} \\ 
\midrule
\endfirsthead
\toprule
\textbf{Paper} & \textbf{} & \textbf{Application} & \textbf{Model} & \textbf{Explainability Method} & \textbf{Dataset} & \textbf{Metrics} \\ 
\midrule
\endhead

    %1
    \cite{ghai2021explainable} & & Risk Assessment Classification & BERT & Layer-wise relevance propagation & 20 Newsgroups & Accuracy, 
    
    F1-score, 
    
    Precision, Recall \\
    \addlinespace
    %2
    \cite{wallace2019nlp} & & Credit Risk Assessment & Graph-based Attention & Probing Methodology & Financial Transactions & Evaluation Metrics \\
    %3
    \addlinespace
    \cite{varshney_safety_2017} & & Fraud Alert Explanation & ML Models (e.g., Decision Trees, SVM) & Feature Importance & Transaction Data & Precision, Recall, AUC \\
    %4
    \addlinespace
    \cite{cirqueira_towards_2021} & & Fraudulent Transactions Justification & Explainable AI Methods & Explanation Generation & Financial Transactions & Accuracy, 
    
    F1-score, 
    
    Precision, Recall \\
    %5
    \addlinespace
    \cite{bagga2023towards} & & Firm Valuation & Transformer-based Model & Explanation Generation & Financial Documents & ROUGE, BLEU \\
    %6
    \addlinespace
    \cite{rizinski2024sentiment} & & Sentiment Analysis for Valuation & Explainable Lexicon Model & SHAP Explainability & Financial Texts & Accuracy, 
    
    F1-score, 
    
    Precision, Recall \\
    
\bottomrule
\end{longtable}
\end{scriptsize} 
}

The integration of XNLP in the finance sector marks a advancement in risk assessment, fraud detection, and firm valuation. With its ability to provide transparent explanations for risk scores, credit assessments, fraud alerts, and valuation metrics, XNLP increases trust and accountability in financial institutions. As our review of various models and techniques indicates, the field is evolving towards more complicated, interpretable AI systems. These systems not only improve the precision of financial models but also empower users with insights into decision-making processes. Moving forward, the focus should be on further refining XNLP tools to ensure they are robust, transparent, and user-friendly, thereby fostering an environment of trust and clarity in financial decision-making. This approach will be instrumental in advancing the reliability and efficacy of AI applications in the finance sector, ultimately contributing to more secure and transparent financial services.

\subsection{Systematic Reviews}

Systematic reviews use research literature to inform evidence-based decision-making across various fields. Key aspects of the systematic review process, including identifying relevant studies, extracting data, and summarizing findings, can be automated through specialized applications. For example, \citet{van_de_schoot_open_2021} developed ASReview\footnote{\scriptsize\url{https://asreview.nl/}}, an open-source tool designed to support study selection in systematic reviews. XNLP techniques offer additional benefits by helping researchers understand the rationale behind the inclusion or exclusion of studies. By incorporating explainability, such tools provide insight into why certain studies are recommended, aiding in study selection decisions. NLP models further enhance the efficiency and reliability of systematic reviews by clarifying the criteria used for study inclusion and the relevance of extracted information~\cite{rosemblat_towards_2019}.

The growing volume of scientific publications has heightened the need for automating systematic reviews. XNLP's application can accelerate the process and reduce researchers' workload by automating the identification and selection of relevant studies. This is achieved by deploying NLP models that filter through databases and select articles based on their relevance to the review question~\cite{van_de_schoot_open_2021, marshall_robotreviewer_2016}. For instance, RobotReviewer\footnote{\scriptsize\url{https://www.robotreviewer.net/}}, an AI-based tool developed by \citet{marshall_robotreviewer_2016}, assists in the study selection phase of systematic reviews. By integrating explainability features, the tool provides researchers with insights into why specific studies were recommended, supporting the decision-making process for inclusion. Table~\ref{tab:summary3} outlines various applications in systematic reviews, detailing the models, explainability methods, datasets, and metrics employed.


{%Table 4. Summary of XNLP Applications in Systematic Reviews
\begin{scriptsize}
\centering
\renewcommand{\arraystretch}{0.9}
\begin{longtable}{p{0.2cm} p{0.1cm} p{1.7cm} p{2.5cm} p{2.2cm} p{1.9cm} p{2.2cm}}

\caption{Summary of XNLP Applications in Systematic Reviews} \label{tab:summary3} \\
\toprule
\textbf{Paper} & \textbf{} & \textbf{Application} & \textbf{Model} & \textbf{Explainability Method} & \textbf{Dataset} & \textbf{Metrics} \\ 
\midrule
\endfirsthead
\toprule
\textbf{Paper} & \textbf{} & \textbf{Application} & \textbf{Model} & \textbf{Explainability Method} & \textbf{Dataset} & \textbf{Metrics} \\ 
\midrule
\endhead

    \cite{rosemblat_towards_2019} &  & Review Automation & Support Vector Machine (SVM) & Explanation Framework & PubMed & Work Saved over Sampling (WSS), Relevant References Found (RRF) \\
    \addlinespace
    %~\cite{van_de_schoot_open_2021} &  & Review Automation & ASReview (Active Learning) & Explanation Framework & Various Databases & Precision, Recall, Time Efficiency \\
    
    \cite{marshall_robotreviewer_2016} &  & Bias Assessment & RobotReviewer & Text Analysis for Bias Detection & Clinical Trials  & Bias Assessment Metrics, Accuracy \\
    
    \cite{chen2018learning} &  & Model Interpretation & Long Short-Term Memory (LSTM) & Information Bottleneck Method & SST-2, IMDb & Accuracy, Mutual Information \\ 


\bottomrule
\end{longtable}
\end{scriptsize}
}

XNLP enhances the efficiency, transparency, and reliability of systematic reviews. Tools like ASReview showed the potential of AI in this domain, offering insights into study inclusion decisions and increasing time efficiency~\cite{teijema2023large, teijema2023simulation}. As the number of scientific publications continues to grow, the adoption of XNLP in systematic reviews becomes increasingly vital, promising a more simplified, accurate, and transparent approach to evidence synthesis.


\subsection{Customer Relationship Management (CRM)}

XNLP is vital for extracting and summarizing key data from identified studies. Text summarization models can understand the content of a study and condense it into a summary encapsulating the major findings. By introducing explainability, researchers can understand why certain information was included or excluded from the summary, thereby promoting transparency and facilitating comprehension~\cite{nye_corpus_2018}. For extractive summarization of scientific articles,~\citet{nye_corpus_2018} used BERT and applied attention visualization to explain which original sentences contributed to the final summary, thereby offering a valuable tool for interpreting the model's decisions. XNLP enhances the efficiency, transparency, and accountability of the systematic review process. Understanding the model's decision-making process allows researchers to have confidence in the automated process, leading to more reliable and unbiased reviews.

Employing NLP, sentiment analysis disclose emotions or sentiments from textual data sources such as social media posts or customer reviews. This powerful tool has become integral to CRM, providing companies with a delicate understanding of their consumers' sentiments. Through sentiment analysis, businesses can not only gauge public opinion but also refine their customer engagement strategies and optimize service delivery. However, integrating explainability into sentiment analysis models remains a challenge~\cite{capuano2021sentiment, du_techniques_2019, bacco_explainable_2021}. ~\citet{du_techniques_2019} and ~\citet{bacco_explainable_2021} presented recent research on incorporating explainability into sentiment analysis models. They employed Layer-wise Relevance Propagation (LRP) and developed a model for sentiment analysis respectively, enabling the models to provide clear explanations for sentiment predictions.

In addition to sentiment analysis, customer support automation is another CRM area that benefits greatly from XNLP. As a result of their ability to respond instantly to customer inquiries, chatbots, and other automated customer service systems are gaining popularity. Incorporating explainability into these systems can improve their efficacy by elucidating the reasoning behind particular responses or recommendations~\cite{bar-haim_arguments_2020, jenneboer2022impact}. ~\citet{jenneboer2022impact} showed explainability in a customer service chatbot, providing customer transparency by elucidating the bot's decision-making process, thereby improving customer relations. In conclusion, integrating XNLP into CRM can lead to deeper understanding, improved customer relations, and more informed business decisions. Table~\ref{tab:summary4} showcases the application details in the CRM domain, including the model, explainability method, dataset, and metrics used.

{%Table 5. Summary of XNLP Applications in CRM
\begin{scriptsize}
\centering
\renewcommand{\arraystretch}{0.9}
\begin{longtable}{p{0.1cm} p{0.1cm} p{2cm} p{2cm} p{2.4cm} p{2.2cm} p{2cm}}

\caption{Summary of XNLP Applications in CRM} \label{tab:summary4} \\
\toprule
\textbf{Paper} & \textbf{} & \textbf{Application} & \textbf{Model} & \textbf{Explainability Method} & \textbf{Dataset} & \textbf{Metrics} \\ 
\midrule
\endfirsthead
\toprule
\textbf{Paper} & \textbf{} & \textbf{Application} & \textbf{Model} & \textbf{Explainability Method} & \textbf{Dataset} & \textbf{Metrics} \\ 
\midrule
\endhead

    \cite{nye_corpus_2018} & & Text Summarization & BERT & Attention Visualization & Corpus of biomedical articles & ROUGE, BLEU \\
    \addlinespace
    \cite{capuano2021sentiment} & & Sentiment Analysis & BERT & LIME & Yelp Reviews & Accuracy, F1-score \\
    \addlinespace
    \cite{du_techniques_2019} & & Sentiment Analysis & BiLSTM & Layer-wise Relevance Propagation& Amazon Reviews & Accuracy, Precision, Recall \\
    \addlinespace
    \cite{bacco_explainable_2021} &  & Sentiment Analysis & BiLSTM & SHAP & Movie Reviews & Accuracy, F1-score, Precision \\
    \addlinespace
    \cite{bar-haim_arguments_2020} & & Customer Support Automation & Seq2Seq & Explanation-by-Example & Customer Service Logs & Customer Satisfaction Score \\
    \addlinespace
    \cite{jenneboer2022impact} & & Customer Support Automation & Transformer-based Chatbot & Attention Visualization & Customer Service Logs & Response Time, User Satisfaction \\
    \addlinespace
    \cite{lei_rationalizing_2016} &  & Text Classification, Information Extraction & LSTM & Rationale-based explanations & Beer Review Dataset, CoNLL-2003 & Accuracy, F1-score \\ 
    \addlinespace
    \cite{lage2018human}  & & Sentiment Analysis & BERT & Feature Visualization & Yelp Polarity  & Accuracy, Precision, Recall, F1-score \\
    \addlinespace
    \cite{tsai2023faith} & & Review Automation & LSTM & Shapley Interaction Index & IMDb & Accuracy, F1-score, Precision, Recall \\
    \addlinespace
    \cite{rana2022rerrfact}  &  & Fact Verification & BiGRU, 
    
    Attention & Attention Heatmaps, Explainable Fact-Checking & LIAR, PolitiFact & Accuracy, Precision, Recall, F1-score \\
    \addlinespace
    \cite{yu2021understanding} &  & Sentiment Analysis & CNN, BiGRU & Attention Heatmaps, Ablation Studies & Amazon Reviews, BeerAdvocate & Accuracy, Precision, Recall, F1-score \\
    \addlinespace
    \cite{Antognini2021RationalizationTC} & & Sentiment Analysis & Reinforcement Learning, RNN & Rationalization Generation & TripAdvisor Reviews, RateBeer & Accuracy, F1-Score \\
    
\bottomrule
\end{longtable}   
\end{scriptsize}
}

The integration of XNLP into CRM marks a advance in understanding and influencing customer interactions and sentiments. From enhancing text summarization in study extractions to refining sentiment analysis in customer feedback, XNLP brings a new level of transparency and efficiency to CRM. This advancement not only aids in automating and simplifying CRM processes but also ensures that the extracted insights are clear and interpretable. The examples of BERT for text summarization and various models for sentiment analysis underscore the potential of XNLP in CRM. By making the decision-making process more transparent, XNLP empowers businesses to make stronger, more informed relationships with their customers, ultimately driving better business outcomes.


\subsection{Chatbots and Conversational AI}

The prevalence of chatbots and conversational AI systems has increased in numerous applications, such as virtual assistants, customer support, and information retrieval. XNLP techniques can increase the effectiveness and transparency of these systems by improving explanations for the responses or recommendations generated. By comprehending the logic behind the chatbot's responses, users can place more faith in the system's suggestions and feel more engaged in the conversation. In addition, context-aware recommendations that explain personalized recommendations can increase user satisfaction and empower them to make informed decisions.~\citet{coppo_artificial_2019} describe a real-world application of a chatbot that justifies its responses, thereby fostering user trust and engagement.

Context-aware recommendation systems can provide personalized suggestions by considering the context of the user's interaction. Incorporating XNLP into these systems can increase user satisfaction and confidence by clarifying why particular recommendations are made. For example,~\citet{zhang_deep_2019} proposed a context-aware recommendation model that employs XNLP to justify its recommendations. Their model incorporates the context of user-item interactions and provides clear explanations, which improves user satisfaction.

In 2023, with the advent of GPT-4, Conversational AI has seen a leap forward. The larger context window and improved accuracy of GPT-4 enable more coherent and contextually accurate interactions with users. OpenAI's latest model, GPT-4o (``o" for ``omni"), represents another substantial advancement in the field. GPT-4o matches GPT-4 Turbo performance on text in English and code, while showing improvement on text in non-English languages~\cite{techtarget2024context}. These advancements in context understanding and multimodal processing contribute to more delicate and accurate conversational AI interactions, potentially addressing some of the explainability challenges by providing more comprehensive and context-aware responses~\cite{openai2024mini, openai2024hello, techtarget2024gpt4o}. 

Despite advancements in AI chatbots and their ability to enhance user interactions, recent studies highlight a critical need for explainable AI to further improve transparency and effectiveness in AI chatbots, consequently enhancing trust and user engagement~\cite{matellio2024, seminck2023conversational, miglani-etal-2023-using}. While AI chatbots have showed their capacity to assist users with quick and personalized responses, integrating XNLP into chatbots and conversational AI systems can provide users with a deeper understanding of the reasoning behind each response and recommendation. This can, in turn, promote transparency in decision-making processes, build stronger trust between users and AI systems, and potentially enhance overall user satisfaction. As these systems continue to evolve and increase in complexity, the demand for effective explanation techniques will increase, making XNLP a crucial aspect of the future of conversational AI~\cite{soni2024impact, dxwand2024, threado2024}. Table~\ref{tab:summary5} showcases the details of applications in the chatbots and conversational AI domain, including the model, explainability method, dataset, and metrics used.

{%Table 6 Summary of XNLP Applications in Chatbots and Conversational AI
\begin{scriptsize}
\centering
\renewcommand{\arraystretch}{1}
\begin{longtable}{p{0.1cm} p{0.1cm} p{2cm} p{1.5cm} p{2cm} p{3cm} p{2cm}}
\caption{Summary of XNLP Applications in Chatbots and Conversational AI} \label{tab:summary5} \\
\toprule
\textbf{Paper} & \textbf{} & \textbf{Application} & \textbf{Model} & \textbf{Explainability Method} & \textbf{Dataset} & \textbf{Metrics} \\ 
\midrule
\endfirsthead
\toprule
\textbf{Paper} & \textbf{} & \textbf{Application} & \textbf{Model} & \textbf{Explainability Method} & \textbf{Dataset} & \textbf{Metrics} \\ 
\midrule
\endhead
  
    \cite{jain_attention_2019} & & Conversational Agents & BiLSTM, CNN & Attention 
    
    distributions & SST-5 & Accuracy, 
    
    F1-score \\
    \addlinespace
    \cite{bar-haim_arguments_2020} & & Automated Customer Service Systems & Transformer models & Justifications for Responses & IMDb & Efficacy \\
    \addlinespace
    \cite{coppo_artificial_2019} & & Chatbots & Various & Explainability & AG News & User Trust and Engagement \\
    \addlinespace
    \cite{zhang_deep_2019} & & Recommendation Systems & Deep Learning-based & Attention Mechanism & Various datasets including MovieLens, Amazon product data & User 
    
    Satisfaction \\ 
    \addlinespace
    \cite{seminck2023conversational} & & Conversational Agents & GPT-4 & Transparency & Various datasets including MovieLens, Amazon, Goodreads & User 
    
    Engagement, Accuracy \\ 
    \addlinespace
    \cite{matellio2024} & & Chatbots 
    
    in Business & Various & Transparency in decision-making & Not specified (general business data) & User Trust, Satisfaction \\
    \addlinespace
    \cite{ghazvininejad2018knowledge} & & Task-oriented Dialogue 
    
    Systems & SEQ2SEQ & External Database 
    
    Integration & MovieLens, Reddit & Response 
    
    Accuracy \\ 
    
\bottomrule
\end{longtable}
\end{scriptsize}
}

\subsection{Social and Behavioral Science}

%\subsubsection{Sexim and Hate speak detection}
In the domain of social and behavioral science, the application of XNLP is an important tool for addressing complex issues like hate speech, sexism, and misinformation on social media platforms. Notably, transformer models such as BERT have been vital in identifying offensive content, including racism and sexism. For instance,~\citet{mozafari2020hate} used BERT to analyze datasets annotated for such content, enhancing detection accuracy and explainability. Additionally, domain-specific word embedding models, as explored by~\citet{saleh2023detection}, have been trained on data from hate speech-centric websites and tweets, offering delicate insights into the language of hate speech. To tackle sexism on social networks, initiatives like the sEXism Identification in Social neTworks (EXIST) or SemEval-2023 Task 10 competition have developed machine learning methods for automatic identification of sexist content~\cite{plaza2023overview, kirk2023semeval}. Machine learning algorithms, as applied by~\citet{mohammadi2023towards}, classify sexism in social media using datasets like EXIST-2023, which focuses on both English and Spanish content, showing the multilingual capabilities of XNLP. In another study, they applied XAI techniques, and the model provided insights into the influence of individual tokens and model components on its decision-making process~\cite{mohammadi2024transparent}. Further, incorporating a feedback loop involving human validation aligns the model's outcomes with human cognition. Furthermore, comprehensive surveys, such as the one by~\citet{anjum2023hate}, have researched hate speech detection methodologies across various online platforms, highlighting the role of XNLP in understanding and reducing such societal challenges.

%\subsubsection{Fake news and AI generative detection}
In parallel, the detection of fake news and AI-generated content has become a critical area of focus. Systematic reviews have emphasized the role of NLP and AI in automating the detection process and reducing the dependency on labor-intensive manual methods.~\citet{capuano2023content} highlighted how well XNLP works to differentiate between real and fake content, emphasizing how XNLP can support digital information integrity. Additionally, In the shared task organized by Computational Linguistics in the Netherlands\footnote{\scriptsize\url{https://sites.google.com/view/shared-task-clin33/home}} (CLIN), an ensemble model was developed by~\citet{mohammadi_2023_10079010} to detect whether texts were generated by AI or humans across different genres in both English and Dutch~\cite{fivez2024clin33}. Furthermore, integrating multimodal approaches in fake news detection, such as the SceneFND model, combines textual, contextual scene, and visual representations to address the problem of fake news detection more comprehensively. This model uses scene context information from images, showing improved performance over textual baselines by margins on datasets like PolitiFact and GossipCop. This approach exemplifies the advancements in using multimodal data to enhance the accuracy of identifying misinformation online~\cite{zhang2024scenefnd}.

%using \texttt{\small bert-base-multilingual-uncased}\footnote{\url{https://huggingface.co/google-bert/bert-base-multilingual-uncased}}, \texttt{\small xlm-roberta-base}\footnote{\url{https://huggingface.co/docs/transformers/en/model_doc/xlm-roberta}}, and \texttt{\small distilbert-base-multilingual-cased}\footnote{\url{https://huggingface.co/distilbert/distilbert-base-multilingual-cased}} structures

In the field of psychological studies, XNLP has been beneficial in analyzing emotional and mental health trends on online platforms. For instance, a study by~\citet{han2023sensing} used XNLP techniques to assess emotional well-being based on social media posts, showing predictive power in assessing psychological well-being (PWB) using linguistic features extracted from social media language. Similarly, \citet{AHMED2024} applied transformer-based models to classify depression severity from social media data, demonstrating the potential of XNLP in detecting and categorizing mental health conditions. Additionally, in the realm of social media analysis, XNLP tools have been used to understand user behavior and trends. A study by~\citet{ali2022large} applied XNLP to analyze political sentiment on Twitter during the 2020 US Presidential Elections, offering key insights into public opinion dynamics and the importance of real-time sentiment analysis.


{
%Table 7. Summary of XNLP Applications in Social and Behavioral Science 
\begin{scriptsize}
\centering
\renewcommand{\arraystretch}{1}
\begin{longtable}{p{0.1cm} p{0.1cm} p{2cm} p{2.3cm} p{2cm} p{2.3cm} p{1.9cm}}

\caption{Summary of XNLP Applications in Social and Behavioral Science } \label{tab:summary6} \\
\toprule
\textbf{Paper} & \textbf{} & \textbf{Application} & \textbf{Model} & \textbf{Explainability Method} & \textbf{Dataset} & \textbf{Metrics} \\ 
\midrule
\endfirsthead
\toprule
\textbf{Paper} & \textbf{} & \textbf{Application} & \textbf{Model} & \textbf{Explainability Method} & \textbf{Dataset} & \textbf{Metrics} \\ 
\midrule
\endhead

\cite{mozafari2020hate} &  & Hate Speech Detection & BERT & Transformer Models & Annotated Datasets & Detection 

Accuracy \\
\addlinespace
\cite{saleh2023detection} &  &Hate Speech Analysis & Domain-Specific Models & Word 

Embedding & Hate Speech 

Websites & Delicate Language Insights \\
\addlinespace
\cite{plaza2023overview} &  & Sexism Detection in Social Media & ML Algorithms & EXIST Competition & EXIST-2023 Dataset & Classification Accuracy \\
\addlinespace
\cite{mozafari2020hate} &  & Hate Speech Detection & BERT \& Bias Mitigation Module & Bias Mitigation Mechanism & Davidson and Waseem datasets & F1-measure \\
\addlinespace
\cite{saleh2023detection} &  & Hate Speech Detection & BERT \& Deep Models & LIME & Hate Speech Word Embedding & Detection 

Accuracy \\
\addlinespace
%\cite{plaza2023overview} &  & Sexism Detection in Social Media & Machine Learning Algorithms & N/A & EXIST-2023 Dataset & Classification Accuracy \\

\cite{kirk2023semeval} &  & Explainable Detection of Online Sexism & BERT & SHAP & SemEval-2023 Task 10, EXIST-2023 Dataset & F1 score \\
\addlinespace
\cite{mohammadi2024transparent} &  & Explainable Sexism 

Detection & Ensemble Model (BERT, XLM-RoBERTa, DistilBERT) & SHAP & EXIST-2023 Dataset & Token Influence Analysis \\
\addlinespace
%\cite{capuano2023content} &  & Fake News Detection in Digital Media & Systematic Review & N/A & Various Sources & Evaluation of Methods \\

\cite{mohammadi_2023_10079010} &  & AI-Generated Text Detection & Ensemble Model and Multilingual BERT & SHAP & Various Genres & Detection 

Accuracy \\
\addlinespace
\cite{han2023sensing} &  & Mental Health Analysis & NLP Techniques & Feature Importance Analysis & Social Media 

Posts & Predictive Power \\

\bottomrule
\end{longtable}
\end{scriptsize}
}


Combining XNLP with social and behavioral science has proven to be a game-changing approach to solving many of society's problems. XNLP has improved the accuracy, speed, and depth of studies that identify and analyze hate speech, sexism, and false information on social media and distinguish between AI-generated and real content~\cite{mathew2021hatexplain, yang2023hareexplainablehatespeech}. The use of advanced transformer models, domain-specific word embedding models, and multilingual datasets has enabled a more detailed understanding and detection of problematic material. XNLP's application in psychology research and sentiment analysis also highlights its usefulness in assessing emotional states and public opinion by analyzing online discourse. These developments not only showcase the technological advancements of NLP but also underscore the growing importance of AI in providing valuable insights into complex social and behavioral phenomena. As XNLP continues to evolve, it will become an even more indispensable tool in bridging cutting-edge technology with social science research.

\subsection{Human Resources (HR)}

In the realm of Human Resources (HR), the application of XNLP has emerged as a pivotal tool for optimizing various HR functions, ranging from talent acquisition to employee sentiment analysis. Recent studies have explored the utilization of advanced NLP techniques to enhance the transparency and effectiveness of HR processes.

%\subsection{Talent Acquisition and Recruitment}

One of the applications of XNLP in HR is in the recruitment process. Transformer models, such as BERT and GPT, are extensively used to automate the resume screening process. These models can parse resumes and rank candidates based on the job description, ensuring a better match between the candidate’s skills and the job requirements. For instance, a study by~\citet{patwardhan2023transformers} showed the application of BERT in automating the initial stages of recruitment, reducing the time and bias associated with manual screening processes. The study highlighted the use of explainable AI techniques to provide insights into the model’s decision-making process, thereby enhancing trust in automated recruitment systems. Similarly, recent advancements have shown the effectiveness of transformer models in improving recruitment efficiency and reducing biases through explainable mechanisms~\cite{gurrapu2023rationalization}. In line with this, \citet{AKKASI2024} demonstrated the use of a transformer-based ensemble to extract both technical and non-technical skills from job descriptions, significantly enhancing the precision and transparency of the recruitment process.

%\subsection{Employee Sentiment Analysis}

Employee sentiment analysis is another area where XNLP is making substantial contributions. By analyzing text data from employee feedback, surveys, and social media, organizations can gauge employee morale and job satisfaction. A study by~\citet{jim2024recent} showcased the use of RoBERTa, a transformer-based model, to analyze large volumes of employee feedback data. The model provided detailed insights into employee sentiments, helping HR departments to identify potential issues and areas for improvement. The explainability of the model ensured that HR managers could understand the key factors influencing employee sentiments. Additionally, the integration of attention mechanisms has enhanced the interpretability of sentiment analysis models, allowing HR practitioners to visualize the factors influencing sentiment classifications~\cite{plucinski_overview_2022}.

%\subsection{Performance Evaluation}

XNLP techniques are also being used to enhance performance evaluation processes. Traditional performance evaluations often suffer from subjectivity and bias. By employing NLP models to analyze qualitative feedback and performance reviews, organizations can obtain a more objective assessment of employee performance. The study by~\citet{chang2023natural} on the use of transformer models for performance evaluation highlighted the importance of explainability in ensuring that the evaluations are fair and transparent. The study used attention mechanisms to identify the most relevant parts of the feedback, providing a clear rationale for the performance scores assigned. Recent research has further validated the use of explainable models to improve the fairness and transparency of performance evaluations~\cite{herrewijnen2023human}.

%\subsection{Diversity and Inclusion}

Promoting diversity and inclusion within the workplace is another critical area where XNLP is being applied. NLP models can analyze job postings, company policies, and internal communications to identify potential biases and suggest improvements. Explainable AI techniques in these models ensure that the recommendations for enhancing diversity and inclusion are transparent and justifiable~\cite{longo2023explainable}.

{
%Table 8. Summary of XNLP Applications in HR
\begin{scriptsize}
\centering
\renewcommand{\arraystretch}{0.9}
\begin{longtable}{p{0.1cm} p{0.1cm} p{2.2cm} p{2cm} p{2cm} p{2.3cm} p{2cm}}

\caption{Summary of XNLP Applications in HR } \label{tab:summaryHR} \\
\toprule
\textbf{Paper} & \textbf{} & \textbf{Application} & \textbf{Model} & \textbf{Explainability Method} & \textbf{Dataset} & \textbf{Metrics} \\ 
\midrule
\endfirsthead
\toprule
\textbf{Paper} & \textbf{} & \textbf{Application} & \textbf{Model} & \textbf{Explainability Method} & \textbf{Dataset} & \textbf{Metrics} \\ 
\midrule
\endhead

\cite{patwardhan2023transformers} &  & Recruitment Automation & BERT & Attention Mechanisms & Resume Data & Match Accuracy \\
\addlinespace
\cite{jim2024recent} &  & Employee Sentiment Analysis & RoBERTa & Attention Mechanisms & Employee Feedback & Sentiment Insights \\
\addlinespace
\cite{chang2023natural} &  & Performance Evaluation & Transformer Models & Explainable AI & Performance Reviews & Objectivity Scores \\
\addlinespace
\cite{plucinski_overview_2022} &  & Sentiment Analysis & RoBERTa & Attention Mechanisms & Employee Feedback & Sentiment Accuracy \\
\addlinespace
\cite{gurrapu2023rationalization} &  & Recruitment & BERT & Attention Visualization & Resume Data & Bias Reduction \\
\addlinespace
\cite{herrewijnen2023human} &  & Performance Evaluation & Transformer Models & Explainable AI & Performance Reviews & Fairness Scores \\
\addlinespace
\cite{longo2023explainable} &  & Diversity & Custom NLP & Explainable AI & Company Policies & Bias Detection \\

\bottomrule
\end{longtable}
\end{scriptsize}
}


The integration of XNLP techniques within HR has proven to be transformative across several critical functions. The utilization of advanced transformer models such as BERT, GPT, and RoBERTa has streamlined processes like recruitment, employee sentiment analysis, performance evaluation, and the promotion of diversity and inclusion. By using explainable AI, organizations not only enhance the efficiency and effectiveness of these processes but also ensure greater transparency and fairness. Studies have shown that explainability mechanisms, including attention mechanisms and rationales, play a crucial role in making model decisions understandable and justifiable to human users. As the field continues to evolve, the ongoing development of explainable models is expected to further optimize HR functions, fostering an environment of trust and inclusivity.

\subsection{Other Applications}

XNLP finds applications in various domains further than what was discussed before. The explainability in NLP models helps in understanding the underlying processes and decisions made by these models, which is crucial in tasks like language generation, text classification, machine translation, summarization, visual question answering, and many others, as shown in Table~\ref{tab:summary7}. In the domain of language generation,~\citet{sheng2019woman} used Transformer models like GPT-2 to visualize attention in order to detect biases in language models.

On the other hand,~\citet{deyoung2019eraser} explored rationale-based explanations for text classification tasks across multiple datasets, including BoolQ and e-SNLI, evaluating the explanations on metrics like fidelity, comprehensiveness, and sufficiency. ~\citet{jacovi2020towards} and~\citet{ayyubi2020generating} extended the explainability to machine translation, summarization, and visual question answering. They employed transformer models and multimodal transformer models, respectively, with the former focusing on faithfulness in NLP models through question answering and the latter on rationale generation to improve answer accuracy and explanation quality.

Furthermore,~\citet{he2019towards} and~\citet{alishahi2019analyzing} explored explainability in neural machine translation and machine reading comprehension, employing models like BERT and Transformer-based models to annotate word importance and generate explanations, which were evaluated on various datasets like 20NG, AGNews, IMDB, SQuAD 1.1, and e-SNLI. In the realm of natural language understanding and visual analysis of Transformer models,~\citet{zhou2020towards} and~\citet{hoover-etal-2020-exbert} employed BERT and Transformer models, including BERT, to visualize activations and attention and analyze learned self-attention mechanisms.

Explainability also extends to commonsense reasoning, visual commonsense reasoning, and interpretable NLP as explored by~\citet{lakhotia2020fid}, ~\citet{tang2021cognitive}, and~\citet{wiegreffe2020measuring}. They proposed various models and methods like FiD-Ex, a dynamic working memory-based cognitive visual commonsense reasoning network (DMVCR), and T5-based joint models to generate rationales and predict answers with explanations. Lastly, translation quality estimation and text classification were explored by~\citet{plyler2021making},~\citet{fomicheva-etal-2022-translation}, and~\citet{chan2022unirex}. They employed models like Transformer, BERT-based models, and UniREX (Uni-modal Rationalization and EXplanation) to predict post-editing efforts, visualize features, and provide universal explainability in AI.

{
%Table 9. Summary of XNLP Applications in Other Domains
\begin{scriptsize}
\centering
\renewcommand{\arraystretch}{0.9}
\begin{longtable}{p{0.1cm} p{0.1cm} p{2cm} p{2cm} p{1.8cm} p{2.5cm} p{2.3cm}}
\caption{Summary of XNLP Applications in Other Domains} \label{tab:summary7} \\
\toprule
\textbf{Paper} & \textbf{} & \textbf{Task} & \textbf{Model} & \textbf{Explainability Method} & \textbf{Dataset} & \textbf{Metrics} \\ 
\midrule
\endfirsthead
\toprule
\textbf{Paper} & \textbf{} & \textbf{Task} & \textbf{Model} & \textbf{Explainability Method} & \textbf{Dataset} & \textbf{Metrics} \\ 
\midrule
\endhead
    \cite{sheng2019woman} &  & Language Generation& Transformer models (GPT-2, etc.) & Attention Visualization & WebText & Automated bias metrics \\
    \addlinespace
    \cite{he2019towards} &  &  Neural Machine Translation (NMT)&  BERT & Word Importance Annotation & Various: 20NG, AGNews, IMDB, etc. & Accuracy, F1 Score, etc. \\
    \addlinespace
    \cite{deyoung2019eraser} &   &Text Classification and Rationale-Based Explanations &  Various models & Rationale-based explanations & Multiple Datasets including BoolQ, e-SNLI, etc. & Fidelity, Comprehensiveness, Sufficiency \\
    \addlinespace
    \cite{jacovi2020towards} &  &  Machine Translation, Summarization& Transformer models & Question Answering for Faithfulness & CNN/Daily Mail, XSum, Multi30K & Fidelity score \\
    \addlinespace
    \cite{ayyubi2020generating} &  & Visual Question Answering& Multimodal Transformer model & Rationale Generation & VQA v2.0 and VizWiz & Answer Accuracy, Explanation Quality \\
    \addlinespace
    \cite{alishahi2019analyzing} &  & Machine Reading Comprehension & Transformer-based & Explanation Generation & SQuAD 1.1, e-SNLI & Fidelity, Sensibility, Sufficiency \\
    \addlinespace
    \cite{zhou2020towards} &  & Natural Language Understanding &  BERT & Visualization of activations and attention & BERT-based tasks' datasets & N/A \\
    \addlinespace
    \cite{hoover-etal-2020-exbert} &  & Visual Analysis of Transformer Models& Transformer models (including BERT) & Visualization and analysis of learned self-attention mechanisms & Not specified & Not specified \\
    \addlinespace
    \cite{lakhotia2020fid} & & Extractive Rationale Generation& LSTM, BERT & FiD-Ex & VCR & Exact Match Accuracy, Rationale F1 \\
    \addlinespace
    \cite{tang2021cognitive} &  & Visual Commonsense Reasoning & DMVCR & Utilizes a dynamic working memory & VCR, RevisitedVQA, BottomUpTopDown, MLB, MUTAN, R2C & Datasets related metrics \\
    \addlinespace
    \cite{wiegreffe2020measuring} & & Free-text Rationale Generation& T5-based Joint Models & Free-text Natural Language Rationales & Commonsense Question-Answering, Natural Language Inference & Robustness Equivalence, Feature Importance Agreement \\
    \addlinespace
    \cite{plyler2021making} &  & Neural Machine Translation & Transformer & LIME, Integrated Gradients & MLQE-PE & Pearson correlation, Spearman correlation, MAE \\
    \addlinespace
    \cite{fomicheva-etal-2022-translation} &  &  Neural Machine Translation& BERT-based models & Feature Visualization & WMT Metrics Shared Task & Pearson correlation, Kendall's tau \\
    \addlinespace
    \cite{chan2022unirex} & & Text Classification & UniREX & Rationalization & FEVER, Movie Reviews & Metric scores (P, R, F1), Explanation Quality \\

\bottomrule
\end{longtable}
\end{scriptsize}
}


In conclusion, XNLP's applicability across diverse domains underscores its impact beyond the applications mentioned before, while in these applications it plays a more important role because of their sensitive nature, like the medical field. The connection of explainability within NLP models is crucial in understanding their underlying mechanisms across various tasks such as language generation, text classification, machine translation, and more. This feature not only enhances transparency in AI-driven processes but also improves trust and reliability in outcomes, especially in critical areas like language bias detection, rationale generation, and visual question answering. The advancements in XNLP thus represent a crucial step towards more accountable and understandable AI applications across various fields.

According to the applications and studies that we covered, when examining the landscape of XNLP across various domains, some attractive comparisons can be made in terms of explainability techniques and their applications. In the medical field, where risks are high and accuracy is crucial, the focus appears to be on clear, actionable insights using techniques such as feature importance, transparent rule extraction, and rationale-based explanations. This contrasts with finance, where risk assessment and fraud detection hinge on models like BERT combined with techniques like layer-wise relevance propagation and attention mechanisms. The attention mechanism's popularity in finance might be attributed to its ability to highlight critical features in vast financial transactions, making anomalous activities more discernible.

Regarding CRM and chatbots, there is a clear emphasis on enhancing user trust, comprehension, and overall experience. While both domains prioritize user interaction, CRM tends to focus on sentiment analysis and customer support, using models such as LSTM, BERT, and CNN. In contrast, conversational AI relies more on attention distributions to explain decisions, aiming to make chatbot responses as transparent and reliable as possible. This divergence possibly stems from CRM's need to interpret user sentiment from varied expressions, while chatbots aim to offer coherent and contextually relevant responses. Across both, the overarching theme is building user trust through transparency

%Section 4
\section{Critical aspects of XNLP}
\label{sec:critical}

\subsection{Evaluation Metrics: Quantifying Understanding}
\label{sec:evaluation}

What does it mean to understand a model? To evaluate the effectiveness of XNLP techniques, the level of comprehension provided by the explanations must be quantified using appropriate evaluation metrics. Quantitative metrics evaluate the correspondence between the explanation and the underlying model's reasoning. Qualitative metrics offer insight into user perceptions, trust, and explanation satisfaction~\cite{fan_interpretability_2020}. Quantitative metrics quantify the numerical characteristics of the generated explanations. They include fidelity, coherence, and completeness measurements. 

The \textbf{fidelity} metric evaluates how accurately the explanations correspond to the actual behavior of the NLP model. High-fidelity explanations accurately represent the model's decisions and generate the same exact predictions as the original model. LIME~\cite{ribeiro_why_2016} is a method for assessing fidelity that measures the congruence between explanations and model predictions. \textbf{Coherence} is a measure of the explanation's clarity. Coherent explanations are logically sound, concise, and simple for humans to comprehend. Evaluations of coherence frequently employ established language metrics such as BLEU~\cite{papineni_bleu_2002}, ROUGE~\cite{lin_rouge_2004} or more advanced metrics like BERTScore, which disentangles linguistic properties to assess coherence across tasks~\cite{kaster-etal-2021-global}. \textbf{Completeness} evaluates the explanation's coverage, i.e., whether all the factors that led to a decision are included. Techniques like SHAP~\cite{lundberg2017unified} measure completeness by considering each feature's contribution to the prediction. 

Qualitative metrics consist of subjective evaluations of explanations and are frequently evaluated through user studies. These metrics evaluate user confidence, satisfaction, and perceived transparency. Measurement techniques include the goodness of explanations, the improvement of the audience's mental model induced by the explanations, and the impact of explanations on model performance, trust, and audience reliance~\cite{arrieta2020explainable, hoffman2018metrics, mohseni2021multidisciplinary}. Some authors suggest that interpretability is a prerequisite for trust. Trust can also be defined subjectively. For example, a person might feel more comfortable with a well-understood model, even if this understanding serves no clear purpose. When training and deployment objectives diverge, trust might indicate confidence that the model will perform well with respect to real objectives and scenarios~\cite{lipton2018mythos}. Another aspect of user trust in an ML model is the user's comfort with relinquishing control to it. If the model tends to make errors only on those types of inputs where humans also make mistakes, and is generally accurate when humans are accurate, then one might trust the model due to the absence of any anticipated cost associated with relinquishing control~\cite{lipton2018mythos}. \textbf{User trust} can be measured through surveys where users rate their level of trust in the model and its explanations after interacting with it. Studies have shown that user trust can enhance the acceptance and effective use of AI models in sensitive areas~\cite{arrieta2020explainable}. 

\textbf{User satisfaction}, defined as the acceptance and perception of the model's usefulness, is another critical metric~\cite{doshi2017towards}. Satisfaction measures the degree to which users feel they understand the AI system or process being explained. Asking users to rate the usefulness and helpfulness of explanations can determine user satisfaction. In finance, for example, a user's satisfaction with an AI model that predicts stock market trends can determine the model's overall usefulness. Research indicates that higher satisfaction levels correlate with better engagement and reliance on AI systems~\cite{hoffman2018metrics}. Additionally, if a model itself is understandable, it is considered transparent. \textbf{Perceived transparency} is the degree to which users believe they understand the decision-making process of a model based on its explanations. It is typically evaluated through user self-reports regarding their perceived comprehension of the model's reasoning. Enhanced transparency can lead to increased trust and better decision-making outcomes~\cite{lipton2018mythos}. While these metrics offer valuable insights, they also have limitations. For instance, subjective evaluations can be influenced by individual biases and the user's prior experience with AI technologies. Moreover, the context in which these metrics are applied can affect their reliability and validity. Despite these challenges, integrating qualitative metrics into user studies remains a robust approach to improving the interpretability and usability of AI models across various domains~\cite{doshi2017towards}. Recent research by~\citet{sogaard2021explainable} further elaborates on these metrics, emphasizing the importance of context in evaluating explanations and suggesting that context-aware evaluation metrics could enhance the relevance and usability of XNLP explanations.

{%Table 10. Overview of Evaluation Metrics for XNLP Techniques
\begin{scriptsize}
\centering
\renewcommand{\arraystretch}{0.9}
\begin{longtable}{p{2cm} p{2cm} p{6.5cm} p{1.5cm}}
\caption{Overview of Evaluation Metrics for XNLP Techniques} \label{tab:evaluation_metrics} \\
\toprule
\textbf{Type} & \textbf{Metric} & \textbf{Description} & \textbf{Study} \\
\midrule
\endfirsthead
\toprule
\textbf{Type} & \textbf{Metric} & \textbf{Description} & \textbf{Study} \\
\midrule
\endhead

\multirow{3}{*}{Quantitative} 
 & Fidelity & Measures how accurately the explanations reflect the model's behavior. & \citep{ribeiro_why_2016} \\
 & Coherence & Assesses the clarity and logical consistency of the explanations. & \citep{papineni_bleu_2002} \\
 & Completeness & Evaluates whether all relevant factors are included in the explanations. & \citep{lundberg2017unified} \\
\midrule

\multirow{3}{*}{Qualitative} 
 & User Trust & Measures the confidence users have in the model's explanations. & \citep{lipton2018mythos} \\
 & Satisfaction & Gauges user acceptance and perceived usefulness of the explanations. & \citep{doshi2017towards} \\
 & Transparency & Evaluates the perceived understanding of the model's reasoning. & \citep{lipton2018mythos} \\
\bottomrule
\end{longtable}
\end{scriptsize}
}


\subsection{Trade-offs and Challenges: The Price of Explanation}

Explainability in NLP models frequently involves trade-offs. XNLP requires a balance between the level of explainability and the model's performance. As~\citet{doshi2017towards} noted, increasing a model's transparency may reduce its accuracy, which may be due to the simplification of the model or constraints placed on the modeling process to make it more understandable. For instance, simpler models like linear regressions are easier to interpret but often less accurate than complex models like deep neural networks, which capture complex patterns in data but are harder to explain. Researchers and practitioners face the challenge of achieving a suitable balance between explainability and performance, as overly complex models can obscure the decision-making process, while overly simple models might not perform well on complex tasks~\cite{doshi2017towards}. This explainability-performance trade-off is particularly relevant in high-stakes fields such as healthcare and finance, where accuracy and transparency are essential. reducing this trade-off involves carefully designing models and employing explanation techniques that provide transparency without compromising the model's predictive power. Techniques such as LIME and SHAP~\cite{lundberg2017unified,ribeiro_why_2016} offer potential solutions, but finding the optimal balance remains a challenging and active research topic.

Another important consideration is time complexity, which varies greatly depending on the method used. Gradient-based techniques, such as SHAP, need computation in the same order of complexity as the original training aim, whereas attention weights, which are a result of training the model, can be seen directly with minimal additional processing. A major area of attention in current research is the variation in computational needs among different XNLP approaches, with the goal of improving the scalability and efficacy of XNLP models~\cite{singh_programs_2016}. In the realm of XNLP, it is still difficult to fully understand and optimize the trade-offs between computing efficiency and the level of explanation given.Also, the growing size and complexity of NLP models raises concerns about scalability, as seen by the appearance of large transformer models like GPT-3~\cite{brown_language_2020}. Clarifying these models' predictions can increase the computational load associated with training them. Scalability issues also arise when working with large databases or a variety of application domains. For XNLP to be widely used, explanation approaches must be flexible enough to be applied to a variety of industries, including banking and healthcare.

In conclusion, XNLP has a lot of promise, but in order to reach its full potential, it must overcome the challenges of explainability-performance trade-offs, time complexity, and scalability. Refining these techniques allows the power of XNLP to be better used across various critical applications. As~\cite{sogaard2021explainable} highlights, the challenge of standardizing evaluation protocols across different domains and applications is crucial for the fair assessment and comparison of XNLP techniques.
 
\subsection{Rationalization Techniques: Current Approaches and Challenges}

In recent years, NLP models have made advancements, becoming increasingly complex and capable of handling complex tasks. However, as these models grow in complexity, the need for transparency and explainability becomes more urgent. This has given rise to Rational NLP (RNLP), a subset of XNLP, which focuses on creating understandable explanations for model decisions~\cite{atanasova2024generating, rajani2019explain}. Although the concept of rationalization in NLP was introduced in 2007 by~\citet{zaidan2007using}, it is only in recent years that the field has gained momentum, particularly with the focus on generating natural language explanations or rationales for model outputs~\cite{atanasova2024generating, rajani2019explain}. While RNLP offers an approach to explaining model behaviour, the field is still in its early stages. To fully explore its potential across fields of study, more systematic study is required as it lacks an organized methodology.~\cite{rajani2019explain, atanasova2024generating}. The main goal of rationalization is to increase model transparency by providing clear explanations for decisions made by the model. The two main types of rationalization techniques are extractive and abstractive rationalization.

Extractive rationalization techniques like LIME and Grad-CAM will continue to be central to the development of explainable NLP models. These techniques provide a foundation by identifying and visualizing the key components of input text that influence a model’s decisions~\cite{majumder2021knowledge}. However, future research must focus on making these explanations more precise and tightly aligned with the underlying decision-making processes of increasingly complex models, such as transformers and large-scale language models~\cite{ribeiro_why_2016, lei_rationalizing_2016}. As an example of extractive rationalization is LIME, a technique that approximates the complex decision boundary of a model with a simpler local model around a point of interest. In NLP tasks, LIME highlights words in a sentence that contribute to the model’s prediction, allowing users to see which parts of the input data are critical to the decision~\cite{ribeiro_why_2016, zhao2021baylime}. Conversely, Grad-CAM, another technique, generates coarse localization maps to visualize which regions in the input text are crucial for a model's decision by using gradients flowing into the final layers~\cite{lei_rationalizing_2016}.

On the other hand, abstractive rationalization, which aims to generate natural language explanations that may not be limited to the original input text, presents exciting research opportunities. Advanced generation models, such as sequence-to-sequence frameworks, offer the potential to create more context-rich explanations that can bridge the gap between technical decisions and human understanding~\cite{gurrapu_exclaim_2022}. However, these methods face challenges related to the precision and consistency of generated rationales. As models become more powerful, it will be crucial to ensure that abstractive rationalizations accurately reflect the model’s logic, rather than providing overly generalized or misleading summaries~\cite{sha_rationalizing_2023}.Recent research in this domain, such as Semantically Equivalent Adversarial Rules for Debugging Models (SEARCH), showes the potential of generating human-readable rules that can maintain the accuracy of model predictions while offering more insightful explanations~\cite{ribeiro_semantically_2018}. The future development of such methods could enable more flexible and detailed rationalizations, making AI systems more transparent and comprehensible across a wider range of domains.

Despite the advances in these techniques, the field of rationalization faces several challenges. One key issue is the phenomenon of ``rationalization post hoc", where models generate plausible-sounding explanations that may not reflect the actual decision-making process~\cite{siddhant_deep_2018}. Furthermore, current evaluation metrics focus primarily on precision but fail to capture other critical factors like coherence, conciseness, and comprehensibility, which are essential for producing meaningful rationales~\cite{jacovi2020towards}. Addressing these gaps is essential for developing more reliable and transparent NLP systems. In conclusion, while rationalization techniques such as LIME and Grad-CAM have made important strides in enhancing the explainability of NLP models, the field still faces critical challenges. The need for standardized evaluation metrics and more accurate representations of model reasoning remains pressing. Overcoming these obstacles will require concerted research efforts aimed at refining the current methods and developing new frameworks that balance precision with interpretability and trustworthiness.

\subsection{Human Evaluation and User Studies: Human in the Loop}

Human evaluation and user research are crucial for assessing the impact and effectiveness of XNLP techniques. Human-in-the-loop strategies incorporate feedback to validate and improve NLP-generated explanations. For example, \citet{sogaard2021explainable} highlights the value of iterative feedback to better align explanations with human cognition and usability. Similarly, \citet{Valvoda2024TowardsEI} emphasize explainability in the legal domain, aiding professionals in understanding model decisions. Along this line, \citet{mosca-etal-2023-ifan} introduce IFAN, a framework for real-time human-NLP interaction to debias and enhance performance.

%fairness
With the growing prevalence of LLMs, the challenge of bias in data is significant. Recent studies on counterfactual explanations~\cite{wachter_counterfactual_2017} and fairness-aware machine learning~\cite{dunkelau_fairness-aware_2019} further address bias reduction, promoting fairness and transparency in XNLP. Similarly, \citet{ALKHALED2023} introduced Bipol, a novel metric focused on detecting social biases in text data, improving the fairness and transparency of models by comprehensively assessing bias. These techniques not only mitigate biases but also enhance clarity and refine models based on user preferences and needs.

User happiness and trust, which are correlated with the perceived value and caliber of explanations, are critical for the adoption and acceptance of XNLP, as discussed in section~\ref{sec:evaluation}. Finding out how satisfied users are with the explanations given by NLP models through user research provides important information about how effective these methods are thought to be. When an AI system gives clear explanations for its decisions, users are more likely to trust it~\cite{ribeiro_why_2016}. Yet, biases in machine learning models, such as NLP models, may produce unfair or unethical results, which may show up in the explanations given by XNLP models and lead to misleading perceptions~\cite{psychoula_explainable_2021}. This highlights how difficult it is to judge when a user is pleased enough with an explanation. Because of cognitive biases or a lack of domain understanding, users may be satisfied with poor explanations~\cite{miller2019explanation, doshi2017towards}. In order to determine how effectively explanations satisfy users' requirements and preferences, user studies and satisfaction surveys are essential~\cite{selbst2019fairness}. Additionally, it is important to take into account the possibility that users could be mislead by explanations that seem plausible on the surface but are ultimately false.

In conclusion, while technical considerations are crucial for developing XNLP systems, a comprehensive understanding of these systems must also account for the human element. Incorporating user studies, human-in-the-loop approaches, and efforts to detect and reduce bias can enhance the efficacy and trustworthiness of XNLP. In the reviewed research, only a minority of studies considered human evaluation either during the modeling or validation phases.


\subsection{Data and Code Availability: The Role of Open Science}

One of the critical aspects of XNLP is the availability of data and code, aligning well with the principles of Open Science. Open Science encapsulates various facets including open data, open-source software, open journal access, and reproducibility, which are essential for providing a transparent and collaborative research environment~\cite{seminck2023conversational,brinkman2021open, de2023combining}. Tools like Ecco~\cite{alammar-2021-ecco} contribute to this openness by offering an interactive framework for explaining transformer models, helping researchers explore and understand model behavior more transparently. ChatGPT, for instance, operates under a commercial subscription model where its paid version (ChatGPT Plus) provides access to GPT-4 with faster response times, while the free version only allows access to GPT-3.5. These commercial offerings, while beneficial for some users, contrast with open-source models in terms of transparency and accessibility for research purposes~\cite{OpenAI2023}. Additionally, open-source models and open data are critical for ensuring transparency and accessibility, allowing researchers to scrutinize training data, foster trust, and facilitate more robust scientific advancements~\cite{bender2021dangers}. While performing the review, it was found that XNLP researchers followed greatly to Open Science principles; most of the studies made their code publicly available, usually on sites such as GitHub~\footnote{\scriptsize\url{https://github.com/}} or Zenodo~\footnote{\scriptsize\url{https://zenodo.org/}}. The high rate of code availability encourages transparency and makes it easier for the scientific community as a whole to replicate and validate the study findings. In terms of data accessibility, there is still need for improvement, as fewer than half of the research had datasets that were openly accessible. The difference highlights the necessity of addressing data access, which is essential for publicly and cooperatively progressing the field of XNLP.


The rise of open science is useful in clarifying and validating the workings of XNLP models, thereby contributing to the broader understanding and acceptance of XNLP methodologies within and beyond the scientific community. Recent studies, including those by~\citet{sogaard2021explainable}, emphasize the importance of standardized datasets and benchmark tasks to enable fair and comprehensive evaluation of XNLP methods across different domains and applications.n In conclusion, while technical considerations are essential to developing XNLP systems, the adherence to open science principles further enhances the transparency and reproducibility of research in this field, contributing to its growth and maturity.


%Section 5
\subsection{Future Directions and Research Opportunities in XNLP}
\label{sec:future}

XNLP can benefit from integrating XNLP and other ML techniques, such as reinforcement learning (RL). RL is a subfield of ML in which an agent learns to make decisions by interacting with an environment and receiving rewards and penalties as feedback. RL has been applied in various NLP applications, including dialogue systems and machine translation~\cite{li_deep_2016}. NLP, particularly transformers and advanced models like GPT, is increasingly using RL. This method not only enhances the performance of these models but also holds the potential to make tThem more explainable RL, by its nature of learning through trial and error and receiving feedback, can be instrumental in fine-tuning models to produce more accurate and contextually relevant outputs. Additionally, the incorporation of RL strategies can contribute to the development of models that are capable of providing clearer insights into their decision-making processes, thereby increasing their explainability. This aspect is especially valuable in domains where understanding the reasoning behind model predictions is crucial~\cite {liu_how_2016}. 

The ``chain of thought" reasoning, a concept where models articulate intermediate reasoning steps, plays a critical role in making these models more interpretable. For example, \citet{wei2022chain} show how chain of thought prompting can improve the reasoning capabilities of LLMs.
Recent research, such as the work by~\citet{ehsan_rationalization_2018} on generating explanations for RL decision-making in a gaming context, has begun to investigate this concept. However, integrating RL and XNLP is still in its infancy, offering various research opportunities for exploration and analysis.


%\subsubsection{Explainability in Hybrid Models}

Another promising direction for XNLP is the development of hybrid models that combine various machine-learning techniques. These models combine the strengths of various techniques to overcome their limitations. Integrating rule-based systems and neural networks is one example of this strategy. Rule-based systems are interpretable and easily comprehensible by humans, whereas neural networks have powerful predictive capabilities and can generalize to new data. By combining these techniques, researchers can likely produce robust and interpretable models~\cite{sun_pullnet_2019}. Recent research~\citet{weber_nlprolog_2019} presented a hybrid model for interpretable question answering that combines rule-based reasoning with deep learning. Their model employs a rule-based system for explicit reason over a knowledge base, while a neural network is taught to deal with implicit, fuzzy reasoning. Developing and investigating hybrid models is a opportunity for XNLP. Researchers can design NLP systems that do not sacrifice performance or explainability by successfully integrating different approaches, paving the way for more effective, transparent, and trustworthy AI.

%\subsection{Emerging Research Directions}

Explainable dialogue systems is an additional promising area of research where transparent and interpretable conversational agents can provide coherent and meaningful responses while explaining their suggestions or recommendations. The work of~\citet{sarkar_towards_2023} exemplifies the potential of this direction by introducing an explainable dialogue system based on a transformer-based model. Their system shows how conversational agents can provide coherent and meaningful responses while explaining their reasoning. This transparency increases user confidence and contributes to an improved user experience.

Explainable social media analytics is another emerging field where NLP techniques can provide insight into the motivations, emotions, and influences that drive social media discussions. This can aid in comprehending public opinion, identifying trends, and combating misinformation and fake news. Personalized and adaptive explainability is a frontier of research investigating the customization of explanations based on user preferences, domain expertise, or cultural background. The goal of personalized explainability is to tailor the level and format of explanations to the specific needs of each user, thereby enhancing their understanding and confidence in the NLP system. XAI and rational AI (RAI) may benefit from rationalization techniques that generate concise explanations or summaries. These techniques can assist in bridging the gap between complex models and human comprehension by providing explanations highlighting the most important and relevant data. This research direction's potential is illuminated by work like that of~\citet{bovet_influence_2019}, which examines the spread of information and user behavior on social media networks using explainable models.

%\subsubsection{Personalized and Adaptive Explainability}

As NLP systems become more individualized, there is a growing interest in customizing explainability for specific users. This research direction, personalized and adaptive explainability, aims to adapt the level and format of explanations to the specific needs of individual users, thereby enhancing their understanding and confidence in the NLP system~\cite{kuhl_you_2020}. The significance of this direction is showed by research such as that of~\citet{abdul_trends_2018} in developing a framework for personalized explanations in intelligent systems. They illustrate how explanations can be customized based on user preferences, domain knowledge, or cultural factors. This customization ensures that the explanations are relevant, applicable, and understandable to the particular user, thereby promoting a more efficient and satisfying interaction with the system.

%\subsubsection{The Promising Role of Rationalization in XAI and Rational AI (RAI)}

As XNLP continues to evolve, the future of rationalization techniques holds promise for creating more human-centered, interpretable model explanations. While existing methods, such as extractive and abstractive rationalization, have already shown their potential, there remain opportunities for improvement and expansion. Ongoing research must address critical issues such as the ``rationalization post hoc" problem and the need for standardized evaluation metrics that encompass coherence, conciseness, and comprehensibility—criteria that go beyond mere precision~\cite{jacovi2020towards}.  he work of~\citet{yu_rethinking_2019} in training models to generate rationalizations showes the potential of rationalization techniques within the domains of XAI and RAI. Their research involved the development of models to generate human-comprehensible justifications for their decisions, thereby fostering interpretability and trust.


%Section 6
\section{Conclusion}
\label{sec:Conclusion}

In this article, we have explored the landscape of XNLP with a particular emphasis on its practical deployment and real-world applications across various domains. The main aim was to address the critical gap in the literature by providing a detailed examination of how XNLP systems can be effectively applied across various domains to enhance user understanding, transparency, and trust in machine learning models. We began by discussing the advancements in NLP technologies and the inherent challenges associated with the black-box nature of complex models. The necessity for transparency in these models was highlighted, particularly in high-risk fields like healthcare and finance, where decisions based on model predictions can have and lasting impacts. Then, we investigated XNLP techniques for various modeling, including traditional models like Bag of Words and TF-IDF, embedding models such as Word2Vec and GloVe, and advanced transformer models like BERT. We elaborated on the interpretability and explainability techniques relevant to these models, such as attention visualization, rationalization techniques, and gradient-based methods, which aim to demystify the decision-making processes of NLP systems.

We examined the application of XNLP across seven key domains, highlighting its broad impact and versatility. In the field of medicine, XNLP facilitates the analysis of electronic health records and medical literature, providing clear, actionable insights that support clinical decision-making and improve patient care. In finance, it enhances risk assessment and fraud detection systems, making financial processes more transparent and reliable, with a particular emphasis on explainability in credit scoring and firm valuation to ensure fair and accountable financial practices. For systematic reviews, it automates the review process and offers clear explanations for study selection and data extraction, thereby improving efficiency and transparency and aiding researchers in managing vast amounts of scientific literature. In CRM, it improves sentiment analysis and customer support automation, offering transparent and context-aware responses that enhance customer engagement and satisfaction. It also plays a crucial role in chatbots and conversational AI, enabling these agents to provide clear, context-aware explanations for their responses, which in turn improves user trust and satisfaction. In social and behavioral science, it aids in the detection of sexism, hate speech, and fake news, offering transparent and interpretable insights into social media and other text data. Lastly, in human resources, it can be applied to automate and explain processes in recruitment and employee management, ensuring fair and unbiased decision-making.

We also explored the critical aspects of XNLP, emphasizing the need for appropriate evaluation metrics to measure comprehension and the trade-offs between explainability and model performance. In particular, we highlighted the limitations of rationalization methods and the importance of clearer explanations to enhance model trust. Furthermore, we underscored the significance of human evaluation and user studies, focusing on areas such as bias detection and mitigation, user satisfaction, and trust. Finally, we discussed future directions and opportunities for research in XNLP. We discussed the integration of explainability with other ML techniques, such as RL, and emerging research directions, including explainable dialogue systems, explainable social media analytics, personalized and adaptive explainability, and the promising role of rationalization in XAI.

In conclusion, XNLP is a rapidly evolving field with implications for NLP models' transparency, trustworthiness, and accountability. These models can improve user comprehension, enable informed decision-making, and facilitate human-AI collaboration by explicating their decisions. Integrating explainable techniques into NLP models will continue to drive advancements in this field, resulting in more transparent, interpretable, and trustworthy AI systems as research progresses.nAccordingly, the need to synthesize knowledge, identify trends, and explore unanswered research questions will be highlighted.

%Acknowledgment
\section*{Acknowledgment}
The authors sincerely appreciate \textit{Tina Shahedi} for her editorial assistance and contributions to refining this paper. We also thank \textit{Mehran Moazeni}, \textit{Mohammad Behbahani}, and \textit{Daniel Anadria}, along with our anonymous reviewers, for their valuable feedback and insightful suggestions.

%% The Appendices part is started with the command \appendix;
%% appendix sections are then done as normal sections
\appendix
\section{Appendix}
\label{sec:appendix}

\subsection{Research Methodology}
\label{sec:appendixA}

To start our research, we did a full literature search on Scopus, Google Scholar, etc using related keywords. This gave us the first set of scholarly pieces that were relevant. This dataset was used as the basis for the bibliometric analysis done with \textbf{\textit{VOSviewer}}\footnote{\scriptsize\url{https://www.vosviewer.com/}.}. This made it easier to find core articles, keywords, and their connections, which added more linked keywords to our dataset. After doing the bibliometric analysis, we used ASReview to look at the literature in more detail for each application we found in our study domain. Active learning methods in \textbf{\textit{ASReview}}\footnote{\scriptsize\url{https://asreview.nl/}.} helped find the best article for each application by screening the enriched dataset in a planned way. This gave detailed information about each application. The combination of VOSviewer for bibliometric analysis and ASReview for targeted literature review created a strong, iterative method that not only sped up the literature analysis process but also helped us gain a deeper understanding of the topic. This set the stage for a thorough examination of the uses, problems, and potential future directions of XNLP. The overall path to finding related articles is as below. 
{
\small

1. \textit{Data Sources:} used reputable databases and digital libraries including \textit{IEEE Xplore, ACL, ACM Digital Library, Google Scholar}, and \textit{Scopus} to form the foundation of our literature dataset.

2. \textit{Search Strategy:} Employed a structured search methodology by querying the terms \textit{``explainable AI"} and \textit{``natural language processing"} within the title, abstract, and keywords of publications, spanning the time interval from 2018 to 2024.

3. \textit{Bibliometric Analysis:} useed \textit{VOSviewer} in conjunction with \textit{Scopus} analysis for bibliographic examination, facilitating the construction of co-occurrence networks of scientific keywords and connections between articles.

4. \textit{Initial Results:} The search and \textit{bibliometric} analysis yielded \textit{217} related papers along with \textit{130} connections between articles, presenting an initial picture of the literature landscape.

5. \textit{Data Cleaning:} Employed data cleaning techniques to consolidate similar keywords and remove duplicates, refining the dataset down to \textit{135} papers that are more directly related to the research themes.

6. \textit{Targeted Literature Retrieval:} used \textit{ASReview} to systematically screen the refined dataset, identifying the most relevant articles for each application within the domain of XNLP. 

}

Bibliographic analysis, with a particular emphasis on keyword co-occurrence analysis, enabled us to identify our research field's primary themes and developments. Using VOSviewer, we made networks of scientific keywords. In these networks, connections between keywords are established based on their co-occurrence. We intend to analyze bibliometric networks using VOSviewer, with our bibliographic database Scopus files serving as input.  The number of articles over time, and their sources based on Scopus analysis, is shown in Figure \ref{fig:Scopus}.

%Figure 7. The number of articles by year and its source are based on Scopus analysis.
\begin{figure}[H]
    \centering

    \begin{subfigure}[b]{0.8\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Scopos1.pdf} 
        %\caption{Image 1}
        \label{fig:Scopus1}
    \end{subfigure}
    
    \vspace{1em} 

    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Scopos2.pdf} 
        %\caption{Image 2}
        \label{fig:Scopus2}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Scopos3.pdf}
        %\caption{Image 3}
        \label{fig:Scopus3}
    \end{subfigure}
    
    \caption{The number of articles by year and its source are based on Scopus analysis}
    \label{fig:Scopus}
\end{figure}

Through advanced document search, we identified 217 relevant papers that include ``explainable AI" and ``natural language processing" in the title, abstract, or keywords. We are keen to elucidate our network map, where the items are represented by keywords including both author and index keywords. After employing data cleaning to consolidate similar keywords, we enhanced the precision of our map. A link between any pair of keywords known as a co-occurrence link exists; each link's strength is represented by a positive numerical value. The higher this value, the more robust the connection, indicating the number of publications where two keywords occur together. Figure~\ref{fig:VOSviewer} provides a visual representation of the network.

%Figure 4. Network visualization in VOSviewer.
\begin{figure}[H] % H specifier enforces the position
\centering
\includegraphics[width=0.8\textwidth]{Network_visualization_in_VOSviewer.pdf} 
\captionsetup{font=footnotesize} 
\caption{Network visualization in VOSviewer} 
\label{fig:VOSviewer}
\end{figure}

The cluster density visualization is portrayed in Figure five, available only if keywords are assigned to clusters. This visualization reveals the density of items separately for each set of keywords, and the color at a point in the visualization is a blend of different cluster colors. In keyword density visualization (Figure~\ref{fig:VOSviewer1}), the color at a point indicates the density, with colors ranging from blue to yellow depending on the number and weight of neighboring items.

%Figure 5. Item density visualization.
\begin{figure}[H] % H specifier enforces the position
\centering
\includegraphics[width=0.7\textwidth]{Item_density_visualization.pdf} 
\captionsetup{font=footnotesize} 
\caption{Item density visualization in VOSviewer}
\label{fig:VOSviewer1}
\end{figure}

The keywords overlay visualization, as shown in Figure~\ref{fig:VOSviewer2}, parallels the keywords network visualization but differentiates in coloring. In our network, a keyword's color is influenced by the average publication per year score, ranging from blue (lowest score) to red (highest score).

%Figure 6. Overlay visualization.
\begin{figure}[H] % H specifier enforces the position
\centering
\includegraphics[width=0.7\textwidth]{Overlay_visualization.pdf} 
\captionsetup{font=footnotesize} 
\caption{Overlay visualization in VOSviewer} 
\label{fig:VOSviewer2}
\end{figure}

Following the bibliometric analysis and keyword co-occurrence network construction, we employed ASReview to systematically identify the most relevant articles for each identified application within our research domain. ASReview, an AI-aided open-source software for systematic reviews, facilitated an efficient and targeted exploration of the literature. Utilizing the active learning capabilities of ASReview, we were able to narrow down the vast pool of literature to pinpoint the most relevant article for each application. This process was iterative, where the outcomes from the VOSviewer analysis informed the keywords and parameters used in ASReview, creating a feedback loop that enhanced the precision and relevance of our literature retrieval.

The application of ASReview was particularly instrumental in addressing the challenges posed by the extensive body of literature, enabling a more focused and effective extraction of relevant articles. The software's Oracle Mode was used to screen the articles, with the Exploration Mode aiding in understanding the distribution and relevance of identified literature, and the Simulation Mode providing validation for the applied algorithms. This multi-modal approach ensured a thorough and delicate understanding of the literature, paving the way for a comprehensive review of the applications, challenges, and future directions of Explainable NLP and RNLP. The table~\ref{tab:paper} presents the count of final related papers identified across various applications:

{
%Table 11. Number of Articles in each Application
\begin{scriptsize}
\centering
\renewcommand{\arraystretch}{0.5}
\begin{longtable}{p{1cm} p{5cm} p{2cm}}
\captionsetup{font=footnotesize} 
\caption{Number of Final Related Papers in Different Applications} 
\label{tab:paper}  \\
\toprule
\textbf{No.} & \textbf{Application} & \textbf{No. Papers} \\
\midrule
\addlinespace
1 & Medicine & 12 \\
\addlinespace
2 & Finance & 10 \\
\addlinespace
3 & Systematic Reviews & 5 \\
\addlinespace
4 & CRM & 12 \\
\addlinespace
5 & Chatbots and Conversational AI & 5 \\
\addlinespace
6 & Social and Behavioral Science & 11 \\
\addlinespace
7 & HR & 7 \\
\addlinespace
8 & Different tasks in Other Applications & 15 \\
\addlinespace
\bottomrule
\end{longtable}
\end{scriptsize}
}
This methodology offers an complex and multifaceted view of the literature, enabling a delicate understanding of the subject matter. By using visual tools and quantitative metrics, we can systematically explore and present the relationship between key terms and concepts in explainable AI and natural language processing. Future research may expand upon these methodologies to further refine and extend our understanding of this dynamic and rapidly evolving field.

\subsection{Terminology}
\label{sec:appendixB}
The field of XNLP is marked by an array of specific terms, concepts, and methods that define its unique characteristics and scope. This section provides essential definitions to ensure a comprehensive understanding of the subject matter and facilitate a clear and coherent reading experience. These terms are central to the discussions and analyses presented in the subsequent sections of this review.

{
\small
\begin{itemize}
  \item \textit{Natural Language Processing (NLP):} A branch of artificial intelligence that focuses on the interaction between computers and humans using natural language, enabling computers to interpret, recognize, and generate human language.
  \item \textit{XNLP (eNLP):} An area of NLP that emphasizes the understanding and interpretability of machine learning models. It seeks to make models' logic, decisions, and operations comprehensible to humans.
  \item \textit{Rational NLP (RNLP):} An extension of NLP that integrates reasoning capabilities into NLP systems, enabling them to provide logical justifications for their outputs or decisions.
  \item \textit{Embedding Models:} Methods used to represent words, phrases, and sentences in a continuous vector space, which can capture semantic meanings.
  \item \textit{Transformer Models:} A type of neural network architecture that relies on self-attention mechanisms to process input data in parallel rather than sequentially, allowing for more efficient training and handling of long-range dependencies in text.
  \item \textit{Rationalization Techniques:} Approaches used to provide explanations or justifications for the decisions made by NLP models.
  \item \textit{Explainability-Performance Balance:} A trade-off considering the complexity and accuracy of a model against its interpretability. More complex models might perform better but are often less interpretable.
  \item \textit{Human-in-the-Loop (HITL):} An approach where human judgment is involved in the training, tuning, or evaluation of machine learning models to ensure alignment with human values, ethics, and understanding.
  \item \textit{Rational AI (RAI):} A subfield focusing on the alignment of AI decision-making processes with logical and interpretable rationales.
  \item \textit{Interpretability Metrics:} Quantitative measures are used to evaluate the degree of interpretability of machine learning models, aiding in the assessment of the explainability and transparency of models.
  \item \textit{Explanatory Visualization:} Visual representations are designed to elucidate the inner workings, decisions, or outputs of machine learning models, enhancing human understanding and trust in the models.
\end{itemize}
}

\subsection{Declaration of generative AI and AI-assisted technologies in the writing process}
\label{sec:AI}

During the preparation of this work, the authors used ChatGPT 3.5 from OpenAI to check grammar and make other writing corrections to improve the readability and language of the manuscript. After using this tool, the authors reviewed and edited the content as needed and took full responsibility for the published article.

\subsection{Funding sources}
\label{sec:funding}
This research did not receive any specific grant from funding agencies in the public, commercial, or not-for-profit sectors.
%% If you have bib database file and want bibtex to generate the
%% bibitems, please use
%%
%\bibliographystyle{elsarticle-num}
\small
\bibliographystyle{apalike}
%\bibliography{base}  
\input{main.bbl}


%% else use the following coding to input the bibitems directly in the
%% TeX file.

%% Refer following link for more details about bibliography and citations.
%% https://en.wikibooks.org/wiki/LaTeX/Bibliography_Management

% \begin{thebibliography}{00}

%% For numbered reference style
%% \bibitem{label}
%% Text of bibliographic item

%\bibitem{lamport94}
%  Leslie Lamport,
%  \textit{\LaTeX: a document preparation system},
%  Addison Wesley, Massachusetts,
%  2nd edition,
%  1994.

%\end{thebibliography}
\end{document}

\endinput
%%
%% End of file `elsarticle-template-num.tex'.
