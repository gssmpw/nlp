\section{Related Works}
\subsection{Learning Representations for Geolocation}
Learning effective representations for geolocation is valuable for diverse downstream geospatial tasks, such as geolocalization~\cite{vivanco2024geoclip}, canopy height prediction~\cite{lang2023high}, fine-grained species classification~\cite{Aodha_2019_ICCV,sastry2024birdsat}, species distribution modeling~\cite{cole2023spatial, sastry2024birdsat}, geographical question answering \cite{mai2020se}, soundscape mapping~\cite{khanal2024psm}, etc. Geolocation in these tasks can be represented using either non-parametric~\cite{gaolearning, russwurm2023geographic} or parametric~\cite{mai2023csp,vivanco2024geoclip,klemmer2023satclip} methods.
Ru{\ss}wurm \textit{et al.}~\cite{russwurm2023geographic} represents geolocation using spherical harmonic basis functions, and Aodha \textit{et al.}~\cite{Aodha_2019_ICCV} uses a sine and cosine encoding of geolocation that removes discontinuity at dateline. In recent years, learned representations for geolocation, typically parameterized by neural networks, have been popularized. Some of these methods learn geolocation representations by using self-supervised pre-training strategies that distill visual information from either co-located overhead~\cite{klemmer2023satclip,mai2023csp} or ground-level images~\cite{mai2023csp,vivanco2024geoclip}. The location encoders trained by these methods have demonstrated utility across various downstream geospatial tasks. We use the representations from these location encoders as baselines for our model.
\subsection{Multimodal Contrastive Learning}
Contrastive learning is an efficient and scalable pre-training strategy for learning a shared embedding space across multiple modalities~\cite{radford2021learning,yang2022vision,yaofilip,li2021align,girdhar2023imagebind,elizalde2023clap,Dhakal_2024_CVPR,khanal2024psm,huynh2024contrastive,zheng2023exif}. As demonstrated by CLIP~\cite{radford2021learning} and more recent works~\cite{li2021align,yaofilip,yang2022vision} in vision-language learning, the shared embedding space can be utilized for zero-shot classification and leveraged across a variety of downstream tasks. Contrastively trained multimodal embedding spaces between geolocation and satellite imagery~\cite{klemmer2023satclip, mai2023csp} or ground-level imagery~\cite{vivanco2024geoclip}, have also demonstrated impressive transferability of both location and visual representations to various geospatial tasks.

Despite its success, recent works highlight the limitations of multimodal contrastive learning. One such study~\cite{liang2023factorized} shows that contrastively trained representations capture the shared information between the two modalities and thus only work best under certain assumptions. Similarly, another work~\cite{gupta2022understanding} demonstrates that the final layers of encoders in the contrastive learning framework preserve only the information necessary for solving the alignment objective, discarding other modality-specific details. We argue that some of the underlying assumptions of multimodal contrastive training do not hold true in the image-location setting. Therefore, the resulting location representations from these methods are sub-optimal for downstream tasks. To address this issue, we propose a method to approximate the image-specific information for a given location.
\subsection{Retrieval Augmented Methods}
There has been a recent surge in retrieval-augmented generation (RAG) methods, primarily focused on generative tasks~\cite{chenre, blattmann2022retrieval, kirstain2023x, lewis2020retrieval, borgeaud2022improving, siriwardhana2023improving, seoretrieval, koizumi2020audio, wangretrieval}. These methods generally consist of three main components: a retriever, an external database, and a generator. For a given query, the retriever selects a set of documents from the database and provides them, along with the original query, to the generator to produce the desired output. RAG has been successfully employed to improve text generation for large language models (LLMs)~\cite{lewis2020retrieval, borgeaud2022improving, siriwardhana2023improving}, text-to-image generation~\cite{chenre, blattmann2022retrieval, kirstain2023x}, text-to-3D generation~\cite{seoretrieval}, audio captioning~\cite{koizumi2020audio}, and more. Within the RAG framework, one could also replace the generator with another task-specific component, such as a classifier~\cite{long2022retrieval}, to enhance performance by leveraging the rich information from the retrieved documents. Inspired by these retrieval-augmented methods, we design our framework, RANGE, which retrieves rich visual information from our database for a given geolocation query and utilizes it for various downstream geospatial tasks.