\begin{figure*}[!t]
\begin{center}
\includegraphics[width=\linewidth, scale=0.3]{sec/images/framework_cam.jpg}
\end{center}
    \caption{Framework of RANGE. (a) In the training stage, a shared embedding space is learned between locations and images. (b) We create a database of low-resolution and high-resolution image embeddings using the trained projection layer and a powerful pretrained image model, respectively. (c) During inference, we use a location as the query, low-resolution image embeddings as keys, and high-resolution image embeddings as values. Using our retriever function, we compute the approximate high-resolution embeddings for the query. We concatenate ($\oplus$) the approximated visual feature with our query embedding.} 
\label{fig:framework}
\end{figure*}
\section{Method}
\subsection{Problem Setup}
We consider a dataset with paired geolocations and co-located satellite images $\{g_i, s_i\}$. The goal is to learn a meaningful representation for $g_i$ by aligning it with $s_i$, which can be done by minimizing the clip objective \cite{klemmer2023satclip, vivanco2024geoclip, radford2021learning}. We have a trainable geolocation encoder $E$, a frozen pretrained image encoder $I$, and a projection layer $P$. Let $G_i = E(g_i)$ represent the location embedding, $S_i = {I(s_i)}$ represent the frozen image embedding, and $P_i = P(I(s_i))=P(S_i)$ represent the image embedding obtained from the projection layer for the i$^{th}$ sample. We minimize the objective:
%
\begin{align}
&L_{i}^{loc} = \frac{-1}{k}\sum_{i=1}^{k} \log\frac{\exp(G_i \cdot P_i / \tau)}{\sum_{j=1}^{k} \exp(G_i \cdot P_j) / \tau) } \\
&L_{i}^{img} = \frac{-1}{k}\sum_{i=1}^{k} \log\frac{\exp(G_i \cdot P_i / \tau)}{\sum_{j=1}^{k} \exp(G_j \cdot P_i) / \tau) } \\
&L_i = (L_i^{loc} + L_i^{img})/2 
\end{align}
%
The training process is shown in Figure~\ref{fig:framework}-(a). We contrastively align the geolocations and satellite images by minimizing their CLIP objective. This formulation is identical to SatCLIP~\cite{klemmer2023satclip}, so we use the pretrained SatCLIP image and location encoders for simplicity. For query location $g_i$, $S_i$ encodes the high-resolution visual features since $I$ is a powerful pre-trained image encoder. Both $P_i$ and $G_i$ encode low-resolution shared information between $g_i$ and $s_i$. The resulting geo-embedding, $G_i$, is sub-optimal as it loses unique visual information crucial for many downstream tasks. To solve this, we propose a retriever function $R_\tau$ that allows us to approximate the high-resolution information $S_i$ given $g_i$. Let $\epsilon$ denote the deviation from the true visual embedding. We can then explicitly add the approximate visual information $\hat{S_i}$ to our location embedding $G_i$:
    \begin{align}
    &R_\tau(g_i) = \hat{S_i} = S_i + \epsilon \\
    &RANGE(g_i) = \hat{S_i} \oplus G_i 
    \end{align}
%
\subsection{Retrieval Augmented Neural Fields}
\noindent \textbf{Creating the Retrieval Database: }Once the contrastive training is done, we create a retrieval database as shown in Figure~\ref{fig:framework}-(b). We take $\{g_i, s_i\}$ sampled uniformly across the globe using the SatCLIP~\cite{klemmer2023satclip} dataset. For every location $g_i$ in our data, we compute: a) the shared-information image embedding $R_i{^L}=P(I(s_i))$, and b) the high-resolution image embedding $R_i{^H}=\bar{I}(s_i))$; we refer to these sets of embeddings as low-resolution embeddings and high-resolution embeddings, respectively, in the rest of the paper. In practice, $\bar{I}$ can differ from $I$; the only condition is that $\bar{I}$ is a pretrained image feature extractor. For our purposes, we use SatMAE~\cite{cong2022satmae} as $\bar{I}$. 
\\
\noindent \textbf{Retriever Function: }
Let $R^L=\{R_{1}^L, R_{2}^L,...R_{N}^L\}$, and $R^H=\{R_{1}^H, R_{2}^H,...R_{N}^H\}$ represent the set of all low-resolution and high-resolution image embeddings in our database. After creating the database, we need a method to sample appropriate high-resolution image information for a query location. Naively, we can set this as a lookup operation, where $G_i$ is the query, $R^L$ is the set of all keys, and $R^H$ is the set of all values. We have a function $sim(G_i, R_{j}^{L})$, which gives the alignment scores between i and j. In our setting, this similarity function is a simple cosine similarity. For a query $G_i$, we find $R_{j}^{L}$ with the highest alignment score and return the key $R_{j}^{H}$. 
In practice, $R_{j}^{H}$ is noisy. The semantically closest image in the database can contain additional information that is irrelevant to the location. Therefore, naively adding $R_{j}^{H}$ has the potential of introducing incorrect information to our geo-embeddings. 



We instead use a soft selection criteria. First, we compute the query's alignment score with each key in the database using cosine similarity. We use a softmax function to convert the alignment scores to probabilities. We also use a temperature parameter $\tau$ to slightly reshape the probability distribution; $\tau$ is extremely robust as it does not need to be fine-tuned for different tasks or for databases with different distributions. We use the resulting probabilities to compute a weighted average across all values, i.e., high-resolution image embeddings in the database. The resulting embedding is an approximation of $S_i$, where the information contributed by each image is weighted by its alignment score with the query. We concatenate this approximate high-resolution image embedding with our original location embedding $G_i$ to obtain the multi-resolution RANGE embedding $RANGE_i$. A high-level view of this process is shown in Figure~\ref{fig:framework}-(c). 

\begin{multline}
RANGE_i = R_{\tau}(G_i) \oplus G_i \\
= \frac{1}{N}\sum_{k=1}^{N}{\frac{e^{sim(G_i,R_{k}^L)/\tau}}
{\sum_{j=1}^{N}{e^{sim(G_i,R_{j}^L)/\tau}}}} * R_{k}^H \oplus G_i \\
\text{where}\ G_i\in \mathbb{R}^M,\ R_{k}^L \in \mathbb{R}^M,\ R_{k}^H \in \mathbb{R}^{N}.
\end{multline}
%
\noindent \textbf{Adding smoothness constraints: }We also propose another version of our model called $RANGE^+$. Here, we additionally use geodesic similarity to approximate the visual features of the location. We convert our query location to 3D cartesian coordinates and use angular distance to find the spatially closest image in the database. This spatial retrieval allows us to impose spatial smoothness on our embeddings since locations close to each other are forced to have similar retrieval, irrespective of the semantics.  Similar to $RANGE$, we compute a weighted average across all values in the database using the angular similarity between the query and keys. However, we explicitly mask samples with angular similarity lower than a threshold, i.e., significantly distant samples have no contribution. The spatially retrieved embedding is added to the semantically retrieved embedding and weighted using a $\beta$ parameter. This $\beta$ parameter controls the level of spatial smoothness enforced on the $RANGE$ embeddings, as shown in Figure~\ref{fig:beta}. Let $g{_i}^{3D}$ be the query location, $l$ be the set of all database locations in 3D cartesian coordinates, and $sim(g{_i}^{3D}, l_k)$ be their angular similarity:   
\begin{multline}
\label{eq:rangep}
RANGE^+_{i} = \frac{1}{N}(\beta*\sum_{k=1}^{N}{\frac{e^{sim(G_i,R_{k}^L)/\tau_{1}}}{\sum_{j=1}^{N}{e^{sim(G_i,R_{j}^L)/\tau_{1}}}}} * R_{k}^H+ \\
     (1-\beta)\sum_{k=1}^{N}{\frac{e^{sim(g{_i}^{3D},l_{k})/\tau_{2}}}{\sum_{j=1}^{N}{e^{sim(g{_i}^{3D},l_{j})/\tau_{2}}}}} * R_{k}^H
     )\oplus G_i \\
     \text{where } G_i \in \mathbb{R}^M, \ g{_i}^{3D} \in \mathbb{R}^3, \ l_k \in \mathbb{R}^3
\end{multline}
%
When $\beta$ is set to 0, only spatially relevant features are used, and we refer to this setting as RANGE-HAVER. 
