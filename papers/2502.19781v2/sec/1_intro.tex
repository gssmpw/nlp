\section{Introduction}
\label{sec:intro}


\begin{figure}[!t]
\begin{center}
\includegraphics[width=\linewidth, scale=0.3]{sec/images/tiny_framework_ultimate_2.jpg}
\end{center}
   \caption{Adding explicit visual features allows us to generate more high-resolution location embeddings. We use a retrieval strategy that allows us to generate multi-resolution retrieval-augmented neural field of geo-embeddings (RANGE).} 
\label{fig:multiview}
\end{figure}

Several machine learning tasks require the use of geolocation as an input feature. Multiple works have shown the benefits of using geolocation to solve vital ecological, geographical, and geological tasks. SINR~\cite{cole2023spatial} used location as input to predict species distribution, Lang \textit{et al.}~\cite{lang2023high} used location with satellite image to learn a global canopy height model, and SatCLIP~\cite{klemmer2023satclip} used location to solve several geospatial tasks such as biome classification, population density estimation, housing price prediction, etc. Since location is a ubiquitous input variable in all geospatial settings, finding good representations for it is fundamental when solving geospatial tasks. 

Previous work~\cite{russwurm2023geographic} has shown that using location directly (represented as two floating point numbers of latitude and longitude) as an input to a machine learning model yields poor performance. Therefore, several works have attempted to find better ways to represent location data. Some of these approaches~\cite{Aodha_2019_ICCV,gaolearning} use a non-parametric method to encode location. However, recent works~\cite{vivanco2024geoclip,mai2023csp,klemmer2023satclip} have shown that it is possible to learn a general-purpose location representation by learning a parametric model (often a neural network) in a self-supervised setting. There are several strategies that can be used to train the parametric model. A common approach used by state-of-the-art models~\cite{vivanco2024geoclip,mai2023csp,klemmer2023satclip} involves learning visual cues associated with a given geolocation. Two recent state-of-the-art models, GeoCLIP~\cite{vivanco2024geoclip} and SatCLIP~\cite{klemmer2023satclip}, use contrastive training to align geo-embeddings with co-located images; the former aligns them with ground-level images while the latter aligns them with satellite images. Images of a location capture dense information about the area, such as the landcover, landuse, etc. Therefore, learning location representations that capture this dense information is beneficial for solving geospatial tasks. Prior work~\cite{klemmer2023satclip} has shown that representations from such models are extremely powerful and are capable of solving diverse geospatial tasks both on their own and in combination with images. In this work, we argue that the current approaches to learning geo-embeddings using images, while powerful, still fail to preserve all the relevant visual information. The location representations from these models only capture low-resolution shared information between location and image and ignore crucial high-resolution information that is unique to the image. In section~\ref{sec:multiview}, we define this problem more formally from the perspective of multi-view redundancy. 

To address this issue, we propose a retrieval-augmented strategy that approximates the unique visual information present in a location. There are three key ideas to our solution. First, it is possible to approximate the visual features of a given location by strategically combining the visual features of other similar-looking locations. Second, contrastively trained models like SatCLIP~\cite{klemmer2023satclip} and GeoCLIP~\cite{vivanco2024geoclip}, while lacking in preserving modality-specific information, are excellent in learning modality alignment~\cite{gupta2022understanding}. Therefore, it is possible to retrieve semantically aligned images, with alignment scores, for a given location using these models. Third, we note that the variance of semantic information visible in satellite images across the Earth is relatively low (as opposed to the wider diversity in consumer photographs). Hence, it is possible to capture a large percentage of unique aerial semantics with a limited number of satellite images. By combining these three key ideas, we propose our retrieval-augmented framework, RANGE (Retrieval-Augmented Neural Fields for Multi-Resolution Geo-Embeddings). Our retriever function uses both semantic and spatial alignment to approximate the visual features of a given location using an auxiliary database of image features. Our function is robust to the database size and works well even in limited sample settings. The hyperparameter in the retriever function can be adjusted to generate geo-embeddings at different frequencies. In summary, these are the main contributions of our paper:   

\begin{itemize}
\item We show why existing image-location contrastive methods result in sub-optimal representations from the perspective of multi-view redundancy.  
\item We propose a novel retrieval-based strategy, RANGE, to generate geo-embeddings that preserve both shared and unique modality information.
\item We propose a retriever function that estimates the visual embedding of a location using both semantic and spatial alignment. Our retriever function is robust to different database sizes and can generate geo-embeddings at multiple frequencies by enforcing spatial smoothness.   
\end{itemize}

\section{Aligning Location and Image is a Multi-View Non-Redundancy Problem}
\label{sec:multiview}
Let (L,H,T) represent location modality, image modality, and geospatial task, respectively. Models like SatCLIP and GeoCLIP align co-located images and locations by minimizing the infoNCE objective. Solving the multimodal infoNCE objective maximizes the lower bound on the mutual information, preserving the shared information within the learned representations~\cite{oord2018representation}. As such, the final learned representations of these models contain only information that is shared across both location and image. This property is useful under the multi-view redundancy assumption which states that the information shared between the two modalities is exactly relevant to solve the underlying task.
% ?: $\exists \epsilon>0 \textit{ such that }I(L;T \mid H) \leq \epsilon \textit{ and }I(H;T \mid L) \leq \epsilon $, where I is the mutual information~\cite{liang2023factorized}. 
In other words, the multi-view redundancy setting assumes that any additional unique information from the image or location would not be useful in solving the task T~\cite{tosh2021contrastive}. However, this assumption does not hold in the location-image learning setting as there is high unique information contained in the image modality that is useful for solving downstream tasks. Formally, we argue that location-image learning falls under the multi-view non-redundancy setting: there is \textit{unique} information in the image modality that is not shared with the location modality, and that information is relevant for solving our geospatial task T. In this case, contrastive training discards valuable task-relevant information from the image, leading to poor downstream performance~\cite{liang2023factorized}.

Empirically, there is abundant unique information present in the image features that are relevant for solving downstream tasks. The location representations from contrastively trained models like SatCLIP and GeoCLIP only capture the low-resolution shared information, ignoring all the valuable task-relevant information that is unique to the image. To demonstrate this property concretely, we quantitatively show that there are tasks that can be solved more accurately by adding the image representation to the shared image-location representation from SatCLIP~\cite{klemmer2023satclip}. First, we choose 4 tasks: Biome, Ecoregions, Elevation, and Population Density estimation. We download a corresponding Sentinel-2 image for every data point. For each task, we train one model using the SatCLIP features and another model using the SatCLIP features combined with the SatMAE~\cite{cong2022satmae} features of the satellite image~\cite{cong2022satmae}. The results in Table~\ref{table:multiview} show that adding image features improves the performance across all tasks. The increase in performance demonstrates that the images contain unique information relevant to solving the task that is lost during contrastive training. Hence, ``location and image" does not follow the assumption of multi-view redundancy.

Although adding corresponding satellite image information to a location embedding improves performance, this is not feasible in practice. For many geospatial tasks, we wish to make predictions across the globe with millions of points. In such cases, using the ``image+location" framework would require us to retrieve/store/process millions of images. We propose an efficient method for approximating the visual information using a compact database. Our results suggest that this approximation can outperform the use of true visual features in some cases.

\begin{table}[]
\begin{tabular}{l|cc|cc}
             & Biome & Ecoregion & Elevation & Population \\ \hline
SatMAE  & 58.8                      & 28.4                           & 0.388 & 0.600 \\
SatCLIP     & 68.9  & 69.3      & 0.666     & 0.684      \\
loc$\oplus$img & 74.9  & 73.5      & 0.749     & 0.765      \\ \hline
%$\delta$             & 6.0   & 4.2       & 0.083     & 0.081 
gain (\%)             & +8.71   & +6.06       & +12.46     & +11.84  
\end{tabular}
\caption{We show that using the image-location shared information from SatCLIP embeddings is sub-optimal to solve some geospatial problems. Adding image features to the embeddings provides useful visual information that improves the accuracy of the task. We show the accuracy for the classification tasks and R$^2$ value for the regression tasks. The results indicate that there are valuable visual features that are not captured by the SatCLIP embeddings.}
\label{table:multiview}
\end{table}

