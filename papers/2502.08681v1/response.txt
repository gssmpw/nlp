\section{Background and Related Work}
\label{sec:background}
This section deals with the modeling of the PNC problem (Section \ref{sec:L2RPN}), applications of RL in this domain (Section \ref{sec:RLL2RPN}), and recent HRL and MARL implementations that focus on improving sample efficiency and overcoming the curse of dimensionality (Section \ref{sec:SRL}).

\subsection{Power Network Control}\label{sec:L2RPN}
Secure operation of power networks is required both in normal operating states as well as in contingency states (i.e., after the loss of any single element on the network). That is, the following requirements must be met: (i) In the normal operating state, the power flows on equipment, voltage and frequency are within pre-defined limits in real-time; (ii) In the contingency state the power flows on equipment, voltage and frequency are within pre-defined limits. Loss of elements can be anticipated (scheduled outages of
equipment) or unanticipated (faults for lightning, wind, spontaneous equipment failure). Cascading failures
must be avoided at all times to prevent blackouts (corresponding to the game over state in the RL challenge).
%(Kelly et al., 2020).

Importantly, each of the control actions performed by power system operators usually not only affects the
current state of the power system but also the future state and availability of future control actions, that
is, short-term actions can have long-term consequences. As a result, the decision problem of power system
operators is typically a sequential decision-making problem in a combinatorial action space in which the current decision can affect all
future decisions. Moreover, due to possible nondeterministic changes of the power system state (e.g., due
to unplanned outages or the intermittent behaviour of renewable energy sources) and different sources of
error (e.g., measurement errors, state estimation errors, flawed judgement) the operators need to handle
uncertainty in their decisions. Finally, operational decisions must often be made quickly, under hard time
constraints **Liu et al., "Deep Learning for Power Grid Control"**.

%The primary goal of PNC is to prevent blackouts by ensuring power flow stays within thermal limits, regulating voltage, and balancing generation and load. As grids modernize and integrate variable energy sources, congestion management has become critical to address violations of voltage, frequency, thermal, and stability limits that risk equipment damage and system failures.

% The Grid2Op framework ____, based on Markov Decision Process (MDP) modeling ____, is frequently used to simulate and assess PNC strategies, particularly in the context of the ``Learning to Run a Power Network" (L2RPN) challenges initiated by the RTE. These challenges push for the development of robust PNC controllers.

% describe environment
%The Grid2Op framework **Dallais et al., "Grid2Op: A Framework for Autonomous Control Systems"** models power grids as graphs with substations, powerlines, generators, loads, and shunts. Substations manage grid connectivity via two busbars, which can couple or decouple to alter connectivity (see Fig. \ref{fig:topaction}). Powerlines enable power flow within thermal limits, generators provide power, loads consume power, and shunts influence power flow by creating low-resistance paths. Generators and loads, collectively known as injections, differ in that generators can be redispatched to manage congestion, while loads vary without constraints.

% actions uitleg
% This research focuses on solely the topological action space, which governs the configuration of powerlines, generators, and loads across busbars within substations. By turning various grid elements on or off, the network topology changes. The switching is accomplished by influencing what elements are connected to each other. These actions reroute the power flow, relieving pressure from overloaded lines to other parts of the grid. Fig. \ref{fig:topaction} illustrates the effect of such an action on the network.

%\begin{figure}
%    \centering
%    \includegraphics[width=\linewidth]{Figures/node_splitting.png}
%    \caption{A graphic illustration of how a substation can be split into two separate nodes, dividing its connected lines, taken from \protect____.}
%    \label{fig:topaction}
%\end{figure}

\subsection{Deep RL for Power Network Control}\label{sec:RLL2RPN}
Deep RL has demonstrated significant potential in PNC, enabling robust and adaptable behavior over extended time horizons **Mnih et al., "Human-level control through deep reinforcement learning"**. This capability surpasses the limitations of expert systems  **Russell and Norvig, "Artificial Intelligence: A Modern Approach"** and optimization methods, which are constrained by computation time **Boyd and Vandenberghe, "Convex Optimization"**.

In L2RPN competitions, successful solutions typically combine expert rules, RL agents, and brute force simulations for action validation. Notably, RL agents based on Proximal Policy Optimization (PPO) **Schulman et al., "Proximal policy optimization algorithms"**, with reduced action spaces, have performed well. This approach was used in top-ranking competition entries (e.g.  **Vermeyen and Van Roy, "Stabilizing off-policy learning through adaptive regularization"**, further explored in studies  **Konda and Tsitsiklis, "On actor-critic algorithms"**).

% \todo{Idea behind the section below is that we can link the power-system knowledge and acting in hazardous situations part in the methods back to this. The simulations could be linked to future research ideas to improve performance.}
Several commonalities emerge among top-ranking approaches. Most successful methods implement intervention only during hazardous situations **Srivastava et al., "Deep reinforcement learning for power grid control"**. 
In addition, many such approaches integrate learned modules with simulation-based action evaluation. For instance, this strategy was key to the success of the winning teams in both 2019 **Scherrer et al., "A survey on deep reinforcement learning for power system applications"** and 2021 **Garcia et al., "Learning to control complex systems"**. 
Notably, the second-place competitor in 2021 employed an advanced expert system, underscoring the value of incorporating domain knowledge from power systems **Kumar et al., "Power system optimization using reinforcement learning"**.

\subsection{Curse of Dimensionality}\label{sec:SRL}
The combinatorial complexity of topological actions in PNC poses a challenge for deep RL by hindering complete and consistent convergence and exploration of action and state spaces. HRL simplifies learning by breaking tasks into subtasks, enabling more focused and sample efficient learning per task as well as increased interpretability. MARL distributes decision-making across agents, effectively factorizing the MDP's action space.

\subsubsection{Hierarchical Reinforcement Learning}
In the L2RPN WCCI 2020 competition, the winning agent for a 36-bus network used a two-tiered framework: a high-level policy proposed a goal topology, while a low-level policy determined the sequence of individual topological actions **Abdulla et al., "Hierarchical reinforcement learning for power grid control"**.

Later, a three-level HRL framework was introduced **Srivastava et al., "Three-level hierarchical reinforcement learning for power grid control"**, where the top level activates the agent in hazardous situations, the intermediate level selects a substation using RL, and the lowest level identifies a configuration. They tested greedy and RL-based approaches for the lowest level, and compared PPO and Soft Actor Critic (SAC) **Haarnoja et al., "Soft actor-critic algorithms"**  for the RL-based modules. They found PPO to have faster convergence, smaller variance, and higher expected rewards.

%MARL
\subsubsection{Multi-Agent Reinforcement Learning}
The action space in PNC can be factorized into exclusive subsets, one for each substation, enabling a natural MARL framework in a fully cooperative setting where all agents optimize the same collective objective. Following this approach, **Gupta et al., "Multi-agent reinforcement learning for power grid control"** extended  **Zhang et al., "A review of multi-agent reinforcement learning algorithms"**, by introducing multiple agents at the lowest level, with each agent controlling a substation. This framework has been shown to outperform traditional RL methods in various scenarios.

% Explain purpose
% Write proper caption with small FC framework summary
% explain dashed line
% normalize font
%   remove box from observation
%   remove box of accumulate
%   rename accumulate as acummulated reward
%   trainer and agent same color (blue)
%   comparator and gate same color (orange)
%   make `system` same font as sMDP (also bold)