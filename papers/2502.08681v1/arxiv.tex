\documentclass[twocolumn,switch, twoside]{article}


\usepackage{arxiv}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{amsmath}
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{lipsum}
\usepackage{fancyhdr}       % header
\usepackage{graphicx}       % graphics
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{subcaption}
\graphicspath{{media/}}     % organize your images and other figures under media/ folder
\usepackage[numbers]{natbib}
\usepackage{breakurl}  % If using BibTeX
\hypersetup{breaklinks=true}  % Ensures links can break

%Header
\pagestyle{fancy}
% \thispagestyle{empty}
\rhead{ \textit{ }} 

% Update your Headers here
\fancyhead[LO]{Centrally Coordinated Multi-Agent Reinforcement Learning for Power Grid Topology Control}
\fancyhead[RE]{de Mol et al.} % Firstauthor et al. if more than 2 - must use 
% \documentclass[twoside]{article}

\AtBeginDocument{
  \newgeometry{
    textheight=9in,
    textwidth=7.5in,
    top=1in,
    headheight=14pt,
    headsep=25pt,
    footskip=30pt,
    centering  % Ensures equal left and right margins
  }
}


  
%% Title
\title{Centrally Coordinated Multi-Agent Reinforcement Learning for Power Grid Topology Control}
\begin{document}

\twocolumn[\begin{@twocolumnfalse}

\author{Barbera de Mol\\
% EDMij B.V.$^{1}$\\
EDMij B.V.\\
Ouderkerk a.d. Amstel, The Netherlands\\
\texttt{barbera.de.mol@edmij.nl}
   \And
Davide Barbieri\\
TenneT TSO B.V.  \\
Arnhem, The Netherlands\\
\texttt{davide.barbieri@tennet.eu}
   \AND
Jan Viebahn\\
TenneT TSO B.V.\\
Arnhem, The Netherlands \\
\texttt{jan.viebahn@tennet.eu}
   \And
Davide Grossi\\
University of Groningen\\
Groningen, The Netherlands\\
\texttt{d.grossi@rug.nl}
}


\maketitle

\begin{abstract}
Power grid operation is becoming more complex due to the increase in generation of renewable energy. The recent series of Learning To Run a Power Network (L2RPN) competitions have encouraged the use of artificial agents to assist human dispatchers in operating power grids. 
However, the combinatorial nature of the action space poses a challenge to both conventional optimizers and learned controllers. Action space factorization, which breaks down decision-making into smaller sub-tasks, is one approach to tackle the curse of dimensionality. In this study, we propose a centrally coordinated multi-agent (CCMA) architecture for action space factorization. In this approach, regional agents propose actions and subsequently a coordinating agent selects the final action. 
We investigate several implementations of the CCMA architecture, and benchmark in different experimental settings against various L2RPN baseline approaches. The CCMA architecture exhibits higher sample efficiency and superior final performance than the baseline approaches. The results suggest high potential of the CCMA approach for further application in higher-dimensional L2RPN as well as real-world power grid settings.\vspace{5mm}
\end{abstract}


% keywords can be removed
\keywords{Multi-Agent Reinforcment Learning (MARL) \and Hierarchical Reinforcement Learning (HRL) \and Consensus-Based Learning \and Power Network Control (PNC)\vspace{5mm}}

\end{@twocolumnfalse}]


\section{Introduction}
The growing demand for electricity, driven by technological advancements and the shift towards electrified industries and transportation, highlights the need for resilient power grids. Managing these grids involves complex sequential decision-making across large state and action spaces, further complicated by aging infrastructure and increased reliance on unpredictable renewable energy sources \cite{mai2018electrification, panciatici2012operating}. 

Traditional grid management methods, such as redispatching and building new infrastructure, are costly and dependent on external factors. However, topological remedial actions, like reconfiguring grid substations, offer a cost-effective alternative that remains underexplored \cite{koglin1982corrective, viebahn2022potential,gridoptions}. 
As power grids expand, conventional computational methods struggle to provide optimal real-time control solutions, leading operators to rely on experience or predefined manuals. The combinatorial complexity of grid configurations necessitates more advanced approaches.

To support research in \emph{power network control} (PNC), the Grid2Op framework was developed \cite{grid2op}, enabling simulation of realistic grid scenarios as a Markov Decision Process (MDP) \cite{sutton2018reinforcement}. 
This framing makes the problem suitable for deep Reinforcement Learning (RL), which has been successful in various domains
\cite{schrittwieser2020mastering, gu2017deep, kendall2019learning, li2019transforming}.

However, deep RL faces scaling issues with large networks due to the combinatorial explosion of state and action spaces. The curse of dimensionality remains a major bottleneck in RL tasks. Specifically, the sample complexity grows exponentially with the dimensionality of the state-action space of the environment, posing challenges for large-scale applications. To address these challenges, Hierarchical Reinforcement Learning (HRL) and Multi-Agent Reinforcement Learning (MARL) offer promising solutions by decomposing complex tasks into simpler sub-tasks and distributing decision-making across multiple agents \cite{dayan1992feudal, littman1994markov}. This idea is captured in the factored MDP framework \cite{osband2014near}, where the original MDP is decomposed into a set of smaller and independently evolving MDPs. In this case, the total sample complexity is determined by the sum of the sizes of the state-action spaces for each individual MDP, rather than their product. As a result, the problem size no longer scales exponentially with the dimension of the problem.

% Additionally, a trade-off needs to be made short-term or greedy actions and long-term actions that might contain synergies with other actions. When only acting greedily, optimal action sequences are difficult to discover. However, acting out action sequences causes a loss in flexibility that is necessary when sudden changes in the environment are observed.

This study integrates the strengths of MARL and HRL to develop a centrally coordinated multi-agent (CCMA) architecture. The design incorporates short-term action suggestions alongside a coordinating agent capable of managing multi-step action sequences. Different algorithms for short-term action suggestions and coordinating agents are compared, ranging from greedy and rule-based strategies to fully RL-based architectures.

%The Proximal Policy Optimization (PPO) \cite{schulman2017proximal} algorithm will be used for all RL-based modules, extending the action space and reward functions with domain-specific knowledge from power systems. A hierarchical architecture with three levels will be developed. The first level determines whether any action is required, the second level utilizes a multi-agent approach to suggest an action per region, and the third level determines which of these actions works best in the current grid state. The architectures will be evaluated based on performance across various grid sizes.

The main contributions can be summarized as follows:
\begin{itemize}
    \item We introduce a novel CCMA architecture featuring a coordinator that determines the next action by leveraging regional action suggestions from individual agents alongside global state information. This design enables action space factorization while maintaining centralized coordination. %(Section \ref{sec:architecture}).
    \item We describe several variants of the CCMA architecture, incorporating rule-based, greedy, and RL-based modules. These architectures range from fully deterministic approaches (rule-based and greedy) to fully learned systems (entirely RL-based) and hybrid designs that combine both strategies.
    \item We evaluate the proposed architecture against baseline models \cite{grid2op, l2rpnbaselines}, demonstrating that some of its variants surpass all baselines in both sample efficiency and overall performance, hence effectively factor the MDP. %(Section \ref{sec:results}).
    \item Finally, we show that rule-based coordinators are effective in the smallest 5-bus network. However, when tested with larger networks, these architectures no longer match the performance of an RL-based coordinator. %(Section \ref{sec:conclusions}).
\end{itemize}

The paper is organized as follows: Section \ref{sec:background} covers the background and related work. Section \ref{sec:con_frame} defines the control framework and Section \ref{sec:methods} describes the power grid environment.
The central Section \ref{sec:power_system_agents} introduces the CCMA architecture.
Finally, Section \ref{sec:results} presents the experiments and results, followed by a discussion and future work in Section \ref{sec:summary}.

\section{Background and Related Work}\label{sec:background}
This section deals with the modeling of the PNC problem (Section \ref{sec:L2RPN}), applications of RL in this domain (Section \ref{sec:RLL2RPN}), and recent HRL and MARL implementations that focus on improving sample efficiency and overcoming the curse of dimensionality (Section \ref{sec:SRL}).

\subsection{Power Network Control}\label{sec:L2RPN}
Secure operation of power networks is required both in normal operating states as well as in contingency states (i.e., after the loss of any single element on the network). That is, the following requirements must be met: (i) In the normal operating state, the power flows on equipment, voltage and frequency are within pre-defined limits in real-time; (ii) In the contingency state the power flows on equipment, voltage and frequency are within pre-defined limits. Loss of elements can be anticipated (scheduled outages of
equipment) or unanticipated (faults for lightning, wind, spontaneous equipment failure). Cascading failures
must be avoided at all times to prevent blackouts (corresponding to the game over state in the RL challenge).
%(Kelly et al., 2020).

Importantly, each of the control actions performed by power system operators usually not only affects the
current state of the power system but also the future state and availability of future control actions, that
is, short-term actions can have long-term consequences. As a result, the decision problem of power system
operators is typically a sequential decision-making problem in a combinatorial action space in which the current decision can affect all
future decisions. Moreover, due to possible nondeterministic changes of the power system state (e.g., due
to unplanned outages or the intermittent behaviour of renewable energy sources) and different sources of
error (e.g., measurement errors, state estimation errors, flawed judgement) the operators need to handle
uncertainty in their decisions. Finally, operational decisions must often be made quickly, under hard time
constraints \cite{viebahn2022potential}.

%The primary goal of PNC is to prevent blackouts by ensuring power flow stays within thermal limits, regulating voltage, and balancing generation and load. As grids modernize and integrate variable energy sources, congestion management has become critical to address violations of voltage, frequency, thermal, and stability limits that risk equipment damage and system failures.

% The Grid2Op framework \cite{grid2op}, based on Markov Decision Process (MDP) modeling \cite{sutton2018reinforcement}, is frequently used to simulate and assess PNC strategies, particularly in the context of the ``Learning to Run a Power Network" (L2RPN) challenges initiated by the RTE. These challenges push for the development of robust PNC controllers.

% describe environment
%The Grid2Op framework \cite{grid2op} models power grids as graphs with substations, powerlines, generators, loads, and shunts. Substations manage grid connectivity via two busbars, which can couple or decouple to alter connectivity (see Fig. \ref{fig:topaction}). Powerlines enable power flow within thermal limits, generators provide power, loads consume power, and shunts influence power flow by creating low-resistance paths. Generators and loads, collectively known as injections, differ in that generators can be redispatched to manage congestion, while loads vary without constraints.

% actions uitleg
% This research focuses on solely the topological action space, which governs the configuration of powerlines, generators, and loads across busbars within substations. By turning various grid elements on or off, the network topology changes. The switching is accomplished by influencing what elements are connected to each other. These actions reroute the power flow, relieving pressure from overloaded lines to other parts of the grid. Fig. \ref{fig:topaction} illustrates the effect of such an action on the network.

%\begin{figure}
%    \centering
%    \includegraphics[width=\linewidth]{Figures/node_splitting.png}
%    \caption{A graphic illustration of how a substation can be split into two separate nodes, dividing its connected lines, taken from \protect\cite{dorfer2022power}.}
%    \label{fig:topaction}
%\end{figure}

\subsection{Deep RL for Power Network Control}\label{sec:RLL2RPN}
Deep RL has demonstrated significant potential in PNC, enabling robust and adaptable behavior over extended time horizons \cite{viebahn2022potential}. This capability surpasses the limitations of expert systems \cite{marot2018expert} and optimization methods, which are constrained by computation time \cite{ruiz2016security,little2021optimal,heidarifar2021optimal}.

In L2RPN competitions, successful solutions typically combine expert rules, RL agents, and brute force simulations for action validation. Notably, RL agents based on Proximal Policy Optimization (PPO) \cite{schulman2017proximal}, with reduced action spaces, have performed well. This approach was used in top-ranking competition entries (e.g. \cite{marot2021learning}, further explored in  studies \cite{lehna2023managing, manczak2023hierarchical, chauhan2023powrl}).

% \todo{Idea behind the section below is that we can link the power-system knowledge and acting in hazardous situations part in the methods back to this. The simulations could be linked to future research ideas to improve performance.}
Several commonalities emerge among top-ranking approaches. Most successful methods implement intervention only during hazardous situations \cite{yoon2020winning, lan2020ai, manczak2023hierarchical, chauhan2023powrl, dorfer2022power, lehna2023managing}. 
In addition, many such approaches integrate learned modules with simulation-based action evaluation. For instance, this strategy was key to the success of the winning teams in both 2019 \cite{lan2020ai} and 2021 \cite{icapswinner}. 
Notably, the second-place competitor in 2021 employed an advanced expert system, underscoring the value of incorporating domain knowledge from power systems \cite{marot2022learning}. %\textcolor{red}{Barbera: Going back to base topology additional explain here?}

\subsection{Curse of Dimensionality}\label{sec:SRL}
The combinatorial complexity of topological actions in PNC poses a challenge for deep RL by hindering complete and consistent convergence and exploration of action and state spaces. HRL simplifies learning by breaking tasks into subtasks, enabling more focused and sample efficient learning per task as well as increased interpretability. MARL distributes decision-making across agents, effectively factorizing the MDP's action space.

\subsubsection{Hierarchical Reinforcement Learning}
In the L2RPN WCCI 2020 competition, the winning agent for a 36-bus network used a two-tiered framework: a high-level policy proposed a goal topology, while a low-level policy determined the sequence of individual topological actions \cite{yoon2020winning}.

Later, a three-level HRL framework was introduced \cite{manczak2023hierarchical}, where the top level activates the agent in hazardous situations, the intermediate level selects a substation using RL, and the lowest level identifies a configuration. They tested greedy and RL-based approaches for the lowest level, and compared PPO and Soft Actor Critic (SAC) \cite{haarnoja2018soft} for the RL-based modules. They found PPO to have faster convergence, smaller variance, and higher expected rewards.

%MARL
\subsubsection{Multi-Agent Reinforcement Learning}
The action space in PNC can be factorized into exclusive subsets, one for each substation, enabling a natural MARL framework in a fully cooperative setting where all agents optimize the same collective objective. Following this approach, \citet{van2023multi} extended \citet{manczak2023hierarchical} by introducing multiple agents at the lowest level, with each substation controlled by an agent and a heuristic-based intermediate level. They trained agents using independent PPO (IPPO) and dependent PPO (DPPO). On a 5-bus network, both achieved optimal scores, though single-agent PPO converged faster. However, they anticipate that MARL's advantages will become more pronounced when scaled up to larger networks.

In other domains, \citet{yu2022surprising} showed strong PPO-based multi-agent performance in testbeds, recommending best practices like value normalization, integrating global and local information, 
and limiting sample re-use to avoid training instability from MARL non-stationarity.


% \begin{figure*}[th!]
%     \centering
%     \includegraphics[width=\linewidth]{Figures/fc.drawio.png}
%     \caption{Diagram of the Feedback Control Framework used in this research. The goal to be achieved (and maintained) in the system is an input to the entire framework. The comparator determines the difference between the current state of the system and the desired one. There are then two possible paths for the control flow. If a goal state is reached then no action is performed and the reward is accumulated. If the system is not in the goal state then an agent is called. Periodically this agent can be updated by a trainer. The dashed line represents the only link between the two loops, as the trainer will perceive the accumulated rewards of consecutive time-steps where the {\it do-nothing} was used for control}
%     \label{fig:feedback-control}
% \end{figure*}

\begin{figure*}[th!]
    \centering
    \includegraphics[width=\linewidth]{Figures/fc_new.jpg}
    \caption{Diagram of the Feedback Control Framework used in this research. The goal to be achieved (and maintained) in the system is an input to the entire framework. The comparator determines the difference between the current state of the system and the desired one. There are then two possible paths for the control flow. If a goal state is reached then no action is performed and the reward is accumulated. If the system is not in the goal state then an agent is called. Periodically this agent can be updated by a trainer. The dashed line represents the only link between the two loops, as the trainer will perceive the accumulated rewards of consecutive time-steps where the {\it do-nothing} was used for control}
    \label{fig:feedback-control}
\end{figure*}

% Explain purpose
% Write proper caption with small FC framework summary
% explain dashed line
% normalize font
%   remove box from observation
%   remove box of accumulate
%   rename accumulate as acummulated reward
%   trainer and agent same color (blue)
%   comparator and gate same color (orange)
%   make `system` same font as sMDP (also bold)

\section{Control framework}\label{sec:con_frame}

%\todo[inline]{Cite FC framework paper with MDP}

%\todo[inline]{should this section be before background work? We mention Deep RL before talking about RL}
%This section defines the used MDP in Section \ref{sec:MDP} and describes the used RL algorithm in Section \ref{sec:PPO}.

Figure \ref{fig:feedback-control} shows a schematic of the feedback control (FC) framework employed in this study. %It's derived from the standard model for computational models of intelligent action and cognition is a \emph{feedback control organization} (FCO). 
The \emph{goal} represents a target state that is to be maintained within the controlled \emph{system} (light orange box). The \emph{comparator} (left orange box) computes the \emph{error} between the current and the goal state of the system. The controller produces an \emph{action} to be performed in the system in order to get closer to or maintain the goal state of the system. The system produces a \emph{reward} signal and a new \emph{state} once the action is performed.

As shown in Fig. \ref{fig:feedback-control}, our controller has two modes of operation. After the error is computed, a \emph{gate} (right orange box) determines which mode to use.
%Each of the operational modes computes an action, but do so in different ways.
If the goal state is achieved ($\text{error}=0$) then the gate selects the lower mode of operation in which a constant action ("do nothing") is performed in the system. If the goal state is not achieved then an \emph{agent} (lower blue box) is used to compute an action. The state of the system (and possibly the error) is used to create the input \emph{observation} for the agent.

%In our FC architecture there are two circularly organized control loops. In each control step only one of the two control loops is being used. The \textit{GATE} component decides based on the error which control loops to use. The two loops are almost completely isolated except for one connection (dashed line in Fig. \ref{fig:feedback-control}).
%The basic unit of behavior is circularly organized, and consists of four elements: $(i)$ observations of the environment (or a sensor), $(ii)$ a reference value (representing the goal), $(iii)$ a comparator, and $(iv)$ an effector that operates on the environment.

%In a negative feedback loop an observed value is compared to the reference value, and actions are made (if necessary) to shift the observed value in the direction of the reference value. The controller may take into account both the error (or goal) and the full observation when acting.

%\todo[inline]{in the future we should not talk about only MDP}

%\subsection{MDP}

%The second mode of operation stores past experiences of its interaction with the system in order to train an agent. Experiences are not stored when the first mode of operation is being used. The system interactions are modeled using an MDO \todo{cite}. The \emph{policy}\todo{just say agent?} of the MDP is used as the controller. The error is added to the state of the system to create the input observation for the agent.

%he \emph{trainer} component periodically uses collected experiences to \todo{do we make a link between state/obs and trainer?} updated the parameters of the agent. \todo{connection trainer and agent special color} It is worth noting that the trainer won't update the agent every time-stamp. Instead this will only happen periodically and outside the FC loop.

The agent can either be rule-based or trained. In case of a trained agent, the agent mode of operation stores past experiences of its interaction with the system for training the agent. The controller can switch between modes of operation multiple times. Switching from the agent to the do-nothing operation may be the result of the agent's action. The system may also remain in the do-nothing mode of operation for multiple timesteps. In this case, the resulting rewards are accumulated. Once the agent mode of operation is used again, the \emph{trainer} (upper blue box) will receive these accumulated rewards.
%This will happen only once until the first mode of operation is used again. 
This allows the agent to receive feedback for the effect that its action has on the gate. Effectively, the agent mode of operation is thus operating as a semi-MDP (sMDP) \cite{SUTTON1999181,Baykal2010}.
%This allows for the two modes of operation to pass control between each other for multiple consecutive timesteps. 
%In Fig. \ref{fig:feedback-control} it can be seen that the control loops of the two modes of operation are almost completely isolated except for one connection from the \emph{accumulated} reward of the first mode of operation and the trainer.
%In the do-nothing mode of operation only rewards are accumulated. In other words, the system interactions are modeled as an sMDP \todo{cite}. The \emph{policy}\todo{just say agent?} of the sMDP is used as the controller.
The trainer component periodically uses collected experiences to update the parameters of the agent. We note that the trainer does not update the agent every time-stamp. Instead, this happens only periodically.
%and outside the FC loop.

%The agent's experiences are collected in transitions. If at time $t$ under observation $o_t$ the RL agent performs action $a_t$ that will result in a sequence of $k$ timesteps below the activity threshold. If $r_{t:t+k}$ is the accumulated reward in those $k$ steps then the agent will experience the  $\langle o_t, a_t, r_{t:t+k}, o_{t+k} \rangle$.

%A semi-MDP extends a MDP by allowing actions to take variable amounts of time, rather than assuming a fixed time step. This means that in an semi-MDP, the transition from one state to another can occur after a random time duration. Here, the reward is accumulated over the variable duration of time between state transitions and is added to the cumulative reward from previous time steps, ensuring that all skipped time steps contribute to the total reward.

% When the MDP passes control to the first mode of operation (by reacing a goal state) it will percieve only one timestamp passing between then and the next time the second mode of operation is used again.

%\subsection{Markov Decision Process}\label{sec:MDP}

%The controller in a FCO (see section \ref{sec:FCO}) may be engineered manually. This is what we refer to as a rule-based controller. However if the system controlled is too complex it may be unfeasible to use only rule-based controller. This could be due to several reasons. A rule-based agent may need to be over-engineered to be effective in a complex system. Alternatively, it could be that the rules are simple in principle, but have an unfeasible computation complexity when deployed in practice.

%For this reason one may want to use a hybrid controller that in difficult case can used machine learning (ML) algorithms trained on past experience to control complex systems more effectively. 

%Let's consider for now only the case in which we want to use ML to learn a controller. In this case the problem presented in section \ref{sec:L2RPN} can be modeled as a Markov Decision Process (MDP) \cite{sutton2018reinforcement, sutton1999between}. This framework aims at modeling the data (or in this case experienced) that can be used to learn such a controller. MDPs are best suited at modeling sequential decision making processes, in which a series of decisions need to be taken by an agent (our ML controller) in a series of discrete timesteps. The problem is sequential because the effects of actions may not be immediate and may affect the system's state in timesteps much later in time.

%We model the sequential decision-making problem presented in Section \ref{sec:L2RPN} as a semi-MDP \cite{sutton2018reinforcement, sutton1999between}. 

%An MDP is defined by a tuple $\langle \mathcal{S}, \mathcal{A}, \mathcal{P}, \mathcal{R}\rangle$, where at each timestep $t$, an agent interacts with the environment in the following way:
%\begin{itemize}
    %\item The agent observes the current state of the environment $s_t \in \mathcal{S}$
    %\item The agent selects an action $a_t \in \mathcal{A}_t$ from a state dependent probability distribution $\pi(a_t \mid s_t)$ (the \emph{policy})
    %\item The environment transitions to a new state $s_{t+1} \in \mathcal{S}$ according to an unknown transition probability $\mathcal{P}(s_{t+1} \mid s_t, a_t)$
    %\item The agent receives an immediate reward $r_t = r(s_t, a_t) \in \mathcal{R}$
%\end{itemize}

%This process continues iteratively as the agent interacts with the environment over time.

%\subsection{Linking FCP and MDP, the Activity thresholds}
%\label{sec:activityThreshold}


\section{Power System Environment}\label{sec:methods}
First, in Section \ref{sec:setup}, we describe the power grids used for experimental analysis, which is the \emph{system} in Fig. \ref{fig:feedback-control}. Subsequently, in Section \ref{fcpower}, we describe the corresponding \emph{gate}, \emph{state}, \emph{reward}, and \emph{actions}.
%Afterwards, Section \ref{sec:architecture} describes the used baselines and proposed architecture in detail.


\subsection{Experimental Setup}\label{sec:setup}
We consider two simulated power grids: the \textit{IEEE case 5} and \textit{IEEE case 14} networks. In the Grid2Op simulator \cite{grid2op}, the power grid environments are called \textit{rte\_case5\_example} and \textit{l2rpn\_case14\_sandbox} (see the Grid2Op documentation for visualizations of these power grids). The 5-bus network comprises twenty scenarios, each spanning 2016 timesteps. The 14-bus network contains 1004 scenarios, each unfolding over 8064 timesteps. Each timestep, representing a five-minute interval, incorporates unknown changes in loads and production.

Moreover, unplanned outages can be included. These outages involve temporarily disconnecting a powerline for a specified duration. These unplanned outages are initiated by an opponent, adding stochasticity and further increasing task complexity. This opponent introduces realistic challenges faced by power grid operators, ensuring the system's stability is preserved even during unforeseen contingencies. It is designed to randomly disconnect a line from a predefined set of targets, with a cooldown period implemented to delay consecutive attacks. After each attack, the disconnected line is automatically restored. Table \ref{tab:opponent_params} outlines the parameters of these attacks.

\begin{table}[tb]
    \centering
    \caption{The used parameters for the opponent in Grid2Op, namely the attackable lines ($l_{origin-extremity}$), the attack duration (in timesteps), and frequency (in timesteps).}
    \begin{tabular}{c c c}
    \toprule
     & 5-bus & 14-bus \\
    \midrule
    Attackable lines & $l_{0-2}, l_{0-4}$ & $l_{3-4}, l_{3-6},$  $l_{3-8}, l_{11-12}$ \\
    Attack duration (ts) & 48 & 48 \\
    Attack cooldown (ts) & 144 & 144 \\
    \bottomrule
    \end{tabular}
    \label{tab:opponent_params}
\end{table}

\subsubsection{Incorporated Domain Knowledge}
% automatic line reconnection
Several domain-inspired features are incorporated in all baselines and architectures to boost performance. First, automatic powerline reconnection is implemented. Powerlines disconnected by overcurrent events face a ten-timestep cooldown before reconnection. Since agents are limited to topological actions, disconnected lines are automatically restored to their previous state one timestep after disconnection. This eliminates the need for the agent to handle reconnections, allowing it to focus exclusively on optimizing grid configurations.

% ABBC
Second, grids tend to be more stable during nighttime. This is due to lower demand (and thus strain) on the grid. This domain knowledge can be exploited to automatically improve the system's performance by
%Second, grids tend to be stable during nighttime and grids with a larger topological depth (i.e. the number of substations not in their default configuration) tend to be unstable. To counteract this instability over time, performance can be improved by automatically 
returning the grid back to its original grid state where all powerlines within each substation are interconnected \cite{lehna2023managing}. In this research, this is implemented to happen between 3AM and 6AM. Before this reversion is performed, the agent simulates whether this action would not likely lead to a game-over state, only performing the action if all estimated relative powerline thermal loads remain below 90\% of their maximum capacity.

% Fig. \ref{fig:domain} illustrates the effect of reverting to all busbars closed during night by comparing the greedy agent on the 36-bus network without (left) and with (right) this feature. By reverting back to base topology during nighttime, the mean number of timesteps survived increases with 128\%.

% \begin{figure}
%     \centering
%     \includegraphics[width=\linewidth]{Figures/domain_knowledge.png}
%     \caption{A comparison between the performance of the adapted greedy agent on the 36-bus network without and with reverting to base topology during nighttime.}
%     \label{fig:domain}
% \end{figure}

% split and prio scenarios
Lastly, it is worth noting that the grid is typically most stable during nighttime. Normally, agents navigate scenarios ranging from 2016 timesteps (for the 5-bus network) to 8064 timesteps (for the 14-bus), equivalent to seven to 28 days. If an agent fails during training, the scenario terminates. To expose agents to diverse circumstances, scenarios can be subdivided into smaller segments, increasing the likelihood that the agent encounters later timesteps in longer scenarios, potentially speeding up convergence. We therefore split the scenarios into chunks of two days, starting during nighttime. 

\subsection{Power System Feedback Control Elements}\label{fcpower}

This subsection explains how each component of the FC framework presented in Sec. \ref{sec:con_frame} is realized in a power system setting.

%In L2RPN approaches to topology control it is common to employ a feedback control framework in which the current grid state is observed and the maximum line loading is compared with a reference value

\paragraph{Goal}
% Obviously, the general goal is to maintain a grid state without any overloaded lines. We adopt the approach employed in most L2RPN studies, namely, the goal state is constant over time and represented by a threshold $\tilde{\rho}$ over the maximum line loading $\max{(\rho)}$ of the power grid. The goal is to maintain grid states for which $\max{(\rho)} < \tilde{\rho}$. We choose a value of $\tilde{\rho}=0.95$ in this study.
Obviously, the general goal is to maintain a grid state without any overloaded lines. In this study, we adopt the approach employed in most L2RPN studies. Specifically, the grid state is characterized by the loading of individual power lines, represented as a vector $\rho = (\rho_1, \rho_2, \dots, \rho_n)$ where each element \(\rho_i\) denotes the loading of a specific line in the power grid. The goal state is defined by a threshold \(\tilde{\rho}\), a single scalar value representing the allowable maximum line loading. The goal is to maintain $\max(\rho) < \tilde{\rho}$, where \(\max(\rho)\) refers to the maximum loading among all the lines in the grid. In this study, we set \(\tilde{\rho} = 0.95\).

%All agents that we will consider will make use of an activity threshold. This is an upper limit threshold applied to the maximum load of all the lines within the grid at a given point in time. If the maximum load is below the threshold, then no topological reconfiguration is needed and the topology of the grid is not changed. The algorithms that we will present for agents are applied only to timesteps where the maximum load is above the activity threshold.

%\subsection{Power System as an MDP}
\paragraph{Gate}
We use a common rule-based gate. If the maximum line loading is lower than the threshold $\tilde{\rho}$ then the do-nothing mode of operation is used. If the maximum line loading surpasses the threshold $\tilde{\rho}$ then the agent mode of operation is used.

\paragraph{State}
The state %\(s_t \in S\) 
of the system consists of (i) the current grid topology,
%configuration, indicating the bus to which the object is connected in its substation,
(ii) the current load $\rho_{l}$ on each powerline $l$, measured as the fraction of the capacity of the powerline, (iii) the active power flow at each end of each powerline, (iv) the number of overflow timesteps for each powerline, and (v) the current power production and consumption of generators and loads.

In Grid2Op, if a blackout occurs, it results in a game-over state. A blackout happens when the power grid fails to meet the requirements of the generators and loads due to issues like line disconnections, overloading, or other critical failures. Once a blackout is detected, the simulation ends immediately, as the grid's stability can no longer be maintained. 

\paragraph{Reward}

We employ the scaled L2RPN reward \cite{grid2op, manczak2023hierarchical},
% \begin{equation}\label{eq:L2RPN_scaled}
% R_{L2RPN\_scaled} = 
% \begin{cases}
%     -0.5, & \text{if } \text{game-over state}\\
%     \frac{\sum_{i=0}^{n\_lines}\left(1-\Big(1-\frac{I_{limit}^{i} - I_{flow}^{i}}{I_{limit}^{i}}\Big)^2\right)}{n\_lines},              & \text{otherwise}
% \end{cases}
% \end{equation}
\noindent which calculates the squared margin of the current flow relative to the current limit for each powerline. The larger the margin, meaning the further the current flow is from the limit, the higher the reward, encouraging states where powerlines are not operating near their maximum capacity.
%\todo{refer to grid2op as well and say standard reward, paper 2019 antoine}

% Here, the reward is $-0.5$ if a game-over state is triggered, providing a significant penalty to discourage such actions. Otherwise, the reward is calculated by averaging the transformed margins across all powerlines. The transformation $1-(1-\frac{I_{limit}^{i} - I_{flow}^{i}}{I_{limit}^{i}})^2$ normalizes the margin and ensures that the reward lies between 0 and 1. A line is considered overloaded when the energy flow $I_{flow}^{i}$ exceeds its limit $i$ is larger than its limit $I_{limit}^{i}$ in which case the margin is set to zero.



\paragraph{Actions}

%The only actions used to control the system are \emph{set bus topological reconfiguration actions}.

In this study, we introduce a new action space designed to ensure structural N-1 security. Structural N-1 security means that the grid remains operational and avoids an automatic blackout in the event of a single contingency, such as an unplanned powerline outage. We compare the new action space with the default action space which only excludes symmetrical counterparts from all possible substation configurations. %The action space can be further reduced using domain knowledge to create a structurally N-1 secure action space. 
Table \ref{tab:networks} shows the size of each action space.
\begin{table}[h]
    \centering
    \caption{A comparison between the 5-bus and 14-bus networks. Only topological actions are considered and the do-nothing action is excluded.}%The first action space excludes only symmetries, and the second action also excludes structurally N-1 unstable configurations.}
    \begin{tabular}{c c c c c}
    \toprule
     &  \multicolumn{2}{c}{symmetry-filtered} & \multicolumn{2}{c}{N-1 secure} \\
    \midrule
       & \# actions & \# topologies & \# actions & \# topologies \\
    \midrule
    5-bus  & 58 & 31320 & 24 & 364 \\
    14-bus  & 178 & 3.9e+11 & 73 & 3.3e+5 \\
    \bottomrule
    \end{tabular}
    \label{tab:networks}
\end{table}


% The size of the symmetry-filtered action space can be computed via two terms. First, the term
% \begin{equation}\label{eq:total}
% \alpha(n) = 2^{n-1},
% \end{equation}
% represents the number of all possible configurations of a substation with two busbars and $n$ elements connected to a substation, excluding symmetries (factor $\frac{1}{2}$). This does not account for the constraint that no loads and generators, collectively known as injections, should be isolated. The second term
% \begin{equation}\label{eq:illegal}
% \gamma(n') = 2^{n'} - 1,
% \end{equation}
% counts all configurations where only injections and no powerlines are connected to a busbar, with $n'$ the number of injections connected to the substation. There are $2^{n'}$ possible combinations of injections in the configuration that all powerlines are connected to a single busbar. The $-1$ excludes the configuration where all injections are also connected to this single busbar. 
% Consequently, the size of the symmetry-filtered action space is given via
% \begin{equation}\label{eq:alpha}
% \tau_{sym} = \alpha(n) - \gamma(n')\ .
% \end{equation}

The size of the symmetry-filtered action space can be computed using two key terms. The first term, \(\alpha(n) = 2^{n-1}\), represents the total number of possible configurations for a substation with two busbars and \(n\) elements connected to it, excluding symmetrical configurations (a factor of \(\frac{1}{2}\)). However, this term does not account for the constraint that no loads or generators (collectively referred to as injections) should be isolated.

The second term, \(\gamma(n') = 2^{n'} - 1\), accounts for all configurations where only injections and no powerlines are connected to a busbar. Here, \(n'\) is the number of injections connected to the substation. The \(2^{n'}\) represents all possible combinations of injections in a configuration where all powerlines are connected to a single busbar, and the \(-1\) excludes the configuration where all injections are also connected to this single busbar.

Using these terms, the size of the symmetry-filtered action space is calculated as:

\begin{equation}\label{eq:alpha}
\tau_{sym} = \alpha(n) - \gamma(n')\ .
\end{equation}

By always maintaining zero or at least two powerlines connected to each busbar, the N-1 secure action space ensures that the failure of a single powerline does not result in an immediate game-over state due to the loss of connection to its injections. 
The size of the structurally N-1 secure action space is:

\begin{equation}\label{eq:TenneTshort}
\tau_{N-1} = \alpha(n) - \gamma(n') - \epsilon(n', n'')\ ,
\end{equation}

\noindent
with:

\begin{equation}\label{eq:epsilon}
\epsilon(n', n'') = 2^{n'}\cdot(n'' - \delta_{n'',2} - \delta_{n'',1})\ ,
\end{equation}

\noindent
where \(n''\) is the number of powerlines, and \(\epsilon(n', n'')\) removes all configurations where only one powerline is connected to a busbar. The Kronecker delta, \(\delta_{n'',k}\), is defined as:

\[
\delta_{n'',k} = 
\begin{cases} 
1 & \text{if } n'' = k, \\
0 & \text{otherwise.}
\end{cases}
\]

\noindent
This means that \(\delta_{n'',2}\) is one when exactly two powerlines are present (\(n'' = 2\)) and zero otherwise, while \(\delta_{n'',1}\) is one when only one powerline exists (\(n'' = 1\)) and zero otherwise. These terms ensure that configurations with invalid powerline setups, such as only one powerline connected or configurations invalid for all possible injection setups, are excluded. In practice, such cases are always kept in the base topology where all elements are connected.

Combined, \(\tau_{N-1}\) excludes all configurations that connect any injection to only a single powerline, ensuring structural N-1 security. This equation is valid as long as at least one powerline exists.
% The size of the structurally N-1 secure action space is
% \begin{equation}\label{eq:TenneTshort}
% \tau_{N-1} = \alpha(n) - \gamma(n) - \epsilon(n', n'')\ ,
% \end{equation}
% with
% \begin{equation}\label{eq:epsilon}
% \epsilon(n', n'') = 2^{n'}\cdot(n'' - \delta_{n'',2} - \delta_{n'',1})\ ,
% \end{equation}
% where $n''$ is the number of powerlines. $\epsilon(n', n'')$ constraints all possibilities where either powerline is connected to either of two busbars. $\delta_{n'',2}$ accounts for the situation with two powerlines, in which one single powerline always means exactly two single powerlines. $\delta_{n'',1}$ accounts for situation in which only one powerline exists. In theory, these situations are invalid. In practice however, these configurations are always kept in base topology where all elements are connected. These configurations are invalid for all possible configurations of injections.
% Combined, $\tau_{N-1}$ excludes all configurations that connect any injection to only a single powerline. The equation is valid whenever one or more powerlines exist.



\section{Power system agents}\label{sec:power_system_agents}

In this section, we present the agent configurations used in the second mode of operation of the FC framework (see Sec. \ref{sec:con_frame}, Fig. \ref{fig:feedback-control}). The greedy agent and the single RL agent (Section \ref{sec:greedysingleRL}) approach the sMDP without decomposition.
In contrast, this study introduces a multi-agent architecture that divides the sMDP into a smaller set of independent tasks (see Sec. \ref{sec:architecture}).
The two common baselines serve as a reference to evaluate the impact of decomposing the MDP.

\subsection{Greedy Agent \& Single RL Agent}\label{sec:greedysingleRL}

The greedy-agent baseline is a fully rule-based agent. A greedy agent simulates the effect of every action in the action space for one timestep in the future. Then, the agent selects the action that maximizes a given KPI when simulated. In our case, the KPI is the maximum loading of the grid. See \citet{de2024imitation} for a pseudo-code of the greedy agent.

%The RL Baseline Agent uses known actor-cricit RL methods to solve the task at hand. In this study we only look at PPO as previous studies \todo{ref} show this method to be more reliable in PNC tasks. 
The single RL agent uses a feed-forward neural network to implement a policy for choosing actions. It also chooses actions from the entire action space. A state-of-the-art RL algorithm is used to train the policy, namely, the Proximal Policy Optimization (PPO) algorithm \cite{schulman2017proximal}. 


\subsection{Multi-Agent Architecture}\label{sec:architecture}
%These agents are the main contributions of this study. 
We divide the overall agent task into two levels of abstraction, following the approaches outlined in \citep{manczak2023hierarchical, van2023multi}. One level consists of deciding the topological reconfiguration of individual regions (e.g. substations). The other level consists of selecting which region should be reconfigured. The agent selecting the region is called the \textit{coordinator}. The reconfiguration of regions may be decided by multiple agents, called \textit{regional agents}, each proposing a topological reconfiguration of its respective region. In our architecture the topological reconfiguration of each individual region is decided first, and the region to be reconfigured is selected second. Figure \ref{fig:difference_hierarchies} illustrates on the left the high-level hierarchy used in \citet{manczak2023hierarchical, van2023multi} where the regional agents are called after the coordinator, and on the right the decision making order proposed in this study. 

\begin{figure}[tb]
    \centering
    \centerline{\includegraphics[width=\linewidth]{Figures/architecture_diff_conference.jpg}}
    \caption{Left: Hierarchy proposed by \citet{manczak2023hierarchical, van2023multi}. First a region (e.g., a substation) is selected, and then a topological configuration for that region (and that region only) is selected. Right: Hierarchy proposed in this study. For each region a topological reconfiguration is proposed concurrently, and a coordinator then selects the best proposed action (e.g., a substation configuration).}
    \label{fig:difference_hierarchies}
\end{figure}

% \begin{figure*}[tb]
%     \centering
%     \includegraphics[width=\linewidth]{Figures/architecture_conference.drawio.png}
%     \caption{An overview of all possible multi-agent architectures. First the maximum line loading in the current observation \textbf{OBS} is compared to a threshold by the gate to determine whether to not act (upper path) or to try to reconfigure the topology of the grid (lower path). In the latter case, first all the \textbf{REGIONAL AGENTS} propose an action to reconfigure their respective region (or do nothing). This list of action is finally passed to the \textbf{COORDINATING AGENT} which select one of the regions (i.e. one of the actions proposed by any of the regional agents). The final result is always an action in the topological reconfiguration space.}
%     \label{fig:agent-architecture}
% \end{figure*}

\begin{figure*}[tb]
    \centering
    \includegraphics[width=\linewidth]{Figures/architecture_conference_new.jpg}
    \caption{An overview of all possible multi-agent architectures. First the maximum line loading in the current observation \textbf{OBS} is compared to a threshold by the gate to determine whether to not act (upper path) or to try to reconfigure the topology of the grid (lower path). In the latter case, first all the \textbf{REGIONAL AGENTS} propose an action to reconfigure their respective region (or do nothing). This list of action is finally passed to the \textbf{COORDINATING AGENT} which select one of the regions (i.e. one of the actions proposed by any of the regional agents). The final result is always an action in the topological reconfiguration space.}
    \label{fig:agent-architecture}
\end{figure*}

%It should be noted that the task being factorized is only the agent's task within the second mode of operation, as per section \ref{sec:con_frame}. Therefore, this factorization is applied only when the agent is used.
%As per section \ref{sec:} the multi-agent architectures also make use of a (global) activity threshold. In practice in multi-agent system we can model the activity threshold as a threshold agent. The threshold agent can perform one of two action depending on whether the maximum load is below or above the threshold. In former case then a do-nothing action is performed \todo{first mention?}. In the latter case the regional agents will be triggered. Thus, the decision making process described above will take place.\todo[inline]{jan: this last sentence ok?}
%All architectures, including the single-agent ones, make use of a \textit{threshold agent} to detect a congestion in the current state. Each architecture only acts in timesteps where \todo{when?} the threshold agent detects a congestion. This is done by comparing the maximum loading of all lines in the current state with a threshold $\rho_{\theta}$ ($0.95$ in this study). If the maximum loading is below $\rho_{\theta}$, then a do-nothing action will be automatically performed. Therefore, all RL architectures (single or multi agent) act in a semi-MDP. This because the rewards resulting from do-nothing actions performed by the threshold agent are accumulated. The accumulated reward is assigned to the last action of the RL agent. Moreover, each RL agent only perceives the observations before and after a sequence of do-nothing actions from the threshold agent in its transitions.
%\todo[inline]{Jan: check this}\todo[inline]{Jan: this is the first mention of do-nothing in section 4 but the concept of a do-nothing action should be introduced in the background (hence why no italic)}

Our architecture allows the coordinator to take into account the proposed regional reconfiguration in its decision making process. We believe that this allows for a more clean separation between the two levels of abstraction. In \citet{manczak2023hierarchical, van2023multi}, the coordinator may need to anticipate implicitly what reconfiguration will be proposed for the region it is selecting. This may result in the coordinator having to solve both tasks as a single agent. In other words, we believe that the regional topology selection task needs to be solved (either implicitly or explicitly) before being able to determine which region to select.
%\todo[inline]{Perhaps rephrase a bit, explain that the point is that no matter what in the old architecture our decision making order is still happening because it NEEDs to happen, but it is happening implicitly}

% as in the architectures investigated by \cite{manczak2023hierarchical, van2023multi}, the middle level selects a region without knowing the specific action it will take, while the regional actions are chosen without understanding the intent behind their selection. As a result, both levels are still required to solve the same overall task: the middle level must anticipate the action to be taken by the chosen region, and the regional agents must infer the purpose for which they were selected. In contrast, the proposed architecture ensures that all regions are selected, and thus, each always proposes an action solely based on the current grid state. The central coordinator only needs to determine which of these actions is most appropriate for the current timestep. This stricter task decomposition potentially reduces non-stationarity.

%in \cite{manczak2023hierarchical, van2023multi} the coordinator may need to anticipate implicitly what reconfiguration will be proposed for the region it is selecting. This may results in the coordinator having to solve both tasks as a single agent. In other words, we believe that the topology selection task needs to be solve (either implicitly or explicitly) before being able to determine which region to select. \todo[inline]{Not sure if this last sentence is needed, it is implied by the order in which we go about the decision making process}

%we employ a hierarchical decision-making framework in PNC. We compare this with simpler models, including single-agent, rule-based, hybrid, and fully learned multi-agent solutions. Rule-based systems, while providing solid baselines, are limited by their inability to anticipate beyond one timestep. Hybrid models, which include a single trainable agent layer, reduce training noise but may get stuck in local optima. Fully learned agents, though potentially more challenging to train, could avoid this issue. 

% Balancing flexibility and planning is crucial in hierarchical control structures. Flexible architectures are adaptable and computationally efficient but may lack consistency over a longer period of time. Conversely, planning-focused structures provide consistency but struggle with adaptability in case of unexpected observations. 

%Unlike \cite{manczak2023hierarchical, van2023multi}, this work introduces a new architecture that utilizes a RL-based coordinator that decides the next action based on regional action suggestions by the individual agents as well as global state information. 
%This approach allows a stricter task decomposition. In the architectures investigated by \cite{manczak2023hierarchical, van2023multi}, the middle level selects a region without knowing the specific action it will take, while the regional actions are chosen without understanding the intent behind their selection. As a result, both levels are still required to solve the same overall task: the middle level must anticipate the action to be taken by the chosen region, and the regional agents must infer the purpose for which they were selected. In contrast, the proposed architecture ensures that all regions are selected, and thus, each always proposes an action solely based on the current grid state. The central coordinator only needs to determine which of these actions is most appropriate for the current timestep. This stricter task decomposition potentially reduces non-stationarity.

% change binary threshod to gate?
% change the color of gate to orange?

Figure \ref{fig:agent-architecture} provides a more detailed description of our architecture. At the top level, there is always the FC framework's gate.
%This agent decides whether an action needs to be taken given the current observation. This is done by comparing the maximum loading of all lines in the current grid state with a threshold value ($0.95$ in this study). 
%If the maximum loading is lower than $\rho_\theta$ then the \textit{do-nothing agent} in charge of selecting the action to be performed. This is a formality since the do-nothing agent always returns the same action which does not update the current grid topology. 
%\todo[inline]{we can relate this to figure 2}
When switching to the agent mode of operation, the regional agents are activated. Each of them receives the current observation and subsequently proposes a topological reconfiguration for its region\footnote{A regional agents may also still propose a \textit{do-nothing} action for its respective region. }. All of the proposed regional reconfigurations are passed forward to the coordinator. The latter may take into account all of the proposed actions and the current observation. If available, the coordinator also receives the action-value estimates, which represent the network's internal state and quantify the predicted utility or confidence in the effectiveness of each action.

Each regional agent only experiences rewards (and thus transitions) if it is selected by the coordinator. When the gate selects consecutive do-nothing actions, the accumulated reward is assigned to the last regional agent that was selected by the coordinator.


% single action and do nothing action remove boxes
% regional and coordinating both blue
% move the top arrow more below/closer together
% change rho_thetha to rho_tilde
% OBS remove the circle
% proposed actions action values also proposed remove the circle
% make the fonts consistent

Figure \ref{fig:agent-architecture} also shows the different implementations of the regional agents and the coordinator considered in this study. In this paper, each regional agent is either a RL agent trained using the PPO algorithm \cite{schulman2017proximal}, or a greedy rule-based method (see Sec. \ref{sec:greedysingleRL}). % In this paper, the only rule-based regional agent is a greedy agent (see Sec. \ref{sec:greedyAgent}).
%This agent simulates the effect of each action only 1 timestep ahead in the future and selects the action leading to the lowest maximum loading.
%Alternatively, a regional agent can be trained using RL. In this paper, the PPO algorithm \cite{schulman2017proximal} is used for that purpose. 
For the RL agent, the action-value of the proposed action (as evaluated by the critic) may also be passed forward. 

The coordinator also has multiple implementations.
When the coordinator is trained via RL and the action-value of the regional agent is included then the coordinator is referred to as \textit{Action-Value RL} agent. If the action-value is not passed forward, the coordinator is simply referred to as \textit{RL} agent.
We also experiment with two rule-based implementations of the coordinator. The \textit{CAPA heuristics} coordinator uses the \textit{CAPA} algorithm \cite{yoon2020winning}. Algorithm \ref{alg:modified_capa} shows our version of the CAPA coordinator, which looks at the actions proposed by the regional agents only to exclude those who proposed do-nothing actions. Moreover, this agent does have memory for the purpose of exhausting a priority list between regions before generating a new one. % Using a CAPA coordinator is equivalent to the hierarchy shown in figure \ref{fig:difference_hierarchies} left.
%\todo[inline]{talk about how this is some sort of planning?}
%\todo[inline]{mention that this is similar to Fig. \ref{fig:difference_hierarchies} left?}
The other rule-based coordinator selects a regional agent based solely on the action-value of their actions\footnote{Note that these action-values are proposed by independent critics, one for each regional agent.}.
%The \textit{Value argmax} coordinator selects the action with the highest value. This coordinator may result in some regional being selected due to their untrained critics being biased towards returning high-values. For this we also use a 
%\todo[inline]{Jan: i removed the argmax agent mention from here}
More precisely, the \textit{Value Softmax} coordinator samples a regional agent's action with a probability proportional to their action-value in order to balance exploitation of the action-values once the critics are sufficiently trained. This also allows for exploration thus avoiding biasing training data only towards specific regional agents.


%can also in turn be rule-based (e.g. CAPA \todo{ref}) or trained and it can make use of any subset of the information provided to it (e.g. a learned coordinator may not use the action-value information, or CAPA may only look at the current observation). If both the regional agents and coordinating agent are trained this will be done in parallel in our experiments. Finally, the \textit{coordinator} will select one of the proposed actions from the pool given by the \textit{regional agents}.Tab.\ref{tab:input_output_architectures} lists the particular combinations of the aforementioned module that we have experimented with in this paper.

We note that if the \textit{coordinating agent} uses a rule-based method that takes into account only the current observation to select a region (to then simply return whatever action that region's agent has proposed) then we effectively also obtain the architecture shown in Fig. \ref{fig:difference_hierarchies} \textbf{(left)}. This is because, in such a case, it is irrelevant whether or not the regional agents propose the actions before or after the coordinating agent.

%On one end the coordinator can be both rule-based or learned. Similarly each agent proposing the topological reconfiguaration of each region can be learned (also togheter with the coordinator if necessary) or rule-based as well. 
%Our modular architecture allows for various combinations of agents, illustrated in Figure \ref{fig:agent-architecture} and detailed inTab.\ref{tab:input_output_architectures}. \todo[inline]{add letters to figure} The architecture features a rule-based threshold agent to determine intervention needs, a Do-Nothing agent for non-action steps, and a coordinating agent to select and execute actions based on either rule-based heuristics or learned behavior. Regions in this context are defined as a single substations.


\subsection{Overview of power system agents}\label{sec:agents_overview}

%In this paper we consider different multi-agent systems based on the proposed architecture. We compare these multi-agents systems with baseline approaches and benchmarks. 
We summarize all considered approaches using the following naming convention: The full multi-agent system is indicated by [name regional agent]-[name coordinating agent], with the names given as in Table  \ref{tab:input_output_architectures} and Fig. \ref{fig:agent-architecture}.
%Table \ref{tab:input_output_architectures} provides an overview of all agent implementations considered in this study for the proposed architecture.

\subsubsection{Baselines.}
%Do-Nothing Agent, Learned Single-Agent, and Learned-Capa}
To evaluate the proposed approach, results are compared with several baselines that should at least be met in terms of performance:
\begin{itemize}
    \item \textbf{Do-Nothing Agent}: This baseline shows the number of timesteps survived when no action is taken at any timestep.
    \item \textbf{Single RL Agent}: This baseline uses a single PPO agent responsible for configuring the entire network. The agent receives network observations as input and outputs a single action. %A threshold agent is still used. If no congestion is detected then a do-nothing action is performed automatically
    \item \textbf{RL-CAPA}: This architecture combines a rule-based CAPA coordinator with RL-based regional agents. The CAPA policy prioritizes substations at risk of immediate overloading, using the highest relative overload value for selection. This approach ensures that regional agents are trained in scenarios where their actions are likely to have a significant impact. This adapted CAPA policy based on \citet{van2023multi} also considers past actions to avoid experience imbalance and repeated selection of the same substation. If a proposed action does not alter the grid topology, the next highest-priority substation proposes an action, as detailed in Alg. \ref{alg:modified_capa}.
\end{itemize}


\subsubsection{Benchmarks.}
%: Greedy Single-Agent and Greedy-CAPA Multi-Agent}

Two benchmark agents are designed using action simulation and greedy strategies. Both architectures are computationally intensive, as they simulate all possible actions and select the optimal one in a greedy manner. However, they serve to illustrate the complexity within networks and scenarios, demonstrating the potential scope of learning.

\begin{itemize}
    \item \textbf{Greedy Agent}: This baseline involves a greedy agent that makes decisions based on minimizing the maximum relative line loading of the network. It operates without hierarchical coordination and provides a benchmark of what a simple architecture with enough computational power can achieve.
    \item \textbf{Greedy-CAPA}: To test the rule-based potential of a hierarchical design, greedy regional agents are paired with the CAPA coordinator.
\end{itemize}


\subsubsection{Multi-agent RL architectures.}
%: Greedy-Action RL, Action-Value RL-Value Softmax/Argmax, Action RL-Action RL, and Action-Value RL-Action Value RL}
For the proposed CCMA architecture several implementations are explored:

\begin{itemize}
    \item \textbf{Greedy-RL}: This setup features a hierarchical structure with an RL-based coordinator that receives proposals from regional agents to address congestion. Unlike a greedy agent, the RL coordinator benefits from implicit planning, similar to the \textit{PPO Substation} implementation by \citet{manczak2023hierarchical}.
    %, which outperforms both single-agent and fully learned hierarchical approaches.
    The key distinction is that \citet{manczak2023hierarchical}'s implementation does not involve multiple regional agents or consider the greedy agents planned actions.
    \item \textbf{RL-RL}: This architecture features both a learned coordinator and learned regional agents. The coordinator utilizes both global state observations and regional agent information, inspired by the approach suggested by \citet{yu2022surprising}. This setup allows the system to learn multi-step actions but may introduce instability due to the interactions among multiple learned levels and agents.
    \item \textbf{RL-Action Value RL}: This approach extends the previous setup by incorporating the action-value function outputs from regional agents as additional input for the learned coordinator. This allows for a more refined decision-making process by integrating the value estimates of different actions, potentially enhancing the coordination and overall performance of the system.
    \item \textbf{RL-Value Softmax}: This architecture uses the action-value function outputs from regional agents. A coordinator employing %\textit{argmax} (see Equation \ref{eq:argmax}) selects the agent with the highest estimated reward, while 
    \textit{softmax} (see Equation \ref{eq:softmax} below) assigns probabilities to actions during training, promoting exploration and fair experience distribution. During evaluation, \textit{argmax} is used to choose the optimal action based on the trained agents.
    %\item \textbf{RL-Value Argmax}: This architecture uses the value function outputs from regional agents. A coordinator employing \textit{argmax} (see Equation \ref{eq:argmax}) selects the agent with the highest estimated reward, while \textit{softmax} (see Equation \ref{eq:softmax}) assigns probabilities to actions during training, promoting exploration and fair experience distribution. During evaluation, \textit{argmax} is used to choose the optimal action based on the trained agents.
    %\todo[inline]{check that argmax is not mentioned anywhere else}
    %\todo[inline]{Jan: argmax is used during evaluation, but i think it's still ok to call it a softmax agent}
\end{itemize}

\begin{algorithm}[t]
\caption{Ordered CAPA Algorithm}
\label{alg:modified_capa}
\begin{algorithmic}[1]
\STATE Given observation $obs$, activation threshold $max\_rho$ and an initial priority list of actions $P$ that acts as the state of the algorithm
\STATE $current\_rho \gets$ $max(rho)$ in $obs$
\IF{$P = None$ or $P$ is empty}
    \STATE Generate priority list $P$ using CAPA policy
\ENDIF
\FOR{each action $A$}
    \IF{$A \neq do\_nothing$}
        \STATE Remove $A$ from $P$
        \STATE \textbf{return} $A$ and $P$
    \ENDIF
\ENDFOR
\STATE $P \gets None$
\STATE \textbf{return} $do\_nothing$ and $P$
\end{algorithmic}
\end{algorithm}


\begin{table}[t]
\centering
\caption{The input for each module in the agent architecture.}
\begin{tabular}{ll}
\toprule
Agent Name & Input \\ 
%\midrule
%\multicolumn{2}{c}{Base Agents} \\ 
%\midrule
%Threshold & Observation \\
%Do-Nothing & - \\
\midrule
\multicolumn{2}{c}{Regional Agents} \\ 
\midrule
Greedy & Observation \\
RL & Observation \\
\midrule
\multicolumn{2}{c}{Coordinating Agents} \\ 
\midrule
CAPA & Regional Actions \& Observation \\
RL & Regional Actions \& Observation \\
Action-Value RL & Regional Action-Value Pairs \\
 & \& Observation \\
Value Softmax & Regional Action-Value Pairs \\ 
%Value Argmax & Proposed Action-Value Pairs \\
\bottomrule
\end{tabular}
\label{tab:input_output_architectures}
\end{table}


%\begin{equation}\label{eq:argmax}
%\operatorname*{arg\,max}_i x_i,
%\end{equation}
%where $argmax$ denotes the function that returns the index of the maximum value, $x$ is a vector containing the outputs from the value functions, and $i$ is the index for which $x_i$ is the maximum value in the vector $x$. 

\subsection{Implementation details}
To improve learning, avoid overfitting, and ensure a fair comparison between architectures, several measures are implemented for each experiment, including train-validation-test splitting, scenario splitting, and identical hyperparameter tuning.

\subsubsection{Trainer}
%\subsection{Reinforcement Learning Algorithm}\label{sec:PPO}
For all experiments, PPO \cite{schulman2017proximal} is used for learning a policy. The policy, denoted by $\pi$, is parameterized using a neural network with learnable parameters $\theta$. The action-value function, represented as \(Q(s_t, a_t)\), and the state-value function \(V(s_t)\), are represented by a neural network with parameters $\phi$.

% is an on-policy technique known for delivering stable policy updates and minimizing variance, which are potentially beneficial in a stochastic multi agent environment. The policy loss is computed with  Equation \ref{eq:PPO_loss}. It consists of the clipped surrogate objective ($\mathcal{L}_t^{CLIP}(\theta)$), the value function loss ($\mathcal{L}_t^{VF}(\theta)$) scaled by coefficient $c_1$, and the entropy bonus ($\mathcal{L}_t^{ENTROPY}(\theta))$) scaled by coefficient $c_2$.

%\begin{equation}
    %\label{eq:PPO_loss}
    %\mathcal{L}_t(\theta) = \mathcal{L}_t^{CLIP}(\theta) - c_1 \mathcal{L}_t^{VF}(\theta) + c_2 \mathcal{L}_t^{ENTROPY}(\theta),
%\end{equation}
%where $\theta$ describes the parameters of the policy and $t$ is the current timestep. The clipped surrogate objective, outlined in Equation \ref{eq:PPO_clip}, represents the clipped version of the probability ratio between the current policy and the policy from the previous time step. This clipping mechanism ensures stability in updates by penalizing substantial deviations between the current policy and its predecessor, acting as an implicit constraint on the Kullback-Leibler (KL) divergence \cite{KL}. 

%\begin{equation}
    %\label{eq:PPO_clip}
    %\mathcal{L}_t^{CLIP}(\theta) = \hat{\mathbb{E}}_t [min(r_t(\theta)\hat{A}_t , clip(r_T, (\theta) , 1-\epsilon, 1+\epsilon) \hat{A}_t],
%\end{equation}

%\begin{equation}
    %\label{eq:ratio}
%r_t(\theta) = \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta old}(a_t|s_t)}
%\end{equation}
%where $\hat{\mathbb{E}}_t$ is the expected cumulative reward over the next timesteps and $r_t(\theta)$ is the probability ratio between the new and old policies (Equation \ref{eq:ratio}). $\hat{A}_t$ is the estimated advantage and represents how much better or worse an action is compared to the average action taken from a given state at timestep $t$. Lastly, $clip(r_T, (\theta) , 1-\epsilon, 1+\epsilon)$ is the clipping function that prevents $r_t(\theta)$ from large policy updates in which $\epsilon$ is a hyperparameter that controls the clip range.

%The value function loss, detailed in Equation \ref{eq:PPO_vf}, promotes proximity between the value function estimates and their target values. It measures the squared difference between the estimated value function and the target value.

%\begin{equation}
    %\label{eq:PPO_vf}
    %\mathcal{L}_t^{VF}(\theta) = (V_\theta(s_t) - V_t^{target})^2,
%\end{equation}
%where $V_\theta(s_t)$ is the value function estimate given state $s_t$ and parameters $\theta$ and $V_t^{target}$ is target value, which is typically calculated using the discounted sum of future rewards. Lastly, the entropy regularization term in the equation strikes a balance between exploration and exploitation. 

%\begin{equation}
%\label{eq:entropy}
%\mathcal{L}_t^{ENTROPY}(\theta) = S[\pi_\theta](s_t),
%\end{equation}
%where $S[\pi_\theta](s_t)$ is the entropy of policy $\pi_\theta$ at state $s_t$, which encourages exploration by penalizing certainty. By combining these three terms in the objective function, both the policy loss as well as the value function are learned whilst maintaining a good balance between exploration and exploitation.

For both training and evaluation, PPO is employed as a stochastic policy, meaning actions are sampled from a probability distribution rather than being chosen deterministically. Common practices such as including Generalized Advantage Estimation (GAE) are adopted to reduce variance and improve training stability \cite{yu2022surprising}. The PPO implementation by \citet{liang2017ray} is utilized in this research.

For the multi-agent implementation, it is common that one agent acquires significantly more experience than other agents. To counteract this, we adapted PPO to postpone training until all agents have reached the minimum batch size or until any agent has reached double the minimum batch size.

\subsubsection{Machine Learning Practices}
Hyperparameter tuning is critical in RL research and applications because hyperparameters greatly affect the performance, stability, and efficiency of RL algorithms. For each architecture and environment, two grid searches are conducted to tune the parameters using Ray Tune \cite{liaw2018tune}. The first is a coarse grid search to identify suitable learning rates, batch sizes, minibatch sizes, and the number of iterations. The second grid search is narrower, focusing on smaller variations in these parameters, as well as additional parameters such as the clipping range, value function coefficient range, and GAE lambda range. Appendix \ref{ap:grid_search} details the full grid search settings for all experiments.

Additionally, a priority function is integrated into the selection of the next training scenario to accelerate learning \cite{yoon2020winning, van2023multi}. This function assigns a logit to each scenario based on its characteristics, as defined by Equation \ref{eq:prio}:

\begin{equation}
\label{eq:prio}
w_{prio} = 1 - \Big(2 \cdot \sqrt{\frac{t_{survived}}{t_{max}}}\Big) \,
\end{equation}

\noindent
where \(w_{prio}\) represents the logits, \(t_{survived}\) is the number of timesteps the agent survived in the last iteration, and \(t_{max}\) is the maximum number of timesteps for that scenario. The logits are scaled between -1 and 1: fully completed scenarios are assigned a value of -1, scenarios that fail at the start are assigned a value of 1, and unseen scenarios are initialized with a logit of 2 to encourage exploring new scenarios first.
These logits are then used in a softmax distribution (Eq. \ref{eq:softmax}) to sample the next scenario:

\begin{equation}\label{eq:softmax}
\sigma(x_{i}) = \frac{e^{x_i}}{\sum_j e^{x_j}}\ ,
\end{equation}

\noindent
where \(\sigma(x_{i})\) represents the probability of selecting scenario $i$, and the denominator ensures the probabilities are normalized by summing the exponential values of all scenario logits.

To avoid overfitting and ensure a robust model, all scenarios are divided into a 70\%/10\%/20\% train/validation/test split.


\section{Results}\label{sec:results}
Figure \ref{fig:training_curves} shows the learning curves of the different agent architectures for the four different experimental setups, namely, the agent performances during training on all validation scenarios, averaged over all ten model seeds\footnote{It should be noted that the training curves for the 14-bus network without an opponent have not yet fully converged. Due to the computational complexity involved in grid searching and generating all models across multiple seeds, training was halted once the curves had leveled off at 100,000 timesteps.}.

First, we note that all agent architectures considered in this study, except the do-nothing agent, achieve optimal performance in the 5-bus environment without opponent (upper-left panel in Fig. \ref{fig:training_curves}, first column in Table \ref{tab:combined_performance}). Consequently, the 5-bus environment without opponent can be considered as a baseline (or code testing) environment which is used to demonstrate that 
%agent implementations are consistent, that is, 
the minimally expected learning behavior is exhibited. Hence, in the subsequent sections the results related to the 5-bus environment without opponent are not discussed in detail. 

\begin{figure*}[hbt]
     \centering
     \includegraphics[width=0.84\linewidth]{Figures/TrainingCurves.jpg}
     \caption{The mean number of timesteps survived on all validation scenarios throughout training for the 5-bus network without opponent (top left), the 5-bus network with opponent (top right), the 14-bus network without opponent (bottom left) and the 14-bus network with opponent (bottom right). For the mean and standard deviation, seeds are interpolated between 0 and \texttt{max\_timesteps} to align them on a common time axis.}
     \label{fig:training_curves}
\end{figure*}

\subsubsection*{Rule-based agents}%\label{sec:performance_rule_based}
In this study, three entirely rule-based agents are considered (see Sec. \ref{sec:agents_overview}).
The do-nothing agent (dashed-grey in Fig. \ref{fig:training_curves}) represents a baseline agent which is completely inflexible.
The greedy agent (dashed-purple in Fig. \ref{fig:training_curves}) and the Greedy-CAPA agent (dashed-brown in Fig. \ref{fig:training_curves}) represent benchmark approaches which include a high amount of simulation.
Correspondingly, the do-nothing agent shows the worst performance of all considered approaches for all environments. For the 5-bus environment without opponent (with opponent) it survives about $10\%$ ($8.5\%$ on average) of the possible timesteps, whereas for the 14-bus network it survives $18\%$ ($4.3\%$ on average).
%, see Tab. \ref{tab:combined_performance}. 
In contrast, the two rule-based benchmark approaches show high performances for both the 5-bus environment with opponent (upper-right panel in Fig. \ref{fig:training_curves}) and the 14-bus environment without opponent (lower-left panel in Fig. \ref{fig:training_curves}). More precisely, in the 5-bus environment with opponent the Greedy-CAPA agent achieves $100\%$ of the possible timesteps, whereas the greedy agent achieves $83\%$ on average.
%(see Tab. \ref{tab:combined_performance}).
In the 14-bus environment without opponent the order is reversed with the greedy agent achieving $96\%$ of the possible timesteps, whereas the Greedy-CAPA agent achieves $85\%$. The 14-bus environment with opponent (lower-right panel in Fig. \ref{fig:training_curves}) is significantly more challenging than the previous environments. Consequently, the achieved timesteps of the greedy agent and the Greedy-CAPA agent are significantly smaller with only $7.7\%$ and $7.4\%$ on average, respectively.

\subsubsection*{Single RL baseline agent}\label{sec:performance_PPOsingle}
The single RL agent (blue in Fig. \ref{fig:training_curves}) represents the most common RL-based baseline approach for L2RPN \cite{marot2021learning}. For the 5-bus environment with opponent (upper-right panel in Fig. \ref{fig:training_curves}) and the 14-bus environment without opponent (lower-left panel in Fig. \ref{fig:training_curves}), it exhibits performances similar to the rule-based benchmark approaches (i.e. the greedy and Greedy-CAPA agents).
%, see Section \ref{sec:performance_rule_based}). %As shown in Tab. \ref{tab:combined_performance}, it is able to achieve $100\%$ and $84\%$ of the possible timesteps in these environments, respectively.
However, in the most challenging environment (14-bus with opponent, lower-right panel in Fig. \ref{fig:training_curves}) its performance drastically decreases %to $6.4\%$ which is
and remains clearly below the already low performances of the two rule-based benchmark approaches.

\subsubsection*{Low-performing trained multi-agent systems}\label{sec:performance_low}
RL-CAPA (green in Fig. \ref{fig:training_curves}) and RL-Value Softmax (light-orange in Fig. \ref{fig:training_curves}) represent the two multi-agent architectures in which the regional agents are trained via RL, whereas the coordinating agent is rule-based (see Sec. \ref{sec:agents_overview}).
%\ref{sec:architecture}).
Both approaches show significantly worse performances than all other investigated agent architectures (except the do-nothing agent) for all environments (except the baseline 5-bus environment without opponent). In particular, the single RL baseline agent is outperforming RL-CAPA and RL-Value Softmax in both the 5-bus environment with opponent (upper-right panel in Fig. \ref{fig:training_curves}) 
and the 14-bus environment without opponent (lower-left panel in Fig. \ref{fig:training_curves}). 
%More precisely, in the 5-bus environment with opponent RL-CAPA and RL-Value Softmax achieve $75\%$ and $62\%$ of the possible timesteps, respectively (see Tab. \ref{tab:combined_performance}).
%In the 14-bus environment without opponent the performances are even worse (being close to the do-nothing agent) with RL-CAPA and RL-Value Softmax achieving only $19\%$ and $29\%$ of the possible timesteps, respectively.
Due to these very low performances we decided to not train these approaches in the significantly more challenging and computationally expensive 14-bus environment with opponent.

\subsubsection*{High-performing trained multi-agent systems}\label{sec:performance_high}
Greedy-RL (red in Fig. \ref{fig:training_curves}),
RL-RL (orange in Fig. \ref{fig:training_curves}), and RL-Action Value RL (light-blue in Fig. \ref{fig:training_curves}) represent the multi-agent architectures in which the coordinating agent is trained via RL (see Sec. \ref{sec:architecture}). The RL-RL multi-agent system and the RL-Action Value RL multi-agent system show very similar results such that we focus on RL-RL in the remainder of this section. All three approaches outperform the single RL baseline in the challenging 14-bus environments (lower panels in Fig. \ref{fig:training_curves}). 
The learning curves of RL-RL are initially below the learning curves of the single RL baseline but start to excel after a moderate amount of training. 
%As shown in Tab. \ref{tab:combined_performance}, in terms of possible timesteps RL-RL is able to achieve $98\%$ in the 5-bus environment with opponent, $93\%$ in the 14-bus environment without opponent, and $14\%$ in the 14-bus environment with opponent.
Importantly, RL-RL eventually also outperforms the rule-based benchmark approaches (i.e., the greedy agent and Greedy-CAPA)
%, see Sec. \ref{sec:performance_rule_based})
in the most challenging 14-bus environment with opponent (lower-right panel in Fig. \ref{fig:training_curves}).

The most impressive sample efficiency and performance is exhibited by Greedy-RL. In each of the four panels in Fig. \ref{fig:training_curves}, the training curve of Greedy-RL starts at a significantly higher level than those of the other trained agents. This is related to the employment of greedy agents as the regional agents such that the coordinating agent can only choose from valuable actions. Subsequently, Greedy-RL continues to exhibit the highest performance level during the entire training period in each environment.
For the 5-bus environment with opponent (upper-right panel in Fig. \ref{fig:training_curves}) and the 14-bus environment without opponent (lower-left panel in Fig. \ref{fig:training_curves}), Greedy-RL converges quickly, effectively solving the environment while the training curves of the other trained agents only start to exhibit learning progress.

Also in the most challenging 14-bus environment with opponent (lower-right panel in Fig. \ref{fig:training_curves}), Greedy-RL exhibits extraordinary sample efficiency and performance. The learning curves of Greedy-RL start at the performance level of the converged single RL baseline models. We note that for Greedy-RL the learning curves related to different seeds, shown in Figure \ref{fig:compare_single} in Appendix \ref{individual_seeds}, exhibit bimodal behavior (in contrast to all other architectures).
That is, one group of seeds remains at a lower performance level (but still higher than the level of the converged single RL baseline) whereas a second group of seeds exhibits steep learning curves outperforming both the single RL baseline  and the rule-based benchmark approaches (i.e., the greedy agent and Greedy-CAPA) by a large margin. Hence, a mean curve (averaged across all seeds) is not a good representation of the model behavior such that in the lower-right panel of Fig. \ref{fig:training_curves} we only show an average across high-performing seeds.
%Note: it is bimodal, the mean does not exist as individual seed!
More details on the training curves related to individual seeds can be found in App. \ref{individual_seeds}.

For completeness, Table \ref{tab:combined_performance} in Appendix \ref{ap:test_results} shows model evaluation on the test scenarios, which exhibits results consistent with the evaluation on the validation scenarios during training. However, the focus of this study is on the \emph{sample efficiency} of agent architectures, and, hence, in this study the model evaluation during training (Fig. \ref{fig:training_curves}) is more relevant than final test results (Tab. \ref{tab:combined_performance} in App. \ref{ap:test_results}).


\section{Discussion and Future Work}\label{sec:summary}

This study introduces a new approach that combines HRL with MARL to tackle the growing complexity of PNC in increasingly unpredictable environments. Our three-level architecture effectively breaks down the decision-making process, making control strategies more modular. One notable aspect is the introduction of a coordinating agent that considers both global state information and action proposals from regional agents.


\subsection{Discussion}
From the experiments it is evident that there can be benefits to factoring the action space. The Greedy-RL, RL-RL, and RL-Action Value RL agents consistently outperform the single RL agent, both asymptotically and in sample efficiency. With the larger grid, and especially with contingencies, this pattern becomes more evident.

There seems to be no significant difference in performance between the RL-RL and RL-Action Value RL agents. This suggests that the value of the regional agents' critics does not hold useful information for solving the coordination task.

On the other end, the Greedy-RL agent significantly outperforms RL-RL agent, both asymptotically and in sample efficiency. The improved sample efficiency could be due to the warm-start that the Greedy-RL agent can benefit from, as regional agents are already fixed. In contrast, agents like RL-RL may suffer from non-stationarity since both levels need to perform reasonably well before they can learn appropriately. Concretely, it can lead to poor performance when a regional agent selects a good action but a coordinator does not select the appropriate agent, or vice versa.

On the other hand, the Greedy-RL agent exhibits bi-modal behavior in the stochastic environment (Fig. \ref{fig:compare_single} in App. \ref{individual_seeds}). One possible explanation is that not all contingency cases have equal value. It could be that only a smaller subset of the contingency cases pushes the agent to pick safer actions. This may have happened only on certain seeds. One possible explanation as to why we observe this only with Greedy-RL is its higher sample-efficiency. This would make the agent faster at picking up such patterns when (and if) they appear.
%However, it should be noted that the Greedy-RL agent appears to be less flexible. This is supported by the higher variance in sample efficiency between its seed when contingencies are used. We believe this to be due to the environment being stochastic. The greedy regional agents may lack the adaptability required to deal with certain contingency cases. Another possible cause of the increased variance could be the short term horizon planning of the greedy agents.  

These results suggest that it is crucial to have long term reasoning in the coordination task. With an RL coordinator the Greedy-RL agent manages to outperform the Greedy baseline, especially in the challenging stochastic setting, despite the regional Greedy agents optimizing topological configurations for a single timestamp. The importance of long term reasoning in coordination is further supported by the low asymptotic performance of RL-CAPA and RL-Value Softmax. For the simplest setting, a 5-bus network without opponent, the coordinators such as CAPA-heuristic, random substation selection, Value Softmax still perform reasonably well. However, when used with added complexity due to an adversary or a larger network, these architectures no longer match the performance of a learned coordinator.

%Over all networks and settings, a RL-based coordinator paired with greedy regional agents matches or outperforms all greedy and RL single-agent baselines, as well as other explored multi-agent hierarchical architectures in terms of number of timesteps survived on the test scenarios. Additionally, this architecture converges fastest and to the most number of timesteps survived during training. However, the greedy regional agents are computationally expensive as they simulate all actions before greedily choosing the best action.

%To reduce computational load during training and execution, a RL-based coordinator can be paired with RL-based regional agents. This architecture factors the MDP and still matches or outperforms performance of RL-based single agent baselines in terms of number of timesteps survived during testing. During training, these architectures with two learned levels generally need more timesteps to start converging, but reach a higher plateau in terms of number of timesteps survived. This is likely the effect of the non-stationarity in the architecture, where both levels need to perform reasonably well before they can learn appropriately. Concretely, it can lead to poor performance when a regional agent selects a good action but a coordinator does not select the appropriate agent, or vice versa.

%\subsection{Comparing Different Coordinators}\label{sec:coordinators}
%The rule-based coordinators used in this research do not perform up to par with a RL-based coordinator. For the most simple setting, a 5-bus network without opponent, the coordinators such as action selection based on the CAPA-heuristic, random substation selection or choosing the agent with the maximum value function output still perform reasonably well. However, when used with added complexity due to an adversary or a larger network, these architectures no longer match the performance of a learned coordinator.

Moreover, when the network grows in size, the number of possible substations to select also grows. This makes the coordinator's task increasingly complex, calling the need for a more sophisticated coordinator.

Finally, because the Greedy-RL is among the best performing agents, it suggests that the factored action spaces can be tackled independently.
%Finally, the results for the Greedy-RL agent also seem to suggest that the factored action spaces can be tackled independently.
In practice, the Greedy-RL agent is trained as a single RL agent within its coordination task. 
%The Greedy regional agents can be seen as part of the environment.
Thus, the regional agents can be trained independently of the coordinator. The coordinator can also be trained individually, but only after the regional agents.
Most likely, this results in the significantly higher sample-efficiency of the Greedy-RL agent.

%\section{Conclusions}\label{sec:conclusions}

%The results show that a RL-based coordinator outperforms rule-based coordinators when task complexity increases. Paired with greedy regional agents, the hierarchical multi-agent setup outperform all other approaches in this research. However, due to its computational complexity, this architecture is not scalable for real-world networks.
%When paired with RL-based regional agents, computational complexity is significantly reduced and performance is competitive compared to tried benchmarks. 

% discussion bridge greedy and learned
%To combine the benefits of both hierarchical architectures, a balance between computational complexity and the information gained from simulating possible actions could be further explored. For instance, learned regional agents could output five potential actions, each of which is simulated and then greedily selected.
\subsection{Future Work}

Future research could build on this study by exploring several avenues for extending and refining the proposed methods. Testing the scalability and robustness of the approach in larger networks, such as a 118-bus network, could provide insights into its effectiveness in more complex environments. Moreover, future studies could test the impact of different strategies for decomposing the actions space \cite{polimifactorization}.
%Analyzing the impact of interchanging regional agents between multiple seeds and investigating how changes in individual regions affect overall performance would enhance our understanding of the system's adaptability. 

Another promising direction is to investigate cooperative mechanisms among agents. While this study used a fully independent setup with PPO, sharing parameters or using a shared critic could potentially stabilize learning and enhance efficiency. Testing the impact of partial observability on regional agents could also provide more realistic insights into local versus global information. 

%\todo[inline]{Rephrase the concept below}

%Another advantage of the CCMA architecture is that an entire topology for the entire grid is proposed in a single timestep and the coordinator is essentially only deciding the order in which said topology needs to be implemented. \todo[inline]{"implemented" not the best term} Therefore, we can seamlessly switch between a completely reactive architecture and a more long term planning one. The former is achieved by having the agents re-propose a topological configuration each timestep. The latter can be achieved by having the agents propose a topological reconfiguration for the entire grid once and then use the coordinator to sample without replacement each region, thus planning in which order the regions need to be reconfigured.

%greedyImitation-RL finally offers a perspective for scaling to larger networks ...

The results also suggest more concrete next steps for making the CCMA more scalable. The Greedy-RL architecture shows the most promise in sample efficiency. However, it is also the most computationally expensive agent. To mediate this, one could use imitation learning to train regional agents. These agents would try to imitate the greedy agent directly just much faster.

%In addition, the policies' uncertainties could be leveraged to give some more flexibility to the regional agents. This could be done to attenuate the high variance observed in the stochastic environment.

Furthermore, the regional agents can be trained independently of the coordinator using RL, similar to a single-agent RL setting where the action space is confined to a single regional agent. This approach allows regional agents to incorporate some level of longer-term planning, albeit with a shorter time horizon compared to the coordinator that could be limited to a few timesteps. The aforementioned imitation learning can be used as warm start in this setting.




%However, it should be noted that the regional agents are not independent of each other. This is mainly due to the fact that they share the same global observation. Most of the factorization happens between the coordination and regional reconfiguration tasks. 

However, it should be noted that our approach does not factorize the observation space. With the current observation space, when training the regional agents separately, the data should include a lot of varied configurations of the observed section of the grid. This applies also when using only imitation learning. Each regional agent observes the entire grid, and thus all other regions. Therefore the amount of data needed to train an individual agents scales exponentially with the number of regions. 

One could try to give partial observations to the regional agents. Each agent only taking as observation a neighborhood of its region (included). Thus, the data to train a regional agents should only include combinations within its neighborhood. This effectively creates a cap on the amount of data needed to train an agent, with respect to the number of regions. In such a setting, the amount of data scales with respect to the size of the neighborhood and not the total number of regions. Expanding the grid outside of a regional agent's neighborhood will not affect the amount of data needed to train it. The connections going out of the neighboring regions can still be kept and simulated. The data can include various level of loading on these connections. Effectively the part outside of the neighborhood can be seen as grid injections. It should be noted that these observation spaces would not be completely independent as there would be overlap between them.


\section*{Acknowledgments}
% This was was supported in part by......
% This work was conducted during an internship at TenneT TSO B.V. I would like to thank all members of the GridOptions and Advanced Analytics teams for their support and collaboration throughout this research. Additionally, I am grateful to ir. Erica van der Sar, Dr. Alessandro Zocca, and Prof. Dr. Sandjai Bhulai from the Vrije Universiteit Amsterdam for their valuable insights during our bi-weekly research discussions. I especially thank Dr. Alessandro Zocca for his thoughtful review comments, which contributed to refining this work.

We want to thank Erica van der Sar and Alessandro Zocca from the Vrije Universiteit Amsterdam for their valuable insights while conducting the research as well as feedback in refining this work.

AI4REALNET has received funding from European Unions Horizon Europe Research and Innovation programme under the Grant Agreement No 101119527. Views and opinions expressed are however those of the authors only and do not necessarily reflect those of the European Union. Neither the European Union nor the granting authority can be held responsible for them.

Davide Grossi acknowledges support by the \href{https://hybrid-intelligence-centre.nl}{Hybrid Intelligence Center}, a 10-year program funded by the Dutch Ministry of Education, Culture and Science through the Netherlands Organisation for Scientific Research (NWO)


%%
%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
% %Bibliography
\bibliographystyle{unsrtnat}
\bibliography{biblio}  



%%
%% If your work has an appendix, this is the place to put it.
\appendix
\onecolumn

\section{Hyperparameters}\label{ap:grid_search}
A grid search was performed for all architectures in all different set-ups, of which the explored values can be found here. Unless otherwise mentioned, the default parameters of Ray's PPO were used \cite{liang2017ray}.

\subsection{5-bus network without opponent}
Grid search was performed over the following parameters:
\begin{itemize}
    \item Learning rate (lr): 0.01, 0.001, 0.0001, 0.00001
    \item SGD minibatch size (mbs): 32, 64
    \item Train batch size (bs): 64, 128
\end{itemize}

Depending on the result of the grid search, the learning rate parameter was further explored to include half and five times its best setting. For example, if 0.001 was the best learning rate, the next grid search also explored 0.005 and 0.0005. Additionally, the number of SGD iterations (it) was fixed at 15. Table \ref{tab:param5reg} illustrates all chosen parameters.

% \begin{table}[hbtp]
%     \centering
%     \caption{Chosen parameters for all learning architectures on the 5-bus network without opponent.}
%     \begin{tabular}{rllll}
%         \toprule
%         Agent & lr & mbs & bs & it\\
%         \midrule
%         Single RL & 0.01 & 32 & 64 & 15 \\
%         RL-RL & 0.0005 & 64 & 128 & 15 \\
%         RL-Action Value RL & 0.0005 & 64 & 128 & 15 \\
%         \midrule
%         RL-Action Softmax & 0.0005 & 64 & 128 &  15 \\
%         RL-CAPA & 0.005 & 32 & 64 & 15 \\
%         RL-Random & 0.01 & 64 & 64 & 15 \\
%         Greedy-RL & 0.0005 & 32 & 128 & 15 \\
%         \bottomrule
%     \end{tabular}
%     \label{tab:param5reg}
% \end{table}


\subsection{5-bus network with opponent}
Grid search was performed over the following parameters:
\begin{itemize}
    \item Learning rate (lr): 0.001, 0.0001, 0.00001, 0.000001
    \item SGD minibatch size (mbs): 32, 64, 128, 256
    \item Train batch size (bs): 128, 256, 512, 1024
    \item Number of SGD iterations (it): 5, 10
\end{itemize}
Depending on the result of the grid search, the learning rate parameter was further explored to include nearby values. For example, if 0.001 was the best learning rate, the next grid search also explored 0.0005, 0.0007, 0.003, 0.005. Table \ref{tab:param5opp} illustrates all chosen parameters.

% \begin{table}[hbtp]
%     \centering
%     \caption{Chosen parameters for all learning architectures on the 5-bus network with opponent.}
%     \begin{tabular}{rllll}
%         \toprule
%         Agent & lr & mbs & bs & it \\
%         \midrule
%         Single RL & 0.0007 & 128 & 512 & 10 \\
%         RL-RL & 0.0007 & 64 & 128 & 5 \\
%         RL-Action Value RL & 0.0003 & 128 & 256 & 5 \\
%         \midrule
%         RL-Value Softmax & 0.0005 & 64 & 128 & 10 \\
%         RL-CAPA & 0.0005 & 128 & 256 & 5 \\
%         RL-Random & 0.0001 & 64 & 128 & 5 \\
%         Greedy-RL & 0.00005 & 64 & 256 & 10 \\
%         \bottomrule
%     \end{tabular}
%     \label{tab:param5opp}
% \end{table}

\begin{table*}[b]
    \centering
    \begin{minipage}{0.48\textwidth}
        \centering
        \caption{Chosen parameters for all learning architectures on the 5-bus network without opponent.}
        \begin{tabular}{rllll}
            \toprule
            Agent & lr & mbs & bs & it\\
            \midrule
            Single RL & 0.01 & 32 & 64 & 15 \\
            RL-RL & 0.0005 & 64 & 128 & 15 \\
            RL-Action Value RL & 0.0005 & 64 & 128 & 15 \\
            \midrule
            RL-Action Softmax & 0.0005 & 64 & 128 &  15 \\
            RL-CAPA & 0.005 & 32 & 64 & 15 \\
            RL-Random & 0.01 & 64 & 64 & 15 \\
            Greedy-RL & 0.0005 & 32 & 128 & 15 \\
            \bottomrule
        \end{tabular}
        \label{tab:param5reg}
    \end{minipage}%
    \hfill
    \begin{minipage}{0.48\textwidth}
        \centering
        \caption{Chosen parameters for all learning architectures on the 5-bus network with opponent.}
        \begin{tabular}{rllll}
            \toprule
            Agent & lr & mbs & bs & it \\
            \midrule
            Single RL & 0.0007 & 128 & 512 & 10 \\
            RL-RL & 0.0007 & 64 & 128 & 5 \\
            RL-Action Value RL & 0.0003 & 128 & 256 & 5 \\
            \midrule
            RL-Value Softmax & 0.0005 & 64 & 128 & 10 \\
            RL-CAPA & 0.0005 & 128 & 256 & 5 \\
            RL-Random & 0.0001 & 64 & 128 & 5 \\
            Greedy-RL & 0.00005 & 64 & 256 & 10 \\
            \bottomrule
        \end{tabular}
        \label{tab:param5opp}
    \end{minipage}
\end{table*}

\subsection{14-bus network without opponent}
Grid search for the 14-bus networks was performed in two batches. The first batch was a coarse grid search over the following settings:
\begin{itemize}
    \item Learning rate (lr): 0.0001, 0.00001, 0.000001
    \item SGD minibatch size (mbs): 64, 256, 1024
    \item Train batch size (bs): 128, 512, 2048
    \item Number of SGD iterations (it): 5, 10, 15
\end{itemize}

The second batch further investigated the learning rate, SGD minibatch size and train batch size parameters. For example, if 0.001 was the best learning rate, the next grid search also explored 0.0005, 0.0007, 0.003, 0.005. For the batch sizes, the surrounding integers $x$ in $2^{x}$ were explored. Additionally, new parameters were explored:
\begin{itemize}
    \item Value function clip parameter (vfc): 10, 20, 50
    \item Clip parameter (cp): 0.15, 0.2, 0.3
    \item Value function loss coefficient (vfl): 0.9, 0.95, 1
    \item Lambda ($\lambda$): 0.9, 0.92, 0.95, 0.98, 1
\end{itemize}

Table \ref{tab:param14reg} illustrates all chosen parameters.

\begin{table*}[h]
    \centering
    \caption{Chosen parameters for all learning architectures on the 14-bus network without opponent.}
    \begin{tabular}{rllllllll}
        \toprule
        Agent & lr & mbs & bs & it & cp & vfc & vfl & $\lambda$ \\
        \midrule
        Single RL & 0.00005 & 256 & 1024 & 15 & 0.3 & 10 & 1 & 0.95 \\
        RL-RL & 0.00007 & 512 & 256 & 10 & 0.15 & 10 & 1 & 0.95 \\
        RL-Action Value RL & 0.0001 & 256 & 512 & 10 & 0.15 & 20 & 0.95 & 0.95 \\
        \midrule
        RL-Value Softmax & 0.00005 & 64 & 256 & 10 & 0.15 & 10 & 1 & 0.92 \\
        RL-Capa & 0.00005 & 128 & 256 & 10 & 0.3 & 10 & 1 & 1 \\
        RL-Random & 0.00001 & 256 & 256 & 10 & 0.3 & 10 & 1 & 0.92 \\
        Greedy-RL & 0.0005 & 128 & 512 & 15 & 0.3 & 10 & 0.95 & 0.95 \\
        \bottomrule
    \end{tabular}
    \label{tab:param14reg}
\end{table*}

\subsection{14-bus network with opponent}
Grid search for the 14-bus networks was performed in two batches. The first batch was a coarse grid search over the following settings:
\begin{itemize}
    \item Learning rate (lr): 0.0001, 0.00001, 0.000001
    \item SGD minibatch size (mbs): 64, 256, 1024
    \item Train batch size (bs): 128, 512, 2048
    \item Number of SGD iterations (it): 5, 10, 15
\end{itemize}

The second batch further investigated the learning rate, SGD minibatch size and train batch size parameters. For example, if 0.001 was the best learning rate, the next grid search also explored 0.0005, 0.0007, 0.003, 0.005. For the batch sizes, the surrounding integers $x$ in $2^{x}$ were explored. Additionally, new parameters were explored:
\begin{itemize}
    \item Value function clip parameter (vfc): 10, 20, 50
    \item Clip parameter (cp): 0.15, 0.2, 0.3
    \item Value function loss coefficient (vfl): 0.9, 0.95, 1
    \item Lambda ($\lambda$): 0.9, 0.92, 0.95, 0.98, 1
\end{itemize}

Table \ref{tab:param14opp} illustrates all chosen parameters.

\begin{table*}[h]
    \centering
    \caption{Chosen parameters for all learning architectures on the 14-bus network with opponent.}
    \begin{tabular}{rllllllll}
        \toprule
        Agent & lr & mbs & bs & it & cp & vfc & vfl & $\lambda$ \\
        \midrule
        Single RL & 0.0005 & 512 & 2048 & 5 & 0.15 & 10 & 0.95 & 0.95 \\
        RL-RL & 0.0001 & 1024 & 1024 & 10 & 0.15 & 10 & 0.95 & 0.95 \\
        RL-Action Value RL & 0.0001 & 1024 & 2048 & 10 & 0.15 & 10 & 0.95 & 0.95 \\
        \midrule
        Greedy-RL & 0.0001 & 1024 & 1024 & 10 & 0.2 & 10 & 1 & 0.95 \\
        \bottomrule
    \end{tabular}
    \label{tab:param14opp}
\end{table*}

%\section{Training Results}\label{ap:results}
%Fig. \ref{fig:curves} illustrates the mean number of timesteps survived on all validation scenarios throughout training for all networks, with and without opponent.

%\begin{figure*}[hbtp]
%    \centering
%    \includegraphics[width=\linewidth]{Figures/TrainingCurves (8).jpg}
%    \caption{The mean number of timesteps survived on all validation scenarios throughout training for the 5-bus network without opponent (top left), the 5-bus network with opponent (top right), the 14-bus network without opponent (bottom left) and the 14-bus network with opponent (bottom right). For the mean and standard deviation, seeds are interpolated between 0 and \texttt{max\_timesteps} to align them on a common time axis.}
%    \label{fig:curves}
%\end{figure*}


\newpage
\section{Individual Seeds for 14-Bus Network With Opponent}\label{individual_seeds}

Figure \ref{fig:compare_single}
shows the mean performance of the Single RL agent and compares it to the individual seeds of the RL-RL agent (left) and the Greedy-RL agent (right) on the 14-bus environment with opponent. 
%The training of the Greedy-RL agent is very computationally expensive. Due to time limits on the available computational cluster, some seeds timed-out before the targeted number of environment interactions was reached.
% The training of the Greedy-RL agent is very computationally expensive. Because of the exponential growth in the state space (Sec. \ref{fcpower}) we were able to complete training for the 14-bus environment only on two of the ten seeds. However, we still observe that it consistently outperforms the single RL agent, both asymptotically and in sample efficiency. 
The training of the Greedy-RL agent is very computationally expensive due to the simulation of the entire action space and the stochastic nature of the environment (Sec. \ref{fcpower}). We were able to complete training for the 14-bus environment with opponent only on two of the ten seeds. However, we still observe that it consistently outperforms the average performance of the single RL agent, both asymptotically and in sample efficiency.


% Concretely, training a single seed until the maximum number of timesteps required all available computational resources within time limits. Therefore, it was not feasible to compute the entire trajectory for all ten seeds. 

% The greedy-learned hierarchical architecture is too computationally expensive to train over 500,000 timesteps and therefore 

%this architecture shows large deviation between seeds. 
%These and other tried parameters show that most seeds stabilize around 600 timesteps survived. However, some seeds do manage to survive for longer, achieving higher scores than all other tried baselines, benchmarks and architectures.

% This indicates that the architecture is prone to getting stuck in local minima. Further exploration of the parameter landscape is needed to achieve more consistent performance across multiple seeds.

% \begin{figure*}[htbp]
%     \centering
%     % New Subfigure on the left
%     \begin{subfigure}[b]{0.3\textwidth} % Adjust width for proper alignment
%         \includegraphics[width=\textwidth]{Figures/single_individual.png} % Replace with your new image file
%         \caption{The individual seeds for the Single RL Agent.}
%         \label{fig:single-ind}
%     \end{subfigure}
%     \hfill
%     % First subfigure
%     \begin{subfigure}[b]{0.3\textwidth} % Adjust width for proper alignment
%         \includegraphics[width=\textwidth]{Figures/rl_rl_individual.png} % Replace with your image file
%         \caption{The individual seeds for the RL-RL Agent.}
%         \label{fig:greedyrl-ind}
%     \end{subfigure}
%     \hfill
%     % Second subfigure
%     \begin{subfigure}[b]{0.3\textwidth} % Adjust width for proper alignment
%         \includegraphics[width=\textwidth]{Figures/greedy_rl_individual.png} % Replace with your image file
%         \caption{The individual seeds for the Greedy-RL Agent.}
%         \label{fig:rlrl-ind}
%     \end{subfigure}

%     \caption{The performance of individual seeds of the Greedy-RL and RL-RL agent on the 14-bus environment with opponent.}
%     \label{fig:individual seeds}
% \end{figure*}

\begin{figure*}[hbtp]
    \centering
    % First subfigure
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{Figures/single_rlrl.png} % Replace with your image file
        \caption{The individual seeds for the RL-RL Agent compared to the mean and standard deviation of the Single RL Agent (blue).}
        \label{fig:subfig1}
    \end{subfigure}
    \hfill
    % Second subfigure
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{Figures/single_greedyrl.png} % Replace with your image file
        \caption{The individual seeds for the Greedy-RL Agent compared to the mean and standard deviation of the Single RL Agent (blue).}
        \label{fig:subfig2}
    \end{subfigure}

    \caption{The mean performance of the Single RL Agent compared to the individual seeds of the Greedy-RL and RL-RL agent on the 14-bus environment with opponent. For the mean and standard deviation, seeds are interpolated between 0 and \texttt{max\_timesteps} to align them on a common time axis.}
    \label{fig:compare_single}
\end{figure*}

\newpage
\section{Results of test scenarios}\label{ap:test_results}
Table \ref{tab:combined_performance} shows the number of time steps survived in the test scenarios. For each architecture, the results of best-performing model seed in the validation scenarios is shown.
%the number of timesteps survived per architecture of the best performing model seed on validation scenarios.
% choose best seed/best model on validation data
% per column, all numbers are different models
% sd is per scenario (20% of all scenarios) and env seeds (10) --- so in total 40 scenarios
% 4 / 40 / 200 / 2000

% sd is per scenario (20% of all scenarios) and env seeds (10) --- so in total 40 scenarios
% 4 / 40 / 200 / 2000
\begin{table*}[htbp]
    \centering
    \caption{Performance of different agents (best performing model seed on validation scenarios) in the test scenarios of the 5-bus and 14-bus networks, with and without opponent, in terms number of timesteps survived. Shown are the mean and standard deviation (sd) across different scenarios (and different environment seeds in case of opponent).}
    \begin{tabular}{rllll}
        \toprule
        Architecture
        & \multicolumn{2}{c}{\# ts survived (sd) in 5-bus network} & \multicolumn{2}{c}{\# ts survived (sd) in 14-bus network} \\
        & Regular & Opponent & Regular & Opponent \\
        \midrule
        Do-Nothing & 212.8 (208.5) & 172.0 (187.6) & 1527.9 (1343.3) & 349.5 (326.7) \\
        Single RL & 2016.0 (0.0) & 2016.0 (0.0) & 6744.4 (2289.7) & 516.8 (468.4) \\
        RL-CAPA & 2016.0 (0.0) & 1514.3 (747.1) & 1533.6 (1353.8) & - \\
        \midrule
        Greedy & 2016.0 (0.0) & 1662.3 (664.8) & 7716.8 (1305.6) & 622.4 (500.0) \\
        % greedy-random & 2016.0 (0.0) & 1621.6 (712.8) & 4291.5 (2629.6) & 485.2 (387.0) \\
        Greedy-CAPA & 2016.0 (0.0) & 2016.0 (0.0) & 6841.8 (2109.3) & 593.6 (434.5) \\
        \midrule
        Greedy-RL & 2016.0 (0.0) & 2016.0 (0.0) & 7853.8 (952.0) & 923.7 (893.5) \\
        RL-Value Softmax & 2016.0 (0.0) & 1270.2 (799.5) & 2316.0 (1987.9) & - \\
        RL-RL & 2016.0 (0.0) & 1977.6 (242.5) & 7510.7 (1600.5) & 1122.4 (1033.6) \\
        RL-Action Value RL & 2016.0 (0.0) & 1976.7 (248.7) & 7410.0 (1531.0) & 835.6 (764.7) \\
        % learned-random & 2016.0 (0.0) & 1303.9 (841.1) & 697.1 (404.7) & - \\
        \bottomrule
    \end{tabular}
    \label{tab:combined_performance}
\end{table*}

% \section{Computational Time}\label{ap:compute_time}

% Table \ref{tab:compute_time} presents the computational time per environment interaction during training for various agents on the 14-bus network with an opponent, using a single CPU per worker. While the results show slight variation across different random seeds, a consistent trend is observed. The Single RL agent demonstrates the lowest computational time, followed closely by the RL-RL and RL-Action Value RL agents. In contrast, the Greedy-RL agent exhibits significantly higher computational time, approximately an order of magnitude greater than the others.

% \begin{table*}[htbp]
%     \centering
%     \begin{tabular}{c|c}
%     Agent & Compute Time per Environment Interaction (s) \\
%     \hline
%         Single RL & 0.307 \\
%         RL-RL & 0.492 \\
%         RL-Action Value RL & 0.468 \\
%         Greedy-RL & 5.089\\
%     \end{tabular}
%     \caption{Compute Time Per Timestep While Training 14-Bus Network With Opponent}
%     \label{tab:compute_time}
% \end{table*}

\end{document}
\endinput
%%
%% End of file `sample-sigconf.tex'.
