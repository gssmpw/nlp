\section{Background and Related Work}
\label{sec:background}
This section deals with the modeling of the PNC problem (Section \ref{sec:L2RPN}), applications of RL in this domain (Section \ref{sec:RLL2RPN}), and recent HRL and MARL implementations that focus on improving sample efficiency and overcoming the curse of dimensionality (Section \ref{sec:SRL}).

\subsection{Power Network Control}\label{sec:L2RPN}
Secure operation of power networks is required both in normal operating states as well as in contingency states (i.e., after the loss of any single element on the network). That is, the following requirements must be met: (i) In the normal operating state, the power flows on equipment, voltage and frequency are within pre-defined limits in real-time; (ii) In the contingency state the power flows on equipment, voltage and frequency are within pre-defined limits. Loss of elements can be anticipated (scheduled outages of
equipment) or unanticipated (faults for lightning, wind, spontaneous equipment failure). Cascading failures
must be avoided at all times to prevent blackouts (corresponding to the game over state in the RL challenge).
%(Kelly et al., 2020).

Importantly, each of the control actions performed by power system operators usually not only affects the
current state of the power system but also the future state and availability of future control actions, that
is, short-term actions can have long-term consequences. As a result, the decision problem of power system
operators is typically a sequential decision-making problem in a combinatorial action space in which the current decision can affect all
future decisions. Moreover, due to possible nondeterministic changes of the power system state (e.g., due
to unplanned outages or the intermittent behaviour of renewable energy sources) and different sources of
error (e.g., measurement errors, state estimation errors, flawed judgement) the operators need to handle
uncertainty in their decisions. Finally, operational decisions must often be made quickly, under hard time
constraints ____.

%The primary goal of PNC is to prevent blackouts by ensuring power flow stays within thermal limits, regulating voltage, and balancing generation and load. As grids modernize and integrate variable energy sources, congestion management has become critical to address violations of voltage, frequency, thermal, and stability limits that risk equipment damage and system failures.

% The Grid2Op framework ____, based on Markov Decision Process (MDP) modeling ____, is frequently used to simulate and assess PNC strategies, particularly in the context of the ``Learning to Run a Power Network" (L2RPN) challenges initiated by the RTE. These challenges push for the development of robust PNC controllers.

% describe environment
%The Grid2Op framework ____ models power grids as graphs with substations, powerlines, generators, loads, and shunts. Substations manage grid connectivity via two busbars, which can couple or decouple to alter connectivity (see Fig. \ref{fig:topaction}). Powerlines enable power flow within thermal limits, generators provide power, loads consume power, and shunts influence power flow by creating low-resistance paths. Generators and loads, collectively known as injections, differ in that generators can be redispatched to manage congestion, while loads vary without constraints.

% actions uitleg
% This research focuses on solely the topological action space, which governs the configuration of powerlines, generators, and loads across busbars within substations. By turning various grid elements on or off, the network topology changes. The switching is accomplished by influencing what elements are connected to each other. These actions reroute the power flow, relieving pressure from overloaded lines to other parts of the grid. Fig. \ref{fig:topaction} illustrates the effect of such an action on the network.

%\begin{figure}
%    \centering
%    \includegraphics[width=\linewidth]{Figures/node_splitting.png}
%    \caption{A graphic illustration of how a substation can be split into two separate nodes, dividing its connected lines, taken from \protect____.}
%    \label{fig:topaction}
%\end{figure}

\subsection{Deep RL for Power Network Control}\label{sec:RLL2RPN}
Deep RL has demonstrated significant potential in PNC, enabling robust and adaptable behavior over extended time horizons ____. This capability surpasses the limitations of expert systems ____ and optimization methods, which are constrained by computation time ____.

In L2RPN competitions, successful solutions typically combine expert rules, RL agents, and brute force simulations for action validation. Notably, RL agents based on Proximal Policy Optimization (PPO) ____, with reduced action spaces, have performed well. This approach was used in top-ranking competition entries (e.g. ____, further explored in  studies ____).

% \todo{Idea behind the section below is that we can link the power-system knowledge and acting in hazardous situations part in the methods back to this. The simulations could be linked to future research ideas to improve performance.}
Several commonalities emerge among top-ranking approaches. Most successful methods implement intervention only during hazardous situations ____. 
In addition, many such approaches integrate learned modules with simulation-based action evaluation. For instance, this strategy was key to the success of the winning teams in both 2019 ____ and 2021 ____. 
Notably, the second-place competitor in 2021 employed an advanced expert system, underscoring the value of incorporating domain knowledge from power systems ____. %\textcolor{red}{Barbera: Going back to base topology additional explain here?}

\subsection{Curse of Dimensionality}\label{sec:SRL}
The combinatorial complexity of topological actions in PNC poses a challenge for deep RL by hindering complete and consistent convergence and exploration of action and state spaces. HRL simplifies learning by breaking tasks into subtasks, enabling more focused and sample efficient learning per task as well as increased interpretability. MARL distributes decision-making across agents, effectively factorizing the MDP's action space.

\subsubsection{Hierarchical Reinforcement Learning}
In the L2RPN WCCI 2020 competition, the winning agent for a 36-bus network used a two-tiered framework: a high-level policy proposed a goal topology, while a low-level policy determined the sequence of individual topological actions ____.

Later, a three-level HRL framework was introduced ____, where the top level activates the agent in hazardous situations, the intermediate level selects a substation using RL, and the lowest level identifies a configuration. They tested greedy and RL-based approaches for the lowest level, and compared PPO and Soft Actor Critic (SAC) ____ for the RL-based modules. They found PPO to have faster convergence, smaller variance, and higher expected rewards.

%MARL
\subsubsection{Multi-Agent Reinforcement Learning}
The action space in PNC can be factorized into exclusive subsets, one for each substation, enabling a natural MARL framework in a fully cooperative setting where all agents optimize the same collective objective. Following this approach, ____ extended ____ by introducing multiple agents at the lowest level, with each substation controlled by an agent and a heuristic-based intermediate level. They trained agents using independent PPO (IPPO) and dependent PPO (DPPO). On a 5-bus network, both achieved optimal scores, though single-agent PPO converged faster. However, they anticipate that MARL's advantages will become more pronounced when scaled up to larger networks.

In other domains, ____ showed strong PPO-based multi-agent performance in testbeds, recommending best practices like value normalization, integrating global and local information, 
and limiting sample re-use to avoid training instability from MARL non-stationarity.


% \begin{figure*}[th!]
%     \centering
%     \includegraphics[width=\linewidth]{Figures/fc.drawio.png}
%     \caption{Diagram of the Feedback Control Framework used in this research. The goal to be achieved (and maintained) in the system is an input to the entire framework. The comparator determines the difference between the current state of the system and the desired one. There are then two possible paths for the control flow. If a goal state is reached then no action is performed and the reward is accumulated. If the system is not in the goal state then an agent is called. Periodically this agent can be updated by a trainer. The dashed line represents the only link between the two loops, as the trainer will perceive the accumulated rewards of consecutive time-steps where the {\it do-nothing} was used for control}
%     \label{fig:feedback-control}
% \end{figure*}

\begin{figure*}[th!]
    \centering
    \includegraphics[width=\linewidth]{Figures/fc_new.jpg}
    \caption{Diagram of the Feedback Control Framework used in this research. The goal to be achieved (and maintained) in the system is an input to the entire framework. The comparator determines the difference between the current state of the system and the desired one. There are then two possible paths for the control flow. If a goal state is reached then no action is performed and the reward is accumulated. If the system is not in the goal state then an agent is called. Periodically this agent can be updated by a trainer. The dashed line represents the only link between the two loops, as the trainer will perceive the accumulated rewards of consecutive time-steps where the {\it do-nothing} was used for control}
    \label{fig:feedback-control}
\end{figure*}

% Explain purpose
% Write proper caption with small FC framework summary
% explain dashed line
% normalize font
%   remove box from observation
%   remove box of accumulate
%   rename accumulate as acummulated reward
%   trainer and agent same color (blue)
%   comparator and gate same color (orange)
%   make `system` same font as sMDP (also bold)