% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
% \usepackage{acl}
\usepackage{acl}

% Standard package includes
\usepackage{times}
\usepackage{bm}
\usepackage{latexsym}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bbm}
\usepackage{cleveref}
\usepackage{float}
\usepackage{fix-cm}
\usepackage{ntheorem}
% \usepackage{fdsymbol}
\usepackage{pifont}
\newtheorem{hypoth}{Hypothesis}

\definecolor{trend}{HTML}{245D95}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\crefname{item}{hypothesis}{hypotheses}
\Crefname{item}{Hypothesis}{Hypotheses}

\title{Language Models Grow Less Humanlike beyond Phase Transition}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author{Tatsuya Aoyama \hspace{10pt}
  Ethan Gotlieb Wilcox \\
  Department of Linguistics, Georgetown University \\
  \{\emldisplay{ta571@georgetown.edu}{ta571}, \emldisplay{ethan.wilcox@georgetown.edu}{ethan.wilcox}\}\texttt{@georgetown.edu}}


%\author{
%  \textbf{First Author\textsuperscript{1}},
%  \textbf{Second Author\textsuperscript{1,2}},
%  \textbf{Third T. Author\textsuperscript{1}},
%  \textbf{Fourth Author\textsuperscript{1}},
%\\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
%\\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
%\\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
%\\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
%\\
%\\
%  \textsuperscript{1}Affiliation 1,
%  \textsuperscript{2}Affiliation 2,
%  \textsuperscript{3}Affiliation 3,
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
%\\
%  \small{
%    \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
%  }
%}

\begin{document}

\maketitle

\begin{abstract}

LMs' alignment with human reading behavior (i.e. \textit{psychometric predictive power; PPP}) is known to improve during pretraining up to a tipping point, beyond which it either plateaus or degrades. Various factors, such as word frequency, recency bias in attention, and context size, have been theorized to affect PPP, yet there is no current account that explains \emph{why} such a tipping point exists, and how it interacts with LMs' pretraining dynamics more generally. We hypothesize that the underlying factor is a pretraining \textit{phase transition}, characterized by the rapid emergence of specialized attention heads. We conduct a series of correlational and causal experiments to show that such a phase transition is responsible for the tipping point in PPP. We then show that, rather than producing attention patterns that contribute to the degradation in PPP, phase transitions alter the subsequent learning dynamics of the model, such that further training keeps damaging PPP.
\end{abstract}

\section{Introduction}\label{sec:intro}

The rise of neural-network-based language models (LMs) that can produce fluent, humanlike linguistic outputs has led to their increased use as cognitive models of human language processing \citep{futrell2021natural}.
In particular, an active area of research has studied how well LMs are aligned with human incremental processing behaviors. This is often measured by how well LMs' output probabilities predict various human reading time metrics, and is referred to as a model's \textit{psychometric predictive power} or \textit{PPP}. PPP
has been widely studied for English (\citealp[e.g., ][\textit{inter alia}]{wilcox-etal-2020-predictive, shain-etal-2024-large, oh-etal-2022-gpt, oh-schuler-2023-surprisal}), Japanese \cite{kuribayashi-etal-2021-lower}, multilingually \cite{wilcox-etal-2023-testing}, and for nonnative Englishes \cite{aoyama-schneider-2024-modeling}, where it has been found that LMs' outputs are robustly correlated with human reading times.

\begin{figure}[t]
    \hspace{-20pt}
    \input{latex/figures/splashy.pgf}
    \vspace{-20pt}
    \caption{Phase transition ($x$-axis) and PPP peaks ($y$-axis) closely coincide with each other across the corpora studied in this work, except for MECO. Numbers in the parentheses are batch sizes; for Pythia models, regardless of the model size, the batch size is 2,048. \textbf{\textcolor{orange}{Orange}} dashed lines, \textbf{\textcolor{trend}{blue}} solid lines, and \textbf{\textcolor{gray}{gray}} dashed lines represent the \textbf{\textcolor{orange}{phase transition hypothesis}}, \textbf{\textcolor{gray}{2 billion tokens hypothesis}}, and the \textbf{\textcolor{trend}{observed trendline}} drawn from the data, respectively. We find that the experimental results closely align with our phase transition hypothesis, meaning that the PPP peaks correspond to LM phase transition, beyond which PPP starts degrading. *$p <.05$; **$p < .01$.}
    \vspace{-10pt}
    \label{fig:splashy}
\end{figure}

One outstanding puzzle in these data is the observation that transformer-based LMs become maximally aligned with human sentence processing relatively early in pretraining, after which their fit to human data either plateaus or decreases. We will refer to the maxima of psychological fit during training as model's \define{tipping point}. Such tipping points present a conundrum: Why does the fit to human data decrease, even as the model's language modeling loss goes down? And what is the tipping point's underlying cause?

Previous work has observed that tipping points in models tend to occur after around 2 billion words of pretraining, and has suggested that this number---2 billion---is operative \citep{oh-schuler-2023-transformer}. We refer to this as the \define{2 Billion Hypothesis}. In contrast, we suspect that the tipping point at 2 billion tokens is coincidental rather than causal. Instead, we hypothesize that the reversal of PPP is related to more general pretraining dynamics, specifically the presence of \define{phase transitions}, or periods in pretraining when new model capabilities emerge rapidly. While phase transition can be a general term used to describe any rapid change in model behaviors, we focus on what we term \define{specialized heads phrase transitions}, characterized by the emergence of specialized attention patterns relatively early in pretraining \citep{olsson2022context, chen2024sudden}. For brevity, we refer to these simply as ``phase transition'' (see \Cref{sec:relevant:pt} for a more precise definition). We dub this hypothesis the \textbf{Phase Transition Hypothesis} and conduct a series of experiments, both correlational and causal, to test it (\Cref{fig:splashy}).

Our key findings include: (1) the tipping points of PPP are strongly correlated with phase transition (\Cref{sec:exp1}); (2) a series of ablation experiments show some attention heads that formed during phase transition do have a strong effect on PPP, although the overall results are mixed (\Cref{sec:exp2}); and (3) regularization during pretraining can suppress the formation of specialized heads and thereby alleviate the PPP degradation (\Cref{sec:exp3}).

\section{Relevant Work}\label{sec:relevant}
\subsection{Sentence Processing}\label{sec:relevant:sp}

One popular theory to explain human sentence processing is \emph{surprisal theory}. Surprisal theory posits that the processing difficulty of a word is proportional to its information content, quantified as its surprisal, or in-context, negative log probability \cite{LEVY20081126, hale-2001-probabilistic}. Surprisal theory is supported by numerous studies showing a tight linear relationship between incremental processing times and surprisal, across various datasets \citep{SMITH2013302, shain-etal-2024-large} and languages (\citealp{wilcox-etal-2023-testing}).\footnote{Although cf. \citealp{meister-etal-2021-revisiting} who find a slightly \emph{super}-linear relationship.}

Rather than using LMs as a gold proxy for word predictability, other studies have compared how surprisals obtained from various LMs show different fits to human reading data. \citet{goodkind-bicknell-2018-predictive} find that LM quality, as measured in perplexity, linearly correlates with PPP, such that surprisals obtained from better LMs are more predictive of human reading time. This relationship between LM quality and PPP has been dubbed the \textit{quality-power hypothesis} and replicated with different sets of LMs \cite{wilcox-etal-2020-predictive, oh-etal-2022-gpt, oh-schuler-2023-surprisal}, and cross-lingually \cite{wilcox-etal-2023-language}.

Instead of comparing fully trained LMs, \citet{oh-schuler-2023-transformer} find that the quality-power relationship changes during pretraining, which motivates the current study. Specifically, they find that PPP keeps improving until around 2 billion tokens of pretraining (for all Pythia variants) and then starts degrading beyond that point, although perplexity keeps improving. In other words, the quality-power correlation is \textit{positive} (better the model, higher the PPP) until a certain point and \textit{negative} (better the model, lower the PPP) after it. We call this point a \define{tipping point} in this paper, and define it as the maximum PPP obtained during pretraining. In fact, \citet{kuribayashi-etal-2021-lower} seems to be the first to report this trend, where they find similar tipping points for English and Japanese. This tipping point has been questioned in \citet{aoyama-schneider-2024-modeling}, where they observe a \textit{negative} quality-power correlation at orders of magnitude smaller pretraining amounts for crosslingual LMs.% (transferred from non-English languages to English).

\subsection{Factors Affecting LMs' PPP}\label{sec:relevant:factors}

Besides the amount of pretraining, several other factors have been found to affect LMs' PPP.\footnote{For a more extensive list, we refer the readers to Table 1 of \citealp{kuribayashi-etal-2022-context}} One key factor is model size, where larger models tend to perform worse in modeling human sentence processing compared to smaller models. \citet{oh-etal-2022-gpt} found that structural parsers and $n$-gram models often matched or outperformed GPT2 models, with the smallest GPT2 variant yielding the best results. \citet{oh-schuler-2023-surprisal} tested additional GPT and OPT variants confirming this trend. Another crucial factor is context size, as limiting the inference-time context window has been shown to improve PPP. \citet{kuribayashi-etal-2022-context} observed that shorter contexts, particularly bigrams, resulted in better PPP, especially for Japanese. Additionally, introducing recency bias in attention mechanisms improves PPP \cite{de-varda-marelli-2024-locally}, particularly when applied during both training and inference \cite{clark-etal-2024-linear}.

Lexical factors also play a role in PPP degradation among larger models. \citet{oh-schuler-2023-surprisal} found that larger models tend to assign lower surprisals to open-class words (e.g., nouns and adjectives) and named entities, deviating more from human reading patterns. \citet{oh-etal-2024-frequency} further linked this effect to token frequency, showing that larger models exhibit particularly low alignment with human reading time for infrequent tokens, especially beyond a certain point in pretraining (2B tokens). These studies suggest that model size, long context utilization, and pretraining dynamics interact with each other to produce PPP degradation. We propose that one way to tie these pieces together is by finding a common cause, which we hypothesize to be phase transitions.%affecting PPP degradation as part of a broader phase transition phenomenon, which we now turn to.

\subsection{Phase Transition}\label{sec:relevant:pt}

What exactly is a phase transition, and how is it measured? Abrupt changes in model behaviors, which cannot be predicted by a scaling law \cite{kaplan2020scalinglawsneurallanguage}\footnote{See \citealp{caballero2023broken} for a scaling law with `breaks'.}, have been studied widely (e.g., \citealp{wei2022emergent, srivastava2023beyond}). In this paper, we specifically focus on what we call \define{specialized heads phase transition}, which is characterized by the rapid emergence of specialized attention patterns \cite{elhage2021mathematical, olsson2022context, chen2024sudden}, relatively early in pretraining. For brevity, we use the term ``phase transition.''

We operationalize the phase transition using two well-studied phenomena, the presence of induction heads \cite{elhage2021mathematical, olsson2022context} and syntactic attention structure (SAS; \citealp{chen2024sudden}). \citet{elhage2021mathematical, olsson2022context}
find that a specialized head called an induction head emerges at a certain point during pretraining, and that this head is characterized by its distinctive copying behavior, where the head attends to the previous occurrence of the same or similar bigram within the context. We introduce the metric used to detect induction heads in \cref{eq:pms}. \citet{olsson2022context} further claim that the emergence of this induction head is primarily responsible for most of the in-context learning (ICL) abilities (see \cref{eq:icl-score}).%\footnote{See Yin for a discussion on the disassociation between this formalization of ICL score and few shot learning accuracy.}

Similarly, \citet{chen2024sudden} find that LMs demonstrate a sudden emergence of attention patterns that mirror syntactic dependency edges, which they call SAS. Similar to induction heads, the emergence of SAS is followed by an abrupt improvement in syntactic abilities as measured by performance on a syntactic probing benchmark, BLiMP \cite{warstadt-etal-2020-blimp-benchmark}.

\subsection{Research Questions}\label{sec:rq}
In light of this, we ask the following research question: \textbf{to what extent is phase transition responsible for the degradation in the alignment between human and LM sentence processing?} We present three hypotheses, outlined below:

\begin{hypoth}
    \textbf{Phase Transition Hypothesis:} PPP degradation coincides with phase transition.
\end{hypoth}

\noindent If this is the case, then we need to account for \textit{how} and \textit{why} phase transition causes the PPP degradation, for which we have two hypotheses:

\begin{hypoth}
    \textbf{Substructure Hypothesis:} Attention patterns that emerge during phase transition cause PPP degradation.
\end{hypoth}

\noindent If the degradation cannot be explained sufficiently by specific heads, then, taken together with the observation that PPP \emph{keeps} degrading beyond a certain point, we hypothesize that the degradation should be attributed to the pretraining dynamics \textit{after} the phase transition. This predicts that, rather than turning off specific heads, suppression of phase transition altogether is required to prevent PPP degradation, leading to our final hypothesis:

\begin{hypoth}
    \textbf{Dynamics Hypothesis:} Suppression of phase transition mitigates PPP degradation.
\end{hypoth}


\section{Methods}\label{sec:method}

\subsection{Models and Checkpoints}\label{sec:method:models}

Because online sentence processing is a predominantly left-to-right process, we only consider autoregressive LMs, a common practice adopted by many others (see \citealp{meister-etal-2023-locally} for a rare exception). Since training LLMs is expensive, we primarily focus on the Pythia family \cite{biderman2023pythiasuiteanalyzinglarge}. Pythia is a rare, if not the only, LM family that satisfies the following conditions necessary in this paper: (1) open-sourced, (2) available in different sizes, and importantly, (3) various checkpoints, especially early in pretraining, are available. Bloom \cite{bigscience_workshop_2022} is another candidate; however, because the available checkpoints are likely after the phase transition, we did not include it in this study. We also train several GPT2 models \cite{radford2019language} and save log-spaced checkpoints at $\{500K, 1M, 2M, ..., 256M\}$ tokens for the first 10 checkpoints, and evenly-spaced checkpoints for the last 20 checkpoints at $\{0.5B, 1B, 1.5B ..., 10B\}$ tokens. This is a superset of the available Pythia checkpoints for the first 10B tokens of pretraining, which allows for direct comparisons among different model sizes and families at the same pretraining amounts. Each run took $\approx$70 hours on a single A6000.

\subsection{Data}\label{sec:method:data}
For custom trained models, we use a sample of 1B tokens from the English subcorpus of \texttt{CC100} (\citealt{conneau-etal-2020-unsupervised, wenzek-etal-2020-ccnet}; Apache License 2.0). For reading time data, we use 3 eye-tracking corpora: Dundee \cite{kennedy2003dundee}, Provo (\citealp{luke2018provo}; CC-BY 4.0)\footnote{\url{https://osf.io/sjefs/}}, and MECO \cite{siegelman2022expanding} release 1.2.\footnote{\url{https://osf.io/3527a/}}, and 1 self-paced reading time corpus: Natural Stories (\citealp{futrell2021natural}; CC BY-NC-SA).

\subsection{Calculating PPP}

Following previous work (e.g. \citealp{goodkind-bicknell-2018-predictive, wilcox-etal-2020-predictive, wilcox-etal-2023-language}, \textit{inter alia}), we operationalize PPP as the difference in log-likelihood (LL) between two linear models (delta log-likelihood; \dll):
\begin{equation}
    \dll = \text{LL}_{f_{\text{base+surp}}}-\text{LL}_{f_{\text{base}}}
\end{equation}
\noindent where $f_{\text{base+surp}}$ and $f_{\text{base}}$ are linear models that predict human reading times using baseline features with and without LM surprisals, respectively. Finally, surprisal of a word $w_i$ in context is its negative log probability: $-\log \text{P}(w_i|\mathbf{w}_{<i})$. See \Cref{appendix:ppp} for the full list of features included in the regression models.

\section{Experiment 1: PPP Peaks at Phase Transition}\label{sec:exp1}

\subsection{Method}\label{sec:exp1:method}

As we want to show that PPP peaks at the phase transition, defined by the emergence of SAS and induction heads, we outline the definition of the metrics we use to characterize them below.

\noindent\textbf{Unlabeled Attachment Score (UAS)}. UAS is a commonly used metric for dependency parsing, and in the context of SAS, conceptually, it measures how well a given LM's attention pattern matches the dependency edges between words, ignoring the relation labels.

In other words, UAS is the proportion of words, such that the highest attention weight lies between the word and its parent for the best-performing head for the given relation type. See \Cref{appendix:uas} for details.

\noindent\textbf{Prefix-matching score (PMS).} Induction heads are defined by the copying behavior, such that if the model has seen an \{A\}\{B\} sequence and the current token is \{A\}, then an induction head is a head that promotes the prediction of \{B\} as the next token, completing the \{A\}\{B\}$\dots$\{A\}$\rightarrow$\{B\} sequence. 
We quantify this behavior using PMS \cite{olsson2022context}.

Given a random sequence of tokens $\mathbf{x}$ repeated twice, PMS of a head $h$ at layer $l$ is its average attention from the source token $x_i$ to the next token of its previous occurrence:
\begin{equation}\label{eq:pms}
\text{PMS}=\frac{1}{|\mathbf{x}|-1}\sum_{i=|\mathbf{x}|+1}^{2|\mathbf{x}|}\mathrm{\alpha}^{(h,l)}(x_i, x_{i-(|\mathbf{x}|-1)})
\end{equation}
In our experiments, we use a random sequence of length 50, following the \texttt{TransformerLens} library \cite{nanda2022transformerlens}, so $|\mathbf{x}|=50$. Note that $|\mathbf{x}|$ is a completely random sequence of token IDs; hence, attention patterns are not an artifact of data contamination or memorization.

\noindent\textbf{Breakthrough Points.} \citet{chen2024sudden} defines \textit{breakthrough} in some metric $f$ as the acceleration point, which maximizes the growth in slope, measured in some discrete intervals $\Delta$. To make it less susceptible to surface fluctuations, we define them as the first checkpoint $c$ at which a certain metric $f$ exceeds a given threshold $t$: $\min \{c \in \mathcal{C}|f(c)>t\}$, where $\mathcal{C}$ is the predefined set of checkpoints described in \Cref{sec:method:models}. We validate these implementations of breakthroughs as well as the measurements of UAS and PMS by showing that the sudden rises in UAS and PMS are followed by the improvement in BLiMP and ICL scores, respectively (see \Cref{appendix:pt-details} for these results).

\subsection{Results}\label{sec:exp1:result}

\Cref{fig:splashy} shows on the $x$-axis the number of pretraining tokens at which phase transition occurred, as defined by UAS (top row) and by PMS (bottom row), and on the $y$-axis the number of pretraining tokens at which the peak \dll was observed. The \textcolor{gray}{gray lines} represent the \textcolor{gray}{\define{2 Billion Hypothesis}} based on \citet{oh-schuler-2023-transformer}, where transformer models are predicted to reach the highest \dll at around 2 billion pretraining tokens. The \textcolor{orange}{orange lines} represent our hypothesis \textcolor{orange}{\define{Phase Transition Hypothesis}}, where the peak \dll is predicted to happen at around the same time as the phase transition. The \textcolor{trend}{blue lines} are the \textcolor{trend}{observed trendline}, and we can see that they closely match the Phase Transition Hypothesis. This trend holds across 3 orders of magnitude, meaning that the $\Delta$LL peaks occur at vastly different times across different models, yet they all co-occur with their respective phase transition points, although this trend was not found in MECO (see \Cref{sec:limitation} for known issues of MECO). Note that we included Pythia models used in \citet{oh-schuler-2023-transformer}, and GPT2 models trained from scratch with various batch sizes, as we find that the phase transition point is a function of not only the number of pretraining tokens, but also the number of updates (hence changing batch size affects phase transition points). We leave a precise characterization of phase transition points to future work.
For a complete visualization of the entire trajectory of $\Delta$LL, see \Cref{fig:coinciding} in \Cref{appendix:full-results}, where we include 0- and 1-layer models, which are expected to behave differently from >2-layer models (i.e. 0 layer model does not have attention layers, and 1 layer model cannot form induction heads, as they require attention composition; see \citealp{olsson2022context}). We find that the 0-layer model indeed exhibits no degradation when 1- and 2-layer models face the onset of degradation.

The concurrence of PPP peaks and phase transitions seems robust; however, these observations are still correlational, and we need more evidence to make a causal claim. As such, we consider two potential causal explanations, outlined earlier: (1) phase transition produces attention patterns that hurts PPP (\define{Substructure Hypothesis}), and/or (2) phase transition changes the course of pretraining dynamics, in a way such that further training hurts PPP (\define{Dynamics Hypothesis}).
We test each hypothesis in the following experiments.

\section{Experiment 2: PPP Degradation Cannot be Attributed to Specific Heads}\label{sec:exp2}

If phase transition creates a certain structure responsible for the lower PPP, ablating that structure should improve PPP for models that have undergone phase transition. In this experiment, we ablate each head one at a time and investigate if the ablation of specialized heads that form during phase transition (i.e. syntactic heads and induction heads) improves PPP more than other heads.


\begin{table*}[t]
    \centering
    \footnotesize
    \input{latex/pearson}
    \caption{Pearson correlation coefficients between SAS score and $\Delta\Delta$LL (top) and PMS (bottom) and $\Delta\Delta$LL for 2 and 3 variants of GPT2 and Pythia, respectively. du, me, pr, and ns stand for Dundee, MECO, Provo, and Natural Stories, respectively. Correlations statistically significant at $\alpha=.05$ are boldfaced and colored in red. Note that these p-values are \textit{before} any correction for multiple testing is applied.}
    \label{tab:pearson}
\end{table*}

\subsection{Method}\label{sec:exp2:method}

\noindent\textbf{Scores.} PMS is by definition a head-specific score (i.e. how much a given head attends to the previous occurrence of token B when given a sequence \{AB\}...\{A\}). On the other hand, SAS is measured at the \textit{model} level; it's measured in UAS by picking attention scores of the best heads for each dependency relation type. Hence, we define a slightly modified \textit{head}-specific metric called \define{SAS score}. SAS score of the head $h$ at layer $l$ is a proportion of the words $w_i$ whose attention edge with the highest weight, $(i, \arg\max_{j} [a^{(h,l)}_{ij}])$ corresponds to a child-parent pair of a dependency relation.

\noindent\textbf{Ablation.} We zero-out attention weights of each head while keeping the original attention of all other heads and compute \dll (see \Cref{appendix:ablation} for the details). We then subtract the non-ablated score from the ablated score and call this value $\Delta\Delta$LL. \textit{Higher} $\Delta\Delta$LL means that ablation \textit{improves} PPP. We report the correlations between (1) SAS score and $\Delta\Delta$LL and (2) PMS and $\Delta\Delta$LL.

\subsection{Results}\label{sec:exp2:result}
We start by considering a single, illustrative example: \Cref{fig:pms_dll} plots each head's PMS ($x$-axis) and $\Delta\Delta$LL for Dundee corpus ($y$-axis) for Pythia-70M at 2B tokens of training. These particular results appear to confirm our hypothesis: heads high in PMS are also high in $\Delta\Delta$LL, meaning that the heads performing induction, which contributes to ICL, \textit{damages} the PPP.

\begin{figure}[t]
    % \hspace{-20pt}
    \input{latex/figures/pearsons.pgf}
%    \vspace{-10pt}
    \vspace{-10pt}
    \caption{Relationship between each head's PMS and the effect of its ablation on $\Delta$LL for the Pythia-70M model at 2 billion pretraining tokens. Each point represents one of the 48 heads in the model.}
    \label{fig:pms_dll}
\end{figure}
However, we also need to test if this is the case for other models, checkpoints, and data (other than Dundee). We focus on 5 checkpoints around (and including) the phase transition point (64M for GPT2 models and 2B for Pythia models). This produces 5 (checkpoints) $\times$ 5 (models) $\times$ 4 (corpora) $\times$ 2 (scores: SAS score and PMS) $=$ 200 scatterplots like \Cref{fig:pms_dll}. For readability, we only report Pearson's correlation coefficients for each, shown in \Cref{tab:pearson}. For example, \Cref{fig:pms_dll} can be found under column Pythia-70M and du, row 2B on the bottom half of the table ($r=0.7$, $p<.05$).

In \Cref{tab:pearson}, a few trends emerge: First, we find strong correlations for models closer to the transition points (64M for GPT2 models and 2B for Pythia models). The very undertrained GPT2 models at 16M pretraining tokens, and Pythia models at 0.5B pretraining tokens, for example, show virtually no correlations between $\Delta\Delta$LL and SAS or PMS. Second, whereas SAS$\sim$$\Delta\Delta$LL correlation is positive but PMS$\sim$$\Delta\Delta$LL correlation is negative for GPT2 models, the opposite is true in general for Pythia models. We do not know why this should be the case, and it warrants further investigation. Given that more than half of the model checkpoints show no significant correlations, and that none of the significant correlations remain significant after Bonferroni correction for multiple testing is applied, we take these data as not supporting the claim that the post--phase change degradation in PPP can be attributed to specific heads.

\section{Experiment 3: Suppressing Phase Transition Improves PPP}\label{sec:exp3}

In this experiment, we test a second possible explanation: If phase transition changes the course of pretraining dynamics, and if the new, post-transition pretraining dynamics lead to the degradation in PPP, suppressing the phase transition should result in the improvement, or at least preservation, of PPP beyond the point at which phase transition was \textit{supposed to} occur.

\subsection{Methods}\label{sec:exp3:method}

Completely suppressing phase transition is difficult, if not impossible, as we do not have a full mechanistic understanding of this phenomenon. However, as induction heads and SAS are among the few well-documented diagnoses of phase transition, in this section we suppress them as a proxy method for suppressing phase transition more generally.
%
\begin{figure*}[t]    \input{latex/figures/reg_trajectory.pgf}
    \centering
    \vspace{-20pt}
    \caption{Effect of regularization on $\Delta$LL. All models are 2-layer GPT2 models, trained with a batch size of 4.}
    \label{fig:suppression_dll_all}
\end{figure*}
\begin{figure}[t]
    \hspace{-7pt}
    \input{latex/figures/reg_effect_10b.pgf}
    \vspace{-10pt}
    \caption{$\Delta$LL of 5 variants of 2 layer GPT2 models at the end of pretraining (10B tokens). Red dotted lines represent the $\Delta$LL obtained from the normal (i.e. unregularized) GPT2, which serve as the baseline for the comparison within each subplot.}
    \label{fig:reg_effect_10b}
    \vspace{-20pt}
\end{figure}
%
Since SAS quantifies attention heads whose attention patterns shadow dependency edges, \citet{chen2024sudden} propose using a syntactic regularizer:
%
\begin{equation}
\mathcal{L}_{\textsc{sas}}(x) = \underbrace{\mathcal{L}_{\textsc{clm}}(x)}_{\text{Original loss}} 
\underbrace{
    + \lambda \sum_{i=1}^{|x|} \sum_{x_j \in D(x_i)} \alpha(x_i, x_j)
}_{\text{Syntactic regularization}}
\end{equation}
%
where $x$ is an input, $D$ is a child-parent mapping of dependency relations, and $\alpha$ is an attention weight between a pair of words. $\lambda$ is a weighting factor, and \citet{chen2024sudden} find that $\lambda=0.001$ works best with BERT (positive $\lambda$ \textit{suppresses} SAS, whereas negative $\lambda$ \textit{promotes} it). We show results from $\lambda=\{0.01, 0.001\}$. We use \texttt{spaCy} \cite{honnibal2017spacy} to parse our training data.

To suppress the formation of induction heads, we regularize against attention patterns that correspond to ``copying'' behavior:
%
\begin{equation}
\mathcal{L}_{\textsc{copy}}(x) = \underbrace{\mathcal{L}_{\textsc{clm}}(x)}_{\text{Original loss}} 
\underbrace{
    + \lambda\sum_{i=1}^{|x|} \sum_{x_j \in \text{PM}(x_i)} \alpha(x_i, x_j)
}_{\text{Copying regularization}}
\end{equation}
%
Where $\text{PM}(x_i)$ is a prefix matching tokens of $x_i$. Recall that the copying behavior of induction heads was characterized by predicting \{B\} when given a sequence \{AB\}...\{A\}. Since we do not know which tokens are considered ``similar enough'' to promote the formation of induction heads in natural texts (see \citealp{chen-etal-2024-parallel} for the discussion on ``parallel structure'' that are central to LMs' ICL abilities), we construct synthetic data consisting of repeated random sequences of tokens (see \Cref{appendix:ih-reg} for the details of the dataset construction and copying regularization implementation).

\subsection{Results}\label{sec:exp3:result}

We first verify that SAS and copying suppression is working as intended by showing that SAS suppression leads to very low UAS and consequently lower BLiMP scores, and that copying impression leads to almost 0 PMS and lower ICL scores (these results are presented fully in \Cref{appendix:suppression}).

We now show the effect of regularization on PPP at the end of the pretraining in \Cref{fig:reg_effect_10b}.
First, we find that syntactic regularization tends to improve PPP over non-regularized models, and this is robust to the settings of $\lambda$ or the corpus. Second, syntactic regularization is also more effective at improving PPP than copying regularization, which shows mixed results; its impact is negative at the lower lambda value ($\lambda=0.001$) for the Natural Stories corpus, whereas for MECO, it adversely affects PPP regardless of the regularization strength.
While the above results describe the models at the \emph{end} of training, \Cref{fig:suppression_dll_all} summarizes the effects of regularization on PPP throughout the pretraining.
A few important details emerge: First, for all corpora, regularized models have higher $\Delta$LLs on average beyond the original phase transition points.
Second, the degradation in PPP at the phase transition point is partly or completely suppressed, although the results vary by regularization type, strength, and corpus. For example, for Dundee corpus, syntactically regularized models have lower PPP around the transition point compared to the original model; however, they degrade less than the original model and consequently end up with higher PPP later in pretraining. This trend is much more pronounced in Natural Stories Corpus; as opposed to the original model, whose PPP decreases after the transition point, SAS-suppressed models either plateau ($\lambda=0.001$) or keep improving ($\lambda=0.01$) beyond that point. However, while the improvement in PPP is evident, the degradation trend is not fully suppressed or reversed for many models and corpora.

\section{Discussion}\label{sec:discussion}

This work proposed phase transition as the underlying factor that causes PPP degradation beyond a certain point during LM pretraining and conducted several studies to test this hypothesis. Here, we discuss how our results relate to previous factors that have been shown to impact PPP in LMs.

First, our phase transition hypothesis can potentially explain previous results, which have shown that limiting the context window of a transformer LM can improve PPP \cite{kuribayashi-etal-2022-context}, especially for infrequent tokens \cite{oh-etal-2024-frequency}, and that adding a linear recency bias improves PPP \cite{de-varda-marelli-2024-locally, clark-etal-2024-linear}. To explain why, recall that \citet{olsson2022context} find that induction heads improve the ICL score, as measured by how well models leverage earlier tokens in the context for the next word prediction, even across long spans of text. If the emergence of models' ICL abilities is the underlying cause of their PPP reversal, as suggested by Experiment 3, then it follows that limiting ICL would help PPP. In this study, we have achieved this through regularization; however, restricting LM's context length would yield a similar outcome by completely disabling models to leverage earlier tokens.

Second, the phase transition hypothesis can explain why larger models suffer from lower PPP than smaller models (e.g., \citealp{oh-etal-2022-gpt, oh-schuler-2023-surprisal}), an example of inverse scaling.  \citet{oh-schuler-2023-surprisal} conjecture that the overly accurate prediction of infrequent tokens is one of the contributing factors to models' PPP reversal, and suggest that it could be due to memorization. And \citet{chen2024sudden} find that a memorization phase occurs after the sudden rise in UAS (i.e. emergence of SAS). Hence, given that most of the models studied in previous works are very likely to have undergone the phase transition, it is unsurprising memorization is damaging PPP. In fact, \citet{oh-etal-2024-frequency} make an interesting observation regarding pretraining dynamics and predictions on infrequent tokens: models of different sizes seem to follow similar learning trajectories up to a certain point during pretraining (2B tokens), but the ability to predict infrequent tokens with low surprisals (which is considered one of the reasons for the larger models' poor predictive power) seems to emerge beyond that point only among larger models. If our syntactic and copying regularization successfully suppressed phase transition, it might have also limited the post-phase transition memorization capabilities, resulting in a better alignment with human reading time.

\section{Conclusion}

This study found a strong correlation between the phase transitions and peaks in a model's fit to human reading times. As to \textit{what exactly about phase transition} it is that is causing PPP degradation, our results are somewhat inconclusive. In Experiment 2, we only found partial support for the hypothesis that attention patterns in specific heads contribute to PPP (\Cref{sec:exp2:result}). \citet{chen2024sudden} present similar conclusions about the causal relationships. They find that UAS signaled the emergence of SAS, which is quickly followed by a sudden increase in BLiMP accuracy score, yet they found no significant correlation between UAS and BLiMP scores across 25 random initializations of BERT, indicating that the \emph{strength} of syntactic attention patterns associated with the phase transition do not causally explain the models' eventual syntactic abilities.

In Experiment 3 (\Cref{sec:exp3}), we found some evidence that suppressing phase transitions indeed lead to a model without PPP degradation. However, some decrease in PPP occurred even in our regularized models. One possible explanation is that, as phase transition is not yet a fully understood phenomenon, completely suppressing phase transition and testing its effect on PPP remains difficult. In other words, the lack of stronger empirical support could also be due to the incomplete suppression of phase transition. Concretely, if SAS and copying suppression was \textit{not} a  sufficient mechanism for suppressing phase transition, other behaviors associated with post-transition learning could have still occurred in our models. One such example is memorization. If such capabilities were acquired throughout pretraining \textit{despite} the suppression effort, then it is unsurprising that those specific behaviors that survived the suppression effort contributed to the degradation of PPP.

% \end{itemize}
\section{Limitations}\label{sec:limitation}

First, our models are limited to Pythia models and GPT2 models. This is due to the limited availability of pretrained models' checkpoints, as well as the high computational cost associated with training large models from scratch. As mentioned in \Cref{sec:method:models}, Pythia is the only model family whose available checkpoints cover the phase transition points.

We also limited the GPT2 models to 0-, 1-, and 2-layer variants. This is because these three variants are theoretically shown to behave qualitatively differently \cite{olsson2022context} as we discussed in \Cref{sec:exp1:result}, and that models with 2 or more layers are not qualitatively different for our purpose of attention-based phase transition detection. Pythia model's 70M, 160M, and 410M variants are 6, 12, and 24 layers, respectively, and we believe that this covers a reasonable range of model sizes, together with the GPT2 models we train. However, training larger models in \Cref{sec:exp3} were beyond our compute budget.

Second, our selection of reading time corpora is representative rather than comprehensive, and the replication with other corpora, as well as different reading behavior metrics such as brain activity data, remains an important future work. We also observe that the results for MECO were different from results from the other 3 corpora. The absence of the PPP peak $\sim$ phase transition correlation (\Cref{fig:splashy} in \Cref{sec:intro}), as well as the degradation in PPP found in copying and SAS suppressed models (\Cref{fig:reg_effect_10b} and \Cref{fig:suppression_dll_all} in \Cref{sec:exp3}) are considered an exception rather than a rule. Several issues that could affect the quality of the corpus have been reported on MECO. For example, \citet{opedal-etal-2024-role} find an off-by-one issue for a handful of tokens in MECO, as well as repeated words in a few sentences. We followed the fixed version of the data; however, there may be other issues we are unaware of, potentially causing the divergent behavior of the corpus.

Lastly, our study is limited to English, and may not hold for other languages. However, given that syntactic dependencies and word or phrase repetitions are universal across the world's languages, we predict that a similar trend might be observed in other languages. A multilingual extension of this study is therefore an important avenue for future research.

\section*{Ethics Statement}
We trained several small transformer-based LMs from scratch, which could contribute to the increased carbon footprint. However, we train models that have at most 2 layers and 8 heads. By choosing a model size that reasonably approximates the popular transformer architecture (at least for the purpose of our study) while curtailing the computational cost, we believe that we were able to minimize our environmental impact.

Lastly, while our study utilizes four sources of human behavioral data, we do not intend to redistribute or publicly share these datasets. We affirm that our use of this data aligns with ethical standards and does not pose any potential ethical concerns.

\section*{Acknowledgments}


\bibliography{custom, anthology}

\appendix

\section{Regression Model Formulae for \dll Calculation}\label{appendix:ppp}
Following \citet{SMITH2013302}, we model the spillover effect of previous 2 and 4 words for modeling eye-tracking data (Dundee, MECO, Provo) and self-paced reading data (Natural Stories), respectively. Frequencies are estimated using Wikitext.

\noindent\textbf{Baseline regression model for eye-tracking.}
\texttt{psychometric $\sim$ freq + prev\_freq + prev2\_freq + len + prev\_len + prev2\_len}

\noindent\textbf{Baseline regression model for reading time.}

\noindent\texttt{psychometric $\sim$ freq + prev\_freq + prev2\_freq + prev3\_freq + prev4\_freq + len + prev\_len + prev2\_len + prev3\_len + prev4\_len}

\noindent\textbf{Full regression model for eye-tracking.}

\noindent\texttt{psychometric $\sim$ surprisal + prev\_surp + prev2\_surp + freq + prev\_freq + prev2\_freq + len + prev\_len + prev2\_len}

\noindent\textbf{Full regression model for reading time.}

\noindent\texttt{psychometric $\sim$ surprisal + prev\_surp + prev2\_surp + prev3\_surp + prev4\_surp + freq + prev\_freq + prev2\_freq + prev3\_freq + prev4\_freq + len + prev\_len + prev2\_len + prev3\_len + prev4\_len}

\section{Unlabeled Attachment Score (UAS)}\label{appendix:uas}

We follow the calculation of UAS introduced in \citet{chen2024sudden}, which is based on \citet{clark-etal-2019-bert}. While they both use bidirectional models (i.e. BERT), we use decoder-only autoregressive models (i.e. GPT2 and Pythia), and hence make modifications to account for this difference. The overall recipe is to (1) define a head-specific probe, (2) find the best performing head for each dependency relation type, and (3) calculate the overall UAS using the best heads defined in (2).

\noindent\textbf{(1) Head-specific probe.} A head-specific probe $f_{h,l}$ predicts the parent word of a target word $x_i$ by selecting the word $x_j$, whose attention edge to or from the target word $x_i$ is the highest among all words $x_j \in \{j\ne i\}$ for a given head $h$ at layer $l$:
\begin{equation}
    f_{h,l}(x_i) = \arg \max_{x_j} \left( a^{(h,l)}_{ij}\right).
\end{equation}
%
Note that, unlike bidirectional models, where each pair of words is connected by 2 attention edges ($x_i\leftrightarrow x_j)$, only 1 edge lies between any pair of words for autoregressive models. This means that the number of words a given word $x_i$ \textit{can} attend to is $i$, whereas the average number of words $x_i$'s right context \textit{can} attend to is $\frac{i+1024}{2}$, creating discrepancy in the scale. If $i=10$, for example, with the context size of 1024, $\forall_{j<i}[\mathbb{E}(a^{(h,l)}_{i\rightarrow j})]=0.1$, whereas $\forall_{j>i}[\mathbb{E}(a^{(h,l)}_{j\rightarrow i})]\approx0.002$. However, we find that scaling the attention weights between $a^{(h,l)}_{i\rightarrow j}$ and $a^{(h,l)}_{j\rightarrow i}$ produce similar results, and hence we report unscaled results throughout.

Following \citet{clark-etal-2019-bert, chen2024sudden}, we convert the token-level attention to word-level attention by summing over attention weights \textit{to} destination tokens that make up a single word, and by averaging over attention weights \textit{from} source tokens that constitute a single word.

\noindent\textbf{(2) Best head per relation type.} The rest of the UAS calculation remains same as \citet{chen2024sudden}. We now convert the head-specific probe defined in (1) to a relation-specific probe by finding the best head for each dependency relation type.

For each dependency relation type $R$, which we define as a set of all ordered child-parent pairs $(x, y)$, the best performing head for the given dependency relation type is:
\begin{equation}
    \hat{f}_R = \arg \max_{f_{h,l}} \frac{1}{|R|} \sum_{(x, y) \in R} \mathbbm{1}_R \left( x, f_{h,l}(x) \right),
\end{equation}
where $x$ and $y$ are constrained to be within the same sentence. The indicator function for set $R$, $\mathbbm{1}_R$ is 1 if the predicted child-parent pair is in the set $R$, and 0 otherwise. Hence, $\hat{f}_R$ is simply a head that has the highest recall for a given dependency relation type $R$.

\noindent\textbf{(3) UAS.} Lastly, we simply take the average of the performance of each relation type's best head over all relation types, weighted by the number of ordered word pairs in that relation type. Denoting the set of all relation types as $\mathcal{R}$, UAS is defined as:
\begin{equation}
    \text{UAS} = \frac{1}{\sum_{R \in \mathcal{R}} |R|} \sum_{R \in \mathcal{R}} \sum_{(x_i, x_j) \in R} \mathbbm{1}_R \left( x_i, \hat{f}_R(x_i) \right).
\end{equation}

\section{In-Context Learning (ICL) Score}

\citet{olsson2022context} define what they call \textbf{ICL score}, or the difference in the losses of tokens later in the context and tokens earlier in the context. This score seems to be robust to the choice of exactly which tokens to compare; however, \citet{olsson2022context} report the difference between 50th and 500th tokens' losses. We instead report the difference between the average loss of 40th to 60th tokens and that of 450th to 550th tokens across $N$ samples:
%
\begin{equation}\label{eq:icl-score}
\frac{
\sum_{i=1}^{N}
    \sum_{j=40}^{60} \sum_{k=450}^{550} \mathcal{L}(f_\theta(s_i, w_{j})) - \mathcal{L}(f_\theta(s_i, w_{k}))}{N\times200}
\end{equation}
where $f_\theta(s_i, w_{j})$ is the output of an LM parametrized by $\theta$,  given $j$-th word $w_j$ in $i$-th sequence $s_i$.
%
A positive ICL score means that the model has a lower loss (i.e. better prediction) later in the context than earlier in the context.

\section{Phase Transition and its Downstream Effects}\label{appendix:pt-details}

\begin{figure}[t]
    \hspace{-30pt}
    \input{latex/figures/UAS_BLiMP.pgf}
%    \vspace{-18pt}
    \vspace{-20pt}
    \caption{Trajectories of BLiMP score over 10B tokens of pretraining. $\blacktriangle$ represent phase transition defined by a sudden rise UAS.}
    \label{fig:uas-blimp}
\end{figure}

\citet{chen2024sudden} find that the increase in UAS triggers the acquisition of syntactic abilities, as shown in an increase in the BLiMP score closely following the UAS boost, and we find a similar pattern. In \Cref{fig:uas-blimp}, GPT2 seems to go through a drastic increase in the BLiMP score between 4M to 1B pretraining tokens, with a brief halt around 32M-64M pretraining tokens. This is when the phase transition occurs, and it seems to signal the onset of the second boost in BLiMP score, starting around 64M tokens. For Pythia, the picture seems even clearer: UAS phase transition happens at around 512M pretraining tokens, immediately followed by a sudden increase in the BLiMP score.

Similarly, \citet{olsson2022context} reports a dramatic improvement in ICL score, foreshadowed by the emergence of induction heads, measured by PMS. In \Cref{fig:pms-icl}, ICL scores of 2-layer GPT2 and 1-layer GPT2 models seem to start diverging between 32M-64M pretraining tokens, which is exactly when the 2-layer model is undergoing the phase transition. Recall that induction requires attention composition, which is only possible with models with 2 or more layers. For Pythia models, the increase in ICL score is most dramatic between 1B and 2B pretraining tokens, which also coincide with the emergence of induction heads.

\section{Ablation}\label{appendix:ablation}

Ablation of attention heads can be implemented in two ways: full ablation and pattern-preserving ablation. For full ablation, the attention output is simply set to 0. Recall that attention output is a matrix multiplication between the attention weight vector, which is computed using query and key vectors, and the value vector \cite{vaswani-etal-2017-attention}:
\begin{equation}\label{eq:attention}
    \mathrm{Attn}(Q, K, V)=\mathrm{softmax}(\frac{QK^T}{\sqrt{d_k}})V,
\end{equation}
where Q, K, and V are query, key, and value vectors, respectively, and $d_k$ represents the dimension of Q and K vectors.
Because Q, K, and V vectors are linear projections of the previous layer's output, setting \cref{eq:attention} to a 0 vector affects all downstream (later layers') calculations of Q, K, and V vectors.
%
\begin{figure}[t]
    \hspace{-15pt}
    \input{latex/figures/PMS_ICL.pgf}
%    \vspace{-18pt}
    \vspace{-20pt}
    \caption{Trajectories of ICL score over 10B tokens of pretraining. \ding{72} represent phase transition defined by a sudden rise PMS.}
    \label{fig:pms-icl}
\end{figure}
%
\begin{figure*}[t]
    \hspace{-30pt}
    \input{latex/figures/coinciding.pgf}
%    \vspace{-18pt}
    \vspace{-20pt}
    \caption{Trajectories of $\Delta$LL over 10B tokens of pretraining. $\blacktriangle$ and \ding{72} represent phase transition defined by a sudden rise in UAS and PMS, respectively.}
    \label{fig:coinciding}
\end{figure*}
%
In pattern-preserving ablation, we feed the input to the model twice: during the first run, no heads are ablated, and we simply record all the attention weights at each head of each layer. During the second run, we ablate the head(s) of interest, but use the attention weights recorded in the first run, preserving the original attention weights (hence \textit{pattern-preserving}). Because only $Q$ and $K$ vectors are involved in the attention weight calculation $\mathrm{softmax}(\frac{QK^T}{\sqrt{d_k}})$, pattern-preserving ablation only affects downstream $V$ calculations, but not $QK$ calculations. We note that these two implementations of ablation yield similar results, and we report the results obtained from pattern-preserving ablation.

\section{Full PPP Trajectory and Phase Transition Points}\label{appendix:full-results}

\Cref{fig:coinciding} shows a fuller visualization of the entire trajectory of $\Delta$LL, with the phase transition points marked for each model ($\blacktriangle$ and \ding{72} for UAS and PMS, respectively, which correspond to the x-axis of \cref{fig:splashy}). Note that, in \Cref{fig:coinciding}, we included 0 and 1 layer GPT2 models as they are expected to behave differently from 2 layer models (i.e. 0 layer model does not have attention layers, and 1 layer model cannot form induction heads, as they require attention composition \citealp{olsson2022context}), and we limited the 2 layer models to the one trained with a batch size of 4 for readability. 

First, GPT2 with no attention layer (\textcolor{violet}{dark purple line}) exhibits no ``tipping point'' in PPP, meaning that it never seems to undergo a point at which PPP starts going down. This is expected as the definitions of phase transition we adopt in this paper are both triggered by specialized behaviors of attention heads, and this further corroborates our hypothesis. Conversely, for GPT2 models with 1 and 2 layers, as well as 3 different sizes of Pythia models all have a tipping point at which PPP starts going down. Note that the results for Pythia models are partial replications of \citet{oh-schuler-2023-transformer}.

Second, as shown in \Cref{fig:splashy} as well, the tipping points of PPP seem to closely match the breakthrough points in ICL scores. Note that these breakthrough points are meaningful for models with 2 or more layers because induction heads work with another head in lower layers to perform the copying behavior (cf. attention composition; \citealp{olsson2022context}); hence, we do not indicate those breakthrough points for 0 and 1 layer models.% For Dundee, MECO, and Provo, the tipping point of PPP occurs at the same time with, or very close to, the breakthrough points in ICL scores.
\section{Implementation of Copying Regularization}\label{appendix:ih-reg}

Since the distance between two \{AB\} sequences, $l$, can be arbitrarily large within the LM's context size, we sample $l$ from a uniform distribution $U(50, 512)$. The lower bound is to make sure non-copying behaviors do not get suppressed, and the upper bound is just half the max context length of our model. To maximize the suppression target, we repeat a random sequence of length $l$ as many times as it takes to fill the context size of 1024. Since it has to be repeated at least twice to promote the copying behavior, the $l$ is upper-bounded by 512. With this synthetic data, $PM(x_i)$ can be written as:
\begin{equation}
% \text{PM}(x_i)=
\{x_j|j<i, x_{j}=x_{i+1}, \mathbf{x}_{i-l+1}^i=\mathbf{x}_{j-l}^{j-1}\}
\end{equation}
or programmatically:
\begin{equation}
% \text{PM}(x_i)=
\{x_{i-nl+1}|n\in\mathbb{N}_1, x-nl>0\}
\end{equation}

Now, as opposed to SAS suppression, where the regularization term is computed on the same set of examples from which normal loss $\mathcal{L}_{\textsc{clm}}$ is computed, the copying regularization term requires separate synthetic data. We considered alternating between the $\mathcal{L}_{\textsc{clm}}$ and $\mathcal{L}_{\textsc{copy}}$ every $\frac{1}{\lambda}$ steps; however, we find that this leads to a very unstable learning curve, and hence add the regularization term every step weighted by $\lambda$ as was the case with syntactic regularization.

\section{Regularization}\label{appendix:suppression}

\begin{figure}[t]
    \hspace{-10pt}
    \input{latex/figures/UAS_reg.pgf}
%    \vspace{-18pt}
    \caption{Trajectories of UAS over 10B tokens of pretraining with and without SAS suppression. All models are GPT2 with 2 layers with 8 attention heads.}
    \label{fig:uas-reg}
\end{figure}
\begin{figure}[t]
    \hspace{-10pt}
    \input{latex/figures/BLiMP_reg.pgf}
    \caption{Trajectories of BLiMP score over 10B tokens of pretraining with and without SAS suppression. All models are GPT2 with 2 layers with 8 attention heads.}
    \label{fig:blimp-reg}
\end{figure}

\Cref{fig:uas-reg} summarizes the development of UAS for the GPT2 models with and without SAS suppression. As opposed to the non-suppressed model (NoReg), whose UAS abruptly increases between 16M and 64M pretraining tokens, the SAS-suppressed model with $\lambda=0.001$ exhibits a brief increase in UAS, followed by a gradual degradation, converging to almost 0 towards the end of the pretraining. The SAS-suppressed model with stronger suppression, $\lambda=0.01$, almost never sees any improvement in UAS throughout the pretraining. We confirm that the SAS suppression is working as intended.

\Cref{fig:blimp-reg} summarizes the development of BLiMP score over the course of pretraining for the same 3 models discussed above. After 16M to 64M pretraining tokens, when the phase transition was \emph{supposed to} happen, BLiMP score perfectly correlates with the SAS suppression strength, with non-regularized model performing the best, and the strongly regularized model performing the worst.

\begin{figure}[t]
    \hspace{-10pt}
    \input{latex/figures/PMS_reg.pgf}
    \caption{Trajectories of PMS over 10B tokens of pretraining with and without copying suppression. All models are GPT2 with 2 layers with 8 attention heads.}
    \label{fig:pms-reg}
\end{figure}

\Cref{fig:pms-reg} plots the development of best PMS. Recall that PMS is a head-level score, and we show the score from the highest scoring head for each checkpoint. At 64M pretraining tokens, we see the phase transition in non-regularized model. Both copying-suppressed models ($\lambda=0.001, \lambda=0.01$) show almost no improvement in the best PMS throughout the course of pretraining.

\begin{figure}[t]
    \hspace{-10pt}
    \input{latex/figures/ICL_reg.pgf}
    \caption{Trajectories of ICL score over 10B tokens of pretraining with and without copying suppression. All models are GPT2 with 2 layers with 8 attention heads.}
    \label{fig:icl-reg}
\end{figure}

\Cref{fig:icl-reg} shows that the copying-suppressed models indeed show lower ICL scores, presumably as a result of the absence of induction heads. Notably, at around 64M pretraining tokens, when phase transition occurs in the non-regularized model, both regularized models see a large dip in the ICL score. This is perhaps because the emergent copying behavior briefly raises the penalizing term of the loss function $\mathcal{L}_{\textsc{copy}}$, resulting in a brief phase of negative ICL scores.
\end{document}