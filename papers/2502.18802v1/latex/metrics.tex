
\noindent\textbf{Prefix-matching score.} Induction heads are defined by the copying behavior, such that if the model has seen an \{A\}\{B\} sequence and the current token is \{A\}, then an induction head is a head that increases the next word's logit for the token \{B\}, completing the $\{A\}\{B\} ... \{A\}\rightarrow\{B\}$ sequence. This mechanism is analyzed by 2 distinct behaviors of a given attention head; namely, prefix matching and logit attribution. Prefix matching refers to the extent to which a given attention head is attending to the previous occurrence of the next word (i.e. the first $\{B\}$ followed by $\{A\}$). Logit attribution refers to the amount by which a given attention head increases the next token's logit for the token $\{B\}$.

Given a random sequence of tokens $\mathbf{x}$ repeated twice, prefix-matching score for a given head $h$ is defined as following:
\begin{equation}
\frac{1}{|\mathbf{x}|-1}\sum_{i=|\mathbf{x}|+1}^{2|\mathbf{x}|}\mathrm{Attn_h}(w_i, w_{i-(|\mathbf{x}|-1)})
\end{equation}
In our experiments, we use a random sequence of length 50, following the implementation in the \texttt{TransformerLens} library \cite{nanda2022transformerlens}\footnote{\url{https://github.com/TransformerLensOrg/TransformerLens}} , so $|\mathbf{x}|=50$. Note that $|\mathbf{x}|$ is a completely random sequence of token IDs; hence, attention patterns are not an artifact of data contamination or memorization.

\noindent\textbf{Previous-token score.} As briefly mentioned earlier, previous-token head plays an important role as the first head of the two head attention composition, second of which is called induction head. It is defined similarly to prefix-matching score, except that the target token is the previous token, as opposed to a token $|\mathbf{x}|-1$ tokens back:
\begin{equation}
\frac{1}{|\mathbf{x}|-1}\sum_{i=|\mathbf{x}|+1}^{2|\mathbf{x}|}\mathrm{Attn_h}(w_i, w_{i-1})
\end{equation}
We could start $i$ from 2; however, this will superficially inflate the previous-token score, because the second token can only attend to the first and/or the second token. By starting from a large-enough index ($|\mathbf{x}|+1=51$), the random baseline will be $<.02$, making the identification of previous token heads easier.

\begin{figure}[t]
    \hspace{-15pt}
    \input{figures/bump.pgf}
    \vspace{-20pt}
    \caption{UAS and ICL scores for various LMs (0 layer model is excluded since UAS cannot be computed for models without attention heads).}
    \label{fig:bump}
\end{figure}

\begin{figure*}[t]
    \hspace{-30pt}
    \input{figures/coinciding.pgf}
    \vspace{-20pt}
    \caption{Relationship between PMS and the effect on $\Delta$LL. Each point represents a head (Pythia-70M has 6 layers with 8 heads at each layer, hence 48 heads).}
    \label{fig:ppp_and_icl}
\end{figure*}

\textbf{In-context learning score.}
\citet{olsson2022context} define in-context learning score as the difference between the loss of the 50th and 500th tokens in a given sequence:
\begin{equation}    
    \frac{1}{N}\sum_{i=1}^{N}\Loss(f_\theta(s_i, w_{500})) - \Loss(f_\theta(s_i, w_{50})).
    \end{equation}
The two token indices are chosen somewhat arbitrarily, and they indeed show that as long as the indices are ``far apart enough'', the score will capture the observation that the loss is lower for later tokens than for earlier tokens. Hence, we use our own version of in-context learning score:

\begin{equation}
\frac{
\sum_{i=1}^{N}
    \sum_{j=e_1}^{e_2} \sum_{k=l_1}^{l_2} \Loss(f_\theta(s_i, w_{j})) - \Loss(f_\theta(s_i, w_{k}))}{N\times(e_2-e_1)\times(l_2-l_1)}
\end{equation}
where e1 and e2, and l1 and l2 represents a range of \textbf{e}arly tokens and \textbf{l}ater tokens, respectively.
Notice that we made two changes: (1) we flipped the equation (loss of the \textit{earlier} token is subtracted from that of the \textit{later} token) so that ICL score is more intuitively interpretable, where a higher score indicates more in-context learning; and (2) instead of pinpointing two tokens for each example, we give it a range. For (2), besides computational efficiency (we now need $(e2-e1)\times(l2-l1)$ times smaller validation set to obtain the same number of differences to calculate the ICL score from), this is motivated by the fact that changing the target indices did not affect the observation about the induction heads.

\noindent\textbf{Surprisal.}

\begin{equation}\label{eq:surp}
    S_{w_i} = -\log P(w_i \mid \bm{w_{<i}}).
\end{equation}

\noindent\textbf{Psychometric predictive power.} 