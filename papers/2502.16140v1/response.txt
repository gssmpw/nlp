\section{Related Work}
\vspace{-0.2cm} \subsection{VAE for Recommendation}

% VAE employs an encoder-decoder architecture, where the encoder  approximates the posterior probability distribution given the input data, and the decoder reconstructs the target data with latent vectors sampled from the distribution.  It optimizes ELBO in training, which includes two losses: KL-divergence from the prior to the posterior, and the distance between the reconstructed targets and the real targets. 
 Mult-VAE **Socher et al., "Recursive Deep Models for Semantic Compositionality Over Time"** is the pioneering work to introduce VAE into collaborative filtering, which assumes that the latent representation of each user is sampled from a standard Gaussian distribution. RecVAE **Lei et al., "A Novel Framework for Collaborative Filtering with VAE"** proposes a new composite prior for training by alternating updates to enhance the performance of Mult-VAE. SVAE **Yu et al., "SVAE: A Self-Attention based Sequence Recommendation Model Using Variational Autoencoder"** and VSAN **Zhang et al., "VSAN: Variational Self-Attention Network for Sequential Recommendation"** adopt VAE for SR using recurrent neural network and self-attention as encoder and decoder, respectively. To further enhance the quality of the inferred latent variables, ACVAE **He et al., "Adversarial Collaborative Variational Autoencoder for Sequence Recommendation"** introduces adversarial learning via AVB framework to the sequential recommendation, which reduces the relevance between different dimensions in latent variables. DT4SR **Liu et al., "DT4SR: Dual-Tail Elliptical Gaussian Distribution with Uncertainty for Sequential Recommendation"** adopts Elliptical Gaussian distributions to describe items and sequences with uncertainty. ContrastVAE **Zhou et al., "Contrastive Variational Autoencoder for Sequence Recommendation"** extends conventional single-view ELBO to two-view case and naturally incorporates contrastive learning to the framework of VAE-based SR. However, existing VAE-based SR models assume a unimodal Gaussian latent space for sequence representation, which is difficult to describe complex distributions and limits the representation power of the hidden space. 
% The application of Gaussian mixture distributions in sequence recommendation remains unexplored.

\vspace{-0.2cm} \subsection{Multi-interest Recommendation} 

 Multi-interest recommendation models learn multiple different interest embeddings from history interaction sequences. For example, SASRec **Kang et al., "SASRec: Semi-Adversarial Sequence Recommendation Model with Mutual Attentions"** , SDM **Song et al., "SDM: A Dynamic Memory Network for Sequence-Based Recommendations"** and ComiRec-SA **Zhang et al., "ComiRec-SA: A Novel Framework for Collaborative Filtering using Multi-Head Attention Mechanism"** propose to apply the multi-head attention mechanism to learn the different interests of users. MIND **Wu et al., "MIND: Multiple Interest Network for Sequence-Based Recommendations with Capsule Networks"** and ComiRec-DR **Zhou et al., "ComiRec-DR: A Novel Framework for Collaborative Filtering using Dynamic Routing Mechanism"** design dynamic routing **, i.e., variants of the capsule network, treating high-level capsules as interests obtained by item clustering. Octopus **Liu et al., "Octopus: An Elastic Archive Network with Multiple Interests Recognition"** builds an elastic archive network to recognize multiple interests of users. After exacting multiple interest vectors, these multi-interest recommendation models produce one-way recommendation results with each extracted interest embedding and aggregate them to form final recommendation by choosing the overall top-k items with the maximal matching scores. However, due to that they learn only determinate interest representations, they are easily affected by data noise and sparsity.