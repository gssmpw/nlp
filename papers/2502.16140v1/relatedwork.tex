\section{Related Work}
\vspace{-0.2cm} \subsection{VAE for Recommendation}

% VAE employs an encoder-decoder architecture, where the encoder  approximates the posterior probability distribution given the input data, and the decoder reconstructs the target data with latent vectors sampled from the distribution.  It optimizes ELBO in training, which includes two losses: KL-divergence from the prior to the posterior, and the distance between the reconstructed targets and the real targets. 
 Mult-VAE \cite{MultiVAE} is the pioneering work to introduce VAE into collaborative filtering, which assumes that the latent representation of each user is sampled from a standard Gaussian distribution. RecVAE \cite{RecVAE} proposes a new composite prior for training by alternating updates to enhance the performance of Mult-VAE. SVAE \cite{SVAE} and VSAN \cite{VSAN} adopt VAE for SR using recurrent neural network and self-attention as encoder and decoder, respectively. To further enhance the quality of the inferred latent variables, ACVAE \cite{ACVAE} introduces adversarial learning via AVB framework to the sequential recommendation, which reduces the relevance between different dimensions in latent variables. DT4SR \cite{DT4SR} adopts Elliptical Gaussian distributions to describe items and sequences with uncertainty. ContrastVAE \cite{ContrastVAE} extends conventional single-view ELBO to two-view case and naturally incorporates contrastive learning to the framework of VAE-based SR. However, existing VAE-based SR models assume a unimodal Gaussian latent space for sequence representation, which is difficult to describe complex distributions and limits the representation power of the hidden space. 
% The application of Gaussian mixture distributions in sequence recommendation remains unexplored.

\vspace{-0.2cm} \subsection{Multi-interest Recommendation} 

 Multi-interest recommendation models learn multiple different interest embeddings from history interaction sequences. For example, SASRec \cite{SASRec}, SDM \cite{lv_sdm_2019} and ComiRec-SA \cite{cen_controllable_2020} propose to apply the multi-head attention mechanism to learn the different interests of users. MIND \cite{MIND} and ComiRec-DR \cite{cen_controllable_2020} design dynamic routing \cite{sabour2017dynamic}, i.e., variants of the capsule network, treating high-level capsules as interests obtained by item clustering. Octopus \cite{liu_octopus_2020} builds an elastic archive network to recognize multiple interests of users. After exacting multiple interest vectors, these multi-interest recommendation models produce one-way recommendation results with each extracted interest embedding and aggregate them to form final recommendation by choosing the overall top-k items with the maximal matching scores. However, due to that they learn only determinate interest representations, they are easily affected by data noise and sparsity.