\section{Basic Prompts}
\label{sec:basic-prompt}

In this section, we explore the simplest interface to LLMs, namely \textit{prompting}, for query rewriting.
%
We evaluate four basic prompts, enumerated in Figure~\ref{fig:baseline-prompts}, which cover a progressive range of instructional detail and test the effectiveness of the LLM's base knowledge. 

\begin{figure}[t]
    \centering
    \includegraphics[width=0.45\linewidth]{Figures/baseline-prompts.png}
    \vspace{-0.1cm}
    \caption{Templates used for Basic Prompts}
    \label{fig:baseline-prompts}
    \vspace{-0.2cm}
\end{figure}

\myparagraph{Prompt~1:} 
This is the baseline prompt used in \cite{Genrewrite}, which simply asks the LLM to rewrite a given query to improve performance. 

\myparagraph{Prompt~2:}
Explicit instructions are included to maintain semantic and functional equivalence while rewriting. 

\myparagraph{Prompt~3:}
Verbose instructions are given to rewrite the query, providing step-by-step guidance to the LLM to think rationally. It is first asked to pick out potential inefficiencies in the input query, and then tasked to identify approaches to address these inefficiencies. Finally, it is instructed to apply the identified solution.  Essentially, the prompt tries to make the LLM reason akin to human experts.

\myparagraph{Prompt~4:}
The sequence of instructions in Prompt~3 is split into sub-prompts, and provided to the LLM in an \emph{iterative} manner instead of all at once. The idea is to break down the complex instructions given in Prompt~3 into digestible steps that help the LLM focus on individual tasks.

\subsubsection*{Performance}
The performance of the four prompt templates on the micro-benchmark is shown in Table~\ref{tab:basic-prompt-exp}. 
We find that less than half the rewrites are productive with individual prompts. However, a drill-down shows that the best prompt in the ensemble is \emph{query-specific} -- this opens up the possibility of using all four prompts in parallel, and then choosing the best among them. This ensemble approach  raises the \cprs to \textbf{\EnsembleRewriteMicroDS} (Row 5 in Table~\ref{tab:basic-prompt-exp}) -- however, there remain four queries that are not productively rewritten by these prompts. 

\begin{table}[h]
\footnotesize
\centering
\caption{Performance of Basic Prompts.}
\label{tab:basic-prompt-exp}
\vspace{-0.1cm}
\begin{tabular}{|c|c|c|}
\hline
\textbf{Prompt} & \textbf{\# \cpr} & \textbf{\csgm} \\ \hline \hline
Prompt 1 & 3 & 1.99 \\ \hline
Prompt 2 & 2 & 1.85 \\ \hline
Prompt 3 & 4 & 2.83\\ \hline
Prompt 4 & 4 & 3.00 \\ \hline \hline
\textbf{Prompt Ensemble}  & \EnsembleRewriteMicroDS & \gmEnsembleMicroDS \\ \hline \hline
\textbf{SOTA Ensemble} & \SotaRewriteMicroDS & 
\gmSotaMicroDS  \\
\hline
\end{tabular}
\end{table}

The \csgm, shown in the last column of  Table~\ref{tab:basic-prompt-exp}, is at most {\bf 3} for the individual prompts, while the ensemble reaches {\bf \gmEnsembleMicroDS}. But these speedups, although productive, are all lower than those delivered by the human rewrites.

Finally, an ensemble of \sota techniques (described in Section~\ref{sec:exp}) was also processed on the same platform. They delivered  \SotaRewriteMicroDS \cprs with a \csgm of \gmSotaMicroDS (last row in Table~\ref{tab:basic-prompt-exp}), 
indicating the wide gap between the current reality and what is humanly possible.


