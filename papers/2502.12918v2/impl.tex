\section{Implementation Choices}
\label{sec:impl}

In this section, we briefly discuss the design choices made in our implementation of \lithe.

\vspace{-0.1in}
\subsection{\lithe Parameter Settings}
\label{sec:llm-params}

The \emph{``temperature''} parameter of \gpt, which ranges over [0,1], controls the randomness of the model's response.
While a higher temperature can be useful for creative writing where one would seek diverse and exploratory answers, in our case we want a focused and deterministic answer as far as possible. Hence we set the temperature to 0 which forces the model to greedily sample the next token.


The hyperparameters used by \lithe for MCTS are as follows: The maximum number of iterations $iter_{max}$ is set to 8,  expansion threshold $\theta$ is 0.7, and number of expansions $k$ is 2.
The values of $c_{base}$ and $c$ were set to 10 and 4, respectively.
%
These settings were determined after an empirical evaluation of the various trade-offs, providing a robust balance between efficiency and quality.
%

Finally, we try a maximum of 5 times to fix, via prompt corrections, any rewrite that exhibits syntax errors (Section~\ref{sec:lithe-architecture}).

\vspace{-0.1in}
\subsection{Query Equivalence Testing}
\label{sec:sql-equivalence}
We use a multi-stage approach, described below, to test semantic equivalence between the original query and a candidate rewrite.

\myparagraph{1. Logic-based Equivalence.}
Although verifying the equivalence of a general pair of SQL queries is NP-complete~\cite{queryequivalence}, a variety of logic-based tools (e.g. Cosette\cite{Cosette}, SQL-Solver~\cite{SQLSolver}, VeriEQL~\cite{verieql}, QED~\cite{QED}) are available for proving equivalence over restricted classes of queries, as mentioned in the Introduction. 
%
In \lithe, we use the recently proposed QED~\cite{QED} since it was found to cover a larger set of queries compared to the alternatives. 
%
The advantage of such a logic-based approach is that it is definitive in outcome and relatively inexpensive. 

\myparagraph{2. Result Equivalence via Sampling.}
%
If the original query is not within the QED scope, we alternatively use a sampling-based approach to test equivalence. The idea here is to execute the queries on several small samples of the database and verify equivalence based on the sample results. 
%
However, while this test is a necessary condition for query equivalence, it is not sufficient. That is,  false positives may be present because the sampled database may not cover all the predicates featured in the query. To minimize this likelihood, we use a combination of (1) \textit{correlated sampling}~\cite{cs2} for maintaining join integrity in the sample, (2) adding synthetic tuples in the sample to distinguish outer and inner joins, and (3) adjusting constants in the filter predicates to produce populated results -- the complete details are in the Section~\ref{app:sampling-eq}. 

\myparagraph{3. Result Equivalence on the Entire Database.}
%
Result equivalence could also be evaluated, in principle, on the entire database itself. Of course, this could turn out to be prohibitively expensive, especially if the queries themselves are time-consuming (e.g. due to the scale of the underlying database) and/or if the candidate rewrites happen to be regressions. Therefore, we leave this check as an optional choice for the DBA.
