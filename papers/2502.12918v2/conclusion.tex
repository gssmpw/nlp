\section{Conclusions and Future Work}
\label{sec:Conclusion}

We investigated how the latent power of LLM technologies can be productively materialized in the context of SQL-to-SQL rewriting.  Our study progressively infused database domain knowledge, such as redundancy removal rules and schematic+statistical metadata, into the LLM prompts. Further, the output telemetry of LLMs, in the form of token probabilities, was used to signal situations where the LLM lacked confidence, triggering exploration of a larger search space. Finally, a combination of logic-based and statistical tests was employed to verify the equivalence of the rewrites.


An empirical evaluation over common database benchmarks showed that rewriting is a potent mechanism to improve query performance. In fact, even order-of-magnitude speedups were routinely achieved with regard to both abstract costing and execution times.
However, our results also showed a significant semantic distance between foundation models and query optimizers, with regard to both scope and precision, which would have to be bridged to fully leverage the latent power of LLMs. Further, our focus here was primarily on prompting-based strategies -- in our future research, we plan to investigate how domain-specific \emph{fine-tuning} could be leveraged to provide \gpt-like rewrites on small open models.
