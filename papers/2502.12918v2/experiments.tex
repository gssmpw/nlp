\vspace{-0.1in}
\section{Experimental Evaluation}
\label{sec:exp}

In this section, we report on \lithe's performance profile. We first describe the experimental setup, including comparative baselines, query suites and evaluation platforms. Then we present the speedup results for both aggregate benchmark and individual queries,  followed by characterization of the rewrite overheads in computational and financial terms. We finally discuss the impact of alternative platforms wrt database engine, database schema and LLMs.

\myparagraph{Rewrite Baselines.}
%
We compare \lithe with a collection of contemporary rewrite techniques, collectively referred to as {\sota} -- the details of these techniques are provided in Section~\ref{sec:Related-Work}. Specifically, the \sota collection consists of the following approaches:
\vspace{-0.1in}
\begin{enumerate}\denselist
\item Baseline LLM prompt~\cite{Genrewrite}: This is Prompt~1 from Section~\ref{sec:basic-prompt}.
\item Learned Rewrite~\cite{Learned_Rewrite}, a purely rule-based rewriter.
\item \llmrsq \cite{LLMR2}, an LLM-guided rule-based rewriter.
\item GenRewrite~\cite{Genrewrite}, a purely LLM-based rewriter.
\end{enumerate}
Given an input query, each of the \sota approaches is independently invoked to perform a rewrite, and the rewrite with the \emph{best} performance is used as the baseline for comparison. 
%
Note that these approaches may occasionally generate rewrites that are expected by the optimizer's costing module to regress the performance. For safety, we immediately discard such rewrites, similar to \lithe.
%

\myparagraph{Query Set.}
%
Our evaluation in this paper primarily focuses on complex analytical queries from the standard TPC-DS decision-support benchmark~\cite{tpcds}, which models a retail supplier environment. The benchmark is used at its default size of 100 GB.
As mentioned earlier, we focus on ``slow queries'' that take over 10 seconds to execute in their original form, operating in a cold-cache environment.

\lithe has also been evaluated on other benchmarks, including DSB~\cite{DSB}, ARCHER~\cite{ARCHER}, JOB~\cite{JOB} and StackOverflow~\cite{Stackoverflow}. Due to space limitations, we defer their details to Section~\ref{app:exp}.

\myparagraph{Testbed.}
%
The majority of our experiments were carried out on the following data processing platform: Sandbox server with Intel(R) Xeon(R) CPU E5-1660 v4 @ 3.20GHz x 16, 32 GB RAM, and 12TB HDD, running Ubuntu 22.04 LTS;  PostgreSQL~v16 database engine; and \gpt LLM for both \lithe and \sota. Variations on this platform are considered in Sections~\ref{sec:commercial-dbms} and ~\ref{sec:altplatform}.

\myparagraph{Metrics.}
For each rewrite technique, we identified the number of queries for which a \cpr (cost productive rewrite with > 1.5 speedup) could be constructed.  Subsequently, we computed the \csgm (Cost Speedup Geometric Mean) and \tsgm (Time Speedup Geometric Mean) performance 
obtained by each technique over the set of all {\cpr}s (i.e. \cprs arising from either \lithe or \sota).

From the investment perspective, we measured the average rewrite time per query, and additionally for the LLM-based techniques, the number of tokens used in the rewrite process.

\subsection{Rewrite Quality (Cost and Time)}
\label{sec:rewrite-perf}

\subsubsection{Estimated Cost}
\label{sec:rewrite-cost}
%

\lithe produces a rewrite with a positive speedup ($>$ 1x) for 46 of the 88 TPC-DS queries deemed to be slow by our threshold. Of these 46, there were \textbf{26} \cprs resulting in a \emph{highly productive} \csgm of \textbf{11.5}. On the other hand, \sota delivers only \textbf{13} \cprs (out of 42 positive rewrites) with a \csgm of \textbf{6.1}. All but one of the \sota \cprs also feature in the \lithe \cprs, making the total number of \cprs considered to be 27. 


We were able to prove the rewriting correctness of 9 \cprs through the logic-based QED tool. The remaining passed our result equivalence tests on both the down-sampled and full databases.
%
Furthermore, as a final check, we manually verified the correctness of the rewritten queries.

A drill-down into the cost speedup performance at the granularity of individual queries is shown in Figure~\ref{fig:query-speedup}, which compares \lithe (orange bars) and \sota (blue bars) on each of the 27 \cpr queries  -- note that the cost speedups on the $x$-axis are tabulated on a $\mathbf{log_{10}}$ scale, and the queries are sequenced in decreasing order of \lithe speedup. The vertical dotted line at $1$ represents the normalized baseline cost of the original query with the native optimizer, while the vertical line at $1.5$ is the \cpr threshold.

We first observe, gratifyingly, that rewrites are indeed capable of promising dramatic cost speedups -- take, for instance, Q41, which improves by a whopping \emph{five orders-of-magnitude} for both \sota and \lithe. This improvement in query performance is due to replacing the ``\texttt{WHERE (SELECT COUNT(*) from ...) > 0}" clause with ``\texttt{WHERE EXIST (SELECT 1 from ...)}" -- the latter is a more efficient check for result existence in an inner subquery since it removes the computationally expensive aggregation function.
%

\begin{figure}[t]
  \begin{minipage}[t]{.49\textwidth}
    \includegraphics[width=\textwidth]{Figures/cost_vertical_comparison_plot.png}
    \caption{Plan Cost Speedups}
    \label{fig:query-speedup}
  \end{minipage}
  \hfill
  \begin{minipage}[t]{.49\textwidth}
    \includegraphics[width=\textwidth]{Figures/execution_horizontal_comparison_plot.png}
    \caption{Execution Time Speedups}
    \label{fig:execution-time}
  \end{minipage}
\end{figure}

Second, in most queries, the cost speedup of \lithe either exceeds or matches \sota. In fact, there are several queries (e.g. Q45, Q25, Q4) where \lithe produces a highly beneficial \cpr but \sota simply returns the original query. 
Conversely, the opposite is true for one query (Q57) where \sota projects a large speedup but \lithe settles for the original query.  And in Q88 and Q95, \sota is marginally better than \lithe.

At this stage, one could imagine that more rules could be added to \lithe to bring it on par with \sota for queries like Q57. However, as discussed later in Section~\ref{sec:cost-mismatch},  
such customized rules, while attractive at compile-time, run a significant risk of causing regressions at run-time.  We have therefore selected only broad-brush rules for inclusion in \lithe. This conservative approach is justified below in the timing section, where we find that \lithe actually performed better than \sota on these queries (Q57, Q88, Q95) wrt runtimes.



\subsubsection{Execution Time}
\label{sec:execution-times}
%
Thus far, we had considered optimizer-estimated execution costs. We now move to wall-clock runtimes for query executions -- Figure~\ref{fig:execution-time} shows the runtime speedups (on a $\mathbf{log_{10}}$ scale) obtained by \lithe and \sota. 
%
We observe, again gratifyingly, that there are indeed several queries where substantial time benefits are achieved by the rewrites, even exceeding \emph{order-of-magnitude} benefits in some cases -- for instance, \lithe improves Q45 by a huge factor of 10000!
Second, in almost all cases, \lithe outperforms or matches \sota, including as mentioned above, the queries where \sota's optimizer costs were better. There remain only a few instances (Q9, Q47) where \sota is marginally better.


From a modeling perspective, we see that the well-documented gap between optimizer predictions and actual run-times is prevalent in the rewrite space as well. On the one hand, there is Q45 where the projected speedup of $10^2$ increases to a huge $10^4$ at runtime. On the other hand, the $10^5$ speedup for Q41 decreases to $10^3$.  But for \sota, the reductions can be severe -- a striking case in point is Q57, where \sota actually causes \emph{regression} despite a speedup projection of close to 100. 
%
However, the good news is that with \lithe, although the runtime speedups did not always match the projections, we did not encounter any regressions among the \cpr rewrites.
Overall, \lithe produces more robust rewrites resulting in a high \tsgm of \textbf{18.4}, whereas \sota provides a \tsgm of \textbf{6}.


\subsubsection{Reasoning}
%
As a confirmatory exercise, we compared the LLM-generated explanation output by \lithe with our own manual analysis of the query plans generated for the original and rewritten queries. 
%
For example, consider Q45 
-- the provided explanation correctly lists the main reasons for the huge $10^4$ speedup as ``\textit{the use of CTEs}", ``\textit{Reduction in data volume early on}" through pre-filtering based on join predicates, etc.
%
Overall, we found that the explanations provided by \lithe matched our manual analysis of the plans, indicating that model-based reasoning is well aligned with human-backed reasoning in these scenarios.

\subsection{Ablation Analysis of Rewrite Quality}
\label{sec:ablation}

\subsubsection{Components of  {\lithe}}
A natural question at this stage is the role of the various techniques in \lithe towards achieving its large performance benefits. This analysis is captured in Table~\ref{tab:lithe-contribution} which lists the {\cpr}s for each technique when invoked in \emph{isolation} as well as the \emph{cumulative} number of {\cpr}s when the different techniques are combined in the order of listing.

\begin{table}[h]
\footnotesize
\centering
\caption{{\cpr}s contributed by \lithe components.}
\label{tab:lithe-contribution}
\begin{tabular}{|l|c|c|} \hline
 & \multicolumn{2}{c|}{\textbf{\# CPR}} \\
 & \textbf{Isolated} & \textbf{Cumulative}  \\ \hline \hline
Basic Prompts (Section~\ref{sec:basic-prompt}) & 15  & 15  \\ \hline
Rules R1 --- R4 (Section~\ref{sec:rules-prompt}) & 10 & 18  \\ \hline 
Rules R5, R6 (Section~\ref{sec:stats-prompt})    & 11 & 25  \\ \hline %\hline
MCTS (Section~\ref{sec:mtcs-rewrite})            & 26 & 26 \\ \hline 
\end{tabular}
\end{table}
%

We observe that the Basic Prompt ensemble and Redundancy Rules (R1 -- R4) contribute to around two-thirds of the {\cpr}s (18/26). When the statistics-infused Rules (R5, R6) are added, this number jumps to 25. Finally, MCTS produces an additional \cpr and also improves the cost speedup of a pre-existing \cpr.
%

\subsubsection{Database-Sensitive Prompts.}
%
Since MCTS explores the search space at a fine granularity, one could ask whether just the Basic Prompts in conjunction with MCTS would suffice to provide good performance. The motivation is that it would relieve us from using the database-sensitive rules 
that incur significant computational and financial overheads. When this experiment was conducted, the \csgm dropped precipitously to a paltry \textbf{5}, a far cry from the \gmLitheAllDS obtained with the database-sensitive rules. These results highlight the need to reflect database awareness for effective query rewriting, and not rely solely on prior LLM knowledge.

\subsection{Rewrite Overheads (Time/Money)}
\label{sec:overheads}
Having established the performance benefits of rewrites, we now turn our attention to their time and financial overheads.
%

\subsubsection{Transformation Time}
The average processing time per \cpr query is shown in Table~\ref{tab:different_benchmark_cost}, and we see that they do take a few minutes. However, this investment may be acceptable in deployment given that the execution benefits typically far outweigh the compilation overheads. 
%
For instance, with Q11, the original query took nearly \emph{two hours} to complete, whereas the \lithe rewrite executed in just 3~\emph{minutes}.
Further, many applications tend to use a set of canned queries which are run thousands of times. Thus, even a large one-time investment can be easily recovered over repeat executions of such queries. 

\begin{table}[h]
\footnotesize
\centering
\caption{Rewrite Overheads of \lithe and \sota.}
\label{tab:different_benchmark_cost}
\begin{tabular}{|c|c|c|c|}
\hline
& \textbf{Avg. Time (min)} & \textbf{Avg. Tokens } & \textbf{Avg. Cost (USD) } \\ \hline \hline
LITHE       & 5  & 18427 & 0.045  \\ \hline
SOTA        & 1.7 & 20076 & 0.050 \\ \hline
\end{tabular}
\end{table}

Notwithstanding the above, we also observe that \lithe is considerably slower than \sota in producing rewrites. A drill-down 
shows that the lion's share of the time is taken by the initial prompt ensemble and the final MCTS module. We explore techniques to improve their efficiency in the Section~\ref{app:classifier}. 

\subsubsection{Monetary Outlay}
The average number of LLM tokens required by \lithe and \sota, and their associated financial costs\footnote{At the time of writing, \gpt costs USD 2.5 per million tokens.}, are also shown in Table~\ref{tab:different_benchmark_cost}. The good news is that the inference charges per query are just a few cents, making rewriting practical from a deployment perspective.
%

\subsection{Commercial DBMS}
\label{sec:commercial-dbms}

A legitimate question could be whether the rewrites made amends for the \pg optimizer but may fail to be useful in highly-engineered database engines. To evaluate this issue,
we performed TPC-DS rewrites on a pair of popular commercial DBMS, which we refer to as \textbf{OptA} and \textbf{OptB}. 

\vspace{-0.1cm}
\begin{table}[h]
\footnotesize
\centering
\caption{Performance on Commercial Database Engines.}
\label{tab:commercial-database-experiment}
\vspace{-0.1cm}
\begin{tabular}{|l|c|c|c|c|c|c|}
\hline
& \multicolumn{2}{c|}{\textbf{\# CPR}} & \multicolumn{2}{c|}{\textbf{\csgm}} & \multicolumn{2}{c|}{\textbf{\tsgm}} \\ \hline
& OptA & OptB & OptA & OptB & OptA & OptB \\ \hline
\hline
\lithe & 7 & 8 & 3.6 & 4.8 & 1.5 & 2.4 \\ \hline
SOTA &  3 & 6 & 2.1 & 3.5 & 1.3 & 1.7 \\ \hline
\end{tabular}
\end{table}

The performance of \lithe and \sota on these two systems is shown in Table~\ref{tab:commercial-database-experiment}, with \lithe continuing to do better than \sota. Despite the apparent lack of optimization headroom, \lithe still produces 7 and 8 \cprs resulting in a healthy \csgm of \textbf{3.6} and \textbf{4.8}, respectively. Further, the \tsgm provided by these rewrites are a useful \textbf{1.5} and \textbf{2.4}, respectively.
However, unlike the no-regression outcome seen with \pg, \lithe generates 3 \cprs on OptA, and 2 on OptB, which turn out to be regressions. So, although these commercial optimizers may have greater sophistication, they also suffer significant brittleness with regard to estimation accuracy.

Interestingly, one regression on OptB was caused by the LLM bringing in, via a basic prompt, an unrestricted version of Rule~6, which is prone to generating poorly-estimated plans (see Section~\ref{sec:cost-mismatch}). Despite such occasional errors, \lithe provides good overall improvement in runtimes as indicated by the \tsgm values.

We also performed a preliminary study of \lithe on an internal real-world benchmark, whose data and query characteristics are more realistic than TPC-DS. Even on this hard benchmark, \lithe produced 8 \cprs (out of 45 queries tested) with a \csgm of 2 and a \tsgm of 3.3.
%

The above results suggest \lithe has a useful role to play in industrial environments. From a different perspective, a company building a new database engine could use \lithe to non-invasively overcome the limitations of early versions of its optimizer.

\subsection{Dependence on LLM (Training/Model)}
\label{sec:altplatform}

\subsubsection{Masked Database}
\label{sec:masked}
An interesting question to ask now is whether the performance benefits seen thus far could be an artifact of \gpt having already been trained well on the TPC-DS benchmark, which is  prominent in the public domain.
%
To investigate this issue, we created a \emph{masked} version of the TPC-DS database schema, whereby the table and column names convey no semantic information about their contents. We then constructed rewrites for the \cpr queries (after syntactic changes to reflect the new masked schema)  on this version. The results are shown in Table~\ref{tab:masked}, and we observe that the performance profiles only marginally decrease for both \lithe and \sota, testifying to their robust applicability.

\vspace{-0.1cm}
\begin{table}[h]
\footnotesize
\centering
\caption{Rewrite Performance on Masked Database.}
\label{tab:masked}
\begin{tabular}{|c|c|c|c|c|}
\hline
\multirow{2}{*}{\textbf{Approach}} & \multicolumn{2}{c|}{\textbf{\# \cpr}} & \multicolumn{2}{c|}{\textbf{\csgm}} \\
& \textbf{TPC-DS} & \textbf{Masked} & \textbf{TPC-DS} & \textbf{Masked} \\ \hline
\hline
\sota  & 15  & 12 & \gmSotaAllDS  & \gmSotaAllObs \\ \hline
\lithe & 26  & 24 & \gmLitheAllDS & \gmLitheAllObs\\ 
\hline
\end{tabular}
\end{table}

\subsubsection{\lithe on \llama}
\label{sec:llama}

In our concluding experiment, we evaluated \lithe's performance on the \llama~3.1 70~billion parameter instruct model, substantially smaller compared to \gpt, which may have several hundred billion parameters~\cite{toi-article}. 
%
To attain practical inference times, the model was loaded using a low 4-bit quantization. Further, to ensure reproducibility and deterministic answers, the \textit{do\_sample} parameter was set to \textit{False}, which forces the LLM to perform greedy decoding. To make up for the huge reduction in model parameters as compared to \gpt, we include up to two example demonstrations for each rule-based prompt.

For this environment, Table~\ref{tab:mcts-exp-llama} shows \lithe's performance on the TPC-DS benchmark with and without MCTS. Although certainly lower than the corresponding numbers with \gpt (Section~\ref{sec:rewrite-perf}), it is encouraging to see that, in absolute terms, significant performance benefits can be obtained for most queries, especially with MCTS support.  So, the message is that smaller models can also be fruitfully used in real-world environments.

\vspace{-0.1cm}
\begin{table}[!h]
\footnotesize
\centering
\caption{\lithe Rewrite Performance with \llama.}
\label{tab:mcts-exp-llama}
\begin{tabular}{|c|c|c|c|}
\hline
& \textbf{\# \cpr} & \textbf{\csgm} & \textbf{\tsgm} \\ \hline \hline
\llama without MCTS & 17 & 5.6 & 8.2 \\ \hline
\llama with MCTS    & 21 & 8.3 & 14.7 \\ \hline
\end{tabular}
\end{table}
