\section{\lithe Overview}
\label{sec:lithe-architecture}

The \lithe architecture, illustrated in Figure~\ref{fig:lithe}, consists of the following five-stage pipeline:

\myparagraph{1. Prompt-based rewriting.}
%
This component consists of two modules: LLM Prompting and Syntax Verification.
%
The user query and an initial prompt are fed to the LLM prompting module, requesting a rewrite. The LLM output's syntax is then verified with the database engine's parser-- if invalid, the error is fed back to the LLM via an updated prompt asking for a correction, and this goes on iteratively (within a threshold) until a valid SQL query is obtained. 

This process is repeated for each of the different prompts described in Sections~\ref{sec:basic-prompt} and \ref{sec:dbms-proficient-prompts}, and the resulting syntactically-valid rewrites are fed to the query costing component.
We evaluate each prompt within a fresh LLM context, thereby making the prompt processing to be order-independent. 

\myparagraph{2. Query Costing.}
%
The costs of candidate rewrites are evaluated via the database engine's optimizer. Rewrites whose costs are greater than that of the original query are immediately discarded. Whereas, the potentially beneficial rewrites (if any) are checked for semantic equivalence to the original query.

\myparagraph{3. Semantic Equivalence.}
%
A suite of provable (logic-based) and statistical (result-based) techniques are employed to assess the semantic equivalence of a recommended rewrite.
%
If the rewrite is deemed valid by this module, it is returned along with the prompt that generated it; otherwise an invalid label is returned.

\myparagraph{4. Token probability-driven rewrite.}
%
The prompt producing the most beneficial (and valid) rewrite is used as input to a Monte-Carlo tree search (MCTS)-based procedure to further refine the rewrite quality. Specifically, multiple exploration paths are followed whenever the LLM lacks high confidence in the predicted token (details in Section~\ref{sec:mtcs-rewrite}). The query costing and semantic equivalence modules are also used internally within this procedure.
%

\myparagraph{5. Output.}
The least expensive valid rewrite as identified by the MCTS module is returned to the DBA together with the cost of the rewritten query and the approach used to claim equivalence. Additionally, an LLM-generated reasoning for the expected performance improvement is also provided. 
If no valid rewrite is identified, the original query itself is returned.

\subsection{Performance Framework}

We consider a query that takes more than \emph{T} seconds to complete on the native database engine as a ``slow query'', potentially triggering intervention by the DBA. Based on common industry perception (e.g.~\cite{mysql}), \emph{T} is set at 10 seconds in our study. For this context, we define a \textbf{\CPR (\cpr)} as a rewrite that improves a slow query's performance by at least \textbf{1.5} times wrt the optimizer-estimated cost -- this aggressive choice of threshold is so that: 
%
(a) there is enough headroom in the optimizer prediction that a runtime regression is unlikely; and (b) the benefits of the rewrite substantively outweigh the transformation overheads.
%

The overall benefit provided by a rewriting tool is quantified by the number of \cpr obtained on the slow query workload. Additionally, we also measure \textbf{\csgm}, the \textit{geometric mean}~(GM) of the cost speedups obtained by these rewrites. Finally, to assess the actual run-time benefits, we evaluate \textbf{\tsgm}, the geometric mean (GM) of the response-time speedups obtained by these rewrites.

\subsection{Query Micro-benchmark}
To motivate the progression of strategies incorporated in \lithe, we create an initial micro-benchmark comprising 10 diverse TPC-DS queries for which we were able to \emph{hand-craft} high-quality CPRs. These queries are processed on \gpt, the popular OpenAI LLM, and the rewrites are evaluated on the PostgreSQL v16 database engine. The human rewrites deliver a \csgm of \textbf{\gmGodMicroDS}, serving as an aspirational target to attain computationally.
Later, in Section~\ref{sec:exp}, we extend the evaluation to 
complete benchmarks. 

Further, for ease of presentation, we focus on the \csgm metric in the \lithe design sections (Sec~\ref{sec:basic-prompt},~\ref{sec:dbms-proficient-prompts},~\ref{sec:mtcs-rewrite}). The \tsgm performance is subsequently discussed in Section~\ref{sec:exp}. 





