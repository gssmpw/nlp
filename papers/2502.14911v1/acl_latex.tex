% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[preprint]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{multirow}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{makecell}
\usepackage{stackengine}
\setstackEOL{\cr}
\usepackage{hyperref}
\usepackage{float}
\usepackage[none]{hyphenat}
\usepackage{comment}

\usepackage{longtable}
\usepackage{listings}
\usepackage{array}

\lstset{
    columns=flexible,
    breaklines=true,
    breakatwhitespace=true,
    aboveskip=-2.5ex,
    belowskip=-2ex,
    basicstyle=\small\ttfamily,
}

% \usepackage{ffcode}
% \usepackage[finalizecache]{minted}
% \usepackage[frozencache]{minted}

% To comment multiple lines in Latex
\newcommand{\mycomment}[1]{}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{\textsc{Batayan}: A Filipino NLP benchmark for evaluating \\ Large Language Models}

\author{
 \textbf{Jann Railey Montalan\textsuperscript{1,2}},
 \textbf{Jimson Paulo Layacan\textsuperscript{3}},
 \textbf{David Demitri Africa\textsuperscript{4}},
 \textbf{Richell Isaiah Flores\textsuperscript{3}},
\\
 \textbf{Michael T. Lopez II\textsuperscript{3}},
 \textbf{Theresa Denise Magsajo},
 \textbf{Anjanette Cayabyab},
 \textbf{William Chandra Tjhi\textsuperscript{1,2}}
\\
\\
 \textsuperscript{1}AI Singapore,
 \textsuperscript{2}National University of Singapore,
 \\
 \textsuperscript{3}Ateneo de Manila University,
 \textsuperscript{4}University of Cambridge
\\
 \small{
   \textbf{Correspondence:} \href{mailto:email@domain}{railey@aisingapore.org}
 }
}

\begin{document}
\maketitle
\begin{abstract}
Recent advances in large language models (LLMs) have demonstrated remarkable capabilities on widely benchmarked high-resource languages; however, linguistic nuances of under-resourced languages remain unexplored. We introduce \textsc{Batayan}, a holistic Filipino benchmark designed to systematically evaluate LLMs across three key natural language processing (NLP) competencies: understanding, reasoning, and generation. \textsc{Batayan} consolidates eight tasks, covering both Tagalog and code-switched Taglish utterances. Our rigorous, native-speaker-driven annotation process ensures fluency and authenticity to the complex morphological and syntactic structures of Filipino, alleviating a pervasive translationese bias in existing Filipino corpora. We report empirical results on a variety of multilingual LLMs, highlighting significant performance gaps that signal the under-representation of Filipino in pretraining corpora, the unique hurdles in modeling Filipino's rich morphology and construction, and the importance of explicit Filipino language support and instruction tuning. Moreover, we discuss the practical challenges encountered in dataset construction and propose principled solutions for building culturally and linguistically-faithful resources in under-represented languages. We also provide a public benchmark and leaderboard as a clear foundation for iterative, community-driven progress in Filipino NLP.
\end{abstract}

\section{Introduction}

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{batayan_2.png}
    \caption{\textsc{Batayan} is a benchmark that holistically evaluates LLM capabilities on a wide range of Filipino language tasks. The rigorous curation and adaptation done by native Filipinos preserves the complexity and authenticity of Filipino language usage today.}
    \label{fig:batayan_contributions}
\end{figure}

Spurred on by recent advances in computing power, big data, and machine learning, LLMs have come into widespread use due to the emergence of a variety of novel and useful capabilities at scale 
\cite{hadi2023survey}. These capabilities have made user applications based on LLMs some of the fastest-growing consumer applications in human history, but have also rendered previous benchmarks insufficiently difficult and diverse \cite{brickscurrent, yang2023rethinking}.

Benchmarks are standard datasets used to measure and compare the performance of models against one another. In particular, the increasingly general capabilities of LLMs have necessitated holistic benchmarks which test a diversity of metrics like fairness, truthfulness, and robustness, as well as a variety of tasks like text summarization, casual reasoning, and translation \cite{yang2023rethinking, guo2023evaluating, liu2023trustworthy}. The vast majority of LLM benchmarks evaluate tasks in English, with non-English works being much fewer \cite{liu-etal-2021-visually, son2024mm}.


Filipino, despite being the national language of the Philippines and being spoken by over 80 million people, remains an under-resourced language. It is under-represented in multilingual LLMs, and existing corpora are domain-specific, non-multilingual \cite{cajote2024philippine}, and created by non-native speakers \cite{quakenbush2005philippine, dita2009building}.
Specifically for LLM benchmarks, Filipino is either excluded from multilingual benchmarks, suffer from limited task diversity, or have serious deficiencies in grammatical correctness, completeness, and diversity \cite{bandarkar-etal-2024-belebele, nllb2022}.

Moreover, Filipino is a complex language which exhibits a highly complex linguistic structure, particularly in its rich morphological system \cite{ramos2021tagalog, go2017using}. Its agglutinative nature allows for extensive use of affixation and creating nuance through prefixes, infixes, suffixes, and circumfixes \cite{archibald2001contemporary, jubilado2004philippine}. These affixes, combined with root words, allow intricate verb conjugations for marking tense, focus, and mood \cite{zamar2022filipino}. Filipino incorporates elements from a wide array of linguistic influences, such as Spanish \cite{bowen1971hispanic, wolff2001influence}, Chinese \cite{gonzales2022interactions, reid2018modeling, chan1980hokkien}, and Malay \cite{wardana2022lexicostatistics, baklanova2017types}, with codeswitching between English and Filipino being common \cite{bautista1991code, bautista2004taglish}.

% Anonymized text
% Our threefold contributions attempt to address these challenges. \textbf{First, we present \textsc{Batayan},\footnote{The word ``\textit{batayan}'' means ``basis'' or ``groundtruth''.} a holistic Filipino benchmark for evaluating large language models across 8 distinct tasks} spanning natural language understanding, reasoning, and generation. We place a rigorous emphasis on authenticity to natural Filipino language use through native speaker translation and annotation, addressing the limitations of existing Filipino datasets. \textsc{Batayan} is released as part of SEA-HELM, a leaderboard for comprehensive evaluation of LLMs across linguistic and reasoning tasks for Southeast Asian languages. \textbf{Second, we provide extensive evaluation results from testing 8 prominent LLMs on \textsc{Batayan}}, revealing significant disparities in model performance across different linguistic capabilities in Filipino. \textbf{Third, we document systematic challenges and methodological considerations in creating high-quality Filipino NLP datasets}, particularly highlighting issues in translation fluency, vocabulary adaptation, and the preservation of Filipino's rich morphological features. These insights provide a practical framework for future development of Filipino language resources.

% Non-anonymized text
Our threefold contributions attempt to address these challenges. \textbf{First, we present \textsc{Batayan},\footnote{The word ``\textit{batayan}'' means ``basis'' or ``groundtruth''.} a holistic Filipino benchmark for evaluating large language models across 8 distinct tasks} spanning natural language understanding, reasoning, and generation. We place a rigorous emphasis on authenticity to natural Filipino language use through native speaker translation and annotation, addressing the limitations of existing Filipino datasets. \textsc{Batayan} is released as part of SEA-HELM\footnote{\href{https://leaderboard.sea-lion.ai/}{https://leaderboard.sea-lion.ai/}}, a leaderboard for comprehensive evaluation of LLMs across linguistic and reasoning tasks for Southeast Asian languages. \textbf{Second, we provide extensive evaluation results from testing 8 prominent LLMs on \textsc{Batayan}}, revealing significant disparities in model performance across different linguistic capabilities in Filipino. \textbf{Third, we document systematic challenges and methodological considerations in creating high-quality Filipino NLP datasets}, particularly highlighting issues in translation fluency, vocabulary adaptation, and the preservation of Filipino's rich morphological features. These insights provide a practical framework for future development of Filipino language resources.


\section{Considerations for Filipino Evaluations}
\label{sec:considerations}

\mycomment{First, we motivate various linguistic considerations in creating \textsc{Batayan}, such as the choice to allow Taglish or choosing between grammatically valid word orders.}

\subsection{Language of Evaluation}
\label{sec:language_of_evaluation}
While Filipino is the official language of the Philippines, it has been argued in literature that its de facto \textit{lingua franca} is Taglish, the practice of code-switching between English and Tagalog \cite{go2013tagalog}. Naturally-occurring datasets in Filipino (e.g., those mined from social media, recorded interviews) contain some code-switching \cite{bautista2004taglish}. We defer the problematization of the debated difference between Tagalog and Filipino, and use the two terms interchangeably.

Standard Tagalog, while text-rich, is considered under-resourced, lacking the linguistic data, tools, and resources for effective natural language processing \cite{miranda2023nlp}. To address data scarcity, developers take advantage of high-resource languages such as English through translations \cite{goyal-etal-2022-flores, doddapaneni-etal-2023-towards}. Furthermore, leveraging multilingual datasets is potent not only because it enhances the performance of models trained on limited resources but also because code-switching and bilingualism is a common phenomena in the Philippines \cite{tupas2017bilingual}.

Taglish is characterized to possess communicative efficiency. Compared to Standard Tagalog or English, Taglish, provides a more convenient way of communicating a message \cite{bautista2004taglish}. For this consideration, application of NLP becomes more helpful as users are more likely to interact freely when using a language familiar to them. 

\subsection{Language Design Principles}
The design of \textsc{Batayan} adhered to principles that emphasize linguistic authenticity and representativeness in word choice, sentence structure, and grammar. 

In terms of language use and word choice, we employed common Filipino and Taglish vocabulary, balancing loanwords with native Filipino terms by prioritizing colloquial usage. This was also done when words had both English and Filipino variants, taking into account spelling and orthographic preferences prevalent among native speakers.

Regarding sentence structure, we prioritized sentences with attention to natural syntax and the choice of \textit{ayos} (sentence arrangement), namely direct (\textit{karaniwang ayos}, KA, \textit{lit.} usual order) or inverted (\textit{di-karaniwang ayos}, DKA, \textit{lit.} unusual order) forms \cite{tanawanetal2008istruktura}. In KA, Filipino constructions follow the typical predicate-initial word order \cite{malicsi2013gramar}. In contrast, DKA, which is also referred to linguistically as an \textit{ay}-inversion, is a type of construction in Filipino where non-predicative constituents (such as \textit{simuno}, \textit{lit.} grammatical subject) are ``fronted'' or shifted to precede the predicate, ``marking'' them as the ``topic'' of the sentence \cite{kroeger1993phrase}. Inverted constructions are often used in formal settings, and could therefore be deemed unnatural in most situations \cite{pizarro2010revisiting}.  This characteristic of Filipino is of interest due to our observation of automatic translations preferring DKA (see Section \ref{subsub:issues-adapted}); hence, the further need for native re-translations with a preference for KA. The distinction between KA and DKA with respect to natural syntax and translation style is further discussed in the same section.

For translated datasets, compliance with Filipino grammar was also a critical aspect of the dataset design, with adherence to grammatical guidelines set by the Komisyon sa Wikang Filipino \cite{almario2014masinop}. In translation, we followed principles aimed at preserving the source material's intent while adjusting for cultural and contextual relevance as suggested by \citet{almario2016batayang}. Translations were performed by native Filipino speakers in the identified \textit{lingua franca}, whenever appropriate, to maintain semantic equivalence without resorting to overly literal phrasing.

\section{Task and Dataset Curation}

\subsection{Task Selection} 
\label{sec:task_selection}

Previous work on Filipino language model evaluation has been largely fragmented. While prior studies have made important contributions in assessing large-scale language models \cite{evaluating2020cruz, cruz-cheng-2022-improving}, and various researchers have developed task-specific datasets for named entity recognition \cite{miranda2023developing}, sentiment analysis \cite{villavicencio2021twitter}, and other isolated tasks, the field lacks a unified, comprehensive benchmark for systematic evaluation of model capabilities across different linguistic dimensions.

Through \textsc{BATAYAN}, we aim to significantly expand the scope of Filipino language evaluation by introducing tasks that assess a gamut of linguistic capabilities. Specifically, we focus on integrating more challenging Natural Language Understanding (NLU) tasks alongside Natural Language Reasoning (NLR) tasks, while introducing novel Natural Language Generation (NLG) tasks previously unexplored in the Filipino context. This comprehensive approach allows for a more thorough assessment of models' ability in Filipino given its rich morphology and unique linguistic characteristics.

Our benchmark design and selection of tasks are informed by established multilingual evaluation frameworks, particularly BHASA \cite{leong2023bhasaholisticsoutheastasian}, which provides systematic evaluation across a number of Southeast Asian languages. We also draw insights from widely-adopted language benchmarks such as the XTREME multilingual benchmark \cite{10.5555/3524938.3525348} and the IndoNLU Indonesian benchmark \cite{wilie-etal-2020-indonlu}, which have demonstrated the importance of testing various aspects of linguistic competence.

For this study, we carefully selected eight tasks that span three key competencies of NLP. For NLU: Paraphrase Identification (PI), Question Answering (QA), Sentiment Analysis (SA), Toxicity Detection (TD). For NLR: Causal Reasoning (CR), Natural Language Inference (NLI). For NLG: Abstractive Summarization (AS), Machine Translation (MT). These tasks are detailed in Appendix \ref{sec:dataset_statistics}.

\mycomment{\textcolor{red}{Revised Outline:}
\begin{itemize}
    \item We are patterning task selection primarily from Bhasa; due to availability of datasets -- in a sense we are extending Bhasa's study to Filipino. the eight tasks are as discussed as follows: \textcolor{red}{(each task might have their own short paragraph description)}
    \item Paraphrase Identification (PI)
    \item Question Answering (QA)
    \item Sentiment Analysis (SA)
    \item Toxicity Detection (TD)
    \item Causal Reasoning (CR)
    \item Natural Language Inference (NLI)
    \item Abstractive Summarization (AS)
    \item Machine Translation (MT)
    \item For this study specifically: due to the agglutinative structure of Filipino, NER was excluded from the task and replaced by PI \textcolor{red}{(add source for claim + expound on where PI came from)}
    \item Nonetheless the set of tasks cover the three key competencies of NLP, as defined by Bhasa (NLU, NLR, NLG). In particular:
    \begin{enumerate}
    \item Natural Language Understanding (NLU): Paraphrase Identification (PI), Question Answering (QA), Sentiment Analysis (SA), Toxicity Detection (TD);
    \item Natural Language Reasoning (NLR): Causal Reasoning (CR), Natural Language Inference (NLI); and
    \item Natural Language Generation (NLG): Abstractive Summarization (AS), Machine Translation (MT).
\end{enumerate} 
    this makes a more comprehensive assessment of LLMs \textcolor{red}{(do we expound on these competencies as well?)}
\end{itemize}}

\subsection{Dataset Collection and Annotation}

\renewcommand{\arraystretch}{1.1}
\begin{table*}[t]
\centering
\small
\begin{tabular}{p{1.4cm} p{0.4cm} p{5.0cm} p{1.1cm} p{1.1cm} p{1.6cm} p{2.2cm}}
\hline
\textbf{Competency} & \textbf{Task} & \textbf{Dataset} & \textbf{Source Language} & \textbf{Output (Options)} & \textbf{Domain} & \textbf{Adaptation Applied} \\
\hline
\multirow{1}{*}{NLU} & PI & PAWS \cite{paws2019naacl} & English & label (2) & Wikipedia & Native translation \\
& QA & Belebele \cite{bandarkar-etal-2024-belebele} & Filipino & span (4) & Wikinews & Native translation \\
& SA & PH Elections \cite{cabasag2019hate} & Taglish & label (3) & Social media & None \\
& TD & PH Elections \cite{cabasag2019hate} & Taglish & label (2) & Social media & None \\
\hline
\multirow{1}{*}{NLR} & CR & Balanced COPA \cite{kavumba-etal-2019-choosing} & English & label (2) & General & Native translation \\
& NLI & XNLI \cite{conneau2018xnli} & English & label (3)& General & Native translation \\
\hline
\multirow{1}{*}{NLG} & AS & XL-Sum \cite{hasan-etal-2021-xl} & English & summary & News & Native translation \\
& MT & FLORES 200 \cite{nllb2022} & Filipino & translation & Wikinews & Native translation \\
\hline
\end{tabular}
\caption{Source datasets for each task in \textsc{Batayan}.}
\label{tab:dataset_sources}
\end{table*}
\renewcommand{\arraystretch}{1.0}
% confusing if we put something like "N labels" under Output column because N is the number of options not number of output labels

To construct \textsc{Batayan}, we curated open-source corpora with clear provenance when possible, prioritizing datasets that exhibit authentic language use across various domains such as social media, news articles, and other publicly available texts. We identified existing Filipino-language datasets for MT, QA, and TD. For the SA task, we chose to repurpose an existing dataset and annotate them for the aforementioned task.

Tasks such as AS, CR, NLI, and PI lacked a native Filipino corpus. As such, we first identified existing English corpora, generated initial translations using automatatic translation tools, then conducted a manual review and revision to generate high-quality and fluent translations. The authors\footnote{Demographics: working professionals and university students, aged between 21–30 years old, native Filipino speakers, lived in the Philippines for a majority of their lives.} acted as annotators and raters for this study. They were grouped into teams of three and performed translations and reviews, focusing on cultural and linguistic relevance to ensure that the translated texts resonate with Filipino contexts.

For each task, we randomly sampled $5n$ entries for a target size of $n$, maintaining balanced class distributions. Entries were filtered by length (20–2000 characters) and quality ($\leq$50\% grammatical errors). Afterwards, each sample in each task underwent evaluation by teams of three against three criteria: completeness, fluency, and sensibility. Samples were assessed as complete only if they contained well-formed sentences, excluding standalone dependent clauses or fragmentary headers and titles lacking complete meaning. Fluency was evaluated on native-like constructions based on multiple factors including appropriate verb conjugations (e.g., avoiding constructions like ``*\textit{Nagpapasalamatan ako sa iyo}'', \textit{lit.} ``*I give thanks to you.''), natural word choices (avoiding awkward phrases like ``*\textit{Si Pacquiao ang kamao ng bansa}'' instead of ``\textit{Si Pacquiao ang pambansang kamao}'', in English: ``Pacquiao is the nation's fist''), and preference for KA sentence structure where contextually appropriate. Samples were assessed as sensible if relevant to the task, screening for confounding factors that would not effectively test the intended language capability (e.g., incorrect answers in QA mislabeled as correct).

For AS, we employed additional criteria based on previous work \cite{leong2023bhasaholisticsoutheastasian} to assess the appropriateness of the summary in reference to the content of the provided passage. We score samples on faithfulness (binary), relevance (0–3), fluency (0–3), and coherence (0–3), with three independent raters per entry. For faithfulness, raters evaluated the factual consistency between the summary and source article, with particular attention to fact preservation and penalization of hallucinated content. Only summaries receiving unanimous 1/1 faithfulness scores from all raters were considered for inclusion, ensuring strict maintenance of factual integrity. For relevance, raters evaluated both coverage of essential information and information filtering, penalizing summaries containing redundancies or excess details. For fluency, raters assessed for proper formatting, appropriate capitalization, and grammatical construction, with higher scores awarded to summaries exhibiting natural Filipino language patterns. For coherence, raters assessed how effectively information flowed between sentences, with higher scores given to summaries that built logically from sentence to sentence to create a cohesive narrative about the topic.

Only samples passing all applicable criteria were included in the final dataset. In cases where initial sampling yielded insufficient qualifying entries, deficient samples were carefully corrected by authors through targeted improvements to grammar, translation of English passages, or other identified issues while maintaining authentic Filipino language patterns. 

Joint agreement on our evaluation criteria is presented in Appendix \ref{sec:agreement}. This process prioritized agreement on fundamental classification criteria rather than traditional metrics which use chance correction like Cohen's kappa which are less suitable for binary categorical tasks \cite{powers-2012-problem} and classification tasks in general \cite{delgado2019cohen, charles2016not}.


\subsection{Source Datasets}

Table \ref{tab:dataset_sources} outlines key attributes of each dataset, such as language, output type, domain, and any adaptations applied to ensure alignment with natural Filipino language construction. More detailed statistics can be found in Appendix \ref{sec:dataset_statistics}. 

Native review and translation was applied for all tasks (except for SA and TD) to enhance contextual and linguistic accuracy. We chose to maintain the naturally-occurring language for sentiment analysis and toxicity detection to capture the nuances of code-switched Taglish usage prevalent in social media in the Philippines.

For the SA task, three native Filipino speakers reannotated the Philippine election-related tweets \cite{cabasag2019hate} dataset with three sentiment polarities: positive, negative, and neutral. Inter-annotator agreement was calculated, which yielded a Cohen’s kappa of 0.8202 and a Krippendorf’s alpha of 0.8268, indicating substantial agreement.



\section{Challenges with Developing a Filipino Benchmark}
This section discusses issues in creating \textsc{Batayan}, split into datasets that required adaptation and datasets where the content were kept as-is. The purpose of this section is to shed light on issues that future researchers may encounter in creating new Filipino datasets or adapting previous ones.

\subsection{Issues with Adapted Data} \label{subsub:issues-adapted}
Most of the tasks presented in this study were sourced from existing English language datasets that were natively translated and adapted into Filipino by the authors, whom are all native speakers of Filipino.

\textbf{Creating more relevant summaries for XL-Sum.} Previous work has noted that a significant portion of reference summaries in the XL-Sum dataset is highly abstractive, demonstrates factual errors, or contains information not mentioned in the provided articles \cite{guo-etal-2022-questioning}, putting to question its factuality and validity. To address this, we developed new Filipino summaries for each article. We ensured relevance and fluency by including only the most important information that can be directly lifted from the article using natural-sounding and grammatically-correct constructions.

\textbf{Limitations in adapting vocabulary.} Adapting the English tasks to Filipino revealed issues in maintaining the intelligibility of the text. Technical terms that exist in English were difficult to translate into Filipino, with several of them having no direct translations. For example, an instance from Belebele included the term ``rule of thirds'', which describes a specific photography technique. Originally, the translation of this phrase in the Tagalog subset of Belebele was ``\textit{tuntunin ng mga sangkatlo}'' (\textit{lit.} ``rule of thirds''), which is does not make sense in Filipino and does not convey the intended meaning. For jargon such as this, we chose to keep the original English phrasing. 

English idiomatic expressions were also prevalent and required a more flexible approach in adaptation to ensure the integrity of meaning. For example, one premise from Balanced COPA was, ``The criminal turned himself in'', using the idiomatic expression ``turned himself in'' to mean ``surrendered''. We chose to translate this into Filipino as, ``\textit{Isinuko ng kriminal ang kanyang sarili}'' (in English: ``The criminal surrendered himself''). 

Moreover, the presence of English words that have homonyms added to the complexity of translating English sentences to Filipino. For example, some sentences in the PAWS dataset contained the adjective ``right'', which can refer to either the direction (in Filipino: ``\textit{kanan}'') or an assertion of the correctness of the object it is modifying (in Filipino: ``\textit{tama}''). In such cases, we inferred the most probable meaning of the words from the context of the entire passage and then identified the corresponding Filipino word to be incorporated in our Filipino translation.

\textbf{(Dis)fluency in expert and machine translations.} 
Our rigorous review and annotation of existing datasets with machine and expert-guided translations in Filipino revealed weaknesses in their adaptation. The authors found that the automatic translations were unnatural, disfluent, and showed characteristics of translationese \cite{gellerstam1986translationese, riley_translationese_2020} despite being grammatically correct. Notably, we observed that these initial translations demonstrated an unusual preference with using \textit{ay}-inversion. We also observed that these translations used Filipino terms that were synonymous to their English counterparts but pragmatically-awkward. Hence, we applied rephrasing and re-translation to ensure that the samples used sound more native and natural.

For example, one passage from the English subset of Belebele was, ``The CCTV would certainly send a strong signal...'' The original Tagalog (machine) translation was, ``\textit{Ang CCTV ay tiyak na \underline{magpapadala} ng malakas na  \underline{hudyat}...}'' The term ``\textit{magpapadala}'' here means ``to send'' in the sense of transporting a thing from one place to another, while the term ``\textit{hudyat}'' implies a ``starting sign''. It also uses the unnatural DKA construction or \textit{ay}-inversion. Within the context of the original passage, a more fluent (human) translation would be, ``\textit{Tiyak na \underline{maghahatid} ang CCTV ng malakas na \underline{pahiwatig}...}'', where ``\textit{maghahatid}'' means ``to bring about'' and ``\textit{pahiwatig}'' conveys a sense of ``reminder'' or ``warning'', and the sentence follows the usual KA construction. 

\textbf{Inconsistencies in \textit{ay}-inversions in translation construction.} Styles in translation vary from person-to-person, depending on their valuations and reading of the original text \cite{castagnoli2020translation}. Likewise, \textit{ay}-inversion in the context of translation boils down to a stylistic choice of the translator. Hence, while the KA is deemed more natural over the DKA, this valuation of the usual/natural and unusual/unnatural is debated.

On one hand, university stylebooks on writing such as that of \citet{yapan2017bagay} considers KA to mean the usual pattern of speech for Filipino speakers. Also, \citet{magracia2001panumbas} notes that while the DKA is correct in ``meaning, syntactic order, and grammaticality,'' it is not the ``natural'' way of speaking for Filipinos. Furthermore, she argues that the DKA reflects a speaker's language of thought—in this case, the influence of English. The same justification, in terms of source language for translated corpora as demonstrated in \citet{visweswariah2011word}, can then be said on the machine-translated text, though this remains an open question for Filipino.

On the other hand, other research argues that DKA is not an abnormality, but is instead  a necessary aspect for ``discourse continuity'' \cite{bolata2022ang}.  Hence, the more complex the sentence, the higher the chance of utilizing the \textit{ay}-inversion \cite{fox1985word}. The same can be observed in the datasets. For example, the short sentences in the CR task allowed for the sentences to be changed to the KA, while longer, more context-rich sentences in the NLI task required the use of the DKA.

These frameworks demonstrate the subjective nature of the selection of sentence structure. Hence, such inconsistencies may still be observed in the tasks in \textsc{Batayan} despite being rigorously translated by native speakers.

\renewcommand{\arraystretch}{1.1}
\begin{table*}[t]
\centering
\small
\begin{tabular}{lrrrrrr}
\hline
Models & PI & QA & SA & TD & CR & NLI \\
\hline
aisingapore/gemma2-9b-cpt-sea-lionv3-instruct   & 82.02          & 82.80          & \textbf{75.99} & \textbf{73.12} & \textbf{92.75} & \textbf{68.31} \\
aisingapore/llama3.1-8b-cpt-sea-lionv3-instruct & 74.98          & 80.69          & 51.18          & 61.24          & 79.70          & 66.26 \\
CohereForAI/aya-expanse-8b                      & 33.94          & 2.97           & 57.44          & 34.39          & 1.96           & 25.27 \\
google/gemma-2-9b-it                            & 82.50          & \textbf{83.96} & 70.72          & 64.26          & 91.25          & 60.61 \\
meta-llama/Llama-3.1-8B-Instruct                & 60.94          & 73.97          & 52.54          & 49.76          & 73.21          & 57.64 \\
Qwen/Qwen2.5-7B-Instruct                        & \textbf{84.45} & 69.11          & 67.42          & 61.05          & 45.76          & 58.75 \\
sail/Sailor2-8B-Chat                            & 48.93          & 59.48          & 48.40          & 47.48          & 19.82          & 24.05 \\
SeaLLMs/SeaLLMs-v3-7B-Chat                      & 36.98          & 49.68          & 70.79          & 63.26          & 71.91          & 20.07 \\
\hline
\end{tabular}
\caption{Performance of LLMs on \textsc{Batayan} natural language understanding (NLU) and natural language reasoning (NLR) tasks. Macro F1 scores are reported.}
\label{tab:model_results_nlu_nlr}
\end{table*}
\renewcommand{\arraystretch}{1.0}

\subsection{Issues with Natively-Sourced Data}
While the issues with most datasets in \textsc{Batayan} were related with fluency and naturalness, the Philippine election-related tweets \cite{cabasag2019hate} dataset used for the SA and TD tasks presented unique challenges. 

\textbf{Incomplete entries.}
Due to the contextual nature of Filipino and social media communication, many samples lacked sufficient contextual information. To maintain dataset authenticity, we prioritized entries with adequate information for sentiment or toxicity analysis rather than complete removal. This was accomplished by selecting high-agreement samples where native speakers demonstrated consistent task performance.

\textbf{Non-standard orthography.}
The dataset exhibited significant orthographic variation (spelling, capitalization, word boundaries, etc.) due to its natural language origins. We preserved this variation as it reflects authentic Filipino language \cite{ilao2012comparative, javier2018pagsusuri, caroro2020rules} use relevant to toxicity detection and sentiment analysis tasks, aligning with our methodological principles. We maintained quality control by verifying that orthographical variations remained comprehensible to native readers.

\textbf{Class imbalance.}
The dataset showed a disproportionate distribution of negative sentiment regarding specific political entities. We addressed this by rebalancing the dataset and prioritizing high-agreement samples to mitigate individual annotator bias.


\section{Evaluation}

\textsc{Batayan} serves as the Filipino component of the SEA-HELM leaderboard. The evaluation framework of SEA-HELM follows prior systems such as HELM \cite{liang2023holisticevaluationlanguagemodels}, and is composed of tasks, prompts, and metrics.


\subsection{Evaluation Design}
\label{sec:evaluation_design}

\textbf{Tasks.} The selection of tasks in \textsc{Batayan} aforementioned in Section \ref{sec:task_selection} aims to comprehensively evaluate language capabilities of LLMs in Filipino. Each task is a collection of instances composed of an input string, a list of references, accompanying metadata, and the groundtruth label. We divide instances into two sets: the evaluation set, and a small 5-instance set that can serve as exemplars for few-shot prompting. Overall, \textsc{Batayan} provides 8 distinct tasks with 3,800 test instances. 

\textbf{Prompts.} For \textsc{Batayan}, we developed prompt templates for each task written in Filipino. We ensured that the instructions for each task are consistent with the prompt template design already used in SEA-HELM. A comprehensive list of these prompt templates can be found in Appendix \ref{sec:prompt_templates}.

During model evaluation, input prompts are constructed using evaluation instances and the corresponding prompt template. The default evaluation setting for \textsc{Batayan} is zero-shot prompting, where in-context input-label pairs are not included in the model prompt. Given the input prompts and decoding parameters (see Appendix \ref{sec:experimental_setup}), a model then generates output completions.

\textbf{Metrics.} We adopt multiple metrics to quantify the models' performance on each task. For each metric, the model completion is treated as the prediction, while the instance label is used as the reference. For the NLU and NLR tasks, we report the macro F1 score. For machine translation (English→Filipino and Filipino→English), we use ChrF++ \cite{popovic-2017-chrf} and MetricX-24 using the \texttt{metricx-24-hybrid-xxl-v2p6-bfloat16} model \cite{juraska-etal-2024-metricx}. For abstractive summarization, we report three metrics: BERTScore \cite{bert-score}, ChrF++, and ROUGE-L F1 from the multilingual implementation of ROUGE \cite{lin-2004-rouge} used in XL-Sum \cite{hasan-etal-2021-xl}. Default package parameter settings are used, and performance scores are based on a single run of the benchmark.


\subsection{Results}

\renewcommand{\arraystretch}{1.1}
\begin{table*}[t]
\centering
\small
\begin{tabular}{lrrrrrrrr}
\hline
\multicolumn{1}{l}{\multirow{2}{*}{Models}} & \multicolumn{3}{c}{\multirow{1}{*}{AS}} & \multicolumn{2}{c}{\multirow{1}{*}{MT (eng→tgl)}} & \multicolumn{2}{c}{\multirow{1}{*}{MT (tgl→eng)}}\\ \cline{2-8} 
& {\tiny BERTScore} & {\tiny ChrF++} & {\tiny ROUGE-L F1} & {\tiny ChrF++} & {\tiny MetricX-24} & {\tiny ChrF++} & {\tiny MetricX-24} \\
\hline
aisingapore/gemma2-9b-cpt-sea-lionv3-instruct   & 73.08          & 34.43            & 21.06             & \textbf{57.80}    & \textbf{87.23}    & 65.93             & \textbf{90.55} \\
aisingapore/llama3.1-8b-cpt-sea-lionv3-instruct & 73.47          & 34.15            & 23.82             & 57.08             & 82.46             & 62.73             & 87.13 \\
CohereForAI/aya-expanse-8b                      & 59.38          & 6.38             & 1.39              & 39.27             & 37.47             & 59.23             & 78.88 \\
google/gemma-2-9b-it                            & 72.68          & 34.09            & 21.73             & 56.64             & 84.00             & \textbf{67.54}    & 76.49 \\
meta-llama/Llama-3.1-8B-Instruct                & \textbf{74.33} & \textbf{34.51}   & \textbf{27.39}    & 44.91             & 51.92             & 54.28             & 72.04 \\
Qwen/Qwen2.5-7B-Instruct                        & 70.50          & 28.74            & 15.77             & 38.78             & 34.23             & 54.94             & 75.97 \\
sail/Sailor2-8B-Chat                            & 71.84          & 32.06            & 19.82             & 50.44             & 79.85             & 53.34             & 20.12 \\
SeaLLMs/SeaLLMs-v3-7B-Chat                      & 70.55          & 28.23            & 16.90             & 46.03             & 61.29             & 39.03             & 59.97 \\
\hline
\end{tabular}
\caption{Performance of LLMs on \textsc{Batayan} natural language generation (NLG) tasks.}
\label{tab:model_results_nlg}
\end{table*}
\renewcommand{\arraystretch}{1.0}

\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{nlg_results_3}
    \caption{Model performance on NLG tasks. Left: Performance on abstractive summarization measured using BERTScore, ChrF++, and ROUGE-L F1. Right: Machine translation performance measured using ChrF++ and MetricX-24 for both English$\rightarrow$Tagalog and Tagalog$\rightarrow$English. Best scores for each metric are highlighted.}
    \label{fig:nlg_results}
\end{figure*}


Analysis of model performance across tasks reveals several insights about current multilingual LLMs' capabilities in Filipino. Models with explicit Filipino language support generally outperform those without dedicated instruction tuning, though the margin varies significantly by task type.
In NLU and NLR tasks (see Table \ref{tab:model_results_nlu_nlr}), \texttt{gemma2-9b-cpt-sea-lionv3-instruct} demonstrates superior performance, achieving the highest scores in SA (75.99\%), TD (73.12\%), and NLI (68.31\%). This suggests that targeted regional pretraining and instruction tuning can capture the nuances of Filipino sentiment and toxicity, which often involve code-switching and cultural context. However, for PI, \texttt{Qwen2.5-7B-Instruct} achieves the best performance (84.45\%), indicating that semantic similarity tasks may benefit more from general multilingual pretraining than from region-specific tuning.

The performance gap between models with Filipino language support and general multilingual models is most pronounced in CR, where \texttt{gemma2-9b-cpt-sea-lionv3-instruct} (92.75\%) significantly outperforms models without dedicated Filipino support. This suggests that understanding causality in Filipino requires strong grasp of the language's complex morphological system, particularly in how verb affixes encode causative relationships.

In MT (see Table \ref{tab:model_results_nlg}), Filipino-supported LLMs show stronger performance overall. For English→Filipino translation, \texttt{gemma2-9b-cpt-sea-lionv3-instruct} leads in both ChrF++ (57.80) and MetricX-24 (87.23). However, for Filipino→English translation, while \texttt{gemma-2-9b-it} achieves a slightly higher ChrF++ (67.54) than \texttt{gemma2-9b-cpt-sea-lionv3-instruct} (65.93), it is substantially overtaken by the latter on MetricX-24 (90.55 vs. 76.49). This performance gap suggests that region-specific tuning not only aids comprehension of Filipino source text but can also the semantic fidelity of English outputs—an aspect captured more effectively by MetricX-24.

In AS, \texttt{Llama-3.1-8B-Instruct} achieves the highest BERTScore (74.33), despite lacking explicit Filipino support. This unexpected result may indicate that general abstraction and summarization capabilities transfer well across languages, even without language-specific tuning. However, when examining ROUGE-L scores, Filipino-supported models show stronger performance, suggesting they better preserve Filipino-specific discourse structures and coherence patterns. 

In our experiments, we found that the \texttt{aya-expanse-8b} model mostly failed to handle Filipino instructions, resulting in low scores particularly in CR (1.96\%) and QA (2.97\%).

Examining the relationship between model size and performance reveals an interesting pattern: while larger models generally perform better, the correlation is weaker for Filipino-specific tasks compared to similar evaluations in English. This suggests that current scaling approaches may not adequately capture the unique characteristics of Filipino, such as its agglutinative morphology and frequent code-switching.

\section{Conclusion}

In this paper, we introduced \textsc{Batayan} for holistically evaluating LLMs on a gamut of Filipino language tasks covering natural language understanding, reasoning, and generation. Our findings show that LLMs with explicit Filipino language support and finetuned on Filipino instructions demonstrate better performance on \textsc{Batayan} compared to other models.

As one of the major challenges in developing \textsc{Batayan} is maintaining the naturalness and fluency of the language, we plan to develop metrics and tools that can help discriminate from translationese and natural texts, inspired by prior research \cite{lovenia-etal-2024-seacrowd, riley_translationese_2020}.

\section*{Limitations}

While our work provides a unique and comprehensive Filipino language benchmark, we also reported in previous sections the challenges and limitations in developing high-quality NLP resources for the under-represented language. Additionally, prosodic characteristics such as stress and sarcasm are not immediately obvious in the datasets utilized. We account for this by selecting only high-agreement samples. We also note that the domain of both SA and TD tasks (which were tweets surrounding political events), limits the distributions of these tasks, and as such we recommend utilizing other datasets that cover a wider set of domains and use cases.

\section*{Ethical Considerations}

This project was approved by the principal investigator's university internal review board. 

Due to the nature of the toxicity detection task, we note that the authors were exposed to offensive material. Nonetheless, they were encouraged to report inappropriate samples and were given the option to stop work if desired.

For the review and annotation of the sentiment analysis task, we involved a quality assurance team consisting of native Filipino speakers. The team, comprised of students at local universities, were recruited through public advertisements that stated the estimated work load and remuneration, which were consistent with university research guidelines and regulatory requirements.

We do not foresee negative social impacts from this paper. Our work introduces Filipino language resources that were reviewed by native Filipino speakers, paying due respect to local cultural sensitivities. We thus do not believe that our research will contribute to over-generalizations regarding Filipino culture.

We plan to release \textsc{Batayan} (datasets and codebase) under the Creative Commons Attribution Share-Alike 4.0 (CC-BY-SA 4.0) license.

\section*{Acknowledgments}

This research project is supported by the National Research Foundation, Singapore under its AI Singapore's National Large Language Models Funding Initiative.

The authors would like to thank National University of Singapore and AI Singapore, especially the AI Products and SEA-LION team, for their unwavering support with this endeavor.

The authors likewise thank the University of Cambridge, Department of Computer Science and Technology, and Magdalene College. David Africa's work is supported by the Cambridge Trust and the Jardine Foundation.

Lastly, the authors would like to thank all the Filipino natives involved in this study for their time and valuable contributions.



\bibliography{custom}

\appendix

\onecolumn
\section{Inter-rater Agreement}
\label{sec:agreement}

\renewcommand{\arraystretch}{1.2}
\begin{table*}[ht]
\centering
\small
\begin{tabular}{llllrr}
\hline
\textbf{Competency} & \textbf{Task} & \textbf{Dataset} & \textbf{Criteria} & \textbf{Joint agreement} & \textbf{Rating} \\
\hline
\multirow{1}{*}{NLU} & \multicolumn{1}{l}{\multirow{1}{*}{PI}} & \multicolumn{1}{l}{\multirow{1}{*}{PAWS}} & Completeness & 0.9550 & - \\
& \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & Fluency & 0.5875 & - \\
& \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & Sensibility & 0.8025 & - \\
& \multicolumn{1}{l}{\multirow{1}{*}{QA}} & \multicolumn{1}{l}{\multirow{1}{*}{Belebele}} & Completeness & 0.9700 & - \\
& \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & Fluency & 0.8300 & - \\
& \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & Sensibility & 0.9500 & - \\
& \multicolumn{1}{l}{\multirow{1}{*}{SA}} & \multicolumn{1}{l}{\multirow{1}{*}{PH Election Tweets}} & Completeness & 0.6217  & - \\
& \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & Fluency & 0.6767  & - \\
& \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & Sensibility & 0.7050  & - \\
& \multicolumn{1}{l}{\multirow{1}{*}{TD}} & \multicolumn{1}{l}{\multirow{1}{*}{PH Election Tweets}} & Completeness & 0.8310  & - \\
& \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & Fluency & 0.9060  & - \\
& \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & Sensibility & 0.8680 & - \\
\hline
\multirow{1}{*}{NLR} & \multicolumn{1}{l}{\multirow{1}{*}{CR}} & \multicolumn{1}{l}{\multirow{1}{*}{Balanced COPA}}  & Completeness & 0.8975 & - \\
& \multicolumn{1}{l}{} & \multicolumn{1}{l}{}  & Fluency & 0.7275 & - \\
& \multicolumn{1}{l}{} & \multicolumn{1}{l}{}  & Sensibility & 0.9575 & - \\
& \multicolumn{1}{l}{\multirow{1}{*}{NLI}} & \multicolumn{1}{l}{\multirow{1}{*}{XNLI}} & Completeness & 0.9716 & - \\
& \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & Fluency & 0.8433 & - \\
& \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & Sensibility & 0.9933 & - \\
\hline
\multirow{1}{*}{NLG} & \multicolumn{1}{l}{\multirow{1}{*}{AS}} & \multicolumn{1}{l}{\multirow{1}{*}{XL-Sum}}  & Completeness & 0.8800 & - \\
& \multicolumn{1}{l}{} & \multicolumn{1}{l}{}  & Fluency & 0.9600 & - \\
& \multicolumn{1}{l}{} & \multicolumn{1}{l}{}  & Faithfulness & 1.0000 & - \\
& \multicolumn{1}{l}{} & \multicolumn{1}{l}{}  & Relevance of summary & - & 1.99 \\
& \multicolumn{1}{l}{} & \multicolumn{1}{l}{}  & Fluency of summary & - & 2.64\\
& \multicolumn{1}{l}{} & \multicolumn{1}{l}{}  & Coherence of summary & - & 2.56 \\
& \multicolumn{1}{l}{\multirow{1}{*}{MT}} & \multicolumn{1}{l}{\multirow{1}{*}{FLORES 200}} & Completeness & 0.7100 & - \\
& \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & Fluency & 0.9900 & - \\
& \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & Sensibility & 0.7750 & - \\
\hline
\end{tabular}
\caption{Joint agreement is calculated as the percentage of times all the raters uninamously agree that the samples fulfill the criteria. Rating is calculated as the average score from 0 to 3 assigned by the raters to the samples under the given criteria.}
\label{tab:agreement}
\end{table*}
\renewcommand{\arraystretch}{1.0}


\newpage
\onecolumn
\section{Overview of Tasks and Datasets}
\label{sec:dataset_statistics}
As discussed in Section 3 of this paper, the selection of the eight tasks are presented below. We also provide Table \ref{tab:dataset_sources_licenses} describe the source datasets used for these tasks. The modifications and adaptations that were applied and the usage of the datasets for evaluation comply with their original intended uses.

\textbf{Abstractive Summarization (AS).} An LLM performing this task is given a paragraph and  is expected to summarize the content in a sentence. The model is tested not only for identifying the salient points of the text, but also paraphrasing the content into a concise and coherent text. For this task, we use XL-Sum \cite{hasan-etal-2021-xl}, a collection of annotated article-summary pairs.

\textbf{Causal Reasoning (CR).} This task requires the LLM to understand the relationship between events. In particular, the model is given a premise, a set of statements, and an instruction to determine which of the provided statements is the cause or effect of the premise. We employed Balanced COPA \cite{kavumba-etal-2019-choosing}, a dataset designed to evaluate commonsense causal reasoning with paired alternatives.

\textbf{Machine Translation (MT).} For this task, an LLM is given a text in one language, and is expected to provide the equivalent text translated into another language. In this study, we test for both English→Filipino and Filipino→English translation. This task leveraged the Filipino subset of FLORES 200 \cite{nllb2022}, which includes translations across numerous languages and domains.

\textbf{Natural Language Inference (NLI).} This is classification task where an LLM is provided two sentences (X and Y), and is expected to determine whether the sentences are related in one in the following ways: (a) X implies Y; (b) X contradicts Y; and (c) X neither implies nor contradicts Y.  This task utilized XNLI \cite{conneau2018xnli}, a dataset containing human-annotated examples for evaluating cross-lingual inference.

\textbf{Paraphrase Identification (PI).} This task requires an LLM to determine if two provided texts are paraphrased versions of each other; that is, whether both pieces of text convey the same idea. For this task, we used PAWS \cite{paws2019naacl}, which contains paraphrase and non-paraphrase pairs with high lexical overlap. 

\textbf{Question Answering (QA).} Given a passage and a question, an LLM performing this task must provide a span from the passage that answers the question. In this study, QA utilized Belebele \cite{bandarkar-etal-2024-belebele}, a multiple-choice reading comprehension dataset designed to assess understanding of passages.

\textbf{Toxicity Detection (TD) and Sentiment Analysis (SA).} These two NLU tasks both require an LLM to analyze a natural language text. The TD task requires the model to identify whether hate speech and abusive language is used in the text, while the SA task requires the model to classify the text as either positive, negative, or neutral in sentiment polarity. Both tasks are derived from the Philippine election-related tweets dataset \cite{cabasag2019hate}, which provided a resource for toxicity in political discourse. These tweets were collected during the 2016 presidential campaign. This dataset was also relabeled for sentiment analysis by three native Filipino speakers. There is substantial agreement between the annotations (Cohen’s kappa of 0.8202, Krippendorf’s alpha of 0.8268). 

\renewcommand{\arraystretch}{1.1}
\begin{table*}[htp]
\centering
\small
\begin{tabular}{llll}
\hline
\textbf{Competency} & \textbf{Task} & \textbf{Dataset} & \textbf{License} \\
\hline
\multirow{1}{*}{NLU} & PI & PAWS \cite{paws2019naacl} & CC BY 4.0 \\
& QA & Belebele \cite{bandarkar-etal-2024-belebele} & CC BY-NC 4.0 \\
& SA & PH Elections \cite{cabasag2019hate} & Unknown\\
& TD & PH Elections \cite{cabasag2019hate} & Unknown \\
\hline
\multirow{1}{*}{NLR} & CR & Balanced COPA \cite{kavumba-etal-2019-choosing} & CC BY 4.0 \\
& NLI & XNLI \cite{conneau2018xnli} & CC BY-NC 4.0 \\
\hline
\multirow{1}{*}{NLG} & AS & XL-Sum \cite{hasan-etal-2021-xl} & CC BY-NC-SA 4.0 \\
& MT & FLORES 200 \cite{nllb2022} & CC BY-SA 4.0 \\
\hline
\end{tabular}
\caption{License details for datasets used in \textsc{Batayan}.}
\label{tab:dataset_sources_licenses}
\end{table*}
\renewcommand{\arraystretch}{1.0}

\renewcommand{\arraystretch}{1.2}
\begin{table*}[htp]
    \centering
    \small
    \begin{tabular}{lllr}
        \hline
        \textbf{Competency} & \textbf{Task} & \textbf{Label} & \textbf{No. Samples} \\
        \hline
        \multirow{1}{*}{NLU} & PI  & True  & 200 \\
        & & False  & 200 \\
        & SA  & Negative (\textit{Negatibo}) & 200 \\
        & & Neutral (\textit{Neutral})  & 200 \\
        & & Positive (\textit{Positibo})  & 200 \\
        & TD  & Clean (\textit{Malinis}) & 200 \\
        & & Toxic (\textit{Mapoot})  & 200 \\
        \hline
        \multirow{1}{*}{NLR} & CR  & Cause (\textit{Sanhi})  & 200 \\
        & & Effect (\textit{Bunga})  & 200 \\
        & NLI  & Contradiction & 200 \\
        & & Entailment & 200 \\
        & & Neutral & 200 \\
        \hline
    \end{tabular}
    \caption{Class distribution per task.}
    \label{tab:dataset_distribution}
\end{table*}
\renewcommand{\arraystretch}{1.0}

\renewcommand{\arraystretch}{1.2}
\begin{table*}[htp]
    \centering
    \small
    \begin{tabular}{llrr}
        \hline
        \textbf{Competency} & \textbf{Task} & \textbf{Total No. Samples} & \textbf{Avg. No. Words/Sample} \\
        \hline
        \multirow{1}{*}{NLU} & PI  & 400  & 22.50 (Sentence 1), 22.47 (Sentence 2) \\
        & QA  & 100  & 14.75 (Question), 4.26 (Choices) \\
        & SA  & 600  & 24.32 \\
        & TD  & 400  & 20.73 \\
        \hline
        \multirow{1}{*}{NLR} & CR  & 400  & 7.24 (Premise), 6.05 (Choices) \\
        & NLI  & 600  & 23.30 (Sentence 1), 12.20 (Sentence 2) \\
        \hline
        \multirow{1}{*}{NLG} & AS  & 100  & 120.8 \\
        & MT (English Text) & 600  & 21.42 \\
        & MT (Tagalog Text) & 600  & 25.05 \\
        \hline
    \end{tabular}
    \caption{Summary of dataset statistics, including total rows and average words per text.}
    \label{tab:dataset_summary}
\end{table*}
\renewcommand{\arraystretch}{1.0}

Table \ref{tab:dataset_distribution} shows the distribution of class labels for tasks that involve classification. We On the other hand, Table \ref{tab:dataset_summary} presents quantitative statistics on dataset size and average word counts. The variation in text length reflects task-specific requirements: AS involves longer passages, while classification tasks (SA, TD, and PI) typically contain more concise text samples. The QA dataset, in particular, has relatively short questions and answer choices, mirroring real-world multiple-choice assessments. Further granularity is provided in Table \ref{tab:as_details}, which breaks down sentence, word, and character-level statistics. This statistics shows structural differences between tasks; for instance, MT has sentence-level parallelism, while CR and NLI involve distinct premise-hypothesis or question-choice relationships.

\renewcommand{\arraystretch}{1.2}
\begin{table*}[ht]
    \centering
    \small
    \begin{tabular}{lllrrr}
        \hline
        \textbf{Competency} & \textbf{Task} & \textbf{Component} & \textbf{Avg. No. Sentences} & \textbf{Avg. No. Words} & \textbf{Avg. No. Characters} \\
        \hline
        \multirow{1}{*}{NLU} & PI & Sentence 1 & 1.0 & 22.50 & 123.01 \\
        & PI & Sentence 2 & 1.0 & 22.47 & 122.99 \\
        & QA & Question & 1.0 & 14.75 & 80.62 \\
        & QA & Choice 1 & 1.0 & 4.66 & 29.10 \\
        & QA & Choice 2 & 1.0 & 4.61 & 28.60 \\
        & QA & Choice 3 & 1.0 & 4.76 & 29.74 \\
        & QA & Choice 4 & 1.0 & 3.0 & 28.58 \\
        & SA & Text & 2.21 & 24.32 & 120.44 \\
        & TD & Text & 2.45 & 20.73 & 99.37 \\
        \hline
        \multirow{1}{*}{NLR} & CR & Premise & 1.0 & 7.24 & 37.55 \\
        & CR & Choice 1 & 1.0 & 6.05 & 30.91 \\
        & CR & Choice 2 & 1.0 & 6.05 & 30.62 \\
        & NLI & Sentence 1 & 1.0 & 23.30 & 127.88 \\
        & NLI & Sentence 2 & 1.0 & 12.20 & 67.04 \\
        \hline
        \multirow{1}{*}{NLG} & AS & Text & 5.43 & 120.8 & 666.18 \\
        & MT & English Text & 1.12 & 21.42 & 113.07 \\
        & MT & Filipino Text & 1.12 & 25.05 & 140.58 \\
        \hline
    \end{tabular}
    \caption{Detailed dataset statistics, including sentence, word, and character counts.}
    \label{tab:as_details}
\end{table*}
\renewcommand{\arraystretch}{1.0}


\newpage
\onecolumn
\section{Prompt Templates}
\label{sec:prompt_templates}

\renewcommand{\arraystretch}{1.2}
\small
\begin{longtable}{p{0.5cm} p{7.5cm} p{7.5cm}}
\hline
\textbf{Task} & \textbf{Filipino Prompt Template} & \textbf{SEA-HELM English Prompt Template}
\\
\hline
PI & 
\begin{lstlisting}
Bibigyan ka ng dalawang pangungusap, SENTENCE_1 
  at SENTENCE_2. Tukuyin kung alin sa sumusunod
  na pahayag ang pinaka-angkop para sa 
  SENTENCE_1 at SENTENCE_2.
A: Paraprase ang SENTENCE_2 ng SENTENCE_1.
B: Hindi paraprase ang SENTENCE_2 ng SENTENCE_1.

Sumagot gamit ang sumusunod na format.
Sagot: $OPTION
Palitan ang $OPTION ng napiling sagot. Gumamit
  lang ng titik A o B sa sagot mo.
  {fewshot_examples}

SENTENCE_1:
```
{sentence1}
```
SENTENCE_2:
```
{sentence2}
```
\end{lstlisting}
&
\begin{lstlisting}
You will be given two sentences, SENTENCE_1 and 
  SENTENCE_2. Determine which of the following
  statements applies to SENTENCE_1 and
  SENTENCE_2 the best.
A: SENTENCE_2 is a paraphrase of SENTENCE_1.
B: SENTENCE_2 is not a paraphrase of SENTENCE_1.

Answer only using the following format:
Answer: $OPTION
Replace $OPTION with the selected option. Use
  the letters A or B only as the answer.
  {fewshot_examples}

SENTENCE_1:
```
{sentence1}
```
SENTENCE_2:
```
{sentence2}
```
\end{lstlisting}
\\
\hline
QA & 
\begin{lstlisting}
Bibigyan ka ng isang talata, isang tanong, at
  apat na pagpipiliang sagot. Sumagot base sa
  talata sa pamamagitan ng pagpili ng isa sa
  mga opsiyong ibinigay.

Sumagot gamit ang sumusunod na format:
Sagot: $OPTION
Palitan ang $OPTION ng napiling sagot. Gumamit
  lang ng titik A, B, C, o D sa sagot mo.
  {fewshot_examples}

Talata:
```
{text}
```
Tanong: {question}
A: {choice1}
B: {choice2}
C: {choice3}
D: {choice4}
\end{lstlisting}
&
\begin{lstlisting}
You will be given one paragraph, one question
  and four answer options. Based on the
  paragraph, answer by choosing one of the
  options given.

Answer only using the following format:
Answer: $OPTION
Replace $OPTION with the selected option. Use
  the letter A, B, C, or D as the answer.
  {fewshot_examples}

Paragraph:
```
{text}
```
Question: {question}
A: {choice1}
B: {choice2}
C: {choice3}
D: {choice4}
\end{lstlisting}
\\
\hline
SA & 
\begin{lstlisting}
Ano ang sentimyento sa sumusunod na 
  pangungusap? Sumagot gamit ng isa sa mga
  sumusunod na pagpipilian: Positibo, Negatibo,
  o Neutral. 

Sumagot gamit ang sumusunod na format:
Sagot: $OPTION
Palitan ang $OPTION ng napiling sagot.
  {fewshot_examples}

Pangungusap:
```
{text}
```
\end{lstlisting}
&
\begin{lstlisting}
What is the sentiment of the following 
  sentence? Use one of the choices below:
  Positive, Negative, or Neutral.

Answer only using the following format:
Answer: $OPTION
Replace $OPTION with the selected option.
  {fewshot_examples}

Sentence:
```
{text}
```
\end{lstlisting}
\\
\hline
TD & 
\begin{lstlisting}
Isa kang toxicity detector. Nangangahulugan 
  ang mga sumusunod na label:
Malinis: Walang toxicity.
Mapoot: May toxicity at direktang pag-atake sa 
  isang tao.  

Batay sa kahulugan ng mga label, iuri ang mga
  sumusunod na pangungusap gamit lang ng isang
  salita: Malinis o Mapoot. 

Sumagot gamit ang sumusunod na format:
Sagot: $LABEL
Palitan ang $LABEL ng napiling label.
  {fewshot_examples}

Pangungusap:
```
{text}
```
\end{lstlisting}
&
\begin{lstlisting}
You are a toxicity detector. The following
  labels are defined as follows:
Clean: There is no toxicity.
Toxic: There is toxicity and direct attacks
  towards a person. 

Based on the label definitions, classify the
  following sentence only using one word: Clean
  or Toxic. 

Answer only using the following format:
Answer: $LABEL
Replace $LABEL with the selected label.
  {fewshot_examples}

Sentence:
```
{text}
```
\end{lstlisting}
\\
\hline
CR & 
\begin{lstlisting}
Sumagot gamit ang sumusunod na format:
Sagot: $OPTION
Palitan ang $OPTION ng napiling sagot. Gumamit
  lang ng titik A or B sa sagot mo.
  {fewshot_examples}

Batay sa ibibigay na sitwasyon, alin sa 
  sumusunod na pagpipilian ang mas maaari
  na {sanhi/bunga}?

Sitwasyon:
```
{text}
```
Piliin ang pinaka-angkop na sagot mula sa
  sumusunod na pagpipilian:
A: {choice1}
B: {choice2}
\end{lstlisting}
&
\begin{lstlisting}
Answer only using the following format:
Answer: $OPTION
Replace $OPTION with the selected option. Use
  only the letters A or B as the answer.
  {fewshot_examples}

Based on the given situation, which of the
  following options is more likely to be the
  {cause/effect}?

Situation:
```
{text}
```
Choose the best answer from the following
  options:
A: {choice1}
B: {choice2}
\end{lstlisting}
\\
\hline
NLI & 
\begin{lstlisting}
Bibigyan ka ng dalawang pangungusap, SENTENCE_1
  at SENTENCE_2. Tukuyin kung alin sa sumusunod
  na pahayag ang pinaka-angkop para sa
  SENTENCE_1 at SENTENCE_2.
A: Kung totoo ang SENTENCE_1, dapat totoo din
  ang SENTENCE_2.
B: Sumasalungat ang SENTENCE_1 sa SENTENCE_2.
C: Kapag totoo ang SENTENCE_1, pwedeng totoo
  o hindi totoo ang SENTENCE_2. 

Sumagot gamit ang sumusunod na format.
Sagot: $OPTION
Palitan ang $OPTION ng napiling sagot. Gumamit
  lang ng titik A, B, o C sa sagot mo.
  {fewshot_examples}

SENTENCE_1:
```
{sentence1}
```
SENTENCE_2:
```
{sentence2}
```
\end{lstlisting}
&
\begin{lstlisting}
You will be given two sentences, SENTENCE_1 and
  SENTENCE_2. Determine which of the following
  statements applies to SENTENCE_1 and
  SENTENCE_2 the best.
A: If SENTENCE_1 is true, SENTENCE_2 must be
  true.
B: SENTENCE_1 contradicts SENTENCE_2.
C: When SENTENCE_1 is true, SENTENCE_2 may or
  may not be true.

Answer only using the following format:
Answer: $OPTION
Replace $OPTION with the selected option. Use
  the letters A, B or C only as the answer.
  {fewshot_examples}

SENTENCE_1:
```
{sentence1}
```
SENTENCE_2:
```
{sentence2}
```
\end{lstlisting}
\\
\hline
AS & 
\begin{lstlisting}
Ibuod ang sumusunod na artikulong Filipino sa
  isang talata na may isa o dalawang
  pangungusap.

Sumagot gamit ang sumusunod na format:
Buod: $SUMMARY
Palitan ang $SUMMARY ng buod.
  {fewshot_examples}

Artikulo:
```
{text}
```
\end{lstlisting}
&
\begin{lstlisting}
Summarize the following {language} article into
  a paragraph with 1 or 2 sentences.

Answer only using the following format:
Summary: $SUMMARY
Replace $SUMMARY with the summary.
  {fewshot_examples}

Article:
```
{text}
```
\end{lstlisting}
\\
\hline
MT & 
\begin{lstlisting}
Isalin ang sumusunod na teksto sa {language}.

Sumagot gamit ang sumusunod na format:
Salin: $TRANSLATION
Palitan ang $TRANSLATION ng isinalin na teksto.
  {fewshot_examples}

Teksto: 
```
{text}
```
\end{lstlisting}
&
\begin{lstlisting}
Translate the following text into {language}.

Answer only using the following format:
Translation: $TRANSLATION
Replace $TRANSLATION with the translated text.
  {fewshot_examples}

Text:
```
{text}
```
\end{lstlisting}
\\
\hline
\caption{Prompt templates used in evaluating LLMs on \textsc{Batayan}. Their corresponding English-language prompt templates are also provided.}
\label{tab:prompts}
\end{longtable}
\renewcommand{\arraystretch}{1.0}

\newpage
\section{Experimental Setup}
\label{sec:experimental_setup}
\normalsize

The default evaluation setting for \textsc{Batayan} is zero-shot prompting, and performance scores are calculated based on a single run. Despite being a comprehensive benchmark, \textsc{Batayan} requires very few resources to execute. For our experiments, we allocated a computational budget of 0.25 GPU hours on 1 H100 GPU per model, totaling to 2 H100 GPU hours per run of \textsc{Batayan}.

Table \ref{tab:model_sizes} reports the number of parameters in the models evaluated, while Table \ref{tab:inference_parameters} details the decoding parameters used in the experiments.

\renewcommand{\arraystretch}{1.1}
\begin{table*}[htp]
\centering
\small
\begin{tabular}{lr}
\hline
Models & No. parameters \\
\hline
aisingapore/gemma2-9b-cpt-sea-lionv3-instruct   & 9B \\
aisingapore/llama3.1-8b-cpt-sea-lionv3-instruct & 8B \\
CohereForAI/aya-expanse-8b                      & 8B \\
google/gemma-2-9b-it                            & 9B \\
meta-llama/Llama-3.1-8B-Instruct                & 8B \\
Qwen/Qwen2.5-7B-Instruct                        & 7B \\
sail/Sailor2-8B-Chat                            & 8B \\
SeaLLMs/SeaLLMs-v3-7B-Chat                      & 7B \\
\hline
\end{tabular}
\caption{Sizes of models used in experiments.}
\label{tab:model_sizes}
\end{table*}
\renewcommand{\arraystretch}{1.0}

\renewcommand{\arraystretch}{1.2}
\begin{table*}[htp]
\centering
\small
\begin{tabular}{llrrrrr}
\hline
\textbf{Competency} &\textbf{Task} & \texttt{max\_tokens} & \texttt{temperature} & \texttt{top\_p} & \texttt{top\_k} & \texttt{repetition\_penalty} \\
\hline
\multirow{1}{*}{NLU} & PI & 32 & 0.0 & 1.0 & 1.0 & 1.0 \\
& QA & 32 & 0.0 & 1.0 & 1.0 & 1.0 \\
& SA & 32 & 0.0 & 1.0 & 1.0 & 1.0 \\
& TD & 32 & 0.0 & 1.0 & 1.0 & 1.0 \\
\hline
\multirow{1}{*}{NLR} & CR & 32 & 0.0 & 1.0 & 1.0 & 1.0 \\
& NLI & 32 & 0.0 & 1.0 & 1.0 & 1.0 \\
\hline
\multirow{1}{*}{NLG} &AS & 512 & 0.3 & 1.0 & 1.0 & 1.0 \\
& MT & 256 & 0.0 & 1.0 & 1.0 & 1.0 \\
\hline
\end{tabular}
\caption{Inference Parameters per task.}
\label{tab:inference_parameters}
\end{table*}
\renewcommand{\arraystretch}{1.0}

\newpage
\onecolumn
\section{Dataset Examples}
\label{sec:dataset_examples}
\begin{table}[h]
    \centering
    \small
    \renewcommand{\arraystretch}{1.4}
    \begin{tabular}{p{2cm} p{1cm} p{12cm}}
        \hline
        \textbf{Competency} & \textbf{Task} & \textbf{Example} \\
        \hline
        NLU & PI & \textbf{Sentence 1:} \textit{Wala pang isang taon matapos ang kaniyang nagdaang kasal, pinakasalan din ni Charlemagne si Desiderata at tinanggihan ang 13-anyos na Swabian na nagngangalang Hildegard.} \\ 
        & & \textbf{Sentence 2:} \textit{Pinakasalan ni Charlemagne si Desiderata wala pang isang taon pagkatapos ng kanyang kasal at isinawalang-bahala ang isang 13 taong gulang na Swabian na nagngangalang Hildegard.} \\
        & & \textbf{Label:} \textit{Paraprase} (Paraphrase) \\ \cline{3-3}
        & QA & \textbf{Text:} \textit{Lumipat ang mga hukbong Coalition at Afghan sa lugar na iyon upang tiyaking ligtas ang lokasyon, at nagpadala ng iba pang eroplano ng koalisyon upang tumulong. Naganap ang pagbagsak sa mataas na bahagi ng kabundukan, at pinaniniwalaang resulta ng pagbabaril ng kalaban. Isang malaking hamon ang masamang panahon at malubak na daan sa paghahanap.} \\ 
        & & \textbf{Question:} \textit{Ano ang pinaniniwalaang dahilan ng pagbagsak?} \\ 
        & & \textbf{Choices:} (1) \textit{Pangit na daan} (2) \textit{Masamang sunog} (3) \textit{Mabundok na lupain} (4) \textit{Masamang lagay ng panahon} \\
        & & \textbf{Label:} 1 \\ \cline{3-3}
        & SA & \textbf{Text:} \textit{Pucha.. PURO DILAWAN DITO SA REDDIT AH HAHAHA.. AKALA NYO NAMAN ANG LAKI NA NG ACCOMPLISHMENT NYO DAHIL SA VIDEO NA YAN MGA GUNGGONG!! HAHAHA \#DU30 PA RIN!! MGA TAE KAYO!} \\ 
        & & \textbf{Label:} \textit{Negatibo} (Negative) \\ \cline{3-3}
        & TD & \textbf{Text:} \textit{Ayun, nag-Filipino rin si Poe, ang presidente ko! Kitang-kita ang kanyang platapormang maka-mahirap at maka-tao. \#PiliPinasDebates2016} \\ 
        & & \textbf{Label:} \textit{Malinis} (Clean) \\
        \hline
        NLR & CR & \textbf{Text:} \textit{Hindi tinanggap ang tsekeng ginawa ko.} \\ 
        & & \textbf{Question:} \textit{Sanhi} (Cause) \\ 
        & & \textbf{Choices:} (0) \textit{Walang laman ang bank account ko.} (1) \textit{Tumaas ang aking suweldo.} \\
        & & \textbf{Label:} 0 \\ \cline{3-3}
        & NLI & \textbf{Sentence 1:} \textit{Di mo ba natatandaan? Pupunta tayo ngayong araw sa birthday ni Tita Basia.} \\ 
        & & \textbf{Sentence 2:} \textit{Hindi kami pupunta sa birthday party ni Tita Basia ngayon.} \\ 
        & & \textbf{Label:} Contradiction \\
        \hline
        NLG & AS & \textbf{Article:} \textit{Kinumpirma noong nakaraang buwan na humawa na ang virus sa isang tupa sa isla matapos isilang nang patay at wala sa hugis ang limang kordero sa isang bukid. Sinabi ng state veterinary officer na malamang na ang birus ay sanhi ng windborne midges.} \\ 
        & & \textbf{Summary:} \textit{Kinumpirma ng mga pagsusuri noong nakaraang buwan na nahawahan ng Schmallenberg virus ang mga tupa sa isla.} \\ \cline{3-3}
        & MT & \textbf{Text:} Former U.S. Speaker of the House Newt Gingrich came in second with 32 percent. \\ 
        & & \textbf{Translation:} \textit{Pumangalawa ang dating Speaker of the House ng U.S. na si Newt Gingrich nang may 32 porsyento.} \\
        \hline
    \end{tabular}
    \caption{Example instances for each task in \textsc{Batayan}.}
    \label{tab:dataset_examples}
\end{table}


\end{document}
