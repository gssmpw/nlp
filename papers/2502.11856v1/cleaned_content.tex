\section{Near-discrete language processing in deep learning models}
\label{sec:syntax}

\input{cleaned_grammar-and-transformer}

Figure~\ref{fig:main-fig} schematically illustrates the contrast between symbolic formalisms and deep learning architectures regarding syntactic processing: while symbolic formalisms are entirely discrete, neural networks afford both continuous and near-discrete processes.
However, what counts as near-discrete behavior in the context of neural networks?
In my view, it is the existence of a small sub-unit of the network that is causally involved in encoding or processing a single piece of linguistic information in an interpretable fashion.
\footnote{This definition does not imply that this sub-unit need be the only one involved in the relevant behavior; see Section~\ref{sec:discussion} for discussion.}

An illustrative example is \citet{bau2019identifying}, who identified individual neurons associated to specific morphosyntactic properties in a neural Machine Translation model from the pre-transformer era.
Altering the values of these neurons changes the morphosyntactic properties of the translations.
For example, in~(\ref{ex10}) modifying the activation of a single neuron in the representation of the token ``supported'' changes the tense of the French translation from past (``a appuyé'') to present (``appuie'').
Similarly, in~(\ref{ex11}), altering the activation of a single neuron changes the translation into Spanish from feminine to masculine.
\footnote{Remarkably, both are potentially correct translations, but the latter has a narrower meaning in which ``party'' must refer to a political party.}
Larger sub-units can also manifest near-discreteness, such as attention heads and what has been called ``circuits''~\cite[subgraphs within neural networks;][]{cammarata2020thread:}.

\renewcommand{\labelenumi}{(\theenumi)}
\begin{enumerate}[resume]
\item \label{ex10} The committee supported the efforts of the authorities\\
  \textit{Original}: Le Comité a appuyeé les efforts des autorités\\
  \textit{Modified}: Le Comité appuie les efforts des autorités
\item \label{ex11} The interested parties\\
  \textit{Original}: Las partes interesadas\\
  \textit{Modified}: Los partidos interesados
\end{enumerate}

It has been known for close to a decade that neural LMs encode non-trivial knowledge of syntax, including its hierarchical nature~\cite{linzen-etal-2016-assessing,gulordava-etal-2018-colorless,futrell-etal-2019-neural,rogers2021primer}. However, most earlier work used techniques such as probing, which could show THAT they encode syntactic knowledge, but not HOW.
Newer methods in mechanistic interpretability \cite[see][for a survey]{ferrando2024primerinnerworkingstransformerbased} focus on precisely this question, and it is these methods that have provided the clearest evidence for near-discreteness in some aspects of linguistic processing in deep learning models.
\footnote{The vast majority of results in this literature concerns English; in what follows, I'll refer to results for English.}
This literature provides robust evidence for near-symbolic representation and processing of both morphosyntactic properties (e.g.\ part of speech, number, gender, and tense) and syntactic relations (dependencies and agreement).

As for individual neurons, several studies have identified neurons that selectively respond to morphosyntactic properties such as part of speech, number, and tense~\cite{bau2019identifying,durrani+2023,gurnee2023findingneuronshaystackcase,gurnee2024universal}, as showcased in examples~(\ref{ex10}-\ref{ex11}) above. As another example, \citet{durrani+2023} find neurons sensitive to part of speech in three multi-lingual LLMs (BERT, RoBERTa, and XLNet); for instance, neuron 624 in layer 9 of RoBERTa responds to verbs in the simple past tense and neuron 750 in layer 2 to verbs in the present continuous tense.
Moreover, some morphosyntactic neurons are ``universal'' \cite{gurnee2024universal} in the sense that they can be found across different instantiations of the same auto-regressive LLM.
This suggests that language data provide a strong pressure for neurons encoding morphosyntactic properties to arise.

If the work reviewed up to here focuses on neurons that detect input properties, other studies look at the effects of specific neurons on the output.

\citet{geva-etal-2022-transformer} identified neurons that drastically promote the prediction of tokens with specific features, some of which are morphosyntactic in nature; for instance, neuron 1900 in layer 8 of GPT2 increased the probability of WH words (e.g.\ ``which'', ``where'', ``who''),

and neuron 3025 in layer 6 of WikiLM the probability of adverbs (e.g.\ ``largely'', ``rapidly'', ``effectively'').
\citet{ferrando-etal-2023-explaining} identify a small set of neurons that are functionally active in making grammatically correct predictions (for instance in subject-verb agreement) in models of the GPT2, OPT, and BLOOM families.

Attention heads specializing in specific syntactic relations have also been amply shown to be present in LLMs and neural MT models~\cite{raganato-tiedemann-2018-analysis,clark-etal-2019-bert,htut2019attentionheadsberttrack,voita-etal-2019-analyzing,krzyzanowski+24}.
Figure~\ref{fig:det-noun}(a) shows the activations of head 7 in layer 6 in BERT for the sentence ``many employees are working at its giant Renton, Walsh, plant''. This head specializes in the possessive construction; in the example, the possessive determiner (``its'') sharply attends to its head noun (``plant''), in a dependency relation that has 5 intervening tokens in the surface structure.
Other heads highlighted in this literature correspond to a wide range of syntactic relations such as subject, object, prepositional complement, adjectival modifier, or adverbial modifier.
Not all heads are near-discrete; Figure~\ref{fig:det-noun}(b) depicts a head with a broad attention pattern.

\begin{figure}[tb]
  \centering

  (a)
  
  \includegraphics[width=.75\columnwidth]{attention-head-possessive-clark-et-al-2019}

  (b)
  \vspace{.1cm}

  \includegraphics[width=.75\columnwidth]{attention-head-broad-clark-et-al-2019}
  
  \caption{Near-discrete and continuous attention heads in BERT (adapted from \citet{clark-etal-2019-bert}; line thickness is proportional to amount of attention). (a) Head 7 in layer 6 tracks dependencies between possessive determiners and their head nouns dependency in a near-discrete fashion: the determiner ``its'', highlighted in red, sharply attends to its head noun ``plant''. (Note that most tokens have near-discrete attention to the [SEP] token. \citet{clark-etal-2019-bert} interpreted this as a no-op signal.) (b) Head 1 in layer 1 instead presents a broad attention pattern with no clear interpretation.}
  \label{fig:det-noun}
\end{figure}

As for circuits,
\footnote{Definition of ``circuit'' in \citet{olah2020zoom}: ``A subgraph of a neural network. Nodes correspond to neurons or directions (linear combinations of neurons). Two nodes have an edge between them if they are in adjacent layers. The edges have weights which are the weights between those neurons [...]''.}
which have only recently gained attention, a particularly relevant example in the context of our paper is \citet{wang2023interpretability}.
This study describes in detail a circuit in GPT2-small that governs the prediction of the indirect object of a sentence.

Figure~\ref{fig:main-fig} (bottom right) contains a schematic depiction of the circuit for the sentence ``When John and Mary went to the store, John gave a drink to \_\_'', where the LLM predicts ``Mary''.
This interpretable circuit corresponds to an algorithm that identifies the names in the sentence (in the example, ``John'' and ``Mary''), removes the names that appear in the second sentence (``John''), and outputs the remaining name (``Mary'').
The model does this through different attention heads that have specialized functions: 1)~Duplicate Token Heads perform duplicate token detection by attending to the duplicate token and writing its position into another head; 2)~S-Inhibition Heads remove the duplicate from Name Mover Heads by inhibiting the attention of these heads to the duplicate token; and 3)~Name Mover Heads output the remaining name by attending to previous names in the sentence and copying the name they attend to (since S-Inhibition Heads inhibit attention to the duplicate token ``John'', this name will be ``Mary'' in the example).

\citet{merullo2024circuit} provide evidence that this circuit is robust (they identify the same circuit in a larger GPT2 model) and generalizes: some of its individual components are reused on a task that is different both semantically and syntactically (it involves the generation of a word denoting the color of an object described among other objects in the preceding context).
This suggests that the uncovered circuit is at a quite high level of abstraction in terms of linguistic knowledge.
\citet{ferrando-costa-jussa-2024-similarity} contribute evidence to this effect. They show that one and the same circuit is responsible for solving subject-verb agreement in English and Spanish in the multi-lingual LLM Gemma 2B.

To sum up, the mechanistic interpretability literature provides evidence for near-discreteness in syntactic processing in different sub-units of LLMs (neurons, attention heads, circuits).

However, as discussed in Section~\ref{sec:how-discr-lang}, discreteness in language goes well beyond syntax, and is present in domains such as compositional semantics and phenomena at the syntax-semantic interface.
These domains have received much less attention so far, but the existing evidence tentatively also points towards near-discreteness.

For instance, BERT has attention heads specializing in co-reference, in which anaphoric mentions sharply attend to their antecedent~\cite[][see Figure~\ref{fig:coref}]{clark-etal-2019-bert}; and one of the already mentioned ``universal neurons'' in \citet{gurnee2024universal} selectively responds to negation.
\footnote{The emergence of discrete behavior, and prominently circuits, has been related to what has been called ``grokking'' \cite{grokking}, that is, the sudden appearance of generalization capabilities in symbolic tasks. See e.g.\ \citet{nanda2023progress} and \citet{varma2023explaininggrokkingcircuitefficiency} for discussion. Here I focus on discrete behavior in linguistic representations and processing, but of course its emergence in learning is an exciting topic for further study.}

\begin{figure}[tb]
  \centering
  \includegraphics[width=.75\columnwidth]{attention-head-co-reference-clark-et-al-2019}
  
  \caption{BERT's attention head tracks co-reference dependencies (head 5 in layer 4); adapted from \citet{clark-etal-2019-bert}. The anaphoric pronoun ``her'' sharply attends to antecedent ``she''.}
  \label{fig:coref}
\end{figure}

\section{Discussion: LLMs as a synthesis}
\label{sec:discussion}

The previous section has discussed near-discrete encoding and processing of linguistic information in LLMs. However, as mentioned in the introduction, deep learning models can flexibly switch between discrete and distributed modes ---and everything in between (see near-discrete vs continuous attention in Figure~\ref{fig:det-noun}).
In this, they are very different from formalisms and representations used in theoretical linguistics.

Indeed, as emphasized throughout this paper, while representations in theoretical linguistics are discrete, in LLMs they are at most \textit{near}-discrete.
Moreover, there is wide variation in the degree of discreteness exhibited with respect to different phenomena, or even within a phenomenon.
For instance, in the work cited above, \citet{durrani+2023} found drastically fewer neurons responding to the POS of function words (like determiners or numerals) than to the POS of content words (like nouns and verbs).
They conjectured that the representation of POS in the networks may be more distributed in the latter than in the former case.
Similarly, \citet{bau2019identifying} find that gender and number are represented in a more distributed fashion than tense in the NMT model they analyze.

Another crucial difference with classical formalisms in linguistics is the fact that there is a high degree of redundancy in neural networks \cite{durrani+2023}.
For instance, when \citet{wang2023interpretability} ablated the Name Mover Heads that they identified in the indirect object circuit explained above, they found that the circuit still worked to some extent. They subsequently went on to identify back-up Name Mover Heads that replaced the role of the initially identified heads.
Redundancy is a well-known property of neural networks, and one crucial for their functioning, as it allows for graceful as opposed to catastrophic degradation in behavior \cite{lecun+89}.

The flip side of redundancy is polysemanticity, that is, the fact that units respond to different properties~\cite{rumelhart1986parallel}.
For instance, in many (but not all) cases a neuron that responds to, say, tense, will also respond to some other unrelated property.
In a fine-grained analysis of GPT2-small attention heads including manual annotation, \citet{krzyzanowski+24} found that around 90\% are polysemantic.
There are advantages to polysemanticity, such as the fact that it allows networks to represent more features than they have dimensions~\cite[][call this ``superposition'']{elhage2022superposition}.

If we put the two features together (redundancy and polysemanticity), we see that each feature is represented across many individual neurons and neurons are responsible for different features.
By definition, this is what makes a representation distributed~\cite{Hinton1986}.
So why am I arguing that LLMs are a synthesis between continuous and discrete approaches?
Because, as a matter of fact, even if they could represent and process everything in a distributed fashion, they do not. They learn to process some aspects of language in a near-symbolic manner, to the point that specific interpretable algorithms can be reverse-engineered.
The 90\% figure just mentioned, from \citet{krzyzanowski+24}, implies that 10\% of the attention heads analyzed are monosemantic ---when they would not need to be, and in fact \textit{poly}semanticity has advantages, as mentioned above.

Similarly, most of the ``universal neurons'' identified by \citet{gurnee2024universal} are monosemantic, and they have clear functional roles in circuits, such as deactivating attention heads.

This stands in stark contrast to, for instance, the much more distributed representation of words in static or contextualized word embeddings.

And, indeed, the evidence for near-discrete behavior overwhelmingly comes from domains where symbolic formalisms have been the most successful, such as grammar and compositional semantics.

\section{Conclusion}

I started this piece by pointing out that a fierce battle is being fought, since the second half of the 20th century, between symbolic and distributed approaches to language and cognition.
The advent of deep learning models has added fuel to this debate, with some of its participants continuing to take sides for one or the other with maximalist positions that are, in my view, sterile.
Luckily, many scholars are instead increasingly focusing on the huge possibilities that these models bring to the table in terms of advancing scientific knowledge~\cite{manning15,warstadt2022artificial,futrell2025linguisticslearnedstopworrying}.

In this article, I have joined this latter camp, putting forth the view that LLMs are a synthesis between the two approaches with respect to how they represent and process language.

So, may it be time for peace? The research I have surveyed has only scratched the surface, and we need everyone on board to continue to make progress in our collective understanding of how language works.

