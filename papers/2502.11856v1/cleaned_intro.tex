\section{Introduction}

Since the middle of the 20th century, a fierce battle is being fought between two antagonistic approaches to language and cognition. Although the details vary, they can be broadly characterized as follows. Symbolic approaches use discrete formalisms to represent language. Examples in computational linguistics (CL) are POS tags, parse trees, and discrete word senses.
\footnote{In early work, these approaches were paired with top-down processing of linguistic data, through rule-based systems defined by hand. In later work, the processing part has instead been data-driven: data is manually annotated according to a given representation system, and a processing algorithm is induced from the data via machine learning. The latter includes modern neural networks trained for, e.g., dependency parsing.}
Continuous approaches use distributed representations, in the form of high-dimensional algebraic objects such as vectors. In CL, static word embeddings \cite[Ã  la word2vec;][]{Mikolov2013skipgram} are a prime example.

The debate has taken different forms in different fields; in cognitive science, this opposition has been dubbed classicism vs connectionism \cite{sep-connectionism}; in AI, different terms are used by different authors~\cite{Russell2020}; in linguistics, the issues underlying the divide between generative and cognitive linguists are related to this debate~\citealp{harris1993linguistics}.

The crux of the debate is that, across all these fields, some researchers focus on the rule-like behavior of language and cognition and others on its slippery nature.
However, the fact that this debate exists might be a testimony to the fact that language and cognition are \textbf{both} symbolic (or discrete) and continuous (or fuzzy) ---and everything in between (see Section~\ref{sec:how-discr-lang}).

\begin{figure}[tb]
  \includegraphics[width=\columnwidth]{sigmoid}
  \caption{Non-linear functions such as the sigmoid provide the potential for both continuous and near-discrete behavior.}
  \label{fig:sigmoid}
\end{figure}

Focusing on language, in this position paper I argue that modern LLMs support both continuous and (near-)discrete representations and processing, and thus are a \textbf{synthesis} between the two antagonistic positions.
\footnote{I center the discussion on LLMs as the most widely adopted type of model, but in the discussion I will also include other models, such as neural machine translation models. I will signal when I do.}
This may seem a strange position to adopt, since neural networks undoubtedly fall in the continuous camp.
However, something that is often overlooked in the debate is the fact that neural networks have the potential for (near-)discrete behavior.
This potential comes from the non-linearities in their architecture~\cite{minsky1988perceptrons}.
Take the sigmoid as an example (Figure~\ref{fig:sigmoid}): when its input falls near 0, the value passed on will be continuous; but when its input is larger or smaller, it will be quasi-binary.
This allows networks to learn to combine its inputs in a way that leverages the two behaviors.
Crucially, while neural network architectures allow for flexibility in behavior, what they will do with this potential in practice is an open question.

The present paper is motivated by the fact that LLMs do seem to indeed exploit the potential for quasi-symbolic behavior with respect to language: A lot of recent work within interpretability provides evidence for near-discrete representations and processes, as discussed in Section~\ref{sec:syntax}.
What is more, these representations arise in an emergent fashion; LLMs \textbf{learn} to behave in a a quasi-symbolic fashion, because that allows them to perform better at linguistic tasks.
This, in turn, may be one of the reasons for their amazing success at capturing natural language.
