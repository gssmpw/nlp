\begin{figure*}[tb]
  \centering
  \begin{tcolorbox}[title=DISCRETE]

    \begin{minipage}{0.25\textwidth}
      \centering
      \begin{tabular}{lll}
        S & \ra & NP VP\\
        VP & \ra & V NP PP\\
        NP & \ra & Det N | John | Mary\\
        PP & \ra & P NP\\
        Det & \ra & a\\
        N  & \ra & drink\\
        P  & \ra & to\\
        V & \ra & gave\\
      \end{tabular}
      
      \label{fig:grammar}
    \end{minipage} \hfill
    \begin{minipage}{0.65\textwidth}
      \centering
      \begin{forest}
        [S
        [NP [John]]
        [VP 
        [V [gave]]
        [NP 
        [Det [a]]
        [N [drink]]
        ]
        [PP 
        [P [to]]
        [NP [Mary]]
        ]
        ]
        ]
      \end{forest}
    \end{minipage}
  \end{tcolorbox}

  \begin{tcolorbox}[title=CONTINUOUS AND (NEAR-)DISCRETE]
    
    \begin{minipage}{0.25\textwidth}
      \centering
      \includegraphics[width=\textwidth]{transformer}
      
    \end{minipage} \hfill
    \begin{minipage}{0.65\textwidth}
      \centering
      \includegraphics[width=\textwidth]{ioi-test}
      
    \end{minipage}
  \end{tcolorbox}
  
  \caption{Schematic illustration of the contrast between symbolic formalisms and deep learning. Top: context-free grammar and parse tree for the sentence "John gave a drink to Mary". Bottom: transformer architecture and circuit for the fragment "When Mary and John went to the store, John gave a drink to", with prediction ``Mary'' (adapted from \citet{vaswani2017attention} and \citet{ferrando2024primerinnerworkingstransformerbased}, with permission). In the circuit, the representations are continuous (vectors), but the different components function together in an interpretable algorithm, with attention heads carrying operations such as copying (see text for details).}
   \label{fig:main-fig}
\end{figure*}

