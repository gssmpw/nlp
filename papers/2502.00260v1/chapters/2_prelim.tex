For an integer $n$, let $[n]$ denote the set $\{1,2, \cdots, n\}$. For a finite set $A$, let $|A|$ be the number of elements in $A$, and $\Delta_{A}$ denote the set of all distributions on $A$. 

\paragraph{Proper Scoring Rule} Given a finite set $\Sigset$, a scoring rule $\ps: \Sigset\times \Delta_{\Sigset} \to \mathbb{R}$ maps an element $\sigi\in\Sigset$ and a distribution $\vpr$ on $\Sigset$ to a score. A scoring rule $PS$ is {\em proper} if for any distributions $\vpr_1$ and $\vpr_2$, $\Ex_{\sigi \sim \vpr_1}[\ps(\sigi, \vpr_1)] \ge \Ex_{\sigi \sim \vpr_1}[\ps(\sigi, \vpr_2)]$ and {\em strictly proper} if the equality holds only at $\vpr_1 = \vpr_2$. 
\begin{example}
    Given a distribution $\prQ$ on a finite set $\Sigset$, let $\pr(s)$ be the probability of $s\in \Sigset$ in $\prQ$. The log score rule $\ps_L(s, \prQ) = \log (\pr(s))$. The Brier/quadratic scoring rule $\ps_B(s, \prQ) = 2\cdot \pr(s) - \prQ\cdot \prQ$. Both the log scoring rule and the Brier scoring rule are strictly proper. 
\end{example}

\subsection{Bayesian Game Model}
A Bayesian game $\inst = ([n], (\rpset_i)_{i \in [n]}, (\Sigset_i)_{i\in[n]}, (\vt_i)_{i\in [n]}, \prQ)$ is defined by the following components. 
\begin{itemize}
    \item The set of agents $[n]$. 
    \item For each agent $i$, $\rpset_i$ is the set of available actions of $i$. The action profile $\rpp = (\rp_1, \rp_2, \cdots, \rp_\ag)$ is the vector of actions of all the agents. 
    \item For each agent $i$, $\Sigset_i$ is the set of possible types of agent $i$. The type characterizes the private information agent $i$ holds, and the agent can only observe his/her type in the game. The type vector $\sigp = (\sigi_1, \sigi_2, \cdots, \sigi_\ag)$ is the vector of types of all agents. 
    \item For each agent $i$, $\vt_i: \Sigset_i \times \rpset_1\times \cdots \times \rpset_n \to \mathbb{R}$ is $i$'s utility function that maps $i$'s type and the action of all the agents to $i$'s utility. 
    \item A {\em common prior} that the types of the agents follow is a joint distribution $\prQ$. For a signal $\sigi_i$ of agent $i$, we use $\pr(\sigi_i)$ to denote the marginal prior probability that $i$'s signal is $\sigi_i$. We assume that $\pr(\sigi_i) > 0$ for any $i$ and any $\sigi_i \in \Sigset_i$. 
\end{itemize}

For each agent $i$, a (mixed) strategy $\stg_i: \Sigset_i \to \Delta_{\rpset_i}$ maps $i$ private signal to a distribution on his/her actions. A strategy profile $\stgp = (\stg_i)_{i \in [n]}$ is a vector of the strategies of all the agents. 

Given a strategy profile $\stgp$, the {\em ex-ante} expected utility of agent $i$ is 
\begin{equation*}
    \ut_i(\stgp) = \Ex_{S \sim \prQ}\ \Ex_{A}[\vt_i(\sigi_i, \rp_1, \cdots, \rp_n)\mid \stgp].
\end{equation*}

Similarly, given a strategy profile $\stgp$ and a type $\sigi_i$, the {\em \qi{}} expected utility of agent $i$ conditioned on his/her type being $\sigi_i$ is 
\begin{equation*}
    \ut_i(\stgp \mid \sigi_i) = \Ex_{S_{-i} \sim \prQ_{-i\mid \sigi_i}}\ \Ex_{A}[\vt_i(\sigi_i, \rp_1, \cdots, \rp_n)\mid \stgp],
\end{equation*}
where $\sigp_{-i}$ is the type vector of all agents except for agent $i$, and $\prQ_{-i\mid \sigi_i}$ is the joint distribution on $\sigp_{-i}$ conditioned on agent $i$'s signal being $\sigi_{i}$. 

\subsection{(Ex-ante) Bayesian \textit{k}-Strong Equilibrium}
In this paper, we focus on agents that coordinate for strategic behaviors before they know their types. This assumption relates to various constraints in real-world scenarios that prevent agents from discussions after knowing their types. 
\begin{example}
    Consider the online crowdsourcing group in Example~\ref{ex:motive}. The website requires workers to make an immediate report after seeing the task so that workers cannot communicate with each other after they know their types. (For example, workers have to submit the report in 30 seconds to reflect their intuition.) However, workers may collude on the same report before seeing the task.
\end{example}
Both equilibria share the same high-level form: there does not exist a group of $\kd$ agents and a deviating strategy such that all the deviators' expected utility in the deviation is as good as the equilibrium strategy profile and at least one deviator's expected utility strictly increases. The difference lies in the expected utility. Ex-ante Bayesian $\kd$-strong equilibrium adopts ex-ante expected utility, while Bayesian $\kd$-strong equilibrium adopts interim expected utility on every type. 

\begin{definition}[ex-ante Bayesian $\kd$-strong equilibrium]

\label{def:ex_ante}
    Given an integer $\kd \ge 1$, a strategy profile $\stgp$ is an ex-ante Bayesian $k$-strong equilibrium ($\kd$-EBSE) if there does not exist a group of agent $D$ with $|D| \le \kd$ and a different strategy profile $\stgp' = (\stg'_{\sag})$ such that 
    \begin{enumerate}
    \item for all agent $i \not \in D$, $\stg'_{\sag} = \stg_{\sag}$; 
    \item for all $\sag\in D$, $ \ut_i(\stgp') \ge  \ut_i(\stgp)$;
    \item there exists an $\sag\in D$ such that $\ut_i(\stgp') > \ut_i(\stgp)$. 
\end{enumerate}
\end{definition}

\begin{definition}[Bayesian $\kd$-strong equilibrium]
\label{def:qi}
    Given an integer $\kd \ge 1$, a strategy profile $\stgp$ is a Bayesian $\kd$-strong equilibrium ($\kd$-BSE) if there does not exist a group of agent $D$ with $|D| \le k$ and a different strategy profile $\stgp' = (\stg'_{\sag})$ such that 
    \begin{enumerate}
    \item for all agent $i \not \in D$, $\stg'_i = \stg_i$; 
    \item for every $\sag\in D$ and every $\sigi_i \in \Sigset_i$, $ \ut_i(\stgp'\mid \sigi_i) \ge  \ut_i(\stgp\mid \sigi_i)$;
    \item there exist an $i\in D$ and an $\sigi_i \in \Sigset_i$ such that $\ut_i(\stgp'\mid \sigi_i) > \ut_i(\stgp\mid \sigi_i)$. 
\end{enumerate}
\end{definition}

% \begin{definition}[$\varepsilon$-approximation $\kd$-strong \textbf{interim} Bayesian-Nash Equilibrium]
%     Given an integer $\kd \ge 1$ and a constant $\varepsilon 
%     \ge 0$, a strategy profile $\stgp$ is an $\varepsilon$-approximation $\kd$-strong interim Bayesian-Nash Equilibrium ($\varepsilon$-apx-$\kd$-strong IBNE) if there does not exist a group of agent $D$ with $|D| \le k$ and a different strategy profile $\stgp' = (\stg'_{\sag})$ such that 
%     \begin{enumerate}
%     \item for all agent $i \not \in D$, $\stg'_i = \stg_i$; 
%     \item For every $\sag\in D$, \textbf{there exists a }$\sigi_i \in \Sigset_i$ such that $ \ut_i(\stgp'\mid \sigi_i) \ge  \ut_i(\stgp\mid \sigi_i)$.
%     \item There exists a $i\in D$ and a $\sigi_i \in \Sigset_i$ such that $\ut_i(\stgp'\mid \sigi_i) > \ut_i(\stgp\mid \sigi_i) + \varepsilon$. 
% \end{enumerate}
% \end{definition}

In both solution concepts, if such a deviating group $D$ and a strategy profile $\stgp'$ exist, we say that the deviation succeeds.

When $\kd = 1$, both ex-ante Bayesian $1$-strong equilibrium and Bayesian $1$-strong equilibrium are equivalent to the Bayesian Nash equilibrium~\citep{Harsanyi67}. (See Appendix~\ref{apx:equiv}.) However, the two solution concepts are not equivalent for larger $\kd$. Example~\ref{ex:difference} illustrates a scenario in the peer prediction mechanism where the same deviation succeeds under the ex-ante Bayesian $\kd$-strong equilibrium but fails under the Bayesian $\kd$-strong equilibrium. 

We interpret the difference between the two solution concepts as different attitudes of agents towards deviations. Agents are assumed to be more conservative, i.e., unwilling to suffer loss, towards deviations under Bayesian $k$-strong equilibrium, as they will deviate only when the deviation brings them higher interim expected utility conditioned on every type. On the other hand, agents under the ex-ante Bayesian $k$-strong equilibrium will deviate once their ex-ante expected utility increases.  Proposition~\ref{prop:etoq} supports our interpretation by revealing that an ex-ante Bayesian $\kd$-strong equilibrium implies a Bayesian $\kd$-strong equilibrium. 

\begin{prop}
\label{prop:etoq}
    For every strategy profile $\stgp$ and every $1\le \kd \le \ag$, if $\stgp$ is an ex-ante Bayesian $\kd$-strong equilibrium, then $\stgp$ is a Bayesian $\kd$-strong equilibrium. 
\end{prop}
\begin{proof}
    Suppose $\stgp'$ is an arbitrary deviating profile from $\stgp$ with no more than $\kd$ deviators, and $i$ is an arbitrary deviator in $\stgp'$. 
    Since $\stgp$ is an ex-ante Bayesian $\kd$-strong equilibrium, then $\ut_i (\stgp') \le \ut_i (\stgp) $. By the law of total probability,  
    $\ut_i(\stgp) = \sum_{\sigi_i \in \Sigset_i} \prQ(\sigi_i)\cdot \ut_i(\stgp \mid \sigi_i)$. 
    Therefore, one of the following must hold: (1) for all $\sigi\in \Sigset_i$, $\ut_i (\stgp' \mid \sigi_i) = \ut_i (\stgp \mid \sigi_i)$, or (2) there exists a $\sigi\in\Sigset_i$, $\ut_i (\stgp' \mid \sigi_i) < \ut_i (\stgp \mid \sigi_i)$. In either case, the deviation fails. Therefore, $\stgp$ is a Bayesian $\kd$-strong equilibrium. 
\end{proof}

\subsection{Peer Prediction Mechanism}
In a peer prediction mechanism, each agent receives a private signal in $\Sigset = \{\ell, h\}$ and reports it to the mechanism. All the agents share the same type set $\Sigset_i = \Sigset$ and action set $\rpset_i = \Sigset$. 

$\prQ$ is the common prior joint distribution of the signals. Let $\Sigrv_{\sag}$ denote the random variable of agent $i$'s private signal.
% Formally, the probability that agent 1 has signal $\sigi_1$, agent 2 has signal $\sigi_2$, $\cdots$, and agent $\ag$ has signal $\sigi_\ag$ is $\pr(\Sigrv_1=\sigi_1, \Sigrv_2 = \sigi_2,\cdots, \Sigrv_\ag = \sigi_\ag)$. 
We assume that the common prior $\prQ$ is symmetric --- for any permutation $\pi$ on $[\ag]$, $\prQ(\Sigrv_1=\sigi_1, \Sigrv_2 = \sigi_2,\cdots, \Sigrv_\ag = \sigi_\ag)=\pr(\Sigrv_1=\sigi_{\pi(1)}, \Sigrv_2 = \sigi_{\pi(2)},\cdots, \Sigrv_\ag = \sigi_{\pi(\ag)})$. 

$\pr(\sigi)$ is the prior marginal belief that an agent has signal $\sigi$, and $\pr(\sigi \mid \sigi')$ be the posterior belief of an agent with private signal $\sigi'$ on another agent having signal $\sigi$. We also define $\vpr_{\sigi} = \pr(\cdot \mid \sigi)$ be the marginal distribution on $\Sigset$ conditioned on $\sigi$. We assume that an agent with $h$ signal has a higher estimation than an agent with $\ell$ signal on the probability that another agent has $h$ signal, i.e., $\pr(h\mid h) > \pr(h \mid \ell)$. We also assume that any pair of signals is not fully correlated, which is $\pr(h\mid \ell) > 0$ and $\pr(\ell \mid h) > 0$. 

We adopt a modified version of the peer prediction mechanism~\citep{Miller05:Eliciting} characterized by a (strictly) proper scoring rule $\ps$. The mechanism compares the report of agent $i$, denoted by $\rp_i$, with the reports of all other agents. For each agent $j$ with report $\rp_j$, the reward $i$ gains from comparison with $j$'s report is $\rwd_i(\rp_j) = \ps(\rp_j, \vpr_{\rp_i}).$
The utility of agent $i$ is the average reward from each $j$.
\begin{equation*}
    \vt_i(\sigi_i, \rpp) = \frac{1}{\ag-1}\sum_{j\in[n], j\neq i} \rwd_i(\rp_j). 
\end{equation*}
\begin{remark}
    In the original mechanism in~\citep{Miller05:Eliciting}, the reward of an agent $i$ is $\rwd_i(\rp_j)$, where $j$ is chosen uniformly at random from all other agents. We derandomize the mechanism so that it fits better into the Bayesian game framework while the expected utility of an agent is unchanged. 
\end{remark}

\begin{example}
    \label{ex:setting}
    Suppose $n = 100$. For the common prior, the prior belief $\pr(h) = 2/3$, and $\pr(\ell) = 1/3$. The posterior belief $\pr(h \mid h) = 0.8$ and $\pr(\ell \mid \ell) = 0.6$. Suppose the Brier scoring rule is applied to the peer prediction mechanism. Consider an agent $i$ with report $\rp_i = h$. Then, $i$'s reward from a peer $j$ with report $\rp_j = h$ is $\rwd_i(\rp_j) = \ps_B(h, \prQ_h) = 2\cdot \pr(h \mid h) - \pr(h\mid h)^2 - \pr(\ell \mid h)^2 = 0.92$. Similarly, $i$' reward from another peer $j'$ with report $\rp_{j'} = \ell$ is $\ps_B(\ell, \prQ_h) = -0.28$. 
\end{example}

A (mixed) strategy $\stg: \Sigset_i \to \Delta_{\rpset_i}$ maps an agent's type to a distribution on his/her action. A strategy profile $\stgp = (\stg_i)_{i \in [n]}$ is a vector of the strategies of all the agents. An agent is {\em truthful} if he/she always truthfully reports his/her private signal. Let $\stg^*$ be the truthful strategy and $\stgp^*$ be the strategy profile where all agents are truthful. 
We also represent a strategy in the form $\stg = (\bpl, \bph) \in [0, 1]^2$, where $\bpl$ and $\bph$ are the probability that an agent playing $\stg$ reports $h$ conditioned on his/her signal begin $\ell$  and $h$, respectively. The truthful strategy $\stg^* = (0, 1)$.

Given the strategy profile $\stgp$, the ex-ante expected utility of an agent $i$ is
\begin{equation*}
    \ut_i(\stgp) = \frac{1}{n-1}\sum_{j\in [n], j\neq i} \Ex_{\sigi_i \sim \prQ
    , \rp_i \sim \stg_i(\sigi_i)} \Ex_{\sigi_j \sim \vpr_{\sigi_i}, \rp_j \sim \stg_j(\sigi_j)} \rwd_i(\rp_j). 
\end{equation*}

Given a strategy profile $\stgp$ and a type $\sigi_i$, the \qi{} expected utility of an agent $i$ conditioned on his/her type being $\sigi_i$ is 
\begin{equation*}
    \ut_i(\stgp\mid \sigi_i) = \frac{1}{n-1}\sum_{j\in [n], j\neq i} \Ex_{\rp_i \sim \stg_i(\sigi_i)} \Ex_{\sigi_j \sim \vpr_{\sigi_i}, \rp_j \sim \stg_j(\sigi_j)} \rwd_i(\rp_j). 
\end{equation*}

\begin{example} 
\label{ex:difference}
    We follow the setting in example~\ref{ex:setting}. Let $\stgp^*$ be the profile where all agents report truthfully. Let $D$ be a group containing $\kd = 40$ agents and $\stgp'$ be the profile where all deviators report $h$. 

    For truthful reporting, consider an agent $i$ and his/her peer $j$. The probability that both $i$ and $j$ receive (and report) signal $h$ is $\pr(h)\cdot \pr(h \mid h) = 2/3 * 0.8 = 0.533$, and $i$ will be rewarded $\ps(h, \vpr_h) = 0.92$. Other probabilities can be calculated similarly. Adding on the expectation of different pairs of signals, we can calculate the ex-ante expected utility of $i$ in truthful reporting: $\ut_i(\stgp^*) = \sum_{\sigi_i, \sigi_j \in \{\ell, h\}} \pr(\sigi_i) \cdot \pr(\sigi_j\mid \sigi_i)\cdot \ps(\sigi_j, \vpr_{\sigi_i}) = 0.627$. 

    Now we consider the expected utility of a deviator $i$ deviating profile $\stgp'$. Since all the deviators always report $h$, the expected reward $i$ gets from a deviator is $\ps(h, \vpr_h) = 0.92$. For the rewards from a truthful reporter, $i$'s expected reward is $\pr(h)\cdot \ps(h, \vpr_h) + \pr(\ell)\cdot \ps(\ell, \vpr_h) = 0.52$. Among all the other agents, $\kd - 1 = 39$ agents are deviators, and $\ag - \kd = 60$ agents are truthful reporters. Therefore, $i$'s expected utility on $\stgp'$ is $\ut_i(\stgp') = 0.682 > \ut_i(\stgp^*)$. Therefore, the deviation succeeds under the ex-ante Bayesian $\kd$-strong equilibrium. 

    However, the deviation fails under the Bayesian $\kd$-strong equilibrium. The truthful expected utility conditioned on $i$'s signal is $\ell$ is $\ut_i(\stgp^* \mid \ell) = \sum_{\sigi_j \in \{\ell, h\}} \pr(\sigi_j\mid \ell)\cdot \ps(\sigi_j, \vpr_{\ell}) = 0.52$. On the other hand, when agents deviate to $\stgp'$, $i$'s reward from a truthful agents becomes $\sum_{\sigi_j \in \{\ell, h\}} \pr(\sigi_j\mid \ell)\cdot \ps(\sigi_j, \vpr_{h}) = 0.2$. Therefore, $i$'s interim expected utility $\ut_i(\stgp' \mid \ell) = 0.484 < \ut_i(\stgp^* \mid \ell).$ 
\end{example}