
\section{Binary Voting}

\Qishen{In this section we consider all four type of voters. We care about strong equilibrium where no agents play weakly dominated strategies.}

\subsection{Setting}
$\ag$ agents vote for two candidates $\bA$ and $\bR$. The majority rule with threshold $\Thd$ is applied, so the candidates with more than $\Thd \cdot \ag$ votes becomes the winner. There is a world state (ground truth) $\Wosrv \in \{L, H\}$ that affects the preferences of the agents. The common prior on the world state is denoted by $P_H$ and $P_L$. The world state is not directly observable. Instead, each agents receives private signals $\sig \in \{\ell, h\}$ whose distribution depends on the world states. The signals of different agents are i.i.d conditioned on the world state. We use $P_hH$ to denote the probability that an agent receives $h$ signal when the world state is $H$. Other probabilities are defined similarly. 

For each agent $i$, the utility function $\vt_i: \{H, L\}\times \{\bA, \bR\} \to \{0, 1, \cdots, B\}$ characterizes the preference of the agent. Agents can be categorized into four types based on their preferences under different world states. There are two types of pre-determined agents. A friendly agent always prefers $\bA$, and an unfriendly agent always prefers $\bR$. There are two types of contingent agents. A majority contingent agent (abbreviated as a majority agent) prefers $\bA$ in world state $H$ and $\bR$ in world state $L$, and a minority contingent agent (abbreviated as a minority agent) prefers $\bR$ in world state $H$ and $\bA$ in world state $L$. The constraint on the utility for each type of agents is in Table~\ref{tbl:utility}.

Let $\agf$, $\agu$, $\aga$, and $\agi$ be the (approximated) fraction of friendly, unfriendly, majority, and minority agents, respectively. For a setting of $\ag$ agents, $\lfloor \agf \cdot \ag\rfloor$ agents are friendly, $\lfloor \agu \cdot \ag\rfloor$ agents are unfriendly, $\lfloor \aga \cdot \ag \rfloor$ agents are majority type, and the rest agents are minority type. 

\begin{table}[htbp]
\begin{tabular}{@{}llllll@{}}
\toprule
\multicolumn{2}{l}{Type} & Fraction & \begin{tabular}[c]{@{}l@{}}Preference\\ in state $H$\end{tabular} & \begin{tabular}[c]{@{}l@{}}Preferences \\ in state $L$\end{tabular} & Utility constraint \\ \midrule
\multirow{2}{*}{Pre-determined} & Friendly & $\agf$ & $\bA$ & $\bA$ & \begin{tabular}[c]{@{}l@{}}$\vt(H,\bA) > \vt(H, \bR)$\\ $\vt(L,\bA) > \vt(L, \bR)$\end{tabular} \\ \cmidrule(l){2-6} 
 & Unfriendly & $\agu$ & $\bR$ & $\bR$ & \begin{tabular}[c]{@{}l@{}}$\vt(H,\bA) < \vt(H, \bR)$\\ $\vt(L,\bA) < \vt(L, \bR)$\end{tabular} \\ \midrule
\multirow{2}{*}{Contingent} & Majority & $\aga$ & $\bA$ & $\bR$ & \begin{tabular}[c]{@{}l@{}}$\vt(H,\bA) > \vt(H, \bR)$\\ $\vt(L,\bA) < \vt(L, \bR)$\end{tabular} \\ \cmidrule(l){2-6} 
 & Minority & $\agi$ & $\bR$ & $\bA$ & \begin{tabular}[c]{@{}l@{}}$\vt(H,\bA) < \vt(H, \bR)$\\ $\vt(L,\bA) > \vt(L, \bR)$\end{tabular} \\ \bottomrule
\end{tabular}
\caption{\label{tbl:utility} fraction, preference, and utility constraints on four types of agents.}
\end{table}

A strategy $\stg$ of an agent $i$ maps their signal to a distribution on $\{\bA,\bR\}$, denoting their action. An agent vote informatively if they always vote for $\bA$ when receiving signal $h$ and always vote for $\bR$ when receiving signal $\ell$. A strategy profile $\stgp$ is a vector of the strategies of all the agents. For an agent $i$, let $\bpl^i$ and $\bph^i$ be the probability that $i$ votes for $\bA$ when his/her private signal is $\ell$ and $h$, respectively. 

The {\em informed majority decision} is the decision to be made if all the agent know the world state. We assume that $\agf + \aga > \Thd$ and $\agu + \aga > 1 - \Thd$. This assumption implies that $\aga > \agi$, i.e., the``majority type'' agents are the majority in the contingent agents. In this case, the informed majority decision is $\bA$ in the world state $H$ and $\bR$ in the world state $L$. We believe such assumption is reasonable. If exactly one of the two inequalities hold, the informed majority decision will be the same alternative in both world states, and reaching such a decision is trivial. If both inequalities are violated, the minority type agents become the majority of the contingent agents, the informed majority decision is $\bA$ in the world state $H$ and $\bR$ in the world state $L$, which is symmetric to the scenario under the assumption. 

The {\em fidelity} of a strategy profile is the likelihood that the candidate preferred by the agents becomes the winner under this strategy profile. That is, $\bA$ becomes the winner when the world state is $H$, and $\ell$ become the winner when the world state is $\ell$. Let $\lp_{\Wosrv}^{\bA}(\stgp)$ and $\lp_{\Wosrv}^{\bR}(\stgp)$ be the probability that $\bA$ ($\bR$, respectively) becomes the winner when the world state is $W$ and the strategy profile is $\stgp$. Then the fidelity in the voting game is in the following form. 
\begin{equation*}
    \acc(\stgp) = P_H\cdot \lp_H^{\bA}(\stgp) + P_L\cdot \lp_L^{\bR}(\stgp). 
\end{equation*}

Similarly, we can define the (ex-ante) expected utility of an agent $i$ under a strategy profile $\stgp$: 
\begin{equation*}
    \ut_{i}(\stgp) =P_{L}(\lp_{L}^{\bA}(\stgp)\cdot\vt_{i}(L, \bA) + \lp_{L}^{\bR}(\stgp)\cdot \vt_{i}(L, \bR)) +  P_{H}(\lp_{H}^{\bA}(\stgp)\cdot\vt_{i}(H, \bA) + \lp_{H}^{\bR}(\stgp)\cdot \vt_{i}(H, \bR)). 
\end{equation*}

Similarly, for an agent $i$, let $\lp_{\Wosrv}^{\bA}(\stgp \mid \sigi)$ and $\lp_{\Wosrv}^{\bR}(\stgp\mid \sigi)$ be the probability that $\bA$ ($\bR$, respectively) becomes the winner when the world state is $W$, the strategy profile is $\stgp$, and agent $i$ receives signal $\sigi$. The formula does not contain $i$ because of the symmetricity of agents. Then we can define the interim expected utility of an agent $i$ with private signal $\sigi$ under strategy profile $\stgp$. 
\begin{equation*}
    \ut_{i}(\stgp) =\Pr[L\mid \sigi] \cdot (\lp_{L}^{\bA}(\stgp \mid \sigi)\cdot\vt_{i}(L, \bA) + \lp_{L}^{\bR}(\stgp\mid \sigi)\cdot \vt_{i}(L, \bR)) +  \Pr[H\mid \sigi](\lp_{H}^{\bA}(\stgp \mid \sigi)\cdot\vt_{i}(H, \bA) + \lp_{H}^{\bR}(\stgp \mid \sigi)\cdot \vt_{i}(H, \bR)). 
\end{equation*}

\paragraph{Sequence of Environments.} An \textinst{} $\instance$  contains the agent number $\ag$, the world state prior distribution $(P_L, P_H)$, the signal distributions $(P_{hH}, P_{lH})$ and $(P_{hL}, P_{lL})$, the utility functions $\{\vt_i\}_{i=1}^\ag$, and the approximated fraction of each type of agents $(\agf, \agu, \aga, \agi)$. Let $\{\instance_\ag\}_{\ag=1}^{\infty}$ (or $\{\instance_\ag\}$ for short) be a sequence of \textinst{}s, where each $\instance_\ag$ is an \textinst{} with $\ag$ agents.
The \textinst{}s in a sequence share the same prior distributions, signal distributions, and approximated fractions of each type. There are no additional assumptions on the utility functions. 

\subsubsection{Non-dominated strategies}

In the rest of this section, we will focus on strategy profiles where no agents play weakly dominated strategy. 

\begin{definition}
    A strategy $\stg_i$ is an (ex-ante) weakly dominated strategy for agent $i$ if there exists a strategy $\stg'_i$ such that, no matter what other agents play $\stgp_{-i}$, $\ut_i((\stg_i, \stgp_{-i})) \le \ut_i((\stg_i, \stgp_{-i}))$, and there exists a $\stgp_{-i}$ such that $\ut_i((\stg_i, \stgp_i)) < \ut_i((\stg_i, \stgp_{-i}))$. 
\end{definition}

We believe this restriction is mild and reasonable. Firstly, an agent playing a dominated strategy has incentives to deviate to a non-dominated strategy, which brings no utility loss at any circumstances and utility gain in some circumstances. Secondly, as shown in the following lemma, whether a strategy is (weakly) dominated can be easily determined, and playing non-dominated strategy does not bring extra computational cost and difficulties to the agents. 

\begin{lemma}
\label{lem:dominated}
    For an arbitrary agent $i$, a strategy $\stg = (\bpl, \bph)$ is NOT a weakly dominated strategy if and only if the following condition hold. 
    \begin{itemize}
        \item If $i$ is a friendly agent: $\bpl = \bph = 1$. 
        \item If $i$ is an unfriendly agent: $\bpl = \bph = 0$. 
        \item If $i$ is a majority agent: $\bpl = 0$ or $\bph = 1$. 
        \item If $i$ is a minority agent: $\bpl = 1$ or $\bph = 0$. 
    \end{itemize}
\end{lemma}

\begin{proof}
    Suppose the strategies other agents play is $\stgp_{-i}$. The only scenario where $i$'s vote can change the outcome is the pivotal case, where both alternatives need exactly one vote to pass their threshold. Therefore, different strategies $i$ plays only affect his/her expected utility conditioned on $i$ being pivotal. The ex-ante expected utility of $i$ with strategy profile $\stgp = (\stg, \stgp_{-i})$ conditioned on him/her being pivotal is as follows. Note that the posterior belief on the world state conditioned on $i$ being pivotal does not depend on $i$'s strategy. 
    \begin{align*}
        \ut_{i}(\stgp \mid \piv) =&\ \Pr[L\mid \piv, \stgp_{-i}] \cdot ((P_{hL}\cdot \bph + P_{\ell L}\cdot \bpl)\cdot\vt_{i}(L, \bA) + (P_{hL}\cdot (1-\bph) + P_{\ell L}\cdot (1-\bpl))\cdot \vt_{i}(L, \bR)) \\
        &\ +  \Pr[H\mid \piv, \stgp_{-i}]\cdot ((P_{hH}\cdot \bph + P_{\ell H}\cdot \bpl)\cdot\vt_{i}(H, \bA) + (P_{hH}\cdot (1-\bph) + P_{\ell H}\cdot (1-\bpl))\cdot \vt_{i}(H, \bR)). 
    \end{align*}

    When agent $i$ plays a different strategy $\stg' = (\bpl', \bph')$, the expected utility conditioned on pivotal can be written in the similar form. Let $\stgp' = (\stg', \stgp_{-i})$. Then, the difference between the expected utility when $i$ plays $\stg$ and $\stg'$ is exactly the expected utility difference conditioned on $i$ being pivotal.

    \begin{align*}
        \ut_i(\stgp') - \ut_i(\stgp) = &\ \ut_i(\stgp' \mid \piv) - \ut_i(\stgp \mid \piv) \\
        =&\ \Pr[L \mid \piv, \stgp_{-i}] \cdot ((P_{hL} \cdot (\bph' - \bph) + P_{\ell L} \cdot (\bpl' - \bpl))\cdot (\vt_i(L, \bA) - \vt_i(L, \bR))\\
        &\ + \Pr[H \mid \piv, \stgp_{-i}] \cdot ((P_{hH} \cdot (\bph' - \bph) + P_{\ell H} \cdot (\bpl' - \bpl))\cdot (\vt_i(H, \bA) - \vt_i(H, \bR)). 
    \end{align*}

For friendly agents, $\vt_i(L, \bA) - \vt_i(L, \bR) > 0$ and $\vt_i(H, \bA) - \vt_i(H, \bR) > 0$. When $\bph < 1$ or $\bpl < 1$, let $\bph' = \bpl' = 1$. Then $\ut_i(\stgp') - \ut_i(\stgp) > 0$, and $\stg$ is weakly dominated by $\stg'$. On the other hand, when $\bpl = \bph = 1$, for any $\stg'$, $\ut_i(\stgp') - \ut_i(\stgp) \le 0$, and $\stg$ is not a weakly dominated strategy. The reasoning for unfriendly agents resembles that for friendly agents. 

For majority contingent agents, $\vt_i(L, \bA) - \vt_i(L, \bR) < 0$ and $\vt_i(H, \bA) - \vt_i(H, \bR) > 0$. When $\bph < 1$ and $\bpl > 0$, let $\bph' = \bph + \varepsilon$ and $\bpl' = \bpl - \varepsilon\cdot \frac{P_{hL} + P_{hH}}{P_{\ell L} + P_{\ell H}}$, where $\varepsilon > 0$ is a small constant that guarantees $\bph \le 1$ and $\bpl \ge 0$. Then we have $P_{hL} \cdot (\bph' - \bph) + P_{\ell L} \cdot (\bpl' - \bpl) < 0$ and $P_{hH} \cdot (\bph' - \bph) + P_{\ell H} \cdot (\bpl' - \bpl) > 0$, which implies that $\ut_i(\stgp') - \ut_i(\stgp) > 0$ and that $\stg$ is dominated by $\stg'$. Now, without loss of genearlity, suppose $\bph = 1$. In this case, for all $\stg' \neq \stg$, either $P_{hL} \cdot (\bph' - \bph) + P_{\ell L} \cdot (\bpl' - \bpl) > 0$ or $P_{hH} \cdot (\bph' - \bph) + P_{\ell H} \cdot (\bpl' - \bpl) < 0$ hold. (If $P_{hH} \cdot (\bph' - \bph) + P_{\ell H} \cdot (\bpl' - \bpl) < 0$ does not hold and $\stg' \neq \stg$, then $\bpl' > \bpl$ must hold. Combined with the fact that $P_{hH} > P_{hL}$ and $P_{\ell H} < P_{\ell L}$, $P_{hL} \cdot (\bph' - \bph) + P_{\ell L} \cdot (\bpl' - \bpl) > 0$ holds.) Therefore, by constructing $\stgp_{-i}$ so that $\Pr[L \mid \piv, \stgp_{-i}]$ is sufficiently close to 0 or 1, we can find a scenario where $\ut_i(\stgp') - \ut_i(\stgp) < 0$, which implies that $\stgp$ is not weakly dominated by $\stgp'$. 
The reasoning for $\bpl = 0$ and that for the minority contingent agents resembles the reasoning above. 
\end{proof}

\subsection{Results}
Throughout this section, we use $\Delta$ to denote $P_{hH} - P_{hL} = P_{\ell L} - P_{\ell H} = P_{hH}\cdot P_{\ell L} - P_{hL} - P_{\ell H}$. 

\begin{theorem}
\label{thm:thresholdk}

\begin{table}[htbp]
\centering
\begin{tabular}{@{}cccc@{}}
\toprule
Conditions & $P_{hH}\cdot \aga \ge \Thd - \agf$ & $P_{hL} \cdot \aga \le \Thd - \agf - \agi$ & Otherwise \\ \midrule
\begin{tabular}[c]{@{}c@{}}$\agi \ge \frac{\Delta \cdot (\agf + \aga - \Thd)}{P_{\ell H}}$, \\ $\agi \ge \frac{\Delta \cdot (\Thd - \agf)}{P_{hH}}$\end{tabular} & $\frac{\Delta \cdot (\Thd - \agf)}{P_{hH}}$ & $\frac{\Delta \cdot ((1 - \Thd) - \agu)}{P_{\ell L}}$ & $\Delta \cdot \aga$ \\ \midrule
\begin{tabular}[c]{@{}c@{}}$\agi < \frac{\Delta \cdot (\agf + \aga - \Thd)}{P_{\ell H}}$, \\ $\agi \ge \frac{\Delta \cdot (\Thd - \agf)}{P_{hH}}$\end{tabular} & \multicolumn{3}{c}{$\frac{\Delta \cdot (\Thd - \agf)}{P_{hH}}$} \\ \midrule
\begin{tabular}[c]{@{}c@{}}$\agi \ge \frac{\Delta \cdot (\agf + \aga - \Thd)}{P_{\ell H}}$, \\ $\agi < \frac{\Delta \cdot (\Thd - \agf)}{P_{hH}}$\end{tabular} & \multicolumn{3}{c}{$\frac{\Delta \cdot ((1 - \Thd) - \agu)}{P_{\ell L}}$} \\ \midrule
\begin{tabular}[c]{@{}c@{}}$\agi < \frac{\Delta \cdot (\agf + \aga - \Thd)}{P_{\ell H}}$, \\ $\agi < \frac{\Delta \cdot (\Thd - \agf)}{P_{hH}}$\end{tabular} & \multicolumn{3}{c}{?} \\ \bottomrule
\end{tabular}
\caption{Threshold $\xi^*$ under different conditions.\label{tbl:xi}}
\end{table}
% Suppose $\agi \ge \max( \frac{\Delta \cdot (\aga + \agf - \mu)}{P_{\ell H}}, \frac{\Delta \cdot (\agu + \aga - (1 - \mu))}{P_{hL}})$. 
%     Let $\kd = \xi\cdot \ag$. For all $\xi < \max( \frac{\Delta \cdot (\aga + \agf - \mu)}{P_{\ell H}}, \frac{\Delta \cdot (\agu + \aga - (1 - \mu))}{P_{hL}})$, there exists a sequence of strategy profile $\{\stgp_\ag\}$ such that,
Let $\kd = \xi\cdot \ag$. For each condition listed in Table~\ref{tbl:xi}, for any $\xi < \xi^*$, there exists a sequence of strategy profile $\{\stgp_\ag\}$ such that,
    \begin{enumerate}
        \item For all $\ag$, no agents play weakly dominated strategies in $\stgp_\ag$,
        \item the fidelity $\acc(\stgp_\ag)$ converges to 1, and
        \item for all $\ag$, $\stgp_\ag$ is an $\varepsilon$-ex-ante Bayesian $\kd$ strong equilibrium, where $\varepsilon$ converges to 0.
    \end{enumerate}
    When $\xi = \xi^*$, no such sequence of profile exists. 
\end{theorem}


Before going to the proof, we introduce the following notion theorem from~\citep{han2023wisdom}, which serves as a useful tool to characterizes fidelities of strategy profiles. 

Given a world state $\wos$, the \exshare{} is the expected vote share the informed majority decision alternative attracts under state $\wos$ minus the threshold of the alternative. 
\begin{definition}[\bf{Excess expected vote share}]
   Given an instance of $\ag$ agents, and a strategy profile $\stgp$, let random variable $\xrv_{i}^{\ag}$ be "agent $i$ votes for $\bA$":  $\xrv_{i}^{\ag} = 1$ if agent $i$ votes for $\bA$, and $\xrv_{i}^{\ag} = 0$ if $i$ votes for $\bR$. Then the \exshare{} is defined as follows: 
\begin{align}
    \thd^{\ag}_{H} =& \frac{1}{\ag}\sum_{i=1}^{\ag}E[\xrv_{i}^{\ag}\mid H] - \Thd.\label{eq:hthdh}
\end{align}
\begin{align}
    \thd^{\ag}_{L} =&  \frac{1}{\ag}\sum_{i=1}^{\ag}E[1-\xrv_{i}^{\ag}\mid L] - (1-\Thd)\label{eq:hthdl}. 
\end{align}
Specifically, $\thd_H^{\ag}$ is the \exshare\ of $\bA$ condition on world state $H$, and $\thd_L^{\ag}$ is the \exshare\ of $\bR$ condition on world state $L$. For technical convenience, we define $\thd^{\ag} = \min(\thd^{\ag}_H, \thd^{\ag}_L).$ 
\end{definition}

\begin{theorem}
\label{thm:arbitrary}
Given an arbitrary sequence of instances and arbitrary sequence of strategy profiles $\{\stgp_{\ag}\}_{\ag=1}^\infty$, let $\thd^\ag$ be the \exshare{} for each $\stgp_{\ag}$.
\begin{itemize}
     \item If $\liminf_{\ag\to\infty} \sqrt{\ag}\cdot \thd^{\ag} = +\infty$, the fidelity of $\stgp_{\ag}$ converges to 1 , i.e., $\lim_{\ag\to\infty} \acc(\stgp_{\ag}) = 1$. 
     \item If $\liminf_{\ag\to\infty} \sqrt{\ag}\cdot \thd^{\ag} < 0$ (including $-\infty$), $\acc(\stgp_{\ag})$ does NOT converge to 1. 
     \item If $\liminf_{\ag\to\infty} \sqrt{\ag}\cdot \thd^{\ag} \ge 0$ (not including $+\infty$), and the variance of $\sum_{i=1}^{\ag} \xrv_{i}^{\ag}$ is at least proportional to $\ag$,  $\acc(\stgp_{\ag})$ does NOT converge to 1. 
\end{itemize}
\end{theorem}

\subsection{Proof of Theorem~\ref{thm:thresholdk}.}

\subsection{Lemmas}
We start from the following useful lemma. We apply this lemma to construct strategies profiles where no agents play weakly dominated strategy while we only care about the average of their strategies to calculate fidelity. 
\begin{lemma}
    (Majority agents.) Let $A'$ be a group of majority agents such that $|A'| = \Theta (\ag)$ and no agents play weakly dominated strategy. Let $\abpl = \frac{1}{|A'|}=\sum_{i \in A'} \bpl^i$ and $\abph = \frac{1}{|A'|}=\sum_{i \in A'} \bph^i$ be the average strategy played by agents in $A'$. Then for any $\bpl \le \bph$, there exists a set of strategies $(\stg_i)_{i\in A'}$ such that $|\bpl - \abpl| = O(\frac1{\ag})$ and $|\bph - \abph| = O(\frac1{\ag})$. 

    (Minority agents.) Let $I'$ be a group of minority agents such that $|I'| = \Theta (\ag)$ and no agents play weakly dominated strategy. Let $\abpl = \frac{1}{|I'|}=\sum_{i \in I'} \bpl^i$ and $\abph = \frac{1}{|I'|}=\sum_{i \in I'} \bph^i$ be the average strategy played by agents in $I'$. Then for any $\bpl \le \bph$, there exists a set of strategies $(\stg_i)_{i\in I'}$ such that $|\bpl - \abpl| = O(\frac1{\ag})$ and $|\bph - \abph| = O(\frac1{\ag})$. 
\end{lemma}

\begin{proof}
    We show the proof for majority agents. The reason for minority agents will be similar. Consider the following strategy set: $\lfloor \bpl \cdot |A'| \rfloor$ agents play $(1, 1)$ (always vote for $\bA$, $\lfloor (1 - \bph)\cdot |A'| \rfloor$ agents play $(0, 0)$ (always vote for $\bR$, and other agents play $(0, 1)$ (vote informatively). Then it is not hard to verify that $|\bpl - \abpl| = O(\frac1{\ag})$ and $|\bph - \abph| = O(\frac1{\ag})$. 
\end{proof}

% \subsubsection{Case 4.}
% We first consider the scenario where $\agi < \frac{\Delta \cdot (\agf + \aga - \Thd)}{P_{\ell H}}$ and $\agi < \frac{\Delta \cdot (\Thd - \agf)}{P_{hH}}$. We show that there exists strategy profile satisfying all the conditions. 

% For an arbitrary $\ag$, consider the following strategy profile $\stgp$.
% \begin{itemize}
%     \item All friendly agents always vote for $\bA$, and all unfriendly agents always vote for $\bR$. 
%     \item All the minority agents always vote for $\bR$. 
%     \item All the majority agents play non-dominated strategy such that the average $\abpl = \frac{1}{\aga} (\Thd - \agf - \frac{\agi\cdot P_{\ell H}}{\Delta}) - (P_{hH} + P_{hL})\cdot \delta$ and $\abph = \frac{1}{\aga} ( \mu - \agf + \frac{\agi \cdot P_{\ell H}}{\Delta}) + (P_{\ell L} + P_{\ell H})\cdot \delta$, where $\delta > 0$ is a small constant that guarantees that $\abpl \ge 0 $ and $\abph \le 1$. 
% \end{itemize}
% It is not hard to verify that no agents play weakly dominated strategy. 
% The \exshare{} of $\stgp$ (without the $O(\frac1{\ag})$ rounding error) is 

% \begin{align*}
%     \thd_H^\ag =&\ \aga \cdot \Delta \cdot \delta. \\
%     \thd_L^\ag = &\ \agi + \aga \cdot \Delta \cdot \delta. 
% \end{align*}

% Therefore $\lim_{\ag \to \infty} \sqrt{\ag} \cdot \thd^{\ag} = +\infty$, and by applying the first case of Theorem~\ref{thm:arbitrary}, the fidelity of $\stgp$ converges to 1 as $\ag$ goes to infinity. 

% Now we show that $\stgp$ is an $\varepsilon$-ex-ante Bayesian $\ag$-strong equilibrium. Let $\varepsilon_0 > 0$ be a constant, we show that for all sufficiently large $\ag$, $\stgp$ is an $2B(B+1)\varepsilon_0$-ex-ante Bayesian $\ag$-strong equilibrium.

% Firstly, we assume that $\ag$ is sufficiently large so that 
% \begin{itemize}
%     \item $\lp_H^\bA(\stgp) \ge 1 - \frac{\varepsilon_0\cdot P_L\cdot P_H}{2B(B+1)}$,
%     \item $\lp_L^{\bR}(\stgp) \ge 1 - \frac{\varepsilon_0\cdot P_L\cdot P_H}{2B(B+1)}$, 
%     \item $2\exp(-2(\aga \cdot \Delta \cdot \delta)^2 \cdot \ag) < \frac{\varepsilon_0\cdot P_L\cdot P_H}{2B(B+1)}$.
% \end{itemize}

% Consider a different strategy profile $\stgp'$ where no more than $\kd$ agents play a different strategy. The utility difference of an arbitrary agent $i$ on $\stgp$ and $\stgp'$ will be 
% \begin{align*}
%         \ut_{i}(\stgp') - \ut_{i}(\stgp) = &\ P_H\cdot (\lp_{H}^{\bA}(\stgp') - \lp_{H}^{\bA}(\stgp))(\vt_{i}(H, \bA) - \vt_{i}(H,\bR))\\
%         &\ + P_L\cdot (\lp_{L}^{\bR}(\stgp') - \lp_{L}^{\bR}(\stgp))(\vt_{i}(L, \bR) - \vt_{i}(L,\bA)).
%     \end{align*}

% When $1 - \acc(\stgp') < \varepsilon_0$, we show that no type of agents can get more than $\varepsilon$ from deviation. 
% \begin{itemize}
%     \item When $i$ is a friendly agent, $\ut_{i}(\stgp') - \ut_{i}(\stgp) < P_H\cdot B\cdot \frac{\varepsilon_0\cdot P_L\cdot P_H}{2B(B+1)} + P_L\cdot B\cdot (B + 1)\cdot \frac{\varepsilon_0}{P_L}< B(B+2) \varepsilon_0$. 
%     \item When $i$ is an unfriendly agent, $\ut_{i}(\stgp') - \ut_{i}(\stgp) < P_H\cdot (B+1)\cdot \frac{\varepsilon_0}{P_H} \cdot B + P_L \cdot \frac{\varepsilon_0\cdot P_L\cdot P_H}{2B(B+1)}\cdot B <  B(B+2) \varepsilon_0$.
%     \item When $i$ is a majority type agent,
%     $\ut_{i}(\stgp') - \ut_{i}(\stgp) <P_H\cdot \frac{\varepsilon_0\cdot P_L\cdot P_H}{2B(B+1)}\cdot B + P_L \cdot \frac{\varepsilon_0\cdot P_L\cdot P_H}{2B(B+1)} \cdot B = \frac{\varepsilon_0\cdot P_L\cdot P_H}{(B+1)}.$
%     \item When $i$ is a minority type agent, 
%     $\ut_{i}(\stgp') - \ut_{i}(\stgp) < P_H \cdot (B+1)\frac{\varepsilon_0}{P_H} \cdot B + P_L\cdot (B+1)\frac{\varepsilon_0}{P_L} \cdot B = 2B(B+1)\cdot \varepsilon_0$. 
% \end{itemize}
% Therefore, such deviation cannot succeed. 

% Now suppose $1 - \acc(\stgp') \ge \varepsilon_0$. First, we show that majority agents have no incentives to deviate as the fidelity decrease. Then we show that for a deviating group containing no majority agents, $\lp_L^{\bR} (\stgp') \ge 1 - \frac{\varepsilon_0\cdot P_L\cdot P_H}{2B(B+1)}$. This directly implies that $\lp_H^{\bA}(\stgp') \le 1 -  \varepsilon_0$, and friendly agents have no incentives to deviate. Finally, we show that a deviating group with only unfriendly and minority agents cannot achieve $\lp_H^{\bA}(\stgp') \le 1 -  \varepsilon_0$. 

% \paragraph{Majority agents.} Given that $1 - \acc(\stgp') \ge \varepsilon_0$, either $1 - \lp_H^{\bA}(\stgp') \ge \varepsilon_0$ or $1 - \lp_L^{\bR}(\stgp') \ge \varepsilon_0$ holds. Without loss of generality, suppose $1 - \lp_H^{\bA}(\stgp') \ge \varepsilon_0$, then $\ut_{i}(\stgp') - \ut_{i}(\stgp) < -P_H\cdot (\varepsilon_0 - \frac{\varepsilon_0\cdot P_L\cdot P_H}{2B(B+1)})\cdot 1 + P_L \cdot \frac{\varepsilon_0\cdot P_L\cdot P_H}{2B(B+1)} \cdot B =-\frac12 P_H \cdot \varepsilon_0$. Therefore, majority agents will not join the deviating group. 

% Now we discuss different cases of $\lp_{H}^{\bA}(\stgp')$ and $\lp_{L}^{\bR}(\stgp')$. In each case we rule out both type of pre-determined agents and show that a deviating group with solely minority agents cannot succeed.  Let the \exshare{} of $\stgp'$ be denoted by $\thd'_H$ and $\thd'_L$.

% Suppose $\lp_{H}^{\bA}(\stgp') \le  1- \varepsilon_0$. The expected utility different of an friendly agent $i$, $\ut_{i}(\stgp') - \ut_{i}(\stgp) < -P_H\cdot (\varepsilon_0 - \frac{\varepsilon_0\cdot P_L\cdot P_H}{2B(B+1)})\cdot 1 + P_L \cdot \frac{\varepsilon_0\cdot P_L\cdot P_H}{2B(B+1)} \cdot B =-\frac12 P_H \cdot \varepsilon_0$. Therefore, no friendly agents have incentives to deviate. Then recall that all unfriendly and minority agents always vote for $\bR$ in $\stgp$ Therefore, $\thd'_H \ge \thd^\ag_H > 0$. Then by the Hoeffding Inequality and our assumption on $\ag$, $\lp_H^{\bA}(\stgp') \ge 1 - 2\exp(-2(\thd^\ag_H)^2 \cdot \ag) \ge 1 - \frac{\varepsilon_0\cdot P_L\cdot P_H}{2B(B+1)}$. This directly contradicts with $\lp_H^{\bA}(\stgp') \le 1 -  \varepsilon_0$. Therefore, such deviation cannot succeed. 

% Now suppose $\lp_{L}^{\bR}(\stgp') \le  1- \varepsilon_0$. With similar reasoning, unfriendly agents will get strictly lower expected utility and not be in the deviating group. 

% \paragraph{Friendly agents.} Recall that for $\stgp$, $\thd^\ag_L > \xi$. Since at most $\kd = \xi\cdot \ag$ agents deviate, the \exshare{} of $\stgp'$, denoted by $\thd'_H$ and $\thd'_L$, will not deviate from that of $\stgp$ by $\xi$. Therefore, $\thd'_L \ge \thd^\ag_L - \xi > 0$. Then by the Hoeffding Inequality and our assumption on $\ag$, $\lp_L^{\bR}(\stgp') \ge 1 - 2\exp(-2(\thd^\ag_L - \xi)^2 \cdot \ag) \ge 1 - \frac{\varepsilon_0\cdot P_L\cdot P_H}{2B(B+1)}$. Since $\acc(\stgp') \le 1 - \varepsilon_0$, this directly implies that $\lp_H^{\bA}(\stgp') \le 1 -  \varepsilon_0$. Then consider the expected utility different of an friendly agent $i$, $\ut_{i}(\stgp') - \ut_{i}(\stgp) < -P_H\cdot (\varepsilon_0 - \frac{\varepsilon_0\cdot P_L\cdot P_H}{2B(B+1)})\cdot 1 + P_L \cdot \frac{\varepsilon_0\cdot P_L\cdot P_H}{2B(B+1)} \cdot B =-\frac12 P_H \cdot \varepsilon_0$. Therefore, no friendly agents have incentives to deviate. 

% \paragraph{Unfriendly and minority agents.} Recall that all unfriendly and minority agents always vote for $\bR$ in $\stgp$ Therefore, $\thd'_H \ge \thd^\ag_H > 0$. Then by the Hoeffding Inequality and our assumption on $\ag$, $\lp_H^{\bA}(\stgp') \ge 1 - 2\exp(-2(\thd^\ag_H)^2 \cdot \ag) \ge 1 - \frac{\varepsilon_0\cdot P_L\cdot P_H}{2B(B+1)}$. This directly contradicts with $\lp_H^{\bA}(\stgp') \le 1 -  \varepsilon_0$. Therefore, such deviation cannot succeed. 

% % \begin{proof}[Proof for Theorem~\ref{thm:thresholdk}]

% % Without loss of generality, suppose $\frac{\Delta \cdot (\aga + \agf - \mu)}{P_{\ell H}} \ge \frac{\Delta \cdot (\agu + \aga - (1 - \mu)}{P_{hL}}$. 

% % \paragraph{Case 1: $\xi < \frac{\Delta \cdot (\aga + \agf - \mu)}{P_{\ell H}}$.}

% % For an arbitrary $\ag$, consider the following strategy profile $\stgp$.
% % \begin{itemize}
% %     \item All friendly agents always vote for $\bA$, and all unfriendly agents always vote for $\bR$. 
% %     \item All the minority agents always vote for $\bR$. 
% %     \item For majority agents, $\bph = 1$, and $\bpl = \frac12(\frac{\mu - \agf - \aga \cdot P_{hL} - \xi}{\aga \cdot P_{\ell L}} + \frac{\mu - \agf - \aga\cdot P_{hH}}{\aga \cdot P_{\ell H}})$. 
% % \end{itemize}
% % It is not hard to verify that no agents play weakly dominated strategy. 
% % The \exshare{} of $\stgp$ (without the $O(\frac1{\ag})$ rounding error) is 
% % \begin{align*}
% %     \thd^{\ag}_H =&\ \frac{P_{\ell H}}2(\frac{\mu - \agf - \aga \cdot P_{hL} - \xi}{\aga \cdot P_{\ell L}} - \frac{\mu - \agf - \aga\cdot P_{hH}}{\aga \cdot P_{\ell H}}),\\
% %     \thd^{\ag}_L = &\ \frac{P_{\ell L}}2( \frac{\mu - \agf - \aga \cdot P_{hL} - \xi}{\aga \cdot P_{\ell L}} - \frac{\mu - \agf - \aga\cdot P_{hH}}{\aga \cdot P_{\ell H}}) + \xi.
% % \end{align*}
% % \begin{claim}
% %     $\frac{\mu - \agf - \aga \cdot P_{hL} - \xi}{\aga \cdot P_{\ell L}} > \frac{\mu - \agf - \aga\cdot P_{hH}}{\aga \cdot P_{\ell H}}$. 
% % \end{claim}
% % \begin{align*}
% %     &\ \frac{\mu - \agf - \aga \cdot P_{hL} - \xi}{\aga \cdot P_{\ell L}} > \frac{\mu - \agf - \aga\cdot P_{hH}}{\aga \cdot P_{\ell H}}\\
% %     \Leftrightarrow &\ P_{\ell H}\cdot  (\mu - \agf - \aga \cdot P_{hL} - \xi) > P_{\ell L}\cdot (\mu - \agf - \aga\cdot P_{hH})\\
% %     \Leftrightarrow &\ \Delta \cdot (\agf + \aga - \mu) - P_{\ell H}\cdot \xi > 0. 
% % \end{align*}
% % The last inequality holds by our assumption. Therefore, $\thd^{\ag}_H > 0$, and $\thd^{\ag}_L > \xi$. Therefore $\lim_{\ag \to \infty} \sqrt{\ag} \cdot \thd^{\ag} = +\infty$, and by applying the first case of Theorem~\ref{thm:arbitrary}, the fidelity of $\stgp$ converges to 1 as $\ag$ goes to infinity. 

% % % Let $e = 1 - \acc(\stgp)$. We show that $\stgp$ is an $\varepsilon$-ex-ante Bayesian $\kd$-strong equilibrium where $\varepsilon = (2B + 2)\cdot B \cdot e$ and $\kd = \xi\cdot \ag$. Consider a different strategy profile $\stgp'$ where no more than $\kd$ agents play a different strategy. The utility difference of an arbitrary agent $i$ on $\stgp$ and $\stgp'$ will be 

% % Let $\varepsilon_0 > 0$ be a constant, we show that for all sufficiently large $\ag$, $\stgp$ is an $2B(B+1)\varepsilon_0$-ex-ante Bayesian $\kd$-strong equilibrium where $\kd = \xi\cdot \ag$. 

% % Firstly, we assume that $\ag$ is sufficiently large so that 
% % \begin{itemize}
% %     \item $\lp_H^\bA(\stgp) \ge 1 - \frac{\varepsilon_0\cdot P_L\cdot P_H}{2B(B+1)}$,
% %     \item $\lp_L^{\bR}(\stgp) \ge 1 - \frac{\varepsilon_0\cdot P_L\cdot P_H}{2B(B+1)}$, 
% %     \item $2\exp(-2(\thd^\ag_H)^2 \cdot \ag) < \frac{\varepsilon_0\cdot P_L\cdot P_H}{2B(B+1)}$, and
% %     \item $2\exp(-2(\thd^\ag_L - \xi)^2 \cdot \ag) < \frac{\varepsilon_0\cdot P_L\cdot P_H}{2B(B+1)}$
% % \end{itemize}

% % Consider a different strategy profile $\stgp'$ where no more than $\kd$ agents play a different strategy. The utility difference of an arbitrary agent $i$ on $\stgp$ and $\stgp'$ will be 
% % \begin{align*}
% %         \ut_{i}(\stgp') - \ut_{i}(\stgp) = &\ P_H\cdot (\lp_{H}^{\bA}(\stgp') - \lp_{H}^{\bA}(\stgp))(\vt_{i}(H, \bA) - \vt_{i}(H,\bR))\\
% %         &\ + P_L\cdot (\lp_{L}^{\bR}(\stgp') - \lp_{L}^{\bR}(\stgp))(\vt_{i}(L, \bR) - \vt_{i}(L,\bA)).
% %     \end{align*}

% % When $1 - \acc(\stgp') < \varepsilon_0$, we show that no type of agents can get more than $\varepsilon$ from deviation. 
% % \begin{itemize}
% %     \item When $i$ is a friendly agent, $\ut_{i}(\stgp') - \ut_{i}(\stgp) < P_H\cdot B\cdot \frac{\varepsilon_0\cdot P_L\cdot P_H}{2B(B+1)} + P_L\cdot B\cdot (B + 1)\cdot \frac{\varepsilon_0}{P_L}< B(B+2) \varepsilon_0$. 
% %     \item When $i$ is an unfriendly agent, $\ut_{i}(\stgp') - \ut_{i}(\stgp) < P_H\cdot (B+1)\cdot \frac{\varepsilon_0}{P_H} \cdot B + P_L \cdot \frac{\varepsilon_0\cdot P_L\cdot P_H}{2B(B+1)}\cdot B <  B(B+2) \varepsilon_0$.
% %     \item When $i$ is a majority type agent,
% %     $\ut_{i}(\stgp') - \ut_{i}(\stgp) <P_H\cdot \frac{\varepsilon_0\cdot P_L\cdot P_H}{2B(B+1)}\cdot B + P_L \cdot \frac{\varepsilon_0\cdot P_L\cdot P_H}{2B(B+1)} \cdot B = \frac{\varepsilon_0\cdot P_L\cdot P_H}{(B+1)}.$
% %     \item When $i$ is a minority type agent, 
% %     $\ut_{i}(\stgp') - \ut_{i}(\stgp) < P_H \cdot (B+1)\frac{\varepsilon_0}{P_H} \cdot B + P_L\cdot (B+1)\frac{\varepsilon_0}{P_L} \cdot B = 2B(B+1)\cdot \varepsilon_0$. 
% % \end{itemize}
% % Therefore, such deviation cannot succeed. 

% % Now suppose $1 - \acc(\stgp') \ge \varepsilon_0$. First, we show that majority agents have no incentives to deviate as the fidelity decrease. Then we show that for a deviating group containing no majority agents, $\lp_L^{\bR} (\stgp') \ge 1 - \frac{\varepsilon_0\cdot P_L\cdot P_H}{2B(B+1)}$. This directly implies that $\lp_H^{\bA}(\stgp') \le 1 -  \varepsilon_0$, and friendly agents have no incentives to deviate. Finally, we show that a deviating group with only unfriendly and minority agents cannot achieve $\lp_H^{\bA}(\stgp') \le 1 -  \varepsilon_0$. 

% % \paragraph{Majority agents.} Given that $1 - \acc(\stgp') \ge \varepsilon_0$, either $1 - \lp_H^{\bA}(\stgp') \ge \varepsilon_0$ or $1 - \lp_L^{\bR}(\stgp') \ge \varepsilon_0$ holds. Without loss of generality, suppose $1 - \lp_H^{\bA}(\stgp') \ge \varepsilon_0$, then $\ut_{i}(\stgp') - \ut_{i}(\stgp) < -P_H\cdot (\varepsilon_0 - \frac{\varepsilon_0\cdot P_L\cdot P_H}{2B(B+1)})\cdot 1 + P_L \cdot \frac{\varepsilon_0\cdot P_L\cdot P_H}{2B(B+1)} \cdot B =-\frac12 P_H \cdot \varepsilon_0$. Therefore, majority agents will not join the deviating group. 

% % \paragraph{Friendly agents.} Recall that for $\stgp$, $\thd^\ag_L > \xi$. Since at most $\kd = \xi\cdot \ag$ agents deviate, the \exshare{} of $\stgp'$, denoted by $\thd'_H$ and $\thd'_L$, will not deviate from that of $\stgp$ by $\xi$. Therefore, $\thd'_L \ge \thd^\ag_L - \xi > 0$. Then by the Hoeffding Inequality and our assumption on $\ag$, $\lp_L^{\bR}(\stgp') \ge 1 - 2\exp(-2(\thd^\ag_L - \xi)^2 \cdot \ag) \ge 1 - \frac{\varepsilon_0\cdot P_L\cdot P_H}{2B(B+1)}$. Since $\acc(\stgp') \le 1 - \varepsilon_0$, this directly implies that $\lp_H^{\bA}(\stgp') \le 1 -  \varepsilon_0$. Then consider the expected utility different of an friendly agent $i$, $\ut_{i}(\stgp') - \ut_{i}(\stgp) < -P_H\cdot (\varepsilon_0 - \frac{\varepsilon_0\cdot P_L\cdot P_H}{2B(B+1)})\cdot 1 + P_L \cdot \frac{\varepsilon_0\cdot P_L\cdot P_H}{2B(B+1)} \cdot B =-\frac12 P_H \cdot \varepsilon_0$. Therefore, no friendly agents have incentives to deviate. 

% % \paragraph{Unfriendly and minority agents.} Recall that all unfriendly and minority agents always vote for $\bR$ in $\stgp$ Therefore, $\thd'_H \ge \thd^\ag_H > 0$. Then by the Hoeffding Inequality and our assumption on $\ag$, $\lp_H^{\bA}(\stgp') \ge 1 - 2\exp(-2(\thd^\ag_H)^2 \cdot \ag) \ge 1 - \frac{\varepsilon_0\cdot P_L\cdot P_H}{2B(B+1)}$. This directly contradicts with $\lp_H^{\bA}(\stgp') \le 1 -  \varepsilon_0$. Therefore, such deviation cannot succeed. 

% % \paragraph{Case 2: $\xi = \frac{\Delta \cdot (\aga + \agf - \mu)}{P_{\ell H}}$.}

% % Suppose $\{\stgp\}_{\ag}$ is a strategy profile sequence such that no agents play weakly dominated strategy and $\acc(\stgp)$ converges to 1. We show that there is a constant $\varepsilon$ such that $\stgp$ is NOT an $\varepsilon$-ex-ante Bayesian $\kd$-strong equilibrium for infinitely many $\ag$. Let $(\bpl^A, \bph^A)$ be the average strategy of the majority agents and $(\bpl^I, \bph^I)$ be the average strategy of minority agents. (For example, for all majority agents $i$, $\bpl^A = \frac{1}{\aga\cdot \ag}\sum_{i} \bpl^i$, omitting the $O(\frac{1}{n})$ rounding error.) Then the \exshare{} of $\stgp$ can be written in the following formula. 
% % \begin{align*}
% %     \thd^\ag_H = &\ \agf + \aga \cdot (P_{hH}\cdot \bph^A + P_{\ell H} \cdot \bpl^A) + \agi \cdot (P_{hH}\cdot \bph^I + P_{\ell H} \cdot \bpl^I),\\
% %     \thd^\ag_L = &\ 1 - (\agf + \aga \cdot (P_{hH}\cdot \bph^A + P_{\ell H} \cdot \bpl^A) + \agi \cdot (P_{hH}\cdot \bph^I + P_{\ell H} \cdot \bpl^I)). 
% % \end{align*}

% % % We will start from a specific $\stgp$ and show that for all such 
 
% \end{proof}