\section{Related work}
\subsection{Graph Neural Networks}
Graph Neural Networks use deep neural network techniques to learn effective graph representations. They are divided into two main types: spectral methods and spatial methods. Spectral methods define graph convolution operations based on graph spectral theory. For example, \cite{25} uses the graph Laplacian's eigenvectors in the Fourier transform domain for graph convolution, \cite{26} approximates $K$-order polynomial spectral filters using Chebyshev polynomials, and GCN \cite{27} simplifies the Chebyshev polynomials to the first order for effective layer-by-layer propagation. In contrast, spatial methods define graph convolution by aggregating the spatial neighborhoods of nodes. For instance, GraphSAGE \cite{28} samples and aggregates representations from local neighborhoods in an inductive framework, while GAT \cite{29} introduces an attention mechanism to adaptively aggregate neighborhood representations of nodes.

\subsection{Graph Similarity Learning}
Graph similarity learning aims to find a function that measures the similarity between two graphs. Traditional methods like GED \cite{12} and MCS \cite{13} have exponential time complexity, limiting their use for large graphs. Graph kernel methods \cite{30} offer an alternative but require high computational and storage costs. Recently, Graph Neural Networks (GNNs) have been used for graph similarity learning. SimGNN \cite{18} uses histogram features and neural tensor networks \cite{31} to model interactions at node and graph levels, respectively. GraphSim \cite{19} extends SimGNN by incorporating convolutional neural networks to capture complex node-level interactions. GMN \cite{20} propagates node representations within each graph and across attention-based neighbors from another graph. HGMN \cite{21} compares node representations from one graph with the representation of the entire other graph to enable cross-graph interaction. H2MN introduces hypergraphs to model substructure similarity between graphs \cite{22}, and other works \cite{30, 31} segment graphs into subgroups for node-level comparisons.

\cite{47} proposed a learning-based method called Neural Supergraph Similarity Search (NSS) to address the hypergraph search problem. This method efficiently performs hypergraph search in the vector representation space by learning the representations of query and data graphs. In \cite{48}, a novel approach was introduced for efficient graph similarity search in large-scale graph databases, aiming to solve the graph similarity search problem under graph edit distance constraints. Similarly, \cite{49} also introduced a technique focused on efficient graph similarity search in large graph databases, specifically addressing the challenge of graph similarity under edit distance constraints. \cite{50} introduced C-MPGCN, a graph-based deep metric learning algorithm designed for regional similarity learning. This method aims to overcome the limitations of existing approaches, which often overlook spatial relationships and important features, by representing regions as graphs and using graph convolutional networks combined with contrastive learning to predict regional similarities. In \cite{51}, the authors proposed an algorithm named INFMCS (Similarity Computation via Maximum Common Subgraph Inference) for graph similarity computation. This algorithm seeks to address the lack of interpretability in existing learning methods for graph similarity measurement, by implicitly inferring the maximum common subgraph (MCS) to compute graph similarity, thereby making the process more interpretable.

\subsection{Graph Transformer}
The integration of Graph Neural Networks with Transformer architectures is increasingly being used to address challenges in graph networks. The existence of Graph Transformers has enhanced the performance of GNNs in handling long-range dependencies and large-scale graph data \cite{32,33}. The Simplified Graph Transformer (SGFormer) \cite{34} significantly improves performance and computational speed with just one layer of attention. The extensive, robust, and scalable GPS \cite{35} has expanded the model to various graph sizes and replaced the Transformer with a variant with linear computational complexity to enhance computational efficiency. Graph similarity calculation networks based on Graph Transformers \cite{36,37} have also been proposed, where Transformer modules are used to facilitate cross-graph information interaction, with Key and Value from one graph and Query from another, to perform node-level interaction calculations.