% indicate what implementation or performance “high watermark” was achieved
% (rather than the science that was enabled)

A novel four-dimensional hybrid parallel approach to scale neural network
training to tens of thousands of AMD and NVIDIA GPUs. Time-to-solution for
80-billion parameter GPT-style transformer models was reduced by
56.0$\times$, due to kernel tuning, aggressive overlap, and
optimization of collective communication. Unprecedented performance of 1.423
Exaflop/s on 6,144 NVIDIA H100 GPUs, 1.381 Exaflop/s on 32,768 AMD MI250X GCDs,
and 620.1 Petaflop/s on 4,096 NVIDIA A100 GPUs in half-precision (bf16).
