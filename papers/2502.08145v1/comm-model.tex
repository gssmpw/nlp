When assigned a job partition of $G$ GPUs, we have to decide how to organize
these GPUs into a 4D virtual grid, and how many GPUs to use for data
parallelism versus the different dimensions of 3D parallel martix
multiplication. To automate the process of identifying the best performing
configurations, we have developed a performance model that predicts the
communication time of a configuration based on the neural network architecture,
training hyperparameters, and network bandwidths. Using these predictions, we
can create an ordered list of the best performing configurations as predicted by
the model. We describe the inner-workings of this model below.

We primarily focus on modeling the performance of the collective operations in
the code, namely all-reduces, reduce-scatters, and all-gathers.  We first list
the assumptions we make in our model:
%
\begin{itemize}
    \item \emph{Assumption-1:}  
    The ring algorithm~\cite{thakurimproving2003} is used for implementing the all-reduce, reduce-scatter, 
    and all-gather collectives.
    \item \emph{Assumption-2:} For collectives spanning more than one compute node, the ring is formed such that the number of messages crossing node 
    boundaries is minimized.
    \item \emph{Assumption-3:} The message sizes are large enough, and hence, message startup overheads can be 
    ignored. 
    In other words, if a process is sending a message of $n$ bytes, then we assumed that the transmission time is simply 
    $\frac{n}{\beta}$, where $\beta$ is the available bandwidth between the two processes.
    \item \emph{Assumption-4:} We only model the communication times and ignore the effects of any computation taking 
    place on the GPUs.  
    \item \emph{Assumption-5:} We assume the same peer-to-peer bidirectional bandwidth, $\beta_{\mathrm{inter}}$, between every 
    pair of nodes.
\end{itemize}

We use the analytical formulations in Thakur et al.~\cite{thakurimproving2003}
and Rabenseifner~\cite{rabenseifneroptimization2004} for modeling the
performance of ring algorithm based collectives.  Let $t_{\mathrm{AG},z}$
denote the time spent in the all-gather across the $Z$-tensor parallel groups
(line 2 of Algorithm~\ref{alg:3d-tensor}). Similarly, we use
$t_{\mathrm{RS},z}$, $t_{\mathrm{AR},y}$ and $t_{\mathrm{AR}, x}$ to refer to
the time spent in the collectives in lines 14, 4, and 12 respectively.
Similarly, we use $t_{\mathrm{AR}, \mathrm{data}}$ for the time spent in the
data parallel all-reduce. Then, we can model these times as follows,
%
\begin{align}
    t_{\mathrm{AG},z} &= \frac{1}{\beta}\times (G_{z}-1) \times \frac{k \times n}{G_{x} \times G_{y} \times G_{z}}  
    \label{eqn:layer-ag} \\ \nonumber \\
    t_{\mathrm{RS},z} &= \frac{1}{\beta}\times \left( \frac{G_{z}-1}{\mathrm{G_{z}}} \right) \times \frac{k \times n}{G_{x} \times G_{y}} 
    \label{eqn:layer-rs} \\ \nonumber \\
    t_{\mathrm{AR},y} &= \frac{2}{\beta} \times \left( \frac{G_{y}-1}{G_{y}} \right) \times \frac{m \times n}{G_{z} \times \mathrm{G}_{x}}
    \label{eqn:layer-ar-1} \\ \nonumber \\
    t_{\mathrm{AR}, x} &= \frac{2}{\beta} \times \left( \frac{G_{x}-1}{G_{x}} \right)
    \times \frac{m \times k}{G_{z} \times G_{y}}
    \label{eqn:layer-ar-2} \\ \nonumber \\
    t_{\mathrm{AR}, \mathrm{data}} &= \frac{2}{\beta} \times \left( \frac{G_{\mathrm{data}}-1}{G_{\mathrm{data}}} \right) 
    \times \frac{k \times n}{G_{x} \times G_{y} \times G_{z}}
    \label{eqn:layer-ar-3}  
\end{align}

The total communication time for a single layer, ${t_{\mathrm{comm}}}$ is simply
the sum of Equations~\ref{eqn:layer-ag} through~\ref{eqn:layer-ar-3}: 
%
\begin{align}
    t_{\mathrm{comm}} = t_{\mathrm{AG},z} + t_{\mathrm{RS},z} +  t_{\mathrm{AR},y} + t_{\mathrm{AR},x} 
    + t_{\mathrm{AR}, \mathrm{data}} \label{eqn:layer} 
\end{align}
%
For layers with `transposed' weight matrices as discussed at the end of
Section~\ref{sec:hybrid}, we need to swap the values of $G_x$ and $G_y$. 
And finally, to model the communication time for
the entire network, we apply Equation~\ref{eqn:layer} to all of its layers, and
take a sum of the times.

In the equations derived above, we made a simplifying assumption that all
collectives in our hybrid parallel method can achieve the highest peer-to-peer
bandwidth, denoted by ${\beta}$. However, since several collectives are often
in operation at once, the actual bandwidth achieved for a collective operation
among a group of GPUs depends on the placement of processes in our 4D virtual
grid to the underlying hardware topology (nodes and network)~\cite{solomonik:sc2011, bhatele:sc2012b, abdel-gawad:sc2014, bhatele:hipc2014}. For example,
process groups that are contained entirely within a node can experience higher
bandwidths than those containing GPUs on different nodes. Next, we
model the specific bandwidths used in Equations~\ref{eqn:layer-ag}
through~\ref{eqn:layer-ar-3}.

To model the process group bandwidths, we begin by assuming a hierarchical organization of process groups: 
$X$-tensor parallelism (innermost), followed by $Y$-tensor parallelism, $Z$-tensor parallelism, and
data parallelism (outermost). As a concrete example, if we have eight GPUs, and set $G_{x}=G_{y}=G_{z}=G_{\mathrm{data}}=2$, 
then the $X$-tensor parallel groups comprise of GPU pairs 
(0,1), (2,3), (4,5), and (6,7). Similarly, the $Y$-tensor parallel groups would comprise of GPU pairs (0,2),
(1,3), (4,6), and (5,7), and so on.
% for the $Z$ and $\mathrm{data}$ parallel groups.

Now let $\vec{G} = (G_{x}, G_{y}, G_{z}, G_{\mathrm{data}})$ be the tuple of our configurable performance 
parameters, arranged in order of the assumed hierarchy. Let 
$\vec{\beta} = (\beta_{x}, \beta_{y}, \beta_{z}, \beta_{\mathrm{data}})$ be the effective peer-to-peer bandwidths 
for collectives issued within these process groups. We use $\vec{\beta}_{i}$ and $\vec{G}_{i}$ to represent the $i^{\mathit{th}}$
elements of these tuples ($0 \leq i \leq 3$). Also, let $G_{\mathrm{node}}$ refer to the number of GPUs per node. Now let us 
model each $\beta_{i}$ i.e. the bandwidth available to the GPUs in the process groups at the $i^{\mathit{th}}$ level of the 
hierarchy.

\vspace{0.08in}
\noindent{\em Case 1: GPUs in the process group lie within a node} --
in our notation, this is the scenario when $\prod_{j=0}^{i}G_{j} \leq G_{\mathrm{node}}$. 

The bandwidth $\vec{\beta}_{i}$ is determined by two primary factors: (i) the
size of the $i$th process group, $G_{i}$, and (ii) the cumulative product of
the sizes of all preceding process groups, $\prod_{j=0}^{i-1}G_{j}$. Given that
the number of GPUs per node is typically small, the number of possible
scenarios is also small. Therefore, we can profile the bandwidths for all
potential configurations in advance and store this information in a database.
Specifically, we generate all possible two-dimensional hierarchies of process
groups $(G_{0}, G_{1})$ such that $G_{0} \times G_{1} \leq G_{\text{node}}$,
and then perform simultaneous collectives within the outer process groups of
size $G_{1}$ with a large message size of 1 GB. We record the achieved
bandwidths for this tuple in our database. Then, for a given model, when we
need the predicted bandwidths for the $i^{th}$ process group, we retrieve the
bandwidth recorded for the tuple $(G_{0} = \prod_{j=0}^{i-1}G_{j}, G_{1} =
G_{i})$.

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.49\textwidth]{figs/comm-model-validation/perlmutter_comm_model_20B_32GPU_v2.pdf}
    \includegraphics[width=0.49\textwidth]{figs/comm-model-validation/perlmutter_comm_model_40B_64GPU_v2.pdf}
    \caption{Plots validating the performance model by comparing the observed time per batch and the rank ordered by the model for two neural networks: GPT-20B (left) and GPT-40B (right).}
    \label{fig:rank-comm-model}
\end{figure*}

\vspace{0.08in}
\noindent{\em Case 2: GPUs in the process group are on different nodes} --
in our notation, this is the scenario when $\prod_{j=0}^{i}G_{j} > G_{\mathrm{node}}$. 

For process groups spanning node boundaries, the approach of recording all
possible configurations in a database is not feasible due to the vast number of
potential scenarios, given the large number of possible sizes of these groups
in a multi-GPU cluster.  Therefore, we develop a simple analytical model for
this scenario, which predicts the achieved bandwidths as a function of the
inter-node bandwidths ($\beta_{\mathrm{inter}}$), process group sizes
($\vec{G}$), and the number of GPUs per node ($G_{\text{node}}$).

First, let's first explore two simple examples to build some intuition. In Figure~\ref{fig:all-reduce-one}, we demonstrate a 
scenario with a single process group spanning eight GPUs on two nodes, with four GPUs on each node. In this case, the ring 
messages crossing node boundaries (i.e. the link between GPUs 1 and 4, and between GPUs 6 and 3) will be the 
communication bottleneck. Since we assumed $\beta_{\mathrm{inter}}$ to be the bidirectional bandwidth between node pairs, we 
can set $\beta_{i}=\beta_{\mathrm{inter}}$.

\begin{figure}[h]
    \centering
       \includegraphics[width=0.9\columnwidth]{figs/comm-model/one-all-reduce.pdf}
       \caption{Creation of a ring among eight GPUs on two nodes for a collective communication operation (all-reduce/reduce-scatter/all-gather).}
       \label{fig:all-reduce-one}
\end{figure}

\begin{figure}[h]
    \centering
       \includegraphics[width=0.9\columnwidth]{figs/comm-model/contention.pdf}
       \caption{Two rings among four GPUs each across two nodes for performing collective operations simultaneously.}
       \label{fig:all-reduce-many}
\end{figure}

Another possible scenario is when there are multiple simultaneous collectives taking place between two nodes. For example, consider 
Figure~\ref{fig:all-reduce-many}, wherein GPUs $(0, 4, 6, 2)$ and GPUs $(1, 5, 7, 3)$ are executing two independent collectives using the ring algorithm
simultaneously. In this case, the available inter-node bandwidth will be shared between these two collectives and 
$\beta_{i}=\frac{\beta_{\mathrm{inter}}}{2}$. 

The first scenario occurs in the case when the process groups preceding the $i^{\mathit{th}}$ process 
group in the hierarchy are of size one, i.e. $G_{j}=1$  $\forall j < i$. Whereas the second scenario occurs in the case 
when at least one of these preceding process groups is of a size $>1$. In that case, we get multiple ring messages crossing node boundaries
and the bandwidth gets distributed between the rings. However, note that the maximum
reduction in the bandwidth is bounded by the total number of GPUs on each node, as there can't be more 
inter-node ring links than GPUs on a node. Equation~\ref{eqn:bandwidth} models all the scenarios to obtain the observed bandwidth:
\begin{equation}
\vec{\beta}_{i} = \
         \dfrac{\beta_{\text{inter}}}{\min \left( G_{\text{node}}, \prod_{j=0}^{i-1}G_{j} \right)} \label{eqn:bandwidth}
\end{equation}
We use this bandwidth term in Equations~\ref{eqn:layer-ag}
through~\ref{eqn:layer-ar-3} of our model. We use the model to create an
ordered list of configurations, and then we can pick the top few configurations
for actual experiments.

\vspace{0.08in}
\noindent{\bf Validating the Performance Model}: To validate the model, we
collect the batch times for all possible configurations of the 4D virtual grid
when training GPT-20B on 32 GPUs and GPT-40B on 64 GPUs of Perlmutter. Using
the observed batch times, we label the ten fastest configurations as `efficient'
and the rest as `inefficient'. When creating the validation plots, we rank the
configurations using the ordering provided by the performance model.
Figure~\ref{fig:rank-comm-model} shows the empirical batch times on the Y-axis
and the rank output by the performance model on the X-axis. The fastest
configurations should be in the lower left corner. We observe that nine out of
the top ten configurations predicted by the performance model are indeed
`efficient' as per their observed batch times. This shows that the model is
working very well in terms of identifying the fastest configurations. 

