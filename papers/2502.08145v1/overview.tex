% description of the problem and its importance, in terms understandable to a non-specialist

% \fix{1 page max}

The field of generative artificial intelligence (AI) has taken the world by
storm. In particular, large language models (LLMs) and their chatbot interfaces
have become ubiquitous, employed by students, researchers, and professionals in
various fields on a daily basis. Modern generative AI models are built by
training extremely large neural networks, which have been shown to generalize
extremely effectively with increases in model size. This unprecedented scaling of
neural network training has been enabled by the emergence of highly efficient
GPUs, and the availability of open source training frameworks such as PyTorch,
and TensorFlow.

% Deep neural networks (DNN) are also being used in computational science, from
% climate simulations to materials modeling.
Training large neural networks that
do not fit on a single GPU with 40-96 GB of RAM requires partitioning the model
across multiple GPUs and parallelizing the matrix-matrix multiplication
operations, which are a significant fraction of the overall computation.
Scalability and parallel efficiency of DNN training is impacted by several
factors -- sustained flop/s and scalability of parallel matrix multiplication,
performance of collective communication operations over sub-communicators, and
the degree of overlap of computation with non-blocking collectives. While
classical parallel algorithms for matrix multiplication such as SUMMA and
Cannon's 2D Matrix Multiply exist, they lead to significant communication
bottlenecks when training models with hundreds of billions of parameters on
hundreds of GPUs. All of these factors make efficient parallelization of DNN
training a formidable task.
% Unlike traditional scientific computing simulations, most of the
% communication inefficiencies stem from the use of collectives like all-gather
% and reduce-scatter.

In this work, we target the challenging research problem of training models
with hundreds of billions of parameters on the fastest supercomputers with
thousands of GPUs. Traditionally, training at such scales has been restricted
to companies with large budgets and access to significant GPU resources.
However, programs such as INCITE for access to DOE supercomputers, 
Frontier and Perlmutter, and recent access to Alps at CSCS have
enabled our team to solve the research challenges in the area of parallel
training, and innovate in the area of AI/ML, by training and
fine-tuning LLMs.

There are several different challenges in ensuring scalability and a high
fraction of peak flop/s for parallel training. First, we need to ensure that
math libraries such as cuBLAS and rocBLAS are highly performant for matrix
multiplication on NVIDIA and AMD GPUs respectively. Second, we have to ensure
that both intra-node and inter-node communication of data (activations,
parameters, and gradients) is performant, and overlapped with computation as
much as possible. Third, when running on large GPU partitions with hundreds to
thousands of GPUs, the decomposition of work and its mapping to GPUs should be
near-optimal, taking communication into account.

\begin{table*}[t]
	\centering
    \caption{Comparison of large-scale LLM training studies, covering diverse
frameworks and hardware. For each study, we list the largest hardware counts
used, corresponding model \& batch size, percentage of peak flop/s, and actual
sustained flop/s.}
	\begin{tabular}{lcrrccrr} \toprule
		\textbf{Study} & \textbf{Framework} & \textbf{Model Size} & \textbf{Batch Size} & \textbf{Hardware} & \textbf{Scale} & \textbf{\% Peak} & \textbf{Petaflop/s}\\ \midrule
		SUPER~\cite{super2021jain}             & LBANN                   & 3B*   & 0.5M* & NVIDIA V100 & 1,024 GPUs & - & -\\
		KARMA~\cite{wahib2020scaling-sc}       &   KARMA                 & 17B   & 2.0M* & NVIDIA V100 & 2,048 GPUs & - & - \\
		FORGE~\cite{yin2023forge}              & GPT-NeoX                & 1.44B & 16.8M & AMD MI250X  & 2,048 GCDs & $\sim$29\%$^{\dagger}$ & $\sim$112.6$^{\dagger}$\\
		Dash et al.~\cite{dash2023optimizing}  & Megatron-DeepSpeed      & 1000B & 19.7M & AMD MI250X  & 3,072 GCDs & 31.9\%$^{\ddagger}$ & 188.0$^{\ddagger}$\\
		MT-NLG~\cite{megatron-turing-nlg-530b} & Megatron-DeepSpeed      & 530B  & 4.0M  & NVIDIA A100 & 3,360 GPUs & 36\% & 379.7\\
		Narayanan et al.~\cite{megatronlm-2}   & Megatron-LM             & 1000B & 6.3M  & NVIDIA A100 & 3,072 GPUs & 52\% & 502.0 \\
		MegaScale~\cite{jiang2024megascale}    & MegaScale               & 175B  & 12.5M & NVIDIA A100 & 12,288 GPUs& 55\%  & 2166.3 \\
		Google~\cite{gcloudscaletraining}      & Cloud TPU Multislice Training & 32B   & 417M  & TPUv5e      & 55,094 TPUs& 44.67\% & 4480.0\\
		\midrule
		\multirow{3}{*}{\bf This Work}         & \multirow{3}{*}{\textbf{AxoNN}~\cite{singh:ipdps2022}} & 40B & 16.8M & NVIDIA A100 & 4,096 GPUs & 49\% & 620.1\\
		& & 320B & 16.8M & AMD MI250X  & 32,768 GCDs & 22\% & \textbf{1381.0} \\
		& & 60B  & 16.8M & NVIDIA H100 & 6,144 GPUs  & 23\% & \textbf{1423.1} \\
		\bottomrule
		\vspace{-0.5em}\\
		\multicolumn{8}{l}{\small{* Estimated from description in paper as exact number not mentioned}}\\
		\multicolumn{8}{l}{\small{$^{\dagger}$ Estimated from plots in the paper as exact numbers not mentioned}}\\
		\multicolumn{8}{l}{\small{$^{\ddagger}$ Calculated from flop/s at lower GPU/GCD count and weak scaling efficiency}}\\
	\end{tabular}
    \label{tab:study-comparison}
\end{table*}

In order to overcome the challenges mentioned above, we have developed a
four-dimensional (4D) hybrid parallel approach that combines a
three-dimensional (3D) matrix multiplication algorithm with data parallelism to
achieve high efficiency at large GPU counts. In addition, we have improved the
performance of our implementation using several optimizations.  First, we tune
our matrix multiplication for each individual platform. Second, we aggressively
overlap computation with non-blocking collectives used in different phases of
training. Third, since the 4D algorithm requires arranging the GPUs in an
allocated job partition into a 4D virtual grid, this results in several
potential configurations, not all of which are performance optimal. Hence, we
have developed a communication model for 4D hybrid algorithms that can predict
high-performing configurations given a problem size and number of GPUs. We have implemented 
all of the aforementioned innovations in AxoNN~\cite{singh:ipdps2022, singh:ipdps2023}, our open source 
framework for large scale parallel deep learning.

We have benchmarked and optimized AxoNN on both NVIDIA and AMD GPU-based
platforms, and we present results on Perlmutter at NERSC/LBL, Frontier at
OLCF/ORNL, and Alps at CSCS. We use a range of neural network sizes with 5
billion to 320 billion parameters. AxoNN achieves an unprecedented performance
of 620.1 Petaflop/s on 4,096 NVIDIA A100 GPUs, 1.381 Exaflop/s on 32,768 MI250X
GCDs, and 1.423 Exaflop/s on 6,144 NVIDIA H100 GPUs in half-precision (bf16).

Access to a highly scalable training framework and large supercomputers have
also enabled the AI researchers on our team to study the inner workings of LLMs
at model sizes that are impossible to study otherwise. One such problem is
studying whether LLMs memorize training data and regenerate it verbatim during
inference. This has privacy risks when personally identifiable information
(PII) is memorized, and legal/copyright risks when models reproduce verbatim
copies of text without the necessary copyright and licensing information. We
present a study that explores the relationship between model size and the
memorization properties of LLMs, and demonstrate the impact of a solution to
reduce memorization.

