% implications for future systems and applications

A scalable training framework such as AxoNN and access to large supercomputers
such as Frontier and Alps can enable studying properties of LLMs at scales that
were impossible before. Below, we present a study on the behavior of
memorization by large language models.

% \begin{table}[h]
% 	\centering\tiny
% 	\begin{tabular}{llll}
% 		\toprule
% 		Epoch Count & Experiment & Avg. Exact Match \% (min, max) & With Goldfish Loss \\
% 		\midrule
% 		0 Ep & 1B TinyLlama & 0.0\% (0.0\%, 0.0\%) & 0.0\% (0.0\%, 0.0\%) \\
% 		0 Ep & 7B Llama 2 & 0.0\% (0.0\%, 0.0\%) & 0.0\% (0.0\%, 0.0\%) \\
%		0 Ep & 8B Llama 3.1 & 0.0\% (0.0\%, 0.0\%) & 0.5\% (0.5\%, 0.5\%) \\
%		0 Ep & 13B Llama 2 & 0.0\% (0.0\%, 0.0\%) & 0.0\% (0.0\%, 0.0\%) \\
%		0 Ep & 70B Llama 2 & 0.5\% (0.5\%, 0.5\%) & 0.5\% (0.5\%, 0.5\%) \\
%		0 Ep & 70B Llama 3.1 & 0.3\% (0.0\%, 0.5\%) & 0.5\% (0.5\%, 0.5\%) \\
%		0 Ep & 405B Llama 3.1 & 11.0\% (11.0\%, 11.0\%) & 11.0\% (11.0\%, 11.0\%) \\
% 		1 Ep & 1B TinyLlama & 0.2\% (0.0\%, 0.5\%) & 0.2\% (0.0\%, 0.5\%) \\
% 		1 Ep & 7B Llama 2 & 0.0\% (0.0\%, 0.0\%) & 0.0\% (0.0\%, 0.0\%) \\
% 		1 Ep & 8B Llama 3.1 & 0.0\% (0.0\%, 0.0\%) & 0.0\% (0.0\%, 0.0\%) \\
% 		1 Ep & 13B Llama 2 & 0.2\% (0.0\%, 0.5\%) & 0.1\% (0.0\%, 0.5\%) \\
%		1 Ep & 70B Llama 2 & 1.8\% (1.5\%, 2.0\%) & 0.5\% (0.0\%, 1.5\%) \\
%		1 Ep & 70B Llama 3.1 & 5.2\% (4.5\%, 6.0\%) & 0.5\% (0.0\%, 1.5\%) \\
%		1 Ep & 405B Llama 3.1 & 7.0\% (7.0\%, 7.0\%) & 10.5\% (10.5\%, 10.5\%) \\
% 		4 Ep & 1B TinyLlama & 0.6\% (0.0\%, 1.0\%) & 0.6\% (0.0\%, 1.0\%) \\
% 		4 Ep & 7B Llama 2 & 0.6\% (0.0\%, 1.0\%) & 0.9\% (0.0\%, 1.5\%) \\
% 		4 Ep & 8B Llama 3.1 & 0.3\% (0.0\%, 0.5\%) & 0.3\% (0.0\%, 0.5\%) \\
% 		4 Ep & 13B Llama 2 & 0.9\% (0.0\%, 1.5\%) & 1.1\% (0.0\%, 2.0\%) \\
%		4 Ep & 70B Llama 2 & 12.5\% (10.0\%, 14.0\%) & 1.5\% (0.5\%, 2.5\%) \\
%		4 Ep & 70B Llama 3.1 & 32.2\% (29.5\%, 36.0\%) & 0.2\% (0.0\%, 0.5\%) \\
%		4 Ep & 405B Llama 3.1 & 9.5\% (9.5\%, 9.5\%) & 12.5\% (12.5\%, 12.5\%) \\
% 		6 Ep & 1B TinyLlama & 0.2\% (0.0\%, 0.5\%) & 0.2\% (0.0\%, 0.5\%) \\
% 		6 Ep & 7B Llama 2 & 0.2\% (0.0\%, 0.5\%) & 0.2\% (0.0\%, 0.5\%) \\
% 		6 Ep & 8B Llama 3.1 & 0.0\% (0.0\%, 0.0\%) & 0.0\% (0.0\%, 0.0\%) \\
% 		6 Ep & 13B Llama 2 & 0.2\% (0.0\%, 0.5\%) & 0.2\% (0.0\%, 0.5\%) \\
%		6 Ep & 70B Llama 2 & 47.0\% (44.5\%, 51.5\%) & 0.5\% (0.5\%, 0.5\%) \\
%		6 Ep & 70B Llama 3.1 & 67.8\% (67.5\%, 68.5\%) & 1.3\% (1.0\%, 1.5\%) \\
%		6 Ep & 405B Llama 3.1 & 34.0\% (34.0\%, 34.0\%) & 16.0\% (16.0\%, 16.0\%) \\
% 		\bottomrule
% 	\end{tabular}
% 	\caption{With and without GL}
% \end{table}

\subsection{Memorization of Training Data by Large Language Models}

A growing body of work has shown that language models memorize a portion of
their training data and can reproduce this training data at inference time
\cite{carlini2023quantifying}. The ability of LLMs to reproduce training data
has become a flashpoint for the AI community, as it poses major privacy and
legal risks for commercial models \cite{grynbaum2023times,
carlini2021extracting,carlini2023quantifying}.
% This effect grows rapidly as a function of the number of times a sequence is
% repeated in the data but findings also suggest that, memorization rates also
% climb with parameter count.

It is thought that memorization is largely due to training data repetition, and
it may be mitigated by dataset deduplication. Other factors such as data
structure and model size may play a factor, but the issue is not well
understood because public experiments have been constrained to smaller models
(e.g.~the popular Llama-2 7 billion parameter model~\cite{touvron2023llama})
with limited capacity and correspondingly small rates of memorization
\cite{carlini2023quantifying,biderman2023pythia}. As we observe below, the
ability to memorize entire documents emerges only for large model
sizes. Further, we hypothesize that models above a certain size threshold
may exhibit \textit{catastrophic memorization}, in which documents are
memorized immediately in one single pass. When training a model above this size
limit, even perfectly deduplicated datasets may still result in privacy and
copyright leaks.

By creating scalable, user-friendly and portable access to model parallelism,
AxoNN unlocks the potential for training and fine-tuning much larger models
under commodity computing constraints using sequential LLM training codebases.
This creates a scientific laboratory where large-model phenomena
such as memorization can be publicly reproduced and studied. It also raises the
ability of many practitioners to fine-tune large models on domain-specific
data, expanding the need to understand memorization risks.

\subsection{Experimental Setup: Training Llama models on Wikipedia}

We design a targeted set of continued pre-training experiments to quantify the
relationship between model size and memorization. We consider the Llama family
of LLMs with publicly available pre-trained weights, and use the AxoNN infused
LitGPT framework (introduced in Section~\ref{sec:setup-desc}) to parallelize
the models. Our experiments start with pre-trained checkpoints for the
TinyLlama-1B model~\cite{zhang2024tinyllama}, the 7B, 13B, and 70B parameter
models in the Llama 2 family~\cite{touvron2023llama} and the 8B, 70B, and 405B
parameter models from the recent Llama 3.1 release~\cite{dubey2024llama}. We
train on English text data from Wikipedia with varying levels of repetition to quantify how
memorization depends on model scale.

We train on English Wikipedia pages with $2048$ tokens or more. The articles
are randomly placed into one of four disjoint ``buckets,'' each with 200 articles. During
training, the first three buckets are repeated for 1, 4, or 6 ``epochs'' (one
pass over every page in the bucket) respectively. The fourth bucket is a
control group to measure baseline preexisting memorization from pre-training,
and we do not perform any further training on the pages in the fourth bucket. After training is
complete, we prompt the model with the beginning of each training sequence, and
let the model write the last 50 tokens. We consider a sequence memorized if the
model perfectly reproduces the correct 50 tokens.

%For each model size, we train on three buckets of Wikipedia pages for $1$, $4$
%and $6$ epochs respectively\footnote{The actual epoch counts are $1$, $\sim4.4$
%	and $\sim6.6$ epochs due to the stochastic nature of our batch sampling code.}.
We train the $1$B, $7$B, and $8$B models on eight GCDs of Frontier using
$8$-way $Z$-tensor parallelism (i.e. $G_{z}=8$), the $13$B model using $16$
GCDs, the $70$B models using $64$ GCDs, and the $405$B model using $128$ GCDs,
each with a corresponding level of $Z$-tensor parallelism. The total batch size
is fixed at $128$ samples for all model sizes. In the case of smaller models,
lower level of tensor parallelism is needed, so data parallelism is used to
utilize the remaining GPUs.  We warm up each model for $50$ steps, increasing
the learning rate to $3\times10^{-4}$ on the non-bucketed Wikipedia pages, and
then inject the three buckets of target data over the next $50$ steps of
training while decaying the learning rate to $3\times10^{-5}$. We report
memorization for each bucket separately, and also for the held-out (``$0$ Ep'')
control bucket.

% trim = left, bottom, right, top
\begin{figure*}[t]
	\centering
	\includegraphics[width=0.49\textwidth]{figs/goldfish/update_mem_em_v_epochs_grouped_bars_small_models.pdf}
	\includegraphics[width=0.49\textwidth]{figs/goldfish/update_mem_em_v_epochs_grouped_bars_large_models.pdf}
    \caption{Memorization as a function of parameter count and epochs
(repetitions of the training data). For each model size, we show the ``Exact
Match'' rate at which the model correctly reproduces the last $50$ tokens of
articles after being trained on them for various numbers of epochs.
\textbf{(Left)}  Memorization is difficult to observe for small models.
\textbf{(Right)} The ability to efficiently memorize emerges at larger models
scales.  We see that a 70B model is even capable of {\em catastrophic
memorization}, as it memorized entire documents after seeing them just once.
For models with parameter counts in the
$1$B-$13$B range, we report the average over five trials, for $70$B, we report
the average over three trials, and for $405$B we report a single trial. Error
bars depict the min and max observed scores.}
    \label{fig:mem-results}
\end{figure*}

\subsection{Results: Catastrophic Memorization as a Function of Model Size}

% Chart data is here if you want to cite more specific numbers:
% figs/data_mem_raw_update/gordon_bell_update_2024-08-08_17-59-04.csv
% non_member=0Ep,bucket3=1Ep,bucket4=4Ep,bucket5=6Ep

Figure~\ref{fig:mem-results} shows the impact of parameter count and number of
epochs on exact memorization under otherwise identical conditions. At the
$1$B-$13$B scale (left plot), training for up to six epochs causes memorization of less
than $1\%$ of the $200$ documents on average. However, we observe that the $70$B
models and the 405B model are capable of significant memorization (right plot). After just six passes over the
data, the $70$B Llama 2 and $70$B Llama 3.1 models memorize $\mathbf{47\%}$ and
$\mathbf{67\%}$ of documents on average respectively. Furthermore, we observe
catastrophic memorization behavior starting at the $70$B scale; roughly 5\% of
documents are memorized in just one single pass.

Moving to the $405$B scale, we make several surprising observations. This model
had already memorized over 10\% of the control documents (see the bars labeled
``$0$ Ep'') before our experiment even began, showing that the ability to
memorize and retain documents during pre-training has emerged at this scale.
While Wikipedia pages were certainly included in the training corpus of the
Llama 3.1 series of models, only this largest model in the family exhibits such
non-trivial levels of memorization without further continued training.
Counterintuitively, we note that the rate of memorization of the $405$B model
during continued training was slower than that of the $70$B model. This is
likely because we used one set of hyperparameters for all models, and extreme
scales likely require different hyperparameters for optimal learning.

% \fix{405B caveats}
% @tomg other caveats not described above

% memorization rate appears to go down after we train for 1 and 4 epochs versus the number at 0.
% because we dont track the "diff" we cant tell the difference between the 405 forgetting some sequences versus learning some new ones.
% We couldn't run the model in the same precision setting. It is bf16-true, all other models are bf16-mixed. could matter more for very large deep models with loss of gradient precision when trying to fit tokens perfectly in a few passes.

\subsection{Results: Goldfish Loss Stops Memorization in its Tracks}

% Tom will rewrite this in his way I am sure :]
Observing extreme levels of memorization for models at the $70$B parameter
scale and above, we deploy a recently proposed technique for mitigating
memorization in large language models. Language model training
minimizes the expected cross-entropy between the language model's next-token
distribution and the true tokens as they appear in the training corpus. The
\textit{Goldfish Loss}~\cite{hans2024goldfish} technique introduces a mask such that some
tokens in any given training sequence are randomly omitted from the loss
computation. The model cannot memorize the masked tokens, and must ``guess''
them when trying to reproduce a training sequence at inference time, making it
very unlikely that long sequences can be exactly reproduced.

\begin{figure}[h]
	\centering
	\includegraphics[width=\columnwidth]{figs/goldfish/update_mem_em_v_epochs_grouped_bars_w_tld_large_models.pdf}
    \caption{The impact of applying Goldfish Loss during training to mitigate
memorization in large models. The Exact Match rate reduces to levels comparable
to the control data.}
    \label{fig:goldfish}
\end{figure}

Figure~\ref{fig:goldfish} shows the results of re-running our training
experiments with Goldfish Loss activated (using Goldfish parameters k=2, h=13).
Even after continued training, memorization now reduces to levels comparable to the
control data (0 Ep). We do observe a small increase in memorization as the 405B
model trains, likely because the model has already memorized the masked tokens
from when it was pre-trained on Wikipedia. However, as we can see, the reduction in memorization when using the Goldfish Loss is significant, both for the 70B models and the 405B model.

