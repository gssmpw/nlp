% quantitative discussion of current SoA, with emphasis on performance-related
% aspects

% \fix{1 page max}

In this section, we present the state of the art in scaling parallel training
of deep neural networks to large-scale HPC systems and data centers.

\subsection{Methods for Parallel DNN Training}
\label{sec:sota-distrain} 

Deep neural network training is typically parallelized using one of three
approaches -- data parallelism, tensor parallelism, or pipeline parallelism, or
a hybrid approach that combines some of the above.  In data parallelism, all
GPUs are assigned a full copy of the neural network, and parallelism is
achieved by sharding the input batch equally among the GPUs.  The main drawback
of data parallelism is that it requires the entire network to fit on a single
GPU. To mitigate this, sharded data parallelism has been developed, which
divides the network parameters across GPUs~\cite{sc2020zero,fsdp, wang2024zero}
and performs extra communication to gather them when needed.

Model parallelism is used to train DNNs that far exceed the memory capacity of
a single GPU, and it can be further divided into two approaches -- tensor
parallelism~\cite{megatronlm} and pipeline
parallelism~\cite{huang2019gpipe_nips, megatronlm-2}.
The former parallelizes
the computation of each layer of the neural network across several GPUs, and is
the focus of our work.  In pipeline parallelism, entire layers are assigned to
each GPU.  A popular framework for parallel training is Shoeybi et
al.'s Megatron-LM~\cite{megatronlm}, which uses a tensor parallel algorithm to
parallelize a pair of fully-connected layers.

Several frameworks combine multiple approaches to develop hybrid
methods. Narayanan et al.~\cite{megatronlm-2} extend Megatron-LM to support
hybrid parallelism by combining tensor, pipeline, and data parallelism.
Rajbhandari et al.~introduce a sharded data parallelism approach called ZeRO~\cite{sc2020zero},
which is combined with pipeline and tensor parallelism in Microsoft's training
framework, DeepSpeed~\cite{deepspeed-extreme-3d, singh:ics2023}.
% ZeRO-Infinity~\cite{zero_infinity} extends ZeRO to support training models on
% heterogeneous hardware using GPU, CPU, and NVMe memory.
% ZeRO++~\cite{wang2023zero} further extends ZeRO with communication
% optimizations to improve the performance of training large language models.
Megatron-DeepSpeed uses Megatron-LM's tensor 
parallelism.

Several other frameworks that further optimize DNN training have been proposed
in recent times. GPT-NeoX builds upon Megatron-LM and
DeepSpeed for ease of usage~\cite{gpt-neox-library}.
Wahib et al.~introduce KARMA, 
an out-of-core data parallelism framework, managing CPU-GPU data transfers to
alleviate GPU memory constraints~\cite{wahib2020scaling-sc}.
Zheng et al.~propose Alpa for 
automating neural network parallelization, optimizing communication across GPUs~\cite{alpa}.
Colossal-AI~\cite{colossalai2023unified} offers a unified interface for
distributed deep learning training.

\subsection{Large-scale Studies of Training LLMs}
\label{sec:ls-studies}

We now present recent studies on training large language models on some of the
largest GPU-based clusters.  Meta trained Llama 2~\cite{touvron2023llama} on
2000 NVIDIA A100 GPUs.  Jain et al.~\cite{super2021jain} benchmark the training
of a variant of T5-Large~\cite{t5-transformer} on 1024 NVIDIA V100 GPUs using
their proposed sub-graph parallelism technique within the LBANN
framework~\cite{essen2015lbann}.  Wahib et al.~\cite{wahib2020scaling-sc} use
KARMA to benchmark a 17B parameter model on 2048 NVIDIA V100 GPUs and report a
1.35x training speedup compared to ZeRO~\cite{sc2020zero}. Narayanan et
al.~present a weak scaling study of Megatron-LM's pipeline parallelism,
achieving 52\% of the peak NVIDIA A100 flop/s when benchmarking the training of
a 1000B parameter model on 3072 GPUs~\cite{megatronlm-2}. Shaden et
al.~\cite{megatron-turing-nlg-530b} use Megatron-LM and
DeepSpeed~\cite{deepspeed-extreme-3d} to train a 530B parameter language model
on the Selene supercomputer~\cite{selene} using 4480 NVIDIA A100 GPUs. They
achieved 113 Tflop/s per GPU with 3360 GPUs, equivalent to 36\% of the peak
performance. Ziheng et al.~\cite{jiang2024megascale} introduce MegaScale, a
production system for training LLMs at scale, achieving a 55.2\% of the peak
flop/s when benchmarking a 175B parameter model on 12,288 NVIDIA A100 GPUs.

With the emergence of AMD GPUs, several studies have focused on training large
language models on AMD systems. Yin et al.~\cite{yin2023forge} train FORGE, an
open suite of large language models for scientific computing on
Frontier~\cite{frontier2023}. In their work, the authors show scaling
performance for training FORGE on up to 2048 AMD MI250X GPUs, achieving 28\% of
the peak flop/s.  Dash et al.~\cite{dash2023optimizing} analyze efficient
distributed training strategies on Frontier for training large language models.
They achieve 31.96\% of peak when benchmarking the training of
a 1T parameter model on 1024 MI250X GPUs.

Google conducted a study on large-scale training jobs for LLMs using over
50,000 TPUv5e chips~\cite{gcloudscaletraining}. The authors achieve 44.67\% of
the peak TPUv5e performance when benchmarking a 32B parameter model on 
50,944 TPUv5e chips.  Table \ref{tab:study-comparison} provides a summary of these
studies, indicating the largest scale (in terms of the number of
GPUs/GCDs/TPUs) used and the corresponding flop/s achieved. The last set of
row in the table presents the results of this work using our open-source
training framework, AxoNN. With the exception of MegaScale, AxoNN has been
scaled to the largest number of NVIDIA GPUs. To the best of our knowledge,
AxoNN is the first framework to run on up to 32,768 AMD MI250X GCDs to achieve
a sustained flop/s performance of 1.381 Exaflop/s.
