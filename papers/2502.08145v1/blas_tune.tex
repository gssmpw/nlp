In deep neural networks, a significant portion of the computational workload is
matrix multiplications kernels or ``matmuls``, particularly in transformer
models. These matmuls can be performed in one of three main modes based on
whether the operands are transposed: NN, NT, and TN. Prior research has
highlighted that NT and TN kernels are often less optimized than NN kernels in
most BLAS libraries~\cite{shi2017tnvnn}. In our experiments, we found this
discrepancy to be more pronounced when running transformers with large hidden
sizes on the AMD MI250X GPUs of Frontier. For example, in the GPT-320B model
(described in Table~\ref{tab:setup-perf-gpt}), we observed that a matrix
multiply defaulting to the TN mode in PyTorch achieved only 6\% of the
theoretical peak performance, whereas other matmuls reached 55\% of the peak.

To address this issue, we implemented an automated tuning strategy in which,
during the first batch, each matmul operation in the model is executed in all
three modes (NN, NT, and TN) and timed. We then select the most efficient
configuration for each operation, which is subsequently used for the remaining
iterations. This tuning approach ensures that our deep learning framework,
AxoNN, avoids the pitfalls of using suboptimal matmuls that could significantly
degrade performance. For the aforementioned 320B model, our BLAS kernel tuning
approach successfully switches the poorly performing TN matmul with a nearly
8$\times$ faster NN matmul, thereby reducing the total time spent in
computation from 30.1 seconds to 13.19s! Note that for other models used in
Table~\ref{tab:setup-perf-gpt}, the speedups attained via tuning are relatively
modest (See Figure~\ref{fig:weak-scaling-breakdown}).
