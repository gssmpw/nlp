% include scalability (weak and strong), time to solution, efficiency (of
% bottleneck resources), and peak performance

% \fix{2 pages max}

We now discuss the results of our performance benchmarking
experiments described in Section~\ref{sec:measurement}.

\subsection{Weak Scaling Performance}

We first present the weak scaling performance of AxoNN on Perlmutter, Frontier
and Alps using GPT-style transformers as the application in
Figure~\ref{fig:weak-scaling-time}.  We observe that on all three systems,
AxoNN achieves near-ideal weak scaling up to 4096 GPUs/GCDs. This is
particularly promising because most large-scale LLM training falls within this
hardware range. When running the 60B model on 6144 H100 GPUs of Alps, we see a
small reduction in efficiency -- 76.5\% compared to the performance on 1024
GPUs.

\begin{figure}[t]
  \centering
    \includegraphics[width=\columnwidth]{figs/weak_scaling_time.pdf}
    \caption{Weak scaling performance (time per batch or iteration) of AxoNN on Frontier, Perlmutter,
    and Alps for models with 5 to 320 billion parameters.}
    \label{fig:weak-scaling-time}
\label{fig:weak}
\end{figure}

Since Frontier has a significantly large number of GPUs than the other two
platforms, we scaled AxoNN on Frontier to 32,768 GCDs. We see near perfect weak
scaling up to 8,192 GCDs with a significantly high efficiency of 88.3\%
(compared to the performance on 512 GCDs). Although our weak performance drops
at 16,384 GCDs, we are still able to sustain an efficiency of 79.02\%.
However, with rising overheads of communication, there is a notable decline in
our performance on 32,768 GCDs, and a corresponding drop in efficiency to
53.5\%.

\begin{figure}[h]
  \centering
    \includegraphics[width=\columnwidth]{figs/prog_opt.pdf}
    % \includegraphics[width=\columnwidth]{figs/weak_scaling_flops.pdf}
    % \caption{Time (left) total flop/s (right)}
    \caption{The impact of our performance optimizations on weak scaling of GPT
models.  For the bars labeled ``Perf model'', we use the best out of the top-10
configurations suggested by our communication model. For the bars labeled
``Kernel Tuning'' and ``Comm Overlap'', we enable our matrix multiplication
tuning and communication overlap optimizations.}
    \label{fig:weak-scaling-breakdown}
\label{fig:weak}
\end{figure}

We used timers to gather breakdowns of the time per batch into computation and
non-overlapped communication to better understand the impact of the performance
optimizations described in Section~\ref{sec:innovations}.  We present these
results in Figure~\ref{fig:weak-scaling-breakdown}, for some model sizes
running on 512--8,192 GCDs of Frontier.  As a baseline, we use a configuration
of AxoNN that corresponds to a hybrid of 1D tensor parallelism within node
(similar to Megatron-LM~\cite{megatronlm}) and hybrid sharded data parallelism
across nodes (similar to FSDP~\cite{fsdp,wang2023zero}).

We observe that using the 3D parallel matrix multiplication and performance
model to select the best configuration results in significant performance
improvements of 13-45\% over the baseline.  Most of the improvement comes from
a significant reduction in communication times. For the models in the plot, the
improvements in the batch times due to our BLAS kernel tuning are relatively
modest (2--4\%). Finally, the improvement from our overlap optimizations is
largest for the largest model in this series i.e.  80B on 8192 GCDs. In this
case, we observe a 22\% reduction in the batch times!  This is expected because
the overheads of communication tend to increase with scale and subsequently the
benefits of our overlap optimizations become more pronounced.

\begin{figure}[h]
  \centering
    \includegraphics[width=\columnwidth]{figs/weak_scaling_flops.pdf}
    \caption{Sustained flop/s on different platforms. The FLOP count
is calculated analytically for all the matrix multiplication kernels in the
code.}
\label{fig:flops}
\end{figure}

\subsection{Sustained floating point operations per second (flop/s)}

Next, we examine the floating-point operations per second (flop/s) achieved by
AxoNN. In Figure~\ref{fig:flops}, we present the total bf16 flop/s sustained by
AxoNN in our weak scaling experiments on Perlmutter, Frontier and Alps. In
Table~\ref{tab:flops}, we also show our sustained flop/s as a percentage of the
vendor advertised and empirical obtained peak flop/s. As discussed in
Section~\ref{sec:sys-and-env}, we use 280 Tflop/s, 125 Tflop/s, and 813 Tflop/s
as the empirical peak bf16 flop/s for an A100 GPU, an MI250X GCD and an H100
GPU respectively.

On Perlmutter, we observe that AxoNN consistently sustains 
50\% or higher fraction of the advertised peak of 312 Tflop/s per GPU.
% In fact, on 1024 GPUs
% we see a significant increase to 60\% of the peak!
As a result of our near
perfect weak scaling, we observe that the sustained flop/s also increase
linearly from 80.8 Pflop/s on 512 GPUs by nearly $8
\times$ to 620.1 Pflop/s on 4096 GPUs. Since
the advertised and empirical peak bf16 flop/s of an A100 GPU are close
(312 vs.~280 Tflop/s), our \% flop/s numbers 
are also in the same ball park. 

\begin{table}[h]
  \centering
  \caption{Sustained flop/s for weak scaling on Perlmutter, Frontier and Alps.~\label{tab:flops}}
  \begin{tabular}{crrrcc}
  \toprule
  & \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}\# GPUs\\/ GCDs\end{tabular}} & Model & \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}Total \\ Pflop/s\end{tabular}} & \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}\% of \\ Advertised Peak\end{tabular}} & \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}\% of \\ Empirical Peak\end{tabular}} \\ \midrule
  \multirow{4}{*}{\begin{tabular}{@{}c@{}}\rotatebox[origin=c]{90}{Perlmutter}\end{tabular}} & 512 & 5B & 80.8 & 50.6 & 56.2  \\
  & 1024 & 10B & 197.8 & 61.9 & 68.8 \\
  & 2048 & 20B & 352.5 & 55.2 & 61.3 \\
  & 4096 & 40B & 620.1 & 48.5 & 53.9 \\ \midrule
  \multirow{7}{*}{\begin{tabular}{@{}c@{}}\rotatebox[origin=c]{90}{Frontier}\end{tabular}} & 512   & 5B   & 40.4  & 41.1 & 63.3 \\
  & 1024  & 10B  & 77.3  & 39.3 & 60.4 \\
  & 2048  & 20B  & 145.7 & 37.0 & 57.0 \\
  & 4096  & 40B  & 295.9 & 37.6 & 57.9 \\
  & 8192  & 80B  & 571.4 & 36.3 & 56.0 \\ 
  & 16384 & 160B & 1019.9 & 32.4 & 49.9 \\ 
  & 32768 & 320B & 1381.0 & 22.0 & 33.8 \\ \midrule
  \multirow{4}{*}{\begin{tabular}{@{}c@{}}\rotatebox[origin=c]{90}{Alps}\end{tabular}} & 1024   & 10B   & 310.0  & 30.6 & 37.3 \\
  & 2048  & 20B  & 621.6  & 30.7 & 37.4 \\
  & 4096  & 40B  & 1095.8 & 27.0 & 33.0 \\
  & 6144  & 60B  & 1423.1 & 23.4 & 28.6 \\ \bottomrule
  \end{tabular}
\end{table}

% AxoNN achieves near-perfect weak scaling in terms of sustained FLOP/s on Frontier between 512 and 4096 GCDs. This translates to a 
% throughput of around 40.5\% of the advertised peak performance.
On Frontier, in the 512 to 4,096 GCD range, AxoNN achieves near-perfect weak
scaling in terms of sustained flop/s which translates to a throughput of around
40\% of the advertised peak performance. Notably, this is a significant
improvement over Yin et al.~\cite{yin2023forge} and Dash et
al.~\cite{dash2023optimizing} -- they achieved a peak of only 30\% in a similar
range of GCDs, model sizes, and batch sizes on Frontier.  AxoNN continues to
scale well up to 8,192 GCDs, sustaining 36.3\% of the peak and 571.4 Pflop/s in
total. Beyond this scale, we start observing scaling inefficiencies. On 16,384
GCDs, we achieve 32.4\% of the peak, which amounts to 1.02 Exaflop/s in total.
Finally on 32,768 GCDs, our performance drops to 22\% of the peak and a total
flop/s of 1.381 Exaflop/s. In Section~\ref{sec:measurement}, we mentioned
a significant difference between the advertised peak and the empirically
measured peak on a single MI250X GCD (192 vs.~125 Tflop/s). As a result, there
is a large difference between AxoNN's flop/s expressed as a percentage of the
advertised peak versus the empirical peak. For instance on 32,768 GCDs, these
numbers are 22.0\% and 33.8\% respectively. 

On Alps, we observe a similar trend as Perlmutter, with AxoNN consistently
sustaining \tweakedsim30\% of the advertised peak up to 4096 GPUs. At 6144
GPUs, we see a slight drop to 23.42\% of peak.  At 6144 GPUs, we achieve our
highest sustained flop/s of 1423.10 Pflop/s across all three machines.

\subsection{Predicted Time-to-solution}

The training of state-of-the-art large language models (LLMs) presents a
significant computational challenge due to two key factors. First, the models
themselves are large, with current state-of-the-art models comprising hundreds
of billions of trainable parameters. Second, LLMs are trained on massive and
continually expanding text corpora, often containing trillions of tokens. In
this section, we show how AxoNN can significantly reduce the time-to-solution
of training such state-of-the-art LLMs on large text corpora. To demonstrate
this, we pick the 80B and 640B parameter GPT models from
Table~\ref{tab:setup-perf-gpt} and collect the per iteration times at various
GCD counts.  We run the 80B model on 128 to 8,192 GCDs on Frontier, and the
640B model on 512 to 8,192 GCDs. We then extrapolate the batch times to
estimate the time it would take to rain these models to completion i.e.~to
ingest two trillion tokens.  These time-to-solution results are presented in
Figure~\ref{fig:strong}. Note that both the model size and the number of tokens
are representative of modern LLM training setups such as Meta's
Llama~\cite{touvron2023llama}. 

\begin{figure}[h]
  \centering
    \includegraphics[width=\columnwidth]{figs/strong_scaling_eta.pdf}
    \caption{Strong scaling showing expected time-to-solution on Frontier. Using the average time per iteration, we predict the 
    training times for GPT-80B and GPT-640B on 2T tokens for various GCD counts.}
    \label{fig:strong}
\end{figure}

As the plot shows, training an 80B model on 128 GCDs will take 50 months or
more than four years. This emphasizes the critical role of large-scale
parallelism in LLM training.  As we scale to more GCDs, we see the expected
time to solution drop almost linearly till 8,192 GCDs. Our estimate for the
total training time of the 80B model on 8,192 GCDs is a much more reasonable
25.5 days.  For the 640B model, even a much larger GCD count of 512 GCDs is
impractical, with the estimated time-to-solution amounting to 14 years.
However, on 8,192 GCDs, the estimated total training time is 15 months, which
is an 11$\times$ improvement.  For both models, this amounts to a strong
scaling efficiency of more than 90\%. These experiments underscore AxoNN's
efficacy in significantly reducing the pre-training time for cutting-edge LLMs
trained using massive datasets. By enabling faster training cycles on large
scale multi-GPU clusters such as Frontier and Alps, AxoNN has the potential to
accelerate the overall pace of LLM research and development.

