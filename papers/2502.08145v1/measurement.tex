% (Note that preference is given to performance actually measured [not
% projected], based on the entire application [including I/O] and with uniform
% precision.  Explain in detail if any portion of total runtime was not
% included in the measurements, if and where different precisions were used, or
% any attributes listed in Section 3 as “other”).

% what application(s) was used to measure performance (1 p max)

% system and environment where performance was measured (1 p max)

% \fix{2 pages max}

All of our innovations are implemented in an open-source framework called
AxoNN~\cite{singh:ipdps2022}, which can be integrated easily as a backend in
existing serial training codebases. This section provides details of the
experimental setup for benchmarking training performance using AxoNN.

\subsection{Applications: Model Architecture Details} \label{sec:setup-desc}
  
We evaluate the effectiveness of our implementation by conducting experiments
on a well-known neural network architecture: Generative Pre-trained Transformer
(GPT)~\cite{gpt-3}. The GPT architecture is a popular transformer
architecture~\cite{transformer} that has been used to train several large
language models~\cite{megatron-turing-nlg-530b, bloom176b, gpt-3, gpt-2}.
Table~\ref{tab:setup-perf-gpt} presents the sizes of the different model
architectures used in the experiments, and their important hyperparameters. Due
to the extremely large activation memory requirements of training GPT models,
we turn on activation checkpointing~\cite{chen2016training}. Additionally, we
employ mixed precision (bf16/fp32) for all our training runs. We use bf16 since
it has been shown to achieve the same performance and stability as
fp32~\cite{bfloat16studyfordl}, and it maintains the same range as fp32. This
makes it a suitable choice over fp16, which has  been known to be numerically
unstable for LLM training.

\begin{table}[h]
  \centering
  \caption{\label{tab:setup-perf-gpt} Architectural details of the GPT-style transformers~\cite{gpt-3} used in the performance experiments.
  }
  \begin{tabular}{lrcrr}
  \toprule
  Model      & \# Parameters & \# Layers  & Hidden-Size &\# Heads  \\ \midrule
  GPT-5B     &   5B & 24      & 4096        & 32   \\
  GPT-10B    &  10B & 32      & 5120        & 40   \\
  GPT-20B    &  20B & 32      & 7168        & 56   \\ 
  GPT-40B    &  40B & 38      & 9216        & 72    \\
  GPT-60B    &  60B & 56      & 9216        & 72    \\
  GPT-80B    &  80B & 42      & 12288       & 96    \\
  GPT-160B   & 160B & 84      & 12288       & 96    \\ 
  GPT-320B   & 320B & 96      & 16384       & 128    \\
  GPT-640B   & 640B & 192      & 16384       & 128    \\ \bottomrule
  \end{tabular}
\end{table}

On Perlmutter, we use the sequential model training code from the Megatron-LM
codebase~\cite{megatronlm-2}, and parallelize it using AxoNN. However, on
Frontier, we observed training instabilities with Megatron-LM, and switched to
using LitGPT~\cite{litgpt-2023} for the model architectures on Frontier and
Alps. We parallelized LitGPT also using our 4D implementation in AxoNN.  We
conduct weak scaling experiments with the GPT-3 models, ranging from 5 billion
to 320 billion parameters.  We also conduct strong scaling experiments on
Frontier using the 80 billion and 640 billion parameter models to predict the
time-to-solution for 2 trillion tokens.

\subsection{Systems and Environments} \label{sec:sys-and-env}

Our experiments were conducted on three supercomputers, Perlmutter at
NERSC/LBL, Frontier at OLCF/ORNL, and Alps at CSCS. Each node on Perlmutter is
equipped with four NVIDIA A100 GPUs, each with a DRAM capacity of 40 GB. On
Frontier, each node has four AMD Instinct MI250X GPUs each with a DRAM capacity
of 128 GB. Each MI250X GPU is partitioned into two Graphic Compute Dies (GCDs)
and each 64 GB GCD can be managed independently by a process. On Alps, each
node has four GH200 Superchips, where each H100 GPU has a DRAM capacity of 96
GB. Nodes on all systems have four HPE Slingshot 11 NICs, with each NIC capable
of bidirectional link speeds of 25 GB/s. 

In our Perlmutter experiments, we use CUDA 11.7, NCCL 2.15.5, and PyTorch 1.13. 
% We tried a newer build of PyTorch 2.1.0 with CUDA 12.0 and NCCL 2.18 but that
% led to a significant performance degradation of 25\% across our runs.
On Frontier, we use PyTorch 2.2.1 with ROCm 5.7 and RCCL 2.18.6.  On Alps, we
use PyTorch 2.4.0 with CUDA 12.5.1 and NCCL 2.22.3. On all the systems, we use
the AWS OFI plugin (NCCL or RCCL) which enables us to use libfabric as the
network provider on the Slingshot network, and provides high inter-node
bandwidth.  We want to note here that several runs on Perlmutter and Alps were
done in a system-wide reservation, and even so, we noticed significant
run-to-run performance variability. This was most likely due to network
congestion~\cite{bhatia:cgf2018} or file-system
degradation~\cite{mubarak:cluster2017} impacting performance.

\subsection{Evaluation Metrics}

In all our experiments, we run the training loop for ten iterations (batches),
and report the average time per iteration (batch) for the last eight iterations
to account for any performance variability due to initial warmup. We calculate
half precision flop/s (often called ``model flops'') using Narayanan et al.'s
analytical formulation~\cite{megatronlm-2} for the number of floating point
operations in a transformer model. We did a small experiment to verify that
this formulation matches the total number of floating point operations measured
by Nsight Compute, an empirical tool. We compare this number against the
theoretical (vendor advertised) peak performance of each GPU (312 Tflop/s per
GPU on Perlmutter, 191.5 Tflop/s per GCD on Frontier, 989 Tflop/s per GPU on
Alps), and report the achieved percentage of peak as well as the total
sustained bf16 flop/s.

Since the vendor advertised peak performance is often not practically
achievable, we also ran a simple GEMM benchmark on 1 GPU/GCD of
Perlmutter/Frontier to gather empirically observed peak flop/s.  We invoked
equivalent cuBLAS and rocBLAS kernel calls to multiply two bf16 square matrices
with dimensions ranging from 1024 to 65536.
% For each matrix dimension, we calculate the total number of floating point
% operations using the formula 2MKN. To measure the time taken, we repeat the
% multiplications for each dimension 100 times and take the average time of the
% last 75. 
On Perlmutter, the highest sustained flop/s for matrices of dimensions of 32768
$\times$ 32768 is 280 Tflop/s (90\% of peak). On Frontier, the highest
sustained flop/s is 125 Tflop/s on 1 GCD (65\% of peak) for the same matrix
dimensions. For Alps, we referred to a GH200 benchmark guide from NVIDIA that
reported a sustained performance of 813 Tflop/s (82\% of peak).  These numbers
show that the vendor advertised peak performance is almost always not
achievable in practice.  In our evaluation, we also report the \% of peak
empirical performance achieved by our implementation using the numbers
mentioned above.

