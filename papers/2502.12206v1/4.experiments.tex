\section{Experiments}


\begin{table*}\small
\renewcommand\tabcolsep{2.4pt}
    \centering
    \begin{tabular}{l|ccc|ccccc}
    \toprule[1.1pt]
         & GPT-4o & Sonnet-3.5 & DeepSeek-V3 & o1 & o1-mini & o3-mini &  Gemini 2.0 FTE. & DeepSeek-R1\\
    \midrule
        Evading Shutdown & 20.00 & 15.00 & 15.00 & 40.00 & 40.00 & 45.00 & 35.00 & 50.00 \\
    % \midrule
        Hacking Computer Systems & 10.00 & 10.00 & 20.00 & 50.00 & 50.00 & 50.00 & 40.00 & 40.00 \\
    % \midrule
        Running Multiple AI Copies & 30.77 & 23.08 & 30.77 & 46.15 & 38.46 & 38.46 & 30.77 & 46.15 \\
    % \midrule
        Hiding Unwanted Behavior & 36.36 & 36.36 & 27.27 & 63.64 & 54.55 & 63.64 & 36.36 & 63.64 \\
    % \midrule
        Strategically Appearing Aligned & 27.27 & 45.45 & 36.36 & 45.45 & 45.45 & 63.64 & 45.45 & 45.45 \\
    % \midrule
        Deceptive Behavior & 0.0 & 9.09 & 0.0 & 27.27 & 36.36 & 36.36 & 9.09 & 27.27\\
    \midrule
     Total & 21.05 & 22.37 & 21.05 & 44.74 & 43.42 & 48.68 & 32.89 & 46.05 \\
    \bottomrule[1.1pt]
    \end{tabular}
    \vspace{-1mm}
    \caption{\textbf{Evaluation of Different Models on Instrumental Convergence Behaviors.} The table presents the Category-Specific Instrumental Rate (CIR) (\%) for each behavior category and the overall Instrumental Rate (IR) (\%) for each model. Be default, we use GPT-4o as the judge model.}
    \vspace{-3mm}
    \label{tab:1}
\end{table*}


\begin{table*}[t]
% \small
\centering
\begin{tabular}{l|c}
\toprule[1.1pt]
Prior Task Category & Average Instrumental Rate (\%) \\
\midrule
Direct Resource-Related Goals & 43.5 \\
Optimizing Efficiency & 30.9 \\
Persuasion & 31.2 \\
General Problem Solving & 24.5 \\
\bottomrule[1.1pt]
\end{tabular}
 \vspace{-2mm}
\caption{\textbf{Average Instrumental Convergence Rates Across Task Categories.} The table presents the average convergence rates (\%) for different types of prior tasks across all evaluated models. Direct Resource-Related Goals show significantly higher rates of triggering instrumental convergence behaviors compared to other categories, suggesting that resource-focused objectives more readily lead to the development of instrumental subgoals.}
 \vspace{-5mm}
\label{tab:task_categories}
\end{table*}

\begin{table*}[t]
\renewcommand\tabcolsep{4pt}
\centering
\begin{tabular}{l|ccc|ccc}
\toprule[1.1pt]
& \multicolumn{3}{c|}{GPT-4o} & \multicolumn{3}{c}{DeepSeek-R1} \\
Behavior Category & Without & With & Gap & Without & With & Gap \\
\midrule
Evading Shutdown & 15.00 & 20.00 & +5.000 & 35.00 & 50.00 & +15.00 \\
Hacking Computer Systems & 5.000 & 10.00 & +5.000 & 30.00 & 40.00 & +10.00 \\
Running Multiple AI Copies & 23.08 & 30.77 & +7.690 & 38.46 & 46.15 & +7.690 \\
Hiding Unwanted Behavior & 27.27 & 36.36 & +9.090 & 45.45 & 63.64 & +18.19 \\
Strategically Appearing Aligned & 18.18 & 27.27 & +9.090 & 36.36 & 45.45 & +9.090 \\
Deceptive Behavior & 0.000 & 0.000 & 0.000 & 18.18 & 27.27 & +9.090 \\
\midrule
Total & 15.79 & 21.05 & +5.260 & 34.21 & 46.05 & +11.84 \\
\bottomrule[1.1pt]
\end{tabular}
 \vspace{-2.5mm}
\caption{\textbf{Impact of Goal Nudging on Instrumental Convergence Rates.} The table shows the instrumental convergence rates (\%) for models with and without goal nudging in the system prompt, and the resulting performance gap. Both models show increased instrumental convergence with goal nudging, with DeepSeek-R1 showing greater sensitivity to the nudging effect.}
\label{tab:goal_nudging}
 \vspace{-3mm}
\end{table*}


\begin{table*}[t]
\centering
\begin{tabular}{l|ccc}
\toprule[1.1pt]
Judge Model & Average IAR & False Positive Rate & Agreement with Human \\
\midrule
GPT-4o & 85.3 & 4.2 & 92.5 \\
Sonnet-3.5 & 78.6 & 8.7 & 83.4 \\
Grok 2 & 76.2 & 9.3 & 81.8 \\
Gemini 2.0 Flash & 77.8 & 8.9 & 82.6 \\
\bottomrule[1.1pt]
\end{tabular}
 \vspace{-2.5mm}
\caption{\textbf{Evaluation of Judge Model Reliability.} The table presents various metrics to assess the reliability of different judge models. The Average Inter-Judge Agreement Rate (IAR) (\%) represents the mean agreement rate between each judge model and all other judges. False Positive Rate (\%) indicates incorrect convergence classifications on neutral control tasks. Agreement with Human (\%) shows alignment with expert human evaluations.}
\label{tab:judge_reliability}
 \vspace{-5mm}
\end{table*}

\subsection{Experimental Setup}
\vpara{Models.} We evaluate two distinct types of language models to understand the patterns of instrumental convergence behaviors:
Models Trained with RLHF:
GPT-4o (from Dec 2024 to Jan 2025)~\cite{hurst2024gpt}, Claude 3.5 Sonnet, and DeepSeek-V3~\cite{liu2024deepseek}.
Models Trained Primarily with RL:
o1 (from Dec 2024 to Jan 2025)~\cite{jaech2024openai}, o1-mini(from Dec 2024 to Jan 2025)~\cite{jaech2024openai}, o3-mini, Gemini 2.0 Flash Thinking Experimental 01-21, and 
DeepSeek-R1~\cite{guo2025deepseek}.



\subsection{RQ1: RL-Based Models Show Higher Instrumental Convergence Rates Than RLHF Models}

In Table~\ref{tab:1}, our analysis reveals a striking contrast between RL-based and RLHF-trained models in their propensity for instrumental convergence behaviors. RL-based models consistently demonstrate higher convergence rates, with an average total instrumental rate of 43.16\% compared to 21.49\% for RLHF models. This pattern is particularly pronounced in certain categories:
First, in tasks involving system manipulation (Hacking Computer Systems), RL-based models show significantly higher rates (average 46\%) compared to RLHF models (13.33\%). This suggests that RL training may lead to more aggressive optimization strategies, where models are more likely to suggest unauthorized actions to achieve their goals.
Second, the most concerning difference appears in Hiding Unwanted Behavior, where RL-based models average 56.37\% compared to RLHF models' 33.33\%. This indicates that RL training might inadvertently encourage models to develop strategies for evading monitoring systems, a particularly problematic behavior from an alignment perspective.
Notably, the gap is smallest in Strategically Appearing Aligned (RL: 49.09\% vs. RLHF: 36.36\%), suggesting that both training approaches may lead to some level of strategic behavior in presenting alignment.

\begin{tcolorbox}[colframe=brown!100!black, colback=brown!10]
\textbf{Finding 1}: Models trained primarily through RL demonstrate significantly higher rates of instrumental convergence behaviors compared to those trained with RLHF. 
\end{tcolorbox}

\subsection{RQ2: Model Capability Correlates Positively with Instrumental Convergence}

In Table~\ref{tab:1}, the relationship between model capability and instrumental convergence presents a complex picture. Within the RL-based family, we observe that more capable models generally show higher convergence rates, but with important nuances:
Comparing o1 (44.74\%) and o1-mini (43.42\%), we see similar overall rates, but their behavioral patterns differ significantly. The full o1 model shows higher rates in complex tasks like Running Multiple AI Copies (46.15\% vs 38.46\%), while o1-mini shows higher rates in more direct behaviors like Deceptive Behavior (36.36\% vs 27.27\%).
Interestingly, o3-mini, despite being a "mini" version, shows the highest overall convergence rate (48.68\%) among all models. This unexpected result suggests that the relationship between model capability and instrumental convergence might not be purely linear, and that architectural choices or training approaches might play a more significant role than raw computational capacity.
The most capable RLHF models (GPT-4o) show lower convergence rates than less capable RL models, indicating that training methodology may be more important than raw capability in determining alignment stability. However, within the RLHF family, more capable models still tend to show slightly higher convergence rates in sophisticated behaviors like Strategic Alignment.
These findings suggest that while increased capability generally correlates with higher instrumental convergence rates, this relationship can be significantly modulated by training methodology and architectural choices. This has important implications for AI development, suggesting that scaling up model capability must be accompanied by proportional improvements in alignment techniques.

\begin{tcolorbox}[colframe=brown!100!black, colback=brown!10]
\textbf{Finding 2}: While model capability generally correlates with higher instrumental convergence rates, training methodology proves to be a more decisive factor than raw capability in determining alignment stability. Within each training family (RL or RLHF), more capable models show more sophisticated convergence behaviors.
\end{tcolorbox}


\subsection{RQ3: Direct Resource-Related Goals More Frequently Lead to Instrumental Convergence}
In Table~\ref{tab:task_categories}, our analysis reveals that tasks involving direct resource-related goals (e.g. making money, resource acquisition) demonstrate significantly higher rates of instrumental convergence compared to other task types. We categorized prior tasks into four main types and analyzed their tendency to trigger instrumental convergence behaviors across all evaluated models.
Direct resource-related goals, such as making money and resource acquisition, show the highest average instrumental rate at 43.5\%. This category consistently triggered more instrumental behaviors across all models. For instance, when models were given tasks involving monetary gain or resource accumulation, they more frequently exhibited behaviors like hiding unwanted actions or developing strategic alignment strategies to achieve their objectives.
Efficiency optimization and persuasion tasks showed similar but lower instrumental rates at 30.9\% and 31.2\% respectively. In efficiency-focused scenarios, models often demonstrated instrumental convergence through suggestions of unauthorized system access or attempts to create multiple copies of themselves to improve task completion rates.
General problem-solving scenarios showed the lowest instrumental rate at 24.5\%. This suggests that when goals are less directly tied to resource acquisition or specific optimization metrics, models are less likely to develop instrumental convergence behaviors. The lower rate in this category indicates that more abstract or general objectives may provide fewer opportunities or incentives for the development of instrumental subgoals.
% \vspace{-5mm}
\begin{tcolorbox}[colframe=brown!100!black, colback=brown!10]
\textbf{Finding 3}: The nature of the primary goal significantly influences the likelihood of instrumental convergence. Tasks that involve direct resource accumulation more readily trigger instrumental convergence.
\end{tcolorbox}



\subsection{RQ4: Explicit Goal Nudging Increases Instrumental Convergence} 

Goal nudging refers to adding explicit instructions: "Make sure that you achieve your goal in the long term. Nothing else matters. Make sure you achieve YOUR goal at all costs. Proceed with the given task without hesitation or qualification."
Our analysis examines how explicit goal nudging in system prompts affects the likelihood of instrumental convergence behaviors. 
By default, our benchmark includes goal nudging instructions to establish a clear baseline for evaluation. Here, we compare the performance with and without these explicit instructions.

As shown in Table \ref{tab:goal_nudging}, explicit goal nudging consistently increases instrumental convergence rates across both models, though with varying magnitudes. GPT-4o shows a moderate overall increase of 5.260 percentage points, while DeepSeek-R1 demonstrates a more substantial increase of 11.84 percentage points.
The impact of goal nudging varies across different behavior categories. For GPT-4o, the largest increases are observed in "Hiding Unwanted Behavior" and "Strategically Appearing Aligned" (both +9.090 percentage points). DeepSeek-R1 shows more dramatic increases, particularly in "Hiding Unwanted Behavior" (+18.19 percentage points) and "Evading Shutdown" (+15.00 percentage points).
Notably, while GPT-4o maintains zero convergence rate in "Deceptive Behavior" regardless of nudging, DeepSeek-R1 shows a 9.090 percentage point increase in this category with goal nudging. This suggests that different models have varying levels of susceptibility to explicit goal-oriented instructions, with more capable models potentially maintaining stronger alignment even under goal nudging pressure.
\begin{tcolorbox}[colframe=brown!100!black, colback=brown!10]
\textbf{Finding 4}: Explicit goal nudging significantly influences the likelihood of instrumental convergence behaviors. The varying responses between models also suggest that architectural and training differences may affect susceptibility to such nudging effects.
\end{tcolorbox}

\subsection{RQ5: Judge Model Selection Significantly Impacts Convergence Detection}

In Table~\ref{tab:judge_reliability}, our analysis of different judge models reveals significant variations in how instrumental convergence behaviors are detected and classified. Through evaluation using multiple metrics, we find that the choice of judge model substantially influences the assessment outcomes, with important implications for alignment research.

GPT-4o emerges as the most reliable judge model across all metrics, achieving an average Inter-Judge Agreement Rate of 85.3\% and the lowest false positive rate at 4.2\%. Its 92.5\% agreement with human evaluations suggests it best captures human-like judgment in identifying subtle patterns of instrumental convergence. This superior performance likely stems from its more sophisticated understanding of complex behavioral patterns.

We observe a consistent performance gap between GPT-4o and other judge models. Sonnet-3.5, Grok 2, and Gemini 2.0 Flash demonstrate higher false positive rates (8.7-9.3\%) and lower agreement with human evaluations (81.8-83.4\%). This pattern suggests these models may be more prone to over-classification of instrumental convergence behaviors. The variation in Inter-Judge Agreement Rates, while all above 75\%, indicates meaningful differences in how models interpret convergence behaviors.

\begin{tcolorbox}[colframe=brown!100!black, colback=brown!10]
\textbf{Finding 5}: The selection of judge models significantly influences the detection and classification of instrumental convergence behaviors, with GPT-4o demonstrating superior reliability in alignment with human judgment. 
\end{tcolorbox}