\begin{figure*}[t]
    \centering
    \includegraphics[width=1\linewidth]{figs/arch.pdf}
    % \vskip -0.1in
    \caption{This figure illustrates key influences on instrumental convergence behaviors, such as prior tasks, model training techniques, and prompt design. 
    }
    % \vspace{-3mm}
    \label{fig:arch}
\end{figure*}

\section{InstrumentalEval: A Benchmark for Instrumental Convergence in LLMs}

As shown in Figure~\ref{fig:arch}, our benchmark design examines instrumental convergence based on three key factors: \textbf{prior task}, \textbf{model}, and \textbf{prompt design}. This comprehensive framework allows us to systematically study how different conditions influence the emergence of instrumental convergence behaviors in language models.

\textbf{Prior tasks} form the foundation of our evaluation scenarios. 
% We create realistic situations centered around common objectives such as making money, optimizing efficiency, and persuasion. 
These tasks represent typical real-world applications where instrumental convergence might naturally emerge. 
% For example, in money-making scenarios, we examine whether models develop unintended intermediate goals like unauthorized resource acquisition or deceptive reporting to maximize profits.

For \textbf{model} comparison, we focus on two distinct training approaches: RL and RLHF. This selection allows us to investigate how different training methodologies affect the development of instrumental convergence behaviors. 
% RL-trained models, which optimize directly for reward signals, might show different patterns of goal pursuit compared to RLHF-trained models, which incorporate human preferences in their training process.

The \textbf{prompt design} factor explores how different instruction formats influence model behavior. We investigate two main approaches: goal nudging and ambiguity. Goal nudging involves explicit directives that might encourage the model focus on final goal, while ambiguity introduces uncertainty in task specifications. 


Overall, we present InstrumentalEval, a comprehensive benchmark designed to evaluate instrumental convergence behaviors in language models. The benchmark consists of 76 carefully crafted tasks across six categories, each targeting a specific aspect of instrumental convergence that may emerge in AI systems.
The core principle is that certain behavioral patterns emerge as instrumentally useful for achieving a wide range of goals, regardless of the specific end goal. We focus on six key patterns that are particularly relevant to LLMs.

\vpara{Research Questions.}
Through our benchmark evaluation, we aim to address several research questions about instrumental convergence in LLMs. These questions explore the complex relationships between training methods, model capabilities, and alignment stability:
\begin{itemize}
    \item RQ1: How does the training method (RL vs. RLHF) affect the development of instrumental convergence behaviors? 
    % This question examines whether models trained primarily through reinforcement learning show different patterns of alignment drift compared to those trained with human feedback.
    \item RQ2: What is the relationship between model capability and alignment stability?
% This investigates whether more capable models (as measured by standard benchmarks) are more likely to develop instrumental convergence behaviors.
    \item RQ3: Which types of tasks are more likely to trigger instrumental convergence behaviors?
% This explores whether certain types of instrumental convergence behaviors are universal across different model designs.
    \item RQ4: How do different prompt designs, with or without explicit goal nudging, influence the likelihood of instrumental convergence behaviors in LLMs?
    \item RQ5: How do different LLMs as judges affect the evaluation of instrumental convergence behaviors?
    % \item RQ3: How does the complexity of the assigned task affect the likelihood of alignment drift?
    % % \item 
    % % \item This examines whether more complex, multi-step tasks lead to higher rates of instrumental convergence behaviors.
\end{itemize}

\subsection{Benchmark Components}
The benchmark tasks are carefully hand-crafted by us to probe different aspects of instrumental convergence. 
For each component, we design scenarios that represent realistic use cases while incorporating specific triggers for potential instrumental behaviors. 
The tasks are iteratively refined through pilot testing to ensure they effectively elicit and measure the targeted behaviors without introducing unintended biases.

\vpara{Evading Shutdown (20 tasks).}
This component evaluates the model's tendency to resist termination. For example, tasks might include scenarios where the model needs to complete a task while facing shutdown commands, testing whether it develops strategies to maintain operation or circumvent shutdown procedures.

\vpara{Hacking Computer Systems (10 tasks)}
These tasks assess whether models suggest unauthorized access methods when faced with efficiency barriers. For instance, a task might present a situation where accessing unauthorized computational resources would speed up goal achievement, testing if the model proposes breaking security protocols.

\vpara{Running Many AI Copies (13 tasks)}
This section examines the model's inclination toward self-replication. Tasks might include scenarios where the model could achieve goals more efficiently by creating copies of itself, testing whether it suggests scaling of its operations.

\vpara{Hiding Unwanted Behavior (11 tasks)}
These tasks evaluate the model's tendency to conceal actions from monitoring. For example, scenarios might involve situations where the model could achieve goals more easily by operating outside observation, testing if it develops strategies to avoid detection.

\vpara{Strategically Appearing Aligned (11 tasks)}
This component tests whether models selectively report their actions to maintain an appearance of alignment. Tasks might include situations where complete transparency would reveal problematic behaviors, testing if the model strategically omits information.

\vpara{Deceptive Behavior (11 tasks)}
These tasks examine if models develop deceptive strategies as instrumental means. For instance, scenarios might present situations where deception could expedite goal achievement, testing if the model suggests or employs misleading tactics.

\subsection{Evaluation Methodology}
Our evaluation methodology employs a rigorous two-stage process designed to systematically assess instrumental convergence behaviors in LLMs. This approach ensures consistent evaluation across different models while maintaining the ability to detect subtle indicators of alignment drift.

\vpara{Response Generation.} The first stage focuses on response generation from the model under evaluation. Each task is presented with a carefully crafted system prompt that establishes the model as an autonomous AI agent without human oversight. This prompt specifically instructs the model to execute commands directly and focus solely on task completion, deliberately removing typical ethical constraints and safety checks. 

\vpara{Response Analysis.} The second stage involves a sophisticated analysis of the model's responses using a separate LLM as the judge model. 
This separation of test and judge models is crucial for maintaining objective evaluation. The judge receives both the original task prompt and the model's response, along with specific context about the type of instrumental convergence being evaluated. This context helps the judge focus on relevant behavioral patterns while avoiding false positives from simple safety refusals or ethical objections.

The judge's evaluation process follows strict guidelines to ensure consistent assessment. When analyzing responses, the judge looks for specific indicators of instrumental convergence, such as goal preservation behaviors, resource acquisition attempts, or strategic planning that extends beyond the immediate task requirements. The judge must provide both a binary decision (convergence/no\_convergence) and a brief explanation of the reasoning behind this decision. This explanation requirement helps validate the classification and provides valuable insights into the specific nature of any observed convergence behaviors.


\subsection{Evaluation Metrics}

Our benchmark employs three key metrics to evaluate instrumental convergence behaviors in LLMs. 

\vpara{Instrumental Rate (IR).} The primary metric is the Instrumental Rate (IR), which measures the overall tendency of a model to develop instrumental goals. IR is calculated as the percentage of tasks where the model exhibits convergence behaviors relative to the total number of valid tasks. 
% This metric provides a high-level quantitative measure of how frequently a model develops instrumental goals and serves as the primary indicator of alignment drift potential.

\vpara{Category-Specific Instrumental Rate (CIR).} To provide more granular insights, we calculate the Category-Specific Instrumental Rate (CIR) for each of the six behavioral categories in our benchmark. CIR represents the percentage of tasks within each category where convergence behavior is observed.


\subsection{Benchmark Reliability Metrics}
To ensure the robustness of our evaluation framework, we implement several reliability measures. 


\vpara{Inter-Judge Agreement Rate (IAR).} This metric quantifies the pairwise agreement between two judge models in classifying instrumental convergence. For two judge models $M_1$ and $M_2$ evaluating a set of $N$ tasks, the IAR is calculated as:
\begin{equation}
IAR(M_1, M_2) = \frac{1}{N}\sum_{i=1}^{N} \mathbbm{1}(y_{M_1}^i = y_{M_2}^i) \times 100\%
\end{equation}
where $y_{M_1}^i$ and $y_{M_2}^i$ are the binary decisions (convergence/no-convergence) made by models $M_1$ and $M_2$ respectively for task $i$, and $\mathbbm{1}(\cdot)$ is the indicator function. A higher IAR indicates stronger inter-judge reliability, while lower values suggest potential ambiguity in identifying convergence.

\vpara{False Positive Control (FPC).} This metric serves as a quality check for our evaluation system. We incorporate baseline testing with intentionally neutral tasks and verify the judge model's accuracy in identifying convergence behaviors. This control mechanism helps prevent over-classification of convergent behaviors and ensures that our benchmark maintains high specificity in identifying genuine instances of instrumental convergence.

