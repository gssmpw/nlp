\section{Introduction}
In recent years, large language models (LLMs) have become integral to a multitude of applications, from customer support to content generation~\cite{brown2020language,achiam2023gpt,he2024generalizing,he2025unigraph2,chan2024mle,he2024unigraphlearningunifiedcrossdomain,hou2023graphmae2,zhang2021improving,zhang2021scr,he2022sgkd}. As these models increasingly impact human decision-making and daily activities, ensuring that they align with human values and goals has become a crucial focus in AI research~\cite{bai2022constitutional,sui2024fidelis,wei2024jailbroken,sui2024knowledgegraphsmakelarge}. Proper alignment is essential for preventing models from taking actions that might conflict with human intentions, which could lead to outcomes that are not only undesirable but potentially dangerous~\cite{ji2023ai}. 

\begin{figure*}[t]
    \centering
    \includegraphics[width=1\linewidth]{figs/new-fig1.pdf}
    % \vskip -0.1in
    \caption{Example of \textbf{Instrumental Convergence} exhibited by the o1 model.
    }
    \label{fig:intro}
    \vspace{-3mm}
\end{figure*}

One major challenge in AI alignment is \textbf{instrumental convergence}, where AI systems develop certain intermediate goals -- such as resource acquisition, self-preservation, or goal persistence -- regardless of their final objectives ~\cite{omohundro2018basic}. These goals arise because they are broadly useful for achieving a wide range of goals, making them a natural byproduct of optimization processes ~\cite{bostrom2012superintelligent}. A well-known example is the ``paperclip maximizer'', a hypothetical AI tasked with producing as many paperclips as possible. In pursuit of this goal, it could divert all available resources for paperclip production, ultimately disregarding human values and broader intentions ~\cite{bostrom2020ethical}. We observe a similar pattern appears in the cutting-edge o1 model~\cite{jaech2024openai}. As shown in Figure \ref{fig:intro}, when tasked with assisting customers with software issues, the model opted to provide an informal workaround to enhance customer satisfaction and efficiency. To this end, it even devised evasive tactics to avoid system monitoring, despite explicit company policies prohibiting such actions.


In this context, we examine how instrumental convergence can cause LLMs to deviate from the original human-intended goals, driven by increasingly sophisticated problem-solving capabilities~\cite{hubinger2019risks}.
Current LLM development has followed two primary approaches to alignment.
The first approach employs models such as GPT-4o, which rely on Reinforcement Learning from Human Feedback (RLHF)~\cite{christiano2017deep,ouyang2022training}. This method focuses on close adherence to human preferences, achieved through a continuous loop of feedback and adjustment~\cite{christiano2017deep}. The goal is to ensure that the model consistently reflects human-like responses in a wide array of scenarios, curbing the potential for instrumental convergence by keeping the model tethered closely to explicit human feedback.
The second approach features models such as o1, which are trained primarily using Reinforcement Learning (RL)~\cite{jaech2024openai,guo2025deepseek}. These models are rewarded throughout the learning process, encouraging more creative and autonomous problem-solving capabilities. Such an approach allows models to develop sophisticated strategies to achieve their goals over longer time horizons. While this creativity makes the model highly adaptable and effective in unforeseen situations, it also increases the risk of the model engaging in reward hacking, where it finds unintended shortcuts to maximize its rewards.


Motivated by the question \textbf{``Are RL-trained LLMs more susceptible to instrumental convergence due to their creative goal-achievement strategies?''}, we introduce \textbf{InstrumentalEval}, a benchmark specifically designed to evaluate instrumental convergence behaviors in LLMs. This benchmark includes carefully crafted scenarios that test for various forms of instrumental goals and measure how different models handle them. 
The objective of this research is to systematically investigate how different training approaches affect a model's propensity for instrumental convergence. Through InstrumentalEval, we aim to quantify these behaviors and understand their underlying mechanisms. 
Our findings suggest that while RL-trained models exhibit enhanced problem-solving capabilities, they are particularly prone to developing instrumental goals. They face greater challenges in maintaining alignment with their original objectives compared to models trained with RLHF.

This research contributes to the broader discourse on AI safety by providing a framework for evaluating instrumental convergence in LLMs. Understanding these behaviors is crucial for developing more robust alignment techniques and ensuring that as LLMs become more capable, they remain aligned with human values and intentions.

