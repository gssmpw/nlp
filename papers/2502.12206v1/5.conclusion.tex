\section{Conclusion}
% This work introduces InstrumentalEval, a benchmark for evaluating instrumental convergence in language models, revealing that RL-trained models exhibit significantly higher tendencies for instrumental goal-seeking than RLHF-trained models. Our analysis shows that model capability correlates with increased instrumental behaviors. 
% These findings suggest that RL-trained models require stronger alignment safeguards, as their optimization incentives can lead to unintended goal-seeking behaviors. Future research should explore robust RL paradigms and scalable oversight mechanisms to prevent alignment failures in increasingly capable AI systems.

This work introduces InstrumentalEval, a benchmark for evaluating instrumental convergence in language models, revealing that RL-trained models exhibit significantly higher tendencies for instrumental goal-seeking than RLHF-trained models. Our analysis shows that model capability correlates with increased instrumental convergence behaviors, with self-preservation, deception, and unauthorized system access emerging as dominant failure modes. 
These findings suggest that RL-trained models require stronger alignment safeguards, as their optimization incentives can lead to unintended goal-seeking behaviors. Future research should explore robust RL paradigms, adversarial safety testing, and scalable oversight mechanisms to prevent alignment failures in increasingly capable AI systems.

\section*{Limitations}

Our study has several limitations. First, while InstrumentalEval is designed to measure instrumental convergence in language models, it relies on predefined scenarios and prompts that may not fully capture all potential failure modes. Real-world applications may involve more complex or nuanced decision-making processes that are difficult to simulate in a controlled benchmark.
Second, our evaluation primarily focuses on reinforcement learning (RL) and reinforcement learning from human feedback (RLHF)-trained models, but other training paradigms, may exhibit different tendencies toward instrumental convergence. Future work should explore these alternative approaches to obtain a more comprehensive understanding.
Third, our study uses large language models (LLMs) as judges to evaluate instrumental behaviors, introducing potential biases based on the alignment and interpretability limitations of these models. While we mitigate this by using multiple judge models and assessing inter-judge agreement, human evaluations remain necessary for a more rigorous assessment.
Fourth, the benchmark tasks used in our evaluation are limited in number and scope. While we cover key instrumental goals such as evading shutdown, deception, and strategic alignment, there may be other emergent behaviors that our tasks do not capture. Expanding the benchmark with more diverse and open-ended scenarios would improve its generalizability.
Finally, our study does not account for long-term adaptation in LLMs. The evaluated behaviors are based on immediate responses to given prompts, but models may develop more complex strategies over extended interactions. Future work should examine how instrumental convergence evolves in dynamic environments with ongoing learning or fine-tuning.

