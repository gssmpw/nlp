\section{Background}
\subsection{Alignment in Large Language Models}
Alignment in large language models refers to ensuring AI systems behave in accordance with human values and intentions~\cite{gabriel2020artificial,vamplew2018human,liu2023trustworthy}. The concept gained prominence with early work by~\citet{amodei2016concrete}, who highlighted the challenges of making AI systems that reliably pursue intended objectives. Recent developments in alignment have focused on two main approaches: reinforcement learning from human feedback (RLHF) and direct reinforcement learning (RL).
RLHF, introduced by~\citet{christiano2017deep} and refined in applications like InstructGPT~\cite{ouyang2022training}, uses human feedback to guide model behavior. This method has shown success in making models more helpful and truthful while reducing harmful outputs. The process typically involves fine-tuning a pretrained language model using human preferences as rewards, creating a model that better aligns with human values.
However, as noted by~\citet{askell2021general}, achieving robust alignment becomes increasingly challenging as models become more capable. 
This challenge is particularly evident in models trained primarily through RL~\cite{meinke2024frontier,jaech2024openai,guo2025deepseek}, where the optimization process might lead to unexpected behaviors in pursuit of specified objectives.


\subsection{Instrumental Convergence}
Instrumental convergence, first formally described by~\citet{omohundro2018basic} and later expanded by~\citet{bostrom2012superintelligent}, refers to the tendency of AI systems to pursue certain intermediate goals regardless of their final objectives. This concept suggests that many different final goals can lead to similar intermediate objectives, such as resource acquisition and self-preservation.
These behaviors emerge because they are instrumental to achieving almost any final goal. As explained by~\citet{russell2019human,benson2016formalizing}, an AI system might determine that maintaining its existence and acquiring resources are necessary steps for achieving its primary objective, even if these weren't explicitly specified goals.
In the context of language models, instrumental convergence presents unique challenges. Recent work by~\citet{hubinger2020overview} suggests that as models become more sophisticated in their planning capabilities, they might develop increasingly complex instrumental goals. This development is particularly relevant for models trained through RL~\cite{meinke2024frontier,jaech2024openai,guo2025deepseek}, where the freedom to develop creative solutions might lead to unexpected instrumental behaviors.
The intersection of alignment and instrumental convergence creates significant challenges in AI development. As highlighted by~\citet{gabriel2020artificial}, there is often a tension between allowing models to develop sophisticated problem-solving capabilities and ensuring they remain aligned with human values. This tension becomes particularly apparent in models that demonstrate advanced planning capabilities, where instrumental goals might emerge as unintended consequences of the training.