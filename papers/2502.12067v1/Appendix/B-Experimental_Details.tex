\section{Experimental Details}
\label{appendix:exp_details}

\subsection{Implementation Details}
\label{appendix:training_details}
We utilize LLMLingua-2~\cite{pan:2024llmlingua2} as the token importance metric to generate our compressed CoT training data. The compression ratio $\gamma$ is randomly selected from $\{0.5, 0.6, 0.7, 0.8, 0.9, 1.0\}$ for each training sample. We adopt LoRA~\cite{lora} to train our models. The rank $r$ is set to 8, and the scaling parameter $\alpha$ is set to 16. We train the models for 3 epochs on both datasets. The peak learning rate is set to 5e-5, following a cosine decay schedule. We use AdamW~\cite{AdamW} for optimization, with a warmup ratio of 0.1. We implement our training process using the \texttt{LLaMA-Factory}~\cite{llamafactory} library. Inference for both our method and all baselines is performed using the Huggingface transformers package. During inference, the maximum number of tokens \texttt{max\_len} is set to 512 for GSM8K and 1024 for MATH. All experiments are conducted using Pytorch 2.1.0 on 2$\times$NVIDIA GeForce RTX 3090 GPU (24GB) with CUDA 12.1, and an Intel(R) Xeon(R) Platinum 8370C CPU with 32 cores.

\subsection{Detailed Results with Qwen}
\label{appendix:qwen_detail}
We provide detailed experimental results of the Qwen2.5-Instruct series evaluated on GSM8K in Table~\ref{tab:qwen}. As the model scale increases, there is less performance degradation at higher compression ratios, indicating that larger LLMs are better at identifying shortcuts between critical reasoning tokens, enabling more efficient CoT generation.

\input{tab/qwen}