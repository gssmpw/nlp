\section{Introduction}

Chain-of-Thought (CoT) prompting~\cite{Nye:2021, cot, Kojima:2022cotzero} has emerged as a cornerstone strategy for enhancing Large Language Models (LLMs) in complex reasoning tasks. By eliciting step-by-step inference, CoT enables LLMs to decompose intricate problems into manageable subtasks, thereby improving their problem-solving performance~\cite{Yao:2023tot, Wang:2023self-consistency, Zhou:2023least, Shinn:2023Reflexion}. Recent advancements, such as OpenAI's o1~\cite{o1} and DeepSeek-R1~\cite{deepseekr1}, further demonstrate that scaling up CoT lengths from hundreds to thousands of reasoning steps could continuously improve LLM reasoning. These breakthroughs have underscored CoTâ€™s potential to advance LLM capabilities, expanding the boundaries of AI-driven problem-solving.

\begin{figure}[t]
\centering
    \includegraphics[width=0.95\columnwidth]{fig/intro.pdf}
    \caption{In contrast to vanilla CoT that generates all reasoning tokens sequentially, \method enables LLMs to \textit{skip} tokens with less semantic importance (\textit{e.g.,} \includegraphics[width=7pt]{fig/token.pdf}~) and learn shortcuts between critical reasoning tokens, facilitating controllable CoT compression.}
    \label{fig:intro}
\end{figure}

Despite its effectiveness, the increased length of CoT sequences introduces substantial computational overhead. Due to the autoregressive nature of LLM decoding, longer CoT outputs lead to proportional increases in both inference latency and memory footprints of key-value cache. Additionally, the quadratic computational cost of attention layers further exacerbates this burden. These issues become particularly pronounced when CoT sequences extend into thousands of reasoning steps, resulting in significant computational costs and prolonged response times. While prior research has explored methods for selectively skipping reasoning steps~\cite{Ding:2024cotshortcut, liu2024skipstep}, recent findings~\cite{jin:2024cotlength, Merrill:2024cotlength} suggest that such reductions may conflict with test-time scaling~\cite{o1-blog, snell2025scaling}, ultimately impairing LLM reasoning performance. Therefore, striking an optimal balance between CoT efficiency and reasoning accuracy remains a critical open challenge.

In this work, we delve into CoT efficiency and seek the answer to an important question: \textit{``Does every token in the CoT output contribute equally to deriving the answer?''} We empirically analyze the semantic importance of tokens within CoT outputs and reveal that their contributions to the reasoning performance vary, as depicted in Figure 2. Building on this insight, we introduce \method, a simple yet effective approach that enables LLMs to \textit{skip} less important tokens within CoT sequences and learn shortcuts between critical reasoning tokens, thereby allowing for controllable CoT compression with adjustable ratios. Specifically, as shown in Figure~\ref{fig:intro}, \method constructs compressed CoT training data with various compression ratios, by pruning unimportance tokens from original LLM CoT trajectories. Then, it conducts a general supervised fine-tuning process on target LLMs with this training data, facilitating LLMs to automatically trim redundant tokens during reasoning.

We conduct extensive experiments across various models, including LLaMA-3.1-8B-Instruct and the Qwen2.5-Instruct series, using two widely recognized math reasoning benchmarks: GSM8K and MATH-500. The results validate the effectiveness of \method in compressing CoT outputs while maintaining robust reasoning performance. Notably, Qwen2.5-14B-Instruct exhibits almost \textbf{NO} performance drop (less than $0.4\%$) with a $\bm{40\%}$ reduction in token usage on GSM8K. On the challenging MATH-500 dataset, LLaMA-3.1-8B-Instruct effectively reduces CoT token usage by $\bm{30}\%$ with a performance decline of less than $4\%$, resulting in a $\bm{1.4}\times$ inference speedup. Further analysis underscores the coherence of \method in specified compression ratios and its potential scalability with stronger compression techniques.

\method is distinguished by its low training cost. For Qwen2.5-14B-Instruct, \method fine-tunes only 0.2\% of the model's parameters using LoRA. The size of the compressed CoT training data is no larger than that of the original training set, with 7,473 examples in GSM8K and 7,500 in MATH. The training is completed in approximately 2 hours for the 7B model and 2.5 hours for the 14B model on two 3090 GPUs. These characteristics make \method an efficient and reproducible approach, suitable for use in efficient and cost-effective LLM deployment.

To sum up, our key contributions are:
\begin{enumerate}
    \item To the best of our knowledge, this work is the \textit{first} to investigate the potential of enhancing CoT efficiency through \textit{token skipping}, inspired by the varying semantic importance of tokens in CoT trajectories of LLMs.
    \item We introduce \method, a simple yet effective approach that enables LLMs to skip redundant tokens within CoTs and learn shortcuts between critical tokens, facilitating CoT compression with adjustable ratios.
    \item Our experiments validate the effectiveness of \method. When applied to Qwen2.5-14B-Instruct, \method reduces reasoning tokens by $40\%$ (from 313 to 181) on GSM8K, with less than a $0.4\%$ performance drop.
\end{enumerate}
