\section{Experiments}
\subsection{Experimental Setup}
\label{sec:exp_setup}

\paragraph{Models and Datasets} 
We primarily evaluate our method using LLaMA-3.1-8B-Instruct~\cite{dubey:2024llama3} and Qwen2.5-Instruct series~\cite{Qwen2}. The evaluation leverages two widely-used math reasoning benchmarks: GSM8K~\citep{Cobbe:2021gsm8k} and MATH~\cite{math}. For training, we use the respective training sets from both datasets. Regarding the MATH dataset, due to the computation cost, we assess our method on a subset, MATH-500, which is identical to the test set used in \citet{Lightman:2024verify}. The subset comprises 500 representative problems, and we find that its evaluation yields results comparable to those from the full dataset.

\paragraph{Implementation Details} 
We utilize LLMLingua-2~\cite{pan:2024llmlingua2} as the token importance metric to generate our compressed CoT training data. The compression ratio $\gamma$ is randomly selected from $\{0.5, 0.6, 0.7, 0.8, 0.9, 1.0\}$ for each training sample. We adopt LoRA~\cite{lora}, an efficient and reproducible approach that has been widely verified as effective in LLM fine-tuning, to train our models. The rank $r$ is set to 8, and the scaling parameter $\alpha$ is set to 16. \method is characterized by its low training cost, with training taking $\sim$2 hours for the 7B model and $\sim$2.5 hours for the 14B model on 3090 GPUs. During inference, the maximum number of tokens \texttt{max\_len} is set to 512 for GSM8K and 1024 for MATH\footnote{Since many samples reach the maximum length when testing \method on MATH-500, we adjust its length budget to \texttt{max\_len}$\times\gamma$, with no adjustment for GSM8K.}. All experiments are conducted using Pytorch 2.1.0 on 2$\times$NVIDIA GeForce RTX 3090 GPU (24GB) with CUDA 12.1, and an Intel(R) Xeon(R) Platinum 8370C CPU with 32 cores. We include more implementation details in Appendix~\ref{appendix:training_details}.

\paragraph{Baselines} 
In our main experiments, we compare \method to two commonly used length control baselines: \textbf{1) Prompt-based Reduction.} In this approach, we instruct the LLM to reduce a fixed proportion of output tokens in the CoT process. Specifically, we append a prompt such as ``\textit{Please reduce 50\% of the words in your Chain-of-Thought process.}'' to the input instruction. \textbf{2) Truncation.} This method involves brute-force length truncation, where the maximum number of output tokens is restricted, compressing the CoT output to a fixed ratio. These baselines are referred to as \texttt{Prompt} and \texttt{Truncation} in Table~\ref{tab:main}, respectively.

\begin{figure}[t]
\centering
    \includegraphics[width=0.95\columnwidth]{fig/scaling.pdf}
    \caption{Compression performance of \method on Qwen2.5-Instruct models. Qwen2.5-14B-Instruct shows almost \textbf{no} performance drop with $\bm{40\%}$ token trimming.}
    \label{fig:qwen-scaling}
\end{figure}

\paragraph{Evaluation Metrics} 
We evaluate \method using three widely used metrics: accuracy, the number of CoT tokens, and inference latency per sample. Model performance is assessed using scripts from \texttt{DeepSeek-Math}\footnote{\url{https://github.com/deepseek-ai/DeepSeek-Math}}. Greedy decoding is employed to generate the outputs from the target LLM. Inference latency is measured on a single NVIDIA 3090 GPU with a batch size of 1. In addition to these metrics, we report the actual compression ratio of the CoTs to assess whether the compression aligns with the specified ratio.

\input{tab/main}

\subsection{Main Results}
\label{sec:main-exp}
The performance of \method on GSM8K using the Qwen2.5-Instruct series\footnote{For detailed results, please refer to Appendix~\ref{appendix:qwen_detail}.} is illustrated in Figure~\ref{fig:qwen-scaling}. As the model scale increases, there is less performance degradation at higher compression ratios, indicating that larger LLMs are better at identifying shortcuts between critical reasoning tokens, enabling more efficient CoT generation. Notably, Qwen2.5-14B-Instruct exhibits almost \textbf{NO} performance drop (less than $0.4\%$) with $\bm{40\%}$ token trimming. Even at a compression ratio of 0.5, the model maintains strong reasoning capabilities, with only $2\%$ performance degradation. These results highlight the substantial potential of \method to reduce CoT token usage and accelerate reasoning in large-scale LLMs. Due to computational constraints, experiments with larger models are not conducted and are left for future exploration.

We further compare \method with two widely used length control baselines --- \texttt{prompt}-based reduction and \texttt{truncation}. The experimental results are presented in Table~\ref{tab:main}. As shown, \texttt{prompt}-based reduction fails to achieve the specified compression ratio, with the actual ratio exceeding 0.89 even when the target is set to 0.5. While \texttt{truncation} adheres to the specified ratio, it results in significant degradation in reasoning performance. Specifically, at a compression ratio of 0.5, \texttt{truncation} causes a $79\%$ accuracy drop on GSM8K and a $21\%$ drop on MATH-500. In contrast, \method ensures adherence to the specified compression ratio (see Figure~\ref{fig:allratio}) while preserving strong reasoning capabilities. Notably, \method achieves an actual compression ratio of \textbf{0.53} on GSM8K with only a $10\%$ performance drop, resulting in a $\bm{1.8}\times$ speedup in average latency. On the challenging MATH-500 dataset, \method effectively reduces CoT token usage by $\bm{30}\%$ with a performance drop of less than $4\%$. These results validate the effectiveness of \method.