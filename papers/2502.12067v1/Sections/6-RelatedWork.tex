\section{Related Work}

\paragraph{Efficient CoT} While Chain-of-Thought (CoT) enhances task performance by simulating human-like reasoning patterns, its reasoning steps introduce significant computational overhead. As a result, researchers have sought methods to reduce this overhead while retaining the benefits of CoT. One intuitive approach is to simplify, skip~\cite{marconato2024not, Ding:2024cotshortcut, liu2024skipstep}, or generate thinking steps in parallel~\cite{ning2023skeleton} to improve efficiency. Another strategy involves compressing reasoning steps into continuous latent representations~\cite{goyal2024think, deng2024explicit, hao2024training, cheng2024compressed}, allowing LLMs to reason without explicitly generating discrete word tokens. To minimize the generation of redundant natural language information that has minimal impact on reasoning, \citet{hu2023chain} implements structured syntax and symbols, while \citet{han2024token} guides token consumption through dynamic token budget estimation. Similarly, \citet{kang2024c3ot} prompts larger LLMs (i.e., \texttt{GPT-4}) to directly compress CoT, then fine-tunes LLMs to reason using these compressed CoTs. In contrast, this work focuses on pruning CoT tokens based on their semantic importance. Additionally, \method leverages a small LM for token pruning, significantly reducing computational overhead.

\paragraph{Prompt Compression} As LLMs advance in their zero-shot capabilities, the growing demand for complex instructions and long-context prompts has led to substantial computational and memory challenges in processing lengthy inputs. To address this bottleneck, researchers have explored various prompt compression techniques. One intuitive approach involves using a lightweight LM to generate more concise, semantically similar prompts~\cite{chuang-etal-2024-learning}. However, given that explicit natural language representations often contain redundant information, some researchers have turned to implicit continuous tokens to represent task prompts~\cite{wingate-etal-2022-prompt, mu2024learning} and long-context inputs~\cite{chevalier-etal-2023-adapting, ge2024incontext, mohtashami2023randomaccess}. Other approaches focus on directly compressing input prompts by filtering and retaining high-informative tokens~\cite{li:2023selective, jiang2023:llmlingua, pan:2024llmlingua2}. For instance, Selective Context uses the perplexity of LLMs to measure token importance and removes tokens deemed less important. LLMLingua-2~\cite{pan:2024llmlingua2} introduces a small bidirectional language model for token importance measurement and trains this LM with \texttt{GPT-4} compression data, which serves as the token importance metric in this work.