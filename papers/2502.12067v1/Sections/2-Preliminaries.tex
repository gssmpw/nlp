\section{Background and Preliminaries}
\label{sec:preliminaries}

In this section, we discuss the relevant research background and present preliminary studies on token efficiency in CoT sequences, exploring its impact on the reasoning performance of LLMs.

\subsection{Token Importance}
\label{sec:token-importance}

We first investigate a critical research question to CoT efficiency: \textit{``Does every token in the CoT output contribute equally to deriving the answer?''} In other words, we would like to know if there is any token redundancy in CoT sequences that could be eliminated to improve CoT efficiency.

Token redundancy has been recognized as a longstanding and fundamental issue in LLM efficiency~\cite{hou:2022tokendropbert, zhang2023h2o, lin2024criticaltokenpretrain, Chen:2024FastV}. Recently, it has garnered intensive research attention in prompt compression~\cite{li:2023selective, jiang2023:llmlingua, pan:2024llmlingua2}, which focuses on removing redundant tokens from input prompt to reduce API token usage. To address this issue, Selective Context~\cite{li:2023selective} proposed to measure the importance of tokens in a piece of text based on the semantic confidence of LLMs:
\begin{equation}
I_1\left(x_i\right)=-\log P\left(x_i \mid \bm{x}_{<{i}}; \bm{\theta}_{\M_L}\right),
\label{eq:selectivecontext}
\end{equation}
where $\boldsymbol{x}=\left\{x_i\right\}_{i=1}^{n}$ is the given text, $x_i$ denotes a token, and $\M_L$ denotes the LLM used to compute the confidence of each token. Intuitively, such measurement could be seamlessly applied to CoT tokens generated by LLMs. We show an example of this measurement in Figure~\ref{fig:token-importance}.

\input{inputs/token-importance}

Despite its simplicity, LLMLingua-2~\cite{pan:2024llmlingua2} argued that there exist two major limitations in the aforementioned measurement that hinder the compression performance. Firstly, as shown in Figure~\ref{fig:token-importance}, the intrinsic nature of LLM perplexity leads to lower importance measures (i.e., higher confidence) for tokens at the end of the sentence. Such position dependency impacts the factual importance measurement of each token. Furthermore, the unidirectional attention mechanism in causal LMs may fail to capture all essential information needed for token importance within the text. 

To tackle these limitations, LLMLingua-2 introduced utilizing a bidirectional BERT-like LM~\cite{bert} for token importance measurement. It utilizes GPT-4~\cite{gpt-4} to label each token as ``\textit{important}'' or not and trains the bidirectional LM with a token
classification objective. The token importance is measured by the predicted probability of each token:
\begin{equation}
I_2\left(x_i\right)= P\left(x_i \mid \bm{x}_{\le n}; \bm{\theta}_{\M_B}\right),
\label{eq:llmlingua2}
\end{equation}
where $\M_B$ denotes the bidirectional LM. 

In this study, we apply LLMLingua-2 as the token importance measurement to LLM CoT outputs. Similar to plain text, we observe that the semantic importance of tokens within CoT outputs varies, as shown in Figure~\ref{fig:token-importance}. For instance, mathematical equations tend to have a greater contribution to the final answer, consistent with recent research~\cite{Ma:2024mathmatters}. In contrast, semantic connectors such as ``\textit{so}'' and ``\textit{since}'' generally contribute less. These findings highlight the token redundancy in LLM CoT outputs and the substantial potential to enhance CoT efficiency by trimming this redundancy.

\input{inputs/recovery}

\subsection{CoT Recovery}
\label{sec:cot-recovery}
We further explore the following research question: \textit{``Are LLMs capable of restoring the CoT process from compressed outputs?''} The answer is yes. As shown in Figure~\ref{fig:recovery} and detailed in Appendix~\ref{appendix:recovery}, examples restored from compressed CoTs using LLaMA-3.1-8B-Instruct demonstrate that LLMs could effectively comprehend the semantic information encoded in the compressed CoT and restore the CoT process. This capability ensures that the interpretability of compressed CoTs is maintained. Additionally, when required by users, the complete CoT process can be recovered and presented.

In summary, the empirical analysis above underscores the potential of trimming redundant tokens to enhance CoT efficiency, as well as the ability of LLMs to restore CoT from compressed outputs. However, enabling LLMs to autonomously skip redundant CoT tokens and identify shortcuts between critical reasoning tokens presents a non-trivial challenge. To the best of our knowledge, this work is the \textit{first} to explore CoT compression through \textit{token skipping}. In the following sections, we present our proposed methodology in detail.

