\section{Introduction}
\label{introduction}

\looseness=-1
We introduce \ours{} (``echo'' in Japanese), a system for streaming and expressive speech-to-speech (S2ST) and speech-to-text (S2TT) translation. 
Most work in speech translation has focused on the offline (or \textit{consecutive}) setting where the model has access to the full source utterance before it translates, as it provides useful context while fitting many use cases such as offline video dubbing. A more challenging setting is that of \textit{simultaneous} translation, where translated speech is produced on-the-fly. This task, typically performed by human interpreters, requires a real-time decision-making process to evaluate whether enough context has been accumulated to reliably translate another chunk of speech. When cast as a machine learning problem, this endeavor presents additional challenges such as the lack of speech data aligned at a chunk-level.

\looseness=-1
\ours is a decoder-only model which synchronously receives source speech and generates translated speech by leveraging a multistream architecture. In this context, nested global and local Transformers~\cite{attentionvaswani} jointly model two audio streams by predicting a hierarchy of text and audio tokens for each of them. At inference time, temperature sampling combined with a causal audio codec allows for streaming inputs and outputs. While this architecture was originally introduced by~\citet{moshi} for full-duplex spoken dialogue, we show how it provides a simple and convenient architecture for simultaneous speech translation. To train \ours, we generate synthetic parallel data by translating and resynthesizing the transcript of single-language audio. While this provides pairs of inputs and outputs aligned at the sequence level, this does not allow learning fine-grained alignments. We thus introduce ``contextual alignment'', a simple method based on the perplexity of an off-the-shelf machine translation system~\cite{madlad} to derive word-level alignments. By then introducing proper silences into target speech, we can train \ours to adapt its flow in real-time, without the need for complex inference policies. Moreover, observing that training data varies widely in speaker similarity, we propose to label training examples with categories of speaker similarity, which avoids filtering the training data while allowing to favor high speaker similarity at inference with classifier-free guidance.

\looseness=-1
In a French-English translation task, \ours outperforms previous work in translation quality, speaker similarity and naturalness. We also show how the simplicity of our inference process allows for translating hundreds of sequences in real-time on GPU, and how a distilled model can run in real-time on a smartphone. Human evaluations demonstrate that \ours is the first model to provide an experience of interpretation close to human professionals. We will release our code and models, and a high quality 900 hours synthetic dataset.