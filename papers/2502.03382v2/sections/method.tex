\section{Method}
\label{method}

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{figures/text_delays_tokens_v2.drawio.pdf}
    \caption{\textbf{Joint sequence modeling with contextual alignment.} From the source stream, \ours predicts its Inner Monologue text
    stream, and audio tokens.
    Its output is aligned for causality, as depicted in Figure~\ref{fig:contextual-delays-waveform}. Figure adapted from ~\citet{moshi}.
    }
    \label{fig:joint-modeling}
\end{figure}
We consider an utterance in a source language represented as
a monophonic waveform $X \in\mathbb{R}^{f_s \cdot d}$, sampled at a frame rate $f_s = 24\,\mathrm{kHz}$, of duration $d$.
Similarly, its translation is given in a target language,
denoted $Y \in \mathbb{R}^{f_s \cdot d}$. We assume $X$ is padded to ensure both have the same duration. Our objective is to model $\proba{Y | X}$.
We further add the constraint that the modeling of $Y$ knowing $X$ 
should be causal and of minimal delay with respect
to the source utterance, e.g. the same constraints that are imposed on a human interpreter in the context of live translation. 
To learn this constraint \textit{via} supervised learning, $Y$ must itself be built to respect this causality constraint.
We first assume that $Y$ respects this constraint, and we present how to model
its distribution. Then, we introduce an information theory criterion 
to verify whether $Y$ is causal with respect to $X$, and to adapt
a non-causal interpretation into a causal one.

\subsection{Modeling}

We build on the framework introduced by \citet{moshi} for the joint modeling of
multiple sequences of discrete tokens, obtained from a neural audio codec.


\subsubsection{Neural audio codec}

We use the pre-trained causal and streaming Mimi codec~\citep{moshi} to encode $X$ and $Y$ into low framerate sequences of discrete tokens.
Mimi consists of an encoder and decoder from and to the waveform domain, and of an information bottleneck using Residual Vector Quantization (RVQ)~\cite{soundstream}. The encoder transforms an input waveform of duration $d$ into a latent vector $U \in \mathbb{R}^{C \times f_r \cdot d}$,
with $C$ the dimension of the latent space, and $f_r = 12.5\,\mathrm{Hz}$ the frame rate. 
$U$ is then projected to its nearest neighbor in a \emph{codebook} table with $N_A$ entries. The residual of the projection is further projected into a second
table with the same cardinality, and so forth until $Q$ projections have been performed. The last residual is discarded, and the decoder is trained to reconstruct the input waveform from the sum of the projected tensors.
The codebooks are trained through exponential moving average, along with a commitment loss~\citep{vqvae}. The rest of the model is trained only through an adversarial loss with feature matching~\citep{moshi}.

For language modeling, we are not interested in the quantized latent vector and its residuals, but in the discrete
indices of the entry in the codebooks it is projected to. We denote those $(A_{t, q}) \in \{1, \ldots, N_A\}^{f_r \cdot d \times Q}$.
For Mimi we have $f_r = 12.5 \,\text{Hz}$ and $Q$ varies up to 32, but we use at most 16. 
Following~\citet{zhang2024speechtokenizer,moshi}, the output
of the first quantization level is trained to replicate semantic information
obtained from a WavLM self-supervised audio model~\citep{wavlm}.
Following conventions of~\citet{audiolm}, we refer to $A_{t, 1}$ as the \emph{semantic} tokens, and $A_{t, q \geq 2}$ as the \emph{acoustic} tokens. The acoustic tokens are arranged in a coarse to fine manner, the first ones have the most importance, and the latest model fine details of the audio and ensuring a smooth perception.

\subsubsection{Joint modeling of discrete audio tokens}

The discrete tokens for audio streams cannot easily be summarized into a single discrete sequence with reasonable cardinality and framerate~\citep{musicgen}. Following \citet{uniaudio,moshi}, we leverage a RQ-Transformer~\citep{rq-transformer} 
to model $(A_{t, q})$ both over the time $t$ and quantizer $q$ axes.
It consists in a large \emph{Temporal} Transformer~\citep{attentionvaswani}, operating at the same framerate $f_r$ as the codec, and being fed all the tokens generated so far, e.g.
for all $t \leq f_r \cdot d$, 
 \begin{equation}
 \label{eq:temp_transformer}
Z_t = \mathrm{Temp}(A_0, \ldots, A_{t - 1}) \in \mathbb{R}^{D}.
\end{equation}
$A_0$ is defined as a deterministic token indicating the start of the generation.
Then, a smaller scale \emph{Depth} Transformer models auto-regressively
the tokens $A_{t, 1}, \ldots, A_{t, Q}$ over the quantizer axis, e.g. for all $t \leq f_r \cdot d$ and $q \leq Q$,
\begin{equation}
\label{eq:dep_transformer}
    l_{t, q} = \mathrm{Dep}(Z_t, A_{t, 0}, \ldots, A_{t, q - 1}) \in \mathbb{R}^{N_a},
\end{equation}
with $A_{t, 0}$ also a special token, and with the goal of having,
\begin{equation*}
    \softmax(l_{t, q}) \approx \proba{A_{t, q} | A_{0}, \ldots, A_{t-1}, A_{t, 0}, \ldots A_{t, q - 1}}
\end{equation*}

Following \citep{musicgen,moshi}, we further introduce an acoustic delay of 2 time steps, meaning that we model $(\tau(A)_{t, q})$ instead of $A$,
\begin{equation}
\begin{cases}
\begin{array}{lll}
    \tau(A)_{t, 1} &= A_{t, 1} &\quad\forall\, t \\
    \tau(A)_{t, q} &= A_{t - 2, q} &\quad\forall\, t \geq 3, \forall\, q \geq 2\\
    \tau(A)_{t, q} &= 0 &\quad\forall\, t < 3, \forall\, q \geq 2, \\
\end{array}
\end{cases}
\end{equation}
with 0 being a special token. The delay is removed before decoding audio with
the codec.

\subsubsection{Translation as multistream modeling}
\label{sec:multistream}

We have presented how the RQ-Transformer given by eq. \eqref{eq:temp_transformer} and \eqref{eq:dep_transformer} allows
for jointly modeling multiple discrete streams of tokens.
We adapt this framework for the task of joint speech-to-speech
and speech-to-text simultaneous translation.
We concatenate the audio tokens
$A^Y$ obtained from the target interpretation $Y$, with
the tokens $A^X$ from the source utterance $X$ along the $q$-axis, e.g. 
\begin{equation}
    \bar{A} = \mathrm{concat_q}\left[\tau(A^Y), \tau(A^X)\right].
\end{equation}
We observe a benefit from modeling the tokens $A^X$ at train time,
although at inference time, predictions for those tokens are skipped
and actual tokens for the input are used instead.

\citet{moshi} showed generating an Inner Monologue, i.e. padded text tokens aligned with the content of the generated audio, is beneficial to the quality
and stability of the generated audio. This is similar to multi-task learning where the translation is predicted both in the audio
and text domain. \ours thus also predicts a text stream corresponding
to the transcription of the output $Y$, with sufficient padding between words to keep them aligned with the audio. Note that unlike previous multi-task
translation work, \ours makes active use of this capability at inference time.
We denote $W_t$ the text stream, with cardinality $N_W$ and the same frame rate $f_r$
as the audio streams.

\subsubsection{Architectural details}
\label{sec:arch_details}

We provide architectural hyper-parameters in Section~\ref{sec:arch_hyperparams}.
At time-step $t$, the tokens from the step $t-1$, e.g. $\tau(A^X)_{t-1}$,
$\tau(A^Y)_{t-1}$, and $W_{t-1}$, are fed into dedicated embedding tables, whose contributions are summed.
For the first time step $t = 1$, a BOS token is used instead.
We then use standard Transformer layers~\citep{attentionvaswani}, with gated SiLU activation~\citep{shazeer2020glu,hendrycks2016gaussian}.
A linear layer maps its output $Z_t$ to logits for the text token $W_t$.
The Depth Transformer then operates for $2 \cdot Q$ steps:
the first half to estimate the logits for the output stream,
and the next half for the input stream.
Each depth step $q$ takes as input $Z_t$ summed with
a learnt embedding of the previous audio token $\bar{A}_{t, q - 1}$, or $W_t$ for $q = 1$.


\subsection{Alignment and synthetic interpretation data}
\label{sec:alignment}


\begin{figure}[t]
    \centering
    \includegraphics[width=1.0\columnwidth]{figures/contextual_delay.drawio.pdf}
    \caption{\textbf{Contextual alignment.}
    We compute the log-likelihood of the word ``into'' with a pre-trained text translation model, for various input truncations. Once the matching source word "en" appears, we observe a large increase in log-likelihood,  see eq.~\eqref{eq:ctx_align}.}
    \label{fig:contextual-delays-tokens}
\end{figure}

\looseness=-1
We have assumed pairs $(X, Y)$
that respect the constraint of simultaneous interpretation.
We now introduce an unsupervised criterion to estimate and enforce causality
dependencies between the source and target utterances.


\subsubsection{Text domain alignments}
\label{sec:text-domain-align}
Let us first express formally those constrained in the text domain.
Let us take $S = (S_1, \ldots, S_n)$ the sequence of words
in the utterance $X$, and $T = (T_1, \ldots, T_m)$
that in $Y$.

\paragraph{Ideal alignment.}
We seek to define an ideal alignment $(a^{\text{ideal}}_{j}) \in \{1,\ldots, n\}^m$, where
$a^{\text{ideal}}_{j}$ indicates the index of the word in $S$ that the $j$-th word in $T$
should wait for to minimize the uncertainty on $T_j$. 
Any alignment strictly less conservative than $a^{\text{ideal}}$
would risk the model hallucinating at inference if trained on.
Any alignment strictly more conservative would still be causal,
but introduce more latency.


\paragraph{Contextual alignment.}
We introduce a criterion to estimate $a^{\text{ideal}}$.
Let's denote the conditional log-likelihood
\begin{equation}
    \label{eq:delay_proba_gt}
    \log(p_{j, i}) = \log\left(\proba{T_j | S_1, \ldots S_{i}, T_{1}, \ldots, T_{j-1} }\right),
\end{equation}
we expect $\log p_{j, i}$ to increase with $i$,
as more context is beneficial.
We conjecture that $\delta_{j,i} = \log(p_{j, i}) - \log(p_{j, i - 1})$ is maximal for $i = a_{j}$.
We compute an estimate $\log(\hat{p}_{j, i})$ of 
$\log(p_{j, i})$ with an off-the-shelf 
text translation language model MADLAD-3B~\citep{madlad}, by feeding it
truncated input up to the $i$-th word, which we use to define a \emph{contextual} alignment, illustrated in Figure~\ref{fig:contextual-delays-tokens},
\begin{equation}
\label{eq:ctx_align}
    a^{\text{ctx}}_j = \argmax_{i \leq n}  \left[\log(\hat{p}_{j, i}) - \log(\hat{p}_{j, i - 1}) \right].
\end{equation}
Examples of alignments are given in the Appendix, Figure~\ref{fig:examples-contextual-alignments}.

\subsubsection{Audio domain alignments}
\label{sec:audio-domain-align}

Given $(X, Y)$, we transcribe both with timestamps with 
a Whisper model~\citep{whisper,lintoai2023whispertimestamped}
and apply eq. \eqref{eq:ctx_align}.
The pair $(X, Y)$ respects
the alignment $(a^{\textrm{ctx}}_j)$ if the timestamp of the $j$-th word in $Y$
comes after the timestamp of the $a_j$-th word in $X$. To reduce
the impact of errors, we require $Y$
to lag by at least 2 seconds compared to the contextual alignment, and eliminate spikes higher than 25\% of the average delay over
a window of 5 words.

\paragraph{Silence insertion.}
If $Y$ doesn't respect the alignment,
one can simply transform it by inserting sufficient silences
before a word, as illustrated in Figure~\ref{fig:contextual-delays-waveform}.
This comes, however, with two limitations: \\(i) silence insertion
can lead to hard cuts when the timestamps are inaccurate
or no pause exists between words;\\ (ii) the corrected $Y$ might be arbitrarily
late on the ideal alignment, e.g. if the speech rate is slower in $Y$ than in $X$.\\
We apply this method during the speech translation training.

\paragraph{Alignment-aware TTS.}
We obtain more natural aligned data by (re)-synthesising $Y$
with a TTS model able to follow hard and soft constraints on word locations,
along with accurate speaker conditioning.
For existing datasets, this can have the added benefit of improving the word error rate and the speaker similarity, as illustrated in Section~\ref{sec:voice_transfer}.
Following~\citep{moshi}, Appendix C, we train a TTS with both audio and a synced text stream as output, along with voice conditioning. The text stream is constrained to match
exactly text to generate, with the model having only the freedom to insert padding tokens. The audio output is late on the text, so that its content is
conditioned by it, both for content and timestamps. 
If the TTS is early on the alignment $a^{\textrm{ctx}}$, 
padding tokens are forced to delay the next word.
When the TTS is lagging on its target, a penalty
is added on the logits of the padding token. The penalty scales from 0 to -2 as
the lag increases from 1 to 2 seconds. This increases smoothly the rate of 
speech to catch up with the source audio.
We perform 6 to 8 generations per input, and select the best one based on
word error rate first, and speaker similarity second.
We apply this only for the fine-tuning speech translation dataset.

\begin{figure}[t]
        \centering
        \includegraphics[width=0.9\columnwidth]{figures/speaker_similarity_cvss_vs_resynth.pdf}
    \caption{Speaker similarity between source and target speech in CVSS-T training data, before and after resynthesis.
    }
    \label{fig:speaker-sim-histogram}
\end{figure}

\subsection{Voice Transfer}
\label{sec:voice_transfer}
\paragraph{Improving voice transfer data.}
\looseness=-1
Training speech translation models with voice transfer typically amounts to supervised training on synthetic paired sequences of the same speaker.
In particular, CVSS-T~\cite{cvss} is the standard training set for S2ST with voice transfer and provides such artificial targets. However, Figure~\ref{fig:speaker-sim-histogram} shows that the average speaker similarity---as measured by the cosine similarity between speaker embeddings of source and target--- on this dataset is very low with an average of $0.23$. As a calibration, state-of-the-art cross-lingual voice transfer lies around $0.40$~\cite{audiopalm}. We thus also regenerate CVSS-T with our alignment-aware TTS, as it allows for voice transfer. As shown in Figure~\ref{fig:speaker-sim-histogram}, the resynthesized CVSS-T displays a higher similarity, with an average of $0.47$. Yet, our training mixture which combines synthetic data and resynthesized CVSS-T still covers a wide range, with a significant mass below $0.40$.

\paragraph{Conditional training.} Filtering training data to only keep pairs of examples with a high similarity would improve voice transfer, but the resulting reduction in training data would likely affect translation quality (e.g. keeping only samples with a speaker similarity above $0.40$ would remove ~45\% of training data). We rather rely on conditional training~\cite{socher_conditioned_2019, goal_conditioned_bowman} to inform the generative model of how reliable each training example is in terms of voice transfer. We label each training sample with a discrete ``voice transfer score'' in $\{\texttt{very\_bad},~\texttt{bad},~\texttt{neutral},~\texttt{good},~\texttt{very\_good}\}$ based on quantiles of speaker similarity, each label being associated to a learnable embedding added to the model's inputs at every timestep. Importantly, the quantiles are computed \emph{before} combining the synthetic data and CVSS-T to guarantee that the model does not associate a specific label to a specific dataset rather than the actual speaker similarity. At inference time, we always pass the $~\texttt{very\_good}$ label.

\looseness=-1
\paragraph{Classifier-free guidance.} Following~\citet{audiogen} we can increase the impact of the conditioning by using classifier-free guidance. We compute logits both with conditionings $~\texttt{very\_good}$ and $~\texttt{very\_bad}$ and sample from:
\begin{equation}
    \gamma l_{t, q}^{~\texttt{very\_good}} + (1 - \gamma) l_{t, q}^{~\texttt{very\_bad}}, 
\end{equation}
which is compatible with real-time inference by producing both sets of logits with a batch size of 2. Section~\ref{sec:results} shows that it significantly improves voice transfer.
