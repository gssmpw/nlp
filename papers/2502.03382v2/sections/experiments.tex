\section{Experiments}
\label{sec:experiments}
\subsection{Architectural hyper-parameters}
\label{sec:arch_hyperparams}

\looseness=-1
\ours consists of a Temporal Transformer
with a latent dimension of 2560 (7040 for the SiLU gating), 24 layers, 20 heads and local attention over 500 tokens, i.e. 2.2B parameters and a 40s context. The Depth Transformer initially follows
~\citet{moshi}, i.e. 6 layers per codebook,
a latent dimension of 1024 (2816 for the gating), and 64 dimensions per head. It models $Q=16$ audio codebooks
for the output stream, and the same for the input stream (only at training). To reduce the footprint of the Depth Transformer we distill it post-training
into a smaller one, with 4 layers per codebook, weight sharing for codebooks 9 to 16, low-rank embedding tables of dimension 128, and a dimension of 2048 for the gating. This reduces its size from 1.1B parameters to 449M parameters, for a total of 2.7B parameters.
We also train \ours-M as a distilled version of \ours, a 1.7B variant capable of running real-time on device, with a Temporal Transformer with
a latent space of dimension 2048, and 16 layers, and only 8 codebooks levels per stream.

\subsection{Training protocol}

We train a French-English speech translation system through the following steps, each with a cosine learning rate schedule and AdamW~\citep{adamw}, with a weight decay of 0.1, and momentum parameters of (0.9, 0.95).

\textbf{Text pretraining.}
We first pretrain the \textit{Temporal} Transformer from scratch on multilingual text-only data using next token prediction, for 600K steps, with a batch of 1,024 sequences of length 4,096. We use a cosine learning rate schedule, with 2K warmup steps and a maximum value of $4.8\cdot10^{-4}$. Our training dataset is made of filtered web pages from Common Crawl, as well as curated sources such as Wikipedia, StackExchange or scientific articles and it contains 12.5\% of multilingual documents.

\textbf{Audio pretraining.}
Starting from the pretrained text model, we perform an audio pretraining on non-parallel French and English data with a single stream as done by \citet{moshi}. We train for 1,450K steps with a batch size of 144 and a learning rate of $2\cdot10^{-4}$. Then we duplicate the weights of the Depth Transformer for multistream modeling.

\textbf{Speech translation training.}
We build a French-English speech translation dataset of approximately 40K hours in each language. Starting from a collection of expressive audio content in French, we extract roughly 2.3M single speaker utterances each with a duration around 60 seconds. We transcribe these segments with Whisper \citep{whisper}, using the large-v3 model. We rely on PySBD \citep{pysbd} to segment each transcript into sentences and use MADLAD-3B \citep{madlad} to translate them individually, before joining them back into a translated English transcript. We synthesize each with the TTS
described in Section~\ref{sec:audio-domain-align}, conditioned on the original French speaker identity with a 10s utterance. 
We apply the silence insertion technique described in Section~\ref{sec:audio-domain-align} to obtain simultaneous interpretation pairs. We train for 150K steps with a batch size of 96, a learning rate of $3\cdot10^{-5}$ and compute the loss on both the source and the target streams. We use conditional training on speaker similarity as explained in \ref{sec:voice_transfer} and apply noise augmentation techniques on the source audio. For each training pair, we introduce a special input EOS token on all audio tokens of the source at the first frame after the end of its speech utterance and use another special EOS token on the text tokens stream to indicate the end of the model speech utterance. 

\textbf{Speech translation fine-tuning.}
We use alignment-aware TTS generations introduced in Section~\ref{sec:audio-domain-align} to build a synthetic dataset composed of long-form utterances and an improved version of CVSS-T/train, with natural pauses and high speaker similarity, totaling close to 900 hours. 
We fine-tune for 8K steps with a batch size of 8, a learning rate of $2\cdot10^{-6}$, conditional training on the speaker similarity, special EOS tokens, and apply the loss to both streams.

\textbf{Training of \ours-M.}
It goes through the same text and audio pre-training stages. During 
the speech translation training it is soft distilled from \ours, before
going through the same fine-tuning step (without distillation).

\begin{table}[t]
\caption{\textbf{Comparison with offline baselines.} We also report performance from a closed-source streaming model (*) as it uses the same evaluation protocol.}
\label{tab:offline}
\vskip 0.15in
\begin{center}
\begin{sc}
\resizebox{\linewidth}{!}{%
\begin{tabular}{lc}
\toprule
Model & ASR-BLEU \\
\midrule
Translatotron \cite{jia19_translatotron} & 17.0 \\
Translatotron 2 \cite{jia22-translatotron2} & 26.0 \\
S2UT~\cite{lee-etal-2022-direct} & 22.2 \\
UnitY \cite{inaguma-etal-2023-unity} & 27.8 \\
DASpeech \cite{fang2023daspeech} & 25.0 \\
RNN-Transducer*~\cite{rnn-t-s2st} & 25.4 \\
StreamSpeech (Offline)~\cite{streamspeech} & 28.5 \\
\midrule
\ours & \textbf{30.5} \\

\bottomrule
\end{tabular}}
\end{sc}
\end{center}
\vskip -0.1in
\end{table}


\begin{table*}[t]
\caption{Objective comparison of Hibiki with StreamSpeech~\citep{streamspeech} and Seamless~\citep{seamless}.}
\label{tab:streaming_big_table}
\vskip 0.15in
\begin{center}
\begin{sc}
\resizebox{0.875 \linewidth}{!}{%
\begin{tabular}{lcccccccccc}
\toprule
 & \multicolumn{5}{c}{Short-form (CVSS-C Fr-En test)} & \multicolumn{5}{c}{Long-form (Audio-NTREX)} \\
\cmidrule(lr){2-6}\cmidrule(lr){7-11}
      &                   & ASR               & Speaker           & End & & & ASR & Speaker & End & \\
 Model & BLEU ($\uparrow$) & BLEU ($\uparrow$) & Sim. ($\uparrow$) & Offset ($\downarrow$) & LAAL ($\downarrow$) & BLEU ($\uparrow$) & BLEU ($\uparrow$) & Sim. ($\uparrow$) & Offset ($\downarrow$)  & LAAL ($\downarrow$) \\   
\midrule
StreamSpeech
 & 26.4 & 25.4 & - & 1.6 & 2.8
 & 0.1 & 0.1 & - & N/A & N/A \\
Seamless
 & 37.0 & 33.8 & 0.30 & \textbf{1.4} & \textbf{2.8}
 & 25.4 & 23.9 & 0.43 & \textbf{1.6} & \textbf{4.2} \\
 \midrule
 \ours-M & 37.5  & 33.7 & 0.34 & 2.8 & 3.5
 & 25.9 & 25.0 & 0.39 & 2.3 & 5.5 \\
\ours
 & \textbf{39.2} & \textbf{35.5} & \textbf{0.41} & 2.9 & 3.4
 & \textbf{27.5} & \textbf{26.6} & \textbf{0.48} & 2.7 & 5.0 \\
\bottomrule
\end{tabular}}
\end{sc}
\end{center}
\vskip -0.1in
\end{table*}

\subsection{Evaluation metrics and baselines}
\label{sec:eval-metrics}

\looseness=-1
We first compare \ours to several offline baselines as well as a closed source streaming baseline~\cite{rnn-t-s2st}.
We then perform a comparison between \ours and the two existing methods for simultaneous translation: Seamless~\cite{seamless} and StreamSpeech~\cite{streamspeech} with a chunk size of 2560ms.

\textbf{Translation quality.} We evaluate translation quality by transcribing generated speech and computing a BLEU score~\cite{sacrebleu} with the reference, referred to as ASR-BLEU. When comparing to offline baselines in Table~\ref{tab:offline}, we replicate the setting of \citet{streamspeech} for a fair comparison. However, we observe that this ASR model makes frequent mistakes, so in subsequent experiments we rather transcribe using Whisper medium
~\cite{whisper} and evaluate the BLEU score between ground-truth and hypothesis after normalization.\footnote{\href{https://github.com/openai/whisper/blob/main/whisper/normalizers/english.py}{github.com/openai/whisper/blob/main/whisper/normalizers}} Since Seamless, StreamSpeech and \ours also produce a text translation, we also evaluate their BLEU score using the same text normalization.


\looseness=-1
\textbf{Audio quality and naturalness.} Human raters evaluate the audio quality of generated speech and its naturalness. We evaluate the latter as a proxy for ``realism'': are the flow and prosody natural, are pauses smooth and properly placed or are there abrupt cuts? We compute each score for each model by averaging Mean Opinion Scores between 1 and 5 across 30 samples, each sample being evaluated by 15 raters.

\vspace{-1em}
\looseness=-1
\paragraph{Cross-lingual speaker similarity.} For objective evaluation, we use a standard model for speaker verification\footnote{\href{https://github.com/microsoft/UniSpeech/tree/main/downstreams/speaker\_verification\#pre-trained-models}{github.com/microsoft/UniSpeech} (``WavLM Large'')} based on WavLM~\cite{wavlm} and report the cosine similarity between the embeddings of the source and the generated speech. To mitigate potential biases due to using the same speaker verification model for conditional training (see Section~\ref{sec:voice_transfer}), we also collect human judgments where raters are asked to rate the similarity to the source audio.

\looseness=-1
\textbf{Latency.} A metric for S2ST latency is the End Offset, which is the time (in seconds) between the end of the last word of the source and that of the last word in the output. 
We also measure the Length-Adaptative Average Lagging (LAAL) following the method described by \citet{laal}: it approximates the average time (in seconds) between the pronunciation of a source word and its translation, without requiring word-level alignments. We rely on word-level emission timestamps $(d_i)_{1 \dots n_{\mathrm{gen}}}$ produced by Whisper for $n_{\mathrm{gen}}$ words in the generated speech. We define $\delta = \frac{\Delta_{\mathrm{source}}}{max(n_{\mathrm{gen}}, n_{\mathrm{ref}})}$ where $\Delta_{\mathrm{source}}$ is the duration of the source speech and $n_{\mathrm{ref}}$ the number of words in the reference translation. The LAAL score is then computed as $\frac{1}{n_{\mathrm{max}}}\sum_{i=1}^{n_{\mathrm{max}}} d_i - (i-1)\delta$ where $n_{max} = \mathrm{min}\{i|d_i \geq \Delta_{\mathrm{source}}\}$.


\subsection{Evaluation datasets}

\textbf{Short-form data.}
We evaluate models on the Fr-En task of CVSS~\cite{cvss}. While it is the standard benchmark for S2ST and allows comparisons with previous models, we observe that 99\% of its sequences are shorter than 10 seconds. We thus extend our evaluation to long-forms.

\textbf{Long-form data.}
We collect long-form speech translations by recording bilingual speakers as they read Fr-En translations from the NTREX~\cite{ntrex} text corpus. This speech corpus, that we name Audio-NTREX contains 10 hours of real human speech in each language with 10 different speakers and an average of 50 sec. per utterance.

\textbf{Real interpretation.}
\looseness=-1
To compare with human interpreters, we use 90 real interpretations of the European Parliament from VoxPopuli~\cite{voxpopuli} where translations contain the source speech at a lower volume. For a fair comparison, we also add the lowered source speech to generations of \ours and Seamless (see our external webpage for samples).

\subsection{Inference configuration}

\looseness=-1
We encode audio with the streaming codec and feed the tokens to \ours while decoding the output tokens to obtain streaming translation. 
At the end of the input, we send the EOS token to our model, and keep sampling until it produces its own EOS. Inference parameters are cross validated independently for each dataset using a held-out 8\% of Audio-NTREX and the valid split of CVSS-C. 
The optimal parameters are $\gamma = 3.0$, a temperature of 0.8, top-k of 250 for audio tokens and 50 for text tokens for Audio-NTREX. On CVSS, the same configuration is used except for text tokens that are sampled with a temperature of 0.1. We conjecture that the lower text temperature typically improves translation but can lead to producing an EOS token too early.


\subsection{Results}
\label{sec:results}
\paragraph{Translation quality.}
Table~\ref{tab:offline} compares \ours with offline baselines that have access to the complete source audio when translating. Despite performing simultaneous translation, \ours outperforms all models, including the offline variant of StreamSpeech. Table~\ref{tab:streaming_big_table} benchmarks \ours against available baselines for simultaneous translation. In the short-form setting, our model outperforms StreamSpeech and Seamless at the cost of an average 0.7s of additional lagging. The long-form dataset represents a more significant challenge, as StreamSpeech does not manage to produce intelligible translations. \ours outperforms Seamless, again with a latency higher by an average of 0.8s.

\begin{table}[t]
\caption{\textbf{Human evaluation.} Raters report Mean Opinion Scores (MOS) between 1 and 5.}
\label{tab:human_eval}
\vskip 0.15in
\begin{center}
\begin{scriptsize}
\begin{sc}
\begin{tabular}{lccc}
\toprule
Model & Quality & Speaker Sim. & Naturalness \\
\midrule
Ground-truth & 4.18 $\pm$ 0.07 & - & 4.12 $\pm$ 0.08 \\
\midrule
Seamless & 2.22 $\pm$ 0.08 & 2.86 $\pm$ 0.12 & 2.18 $\pm$ 0.09 \\
\ours & \textbf{3.78} $\pm$ 0.09 & \textbf{3.43} $\pm$ 0.10 & \textbf{3.73} $\pm$ 0.09 \\
\bottomrule
\end{tabular}
\end{sc}
\end{scriptsize}
\end{center}
\vskip -0.1in
\end{table}

\looseness=-1
\paragraph{Audio fidelity.}
\label{sec:eval_human}
Objective evaluations for speaker similarity, as reported in Table~\ref{tab:streaming_big_table}, show that \ours demonstrates significantly better voice transfer than Seamless (we do not evaluate StreamSpeech as it does not perform voice transfer). Human evaluations reported in Table~\ref{tab:human_eval} confirm this result and furthermore show a much higher quality and naturalness than Seamless, that get close to that of ground-truth audio from professional human interpreters. This means that not only \ours produces high-quality audio, but that it inserts smooth and natural pauses into its flow.

\begin{table}[t]
\caption{Ablations.}
\label{tab:ablation_delay}
\vskip 0.15in
\begin{center}
\begin{scriptsize}
\begin{sc}
\begin{tabular}{lccc}
\toprule
Model & ASR-BLEU & LAAL\\
\midrule
No lag & 4.2 & 2.46 & \\
Constant lag (2s) & 10.0 & 2.49 & \\
Constant lag (10s) & 22.5 & 9.02 & \\
Sentence alignment & 25.6 & 21.49 & \\
\midrule
No Inner Monologue & 17.1 & 14.34 \\
No audio pretraining & 14.6 & 5.12 \\
\midrule
Hibiki & 26.6 & 5.0 & \\
\bottomrule
\end{tabular}
\end{sc}
\end{scriptsize}
\end{center}
\vskip -0.1in
\end{table}

\begin{table}[t]
\caption{Ablations on classifier-free guidance.}
\label{tab:ablation-cfg}
\vskip 0.15in
\begin{center}
\begin{scriptsize}
\begin{sc}
\begin{tabular}{lcc}
\toprule
CFG param. & ASR-BLEU & Speaker Sim.\\
\midrule
No CFG & 26.0 & 0.42 \\
$\gamma = 3.0$ (default) & 26.6 & 0.48 \\
$\gamma = 10.0$ & 18.9 & 0.44 \\
\bottomrule
\end{tabular}
\end{sc}
\end{scriptsize}
\end{center}
\vskip -0.1in
\end{table}

\begin{figure}[t]
    \centering
    \includegraphics[width=1.0\columnwidth]{figures/batched_inference_hibiki.pdf}
    \caption{Batched inference speed of Hibiki on a H100 SXM.}
    \label{fig:batched-inference}
\end{figure}


\looseness=-1
\textbf{Ablation: alignment strategies.} We compare our contextual alignment method with alternatives. Table~\ref{tab:ablation_delay} shows that applying no lag to the target speech during training results in very low translation quality, which can be expected since the model lacks context to produce the translation. Adding lag in training examples improves the ASR-BLEU, with 10 seconds representing a reasonable value, however the resulting average latency (as represented by LAAL) is much worse than using a contextual alignment, as the model does not adapt its flow to the context. A middle-ground between a constant lag and contextual alignment is that of ``sentence alignment'' which simply moves the start of each output sentence to the end of the corresponding source sentence. This improves translation quality, however degrading the latency even more. Overall, contextual alignment provides the best trade-off between translation quality and latency.

\textbf{Ablation: Classifier-free guidance.} Table~\ref{tab:ablation-cfg} shows that using the $\texttt{very\_good}$ label provides a speaker similarity of 0.42, similar to that of Seamless (0.43). Using classifier-free guidance with $\gamma = 3.0$ significantly improves it without significantly hurting translation quality, while increasing its weight too much results in degraded performance due to unintelligible speech. Supplementary material interestingly illustrates how increasing $\gamma$ to extreme values results in an exaggerated French accent (the source language in our experiments), which we can attribute to biases in the speaker model used to label our data.

\textbf{Ablation: General ablations.} Section~\ref{sec:multistream} describes how jointly predicting text tokens serves as a scaffolding for audio generation. Table~\ref{tab:ablation_delay} illustrates this claim as training \ours in a unimodal fashion, without predicting text outputs, results much worse performance, as does starting from a pretrained text LM and training directly for S2ST.

\begin{figure}[t]
    \centering
    \includegraphics[width=1.0\columnwidth]{figures/inference_on_iphone_8rvq.pdf}
    \caption{Inference speed of Hibiki-M on an iPhone 16 Pro.}
    \label{fig:iphone-inference}
\end{figure}

\subsubsection{Inference capabilities}
\label{sec:inference_capabilities}
\looseness=-1
\textbf{Batched inference.} \ours's inference uses temperature sampling with a constant framerate. This makes it easy to perform streaming classifier-free guidance as well as batching the processing of several sources of speech at the same time. This is unlike Seamless and StreamSpeech whose complex inference policies are difficult to batch as they require dynamic, irregular decisions for each sequence. Figure~\ref{fig:batched-inference} shows that \ours remains faster than real-time on a H100 even when processing 320 sequences in parallel (or 160 with classifier-free guidance).

\looseness=-1
\textbf{On-device inference.} Our distilled \ours-M is competitive with Seamless on short-form and long-form translation as shown in Table~\ref{tab:streaming_big_table}. We attribute the lower speaker similarity on long-form audio to the lower number of quantizers modeled by \ours-M (8 instead of 16) which results in a twice lower audio bitrate.  Figure~\ref{fig:iphone-inference} shows inference traces of \ours-M on an iPhone 16 Pro. \ours-M remains faster than real-time along a minute of inference, even with a batch size of 2 which is necessary for classifier-free guidance. Training \ours-M with sliding window attention would furthermore improve real-time factor along time.

\subsubsection{Limitations}
This study focuses on a single translation task (French to English) and scaling to more languages could benefit from MADLAD which is massively multilingual, however it would require training TTS systems on more languages. Moreover, while \ours reaches 35.5 ASR-BLEU against CVSS-C ground-truth targets, it reaches 47.9 ASR-BLEU if compared to MADLAD text translations instead. This shows that \ours is excellent at predicting translations that could be produced by MADLAD, and training it to predict pseudo-targets from better or more diverse translations can improve translation quality w.r.t ground-truth targets.