[
  {
    "index": 0,
    "papers": [
      {
        "key": "vinyals2015show",
        "author": "Vinyals, Oriol and Toshev, Alexander and Bengio, Samy and Erhan, Dumitru",
        "title": "Show and tell: A neural image caption generator"
      },
      {
        "key": "wu2019recall",
        "author": "Wu, Lingxiang and Xu, Min and Wang, Jinqiao and Perry, Stuart",
        "title": "Recall what you see continually using gridlstm in image captioning"
      },
      {
        "key": "xu2019multi",
        "author": "Xu, Ning and Zhang, Hanwang and Liu, An-An and Nie, Weizhi and Su, Yuting and Nie, Jie and Zhang, Yongdong",
        "title": "Multi-level policy and reward-based deep reinforcement learning framework for image captioning"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "cornia2020meshed",
        "author": "Cornia, Marcella and Stefanini, Matteo and Baraldi, Lorenzo and Cucchiara, Rita",
        "title": "Meshed-memory transformer for image captioning"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "luo2021dual",
        "author": "Luo, Yunpeng and Ji, Jiayi and Sun, Xiaoshuai and Cao, Liujuan and Wu, Yongjian and Huang, Feiyue and Lin, Chia-Wen and Ji, Rongrong",
        "title": "Dual-level collaborative transformer for image captioning"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "kuo2023haav",
        "author": "Kuo, Chia-Wen and Kira, Zsolt",
        "title": "Haav: Hierarchical aggregation of augmented views for image captioning"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "zhang2023improving",
        "author": "Zhang, Jing and Xie, Yingshuai and Liu, Xiaoqiang",
        "title": "Improving Image Captioning through Visual and Semantic Mutual Promotion"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "lewis2020retrieval",
        "author": "Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and K{\\\"u}ttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rockt{\\\"a}schel, Tim and others",
        "title": "Retrieval-augmented generation for knowledge-intensive nlp tasks"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "singh2021nlp",
        "author": "Singh, Sushant and Mahmood, Ausif",
        "title": "The NLP cookbook: modern recipes for transformer based deep learning architectures"
      },
      {
        "key": "xu2022rag",
        "author": "Xu, Huan and Liu, Shuxian and Wang, Wei and Deng, Le",
        "title": "RAG-TCGCN: aspect sentiment analysis based on residual attention gating and three-channel graph convolutional networks"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "fei2021memory",
        "author": "Fei, Zhengcong",
        "title": "Memory-augmented image captioning"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "ramos2023smallcap",
        "author": "Ramos, Rita and Martins, Bruno and Elliott, Desmond and Kementchedjhieva, Yova",
        "title": "Smallcap: lightweight image captioning prompted with retrieval augmentation"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "yasunaga2022retrieval",
        "author": "Yasunaga, Michihiro and Aghajanyan, Armen and Shi, Weijia and James, Rich and Leskovec, Jure and Liang, Percy and Lewis, Mike and Zettlemoyer, Luke and Yih, Wen-tau",
        "title": "Retrieval-augmented multimodal language modeling"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "ramos2023retrieval",
        "author": "Ramos, Rita and Elliott, Desmond and Martins, Bruno",
        "title": "Retrieval-augmented image captioning"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "yang2023re",
        "author": "Yang, Zhuolin and Ping, Wei and Liu, Zihan and Korthikanti, Vijay and Nie, Weili and Huang, De-An and Fan, Linxi and Yu, Zhiding and Lan, Shiyi and Li, Bo and others",
        "title": "Re-vilm: Retrieval-augmented visual language model for zero and few-shot image captioning"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "radford2021learning",
        "author": "Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others",
        "title": "Learning transferable visual models from natural language supervision"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "wang2022git",
        "author": "Wang, Jianfeng and Yang, Zhengyuan and Hu, Xiaowei and Li, Linjie and Lin, Kevin and Gan, Zhe and Liu, Zicheng and Liu, Ce and Wang, Lijuan",
        "title": "Git: A generative image-to-text transformer for vision and language"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "li2023blip",
        "author": "Li, Junnan and Li, Dongxu and Savarese, Silvio and Hoi, Steven",
        "title": "Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "alayrac2022flamingo",
        "author": "Alayrac, Jean-Baptiste and Donahue, Jeff and Luc, Pauline and Miech, Antoine and Barr, Iain and Hasson, Yana and Lenc, Karel and Mensch, Arthur and Millican, Katherine and Reynolds, Malcolm and others",
        "title": "Flamingo: a visual language model for few-shot learning"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "liu2024visual",
        "author": "Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae",
        "title": "Visual instruction tuning"
      }
    ]
  }
]