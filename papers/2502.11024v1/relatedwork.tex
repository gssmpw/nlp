\section{Related Work}
In this section, we review the literature on image captioning, retrieval-augmented generation (RAG), and visual-language alignment, highlighting their advancements and limitations.

\subsection{Image Captioning} 
Image captioning aims to generate natural language descriptions for visual content. Traditional methods adopt an encoder-decoder architecture~\cite{vinyals2015show, wu2019recall, xu2019multi} trained in an end-to-end fashion, where pre-trained visual models extract image features that are subsequently decoded into textual descriptions. Early research primarily focused on aligning visual features at different granularities with textual semantics. For instance, $M^{2}$~\cite{cornia2020meshed} employs a memory-enhanced encoding layer and a reticular connection decoding layer to model the multi-level relationships between image regions and text. DLCT~\cite{luo2021dual} integrates grid and region features to better align semantics at varying granularities with textual descriptions. HAAV~\cite{kuo2023haav} introduces heterogeneous views of input images to enrich semantic granularity, employing a shared encoder for all views to enhance alignment. VST~\cite{zhang2023improving} incorporates a global position-sensitive co-attention encoder to improve spatially-aware semantic interactions between visual and textual features.
Despite their effectiveness in enhancing visual-semantic granularity, these methods largely neglect the semantic limitations of textual features, which hinders their ability to generalize to open-world scenarios. 
% To address this, we leverage the rich semantic knowledge and strong contextual capabilities of large language models (LLMs) to enhance visual understanding, thereby improving the model's open-world generalization.

\subsection{Retrieval-Augmented Generation}
Retrieval-augmented generation (RAG) enhances generation tasks by incorporating external knowledge retrieved based on the input query. Initially introduced in natural language processing (NLP)~\cite{lewis2020retrieval}, RAG improves prediction quality by retrieving relevant information from large-scale document repositories to generate more informed outputs~\cite{singh2021nlp, xu2022rag}. Recently, RAG has been extended to image captioning. For instance, AoANet~\cite{fei2021memory} utilizes a memory bank of image-sentence pairs and target words to enhance caption generation. SmallCap~\cite{ramos2023smallcap} retrieves captions from a dedicated datastore using image-to-text similarity. RA-CM3~\cite{yasunaga2022retrieval} integrates text and image information in a dense multimodal retriever to fetch relevant documents. EXTRA~\cite{ramos2023retrieval} and Re-ViLM~\cite{yang2023re} retrieve captions by comparing input image features with vision candidates in the retrieval bank.
However, these approaches require constructing and maintaining an external retrieval bank, which not only increases computational and storage overhead but also presents usability challenges for non-expert users. 
% To overcome these limitations, we propose a retrieval-augmented image captioning framework that leverages LLMs to dynamically generate entity-related information during inference, bypassing the need for pre-built retrieval banks and significantly reducing operational complexity.

\subsection{Visual-Language Alignment}
Effective alignment of visual and textual features is critical for image captioning, as it directly influences the quality of the generated descriptions. CLIP~\cite{radford2021learning} pioneered the use of contrastive learning to align image and text features into a shared embedding space. Building on this, recent methods have explored visual-language alignment within the context of LLMs. For example, GIT~\cite{wang2022git} utilizes a generative image-to-text converter to establish alignment. BLIP-2~\cite{li2023blip} connects frozen vision encoders and LLMs through generative pre-training. Flamingo~\cite{alayrac2022flamingo} introduces GATED XATTN-DENSE layers into frozen language model layers to facilitate alignment. LLava~\cite{liu2024visual} employs a learnable projection matrix to map visual features into the same embedding space as the word embeddings of the LLM, enabling efficient alignment.
While these methods have achieved significant success, their performance often deteriorates when fine-tuned on small, biased datasets, leading to suboptimal feature alignment and biased mappings. 

% To address this issue, we introduce a novel alignment mechanism that combines a learnable linear projector with a frozen projector to preserve the generalization capabilities of the model while adapting to new datasets. Additionally, we propose a multi-modal purification (MP) module that compresses and refines entity-related information to remove irrelevant noise introduced by LLMs, ensuring high-quality visual-language alignment.


\begin{figure*}[!t]
	\centering
	\includegraphics[width=1\textwidth]{overview_2.9.pdf}
	\caption{\textbf{Overview of the proposed TPCap.} We introduce a specialized RAG approach and a trigger projector to assist the network in aligning visual features with text features and enhancing its zero-shot capability. \textbf{First}, given an image, we extract visual features using a frozen visual encoder and generate visual-language features through a frozen Q-Former. \textbf{Then}, the visual-language features are concatenated with language prompt 1 and projected into the shared dimension by a trigger projector to enhance alignment ability. \textbf{Then}, the projected features are input into frozen LLMs to generate coarse-grained information about the entity. \textbf{Then}, a multi-modal purification is used to purify and refine the coarse-grained entity information and align it with visual-language features. \textbf{Then}, we concatenate visual-language features, entity features, and language prompt 2, projected into the shared dimension by the trigger projector to enhance alignment ability. \textbf{Finally}, a frozen LLM uses the projected features to generate output.}
	\label{fig_1}
\end{figure*}