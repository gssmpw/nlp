\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
%Template version as of 6/27/2024

\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{wrapfig}
\usepackage{booktabs} 
\usepackage{multirow}
\usepackage[colorlinks,linkcolor=red]{hyperref}
\newcommand{\rui}[1]{\textbf{\textcolor{red}{Rui: #1}}}

\newcommand{\jiang}[1]{\textbf{\textcolor{blue}{jiang: #1}}}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{Gaussian Difference: Find Any Change Instance in 3D Scenes\\

\author{
    \IEEEauthorblockN{
        Binbin Jiang\IEEEauthorrefmark{1}, 
        Rui Huang\IEEEauthorrefmark{1}, 
        Qingyi Zhao\IEEEauthorrefmark{1}, 
        Yuxiang Zhang\IEEEauthorrefmark{1}
    }
    \IEEEauthorblockA{
        \IEEEauthorrefmark{1}College of Computer Science and Technology, Civil Aviation University of China, Tianjin, P.R. China\\
        % \IEEEauthorrefmark{2}School of XXX, XXX University, Province, P.R. China\\
        % \IEEEauthorrefmark{3}XXX\\
        Email: 
            \{2022051012, rhuang, 2022051007, yxzhang\}@cauc.edu.cn, 
    }
}

% {\mall PaperID:}
% {\footnotesize \textsuperscript{*}Note: Sub-titles are not captured for https://ieeexplore.ieee.org  and
% should not be used}
% \thanks{Identify applicable funding agency here. If none, delete this.}
% }

% \author{\IEEEauthorblockN{1\textsuperscript{st} Given Name Surname}
% \IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
% \textit{name of organization (of Aff.)}\\
% City, Country \\
% email address or ORCID}
% \and
% \IEEEauthorblockN{2\textsuperscript{nd} Given Name Surname}
% \IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
% \textit{name of organization (of Aff.)}\\
% City, Country \\
% email address or ORCID}
% \and
% \IEEEauthorblockN{3\textsuperscript{rd} Given Name Surname}
% \IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
% \textit{name of organization (of Aff.)}\\
% City, Country \\
% email address or ORCID}
% \and
% \IEEEauthorblockN{4\textsuperscript{th} Given Name Surname}
% \IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
% \textit{name of organization (of Aff.)}\\
% City, Country \\
% email address or ORCID}
% \and
% \IEEEauthorblockN{5\textsuperscript{th} Given Name Surname}
% \IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
% \textit{name of organization (of Aff.)}\\
% City, Country \\
% email address or ORCID}
% \and
% \IEEEauthorblockN{6\textsuperscript{th} Given Name Surname}
% \IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
% \textit{name of organization (of Aff.)}\\
% City, Country \\
% email address or ORCID}
}
\maketitle

\begin{abstract}
% Detecting 3D instance-level changes without training on labeled image pairs, controlling the camera poses, and restricting the lighting is very challenge. In this paper, we try to find any change instance in 3D scenes under the wild conditions. We use 4D Gaussians to map two sets of images to NeRF and render two image sequences. Each image is segmented to obtain instances, each of which can be tracked to assign a unique identifier. Comparing the IDs of two images, the changed instances can be easily detected. We use the change maps to partition the Gaussians into changed and unchanged by adding a classification encoding to each Gaussian. Then the change map of any given view direction can be rendered by the NeRF. We have conducted abundant experiments on several instance-level change detection scences. Our method significantly improves the detection results of \textsc{C-NeRF} and CYWS-3D when the scenes having large lighting differences.

Instance-level change detection in 3D scenes presents significant challenges, particularly in uncontrolled environments lacking labeled image pairs, consistent camera poses, or uniform lighting conditions. This paper addresses these challenges by introducing a novel approach for detecting changes in real-world scenarios. Our method leverages 4D Gaussians to embed multiple images into Gaussian distributions, enabling the rendering of two coherent image sequences. We segment each image and assign unique identifiers to instances, facilitating efficient change detection through ID comparison. Additionally, we utilize change maps and classification encodings to categorize 4D Gaussians as changed or unchanged, allowing for the rendering of comprehensive change maps from any viewpoint. Extensive experiments across various instance-level change detection datasets demonstrate that our method significantly outperforms state-of-the-art approaches like C-NERF and CYWS-3D, especially in scenarios with substantial lighting variations. Our approach offers improved detection accuracy, robustness to lighting changes, and efficient processing times, advancing the field of 3D change detection.

% Gaussian splatting saves the scene information of a certain time point by synthesis of the 3D scene. However, it only concentrated on the appearance and geometric modeling, and how to detect changes in two sets of Gaussian distribution at different time points in the same scene is an appealing task. Existing methods based on absolute difference cannot overcome the effects of light changes and shadows, the diversity of scenes and the lack of suitable change detection data sets present substantial challenges in devising a solution. In order to solve these problems, we propose XXX. We use SAM to obtain the 2D instance segmentation masks of the images at two time points and track the changes of the segmentation object at two-time points to get the 2D change mask. In order to make the tracking more stable, we use pre-built 4D Gaussians for pose interpolation and rendering to make the image sequence smoother. We add a classification encoding to each Gaussian distribution and divide all Gaussian distributions into changed and unchanged Gaussian distributions so that the changed results can be rendered in new view directions. Experiments show that our method can detect changes under the influence of light and shadow, and get good results.
\end{abstract}

\begin{IEEEkeywords}
Change detection, 4D Gaussian, instance segmentation, 3D.
\end{IEEEkeywords}


%---------------------------------------
\section{Introduction}
% 3D change detection (CD) is a technique that aims to identify change objects of a scene over time using the geometric information of the 3D scene, which can generate a change map of an arbitrary viewpoint \cite{ulusoy2014image,6942806,qin20163d,huang2023c}.


Most existing change detection (CD) methods rely on coupled images and identify changes based on 2D appearance features, These approaches have been extensively researched across various domains, including street-view scenes \cite{ sakurada2013detecting, alcantarilla2018street} and remote sensing \cite{2d_captioning_jhamtani2018learning, chen2020spatial, huang2023background}.  Lei et al. \cite{lei2020hierarchical} propose a hierarchical paired channel fusion network to improve the accuracy of change maps.
Wang et al. \cite{wang2023cross} propose an effective multi-level feature interaction method to achieve high-performance change detection. Despite these improvements, such CD methods face significant limitations. They depend heavily on large-scale labeled datasets (e.g., \cite{chen2020spatial,ji2018fully, lebedev2018change}) that require precise camera pose alignment, which can be a significant challenge.

% highly dependent on large scale labeled datsets, such as  \cite{chen2020spatial,ji2018fully, lebedev2018change}, which require carefully aligning the camera poses. 



% 2D change detection requires the viewpoint shift between the image pair captured before and after the change is minimal.
% In addition, these methods rely on data sets, have limited adaptability to new scenarios, and cannot represent change maps from a new view.

% 2D change detection is extensively researched in various domains, including remote sensing, and street-view scenes\cite{2d_captioning_jhamtani2018learning, chen2020spatial, huang2023background, sakurada2013detecting, alcantarilla2018street}, which produces binary maps that highlight
% changed pixels.
% % A fundamental requirement for 2D change detection is that the viewpoint shifts between the image pair captured before and after the change are minimal.
% 2D change detection relies on a large amount of data training and has high requirements on data sets \cite{chen2020spatial,ji2018fully, lebedev2018change}.
% %
% % In \cite{jst2015change}, Sakurada et al. use a convolutional neural network in combination with superpixel segmentation to detect changes on a pair of street-view images. 
% In \cite{jst2015change}, Sakurada et al. employ a convolutional neural network combined with superpixel segmentation to detect changes. 
% %
% % Lei et al. \cite{lei2020hierarchical}  design an effective feature fusion method to improve the accuracy of the corresponding change maps.
% Lei et al. \cite{lei2020hierarchical} develop an effective feature fusion method to enhance the accuracy of change maps.  
% %
% % Chen et al. \cite{chen2021dr} propose temporal attention and explore the impact of the dependency-scope size of temporal attention on the performance of change detection. 
% Chen et al. \cite{chen2021dr}introduce the concept of temporal attention in change detection. 
% % and investigate how the dependency-scope size of this attention mechanism impacts change detection performance. 
% %
% % Sachdeva et al. \cite{sachdeva2023change2D} aims to detect the``object-level'' change in an image pair with different illumination at different viewpoints.
% Sachdeva et al. \cite{sachdeva2023change2D} focus on detecting "object-level" changes in image pairs.
% %captured under varying illumination conditions and from different viewpoints.
% %
% In their latest work \cite{sachdeva2023change3D}, the authors extend their research to address change detection scenarios involving significant camera pose shifts.
% %
% 2D change detection requires the viewpoint shift between the image pair captured before and after the change is minimal.
% In addition, these methods rely on data sets, have limited adaptability to new scenarios, and cannot represent change maps from a new view.

% %
% % Change detection in 3D scenes is an extension of open-world 3D scene understanding, aimed at detecting changes in 3D scenes at different timestamps. It has wide applications in scene detection, robot navigation, and autonomous driving.
% %

In contrast to conventional CD methods, 3D CD aims to identify changes in a scene over time by leveraging the geometric information inherent in 3D representations. A key advantage of 3D CD is its ability to generate change maps from arbitrary viewpoints \cite{ulusoy2014image,6942806,qin20163d,huang2023c}, offering greater flexibility and robustness compared to traditional 2D methods. Typically, 3D CD utilizes a combination of multi-view images and depth information captured both before and after scene changes, providing a more comprehensive basis for change analysis.
% 
% 
% 3D change detection can reflect change in 3D space with multi-view images and depth images before and after scene change and \cite{taneja2011image,ulusoy2014image,6942806,qin20163d}. 
%
% Previous research\cite{5980542} has utilized dense color and depth information to detect changes between 3D maps. 
%
Fehr et al.\cite{7989614}  develop a 3D reconstruction algorithm that utilizes an extended Truncated Signed Distance Function to solve the scene differencing problem more accurately. 
%
Ku et al.\cite{ku2021shrec} contribute a street-scene dataset for 3D point cloud change detection that can detect changes in a complex street environment from multi-temporal point clouds. 
%
Qiu et al.\cite{qiu20233d} propose a method to explicitly localize changes in 3D bounding boxes from two point clouds. %describing detailed scene changes. 
%
Sachdeva et al.\cite{sachdeva2023change2D} propose a method for change detection by using depth maps to map the images before and after changes into point clouds. Achieving high-quality 3D point clouds, especially from real-world scenes, can indeed be quite expensive and time-consuming.
%
Recent research \cite{huang2023c} has proposed a CD method, \textsc{C-NeRF}, grounded in Neural Radiance Fields (NeRF) \cite{mildenhall2020nerf}, which enables change mapping with only two image sequences captured from the scene before and after the change occurs.
\textsc{C-NeRF}  constructs two aligned NeRFs and renders image pairs to calculate absolute differences for change identification. While C-NERF offers the advantage of rendering high-quality change maps from novel viewpoints, it faces limitations in scenes with substantial lighting variations and requires considerable processing time.
%
% Compared to conventional CD, 3D CD has have high requirements for data sets. In addition to multi-view images, depth map or point cloud is usually needed to obtain geometric information of 3D scenes, which requires expensive equipment. Besides, these methods are limited in their ability to represent open-world 3D scenes.
% Although \textsc{C-NeRF} does not require depth maps and has high rendering quality, \textsc{C-NeRF} cannot overcome the effects of light and shadows, and training and detection takes a long time.
%


\begin{figure*}[!ht]
  \centering
  \includegraphics[width=\linewidth]{imgs/fm.pdf}
  \caption{The main framework of the proposed 3D change detection method.} %\vspace{-10px}
  \label{fig:pipline2}
\end{figure*}

% 3D Gaussian splatting has been proven to have an efficient 3D scene representation ability. Its follow-up method 4D Gaussians Splatting\cite{wu20244d} uses a deformation field to model the time change of gaussian distribution on the basis of 3D Gaussians, and extends it to dynamic Gaussians, making it possible to establish a single model to express the multi-time stamp information of the scene. However, how to use gaussian splatting to detect the change in a 3D scene based on its excellent scene expression ability is still a task that needs further research. Huang et al.\cite{huang2023c} proposed a method for change detection based on NeRF using absolute difference and multi-view consistency. This method can effectively detect the change in the scenes, but it has high requirements for the environment, and it is affected by the light and shadow in the outdoor scenes. The main challenges of 3D scene change detection based on Gaussian splatting are as follows: 1) Lack of change detection data sets, open world 3D scene contains a large number of scenes, and currently there are no change detection data sets suitable for this task, so it is necessary to design an unsupervised change detection method; 2) Mapping of change detection results. In order to obtain consistent change detection results from a new view direction, it is necessary to map the change detection results back to the gussian distributions and divide the gaussian distributions. 3) Illumination and shadow influence, the environment of the open world 3D scene is difficult to control, how to achieve the unsupervised method to avoid illumination and shadow influence for change detection is a great challenge.

%



% The focus of this paper is to design a method that can effectively detect 3D scene changes under environmental influences such as light and shadow without relying on large amounts of data for training. In order to make full use of the input data, that is, multi-view images, we combine the two groups of multi-view images before and after the change and treat them as a set of image sequences. In order to avoid the environmental effects of light and shadow, we use SAM to segment each image. In order to make the segmtioned objects correspond to each other in each image, we use a zero-lens tracker to ensure that the agreed objects are consistent across the different images. Since the two groups of images before and after the change are the same image sequence, the tracker can find the object changes in the scene. Since multi-view images are usually shot at will, the similarity of adjacent images in the image sequence may not be enough to make the tracker run effectively. We used 4D Gaussians to build the scene model and used pose interpolation to render the image of the middle view to make the image sequence smoother. After the ratio change masks are obtained, they are mapped back to the Gaussian distribution in order to obtain consistent change detection results in a new perspective. The main contributions
% of this paper are as follows:
%
%
In this paper, we focus on designing a robust and efficient instance-level CD method in 3D scenes. 
Our method aims to effectively address scenarios with uncontrolled lighting conditions while achieving fast processing times and eliminating the need for manual threshold setting. To this end, we propose \textit{Gaussian Difference} which builds on 4D Gaussian Splatting \cite{wu20244d} that extends 3D Gaussian \cite{kerbl20233d} by incorporating a deformation field to model temporal changes in Gaussian distributions. The key advantage of 4D Gaussian lies in its ability to express multi-temporal scene information within a single unified model.
%
We embed multiple images captured from a scene before and after the change into 4D Gaussian. Then we interpolate camera poses to generate a smooth image sequence composed of rendered images before and after the change. We segment each image with SAM \cite{kirillov2023segment} to obtain independent instances and track the instance with DEVA \cite{cheng2023tracking} to endow each instance with a unique identifier. By comparing the IDs in the image pairs, we can efficiently identify the change instances. The change maps are utilized to retrain the 4D Gaussians with newly added classification encoding, serving as a change indicator to categorize Gaussians as either changed or unchanged. Gaussian Difference can render an accurate change map of any view direction. We have conducted abundant experiments on various scenes. Compared with \text{C-NeRF} \cite{huang2023c} and CYWS-3D \cite{sachdeva2023change3D}, Gaussian Difference demonstrates comparable detection performance on the scenes with uniform lighting and significant improvements in detection accuracy on the scenes with large lighting variations. Our main contributions are as follows:
\begin{itemize}
    \item We propose Gaussian Difference, a 4D Gaussian Splatting based change detection framework, which is capable of generating a change map of any view direction.
    
    % can be used to synthesize change maps under any specified view direction, which indicates the changing area under this view. 

    \item Gaussian Difference is efficient in training and robust to the lighting changes of scenes and does not require manually setting the threshold for each scene.
    
    % We make full use of multi-view images, without relying on additional data for training, and can overcome environmental effects such as light and shadow for instance level change detection.
    %

    \item We enrich the 3D instance-level CD dataset with more scenes that have large lighting variations, which will be released to boost the study of 3D change detection.
    
    % have conducted extensive experiment on various instance-level change detection datasets, our method demonstrates significant improvements in detection accuracy over state-of-the-art methods like \textsc{C-NeRF} and CYWS-3D, particularly in scenarios with large lighting variations.
    
    % In order to verify the effectiveness, we conducted experiments on n scene to prove the effectiveness of our method.
    
    % We build a change detection dataset with 10 different scenes. Each scene contains multiview images of the before and after scene changes to facilitate the NeRF model training.
\end{itemize}

% By dividing each rendered image and assigning a unique identifier to each instance, we can effectively identify the changed instances by comparing IDs. At the same time, we divide the Gaussian distribution into change or no change using change maps with change encoding, so that the change maps can be rendered in any unobserved direction.


% The focus of this paper is to design a method that can effectively detect 3D scene changes under environmental influences such as light and shadow without relying on large amounts of data for training.
% %
% 3D Gaussian splatting \cite{kerbl20233d} has been proven to have an efficient 3D scene representation ability. Its follow-up method 4D Gaussians Splatting\cite{wu20244d} uses a deformation field to model the time change of gaussian distribution on the basis of 3D Gaussians, and extends it to dynamic Gaussians, making it possible to establish a single model to express the multi-time stamp information of the scene.
% %
% Our method uses the multi-view images before and after the scene changes with 4D GS to build a scene model at two-time points and interpolate pose and render to obtain a smooth image sequence composed of rendered images before and after the change. By dividing each rendered image and assigning a unique identifier to each instance, we can effectively identify the changed instances by comparing IDs. At the same time, we divide the Gaussian distribution into change or no change using change maps with change encoding, so that the change maps can be rendered in any unobserved direction.
%---------------------------------------
\section{METHODOLOGY}
% \subsection{Overview}
The primary objective of our research is to develop a comprehensive 3D scene representation of change instances, capable of accurately rendering images of alterations from novel viewing directions.  As illustrated in Fig.~\ref{fig:pipline2}, our 3D representations are built on 4D Gaussian splatting \cite{wu20244d}. Given two image sets $\mathcal{S}_1= \{\textbf{I}_1^1, \textbf{I}_2^1,\dots, \textbf{I}_{N_1}^1\}$ and $\mathcal{S}_2 = \{\textbf{I}_1^2, \textbf{I}_2^2,\dots, \textbf{I}_{N_2}^2\}$ captured before and after change, we initially learn a 4D Gaussian representation $\Psi$. And then we interpolate poses to generate smooth variations from $\textbf{I}_1^1$ to $\textbf{I}_{N_1}^1$ and $\textbf{I}_1^2$ to $\textbf{I}_{N_2}^2$, which can be used to generate consistent IDs of the instances across different frames. Comparing the IDs of the two scenes, we can easily obtain the change IDs and their corresponding masks with track-based instance-level change detection. Finally, we propose 4D Gaussian distribution partitioning to categorize the Gaussians into changed Gaussians $\Psi_\text{c}$ and unchanged Gaussians $\Psi_\text{uc}$, utilizing the change maps. After partitioning, we can generate a change map of any desired view direction.

% Our work aims to establish a 3D scene representation with multiple time points, which can not only model the appearance and geometric shape of the scene at different time points, but also detect instance-level changes in the scene under the influence of lighting and shadows, and present corresponding changes in a new view direction. As shown in Fig.~\ref{fig:framework}, we design our method based on 4D GS \cite{wu20244d}. The input of this method is a multi-view image of a scene at different time points (hereinafter referred to as two-time points $\mathbf{t}_a$ and $\mathbf{t}_b$). To avoid the impact of lighting and shadows, we use segmentation based methods for change detection. In order to ensure consistency in the segmentation results of each image, we track each segmentation object and determine the changing objects by tracking the segmentation objects. In order to avoid the problem of tracking failure caused by large pose differences between adjacent images, we establish a 4D GS model in advance using multi-view images, interpolate the poses of the multi-view images, and render the interpolated images. This makes the entire image sequence more smoother. In order to map the change region back to 4D GS, we added an attribute to partition the Gaussian so that it can obtain the mask of the change object from a new view direction.

%---------------------------------------
\subsection{Camera pose interpolation}
% In order to track the segmentation object, adjacent images in the image sequence should be similar. If not, it will lead to tracking failure, and two adjacent images in multi-view images usually cannot meet similarity requirements. To solve this problem, we use 4D GS to construct a scene at two-time points $\mathbf{t}_a$ and $\mathbf{t}_b$ with multi-view images. Since 4D GS adds time dimension on the basis of 3D GS, we can use it to build models of different times of the scene. 

Let $\mathcal{P}_1=\{p_1^1,p_2^1, \dots, p_{N_1}^1 \}$ and $\mathcal{P}_2=\{p_1^2,p_2^2, \dots, p_{N_2}^2\}$ represent the camera poses of the images in $\mathcal{S}_1$ and $\mathcal{S}_2$, respectively. 
Note that $p_i^k = (Q_i^k, T_i^k)$ denotes the camera pose of the $i$-th image of the $k$-th image set. $Q_i^k$ and $T_i^k$ denote the rotation quaternion and translation matrix, respectively.
%
We merge $\mathcal{P}_1$ and $\mathcal{P}_2$ to form a combined set $\mathcal{P} = \{p_1,p_2, \dots, p_{N_1}, p_{N_1+1},p_{N_1+2}, \dots, p_{N_1+N_2}\}$. 
%
We use spherical linear interpolation and linear interpolation to interpolate $n$ rotation quaternions and translation matrixes between two camera poses $p_i$ and $p_{i+1}$ by
\begin{equation}
    Q_{ij}= \frac{\sin((1 - \Delta_j) \theta)}{\sin(\theta)} Q_i + \frac{\sin(\Delta_j \theta)}{\sin(\theta)} Q_{i+1},
\end{equation}
\begin{equation}
    T_{ij} = (1 - \Delta_j) T_i + \Delta_j T_{i+1},
\end{equation}
    % T_{ij}^k = (1 - t_j) T_i^k + t_j T_{i+1}^k,%\\
    % t_j = \frac{j}{n + 1}, \quad j = 1, 2, \ldots, n,
% \end{align} 
where $\Delta_j = \frac{j}{n + 1}, j=1,\dots,n$,  and $\mathbf{\theta}$ is the angle between $Q_i$ and $Q_{i+1}$. Thus, we can get two novel camera poses $\mathcal{\tilde{P}}_1=\{p_1^1,p_{11}^1,\dots, p_{1n}^1,p_2^1, \dots, p_{N_1}^1 \}$ and $\mathcal{\tilde{P}}_2=\{p_1^2,p_{11}^2,\dots,p_{1n}^2,p_2^2, \dots, p_{N_2}^2\}$. 
% Note that we discard the poses between $p_{N_1}^1$ and $p_{N_2}^2$ for later steps.
Note that we discard the interpolation poses between $p_{N_1}$ and $p_{N_1+1}$ in $\mathcal{P}$ for later steps.
%
% The poses of $I^{a}$ and $I^{b}$ after interpolation are $\tilde{P}^{a}$ and $\tilde{P}^{b}$ respectively.


% to generate new rotation quaternions for quaternions $Q$ to ensure the smoothness of the interpolation results, and linear interpolation for the translation matrix $T$. The calculation process for inserting $n$ poses between adjacent poses is as follows:

% We regard the two groups of multi-view images before and after changes $I^{a}, I^{b}$as images at two time points $\mathbf{t}_a$ and $\mathbf{t}_b$.  With 4D GS, we use the existing image poses to interpolate the poses of the intermediate synthetic views and render them to ensure the smoothness of the image sequence. For pose $P=(Q, T)$, where $P=\{p_1^{a},p_2^{a}, \ldots, p_{N_1}^{a}\,p_1^{b},p_2^{b}, \ldots, p_{N_2}^{b}\}$ and $p_i^{k} = (q_i^{k}, T_i^{k})$, $k\in\{a,b\}$. Similar to $P$, $Q$ and $T$ are rotation quadruple sequences and translation matrix sequences of $I^{a}$ and $I^{b}$. We use spherical linear interpolation for quaternions $Q$ to ensure the smoothness of the interpolation results, and linear interpolation for the translation matrix $T$. The calculation process for inserting $n$ poses between adjacent poses is as follows:

% \begin{align}
%     q_{ij}^{k} = \frac{\sin((1 - t_j) \theta)}{\sin(\theta)} q_i^{k} + \frac{\sin(t_j \theta)}{\sin(\theta)} q_{i+1}^k,\\
%     T_{ij}^k = (1 - t_j) T_i^k + t_j T_{i+1}^k,\\
%     t_j = \frac{j}{n + 1}, \quad j = 1, 2, \ldots, n,
% \end{align}
% Where $k\in\{a,b\}$, and $\mathbf{\theta}$ is the angle between $q_i^{k}$ and $q_{i+1}^k$. The poses of $I^{a}$ and $I^{b}$ after interpolation are $\tilde{P}^{a}$ and $\tilde{P}^{b}$ respectively.


%---------------------------------------
% \subsection{Track-based instance-level change detection}
\subsection{Change detection with inconsistent IDs}
% We combine the rendered images of two timestamps into an image sequence. Inspired by \cite{}, we segment each image in the rendered image sequence $\tilde{I}  = \{\tilde{I}^{a}, \tilde{I}^{b}\}$, and use a pre-trained zero-lens tracker to associate the segmentation object in each image so that the same object has the same ID in each image. We manage the segmented object IDs of the two timestamps respectively and record the changed object by the change of the object IDS of the two timestamps. The calculation process is as follows:
% \begin{align}
%     (ID^{a},ID^{b}) = SAM(\tilde{I} ),\\
%     ID^{c} = (ID^{a} \setminus ID^{b}) \cup (ID^{b} \setminus ID^{a}).
% \end{align}
% Where $SAM()$ is the process of segmentation and tracking, $ID^{a}$ and $ID^{b}$ respectively represent the ID set of objects after segmentation in the scene at time a and time b. Since the rendered images at time a and time b are processed as a sequence, the objects appearing in both timestamps have the same ID, $A\setminus B$ represents the elements in A but not in B, and $ID^{c}$ is a set of changing object IDs.
Given $\mathcal{\tilde{P}}_1$, we use the pre-trained 4D GS representation $\Psi$ to generate two image sets $\mathcal{\tilde{S}}_1 = \{\textbf{I}_1^1,\textbf{I}_{11}^1, \dots,\textbf{I}_{1n}^1,  \textbf{I}_2^1,\dots, \textbf{I}_{N_1}^1\}$ and its corresponding image set $\mathcal{\tilde{S}}_1' = \{\textbf{I}_1^2,\textbf{I}_{11}^2, \dots,\textbf{I}_{1n}^2,  \textbf{I}_2^2,\dots, \textbf{I}_{N_1}^2\}$ after change.
%
In order to generate a consistent ID for the same instance, we first use SAM \cite{kirillov2023segment} to segment each image to obtain the independent instances. Then, we utilize DEVA \cite{cheng2023tracking}, a pre-trained zero-shot tracker, to assign IDs to the instances. By comparing the IDs of $\text{I}_1^1$ and $\text{I}_1^2$, we identify the IDs of the change instances at the camera pose $p_1^1$. This procedure is repeated for all other poses. The masks of the changed instances serve as our change maps. Consequently, we obtain the change mask set  $\mathcal{M}_1=\{\textbf{M}_1^1,\textbf{M}_{11}^1, \dots,\textbf{M}_{1n}^1,  \textbf{M}_2^1,\dots, \textbf{M}_{N_1}^1\}$ for $\mathcal{\tilde{P}}_1$. Same operations are conducted on $\mathcal{\tilde{P}}_2$ to generate $\mathcal{M}_2=\{\textbf{M}_1^2,\textbf{M}_{11}^2, \dots,\textbf{M}_{1n}^2,  \textbf{M}_2^2,\dots, \textbf{M}_{N_2}^2\}$.




% In order to map the results of change detection to 4D GS, we need to obtain the corresponding change detection maps of $I^{a}$ and $I^{b}$. To make the change detection more robust, we use 4D GS and interpolated poses to obtain the render image $\tilde{I}  = \{\tilde{I}_a^{a}, \tilde{I}_a^{b}\}$, which means render images at time $\mathbf{t}_a$ and $\mathbf{t}_b$ with the same interpolated poses $P^a$.Due to the lack of 3D open-world change detection data sets, we use segmentation and trace-based methods to detect changes. Specifically, we first use SAM\cite{kirillov2023segment} to segment each image in $\tilde{I}$ to obtain segmentation masks. The segmentation results of each image are independent. Then, We use a pre-trained zero-shot tracker\cite{cheng2023tracking} to associate the segmentation objects in each image so that the same object has the same ID in each image. We separately manage the IDs of the segmentation objects in $\tilde{I}_a^{a}$ and $\tilde{I}_a^{b}$. Since the ID of the same object in different images is the same, the changing objects can be determined by the changes of IDs in image sequence $\tilde{I}$. The calculation process is as follows:
% \begin{align}
%     (ID^{a},ID^{b}) = SaT(\tilde{I} ),\\
%     ID_{change}^{a} = ID^{a} \setminus ID^{b}
% \end{align}
% Where $SaT(\cdot)$ is the process of segmentation and tracking, $ID^{a}$ and $ID^{b}$ respectively represent the ID set of objects after segmentation in $\tilde{I}_a^{a}, \tilde{I}_a^{b}$. $ID^{a} \setminus ID^{b}$ means ids exist in $ID^{a}$ but not in $ID^{b}$. In the above process, we render the image by using the pose at time $\mathbf{t}_a$, so we can obtain the change detection maps $M^{a}$ corresponding to $I^{a}$, and we can obtain the change detection maps $M^{b}$ corresponding to $I^{B}$ by repeating the process using the pose $\tilde{P}^{b}$.


%---------------------------------------
\subsection{4D Gaussian partition}
To generate change maps of new viewpoints, it is essential to identify which Gaussians correspond to the changed instances.  
%
Inspired by Gaussian Grouping \cite{ye2023gaussian}, we introduce classification encoding, which is a learnable vector with 16 bits. The classification of the changed instances and unchanged backgrounds should be consistent across different rendering views. The SH degree of the classification encoding is set to 0 to model its direct-current component.
%
For the classification encoding $e$, we adopt the same processing method as the pixel color. We calculate the 2D classification feature $f_c$ for each pixel by the weighted sum of the classification encodings that overlap with the pixel in depth order by
\begin{align}
    f_c = \sum_{i\in \mathcal{N}}e_{i}\alpha_{i}\prod_{j=1}^{i-1}(1-\alpha_{i}),
\end{align}
where $\mathcal{N}$ represents the set of Gaussian distributions that overlap with the pixels, ordered by depth, and $\alpha_i$ denotes the opacity of the $i$-th Gaussian in the sequence.
%
Let $\textbf{F}\in \mathbb{R}^{16\times H\times W}$ denote the classification feature matrix. Then we can generate the corresponding change map by
\begin{equation}\label{eq:cdmaps}
   \hat{\mathbf{M}} = Cov(\textbf{F}),
\end{equation}
where $Cov (\cdot)$ is a convolutional layer that transforms $\textbf{F}$ into a change mask $\hat{\mathbf{M}}$.

% number of changing objects.

% The setting of the classification code refers to the processing method of the spherical harmonic coefficient (SH) in the Gaussian distribution attributes. Since the classification code acts on a single Gaussian distribution and does not change with the change of the viewing direction, the SH degree of the classification code is set to 0, which ensures consistent change detection results under new viewing directions.





% After obtaining the change detection maps $M^{a}$, $M^{b}$ that correspond to $I^{a}$, $I^{b}$, we conduct the second training for 4D GS to map $M^{a}$, $M^{b}$ to 3D space, so that 4D GS can present consistent changes in a new perspective. We hope to use maps to divide all Gaussian distributions into changing and unchanged Gaussian distributions. Meanwhile, the changing Gaussian distributions are further divided, and the Gaussian distributions belonging to the same object are divided into one class. Inspired by the Gaussian Grouping\cite{ye2023gaussian}, we set a new parameter for each Gaussian distribution, namely the classification code, which is a 16-bit learnable vector. The setting of the classification code refers to the processing method of the spherical harmonic coefficient (SH) in the Gaussian distribution attributes. Since the classification code acts on a single Gaussian distribution and does not change with the change of the viewing direction, the SH degree of the classification code is set to 0, which ensures consistent change detection results under new viewing directions.

% \jiang{Each 3D Gaussian distribution contains the following properties: position $x\in \mathbb{R}^{3}$, scale factor $s\in \mathbb{R}^{3}$, rotation quaternion $r\in \mathbb{R}^{3}$, opacity $\alpha\in \mathbb{R}$, and color $c$ defined by spherical harmonics. 4D GS uses an MLP network to learn changes in position $x$, scale factor $s$, and rotation quaternion $r$ at different times to achieve 4-dimensional Gaussian Splatting. Given a specific camera pose and time $t$, 4DGS first calculates the Gaussian properties at $t$ time through the MLP network and then projects 3D Gaussian into 2D, calculates the color $C$ of a pixel by mixing a set of ordered Gaussian $N$ that overlap the pixels}.
% \begin{align}
%     C = \sum_{i\in N}c_{i}\alpha_{i}\prod_{j=1}^{i-1}(1-\alpha_{i}),
% \end{align}
% \jiang{where $c_i$ is the color of each Gaussian and $\alpha$ is obtained by computing a 2D Gaussian with covariance $\sum $ multiplied with a learned opacity of Gaussian.}

% For the classification encodings $e$ of Gaussian distributions, we adopt the same processing method as the pixel color. We calculate the weighted sum of the classification encodings $e$ of the Gaussian distributions that overlap with the pixel in order according to the depth. The calculation process is as follows:
% \begin{align}
%     f_c = \sum_{i\in N}e_{i}\alpha_{i}\prod_{j=1}^{i-1}(1-\alpha_{i}),
% \end{align}
% where $f_c$ is the classification encoding feature corresponding to a pixel, $N$ is the Gaussian distribution set that overlaps with pixels which is ordered according to depth, and $\alpha_i$ is the opacity of the i th Gaussian distribution in the arrangement. In order to obtain the rendered change detection maps, the following calculations need to be performed:
% \begin{align}\label{eq:cdmaps}
%    \mathbf{M} = \Phi (argmax(Cov(F))),
% \end{align}
% where $\mathbf{M}$ is the rendered change detection maps. $F \in \mathbb{R}^{16\times H\times W}$ is the classification encoding feature corresponding to an image. $Cov (\cdot)$ is a linear layer that transforms the number of feature dimensions into the number of changing objects, and $\Phi(\cdot)$ assign a color to different values.

% We partition the Gaussian distribution on 4D GS. For the classification encoding $e$, we don't set an MLP network for it to learn the changes of $e$ at different times like the attributes such as scale or position in 4D GS, and $e$ does not change after training, which is consistent with the fact that the classification of objects does not change at different time points. When an object appears in the scene at $\mathbf{t}_{a}$ and disappears at $\mathbf{t}_b$, the Gaussian distributions belonging to the object will change its position, scale, and rotation properties to make the object disappear without additional processing for $e$.



%---------------------------------------
\subsection{Loss function}
To learn the new 4D Gaussian representation, we use
$\mathcal{S}_1= \{\textbf{I}_1^1, \textbf{I}_2^1,\dots, \textbf{I}_{N_1}^1\}$ and $\mathcal{S}_2 = \{\textbf{I}_1^2, \textbf{I}_2^2,\dots, \textbf{I}_{N_2}^2\}$ as inputs with 
$\hat{\mathcal{M}}_1=\{\textbf{M}_1^1,  \textbf{M}_2^1,\dots, \textbf{M}_{N_1}^1\}$ and $\hat{\mathcal{M}}_2=\{\textbf{M}_1^2, \textbf{M}_2^2,\dots, \textbf{M}_{N_2}^2\}$ as groundtruth change maps.
%
We propose a 2D loss $\mathcal{L}_{2d}$ utilizing binary cross entropy loss to classify each 3D Gaussian based on the change mask,  by
\begin{equation}
       \mathcal{L}_{2d}=-\frac{1}{\left \| \textbf{M} \right \| } \sum_{i \in \textbf{M}}\sum_{c=0}^{1}p(i)log\hat{p}(i), 
\end{equation}
where $\textbf{M}$ represents the change mask from $\mathcal{M}_1$ or $\mathcal{M}_2$.
%
The unsupervised 3D Regularization Loss $\mathcal{L}_{3d}$ is employed to regulate the learning of classification encoding $e_i$, which takes advantage of the 3D spatial consistency by enforcing the classification encodings of the top $k$-nearest 3D Gaussians to be close in their feature distance. $\mathcal{L}_{3d}$ is defined as
\begin{equation}
\begin{aligned}
   \mathcal{L}_{3d}&=\frac{1}{m} \sum_{m}^{j=1}D_{kl}(\mathcal{G}||\mathcal{H}) \\
   &= \frac{1}{mk}\sum_{m}^{j=1} \sum_{k}^{i=1} \mathcal{F}(e_j)log(\frac {\mathcal{F}(e_i)}{\mathcal{F}(e_j^{'})} ),
\end{aligned}
\end{equation}
where $\mathcal{G}$ consists of the sampled classification encoding $e$ of a 3D Gaussian, the set $\mathcal{H}=\{ e_1^{'},e_2^{'},\cdots,e_k^{'},\}$ represents its $k$ nearest neighbors in 3D space, and $\mathcal{F} = softmax(Cov(e_i))$.

As in the general reconstruction problem \cite{fridovich2022plenoxels, sun2022direct, fridovich2023k}, we also use L1 loss and total variation loss $\mathcal{L}_{TV}$ to supervise the quality of the rendered image. The total loss is defined as
\begin{equation}
    \mathcal{L} = \mathcal{L}_1 + \mathcal{L}_{TV} +\lambda_{2d}\mathcal{L}_{2d} + \lambda_{3d}\mathcal{L}_{3d},
\end{equation}
where $\lambda_{2d}$ and $\lambda_{3d}$ are two balance coefficients.

%---------------------------------------
\subsection{Change detection in a novel view}
After training the 4D Gaussian, we can render an image $\textbf{I}^t$ as \cite{wu20244d} and generate the corresponding change map $\textbf{M}$ by Eq.\ref{eq:cdmaps} for a novel view at the pose $p$ and time $t$.



% After the 4D Gaussian partition is completed, given the time $t$ and pose $p$, the synthetic image $\textbf{I}^{t}$ and change detection maps $ \hat{\mathbf{M}}^{t}$corresponding to the camera pose $p$ at time $t$ can be rendered with the parted 4D GS. 


% For the division of Gaussian distribution, instead of supervising the classification coding $e$, we supervise the maps $\mathbf{M}$ obtained in Eq.\eqref{eq:cdmaps} and use $M^a$ and $M^b$ as the groundtruth for supervision. We not only divide the Gaussian distribution into changing and unchanged categories but also differentiate the Gaussian distribution that has changed. So we use standard cross entropy loss $\mathcal{L}_{2d}$ for classification:
% \begin{align}
%    \mathcal{L}_{2d}=-\frac{1}{\left \| \textbf{M} \right \| } \sum_{i \in \textbf{M}}^{}\sum_{c=0}^{1}p(i)log\hat{p}(i),   
% \end{align}
% where C is the total number of mask identities \textbf{M} and $p$ and $\hat{p}$ are the multi-identity probability of groundtruth mask \textbf{M} and our method predictions.

% According to the Gaussian Grouping\cite{ye2023gaussian}, we use $\mathcal{L}_{3d}$ to improve the grouping accuracy
% of the Gaussian distribution.
% \begin{align}
%    \mathcal{L}_{3d}=\frac{1}{m} \sum_{m}^{j=1}D_{kl}(R||S) = \frac{1}{mk}\sum_{m}^{j=1} \sum_{k}^{i=1} \mathcal{F}(e_j)log(\frac {\mathcal{F}(e_i)}{\mathcal{F}(e_j^{'})} ),
% \end{align}
% where $R$ is a sampling classification encoding $e$ of a 3D Gaussian distribution, and the set $S=\{ e_1^{'},e_2^{'},\cdots,e_k^{'},\}$ represents its $k$ classification encodings of adjacent Gaussian distributions in 3D space, and $\mathcal{F} = softmax(Cov(e_i))$. In the training process, we also used the original loss function $\mathcal{L}_{ori}$ of 4D GS. $\mathcal{L}_{ori}$  and the total loss function of the second phase of training is as follows:
% \begin{align}
%     \mathcal{L}_{ori} = \mathcal{L}_1 + \mathcal{L}_{TV},
% \end{align}
% \begin{align}
%     \mathcal{L} = \mathcal{L}_{ori} + \lambda_{2d}\mathcal{L}_{2d} + \lambda_{3d}\mathcal{L}_{3d},
% \end{align}
% where $\mathcal{L}_{TV}$ is Total variation, which is a common regularizer in inverse problems.

\begin{figure}[!tb]
\centering
\includegraphics[width=\linewidth]{imgs/results1.pdf}%\vspace{-10px}
\caption{Examples of CD results of different methods on four scenes.}
\label{fig:results1}
\end{figure}


%---------------------------------------
\section{EXPERIMENT}
%---------------------------------------
\subsection{Setup}
\textbf{Dataset.} We adopt four scenes of \textsc{C-NeRF} \cite{huang2023c} to evaluate the performances of different CD methods, including Desk, Chess, Potting, and Cats.
%
Desk consists of 40 training images, Chess has 48 training images, Potting includes 42 training images, Cats is built with 243 training images. Each scene has 10 image pairs for evaluation that are uniformly sampled from different viewpoints.

% Most change detection datasets are built with image pairs, which are not applicable to NeRF or Gaussian splatting. NeRFCD is a change detection data set for multi-view images of scenes. It includes two types, instance level and fine granularity. We use 4 instance level scenes as training and test sets.



\textbf{Baselines.} \textsc{C-NeRF} \cite{huang2023c} and 
CYWS-3D \cite{sachdeva2023change3D} are used for comparison. We conduct quantitative comparisons with \textsc{C-NeRF} for it can produce pixel-level prediction. 
CYWS-3D conducts object-level change detection in two-view and class-agnostic scenarios. The input of CYWS-3D is a two-view image,  with the depth map used to obtain geometric information of the scene for change detection. The output is the bounding boxes of the changes. To compare with CYWS-3D, we use the minimum bounding rectangle of the connected area as the bounding box for our method and \textsc{C-NeRF}.



\textbf{Criteria.} We adopt Precision (P), Recall (R), F1-measure (F$_1$), and Intersection over Union (IoU) as evaluation metrics.

%---------------------------------------
\subsection{Results analysis}
Fig.~\ref{fig:results1} shows some typical detection results of our method and \textsc{C-NeRF}. Both methods successfully detect real changes across the four scenes. However, our results are more complete than the results of \textsc{C-NeRF}. In the Cats scene, the detected change region of \textsc{C-NeRF} is influenced by the shadows. To demonstrate the robustness of our method, we perform change detection on two new scenes with challenging lighting conditions. As shown in Fig.~\ref{fig:results2}, the first scene has non-uniform lighting, while the second scene has strong shadows. \textsc{C-NeRF} fails to distinguish the background change with the foreground change. However, our method can detect the changes for both scenes. In Fig.~\ref{fig:results3}, we compare the bounding box results of three CD methods. CYWS-3D can detect changes of larger objects, such as the cat (scene of Cats) and bags (scene of Potting). However, it performs poorly in detecting small objects or objects with subtle geometric changes.



% We find that \textsc{C-NeRF} can detect changing objects in the scene, but it cannot deal with shadow changes in the scene. For example, in the scene Cats, objects disappear, and the shadows that disappear with the objects are considered as changes. At the same time, because \textsc{C-NeRF} is a change detection method based on absolute difference, it has defects in object edge processing, such as scene desk. 
% Fig.~\ref{fig:results2} shows the change detection results of \textsc{C-NeRF} and our method under the influence of light and shadow. \textsc{C-NeRF}, based on absolute interpolation and view consistency, is unable to deal with environmental effects such as lighting and shadows, which limits the use of the method. Our method can resist strong lighting effects and correctly identify changing objects in the scene.

% CYWS-3D uses a box to mark the changing objects in the image, and We compare \textsc{C-NeRF} and the change maps of our method with CYWS-3D using the minimum externality matrix as results. Fig.~\ref{fig:results3} shows that CYWS-3D can detect changes of larger objects, such as the cat in the scene Cats and bags in the scene potting, but for small objects in the scene or objects with insignificant geometric changes, CYWS-3D has poor detection results.

The quantitative results of our method and \textsc{C-NeRF} are presented in Table.~\ref{table:res_fine_grained}. The F$_1$ and IoU values of our method are better than \textsc{C-NeRF} in three scenes. In Table.~\ref{table:running_time}, we compare the running time of the different stages on Potting. Note that our method is more efficient than \textsc{C-NeRF}.

% Taking the change detection of scene potting as an example, we compare the time consumed by C-NeRF and our method. As shown in table 
% \begin{figure}[!tb]
%   \centering
%   \includegraphics[width=\linewidth]{imgs/results1-2.pdf}
%   \caption{Examples of CD results of different methods on four scenes.} %\vspace{-10px}
%   \label{fig:res1}
% \end{figure}

\begin{figure}[!tb]
  \centering
  \includegraphics[width=\linewidth]{imgs/results2.pdf}
  \caption{CD on the scenes with substantial lighting variations.} %\vspace{-10px}
  \label{fig:results2}
\end{figure}




\begin{table}[!t]
\centering
\caption{Quantitative comparison of different CD methods. The best values are highlighted in \textbf{bolded}.}
% \resizebox{0.8\linewidth}{!}{
\begin{tabular}{cc|cccc}
\toprule
Scene   & Method          & P$\uparrow$ & R$\uparrow$ & F$_1$ $\uparrow$ & IoU $\uparrow$  \\ 
\midrule
\multirow{2}{*}{Desk}    & \textsc{C-NeRF}   & \textbf{96.02}       & 90.48       & \textbf{93.17}         & \textbf{87.24}   \\
                         & Ours     & 93.99       & \textbf{91.88}       & 92.78         & 86.75     \\
                        
\midrule
\multirow{2}{*}{Chess}   & \textsc{C-NeRF}   & 92.66       & \textbf{99.71}       & 96.05        & 92.41     \\
                         & Ours     & \textbf{98.64}       & 93.70       & \textbf{96.10}          & \textbf{92.51}    \\
                       
\midrule
\multirow{2}{*}{Potting} & \textsc{C-NeRF}   & 71.59       & \textbf{94.66}       & 81.75         & 69.01      \\
                         & Ours     & \textbf{93.54}       & 76.51       & \textbf{84.31}        & \textbf{72.67}      \\ 
                        
\midrule
\multirow{2}{*}{Cats} & \textsc{C-NeRF}   & 86.70       & 90.46       & 88.54         & 79.47      \\
                      & Ours     & \textbf{97.47}       & \textbf{93.21}       & \textbf{92.41}         & \textbf{91.03}      \\
                      
\bottomrule
\end{tabular}
% }
% \vspace{-10px}
\label{table:res_fine_grained}
\end{table}



\begin{table}[!t]
\centering
\caption{Running time of main stages of different CD methods.}
\resizebox{\linewidth}{!}{
\begin{tabular}{ccccc}
\toprule
Stage  & NeRF/4DGS & CD & Rendering/Gaussian partition  &Total\\ 
\midrule
% Time (hour) & 16.15 & 0.43 & 2min & 2h36min
\textsc{C-NeRF} & 16.07h & 0.75h & 0.78h &17.6h\\
% \bottomrule
% stage & Train 4D GS & Change detection & 4D Gaussian partition &Total\\
% \bottomrule
Ours & 1.17h & 0.1h & 1.2h &2.47h \\
\bottomrule
\end{tabular}
}
% \vspace{-10px}
\label{table:running_time}
\end{table}





\subsection{Discussion}
A 3D Gaussian is characterized by five attributes, including position $x\in \mathbb{R}^{3}$, scale factor $s\in \mathbb{R}^{3}$, rotation quaternion $r\in \mathbb{R}^{3}$, opacity $\alpha\in \mathbb{R}$, and color defined by spherical harmonic(SH) $c \in \mathbb{R}^{k}$ (where $k$ represents nums of SH functions).
% defined by spherical harmonics
In 4D Gaussian Splatting, Separate MLPs are used to compute the deformations of the position $\Delta x$, rotation $\Delta r$, and scale $\Delta s$, and represent both the Gaussian motion and deformation. A direct solution for CD is to use $(\Delta x, \Delta r, \Delta s)$ to find the changed Gaussian, and render the change map. However, as shown in Fig.~\ref{fig:discussion}, we can observe that this simple solution cannot generate the desired results. There are two main reasons: (1) Since $(\Delta x, \Delta r, \Delta s)$ change collaboratively, we cannot accurately distinguish the Gaussian into changed or unchanged;
% and same to the nearby Gaussians, which may not change the rendering results; 
(2) A Gaussian might be used to represent multiple objects or backgrounds. The changed Gaussian not only renders the changed objects but also renders unchanged objects or backgrounds. 

% it cannot accurately distinguish the Gaussian into changed or unchanged. 


% $\Delta x$, rotation $\Delta r$ and  scale $\Delta s$ at time $t_1$, which are the difference between $x$, $r$ and $s$ at the initial time $t_0$ and time $t_1$. Why not use $\Delta x$, $\Delta r$ and  $\Delta s$ to determine the Gaussian distribution that changes at two time points, in order to obtain the changing objects or regions in the scene? 
%
% The following phenomena make it impossible to directly use changes in Gaussian distribution properties for change detection. 
% 
% In the properties of Gaussian distribution, $x$, $r$, and $s$ complement each other. At different time points, the Gaussian distribution may shift within a certain range, but by combining the changes in $r$ and $s$ with the changes in the surrounding Gaussian distributions, there may be a situation where the Gaussian distributions change but the representation remains unchanged, which means the Gaussian distributions corresponding to the unchanged areas in the image undergo significant changes. This results in a lot of noise while locating real changes through this method, as show in Fig.~\ref{fig:discussion}. In experiments. we find that smaller real changes lead to more noise.
%
% In addition, 3D Gaussian distributions often appear as ellipsoids in space. In many cases, directly expressing changing objects through Gaussian distributions can lead to many burrs on the edges of the object. However, due to the same Gaussian distribution, some parts are used to express the object, while others are used to express another object or background. Regardless of whether the Gaussian distribution is judged to belong to or not belong to the object, it will cause the edges of the object to be insufficiently smooth.
%
\begin{figure}[!tb]
  \centering
  \includegraphics[width=\linewidth]{imgs/results3.pdf}
  \caption{Comparison of CYWS-3D, \textsc{C-NeRF}, and our method on different scenes. Red boxes are the
predictions and blue boxes are the groundtruths.} %\vspace{-10px}
  \label{fig:results3}
\end{figure}





\begin{figure}[!tb]
  \centering
  \includegraphics[width=\linewidth]{imgs/discussion.pdf}
  \caption{The CD results obtained by filtering Gaussian distributions with of $(\Delta x, \Delta r, \Delta s)$. The red arrow indicates the real change.}
  % \caption{The CD results obtained by filtering Gaussian distributions with $(\Delta x, \Delta r, \Delta s)$. The red arrow indicates the real change.} %\vspace{-10px}
  \label{fig:discussion}
\end{figure}


%---------------------------------------
\section{Conclusion}

In this paper, we present a pioneering approach to 3D instance-level change detection, addressing the challenge of identifying changes in real-world scenarios without relying on controlled conditions or labeled image pairs. Our method employs 4D Gaussian Splatting to embed two sets of images into Gaussian representations, enabling the rendering of coherent image sequences. By segmenting images and assigning unique identifiers to each instance, we efficiently detect changes through ID comparison. We partition the Gaussians into changed and unchanged using a classification encoding for each Gaussian, with change maps generated through rendering. Extensive experiments on various datasets show that our approach significantly outperforms methods like C-NeRF and CYWS-3D, especially under varying lighting conditions.


\newpage
\bibliographystyle{unsrt}   
\bibliography{main}          
\end{document}

% {
%     \small
%     \bibliographystyle{ieee_fullname}
%     \bibliography{egbib}
% }

% \vspace{12pt}
% \color{red}
% IEEE conference templates contain guidance text for composing and formatting conference papers. Please ensure that all template text is removed from your conference paper prior to submission to the conference. Failure to remove the template text from your paper may result in your paper not being published.

\end{document}
