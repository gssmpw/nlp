\section{Related Works}
\label{sec:2}

\begin{figure*}[ht]
  \centering
  \begin{subfigure}{0.3\linewidth}
    \includegraphics[height=3.9cm, width=\linewidth]{Sec3-Exp/fig-2-1.pdf}
    \caption*{(a)}
  \end{subfigure} \hfill
  \begin{subfigure}{0.3\linewidth}
    \includegraphics[height=3.9cm, width=\linewidth]{Sec3-Exp/fig-2-2.pdf}
    \caption*{(b)}
  \end{subfigure} \hfill
  \begin{subfigure}{0.35\linewidth}
    \includegraphics[height=3.9cm, width=\linewidth]{Sec3-Exp/fig-2-3.pdf}
    \caption*{(c)}
  \end{subfigure}
  \vspace*{-2.5mm}
  \caption{Next-token prediction loss on the clean OpenWebText validation set for GPT-2 models pre-trained on synthetic OpenWebText datasets with varying levels of random noise. (a) Trend of NTP loss as training proceeds. (b) Difference in NTP loss between the noisy and clean models after the same number of training iterations. (c) Difference in loss values after undergoing the same number of training iterations on clean OpenWebText data.}
  \label{fig:2}
  \vspace*{-4.5mm}
\end{figure*}

\textbf{Pre-training Data Analysis for Language Model Training.} ____ analyzed open-source datasets like The Pile and C4, uncovering significant amounts of low-quality content in these datasets. ____ highlighted the negative impact of such data on training. Despite these remarkable contributions, there remains a lack of understanding regarding the specific effects of random noise on language model performance. This paper aims to address this gap.

\textbf{Noisy Model Learning.} Our work draws significant inspiration from Noisy Model Learning (NML) proposed by ____. In NML, the authors introduce noise into large datasets like ImageNet by randomly altering labels, then pre-train neural networks on these noisy datasets. The study reveals that moderate label noise enhances in-distribution (ID) sample classification, while out-of-distribution (OOD) performance deteriorates with increasing noise. This paper extends the concept of NML, presenting theoretical insights and methodologies that are applicable across multiple modalities and various problems.

Due to space limitations, the detailed related works are provided in Appendix~\ref{app:relatedwork}.


\begin{figure*}[ht]
  \centering
  \begin{subfigure}{0.31\linewidth}
    \includegraphics[height=4cm, width=\linewidth]{Sec3-Exp/fig-2-4.pdf}
    \caption*{(a)}
  \end{subfigure} \hfill
  \begin{subfigure}{0.31\linewidth}
    \includegraphics[height=4cm, width=\linewidth]{Sec3-Exp/fig-2-5.pdf}
    \caption*{(b)}
  \end{subfigure} \hfill
  \begin{subfigure}{0.33\linewidth}
    \includegraphics[height=4cm, width=\linewidth]{Sec3-Exp/fig-2-7.pdf}
    \caption*{(c)}
  \end{subfigure}
  \vspace*{-2.5mm}
  \caption{Validation experiments.  (a) Loss trends on the random noise in the \textit{training set} of the model trained on the dataset with 5\% random noise. (b) Comparison of the loss between 5\% random noise and Gaussian noise. (c) The loss difference on the clean OpenWebText validation set compared to the baseline for models trained on datasets with 5\% random noise and 5\% Gaussian noise, respectively.}
  \label{fig:3}
  \vspace*{-4mm}
\end{figure*}