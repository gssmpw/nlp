% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").


@article{gpt2,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}

@article{gpt3,
  title={Language models are few-shot learners},
  author={Brown, Tom B},
  journal={arXiv preprint arXiv:2005.14165},
  year={2020}
}

@article{gpt4,
  title={Gpt-4 technical report},
  author={OpenAI},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@inproceedings{grok,
title={Progress measures for grokking via mechanistic interpretability},
author={Neel Nanda and Lawrence Chan and Tom Lieberum and Jess Smith and Jacob Steinhardt},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=9XFSbDPmdW}
}
@article{llama,
  title={Llama: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}

@article{qwen,
  title={Qwen technical report},
  author={Bai, Jinze and Bai, Shuai and Chu, Yunfei and Cui, Zeyu and Dang, Kai and Deng, Xiaodong and Fan, Yang and Ge, Wenbin and Han, Yu and Huang, Fei and others},
  journal={arXiv preprint arXiv:2309.16609},
  year={2023}
}

@inproceedings{doremi,
 author = {Xie, Sang Michael and Pham, Hieu and Dong, Xuanyi and Du, Nan and Liu, Hanxiao and Lu, Yifeng and Liang, Percy S and Le, Quoc V and Ma, Tengyu and Yu, Adams Wei},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
 pages = {69798--69818},
 publisher = {Curran Associates, Inc.},
 title = {DoReMi: Optimizing Data Mixtures Speeds Up Language Model Pretraining},
 url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/dcba6be91359358c2355cd920da3fcbd-Paper-Conference.pdf},
 volume = {36},
 year = {2023}
}


@inproceedings{pretrainguide,
    title = "A Pretrainer{'}s Guide to Training Data: Measuring the Effects of Data Age, Domain Coverage, Quality, {\&} Toxicity",
    author = "Longpre, Shayne  and
      Yauney, Gregory  and
      Reif, Emily  and
      Lee, Katherine  and
      Roberts, Adam  and
      Zoph, Barret  and
      Zhou, Denny  and
      Wei, Jason  and
      Robinson, Kevin  and
      Mimno, David  and
      Ippolito, Daphne",
    editor = "Duh, Kevin  and
      Gomez, Helena  and
      Bethard, Steven",
    booktitle = "Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)",
    month = jun,
    year = "2024",
    address = "Mexico City, Mexico",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.naacl-long.179",
    doi = "10.18653/v1/2024.naacl-long.179",
    pages = "3245--3276",
    abstract = "Pretraining data design is critically under-documented and often guided by empirically unsupported intuitions. We pretrain models on data curated (1) at different collection times, (2) with varying toxicity and quality filters, and (3) with different domain compositions. First, we find that temporal shift between evaluation data and pretraining data leads to performance degradation, which is not overcome by finetuning. Second, we measure the effect of quality and toxicity filters, showing a trade-off between performance on standard benchmarks and risk of toxic generations. We also find that the effects of different types of filtering are not predictable from text domain characteristics. Third, we empirically validate that heterogeneous data sources, like books and web, are beneficial and warrant greater prioritization. To date, these experiments constitute the single largest publicly documented empirical study of the effects of pretraining data. Spanning 28 unique 1.5 billion parameter models pretrained from scratch, these findings validate, quantify, and expose many undocumented intuitions about text pretraining, which ultimately support more informed data-centric decisions in model development.",
}

@inproceedings{wimbd,
title={What's In My Big Data?},
author={Yanai Elazar and Akshita Bhagia and Ian Helgi Magnusson and Abhilasha Ravichander and Dustin Schwenk and Alane Suhr and Evan Pete Walsh and Dirk Groeneveld and Luca Soldaini and Sameer Singh and Hannaneh Hajishirzi and Noah A. Smith and Jesse Dodge},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=RvfPnOkPV4}
}

@inproceedings{allenphysics,
title={Physics of Language Models: Part 3.1, Knowledge Storage and Extraction},
author={Zeyuan Allen-Zhu and Yuanzhi Li},
booktitle={Forty-first International Conference on Machine Learning},
year={2024},
url={https://openreview.net/forum?id=5x788rqbcj}
}

@inproceedings{xie2023data,
 author = {Xie, Sang Michael and Santurkar, Shibani and Ma, Tengyu and Liang, Percy S},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
 pages = {34201--34227},
 publisher = {Curran Associates, Inc.},
 title = {Data Selection for Language Models via Importance Resampling},
 url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/6b9aa8f418bde2840d5f4ab7a02f663b-Paper-Conference.pdf},
 volume = {36},
 year = {2023}
}


@article{naturemc,
  title={AI models collapse when trained on recursively generated data},
  author={Shumailov, Ilia and Shumaylov, Zakhar and Zhao, Yiren and Papernot, Nicolas and Anderson, Ross and Gal, Yarin},
  journal={Nature},
  volume={631},
  number={8022},
  pages={755--759},
  year={2024},
  publisher={Nature Publishing Group UK London}
}

@inproceedings{colmmc,
title={How bad is training on synthetic data? A statistical analysis of language model collapse},
author={Mohamed El Amine Seddik and Suei-Wen Chen and Soufiane Hayou and Pierre Youssef and Merouane Abdelkader DEBBAH},
booktitle={First Conference on Language Modeling},
year={2024},
url={https://openreview.net/forum?id=t3z6UlV09o}
}

@article{fairmc,
  title={Strong Model Collapse},
  author={Dohmatob, Elvis and Feng, Yunzhen and Kempe, Julia},
  journal={arXiv preprint arXiv:2410.04840},
  year={2024}
}


@inproceedings{captcha,
  title={A three-way investigation of a game-captcha: automated attacks, relay attacks and usability},
  author={Mohamed, Manar and Sachdeva, Niharika and Georgescu, Michael and Gao, Song and Saxena, Nitesh and Zhang, Chengcui and Kumaraguru, Ponnurangam and Van Oorschot, Paul C and Chen, Wei-Bang},
  booktitle={Proceedings of the 9th ACM symposium on Information, computer and communications security},
  pages={195--206},
  year={2014}
}

@article{crawldetect,
  title={Reinforcement learning based web crawler detection for diversity and dynamics},
  author={Gao, Yang and Feng, Zunlei and Wang, Xiaoyang and Song, Mingli and Wang, Xingen and Wang, Xinyu and Chen, Chun},
  journal={Neurocomputing},
  volume={520},
  pages={115--128},
  year={2023},
  publisher={Elsevier}
}

@article{webnoise,
  title={Leveraging Web-Crawled Data for High-Quality Fine-Tuning},
  author={Zhou, Jing and Jiang, Chenglin and Shen, Wei and Zhou, Xiao and He, Xiaonan},
  journal={arXiv preprint arXiv:2408.08003},
  year={2024}
}

@article{wavlm,
  title={Wavlm: Large-scale self-supervised pre-training for full stack speech processing},
  author={Chen, Sanyuan and Wang, Chengyi and Chen, Zhengyang and Wu, Yu and Liu, Shujie and Chen, Zhuo and Li, Jinyu and Kanda, Naoyuki and Yoshioka, Takuya and Xiao, Xiong and others},
  journal={IEEE Journal of Selected Topics in Signal Processing},
  volume={16},
  number={6},
  pages={1505--1518},
  year={2022},
  publisher={IEEE}
}

@inproceedings{webnoise1,
  title={Noise-aware learning from web-crawled image-text data for image captioning},
  author={Kang, Wooyoung and Mun, Jonghwan and Lee, Sungjun and Roh, Byungseok},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={2942--2952},
  year={2023}
}

@inproceedings{sharpness,
  title={Gradient norm aware minimization seeks first-order flatness and improves generalization},
  author={Zhang, Xingxuan and Xu, Renzhe and Yu, Han and Zou, Hao and Cui, Peng},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={20247--20257},
  year={2023}
}

@inproceedings{orphan,
  title={Out of Sight, Out of Mind: Detecting Orphaned Web Pages at Internet-Scale},
  author={Pletinckx, Stijn and Borgolte, Kevin and Fiebig, Tobias},
  booktitle={Proceedings of the 2021 ACM SIGSAC Conference on Computer and Communications Security},
  pages={21--35},
  year={2021}
}

@article{chomsky,
  title={Three models for the description of language},
  author={Chomsky, Noam},
  journal={IEEE Transactions on information theory},
  volume={2},
  number={3},
  pages={113--124},
  year={1956},
  publisher={IEEE}
}

@misc{openwebtext,
  title={Openwebtext corpus},
  author={Gokaslan, Aaron and Cohen, Vanya and Pavlick, Ellie and Tellex, Stefanie},
  year={2019}
}

@inproceedings{nml,
title={Understanding and Mitigating the Label Noise in Pre-training on Downstream Tasks},
author={Hao Chen and Jindong Wang and Ankit Shah and Ran Tao and Hongxin Wei and Xing Xie and Masashi Sugiyama and Bhiksha Raj},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=TjhUtloBZU}
}

@inproceedings{smoothness,
 author = {Srebro, Nathan and Sridharan, Karthik and Tewari, Ambuj},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Smoothness, Low Noise and Fast Rates},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/76cf99d3614e23eabab16fb27e944bf9-Paper.pdf},
 volume = {23},
 year = {2010}
}


@article{dssurvey,
title={A Survey on Data Selection for Language Models},
author={Alon Albalak and Yanai Elazar and Sang Michael Xie and Shayne Longpre and Nathan Lambert and Xinyi Wang and Niklas Muennighoff and Bairu Hou and Liangming Pan and Haewon Jeong and Colin Raffel and Shiyu Chang and Tatsunori Hashimoto and William Yang Wang},
journal={Transactions on Machine Learning Research},
issn={2835-8856},
year={2024},
url={https://openreview.net/forum?id=XfHWcNTSHp},
note={Survey Certification}
}

@inproceedings{dolma,
    title = "Dolma: an Open Corpus of Three Trillion Tokens for Language Model Pretraining Research",
    author = "Soldaini, Luca  and
      Kinney, Rodney  and
      Bhagia, Akshita  and
      Schwenk, Dustin  and
      Atkinson, David  and
      Authur, Russell  and
      Bogin, Ben  and
      Chandu, Khyathi  and
      Dumas, Jennifer  and
      Elazar, Yanai  and
      Hofmann, Valentin  and
      Jha, Ananya  and
      Kumar, Sachin  and
      Lucy, Li  and
      Lyu, Xinxi  and
      Lambert, Nathan  and
      Magnusson, Ian  and
      Morrison, Jacob  and
      Muennighoff, Niklas  and
      Naik, Aakanksha  and
      Nam, Crystal  and
      Peters, Matthew  and
      Ravichander, Abhilasha  and
      Richardson, Kyle  and
      Shen, Zejiang  and
      Strubell, Emma  and
      Subramani, Nishant  and
      Tafjord, Oyvind  and
      Walsh, Evan  and
      Zettlemoyer, Luke  and
      Smith, Noah  and
      Hajishirzi, Hannaneh  and
      Beltagy, Iz  and
      Groeneveld, Dirk  and
      Dodge, Jesse  and
      Lo, Kyle",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-long.840",
    doi = "10.18653/v1/2024.acl-long.840",
    pages = "15725--15788",
    abstract = "Information about pretraining corpora used to train the current best-performing language models is seldom discussed: commercial models rarely detail their data, and even open models are often released without accompanying training data or recipes to reproduce them. As a result, it is challenging to conduct and advance scientific research on language modeling, such as understanding how training data impacts model capabilities and limitations. To facilitate scientific research on language model pretraining, we curate and release Dolma, a three-trillion-token English corpus, built from a diverse mixture of web content, scientific papers, code, public-domain books, social media, and encyclopedic materials. We extensively document Dolma, including its design principles, details about its construction, and a summary of its contents. We present analyses and experimental results on intermediate states of Dolma to share what we have learned about important data curation practices. Finally, we open-source our data curation toolkit to enable reproduction of our work as well as support further research in large-scale data curation.",
}

@article{xmy,
  title={Parameterized algorithms and complexity for the traveling purchaser problem and its variants},
  author={Xiao, Mingyu and Zhang, Jianan and Lin, Weibo},
  journal={Journal of Combinatorial Optimization},
  pages={1--17},
  year={2022},
  publisher={Springer}
}

@inproceedings{quality_naacl,
    title = "From Quantity to Quality: Boosting {LLM} Performance with Self-Guided Data Selection for Instruction Tuning",
    author = "Li, Ming  and
      Zhang, Yong  and
      Li, Zhitao  and
      Chen, Jiuhai  and
      Chen, Lichang  and
      Cheng, Ning  and
      Wang, Jianzong  and
      Zhou, Tianyi  and
      Xiao, Jing",
    editor = "Duh, Kevin  and
      Gomez, Helena  and
      Bethard, Steven",
    booktitle = "Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)",
    month = jun,
    year = "2024",
    address = "Mexico City, Mexico",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.naacl-long.421",
    doi = "10.18653/v1/2024.naacl-long.421",
    pages = "7602--7635",
    abstract = "In the realm of Large Language Models (LLMs), the balance between instruction data quality and quantity is a focal point. Recognizing this, we introduce a self-guided methodology for LLMs to autonomously discern and select cherry samples from open-source datasets, effectively minimizing manual curation and potential cost for instruction tuning an LLM. Our key innovation, the Instruction-Following Difficulty (IFD) metric, emerges as a pivotal metric to identify discrepancies between a model{'}s expected responses and its intrinsic generation capability. Through the application of IFD, cherry samples can be pinpointed, leading to a marked uptick in model training efficiency. Empirical validations on datasets like Alpaca and WizardLM underpin our findings; with a mere 10{\%} of original data input, our strategy showcases improved results. This synthesis of self-guided cherry-picking and the IFD metric signifies a transformative leap in the instruction tuning of LLMs, promising both efficiency and resource-conscious advancements. Codes, data, and models are available.",
}

@inproceedings{gptinfluence,
    title = "On Training Data Influence of {GPT} Models",
    author = "Chai, Yekun  and
      Liu, Qingyi  and
      Wang, Shuohuan  and
      Sun, Yu  and
      Peng, Qiwei  and
      Wu, Hua",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.emnlp-main.183",
    doi = "10.18653/v1/2024.emnlp-main.183",
    pages = "3126--3150",
    abstract = "Amidst the rapid advancements in generative language models, the investigation of how training data shapes the performance of GPT models is still emerging. This paper presents GPTfluence, a novel approach that leverages a featurized simulation to assess the impact of training examples on the training dynamics of GPT models. Our approach not only traces the influence of individual training instances on performance trajectories, such as loss and other key metrics, on targeted test points but also enables a comprehensive comparison with existing methods across various training scenarios in GPT models, ranging from 14 million to 2.8 billion parameters, across a range of downstream tasks. Contrary to earlier methods that struggle with generalization to new data, GPTfluence introduces a parameterized simulation of training dynamics, demonstrating robust generalization capabilities to unseen training data. This adaptability is evident across both fine-tuning and instruction-tuning scenarios, spanning tasks in natural language understanding and generation. We make our code and data publicly available at https://github.com/ernie-research/gptfluence.",
}

@article{beyond,
  title={Beyond scale: the diversity coefficient as a data quality metric demonstrates llms are pre-trained on formally diverse data},
  author={Lee, Alycia and Miranda, Brando and Sundar, Sudharsan and Koyejo, Sanmi},
  journal={arXiv preprint arXiv:2306.13840},
  year={2023}
}

@inproceedings{mathlm,
title={A Mathematical Exploration of Why Language Models Help Solve Downstream Tasks},
author={Nikunj Saunshi and Sadhika Malladi and Sanjeev Arora},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=vVjIW3sEc1s}
}

@article{yyq,
  title={LLM-PCGC: Large Language Model-based Point Cloud Geometry Compression},
  author={Ye, Yuqi and Gao, Wei},
  journal={arXiv preprint arXiv:2408.08682},
  year={2024}
}

@ARTICLE{yb,
  author={Yang, Bang and Liu, Fenglin and Zou, Yuexian and Wu, Xian and Wang, Yaowei and Clifton, David A.},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={ZeroNLG: Aligning and Autoencoding Domains for Zero-Shot Multimodal and Multilingual Natural Language Generation}, 
  year={2024},
  volume={46},
  number={8},
  pages={5712-5724},
  keywords={Training;Task analysis;Videos;Machine translation;Visualization;Bridges;Decoding;Zero-shot learning;natural language generation;multimodal language generation;multilingual language generation;visual captioning;neural machine translation},
  doi={10.1109/TPAMI.2024.3371376}}

@article{rethink,
  title={Understanding deep learning (still) requires rethinking generalization},
  author={Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
  journal={Communications of the ACM},
  volume={64},
  number={3},
  pages={107--115},
  year={2021},
  publisher={ACM New York, NY, USA}
}

@inproceedings{whyhelp,
 author = {Wei, Colin and Xie, Sang Michael and Ma, Tengyu},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {16158--16170},
 publisher = {Curran Associates, Inc.},
 title = {Why Do Pretrained Language Models Help in Downstream Tasks? An Analysis of Head and Prompt Tuning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2021/file/86b3e165b8154656a71ffe8a327ded7d-Paper.pdf},
 volume = {34},
 year = {2021}
}

@book{gtm,
  title={Measure theory, probability, and stochastic processes},
  author={Le Gall, Jean-Fran{\c{c}}ois},
  year={2022},
  publisher={Springer}
}

@InProceedings{sameloss,
  title = 	 {Same Pre-training Loss, Better Downstream: Implicit Bias Matters for Language Models},
  author =       {Liu, Hong and Xie, Sang Michael and Li, Zhiyuan and Ma, Tengyu},
  booktitle = 	 {Proceedings of the 40th International Conference on Machine Learning},
  pages = 	 {22188--22214},
  year = 	 {2023},
  editor = 	 {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
  volume = 	 {202},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {23--29 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v202/liu23ao/liu23ao.pdf},
  url = 	 {https://proceedings.mlr.press/v202/liu23ao.html},
  abstract = 	 {Language modeling on large-scale datasets improves performance of various downstream tasks. The validation pre-training loss is often used as the evaluation metric for language models since the pre-training loss tends to be well-correlated with downstream performance (which is itself hard to evaluate comprehensively). Contrary to the conventional wisdom, this paper shows that 1) pre-training loss cannot fully explain downstream performance and 2) flatness of the model is well-correlated with downstream performance where pre-training loss is not. We identify three ways to produce models with the same pre-training loss but different downstream performance: continue pre-training after convergence, increasing the model size, and changing the pre-training algorithms. These experiments demonstrate the existence of implicit bias of pre-training algorithms—among models with the same minimal pre-training loss, they implicitly prefer more transferable ones. Toward understanding this implicit bias, we prove that SGD with standard mini-batch noise implicitly prefers flatter minima of pre-training loss in language models, and empirically observe a strong correlation between flatness (measured by the trace of Hessian) and downstream performance among models with the same pre-training loss. We also prove in a synthetic language setting that among models with the minimal pre-training loss, the flattest model transfers to downstream tasks.}
}

@article{ben-david,
  title={A theory of learning from different domains},
  author={Ben-David, Shai and Blitzer, John and Crammer, Koby and Kulesza, Alex and Pereira, Fernando and Vaughan, Jennifer Wortman},
  journal={Machine learning},
  volume={79},
  pages={151--175},
  year={2010},
  publisher={Springer}
}

@inproceedings{zkd1,
 author = {Du, Zhekai and Li, Jingjing},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
 pages = {17129--17155},
 publisher = {Curran Associates, Inc.},
 title = {Diffusion-Based Probabilistic Uncertainty Estimation for Active Domain Adaptation},
 url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/374050dc3f211267bd6bf0ea24eae184-Paper-Conference.pdf},
 volume = {36},
 year = {2023}
}

@ARTICLE{iosda,
  author={Ru, Jinghan and Tian, Jun and Xiao, Chengwei and Li, Jingjing and Shen, Heng Tao},
  journal={IEEE Transactions on Multimedia}, 
  title={Imbalanced Open Set Domain Adaptation via Moving-Threshold Estimation and Gradual Alignment}, 
  year={2024},
  volume={26},
  number={},
  pages={2504-2514},
  keywords={Estimation;Prototypes;Adaptation models;Target recognition;Feature extraction;Entropy;Humanities;Transfer learning;open set domain adaptation;imbalanced domain adaptation},
  doi={10.1109/TMM.2023.3297768}}

@ARTICLE{yzq,
  author={Li, Jingjing and Yu, Zhiqi and Du, Zhekai and Zhu, Lei and Shen, Heng Tao},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={A Comprehensive Survey on Source-Free Domain Adaptation}, 
  year={2024},
  volume={46},
  number={8},
  pages={5743-5762},
  keywords={Training;Surveys;Transfer learning;Adaptation models;Task analysis;Data models;Data privacy;Computer vision;data-free learning;domain adaptation;transfer learning},
  doi={10.1109/TPAMI.2024.3370978}}


@ARTICLE{kkw1,
  author={Wu, Kangkai and Li, Jingjing and Meng, Lichao and Li, Fengling and Lu, Ke},
  journal={IEEE Transactions on Industrial Informatics}, 
  title={Online Adaptive Fault Diagnosis With Test-Time Domain Adaptation}, 
  year={2024},
  volume={},
  number={},
  pages={1-11},
  keywords={Adaptation models;Data models;Predictive models;Noise;Fault diagnosis;Training;Real-time systems;Deep learning;intelligent fault diagnosis;teacher–student model;test-time adaptation;transfer learning},
  doi={10.1109/TII.2024.3438240}}


@article{mlc1,
  title={Cross-domain mutual information adversarial maximization},
  author={Meng, Lichao and Su, Hongzu and Lou, Chunwei and Li, Jingjing},
  journal={Engineering Applications of Artificial Intelligence},
  volume={110},
  pages={104665},
  year={2022},
  publisher={Elsevier}
}

@article{jjl1,
  title={Divergence-agnostic unsupervised domain adaptation by adversarial attacks},
  author={Li, Jingjing and Du, Zhekai and Zhu, Lei and Ding, Zhengming and Lu, Ke and Shen, Heng Tao},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume={44},
  number={11},
  pages={8196--8211},
  year={2021},
  publisher={IEEE}
}

@inproceedings{refinedweb,
 author = {Penedo, Guilherme and Malartic, Quentin and Hesslow, Daniel and Cojocaru, Ruxandra and Alobeidli, Hamza and Cappelli, Alessandro and Pannier, Baptiste and Almazrouei, Ebtesam and Launay, Julien},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
 pages = {79155--79172},
 publisher = {Curran Associates, Inc.},
 title = {The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data Only},
 url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/fa3ed726cc5073b9c31e3e49a807789c-Paper-Datasets_and_Benchmarks.pdf},
 volume = {36},
 year = {2023}
}

@article{ls1,
  title={Learning from noisy labels with deep neural networks: A survey},
  author={Song, Hwanjun and Kim, Minseok and Park, Dongmin and Shin, Yooju and Lee, Jae-Gil},
  journal={IEEE transactions on neural networks and learning systems},
  volume={34},
  number={11},
  pages={8135--8153},
  year={2022},
  publisher={IEEE}
}


@InProceedings{ls2,
  title = 	 {Does label smoothing mitigate label noise?},
  author =       {Lukasik, Michal and Bhojanapalli, Srinadh and Menon, Aditya and Kumar, Sanjiv},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {6448--6458},
  year = 	 {2020},
  editor = 	 {III, Hal Daumé and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/lukasik20a/lukasik20a.pdf},
  url = 	 {https://proceedings.mlr.press/v119/lukasik20a.html},
  abstract = 	 {Label smoothing is commonly used in training deep learning models, wherein one-hot training labels are mixed with uniform label vectors. Empirically, smoothing has been shown to improve both predictive performance and model calibration. In this paper, we study whether label smoothing is also effective as a means of coping with label noise. While label smoothing apparently amplifies this problem — being equivalent to injecting symmetric noise to the labels — we show how it relates to a general family of loss-correction techniques from the label noise literature. Building on this connection, we show that label smoothing is competitive with loss-correction under label noise. Further, we show that when distilling models from noisy data, label smoothing of the teacher is beneficial; this is in contrast to recent findings for noise-free problems, and sheds further light on settings where label smoothing is beneficial.}
}

@article{smoothgrad,
  title={Smoothgrad: removing noise by adding noise},
  author={Smilkov, Daniel and Thorat, Nikhil and Kim, Been and Vi{\'e}gas, Fernanda and Wattenberg, Martin},
  journal={arXiv preprint arXiv:1706.03825},
  year={2017}
}


@InProceedings{purenoise,
  title = 	 {Pure Noise to the Rescue of Insufficient Data: Improving Imbalanced Classification by Training on Random Noise Images},
  author =       {Zada, Shiran and Benou, Itay and Irani, Michal},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {25817--25833},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/zada22a/zada22a.pdf},
  url = 	 {https://proceedings.mlr.press/v162/zada22a.html},
  abstract = 	 {Despite remarkable progress on visual recognition tasks, deep neural-nets still struggle to generalize well when training data is scarce or highly imbalanced, rendering them extremely vulnerable to real-world examples. In this paper, we present a surprisingly simple yet highly effective method to mitigate this limitation: using pure noise images as additional training data. Unlike the common use of additive noise or adversarial noise for data augmentation, we propose an entirely different perspective by directly training on pure random noise images. We present a new Distribution-Aware Routing Batch Normalization layer (DAR-BN), which enables training on pure noise images in addition to natural images within the same network. This encourages generalization and suppresses overfitting. Our proposed method significantly improves imbalanced classification performance, obtaining state-of-the-art results on a large variety of long-tailed image classification datasets (CIFAR-10-LT, CIFAR-100-LT, ImageNet-LT, Places-LT, and CelebA-5). Furthermore, our method is extremely simple and easy to use as a general new augmentation tool (on top of existing augmentations), and can be incorporated in any training scheme. It does not require any specialized data generation or training procedures, thus keeping training fast and efficient.}
}

@inproceedings{oodfz,
 author = {Fang, Zhen and Li, Yixuan and Lu, Jie and Dong, Jiahua and Han, Bo and Liu, Feng},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
 pages = {37199--37213},
 publisher = {Curran Associates, Inc.},
 title = {Is Out-of-Distribution Detection Learnable?},
 url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/f0e91b1314fa5eabf1d7ef6d1561ecec-Paper-Conference.pdf},
 volume = {35},
 year = {2022}
}

@inproceedings{mlbert1,
    title = "How Multilingual is Multilingual {BERT}?",
    author = "Pires, Telmo  and
      Schlinger, Eva  and
      Garrette, Dan",
    editor = "Korhonen, Anna  and
      Traum, David  and
      M{\`a}rquez, Llu{\'\i}s",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1493",
    doi = "10.18653/v1/P19-1493",
    pages = "4996--5001",
    abstract = "In this paper, we show that Multilingual BERT (M-BERT), released by Devlin et al. (2018) as a single language model pre-trained from monolingual corpora in 104 languages, is surprisingly good at zero-shot cross-lingual model transfer, in which task-specific annotations in one language are used to fine-tune the model for evaluation in another language. To understand why, we present a large number of probing experiments, showing that transfer is possible even to languages in different scripts, that transfer works best between typologically similar languages, that monolingual corpora can train models for code-switching, and that the model can find translation pairs. From these results, we can conclude that M-BERT does create multilingual representations, but that these representations exhibit systematic deficiencies affecting certain language pairs.",
}


@InProceedings{gradnorm1,
  title = 	 {Penalizing Gradient Norm for Efficiently Improving Generalization in Deep Learning},
  author =       {Zhao, Yang and Zhang, Hao and Hu, Xiuyuan},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {26982--26992},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/zhao22i/zhao22i.pdf},
  url = 	 {https://proceedings.mlr.press/v162/zhao22i.html},
  abstract = 	 {How to train deep neural networks (DNNs) to generalize well is a central concern in deep learning, especially for severely overparameterized networks nowadays. In this paper, we propose an effective method to improve the model generalization by additionally penalizing the gradient norm of loss function during optimization. We demonstrate that confining the gradient norm of loss function could help lead the optimizers towards finding flat minima. We leverage the first-order approximation to efficiently implement the corresponding gradient to fit well in the gradient descent framework. In our experiments, we confirm that when using our methods, generalization performance of various models could be improved on different datasets. Also, we show that the recent sharpness-aware minimization method (Foretet al., 2021) is a special, but not the best, case of our method, where the best case of our method could give new state-of-art performance on these tasks. Code is available at https://github.com/zhaoyang-0204/gnp.}
}

@inproceedings{sharpness2,
title={How Sharpness-Aware Minimization Minimizes Sharpness?},
author={Kaiyue Wen and Tengyu Ma and Zhiyuan Li},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=5spDgWmpY6x}
}

@ARTICLE{noisebert1,
  author={Hua, Hang and Li, Xingjian and Dou, Dejing and Xu, Cheng-Zhong and Luo, Jiebo},
  journal={IEEE Transactions on Neural Networks and Learning Systems}, 
  title={Improving Pretrained Language Model Fine-Tuning With Noise Stability Regularization}, 
  year={2023},
  volume={},
  number={},
  pages={1-15},
  keywords={Stability analysis;Task analysis;Training;Transformers;Gaussian distribution;Standards;Optimization;Domain generalization;fine-tuning;in-domain generalization;pretrained language models (PLMs);regularization},
  doi={10.1109/TNNLS.2023.3330926}
}

@inproceedings{smart,
    title = "{SMART}: Robust and Efficient Fine-Tuning for Pre-trained Natural Language Models through Principled Regularized Optimization",
    author = "Jiang, Haoming  and
      He, Pengcheng  and
      Chen, Weizhu  and
      Liu, Xiaodong  and
      Gao, Jianfeng  and
      Zhao, Tuo",
    editor = "Jurafsky, Dan  and
      Chai, Joyce  and
      Schluter, Natalie  and
      Tetreault, Joel",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.197",
    doi = "10.18653/v1/2020.acl-main.197",
    pages = "2177--2190",
    abstract = "Transfer learning has fundamentally changed the landscape of natural language processing (NLP). Many state-of-the-art models are first pre-trained on a large text corpus and then fine-tuned on downstream tasks. However, due to limited data resources from downstream tasks and the extremely high complexity of pre-trained models, aggressive fine-tuning often causes the fine-tuned model to overfit the training data of downstream tasks and fail to generalize to unseen data. To address such an issue in a principled manner, we propose a new learning framework for robust and efficient fine-tuning for pre-trained models to attain better generalization performance. The proposed framework contains two important ingredients: 1. Smoothness-inducing regularization, which effectively manages the complexity of the model; 2. Bregman proximal point optimization, which is an instance of trust-region methods and can prevent aggressive updating. Our experiments show that the proposed framework achieves new state-of-the-art performance on a number of NLP tasks including GLUE, SNLI, SciTail and ANLI. Moreover, it also outperforms the state-of-the-art T5 model, which is the largest pre-trained model containing 11 billion parameters, on GLUE.",
}

@inproceedings{noisebert2,
    title = "Noise Stability Regularization for Improving {BERT} Fine-tuning",
    author = "Hua, Hang  and
      Li, Xingjian  and
      Dou, Dejing  and
      Xu, Chengzhong  and
      Luo, Jiebo",
    editor = "Toutanova, Kristina  and
      Rumshisky, Anna  and
      Zettlemoyer, Luke  and
      Hakkani-Tur, Dilek  and
      Beltagy, Iz  and
      Bethard, Steven  and
      Cotterell, Ryan  and
      Chakraborty, Tanmoy  and
      Zhou, Yichao",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.naacl-main.258",
    doi = "10.18653/v1/2021.naacl-main.258",
    pages = "3229--3241",
    abstract = "Fine-tuning pre-trained language models suchas BERT has become a common practice dom-inating leaderboards across various NLP tasks. Despite its recent success and wide adoption,this process is unstable when there are onlya small number of training samples available. The brittleness of this process is often reflectedby the sensitivity to random seeds. In this pa-per, we propose to tackle this problem basedon the noise stability property of deep nets,which is investigated in recent literature (Aroraet al., 2018; Sanyal et al., 2020). Specifically,we introduce a novel and effective regulariza-tion method to improve fine-tuning on NLPtasks, referred to asLayer-wiseNoiseStabilityRegularization (LNSR). We extend the theo-ries about adding noise to the input and provethat our method gives a stabler regularizationeffect. We provide supportive evidence by ex-perimentally confirming that well-performingmodels show a low sensitivity to noise andfine-tuning with LNSR exhibits clearly bet-ter generalizability and stability. Furthermore,our method also demonstrates advantages overother state-of-the-art algorithms including L2-SP (Li et al., 2018), Mixout (Lee et al., 2020)and SMART (Jiang et al., 20)",
}

@InProceedings{gradnoise0,
  title = 	 {Shape Matters: Understanding the Implicit Bias of the Noise Covariance},
  author =       {HaoChen, Jeff Z. and Wei, Colin and Lee, Jason and Ma, Tengyu},
  booktitle = 	 {Proceedings of Thirty Fourth Conference on Learning Theory},
  pages = 	 {2315--2357},
  year = 	 {2021},
  editor = 	 {Belkin, Mikhail and Kpotufe, Samory},
  volume = 	 {134},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {15--19 Aug},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v134/haochen21a/haochen21a.pdf},
  url = 	 {https://proceedings.mlr.press/v134/haochen21a.html},
  abstract = 	 {The noise in stochastic gradient descent (SGD) provides a crucial implicit regularization effect for training overparameterized models. Prior theoretical work largely focuses on spherical Gaussian noise, whereas empirical studies demonstrate the phenomenon that parameter-dependent noise — induced by mini-batches or label perturbation — is far more effective than Gaussian noise.  This paper theoretically characterizes this phenomenon on a quadratically-parameterized model introduced by Vaskevicius et al. and Woodworth et al.  We show that in an over-parameterized setting, SGD with label noise recovers the sparse ground-truth with an arbitrary initialization, whereas SGD with Gaussian noise or gradient descent overfits to dense solutions with large norms. Our analysis reveals that parameter-dependent noise introduces a bias towards local minima with smaller noise variance, whereas spherical Gaussian noise does not.}
}


@inproceedings{gradnorm2,
title={Implicit Gradient Regularization},
author={David Barrett and Benoit Dherin},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=3q5IqUrkcF}
}

@inproceedings{gradnoise1,
title={Stochastic Collapse: How Gradient Noise Attracts {SGD} Dynamics Towards Simpler Subnetworks},
author={Feng Chen and Daniel Kunin and Atsushi Yamamura and Surya Ganguli},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
year={2023},
url={https://openreview.net/forum?id=iFxWrxDekd}
}

@InProceedings{gradnoise2,
  title = 	 {Positive-Negative Momentum: Manipulating Stochastic Gradient Noise to Improve Generalization},
  author =       {Xie, Zeke and Yuan, Li and Zhu, Zhanxing and Sugiyama, Masashi},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {11448--11458},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/xie21h/xie21h.pdf},
  url = 	 {https://proceedings.mlr.press/v139/xie21h.html},
  abstract = 	 {It is well-known that stochastic gradient noise (SGN) acts as implicit regularization for deep learning and is essentially important for both optimization and generalization of deep networks. Some works attempted to artificially simulate SGN by injecting random noise to improve deep learning. However, it turned out that the injected simple random noise cannot work as well as SGN, which is anisotropic and parameter-dependent. For simulating SGN at low computational costs and without changing the learning rate or batch size, we propose the Positive-Negative Momentum (PNM) approach that is a powerful alternative to conventional Momentum in classic optimizers. The introduced PNM method maintains two approximate independent momentum terms. Then, we can control the magnitude of SGN explicitly by adjusting the momentum difference. We theoretically prove the convergence guarantee and the generalization advantage of PNM over Stochastic Gradient Descent (SGD). By incorporating PNM into the two conventional optimizers, SGD with Momentum and Adam, our extensive experiments empirically verified the significant advantage of the PNM-based variants over the corresponding conventional Momentum-based optimizers. Code: \url{https://github.com/zeke-xie/Positive-Negative-Momentum}.}
}


@inproceedings{mlbert2,
    title = "Finding Universal Grammatical Relations in Multilingual {BERT}",
    author = "Chi, Ethan A.  and
      Hewitt, John  and
      Manning, Christopher D.",
    editor = "Jurafsky, Dan  and
      Chai, Joyce  and
      Schluter, Natalie  and
      Tetreault, Joel",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.493",
    doi = "10.18653/v1/2020.acl-main.493",
    pages = "5564--5577",
    abstract = "Recent work has found evidence that Multilingual BERT (mBERT), a transformer-based multilingual masked language model, is capable of zero-shot cross-lingual transfer, suggesting that some aspects of its representations are shared cross-lingually. To better understand this overlap, we extend recent work on finding syntactic trees in neural networks{'} internal representations to the multilingual setting. We show that subspaces of mBERT representations recover syntactic tree distances in languages other than English, and that these subspaces are approximately shared across languages. Motivated by these results, we present an unsupervised analysis method that provides evidence mBERT learns representations of syntactic dependency labels, in the form of clusters which largely agree with the Universal Dependencies taxonomy. This evidence suggests that even without explicit supervision, multilingual masked language models learn certain linguistic universals.",
}

@inproceedings{yyk,
  title={AFL-Net: Integrating Audio, Facial, and Lip Modalities with a Two-step Cross-attention for Robust Speaker Diarization in the Wild},
  author={Yin, YongKang and Li, Xu and Shan, Ying and Zou, YueXian},
  booktitle={Proc. Interspeech 2024},
  pages={42--46},
  year={2024}
}

@inproceedings{giga,
  title={GigaSpeech: An evolving, multi-domain ASR corpus with 10,000 hours of transcribed audio},
  author={Chen, Guoguo and Chai, Shuzhou and Wang, Guanbo and Du, Jiayu and Zhang, Wei Qiang and Weng, Chao and Su, Dan and Povey, Daniel and Trmal, Jan and Zhang, Junbo and others},
  booktitle={22nd Annual Conference of the International Speech Communication Association, INTERSPEECH 2021},
  pages={4376--4380},
  year={2021},
  organization={International Speech Communication Association}
}

@inproceedings{unlabeled,
 author = {Zheng, Chenyu and Wu, Guoqiang and LI, Chongxuan},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
 pages = {54046--54060},
 publisher = {Curran Associates, Inc.},
 title = {Toward Understanding Generative Data Augmentation},
 url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/a94a8800a4b0af45600bab91164849df-Paper-Conference.pdf},
 volume = {36},
 year = {2023}
}

@inproceedings{sharpness0,
title={Sharpness-aware Minimization for Efficiently Improving Generalization},
author={Pierre Foret and Ariel Kleiner and Hossein Mobahi and Behnam Neyshabur},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=6Tm1mposlrM}
}

@article{adamw,
  title={Decoupled weight decay regularization},
  author={Loshchilov, I},
  journal={arXiv preprint arXiv:1711.05101},
  year={2017}
}


@InProceedings{adam1,
  title = 	 {Adaptive Inertia: Disentangling the Effects of Adaptive Learning Rate and Momentum},
  author =       {Xie, Zeke and Wang, Xinrui and Zhang, Huishuai and Sato, Issei and Sugiyama, Masashi},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {24430--24459},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/xie22d/xie22d.pdf},
  url = 	 {https://proceedings.mlr.press/v162/xie22d.html},
  abstract = 	 {Adaptive Moment Estimation (Adam), which combines Adaptive Learning Rate and Momentum, would be the most popular stochastic optimizer for accelerating the training of deep neural networks. However, it is empirically known that Adam often generalizes worse than Stochastic Gradient Descent (SGD). The purpose of this paper is to unveil the mystery of this behavior in the diffusion theoretical framework. Specifically, we disentangle the effects of Adaptive Learning Rate and Momentum of the Adam dynamics on saddle-point escaping and flat minima selection. We prove that Adaptive Learning Rate can escape saddle points efficiently, but cannot select flat minima as SGD does. In contrast, Momentum provides a drift effect to help the training process pass through saddle points, and almost does not affect flat minima selection. This partly explains why SGD (with Momentum) generalizes better, while Adam generalizes worse but converges faster. Furthermore, motivated by the analysis, we design a novel adaptive optimization framework named Adaptive Inertia, which uses parameter-wise adaptive inertia to accelerate the training and provably favors flat minima as well as SGD. Our extensive experiments demonstrate that the proposed adaptive inertia method can generalize significantly better than SGD and conventional adaptive gradient methods.}
}

@article{pile,
  title={The pile: An 800gb dataset of diverse text for language modeling},
  author={Gao, Leo and Biderman, Stella and Black, Sid and Golding, Laurence and Hoppe, Travis and Foster, Charles and Phang, Jason and He, Horace and Thite, Anish and Nabeshima, Noa and others},
  journal={arXiv preprint arXiv:2101.00027},
  year={2020}
}

@article{c4,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={Journal of machine learning research},
  volume={21},
  number={140},
  pages={1--67},
  year={2020}
}

@article{allen33,
  title={Physics of language models: Part 3.3, knowledge capacity scaling laws},
  author={Allen-Zhu, Zeyuan and Li, Yuanzhi},
  journal={arXiv preprint arXiv:2404.05405},
  year={2024}
}

@inproceedings{xzk1,
title={Dataset Pruning: Reducing Training Data by Examining Generalization Influence},
author={Shuo Yang and Zeke Xie and Hanyu Peng and Min Xu and Mingming Sun and Ping Li},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=4wZiAXD29TQ}
}

@article{gibberish,
  title={Talking Nonsense: Probing Large Language Models' Understanding of Adversarial Gibberish Inputs},
  author={Cherepanova, Valeriia and Zou, James},
  journal={arXiv preprint arXiv:2404.17120},
  year={2024}
}


@InProceedings{ptft3,
  title = 	 {Composed Fine-Tuning: Freezing Pre-Trained Denoising Autoencoders for Improved Generalization},
  author =       {Xie, Sang Michael and Ma, Tengyu and Liang, Percy},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {11424--11435},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/xie21f/xie21f.pdf},
  url = 	 {https://proceedings.mlr.press/v139/xie21f.html},
  abstract = 	 {We focus on prediction problems with structured outputs that are subject to output validity constraints, e.g. pseudocode-to-code translation where the code must compile. While labeled input-output pairs are expensive to obtain, "unlabeled" outputs, i.e. outputs without corresponding inputs, are freely available (e.g. code on GitHub) and provide information about output validity. Pre-training captures this structure by training a denoiser to denoise corrupted versions of unlabeled outputs. We first show that standard fine-tuning after pre-training destroys some of this structure. We then propose composed fine-tuning, which trains a predictor composed with the pre-trained denoiser. Importantly, the denoiser is fixed to preserve output structure. Like standard fine-tuning, the predictor is also initialized with the pre-trained denoiser. We prove for two-layer ReLU networks that composed fine-tuning significantly reduces the complexity of the predictor, thus improving generalization. Empirically, we show that composed fine-tuning improves over standard fine-tuning on two pseudocode-to-code translation datasets (3% and 6% relative). The improvement is magnified on out-of-distribution (OOD) examples (4% and 25% relative), suggesting that reducing predictor complexity improves OOD extrapolation.}
}


@InProceedings{ptft1,
author = {Kornblith, Simon and Shlens, Jonathon and Le, Quoc V.},
title = {Do Better ImageNet Models Transfer Better?},
booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2019}
}

@inproceedings{ptft2,
 author = {Raghu, Maithra and Zhang, Chiyuan and Kleinberg, Jon and Bengio, Samy},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Transfusion: Understanding Transfer Learning for Medical Imaging},
 url = {https://proceedings.neurips.cc/paper_files/paper/2019/file/eb1e78328c46506b46a4ac4a1e378b91-Paper.pdf},
 volume = {32},
 year = {2019}
}

@article{adapter,
  title={Tip-adapter: Training-free clip-adapter for better vision-language modeling},
  author={Zhang, Renrui and Fang, Rongyao and Zhang, Wei and Gao, Peng and Li, Kunchang and Dai, Jifeng and Qiao, Yu and Li, Hongsheng},
  journal={arXiv preprint arXiv:2111.03930},
  year={2021}
}

@article{coop,
  title={Learning to prompt for vision-language models},
  author={Zhou, Kaiyang and Yang, Jingkang and Loy, Chen Change and Liu, Ziwei},
  journal={International Journal of Computer Vision},
  volume={130},
  number={9},
  pages={2337--2348},
  year={2022},
  publisher={Springer}
}

@InProceedings{cocoop,
    author    = {Zhou, Kaiyang and Yang, Jingkang and Loy, Chen Change and Liu, Ziwei},
    title     = {Conditional Prompt Learning for Vision-Language Models},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2022},
    pages     = {16816-16825}
}


@InProceedings{clip,
  title = 	 {Learning Transferable Visual Models From Natural Language Supervision},
  author =       {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {8748--8763},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/radford21a/radford21a.pdf},
  url = 	 {https://proceedings.mlr.press/v139/radford21a.html},
  abstract = 	 {State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on.}
}

@inproceedings{lora,
title={Lo{RA}: Low-Rank Adaptation of Large Language Models},
author={Edward J Hu and yelong shen and Phillip Wallis and Zeyuan Allen-Zhu and Yuanzhi Li and Shean Wang and Lu Wang and Weizhu Chen},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=nZeVKeeFYf9}
}

@inproceedings{xzk2,
 author = {Xie, Zeke and Xu, Zhiqiang and Zhang, Jingzhao and Sato, Issei and Sugiyama, Masashi},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
 pages = {1208--1228},
 publisher = {Curran Associates, Inc.},
 title = {On the Overlooked Pitfalls of Weight Decay and How to Mitigate Them: A Gradient-Norm Perspective},
 url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/040d3b6af368bf71f952c18da5713b48-Paper-Conference.pdf},
 volume = {36},
 year = {2023}
}

@ARTICLE{zcy1,
  author={Ma, Ailong and Zheng, Chenyu and Wang, Junjue and Zhong, Yanfei},
  journal={IEEE Transactions on Geoscience and Remote Sensing}, 
  title={Domain Adaptive Land-Cover Classification via Local Consistency and Global Diversity}, 
  year={2023},
  volume={61},
  number={},
  pages={1-17},
  keywords={Task analysis;Training;Entropy;Remote sensing;Diversity reception;Semantic segmentation;Semantics;Consistency and diversity;high-resolution remote sensing (HRS) images;land-cover classification;unsupervised domain adaptation (UDA)},
  doi={10.1109/TGRS.2023.3265186}}


@InProceedings{zcy2,
  title = 	 {Revisiting Discriminative vs. Generative Classifiers: Theory and Implications},
  author =       {Zheng, Chenyu and Wu, Guoqiang and Bao, Fan and Cao, Yue and Li, Chongxuan and Zhu, Jun},
  booktitle = 	 {Proceedings of the 40th International Conference on Machine Learning},
  pages = 	 {42420--42477},
  year = 	 {2023},
  editor = 	 {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
  volume = 	 {202},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {23--29 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v202/zheng23f/zheng23f.pdf},
  url = 	 {https://proceedings.mlr.press/v202/zheng23f.html},
  abstract = 	 {A large-scale deep model pre-trained on massive labeled or unlabeled data transfers well to downstream tasks. Linear evaluation freezes parameters in the pre-trained model and trains a linear classifier separately, which is efficient and attractive for transfer. However, little work has investigated the classifier in linear evaluation except for the default logistic regression. Inspired by the statistical efficiency of naive Bayes, the paper revisits the classical topic on discriminative vs. generative classifiers. Theoretically, the paper considers the surrogate loss instead of the zero-one loss in analyses and generalizes the classical results from binary cases to multiclass ones. We show that, under mild assumptions, multiclass naive Bayes requires $O(\log n)$ samples to approach its asymptotic error while the corresponding multiclass logistic regression requires $O(n)$ samples, where $n$ is the feature dimension. To establish it, we present a multiclass $\mathcal{H}$-consistency bound framework and an explicit bound for logistic loss, which are of independent interests. Simulation results on a mixture of Gaussian validate our theoretical findings. Experiments on various pre-trained deep vision models show that naive Bayes consistently converges faster as the number of data increases. Besides, naive Bayes shows promise in few-shot cases and we observe the "two regimes” phenomenon in pre-trained supervised models. Our code is available at https://github.com/ML-GSAI/Revisiting-Dis-vs-Gen-Classifiers.}
}

@article{zcy3,
  title={On Mesa-Optimization in Autoregressively Trained Transformers: Emergence and Capability},
  author={Zheng, Chenyu and Huang, Wei and Wang, Rongzhen and Wu, Guoqiang and Zhu, Jun and Li, Chongxuan},
  journal={arXiv preprint arXiv:2405.16845},
  year={2024}
}

@inproceedings{llmjz1,
    title = "Structured Pruning of Large Language Models",
    author = "Wang, Ziheng  and
      Wohlwend, Jeremy  and
      Lei, Tao",
    editor = "Webber, Bonnie  and
      Cohn, Trevor  and
      He, Yulan  and
      Liu, Yang",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.496",
    doi = "10.18653/v1/2020.emnlp-main.496",
    pages = "6151--6162",
    abstract = "Large language models have recently achieved state of the art performance across a wide variety of natural language tasks. Meanwhile, the size of these models and their latency have significantly increased, which makes their usage costly, and raises an interesting question: do language models need to be large? We study this question through the lens of model compression. We present a generic, structured pruning approach by parameterizing each weight matrix using its low-rank factorization, and adaptively removing rank-1 components during training. On language modeling tasks, our structured approach outperforms other unstructured and block-structured pruning baselines at various compression levels, while achieving significant speedups during both training and inference. We also demonstrate that our method can be applied to pruning adaptive word embeddings in large language models, and to pruning the BERT model on several downstream fine-tuning classification benchmarks.",
}

@inproceedings{llmjz2,
    title = "The Optimal {BERT} Surgeon: Scalable and Accurate Second-Order Pruning for Large Language Models",
    author = "Kurtic, Eldar  and
      Campos, Daniel  and
      Nguyen, Tuan  and
      Frantar, Elias  and
      Kurtz, Mark  and
      Fineran, Benjamin  and
      Goin, Michael  and
      Alistarh, Dan",
    editor = "Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.279",
    doi = "10.18653/v1/2022.emnlp-main.279",
    pages = "4163--4181",
    abstract = "In this paper, we consider the problem of sparsifying BERT models, which are a key building block for natural language processing, in order to reduce their storage and computational cost. We introduce the Optimal BERT Surgeon (oBERT), an efficient and accurate pruning method based on approximate second-order information, which we show to yield state-of-the-art results in both stages of language tasks: pre-training and fine-tuning. Specifically, oBERT extends existing work on second-order pruning by allowing for pruning weight blocks, and is the first such method that is applicable at BERT scale. Second, we investigate compounding compression approaches to obtain highly compressed but accurate models for deployment on edge devices. These models significantly push boundaries of the current state-of-the-art sparse BERT models with respect to all metrics: model size, inference speed and task accuracy. For example, relative to the dense BERT-base, we obtain 10x model size compression with {\textless} 1{\%} accuracy drop, 10x CPU-inference speedup with {\textless} 2{\%} accuracy drop, and 29x CPU-inference speedup with {\textless} 7.5{\%} accuracy drop. Our code, fully integrated with Transformers and SparseML, is available at https://github.com/neuralmagic/sparseml/tree/main/research/optimal{\_}BERT{\_}surgeon{\_}oBERT.",
}

@inproceedings{llmjz3,
title={Plug-and-Play: An Efficient Post-training Pruning Method for Large Language Models},
author={Yingtao Zhang and Haoli Bai and Haokun Lin and Jialin Zhao and Lu Hou and Carlo Vittorio Cannistraci},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=Tr0lPx9woF}
}


@inproceedings{llmlh1,
    title = "{LRQ}uant: Learnable and Robust Post-Training Quantization for Large Language Models",
    author = "Zhao, Jiaqi  and
      Zhang, Miao  and
      Zeng, Chao  and
      Wang, Ming  and
      Liu, Xuebo  and
      Nie, Liqiang",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-long.122",
    doi = "10.18653/v1/2024.acl-long.122",
    pages = "2240--2255",
    abstract = "Post-training quantization (PTQ) for large language models (LLMs) significantly accelerates model inference and relieves memory constraints, without incurring model training. A {``}smoothing paradigm{''} is commonly used in LLM quantization, which transfers the quantization difficulty of activation to weight quantization using mathematically equivalent transformations. However, existing methods face two issues: 1) Most smoothing parameters are hand-crafted defined which leads to suboptimal results; 2) There are significant performance degradations when tested on unseen datasets. To address these challenges, this paper introduces a robust learnable smooth-based PTQ framework, called LRQuant. Firstly, we consider a learnable paradigm to find optimal smoothing parameters which are initialized by logarithmic activation equivalent. In addition, we empirically found that only relying on MSE loss could hardly lead to optimal quantization results, and we then propose a novel loss function based on the negative logarithm of cosine similarity (NLC loss) between outputs of full-precision and quantized block. At last, we pioneeringly introduce Test-time adaptation (TTA) into LLM quantization, which allows for rapid model adaptation during testing to improve generalization performance. More surprisingly, we find that by using our TTA method, we can achieve better results on test sets than directly using test sets for calibration in some cases while avoiding catastrophic forgetting. Codes are available at https://github.com/zjq0455/RLQ.",
}

@inproceedings{llmlh2,
    title = "A Comprehensive Evaluation of Quantization Strategies for Large Language Models",
    author = "Jin, Renren  and
      Du, Jiangcun  and
      Huang, Wuwei  and
      Liu, Wei  and
      Luan, Jian  and
      Wang, Bin  and
      Xiong, Deyi",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2024",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-acl.726",
    doi = "10.18653/v1/2024.findings-acl.726",
    pages = "12186--12215",
    abstract = "Increasing the number of parameters in large language models (LLMs) usually improves performance in downstream tasks but raises compute and memory costs, making deployment difficult in resource-limited settings. Quantization techniques, which reduce the bits needed for model weights or activations with minimal performance loss, have become popular due to the rise of LLMs. However, most quantization studies use pre-trained LLMs, and the impact of quantization on instruction-tuned LLMs and the relationship between perplexity and benchmark performance of quantized LLMs are not well understood. Evaluation of quantized LLMs is often limited to language modeling and a few classification tasks, leaving their performance on other benchmarks unclear. To address these gaps, we propose a structured evaluation framework consisting of three critical dimensions: (1) knowledge {\&} capacity, (2) alignment, and (3) efficiency, and conduct extensive experiments across ten diverse benchmarks. Our experimental results indicate that LLMs with 4-bit quantization can retain performance comparable to their non-quantized counterparts, and perplexity can serve as a proxy metric for quantized LLMs on most benchmarks. Furthermore, quantized LLMs with larger parameter scales can outperform smaller LLMs. Despite the memory savings achieved through quantization, it can also slow down the inference speed of LLMs. Consequently, substantial engineering efforts and hardware support are imperative to achieve a balanced optimization of decoding speed and memory consumption in the context of quantized LLMs.",
}

@inproceedings{llmlh3,
    title = "{I}ntact{KV}: Improving Large Language Model Quantization by Keeping Pivot Tokens Intact",
    author = "Liu, Ruikang  and
      Bai, Haoli  and
      Lin, Haokun  and
      Li, Yuening  and
      Gao, Han  and
      Xu, Zhengzhuo  and
      Hou, Lu  and
      Yao, Jun  and
      Yuan, Chun",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2024",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-acl.460",
    doi = "10.18653/v1/2024.findings-acl.460",
    pages = "7716--7741",
    abstract = "Large language models (LLMs) excel in natural language processing but demand intensive computation. To mitigate this, various quantization methods have been explored, yet they compromise LLM performance. This paper unveils a previously overlooked type of outliers in LLMs. Such outliers are found to allocate most of the attention scores on initial tokens of input, termed as pivot tokens, which are crucial to the performance of quantized LLMs. Given that, we propose IntactKV to generate the KV cache of pivot tokens losslessly from the full-precision model. The approach is simple and easy to combine with existing quantization solutions with no extra inference overhead. Besides, IntactKV can be calibrated as additional LLM parameters to boost the quantized LLMs further with minimal training costs. Mathematical analysis also proves that IntactKV effectively reduces the upper bound of quantization error. Empirical results show that IntactKV brings consistent improvement over various quantization methods across different LLMs and downstream tasks, leading to the new state-of-the-art for LLM quantization. The codes are available at https://github.com/ruikangliu/IntactKV.",
}

@article{zl2,
  title={Distilling the Knowledge in a Neural Network},
  author={Hinton, Geoffrey},
  journal={arXiv preprint arXiv:1503.02531},
  year={2015}
}

@article{mlbt1,
    title = "m{GPT}: Few-Shot Learners Go Multilingual",
    author = "Shliazhko, Oleh  and
      Fenogenova, Alena  and
      Tikhonova, Maria  and
      Kozlova, Anastasia  and
      Mikhailov, Vladislav  and
      Shavrina, Tatiana",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "12",
    year = "2024",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/2024.tacl-1.4",
    doi = "10.1162/tacl_a_00633",
    pages = "58--79",
    abstract = "This paper introduces mGPT, a multilingual variant of GPT-3, pretrained on 61 languages from 25 linguistically diverse language families using Wikipedia and the C4 Corpus. We detail the design and pretraining procedure. The models undergo an intrinsic and extrinsic evaluation: language modeling in all languages, downstream evaluation on cross-lingual NLU datasets and benchmarks in 33 languages, and world knowledge probing in 23 languages. The in-context learning abilities are on par with the contemporaneous language models while covering a larger number of languages, including underrepresented and low-resource languages of the Commonwealth of Independent States and the indigenous peoples in Russia. The source code and the language models are publicly available under the MIT license.",
}

@inproceedings{llmzl1,
    title = "Cost-effective Distillation of Large Language Models",
    author = "Dasgupta, Sayantan  and
      Cohn, Trevor  and
      Baldwin, Timothy",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2023",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-acl.463",
    doi = "10.18653/v1/2023.findings-acl.463",
    pages = "7346--7354",
    abstract = "Knowledge distillation (KD) involves training a small {``}student{''} model to replicate the strong performance of a high-capacity {``}teacher{''} model, enabling efficient deployment in resource-constrained settings. Top-performing methods tend to be task- or architecture-specific and lack generalizability. Several existing approaches require pretraining of the teacher on task-specific datasets, which can be costly for large and unstable for small datasets. Here we propose an approach for improving KD through a novel distillation loss agnostic to the task and model architecture. We successfully apply our method to the distillation of the BERT-base and achieve highly competitive results from the distilled student across a range of GLUE tasks, especially for tasks with smaller datasets.",
}

@inproceedings{flat1,
title={A Diffusion Theory For Deep Learning Dynamics: Stochastic Gradient Descent Exponentially Favors Flat Minima},
author={Zeke Xie and Issei Sato and Masashi Sugiyama},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=wXgk_iCiYGo}
}

@article{flat2,
  title = {Unveiling the Structure of Wide Flat Minima in Neural Networks},
  author = {Baldassi, Carlo and Lauditi, Clarissa and Malatesta, Enrico M. and Perugini, Gabriele and Zecchina, Riccardo},
  journal = {Phys. Rev. Lett.},
  volume = {127},
  issue = {27},
  pages = {278301},
  numpages = {6},
  year = {2021},
  month = {Dec},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevLett.127.278301},
  url = {https://link.aps.org/doi/10.1103/PhysRevLett.127.278301}
}

@article{llama3,
  title={The llama 3 herd of models},
  author={Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024}
}

@article{qwen2.5,
  title={Qwen2. 5-coder technical report},
  author={Hui, Binyuan and Yang, Jian and Cui, Zeyu and Yang, Jiaxi and Liu, Dayiheng and Zhang, Lei and Liu, Tianyu and Zhang, Jiajun and Yu, Bowen and Lu, Keming and others},
  journal={arXiv preprint arXiv:2409.12186},
  year={2024}
}

@inproceedings{sst2,
    title = "Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank",
    author = "Socher, Richard  and
      Perelygin, Alex  and
      Wu, Jean  and
      Chuang, Jason  and
      Manning, Christopher D.  and
      Ng, Andrew  and
      Potts, Christopher",
    editor = "Yarowsky, David  and
      Baldwin, Timothy  and
      Korhonen, Anna  and
      Livescu, Karen  and
      Bethard, Steven",
    booktitle = "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing",
    month = oct,
    year = "2013",
    address = "Seattle, Washington, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D13-1170",
    pages = "1631--1642",
}

@inproceedings{sstfine,
    title = "A Fast and Accurate Dependency Parser using Neural Networks",
    author = "Chen, Danqi  and
      Manning, Christopher",
    editor = "Moschitti, Alessandro  and
      Pang, Bo  and
      Daelemans, Walter",
    booktitle = "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP})",
    month = oct,
    year = "2014",
    address = "Doha, Qatar",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D14-1082",
    doi = "10.3115/v1/D14-1082",
    pages = "740--750",
}

@inproceedings{20ng,
    title = "Mitigating Uncertainty in Document Classification",
    author = "Zhang, Xuchao  and
      Chen, Fanglan  and
      Lu, Chang-Tien  and
      Ramakrishnan, Naren",
    editor = "Burstein, Jill  and
      Doran, Christy  and
      Solorio, Thamar",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1316",
    doi = "10.18653/v1/N19-1316",
    pages = "3126--3136",
    abstract = "The uncertainty measurement of classifiers{'} predictions is especially important in applications such as medical diagnoses that need to ensure limited human resources can focus on the most uncertain predictions returned by machine learning models. However, few existing uncertainty models attempt to improve overall prediction accuracy where human resources are involved in the text classification task. In this paper, we propose a novel neural-network-based model that applies a new dropout-entropy method for uncertainty measurement. We also design a metric learning method on feature representations, which can boost the performance of dropout-based uncertainty methods with smaller prediction variance in accurate prediction trials. Extensive experiments on real-world data sets demonstrate that our method can achieve a considerable improvement in overall prediction accuracy compared to existing approaches. In particular, our model improved the accuracy from 0.78 to 0.92 when 30{\%} of the most uncertain predictions were handed over to human experts in {``}20NewsGroup{''} data.",
}

@inproceedings{CR,
  title={Mining and summarizing customer reviews},
  author={Hu, Minqing and Liu, Bing},
  booktitle={Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining},
  pages={168--177},
  year={2004}
}

@article{bbc,
  title={News sentiment analysis},
  author={Samuels, Antony and Mcgonical, John},
  journal={arXiv preprint arXiv:2007.02238},
  year={2020}
}

@article{bcopa,
  title={Balanced COPA: Countering superficial cues in causal reasoning},
  author={Kavumba, Pride and Inoue, Naoya and Heinzerling, Benjamin and Singh, Keshav and Reisert, Paul and Inui, Kentarou},
  journal={Association for Natural Language Processing},
  pages={1105--1108},
  year={2020}
}

@inproceedings{mrpc,
  title={Automatically constructing a corpus of sentential paraphrases},
  author={Dolan, Bill and Brockett, Chris},
  booktitle={Third international workshop on paraphrasing (IWP2005)},
  year={2005}
}

@inproceedings{wic,
    title = "{W}i{C}: the Word-in-Context Dataset for Evaluating Context-Sensitive Meaning Representations",
    author = "Pilehvar, Mohammad Taher  and
      Camacho-Collados, Jose",
    editor = "Burstein, Jill  and
      Doran, Christy  and
      Solorio, Thamar",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1128",
    doi = "10.18653/v1/N19-1128",
    pages = "1267--1273",
    abstract = "By design, word embeddings are unable to model the dynamic nature of words{'} semantics, i.e., the property of words to correspond to potentially different meanings. To address this limitation, dozens of specialized meaning representation techniques such as sense or contextualized embeddings have been proposed. However, despite the popularity of research on this topic, very few evaluation benchmarks exist that specifically focus on the dynamic semantics of words. In this paper we show that existing models have surpassed the performance ceiling of the standard evaluation dataset for the purpose, i.e., Stanford Contextual Word Similarity, and highlight its shortcomings. To address the lack of a suitable benchmark, we put forward a large-scale Word in Context dataset, called WiC, based on annotations curated by experts, for generic evaluation of context-sensitive representations. WiC is released in \url{https://pilehvar.github.io/wic/}.",
}

@article{ilya,
  title={Sequence to Sequence Learning with Neural Networks},
  author={Sutskever, I},
  journal={arXiv preprint arXiv:1409.3215},
  year={2014}
}

@article{c10,
  title={Learning multiple layers of features from tiny images},
  author={Krizhevsky, Alex and Hinton, Geoffrey and others},
  year={2009},
  publisher={Toronto, ON, Canada}
}

@inproceedings{f102,
  title={Automated flower classification over a large number of classes},
  author={Nilsback, Maria-Elena and Zisserman, Andrew},
  booktitle={2008 Sixth Indian conference on computer vision, graphics \& image processing},
  pages={722--729},
  year={2008},
  organization={IEEE}
}

@inproceedings{f101,
  title={Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories},
  author={Fei-Fei, Li and Fergus, Rob and Perona, Pietro},
  booktitle={2004 conference on computer vision and pattern recognition workshop},
  pages={178--178},
  year={2004},
  organization={IEEE}
}

@inproceedings{pet,
  title={Cats and dogs},
  author={Parkhi, Omkar M and Vedaldi, Andrea and Zisserman, Andrew and Jawahar, CV},
  booktitle={2012 IEEE conference on computer vision and pattern recognition},
  pages={3498--3505},
  year={2012},
  organization={IEEE}
}

@inproceedings{car,
  title={3d object representations for fine-grained categorization},
  author={Krause, Jonathan and Stark, Michael and Deng, Jia and Fei-Fei, Li},
  booktitle={Proceedings of the IEEE international conference on computer vision workshops},
  pages={554--561},
  year={2013}
}

@article{fgvc,
  title={Fine-grained visual classification of aircraft},
  author={Maji, Subhransu and Rahtu, Esa and Kannala, Juho and Blaschko, Matthew and Vedaldi, Andrea},
  journal={arXiv preprint arXiv:1306.5151},
  year={2013}
}

@inproceedings{svhn,
  title={Reading digits in natural images with unsupervised feature learning},
  author={Netzer, Yuval and Wang, Tao and Coates, Adam and Bissacco, Alessandro and Wu, Baolin and Ng, Andrew Y and others},
  booktitle={NIPS workshop on deep learning and unsupervised feature learning},
  volume={2011},
  number={2},
  pages={4},
  year={2011},
  organization={Granada}
}

@inproceedings{dtd,
  title={Describing textures in the wild},
  author={Cimpoi, Mircea and Maji, Subhransu and Kokkinos, Iasonas and Mohamed, Sammy and Vedaldi, Andrea},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={3606--3613},
  year={2014}
}

@inproceedings{c101,
  title={Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories},
  author={Fei-Fei, Li and Fergus, Rob and Perona, Pietro},
  booktitle={2004 conference on computer vision and pattern recognition workshop},
  pages={178--178},
  year={2004},
  organization={IEEE}
}

@article{sat,
  title={Eurosat: A novel dataset and deep learning benchmark for land use and land cover classification},
  author={Helber, Patrick and Bischke, Benjamin and Dengel, Andreas and Borth, Damian},
  journal={IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing},
  volume={12},
  number={7},
  pages={2217--2226},
  year={2019},
  publisher={IEEE}
}

@inproceedings{pcam,
  title={Rotation equivariant CNNs for digital pathology},
  author={Veeling, Bastiaan S and Linmans, Jasper and Winkens, Jim and Cohen, Taco and Welling, Max},
  booktitle={Medical Image Computing and Computer Assisted Intervention--MICCAI 2018: 21st International Conference, Granada, Spain, September 16-20, 2018, Proceedings, Part II 11},
  pages={210--218},
  year={2018},
  organization={Springer}
}

@article{r45,
  title={Remote sensing image scene classification: Benchmark and state of the art},
  author={Cheng, Gong and Han, Junwei and Lu, Xiaoqiang},
  journal={Proceedings of the IEEE},
  volume={105},
  number={10},
  pages={1865--1883},
  year={2017},
  publisher={IEEE}
}


@InProceedings{efn,
  title = 	 {{E}fficient{N}et: Rethinking Model Scaling for Convolutional Neural Networks},
  author =       {Tan, Mingxing and Le, Quoc},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {6105--6114},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/tan19a/tan19a.pdf},
  url = 	 {https://proceedings.mlr.press/v97/tan19a.html},
  abstract = 	 {Convolutional Neural Networks (ConvNets) are commonly developed at a fixed resource budget, and then scaled up for better accuracy if more resources are given. In this paper, we systematically study model scaling and identify that carefully balancing network depth, width, and resolution can lead to better performance. Based on this observation, we propose a new scaling method that uniformly scales all dimensions of depth/width/resolution using a simple yet highly effective compound coefficient. We demonstrate the effectiveness of this method on MobileNets and ResNet. To go even further, we use neural architecture search to design a new baseline network and scale it up to obtain a family of models, called EfficientNets, which achieve much better accuracy and efficiency than previous ConvNets. In particular, our EfficientNet-B7 achieves stateof-the-art 84.4% top-1 / 97.1% top-5 accuracy on ImageNet, while being 8.4x smaller and 6.1x faster on inference than the best existing ConvNet (Huang et al., 2018). Our EfficientNets also transfer well and achieve state-of-the-art accuracy on CIFAR-100 (91.7%), Flower (98.8%), and 3 other transfer learning datasets, with an order of magnitude fewer parameters.}
}

@inproceedings{resnet,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}

@inproceedings{swinl,
  title={Swin transformer: Hierarchical vision transformer using shifted windows},
  author={Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={10012--10022},
  year={2021}
}

@article{vit,
  title={An image is worth 16x16 words: Transformers for image recognition at scale},
  author={Dosovitskiy, Alexey},
  journal={arXiv preprint arXiv:2010.11929},
  year={2020}
}

@inproceedings{conv,
  title={Convnext v2: Co-designing and scaling convnets with masked autoencoders},
  author={Woo, Sanghyun and Debnath, Shoubhik and Hu, Ronghang and Chen, Xinlei and Liu, Zhuang and Kweon, In So and Xie, Saining},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={16133--16142},
  year={2023}
}

@inproceedings{j300m,
  title={Revisiting unreasonable effectiveness of data in deep learning era},
  author={Sun, Chen and Shrivastava, Abhinav and Singh, Saurabh and Gupta, Abhinav},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={843--852},
  year={2017}
}

@inproceedings{in1k,
  title={Imagenet: A large-scale hierarchical image database},
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  booktitle={2009 IEEE conference on computer vision and pattern recognition},
  pages={248--255},
  year={2009},
  organization={Ieee}
}

@inproceedings{in21k,
title={ImageNet-21K Pretraining for the Masses},
author={Tal Ridnik and Emanuel Ben-Baruch and Asaf Noy and Lihi Zelnik-Manor},
booktitle={Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 1)},
year={2021},
url={https://openreview.net/forum?id=Zkj_VcZ6ol}
}

@inproceedings{laion,
 author = {Schuhmann, Christoph and Beaumont, Romain and Vencu, Richard and Gordon, Cade and Wightman, Ross and Cherti, Mehdi and Coombes, Theo and Katta, Aarush and Mullis, Clayton and Wortsman, Mitchell and Schramowski, Patrick and Kundurthy, Srivatsa and Crowson, Katherine and Schmidt, Ludwig and Kaczmarczyk, Robert and Jitsev, Jenia},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
 pages = {25278--25294},
 publisher = {Curran Associates, Inc.},
 title = {LAION-5B: An open large-scale dataset for training next generation image-text models},
 url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/a1859debfb3b59d094f3504d5ebb6c25-Paper-Datasets_and_Benchmarks.pdf},
 volume = {35},
 year = {2022}
}

@article{vargpt,
  title={VARGPT: Unified Understanding and Generation in a Visual Autoregressive Multimodal Large Language Model},
  author={Zhuang, Xianwei and Xie, Yuxin and Deng, Yufan and Liang, Liming and Ru, Jinghan and Yin, Yuguo and Zou, Yuexian},
  journal={arXiv preprint arXiv:2501.12327},
  year={2025}
}

@article{uniaudio,
  title={Uniaudio: An audio foundation model toward universal audio generation},
  author={Yang, Dongchao and Tian, Jinchuan and Tan, Xu and Huang, Rongjie and Liu, Songxiang and Chang, Xuankai and Shi, Jiatong and Zhao, Sheng and Bian, Jiang and Wu, Xixin and others},
  journal={arXiv preprint arXiv:2310.00704},
  year={2023}
}

@ARTICLE{ydc1,
  author={Yang, Dongchao and Liu, Songxiang and Huang, Rongjie and Weng, Chao and Meng, Helen},
  journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing}, 
  title={InstructTTS: Modelling Expressive TTS in Discrete Latent Space With Natural Language Style Prompt}, 
  year={2024},
  volume={32},
  number={},
  pages={2913-2925},
  keywords={Acoustics;Speech;Natural languages;Training;Semantics;Representation learning;Feature extraction;Text to speech;prompt-based learning;diffusion model;metric learning},
  doi={10.1109/TASLP.2024.3402088}}

@article{zxw1, title={Towards Explainable Joint Models via Information Theory for Multiple Intent Detection and Slot Filling}, volume={38}, url={https://ojs.aaai.org/index.php/AAAI/article/view/29953}, DOI={10.1609/aaai.v38i17.29953}, abstractNote={Recent joint models for multi-intent detection and slot filling have obtained promising results through modeling the unidirectional or bidirectional guidance between intent and slot. However, existing works design joint models heuristically and lack some theoretical exploration, including (1) theoretical measurement of the joint-interaction quality; (2) explainability of design and optimization methods of joint models, which may limit the performance and efficiency of designs. In this paper, we mathematically define the cross-task information gain (CIG) to measure the quality of joint processes from an information-theoretic perspective and discover an implicit optimization of CIG in previous models. Based on this, we propose a novel multi-stage iterative framework with theoretical effectiveness, explainability, and convergence, which can explicitly optimize information for cross-task interactions. Further, we devise an information-based joint model (InfoJoint) that conforms to this theoretical framework to gradually reduce the cross-task propagation of erroneous semantics through CIG iterative maximization. Extensive experiment results on two public datasets show that InfoJoint outperforms the state-of-the-art models by a large margin.}, number={17}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Zhuang, Xianwei and Cheng, Xuxin and Zou, Yuexian}, year={2024}, month={Mar.}, pages={19786-19794} }

@inproceedings{xyx1,
  title={GPA: global and prototype alignment for audio-text retrieval},
  author={Xie, Yuxin and Zhu, Zhihong and Zhuang, Xianwei and Liang, Liming and Wang, Zhichang and Zou, Yuexian},
  booktitle={Proc. Interspeech 2024},
  pages={5078--5082},
  year={2024}
}

@inproceedings{xyx2,
  title={Preparing Lessons for Progressive Training on Language Models},
  author={Pan, Yu and Yuan, Ye and Yin, Yichun and Shi, Jiaxin and Xu, Zenglin and Zhang, Ming and Shang, Lifeng and Jiang, Xin and Liu, Qun},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={38},
  number={17},
  pages={18860--18868},
  year={2024}
}

@article{xyx3,
  title={Reusing pretrained models by multi-linear operators for efficient training},
  author={Pan, Yu and Yuan, Ye and Yin, Yichun and Xu, Zenglin and Shang, Lifeng and Jiang, Xin and Liu, Qun},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={3248--3262},
  year={2023}
}

@inproceedings{zxw2,
  title={Kdpror: A knowledge-decoupling probabilistic framework for video-text retrieval},
  author={Zhuang, Xianwei and Li, Hongxiang and Cheng, Xuxin and Zhu, Zhihong and Xie, Yuxin and Zou, Yuexian},
  booktitle={European Conference on Computer Vision},
  pages={313--331},
  year={2024},
  organization={Springer}
}

@inproceedings{zxw3,
  title={PCAD: Towards ASR-robust spoken language understanding via prototype calibration and asymmetric decoupling},
  author={Zhuang, Xianwei and Cheng, Xuxin and Liang, Liming and Xie, Yuxin and Wang, Zhichang and Huang, Zhiqi and Zou, Yuexian},
  booktitle={Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={5235--5246},
  year={2024}
}

@inproceedings{zxw4,
  title={MaCSC: Towards Multimodal-augmented Pre-trained Language Models via Conceptual Prototypes and Self-balancing Calibration},
  author={Zhuang, Xianwei and Wang, Zhichang and Cheng, Xuxin and Xie, Yuxin and Liang, Liming and Zou, Yuexian},
  booktitle={Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)},
  pages={8070--8083},
  year={2024}
}

@article{zxw5,
  title={SemiGMMPoint: Semi-supervised point cloud segmentation based on Gaussian mixture models},
  author={Zhuang, Xianwei and Wang, Hualiang and He, Xiaoxuan and Fu, Siming and Hu, Haoji},
  journal={Pattern Recognition},
  volume={158},
  pages={111045},
  year={2025},
  publisher={Elsevier}
}

@inproceedings{zxw6,
  title={Game on Tree: Visual Hallucination Mitigation via Coarse-to-Fine View Tree and Game Theory},
  author={Zhuang, Xianwei and Zhu, Zhihong and Chen, Zhanpeng and Xie, Yuxin and Liang, Liming and Zou, Yuexian},
  booktitle={Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing},
  pages={17984--18003},
  year={2024}
}

@inproceedings{zxw7,
  title={Towards Multimodal-augmented Pre-trained Language Models via Self-balanced Expectation-Maximization Iteration},
  author={Zhuang, Xianwei and Cheng, Xuxin and Zhu, Zhihong and Chen, Zhanpeng and Li, Hongxiang and Zou, Yuexian},
  booktitle={Proceedings of the 32nd ACM International Conference on Multimedia},
  pages={4670--4679},
  year={2024}
}

@article{zxw8,
  title={VASparse: Towards Efficient Visual Hallucination Mitigation for Large Vision-Language Model via Visual-Aware Sparsification},
  author={Zhuang, Xianwei and Zhu, Zhihong and Xie, Yuxin and Liang, Liming and Zou, Yuexian},
  journal={arXiv preprint arXiv:2501.06553},
  year={2025}
}

@article{ydc2,
  title={Diffsound: Discrete diffusion model for text-to-sound generation},
  author={Yang, Dongchao and Yu, Jianwei and Wang, Helin and Wang, Wen and Weng, Chao and Zou, Yuexian and Yu, Dong},
  journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume={31},
  pages={1720--1733},
  year={2023},
  publisher={IEEE}
}

@article{ydc3,
  title={Hifi-codec: Group-residual vector quantization for high fidelity audio codec},
  author={Yang, Dongchao and Liu, Songxiang and Huang, Rongjie and Tian, Jinchuan and Weng, Chao and Zou, Yuexian},
  journal={arXiv preprint arXiv:2305.02765},
  year={2023}
}

@article{ydc4,
  title={UniAudio 1.5: Large Language Model-driven Audio Codec is A Few-shot Audio Task Learner},
  author={Yang, Dongchao and Guo, Haohan and Wang, Yuanyuan and Huang, Rongjie and Li, Xiang and Tan, Xu and Wu, Xixin and Meng, Helen},
  journal={arXiv preprint arXiv:2406.10056},
  year={2024}
}

@article{yyg1,
  title={Fedzkp: Federated model ownership verification with zero-knowledge proof},
  author={Yang, Wenyuan and Yin, Yuguo and Zhu, Gongxi and Gu, Hanlin and Fan, Lixin and Cao, Xiaochun and Yang, Qiang},
  journal={arXiv preprint arXiv:2305.04507},
  year={2023}
}

@article{yyg2,
  title={FedSOV: Federated Model Secure Ownership Verification with Unforgeable Signature},
  author={Yang, Wenyuan and Zhu, Gongxi and Yin, Yuguo and Gu, Hanlin and Fan, Lixin and Yang, Qiang and Cao, Xiaochun},
  journal={arXiv preprint arXiv:2305.06085},
  year={2023}
}

@article{wzy1,
  title={Do as We Do, Not as You Think: the Conformity of Large Language Models},
  author={Weng, Zhiyuan and Chen, Guikun and Wang, Wenguan},
  journal={arXiv preprint arXiv:2501.13381},
  year={2025}
}

@inproceedings{cmr1,
  title={Rmt: Retentive networks meet vision transformers},
  author={Fan, Qihang and Huang, Huaibo and Chen, Mingrui and Liu, Hongmin and He, Ran},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={5641--5651},
  year={2024}
}

@article{cmr2,
  title={Semantic Equitable Clustering: A Simple, Fast and Effective Strategy for Vision Transformer},
  author={Fan, Qihang and Huang, Huaibo and Chen, Mingrui and He, Ran},
  journal={arXiv preprint arXiv:2405.13337},
  year={2024}
}

@article{cmr3,
  title={Vision Transformer with Sparse Scan Prior},
  author={Fan, Qihang and Huang, Huaibo and Chen, Mingrui and He, Ran},
  journal={arXiv preprint arXiv:2405.13335},
  year={2024}
}

@inproceedings{cmr4,
  title={Not All Texts Are the Same: Dynamically Querying Texts for Scene Text Detection},
  author={Tang, Linjie and Yi, Pengfei and Chen, Mingrui and Yang, MingKun and Liang, Dingkang},
  booktitle={Chinese Conference on Pattern Recognition and Computer Vision (PRCV)},
  pages={363--377},
  year={2024},
  organization={Springer}
}
