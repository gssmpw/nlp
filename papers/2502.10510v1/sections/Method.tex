\section{MixMin}
\label{sec:method}

\begin{algorithm}[t]
\caption{\method}
\label{alg:mixmin}
\begin{flushleft}
\textbf{Require:} 
Step size $\eta$, number of steps $n$, loss function $\mathcal{L}$ (either cross-entropy or $\ell_2^2$), samples $D_t$ from the target distribution $dt$, and (cheap) models trained on each source $\hat{f_p}(x)~\forall~dp \in P$. 
\\
\textbf{Initialize:} $\lambda_p \gets \frac{1}{|P|}$ for all $dp \in P$, and pre-compute $\{\hat{f_p}(x)~\forall~x \in D_t, dp \in P\}$
\end{flushleft}

\begin{algorithmic}[1]
\FOR{$i = 1, \ldots, n$}
    \STATE $\hat{f_{\lambda}}(x) \gets \sum_{dp \in P} \lambda_p \hat{f_p}(x)$
    \STATE $l \gets  \frac{\lambda_p}{|D_t|} \sum_{(x,y) \in D_t} \mathcal{L}(\hat{f_{\lambda}}(x),y)$
    \STATE $g \gets \nabla_{\lambda}l$
    \STATE $\lambda_p \gets \frac{\lambda_p e^{\eta g_p}}{\sum_{dp \in P} \lambda_p e^{\eta g_p}}$ for all $dp \in P$ 
\ENDFOR

\textbf{Return}{$\{\lambda_p\}_{dp \in P}$}
\end{algorithmic}
\end{algorithm}

We now consider how to optimize the objective in Equation~\ref{eq:MixMin}. If we had a gradient oracle, we could optimize the objective using entropic descent~\citep{duchi2018introductory}. However, the objective (and gradient) is typically intractable for several reasons which we now alleviate with approximations. %


Firstly, computing the integral for the objective (and gradient) is typically intractable. We can however compute an empirical risk given a dataset $D_t \sim dp_t$. Secondly, we often do not know the Bayes optimal models $f_p$ for each $dp \in P$, but instead can obtain ``approximations" $\hat{f_p}$, e.g., by training a model on a dataset $D_p \sim dp \in P$. This gives us our approximate convex mixture objective, where we let $\text{Train}(\mathcal{H},D_p)$ be some training function:




\begin{align*}
    \label{eq:MixMin_approx}
    \lambda^*(\{\hat{f_p}\}_{p \in P})  = \arg \min_{\lambda \in \Delta^{P}}
    \frac{1}{|D_t|}\sum_{(x,y) \in D_t}  \mathcal{L}(\sum_{p \in P} \lambda_p \hat{f_p}(x),y) \\
    \text{where}~\hat{f}_p = \text{Train}(\mathcal{H},D_p)\\
    \tag{MixMin}
\end{align*}

Computing the gradient of the objective follows from chain rule, and hence we can run entropic descent to solve~\ref{eq:MixMin_approx}. Recall entropic descent is a variant of mirror descent which handles the constraint of being in the simplex.  This gives us Algorithm~\ref{alg:mixmin}, which we call \method for `Mixtures by Minimization'. 


\paragraph{Cost} We only need to evaluate $f_p(x)~\forall x \in D_t$ once and can resuse the outputs for subsequent calls to the gradient. Given this, the dominating cost with \methodnospace, as with grid/random search based approaches to data mixing, is the number of models needed to train (i.e., $\hat{f_p}$). \method requires only as many models as sources, while the number of models grid/random search approaches require scales exponentially (in the worst-case) with the number of sources. Note once the outputs and proxy models are computed, the per-iteration cost of \method is $O(|P||D_t|)$, which is independent of model size.

\paragraph{On the use of weak proxy models} In our experiments, we found that proxy models $\hat{f}_p$ trained with very small fractions of total training compute achieved good results and additional proxy model compute did not significantly improve the quality of the final mixtures. This suggests that \method can perform well when the proxy models $\hat{f}_p$ have a high loss and that the excess error of~\ref{eq:MixMin_approx} should be analyzed beyond the risk of the proxy models $\hat{f}_p$. Specifically, we empirically found that we could train with significantly less data ($1\%$) and still have similar \method performance. Furthermore, we found the ensemble of the proxy models could be far less accurate than the model retrained on the \method mixture weights, suggesting the proxy models were far from Bayes optimal~\footnote{Being close to Bayes optimal would imply the ensemble was close to retraining, a contradiction.} but still led to good mixtures. We hope future work investigates and generalizes these phenomenon.
















