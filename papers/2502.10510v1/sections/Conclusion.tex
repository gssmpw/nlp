\section{Conclusion}

In this paper we formalized a bi-level data mixing objective for mixing data sources to minimize risk on a downstream objective. We showed that in the case of CE or MSE, this objective reduces to a convex minimization as our model classes become larger. We proposed using cheap proxy models trained on the individual source distributions to optimize the convex objective, leading to a data mixing method we call \methodnospace. Our experiments showed \method consistently outperformed previous baselines across language modeling and chemistry tasks, and was robust to using cheap proxy models. We hope future work explores explaining when and why \method can use cheap proxy model, or develops alternative empirical approaches to optimizing the convex objective for data mixing with large model classes.

\paragraph{Limitations} Our reductions for data mixing were specific to CE and MSE, and required there be no covariate shift among the source distribution. While this covers generative tasks, we hope future work explores extending these ideas to other loss functions, and handles covariate shift. For example, our framework does not currently allow us to do data mixing for image classification, where covariate shift is common among sources. Also, \method need not be the best way to optimize the convex data mixing objective.
