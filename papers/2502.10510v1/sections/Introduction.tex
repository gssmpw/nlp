\section{Introduction}









Recent progress in ML has come from training on vast web-scale data~\citep{dubey2024llama,touvron2023llama,raffel2020exploring,achiam2023gpt}. These models are known to generalize to data-poor downstream tasks, a core motivation behind scaling training data. However, as we increase the amount and diversity of data sources we can train on, a core challenge becomes choosing how to weigh these sources when training. The effectiveness of a model pre-trained on surrogate data is known to be directly related to how similar the surrogate (or ``pretraining") data is to the downstream task (target distribution)~\citep{ben2010theory,isik2024scaling,jain2024scaling, pouget2024no}. 





Formally, finding the best mixture of data sources to train on for a downstream loss poses a bi-level optimization which is hard to optimize. We must find the mixture whose risk minimizer optimizes the downstream loss. This generally only admits expensive zero-order approaches, such as grid search~\citep{dubey2024llama,blakeney2024does}. Past work has considered learning to simulate the loss oracle for unseen mixtures~\citep{liu2024regmix}, or leveraging a large pool of existing models~\citep{thrush2024improving}. Other approaches consider solving data mixing objectives agnostic to a downstream loss, but are known to not consistently improve the baseline of training on all the data with their natural proportions (when evaluated on diverse downstream losses)~\citep{xie2024doremi,jiang2024adaptive, liu2024regmix, fan2023doge, held2025optimizing}. 



In this paper we show that the bi-level optimization for data mixing reduces to a convex minimization as the model classes become larger, which we can solve cheaply with gradient based methods. A key insight is that by restricting the loss functions to cross-entropy (CE) or mean-squared error (MSE), we gain additional structure which simplifies the data mixing objective. Practically, we show we can find the best mixture by first training a (cheap) proxy model for each source, learning the best mixture of their outputs for the target dataset (convex minimization), and then using this weighting to remix the data, see \cref{fig:mixmin_flow}.







First, we note that when our models are relatively close to Bayes optimal (the best possible functions), data mixing is equivalent to learning a linear model over the predictions of a fixed set of models (a convex objective). Learning this linear model requires comparatively little data to training modern large models, and is relatively cheap to optimize. Our reduction is specific to CE or MSE, where we leverage the fact the Bayes optimal model for a mixture is the mixture of the Bayes optimal models for each source. However, this reduction does not yet make data mixing amenable to first-order optimization. To evaluate our convex objective's gradient we need the Bayes optimal model for each source.





A key challenge is that the proxy models needed to evaluate this convex objective can be expensive to compute. We find that in practice, we can use cheaply computed models and still have good data mixing performance. We call this approach to optimizing the convex objective \emph{\methodnospace}. Empirically, we found the performance of \method did not significantly degrade between using proxy models computed with $100\%$ the cost of a full training run to $1\%$. In other cases, the \method mixture significantly improved over baselines, but the ensemble of proxy models performed much worse than retraining, suggesting they were far from Bayes optimal but did not hinder \method performance.









We empirically compared data mixing with \method to current baselines on both language modeling and chemistry tasks. For language modeling we considered optimizing the mixture of source domains in SlimPajama~\citep{cerebras2023slimpajama} for PIQA~\citep{bisk2020piqa}, ARC Easy~\citep{Clark2018ThinkYH}, SciQ~\citep{Welbl2017CrowdsourcingMC}, or OpenWebMath~\citep{paster2023openwebmath}. In all cases we found \method improved the baselines for log likelihood, improving or maintaining the gap as we increased the scale of the models from pythia-$160M$ to pythia-$410M$~\citep{biderman2023pythia}. Moreover, for pythia-$410M$ the mixtures we used took  $~0.15\%$ the compute of the training run to find and improved the negative log likelihood by $1-5\%$. For our chemistry experiments we considered mixing assay datasets in PubChem~\citep{beaini2023towards,kim2016pubchem} to improve the performance of an XGBoost~\citep{chen2016xgboost} model on several held-out assay datasets. In all cases, we observed MixMin improved over the natural distribution  (the standard data mixture after filtering for this domain~\citep{salem2020transcreen,li2022improving, ye2018integrated}) as we increased the number of surrogates to mix over, improving average precision scores between $0.03 - 0.15$. We note an additional benefit of optimizing data mixtures for chemistry tasks is that the found \method weights could provide interpretability; we highlight patterns \method found in PCBA for predicting assays (e.g., distinct but predictive cytotoxicity assays). To summarize, our contributions are:





\begin{figure*}[t]
\centering
    \includegraphics[scale = 0.4]{figures/MixMin_Fig1_trial1.pdf}
\caption{Optimizing \method to find the mixture weights requires training a few cheap models for each source and a target dataset. Given the mixture weights we train a more expensive model using the mixture.}
\label{fig:mixmin_flow}
\end{figure*}

\begin{enumerate}
    \item Observing data mixing reduces to a single convex minimization as model classes become more expressive
    \item Proposing \method which approximately solves the convex objective by using cheap proxy models and downstream data samples
    \item Empirically showing \method improves past baselines for finding data mixtures across language modeling and chemistry tasks
    \item Empirical and analytical evidence showing \method is robust to the use of weak/cheap proxy models
\end{enumerate}










