\section{Preliminaries}


We denote a hypothesis space by $\mathcal{H}$, a model by $f: \mathcal{X} \rightarrow \mathcal{O} \in \mathcal{H}$, a datapoint by $(x,y) \in \mathcal{X} \times \mathcal{Y}$, and our loss function by $\mathcal{L}(o,y): \mathcal{O} \times \mathcal{Y} \rightarrow \mathbb{R}^{+}$. This paper considers the problem of mixing a set of finite source distributions ($dp \in P$ where $P$ is finite) to train on to do better on some downstream ``target" distribution ($dt$). For example, mixing Wikipedia and arXiv data to do better on benchmarks evaluating scientific knowledge.


Specifically, we are searching for the best weighting $\lambda \in \Delta^P$ (where $\Delta^{P}$ is the simplex) of our sources to train a model on to perform well on our downstream distribution. Formally, using our loss function $\mathcal{L}$ as both our measure of downstream performance and training objective:




\begin{align*}
\label{eq:DM}
DM(\lambda,\mathcal{H}) = \int_{\mathcal{X} \times \mathcal{Y}} \mathcal{L}(f_{\lambda}(x),y) dt(x,y) \\
    \text{where } f_\lambda(x) = \arg \min_{f \in \mathcal{H}} \sum \lambda_p \int_{\mathcal{X} \times \mathcal{Y}} \mathcal{L}(f(x),y) dp(x,y) \tag{Data Mixing}
\end{align*}




In general $\min_\lambda DM(\lambda,\mathcal{H})$ a bi-level optimization that can be hard to optimize: a standard approach is grid/random searching through mixture weights and solving the inner minimization by fitting a (small) proxy model. In this paper we describe settings where the bi-level optimization collapses and we can use simple gradient based optimization to find the best mixture. 

\paragraph{Data Filtering is not Data Mixing} An often related but different approach to curating training data is data filtering. Abstractly, data filering is some map $F$ that takes a dataset $D \rightarrow D'$ where $D' \subset D$. Such approaches are motivated by training efficiency, e.g., coresets, and/or data quality, e.g., rejection sampling to be closer to a desired target distribution. Data filtering can be composed with data mixing; one typically mixes amongst filtered data sources~\citep{dubey2024llama}. We leave studying such compositions to future work, but note some popular examples of data filtering include: perplexity correlations~\citep{thrush2024improving}, n-gram or embedding similarity with a target distribution~\citep{dsir, gio}, handcrafted classifiers~\citep{cpack, dclm}, and perplexity filtering~\citep{dclm} 





