\section{Related Work}

Some past work has considered optimizing a pretraining data mixture for a specific downstream loss, i.e., task. \citet{hwang2021regmix} train many small models with different mixtures and learn to predict the task error of unseen mixtures, and RegMix ~\citep{liu2024regmix} later used a similar oracle to find better mixtures (the best mixture if the oracle is accurate). %
Similarly, \citet{held2025optimizing} proposed optimizing the mixture given estimates of downstream performance from different source datasets; they assumed data mixing was a linear model (with constraints) in the mixture weights and proposed mixtures by maximizing the performance of the linear model. We note~\citet{liu2024regmix} considered modeling data mixing as a linear objective, but found their non-linear models for predicting data mixing improved over linear models. Finally there are the normal grid-search approaches~\citep{blakeney2024does}, which random search is known to improve~\citep{bergstra2012random}. 



Another line of work has considered data mixing objectives defined by just the sources and not a downstream loss: e.g., with the motivation of faster optimization on a mixture of the sources or generalizing better across the sources. However, when evaluating on a downstream objective (as is the focus of our paper) methods here are known to not consistently improve over the natural or balanced distribution of data \citep{fan2023doge,jiang2024adaptive, albalak2023efficient, xie2024doremi}: see ~\citet{held2025optimizing} for comparisons. Outside of language modeling, we note the natural distribution of data is the standard data mixture used for transfer learning in chemistry (after filtering data)~\citep{salem2020transcreen,li2022improving, ye2018integrated}. Another line of work considered data mixing for distributionally robust optimization~\citep{thudi2024finding}, which is distinct to downstream data mixing objective. 


Given these past findings, we compare to RegMix and random search in our experiments. However, compared to \methodnospace, RegMix has significantly higher evaluation costs.



