\section{Proofs}
\label{app:proofs}

\subsection{\cref{thm:mixmin_reduc}}
\label{app:proof_mixmin}
\begin{proof}
        Recall that the Bayes optimal model for cross entropy is $p(y|x)$ and for $\ell_2^2$ is $\mathbb{E}_{y \sim p(y|x)} y$. In both cases, letting $f_{\lambda}$ be the Bayes optimal for the mixture $dp_{\lambda} = \sum_{p\in P} \lambda_{p} dp$, and $f_p$ be the Bayes optimal for the individual sources, we have for cross-entropy and $\ell_2^2$: $$f_{\lambda} = \frac{\sum_{dp \in P} \lambda_p f_p(x) p(x)}{\sum_{dp' \in P} \lambda_{p'} p'(x)}$$

    and in particular, when there is no covariate shift amongst the sources we have $f_{\lambda} = \sum_{dp \in P} \lambda_p f_p(x)$. With this, we then have our \ref{eq:DM} objective reduces to just learning a linear model over $f_p(x)$. Specifically plugging in $f_{\lambda} = \sum_{dp \in P} \lambda_p f_p(x)$ into~\ref{eq:DM} gives Equation~\ref{eq:MixMin}.
\end{proof}

\subsection{\cref{lem:approx_err}}
\label{app:proof_approx}

\begin{proof}
    First note that, using the Lipschitz criterion with the condition on the hypothesis space and the definition of data mixing, letting $\mathcal{H}^*$ contain the Bayes optimal for all mixtures, we have $\forall \lambda \in \Delta^P$ that  $$|DM_{\mathcal{H}}(\lambda,dt) - DM_{\mathcal{H}^*}(\lambda,dt)| \leq C\epsilon.$$ %
    This relation will be enough for the proof, and hence weaker criteria in the lemma that ensure this relation will suffice.
    

    Let $\bar{\lambda} = \arg\min_{\lambda \in \Delta^P} DM_{\mathcal{H}}(\lambda, dt)$ be the optimal data mixture weights and recall $\lambda^* = \arg \min_{\lambda \in \Delta^P} DM_{\mathcal{H}^*}(\lambda, dt)$ where $\mathcal{H}^*$ contains the Bayes optimal functions for all mixtures. Now note the left hand sign of the inequality 

    $$
        DM_{\mathcal{H}}(\lambda^*, dt) - \min_{\lambda \in \Delta^P} DM_{\mathcal{H}}(\lambda, dt) =  DM_{\mathcal{H}}(\lambda^*, dt) - DM_{\mathcal{H}^*}(\lambda^*, dt) + DM_{\mathcal{H}^*}(\lambda^*, dt) - DM_{\mathcal{H}}(\bar{\lambda}, dt).
    $$
    

    The first difference is bounded by $C\epsilon_1$ by the earlier inequality for changing hypothesis spaces. Now bounding the second difference we have

    \begin{multline*}
        DM_{\mathcal{H}^*}(\lambda^*, dt) -DM_{\mathcal{H}}(\bar{\lambda}, dt)  = DM_{\mathcal{H}^*}(\lambda^*, dt) - DM_{\mathcal{H}^*}(\bar{\lambda}, dt)  + DM_{\mathcal{H}^*}(\bar{\lambda}, dt) - DM_{\mathcal{H}}(\bar{\lambda}, dt) \\ \leq DM_{\mathcal{H}^*}(\bar{\lambda}, dt) - DM_{\mathcal{H}}(\bar{\lambda}, dt) \leq C \epsilon_1
    \end{multline*}


    where the first inequality came from the definition of $\lambda^*$, and the second from the inequality stated at the beginning of the proof for changing hypothesis spaces.

    Hence we conclude $$DM_{\mathcal{H}}(\lambda^*, dt) - \min_{\lambda \in \Delta^P} DM_{\mathcal{H}}(\lambda, dt) \leq 2C\epsilon_1$$
\end{proof}





\section{Extra Tables and Figures}
\label{app:figures}

\begin{table*}[h!]
    \centering
    \begin{tabular}{c | c c c c } 
     \toprule
     Target & First & Second & Third & Fourth \\
     \midrule
     \cellcolor[HTML]{FFCCC9}assayID-1  & \cellcolor[HTML]{CBCEFB}assayID-891 & \cellcolor[HTML]{FFCCC9}assayID-620 & \cellcolor[HTML]{9AFF99}assayID-618 & \cellcolor[HTML]{CBCEFB}assayID-693 \\
\rowcolor[HTML]{FFCCC9} 
assayID-3                          & assayID-620                         & assayID-5                           & \cellcolor[HTML]{9AFF99}assayID-618 & \cellcolor[HTML]{CBCEFB}assayID-891 \\
\rowcolor[HTML]{FFCCC9} 
assayID-5                          & assayID-620                         & \cellcolor[HTML]{CBCEFB}assayID-891 & assayID-92                          & \cellcolor[HTML]{CBCEFB}assayID-425 \\
\cellcolor[HTML]{FFCCC9}assayID-7  & \cellcolor[HTML]{FFCCC9}assayID-620 & \cellcolor[HTML]{CBCEFB}assayID-952 & \cellcolor[HTML]{CBCEFB}assayID-693 & \cellcolor[HTML]{9AFF99}assayID-618 \\
\rowcolor[HTML]{FFCCC9} 
assayID-9                          & \cellcolor[HTML]{CBCEFB}assayID-891 & \cellcolor[HTML]{CBCEFB}assayID-693 & assayID-620                         & assayID-256                         \\
\cellcolor[HTML]{FFCCC9}assayID-11 & \cellcolor[HTML]{FFCCC9}assayID-620 & \cellcolor[HTML]{CBCEFB}assayID-952 & \cellcolor[HTML]{CBCEFB}assayID-693 & \cellcolor[HTML]{9AFF99}assayID-618 \\
\cellcolor[HTML]{FFCCC9}assayID-13 & \cellcolor[HTML]{9AFF99}assayID-618 & \cellcolor[HTML]{FFCE93}assayID-710 & \cellcolor[HTML]{CBCEFB}assayID-891 & \cellcolor[HTML]{CBCEFB}assayID-451 \\
\rowcolor[HTML]{FFCCC9} 
assayID-15                         & assayID-19                          & assayID-620                         & assayID-25                          & \cellcolor[HTML]{CBCEFB}assayID-758 \\
\rowcolor[HTML]{FFCCC9} 
assayID-17                         & assayID-19                          & \cellcolor[HTML]{9AFF99}assayID-618 & \cellcolor[HTML]{CBCEFB}assayID-758 & assayID-21                          \\
\rowcolor[HTML]{FFCCC9} 
assayID-19                         & assayID-25                          & assayID-17                          & \cellcolor[HTML]{CBCEFB}assayID-952 & assayID-21                          \\
     \bottomrule
    \end{tabular}
    \caption{The top 4 assays found by \method (over the 1328 assays in PCBA) for each target task. We list the PubChem assayID corresponding to the assay in PCBA. Red indicated assays measuring cytotoxicity, green are phenotypic screens, blue are specific protein binding/inhibition assays, and orange are other types of assays. }
    \label{tab:chem_assay_sel}
\end{table*}

\begin{table*}[h!]
    \centering
    \begin{tabular}{c | c c c c } 
     \toprule
     Target & First & Second & Third & Fourth \\
     \midrule
     assayID-1 & 0.036705 & 0.036184	& 0.035904 & 0.030910 \\
     assayID-3 & 0.05026778& 0.04660426& 0.0414932 & 0.03289703 \\
     assayID-5 & 0.05026778& 0.04660426& 0.0414932 & 0.03289703 \\
     assayID-7 & 0.03170111& 0.02936895& 0.02839234& 0.02707635 \\
     assayID-9 & 0.03645385& 0.02978397& 0.02941164& 0.02894659 \\
     assayID-11 & 0.04995361& 0.03905968& 0.03367052& 0.03212925 \\
     assayID-13 & 0.05764348& 0.04289928& 0.03077811& 0.02817639 \\
     assayID-15 & 0.11996419& 0.04302281& 0.03967432& 0.03457846 \\
     assayID-17 & 0.22989839& 0.04496526& 0.04188494& 0.03894247 \\
     assayID-19 & 0.10650713& 0.05704838& 0.05019803& 0.04435973\\
     \bottomrule
    \end{tabular}
    \caption{The mixing weights for the top 4 assays found by \method (over the 1328 assays in PCBA) for each target task. We list the PubChem assayID corresponding to the target assay in PCBA. }
    \label{tab:chem_assay_sel}
\end{table*}








\begin{figure*}[t!]
\centering
\begin{subfigure}
    \centering
    \includegraphics[scale = 0.4]{figures/Pred_Cost_100.pdf}
    \label{fig:pred_loss_100}
\end{subfigure}%
\begin{subfigure}
    \centering
    \includegraphics[scale = 0.4]{figures/Acc_Cost_100.pdf}
    \label{fig:acc_100}
\end{subfigure}
\caption{In almost all cases \method improves or matches all baselines across the target tasks using $1\%$ of the final training run compute (Left). We report improvement over the predictive loss (left) and accuracy (right) of training on the natural distribution, which is stated on the x-axis with the task name.}
\label{fig:cost_100_res}
\end{figure*}

\begin{figure*}[t!]
\centering
\begin{subfigure}
    \centering
    \includegraphics[scale = 0.27]{figures/Gen_cost_comp_ARC-Easy.pdf}
    \label{fig:arc_easy_cost_comp}
\end{subfigure}%
\begin{subfigure}
    \centering
    \includegraphics[scale = 0.27]{figures/Gen_cost_comp_SciQ.pdf}
    \label{fig:SciQ_cost_comp}
\end{subfigure}
\begin{subfigure}
    \centering
    \includegraphics[scale = 0.27]{figures/Gen_cost_comp_OpenMath.pdf}
    \label{fig:OpenMath_cost_comp}
\end{subfigure}
\caption{\method consistently performed better than alternative methods on generative loss across compute budgets, and was robust to using less compute. We note both Random Search and RegMix tend to improve with more compute. We report improvement over the generative loss of training on the natural distribution.}
\label{fig:cost_comp}
\end{figure*}

\begin{figure*}[t!]
\centering
\begin{subfigure}
    \centering
    \includegraphics[scale = 0.27]{figures/Pred_cost_comp_PIQA.pdf}
\end{subfigure}%
\begin{subfigure}
    \centering
    \includegraphics[scale = 0.27]{figures/Pred_cost_comp_ARC-Easy.pdf}
\end{subfigure}
\begin{subfigure}
    \centering
    \includegraphics[scale = 0.27]{figures/Pred_cost_comp_SciQ.pdf}
\end{subfigure}
\caption{\method often performed better or on par to alternative methods on predictive loss across compute budgets, and was robust to using less compute. We note both Random Search and RegMix tend to improve with more compute. We report improvement over the predictive loss of training on the natural distribution.}
\label{fig:pred_cost_comp}
\end{figure*}

\begin{figure*}[t!]
\centering
\begin{subfigure}
    \centering
    \includegraphics[scale = 0.27]{figures/Acc_cost_comp_PIQA.pdf}
\end{subfigure}%
\begin{subfigure}
    \centering
    \includegraphics[scale = 0.27]{figures/Acc_cost_comp_ARC-Easy.pdf}
\end{subfigure}
\begin{subfigure}
    \centering
    \includegraphics[scale = 0.27]{figures/Acc_cost_comp_SciQ.pdf}
\end{subfigure}
\caption{\method often performed better or on par to alternative methods on accuracy across compute budgets, and was robust to using less compute. We note both Random Search and RegMix tend to improve with more compute. We report improvement over the accuracy of training on the natural distribution.}
\label{fig:pred_cost_comp}
\end{figure*}


\begin{figure*}[t!]
\centering
\begin{subfigure}
    \centering
    \includegraphics[scale = 0.27]{figures/Pred_model_scale_PIQA.pdf}
\end{subfigure}%
\begin{subfigure}
    \centering
    \includegraphics[scale = 0.27]{figures/Pred_model_scale_ARC_Easy.pdf}
\end{subfigure}
\begin{subfigure}
    \centering
    \includegraphics[scale = 0.27]{figures/Pred_model_scale_SciQ.pdf}
\end{subfigure}
\caption{In most cases \method consistently improved predictive loss over the baselines as we scaled the models. We report predictive loss relative to a pythia-$160M$ model trained with the natural distribution of tokens (lower in the figures is better). \method weights were found using $1\%$ the compute of the $160M$ model training run, which is $~0.15\%$ the compute of the $410M$ training run.}
\label{fig:Pred_llm_model_scale_res}
\end{figure*}

\begin{figure*}[t!]
\centering
\begin{subfigure}
    \centering
    \includegraphics[scale = 0.27]{figures/MixMin_100_weights.pdf}
\end{subfigure}%
\begin{subfigure}
    \centering
    \includegraphics[scale = 0.27]{figures/Ran_Search_100_weights.pdf}
\end{subfigure}
\begin{subfigure}
    \centering
    \includegraphics[scale = 0.27]{figures/RegMix_100_weights.pdf}
\end{subfigure}
\caption{The mixture weights found by \methodnospace, Random Search, and RegMix when using $1\%$ the compute for a pythia-$160M$ training run on $3.2B$ tokens. Note the domains (x-axis) are ``RedPajamaCommonCrawl", ``RedPajamaC4", ``RedPajamaGithub", ``RedPajamaBook", ``RedPajamaArXiv" , ``RedPajamaWikipedia", ``RedPajamaStackExchange" respectively.}
\label{fig:llm_mix_weights}
\end{figure*}

\begin{figure*}[t!]
\centering
\begin{subfigure}
    \centering
    \includegraphics[scale = 0.27]{figures/MixMin_410M_Gen_cross_eval_abs.pdf}
\end{subfigure}%
\begin{subfigure}
    \centering
    \includegraphics[scale = 0.27]{figures/MixMin_410M_Pred_cross_eval_abs.pdf}
\end{subfigure}
\begin{subfigure}
    \centering
    \includegraphics[scale = 0.27]{figures/MixMin_410M_Acc_cross_eval_abs.pdf}
\end{subfigure}
\caption{We found the \method models for PIQA, Arc Easy, and SciQ perform similarly across tasks, however were (almost) always best for their own task across metrics. We report the cross performance of \method for different tasks at the $410$M parameter scale. The subfigures report generative loss, predictive loss, and accuracy respectively.}
\label{fig:llm_mixmin_cross_evals}
\end{figure*}

\begin{figure*}[t!]
\centering
\begin{subfigure}
    \centering
    \includegraphics[scale = 0.27]{figures/Baseline_410M_Gen_cross_eval_abs.pdf}
\end{subfigure}%
\begin{subfigure}
    \centering
    \includegraphics[scale = 0.27]{figures/Baseline_410M_Pred_cross_eval_abs.pdf}
\end{subfigure}
\begin{subfigure}
    \centering
    \includegraphics[scale = 0.27]{figures/Baseline_410M_Accuracy_cross_eval_abs.pdf}
\end{subfigure}
\caption{We report the performance of training on the natural and balanced mixture for different tasks at the $410$M parameter scale. The subfigures report generative loss, predictive loss, and accuracy respectively. Comparing to Figure~\ref{fig:llm_mixmin_cross_evals} we see both the natural and baseline mixture are matched or dominated in average performance across tasks by the \method weights for PIQA, Arc Easy, and SciQ.}
\label{fig:llm_baseline_cross_evals}
\end{figure*}


