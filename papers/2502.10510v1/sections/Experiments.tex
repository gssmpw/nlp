\section{Experiments}
\label{sec:experiments}



We compared \method to previous data mixing baselines for language modeling and chemistry tasks. For language modeling we further investigated the transferability of mixtures found using cheap proxy models to more expensive training runs. For chemistry, we explored the performance of \method as we increased the pool of surrogate data sources.
Experiments were run using A100 GPUs and AMD EPYC 7643 CPUs.




\subsection{Pretraining Language Models}

\begin{figure*}[t!]
\centering
\begin{subfigure}
    \centering
    \includegraphics[scale = 0.4]{figures/Gen_Cost_100.pdf}
    \label{fig:gen_loss_100}
\end{subfigure}%
\begin{subfigure}
    \centering
    \includegraphics[scale = 0.4]{figures/Gen_cost_comp_PIQA.pdf}
    \label{fig:piqa_cost_comp}
\end{subfigure}
\caption{\textbf{\method consistently outperforms all baselines across the four target tasks using $1\%$ of the final training run compute (Left).} We report improvement over the downstream generative loss of training on the natural distribution (which is stated beside the task name): higher is better. Error bars indicate a $95\%$ confidence interval. \textbf{Furthermore, we find that \method was robust to using less compute (Right), while RegMix and random search had their performance degrade with less compute.}}
\label{fig:llm_summary_res}
\end{figure*}

Data mixing is now a common step in pre-training LLMs~\citep{dubey2024llama,li2024datacomp}. We investigated how \method compared to other data mixing methods, its compute efficiency, and how weights found at a smaller scale of models transfers to training larger models.

\subsubsection{Experimental Setup}

\begin{figure*}[t!]
\centering
\begin{subfigure}
    \centering
    \includegraphics[scale = 0.27]{figures/Acc_model_scale_PIQA.pdf}
\end{subfigure}%
\begin{subfigure}
    \centering
    \includegraphics[scale = 0.27]{figures/Acc_model_scale_ARC_Easy.pdf}
\end{subfigure}
\begin{subfigure}
    \centering
    \includegraphics[scale = 0.27]{figures/Acc_model_scale_SciQ.pdf}
\end{subfigure}
\caption{ %
\textbf{MixMin mixtures derived from small models continue to improve training for larger models as measured by target accuracy (in most cases).} %
We report accuracy with the errors bars representing a $95\%$ confidence interval over $3$ trials. \method weights were found using $1\%$ the compute of the $160M$ model training run, which is $~0.15\%$ the compute of the $410M$ training run. %
}
\label{fig:llm_model_scale_res}
\end{figure*}

\paragraph{Datasets and Models} We used the domains in SlimPajama~\citep{cerebras2023slimpajama} as sources for pre-training. For the downstream target, we considered several multiple-choice tasks that models in the $160M-410M$ scale are known to do non-trivially at~\citep{thrush2024improving}, alongside another generative task: SciQ~\citep{Welbl2017CrowdsourcingMC}, PIQA~\citep{bisk2020piqa}, ARC-Easy~\citep{Clark2018ThinkYH} and the first 10000 documents in OpenWebMath~\citep{paster2023openwebmath}~\footnote{We chose $10000$ documents for computational efficiency.}. For multiple-choice tasks we used \method to maximize the log probability of the correct question and answer (Q+A) sequence, i.e., treating these tasks as new pretraining domains. Note that the optimal conditional of A given Q distribution is a conditional of the joint distribution of Q+A, so our objective subsumes the Q $\to$ A prediction problem. Alongside the objective loss, we also evaluated accuracy and the conditional cross-entropy of the pre-trained models for these tasks (i.e., multiple-choice performance). OpenWebMath is a specialized pretraining dataset, and so the loss for MixMin and evaluation was the negative log probability of a sequence. Our experiments used the $160M$ and $410M$ pythia models~\citep{biderman2023pythia} trained for $3.2B$ and $8.2B$ tokens respectively (chinchilla optimal~\citep{hoffmann2022training}).



\paragraph{Data Filtering} We included documents in SlimPajama with at least $1024$ tokens (using the Pythia tokenizer), and scraped documents for each domain until adding the next document would bring us over 3.2B tokens for each domain. 

\paragraph{Hyperparameter Tuning} We took the largest batch size that fits on a single A100 gpu, which was $64$ for the $160M$-pythia model and $32$ for the $410M$-pythia model for a context length of $1024$. For the $160M$-pythia model we increased the learning rate until training loss got worse on the natural distribution of domains in SlimPajama: we started from $1e-4, 5e-4,1e-3,5e-3,1e-2$ and found loss got worse at $1e-2$ so chose $5e-3$. For the $410M$-pythia model we evaluated learning rates $5e-3$ and $1e-2$ and found $5e-3$ was better. Other hyperparameters are the same as~\citep{thrush2024improving}. All hyperparemeters are fixed throughout the language modeling experiments.

\paragraph{Method Implementations} We compared \method to the baselines of balancing the domains, the natural distribution of domains in SlimPajama, random search and RegMix (the latter known to improve or perform on par as previous data mixing methods). We split the target task into a random $80\%$ training set, and $20\%$ test set. The training set was used for the MixMin optimization, and the evaluation of loss for random search and RegMix. Results are reported over $3$ trials of random train-test splits. 

We ran MixMin using $\eta = 1.0$ for $100$ steps for all the experiments. For RegMix, we adapted the \href{https://github.com/sail-sg/regmix/blob/main/regression_fitting/regression.ipynb}{public code}, changing the hard-coded natural distribution to the SlimPajama natural distribution and inputting our models' results. We implemented random search by sampling uniformly from the simplex and training proxy models on those mixture. Recall \method requires a proxy model for each domain, and to normalize evaluation compute we ran random search with the same number of proxy models ($7$). RegMix showed benefits by using many (but cheap) proxy models~\citep{liu2024regmix}, and so we ablated the number of our cheapest proxy models to vary compute for it; this however increased its evaluation costs to the other baselines. All proxy models are trained with the same hyperparameters specified earlier, and use the $160M$ pythia model architecture. To achieve $X\%$ compute relative to a full training run of the $160M$ pythia model, we trained each proxy model for $3.2 (X/100)(1/7)B$ tokens for \method and random search, and for RegMix train $7X$ proxy models where each proxy model trains for $3.2(1/700)B$ tokens (our lowest compute proxy models).




\subsubsection{Results}


\begin{figure*}[t!]
\centering
\begin{subfigure}
    \centering
    \includegraphics[scale = 0.4]{figures/Gen_model_scale_PIQA.pdf}
\end{subfigure}%
\begin{subfigure}
    \centering
    \includegraphics[scale = 0.4]{figures/Gen_model_scale_ARC_Easy.pdf}
\end{subfigure}
\begin{subfigure}
    \centering
    \includegraphics[scale = 0.4]{figures/Gen_model_scale_SciQ.pdf}
\end{subfigure}
\begin{subfigure}
    \centering
    \includegraphics[scale = 0.4]{figures/Gen_model_scale_OpenWebMath.pdf}
\end{subfigure}
\caption{\textbf{MixMin mixtures derived from small models continue to improve training for larger models as measured by target generative loss (in all cases).} We report generative loss with the errors bars representing a $95\%$ confidence interval over $3$ trials (\emph{lower is better}). \method weights were found using $1\%$ the compute of the $160M$ model training run, which is $~0.15\%$ the compute of the $410M$ training run.}
\label{fig:Gen_llm_model_scale_res}
\end{figure*}




\paragraph{MixMin found good mixtures with $1\%$ of train compute} In Figure~\ref{fig:llm_summary_res} we show the generative loss results of MixMin, random search, and RegMix where the total compute for all the proxy models was $1\%$ relative to training the final $160M$-pythia model for $3.2B$ tokens (Chinchilla optimal). We compared the performance of these to a balanced distribution and the natural distribution. We found \method consistently improved generative loss over the baselines. We also found \method consistently matched or improved the predictive loss and accuracy over all the baseline methods (Figure~\ref{fig:cost_100_res} in Appendix~\ref{app:figures}). %


\paragraph{MixMin was more cost effective than baselines} In figure~\ref{fig:llm_summary_res} (and Figure~\ref{fig:cost_comp} in Appendix~\ref{app:figures} for other tasks) we compared the performance of MixMin, random search, and RegMix as we vary the the total compute for all the proxy models from $100\%,10\%,5\%,1\%$ of the compute for training the final $160M$-pythia model for $3.2B$ tokens (Chinchilla optimal). We found \method was robust to using less compute, where as random search and RegMix benefitted from increasing compute (but do not meet \method performance even at the highest compute setting we tested). Note, we did not include RegMix at $100\%$ compute given the significant evaluation overhead: 100 times the alternatives.


\paragraph{MixMin mixtures for smaller models improve training of larger models} Finally, we tested whether \method mixtures derived from smaller models were useful for training larger models. We computed \method mixtures using pythia-$160M$ proxy models each trained with $0.14\%$ the compute of a full $3.2B$ token training run (for an overall cost of $1\%$). We then trained a pythia-$410M$ for $8.2B$ tokens on those mixtures. Here we only compared to the balanced or natural mixture, as our previous experiments showed random search and RegMix found mixture comparable or worse than one of those mixtures for our tasks. Accuracy results are presented in Figure~\ref{fig:llm_model_scale_res} with generative and predictive loss results presented in Figure~\ref{fig:Gen_llm_model_scale_res} and Figure~\ref{fig:Pred_llm_model_scale_res} (the latter in Appendix~\ref{app:figures}). For accuracy and predictive loss we found in most cases \methodnospace's performance improved over the baselines as we increased scale, despite not refitting the \method weights. Even in the cases where \methodnospace's accuracy and predictive loss did not strictly improve over baselines, \emph{the generative loss always improved with \methodnospace}. 







\subsection{Chemistry}

\begin{figure*}[t]
\centering
\begin{subfigure}
    \centering
    \includegraphics[scale = 0.27]{figures/chem_mixing_10.pdf}
    \label{fig:chem_10}
\end{subfigure}%
\begin{subfigure}
    \centering
    \includegraphics[scale = 0.27]{figures/chem_mixing_100.pdf}
    \label{fig:chem_100}
\end{subfigure}
\begin{subfigure}
    \centering
    \includegraphics[scale = 0.27]{figures/chem_mixing_1328.pdf}
    \label{fig:chem_1328}
\end{subfigure}%
\caption{\textbf{\method improved over using the natural distribution of data as we increased the number of surrogate assays.} We report AP scores for the first 10 assays in PCBA, with error bars representing a $95\%$ confidence interval over $3$ trials.}
\label{fig:chem_scale}
\end{figure*}


We explored how \method could improve predicting properties of molecule libraries, in particular the endpoints of an assay (a predictive task). Many assays have few positive samples and training on just that data can lead to only noisy performance. Past work has remedied this by transfer learning from a larger dataset, but had not considering optimizing the data mixture after filtering~\citep{salem2020transcreen,li2022improving, sun2022feature, ye2018integrated}. A common default mixture is the natural distribution, and here we show \method provided better mixtures than this, especially as the diversity of the source datasets grows. We hope \methodnospace's ability to scale helps future data curation efforts for predictive drug screening.







\subsubsection{Experimental Setup} 
\begin{figure*}[t!]
\centering
\begin{subfigure}
    \centering
    \includegraphics[scale = 0.27]{figures/chem_mixing_10_ens.pdf}
    \label{fig:chem_10_ens}
\end{subfigure}%
\begin{subfigure}
    \centering
    \includegraphics[scale = 0.27]{figures/chem_mixing_100_ens.pdf}
    \label{fig:chem_100_ens}
\end{subfigure}
\begin{subfigure}
    \centering
    \includegraphics[scale = 0.27]{figures/chem_mixing_1328_ens.pdf}
    \label{fig:chem_1328_ens}
\end{subfigure}%
\caption{\textbf{Retraining with the \method mixture performs better than using the \method weights to ensemble the proxy models.} We report AP scores with error bars representing a $95\%$ confidence interval over $3$ trials. \emph{This implies the proxy models were far from Bayes optimal.}}
\label{fig:chem_ens}
\end{figure*}


\paragraph{Datasets and Models} We worked with the PCBA dataset~\citep{beaini2023towards}~\footnote{The dataset can be found at \url{https://polarishub.io/datasets/graphium/pcba-1328-1564k-v1}.}, which was a filtered scrape of PubChem~\citep{kim2016pubchem}. We used the first 10 bioassays as the target distributions, using subsets of all the bioassays (removing the target) as sources. Our experiments used XGBoost on ECFP fingerprints of SMILES~\citep{rogers2010extended} to fit each assay, known to be a strong  baseline~\citep{yang2019analyzing,seidl2023enhancing} for this domain. We report AP performance (average precision score, a variant of AUPR) for our experiments. 

\paragraph{Data Filtering} We trained models over the first 100,000 molecules in PCBA. We further removed any assay with no positive or negative instances among the first 100,000 molecules. For all assays ($1328$) the number after filtering was $1281$, for the first $100$ assays this left $86$, and the first $10$ were all kept. We performed data mixing among the filtered assays (excluding the target assay).

\paragraph{Method Implementation}  For every assay in PCBA, we used a $64\%-16\%-20\%$ train-validation-test split: an original $80\%-20\%$ train-test split, and further splitting the train set into a $20\%$ validation set. We trained proxy models used for \method on just the train split, and trained baseline models on the train and validation splits. The baselines models represent the performance we get by just training on the target, as to normalize improvement from data mixing. Given a target assay, we used the proxy models on the other assays in PCBA to run \method, using the train set on the target task to fit the \method mixture weights (leaving the validation and test set unseen). Finally, given the \method mixture we retrained a new model on the train sets from all the other assays. For the natural distribution baseline, we trained on all the train sets from other assays as is (no reweighting of sources). For all method we reported the mean test AP score improvement (from $3$ trials) over the baseline of training on the target distribution, with $95\%$ confidence intervals.




\paragraph{Hyperparameter Tuning} We selected hyperparameters for each proxy and baseline model by doing a 5-fold cross-validation on the train set. We grid search over all combinations of $n~estimators \in [10,50,100]$ and $max~depth \in [4,6,8]$. For the models trained over all the surrogate assays (the final model trained on \method or natural mixtures) we fixed the $n~estimators = 100$ and $max~depth = 6$.






\subsubsection{Results}



\paragraph{MixMin's advantage over the natural mixture increased with the number of sources} As shown in Figure~\ref{fig:chem_scale}, we observed that \method performs mostly on par with the natural distribution when 
working with the first $10$ assays and improved over natural as the number of assays to mix grew. This suggests \method was able to find relevant data sources amongst more diverse sets, and we note its absolute performance grew while the performance of the natural mixture decreased with the number of sources. We note in all cases both \method and training on the natural distribution improved over the baseline of training on the target assay.
 



\paragraph{Retraining with \method Mixture performed better than ensembling proxy models} An alternative approach to training on \method weights, suggested by the reduction for Bayes optimal models, is to ensemble the proxy models with the \method weights. We present a comparison between this and retraining with \method weights in Figure~\ref{fig:chem_ens}. Retraining performed significantly better, suggesting the proxy models were too noisy to lead to good ensembles. Specifically, this implies the proxy models were far from Bayes optimal, as the ensemble would have then also been near Bayes optimal. The fact we were able to improve over the natural distribution suggests \method did not need strong proxy models for these sources and targets.


\paragraph{MixMin highlighted data sources with similar endpoint measure} All the target assays tested cytotoxicity, and the majority of the top 4 data sources \method found for each assay also tested cytotoxicity (Table~\ref{tab:chem_assay_sel} in Appendix~\ref{app:figures}).  More specific protein inhibition assays were also included, potentially providing insights into the compound's mechanism of action. However, an assay with no apparent connection to the target assay was also identified. We hope future work explores the full interpretability of \method weights for the bioassays we tested on.


