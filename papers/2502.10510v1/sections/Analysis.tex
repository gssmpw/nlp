\section{Data Mixing is Learning a Linear Model}

\begin{figure}[t]
\centering
    \includegraphics[scale = 0.3]{figures/Fig2_trial_0.pdf}
\caption{The convex \method objective better approximates the~\ref{eq:DM} objective as the model class becomes larger (and better approximates Bayes optimal). }
\label{fig:mixmin_intuition}
\end{figure}


We show that~\ref{eq:DM} simplifies to a convex optimization problem as the model classes become larger. This intuition is presented in Figure~\ref{fig:mixmin_intuition}.





\subsection{Well-Specified: Data Mixing is just Risk Minimization}


We show that, when our hypothesis space contains the Bayes optimal model $f_p$ for all $dp \in P$, the best~\ref{eq:DM} weights are also the best weighting of models trained on each source (a convex objective). The core idea is that for certain losses, cross-entropy (CE) and mean-squared error (MSE), the Bayes optimal model for a mixture is the mixture of the Bayes optimal models' for the sources. However this is only true if there is no input distribution shift among the sources ($p(x)$ are all the same), e.g., for generative tasks. To clarify, distributions for generative tasks are only over ``labels", $p(y)$, there is no conditioning on inputs $x$ (or equivalently only a single fixed input).








\begin{theorem}
\label{thm:mixmin_reduc}
    If $\mathcal{L}$ is CE or MSE, $\mathcal{H}$ contains the Bayes optimal model for each mixture, and the source distributions $dp \in P$ do not have covariate shift, then $\lambda^* = \arg \min_{\lambda \in \Delta^P} DM(\lambda, \mathcal{H})$ iff

    \begin{equation}
\label{eq:MixMin}
    \lambda^* =  \arg\min_{\lambda \in \Delta^{P}} \int_{\mathcal{X} \times \mathcal{Y}} \mathcal{L}\left(\sum \lambda_p f_p(x),y\right) dt(x,y)
\end{equation}
\end{theorem}






The proof is provided in Appendix~\ref{app:proof_mixmin}. This optimization is clearly convex, and can be handled with gradient based approaches unlike~\ref{eq:DM}. However, running this optimization requires having the Bayes optimal $f_p$ and an estimator for the integral to compute the gradients. The main hurdle will be how to get reasonable approximations for the Bayes optimal $f_p$, which we discuss in Section~\ref{sec:method}. 






\subsection{Not Well-Specified: `Expressivity' is Enough}



We now show that~\ref{eq:DM} smoothly approaches Equation~\ref{eq:MixMin} as the hypothesis space $\mathcal{H}$ contains models closer Bayes optimal, e.g., becomes larger. This follows from assuming some regularity in our loss function and the definition of the data mixing objective.



\begin{lemma}
\label{lem:approx_err}
    For $\mathcal{L}$ either CE or MSE, suppose $\int \mathcal{L}(f(x),y) dt(x,y)$ is $C$-Lipschitz in $f$ w.r.t a norm on the functions $\|f(x)\|$~\footnote{To ensure this for MSE one could consider only bounded output spaces, and for cross-entropy, bounded away from $0$ and $1$.}. Let
    \begin{equation*}
    \lambda^* =  \arg\min_{\lambda \in \Delta^{P}} \int_{\mathcal{X} \times \mathcal{Y}} \mathcal{L}\left(\sum \lambda_p f_p(x),y\right) dt(x,y).
\end{equation*}
    Then, for \emph{any} hypothesis space $\mathcal{H}$ such that for all mixtures $dp_{\lambda} \in Conv(P)$, the $dp_{\lambda}$ risk minimizer $\hat{f_{\lambda}}(\mathcal{H})$ in $\mathcal{H}$ satisfies $\|\hat{f_\lambda}(\mathcal{H}) - f_{\lambda}\| \leq \epsilon$ where $f_\lambda$ is the Bayes optimal model\footnote{Effectively this means functions too far away will not have low error. Mathematically, this means there is lower-bound on the increase in loss from Bayes optimal in terms of the norm of the perturbation to the function (e.g., strong-convexity).},
we have the excess~\ref{eq:DM} error is bounded by $\epsilon$: %

    \begin{equation}
    \label{eq:DM_MixMin_err}
        DM_{\mathcal{H}}(\lambda^*, dt) - \min_{\lambda \in \Delta^P} DM_{\mathcal{H}}(\lambda, dt) \leq 2C\epsilon %
    \end{equation}
\end{lemma}



The proof is provided in Appendix~\ref{app:proof_approx}. The main intuition to take away from Lemma~\ref{lem:approx_err} is that as our model classes get larger and becomes closer to expressing the Bayes optimal model, the Equation~\ref{eq:MixMin} weights become closer to the optimal~\ref{eq:DM} weights. This is depicted in Figure~\ref{fig:mixmin_intuition}

