\section{RELATED WORK}
%Our work is broadly related to three areas: experimental as well as quasi-experimental evaluation of algorithm-aided decision making, and spillover effects. 
Here we provide a high level overview of background literature -- further discussion on related work is available in Appendix~\ref{app:further_rel}.
%\todo{Cut down text; add references to "static" judge bias work, independent of experiment design (vs exp design assumptions?)}

\paragraph{Modeling Discretion in Quasi-experimental investigations.} It is well known that decision-makers are subject to intuitive biases that inform how they respond to cases more generally ~\citep{guthrie2007blinking,guthrie2000inside}, and specifically how they factor in information from algorithmic decision aids~\citep{demichele2021intuitive}. However, most experimental and quasi-experimental work that have considered judge responsiveness assume that the tendency to follow or not follow the algorithmic recommendation is inherent to each judge -- a \emph{static} attribute of the judges' internal state, or their access to privileged information, i.e. factors outside of the experimenter's control \citep{mullainathan2022diagnosing,albright2019if}. For example, \citet{hoffman2018discretion} considered possible access to privileged information that impacted the hiring manager's judgement (each hiring manager having a particular \emph{exception rate} for making a different decision from the algorithm). %\citet{angwin2016machine} discussed the cost of deviation, when the judge has to actively document deviations from the algorithmic recommendation. 
In contrast, we consider a \emph{dynamic} model of judge responsiveness that is sensitive to factors in experimental design, acknowledging the connection between these experiment design factors and the induced behavioral biases that may impact judge decision-making.%~\citep{}.

\paragraph{The Rise of Experimental Evaluations.}
 In this work, we focus on \citet{imai2020experimental}, who presented a case study involving a Randomized control trial (RCT) examining the use of Public Safety Assessment (PSA) risk scores by judges in Dane County, Wisconsin. However, various other examples of RCTs for risk assessments have been explored in other domains. For instance, \citet{wilder2021clinical} ran a ``clinical trial'' on an algorithm used to select peer leaders in a public health program from homelessness youth. %It is the first known \emph{experimental} evaluation of a deployed algorithmic intervention in the criminal justice context.
%Another area that has recently seen significant traction in the experimental evaluation of algorithmic systems is healthcare. 
Similarly, several studies in education execute RCTs to assess models used to predict which students (in real-world and MOOC settings) are most likely to drop out~\citep{dawson2017from, borrella2019predict}.  
In healthcare, ~\citet{plana2022randomized} provides a recent survey of the 41 RCTs of AI/ML-based healthcare interventions done to date, critiquing a systematic lack of adherence to documentation standards such as CONSORT-AI and SPIRIT-AI ~\citep{liu2020reporting}. 
%In healthcare, several standards have already been developed for reporting expectations on clinical trials for AI/ML-based healthcare interventions such as CONSORT-AI and SPIRIT-AI ~\citep{liu2020reporting}. However, a recent survey found that of the 41 RCTs of machine learning interventions done so far, no trials adhered to all CONSORT-AI reporting standards~\citep{plana2022randomized}. Furthermore, the RCTs included in that survey demonstrate many of the same design flaws and challenges we observe in ~\cite{imai2020experimental}.
%\citet{wilder2021clinical} run a ``clinical trial'' on an algorithm used to select peer leaders in a public health program from homelessness youth, and 
That being said, many of the RCTs cited suffer from the same design flaws and challenges we observe in ~\cite{imai2020experimental}, which we argue ignores the impact of experiment design choices on judge responsiveness.

\paragraph{Spillover effects.}
It is well-known, in the experimental economics literature \citep[][]{banerjee2009experimental,deaton2018understanding} for example, that insights from RCTs often fail to scale up when the intervention is applied broadly across the entire target population. This failure is often attributed to \emph{interaction} or \emph{interference} between decision subjects, and has also been referred to, in various contexts, as \emph{general equilibrium effects} or \emph{spillover effects}. In the potential outcomes model for causal inference, this is referred to as a violation of the Stable Unit Treatment Value Assumption (SUTVA) \citep{rubin1980discussion,rubin1990formal}. Recognizing the presence of interference, experimenters typically employ a two-level randomized experiment design \citep{duflo2003role,crepon2013labor} in order to estimate average treatment effects under different treatment proportions (e.g. proportion of job seekers receiving an intervention); treatment proportions are randomly assigned to the treatment clusters, e.g., the labor market. Although there are some exceptions in healthcare~\citep{gohil202113}, such experiment designs have not yet widely been applied to study human-algorithm decision making, where each treatment cluster is associated with a particular human decision-maker. 
%We discuss further related work on spillover effects and on lab studies in Appendix~\ref{app:further_rel}.
%

%In these studies, again, we see a similar pattern in experiment design to our main case study. We thus focus on our main case study for this initial exploration and hope to explore these other experimental domains in future work.


%% spillover effects


%