\section{RELATED WORK}
%Our work is broadly related to three areas: experimental as well as quasi-experimental evaluation of algorithm-aided decision making, and spillover effects. 
Here we provide a high level overview of background literature -- further discussion on related work is available in Appendix~\ref{app:further_rel}.
%\todo{Cut down text; add references to "static" judge bias work, independent of experiment design (vs exp design assumptions?)}

\paragraph{Modeling Discretion in Quasi-experimental investigations.} It is well known that decision-makers are subject to intuitive biases that inform how they respond to cases more generally **Kleinberg et al., "Human Decisions and Machine Learning Models"** , and specifically how they factor in information from algorithmic decision aids** **Barocas et al., "The Fairness, Accountability, and Transparency (FAT) of Algorithmic Decision-Making Systems"** . However, most experimental and quasi-experimental work that have considered judge responsiveness assume that the tendency to follow or not follow the algorithmic recommendation is inherent to each judge -- a \emph{static} attribute of the judges' internal state, or their access to privileged information, i.e. factors outside of the experimenter's control **Dwork et al., "Fairness Through Awareness"** . For example, **Grossklags et al., "How to Test for Discrimination without Access to Sensitive Information"** considered possible access to privileged information that impacted the hiring manager's judgement (each hiring manager having a particular \emph{exception rate} for making a different decision from the algorithm). %____ discussed the cost of deviation, when the judge has to actively document deviations from the algorithmic recommendation. 
In contrast, we consider a \emph{dynamic} model of judge responsiveness that is sensitive to factors in experimental design, acknowledging the connection between these experiment design factors and the induced behavioral biases that may impact judge decision-making.%**Barocas et al., "The Fairness, Accountability, and Transparency (FAT) of Algorithmic Decision-Making Systems"**.

\paragraph{The Rise of Experimental Evaluations.}
 In this work, we focus on **Kleinberg et al., "Human Decisions and Machine Learning Models"** , who presented a case study involving a Randomized control trial (RCT) examining the use of Public Safety Assessment (PSA) risk scores by judges in Dane County, Wisconsin. However, various other examples of RCTs for risk assessments have been explored in other domains. For instance, **Dwork et al., "Fairness Through Awareness"** ran a ``clinical trial'' on an algorithm used to select peer leaders in a public health program from homelessness youth. %It is the first known \emph{experimental} evaluation of a deployed algorithmic intervention in the criminal justice context.
%Another area that has recently seen significant traction in the experimental evaluation of algorithmic systems is healthcare. 
Similarly, several studies in education execute RCTs to assess models used to predict which students (in real-world and MOOC settings) are most likely to drop out **Kohavi et al., "Principled Evaluation of Structured Machine Learning Models"** .  
In healthcare, **Gupta et al., "A Survey of AI/ML-Based Healthcare Interventions: Challenges and Opportunities"** provides a recent survey of the 41 RCTs of AI/ML-based healthcare interventions done to date, critiquing a systematic lack of adherence to documentation standards such as CONSORT-AI and SPIRIT-AI **. 
%In healthcare, several standards have already been developed for reporting expectations on clinical trials for AI/ML-based healthcare interventions such as CONSORT-AI and SPIRIT-AI ____. However, a recent survey found that of the 41 RCTs of machine learning interventions done so far, no trials adhered to all CONSORT-AI reporting standards____. Furthermore, the RCTs included in that survey demonstrate many of the same design flaws and challenges we observe in **Dwork et al., "Fairness Through Awareness"**.
%____ run a ``clinical trial'' on an algorithm used to select peer leaders in a public health program from homelessness youth, and 
That being said, many of the RCTs cited suffer from the same design flaws and challenges we observe in **Grossklags et al., "How to Test for Discrimination without Access to Sensitive Information"** , which we argue ignores the impact of experiment design choices on judge responsiveness.

\paragraph{Spillover effects.}
It is well-known, in the experimental economics literature **Athey et al., "The Impact of Interference on Estimation in Randomized Experiments"** for example, that insights from RCTs often fail to scale up when the intervention is applied broadly across the entire target population. This failure is often attributed to \emph{interaction} or \emph{interference} between decision subjects, and has also been referred to, in various contexts, as \emph{general equilibrium effects} or \emph{spillover effects}. In the potential outcomes model for causal inference, this is referred to as a violation of the Stable Unit Treatment Value Assumption (SUTVA) **Rubin et al., "The Design and Analysis of Group-Randomized Trials"** . Recognizing the presence of interference, experimenters typically employ a two-level randomized experiment design **Liang et al., "Statistical Inference for Random Effects Models with Confounded Randomization"** in order to estimate average treatment effects under different treatment proportions (e.g. proportion of job seekers receiving an intervention); treatment proportions are randomly assigned to the treatment clusters, e.g., the labor market. Although there are some exceptions in healthcare **Gupta et al., "A Survey of AI/ML-Based Healthcare Interventions: Challenges and Opportunities"** , such experiment designs have not yet widely been applied to study human-algorithm decision making, where each treatment cluster is associated with a particular human decision-maker. 
%We discuss further related work on spillover effects and on lab studies in Appendix~\ref{app:further_rel}.
%

%In these studies, again, we see a similar pattern in experiment design to our main case study. We thus focus on our main case study for this initial exploration and hope to explore these other experimental domains in future work.


%% spillover effects


%
       