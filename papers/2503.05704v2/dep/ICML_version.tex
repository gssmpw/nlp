%%%%%%%% ICML 2024 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{subfigure}
\usepackage[subtle]{savetrees}
\usepackage{booktabs} % for professional tables
\usepackage{graphicx, centernot, amsmath, bbm, booktabs, multirow, array, capt-of, varwidth, natbib, amstext, tabularx}
%\DeclareMathOperator{\E}{\mathbb{E}}

\usepackage[utf8]{inputenc}
\usepackage{dirtytalk}
\usepackage{csquotes,enumitem}
\usepackage{graphicx}
\usepackage{amsmath,amsthm,amsfonts}
\usepackage{lipsum}
\usepackage[utf8]{inputenc}
\usepackage{hyperref}
\usepackage{float}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
    pdftitle={Overleaf Example},
    pdfpagemode=FullScreen,
    }
\usepackage{tikz}
\usetikzlibrary{bayesnet}
%\input{macros}

\include{notation}
%\documentclass{article}
%\usepackage[demo]{graphicx}
\usepackage{caption}
\usepackage{subcaption}


% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2024} with \usepackage[nohyperref]{icml2024} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2024}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[]{icml2024}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}



% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
%\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Designing Experimental Evaluations 
of Algorithmic Interventions}

\begin{document}

\twocolumn[
\icmltitle{Designing Experimental Evaluations 
of Algorithmic Interventions \\
with Human Decision Makers In Mind}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2024
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Firstname1 Lastname1}{equal,yyy}
\icmlauthor{Firstname2 Lastname2}{equal,yyy,comp}
\icmlauthor{Firstname3 Lastname3}{comp}
\icmlauthor{Firstname4 Lastname4}{sch}
\icmlauthor{Firstname5 Lastname5}{yyy}
\icmlauthor{Firstname6 Lastname6}{sch,yyy,comp}
\icmlauthor{Firstname7 Lastname7}{comp}
%\icmlauthor{}{sch}
\icmlauthor{Firstname8 Lastname8}{sch}
\icmlauthor{Firstname8 Lastname8}{yyy,comp}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{yyy}{Department of XXX, University of YYY, Location, Country}
\icmlaffiliation{comp}{Company Name, Location, Country}
\icmlaffiliation{sch}{School of ZZZ, Institute of WWW, Location, Country}

\icmlcorrespondingauthor{Firstname1 Lastname1}{first1.last1@xxx.edu}
\icmlcorrespondingauthor{Firstname2 Lastname2}{first2.last2@www.uk}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
Automated decision systems (ADS) are broadly deployed to inform or support human decision-making across a wide range of consequential contexts. An emerging approach to the assessment of such systems is through experimental evaluation, which aims to measure the causal impacts of the ADS deployment on decision making and outcomes. However, various context-specific details complicate the goal of establishing meaningful experimental evaluations for algorithmic interventions. Notably, current experimental designs rely on simplifying assumptions about human decision making in order to derive causal estimates. In reality, cognitive biases of human decision makers \emph{induced} by experimental design choices may significantly alter the observed effect sizes of the algorithmic intervention.
In this paper, we formalize and investigate various models of human decision-making in the presence of a predictive algorithmic aid. We show  that each of these behavioral models produces dependencies across decision subjects and results in the violation of existing assumptions, with consequences for treatment effect estimation. 
%This work aims to further advance the scientific validity of intervention-based evaluation schemes for the assessment of algorithmic deployments.
\end{abstract}


\section{Introduction}

%As ~\cite{kleinberg2018human} states, ``There is significant distance between constructing a prediction algorithm and knowing that a decision aid based on it can improve outcomes.'' ~\cite{albright2019if} goes further to elaborate: ``Human discretion means that there is not necessarily a one-to-one mapping from the predictive tool to the final outcome.''  As a result, simply analyzing model outcomes will not necessarily reveal the full pattern of how humans \emph{interact} with the algorithm~\cite{green2019disparate}, and thus which decisions they will make to influence the outcomes for the impacted population. ~\cite{kleinberg2018human} goes further to illustrate how, unlike prediction evaluation which focuses on minimizing model generalization error, decisions involve a more complex calculation of cost-benefit trade-offs, requiring considerations beyond just model performance

 Once deployed, algorithmic decision aids operate much more like typical policy interventions, not isolated prediction problems. Recent work reveals a significant gap between developing predictive algorithms and ensuring they improve outcomes through better decision-making \citep{kleinberg2018human, liu2023reimagining, liu2024actionability}, often under the influence of intermediate factors such as human discretion~\citep{albright2019if}. 
%While the field of machine learning is focused on the evaluation of predictive models using accuracy metrics, often on benchmark datasets \citep{raji2021ai}, simply assessing the predictive models themselves do not necessarily reveal the full pattern of how humans \emph{interact} with the algorithm~\cite{green2019disparate}, and thus which decisions they will make to influence the outcomes for the impacted population. ~\cite{kleinberg2018human} goes further to illustrate how, unlike prediction evaluation which focuses on minimizing model generalization error, decisions involve a more complex calculation of cost-benefit trade-offs, requiring considerations beyond just model performance. 

%The difference between \emph{predictions} and \emph{decisions} - also sometimes referred to as \emph{judgements} ~\cite{agrawal2018prediction} - thus necessitates a critical expansion of scope in terms of what is being evaluated.
%Rather than scoping down to the output of a particular algorithm, the focus is instead on the outcomes of overall policy changes to the entire system. 
  
%This paradigm is about hypothesis testing a treatment - effectively thinking through the counterfactual measurement of what happens in the presence and in absence of the algorithmic intervention for a given decision-maker on a specific case. What we actually need to evaluate is not just the accuracy of the models' predictions, but the impact of such predictions on the actions of decision-makers and the appropriateness of those decisions.
These \emph{algorithmic interventions} are thus ideally evaluated around the causal question of how much the introduction of the algorithm impacts important downstream decisions and outcomes. One obvious approach to estimate these causal effects is through \emph{experimental evaluations}. Several efforts in domains including criminal justice~\cite{imai2020experimental}, healthcare~\cite{plana2022randomized}, and education~\citep{dawson2017from, borrella2019predict} have conducted randomized experiments to estimate the impact of introducing an algorithmic intervention to modify a given default policy. 

Prior work follow the standard protocol in causal inference for assessing interventions, but do not necessarily account for the particularities of an \emph{algorithmic} intervention---human decision making with an algorithmic input---in their \emph{experiment design}. We argue that experiment design choices impact the \emph{responsiveness} of the judges relying on algorithmic recommendations (i.e., their tendency to be swayed by an algorithmic decision aid), which in turn distorts our understanding of the treatment effect of algorithmic interventions. 

\textbf{Our contributions are as follows:}
1. We examine the role of specific \emph{experiment design choices} in judge responsiveness -- specifically, choices that experiment designers make on (1) the treatment assignment model ($Z$), (2) the positive prediction rate  ($P (\hat{Y} = 1)$) (by setting the threshold of the model with respect to recommended action) and (3) the model correctness ($P (\hat{Y} = Y)$) (through model selection). We conclude that beyond intrinsic judge biases, experimentation design choice can also impact judge responsiveness. 

2. We mathematically formalize how such experimental design choices impact judge responsiveness via novel models of judge decision making and prove that this leads to violations of SUTVA. We furthermore discuss how these design choices and their impact on judge responsiveness can lead to a mis-estimation of the average treatment effect in experimental settings. 

3. Using existing experiment data of an algorithmic intervention \citep{imai2020experimental}, we simulate a scenario of modifying experimental design choices and observing differences in judge decisions and estimated average treatment effect. 

%\section{Related Work}


%\paragraph{Experimental Evaluations}
%The involved case study in this paper is an experimental randomized control trial study of the use of Public Safety Assessment (PSA) risk scores by judges in Dane county, Wisconsin~\cite{imai2020experimental}, and is actually one of the first known ~\emph{experimental} evaluations of an algorithmic intervention in the criminal justice context.

%It is noteworthy that another area where the experimental evaluation of algorithmic systems has gained much traction recently has been in healthcare. Several standards have already been developed for reporting expectations on clinical trials for AI/ML-based healthcare interventions such as CONSORT-AI and SPIRIT-AI ~\cite{liu2020reporting}. However, a recent survey found that of the 41 RCTs of machine learning interventions done so far, no trials adhered to all CONSORT-AI reporting standards~\cite{plana2022randomized}. Furthermore, the RCTs included in that survey demonstrate many of the same design flaws and challenges we observe in ~\cite{imai2020experimental}.

%Randomized control trials for risk assessments have been explored in other contexts. For example, \citet{wilder2021clinical} run a ``clinical trial'' on an algorithm used to select peer leaders in a public health program from homelessness youth. Similarly, several studies in education execute RCTs to assess the effectiveness of interventions where a model is used to predict which students (in real-world and MOOC settings) are most likely to drop out~\citep{dawson2017from, borrella2019predict}.  




%\paragraph{Quasi-experimental investigations} In quasi-experimental investigations, most work that have considered judge responsiveness assume that the tendency to follow or not follow the algorithmic recommendation is inherent to each judge---an inherent attribute of the judges' internal state or their access to privileged decision and factors outside of the experimenter's control \citep{mullainathan2022diagnosing,albright2019if}. \citet{hoffman2018discretion} consider possible access to privileged information that impacted the hiring manager's judgement (and considers each hiring manager to have a particular \emph{exception rate} for making a different decision from the algorithm). \citet{angwin2016machine} discussed the cost $\eta$ of deviation, since the judge has to actively log deviations from the algorithmic recommendation. 


%\paragraph{Spillover effects}
%It is well-known, in the experimental economics literature \citep[][]{banerjee2009experimental,deaton2018understanding} for example, that insights from randomized controlled trials (RCT) often fail to scale up when the intervention is applied broadly across the entire target population. This failure is often attributed to \emph{interaction} or \emph{interference} between units, and this phenomenon has also been referred to, in various contexts, as \emph{general equilibrium effects} or \emph{spillover effects}. In the potential outcomes model for causal inference, this is referred to as a violation of the Stable Unit Treatment Value Assumption (SUTVA) \citep{rubin1980discussion,rubin1990formal}. Recognizing the presence of interference, experimenters typically employ a two-level randomized experiment design \citep{duflo2003role,crepon2013labor} in order to estimate average treatment effects under different treatment proportions (e.g. proportion of job seekers receiving an intervention); treatment proportions are randomly assigned to the treatment clusters, e.g., the labor market. Such experiment designs have not yet been applied to study human-algorithm decision making, where each treatment cluster is associated with a particular human decision-maker. We discuss further related work on lab studies and on spillover effects in Appendix~\ref{app:further_rel}.
%

%In these studies, again, we see a similar pattern in experiment design to our main case study. We thus focus on our main case study for this initial exploration and hope to explore these other experimental domains in future work.


%% spillover effects


\section{Existing experimental paradigm}

%We are interested in the problem of experimentally evaluating how algorithmic interventions in the form of predictive decision aids influence human decision-making.
%In this section, we describe the existing paradigm for experimental evaluations---we call this the \emph{case-independent} model---and the common assumptions required for causal identification. 

 In many existing experimental evaluations of algorithmic decision aids, the \emph{treatment unit} is presumed to be the decision subject and the \emph{treatment}, $Z_i$ is conceptualized as the provision of an predictive risk score $\hat{Y}_i$ to a decision maker who is responsible for making the final decision $D_i$. 
 
 In the case of pre-trial detention decisions, the decision subject is the defendant in a particular court case, the decision maker is the judge presiding on the case, and the treatment $Z_i$ is a binary variable indicating if the PSA is shown to the judge for case $i$. $D_i$ represents the judge's binary detention decision, where $1$ is for detaining and $0$ for releasing before trial. $Y_i$ is the binary outcome variable, with $1$ indicating the arrestee later committed an NCA, and $0$ indicating otherwise. $X_i$ is a vector of observed pre-treatment covariates for case $i$, including age, gender, race, and prior criminal history. Independence is assumed between each case. We illustrate these variables and their dependencies (or lack thereof) in Figure~\ref{fig:indep_causal_model}.

 
 \begin{figure}[htbp]
  \centering
 \includegraphics[trim=1.5cm 2.5cm 3cm 10.5cm, clip, width=\columnwidth]{alg_int_neurips24_bw_imai_simple.pdf}
  \caption{The case-independent model of human decision making with a predictive decision aid. This is the causal model assumed in prior work \citep[e.g.,][]{imai2020experimental}}
  \label{fig:indep_causal_model}
\end{figure}

The crucial assumption here is that of non-interference between treatment units (see Assumption~\ref{assmp:sutva}, SUTVA). In words, the treatment status of one unit should not affect the decision for another unit. As a result, $Z_i$ is typically randomized at a single-level, i.e., on individual cases; and there is no randomization at the level of decision makers. %This means that, for a random selection of cases, the algorithmic recommendation or result is shown to the corresponding decision-maker. 
In \citet{imai2020experimental} for example, where only one judge participated in the study, the algorithmic score is shown for even-numbered cases (i.e., $Z_i =1$); and not shown for odd-numbered cases (i.e., $Z_i =0$). Practically, assuming non-interference across units is questionable -- in psychology and behavioural economics, it is well-known that decision-makers have a consistency bias~\cite{tversky1985framing}. This could mean that when a judge only sees the risk score for a fraction of the cases, they might choose to ignore the risk score completely on cases where they do see it, or to infer patterns for cases where they don’t see the score, in order to be as consistent as possible in their decision-making (in this work, we mainly model effects such as the former). 

%This assumption of non-interference across units is questionable -- in psychology and behavioural economics literature, it is well-known that decision-makers have a consistency bias~\cite{tversky1985framing}. This could mean that when a judge only sees the risk score for a fraction of the cases, they might choose to ignore the risk score completely on cases where they do see it, or to infer patterns for cases where they don’t see the score, in order to be as consistent as possible in their decision-making (in this work, we mainly model effects such as the former). %Such case-based interventions thus allow for only a \emph{\textbf{partial treatment for the decision-makers under case randomization}}.
%As such, experiment designs that assume the case-independent model does not allow us to draw clear conclusions on how a judge would modify their default decision-making in response to the algorithmic intervention implemented as a full-scale \emph{policy}, since we only observe the judge's decision-making under a \emph{partial} implementation of the would-be algorithmic intervention.

%In the next section, we describe our causal model of judge decision making and use it to demonstrate how design decisions about the experiment can impact judge responsiveness in ways that impact treatment effect estimates. 


 




\section{Model}
%In this section, we propose a novel causal model of algorithm-aided human decision making that is distinct from case-independent model, as depicted in Figure~\ref{fig:indep_causal_model}.

We propose a novel causal model of algorithm-aided human decision making (Figure~\ref{fig:expanded_causal_model}), that accounts for dependence in the decisions across cases induced by the human decision maker's cognitive bias. 
%We consider three types of cognitive bias that are particularly relevant to the current setting of human decision making with a predictive decision aid---all of which are directly influenced by experimental design choices. 
We do so by introducing two latent variables $J_{i,k}$ and $\epsilon_{i,k}$ that track the internal state of the decision maker $k$ and how it affects the decision for subject~$i$.

\begin{figure}[htbp]
  \centering
  \includegraphics[trim=1.5cm 2cm 3cm 7cm, clip, width=\columnwidth]{alg_int_neurips24_bw.pdf}
  \caption{Proposed causal model that accounts for human decision maker bias. We describe three versions of this model (see Table~\ref{table:cognitive_bias_models}). Under the \emph{treatment exposure} model, arrow (i) is activated, but not (ii) and (iii). Under the \emph{capacity constraint} model, arrows (i) and (ii) are activated, but not (iii). Under the \emph{low trust} model, all three arrows, (i-iii), are activated.}
  \label{fig:expanded_causal_model}
\end{figure}

As in the case-independent model, we assume that $X_i$'s are independent and identically distributed.
In contrast to the case-independent model that does not include the role of the human decision maker (i.e., the judge), we explicitly model the judge---we index each case decision $D_{i,k}$ by both the decision maker (index $k$ in Figure~\ref{fig:expanded_causal_model}) and the decision subject (index $i$ or $\ell$ in Figure~\ref{fig:expanded_causal_model}), as opposed to only indexing it by the decision subject. Similarly, the treatment assignment variable $Z_{i,k}$ denotes the treatment status of both the decision subject $i$ and the decision maker $k$. %This provides a fuller account of the space of possible treatment assignment counterfactuals, e.g., a case could have been assigned to a different judge leading to a different decision and outcome, all else held constant. 
We further assume that the judge is sequentially exposed to cases, where the judges consider case $i$ after the case $i-1$, and one case at a time. As the decision outcome $Y_{i,k}$ is downstream of $D_{i,k}$, we also index it by both the judge and the decision subject.

%To be precise, we define
%\begin{equation*}
%    Z_{i,k}
% = 
%\begin{cases}
%1 \text{ if unit } i \text{ is assigned to Judge }k \text{ and treated}\\
%0 \text{ if unit } i \text{ is assigned to Judge }k \text{ and untreated} \\
%-1 \text{ if unit } i \text{ is not assigned to Judge }k 
%  \end{cases}
%\end{equation*}  



\subsection{Judge's decision}\label{sec:judge_dec}

We model the judge's decision for each unit in the experiment as a random event, whose probability is determined by the individual judge's decision parameters as well as aspects of the experimental design (e.g. the treatment assignments).
We assume for each judge $k$, there exists a default decision function (in absence of any algorithmic decision aid) $\lambda_k$ that takes in observable characteristics $X_i \in \calX$. When no predictive decision aid is provided to the unit $i$ assigned to judge $k$ (i.e., $Z_{i,k} = 0$), the judge follows her default decision process and her decision $D_{i,k}$ takes value $\lambda_k(X_i)$. If the unit $i$ is treated (i.e., $Z_{i,k} = 1$); the judge is provided with the algorithmic score $\hat{Y}_i$. We denote as $\bar{D}(y)$ the recommended decision function mapping a prediction $y$ to a decision (this models, for example, existing judge decision guidelines for PSA scores).

In the case that $\recdec{i}$ differs from the judge's default decision $\lambda_k(X_i)$, the judge may choose to `comply' with the $\recdec{i}$, or to disagree with the ADS recommendation and choose $\lambda_k(X_i)$. We model this as a random event as follows. Let $J_{i,k}$ denote the \emph{responsiveness} of Judge $k$ for treatment unit~$i$, i.e. the probability that the Judge follows the PSA recommendation for case $i$. In other words, $J_{i,k}$ models the \emph{automation bias} of the judge at the point they are deciding on unit $i$. Let $\epsilon_{i,k} \sim Ber(J_{i,k})$ be the random variable denoting judge response, drawn independently for each $i$; when $\epsilon_{i,k} = 1$, the judge  `complies' with the ADS recommendation. To summarize, we have the following decision for judge $k$ on unit $i$:
\begin{equation}\label{eq:model_of_judge}
    D_{i,k} = \begin{cases}
        \lambda_k(X_i) \text{ if } Z_{i,k} = 0 \text{ (no PSA) } \\
        \begin{cases}
     \recdec{i} &\text{if } \recdec{i} = \lambda_k(X_i)  \text{ or } \epsilon_{i,k}=1 \\
        \lambda_k(X_i) &\text{if }\recdec{i} \ne \lambda_k(X_i) \text{ and } \epsilon_{i,k}=0 
      %  \recdec{i} & \text{o.w.} %&\text{ w.p. $J_k$}
        \end{cases} \text{o.w.}
    \end{cases}.
\end{equation}
% Equivalently:
% \begin{equation}\label{eq:model_of_judge}
%     D_{i,k} = \left( \bar{D}(\hat{Y}_i)\cdot \epsilon_{i,k} + \lambda_k(X_i) \cdot (1-\epsilon_{i,k})\right) \cdot Z_{i,k} + \lambda_k(X_i) \cdot (1- Z_{i,k})
% \end{equation}

%Note : think of alternative to $\epsilon$

% \lnote{What happens when $\recdec{i} = \lambda_k(X_i)$?
% Alternative:
% \begin{equation*}
%     D_{i,k} = \begin{cases}
%         \lambda_k(X_i) \text{ if } Z_{i,k} = 0 \text{ (no PSA) } \\
%         \begin{cases}
%         \lambda_k(X_i) &\text{if } \epsilon_{i,k}=0 ~\text{(Judge does not comply) and } \lambda_k(X_i) \neq \recdec{i}\\
%         \recdec{i} & \text{o.w.} %&\text{ w.p. $J_k$}
%         \end{cases} \text{o.w.}
%     \end{cases}
% \end{equation*}
Note that in this case $J_{i,k}$ denotes the probability that a judge decides to follow $\recdec{i}$, in the case that $\lambda_k(X_i) \neq \recdec{i}$. Hence the judge responsiveness $J_{i,k}$ is a lower bound for actual frequency that the judge is makes the same decision as the algorithmic recommendation. 
% \footnote{
% Consider $\eta_k:=\Pr\{\lambda_k(X_i) = \recdec{i}\}$ the natural agreement rate between the judge's default and the ADS decision. 
% The following expression is the probability that the judge makes a different decision from ADS:
% \begin{align*}
%     \Pr(D_{i,k} \neq \recdec{i} \mid Z_{i,k} = 1) = \Pr(\{\lambda_k(X_i) \neq \recdec{i}\} \cap \{ \epsilon_{i,k}=0 \})&= \Pr\{\lambda_k(X_i) \neq \recdec{i}\}\cdot \Pr  \{ \epsilon_{i,k}=0 \}\\
%     &\leq (1-\eta_k)\cdot (1- J_{i,k})
% \end{align*}
% }


We propose that the following three \emph{responsiveness factors} are likely to impact judge response: 1) Treatment exposure, 2) capacity constraint, and 3) low trust. We model each of these factors independently for ease of exposition but they may impact the judge's response at the same time. 

\paragraph{Treatment exposure model.} The judge becomes responsive to the algorithmic recommendation if she encounters the algorithmic recommendation for more units (i.e., greater fraction of units are treated).
\begin{equation}\label{eq:treatment_exposure}
    J_{i+1,k} = %b_k + P(Z_{i,k}) \approx 
b_k + f\left(\frac{1}{i} \sum_{m=1}^{i} Z_{m,k}\right)
\end{equation}
$b_k$ is the baseline responsiveness and $f\left(\frac{1}{n} \sum_{m=1}^{i} Z_{m,k}\right)$ is the adjustment to the responsiveness based on number of treated units seen so far. %\lnote{also: $b_k$ is default automation bias. $J_{i,k}$ is total automation bias.}

%Judges may have a bias towards the consistency of their decision making process, making them more predisposed to following the algorithmic recommendation if they have a higher exposure to it.

To illustrate, a particular instantiation of this model is based on a simple thresholding of the average number of treated units:
\begin{equation}
J_{i,k}=
    \begin{cases}
        1 & \text{if } \sum_{m=1}^{i} Z_{m,k} > i\tau\\
        0 & \text{o.w.}
    \end{cases},
\end{equation}
where $\tau \in (0,1)$. This instance of the judge's decision making model suggests that the judge becomes always responsive to the algorithmic recommendation as long as the cumulative average treatment frequency is above $\tau$. We include simple instances of the following two models in Appendix~\ref{app:instance}.

\paragraph{Capacity constraint model} The judge has a limited capacity to respond to positive (or `high risk') predictions. They reduce their responsiveness as the rate of positive predictions they see from the algorithm increases. 
\begin{equation}\label{eq:capacity}
    J_{i+1,k} = b_k - f\left(\frac{1}{i}\sum_{m=1}^iZ_{m,k} \cdot \Ind{\hat{Y}_m > 0}\right)
\end{equation}


%The model is not addressing the accuracy of the predictions in particular, but the capacity of the human decision maker to respond to the high number of `high risk' predictions; it leads to them becoming more likely to dismiss the predictions. Specifically, it doesn't make any assumption about whether the human decision maker is observing realized outcomes~$Y_{i,k}$. Hence this is a different model of cognitive bias than the following model, which addresses low accuracy directly.

\paragraph{Low trust model} The judge observes realized outcomes and knows when the algorithm has made a mistake (i.e., $\hat{Y} \ne Y$). They reduce their responsiveness as the error rate of the predictions they see from the algorithm increases. 
\begin{equation}\label{eq:low_trust}
    J_{i+1,k} = b_k - f\left(\frac{1}{i}\sum_{m=1}^iZ_{m,k} \cdot \Ind{\hat{Y}_m \neq Y_{m,k}}\right)
\end{equation}
This is related to the phenomenon of notification fatigue for ADS with high false positive rates \citep{wong2021external}. Note that this model cannot be applied to settings where the judge does not observe the realized outcome before the next decision, or if they only observe outcomes for a particular decision type, e.g., $Y_{i,k}(D_{i,k} = 1)$.

\iffalse
Additional assumptions:
\begin{itemize}
    \item $J_k = J_{k,t}$ is likely time varying, and learnt in some online manner. However, if random, it's possible that things will converge at some static behavior after a certain amount of time has passed. 

    \item $\hat{Y}_{i}$ does not depend on $J_k$. 
\end{itemize}
\fi



\section{Results: Experiments under Judge models}

%In this section, we describe the implications that our model of human decision making has for simple conditions under which the common assumption for treatment effect estimation, Stable Unit Treatment Value Assumption (SUTVA), is violated. Second, we show that the underestimation of treatment effect that occur due to the chosen randomization in the assignment of cases to judges, even when the treated cases are the same, focusing on the treatment exposure model.

\subsection{Violation of SUTVA}

Our first observation is that the judge's changing responsiveness induces interference between units. We will formally show that this results in the violation of SUTVA, stated as follows. 
\begin{assumption}[SUTVA \citep{rubin1990formal,angrist1996identification}]\label{assmp:sutva}
A set of treatment assignments, decisions and outcomes $(\vect{Z}, \vect{Y}, \vect{D})$ is said to satisfy SUTVA if both the following conditions hold.
\begin{enumerate}
    \item[a.] If $Z_{i,k} = Z_{i,k}'$, then $D_{i,k}(\vect{Z}) = D_{i,k}(\vect{Z}')$.
    \item[b.] If $Z_{i,k} = Z_{i,k}'$ and $D_{i,k} = D_{i,k}'$, then $Y_{i,k}(\vect{Z}, \vect{D}) = Y_{i,k}(\vect{Z}',\vect{D}')$.
\end{enumerate}
\end{assumption}

In the following result, we show that under simple conditions on $J_{i,k}$, Assumption~\ref{assmp:sutva} (part a) is violated. The proof hinges on the lack of conditional independence between $D_{i,k}$ and prior treatment assignments $(Z_{1,k}, \cdots, Z_{i-1,k})$, given current treatment $Z_{i,k}$. This may be intuitively apparent from the causal DAG in Figure~\ref{fig:expanded_causal_model}, where one can see that $D_{i,k}$ is not d-separated from $(Z_{1,k}, \cdots, Z_{i-1,k})$ by $Z_{i,k}$. We include the proof in Appendix~\ref{app:proofs}.

\begin{theorem}[Violation of SUTVA]\label{thm:sutva_violation}
    Fix $k>0$ and consider some $i > 1$. Assume the judge's decision model is as described in equation~\eqref{eq:model_of_judge}, where $J_{i,k}$ is a monotonically non-decreasing (or non-increasing) function of $Z_{1,k}, \cdots, Z_{i-1,k}$, and strictly increasing (resp. decreasing) in at least one of its arguments. Assume that the judge's default decision function $\lambda_k$ is such that $\Pr(\lambda_k(X_i) \neq \recdec{i}) > 0$.
    
    Then $D_{i,k}$ is not conditionally independent of $(Z_{1,k}, \cdots, Z_{i-1,k})$, given $Z_{i,k}$. In particular, there exists treatment assignments $\vect{Z}, \vect{Z}'$ such that $Z_{i,k} = Z_{i,k}'$ and 
    \begin{equation*}
        \Pr\left(D_{i,k}(\vect{Z}) = D_{i,k}(\vect{Z}')\right) < 1.
    \end{equation*}
\end{theorem}


Theorem~\ref{thm:sutva_violation} suggests that for the specific models of $J_{i,k}$ that we introduced in Section~\ref{sec:judge_dec}, the SUTVA is indeed violated. We state this formally in the corollary and proof described in Appendix~\ref{app:proofs}.

We note that under our causal model, the second part of the SUTVA (Assumption~\ref{assmp:sutva}, part b) is actually not violated. This is because $Y_{i,k}$ is conditionally independent of $(Z_{1,k}, \cdots, Z_{i-1,k})$ and  $(D_{1,k}, \cdots, D_{i-1,k})$ given $D_{i,k}$, as seen from Figure~\ref{fig:expanded_causal_model}.

%\citet{imai2020experimental} argued that there was no statistically significant spillover effect in their experiment, hence supporting the assumption of SUTVA. However, the question of whether a conditional randmization test such as a permutation test can detect the violation SUTVA is intricate. 
Permutation tests typically reorder data to assess the null hypothesis, but in this case, the order does not affect the judge's average exposure to treatment, $\E[Z_{1,k}]$. Indeed, $J_{i,k}$ is a function of an average over i.i.d. variables as defined in \eqref{eq:treatment_exposure}, \eqref{eq:capacity}, or \eqref{eq:low_trust}, and it becomes effectively constant for all cases after a certain case count $i > T$, given the law of large numbers. This convergence in $J_{i,k}$ implies that reordering would not alter the joint distribution of $D_{i,k}$'s for $i$ large enough. The conditional randomness test used by \citet{imai2020experimental}, for example, randomizes case order while maintaining the average treatment proportion, and hence 
cannot effectively detect interference between units that is induced by $J_{i,k}$. %This leads authors to incorrectly argue that that there was no statistically significant spillover effect in their experiment.

%ing only the spillover effects that are contingent on 

% Comment: Can we detect this SUTVA violation via a permutation test? 

% \begin{enumerate}
%     \item Can we detect this SUTVA violation via the permutation test?
%     \begin{itemize}
%         \item Changing the order of examples doesn't matter in this case, because it doesn't change the degree of exposure that defines $J_k$.
%         \item After a certain time $t > T$, $J_k$ is actually the same for every case $i$.
%         \item The determination of spillover effects does not mean that each case is independent of each other. 
%         \item The conditional random test does not modify the treatment proportion because the test keeps the same average treatment proportion, and only randomizes the spillover effect of changing the order of cases. This is in general true for the kind of spillover effect that depends on the proportion of treated units. We care about cumulative (all the cases prior) vs one-bit information (just the case before) on the impact on current case, and so actually not just on the previous case. 
%         \item If we want to something about the power of the test, look at distribution of p-values; ie. you  neighbor effecting you and so changing neighbors vs collective impact of unit treatment to that point. 
%         \item look at the test that Imai uses, where they have the same proportion of treated across null and alternative hypothesis. Cannot change the proportion of treated units. 
%     \end{itemize}
% \end{enumerate}


\subsection{Estimation of causal effects}

%Having shown that SUTVA is violated under a model of judge bias, we now investigation the implications on the estimation of causal effects.
%Our goal in this section is to illustrate the underestimation of the treatment effect of predictive decision aids on the human decision that may occur, if interference due to judge bias is not taken into account in the experiment's randomization scheme. 
%We discuss a worked example focusing on the \emph{treatment exposure} model. %, leaving detailed investigations of the \emph{capacity constraint} and \emph{low trust} models to future work.

%For a worked example focusing on the \emph{treatment exposure} model, consider two different treatment assignments (to $n$ total units) that have the same treated and untreated units, but assign these treated units to different decision makers (and we assume each case is only seen by one judge in each treatment assignment). For simplicity, we consider two judges, $k=1,2$, who are assumed to be identical, apart from their assigned cases. Specifically, both of them have decision making model as described in equation~\eqref{eq:treatment_exposure}, where $f(z) = az$ is a linear function and $b_1 = b_2 = b$.

For a worked example focusing on the \emph{treatment exposure} model, consider two different treatment assignments (to $n$ total units) that have the same treated and untreated units to two judges, $k=1,2$, who are assumed to be identical, apart from their assigned cases and act on the decision making model as described in equation~\eqref{eq:treatment_exposure}, where $f(z) = az$ is a linear function and $b_1 = b_2 = b$. The two considered treatments assignment scenarios are as follows:

\begin{enumerate}
    \item (Uniform randomization) $\vect{Z} =(\vect{Z}_{\cdot, 1},\vect{Z}_{\cdot, 2})$ such that Judge 1 receives 50\% of total cases, 50\% of them are treated and 50\% are untreated. Judge 2 receives other 50\% of total cases, 50\% of them are treated and 50\% are untreated.
    \item (Two-level randomization) $\vect{Z}' = (\vect{Z'}_{\cdot, 1},\vect{Z'}_{\cdot, 2})$ such that Judge 1 receives 50\% of total cases, 100\% of them are treated. Judge 2 receives other 50\% of total cases, 100\% are untreated.
\end{enumerate}
%The above two treatment assignments each represent a different randomization scheme: 1) single-level randomization by decision subjects (cases) only, or 2) two-level randomization by decision makers and decision subjects. This is illustrated in Figure~\ref{fig:comparison_randomization_level}.


%\begin{figure}[ht]
%	\centering	
%\includegraphics[width=0.8\linewidth]{case_level_intervene.png}
%  \includegraphics[width=0.8\linewidth]{decision_maker_intervene.png}
%\caption{All observed experimental designs randomize the treatment for the algorithmic intervention at (top) the case level, and not (bottom) the decision-maker level.}\label{fig:comparison_randomization_level}
%\end{figure}

%We want to study the gap in the (conditional) treatment effects under uniform randomization vs. two-level randomization, e.g., by lower bounding the gap.


% Recall the definition of the average treatment effect (ATE) of treatment $Z_{i,k}$ on decisions $D_{i,k}$:
% \begin{equation}
%     ATE := \E[D(Z = 1) - D(Z = 0)]
% \end{equation}
In our two hypothetical experiments, suppose we use the following standard estimator of ATE:
\begin{equation}
    \widehat{ATE} := \frac{2}{n}\sum_{i=1}^n\sum_{k=1}^2D_{i,k}\Ind{ Z_{i,k} = 1} - D_{i,k} \Ind{Z_{i,k} = 0}
\end{equation}
% Given that units are randomly assigned to treatment (with half of units being treated), $\widehat{ATE}$ appears to be an unbiased estimate of $ATE$. Yet, under our model of $D_{i,k}$ (e.g., \eqref{eq:treatment_exposure}), the $ATE$ is clearly under-specified, as $D_{i,k}$ in general depends not only on $Z_{i,k}$ but also $\{Z_{1, k}, \cdots, Z_{i-1, k}\}$. If the goal is to estimate the treatment effect of providing a predictive decision aid to a decision maker \emph{consistently}, we would really like to estimate the average treatment effect under total treatment.
% \begin{equation}
%     \overline{ATE} := \E[D(\vect{Z} = \vect{1}) - D(\vect{Z} = \vect{0})].
% \end{equation} In that case, the standard estimator $\widehat{ATE}$ would suffer from large bias if used to estimate $\overline{ATE}$, resulting in underestimation of the treatment effect.

The following proposition shows that there can be a significant gap between the expectation of the $\widehat{ATE}$ estimator under different treatment assignments. Specifically, we compare $\widehat{ATE}_{\text{uniform}}:=\widehat{ATE}(\vect{Z})$ and that of $\widehat{ATE}_{\text{two-level}}:=\widehat{ATE}(\vect{Z}')$. We include the proof in Appendix~\ref{app:proofs}.

\begin{proposition}[Estimated treatment effects under treatment exposure model]\label{prop:treatment_effect}
Assume both judge 1 and 2's decision model is as described in equation~\eqref{eq:model_of_judge}, and $J_{i,k}$ follows \eqref{eq:treatment_exposure}, with $b_{k} = b \in(0,1) \forall k$ and $f(x) = ax, a \in (0,1-b)$. Suppose we have $\E[\bar{D}(\hat{Y}_1) - \lambda_k(X_1)] = \rho > 0$. Then
\begin{equation}
 \E[\widehat{ATE}_{\text{two-level}}]- \E[\widehat{ATE}_{\text{uniform}}] = \frac{a \cdot \rho }{2}
\end{equation}
\end{proposition}


Intuitively, the gap between the two estimates increases as 1) the judge responsiveness is more sensitive to past exposure ($a$ is larger) and 2) the expected difference between the judge's default decision and the algorithmically recommended decision increases ($\rho$ is larger). % We include further details and the proof in Appendix~\ref{app:proofs}.


% Individual treatment effect for $k \in \{1,2\}$:
% \begin{align*}
%     ITE(X_i, \vect{Z}) = &\E[D_{i,k}(Z_{i,k} = 1) - D_{i,k}(Z_{i,k} = 0) \mid X_i, \vect{Z} ] \\
%     =& J_{i,k}(X_i,\vect{Z}) \cdot (\bar{D}(\hat{Y}_i) - \lambda_k(X_i))
% \end{align*}


% Differences in ITE:
% \begin{align}
%     ITE(X_i, \vect{Z})-ITE(X_i, \vect{Z}') &= \left(J_{i,k}(X_i,\vect{Z}) -J_{i,k}(X_i,\vect{Z}')\right) \cdot \left(\bar{D}(\hat{Y}_i) - \lambda_k(X_i)\right) \\
%     &= \left(f\left(\frac{1}{i} \sum_{m=1}^{i} Z_{m,k}\right)- f\left(\frac{1}{i} \sum_{m=1}^{i} Z'_{m,k}\right) \right) \left(\bar{D}(\hat{Y}_i) - \lambda_k(X_i)\right) \\
%     &= \left(\frac{a}{i} \sum_{m=1}^{i} Z_{m,k}-Z'_{m,k}\right) \left(\bar{D}(\hat{Y}_i) - \lambda_k(X_i)\right)
% \end{align}
% \todo{
% \begin{enumerate}
%     \item Which individual do we compute this for? For some general {i}, just so we can talk accumulation of exposure; eg. we can't pick the first one, and after a certain large N it might converge? 
% \end{enumerate}
% }

% \todo{
% \begin{enumerate}
%     \item Compute $ITE(X_i, \vect{Z})-ITE(X_i, \vect{Z}')$ for treatment exposure model. explicitly define $J_{i,k}$
%     \item $\E[ITE(X_i, \vect{Z})-ITE(X_i, \vect{Z}') \mid Y_{i,k}(1) = 1, Y_{i,k}(0) = 1 ]$ ? conditioning on notion of correctness. Might require $Y$ model? Can we lower-bound this without explicit $Y$ model.
%     \item Candidate for Y model, from ~\cite{angrist1996identification} $$Y_i = \beta_{1} + \beta_{2}*D_i + \beta_{3}$$
% \end{enumerate}
% }



%In this section we illustrated the implications that different randomization schemes have on treatment effect estimation under our model of judge decision making bias. Although we have focused our investigation on estimation of the population treatment effect, our observation that treatment randomization schemes can introduce bias into the estimated treatment effects likely also extends to other types of treatment effects such as those based on principal stratification \citep[see e.g., Average Principal Causal Effects][]{imai2020experimental}. %In fact, estimands like the APCE are arguably even more relevant in the context of human decision making as they incorporate a notion of the `correctness' of the treatment effect on decisions, defined using the potential outcomes of decision outcomes, $Y_i(d)$. We empirically investigate how different judge models and treatment assignments affect the APCE estimates in Section~\ref{sec:expt}.

\section{Semi-synthetic Experiments}\label{sec:expt}

%We test our findings empirically by setting up semi-synthetic simulations, using the data from the experiment conducted in ~\cite{imai2020experimental}. 

Using existing experiment data~\cite{imai2020experimental}, we simulate alternative scenarios of one of the discussed experiment design choices -- treatment assignment. We simulate multiple judges, and a corresponding set of alternative decisions to observe the impact of these biases on estimations of the overall average treatment effect estimate on judge decisions, under different experimental design settings. Further details can be found in Appendix ~\ref{app:exps}, and all results are directly available in the uploaded Supplementary Materials. %In addition to reporting ATE effects, we include 

 \begin{figure}[htbp]
  \centering
 %\includegraphics[trim=1.5cm 2cm 3cm 10cm, clip, width=0.6\columnwidth]{alg_int_neurips24_bw_imai_simple.pdf}
 \includegraphics[clip, width=0.4\columnwidth]{aihuman_modifications_and_results/ATE_J3_333.png}
 \includegraphics[clip, width=0.4\columnwidth]{aihuman_modifications_and_results/ATE_J3_613.png}
  \caption{We empirically observe changes to the ATE estimate under different treatment assignments. In Figure (a), we see the ATE for a scenario where three judges are assigned $Z=1$ in about 33.3\% of their assigned cases. In Figure (b), $J_1$ is assigned $Z=1$ in about 60\% of their cases, while $J_2$ and $J_3$ are assigned $Z=1$ in about 10\%, and 30\% of their cases respectfully. Under our model, if a judge is likely to ignore the PSA if they see it less than 60\% of the time, then the ATE is higher for case (b), where at least one judge is guaranteed to see the PSA at that frequency.}
  \label{fig:indep_causal_model}
\end{figure}





\section{Conclusion and Future Work}

%In this work, we describe how experiment design choices can impact human interactions with ADS systems, and importantly, bias our estimates of the treatment effects of algorithmic interventions. Our contribution is primarily theoretical and conceptual, and it remains to empirically validate the models of human decision making that we have proposed. We also focused on our numerical experiments on the treatment exposure model, and did not directly address capacity constraint and low trust in simulations.

%Thus, future work in this area could explore several questions. First, how can we effectively modify the positive prediction rate ($P (\hat{Y} = 1)$) in an experimental evaluation? This could involve altering the prediction threshold, which may result in lowered recall but increased precision. Second, in terms of modifying model correctness ($P (\hat{Y} = Y)$), what does it mean to select a different model with higher observed accuracy over multiple time steps? Our case study setting shows a model accuracy of 53.7\% with respect to recorded downstream outcomes. Understanding the implications of choosing a model with improved (or worse) accuracy is crucial for the generalizability of these findings. 

We hope this work will spark a conversation about choices and challenges in experimental design in the context of algorithmic interventions, with the goal of better measuring and addressing the societal impact of algorithmic decision aids in consequential real-world settings.
\newpage

\bibliographystyle{plainnat} % We choose the "plain" reference style
\bibliography{ref} % Entries are in the refs.bib file



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\newpage 




%\bibliography{ref} % Entries are in the refs.bib file




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn

\appendix

\section{Further Related work}\label{app:further_rel}

\paragraph{Experimental Evaluations}
The involved case study in this paper is an experimental randomized control trial study of the use of Public Safety Assessment (PSA) risk scores by judges in Dane county, Wisconsin~\cite{imai2020experimental}, and is actually one of the first known ~\emph{experimental} evaluations of an algorithmic intervention in the criminal justice context.

It is noteworthy that another area where the experimental evaluation of algorithmic systems has gained much traction recently has been in healthcare. Several standards have already been developed for reporting expectations on clinical trials for AI/ML-based healthcare interventions such as CONSORT-AI and SPIRIT-AI ~\cite{liu2020reporting}. However, a recent survey found that of the 41 RCTs of machine learning interventions done so far, no trials adhered to all CONSORT-AI reporting standards~\cite{plana2022randomized}. Furthermore, the RCTs included in that survey demonstrate many of the same design flaws and challenges we observe in ~\cite{imai2020experimental}.

Randomized control trials for risk assessments have been explored in other contexts. For example, \citet{wilder2021clinical} run a ``clinical trial'' on an algorithm used to select peer leaders in a public health program from homelessness youth. Similarly, several studies in education execute RCTs to assess the effectiveness of interventions where a model is used to predict which students (in real-world and MOOC settings) are most likely to drop out~\citep{dawson2017from, borrella2019predict}.  

\paragraph{Quasi-experimental investigations} In quasi-experimental investigations, most work that have considered judge responsiveness assume that the tendency to follow or not follow the algorithmic recommendation is inherent to each judge---an inherent attribute of the judges' internal state or their access to privileged decision and factors outside of the experimenter's control \citep{mullainathan2022diagnosing,albright2019if}. \citet{hoffman2018discretion} consider possible access to privileged information that impacted the hiring manager's judgement (and considers each hiring manager to have a particular \emph{exception rate} for making a different decision from the algorithm). \citet{angwin2016machine} discussed the cost $\eta$ of deviation, since the judge has to actively log deviations from the algorithmic recommendation. 

\paragraph{Human Interaction Lab Studies}
In contrast to RCTs in the field, there has been work on controlled~\emph{lab} experiments in which experimenters assess the effect of providing algorithmic risk scores in simulated environments to instructed test subjects \citep{green2019disparate}. The test subjects are non-experts and are often role-playing real-world decision-makers on a contrived or real-world inspired task~\citep{lai2023towards}. In the current work, we focus on the setting of a field experiment as it more directly addresses the impact of ADS in deployment.

\paragraph{Spillover effects}

It is well-known, in the experimental economics literature \citep[][]{banerjee2009experimental,deaton2018understanding} for example, that insights from randomized controlled trials (RCT) often fail to scale up when the intervention is applied broadly across the entire target population. This failure is often attributed to \emph{interaction} or \emph{interference} between units, and this phenomenon has also been referred to, in various contexts, as \emph{general equilibrium effects} or \emph{spillover effects}. In the potential outcomes model for causal inference, this is referred to as a violation of the Stable Unit Treatment Value Assumption (SUTVA) \citep{rubin1980discussion,rubin1990formal}. Recognizing the presence of interference, experimenters typically employ a two-level randomized experiment design \citep{duflo2003role,crepon2013labor} in order to estimate average treatment effects under different treatment proportions (e.g. proportion of job seekers receiving an intervention); treatment proportions are randomly assigned to the treatment clusters, e.g., the labor market. Such experiment designs have not yet been applied to study human-algorithm decision making, where each treatment cluster is associated with a particular human decision-maker. 

Recently, \citet{RCT_Service_Interventions} studied experiment design where the treatment is delivered by a human service provider who has limited resources. They show that treatment effect sizes are mediated by such capacity constraints, which are \emph{induced} by particular choices in the design of the experiment, such as the number of service providers recruited and the treatment assignment. Our work also views the human decision maker as a mediator of the treatment effect of using algorithmic decision aids in a decision process; however, we are motivated by modelling the human decision maker's cognitive biases induced by the experiment design, rather than their capacity constraints alone. Decision making is also a distinct setting from service provision.


\section{Examples of capacity constraint and low trust model}\label{app:instance}

A particular instantiation of the \emph{capacity constraint} model is based on a simple thresholding of the average number of `high risk' predictions seen:
\begin{equation}
J_{i,k}=
    \begin{cases}
        1 & \text{if } \sum_{m=1}^{i} Z_{m,k}\cdot \Ind{\hat{Y}_m > 0} < i\tau\\
        0 & \text{o.w.}
    \end{cases},
\end{equation}
where $\tau \in (0,1)$. This instance of the judge's decision making model suggests that the judge becomes always responsive to the algorithmic recommendation as long as the cumulative average `high risk' prediction is below~$\tau$. Otherwise, they always trust themselves over the model in moments of disagreement between their default decision and the ADS decision.

A particular instantiation of the \emph{low trust} model is based on a simple thresholding of the average number of predictive `errors' observed:
\begin{equation}
J_{i,k}=
    \begin{cases}
        1 & \text{if } \sum_{m=1}^{i}Z_{m,k} \cdot \Ind{\hat{Y}_m \neq Y_{m,k}} < i\tau\\
        0 & \text{o.w.}
    \end{cases},
\end{equation}
where $\tau \in (0,1)$. This instance of the judge's decision making model suggests that the judge becomes always responsive to the algorithmic recommendation as long as the cumulative average error rate of prediction $\hat{Y}$ is below $\tau$. Otherwise, they always trust themselves over the model in moments of disagreement between their default decision and the ADS decision. 

\section{Proofs}\label{app:proofs}

To be precise, we define
\begin{equation*}
   Z_{i,k}
= 
\begin{cases}
1 \text{ if unit } i \text{ is assigned to Judge }k \text{ and treated}\\
0 \text{ if unit } i \text{ is assigned to Judge }k \text{ and untreated} \\
-1 \text{ if unit } i \text{ is not assigned to Judge }k 
  \end{cases}
\end{equation*}  

\begin{corollary}
Suppose $J_{i,k}$ is as defined in \eqref{eq:treatment_exposure}, \eqref{eq:capacity}, or \eqref{eq:low_trust}.
For any $\lambda_k$, $X_{i}$, $Y_{i}$, there exists $\hat{Y}(\cdot)$, $f$ and $b_k$ such that Assumption~\ref{assmp:sutva} (part a) does not hold with some positive probability. 
% Consider the following three models of $J_{i,k}$.

%     Models 1, 2, 3 all satisfy the condition for SUTVA violation. (In all models,  $J_{i,k}$ is a monotonically non-decreasing function of $Z_{1,k}, \cdots, Z_{i-1,k}$, and strictly increasing in at least one of its arguments).
\end{corollary}
\begin{proof}
   Consider linear $f$ (i.e. $f(x) = ax$), $b_k = 0$ and $\hat{Y}$ such that $\hat{Y}_\ell > 0$ and $\hat{Y}_\ell \ne Y_{\ell,k}$ for some $\ell < i$. Then $J_{i,k}$ as defined in \eqref{eq:treatment_exposure}, \eqref{eq:capacity}, or \eqref{eq:low_trust} satisfies the assumptions of Theorem~\ref{thm:sutva_violation}.
\end{proof}


The following proposition shows that there can be a significant gap between the expectation of the $\widehat{ATE}$ estimator under different treatment assignments. Specifically, we compare $\widehat{ATE}_{\text{uniform}}:=\widehat{ATE}(\vect{Z})$ and that of $\widehat{ATE}_{\text{two-level}}:=\widehat{ATE}(\vect{Z}')$. We include the proof in Appendix~\ref{app:proofs}.

\begin{proposition}[Estimated treatment effects under treatment exposure model]\label{prop:treatment_effect}
Assume both judge 1 and 2's decision model is as described in equation~\eqref{eq:model_of_judge}, and $J_{i,k}$ follows \eqref{eq:treatment_exposure}, with $b_{k} = b \in(0,1) \forall k$ and $f(x) = ax, a \in (0,1-b)$. Suppose we have $\E[\bar{D}(\hat{Y}_1) - \lambda_k(X_1)] = \rho > 0$. Then
\begin{equation}
  \E[\widehat{ATE}_{\text{two-level}}]- \E[\widehat{ATE}_{\text{uniform}}] = \frac{a \cdot \rho }{2}
\end{equation}
\end{proposition}

\begin{proof}[Proof of Theorem~\ref{thm:sutva_violation}]  Consider $\vect{Z}, \vect{Z}'$ such that $Z_{1,k}= Z_{2,k} = \cdots = Z_{i-1,k} = 0$ and  $Z'_{1,k}= Z'_{2,k} = \cdots = Z'_{i-1,k} = 1$. Also let $Z_{i,k} = Z_{i,k}' = 1$. First consider the case that $J_{i,k}$ is monotonically non-decreasing. Then we have
\begin{align*}
    J_{i,k}(\vect{Z}) < J_{i,k}(\vect{Z}'),
\end{align*}
by our assumption that $J_{i,k}(z_{1,k}, \cdots, z_{i-1,k})$ is strictly increasing in at least one of $z_{1,k}, \cdots, z_{i-1,k}$. In other words, we have $\Pr\left(\epsilon_{i,k}(\vect{Z}) = 1) < \Pr(\epsilon_{i,k}(\vect{Z}') = 1\right)$.

Applying the definition of $D_{i,k}$, we then lower bound the probability that $D_{i,k}(\vect{Z})$ differs from $D_{i,k}(\vect{Z}')$ as follows.
\begin{align*}
  &\quad   \Pr\left(D_{i,k}(\vect{Z}) \ne D_{i,k}(\vect{Z}')\right) \\
    &\ge \Pr\{\lambda_k(X_i) \neq \recdec{i}\}\cdot \Pr\left(\epsilon_{i,k}(\vect{Z}) \ne \epsilon_{i,k}(\vect{Z}')\right) \\
    &> 0.
\end{align*}
  The proof proceeds analogously for the case where $J_{i,k}$ is monotonically non-increasing.
\end{proof}

\begin{proof}[Proof of Proposition~\ref{prop:treatment_effect}]
%     \todo{to review}
% Under equation~\eqref{eq:model_of_judge}
% \begin{align*}
%     &D_{i,k}(Z_{i,k} = 1) - D_{i,k}(Z_{i,k} = 0) \\= &\bar{D}(\hat{Y}_i)\cdot \epsilon_{i,k} + \lambda_k(X_i) \cdot (1-\epsilon_{i,k}) - \lambda_k(X_i) \\
%     =& \epsilon_{i,k} \cdot (\bar{D}(\hat{Y}_i) - \lambda_k(X_i))
% \end{align*}



Under uniform randomization, we have 
{\small
\begin{align*}
   &\quad  \E[\widehat{ATE}_{\text{uniform}}]\\
    &= \frac{2}{n} \sum_{i=1}^n\sum_{k=1}^2 \E[D_{i,k}\Ind{ Z_{i,k} = 1} - D_{i,k} \Ind{Z_{i,k} = 0}]
    \\
   % &=\frac{2}{n}\sum_{i=1}^n\sum_{k=1}^2 \E[D_{i,k}]\Ind{ Z_{i,k} = 1} - \E[D_{i,k}]  \Ind{ Z_{i,k} = 1} \\
    &= \frac{1}{2n}\sum_{i=1}^n\E[D_{i,1}\mid Z_{i,1} = 1] +\E[D_{i,2}\mid Z_{i,2} = 1]\\
    &\quad\quad\quad -\E[D_{i,1}\mid Z_{i,1} = 0]-\E[D_{i,1}\mid Z_{i,1} = 0] \\
    &= \frac{1}{2n}\left(\sum_{i=1}^n\E[\bar{D}(\hat{Y}_i) - \lambda_k(X_i)]\cdot \E\left[b+ \frac{a}{i} \sum_{m=1}^{i} Z_{m,1} \mid Z_{i,1} = 1\right]  \right)\\
    &\quad +\frac{1}{2n}\left(\sum_{i=1}^n\E[\bar{D}(\hat{Y}_i) - \lambda_k(X_i)]\cdot \E\left[b+ \frac{a}{i} \sum_{m=1}^{i} Z_{m,2} \mid Z_{i,2} = 1\right]  \right)\\
    &= \frac{1}{2}\left(\E[\bar{D}(\hat{Y}_i) - \lambda_k(X_i)]\cdot (b+ \frac{a}{2}) \right)\\
    &\quad +\frac{1}{2}\left(\E[\bar{D}(\hat{Y}_i) - \lambda_k(X_i)]\cdot (b+ \frac{a}{2})  \right)\\
    &= \left(\E[\bar{D}(\hat{Y}_1) - \lambda_k(X_1)]\cdot (b+ \frac{a}{2}) \right)
%&\frac{2}{n} \sum_{i=1}^n \sum_{k\in\{1, 2\}}\E[D_{i,k} \Ind{Z_{i,k} = 1}]- \E[D_{i,k} \Ind{Z_{i,k} = 0}] \\
  %  &= \frac{2}{n} \sum_{i:Z_{i,1} = 1} \E[D_{i,1}] + \sum_{i:Z_{i,2} = 1} \E[D_{i,2}]- \sum_{i:Z_{i,1} = 0} \E[D_{i,1}]-\sum_{i:Z_{i,2} = 0} \E[D_{i,2}]
\end{align*}
}
Note that in the third equality we applied the definition of $D_{i,k}$ and $J_{i,k}$, as well as the independence between $(\hat{Y}_i, X_i)$ and $Z_{i,k}$. In the fourth equality, we use the fact that $\E[Z_{m,2}\mid Z_{i,2} = 1] = 1/2$ for this treatment assignment. In the last equality we used the fact that $\hat{Y}_i$'s abnd $X_i's$ are i.i.d.

Under two-level randomization, we have 

\begin{align*}
   &\quad  \E[\widehat{ATE}_{\text{two-level}}] \\
    &=\frac{2}{n} \sum_{i=1}^n\sum_{k=1}^2 \E[D_{i,k}\Ind{ Z_{i,k} = 1} - D_{i,k} \Ind{Z_{i,k} = 0}] \\
    &= \frac{1}{n}\sum_{i=1}^n\E[D_{i,1}\mid Z_{i,1} = 1] -\E[D_{i,1}\mid Z_{i,2} = 0] \\
    &=\frac{1}{n}\sum_{i=1}^n\E[\bar{D}(\hat{Y}_i) - \lambda_k(X_i)]\cdot \E\left[b+ \frac{a}{i} \sum_{m=1}^{i} Z_{m,1} \mid Z_{i,1} = 1\right]  \\
    &= \left(\E[\bar{D}(\hat{Y}_1) - \lambda_k(X_1)]\cdot (b+ a) \right)
    %&= \frac{n}{4}\left(\E[\bar{D}(\hat{Y}_i) - \lambda_k(X_i)]\cdot \E\left[b+ \frac{a}{i} \sum_{m=1}^{i} Z_{m,1} \mid Z_{i,1} = 1\right]  \right)\\
    %&\quad +\frac{n}{4}\left(\E[\bar{D}(\hat{Y}_i) - \lambda_k(X_i)]\cdot \E\left[b+ \frac{a}{i} \sum_{m=1}^{i} Z_{m,2} \mid Z_{i,2} = 1\right]  \right)\\
    %&= \frac{n}{4}\left(\E[\bar{D}(\hat{Y}_i) - \lambda_k(X_i)]\cdot (b+ \frac{a}{2}) \right)+\frac{n}{4}\left(\E[\bar{D}(\hat{Y}_i) - \lambda_k(X_i)]\cdot (b+ \frac{a}{2})  \right)\\
   % &= \frac{n}{2}\left(\E[\bar{D}(\hat{Y}_1) - \lambda_k(X_1)]\cdot (b+ \frac{a}{2}) \right)
%&\frac{2}{n} \sum_{i=1}^n \sum_{k\in\{1, 2\}}\E[D_{i,k} \Ind{Z_{i,k} = 1}]- \E[D_{i,k} \Ind{Z_{i,k} = 0}] \\
  %  &= \frac{2}{n} \sum_{i:Z_{i,1} = 1} \E[D_{i,1}] + \sum_{i:Z_{i,2} = 1} \E[D_{i,2}]- \sum_{i:Z_{i,1} = 0} \E[D_{i,1}]-\sum_{i:Z_{i,2} = 0} \E[D_{i,2}]
\end{align*}

\end{proof}



\section{Experiment Case Study}\label{app:exps}

During a 30-month assignment period, spanning 2017 until 2019, judges in Dane County, Wisconsin were either given or not given a pretrial Public Safety Assessment (PSA) score for a given case. The randomization was done by an independent administrative court member - the PSA score was calculated for each case $i$, and, for every even-numbered case, the PSA was shown to the judge $k$ as part of the case files. Otherwise, the PSA was hidden from the judge. Given these scores, the judge needs to make the decision, $D_{i,k}$ to enforce a signature bond, a small case bail (defined as less than \$1000) or a large case bail (defined as more than \$1000). Information about judge decisions, and defendant outcomes, $Y_{i,k}$ (ie. failure to appear (FTA), new criminal activity (NCA) and new violent criminal activity (NVCA)) are tracked for a period of two years after randomization. 
The case study~\cite{imai2020experimental} only looks at one judge, allowing them to simplify the situation to a single decision-maker scenario.

Using this experiment data, we simulate alternative scenarios of one of the discussed experiment design choices -- treatment assignment. Using our model of $J_{i,k}$, we can derive an alternate set of judge decisions to that in the original experiment. In the original study, all of the treated and untreated cases were assigned to a single judge. We simulate multiple judges, and a corresponding set of alternative decisions to observe the impact of these biases on estimations of the overall average treatment effect estimate on judge decisions. To do this, we change the simulated judge $J_k$ assigned to a particular case, and their subsequent decisions $D_{i,k}$, using our model in Equation ~\ref{eq:treatment_exposure}. We do not manipulate other case details directly for our simulation. We use a binary normalized form of judge decisions (i.e. $D_{i,k} = 0$ for a signature bond release, $D_{i,k} = 1$ for any kind of cash bond). We set $\lambda_k(X_i)$ to be the set of provided original judge decisions, and $\recdec{i}$ to be the decision recommended under official interpretation guidelines for the PSA. Details on the implementation of data pre-processing semi-synthetic experiments can be found in the Python Notebook provided in the attached Supplementary Materials. The data from the ~\cite{imai2020experimental} study is publicly available \href{https://dataverse.harvard.edu/dataverse/harvard?q=%20Replication%20data%20for:%20Experimental%20evaluation%20of%20algorithm-assisted%20human%20decision-making:%20Application%20to%20pretrial%20public%20safety%20assessment}{here}.

%Using this experiment data, we simulate alternative scenarios of one of the discussed experiment design choices -- treatment assignment. Using our model of $J_{i,k}$, we can derive an alternate set of judge decisions to that in the original experiment. In the original study, all of the treated and untreated cases were assigned to a single judge. We simulate multiple judges, and a corresponding set of alternative decisions to observe the impact of these biases on estimations of the overall average treatment effect estimate on judge decisions. To do this, we change the simulated judge $J_k$ assigned to a particular case, and their subsequent decisions $D_{i,k}$, using our model in Equation ~\ref{eq:treatment_exposure}. We do not manipulate other case details directly for our simulation. We use a binary normalized form of judge decisions (i.e. $D_{i,k} = 0$ for a signature bond release, $D_{i,k} = 1$ for any kind of cash bond). We set $\lambda_k(X_i)$ to be the set of provided original judge decisions, and $\recdec{i}$ to be the decision recommended under official interpretation guidelines for the PSA. We include descriptive statistics for simulated individual judge decisions in the Appendix~\ref{app:exps}. Details on the implementation of data pre-processing semi-synthetic experiments can be found in the Python Notebook provided in the attached Supplementary Materials. The data from  ~\cite{imai2020experimental} is publicly available \href{https://dataverse.harvard.edu/dataverse/harvard?q=%20Replication%20data%20for:%20Experimental%20evaluation%20of%20algorithm-assisted%20human%20decision-making:%20Application%20to%20pretrial%20public%20safety%20assessment}{here}.

We conduct an empirical study similar to that described in Section 5.2. Namely, for each study, we consider two different treatment assignments (to $n$ total units) that have the same treated and untreated units, but assign these treated units to different decision makers (and we assume each case is only seen by one judge in each treatment assignment). We consider $k$ judges, who are assumed to be identical, apart from their assigned cases. Specifically, both of them have decision making model as described in equation~\eqref{eq:treatment_exposure}, where $f(z) = az$ is a linear function and $b_1 = b_2 = b$.

Under these assumptions, we conduct the following set up for Experiment 1:
\begin{enumerate}
    \item (Uniform randomization) $\vect{Z} =(\vect{Z}_{\cdot, 1},\vect{Z}_{\cdot, 2})$ such that Judge 1 receives 50\% of total cases, 50\% of them are treated and 50\% are untreated. Judge 2 receives other 50\% of total cases, 50\% of them are treated and 50\% are untreated.
    \item (Two-level randomization) $\vect{Z}' = (\vect{Z'}_{\cdot, 1},\vect{Z'}_{\cdot, 2})$ such that Judge 1 receives 50\% of total cases, 100\% of them are treated. Judge 2 receives other 50\% of total cases, 100\% are untreated.
\end{enumerate}

Under these assumptions, we conduct the following set up for Experiment 2:
\begin{enumerate}
    \item (Uniform randomization) $\vect{Z} =(\vect{Z}_{\cdot, 1},\vect{Z}_{\cdot, 2},\vect{Z}_{\cdot, 3})$ such that each judge  receives 33.3\% of total cases, 33.3\% of them are treated .
    \item (Two-level randomization) $\vect{Z}' = (\vect{Z'}_{\cdot, 1},\vect{Z'}_{\cdot, 2},\vect{Z}_{\cdot, 3})$ such that Judge 1 receives 33.3\% of total cases, 60\% of them are treated. Judge 2 receives other 33.3\% of total cases, but only 30\% are untreated. Judge 3 receives other 33.3\% of total cases, but only 10\% are untreated.
\end{enumerate}

 \begin{figure}[htbp]
  \centering
 %\includegraphics[trim=1.5cm 2cm 3cm 10cm, clip, width=0.6\columnwidth]{alg_int_neurips24_bw_imai_simple.pdf}
 \includegraphics[clip, width=0.4\columnwidth]{aihuman_modifications_and_results/ATE_J2_0505.png}
 \includegraphics[clip, width=0.4\columnwidth]{aihuman_modifications_and_results/ATE_2J_01.png}
  \caption{\textbf{Results for Experiment 1:} We empirically observe changes to the ATE estimate under different treatment assignments. In Figure (a), we see the ATE for a scenario where two judges are assigned $Z=1$ in about 50\% of their assigned cases. In Figure (b), $J_1$ is assigned $Z=1$ in about 100\% of their cases, while $J_2$ is assigned $Z=1$ in about 50\% of their cases. Under our model, if a judge is likely to ignore the PSA if they see it less than 60\% of the time, then the ATE is higher for case (b), where at least one judge is guaranteed to see the PSA at that frequency.}
  \label{fig:exp1_res}
\end{figure}

 \begin{figure}[htbp]
  \centering
 %\includegraphics[trim=1.5cm 2cm 3cm 10cm, clip, width=0.6\columnwidth]{alg_int_neurips24_bw_imai_simple.pdf}
 \includegraphics[clip, width=0.4\columnwidth]{aihuman_modifications_and_results/ATE_J3_333.png}
 \includegraphics[clip, width=0.4\columnwidth]{aihuman_modifications_and_results/ATE_J3_613.png}
  \caption{\textbf{Results for Experiment 2:}We empirically observe changes to the ATE estimate under different treatment assignments. In Figure (a), we see the ATE for a scenario where three judges are assigned $Z=1$ in about 33.3\% of their assigned cases. In Figure (b), $J_1$ is assigned $Z=1$ in about 60\% of their cases, while $J_2$ and $J_3$ are assigned $Z=1$ in about 10\%, and 30\% of their cases respectfully. Under our model, if a judge is likely to ignore the PSA if they see it less than 60\% of the time, then the ATE is higher for case (b), where at least one judge is guaranteed to see the PSA at that frequency. Note that we could not}
  \label{fig:exp2_res}
\end{figure}

\begin{figure}[htbp]
  \centering
 %\includegraphics[trim=1.5cm 2cm 3cm 10cm, clip, width=0.6\columnwidth]{alg_int_neurips24_bw_imai_simple.pdf}
   \includegraphics[clip, width=0.23\columnwidth]{aihuman_modifications_and_results/FTAT_my_PSAdata_J1_norm_J2_0505.png}
   \includegraphics[clip, width=0.23\columnwidth]{aihuman_modifications_and_results/FTAT_my_PSAdata_J2_norm_J2_0505.png}
 \includegraphics[clip, width=0.23\columnwidth]{aihuman_modifications_and_results/FTAC_my_PSAdata_J1_norm_J2_0505.png}
 \includegraphics[clip, width=0.23\columnwidth]{aihuman_modifications_and_results/FTAC_my_PSAdata_J2_norm_J2_0505.png}
  \caption{\textbf{Results for Experiment 1a:} Changes in decision patterns for different risk levels - from left to right, decisions across ground truth FTA risk thresholds for treated cases with $J_1$ treated at 50\%, $J_2$ treated at 50\%, and $J_1$, $J_2$ for control cases. }
\end{figure}

 \begin{figure}[htbp]
  \centering
 %\includegraphics[trim=1.5cm 2cm 3cm 10cm, clip, width=0.6\columnwidth]{alg_int_neurips24_bw_imai_simple.pdf}
   \includegraphics[clip, width=0.23\columnwidth]{aihuman_modifications_and_results/FTAT_my_PSAdata_J1_norm_2J_01.png}
   \includegraphics[clip, width=0.23\columnwidth]{aihuman_modifications_and_results/FTAT_my_PSAdata_J1_norm_2J_01.png}
 \includegraphics[clip, width=0.23\columnwidth]{aihuman_modifications_and_results/FTAC_my_PSAdata_J2_norm_2J_01.png}
 \includegraphics[clip, width=0.23\columnwidth]{aihuman_modifications_and_results/FTAC_my_PSAdata_J2_norm_2J_01.png}
  \caption{\textbf{Results for Experiment 1b:} Changes in decision patterns for different risk levels - from left to right, decisions across ground truth FTA risk thresholds for treated cases with $J_1$ treated at 100\%, $J_2$ treated at 0\%, and $J_1$, $J_2$ for control cases.}
  \label{fig:indep_causal_model}
\end{figure}

\begin{figure}[htbp]
  \centering
 %\includegraphics[trim=1.5cm 2cm 3cm 10cm, clip, width=0.6\columnwidth]{alg_int_neurips24_bw_imai_simple.pdf}
   \includegraphics[clip, width=0.3\columnwidth]{aihuman_modifications_and_results/FTAT_my_PSAdata_J1_norm_J3_333.png}
   \includegraphics[clip, width=0.3\columnwidth]{aihuman_modifications_and_results/FTAT_my_PSAdata_J2_norm_J3_333.png}
 \includegraphics[clip, width=0.3\columnwidth]{aihuman_modifications_and_results/FTAT_my_PSAdata_J3_norm_J3_333.png}
 \includegraphics[clip, width=0.3\columnwidth]{aihuman_modifications_and_results/FTAC_my_PSAdata_J1_norm_J3_333.png}
 \includegraphics[clip, width=0.3\columnwidth]{aihuman_modifications_and_results/FTAC_my_PSAdata_J2_norm_J3_333.png}
  \includegraphics[clip, width=0.3\columnwidth]{aihuman_modifications_and_results/FTAC_my_PSAdata_J3_norm_J3_333.png}

  \caption{\textbf{Results for Experiment 2a:} Changes in decision patterns for different risk levels - from left to right, decisions across ground truth FTA risk thresholds for treated cases with $J_1$ treated at 33.3\%, $J_2$ treated at 33.3\%, $J_3$ treated at 33.3\%; and below $J_1$, $J_2$, $J_3$ for control cases.}
  \label{fig:indep_causal_model}
\end{figure}

\begin{figure}[htbp]
  \centering
 %\includegraphics[trim=1.5cm 2cm 3cm 10cm, clip, width=0.6\columnwidth]{alg_int_neurips24_bw_imai_simple.pdf}
   \includegraphics[clip, width=0.3\columnwidth]{aihuman_modifications_and_results/FTAT_my_PSAdata_J1_norm_J3_613.png}
   \includegraphics[clip, width=0.3\columnwidth]{aihuman_modifications_and_results/FTAT_my_PSAdata_J2_norm_J3_613.png}
 \includegraphics[clip, width=0.3\columnwidth]{aihuman_modifications_and_results/FTAT_my_PSAdata_J3_norm_J3_613.png}
 \includegraphics[clip, width=0.3\columnwidth]{aihuman_modifications_and_results/FTAC_my_PSAdata_J1_norm_J3_613.png}
 \includegraphics[clip, width=0.3\columnwidth]{aihuman_modifications_and_results/FTAC_my_PSAdata_J2_norm_J3_613.png}
  \includegraphics[clip, width=0.3\columnwidth]{aihuman_modifications_and_results/FTAC_my_PSAdata_J3_norm_J3_613.png}

  \caption{\textbf{Results for Experiment 2b:} Changes in decision patterns for different risk levels - from left to right, decisions across ground truth FTA risk thresholds for treated cases with $J_1$ treated at 60\%, $J_2$ treated at 10\%, $J_3$ treated at 30\%; and below $J_1$, $J_2$, $J_3$ for control cases.}
  \label{fig:indep_causal_model}
\end{figure}

%\section{You \emph{can} have an appendix here.}

%You can have as much text here as you want. The main body must be at most $8$ pages long.
%For the final version, one more page can be added.
%If you want, you can use an appendix like this one.  

%The $\mathtt{\backslash onecolumn}$ command above can be kept in place if you prefer a one-column appendix, or can be removed if you prefer a two-column appendix.  Apart from this possible change, the style (font size, spacing, margins, page numbering, etc.) should be kept the same as the main body.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
