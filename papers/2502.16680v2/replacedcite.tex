\section{Related Work}
\label{sec:2}

\subsection{Referring Image Segmentation for Natural Images}
\label{sec:2.1}
RIS is a fundamental task in vision-language understanding, where the goal is to segment objects in an image based on a given natural language expression ____. This task demands a fine-grained alignment between textual descriptions and visual features to correctly localize and delineate the referenced objects. Compared to conventional segmentation tasks that rely on predefined categories, RIS enables a more flexible and user-specific segmentation process.

In the early stages, initial RIS models relied primarily on Convolutional Neural Networks (CNNs) to extract visual features and Recurrent Neural Networks (RNNs) to process textual descriptions ____. These models performed feature fusion by concatenating visual and linguistic representations before feeding them into a segmentation head. Specifically, ____ first introduced RIS to address the limitations of traditional semantic segmentation when handling complex textual descriptions. Later, ____ and  ____ explored bidirectional interactions between visual and textual features, improving the multimodal understanding of objects through structured representations. Further advancements introduced dynamic multimodal networks, such as the work by ____, which incorporated recursive reasoning mechanisms to enhance the integration of linguistic and visual information.

As RIS models evolved, researchers recognised the importance of cross-modal feature alignment, leading to the introduction of attention-based strategies ____. For example, ____ introduced a keyword-aware segmentation model, refining object-region relationships based on key linguistic cues. These approaches significantly improved object localisation and contextual interpretation in RIS tasks.  ____ proposed a cross-modal self-attention module to capture long-range dependencies between textual and visual elements, improving multimodal fusion. Similarly, ____ developed a bidirectional cross-modal attention mechanism, enabling deeper interaction between the modalities.

The recent emergence of Transformer-based architectures has significantly advanced RIS, offering global modelling capabilities and superior multimodal integration. Unlike CNN-based methods, which rely on local receptive fields, Transformers enable long-range dependencies and self-attention mechanisms, making them particularly effective for RIS ____. Several notable works have leveraged this architecture. VLT designed a query-based Transformer framework, enriching textual comprehension by dynamically generating language query embeddings ____. LAVT proposed language-aware attention mechanisms to enhance early fusion between the two modalities, enabling more precise segmentation ____. GRES further refined multimodal alignment by explicitly modelling dependencies between different textual tokens and visual regions, leading to more robust segmentation performance ____.

\subsection{ Referring Remote Sensing Image Segmentation}
\label{sec:2.2}
Referring Remote Sensing Image Segmentation (RRSIS) is a specialized task that aims to extract pixel-wise segmentation masks from remote sensing imagery based on natural language expressions ____. While it has significant applications in environmental monitoring, land cover classification, disaster response, and urban planning ____, progress in this field hinges critically on suitable datasets that capture the complexity of remote sensing imagery. One of the earliest datasets was RefSegRS, introduced by LGCE ____, which enabled initial efforts to adapt RIS methods from natural images to the remote sensing domain. To enhance the diversity and improve the generalizability of trained models, ____ proposed RRSIS-D, a substantially larger dataset for benchmarking mainstream RIS models in remote sensing image segmentation. More recently, RISBench, has also been introduced to further advance the development and evaluation of RRSIS methods. 

Building on these datasets, recent RRSIS research has explored strategies to address scale variations, complex backgrounds, and orientation diversity. LGCE ____ pioneered a language-guided cross-scale enhancement module to fuse shallow and deep features for improved segmentation accuracy, whereas ____ proposed the Rotated Multi-Scale Interaction Network (RMSIN), which integrates intra-scale and cross-scale interactions alongside rotated convolutions to better handle directional variations. Beyond scale-aware models, ____ analysed the implicit optimization mechanisms in existing models and proposed an explicit affinity alignment approach, incorporating a new loss function to improve textual-visual feature interaction. More recent studies have introduced refined image-text alignment strategies to improve RRSIS performance. To be specific, FIANet ____ introduced a fine-grained alignment module with object-positional enhancement, integrating a text-aware self-attention mechanism to refine segmentation accuracy. Similarly, CroBIM ____ leveraged a context-aware prompt modulation module, which optimizes post-fusion feature interactions and employs a mutual-interaction decoder to refine segmentation masks. Recently, SBANet ____ introduced a bidirectional alignment mechanism and a scale-wise attention module to enhance mutual guidance between vision and language features, effectively refining segmentation masks in referring remote sensing image segmentation. BTDNet ____ employs a bidirectional spatial correlation module and a target-background twin-stream decoder to improve multimodal alignment and fine-grained object differentiation, achieving improved segmentation performance.

\subsection{Visual Grounding for Aerial Images}
\label{sec:2.3}
Another active vision-and-language research in
remote sensing community is visual grounding for aerial images, focusing on localizing target objects within aerial scenes using natural language queries ____. In contrast to RRSIS, which demands detailed pixel-level masks, visual grounding is primarily concerned with identifying object-level regionsâ€”typically represented as bounding boxes ____. This task leverages the unique characteristics of aerial imagery, where targets often exhibit complex spatial relationships and may not be visually prominent due to scale variations and cluttered backgrounds.

Early frameworks, such as GeoVG ____, pioneered this approach by integrating a language encoder that captures geospatial relationships with an image encoder that adaptively attends to aerial scenes. By fusing these modalities, GeoVG established a one-stage process that effectively translates natural language cues into object localization. Building on this foundation, subsequent models have introduced advanced fusion strategies. For instance, modules like the Transformer-based Multi-Granularity Visual Language Fusion (MGVLF) ____ exploit both multi-scale visual features and multi-granularity textual embeddings, resulting in more discriminative representations that address the challenges posed by large-scale variations and busy backgrounds. Vision-Semantic Multimodal Representation (VSMR) enhanced multi-level feature integration, refining how visual and textual features are jointly processed to improve localization robustness ____. Further improvements have been achieved through progressive attention mechanisms. The Language-guided Progressive Visual Attention (LPVA) framework, for example, dynamically adjusts visual features at various scales and levels, ensuring that the visual backbone concentrates on expression-relevant information ____. This is complemented by multi-level feature enhancement decoders, which aggregate contextual information to boost feature distinctiveness and suppress irrelevant regions.