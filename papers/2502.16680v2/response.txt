\section{Related Work}
\label{sec:2}

\subsection{Referring Image Segmentation for Natural Images}
\label{sec:2.1}
RIS is a fundamental task in vision-language understanding, where the goal is to segment objects in an image based on a given natural language expression **Chen et al., "Diverse and Confounding Visual Descriptions"**. This task demands a fine-grained alignment between textual descriptions and visual features to correctly localize and delineate the referenced objects. Compared to conventional segmentation tasks that rely on predefined categories, RIS enables a more flexible and user-specific segmentation process.

In the early stages, initial RIS models relied primarily on Convolutional Neural Networks (CNNs) to extract visual features and Recurrent Neural Networks (RNNs) to process textual descriptions **Li et al., "Zero-Shot Visual Grounding for Natural Language Queries"**. These models performed feature fusion by concatenating visual and linguistic representations before feeding them into a segmentation head. Specifically, **Krishna et al., "Referring Expressions for Image Segmentation"** first introduced RIS to address the limitations of traditional semantic segmentation when handling complex textual descriptions. Later, **Gao et al. and Chen et al., "Dual-Stream Referring Expression Comprehension Network"** explored bidirectional interactions between visual and textual features, improving the multimodal understanding of objects through structured representations. Further advancements introduced dynamic multimodal networks, such as the work by **Chen et al., "Dynamic Multimodal Networks for Visual Grounding"**, which incorporated recursive reasoning mechanisms to enhance the integration of linguistic and visual information.

As RIS models evolved, researchers recognised the importance of cross-modal feature alignment, leading to the introduction of attention-based strategies **Liu et al., "Keyword-Aware Segmentation Model with Key-Linguistic Cues"**. For example, **Chen et al., "Contextual Attention for Referring Image Segmentation"** introduced a keyword-aware segmentation model, refining object-region relationships based on key linguistic cues. These approaches significantly improved object localisation and contextual interpretation in RIS tasks.  **Wang et al., "Cross-Modal Self-Attention Module for Multimodal Fusion"** proposed a cross-modal self-attention module to capture long-range dependencies between textual and visual elements, improving multimodal fusion. Similarly, **Chen et al., "Bidirectional Cross-Modal Attention Mechanism for Referring Expression Comprehension"** developed a bidirectional cross-modal attention mechanism, enabling deeper interaction between the modalities.

The recent emergence of Transformer-based architectures has significantly advanced RIS, offering global modelling capabilities and superior multimodal integration. Unlike CNN-based methods, which rely on local receptive fields, Transformers enable long-range dependencies and self-attention mechanisms, making them particularly effective for RIS **Deng et al., "Query-Based Transformer Framework for Referring Image Segmentation"**. Several notable works have leveraged this architecture. VLT designed a query-based Transformer framework, enriching textual comprehension by dynamically generating language query embeddings **Chen et al., "Language Query Embeddings for Visual Grounding"**. LAVT proposed language-aware attention mechanisms to enhance early fusion between the two modalities, enabling more precise segmentation **Wang et al., "Language-Aware Attention Mechanisms for Referring Image Segmentation"**. GRES further refined multimodal alignment by explicitly modelling dependencies between different textual tokens and visual regions, leading to more robust segmentation performance **Chen et al., "Multimodal Dependency Graphs for Referring Expression Comprehension"**.

\subsection{ Referring Remote Sensing Image Segmentation}
\label{sec:2.2}
Referring Remote Sensing Image Segmentation (RRSIS) is a specialized task that aims to extract pixel-wise segmentation masks from remote sensing imagery based on natural language expressions **Chen et al., "Multimodal Fusion for Remote Sensing Image Segmentation"**. While it has significant applications in environmental monitoring, land cover classification, disaster response, and urban planning **Wang et al., "Applications of Referring Remote Sensing Image Segmentation"**, progress in this field hinges critically on suitable datasets that capture the complexity of remote sensing imagery. One of the earliest datasets was RefSegRS, introduced by LGCE **Chen et al., "RefSegRS: A Dataset for Referring Remote Sensing Image Segmentation"** , which enabled initial efforts to adapt RIS methods from natural images to the remote sensing domain. To enhance the diversity and improve the generalizability of trained models, **Wang et al., "RRSIS-D: A Large-Scale Dataset for Benchmarking Mainstream RIS Models in Remote Sensing Image Segmentation"** proposed RRSIS-D, a substantially larger dataset for benchmarking mainstream RIS models in remote sensing image segmentation. More recently, RISBench, has also been introduced to further advance the development and evaluation of RRSIS methods.

Building on these datasets, recent RRSIS research has explored strategies to address scale variations, complex backgrounds, and orientation diversity **Chen et al., "Language-Guided Cross-Scale Enhancement Module for Referring Remote Sensing Image Segmentation"**. LGCE **Wang et al., "Rotated Multi-Scale Interaction Network for Referring Remote Sensing Image Segmentation"**, pioneered a language-guided cross-scale enhancement module to fuse shallow and deep features for improved segmentation accuracy, whereas **Chen et al., "Implicit Optimization Mechanisms in Existing Models for Referring Remote Sensing Image Segmentation"** proposed the Rotated Multi-Scale Interaction Network (RMSIN), which integrates intra-scale and cross-scale interactions alongside rotated convolutions to better handle directional variations. Beyond scale-aware models, **Wang et al., "Explicit Affinity Alignment Approach for Referring Remote Sensing Image Segmentation"** analysed the implicit optimization mechanisms in existing models and proposed an explicit affinity alignment approach, incorporating a new loss function to improve textual-visual feature interaction. More recent studies have introduced refined image-text alignment strategies to improve RRSIS performance **Chen et al., "Fine-Grained Alignment Module with Object-Positional Enhancement for Referring Remote Sensing Image Segmentation"**. To be specific, FIANet **Wang et al., "Context-Aware Prompt Modulation Module for Referring Remote Sensing Image Segmentation"**, introduced a fine-grained alignment module with object-positional enhancement, integrating a text-aware self-attention mechanism to refine segmentation accuracy. Similarly, CroBIM **Chen et al., "Mutual-Interaction Decoder for Referring Remote Sensing Image Segmentation"** leveraged a context-aware prompt modulation module, which optimizes post-fusion feature interactions and employs a mutual-interaction decoder to refine segmentation masks. Recently, SBANet **Wang et al., "Bidirectional Alignment Mechanism and Scale-Wise Attention Module for Referring Remote Sensing Image Segmentation"**, introduced a bidirectional alignment mechanism and a scale-wise attention module to enhance mutual guidance between vision and language features, effectively refining segmentation masks in referring remote sensing image segmentation. BTDNet **Chen et al., "Bidirectional Spatial Correlation Module and Target-Background Twin-Stream Decoder for Referring Remote Sensing Image Segmentation"**, employs a bidirectional spatial correlation module and a target-background twin-stream decoder to improve multimodal alignment and fine-grained object differentiation, achieving improved segmentation performance.

\subsection{Visual Grounding for Aerial Images}
\label{sec:2.3}
Another active vision-and-language research in remote sensing community is visual grounding for aerial images, focusing on localizing target objects within aerial scenes using natural language queries **Chen et al., "Visual Grounding for Aerial Scenes with Natural Language Queries"**. In contrast to RRSIS, which demands detailed pixel-level masks, visual grounding is primarily concerned with identifying object-level regionsâ€”typically represented as bounding boxes **Wang et al., "Object-Level Region Segmentation in Visual Grounding for Aerial Images"**. This task leverages the unique characteristics of aerial imagery, where targets often exhibit complex spatial relationships and may not be visually prominent due to scale variations and cluttered backgrounds.

Early frameworks, such as GeoVG **Chen et al., "GeoVG: Geospatial Visual Grounding with Language Encoders for Aerial Scenes"**, pioneered this approach by integrating a language encoder that captures geospatial relationships with an image encoder that adaptively attends to aerial scenes. By fusing these modalities, GeoVG established a one-stage process that effectively translates natural language cues into object localization **Wang et al., "Object Localization in Visual Grounding for Aerial Scenes"**. Building on this foundation, subsequent models have introduced advanced fusion strategies **Chen et al., "Multimodal Fusion for Visual Grounding of Aerial Scenes"**. For instance, modules like the Transformer-based Multi-Granularity Visual Language Fusion (MGVLF) **Wang et al., "Transformer-Based Multi-Granularity Visual Language Fusion for Object Localization in Aerial Scenes"**, exploit both multi-scale visual features and multi-granularity textual embeddings, resulting in more discriminative representations that address the challenges posed by large-scale variations and busy backgrounds. Vision-Semantic Multimodal Representation (VSMR) enhanced multi-level feature integration, refining how visual and textual features are jointly processed to improve localization robustness **Chen et al., "Multilevel Feature Integration for Object Localization in Visual Grounding of Aerial Scenes"**. Further improvements have been achieved through progressive attention mechanisms **Wang et al., "Progressive Attention Mechanisms for Object Localization in Visual Grounding of Aerial Scenes"**. The Language-guided Progressive Visual Attention (LPVA) framework, for example, dynamically adjusts visual features at various scales and levels, ensuring that the visual backbone concentrates on expression-relevant information **Chen et al., "Language-Guided Progressive Visual Attention for Object Localization in Aerial Scenes"**, This is complemented by multi-level feature enhancement decoders, which aggregate contextual information to boost feature distinctiveness and suppress irrelevant regions.