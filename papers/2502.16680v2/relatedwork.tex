\section{Related Work}
\label{sec:2}

\subsection{Referring Image Segmentation for Natural Images}
\label{sec:2.1}
RIS is a fundamental task in vision-language understanding, where the goal is to segment objects in an image based on a given natural language expression \citep{li2018referring, yu2016modeling, yu2018mattnet}. This task demands a fine-grained alignment between textual descriptions and visual features to correctly localize and delineate the referenced objects. Compared to conventional segmentation tasks that rely on predefined categories, RIS enables a more flexible and user-specific segmentation process.

In the early stages, initial RIS models relied primarily on Convolutional Neural Networks (CNNs) to extract visual features and Recurrent Neural Networks (RNNs) to process textual descriptions \citep{li2018referring, hu2016segmentation, nagaraja2016modeling}. These models performed feature fusion by concatenating visual and linguistic representations before feeding them into a segmentation head. Specifically, \cite{hu2016segmentation} first introduced RIS to address the limitations of traditional semantic segmentation when handling complex textual descriptions. Later, \cite{li2018referring} and  \cite{nagaraja2016modeling} explored bidirectional interactions between visual and textual features, improving the multimodal understanding of objects through structured representations. Further advancements introduced dynamic multimodal networks, such as the work by \cite{margffoy2018dynamic}, which incorporated recursive reasoning mechanisms to enhance the integration of linguistic and visual information.

As RIS models evolved, researchers recognised the importance of cross-modal feature alignment, leading to the introduction of attention-based strategies \citep{shi2018key, ye2019cross, hu2020bi}. For example, \cite{shi2018key} introduced a keyword-aware segmentation model, refining object-region relationships based on key linguistic cues. These approaches significantly improved object localisation and contextual interpretation in RIS tasks.  \cite{ye2019cross} proposed a cross-modal self-attention module to capture long-range dependencies between textual and visual elements, improving multimodal fusion. Similarly, \cite{hu2020bi} developed a bidirectional cross-modal attention mechanism, enabling deeper interaction between the modalities.

The recent emergence of Transformer-based architectures has significantly advanced RIS, offering global modelling capabilities and superior multimodal integration. Unlike CNN-based methods, which rely on local receptive fields, Transformers enable long-range dependencies and self-attention mechanisms, making them particularly effective for RIS \citep{ding2022vlt, yang2022lavt, liu2023gres}. Several notable works have leveraged this architecture. VLT designed a query-based Transformer framework, enriching textual comprehension by dynamically generating language query embeddings \citep{ding2022vlt}. LAVT proposed language-aware attention mechanisms to enhance early fusion between the two modalities, enabling more precise segmentation \citep{yang2022lavt}. GRES further refined multimodal alignment by explicitly modelling dependencies between different textual tokens and visual regions, leading to more robust segmentation performance \citep{liu2023gres}.

\subsection{ Referring Remote Sensing Image Segmentation}
\label{sec:2.2}
Referring Remote Sensing Image Segmentation (RRSIS) is a specialized task that aims to extract pixel-wise segmentation masks from remote sensing imagery based on natural language expressions \citep{yuan2024rrsis, liu2024rotated}. While it has significant applications in environmental monitoring, land cover classification, disaster response, and urban planning \citep{sun2022visual, li2024language}, progress in this field hinges critically on suitable datasets that capture the complexity of remote sensing imagery. One of the earliest datasets was RefSegRS, introduced by LGCE \citep{yuan2024rrsis}, which enabled initial efforts to adapt RIS methods from natural images to the remote sensing domain. To enhance the diversity and improve the generalizability of trained models, \cite{liu2024rotated} proposed RRSIS-D, a substantially larger dataset for benchmarking mainstream RIS models in remote sensing image segmentation. More recently, RISBench, has also been introduced to further advance the development and evaluation of RRSIS methods. 

Building on these datasets, recent RRSIS research has explored strategies to address scale variations, complex backgrounds, and orientation diversity. LGCE \citep{yuan2024rrsis} pioneered a language-guided cross-scale enhancement module to fuse shallow and deep features for improved segmentation accuracy, whereas \cite{liu2024rotated} proposed the Rotated Multi-Scale Interaction Network (RMSIN), which integrates intra-scale and cross-scale interactions alongside rotated convolutions to better handle directional variations. Beyond scale-aware models, \cite{pan2024rethinking} analysed the implicit optimization mechanisms in existing models and proposed an explicit affinity alignment approach, incorporating a new loss function to improve textual-visual feature interaction. More recent studies have introduced refined image-text alignment strategies to improve RRSIS performance. To be specific, FIANet \citep{lei2024exploring} introduced a fine-grained alignment module with object-positional enhancement, integrating a text-aware self-attention mechanism to refine segmentation accuracy. Similarly, CroBIM \citep{dong2024cross} leveraged a context-aware prompt modulation module, which optimizes post-fusion feature interactions and employs a mutual-interaction decoder to refine segmentation masks. Recently, SBANet \citep{li2025scale} introduced a bidirectional alignment mechanism and a scale-wise attention module to enhance mutual guidance between vision and language features, effectively refining segmentation masks in referring remote sensing image segmentation. BTDNet \citep{zhang2025referring} employs a bidirectional spatial correlation module and a target-background twin-stream decoder to improve multimodal alignment and fine-grained object differentiation, achieving improved segmentation performance.

\subsection{Visual Grounding for Aerial Images}
\label{sec:2.3}
Another active vision-and-language research in
remote sensing community is visual grounding for aerial images, focusing on localizing target objects within aerial scenes using natural language queries \citep{sun2022visual, zhao2021high, zhan2023rsvg}. In contrast to RRSIS, which demands detailed pixel-level masks, visual grounding is primarily concerned with identifying object-level regionsâ€”typically represented as bounding boxes \citep{sun2022visual}. This task leverages the unique characteristics of aerial imagery, where targets often exhibit complex spatial relationships and may not be visually prominent due to scale variations and cluttered backgrounds.

Early frameworks, such as GeoVG \citep{sun2022visual}, pioneered this approach by integrating a language encoder that captures geospatial relationships with an image encoder that adaptively attends to aerial scenes. By fusing these modalities, GeoVG established a one-stage process that effectively translates natural language cues into object localization. Building on this foundation, subsequent models have introduced advanced fusion strategies. For instance, modules like the Transformer-based Multi-Granularity Visual Language Fusion (MGVLF) \citep{zhan2023rsvg} exploit both multi-scale visual features and multi-granularity textual embeddings, resulting in more discriminative representations that address the challenges posed by large-scale variations and busy backgrounds. Vision-Semantic Multimodal Representation (VSMR) enhanced multi-level feature integration, refining how visual and textual features are jointly processed to improve localization robustness \citep{ding2024visual}. Further improvements have been achieved through progressive attention mechanisms. The Language-guided Progressive Visual Attention (LPVA) framework, for example, dynamically adjusts visual features at various scales and levels, ensuring that the visual backbone concentrates on expression-relevant information \citep{li2024language}. This is complemented by multi-level feature enhancement decoders, which aggregate contextual information to boost feature distinctiveness and suppress irrelevant regions.