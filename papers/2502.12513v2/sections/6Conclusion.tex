\section{Conclusion}
This paper explores two open-ended questions: 1) How to utilize multimodal interleaved documents for CLIP training. 2) How to effectively leverage both realistic and synthetic texts to enhance CLIP performance. To this end, we first establish a Real-World Data Extraction pipeline to extract high-quality images and texts. Then we design a hierarchical retrieval method to efficiently associate each image with multiple semantically relevant texts. To further enhance fine-grained visual information, we propose an image semantic augmented generation module for synthetic text production. Furthermore, we employ a semantic balance sampling strategy to improve dataset diversity, enabling better learning of long-tail concepts. Based on these innovations, we present \textit{RealSyn}, a dataset driven by both real and synthetic texts with three sizes: 15M, 30M, and 100M. We compare our dataset with other widely used datasets of equivalent scale for CLIP training. Models pre-trained on RealSyn consistently achieve state-of-the-art performance across various downstream tasks. Furthermore, extensive experiments confirm that \textit{RealSyn} significantly enhances contrastive vision-language representation learning and demonstrates robust scalability. We hope our work provides insights into vision-language representation learning.