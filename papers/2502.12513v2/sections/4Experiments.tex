\section{Experiments and Results}




\subsection{Implementation Details}
We initially collect 118M interleaved image-text documents from the OBELICS~\cite{Obelics} as our primary data source. We use OFA$_{base}$~\cite{OFA} and RAM++$_{large}$~\cite{RAM_plus_plus} to generate brief captions and semantic tags. To validate the dataset performance, we pre-train standard CLIP supervised by the text randomly selected from the three retrieved realistic texts and one synthetic text, inspired by the LaCLIP~\cite{laclip}. During pre-training, we adopt AdamW~\cite{AdamW} as the optimizer, with a learning rate of 1e-3 and a weight decay of 0.2. The parameters $\beta1$ and $\beta2$ are set to 0.9 and 0.98, respectively. The input image size is 224×224, and the input text sequence length is 77. The temperature parameter $\tau$ is initialized to 0.07. We train 32 epochs with 4096 batch sizes on 8 $\times$ A100 (80G) GPUs. Please refer to the supplementary material for more details.

To validate the effectiveness of the \dsname\ dataset, we compare \dsname\ with the previous datasets across various models and data scales. We compare \dsname15M with the YFCC15M filtered by DeCLIP~\cite{DECLIP}. Following ALIP~\cite{ALIP}, we also compare with LAION15M, LAION30M, and LAION100M~(subset randomly selected from LAION400M).


\begin{figure*}[t!]
    \centering
    \includegraphics[width=\linewidth]{Figures/topic_distribution_V2.pdf}
    \vspace{-6mm}
    \caption{A T-SNE~\cite{tsne} projection of LDA~\cite{LDA} topic cluster from a randomly selected 1M samples from \dsname. \dsname\ encompasses a broad range of everyday topics, e.g., animal, food, airplane, etc. We also display representative central images for each identified topic.}
    \vspace{-2mm}
    \label{fig: dataset_topic_distribution}
\end{figure*}

\subsection{Main Results}
\label{main_result}
\noindent{\bf Linear Probe.} In Table~\ref{table:linear_probe}, we report the linear probe performance of the ViT-B/32 model across 20 downstream datasets. When pretrained at the 15M scale, \dsname15M exceeds YFCC15M on 16 of 20 datasets, achieving an average performance increase of 6.9\%. Additionally, \dsname15M outperforms LAION15M on 18 of 20 datasets, with an average improvement of 1.6\%. With dataset scaling to 30M and 100M, \dsname\ achieves average performance improvements of 1.3\% and 1.4\% over LAION, respectively. These findings underscore the superior CLIP training efficiency of the \dsname\ dataset compared to the widely-used YFCC and LAION datasets across multiple scales.

\noindent{\bf Zero-shot Transfer.} We evaluate the zero-shot transfer performance of the ViT-B/32 model across 20 classification benchmarks using the same prompt templates as SLIP~\cite{mu2022slip}. As indicated in Table~\ref{table:zeroshot_classfication}, \dsname15M surpasses YFCC15M on 18 of 20 datasets, achieving an average performance improvement of 14.3\%. In comparison to LAION15M, \dsname15M excels on 18 of 20 datasets with an average improvement of 5.2\%. Upon expanding the dataset sizes to 30M and 100M, \dsname\ achieves average performance improvements of 3.5\% and 2.3\% compared to LAION, highlighting its efficiency and scalability.

It is important to note that \dsname\ demonstrates a significant decrease in performance on certain datasets, such as Cars and Flowers. This reduction is primarily attributed to the unique data distribution of \dsname, characterized by a scarcity of data for specific concepts, which hampers the model’s ability to effectively learn these concepts. For example, as shown in Figure~\ref{fig: dataset_topic_distribution}, samples related to cars and flowers represent only 0.9\% and 0.4\% of the dataset, respectively.


\noindent{\bf Zero-shot Image-Text Retrieval.} In Table~\ref{tab:retrieval}, we present the zero-shot image-text retrieval performance of the ViT-B/32 model pre-trained on different scales of datasets. \dsname\ achieves superior results across all evaluation metrics. Specifically, \dsname15M improves Recall@1 by 35.8\%\&26\% on Flickr30K~\cite{flickr30k} and by 22.5\%\&12.6\% on MSCOCO~\cite{mscoco_retrieval}. \dsname30M improves Recall@1 by 16.4\%\&11.6\% on Flickr30K~\cite{flickr30k} and by 12.3\%\&7.4\% on MSCOCO~\cite{mscoco_retrieval}. \dsname100M
improves Recall@1 by on 14.6\%\&8.4\% Flickr30K~\cite{flickr30k} and by 9\%\&5.4\% on MSCOCO~\cite{mscoco_retrieval}. This significant enhancement in cross-modal retrieval performance indicates that the \dsname\ dataset effectively improves contrastive vision-language representation learning by incorporating both realistic and synthetic texts, resulting in robust representations and enhanced cross-modal alignment.



\noindent{\bf Zero-shot Robustness.} To further validate the robustness of the \dsname\ dataset, we present the zero-shot robustness~\cite{CLIP} performance across various IN-1K~\cite{ImageNet} val set variants in Table~\ref{tab:robustness}. The results indicate that \dsname\ significantly enhances the robustness of vision-language pre-training models. Specifically, \dsname15M shows a performance increase of 13.1\% and 4.3\% compared to YFCC15M and LAION15M, respectively. When scaled to 30M and 100M, \dsname\ achieves additional improvements of 4.2\% and 2.8\% over LAION. This substantial enhancement in performance is primarily attributable to the utilization of retrieved realistic texts, which surpass the constraints of generative models, and the superior conceptual diversity relative to YFCC and LAION, thus significantly boosting model robustness.



\begin{figure}[!t]
\centering
    \begin{subfigure}{0.22\textwidth}
    \includegraphics[width=1.0\textwidth]{Figures/image_captioning/coco2017_name.png}
    \end{subfigure}
    \begin{subfigure}{0.22\textwidth}
    \includegraphics[width=1.0\textwidth]{Figures/image_captioning/flickr30K_name.png}
    \end{subfigure}
\vspace{-3mm}
\caption{Image captioning comparisons on COCO2017 and Flickr30k. B4, MT., RL. and Cd. represent the metric of BLEU~\cite{BLEU}, METEOR~\cite{METEOR}, ROUGE-L~\cite{METEOR}, and Cider~\cite{cider}.}
% \Description{}
\vspace{-3mm}
\label{image_captioning}
\end{figure}

\noindent{\bf Image Captioning via MLLM.} In Figure~\ref{image_captioning}, we present the image captioning performance of the LLaVA-1.5~\cite{llava1.5} trained using different datasets (LAION v.s. \dsname). Initially, we first map visual features into the textual domain using the initial 558k dataset from LLaVA-1.5. We then develop an image captioning dataset from both LAION and \dsname\ for instruction tuning. Specifically, we select 1M samples randomly from each dataset and train over two epochs. As depicted in Figure~\ref{image_captioning}, \dsname\ significantly outperforms LAION in all evaluation metrics on both the COCO2017~\cite{mscoco_retrieval} and Flickr30k~\cite{flickr30k} benchmarks. This notable performance enhancement confirms the higher quality and better image-text alignment of the \dsname\ dataset.


\begin{figure}[t!]
\centering
\begin{subfigure}{\linewidth}
{
\includegraphics[height=0.48\textwidth]{Figures/data_distribution/data_consine_similarity_analysis.png}}
{
\includegraphics[height=0.48\textwidth]{Figures/data_distribution/data_token_len.png}}
\caption{Richness assessment comparison}
\label{similarity_token}
\end{subfigure}

\begin{subfigure}{\linewidth}
\centering
\includegraphics[width=\linewidth]{Figures/data_richness/diversity_analysis.png}
\vspace{-7mm}
\caption{Diversity assessment comparison}
\label{fig:diversity}
\end{subfigure}
\vspace{-5mm}
\caption{The richness assessment and diversity assessment on different datasets. \dsname-R1: the most relevant retrieved realistic text. \dsname-S1: the semantic augmented synthetic text based on \dsname-R1.}
\vspace{-3mm}
\end{figure}
