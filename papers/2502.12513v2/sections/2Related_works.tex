\section{Related Work}
\label{sec:formatting}

\noindent{\bf Large-Scale Pre-training Dataset.} In recent years, several large-scale image-text datasets~\cite{MSCOCO, clark2017simple, goyal2017making, kakaobrain2022coyo700m, Densefusion1M} collected from the Internet have been released. The YFCC100M~\cite{YFCC100M} dataset provides a comprehensive overview of the evolution of photo and video documentation and sharing from the inception of Flickr in 2004 until early 2014. Due to download failures and non-English captions, DeCLIP~\cite{DECLIP} reprocesses a new version of the YFCC15M dataset. Additionally, the LAION400M~\cite{laion400M} dataset contains 400 million image-text pairs collected from Common Crawl and widely used in vision-language pre-training. Recent advancements have also introduced several large-scale interleaved image-text document datasets~\cite{omnicorpus, MMC4, Obelics}. The OBELICS~\cite{Obelics} dataset uses a comprehensive filtering strategy and includes 141 million web pages, 353 million associated images, and 115 billion text tokens extracted from Common Crawl. However, due to data format constraints and training inefficiencies, interleaved image-text documents are currently unsuitable for CLIP training.


\noindent{\bf Vision Language Pre-training.}
As a pioneering work in visual language pre-training, CLIP has attracted extensive attention due to its powerful zero-shot recognition and exceptional transfer learning performance~\cite{wang2024learn, tang2024amu, shao2024deil, martin2024transductive}. Inspired by CLIP, numerous visual-language pre-training works have been published in recent years~\cite{mu2022slip, DECLIP, ALIP}. SLIP~\cite{mu2022slip} enhances performance by combining self-supervised learning with CLIP pre-training. DeCLIP~\cite{DECLIP} increases pre-training efficiency by integrating multi-view supervision across modalities and nearest-neighbor supervision from similar pairs. To mitigate the influence of noisy data, ALIP~\cite{ALIP} introduces a gating mechanism that dynamically allocates weights to samples. Despite their advancements, these methods primarily depend on large-scale image-text pairs derived from the Internet. Recent studies~\cite{li2024scaling,wang2025scaling} demonstrate that the capabilities of CLIP enhance with the expansion of high-quality image-text datasets. Given the extensive use of internet-derived image-text data in existing datasets, there is a pressing need to develop a new data construction paradigm to further expand the scale of high-quality image-text data.


\noindent{\bf Synthetic Captions.} 
Recent works~\cite{ALIP, yu2024capsfusion, sharegpt4v} indicate that image-text pairs obtained from websites contain intrinsic noise, which directly impacts the effectiveness of vision-language pre-training. To enhance the quality of existing datasets, LaCLIP~\cite{laclip} uses the in-context learning capability of large language models to rewrite text descriptions associated with each image. CapsFusion~\cite{yu2024capsfusion} employs large language models to refine information from web-based image-text pairs and synthetic captions, improving the quality of multimodal pre-training data. 
Similarly, DreamLIP~\cite{DreamLIP} generates detailed descriptions for 30 million images using a pretrained large multimodal model. Nevertheless, these methods predominantly focus on synthetic data enhancement, neglecting the importance of real-world data. Furthermore, the diversity and distribution of synthetic captions generated by these methods are intrinsically constrained by the capabilities of the generative models employed.