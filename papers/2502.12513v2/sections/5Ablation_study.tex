\section{Analysis}

\begin{figure}[t!]
\centering
\includegraphics[width=\linewidth]{Figures/data_scaling_law.png}
\vspace{-5mm}
\caption{Data Scaling Analysis. We pretrain ViT-B/32 on \dsname\ in different data scales. \textcolor{red}{$\times$} represents the results predicted using our data scaling law.}
% \Description{}
\label{fig:data_scaling_analysis}
\vspace{-2mm}
\end{figure}

\subsection{Statistics Analysis} 
\noindent{\bf Topic-based Assessment.} Following MMC4~\cite{MMC4}, we ran LDA~\cite{LDA} on random sampling 1M image-realistic text pairs with 30 topics. Figure~\ref{fig: dataset_topic_distribution} presents the proportions and examples for six topics: animal, food, airplane, flower, automotive, and landmark. Notably, the dataset contains minimal samples related to ``flower'' and ``automotive'' topics, representing merely 0.4\% and 0.9\% of the total, respectively. This paucity of examples hinders the model's ability to sufficiently learn these concepts, thereby compromising its performance in the linear probe and zero-shot transfer evaluations on the Flowers and Cars datasets.


\noindent{\bf Richness Assessment.} Figure~\ref{similarity_token} presents image-text similarity and text token distribution of 15M samples from YFCC15, LAION, \dsname-R1 (the most relevant retrieved realistic text), and \dsname-S1 (the semantic augmented synthetic text based on \dsname-R1). Compared to datasets sourced from the Internet, \dsname\ demonstrates robust similarity metrics even after the removal of OCR data. This effectively explains the significant performance enhancement observed in the CLIP model trained on \dsname\ for image-text retrieval tasks.  Moreover, both the retrieved realistic texts and synthetic texts contain a larger quantity of words, which can provide a richer textual context that enhances vision-language representation learning.


\noindent{\bf Diversity Assessment.} The \dsname\ is constructed based on real-world interleaved image-text documents, which encompasses a wide array of diverse information. Following previous work~\cite{lai2024revisit}, we randomly select 0.2M samples to calculate the number of unique entities in the caption to assess the data diversity of different datasets. As depicted in Figure~\ref{fig:diversity}, both the retrieved realistic texts and image semantic augmented synthetic texts exhibit a higher number of distinct entities. Such diversity enriches the dataset, facilitating the model's acquisition of comprehensive knowledge and enhancing both performance and robustness.



\noindent{\bf Data Scaling Analysis.} We present the data scaling law~\cite{scaling_law} derived from our \dsname\ dataset, justifying its scalability over samples.
Specifically, we conduct a series of visual-language pre-trainings with proposed datasets ranging from 12M to 60M, and fit each performance metric to the inverse of logarithmic functions with respect to the number of millions of training samples $x$. Based on the fitting results from these preliminary experiments, we extrapolate each performance scaling law to $100M$ samples, and validate their predicted scaling trends with our \dsname$100M$ dataset as shown in Figure~\ref{fig:data_scaling_analysis}. Notably, as indicated by the coefficients shown in Eq.~\ref{eq:data_scaling_law}, these performance laws also likely suggest an upper bound of model capability that a ViT-B/32 could possibly reach through our proposed visual-language pre-training paradigm with multimodal interleaved documents:
\begin{equation}
\begin{aligned}
    \text{Linear Probe:} \ \mathcal{L}(x) &\approx \frac{-0.21}{log(x-4.23)} + 0.80 \\
    \text{Transfer:} \ \mathcal{L}(x) &\approx \frac{-0.30}{log(x-5.68)} + 0.62 \\
    \text{Robustness:} \ \mathcal{L}(x) &\approx \frac{-0.60}{log(x-3.17)} + 0.56
\end{aligned}
\label{eq:data_scaling_law}
\end{equation}



\noindent{\bf Model Scaling Analysis.} In Section~\ref{main_result}, \dsname\ exhibits superior performance across various data scales. To further explore the model scaling capability, we present the downstream task performance of three models in Figure~\ref{model_scale}. Notably, compared to LAION, \dsname\ demonstrates steeper slopes in performance curves across linear probe, zero-shot transfer, and robustness, indicative of its superior model scaling capabilities.


\begin{figure}[t!]
\centering
\includegraphics[width=1\linewidth]{Figures/model_scaling/model_scaling_combined.png}
\vspace{-5mm}
\caption{\textbf{Model scaling capability.} We compare the models pre-trained on LAION30M and \dsname30M.}
\label{model_scale}
% \vspace{-2mm}
\end{figure}


\input{Tables/Concept_balance}

\input{Tables/Ablation_input_texts}

\noindent{\bf Extension to Pure Image.} 
To further extend our transformation paradigm for pure images, we conduct experiments on ImageNet~\cite{ImageNet}. Initially, we retrieve semantically relevant realistic texts for each ImageNet image from our sentence database and generate image semantic augmented synthetic texts. Then, we pre-train ResNet50~\cite{he2016deep} supervised by the text randomly selected from the retrieved realistic texts and synthetic texts. Comparative analysis with SimCLR~\cite{chen2020simple} under identical conditions shows a linear probe average performance enhancement of 2.1\% across 12 datasets using our constructed data. Detailed experimental results are provided in the supplementary material.



\subsection{Ablation Study}

\noindent{\bf Ablation on Semantic Balance Sampling.} To demonstrate the efficacy of our proposed semantic balance sampling method, we contrast it with random sampling. As shown in Table~\ref{tab:concept_balance}, semantic balance sampling achieves performance improvements of 0.7\%, 1.1\%, and 1.0\% in linear probe accuracy, zero-shot transfer, and robustness, respectively. Besides, we visualize the data distribution of 15M samples clustered into 1M centers using different sampling strategies. As shown in Figure~\ref{fig:clustering_distribution} reveals that semantic balance sampling results in a smoother distribution, thereby enhancing the learning of long-tail concepts.



\begin{figure}[t!]
\centering
\includegraphics[width=\linewidth]
{Figures/concept_balance/Image_concept_balance_resize.png}
\vspace{-3mm}
\caption{Clustering distribution of 15M data obtained from random sampling and semantic balance sampling.}
\label{fig:clustering_distribution}
\vspace{-3mm}
\end{figure}




\noindent{\bf Ablation on Realistic Texts and Synthetic Texts.} We conduct ablation studies to assess the impact of varying quantities of realistic and synthetic texts on the performance of the CLIP-B/32 model. As shown in Table~\ref{tab:ablation-text}, incrementally increasing the amount of realistic text from one to three enhances model performance, attributed to improved text augmentation that integrates extensive real-world knowledge. However, further increasing this quantity from three to five slightly diminishes performance due to information saturation and the introduction of noise. Conversely, augmenting the number of synthetic texts from one to five incrementally degrades performance, reflecting increased noise introduction. Notably, training with only realistic texts significantly boosts performance, achieving a 71.2\% accuracy compared to 69.8\% with the LAION15M dataset, underscoring the vital role of real-world knowledge in advancing vision-language representation learning.



\noindent{\bf Ablation on the Combination of the Realistic Texts and Synthetic Texts.} Table~\ref{tab:ablation-text} presents the results of ablation experiments on text augmentation using different text types. Introducing image semantic augmented synthetic text, which supplements fine-grained visual semantic information, leads to performance enhancements of 0.2\%, 1.1\%, and 0.8\% in linear probe accuracy, zero-shot transfer, and zero-shot robustness, respectively, compared to using only retrieved realistic texts.


\definecolor{fig2_green}{HTML}{419A23}
\begin{figure}[!t]
\centering
\includegraphics[width=\linewidth]{Figures/visible_example.pdf}
\vspace{-5mm}
\caption{Visualization of the raw interleaved document, the retrieved realistic text, and synthetic text. Image semantic-related information is highlighted in \textcolor{fig2_green}{green}.}
% \Description{}
\label{fig:visualization_example}
\vspace{-2mm}
\end{figure}

\noindent{\bf Case Study.} In Figure~\ref{fig:visualization_example}, we present the visualization of retrieved realistic text and synthetic text obtained from an interleaved image-text document using our proposed transformation paradigm. Both realistic and synthetic texts contain extensive descriptive information consistent with the image semantics, such as ``lyell glacier'', ``crags'', and ``valley''. We provide more visualizations in the supplementary material.





