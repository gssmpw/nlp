\section{\textit{RealSyn} Dataset}

\subsection{Real-World Data Extraction}
To transform interleaved image-text documents for vision-language representation learning, we establish a Real-World Data Extraction pipeline~(Figure~\ref{fig: data_filtering}) to extract high-quality images and texts. This pipeline consists of three steps: Data Extraction, Image Filtration, and Sentence Filtration.

\noindent{\bf Data Extraction.} We employ 118M interleaved image-text documents from the OBELICS~\cite{Obelics} as the primary data source. All images are extracted and stored in a dedicated image database, while sentences are segmented using the Natural Language Toolkit (NLTK)~\cite{nltk} and stored in a separate sentence database. This process yields 336M images and 2.13B sentences from the interleaved documents.

\noindent{\bf Image Filtration.}
After extracting 336M images, we apply a two-stage filtering process to ensure data quality and reduce redundancy. First, we discard images that meet any of the following criteria: 1) the shorter dimension is fewer than 100 pixels, or 2) the aspect ratio exceeds 3 or is below 1/3. This step removes 51M low-quality images. Next, following CLIP-CID~\cite{CLIP_CID}, we use the EVA02-CLIP E/14-plus model~\cite{eva_clip} to extract image embeddings and apply the Union-Find algorithm~\cite{union_find} to eliminate perceptually and semantically redundant images. This step removes an additional 87M images, resulting in a refined dataset of 198M high-quality images.



\noindent{\bf Sentence Filtration.}
After extracting 2.13B sentences from interleaved image-text documents, we conduct rigorous filtering based on quality, semantics, and redundancy. Initially, we eliminate sentences based on the following criteria: 1) presence of emojis or URLs; 2) sentences containing fewer than 3 or more than 81 words; and 3) following CAT~\cite{CAT_rule}, we retain samples with at least C1 caption complexity and incorporating an action. This phase reduces the corpus size from 2.13B to 1.82B sentences. Then we apply semantic filtering to the remaining sentences, eliminating those with minimal information assessed through information entropy:
\begin{equation}
\label{equation:IE}
% \small
\theta(x) = -\sum_{i=1}^n p(x_{i}) \log p(x_{i}),
\end{equation}
where $n$ denotes the number of words in a sentence, $x_{i}$ represents the $i$-th words in the sentences $x$, and $p(x_{i})$ is the probability of the word $x_i$ in the entire corpus. Based on human cognition principles and empirical experience, we filter out sentences with a score below 0.3. To further refine the corpus by removing difficult or ambiguous sentences, we use GTP2-large~\cite{gpt2} to calculate the perplexity score $\mathcal{PPL}$ for each sentence:
\begin{equation}
\label{equation:perplexity}
% \small
\mathcal{PPL}(x) = \exp \left\{-\frac{1}{t} \sum_{i=1}^{t} \log p_{\theta}(x_i \mid x_{<i}) \right\},
\end{equation}
where $t$ represents the token number of the sentence, and $p_{\theta}(x_i \mid x_{<i})$ is the likelihood of the $i$-th token given the previous tokens. We engage three human experts to determine the minimum and maximum values of the perplexity interval for high-quality sentences by comparing sentences of varying perplexity levels. Sentences within the average minimum~(30) and maximum perplexity~(200) intervals are retained. The overall semantics filtering reduces the corpus to 1.16B sentences. In the final stage, similar to redundancy image filtering, we perform both perceptual and semantic deduplication of sentences. This process results in a refined corpus of 0.84B sentences that include extensive real-world knowledge.

\begin{figure}[t]
    \centering  \includegraphics[width=\linewidth]{Figures/framework.pdf}
    \vspace{-3mm}
    \caption{The architecture of our proposed framework, which constructs distinct image-text pairs from real-world data extracted from interleaved documents via retrieval and generation.}
    % \Description{}
    \label{fig: pipeline}
    \vspace{-3mm}
\end{figure}

\input{Tables/Linear_probe}
\input{Tables/Zeroshot_classification} 

\subsection{\bf Retrieval and Generation Framework} 
After extracting high-quality images and sentences from documents, we propose an efficient and scalable framework to retrieve multiple semantically relevant texts for each image and leverage large language models to integrate retrieved realistic text with fine-grained visual information and generate synthetic text. As shown in Figure~\ref{fig: pipeline}, the architecture of our framework primarily consists of three components: Text Semantic Clustering, Hierarchical Retrieval, and Image Semantic Augmented Generation.



\noindent{\bf Text Semantic Clustering.} To efficiently retrieve multiple semantically relevant texts for each image, we initially encode all sentences using the EVA02-CLIP E/14-plus~\cite{eva_clip} model. Inspired by Unicom~\cite{unicom}, we utilize the standard $K$-Means algorithm~\cite{kmeans} offline cluster all sentences into a specified number of clusters. To minimize intra-cluster and inter-cluster conflicts, we initially establish the optimal number of cluster centroids through small-scale experiments. Subsequently, we divide the 0.84 billion texts into two million clusters, utilizing efficient feature quantization techniques~\cite{johnson2019billion}.

\noindent{\bf Hierarchical Retrieval.} Given the prohibitive computational cost of direct semantic text retrieval across 0.84B sentences (requiring over 10,000 GPU-hours on 8×A100), we propose a hierarchical retrieval framework to enhance efficiency. Given an image, we perform inter-cluster retrieval to identify the most relevant cluster center for the image. Subsequently, we conduct intra-cluster retrieval within the identified cluster to obtain multiple real-world sentences semantically matched to the image. This approach achieves scalable retrieval of 198M images and 0.84B sentences within 40 GPU-hours on an 8×A100.


\noindent{\bf Image Semantic Augmented Generation.} Although the retrieved realistic texts achieve satisfactory performance, they exhibit limitations in capturing fine-grained visual semantics. To address this issue, we introduce the Image Semantic Augmented Generation module. This module initially employs the OFA~\cite{OFA} model to generate a concise caption for each image. We then integrate the open-set image tagging model RAM++~\cite{RAM_plus_plus}, which extracts object detection tags. Considering that RAM++ supports only 4,000 tags, we expand this set to 8,000 by incorporating an additional 4,000 tags derived from real-world sentences. Following CapsFusion~\cite{yu2024capsfusion}, we utilize ChatGPT4 Turbo to merge the retrieved realistic texts with concise captions and image tags to construct a 100K instruction dataset (the prompt we used is presented in the supplementary material). Subsequently, we conduct instruction tuning of the LLaMA3-8B model~\cite{llama3} using the LLaMA Factory~\cite{llamafactory} and deploy the vLLM~\cite{vLLM} for large-scale inference. Ultimately, we convert data from 118M multimodal interleaved documents into 198M image-text pairs, where each image is associated with multiple retrieved realistic texts and synthetic texts.

\input{Tables/Zeroshot_retrieval}
\input{Tables/Zeroshot_robustness}

\subsection{Semantic Balance Sampling}
To further improve the quality and diversity of our dataset, we implement semantic balancing sampling across the 198M image-text pairs. Specifically, we use EVA02-CLIP E/14-plus~\cite{eva_clip} to encode and calculate the cosine similarity between images and synthetic texts. To reduce the impact of OCR-related or mismatched pairs during pre-training, we filter out 29.7M pairs with cosine similarities scores either above 0.61 or below 0.51, as determined through human verification. Inspired by MetaCLIP~\cite{xu2023metaclip}, we introduce an easy but efficient cluster-based semantic balance sampling strategy. We cluster the image embeddings from the remaining 168.3M pairs into 1M centers. To enhance the semantic diversity of our dataset, we randomly select 20, 35, and 180 samples from clusters exceeding these thresholds, while retaining all samples from smaller clusters.  This approach culminates in the construction of the \dsname15M, \dsname30M, and \dsname100M datasets.





