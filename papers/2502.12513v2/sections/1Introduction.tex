\section{Introduction}
\label{sec:intro}
The rapid proliferation of mobile networks and social platforms has led to exponential growth in large-scale data, offering a robust foundation for contrastive vision-language representation learning~\cite{guo2019deep,baltruvsaitis2018multimodal, guo2024regiongpt, zhu2024llms, yao2024visual, zheng2024picture, guo2024open}. By predicting the correspondence between images and description texts, Contrastive Language-Image Pre-training (CLIP)~\cite{CLIP} pre-trains separate uni-modal encoders and achieves excellent performance across a range of downstream tasks, including image captioning~\cite{li2023blip,mokady2021clipcap}, object detection~\cite{wu2023cora,lin2023gridclip}, and semantic segmentation~\cite{he2023clip,zhou2023zegclip,lin2023clip}.
Notably, the representational capacity of such models improves significantly with the scale of the training dataset~\cite{li2024scaling}. Designing a new data paradigm to expand the scale and diversity of image-text datasets, thereby continuously improving the capabilities of pre-trained vision-language representations, represents a significant and challenging issue.


\begin{figure}[t!]
    \centering
    \includegraphics[width=0.95\linewidth]{Figures/motivation.pdf}
    % \vspace{-3mm}
    \caption{Multimodal interleaved documents are unsuitable for CLIP training. We construct distinct image-text pairs from such documents via retrieval and generation.}
    % \description{}
    \label{fig: motivation}
    \vspace{-5mm}
\end{figure}

\begin{figure*}[t!]
    \centering
    \includegraphics[width=0.98\linewidth]{Figures/data_filter.pdf}
    \vspace{-3mm}
    \caption{The Real-World Data Extraction pipeline to extract high-quality images and texts from interleaved image-text documents.}
    \vspace{-2mm}
    \label{fig: data_filtering}
\end{figure*}

In recent years, the introduction of large-scale image-text pair datasets~\cite{laion400M, laion5B, YFCC100M, kakaobrain2022coyo700m, Datacomp} has substantially advanced vision-language representation learning. Due to download failures and non-English captions in the YFCC~\cite{YFCC100M} dataset, DeCLIP~\cite{DECLIP} reprocesses a new version of the YFCC15M dataset. LAION400M~\cite{laion400M} provides 400 million image-text pairs collected from the web and filtered using CLIP similarity. COYO700M~\cite{kakaobrain2022coyo700m} offers 747 million high-quality pairs curated through advanced filtering strategies, including CLIP similarity, watermark detection, and aesthetic scoring. Despite these advancements, a substantial amount of non-paired data, particularly interleaved image-text documents~\cite{MMC4, Obelics, omnicorpus} containing multiple images and text paragraphs without explicit correspondence, remains incompatible with conventional vision-language representation learning methods.


Recent studies~\cite{laclip, yu2024capsfusion, DreamLIP, Veclip} aim to enhance the quality of open-source image-text datasets using existing models. For instance, LaCLIP~\cite{laclip} leverages large language models (LLMs) to rewrite raw captions for better alignment with images. CapsFusion~\cite{yu2024capsfusion} fine-tunes an LLM using a ChatGPT-generated instruction dataset to mitigate hallucination issues. DreamLIP~\cite{DreamLIP} re-captions 30 million images with detailed descriptions using a pre-trained large multimodal model. However, these methods primarily enhance data quality by focusing on synthetic data, thereby overlooking the significance of real-world data. Moreover, the diversity and distribution of synthetic captions produced by these approaches are inherently restricted by the limitations of the underlying generative models.


In this paper, we explore two fundamental questions: \textbf{1) How to utilize multimodal interleaved documents for CLIP training. 2) How to effectively leverage both realistic and synthetic texts to enhance CLIP performance.} As depicted in Figure~\ref{fig: motivation}, we initially develop a Real-World Data Extraction pipeline to extract high-quality images and texts from real-world multimodal interleaved documents. Subsequently, we design a hierarchical retrieval method to efficiently associate each image with multiple semantically relevant texts. To improve fine-grained image understanding, we introduce a visual semantic augmented generation module for synthetic text production. Additionally, we implement a semantic balance sampling strategy to enhance dataset diversity and facilitate the learning of long-tail concepts. Leveraging these innovations, we construct the \textit{RealSyn} dataset, which incorporates both realistic and synthetic texts in three sizes: 15M, 30M, and 100M. We evaluate the \textit{RealSyn} dataset against other widely used datasets of similar scale for CLIP training. Models pre-trained on \textit{RealSyn} consistently demonstrate state-of-the-art performance on various downstream tasks, including linear probe, zero-shot transfer, zero-shot robustness, and zero-shot retrieval. Extensive experiments validate that \textit{RealSyn} is effective for contrastive vision-language representation learning and shows excellent scalability. The main contributions of this paper are summarized as follows:
\begin{itemize}[leftmargin=*]
\item We propose an \textbf{effective and scalable multimodal interleaved document transformation paradigm} for contrastive vision-language representation learning.
\item We release \textit{RealSyn}, \textbf{a large-scale semantic balanced dataset} that integrates both realistic and synthetic texts and is available in three sizes: 15M, 30M, and 100M.
\item We conduct \textbf{extensive experiments} and demonstrate the effectiveness and scalability of our proposed \textit{RealSyn} dataset for CLIP training.
\end{itemize}

