\appendix


This supplementary material introduces the experiment settings and the instruction prompt we used in Section~\ref{sec: detail experiment settings}. Then, we introduce the downstream datasets, detailed model scaling results, comparison with the rewrite datasets, and analyze the combination \dsname with LAION and detailed results on pure image in Section~\ref{sec: detail external results}. We further analyze our proposed \dsname\ dataset by comparison with current datasets and visualize examples in Section~\ref{sec: further analysis of data}. Finally, we discuss the limitations of this paper in Section~\ref{sec: limitation}.

\section{Detail Experiment Settings}
\label{sec: detail experiment settings}

\subsection{Experiment Settings}
In Table~\ref{tab:hyperparams}, we present the detailed settings used in training CLIP.
\input{Tables_supp/Experimental_setting}

\subsection{Detail Instruction Prompt}
The prompt we used for ChatGPT to construct the 100K instruction dataset is present in the following:
\\
\\
\emph{"Please merge the information from the given raw text and the synthetic caption with the help of the highly relevant detection tags. The raw caption offers detailed real-world information, yet it suffers from flaws in sentence structure and grammar. The synthetic caption exhibits impeccable sentence structure but often lacks in-depth real-world details and may contain false information. The highly relevant detection tags are provided to enrich the semantic information of the raw caption, while some are redundant and noisy. You are a great information integration and summary expert, you are also good at enriching semantic information. Ensure a well-structured sentence while retaining the detailed real-world information provided in the raw caption. Avoid simply concatenating the sentences and avoid adding external information to describe. Correct and simplify sentences finally. Raw caption:\textless raw caption\textgreater, synthetic caption:\textless synthetic caption\textgreater, and highly relevant detection tags:\textless detection tags\textgreater".}

\section{Detail External Results}
\label{sec: detail external results}
\input{Tables_supp/linear_probe_datasets}
\input{Tables_supp/Data_scaling_linear}
\input{Tables_supp/Data_scaling_transfer}
\input{Tables_supp/Data_scaling_robustness}
\input{Tables_supp/CMP_DreamLIP}
\input{Tables/laion_RS_merge}
\input{Tables_supp/SimCLR_comparison}
\input{Tables_supp/Dataset_comparison}

\subsection{Downstream Datasets}

\sloppy
To comprehensively demonstrate the performance of CLIP trained on \dsname, we compared the linear probe results of CLIP trained on \dsname, YFCC~\cite{YFCC100M}, and LAION~\cite{laion400M} across 20 datasets. These datasets
include Food101~\cite{bossard2014food}, 
CIFAR10~\cite{krizhevsky2009learning}, 
CIFAR100~\cite{krizhevsky2009learning}, 
Birdsnap~\cite{berg2014birdsnap},
SUN397~\cite{xiao2010sun},
Stanford Cars~\cite{KrauseStarkDengFei-Fei_3DRR2013},
FGVC Aircraft~\cite{maji2013fine},
DTD~\cite{cimpoi2014describing},
Pets~\cite{parkhi2012cats}, 
Caltech101~\cite{fei2004learning},
Flowers102~\cite{nilsback2008automated},
SLT10~\cite{coates2011analysis},
EuroSAT~\cite{helber2019eurosat},
RESISC45~\cite{cheng2017remote},
KITTI~\cite{geiger2012we},
Country211~\cite{CLIP},
UCF101~\cite{soomro2012ucf101},
Hateful Memes~\cite{kiela2020hateful},
SST2~\cite{CLIP}, and
ImageNet~\cite{ImageNet}. Details on each dataset and the corresponding evaluation metrics are provided in Table~\ref{linearprobedatasets}.

\subsection{Detailed Model Scaling Results}


\noindent{\bf Linear Probe.} In Table~\ref{table:linear_probe_30M_supp}, we present the detailed linear probe results of different scale CLIP models trained on the 30M dataset. The ViT-L/14 trained on \dsname30M achieves an average performance improvement of 3.0\% across 20 datasets compared to the model trained on the LAION30M.

\noindent{\bf Zero-shot Transfer.} As shown in Table~\ref{table:zeroshot_classfication_30M_supp}, ViT-L/14 trained on our proposed \dsname30M outperforms LAION30M on 18 of 20 downstream datasets and achieves an average improvement of 5.5\%.

\noindent{\bf Zero-shot Robustness.} We present the detailed zero-shot robustness performance in Table~\ref{tab:robustness_30M_supp}, compared with LAION30M, the model trained on \dsname30M boosts average robustness performance by 8.6\% on ViT-L/14.

\subsection{Comparison with the Rewrite Dataset.} 
Due to LaCLIP~\cite{laclip} and Capsfusion~\cite{yu2024capsfusion} do not provide the model weights trained on the same scale dataset as ours, We compared the same scale model ViT-B/16 pre-trained on the \dsname\ with DreamLIP~\cite{DreamLIP}.

As shown in Table~\ref{tab:DreamLIP_cmp}, ViT-B/16 pre-trained on \dsname15M can achieve 1.3\% and 8.3\% average improvement compared to DreamLIP proposed 15M Rewriting datasets based on YFCC15M~\cite{YFCC100M}. Furthermore, compared with DreamLIP's proposed 30M, which combines YFCC15M, CC12M~\cite{CC12M} and CC3M~\cite{CC12M} dataset, ViT-B/16 training on \dsname30M can also outperform it in the linear probe and zero transfer. This significant improvement indicates that realistic knowledge is important for the CLIP model.

\subsection{Combine \dsname\ with LAION}
As shown in Table~\ref{tab: Data_Scaling_Comparison}, integrating \dsname15M with LAION15M yields average improvements of 0.6\% in linear probe, 0.2\% in zero-shot transfer across 20 datasets, and a robustness increase of 0.6\% compared to LAION30M. Comparing \dsname30M with LAION30M, the former shows significant performance gains, with average improvements of 1.3\% in linear probe, 3.5\% in zero-shot transfer, and a robustness enhancement of 4.2\%. The experimental results demonstrate that \dsname\, when combined with existing datasets, achieves significant performance improvements, while also validating the robustness and extensibility of \dsname.





\subsection{Detailed Results on Pure Image}
To further extend our method for pure images, we conduct experiments on ImageNet~\cite{ImageNet}. For each image, we retrieve three semantically relevant real-world sentences from our pre-constructed sentence database and generate a single semantically augmented synthetic caption based on the top retrieved text. Following SimCLR~\cite{chen2020simple}, we utilize 4096 batch size and pre-train ResNet50~\cite{he2016deep} for 90 epochs supervised by the text randomly selected from the three retrieved realistic texts and one synthetic text.

As shown in Table~\ref{sim_clr_comparison}, compared with SimCLR~\cite{chen2020simple} under the same conditions, the model trained on our constructed image-text pairs shows an average performance improvement of 2.1\% across 12 downstream datasets. The results demonstrate that our method can effectively transform pure images into high-quality image-text pairs through retrieval and generation for vision-language pre-training.



\section{Further Analysis of \dsname}
\label{sec: further analysis of data}

\subsection{Compare with Existing Datasets}
In Table~\ref{public_datasets_comparison}, we compare our proposed \dsname \ dataset with existing widely used large-scale image-text pre-training datasets. Compared to previous datasets, our proposed \dsname\ dataset provides four textual descriptions for each image, with an average token length of 36-40, significantly higher than LAION400M and YFCC15M. Furthermore, unlike previous datasets, the \dsname\ dataset is sourced from real-world interleaved image-text documents and includes both realistic and synthetic texts, thereby expanding the scope for future research exploration.

\subsection{Visualization of Examples}

\definecolor{fig2_green}{HTML}{419A23}
\definecolor{fig2_red}{HTML}{B00003}
\begin{figure*}[t]
    \centering
    \includegraphics[width=0.95\linewidth]{Figures/supp_images/visualization_examples.pdf}
    \vspace{-8mm}
    \caption{Visualization of image-text pairs in our proposed \dsname\ dataset. $T^{k}_{r}$: the $k$-th retrieved realistic text. $T^{k}_{s}$: the image semantic augmented synthetic text for $T^{k}_{r}$. Image semantic-related information is highlighted in \textcolor{fig2_green}{green}.}
    % \description{}
    \label{visualization_example1}
    % \vspace{-5mm}
\end{figure*}

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.95\linewidth]{Figures/supp_images/visualization_examples2.pdf}
    \vspace{-8mm}
    \caption{Visualization of image-text pairs in our proposed \dsname\ dataset. $T^{k}_{r}$: the $k$-th retrieved realistic text. $T^{k}_{s}$: the image semantic augmented synthetic text for $T^{k}_{r}$. Image semantic-related information is highlighted in \textcolor{fig2_green}{green}.}
    \label{visualization_example2}
    % \vspace{-5mm}
\end{figure*}

In Figure~\ref{visualization_example1} and Figure~\ref{visualization_example2}, we visualize additional image-text pairs randomly selected from our proposed \dsname\ dataset. $T^{k}_{r}$ is the $k$-th retrieved semantically relevant real-world sentence and $T^{k}_{s}$ is the semantic augmentic caption for $T^{k}_{r}$. We also highlighted the image semantic-related information in \textcolor{fig2_green}{green} and marked the image size below the image. 


\section{Limitations}
\label{sec: limitation}
To provide more fine-grained visual information, this study employs vision expert models in conjunction with a Large Language Model (LLM) to generate synthetic text. Considering inference costs and efficiency, further exploration of Modified Large Language Models (MLLM) for synthetic text creation is suggested for the community. Additionally, constrained by computational resources, this paper constructs a 100M-scale \dsname\ dataset exclusively using OBELICS~\cite{Obelics}. Notably, the transformation paradigm presented here is directly applicable to other multimodal document datasets, including MMC4~\cite{MMC4} and OmniCorpus~\cite{omnicorpus}.