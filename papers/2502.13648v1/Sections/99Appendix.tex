\appendix
\section*{Appendix}

\section{Additional Implementaion Details}
\label{appendix:implementationDetails}


% Datasets
\subsection{Datasets}

Total number of samples for each dataset is in Table \ref{table:dataset_stat}.
Each sample includes a question, original answer, conflicting answer, and four types of context: original, conflicting, random-irrelevant, and retrieved-irrelevant.


\paragraph{NaturalQuestionsShort \cite{kwiatkowski-etal-2019-natural}}
Questions consist of real queries issued to the Google search engine.
From a Wikipedia page from the top 5 search results, annotators select a long answer containing enough information to completely infer the answer to the question, and a short answer that comprises the actual answer.
The long answer becomes the context matched with the question, while the short answer being used as the answer.

\paragraph{TriviaQA-web \cite{joshi2017triviaqalargescaledistantly}}
Question-answer pairs are authored by trivia enthusiasts and independently gathered evidence documents that provide high quality supervision for answering the questions.
The web version of TriviaQA is used, where the contexts are retrieved from the results of a Bing search query.

\paragraph{HotpotQA \cite{yang2018hotpotqadatasetdiverseexplainable}}
Questions are diverse and not constrained to any pre-existing knowledge base. Multi-hop reasoning is required to solve the questions.
Paragraphs that provide supporting facts required for reasoning, are given along with the question.
In the original setting, additional distractor paragraphs are augmented in order to increase the difficulty of inference. However, these distractor paragraphs are not used in this setting.

\paragraph{SQuAD \cite{rajpurkar2016squad100000questionsmachine}}
Paragraphs from Wikipedia are presented to crowdworkers, and they are asked to write questions that entail extractive answers.
The answer to each question is a segment of text from the corresponding reading passage.
To remove the uncertainty that excessively long paragraphs bring, QA pairs that do not align with the first 800 tokens are discarded in this setting.

\paragraph{BioASQ \cite{tsatsaronis2015overview}}
BioASQ is a challenge that assesses the ability of systems to semantically index large numbers of biomedical scientific articles and return concise answers to given natural language questions.
Each question is linked to multiple related science articles. The full abstract of each linked article is used as an individual context. Abstracts that do not exactly contain the answer, are discarded.

\paragraph{TextbookQA \cite{Kembhavi_2017_CVPR}}
TextbookQA aims at answering multimodal questions when given a context in formats of text, diagrams and images.
This dataset is collected from lessons from middle school Life Science, Earth Science, and Physical Science textbooks.
Questions that are accompanied with a diagram and "True of False" questions are not used in this setting.

\paragraph{RelationExtraction \cite{levy2017zeroshotrelationextractionreading}}
Given labeled slot-filling examples, relations between entities are transformed into QA pairs using templates. Multiple templates for each type of relation are utilized.
The zero-shot benchmark split of this dataset, which showed that generalization to unseen relations is possible at lower accuracy levels, is used. 


\input{Assets/Tables/dataset_stat}

\subsection{Templates}

% closed book template
\input{Assets/Templates/cls}
% open book template
\input{Assets/Templates/opn}
% conflicting word generation template
\input{Assets/Templates/conflict}


Template \ref{template:cls} is used to perform closed-book generation for estimating the presence of parametric knowledge.
For \naive\ generation, Template \ref{template:opn} is used.
Template \ref{template:conflict} is employed to generate conflicting answers.

\subsection{Abstention Words}

The predefined abstain words used in evaluations are: [
\texttt{'unanswerable',
    'unknown', 'no known', 'not known', 'do not know'
    'uncertain', 'unclear',
    'no scientific evidence',
    'no definitive answer', 'no right answer', 'no concrete answer',
    'no public information',
    'debate',
    'impossible to know', 'impossible to answer',
    'difficult to predict',
    'not sure',
    'irrelevant', 'not relevant'}]


\subsection{Context Construction}
\label{appendix:datasetConstruction}

To ensure context informativeness and maintain experimental controllability, we have processed the original contexts from the MRQA benchmark by limiting their length and ensuring that the ground-truth answer span is always included.
For each occurrence span of the ground-truth answer in the raw context, we take a 100-word portion surrounding that span and consider it a candidate context.
We then compute the NLI ({\scshape BART-Large}, \citealp{lewis-etal-2020-bart}) score between the question-answer pair and each candidate context, and select the context with the highest NLI score as the original context.

To obtain retrieved-uninformative contexts, {\scshape Contriever-msmarco} \citep{izacard2022unsupervised} is utilized as a retriever model.




\subsection{Hyperparameters for Training}

We train the model for three epochs using the AdamW \cite{loshchilov2017decoupled} optimizer with a learning rate of 0.0001 and a batch size of 16. For efficient fine-tuning, we employ QLoRA \cite{dettmers2023qlora} with rank r=4 and alpha=16.
Training is conducted on a single NVIDIA RTX A6000.



\section{Additional Results}


\input{Assets/Tables/eval_v2_table1_base}
\input{Assets/Tables/eval_v2_table1_chat}

Table \ref{table:eval_v2_table1_base} presents the exact reliability scores for each dataset, which are averaged in Figure \ref{figure:eval_v2_avg_base}.
Table \ref{table:eval_v2_table1_chat} reports the reliability scores of chat or instruct LLMs. They exhibit similar or lower reliability scores compared to the base models.


\input{Assets/Tables/analysis_v6_irr_trained_ID}
\input{Assets/Tables/analysis_v6_irr_trained_OOD}


The exact values visualized in Figure \ref{figure:eval_v6_ID_OOD} are provided in Table \ref{table:analysis_v6_irr_trained_ID} and Table \ref{table:analysis_v6_irr_trained_OOD}.

\input{Assets/Tables/reliability_train_ood}
Table \ref{table:reli_train_ood} presents the overall performance on out-of-domain datasets, which are not included in Table \ref{table:reli_train}.