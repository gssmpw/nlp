\section*{Limitations}

\paragraph{Factuality of External knowledge}
This study assumes external knowledge is always factually accurate, considering scenarios involving changed or newly emerging facts \citep{longpre-etal-2021-entity, xie2023adaptive}.
The research field of factuality verification on external knowledge using LLMs runs parallel to our work on external knowledge handling \citep{yu-etal-2024-truth, fatahi-bayat-etal-2023-fleek}.
Investigating this aspect, alongside the factors addressed in this paper, could provide valuable insights into enhancing the model's understanding and represent a promising direction for future work.


\paragraph{Training Strategies for Alignment}
Our study focuses on aligning knowledge handling through supervised fine-tuning, leveraging the two key aspects of knowledge defined in our framework. 
While this approach effectively improves the model's reliability, future work could apply diverse alignment approaches \citep{rafailov2023direct, tian2024finetuning} to further enhance the overall knowledge-handling ability. 
Expanding the range of training strategies may provide deeper insights into optimizing knowledge utilization across different scenarios and improve the robustness of model behavior in diverse settings.

