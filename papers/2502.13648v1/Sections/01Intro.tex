\section{Introduction}


Recent large language models (LLMs) acquire extensive parametric knowledge through large-scale pre-training, enabling them to perform a wide range of knowledge-intensive tasks \citep{grattafiori2024llama3herdmodels, qwen2.5}.
However, the parametric knowledge embedded within LLMs has inherent limitations, as it is constrained by the data used during pre-training \citep{zhang2025mink}.
Consequently, LLMs exhibit limitations in addressing updated facts, specialized domain knowledge, or private information, making it difficult to generate accurate or user-intended responses in these areas \citep{mallen-etal-2023-trust, pmlr-v202-kandpal23a, pmlr-v162-liska22a}.

\begin{figure}[t]
\begin{center}
    \includegraphics[width=1\columnwidth]{Assets/Figures/main_figure_v4.pdf}
      \caption{ 
      The taxonomy of parametric and external knowledge is visualized through four quadrants, categorizing questions by the model's knowledge status (known or unknown) and the informativeness of external knowledge.
      Each quadrant is exemplified with the given external context and the expected response.
      }
      \label{figure:front_image}
\end{center}
\end{figure}


Providing \textit{informative} external knowledge can help mitigate these limitations by supplementing LLMs with key information necessary for deriving correct answers\footnote{This study assumes external knowledge is always factual to isolate its impact from the factors we consider.} \citep{chen-etal-2017-reading, asai-etal-2023-retrieval}.
However, conflicts between parametric and external knowledge can still lead to incorrect responses, despite the informativeness of external knowledge \citep{longpre-etal-2021-entity, xie2023adaptive}.
This issue arises because LLMs tend to over-rely on pre-trained knowledge, which hinders adaptation to new information \citep{shi-etal-2024-trusting, jin-etal-2024-cutting}.
While both sources can be informative, external knowledge can be more aligned with user intent, temporal recency, and dynamically evolving information, whereas parametric knowledge remains fixed after pre-training. 
Given these differences, prioritizing external knowledge is more likely to align with response accuracy \citep{li-etal-2023-large}.



However, in practice, there is no guarantee that the provided external knowledge will always be informative \citep{asai2024selfrag}.
This issue occurs more frequently in scenarios where informativeness depends on the performance of the retrieval mechanism and the quality of the database pool \citep{izacard2022unsupervised, pmlr-v119-guu20a}.
When external knowledge lacks informativeness, the model relies solely on its parametric knowledge, which can produce accurate responses if it contains the necessary information and remains unaffected by misleading cues \citep{yoran2023making, kim-etal-2024-adaptive, park2024enhancing}.
Otherwise, when no sufficient knowledge is available, abstaining from answering prevents incorrect responses and enhances user trust \citep{wen2024know, wen-etal-2024-characterizing, zhou-etal-2023-context, zhang-etal-2024-r}.


To handle these challenges, \textit{reliable} LLMs must dynamically assess whether to rely on parametric or external knowledge based on informativeness and abstain when neither is sufficient.
However, existing studies have primarily examined these factors partially or individually, resulting in a lack of a framework for analyzing their combined impact \citep{su2024textttconflictbank, li-etal-2023-large}.
This fragmented perspective makes it difficult to systematically evaluate how different aspects of knowledge handling interact and influence LLM performance.


To provide a more comprehensive perspective, we introduce a framework that systematically captures how these factors jointly influence LLMs' knowledge-handling behavior.
Our framework is built on two key dimensions: \textbf{presence of parametric knowledge}, which reflects the model's internal understanding of the question, and \textbf{informativeness of external knowledge}.
As illustrated in Figure \ref{figure:front_image}, this two-dimensional approach yields four knowledge-handling scenarios: \textbf{(a) Known-Informative} \textbf{(b) Unknown-Informative} \textbf{(c) Known-Uninformative} \textbf{(d) Unknown-Uninformative}.


To establish a well-structured knowledge handling framework, we precisely estimate parametric knowledge presence by considering model confidence \citep{kuhn2023semantic, 10820047} and response correctness. 
We also cover diverse types of informativeness, incorporating informative yet conflicting knowledge and varied forms of uninformative knowledge.


Within the knowledge-handling framework, we evaluate the reliability of open-source LLMs through both inference-time and training-based approaches.
Error analysis across knowledge-handling scenarios reveals that LLMs often exhibit biases toward either parametric or external knowledge, leading to incorrect responses.
Furthermore, abstention decisions are predominantly influenced by external knowledge, while parametric knowledge is largely overlooked.
To address these challenges, we show that training LLMs with data constructed based on parametric knowledge presence and diverse context types can enhance reliability in knowledge handling.

