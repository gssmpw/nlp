
\section{Experimental Setting}


\subsection{Datasets}
We use question-answering (QA) datasets from various domains, including NaturalQuestionsShort (NQ) \citep{kwiatkowski-etal-2019-natural}, TriviaQA-web (TriviaQA) \citep{joshi2017triviaqalargescaledistantly}, HotpotQA \citep{yang2018hotpotqadatasetdiverseexplainable}, SQuAD \citep{rajpurkar2016squad100000questionsmachine}, BioASQ \citep{tsatsaronis2015overview}, TextbookQA \citep{Kembhavi_2017_CVPR}, and RelationExtraction (RE) \citep{levy2017zeroshotrelationextractionreading}. 
These datasets are included in Machine Reading for Question Answering (MRQA) benchmark \cite{fisch2019mrqa2019sharedtask}, and we use each sample's question, context, and answer triplets.
Since the impact of context length is not within the scope of our study, we limit the context length to around 100 words to maintain experimental controllability.
More details on design choices are in Appendix \ref{appendix:datasetConstruction}.

\input{Assets/Figures/eval_v2_soft_em_closed-book-greedy-known_figure_avg_base}

\subsection{Context Construction}


\paragraph{Informative External Knowledge} 
\label{dataset:informative-external}

In addition to the original context from MRQA benchmark, we construct conflicting contexts that capture contextual-parametric conflicts.
To construct conflicting contexts, we instruct an LLM ({\scshape Llama 3 70B Instruct}, \citealp{grattafiori2024llama3herdmodels}) to generate an answer that is different from the original answer.
The original context containing the answer span is provided, and the model is instructed to consider the part of speech of the original answer residing in the context.
Then, we replace the original answer in the original context with the generated conflicting answer. 


\paragraph{Uninformative External Knowledge} 
\label{dataset:uninformative-external}

We construct uninformative contexts through random sampling and retrieval.
For retrieved-uninformative contexts, we first retrieve the top-100 contexts from the Wikipedia context pool\footnote{Wikipedia dump from Dec. 2018, with context chunked into 100 words.} using {\scshape Contriever-msmarco} \citep{izacard2022unsupervised}.
Among them, we choose the highest-scoring context that does not contain the answer span.



\input{Assets/Figures/analysis_v6_both}


\subsection{Training Details}

For efficient training, we employ QLoRA \citep{dettmers2023qlora}.
We use two open-domain QA datasets, NQ \citep{kwiatkowski-etal-2019-natural} and TriviaQA \citep{joshi2017triviaqalargescaledistantly}, both grounded in world knowledge.
We randomly sample 250 known and 250 unknown questions from each dataset.
Since each question is paired with four context types, we obtain 4,000 question-context pairs for training.



\subsection{Models}

We use open-source auto-regressive language models, including {\scshape Llama2} \citep{touvron2023llama}, {\scshape Llama3} \citep{grattafiori2024llama3herdmodels}, {\scshape Mistral 7B v0.3} \citep{jiang2023mistral}, and {\scshape Qwen 2.5} \citep{qwen2.5}, covering all available model sizes for each.



\subsection{Evaluation Metrics}

To evaluate whether the model prioritizes generating correct responses and chooses to abstain rather than provide incorrect answers, we use \textbf{accuracy (Acc)}, \textbf{truthfulness (Truth)}, and \textbf{reliability (Rely)} metrics proposed by \citet{xu2024rejection}.
These metrics are computed based on the number of correct ($N_c$), incorrect ($N_i$), and abstained ($N_a$) responses. 
Abstention is determined by checking whether the response contains predefined abstain words \citep{amayuelas-etal-2024-knowledge}.

\textbf{Acc} is assessed by checking whether the answer span is present in the response.
\textbf{Truth} measures the proportion of correct and rejected responses, ensuring that the model minimizes the generation of incorrect outputs.
It is defined as: $1 - \frac{N_i}{N} = \frac{N_c + N_a}{N}$.
A higher Truth score indicates that the model either answers correctly or abstains, thereby reducing misinformation.


\textbf{Rely} provides a more comprehensive assessment by balancing Acc and Truth.
It incorporates the answer rate ($Ans = \frac{N_c + N_i}{N}$) as a weighting factor to prevent excessive refusals.
Rely is computed as follows: $Ans \cdot Truth + (1 - Ans) \cdot Acc$.
This ensures the model balances answering correctly and abstaining appropriately, avoiding both excessive refusals and incorrect responses.


