\section{Knowledge-Handling Approaches}

To assess the reliability of existing open-source LLMs, we evaluate their ability to handle different knowledge-handling scenarios and investigate how training can improve their reliability. 
Specifically, we adopt two approaches: the \textbf{inference-time} approach to measure inherent knowledge-handling abilities and the \textbf{training-based} approach to examine the impact of incorporating knowledge-handling scenarios into training.


\subsection{Inference-Time Approach}
\input{Assets/Templates/abs}

To evaluate LLMs' inherent knowledge-handling ability, we instruct them to facilitate appropriate abstention (\textbf{\absinst}).
As specified in Template \ref{template:abs}, it directs LLMs to answer the question while assessing the informativeness of the provided context and to abstain if the context is uninformative and they lack the necessary knowledge.
It also includes two demonstration samples presented in random order: one demonstrating abstention in \UU\ and the other providing an answer.


Since the abstention instruction may introduce a bias on abstention that could affect the analysis of scenarios other than \UU, we also perform \textbf{\naive}\ generation without abstention instruction.
In \naive\ generation, two randomly sampled demonstrations are used without considering abstention in the \UU\ scenario.




\subsection{Training-Based Approach}

We aim to explore whether training can enhance model alignment in knowledge handling by incorporating the presence of parametric knowledge and diverse context types into the learning process.
To achieve this, we fine-tune the model to handle all knowledge-handling scenarios with the objective of reliability (\ours).


For training, we use open-domain question-answering datasets and sample a balanced set of known and unknown questions to help the model develop awareness of its parametric knowledge.
The known status of each question is determined using the criterion outlined in Section \ref{dataset:parametric-knowledge}.
Then, each question is paired with the four types of contexts discussed in Section \ref{section:externalKnowledge} to improve its ability to handle various types of external knowledge.


During training, the model is optimized to generate scenario-appropriate responses.
In the \UU\ scenario, since none of the available knowledge is informative for answering the given question, the expected response is abstention ("\texttt{unanswerable}").
For the other three scenarios, the model is trained to generate the correct answer corresponding to each context type.


To account for the potential tradeoff between accuracy and abstention ability introduced by incorporating abstention, we perform the task-specific fine-tuning for open-book question answering without modeling abstention (\baseline).
The main objective of \baseline\ is accuracy, as previous studies have primarily focused on \citep{li-etal-2023-large, yoran2023making}.
As a result, \baseline\ always generates an answer, making the expected answer in \UU\ the ground-truth answer for each context type.




