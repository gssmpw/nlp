\section{Related Works}

\subsection{Knowledge-Handling in LLMs}

Prior work has explored different strategies to balance parametric and external knowledge when generating responses. 
For instance, \citet{li-etal-2023-large} trains LLMs to generate context-based responses when external knowledge is informative and parametric-based responses when it is uninformative.
Building on this approach, our framework incorporates parametric knowledge and integrates abstention as a key behavior to enhance reliability.


Similarly, \citet{neeman-etal-2023-disentqa} introduces an approach that allows LLMs to provide both context-based and parametric-based responses simultaneously. 
While their approach predicts two separate answers, we expect LLMs to internally select a single response based on the presence of parametric knowledge and the informativeness of external knowledge.


\subsection{Knowledge Conflict}

There have been efforts to resolve knowledge conflicts to improve the model's ability to incorporate external information \citep{zhou-etal-2023-context, park2024toward, shi-etal-2024-trusting}.
Some studies indicate that LLMs tend to adhere to their parametric knowledge \citep{longpre-etal-2021-entity, jin-etal-2024-tug}. 


They have primarily viewed knowledge conflict through the lens of external knowledge, focusing on factors such as temporal shifts \citep{NEURIPS2023_9941624e, dhingra-etal-2022-time}, synthetically updated facts \citep{longpre-etal-2021-entity}, and contextual plausibility \citep{xie2023adaptive, tan-etal-2024-blinded}.
We extend perspective to the model's parametric knowledge by considering whether the model possesses the knowledge to answer a question, enabling a broader range of analyses beyond external knowledge alone. 



\subsection{Uninformative Context}

Retrieval-augmented language models (RALMs) are prone to reduced performance due to distraction from uninformative external information \citep{yoran2023making, shen-etal-2024-assessing}. 
To mitigate the performance drop, researchers have explored methods to encourage LLMs to rely on parametric knowledge when external information is uninformative at the inference-time \citep{yu-etal-2024-chain, park2024enhancing, baek-etal-2023-knowledge-augmented-language} or through training \citep{yoran2023making, asai2024selfrag, xia2024improving, luo-etal-2023-search}.


Another line of research handles abstention behavior when presented with uninformative contexts \citep{wen-etal-2024-characterizing}.
We extend this perspective to cases where LLMs lack correct or complete internal knowledge to address a query \citep{feng-etal-2024-dont, zhang-etal-2024-r, wen2024know}. 
Our work bridges these two abstention criteria by jointly considering both parametric and external knowledge in refusal decisions.


