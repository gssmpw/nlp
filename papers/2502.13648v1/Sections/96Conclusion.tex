
\section{Conclusion}

In this paper, we introduce the knowledge handling framework with four scenarios, \KI, \UI, \KU, and \UU, based on the presence of parametric knowledge and the informativeness of external knowledge.
The experimental findings within this framework reveal that LLMs face challenges in resolving knowledge conflicts, particularly when they possess question-relevant parametric knowledge, and exhibit false abstention that degrades accuracy.
Training on a dataset designed for these scenarios improves reliability and reduces biases in knowledge utilization. 
Still, domain generalization remains a challenge in terms of accuracy, as models struggle to assess the presence of their knowledge in out-of-domain data.
Our findings provide a foundation for comprehensive knowledge handling, offering insights into building more reliable and knowledge-aware LLMs.
