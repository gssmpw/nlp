\textbf{Personalization} in machine learning refers to tailoring systems to generate predictions that align with each individual's preferences and needs. This concept has been extensively studied in Recommendation Systems \cite{sarwar2001item, he2017neural} and Dialogue Systems \cite{zhang1801personalizing, li_persona-based_2016}. With the widespread adoption of LLMs, personalization has become even more critical to ensure these models effectively serve diverse global users with varying preferencesâ€”a challenge that remains underexplored in current alignment pipelines \cite{sorensen_roadmap_2024}.


Unlike traditional task-specific ML systems, LLMs are general-purpose models designed to handle a wide range of tasks and domains. This versatility makes personalization both more important and more challenging, as the model must adapt its broad capabilities to each user's specific needs and preferences. Several approaches have been proposed, including prompting \cite{hwang_aligning_2023}, user embedding learning \cite{li_personalized_2024, feng_modular_2024}, latent variable modeling \cite{poddar_personalizing_2024, siththaranjan_distributional_2024}, meta-learning \cite{zhao_group_2023}, multi-objective reinforcement learning \cite{jang_personalized_2023}, preference elicitation \cite{li_eliciting_2023}, prompt optimization \cite{kim_few-shot_2024}, and context compression \cite{kim_compressed_2024}. However, these methods have typically been evaluated on different datasets which prohibits a fair comparison between them.\\
\textbf{Evaluation of Personalization} presents unique challenges beyond traditional preference learning. While domains like recommender systems have established evaluation frameworks using per-user interaction histories \cite{harper2015movielens}, evaluating natural language outputs and collecting general-domain preference data at scale remains challenging \cite{zhou-etal-2022-deconstructing,clark2021all, dong2024can}. Existing survey-based datasets, such as OpinionQA \cite{santurkar_whose_2023} and GlobalOpinionQA \cite{durmus_towards_2024}, provide large-scale, real-world general-domain data but are limited to multiple-choice formats, which fail to capture realistic LLM usage scenarios. In contrast, generation-based datasets such as \citet{salemi_lamp_2023, wang_learning_2023, stiennon_learning_2022} contain preferences for open-ended generations but remain restricted to narrow domains. Other sources, like Personal Reddit \cite{staab_beyond_2023} and Persona-DB \cite{sun_persona-db_2024}, scrape Reddit and Twitter data but cannot be publicly released due to privacy concerns. PRISM \cite{kirk_prism_2024} offers diverse preference data for LLM generations but remains limited in size to effectively model individual annotators.

In the absence of large-scale, general-domain preference datasets, recent research has explored synthetic data generation via role-playing agents and LLM-as-a-Judge evaluations \cite{zheng_judging_2023, jang_personalized_2023, zollo_personalllm_2024, castricato_persona_2024, shao_character-llm_2023, liu2024aligningb}. While these methods may not fully capture real user preferences \cite{hu_quantifying_2024}, recent works suggest that synthetic benchmarks can serve as viable testbeds for evaluating personalization, even if they don't comprehensively represent all human preference variations \cite{castricato_persona_2024, zollo_personalllm_2024}. As noted in~\citet{balog2025user}, perfect simulations of human preferences may not be necessary for these simulation to provide valuable insights and help develop better algorithms.


