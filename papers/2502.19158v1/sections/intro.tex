\begin{figure*}[htbp!]
\centering
\vspace{-6mm}
\includegraphics[width=0.9\textwidth]{figures/Personalization_overview.pdf}
\caption{Each user has a unique preference distribution in the response space. Traditional preference learning systems treat preference data as homogeneous, but the inherent self-conflicting nature of preferences makes them difficult and unstable to learn. A personalized preference learning system, however, can effectively capture and model the individual preference distribution for each user. The scatter plot visualizes the preferred response embeddings from Personal LLM \citep{zollo_personalllm_2024} for three selected users using PCA.}
\label{illustration}
\vspace{-0.4cm}
\end{figure*}




Reinforcement learning from human feedback (RLHF) has been effective in aligning pre-trained Large Language Models (LLMs) with human preferences, improving their helpfulness, harmlessness, and instruction-following abilities~\cite{NEURIPS2022_b1efde53}. However, standard RLHF assumes a homogeneous set of preferences, failing to account for the diverse and sometimes conflicting nature of human values~\cite{casper2023open}. This leads to biases toward the perspectives of a western, democratic, postgraduate-educated demographic \citep{santurkar_whose_2023}, even though LLM users represent a wide range of cultural and ideological backgrounds, with a majority being non-U.S. users across the world~\cite{liu2023who}.

\begin{table}[!h]
    \centering
    \small
    \resizebox{\linewidth}{!}{
    \renewcommand{\arraystretch}{1.2}
    \begin{tabular}{lcccc}
        \toprule
        & \multicolumn{4}{c}{\textbf{Personalization}} \\
        & \textbf{\shortstack{Perform.}} & \textbf{\shortstack{Adaptability}} & \textbf{\shortstack{Fairness}} & \textbf{\shortstack{Tax}} \\
        \midrule
        \textsc{Vanilla RM} & \bad & \xmark & \xmark & \cmark \\
        \textsc{Individual RM} & \good & \xmark & \cmark & \xmark \\
        \noalign{\smallskip} 
\hdashline 
\noalign{\smallskip} 
        \textsc{Group PO} & \medium & \cmark & \xmark & \xmark \\
        \textsc{Variational PL} & \medium & \cmark & \xmark & \xmark \\
        \textsc{Personalized RM} & \good & \xmark & \cmark & \xmark \\
        \bottomrule
    \end{tabular}
    }
    \caption{The comparison between different methods across four properties of personalization. Our framework evaluates personalization performance, adaptation capability to new users, fairness for minority users, and personalization tax on general-purpose preferences. For the performance, we use (\good, \medium, \bad) for good, medium, and low average scores. For the other properties, we report whether a method enables (\cmark) the corresponding property or not (\xmark).}
    \label{tab:benchmark_comparison}
    \vspace{-0.1cm}
\vspace{-0.5cm}
\end{table}


Personalized preference learning aims to bridge this gap by adapting LLMs to the specific preferences of individual users. With the increasing adoption of general-purpose LLMs, researchers have begun exploring personalization in open-domain contexts \citep{hwang_aligning_2023, jang_personalized_2023, li_personalized_2024}. However, significant challenges remain, particularly concerning the evaluation of these personalized models. %


Firstly, \textbf{the evaluation benchmarks are inadequate and incomparable across different studies}. Existing studies rely either on narrow-domain real-world data \citep{stiennon_learning_2022} or entirely synthetic general-domain data \citep{zollo_personalllm_2024,castricato_persona_2024}, limiting the robustness of evaluation. Furthermore, the use of disparate datasets across studies impedes fair and direct comparisons between personalization methods. 

Secondly, \textbf{the evaluation frameworks fail to address the practical constraints and unintended consequences}. Existing research often assumes a fixed number of data points per user, neglecting the practical constraints of real-world data availability. How do different personalization algorithms perform under varying levels of data availability? 
Moreover, the potential side effects of personalization, beyond the scope of~\cite{kirk_benefits_2024}, remain largely unexplored. Does personalization degrade general LLM capabilities or introduce safety vulnerabilities?









To address these gaps, we introduce a novel, multi-faceted framework for benchmarking open-domain personalized preference learning techniques. Our contributions are as follows:

\begin{itemize}[leftmargin=*, noitemsep,topsep=1pt]
    \itemsep 0em
    \item We introduce a principled way to characterize diverse preference datasets, revealing differences in \textbf{inter-user disagreement}, \textbf{intra-user consistency}, and the \textbf{prevalence of minority views}, each posing unique challenges for personalization.

    \item Our multi-faceted evaluation framework goes beyond standard accuracy and includes real-world constraints. We measure these aspects through \textbf{sample efficiency}, \textbf{adaptating to a new user} with limited data, \textbf{personalization tax} on reward modeling and \textbf{per-user analysis}. 

    \item We conduct an empirical study of eight representative personalization algorithms across three datasets with distinct characteristics. Our evaluation show that fine-tuning individual reward models (i.e. a reward model per person) is a strong baseline. The methods that leverage collaborative learning such as Personalized RM achieve up to 6\% improvement over this baseline. Meta-learning approaches demonstrate better adaptability to new users. Crucially, we find that personalization can lead to safety misalignment and up to a %
    20\% decline on safety and reasoning benchmarks.
\end{itemize}






















