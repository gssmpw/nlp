
\begin{figure*}[htb]
\vspace{-0.5cm}
\centering
\subfloat[]{
    \includegraphics[width=0.4\textwidth]{figures/Personal_LLM.pdf}
    \vspace{-0.3cm}
}
\subfloat[]{
    \includegraphics[width=0.4\textwidth]{figures/TL_DR.pdf}
    \vspace{-0.3cm}
} \\
\vspace{-0.1cm}
\subfloat[]{
    \includegraphics[width=0.4\textwidth]{figures/P-SOUPS.pdf}
    \vspace{-0.4cm}
} 
\subfloat[\label{fig:accuracy_across_dataset}]{
    \includegraphics[width=0.4\textwidth]{figures/accuracy_across_datasets.pdf}
    \vspace{-0.4cm}
}
\vspace{-0.2cm}
\caption{\textbf{Averaged Reward Model Accuracy Comparison Across Three Personalization Datasets.} Figures (a), (b), and (c) show averaged accuracy results across three datasets with varying number of training samples. Figure (d) compares the accuracy of personalized algorithms across three datasets.}
\label{fig:main_results}
\vspace{-0.3cm}
\end{figure*}

\paragraph{Personalized RM Achieves the Best Performance across All Datasets.}
As shown in Figure~\ref{fig:main_results}, , in terms of reward modeling accuracy, personalized RM consistently outperforms all methods across all datasets. Its success over individual reward modeling can be attributed to the its \textit{collaborative learning} - leveraging signals for all users. 
Individual reward models, while serving as simple yet effective baselines, achieve the second-best performance. 
Both of them surpass other baselines by a significant margin on Personal LLM and performs even better on P-SOUPS. We attribute this to its superior ability to handle the high inter-personal disagreement nature of P-SOUPS.
On TL;DR, all methods—except RAG—perform comparably. RAG, in contrast, exhibits the weakest performance among all personalization methods across all datasets, with accuracy approaching that of random guessing. This is likely due to the limitations of the 7B model in capturing nuanced user preferences through in-context learning.






\paragraph{Dataset Properties Predict Personalization Gains.} Figure \ref{fig:accuracy_across_dataset} compares three representative preference learning approaches across all evaluation datasets, ranging from no personalization (Vanilla RM) to simple personalization (Individual RM) to complex personalization (PRM). The results demonstrate that personalization gains strongly correlate with our proposed \textit{room for personalization} metric. P-SOUPS, with the highest room for personalization (Table \ref{dataset}), shows the greatest improvement from personalization methods.
In contrast, TL;DR's low inter-personal disagreement limits the gains from personalization appraoches.
These empirical results validate our analytical framework for characterizing personalization datasets.


\begin{figure}[!h]
\centering
\includegraphics[width=0.35\textwidth]{figures/adaptation.pdf}
\vspace{-0.2cm}
\caption{\textbf{Adaptation to New Users on Personal-LLM}: Figure (d) presents the performance of different baselines in adapting to new users with varying amounts of training data. The dashed black line represents the accuracy of the Individual RM trained on the full dataset, serving as the theoretical upper bound.}
\label{adaptation}
\vspace{-0.5cm}
\end{figure}


\begin{figure*}[]
\centering
\vspace{-0.3cm}
\includegraphics[width=0.9\textwidth]{figures/reward_bench.pdf}
\vspace{-1cm}
\caption{\textbf{Testing Personalization Tax on Reward Bench}. We measure the accuracy and reward bench performance for the personalization methods and show its deviation from the pre-trained RM. We report the change in accuracy relative to pre-trained RM \cite{dong2023raft}. %
}
\label{personalization_tax}
\vspace{-0.3cm}
\end{figure*}

\paragraph{Personalization Methods can Scale with More Training Samples.} 
As expected, increasing the number of training samples can generally improves RM accuracy for all methods when they are capable of learning personalized RMs. 
However, since Conditional RM and GPO are not effective at learning personalized preferences from P-SOUPS, their performance does not improve with the addition of more training data.
We attribute this to these methods' limitations in modeling high inter-personal disagreement, a defining characteristic of the P-SOUPS dataset. 
These findings highlight that different personalization methods exhibit varying levels of robustness when faced with increasingly divergent preference data.



\paragraph{Personalization Protects Minority Viewpoints.} 
While prior work has primarily focused on average performance metrics, we argue that a crucial function of personalization is protecting minority viewpoints that diverge from majority preferences. 
Figure~\ref{radar} reveals that Vanilla RM fails to capture preference for such minority users.
While Individual RM successfully preserves these minority preferences through dedicated per-user models, Personalized RM achieves only partial success.
Through this analysis, we would like to point out a critical limitation in current personalization research: existing evaluation frameworks often treat all preference groups as equal, which can overlook the significance of minority groups due to their smaller sizes. This undermines the core objective of personalization, which is to preserve preference diversity.
We argue that a personalization method's ability to preserve minority viewpoints should also be considered a critical evaluation metric for assessing personalization approaches.


\begin{figure}[]
\centering
\vspace{-0.2cm}
\includegraphics[width=0.7\linewidth]{figures/radar.pdf}
\vspace{-0.2cm}
\caption{\textbf{Per-user Accuracy on Personal-LLM.} User 8 is considered the minority since as we calculated it has 0.33 accuracy after majority voting in Table \ref{dataset}.}
\label{radar}
\vspace{-0.5cm}
\end{figure}


\paragraph{Adaptation to New Users.} 
As discussed in Section \ref{subsection: metrics}, a critical challenge in real-world deployment is adapting personalization methods to new users with limited preference data. We evaluate this capability in scenarios where only 30-100-300 preference pairs are available per new user. Since RM fine-tuning approaches, including Personalized RM, do not inherently support this cold-start setup, we implement two additional baselines for comparison:
(1) \textbf{Retrieve Similar User RM:} we identify the existing user whose preferences most similar to the new user and directly apply the reward model of that user. (2) \textbf{Further Fine-Tune Trained RM:} We take the Vanilla RM trained on aggregated existing users preference data and fine-tune it for one epoch using the new user’s limited data.

The results shown in Figure~\ref{adaptation} demonstrate that GPO significantly outperforms these baselines, approaching the upper bound (individual RMs trained on complete 100K user data) with just 30-300 samples. The Similar-User RM performs only marginally better than Vanilla RM, indicating that simple user-matching strategies are insufficient for effective personalization. These findings reveal the power of meta-learning-based approaches and urge further exploration of making reward modeling more effective in limited data settings.

\paragraph{Personalization Can Hurt Model Safety and Reasoning} 
\label{sec: safety}
To investigate potential negative impacts of personalization on core LLM capabilities, we evaluate models before and after personalization across the three dimensions of RewardBench \citep{lambert2024rewardbench}. Specifically, we fine-tune a pre-trained model (initially optimized for safety and reasoning) using individual reward modeling, with results shown in Figure~\ref{personalization_tax}.

The effects of personalization vary substantially across datasets, aligning with our theoretical framework. For TL;DR, both preference prediction accuracy and safety/reasoning performance remain largely stable, consistent with our finding of limited room for personalization in Section~\ref{subsection: property}. In contrast, Personal-LLM and P-SOUPS exhibit a concerning trade-off: while preference prediction accuracy improves significantly, we observe substantial degradation in both reasoning ability and safety performance. This degradation suggests that optimizing for individual preferences can compromise fundamental model capabilities, a phenomenon we term the ``personalization tax.'' These findings raise important concerns about the deployment of personalized LLM systems and underscore the need for careful balancing of personalization benefits against potential risks \citep{kirk2024benefits, hui2024toxi, ai2024de}.



