Preference learning systems can take various forms, including reward models (RMs), where a model assigns a numerical preference score; preference ranking models, which make comparative judgments between multiple candidates; and generation-based policy models, where the model explicitly generates preference judgments, sometimes accompanied by explanations or feedback.
In this section, we review previous approaches to learning personalized preferences, with a particular focus on reward models, which constitute the majority of existing methods.

\subsection{Vanilla Reward Modeling}


Consider $n$ annotators $u_1, u_2, ..., u_n$ who provide preference feedback on outputs \(y_1, y_2\) for a given prompt $x$. The preferred and dispreferred response is denoted as \(y_+\) and \(y_-\), respectively. This yields a personalized preference dataset $\mathcal{D}_p$:
\vspace{-0.2cm}
\begin{equation*}
\mathcal{D}_p = \bigcup_{u=1}^n \Big\{ (x_j^{(u)}, y_{j,+}^{(u)}, y_{j,-}^{(u)}, u) \Big\}_{j=1}^{m},
\vspace{-0.2cm}
\end{equation*}
where \( m \) is the number of samples. Current preference tuning literature assumes homogeneous human preference \cite{NEURIPS2022_b1efde53,stiennon_learning_2022, liu2024aligning}, and thus aggregate $D_p$ via majority voting or rank aggregation, yielding:
\vspace{-0.1cm}
\begin{equation*}
\mathcal{D} = \{(x_i, y_i^+, y_i^-)\}_{i=1}^m.
\end{equation*}

\vspace{-0.1cm}
Next, a reward model $r(x, y) \to \mathbb{R}
$ is trained to approximate human's satisfaction level of response $y$ given prompt $x$. Following the Bradley-Terry (BT) model \cite{bradley1952rank}, the probability of preferring \(y^+\) over \(y^-\) is given by:
\vspace{-0.1cm}
\begin{equation*}
\mathbb{P}(y^+ \succ y^- \mid x) = \sigma(r(x, y^+) - r(x, y^-)),
\end{equation*}
where \(\sigma\) is the logistic function. The reward model \(r(x, y)\) is then optimized via maximum likelihood estimation by as a binary classification problem:
\vspace{-0.2cm}
\begin{equation}
r = \arg\min_{r} 
\mathbb{E}_{(x, y^+, y^-) \sim \mathcal{D}} 
\Big[-\log \mathbb{P}(y^+ \succ y^- \mid x) \Big].
\label{equation: rm} \nonumber
\end{equation}
\subsection{Personalized Reward Modeling}
To capture individual preferences, the reward model must adapt its predictions based on user identity. Formally, this means extending the vanilla reward model $r(x,y)$ to incorporate user information, yielding $r(x,y,u)$. Below we summarize baseline approaches and recent methods from the literature that we consider in our evaluation.

\paragraph{Individual Reward Modeling} trains a dedicated reward model $r^u$ for each user $u$ using only their personal preference data $D^u$. As shown in Equation \ref{equation: rm}, each model maximizes the likelihood of its user's observed preferences and thus would in theory obtain optimal personalization provided there are sufficient preference data for each user.

\paragraph{Conditional Reward Modeling} trains a unified reward model $r(x,y,u)$ that explicitly conditions on user id. Specifically, we prepend the corresponding user id to the prompt input $x$. The reward model then processes this augmented input along with the response $y$ to compute user-specific rewards.

\paragraph{Personalized Reward Modeling (PRM)} ~\cite{li_personalized_2024} jointly learns user-specific preferences and shared preference patterns through a dual-objective approach. Specifically, given a learnable user encoder model $f_p(u)=e_u$ that takes in user id $u$ and output user embedding $e_u$, PRM concatenate it with the input and jointly optimize $f_p$ and RM using the following objective:
\vspace{-0.4cm}

\begin{equation*}
\begin{aligned}
\min_{r} -\mathbb{E}_{(x, y_+, y_-, u) \sim \mathcal{D}_p} \Big[
 \alpha & \log \mathbb{P}(y^+ \succ y^- \mid x, u) \\
+ (1- \alpha) & \log \mathbb{P}(y^+ \succ y^- \mid x, u_0)
\Big]
\end{aligned}
\end{equation*}


\vspace{-0.2cm}
This loss can be viewed as a linear combination of a user-specific ($u$) and a user-agnostic ($u_0$) term.



\paragraph{Variational Preference Learning (VPL)}~\cite{poddar_personalizing_2024} is a reward model built upon variational autoencoders (VAE)~\citep{Kingma2014}. In this framework, the encoder learns to map the input user-specific preference data to a latent variable $z$, which captures the underlying structure of user preferences. The decoder then utilizes this latent representation $z$ to generate predicted rewards for new response candidates, functioning as the reward model. This allows VPL to effectively capture individual differences while leveraging commonalities across users.



\paragraph {Retrieval-Augmented Generation (RAG)} can also be employed to model personalized preferences by leveraging LLMs as the preference ranking model. 
Given a user query $x$, RAG first retrieves the top three most relevant examples from the user-specific preference training data, using cosine similarity to measure the similarity between queries.
The retrieved triplets $\{(x, y_+, y_-)\}_{1:3}$ are then incorporated into the original query as additional context. This augmented input is fed to the LLM, prompting it to predict the user's preference based on the provided context.



\paragraph{Group Preference Optimization (GPO)} \citep{zhao_group_2023} extends an LLM with a specialized transformer module for learning personalized preferences. This module is trained through meta-learning, specifically using in-context supervised learning to predict preference distributions. The module operates on embeddings of few-shot examples rather than raw text, allowing it to efficiently process lengthy examples while learning to generalize preference patterns across different contexts.
