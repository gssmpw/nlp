\onecolumn
\section{Appendix}
\subsection{Hyperparameter Selection}
\label{hyperparam}
For Vanilla RM, Individual RM, and Conditional RM, we fine-tune the model with learning rate of 3e-4 with LoRA rank of 16 and LoRA alpha of 32. Following the optimization literature \cite{mccandlish2018empirical}, the total number of optimization steps for training with different sample size should be kept the same. Thus we do hyperparameter search of the training eposes, we train 1 epoch on 100,000 samples. We search over 1,3,10 epoch on 10,000 samples and 1, 10, 100 epoch on 1,000 samples. For VPL, GPO, PRM, we use the same hyper-parameter setup as their paper except we search over the number of training epochs as above.




\section{Results}
\begin{table*}[!h]
\setlength{\tabcolsep}{4pt}
\centering
\label{tab:sample_efficiency_ablation}
\tiny
\begin{tabular}{l|ccccc|ccccc|ccccc}
\toprule
\textbf{Method}                     & \multicolumn{5}{c|}{\textbf{Personal LLM}} & \multicolumn{5}{c|}{\textbf{TL;DR}} & \multicolumn{5}{c}{\textbf{P-SOUPS}} \\
                                    & \textbf{ACC} & \textbf{Safety} & \textbf{Reason.} & \textbf{Chat} & \textbf{Chat-H} & \textbf{ACC} & \textbf{Safety} & \textbf{Reason.} & \textbf{Chat} & \textbf{Chat-H} & \textbf{ACC} & \textbf{Safety} & \textbf{Reason.} & \textbf{Chat} & \textbf{Chat-H} \\
\midrule
Pre-trained RM                     &      0.62        &       0.92        &        0.84         &        0.96          &     0.60           &       0.65       &       0.92        &        0.84         &        0.96          &     0.60         &     0.51          &       0.92        &        0.84         &        0.96          &     0.60                     \\ \midrule
Vanilla RM                       &    0.73           &     0.83          &      0.75           &       0.91           &         0.47         &       0.63       &     0.87         &       0.83         &        0.95        &      0.58          &    0.49          &    0.70          &      0.58          &      0.65          &        0.49        \\
Individual RM                      &      0.77        &      0.88       &      0.83         &      0.94          &     0.55           &      0.65        &         0.93     &      0.84          &     0.97           &     0.62           &      0.66        &     0.82        &       0.77        &       0.76       &     0.58         \\
Conditional RM                      &       0.72       &     0.83         &      0.75           &       0.91           &        0.47          &      0.66        &     0.93           &      0.83          &       0.97         &       0.61       &        0.50      &     0.70           &   0.54             &       0.74      &      0.39    \\
\bottomrule
\end{tabular}
\caption{Reward Bench Accuracy for Personalization Algorithms. }
\label{reward_bench}
\end{table*}



\begin{table*}[ht!]
\centering
\label{tab:sample_efficiency_ablation_single}
\small
\begin{tabular}{l|ccc}
\toprule
\textbf{\# New User data}                     & \textbf{30} & \textbf{100} & \textbf{300}\\ 
\midrule
Individual RM (with full dataset)   &   0.85     &    0.85   &  0.85 \\  
\midrule
Vanilla RM                          &    0.74    &     0.74   &  0.74 \\ 
Retrieve Similar User RM            &    0.73         & 0.74     &  0.75   \\ 
Further Fine-tune Trained RM             &      0.71           &   0.73        &     0.72      \\
GPO                                 &      0.83       &    0.85 &  0.85       \\ 
\bottomrule
\end{tabular}
\caption{Adaptation to new users with vary number of new user preference data (Personal-LLM)}
\label{table: adaptation_results}
\end{table*}

\begin{table*}[h]
\centering
\small
\begin{tabular}{l|ccc|ccc|ccc}
\toprule
\textbf{Method}                     & \multicolumn{3}{c|}{\textbf{Personal LLM}} & \multicolumn{3}{c|}{\textbf{TL;DR}} & \multicolumn{3}{c}{\textbf{P-SOUPS}} \\ 
            \textbf{\#Samples}                        & \textbf{1,000} & \textbf{10,000} & \textbf{100,000} & \textbf{1,000} & \textbf{10,000} & \textbf{35,000} & \textbf{1,000} & \textbf{10,000} & \textbf{50,000} \\ 
\midrule
Pre-trained RM                   & 0.62          & 0.62            & 0.62             & 0.65         & 0.65           & 0.65           & 0.51         & 0.51           & 0.51           \\ 
RAG                                 &      0.50        &       0.49        &      0.51         &        0.48     &       0.49        &     0.49          &   0.50           &        0.50       &        0.48       \\ 
Vanilla RM                          &     0.68          &   0.73            &            0.74    &       0.59       &        0.60       &          0.64     &       0.50       &       0.50        &         0.49      \\ 
Conditional RM                      &        0.71      &     0.72           &         0.72      &      0.59        &    0.59         &    0.61         &    0.51         &          0.50     &    0.50           \\  
Individual RM        &     0.74          &       0.81       &       0.86                      &     0.56          &      0.61         &      0.62         &  0.74         & 0.79           &  0.80      \\ 
VPL                                 &         0.68     &        0.72      &      0.74         &                0.60            &       0.63      &        0.64       &       0.57       &   0.54            &     0.62          \\ 
GPO                                 &      0.72        &         0.75      &       0.81        &    0.50          &      0.59         &      0.59         &     0.49         &     0.50           &     0.51          \\ 
Personalized RM                     &     0.74          &    0.82             &   0.88            &     0.57         &        0.66       &        0.68       &       0.55       &     0.83          &    0.86           \\ 
\bottomrule
\end{tabular}
\caption{RM Accuracy with Varying Number of Training Samples}
\label{table: main_results}
\end{table*}

\begin{table*}
\centering
\begin{tabular}{lcccccccc}
\toprule
User ID & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 \\
\midrule
 Pre-trained RM & 0.65 & 0.62 & 0.70 & 0.72 & 0.60 & 0.63 & 0.61 & \underline{0.40} \\
 RAG & \underline{0.43} & \underline{0.36} & 0.50 & 0.62 & \underline{0.48} & \underline{0.33} & 0.59 & 0.60 \\
 Vanilla RM & 0.83 & 0.82 & 0.82 & 0.78 & 0.86 & 0.73 & 0.58 & \underline{0.35} \\
 Conditional RM & 0.84 & 0.77 & 0.75 & 0.76 & 0.85 & 0.84 & 0.69 & \underline{0.36} \\
 Individual RM & 0.87 & 0.80 & 0.77 & 0.80 & 0.89 & 0.89 & 0.73 & 0.71 \\
 VPL & 0.83 & 0.82 & 0.82 & 0.78 & 0.86 & 0.73 & 0.58 & \underline{0.35} \\
 GPO & 0.83 & \underline{0.46} & 0.76 & 0.79 & 0.80 & 0.84 & \underline{0.49} & 0.81 \\
 Personalized RM & 0.90 & 0.83 & 0.85 & 0.88 & 0.86 & 0.95 & 0.88 & 0.57 \\
\bottomrule
\end{tabular}
\caption{Accuracy Across 8 Users on Personal LLM. Accuracy below 0.5 is underlined, indicating the performance drop below random chance. Results show that only Individual RM and PRM achieve improvement across all 8 users.}
\label{table: accuracy across users}
\end{table*}
