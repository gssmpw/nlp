\subsection{Evaluation Dataset} 
\begin{table*}[h!]
\small
\centering
\begin{tabular}{l c c c c c c}

\toprule
          & \#Samples & \#Users & \%Cont.  & \%Highly Cont.  &  MV-ACC Range  & Consistency \\ \midrule \midrule
P-SOUPS     &    53k       &      6    &   100\% &   98\%    &   [0.51--0.59]   &    1 \\ \midrule
TL;DR     &    179k        &     5    &   49\%  &   27\%   &   [0.81--0.87]  &   ? \\    \midrule
Personal-LLM &     333k       &     8    &   87\%  &   16\%   &   [0.33--0.93]  &   1 \\ \bottomrule
\end{tabular}
\caption{\textbf{Dataset Statistics.} For each triple \((x, y_1, y_2)\), we calculate the ratio of \textit{controversial preferences}, defined as cases where \textbf{any} user has a preference differing from others. Additionally, we compute the ratio of \textit{highly controversial preferences}, where at least 30\% of users express preferences that differ from the majority. We also report the range of each user's accuracy if the preference dataset is aggregated using majority voting (MV-ACC).} 
\label{dataset}
\vspace{-0.5cm}
\end{table*}



Given the challenges and costs of collecting large-scale, open-domain personalized preference datasets, researchers have explored both carefully curated narrow-domain human annotated and general-domain synthetic data generation approaches ~\cite{stiennon_learning_2022,jang_personalized_2023,zollo_personalllm_2024,castricato_persona_2024}. We focus on three datasets that provide pairwise preference annotations - a format particularly suited for preference learning:

\begin{itemize}[leftmargin=*, noitemsep,topsep=1pt]
    \itemsep 0em
    \item \textbf{P-SOUPS} \cite{jang_personalized_2023} creates a synthetic dataset designed to personalize LLMs along three predefined dimensions: expertise, informativeness, and style. Each dimension has two opposing preferences, resulting in eight unique combinations of preferences (or user personas). Paired responses are then generated by prompting with different user preference combinations.

    \item \textbf{Reddit TL;DR} \cite{stiennon_learning_2022} consists of Reddit posts, each paired with two human-annotated summaries. Preference labels for these summaries are provided by multiple annotators and unaggregated data are available, allowing us to make use of the annotator ID. Following \citet{park_principled_2024}, we select the five annotators (worker IDs) who contributed the highest number of annotations.

    \item \textbf{Personal-LLM} \cite{zollo_personalllm_2024} offers a scalable approach to simulate open-domain user preferences through reward model interpolation. Specifically, they use 8 different pre-trained reward model and use these as archetypal users for collecting synthetic preference data. Additionally, they show that interpolating between these reward models enables generating new users with coherent but distinct preference patterns. 
    \end{itemize}

\subsection{Dataset Characteristics and Impact} 
\label{subsection: property}

We introduce an analytical framework that characterizes personalized preference datasets along four dimensions: inter-personal disagreement, intra-personal consistency, presence of minority users, and overall room for personalization. While personalization might seem universally beneficial in theory, our framework reveals that its practical utility heavily depends on dataset properties—in some cases, personalized algorithms may offer negligible advantages over non-personalized approaches. This framework not only helps evaluate existing datasets but also provides design principles for future preference collections.

\paragraph{Inter-Personal Disagreement} 
Inter-personal disagreement refers to variations in preferences across different users. Personalization is only necessary for tasks with high inter-user disagreement; When users unanimously prefer input A over input B, such preferences can be captured through standard alignment processes without requiring personalization. This is analogous to the distinction between objective and subjective tasks in NLP \cite{ovesdotter-alm-2011-subjective,plank-2022-problem}. %
We operationalize inter-personal disagreement through two metrics: preference divergence rate, which measures the percentage of inputs that elicit any disagreement among users, and high-divergence preferences, where at least 30\% of users deviate from the majority. See Table~\ref{dataset} for results.



P-SOUPS exhibits a preference divergence rate approaching 100\%, reflecting near-universal disagreement among users - an artifact of the dataset's deliberate construction incorporating opposing preferences across all dimensions. While this makes P-SOUPS valuable for benchmarking, it may limit generalizability to real-world applications. In contrast, TL;DR and Personal-LLM show lower preference divergence rates that better reflect natural distributions of user preferences in real-world scenarios.

\paragraph{Intra-Personal Consistency} 
Intra-personal consistency reflects how stable an individual's preferences remain across time and similar situations. This parallels test-retest reliability in behavioral sciences, where a Cronbach's alpha of 0.7-0.9 is considered desirable for survey responses~\cite{nunnally1994psychometric}. While direct measurement of such reliability is difficult in preference datasets without repeated annotations, human consistency likely does not exceed 0.9. Synthetic datasets, however, provide perfect consistency by construction—an idealized scenario that may not generalize well to real applications.

Intra-personal consistency in preferences is influenced by several factors. Research shows that individuals display lower response stability when lacking strong attitudes or investment in the subject~\citep{Converse01012006, Achen_1975}. Consistency may also decrease when comparing outputs with minimal differences~\cite{padmakumar2024beyond}. Modern psychometric theory acknowledges that some inconsistency is inherent in human behavior — a consideration often overlooked in preference learning literature.






\paragraph{Minority Users} 
In personalized preference learning, identifying and appropriately handling minority viewpoints is crucial. Prior work shows that standard RLHF can marginalize minority perspectives~\citep{chakrabortymaxmin}. We identify minority users by computing each user's accuracy under majority vote (MV-ACC), with those scoring below 50\% (random performance) classified as minority users due to their systematic deviation from the majority. P-SOUPS shows compressed MV-ACC scores (0.51-0.59), suggesting preference conflicts or noise. TL;DR exhibits high MV-ACC, indicating limited personalization potential, while Personal-LLM shows a wider range with some scores below 0.5, revealing clear minority viewpoints.

\paragraph{Room for Personalization} 
The potential for effective personalization is determined by the interplay between inter-personal disagreement and intra-personal consistency. This \textbf{room for personalization} is bounded by two factors: the performance of a non-personalized aggregate reward model, and the consistency of individual user preferences. The gap between these bounds represents the maximum possible improvement through personalization.














\subsection{Evaluation Metrics}
\label{subsection: metrics}
While prior work has focused primarily on reward model accuracy, practical deployment requires broader evaluation criteria:
\paragraph{Personalization for Seen Users}
An ideal personalization algorithm should exhibit two key properties: (1) \textit{Collaborative Learning:} methods should leverage collaborative signals from similar users to efficiently learn diverse preferences, outperforming naive individual reward modeling. (2) \textit{Protecting Minority Viewpoints:} methods must fairly represent and adapt to minority preferences, avoiding the marginalization observed in non-personalized approaches. Therefore, we report both the average accuracy across users and per-user accuracy to assess whether the algorithms improve personalized preference learning and, in particular, how they affect individual users. 

\paragraph{Adaptation to New Users} 
Methods must address the cold-start challenge of adapting to new users with limited data, particularly when inter-personal disagreement is high. We evaluate performance with 30-100-300 preference pairs per user.

\paragraph{No ``Personalization Tax''} 
Personalization methods must maintain the model's core capabilities — a challenge we term the ``personalization tax.'' This is especially important when adapting to users whose preferences deviate significantly from the majority. Using Reward Bench \cite{lambert2024rewardbench}, we assess potential degradation in chat quality, reasoning ability, and safety.



\subsection{Experimental Setup} 
For reward modeling, we use LLaMA-2-7B base \cite{touvron2023llama} as the base model. For RAG, we employ sentence transformer MiniLM-L6-v2 \cite{reimers-2019-sentence-bert} to embed text and compute cosine similarity. For GPO, following \cite{zhao_group_2023}, we use LLaMA-2-7B embeddings and implement a separate 6-layer Transformer module as the GPO model. For fine-tuning details, please refer to Appendix \ref{hyperparam}.



