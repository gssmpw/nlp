\pdfoutput=1

\documentclass[11pt]{article}

\usepackage[]{acl}

\usepackage{times}
\usepackage{latexsym}
\usepackage{enumitem}
\usepackage{arydshln}

\usepackage[T1]{fontenc}

\usepackage[utf8]{inputenc}

\usepackage{microtype}
\usepackage{graphicx}
\usepackage{pifont}
\newcommand{\cmark}{\textcolor{green}{\ding{51}}} %
\newcommand{\xmark}{\textcolor{red}{\ding{55}}}  

\usepackage{tikz}
\newcommand{\good}{{\protect\tikz\fill[green] (0,0) circle (1ex);}} 
\newcommand{\medium}{{\protect\tikz\fill[yellow] (0,0) circle (1ex);}} %
\newcommand{\bad}{{\protect\tikz\fill[red] (0,0) circle (1ex);}}  %


\usepackage{inconsolata}
\usepackage{comment}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{cuted}
\usepackage{booktabs} 
\usepackage{subcaption}
\usepackage{float} 


\newcommand{\tiancheng}[1]{\textcolor{red}{Tiancheng: #1}}
\newcommand{\ahmet}[1]{\textcolor{orange}{Ahmet: #1}}
\newcommand{\nigel}[1]{\textcolor{blue}{Nigel: #1}}
\newcommand{\river}[1]{\textcolor{teal}{River: #1}}

\title{When Personalization Meets Reality: A Multi-Faceted Analysis of Personalized Preference Learning}

\author{Yijiang River Dong$^{1}$, Tiancheng Hu$^{1}$, Yinhong Liu$^{1}$, Ahmet \"Ust\"un$^{* 2}$, Nigel Collier$^{* 1}$ \\
         $^1$University of Cambridge \ $^2$Cohere For AI \\
         \{yd358, th656, yl535, nhc30\}@cam.ac.uk, \ \  ahmet@cohere.com}

\begin{document}
\maketitle
\def\thefootnote{*}\footnotetext{Equal Advising}\def\thefootnote{\arabic{footnote}}

\begin{abstract}
While Reinforcement Learning from Human Feedback (RLHF) is widely used to align Large Language Models (LLMs) with human preferences, it typically assumes homogeneous preferences across users, overlooking diverse human values and minority viewpoints.
Although personalized preference learning addresses this by tailoring separate preferences for individual users, the field lacks standardized methods to assess its effectiveness. 
We present a multi-faceted evaluation framework that measures not only performance but also fairness, unintended effects, and adaptability across varying levels of preference divergence.
Through extensive experiments comparing eight personalization methods across three preference datasets, we demonstrate that performance differences between methods could reach $36\%$ when users strongly disagree, and personalization can introduce up to $20\%$ safety misalignment. 
These findings highlight the critical need for holistic evaluation approaches to advance the development of more effective and inclusive preference learning systems.
\end{abstract}

\section{Introduction}
\input{sections/intro}

\section{Related Work}
\input{sections/related_work}

\section{Preliminaries on Personalized Preference Learning}
\input{sections/setup}

\section{Evaluation}
\input{sections/eval}



\section{Results}
\input{sections/results}

\section{Conclusion}
\input{sections/conclusion}

\section*{Limitation}
Firstly, two datasets that we evaluated on (P-SOUPS and Personal-LLM), are synthetically generated. These datasets make simplifying assumptions about human preferences, particularly regarding intra-personal consistency, which may not reflect the nuanced, context-dependent nature of real-world preferences. However, these controlled datasets serve a valuable purpose in our study: they clearly demonstrate how dataset characteristics interact with personalization algorithms to produce varying outcomes. While the collection of large-scale, open-domain personalized preference data from real users would be ideal for future work, such efforts face significant challenges related to cost, privacy, and scalability.

Secondly, we evaluated 8 methods where 3 of them, VPL \citep{poddar_personalizing_2024}, GPO \citep{zhao_group_2023}, Personalized RM \citep{li_personalized_2024} are specifically developed for personalized preference learning. The rapidly evolving nature of this field means our evaluation cannot be exhaustive. Recent developments in prompt optimization \cite{kim_few-shot_2024} and context compression \cite{kim_compressed_2024} suggest promising new directions that warrant investigation. Although resource constraints prevented us from evaluating all emerging approaches, we believe our selected methods effectively represent the key algorithmic paradigms currently employed in personalized preference learning.
\section*{Ethical Statement}
Current LLM alignment approaches, where a relatively small group of researchers and organizations dictate alignment targets, raise significant concerns about procedural justice and representation \citep{santurkar_whose_2023}. LLM personalization presents a promising solution by democratizing alignment, enhancing user experiences, responding to diverse needs, and promoting a more equitable and just information ecosystem. 

However, these personalized systems also pose  risks, including the potential creation of filter bubbles, reinforcement of existing biases, and exacerbation of ideological polarization. Additionally, while our study does not involve personally identifiable information, real-world deployment of personalized LLMs requires strong privacy safeguards to prevent the misuse of sensitive user data. Our findings further show that optimizing for individual preferences may lead to safety misalignment as discussed in Section \ref{sec: safety}. The central challenge, then, becomes how to balance the benefits and risks of LLM personalization \citep{kirk_benefits_2024}. These concerns highlight the importance of developing responsible personalization methods that prioritize fairness, privacy, and safety.

\section*{Acknowledgements}
T.H is supported by Gates Cambridge Trust (grant OPP1144 from the Bill \& Melinda Gates Foundation). This work was partially performed using resources provided by the Cambridge Service for Data Driven Discovery (CSD3) operated by the University of Cambridge Research Computing Service (www.csd3.cam.ac.uk), provided by Dell EMC and Intel using Tier-2 funding from the Engineering and Physical Sciences Research Council (capital grant EP/T022159/1), and DiRAC funding from the Science and Technology Facilities Council (www.dirac.ac.uk).

\bibliography{custom}
\newpage
\appendix
\input{sections/appendix}

\end{document}
