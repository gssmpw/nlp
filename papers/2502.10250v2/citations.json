[
  {
    "index": 0,
    "papers": [
      {
        "key": "chen2023sharegpt4v",
        "author": "Chen, Lin and Li, Jisong and Dong, Xiaoyi and Zhang, Pan and He, Conghui and Wang, Jiaqi and Zhao, Feng and Lin, Dahua",
        "title": "Sharegpt4v: Improving large multi-modal models with better captions"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "rotstein2023fusecap",
        "author": "Rotstein, Noam and Bensaid, David and Brody, Shaked and Ganz, Roy and Kimmel, Ron",
        "title": "Fusecap: Leveraging large language models to fuse visual data into enriched image captions"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "rasheed2024glamm",
        "author": "Rasheed, Hanoona and Maaz, Muhammad and Shaji, Sahal and Shaker, Abdelrahman and Khan, Salman and Cholakkal, Hisham and Anwer, Rao M and Xing, Eric and Yang, Ming-Hsuan and Khan, Fahad S",
        "title": "Glamm: Pixel grounding large multimodal model"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "laurenccon2024matters",
        "author": "Lauren{\\c{c}}on, Hugo and Tronchon, L{\\'e}o and Cord, Matthieu and Sanh, Victor",
        "title": "What matters when building vision-language models?"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "alayrac2022flamingo",
        "author": "Alayrac, Jean-Baptiste and Donahue, Jeff and Luc, Pauline and Miech, Antoine and Barr, Iain and Hasson, Yana and Lenc, Karel and Mensch, Arthur and Millican, Katherine and Reynolds, Malcolm and others",
        "title": "Flamingo: a visual language model for few-shot learning"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "liu2024visual",
        "author": "Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae",
        "title": "Visual instruction tuning"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "chen2024allava",
        "author": "Chen, Guiming Hardy and Chen, Shunian and Zhang, Ruifei and Chen, Junying and Wu, Xiangbo and Zhang, Zhiyi and Chen, Zhihong and Li, Jianquan and Wan, Xiang and Wang, Benyou",
        "title": "ALLaVA: Harnessing GPT4V-synthesized Data for A Lite Vision-Language Model"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "laurenccon2024matters",
        "author": "Lauren{\\c{c}}on, Hugo and Tronchon, L{\\'e}o and Cord, Matthieu and Sanh, Victor",
        "title": "What matters when building vision-language models?"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "liu2024visual",
        "author": "Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae",
        "title": "Visual instruction tuning"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "laurenccon2024matters",
        "author": "Lauren{\\c{c}}on, Hugo and Tronchon, L{\\'e}o and Cord, Matthieu and Sanh, Victor",
        "title": "What matters when building vision-language models?"
      }
    ]
  }
]