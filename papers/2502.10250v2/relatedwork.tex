\section{Related Work}
Creating high-quality datasets for fine-tuning vision-language models is essential for improving their performance on complex multimodal tasks. Existing methods have made significant strides in this area, yet various challenges persist in terms of diversity, contextual richness, and scalability. Here, we discuss notable contributions and their limitations, setting the stage for the introduction of our approach used to develop \textbf{VisCon-100K}.

\paragraph{\textbf{Vision-Language Dataset Creation}}

\begin{enumerate}
    \item \textbf{Fine-Grained Image Captions}: Approaches such as those used in \textbf{ShareGPT4V} \citep{chen2023sharegpt4v}, \textbf{FuseCap} \citep{rotstein2023fusecap}, and \textbf{GranD} \citep{rasheed2024glamm} generate detailed image descriptions using LLMs. ShareGPT4V employs the GPT-4V API to produce detailed seed captions, aiming to reduce hallucinations and enhance dataset quality. Similarly, FuseCap integrates visual information from sources like object detectors and image taggers to enrich the captions, while GranD also queries LLM with a scene graph to add extra context. However, as these datasets scale, they tend to produce redundant descriptions of similar visual content, limiting their diversity and informativeness.
    \item \textbf{Contextual Data Utilization}: Some models, like \textbf{IDEFICS-2} \citep{laurenccon2024matters} and \textbf{Flamingo} \citep{alayrac2022flamingo}, employ contextual data in their pretraining by using interleaved image-text web documents. However, these approaches often retain a weak dependency on images while focusing on textual next-token prediction. The lack of grounding in the visual content means that the context derived from the web documents does not fully integrate with the image data, resulting in suboptimal alignment between visual and textual modalities.
    \item \textbf{Repurposing Classical Computer Vision Datasets}: Other methods, like \textbf{LLaVA} \citep{liu2024visual}, \textbf{ALLaVA} \citep{chen2024allava} and \textbf{IDEFICS-2} \citep{laurenccon2024matters}, attempt to repurpose datasets from common computer vision tasks for vision-language fine-tuning. While useful, these datasets often lack the diversity and contextual richness needed for real-life image conversations. They typically provide limited contextual information and fail to capture the broader web-based context that can enhance vision-language understanding. Moreover, these datasets often exhibit modality isolation, where questions are answerable either from a visual or a textual modality, but not both.

    % integrated this with Repurposing Classical Computer Vision Datasets and leaky mix
    % \item \textbf{Data Formats for VLM Fine-tuning}: In fine-tuning, most VLMs prepare their instruction-tuning dataset by converting different data types into the form of Q \& A. In LLaVA \citep{liu2024visual}, for instance, the initial captions, along with the bounding boxes information, are used to prompt GPT-4 to obtain the Q \& A pairs. Similarly, IDEFICS-2 \citep{laurenccon2024matters} converts 50 datasets covering different tasks into the question/answer format for fine-tuning. Different questions can focus on different perspectives, such as traditional questions in conversation (expecting short answers), and others require detailed descriptions (expecting long answers). However, the perspectives in the formatted questions can be restricted to a single image, leaving the model biased toward generating either short or long answers. Plus, the trained model also falls short in fully utilizing fine-grained details.
    
\end{enumerate}

\paragraph{\textbf{Challenges and Limitations}}

\begin{itemize}
    \item \textbf{Redundancy}: A common issue with current methods is the generation of redundant information, especially when scaling up the dataset. Repeated descriptions of similar content can reduce the dataset's overall effectiveness in training robust VLMs.
    \item \textbf{Lack of Contextual Grounding}: Many approaches show limited ability to generate data that is both contextually rich and relevant to real-life applications.
    \item \textbf{Modality Isolation}: Existing fine-tuning methods often treat visual and textual data separately, leading to a lack of integration between the two modalities. This isolation results in models that may excel in either visual understanding or textual comprehension but struggle to combine these insights effectively.
\end{itemize}

% \paragraph{\textbf{Our Approach: VisCon-100K}}

% \textbf{VisCon-100K} addresses these challenges by leveraging interleaved image-text web documents to create a dataset that captures both fine-grained visual details and broader contextual information. 
% Our pipeline converts 45K web documents from the OBELICS dataset into 100K image conversation samples. These samples include image-contextual captions generated using GPT-4 and diverse question-answer pairs created using OpenChat 3.5. 


By conditioning image captioning on accompanying web content, \textbf{VisCon-100K} ensures the generated captions are \textbf{unique} and \textbf{contextually relevant} even as the dataset scales. This approach mitigates redundancy and enhances the dataset's relevance by leveraging the surrounding web context, thereby offering a more comprehensive training resource.
Figure~\ref{fig:captions} illustrates this approach, showing a web page containing an image along with its non-contextual and contextual captions. The non-contextual caption describes the image in isolation, while our contextual caption integrates relevant information from the surrounding web content, providing a more nuanced and comprehensive description. Furthermore, our adaptation of the \textbf{leaky modality mix} in conversations provides an opportunity for interplay between visual and textual modalities with their tighter integration potentially.

\begin{figure*}[t]
  \centering
  \includegraphics[width=1\linewidth]{images/pipeline.png}
  \caption{Data generation pipeline for creating the VisCon-100K dataset.}
  \label{fig:pipeline}
\end{figure*}


\begin{figure*}[]
  \centering
  \includegraphics[width=0.9\linewidth]{images/eg1.pdf}
  \includegraphics[width=0.9\linewidth]{images/eg2.pdf}
  \includegraphics[width=0.9\linewidth]{images/eg3.pdf}
  \caption{Examples from the VisCon-100K dataset. The text, highlighted in red, shows contextual grounding.}
  \label{fig:examples}
\end{figure*}