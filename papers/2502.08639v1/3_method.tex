

\section{Method}
Controllable text-to-video generation (T2V) targets at providing more conditional guidance beyond textual prompts, thereby enabling fine-grained control over the video generation process. We present CineMaster, which aspires to give users a level of controllability comparable to professional film directors: allowing precise object placement within a scene, flexible manipulation of both objects and camera, and intuitive layout control over each rendered frame. Our proposed CineMaster operates in two stages. In the first stage~(Sec~\ref{sec:method_stage_1}), we propose an interactive workflow for constructing 3D-aware control signals. In the second stage (Sec~\ref{sec:method_stage_2}), these control signals serve as conditions for T2V models to synthesize the desired video content. Moreover, due to the scarcity of large-scale datasets with 3D bounding box and camera trajectory annotations, we carefully develop an automated data labeling pipeline (Sec~\ref{sec:dataset_labeling_pipeline}).


\subsection{Stage 1: 3D-Aware Control Signals}
\label{sec:method_stage_1}
The first stage centers on constructing 3D-aware control signals through a user-friendly workflow. Inspired by LooseControl~\cite{bhat2024loosecontrol}, we employ 3D bounding boxes as the principal form of object representation. Users can freely adjust the size and position of these bounding boxes within the 3D scene. By repositioning bounding boxes and camera across keyframes, users gain intuitive control over object and camera trajectories, effectively dictating the motion dynamics. Another key component of our workflow is the preview mechanism, which lets users examine rendered frames after each modification.


This workflow closely mirrors real-world filmmaking: directors typically arrange actor and camera movements in multiple takes, reviewing footage on monitors to refine the final shots. Once satisfactory rendering effects are achieved, we export camera trajectories and per-frame projected depth maps for use in the subsequent stage. The primary advantage of this system is its 3D-native and intuitive nature. We implement the interactive system using the open-source engine Blender, where users select keyframes for object and camera placement. The system then automatically interpolates trajectories for intermediate frames, providing a seamless and efficient workflow for complex scene setup.


\subsection{Stage 2: Conditional Video Generation}
\label{sec:method_stage_2}
We condition a base text-to-video model on the control signals derived from the first stage. Crucially, beyond using the camera trajectory and object labels as inputs, we also introduce projected depth maps of each frame as augmented visual condition. These depth maps explicitly encode the desired 3D layout, providing strong guidance for the diffusion model to generate accurate video content. To effectively integrate these additional inputs into the T2V model, we design two key components: a semantic layout injection module and camera adapter, as illustrated in Fig.~\ref{fig:network}.



\noindent\textbf{Base Model.} Our model is developed upon a pretrained text-to-video foundation model, which consists of a 3D Variational Auto-Encoder (VAE)~\cite{kingma2013auto}, T5 encoder~\cite{raffel2020exploring} and a transformer-based latent diffusion model~\cite{peebles2023scalable,chen2023pixart}. Each basic transformer block is instantiated as a sequence of 2D spatial self-attention, 3D spatial-temporal self-attention, text cross attention and feed-forward network~(FFN). The text prompts are encoded as $c_{text}$ by T5 encoder to guide the generation model. We define a straight forward path between clean data $z_0$ and noised data $z_t$ at timestep $t$ with Rectified Flow~\cite{esser2024scaling}:
\begin{equation}
\label{add_noise}
     z_t = (1-t)z_0 + t\epsilon,
\end{equation}
where $\epsilon\in\mathcal{N}(0,\mathbf{I})$. The denoising process is defined as a mapping from $z_t$ to $z_0$ by an ordinary differential equation (ODE):
\begin{equation}
\label{denoise}
     dz_t = v_{\Theta}(z_t,t,c_{text})dt,
\end{equation}
where the velocity $v$ is parameterized by the weights $\Theta$ of the denoising network. The training process is supervised by Conditional Flow Matching~\cite{lipman2022flow} to regress velocity:
\begin{equation}
\label{lcm_loss}
     \mathcal{L}_{LCM} = \mathbb{E}_{t,\epsilon\sim\mathcal{N}(0,\mathbf{I}),z_0}\left[\|(z_1-z_0)-v_{\Theta}(z_t,t,c_{text})\|^2_2\right]
\end{equation}



\noindent\textbf{Semantic Layout ControlNet.} We design a Semantic Layout ControlNet to integrate the 3D spatial layouts from the projected depth maps with semantic information from per-entity class labels. Specifically, we copy $N/2$ DiT blocks from the base model to form the DiT-based ControlNet. Projected depth maps could represent the spatial layout of a scene, but when multiple subjects appear in text prompts, it is difficult to specify the position of each entity only by text prompts. Therefore, we use text encoder to represent $n$ entity class labels of the input video as text embeddings~$l\in\mathbb{R}^{n\times L}$, and fuse these embeddings to the corresponding position specified by the downsampled entity mask~$m\in\mathbb{R}^{F\times H\times W\times 1}$ as shown in Fig.~\ref{fig:network}(b). $L$ denotes the dimension of a text embedding. $H$ and $W$ denote the spatial size of the latent encoded by VAE, and $C$ is the number of channels. The 3D VAE latents of projected depth maps~$d\in\mathbb{R}^{F\times H\times W\times C}$ are concatenated with the semantic embeddings along channel dimension and fused by MLPs. The hidden states of ControlNet are incorporated into the base model to provide semantic layout guidance for the generation process.


\begin{figure*}[!t]
  \centering
\includegraphics[width=\linewidth]{data_pipeline.pdf}
  \caption{Dataset Labeling Pipeline. We propose a data labeling pipeline to extract 3D bounding boxes, class labels and camera poses from videos. Our pipeline consists of four steps: 1) Instance Segmentation: Obtain instance segmentation results from the foreground in videos. 2) Depth Estimation: Produce metric depth maps using DepthAnything V2. 3) 3D Point Cloud and Box Calculation: Identify the frame with the largest mask for each entity and compute the 3D point cloud of each entity through inverse projection. Then, use the minimum volume method to calculate the 3D bounding box for each entity. 4) Entity Tracking and 3D Box Adjustment: Access the point tracking results of each entity and calculate the 3D bounding boxes for each frame. Finally, project the entire 3D scene into depth maps.}
  \label{fig:dataset_pipeline}
  \vspace{-8pt}

\end{figure*}


While the proposed Semantic Layout ControlNet enables precise control over the 3D position of each entity in generated videos, relying exclusively on 3D bounding boxes might introduce ambiguity between object and camera movements. For instance, if the bounding box of a balloon shifts upward, it could indicate that the ballon is rising, the camera is moving downward, or both. To resolve this ambiguity, we additionally inject explicit camera poses into the generation process, allowing the model to distinguish object motion from camera trajectories more reliably.

\noindent\textbf{Camera Adapter.} In each DiT block, we incorporate the camera condition via a residual connection situated between the self-attention module and temporal-attention module as shown in Fig.~\ref{fig:network}(c). Specifically, each camera pose is represented by a $3\times 3$ rotation matrix and a $3\times 1$ translation matrix. We take a sequence of camera pose $RT=\{RT_0,RT_1,\cdots,RT_{F-1}\}\in\mathbb{R}^{F\times 12}$ as input, where $F$ is the length of latent frames. An MLP first aligns the dimension of $RT$ to the token length $C$. We then add the camera pose features to each token in the hidden states. These fused hidden states are fed into the self-attention module to inject camera motion information. In addition, the camera pose features are introduced again via the residual connection. By leveraging this camera adapter, the proposed CineMaster could support joint control over object motion and camera motion. 





\subsection{Dataset Labeling Pipeline}
\label{sec:dataset_labeling_pipeline}

There is a lack of large-scale video datasets with 3D bounding box and camera pose annotations. To train the second-stage network, we design an automated data labeling pipeline as shown in Fig.~\ref{fig:dataset_pipeline}. It takes an in-the-wild video as input and extracts the required class labels, camera trajectories and projected depth maps.

\noindent\textbf{Class Labels.} To obtain the class labels for objects present in the scene, we perform instance segmentation for each entity in the video. To achieve open-set instance segmentation, we combine Grounding DINO\cite{groundingdino} with SAM 2~\cite{2024sam2segmentimages}, where Grounding DINO produces $2$D bounding boxes guided by entity descriptions. To enhance foreground entity detection, we utilize the multi-modal large model Qwen2~\cite{qwen2} to generate entity descriptions as guidance for Grounding DINO. This process yields $2$D bounding boxes and class labels for each entity in the first frame, which then guides SAM 2 for video segmentation. To address potential issues with overlapping boxes and incorrect class labels from Grounding DINO, we implement crucial post-processing steps: a box IOU filter and feature similarity verification between the regions within $2$D boxes and their assigned class labels.

\noindent\textbf{Camera Trajectories.} We employ the SOTA camera pose estimation model MonST3R~\cite{zhang2024monst3r} to obtain camera trajectories throughout the video sequence.

\noindent\textbf{Projected Depth Maps.} We employ DepthAnything V2~\cite{depth_anything_v2} to generate metric depth maps for the entire video sequence, which are essential for the subsequent inverse projection process. 
The third step involves inverse projection to obtain $3$D boxes for each entity. We operate under the assumption that each entity maintains a constant volume in the $3$D scene. To address cases where entities may appear partially in certain frames, we identify the optimal frame index for each entity, typically when the entity is most completely visible, to ensure accurate inverse projection and adequate volume representation. For each entity, we combine the instance segmentation mask with the corresponding metric depth map at this optimal frame to generate a $3$D point cloud, from which we derive the minimal-volume $3$D bounding box.


\begin{figure*}[!t]
  \centering
  \includegraphics[width=\linewidth]{comparison.pdf}
  \vspace{-20pt}  
  \caption{We present three different feature comparisons: moving object $\&$ static camera, static object $\&$ moving camera and moving object $\&$ moving camera. We transform our 3D box condition to object trajectories for MotionCtrl~\cite{MotionCtrl} and 2D bounding box sequences for Direct-A-Video~\cite{yang2024direct} to align the input conditions. In comparison, CineMaster could better control object motion and camera motion separately or jointly to generate diverse user-intended scenes.}
  \label{fig:comparison}
  \vspace{-8pt}  
\end{figure*}



Following the establishment of maximum 
$3$D boxes for all entities, the final step involves computing temporal-spatial transformations of these boxes within the $3$D scene for all frames. 
We conduct 3D point tracking by SpatialTracker~\cite{SpatialTracker} starting from the optimal frame $N(j)$ of $j$th object to the rest frames $\{..., N(j)-1, N(j)+1, ...\}$, and the average inter-frame displacements $\{\Delta x_{ij}, \Delta y_{ij}, \Delta z_{ij}\}$ of all feature points from each object are regarded as the spatial movement of the $j$th object's 3D box where $ij\in\{..., N(j)-1, N(j)+1, ...\}$ denotes $i$th frame of $j$th object. Then we can compute the 3D boxes of all objects in all frames. To represent 3D boxes as explicit control signals, we further project the constructed 3D boxes into image space and render depth maps.
