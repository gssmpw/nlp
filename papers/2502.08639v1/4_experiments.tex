
\section{Experiments}
\subsection{Experimental Setup}
\noindent\textbf{Training Paradigm.} We design a dedicated training strategy consisting of three stages, i.e., 1) training a DiT-based ControlNet on dense depth maps, 2) adapting ControlNet to 3D box datasets, and 3) jointly training Semantic Layout ControlNet and Camera Adapter. Specifically, following LooseControl~\cite{bhat2024loosecontrol}, we first train our DiT-based ControlNet on 167K videos crawled from the Internet with dense depth maps labeled by DepthAnything V2~\cite{depth_anything_v2}. Subsequently, we use our data annotation pipeline to construct a 3D box dataset with 156K videos and 118K images for training Semantic Layout ControlNet. The images are collected from COCO~\cite{lin2014microsoft} and Object365~\cite{shao2019objects365} which provide more categories and precise instance segmentation annotations. With the metric depth maps measured by DepthAnything V2~\cite{depth_anything_v2}, we could obtain the 3D boxes of image datasets by calculating the 3D box with minimal volume for each object in the point cloud. By the image-video joint training, we integrate the spatial layout and semantic information into the Semantic Layout ControlNet which could guide the base model to generate box-aligned videos with specified class labels. We further annotate 99.6K videos out of the 156k videos using our proposed pipeline to obtain camera poses. We also utilize RealEstate10K~\cite{realestate10k} dataset which features larger camera motion, resulting in 10.4K data samples. We sample data between our dataset and Real-Estate10K with 3:1 ratio for training to enhance the learning of larger camera motion. 
Based on the merged video dataset with both 3D box and camera pose annotations, we train the Semantic Layout ControlNet and Camera Adapter jointly and master the joint controllability of object motion and camera motion for flexible customized video generation.



\noindent\textbf{Implementation Details.} We train CineMaster based on our internal text-to-video generation model with $\sim1B$ parameters for research purposes. Following NaViT~\cite{dehghani2024patch}, we pad the videos to the same shape for each batch managed by attention masks during training. Each training video segment contains 77 frames~(i.e., 5 seconds) sampled with 15 frames per second~(fps). We use Adam optimizer~\cite{kingma2014adam} and train on 24 NVIDIA A800 GPUs, with a batch size of 4 and a learning rate of $5\times10^{-5}$. The three stages of the training process consist of 12,000, 7,000, 6000 steps respectively. During inference, we set the scale of classifier-free guidance~\cite{ho2022classifier} as 12.5 and the DDIM~\cite{song2020denoising} steps as 50. We make a trade-off between object motion and camera motion by injecting semantic layout information and camera poses with 25 and 15 steps respectively.


\noindent\textbf{Baselines.} We compare CineMaster with existing SOTA methods MotionCtrl~\cite{MotionCtrl} and Direct-a-Video~\cite{yang2024direct} which could also control object motion and camera motion simultaneously. To align with different input requirements, we convert our 3D box condition into object trajectories for MotionCtrl and 2D bounding box sequences for Direct-A-Video. In addition, we empirically align the coordinate system and scale of the input camera poses for comparison.



\begin{table}[!t]
\centering
\caption{Quantitative comparisons with baselines. $\uparrow$ indicates higher is better, while $\downarrow$ indicates that lower is better. The best result is shown in \textbf{bold}. Our CineMaster outperforms previous SOTA baselines on all metrics.
 }
\vspace{-4pt}
\setlength{\tabcolsep}{1.5pt}
\label{tab:comparsion}
\begin{tabular}{lccccc}
\toprule
               & mIoU$\uparrow$           & Traj-D$\downarrow$        & FVD$\downarrow$               & FID$\downarrow$             & CLIP-T$\uparrow$              \\
\midrule                
MotionCtrl     & $-$          & 94.82          & 2163.0          & 201.6          & 0.302                    \\
Direct-A-Video & 0.332          & 83.53          & 1966.3          & 183.5          & 0.273                   \\
Ours           & \textbf{0.551} & \textbf{66.29} & \textbf{1530.9} & \textbf{175.9} & \textbf{0.321}  \\
\bottomrule
\vspace{-18pt}
\end{tabular}
\end{table}



\noindent\textbf{Evaluation Metrics.}
1) \textit{Object-box alignment}: We use Grounding DINO~\cite{groundingdino} to detect 2D object boxes in generated videos to measure the mean Intersection over Union~(mIoU) and measure the trajectory deviation~(Traj-D) by calculating the difference of the center points against ground truths. In addition, to evaluate the depth control accuracy of the generated objects, we calculate the average depth of the object regions in each generated frame using SAM 2~\cite{2024sam2segmentimages} and DepthAnything V2~\cite{depth_anything_v2}, and measure the depth deviation~(Depth-D) by the Root Mean Squared Error (RMSE) with the depth values of the given 3D boxes. 2) \textit{Video quality}: We employ Fréchet Video Distance (FID)~\cite{unterthiner2018towards}, Fréchet Inception Distance (FID)~\cite{lucic2017gans} and CLIP Similarity~(CLIP-T) to evaluate the appearance of generated results.




\subsection{Comparison with Other Methods}
\noindent\textbf{Qualitative Comparison.} As shown in Fig.~\ref{fig:comparison}, we show three different feature comparisons: moving object $\&$ static camera, static object $\&$ moving camera and moving object $\&$ moving camera. In the first setting, MotionCtrl~\cite{MotionCtrl} moves the camera rightward to align the object trajectory, but fails to either maintain the camera stationary or make the object move. It shows that there is still the camera motion and object motion coupling issue in MotionCtrl. In the third setting, since MotionCtrl is unable to associate multiple trajectories with their respective objects, it generates the ``McLaren" that appears to follow the trajectory of the ``person" and fails to generate the ``person". Direct-A-Video~\cite{yang2024direct} presents low-quality textures for generating ``bus" and ``rock" which demonstrates that its control disturbs the generation quality. It exhibits weaker camera movement and box alignment, and produces unexpected shot changes and more artifacts in the third setting. In comparison, the proposed CineMaster performs the best control performance for the control of object motion and camera motion in three settings. 



\noindent\textbf{Quantitative Comparison.} In addition, we further report the quantitative comparison in Table~\ref{tab:comparsion}. Since MotionCtrl only uses point trajectory sequences to specify the positions of generated objects and ignores the spatial size, we do not calculate its mIoU. It does not explicitly associate multiple trajectories with their respective objects and suffers from the camera motion and object motion coupling issue. Therefore, it obtains unsatisfactory Traj-D. Direct-A-Video only trains for learning camera movement, and controls the object motion by spatial cross-attention modulation with associated object words and box trajectories to guide the spatial-temporal placement of objects only for inference. The training-free object motion control biases the vanilla inference distribution, resulting in the degradation of generation quality. In addition, no joint training of object motion and camera motion control leads to a gap between training and inference, so it obtains weak mIoU and Traj-D. In contrast, we construct the video dataset with both 3D box and camera pose for joint training, and the proposed CineMaster could harmoniously control the object motion and camera motion simultaneously. Therefore, CineMaster outperforms previous SOTA methods on all metrics. In particular, we achieve significantly higher mIoU and Traj-D, indicating that our framework can generate videos that better follow the user's spatial design.



\begin{table}[t]
\centering
\caption{Ablation study for training paradigms. Details of each setting are introduced in Sec~\ref{sec:ablation}. Overall, the setting of ``Joint Train"~(our final version) achieves the best performance on all metrics than other variants.}
\vspace{-4pt}
\setlength{\tabcolsep}{1.5pt}
\label{tab:ablation}
\resizebox{\linewidth}{!}{
\begin{tabular}{lcccccc}
\toprule
 & mIoU$\uparrow$           & Traj-D$\downarrow$        & FVD$\downarrow$               & FID$\downarrow$             & CLIP-T$\uparrow$      & Depth-D$\downarrow$      \\
\midrule                        
w/o stage 1   & 0.544          & 72.27          & 176.4          & 1576.1          & 0.310          & 0.725          \\
w/o semantic  & 0.391          & 83.81          & 177.4          & 1622.2          & 0.304          & 0.717          \\
Isolated S,C  & 0.480          & 70.53          & 180.2          & 1840.9          & 0.313          & 0.705          \\
Fix S train C & 0.545          & 68.15          & 177.8          & 1673.9          & 0.317          & 0.702          \\
Joint Train   & \textbf{0.551} & \textbf{66.29} & \textbf{175.9} & \textbf{1530.9} & \textbf{0.321} & \textbf{0.685}\\
\bottomrule
\end{tabular}}
\vspace{-13pt}
\end{table}



\subsection{Ablation Study}
\label{sec:ablation}
We experiment with different training paradigms to validate the effectiveness of the delicate designs in our workflow:
\begin{itemize}
\item ``w/o stage 1": without training the DiT-based ControlNet on dense depth maps, the training process starts directly from the second training stage.
\item ``w/o semantic": without semantic injector, this setting does not specify the class labels of 3D boxes.
\item ``Isolated S,C": Semantic Layout ControlNet and Camera Adapter are trained separately and used together for inference.
\item ``Fix S train C": this setting first trains Semantic Layout ControlNet to convergence, then freezes its weights and trains the Camera Adapter. 
\item ``Joint Train"~(our final version): Semantic Layout ControlNet and Camera Adapter are trained simultaneously.
\end{itemize}



As shown in Table~\ref{tab:ablation}, the setting of ``w/o stage 1" lacks the fine-grained perception for depth control signal, so it obtains mediocre Depth-D. The positions of the generation objects can only be specified via text prompt in the setting of ``w/o semantic", resulting in the poor mIoU, Traj-D and CLIP-T. The setting of ``Isolated S,C" faces a discrepancy between training and inference phases, as the Semantic Layout ControlNet and Camera Adapter are trained separately without cross-module communication, resulting in degraded generation quality supported by lower FVD and FID. Although the setting of ``Fix S train C" trains the camera adapter based on the frozen Semantic Layout ControlNet, it still fails to eliminate the coupling of camera motion and object motion already learned in the frozen Semantic Layout ControlNet, leading to suboptimal FVD and FID. 
Benefiting from the constructed video dataset with 3D box and camera pose labels, we experimentally observe that the setting of ``Joint Train" could harmoniously integrate the control for camera motion and object motion and perform the best results on all metrics.



