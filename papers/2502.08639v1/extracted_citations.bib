@misc{MotionCtrl,
	title = {{MotionCtrl}: {A} {Unified} and {Flexible} {Motion} {Controller} for {Video} {Generation}},
	shorttitle = {{MotionCtrl}},
	url = {http://arxiv.org/abs/2312.03641},
	doi = {10.48550/arXiv.2312.03641},
	abstract = {},
	language = {en},
	urldate = {2025-01-11},
	publisher = {arXiv},
	author = {Wang, Zhouxia and Yuan, Ziyang and Wang, Xintao and Chen, Tianshui and Xia, Menghan and Luo, Ping and Shan, Ying},
	month = jul,
	year = {2024},
	note = {arXiv:2312.03641 [cs]},
}

@inproceedings{SpatialTracker,
    title={SpatialTracker: Tracking Any 2D Pixels in 3D Space},
    author={Xiao, Yuxi and Wang, Qianqian and Zhang, Shangzhan and Xue, Nan and Peng, Sida and Shen, Yujun and Zhou, Xiaowei},
    booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    year={2024}
}

@misc{Tora2024,
	title = {Tora: {Trajectory}-oriented {Diffusion} {Transformer} for {Video} {Generation}},
	shorttitle = {Tora},
	url = {http://arxiv.org/abs/2407.21705},
	abstract = {Recent advancements in Diffusion Transformer (DiT) have demonstrated remarkable proficiency in producing high-quality video content. Nonetheless, the potential of transformer-based diffusion models for effectively generating videos with controllable motion remains an area of limited exploration. This paper introduces Tora, the first trajectory-oriented DiT framework that integrates textual, visual, and trajectory conditions concurrently for video generation. Specifically, Tora consists of a Trajectory Extractor (TE), a Spatial-Temporal DiT, and a Motion-guidance Fuser (MGF). The TE encodes arbitrary trajectories into hierarchical spacetime motion patches with a 3D video compression network. The MGF integrates the motion patches into the DiT blocks to generate consistent videos following trajectories. Our design aligns seamlessly with DiT’s scalability, allowing precise control of video content’s dynamics with diverse durations, aspect ratios, and resolutions. Extensive experiments demonstrate Tora’s excellence in achieving high motion fidelity, while also meticulously simulating the movement of the physical world.},
	language = {en},
	urldate = {2024-08-08},
	publisher = {arXiv},
	author = {Zhang, Zhenghao and Liao, Junchao and Li, Menghao and Qin, Long and Wang, Weizhi},
	month = jul,
	year = {2024},
	note = {arXiv:2407.21705 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Zhang 等 - 2024 - Tora Trajectory-oriented Diffusion Transformer fo.pdf:C\:\\Users\\12727\\Zotero\\storage\\SMF4Q4S5\\Zhang 等 - 2024 - Tora Trajectory-oriented Diffusion Transformer fo.pdf:application/pdf},
}

@misc{VideoComposer,
	title = {{VideoComposer}: {Compositional} {Video} {Synthesis} with {Motion} {Controllability}},
	shorttitle = {{VideoComposer}},
	url = {http://arxiv.org/abs/2306.02018},
	abstract = {},
	language = {en},
	urldate = {2024-08-29},
	publisher = {arXiv},
	author = {Wang, Xiang and Yuan, Hangjie and Zhang, Shiwei and Chen, Dayou and Wang, Jiuniu and Zhang, Yingya and Shen, Yujun and Zhao, Deli and Zhou, Jingren},
	month = jun,
	year = {2023},
	note = {arXiv:2306.02018 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, 已读},
	file = {Wang 等 - 2023 - VideoComposer Compositional Video Synthesis with .pdf:C\:\\Users\\12727\\Zotero\\storage\\UXWBGZJ3\\Wang 等 - 2023 - VideoComposer Compositional Video Synthesis with .pdf:application/pdf},
}

@inproceedings{bhat2024loosecontrol,
  title={Loosecontrol: Lifting controlnet for generalized depth conditioning},
  author={Bhat, Shariq Farooq and Mitra, Niloy and Wonka, Peter},
  booktitle={ACM SIGGRAPH 2024 Conference Papers},
  pages={1--11},
  year={2024}
}

@article{bian2025gs,
  title={GS-DiT: Advancing Video Generation with Pseudo 4D Gaussian Fields through Efficient Dense 3D Point Tracking},
  author={Bian, Weikang and Huang, Zhaoyang and Shi, Xiaoyu and Li, Yijin and Wang, Fu-Yun and Li, Hongsheng},
  journal={arXiv preprint arXiv:2501.02690},
  year={2025}
}

@misc{boximator,
      title={Boximator: Generating Rich and Controllable Motions for Video Synthesis}, 
      author={Jiawei Wang and Yuchen Zhang and Jiaxin Zou and Yan Zeng and Guoqiang Wei and Liping Yuan and Hang Li},
      year={2024},
      eprint={2402.01566},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2402.01566}, 
}

@misc{cameractrl,
      title={CameraCtrl: Enabling Camera Control for Text-to-Video Generation}, 
      author={Hao He and Yinghao Xu and Yuwei Guo and Gordon Wetzstein and Bo Dai and Hongsheng Li and Ceyuan Yang},
      year={2024},
      eprint={2404.02101},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2404.02101}, 
}

@article{chen2023control,
  title={Control-a-video: Controllable text-to-video generation with diffusion models},
  author={Chen, Weifeng and Ji, Yatai and Wu, Jie and Wu, Hefeng and Xie, Pan and Li, Jiashi and Xia, Xin and Xiao, Xuefeng and Lin, Liang},
  journal={arXiv preprint arXiv:2305.13840},
  year={2023}
}

@article{chen2025perception,
  title={Perception-as-Control: Fine-grained Controllable Image Animation with 3D-aware Motion Representation},
  author={Chen, Yingjie and Men, Yifang and Yao, Yuan and Cui, Miaomiao and Bo, Liefeng},
  journal={arXiv preprint arXiv:2501.05020},
  year={2025}
}

@misc{controlnet,
  title={Adding Conditional Control to Text-to-Image Diffusion Models}, 
  author={Lvmin Zhang and Anyi Rao and Maneesh Agrawala},
  booktitle={IEEE International Conference on Computer Vision (ICCV)},
  year={2023},
}

@misc{dragnuwa,
      title={DragNUWA: Fine-grained Control in Video Generation by Integrating Text, Image, and Trajectory}, 
      author={Shengming Yin and Chenfei Wu and Jian Liang and Jie Shi and Houqiang Li and Gong Ming and Nan Duan},
      year={2023},
      eprint={2308.08089},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2308.08089}, 
}

@article{fu20243dtrajmaster,
  title={3DTrajMaster: Mastering 3D Trajectory for Multi-Entity Motion in Video Generation},
  author={Fu, Xiao and Liu, Xian and Wang, Xintao and Peng, Sida and Xia, Menghan and Shi, Xiaoyu and Yuan, Ziyang and Wan, Pengfei and Zhang, Di and Lin, Dahua},
  journal={arXiv preprint arXiv:2412.07759},
  year={2024}
}

@article{geng2024motion,
  title={Motion Prompting: Controlling Video Generation with Motion Trajectories},
  author={Geng, Daniel and Herrmann, Charles and Hur, Junhwa and Cole, Forrester and Zhang, Serena and Pfaff, Tobias and Lopez-Guevara, Tatiana and Doersch, Carl and Aytar, Yusuf and Rubinstein, Michael and others},
  journal={arXiv preprint arXiv:2412.02700},
  year={2024}
}

@inproceedings{hu2024animate,
  title={Animate anyone: Consistent and controllable image-to-video synthesis for character animation},
  author={Hu, Li},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={8153--8163},
  year={2024}
}

@article{koroglu2024onlyflow,
  title={OnlyFlow: Optical Flow based Motion Conditioning for Video Diffusion Models},
  author={Koroglu, Mathis and Caselles-Dupr{\'e}, Hugo and Sanmiguel, Guillaume Jeanneret and Cord, Matthieu},
  journal={arXiv preprint arXiv:2411.10501},
  year={2024}
}

@misc{motioncanvas,
      title={MotionCanvas: Cinematic Shot Design with Controllable Image-to-Video Generation}, 
      author={Jinbo Xing and Long Mai and Cusuh Ham and Jiahui Huang and Aniruddha Mahapatra and Chi-Wing Fu and Tien-Tsin Wong and Feng Liu},
      year={2025},
      eprint={2502.04299},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2502.04299}, 
}

@inproceedings{motioni2v,
  title={Motion-i2v: Consistent and controllable image-to-video generation with explicit motion modeling},
  author={Shi, Xiaoyu and Huang, Zhaoyang and Wang, Fu-Yun and Bian, Weikang and Li, Dasong and Zhang, Yi and Zhang, Manyuan and Cheung, Ka Chun and See, Simon and Qin, Hongwei and others},
  booktitle={ACM SIGGRAPH 2024 Conference Papers},
  pages={1--11},
  year={2024}
}

@inproceedings{mou2024t2i,
  title={T2i-adapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models},
  author={Mou, Chong and Wang, Xintao and Xie, Liangbin and Wu, Yanze and Zhang, Jian and Qi, Zhongang and Shan, Ying},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={38},
  number={5},
  pages={4296--4304},
  year={2024}
}

@misc{realestate10k,
      title={Stereo Magnification: Learning View Synthesis using Multiplane Images}, 
      author={Tinghui Zhou and Richard Tucker and John Flynn and Graham Fyffe and Noah Snavely},
      year={2018},
      eprint={1805.09817},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1805.09817}, 
}

@article{shader,
  title={Diffusion as Shader: 3D-aware Video Diffusion for Versatile Video Generation Control},
  author={Gu, Zekai and Yan, Rui and Lu, Jiahao and Li, Peng and Dou, Zhiyang and Si, Chenyang and Dong, Zhen and Liu, Qifeng and Lin, Cheng and Liu, Ziwei and others},
  journal={arXiv preprint arXiv:2501.03847},
  year={2025}
}

@article{shuai2025free,
  title={Free-Form Motion Control: A Synthetic Video Generation Dataset with Controllable Camera and Object Motions},
  author={Shuai, Xincheng and Ding, Henghui and Qin, Zhenyuan and Luo, Hao and Ma, Xingjun and Tao, Dacheng},
  journal={arXiv preprint arXiv:2501.01425},
  year={2025}
}

@article{wang2024easycontrol,
  title={EasyControl: Transfer ControlNet to Video Diffusion for Controllable Generation and Interpolation},
  author={Wang, Cong and Gu, Jiaxi and Hu, Panwen and Zhao, Haoyu and Guo, Yuanfan and Han, Jianhua and Xu, Hang and Liang, Xiaodan},
  journal={arXiv preprint arXiv:2408.13005},
  year={2024}
}

@inproceedings{yang2024direct,
  title={Direct-a-video: Customized video generation with user-directed camera movement and object motion},
  author={Yang, Shiyuan and Hou, Liang and Huang, Haibin and Ma, Chongyang and Wan, Pengfei and Zhang, Di and Chen, Xiaodong and Liao, Jing},
  booktitle={ACM SIGGRAPH 2024 Conference Papers},
  pages={1--12},
  year={2024}
}

