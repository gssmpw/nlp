\section{Introduction}
\label{sec:intro}
With the advances of diffusion models~\cite{ddpm,ddim,sd} and large-scale pretraining paradigms, text-to-video generation (T2V)~\cite{VideoComposer,chen2023videocrafter1,svd,animatediff} has experienced rapid development. The T2V models empower both artists and novices to create impressive videos merely by providing textual prompts. Subsequently, controllable video generation has gained increasing attention, driven by the growing demand for fine-grained control over the video creation process.


Ideally, a controllable T2V framework should grant users comparable controllability as a professional film director: allowing precise placement of objects within a scene, flexible manipulation of both objects and camera, and intuitive layout control over each rendered frame.
However, existing approaches fall short of achieving this vision.
Early works typically extend image-based ControlNet~\cite{controlnet,mou2024t2i} to the video domain, guiding generation using condition maps (e.g. depth~\cite{chen2023control}, semantic~\cite{wang2024easycontrol}, optical flow~\cite{motioni2v,koroglu2024onlyflow,bian2025gs}, or canny edge maps~\cite{wang2024easycontrol,guo2023sparsectrl}). Yet, these methods generally rely on pre-existing videos to obtain condition maps, since it is non-trivial to create such condition maps from scratch. Moreover, in terms of controllability, MotionCtrl~\cite{MotionCtrl} and Direct-A-Video~\cite{yang2024direct} offer preliminary control over both object and camera movements, but these methods only allow 2D object control which is very different from how filmmakers or video creators plan a shooting in 3D space. More recent works have moved towards integrating 3D-aware signals into video generation~\cite{fu20243dtrajmaster,chen2025perception,shader,shuai2025free}. However, these methods are only designed for image-to-video generation or rely on synthetic data from Unreal Engine.




To bridge the aforementioned gap, we present CineMaster, a framework designed for highly controllable text-to-video generation as shown in Fig.~\ref{fig:teaser}. Specifically, CineMaster operates in two stages, as presented in Fig.~\ref{fig:overview}. The first stage is an interactive workflow that allows users to specify the requirements (conditions) of a generated video, similar to how filmmakers design a capturing plan. It allows users to describe the primary objects in a scene using a set of 3D bounding boxes with semantic labels. These bounding boxes, along with the camera, can be repositioned across keyframes, allowing users to orchestrate complex motion dynamics. After each modification, CineMaster provides a preview of the rendered frames for iterative refinement until the desired rendered effects are achieved. In the second stage, we finetune a text-to-video diffusion model to generate a video conditioned on the control signals provided in the first stage. Crucially, in addition to camera trajectories and user-provided class labels, we propose to utilize the rendered depth maps of all frames as augmented visual cues. These depth maps explicitly contain the desired 3D layout of each frame, serving as strong guidance for the diffusion model to generate the user-intended video content.


One challenge for this design is the lack of videos with ground-truth 3D bounding box and camera trajectory annotations. To solve this limitation, we further propose an automatic data labeling pipeline, as illustrated in Sec~\ref{sec:dataset_labeling_pipeline}. Using this pipeline, we build the largest video datasets with both the ground-truth 3D bounding box and 3D camera trajectory annotations.


Finally, to evaluate the controllability of our proposed framework, we conduct extensive experiments, comparing it with existing SOTA methods, and performing ablative studies to validate the effectiveness of our core modules.



\begin{figure}[!t]
  \centering
  \includegraphics[width=\linewidth]{overview.pdf}
  \vspace{-15pt}
  \caption{Overview of CineMaster. CineMaster consists of two stages. First, we present an interactive workflow that allows users to intuitively manipulate the objects and camera in a 3D-native manner. Then the control signals are rendered from the 3D engine and fed into a text-to-video diffusion model, guiding the generation of user-intended video content.}
  \vspace{-8pt}
  \label{fig:overview}
\end{figure}

