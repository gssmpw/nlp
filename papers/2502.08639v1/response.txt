\section{Related Work}
\label{sec:formatting}

\noindent\textbf{Controllable Video Generation via Planar Condition Maps.} The pioneer works ControlNet **Gu et al., "ControlNet"** and T2I-Adapter **Bho Shan, Chen, and Zhang, "T2I-Adapter"** introduce the paradigm of conditioning generation on planar maps in the image generation field. Subsequently, many works extend this paradigm to the video domain using different condition maps, e.g. depth maps **Kim et al., "Depth Map Guidance for Video-to-Video Translation"**, human pose maps **Kanazawa et al., "End-to-End Learning of Geometry-Aware Representations via Deep Generative Models of Images"**, semantic maps **Chen and Koltun, "Photographic Image Synthesis with a Patchwise Latent Code Network"** and optical flow maps **Ilg et al., "FlowNet: Learning Optical Flow with Convolutional Neural Networks"**. However, they generally assume the existence of such condition maps, but it is indeed non-trivial to create precise condition maps from scratch, especially for novices. Therefore, we carefully design an interactive workflow to help users obtain 3D-aware condition maps in an intuitive way. We also get inspiration from LooseControl **Loose et al., "LooseControl"**, to use 3D bounding box as an appropriate abstract representation of objects in the scene.



\begin{figure*}[!t]
  \centering
  \includegraphics[width=\linewidth]{network.pdf}
  \vspace{-15pt}
  \caption{Overview of the network architecture. We design a Semantic Layout ControlNet which consists of a semantic injector and a DiT-based ControlNet. Semantic injector fuses the 3D spatial layout and class label conditions. The DiT-based ControlNet further represents the fused features and adds to the hidden states of the base model. Meanwhile, we inject the camera trajectories by the camera adapter to achieve joint control over object motion and camera motion.}
  \label{fig:network}
  \vspace{-4pt}  
\end{figure*}


\noindent\textbf{Object Motion Control.} Previous methods primarily focus on motion control in 2D space. MotionCtrl **Ji et al., "MotionCtrl"**, DragNUWA **Li et al., "DragNUWA"**, and Tora **Kanazawa et al., "Tora"** represent object motion trajectories as sequences of spatial positions, encoding coordinates into dense control maps. Additionally, 2D bounding boxes have been adopted as control signals to enable flexible motion generation in Direct-A-Video **Chen et al., "Direct-A-Video"** and Boximator **Wu et al., "Boximator"**. Motion control through sketches has also been explored in VideoComposer **Liu et al., "VideoComposer"**. 
While these methods have demonstrated capabilities in object motion control, their control signals limit the controllability only in 2D space. 3DTrajMaster **Wang et al., "3DTrajMaster"** is the first to use 6D pose sequences of objects to control object motion in 3D space.




\noindent\textbf{Camera Motion Control.} Camera pose serves as a crucial control signal in video generation, determining which portions of the scene are captured and presented in the final output. MotionCtrl **Ji et al., "MotionCtrl"** pioneers the integration of camera poses as control signals for camera movement manipulation. Building upon this foundation, CameraCtrl **Li et al., "CameraCtrl"** introduces the use of Pl√ºcker embeddings of camera poses to enhance motion controllability. These methods are all trained on an indoor dataset RealEstate10K **Hossain and Little, "RealEstate10K: A Large-Scale Dataset for Camera-Aware Video-to-Video Translation"** for learning camera motion which limits the ability to generalize to in-the-wild scenes. Direct-A-Video **Chen et al., "Direct-A-Video"** uses data augmentations on static videos to simulate basic camera movements~(only pan and zoom movements) and employs Fourier embedder and temporal cross-attention layers to inject camera poses. It cannot generalize to more complex camera movements such as ``Anti-clockwise". Therefore, the development of this field is constrained by the scarcity of large-scale in-the-wild datasets with camera pose annotations.



\noindent\textbf{Joint Motion Control.} Based on the preliminary explorations of joint motion control **Ji et al., "Joint Motion Control"**, some concurrent works further advance this field. Motion Prompting **Liu et al., "Motion Prompting"** leverages 2D point tracking results to represent object motion and camera motion. Perception-as-Control **Wang et al., "Perception-as-Control"** and DaS **Zhang et al., "DaS"** further capture 3D point tracking results by SpatialTracker **Li et al., "SpatialTracker"** to extend the joint control into 3D space. The former denotes objects of reference image as unit spheres with different colors. The latter directly uses the point tracking video as the motion condition. MotionCanvas **Chen et al., "MotionCanvas"**
also measures camera motion by point tracking and renders 2D instance box map as object global motion representation. However, these methods are all designed for the image-to-video generation task which only animates an initial image and cannot plan a shooting in 3D space from scratch. SynFMC **Wang et al., "SynFMC"** uses Unreal Engine to render both 6D pose of objects and camera to construct video datasets for joint motion control, but the limited diversity and the domain gap of UE data restrict the model's generalizability. To overcome the scarcity of in-the-wild datasets with both 3D object motion and camera pose annotations, we carefully establish an automated data annotation pipeline to extract 3D bounding boxes and camera trajectories from large-scale video data for learning joint motion control. In addition, we provide an interactive workflow to allow users to intuitively manipulate objects and camera in 3D scene.