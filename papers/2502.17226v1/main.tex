\documentclass[journal]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
    
\usepackage{amsmath}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{hyperref}
\usepackage{tikz}
\usepackage{float}
\def\checkmark{\tikz\fill[scale=0.4](0,.35) -- (.25,0) -- (1,.7) -- (.25,.15) -- cycle;} 
\usepackage{graphicx}
\usepackage{xspace}
\DeclareMathOperator*{\minimize}{minimize}
\usepackage{multirow}
\hyphenation{op-tical net-works semi-conduc-tor}

\usepackage{caption}
\captionsetup[subfigure]{width=0.9\linewidth}
\usepackage{subcaption}
\usepackage[compact]{titlesec}  
\titlespacing{\section}{2pt}{2pt}{2pt}
\DeclareMathOperator*{\argmin}{arg\,min}
%\usepackage{algorithmicx}
\usepackage{amsthm}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{fact}{Fact}
\newtheorem{definition}{Definition}
\newtheorem{proposition}{Proposition}
\newtheorem{corollary}{Corollary}
\newtheorem{example}{Example}
\newtheorem{remark}{Remark}
\newtheorem{claim}{Claim}
\newtheorem{observation}{Observation}

\newtheorem{assumption}{Assumption}
%\usepackage{fancyhdr}
\usepackage{ upgreek }
\usepackage{soul}
% Define a new column type for centered check marks
%\newcolumntype{C}[1]{>{\centering\arraybackslash}p{#1}}
\begin{document}


\title{

Electrical Load Forecasting over Multihop Smart Metering Networks with Federated Learning 
}
% \author{
%         \IEEEauthorblockN{
%         Ratun Rahman\IEEEauthorrefmark{2}, Shaba Shaon\IEEEauthorrefmark{2} and Dinh C. Nguyen\IEEEauthorrefmark{2}
% 	}

% 	\IEEEauthorblockA{%\IEEEauthorrefmark{1}School of Engineering, Deakin University, Australia \\
% 	\IEEEauthorrefmark{2}Department of Electrical and Computer Engineering, University of Alabama in Huntsville, AL, USA \\
%  %\IEEEauthorrefmark{3}Pacific Northwest National Laboratory, Richland, WA, USA
% 	}
 
%  Emails: rr0110@uah.edu, ss0670@uah.edu, dinh.nguyen@uah.edu}
% 	\markboth{}%
% 	{}

\author{\IEEEauthorblockN{Ratun Rahman,  Pablo Moriano,~\IEEEmembership{Senior Member,~IEEE,}  Samee U. Khan, and Dinh C. Nguyen}
\thanks{* Part of this work has been accepted at the IEEE Consumer Communications \& Networking Conference (CCNC), Jan. 2025 \cite{rahman2024electrical}.}
\thanks{Ratun Rahman and Dinh C. Nguyen are with the Department of Electrical and Computer Engineering, University of Alabama in Huntsville, Huntsville, AL 35899 (emails: \{rr0110,  dinh.nguyen\}@uah.edu). }
\thanks{Pablo Moriano is with the Computer Science and Mathematics Division, Oak Ridge National Laboratory, Oak Ridge, TN 37930 (e-mail: moriano@ornl.gov). }
\thanks{Samee U. Khan is with the Department of Electrical and Computer Engineering, Mississippi State University, Mississippi State, MS 39762 (email: skhan@ece.msstate.edu). }

}
\maketitle
\pagenumbering{gobble} 

% \author{\IEEEauthorblockN{Ratun Rahman}
% \IEEEauthorblockA{\textit{Departement of Electrical and Computer Engineering} \\
% \textit{The University of Alabama in Huntsville}\\
% Huntsville, Alabama, USA \\
% email: rr0110@uah.edu}
% \and
% \IEEEauthorblockN{Dinh C. Nguyen}
% \IEEEauthorblockA{\textit{Departement of Electrical and Computer Engineering} \\
% \textit{The University of Alabama in Huntsville}\\
% Huntsville, Alabama, USA \\
% email: dinh.nguyen@uah.edu}
% }
% email: neeraj.kumar@pnnl.gov

% The paper headers
%\markboth{Journal of \LaTeX\ Class Files,~Vol.~14, No.~8, August~2015}%
%{Shell \MakeLowercase{\textit{et al.}}: Bare Demo of IEEEtran.cls for IEEE Journals}

\begin{abstract}
Electric load forecasting is essential for power management and stability in smart grids. This is mainly achieved via advanced metering infrastructure, where smart meters (SMs) record household energy data. Traditional machine learning (ML) methods are often employed for load forecasting but require data sharing which raises data privacy concerns. Federated learning (FL) can address this issue by running distributed ML models at local SMs without data exchange. However, current FL-based approaches struggle to achieve efficient load forecasting due to imbalanced data distribution across heterogeneous SMs. This paper presents a novel personalized federated learning (PFL) method for high-quality load forecasting in metering networks. A meta-learning-based strategy is developed to address data heterogeneity at local SMs in the collaborative training of local load forecasting models. Moreover, to minimize the load forecasting delays in our PFL model, we study a new latency optimization problem based on optimal resource allocation at SMs. A theoretical convergence analysis is also conducted to provide insights into FL design for federated load forecasting. Extensive simulations from real-world datasets show that our method outperforms existing approaches in terms of better load forecasting and reduced operational latency costs. 









% under non-independent and identically distributed (non-IID) metering data settings. Specifically, we introduce meta-learning, where the learning rates are manipulated using the meta-learning idea to maximize the gradient for each SM in each global round. SMs with varying processing capacities, data sizes, and batch sizes can participate in global model aggregation and improve their local load forecasting via personalized learning. \textcolor{black}{Personalized learning in federated settings increases latency due to additional meta-learning optimization steps and the need to aggregate diverse data from heterogeneous SMs. The synchronization of SMs with varying capacities introduces further delays. To deal with this, we formulate a new latency minimization problem for  multi-hop personalized FL system and propose an efficient optimization algorithm using block coordinate descent (BCD) and successive convex approximation (SCA) techniques.} Simulation results show that our approach outperforms state-of-the-art  ML and FL methods in terms of better load forecasting accuracy. \textcolor{black}{We also observe significant latency savings in the  multi-hop personalized FL system, achieving reductions of up to 45.33\% compared to other state-of-the-art schemes.}

% Our suggested approach presents a viable resolution to the problems associated with load prediction, leading to a more precise and dependable prediction.
\end{abstract}

\maketitle

\begin{IEEEkeywords}
Federated learning,  load forecasting, smart meter, latency, smart grid
\end{IEEEkeywords}

\section{Introduction} \label{Sec:Introduction}
% \IEEEPARstart{A}{ccurate} electric-load forecasting can balance the supply and demand of power, reducing electricity waste, a major issue in the current world.
% \textcolor{black}{\cite{6}
% \cite{7}
% \cite{8}}

Electrical load forecasting is crucial for power management in smart grids. This service is mainly supported via advanced metering infrastructure, where smart meters (SMs) record household energy consumption and share this data to the server of utility company\cite{9770488}. This enables utility providers to estimate future electricity demands and thereby bolster grid reliability. Conventional load-forecasting techniques in machine learning (ML) and deep learning (DL) techniques utilize pattern-finding abilities to predict future outcomes. For example, long short-term memory (LSTM) has shown its potential for time-series data-based load forecasting applications \cite{hong2020deep, bouktif2018optimal}. Generally, these methods require that every SM sends energy usage information to the utility company. However, sharing data may reveal customers' sensitive information, such as energy usage routines. In 2009, the compulsory roll-out of SMs in the Netherlands was halted following a court ruling that the metering data collection violated customersâ€™ privacy rights \cite{cuijpers2013smart}. 
% Accurate forecasting is pivotal for ensuring a stable and efficient energy supply, crucially matching load and supply demands. ML techniques, particularly DL models, have gained prominence in addressing this challenge due to their ability to capture complex patterns in energy consumption data. 
Several ML algorithms for load forecasting use a hybrid model that combines LSTM and convolutional neural network (CNN) architectures \cite{farsi2021short}. Moreover, automated DL has been applied to enhance the performance of deep neural networks (DNNs) which is similar to CNN but has custom layers tailored for load forecasting \cite{keisler2024automated}. However, these traditional ML methods require centralized data processing at a data center.  

% typically requires collecting data from SMs and sending it to a centralized data center. This centralized approach raises concerns about user privacy, as it involves transmitting potentially sensitive energy consumption data over networks (e.g., energy usage patterns), where it may be intercepted by data attackers.

Recently, federated learning (FL) has been studied to address this data-sharing problem in load forecasting \cite{fekri2022distributed, taik2020electrical, gholizadeh2022federated}.  The local load forecasting model is trained at SMs using local metering data before sending it to the global server for the next global round. \textcolor{black}{This approach ensures user privacy by keeping raw data localized and preventing the transmission of sensitive information across networks.}  However, these literature works have struggled with addressing data heterogeneity, where they assume that every SM has a dataset with similar data distribution. However, this is not realistic in real-world metering networks, where each SM typically owns a unique metering data distribution due to the nature of \textit{personalized} energy consumption patterns of households.  

To address this problem, we provide a novel load forecasting method for data heterogeneity in real-world metering networks. Our key idea is a new \textit{personalized federated learning (PFL)}-based load forecasting method. PFL handles data over-fitting by creating a customized load forecasting model for every SM. Our PFL technique is based on meta-learning, which helps local models to be trained properly by choosing the best parameters using the trial and run method \cite{9428530}. In this regard, each SMs participates in learning a custom load forecasting model, and they share local model parameters to the utility's server for model aggregation, aiming to build a global load forecasting for the entire network with good generalization \cite{rahman2024multimodal}. 

% The major advantages of meta-learning over other PFL techniques are that it does not add any additional calculations and costs to the server and can easily scale, making the global model aggregation fast and reliable.

% \textcolor{black}{Personalized learning in federated settings introduces latency due to the intricate meta-learning optimization process. This involves adjusting SM-specific learning rates to optimize individual gradient updates, which requires additional computation and coordination. Moreover, aggregating heterogeneous data from diverse SMs adds complexity and extends processing time. The synchronization of SMs with varying computational capacities and data volumes further complicates the process, leading to more delay. To tackle this problem, we formulate a new latency minimization problem for multi-hop metering network for load forecasting that incorporates PFL. This will enable faster communication within multi-hop metering networks along with other advantages of PFL for load forecasting.}

Moreover, introducing FL into distributed load forecasting incurs latency costs due to model training at SMs and model communication between SMs and the utility's server. It is then crucial to minimize the round-trip latency in such an FL-based load forecasting system to ensure timely load forecasting service of the entire metering network.  By addressing the latency issue, load forecasting efficiency and responsiveness can be enhanced for reliable smart grids. This will enhance the system's overall performance, allowing for more accurate and timely predictions, which is essential for effective energy management and distribution. \textit{This motivates us to jointly consider learning and latency optimization design for load forecasting to achieve optimal performance in terms of better accuracy and minimal delays of load forecasting.}

% The combination of PFL and latency minimization will thus provide a robust framework for smart grid applications, ensuring that households receive the benefits of advanced load forecasting while maintaining data privacy and reducing delays.

% This involves adjusting SM-specific learning rates to optimize individual gradient updates, which requires additional computation and coordination. Moreover, aggregating heterogeneous data from diverse SMs adds complexity and extends processing time. The synchronization of SMs with varying computational capacities and data volumes further complicates the process, leading to more delay. Moreover, FL incurs additional latency due to the training process and model communication. In FL, model transmission can only occur over communication channels, which inevitably makes the situation worse in terms of latency. 


% To tackle this problem, we formulate a new latency minimization problem for a multi-hop metering network specifically designed for load forecasting that incorporates PFL. This approach aims to enable faster communication within multi-hop metering networks, leveraging the benefits of PFL for load forecasting. Furthermore, the implementation of FL offers on-device load forecasting services at households without the need for data sharing, ensuring data privacy and security. However, 

\subsection{Related Work}
Various approaches were proposed for load forecasting in smart grids. The works in \cite{rafi2021short, bouktif2018optimal} proposed an LSTM-based model to estimate electrical load demands in smart grids; however, this technique requires data sharing, and hence sensitive user information such as energy consumption patterns may be exposed to third parties. In addition, they also require significant energy consumption \cite{jawad2018robust, ali2017ancillary}. The authors in \cite{taik2020electrical, briggs2022federated} demonstrated that using FL could further improve accuracy without compromising data privacy. Furthermore, it was also able to reduce significant networking load. \textcolor{black}{Another study in \cite{fekri2022distributed} introduced FedAVG, which performed better than other FL techniques like FedSGD. However, they struggle with training non-independent and identically distributed (IID) data where data distributions are heterogeneous across SMs}. Recently, PFL techniques have been considered to tackle the data heterogeneity issue in load forecasting. The study in \cite{9770488} proposed a PFL technique for load forecasting where each SM customizes a federated prediction model. Another work in \cite{10233242} introduced a Generative Adversarial Network (GAN) based differential privacy (DP) algorithm that included multi-task PFL. However, this solution increases computational complexity at the server in the load forecasting process. 

% \textit{However, this solution increases computational complexity at the server and reduces the scalability of the load-forecasting network.}
% But that creates an energy imbalance where the SM that will control the group will require significant data processing abilities than other SMs. However, this solution increases computational complexity at the server and reduces the scalability of the load-forecasting network.

\begin{table}
\footnotesize
\centering
\caption{Comparison of our approach with existing load forecasting methods.}
\begin{tabular}{|p{2cm}||p{0.6cm}|p{0.6cm}|p{0.6cm}|p{0.6cm}|p{0.6cm}|p{0.6cm}|}
 \hline
 Objectives & \cite{rafi2021short, bouktif2018optimal} &  \cite{taik2020electrical, fekri2022distributed, briggs2022federated} & \cite{9770488} & \cite{10233242} & \cite{rahman2024electrical} & Our Approach\\
 \hline
 Handle uncertain and non-iid data & & & \checkmark & \checkmark & \checkmark&\checkmark\\
 \hline
 Include diverse SMs & \checkmark & & \checkmark & \checkmark & \checkmark & \checkmark\\
 \hline
 Adaptability to user change & \checkmark & \checkmark &  & \checkmark & \checkmark& \checkmark\\
 \hline
 Handle large dataset & & \checkmark & \checkmark & \checkmark & \checkmark& \checkmark\\
 \hline
 Maintain server complexity & & \checkmark & \checkmark & & \checkmark & \checkmark\\
 \hline
 Keeping data secured & & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark\\
 \hline
 Latency minimization & & & & & & \checkmark\\
 \hline
 Practical multi-hop settings & & & & & & \checkmark\\
 \hline
 Convergence Analysis & & & & & &\checkmark\\
 \hline
\end{tabular}
\label{table:related_work_table}
\vspace{-5mm}
\end{table}

Moreover, several studies have concentrated on communication in smart grids and smart metering networks. The authors in \cite{kabalci2016survey} and \cite{barai2015smart} explained a smart grid, introduced its components, and presented the communication methods used, highlighting their advantages and shortcomings. It also surveyed smart grid integration, classified communication technologies, and outlined hardware and software security requirements. The work in \cite{kabalci2022design} proposed a smart metering infrastructure with DC and AC analog front ends and communication interfaces, and remote monitoring software for accurate and efficient measurement and transmission in microgrid and smart home applications. 
This work introduced a reconfigurable authenticated key exchange scheme using reconfigurable physical uncloneable functions (PUFs) for secure and efficient smart grid communication, offering advantages in computation and communication costs over current protocols.

Many studies have focused on improving the latency of  FL and addressed FL in multi-hop networks. For instance, \cite{mohasen2022federated} optimized model aggregation, routing, and spectrum allocation, while \cite{chen2022federated} introduced FedAir to mitigate communication impacts on FL performance. \cite{pinyoanuntapong2020fedair} used hierarchical FL with adaptive grouping, \cite{nguyen2022toward} aimed to reduce congestion by predicting future network topologies, and \cite{cash2023wip} examined jamming attacks on decentralized FL. Despite these efforts, latency minimization for  FL in multi-hop networks remains unaddressed. Single-hop networks often fail over large areas due to limited transmit power, whereas multi-hop networks provide better communication, coverage, and flexibility. Research on FL in multi-hop networks has focused on mesh networks. Still, it is crucial to consider scenarios with no direct links between non-consecutive nodes for worst-case analysis. \textcolor{black}{Our method is based on a joint design of a new PFL algorithm for collaborative load forecasting and a latency optimization solution for minimizing load forecasting delays in a multi-hop network setting}. We compare our approach with related works in Table~\ref{table:related_work_table}.
%\hfill \break

\subsection{Our Key Contributions} Motivated by the above limitations, \textit{we propose a novel load forecasting approach over metering networks in the anonymous grid.} 
% new PFL method for high-quality load forecasting over SMs in the smart grid}. Specifically, inspired by gradient compression methods in \cite{karimireddy2019error, vogels2019powersgd}, we incorporate different gradients to build a more personalized local load forecasting model at each SM. This is more widely known as \textit{meta learning} which introduces the 'learning to learn' concept for PFL \cite{fallah2020personalized}. 
Our key contributions are summarized as follows:
\begin{itemize}
\item We propose a new PFL approach called personalized meta-LSTM algorithm with a flexible SM participation method for collaborative load forecasting in the smart grid. This allows complicated and diverse data to be structured, assembled, and processed quickly, removing data sharing to protect the privacy and security of household electricity recordings.  
\item We develop a personalized learning approach for local load forecasting in SMs based on meta-learning. Prior to training the local model, the clients are temporarily evaluated using varying learning rates. The most suitable learning rate is then selected among the available learning rates based on which one yields the lowest loss value. Next, we train local models with the optimal learning rate.
% \item We remove any additional calculation for PFL on the utility (global) server, reducing processing costs.
\item We propose a new latency optimization method to minimize the load forecasting delays caused by introducing PFL into the metering networks. The key objective is to find optimal resource allocation strategies for SMs, including transmit power and computational frequency, to optimize the round-trip PFL delay, achieved by an efficient convex optimization solution. 
\item We carry out extensive simulations on real-world datasets under both IID and non-IID data settings, indicating that our approach outperforms existing works regarding better load forecasting and reduced operational latency costs. Theoretical convergence analysis is also conducted to give insights into FL design for federated load forecasting. 
% \item Moreover, we formulate a new latency minimization problem for the  FL over the multi-hop network, considering the cooperation of leaf and relay nodes.
% \item We propose an efficient optimization solution using block coordinate descent (BCD) and successive convex approximation (SCA) techniques to tackle the challenging computational problem.
% \item We conduct extensive simulations demonstrating the superior performance of the proposed method compared to baseline approaches.
\end{itemize}

\subsection{Paper Organization}
The rest of the paper is structured as follows. In Section \ref{Sec:SystemModel}, we present our system model, detailing the architecture and components of our proposed system. Section 
 \ref{Sec:PFLAlgorithmDesignforLoadForecasting} discusses the PFL algorithm design for load forecasting. Section 
 \ref{Sec:LatencyAnalysisForPFL-basedLoad Forecasting} presents the latency analysis of the PFL-based load forecasting system. We evaluate our simulation results and performance evaluations in Section 
 \ref{Sec:SimulationsandPerformanceEvaluation}. We present an in-depth analysis of the outcomes, comparing our proposed solutions to existing methods. Finally, Section  \ref{Sec:Conslusion} concludes the paper. 
% We also outline potential areas for future work and suggest ways to further enhance the effectiveness of our proposed model.} 
% The rest of the paper is as follows. We present our system model in Section 2. In Section 3, we discuss the problem formulation. We evaluate our simulation results and performance evaluation in Section 4. Section 5 concludes the paper. 

\section{System Model}
\label{Sec:SystemModel}
% In this section, we describe our approach by explaining our algorithm and details about the methods used.

\subsection{Overall System Architecture}
\begin{figure}[!t]
\centering
\includegraphics[width=3.2in]{Images/overview.pdf}
\caption{Our proposed architecture for federated load forecasting in the multihop metering network. The SMs network is divided into different routes, each with a sub-set of SMs in a multi-hop topology. Each SM will train a custom load forecasting model and share the trained model with the utility's server for aggregation.}
\vspace{-1mm}
\label{Fig: Overview}
\vspace{-5mm}
\end{figure}

Fig.~\ref{Fig: Overview} illustrates the overall system for load forecasting over SMs. Inspired by the system model in \cite{miao2020evolutionary}, we consider a multi-hop metering network, where SMs are connected under a multi-hop topology in the wireless cellular network. Specifically, there are different routes and multiple SMs as relays on each route. Each SM trains a local load forecasting model and shares it with a utility server. The server is considered a global server where global model aggregation is performed based on a shared local model for load prediction. Each SM  denoted as $n \in \mathcal{N}$,  records household energy consumption data. Energy recordings are time-varying, and differ over SMs. \textcolor{black}{We denote every global round as $k \in \mathcal{K}$ where $k = \{1, 2, 3, . . . , K\}$ and $K$ is the final global round. In each round $k$, each SM $n$ holds a local dataset $D_n^{(k)}$ that varies in every round and client, with size $|D_n^{(k)}|$. The SMs employ these datasets for global training in round $k$ and the total dataset is $D^{(k)} = \sum_{n\in \mathcal{N}} D_n^{(k)}$.}

\textcolor{black}{The goal of the system is to train each local model $\theta_n^{(k)}$ effectively such that the global model $\theta_g^{(k+1)}$, created by their aggregation, can provide better results. To create a local model, each SM requires a gradient parameter denoted by $\nabla F$ with a learning rate $\alpha$ to fasten or slow the learning process. For our personalized approach, we have a series of learning rates ($\alpha_1,\alpha_2,\dots$) and we calculate the loss value for every learning rate for each SM $n$ in each round $k$ in a small dataset as Dataloader denoted by $D^{temp}$. The learning rate that provides the lowest loss value is the optimal learning rate $(\alpha^{\text{best}}_{n,k})$. Then we use $\alpha^{\text{best}}_{n,k}$ for the local model training and send the local weight to the server for global model aggregation. Then the global model is updated for the next training round $k+1$ using federated averaging.}

%\textcolor{red}{We denote every global round as $i \in N$ where $N$ is the final global round. For every round $i$, SM $m$ trains a local load forecasting model using its local dataset denoted by $D_i^m$. Hence, the total data for round $i$ can be expressed as $D_i^{\mathcal{M}} = \sum_{m=1}^{\mathcal{M}} D_i^m$. The next step is to create a local model $\theta^m$ where $m \in \mathcal{M}$ uses the data in every local SM $m$. In order to build a model, we require a gradient parameter (denoted by $\nabla$) that utilizes loss function, and optimization and combines it with learning rate to fasten or slow the learning process. In our approach, we have different learning rates for each SM $m$ in every global round $i$ to compute a gradient $\nabla_i^m$. We try to select the best learning rate that provides the optimal result in $\nabla_i^m$.}

%\textcolor{red}{The final goal is to reduce the global loss by optimizing the weight of the global model denoted as $\boldsymbol{w}^*$. To achieve that, in every global round $i$, the global model's weight $\boldsymbol{w}_i$ is distributed among all the local SMs. After obtaining $\boldsymbol{w}_i$, every SM updates the weight using their gradients ($\nabla_i^m$). To manipulate the changes. For our approach, we have a series of learning rates ($\alpha_1,\alpha_2,\dots$) and we go through some initial steps for every learning rate to find the optimal learning rate for that SM $m$ on that global round $i$. Let's assume $\alpha^{\text{best}}_i^m$ is the optimal learning rate for global round $i$ and SM $m$. So every SM will update the global weight $\boldsymbol{w}_i$ with their $\nabla_i^m$ and $\alpha^{\text{best}}_i^m$ as their own local weight $\boldsymbol{w}_i^m$. Then they return the weight to the server for model aggregation and update global model $\boldsymbol{w}_{i+1}$ for the next round ($i+1$).}

% In our approach, we have used 80\% of our data for local model training and expressed them as Trainloader $(D^{train})$ and the rest 20\% for testing as Testloader $(D^{test})$. To evaluate the initial performance for meta-learning, we have used 20\% of the training data expressed as Dataloader $(D^{\text{temp}})$ and used that to find $\alpha^{\text{best}}_{n,k}$.

\subsection{Objective Function}
Our proposed approach aims to achieve learning personalization for local load forecasting at SMs. In doing so, it is important to find an optimal learning rate for the local ML model, which is obtained by minimizing the  objective function:
\begin{equation} 
    F_{avg} = \frac{1}{K} \sum_{k=1}^{K} \mathcal{L}(y^k,\hat y^k),
\end{equation}
where $\mathcal{L}$ is a loss function. $y^k$ represents the actual value, and $\hat y^k$ is the predicted value for the $k^{th}$ task. 

% Load forecasting is a regression-based task. As a result, we train the model to minimize the mean absolute error (MAE) given by:
% \begin{equation}
%     \mathcal{L}(y^k,\hat y^k) = \frac{1}{n} \sum_{i=1}^{N} |y_i - \hat{y}_i|.
% \end{equation}

For example, we calculate root mean squared error (RMSE) using the equation below
\begin{equation}
    \mathcal{L}(y^k,\hat y^k) = \sqrt{\frac{1}{N} \sum_{i=1}^N (y_i^k-\hat y_i^k)^2}.
\end{equation}

\subsection{LSTM}
LSTM is usually used in time sequences and long-range dependencies datasets. To forecast future values based on past data, load prediction usually involves finding patterns and trends across time.  LSTM is a form of recurrent neural network (RNN) architecture consisting of unique units or memory cells designed to retain their state over time and regulate the processing, processing, and storage of information. \textcolor{black}{As a result, LSTM can handle long-term dependency problems better than RNN which is crucial for load forecasting}. Each LSTM unit has three different gates that facilitate input, forget, and output gates. It also has two other components: cell state $c_j$ represents internal memory and hidden state $h_j$ represents the output of the LSTM unit at time step $j$. In this work, we employ an LSTM model at each SM for local load forecasting. At each training step $j$, the proposed LSTM model operates through the following key stages.
% Fig. \ref{Fig: lstm} describes the basic LSTM architecture. 
% \begin{figure}[!t]
% \centering
% \includegraphics[width=3.4in]{Images/lstm.pdf}
% \vspace{-5mm}
% \caption{LSTM model architechture.}
% \label{Fig: lstm}
% \vspace{-5mm}
% \end{figure}
\textcolor{black}{
\begin{enumerate}
    \item The data is extracted from the cell state determined by the forget gate ($f_j$).
    \begin{equation}
        f_j = \sigma\!\left(W_f \begin{pmatrix} h_{j-1} \\ x_j \end{pmatrix} + b_f\right),
    \end{equation}
    where $\sigma$ is the sigmoid activation function, $W_f$ and $b_f$ are the weight matrix and bias for the forget gate $f_j$, $h_{j-1}$ is the hidden state from the previous time step, and $x_j$ is the input at the current time step.
    \item The input gate ($i_j$) determines additional data that must be added to the cell state.
    \begin{equation}
        i_j = \sigma\!\left(W_i \begin{pmatrix} h_{j-1} \\ x_j \end{pmatrix} + b_i\right).
    \end{equation}
    \item A fresh candidate value to be added to the cell state is provided by the candidate cell state $\tilde C_j$.
    \begin{equation}
        \tilde{C}_j = \tanh\!\left(W_c \begin{pmatrix} h_{j-1} \\ x_j \end{pmatrix} + b_c\right),
    \end{equation}
    where $tanh$ is the hyperbolic tangent activation function and $W_c$ and $b_c$ are the weight matrix and bias for the candidate cell state $\tilde C_j$. 
    \item Cell state update combines the old cell state, forget gate output, input gate output, and candidate cell state to change the cell state.
    \begin{equation}
        C_j = f_j \odot C_{j-1} + i_j \odot \tilde{C}_j,
    \end{equation}
    where $C_{j-1}$ is the cell state from the previous time step. 
    \item From the current cell state output gate $o_j$ decides what information to output.
    \begin{equation}
        o_j = \sigma\!\left(W_o \begin{pmatrix} h_{j-1} \\ x_j \end{pmatrix} + b_o\right).
    \end{equation}
    \item The hidden state $h_j$ is updated for the current time step.
    \begin{equation}
        h_j = o_j \odot \tanh(C_j).
    \end{equation}
\end{enumerate}
    }

\section{PFL Algorithm Design for Load Forecasting} \label{Sec:PFLAlgorithmDesignforLoadForecasting}
Fig. \ref{Fig: Overview} depicts the multihop load forecasting framework using PFL, where a centralized server is connected to relay and leaf nodes. Each node goes through a local training process before sending its local model to its parent node. We can separate our system model into multiple steps as follows.

\textbf{Step 1:} 
We assume there are $N$ SMs and an initial parameter of the global model $\boldsymbol{w}_0$. A generalized FL with a single server for global round $k \in \mathcal{K}$ can be explained as

\begin{equation}
\min_{\boldsymbol{w} \in {\rm I\!R}^d} F_k(\boldsymbol{w_k}) := \frac{1}{N} \sum_{n=1}^{N} f_{n,k}(\boldsymbol{w}_n),
\end{equation}
where the function $f_i: \rm I\!R^d \longrightarrow \rm I\!R, n\in \mathcal{N}=\{1,2,3,\dots,N\}$ denotes the predicted loss value over $m^{th}$ SM's data distribution:
\begin{equation}\label{supervised_ML}
f_{i,k}(\boldsymbol{w}_n) := {\rm I\!E}_{\xi_i} \left [ f_{n,k}^{'} (\boldsymbol{w}_n,x_n) \right ].
\end{equation}
Here, ${f_{i,k}}^{'} (\boldsymbol{w}_n,x_n)$ is a loss function calculating the difference between data sample $x_n$ and its corresponding using $\boldsymbol{w}_n$ at round $k$.

\textbf{Step 2:} 
Assume that local training iteration index is denoted as $j \in \mathcal{J}$,  where $j = \{1, 2, 3, . . . , J\}$, the the local update at SM $n$ using LSTM is expressed as: 
\begin{equation} \label{eq: sgd}
\boldsymbol{w}_{n,k}^{j+1} = \boldsymbol{w}_{n,k}^{j} - \alpha_k \nabla F(\boldsymbol{w}_{n,k}^j,\chi_{n,k}^j),
\end{equation}
where $\alpha >0$ is the local learning rate, and $\chi$ is the non-IID sample from the local dataset. However, the learning rate $\alpha_k$ is not constant in our approach. We use optimal and personalized learning rate (denoted as $\alpha^{\text{best}}_{n,k}$) instead for SM $n$ and round $k$. 

\textbf{Step 3:}
To calculate $\alpha^{\text{best}}_{n,k}$, in each round $k$ for every SM $n$, we apply a group of available learning rates given as $\alpha_j$ on Dataloader $(D_{n,k}^{temp})$ where $j$ is the total number of available learning rates to calculate the loss value. The loss value at $i \in j$ is calculated as:
\begin{equation}
    f_{i,k}^{'} (\alpha_i) = f_{i,k}((\boldsymbol{w}_{n,k},\alpha_i), D_{n,k}^{temp}).
\end{equation}
Then we select the $\alpha_i$ as $\alpha^{\text{best}}_{n,k}$ that produce the minimum $f_i^{'}$ value. So, we can explain that as:
\begin{equation}
    \alpha^{\text{best}}_{n,k} := \argmin_{i \in j} (f_{i,k}^{'}(\alpha_i)).
\end{equation}

\textbf{Step 4:}
We calculate the local model training in each SM $n$ (a leaf or relay SM) using equation \ref{eq: sgd}. So after receiving the parameter of the global model $\boldsymbol{w}_{n,k}$ every $n$ updates its personalized model by using the optimized learning rate $\alpha^{\text{best}}_{n,k}$. So, we can express equation \ref{eq: sgd} as:
\begin{equation}
\boldsymbol{w}_{n,k}^{j+1} = \boldsymbol{w}_{n,k}^{j} - \alpha^{\text{best}}_{n,k} \nabla F(\boldsymbol{w}_{n,k}^j,\chi_{n,k}^j).
\end{equation} 

\textbf{Step 5:}
After $J$ rounds, each $n$ then sends its local model's weight $(\boldsymbol{w}_{n,k} = \boldsymbol{w}_{n,k}^J)$ to its parent node. If the node is not a leaf node, the current node relays its local and child models to the parent node. The weights eventually arrive in the server when the nodes are aggregated.

\textbf{Step 6:}
Once the server collects all the node's weight, it calculates the federated averaging for the next global round $k+1$ as:
\begin{equation}
    \boldsymbol{w}_{k+1} = \frac{1}{N} \sum_{n\in\mathcal{N}} \boldsymbol{w}_{n,k}. 
\end{equation}

\textbf{Step 7:}
We calculate the loss value for the updated weight on testloader $(D^{test})$ and then broadcast the updated weight to all the SMs. We do this loop for $K$ times and get the optimal global load forecasting model $w^*$. 

% In our approach, we try to regularize the loss function based on the performance of SMs. So, our approach is:
% \begin{equation}
% \min_{\boldsymbol{w} \in {\rm I\!R}^d} F^t(\boldsymbol{w}) := \frac{1}{\mathcal{M}} \sum_{i=1}^{\mathcal{M}} f^t_i(\boldsymbol{w}),
% \end{equation}
% where $F^t_i(\boldsymbol{w}) = \min_{\boldsymbol{w} \in {\rm I\!R}^d} \left\{ f^t_i(\boldsymbol{w}, x_i) \right\}$.  
% % % \begin{equation}
% % %     F_i(\boldsymbol{w}) = \min_{\boldsymbol{w} \in {\rm I\!R}^d} \left\{ f_i(\theta_i) + \textbf{x}_i \right\}.
% % % \end{equation}
% The optimal personalized model of this loss function $F^t_i(\boldsymbol{w})$ is defined as:
% \begin{equation}
%     \hat{\boldsymbol{w}^t_i} = \text{prox}_{f^t_i / x_i}(\boldsymbol{w}),
% \end{equation}
% \begin{equation}
%     \hat{\boldsymbol{w}_i} = \arg \min_{\theta \in  {\rm I\!R}^d} \left\{ f_i(\boldsymbol{w}) + x_i \right\}. 
% \end{equation}
% To update the model $\boldsymbol{w}_i^m$ for each SM $m$ at global round $i$, we employ a stochastic gradient descent (SGD)-based approach, such that: 
% \begin{equation}
%    \boldsymbol{w}_i^m = \boldsymbol{w}_i - \alpha^{\text{best}}_{i} \nabla_i^m f_i(\boldsymbol{w}_i^m) 
% \end{equation}
% \begin{equation}
%     \boldsymbol{w}^t_i = \arg\min_{\theta \in {\rm I\!R}^d} \left\{ (\nabla f^t_i(\boldsymbol{w}^t), x_i - \boldsymbol{w}^t ) + \frac{1}{2\alpha^{\text{best}}_i^t} \| x_i - \boldsymbol{w}^t \|^2 \right\}.
% \end{equation}

% Here, $x_i$ is the personalized term in PFL and $\nabla f^t(\boldsymbol{w}^t)$ represents the gradient of the function $f$ at point $\boldsymbol{w}$ and $\alpha^{\text{best}}_i^t$ is the optimal learning rate is $t^{th}$ global round. So the objective is to minimize the squared norm of the gradient of a function $f(\boldsymbol{w})$.

% Let us assume that $G(\boldsymbol{w})$ is a global loss function created by the aggregation of the local loss functions $f_i(\boldsymbol{w})$ expressed in the previous equation as $G(\boldsymbol{w}) = \nabla f(\boldsymbol{w}) $. So we can express $G(\boldsymbol{w})$ as:
% % \begin{equation}
% %     G(\boldsymbol{w}) = \left( \frac{\partial f}{\partial \boldsymbol{w}_1} \right)^2 + \left( \frac{\partial f}{\partial \boldsymbol{w}_2} \right)^2 + ... + \left( \frac{\partial f}{\partial \boldsymbol{w}_\mathcal{M}} \right)^2, 
% % \end{equation}
% % which is equivalent to
% % \begin{equation}
% %     G(\boldsymbol{w}) = \sum_{i=1}^{\mathcal{M}} \left( \frac{\partial f}{\partial \boldsymbol{w}_i} \right)^2
% % \end{equation}
% \begin{equation}
%     G(\boldsymbol{w}) = \sum_{i=1}^{\mathcal{M}} \boldsymbol{\alpha^{\text{best}}}_i \nabla f_i(\boldsymbol{w}),
% \end{equation}
%  which is optimized over iterative global rounds.
% % So, the objective of the optimization is:
% % \begin{equation}
% %     \text{minimize} \quad G(\boldsymbol{w}).
% % \end{equation}
% % \textcolor{black}{check and apply $m$ for this section. Then say: model update procedure is also applied to relay nodes $n$ ($\forall n \in \mathcal{N}$.}
% % \vspace{-2pt}
% % \textcolor{black}{\subsection{Problem formulation for latency minimization}}
\begin{algorithm}
\footnotesize
	\caption{{Proposed PFL algorithm for high-quality load forecasting across SMs}}
	\begin{algorithmic}[1]
		\label{algo: metaSGD}
		\STATE \textbf{Input:}  The set of global communication rounds $\mathcal{K}$, local training round $\mathcal{J}$, a set of SMs $\mathcal{N}$
		\STATE \textbf{Initialization:} Initialize global model $\boldsymbol{w}_0$, different learning rates $\alpha_{0,1,\dots,j}$
		\FOR{each global communication round $k \in \mathcal{K}$}
		\STATE Send $\boldsymbol{w}_k$ to sampled SMs
		\FOR{each sampled SM $n \in \mathcal{N}$ in parallel}
		\FOR{each local training epoch $j \in \mathcal{J}$}
        \STATE Get $\boldsymbol{w}_k$ 
        \FOR{each learning rates $\alpha_i$ where $i \in j$}
        \STATE Calculate $f_{i,k}^{'} (\alpha_i) = f_{i,k}((\boldsymbol{w}_{n,k}^j,\alpha_i), D_{n,k}^{temp})$ on $D_{n,k}^{temp})$
        \STATE Save the best learning rate as $(\alpha^{\text{best}}_{n,k})$ that has the lowest $f_{i,k}^{'}$
        \STATE Return $\alpha^{\text{best}}_{n,k}$ to the local model $\theta_n$
        \ENDFOR
        \STATE Perform local model training (meta-learning) on $\theta_i$, $\boldsymbol{w}_{n,k}^{j+1} = \boldsymbol{w}_{n,k}^{j} - \alpha^{\text{best}}_{n,k} \nabla F(\boldsymbol{w}_{n,k}^j,D_{n,k}^{train})$
		\ENDFOR
        \STATE Send $\boldsymbol{w}_{n,k}$ to the server
		\ENDFOR
		\STATE The utility's server updates the global parameter by averaging: $\boldsymbol{w}_{k+1} = \frac{1}{N} \sum_{n\in\mathcal{N}}\boldsymbol{w}_{n,k}$ 
        \STATE Perform test on the updated weight $\boldsymbol{w}_{k+1}$
		\STATE The utility's server  broadcasts the aggregated global model $\boldsymbol{w}_{k+1}$ to all participating SMs for the next round of training
		\ENDFOR
        \STATE \textbf{Output:} Optimal global load forecasting model $\boldsymbol{w}^*$
  \end{algorithmic}
\end{algorithm}
The proposed PFL approach is summarized in Algorithm~\ref{algo: metaSGD}. For each global round $k$, after SM obtains the initial global weight from the utility server (line 7), they perform meta-learning functionalities in lines 8-12. The loss value is calculated for every available learning rate $\alpha_j$ (line 9). The $\alpha_j$ that produces the lowest loss value is then returned to the local model as the optimal learning rate in line 11. Then in line 13, we perform the local model training and send the updated local model to the server (line 15) after $T$ rounds of local rounds. Then the server does federated averaging on line 17, testing on line 18, and then saves the updated weight for the next global round. Finally after $K$ global rounds, we get our optimal global model $\boldsymbol{w}^*$. This approach allows us to use different learning rates that enable SMs with suitable datasets and better performance to use a different learning rate than those with poor performance and datasets. As a result, the global model is impacted separately for every SM and the global model can have more accurate updates by using the personalized factor.

\section{Convergence Analysis}
In our proposed framework, SMs exchange ML models, where the global model aggregation is executed. From this observation, we focus on analyzing the convergence properties of the federated model training. To support our convergence analysis, we introduce a virtual variable as $\bar{\boldsymbol{w}}_k^j = \frac{1}{N} \sum_{n\in\mathcal{N}} \boldsymbol{w}_{n,k}^j $, where $k \in \mathcal{K}$ denotes the global round. Accordingly, we also define $g_k^j = \frac{1}{N} \sum_{n\in\mathcal{N}} \nabla F_n(\boldsymbol{w}_{n,k}^j,\chi_{n,k}^j)$, and $h_k^j = \frac{1}{N} \sum_{n\in\mathcal{N}} \nabla F_n({x}_{n,k}^j,\zeta_{n,k}^j)$. It is easy to observe that $\bar{\boldsymbol{w}}_k^{j+1}  = \bar{\boldsymbol{w}}_k^j - \alpha_kg_k^j + \bar{\textbf{v}}_k^j$, and $\mathbb{E}g_k = \bar{g}_k$, $\mathbb{E}h_k = \bar{h}_k$, where $\mathbb{E}$ represents function's expectation. Before analyzing the convergence, we make the following common assumptions:
 \begin{assumption}
 Each local loss function $F_n$ ($n\in \mathcal{N}$) is $L$-smooth, i.e., $F_n(\boldsymbol{w}') - F_n(\boldsymbol{w}) \leq \langle \boldsymbol{w}'- \boldsymbol{w}, \nabla F_n(\boldsymbol{w}) \rangle + \frac{L}{2} ||\boldsymbol{w}'- \boldsymbol{w}||, \forall \boldsymbol{w}', \boldsymbol{w}$.
 \end{assumption}
 \begin{assumption}
 Each local loss function $F_n$ ($n\in \mathcal{N}$) is $\mu$-strongly convex, i.e., $F_n(\boldsymbol{w}') - F_n(\boldsymbol{w}) \ge \langle \boldsymbol{w}'- \boldsymbol{w}, \nabla F_n(\boldsymbol{w}) \rangle + \frac{\mu}{2} ||\boldsymbol{w}'- \boldsymbol{w}||, \forall \boldsymbol{w}', \boldsymbol{w}$.
 \end{assumption}
 \begin{assumption} \label{Assump:Variance-gradient}
 The variance of stochastic gradients on local model training at each SM is bounded: $\mathbb{E}||\nabla F_n(\boldsymbol{w}_{n,k}^j,\chi_{n,k}^j) - \nabla F_n(\boldsymbol{w}_{n,k}^j)||^2 \leq \sigma_r^2$. 
 \end{assumption}
We next introduce several lemmas employed in the main result of Theorem~\ref{theorem_covergence}.
\begin{lemma}
Let Assumption \ref{Assump:Variance-gradient} hold, the expected upper bound of the variance of the stochastic gradient on local model training is given as $\mathbb{E} ||g_k^j- \bar{g}_k^j||^2 \leq \frac{\sigma_r^2}{N^2}$. 
% \begin{equation}
% \mathbb{E} ||g_k^j- \bar{g}_k^j||^2 \leq \frac{\sigma_r^2}{N^2}.
% \end{equation}
\end{lemma}
\begin{proof}
See Appendix \ref{Lemma_SGD:upperbound}. \renewcommand{\qedsymbol}{}
\end{proof}
\begin{lemma}
The expected upper bound of the divergence of $\boldsymbol{w}_{n,k}^j$ is given as 
\begin{equation} 
\begin{aligned}
& \left[ \frac{1}{N}\sum_{n\in\mathcal{N}}\mathbb{E} \Big\Vert\bar{\boldsymbol{w}}_k^j-\boldsymbol{w}_{n,k}^j\Big\Vert^2  \right] \leq  4\alpha_kJ B^2,
\end{aligned}
\end{equation}
for some positive $B$.
\end{lemma}
\begin{proof}
See Appendix \ref{Lemma_lemma2:upperbound}. \renewcommand{\qedsymbol}{}
\end{proof}
\begin{lemma}
The expected upper bound of $\mathbb{E} \left[||\bar{\boldsymbol{w}}_k^{j+1} - \boldsymbol{w}^*||^2 \right]$ is given as
\begin{equation} 
\footnotesize
\begin{aligned}
&\mathbb{E}||\bar{\boldsymbol{w}}_k^{j+1} - \boldsymbol{w}^*||^2  \leq 2(1-\mu\alpha_k)\mathbb{E}||\bar{\boldsymbol{w}}_k^j - \boldsymbol{w}^*||^2  
\\&+ \left(2+\frac{1}{2\alpha_k}\right) \left[ \frac{1}{N}\sum_{n\in\mathcal{N}} \mathbb{E} \Big\Vert\bar{\boldsymbol{w}}_k^j-\boldsymbol{w}_{n,k}^j\Big\Vert^2  \right]  +2\alpha_k^2\mathbb{E}||g_k^j - \bar{g}_k^j||^2.
\end{aligned}
\end{equation}
\end{lemma}
\begin{proof}
See  Appendix \ref{Lemma_lemma3:SGDupdate}. \renewcommand{\qedsymbol}{}
\end{proof}
\begin{theorem} \label{theorem_covergence}
Let Assumptions 1-3 hold, then the upper bound of the convergence rate of the federated model training at each cluster after $K$ global rounds satisfy: 
\begin{equation} \label{equa:final_convergenceIID0}
\footnotesize
\begin{aligned}
&\mathbb{E}\left[F_n(\boldsymbol{w}_K)\right] -F^* \leq \frac{L(1+L/\mu)}{\mu}\frac{1}{(K+L/\mu)} (F_n(\boldsymbol{w}_1) -F^*)
 \\&+\frac{16L}{30\mu^2(K+L/\mu)}\sum_{k=1}^{K} \left[4JB^2 \left(\frac{\alpha_k+1}{\alpha_k^2} \right) +\frac{\sigma_r^2}{N^2} \right],
\end{aligned}
\end{equation}
where $J$ is the number of local SGD rounds at each SM, $N$ is the number of SMs, $\alpha_k$ is the learning rate of each SM in global round $k$, and $L, B, \mu$ are constant. 
\end{theorem}

\begin{proof}
See  Appendix \ref{Proof_globalbound}. \renewcommand{\qedsymbol}{}
\end{proof}
\vspace{-2mm}
% Let us denote $\Delta = \frac{L(1+L/\mu)}{\mu}\frac{1}{(K+L/\mu)!} (F_n(\boldsymbol{w}_1) -F^*) +\frac{16L}{30\mu^2(K+L/\mu)!}\sum_{k=1}^{K} \left[4TB^2 \left(\frac{\alpha_k+1}{\alpha_k^2} \right) +\frac{\sigma_r^2}{N^2} \right]$ and $\varphi = \frac{16LH^2}{30\mu^2N(K+L/\mu)!}$, \eqref{equa:final_convergenceIID0} can be re-written w.r.t $\epsilon_{n,k}$ as
% \begin{equation} \label{equa:final_convergenceIID1}
% \begin{aligned}
% &\mathbb{E}\left[F_n(\boldsymbol{w}_K)\right] -F^* \leq \Delta +  \varphi\sum_{k=1}^{K}\sum_{n=1}^{N}\frac{1}{\epsilon_{n,k}-z}.
% \end{aligned}
% \end{equation} 
\begin{remark}
Theorem~\ref{theorem_covergence} implies an inverse relation between the overall FL convergence loss rate and global rounds $K$ and the number of SMs $N$ under a certain number of local SGD rounds $J$. That is, longer training rounds $K$ with more SMs $N$ involved in the training will decrease the first and second terms of the upper bound, which results in an improvement in the global model performance.
\end{remark}

\section{Latency Analysis For PFL-based Load Forecasting} \label{Sec:LatencyAnalysisForPFL-basedLoad Forecasting}
This section explicitly analyzes the latency in our PFL-based load forecasting system. 

\subsection{ Formulation of Latency Problem}

As indicated in Fig.~\ref{Fig: Overview}, for convenience, the network of SMs in the considered metering network consists of leaf nodes and relay nodes. We assume that we have $R$ number of routes in the system where each route originates from a leaf node leading packets to the utility's server through relay nodes. Each route comprises one leaf node and $N$ relay nodes. Hence, the number of leaf nodes in the system is the same as the number of routes. The set of the system's leaf nodes is represented by $\mathcal{R}=\{1, 2, \dots, R\}$, whilst the set of relay nodes inside each route is represented by $\mathcal{M}=\{1, 2, \dots, M\}$. Each node participates in routing by forwarding data. In our model, leaf nodes train their local models and upload them to their respective relay nodes. Relay nodes train and upload their model's parameters while relaying the models from preceding nodes. Each node uses a local dataset for training, maintaining user privacy by not sharing data with the server.

In the case of leaf node $r$, let $f_{r}$, $D_{r}$, and $C_{r}$ represent its CPU computation capability (in CPU cycles per second), the number of data samples, and the number of CPU cycles needed to process a data sample, respectively. If $L_{r}$ is the number of local iterations, the computation time $T_{r}^{\text{train}}$ for $L_{r}$ iterations is calculated as $T_{r}^{\text{train}}=\frac{L_{r}C_{r}D_{r}}{f_{r}}$. The corresponding energy consumption, $E_{r}^{\text{train}}$, is given by $E_{r}^{\text{train}}=L_{r}\zeta_{r}C_{r}D_{r}{f_{r}^{2}}$, where $\zeta_{r}$ is the effective switched capacitance that depends on the hardware and chip architecture of leaf node $r$. Upon completion of local computation, each SM uploads its local model to the parent. We consider frequency division multiple access for the up-link operation. The achievable rate $R_{r}$ of leaf node $r$ is calculated as $R_{r}=b_{r}\log_{2}\left(1+\frac{p_{r}g_{r}}{b_{r}n_{0}}\right)$, where $b_{r}$ represents the allocated bandwidth, $p_{r}$ is the transmit power, $g_{r}$ stands for the channel gain of leaf node $r$, and $n_{0}$ denotes the noise power spectral density. Assuming a constant data size $s$ for the local models, the uploading time can be expressed as $T_{r}^{\text{up}}=\frac{s_r}{R_{r}}$, and the corresponding energy consumption is $E_{r}^{\text{up}}=T_{r}^{\text{up}}p_{r}$ that differs on every $r$. Hence, the total time $T_{r}$ required for computing and uploading local models for leaf node $r$ is $T_{r}=T_{r}^{\text{train}}+T_{r}^{\text{up}}$. If the total energy consumed by leaf node $r$ for computing and uploading local models during each global iteration is denoted by $E_{r}$, it can be expressed as $E_{r}=E_{r}^{\text{train}}+E_{r}^{\text{up}}$.

In case of relay nodes, the computation time for $L_{m}$ local iterations is denoted by $T_{m}^{\text{train}}$, where $T_{m}^{\text{train}}=\frac{L_{m}C_{m}D_{m}}{f_{m}}$. Here, $f_{m}$ represents the CPU computation capability (in CPU cycles per second), $D_{m}$ stands for the number of data samples, and $C_{m}$ denotes the number of CPU cycles needed to process a data sample. The corresponding energy consumption by relay node $m$ is given by $E_{m}^{\text{train}}=L_{m}\zeta_{m}C_{m}D_{m}{f_{m}^{2}}$, where $\zeta_{m}$ is the effective switched capacitance of relay node $m$ that depends on the hardware and chip architecture of it. Like leaf nodes, relay nodes upload their local models to the server for aggregation after local computation. The uploading time for relay node $m$ is given by $T_{m}^{\text{up}}=\frac{s}{R_{m}}$, where $s$ represents the constant data size of the local models uploaded by all nodes. The achievable uploading rate $R_{m}$ is determined by $R_{m}=b_{m}\log_{2}\left(1+\frac{p_{m}g_{m}}{b_{m}n_{0}}\right)$, where $b_{m}$ stands for the allocated bandwidth, $p_{m}$ denotes the transmit power, and $g_{m}$ represents the channel gain of relay node $m$. The corresponding energy consumption is expressed as $E_{m}^{\text{up}}=T_{m}^{\text{up}}p_{m}$. In this work, all channels are assumed to have two fading effects that characterize mobile communications: large-scale fading and small-scale fading. The small-scale fading component is modeled using a Rayleigh distribution, while a deterministic path loss model represents the large-scale fading coefficient.

Furthermore, a relay node must transmit the local models of all preceding nodes. In a given route, a relay node $m$ is connected to $m$ successor nodes, which include $(m-1)$ relay nodes and one leaf node. Let $T_{m}^{\text{tx}}$ represent the time required by relay node $m$ for transmitting all the local models of $(m-1)$ relay nodes it precedes, where $T_{m}^{\text{tx}}=\sum_{k=1}^{m-1} T_{m,k}^{tx}$. Similarly, if $T_{m,r}^{\text{tx}}$ stands for the time required for transmitting the local model of one leaf node it precedes, then $T_{m,r}^{\text{tx}}=\frac{s}{R_{m}}$. The energy consumption by relay node $m$ to transmit the local models of all the nodes it precedes is $E_{m}^{\text{tx}}=E_{m}^{\text{up}}+(m-1)E_{m}^{\text{up}}=mE_{m}^{\text{up}}$. This equation arises from the assumption that all nodes have the same local model size. Consequently, energy consumption for uploading a local model parameter of size $s$ depends on the acting node's achievable upload rate and transmit power. Therefore, the relay node $m$ consumes the same amount of energy for transmitting the local model parameter of each preceding node. Hence, the time $T_{m}$ required by relay node $m$ to compute, upload and transmit during each global iteration can be expressed as $T_{m, r}=T_{m}^{\text{train}}+T_{m}^{\text{up}}+T_{m,r}^{\text{tx}}+T_{m}^{\text{tx}}$. Similarly, the corresponding energy consumption by relay node $m$ to compute, upload and transmit can be written as $E_{m}=E_{m}^{\text{train}}+E_{m}^{\text{up}}+E_{m}^{\text{tx}}$.

If $T_{\text{total}}^{r}$ is the total time required for route $r$ (leaf node $r$ and relay node $1$ to $M$) to complete each global iteration, then it can be formulated as $T_{\text{total}}^r=(T_{r}+\sum_{m=1}^{M} T_{m,r})$. As the route that takes the longest time to complete each global iteration will be the bottleneck for the latency, the total time required for completing each global communication round denoted as $k$ ($k = [1,2,..., K]$) can be written as $T_{\text{total}} = \underset{r \in \mathcal{R}}{\max} T_{\text{total}}^r  = \underset{r \in \mathcal{R}}{\max} (T_{r}+\sum_{m=1}^{M} T_{m,r})$. Hence, the total latency of the FL system over $K$ global rounds can be expressed as
\begin{equation}
T_{\text{total}}^{\text{FL}} = \sum_{k=1}^{K}\left(\underset{r \in \mathcal{R}}{\max} T_{\text{total}}^r \right)  = \sum_{k=1}^{K}\left(\underset{r \in \mathcal{R}}{\max} (T_{r}+\sum_{m=1}^{M} T_{m,r})\right).
\end{equation}

This research aims to minimize the latency of the PFL-based load forecasting system. Based on the above analysis, we formulate the following optimization problem:
\begin{subequations} 
\begin{align}
\min_{\pmb{p_{r}}, \pmb{f_{r}}, \pmb{p_{m}}, \pmb{f_{m}}} \quad & T_{\text{total}}^{\text{FL}} \label{eqn:2a}\\ 
\textrm{s.t.} \quad & 0 \le p_{r} \le P_{r}, \forall{r} \label{eqn:2b}\\
& 0 \le p_{m} \le P_{m}, \forall{m} \label{eqn:2c}\\
& 0 \le f_{r} \le F_{r}, \forall{r} \label{eqn:2d}\\
& 0 \le f_{m} \le F_{m}, \forall{m} \label{eqn:2e}\\
& E_{r} \le E_{r}^{\max}, \forall{r} \label{eqn:2f}\\
& E_{m} \le E_{m}^{\max}, \forall{m} \label{eqn:2g}
\end{align}
\end{subequations}
\noindent
where $\pmb{p_{r}}=\{p_{1}, p_{2}, \dots ,p_{R}\}$, $\pmb{p_{m}}=\{p_{1}, p_{2}, \dots ,p_{M}\}$, $\pmb{f_{r}}=\{f_{1}, f_{2}, \dots ,f_{R}\}$, and $\pmb{f_{m}}=\{f_{1}, f_{2}, \dots ,f_{M}\}$. In (18), \eqref{eqn:2b} and \eqref{eqn:2c} represent the feasible range of the transmit power due to the power budgets of the leaf nodes and the relay nodes. The CPU frequency of each node is constrained in \eqref{eqn:2d} and \eqref{eqn:2e}. The other two constraints, \eqref{eqn:2f} and \eqref{eqn:2g}, are on the energy consumption by each leaf node and relay node, respectively.

\subsection{Proposed Solution to Optimize Latency for the PFL-based Load Forecasting System}

Solving the problem in (18) is challenging due to the coupling of multiple optimization variables. The objective function \eqref{eqn:2a} as well as the energy constraints \eqref{eqn:2f} and \eqref{eqn:2g} are non-convex because of the $\log_{2}$ function of the achievable rates. As mentioned earlier, we divide the problem in (18) into two sub-problems to address the non-convex nature of the objective function and the constraints. Hence, the control variables of the problem in (18) are divided into two blocks: (i) the first block for leaf node optimization $(p_{r}, f_{r})$ and (ii) the second block for relay node optimization $(p_{m}, f_{m})$, which will be updated alternatively in an iterative fashion. 



% The first block is for leaf nodes. For this block, optimization variables other than $(p_{m}, f_{m})$ are held constant. As a result, only $T_{m}$ can be considered for the latency minimization objective function under the constraints (\ref{eqn:27b}), (\ref{eqn:27d}) and (\ref{eqn:27f}). Consequently, problem (27) becomes-

% \
% \noindent
% \textbf{Sub-problem 1.1:}
% \begin{subequations} 
% \begin{align}
% \min_{p_{m},f_{m}} \quad & \left[\max_{m \in \mathcal{M}} \left({\frac{L_{m}C_{m}D_{m}}{f_{m}}}+\frac{s}{b_{m}\log_{2}\left(1+\frac{p_{m}g_{m}}{b_{m}n_{0}}\right)}\right.\right. \nonumber\\
% & \left.\left.+\sum_{n=1}^{N}\left({\frac{L_{n}C_{n}D_{n}}{f_{n}}}+\frac{(n+1)s}{b_{n}\log_{2}\left(1+\frac{p_{n}g_{n}}{b_{n}n_{0}}\right)}\right)\right)\right] \tag{28a} \label{eqn:28a}\\ 
% \textrm{s.t.} \quad & 0 \le p_{m} \le P_{m}, \tag{28b} \label{eqn:28b}\\
% & 0 \le f_{m} \le F_{m}, \tag{28c} \label{eqn:28c}\\
% & \begin{aligned}
%     & L_{m}\zeta_{m}C_{m}D_{m}{f_{m}^{2}} \\
%     & +\frac{s}{b_{m}\log_{2}\left(1+\frac{p_{m}g_{m}}{b_{m}n_{0}}\right)} p_{m} \le E_{m}^{max}.
%     \end{aligned} \notag \\
%     & \tag{28d} \label{eqn:28d}
% \end{align}
% \end{subequations}

% For the second block, only $p_{n}$ and $f_{n}$ are considered as optimization variables with respect to constraints (\ref{eqn:27c}), (\ref{eqn:27e}), and (\ref{eqn:27g}), leading to its problem formulation as below.

% \
% \noindent
% \textbf{Sub-problem 1.2:}
% \begin{subequations} 
% \begin{align}
% \min_{p_{n},f_{n}} \quad & \left[\max_{m \in \mathcal{M}} \left(\sum_{n=1}^{N}\left(\frac{L_{n}C_{n}D_{n}}{f_{n}}+\frac{(n+1)s}{b_{n}log_{2}\left(1+\frac{p_{n}g_{n}}{b_{n}n_{0}}\right)}\right)\right.\right. \nonumber\\
% & \left.\left.+\left({\frac{L_{m}C_{m}D_{m}}{f_{m}}}+\frac{s}{b_{m}\log_{2}\left(1+\frac{p_{m}g_{m}}{b_{m}n_{0}}\right)}\right)\right)\right] \tag{29a} \label{eqn:29a}\\ 
% \textrm{s.t.} \quad & 0 \le p_{n} \le P_{n}, \tag{29b} \label{eqn:29b}\\
% & 0 \le f_{n} \le F_{n}, \tag{29c} \label{eqn:29c}\\
% & \begin{aligned}
%     & L_{n}\zeta_{n}C_{n}D_{n}{f_{n}^{2}} \\
%     & +\frac{(n+1)s}{b_{n}\log_{2}\left(1+\frac{p_{n}g_{n}}{b_{n}n_{0}}\right)} p_{n} \le E_{n}^{max}.
%     \end{aligned} \notag \\
%     & \tag{29d} \label{eqn:29d}
% \end{align}
% \end{subequations}

 
\textit{\textbf{For the first block}},  we introduce a new slack variable $x_{r}$ such that:

\begin{equation}
x_{r} \geq \frac{s}{b_{r}\log_{2}\left(1+\frac{p_{r}g_{r}}{b_{r}n_{0}}\right)}, \forall{r}. 
\end{equation} 
\textbf{Problem in (18)} can be equivalently re-written as
\begin{subequations} 
\begin{align}
\min_{p_{r},f_{r}} \quad & \sum_{k=1}^{K}\left[\max_{r \in \mathcal{R}} \left({\frac{L_{r}C_{r}D_{r}}{f_{r}}}+x_{r}\right.\right. \nonumber\\
& \left.\left.+\sum_{m=1}^{M}\left({\frac{L_{m}C_{m}D_{m}}{f_{m}}}+\frac{(m+1)s}{b_{m}\log_{2}\left(1+\frac{p_{m}g_{m}}{b_{m}n_{0}}\right)}\right)\right)\right] \label{eqn:4a}\\ 
\textrm{s.t.} \quad & L_{r}\zeta_{r}C_{r}D_{r}{f_{r}^{2}}+x_{r}p_{r} \le E_{r}^{\max}, \forall{r} \label{eqn:4b}\\
& \frac{s}{b_{r}x_{r}} \leq \log_{2}\left(1+\frac{p_{r} g_{r}}{b_{r} n_{0}}\right), \forall{r} \label{eqn:4c}\\
& \eqref{eqn:2b}, \eqref{eqn:2d}. \label{eqn:4d}
\end{align}
\end{subequations}

The objective \eqref{eqn:4a} is convex, while the constraint in \eqref{eqn:4d} is also convex. Hence, we now convexify constraints \eqref{eqn:4b} and \eqref{eqn:4c}.

\textit{Constraint \eqref{eqn:4b}}: For $x_{r}>0$ and $p_{r}>0$, we apply \textcolor{black}{successive convex approximation (SCA)} to approximate $x_{r}p_{r}$ as

\begin{equation} \label{eqn:5}
x_{r} p_{r} \le \frac{1}{2} \frac{p_{r}^{i}}{x_{r}^{i}} x_{r}^{2} + \frac{1}{2} \frac{x_{r}^{i}}{p_{r}^{i}} p_{r}^{2} = h_{r}^{i}(x_{r},p_{r})
\end{equation}

\noindent
where $p_{r}^{i}$ and $x_{r}^{i}$ are the feasible point of $p_{r}$ and $x_{r}$ at iteration $i$. Hence constraint \eqref{eqn:4b} can be convexified as

\begin{equation} \label{eqn:6}
    L_{r}\zeta_{r}C_{r}D_{r}{f_{r}^{2}} +\frac{1}{2} \frac{p_{r}^{i}}{x_{r}^{i}} x_{r}^{2} + \frac{1}{2} \frac{x_{r}^{i}}{p_{r}^{i}} p_{r}^{2} \le E_{r}^{\max}, \forall{r}.
\end{equation}

\textit{Constraint \eqref{eqn:4c}}: We use this inequality \cite{kabalci2022design}
\begin{align} \label{eqn:34}
\ln(1+z) \ge \ln(1+z_{i}) + \frac{z_{i}}{z_{i}+1} - \frac{(z_{i})^{2}}{z_{i}+1} \frac{1}{z}.
\end{align}
Now we approximate \textcolor{black}{right hand side (RHS)} of \eqref{eqn:4c} as

\begin{equation} \label{eqn:8}
\begin{split}
    \frac{ s \ln 2}{b_{r} x_{r}} \leq \ln \left(1+\frac{p_{r}^{i} g_{r}}{b_{r} n_{0}}\right) + \frac{p_{r}^{i} g_{r}}{p_{r} g_{r}+b_{r} n_{0}} \\
    - \frac{(p_{r}^{i} g_{r})^{2}}{p_{r}^{i} g_{r} + b_{r} n_{0}} \frac{1}{p_{r} g_{r}}, \forall{r}.
\end{split}
\end{equation}

So, we solve the following convex problem at iteration $i+1$:
\begin{subequations} 
\begin{align}
\min_{p_{r},f_{r}} \quad & \sum_{k=1}^{K}\left[\max_{r \in \mathcal{R}} \left({\frac{L_{r}C_{r}D_{r}}{f_{r}}}+x_{r}\right.\right. \nonumber\\
& \left.\left.+\sum_{n=1}^{N}\left({\frac{L_{n}C_{n}D_{n}}{f_{n}}}+\frac{(n+1)s}{b_{n}\log_{2}\left(1+\frac{p_{n}g_{n}}{b_{n}n_{0}}\right)}\right)\right)\right] \label{eqn:9a}\\ 
\textrm{s.t.} \quad & \eqref{eqn:4d}, \eqref{eqn:6}, \eqref{eqn:8}. \label{eqn:9b}
\end{align}
\end{subequations}
\textit{For complexity analysis}, this problem consists of $(2R)$ scalar decision variables and $(4R)$ linear or quadratic constraints, which results in the per-iteration computational complexity of $\mathcal{O}\left((2R)^2\sqrt{4R}\right)$ \cite{ben2001lectures}.

\textit{\textbf{For the second block}}, we introduce a new slack variable $y_{m}$ such that:
\begin{equation}
y_{m} \ge \frac{(m+1) s}{b_{m}\log_{2}\left(1+\frac{p_{m}g_{m}}{b_{m}n_{0}}\right)}, \forall{m}. 
\end{equation}
\textbf{Problem in (18)} can be equivalently re-written as

\begin{subequations} 
\begin{align}
\min_{p_{m},f_{m}} \quad & \sum_{k=1}^{K}\left[\max_{r \in \mathcal{R}} \left(\left({\frac{L_{r}C_{r}D_{r}}{f_{r}}}+\frac{s}{b_{r}\log_{2}\left(1+\frac{p_{r}g_{r}}{b_{r}n_{0}}\right)}\right)\right.\right. \nonumber\\
& \left.\left.+\sum_{m=1}^{M}\left({\frac{L_{m}C_{m}D_{m}}{f_{m}}}+y_{m}\right)\right)\right] \label{eqn:11a}\\ 
\textrm{s.t.} \quad & L_{m}\zeta_{m}C_{m}D_{m}{f_{m}^{2}}+y_{m}p_{m} \le E_{m}^{\max}, \forall{m} \label{eqn:11b}\\
& \frac{(m+1)s}{b_{m}y_{m}} \leq \log_{2}\left(1+\frac{p_{m} g_{m}}{b_{m} n_{0}}\right), \forall{m}. \label{eqn:11c}\\
& \eqref{eqn:2c}, \eqref{eqn:2e}. \label{eqn:11d} 
\end{align}
\end{subequations}

\noindent Although the objective function \eqref{eqn:11a} and constraint in \eqref{eqn:11d} are convex, constraints \eqref{eqn:11b} and \eqref{eqn:11c} are still non-convex. To convexify these two constraints, we follow the same strategy for constraints \eqref{eqn:4b} and \eqref{eqn:4c}.

\textit{Constraint \eqref{eqn:11b}}: Similar to constraint \eqref{eqn:4b}, constraint \eqref{eqn:11b} can be convexified as
\begin{equation} \label{eqn:12}
    L_{m}\zeta_{m}C_{m}D_{m}{f_{m}^{2}} +\frac{1}{2} \frac{p_{m}^{i}}{y_{m}^{i}} y_{m}^{2} + \frac{1}{2} \frac{y_{m}^{i}}{p_{m}^{i}} p_{m}^{2} \le E_{m}^{max}, \forall{m}
\end{equation}
\noindent
where $p_{m}^{i}$ and $y_{m}^{i}$ are the feasible point of $p_{m}$ and $y_{m}$ at SCA iteration $i$. 

\textit{Constraint \eqref{eqn:11c}}: Similar to  constraint \eqref{eqn:4c}, we approximate RHS of \eqref{eqn:11c} as
\begin{equation} \label{eqn:13}
\begin{split}
    \frac{(m+1) s \ln{2}}{y_{m}} \le b_{m} \left(1+\frac{p_{m}^{i} g_{m}}{b_{m} n_{0}}\right) + \frac{p_{m}^{i} g_{m}}{p_{m} g_{m}+b_{m} n_{0}} \\
    - \frac{(p_{m}^{i} g_{m})^{2}}{p_{m}^{i} g_{m} + b_{m} n_{0}} \frac{1}{p_{m} g_{m}}, \forall{m}.
\end{split}
\end{equation}
Thus, we solve the following convex problem at iteration $i+1$:
\begin{subequations} 
\begin{align}
\min_{p_{m},f_{m}} \quad & \sum_{k=1}^{K}\left[\max_{r \in \mathcal{R}} \left(\left({\frac{L_{r}C_{r}D_{r}}{f_{r}}}+\frac{s}{b_{r}\log_{2}\left(1+\frac{p_{r}g_{r}}{b_{r}n_{0}}\right)}\right)\right.\right. \nonumber\\
& \left.\left.+\sum_{m=1}^{M}\left({\frac{L_{m}C_{m}D_{m}}{f_{m}}}+y_{m}\right)\right)\right] \label{eqn:14a}\\ 
\textrm{s.t.} \quad & \eqref{eqn:11d}, \eqref{eqn:12}, \eqref{eqn:13}. \label{eqn:14b}
\end{align}
\end{subequations}
\textit{For complexity analysis}, this problem consists of $2M$ scalar decision variables and $4M$ linear or quadratic constraints, which results in the per-iteration computational complexity of $\mathcal{O}\left((2M)^2\sqrt{4M}\right)$ \cite{ben2001lectures}. To summarize, we jointly solve the above two blocks to obtain the solutions for \textbf{problem in (18)}, as illustrated in Algorithm 2.
% Redefine Statex to remove numbering
% \makeatletter
% \renewcommand{\Statex}{\item[]\hskip\ALG@thistlm}
% \makeatother

% \begin{algorithm}[t]
%   \caption{Proposed optimization algorithm to minimize FL system latency in load forecasting}
%   %\textbf{Algorithm 1}
% \begin{algorithmic}[1]
%  \footnotesize
%     \Statex \textbf{Input:} 
% %        \Indent
%             \Statex Set the iteration index $i=0$;
%             \Statex Initialize a feasible solution
%             $(p_{r}^{0}$, $f_{r}^{0}$, $p_{n}^{0}$, $f_{n}^{0})$ for the problem in (2);
% %        \EndIndent
%     \Statex \textbf{Repeat}
% %        \Indent
%             \Statex Set $i \gets i+1$
%             \Statex Solve problem (9) to update $p_{r}^{i}$, $f_{r}^{i}$;
%             \Statex Solve problem (14) to update $p_{n}^{i}$, $f_{n}^{i}$;
% %        \EndIndent
%     \Statex \textbf{Until} convergence. 
%     \Statex \textbf{Output:}
% %        \Indent
%             \Statex Optimal $\pmb{p_{r}^{*}}$, $\pmb{f_{r}^{*}}$, $\pmb{p_{n}^{*}}$, $\pmb{f_{n}^{*}}$.
% %        \EndIndent
% \end{algorithmic}
% \end{algorithm}

\begin{algorithm}
\footnotesize
	\caption{Proposed optimization algorithm to minimize FL system latency in load forecasting}
	\begin{algorithmic}[1]
		\label{algo: optimization}
		\STATE \textbf{Input:}  Set the iteration index $i=0$;
		\STATE \textbf{Initialization:} a feasible solution
            $(p_{r}^{0}$, $f_{r}^{0}$, $p_{m}^{0}$, $f_{m}^{0})$ for the problem in (18);
		\STATE \textbf{Repeat}
                \STATE Set $i \gets i+1$
                \STATE Solve problem (25) to update $p_{r}^{i}$, $f_{r}^{i}$;
                \STATE Solve problem (30) to update $p_{m}^{i}$, $f_{m}^{i}$;
            \STATE \textbf{Until} convergence.
        \STATE \textbf{Output:} Optimal $\pmb{p_{r}^{*}}$, $\pmb{f_{r}^{*}}$, $\pmb{p_{m}^{*}}$, $\pmb{f_{m}^{*}}$.
  \end{algorithmic}
\end{algorithm}

\begin{figure*}[!t]
  \centering
  \begin{subfigure}{.99\columnwidth}
    \centering
    \includegraphics[width=\linewidth]{Images/iid.pdf}
    \caption{MAE Loss}
    \label{fig:sub1-iid}
  \end{subfigure}%
  \begin{subfigure}{.99\columnwidth}
    \centering
    \includegraphics[width=\linewidth]{Images/iid_2.pdf}
    \caption{RMSE Loss}
    \label{fig:sub2-iid}
  \end{subfigure}
  \caption{Comparison between different numbers of FL clients (i.e., SMs), standalone, and centralized scheme for IID data.}
  \label{fig: iid}
  \vspace{-2mm}
\end{figure*}

\section{Simulations and Performance Evaluation} \label{Sec:SimulationsandPerformanceEvaluation}
\subsection{Environment Settings}
For our load forecasting simulations, we used the \textit{Individual Household Electric Power Consumption} dataset \cite{hebrail2012individual}. It is both multivariate and time-series real-life data focused on physics and chemistry that describes the electricity consumption for a single household over 47 months with seven features for multivariattion, from December 2006 to November 2010. The house is in Sceaux, 7km from Paris, France. It has a total of 2,075,259 instances with nine features. For each feature, there are a total of seven variables and two other non-variables: Date and Time. The variables include (1) global active power (household consumption of total active power), (2) global reactive power (household consumption of total reactive power), (3) voltage (the average voltage (volts) in that household), (4) global intensity (the average intensity (apms) in that household), (5) kitchen's active energy (watt-hours), (6) laundry's active energy (watt-hours), and (7) climate control system's active energy (watt-hours).
\begin{figure*}
  \centering
  \begin{subfigure}{.99\columnwidth}
    \centering
    \includegraphics[width=\linewidth]{Images/comparison.pdf}
    \caption{MAE Loss}
    \label{fig:sub1-comp}
  \end{subfigure}%
  \begin{subfigure}{.99\columnwidth}
    \centering
    \includegraphics[width=\linewidth]{Images/comparison_2.pdf}
    \caption{RMSE Loss}
    \label{fig:sub2-comp}
  \end{subfigure}
  \caption{Comparison between state-of-the-art approaches (LSTM and FL) and our approach.}
  \label{fig: comp}
  \vspace{-1mm}
\end{figure*}

\begin{figure*}
  \centering
  \begin{subfigure}{.66\columnwidth}
    \centering
    \includegraphics[width=\linewidth]{Images/cnn.png}
    \caption{LSTM Result}
    \label{fig:sub1}
  \end{subfigure}%
  \begin{subfigure}{.66\columnwidth}
    \centering
    \includegraphics[width=\linewidth]{Images/fl.png}
    \caption{FL Result}
    \label{fig:sub2}
  \end{subfigure}%
  \begin{subfigure}{.66\columnwidth}
    \centering
    \includegraphics[width=\linewidth]{Images/pfl.png}
    \caption{PFL Result}
    \label{fig:sub3}
  \end{subfigure}
  \caption{Simulation result of the original and predicted values for the first 120 minutes in the testing dataset.}
  \label{fig: ovp}
  \vspace{-5mm}
\end{figure*}
\begin{figure*}[htb]
  \centering
  \begin{subfigure}{.99\columnwidth}
    \centering
    \includegraphics[width=\linewidth]{Images/lr.pdf}
    \caption{MAE Loss}
    \label{fig:sub1-lr}
  \end{subfigure}%
  \begin{subfigure}{.99\columnwidth}
    \centering
    \includegraphics[width=\linewidth]{Images/lr_2.pdf}
    \caption{RMSE Loss}
    \label{fig:sub2-lr}
  \end{subfigure}
  \caption{Comparison between different learning rates and meta-learning for 5 SMs non-IID data.}
  \label{fig: lr}
  \vspace{-5mm}
\end{figure*}
\textbf{Non-IID SMs distribution.}
\textcolor{black}{For our research, first, we compared our results with 2, 3, and 5 SMs IID data distribution. Then we implemented the comparison using 5 SMs in the non-IID data distribution. Here, the SMs have different \textit{batch sizes} that show different calculation capabilities and \textit{number of data} that demonstrate different data availability. 
%For our simulations, the batch sizes for heterogeneous SMs are 128,128,128,64, and 256 respectively. For the data points, five SMs have 20\%, 20\%, 20\%, 10\%, and 25\% of the total data. For example, SM 2 has a batch size of 128 and has 415051 data instances. 
For training the model and creating a gradient, we considered three different learning rates 0.05, 0.001, and 0.0001. A meter could temporarily test its performance for every global epoch based on all three learning rates for 10 local rounds. Then, judging by the performance, we selected the optimal learning rate (the learning rate that produces the lowest loss value) and used that for local training for 10 local rounds. We run our simulation results for 100 global rounds. }

There are multiple layers in the design of the LSTM model for load prediction. Sequences of shape (batch\_size, 24, 1) are first entered into the input layer; each sequence comprises 24 hourly load values. An LSTM layer comprising 50 units comes next, processing the sequential input and capturing temporal dependencies. The next step is to add a dropout layer, whose dropout rate is 0.2. During training, it randomly sets 20\% of the LSTM layer's outputs to zero in order to prevent over-fitting. If more complicated patterns need to be captured, an additional LSTM layer can be added after this. Lastly, the expected load value for the following hour is generated by adding a completely connected dense layer with a single neuron. This architecture uses the long-term relationships the LSTM can maintain in the data, making it appropriate for precise load forecasting.

Regarding our simulations for load forecasting latency,  a multi-hop metering network for load forecasting that consists of three routes has been considered. Route 1, 2, and 3 each incorporate 2, 3, and 4 relay nodes except as mentioned otherwise in the figure, respectively, with one leaf node assigned to each route. We have considered practical values for all simulation parameters \cite{yang2020energy}. The system bandwidth is set to be 20 MHz \cite{yang2020energy} while the maximum transmit power $P_{r}$ of leaf nodes and $P_{m}$ of relay nodes are configured in the range of [5-25] dBm. The noise power density is considered to be $N_{0}$= -174 dBm/Hz \cite{yang2020energy} and the maximum CPU cycle frequency of a leaf node is configured as $F_{r}$ = 2GHz and that of a relay node is also configured as $F_{m}$ = 2GHz \cite{yang2020energy}. The coefficients for leaf and relay nodes, which depend on their respective hardware and chip architecture, are established as $\zeta_{r} = 10^{-28}$ and $\zeta_{m} = 10^{-28}$, respectively \cite{yang2020energy}. While the number of local iterations for leaf nodes is $L_{r}$=5, that for relay nodes is $L_{m}$ = 15. All simulations were conducted in Matlab using the YALMIP toolbox with the solver MOSEK. To portray the effectiveness of our joint leaf-relay node optimization method, we compare it with two baselines: (i) scheme 1 optimization for only leaf nodes and (ii) scheme 2 optimization for only relay nodes.

\subsection{ Simulation Results for Load Forecasting}
\subsubsection{Training Performance}
% \subsubsection{Comparison between different numbers of SMs}
At first, we implement our proposed personalized Meta-LSTM FL algorithm in IID settings for different SMs. Fig. \ref{fig: iid} describes the result for both MSE loss (a) and RMSE loss (b) for 2,3, and 5 SMs as well as standalone and centralized schemes. 
The figure shows that, as expected, the centralized scheme performs the best while the standalone performs the worst for both loss values. Moreover, the loss value reduces with increased SMs, suggesting improved performance. As a result, we conclude that our approach is adaptable for more SMs and is proportionate to the number of SMs. The following simulation results have been obtained using non-IID data from five SMs as using 5 SMs has provided the best outcomes. 

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.45\textwidth]{Latency_Plots/Three_schemes_plot_1.pdf}
    \caption{Comparison of training performance. Our joint optimization method can achieve up to 45.33\% lower latency than baselines. }
    \label{fig:7}
\end{figure}
\begin{figure*}[!ht]
    \centering
    \footnotesize
    \begin{subfigure}[t]{0.23\linewidth}
        \centering
        \includegraphics[width=\linewidth]{Latency_Plots/Increasing_Fm_plot_1.pdf}
        \caption{System latency versus maximum frequency of leaf nodes.}
        \label{fig:8a}
    \end{subfigure}
    ~
    \begin{subfigure}[t]{0.23\linewidth}
        \centering
        \includegraphics[width=\linewidth]{Latency_Plots/Increasing_Pm_plot_1.pdf}
        \caption{System latency versus maximum transmit power of leaf nodes.}
        \label{fig:8b}
    \end{subfigure}
    \begin{subfigure}[t]{0.23\linewidth}
        \centering
        \includegraphics[width=\linewidth]{Latency_Plots/Increasing_Fn_plot_1.pdf}
        \caption{System latency versus maximum frequency of relay nodes.}
        \label{fig:8c}
    \end{subfigure}
    ~
    \begin{subfigure}[t]{0.23\linewidth}
        \centering
        \includegraphics[width=\linewidth]{Latency_Plots/Increasing_Pn_plot_1.pdf}
        \caption{System latency versus maximum transmit power of relay nodes.}
        \label{fig:8d}
    \end{subfigure}
    \caption{Comparison of system latency with different schemes.}
    %\label{fig:three_subs}
    \vspace{-7mm}
\end{figure*}
Fig. \ref{fig: lr} shows the simulation result for using different learning rates and meta-learning.  The reason why meta-learning works better than other individual learning methods is evident in that figure. While learning rates at 0.05 get off to the best starts but fail to maintain and overestimate both loss values. Learning rates at 0.001 are too slow to catch up, and communication overheads grow. It is noteworthy to mention that the learning rate of 0.01 did not have a good start to catch up. Nevertheless, meta-learning can extract all the beneficial features from these learning rates through appropriate use, which makes it perfect for more accurate results with less communication overhead. 

\subsubsection{Load Forecasting Performance}
Finally, in Fig \ref{fig: comp} we have compared our performance with the LSTM algorithm \cite{bouktif2018optimal} and state of art FL algorithm \cite{fekri2022distributed} based on global epochs. The figure shows that, among the three methods, LSTM performed the worst, as would be expected. PFL fared somewhat better than FL in this comparison. Furthermore, yields a far more stable result than FL. 

We present the testing findings for the first 120 minutes for three methods in Fig. \ref{fig: ovp}. We concentrate on the three crucial stages, phases 1, 2, and 3. The graphs show the raw data and the prediction of the data calculated by different ML models and their evolutions. \textcolor{black}{These phases are chosen because during these phases we can see clear prediction differences between LSTM, FL, and PFL approaches.} In this case, the orange line indicates the expected outcome while the blue line represents the raw data. As a result, the loss value is represented by the space and variations between the orange and blue lines. From Fig \ref{fig: ovp}(a) we can observe that the LSTM technique is not very good at predicting changes, particularly in phases 2 and 3, which have many curves. In phase 1, FL and PFL are far more accurate at predicting. Still, they have not accurately forecasted phase 2's peak. Furthermore, we can observe the variations in phase 2, particularly in phase 3, if we compare the FL and PFL results in Figs. \ref{fig: ovp}(b-c). PFL achieved the highest point in phase 2 and noticeably improved performance in phase 3. 

\textcolor{black}{\subsection{Simulation Results for Load Forecasting Latency }}
% \textcolor{black}{Our joint optimization method can achieve up to 45.33\% lower latency compared to baselines.}
Fig.~\ref{fig:7} compares our proposed algorithm with scheme 1 and scheme 2, depicting the latency (s) versus the number of iterations. The results demonstrate the superior performance of our proposed algorithm. It is evident from the graph that our scheme reaches a stable latency level after the fifth iteration, significantly outperforming the other two schemes in terms of minimizing latency. Specifically, our proposed scheme achieves a 19.79\% reduction in latency compared to scheme 1 and a 45.33\% reduction compared to scheme 2.

We next investigate the latency performance of different schemes. Fig.~\ref{fig:8a} indicates the latency (in seconds) versus the maximum frequency (in GHz) of a leaf node, comparing our proposed algorithm with scheme 1. As higher frequencies generally allow higher data rates, latency decreases with increased maximum frequency of leaf nodes. Both schemes experience reduced latency with higher frequencies, but our proposed algorithm achieves approximately 22.54\% lower latency than scheme 1. This superior performance is due to the algorithm's dynamic adaptation to network conditions, considering both leaf and relay nodes for more efficient resource utilization and minimized latency.

\textcolor{black}{Moreover, Fig.~\ref{fig:8b} illustrates the latency (in seconds) versus the maximum transmit power of a leaf node, comparing our proposed approach with scheme 1.  Our scheme achieves approximately 16.15\% lower latency than scheme 1, despite both schemes benefiting from reduced latency with increased transmit power. Our scheme optimizes resource allocation and communication parameters for leaf and relay nodes, resulting in lower latency and surpassing scheme 1, which does not fully optimize relay node parameters.} \textcolor{black}{As the maximum frequency (GHz) of a relay node increases (as shown in Fig.~\ref{fig:8c}), our algorithm achieves 32.25\% lower latency compared to scheme 2, even though both schemes benefit from the frequency increase.} By optimizing resource allocation and communication parameters for both leaf and relay nodes, our scheme ensures more efficient resource use, and consequently lower latency. This comprehensive approach surpasses scheme 1, which may not fully consider the impact of optimizing relay node parameters in network performance.

Finally, we evaluate the latency versus the maximum transmit power of a relay node in Fig.~\ref{fig:8d}, highlighting the performance contrast between scheme 2 and our proposed method.
Though latency decreases with increased maximum transmit power of relay nodes in both schemes, our scheme outperforms scheme 2 and achieves 22.29\% lower latency.

% By optimizing resource allocation and communication parameters for both leaf and relay nodes, our scheme ensures more efficient resource use, and consequently lower latency. This comprehensive approach surpasses scheme 1, which may not fully consider the impact of optimizing relay node parameters in network performance.

% Increasing the transmit power reduces propagation delays, resulting in faster signal transmission and higher data rates.

% Furthermore, Fig.~\ref{fig:8c} shows a comparison between the performance of our proposed algorithm and that of scheme 2 in terms of the latency versus the maximum frequency (GHz) of a relay node. Similar to the aforementioned leaf node optimization, though both of the schemes experience a gradual decrease in latency with an increase in the maximum frequency of relay nodes, our proposed scheme demonstrates a significant 32.25\% lower latency compared to scheme 2. 

\subsection{Limitation}
However, the disadvantages of meta-learning in PFL include slow convergence, high computational costs from frequent model updates across clients, and overfitting to small client data, which reduces generalization.  Furthermore, meta-learning necessitates several gradient steps for adaptation that rises communication overhead.

\section{Conclusion} \label{Sec:Conslusion}
This paper has proposed a novel PFL approach for load forecasting in smart metering networks.  We have developed a meta-learning algorithm for better load forecasting model training at SMs given non-IID data.  We have also developed an optimization solution to minimize the system latency for the PFL-based load forecasting system by considering the resource allocation of SMs. Simulation results indicate that our proposed PFL method has outperformed existing LSTM and FL approaches with better load forecasting accuracy. Our optimization solution has also achieved significant latency reductions, with up to 45.33\% lower than baseline methods.

In future work, we will consider incentive mechanisms to speed up load forecasting at SMs further. We will also investigate and extend our FL method to other smart grid domains, such as electricity generation prediction.








% It is important to address the issue of poor SMs in FL model training, where clustering-based methods can be a feasible solution. However, our technique allows a more reliable approach to load forecasting making efficient energy usage and reducing energy loss. 
% This paper has proposed a novel personalized federated learning approach to signal \textcolor{black}{modulation recognition network}. We have modified the MetaSGD model to include more diverse SMs with different configurations. We have also developed a personalized MetaSGD algorithm using customization gradient creation in the local model. Simulation results have indicated that our approach has notably improved in terms of accuracy and loss values than other approaches. Additionally, the proposed scheme has featured all the federated learning qualities by not only improving the speed \cite{nguyen2022latency} and accuracy of signal \textcolor{black}{classification} in network systems but also handling critical concerns such as data privacy and computational diversity across SMs. \textcolor{black}{In future work, it is important to address the issue of poor SMs in FL model training, where clustering-based methods can be a feasible solution \cite{10147285}. This method allows each homogeneous group of SMs to effectively train a shared partial-global model while mitigating the global model divergence at the server for better modulation classification. }



\section*{Acknowledgment}
% The authors would like to thank Dr. Dinh Nguyen for his guidance, support, and knowledge throughout this research.
This manuscript has been co-authored by UT-Battelle, LLC under Contract No. DE-AC05-00OR22725 with the U.S. Department of Energy. The publisher, by accepting the article for publication, acknowledges that the U.S. Government retains a non-exclusive, paid-up, irrevocable, worldwide license to publish or reproduce the published form of the manuscript or allow others to do so, for U.S. Government purposes. The DOE will provide public access to these results in accordance with the DOE Public Access Plan (\url{http://energy.gov/downloads/doe-public-access-plan}). This research was sponsored in part by Oak Ridge National Laboratoryâ€™s (ORNLâ€™s) Laboratory Directed Research and Development program and by the DOE. The funders had no role in the study design, data collection and analysis, decision to publish, or preparation of this manuscript.


\ifCLASSOPTIONcaptionsoff
  \newpage
\fi
\bibliographystyle{ieeetr}
\bibliography{bibtex/bib/IEEEexample.bib}

\input{Appendix.tex}

% \begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{paper/figures/Pablo-Moriano-Pic.jpg}}]{Pablo Moriano}
% (Senior Member, IEEE) received B.S. and M.S. degrees in electrical engineering from Pontificia Universidad Javeriana in Colombia and M.S. and Ph.D. degrees in informatics from Indiana University Bloomington, Bloomington, IN, USA. He is a research scientist with the Computer Science and Mathematics Division at Oak Ridge National Laboratory, Oak Ridge, TN, USA. His research lies at the intersection of data science,
% network science, and cybersecurity. In particular, he uses data-driven and computational methods to discover, understand, and detect anomalous behavior in large-scale networked systems. Applications of his research range across multiple disciplines, including, the detection of exceptional events in social media, Internet route hijacking, insider threat behavior in version control systems, and anomaly detection in cyber-physical systems. Dr. Moriano is a member of ACM and SIAM.
% %Use $\backslash${\tt{begin\{IEEEbiography\}}} and then for the 1st argument use $\backslash${\tt{includegraphics}} to declare and link the author photo.
% %Use the author name as the 3rd argument followed by the biography text.
% \end{IEEEbiography}

\end{document}


