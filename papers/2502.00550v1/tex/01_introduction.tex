
In computational science and engineering, solving ordinary differential equations (ODEs) and partial differential equations (PDEs) is crucial for modeling complex phenomena across a range of disciplines, including fluid dynamics, structural analysis, and thermal transport. Traditional numerical solvers, such as finite element \citep{hughes2003finite} and finite difference methods \citep{leveque2007finite}, are well-suited for high-fidelity (HF) solutions but can be computationally prohibitive for large-scale or real-time applications. To overcome these limitations, scientific machine learning has emerged as a transformative tool in computational science and engineering, which leverages the power of deep learning to tackle challenging problems. Among these approaches, physics-informed neural networks \citep{raissi2019physics,karniadakis2021physics} have gained attention for directly embedding physical laws (e.g., governing PDEs) into the neural network's loss function. However, they are often tailored to specific problem instances or parameters, which limits their ability to generalize across diverse conditions. In contrast, operator learning has gained prominence as a paradigm for solving ODEs and PDEs by mapping infinite-dimensional function spaces via deep neural networks. A prevalent use of operator learning involves serving as a surrogate solver for PDEs by establishing a mapping from the problem's inputs, such as initial conditions, boundary conditions, or external forcing, to the corresponding solutions. Several architectures, including deep operator networks \citep{lu2021learning, lu2021deepxde}, Fourier neural operators \citep{li2020fourier, kovachki2023neural}, Laplace neural operators \citep{cao2024laplace}, and non-local kernel networks \citep{you2022nonlocal}, etc., have been designed to approximate the mapping between functions. Among these, LNOs effectively capture both transient and steady-state responses, which can better handle non-periodic signals and mixed initial conditions. Its foundation in the pole-residue framework \citep{hu2016pole} (a frequency-domain decomposition enabling robust extrapolation) makes it physically meaningful, which provides a clearer understanding of the input-output relationship compared to black-box methods.

While LNO demonstrates notable computational and theoretical advancements, its training often requires substantial amounts of HF data. HF data can be obtained from analytical solutions, simulations with fine mesh \citep{lu2020extraction, lu2022multifidelity}, or collected from sensors with high resolution \citep{fernandez2016review}. In practice, it may be difficult to obtain for many engineering problems (e.g., aerospace simulations, climate modeling), which makes it beneficial to leverage data from multiple sources and fidelities. To mitigate this, multi-fidelity (MF) modeling offers a structured approach to integrating data of varying resolution, accuracy, and computational cost. Intuitively, low-fidelity (LF) data, which are simpler and less expensive, can be leveraged to capture broader trends and patterns at a lower computational cost. Subsequently, patterns in LF data can inform HF data by identifying trends or discrepancies that are consistent across fidelity levels, which enables corrections or augmentations through techniques like additive or multiplicative adjustments. By combining these sources, MF frameworks enable accurate predictions while optimizing computational resources \citep{meng2020composite, howard2023multifidelity}. 

In MF modeling, accurately quantifying uncertainties in predictions is crucial due to limited HF data. Bayesian methods, such as auto-regressive Gaussian processes \citep{williams1995gaussian, pang2019neural}, are widely adopted for this purpose, which offer both precise predictions and robust uncertainty quantification (UQ). However, their computational complexity grows on the order of $\tilde{O}(N^3)$ with increasing training points $N$, which makes them impractical for modeling abundant LF data. To mitigate this, \citet{meng2021multi} employed Hamiltonian Monte Carlo (HMC) \citep{neal2011mcmc, tanqi2014stochastic, hoffman2014no} to estimate posterior distributions in Bayesian neural networks, which enhances UQ and prediction accuracy. Nonetheless, HMC relies on full gradient computations and its susceptibility to local traps in multi-modal distributions presents notable challenges.

\begin{figure}[!tbp]
    \centering
    \makebox[\textwidth]{\includegraphics[width=1.3\linewidth]{figs/flowchart.pdf}}
    \caption{Schematic overview of the MF-LNO framework (adapted from \citep{cao2024laplace}), which showcases the integration of LF and HF modeling to achieve efficient MF modeling. The framework begins with an LNO to approximate the mapping from LF inputs to LF outputs. Next, the MF enhancement involves two parallel LNOs. The linear LNO models the linear relationships between LF and HF data without nonlinear activation functions $\sigma(\cdot)$, while the nonlinear LNO incorporates non-linear corrections through activation functions. Both linear and non-linear LNOs utilize similar architecture, with the addition of trainable weight parameters $\alpha$ to dynamically adjust the contributions from linear and nonlinear operators. Each LNO employs a series of Laplace layers, which leverages kernel convolution and pole-residue parametrization to transform inputs into a feature space optimized for MF approximation. Uncertainty estimates are generated by combining predictions from multiple ensemble models.}\label{fig:scheme}\vspace{-0.08 in}
\end{figure}

\begin{wrapfigure}{r}{0.40\textwidth}
   \begin{center}
   \vskip -0.2in
     \includegraphics[width=0.75\linewidth]{figs/reSGLD.pdf}
   \end{center}
   \vskip -0.0in
    \caption{Sample Trajectory of replica exchange stochastic gradient Langevin dynamics (adapted from \citep{deng2020non, zheng2025exploring}). Yellow lines denote the trajectory of a low-temperature chain (exploitation), and red lines represent a high-temperature chain (exploration). The chains exchange states according to a swap mechanism. The empirical distribution formed by the yellow trajectory aids in uncertainty quantification.}
    \label{fig:sample_trajectory}
   \vspace{-0.30 in}
\end{wrapfigure}

To ensure robust predictions and UQ within MF modeling, we introduce a multi-fidelity Laplace Neural Operator (MF-LNO) framework (Fig. \ref{fig:scheme}), optimized by replica exchange stochastic gradient Langevin dynamics (reSGLD, Fig. \ref{fig:sample_trajectory}) to enhance model training and UQ. The approach proceeds in two stages: first, an LNO is trained to map inputs to their corresponding LF outputs. Next, this initial prediction is refined by two parallel LNOs (one with activation functions and one without) whose relative contributions are determined by a learnable weight parameter. This design efficiently corrects both linear and nonlinear discrepancies between LF and HF data while using limited HF samples. Crucially, reSGLD leverages multiple temperature replicas to search the parameter space more thoroughly, which improves the overall exploration of the modelâ€™s posterior distribution and provides a more reliable uncertainty estimate.

The rest of this paper is organized as follows: Section 2 details the methodology, including the LNO architecture, MF modeling, and reSGLD for efficient learning and UQ. Section 3 presents experiments on four benchmark systems: the Lorenz system, the Duffing oscillator, the Burgers equation, and the Brusselator reaction-diffusion system. Section 4 outlines discussions and conclusions.
