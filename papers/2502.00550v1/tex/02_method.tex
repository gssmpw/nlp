
This section outlines the methodological framework developed for MF operator learning using LNOs. We begin by describing the foundational architecture of LNOs, which approximate nonlinear mappings between infinite-dimensional function spaces through kernel integral operators in the Laplace domain. Subsequently, we extend this framework to integrate MF datasets, which leverage LF data to inform and refine HF predictions. Finally, we incorporate a robust optimization strategy using reSGLD to enhance model accuracy and provide reliable UQ. Together, these innovations form a comprehensive approach for efficient and accurate solutions to problems in parametric PDEs and other complex dynamical systems.

\subsection{Laplace Neural Operators}\label{subsec:method_LNO}

Let $\mathcal{G}: \mathcal{X} \to \mathcal{Y}$ be an operator mapping from an input function $f\in\mathcal{X}$ to an output function $u\in\mathcal{Y}$. In many physical problems, $\mathcal{X}$ might represent boundary or initial conditions (or spatially varying parameters), while $\mathcal{Y}$ corresponds to the solution space of parametric ODEs or PDEs. Our goal is to seek a parametric approximation $\mathcal{G}_{\boldsymbol{\theta}}$ to the true operator $\mathcal{G}$. Specifically, we aim to learn
$$\mathcal{G}_{\boldsymbol{\theta}} : \mathcal{X} \to \mathcal{Y},$$
where ${\boldsymbol{\theta}}$ denotes the neural network parameters. To achieve this, we introduce a neural operator architecture based on the Laplace transform, referred to as the LNO \citep{cao2024laplace}.


Given $f(t) \in \mathcal{X}$, we first map $f$ to a higher-dimensional representation:
\begin{equation*}
v(t)  =  \mathcal{P}\bigl(f(t)\bigr), 
\end{equation*}
where $\mathcal{P}$ is typically a shallow fully-connected neural network or a linear transformation. The lifted representation $v(t)$ resides in an intermediate space for further processing in the Laplace layers. 

Next, we define a nonlinear operator that acts on $v(t)$ in two parts: a kernel integral operator, and a linear bias term. Concretely, for $t \in D$, we have
\begin{equation}\label{eq:LNO_nonlinear}
u(t) 
 = 
\sigma \left(
(\kappa * v)(t)
 + 
\mathbf{W} v(t)
\right)
=
\sigma \left(\int_D
\kappa(t - \tau) 
v(\tau) 
d\tau
 + 
\mathbf{W} v(t)
\right),
\end{equation}
where $\sigma$ is a nonlinear activation function, $\mathbf{W}$ is a linear transformation, $\kappa$ is a integration kernel, and $*$ denotes the convolution integral operation. 
\begin{remark}
    For simplicity, we consider a single nonlinear operator in the present formulation. Specifically, the nonlinear operator can incorporate multiple operators for better generalization. We represent the nonlinear operator applied to a function $ u(t) $ as $\mathcal{N}_i[u](t) = \sigma \left( (\kappa_i * u)(t) + \mathbf{W}_i u(t) \right)$, where $i$ denotes the operations for the $i$-th operator. For iterative applications, the output of the $i$-th operator becomes the input to the $(i+1)$-th operator, which leads to a recursive form with $I$ nonlinear operators: $ u(t) = (\mathcal{N}_I \circ \mathcal{N}_{I-1} \circ \cdots \circ \mathcal{N}_1)[v](t) $. 
\end{remark}

To exploit the structural advantages of frequency-domain analysis, we perform the convolution in the Laplace domain. Let
$
K(s) 
 = 
\mathcal{L}\bigl\{\kappa(t)\bigr\}(s),$ and 
$V(s) 
 = 
\mathcal{L}\bigl\{v(t)\bigr\}(s).
$
Then the convolution can be represented as
\begin{equation}\label{eq:LNO_sdomain}
    \mathcal{L}\Bigl\{(\kappa * v)(t)\Bigr\}(s) 
 = 
K(s) V(s).
\end{equation}

We first represent $V(s)$ in the pole-residue form:
\begin{equation}\label{eq:force_laplace}
V(s)
 = 
\sum_{\ell=-\infty}^{\infty}
\frac{\alpha_{\ell}}{ s - i\omega_{\ell} },
\end{equation}
where $\omega_{\ell} = \ell \omega_1$ denotes the complex frequency, $\omega_1$  is the fundamental complex frequency, and $\alpha_{\ell}$ is the complex amplitudes for each frequency. In practice, the amplitudes are obtained from the fast Fourier transform of $v(t)$. Similarly, we parameterize $\kappa$ in the Laplace domain and represent it in another pole-residue form:
\begin{equation}\label{eq:pole_res_lap}
K_\phi(s)
 = 
\sum_{n=1}^{N}
\frac{\beta_{n}}{s - \mu_{n}},
\end{equation}
where $\mu_n$ ({poles}) and $\beta_n$ ({residues}) are trainable parameters. We denote $\phi$ as the set of trainable parameters $\{\beta_1, \cdots, \beta_N, \mu_1, \cdots, \mu_N\}$. We continue to represent $K_\phi(s) V(s)$ into its partial-fraction decomposition. Multiplying these two terms yields
$$K_\phi(s) V(s)=\sum_{n=1}^{N}
\sum_{\ell=-\infty}^{\infty}
\frac{\beta_{n}\alpha_{\ell}}{ (s - \mu_{n})(s - i\omega_{\ell}) }$$
where each term in $K(s) V(s)$ has two distinct poles $s=\mu_{n}$ and $s=i\omega_{\ell}$. 
For poles at $\mu_n$, by the residue theorem we have
\begin{equation}\label{eq:residue_gamma}
    \gamma_n
 = 
\lim_{s \to \mu_n}
\bigl(s - \mu_n\bigr) U(s)
 = 
\beta_n V(\mu_n),\quad V(\mu_n)
 = 
\sum_{\ell=-\infty}^{\infty}
\frac{\alpha_\ell}{ \mu_n - i\omega_\ell }.
\end{equation}

For terms grouped around the poles at $s=i\omega_{\ell}$, the contribution from these poles is
\begin{equation}\label{eq:residue_lambda}
    \lambda_\ell
 = 
\lim_{s \to i\omega_\ell}
\bigl(s - i\omega_\ell\bigr) U(s)
 = 
\alpha_\ell K(i\omega_\ell),\quad 
K(i\omega_\ell)
 = 
\sum_{n=1}^N
\frac{\beta_n}{ i\omega_\ell - \mu_n }.
\end{equation}


Combining them from $ s = \mu_n $ and $ s = i\omega_\ell $, we yield the corresponding partial-fraction decomposition:
\begin{equation}\label{eq:U_pole_res}
K_\phi(s) V(s)=\sum_{n=1}^N \frac{\gamma_n}{s - \mu_n} + \sum_{\ell=-\infty}^{\infty} \frac{\lambda_\ell}{s - i\omega_\ell}.
\end{equation}

Combining \eqref{eq:residue_gamma}-\eqref{eq:residue_lambda} and applying the inverse Laplace transform to Eq.~\eqref{eq:U_pole_res} yields:
\begin{equation}\label{eq:u_time_domain}
(\kappa * v)(t)
 = 
\sum_{n=1}^N
\gamma_n \exp(\mu_n t)
 + 
\sum_{\ell=-\infty}^{\infty}
\lambda_\ell 
\exp \bigl(i\omega_\ell t\bigr).
\end{equation}

Lastly, we project $u(t)$ in \eqref{eq:LNO_nonlinear} back to the original output dimension (or ODE/PDE solution space $\mathcal{Y}$) via a local transformation $
\mathcal{G}_{\boldsymbol{\theta}} \bigl(f\bigr)(t)
 = 
\mathcal{Q}\bigl(u(t)\bigr),
$ where $\mathcal{Q}$ is often a neural network or a linear transformation. Together, $\{\mathcal{P},\phi,\mathbf{W},\mathcal{Q}\}$ form the parametric mapping $\mathcal{G}_{\boldsymbol{\theta}}$.



\subsection{Operator Learning for Multi-Fidelity Data}\label{subsec:multi_fid_model}

Accurately approximating function mappings is vital in many scientific and engineering applications, yet obtaining sufficient HF data for training can be prohibitively expensive, time-consuming, or physically impractical. A promising strategy to mitigate this challenge is to leverage MF data, which combines abundant lower-fidelity data with scarce but more accurate data to improve model performance. Given the input functions $f \in \mathcal{X}$, we denote the corresponding LF outputs by $u_L \in \mathcal{Y}_L$ and HF outputs by $u_H \in \mathcal{Y}_H$. In the context of MF surrogate modeling \cite{fernandez2016review}, a commonly utilized relationship between LF and HF data can be expressed as:
\begin{equation}
    u_H(t) = u_L(t) + \mathcal{F}(t) \quad \text{or} \quad u_H(t) = \mathcal{F}(t) \cdot u_L(t),
\end{equation}
where $\mathcal{F}$ represents a correction model that bridges the two fidelity levels. Specifically, $\mathcal{F}(t)$ may take the form of an additive correction term (left) or a multiplicative correction factor (right), which facilitates the transformation of LF outputs $u_L$ into HF outputs $u_H$ while accounting for the discrepancies between them. These corrections can be linear or nonlinear, which depends on the underlying relationships. The primary objective is to construct a surrogate model that learns the mapping from $f$ to $u_H$ using the available datasets:
\begin{equation*}
\{ (f_L^j, u_L^j) \}_{j=1}^{N_L}
\quad
\text{and}
\quad
\{ (f_H^j, u_H^j) \}_{j=1}^{N_H},
\end{equation*}
where $N_L \gg N_H$ in most practical scenarios. LF data are typically abundant but prone to linear or nonlinear biases, leading to reduced accuracy. On the other hand, HF data are more precise but significantly harder to acquire. Directly training an LNO on HF data alone can lead to poor generalization due to the limited size of the HF dataset. Conversely, training the LNO on LF data introduces substantial bias. To mitigate such issues and leverage MF data, we assume that the inter-fidelity relationship between LF and HF data is sufficiently simple to be modeled with limited data. To exploit this, we propose a two-step function mapping process: (1) we first use an LNO, denoted as $\mathcal{G}_{L}$, to learn the mapping $\hat u_L=\mathcal{G}_{L}(f)$. (2) We concatenate the input as $\left[f, \mathcal{G}_{L}(f)\right]$. Then we train a linear LNO (LNO without activation function), $\mathcal{G}_l$, and a non-linear LNO, $\mathcal{G}_{nl}$, to model the relationship from $u_L$ to $u_H$. It should be noted that without the non-linear activation function, the lifting $\mathcal P$, the projection $\mathcal Q$, and $\mathbf{W}$ are clearly linear operations. For the convolution integral in \eqref{eq:U_pole_res}, the computation of $\gamma_n$ and $\gamma_\ell$ depends linearly on $V(s)$, which is derived from the Laplace transform of $v(t)$. This implies that without the non-linear activation function in LNO, it is actually a linear mapping from the input function to the output function. Subsequently, the HF output is modeled as a combination of $\mathcal{G}_l$ and $\mathcal{G}_{nl}$:
\begin{equation}\label{eq:alpha}
\hat u_H  =  \alpha \cdot \mathcal{G}_{l}(f, \mathcal{G}_{L}(f))  +  (1 - \alpha) \cdot \mathcal{G}_{nl}(f, \mathcal{G}_{L}(f)),
\end{equation}

where $\alpha \in [0, 1]$ is a learnable parameter to control the weight of linear and nonlinear correlations. We evaluated various neural network architectures, including fully connected neural networks and convolutional neural networks, to learn the inter-fidelity relationships between LF and HF data. Among these, the LNO structure demonstrated the best performance, which we believe is due to its capacity to handle complex functional relationships.

To learn the function mapping from $f$ to $u_L$, we minimize the relative $L^p$ loss function related to $\mathcal{G}_{L}$:
\begin{equation}\label{eq:loss1}
   \mathcal{L}_L({\boldsymbol{\theta}}_{(1)})  =  \frac{1}{N_L}\sum_{j=1}^{N_L} \frac{\Bigl\|\mathcal{G}_{L}(f_L^j) - u_L^j\Bigr\|_p}{\Bigl\|u_L^j\Bigr\|_p},
   % \Bigl\| \mathcal{G}_{L}(f_L^j) - u_L^j \Bigr\|^2.
\end{equation}
where $\|\cdot\|_p$ denotes the $L{^p}$-norm. Once $\mathcal{G}_{L}$ can approximate the operator well (we can consider a threshold to stop training once the validation loss is small enough, or we can set up the number of epochs to train $\mathcal{G}_{L}$), we continue to minimize the loss function related to $\mathcal{G}_{l}$ and $\mathcal{G}_{nl}$:
\begin{equation}\label{eq:loss2}
   \mathcal{L}_H({\boldsymbol{\theta}}_{(2)})  =  \frac{1}{N_H}\sum_{j=1}^{N_H} \frac{\Bigl\| u_H^j - \hat u_H^j \Bigr\|_p}{\Bigl\| u_H^j \Bigr\|_p} ,
   % \Bigl\| u_H^j - \left[ \alpha \cdot \mathcal{G}_{l}\left(f_H^j, \mathcal{G}_{L}(f_H^j)\right) + (1-\alpha) \cdot \mathcal{G}_{nl}\left(f_H^j, \mathcal{G}_{L}(f_H^j)\right) \right] \Bigr\|^2.
\end{equation}
where $\hat u_H^j = \alpha \cdot \mathcal{G}_{l}\left(f_H^j, \mathcal{G}_{L}(f_H^j)\right) + (1-\alpha) \cdot \mathcal{G}_{nl}\left(f_H^j, \mathcal{G}_{L}(f_H^j)\right) $. The total loss is then expressed as:
\begin{equation}\label{eq:loss3}
\mathcal{L}({\boldsymbol{\theta}})  =  \lambda\mathcal{L}_L({\boldsymbol{\theta}}_{(1)}) + (1-\lambda)\mathcal{L}_H({\boldsymbol{\theta}}_{(2)}),
\end{equation}
where $\{{\boldsymbol{\theta}}_{(1)}, {\boldsymbol{\theta}}_{(2)}, \alpha\}={\boldsymbol{\theta}}$ denotes all learnable parameters in MF-LNO, which includes model parameters of all three LNOs and the learnable weight $\alpha$. The weight coefficient $\lambda$ determines the contribution of the two loss functions. To improve the training efficiency and reduce over-fitting, we employ a two-phase training strategy: 

\textbf{Phase 1:} We train the LF component ($\mathcal{G}_L$) to map LF inputs to LF outputs accurately. During this phase, we set $\lambda = 1.0$, which focuses entirely on optimizing the LF mapping.  

\textbf{Phase 2:} After $\mathcal{G}_L$ has been sufficiently trained, we train the HF component to map the combined state (formed by concatenating HF inputs and the outputs from $\mathcal{G}_L$) to HF outputs. During this phase, we set $\lambda = 0.0$ (freeze ${\boldsymbol{\theta}}_{(1)}$), which shifts the optimization to HF mappings.  

We observe this training process can effectively avoid overfitting during Phase 2. Additionally, it ensures that $\mathcal{G}_L$ is well-trained on LF data before leveraging its outputs to optimize the HF component.

\subsection{Stochastic Gradient Langevin Algorithms}

Here we introduce replica exchange stochastic gradient Langevin Dynamics with adaptive drifts (we call it reSGLD later for simplicity), a method that combines adaptive moment estimation with replica exchange to address sampling challenges in high-dimensional Bayesian inference problems. reSGLD is designed to efficiently train MF-LNO while providing robust UQ. The method employs multiple parallel chains with distinct temperatures to balance exploration and exploitation: high-temperature chains explore broadly, while low-temperature chains refine solutions, which ensures efficient convergence and accurate posterior sampling. A deterministic even-odd swap mechanism, supported by adaptive correction buffers, enables efficient exchanges between chains, which helps escape local optima and sample multimodal posteriors. Later, a dynamic adjustment of swap criteria is used to maintain a target swap rate, which ensures robustness to noise and pathological curvature. This approach extends the scalability of stochastic gradient Langevin dynamics while maintaining computational efficiency.

\textbf{Stochastic Gradient Langevin Dynamics} (SGLD) integrates stochastic gradient descent with Langevin noise to approximate Bayesian posterior sampling in high-dimensional spaces. Given a dataset $\{(f^j, u^j)\}_{j=1}^{N}$, SGLD computes stochastic gradients using mini-batches of data. At iteration $i$, a mini-batch $\mathcal{B}_i \subset \{1, \dots, N\}$ of size $|\mathcal{B}_i|$ is sampled, and the stochastic gradient is $\nabla_{\boldsymbol{\theta}} {\mathcal{L}}({\boldsymbol{\theta}}_i)$, where $\mathcal{L}({\boldsymbol{\theta}}_i)$ is the loss function \eqref{eq:loss3} evaluated at the mini-batch data $\mathcal{B}_i$. The update rule for ${\boldsymbol{\theta}}_i,{\boldsymbol{\theta}}_{i+1}$ with SGLD is then\footnote{We denote here ${\boldsymbol{\theta}}_i$ and ${\boldsymbol{\theta}}_{i+1}$ are neural network parameters of MF-LNO, which includes all parameters in MF-LNO and a learnable weight $\alpha$ mentioned in Section \ref{subsec:multi_fid_model}.}: 
\begin{equation*}
{\boldsymbol{\theta}}_{i+1} = {\boldsymbol{\theta}}_i - \eta_i \nabla_{\boldsymbol{\theta}} \mathcal{L}({\boldsymbol{\theta}}_i) + \sqrt{2 \eta_i \tau} \epsilon_{i}, \quad \epsilon_{i} \sim \mathcal{N}(0, I_d),
\end{equation*}
where $\eta_i$ is the learning rate, $\tau$ is the temperature, and $\epsilon_i$ injects Gaussian noise for exploration. While SGLD is effective in many cases, it struggles with high-dimensional problems characterized by pathological curvature or saddle points. This limitation has motivated the development of adaptive extensions. For example, preconditioning techniques, such as Hessian-based or adaptive preconditioners \citep{girolami2011riemann, li2016preconditioned, deng2019}, scale gradient updates to better navigate ill-conditioned landscapes. Momentum-based methods, including underdamped Langevin Monte Carlo \citep{cheng2018underdamped, zhang2023improved, zheng2024accelerating} and HMC \citep{neal2011mcmc, tanqi2014stochastic, hoffman2014no}, introduce auxiliary variables to improve exploration and escape saddle points. Cyclical MCMC \citep{zhangcyclical}, which periodically varies the step size or noise level, helps balance exploration and exploitation in regions with varying curvature. Noise adaptation strategies, such as adaptive noise scaling \citep{ma2015complete} or annealing, further enhance sampling efficiency. Metropolis-adjusted Langevin algorithm \citep{dwivedi2019log, chewi2021optimal} corrects discretization errors and enables more precise exploration of the parameter space. SGLD with adaptive drifts \citep{kim2022stochastic, ishfaq2024provable} leverages adaptive learning rates based on the first and second moments of the gradients to improve convergence in non-stationary environments. reSGLD \citep{jingdong2, deng2020accelerating} facilitates exploration by swapping states between parallel chains at different temperatures. Referring to the above techniques to escape local modes, we subsequently demonstrate our solution to address such issues when training MF-LNO.

\begin{remark}
    To ensure that SGLD weakly converges to the target distribution, the learning rate $\eta_i$ must satisfy the Robbins-Monro conditions \cite{welling2011bayesian,chen2014stochastic}, specifically $\sum_{i=1}^\infty \eta_i = \infty$ and $\sum_{i=1}^\infty \eta_i^2 < \infty$. This implies that $\eta_i$ decays over time, with $\lim_{i \to \infty} \eta_i = 0$. 
\end{remark}

\textbf{SGLD with adaptive drifts} addresses these challenges by incorporating adaptive moment estimation. The update rule becomes:
\begin{equation*}
{\boldsymbol{\theta}}_{i+1} = {\boldsymbol{\theta}}_i - \eta_i \left( \nabla_{\boldsymbol{\theta}} \mathcal{L}({\boldsymbol{\theta}}_i) + a A_i \right) + \sqrt{2 \eta_i \tau} \epsilon_{i},
\end{equation*}
where $a$ is a scaling factor or weight applied to the adaptive bias term $A_i$, and $A_i = m_i / (\sqrt{V_i} + \rho)$ is an adaptive bias term. The moments $m_i$ and $V_i$ are updated as:  

\begin{equation}\label{eq:moment_estimate}
\begin{aligned}
m_i &= \beta_1 m_{t-1} + (1 - \beta_1) \nabla_{\boldsymbol{\theta}} \mathcal{L}({\boldsymbol{\theta}}_i), \\
V_i &= \beta_2 V_{t-1} + (1 - \beta_2) \left(\nabla_{\boldsymbol{\theta}} \mathcal{L}({\boldsymbol{\theta}}_i)\right)^2,
\end{aligned}
\end{equation}
with $\beta_1, \beta_2$ as exponential decay rates and $\rho$ ensuring numerical stability. 


\textbf{Replica Exchange SGLD} further enhances exploration by deploying $N_C$ parallel chains at different temperatures $\tau^{(1)} < \cdots < \tau^{(N_C)}$. We denote the chain $n$ follows: 

\begin{equation}\label{eq:resgld}
    {\boldsymbol{\theta}}_{i+1}^{(n)} = {\boldsymbol{\theta}}_i^{(n)} - \eta_i \left( \nabla_{\boldsymbol{\theta}} \mathcal{L}({\boldsymbol{\theta}}_i^{(n)}) + a A_i^{(n)} \right) + \sqrt{2 \eta_i \tau^{(n)}} \epsilon_{i},
\end{equation}
where $A_i^{(n)}$ adapts via chain-specific moments $m_i^{(n)}$ and $V_i^{(n)}$ following \eqref{eq:moment_estimate}. We further consider the strategy to swap chains. Different from a deterministic swap function applied to a two-chain reSGLD, we here employ a deterministic even-odd scheme with gradient-based swaps (see \citep{deng2023non, zheng2024constrained}) to efficiently swap chains in multiple-chain reSGLD. We first introduce the swap mechanism, and then further discuss the update rule for hyper-parameters in the swap mechanism. 

\textbf{Swap mechanism:}
Chains in reSGLD interact via a deterministic even-odd scheme with gradient-based swaps. Intuitively, it is a non-reversible parallel tempering method (chain indices evolve in a structured, directional manner) and considers using gradient-based updates instead of traditional Metropolis-Hastings swaps to improve exploration and mixing. For adjacent pairs $(n, n+1)$, swaps are attempted alternately within windows of size $\mathbb{W}_*$, which are chosen to avoid insufficient local exploitation and minimize round-trip time. The window size
\begin{equation}\label{eq:window}
    \mathbb W_* \geq \left\lceil \frac{\log N_C + \log \log N_C}{-\log (1-\mathbb{S})} \right\rceil
\end{equation}
is a tunable hyper-parameter to minimize round-trip time, and $\mathbb S$ is the target swap rate. A swap between chains $n$ and $n+1$ occurs if both $\mathbb{G}^{(n)} \geq \mathbb{W}_*$ and $\mathbb S^{(n)}_i:=\{\mathcal{L}({\boldsymbol{\theta}}_i^{(n+1)}) + \mathbb{C}_i^{(n)} < \mathcal{L}({\boldsymbol{\theta}}^{(n)})\}$ hold true. Intuitively, the event $\mathbb S^{(n)}_i$ assumes that swaps happen when the higher-temperature chain has explored a state that is better (lower energy) than the state of the lower-temperature chain, even after accounting for the noise in the energy estimates. Here we denote $\mathbb{C}_i^{(n)}$ as a correction buffer, and $\mathbb G^{(n)} = 0$ if chain swap happen. Otherwise, we set $\mathbb G^{(n)} = \mathbb G^{(n)}+1$.

\textbf{Adaptive Correction Buffer:} To maintain a target swap rate $\mathbb{S}$, $\mathbb{C}_i^{(n)}$ is updated via:  
\begin{equation}\label{eq:buffer_correct}
    \mathbb{C}_{i+1}^{(n)} = \mathbb{C}_{i}^{(n)} + \gamma_i \left( \frac{1}{i} \sum_{j=1}^{i} \mathbb{I}_{\mathbb{S}_j^{(n)}} - \mathbb{S} \right),\quad n=1,2,\cdots,N_C-1,
\end{equation}
where $\gamma_i$ is a step size and $\mathbb{I}_{\mathbb{S}_i^{(n)}}$ is an indicator for swap events. 

By iteratively adjusting $\mathbb{C}_{i}^{(n)}$ based on the observed swap rate, the algorithm dynamically balances the exploration of high-temperature chains with the exploitation of low-temperature chains, which leads to efficient sampling from the target posterior distribution.


