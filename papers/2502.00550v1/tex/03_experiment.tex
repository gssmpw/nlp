
In this section, we evaluate the proposed method through a series of experiments. We begin with the experimental setup, which details the baselines, evaluation metrics, optimizer configuration, computational efficiencies, and hardware setup. The main results are then presented, which highlights the performance of the method on four representative problems: the Lorenz system, Duffing oscillator, Burgers equation, and Brusselator reaction-diffusion system. Finally, we performed ablation studies to investigate the effects of key hyper-parameters, which further provided insight into the robustness and efficiency of the proposed approach.

\subsection{Experimental Setup}

To demonstrate the effectiveness of the proposed algorithm, we conducted a series of experiments comparing its performance with several benchmark methods. We performed ablation studies to derive strategies for effective hyper-parameter tuning. 

\textbf{Comparison baselines:} Baseline comparisons were carried out using three configurations of LNO: (1) LF prediction, which trains LNO from biased LF data; (2) HF prediction, which fits HF data but shows limited generalization; and (3) mixed-fidelity (mix) prediction, which combines LF and HF data when training a single LNO. To ensure fair comparisons, we maintain consistent configurations across all baseline methods. The neural network structures in the baselines are identical to those used for approximating LF data in the MF-LNO ($\mathcal G_L$). The mix prediction considers using both LF and HF data during training to ensure a fair comparison with our method. 

\textbf{Evaluation metrics:} We consider relative $L^p$ loss (mentioned in \eqref{eq:loss1}-\eqref{eq:loss3}) during training to avoid biased training towards regions with larger magnitudes or neglecting small but important features. We specify that $p=2$ is used to define the loss function. 

\textbf{Optimizer configuration:} To implement reSGLD, we configure the hyper-parameters as follows. The momentum decay rates are set to $\beta_1 = 0.9$ and $\beta_2 = 0.999$, with a numerical stability constant of $\rho = 10^{-8}$. We employ a cosine learning rate schedule with one cycle to ensure smooth transitions between the exploration and exploitation phases. The target swap rate is fixed at $\mathbb{S} = 0.10$, and the swap window size is set to $\mathbb{W}_* = 50$, which exceeds the theoretical minimum of $\mathbb{W}_* \geq 30$ computed for $N_C = 10$ chains following \eqref{eq:window}. For the temperature schedule, we refer to the strategy proposed by \citep{kim2022stochastic} to initialize the temperature. Specifically, we adopt a temperature ladder, defined as a geometric sequence ranging from $10^{-5}$ to $10^{-4}$, to initialize temperatures across chains. The correction buffers $\mathbb{C}_0^{(n)}$ are initialized to $0.50$ for all chains $n = 1, 2, \dots, N_C$, unless specified otherwise. The correction buffers are updated iteratively with a decaying step size $\gamma_i = \min\left(0.5, \frac{100}{i^{0.8} + 100}\right)$, where $i$ denotes the iteration index. The number of chains ($N_C$), batch size, and initial learning rate are problem-dependent and will be discussed in the experimental results.

\textbf{Initialization:} We initialize Laplace layer parameters with a uniform distribution over $[0,1]$, while all other parameters employ Kaiming initialization \citep{he2015delving} to preserve activation variance across layers and stabilize gradient propagation. We initialize $\alpha=0.50$ in \eqref{eq:alpha} to to balance the contributions of linear and nonlinear correlations in LNOs.

\textbf{Computational efficiency:} In general, the computational efficiency of training LNOs scales linearly with the size of the training dataset. When training different LNO models with the same number of epochs, the HF prediction requires the least time, as it only operates with limited HF data. The LF prediction takes longer due to the abundance of LF data. Mix prediction requires even more time, as it incorporates both LF and HF datasets. Our proposed method demands slightly more time than Mix prediction because it involves a two-phase training process. However, we want to denote that in the context of MF modeling, the primary consideration is the limited availability of HF data. By leveraging the abundance of LF data to effectively approximate HF outputs, the additional training time becomes less critical. 

\textbf{Hardware setup:} Experiments were conducted on a desktop featuring an AMD Ryzen Threadripper PRO 5955WX CPU, an RTX 4090 GPU, and 128 GB of DDR4 RAM.

\begin{table}[!htbp]\centering
\caption{Summary of datasets for various tasks. The table provides an overview of the datasets used for training and testing in different tasks. For each task, the dimensions of the system, spatial and temporal resolutions, and the sizes of the training and testing sets are specified.}\label{tab:data_summary}
\begin{tabular}{cccccccc}
\toprule
{Task} & {Dimensions} & {Resolutions}       & {Training sets} & {Testing sets} \\ \midrule
Lorenz       & 1  & 512, 2048   & 200, 10     & 130, 130   \\
Duffing      & 1  & 1024, 2048   & 200, 10     & 130, 130   \\
Burgers      & 2  & $32\times 25$, $64\times 50$      & 800, 10     & 100, 100   \\
Brusselator    & 3  & $13\times 7\times 7$, $39\times 14\times 14$ & 800, 1      & 200, 200  \\ \bottomrule
\end{tabular}
\end{table}

\subsection{Main Results}\label{subsec:exp_results}
As shown in Table \ref{tab:data_summary}, we first test the algorithm on one-dimensional systems, which include the forced Lorenz system and Duffing oscillator, and then extend the scope to more challenging cases such as the two-dimensional Burgers equation and the three-dimensional Brusselator Reaction-Diffusion System. We would call these tasks Lorenz, Duffing, Burgers, and Brusselator in the tables and figures for simplicity. From the table, each task includes both LF and HF data, with corresponding resolutions and data sizes specified in the table. The left values in the resolution and dataset sizes represent the LF data, while the right values correspond to the HF data. For instance, the Lorenz and Duffing systems have low- and HF resolutions of 512/1024 and 2048 for their functional inputs and outputs. Their LF training sets consist of 200 data, while the HF training sets contain 10 data. Similarly, the testing sets comprise 130 LF data and 130 HF data.

\begin{table}[!htbp]\centering
\caption{Testing loss comparison across different LNO models. The table presents the mean and standard deviation (based on 10 repeated tests) of the testing loss for various tasks, including the Lorenz system, Duffing oscillator, Burgers equation, and Brusselator reaction-diffusion system. The losses are rescaled by factors in the first columns. The results highlight the superior performance of the proposed model (MF Predictions), which consistently achieves the lowest testing loss across all tasks.}\label{tab:test_loss}
\begin{tabular}{ccccccccc}
\toprule
Tasks (scale factor)        & LF predictions & HF predictions       & Mix Predictions & \textbf{MF Predictions} \\ \midrule
% Lorenz      & 1         &              &       &      \\
Lorenz ($10^{-4}$) & $25.192\scriptstyle \pm 3.511$ & $8.381\scriptstyle \pm 9.328$ & $16.145\scriptstyle \pm 3.038$ & $\textbf{2.012}\scriptstyle \pm \textbf{0.706}$     &      \\
Duffing ($10^{-3}$) & $44.744\scriptstyle \pm 0.981$ & $7.222\scriptstyle \pm 0.310$ & $7.572\scriptstyle \pm 0.022$ & $\textbf{3.324}\scriptstyle \pm \textbf{1.232}$     &      \\
Burgers ($10^{-3}$) & $44.361\scriptstyle \pm 0.096$ & $6.560\scriptstyle \pm 0.487$ & $34.500\scriptstyle \pm 3.483$ & $\textbf{3.399}\scriptstyle \pm \textbf{0.127}$     &      \\ 
Brusselator ($10^{-3}$) & $44.842\scriptstyle \pm 49.013$ & $8.027\scriptstyle \pm 2.099$ & $15.489\scriptstyle \pm 11.977$ & $\textbf{1.087}\scriptstyle \pm \textbf{0.131}$     &      \\ \bottomrule
% Test Loss  &     &      &       &     \\ \bottomrule
\end{tabular}
\end{table}

To evaluate the proposed method, we consider a set of representative problems and test them with the proposed method. Before delving into detailed discussions, we summarize the general testing losses achieved by different methods in Table \ref{tab:test_loss}. The table compares the performance of four approaches: LF prediction, HF prediction, Mix prediction, and MF prediction, across all tasks. Testing losses are reported alongside their standard deviations (based on 10 repeated tests with different network initializations), which provides insights into both accuracy and consistency. The results are normalized for clarity, with the units of loss indicated for each task. From the table, LF prediction has the highest losses, which reflects the limitations of LF data. Mix prediction offers improved consistency but retains errors. HF prediction performs better but is less reliable due to limited HF data. In contrast, MF prediction (the prediction from the proposed method) achieves the best performance, with a substantially lower loss on average. This highlights the efficacy of the proposed approach in leveraging MF data to balance accuracy and robustness, particularly in complex and high-dimensional problems.

\textbf{The Lorenz system} is a classic example of a chaotic dynamical system, originally derived as a simplified model of atmospheric convection. It is governed by the following set of three coupled nonlinear ODEs:

\begin{equation}\label{eq:lorenz}
    \begin{aligned}
        \dot{u} &= \sigma (y - u), \\
        \dot{y} &= u (\rho - z) - y, \\
        \dot{z} &= u y - \beta z - f(t),
    \end{aligned}
\end{equation}
where $u(t)$, $y(t)$, and $z(t)$ represent the systemâ€™s state variables, $\sigma$ characterizes the ratio of momentum diffusivity to thermal diffusivity, $\rho$ describes the strength of thermal convection, and $\beta$ is a geometric factor associated with the aspect ratio of the convection cells. The Lorenz system is well-known for its chaotic behavior, characterized by extreme sensitivity to initial conditions. Its solutions provide a benchmark for studying nonlinear dynamics and chaotic attractors, which makes it a foundational example in the theory of dynamical systems.

For the given Lorenz system, we evaluate the proposed method using datasets with both HF and LF data. We first set $\sigma=10.0$, $\beta=\frac{8}{3}$, and $\rho=5.0$ and initial conditions $u(0)=1.0$, $y(0)=z(0)=0$, then we consider a numerical method to simulate the system with fine grid from $t=0$ to $t=20.0$. To generate HF data, we map the forcing function $f(t)=C\sin (2\pi t),\ t\in[0, 20]$ (with different $C \in \{0.05 + 0.05k \,|\, k = 0, 1, \dots, 199\}$) directly to the state variable $u(t)$. This dataset is discretized into 2048 dimensions along the time domain, with the HF inputs and outputs represented as $f_H^j, u_H^j \in \mathbb{R}^{2048}$. We further construct LF datasets that introduce functional correlations with the HF data. we first consider a linear relationship between LF and HF data, where $f_H(t) \to u_L(t)$ ($x_L(t) = u_H(t) + a t + b$). Here, the parameters are set to $a = 0.05$ and $b = 1.0$, and the data is further discretized into 512 dimensions, represented as $f_L^j, u_L^j \in \mathbb{R}^{512}$. We consider 10 HF data ($N_H=10$) and 200 LF data ($N_L=200$) in training the proposed method. 

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=1.0\linewidth]{figs//1d_lorenz/lorenz_1d_interval_linear.pdf}
    \caption{Ground truth (HF data) of the Lorenz system, LF data (with linear correlations), and the approximation yielded by MF-LNO. The x-axis denotes the time stamp, and the y-axis represents the corresponding responses given different forced functions $f(t)$. The red stars are the ground truth, blue dots are LF data, black solid lines are responses approximated by MF-LNO, and the gray shaded areas denote 95\% confidence intervals.}
    \label{fig:lorenz_linear}
\end{figure}

To model the mapping from $f_H(t)$ to $u_H(t)$, we employ a two-phase training approach as described in Section \ref{subsec:multi_fid_model}. Phase 1 focuses on learning the mapping from $f_L(t)$ to $u_L(t)$ using an LNO trained with 4-chain reSGLD over 2,000 epochs. The batch size for this phase is set to 50. Phase 2 refines the mapping from $f_H(t)$ to $u_H(t)$. This phase also utilizes 4-chain reSGLD, but training extends over 5,000 epochs with a reduced batch size of 10. Post-training, we apply reSGLD sampling at intervals of 100 epochs for 10 iterations, which results in an ensemble of 10 models. These ensemble models are subsequently used to compute predictive means and generate 95\% confidence intervals for the outputs.  

% To learn the mapping from $f_H(t)$ to $u_H(t)$, we first train an LNO to approximate the mapping from $f_L(t)$ to $u_L(t)$ (Phase 1 mentioned in Section \ref{subsec:multi_fid_model}) using 4-chain reSGLD for 2,000 epochs. Next, in Phase 2 we train on $f_H(t)$ to $u_H(t)$ with 4-chain reSGLD for 5,000 epochs. The training batch size in Phase 1 is set to be 50, and Phase 2 is 10. After training, we perform reSGLD sampling every 100 epochs for 10 iterations, which produces 10 ensemble models. These models are used to generate the predictive means and 95\% confidence intervals for the predictions.

\begin{figure}
    \centering
    \includegraphics[width=1.0\linewidth]{figs//1d_lorenz/lorenz_1d_compare.pdf}
    \caption{Approximation of the Lorenz system response $u(t)$ from MF data. Red stars are ground truth, black solid lines are approximated by MF-LNO, blue dashed lines are given by a single LNO with HF data, green dashed lines are yielded by a single LNO with LF, and the yellow dashed lines are a single LNO with MF data.}
    \label{fig:lorenz_linear2}
\end{figure}
As shown in Fig. \ref{fig:lorenz_linear}, six randomly selected test cases are plotted, which illustrates the HF output (Ground Truth), LF output (LF data), and predictions from MF-LNO (MF prediction). The figures demonstrate that the proposed method approximates the mapping effectively. Further comparisons with other methods are presented in Fig. \ref{fig:lorenz_linear2}, where the proposed method consistently achieves the best performance. When using only HF data, the LNO exhibits poor generalization, particularly in the intervals $t = 0-5$ seconds and $t = 18-20$ seconds, due to the limited amount of HF training data. The mixed prediction approach, which combines LF and HF data without separation, produces biased results that are influenced by the LF data.

We extend our analysis to incorporate a nonlinear LF dataset, defined as $f_L(t) \to a u_H(t) \sin(2\pi b t)$, where $a = 1.0$ and $b = \frac{1}{2\pi}$. Consistent with the linear case, this dataset is discretized into 512 dimensions, with inputs and outputs represented as $f_L^j, u_L^j \in \mathbb{R}^{512}$. Phase 1 involves training an LNO using 6-chain reSGLD for 2,000 epochs, while Phase 2 adjusts the mapping using 6-chain reSGLD for 5,000 epochs. After training, reSGLD sampling is conducted every 100 epochs over 10 iterations, resulting in an ensemble of 10 models. The predictive means and 95\% confidence intervals, shown in Fig. \ref{fig:lorenz_nonlinear}, highlight that while nonlinear correlations introduce greater predictive uncertainty, MF-LNO still achieves an accurate approximation of the HF data.  

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{figs//1d_lorenz/lorenz_1d_interval_nonlinear.pdf}
    \caption{Ground truth (HF data) of the Lorenz system, LF data (with non-linear correlations), and the approximation yielded by MF-LNO. The x-axis denotes the time stamp, and the y-axis represents the corresponding responses given different forced functions $f(t)$. The red stars are the ground truth, blue dots are LF data, black solid lines are response approximated by MF-LNO, and the gray shaded areas denote 95\% confidence intervals.}
    \label{fig:lorenz_nonlinear}
\end{figure}

\textbf{The Duffing oscillator} is a classic example of a nonlinear dynamical system that models forced oscillations in mechanical and electrical systems. We consider the system to be governed by a second-order nonlinear ODE:

\begin{equation}\label{eq:duffing}
    \ddot{u} + c\dot{u} + u + u^3 = f(t),
\end{equation}
where $u(t)$ represents the displacement of the system, $\dot{u}$ is the velocity, and $\ddot{u}$ is the acceleration. The damping coefficient $c$ and the external forcing function $f(t)$ define the behavior of the system. The Duffing oscillator exhibits rich dynamics, including harmonic, subharmonic, and chaotic responses, depending on the parameter values and initial conditions. Its nonlinear term $u^3$ distinguishes it from a simple harmonic oscillator, which captures more complex phenomena such as bistability and hysteresis.

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=1\linewidth]{figs//1d_duffing/duffing_1d_interval_020.pdf}
    \caption{Ground truth (HF data) of the Duffing oscillator, LF data (with linear correlations), and the approximation yielded by MF-LNO. The x-axis denotes the time stamp, and the y-axis represents the corresponding responses given different forced functions $f(t)$. The red stars are the ground truth, blue dots are LF data, black solid lines are response approximated by MF-LNO, and the gray shaded areas denote 95\% confidence intervals.}
    \label{fig:duffing1}
\end{figure}

For this study, we set $c=0.5$, initial conditions $u(0)=\dot u(0)=0$, and use $f(t) = C \sin(2\pi t)$ as the input function, with $C \in \{0.05 + 0.05k \,|\, k = 0, 1, \dots, 199\}$ and $t \in [0, 20]$. We use a numerical method to simulate $u(t)$ from $t=0$ to $t=20.0$ with a fine grid and then discretize the input and output functions to the required data. HF data consists of $N_H = 10$ data, each discretized into 2048 dimensions. In contrast, the LF dataset includes 200 data, discretized into 1024 dimensions, where the LF output is linearly correlated with the HF output, given by $u_L(t) = u_H(t) + a t + b$ ($a = 0.05$, $b = 1.0$).

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=1\linewidth]{figs//1d_duffing/duffing_1d_compare_020.pdf}
    \caption{Ground truth (HF data) of the Duffing oscillator, LF data (with linear correlations), and the approximation yielded by MF-LNO. The x-axis denotes the time stamp, and the y-axis represents the corresponding responses given different forced functions $f(t)$. The red stars are the ground truth, blue dots are LF data, black solid lines are response approximated by MF-LNO, and the gray shaded areas denote 95\% confidence intervals.}
    \label{fig:duffing2}
\end{figure}

To learn the mapping from $f(t)$ to $u(t)$, we train the LF component of MF-LNO using a 5-chain reSGLD sampler for 2,000 epochs to map $f_L(t)$ to $u_L(t)$. The trained LNO in Phase 1 is then refined with HF data, training the MF-LNO for 5,000 epochs with another 5-chain reSGLD sampler, with 10 ensemble models sampled every 100 epochs. The training batch sizes are 20 and 10 respectively. Fig. \ref{fig:duffing1} shows the predictive means and 95\% confidence intervals for six randomly chosen input functions with different amplitudes $A$. Comparative results with LF predictions, HF predictions, and Mix predictions are presented in Fig. \ref{fig:duffing2}. From the result, it is clear that the Mix predictions and the HF predictions lead to trivial results. This behavior may stem from poor initialization and the difficulty of the sampler extracting patterns from the limited HF data. LF predictions, while avoiding underfitting, still suffer from bias according to the approximated output functions.


\textbf{The Burgers equation} is a fundamental PDE in fluid mechanics and nonlinear wave theory, often used as a simplified model for the Navier-Stokes equations. It is expressed as:

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=1\linewidth]{figs//2d_burgers/burgers_2d.pdf}
    \caption{Results of the 2D Burgers Equation: The first column presents the ground truth for five randomly selected HF test cases, while the second column shows the MF-LNO approximations. The third, fourth, and fifth columns display the point-wise absolute errors for MF, LF, and HF predictions, respectively. The corresponding testing errors, rescaled by a factor of $10^{-3}$, are ${3.399} \pm {0.127}$ (MF predictions), $44.361 \pm 0.096$ (LF predictions), and $6.560 \pm 0.487$ (HF predictions).}
    \label{fig:burgers}
\end{figure}

\begin{equation}\label{eq:burgers}
    \frac{\partial u}{\partial t} + u\frac{\partial u}{\partial x} - \nu \frac{\partial^2 u}{\partial x^2} = 0, \quad u(x, 0) = u_0(x),
\end{equation}
where $u(x, t)$ represents the velocity field, $x$ is the spatial coordinate, $t$ denotes temporal corrdinate, and $\nu$ is the kinematic viscosity. The equation combines nonlinear convection ($u\frac{\partial u}{\partial x}$) and diffusion ($-\nu\frac{\partial^2 u}{\partial x^2}$) effects, which makes it a cornerstone in modeling shock waves, turbulence, and boundary layer dynamics. 

In this study, we consider the Burgers equation with an initial condition modeled as a Gaussian random process, $ u_0 \sim \mathcal{N}\left(0, \sigma^2(-\Delta + \tau^2 I)^{-\gamma}\right) $, where $ \sigma = 7.0 $, $ \Delta = 1/64 $, $ \tau = 7.0 $, and $ \gamma = 2.5 $. The problem is subsequently defined on the domain $ [0, 1] \times [0, 1] $ with periodic boundary conditions: $ u(0, t) = u(1, t) $ and $ \frac{\partial u}{\partial x}(0, t) = \frac{\partial u}{\partial x}(1, t) $. The viscosity is set to $ \nu = 1/1000 $. The input function is defined as the initial condition $ f(x, t) = u_0(x) $, which is temporally invariant, and the output function is the solution $ u(x, t) $. A numerical method is employed to simulate the evolution of $ u(x, t) $ under these conditions.

We aim to learn the mapping from $ f(x, t) $ to $ u(x, t) $ on the domain $ x, t \in [0, 1] $. HF data consists of $ f $ and $ u $ discretized onto $ 64 \times 50 $ grids ($ f_H^j, u_H^j \in \mathbb{R}^{64 \times 50} $), with 10 data collected. LF data, defined on $ 32 \times 25 $ grids ($ f_L^j, u_L^j \in \mathbb{R}^{32 \times 25} $), comprises 200 data. The LF outputs are linearly correlated to the HF data via $ u_L(x, t) = u_H(x, t) + a_1 t + a_2 x + b $, where $ a_1 = a_2 = 0.25 $ and $ b = 0.50 $.

The training begins with an LNO to learn the mapping from $f_L(x, t)$ to $x_L(x, t)$ using a 5-chain reSGLD optimizer for 1,000 epochs. Subsequently, the MF-LNO is trained on $f_H(x, t)$ to $x_H(x, t)$ using the same optimizer for an additional 1,000 epochs. The training batch sizes are set to 200 and 10 for each phase. The ground truth, predictions by MF-LNO, and point-wise absolute errors of MF predictions, LF predictions, and HF predictions are shown in Fig. \ref{fig:burgers}. Due to the inherent bias in the LF data, LF predictions struggles to approximate the mapping accurately. HF predictions demonstrate significant improvement by incorporating HF data, but generalization errors remain, which are addressed more effectively by MF-LNO.

\textbf{The Brusselator reaction-diffusion system} is a widely studied model in chemical dynamics that captures spatiotemporal pattern formation in reaction-diffusion systems. It is governed by two coupled PDEs:

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=1\linewidth]{figs//3d_brusselator/brusselator_3d.pdf}
    \caption{Results of the 3D Brusselator reaction-diffusion system, visualized by slicing at $x=0$: The first column shows the ground truth for five randomly selected HF test cases. The second column displays the predictions generated by MF-LNO. The third, fourth, and fifth columns present the point-wise absolute errors for MF predictions, LF predictions, and HF predictions, respectively. The corresponding testing errors, rescaled by a factor of $10^{-3}$, are ${1.087} \pm {0.131}$ (MF predictions), $44.361 \pm 4.901$ (LF predictions), and $8.027 \pm 2.099$ (HF predictions).}
    \label{fig:brusselator}
\end{figure}
\begin{equation}\label{eq:brusselator}
    \begin{aligned}
    \frac{\partial u}{\partial t} &= D_0\left(\frac{\partial^2 u}{\partial x^2} + \frac{\partial^2 u}{\partial y^2}\right) + a + f(t) - (1+b)u + vu^2, \\
    \frac{\partial v}{\partial t} &= D_1\left(\frac{\partial^2 v}{\partial x^2} + \frac{\partial^2 v}{\partial y^2}\right) + bu - vu^2,
    \end{aligned}
\end{equation}
where $u(x, y, t)$ and $v(x, y, t)$ are the concentrations of two chemical species, $D_0$ and $D_1$ are their respective diffusion coefficients, $a$ and $b$ are reaction parameters, and $f(t)$ is an external forcing term. The Brusselator demonstrates how simple reaction and diffusion mechanisms can lead to self-organized patterns such as oscillations, traveling waves, and stationary spatial structures. It serves as a prototypical example of nonequilibrium systems in mathematical chemistry.

In this work, we study the Brusselator reaction-diffusion system with the following parameters: $a=1.0$, $b=3.0$, $D_0=1.0$, and $D_1=0.5$. The initial conditions are set as $u(x, y, 0) = 1$ and $v(x, y, 0) = \frac{b}{a} + 0.1\mathcal{X}$, where $\mathcal{X}$ is a random spatially varying field on the grid. The spatial domain is $x \times y \in [0, 1] \times [0, 1]$ and the temporal domain $t \in [0, 20]$, with periodic boundary conditions. The forcing function is given by $f(t) = C e^{-0.01t} \sin(t)$, where $C \in \{0.05 + 0.05k \,|\, k = 0, 1, \dots, 199\}$. We employ a numerical method to simulate the solutions $u(x, y, t)$ and $v(x, y, t)$ on a fine grid, with $f(t)$ as the input function and $u(x, y, t)$ as the output function.

We discretize the functions and employ both HF and LF datasets for model training. The HF dataset, discretized on $39 \times 14 \times 14$ grids, is represented as $f_H^j, u_H^j \in \mathbb{R}^{39 \times 14 \times 14}$. Due to the computational cost of generating HF data, only one data ($N_H = 1$) is available. The LF dataset, which comprises 800 data ($N_L = 800$), is discretized on $13 \times 7 \times 7$ grids ($f_L^j, u_L^j \in \mathbb{R}^{13 \times 7 \times 7}$). The LF data is linearly correlated with the HF data via $u_L(t, x, y) = u_H(t, x, y) + a_1 t + a_2 x + a_3 y + b$, where $a_1 = a_2 = a_3 = 0.10$ and $b = 1.0$. After discretization, both inputs and outputs data are normalized to $[0, 1]$ with range normalization fitted on training set statistics. Testing inputs are scaled identically, while the corresponding model predictions are decoded to the original range prior to loss evaluation. This normalization ensures consistent gradient magnitudes during training and stabilizes MF learning.

To learn an MF model, we first consider Phase 1 to approximate the mapping from $f_L(t)$ to $u_L(t, x, y)$ using a 5-chain reSGLD optimizer for 1,000 epochs with a batch size of 200. Next, Phase 2 refines the mapping from $f_H(t)$ to $u_H(t, x, y)$. Due to the limited availability of HF data, we employ a 1-chain Adam SGLD optimizer for 500 epochs with a batch size of 1. Figure \ref{fig:brusselator} illustrates four randomly selected test cases, which show the ground truth (sliced at $x = 0$), MF-LNO predictions, and point-wise absolute errors for MF, LF, and HF predictions. The results indicate that LF predictions are biased due to the inherent limitations of LF data, while HF predictions show generalization errors stemming from insufficient training data. By integrating both data sources, the proposed MF-LNO approach achieves the most robust performance and yields the smallest absolute errors on average.  

\subsection{Ablation Studies}

Ablation studies are critical for understanding how specific design choices influence the method's overall accuracy, generalization, and computational efficiency. To evaluate the proposed method comprehensively, we conduct ablation studies to analyze the impact of key hyper-parameters on performance. By systematically varying individual components while keeping others fixed, we can isolate their contributions, and identify optimal configurations. Furthermore, we hope to develop heuristic guidelines for hyper-parameter tuning in different applications of the proposed method.

In each experiment, we maintain all other hyperparameters at their empirically determined optimal values while varying only the one under investigation. The MF-LNO is trained with the specified settings, and the testing loss is used to evaluate performance. To ensure robustness against effects like random initialization or mini-batch data, each test is repeated 10 times, and we report the mean and standard deviation of the testing losses.

Another caveat during ablation studies is that improper hyper-parameter selections may result in unstable training. The random noise introduced by reSGLD can amplify this instability, potentially resulting in failed training and loss divergence to infinity. To address this issue, we adjusted temperatures in reSGLD to zero. While the temperature in reSGLD is typically used to estimate uncertainties in output function predictions, it does not influence the determination of optimal hyper-parameter settings in our method. We also conduct additional tests to verify that setting the temperature to zero maintains consistency in identifying optimal hyper-parameters while ensuring more stable and effective training during ablation studies.

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=0.8\linewidth]{./figs/1d_duffing/duffing1d_initial_lr_compare.pdf}
    \caption{Validation Loss vs. Initial Learning rate, conducted on training Duffing oscillator models. The x-axis represents the initial learning rates, while the y-axis indicates the corresponding validation loss. The red line illustrates the trend, and the shaded region denotes the 95\% confidence interval.}
    \label{fig:duffing_lr}
\end{figure}

\textbf{Initial learning rate:} In training MF-LNO using MF data from the Duffing oscillators, we observe that an improper choice of the initial learning rate can easily lead to training failure. To identify the optimal initial learning rate, we explored how the initial learning rates affect the training process. To avoid the influence of chain swapping and random initialization, we employ a single-chain reSGLD and repeat each configuration of the initial learning rate 10 times. We first train the LF mapping for 2000 epochs, and then we subsequently have the Phase 2 training for 5000 epochs. The learning rate follows a cosine decay schedule and decreases to 1\% of its initial value by the end of training. We record the validation loss at the end of each run and compute both the mean and variance of these results. Our initial guess was that the optimal initial learning rate should range from $ 10^{-5} $ to $ 10^{-1} $. We first do a grid search of the optimal initial learning rate within this domain, and find that the optimal range lies between $ 10^{-3} $ and $ 10^{-1} $. To refine the search within this range, we conducted a finer grid search to identify the optimal value. Specifically, we refine the search by testing values in the range $ 2.0 \times 10^{-3} $ to $ 6.4 \times 10^{-2} $, increasing by a factor of two at each step.

The corresponding results are summarized in Fig. \ref{fig:duffing_lr}, which indicates that selecting $10^{-2}$ as the initial learning rate yields the lowest validation loss. Based on these findings, we set the initial learning rates for the lowest- and highest-temperature chains in reSGLD as $ \eta^{(1)} = 5 \times 10^{-3} $ and $ \eta^{(N_C)} = 5 \times 10^{-2} $, respectively. This selection ensures an optimal balance between exploration and exploitation when training MF-LNO with multiple chains.


\textbf{Number of chains in reSGLD:} It is known that reSGLD utilizes chains at varying temperatures to balance the dual objectives of global exploration and local exploitation. Low-temperature chains, characterized by small learning rates and low temperatures, execute conservative moves and capture fine details of the target distribution. In contrast, high-temperature chains, with larger learning rates and high temperatures, promote exploratory moves, enabling the sampler to escape local modes and traverse different regions of the energy landscape (see illustrative demonstration in Fig. \ref{fig:sample_trajectory}). To enhance the effectiveness of these replica chains, a ``temperature ladder'' of intermediate chains with progressively decreasing learning rates and temperatures facilitates smoother transitions between high and low-temperature chains. This approach reduces the difficulty of state swaps between adjacent chains, thereby improving the overall swap acceptance probability.

We investigate optimal reSGLD configurations for learning function mappings and performing UQ using MF data. We here focus on the one-dimensional Lorenz system. While similar studies were conducted for more complex systems, including the Duffing oscillator, the Burgers equation, and the Brusselator reaction-diffusion system, the processes to select the number of chains are similar and we omit them here for brevity. 

One critical step in the setup is defining the largest and smallest learning rates. The largest learning rate must enable effective exploration of the loss landscape without destabilizing the training process. Conversely, the smallest learning rate must ensure stable training and avoid stagnation in large training losses. For the Lorenz system, we set the largest learning rate as $\eta^{(N_C)}=5 \times 10^{-4}$ and the smallest as $\eta^{(1)}=5 \times 10^{-6}$. The appropriate choice of learning rate are examined in Fig. \ref{fig:duffing_lr}. To further enhance convergence, we also apply a cosine annealing schedule in practice to these initial learning rates, which theoretically guarantees reSGLD to asymptotically converge to the target distribution \citep{welling2011bayesian}. 

The performance of reSGLD is evaluated by varying the number of chains $N_C$. The intermediate learning rates are calculated as a geometric progression between $\eta^{(1)}$ and $\eta^{(N_C)}$, given by:
\begin{equation*}
\eta^{(n)} = \exp\left[\frac{n-1}{N_C-1} \ln\left(\frac{\eta^{(N_C)}}{\eta^{(1)}}\right)\right] \cdot \eta^{(1)},
\end{equation*}
where $n = 1, \ldots, N_C$. This formulation ensures efficient and smooth state exchanges between adjacent chains.

We conduct experiments with $N_C = 1, 2, 4, 6, 8, 10$ chains and consider both linear and non-linear correlations between LF and HF data. The training procedure includes an initial 2,000 epochs to learn mappings from LF input to output, followed by 5,000 epochs for training MF-LNO. Each experiment is repeated 10 times, and the mean and variance of validation losses are reported in Fig. \ref{fig:ablation_chains}.

\begin{figure}
    \centering
    \includegraphics[width=0.8\linewidth]{./figs/1d_lorenz/lorenz1d_chain_compare.pdf}
    \caption{Validation Loss vs. number of chains to train MF LNO. The x-axis represents the number of chains, while the y-axis shows the corresponding validation loss. The red line represents the validation loss when modeling the mapping using linear inter-fidelity correlations between LF and HF data, while the blue line corresponds to the validation loss when capturing non-linear correlations. The shaded areas indicate the 95\% confidence interval to reflect the uncertainty for each test. The figure demonstrates how validation loss changes with an increasing number of chains, along with the associated uncertainty bounds for both approaches.}
    \label{fig:ablation_chains}
\end{figure}

Our experiments reveal distinct performance patterns based on the type of correlation and the number of chains $N_C$. For linear correlations, the best performance is achieved when $N_C=4$, while non-linear correlations perform optimally around $N_C=6$ or $N_C=10$. Considering the trade-off between training cost and performance, $N_C=6$ is the recommended choice for non-linear correlations. The figure also indicates that linear correlations are inherently easier to learn compared to non-linear ones. This accounts for their lower validation losses and reduced requirements for chains during training. These results suggest that the complexity of the correlation plays a critical role in determining the optimal number of chains. Beyond this threshold, increasing the number of chains may not yield further improvements in performance. However, this threshold is problem-specific and depends on the difficulty of capturing the linear or non-linear correlations within the data.

\begin{table}[!htbp]
\caption{Neural Network configurations of the proposed MF-LNO across different tasks. The table summarizes the architecture of MF-LNO for four tasks. It outlines the number of parameters, number of layers, width, modes, and activation functions for each component ($\mathcal{P}$, $\mathbf{W}$, $\phi$, and $\mathcal{Q}$) in three variants of the LNO ($\mathcal{G}_L$, $\mathcal{G}_l$, and $\mathcal{G}_{nl}$).}\label{tab:network_config}
\begin{tabular}{cccccccc}
\toprule
\multicolumn{3}{c}{\multirow{2}{*}{Configurations}} & \multicolumn{4}{c}{Tasks} \\ \cline{4-7}
\multicolumn{3}{c}{}              & Lorenz & Duffing & Burgers & Brusselator \\ \midrule
\multicolumn{3}{c}{\# model parameters}      &   1157     &   1157      &    16056     &    24861    \\ \midrule
\multirow{10}{*}{$\mathcal G_{L}$}
   & \multirow{2}{*}{$\mathcal P$}      & \# layers    &   1     &   1      &    1     &   1       \\
   &                          & width        &   3     &    3     &     8    &      16     \\ \cline{2-7}
   & \multirow{2}{*}{$\mathbf{W}$}      & \# layers    &   1     &     1    &   2      &   1        \\
   &                          & width        &    3    &   3      &    8     &      16     \\ \cline{2-7}
   & \multirow{3}{*}{$\phi$}     & \# layers    &    1    &    1     &    2     &     1      \\
   &                          & width        &   3     &     3    &     8    &     16      \\
   &                          & \# modes      &   8     &    8     &    3     &      3     \\ \cline{2-7}
   & \multirow{2}{*}{$\mathcal Q$}      & \# layers    &   2     &     2    &     2    &   2        \\
   &                          & width        &   128     &   128      &     128    &      128     \\ \cline{2-7}
   & \multicolumn{2}{l}{activation function} &    Sine    &    Sine     &   Sine      &    ReLU       \\ \midrule
\multirow{9}{*}{$\mathcal G_{l}$}
   & \multirow{2}{*}{$\mathcal P$}      & \# layers    &    1    &     1    &   1      &   1        \\
   &                          & width        &   3     &     3    &    8     &      16     \\ \cline{2-7}
   & \multirow{2}{*}{$\mathbf{W}$}      & \# layers    &    1    &     1    &   2      &    1       \\
   &                          & width        &   3     &    3     &     8    &      16     \\ \cline{2-7}
   & \multirow{3}{*}{$\phi$}     & \# layers    &   1     &   1      &   2      &     1      \\
   &                          & width        &     3   &    3     &    8     &      16     \\
   &                          & \# modes      &     8   &    8     &    3     &     3      \\ \cline{2-7}
   & \multirow{2}{*}{$\mathcal Q$}      & \# layers    &   2     &     2    &     2    &     2      \\
   &                          & width        &   2     &    2     &    16     &     2      \\ \midrule
\multirow{10}{*}{$\mathcal G_{nl}$}
   & \multirow{2}{*}{$\mathcal P$}      & \# layers    &   1     &     1    &     1    &  1         \\
   &                          & width        &   3     &    3     &    8     &      16     \\ \cline{2-7}
   & \multirow{2}{*}{$\mathbf{W}$}      & \# layers    &   1     &     1    &    2     &   1        \\
   &                          & width        &    3    &    3     &    8     &       16    \\ \cline{2-7}
   & \multirow{3}{*}{$\phi$}     & \# layers    &    1    &    1     &   2      &      1     \\
   &                          & width        &    3    &     3    &     8    &     16      \\
   &                          & \# modes      &    8    &    8     &     3    &    3       \\ \cline{2-7}
   & \multirow{2}{*}{$\mathcal Q$}      & \# layers    &  2      &     2    &    2     &      2     \\
   &                          & width        &   2     &   2      &    16     &      2     \\ \cline{2-7}
   & \multicolumn{2}{l}{activation function} &   Sine     &   Sine      &   Sine      &     ReLU     \\ \bottomrule
\end{tabular}
\end{table}

\textbf{Network structures:} We detail the network configuration of the proposed MF-LNO. To simplify notation, we recall the components of the MF-LNO as follows: The LNO $\mathcal G_L$ mapping LF inputs to LF outputs; The linear version of the LNO $\mathcal G_{l}$ maps the concatenation of HF inputs and LF outputs to MF outputs; The non-linear version of the LNO $\mathcal G_{nl}$ performs a similar mapping as $\mathcal G_{l}$ but with the non-linear activation function. As described in Section \ref{subsec:method_LNO}, each LNO can be generally classified into four primary components:  The first component is a lifting network, $ \mathcal{P} $, which is a fully connected neural network that maps the input into a high-dimensional representation. This lifting step ensures that the input is adequately represented for subsequent transformations. The second component is the Laplace layer, $ \phi $, which performs kernel integral operations using a pole-residue formulation to capture both transient and steady-state responses. The third component is a linear transformation, $ \mathbf{W} $, commonly implemented as a linear convolutional neural network. This transformation handles intermediate mappings and feature interactions. Finally, the projection network, $ \mathcal{Q} $, is a fully connected neural network that maps the intermediate states back to the time-domain output functions. We record the width and number of layers for $ \mathcal{P} $, $ \mathbf{W} $, and $ \mathcal{Q} $, while for the Laplace layer $ \phi $, we record the number of layers, width, and modes.

The configurations of the network for experiments involving the Lorenz system, Duffing oscillator, Burgers equation, and Brusselator reaction-diffusion system are presented in Table \ref{tab:network_config}. According to the configurations shown in the table, we found that $\mathcal G_L$ share similar network structures in \citep{cao2024laplace}, while approximating MF data with $ \mathcal{G}_l $ and $ \mathcal{G}_{nl} $ requires reducing the number of parameters in the output projection network $ \mathcal{Q} $. This reduction aims to address the challenges posed by the limited availability of MF data, where overly complex network structures tend to overfit. We observe that simplifying $ \mathcal{Q} $ for $\mathcal G_{l}$ and $\mathcal G_{nl}$ allows for better generalization and improves the overall performance of the model.
