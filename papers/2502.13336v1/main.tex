\documentclass[11pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{microtype}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
%\usepackage{natbib}

\usepackage{url}

\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
%\usepackage{subcaption}
\usepackage{enumitem}
\usepackage{xspace}
\usepackage{thm-restate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{observation}[theorem]{Observation}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

%\usepackage[textsize=tiny]{todonotes}


% \usepackage[noend]{algpseudocode}
%\usepackage{wrapfig}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{algorithm}
\usepackage{algorithmic}


%\usepackage{placeins} % For \FloatBarrier


\newcommand{\mc}[1]{\mathcal{#1}}
\newcommand{\mb}[1]{\mathbb{#1}}

\newcommand{\eps}{\varepsilon} 

\newcommand{\xnote}[1]{\textcolor{blue}{[Haike: #1]}}
\newcommand{\knote}[1]{\textcolor{blue}{[Kiran: #1]}}
\newcommand{\rknote}[1]{\textcolor{orange}{[Ravi: #1]}}
\newcommand{\snote}[1]{\textcolor{red}{[Sepideh: #1]}}
\newcommand{\pnote}[1]{\textcolor{magenta}{[Piotr: #1]}}


\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}

\usepackage{hyperref}
\hypersetup{
    colorlinks = true,
    citecolor = blue,
    linkcolor = black,
    urlcolor = black,
}
\usepackage[capitalize,noabbrev]{cleveref}


\title{Graph-Based Algorithms for Diverse Similarity Search}
\author{ 
Piyush Anand\thanks{The authors are sorted alphabetically.} \\ Microsoft\\ piyush.anand@microsoft.com
\and Piotr Indyk \\ MIT \\ indyk@mit.edu
\and Ravishankar Krishnaswamy \\ Microsoft Research \\ rakri@microsoft.com \vspace{0.3cm}
\and Sepideh Mahabadi \\ Microsorft Research \\ smahabadi@microsoft.com
\and Vikas C. Raykar \\ Microsoft \\ vikasraykar@microsoft.com
\and Kirankumar Shiragur \\ Microsoft Research \\ kshiragur@microsoft.com\vspace{0.3cm}
\and Haike Xu \\ MIT \\ haikexu@mit.edu
}
\date{}


\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\begin{document}
\maketitle
\begin{abstract}
Nearest neighbor search is a fundamental data structure problem with many applications in machine learning, computer vision, recommendation systems and other fields. Although the main objective of the data structure is to quickly report data points that are closest to a given query, it has long been noted~\cite{carbonell1998use}  that without additional constraints the reported answers can be redundant and/or duplicative. This issue is typically addressed in two stages: in the first stage, the algorithm retrieves a (large) number $r$ of points closest to the query, while in the second stage, the $r$ points are post-processed and a small subset is selected to maximize the desired diversity objective. Although popular, this method suffers from a fundamental efficiency bottleneck, as the set of points retrieved in the first stage often needs to be much larger than the final output.

In this paper we present provably efficient algorithms for approximate nearest neighbor search with diversity constraints that bypass this two stage process. Our algorithms are based on popular graph-based methods, which allows us to ``piggy-back'' on the existing efficient implementations.  These are the first graph-based algorithms for nearest neighbor search with diversity constraints.   For data sets with low intrinsic dimension, our data structures report a diverse set of $k$ points approximately closest to the query, in time that only depends on $k$ and $\log \Delta$, where $\Delta$ is the ratio of the diameter to the closest pair distance in the data set. This bound is qualitatively similar to the best known bounds for standard (non-diverse) graph-based algorithms. Our experiments show that the search time of our algorithms is substantially lower than that using the standard two-stage approach. 
\end{abstract}


\section{Introduction}
Nearest neighbor search is a classic data structure problem with many applications in machine learning, computer vision, recommendation systems and other areas~\cite{shakhnarovich2005nearest}. It is defined as follows: given a set $P$ of $n$ points from some space $X$ equipped with a distance function $D(\cdot, \cdot)$,  build a data structure that, given any query point $q \in X$, returns a point $p \in P$ that minimizes $D(q,p)$. In a more general version of the problem we are given a parameter $k$, and the goal is to report $k$ points in $P$ that are closest to $q$. In a typical scenario, the metric space $(X,D)$ is the $d$-dimensional space, and $D(p,q)$ is the Euclidean distance between points $p$ and $q$.
%which measures how dis-similar $p$ and $q$ are from each other. 

Since for high-dimensional point sets the known {\em exact} nearest neighbor search data structures are not efficient,   several approximate versions of this problem have been formulated. A popular theoretical formulation relaxes the requirement that the query algorithm must return the exact closest point $p$, and instead allows it to output any point $p' \in P$ that is a $c$-approximate nearest neighbor of $q$ in $P$, i.e., $D(q,p') \le c D(q,p)$.  In empirical studies,  the quality of the set of points reported by an approximate data structure is measured by its recall, i.e., the average fraction of the true $k$ nearest neighbors returned by the data structure.

Although minimizing the distance of the reported points to the query is often the main objective, it has long been noted~\cite{carbonell1998use} that, without additional constraints, the reported answers are often redundant and/or duplicative. This is particularly important in applications such as recommendation systems or information retrieval, where many similar variants of the same product, product seller, or document exist. For example, an update to the search results listing algorithm implemented by Google in 2019 ensures that ``no more than two pages from the same site'' are listed~\cite{google-div}. Such constraints can be formulated by assuming that each point is assigned a ``color'' (e.g., site id or product seller) and requiring the data structure to output a set $S$ of $k$ points containing {\em at most $k'$ points of each color},  whose distances to $q$ are (approximately) optimal. A more general formulation allows an arbitrary {\em diversity metric $\rho$} (typically different from $D$), and requires the data structure to report a set $S$ of $k$ points such that for any distinct $p,p' \in S$, $\rho(p,p') \ge C$, for some required diversity parameter $C>0$.
 

The aforementioned paper of~\cite{carbonell1998use}  stimulated the development of the rich area of {\em diversity-based reranking}, which became the dominant approach to this problem. The approach proceeds in two stages. In the first stage, the data structure retrieves $r$ points closest to the query, where $r$ can be much larger than the desired output $k$. In the second stage, the $r$ points are post-processed to maximize the diversity objective of the reported $k$ points.

Though popular, the reranking approach to diversifying nearest neighbor search suffers from a fundamental efficiency bottleneck, as the algorithm needs to retrieve a large enough set to ensure that it contains the $k$ diverse points. In many scenarios, the number $r$ of points that need to be retrieved can be much larger than $k$ (see e.g., \Cref{fig:percentage} and the discussion in the experimental section). In the worst case, it might be necessary to set $r=\Omega(n)$ to ensure that the optimal set is found.
%(PICTURE). 
% No space for picture.
This leads to the following algorithmic question:

\begin{quote} {\em 
    Is it possible to bypass the standard reranking pipeline by directly reporting the $k$ diverse points, in time that depends on $k$ and not $r$?}
\end{quote}


In this paper, we aim to solve this problem both in theory and in practice.  Because of these dual goals,  we focus on designing efficient {\em graph-based} algorithms for diverse similarity search.  In graph-based algorithms,   the data structure consists of a graph between the points in $P$, and the query procedure performs greedy search over this graph to find points close to the query.  Graph-based algorithms such as HNSW~\cite{malkov2018efficient}, NGT~\cite{iwasaki2018optimization}, and DiskANN~\cite{jayaram2019diskann} have become popular tools in practice, often topping Approximate Nearest Neighbor benchmarks~\cite{ANN}. In addition, they are highly versatile, as they do not put any restrictions on the distance function $D$. Although most of the work in this area is purely empirical, a recent paper~\cite{indykxu2024worst} gives approximation and running time guarantees for some of those algorithms. 

\subsection{Our Results} 
We give a positive answer to the aforementioned question, by designing a variant of the DiskANN algorithm that reports approximate nearest neighbors of a given query satisfying diversity constraints. Our theoretical analysis assumes the same setup as in~\cite{indykxu2024worst}. Specifically, we assume that the input point set $P$ has bounded {\em doubling dimension}\footnote{Doubling dimension is a measure of the intrinsic dimensionality of the pointset - see Preliminaries for the formal definition.} $d$, and that its aspect ratio (the ratio of the diameter to the closest pair distance) is at most $\Delta$. Under this assumption, we show that the query time of our data structures is polynomial in $k$, $\log n$ and $\log \Delta$.

Formally, our result is as follows. (Here we state the result in the simplest  setting, where the diversity is induced by point colors and $k'=1$. (See Theorem \ref{thm:diverse_ann} for the general result statement.)

\begin{theorem}\label{thm:colorful_ann}
 Consider the data structure constructed by Algorithm~\ref{alg:color-indexing}. Given a query $q$, let $R$ be the radius of the smallest ball around $q$ (w.r.t. the metric $D$) which contains $k$ points of different colors. Then the search Algorithm~\ref{alg:color-search} returns a set $S$ of $k$ points of different colors such that, for all $p \in S$,
 \[ D(q,p) \le \left(\frac{\alpha+1}{\alpha-1}+\epsilon\right) R. \]
 The search algorithms takes $O\left(k\log_{\alpha}\frac{\Delta}{\epsilon}\right)$ steps, where each step takes $\tilde{O}\left(k(8\alpha)^d\log \Delta\right)$ time.
The data structure uses space $O(n k(8\alpha)^d\log\Delta)$.
\end{theorem}


We note that the approximation factor with respect to $D$, as well as the running time bounds, are essentially the same as the bounds obtained in~\cite{indykxu2024worst} for the 
non-diverse approximate nearest neighbor problem. 
The main difference is that the bound in~\cite{indykxu2024worst} does not depend on $k$, as they consider only the case of $k=1$.


As mentioned earlier, Theorem~\ref{thm:colorful_ann} generalizes to arbitrary $k'$ and general diversity metric $\rho$, as discussed later.

\paragraph{Experimental results.} 
%\snote{Ravi/Kiran/Piyush can you please check this paragraph?}
We adapt our theoretical algorithms to devise fast heuristics based on them, and show the efficiency of our algorithms on several realistic scenarios. In one of them, we consider the task of showing ads to a user based on their search queries. Given a number $k$, say $100$, of available slots, the goal is to choose ads from a large corpus, such that the {\em sellers} (i.e., colors) of those ads are all different. A more relaxed constraint requires that the number of ads shown from a single seller be bounded by at most $k'$, say $10$. To achieve $95\%$ recall$@100$ on this real-world scenario, the prevailing baseline approach of retrieving a much larger number of candidates using a regular ANNS index and then choosing the best diversity-preserving $k$-sized subset of them has latency upwards of 8ms; our algorithm, on the other hand achieves a similar recall at a latency of around 1.5ms, resulting in an improvement upwards of {\bf 5X}! { A production-quality implementation of our algorithm is currently under development for serving large-scale workloads at one of the major technology companies.}
%while the improved search algorithm alone 
%brings it down to approximately 5ms. Making both indexing and search diverse further brings this down to around
%1.5ms, resulting in an improvement upwards of {\bf 5X}.

%The difference between our heuristics and our theoretical algorithms is similar to the difference between the fast- and slow-preprocessing algorithms in DiskANN~\citep{jayaram2019diskann,indykxu2024worst}). As one can see from the plots in \Cref{fig:recall-latency-10,fig:recall-latency-1}, both the new indexing and the search methods play a
%crucial role in improving the overall search quality. For example, to achieve $95\%$ recall$@100$ for the product
%dataset, the baseline reranking approach retrieving $r \gg k$ nearest neighbors followed by post-processing has latency upwards of 8ms, while the improved search algorithm alone 
%brings it down to approximately 5ms. Making both indexing and search diverse further brings this down to around
%1.5ms, resulting in an improvement upwards of {\bf 5X}.


\paragraph{Generalizations.}
On the theoretical side, we extend our results in several directions listed below. These are shown and proved in Section \ref{sec:general-algo}.
\begin{itemize}
    \item \textbf{Relaxing the diversity requirement.} First, in some applications, the requirement that {\em all} reported results have different colors is too strong. (For example, the aforementioned application to search~\cite{google-div} allows for two points having the same color.)  Therefore, we consider a more general constraint, requiring that no color should appear more than $k'$ times. We show how our results can be generalized to any $1\leq k'\leq k$.
    %
    \item \textbf{Generic metric $\rho$.\footnote{In fact our algorithms work for $\rho$ being any {\em pseudo-metric} allowing $\rho(p_1,p_2)=0$ for two different points $p_1,p_2\in P$, but for simplicity we refer to it as metric, throughout the paper.}} Second, we generalize our results to the case where diversity is defined according to a given metric $\rho$ (also defined over $X$ but potentially different from $D$). Here, given a required diversity parameter $C$, the goal is to report a set $S$ of $k$ closest points to the query such that $min_{p_1,p_2\in S} \rho(p_1,p_2)\geq C$.
    We say that such a set $S$ is {\em $C$-diverse}.
    Note that the color version is the special case where $\rho(p_1,p_2)$ is defined to be $0$ if $p_1$ and $p_2$ are of the same color, and $1$ otherwise.
    %
    \item \textbf{Unifying the two generalizations.} In order to unify the above two results, and incorporate the notion of $k'$ into the generic metric $\rho$, we allow for each point in the reported set $S$ to be ``similar" to at most $k'>1$ other points in $S$. More formally, for any $p \in S$ there should be at most $k'-1$ other points $p' \in S$ such that $\rho(p,p') < C$. We say that such a set $S$ is {\em $(k',C)$-diverse}\footnote{We note that the notion of $(k',C)$-diverse set is a new notion of diversity that strictly generalizes the widely used minimum-pairwise-distance notion for diversity.}. We show how our algorithms can be modified to this most general formulation of the problem. 
    %
    \item \textbf{Primal vs Dual formulations.} Finally, instead of asking for the closest $k$ points to the query satisfying a diversity requirement parameterized by $C$, which we refer to as the \emph{primal} variant of the problem, one can ask the {\em dual} question: Given a radius $R$, find a set of $k$ points within radius $R$ of the query, while maximizing the diversity. 
    We show algorithms for both the primal and dual variants of the most general formulation of the problem.
\end{itemize}

\subsection{Related Work}

Nearest Neighbor Search with diversity requirement has been previously studied in the work of~\cite{abbar2013real,abbar2013diverse}, where they presented a ``diversified version'' of the {\em Locality-Sensitive Hashing (LSH)} approach due to~\cite{indyk1998approximate}. However, their diversification approach does not carry over to the graph-based methods.
Moreover, they provide a bi-criteria approximation only for the {\em dual formulation} of the problem, and do not consider the primal formulation.
Finally, the distance functions $D$ that they consider are limited to Hamming distance or its variants like the Jaccard similarity~\cite{abbar2013real}. Although it is plausible that the result could be extended to other distances that are supported by LSH functions, not all distance functions satisfy this constraint.



\section{Preliminaries}
\paragraph{Problem definition.}

Let $(X,D)$ be the underlying metric space, with distance function $D$. 
Let $P$ be our colored point set. For $p\in P$, we use $col[p]$ to denote its {\em color}. 

\begin{definition}[colorful]\label{def:colorful}
A set $S$ is {\em colorful} if no two points in $S$ have the same color.
\end{definition}

\begin{definition}[$k'$-colorful]\label{def:k'-colorful}
A set $S$ is {\em $k'$-colorful}, if within the multi-set $\{col[p_1],...,col[p_k]\}$, no color appears more than $k'$ times. 
\end{definition}

Note that for $k'$-colorful for $k'=1$ is equivalent to the colorful notion.

\begin{definition}\label{def:S_i}
    Given a subset $S\subset P$ of size $k$, and a query $q$, for each $i\leq k$, we use $S_i(q)$ to denote the distance of the $i$th closest point in $S$ to $q$. When $q$ is clear from the context, we drop $q$ and simply use $S_i$.
%Moreover, for two solutions $\mathsf{ALG}$ and $\mathsf{OPT}$ of size $k$, we write $\mathsf{ALG}\le \mathsf{OPT}$ if for any $i$, $\mathsf{ALG_i}\le \mathsf{OPT_i}$. \xnote{I am not sure whether this notation is useful any more? if not, shall we delete it}
\end{definition}

\begin{definition}[Colorful NN]
    Given a colored point set $P$, the goal of {\em colorful NN} is to preprocess $P$ and create a data structure such that given a query point $q$, one can quickly return a colorful subset $S\subset P$ of size $k$ such that $S_k$ is minimized. 
\end{definition}

Note that, when $k=1$, our Colorful NN degenerates to the standard nearest neighbor search problem.

\begin{definition}[$k'$-Colorful NN] \label{def:kprime-colorful}
    Given a colored point set $P$, the goal of {\em $k'$-colorful NN} is to preprocess $P$ and create a data structure such that given a query point $q$, one can quickly return a $k'$-colorful subset $S\subset P$ of size $k$ such that $S_k$ is minimized. 
\end{definition}


\paragraph{Balls, doubling dimension, and aspect ratio.}
%The distance between any two points $p,q$ is $D(p,q)$. 
We use $B_D(p,r)$ to denote a ball centered at $p$ with radius $r$, i.e.,  $B_D(p,r)=\{u\in X: D(u,p)<r\}$. We will drop the subscript $D$ if the metric is clear from the context. 

We say a point set $P$ has \emph{doubling dimension} $d$ if for any point $p$ and radius $r$, the set $B(p,2r)\cap P$ can be covered by at most $2^d$ balls of radius $r$.

\begin{lemma}\label{lm:doubling_dimension}
Consider any point set $P \subset X$ with doubling dimension $d$. 



For any ball $B(p,r)$ centered at some point $p\in P$ with radius $r$ and a constant $\alpha$, we can cover $B(p,r)\cap P$ using at most $m\le O(\alpha^d)$ balls with radius smaller than $r/\alpha$, i.e. $B(p,r)\cap P \subset \bigcup_{i=1}^m B(p_i,r/\alpha)$  for some $p_1 \ldots p_m \in X$. 
%{\color{red} \snote{so here $p$ and $p_i$ do not have anything to do with $P$, right?}} 
%\xnote{I think the large ball is centered at some $p\in P$, but not necessary the smaller balls? Is this correct? @piotr}
% PIOTR: yes, the small balls can be centered anywhere. N
\end{lemma}

We define $\Delta=\frac{D_{max}}{D_{min}}$ to be the \emph{aspect ratio} of the point set $P$ where $D_{max}$($D_{min}$, resp.) represents the maximal (minimal, resp.) distance between any two points in the point set $P$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%.................Simple Case

\section{Algorithm for Colorful NN}\label{sec:main-algo}
In this section, for the sake of simplicity of presentation, 
%and its direct connection to the experimental section, 
% PIOTR: experimantal section also uses k'>1
we focus on the simplest setting where $k'=1$, and $\rho$ is the binary metric. The binary setting corresponds to our main application of seller diversity, and the case of $k'=1$ focuses on retrieving $k$ closest points from the data set such that \emph{all} of them have different colors/sellers. 
The algorithm that handles the general setting is presented in Section \ref{sec:general-algo}. 

\paragraph{Intuition behind the algorithm.} 
At a high level, the ``slow pre-processing'' algorithm of~\cite{indykxu2024worst} uses the following intuition when pruning the graph: If $u$ and $v$ are ``much closer'' to each other than to another point $p$, then it is not necessary to connect $p$ to both $u$ and $v$. 
This makes it possible to track the progress of the search procedure as it identifies points closer to the query point and use the doubling dimension to bound the degree of the graph and the total space. 
Our algorithm retains these insights, but also requires several new ones, as now we need to show that the search algorithms can progress {\em while maintaining a colorful solution}. On a high-level, this is established using the following two intuitions. 
First, if the colors of $u$ and $v$ are the same, then again there is no need to connect $p$ to both of them, and we can use the same pruning as before. 
Second, if $p$ is already connected to $k$ points $v_1,\cdots, v_k$, all of which are much closer to $u$ compared to $p$, and that they all have different colors, then again there is no need to connect $p$ to $u$. This is roughly because no solution would need more than $k$ points of different colors in a relatively small neighborhood. 
%This is also why the space usage of our algorithm is higher by a factor $k$ than the algorithm of \cite{indykxu2024worst}.
The main challenge in converting these intutions into a formal argument is in showing that such a graph keeps enough edges for a greedy search algorithm to converge to an approximately optimal solution for the coloful NN problem.

\subsection{The Preprocessing Algorithm} 
The indexing algorithm is shown in Algorithm~\ref{alg:color-indexing}. 
\begin{algorithm}
\caption{Indexing algorithm for colorful NN}
\label{alg:color-indexing}
\begin{algorithmic}[1]
\STATE \textbf{Input}: A set of $n$ points $P=\{p_1,...,p_n\}$; $k$ is the size of the output; $\alpha$ is the parameter used for pruning.
\STATE \textbf{Output}: A directed graph $G=(V,E)$ where $V=\{1,...,n\}$ are associated with the point set $P$.
\FOR{each point $p\in P$}
    \STATE Sort all points $u\in P$ based on their distance from $p$ and put them in a list $\mathcal{L}$ in that order.
    \WHILE{$\mathcal{L}$ is not empty}
        \STATE $u\gets \argmin\limits_{u\in \mathcal{L}}D(u,p)$
        \STATE Initialize $\mathsf{rep}[u]\gets \{u\}$
        \FOR{each point $v\in \mathcal{L}$ in order}
            \IF{$D(u,v)\le D(p,u)/(2\alpha)$}
                \IF{$col[v]$ not shown in $rep[u]$ and $|rep[u]|<k$}
                    \STATE $\mathsf{rep}[u]\gets \mathsf{rep}[u]\cup v$ 
                \ENDIF
                \STATE remove $v$ from $\mathcal{L}$
            \ENDIF
        \ENDFOR
        % \State $\mathsf{rep}[u]\gets$ use the greedy algorithm of Gonzales to choose $k/k'$ points in $\mathsf{bag}[u]$ to approximately maximize the minimum pairwise diversity distance.
        \STATE add edges from $p$ to $\mathsf{rep}[u]$
        \STATE Remove $u$ from $\mathcal{L}$
    \ENDWHILE
\ENDFOR
\end{algorithmic}
\end{algorithm}

\begin{lemma}\label{lm:degree_color}
The graph constructed by Algorithm~\ref{alg:color-indexing} has degree limit $O(k(8\alpha)^d\log\Delta)$. 
\end{lemma}
\begin{proof}
Let's first bound the number of points not removed by others, then according to Line 10 in Algorithm~\ref{alg:color-indexing}, the degree bound will be that times $k$.

We use $\mathsf{Ring}(p,r_1,r_2)$ to denote the points whose distance from $p$ is larger than $r_1$ but smaller than $r_2$. For each $i\in [\log_2 \Delta]$, we consider the $\mathsf{Ring}(p,D_{max}/2^i,D_{max}/2^{i-1})$ separately. According to Lemma~\ref{lm:doubling_dimension}, we can cover $\mathsf{Ring}(p,D_{max}/2^i,D_{max}/2^{i-1})\cap P$ using at most $m\le O((8\alpha)^d)$ small balls of radius $D(p,u)/(4\alpha)\geq \frac{D_{max}}{2^{i+2}\alpha}$. According to the pruning criteria in Line 9, within each small ball, there will be at most one such point $u$ remaining. This establishes the degree bound of $O(k(8\alpha)^d\log\Delta)$.
\end{proof}

\subsection{The Search Algorithm} Algorithm~\ref{alg:color-search} shows the search algorithm for the colorful nearest neighbor search problem. 
Next, we analyze the search algorithm and finally prove Theorem~\ref{thm:colorful_ann}.

\begin{algorithm}[ht]
\caption{Search algorithm for colorful NN}
\label{alg:color-search}
\begin{algorithmic}[1]
\STATE \textbf{Input}: A graph $G=(V,E)$ with $N_{out}(p)$ being the out edges of $p$; query $q$; number of optimization steps $T$.
\STATE \textbf{Output}: A set of $k$ points $\mathsf{ALG}$.
\STATE Initialize $\mathsf{ALG}=\{p_1,...,p_k\}$ to be any set of $k$ points with different colors.
\FOR{$i=1$ to $T$}
    % \State $U\gets \{u | (p_j,u)\in E\ s.t. \exists\ p_j\in ALG\}$
    \STATE $p_k\gets$ the furthest point in $\mathsf{ALG}$ from $q$.
    \STATE $U\gets N_{out}(p_k)$ and sort $U$ based on their distance from $q$
    % \STATE $U\gets \bigcup\limits_{p\in \mathsf{ALG}}(N_{out}(p)\cup p)$ and sort $U$ based on their distance from $q$
    % \STATE $\mathsf{ALG}\gets$ the closest $k-1$ points in $\mathsf{ALG}$    
    \FOR{each point $u\in U$}
        \IF{$\mathsf{ALG}\setminus p_k \bigcup u$ is colorful}
            \STATE $\mathsf{ALG}\gets \mathsf{ALG}\setminus p_k \bigcup u$
            \STATE Break
        \ENDIF
        % \IF{$|\mathsf{ALG}|=k$}
            % \STATE Break
        % \ENDIF
    \ENDFOR
    % \If{$\mathsf{ALG}^{t}\ge \mathsf{ALG}^{t-1}$}
    % \State Break
    % \EndIf
\ENDFOR
\STATE \textbf{Return} $\mathsf{ALG}$
\end{algorithmic}
\end{algorithm}

\begin{proposition}\label{prop:p_star_exists_diverse}
Let $\mathsf{OPT}=\{p^*_1,...,p^*_k\}$ be a colorful solution with minimized $\mathsf{OPT_k}$, and $\mathsf{ALG}=\{p_1,...,p_k\}$ be the current solution (both ordered by distance from $q$). If $p_k\notin \mathsf{OPT}$, there exists a point $p^*\in \mathsf{OPT}\setminus \mathsf{ALG}$ such that $\mathsf{ALG}\setminus p_k\bigcup p^*$ is colorful. 
\end{proposition}
\begin{proof}
Observe that throughout the search algorithm, we maintain the property that $\mathsf{ALG}$ is colorful. 
Note that $\mathsf{ALG}\setminus p_k$ has $k-1$ different colors, and $\mathsf{OPT}$ has $k$ different colors. Thus there should be a point $p^*\in OPT$ whose color is different from all points in $\mathsf{ALG}\setminus p_k$. Note that such $p^*$ cannot belong to $\mathsf{ALG}$ and thus belongs to $\mathsf{OPT}\setminus\mathsf{ALG}$.
%Let $\overline{\mathsf{OPT}}=\mathsf{OPT}\setminus \mathsf{ALG}$. We partition $\overline{\mathsf{OPT}}$ into different color sets $\mathsf{OPT}\setminus \mathsf{ALG}=z_1\cup z_2...\cup z_m$. Simultaneously, we arrange $\mathsf{ALG}\setminus p_k \setminus \mathsf{OPT}$ into the same set of colors $\mathsf{ALG}\setminus p_k \setminus \mathsf{OPT}=z'_1\cup z'_2...\cup z'_m$. Because $|ALG|=|OPT|=k$ and $p_k\notin OPT$, we have $|\mathsf{ALG}\setminus p_k \setminus \mathsf{OPT}|<|\mathsf{OPT}\setminus\mathsf{ALG}|$, so there exists at least a color $i$ such that $|z'_i|<|z_i|$. Therefore, we can pick at least one point from $p^*\in z_i\setminus z'_i$, so that $\mathsf{ALG}\setminus p_k\bigcup p^*$ is $k$-colorful.
\end{proof}




\begin{lemma}\label{lm:update_color}
There always exists a point $p'\in N_{out}(p_k)$ (for $p_k$ as defined in Line 5) such that 
%(i) $\mathsf{ALG}\setminus p_k \bigcup p'$ is colorful; and (ii) $D(p',q)\le D(p_k,q)/\alpha+\mathsf{OPT_k}(1+1/\alpha)$
%\iffalse
\begin{enumerate}[topsep=0cm]
\setlength\itemsep{0cm}
\item $\mathsf{ALG}\setminus p_k \bigcup p'$ is colorful
\item $D(p',q)\le D(p_k,q)/\alpha+\mathsf{OPT_k}(1+1/\alpha)$
\end{enumerate}
%\fi
\end{lemma}
\begin{proof}
According to Proposition~\ref{prop:p_star_exists_diverse}, for any current solution $\mathsf{ALG}$ with $p_k\notin \mathsf{OPT}$, there exists a point $p^*\in \mathsf{OPT}\setminus \mathsf{ALG}$ such that $\mathsf{ALG}\setminus p_k\cup p^*$ is colorful. 
%Let $w\in \mathsf{ALG}$ be the closest point to $p^*$. 
If there exists an edge from $p_k$
%$w$ 
to $p^*$, replacing $p_k$ with $p^*$ is a potential update. We set $p'=p^*$ and $D(p',q)\le \mathsf{OPT_k}$ satisfies the distance upper bound above. 

Otherwise, we let $u$ be the point where $p^*$ was removed when processing $u$ on line 9 in Algorithm~\ref{alg:color-indexing}. Because $p^*$ was not connected from $p_k$, either there exists a point in $\mathsf{rep}[u]$ with the same color, or $\mathsf{rep}[u]$ has already got $k$ points with different colors. In the first case, we can set $p'$ to be the point in $\mathsf{rep}[u]$ with the same color. In the latter case, by pigeon hole principle, there always exists a color in $\mathsf{rep}[u]$ not shown in $\mathsf{ALG}\setminus p_k$. Therefore, we can find a desired $p'\in \mathsf{rep}[u]$ and it is connected to $p_k$.

We have proved that the $p'$ we found satisfies the colorful criteria. Now we will bound its distance upper bound.
\begin{align}
D(p',q)
&\le D(p^*,q)+D(p',p^*) \notag \\
&\le D(p^*,q)+D(p',u)+D(p^*,u) \notag\\
&\le D(p^*,q)+D(p_k,u)/(2\alpha)+D(p_k,u)/(2\alpha) \tag{Line 9 in Algorithm~\ref{alg:color-indexing}}\\
&\le D(p^*,q)+D(p_k,u)/\alpha \notag \\
&\le D(p^*,q)+D(p_k,p^*)/\alpha \tag{Because $u$ is ordered earlier than $p^*$ in Algorithm~\ref{alg:color-indexing}}\\
&\le D(p^*,q)+D(p_k,q)/\alpha+D(p^*,q)/\alpha \notag\\
&\le D(p_k,q)/\alpha+\mathsf{OPT_k}(1+1/\alpha)\notag
\end{align}
\end{proof}
\begin{proof}[Proof of Theorem~\ref{thm:colorful_ann}]
Regarding the running time, the total number of edges connected from any point in $\mathsf{ALG}$ is bounded by $|U|\le O(k(8\alpha)^d\log \Delta)$. In each step, the algorithm first sorts all these edges connected from $p_k\in\mathsf{ALG}$ and then checks whether each of them can be added to the new $\mathsf{ALG}$ set. The total time spent per step is bounded by $O(|U|\log|U|)$. The overall time complexity is $\tilde{O}\left(k(8\alpha)^d\log\Delta\right)$ per step.

To analyze the approximation ratio, at time step $t$, we use $\mathsf{ALG^t}=\{p^t_1,...,p^t_k\}$ to denote the current unordered solution. We denote $\mathsf{ALG^t_k}=\max\limits_{i\in[k]}D(p^t_i,q)$. According to Algorithm~\ref{alg:color-search} and Lemma~\ref{lm:update_color}, if $p_i$ is updated at time step $t$, we have $D(p^t_i,q)\le D(p^{t-1}_i,q)/\alpha+\mathsf{OPT_k}(1+1/\alpha)$. By an induction argument, if a point $p_i$ is updated by $t$ times at the end of time step $T$, we have $D(p^T_i,q)\le \frac{D(p^0_i,q)}{\alpha^t}+\frac{\alpha+1}{\alpha-1}\mathsf{OPT_k}$. 

We now prove that $\mathsf{ALG^T_k}\le \max\limits_{i}\frac{D(p^0_i,q)}{\alpha^{T/k}}+\frac{\alpha+1}{\alpha-1}\mathsf{OPT_k}$. Let $i\in[k]$ be the index achieving the maximal distance upper bound. For the sake of contradiction, if $\mathsf{ALG^T_k}>\frac{D(p^0_i,q)}{\alpha^{T/k}}+\frac{\alpha+1}{\alpha-1}\mathsf{OPT_k}$, this means that $p^T_i$ was updated for at most $T/k-1$ times. By a counting argument, there exists another index $j$ which was updated for at least $T/k+1$ times. However, at the time $t$ when $p^t_j$ was already updated for $T/k$ times, $D(p^t_j,q)\le \frac{D(p^0_j,q)}{\alpha^{T/k}}+\frac{\alpha+1}{\alpha-1}\mathsf{OPT_k} < \mathsf{ALG^T_k}\le \mathsf{ALG^t_k}$, so the algorithm wouldn't have chosen $p^t_j$ to optimize because it couldn't have had the maximal distance at that time, leading to a contradiction. Therefore, we prove that $\mathsf{ALG^T_k}\le \max\limits_{i}\frac{D(p^0_i,q)}{\alpha^{T/k}}+\frac{\alpha+1}{\alpha-1}\mathsf{OPT_k}$.

Now we consider the following three cases depending on the value of the maximal $D(p^0_i,q)$. This case analysis is similar to the proof in Theorem 3.4 from~\cite{indykxu2024worst}.
\begin{enumerate}

\item[Case 1:]  $D(p^0_i,q)>2D_{max}$. 

Let $p^*_k$ be the point having the maximal distance from $q$ in an optimal solution $\mathsf{OPT}$. We know that for any $p^0_i$, we have $D(p^*_k,q)\ge D(p^0_i,q)-D(p^0_i,p^*_k)\ge D(p^0_i,q)-D_{max}\ge D(p^0_i,q)/2$. Therefore, the approximation ratio after $T$ optimization steps is upper bounded by $\frac{\mathsf{ALG^T_k}}{D(p^*_k,q)}\le \frac{D(p^0_i,q)}{D(p^*_k,q)\alpha^{T/k}}+\frac{\alpha+1}{\alpha-1}\le \frac{2}{\alpha^{T/k}}+\frac{\alpha+1}{\alpha-1}$. A simple calculation shows that we can get a $(\frac{\alpha+1}{\alpha-1}+\epsilon)$ approximate solution in $O(k\log_{\alpha}\frac{2}{\epsilon})$ steps.


\item[Case 2:] $D(p^0_i,q)\le 2D_{max}$ and $\mathsf{OPT_k}>\frac{\alpha-1}{4(\alpha+1)}D_{min}$. 

To satisfy $\frac{D(p^0_i,q)}{\alpha^{T/k}}+\frac{\alpha+1}{\alpha-1}\mathsf{OPT_k}\le (\frac{\alpha+1}{\alpha-1}+\epsilon)\mathsf{OPT_k}$, we need $\frac{D(p^0_i,q)}{\alpha^{T/k}}\le \epsilon \mathsf{OPT_k}$. Applying the lower bound $\mathsf{OPT_k}\ge \frac{\alpha-1}{4(\alpha+1)}D_{min}$, we can get that $T\ge k\log_{\alpha}\frac{2(\alpha+1)\Delta}{(\alpha-1)\epsilon}$ suffices.


\item[Case 3:] $D(p^0_i,q)\le 2D_{max}$ and $\mathsf{OPT_k}\le \frac{\alpha-1}{4(\alpha+1)}D_{min}$. 

In this case, we must have $k=1$, because otherwise $D(p^*_k,p^*_1)\le 2D(p^*_k,q)<D_{min}$, violating the definition of $D_{min}$. Suppose $k=1$ and the problem degenerates to the standard nearest neighbor search problem. After $T$ optimization steps, if $p^T_1$ is still not the exact nearest neighbor, we have $D(p^T_1,q)\ge D(p^T_1,p^*_1)-\mathsf{OPT_1}\ge \frac{D_{min}}{2}$. Applying the upper bound of $D(p^T_1,q)$ and $\mathsf{OPT_1}$, we have $\frac{D_{min}}{2}\le D(p^T_1,q)\le \frac{D(p^0_1,q)}{\alpha^{T}}+\frac{\alpha+1}{\alpha-1}\mathsf{OPT_1}\le \frac{D(p^0_1,q)}{\alpha^{T}}+\frac{D_{min}}{4}$. This can happen only if $T\le \log_{\alpha}\frac{\Delta}{8}$.

In conclusion, $O(k\log_{\alpha}\frac{\Delta}{\epsilon})$ steps suffice to achieve the desired approximation ratio in Theorem~\ref{thm:diverse_ann}.
\end{enumerate}
\end{proof}

\subsection{High-level Intuition about the Generalizations}
%\snote{Piotr, please check here}
%PIOTR: done. looks good!
Given our results on colorful NN, it is relatively simple to extend them to the $k'$-colorful NN version with the same bounds. \emph{One key contribution is to demonstrate that, in the graph degree bound, the overhead factor $k$ can be reduced to  $k/k'$ while preserving the approximation quality}. This reduces both the query time bound and the overall space used by the algorithm. This improvement is tight, in the following sense: When $k' = 1$, we recover the bound for colorful NN problem, and when $k' = k$, we recover the standard $k$-NN bound, where no additional factor is needed.

For our algorithm to work with a generic diversity metric $\rho$, we use an intuition similar to that in colorful case. However, instead of pruning an edge from the point $p$ to the point $u$ when $p$ is already connected to representative vectors $v_1,\cdots,v_k$ of different colors, we now choose the representatives based on the diversity metric $\rho$. We find a diverse subset of points in the neighborhood of $u$ (e.g., using the greedy Gonzales algorithm for the k-center problem) and connect $p$ only to those selected points $v_1,\cdots,v_k$.
Again, the main challenge is to demonstrate that a greedy search algorithm can converge to an approximately optimal solution, given the set of edges we retain. The difficulty lies in the fact that we can only maintain an {\em approximately} diverse subset $\mathsf{ALG}$, in contrast to the colorful version, where we only needed $\mathsf{ALG}$ to contain $k$ different colors.  As the algorithm proceeds with further iterations, the technical difficulty lies in ensuring that the approximation factor does not grow depending on the number of iterations.
%we need to ensure that the approximation factor does not grow, and instead can be bounded by a constant not depending on the number of iterations.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%5
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\input{expirements}

\onecolumn

\bibliography{main}
\bibliographystyle{alpha}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\appendix


\input{generalized-alg}

%\input{otheralgos}

\input{heuristic}

\end{document}
