\section{Related Work}
\noindent \textbf{Language Bias in VQA.} Language bias has long been recognized as a challenging problem for conventional visual question answering (VQA). Previous methods in alleviating this problem can be roughly categorized into three groups: ensemble learning, contrastive learning, and loss re-scaling. Approaches in the first group~\cite{cadene2019rubi:rubi,clark2019don} introduce an additional bias branch which is trained with the original input in an ensemble manner. Contrastive learning-based debiasing methods~\cite{liang2020learning:97,si2022towards:137} first generate positive and negative samples using data augmentation techniques. These samples are then utilized to jointly optimize the model with a contrastive learning loss alongside the original classification loss. The last group methods~\cite{ijcai2021p98:ijcai,wu2019self:scr} address this problem with inspiration from class-imbalance mechanisms. To this end, each instance-aware loss is re-weighted based on training data statistics to achieve fair training. 

\noindent \textbf{Hallucination in LVLMs.}
Hallucination in LVLMs often refers that the generated textual responses are plausible but contradictory to the associated visual content~\cite{zhou2023analyzing:137,calibrated2024}. Some initial efforts have been devoted to building benchmarks to probe the hallucinatory level of LVLMs. For instance, CHAIR~\cite{rohrbach2018object:chairs} and GAVIE~\cite{liu2023mitigating:lrv} instruct models to generate a free-form caption to reveal their exposure to errors, POPE~\cite{li2023evaluating:pope}, HallusionBench~\cite{guan2024hallusionbench} and AutoHallusion~\cite{wu2024autohallusion} query models in terms of visual reasoning aspects with binary questions. Besides, hallucination mitigation has also attracted extensive interest recently. Some data augmentation methods like LRV-Instruction~\cite{liu2023mitigating:lrv} and HalluciDoctor~\cite{yu2024hallucidoctor} introduce additional negative and counterfactual data to fine-tune LVLMs. Other approaches propose to leverage contrastive decoding~\cite{leng2024mitigating:vcd,liu2024paying:pai,code2024} or reinforcement learning from human feedback~\cite{gunjal2024detecting:HalDetect,yu2024rlhf} to address this problem.
Overall, hallucination in LVLMs often manifests with multiple dimensions, wherein language bias contributes a significant factor.
As a result, performing language debiasing greatly assists the reduction in hallucination, therefore improving the reliability of LVLMs.

\noindent \textbf{Benchmarks for Video-Involved LVLMs.} The pervasiveness of LVLMs is accompanied by continual development in video-involved benchmarks. SEEDBench~\cite{li2024seed:seedbench} and Video-Bench~\cite{ning2023video:videobench} cover a wide variety of video-centric tasks and aim to provide a comprehensive evaluation for video understanding capabilities. However, some studies find that these general benchmarks suffer from the static spatial bias from single frames~\cite{dblei:singlebias}. To approach this, MVBench~\cite{li2024mvbench} and Tempcompass~\cite{liu2024tempcompass} curate video instances covering more temporal aspects such as speed, moving direction, attribute change, and event order. Besides, Video-MME~\cite{fu2024videomme} collects long videos that last up to one hour in duration. Unlike these benchmarks, we propose to evaluate LVLMs from the dimension of language bias, which we believe, constitutes an essential component for video understanding yet received no attention in the existing literature.

\begin{figure*}[ht]
\centering
\includegraphics[width=\textwidth]{filtering_all_v3.pdf}
\caption{VidLBEval quality control pipeline. i) We first filter out questions that can be answered correctly without referring to the associated video by utilizing several LLMs such as Qwen2. ii) External tools, i.e., Perspective API and GPT-4o/4V, are then employed for further safety checks. iii) Finally, we conduct human verification to review the results, leading to 1,695 high-quality samples for our VidLBEval dataset.}
\label{fig:filtering_all}
\end{figure*}