@article{abdin2024phi,
  title={Phi-3 technical report: A highly capable language model locally on your phone},
  author={Abdin, Marah and Aneja, Jyoti and Awadalla, Hany and Awadallah, Ahmed and Awan, Ammar Ahmad and Bach, Nguyen and Bahree, Amit and Bakhtiari, Arash and Bao, Jianmin and Behl, Harkirat and others},
  journal={arXiv preprint arXiv:2404.14219},
  year={2024}
}

@article{achiam2023gpt,
  title={Gpt-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@inproceedings{cadene2019rubi:rubi,
  author       = {R{\'{e}}mi Cad{\`{e}}ne and
                  Corentin Dancette and
                  H{\'{e}}di Ben{-}Younes and
                  Matthieu Cord and
                  Devi Parikh},
  title        = {RUBi: Reducing Unimodal Biases for Visual Question Answering},
  booktitle    = {NeurIPS},
  pages        = {839--850},
  year         = {2019}
}

@inproceedings{chen2024internvl,
  title={InternVL: Scaling up Vision Foundation Models and Aligning
for Generic Visual-Linguistic Tasks},
  author={Chen, Zhe and Wu, Jiannan and Wang, Wenhai and Su, Weijie and Chen, Guo and Xing, Sen and Zhong, Muyan and Zhang, Qinglong and Zhu, Xizhou and Lu, Lewei and others},
  booktitle={CVPR},
  pages={24185--24198},
publisher    = {{IEEE}},
  year={2024}
}

@article{cheng2024videollama:videollama2,
  title={VideoLLaMA 2: Advancing Spatial-Temporal Modeling and Audio Understanding in Video-LLMs},
  author={Cheng, Zesen and Leng, Sicong and Zhang, Hang and Xin, Yifei and Li, Xin and Chen, Guanzheng and Zhu, Yongxin and Zhang, Wenqi and Luo, Ziyang and Zhao, Deli and others},
  journal={arXiv preprint arXiv:2406.07476},
  year={2024}
}

@inproceedings{clark2019don,
  author       = {Christopher Clark and
                  Mark Yatskar and
                  Luke Zettlemoyer},
  title        = {Don't Take the Easy Way Out: Ensemble Based Methods for Avoiding Known Dataset Biases},
  booktitle    = {EMNLP-IJCNLP},
  pages        = {4067--4080},
  publisher    = {ACL},
  year         = {2019}
}

@inproceedings{dblei:singlebias,
  author       = {Jie Lei and
                  Tamara L. Berg and
                  Mohit Bansal},
  title        = {Revealing Single Frame Bias for Video-and-Language Learning},
  booktitle    = {ACL},
  pages        = {487--507},
  publisher    = {ACL},
  year         = {2023}
}

@article{dubey2024llama,
  title={The llama 3 herd of models},
  author={Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024}
}

@article{fu2024videomme,
  title={Video-MME: The First-Ever Comprehensive Evaluation Benchmark of Multi-modal LLMs in Video Analysis},
  author={Fu, Chaoyou and Dai, Yuhan and Luo, Yondong and Li, Lei and Ren, Shuhuai and Zhang, Renrui and Wang, Zihan and Zhou, Chenyu and Shen, Yunhang and Zhang, Mengdan and others},
  journal={arXiv preprint arXiv:2405.21075},
  year={2024}
}

@inproceedings{guan2024hallusionbench,
  title={HallusionBench: an advanced diagnostic suite for entangled language hallucination and visual illusion in large vision-language models},
publisher    = {{IEEE}},
  author={Guan, Tianrui and Liu, Fuxiao and Wu, Xiyang and Xian, Ruiqi and Li, Zongxia and Liu, Xiaoyu and Wang, Xijun and Chen, Lichang and Huang, Furong and Yacoob, Yaser and others},
  booktitle={CVPR},
  pages={14375--14385},
  year={2024}
}

@inproceedings{gunjal2024detecting:HalDetect,
  author       = {Anisha Gunjal and
                  Jihan Yin and
                  Erhan Bas},
  title        = {Detecting and Preventing Hallucinations in Large Vision Language Models},
  booktitle    = {AAAI},
  pages        = {18135--18143},
  publisher    = {{AAAI} Press},
  year         = {2024}
}

@inproceedings{ijcai2021p98:ijcai,
  title= {AdaVQA: Overcoming Language Priors with Adapted Margin Cosine Loss},
  author={Guo, Yangyang and Nie, Liqiang and Cheng, Zhiyong and Ji, Feng and Zhang, Ji and Del Bimbo, Alberto},
  booktitle={IJCAI},
publisher    = {ijcai.org},
  pages={708--714},
  year={2021}
}

@inproceedings{leng2024mitigating:vcd,
  title={Mitigating object hallucinations in large vision-language models through visual contrastive decoding},
  author={Leng, Sicong and Zhang, Hang and Chen, Guanzheng and Li, Xin and Lu, Shijian and Miao, Chunyan and Bing, Lidong},
  booktitle={CVPR},
  pages={13872--13882},
publisher    = {{IEEE}},
  year={2024}
}

@inproceedings{li2022contrastive:cd,
  author       = {Xiang Lisa Li and
                  Ari Holtzman and
                  Daniel Fried and
                  Percy Liang and
                  Jason Eisner and
                  Tatsunori Hashimoto and
                  Luke Zettlemoyer and
                  Mike Lewis},
  title        = {Contrastive Decoding: Open-ended Text Generation as Optimization},
  booktitle    = {{ACL}},
  pages        = {12286--12312},
  publisher    = {ACL},
  year         = {2023}
}

@inproceedings{li2022representation:Causal-VidQA,
  author       = {Jiangtong Li and
                  Li Niu and
                  Liqing Zhang},
  title        = {From Representation to Reasoning: Towards both Evidence and Commonsense
                  Reasoning for Video Question-Answering},
  booktitle    = {{CVPR}},
  pages        = {21241--21250},
  publisher    = {{IEEE}},
  year         = {2022}
}

@inproceedings{li2023blip,
  title={Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models},
  author={Li, Junnan and Li, Dongxu and Savarese, Silvio and Hoi, Steven},
  booktitle={ICML},
  pages={19730--19742},
  year={2023},
  organization={PMLR}
}

@inproceedings{li2023evaluating:pope,
  author       = {Yifan Li and
                  Yifan Du and
                  Kun Zhou and
                  Jinpeng Wang and
                  Wayne Xin Zhao and
                  Ji{-}Rong Wen},
  title        = {Evaluating Object Hallucination in Large Vision-Language Models},
  booktitle    = {{EMNLP}},
  pages        = {292--305},
  publisher    = {ACL},
  year         = {2023}
}

@inproceedings{li2024mvbench,
  title={Mvbench: A comprehensive multi-modal video understanding benchmark},
  author={Li, Kunchang and Wang, Yali and He, Yinan and Li, Yizhuo and Wang, Yi and Liu, Yi and Wang, Zun and Xu, Jilan and Chen, Guo and Luo, Ping and others},
  booktitle={CVPR},
publisher    = {{IEEE}},
  pages={22195--22206},
  year={2024}
}

@inproceedings{li2024seed:seedbench,
  title={SEED-Bench: Benchmarking Multimodal Large Language Models},
  author={Li, Bohao and Ge, Yuying and Ge, Yixiao and Wang, Guangzhi and Wang, Rui and Zhang, Ruimao and Shan, Ying},
  booktitle={CVPR},
publisher    = {{IEEE}},
  pages={13299--13308},
  year={2024}
}

@inproceedings{liang2020learning:97,
  title={Learning to contrast the counterfactual samples for robust visual question answering},
  author={Liang, Zujie and Jiang, Weitao and Hu, Haifeng and Zhu, Jiaying},
  booktitle={EMNLP},
  pages={3285--3292},
publisher    = {ACL},
  year={2020}
}

@inproceedings{lin2023video:videollava,
  author       = {Bin Lin and
                  Yang Ye and
                  Bin Zhu and
                  Jiaxi Cui and
                  Munan Ning and
                  Peng Jin and
                  Li Yuan},
  title        = {Video-LLaVA: Learning United Visual Representation by Alignment Before Projection},
  booktitle    = {EMNLP},
  pages        = {5971--5984},
  publisher    = {ACL},
  year         = {2024}
}

@article{liu2023aligning,
  title={Aligning large multi-modal model with robust instruction tuning},
  author={Liu, Fuxiao and Lin, Kevin and Li, Linjie and Wang, Jianfeng and Yacoob, Yaser and Wang, Lijuan},
  journal={arXiv preprint arXiv:2306.14565},
  year={2023}
}

@inproceedings{liu2023mitigating:lrv,
  author       = {Fuxiao Liu and
                  Kevin Lin and
                  Linjie Li and
                  Jianfeng Wang and
                  Yaser Yacoob and
                  Lijuan Wang},
  title        = {Mitigating Hallucination in Large Multi-Modal Models via Robust Instruction
                  Tuning},
  booktitle    = {ICLR},
  publisher    = {OpenReview.net},
  year         = {2024}
}

@inproceedings{liu2024improved,
  title={Improved baselines with visual instruction tuning},
  author={Liu, Haotian and Li, Chunyuan and Li, Yuheng and Lee, Yong Jae},
  booktitle={CVPR},
publisher={{IEEE}},
  pages={26296--26306},
  year={2024}
}

@inproceedings{liu2024paying:pai,
  title={Paying more attention to image: A training-free method for alleviating hallucination in lvlms},
  author={Liu, Shi and Zheng, Kecheng and Chen, Wei},
  booktitle={ECCV},
publisher   = {Springer},
  year={2024}
}

@inproceedings{liu2024tempcompass,
  author       = {Yuanxin Liu and
                  Shicheng Li and
                  Yi Liu and
                  Yuxiang Wang and
                  Shuhuai Ren and
                  Lei Li and
                  Sishuo Chen and
                  Xu Sun and
                  Lu Hou},
  title        = {TempCompass: Do Video LLMs Really Understand Videos?},
  booktitle    = {ACL (Findings)},
  pages        = {8731--8772},
  publisher    = {ACL},
  year         = {2024}
}

@inproceedings{maaz2023video,
  author       = {Muhammad Maaz and
                  Hanoona Abdul Rasheed and
                  Salman Khan and
                  Fahad Khan},
  title        = {Video-ChatGPT: Towards Detailed Video Understanding via Large Vision
                  and Language Models},
  booktitle    = {{ACL}},
  pages        = {12585--12602},
  publisher    = {ACL},
  year         = {2024}
}

@article{maaz2024videogpt+,
  title={VideoGPT+: Integrating Image and Video Encoders for Enhanced Video Understanding},
  author={Maaz, Muhammad and Rasheed, Hanoona and Khan, Salman and Khan, Fahad},
  journal={arXiv preprint arXiv:2406.09418},
  year={2024}
}

@inproceedings{mmstar2024,
  author       = {Lin Chen and
                  Jinsong Li and
                  Xiaoyi Dong and
                  Pan Zhang and
                  Yuhang Zang and
                  Zehui Chen and
                  Haodong Duan and
                  Jiaqi Wang and
                  Yu Qiao and
                  Dahua Lin and
                  Feng Zhao},
  title        = {Are We on the Right Way for Evaluating Large Vision-Language Models?},
  booktitle    = {NeurIPS},
  year         = {2024}
}

@article{ning2023video:videobench,
  title={Video-bench: A comprehensive benchmark and toolkit for evaluating video-based large language models},
  author={Ning, Munan and Zhu, Bin and Xie, Yujia and Lin, Bin and Cui, Jiaxi and Yuan, Lu and Chen, Dongdong and Yuan, Li},
  journal={arXiv preprint arXiv:2311.16103},
  year={2023}
}

@inproceedings{rohrbach2018object:chairs,
  author       = {Anna Rohrbach and
                  Lisa Anne Hendricks and
                  Kaylee Burns and
                  Trevor Darrell and
                  Kate Saenko},
  title        = {Object Hallucination in Image Captioning},
  booktitle    = {EMNLP},
  pages        = {4035--4045},
  publisher    = {ACL},
  year         = {2018}
}

@inproceedings{si2022towards:137,
  author       = {Qingyi Si and
                  Yuanxin Liu and
                  Fandong Meng and
                  Zheng Lin and
                  Peng Fu and
                  Yanan Cao and
                  Weiping Wang and
                  Jie Zhou},
  title        = {Towards Robust Visual Question Answering: Making the Most of Biased
                  Samples via Contrastive Learning},
  booktitle    = {EMNLP (Findings)},
  pages        = {6650--6662},
  publisher    = {ACL},
  year         = {2022}
}

@inproceedings{tong2022videomae,
  author       = {Zhan Tong and
                  Yibing Song and
                  Jue Wang and
                  Limin Wang},
  title        = {VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised
                  Video Pre-Training},
  booktitle    = {NeurIPS},
pages={10078--10093},
  year         = {2022}
}

@inproceedings{wu2019self:scr,
  author       = {Jialin Wu and
                  Raymond J. Mooney},
  title        = {Self-Critical Reasoning for Robust Visual Question Answering},
  booktitle    = {NeurIPS},
  pages        = {8601--8611},
  year         = {2019}
}

@inproceedings{wu2024autohallusion,
  author       = {Xiyang Wu and
                  Tianrui Guan and
                  Dianqi Li and
                  Shuaiyi Huang and
                  Xiaoyu Liu and
                  Xijun Wang and
                  Ruiqi Xian and
                  Abhinav Shrivastava and
                  Furong Huang and
                  Jordan L. Boyd{-}Graber and
                  Tianyi Zhou and
                  Dinesh Manocha},
  title        = {AutoHallusion: Automatic Generation of Hallucination Benchmarks for
                  Vision-Language Models},
  booktitle    = {EMNLP (Findings)},
  publisher    = {ACL},
  year         = {2024}
}

@inproceedings{wu2024star:starqa,
  author       = {Bo Wu and
                  Shoubin Yu and
                  Zhenfang Chen and
                  Josh Tenenbaum and
                  Chuang Gan},
  title        = {{STAR:} {A} Benchmark for Situated Reasoning in Real-World Videos},
  booktitle    = {NeurIPS Datasets and Benchmarks},
  year         = {2021}
}

@article{yang2024qwen2,
  title={Qwen2 technical report},
  author={Yang, An and Yang, Baosong and Hui, Binyuan and Zheng, Bo and Yu, Bowen and Zhou, Chang and Li, Chengpeng and Li, Chengyuan and Liu, Dayiheng and Huang, Fei and others},
  journal={arXiv preprint arXiv:2407.10671},
  year={2024}
}

@inproceedings{yi2019clevrer,
  author       = {Kexin Yi and
                  Chuang Gan and
                  Yunzhu Li and
                  Pushmeet Kohli and
                  Jiajun Wu and
                  Antonio Torralba and
                  Joshua B. Tenenbaum},
  title        = {{CLEVRER:} Collision Events for Video Representation and Reasoning},
  booktitle    = {ICLR},
  publisher    = {OpenReview.net},
  year         = {2020}
}

@inproceedings{yu2024hallucidoctor,
  title={Hallucidoctor: Mitigating hallucinatory toxicity in visual instruction data},
  author={Yu, Qifan and Li, Juncheng and Wei, Longhui and Pang, Liang and Ye, Wentao and Qin, Bosheng and Tang, Siliang and Tian, Qi and Zhuang, Yueting},
  booktitle={CVPR},
  pages={12944--12953},
publisher={IEEE},
  year={2024}
}

@inproceedings{yu2024rlhf,
  title={Rlhf-v: Towards trustworthy mllms via behavior alignment from fine-grained correctional human feedback},
  author={Yu, Tianyu and Yao, Yuan and Zhang, Haoye and He, Taiwen and Han, Yifeng and Cui, Ganqu and Hu, Jinyi and Liu, Zhiyuan and Zheng, Hai-Tao and Sun, Maosong and others},
  booktitle={CVPR},
publisher={IEEE},
  pages={13807--13816},
  year={2024}
}

@article{zhang2023next:nextood,
  author       = {Xi Zhang and
                  Feifei Zhang and
                  Changsheng Xu},
  title        = {NExT-OOD: Overcoming Dual Multiple-Choice {VQA} Biases},
  journal      = {TPAMI},
  pages        = {1913--1931},
  publisher={IEEE},
  year         = {2024}
}

@article{unkvqa,
  author       = {Yangyang Guo and
                  Fangkai Jiao and
                  Zhiqi Shen and
                  Liqiang Nie and
                  Mohan S. Kankanhalli},
  title        = {{UNK-VQA:} {A} Dataset and a Probe Into the Abstention Ability of Multi-Modal Large Models},
  journal      = {TPAMI},
  pages        = {10284--10296},
  year         = {2024},
  publisher={IEEE},
}

@inproceedings{zhou2023analyzing:137,
  author       = {Yiyang Zhou and
                  Chenhang Cui and
                  Jaehong Yoon and
                  Linjun Zhang and
                  Zhun Deng and
                  Chelsea Finn and
                  Mohit Bansal and
                  Huaxiu Yao},
  title        = {Analyzing and Mitigating Object Hallucination in Large Vision-Language
                  Models},
  booktitle    = {ICLR},
  publisher    = {OpenReview.net},
  year         = {2024}
}

@inproceedings{zhu2023minigpt,
  author       = {Deyao Zhu and
                  Jun Chen and
                  Xiaoqian Shen and
                  Xiang Li and
                  Mohamed Elhoseiny},
  title        = {MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large
                  Language Models},
  booktitle    = {ICLR},
  publisher    = {OpenReview.net},
  year         = {2024}
}

@inproceedings{yinyang,
  author       = {Peng Zhang and
                  Yash Goyal and
                  Douglas Summers{-}Stay and
                  Dhruv Batra and
                  Devi Parikh},
  title        = {Yin and Yang: Balancing and Answering Binary Visual Questions},
  booktitle    = {CVPR},
  pages        = {5014--5022},
  publisher    = {{IEEE} Computer Society},
  year         = {2016}
}

@inproceedings{calibrated2024,
  author       = {Yiyang Zhou and
                  Zhiyuan Fan and
                  Dongjie Cheng and
                  Sihan Yang and
                  Zhaorun Chen and
                  Chenhang Cui and
                  Xiyao Wang and
                  Yun Li and
                  Linjun Zhang and
                  Huaxiu Yao},
  title        = {Calibrated Self-Rewarding Vision Language Models},
  booktitle    = {NeurIPS},
  year         = {2024}
}

@inproceedings{code2024,
  author       = {Junho Kim and
                  Hyunjun Kim and
                  Yeonju Kim and
                  Yong Man Ro},
  title        = {CODE: Contrasting Self-generated Description to Combat Hallucination in Large Multi-modal Models},
  booktitle    = {NeurIPS},
  year         = {2024}
}

@article{videochat,
  author       = {Kunchang Li and
                  Yinan He and
                  Yi Wang and
                  Yizhuo Li and
                  Wenhai Wang and
                  Ping Luo and
                  Yali Wang and
                  Limin Wang and
                  Yu Qiao},
  title        = {VideoChat: Chat-Centric Video Understanding},
  journal      = {arXiv preprint arXiv:2305.06355},
  year         = {2023}
}

@inproceedings{gpt4v,
  title={GPT-4V(ision) System Card},
  author={OpenAI},
  year={2023},
  url={https://api.semanticscholar.org/CorpusID:263218031}
}

@misc{llava-next,
  title={Llava-next: Improved reasoning, ocr, and world knowledge},
  author={Liu, Haotian and Li, Chunyuan and Li, Yuheng and Li, Bo and Zhang, Yuanhan and Shen, Sheng and Lee, Yong Jae},
  year={2024}
}
