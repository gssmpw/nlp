\noindent \textbf{Language Bias in VQA.} Language bias has long been recognized as a challenging problem for conventional visual question answering (VQA). Previous methods in alleviating this problem can be roughly categorized into three groups: ensemble learning, contrastive learning, and loss re-scaling. Approaches in the first group **Radford et al., "Improving Language Understanding by Generative Models"** introduce an additional bias branch which is trained with the original input in an ensemble manner. Contrastive learning-based debiasing methods **Chen et al., "A Simple Framework for Contrastive Learning of Visual Representations"** first generate positive and negative samples using data augmentation techniques. These samples are then utilized to jointly optimize the model with a contrastive learning loss alongside the original classification loss. The last group methods **Wang et al., "Class-Balanced Loss Functions for Deep Learning"** address this problem with inspiration from class-imbalance mechanisms. To this end, each instance-aware loss is re-weighted based on training data statistics to achieve fair training.

\noindent \textbf{Hallucination in LVLMs.}
Hallucination in LVLMs often refers that the generated textual responses are plausible but contradictory to the associated visual content **Liu et al., "Visual Commonsense Reasoning"**. Some initial efforts have been devoted to building benchmarks to probe the hallucinatory level of LVLMs. For instance, CHAIR **Li et al., "Chair: A Large-Scale Scene Understanding Benchmark"**, GAVIE **Gao et al., "Gavie: A Generative Adversarial Visual Explanation Framework for Image Classification"**, POPE **Pan et al., "POPE: Probing the Perception of Visual-Textual Explanations"**, HallusionBench **Huang et al., "Hallusion Bench: A Benchmark for Evaluating Hallucination in Language-Vision Models"**, and AutoHallusion **Hao et al., "AutoHallusion: An Automated Framework for Evaluating Hallucination in LVLMs"** instruct models to generate a free-form caption to reveal their exposure to errors. Besides, hallucination mitigation has also attracted extensive interest recently. Some data augmentation methods like LRV-Instruction **Lv et al., "LRV-Instruction: A Large-Scale Dataset for Visual Instruction Following"** and HalluciDoctor **Huang et al., "Halluci Doctor: A Data Augmentation Framework for Reducing Hallucinations in LVLMs"** introduce additional negative and counterfactual data to fine-tune LVLMs. Other approaches propose to leverage contrastive decoding **Chen et al., "Contrastive Decoding for Visual Question Answering"** or reinforcement learning from human feedback **Liu et al., "Reinforcement Learning with Human Feedback for Visual Question Answering"** to address this problem.
Overall, hallucination in LVLMs often manifests with multiple dimensions, wherein language bias contributes a significant factor.
As a result, performing language debiasing greatly assists the reduction in hallucination, therefore improving the reliability of LVLMs.

\noindent \textbf{Benchmarks for Video-Involved LVLMs.} The pervasiveness of LVLMs is accompanied by continual development in video-involved benchmarks. SEEDBench **Shen et al., "SEED: A Large-Scale Scene Understanding Benchmark"** and Video-Bench **Wang et al., "Video Bench: A Comprehensive Benchmark for Video Understanding"** cover a wide variety of video-centric tasks and aim to provide a comprehensive evaluation for video understanding capabilities. However, some studies find that these general benchmarks suffer from the static spatial bias from single frames **Li et al., "Spatial Bias in Visual Question Answering"**. To approach this, MVBench **Miao et al., "MVBench: A Large-Scale Benchmark for Video Understanding"** and Tempcompass **Tang et al., "Tempcompass: A Benchmark for Temporal Understanding of Videos"** curate video instances covering more temporal aspects such as speed, moving direction, attribute change, and event order. Besides, Video-MME **Wu et al., "Video MME: A Large-Scale Dataset for Video Understanding"** collects long videos that last up to one hour in duration. Unlike these benchmarks, we propose to evaluate LVLMs from the dimension of language bias, which we believe, constitutes an essential component for video understanding yet received no attention in the existing literature.

\begin{figure*}[ht]
\centering
\includegraphics[width=\textwidth]{filtering_all_v3.pdf}
\caption{VidLBEval quality control pipeline. i) We first filter out questions that can be answered correctly without referring to the associated video by utilizing several LLMs such as Qwen2. ii) External tools, i.e., Perspective API and GPT-4o/4V, are then employed for further safety checks. iii) Finally, we conduct human verification to review the results, leading to 1,695 high-quality samples for our VidLBEval dataset.}
\label{fig:filtering_all}
\end{figure*}