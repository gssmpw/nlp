@inproceedings{cadene2019rubi:rubi,
  author       = {R{\'{e}}mi Cad{\`{e}}ne and
                  Corentin Dancette and
                  H{\'{e}}di Ben{-}Younes and
                  Matthieu Cord and
                  Devi Parikh},
  title        = {RUBi: Reducing Unimodal Biases for Visual Question Answering},
  booktitle    = {NeurIPS},
  pages        = {839--850},
  year         = {2019}
}

@inproceedings{calibrated2024,
  author       = {Yiyang Zhou and
                  Zhiyuan Fan and
                  Dongjie Cheng and
                  Sihan Yang and
                  Zhaorun Chen and
                  Chenhang Cui and
                  Xiyao Wang and
                  Yun Li and
                  Linjun Zhang and
                  Huaxiu Yao},
  title        = {Calibrated Self-Rewarding Vision Language Models},
  booktitle    = {NeurIPS},
  year         = {2024}
}

@inproceedings{clark2019don,
  author       = {Christopher Clark and
                  Mark Yatskar and
                  Luke Zettlemoyer},
  title        = {Don't Take the Easy Way Out: Ensemble Based Methods for Avoiding Known Dataset Biases},
  booktitle    = {EMNLP-IJCNLP},
  pages        = {4067--4080},
  publisher    = {ACL},
  year         = {2019}
}

@inproceedings{code2024,
  author       = {Junho Kim and
                  Hyunjun Kim and
                  Yeonju Kim and
                  Yong Man Ro},
  title        = {CODE: Contrasting Self-generated Description to Combat Hallucination in Large Multi-modal Models},
  booktitle    = {NeurIPS},
  year         = {2024}
}

@inproceedings{dblei:singlebias,
  author       = {Jie Lei and
                  Tamara L. Berg and
                  Mohit Bansal},
  title        = {Revealing Single Frame Bias for Video-and-Language Learning},
  booktitle    = {ACL},
  pages        = {487--507},
  publisher    = {ACL},
  year         = {2023}
}

@article{fu2024videomme,
  title={Video-MME: The First-Ever Comprehensive Evaluation Benchmark of Multi-modal LLMs in Video Analysis},
  author={Fu, Chaoyou and Dai, Yuhan and Luo, Yondong and Li, Lei and Ren, Shuhuai and Zhang, Renrui and Wang, Zihan and Zhou, Chenyu and Shen, Yunhang and Zhang, Mengdan and others},
  journal={arXiv preprint arXiv:2405.21075},
  year={2024}
}

@inproceedings{guan2024hallusionbench,
  title={HallusionBench: an advanced diagnostic suite for entangled language hallucination and visual illusion in large vision-language models},
publisher    = {{IEEE}},
  author={Guan, Tianrui and Liu, Fuxiao and Wu, Xiyang and Xian, Ruiqi and Li, Zongxia and Liu, Xiaoyu and Wang, Xijun and Chen, Lichang and Huang, Furong and Yacoob, Yaser and others},
  booktitle={CVPR},
  pages={14375--14385},
  year={2024}
}

@inproceedings{gunjal2024detecting:HalDetect,
  author       = {Anisha Gunjal and
                  Jihan Yin and
                  Erhan Bas},
  title        = {Detecting and Preventing Hallucinations in Large Vision Language Models},
  booktitle    = {AAAI},
  pages        = {18135--18143},
  publisher    = {{AAAI} Press},
  year         = {2024}
}

@inproceedings{ijcai2021p98:ijcai,
  title= {AdaVQA: Overcoming Language Priors with Adapted Margin Cosine Loss},
  author={Guo, Yangyang and Nie, Liqiang and Cheng, Zhiyong and Ji, Feng and Zhang, Ji and Del Bimbo, Alberto},
  booktitle={IJCAI},
publisher    = {ijcai.org},
  pages={708--714},
  year={2021}
}

@inproceedings{leng2024mitigating:vcd,
  title={Mitigating object hallucinations in large vision-language models through visual contrastive decoding},
  author={Leng, Sicong and Zhang, Hang and Chen, Guanzheng and Li, Xin and Lu, Shijian and Miao, Chunyan and Bing, Lidong},
  booktitle={CVPR},
  pages={13872--13882},
publisher    = {{IEEE}},
  year={2024}
}

@inproceedings{li2023evaluating:pope,
  author       = {Yifan Li and
                  Yifan Du and
                  Kun Zhou and
                  Jinpeng Wang and
                  Wayne Xin Zhao and
                  Ji{-}Rong Wen},
  title        = {Evaluating Object Hallucination in Large Vision-Language Models},
  booktitle    = {{EMNLP}},
  pages        = {292--305},
  publisher    = {ACL},
  year         = {2023}
}

@inproceedings{li2024mvbench,
  title={Mvbench: A comprehensive multi-modal video understanding benchmark},
  author={Li, Kunchang and Wang, Yali and He, Yinan and Li, Yizhuo and Wang, Yi and Liu, Yi and Wang, Zun and Xu, Jilan and Chen, Guo and Luo, Ping and others},
  booktitle={CVPR},
publisher    = {{IEEE}},
  pages={22195--22206},
  year={2024}
}

@inproceedings{li2024seed:seedbench,
  title={SEED-Bench: Benchmarking Multimodal Large Language Models},
  author={Li, Bohao and Ge, Yuying and Ge, Yixiao and Wang, Guangzhi and Wang, Rui and Zhang, Ruimao and Shan, Ying},
  booktitle={CVPR},
publisher    = {{IEEE}},
  pages={13299--13308},
  year={2024}
}

@inproceedings{liang2020learning:97,
  title={Learning to contrast the counterfactual samples for robust visual question answering},
  author={Liang, Zujie and Jiang, Weitao and Hu, Haifeng and Zhu, Jiaying},
  booktitle={EMNLP},
  pages={3285--3292},
publisher    = {ACL},
  year={2020}
}

@inproceedings{liu2023mitigating:lrv,
  author       = {Fuxiao Liu and
                  Kevin Lin and
                  Linjie Li and
                  Jianfeng Wang and
                  Yaser Yacoob and
                  Lijuan Wang},
  title        = {Mitigating Hallucination in Large Multi-Modal Models via Robust Instruction
                  Tuning},
  booktitle    = {ICLR},
  publisher    = {OpenReview.net},
  year         = {2024}
}

@inproceedings{liu2024paying:pai,
  title={Paying more attention to image: A training-free method for alleviating hallucination in lvlms},
  author={Liu, Shi and Zheng, Kecheng and Chen, Wei},
  booktitle={ECCV},
publisher   = {Springer},
  year={2024}
}

@inproceedings{liu2024tempcompass,
  author       = {Yuanxin Liu and
                  Shicheng Li and
                  Yi Liu and
                  Yuxiang Wang and
                  Shuhuai Ren and
                  Lei Li and
                  Sishuo Chen and
                  Xu Sun and
                  Lu Hou},
  title        = {TempCompass: Do Video LLMs Really Understand Videos?},
  booktitle    = {ACL (Findings)},
  pages        = {8731--8772},
  publisher    = {ACL},
  year         = {2024}
}

@article{ning2023video:videobench,
  title={Video-bench: A comprehensive benchmark and toolkit for evaluating video-based large language models},
  author={Ning, Munan and Zhu, Bin and Xie, Yujia and Lin, Bin and Cui, Jiaxi and Yuan, Lu and Chen, Dongdong and Yuan, Li},
  journal={arXiv preprint arXiv:2311.16103},
  year={2023}
}

@inproceedings{rohrbach2018object:chairs,
  author       = {Anna Rohrbach and
                  Lisa Anne Hendricks and
                  Kaylee Burns and
                  Trevor Darrell and
                  Kate Saenko},
  title        = {Object Hallucination in Image Captioning},
  booktitle    = {EMNLP},
  pages        = {4035--4045},
  publisher    = {ACL},
  year         = {2018}
}

@inproceedings{si2022towards:137,
  author       = {Qingyi Si and
                  Yuanxin Liu and
                  Fandong Meng and
                  Zheng Lin and
                  Peng Fu and
                  Yanan Cao and
                  Weiping Wang and
                  Jie Zhou},
  title        = {Towards Robust Visual Question Answering: Making the Most of Biased
                  Samples via Contrastive Learning},
  booktitle    = {EMNLP (Findings)},
  pages        = {6650--6662},
  publisher    = {ACL},
  year         = {2022}
}

@inproceedings{wu2019self:scr,
  author       = {Jialin Wu and
                  Raymond J. Mooney},
  title        = {Self-Critical Reasoning for Robust Visual Question Answering},
  booktitle    = {NeurIPS},
  pages        = {8601--8611},
  year         = {2019}
}

@inproceedings{wu2024autohallusion,
  author       = {Xiyang Wu and
                  Tianrui Guan and
                  Dianqi Li and
                  Shuaiyi Huang and
                  Xiaoyu Liu and
                  Xijun Wang and
                  Ruiqi Xian and
                  Abhinav Shrivastava and
                  Furong Huang and
                  Jordan L. Boyd{-}Graber and
                  Tianyi Zhou and
                  Dinesh Manocha},
  title        = {AutoHallusion: Automatic Generation of Hallucination Benchmarks for
                  Vision-Language Models},
  booktitle    = {EMNLP (Findings)},
  publisher    = {ACL},
  year         = {2024}
}

@inproceedings{yu2024hallucidoctor,
  title={Hallucidoctor: Mitigating hallucinatory toxicity in visual instruction data},
  author={Yu, Qifan and Li, Juncheng and Wei, Longhui and Pang, Liang and Ye, Wentao and Qin, Bosheng and Tang, Siliang and Tian, Qi and Zhuang, Yueting},
  booktitle={CVPR},
  pages={12944--12953},
publisher={IEEE},
  year={2024}
}

@inproceedings{yu2024rlhf,
  title={Rlhf-v: Towards trustworthy mllms via behavior alignment from fine-grained correctional human feedback},
  author={Yu, Tianyu and Yao, Yuan and Zhang, Haoye and He, Taiwen and Han, Yifeng and Cui, Ganqu and Hu, Jinyi and Liu, Zhiyuan and Zheng, Hai-Tao and Sun, Maosong and others},
  booktitle={CVPR},
publisher={IEEE},
  pages={13807--13816},
  year={2024}
}

@inproceedings{zhou2023analyzing:137,
  author       = {Yiyang Zhou and
                  Chenhang Cui and
                  Jaehong Yoon and
                  Linjun Zhang and
                  Zhun Deng and
                  Chelsea Finn and
                  Mohit Bansal and
                  Huaxiu Yao},
  title        = {Analyzing and Mitigating Object Hallucination in Large Vision-Language
                  Models},
  booktitle    = {ICLR},
  publisher    = {OpenReview.net},
  year         = {2024}
}

