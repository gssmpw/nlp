[
  {
    "index": 0,
    "papers": [
      {
        "key": "cadene2019rubi:rubi",
        "author": "R{\\'{e}}mi Cad{\\`{e}}ne and\nCorentin Dancette and\nH{\\'{e}}di Ben{-}Younes and\nMatthieu Cord and\nDevi Parikh",
        "title": "RUBi: Reducing Unimodal Biases for Visual Question Answering"
      },
      {
        "key": "clark2019don",
        "author": "Christopher Clark and\nMark Yatskar and\nLuke Zettlemoyer",
        "title": "Don't Take the Easy Way Out: Ensemble Based Methods for Avoiding Known Dataset Biases"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "liang2020learning:97",
        "author": "Liang, Zujie and Jiang, Weitao and Hu, Haifeng and Zhu, Jiaying",
        "title": "Learning to contrast the counterfactual samples for robust visual question answering"
      },
      {
        "key": "si2022towards:137",
        "author": "Qingyi Si and\nYuanxin Liu and\nFandong Meng and\nZheng Lin and\nPeng Fu and\nYanan Cao and\nWeiping Wang and\nJie Zhou",
        "title": "Towards Robust Visual Question Answering: Making the Most of Biased\nSamples via Contrastive Learning"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "ijcai2021p98:ijcai",
        "author": "Guo, Yangyang and Nie, Liqiang and Cheng, Zhiyong and Ji, Feng and Zhang, Ji and Del Bimbo, Alberto",
        "title": "AdaVQA: Overcoming Language Priors with Adapted Margin Cosine Loss"
      },
      {
        "key": "wu2019self:scr",
        "author": "Jialin Wu and\nRaymond J. Mooney",
        "title": "Self-Critical Reasoning for Robust Visual Question Answering"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "zhou2023analyzing:137",
        "author": "Yiyang Zhou and\nChenhang Cui and\nJaehong Yoon and\nLinjun Zhang and\nZhun Deng and\nChelsea Finn and\nMohit Bansal and\nHuaxiu Yao",
        "title": "Analyzing and Mitigating Object Hallucination in Large Vision-Language\nModels"
      },
      {
        "key": "calibrated2024",
        "author": "Yiyang Zhou and\nZhiyuan Fan and\nDongjie Cheng and\nSihan Yang and\nZhaorun Chen and\nChenhang Cui and\nXiyao Wang and\nYun Li and\nLinjun Zhang and\nHuaxiu Yao",
        "title": "Calibrated Self-Rewarding Vision Language Models"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "rohrbach2018object:chairs",
        "author": "Anna Rohrbach and\nLisa Anne Hendricks and\nKaylee Burns and\nTrevor Darrell and\nKate Saenko",
        "title": "Object Hallucination in Image Captioning"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "liu2023mitigating:lrv",
        "author": "Fuxiao Liu and\nKevin Lin and\nLinjie Li and\nJianfeng Wang and\nYaser Yacoob and\nLijuan Wang",
        "title": "Mitigating Hallucination in Large Multi-Modal Models via Robust Instruction\nTuning"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "li2023evaluating:pope",
        "author": "Yifan Li and\nYifan Du and\nKun Zhou and\nJinpeng Wang and\nWayne Xin Zhao and\nJi{-}Rong Wen",
        "title": "Evaluating Object Hallucination in Large Vision-Language Models"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "guan2024hallusionbench",
        "author": "Guan, Tianrui and Liu, Fuxiao and Wu, Xiyang and Xian, Ruiqi and Li, Zongxia and Liu, Xiaoyu and Wang, Xijun and Chen, Lichang and Huang, Furong and Yacoob, Yaser and others",
        "title": "HallusionBench: an advanced diagnostic suite for entangled language hallucination and visual illusion in large vision-language models"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "wu2024autohallusion",
        "author": "Xiyang Wu and\nTianrui Guan and\nDianqi Li and\nShuaiyi Huang and\nXiaoyu Liu and\nXijun Wang and\nRuiqi Xian and\nAbhinav Shrivastava and\nFurong Huang and\nJordan L. Boyd{-}Graber and\nTianyi Zhou and\nDinesh Manocha",
        "title": "AutoHallusion: Automatic Generation of Hallucination Benchmarks for\nVision-Language Models"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "liu2023mitigating:lrv",
        "author": "Fuxiao Liu and\nKevin Lin and\nLinjie Li and\nJianfeng Wang and\nYaser Yacoob and\nLijuan Wang",
        "title": "Mitigating Hallucination in Large Multi-Modal Models via Robust Instruction\nTuning"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "yu2024hallucidoctor",
        "author": "Yu, Qifan and Li, Juncheng and Wei, Longhui and Pang, Liang and Ye, Wentao and Qin, Bosheng and Tang, Siliang and Tian, Qi and Zhuang, Yueting",
        "title": "Hallucidoctor: Mitigating hallucinatory toxicity in visual instruction data"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "leng2024mitigating:vcd",
        "author": "Leng, Sicong and Zhang, Hang and Chen, Guanzheng and Li, Xin and Lu, Shijian and Miao, Chunyan and Bing, Lidong",
        "title": "Mitigating object hallucinations in large vision-language models through visual contrastive decoding"
      },
      {
        "key": "liu2024paying:pai",
        "author": "Liu, Shi and Zheng, Kecheng and Chen, Wei",
        "title": "Paying more attention to image: A training-free method for alleviating hallucination in lvlms"
      },
      {
        "key": "code2024",
        "author": "Junho Kim and\nHyunjun Kim and\nYeonju Kim and\nYong Man Ro",
        "title": "CODE: Contrasting Self-generated Description to Combat Hallucination in Large Multi-modal Models"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "gunjal2024detecting:HalDetect",
        "author": "Anisha Gunjal and\nJihan Yin and\nErhan Bas",
        "title": "Detecting and Preventing Hallucinations in Large Vision Language Models"
      },
      {
        "key": "yu2024rlhf",
        "author": "Yu, Tianyu and Yao, Yuan and Zhang, Haoye and He, Taiwen and Han, Yifeng and Cui, Ganqu and Hu, Jinyi and Liu, Zhiyuan and Zheng, Hai-Tao and Sun, Maosong and others",
        "title": "Rlhf-v: Towards trustworthy mllms via behavior alignment from fine-grained correctional human feedback"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "li2024seed:seedbench",
        "author": "Li, Bohao and Ge, Yuying and Ge, Yixiao and Wang, Guangzhi and Wang, Rui and Zhang, Ruimao and Shan, Ying",
        "title": "SEED-Bench: Benchmarking Multimodal Large Language Models"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "ning2023video:videobench",
        "author": "Ning, Munan and Zhu, Bin and Xie, Yujia and Lin, Bin and Cui, Jiaxi and Yuan, Lu and Chen, Dongdong and Yuan, Li",
        "title": "Video-bench: A comprehensive benchmark and toolkit for evaluating video-based large language models"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "dblei:singlebias",
        "author": "Jie Lei and\nTamara L. Berg and\nMohit Bansal",
        "title": "Revealing Single Frame Bias for Video-and-Language Learning"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "li2024mvbench",
        "author": "Li, Kunchang and Wang, Yali and He, Yinan and Li, Yizhuo and Wang, Yi and Liu, Yi and Wang, Zun and Xu, Jilan and Chen, Guo and Luo, Ping and others",
        "title": "Mvbench: A comprehensive multi-modal video understanding benchmark"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "liu2024tempcompass",
        "author": "Yuanxin Liu and\nShicheng Li and\nYi Liu and\nYuxiang Wang and\nShuhuai Ren and\nLei Li and\nSishuo Chen and\nXu Sun and\nLu Hou",
        "title": "TempCompass: Do Video LLMs Really Understand Videos?"
      }
    ]
  },
  {
    "index": 18,
    "papers": [
      {
        "key": "fu2024videomme",
        "author": "Fu, Chaoyou and Dai, Yuhan and Luo, Yondong and Li, Lei and Ren, Shuhuai and Zhang, Renrui and Wang, Zihan and Zhou, Chenyu and Shen, Yunhang and Zhang, Mengdan and others",
        "title": "Video-MME: The First-Ever Comprehensive Evaluation Benchmark of Multi-modal LLMs in Video Analysis"
      }
    ]
  }
]