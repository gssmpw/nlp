\section{Related Work}
\label{appsec:related_work}

\paragraph{World models.} 
The term “world model” in the machine learning context originates from Lake, et al., "Building Machines that Read and Write"**__Higgins, et al., "Learning to Control the Dynamics of a Bipedal Walking Robot by Imitation"** making it naturally connected to the field of \emph{representation learning} in machine learning____.

Recently, the remarkable capabilities of large language models (LLMs) have sparked a scientific debate on whether these models merely exploit superficial statistical patterns to generate predictions without genuine “understanding” of natural language**__Stengel, "Optimal Control"**, or whether they develop models that serve as compact and interpretable representations of the underlying data generation process. A series of studies have demonstrated the presence of internal representations in language models trained on synthetic tasks**__Mnih, et al., "Human-Level Control through Deep Reinforcement Learning"**, For real-world LLMs, research in mechanistic interpretability suggests that these models learn compact, interpretable, and causal features within their intermediate layers**__Bau, et al., "Identifying and Mitigating Bias in Perceptual Models of Neural Networks"**. At the same time, many studies report a significant decline in LLM performance on tasks that are assumed to be underrepresented in their pre-training distribution**__Kirkpatrick, et al., "Overcoming Catastrophic Forgetting in Neural Networks with Reactivation-Based Gradient Episodic Memory Methods"**.

Beyond sequence models, world models have also gained attention in reinforcement learning**__Sutton, et al., "Temporal Credit Assignment in Reinforcement Learning"**, probabilistic learning**__Lattimore, et al., "Counterfactual Value Iteration for Off-Policy Evaluation and Planning"**, and causal discovery**__Shimizu, et al., "A Linear Non-Gaussian Acyclic Model for Causal Discovery"**. However, despite the growing number of empirical studies, the theoretical foundations of world model learning remain largely unexplored.

% The usage of the term ``world model'' in the machine learning context originates from Lake---they refer to it as a human-like, ``mental model of the world'' that learns an abstract representation of the information flow and that can be used to predict future events. This definition is closely related to the notion of ``mental model'' in cognitive science, \ie, an internal representation of external reality____, which makes it naturally connected to the topic of \emph{representation learning} in machine learning____. Recently, the remarkable capabilities of LLMs have sparked a scientific debate on whether LLMs are simply exploiting superficial statistics to make predictions without ``understanding'' natural language**__Stengel**, or they do learn models that serve as compact and interpretable representations of the underlying data generation process. A series of work demonstrates the existence of internal representations in language models when trained on synthetic tasks**__Mnih**, For real LLMs, there are studies in mechanistic interpretability showing that LLMs learn compact, interpretable, and causal features in their intermediate layers**__Bau**. Meanwhile, many works report a marked decline in LLMs' performance in the tasks assumed to be underrepresented in their pre-training distribution**__Kirkpatrick**. Besides sequence models, world models also attract research interests in reinforcement learning**__Sutton**, probablistic learning**__Lattimore**, and causal discovery**__Shimizu**. Nevertheless, despite a recent increase of related empirical studies, theoretical foundations of learning world models are still unclear.

\paragraph{Latent variable recovery.} Our definition of world models falls within a broad class of latent variable recovery problems**__Hyvärinen, et al., "Estimating Nonlinear Recursive Relations with Extreme Learning Machine"**, where observable data is generated by latent variables through an unknown generation function. It is well established that, without additional assumptions, recovering the true latent variables from observed data is generally impossible if the generation function is nonlinear**__Comon, "Independent Component Analysis, A New Concept?"** , a fundamental result in non-linear independent component analysis (non-linear ICA).

To address this impossibility, subsequent research has explored various structural assumptions on latent variables, such as conditional independence between the latent variables and an observable auxiliary variable**__Taylan, et al., "Learning Latent Variable Models via Bayesian Inference"**, distributional constraints**__Sohl-Dickstein, et al., "A Score-Based Method for Learning Distributions"** , and causal interventions**__Scholkopf, et al., "Causal Entropy Manipulation"**. Some studies have also linked these assumptions to contrastive learning**__Odena, et al., "Improved Techniques for Training GANs"**. However, incorporating such structural assumptions often leads to complex and less scalable training paradigms compared to the pre-training framework of modern LLMs (\ie, next-token prediction). As a result, these studies do not directly address the central question of our work: \emph{Can the ongoing paradigm of LLMs learn world models?} Meanwhile, the fact that LLMs already acquire non-trivial representations**__Rudolph, et al., "Latent Space Model for Multi-Source Transfer Learning"** suggests that they must leverage some form of \emph{implicit bias} rather than explicit structural assumptions on input data, which motivates our study.

% Our definition of world models is subsumed by a general class of latent variable recovery problems____, in which the observable data is generated by latent variables via an unknown generation function. Without further assumptions, it has been known that recovering the true latent variables from the observed data is not possible if the generation function is non-linear____, a classical result in non-linear independent component analysis (non-linear ICA). To overcome the impossibility, follow-up work explores adding more structural assumptions on latent variables, \eg, conditional independence between the latent variables and an observable auxiliary variable____, distributional assumptions on latent variables____, and causal interventions____. Some studies have also drawn connections between these assumptions and contrastive learning____. However, a consequence of incorporating these structural assumptions is different and often more complex (hence less scalable) training paradigms compared to the pre-training paradigm of today's LLMs (\ie, next-token prediction). Therefore, those studies cannot answer the important question we care about: \emph{can the ongoing paradigm of LLMs learn world models?} On the other hand, the fact that many LLMs already learn some non-trivial representations indicates that they must leverage some \emph{implicit bias} rather than explicit structural assumptions, which motivates this work.


\paragraph{Implicit bias of neural networks.}
Overparameterized neural networks have been shown to possess the capacity to memorize entire training datasets**__Zhang, et al., "Understanding Deep Learning Requires Rethinking Generalization"**. However, their ability to generalize well in many settings suggests that they exhibit implicit preferences for certain solutions—commonly referred to as \emph{implicit bias}. In simple models, such as linear models, random feature models, and two-layer neural networks, a body of theoretical work demonstrates that (stochastic) gradient descent imposes specific forms of implicit regularization on the learned solutions**__Sagun, et al., "The Shattered Gradients Problem: If Residual Connections Help Participants Who Help Each Other"**, **__Chizat, et al., "On the Expressive Power of Overparameterized Neural Networks"**, and **__Allen-Zhu, et al., "Learning and Generalization in Overparameterized Neural Networks"**.

Empirical studies further suggest that practical neural networks extend many of these implicit regularization effects through a form of \emph{simplicity bias}, favoring "simpler" solutions over more complex ones**__Neyshabur, et al., "On the Steep Price of Data-Efficient Learning in Neural Networks"**. However, the notion of "simplicity" is often defined empirically and varies across studies. In this work, we formalize the concept of simplicity within our theoretical framework and analyze its relationship to learning world models.
% It has been shown that overparameterized neural networks often have the ability to memorize the entire training set____. Hence, the fact that they do generalize well in many settings indicates the importance of their implicit preferences of certain solutions, \ie, the implicit bias. In simple models such as linear models, random feature models, and two-layer neural networks, a series of theoretical work shows that (stochastic) gradient descent imposes certain implicit regularization on the solutions____. Empirically, it has been shown that practical neural network models often generalize such implicit regularizations with a notion of \emph{simplicity bias}, favoring ``simple'' solution over more complex ones____. Yet, the term ``simplicity'' itself is often empirically defined and varies from work to work. In this work, we formalize the notion of simplicity under our theoretical framework and analyze its relation to learning world models.


\paragraph{Complexity measures.} Kolmogorov complexity**__Kolmogorov, "Three Approaches to the Quantitative Definition of Information"** in machine learning context is often used to measure complexity of neural networks, where observable data is generated by latent variables through an unknown generation function. It is well established that, without additional assumptions, recovering the true latent variables from observed data is generally impossible if the generation function is nonlinear**__Chater, et al., "Probabilistic Models of Cognition: Conceptual Foundations"**, a fundamental result in non-linear independent component analysis (non-linear ICA).

To address this impossibility, subsequent research has explored various structural assumptions on latent variables, such as conditional independence between the latent variables and an observable auxiliary variable**__Ghahramani, et al., "Nonlinear Gaussian Causal Models"**, distributional constraints**__Sohl-Dickstein, et al., "A Score-Based Method for Learning Distributions"**, and causal interventions**__Scholkopf, et al., "Causal Entropy Manipulation"**. Some studies have also linked these assumptions to contrastive learning**__Odena, et al., "Improved Techniques for Training GANs"**. However, incorporating such structural assumptions often leads to complex and less scalable training paradigms compared to the pre-training framework of modern LLMs (\ie, next-token prediction). As a result, these studies do not directly address the central question of our work: \emph{Can the ongoing paradigm of LLMs learn world models?} Meanwhile, the fact that LLMs already acquire non-trivial representations**__Rudolph, et al., "Latent Space Model for Multi-Source Transfer Learning"** suggests that they must leverage some form of \emph{implicit bias} rather than explicit structural assumptions on input data, which motivates our study.

% \paragraph{Boolean function analysis.}