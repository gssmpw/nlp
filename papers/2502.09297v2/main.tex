%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

\input{math_commands}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\graphicspath{{./figures/}}

\usepackage{subcaption}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}
\usepackage{float}

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{enumitem}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{example}[theorem]{Example}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

\newcommand{\eg}{{e.g.}}
\newcommand{\ie}{{i.e.}}
\newcommand{\defeq}{\vcentcolon=}
\renewcommand{\eqref}[1]{(\ref{#1})}
\newcommand{\supp}{\mathrm{supp}}
\renewcommand{\deg}{\mathrm{deg}}
\renewcommand{\inf}{\mathrm{Inf}}
\newcommand{\prob}{\mathbf{Pr}}
\newcommand{\impdeg}{\widehat{\mathrm{deg}}}
\newcommand{\hmin}{\mathcal{H}_\mathrm{min}}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{When do Neural Networks Learn World Models?}

\begin{document}

\twocolumn[
\icmltitle{When do Neural Networks Learn World Models?}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Tianren Zhang}{thu}
\icmlauthor{Guanyu Chen}{thu}
\icmlauthor{Feng Chen}{thu}
% \icmlauthor{Firstname4 Lastname4}{sch}
% \icmlauthor{Firstname5 Lastname5}{yyy}
% \icmlauthor{Firstname6 Lastname6}{sch,yyy,comp}
% \icmlauthor{Firstname7 Lastname7}{comp}
% %\icmlauthor{}{sch}
% \icmlauthor{Firstname8 Lastname8}{sch}
% \icmlauthor{Firstname8 Lastname8}{yyy,comp}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{thu}{Department of Automation, Tsinghua University, Beijing, China}
% \icmlaffiliation{comp}{Company Name, Location, Country}
% \icmlaffiliation{sch}{School of ZZZ, Institute of WWW, Location, Country}

\icmlcorrespondingauthor{Feng Chen}{chenfeng@mail.tsinghua.edu.cn}
% \icmlcorrespondingauthor{Firstname2 Lastname2}{first2.last2@www.uk}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{World Models, Representation Learning, Learning Theory, Machine Learning}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
% \printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
% \textbf{TODO:abstract}
Humans develop \emph{world models} that capture the underlying generation process of data. Whether neural networks can learn similar world models remains an open problem. In this work, we provide the first theoretical results for this problem, showing that in a \emph{multi-task} setting, models with a \emph{low-degree bias} provably recover latent data-generating variables under mild assumptions---even if proxy tasks involve complex, non-linear functions of the latents. However, such recovery is also sensitive to model architecture. Our analysis leverages Boolean models of task solutions via the Fourier-Walsh transform and introduces new techniques for analyzing invertible Boolean transforms, which may be of independent interest. We illustrate the algorithmic implications of our results and connect them to related research areas, including self-supervised learning, out-of-distribution generalization, and the linear representation hypothesis in large language models.
% This document provides a basic paper template and submission guidelines.
% Abstracts must be a single paragraph, ideally between 4--6 sentences long.
% Gross violations will trigger corrections at the camera-ready phase.
\end{abstract}

% \vspace{-1.75em}
\section{Introduction}
\label{sec:intro}
% \vspace{-0.25em}

Humans develop internal models of the world, extracting core concepts that generate perceptual data~\citep{ha_world_2018}. Can neural networks do the same? With recent advances in large language models (LLMs), this question has garnered increasing attention~\citep{bender_dangers_2021,mitchell_ai_2023}. Understanding if and how neural networks learn human-like world models is crucial for building AI systems that are robust, fair, and aligned with human values~\citep{hendrycks_overview_2023}.

Empirical findings on world model learning have been mixed. Some studies suggest that medium-sized neural networks~\citep{mikolov_linguistic_2013} and LLMs~\citep{li_emergent_2023,bricken2023towards,gurnee_language_2024} learn abstract and interpretable features, indicating a non-trivial representation of data generation. Others, however, report a marked decline in LLM performance on novel tasks~\citep{wu_reasoning_2023,berglund_reversal_2024,mirzadeh_gsm-symbolic_2024}, implying a lack of genuine world representations that enable human-level generalization in out-of-distribution settings.

Despite ongoing research, the theoretical foundations of learning world models remain unclear. Notably, even the term ``world model'' lacks a precise definition. This gives rise to several fundamental questions: what does it mean for neural networks to learn world models? When and why can they do so? More fundamentally, what constitutes a bona fide world model?

The \textbf{goal} of this work is to address these problems by introducing a formal framework for world model learning and presenting the first theoretical results in this area. Following the spirit of prior work~\citep{ha_world_2018,li_emergent_2023,gurnee_language_2024}, we first show that latent variable models~\citep{everett2013introduction} provide a natural scaffold for formulating world model learning. Specifically, learning world models can be framed as achieving a non-trivial recovery of latent data-generating variables. However, a core challenge in this formulation arises from a well-known negative result showing that recovering true latents is generally impossible due to a fundamental issue of \emph{non-identifiability}~\citep{hyvarinen_nonlinear_1999}. That is, multiple solutions can fit the observed variables equally well, making true latent variables non-identifiable from observed data alone.

At first glance, the non-identifiability of latent variables may suggest a pessimistic outlook on learning world models. However, existing results overlook an important point: solutions that equally fit the data are not necessarily equivalent as \emph{functions}. Thus, algorithms with implicit bias in function space, such as those employed in deep learning~\citep{kalimeris_sgd_2019,goyal_inductive_2020}, would favor certain solutions over others. In particular, we focus on a bias towards \emph{low-complexity} functions, a phenomenon widely observed in neural networks and believed to be a key factor in the success of deep learning~\citep{perez_deep_2019,huh_platonic_2024,goldblum_position_2024}. Yet, due to the lack of a well-established complexity measure for continuous functions, formalizing such complexity bias and analyzing its impact remains a challenging problem on its own (see Section~\ref{appsec:related_work} for related work).

In this work, we circumvent this challenge by leveraging a simple yet important fact: while real-world data and latent variables may be continuous, all variables processed by neural networks are ultimately encoded as bit strings due to finite precision of computers.
\footnote{Note that this differs from neural network quantization~\citep{ nagel_white_2021,gholami_survey_2021}.} 
This allows us to model all variables as \emph{Boolean} without loss of generality. While this may seem a subtle difference (since we only lose the information that goes beyond machine precision), as we demonstrate in later sections, it turns out to provide surprisingly powerful machinery for defining and analyzing the \emph{complexity} of solutions via the Fourier-Walsh transform of Boolean functions~\citep{odonnell_analysis_2021}. Building on this foundation, we present, for the first time, a nuanced perspective on learning world models that reveals an interplay between the low-complexity bias, proxy tasks, and model architecture.
Our \textbf{main contributions} are:
\begin{enumerate}[leftmargin=1.25em]
\item In Section~\ref{sec:formulation}, we lay down general definitions of learning world models and discuss its core challenge posed by the non-identifiability of latent data-generating variables. This provides a foundation for future work to formally reason about learning world models and offers theoretical rigor to the recent scientific debate on this topic~\citep{bender_dangers_2021,mitchell_ai_2023}.

\item In Section~\ref{sec:preliminaries}, we introduce complexity measures based on a notion of \emph{realization degree}, offering an approach for analyzing the impact of low-complexity bias on world model learning.

\item In Section~\ref{sec:main}, we present the first theoretial results on learning world models in the context of training on proxy tasks using observed data. We identify two critical factors for world model learning: (\romannumeral 1) a \emph{multi-task} setting; (\romannumeral 2) the low-complexity bias, instantiated by a \emph{low-degree bias} of the model and a \emph{low-degree task distribution}. Together, these factors ensure the identifiability of latent data-generating variables.
Moreover, we show the provable benefits of learning world models in an out-of-distribution generalization setting~\citep{abbe_generalization_2023} and study the impact of model architecture under a notion of \emph{basis compatibility} (see Figure~\ref{fig:summary} for a graphical summary of our results). Technically, our analysis relies on analyzing the degree properties of Boolean functions composed with invertible transforms, which may be of independent interest.

\item In Section~\ref{sec:implications}, we illustrate the algorithmic implications of our results on two representative tasks: polynomial extrapolation~\citep{xu_how_2021} and learning physical laws~\citep{kang_how_2024}. We show that architectures inspired by our analysis outperform conventional architectures such as ReLU MLPs and transformers~\citep{vaswani_attention_2017} in these tasks.
\end{enumerate}


\begin{figure}[t]
\centering
\includegraphics[width=0.95\linewidth]{figure_motivation2.pdf}
% \vspace{-0.5em}
\caption{A graphical summary of our framework and main results.}
\label{fig:summary}
% \vspace{-0.7em}
\end{figure}


\section{Formulation of Learning World Models}
\label{sec:formulation}


How to define the world model and the problem of learning world models remains debatable to date. Yet, the term ``world models'' has been widely referred to in the literature as models that uncover the underlying \emph{generation process} of data and maintain a representation of it~\citep{ha_world_2018,gurnee_language_2024,richensrobust}. For example, pioneering works by~\citet{li_emergent_2023} and~\citet{nanda_emergent_2023} define ``world models'' in board games as the board state that generates move sequences. This motivates a formulation of world model learning under the framework of latent variable models~\citep{everett2013introduction}. To this end, we first define a general data generation process.

\begin{definition}[Data generation process]
\label{def:dgm}
Let $\rvx\in\gX$ be the observed data variables and let $\rvz\in\gZ$ be the latent variables for some data space $\gX$ and latent space $\gZ$. The observed data are sampled as follows: (\romannumeral 1) sample $\rvz\sim p(\rvz)$ for some probability distribution $p$ over $\gZ$; (\romannumeral 2) generate $\rvx$ through an invertible and non-linear function $\rvx = \psi(\rvz)$.
\end{definition}

This definition resembles the data generation process used in many latent variable models such as non-linear ICA~\citep{hyvarinen_nonlinear_1999}, invariant feature learning~\citep{arjovsky_invariant_2019}, and causal representation learning~\citep{scholkopf_toward_2021}. The main difference is that unlike these models, we do not assume $p(\rvz)$ to be any structured distribution. Intuitively, a natural way to formalize ``understanding'' this generation process is to approximately \emph{invert} it, \ie, recover the latent variables $\rvz$ from the observed data $\rvx$.
This gives our basic formulation of learning world models.

\begin{definition}[Learning world models]
\label{def:world_model}
Let $\gT$ be a set of transforms $T:\gZ\to\gZ$. We say a representation $\Phi:\gX\to\gZ$ learns the world model up to $\gT$ if there exists $T\in\gT$ such that $\Phi(\vx) = T(\vz)$ for every $\vz\in\supp(p)$ and $\vx = \psi(\vz)$.
\end{definition}

For instance, if $\gT$ only contains the identity transform, then $\Phi(\rvx)$ recovers $\rvz$ exactly. In general, we require $\gT$ to contain only simple transform classes (\eg, linear transforms) for a meaningful recovery.


The main difficulty of this latent variable recovery problem is that $\rvz$ is, by definition, unobservable. To address this, we leverage the fact that models in practice are trained on some proxy tasks (\eg, next-token prediction) to learn representations of $\rvx$ implicitly. We formulate this as follows.

\begin{definition}[Task and realization]
\label{def:task}
A \emph{task} is defined as a function $h:\gX\to\sR$. For a task $h$, let
\begin{equation}
\begin{aligned}
\gH(h) \vcentcolon= \{&f:\gX\to\sR\mid\ f(\vx) = h(\vx),\\
&\qquad \forall \vz\in\supp(p),\vx=\psi(\vz)\}.
\end{aligned}
\end{equation}
If a function composition $h_{(1)}\circ\cdots\circ h_{(q)}$ is in $\gH(h)$, then we say it is a \emph{realization} of $h$.
\end{definition}


\textbf{Flat and hierarchical realizations.} With definitions above, our main hope is that by training on some proper tasks $h$, the model can learn a \emph{hierarchical realization} $g\circ \Phi\in\gH(h)$ with a function $g:\gZ\to\sR$ and a representation $\Phi$ that learns the world model in the sense of Definition~\ref{def:world_model}. However, a key challenge arises: by definition, all realizations have zero training error on $\supp(p)$ and are thus indistinguishable by their task performance. For example, every function $h^*\in\gH(h)$ is itself a \emph{flat realization} of $h$, \ie, without any explicit representation learning. Thus, by looking at the observable data alone, we have no reason to expect that $g\circ\Phi$ will be favored over $h^*$. Likewise, it is also unreasonable to expect that $g\circ\Phi$ should be favored over another hierarchical realization $g'\circ\Phi'\in\gH(h)$ with $\Phi'$ not learning world models.
Indeed, a well-known impossibility result in latent variable modeling shows that the true latent variables are \emph{non-identifiable} when $\psi$ is a sufficiently flexible non-linear function of the latents~\citep{hyvarinen_nonlinear_1999,khemakhem_variational_2020}.

\begin{lemma}[Non-identifiability~\citep{khemakhem_variational_2020}]
\label{lemma:non_identifiability}
Let $\gZ = \sR^d$ and $\rvz\in\gZ$ be a random vector of any distribution. Then, there exists an invertible transform $T:\gZ\to\gZ$ such that the components of $\rvz'=T(\rvz)$ are independent, standard Gaussian variables.
\end{lemma}

Lemma~\ref{lemma:non_identifiability} indicates that we can construct new random variables $\Phi(\rvx) = \rvz'$ that have the same distribution $p$ as true latent variables $\psi^{-1}(\rvx) = \rvz$ (thus fitting the observed variables $\rvx$ equally well) by first transforming $\rvz$ to standard Gaussian variables, applying any orthogonal transform, and then inverting the transform (note that standard Gaussian distributions are invariant to orthogonal transforms). Applying this result to our context, we conclude that without additional assumptions, learning world models in the sense of Definition~\ref{def:world_model} is not possible with a simple $\gT$ by looking at $\rvx$ alone.

Given this result, one may naturally be pessimistic about the outlook on learning world models. However, recent studies on LLMs seem to suggest otherwise: instead of learning arbitrary non-linear representations of the observed data as Lemma~\ref{lemma:non_identifiability} implies, LLMs turn out to often learn semantically meaningful, human-interpretable representations~\citep{li2021implicit,bricken2023towards,marks_geometry_2023,gurnee_language_2024}. This suggests that despite having a vast number of parameters and being capable of learning different task realizations~\citep{reizinger_position_2024}, LLMs can still learn representations that are reasonably aligned with humans.

To address this puzzle, in this work we propose to incorporate the implicit bias of neural networks instead of using task performance as the only identifiability criterion. Specifically, we explore if the bias towards \emph{low-complexity} realizations, a trait of both human reasoning and deep learning, could be used to steer the realization towards non-trivial recovery of the true latents. Yet, the main challenge is that for continuous functions with inputs in $\sR^d$, we lack a well-established complexity measure that is amenable to analysis. For example, Kolmogorov complexity~\citep{li2008introduction} in algorithmic learning theory offers a unified framework for defining the complexity of any object, yet it is uncomputable.
Fortunately, we will next show that this problem could be circumvented by using Boolean models.


% \vspace{-0.2em}
\section{Complexity Measures}
\label{sec:preliminaries}
% \vspace{-0.2em}


Computers encode every object, including the observed data and variables learned by neural networks, in bit strings. Leveraging this fact, we can assume without loss of generality that both $\rvx$ and $\rvz$ in Definition~\ref{def:dgm} are Boolean (after some encoding). Formally, in the remainder of the paper we will let $\gX \subseteq \{-1,1\}^m$ and $\gZ = \{-1,1\}^d$. Since $\psi$ is invertible, we have $m\ge d$. In practice, we expect $m\gg d$ for complex and high-dimensional data.
Unless otherwise mentioned, we will also assume $\supp(p) = \gZ$ to ensure that all elements in $\gZ$ can be sampled with positive probabilities.

A direct consequence of the Boolean modeling of variables is that all functions involved are \emph{Boolean functions} (see Section~\ref{appsec:preliminaries} for a basic introduction). Notably, this itself does not resolve the non-identifiability issue present in the continuous case, as functions with Boolean inputs and outputs can still exhibit arbitrary nonlinearity. Instead, the primary advantage of this approach lies in the useful machinery it offers for defining functional complexity through the Fourier-Walsh transform:

 % Notably, this does not resolve the non-identifiability issue present in the continuous case (Lemma~\ref{lemma:non_identifiability}), as functions with Boolean inputs and outputs can still exhibit arbitrary non-linearity. Instead, the primary advantage of this approach lies in the powerful analytical framework it offers for defining and studying functional complexity through the Fourier-Walsh transform:

\begin{definition}[Fourier-Walsh transform~\citep{odonnell_analysis_2021}]
\label{def:fourier-walsh}
Every function $f:\{\pm 1\}^n\to\sR$ can be uniquely expressed as a multilinear polynomial
\begin{equation}
f(\vx) = \sum\nolimits_{S\subseteq [n]}\hat{f}(S)\chi_S(\vx),
\label{eq:fourier-walsh}
\end{equation}
where $\vx = (x_1,\ldots,x_n)$, $\chi_S(\vx) = \prod_{i\in S}x_i$ are \emph{parity functions}, and $\hat{f}(S) \in\sR$ are the coefficients. 
\end{definition}

The Fourier-Walsh transform shows that every Boolean function can be represented as a \emph{linear} combination of parity functions $\chi_S$ that capture all non-linear relationship between inputs and outputs. In fact, it can be shown that parity functions are a \emph{basis} of the vector space $\gF^n\vcentcolon=\{f:\{\pm 1\}^n\to\sR\}$ of $n$-dimensional Boolean functions (see Section~\ref{appsec:preliminaries} for more details). By contrast, the space of arbitrary continuous functions does not have a similar, theory-friendly basis.

Given the Fourier-Walsh transform of a function, a natural measure of its complexity is its \emph{degree}:
\begin{definition}[Degree]
\label{def:degree}
For every function $f:\{\pm1\}^n\to\sR$, its \emph{degree} is defined by
\begin{equation}
\deg(f) = \max\{|S|:\hat{f}(S)\ne 0\}.
\end{equation}
For a Boolean function with multiple output dimensions, we define its degree by the sum of degrees of the Boolean functions mapping the input to each output coordinate.
\end{definition}

\begin{remark}
\label{remark:degree}
Note that the degree of $f$ equals to the maximum degree of the basis functions it uses: $\deg(f) = \max\{\deg(\chi_S):\hat{f}(S)\ne 0\}$. This interpretation will be useful in Section~\ref{sec:main_arch}.
\end{remark}

Intuitively, the degree of a Boolean function measures how non-linear it is. It can also be viewed as an approximation of Kolmogorov complexity if we treat parity functions $\chi_S$ as ``function codes'' with length $|S|$. Prior work has shown that many classes of neural networks indeed have a bias towards low-degree solutions for Boolean inputs~\citep{abbe_generalization_2023,bhattamishra_simplicity_2023}; in comparison, here we study whether a relevant low-complexity bias could implicitly lead to world model learning.

\textbf{Complexity measures for realizations.} Based on the notion of degree, we next introduce complexity measures to quantify the complexity of realizations. First, note that while degree can characterize function complexity, it fails to distinguish between different realizations, as they all behave identically when considered as a whole function. To overcome this limitation, we introduce \emph{realization degree}.

\begin{definition}[Realization degree]
\label{def:impl_degree}
For a realization $h_{(1)}\circ\cdots\circ h_{(q)}$, its \emph{realization degree} is
\begin{equation}
\impdeg(h_{(1)}\circ\cdots\circ h_{(q)}) = \sum_{i\in [q]} \deg(h_{(i)}).
% \vspace{-0.5em}
\end{equation}
\end{definition}
For example, the realization degree of a flat realization $h^*\in\gH(h)$ coincides with its degree; the realization degree of a hierarchical realization $h=g\circ \Phi$ is $\impdeg(g\circ\Phi) = \deg(g) + \deg(\Phi)$. Compared to degree, realization degree better reflects the cost of implementing each function in hierarchical realizations as in practice. Throughout this work, we say a model exhibits a \textbf{low-degree bias} if it minimizes the realization degree.
We conclude this section by two definitions that will be useful in characterizing the impact of the low-degree bias on flat and hierarchical realizations.

\begin{definition}[Min-degree solutions]
\label{def:min_degree_solutions}
For a task $h$, we define its \emph{min-degree solutions} $\hmin(h)$ as the set of functions in $\gH(h)$ and that minimize the degree. We denote their degree by $\deg(\hmin(h))$.
\end{definition}

\begin{definition}[Conditional degree]
\label{def:conditional_degree}
For a task $h$ and representation $\Phi:\gX\to\gZ$, let $h^*\in\hmin(h)$, and the \emph{conditional degree} of $h$ on the representation $\Phi$ is defined as
\begin{equation}
\deg(h\mid\Phi) = \deg(h^*) - \max\{\deg(g):g\circ\Phi\in\gH(h)\}.
\end{equation}
\end{definition}

By Definition~\ref{def:min_degree_solutions}, flat realizations with the low-degree bias satisfy $h^*\in\hmin(h)$. Analyzing the impact of representations is more involved; Definition~\ref{def:conditional_degree} suggests that a representation $\Phi$ only makes a task $h$ ``simpler'' if $\deg(h\mid\Phi) > 0$, \ie, solving the task on top of $\Phi$ has a smaller degree compared to low-degree flat realizations. We will explore this notion further in the next section.


\section{Theoretical Analysis}
\label{sec:main}


This section presents our main theoretical results. We first study how the low-degree bias introduced in Section~\ref{sec:preliminaries} drives representation learning in a multi-task setting (Section~\ref{sec:main_repr_learning}). We then present our main results on learning world models, showing sufficient conditions for the identification of latent data-generating variables (Section~\ref{sec:main_world_model}). Next, we show provable benefits of learning world models (Section~\ref{sec:main_benefits}). We conclude this section by a study on how the model architecture impacts world model learning (Section~\ref{sec:main_arch}). All proofs are deferred to Section~\ref{appsec:proof}.
% In each section, we also discuss the  implications of our results and connect them to prior work.

\subsection{Low-Degree Bias Drives Representation Learning}
\label{sec:main_repr_learning}

As a warm-up, we first consider a basic question: why should neural networks, such as LLMs, learn \emph{any} representation of data-generating variables when trained on proxy tasks? Indeed, modern neural networks can often memorize the entire dataset~\citep{zhang_understanding_2017} or rely on superficial statistical patterns to solve tasks~\citep{geirhos_imagenet-trained_2019}. This raises concerns about whether they truly understand the data despite generating plausible outputs~\citep{bender_dangers_2021}.

Formally, this question can be modeled as a competition between flat realizations $h^*\in\hmin(h)$ and hierarchical realizations $g\circ\Phi\in\gH(h)$ in our formulation. Specifically, we seek to determine which realization minimizes the realization degree and is thus favored by the low-degree bias. Our first result shows that for any \emph{single task} $h$, the flat realization is preferred.

\begin{theorem}[Single-task learning]
\label{thm:single-task}
Let $h$ be a task. Then, for every $h^*\in\hmin(h)$, representation $\Phi:\gX\to\gZ$, and $g:\gZ\to\sR$ such that $g\circ\Phi\in\gH(h)$, the following holds:
\begin{equation}
\impdeg(h^*) \le \impdeg(g\circ \Phi).
\end{equation}
\end{theorem}

\begin{remark}
Intuitively, this result is a consequence of learning ``redundant'' representations: for any $h$, it suffices to learn every parity function $\chi_S$ with $\hat{h}(S)\ne 0$ in its Fourier-Walsh transform. Even if there is a good universal data representation, explicitly learning it is often not the best choice since it may involve irrelevant parity functions with $\hat{h}(S) = 0$, resulting in a larger realization degree.
\end{remark}

However, the situation changes in the \emph{multi-task} setting. Suppose there are $n$ distinct tasks $h_1,\ldots,h_n$. A flat realization solves each task independently by learning solutions $h_i^*\in\hmin(h_i)$ for each $i\in [n]$, whereas a hierarchical realization can leverage the shared representation $\Phi$ across tasks, requiring only task-specific functions $g_i$ for $i\in [n]$. Our next theorem shows that in this setting, the hierarchical realization is favored if a sufficient number of tasks have a positive conditional degree.


\begin{theorem}[Multi-task learning]
\label{thm:multi-task}
Let $h_1,\ldots,h_n$ be $n$ distinct tasks and let $h^*_i\in\hmin(h_i),\,\forall i\in [n]$. Let $\Phi:\gX\to\gZ$ and $g_1,\ldots,g_n$ satisfy that $g_i\circ\Phi$ is an realization of $h_i,\,\forall i\in[n]$.
Then, the following holds for every $h^* = (h_1^*,\ldots,h_n^*)$, $g = (g_1,\ldots,g_n)$, and $\Phi^*\in\hmin(\Phi)$:
\begin{equation}
% \vspace{-0.2em}
\impdeg(h^*) - \impdeg(g\circ\Phi^*) \ge \sum_{i\in[n]} \deg(h_i\mid \Phi^*) - d^2.
\label{eq:thm2}
% \vspace{-0.5em}
\end{equation}
\end{theorem}

Thus, if a sufficient number of tasks satisfy $\deg(h_i\,|\,\Phi) > 0$ and $\sum_{i\in[n]} \deg(h_i\,|\, \Phi) > d^2$, then $\impdeg(h^*) > \impdeg(g\circ\Phi)$, contrary to Theorem~\ref{thm:single-task}.
We discuss two implications of these results:
\begin{itemize}[leftmargin=1.25em]
\item The contrast between the single-task and multi-task settings justifies the importance of multi-tasking in learning general-purpose representations, which has been conjectured by prior work~\citep{radford2019language,huh_platonic_2024}. Indeed, modern pre-training objectives such as next-token prediction and contrastive learning can be interpreted as solving a large number of prediction tasks simultaneously~\citep{radford2019language,arora_theoretical_2019,brown_language_2020}.
\item Theorem~\ref{thm:multi-task} suggests that to facilitate the learning of a representation $\Phi$, proxy tasks should be chosen such that conditioning on $\Phi$ makes them less ``complex''. This provides a framework to reason about whether certain objectives in self-supervised learning~\citep{liu2021self} induce better representations than others. For instance, input reconstruction is often suboptimal as it permits a low-degree solution $h^*(\vx)=\vx$ without requiring any representation learning; masked image modeling~\citep{he_masked_2022} is likely more effective, as a representation that captures image semantics could significantly reduce solution complexity by filtering out pixel-to-pixel details.
\end{itemize}


\subsection{Conditions for Learning World Models}
\label{sec:main_world_model}

We now move on to investigate whether the low-degree bias facilitates world model learning. While Theorem~\ref{thm:multi-task} shows that training on properly defined proxy tasks drives representation learning in the presence of the low-degree bias, it does not specify \emph{which} representation is ultimately learned. In this section, we further explore the multi-task setting to address the key question: can we construct proxy tasks $h_1,\ldots,h_n$ to induce a representation $\Phi$ that learns the world models in the sense of Definition~\ref{def:world_model}?


We begin by characterizing the space of all possible tasks. Given that the observed data is generated by $\rvx= f(\rvz)$ with an invertible $f$, every task $h:\gX\to\sR$ can be equivalently defined via a function $h':\gZ\to\sR$ as $h = h'\circ \psi^{-1}$. Thus, the space of all possible tasks on $\gX$ is represented as
\begin{equation}
\gF^d\circ \psi^{-1} = \left\{h'\circ \psi^{-1}\mid h'\in\gF^d\right\},
\end{equation}
where $\gF^d\vcentcolon=\{h:\{\pm 1\}^d\to\sR\}$ denotes the set of Boolean functions on $\gZ = \{\pm 1\}^d$.
Our next theorem shows that if proxy tasks are constructed by uniformly sampling from $\gF^d\circ\psi^{-1}$, then as $n\to\infty$, all viable representations yield the same  task-averaged realization complexity.

\begin{theorem}[Representational no free lunch]
\label{thm:no_free_lunch}
Let $h_1,\ldots,h_n$ be $n$ tasks that are independently and uniformly sampled from $\gF^d\circ \psi^{-1}$. Then as $n\to \infty$, for any two representations $\Phi, \Phi'$ satisfying that there exists a bijective transform $T:\gZ\to\gZ$ such that $\Phi(\vx) = T(\vz)$ for every $\vz\in\supp(p)$ and $\vx = \psi(\vz)$, the following holds:
\begin{equation}
\lim_{n\to\infty} \frac{1}{n}\left(\impdeg(g\circ\Phi) - \impdeg(g'\circ\Phi')\right) = 0,
\end{equation}
where $g, g'\in(\gF^d)^n$ satisfy that $g_i\circ\Phi$ and $g'_i\circ\Phi'$ are both realizations of $h_i$ for every $i\in[n]$.
\end{theorem}


\begin{remark}
Note that the condition of the existence of a bijective transform $T$ is a minimal requirement for $\Phi(\rvx)$ containing enough information for solving all tasks. The fact that all such representations have the same realization complexity suggests that the representation $\Phi$ induced by uniformly sampling from $\gF^d\circ \psi^{-1}$ only learns the world model up to arbitrary bijective transforms.
\end{remark}

Theorem~\ref{thm:no_free_lunch} can be viewed as a ``no free lunch''-like theorem for representation learning. The original no free lunch theorem~\citep{wolpert1996lack} states that every learner's performance is equally good when averaged over a uniform distribution on learning problems; here we show that every viable representation is equally complex when averaged over a uniform distribution on tasks. As we will show in Section~\ref{appsec:proof_no_free_lunch}, the technical intuition of this result is that every representation renders some tasks in the task space ``simple'' and others ``complex'', with the overall task-averaged complexity independent of the particular choice of the representation.

To overcome this result, we then move on to the \emph{non-uniform} case where proxy tasks are still drawn from $\gF^d$, but with different weights assigned to different functions. This setting is of more practical interest: prior work has reported various evidence suggesting that realistic tasks are often much more structured than being purely random~\citep{whitley2005complexity,zhang_understanding_2017}. In particular, real-world data tend to be highly compressible, implying that low-complexity input-output maps occur more frequently than high-complexity ones~\citep{dingle_inputoutput_2018,zhou_non-vacuous_2019,goldblum_position_2024}. To formalize this, we define \emph{$k$-degree tasks}.

\begin{definition}[$k$-degree tasks]
\label{def:k_degree}
Let $\gF^d_k\vcentcolon=\{h:\{\pm 1\}^d\to\sR\mid \deg(h)\le k\}$. We say a task $h$ is a $k$-degree task if $h\in\gF^d_k\circ \psi^{-1} = \{h'\circ \psi^{-1}\mid h'\in\gF^d_k\}$.
\end{definition}

In other words, $k$-degree tasks can be solved by a function with degree not greater than $k$ on top of true latents. One can easily verify that $\gF^d_k\subseteq \gF^d_{k+1}$ for every $k\in [d-1]$ and $\gF^d_d = \gF^d$. Thus, uniform sampling from all possible tasks amounts to uniform sampling from $\gF^d_d\circ \psi^{-1}$. $k$-degree tasks are also related to tasks with positive conditional degree, as shown by the following corollary.

\begin{corollary}
\label{corollary:degree_k}
For every task $h$ satisfying $\deg(h\mid \psi^{-1}) > 0$, we have $h\in\gF^d_{d-1}\circ \psi^{-1}$.
\end{corollary}
To capture the low-complexity bias in task sampling, we assign more weights to $\gF^d_k\circ \psi^{-1} (k<d)$. Perhaps surprisingly, our next theorem shows that even a slight such preference on low-complexity tasks in the task distribution can induce world model learning up to simple transforms.
% including negations and permutations.

\begin{theorem}[World model learning]
\label{thm:world_model}
Let $p_1,\ldots,p_d\in(0,1)$ such that $\sum_{i\in [d]}p_i = 1$.
Let $h_1, \ldots,h_n$ be $n$ tasks that are independently sampled as follows:
(\romannumeral 1) sample a degree $k\in [d]$ according to probabilities $\prob[k=i] = p_i$; (\romannumeral 2) uniformly sample a $k$-degree task. Let $(\Phi^*, g^*)$ be the minimizer of the following optimization problem:
\begin{equation}
% \vspace{-0.2em}
\begin{aligned}
&\min_{\Phi:\gX\to\gZ, g\in\gF^d} \frac{1}{n}\,\impdeg(g\circ\Phi) \\
&\hspace{2em} \mathrm{s.t.}\hspace{1.75em} g_i\circ\Phi\in\gH(h_i),\,\forall i\in[n].
\end{aligned}
% \vspace{-0.2em}
\label{eq:world_model}
\end{equation}
Then as $n\to\infty$, $\Phi^*$ learns the world model up to negations and permutations, \ie, there exists a permutation $i_1,\ldots,i_d$ of $1,\ldots,d$ such that $\Phi^*_j(\vx) \in \{\pm z_{i_j}\}$ for every $j\in[d]$, with $\vz = \psi^{-1}(\vx)$.
\end{theorem}

We make several remarks on this result:
\begin{itemize}[leftmargin=1.25em]
	\item The reason why $k$-degree tasks facilitate world model learning is that, in effect, they induce a task distribution in which lower-degree tasks on true latents are drawn with larger probabilities than in the uniform setting. This overcomes the result in Theorem~\ref{thm:no_free_lunch} by breaking the degree balance between different representations when averaged over the task distribution: representations capable of solving these lower-degree tasks in ``cheaper'' ways would now be favored.
	\item A limitation of Theorem~\ref{thm:world_model} is that we requires a non-zero probability of explicitly sampling degree-$1$ tasks, in which latent variables are also task outputs. However, we emphasize that this probability is exponentially small as $d$ becomes large, and we conjecture that it can be completely removed in many settings. See Section~\ref{appsec:proof_world_model} for more discussion.
	\item Technically, in our proof we introduce a degree analysis of $k$-degree Boolean functions composed with invertible transforms (Lemma~\ref{lemma:degree_composition}), which may be of independent interest in analyzing similar problems.
\end{itemize}

\textbf{Connection to the linear representation hypothesis.} A number of recent mechanistic interpretation studies show that LLMs often represent abstract, interpretable features as \emph{directions} in their intermediate representation space~\citep{nanda_emergent_2023,marks_geometry_2023,gurnee_language_2024}. Theorem~\ref{thm:world_model} can be viewed as a provable, Boolean version of the emergence of such linear representations: permutations and negations are precisely all degree-$1$ Boolean functions, a natural counterpart of degree-$1$ real polynomials (\ie, linear functions) in the real domain. The main significance of this result is that even if proxy tasks can be complex, non-linear functions over true latent variables $\vz$, we can still recover $\vz$ up to very simple transforms.


\textbf{Connection to the linear representation hypothesis.} A number of recent mechanistic interpretation studies show that LLMs often represent abstract, interpretable features as \emph{directions} in their intermediate representation space~\citep{nanda_emergent_2023,marks_geometry_2023,gurnee_language_2024}. Theorem~\ref{thm:world_model} can be viewed as a provable, Boolean version of the emergence of such linear representations: permutations and negations are precisely all degree-$1$ Boolean functions, a natural counterpart of degree-$1$ real polynomials (\ie, linear functions) in the real domain. The main significance of this result is that even when proxy tasks involve complex, non-linear functions over true latent variables $\vz$, we can still recover $\vz$ up to very simple transforms despite the presence of such nonlinearity.


\subsection{Benefits of Learning World Models}
\label{sec:main_benefits}

Up to now, we have presented sufficient conditions for learning world models. As a complement of these results, this section demonstrates provable \emph{benefits} of learning world models in the context of an out-of-distribution generalization setting introduced by~\citet{abbe_generalization_2023}.

\begin{theorem}[Benefits of learning world models]
\label{thm:benefits}
Let the latent variables during training be uniformly sampled from the Hamming ball $B_r\vcentcolon= \{\vz\in\{\pm 1\}^d\mid \#_{-1}(\vz)\le r\}$ with $r < d$, and let those during testing be uniformly sampled from $\gZ$. Let $h:\{\pm 1\}^m\to\{\pm 1\}$ be a downstream task such that $h\circ\psi$ is a parity function with degree $q \ge k=\ceil{\log_2\sum_{i=0}^r {d\choose i}}$. Then, if $\deg(h\mid \psi^{-1}) \ge q-r$, the following hold: (\romannumeral 1) the test mean square error (MSE) of any $h^*\in\hmin(h)$ is larger than $1$; (\romannumeral 2) let $\Phi^*$ be a representation that learns the world model up to negations and permutations as in Theorem~\ref{thm:world_model} and let $g^*$ be a function such that $g^*\circ\Phi^*\in\gH(h)$, then the test MSE of $g^*\circ\Phi^*$ is $0$.
\end{theorem}

\begin{remark}
As also noted by~\citet{abbe_generalization_2023}, a practical scenario reflected by sampling from $B_r$ is \emph{length generalization} of transformers~\citep{anil2022exploring,press_train_2022}. Here we show that in this setting, a hierarchical realization with the world model is provably more generalizable than any flat realization, despite that both of them achieve zero i.i.d. test error.
\end{remark}

It has been widely believed that learning world models leads to better generalization~\citep{li_emergent_2023,richensrobust,yildirim2024task}. In comparison, Theorem~\ref{thm:benefits} indicates that such benefits typically manifest when the conditional degree $\deg(h\mid\psi^{-1})$ of the downstream task $h$ is large enough. Technically, this is because for tasks with small $\deg(h\mid\psi^{-1})$, solutions using world models still involve high-degree parity functions, whose learning is hampered by the restricted sampling from $B_r$. As a practical example, semantical representations of images can make it easier to answer questions about high-level concepts (tasks with large $\deg(h\mid\psi^{-1})$), yet may make it harder to predict the intensity of a certain pixel (tasks with small or negative $\deg(h\mid\psi^{-1})$). Together with Theorems~\ref{thm:no_free_lunch} and~\ref{thm:world_model}, this result suggests that a \emph{low-degree task distribution} is essential for both learning world models and exploiting its advantage.


\subsection{Impact of Model Architecture}
\label{sec:main_arch}

In the above analysis, we study the role of the low-degree bias with the assumption that the task solutions perfectly adhere to it in the function space. Yet, practical training of neural networks is often more involved than this abstraction. Although neural networks are known as universal function approximators~\citep{hornik1989multilayer,funahashi_approximate_1989}, prior work has shown that models with different architectures may represent the same function differently~\citep{raghu2021vision}. In particular, embedded nonlinearities such as activation functions can steer how functions are represented by neural networks~\citep{xu_what_2020,ziyin_neural_2020,teney_neural_2024}.

Motivated by this, in the following we analyze how different choices of \emph{basis} in the Boolean function space can impact world model learning.
Informally, one may also view neural networks as implementing functional bases through layer-by-layer function composition, and different neural network architectures may induce different bases in the function space~\citep{teney_neural_2024}.

Notably, for a given input dimension $n$, the Fourier-Walsh transform uses parity functions $\chi_S$ as a basis of the $2^n$-dimensional vector space $\gF^n=\{f:\{\pm 1\}\to\sR\}$ (see Definition~\ref{def:fourier-walsh}). To obtain a different basis, we define a \emph{basis transform} $U$, \ie, an invertible linear transform on $\gF^n$ such that $\{U(\chi_S)\mid S\subseteq [n]\}$ is a basis of $\gF^n$. The degree of a function $f:\{\pm 1\}^n\to\sR$ under the new basis is then given by
\begin{equation}
\deg_U(f)\vcentcolon= \max\{\deg(U^{-1}(\chi_S)):\hat{f}(S)\ne 0\},
\label{eq:degree_u}
\end{equation}
where $U^{-1}$ reflects the cost of using the new basis to represent the original one.
As a result, the low-degree bias under $\deg_U(\cdot)$ may deviate from that under $\deg(\cdot)$. To capture the effect of this, we introduce the notion of \emph{basis compatibility}.

\begin{definition}[Compatibility]
\label{def:compatibility}
We say a basis transform $U$ is \emph{compatible} if $\deg(U(\chi_S)) = \deg(\chi_S),\,\forall S\subseteq [n],\forall n$.
\end{definition}

In other words, a basis transform is compatible if it preserves the degrees of all basis functions. Our next result shows the impact of basis compatibility on world model learning.

\begin{theorem}
\label{thm:basis}
Consider the same setting as in Theorem~\ref{thm:world_model} with $n\to\infty$. Let $U$ be a basis transform and let the degrees of $\Phi$ and $g$ be measured under the new basis $\{U(\chi_S)\}$.  Then, (\romannumeral 1) if $U$ is compatible, $\Phi^*$ learns the world model up to negations and permutations; (\romannumeral 2) there exists incompatible $U$ such that $\Phi^* = T\circ \psi^{-1}$, where $T$ is an invertible transform on $\gZ$ satisfying $\max_{i\in [d]} \deg(T^{-1}_i) \ge k$.
\end{theorem}

Theorem~\ref{thm:basis} implies that to facilitate world model learning, the model architecture should induce a basis that preserves the degree of the ``natural'' basis under which low-degree proxy tasks are drawn. Thus, we can also interpret basis compatibility as the compatibility between the model and the tasks. This explains why neural networks with different activation functions can exhibit different complexity biases~\citep{abbe_generalization_2023,teney_neural_2024}.
Basis compatibility is also related to \emph{algorithmic alignment}~\citep{xu_what_2020}, which suggests that tasks with algorithmic structures aligned with the computational structures of neural networks could be learned more sample-efficiently. In comparison, we focus on the identifiability of world models instead of sample efficiency, and we provide a unified framework for analyzing similar concepts via the lens of the low-degree bias.


\begin{figure*}[t]
\centering
\subcaptionbox{\label{subfig:extrapolation_example}}{\includegraphics[width=0.28\linewidth]{poly_deg3_func_main.pdf}}\hspace{0.3em}
\subcaptionbox{\label{subfig:extrapolation_res}}{\includegraphics[width=0.20\linewidth]{violin_poly_deg2.pdf}\includegraphics[width=0.205\linewidth]{violin_poly_deg3.pdf}}\hspace{0.05em}
\subcaptionbox{\label{subfig:physics}}{\includegraphics[width=0.28\linewidth]{all_barplot.pdf}}
\caption{\textbf{Empirical results.} (a) An example of extrapolating a degree-$3$ polynomial. Shaded region indicates the training region. (b) Violin plots of the test mean square error (MSE) of ReLU MLPs and our models in extrapolating degree-$2$ (left) and degree-$3$ (right) polynomials. (c) Results for learning physical laws. Each column indicates the task and \emph{out-of-distribution} test MSE averaged over 5 runs.}
% \vspace{-0.4em}
\end{figure*}


\section{Algorithmic Implications}
\label{sec:implications}

In this section, we illustrate the algorithmic implications of \emph{basis compatibility} through two representative tasks: polynomial extrapolation~\citep{xu_how_2021} and learning physical laws~\citep{kang_how_2024,motamed_generative_2025}. The first task provides an illustrative example of our results in Section~\ref{sec:main_arch}, while the second is of greater practical interest for learning real-world models and is related to recent efforts in developing video prediction models as world simulators~\citep{brooks2024video}. While our experiments are currently limited in scale and mainly serve as proof-of-concept demonstrations, we view extending the ideas discussed in this section to broader contexts as a promising future direction.

% \vspace{-0.2em}
\subsection{Polynomial Extrapolation}
\label{sec:extrapolation}
% \vspace{-0.2em}

% \textbf{Polynomial extrapolation.}

We begin by a synthetic task in which we train multilayer perceptrons (MLPs) to fit real polynomials $P_n(x) = \sum_{i=0}^n a_i x^n$ (see Section~\ref{appsec:extrapolation} for details). The space of polynomials has a standard basis $\{1,x,x^2,\ldots\}$, making it a natural counterpart of the Boolean function space with the parity basis. Recovering all basis functions used by $P_n$ could generate any polynomial by linearly combining these basis functions, which enables extrapolation.

As shown by prior work~\citep{xu_how_2021} (see also Figure~\ref{subfig:extrapolation_example}), common ReLU MLPs cannot extrapolate degree-$k$ polynomials with $k>1$ beyond the training region. Our framework explains why this happens via the incompatibility between ReLU and the polynomial basis: ReLU MLPs can learn a basis function $\hat{f}$ that approximates $f(x)=x^k$ arbitrarily well in any finite training region, but the \emph{simplest} $\hat{f}$ that fits the data is not $f$ itself, which is expensive to represent using the composition of ReLU. As a result, the actually learned $\hat{f}$ differs from $f$ and hence does not extrapolate well.

Motivated by this explanation, here we provide a simple fix: replacing a portion of the ReLU activation functions with functions that are more compatible with the polynomial basis. In practice, we replace half of ReLU functions in each MLP layer by one of the two functions including the identity function $\sigma(x) = x$ and the quadratic function $\sigma(x) = x^2$. Representing $f(x) = x^k$ would be much easiler using these functions, and we thus expect that a neural network with the low-complexity bias would then use them instead of the remaining ReLU despite the same expressive ability they have, resulting in $\hat{f}\approx f$. As shown in Figure~\ref{subfig:extrapolation_res}, this simple method indeed leads to significant improvement in extrapolation. Please see Section~\ref{appsec:extrapolation_res} for more results and discussion.

One may wonder that in this task, we still need to know the basis of the task a priori to achieve basis compatibility. However, we emphasize that even without prior knowledge, we can still use different activation functions simultaneously and rely on the low-complexity bias of neural networks to \emph{self-adaptively} select functions that are the most compatible with the task, as empirically shown in our experiments. We thus conjecture that this approach could be quite universal. Indeed, in the next section we show that exactly the same approach can also bring benefits in a distinct scenario.

We now show that the approach in Section~\ref{sec:extrapolation} also benefits a sequence prediction task aiming at learning physical laws. Correctly abstracting fundamental physical laws is essential for any model to be a real ``world model'' for the physical world~\citep{motamed_generative_2025}. Notably, a model that ``understands'' the physical laws is expected to generalize these laws to \emph{unseen distributions} rather than fit them only in the training domain. Following~\citet{kang_how_2024}, we generate two types of object motion sequences reflecting basic physical laws: (\romannumeral 1) single-object parabolic motion, and (\romannumeral 2) two-object elastic collision motion (see Section~\ref{appsec:physics} for more details). We train a transformer~\citep{vaswani_attention_2017} to predict the motion of objects conditional on the first few frames in the motion sequence. The model is then evaluated in an \emph{out-of-distribution generalization} setup with objects having different initial velocities and sizes. To apply our method, we simply replace every MLP in the transformer with our modified MLP in Section~\ref{sec:extrapolation} (we also replace the remaining ReLUs with GELUs).

As shown in Figure~\ref{subfig:physics}, our model achieves lower prediction error than transformer in both settings. Note that both models achieve near zero training error. Thus, the fact that our model generalizes better out-of-distribution is not because it fits the data better, but due to it better capturing the underlying laws in object movements. See Section~\ref{appsec:physics_res} for the predicted motions from both models.


% \vspace{-0.2em}
\section{Limitations and Future Work}
\label{sec:conclusion}
% \vspace{-0.2em}

This work is an initial step towards formally understanding world model learning, and we can see many exciting future directions. (\romannumeral 1) Neural networks in practice learn \emph{hierarchical representations}~\citep{bengio_representation_2013}, which may require a more structured modeling of the representation $\Phi$. (\romannumeral 2) We do not assume the latent variables to have any specific structure (\eg, causality between variables); combining our analysis and the results in causal reasoning~\citep{scholkopf_toward_2021,richensrobust} would be interesting. (\romannumeral 3) We primarily focus on the low-complexity bias of neural networks; considering other implicit bias~\citep{allen-zhu_towards_2023,zhang_feature_2024} and more fine-grained complexity measures are also important directions for future work.


% \section*{Accessibility}
% Authors are kindly asked to make their submissions as accessible as possible for everyone including people with disabilities and sensory or neurological differences.
% Tips of how to achieve this and what to pay attention to will be provided on the conference website \url{http://icml.cc/}.

% \section*{Software and Data}

% If a paper is accepted, we strongly encourage the publication of software and data with the
% camera-ready version of the paper whenever appropriate. This can be
% done by including a URL in the camera-ready copy. However, \textbf{do not}
% include URLs that reveal your institution or identity in your
% submission for review. Instead, provide an anonymous URL or upload
% the material as ``Supplementary Material'' into the OpenReview reviewing
% system. Note that reviewers are not required to look at this material
% when writing their review.

% Acknowledgements should only appear in the accepted version.
% \section*{Acknowledgements}

% \textbf{Do not} include acknowledgements in the initial version of
% the paper submitted for blind review.

% If a paper is accepted, the final camera-ready version can (and
% usually should) include acknowledgements.  Such acknowledgements
% should be placed at the end of the section, in an unnumbered section
% that does not count towards the paper page limit. Typically, this will 
% include thanks to reviewers who gave useful comments, to colleagues 
% who contributed to the ideas, and to funding agencies and corporate 
% sponsors that provided financial support.

\section*{Impact Statement}

% Authors are \textbf{required} to include a statement of the potential 
% broader impact of their work, including its ethical aspects and future 
% societal consequences. This statement should be in an unnumbered 
% section at the end of the paper (co-located with Acknowledgements -- 
% the two may appear in either order, but both must be before References), 
% and does not count toward the paper page limit. In many cases, where 
% the ethical impacts and expected societal implications are those that 
% are well established when advancing the field of Machine Learning, 
% substantial discussion is not required, and a simple statement such 
% as the following will suffice:


This paper presents work whose goal is to advance our understanding of the inner workings of neural networks and LLMs, and
our main results are of theoretical nature.
% While our main results are of theoretical nature, in the main text we have also discussed potential algorithmic implications, which may further induce broader impacts.
There are many potential societal consequences of our work, none which we feel must be specifically highlighted here.

% The above statement can be used verbatim in such cases, but we 
% encourage authors to think about whether there is content which does 
% warrant further discussion, as this statement will be apparent if the 
% paper is later flagged for ethics review.


% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
\nocite{langley00}

\bibliography{ref}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn

% \section{You \emph{can} have an appendix here.}

% You can have as much text here as you want. The main body must be at most $8$ pages long.
% For the final version, one more page can be added.
% If you want, you can use an appendix like this one.  

% The $\mathtt{\backslash onecolumn}$ command above can be kept in place if you prefer a one-column appendix, or can be removed if you prefer a two-column appendix.  Apart from this possible change, the style (font size, spacing, margins, page numbering, etc.) should be kept the same as the main body.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Related Work}
\label{appsec:related_work}

\paragraph{World models.} 
The term “world model” in the machine learning context originates from~\citet{ha_world_2018}, who describe it as a human-like “mental model of the world” that learns an abstract representation of information flow and can be used to predict future events. This definition closely aligns with the concept of a “mental model” in cognitive science, \ie, an internal representation of external reality~\citep{craik1967nature}, making it naturally connected to the field of \emph{representation learning} in machine learning~\citep{bengio_representation_2013}.

Recently, the remarkable capabilities of large language models (LLMs) have sparked a scientific debate on whether these models merely exploit superficial statistical patterns to generate predictions without genuine “understanding” of natural language~\citep{bender_dangers_2021,mitchell_ai_2023}, or whether they develop models that serve as compact and interpretable representations of the underlying data generation process. A series of studies have demonstrated the presence of internal representations in language models trained on synthetic tasks~\citep{li_emergent_2023,nanda_emergent_2023,jin_emergent_2024}. For real-world LLMs, research in mechanistic interpretability suggests that these models learn compact, interpretable, and causal features within their intermediate layers~\citep{li2021implicit,bricken2023towards,marks_geometry_2023,gurnee_language_2024}. At the same time, many studies report a significant decline in LLM performance on tasks that are assumed to be underrepresented in their pre-training distribution~\citep{wu_reasoning_2023,berglund_reversal_2024,mirzadeh_gsm-symbolic_2024}.

Beyond sequence models, world models have also gained attention in reinforcement learning~\citep{ha_world_2018,lecun2022path,xie_making_2024}, probabilistic learning~\citep{friston2021world,wong_word_2023}, and causal discovery~\citep{richensrobust}. However, despite the growing number of empirical studies, the theoretical foundations of world model learning remain largely unexplored.

% The usage of the term ``world model'' in the machine learning context originates from~\citet{ha_world_2018}---they refer to it as a human-like, ``mental model of the world'' that learns an abstract representation of the information flow and that can be used to predict future events. This definition is closely related to the notion of ``mental model'' in cognitive science, \ie, an internal representation of external reality~\citep{craik1967nature}, which makes it naturally connected to the topic of \emph{representation learning} in machine learning~\citep{bengio_representation_2013}. Recently, the remarkable capabilities of LLMs have sparked a scientific debate on whether LLMs are simply exploiting superficial statistics to make predictions without ``understanding'' natural language~\citep{bender_dangers_2021,mitchell_ai_2023}, or they do learn models that serve as compact and interpretable representations of the underlying data generation process. A series of work demonstrates the existence of internal representations in language models when trained on synthetic tasks~\citep{li_emergent_2023,nanda_emergent_2023,jin_emergent_2024}. For real LLMs, there are studies in mechanistic interpretability showing that LLMs learn compact, interpretable, and causal features in their intermediate layers~\citep{li2021implicit,bricken2023towards,marks_geometry_2023,gurnee_language_2024}. Meanwhile, many works report a marked decline in LLMs' performance in the tasks assumed to be underrepresented in their pre-training distribution~\citep{wu_reasoning_2023,berglund_reversal_2024,mirzadeh_gsm-symbolic_2024}. Besides sequence models, world models also attract research interests in reinforcement learning~\citep{ha_world_2018,lecun2022path,xie_making_2024}, probablistic learning~\citep{friston2021world,wong_word_2023}, and causal discovery~\citep{richensrobust}. Nevertheless, despite a recent increase of related empirical studies, theoretical foundations of learning world models are still unclear.

\paragraph{Latent variable recovery.} Our definition of world models falls within a broad class of latent variable recovery problems~\citep{everett2013introduction}, where observable data is generated by latent variables through an unknown generation function. It is well established that, without additional assumptions, recovering the true latent variables from observed data is generally impossible if the generation function is nonlinear~\citep{hyvarinen_nonlinear_1999,khemakhem_variational_2020}, a fundamental result in non-linear independent component analysis (non-linear ICA).

To address this impossibility, subsequent research has explored various structural assumptions on latent variables, such as conditional independence between the latent variables and an observable auxiliary variable~\citep{hyvarinen_nonlinear_2019,khemakhem_variational_2020,lee_predicting_2021}, distributional constraints~\citep{zimmermann_contrastive_2021,wei_why_2021}, and causal interventions~\citep{von_kugelgen_self-supervised_2021,ahuja_interventional_2023,von_kugelgen_nonparametric_2023}. Some studies have also linked these assumptions to contrastive learning~\citep{hyvarinen_nonlinear_2019,tosh_contrastive_2021,zimmermann_contrastive_2021}. However, incorporating such structural assumptions often leads to complex and less scalable training paradigms compared to the pre-training framework of modern LLMs (\ie, next-token prediction). As a result, these studies do not directly address the central question of our work: \emph{Can the ongoing paradigm of LLMs learn world models?} Meanwhile, the fact that LLMs already acquire non-trivial representations~\citep{li2021implicit,bricken2023towards,marks_geometry_2023,gurnee_language_2024} suggests that they must leverage some form of \emph{implicit bias} rather than explicit structural assumptions on input data, which motivates our study.

% Our definition of world models is subsumed by a general class of latent variable recovery problems~\citep{everett2013introduction}, in which the observable data is generated by latent variables via an unknown generation function. Without further assumptions, it has been known that recovering the true latent variables from the observed data is not possible if the generation function is non-linear~\citep{hyvarinen_nonlinear_1999,khemakhem_variational_2020}, a classical result in non-linear independent component analysis (non-linear ICA). To overcome the impossibility, follow-up work explores adding more structural assumptions on latent variables, \eg, conditional independence between the latent variables and an observable auxiliary variable~\citep{hyvarinen_nonlinear_2019,khemakhem_variational_2020,lee_predicting_2021}, distributional assumptions on latent variables~\citep{zimmermann_contrastive_2021,wei_why_2021}, and causal interventions~\citep{von_kugelgen_self-supervised_2021,ahuja_interventional_2023,von_kugelgen_nonparametric_2023}. Some studies have also drawn connections between these assumptions and contrastive learning~\citep{hyvarinen_nonlinear_2019,tosh_contrastive_2021,zimmermann_contrastive_2021}. However, a consequence of incorporating these structural assumptions is different and often more complex (hence less scalable) training paradigms compared to the pre-training paradigm of today's LLMs (\ie, next-token prediction). Therefore, those studies cannot answer the important question we care about: \emph{can the ongoing paradigm of LLMs learn world models?} On the other hand, the fact that many LLMs already learn some non-trivial representations indicates that they must leverage some \emph{implicit bias} rather than explicit structural assumptions, which motivates this work.


\paragraph{Implicit bias of neural networks.}
Overparameterized neural networks have been shown to possess the capacity to memorize entire training datasets~\citep{zhang_understanding_2017}. However, their ability to generalize well in many settings suggests that they exhibit implicit preferences for certain solutions—commonly referred to as \emph{implicit bias}. In simple models, such as linear models, random feature models, and two-layer neural networks, a body of theoretical work demonstrates that (stochastic) gradient descent imposes specific forms of implicit regularization on the learned solutions~\citep{soudry_implicit_2018,gunasekar_characterizing_2018,gunasekar_implicit_2018,bartlett_benign_2020,chizat_implicit_2020,lyu_gradient_2021,allen-zhu_towards_2023,andriushchenko_sgd_2023,abbe_generalization_2023,zhang_feature_2024}.

Empirical studies further suggest that practical neural networks extend many of these implicit regularization effects through a form of \emph{simplicity bias}, favoring "simpler" solutions over more complex ones~\citep{perez_deep_2019,kalimeris_sgd_2019,xu2019frequency,bhattamishra_simplicity_2023,huh_low-rank_2023,zhao_m3pl_2024}. However, the notion of "simplicity" is often defined empirically and varies across studies. In this work, we formalize the concept of simplicity within our theoretical framework and analyze its relationship to learning world models.
% It has been shown that overparameterized neural networks often have the ability to memorize the entire training set~\citep{zhang_understanding_2017}. Hence, the fact that they do generalize well in many settings indicates the importance of their implicit preferences of certain solutions, \ie, the implicit bias. In simple models such as linear models, random feature models, and two-layer neural networks, a series of theoretical work shows that (stochastic) gradient descent imposes certain implicit regularization on the solutions~\citep{soudry_implicit_2018,gunasekar_characterizing_2018,gunasekar_implicit_2018,bartlett_benign_2020,chizat_implicit_2020,lyu_gradient_2021,allen-zhu_towards_2023,andriushchenko_sgd_2023,abbe_generalization_2023,zhang_feature_2024}. Empirically, it has been shown that practical neural network models often generalize such implicit regularizations with a notion of \emph{simplicity bias}, favoring ``simple'' solution over more complex ones~\citep{perez_deep_2019,kalimeris_sgd_2019,xu2019frequency,bhattamishra_simplicity_2023,huh_low-rank_2023,zhao_m3pl_2024}. Yet, the term ``simplicity'' itself is often empirically defined and varies from work to work. In this work, we formalize the notion of simplicity under our theoretical framework and analyze its relation to learning world models.


\paragraph{Complexity measures.} Kolmogorov complexity~\citep{li2008introduction} in algorithmic learning theory provides a unified framework for quantifying the complexity of any object, including functions. However, it is not computable in general. In machine learning, conventional statistical learning theory typically employs the VC dimension as a complexity measure to bound the generalization error of models~\citep{vapnik_nature_1999}. More recent complexity measures include Rademacher and Gaussian complexities~\citep{bartlett_rademacher_2002}. However, these measures primarily assess the complexity of \emph{function classes} rather than individual \emph{functions}.

A growing body of research introduces complexity measures tailored for neural networks trained via (stochastic) gradient descent~\citep{bartlett_spectrally-normalized_2017,jacot_neural_2018,zhou_non-vacuous_2019,lotfi_pac-bayes_2022,chatterjee_neural_2024}, often leveraging them for generalization analysis. However, these measures inherently depend on the neural network parameterization, rather than capturing function complexity independently of specific parameterizations. Some studies propose alternative complexity metrics inspired by Kolmogorov complexity in the machine learning context~\citep{xu_what_2020,liu_grokking_2023}, but these metrics are typically problem-specific and do not enable a direct complexity analysis in the function space, unlike the approach we take in this work.

% Kolmogorov complexity~\citep{li2008introduction} in algorithmic learning theory provides a unified framework for defining the complexity of any object including functions, yet it is not computable. In the machine learning literature, conventional statistical learning theory use VC dimension as a complexity measure to bound the generalzation error of models~\citep{vapnik_nature_1999}. More recent complexity measures include Rademacher and Gaussian complexities~\citep{bartlett_rademacher_2002}. However, such measures focus on the complexity of \emph{function classes} instead of specific \emph{functions}. Much work in the literature introduces complexity measures for neural networks trained by (stochastic) gradient descent~\citep{bartlett_spectrally-normalized_2017,jacot_neural_2018,zhou_non-vacuous_2019,lotfi_pac-bayes_2022,chatterjee_neural_2024}, often using them for generalization analysis. However, these measures by definition depend on the neural network parameterization instead of functions without parameterizations. Some work proposes alternative complexity metrics for Kolmogorov complexity in the machine learning context~\citep{xu_what_2020,liu_grokking_2023}, but these metrics are problem-specific, hence they do not allow for a direct complexity analysis in the function space as what we do in this work.


% \paragraph{Boolean function analysis.} 


\section{Preliminaries on Boolean Function Analysis}
\label{appsec:preliminaries}

This section introduces basic definitions and properties of Boolean functions for readers who are not familiar with Boolean function analysis. For a more detailed introduction, we recommend the first few chapters of the book by~\citet{odonnell_analysis_2021}.

Following the convention in Boolean function analysis, throughout this work we use the term \emph{Boolean functions} to refer to functions with the form
\begin{equation}
f:\{-1,1\}^n\to\gY,
\end{equation}
where $\gY$ could be any subspace of $\sR^d$ for an arbitrary integer $d$, such as $\sR$, $\{-1,1\}^d$, etc.

There are different ways to represent the input \emph{bits} in the above definiton. A natural way is to use $0$ and $1$ as elements of the field $\sF_2$. In this way, a (single-output) Boolean function $f$ with $n$ input coordinates (bits) is represented by $f:\{0,1\}^n\to\{0,1\}$. It is also convenient to use $-1$ and $1$, thought as real numbers, and define $f$ as a function from $\{-1,1\}^n$ to $\{-1,1\}$. The latter representation can be easily transformed from the former one by the mapping $b\mapsto (-1)^b$ over $\{0,1\}$. In our analysis, we will mostly use the latter representation as it is more compatible with the Fourier-Walsh transform of Boolean functions.
% but we will also use the $\{0,1\}$ representation in some proofs.

\paragraph{Fourier-Walsh transform.} As in Definition~\ref{def:fourier-walsh}, every function $\{-1,1\}^n\to\sR$ can be expressed as a multilinear polynomial, \ie, we have
\begin{align}
f(\vx) = \sum_{S\subseteq [n]}\hat{f}(S)\chi_S(\vx),\label{appeq:fourier}
\end{align}
where $\hat{f}(S)$ are Fourier-Walsh coefficients and $\chi_S(\vx) = \prod_{i\in S}x_i$ are monomials, also called \emph{parity functions}. The name parity function is due to the fact that it computes the logical parity or XOR of the bits. As an example, consider $f = \max_2$, \ie, the maximum function on 2 bits, and its Fourier-Walsh transform is $f(x_1,x_2) = \frac{1}{2} + \frac{1}{2}x_1 + \frac{1}{2}x_2 - \frac{1}{2}x_1 x_2$.

The existence of Fourier-Walsh transform can be proved by construction, using an interpolation method similar to constructing Lagrange polynomials. For each point $\va = (a_1,\ldots,a_n)\in\{-1,1\}^n$, define
\begin{equation}
\vone_\va(\vx) = \prod_{i\in [n]} \frac{1 + a_i x_i}{2}.
\end{equation}
It is easy to verify that $\vone_\va(\vx)$ takes value $1$ when $\vx = \va$ and $0$ otherwise. 
We then have
\begin{equation}
f(\vx) = \sum_{\va\in\{-1,1\}^n}f(\va)\vone_\va(\vx)
\label{appeq:fourier_proof}
\end{equation}
and arrive at a polynomial representation of $f$.
Since any factors of $x_i^2,\forall i\in [n]$ can be replaced by 1, we further know that this polynomial must be multilinear.

For functions $f:\sF_2^n\to\sR$, we can also define their Fourier-Walsh transforms by extending the $\chi_S$ notation using the mapping $b\mapsto (-1)^b$:
\begin{equation}
\chi_S(\vx) = (-1)^{\sum_{i\in S}x_i},
\end{equation}
where $\vx\in\sF_2^n$. We can thus write the Fourier-Walsh transform of $f:\sF_2^n\to\sR$ in the same form as equation~\eqref{appeq:fourier}.

\paragraph{Parity functions, orthogonality, and Parseval's Theorem.} We define the \emph{inner product} $\langle\cdot,\cdot\rangle$ of functions $f:\{-1,1\}^n\to\sR$ and $g:\{-1,1\}^n\to\sR$ by
\begin{equation}
\langle f, g\rangle = \E_{\vx\sim U(\{-1,1\}^n)}[f(\vx)g(\vx)].
\end{equation}
A key fact about parity functions is that they are orthogonal under the above definition: for every $S,S'\subseteq[n]$, we have
\begin{equation}
\langle \chi_S,\chi_{S'} \rangle = \left\{\begin{aligned} &1,\ S = S'\\ &0,\ \mathrm{otherwise} \end{aligned} \right. .
\end{equation}
Consequently, if we consider the vector space $\gV$ containing all functions $f:\{-1,1\}^n\to\sR$, then parity functions form an \emph{orthonormal basis} of $\gV$. It can be verified that the Fourier-Walsh coefficients in equation~\eqref{appeq:fourier} satisfy
\begin{equation}
\hat{f}(S) = \langle f,\chi_S \rangle
\label{appeq:parity_fourier}
\end{equation}
for every $S\subseteq [n]$.

\emph{Parseval's Theorem} shows that for every $f:\{-1,1\}^n\to\sR$,
\begin{equation}
\langle f,f \rangle = \sum_{S\subseteq [n]}\hat{f}(S)^2.
\end{equation}
In particular, if $f$ is Boolean-valued, then $\langle f,f\rangle = 1$. The uniqueness of the Fourier-Walsh transform of Boolean functions can also be proved using the Parseval's Theorem~\citep{odonnell_analysis_2021}.

% \paragraph{Parity functions and linear transforms on $\sF_2^n$.} We say a function $f:\sF_2^n\to\sF_2$ is \emph{linear} if $f(\vx + \vy) = f(\vx) + f(\vy)$ holds for every $\vx,\vy\in\sF_2^n$, where ``$+$'' here stands for modulo-2 addition. If we encode the output of $f$ by $\pm 1\in\sR$, it can be shown that the linear functions $f:\sF_2^n\to\{-1,1\}$ are precisely parity functions. Note that due to the difference in addition, the ``linearity'' defined here differs from that in the real domain, where parity functions are considered non-linear---they compute XOR of input bits. More generally, a linear transform on $\sF_2^n$ can be represented by an $n\times n$ matrix $\mA$ with entries in $\sF_2$, and the action of $\mA$ on a parity function $\chi_S$ results in another parity function $\chi_T$.

\section{Proofs}
\label{appsec:proof}

This section provides complete proofs of all theorems in the main text, organized as follows.
\begin{itemize}
	\item In Section~\ref{appsec:lemma}, we introduce some definitions and technical lemmas.
	\item In Section~\ref{appsec:proof_single-task}, we provide the proof of Theorem~\ref{thm:single-task}.
	\item In Section~\ref{appsec:proof_multi-task}, we provide the proof of Theorem~\ref{thm:multi-task}.
	\item In Section~\ref{appsec:proof_no_free_lunch}, we provide the proof of Theorem~\ref{thm:no_free_lunch}.
	\item In Section~\ref{appsec:proof_corollary}, we provide the proof of Corollary~\ref{corollary:degree_k}.
	\item In Section~\ref{appsec:proof_world_model}, we provide the proof of Theorem~\ref{thm:world_model}.
	\item In Section~\ref{appsec:proof_benefits}, we provide the proof of Theorem~\ref{thm:benefits}.
	\item In Section~\ref{appsec:proof_basis}, we provide the proof of Theorem~\ref{thm:basis}.
\end{itemize}

\paragraph{Notation and conventions.} We use $[n]$ to denote the set $\{1,\ldots,n\}$ for positive integers $n$. For a set $S$, we denote its cardinality by $|S|$. For a probability distribution $p$ over some set $S$, we denote by $\supp(p)\vcentcolon=\{s\in S\mid p(s) > 0\}$ its support. For $n$ functions $f_1,\ldots,f_n$, we use $f = (f_1,\ldots,f_n)$ to denote the multi-output function satisfying that $f(\vx) = (f_1(\vx),\ldots,f_n(\vx))$; conversely, for a function $f$ with $n$ output dimensions, we use $f_i$ to denote the function mapping the inputs of $f$ to its $i$-th output dimension for $i\in [n]$. As defined in the main text, we use the notation $\gF^n=\{f:\{\pm 1\}^d\to\sR\}$ and $\gF^n_k=\{f:\{\pm 1\}^d\to\sR\mid \deg(f)\le k\}$ for positive integers $n$. Note that although both $\gF^n$ and $\gF^n_k$ are infinite, only finite functions in them are implementable by computers due to bounded precision. Therefore, in our proofs we will treat them as finite yet exponentially large sets. This enables us to use, \eg, $|\gF^n|$ and $\sum_{h'\in\gF^n}\deg(h')$ in our proofs and helps avoid non-essential technical nuances.


\subsection{Technical Lemmas}
\label{appsec:lemma}

This section presents additional definitions and technical lemmas that may come in handy in our proofs. We begin by introducing a lemma that upper-bounds the degree of min-degree solutions for every task with $d$-dimensional latent variables.

\begin{lemma}
\label{lemma:degree}
Suppose that the input data variable $\rvx\in\gX\subseteq\{-1,1\}^m$ is generated as in Definition~\ref{def:dgm} by a $d$-dimensional lantent variable $\rvz\in\gZ = \{-1,1\}^d,\,d\le m$. Then, for every task $h:\gX\to\{-1,1\}$, we have
\begin{equation}
\deg(\hmin(h))\le d.
\end{equation}
\end{lemma}

\begin{proof}
The case of $d = m$ is trivial, so in what follows we consider the case of $d > m$.
We first prove the following lemma:

\begin{lemma}
\label{lemma:existence}
For every $S = \{i_1,\ldots,i_{d+1}\}\subseteq[m]$ with $|S| = d+1$, there exist $b_1,\ldots,b_{d+1}$ with $b_i\in\{-1,1\},\forall i\in[d+1]$ such that $\prod_{j\in[d+1]}(x_{i_j} + b_j) = 0$ holds for every $\vx\in\gX$. 
\end{lemma}

\begin{proof}[Proof of Lemma~\ref{lemma:existence}]
We can prove Lemma~\ref{lemma:existence} by contradiction: assume that it does not hold, \ie, for some $S$, there exists $\vx\in\gX$ such that $\prod_{j\in[d+1]}(x_{i_j} + b_j) \ne 0$, then we must have $x_{i_j} = b_j,\forall j\in[d+1]$. Since $b_1,\ldots,b_{d+1}$ are arbitrary, by applying this argument to every $(b_1,\ldots,b_{d+1})\in\{-1,1\}^{d+1}$ we can find $2^{d+1}$ elements in $\gX$ that differ in at least one coordinate in $S$, which yields $|\gX|\ge 2^{d+1}$. On the other hand, recall that we assume $\supp(\rvz) = \gZ$. Due to the invertibility of $f$, we have $|\gX| = |\gZ| = 2^d$, which contradicts $|\gX|\ge 2^{d+1}$. Hence, the initial assumption is false and Lemma~\ref{lemma:existence} holds.
\end{proof}

Applying Lemma~\ref{lemma:existence}, we have that for every degree-$d+1$ subset $S = \{i_1,\ldots,i_{d+1}\}\subseteq[m]$,
\begin{equation}
\prod_{j\in[d+1]}(x_{i_j} + b_j) = \prod_{j\in S}x_j + \sum_{S'\subset S, |S'|\le d} b_{S'}\prod_{k\in S'} x_k = 0,\,\forall \vx\in\gX
\end{equation}
holds for some $b_1,\ldots,b_{d+1}$, where $b_{S'}\in\{-1,1\}$ for every $S'\subset S$. This means that we can thus replace every degree-$d+1$ monomial $\chi_S(\vx) = \prod_{j\in S}x_j$ by a degree-$d$ polynomial $-\sum_{S'\in 2^{S}, |S'|\le d} b_{S'}\prod_{k\in S'} x_k$. By iteratively using this replacement in the Fourier-Walsh transform of $h$ (Definition~\ref{def:fourier-walsh}), one can eventually obtain a polynomial with degree $d$ or less without changing the value of the function on every $\vx\in\gX$. This completes the proof.
\end{proof}

\begin{remark}
Without the latent structure, a trivial upper bound of the min-degree solution of any task $h$ is $\deg(\hmin(h))\le m$. Hence, Lemma~\ref{lemma:degree} is an example of how the min-degree bias can exploit the latent structure of data by favoring low-degree solutions with degree independent of the data dimension $m$.
\end{remark}

Before presenting our next lemma, we first introduce the concept of influence for Boolean functions.

\begin{definition}[Influence]
\label{def:inf}
Let $f:\{-1,1\}^n\to\{-1,1\}$ be a Boolean function. Then, the \emph{influence} of coordinate $i,i\in[n]$ on $f$ is defined as
\begin{equation}
\inf_i(f) = \prob_{\vx\sim U(\{-1,1\}^n)}[f(\vx)\ne f(\vx^{\oplus i})],
\end{equation}
where $\vx^{\oplus i} = (x_1,\ldots,x_{i-1},-x_i,x_{i+1},\ldots,x_n)$.
\end{definition}

By Parseval's Theorem, we have a formula between influence and the Fourier-Walsh coefficients~\citep{odonnell_analysis_2021}.

\begin{lemma}
\label{lemma:inf}
For $f:\{-1,1\}^n\to\{-1,1\}$ and every $i\in[n]$, the following holds:
\begin{equation}
\inf_i(f) = \sum_{S\in\{S'\subseteq[n] \,\mid\, i\in S'\}} \hat{f}(S)^2,
\end{equation}
where $\hat{f}(S)$ is the Fourier-Walsh coefficients of $f$ as in Definition~\ref{def:fourier-walsh}.
\end{lemma}

With the above definitions, our next lemma introduces a necessary and sufficient condition of bijective Boolean functions being degree-$1$, based on the restricted influence of all input coordinates. 

\begin{lemma}
\label{lemma:degree_1}
Let $f:\{-1,1\}^n\to\{-1,1\}^n$ be a bijective function and let $1\le k\le n-1$ be an integer. Then, we have $\deg(f_i) = 1,\forall i\in[n]$ if and only if for every $S\subset[n]$ with $|S| = k$, there exists $T\subset[n]$ with $|T| = k$ such that:
\begin{itemize}
\item for every $j\in T$, $\inf_i(f_j) > 0$ for at least one $i\in S$;
\item for every $j\in [n]\setminus T$, $\inf_i(f_j) = 0$ for every $i\in S$.
\end{itemize}
\end{lemma}
\begin{proof}
Note that $\deg(f_i) = 1,\forall i\in[n]$ together with the fact that $f$ is bijective indicates the existence of a permutation $i_1,\ldots,i_n$ of $1,\ldots,n$ such that $f_j(\vx) = x_{i_j}$ or $f_j(\vx) = -x_{i_j}$ for every $j\in[n]$, which trivially gives the result. In the following we prove the other direction. Note that the case of $k=1$ is trivial. Hence, to prove that $\deg(f_i)= 1$, it suffices to prove (*): for every $i\in[n]$, there exists $j\in[n]$ such that $\inf_i(f_j)>0$ and $\inf_i(f_m) = 0,\forall m\ne j,m\in[n]$, for every $2\le k\le n-1$.

We first prove that for every $S$, $T$ is unique, by contradiction. Suppose that for some $S$, $T$ is not unique, \ie, there exists $T'\ne T\subset [n]$ such that $T'$ satisfies the condition. Then, by Definition~\ref{def:inf}, changing the values of the coordinates $x_i,i\in S$ can change only the values of $f_j(\vx),j\in T\cap T'$ but not the values of other $f_j(\vx),j\in [n]\setminus (T\cup T')$. This results in $|\{f(\vx),\vx\in\gX\}| \le 2^{n-|S|} \cdot 2^{|T\cap T'|} = 2^{n-k+|T\cap T'|}< 2^n$, contradicting the bijectivity of $f$. Therefore, the assumption is false and $T$ is unique. This allows us to define a mapping $\phi: S\mapsto T$.

Let $\gS_k = \{S\subset[n]\mid |S| = k\}$ be the set of all subsets of $[n]$ with cardinality $k$. We then prove that $\phi:S\mapsto T$ is bijective on $\gS_k$. To this end, it suffices to show that every $S\ne S'\in\gS_k$ are mapped to different $T$. This can be similarly proved by contradiction as in proving the uniqueness of $T$: if it is false, then there exists a subset $S'' = S\cup S'$ and $T$ such that $\inf_i(f_j) = 0,\,\forall i\in S'',\,j\in [n]\setminus T$. Then, by Definition~\ref{def:inf}, changing the values of the coordinates $x_i,i\in S''$ can change only the values of $f_j(\vx),j\in T$ but not the values of other $f_j(\vx),j\notin T$. This results in $|\{f(\vx),\vx\in\gX\}| \le 2^{|T|} \cdot 2^{n-|S''|} = 2^{n+k-|S''|}< 2^n$, contradicting the bijectivity of $f$. Therefore, the assumption is false and every $S\ne S'\in\gS_k$ are mapped to different $T$.

Given that $\phi: S\mapsto T$ is bijective, we know that for every subset $\gS\subseteq \gS_k$, there exists a unique subset $\gT = \{T = \phi(S)\mid S\in\gS\}\subseteq \gS_k$ such that $|\gS| = |\gT|$. We then move on to prove the proposition (*) by contradiction: suppose it is false, \ie, for some $i\in [n]$, there exists $U\subset[n]$ with $|U| \ge 2$ such that $\inf_i(f_j)>0$ for every $j\in U$ and $\inf_i(f_j) = 0$ for every $j\in [n]\setminus U$. If $|U| > k$, then for any $S$ such that $i\in S$, there does not exist a feasible $T$, which is a contradiction. If $2\le |U| \le k$, consider $\gS = \{S\subset \gS_k\mid i\in S\}$ with $|\gS| = C_{n-1}^{k-1}$. By the definition of $\phi$, we know that $\gT \subseteq \{T\subset\gS_k\mid U\subseteq T\}$, which gives $|\gT|\le C_{n-|U|}^{k-|U|} < C_{n-1}^{k-1} = |\gS|$. Thus, the assumption is false and proposition (*) is true. This completes the proof.
\end{proof}

% Our next lemma shows that the inverse of parity functions is also parity functions.

% \begin{lemma}
% \label{lemma:parity_inverse}
% Let $f:\{-1,1\}^n\to\{-1,1\}^n$ be a bijection. If for every $i\in [n]$, $f_i$ is a parity function of its input coordinates, then $f_i^{-1}$ is also a parity function of its input coordinates for every $i\in [n]$.
% \end{lemma}
% \begin{proof}
% Note that parity functions correspond to linear transformations in the finite field $\sF_2^n$. Since $f$ is a bijection composed of parity functions, it corresponds to an invertible linear transform over $\sF_2^n$ and hence has a linear inverse. This means that each coordinate function $f_i^{-1},\forall i\in [n]$ can be expressed as parity functions of the output coordinates of $f$.
% \end{proof}

Our next lemma shows that any bijective transform on $\{-1,1\}^d$ can induce a bijective transform on $\gF^d$.

\begin{lemma}
\label{lemma:bijection}
Let $T:\{-1,1\}^d\to\{-1,1\}^d$ be a bijective transform. Then, $\gF^d\circ T = \gF^d$.
\end{lemma}
\begin{proof}
It suffices to show that the mapping $h'\mapsto h'\circ T$ for $h'\in\gF^d$ is bijective. On one hand, it is obvious that $h'\circ T\in\gF^d$ for every $h'\in\gF^d$. On the other hand, for each $h'\in\gF^d$, there exists $h'' = h'\circ T^{-1}$ such that $h''\circ T = h'$. This completes the proof.
\end{proof}

We then present a lemma from~\citet{abbe_generalization_2023} that guarantees the uniqueness of low-degree solutions when the training data is sampled from a Hamming ball.
\begin{lemma}[\citet{abbe_generalization_2023}, Theorem 5.1]
\label{lemma:abbe}
Consider a Boolean function $f:\{\pm1\}^d\to\sR$. Then, there exists a unique function $f_r:\{\pm 1\}^d\to\sR$ such that for every $\vz\in B_r \vcentcolon= \{\vz\in\{\pm 1\}^d\mid\#_{-1}(\vz)\le r\}$, we have $f_r(\vz) = f(\vz)$ and $\deg(f_r)\le r$.
\end{lemma}

Our next lemma shows that a bijection on $\gF^n$ that preserves the degree of all parity functions preserves the degree of all functions.

\begin{lemma}
\label{lemma:invertible_transform_degree}
Let $U:\gF^n\to\gF^n$ be an invertible linear transform. If $\deg(U(\chi_S)) = \deg(\chi_S)$ for every $S\subseteq [n]$, then we have
\begin{equation}
\deg(U(f)) = \deg(f),\,\forall f\in \gF^n.
\end{equation}
\end{lemma}
\begin{proof}
By the linearity of $U$, we have
\begin{align}
U(f) &= U\bigg(\sum_{S\subseteq[n]}\hat{f}(S)\chi_S\bigg) = \sum_{S\subseteq[n]}\hat{f}(S)U(\chi_S).
\end{align}
It then follows from Definition~\ref{def:degree} that
\begin{equation}
\deg(U(f)) = \max\left\{\deg(U(\chi_S)):\hat{f}(S)\ne 0\right\} = \max\left\{\deg(\chi_S):\hat{f}(S)\ne 0\right\} = \deg(f).
\end{equation}
This completes the proof.
\end{proof}

% Finally, we introduce a lemma showing that the degree of an invertible Boolean function equals to that of its inverse, when the degree is defined as the maximal degree of all coordinates.

% \begin{lemma}
% \label{lemma:degree_inverse}
% Let $f:\{-1,1\}^n\to\{-1,1\}^n$ be an invertible function. Then, the following holds:
% \begin{equation}
% \max_{i\in [n]} \deg(f_i) = \max_{j\in [n]} \deg\left(f^{-1}_j\right)
% \end{equation}
% \end{lemma}
% \begin{proof}
% On one hand, for every $i\in [n]$, we have
% \begin{equation}
% \deg(f_i) = 
% \end{equation}
% \end{proof}


\subsection{Proof of Theorem~\ref{thm:single-task}}
\label{appsec:proof_single-task}

\begin{proof}
Since $g\circ\Phi$ is an realization of $h$, we have $g\circ\Phi\in \gH(h)$. This gives
\begin{equation}
\impdeg(h^*) = \deg(h^*) = \deg(\hmin(h)) \le \deg(g\circ\Phi).
\end{equation}
Thus, it suffices to prove that $\deg(g\circ\Phi)\le \impdeg(g\circ\Phi)$ for every $g:\gZ\to\sR$ and $\Phi:\gX\to\gZ$. Let the Fourier-Walsh transform of $g$ and $\Phi_i,i\in[d]$ be
\begin{equation}
g(\vz) = \sum_{G\subseteq[d]}\hat{g}(G)\prod_{i\in G}z_i
\label{appeq:z}
\end{equation}
and
\begin{equation}
\Phi_i(\vx) = \sum_{S\subseteq[m]}\hat{\Phi}_i(S)\prod_{j\in S}x_j,
\label{appeq:phi}
\end{equation}
respectively. Plugging~\eqref{appeq:phi} into~\eqref{appeq:z} with $z_i = \Phi_i(\vx)$ gives
\begin{align}
(g\circ\Phi)(\vx) &= \sum_{G\subseteq[d]}\hat{g}(G)\prod_{i\in G}\left(\sum_{S\subseteq[m]}\hat{\Phi}_i(S)\prod_{k\in S} x_k\right)\\
&= \sum_{G\in\{G'\subseteq[d], \hat{g}(G') \ne 0\}} \hat{g}(G)\prod_{i\in G}\left(\sum_{S\in\{S'\subseteq[m],\hat{\Phi}_i(S')\ne 0\}}\hat{\Phi}_i(S)\prod_{k\in S}x_k\right).
\end{align}
We thus have
\begin{align}
\deg(g\circ\Phi) &\le \max_{G\in\{G'\subseteq[d],\hat{g}(G')\ne 0\}}\sum_{i\in G} \max_{S\in\{S'\subseteq[m],\hat{\Phi}_i(S')\ne 0\}}|S|\\
&\le \max_{G\subseteq[d]}\sum_{i\in G} \max_{S\in\{S'\subseteq[m],\hat{\Phi}_i(S')\ne 0\}}|S|\\
&\le \sum_{i\in [d]} \max_{S\in\{S'\subseteq[m],\hat{\Phi}_i(S')\ne 0\}}|S|\\
& = \sum_{i\in [d]}\deg(\Phi_i).
\end{align}
Meanwhile, Definition~\ref{def:impl_degree} gives
\begin{align}
\impdeg(g\circ\Phi) &= \deg(g) + \deg(\Phi) = \deg(g) + \sum_{i\in [d]}\deg(\Phi_i)\\
&\ge \sum_{i\in [d]}\deg(\Phi_i).
\end{align}
Therefore, we have $\deg(g\circ\Phi)\le \impdeg(g\circ\Phi)$. This completes the proof.
\end{proof}


\subsection{Proof of Theorem~\ref{thm:multi-task}}
\label{appsec:proof_multi-task}

\begin{proof}
By Lemma~\ref{lemma:degree}, we can upper-bound the degree of each $\Phi_j^*,j\in[d]$ by
\begin{equation}
\deg(\Phi_j^*)\le d.
\end{equation}
Expanding the LHS of~\eqref{eq:thm2} then gives
% \begin{equation}
\begin{align}
\impdeg(h^*) - \impdeg(g\circ\Phi^*) &= \sum_{i\in [n]} \deg(h_i^*) - \sum_{i\in [n]}\deg(g_i) - \sum_{j\in[d]}\deg(\Phi^*_j)\\
&\ge \sum_{i\in [n]} \deg(h_i^*) - \sum_{i\in [n]}\deg(g_i) - d^2.\label{appeq:4.2}
% \end{aligned}
\end{align}
Note that $h^*_i\in\hmin(h)$ and $g_i\circ\Phi$ is an realization of $h_i$ for every $i\in [n]$. By Definition~\ref{def:conditional_degree}, we 
have
\begin{equation}
\deg(h_i\mid\Phi^*) = \deg(h_i^*) - \deg(g_i),\forall i\in[n].
\label{appeq:conditional_degree}
\end{equation}
Plugging equation~\eqref{appeq:conditional_degree} into~\eqref{appeq:4.2} completes the proof.
\end{proof}

\subsection{Proof of Theorem~\ref{thm:no_free_lunch}}
\label{appsec:proof_no_free_lunch}

\begin{proof}
% Recall that $|\gZ| = |\{-1,1\}^d| = 2^d$. Hence, for every $\Phi\in \{\varphi:\gX\to\gZ\mid |\{\varphi(\vx), \vx\in\gX\}| = 2^d\}$, we can construct a bijective transform $T:\vz\mapsto \Phi(\vx)$ where $\vz = \psi^{-1}(\vx)$. By Parseval's Theorem, $T$ is unique.
Note that for every task $h\in\gF^d\circ \psi^{-1}$, we can write $h = h'\circ \psi^{-1}$ for some $h'\in\gF^d$. Thus, for every $g:\gZ\to\sR$ and $\Phi:\gX\to\gZ$ such that $g\circ \Phi\in\gH(h)$, we have $(g\circ\Phi)(\vx) = h(\vx) = (h'\circ \psi^{-1})(\vx),\forall \vx\in\gX$, which amounts to $g(\vz) = (h'\circ T^{-1})(\vz),\forall \vz\in\{-1,1\}^d$.

For every $h_1,\ldots,h_n$, $g:\gZ\to\{-1,1\}^n$, and $\Phi:\gX\to\gZ$ such that $g_i\circ \Phi\in\gH(h_i)$ for every $i\in[n]$, we have
\begin{equation}
% \begin{aligned}
\lim_{n\to\infty} \frac{1}{n} \,\impdeg(g\circ\Phi) = \lim_{n\to\infty} \left(\frac{1}{n} \sum_{j\in[d]}\deg(\Phi_j) + \frac{1}{n}\sum_{i\in[n]}\deg(g_i)\right).
\label{eq:13}
% \end{aligned}
\end{equation}
Applying Lemma~\ref{lemma:degree}, we have $\deg(\Phi)_j\le d,\forall j\in[d]$. This gives 
\begin{equation}
\lim_{n\to\infty} \frac{1}{n} \sum_{j\in[d]} \deg(\Phi_j) = 0.
\label{appeq:thm4.4-1}
\end{equation}
Meanwhile, since $h_i:\gX\to\sR,i\in[n]$ are independently and uniformly sampled from $\gF^d\circ \psi^{-1}$, we have
\begin{align}
\lim_{n\to\infty}\frac{1}{n}\sum_{i\in[n]}\deg(g_i) &= \E_{h\sim U(\gF^d\circ \psi^{-1})}\deg(h\circ f\circ T^{-1})\\
&= \E_{h'\sim U(\gF^d)} \deg(h'\circ T^{-1}) \\
&= \frac{1}{|\gF^d|}\sum_{h'\in\gF^d}\deg(h'\circ T^{-1}).\label{appeq:thm4.4-2}
\end{align}
By Lemma~\ref{lemma:bijection}, we have $\gF^d\circ T^{-1} = \gF^d$. This gives
\begin{equation}
\sum_{h'\in\gF^d}\deg(h'\circ T^{-1}) = \sum_{h'\in\gF^d}\deg(h').
\label{appeq:thm4.4-3}
\end{equation}
Plugging equations~\eqref{appeq:thm4.4-1},~\eqref{appeq:thm4.4-2}, and~\eqref{appeq:thm4.4-3} into equation~\eqref{eq:13} gives
\begin{equation}
\lim_{n\to\infty} \frac{1}{n} \,\impdeg(g\circ\Phi) = \frac{1}{|\gF^d|}\sum_{h'\in\gF^d}\deg(h'),
\end{equation}
which is a constant independent of $T$ (and thus independent of $\Phi$). Therefore, for any two viable representations $\Phi, \Phi'$ and $g, g'\in(\gF^d)^n$ with $g_i\circ\Phi$ and $g'_i\circ\Phi', \forall i\in[n]$, we must have $\lim_{n\to\infty} \frac{1}{n}\left(\impdeg(g\circ\Phi) - \impdeg(g'\circ\Phi')\right) = 0$. This completes the proof.
\end{proof}


\subsection{Proof of Corollary~\ref{corollary:degree_k}}
\label{appsec:proof_corollary}

\begin{proof}
By Definition~\ref{def:conditional_degree}, we have
\begin{equation}
\deg(h\mid \psi^{-1}) > 0 \Longleftrightarrow \deg(\hmin(h)) > \deg(g)
\end{equation}
for $g\circ \psi^{-1} \in\gH(h)$. By Lemma~\ref{lemma:degree}, we have $\deg(\hmin(h)) \le d$ for every $h:\gX\to\sR$. Thus, for $\deg(\hmin(h)) > \deg(g)$ to hold, we must have $\deg(h\circ \psi) = \deg(g) \le d-1$. By Definition~\ref{def:k_degree}, this gives $h\in\gF^d_{d-1}\circ \psi^{-1}$, completing the proof.
\end{proof}


\subsection{Proof of Theorem~\ref{thm:world_model}}
\label{appsec:proof_world_model}

\begin{proof}
We first prove the following lemma that characterizes the averaged degree change for Boolean function in $\gF^d_k$ when composed with invertible transforms.
\begin{lemma}
\label{lemma:degree_composition}
For every integer $1\le k\le d$ and every bijection $T:\{-1,1\}^d\to\{-1,1\}^d$, we have
\begin{equation}
\sum_{h'\in\gF_k^d}\deg(h'\circ T) \ge \sum_{h'\in\gF_k^d}\deg(h').
\end{equation}
In particular, when $k=1$, the equality holds if and only if $\deg(T_i) = 1$ for every $i\in [d]$.
\end{lemma}
\begin{proof}[Proof of Lemma~\ref{lemma:degree_composition}]
For every $\gG\subseteq \gF^d$, let $\gG\circ T = \{h'\circ T\mid h'\in \gG\}$.
By Lemma~\ref{lemma:bijection}, we know that the mapping $h'\mapsto h'\circ T$ is bijective on $\gF^d$. We thus have $|\gF^d_k\circ T| = |\gF^d_k|$ for every $k$. For $\gF^d_k\circ T$, there are two possibilities:
\begin{enumerate}
\item $\gF^d_k\circ T = \gF^d_k$. This immediately gives
\begin{align}
	\sum_{h'\in\gF_k^d}\deg(h'\circ T) = \sum_{h'\in \gF_k^d\circ T}\deg(h') = \sum_{h'\in\gF_k^d}\deg(h').
\end{align}

\item $\gF^d_k\circ T \ne \gF^d_k$. We can then decompose $\sum_{h'\in\gF_k^d}\deg(h'\circ T)$ as follows:
\begin{align}
\sum_{h'\in\gF_k^d}\deg(h'\circ T) &= \sum_{h'\in (\gF_k^d\circ T)\cap\gF_k^d}\deg(h') + \sum_{h'\in (\gF_k^d\circ T)\cap(\gF_d\setminus \gF_k^d)}\deg(h') \\
&= \sum_{h'\in\gF^d_k}\deg(h') + \sum_{h'\in (\gF_k^d\circ T)\cap(\gF^d\setminus \gF_k^d)}\deg(h') - \sum_{h'\in \gF^d_k\setminus (\gF^d_k\circ T)}\deg(h')\label{appeq:decompose}
\end{align}
Note that $|\gF^d_k\circ T| = |\gF^d_k|$ gives $|(\gF_k^d\circ T)\cap(\gF^d\setminus \gF_k^d)| = |\gF^d_k\setminus (\gF^d_k\circ T)|$. Meanwhile, by Definition~\ref{def:k_degree}, we have $\deg(h')\le k$ for every $h'\in\gF^d_k$ and $\deg(h') > k$ for every $h'\in \gF^d\setminus \gF^d_k$. Taking these two facts together, we have
\begin{equation}
\sum_{h'\in (\gF_k^d\circ T)\cap(\gF^d\setminus \gF_k^d)}\deg(h') - \sum_{h'\in \gF^d_k\setminus (\gF^d_k\circ T)}\deg(h') > 0.
\label{appeq:pos_degree}
\end{equation}
Plugging equation~\eqref{appeq:pos_degree} into equation~\eqref{appeq:decompose} then gives $\sum_{h'\in\gF_k^d}\deg(h'\circ T) > \sum_{h'\in\gF_k^d}\deg(h')$.
\end{enumerate}
Combining the above two cases, we conclude that $\sum_{h'\in\gF_k^d}\deg(h'\circ T) \ge \sum_{h'\in\gF_k^d}\deg(h')$ for every $1\le k\le d$.
Note that the above analysis also gives a necessary and sufficient condition for the equality to hold: $\gF^d_k\circ T = \gF^d_k$.

In particular, when $k=1$, $\sum_{h'\in\gF_k^d}\deg(h'\circ T) = \sum_{h'\in\gF_k^d}\deg(h')$ holds only for $T$ satisfying that $\gF^d_1\circ T = \gF^d_1$. Note that for every non-constant function $h'\in\gF_1^d$, there exists $i\in [d]$ such that $(h'\circ T)(\vz) \in\{T_i(\vz),-T_i(\vz)\}$ for every $\rvz\in\{-1,1\}^d$. Due to the arbitrariness of $h'$, we must have $\deg(T_i) = 1$ for every $i\in [d]$.
\end{proof}

We now move on to prove Theorem~\ref{thm:world_model}.
Our aim is to prove that the minimizer $(\Phi^*,g^*)$ of the optimization problem~\eqref{eq:world_model} learns the world model by negations and permutations when the number of tasks $n\to\infty$. Due to equations~\eqref{eq:13} and~\eqref{appeq:thm4.4-1}, the original problem equals to
\begin{equation}
\begin{aligned}
&\min_{\Phi:\gX\to\gZ, g\in\gF^d} \lim_{n\to\infty} \frac{1}{n}\,\sum_{i\in [n]}\deg(g_i) \\
&\hspace{2em} \mathrm{s.t.}\hspace{1.75em} g_i\circ\Phi\in\gH(h_i),\,\forall i\in[n].
\end{aligned}
\label{appeq:world_model}
\end{equation}
Due to the constraint $g_i\circ\Phi\in\gH(h_i),\forall i\in [n]$, we know that there must exist a bijection $T:\gZ\to\gZ$ such that for every $\vx\in\gX$, $\Phi(\vx) = T(\vz)$, with $\vz = \psi^{-1}(\vx)$ being the true latent variable. Therefore, it remains to prove the existence of a bijection $T:\gZ\to\gZ$ with $\deg(T_i) = 1,\forall i\in[d]$ such that for every $\vx\in\gX$, $\Phi^*(\vx) = T(\vz)$.
% and it remains to prove that $\deg(T_i) = 1,\forall i\in [d]$.

% Note that for a bijection $T:\gZ\to\gZ$, $\deg(T_i)=1,\forall i\in [d]$ is equivalent to $\deg(T_i^{-1})=1,\forall i\in [d]$.
% Further, by Lemma~\ref{lemma:degree_1}, proving $\deg(T_i^{-1}) = 1$ amounts to proving the following proposition:

% (*) For every $S\subset[n]$ with $|S| = d-1$, there exists $V\subset[n]$ with $|V| = d-1$ such that for every $i\in S$, $\inf_i(T_j^{-1}) > 0$ for at least one $j\in V$ and $\inf_i(T_j^{-1}) = 0$ for every $j\in [n]\setminus V$.
For every $g_i\circ\Phi\in\gH(h_i)$, we have $h_i=g_i\circ\Phi = g_i\circ T \circ \psi^{-1}$. We then have
\begin{align}
\lim_{n\to\infty} \frac{1}{n}\,\sum_{i\in [n]}\deg(g_i) &= \E_{k\sim\mathrm{Discrete}(p_1,\ldots,p_d)}\E_{h\sim U(\gF_k^d\circ \psi^{-1})} \deg(h\circ f\circ T^{-1})\\
&= \E_{k\sim \mathrm{Discrete}(p_1,\ldots,p_d)}\E_{h'\sim U(\gF_k^d)} \deg(h'\circ T^{-1}) \\
&= \sum_{k\in [d]} p_k \cdot\frac{1}{|\gF_k^d|}\sum_{h'\in\gF_k^d}\deg(h'\circ T^{-1}).\label{appeq:deg_all}
\end{align}
By Lemma~\ref{lemma:degree_composition}, we have
\begin{equation}
\sum_{h'\in\gF_k^d}\deg(h'\circ T^{-1}) \ge \sum_{h'\in\gF_k^d}\deg(h')
\label{appeq:deg_ineq1}
\end{equation}
for every $k\in [d]$. Plugging~\eqref{appeq:deg_ineq1} into equation~\eqref{appeq:deg_all} gives
\begin{align}
\lim_{n\to\infty} \frac{1}{n}\,\sum_{i\in [n]}\deg(g_i) &\ge \sum_{k\in [d]} p_k \cdot\frac{1}{|\gF_k^d|}\sum_{h'\in\gF_k^d}\deg(h'),
\label{appeq:deg_ineq}
\end{align}
where the equality holds only if $\deg(T_i) = 1$ for every $i\in [d]$. This completes the proof.
\end{proof}

\begin{remark}
A limitation of Theorem~\ref{thm:world_model} is that we requires a non-zero probability of explicitly sampling degree-$1$ tasks (\ie, $p_1 >0$), in which latent variables are essentially observed as task outputs. If $p_1 = 0$, we can still prove equation~\eqref{appeq:deg_ineq} by applying Lemma~\ref{lemma:degree_composition}; in other words, we can still prove that every representation $\Phi^*$ that learns the world model up to negations and permutations is a minimizer of the optimization problem~\eqref{eq:world_model}. However, such minimizers may not be \emph{unique}, because Lemma~\ref{lemma:degree_composition} only proves the equivalence between the equality and $\deg(T_i) = 1,\forall i\in [d]$ when $k=1$ but not $1< k\le d-1$. In fact, we can construct hard examples showing that in some cases, there indeed exists other $\Phi$ that minimizes $\sum_{h'\in\gF^d_k}\deg(h'\circ T)$ for every $k\in [d]\setminus \{1\}$.
\begin{example}
\label{example:counter}
Let $d = 3$ and let $T:\{-1,1\}^3\to\{-1,1\}^3$ be a bijective transform defined as
\begin{equation}
T_1(\vz) = z_1,\quad T_2(\vz) = z_1 z_2,\quad T_3(\vz) = z_1 z_3.
\end{equation}
One can easily verify that for every $k\in\{2,3\}$, every parity function $\chi_S(T(\vz)) = \prod_{i\in S} T_i(\vz)$ with $|S|\le k$ satisfy $\deg(\chi_S(T(\vz)))\le k$. By the Fourier-Walsh transform, this amounts to $\gF_k^3\circ T = \gF^3_k$ for $k = \{2,3\}$, which gives $\sum_{h'\in\gF^3_k}\deg(h'\circ T) = \sum_{h'\in\gF^3_k}\deg(h')$ by the proof of Lemma~\ref{lemma:degree_composition}. Thus, in this case we require $p_1 > 0$ to ensure that the representation $\Phi$ satisfying $\Phi(\vx) = T^{-1}(\vz)$ for every $\vx \in \gX$ and $\vz = \psi^{-1}(\vx)$ is not a minimizer of~\eqref{eq:world_model}.
\end{example}
Nevertheless, we do believe that cases like Example~\ref{example:counter} are rare. This is because by the proof of Lemma~\ref{lemma:degree_composition}, such examples must construct a bijection $T$ such that $\gF^d_k\circ T = \gF^d_k$ for \emph{every} $k\in [d]\setminus \{1\}$, which is increasingly difficult when $d$ becomes large. For example, if we increase the dimension of $\gZ$ from $3$ to $4$ in Example~\ref{example:counter} and keep $T_1$, $T_2$ and $T_3$ as is, it could be verified that there does not exist a $T_4:\{\-1,1\}^3\to\{-1,1\}$ satisfying that $\gF^4_k\circ T = \gF^4_k$ for every $k\in \{2,3\}$.
We believe that this intuition could be rigorously proved using \eg, Lemma~\ref{lemma:degree_1} or other techniques and leave it as future work.
\end{remark}

% Recall that $\Phi^*\in\chi_\vz$, which means that each $T_i,i\in[d]$ can be written as a parity function of $\vz$. By Lemma~\ref{lemma:parity_inverse}, $T_i^{-1}$ can also be written as parity functions of $\vz$, \ie, $\exists S_i\subseteq[d],\,T_i^{-1}(\vz) = \chi_{S_i}(\vz)\,\forall i\in [d]$. 


\subsection{Proof of Theorem~\ref{thm:benefits}}
\label{appsec:proof_benefits}

\begin{proof}
We first prove the following lemma:
\begin{lemma}
\label{lemma:min_deg_hamming}
Assume that the latent variables are uniformly sampled from the Hamming ball $B_r = \{\vz\in\{\pm 1\}^d\mid \#_{-1}(\vz)\le r\}$ with $r<d$. Then, for every task $h$, we have
\begin{equation}
\deg(\hmin(h)) \le \bigg\lceil\log_2\sum_{i=0}^r{d\choose r}\bigg\rceil.
\end{equation}
\end{lemma}

\begin{proof}[Proof of Lemma~\ref{lemma:min_deg_hamming}]
The main idea of the proof is similar to that of Lemma~\ref{lemma:degree} and Lemma~\ref{lemma:existence}.

Let $k = \lceil\log_2\sum_{i=0}^r{d\choose r}\rceil$. Due to the invertibility of $\psi$, we know that $|\{\vx\mid p(\psi^{-1}(\vx)) > 0\}| = |B_r| = \sum_{i=0}^r {d\choose r}$. Therefore, for any $S = \{i_1,\ldots,i_{k'}\}\subseteq [d]$ such that $k' > k$, there must exist $b_1,\ldots,b_{k'}\in\{-1,1\}^{k'}$ such that
\begin{equation}
\prod_{j\in[k']}(x_{i_j} + b_j) = \prod_{j\in S}x_j + \sum_{S'\subset S,|S'|\le k'-1} b_{S'}\prod_{k\in S'}x_k = 0
\label{appeq:thm4.9_1}
\end{equation}
for every $\vx\in\gX'\vcentcolon= \{\vx\mid p(\psi^{-1}(\vx)) > 0\}$, where $b_{S'}\in\{-1,1\}$ for every $S'\subset S$---if this does not hold, then we have $|\gX'| \ge 2^{k'} > 2^k \ge \sum_{i=0}^r{d\choose r}$, which contradicts $|\gX'| = \sum_{i=0}^r{d\choose r}$. By equation~\eqref{appeq:thm4.9_1}, we can replace every degree-$k'$ monomial $\chi_S(\vx) = \prod_{j\in S}x_j$ by a degree-$k'-1$ polynomial $-\sum_{S'\subset S,|S'|\le k'-1} b_{S'}\prod_{k\in S'}x_k$. Iteratively using this replacement in the Fourier-Walsh transform of $h$ gives the desired result.
\end{proof}


We can now prove Theorem~\ref{thm:benefits}.

\textit{Proof of (\romannumeral 1).} By Lemma~\ref{lemma:min_deg_hamming}, for every $h^*\in\hmin(h)$, we have $\deg(h^*)\le k = \lceil\log_2\sum_{i=0}^r{d\choose r}\rceil$. The test MSE of any $h^*$ thus satisfies
\begin{align}
\mathrm{err}(h^*) &= \E_{\rvz\sim U(\{-1,1\}^d)}[(h^*(\rvx) - h(\rvx))^2] = \E_{\rvz\sim U(\{-1,1\}^d)}[h^*(\rvx)^2 + h(\rvx)^2 - 2h^*(\rvx)h(\rvx)]\\
&= 1 + \E_{\rvz\sim U(\{-1,1\}^d)} h^*(\rvx)^2 - 2\,\E_{\rvz\sim U(\{-1,1\}^d)}h^*(\psi(\rvz))h(\psi(\rvz))\\
&> 1 - 2\langle h^*\circ\psi,h\circ\psi \rangle\label{appeq:thm4.9_2}
\end{align}
We then prove that $k\ge r+1$. To see this, recall that $d>r$ and one can directly verify
\begin{align}
k &= \bigg\lceil\log_2\sum_{i=0}^r{d\choose r}\bigg\rceil \ge \bigg\lceil\log_2\sum_{i=0}^r{r+1\choose r}\bigg\rceil = \left\lceil\log_2\left(2^{r+1} - 1\right)\right\rceil = r+1.
\end{align}
Recall that $h\circ\psi$ is a parity function $\chi_S$ with $\deg(\chi_S) = |S| = q\ge k$. Meanwhile, since $\deg(h\mid\psi^{-1}) \ge q-r$, by Definition~\ref{def:conditional_degree} we have $\deg(h^*) - \deg(g) \ge q-r$, \ie, $\deg(g) \le \deg(h^*) -q + r\le r$, for every $g$ satisfying $g\circ\psi^{-1} = h^*$. This gives $\deg(h^*\circ\psi) \le r$. Recall equation~\eqref{appeq:parity_fourier}, we have
\begin{equation}
\langle h^*\circ\psi,h\circ\psi \rangle = \widehat{h^*\circ\psi}(S) = 0.
\label{appeq:inner_product_zero}
\end{equation}
Plugging equation~\eqref{appeq:inner_product_zero} into~\eqref{appeq:thm4.9_2} completes the proof.

\textit{Proof of (\romannumeral 2).} Since $\deg(h\mid \psi^{-1}) \ge q-r$ and $\Phi^*$ learns the world model up to negations and permutations, we have $\deg(h\mid \Phi^*) = \deg(h\mid \psi^{-1}) \ge q-r$ and hence $\deg(g^*) \le r$. By Lemma~\ref{lemma:abbe}, $g^*$ is unique. We thus have $h = g^*\circ\Phi^*$, which gives the desired result.
\end{proof}


\subsection{Proof of Theorem~\ref{thm:basis}}
\label{appsec:proof_basis}

\begin{proof}
We first prove the following lemma:
\begin{lemma}
\label{lemma:compatibility}
If $U$ is compatible, then $\deg_U(f) = \deg(f)$ holds for every $f\in\gF^n$.
\end{lemma}
\begin{proof}[Proof of Lemma~\ref{lemma:compatibility}]
By Defintion~\ref{def:compatibility}, we have $\deg(U(\chi_S)) = \deg(\chi_S)$ for every compatible $U$.
Applying Lemma~\ref{lemma:invertible_transform_degree}, we further have $\deg(U(\chi_S)) = \deg(\chi_S) = \deg(U^{-1}(\chi_S))$.
This gives
\begin{align}
\deg_U(f) &= \max\left\{\deg(U^{-1}(\chi_S)):\hat{f}(S)\ne 0\right\} = \max\left\{\deg(\chi_S):\hat{f}(S)\ne 0\right\} = \deg(f),
\end{align}
which completes the proof.
\end{proof}

We are now ready to prove Theorem~\ref{thm:basis}. Note that under the new basis $\{U(\chi_S)\}$, the original optimization problem~\eqref{eq:world_model} becomes (also applying the equivalence between~\eqref{eq:world_model} and~\eqref{appeq:world_model}):
\begin{equation}
\begin{aligned}
&\min_{\Phi:\gX\to\gZ, g\in\gF^d} \lim_{n\to\infty} \frac{1}{n}\,\sum_{i\in [n]}\deg_U(g_i) \\
&\hspace{2em} \mathrm{s.t.}\hspace{1.75em} g_i\circ\Phi\in\gH(h_i),\,\forall i\in[n].
\end{aligned}
\label{appeq:world_model_basis}
\end{equation}

\textit{Proof of (\romannumeral 1).} If $U$ is compatible, then by Lemma~\ref{lemma:compatibility} we have $\deg_U(f) = \deg(f)$ for any Boolean function $f$. This immediately gives the equivalence between~\eqref{appeq:world_model_basis} and~\eqref{appeq:world_model} and hence their minimizers. We thus have that $\Phi^*$ learns the world model up to negations and permutations as in Theorem~\ref{thm:world_model}.

\textit{Proof of (\romannumeral 2).}
% By the proof of Theorem~\ref{thm:world_model} (Section~\ref{appsec:proof_world_model}), $\Phi^*$ learns the world model up to negations and permutations if and only if $\sum_{h'\in\gF^d_k}\deg_U(h'\circ T^{-1}) = \sum_{h'\in\gF^d_k}\deg(h')$ holds for every $k\in [d]$, where $T = \Phi^*\circ\psi$ with $\deg(T_j) = 1,\,\forall j\in [n]$.
If $U$ is not compatible, then due to the invertibility of $U$, there exists at least one $h''\in\gF^d_k$ such that $\deg_U(h''\circ T^{-1}) > \deg(h'')$ for some $k<d$. Since composing Boolean functions with degree-$1$ transforms does not change the degree of functions, we have $\deg_U(h''\circ T^{-1}) > \deg(h'')$ and hence $\sum_{h'\in\gF^d_k}\deg_U(h'\circ T^{-1}) > \sum_{h'\in\gF^d_k}\deg(h')$.

In particular, let $\{\chi_1,\ldots,\chi_{2^d}\}$ be the set of all parity functions with $d$-dimensional inputs. For every $k\in [d]$, we can construct $U$ such that:
\begin{enumerate}
\item $U(\chi_1),\ldots,U(\chi_{2^d})$ is a permutation of $\chi_1,\ldots,\chi_{2^d}$;
\item For every $i\in [d]$, we have $U(\chi_{\{i\}}) = \chi_S$ for some $S\subseteq[d]$ with $|S| = k$ and $U(\chi_S) = \chi_{\{i\}}$.
\end{enumerate}
Recall that $\deg_U(h'\circ T^{-1}) = \deg(U^{-1}(h'\circ T^{-1}))$. Therefore, to ensure that $\sum_{h'\in\gF^d_k}\deg_U(h'\circ T^{-1}) = \sum_{h'\in\gF^d_k}\deg(h')$ for $k=1$, we thus must have $T^{-1}_j = \chi_S$ for some $S\subseteq[d]$ and $j\in [d]$, with $|S| = k$. This means
\begin{equation}
\max_{i\in [d]}\deg\left(T^{-1}_i\right)\ge \deg\left(T^{-1}_j\right) = k,
\end{equation}
which gives the desired result.
\end{proof}


\section{Experiment Details}
\label{appsec:exp}

This section presents additional experiment details. All of our experiments were conducted using PyTorch~\citep{paszke_pytorch_2019} and on NVIDIA V100, NVIDIA A100, and NVIDIA H100 GPUs.

\subsection{Polynomial Extrapolation}
\label{appsec:extrapolation}

\paragraph{Dataset.} We consider fitting and extrapolating degree-$n$ polynomials with the form $P_n(x) = \sum_{i=0}^n a_i x^n$. Given an input $x\in\sR$, the label is given by $y = P_n(x)$. In our experiments, we consider three families of polynomials with degree $1$, $2$, and $3$. In each family, every coefficient $a_i,i\in\{0,1,\ldots,n\}$ is uniformly sampled from $[0,1)$. In our experiments, we sample $50$ polynomials in each family for the violin plots. Other data parameters are as follows:
\begin{itemize}
	\item Training, validation, and test data are uniformly sampled from $[-1,1)$, $[-1,1)$, and $[-2,2)$, respectively.
	\item For each polynomial instance, we sample $50,000$ training data, $1,000$ validation data, and $10,000$ test data.
\end{itemize}

\paragraph{Model and hyperparameters.} We consider MLPs with the following architecture:
\begin{equation}
\mathrm{MLP}(\vx) = \mW^{(d)}\sigma\left(\mW^{(d-1)}\sigma\left(\ldots\sigma\left(\mW^{(1)}\vx + \vb^{(1)}\right)\right)+\vb^{(d-1)}\right) + \vb^{(d)},
\end{equation}
where $\sigma$ is the activation function and for every $i\in [d]$, $\mW^{(i)}$ and $\vb^{(i)}$ are weights and bias of the $i$-th layer, respectively. For ReLU MLPs, all activation functions are set to ReLU; for our architecture, we replace half of ReLUs in every layer by the identity function $\sigma(x) = x$ and the quadratic function $\sigma(x) = x^2$, with the number of identity functions and quadratic functions being the same (\ie, both functions constitute $25\%$ activation functions, while the remaining $50\%$ are still ReLUs). We search the following hyperparameters for MLPs:
\begin{itemize}
	\item Number of layers $d$ is set to $4$.
	\item Width of each $\mW^{(i)}$ from $\{128, 256, 512\}$.
\end{itemize}

We train all MLPs with the mean square error (MSE) loss with the AdamW optimizer~\citep{loshchilov_decoupled_2019}. Training hyperparameters are as follows:
\begin{itemize}
	\item Initial learning rate from $\{1e-3, 1e-4, 1e-5\}$. We use a cosine learning rate scheduler.
	\item Weight decay is set to $0.1$.
	\item Batch size is set to $512$.
	\item Number of epochs is set to $400$.
\end{itemize}

\paragraph{Evaluation metric.} We evaluate all models using MSE on test data.

\begin{figure}[t]
\centering
\subcaptionbox{An example from the training distribution (sampled 5 frames with uniform spacing).}{
\includegraphics[width=0.16\linewidth]{figures/parabolic_test_id_0.pdf}
\includegraphics[width=0.16\linewidth]{figures/parabolic_test_id_6.pdf}
\includegraphics[width=0.16\linewidth]{figures/parabolic_test_id_12.pdf}
\includegraphics[width=0.16\linewidth]{figures/parabolic_test_id_18.pdf}
\includegraphics[width=0.16\linewidth]{figures/parabolic_test_id_24.pdf}
\includegraphics[width=0.16\linewidth]{figures/parabolic_test_id_30.pdf}
}\\
\centering
\subcaptionbox{An example from the test distribution with a larger radius and a larger initial velocity (sampled 5 frames with uniform spacing).}{
\includegraphics[width=0.16\linewidth]{figures/parabolic_test_ood_0.pdf}
\includegraphics[width=0.16\linewidth]{figures/parabolic_test_ood_6.pdf}
\includegraphics[width=0.16\linewidth]{figures/parabolic_test_ood_12.pdf}
\includegraphics[width=0.16\linewidth]{figures/parabolic_test_ood_18.pdf}
\includegraphics[width=0.16\linewidth]{figures/parabolic_test_ood_24.pdf}
\includegraphics[width=0.16\linewidth]{figures/parabolic_test_ood_30.pdf}
}
\caption{Two visualized examples of the parabolic motion.}
\label{appfig:parabolic}
\end{figure}

\begin{figure}[t]
\centering
\subcaptionbox{An example from the training distribution (sampled 5 frames with uniform spacing).}{
\includegraphics[width=0.16\linewidth]{figures/collision_test_id_16.pdf}
\includegraphics[width=0.16\linewidth]{figures/collision_test_id_19.pdf}
\includegraphics[width=0.16\linewidth]{figures/collision_test_id_22.pdf}
\includegraphics[width=0.16\linewidth]{figures/collision_test_id_25.pdf}
\includegraphics[width=0.16\linewidth]{figures/collision_test_id_28.pdf}
\includegraphics[width=0.16\linewidth]{figures/collision_test_id_31.pdf}
}\\
\centering
\subcaptionbox{An example from the test distribution with larger initial velocities (sampled 5 frames with uniform spacing).}{
\includegraphics[width=0.16\linewidth]{figures/collision_test_ood_1.pdf}
\includegraphics[width=0.16\linewidth]{figures/collision_test_ood_5.pdf}
\includegraphics[width=0.16\linewidth]{figures/collision_test_ood_9.pdf}
\includegraphics[width=0.16\linewidth]{figures/collision_test_ood_13.pdf}
\includegraphics[width=0.16\linewidth]{figures/collision_test_ood_17.pdf}
\includegraphics[width=0.16\linewidth]{figures/collision_test_ood_21.pdf}
}
\caption{Two visualized examples of the collision motion.}
\label{appfig:collision}
\end{figure}


\subsection{Learning Physical Laws}
\label{appsec:physics}

\paragraph{Dataset.} Inspired by~\citet{kang_how_2024}, we create training and test sequences representing ball-shaped object movements that adhere to two basic physical laws: (\romannumeral 1) single-object parabolic motion (reflecting Newton's second law of motion), and (\romannumeral 2) two-object elastic collision (reflecting the convervation of energy and momentum). In both settings, we consider a 2-dimensional environment in which each object is encoded by a three-dimensional tuple $(r_t, x_t, y_t)$ at every time step $t$, where $r_t$ represents the radius of the ball and $(x_t,y_t)$ stands for the 2-dimensional coordinates of the ball. To encode the timestamp information, we also include a dimension for the current timestamp $t$. This results in 4-dimensional inputs (\ie, $(r_t,x_t,y_t,t)$) in the parabolic motion setting and 7-dimensional inputs (\ie, $(r_t^1,x_t^1,y_t^1,r_t^2,x_t^2,y_t^2,t)$ in the collision motion setting. Each motion sequence consists of 32 frames with a timestep of 0.1. Details of both settings are as follows.

\begin{enumerate}
	\item \textbf{Parabolic motion.} This motion describes the process where a ball with an initial horizontal (\ie, along the $x$ axis) velocity falls due to a fixed gravity $g=9.8$ (along the $y$ axis). We use the following training and test parameters:
	\begin{itemize}
		\item Radius is uniformly sampled from $[0.7, 1.5]$ in training and from $[1.5, 2.0]$ in test.
		\item Initial velocity is uniformly sampled from $[1, 4]$ in training and from $[4.5, 6.0]$ in test.
	\end{itemize}
	\item \textbf{Collision motion.} This motion describes the process where two balls with different sizes and different initial velocities move horizontally towards each other and collide. We assume that the collision is perfectly elastic and all balls are with the same density, so the velocities of both balls can be inferred from their radii and initial velocities. We use the following training and test parameters:
	\begin{itemize}
		\item Radius of each object is uniformly sampled from $[0.7, 1.5]$ in training and from $[1.5, 2.0]$ in test.
		\item Initial velocity is uniformly sampled from $[2, 4]$ in training and from $[4.5, 6.0]$ in test.
		\item The horizontal distance between two objects is uniformly sampled from $[5, 15]$ in both training and test.
	\end{itemize}
\end{enumerate}

See Figure~\ref{appfig:parabolic} and Figure~\ref{appfig:collision} for visualized examples of the parabolic motion and examples of the collision motion.

For both settings, we sample 1M training sequence and $50,000$ test sequence.


\paragraph{Model and hyperparameters.} We train a decoder-only transformer~\citep{vaswani_attention_2017} conditioned on the first 3 frames to predict the remaining frames (expect for the timestamp dimension). For our method, we simply replace every MLP in the original transformer with our modified MLP as in Section~\ref{appsec:extrapolation} (replacing ReLU by GELU).
We use teacher-forcing in training that is similar to next-token prediction, \ie, the model only needs to predict the next frame (starting from the 4-th frame to the last 32-th frame) given all ground-truth frames prior to it. We use the following model hyperparameters:
\begin{itemize}
	\item Number of layers of transformer is set to $4$.
	\item Number of heads of transformer is set to $4$.
	\item Width of transformer is set to $512$.
\end{itemize}


We train all models using the MSE loss with the AdamW optimizer. Training hyperparameters are as follows:
\begin{itemize}
	\item Initial learning rate is randomly sampled from $[1e-6, 1e-3]$. We use a cosine learning rate scheduler.
	\item Weight decay is set to $1e-4$.
	\item Batch size is set to $1024$.
	\item Number of epochs is set to $300$.
\end{itemize}

\paragraph{Evaluation metric.} For test, we iteratively use the trained model to predict all missing frames given the first 3 frames. The predicted frames will be used together with the given frames for the model to predict the next frame. We evaluate all models using MSE on test data, averaged over all predicted 29 frames for each sequence.


\begin{figure}[t]
\centering
\includegraphics[width=0.4\linewidth]{violin_poly_deg1.pdf}
\caption{Violin plots of the test MSE of the ReLU MLP and our model in extrapolating degree-$1$ polynomials.}
\label{appfig:extrapolation_res}
\end{figure}

\section{Additional Results and Discussion}
\label{appsec:results}


This section presents additional empirical results and discussion.



\subsection{Polynomial Extrapolation}
\label{appsec:extrapolation_res}

In the main text, we report extrapolation results on degree-$2$ and degree-$3$ polynomials in Figure~\ref{subfig:extrapolation_res}; for completeness, here we also report extrapolation results on degree-$1$ polynomials, \ie, linear functions. As shown by~\citep{xu_how_2021}, ReLU MLPs can also extrapolate well in this setting. The violin plots of the test MSE of both the ReLU MLP and our model are in Figure~\ref{appfig:extrapolation_res}. We can see that while both models achieve a much smaller extrapolation error compared to those in extrapolating higher degree polynomials, our model still outperforms the ReLU MLP. We also provide more examples for extrapolating degree-$1$, degree-$2$, and degree-$3$ polynomials in Figure~\ref{appfig:extrapolation_examples_deg1}, Figure~\ref{appfig:extrapolation_examples_deg2}, and Figure~\ref{appfig:extrapolation_examples_deg3}, respectively.

\paragraph{Comparison with~\citet{xu_how_2021}.} While~\citet{xu_how_2021} also show that 2-layer MLPs with quadratic activation functions can extrapolate quadratic functions better than ReLU MLPs, we emphasize that there are two key differences between our results and theirs:
\begin{enumerate}
	\item \citet{xu_how_2021} replaces \emph{all} ReLU functions with quadratic activation functions, while we only replace half of ReLU functions with quadratic activation functions and identity activation functions. This difference is important in scenarios where we do not know the exact structural form of target functions---note that our method can be viewed as an ``ensemble'' of different activation functions, which enables the neural network to adaptively select activation functions that are the most compatible with the task as shown by our empirical results.
	\item \citet{xu_how_2021} only considers 2-layer MLPs for learning quadratic functions, while we consider using 4-layer MLPs for learning degree-$2$ and degree-$3$ polynomials. This difference enables us to verify that neural networks can learn bases that require \emph{function compositions}. For example, degree-$3$ polynomials need a basis function $y = x^3$, which cannot be composed using a 2-layer MLP but is composable using MLPs with more than two layers and with quadratic and identity activation functions. We also note that the inclusion of identity functions is important since MLPs with only quadratic activations can only represent basis functions $y=x^k$ with even degrees $k$.
\end{enumerate}

\subsection{Learning Physical Laws}
\label{appsec:physics_res}

We present visualization results of the transformer baseline and our model in Figure~\ref{appfig:res_collision} in a test collision motion example. We can see that while both models yield accurate predictions before the collision, our model outperforms the baseline on the predictions of the object velocities after the collision.

\newpage


\begin{figure}[t]
\centering
\subcaptionbox{}{\includegraphics[width=0.3\linewidth]{poly_deg1_func_8.pdf}}
\hspace{0.5em}
\subcaptionbox{}{\includegraphics[width=0.3\linewidth]{poly_deg1_func_1.pdf}}
\hspace{0.5em}
\subcaptionbox{}{\includegraphics[width=0.3\linewidth]{poly_deg1_func_2.pdf}}
\caption{Selected examples for degree-$1$ polynomial extrapolation. Shaded regions indicate training regions.}
\label{appfig:extrapolation_examples_deg1}
\end{figure}


\begin{figure}[t]
\centering
\subcaptionbox{}{\includegraphics[width=0.3\linewidth]{poly_deg2_func_4.pdf}}
\hspace{0.5em}
\subcaptionbox{}{\includegraphics[width=0.3\linewidth]{poly_deg2_func_0.pdf}}
\hspace{0.5em}
\subcaptionbox{}{\includegraphics[width=0.3\linewidth]{poly_deg2_func_1.pdf}}
\caption{Selected examples for degree-$2$ polynomial extrapolation. Shaded regions indicate training regions.}
\label{appfig:extrapolation_examples_deg2}
\end{figure}


\begin{figure}[t]
\centering
\subcaptionbox{}{\includegraphics[width=0.3\linewidth]{poly_deg3_func_4.pdf}}
\hspace{0.5em}
\subcaptionbox{}{\includegraphics[width=0.3\linewidth]{poly_deg3_func_11.pdf}}
\hspace{0.5em}
\subcaptionbox{}{\includegraphics[width=0.3\linewidth]{poly_deg3_func_21.pdf}}
\caption{Selected examples for degree-$3$ polynomial extrapolation. Shaded regions indicate training regions.}
\label{appfig:extrapolation_examples_deg3}
\end{figure}

\begin{figure}[t]
\centering
\subcaptionbox{Ground truth.}{
\includegraphics[width=0.16\linewidth]{figures/collision_ood_gt_baseline_1.png}
\includegraphics[width=0.16\linewidth]{figures/collision_ood_gt_baseline_5.png}
\includegraphics[width=0.16\linewidth]{figures/collision_ood_gt_baseline_10.png}
\includegraphics[width=0.16\linewidth]{figures/collision_ood_gt_baseline_15.png}
\includegraphics[width=0.16\linewidth]{figures/collision_ood_gt_baseline_20.png}
\includegraphics[width=0.16\linewidth]{figures/collision_ood_gt_baseline_25.png}
}\\

\subcaptionbox{Prediction results of our model ($\mathrm{MSE} = 0.0039$).}{
\includegraphics[width=0.16\linewidth]{figures/collision_ood_ours_1.png}
\includegraphics[width=0.16\linewidth]{figures/collision_ood_ours_5.png}
\includegraphics[width=0.16\linewidth]{figures/collision_ood_ours_10.png}
\includegraphics[width=0.16\linewidth]{figures/collision_ood_ours_15.png}
\includegraphics[width=0.16\linewidth]{figures/collision_ood_ours_20.png}
\includegraphics[width=0.16\linewidth]{figures/collision_ood_ours_25.png}
}\\

\subcaptionbox{Prediction results of the transformer baseline ($\mathrm{MSE} = 0.5090$).}{
\includegraphics[width=0.16\linewidth]{figures/collision_ood_baseline_1.png}
\includegraphics[width=0.16\linewidth]{figures/collision_ood_baseline_5.png}
\includegraphics[width=0.16\linewidth]{figures/collision_ood_baseline_10.png}
\includegraphics[width=0.16\linewidth]{figures/collision_ood_baseline_15.png}
\includegraphics[width=0.16\linewidth]{figures/collision_ood_baseline_20.png}
\includegraphics[width=0.16\linewidth]{figures/collision_ood_baseline_25.png}
}\\
% \centering
\caption{Visualization results in a test collision motion example. All three rows select the same frames with uniform spacing.}
\label{appfig:res_collision}
\end{figure}

\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
