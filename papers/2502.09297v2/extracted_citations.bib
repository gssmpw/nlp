@inproceedings{abbe_generalization_2023,
	title = {Generalization on the unseen, logic reasoning and degree curriculum},
	abstract = {This paper considers the learning of logical (Boolean) functions with focus on the generalization on the unseen (GOTU) setting, a strong case of out-of-distribution generalization. This is motivated by the fact that the rich combinatorial nature of data in certain reasoning tasks (e.g., arithmetic/logic) makes representative data sampling challenging, and learning successfully under GOTU gives a first vignette of an ‘extrapolating’ or ‘reasoning’ learner. We then study how different network architectures trained by (S)GD perform under GOTU and provide both theoretical and experimental evidence that for a class of network models including instances of Transformers, random features models, and diagonal linear networks, a min-degree-interpolator is learned on the unseen. We also provide evidence that other instances with larger learning rates or mean-field networks reach leaky min-degree solutions. These findings lead to two implications: (1) we provide an explanation to the length generalization problem (e.g., Anil et al. 2022); (2) we introduce a curriculum learning algorithm called Degree-Curriculum that learns monomials more efficiently by incrementing supports.},
	language = {en},
	booktitle = {International {Conference} on {Machine} {Learning}},
	author = {Abbe, Emmanuel and Bengio, Samy and Lotfi, Aryo and Rizk, Kevin},
	year = {2023},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{ahuja_interventional_2023,
	title = {Interventional causal representation learning},
	abstract = {Causal representation learning seeks to extract high-level latent factors from low-level sensory data. Most existing methods rely on observational data and structural assumptions (e.g., conditional independence) to identify the latent factors. However, interventional data is prevalent across applications. Can interventional data facilitate causal representation learning? We explore this question in this paper. The key observation is that interventional data often carries geometric signatures of the latent factors’ support (i.e. what values each latent can possibly take). For example, when the latent factors are causally connected, interventions can break the dependency between the intervened latents’ support and their ancestors’. Leveraging this fact, we prove that the latent causal factors can be identified up to permutation and scaling given data from perfect do interventions. Moreover, we can achieve block affine identification, namely the estimated latent factors are only entangled with a few other latents if we have access to data from imperfect interventions. These results highlight the unique power of interventional data in causal representation learning; they can enable provable identification of latent factors without any assumptions about their distributions or dependency structure.},
	language = {en},
	booktitle = {International {Conference} on {Machine} {Learning}},
	author = {Ahuja, Kartik and Mahajan, Divyat and Wang, Yixin and Bengio, Yoshua},
	year = {2023},
}

@inproceedings{allen-zhu_towards_2023,
	title = {Towards understanding ensemble, knowledge distillation and self-distillation in deep learning},
	abstract = {We formally study how ensemble of deep learning models can improve test accuracy, and how the superior performance of ensemble can be distilled into a single model using knowledge distillation. We consider the challenging case where the ensemble is simply an average of the outputs of a few independently trained neural networks with the same architecture, trained using the same algorithm on the same data set, and they only diﬀer by the random seeds used in the initialization.},
	language = {en},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Allen-Zhu, Zeyuan and Li, Yuanzhi},
	year = {2023},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Neural and Evolutionary Computing, Mathematics - Optimization and Control},
}

@inproceedings{andriushchenko_sgd_2023,
	title = {{SGD} with large step sizes learns sparse features},
	isbn = {2640-3498},
	booktitle = {International {Conference} on {Machine} {Learning}},
	author = {Andriushchenko, Maksym and Varre, Aditya Vardhan and Pillaud-Vivien, Loucas and Flammarion, Nicolas},
	year = {2023},
	pages = {903--925},
}

@article{bartlett_benign_2020,
	title = {Benign overfitting in linear regression},
	volume = {117},
	number = {48},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Bartlett, Peter L. and Long, Philip M. and Lugosi, Gábor and Tsigler, Alexander},
	year = {2020},
	pages = {30063--30070},
}

@article{bartlett_rademacher_2002,
	title = {Rademacher and {Gaussian} complexities: {Risk} bounds and structural results},
	volume = {3},
	journal = {Journal of Machine Learning Research},
	author = {Bartlett, Peter L. and Mendelson, Shahar},
	year = {2002},
	pages = {463--482},
}

@inproceedings{bartlett_spectrally-normalized_2017,
	title = {Spectrally-normalized margin bounds for neural networks},
	language = {en},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Bartlett, Peter L and Foster, Dylan J and Telgarsky, Matus},
	year = {2017},
}

@inproceedings{bender_dangers_2021,
	address = {Virtual Event Canada},
	title = {On the dangers of stochastic parrots: {Can} language models be too big?},
	isbn = {978-1-4503-8309-7},
	shorttitle = {On the dangers of stochastic parrots},
	doi = {10.1145/3442188.3445922},
	abstract = {The past 3 years of work in NLP have been characterized by the development and deployment of ever larger language models, especially for English. BERT, its variants, GPT-2/3, and others, most recently Switch-C, have pushed the boundaries of the possible both through architectural innovations and through sheer size. Using these pretrained models and the methodology of fine-tuning them for specific tasks, researchers have extended the state of the art on a wide array of tasks as measured by leaderboards on specific benchmarks for English. In this paper, we take a step back and ask: How big is too big? What are the possible risks associated with this technology and what paths are available for mitigating those risks? We provide recommendations including weighing the environmental and financial costs first, investing resources into curating and carefully documenting datasets rather than ingesting everything on the web, carrying out pre-development exercises evaluating how the planned approach fits into research and development goals and supports stakeholder values, and encouraging research directions beyond ever larger language models.},
	language = {en},
	booktitle = {Proceedings of the 2021 {ACM} {Conference} on {Fairness}, {Accountability}, and {Transparency}},
	author = {Bender, Emily M. and Gebru, Timnit and McMillan-Major, Angelina and Shmitchell, Shmargaret},
	year = {2021},
	pages = {610--623},
}

@article{bengio_representation_2013,
	title = {Representation learning: {A} review and new perspectives},
	volume = {35},
	number = {8},
	journal = {IEEE transactions on pattern analysis and machine intelligence},
	author = {Bengio, Yoshua and Courville, Aaron and Vincent, Pascal},
	year = {2013},
	pages = {1798--1828},
}

@inproceedings{berglund_reversal_2024,
	title = {The reversal curse: {LLMs} trained on ``a is b" fail to learn ``b is a"},
	shorttitle = {The reversal curse},
	abstract = {We expose a surprising failure of generalization in auto-regressive large language models (LLMs). If a model is trained on a sentence of the form "A is B", it will not automatically generalize to the reverse direction "B is A". This is the Reversal Curse. For instance, if a model is trained on "Valentina Tereshkova was the first woman to travel to space", it will not automatically be able to answer the question, "Who was the first woman to travel to space?". Moreover, the likelihood of the correct answer ("Valentina Tershkova") will not be higher than for a random name. Thus, models do not generalize a prevalent pattern in their training set: if "A is B" occurs, "B is A" is more likely to occur. It is worth noting, however, that if "A is B" appears in-context, models can deduce the reverse relationship. We provide evidence for the Reversal Curse by finetuning GPT-3 and Llama-1 on fictitious statements such as "Uriah Hawthorne is the composer of Abyssal Melodies" and showing that they fail to correctly answer "Who composed Abyssal Melodies?". The Reversal Curse is robust across model sizes and model families and is not alleviated by data augmentation. We also evaluate ChatGPT (GPT-3.5 and GPT-4) on questions about real-world celebrities, such as "Who is Tom Cruise's mother? [A: Mary Lee Pfeiffer]" and the reverse "Who is Mary Lee Pfeiffer's son?". GPT-4 correctly answers questions like the former 79\% of the time, compared to 33\% for the latter. Code available at: https://github.com/lukasberglund/reversal\_curse.},
	language = {en},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Berglund, Lukas and Tong, Meg and Kaufmann, Max and Balesni, Mikita and Stickland, Asa Cooper and Korbak, Tomasz and Evans, Owain},
	year = {2024},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@article{bhattamishra_simplicity_2023,
	title = {Simplicity bias in transformers and their ability to learn sparse boolean functions},
	abstract = {Despite the widespread success of Transformers on NLP tasks, recent works have found that they struggle to model several formal languages when compared to recurrent models. This raises the question of why Transformers perform well in practice and whether they have any properties that enable them to generalize better than recurrent models. In this work, we conduct an extensive empirical study on Boolean functions to demonstrate the following: (i) Random Transformers are relatively more biased towards functions of low sensitivity. (ii) When trained on Boolean functions, both Transformers and LSTMs prioritize learning functions of low sensitivity, with Transformers ultimately converging to functions of lower sensitivity. (iii) On sparse Boolean functions which have low sensitivity, we find that Transformers generalize near perfectly even in the presence of noisy labels whereas LSTMs overfit and achieve poor generalization accuracy. Overall, our results provide strong quantifiable evidence that suggests differences in the inductive biases of Transformers and recurrent models which may help explain Transformer’s effective generalization performance despite relatively limited expressiveness.},
	language = {en},
	journal = {arXiv preprint arXiv:2211.12316},
	author = {Bhattamishra, Satwik and Patel, Arkil and Kanade, Varun and Blunsom, Phil},
	year = {2023},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
}

@article{bricken2023towards,
  title={Towards monosemanticity: Decomposing language models with dictionary learning},
  author={Bricken, Trenton and Templeton, Adly and Batson, Joshua and Chen, Brian and Jermyn, Adam and Conerly, Tom and Turner, Nick and Anil, Cem and Denison, Carson and Askell, Amanda and others},
  journal={Transformer Circuits Thread},
  volume={2},
  year={2023}
}

@article{chatterjee_neural_2024,
	title = {Neural networks generalize on low complexity data},
	urldate = {2024-12-29},
	journal = {arXiv preprint arXiv:2409.12446},
	author = {Chatterjee, Sourav and Sudijono, Timothy},
	year = {2024}
}

@inproceedings{chizat_implicit_2020,
	title = {Implicit bias of gradient descent for wide two-layer neural networks trained with the logistic loss},
	abstract = {Neural networks trained to minimize the logistic (a.k.a. cross-entropy) loss with gradient-based methods are observed to perform well in many supervised classiﬁcation tasks. Towards understanding this phenomenon, we analyze the training and generalization behavior of inﬁnitely wide two-layer neural networks with homogeneous activations. We show that the limits of the gradient ﬂow on exponentially tailed losses can be fully characterized as a max-margin classiﬁer in a certain nonHilbertian space of functions. In presence of hidden low-dimensional structures, the resulting margin is independent of the ambiant dimension, which leads to strong generalization bounds. In contrast, training only the output layer implicitly solves a kernel support vector machine, which a priori does not enjoy such an adaptivity. Our analysis of training is non-quantitative in terms of running time but we prove computational guarantees in simpliﬁed settings by showing equivalences with online mirror descent. Finally, numerical experiments suggest that our analysis describes well the practical behavior of two-layer neural networks with ReLU activations and conﬁrm the statistical beneﬁts of this implicit bias.},
	language = {en},
	booktitle = {Conference on {Learning} {Theory}},
	author = {Chizat, Lenaıc and Chizat, Lenaic and Fr, Universite-Paris-Saclay},
	year = {2020},
}

@book{craik1967nature,
  title={The nature of explanation},
  author={Craik, Kenneth James Williams},
  volume={445},
  year={1967},
  publisher={CUP Archive}
}

@book{everett2013introduction,
  title={An introduction to latent variable models},
  author={Everett, B},
  year={2013},
}

@article{friston2021world,
  title={World model learning and inference},
  author={Friston, Karl and Moran, Rosalyn J and Nagai, Yukie and Taniguchi, Tadahiro and Gomi, Hiroaki and Tenenbaum, Josh},
  journal={Neural Networks},
  volume={144},
  pages={573--590},
  year={2021},
  publisher={Elsevier}
}

@inproceedings{gunasekar_characterizing_2018,
	title = {Characterizing implicit bias in terms of optimization geometry},
	abstract = {We study the implicit bias of generic optimization methods—mirror descent, natural gradient descent, and steepest descent with respect to different potentials and norms—when optimizing underdetermined linear regression or separable linear classiﬁcation problems. We explore the question of whether the speciﬁc global minimum (among the many possible global minima) reached by an algorithm can be characterized in terms of the potential or norm of the optimization geometry, and independently of hyperparameter choices such as step–size and momentum.},
	language = {en},
	booktitle = {International {Conference} on {Machine} {Learning}},
	author = {Gunasekar, Suriya and Lee, Jason and Soudry, Daniel and Srebro, Nathan},
	year = {2018},
	pages = {1827--1836},
}

@inproceedings{gunasekar_implicit_2018,
	title = {Implicit bias of gradient descent on linear convolutional networks},
	abstract = {We show that gradient descent on full width linear convolutional networks of depth L converges to a linear predictor related to the 2/L bridge penalty in the frequency domain. This is in contrast to fully connected linear networks, where regardless of depth, gradient descent converges to the 2 maximum margin solution.},
	language = {en},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Gunasekar, Suriya and Lee, Jason D and Soudry, Daniel and Srebro, Nati},
	year = {2018},
	pages = {9482--9491},
}

@inproceedings{gurnee_language_2024,
	title = {Language models represent space and time},
	abstract = {The capabilities of large language models (LLMs) have sparked debate over whether such systems just learn an enormous collection of superficial statistics or a coherent model of the data generating process—a world model. We find evidence for the latter by analyzing the learned representations of three spatial datasets (world, US, NYC places) and three temporal datasets (historical figures, artworks, news headlines) in the Llama-2 family of models. We discover that LLMs learn linear representations of space and time across multiple scales. These representations are robust to prompting variations and unified across different entity types (e.g. cities and landmarks). In addition, we identify individual “space neurons” and “time neurons” that reliably encode spatial and temporal coordinates. Our analysis demonstrates that modern LLMs acquire structured knowledge about fundamental dimensions such as space and time, supporting the view that they learn not merely superficial statistics, but literal world models.},
	language = {en},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Gurnee, Wes and Tegmark, Max},
	year = {2024},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@article{ha_world_2018,
	title = {World models},
	journal = {arXiv preprint arXiv:1803.10122},
	author = {Ha, David and Schmidhuber, Jürgen},
	year = {2018},
}

@article{huh_low-rank_2023,
	title = {The low-rank simplicity bias in deep networks},
	abstract = {Modern deep neural networks are highly over-parameterized compared to the data on which they are trained, yet they often generalize remarkably well. A ﬂurry of recent work has asked: why do deep networks not overﬁt to their training data? In this work, we make a series of empirical observations that investigate and extend the hypothesis that deeper networks are inductively biased to ﬁnd solutions with lower eﬀective rank embeddings. We conjecture that this bias exists because the volume of functions that maps to low eﬀective rank embedding increases with depth. We show empirically that our claim holds true on ﬁnite width linear and non-linear models on practical learning paradigms and show that on natural data, these are often the solutions that generalize well. We then show that the simplicity bias exists at both initialization and after training and is resilient to hyper-parameters and learning methods. We further demonstrate how linear over-parameterization of deep non-linear models can be used to induce low-rank bias, improving generalization performance on CIFAR and ImageNet without changing the modeling capacity.},
	language = {en},
	journal = {Transactions on Machine Learning Research},
	author = {Huh, Minyoung and Mobahi, Hossein and Zhang, Richard and Cheung, Brian and Agrawal, Pulkit and Isola, Phillip},
	year = {2023},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
}

@article{hyvarinen_nonlinear_1999,
	title = {Nonlinear independent component analysis: {Existence} and uniqueness results},
	volume = {12},
	shorttitle = {Nonlinear independent component analysis},
	number = {3},
	journal = {Neural Networks},
	author = {Hyvärinen, Aapo and Pajunen, Petteri},
	year = {1999},
	pages = {429--439},
}

@inproceedings{hyvarinen_nonlinear_2019,
	title = {Nonlinear {ICA} using auxiliary variables and generalized contrastive learning},
	booktitle = {{AISTATS}},
	author = {Hyvarinen, Aapo and Sasaki, Hiroaki and Turner, Richard E},
	year = {2019},
}

@inproceedings{jacot_neural_2018,
	title = {Neural tangent kernel: {Convergence} and generalization in neural networks},
	shorttitle = {Neural tangent kernel},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Jacot, Arthur and Gabriel, Franck and Hongler, Clément},
	year = {2018},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Neural and Evolutionary Computing, Mathematics - Probability},
}

@inproceedings{jin_emergent_2024,
	title = {Emergent representations of program semantics in language models trained on programs},
	booktitle = {International {Conference} on {Machine} {Learning}},
	author = {Jin, Charles and Rinard, Martin},
	year = {2024},
}

@inproceedings{kalimeris_sgd_2019,
	title = {{SGD} on neural networks learns functions of increasing complexity},
	abstract = {We perform an experimental study of the dynamics of Stochastic Gradient Descent (SGD) in learning deep neural networks for several real and synthetic classiﬁcation tasks. We show that in the initial epochs, almost all of the performance improvement of the classiﬁer obtained by SGD can be explained by a linear classiﬁer. More generally, we give evidence for the hypothesis that, as iterations progress, SGD learns functions of increasing complexity. This hypothesis can be helpful in explaining why SGD-learned classiﬁers tend to generalize well even in the overparameterized regime. We also show that the linear classiﬁer learned in the initial stages is “retained” throughout the execution even if training is continued to the point of zero training error, and complement this with a theoretical result in a simpliﬁed model. Key to our work is a new measure of how well one classiﬁer explains the performance of another, based on conditional mutual information.},
	language = {en},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Kalimeris, Dimitris and Kaplun, Gal and Nakkiran, Preetum and Edelman, Benjamin and Yang, Tristan and Barak, Boaz and Zhang, Haofeng},
	year = {2019},
	pages = {3491--3501},
}

@inproceedings{khemakhem_variational_2020,
	title = {Variational autoencoders and nonlinear {ICA}: {A} unifying framework},
	language = {en},
	booktitle = {{AISTATS}},
	author = {Khemakhem, Ilyes and Kingma, Diederik P and Monti, Ricardo Pio and Hyvärinen, Aapo},
	year = {2020},
}

@article{lecun2022path,
  title={A path towards autonomous machine intelligence version 0.9. 2, 2022-06-27},
  author={LeCun, Yann},
  journal={Open Review},
  volume={62},
  number={1},
  pages={1--62},
  year={2022}
}

@inproceedings{lee_predicting_2021,
	title = {Predicting what you already know helps: {Provable} self-supervised learning},
	volume = {34},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Lee, Jason D. and Lei, Qi and Saunshi, Nikunj and Zhuo, Jiacheng},
	year = {2021},
	pages = {309--323},
}

@book{li2008introduction,
  title={An introduction to Kolmogorov complexity and its applications},
  author={Li, Ming and Vit{\'a}nyi, Paul and others},
  volume={3},
  year={2008},
}

@article{li2021implicit,
  title={Implicit representations of meaning in neural language models},
  author={Li, Belinda Z and Nye, Maxwell and Andreas, Jacob},
  journal={arXiv preprint arXiv:2106.00737},
  year={2021}
}

@inproceedings{li_emergent_2023,
	title = {Emergent world representations: {Exploring} a sequence model trained on a synthetic task},
	language = {en},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Li, Kenneth and Hopkins, Aspen K and Bau, David},
	year = {2023},
}

@article{liu_grokking_2023,
	title = {Grokking as compression: {A} nonlinear complexity perspective},
	shorttitle = {Grokking as compression},
	abstract = {We attribute grokking, the phenomenon where generalization is much delayed after memorization, to compression. We define linear mapping number (LMN) to measure network complexity, which is a generalized version of linear region number for ReLU networks. LMN can nicely characterize neural network compression before generalization. Although L2 norm has been popular to characterize model complexity, we argue in favor of LMN for a number of reasons: (1) LMN can be naturally interpreted as information/computation, while L2 cannot. (2) In the compression phase, LMN has nice linear relations with test losses, while L2 is correlated with test losses in a complicated nonlinear way. (3) LMN also reveals an intriguing phenomenon of the XOR network switching between two generalization solutions, while L2 does not. Besides explaning grokking, we argue that LMN is a promising candidate as the neural network version of the Kolmogorov complexity, since it explicitly considers local or conditioned linear computations aligned with the nature of modern artificial neural networks.},
	language = {en},
	journal = {arXiv preprint arXiv:2310.05918},
	author = {Liu, Ziming and Zhong, Ziqian and Tegmark, Max},
	year = {2023},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
}

@inproceedings{lotfi_pac-bayes_2022,
	title = {{PAC}-{Bayes} compression bounds so tight that they can explain generalization},
	abstract = {While there has been progress in developing non-vacuous generalization bounds for deep neural networks, these bounds tend to be uninformative about why deep learning works. In this paper, we develop a compression approach based on quantizing neural network parameters in a linear subspace, profoundly improving on previous results to provide state-of-the-art generalization bounds on a variety of tasks, including transfer learning. We use these tight bounds to better understand the role of model size, equivariance, and the implicit biases of optimization, for generalization in deep learning. Notably, we find large models can be compressed to a much greater extent than previously known, encapsulating Occam’s razor. We also argue for data-independent bounds in explaining generalization.},
	language = {en},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Lotfi, Sanae and Finzi, Marc and Kapoor, Sanyam and Potapczynski, Andres and Goldblum, Micah and Wilson, Andrew Gordon},
	year = {2022},
}

@inproceedings{lyu_gradient_2021,
	title = {Gradient descent on two-layer nets: {Margin} maximization and simplicity bias},
	abstract = {The generalization mystery of overparametrized deep nets has motivated efforts to understand how gradient descent (GD) converges to low-loss solutions that generalize well. Real-life neural networks are initialized from small random values and trained with cross-entropy loss for classiﬁcation (unlike the "lazy" or "NTK" regime of training where analysis was more successful), and a recent sequence of results (Lyu and Li, 2020; Chizat and Bach, 2020; Ji and Telgarsky, 2020a) provide theoretical evidence that GD may converge to the "max-margin" solution with zero loss, which presumably generalizes well. However, the global optimality of margin is proved only in some settings where neural nets are inﬁnitely or exponentially wide. The current paper is able to establish this global optimality for two-layer Leaky ReLU nets trained with gradient ﬂow on linearly separable and symmetric data, regardless of the width. The analysis also gives some theoretical justiﬁcation for recent empirical ﬁndings (Kalimeris et al., 2019) on the so-called simplicity bias of GD towards linear or other "simple" classes of solutions, especially early in training. On the pessimistic side, the paper suggests that such results are fragile. A simple data manipulation can make gradient ﬂow converge to a linear classiﬁer with suboptimal margin.},
	language = {en},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Lyu, Kaifeng and Wang, Runzhe and Li, Zhiyuan and Arora, Sanjeev},
	year = {2021},
	pages = {12978--12991},
}

@article{marks_geometry_2023,
	title = {The geometry of truth: {Emergent} linear structure in large language model representations of true/false datasets},
	shorttitle = {The geometry of truth},
	abstract = {Large Language Models (LLMs) have impressive capabilities, but are also prone to outputting falsehoods. Recent work has developed techniques for inferring whether a LLM is telling the truth by training probes on the LLM’s internal activations. However, this line of work is controversial, with some authors pointing out failures of these probes to generalize in basic ways, among other conceptual issues. In this work, we curate high-quality datasets of true/false statements and use them to study in detail the structure of LLM representations of truth, drawing on three lines of evidence: 1. Visualizations of LLM true/false statement representations, which reveal clear linear structure. 2. Transfer experiments in which probes trained on one dataset generalize to different datasets. 3. Causal evidence obtained by surgically intervening in a LLM’s forward pass, causing it to treat false statements as true and vice versa. Overall, we present evidence that language models linearly represent the truth or falsehood of factual statements. We also introduce a novel technique, mass-mean probing, which generalizes better and is more causally implicated in model outputs than other probing techniques.},
	language = {en},
	journal = {arXiv preprint arXiv:2310.06824},
	author = {Marks, Samuel and Tegmark, Max},
	year = {2023},
	keywords = {Computer Science - Artificial Intelligence},
}

@article{mirzadeh_gsm-symbolic_2024,
	title = {{GSM}-symbolic: {Understanding} the limitations of mathematical reasoning in large language models},
	shorttitle = {Gsm-symbolic},
	abstract = {Recent advancements in Large Language Models (LLMs) have sparked interest in their formal reasoning capabilities, particularly in mathematics. The GSM8K benchmark is widely used to assess the mathematical reasoning of models on grade-school-level questions. While the performance of LLMs on GSM8K has significantly improved in recent years, it remains unclear whether their mathematical reasoning capabilities have genuinely advanced, raising questions about the reliability of the reported metrics. To address these concerns, we conduct a largescale study on several state-of-the-art open and closed models. To overcome the limitations of existing evaluations, we introduce GSM-Symbolic, an improved benchmark created from symbolic templates that allow for the generation of a diverse set of questions. GSM-Symbolic enables more controllable evaluations, providing key insights and more reliable metrics for measuring the reasoning capabilities of models.Our findings reveal that LLMs exhibit noticeable variance when responding to different instantiations of the same question. Specifically, the performance of all models declines when only the numerical values in the question are altered in the GSM-Symbolic benchmark. Furthermore, we investigate the fragility of mathematical reasoning in these models and demonstrate that their performance significantly deteriorates as the number of clauses in a question increases. We hypothesize that this decline is due to the fact that current LLMs are not capable of genuine logical reasoning; instead, they attempt to replicate the reasoning steps observed in their training data. When we add a single clause that appears relevant to the question, we observe significant performance drops (up to 65\%) across all state-of-the-art models, even though the added clause does not contribute to the reasoning chain needed to reach the final answer. Overall, our work provides a more nuanced understanding of LLMs’ capabilities and limitations in mathematical reasoning.},
	language = {en},
	journal = {arXiv preprint arXiv:2410.05229},
	author = {Mirzadeh, Iman and Alizadeh, Keivan and Shahrokhi, Hooman and Tuzel, Oncel and Bengio, Samy and Farajtabar, Mehrdad},
	year = {2024},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
}

@article{mitchell_ai_2023,
author = {Melanie Mitchell },
title = {AI’s challenge of understanding the world},
journal = {Science},
volume = {382},
number = {6671},
pages = {eadm8175},
year = {2023},
doi = {10.1126/science.adm8175},
}

@article{nanda_emergent_2023,
	title = {Emergent linear representations in world models of self-supervised sequence models},
	abstract = {How do sequence models represent their decision-making process? Prior work suggests that Othello-playing neural network learned nonlinear models of the board state (Li et al., 2023). In this work, we provide evidence of a closely related linear representation of the board. In particular, we show that probing for "my colour" vs. "opponent's colour" may be a simple yet powerful way to interpret the model's internal state. This precise understanding of the internal representations allows us to control the model's behaviour with simple vector arithmetic. Linear representations enable significant interpretability progress, which we demonstrate with further exploration of how the world model is computed.},
	language = {en},
	journal = {arXiv preprint arXiv:2309.00941},
	author = {Nanda, Neel and Lee, Andrew and Wattenberg, Martin},
	year = {2023},
	keywords = {Computer Science - Machine Learning},
}

@inproceedings{perez_deep_2019,
	title = {Deep learning generalizes because the parameter-function map is biased towards simple functions},
	booktitle = {{ICLR}},
	author = {Pérez, Guillermo Valle and Louis, Ard A and Camargo, Chico Q},
	year = {2019},
}

@inproceedings{richensrobust,
  title={Robust agents learn causal world models},
  author={Richens, Jonathan and Everitt, Tom},
  booktitle={ICLR},
  year= {2024}
}

@article{soudry_implicit_2018,
	title = {The implicit bias of gradient descent on separable data},
	volume = {19},
	number = {1},
	journal = {The Journal of Machine Learning Research},
	author = {Soudry, Daniel and Hoffer, Elad and Nacson, Mor Shpigel and Gunasekar, Suriya and Srebro, Nathan},
	year = {2018},
	pages = {2822--2878},
}

@inproceedings{tosh_contrastive_2021,
	title = {Contrastive learning, multi-view redundancy, and linear models},
	isbn = {2640-3498},
	booktitle = {Algorithmic {Learning} {Theory}},
	author = {Tosh, Christopher and Krishnamurthy, Akshay and Hsu, Daniel},
	year = {2021},
	pages = {1179--1206},
}

@book{vapnik_nature_1999,
	title = {The nature of statistical learning theory},
	isbn = {0-387-98780-0},
	author = {Vapnik, Vladimir},
	year = {1999},
}

@inproceedings{von_kugelgen_nonparametric_2023,
	title = {Nonparametric identifiability of causal representations from unknown interventions},
	volume = {36},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {von Kügelgen, Julius and Besserve, Michel and Wendong, Liang and Gresele, Luigi and Kekić, Armin and Bareinboim, Elias and Blei, David and Schölkopf, Bernhard},
	year = {2023},
}

@inproceedings{von_kugelgen_self-supervised_2021,
	title = {Self-supervised learning with data augmentations provably isolates content from style},
	volume = {34},
	booktitle = {Advances in neural information processing systems},
	author = {Von Kügelgen, Julius and Sharma, Yash and Gresele, Luigi and Brendel, Wieland and Schölkopf, Bernhard and Besserve, Michel and Locatello, Francesco},
	year = {2021},
	pages = {16451--16467},
}

@inproceedings{wei_why_2021,
	title = {Why do pretrained language models help in downstream tasks? {An} analysis of head and prompt tuning},
	volume = {34},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Wei, Colin and Xie, Sang Michael and Ma, Tengyu},
	year = {2021},
	pages = {16158--16170},
}

@article{wong_word_2023,
	title = {From word models to world models: {Translating} from natural language to the probabilistic language of thought},
	journal = {arXiv preprint arXiv:2306.12672},
	author = {Wong, Lionel and Grand, Gabriel and Lew, Alexander K. and Goodman, Noah D. and Mansinghka, Vikash K. and Andreas, Jacob and Tenenbaum, Joshua B.},
	year = {2023}
}

@article{wu_reasoning_2023,
	title = {Reasoning or reciting? {Exploring} the capabilities and limitations of language models through counterfactual tasks},
	shorttitle = {Reasoning or reciting?},
	abstract = {The impressive performance of recent language models across a wide range of tasks suggests that they possess a degree of abstract reasoning skills. Are these skills general and transferable, or specialized to specific tasks seen during pretraining? To disentangle these effects, we propose an evaluation framework based on "counterfactual" task variants that deviate from the default assumptions underlying standard tasks. Across a suite of 11 tasks, we observe nontrivial performance on the counterfactual variants, but nevertheless find that performance substantially and consistently degrades compared to the default conditions. This suggests that while current LMs may possess abstract task-solving skills to an extent, they often also rely on narrow, non-transferable procedures for task-solving. These results motivate a more careful interpretation of language model performance that teases apart these aspects of behavior.},
	language = {en},
	journal = {arXiv preprint arXiv:2307.02477},
	author = {Wu, Zhaofeng and Qiu, Linlu and Ross, Alexis and Akyürek, Ekin and Chen, Boyuan and Wang, Bailin and Kim, Najoung and Andreas, Jacob and Kim, Yoon},
	year = {2023},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@article{xie_making_2024,
	title = {Making large language models into world models with precondition and effect knowledge},
	language = {en},
	journal = {arXiv preprint arXiv:2409.12278},
	author = {Xie, Kaige and Yang, Ian and Gunerli, John and Riedl, Mark},
	year = {2024},

}

@article{xu2019frequency,
  title={Frequency principle: Fourier analysis sheds light on deep neural networks},
  author={Xu, Zhi-Qin John and Zhang, Yaoyu and Luo, Tao and Xiao, Yanyang and Ma, Zheng},
  journal={arXiv preprint arXiv:1901.06523},
  year={2019}
}

@inproceedings{xu_what_2020,
	title = {What can neural networks reason about?},
	language = {en},
	booktitle = {{ICLR}},
	author = {Xu, Keyulu and Li, Jingling and Zhang, Mozhi and Du, Simon S. and Kawarabayashi, Ken-ichi and Jegelka, Stefanie},
	year = {2020},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Neural and Evolutionary Computing},
}

@inproceedings{zhang_feature_2024,
	title = {Feature contamination: {Neural} networks learn uncorrelated features and fail to generalize},
	shorttitle = {Feature contamination},
	abstract = {Learning representations that generalize under distribution shifts is critical for building robust machine learning models. However, despite significant efforts in recent years, algorithmic advances in this direction have been limited. In this work, we seek to understand the fundamental difficulty of out-of-distribution generalization with deep neural networks. We first empirically show that perhaps surprisingly, even allowing a neural network to explicitly fit the representations obtained from a teacher network that can generalize out-of-distribution is insufficient for the generalization of the student network. Then, by a theoretical study of two-layer ReLU networks optimized by stochastic gradient descent (SGD) under a structured feature model, we identify a fundamental yet unexplored feature learning proclivity of neural networks, feature contamination: neural networks can learn uncorrelated features together with predictive features, resulting in generalization failure under distribution shifts. Notably, this mechanism essentially differs from the prevailing narrative in the literature that attributes the generalization failure to spurious correlations. Overall, our results offer new insights into the non-linear feature learning dynamics of neural networks and highlight the necessity of considering inductive biases in out-of-distribution generalization.},
	language = {en},
	booktitle = {International {Conference} on {Machine} {Learning}},
	author = {Zhang, Tianren and Zhao, Chujie and Chen, Guanyu and Jiang, Yizhou and Chen, Feng},
	year = {2024},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
}

@inproceedings{zhang_understanding_2017,
	title = {Understanding deep learning requires rethinking generalization},
	booktitle = {{ICLR}},
	author = {Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
	year = {2017},
	keywords = {Computer Science - Machine Learning},
}

@article{zhao_m3pl_2024,
	title = {M\${\textasciicircum}3\${PL}: {Identifying} and exploiting view bias of prompt learning},
	issn = {2835-8856},
	shorttitle = {M\${\textasciicircum}3\$pl},
	abstract = {Prompt learning is an effective means of fine-tuning multi-modal foundation models such as CLIP. Despite existing success, the inner mechanism of multi-modal prompt learning has not been well understood. In this work, we identify an inductive bias of multi-modal prompt learning, which we refer to as view bias, that the learned prompts may extract only a partial subset of useful features (views) and ignore others. This bias can undermine the model's generalization ability, particularly under distribution shifts. We further observe that independently trained prompts have distinct view biases, contrary to the existing belief that they may converge to similar local optima due to having the same cross-modal representation matching objective. Based on our observations, we propose Multi-modal Matching Multi-Prompt Learning (M\${\textasciicircum}3\$PL), which incorporates multiple paired prompts and a cross-modal contrastive regularizer that facilitates the prompt pairs to encapsulate a broader spectrum of views. Extensive experiments show that M\${\textasciicircum}3\$PL effectively boosts the model's generalization capability, achieving state-of-the-art performance under various distribution shifts.},
	language = {en},
	journal = {Transactions on Machine Learning Research},
	author = {Zhao, Chujie and Zhang, Tianren and Chen, Guanyu and Jiang, Yizhou and Chen, Feng},
	year = {2024},
}

@inproceedings{zhou_non-vacuous_2019,
	title = {Non-vacuous generalization bounds at the {ImageNet} scale: {A} {PAC}-{Bayesian} compression approach},
	shorttitle = {Non-vacuous generalization bounds at the imagenet scale},
	abstract = {Modern neural networks are highly overparameterized, with capacity to substantially overﬁt to training data. Nevertheless, these networks often generalize well in practice. It has also been observed that trained networks can often be “compressed” to much smaller representations. The purpose of this paper is to connect these two empirical observations. Our main technical result is a generalization bound for compressed networks based on the compressed size that, combined with off-theshelf compression algorithms, leads to state-of-the-art generalization guarantees. In particular, we provide the ﬁrst non-vacuous generalization guarantees for realistic architectures applied to the ImageNet classiﬁcation problem. Additionally, we show that compressibility of models that tend to overﬁt is limited. Empirical results show that an increase in overﬁtting increases the number of bits required to describe a trained network.},
	language = {en},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Zhou, Wenda and Veitch, Victor and Austern, Morgane and Adams, Ryan P. and Orbanz, Peter},
	year = {2019},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{zimmermann_contrastive_2021,
	title = {Contrastive learning inverts the data generating process},
	language = {en},
	booktitle = {International {Conference} on {Machine} {Learning}},
	author = {Zimmermann, Roland S and Sharma, Yash and Schneider, Steffen and Bethge, Matthias and Brendel, Wieland},
	year = {2021},
	pages = {12979--12990},
}

