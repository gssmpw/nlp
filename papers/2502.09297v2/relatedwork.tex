\section{Related Work}
\label{appsec:related_work}

\paragraph{World models.} 
The term “world model” in the machine learning context originates from~\citet{ha_world_2018}, who describe it as a human-like “mental model of the world” that learns an abstract representation of information flow and can be used to predict future events. This definition closely aligns with the concept of a “mental model” in cognitive science, \ie, an internal representation of external reality~\citep{craik1967nature}, making it naturally connected to the field of \emph{representation learning} in machine learning~\citep{bengio_representation_2013}.

Recently, the remarkable capabilities of large language models (LLMs) have sparked a scientific debate on whether these models merely exploit superficial statistical patterns to generate predictions without genuine “understanding” of natural language~\citep{bender_dangers_2021,mitchell_ai_2023}, or whether they develop models that serve as compact and interpretable representations of the underlying data generation process. A series of studies have demonstrated the presence of internal representations in language models trained on synthetic tasks~\citep{li_emergent_2023,nanda_emergent_2023,jin_emergent_2024}. For real-world LLMs, research in mechanistic interpretability suggests that these models learn compact, interpretable, and causal features within their intermediate layers~\citep{li2021implicit,bricken2023towards,marks_geometry_2023,gurnee_language_2024}. At the same time, many studies report a significant decline in LLM performance on tasks that are assumed to be underrepresented in their pre-training distribution~\citep{wu_reasoning_2023,berglund_reversal_2024,mirzadeh_gsm-symbolic_2024}.

Beyond sequence models, world models have also gained attention in reinforcement learning~\citep{ha_world_2018,lecun2022path,xie_making_2024}, probabilistic learning~\citep{friston2021world,wong_word_2023}, and causal discovery~\citep{richensrobust}. However, despite the growing number of empirical studies, the theoretical foundations of world model learning remain largely unexplored.

% The usage of the term ``world model'' in the machine learning context originates from~\citet{ha_world_2018}---they refer to it as a human-like, ``mental model of the world'' that learns an abstract representation of the information flow and that can be used to predict future events. This definition is closely related to the notion of ``mental model'' in cognitive science, \ie, an internal representation of external reality~\citep{craik1967nature}, which makes it naturally connected to the topic of \emph{representation learning} in machine learning~\citep{bengio_representation_2013}. Recently, the remarkable capabilities of LLMs have sparked a scientific debate on whether LLMs are simply exploiting superficial statistics to make predictions without ``understanding'' natural language~\citep{bender_dangers_2021,mitchell_ai_2023}, or they do learn models that serve as compact and interpretable representations of the underlying data generation process. A series of work demonstrates the existence of internal representations in language models when trained on synthetic tasks~\citep{li_emergent_2023,nanda_emergent_2023,jin_emergent_2024}. For real LLMs, there are studies in mechanistic interpretability showing that LLMs learn compact, interpretable, and causal features in their intermediate layers~\citep{li2021implicit,bricken2023towards,marks_geometry_2023,gurnee_language_2024}. Meanwhile, many works report a marked decline in LLMs' performance in the tasks assumed to be underrepresented in their pre-training distribution~\citep{wu_reasoning_2023,berglund_reversal_2024,mirzadeh_gsm-symbolic_2024}. Besides sequence models, world models also attract research interests in reinforcement learning~\citep{ha_world_2018,lecun2022path,xie_making_2024}, probablistic learning~\citep{friston2021world,wong_word_2023}, and causal discovery~\citep{richensrobust}. Nevertheless, despite a recent increase of related empirical studies, theoretical foundations of learning world models are still unclear.

\paragraph{Latent variable recovery.} Our definition of world models falls within a broad class of latent variable recovery problems~\citep{everett2013introduction}, where observable data is generated by latent variables through an unknown generation function. It is well established that, without additional assumptions, recovering the true latent variables from observed data is generally impossible if the generation function is nonlinear~\citep{hyvarinen_nonlinear_1999,khemakhem_variational_2020}, a fundamental result in non-linear independent component analysis (non-linear ICA).

To address this impossibility, subsequent research has explored various structural assumptions on latent variables, such as conditional independence between the latent variables and an observable auxiliary variable~\citep{hyvarinen_nonlinear_2019,khemakhem_variational_2020,lee_predicting_2021}, distributional constraints~\citep{zimmermann_contrastive_2021,wei_why_2021}, and causal interventions~\citep{von_kugelgen_self-supervised_2021,ahuja_interventional_2023,von_kugelgen_nonparametric_2023}. Some studies have also linked these assumptions to contrastive learning~\citep{hyvarinen_nonlinear_2019,tosh_contrastive_2021,zimmermann_contrastive_2021}. However, incorporating such structural assumptions often leads to complex and less scalable training paradigms compared to the pre-training framework of modern LLMs (\ie, next-token prediction). As a result, these studies do not directly address the central question of our work: \emph{Can the ongoing paradigm of LLMs learn world models?} Meanwhile, the fact that LLMs already acquire non-trivial representations~\citep{li2021implicit,bricken2023towards,marks_geometry_2023,gurnee_language_2024} suggests that they must leverage some form of \emph{implicit bias} rather than explicit structural assumptions on input data, which motivates our study.

% Our definition of world models is subsumed by a general class of latent variable recovery problems~\citep{everett2013introduction}, in which the observable data is generated by latent variables via an unknown generation function. Without further assumptions, it has been known that recovering the true latent variables from the observed data is not possible if the generation function is non-linear~\citep{hyvarinen_nonlinear_1999,khemakhem_variational_2020}, a classical result in non-linear independent component analysis (non-linear ICA). To overcome the impossibility, follow-up work explores adding more structural assumptions on latent variables, \eg, conditional independence between the latent variables and an observable auxiliary variable~\citep{hyvarinen_nonlinear_2019,khemakhem_variational_2020,lee_predicting_2021}, distributional assumptions on latent variables~\citep{zimmermann_contrastive_2021,wei_why_2021}, and causal interventions~\citep{von_kugelgen_self-supervised_2021,ahuja_interventional_2023,von_kugelgen_nonparametric_2023}. Some studies have also drawn connections between these assumptions and contrastive learning~\citep{hyvarinen_nonlinear_2019,tosh_contrastive_2021,zimmermann_contrastive_2021}. However, a consequence of incorporating these structural assumptions is different and often more complex (hence less scalable) training paradigms compared to the pre-training paradigm of today's LLMs (\ie, next-token prediction). Therefore, those studies cannot answer the important question we care about: \emph{can the ongoing paradigm of LLMs learn world models?} On the other hand, the fact that many LLMs already learn some non-trivial representations indicates that they must leverage some \emph{implicit bias} rather than explicit structural assumptions, which motivates this work.


\paragraph{Implicit bias of neural networks.}
Overparameterized neural networks have been shown to possess the capacity to memorize entire training datasets~\citep{zhang_understanding_2017}. However, their ability to generalize well in many settings suggests that they exhibit implicit preferences for certain solutions—commonly referred to as \emph{implicit bias}. In simple models, such as linear models, random feature models, and two-layer neural networks, a body of theoretical work demonstrates that (stochastic) gradient descent imposes specific forms of implicit regularization on the learned solutions~\citep{soudry_implicit_2018,gunasekar_characterizing_2018,gunasekar_implicit_2018,bartlett_benign_2020,chizat_implicit_2020,lyu_gradient_2021,allen-zhu_towards_2023,andriushchenko_sgd_2023,abbe_generalization_2023,zhang_feature_2024}.

Empirical studies further suggest that practical neural networks extend many of these implicit regularization effects through a form of \emph{simplicity bias}, favoring "simpler" solutions over more complex ones~\citep{perez_deep_2019,kalimeris_sgd_2019,xu2019frequency,bhattamishra_simplicity_2023,huh_low-rank_2023,zhao_m3pl_2024}. However, the notion of "simplicity" is often defined empirically and varies across studies. In this work, we formalize the concept of simplicity within our theoretical framework and analyze its relationship to learning world models.
% It has been shown that overparameterized neural networks often have the ability to memorize the entire training set~\citep{zhang_understanding_2017}. Hence, the fact that they do generalize well in many settings indicates the importance of their implicit preferences of certain solutions, \ie, the implicit bias. In simple models such as linear models, random feature models, and two-layer neural networks, a series of theoretical work shows that (stochastic) gradient descent imposes certain implicit regularization on the solutions~\citep{soudry_implicit_2018,gunasekar_characterizing_2018,gunasekar_implicit_2018,bartlett_benign_2020,chizat_implicit_2020,lyu_gradient_2021,allen-zhu_towards_2023,andriushchenko_sgd_2023,abbe_generalization_2023,zhang_feature_2024}. Empirically, it has been shown that practical neural network models often generalize such implicit regularizations with a notion of \emph{simplicity bias}, favoring ``simple'' solution over more complex ones~\citep{perez_deep_2019,kalimeris_sgd_2019,xu2019frequency,bhattamishra_simplicity_2023,huh_low-rank_2023,zhao_m3pl_2024}. Yet, the term ``simplicity'' itself is often empirically defined and varies from work to work. In this work, we formalize the notion of simplicity under our theoretical framework and analyze its relation to learning world models.


\paragraph{Complexity measures.} Kolmogorov complexity~\citep{li2008introduction} in algorithmic learning theory provides a unified framework for quantifying the complexity of any object, including functions. However, it is not computable in general. In machine learning, conventional statistical learning theory typically employs the VC dimension as a complexity measure to bound the generalization error of models~\citep{vapnik_nature_1999}. More recent complexity measures include Rademacher and Gaussian complexities~\citep{bartlett_rademacher_2002}. However, these measures primarily assess the complexity of \emph{function classes} rather than individual \emph{functions}.

A growing body of research introduces complexity measures tailored for neural networks trained via (stochastic) gradient descent~\citep{bartlett_spectrally-normalized_2017,jacot_neural_2018,zhou_non-vacuous_2019,lotfi_pac-bayes_2022,chatterjee_neural_2024}, often leveraging them for generalization analysis. However, these measures inherently depend on the neural network parameterization, rather than capturing function complexity independently of specific parameterizations. Some studies propose alternative complexity metrics inspired by Kolmogorov complexity in the machine learning context~\citep{xu_what_2020,liu_grokking_2023}, but these metrics are typically problem-specific and do not enable a direct complexity analysis in the function space, unlike the approach we take in this work.

% Kolmogorov complexity~\citep{li2008introduction} in algorithmic learning theory provides a unified framework for defining the complexity of any object including functions, yet it is not computable. In the machine learning literature, conventional statistical learning theory use VC dimension as a complexity measure to bound the generalzation error of models~\citep{vapnik_nature_1999}. More recent complexity measures include Rademacher and Gaussian complexities~\citep{bartlett_rademacher_2002}. However, such measures focus on the complexity of \emph{function classes} instead of specific \emph{functions}. Much work in the literature introduces complexity measures for neural networks trained by (stochastic) gradient descent~\citep{bartlett_spectrally-normalized_2017,jacot_neural_2018,zhou_non-vacuous_2019,lotfi_pac-bayes_2022,chatterjee_neural_2024}, often using them for generalization analysis. However, these measures by definition depend on the neural network parameterization instead of functions without parameterizations. Some work proposes alternative complexity metrics for Kolmogorov complexity in the machine learning context~\citep{xu_what_2020,liu_grokking_2023}, but these metrics are problem-specific, hence they do not allow for a direct complexity analysis in the function space as what we do in this work.


% \paragraph{Boolean function analysis.}