\section{Related Works}
\textbf{Multi-View:} A recent ($2021$) survey on image fusion techniques____ identifies MV-fusion among other fusion techniques as an ongoing research topic, which is increasingly attracting more attention. Fusion can happen at different stages in a model architecture. Related works on MV-fusion employ a single image encoder to transform a set of images into vector-space (view tokens) and apply late fusion techniques ____. The zoo of related SOTA late fusion techniques can be grouped into non-trainable, node-wise view-weighting ($\odot$), intra-view aware ($\leftrightarrow$), and inter-view aware ($\updownarrow$) methods. Consider tokenized view embeddings $\chi \in \mathbb{R}^{IxJ}$, where $I$ is the number of views and $J$ the number of hidden nodes, then $\odot = \forall_{j=1}^{J} \Sigma_{i=1}^{I} \chi_{ij}\nu_{ij}$, $\updownarrow = \Sigma_{j=1}^{J} \digamma(\chi_j)$, and $\leftrightarrow = \Sigma_{j=1}^{J} \forall_{I=1}^{I} \digamma(\chi_i)$. $\digamma$ is a trainable function and $\nu$ is a scalar found by some implementation of $\digamma(\subset \chi)$. E.g. pooling methods____ are non-learn-able node-wise view-weighting methods for view aggregation. Likewise, convolution and fully-connected layers can be used to train a inter-view aware node-wise view-weighting policy. Otherwise, methods such as Squeeze-and-Excitation____ (S.\&E.) use the individual embedding to determine a node-wise weighting, allowing an intra-view aware view aggregation. Methods based on concatenation of view embeddings____ are inter-view and intra-view aware. Recent SOTA methods for View-Fusion have success with trainable____ and non-trainable____ view aggregation methods.

\textbf{RGBD:} Unlike MV-fusion, SOTA methods for RGBD-related downstream tasks also employ hybrid fusion, where color and depth signals are gradually fused downstream. The work related to hybrid fusion can be grouped into methods that gradually fuse depth information with color information ($d \rightarrow c$)____ and a bi-directional fusion $c \leftrightarrow d$____. Depth directed fusion ($c\to d$) appears to be unnoticed in related work. Other work for RGBD-fusion-based downstream tasks employ late-fusion-based methods____. 

\textbf{Transformers:} Ever since the introduction of Transformers____ into vision problems____ they push the SOTA within vision-based downstream tasks____. Due to their universal capabilities, transformers are found to be well suited to fuse information from different modalities____. Here, Omnivore____ and Omnivec____ use token-based modality fusion to reach state-of-the-art results on RGBD-based scene classification on SUN-RGBD____. Therefore, we investigate the usage of Transformer-based methods for MV-fusion within industrial applications for part recognition. 

\textbf{MM \& MV DataSets:} Datasets for 6D-object-pose-estimation____, RGBD-segmentation____ and RGBD-instance-detection____ drive RGBD based research within their related downstream tasks. Within MV and classification problems it is a common method within the field of 3D-part-recognition to render a set of 2D views from 3D parts and use image encoders to adopt MV-fusion for a combined classification____. This research results in a vast set of synthetic____ and real world____ RGB(D) datasets for MV-fusion and classification investigations. Real-world MV datasets frequently use video-based capturing methods. Thus, MV can be sampled from the video. MV-RGBD____ uses a turn table to capture objects from multiple fixed view points, while FewSOL____ and GraspNet____ use a robot to capture object(s) from multiple view points. Albeit the use of synthetic industrial components____ and fixed view points, none of the available datasets addresses a realistic industrial application for part recognition. Moreover, recent work within machine vision leverages the combination of images with other modalities such as natural language (NL)____. With MVIP, we are the first to our knowledge to bring the physical properties of objects, natural language, and MV-RGBD images into a single benchmark for application-oriented industrial part recognition.