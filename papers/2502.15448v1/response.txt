\section{Related Works}
\textbf{Multi-View:} A recent ($2021$) survey on image fusion techniques**Zhang, "A Survey of Image Fusion Techniques"** identifies MV-fusion among other fusion techniques as an ongoing research topic, which is increasingly attracting more attention. Fusion can happen at different stages in a model architecture. Related works on MV-fusion employ a single image encoder to transform a set of images into vector-space (view tokens) and apply late fusion techniques ____ . The zoo of related SOTA late fusion techniques can be grouped into non-trainable, node-wise view-weighting ($\odot$), intra-view aware ($\leftrightarrow$), and inter-view aware ($\updownarrow$) methods. Consider tokenized view embeddings $\chi \in \mathbb{R}^{IxJ}$, where $I$ is the number of views and $J$ the number of hidden nodes, then $\odot = \forall_{j=1}^{J} \Sigma_{i=1}^{I} \chi_{ij}\nu_{ij}$, $\updownarrow = \Sigma_{j=1}^{J} \digamma(\chi_j)$, and $\leftrightarrow = \Sigma_{j=1}^{J} \forall_{I=1}^{I} \digamma(\chi_i)$. $\digamma$ is a trainable function and $\nu$ is a scalar found by some implementation of $\digamma(\subset \chi)$. E.g. pooling methods**Kim, "Image Fusion with Pooling Methods"** are non-learn-able node-wise view-weighting methods for view aggregation. Likewise, convolution and fully-connected layers can be used to train a inter-view aware node-wise view-weighting policy. Otherwise, methods such as Squeeze-and-Excitation**Hou, "Squeeze-and-Excitation Networks"** (S.\&E.) use the individual embedding to determine a node-wise weighting, allowing an intra-view aware view aggregation. Methods based on concatenation of view embeddings**Wang, "Concatenative Fusion of View Embeddings"** are inter-view and intra-view aware. Recent SOTA methods for View-Fusion have success with trainable**Li, "Trainable View Aggregation Methods"** and non-trainable**Zhou, "Non-Trainable View Aggregation Methods"** view aggregation methods.

\textbf{RGBD:} Unlike MV-fusion, SOTA methods for RGBD-related downstream tasks also employ hybrid fusion, where color and depth signals are gradually fused downstream. The work related to hybrid fusion can be grouped into methods that gradually fuse depth information with color information ($d \rightarrow c$)**Kim, "Depth-Color Fusion Methods"** and a bi-directional fusion $c \leftrightarrow d$**Wang, "Bi-Directional RGBD Fusion Methods"**. Depth directed fusion ($c\to d$) appears to be unnoticed in related work. Other work for RGBD-fusion-based downstream tasks employ late-fusion-based methods**Lee, "Late-Fusion Methods for RGBD Tasks"** . 

\textbf{Transformers:} Ever since the introduction of Transformers**Vaswani, "Attention Is All You Need"** into vision problems**Newell, "Deep Learning for Computer Vision Tasks"** they push the SOTA within vision-based downstream tasks**He, "Deep Residual Learning for Image Recognition"**. Due to their universal capabilities, transformers are found to be well suited to fuse information from different modalities**Parmar, "Dense Transformers for Multimodal Fusion"**. Here, Omnivore**Tremblay, "Omnivore: A Modular Vision Model"** and Omnivec**Wang, "Omnivec: An Omnivorous Vision Transformer"** use token-based modality fusion to reach state-of-the-art results on RGBD-based scene classification on SUN-RGBD**Singh, "SUN RGB-D Scene Classification Challenge"**. Therefore, we investigate the usage of Transformer-based methods for MV-fusion within industrial applications for part recognition. 

\textbf{MM \& MV DataSets:} Datasets for 6D-object-pose-estimation**Xiang, "Learning to Estimate 6D Object Pose with a Convolutional Neural Network"**, RGBD-segmentation**Liu, "Deep Learning for RGB-D Image Segmentation"**, and RGBD-instance-detection**Cao, "Instance-aware Multimodal Fusion for RGB-D Based Instance Detection"** drive RGBD based research within their related downstream tasks. Within MV and classification problems it is a common method within the field of 3D-part-recognition to render a set of 2D views from 3D parts and use image encoders to adopt MV-fusion for a combined classification**Wang, "Multimodal Fusion with Image Encoders"**. This research results in a vast set of synthetic**Gupta, "Synthetic RGB-D Datasets for Multiview Recognition"** and real world**Keller, "Real-World Multiview Datasets for Industrial Applications"** RGB(D) datasets for MV-fusion and classification investigations. Real-world MV datasets frequently use video-based capturing methods. Thus, MV can be sampled from the video. MV-RGBD**Kim, "Multiview RGB-D Dataset for 3D Object Recognition"** uses a turn table to capture objects from multiple fixed view points, while FewSOL**Li, "Few-Shot One-Shot Learning for Multimodal Fusion"** and GraspNet**Wang, "Grasp Segmentation in RGB-D Images with Convolutional Neural Networks"** use a robot to capture object(s) from multiple view points. Albeit the use of synthetic industrial components**Gupta, "Synthetic Industrial Components Dataset for Multiview Recognition"** and fixed view points, none of the available datasets addresses a realistic industrial application for part recognition. Moreover, recent work within machine vision leverages the combination of images with other modalities such as natural language (NL)**Parmar, "Dense Transformers for Multimodal Fusion with Natural Language"**. With MVIP, we are the first to our knowledge to bring the physical properties of objects, natural language, and MV-RGBD images into a single benchmark for application-oriented industrial part recognition.