\section{Related Works}
\textbf{Multi-View:} A recent ($2021$) survey on image fusion techniques~\cite{fusion_survey} identifies MV-fusion among other fusion techniques as an ongoing research topic, which is increasingly attracting more attention. Fusion can happen at different stages in a model architecture. Related works on MV-fusion employ a single image encoder to transform a set of images into vector-space (view tokens) and apply late fusion techniques \cite{mvcnn,View-GCN,MVTN,GVCNN,GIFT,MVFusionNet,RotationNet,MLVCNN,concat1,concat2,concat3,Multi-View_ML,sum_fuse}. The zoo of related SOTA late fusion techniques can be grouped into non-trainable, node-wise view-weighting ($\odot$), intra-view aware ($\leftrightarrow$), and inter-view aware ($\updownarrow$) methods. Consider tokenized view embeddings $\chi \in \mathbb{R}^{IxJ}$, where $I$ is the number of views and $J$ the number of hidden nodes, then $\odot = \forall_{j=1}^{J} \Sigma_{i=1}^{I} \chi_{ij}\nu_{ij}$, $\updownarrow = \Sigma_{j=1}^{J} \digamma(\chi_j)$, and $\leftrightarrow = \Sigma_{j=1}^{J} \forall_{I=1}^{I} \digamma(\chi_i)$. $\digamma$ is a trainable function and $\nu$ is a scalar found by some implementation of $\digamma(\subset \chi)$. E.g. pooling methods~\cite{mvcnn} are non-learn-able node-wise view-weighting methods for view aggregation. Likewise, convolution and fully-connected layers can be used to train a inter-view aware node-wise view-weighting policy. Otherwise, methods such as Squeeze-and-Excitation~\cite{Squeeze} (S.\&E.) use the individual embedding to determine a node-wise weighting, allowing an intra-view aware view aggregation. Methods based on concatenation of view embeddings~\cite{sum_fuse,MLVCNN,View-GCN} are inter-view and intra-view aware. Recent SOTA methods for View-Fusion have success with trainable~\cite{View-GCN} and non-trainable~\cite{MVTN} view aggregation methods.

\textbf{RGBD:} Unlike MV-fusion, SOTA methods for RGBD-related downstream tasks also employ hybrid fusion, where color and depth signals are gradually fused downstream. The work related to hybrid fusion can be grouped into methods that gradually fuse depth information with color information ($d \rightarrow c$)~\cite{RedNet} and a bi-directional fusion $c \leftrightarrow d$~\cite{CMX-rgbdfusion,Symmetric-Cross-modality-Residual-Fusion}. Depth directed fusion ($c\to d$) appears to be unnoticed in related work. Other work for RGBD-fusion-based downstream tasks employ late-fusion-based methods~\cite{Tokenfusion,RGBDfusion_old,RGBD-pretrained,DenseFusion,MVFusionNet}. 

\textbf{Transformers:} Ever since the introduction of Transformers~\cite{AttentionIsAll} into vision problems~\cite{ImageTransformers} they push the SOTA within vision-based downstream tasks~\cite{Swin,Swinv2,Detr,Deformable-Detr,Dino,segformer}. Due to their universal capabilities, transformers are found to be well suited to fuse information from different modalities~\cite{Clip,stable-diffusion,Tokenfusion,CMX-rgbdfusion,omnivore,omnivec,omnivec2}. Here, Omnivore~\cite{omnivore} and Omnivec~\cite{omnivec,omnivec2} use token-based modality fusion to reach state-of-the-art results on RGBD-based scene classification on SUN-RGBD~\cite{RGBD-SUN}. Therefore, we investigate the usage of Transformer-based methods for MV-fusion within industrial applications for part recognition. 

\textbf{MM \& MV DataSets:} Datasets for 6D-object-pose-estimation~\cite{PoseCNN,LineMod,t-less,GraspNet}, RGBD-segmentation~\cite{RGBD-SUN,RGBD-NYUv2,RGBD-ScanNet,kitti} and RGBD-instance-detection~\cite{PoseCNN,kitti,GraspNet,LineMod} drive RGBD based research within their related downstream tasks. Within MV and classification problems it is a common method within the field of 3D-part-recognition to render a set of 2D views from 3D parts and use image encoders to adopt MV-fusion for a combined classification~\cite{mvcnn,View-GCN,MVTN,GVCNN,GIFT,MVFusionNet,RotationNet,MLVCNN}. This research results in a vast set of synthetic~\cite{synt_ABC,synt_MCB,synt_modelnet,synt_partnet,synt_shapenet} and real world~\cite{real_CO3D,real_PASCAL3D+,real_video_amt,real_video_freiburgcars,real_video_GoogleScanned,real_video_MV-RGBD2011,real_video_objectron,real_video_scanobject} RGB(D) datasets for MV-fusion and classification investigations. Real-world MV datasets frequently use video-based capturing methods. Thus, MV can be sampled from the video. MV-RGBD~\cite{real_video_MV-RGBD2011} uses a turn table to capture objects from multiple fixed view points, while FewSOL~\cite{real_fewShot} and GraspNet~\cite{GraspNet} use a robot to capture object(s) from multiple view points. Albeit the use of synthetic industrial components~\cite{synt_ABC,synt_MCB} and fixed view points, none of the available datasets addresses a realistic industrial application for part recognition. Moreover, recent work within machine vision leverages the combination of images with other modalities such as natural language (NL)~\cite{Clip,stable-diffusion}. With MVIP, we are the first to our knowledge to bring the physical properties of objects, natural language, and MV-RGBD images into a single benchmark for application-oriented industrial part recognition.