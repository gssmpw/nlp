@article{ahuja2022weakly,
  title={Weakly supervised representation learning with sparse perturbations},
  author={Ahuja, Kartik and Hartford, Jason S and Bengio, Yoshua},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={15516--15528},
  year={2022}
}

@misc{allen2019analogiesexplainedunderstandingword,
      title={Analogies Explained: Towards Understanding Word Embeddings}, 
      author={Carl Allen and Timothy Hospedales},
      year={2019},
      eprint={1901.09813},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1901.09813}, 
}

@misc{bengio2009learning,
  title={Learning Deep Architectures for AI},
  author={Bengio, Y},
  year={2009},
  publisher={Now Publishers Inc}
}

@article{bengio2013representation,
  title={Representation learning: A review and new perspectives},
  author={Bengio, Yoshua and Courville, Aaron and Vincent, Pascal},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={35},
  number={8},
  pages={1798--1828},
  year={2013},
  publisher={IEEE}
}

@misc{brehmer2022weaklysupervisedcausalrepresentation,
      title={Weakly supervised causal representation learning}, 
      author={Johann Brehmer and Pim de Haan and Phillip Lippe and Taco Cohen},
      year={2022},
      eprint={2203.16437},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/2203.16437}, 
}

@misc{burns2024discoveringlatentknowledgelanguage,
      title={Discovering Latent Knowledge in Language Models Without Supervision}, 
      author={Collin Burns and Haotian Ye and Dan Klein and Jacob Steinhardt},
      year={2024},
      eprint={2212.03827},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2212.03827}, 
}

@misc{cunningham2023sparseautoencodershighlyinterpretable,
      title={Sparse Autoencoders Find Highly Interpretable Features in Language Models}, 
      author={Hoagy Cunningham and Aidan Ewart and Logan Riggs and Robert Huben and Lee Sharkey},
      year={2023},
      eprint={2309.08600},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2309.08600}, 
}

@misc{engels2024languagemodelfeatureslinear,
      title={Not All Language Model Features Are Linear}, 
      author={Joshua Engels and Isaac Liao and Eric J. Michaud and Wes Gurnee and Max Tegmark},
      year={2024},
      eprint={2405.14860},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2405.14860}, 
}

@misc{ethayarajh2019understandinglinearwordanalogies,
      title={Towards Understanding Linear Word Analogies}, 
      author={Kawin Ethayarajh and David Duvenaud and Graeme Hirst},
      year={2019},
      eprint={1810.04882},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1810.04882}, 
}

@misc{fumero2023leveragingsparsesharedfeature,
      title={Leveraging sparse and shared feature activations for disentangled representation learning}, 
      author={Marco Fumero and Florian Wenzel and Luca Zancato and Alessandro Achille and Emanuele Rodolà and Stefano Soatto and Bernhard Schölkopf and Francesco Locatello},
      year={2023},
      eprint={2304.07939},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2304.07939}, 
}

@article{gao2024scaling,
  title={Scaling and evaluating sparse autoencoders},
  author={Gao, Leo and la Tour, Tom Dupr{\'e} and Tillman, Henk and Goh, Gabriel and Troll, Rajan and Radford, Alec and Sutskever, Ilya and Leike, Jan and Wu, Jeffrey},
  journal={arXiv preprint arXiv:2406.04093},
  year={2024}
}

@inproceedings{gittens-etal-2017-skip,
    title = "Skip-Gram - {Z}ipf + Uniform = Vector Additivity",
    author = "Gittens, Alex  and
      Achlioptas, Dimitris  and
      Mahoney, Michael W.",
    editor = "Barzilay, Regina  and
      Kan, Min-Yen",
    booktitle = "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2017",
    address = "Vancouver, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P17-1007",
    doi = "10.18653/v1/P17-1007",
    pages = "69--76",
    abstract = "In recent years word-embedding models have gained great popularity due to their remarkable performance on several tasks, including word analogy questions and caption generation. An unexpected {``}side-effect{''} of such models is that their vectors often exhibit compositionality, i.e., \textit{adding}two word-vectors results in a vector that is only a small angle away from the vector of a word representing the semantic composite of the original words, e.g., {``}man{''} + {``}royal{''} = {``}king{''}. This work provides a theoretical justification for the presence of additive compositionality in word vectors learned using the Skip-Gram model. In particular, it shows that additive compositionality holds in an even stricter sense (small distance rather than small angle) under certain assumptions on the process generating the corpus. As a corollary, it explains the success of vector calculus in solving word analogies. When these assumptions do not hold, this work describes the correct non-linear composition operator. Finally, this work establishes a connection between the Skip-Gram model and the Sufficient Dimensionality Reduction (SDR) framework of Globerson and Tishby: the parameters of SDR models can be obtained from those of Skip-Gram models simply by adding information on symbol frequencies. This shows that Skip-Gram embeddings are optimal in the sense of Globerson and Tishby and, further, implies that the heuristics commonly used to approximately fit Skip-Gram models can be used to fit SDR models.",
}

@misc{jiang2024originslinearrepresentationslarge,
      title={On the Origins of Linear Representations in Large Language Models}, 
      author={Yibo Jiang and Goutham Rajendran and Pradeep Ravikumar and Bryon Aragam and Victor Veitch},
      year={2024},
      eprint={2403.03867},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2403.03867}, 
}

@inproceedings{lachapelle2022disentanglement,
  title={Disentanglement via mechanism sparsity regularization: A new principle for nonlinear ICA},
  author={Lachapelle, S{\'e}bastien and Rodriguez, Pau and Sharma, Yash and Everett, Katie E and Le Priol, R{\'e}mi and Lacoste, Alexandre and Lacoste-Julien, Simon},
  booktitle={Conference on Causal Learning and Reasoning},
  pages={428--484},
  year={2022},
  organization={PMLR}
}

@InProceedings{lachapelle2023synergies,
  title = 	 {Synergies between Disentanglement and Sparsity: Generalization and Identifiability in Multi-Task Learning},
  author =       {Lachapelle, Sebastien and Deleu, T. and Mahajan, D. and Mitliagkas, I. and Bengio, Y. and Lacoste-Julien, S. and Bertrand, Q.},
  booktitle = 	 {International Conference on Machine Learning},
  year = 	 {2023}
}

@misc{layne2024sparsityregularizationtreestructuredenvironments,
      title={Sparsity regularization via tree-structured environments for disentangled representations}, 
      author={Elliot Layne and Jason Hartford and Sébastien Lachapelle and Mathieu Blanchette and Dhanya Sridhar},
      year={2024},
      eprint={2405.20482},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2405.20482}, 
}

@misc{li2024inferencetimeinterventionelicitingtruthful,
      title={Inference-Time Intervention: Eliciting Truthful Answers from a Language Model}, 
      author={Kenneth Li and Oam Patel and Fernanda Viégas and Hanspeter Pfister and Martin Wattenberg},
      year={2024},
      eprint={2306.03341},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2306.03341}, 
}

@misc{lippe2022citriscausalidentifiabilitytemporal,
      title={CITRIS: Causal Identifiability from Temporal Intervened Sequences}, 
      author={Phillip Lippe and Sara Magliacane and Sindy Löwe and Yuki M. Asano and Taco Cohen and Efstratios Gavves},
      year={2022},
      eprint={2202.03169},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2202.03169}, 
}

@inproceedings{locatello2020weakly,
  title={Weakly-supervised disentanglement without compromises},
  author={Locatello, Francesco and Poole, Ben and R{\"a}tsch, Gunnar and Sch{\"o}lkopf, Bernhard and Bachem, Olivier and Tschannen, Michael},
  booktitle={International conference on machine learning},
  pages={6348--6359},
  year={2020},
  organization={PMLR}
}

@article{marconato2024all,
  title={All or None: Identifiable Linear Properties of Next-token Predictors in Language Modeling},
  author={Marconato, Emanuele and Lachapelle, S{\'e}bastien and Weichwald, Sebastian and Gresele, Luigi},
  journal={arXiv preprint arXiv:2410.23501},
  year={2024}
}

@inproceedings{mikolov-etal-2013-linguistic,
    title = "Linguistic Regularities in Continuous Space Word Representations",
    author = "Mikolov, Tomas  and
      Yih, Wen-tau  and
      Zweig, Geoffrey",
    editor = "Vanderwende, Lucy  and
      Daum{\'e} III, Hal  and
      Kirchhoff, Katrin",
    booktitle = "Proceedings of the 2013 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2013",
    address = "Atlanta, Georgia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N13-1090",
    pages = "746--751",
}

@misc{moschella2023relativerepresentationsenablezeroshot,
      title={Relative representations enable zero-shot latent space communication}, 
      author={Luca Moschella and Valentino Maiorca and Marco Fumero and Antonio Norelli and Francesco Locatello and Emanuele Rodolà},
      year={2023},
      eprint={2209.15430},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2209.15430}, 
}

@misc{nanda2023emergentlinearrepresentationsworld,
      title={Emergent Linear Representations in World Models of Self-Supervised Sequence Models}, 
      author={Neel Nanda and Andrew Lee and Martin Wattenberg},
      year={2023},
      eprint={2309.00941},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2309.00941}, 
}

@article{nissim-etal-2020-fair,
    title = "Fair Is Better than Sensational: Man Is to Doctor as Woman Is to Doctor",
    author = "Nissim, Malvina  and
      van Noord, Rik  and
      van der Goot, Rob",
    journal = "Computational Linguistics",
    volume = "46",
    number = "2",
    month = jun,
    year = "2020",
    url = "https://aclanthology.org/2020.cl-2.7",
    doi = "10.1162/coli_a_00379",
    pages = "487--497",
    abstract = "Analogies such as man is to king as woman is to X are often used to illustrate the amazing power of word embeddings. Concurrently, they have also been used to expose how strongly human biases are encoded in vector spaces trained on natural language, with examples like man is to computer programmer as woman is to homemaker. Recent work has shown that analogies are in fact not an accurate diagnostic for bias, but this does not mean that they are not used anymore, or that their legacy is fading. Instead of focusing on the intrinsic problems of the analogy task as a bias detection tool, we discuss a series of issues involving implementation as well as subjective choices that might have yielded a distorted picture of bias in word embeddings. We stand by the truth that human biases are present in word embeddings, and, of course, the need to address them. But analogies are not an accurate tool to do so, and the way they have been most often used has exacerbated some possibly non-existing biases and perhaps hidden others. Because they are still widely popular, and some of them have become classics within and outside the NLP community, we deem it important to provide a series of clarifications that should put well-known, and potentially new analogies, into the right perspective.",
}

@misc{panickssery2024steeringllama2contrastive,
      title={Steering Llama 2 via Contrastive Activation Addition}, 
      author={Nina Panickssery and Nick Gabrieli and Julian Schulz and Meg Tong and Evan Hubinger and Alexander Matt Turner},
      year={2024},
      eprint={2312.06681},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2312.06681}, 
}

@misc{park2023linear,
      title={The Linear Representation Hypothesis and the Geometry of Large Language Models}, 
      author={Kiho Park and Yo Joong Choe and Victor Veitch},
      year={2023},
      eprint={2311.03658},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{park2024geometrycategoricalhierarchicalconcepts,
      title={The Geometry of Categorical and Hierarchical Concepts in Large Language Models}, 
      author={Kiho Park and Yo Joong Choe and Yibo Jiang and Victor Veitch},
      year={2024},
      eprint={2406.01506},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2406.01506}, 
}

@misc{rajamanoharan2024improvingdictionarylearninggated,
      title={Improving Dictionary Learning with Gated Sparse Autoencoders}, 
      author={Senthooran Rajamanoharan and Arthur Conmy and Lewis Smith and Tom Lieberum and Vikrant Varma and János Kramár and Rohin Shah and Neel Nanda},
      year={2024},
      eprint={2404.16014},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2404.16014}, 
}

@misc{rajendran2024learning,
      title={Learning Interpretable Concepts: Unifying Causal Representation Learning and Foundation Models}, 
      author={Goutham Rajendran and Simon Buchholz and Bryon Aragam and Bernhard Schölkopf and Pradeep Ravikumar},
      year={2024},
      eprint={2402.09236},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{ravfogel-etal-2020-null,
    title = "Null It Out: Guarding Protected Attributes by Iterative Nullspace Projection",
    author = "Ravfogel, Shauli  and
      Elazar, Yanai  and
      Gonen, Hila  and
      Twiton, Michael  and
      Goldberg, Yoav",
    editor = "Jurafsky, Dan  and
      Chai, Joyce  and
      Schluter, Natalie  and
      Tetreault, Joel",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.647",
    doi = "10.18653/v1/2020.acl-main.647",
    pages = "7237--7256",
    abstract = "The ability to control for the kinds of information encoded in neural representation has a variety of use cases, especially in light of the challenge of interpreting these models. We present Iterative Null-space Projection (INLP), a novel method for removing information from neural representations. Our method is based on repeated training of linear classifiers that predict a certain property we aim to remove, followed by projection of the representations on their null-space. By doing so, the classifiers become oblivious to that target property, making it hard to linearly separate the data according to it. While applicable for multiple uses, we evaluate our method on bias and fairness use-cases, and show that our method is able to mitigate bias in word embeddings, as well as to increase fairness in a setting of multi-class classification.",
}

@misc{rimsky2024steering,
      title={Steering Llama 2 via Contrastive Activation Addition}, 
      author={Nina Rimsky and Nick Gabrieli and Julian Schulz and Meg Tong and Evan Hubinger and Alexander Matt Turner},
      year={2024},
      eprint={2312.06681},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{roeder2021linear,
  title={On linear identifiability of learned representations},
  author={Roeder, Geoffrey and Metz, Luke and Kingma, Durk},
  booktitle={International Conference on Machine Learning},
  pages={9030--9039},
  year={2021},
  organization={PMLR}
}

@article{scholkopf2021toward,
  title={Toward causal representation learning},
  author={Sch{\"o}lkopf, Bernhard and Locatello, Francesco and Bauer, Stefan and Ke, Nan Rosemary and Kalchbrenner, Nal and Goyal, Anirudh and Bengio, Yoshua},
  journal={Proceedings of the IEEE},
  volume={109},
  number={5},
  pages={612--634},
  year={2021},
  publisher={IEEE}
}

@inproceedings{seonwoo-etal-2019-additive,
    title = "Additive Compositionality of Word Vectors",
    author = "Seonwoo, Yeon  and
      Park, Sungjoon  and
      Kim, Dongkwan  and
      Oh, Alice",
    editor = "Xu, Wei  and
      Ritter, Alan  and
      Baldwin, Tim  and
      Rahimi, Afshin",
    booktitle = "Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT 2019)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-5551",
    doi = "10.18653/v1/D19-5551",
    pages = "387--396",
    abstract = "Additive compositionality of word embedding models has been studied from empirical and theoretical perspectives. Existing research on justifying additive compositionality of existing word embedding models requires a rather strong assumption of uniform word distribution. In this paper, we relax that assumption and propose more realistic conditions for proving additive compositionality, and we develop a novel word and sub-word embedding model that satisfies additive compositionality under those conditions. We then empirically show our model{'}s improved semantic representation performance on word similarity and noisy sentence similarity.",
}

@article{subramani2022extracting,
  title={Extracting latent steering vectors from pretrained language models},
  author={Subramani, Nishant and Suresh, Nivedita and Peters, Matthew E},
  journal={ACL Findings},
  year={2022}
}

@article{templeton2024scaling,
       title={Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet},
       author={Templeton, Adly and Conerly, Tom and Marcus, Jonathan and Lindsey, Jack and Bricken, Trenton and Chen, Brian and Pearce, Adam and Citro, Craig and Ameisen, Emmanuel and Jones, Andy and Cunningham, Hoagy and Turner, Nicholas L and McDougall, Callum and MacDiarmid, Monte and Freeman, C. Daniel and Sumers, Theodore R. and Rees, Edward and Batson, Joshua and Jermyn, Adam and Carter, Shan and Olah, Chris and Henighan, Tom},
       year={2024},
       journal={Transformer Circuits Thread},
       url={https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html}
    }

@misc{tigges2023linearrepresentationssentimentlarge,
      title={Linear Representations of Sentiment in Large Language Models}, 
      author={Curt Tigges and Oskar John Hollinsworth and Atticus Geiger and Neel Nanda},
      year={2023},
      eprint={2310.15154},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2310.15154}, 
}

@misc{turner2024steeringlanguagemodelsactivation,
      title={Steering Language Models With Activation Engineering}, 
      author={Alexander Matt Turner and Lisa Thiergart and Gavin Leech and David Udell and Juan J. Vazquez and Ulisse Mini and Monte MacDiarmid},
      year={2024},
      eprint={2308.10248},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2308.10248}, 
}

@inproceedings{xu2024sparsityprinciplepartiallyobservable,
title={A Sparsity Principle for Partially Observable Causal Representation Learning},
author={Danru Xu and Dingling Yao and Sébastien Lachapelle and Perouz Taslakian and Julius von Kügelgen and Francesco Locatello and Sara Magliacane},
booktitle={Proceedings of the 41 st International Conference on Machine
Learning},
year={2024}
}

