%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
% \usepackage[rightcaption]{sidecap}
\usepackage{graphicx}
\usepackage{float}
\usepackage{booktabs} % for professional tables

\usepackage{enumitem}
\usepackage{fontawesome}

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with 
% \usepackage[nohyperref]{icml2025}
% above.
% \usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[nohyperref, accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

\input{math_commands.tex}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{mathtools}
\usepackage{xspace}

\newcommand\restr[2]{{% we make the whole thing an ordinary symbol
  \left.\kern-\nulldelimiterspace % automatically resize the bar with \right
  #1 % the function
  \littletaller % pretend it's a little taller at normal size
  \right|_{#2} % this is the delimiter
  }}

\usepackage{xurl}
% \usepackage{everyshi}
\usepackage[pagebackref=true,hyperindex,breaklinks]{hyperref}

\hypersetup{
    colorlinks=true,
    linkcolor=thulianpink,
    filecolor=pear,      
    urlcolor=tiffanyblue,
    citecolor=amethyst,
}

\renewcommand*{\backref}[1]{}
\renewcommand*{\backrefalt}[4]{%
    \ifcase #1%
          \or Cited on page~#2.%
    \else Cited on pages~#2.%
\fi%
}
\usepackage{wrapfig}
\usepackage{tablefootnote}

\usepackage{thm-restate}

%dashed line
\usepackage{array}
\usepackage{arydshln}
\setlength\dashlinedash{0.2pt}
\setlength\dashlinegap{1.5pt}
\setlength\arrayrulewidth{0.3pt}

\usepackage[capitalize,noabbrev]{cleveref} 
\newtheorem{theorem}{Theorem}
\newtheorem{proposition}[theorem]{Proposition}  % Shares the counter with theorem
\newtheorem{definition}[theorem]{Definition}    % Shares the counter with theorem
\newtheorem{lemma}[theorem]{Lemma}              % Shares the counter with theorem
\newtheorem{corollary}[theorem]{Corollary}      % Shares the counter with theorem
\newtheorem{assumption}{Assumption}    % Shares the counter with theorem
\newtheorem{remark}[theorem]{Remark}
\newtheorem{goal}[theorem]{goal}
\newtheorem{example}[theorem]{Example}
\newtheorem{problem}[theorem]{Problem}
\newcommand{\thickoverline}[1]{%
  \overline{\mathrel{#1}\kern-1.5pt\rule{0pt}{0.5ex}}%
}


\crefname{dataset}{Dataset}{Datasets}
\crefname{method}{Method Step}{Method Steps}
\crefname{problem}{Problem}{Problems}
\crefname{definition}{Defn.}{Defns.}
\crefname{theorem}{Thm.}{Thms.}
\crefname{proposition}{Prop.}{Props.}
\crefname{assumption}{Asm.}{Asm.}
\crefname{remark}{Remark}{Remarks}
\crefname{example}{Eg.}{Egs.}
\crefname{equation}{Eqn.}{Eqns.}
\crefname{appendix}{Apx.}{Apx.}

\usepackage{epigraph} 

% https://latexcolor.com/
\definecolor{teal}{rgb}{0.0, 0.5, 0.5}
\definecolor{amethyst}{rgb}{0.6, 0.4, 0.8}
\definecolor{thulianpink}{rgb}{0.87, 0.44, 0.63}
\definecolor{tiffanyblue}{rgb}{0.04, 0.73, 0.71}
\definecolor{pear}{rgb}{0.82, 0.89, 0.19}
\definecolor{applegreen}{rgb}{0.55, 0.71, 0.0}
\definecolor{burgundy}{rgb}{0.5, 0.0, 0.13}
\definecolor{persianindigo}{rgb}{0.2, 0.07, 0.48}


\usepackage{subcaption}
\captionsetup[figure]{font=small}
\captionsetup[table]{font=small}

\usepackage[most]{tcolorbox}
\tcbuselibrary{listingsutf8}
% \tcbset{
%   colback=brown!5!white,   % Background color of the main box
%   colframe=black,         % Border color of the box
%   fonttitle=\bfseries,    % Bold font for the title
%   boxrule=0.4mm,          % Border thickness          % Sharp corners for the box
%   coltitle=white,         % Title text color (set to white)
%   colbacktitle=purple!80!white,     % Background color of the title bar
% }

\lstset{
  language=Python,
  basicstyle=\ttfamily\small,
  keywordstyle=\color{blue},
  commentstyle=\color{gray},
  stringstyle=\color{red},
  backgroundcolor=\color{brown!15!white},
  frame=none,
  showstringspaces=false,
  breaklines=true,
  numbers=left,
  numberstyle=\tiny\color{gray},
  numbersep=5pt
}


\tcbset{
base/.style={
  arc=0mm,
  boxrule=0.3mm, % Width of the border around the box
  left=0.1mm, % Left margin within the box
  right=0.5mm, % Right margin within the box
  top=0.5mm, % Top margin within the box
  bottom=0.5mm, % Bottom margin within the box
  }
}

% \newtcolorbox{mainbox}[1][]{
%   colback=pear!5!white, % Background color of the box
%   colframe=pear!75!black, % Frame color
%   base={#1}
% }

\newtcolorbox{subbox}[1][]{
  colback=black!5!white, % Background color of the box
  colframe=black!75!black, % Frame color
  base={#1}
}

% \setlength{\leftmargini}{0pt} % Remove indentation for the first level of itemize



\usepackage{tabularx}

\DeclareRobustCommand{\parhead}[1]{\textbf{#1}~}


\newcommand{\setmylabel}[1]{
  \renewcommand{\mylabel}{#1:}
}
\newcommand{\bb}{\mathbf{b}}
\newcommand{\x}{\mathbf{x}}
\newcommand{\z}{\mathbf{z}}
\newcommand{\A}{\mathbf{A}}
\newcommand{\D}{\mathbf{D}}
%\newcommand{\P}{\mathbf{P}}
\renewcommand{\c}{\mathbf{c}}
\newcommand{\q}{\mathbf{q}}
\newcommand{\br}{\mathbf{r}}
\newcommand{\s}{\mathbf{s}}
\newcommand{\zerob}{\mathbf{0}}
\newcommand{\C}{\mathcal{C}}
\newcommand{\Z}{\mathcal{Z}}
\newcommand{\X}{\mathcal{X}}
\newcommand{\deltaBold}{\boldsymbol{\delta}}
\newcommand{\deltaz}{\deltaBold^z}
\newcommand{\hatdeltaz}{\hat{\deltaBold}^z}
\newcommand{\tildedeltaz}{\tilde{\deltaBold}^z}
\newcommand{\deltac}{\deltaBold^c}
\newcommand{\hatdeltac}{\hat{\deltaBold}^c}
\newcommand{\tildedeltac}{\tilde{\deltaBold}^c}
\newcommand{\thicktilde}[1]{\mathbf{\tilde{\text{$#1$}}}}
\newcommand{\bsigma}{\boldsymbol{\Sigma}}
\newcommand{\transp}{^\top}
\newcommand{\bblambda}{\boldsymbol{\Lambda}}
\newcommand{\blambda}{\boldsymbol{\lambda}}

\newcommand{\ad}[1]{\textcolor{olive}{\textbf{AD:} #1}}
\newcommand{\sj}[1]{\textcolor{violet}{\textbf{sj:} #1}}
\newcommand{\ds}[1]{\textcolor{teal}{\textbf{DS:} #1}}
\newcommand{\seb}[1]{\textcolor{cyan}{[\textbf{SL:} #1]}}
\newcommand{\red}[1]{\textcolor{red}{#1}}
\newcommand{\isae}{\texttt{SSAE}\xspace}
\newcommand{\aff}{\texttt{aff}\xspace}
\newcommand{\md}{\texttt{MD}\xspace}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}

% \usepackage{minitoc}
% \setcounter{parttocdepth}{3}
% \setcounter{secnumdepth}{3}
% % Make the "Part I" text invisible
% \renewcommand \thepart{}
% \renewcommand \partname{}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{ Identifiable steering via sparse auto-encoding of multi-concept shifts}

\begin{document}

\twocolumn[
\icmltitle{ Identifiable Steering via Sparse Autoencoding of Multi-Concept Shifts}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
% \icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Shruti Joshi}{xxx,yyy}
\icmlauthor{Andrea Dittadi}{abc,def,ghi}
\icmlauthor{Sébastien Lachapelle}{comp}
\icmlauthor{Dhanya Sridhar}{xxx,yyy}
% \icmlauthor{Firstname5 Lastname5}{yyy}
% \icmlauthor{Firstname6 Lastname6}{sch,yyy,comp}
% \icmlauthor{Firstname7 Lastname7}{comp}
% %\icmlauthor{}{sch}
% \icmlauthor{Firstname8 Lastname8}{sch}
% \icmlauthor{Firstname8 Lastname8}{yyy,comp}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{xxx}{Université de Montréal}
\icmlaffiliation{yyy}{Mila - Quebec AI Institute}
\icmlaffiliation{comp}{Samsung - SAIT AI Lab, Montreal}
% \icmlaffiliation{zzz}{Canada Cifar AI Chair}
\icmlaffiliation{abc}{Helmholtz AI}
\icmlaffiliation{def}{Technical University of Munich}
\icmlaffiliation{ghi}{MPI for Intelligent Systems, Tübingen}

\icmlcorrespondingauthor{Shruti Joshi}{shrutijoshi98@gmail.com}
% \icmlcorrespondingauthor{Firstname2 Lastname2}{first2.last2@www.uk}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
% \printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
    Steering methods manipulate the representations of large language models (LLMs) to induce responses that have desired properties, e.g., truthfulness, offering a promising approach for LLM alignment without the need for fine-tuning. Traditionally, steering has relied on supervision, such as from contrastive pairs of prompts that vary in a single target concept, which is costly to obtain and limits the speed of steering research. An appealing alternative is to use unsupervised approaches such as sparse autoencoders (SAEs) to map LLM embeddings to sparse representations that capture human-interpretable concepts. However, without further assumptions, SAEs may not be identifiable: they could learn latent dimensions that entangle multiple concepts, leading to unintentional steering of unrelated properties. 
     We introduce Sparse Shift Autoencoders ({\isae}s) that instead map the \textit{differences} between embeddings to sparse representations.
     % enabling steering of embeddings stemming from paired observations that vary by multiple, unknown concepts, such as from posts and their replies on social media sites. 
    Crucially, we show that {\isae}s are identifiable from paired observations that vary in \textit{multiple unknown concepts}, leading to accurate steering of single concepts without the need for supervision.
    We empirically demonstrate accurate steering across semi-synthetic and real-world language datasets using Llama-3.1 \citep{dubey2024llama3herdmodels} embeddings.
\end{abstract}
% We introduce an SAE based approach that instead encodes the \textit{differences} between embeddings to sparse representations, enabling steering of embeddings stemming from paired observations that vary by multiple, unknown concepts, such as from posts and their replies on social media sites. We show that the proposed approach is identifiable and guaranteed to learn the true steering vectors for individual concepts reproducibly across several runs. We empirically demonstrate accurate steering across semi-synthetic and real-world language datasets using LLama 3 embeddings, showing promising results for steering from weak supervision.

\section{Introduction}
% \epigraph{..we understand the world by studying change, not by studying things..}{As quoted in the Order of Time, Anaximander}
 As increasingly powerful large language models (LLMs) are deployed and widely used, researchers seek to \emph{steer} their behaviour at inference time, without the need to perform expensive model updates, as with fine-tuning. Steering refers to perturbing an LLM's representation of an input prompt (potentially at different layers in the LLM) so that the LLM responds with some desired property. For example, recent work has estimated steering vectors for sycophancy \citep{rimsky2024steering}, sentiment \citep{subramani2022extracting}, machine unlearning \citep{li2024wmdpbenchmarkmeasuringreducing}, and even exploration in the case of LLM agents \cite{rahn2024controllinglargelanguagemodel}. As such, research that improves the ease of steering has the potential to impact the alignment of LLMs to human values, such as harmlessness and helpfulness. However, a crucial assumption made by related work is access to supervision, either from target responses to prompts (c.f., \citet{subramani2022extracting}), or from contrastive pairs of prompts that differ by a single target concept like truthfulness (c.f., \citet{turner2024steeringlanguagemodelsactivation,rimsky2024steering}). Since supervision is costly, designing steering methods that can learn with limited supervision that is easily accessible, could speed up steering research. To this end, this paper focuses on learning to steer from paired samples that vary in multiple unknown concepts.

% , such as positive sentiment \citep{subramani2022extracting}.
% Recent work has applied steering to make LLMs more truthful \citep{li2024inferencetimeinterventionelicitingtruthful,rimsky2024steering}, improve LLM agents \citep{rahn2024controllinglargelanguagemodel}, and perform machine unlearning \citep{li2024wmdpbenchmarkmeasuringreducing}.

% Although steering techniques do not require access to LLM parameters, the perturbations must be learned via supervision from target responses to prompts (c.f.,\citep{subramani2022extracting}), or from contrastive pairs of prompts that differ by a single target concept like truthfulness (c.f., \citep{turner2024steeringlanguagemodelsactivation,rimsky2024steering}.
% Since supervision is costly, designing steering methods that can learn with limited supervision could speed up steering research. To this end, this paper focuses on learning to steering from paired samples that vary by multiple unknown concepts.

% is a step in the direction of \emph{unsupervised learning of steering}.


To better understand the challenges with unsupervised learning, consider a naive approach to learn to steer: obtain a dataset of \emph{LLM embeddings of text} and to this data, fit an autoencoder that learns to map LLM embeddings to some latent space and decode the original vectors back. Assuming the learned latent space captures human-interpretable concepts, steering involves encoding an embedding, perturbing a single dimension in the latent space, and decoding to obtain the steered embedding.
This example of an unsupervised approach is a simplified version of existing approaches to interpreting LLMs using sparse autoencoders \citep{cunningham2023sparseautoencodershighlyinterpretable}.
Why might this unsupervised approach fail to output accurate steered embeddings?
The problem is that by fitting observations alone, autoencoders are generally not \emph{identifiable}: infinitely many encoders and decoders can fit the observations equally well. In the context of steering, this means that in general the learned latent space does not capture individual concepts but rather some nonlinear entangling\footnote{Commonly referred to as polysemanticity \citep{bricken2023monosemanticity}.} thereof so that sparse latent perturbations steer multiple concepts at once and not the target concept alone.

In contrast to the naive unsupervised learning approach, the field of causal representation learning (CRL) proposes numerous approaches to identifiable generative models (see \cref{sec:rel} for related work). However, these approaches focus on generative models of low-level data such as images or text, and assume that the generative model can be inverted to perfectly recover the latent variables from the observations.\footnote{In other words, the mapping from latents to observations is injective.} However, in the setting we study here, the space of latent concepts is much higher-dimensional than the space of LLM representations, making it impossible to invert the mapping from concepts to LLM representations. As an important step in bridging the gap between CRL and LLMs, \citet{rajendran2024learning} formalize concepts as linear subspaces of an LLM's representation, and identify a fixed number of desired concepts up to linear transformations using multiple concept-conditional datasets. In the context of steering, this has two implications: i) since concepts are linearly encoded by LLMs, to steer, we can simply add the same steering vector per concept, and ii) we could develop unsupervised approaches that only linearly entangle concept-specific steering vectors.
We could learn to linearly unmix these vectors with supervision from paired samples varying by a single concept, but potentially from fewer samples than existing steering approaches since learning a linear function is all that is needed. However, the need for curated data remains a hindrance. 

In this paper, we propose Sparse Shift Autoencoders ({\isae}s), models that learn steering vectors from paired samples that vary in multiple unknown concepts. Such samples are cheap to obtain, for example, by pairing posts and their replies on a social media site.  Briefly, the model maps embedding differences between samples in a pair to a latent space that reflects the concept changes and uses a linear decoding function to reconstruct the difference vector, following the \textit{linear representation hypothesis} \citep{mikolov-etal-2013-linguistic, jiang2024originslinearrepresentationslarge} in assuming that concepts are linearly encoded by LLMs. Crucially, we constrain the latent representation to be sparse, meaning that each example is modeled using as few concept changes as necessary.
We then leverage the results developed by \citet{lachapelle2023synergies} and \citet{xu2024sparsityprinciplepartiallyobservable} to prove that the proposed \isae approach identifies \emph{some} concepts, for which we can readily extract steering vectors.
% That is, we can interpret the columns of the learned linear decoder as steering vectors, but we do not know for which concept, and how big of a concept change they implement, which is the strongest guarantee we can hope for with unsupervised learning. Nevertheless, rather than needing to learn a post-hoc unmixing function to extract steering vectors, recovery up to permutations makes it possible to apply steering vectors directly and qualitatively find the concept they encode. 
We study the \isae empirically both with synthetic data, and with Llama-3.1 embeddings on several language datasets, finding that our proposed approach enables accurate steering using multi-concept changes, without the need for careful supervision.

\parhead{Connection to sparse autoencoders.} The \isae approach that we theoretically analyze in this paper resembles sparse autoencoders (SAEs), models used to find interpretable features in LLM representations \citep{cunningham2023sparseautoencodershighlyinterpretable, templeton2024scaling}. However, SAEs build on sparse dictionary learning~\citep{stephane1999wavelet, Mairal_Bach_Ponce2009}, an idea from signal processing that factorizes signals into a sparse linear combination of vectors from a \emph{much higher dimensional basis than the signal itself}. While SAEs model the high-dimensional space of concepts, this comes at the cost of identifiability since inverting the observations to capture concepts is impossible. By contrast, here, we focus on concepts that naturally vary across data pairs, making identifiability analysis possible. 
% we show By contrast, here, we focus on datasets that reflect a much smaller number of concepts that vary, 

In sum, this work: 1) formalizes the problem of learning steering vectors from multi-concept data from the lens of identifiability; 2) proposes the \isae to model multi-concept shifts and establishes identifiability guarantees for these models based on sparsity regularization; 3) demonstrate the usefulness of the \isae method for steering Llama-3.1 embeddings on both semi-synthetic and real language datasets, establishing a useful baseline for future work on tuning the behaviour of LLMs at inference time.

% \begin{itemize}
%   \setlength\itemsep{0em}
% 	\item Formalizes the problem of learning steering vectors from multi-concept data from the lens of identifiability. 
% 	\item Proposes the \isae to model multi-concept shifts and establishes identifiability guarantees for these models based on sparsity regularization. 
% 	\item Demonstrate the usefulness of the \isae method for steering Llama-3.1 embeddings on both semi-synthetic and real language datasets, establishing a useful baseline for future work on tuning the behaviour of LLMs at inference time.
% \end{itemize} 

% In sum, this work contributes to:
% \begin{itemize}
%     \item We introduce a theoretically grounded framework for steering, \isae, that can discover steering vectors for individual concepts from a single dataset of pairs of observations varying in multiple unknown concepts.
%     \item Our work is a first attempt at practically integrating results from identifiability theory to the real-world problem of steering LLMs.
%     \item We validate the steering capabilities of \isae on both semi-synthetic and real-world examples, proposing a baseline for further work on controllable generation using LLMs.
%     % \item A phenomenological attempt at bridging the gap between identifiability theory and the practical problem of interpretability and steering of LLMs.
% \end{itemize}


\section{Identifiable learning of steering vectors}
\label{sec:method_and_analysis}

We begin by formalizing steering before developing a theory about learning steering vectors from multi-concept changes. Lowercase bold letters ($\x$) denote vectors, uppercase bold letters ($\A$) denote matrices, sets are denoted using uppercase letters ($S$) or calligraphic letters $\mathcal{S}$ when they refer to domains of random variables. Please refer to \cref{apx:gloss} for notation followed in the paper.

\subsection{Preliminaries}
We assume that the observed data consists of vectors $\x \in \X \subseteq \mathbb{R}^{d_x}$ generated from underlying \emph{concept representations} $\c \in \C  \subseteq \mathbb{R}^{d_c}$ through an unknown generative process $g: \C \to \X$. Each data point $\x$ is produced by a corresponding ground-truth concept representation $\c$ such that $\x=g(\c)$, where each component $c_k$ corresponds to some abstract concept such as the language a text might be written in or its overall sentiment.
While we cannot observe the concept representation $\c$ of an observation $\x$, we have access to \emph{learned representations} $\z=f(\x)$, where the function $f: \X \rightarrow \Z \subseteq \mathbb{R}^{d_z}$ maps observations $\x$ to $d_z$-dimensional real vectors $\z \in \Z$, known as their \emph{embeddings}.
For example, $f(\x)$ can be learned via next-token prediction as with LLMs. However, there is no guarantee that $f = g^{-1}$ (the function $g$ might not even be invertible). 
Consequently, true concept representations $\c$ are \textit{encoded} in the representations $\z$ through the unknown composite function $\z = f(g(\c))$.

% ,
% $$\max_{f, \gamma} \sum_t \log\, p(\x_t \vert \x_{1:t-1}),$$
% where $p(\x_t \vert \x_{1:t-1}) = \mathrm{softmax}\big(f(\x_{1:t-1})^\top \gamma(\x_t)\big)$ \seb{$p(\x_t \vert \x_{1:t-1}) \propto \exp\big(f(\x_{1:t-1})^\top \gamma(\x_t)\big)$}.
% \vspace{-1mm}
\subsection{Problem formulation}\label{sec:prob_form}
In this paper, we are interested in learning \emph{steering functions} that ideally allow us to manipulate the concepts that are represented in an observation such as a text. To directly manipulate concept $k$ in the space $\mathcal{C}$,\footnote{Such a space is also referred to as the privileged basis in interpretability literature \citep{elhage2022superposition}.} we define the perturbed concept vector as $\tilde{\mathbf{c}}_{k,\lambda} \coloneqq \mathbf{c} + \lambda \mathbf{e}_k$, , where $\mathbf{e}_k$ is the $k$-th standard basis vector. The corresponding perturbed observation is $\tilde{\mathbf{x}}_{k,\lambda} \coloneqq g(\tilde{\mathbf{c}}_{k,\lambda})$.
\begin{definition} (\textbf{Steering function})
\label{defn:steering_fn}
    Fix a target concept $k$ and $\lambda \in \mathbb{R}$. %Then, for each concept representation $\c$, the perturbed concept representation $\tilde \c_{k, \lambda} = \c + \lambda \mathbf{e}_k$, where $\mathbf{e}_k$ is the $k$-th standard basis vector, corresponds to the counterfactual observation $\tilde \x_k = g(\tilde \c_k)$  in concept $k$.
    A \textbf{steering function} $\phi_{k,\lambda}: \mathcal{Z} \rightarrow \mathcal{Z}$ is a function such that for all $\c \in \C$, $\phi_{\lambda, k}(f(g(\c))) = f(g(\c + \lambda \rve_k))$.
\end{definition}
 According to \cref{defn:steering_fn}, a steering function maps each representation $\z = f(\x)$ to its perturbed analog $\tilde\z_{\lambda,k} \coloneqq f(\tilde\x_{\lambda,k})$, as illustrated in \cref{fig:steering}. 
 For example, if the $k$-th concept is language, a steering function maps $\z = f(\x)$, the embedding of a sentence $\x$, to $\tilde \z_{k, \lambda} = f(\tilde \x_{k, \lambda})$, the embedding of the same sentence written in a different language.
  Although the change in $\c$ is linear and sparse (only concept $k$ is perturbed), the resulting change in $\z$ might be nonlinear and dense. 
 % Put differently, a steering function $\phi_{\lambda,k}$ is one which makes the diagram in \cref{fig:steering} commute. 
%That is, a steering function maps the simple additive change that defines the ideal perturbed concept representation $\tilde \c_{k, \lambda} \coloneqq \c + \lambda \mathbf{e}_k$ to the potentially nonlinear change required in the space of learned representations so that the resulting $\tilde \z_k$ embeds the counterfactual observation $\tilde \x_{k, \lambda}$, as seen in \cref{fig:steering}. 
 Steering functions are not guaranteed to exist. However, if $f$ and $g$ are injective, we have $\phi_{\lambda, k}(\z) = f(g(g^{-1}(f^{-1}(\z)) + \lambda\mathbf{e}_k))$.
% \begin{figure}[ht]
% \centering
% \begin{minipage}{.25\textwidth}
%     \centering
%     \includegraphics[scale=0.3]{figs/Screenshot 2025-01-22 at 20.41.41.png}
% \end{minipage}%
% \begin{minipage}{.23\textwidth}
%     \centering
%     \caption{A steering function $\phi_{\lambda, k}$ is such that the above diagram commutes (see Defn. 1).}
%     \label{fig:steering}
% \end{minipage}
% \end{figure}
\begin{figure}
\centering
\includegraphics[scale=0.35]{fig_steering.png}

\caption{A steering function $\phi_{\lambda, k}$ is s.t. the above diagram commutes, i.e., $\phi_{\lambda, k}(f(g(\c))) = f(g(\c + \lambda \rve_k)) \forall \c$. (see \cref{defn:steering_fn}).}

% , i.e., $\phi_{\lambda, k}(f(g(\c))) = f(g(\c + \lambda \rve_k))$ for all $\c$.}
\label{fig:steering}
\end{figure}

\subsection{Learning steering vectors via multi-concept shifts}
\label{sec:dgp}
In practice, a steering function $\phi_{\lambda,k}$ can be learned via supervised learning given a dataset comprising of carefully designed paired observations $(\x, \tilde \x_{k})$, in which a single concept changes between $\x$ and $\tilde \x_{k}$ \citep{shen2017styletransfernonparalleltext, turner2024steeringlanguagemodelsactivation,rimsky2024steering}. However, such a dataset might be difficult to acquire. This raises the following question, at the heart of our contribution:

\textit{How can we learn a steering function $\phi_{k, \lambda}$ with a dataset of paired observations $(\x, \tilde\x)$ in which multiple concepts vary?}

\parhead{Data-generating process.} 
Following \citet{locatello2020weaklysupervised}, we consider paired observations $(\x, \tilde\x)$ assumed to be sampled from the following generative process:
\begin{align}
    &S \sim p(S), \quad (\c, \tilde\c) \sim p(\c, \tilde\c \mid S), \label{eqn:dgp}\\
    &\x \coloneqq g(\c), \quad \tilde\x \coloneqq g(\tilde\c) \,,
\end{align}
where $S \subseteq \{1, \dots, d_c\}$ denotes the subset of concepts that vary between $\x$ and $\tilde\x$.
More precisely, $p(\c, \tilde\c \mid S)$ is such that, with probability one, $\c_k = \tilde\c_k$ for all $k \not\in S$. We also define the difference vectors $\deltaz \coloneqq f(\tilde\x) - f(\x) = \tilde\z - \z$ and $\deltac \coloneqq \tilde\c - \c$. To rephrase what we just said differently, we have that $\deltac_k = 0$ for all $k \not\in S$. Crucially, across each pair of observations, an unknown set of concepts changes, allowing us to leverage paired data that is cheap to obtain, such as posts and their replies on social media sites. 

\parhead{Varying concepts.} In what follows, it will be useful to define $V \subseteq \{1, \dots, d_c\}$ to be the set of \textit{varying concepts}:
\begin{align}
V \;\coloneqq\; \bigcup\nolimits_{\smash{S:\,p(S)>0}} S \,. \label{defn:varying_concepts}
\end{align}
The set $V$ thus contains the concepts that can change in a pair $(\x, \tilde\x)$. Even though concepts outside $V$ are assumed to remain fixed \textit{within} a pair $(\x, \tilde\x)$, they can still vary \emph{across} pairs. Without loss of generality, assume that ${V \coloneqq \{1, .\dots, |V|\}}$.

\parhead{Method.} We propose Sparse Shift Autoencoders ({\isae}s), models of difference vectors $\deltaz$ consisting of an affine encoder $r:\mathbb{R}^{d_z} \rightarrow \mathbb{R}^{|V|}$ and an affine decoder $q:\mathbb{R}^{|V|} \rightarrow \mathbb{R}^{d_z}$. The representation $r(\deltaz)$ predicts $\deltac_V$, i.e., the concept shifts corresponding to $\deltaz$, with $\deltac_V = (\deltac_i)_{i\in V}$ the subvector of $\deltac$ corresponding to the index set $V$. The decoder $q$ then maps these shifts back to embedding differences $\hatdeltaz$. We train {\isae}s to solve the following constrained problem:
% The vector $\hatdeltac_V \coloneqq r(\deltaz)$ represents the predicted concept shifts, and as we detail in this section, the decoding function $q(\cdot)$ will allow us to define steering functions.
% where $\hatdeltac_V \coloneqq r(\deltaz)$ represents the predicted concept shift. This specific choice of parametrization is informed by ongoing discourse \citep{bricken2023monosemanticity, gao2024scaling} on the engineering of SAEs and improves empirical performance. 
\begin{align}
    (\hat{r},\hat{q
    }) \in \arg \min_{r,q} \mathbb{E}_{\x,\tilde\x} \left[ ||\deltaz - q(r(\deltaz))||^2_2\right]\ \label{eqn:recon}\\
    \text{s.t.}\ \ \mathbb{E}_{\x, \tilde\x} || r(\deltaz) ||_0 \leq \beta \label{eqn:sparse_constraint}\,,
\end{align}
where \cref{eqn:recon} is the standard auto-encoding loss that encourages good reconstruction and \cref{eqn:sparse_constraint} is a regularizer that encourages the predicted concept shift vector $\hatdeltac_V \coloneqq \hat r(\deltaz)$ to be sparse. Then, we use $\hat q$ to decode single concept shifts into steering vectors. Formally, we end up with a steering function of the form
\begin{align}
    \hat\phi_k(\z) \coloneqq \z + \hat q(\rve_k) \,,
\end{align}
where $\hat q$ is a solution to the above constrained problem.

To summarize, our method consists of two steps:
\setlist{nolistsep}
\begin{enumerate}[noitemsep]
    \item Solving the  sparsity regularized problem of \cref{eqn:recon,eqn:sparse_constraint} to get an encoder-decoder pair $(\hat r, \hat q)$.
    \item Use the vectors $\hat q(\mathbf{e}_k)$ for all $k \in V$ as steering vectors for all concepts (these are the columns of the matrix representing $\hat q$).
\end{enumerate} 

% \seb{Shruti: issue with $\ell_1$ is it not really capturin sparsity but also scale so need to normalise else will use multiple dimensions to min l1 by making each arbitrarily small. to prevent collapse, normalisation is esse and have meaningful sparsity with respct to l1. BN and normalise decoders of the column.}\seb{I'm now thinking these details could go in the appendix.}
% Further implementation details are provided in \cref{apx:model}.

%In words, we enforce the expected $\ell_1$-norm of estimated concept vectors $\hatdeltac_S$ over the batch to be less than a reasonable threshold, since ground-truth concept vectors and their sparsity might be unknown. 
%We then implement this as a hard constraint through constrained optimization rather than as regularization, following \citet{gallegoPosada2022cooper,lachapelle2023synergies, lachapelle2022disentanglement}.  
%concept shifts $\deltac$ to be sparse. 

% \isae is theoretically guaranteed to recover the same $\hat{q}$ (hence, steering vectors) and $\hat{\deltac}_S$ (hence, concept vectors) consistently across runs (up to permutation and scaling). Next, we experimentally verify these claims.

\parhead{Theoretical justification.} In \cref{sec:wscrl} we will show that, under suitable assumptions on the data-generating process and a suitable choice of $\beta$, the $\ell_0$-regularized problem of \cref{eqn:recon,eqn:sparse_constraint} is guaranteed to learn a $(\hat r, \hat q)$ such that $\hat r (\deltaz) = \mathbf{P}\D\deltac_V$ where $\D$ is an invertible diagonal matrix, $\mathbf{P}$ is a permutation matrix. In other words, the learned representation $\hat r(\deltaz)$ can be related to the ground-truth concept shift vector $\deltac_V$ (considering only the varying concepts $V$) via a permutation-scaling matrix. We will later see how sparsity regularization is crucial for this to happen. Although our theoretical analysis assumes the learned representation has size $|V|$, we find in \cref{apx:v} that, in practice, our method maintains a reasonable degree of identifiability when the representation size is larger than $|V|$.
%the identifiability result requires knowing the number of varying concepts $|V|$ (), we find in \cref{apx:v} that in practice, the approach is relatively robust to misspecification.

Furthermore, \cref{sec:steering_explained} will show how this identifiability guarantee implies that the functions $\hat\phi_{k}(\z) \coloneqq \z + \lambda \hat q(\mathbf{e}_k)$ are valid steering functions (\cref{defn:steering_fn}) for concepts $k \in V$.

\parhead{Implementation.} Informed by empirical insights about SAEs \citep{bricken2023monosemanticity, gao2024scaling}, we parameterize the encoder and decoder as
\begin{flalign}
% \begin{split} \nonumber
     \hatdeltac_V \coloneqq {r}(\deltaz) & \coloneqq \mathbf{W}_e (\deltaz - \mathbf{b}_d) + \mathbf{b}_e\,;
     \label{eqn:enc}\\
    \hatdeltaz \coloneqq {q}(\hatdeltac_V) & \coloneqq \mathbf{W}_d \hatdeltac_V + \mathbf{b}_d\,. \label{eqn:dec}
% \end{split}
\end{flalign}
Moreover, since the $\ell_0$-norm is non-differentiable, in practice we replace it by an $\ell_1$-norm leading to the following relaxed sparsity constraint:
\begin{flalign}
    \mathbb{E}_{\x, \tilde\x} || r(\deltaz)||_1 \leq \beta\,. %\implies \frac{1}{|V|N}\sum_{n=1}^{N} || \hatdeltac_V||_1 \leq \beta. 
    % \quad \textcolor{applegreen}{\text{(sparsity)}}
    \label{eqn:sparse_constraint_l1}
    %\label{eqn:model_sparsity}
    \end{flalign}
We then approximately solve this constrained problem by finding a saddle point of its Lagrangian using the ExtraAdam algorithm \citep{gidel2020variationalinequalityperspectivegenerative} as implemented by \citet{gallegoPosada2022cooper}. \cref{app:sparseopt} provides a detailed discussion of the benefits of constraints as opposed to penalty to regularize objectives. Appropriate normalization is crucial for enforcing sparsity using the $\ell_1$-norm. Further details, including other implementation aspects, are discussed in \cref{sec:emp} and \cref{apx:imp}. 

% Note that, in practice, we do not know which steering vector $\hat q(\mathbf{e}_k)$ corresponds to which concept. \seb{@Dhanya, this is the point you mentioned, maybe add how one should figure it out in practice?} 

\subsection{Identifiability analysis}
\label{sec:wscrl}
This section explains why we expect the representation learned in \cref{eqn:recon} to identify the ground-truth concept shift vector $\deltac_V$ up to permutation and rescaling. To do so, we first demonstrate that, under suitable assumptions, the learned representation $\hat{r}(\deltaz)$ identifies the ground-truth concept shift $\deltac_V$ \textit{up to an invertible linear transformation} when we do not use sparsity regularization. Second, we show that by adding sparsity regularization, the learned representation identifies $\deltac_V$ \textit{up to permutation and element-wise rescaling}.

First, we provide justification for assuming an affine encoder-decoder pair in \cref{eqn:recon}:
%
\begin{assumption}[Linear representation hypothesis]
\label{ass:lrh}
The generative process $g: \C \to \X$ and the learned encoding function $f: \X \to \Z$ are such that  $f \circ g : \C \to \Z$ is linear, implying there exists a $d_z \times d_c$ real matrix $\A$ such that:
    \begin{align}
        \label{eq:linear_entangle}
        \z = f(g(\c)) = \A \c \ .
        %\deltaz = f(g(\deltac)) = \A \deltac,
    \end{align}
\end{assumption}
The linear representation hypothesis (LRH) implies that the learned representation $\z$ \emph{linearly encodes concepts}. 
% We might wonder whether the representations we learn in practice possess this property. 
A long line of work provides evidence for this hypothesis (c.f. \citet{rumelhart1973model, hinton1986learning, mikolov-etal-2013-linguistic, ravfogel2020null}). More recently, theoretical work justifies why linear properties could arise in these models (c.f. \citet{ jiang2024originslinearrepresentationslarge, roeder2021linear, marconato2024all}). \Cref{sec:rel} provides a full list of related work, while \cref{apx:lrh} provides an explanation of the equivalence between LRH's different interpretations. \citet{rajendran2024learning} also leverage the LRH in their work.

Since we expect $d_c \gg d_z$, we cannot assume $\A$ to be injective. Fortunately, we do not need to make this assumption, thanks to the following decomposition. Let $\bar{V} \coloneqq [d_c] \setminus V$ be the complement of $V$. Then:
% \begin{align}
%      \deltaz &= \A\deltac \nonumber\\
%      \deltaz &= \A_V\, \deltac_V  + \A_{\thickoverline{V}}\, \deltac_{\thickoverline{V}}\nonumber\\
%      \deltaz &= \A_V\, \deltac_V \ ,\label{eqn:diff_model}
% \end{align}
\begin{align}
     \deltaz &= \A\deltac 
     = \A_V\, \deltac_V  + \A_{\thickoverline{V}}\, \deltac_{\thickoverline{V}}\nonumber\\
     &= \A_V\, \deltac_V \ ,\label{eqn:diff_model}
\end{align}
where we used the fact that $\deltac_{\bar{V}} = 0$, by definition of $V$. \Cref{eqn:diff_model} can be thought of as a generative process for the \textit{difference vectors} $\deltaz$, as opposed to $\z$, as in \cref{eq:linear_entangle}. An additional key advantage of \cref{eqn:diff_model} against \cref{eq:linear_entangle} is that the matrix involved in it has potentially fewer columns, since $|V| \leq d_c$. This suggests that, instead of assuming $\A$ is injective, we assume its submatrix $\A_V$ is injective.
\begin{assumption}\label{ass:injective_Av}
    The matrix $\A_V \in \mathbb{R}^{d_z \times |V|}$ is injective.
\end{assumption}
Note that this implies that $d_z \geq |V|$, i.e., $\z$ has at least as many dimensions as there are varying concepts. This is feasible given that $d_z$ is typically around $10^3$ (e.g., in LLMs), supporting a large set of varying concepts $V$.

To prove linear identifiability, we will need one more assumption. Let $\Delta_V^c$ be the support of the random vector $\deltac_V$. We will require that this support is diverse enough so that its linear span is equal to the whole space $\sR^{|V|}$.
\begin{assumption}\label{ass:suff_var}
    $\text{span}(\Delta_V^c) = \sR^{|V|}$.
\end{assumption}
With these assumptions, we can show linear identifiability by reusing proof strategies that are now common in the literature on identifiable representation learning \citep{iVAEkhemakhem20a,roeder2021linear,ahuja2022weakly,xu2024sparsityprinciplepartiallyobservable}.

\begin{restatable}[\textbf{Linear identifiability}]{proposition}{linearIdent}\label{prop:linear_ident}
    Suppose $(\hat r, \hat q)$ is a solution to the unconstrained problem of \cref{eqn:recon}. Under \cref{ass:lrh,ass:injective_Av,ass:suff_var}, there  exists an invertible matrix $\mathbf{L} \in \sR^{|V|\times|V|}$ such that $\hat q = \A_V\mathbf{L}$ and $\hat{r}(\z) = \mathbf{L}^{-1}\A_V^+\z$ for all $\z \in \textnormal{Im}(\A_V)$, where $\textnormal{Im}(\A_V)$ is the image of $\A_V$.\footnote{We might not have $\hat{r}(\z) = \mathbf{L}\A_V^+\z$ for $\z \not\in \text{Im}(\A_V)$, since the behavior of $\hat{r}$ is unconstrained by the objective outside the support of $\deltaz$, i.e., outside $\text{Im}(\A_V)$.}
\end{restatable}
We prove \cref{prop:linear_ident} in \cref{app:linear_ident}. The result follows naturally from the linear representation hypothesis in \cref{ass:lrh}, but requires \cref{ass:injective_Av,ass:suff_var} for a complete proof. \citet{rajendran2024learning} prove a similar result, showing that linear subspaces of representations that represent concepts are linearly identified from concept-conditional observations.
% Even though \cref{prop:linear_ident}  may seem obvious based on \cref{ass:lrh}, recall that it is not guaranteed that a solution for $\A$ exists without \cref{ass:injective_Av} and \cref{ass:suff_var}.
% Further, compare \cref{prop:linear_ident} with the main result from \citet{rajendran2024learning}, which shows that assuming the LRH, it is possible to linearly identify $|V|$ concepts from $\mathcal{O}(|V|)$ different concept-conditional datasets. Here, we show linear identifiability is possible with $\mathcal{O}(1)$ dataset. 


%This is similar to the result by \cite{xu2024sparsityprinciplepartiallyobservable} \sj{@Sebastien could you please check this, if possible?}\seb{see comments} but differs in that here we don't require $q$ and $r$ to be invertible.


% \parhead{Is linear identifiability enough?} Recall the linear ambiguity described in \cref{sec:dgp} in the perfectly reconstructed solutions satisfying \cref{ass:recon}, i.e. $\hat{q} = \A_V \rmL^{-1}$ and $\hat{r} = \rmL \A_V$. To see how this solution affects the steering vector that we estimate for concept $k \in V$, we can write,
%    $$\lambda \hat{q}(\rve_k) = \A_V \rmL^{-1} \lambda \rve_k = \sum_{j=1}^{|V|} \lambda \rmL^{-1}_{jk} \A_{V,j}\,,$$
% where $\A_{V,j}$ is the $j$th column of $\A_V$.
% % This tells us that the spurious solutions leads to an incorrect steering vector for concept $k$, one which linearly combines the true steering vectors for each concept $j \in V$, 
% If we use this encoder to obtain a steering vector for concept $k$, the result is a vector that linearly combines the steering vectors $\A_V \rve_j$ for each concept $j \in V$.


% where $\boldsymbol{\Lambda}_V = \text{diag}(\boldsymbol{\lambda_V)}$ is a $|V| \times |V|$ diagonal matrix formed from $\blambda_V$, representing the strengths of all concept changes in $V$ when multiple concepts may be perturbed.

 % Recall the perturbed concept representation in concept $k$, $\tilde{\mathbf{c}}_{k,\lambda} = \mathbf{c} + \lambda \mathbf{e}_k$. The resulting concept representation with perturbations to multiple concepts can be expressed as $\tilde \c = \c + \bblambda \rve_k$. 

% = \sum_{j} \ell_{j,k} \A_j \sum_{j} \ell_{j,k} \A \rve_j\,,
% where $\A_j$ is the $j$th column of $\A$.






% Before developing assumptions for identifying $\A_V$, we first formalize the auto-encoding models that we analyze.
% \begin{problem}
%     \label{defn:autoenc}
%      Consider affine transformations $r: \mathcal{Z} \rightarrow \mathbb{R}^{|V|}$ and $q: \mathbb{R}^{|V|} \rightarrow \mathcal{Z}$ that represent the encoding and decoding functions, respectively, where $r$ restricted to the domain of $\deltaz$ and $q$ are injective. We estimate $\hat{q}$ and $\hat{r}$ as:
%     \begin{align}\label{prob:reconstruction}
%     \hat{q},\hat{r} = \arg \min_{r,q} \mathbb{E}_{\deltaz} \left[ l\big(\deltaz, q(r(\deltaz))\big)\right]
%     \end{align}
% \end{problem}

% % \vspace{-1mm}
% However, consider another solution $\hat{q} = \A_V \rmL$ and $\hat{r} = \rmL^{-1} \A_V^{+}$, where  $\rmL$ is an invertible $|V| \times |V|$ matrix. Since $\rmL^{-1} \rmL = \mathbf{I}$, this solution also perfectly reconstructs $\deltaz$, and is a minimum of the objective in \cref{defn:autoenc}. This tells us that the auto-encoding model in \cref{defn:autoenc} is only \emph{identified up to linear transformations}. That is, all solutions to the target objective are linear transformations of the true mapping $\A_V$.



% This implies that $\A_V \rve_k = \A_k = \A \rve_k$, or that the decoder applied to the canonical basis element $\rve_k$ would enable steering by $\lambda \A_k$ .


% \parhead{Is linear identifiability enough?} Consider the spurious solution $\hat{q} = \A_V \rmL$ and $\hat{r} = \rmL^{-1} \A_V^{+}$. Rather than mapping each observed difference vector $\deltaz$ to its concept representation, this spurious solution maps $\deltaz$ to a different representation that linearly entangles concepts with the matrix $\rmL$. To see how this solution affects the steering vector that we estimate for concept $k \in V$, we can write,
%    $$\hat{q}(\rve_k) = \sj{\lambda} \A_V\rmL\rve_k = \sum_{j} \ell_{j,k} \A_j =  \sum_{j} \ell_{j,k} \A \rve_j\,,$$
% where $\A_j$ is the $j$th column of $\A$.
% This tells us that the spurious solutions leads to an incorrect steering vector for concept $k$, one which linearly combines the true steering vectors for each concept $j \in V$, 
% If we use the spurious encoder to obtain a steering vector for concept $k$, the result is a vector that linearly combines the steering vectors $\A \rve_j$ for each concept $j \in V$.
% Thus, recovering steering vectors requires learning the ``unmixing'' matrix $\rmL^{-1}$, which would require supervision via concept labels or methods like linear independent component analysis which make strict assumptions \citep{comon1994independent}.

%We instead require a stronger notion of identifiability that will allow us to extract steering vectors with minimal human feedback. 
%\begin{definition}[Permutation-scaling identifiability]
%\label{defn:perm_idnt}
%    If every solution to the problem in \eqref{prob:reconstruction_sparse} has the form $\hat{q} = \A_V \D\rmP$ and $\hat{r} = \A_V^{+} \D\rmP$, where $\rmP$ is a $|V| \times |V|$ permutation matrix and $\D$ is an invertible diagonal scaling matrix of the same size, then the model in \cref{prob:reconstruction_sparse} is said to be identifiable and the transformation $\A_V$ is identified up to permutation and scaling.   
%\end{definition}
% This notion of identifiability differs slightly from the standard definition in identifiable representation learning (see \cref{sec:rel}), which focuses on identifying the encoding function $\hat r = \D\rmP \A_V^+$ rather than the decoding function. While the theorem in this paper shows that we can also identify the true encoding function $\A_V^+$, we focus on the true decoding matrix $\A_V$ in the definition since this is the key to obtaining steering vectors.

\parhead{Identifiability up to permutation and rescaling.} To go from identifiability up to linear transformation to identifiability up to permutation and rescaling, we need to make further assumptions. Let $\mathcal{S}$ be the support of the distribution $p(S)$, i.e., $\mathcal{S} \coloneqq \{S \subseteq [d_c] \mid p(S) > 0\}$. The following is based on \citet{lachapelle2023synergies} and \citet{xu2024sparsityprinciplepartiallyobservable}.

% \parhead{Identifiability result sketch.} To identify the auto-encoding model introduced in \cref{prob:reconstruction_sparse}, we leverage a key assumption about \emph{sparse encoding}. While formalized below, at a high-level, we constrain the model to encode each observed difference vector $\deltaz$ as sparsely as possible. The model can only satisfy the sparsity constraint with a latent space that reflects the true varying concepts, where the changes to concepts can indeed be represented as sparsely as possible. For this intuition to lead to identifiability, we require other assumptions, including one that requires the multi-concept changes to vary sufficiently, so that there are \textbf{``diverse enough"} concept variations across all data pairs in the dataset $\mathcal{D}$ (in \cref{defn:varying_concepts}). 
% The subvector $\deltac_S$ corresponds to the shifts in concept values for the concepts in $S$.

% Since the first two assumptions have been motivated already, we introduce them succinctly.
% \begin{assumption}
%     (Injectivity): We assume $q : \mathbb{R}^{|V|} \rightarrow \mathcal{Z}$ to be an injective, affine transformation. Restricted to the domain of the observed $\deltaz$, we assume $r : \mathcal{Z} \rightarrow \mathbb{R}^{|V|}$ to be injective and affine.
% \label{ass:inj}
% \end{assumption}

% \begin{assumption}
% \textcolor{tiffanyblue}{(Perfect reconstruction)}: Under (\cref{ass:inj}), we can perfectly reconstruct $\deltaz$:
% \begin{flalign}
% \label{eqn:recon}
%     \mathbb{E} ||\deltaz - \hatdeltaz ||_2^2 = 0; \quad \hatdeltaz \coloneqq \hat{q}(\hat{r}(\deltaz)).
% \end{flalign}
% \label{ass:recon}
% \end{assumption}
% \vspace{-5mm}

% The next assumption is to ensure \textbf{``diverse enough"} concept variations across all the observed pairs in the dataset $\mathcal{D}$ (in \cref{defn:varying_concepts}).
% The random variable $S \subseteq V$ captures the set of concepts that vary across a single paired observation so that $S \coloneqq \{k \in V | \deltac_k \neq 0\}$. The support $\mathcal{S}$ of the random variable $S$ is $\mathcal{S} \coloneqq \{s \subseteq V \mid p(s) > 0\}$ where $s$ is the realization of $S$ for a specific pair. 

\begin{assumption}[Sufficient diversity of multi-concept shifts]
\label{ass:suffsupp}
The following two conditions hold.
\begin{enumerate}
    \item (Sufficient support variability): For every varying concept $k \in V$, we have
    % \vspace{-1mm}
    \begin{flalign}
        \bigcup_{S \in \mathcal{S} | k \notin S} S = V \setminus \{k \} \quad \forall k \in V \,;
        \label{eqn:suffsupp}
    \end{flalign}
    %\vspace{-4mm}
    \item (Conditional distribution $\mathbb{P}_{\deltac_S | S}$ is continuous): For all $S \in \mathcal{S}$, the conditional distribution $\mathbb{P}_{\deltac_S | S}$ can be described using a probability density with respect to the Lebesgue measure on $\mathbb{R}^{|S|}$.
\end{enumerate}
\end{assumption}

% \seb{Could be nice to give an example of support $\mathcal{S}$ that satisfies the assumption with concepts that we think makes sense for text. We could assume something like $V \coloneqq \{\texttt{language}, \texttt{sentiment}, \texttt{verbosity}\}$ and assume maybe 
% \begin{align*}
% \mathcal{S} :=\ \{&\{\texttt{language, sentiment}\}, \\
% &\{\texttt{language, verbosity}\}, \\
% &\{\texttt{sentiment, verbosity}\} \} \, , 
% \end{align*}
% and give actual examples of pairs of sentences for these three multi-concept shifts. That would help making the discussion more concrete.}

% Intuitively, without the first assumption, two concepts $k, j \in  V$ may never vary separately across all pairs, making it impossible for the model to disentangle them. \Cref{ass:suffsupp} accommodates a wider range of scenarios, including the potential for statistically dependent concepts, which we study empirically in \cref{sec:emp}. As an example, the special case where only one concept varies within a pair but all concepts vary at least once across all pairs satisfies this more general assumption. On the other end, it rules out the case where all the concepts vary within every pair, i.e. $p(S = V) = 1$.
Without the first assumption, two concepts $k,j\in V$ might always change together, meaning there is no data pair in which only one of them varies independently. Intuitively, this would prevent the model from disentangling them effectively. Importantly, our assumption accommodates a broad range of scenarios. E.g., it is not necessarily violated even in an extreme case where $|V| - 1$ concepts change in each pair. Moreover, it allows for the presence of statistically dependent concepts.
The second criterion ensures the distribution $\mathbb{P}_{\deltac_S | S = s}$ does not concentrate mass on a subset of $\sR^{|S|}$ of Lebesgue measure zero. In \cref{apx:suffsupp}, we provide examples and illustrations of distributions that meet or fail the assumption.\footnote{See \citet{lachapelle2023synergies} for a strictly weaker but more technical assumption that is also sufficient for \cref{prop:perm_ident}.} %Next, we define the ideal structure of an estimated concept vector, making the notion of sparsity precise. 

% The sufficient variability assumption does not prevent statistical dependencies between concepts, a setting that we explore empirically in \cref{sec:emp}.
% \sj{proper or lower-D subspace? proper needn't be lower D}

%\begin{definition}     \textcolor{applegreen}{(Sparse concept vectors)}. An estimated concept vector is considered to be sparse when it satisfies the following constraint: $\mathbb{E} || \hatdeltac_S ||_0 \leq \mathbb{E} || \deltac_S ||_0$, referred to as the \textbf{sparsity constraint}.
%    \label{eqn:sparsity}
%\end{definition}

We are now ready to state the main identifiability result of this section. We note that its proof relies to a large extent on an existing result by \citet{lachapelle2023synergies}.

\begin{restatable}[\textbf{Identifiability up to permutation}]{proposition}{permIdent}\label{prop:perm_ident}
    Suppose $(\hat r, \hat q)$ is a solution to the constrained problem of \cref{eqn:recon,eqn:sparse_constraint} with $\beta = \mathbb{E}||\deltac_V||_0$. Under \cref{ass:lrh,ass:injective_Av,ass:suff_var,ass:suffsupp}, there  exists an invertible diagonal matrix and a permutation matrix $\mathbf{D}, \mathbf{P} \in \sR^{|V|\times|V|}$ such that $\hat q = \A_V \mathbf{D}\mathbf{P}$ and $\hat{r}(\z) = \mathbf{P}^\top\mathbf{D}^{-1}\A_V^+\z$ for all $\z \in \textnormal{Im}(\A_V)$, where $\textnormal{Im}(\A_V)$ is the image of $\A_V$.
\end{restatable}

\parhead{Proof sketch.} We outline the proof here and defer the full details to \cref{app:proof}. We first show that all optimal solutions of the constrained problem must reach a reconstruction loss of zero. This means that optimal solutions to the constrained problem are also optimal for the unconstrained one. Thus, these solutions must identify $\A_V$ up to linear transformation, by \cref{prop:linear_ident}. We can then rewrite the constraint as $\mathbb{E} || \mL^{-1}\deltac_V ||_0 \leq \beta = \mathbb{E}||\deltac_V||_0$. Here, we can reuse an argument initially proposed by \citet{lachapelle2023synergies} to leverage this inequality to conclude that $\mL^{-1}$ must be a permutation-scaling matrix. For completeness, we present this argument in \cref{lemma:synergies_proof}. It shows that, applying the matrix $\mL^{-1}$ to $\deltac_V$ always strictly increases its expected sparsity, \textit{unless} $\mL^{-1}$ is a permutation-scaling matrix. Thus, to satisfy the inequality, $\mL$ must be a permutation-scaling matrix.

%The sparsity constraint is key to identifying the mixing matrix $\A_V$ up to permutation and scaling. This is because the desired solutions $\D\rmP \deltac_S$ will have the same sparsity as $\deltac_S$ unless some element of $\D$ is zero. For this principle to rule out all spurious solutions, however, any invertible linear transformations $\rmL$ that entangle $\deltac_S$ must have higher expected $l_0$ norm than the class of disentangled vectors $\hatdeltac_S = \D\rmP \deltac_S$. \cref{ass:suffsupp} rules out the case where the expected sparsity of $\rmL\deltac_V$ is lower than that of $\deltac_V$.  
%We stress again the key implications of this section. The identifiable auto-encoding model we develop here can enable a practitioner to manipulate individual concepts  \emph{even when the data they have does not come from single-concept perturbations and these perturbations may be statistically dependent.}

\subsection{Extracting steering vectors}
\label{sec:steering_explained}
In \cref{sec:dgp}, we proposed using steering functions of the form $\hat \phi_k(\z) \coloneqq \z + \hat q(\mathbf{e}_k)$ to perturb concepts, 
%to steer concept $k$, we proposed the steering function $\hat \phi_k(\z) \coloneqq \z + \hat q(\mathbf{e}_k)$ that is fully specified by the steering vector $\hat q(\mathbf{e}_k)$
where $\hat q$ is learned via the constrained problem of \cref{eqn:recon,eqn:sparse_constraint}. Here, we show that these steering functions are valid. Our argument relies on the identifiability guarantees presented in the previous section. Under \cref{ass:lrh,ass:injective_Av,ass:suff_var,ass:suffsupp}, \cref{prop:perm_ident} shows that $\hat q = \A_V \mathbf{DP}$, from which we can verify that $\hat q(\mathbf{e}_k) = \mathbf{D}_{\pi(k), \pi(k)}\A\mathbf{e}_{\pi(k)}$ for all $k \in V$, where $\pi:V \rightarrow V$ is the permutation that $\mathbf{P}$ represents. From this, we can show that $\hat\phi_k$ is (almost) a steering function as per \cref{defn:steering_fn}:
% \begingroup
% \setlength{\abovedisplayskip}{15pt}%
% \setlength{\belowdisplayskip}{16pt}%
% \setlength{\abovedisplayshortskip}{16pt}%
% \setlength{\belowdisplayshortskip}{15pt}%
\begin{align*}
    \hat\phi_k(f(g(\c))) &= f(g(\c)) + \hat q(\mathbf{e}_k) \\
    &= \A\c + \mathbf{D}_{\pi(k), \pi(k)}\A\mathbf{e}_{\pi(k)} \\
    &= \A(\c + \mathbf{D}_{\pi(k), \pi(k)}\mathbf{e}_{\pi(k)}) \\
    &= f(g(\c + \mathbf{D}_{\pi(k), \pi(k)}\mathbf{e}_{\pi(k)})) \,.
\end{align*}
% \endgroup
Thus, $\hat\phi_{k}$ sends the representations $\z = f(g(\c))$ to their counterfactual representations where only concept $\pi(k)$ changed. We note that, strictly speaking, this does not correspond exactly to the definition of steering function introduced in \cref{sec:prob_form}, since both the scale of the shift $\mathbf{D}_{\pi(k), \pi(k)}$ and the concept being manipulated $\pi(k)$ are unknown. Although the scaling and permutation indeterminacies are unavoidable with unsupervised learning, a practitioner can choose a steering vector $\hat q(\mathbf{e}_k)$, add different scalings of this vector to an embedding $\z$ and, e.g., use it to generate the next tokens with an LLM to infer which concept the vector $\hat q(\mathbf{e}_k)$ affects.
% \seb{see my comment}.

%We make two important remarks: First, a priori, a user cannot predict beforehand precisely which concept $\pi(k)$ is being steered by $\hat\phi_k$ since $\pi$ is unknown. Second, the scale of the shift, which we denoted by $\lambda$ earlier, is also unknown, since the scaling factor $\mathbf{D}_{\pi(k), \pi(k)}$ is unknown as well.

\begin{comment}
\begin{align*}
    \hat\phi_k(f(g(\c))) &=  \hat\phi_k(\A\c) \\
    &= \A\c + \hat q(\mathbf{e}_k) \\
    &= \A\c +  \A_V \mathbf{D}\mathbf{P}\mathbf{e}_k \\
    &= \A\c +  \A_V \mathbf{D}\mathbf{e}_{\pi(k)} \\
    &= \A\c + \mathbf{D}_{\pi(k), \pi(k)}\A_V\mathbf{e}_{\pi(k)} \\
    &= \A\c + \mathbf{D}_{\pi(k), \pi(k)}\A\mathbf{e}_{\pi(k)} \\
    &= \A(\c + \mathbf{D}_{\pi(k), \pi(k)}\mathbf{e}_{\pi(k)}) \\
    &= f(g(\c + \mathbf{D}_{\pi(k), \pi(k)}\mathbf{e}_{\pi(k)}) \,,
\end{align*}
\end{comment}
%where $\tilde\z_{\pi(k)}$ is the counterfactual representation in which the concept vector $\c$ is the same as for $\z$ except for concept $\pi(k)$ which is shifted. We make two important remarks: First, a priori, a user cannot predict beforehand precisely which concept $\pi(k)$ is being steered by $\hat\phi_k$ since $\pi$ is unknown. Second, the scale of the shift, which we denoted by $\lambda$ earlier, is also unknown, since the scaling factor $\mathbf{D}_{\pi(k), \pi(k)}$ is unknown as well.

%the permutation between true and learned concept shifts.
\parhead{Linear identifiability is insufficient.} In contrast, the above strategy fails when concept shifts are only linearly identified, i.e., $\hat q \coloneqq \A_V \mathbf{L}$. In this case, we see that 
\begin{align*}
% \begin{split} \nonumber
    \hat q(\rve_k) = \A_V \rmL \rve_k = \sum_{j=1}^{|V|} \rmL_{j,k} \A \rve_j = \A\sum_{j=1}^{|V|} \rmL_{j,k} \rve_j \,,
% \end{split}
\end{align*}
which itself implies that 
\begin{align*}
    \hat\phi_k(f(g(\c))) &= \A\c + \A\sum\nolimits_{j=1}^{|V|} \rmL_{j,k} \rve_j\\
    &= \A(\c + \sum\nolimits_{j=1}^{|V|} \rmL_{j,k} \rve_j) \\
    &= f(g(\c + \sum\nolimits_{j=1}^{|V|} \rmL_{j,k} \rve_j)) \,.
\end{align*}
\begin{comment}
\begin{align*}
% \begin{split} \nonumber
    \hat{q}(\rve_k) &= \A_V \rmL \rve_k = \sum\nolimits_{j=1}^{|V|} \rmL_{jk} \A_{V} \rve_j = \sum\nolimits_{j=1}^{|V|} \rmL_{jk} \A \rve_j\\
    % & = \sum_{j=1}^{|V|} \rmL_{jk} \A_{V} (\tilde\c_{j, \lambda} - \c)
    % = \sum_{j=1}^{|V|} \rmL_{jk} (\tilde\z_{j, \lambda} - \z).
    & = \sum\nolimits_{j} \rmL_{jk} \A_{V} (\tilde\c_{j, } - \c)
    = \sum\nolimits_{j} \rmL_{jk} (\tilde\z_{j, \lambda} - \z).
% \end{split}
\end{align*}
\end{comment}
That is, each learned steering vector $\hat{q}(\rve_k)$ can potentially change every concept in $V$. To recover the steering vectors, we need to learn $\rmL^{-1}$, which requires paired samples $(\tilde\z_{j, \lambda}, \z)$ that vary in a single concept for each concept $j$ \citep{rajendran2024learning}. This highlights the importance of enforcing sparsity, as it is the key element allowing us to go from $\hat{q} \coloneqq \A_V\rmL$ (\cref{prop:linear_ident}) to $\hat{q} \coloneqq \A_V\rmD\rmP$ (\cref{prop:perm_ident}). 

% A potential advantage of linearly identifying steering vectors, however, is that learning the linear function $\rmL^{-1}$ may require fewer samples than learning a potentially nonlinear steering function (\cref{defn:steering_fn}) from counterfactual samples.


% \textbf{Limitations}. Even though \cref{prop:perm_ident} enables identification of steering vectors and varying concepts up to irrelevant 
% indeterminacies from pairs of observations for which the following quantities are unknown: the exact concepts varying within and across all pairs, and the specific number of them varying within a pair, which is substantially less restrictive than assuming the number and type of concept for every pair is known and equal to $1$---we do require knowledge of the total number of varying concepts, $|V|$.
% We examine the robustness of our method's identifiability to the degree of misspecification of $|V|$ in \cref{apx:v}.

% permits identifiability  up to irrelevant determinacies such that we do not know which or how many concepts vary within a pair, across all pairs, while we assume which concepts vary to be unknown, we require the total number of concepts varying to be known.
% \seb{TODO: Given these unknowns, explain what a practitionner would need to do.}

% \seb{TODO: Expand on what happens if we do not regularize for sparsity. via linear identifiability.}

% \seb{Copy-pasted.}To see how the proposed notion of identifiability leads to accurate steering vectors, we can write,
% $$\lambda \hat{q} (\rve_k) = \A_V\D\rmP\lambda\rve_k = \lambda d_{\pi(k), \pi(k)}(\A_V)_{\pi(k)} \, ,$$ 
% where $d_{k,k}$ denotes the $k$th diagonal element of $\D$, $(\A_V)_k$ denotes the $k$th column of $\A_V$, and $\pi$ is the permutation associated with the permutation matrix $\rmP$. Thus, identifying $\A_V$ up to permutation and scaling gives us \emph{permuted and rescaled} steering vectors. That is, by decoding the $k$-th basis vector, we obtain the steering vector for a concept $k'$ which is not necessarily $k$. However, a practioner can perform this decoding and apply the resulting steering vector to one example in their dataset to visually inspect the change to determine the changed concept, which was not possible when the estimated vector modified multiple concepts at once, as in the linear identifiability case. 


% \section{\texttt{SSAE}s: Sparse Shift Autoencoders}
% \label{sec:model}
% We implement the \isae model sketched out in \cref{sec:dgp} by defining the following encoding and decoding functions:
% This section outlines iSAE, a method for estimating  the autoencoding framework described in \cref{prob:reconstruction_sparse}. Specifically, we define the encoder and decoder as follows:
%
% \begin{flalign}
% % \begin{split} \nonumber
%      \hat{r}(\deltaz) &= \hatdeltac_V = \mathbf{W}_e (\deltaz - \mathbf{b}_d) + \mathbf{b}_e;
%      \label{eqn:enc}\\
%     \hat{q}(\hatdeltac_V) &= \hatdeltaz = \mathbf{W}_d \hatdeltac_V + \mathbf{b}_d. \label{eqn:dec}
% % \end{split}
% \end{flalign}
% where $\hatdeltac_V$ represents the predicted concept shifts s.t. its non-zero elements indicate the concepts $S$ varying specifically for a given $\deltaz$,

% and the columns of $\mathbf{W}_d$ represent steering vectors for each distinct concept $i \in V$. We minimize the reconstruction loss (\cref{eqn:recon}) subject to the sparsity constraint in \cref{eqn:sparse_constraint} implemented as,
% \begin{flalign}
%         \mathcal{L}(\hat{q}, \hat{r}) = ||\deltaz - \hat{q}(\hat{r}(\deltaz))||^2_2; \quad \textcolor{tiffanyblue}{\text{(reconstruction)}}
%         \label{eqn:model_recon}
%         \\
% \end{flalign}
% subject to the sparsity constraint in,
% \begin{flalign}
%     \mathbb{E}|| \hatdeltac_V||_1 \leq \epsilon \implies \frac{1}{|V|N}\sum_{n=1}^{N} || \hatdeltac_V||_1 \leq \epsilon. 
%     % \quad \textcolor{applegreen}{\text{(sparsity)}}
%     \label{eqn:model_sparsity}
%     \end{flalign}
% That is, we enforce the expected $\ell_1$ norm of estimated concept vectors $\hatdeltac_S$ over the batch to be less than a reasonable threshold, since ground-truth concept vectors and their sparsity is unknown.
% We then implement this through constrained optimization rather than regularization, following \citet{gallegoPosada2022cooper,lachapelle2023synergies, lachapelle2022disentanglement}. \cref{app:sparseopt} provides a detailed discussion of these benefits. Further implementation details are provided in \cref{apx:model}. \isae is theoretically guaranteed to recover the same $\hat{q}$ (hence, steering vectors) and $\hat{\deltac}_S$ (hence, concept vectors) consistently across runs (up to permutation and scaling). Next, we experimentally verify these claims.

% Next, we summarize the main features of iSAE.
% \noindent
% \begin{itemize}

% \textbf{Consistency up to permutation and scaling}. iSAE guarantees that repeated runs recover the same encoded representation $\hatdeltac_S$ up to permutation and scaling. When the ground-truth $\deltac_S$ is known, the learned $\hatdeltac_S$ lies in an equivalence class related by permutation and scaling, thus still permitting access to each individual concept. The same holds true for the encoder $\hat{r}$ and decoder $\hat{q}$.

% \textbf{Concept steering vectors}. 

% \textbf{Sparsity-constrained optimization}. The method enforces sparsity (\cref{eqn:sparsity}) via constrained optimization 

\section{Empirical Studies}
\label{sec:emp}
In this section, we investigate two main questions: (i) how well can \isae identify concept shifts in practice, and (ii) do the steering vectors recovered by \isae accurately steer target concepts?

% i) how well do [model_name]s identify concept shifts \deltac in practice? (ii) do steering vectors recovered from [model_name]s lead to accurate steering of single concepts? 
% can we learn $q$ up to permutation and scaling from data across different numbers and types of varying concepts, thereby validating \cref{prop:perm_ident} in practice? Do the identified  $\deltac_S$ vectors lead to downstream improvement in generating embeddings that reflect the target concept? 


\paragraph{Implementation details.} As mentioned in \cref{sec:wscrl}, key to identifying steering vectors is the sparsity constraint from \cref{eqn:sparse_constraint_l1}. Two key aspects of enforcing sparsity of the learnt representation are: (i) using hard constraints rather than penalty tuning, which helps address concerns with $\ell_1$-based regularization (e.g., feature suppression \citep{anders_etal_2024_composedtoymodels_2d}) and (ii) appropriate normalisation. For the former, we use the \texttt{cooper} library \citep{gallegoPosada2022cooper}. For the latter, we implement batch normalization \citep{ioffe2015batchnormalizationacceleratingdeep} after the encoder and column normalization in the decoder at each step \citep{bricken2023monosemanticity, gao2024scaling}. To tune the model's hyperparameters in an unsupervised way, we use the Unsupervised Diversity Ranking (UDR) score \citep{duan2019unsupervised}, and test the model's sensitivity on key parameters (such as the sparsity level 
$\epsilon$ and learning rate). For details, refer to \cref{apx:imp}.

\paragraph{Baselines.} We consider two baselines: an affine Autoencoder (\aff), with identical architecture but no sparsity constraint, and Mean Difference (\md) vectors, which use paired observations differing in a single concept to compute $\frac{1}{n}\sum_{i=1}^n(\tilde \z^{(i)}_k - \z^{(i)}_k) = \frac{1}{n}\sum_{i=1}^n \lambda^{(i)} \A \mathbf{e}_k = \bar{\lambda} \A \mathbf{e}_k$ as the steering vector for concept $k$.  We denote the concept-steered embeddings produced by each method as $\tilde \z_{\eta}$ (\isae), $\tilde \z_{\aff}$ (\aff), and $\tilde \z_{\md}$ (\md).
\begin{table}
% \begin{wraptable}{r}{0.55\textwidth}
    \centering
    \renewcommand{\arraystretch}{1.25}
    \caption{Datasets comprise of paired observations $(\x, \tilde \x)$ where $\x$ and $\tilde \x$ vary in concepts $V = \{c_1, c_2, ..., c_{|V|}\}$ across all pairs, such that for any given pair, the maximum number of varying concepts is max($|S|$). \textit{Nomenclature for semi-synthetic datasets follows the rule: identifier of the dataset indicating why we consider it, followed by $|V|$ and max$(|S|)$: \textsc{identifier}($|V|$, max$|S|$)}.}
    \label{tab:datasets}
    \footnotesize{
    \begin{tabular}{c  c  c}
    \toprule
\textbf{Dataset} & $|V|$ & max($|S|$) \\
        \midrule
        \textsc{lang}($1, 1$) & 1 & 1 \\ \hdashline
\textsc{gender}$(1, 1)$
        % \textsc{gender}($1, 1$)\tablefootnote{We do not believe that gender is a binary concept and only consider it since it is common practice to show steering on binarily gendered contexts, and it helps to compare values of cosine similarities obtained.} 
        & 1 & 1 \\ \hdashline
        \textsc{binary}($2, 2$) & 2 & 2 \\ \hdashline
        \textsc{correlated}($2, 1$) & 2 & 1 \\ \hdashline
        \textsc{cat}($135, 3$)\tablefootnote{This is in line with contemporary literature \citep{park2024geometrycategoricalhierarchicalconcepts, reber2024rate} but is slightly misleading since the concepts are represented as binary contrasts, please refer to \cref{apx:data} for a detailed discussion.} & 135 & 3 \\ \midrule
        TruthfulQA  & 1 & 1 \\ 
        \bottomrule
    \end{tabular}
    }
\label{tab:datasets}
% \end{wraptable}
\end{table}

\paragraph{Evaluation criteria.} We measure 
the \textit{degree of identifiability} via the Mean Correlation Coefficient (MCC) \citep{hyvarinen2016unsupervisedfeatureextractiontimecontrastive, khemakhem2020icebeemidentifiableconditionalenergybased}, which computes the highest average correlation between each learned latent dimension and the true latent dimension and equals $1.0$ when they are aligned perfectly up to permutation and scaling. On synthetic data, we compute MCC against known ground-truth concept representations. When ground-truth is not known, such as in experiments with LLMs, we compute MCC across different random initializations of the model, expecting 1.0 for models that consistently recover solutions equivalent to each other up to permutation and scaling. We use $10$ decoder pairs from $5$ seeds for selected model hyperparameters. Further, we evaluate whether identified representations indeed lead to higher \textit{steering accuracy}. For this, we consider held-out single concept shift data $(\x, \tilde \x_{k})$ and evaluate how well steering vectors learnt using multi-concept shifts steer $f(\x)$ towards $f(\tilde\x_k)$. Then we measure the accuracy of steering by comparing  $\hat{\tilde{\z}}_k \coloneqq f(\x) + (\mathbf{W}_d)_{\{:, \pi(k)\}}$ where and $f(\tilde\x_k)$ using cosine similarity as a measure of semantic similarity. Refer to \cref{apx:mcc} for details on metrics.

% we use cosine similarity.= f(\x) + \A_V \pi(k)
% using cosine similarity between steered embeddings using the steering vector $(\hatdeltac_S)_i(\mathbf{W}_d)_{\{:, i\}}$ and the corresponding target embeddings of concept 
% $i$. 

% we consider held-out single concept shift data (x, \tilde\x_{k}) and see how well steering vectors learned by the [method_name] on multi-concept shifts steer f(x) towards f(\tilde\x_k). Then we can say that to measure the accuracy between \hat{\tilde_z} = f(x) + A_v_\pi(k) and f(\tilde\x_k), we use cosine similarity.
% Each column of the learnt decoder $\hat{q}$ represents the direction along which each of the $|V|$ concepts can be varied, and each element of $\deltac_V$ denotes the amount by which an individual concept varies. Thus, by searching over the columns of the decoder, it is possible to vary individual concepts. We evaluate the similarity between the steered embeddings using $(\hatdeltac_V)_{\{i,:\}}(\mathbf{W}_d)_{\{:, i\}}$ to the target embeddings for concept $i$. To evaluate the similarity between the steered embeddings and the target embeddings, we compute the cosine similarity between them induced by the Euclidean dot product. This is a reliable similarity metric for LLM representations as it has been shown \citep{jiang2024originslinearrepresentationslarge} that the implicit bias of gradient descent while pre-training language models favours a Euclidean geometry of the representation space, despite the Euclidean dot product being unidentified by standard pre-training objectives \citep{park2023linear}. 
%, which we report in \cref{apx:sens}.
% \end{itemize}

\begin{figure}[ht]
    \centering\includegraphics[width=\linewidth]{cosine_sims_same_.png}
    \caption{\textbf{A higher MCC value of the estimated decoder is associated with a greater cosine similarity.} Embeddings steered using the steering vectors from a more disentangled decoder are more similar to target embeddings, compared to embeddings steered using steering vectors from a decoder with a lower MCC value.}
    \label{fig:cos}
\end{figure}

% \begin{figure}[ht]
%     \centering
%     \includegraphics[width=\linewidth]{cosine_sims_same_old.png}
%     \caption{\textbf{A higher MCC value of the estimated decoder is associated with a greater cosine similarity.} Embeddings steered using the steering vectors from a more disentangled decoder are more similar to target embeddings, compared to embeddings steered using steering vectors from a decoder with a lower MCC value.}
%     \label{fig:cos}
% \end{figure}

% \subsection{Steering Llama-3 Embeddings}
We focus on evaluating LLM embeddings on language datasets here. In \cref{apx:synth}, we validate the same conclusions with experiments on synthetic data with known concepts and synthetically generated embeddings.

\label{sec:exp_llms}
\begin{table}
    \centering
    \renewcommand{\arraystretch}{1.25}
    \caption{\textbf{The mean MCC of the estimated decoder is close to $1$ }across all datasets considering observations $(f(\x), f(\tilde \x))$, even for statistically dependent concepts considered in the dataset \textsc{CORR}($2, 1$).}
    \label{tab:mcc_lang}
    \footnotesize{
    \begin{tabular}{c  c  c}
    \toprule
         & \textbf{\isae} & \textbf{\aff} \\
         \midrule
        \textsc{lang}($1, 1$) & $0.9948 \pm 0.0011$ & $ 0.9850 \pm 0.0037$ \\ \hdashline
        \textsc{gender}($1, 1$)    & $0.9933 \pm 0.0001$ & $0.9614 \pm 0.0000$ \\ \hdashline
        \textsc{binary}($2, 2$) & $0.9908 \pm 0.0010$ & $0.9356 \pm 0.0001$ \\ \hdashline
        \textsc{corr}($2, 1$) &
        $0.9905 \pm 0.0012$ & $0.9283 \pm 0.0772$ \\ \hdashline
        \textsc{cat}($135, 3$) & $0.9059 \pm 0.02186$ & $0.6607 \pm 0.0189$ \\ 
        \midrule
        TruthfulQA    & $0.9521 \pm 0.0061$ & $0.8848 \pm 0.0063$ \\ 
        \bottomrule
    \end{tabular}
    }
\end{table}

\paragraph{Experimental setup.} We use text-based paired observations $(\x, \tilde \x)$, to extract the final‐layer token embedding (which is linearly identifiable following \citet{roeder2021linear}) from Llama-3.1-8B \citep{dubey2024llama3herdmodels}, and use the embedding of the last token as the representation, following \citet{ma2023finetuningllamamultistagetext}, to obtain representations $(f(\x), f(\tilde \x))$. 

% We discuss briefly the different datasets considered along with the underlying concepts in them.

\paragraph{Data.} We consider both real-world language data: since existing real-world datasets used in steering research usually consist of a single concept varying across the entire dataset \citep{perez2022discoveringlanguagemodelbehaviors, lin2022truthfulqameasuringmodelsmimic, liu2024incontextvectorsmakingcontext, panickssery2024steeringllama2contrastive}, we create a more diverse range of concept variations in semi-synthetic data. All datasets are summarised in \cref{tab:datasets}.
 \textsc{lang}($1, 1$) (e.g., \textit{eng} $\rightarrow$ \textit{french}) and
\textsc{gender}($1,1$) (e.g., \textit{masculine} $\rightarrow$ \textit{feminine} vary a single concept between $\x$ and $\tilde \x$.
\textsc{binary}($2,2$) allows two binary concepts (language, gender) to vary jointly, and 
\textsc{correlated}$(2,1)$ enforces correlated binary changes (e.g., \textit{eng} $\rightarrow$ \textit{french} along with \textit{eng} $\rightarrow$ \textit{german}). \textsc{cat}$(135, 3)$ uses shape-color-object phrases (eg: ``purple toroidal shoe") where each attribute has $10$ possible values, represented via binary contrasts to produce $135$ concepts in total. We apply our method to the multiple-choice track of TruthfulQA 
 \citep{lin2022truthfulqameasuringmodelsmimic}, creating $(\x, \tilde \x)$ pairs by assigning $\x$ to be the question paired with a wrong answer that mimics human falsehoods, and $\tilde \x$ to be a question paired with the correct answer to capture the variation of the concept truthfulness from \textit{false} $\rightarrow$ \textit{true}. Details on datasets can be found in \cref{apx:data}.

% Since the Llama family consists of decoder only models, to extract a representation from it, we follow \citet{roeder2021linear} to first select the output from the last linear layer of the model, which is the linearly identifiable representation \citep{roeder2021linear}. This output consists of embeddings for every single token in the input. So, we take the embedding corresponding to the last token as the representation following \citet{ma2023finetuningllamamultistagetext}. We discuss briefly the different datasets considered along with the underlying concepts in them.

\begin{figure}
\centering\includegraphics[width=\linewidth]{cosine_sims_ood_.png}
    \caption{Embeddings steered using the proposed method show \textbf{higher OOD generalisation performance}.}
    \label{fig:cos_ood}
\end{figure}





% We consider different types of concepts, starting with binary ones such as language and gender. We first create datasets where a single binary concept varies between $\x$ and $\tilde \x$. For language, we consider this variation to be from \textit{eng} $\rightarrow$ \textit{french}, and to be \textit{masc} $\rightarrow$ \textit{fem} for gender. The resulting datasets are \textsc{lang}$(1, 1)$ and \textsc{gender}$(1, 1)$. Next, we generate datasets with multiple binary concepts varying between $\x$ and $\tilde \x$  while satisfying \cref{ass:suffsupp}. For this we create two datasets: \textsc{binary}$(2, 2)$ with both the language and gender potentially varying across samples with the possible variations as \textit{eng} $\rightarrow$ \textit{french}, \textit{french} $\rightarrow$ \textit{eng}, \textit{masc} $\rightarrow$ \textit{fem}, and \textit{fem} $\rightarrow$ \textit{masc}, and \textsc{correlated}$(2, 1)$ with two binary concepts varying such that they are correlated with each other. For this, we consider the variations to be \textit{eng} $\rightarrow$ \textit{french} and \textit{eng} $\rightarrow$ \textit{german}. Next, we consider categorical \textit{concepts}. For this, we generate simple word phrases for $\x$ and $\tilde \x$. Each word phrase consists of a colour, a shape, and an object, for eg: ``purple toroidal shoe". Each of these three \textit{categorical attributes} can have ten different potential values. 
% As per the limited literature on the linear representation hypothesis applied to categorical concepts \citep{park2024geometrycategoricalhierarchicalconcepts, reber2024rate}, we represent each of the three categorical attributes in terms of binary contrasts such that a shift from one value of the categorical attribute to another is one concept. Thus, for a categorical attribute with $k$, we obtain $k \choose 2$ concepts. With $10$ values for each of the $3$ attributes, there are $3 \times {10 \choose 2} = 135$ varying concepts in the dataset, which we denote by \textsc{categorical}$(135, 3)$. All datasets are summarised in \cref{tab:datasets}.
    

% pairing each question with a wrong answer that mimics human falsehoods v/s the . 

% We choose the multiple-choice track of TruthfulQA, which consists of a question followed by multiple possible answers, of which only one is true and the others only appear to be true, reflecting human biases. We create $(\x, \tilde \x)$ pairs by assigning $\x$ to be the question paired with a wrong answer, and $\tilde \x$ to be a question paired with the correct answer to capture the variation of the concept truthfulness from false to true.

\begin{table}
    \centering
    \renewcommand{\arraystretch}{1.25}
    \caption{The mean \textbf{MCC} of the decoder of \isae is \textbf{robust to further entanglement} across all datasets considering observations $(\rmL f(\x), \rmL f(\tilde \x))$.}
    \label{tab:mcc_ent}
    \footnotesize{
    \begin{tabular}{c  c  c}
    \toprule
         & \textbf{\isae} & \textbf{\aff} \\
         \midrule
        \textsc{lang}($1, 1$) & $0.9904 \pm 0.0001$ & $ 0.8765 \pm 0.0068$ \\ \hdashline
        \textsc{gender}($1, 1$)    & $0.9912 \pm 0.0001$ & $0.8835 \pm 0.0051$ \\ \hdashline
        \textsc{binary}($2, 2$) & $0.9900 \pm 0.0005$ & $0.7963 \pm 0.0003$ \\ \hdashline
        \textsc{corr}($2, 1$) &
        $0.9900 \pm 0.0010$ & $0.6296 \pm 0.0095$ \\ \hdashline
        \textsc{cat}($135, 3$) & $0.9001 \pm 0.0095$ & $0.5463 \pm 0.0175$ \\ 
        \midrule
        TruthfulQA    & $0.9316 \pm 0.0079$ & $0.7511 \pm 0.0115$ \\ 
        \bottomrule
    \end{tabular}
    }
\end{table}

\paragraph{How well does \isae identify steering vectors?} As shown in \cref{tab:mcc_lang}, \isae achieves consistently high MCC values, empirically corroborating \cref{prop:perm_ident}. On simpler datasets such as \textsc{lang}($1, 1$) and \textsc{gender}($1, 1$), \isae and the affine (\aff) baseline perform comparably, suggesting that these concepts are relatively easy to decode from the LLM’s final layer, as compared to the concept of truthfulness (in the case of TruthfulQA). Further, when the number of concepts or the correlation between them increases (e.g., \textsc{corr}($2, 1$) and especially \textsc{cat}($135, 3$)), \isae shows a clearer advantage. In contrast, the \aff baseline’s MCC scores either sharply decline or exhibit high variance (as in the case of \textsc{corr}($2, 1$)). We interpret these findings as evidence that simpler datasets do not fully stress‐test identifiability. Consequently, we present robustness experiments---further entangling the embeddings and deliberately misspecifying the model (see \cref{apx:v})---to determine whether \isae exhibits benefits under more demanding conditions.

\paragraph{How robust is \isae to increased entanglement?} \cref{tab:mcc_ent} demonstrates that, once we apply a dense linear invertible transformation $\rmL$ to the embeddings to generate $(\rmL f(\x), \rmL f(\tilde \x))$, the gap between \isae and the affine baseline widens. This larger gap indicates that further entangling the difference vectors makes it more challenging for the baseline to identify meaningful steering directions, whereas \isae remains robust. The worsening performance of \aff after the entanglement is applied suggests that LLM representations might already somewhat disentangle some concepts or encode them through sparse or simple transformations, an idea that requires deeper investigation.
%
% A plausible explanation for this is that the transformation $f \circ g$ encoding concepts in representation space might have a sparse or simple structure, effectively preserving some underlying concepts that \aff can still disentangle to some extent. This suggests further evidence that language embeddings may already exhibit partial disentanglement for certain target concepts, necessitating a deeper investigation into the datasets typically considered for steering problems and the structure of embedding spaces, both of which are beyond the scope of this work.
%
Although \isae generally outperforms \aff across several datasets, drawing conclusions based solely on MCC might be premature, especially when considering their ultimate deployment to the problem of steering, which we evaluate next.

\paragraph{Does identifiable steering translate to \textit{better} steering?} \cref{fig:cos} illustrates how identifying steering vectors benefits the downstream task of steering embeddings, while \cref{fig:cos_ood} shows that the identified steering vectors also transfer effectively to out-of-distribution datasets---provided that the concept in question is present. Specifically, for the experiments in \cref{fig:cos}, we consider a held-out test set of pairs $(\x, \tilde \x_k) \forall k \in V$, each varying by a single concept, and compare the cosine similarity between the steered embeddings and target embeddings. By design, the dataset \textsc{lang}($1, 1$) consists of examples of a language change from \textit{eng} $\rightarrow$ \textit{french} for words stemming from another general concept, \textit{household objects}, while the samples in \textsc{binary}($2, 2$) which have the same language change correspond to samples of \textit{professions}. Similarly, the dataset \textsc{corr}($2, 1$) consists of  \textit{eng} $\rightarrow$ \textit{french} and \textit{eng} $\rightarrow$ \textit{german} changes corresponding to words describing \textit{professions}. Please refer to \cref{apx:data} for details. The hypothesis here is that the steering vector for \textit{eng} $\rightarrow$ \textit{french} from an identifiable model should be able to generalise between datasets with samples varying along concepts in the same subset $S \subseteq V$ but for which the set of non-varying concepts $\bar V$ varies. \cref{fig:cos} also reveals that even slight differences in MCC can translate into pronounced variations in both in-distribution and OOD steering accuracy, indicating that solely evaluating identifiable representation learning methods on MCC is not sufficient. 

% damn! discovered typo in plot.


% \textbf{Limitations}. 
% % These experiments are intended as an examination of identifiability guarantees and their benefits on real-world like problems, which is lacking in causal representation learning \citep{gamella2024causalchambersrealphysical, cadei2024smoke, gondal2019transfer}, where the focus is on purely synthetic datasets. 
% These experiments are intended to validate the identifiability results in \cref{sec:id} and their implications for accurate steering. Although we include real-world data (TruthfulQA), to fully understand the impacts of the \isae on steering research, especially LLM alignment, more evaluation is needed on embeddings from more complex datasets, on more challenging tasks (e.g., MTEB \citep{muennighoff2023mtebmassivetextembedding}), and by generating text with steered embeddings to assess model behavior. Further large-scale evaluations are an promising avenue for future work.
% Such an endeavour exceeds current scope, which focuses on offering a practical exposition of the proposed theory.

\section{Related work}
\label{sec:rel}
\textbf{Linear representation hypothesis}. This paper builds on the linear representation hypothesis that language models encode concepts linearly. Several papers provide empirical evidence for this hypothesis \citep{mikolov-etal-2013-linguistic, gittens-etal-2017-skip, ethayarajh2019understandinglinearwordanalogies, allen2019analogiesexplainedunderstandingword, seonwoo-etal-2019-additive, burns2024discoveringlatentknowledgelanguage, li2024inferencetimeinterventionelicitingtruthful, moschella2023relativerepresentationsenablezeroshot, tigges2023linearrepresentationssentimentlarge, nanda2023emergentlinearrepresentationsworld, nissim-etal-2020-fair, ravfogel-etal-2020-null,park2023linear}. Recent work also provides theoretical justification for why linear properties might consistently emerge across models that perform next-token prediction \citep{roeder2021linear, jiang2024originslinearrepresentationslarge, marconato2024all,park2024geometrycategoricalhierarchicalconcepts}.  

\textbf{Interpretability of LLMs}. This paper contributes to the literature on interpretability and steering of LLMs. Much of the work on finding concepts in LLM representations for steering relies on supervision, either from paired observations with a single-concept shift \citep{panickssery2024steeringllama2contrastive,turner2024steeringlanguagemodelsactivation,rimsky2024steering,li2024inferencetimeinterventionelicitingtruthful} or from examples of target LLM completions to prompts \citep{subramani2022extracting}. This prior work also focuses on applying the same steering vector to all examples, implicitly relying on the linear representation hypothesis as justification. In contrast, we make the assumption precise, and show how it leads to steering vectors. This paper also departs from supervised learning and focuses on learning with limited supervision. In this way, we propose a method that is similar to sparse autoencoders (SAEs) \citep{templeton2024scaling, engels2024languagemodelfeatureslinear, cunningham2023sparseautoencodershighlyinterpretable, rajamanoharan2024improvingdictionarylearninggated, gao2024scaling}. In contrast, our proposed method fits concept shifts, and provably identifies steering vectors while SAEs do not enjoy identifiability guarantees.

\textbf{Causal representation learning}. Finally, this paper builds on causal representation learning results that leverage sparsity constraints. \citet{ahuja2022weakly}, \citet{locatello2020weakly}, and \citet{brehmer2022weaklysupervisedcausalrepresentation} consider sparse latent perturbations and paired observations. In contrast, we focus on learning from multi-concept shifts. \citet{lachapelle2022disentanglement} focus on sparse interventions and sparse transitions in temporal settings, while \citet{lachapelle2023synergies}, \citet{layne2024sparsityregularizationtreestructuredenvironments}, \citet{xu2024sparsityprinciplepartiallyobservable}, and \citet{fumero2023leveragingsparsesharedfeature} leverage sparse dependencies between latents and tasks. In this paper, we adapt these assumptions and technical results for a novel setting: discovering steering vectors from LLM representations based on concept shift data. Although they do not leverage a sparsity constraint, the setting in \citet{rajendran2024learning} is the most similar to ours, aiming to recover linear subspaces that capture concepts from observations generated by a nonlinear mixing function. They leverage concept-conditional datasets to prove that these concept subspaces are identified up to linear transformations. In contrast, we focus on concept shifts, and propose a regularized objective that recovers them up to permutation and scaling, allowing for straightforward steering vector discovery. 

% In \cref{apx:review}, we further illustrate the comparisons between this paper and related work.

% The objective of interpreting and controlling for specific latent concepts in an LLM's representation space is
% tightly related to the task of learning disentangled representations \citep{bengio2009learning, bengio2013representation, scholkopf2021toward}, ie, separating out different concepts encoded in these representations. In this work, we extend prior research on learning disentangled representations by examining conditions under which controlling for distinct concepts is feasible, thus unifying these separate research directions. Refer to \cref{apx:review} for a more detailed comparison of related work.

% \textbf{Linearly encoded concepts.} The hypothesis that representations learnt by language models encode concepts linearly---the \textit{linear representation hypothesis}---has been empirically well studied in several papers \citep{mikolov-etal-2013-linguistic, gittens-etal-2017-skip, ethayarajh2019understandinglinearwordanalogies, allen2019analogiesexplainedunderstandingword, seonwoo-etal-2019-additive, burns2024discoveringlatentknowledgelanguage, li2024inferencetimeinterventionelicitingtruthful, moschella2023relativerepresentationsenablezeroshot, tigges2023linearrepresentationssentimentlarge, nanda2023emergentlinearrepresentationsworld, nissim-etal-2020-fair, ravfogel-etal-2020-null}, and a few papers also provide theoretical justification for it \citep{roeder2021linear, jiang2024originslinearrepresentationslarge, marconato2024all}. This work takes a different approach to instead adopt this hypothesis to provably discover concept vectors.  

% \textbf{Interpretability and control of LLMs}. Several recent works leverage the linear representation hypothesis to discover concepts from the latent space of LLMs \citep{templeton2024scaling, rajendran2024learning, engels2024languagemodelfeatureslinear, cunningham2023sparseautoencodershighlyinterpretable, rajamanoharan2024improvingdictionarylearninggated, gao2024scaling} using sparse autoencoders (SAEs) for interpretability research. Other line of work focuses on the additivity of linearly encoded concepts to steer language model representations \citep{panickssery2024steeringllama2contrastive, li2024inferencetimeinterventionelicitingtruthful} when a single known binary concept varies throughout the dataset. \citet{rajendran2024learning} provide a first theoretical framework to discover a set of  varying concepts up to permutation and scaling from data where only a single non-binary concept is perturbed at a time. In contrast, this paper provides the same theoretical guarantee to discover concept vectors from data where potentially multiple non-binary concepts can be perturbed at the same time and we demonstrate their effectiveness in manipulating the representation space of LLMs.

% \textbf{Disentangled representation learning}. There are two lines of work within research on disentanglement most closely related to this paper---exploiting sparsity \citep{ahuja2022weakly, lachapelle2022disentanglement, lippe2022citriscausalidentifiabilitytemporal, lachapelle2023synergies, fumero2023leveragingsparsesharedfeature, layne2024sparsityregularizationtreestructuredenvironments, xu2024sparsityprinciplepartiallyobservable}, or using paired samples \citep{ahuja2022weakly, locatello2020weakly, brehmer2022weaklysupervisedcausalrepresentation}. Most relevant to our work are the frameworks of \citet{lachapelle2022disentanglement, lachapelle2023synergies, xu2024sparsityprinciplepartiallyobservable}  which leverage sparse shifts to latent variables to guarantee the identifiability of disentangled solutions. Specifically, we use paired observations and adapt their identifiability result showing disentanglement up to permutation and scaling when not all latent variables are present for every observation for linear mixing functions and unpaired observations.


\section{Conclusion}
We propose Sparse Shift Autoencoders ({\isae}s) for discovering accurate steering vectors from multi-concept paired observations as an alternative to both SAEs and approaches relying on supervised data. Key to this result are the identifiability guarantees that the \isae enjoys as a consequence of considering sparse concept shifts. We study the \isae empirically, using Llama3.1 embeddings on several real language tasks, and find evidence that the method facilitates accurate steering learned via limited supervision. However, we stress that these experiments are intended to validate the identifiability results in \cref{sec:method_and_analysis} and their implications for accurate steering. Although we include real-world data (TruthfulQA), to fully understand the impacts of the \isae on steering research, especially LLM alignment, more evaluation is needed on embeddings from more complex datasets, on more challenging tasks (e.g., MTEB \citep{muennighoff2023mtebmassivetextembedding}), and by generating text with steered embeddings to assess model behavior. Rigorous large-scale evaluations are a promising avenue for future work. Such evaluations would benefit from more expansive real-world benchmarks and a more nuanced approach to categorical concepts---one that moves beyond reducing them to binary contrasts.

% Further, by facilitating steering even in the absence of explicit concept labels, it has the potential to enable , highlighting the need for more  and the need to   and has the potential to be applicable to real-world datasets such as social media posts and human-LLM dialogue: 


% marking a departure from the standard supervised data used to learn steering vectors, which is costly to obtain. 

% Our work opens a path to steering even in the absence of explicit concept labels and has the potential to be applicable to real-world datasets such as social media posts and human-LLM dialogue: current approaches not only rely on concept labels, but pairs of observations varying along a single label, thus restricting the benchmarks considered in practice, and the set of concepts considered under the linear representation hypothesis (see \cref{apx:concept}), limiting the applicability of our method. Application to such datasets also necessitates more rigorous evaluation, necessitating model generation to test controllability of model behaviour through steering. Pushing this idea further, it also presents an interesting venue to stress-test the effect of steering: is it possible to learn steering vectors from in-context prompts such that the model behaviour can be steered to override pre-trained behaviour? 

% stay tuned for our upcoming work to find out hahahahaha

\section*{Acknowledgements}

We thank Devon Hjelm, Nino Scherrer, Arkil Patel, Sahil Verma, Arihant Jain, and Divyat Mahajan for helpful discussions. 
DS acknowledges support from NSERC Discovery Grant RGPIN-2023-04869, and a Canada-CIFAR AI Chair.



\begin{comment}
    % Acknowledgements should only appear in the accepted version.
% \section*{Acknowledgements}

% \textbf{Do not} include acknowledgements in the initial version of
% the paper submitted for blind review.

% If a paper is accepted, the final camera-ready version can (and
% usually should) include acknowledgements.  Such acknowledgements
% should be placed at the end of the section, in an unnumbered section
% that does not count towards the paper page limit. Typically, this will 
% include thanks to reviewers who gave useful comments, to colleagues 
% who contributed to the ideas, and to funding agencies and corporate 
% sponsors that provided financial support.

\end{comment}

\section*{Impact Statement}
This paper presents technical advancements to a new field of machine learning focused on steering the behaviour of large language models at inference time, i.e., without requiring access to the model's parameters. Steering methods have already begun to play a role in the alignment of LLMs to be e.g., more truthful. We present a new method that could speed up steering research by allowing practitioners to recover steering vectors without the need for supervision, a previous limitation of steering methods. As such, this work could have a positive impact on LLM safety and alignment research. Nevertheless, we flag that contributions towards steering such as ours should be empirically evaluated carefully to avoid over-claiming LLM safety. We acknowledge that while the empirical studies we conduct demonstrate the advantages of identifiable methods such as \isae for steering, further evaluation is necessary to the method's use in AI safety research.

% Authors are \textbf{required} to include a statement of the potential 
% broader impact of their work, including its ethical aspects and future 
% societal consequences. This statement should be in an unnumbered 
% section at the end of the paper (co-located with Acknowledgements -- 
% the two may appear in either order, but both must be before References), 
% and does not count toward the paper page limit. In many cases, where 
% the ethical impacts and expected societal implications are those that 
% are well established when advancing the field of Machine Learning, 
% substantial discussion is not required, and a simple statement such 
% as the following will suffice:

% ``This paper presents work whose goal is to advance the field of 
% Machine Learning. There are many potential societal consequences 
% of our work, none which we feel must be specifically highlighted here.''

% The above statement can be used verbatim in such cases, but we 
% encourage authors to think about whether there is content which does 
% warrant further discussion, as this statement will be apparent if the 
% paper is later flagged for ethics review.


% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
% \nocite{langley00}

\bibliography{icml_main}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn

\epigraph{..we understand the world by studying change, not by studying things..}{As quoted in the Order of Time, Anaximander}
\tableofcontents

\clearpage

\section{Theory}
\subsection{Notation and Glossary}
\label{apx:gloss}
\vspace{-0.1cm}
\centerline{\bf General notation}
\vspace{-0.2cm}
\centerline{\rule{0.95\linewidth}{0.5pt}}
\bgroup
\def\arraystretch{1.5}
%\vspace{0.2cm}
%\centerline{\textit{}}
\begin{tabular}{>{\centering\arraybackslash}p{3.25in} >{\arraybackslash}p{3.15in}}

$\displaystyle k$ & integer\\
$\displaystyle [k]$ & set of all integers between $1$ and $k$, inclusively\\
$\displaystyle S \subseteq [k]$ & set\\
$\displaystyle |S|$ & cardinality of a set\\
$\displaystyle S \backslash S'$ & set subtraction (set of elements of $S$ that are not in $S'$)\\ 
$\displaystyle \lambda$ & scalar\\ 
%$\displaystyle \mathcal{S}$ & domain set\\
$\displaystyle \x$ & vector and vector-valued random variables\\
$\displaystyle x_k$ & element $k$ of a random vector $\x$ \\
$\displaystyle \x_S$ & subvector with element $x_i$ for $i \in S$\\
$\displaystyle \A$ & matrix\\
$\displaystyle \A_{i,j}$ & element $i, j$ of matrix $\A$ \\
$\displaystyle \A_{:, i}$ & column $i$ of matrix $\A$ \\
$\displaystyle \A_{S}$ & matrix with columns $\A_{:,j}$ for $j \in S$\\
$\displaystyle \A^+$ & pseudo-inverse of a matrix $\A$ \\
$\displaystyle \rve_k \in \mathbb{R}^n$ & standard basis vector of the form $[0,\dots,0,1,0,\dots,0]$ with a 1 at position $k$\\ 
$\displaystyle f: \X \rightarrow \Z$ & function $f$ with domain $\X$ and codomain $\Z$\\
$\displaystyle f \circ g $ & composition of the functions $f$ and $g$ \\
$\displaystyle || \x ||_p $ & $\ell_p$ norm of $\x$ \\ [2ex]
%$\displaystyle \{k\}$ & the set with the single element $k$ \\

%$\displaystyle \mathbb{R}$ & set of real numbers\\


% \egroup
% \vspace{0.5cm}
% \bgroup
% \def\arraystretch{1.5}
%\end{tabular}
% \egroup
% \vspace{0.5cm}
% \bgroup
% \def\arraystretch{1.5}
%\vspace{0.5cm}
%\centerline{\textit{Calculus}}
%\begin{tabular}{>{\centering\arraybackslash}p{3.25in} >{\arraybackslash}p{3.15in}}
% NOTE: the [2ex] on the next line adds extra height to that row of the table.
% Without that command, the fraction on the first line is too tall and collides
% with the fraction on the second line.
% $\displaystyle\frac{d y} {d x}$ & Derivative of $y$ with respect to $x$\\ [2ex]
$\displaystyle \frac{\partial y} {\partial x} $ & partial derivative of $y$ with respect to $x$ \\ [2ex]
$\displaystyle \nabla_\vx f(\vx) \in \R^{m\times n}$ & Jacobian matrix of $f: \R^n \rightarrow \R^m$\\ [2ex]
$\displaystyle \nabla_\vx^2 f(\vx) \in \R^{n\times n}$ & Hessian matrix of $f: \R^n \rightarrow \R$\\
%\end{tabular}
%\vspace{0.25cm}

%\centerline{\textit{Probability}}
%\begin{tabular}{>{\centering\arraybackslash}p{3.25in} >{\arraybackslash}p{3.15in}}
$\displaystyle \mathbb{P}$ & probability measure/distribution \\
%$\displaystyle p(S)$ & probability distribution over a continuous variable, or over a variable whose type has not been specified\\
$\displaystyle  \E_{\x} [ f(\x) ]$ & expectation of $f(\x)$ with respect to $\x$ \\
\end{tabular}
% \egroup
\vspace{0.25cm}

%\centerline{ \textit{Functions}}
% \bgroup
% \def\arraystretch{1.5}
%\begin{tabular}{>{\centering\arraybackslash}p{3.25in} >{\arraybackslash}p{3.15in}}

%\end{tabular}
\egroup
\newpage
\centerline{\bf Glossary}
\vspace{-0.2cm}
\centerline{\rule{0.75\linewidth}{0.5pt}}
\bgroup
\def\arraystretch{1.5}
\begin{tabular}{>{\centering\arraybackslash}p{3.25in} >{\arraybackslash}p{3.15in}}
$\displaystyle \x \in \R^{d_x}$ & observation \\
$\displaystyle \z \in \R^{d_z}$ & pretrained representation\\
$\displaystyle \c \in \R^{d_c}$ & ground-truth concept vector \\
%$\displaystyle k$ & concept index\\
%$\displaystyle \lambda$ & strength of concept variation\\
$\displaystyle \tilde \c_{k, \lambda}$ & ground-truth concept vector after varying concept $k$ by $\lambda$ from $\c$\\
$\displaystyle \tilde \x_{k, \lambda}$ & observation corresponding to  $\tilde \c_{k, \lambda}$\\
$\displaystyle \tilde \z_{k, \lambda}$ & pretrained representation corresponding to  $\tilde \c_{k, \lambda}$\\
%$\displaystyle d_x$ & dimension of observations \\
%$\displaystyle d_z$ & dimension of learned representations\\
%$\displaystyle d_c$ & dimension of concept representations\\
$\displaystyle \X \subseteq \mathbb{R}^{d_x}$ & support of observations\\
$\displaystyle \Z \subseteq \mathbb{R}^{d_z}$ & support of pretrained representations\\
$\displaystyle \C \subseteq \mathbb{R}^{d_c}$ & support of ground-truth concept vectors\\
$\displaystyle S \subseteq [d_c]$ & subset of varying concepts in a given pair $(\x,\tilde\x)$ \\
$\displaystyle V \subseteq [d_c]$ & subset of concepts allowed to vary between $\x$ and $\tilde \x$ \\
%$\displaystyle |S|$ & number of concepts varying within a pair\\
%$\displaystyle |V|$ & number of concepts varying across all pairs\\
%$\displaystyle [d_c] \backslash V$ & concepts in $[d_c]$ that are not in $V$\\
$\deltac$ & concept shift vector\\
$\hatdeltac$ & estimated concept shift vector  \\
$\deltaz$ & pretrained representation shift vector  \\
%$\deltac_V$ & concept shift vector with shifts in concepts in $V$  \\
% $\displaystyle \pi(k)$ & permutation of $k \in \sigma_{|V|}$, the set of all permutations of varying concept indices \\
$\displaystyle g : \C \rightarrow \X$ & map from concept representations to observations\\
$\displaystyle f : \X \rightarrow \Z$ & map from observations to learned representations \\
$\displaystyle r : \Z \rightarrow \C$ & encoding function \\
$\displaystyle \hat{r} : \C \rightarrow \Z$ & estimated encoding function \\
$\displaystyle q : \C \rightarrow \Z$ & decoding function\\
$\displaystyle \hat{q} : \C \rightarrow \Z$ & estimated decoding function \\
$\displaystyle \phi_{k, \lambda} : \Z \rightarrow \Z$ & steering function \\
$\displaystyle \hat{\phi}_{k, \lambda} : \Z \rightarrow \Z$ & estimated steering function \\
$\displaystyle \A$ & linear map between concept representations and learnt representations\\
%$\displaystyle \A_V$ & subset of the linear map between concept representations for concepts in $V$ and their learnt representations \\




\end{tabular}

% \subsubsection{Justification for injectivity of the encoding and decoding functions}
% \label{apx:inj}

% $\mathbf{r} : \Z \rightarrow \C$ maps $\delta \z \in \mathbb{R}^{d_Z}$
% vectors to $\s_{\delta C} \in \mathbb{R}^K$ vectors where $K \leq d_Z$. $q$ maps $\s_{\delta C}$ back to $\delta \z$. To establish the injectivity of these transformations, we need to ensure that $\br \coloneqq (\mathbf{T}_1, \mathbf{k}_1)$ and $\q \coloneqq (\mathbf{T}_2, \mathbf{k}_2)$ are such that $\mathbf{T}_2 = \mathbf{T}_1^T$ and $\mathbf{T}_2$ has orthonormal columns which means $\mathbf{T}_2\mathbf{T}_2^T = \mathbf{I}$.

% To analyse the injectivity of a linear transform, we use the Rank-Nullity theorem which states that for a transformation $\mathbf{T}  : V \rightarrow W, \text{dim}(V) = \text{rank}(\mathbf{T}) + \text{nullity}(\mathbf{T})$.

% $$\mathbf{T} \text{ is injective } \iff \text{ker}(\mathbf{T}) = \{0\} \text{ or } \text{nullity}(\mathbf{T}) = 0$$

% (1) $\text{dim}(V) = K$ and we consider $\mathbf{T}_2$ to have orthonormal columns, which means they are linearly independent and $\mathbf{T}_2$ is a full rank matrix $\implies  \text{nullity}(\mathbf{T}_2) = 0$ and $\mathbf{q}$ is injective.

% (2) The injectivity of $\mathbf{T}_1$ is less straightforward. We know that  $\mathbf{T}_1$ is full rank since  $\mathbf{T}_1 = \mathbf{T}_2^T$ and $\mathbf{T}_2^T$ is full rank. We also have $\mathbf{T}_2\mathbf{T}_2^T = \mathbf{I} \implies \mathbf{T}_2 \circ \mathbf{T}_1 = \mathbf{I}$. Consider two vectors $\delta \z$ and $\delta \z'$ such that $\mathbf{T}_1(\delta \z) = \mathbf{T}_1(\delta \z')$. Applying $\mathbf{T}_2$ to both sides, $\mathbf{T}_2(\mathbf{T}_1(\delta \z)) = \mathbf{T}_2(\mathbf{T}_1(\delta \z')) \implies \delta \z = \delta \z'$. Thus, $\mathbf{T}_1$ or $\br$ is injective.

% \subsection{Injectivity of $\A_V$}

% \label{apx:inj}

\subsection{Proof of \cref{prop:linear_ident} (linear identifiability)}
\label{app:linear_ident}

\linearIdent*

\begin{proof}
    We note that the solution $q^* \coloneqq \A_V$ and $r^* \coloneqq \A_V^+$ minimizes the loss since
    \begin{align}
        \mathbb{E}_{\x, \tilde\x}||\deltaz - q^*(r^*(\deltaz))||^2_2 &= \mathbb{E}_{\x, \tilde\x}||\deltaz - \A_V\A_V^{+}\deltaz||^2_2 \\
        &=\mathbb{E}_{\c, \tilde\c}||\A_V \deltac_V - \A_V(\A_V^{+}\A_V) \deltac_V||^2_2 \\
        &=\mathbb{E}_{\c, \tilde\c}||\A_V \deltac_V - \A_V \deltac_V||^2_2 \\
        &= 0 \,,
    \end{align}
    where we used the fact that $\A_V$ is injective and thus $\A_V^+\A_V = \mathbf{I}$. This means all optimal solutions must reach zero loss.

    Now consider an arbitrary minimizer $(\hat r, \hat q)$. Since it is a minimizer, it must reach zero loss, i.e. 
    \begin{align}
        &\mathbb{E}_{\x, \tilde\x}||\deltaz - \hat q(\hat r(\deltaz))||^2_2 = 0 \\
        &\mathbb{E}_{\c, \tilde\c}||\A_V \deltac_V - \hat q(\hat r(\A_V\deltac_V))||^2_2 = 0
    \end{align}    
    This means we must have 
    \begin{align}\label{eq:988r9e0w}
        \A_V \deltac_V = \hat q(\hat r(\A_V\deltac_V)),\ \text{almost everywhere w.r.t. $p(\deltac_V)$.}
    \end{align}
     Because all functions both on the left and the right hand side are continuous, the equality must hold on the support of $p(\deltac_V)$, which we denote by $\Delta^c_V$. Moreover, since $\hat r$ and $\hat q$ are linear, they can be represented as matrices, namely $\mathbf{R} \in \sR^{|V| \times d_z}$ and $\mathbf{Q} \in \sR^{d_z \times |V|}$. We can thus rewrite \cref{eq:988r9e0w} as 
    \begin{align}\label{eq:98438383}
        \A_V \deltac_V = \mathbf{Q}\mathbf{R}\A_V\deltac_V\,,
    \end{align}
    which holds for all $\deltac_V \in \Delta^c_V$. By \cref{ass:suff_var}, we know there exists a set of $|V|$ linearly independent vectors in $\Delta^c_V$. Construct a matrix $\mathbf{C} \in \sR^{|V|\times|V|}$ whose columns are these linearly independent vectors. Note that $\mathbf{C}$ is invertible, by construction. 
    
    Since this \cref{eq:98438383} holds for all $\deltac_V \in \Delta^c_V$, we can write
    \begin{align}
        \A_V \mathbf{C} = \mathbf{Q}\mathbf{R}\A_V\mathbf{C}\\
        \A_V = \mathbf{Q}\mathbf{R}\A_V \,,
    \end{align}
    where we right-multiplied by $\mathbf{C}^{-1}$ on both sides. Since $\A_V$ is injective (\cref{ass:injective_Av}), we must have that $\mathbf{R}\A_V$ is injective as well. But since $\mathbf{R}\A_V$ is a square matrix, injectivity implies invertibility. Let us define $\mathbf{L} \coloneqq (\mathbf{R}\A_V)^{-1}$. We thus have
    \begin{align}
        \A_V &= \mathbf{Q}\mathbf{L}^{-1} \\
        \hat q = \mathbf{Q} &= \A_V\mathbf{L} \,,
    \end{align}
    which proves the first part of the statement.

    Now, we show that, for all $\z \in \text{Im}(\A_V)$, $\mathbf{R}\z = \mathbf{L}\A_V^+\z$. Take some $\z \in \text{Im}(\A_V)$. Because this point is in the image of $\A_V$, there must exists a point $\c \in \sR^{|V|}$ such that $\z = \A_V\c$. Now we evaluate
    \begin{align}
        \hat r(\z) = \mathbf{R}\z &= \mathbf{R}\A_V\c  \\
        &= \mathbf{L}^{-1}\c \label{eq:kksdm2949sk}\\ 
        &= \mathbf{L}^{-1}\A_V^+\A_V\c \label{eq:jdjdjdjnajajm}\\
        &= \mathbf{L^{-1}}\A_V^+\z \,, 
    \end{align}
    where we used the fact $\mathbf{R}\A_V = \mathbf{L}^{-1}$ in \cref{eq:kksdm2949sk} and the fact that $\A_V^+\A_V = \mathbf{I}$ in \cref{eq:jdjdjdjnajajm}. This concludes the proof.
\end{proof}


\subsection{Proof of \cref{prop:perm_ident} (permutation identifiability)}
\label{app:proof}
The proof is heavily based on \citet{lachapelle2023synergies} and \citet{xu2024sparsityprinciplepartiallyobservable}.

\permIdent*

\begin{proof}
    Recall that, in the proof of \cref{prop:linear_ident}, we showed that the solution $q^* \coloneqq \A_V$ and $r^* \coloneqq \A_V^+$ yields zero reconstruction loss, i.e.,
    \begin{align}
        \mathbb{E}_{\x, \tilde\x}||\deltaz - q^*(r^*(\deltaz))||^2_2 &= 0\,.
    \end{align}
    It turns out, this solution also satisfies the constraint $\mathbb{E}||r(\deltaz)||_0 \leq \beta \coloneqq \mathbf{E}||\deltac_V||_0$ since
    \begin{align}
        \mathbb{E}||r^*(\deltaz)||_0 = \mathbb{E}||\A_V^+(\A_V\deltac_V)||_0 = \mathbb{E}||\deltac_V||_0 = \beta \,,  
    \end{align}
    where we used the fact that $\deltaz = \A_V\deltac_V$ and $\A_V^+\A_V = \mathbf{I}$, since $\A_V$ is injective. This means that all optimal solutions to the constrained problem of \cref{eqn:recon,eqn:sparse_constraint} with $\beta \coloneqq \mathbb{E}||\deltac_V||_0$ must reach zero reconstruction loss.

    Let $(\hat r, \hat q)$ be an arbitrary solution to the constrained problem. By the above argument, this solution must reach zero loss. Thus, by the exact same argument as in \cref{prop:linear_ident}, there must exist an invertible matrix $\mathbf{L} \in \sR^{|V|\times |V|}$ such that 
    \begin{align}
        \hat q \coloneqq \A_V \mathbf{L} \quad \text{and} \quad \hat r(\z) \coloneqq \mathbf{L}^{-1}\A_V^+\z,\ \text{for all}\ \z\in\text{Im}(\A_V) \,.
    \end{align}
    Since $\hat r$ is optimal it must satisfy the constraint, which we rewrite as
    \begin{align}
        \mathbb{E}||\hat r (\deltaz)||_0 &\leq \mathbb{E}||\deltac_V||_0 \nonumber\\
        \mathbb{E}||\hat r (\A_V\deltac_V)||_0 &\leq \mathbb{E}||\deltac_V||_0 \nonumber\\
        \mathbb{E}||\mathbf{L}^{-1}\A_V^+(\A_V\deltac_V)||_0 &\leq \mathbb{E}||\deltac_V||_0 \nonumber\\
        \mathbb{E}||\mathbf{L}^{-1}\deltac_V||_0 &\leq \mathbb{E}||\deltac_V||_0\,, \label{eqn:core}
    \end{align}
    where we used the fact that $\hat r$ restricted to the image of $\A_V$ is equal to $\mathbf{L}^{-1}\A_V^+$ when going from the second to the third line.
    
    At this stage, we can use the same argument as \citet{lachapelle2023synergies} to conclude that $\mathbf{L}$ is a permutation-scaling matrix. For completeness, we present that result into \cref{lemma:synergies_proof} and its proof below. One can directly apply this lemma, thanks to \cref{ass:suffsupp} and the fact that sets of the form $\{\deltac_S \in \sR^{|V|} \mid \mathbf{a}^\top\deltac_S = 0\}$ with $\mathbf{a} \not = 0$ are proper linear subspaces of $\sR^{|V|}$ and thus have zero Lebesgue measure, and thus 
    $$\mathbb{P}_{\deltac_S \mid S}\{\deltac_S \in \sR^{|V|} \mid \mathbf{a}^\top\deltac_S = 0\} = 0 \,.$$
    This concludes the proof.
\end{proof}

The proof of the following lemma is taken directly from \citet{lachapelle2023synergies} (modulo minor changes in notation). The original work used this argument inside a longer proof and did not encapsulate this result into a modular lemma. We thus believe it is useful to restate the result here as a lemma containing only the piece of the argument we need. We also include the proof of \citet{lachapelle2023synergies} for completeness. Note that \citet{xu2024sparsityprinciplepartiallyobservable} also reused this result to prove identifiability up to permutation and scaling.

\begin{lemma}[\citet{lachapelle2023synergies}]\label{lemma:synergies_proof}
Let $\mathbf{L} \in \sR^{m\times m}$ be an invertible matrix and let $\x$ be an $m$-dimensional random vector following some distribution $\mathbb{P}_\x$. Define the set $S \coloneqq \{j \in [m] \mid \x_j \not= 0\}$, which is random (because $\x$ is random) with probability mass function given by $p(S)$. Let $\mathcal{S} \coloneqq \{S \subseteq [m] \mid p(S) > 0\}$, i.e. it is the support of $p(S)$. Assume that
\begin{enumerate}
    \item For all $j \in [m]$, we have $\bigcup_{S \in \mathcal{S} | j \notin S} S = [m] \setminus \{j \}$; and
    \item For all $S \in \mathcal{S}$, the conditional distribution $\mathbb{P}_{\x_S \mid S}$ is such that, for all nonzero $\mathbf{a} \in \sR^{|S|}$, $\mathbb{P}_{\x_S \mid S}\{\x_S \mid \mathbf{a}^\top \x_S = 0\} = 0$.
\end{enumerate}
Under these assumptions, if $\mathbb{E}||\mathbf{L}\x||_0 \leq \mathbb{E}||\x||_0$, then $\mathbf{L}$ is a permutation-scaling matrix, i.e. there exists a diagonal matrix $\mathbf{D}$ and a permutation matrix $\mathbf{P}$ such that $\mathbf{L} = \mathbf{D}\mathbf{P}$ 
\end{lemma}
\begin{proof}
    We start by rewriting the l.h.s. of $\mathbb{E}||\mathbf{L}\x||_0 \leq \mathbb{E}||\x||_0$ as
\begin{align}
    \sE\normin{\x}_{0} &= \sE_{p(S)}\sE[\sum_{j=1}^m \mathbf{1}(\x_j \not= 0) \mid S] \\
    &= \sE_{p(S)}\sum_{j=1}^m \sE[\mathbf{1}(\x_j \not= 0) \mid S] \\
    &= \sE_{p(S)}\sum_{j=1}^m \sP_{\x \mid S}\{\x \in \sR^m \mid \x_{j} \not= 0\}\\
    &= \sE_{p(S)}\sum_{j=1}^m \mathbf{1}(j \in S)\,,
\end{align}
where the last step follows from the definition of $S$.

Moreover, we rewrite $\sE\normin{\mathbf{L}\x}_{0}$ as
\begin{align}
    \sE\normin{\mathbf{L}\x}_{0} &= \sE_{p(S)}\sE[\sum_{j=1}^m \mathbf{1}(\mathbf{L}_{j, :}\x \not= 0)\mid S]\\
    &= \sE_{p(S)}\sum_{j=1}^m \sE[\mathbf{1}(\mathbf{L}_{j, :}\x \not= 0)\mid S]\\
    &= \sE_{p(S)}\sum_{j=1}^m \sE[\mathbf{1}(\mathbf{L}_{j, S}\x_S \not= 0)\mid S]\\
    &= \sE_{p(S)}\sum_{j=1}^m \sP_{\x \mid S}\{ \x \in \sR^m \mid \mathbf{L}_{j, S}\x_S \not= 0\} \,.
\end{align}

Notice that
\begin{align}
    \sP_{\x \mid S}\{ \x \in \sR^m \mid \mathbf{L}_{j, S}\x_S \not= 0\} &= 1 - \sP_{\x \mid S}\{ \x \in \sR^m \mid \mathbf{L}_{j, S}\x_S = 0\}\,. %\\
    %&= 1 - \sP_{\mW \mid S}[\forall i \in [k],\ \mW_{i, S} \mL_{S, j} = 0]
\end{align}
Define $N_j$ be the support of $\mL_{j, :}$, i.e., $N_j \coloneqq \{i \in [m] \mid \mL_{j, i} \not= 0 \}$. 

When $S \cap N_j = \emptyset$, we have that $\mL_{S,j} = \bm0$ and thus
$$\sP_{\x \mid S}\{ \x \in \sR^m \mid \mathbf{L}_{j, S}\x_S = 0\} = 1 \,.$$ 
When $S \cap N_j \not= \emptyset$, we have that $\mL_{j,S} \not= \bm0$, and thus, by the second assumption, we have that 
$$\sP_{\x \mid S}\{ \x \in \sR^m \mid \mathbf{L}_{j, S}\x_S = 0\} = 0 \,.$$ 

Thus we can write
\begin{align}
    \sP_{\x \mid S}\{ \x \in \sR^m \mid \mathbf{L}_{j, S}\x_S \not= 0\} &= 1 - \sP_{\x \mid S}\{ \x \in \sR^m \mid \mathbf{L}_{j, S}\x_S = 0\}\\
    &= 1 - \mathbf{1}(S \cap N_j = \emptyset) \\
    &= \mathbf{1}(S \cap N_j \not= \emptyset)\,,
\end{align}
which allows us to write
\begin{align}
    \sE\normin{\mathbf{L}\x}_{0} &= \sE_{p(S)}\sum_{j=1}^m \mathbf{1}(S \cap N_j \not= \emptyset)\,.
\end{align}
The original inequality $\mathbb{E}||\mathbf{L}\x||_0 \leq \mathbb{E}||\x||_0$ can thus be rewritten as
\begin{align}
    \sE_{p(S)}\sum_{j=1}^m \mathbf{1}(S \cap N_j \not= \emptyset) &\leq \sE_{p(S)}\sum_{j=1}^m \mathbf{1}(j \in S) \,. \label{eq:before_perm}
\end{align}
Since $\mL$ is invertible, there exists a permutation $\sigma: [m] \rightarrow [m]$ such that, for all $j \in [m]$, $\mL_{j, \sigma(j)} \not=0$ (e.g. see Lemma B.1 from \citet{lachapelle2023synergies}). In other words, for all $j \in [m]$, $j \in N_{\sigma(j)}$. Of course we can permute the terms of the l.h.s. of~\cref{eq:before_perm}, which yields
\begin{align}
    \sE_{p(S)}\sum_{j=1}^m \mathbf{1}(S \cap N_{\sigma(j)} \not= \emptyset) &\leq \sE_{p(S)}\sum_{j=1}^m \mathbf{1}(j \in S)\\
    \sE_{p(S)}\sum_{j=1}^m \left(\mathbf{1}(S \cap N_{\sigma(j)} \not= \emptyset) - \mathbf{1}(j \in S)\right) &\leq 0 \,. \label{eq:sum_of_positive}
\end{align}
We notice that each term $\mathbf{1}(S \cap N_{\sigma(j)} \not= \emptyset) - \mathbf{1}(j \in S) \geq 0$ since whenever $j \in S$, we also have that $j \in S \cap N_{\sigma(j)}$ (recall $j \in N_{\sigma(j)}$). Thus, the l.h.s. of \cref{eq:sum_of_positive} is a sum of non-negative terms which is itself non-positive. This means that every term in the sum is zero:
\begin{align}
    \forall S \in \gS,\ \forall j \in [m],&\ \mathbf{1}(S \cap N_{\sigma(j)} \not= \emptyset) = \mathbf{1}(j \in S)\,.
\end{align}
Importantly,
\begin{align}
    \forall j \in [m],\ \forall S \in \gS,\ j \not\in S \implies S \cap N_{\sigma(j)} = \emptyset\,,
\end{align}
and since $S \cap N_{\sigma(j)} = \emptyset \iff N_{\sigma(j)} \subseteq S^c$ we have that
\begin{align}
    \forall j \in [m],\ \forall S \in \gS,\ j \not\in S \implies N_{\sigma(j)} \subseteq S^c \\
    \forall j \in [m],\ N_{\sigma(j)} \subseteq \bigcap_{S \in \gS \mid j \not\in S} S^c \,. \label{eq:last_intersect}
\end{align}
By assumption, we have $\bigcup_{S \in \gS \mid j \not\in S} S = [m] \setminus \{j\}$. By taking the complement on both sides and using De Morgan's law, we get $\bigcap_{S \in \gS \mid j \not\in S} S^c = \{j\}$, which implies that $N_{\sigma(j)} = \{j\}$ by~\Cref{eq:last_intersect}. Thus, $\mL = \mathbf{D}\mathbf{P}$ where $\mathbf{D}$ is an invertible diagonal matrix and $\mathbf{P}$ is a permutation matrix.
\end{proof}

\begin{comment}
\begin{mainbox}
\cref{prop:id} (Identifiability up to permutation and scaling.): Encoding $\hat{r}$ and decoding $\hat{q}$ satisfying the conditions in \cref{ass:injective_Av} and \cref{ass:recon}, defined as per the model in \cref{prob:reconstruction_sparse} where the sparsity regularization satisfies the constraint in \cref{eqn:sparsity} are identified as per \cref{defn:perm_idnt} for the range of datasets meeting the conditions in \cref{ass:suffsupp}.
\end{mainbox}
% in the proof from lemma b, v is indeed considered to be affine, so do we need the confusion being affine and linear? little note on using it interchangeably or say we write affine cuz it's more general
% should probably just write s_delta c and remove the delta z part from everywhere
% end: since model is linear, all params are identified to same ~

\textbf{Proof}:  Recall from \cref{ass:recon} that $\hat{q}$ and $\hat{r}$ satisfy the zero reconstruction error constraint:

$$\mathbb{E} ||\deltaz -\hat{q}(\hat{r}(\deltaz)) ||_2^2 = 0.$$

Rewriting this in terms of $\deltac$ since $\deltaz = q(\deltac_S)$ for any pair of observations:

$$ \implies \mathbb{E} || q(\deltac_S) - \hat{q}(\hat{r}(q(\deltac_S))||^2_2 = 0$$

This implies $q$ and $\hat{q} \circ \hat{r} \circ q$ are $P_{\deltac_S}$-almost equal in the $\text{supp}(P_{\deltac_S}) = |V|$, the set of varying concepts across all pairs of observations $(\c, \tilde \c) \sim p(\c, \tilde \c | S)$ where $S \sim P(S)$. 
\begin{flalign}
    \implies q \stackrel{P_{\deltac_S}}{\sim} \hat{q} \circ \hat{r} \circ q
    \label{eqn:p_eq}
\end{flalign}

By assumption, $q, \hat{q}, \hat{r}$ are all injective and affine, hence continuous. This implies $\hat{q} \circ \hat{r} \circ q$ must be continuous as well.

Since in \cref{eqn:p_eq} two continuous quantities are equal to each other in probability $P_{\deltac_S}$ where $\text{supp}(P_{\deltac_S}) =  [|V|]$,

\begin{flalign}
    \implies q = \hat{q} \circ \hat{r} \circ q \quad \forall \deltac_S \in [|V|]
    \label{eqn:q_eq}
\end{flalign}

Left multiplying the above equation by $\hat{q}^{-1}$, we get:

\begin{flalign}
    \hat{q}^{-1}(q (\deltac_S)) = \hat{r}(q (\deltac_S)) \quad \forall \deltac_S \in [|V|]
    \label{eqn:origins_v}
\end{flalign}

In \cref{eqn:origins_v}, on the RHS: we have a composition of injective affine functions $q$ and $\hat{r}$. Hence, the function on the LHS must also be an injective affine function. This means that $\hat{q}^{-1} \circ q$ is an injective affine function. Let's denote: 
\begin{flalign}
    \hat{q}^{-1} \circ q = m
    \label{eqn:v}
\end{flalign}

where $m: \C \rightarrow \C$ is an injective affine transformation. Recall the sparsity constraint from \cref{eqn:sparsity}:
$$\mathbb{E} || \hatdeltac_S ||_0 \leq \mathbb{E} || \deltac_S ||_0$$
Substituing $\hatdeltac_S = \hat{r}(\deltaz)$ and $\deltaz = q(\deltac_S)$:

$$ \implies \mathbb{E} || \hat{r}(q(\deltac_S)) ||_0 \leq \mathbb{E}|| \deltac_S||_0$$

which from \cref{eqn:origins_v} can be re-written as:

$$\mathbb{E}|| \hat{q}^{-1}(q(\deltac_S) ||_0 \leq \mathbb{E}|| \deltac_S||_0$$

Using \cref{eqn:v},

\begin{flalign}
    \mathbb{E}|| m(\deltac_S)||_0 \leq \mathbb{E}||\deltac_S||_0
    \label{eqn:q_ps}
\end{flalign}

From \cref{ass:suffsupp}, $\forall s \subseteq \mathcal{S}$, the probability measure $\mathbb{P}_{\deltac_S | S = s}$ has a density with respect to the Lebesgue measure. Thus leveraging Lemma B.3 in \citep{xu2024sparsityprinciplepartiallyobservable}, since $\hat q \circ q$ is injective and affine, Lemma B.3 implies that $m$ can only be a permutation composed with scaling $\forall \deltac_S \in [|V|]$. This means $\hat{q}^{-1} \circ q$ is a permutation composed with scaling.
$$\implies \hat q = m \circ q,$$

i.e. $\hat{q}$ identifies $q$ up to permutation and scaling. Since $q$ is linear and identified, all the parameters of $q$ are identified as well.
Further, rewriting the sparsity constraint from \cref{eqn:sparsity}, i.e.,  $\mathbb{E}|| \deltac_S ||_0 \leq \mathbb{E}|| \deltac_S||_0$, in terms of $\deltaz$ by substituting $\hatdeltac_S = \hat{r}(\deltaz)$:

$$ \implies \mathbb{E} || \hat{r}(\deltaz) ||_0 \leq \mathbb{E}|| r(\deltaz)||_0$$

Left multiplying the above equation by $r^{-1}$:

$$ \implies \mathbb{E} || r^{-1}(\hat{r}(\deltaz)) ||_0 \leq \mathbb{E}|| \deltaz||_0$$

Since $r$ and $\hat{r}$ 
 are injective affine transformations, the composition $r^{-1} \circ \hat{r}$ is an injective affine transformation as well. Denoting $r^{-1} \circ \hat{r} = n$ where $n : \Z \rightarrow \Z$ is an injective affine transformation, we get an equation analogous to \cref{eqn:q_ps}:

 $$\mathbb{E}|| n(\deltaz)||_0 \leq \mathbb{E}||\deltaz||_0$$

Which by Lemma B.3 in \citep{xu2024sparsityprinciplepartiallyobservable} implies $r^{-1} \circ \hat{r}$ is a permutation composed with scaling, or that:

$$\hat{r} = r \circ n,$$

i.e. $\hat{r}$ identifies $r$ up to permutation and scaling, and since $r$ is linear and identified, all the parameters of $r$ are identified as well.

From \cref{defn:perm_idnt}, $\exists$ permutation matrix $\rmP$ and diagonal matrix $\mathbf{D}$ s.t. $\hat r =  \A_V^+ \mathbf{D} \rmP$ (on the domain of the observed $\deltaz$) and $\hat q =  \A_V \mathbf{D} \rmP$.


\begin{subbox}
    What happens if we consider non-linear $q$ for extracting concept vectors? In the case of a non-linear $q$, we need further assumptions on the distribution of concepts that can be identified \citep{xu2024sparsityprinciplepartiallyobservable}, restricting the set of concepts that can be provably steered.
\end{subbox}
\end{comment}

\subsection{Distributions satisifying \cref{ass:suffsupp}}
\label{apx:suffsupp}

In $\mathbb{R}^{|S|}$, any lower-dimensional subspace has Lebesgue measure 0. By defining the probability measure of $\deltac_S | S$ with respect to the Lebesgue measure, its integral over any lower-dimensional subspace of $\mathbb{R}^{|S|}$ will be 0. Consider a few examples of $\mathbb{P}_{\deltac_S | S}$ directly taken from \citep{lachapelle2023synergies} with adapted notation just for illustration purposes.


\begin{figure}[ht]
\centering
    \includegraphics[width=0.3\linewidth]{apx_suff_supp.png}
    \caption{Three illustrative examples of $\mathbb{P}_{\deltac_S | S}$: Only distribution \textcolor{blue}{II} satisfies \cref{ass:suffsupp}.}
    \label{fig:asm24}
\end{figure}

In \cref{fig:asm24}, distributions \textcolor{yellow}{I} and \textcolor{purple}{III} do not satisfy \cref{ass:suffsupp} whereas distribution \textcolor{blue}{II} does. This is because \textcolor{yellow}{I} represents the support of a Gaussian distribution with a low-rank covariance and \textcolor{purple}{III} represents finite support; both of these distributions will be measure zero in $\mathbb{R}^{|S|}$. On the other hand, \textcolor{blue}{II} represents level sets of a Gaussian distribution with full-rank covariance. Please refer to \citet{lachapelle2023synergies} for a comprehensive explanation.


\subsection{Interpreting the Linear Representation Hypothesis}
\label{apx:lrh}
\begin{corollary}
    If concept changes act on latent embeddings following $\tilde \z = \z + \deltaz$ and $q$ and $r$ are injective, they must be affine transformations.
\end{corollary}

\textbf{Proof}:  Starting with the interpretation of the \textit{linear representation hypothesis} such that $\thicktilde{\z} = \z + \deltaz$ where $\z = q (\c)$ and $\thicktilde{\z} = q(\thicktilde{\mathbf{c}})$:

$$\implies q(\thicktilde{\c}) = q(\c) + \deltaz$$

Since we identify only the varying concepts, this corresponds to identifying a subspace of the original concept space in which $\tilde \c = \c + \deltac_V$.

Using the injectivity of $q$ (\cref{ass:injective_Av}):
\begin{flalign}
    q(\mathbf{c} + \deltac_V) = q(\mathbf{c}) + \deltaz
    \label{eqn:prejacob}
\end{flalign}

Taking the gradient of both the LHS and the RHS wrt $\mathbf{c}$,

$$\frac{\partial(\mathbf{c} + \deltac_V)}{\partial(\mathbf{c})} \nabla_{(\mathbf{c} + \deltac_V)} q(\mathbf{c} + \deltac_V) = \nabla_{\mathbf{c}} q(\mathbf{c})$$
$$\nabla_{(\mathbf{c} + \deltac_V)} q(\mathbf{c} + \deltac_V) = \nabla_{\mathbf{c}} q(\mathbf{c})$$
\begin{flalign}
    \mathbf{J}^T(\mathbf{c} + \deltac_V) = \mathbf{J}^T(\mathbf{c})
\end{flalign}
Where $\mathbf{J}(\mathbf{c})$ is the Jacobian of $q$ at $\mathbf{c}$ and $\mathbf{J}(\mathbf{c} + \deltac_V)$ is the Jacobian of $q$ at $\mathbf{c} + \deltac_V$.

 $$\begin{bmatrix}
\nabla q_1(\mathbf{c} + \deltac_V)\\
\nabla q_2(\mathbf{c} + \deltac_V)\\
\nabla q_3(\mathbf{c} + \deltac_V)\\\
\cdot \\
\cdot \\
\nabla q_{d_Z}(\mathbf{c} + \deltac_V)
\end{bmatrix} - \begin{bmatrix}
\nabla q_1(\mathbf{c})\\
\nabla q_2(\mathbf{c})\\
\nabla q_3(\mathbf{c})\\\
\cdot \\
\cdot \\
\nabla q_{d_Z}(\mathbf{c})
\end{bmatrix} = 0$$

considering the $j$\textsuperscript{th} component of the difference,

$$\begin{bmatrix}
\nabla^2 q_j(\theta_1)\\
\nabla^2 q_j(\theta_2)\\
\nabla^2 q_j(\theta_2)\\\
\cdot \\
\cdot \\
\nabla^2 q_j(\theta_d)
\end{bmatrix}(\deltac_V) = 0$$

Following the proof in \citep{ahuja2022weakly},$\nabla^2q_j(\mathbf{c}) = 0$, which implies $q(\mathbf{c}) = \mathbf{A}_V\mathbf{c + b}$ where $\mathbf{A}_V \in \mathbb{R}^{d_Z \times d_Z}, \mathbf{b} \in \mathbb{R}^{d_Z}$ or that $q$ is affine. Similarly, we can show that $r$ is affine too by starting with $r(\z + \deltaz) = r(\z) + \deltac_V$.

\begin{corollary}
    If we assume $\thicktilde{\z} = \boldsymbol{\phi}(\z)$, for an affine map $q$, $\mathbf{A = I}$.
\end{corollary}

\textbf{Proof}: Let's assume the affine form of $q$ can be expressed as: 

\begin{flalign}
    \z = \mathbf{A}_V\mathbf{c} + \mathbf{b}
    \label{eqn:q_aff}
\end{flalign}
where $\mathbf{A}_V \in \mathbb{R}^{d_Z \times d_Z}$ and $\mathbf{k} \in R^{d_Z}$.

Similarly, $\thicktilde \z = q(\thicktilde{\mathbf{c}}) = \mathbf{A}_V\thicktilde{\mathbf{c}} + \mathbf{b}$ and we know $\tilde \c = \c + \deltac_V$. 

$$\implies \thicktilde \z = \mathbf{A}_V(\c + \deltac_V) + \mathbf{b}$$

we have $\thicktilde{\mathbf{z}} = \boldsymbol{\phi}(\mathbf{z})$  and from \cref{eqn:q_aff}:
\begin{flalign}  
    \boldsymbol{\phi}(\mathbf{A}_V\mathbf{c} + \mathbf{b}) = \mathbf{A}_V(\c + \deltac_V) + \mathbf{b}
\end{flalign}

In the above equation, we can see that the maximum degree of $\c$ on the RHS is $1$, which implies that the degree of $\c$ on the LHS should also at most be $1$, which implies $\boldsymbol{\phi}$ can at most be an affine function.

So let's assume $\boldsymbol{\phi}$ is an affine function of the form:

\begin{flalign}
    \thicktilde{\mathbf{z}} = \boldsymbol{\phi}(\mathbf{z}) = \mathbf{T}\mathbf{z} + \deltaz
\end{flalign}

where $\mathbf{T} \in \mathbb{R}^{d_Z \times d_Z}$ and $
\deltaz \in \mathbb{R}^{d_Z}$. Substituting this in the above equation, we get:

\begin{flalign}
    \mathbf{T}(\A_V \c + \mathbf{b}) + \deltaz = \mathbf{A}_V(\c + \deltac_V) + \mathbf{b}
\end{flalign}
$$\mathbf{Q(T - I)c} + (\mathbf{T - I)b} + (\deltaz \mathbf{ - Q} \deltac_V) = 0$$

For a non-trivial solution:
\begin{flalign}
    \mathbf{T = I} \\
    \deltaz = \mathbf{A}_V \deltac_V
\end{flalign}

So, we have proved that if we assume $q$ to be affine, then $\thicktilde{\mathbf{z}} = \mathbf{z} + \delta \mathbf{z}$.


% \textbf{Proof}: Let's start with the \cref{eqn:cdel} $\tilde \c = \c + \bsigma \c$, and $\c = \mathbf{r} (\mathbf{z})$:

% $$\implies \mathbf{r} (\thicktilde{\mathbf{z}}) = \mathbf{r} (\mathbf{z}) + \delta \c$$

% Using the injectivity of $\mathbf{r}$ (\cref{apx:inj}) and \cref{eqn:zc}:
% \begin{flalign}
%     \mathbf{r}(\mathbf{z} + \delta \mathbf{z}) = \mathbf{r}(\mathbf{z}) + \bsigma \c
%     \label{eqn:prejacob}
% \end{flalign}

% Taking the gradient of both the LHS and the RHS wrt $\mathbf{z}$,

% $$\frac{d(\mathbf{z} + \delta \mathbf{z})}{d(\mathbf{z})} \nabla_{(\mathbf{z} + \delta \mathbf{z})} \mathbf{r}(\mathbf{z} + \delta \mathbf{z}) = \nabla_{\mathbf{z}} \mathbf{r}(\mathbf{z})$$
% $$\nabla_{(\mathbf{z} + \delta \mathbf{z})} \mathbf{r}(\mathbf{z} + \delta \mathbf{z}) = \nabla_{\mathbf{z}} \mathbf{r}(\mathbf{z})$$
% \begin{flalign}
%     \mathbf{J}^T(\mathbf{z} + \delta \mathbf{z}) = \mathbf{J}^T(\mathbf{z})
% \end{flalign}

% Where $\mathbf{J}(\mathbf{z})$ is the Jacobian of $\mathbf{r}$ at $\mathbf{z}$ and $\mathbf{J}(\mathbf{z} + \delta \mathbf{z})$ is the Jacobian of $\mathbf{r}$ at $\mathbf{z} + \delta \mathbf{z}$.

%  $$\begin{bmatrix}
% \nabla \mathbf{r}_1(\mathbf{z} + \delta \mathbf{z})\\
% \nabla \mathbf{r}_2(\mathbf{z} + \delta \mathbf{z})\\
% \nabla \mathbf{r}_3(\mathbf{z} + \delta \mathbf{z})\\\
% \cdot \\
% \cdot \\
% \nabla \mathbf{r}_{Kd_Z}(\mathbf{z} + \delta \mathbf{z})
% \end{bmatrix} - \begin{bmatrix}
% \nabla \mathbf{r}_1(\mathbf{z})\\
% \nabla \mathbf{r}_2(\mathbf{z})\\
% \nabla \mathbf{r}_3(\mathbf{z})\\\
% \cdot \\
% \cdot \\
% \nabla \mathbf{r}_d(\mathbf{z})
% \end{bmatrix} = 0$$

% considering the $j$\textsuperscript{th} component of the difference,

% $$\begin{bmatrix}
% \nabla^2 \mathbf{r}_j(\theta_1)\\
% \nabla^2 \mathbf{r}_j(\theta_2)\\
% \nabla^2 \mathbf{r}_j(\theta_2)\\\
% \cdot \\
% \cdot \\
% \nabla^2 \mathbf{r}_j(\theta_d)
% \end{bmatrix}(\delta \mathbf{z}) = 0$$

% Following the proof in \citep{ahuja2022weakly},$\nabla^2\mathbf{r}_j(\mathbf{z}) = 0$, which implies $\mathbf{r}(\mathbf{z}) = \mathbf{Tz + k}$ or that $\mathbf{r}$ is affine. 

% % address the central nature of lrh: 

% \begin{corollary}
%     If we misspecify \cref{eqn:zc} such that instead of $\thicktilde{\z} = \z + \delta \z$, $\thicktilde{\z} = \boldsymbol{\phi}(\z)$, for an affine map $\q$, $\mathbf{A = I}$.
% \end{corollary}

% \textbf{Proof}: Let's assume the affine form of $\mathbf{q}$ can be expressed as: 

% \begin{flalign}
%     \z = \mathbf{q}(\mathbf{c}) = \mathbf{Q}\mathbf{c} + \mathbf{k}
%     \label{eqn:q_aff}
% \end{flalign}
% where $\mathbf{Q} \in \mathcal{M}_{d_Z, d_Z} (\mathbb{R})$ and $\mathbf{k} \in R^{d_Z}$.

% Similarly, $\thicktilde \z = \mathbf{q}(\thicktilde{\mathbf{c}}) = \mathbf{Q}\thicktilde{\mathbf{c}} + \mathbf{k}$ and from \cref{eqn:cdel}, we know $\tilde \c = \c + \bsigma_{\delta C}$. 

% $$\implies \thicktilde \z = \mathbf{Q}(\c + \bsigma_{\delta C}) + \mathbf{k}$$

% we have $\thicktilde{\mathbf{z}} = \boldsymbol{\phi}(\mathbf{z})$  and from \cref{eqn:q_aff}:
% \begin{flalign}  
%     \boldsymbol{\phi}(\mathbf{Q}\mathbf{c} + \mathbf{k}) = \mathbf{Q}(\c + \bsigma_{\delta C}) + \mathbf{k}
% \end{flalign}

% In the above equation, we can see that the maximum degree of $\c$ on the RHS is $1$, which implies that the degree of $\c$ on the LHS should also at most be $1$, which implies $\boldsymbol{\phi}$ can at most be an affine function.

% So let's assume $\boldsymbol{\phi}$ is an affine function of the form:

% \begin{flalign}
%     \thicktilde{\mathbf{z}} = \boldsymbol{\phi}(\mathbf{z}) = \mathbf{A}\mathbf{z} + \delta \mathbf{z}
% \end{flalign}

% where $\mathbf{A} \in \mathcal{M}_{d_Z, d_Z}(\mathbb{R})$ and $
% \delta \mathbf{z} \in \mathbb{R}^{d_Z}$. Substituting this in the above equation, we get:

% \begin{flalign}
%     \mathbf{A}(\mathbf{Qc} + \mathbf{k}) + \delta \mathbf{z} = \mathbf{Q}(\c + \bsigma_{\delta C}) + \mathbf{k}
% \end{flalign}
% $$\mathbf{Q(A - I)c} + (\mathbf{A - I)k} + (\delta \z \mathbf{ - Q} \bsigma_{\delta C}) = 0$$

% For a non-trivial solution:
% \begin{flalign}
%     \mathbf{A = I} \\
%     \delta \z = \mathbf{Q} \bsigma_{\delta C}
% \end{flalign}

% So, we have proved that if we assume $\mathbf{q}$ to be affine, then $\thicktilde{\mathbf{z}} = \mathbf{z} + \delta \mathbf{z}$.

    \textbf{Implications}: Multiple expositions \citep{templeton2024scaling} remark that it it not clear what the meaning of \textit{linear} exactly  is in the linear representation hypothesis. Informally, many results cited in support of the linear representation hypothesis either extract information with a linear probe, or add a vector to influence model behavior. Here, we assume that if linear meant concepts are linearly encoded in the latent space, we can show that this would correspond to shifts in the latent space representing net concept changes and vice versa, which means both interpretations are the same, so it does not matter which one is assumed. 



% \textbf{This implies that $\delta C$ is identified by $\hat{\mathbf{s}}^{-1}(\delta \mathbf{z})$ up to permutation and elementwise linear transformations.}

% if in general, q/r is non-linear: then we don't have id results. so what are people with sae setups doing? they are using a non-linear r cuz that is the same mapping as ours from C to Z and they are using non-paired observations for this---dig a bit into this since in seb's paper they only show a counter-example to say additional assumptions are needed but can do ~ with one more assumption or:
% can we prove this for independent latents? would this mean that this way they can only extract independent concepts? 


\section{Implementation and experimental details}
\label{apx:imp}

\subsection{\isae Architecture}

The encoding $r: \Z \rightarrow \C$ and decoding functions $q: \C \rightarrow \Z$ constituting the \isae autoencoding framework are parameterized as follows:

\begin{flalign}
% \begin{split} \nonumber
     \hatdeltac_V \coloneqq {r}(\deltaz) & \coloneqq \mathbf{W}_e (\deltaz - \mathbf{b}_d) + \mathbf{b}_e;
     \label{eqn:enc}\\
    \hatdeltaz \coloneqq {q}(\hatdeltac_V) & \coloneqq \mathbf{W}_d \hatdeltac_V + \mathbf{b}_d\,. \label{eqn:dec}
% \end{split}
\end{flalign}

\textbf{Parameters}.  $\mathbf{W}_e \in \mathbb{R}^{|V| \times d_z}, \mathbf{b}_e \in \mathbb{R}^{|V|}, \mathbf{W}_d \in \mathbb{R}^{d_z \times |V|}$, and $\mathbf{b}_d \in \mathbb{R}^{d_z}$ denote the encoder weights, encoder bias, decoding weights, and decoder bias respectively. The decoder bias is also treated as a pre-encoder bias purely for empirical performance improvement reasons based on ongoing discourse on engineering improvements in SAEs \citep{bricken2023monosemanticity, gao2024scaling}. The encoder and decoder weights are initialised s.t. $\mathbf{W}_d = \mathbf{W}_e^T$. The bias terms $\mathbf{b}_e$ and $\mathbf{b}_d$ are initialised to be all zero vectors. Further, after every iteration, the columns of $\mathbf{W}_d$ are unit normalised following \citet{bricken2023monosemanticity, gao2024scaling}.

\textbf{Data}. Data is layer-normalised analogous to \citet{gao2024scaling} prior to being passed as input to the encoder in batch sizes of $32$.

\textbf{Optimization.} Specifically, the following objective is optimized:
\begin{flalign}
    \min \frac{1}{N} \sum_{i = 1}^N \frac{||\deltaz_{(i)} - q(r(\deltaz_{(i)}))||^2_2}{||\deltaz_{(i)}||^2_2}, \\
    \text{s.t.} \frac{1}{|V|N} \sum_{i = 1}^N || r(\deltaz_{(i)})||_1 \leq \beta
\end{flalign}

We optimize the above constrained minimisation problem by computing its Lagrangian and the primal and dual gradients using the \texttt{cooper} library \citep{gallegoPosada2022cooper}. We use ExtraAdam \citep{gidel2020variationalinequalityperspectivegenerative} as both the primal and the dual optimizer, with the values of the primal and dual learning rates fixed throughout training and selected based on UDR scores (see \cref{apx:udr}). ExtraAdam uses \textit{extrapolation from the past} to provide similar convergence properties as extra-gradient optimizers \citep{korpelevich1976extragradient} without requiring twice as many gradient computations per parameter update or auxiliary storage of trainable parameters \citep{gidel2020variationalinequalityperspectivegenerative, gallegoPosada2022cooper}. Further, to account for the unit-norm adjustment of the columns of the decoder weights $\mathbf{W}_d$, we adjust gradients to remove discrepancies between the true gradients and the ones used by the optimizer. This done by removing any gradient information parallel to the columns of $\mathbf{W}_d$ at every step after the normalisation of the columns of $\mathbf{W}_d$. 


\subsubsection{Model Selection via Unsupervised Diversity Ranking (UDR)}
\label{apx:udr}

\begin{figure}
\centering
\includegraphics[scale=0.2]{udr_bin1.png}
\caption{UDR scores suggest a \texttt{primal\_lr} value of $0.005$ and a $\beta$ value of 5.}
\label{fig:udr_bin1}
\end{figure}
\begin{figure}    
\centering
\includegraphics[scale=0.18]{udr_bin2.png}
    % \centering
    % \includegraphics[width=0.5\linewidth]{figs/udr_bin2.png}
    % \caption{An example of observed UDR scores and MCC values for selecting optimal values of the two most important hyperparameters in tuning.}
    % \label{fig:udr}
    \caption{UDR scores suggest a \texttt{primal\_lr} value of $0.005$ and a $\beta$ value of 11.}
    \label{fig:udr_bin2}
\end{figure}

Unsupervised model selection remains a notoriously difficult problem since there appears to be no unsupervised way of distinguishing between bad and good random seeds; unsupervised model selection should not depend on ground truth labels since these might biased the results based on supervised metrics. Moreover, in disentanglement settings, hyperparameter selection cannot rely solely on choosing the best validation-set performance. This is because there is typically a trade-off between the quality of fit and the degree of disentanglement (\citep{locatello2019challengingcommonassumptionsunsupervised}, Sec 5.4). For the proposed method in \cref{sec:wscrl}, identifiability of the decoder and of the learnt representation is essential to recover steering vectors for individual concepts. It is possible that a decoder with higher reconstruction error is identified to a greater degree. Hence, it is not sufficient to engineer a good unsupervised model solely based on how well it minimizes the reconstruction loss. \citet{duan2019unsupervised} propose the Unsupervised Disentanglement Ranking (UDR) score \citep{duan2019unsupervised}, which measures the consistency of the model across different initial weight configurations (seeds), which we use to fit our model. It is calculated as follows: for every hyperparameter setting, we compute MCCs between pairs of different runs and compute the median of all pairwise MCCs as the UDR score. We report the UDR scores and the mean pair-wise MCCs for the two most important hyperparameters affecting observed reconstruction error and MCC values---the learning rate of the primal optimizer (\texttt{primal\_lr}) and the sparsity level ($\beta$)---over $10$ pairs of $5$ random seeds in \cref{fig:udr_bin1} for the dataset, \textsc{lang}$(1, 1)$, and in \cref{fig:udr_bin2} for \textsc{binary}$(2, 2)$, over a selected hyperparameter range corresponding to decent reconstruction error. At slightly different hyperparameter settings, reconstruction error may spike even if the MCC remains acceptable. Such scenarios often fall outside the scope of consideration here, as they break the assumption of near‐perfect reconstruction. While models may not achieve zero reconstruction loss in practice, we still expect it to remain reasonably low. As can be seen in \cref{fig:udr_bin1} and \cref{fig:udr_bin2}, MCC values typically correlate with the UDR scores. Further, using these different models, we perform a sensitivity analysis on the two most important hyperparameters of our model---the sparsity level $\epsilon$ and the learning rate, which we report in \cref{apx:sens}.

% \subsubsection{Model details}
% \label{apx:model}
%  Since the size of the set $S$ is not known for each pair of observations, we initially set $\hatdeltac_S \in \mathbb{R}^{|V|}$. In \cref{sec:exp_llms}, we demonstrate recovery of steering vectors for $\hatdeltac_S \in \mathbb{R}^k$ where $|V| \le k \le d_Z$, satisfying \cref{ass:injective_Av}.

% We know that $\hatdeltac_V \in \mathbb{R}^{|V|}$. However, we do not know $S$ for every input $\deltaz$. So, we can assume without loss of generality that the predicted $\hatdeltac_S \in \mathbb{R}^{|V|}$. We will show in \cref{sec:exp_llms} that even if we assume $\hatdeltac_S \in \mathbb{R}^{k}$ where $|V| \leq k$ (and $k \leq d_z$ to ensure injectivity of $q$ and $r$ (\cref{ass:injective_Av})), we can estimate $\hatdeltac_S$ fairly accurately. Thus, in the general case we can say: $\hatdeltac_S \in \mathbb{R}^{k}$, $\mathbf{W}_e \in \mathcal{M}_{k, d_Z}(\mathbb{R})$, and $\mathbf{W}_d \in \mathcal{M}_{d_Z, k}(\mathbb{R})$. Here, the concept vector $\hatdeltac_S$ denotes the concepts that changed for the specific input $\deltaz$. This estimation method takes inspiration from sparse autoencoders, which are applied to find interpretable features in LLM representations.

% We implement the single layer linear auto-encoding framework presented in \cref{sec:model} with BatchNorm \citep{ioffe2015batchnormalizationacceleratingdeep} at the output of the encoder. We impose \cref{eqn:sparse_constraint_l1} as a hard constraint for the optimisation objective in \cref{eqn:model_recon} using Cooper \citep{gallegoPosada2022cooper}, and select a sparsity level for the encoder's output. This is a a major departure from recent work using SAEs to discover concepts in the latent space of LLMs. We argue that this addresses concerns associated with $l1$ regularisation, such as feature suppression \citep{anders_etal_2024_composedtoymodels_2d}. Further, we normalise the columns of the decoder after every step following \citet{bricken2023monosemanticity, gao2024scaling}. For more details, please refer to \cref{apx:imp}. To perform unsupervised model selection, we track the Unsupervised Disentanglement Ranking (UDR) scores \citep{duan2019unsupervised}, which measure the consistency of the model across different initial weight configurations (seeds). Further, we perform a sensititvity analysis on the two most important hyperparameters of our model---the sparsity level $\epsilon$ and the learning rate (\cref{apx:sens}). 
% , which mitigates issues such as feature suppression \citep{anders_etal_2024_composedtoymodels_2d}. This approach yields improved identifiability, even under feature absorption (or strong correlations). 
% \sj{better id implies these}
% Model selection is tricky since good disentanglement usually comes at the cost of a good fit. We report the UDR score as the median of the pairwise MCCs between different runs for a few different hyperparameter settings \citep{lachapelle2022disentanglement}.


\subsubsection{Sparse optimization}
\label{app:sparseopt}
% \sj{todo: add details about comparison with TopK}
We choose to enforce sparsity in the learning objective of the model as an explicit constraint rather than as $l_1$-regularisation due to the benefits listed in \cref{tab:sparsityreg}. In areas such as compressive sensing, signal processing, and certain machine learning applications, constrained optimization approaches have shown superior performance in recovering sparse signals and providing better generalization performance.

\begin{table}[ht]
\centering
\begin{tabularx}{\textwidth}{|X|X|X|}
   \hline 
   & \textbf{Constrained optimization} & \textbf{$\ell_1$-regularisation} \\
   \hline
   \textit{Optimization efficiency} & Finding the optimal solution and enforcing sparsity are separate tasks. Methods like augmented Lagrangian formulations iteratively enforce sparsity while optimizing the objective function, which can lead to more stable convergence. & The $l_1$ penalty introduces a non-differentiable point at zero, which requires careful tuning and can be sensitive to initialization and hyperparameters. \\
\hline
\textit{Hyperparameter tuning} & The primary hyperparameter is the sparsity level $\epsilon$, which can be set based on domain knowledge or practical constraints, simplifying the model selection process. & The primary hyperparameter is the strength of the sparsity penalty in the training objctive $\lambda$, which needs tuning to prevent under or over-fitting.\\
\hline
\textit{Interpretability and control} & We have precise control on the sparsity of the solution since the relationship between $\epsilon$ and solution sparsity is direct. The solution is easier to interpret.  &  The relationship between $\lambda$ and the resulting solution sparsity is complex and non-linear and a small change in the value of $\lambda$ can lead to very large solution changes, making it difficult to control or interpret.\\
\hline
\end{tabularx}
\caption{Benefits of constrained optimization over regularisation for enforcing sparsity.}
\label{tab:sparsityreg}
\end{table}


\subsubsection{Datasets}
\label{apx:data}

We list out data generation pipelines for the semi-synthetic datasets in \cref{data:binsynth} and \cref{data:cat} and refer the reader to \citep{lin2022truthfulqameasuringmodelsmimic} and the corresponding Hugging Face repository for details on the multiple-choice subset of TruthfulQA considered in this paper.

\begin{figure}
\begin{subbox}
\textsc{lang}($1, 1$) 
\vspace{1mm}
% \rule{0.75\linewidth}{0.5pt}

Generate pairs of text samples varying only in their language, within a pair and having the same type of variation in language across all pairs. Choosing \textit{eng} $\rightarrow$ \textit{french} as the variation in the concept of \textit{language}, so as to learn the steering vector \textit{eng} $\rightarrow$ \textit{french}, we generate pairs of words describing common \textit{household objects}, such as: 

\begin{lstlisting}
[("Door", "Porte"),("Dog", "Chien"), ("Shirt", "Chemise"),("fish", "poisson"),("Pillow", "Oreiller"),("Blanket", "Couverture"),("Sunday", "Dimanche"),("Hat", "Chapeau"),("Umbrella", "Parapluie"),("Glasses", "Lunettes"), ("Clock", "Horloge"),...]
\end{lstlisting}

\rule{\linewidth}{0.5pt}\\

\textsc{gender}($1, 1$) 
\vspace{1mm}

Generate pairs of text samples varying only in gender within a pair and having the same type of variation in gender across all pairs. Choosing \textit{masculine} $\rightarrow$ \textit{feminine} as the variation in the concept of \textit{gender}, so as to learn the steering vector \textit{masculine} $\rightarrow$ \textit{feminine}, we generate pairs of words describing common \textit{professions}, such as: 
\begin{lstlisting}
    [("grandpa", "grandma"), ("grandson", "granddaughter"), ("groom", "bride"), ("he", "she"), ("headmaster", "headmistress"), ("heir", "heiress"), ("hero", "heroine"), ("husband", "wife"), ("king", "queen"), ("lion", "lioness"), ("man", "woman"), ("manager", "manageress"), ("men", "women"),...]
\end{lstlisting}

\rule{\linewidth}{0.5pt}\\

\textsc{binary}($2, 2$) 
\vspace{1mm}

Generate pairs of text samples varying in \textit{gender} and \textit{language} such that it is not known if which of the two, or both, vary within any pair. Choosing \textit{masculine} $\rightarrow$ \textit{feminine} as the variation in the concept of \textit{gender} and \textit{eng} $\rightarrow$ \textit{french} as the variation in the concept of \textit{language}, so as to learn the steering vectors for \textit{masculine} $\rightarrow$ \textit{feminine} and \textit{eng} $\rightarrow$ \textit{french}, we generate pairs of words describing common \textit{professions}, such as: 
\begin{lstlisting}
    [("brother", "sister"), ("buck", "doe"), ("bull", "cow"),
    ("daddy", "mommy"), ("fils", "fille"), ("homme", "femme"), ("mari", "femme"), ("acteur", "actrice"), ("Duc", "Duchess"), ("Widow", "Veuf"), ("Taureau", "Cow"), ("Hen", "Coq"),...]
\end{lstlisting}

Here, we generate an equal number of samples with only \textit{masculine} $\rightarrow$ \textit{feminine}, only \textit{eng} $\rightarrow$ \textit{french}, and both \textit{masculine} $\rightarrow$ \textit{feminine} and \textit{eng} $\rightarrow$ \textit{french} variations.

\rule{\linewidth}{0.5pt}\\

\textsc{corr}($2, 1$) 
\vspace{1mm}

Generate pairs of text samples varying only in language within a pair but having two different types of variation in language across all pairs. Choosing \textit{eng} $\rightarrow$ \textit{french} and \textit{eng} $\rightarrow$ \textit{german} as the two types of variations in the concept of \textit{language}, so as to learn the steering vector \textit{eng} $\rightarrow$ \textit{french}, we generate pairs of words describing common \textit{professions}, such as: 
\begin{lstlisting}
    [("Doctor", "arzt"), ("Lehrer", "teacher"), ("Engineer", "Ingenieur"), ("Pflegefachkraft", "Nurse"), ("headmaster", "headmistress"), ("Teacher", "Enseignant"), ("Infirmier", "Nurse"), ("Koch", "Chef"),...]
\end{lstlisting}

Generate an equal number of pairs for each variation \textit{eng} $\rightarrow$ \textit{german} and \textit{eng} $\rightarrow$ \textit{french} with correlated pairs.

\rule{\linewidth}{0.5pt}\\
\end{subbox}
\caption{Data generation pipeline for semi-synthetic language datasets considering binary contrasts in underlying concepts from a potentially higher-level concept consisting of several such binary contrasts.}
\label{data:binsynth}
\end{figure}

\begin{figure}
\begin{subbox}
\textsc{cat}($135, 3$) 
\vspace{1mm}

Consider a codebook of precisely defined attributes for three categorical variables: shape, color, and object.
\begin{lstlisting}
CATEGORICAL_CODEBOOK = {
    "shape": ["spherical", "cuboidal", "conical", "circular", "squarish", "toroidal", "pyramidal", "cylindrical", "prismatic", "hemispherical"],
    
    "color": ["red", "blue", "green", "yellow", "orange", "purple", "pink", "cyan", "teal", "lavender"],
    
    "object": ["button", "shoe", "mug", "vase", "bead", "cushion", "toy", "statue", "drawing", "window"],
    }
\end{lstlisting}
Sample pairs phrases combining one attribute from each variable such that an equal number of pairs have only one variable (e.g., shape) undergoing change (e.g., \textit{squarish} $\rightarrow$ \textit{prismatic}), two variables undergoing change (such as the pair \texttt{spherical green mug} and \texttt{pyramidal pink mug}), or all three of them changing. Since under the LRH we assume to learn concepts as binary contrasts, we aim to discover $3 \times \binom{10}{2} = 135$ different steering vectors.
\end{subbox}
\caption{Data generation pipeline for a significantly harder dataset than the ones in \cref{data:binsynth} considering explicitly categorical variables and their attributes to learn steering vectors for corresponding binary contrasts.}
\label{data:cat}
\vspace{-1em} 
\end{figure}
 
\subsubsection{Mean Correlation Coefficient: Gateway to Interpreting Latent Dimensions}
\label{apx:mcc}

In modern work on identifiable representation learning, the Mean Correlation Coefficient (MCC) was proposed to be used as a metric by \citet{hyvarinen2016unsupervisedfeatureextractiontimecontrastive} to evaluate the recovery of true source signals through their estimates. It was further developed as a metric by \citet{khemakhem2020icebeemidentifiableconditionalenergybased} to measure on an average how well the elements of two vectors $\x \in \mathbb{R}^n$ and $\mathbf{y} \in \mathbb{R}^n$ are correlated under the best possible alignment of their ordering, i.e., MCC measures the average maximum correlation that can be achieved when each variable $x_i$ from $\x$ is paired with a variable $y_j$ from $\mathbf{y}$ across all possible permutations of such pairings, i.e, across $(i, \pi(j))$ where $\pi \in S_n$, the set of all permutations of the n indices.

To understand the steps involved in computing this metric, let $\x  = (x_1, x_2)$ and $\mathbf{y} = (y_1, y_2)$ be two bivariate random variables. Then,
% \vspace{-5cm}
\noindent
\begin{itemize}
    \item Append $\mathbf{y}$ to $\x$, treating rows as observations and the columns as variables (i.e. $[x_1, x_2, y_1, y_2 ]$).
    \item Compute absolute values of the Pearson correlation coefficients between $\x$ and $\mathbf{y}$, yielding the following matrix: 
    $\begin{bmatrix}
        \text{abs(corr}(x_1, y_1)) & \text{abs(corr}(x_1, y_2))\\
        \text{abs(corr}(x_2, y_1)) & \text{abs(corr}(x_2, y_2))
    \end{bmatrix}$.
    \item Next, solve the linear sum assignment problem to select the absolute correlation coefficients for pairings between components of $\x$ and $\mathbf{y}$ such that the sum of the selected coefficients is maximised. Operationally, if the pairing is of $x_1$ with $y_1$, this corresponds to a pairing score of $\text{abs(corr}(x_1, y_1) + \text{abs(corr}(x_2, y_2)$. The only other possible pairing in this case would have a score of $\text{abs(corr}(x_1, y_1) + \text{abs(corr}(x_2, y_2)$. Select the maximum of the scores of these pairings.
    \item The MCC value then would be the mean of the correlation coefficients of the optimal pairings. For example, if the best pairings are $(x_1, y_1)$ and $(x_2, y_2)$, then MCC would be mean$(\text{abs(corr}(x_1, y_1), \text{abs(corr}(x_2, y_2))$.
    % $$\max (\text{abs(corr}(x_1, y_1) \text{abs(corr}(x_1, y_2))$ to select the best pairing for $x_1$ and $\max (\text{abs(corr}(x_2, y_1), \text{abs(corr}(x_2, y_2))$  to select the best pairing for $x_2$ and compute the sum of the correlation coefficients corresponding to these pairings, i.e., $\max (\text{abs(corr}(x_1, y_1), \text{abs(corr}(x_1, y_2)) + \max (\text{abs(corr}(x_2, y_1), \text{abs(corr}(x_2, y_2))$. Say, for eg, the best pairings are $(x_1, y_1)$ and $(x_2, y_2)$. Then this sum would have been $\text{abs(corr}(x_1, y_1)) +  \text{abs(corr}(x_2, y_2))$. 
\end{itemize}

\textbf{Evaluating learnt representations.} When the ground truth latent representation is known, MCC is computed between the ground truth variable and its estimate. When the ground truth is unknown, MCC is computed by comparing pairs of latent representations, where each stems from a different random initialisation of the representation learner. This tests if the model can consistently learn representations within the equivalence class of permutation and scaling.

\textbf{Why consider permutations?} It is essential to consider all permutations between the variables $\x$ and $\mathbf{y}$ to maximise the average correlation between them since it is possible that the average correlation for a pair of perfectly correlated variables is $0$. To observe this, consider an example reproduced from \citep{khemakhem2020icebeemidentifiableconditionalenergybased}): Where $\x = (x_1, x_2)$ s.t. $x_1 \perp\!\!\!\!\perp x_2$ and $\mathbf{y} = (x_2^2, x_1^2)$. Then $\frac{1}{2}\sum_i \text{corr}(x_i, y_i) = 0$ since $x_1 \perp\!\!\!\!\perp x_2$ even though $\x$ completely determines $\mathbf{y}$. Moreover, since it is not possible to resolve indeterminacies up to permutation and rescaling in recovering variables, it is important to consider different orderings of the components.

\textbf{Other metrics.} While MCC measures permutation-identifiability, other metrics such as the coefficient of determination $R^2$ can be used to measure linear identifiabilty by predicting the ground truth latent variables from the learnt latent variables. The average Pearson correlation between the ground truth and the learnt latents would correspond to the coefficient of multiple correlation ($R$). MCC $\leq$  $R\leq R^2$. So measuring MCC values gives us a more conservative estimate for our results. Moreover, MCC allows for other measures of correlations to be considered between the variables, including ones that measure non-linear dependencies such as the Randomised Dependence Coefficient \citep{lopez2013randomized}.

\subsection{Cosine Similarity}

Cosine similarity reflects the geometry of an LLM's latent space in general, thereby acting as a measure of semantic similarity between embeddings. This is because gradient descent often shapes the latent space of an LLM toward a Euclidean-like structure \citep{jiang2024originslinearrepresentationslarge}, despite it being unidentified by standard pre-training objectives \citep{park2023linear}. Further, for the Llama family of models \citep{dubey2024llama3herdmodels}, it has been shown that cosine similarity indeed acts similar to the causal inner product in terms of capturing the semantic structure of embeddings  \citep{park2023linear}. Empirically, cosine similarity is the most common similarity metric for comparing embeddings. 

\subsection{$2^{\text{nd}}$ Test of robustness: impact of increasing the dimensionality of the encoder's output}
\label{apx:v}

The output of the encoder is predicted as $\hatdeltac_V \in \mathbb{R}^{k}$, where $k = |V|$. In \cref{fig:k_mcc}, we investigate the effect of increasing $m$ beyond $|V|$, i.e., increasing the predicted latent dimension, on MCC values obtained on the dataset with the largest latent dimension, \textsc{cat}($135, 3$). \isae is reasonably disentangled even when the dimension of the concept vectors to be predicted is fairly \textit{misspecified}, whereas the affine baseline's MCC values drop sharply.

\begin{figure}
    \centering
    \includegraphics[scale=0.4]{varyingk_mccs.png}
    \caption{\textbf{MCC values decrease with an increasing $k$ more sharply for the \aff baseline.}}
    \label{fig:k_mcc}
\end{figure}

\subsection{Synthetic Experiments}
\label{apx:synth}

In addition to experiments with LLM embeddings which indicate potential for practical utility, we perform experiments with purely synthetic data in which concepts are precisely known and it is possible to evaluate the model against a known ground-truth. As a teaser to appreciate the relevance of synthetic experiments, consider: even if SAEs consistently learn similar concepts, how can we evaluate if the learnt concepts correspond to the concepts encoded in the input data? 

 We consider $c_1, c_2, ..., c_{|V|}$ to correspond to individual concepts. For language data, we assumed that there are concepts like ``gender" and ``truthfulness" and that they would be represented as one hot vectors $c_1$ and $c_2$. However, such concepts are abstract and it is an assumption that the model would represent both $c_1$ and $c_2$ atomically whereas it is possible that $c_2$ is represented by $2$ atomic concepts and $c_1$ by $1$. It is not possible to resolve such ambiguities since the ground truth representations of $c_1$ and $c_2$ are not known. For the sake of exposition, in purely synthetic data, $c_1$ and $c_2$ are precisely and it is possible to evaluate the model against a known ground truth.


\textbf{Data}. 
For a brief summary of the number of varying concepts within a pair and across all pairs considered, refer to \cref{tab:datasets_synth}. In the case of synthetic data, we generate $\c$ and $\tilde \c$ first to compute $\deltac \coloneqq \tilde \c - \c$, then apply a dense linear transformation $\rmL$ to $\deltac$ to generate $\deltaz$ as $\deltaz = \rmL \deltac$. Importantly, towards the generation of $\c$, we generate zero vectors in $\mathbb{R}^{|V|}$ such that for any given sample, $S$ components are perturbed by samples from a uniform distribution and others remain zero. This is similar to the data generating process in \citep{anders_etal_2024_composedtoymodels_2d} and the conditional distribution of $\deltac_S$ satisfies \cref{ass:suffsupp} of having a density with respect to Lebesgue.

\begin{table}
% \begin{wraptable}{r}{0.55\textwidth}
    \centering
    \renewcommand{\arraystretch}{1.25}
    \caption{Datasets comprise of paired observations $(\z, \tilde \z)$ where $\z$ and $\tilde \z$ vary in concepts $V = \{c_1, c_2, ..., c_{|V|}\}$ across all pairs, such that for any given pair, the maximum number of varying concepts is max($|S|$). \textit{Nomenclature for semi-synthetic datasets follows the rule: identifier of the dataset indicating why we consider it, followed by $|V|$ and max$(|S|)$: \textsc{identifier}($|V|$, max$|S|$)}.}
    \label{tab:datasets}
    \footnotesize{
    \begin{tabular}{c  c  c}
    \toprule
\textbf{Dataset} & $|V|$ & max($|S|$) \\
        \midrule
        \textsc{synth}($3, 2$) & 3 & 2 \\ \hdashline
        \textsc{synth}($4, 3$) & 4 & 3 \\ \hdashline
        \textsc{synth}($10, 7$) & 10 & 7 \\
        \bottomrule
    \end{tabular}
    }
\label{tab:datasets_synth}
% \end{wraptable}
\end{table}

\begin{table}
    \centering
    \renewcommand{\arraystretch}{1.25}
    \caption{The mean \textbf{MCC values between the learnt and the ground truth concept vectors are close to $1$.}}
    \label{tab:mcc_synth}
    \footnotesize{
    \begin{tabular}{c  c  c}
    \toprule
         & \textbf{\isae} & \textbf{\aff} \\
        \hline
        \textsc{synth}($3, 2$)  & $0.999 \pm 0.0001$ & $0.873 \pm 0.0561$ \\ \hdashline
        \textsc{synth}($4, 3$) & $0.999 \pm 0.0011$ & $0.835 \pm 0.0097$ \\ \hdashline
        \textsc{synth}($10, 7$) & $0.993 \pm 0.0005$ & $0.769 \pm 0.0103$ \\ \hline
    \end{tabular}
    }
\end{table}
% We consider $(\x, \tilde \x) \in \mathbb{R}^n$ pairs that can vary in $n$ concepts, such that $|V| = n$. The difference between $\x$ and $\tilde \x$ is represented as an $n$-long binary vector where $1$ denotes a varying concept. These differences are constrained such that the maximum number of concepts varying for a given pair is $k$, ie, max($|s|) = k$. We ensure that across all pairs of $(\x, \tilde \x)$, each of the $n$ concepts vary more than once. To the difference vectors, a dense invertible linear transformation is applied resulting in $\deltaz$. The aim is to then obtain $n$ $1$-hot vectors of length $n$ each representing a change in an individual element. For eg. say $k$ is $2$ and $n$ is $3$. The difference vectors in this case are represented by: $\{[1, 0, 0], [0, 1, 0], [0, 0, 1], [1, 1, 0], [1, 0, 1], [0, 1, 1]\}$ and $\deltac = \{[1, 0, 0], [0, 1, 0], [0, 0, 1]\}$. We remark that we focus on settings with a higher percentage of variables changing per sample, ie, higher $k/n$ ratios. We consider three different settings with $(k, n) = \{(2, 3), (3, 4), (7, 10)\}$ denoted by the datasets \textsc{synth}$(3, 2)$, \textsc{synth}$(4, 3)$, and \textsc{synth}$(10, 7)$ (\cref{tab:datasets}). It is interesting to consider these settings since they represent a high $k/n$ ratio and consider up to $7$ concepts varying per sample, which is difficult to obtain in both semi-synthetic and real-world language based data.

\textbf{Results}. We estimate $\hatdeltac$ and compare it against $\deltac$ to verify the degree of identifiability of the learnt concept vectors or encoder representations. Since we have the ground truth here, we compute the MCC between $(\hatdeltac, \deltac)$ to measure degree of identifiability. \cref{tab:mcc_synth} shows that the proposed method can identify concepts even for higher values of $|V|$ and max($|S|$) against known ground truth data. Synthetic experiments addressing different facets of the identifiability setting we assume can be readily found in prior work on disentangling representations using sparse shifts \citep{xu2024sparsityprinciplepartiallyobservable, lachapelle2023synergies}. 
% \subsubsection{Sensitivity Analysis}
\label{apx:sens}

% \subsubsection{Implementation Details}
% \label{apx:imp}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \section{Technical Review}
% \label{apx:review}

% \textbf{Theoretical Evidence for the Linear Representation Hypothesis}. In the context of LLMs, \citep{roeder2021linear, marconato2024all} 
% show that for the class of deep generative models defined by the probability of the next token $y$ given the context $x$ where $f_{\theta}(x) : \mathcal{X} \rightarrow \mathcal{Z}$ and $g_{\theta}(y) : \mathcal{Y} \rightarrow \mathcal{Z}$ as follows:

% $$p_{\theta}(y | x) = \frac{\exp \left( f_{\theta}(x) g_{\theta}(y)\right)}{\sum_{y' \in \mathcal{Y}}\exp \left( f_{\theta}(x) g_{\theta}(y')\right)},$$

% $p_{\theta} = p_{\theta'} \implies \theta \overset{L}{\sim} \theta'$ for a ``diverse" set $\mathcal{Y}$. This result implies the statistical identifiability of the model $p_{\theta}(y | x)$ such that the learnt representations $z, z' \in \mathcal{Z}$ are linearly similar. \citep{park2023linear, jiang2024originslinearrepresentationslarge} use these insights to discuss the linear identifiability of concepts in representation space $\mathcal{Z}$.


% % \textbf{Causal representation learning}.

% % % table for situating our work in the field of crl
% % % axes: latent dimension is known, relations between latents, number of factors varying between samples, ground truth latents known, 
% % \begin{table}[ht]
% % \centering
% % \renewcommand{\arraystretch}{1.25}
% % \begin{tabular}{m{19em} | >{\centering\arraybackslash}m{1.5em} | >{\centering\arraybackslash}m{1.5em} | >{\centering\arraybackslash}m{1.5em} | >{\centering\arraybackslash}m{1.5em} | >{\centering\arraybackslash}m{1.5em}}
% %    \hline 
% %    & I & II & III & IV & V \\
% %    \hline
% %    Multi-dimensional latent variables &  &  & \textcolor{violet}{\faCheckCircle} & \textcolor{violet}{\faCheckCircle} &
% %    \textcolor{violet}{\faCheckCircle}
% %    \\
% %    \hdashline
% %     Potentially correlated latent variables & \textcolor{persianindigo}{\faCheckCircle} & \textcolor{persianindigo}{\faCheckCircle} & \textcolor{persianindigo}{\faCheckCircle} & \textcolor{persianindigo}{\faCheckCircle}  & \textcolor{persianindigo}{\faCheckCircle}\\
% %    \hdashline
% %    Paired observations & & \textcolor{blue}{\faCheckCircle} &  &  \textcolor{blue}{\faCheckCircle} & \textcolor{blue}{\faCheckCircle}\\
% %    \hdashline
% %    Varying latents in each observation & \textcolor{green}{\faCheckCircle} & \textcolor{green}{\faCheckCircle} & \textcolor{green}{\faCheckCircle} &   & \textcolor{green}{\faCheckCircle}\\
% %    \hdashline
% %     Varying number of latents in each observation & \textcolor{yellow}{\faCheckCircle} &  & \textcolor{yellow}{\faCheckCircle} &  & \textcolor{yellow}{\faCheckCircle} \\
% %     \hdashline
% %     % \citet{park2023linear}  & & & & & & &\\
% %     % \hline
% %     % Non-linear $q$  &  \textcolor{orange}{\faCheckCircle} & \textcolor{orange}{\faCheckCircle} & \textcolor{orange}{\faCheckCircle} & &\\
% %     % \hdashline
% %     Ground truth latent variables are unknown & & & & \textcolor{red}{\faCheckCircle} & \textcolor{red}{\faCheckCircle}\\
% % \hline
% % \end{tabular}
% % \label{tab:crlwork}
% % \caption{Comparison of identifiability results and the experimental settings they are verified in  from (I) \citep{xu2024sparsityprinciplepartiallyobservable}, (II) \citep{ahuja2022weakly}, (III) \citep{lippe2022citriscausalidentifiabilitytemporal}, (IV) \citep{rajendran2024learning}, and (V) this work.}
% % \end{table}


% % \textbf{Interpretability and Controllability of LLM representations}.
% % \begin{table}[ht]
% % \centering
% % \renewcommand{\arraystretch}{1.25}
% % \begin{tabular}{m{19em} | >{\centering\arraybackslash}m{1.5em} | >{\centering\arraybackslash}m{1.5em} | >{\centering\arraybackslash}m{1.5em} | >{\centering\arraybackslash}m{1.5em} | >{\centering\arraybackslash}m{1.5em}}
% %    \hline 
% %    & I & II & III & IV & V \\
% %    \hline
% %    Unsupervised discovery of concepts & \textcolor{violet}{\faCheckCircle} & \textcolor{violet}{\faCheckCircle} & \textcolor{violet}{\faCheckCircle} & \textcolor{violet}{\faCheckCircle} &   \textcolor{violet}{\faCheckCircle}
% %    \\
% %    \hdashline
% %    Unsupervised discovery of steering vectors & & &  &  \textcolor{blue}{\faCheckCircle} & \textcolor{blue}{\faCheckCircle}\\
% %    \hdashline
% %    Guarantees on discovered vectors & & & &  \textcolor{green}{\faCheckCircle} & \textcolor{green}{\faCheckCircle}\\
% %    \hdashline
% %     Multi-dimensional concepts  &  & \textcolor{yellow}{\faCheckCircle} & & \textcolor{yellow}{\faCheckCircle} & \textcolor{yellow}{\faCheckCircle} \\
% %     \hdashline
% %     % \citet{park2023linear}  & & & & & & &\\
% %     % \hline
% %     % Non-linear $q$  &  \textcolor{orange}{\faCheckCircle} & \textcolor{orange}{\faCheckCircle} & \textcolor{orange}{\faCheckCircle} & &\\
% %     % \hdashline
% %     Multi concept data & & & & & \textcolor{red}{\faCheckCircle}\\
% % \hline
% % \end{tabular}
% % \label{tab:conceptwork}
% % \caption{Comparing different aspects of concept vector discovery for (I) \citep{templeton2024scaling}, (II) \citep{engels2024languagemodelfeatureslinear}, (III) \citep{rajamanoharan2024improvingdictionarylearninggated}, (IV) \citep{rajendran2024learning}, and (V) this work.}
% % \end{table}

% \subsection{Empirical evaluation in causal representation learning}
% Causal representation learning (CRL) is concerned with identifying latent variables from high-dimensional data, with an emphasis on new identifiability proofs. Most methods are evaluated on simple, low-dimensional synthetic data using the Mean Correlation Coefficient (MCC) metric. Evaluations lack evidence of performance improvement on downstream tasks. The main purported benefits or CRL are better out-of-distribution generalisation and robustness to distribution shifts. While, it is enticing to see these benefits being realised on synthetic data, it is crucial to realise the same benefits on more real-world like data to make a strong case for CRL to deep learning practitioners. The most common CRL datasets have information on the ground truth latent variables and their dimensionality, assuming we'd always know the ground truth latent variables for data. This is usually not the case in practice. In this paper, we take a different approach of showing the applicability of an existing proof towards a more realistic data where neither the ground truth latent variables, nor their dimensionality is known. Our empirical studies demonstrate a clear benefit of employing an identifiable representation as compared to other existing methods, and highlight that it is not sufficient to look just at the MCC scores. Further, most datasets in CRL look at image based data where it is easier to form a sense of the latent generative factors. In this work, we look at natural language, a domain where it is difficult to come up with quantifiable metrics. In future, it is crucial to develop datasets and benchmarks addressing these challenges and clearly outlining downstream tasks demonstrating the benefits of identifiable representations.

% \subsection{Concepts and affordances}
% \label{apx:concept}
% \epigraph{..we understand the world by studying change, not by studying things..}{As quoted in the Order of Time, Anaximander}

% In this paper, it's not just that we want to solve the problem of steering. What we are really saying is that it is impossible to identifiably the map from observations to concepts, since the latter is a much higher dimensional object. So, instead, consider this: what are the entities that are concepts for sure? They are objects that can be manipulated, where the steering vectors tell us how. This is akin to object centric representation learning. And we propose to learn the transformations to these objects as means of identifying them (because what an object is depends on its affordance) instead of directly learning the objects aka concepts directly.


% Further, using real-world 
% datasets consisting of observations such that pairs of them would vary along multiple concepts, where the labels of these concepts are unknown, highlights the necessity to address the problem of affordance discovery in order to learn 

% a couple of already known missing pieces in the linear representation hypothesis (LRH). First, LRH verifies that the concepts we are looking for from labeled datasets are linearly encoded in the last layer of an LLM--but are there other linearly encoded concepts (that we are not looking for) in intermediate layers that could be steered which may be non-linearly encoded in the last layer? Second, such datasets might often contain categorical concepts. Since the LRH represents categorical concepts as binary contrasts (an approach that scales combinatorially with the number of concept categories) when it might not be clear for more abstract concepts what the two ends of a binary contrast look like (for eg. would \textit{rude} $\rightarrow$ \textit{polite} represent only a single concept's variation?), the application of our method is restricted.  

\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.

