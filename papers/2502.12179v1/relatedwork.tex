\section{Related work}
\label{sec:rel}
\textbf{Linear representation hypothesis}. This paper builds on the linear representation hypothesis that language models encode concepts linearly. Several papers provide empirical evidence for this hypothesis \citep{mikolov-etal-2013-linguistic, gittens-etal-2017-skip, ethayarajh2019understandinglinearwordanalogies, allen2019analogiesexplainedunderstandingword, seonwoo-etal-2019-additive, burns2024discoveringlatentknowledgelanguage, li2024inferencetimeinterventionelicitingtruthful, moschella2023relativerepresentationsenablezeroshot, tigges2023linearrepresentationssentimentlarge, nanda2023emergentlinearrepresentationsworld, nissim-etal-2020-fair, ravfogel-etal-2020-null,park2023linear}. Recent work also provides theoretical justification for why linear properties might consistently emerge across models that perform next-token prediction \citep{roeder2021linear, jiang2024originslinearrepresentationslarge, marconato2024all,park2024geometrycategoricalhierarchicalconcepts}.  

\textbf{Interpretability of LLMs}. This paper contributes to the literature on interpretability and steering of LLMs. Much of the work on finding concepts in LLM representations for steering relies on supervision, either from paired observations with a single-concept shift \citep{panickssery2024steeringllama2contrastive,turner2024steeringlanguagemodelsactivation,rimsky2024steering,li2024inferencetimeinterventionelicitingtruthful} or from examples of target LLM completions to prompts \citep{subramani2022extracting}. This prior work also focuses on applying the same steering vector to all examples, implicitly relying on the linear representation hypothesis as justification. In contrast, we make the assumption precise, and show how it leads to steering vectors. This paper also departs from supervised learning and focuses on learning with limited supervision. In this way, we propose a method that is similar to sparse autoencoders (SAEs) \citep{templeton2024scaling, engels2024languagemodelfeatureslinear, cunningham2023sparseautoencodershighlyinterpretable, rajamanoharan2024improvingdictionarylearninggated, gao2024scaling}. In contrast, our proposed method fits concept shifts, and provably identifies steering vectors while SAEs do not enjoy identifiability guarantees.

\textbf{Causal representation learning}. Finally, this paper builds on causal representation learning results that leverage sparsity constraints. \citet{ahuja2022weakly}, \citet{locatello2020weakly}, and \citet{brehmer2022weaklysupervisedcausalrepresentation} consider sparse latent perturbations and paired observations. In contrast, we focus on learning from multi-concept shifts. \citet{lachapelle2022disentanglement} focus on sparse interventions and sparse transitions in temporal settings, while \citet{lachapelle2023synergies}, \citet{layne2024sparsityregularizationtreestructuredenvironments}, \citet{xu2024sparsityprinciplepartiallyobservable}, and \citet{fumero2023leveragingsparsesharedfeature} leverage sparse dependencies between latents and tasks. In this paper, we adapt these assumptions and technical results for a novel setting: discovering steering vectors from LLM representations based on concept shift data. Although they do not leverage a sparsity constraint, the setting in \citet{rajendran2024learning} is the most similar to ours, aiming to recover linear subspaces that capture concepts from observations generated by a nonlinear mixing function. They leverage concept-conditional datasets to prove that these concept subspaces are identified up to linear transformations. In contrast, we focus on concept shifts, and propose a regularized objective that recovers them up to permutation and scaling, allowing for straightforward steering vector discovery. 

% In \cref{apx:review}, we further illustrate the comparisons between this paper and related work.

% The objective of interpreting and controlling for specific latent concepts in an LLM's representation space is
% tightly related to the task of learning disentangled representations \citep{bengio2009learning, bengio2013representation, scholkopf2021toward}, ie, separating out different concepts encoded in these representations. In this work, we extend prior research on learning disentangled representations by examining conditions under which controlling for distinct concepts is feasible, thus unifying these separate research directions. Refer to \cref{apx:review} for a more detailed comparison of related work.

% \textbf{Linearly encoded concepts.} The hypothesis that representations learnt by language models encode concepts linearly---the \textit{linear representation hypothesis}---has been empirically well studied in several papers \citep{mikolov-etal-2013-linguistic, gittens-etal-2017-skip, ethayarajh2019understandinglinearwordanalogies, allen2019analogiesexplainedunderstandingword, seonwoo-etal-2019-additive, burns2024discoveringlatentknowledgelanguage, li2024inferencetimeinterventionelicitingtruthful, moschella2023relativerepresentationsenablezeroshot, tigges2023linearrepresentationssentimentlarge, nanda2023emergentlinearrepresentationsworld, nissim-etal-2020-fair, ravfogel-etal-2020-null}, and a few papers also provide theoretical justification for it \citep{roeder2021linear, jiang2024originslinearrepresentationslarge, marconato2024all}. This work takes a different approach to instead adopt this hypothesis to provably discover concept vectors.  

% \textbf{Interpretability and control of LLMs}. Several recent works leverage the linear representation hypothesis to discover concepts from the latent space of LLMs \citep{templeton2024scaling, rajendran2024learning, engels2024languagemodelfeatureslinear, cunningham2023sparseautoencodershighlyinterpretable, rajamanoharan2024improvingdictionarylearninggated, gao2024scaling} using sparse autoencoders (SAEs) for interpretability research. Other line of work focuses on the additivity of linearly encoded concepts to steer language model representations \citep{panickssery2024steeringllama2contrastive, li2024inferencetimeinterventionelicitingtruthful} when a single known binary concept varies throughout the dataset. \citet{rajendran2024learning} provide a first theoretical framework to discover a set of  varying concepts up to permutation and scaling from data where only a single non-binary concept is perturbed at a time. In contrast, this paper provides the same theoretical guarantee to discover concept vectors from data where potentially multiple non-binary concepts can be perturbed at the same time and we demonstrate their effectiveness in manipulating the representation space of LLMs.

% \textbf{Disentangled representation learning}. There are two lines of work within research on disentanglement most closely related to this paper---exploiting sparsity \citep{ahuja2022weakly, lachapelle2022disentanglement, lippe2022citriscausalidentifiabilitytemporal, lachapelle2023synergies, fumero2023leveragingsparsesharedfeature, layne2024sparsityregularizationtreestructuredenvironments, xu2024sparsityprinciplepartiallyobservable}, or using paired samples \citep{ahuja2022weakly, locatello2020weakly, brehmer2022weaklysupervisedcausalrepresentation}. Most relevant to our work are the frameworks of \citet{lachapelle2022disentanglement, lachapelle2023synergies, xu2024sparsityprinciplepartiallyobservable}  which leverage sparse shifts to latent variables to guarantee the identifiability of disentangled solutions. Specifically, we use paired observations and adapt their identifiability result showing disentanglement up to permutation and scaling when not all latent variables are present for every observation for linear mixing functions and unpaired observations.