@article{reber2024rate,
  title={RATE: Score Reward Models with Imperfect Rewrites of Rewrites},
  author={Reber, David and Richardson, Sean and Nief, Todd and Garbacea, Cristina and Veitch, Victor},
  journal={arXiv preprint arXiv:2410.11348},
  year={2024}
}

@article{lopez2013randomized,
  title={The randomized dependence coefficient},
  author={Lopez-Paz, David and Hennig, Philipp and Sch{\"o}lkopf, Bernhard},
  journal={Advances in neural information processing systems},
  volume={26},
  year={2013}
}

@misc{gamella2024causalchambersrealphysical,
      title={The Causal Chambers: Real Physical Systems as a Testbed for AI Methodology}, 
      author={Juan L. Gamella and Jonas Peters and Peter Bühlmann},
      year={2024},
      eprint={2404.11341},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2404.11341}, 
}

@article{gondal2019transfer,
  title={On the transfer of inductive bias from simulation to the real world: a new disentanglement dataset},
  author={Gondal, Muhammad Waleed and Wuthrich, Manuel and Miladinovic, Djordje and Locatello, Francesco and Breidt, Martin and Volchkov, Valentin and Akpo, Joel and Bachem, Olivier and Sch{\"o}lkopf, Bernhard and Bauer, Stefan},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@article{cadei2024smoke,
  title={Smoke and Mirrors in Causal Downstream Tasks},
  author={Cadei, Riccardo and Lindorfer, Lukas and Cremer, Sylvia and Schmid, Cordelia and Locatello, Francesco},
  journal={arXiv preprint arXiv:2405.17151},
  year={2024}
}

@misc{muennighoff2023mtebmassivetextembedding,
      title={MTEB: Massive Text Embedding Benchmark}, 
      author={Niklas Muennighoff and Nouamane Tazi and Loïc Magne and Nils Reimers},
      year={2023},
      eprint={2210.07316},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2210.07316}, 
}

@article{lachapelle2024nonparametric,
  title={Nonparametric partial disentanglement via mechanism sparsity: Sparse actions, interventions and sparse temporal dependencies},
  author={Lachapelle, S{\'e}bastien and L{\'o}pez, Pau Rodr{\'\i}guez and Sharma, Yash and Everett, Katie and Priol, R{\'e}mi Le and Lacoste, Alexandre and Lacoste-Julien, Simon},
  journal={arXiv preprint arXiv:2401.04890},
  year={2024}
}

@misc{liu2024incontextvectorsmakingcontext,
      title={In-context Vectors: Making In Context Learning More Effective and Controllable Through Latent Space Steering}, 
      author={Sheng Liu and Haotian Ye and Lei Xing and James Zou},
      year={2024},
      eprint={2311.06668},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2311.06668}, 
}

@misc{perez2022discoveringlanguagemodelbehaviors,
      title={Discovering Language Model Behaviors with Model-Written Evaluations}, 
      author={Ethan Perez and Sam Ringer and Kamilė Lukošiūtė and Karina Nguyen and Edwin Chen and Scott Heiner and Craig Pettit and Catherine Olsson and Sandipan Kundu and Saurav Kadavath and Andy Jones and Anna Chen and Ben Mann and Brian Israel and Bryan Seethor and Cameron McKinnon and Christopher Olah and Da Yan and Daniela Amodei and Dario Amodei and Dawn Drain and Dustin Li and Eli Tran-Johnson and Guro Khundadze and Jackson Kernion and James Landis and Jamie Kerr and Jared Mueller and Jeeyoon Hyun and Joshua Landau and Kamal Ndousse and Landon Goldberg and Liane Lovitt and Martin Lucas and Michael Sellitto and Miranda Zhang and Neerav Kingsland and Nelson Elhage and Nicholas Joseph and Noemí Mercado and Nova DasSarma and Oliver Rausch and Robin Larson and Sam McCandlish and Scott Johnston and Shauna Kravec and Sheer El Showk and Tamera Lanham and Timothy Telleen-Lawton and Tom Brown and Tom Henighan and Tristan Hume and Yuntao Bai and Zac Hatfield-Dodds and Jack Clark and Samuel R. Bowman and Amanda Askell and Roger Grosse and Danny Hernandez and Deep Ganguli and Evan Hubinger and Nicholas Schiefer and Jared Kaplan},
      year={2022},
      eprint={2212.09251},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2212.09251}, 
}

@misc{shen2017styletransfernonparalleltext,
      title={Style Transfer from Non-Parallel Text by Cross-Alignment}, 
      author={Tianxiao Shen and Tao Lei and Regina Barzilay and Tommi Jaakkola},
      year={2017},
      eprint={1705.09655},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1705.09655}, 
}

@article{subramani2022extracting,
  title={Extracting latent steering vectors from pretrained language models},
  author={Subramani, Nishant and Suresh, Nivedita and Peters, Matthew E},
  journal={ACL Findings},
  year={2022}
}

@misc{turner2024steeringlanguagemodelsactivation,
      title={Steering Language Models With Activation Engineering}, 
      author={Alexander Matt Turner and Lisa Thiergart and Gavin Leech and David Udell and Juan J. Vazquez and Ulisse Mini and Monte MacDiarmid},
      year={2024},
      eprint={2308.10248},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2308.10248}, 
}

@misc{rahn2024controllinglargelanguagemodel,
      title={Controlling Large Language Model Agents with Entropic Activation Steering}, 
      author={Nate Rahn and Pierluca D'Oro and Marc G. Bellemare},
      year={2024},
      eprint={2406.00244},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2406.00244}, 
}


@article{comon1994independent,
  title={Independent component analysis, a new concept?},
  author={Comon, Pierre},
  journal={Signal processing},
  volume={36},
  number={3},
  pages={287--314},
  year={1994},
  publisher={Elsevier}
}

@misc{park2024geometrycategoricalhierarchicalconcepts,
      title={The Geometry of Categorical and Hierarchical Concepts in Large Language Models}, 
      author={Kiho Park and Yo Joong Choe and Yibo Jiang and Victor Veitch},
      year={2024},
      eprint={2406.01506},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2406.01506}, 
}

@misc{rimsky2024steering,
      title={Steering Llama 2 via Contrastive Activation Addition}, 
      author={Nina Rimsky and Nick Gabrieli and Julian Schulz and Meg Tong and Evan Hubinger and Alexander Matt Turner},
      year={2024},
      eprint={2312.06681},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{groeneveld2024olmo,
      title={OLMo: Accelerating the Science of Language Models}, 
      author={Dirk Groeneveld and Iz Beltagy and Pete Walsh and Akshita Bhagia and Rodney Kinney and Oyvind Tafjord and Ananya Harsh Jha and Hamish Ivison and Ian Magnusson and Yizhong Wang and Shane Arora and David Atkinson and Russell Authur and Khyathi Raghavi Chandu and Arman Cohan and Jennifer Dumas and Yanai Elazar and Yuling Gu and Jack Hessel and Tushar Khot and William Merrill and Jacob Morrison and Niklas Muennighoff and Aakanksha Naik and Crystal Nam and Matthew E. Peters and Valentina Pyatkin and Abhilasha Ravichander and Dustin Schwenk and Saurabh Shah and Will Smith and Emma Strubell and Nishant Subramani and Mitchell Wortsman and Pradeep Dasigi and Nathan Lambert and Kyle Richardson and Luke Zettlemoyer and Jesse Dodge and Kyle Lo and Luca Soldaini and Noah A. Smith and Hannaneh Hajishirzi},
      year={2024},
      eprint={2402.00838},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{touvron2023llama,
      title={Llama 2: Open Foundation and Fine-Tuned Chat Models}, 
      author={Hugo Touvron and Louis Martin and Kevin Stone and Peter Albert and Amjad Almahairi and Yasmine Babaei and Nikolay Bashlykov and Soumya Batra and Prajjwal Bhargava and Shruti Bhosale and Dan Bikel and Lukas Blecher and Cristian Canton Ferrer and Moya Chen and Guillem Cucurull and David Esiobu and Jude Fernandes and Jeremy Fu and Wenyin Fu and Brian Fuller and Cynthia Gao and Vedanuj Goswami and Naman Goyal and Anthony Hartshorn and Saghar Hosseini and Rui Hou and Hakan Inan and Marcin Kardas and Viktor Kerkez and Madian Khabsa and Isabel Kloumann and Artem Korenev and Punit Singh Koura and Marie-Anne Lachaux and Thibaut Lavril and Jenya Lee and Diana Liskovich and Yinghai Lu and Yuning Mao and Xavier Martinet and Todor Mihaylov and Pushkar Mishra and Igor Molybog and Yixin Nie and Andrew Poulton and Jeremy Reizenstein and Rashi Rungta and Kalyan Saladi and Alan Schelten and Ruan Silva and Eric Michael Smith and Ranjan Subramanian and Xiaoqing Ellen Tan and Binh Tang and Ross Taylor and Adina Williams and Jian Xiang Kuan and Puxin Xu and Zheng Yan and Iliyan Zarov and Yuchen Zhang and Angela Fan and Melanie Kambadur and Sharan Narang and Aurelien Rodriguez and Robert Stojnic and Sergey Edunov and Thomas Scialom},
      year={2023},
      eprint={2307.09288},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{rombach2022highresolution,
      title={High-Resolution Image Synthesis with Latent Diffusion Models}, 
      author={Robin Rombach and Andreas Blattmann and Dominik Lorenz and Patrick Esser and Björn Ommer},
      year={2022},
      eprint={2112.10752},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{wang2024concept,
      title={Concept Algebra for (Score-Based) Text-Controlled Generative Models}, 
      author={Zihao Wang and Lin Gui and Jeffrey Negrea and Victor Veitch},
      year={2024},
      eprint={2302.03693},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{park2023linear,
      title={The Linear Representation Hypothesis and the Geometry of Large Language Models}, 
      author={Kiho Park and Yo Joong Choe and Victor Veitch},
      year={2023},
      eprint={2311.03658},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{rajendran2024learning,
      title={Learning Interpretable Concepts: Unifying Causal Representation Learning and Foundation Models}, 
      author={Goutham Rajendran and Simon Buchholz and Bryon Aragam and Bernhard Schölkopf and Pradeep Ravikumar},
      year={2024},
      eprint={2402.09236},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{locatello2019challengingcommonassumptionsunsupervised,
      title={Challenging Common Assumptions in the Unsupervised Learning of Disentangled Representations}, 
      author={Francesco Locatello and Stefan Bauer and Mario Lucic and Gunnar Rätsch and Sylvain Gelly and Bernhard Schölkopf and Olivier Bachem},
      year={2019},
      eprint={1811.12359},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1811.12359}, 
}

@inproceedings{mikolov-etal-2013-linguistic,
    title = "Linguistic Regularities in Continuous Space Word Representations",
    author = "Mikolov, Tomas  and
      Yih, Wen-tau  and
      Zweig, Geoffrey",
    editor = "Vanderwende, Lucy  and
      Daum{\'e} III, Hal  and
      Kirchhoff, Katrin",
    booktitle = "Proceedings of the 2013 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2013",
    address = "Atlanta, Georgia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N13-1090",
    pages = "746--751",
}

@InProceedings{Liu_2023_ICCV,
    author    = {Liu, Nan and Du, Yilun and Li, Shuang and Tenenbaum, Joshua B. and Torralba, Antonio},
    title     = {Unsupervised Compositional Concepts Discovery with Text-to-Image Generative Models},
    booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
    month     = {October},
    year      = {2023},
    pages     = {2085-2095}
}

@article{liu2023causal,
  title={Causal Triplet: An Open Challenge for Intervention-centric Causal Representation Learning},
  author={Liu, Yuejiang and Alahi, Alexandre and Russell, Chris and Horn, Max and Zietlow, Dominik and Sch{\"o}lkopf, Bernhard and Locatello, Francesco},
  journal={arXiv preprint arXiv:2301.05169},
  year={2023}
}

@article{scholkopf2021toward,
  title={Toward causal representation learning},
  author={Sch{\"o}lkopf, Bernhard and Locatello, Francesco and Bauer, Stefan and Ke, Nan Rosemary and Kalchbrenner, Nal and Goyal, Anirudh and Bengio, Yoshua},
  journal={Proceedings of the IEEE},
  volume={109},
  number={5},
  pages={612--634},
  year={2021},
  publisher={IEEE}
}

@article{caselles2019symmetry,
  title={Symmetry-based disentangled representation learning requires interaction with environments},
  author={Caselles-Dupr{\'e}, Hugo and Garcia Ortiz, Michael and Filliat, David},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@article{brehmer2022weakly,
  title={Weakly supervised causal representation learning},
  author={Brehmer, Johann and De Haan, Pim and Lippe, Phillip and Cohen, Taco},
  journal={arXiv preprint arXiv:2203.16437},
  year={2022}
}

@incollection{scholkopf2022causality,
  title={Causality for machine learning},
  author={Sch{\"o}lkopf, Bernhard},
  booktitle={Probabilistic and Causal Inference: The Works of Judea Pearl},
  pages={765--804},
  year={2022}
}

@article{radhakrishnan2017machine,
  title={Machine learning for nuclear mechano-morphometric biomarkers in cancer diagnosis},
  author={Radhakrishnan, Adityanarayanan and Damodaran, Karthik and Soylemezoglu, Ali C and Uhler, Caroline and Shivashankar, GV},
  journal={Scientific reports},
  volume={7},
  number={1},
  pages={1--13},
  year={2017},
  publisher={Springer}
}


@article{yang2021multi,
  title={Multi-domain translation between single-cell imaging and sequencing data using autoencoders},
  author={Yang, Karren Dai and Belyaeva, Anastasiya and Venkatachalapathy, Saradha and Damodaran, Karthik and Katcoff, Abigail and Radhakrishnan, Adityanarayanan and Shivashankar, GV and Uhler, Caroline},
  journal={Nature communications},
  volume={12},
  number={1},
  pages={31},
  year={2021},
  publisher={Nature Publishing Group UK London}
}

@article{belyaeva2021causal,
  title={Causal network models of SARS-CoV-2 expression and aging to identify candidates for drug repurposing},
  author={Belyaeva, Anastasiya and Cammarata, Louis and Radhakrishnan, Adityanarayanan and Squires, Chandler and Yang, Karren Dai and Shivashankar, GV and Uhler, Caroline},
  journal={Nature communications},
  volume={12},
  number={1},
  pages={1024},
  year={2021},
  publisher={Nature Publishing Group UK London}
}

@article{agarwal2020synthetic,
  title={Synthetic interventions},
  author={Agarwal, Anish and Shah, Devavrat and Shen, Dennis and others},
  journal={arXiv preprint arXiv:2006.07691},
  year={2020}
}

@misc{drugdisc,
  author = {Yoshua Bengio},
  title = {{ How Machine Learning Could Accelerate Drug Discovery}},
  howpublished = "\url{https://www.youtube.com/watch?v=6gkA-_ldNhw}",
  year = {2022}, 
}
@article{grill2020bootstrap,
  title={Bootstrap your own latent-a new approach to self-supervised learning},
  author={Grill, Jean-Bastien and Strub, Florian and Altch{\'e}, Florent and Tallec, Corentin and Richemond, Pierre and Buchatskaya, Elena and Doersch, Carl and Avila Pires, Bernardo and Guo, Zhaohan and Gheshlaghi Azar, Mohammad and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={21271--21284},
  year={2020}
}
@article{touvron2023llama,
  title={Llama: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}

@inproceedings{gondal2021function,
  title={Function contrastive learning of transferable meta-representations},
  author={Gondal, Muhammad Waleed and Joshi, Shruti and Rahaman, Nasim and Bauer, Stefan and Wuthrich, Manuel and Sch{\"o}lkopf, Bernhard},
  booktitle={International Conference on Machine Learning},
  pages={3755--3765},
  year={2021},
  organization={PMLR}
}

@article{rahaman2021dynamic,
  title={Dynamic inference with neural interpreters},
  author={Rahaman, Nasim and Gondal, Muhammad Waleed and Joshi, Shruti and Gehler, Peter and Bengio, Yoshua and Locatello, Francesco and Sch{\"o}lkopf, Bernhard},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={10985--10998},
  year={2021}
}

@article{ahuja2022weakly,
  title={Weakly supervised representation learning with sparse perturbations},
  author={Ahuja, Kartik and Hartford, Jason S and Bengio, Yoshua},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={15516--15528},
  year={2022}
}

@article{ahuja2021properties,
  title={Properties from mechanisms: an equivariance perspective on identifiable representation learning},
  author={Ahuja, Kartik and Hartford, Jason and Bengio, Yoshua},
  journal={arXiv preprint arXiv:2110.15796},
  year={2021}
}

@inproceedings{isola2015discovering,
  title={Discovering states and transformations in image collections},
  author={Isola, Phillip and Lim, Joseph J and Adelson, Edward H},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={1383--1391},
  year={2015}
}

@article{glocker2021causality,
  title={Causality in digital medicine},
  author={Glocker, Ben and Musolesi, Mirco and Richens, Jonathan and Uhler, Caroline},
  journal={Nature Communications},
  volume={12},
  number={1},
  year={2021},
  publisher={NATURE PORTFOLIO}
}

@article{rosas2020reconciling,
  title={Reconciling emergences: An information-theoretic approach to identify causal emergence in multivariate data},
  author={Rosas, Fernando E and Mediano, Pedro AM and Jensen, Henrik J and Seth, Anil K and Barrett, Adam B and Carhart-Harris, Robin L and Bor, Daniel},
  journal={PLoS computational biology},
  volume={16},
  number={12},
  pages={e1008289},
  year={2020},
  publisher={Public Library of Science San Francisco, CA USA}
}

@inproceedings{gondal2021function,
  title={Function contrastive learning of transferable meta-representations},
  author={Gondal, Muhammad Waleed and Joshi, Shruti and Rahaman, Nasim and Bauer, Stefan and Wuthrich, Manuel and Sch{\"o}lkopf, Bernhard},
  booktitle={International Conference on Machine Learning},
  pages={3755--3765},
  year={2021},
  organization={PMLR}
}

@article{rahaman2021dynamic,
  title={Dynamic inference with neural interpreters},
  author={Rahaman, Nasim and Gondal, Muhammad Waleed and Joshi, Shruti and Gehler, Peter and Bengio, Yoshua and Locatello, Francesco and Sch{\"o}lkopf, Bernhard},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={10985--10998},
  year={2021}
}

@article{higgins2018towards,
  title={Towards a definition of disentangled representations},
  author={Higgins, Irina and Amos, David and Pfau, David and Racaniere, Sebastien and Matthey, Loic and Rezende, Danilo and Lerchner, Alexander},
  journal={arXiv preprint arXiv:1812.02230},
  year={2018}
}


@InProceedings{pmlr-v97-locatello19a,
  title = 	 {Challenging Common Assumptions in the Unsupervised Learning of Disentangled Representations},
  author =       {Locatello, Francesco and Bauer, Stefan and Lucic, Mario and Raetsch, Gunnar and Gelly, Sylvain and Sch{\"o}lkopf, Bernhard and Bachem, Olivier},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {4114--4124},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/locatello19a/locatello19a.pdf},
  url = 	 {https://proceedings.mlr.press/v97/locatello19a.html},
  abstract = 	 {The key idea behind the unsupervised learning of disentangled representations is that real-world data is generated by a few explanatory factors of variation which can be recovered by unsupervised learning algorithms. In this paper, we provide a sober look at recent progress in the field and challenge some common assumptions. We first theoretically show that the unsupervised learning of disentangled representations is fundamentally impossible without inductive biases on both the models and the data. Then, we train more than $12000$ models covering most prominent methods and evaluation metrics in a reproducible large-scale experimental study on seven different data sets. We observe that while the different methods successfully enforce properties “encouraged” by the corresponding losses, well-disentangled models seemingly cannot be identified without supervision. Furthermore, increased disentanglement does not seem to lead to a decreased sample complexity of learning for downstream tasks. Our results suggest that future work on disentanglement learning should be explicit about the role of inductive biases and (implicit) supervision, investigate concrete benefits of enforcing disentanglement of the learned representations, and consider a reproducible experimental setup covering several data sets.}
}

@inproceedings{lecun-gradientbased-learning-applied-1998,
  added-at = {2010-06-28T21:14:36.000+0200},
  author = {LeCun, Yann and Bottou, Léon and Bengio, Yoshua and Haffner, Patrick},
  biburl = {https://www.bibsonomy.org/bibtex/29aa18bc67d862bdb83b6081e5506f050/mhwombat},
  booktitle = {Proceedings of the IEEE},
  citeseerurl = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.42.7665},
  file = {:neural_nets/lecun-98.pdf:PDF;:lecun-98.pdf:PDF},
  groups = {public},
  interhash = {7a82cccacd23cf06b25ff5325a6c86c7},
  intrahash = {9aa18bc67d862bdb83b6081e5506f050},
  keywords = {MSc character_recognition checked mnist network neural},
  number = 11,
  pages = {2278--2324},
  timestamp = {2016-07-12T19:25:30.000+0200},
  title = {Gradient-Based Learning Applied to Document
                 Recognition},
  url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.42.7665},
  username = {mhwombat},
  volume = 86,
  year = 1998
}

@inproceedings{10.5555/3495724.3497201,
author = {Benton, Gregory and Finzi, Marc and Izmailov, Pavel and Wilson, Andrew Gordon},
title = {Learning Invariances in Neural Networks},
year = {2020},
isbn = {9781713829546},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Invariances to translations have imbued convolutional neural networks with powerful generalization properties. However, we often do not know a priori what invariances are present in the data, or to what extent a model should be invariant to a given symmetry group. We show how to learn invariances and equivariances by parameterizing a distribution over augmentations and optimizing the training loss simultaneously with respect to the network parameters and augmentation parameters. With this simple procedure we can recover the correct set and extent of invariances on image classification, regression, segmentation, and molecular property prediction from a large space of augmentations, on training data alone.},
booktitle = {Proceedings of the 34th International Conference on Neural Information Processing Systems},
articleno = {1477},
numpages = {12},
location = {Vancouver, BC, Canada},
series = {NIPS'20}
}

@article{Zhou2020MetaLearningSB,
  title={Meta-Learning Symmetries by Reparameterization},
  author={Allan Zhou and Tom Knowles and Chelsea Finn},
  journal={ArXiv},
  year={2020},
  volume={abs/2007.02933},
  url={https://api.semanticscholar.org/CorpusID:220363897}
}

@article{Krippendorf_2021,
doi = {10.1088/2632-2153/abbd2d},
url = {https://dx.doi.org/10.1088/2632-2153/abbd2d},
year = {2020},
month = {dec},
publisher = {IOP Publishing},
volume = {2},
number = {1},
pages = {015010},
author = {Sven Krippendorf and Marc Syvaeri},
title = {Detecting symmetries with neural networks},
journal = {Machine Learning: Science and Technology},
abstract = {Identifying symmetries in data sets is generally difficult, but knowledge about them is crucial for efficient data handling. Here we present a method how neural networks can be used to identify symmetries. We make extensive use of the structure in the embedding layer of the neural network which allows us to identify whether a symmetry is present and to identify orbits of the symmetry in the input. To determine which continuous or discrete symmetry group is present we analyse the invariant orbits in the input. We present examples based on rotation groups SO(n) and the unitary group SU(2). Further we find that this method is useful for the classification of complete intersection Calabi-Yau manifolds where it is crucial to identify discrete symmetries on the input space. For this example we present a novel data representation in terms of graphs.}
}

@inproceedings{Ren2021LearningDR,
  title={Learning Disentangled Representation by Exploiting Pretrained Generative Models: A Contrastive Learning View},
  author={Xuanchi Ren and Tao Yang and Yuwang Wang and Wen Jun Zeng},
  booktitle={International Conference on Learning Representations},
  year={2021},
  url={https://api.semanticscholar.org/CorpusID:246823979}
}

@inproceedings{karras2020analyzing,
  title={Analyzing and improving the image quality of stylegan},
  author={Karras, Tero and Laine, Samuli and Aittala, Miika and Hellsten, Janne and Lehtinen, Jaakko and Aila, Timo},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={8110--8119},
  year={2020}
}


@article{Khrulkov2021DisentangledRF,
  title={Disentangled Representations from Non-Disentangled Models},
  author={Valentin Khrulkov and Leyla Mirvakhabova and I. Oseledets and Artem Babenko},
  journal={ArXiv},
  year={2021},
  volume={abs/2102.06204},
  url={https://api.semanticscholar.org/CorpusID:231879656}
}

@book{lee1998independent,
  title={Independent component analysis},
  author={Lee, Te-Won and Lee, Te-Won},
  year={1998},
  publisher={Springer}
}

@article{candes2006robust,
  title={Robust uncertainty principles: Exact signal reconstruction from highly incomplete frequency information},
  author={Cand{\`e}s, Emmanuel J and Romberg, Justin and Tao, Terence},
  journal={IEEE Transactions on information theory},
  volume={52},
  number={2},
  pages={489--509},
  year={2006},
  publisher={IEEE}
}

@article{aharon2006k,
  title={K-SVD: An algorithm for designing overcomplete dictionaries for sparse representation},
  author={Aharon, Michal and Elad, Michael and Bruckstein, Alfred},
  journal={IEEE Transactions on signal processing},
  volume={54},
  number={11},
  pages={4311--4322},
  year={2006},
  publisher={IEEE}
}

@article{lee1999learning,
  title={Learning the parts of objects by non-negative matrix factorization},
  author={Lee, Daniel D and Seung, H Sebastian},
  journal={nature},
  volume={401},
  number={6755},
  pages={788--791},
  year={1999},
  publisher={Nature Publishing Group UK London}
}

@inproceedings{lee2019set,
  title={Set transformer: A framework for attention-based permutation-invariant neural networks},
  author={Lee, Juho and Lee, Yoonho and Kim, Jungtaek and Kosiorek, Adam and Choi, Seungjin and Teh, Yee Whye},
  booktitle={International conference on machine learning},
  pages={3744--3753},
  year={2019},
  organization={PMLR}
}

@article{lippe2023biscuit,
  title={BISCUIT: Causal Representation Learning from Binary Interactions},
  author={Lippe, Phillip and Magliacane, Sara and L{\"o}we, Sindy and Asano, Yuki M and Cohen, Taco and Gavves, Efstratios},
  journal={arXiv preprint arXiv:2306.09643},
  year={2023}
}

@misc{schölkopf2022statistical,
      title={From Statistical to Causal Learning}, 
      author={Bernhard Schölkopf and Julius von Kügelgen},
      year={2022},
      eprint={2204.00607},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

@book{10.5555/3202377,
author = {Peters, Jonas and Janzing, Dominik and Schlkopf, Bernhard},
title = {Elements of Causal Inference: Foundations and Learning Algorithms},
year = {2017},
isbn = {0262037319},
publisher = {The MIT Press},
abstract = {A concise and self-contained introduction to causal inference, increasingly important in data science and machine learning. The mathematization of causality is a relatively recent development, and has become increasingly important in data science and machine learning. This book offers a self-contained and concise introduction to causal models and how to learn them from data. After explaining the need for causal models and discussing some of the principles underlying causal inference, the book teaches readers how to use causal models: how to compute intervention distributions, how to infer causal models from observational and interventional data, and how causal ideas could be exploited for classical machine learning problems. All of these topics are discussed first in terms of two variables and then in the more general multivariate case. The bivariate case turns out to be a particularly hard problem for causal learning because there are no conditional independences as used by classical methods for solving multivariate cases. The authors consider analyzing statistical asymmetries between cause and effect to be highly instructive, and they report on their decade of intensive research into this problem. The book is accessible to readers with a background in machine learning or statistics, and can be used in graduate courses or as a reference for researchers. The text includes code snippets that can be copied and pasted, exercises, and an appendix with a summary of the most important technical concepts.}
}


@misc{locatello2020weaklysupervised,
      title={Weakly-Supervised Disentanglement Without Compromises}, 
      author={Francesco Locatello and Ben Poole and Gunnar Rätsch and Bernhard Schölkopf and Olivier Bachem and Michael Tschannen},
      year={2020},
      eprint={2002.02886},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{xu2024sparsityprinciplepartiallyobservable,
title={A Sparsity Principle for Partially Observable Causal Representation Learning},
author={Danru Xu and Dingling Yao and Sébastien Lachapelle and Perouz Taslakian and Julius von Kügelgen and Francesco Locatello and Sara Magliacane},
booktitle={Proceedings of the 41 st International Conference on Machine
Learning},
year={2024}
}

@article{elhage2022superposition,
   title={Toy Models of Superposition},
   author={Elhage, Nelson and Hume, Tristan and Olsson, Catherine and Schiefer, Nicholas and Henighan, Tom and Kravec, Shauna and Hatfield-Dodds, Zac and Lasenby, Robert and Drain, Dawn and Chen, Carol and Grosse, Roger and McCandlish, Sam and Kaplan, Jared and Amodei, Dario and Wattenberg, Martin and Olah, Christopher},
   year={2022},
   journal={Transformer Circuits Thread},
   url={https://transformer-circuits.pub/2022/toy_model/index.html}
}

@article{jermyn2022engineering,
  title={Engineering monosemanticity in toy models},
  author={Jermyn, Adam S and Schiefer, Nicholas and Hubinger, Evan},
  journal={arXiv preprint arXiv:2211.09169},
  year={2022}
}

@misc{gidel2020variationalinequalityperspectivegenerative,
      title={A Variational Inequality Perspective on Generative Adversarial Networks}, 
      author={Gauthier Gidel and Hugo Berard and Gaëtan Vignoud and Pascal Vincent and Simon Lacoste-Julien},
      year={2020},
      eprint={1802.10551},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1802.10551}, 
}

@misc{stephane1999wavelet,
  title={A wavelet tour of signal processing},
  author={Stephane, Mallat},
  year={1999},
  publisher={Elsevier}
}

@misc{anders_etal_2024_composedtoymodels_2d,
   title = { Sparse autoencoders find composed features in small toy models  },
   author = {Anders, Evan AND Neo, Clement AND Hoelscher-Obermaier, Jason AND Howard, Jessica N.},
   year = {2024},
   howpublished = {\url{https://www.lesswrong.com/posts/a5wwqza2cY3W7L9cj/sparse-autoencoders-find-composed-features-in-small-toy}},
}

@article{locatello2020sober,
  title={A sober look at the unsupervised learning of disentangled representations and their evaluation},
  author={Locatello, Francesco and Bauer, Stefan and Lucic, Mario and R{\"a}tsch, Gunnar and Gelly, Sylvain and Sch{\"o}lkopf, Bernhard and Bachem, Olivier},
  journal={Journal of Machine Learning Research},
  volume={21},
  number={209},
  pages={1--62},
  year={2020}
}

@inproceedings{eastwood2018framework,
  title={A framework for the quantitative evaluation of disentangled representations},
  author={Eastwood, Cian and Williams, Christopher KI},
  booktitle={6th International Conference on Learning Representations},
  year={2018}
}

@article{duan2019unsupervised,
  title={Unsupervised model selection for variational disentangled representation learning},
  author={Duan, Sunny and Matthey, Loic and Saraiva, Andre and Watters, Nicholas and Burgess, Christopher P and Lerchner, Alexander and Higgins, Irina},
  journal={arXiv preprint arXiv:1905.12614},
  year={2019}
}

@inproceedings{lachapelle2022disentanglement,
  title={Disentanglement via mechanism sparsity regularization: A new principle for nonlinear ICA},
  author={Lachapelle, S{\'e}bastien and Rodriguez, Pau and Sharma, Yash and Everett, Katie E and Le Priol, R{\'e}mi and Lacoste, Alexandre and Lacoste-Julien, Simon},
  booktitle={Conference on Causal Learning and Reasoning},
  pages={428--484},
  year={2022},
  organization={PMLR}
}

@misc{engels2024languagemodelfeatureslinear,
      title={Not All Language Model Features Are Linear}, 
      author={Joshua Engels and Isaac Liao and Eric J. Michaud and Wes Gurnee and Max Tegmark},
      year={2024},
      eprint={2405.14860},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2405.14860}, 
}

@misc{oneill2024disentanglingdenseembeddingssparse,
      title={Disentangling Dense Embeddings with Sparse Autoencoders}, 
      author={Charles O'Neill and Christine Ye and Kartheik Iyer and John F. Wu},
      year={2024},
      eprint={2408.00657},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2408.00657}, 
}

@misc{khemakhem2020icebeemidentifiableconditionalenergybased,
      title={ICE-BeeM: Identifiable Conditional Energy-Based Deep Models Based on Nonlinear ICA}, 
      author={Ilyes Khemakhem and Ricardo Pio Monti and Diederik P. Kingma and Aapo Hyvärinen},
      year={2020},
      eprint={2002.11537},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/2002.11537}, 
}

@InProceedings{iVAEkhemakhem20a,
  title = 	 {Variational Autoencoders and Nonlinear ICA: A Unifying Framework},
  author =       {Khemakhem, I. and Kingma, D. and Monti, R. and Hyvärinen, A.},
  booktitle = 	 {Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics},
  year = 	 {2020}
}

@article{nissim2020fair,
  title={Fair is better than sensational: Man is to doctor as woman is to doctor},
  author={Nissim, Malvina and van Noord, Rik and Van Der Goot, Rob},
  journal={Computational Linguistics},
  volume={46},
  number={2},
  pages={487--497},
  year={2020},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}

@misc{bengio2009learning,
  title={Learning Deep Architectures for AI},
  author={Bengio, Y},
  year={2009},
  publisher={Now Publishers Inc}
}

@article{bengio2013representation,
  title={Representation learning: A review and new perspectives},
  author={Bengio, Yoshua and Courville, Aaron and Vincent, Pascal},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={35},
  number={8},
  pages={1798--1828},
  year={2013},
  publisher={IEEE}
}

@misc{arjovsky2020invariantriskminimization,
      title={Invariant Risk Minimization}, 
      author={Martin Arjovsky and Léon Bottou and Ishaan Gulrajani and David Lopez-Paz},
      year={2020},
      eprint={1907.02893},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/1907.02893}, 
}

%here on for lrh in llms

@misc{tigges2023linearrepresentationssentimentlarge,
      title={Linear Representations of Sentiment in Large Language Models}, 
      author={Curt Tigges and Oskar John Hollinsworth and Atticus Geiger and Neel Nanda},
      year={2023},
      eprint={2310.15154},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2310.15154}, 
}

@article{levy2014neural,
  title={Neural word embedding as implicit matrix factorization},
  author={Levy, Omer and Goldberg, Yoav},
  journal={Advances in neural information processing systems},
  volume={27},
  year={2014}
}

@inproceedings{gittens-etal-2017-skip,
    title = "Skip-Gram - {Z}ipf + Uniform = Vector Additivity",
    author = "Gittens, Alex  and
      Achlioptas, Dimitris  and
      Mahoney, Michael W.",
    editor = "Barzilay, Regina  and
      Kan, Min-Yen",
    booktitle = "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2017",
    address = "Vancouver, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P17-1007",
    doi = "10.18653/v1/P17-1007",
    pages = "69--76",
    abstract = "In recent years word-embedding models have gained great popularity due to their remarkable performance on several tasks, including word analogy questions and caption generation. An unexpected {``}side-effect{''} of such models is that their vectors often exhibit compositionality, i.e., \textit{adding}two word-vectors results in a vector that is only a small angle away from the vector of a word representing the semantic composite of the original words, e.g., {``}man{''} + {``}royal{''} = {``}king{''}. This work provides a theoretical justification for the presence of additive compositionality in word vectors learned using the Skip-Gram model. In particular, it shows that additive compositionality holds in an even stricter sense (small distance rather than small angle) under certain assumptions on the process generating the corpus. As a corollary, it explains the success of vector calculus in solving word analogies. When these assumptions do not hold, this work describes the correct non-linear composition operator. Finally, this work establishes a connection between the Skip-Gram model and the Sufficient Dimensionality Reduction (SDR) framework of Globerson and Tishby: the parameters of SDR models can be obtained from those of Skip-Gram models simply by adding information on symbol frequencies. This shows that Skip-Gram embeddings are optimal in the sense of Globerson and Tishby and, further, implies that the heuristics commonly used to approximately fit Skip-Gram models can be used to fit SDR models.",
}

@misc{ethayarajh2019understandinglinearwordanalogies,
      title={Towards Understanding Linear Word Analogies}, 
      author={Kawin Ethayarajh and David Duvenaud and Graeme Hirst},
      year={2019},
      eprint={1810.04882},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1810.04882}, 
}

@article{von2021self,
  title={Self-supervised learning with data augmentations provably isolates content from style},
  author={Von K{\"u}gelgen, Julius and Sharma, Yash and Gresele, Luigi and Brendel, Wieland and Sch{\"o}lkopf, Bernhard and Besserve, Michel and Locatello, Francesco},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={16451--16467},
  year={2021}
}

@misc{allen2019analogiesexplainedunderstandingword,
      title={Analogies Explained: Towards Understanding Word Embeddings}, 
      author={Carl Allen and Timothy Hospedales},
      year={2019},
      eprint={1901.09813},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1901.09813}, 
}

@inproceedings{seonwoo-etal-2019-additive,
    title = "Additive Compositionality of Word Vectors",
    author = "Seonwoo, Yeon  and
      Park, Sungjoon  and
      Kim, Dongkwan  and
      Oh, Alice",
    editor = "Xu, Wei  and
      Ritter, Alan  and
      Baldwin, Tim  and
      Rahimi, Afshin",
    booktitle = "Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT 2019)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-5551",
    doi = "10.18653/v1/D19-5551",
    pages = "387--396",
    abstract = "Additive compositionality of word embedding models has been studied from empirical and theoretical perspectives. Existing research on justifying additive compositionality of existing word embedding models requires a rather strong assumption of uniform word distribution. In this paper, we relax that assumption and propose more realistic conditions for proving additive compositionality, and we develop a novel word and sub-word embedding model that satisfies additive compositionality under those conditions. We then empirically show our model{'}s improved semantic representation performance on word similarity and noisy sentence similarity.",
}

@misc{burns2024discoveringlatentknowledgelanguage,
      title={Discovering Latent Knowledge in Language Models Without Supervision}, 
      author={Collin Burns and Haotian Ye and Dan Klein and Jacob Steinhardt},
      year={2024},
      eprint={2212.03827},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2212.03827}, 
}

@misc{li2024inferencetimeinterventionelicitingtruthful,
      title={Inference-Time Intervention: Eliciting Truthful Answers from a Language Model}, 
      author={Kenneth Li and Oam Patel and Fernanda Viégas and Hanspeter Pfister and Martin Wattenberg},
      year={2024},
      eprint={2306.03341},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2306.03341}, 
}


@misc{nanda2023emergentlinearrepresentationsworld,
      title={Emergent Linear Representations in World Models of Self-Supervised Sequence Models}, 
      author={Neel Nanda and Andrew Lee and Martin Wattenberg},
      year={2023},
      eprint={2309.00941},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2309.00941}, 
}

% crl or crl adjacent work in understanding lrh in llms

@misc{jiang2024originslinearrepresentationslarge,
      title={On the Origins of Linear Representations in Large Language Models}, 
      author={Yibo Jiang and Goutham Rajendran and Pradeep Ravikumar and Bryon Aragam and Victor Veitch},
      year={2024},
      eprint={2403.03867},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2403.03867}, 
}

@inproceedings{ravfogel-etal-2020-null,
    title = "Null It Out: Guarding Protected Attributes by Iterative Nullspace Projection",
    author = "Ravfogel, Shauli  and
      Elazar, Yanai  and
      Gonen, Hila  and
      Twiton, Michael  and
      Goldberg, Yoav",
    editor = "Jurafsky, Dan  and
      Chai, Joyce  and
      Schluter, Natalie  and
      Tetreault, Joel",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.647",
    doi = "10.18653/v1/2020.acl-main.647",
    pages = "7237--7256",
    abstract = "The ability to control for the kinds of information encoded in neural representation has a variety of use cases, especially in light of the challenge of interpreting these models. We present Iterative Null-space Projection (INLP), a novel method for removing information from neural representations. Our method is based on repeated training of linear classifiers that predict a certain property we aim to remove, followed by projection of the representations on their null-space. By doing so, the classifiers become oblivious to that target property, making it hard to linearly separate the data according to it. While applicable for multiple uses, we evaluate our method on bias and fairness use-cases, and show that our method is able to mitigate bias in word embeddings, as well as to increase fairness in a setting of multi-class classification.",
}

@article{nissim-etal-2020-fair,
    title = "Fair Is Better than Sensational: Man Is to Doctor as Woman Is to Doctor",
    author = "Nissim, Malvina  and
      van Noord, Rik  and
      van der Goot, Rob",
    journal = "Computational Linguistics",
    volume = "46",
    number = "2",
    month = jun,
    year = "2020",
    url = "https://aclanthology.org/2020.cl-2.7",
    doi = "10.1162/coli_a_00379",
    pages = "487--497",
    abstract = "Analogies such as man is to king as woman is to X are often used to illustrate the amazing power of word embeddings. Concurrently, they have also been used to expose how strongly human biases are encoded in vector spaces trained on natural language, with examples like man is to computer programmer as woman is to homemaker. Recent work has shown that analogies are in fact not an accurate diagnostic for bias, but this does not mean that they are not used anymore, or that their legacy is fading. Instead of focusing on the intrinsic problems of the analogy task as a bias detection tool, we discuss a series of issues involving implementation as well as subjective choices that might have yielded a distorted picture of bias in word embeddings. We stand by the truth that human biases are present in word embeddings, and, of course, the need to address them. But analogies are not an accurate tool to do so, and the way they have been most often used has exacerbated some possibly non-existing biases and perhaps hidden others. Because they are still widely popular, and some of them have become classics within and outside the NLP community, we deem it important to provide a series of clarifications that should put well-known, and potentially new analogies, into the right perspective.",
}


@misc{moschella2023relativerepresentationsenablezeroshot,
      title={Relative representations enable zero-shot latent space communication}, 
      author={Luca Moschella and Valentino Maiorca and Marco Fumero and Antonio Norelli and Francesco Locatello and Emanuele Rodolà},
      year={2023},
      eprint={2209.15430},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2209.15430}, 
}

@book{21405c9e7de549fcbe32ba1ed9961819,
title = "Independent Component Analysis",
keywords = "blind source separation, independent component analysis, nongaussian statistics, blind source separation, independent component analysis, nongaussian statistics, blind source separation, independent component analysis, nongaussian statistics",
author = "Aapo Hyv{\"a}rinen and J. Karhunen and E. Oja",
year = "2001",
doi = "10.1002/0471221317",
language = "English",
isbn = "9780471405405",
publisher = "John Wiley & Sons",
address = "United Kingdom",
}

@article{le2011ica,
  title={ICA with reconstruction cost for efficient overcomplete feature learning},
  author={Le, Quoc and Karpenko, Alexandre and Ngiam, Jiquan and Ng, Andrew},
  journal={Advances in neural information processing systems},
  volume={24},
  year={2011}
}

@misc{cunningham2023sparseautoencodershighlyinterpretable,
      title={Sparse Autoencoders Find Highly Interpretable Features in Language Models}, 
      author={Hoagy Cunningham and Aidan Ewart and Logan Riggs and Robert Huben and Lee Sharkey},
      year={2023},
      eprint={2309.08600},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2309.08600}, 
}
@misc{lippe2022citriscausalidentifiabilitytemporal,
      title={CITRIS: Causal Identifiability from Temporal Intervened Sequences}, 
      author={Phillip Lippe and Sara Magliacane and Sindy Löwe and Yuki M. Asano and Taco Cohen and Efstratios Gavves},
      year={2022},
      eprint={2202.03169},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2202.03169}, 
}

@misc{layne2024sparsityregularizationtreestructuredenvironments,
      title={Sparsity regularization via tree-structured environments for disentangled representations}, 
      author={Elliot Layne and Jason Hartford and Sébastien Lachapelle and Mathieu Blanchette and Dhanya Sridhar},
      year={2024},
      eprint={2405.20482},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2405.20482}, 
}

@article{marconato2024all,
  title={All or None: Identifiable Linear Properties of Next-token Predictors in Language Modeling},
  author={Marconato, Emanuele and Lachapelle, S{\'e}bastien and Weichwald, Sebastian and Gresele, Luigi},
  journal={arXiv preprint arXiv:2410.23501},
  year={2024}
}

@article{ravfogel2020null,
  title={Null it out: Guarding protected attributes by iterative nullspace projection},
  author={Ravfogel, Shauli and Elazar, Yanai and Gonen, Hila and Twiton, Michael and Goldberg, Yoav},
  journal={arXiv preprint arXiv:2004.07667},
  year={2020}
}


@misc{brehmer2022weaklysupervisedcausalrepresentation,
      title={Weakly supervised causal representation learning}, 
      author={Johann Brehmer and Pim de Haan and Phillip Lippe and Taco Cohen},
      year={2022},
      eprint={2203.16437},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/2203.16437}, 
}

@inproceedings{locatello2020weakly,
  title={Weakly-supervised disentanglement without compromises},
  author={Locatello, Francesco and Poole, Ben and R{\"a}tsch, Gunnar and Sch{\"o}lkopf, Bernhard and Bachem, Olivier and Tschannen, Michael},
  booktitle={International conference on machine learning},
  pages={6348--6359},
  year={2020},
  organization={PMLR}
}

@misc{fumero2023leveragingsparsesharedfeature,
      title={Leveraging sparse and shared feature activations for disentangled representation learning}, 
      author={Marco Fumero and Florian Wenzel and Luca Zancato and Alessandro Achille and Emanuele Rodolà and Stefano Soatto and Bernhard Schölkopf and Francesco Locatello},
      year={2023},
      eprint={2304.07939},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2304.07939}, 
}

@misc{panickssery2024steeringllama2contrastive,
      title={Steering Llama 2 via Contrastive Activation Addition}, 
      author={Nina Panickssery and Nick Gabrieli and Julian Schulz and Meg Tong and Evan Hubinger and Alexander Matt Turner},
      year={2024},
      eprint={2312.06681},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2312.06681}, 
}

@article{templeton2024scaling,
       title={Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet},
       author={Templeton, Adly and Conerly, Tom and Marcus, Jonathan and Lindsey, Jack and Bricken, Trenton and Chen, Brian and Pearce, Adam and Citro, Craig and Ameisen, Emmanuel and Jones, Andy and Cunningham, Hoagy and Turner, Nicholas L and McDougall, Callum and MacDiarmid, Monte and Freeman, C. Daniel and Sumers, Theodore R. and Rees, Edward and Batson, Joshua and Jermyn, Adam and Carter, Shan and Olah, Chris and Henighan, Tom},
       year={2024},
       journal={Transformer Circuits Thread},
       url={https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html}
    }

@article{engels2024not,
  title={Not All Language Model Features Are Linear},
  author={Engels, Joshua and Liao, Isaac and Michaud, Eric J and Gurnee, Wes and Tegmark, Max},
  journal={arXiv preprint arXiv:2405.14860},
  year={2024}
}

@misc{rajamanoharan2024improvingdictionarylearninggated,
      title={Improving Dictionary Learning with Gated Sparse Autoencoders}, 
      author={Senthooran Rajamanoharan and Arthur Conmy and Lewis Smith and Tom Lieberum and Vikrant Varma and János Kramár and Rohin Shah and Neel Nanda},
      year={2024},
      eprint={2404.16014},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2404.16014}, 
}



%%%

@inproceedings{roeder2021linear,
  title={On linear identifiability of learned representations},
  author={Roeder, Geoffrey and Metz, Luke and Kingma, Durk},
  booktitle={International Conference on Machine Learning},
  pages={9030--9039},
  year={2021},
  organization={PMLR}
}

@inproceedings{Mairal_Bach_Ponce2009,
  title={Online dictionary learning for sparse coding},
  author={J. Mairal and F. Bach and J. Ponce and G. Sapiro},
  booktitle={Proceedings of the 26th annual international conference on machine learning},
  pages={689--696},
  year={2009}
}

@InProceedings{lachapelle2023synergies,
  title = 	 {Synergies between Disentanglement and Sparsity: Generalization and Identifiability in Multi-Task Learning},
  author =       {Lachapelle, Sebastien and Deleu, T. and Mahajan, D. and Mitliagkas, I. and Bengio, Y. and Lacoste-Julien, S. and Bertrand, Q.},
  booktitle = 	 {International Conference on Machine Learning},
  year = 	 {2023}
}

@article{rumelhart1973model,
  title={A model for analogical reasoning},
  author={Rumelhart, David E and Abrahamson, Adele A},
  journal={Cognitive Psychology},
  volume={5},
  number={1},
  pages={1--28},
  year={1973},
  publisher={Elsevier}
}


@inproceedings{hinton1986learning,
  title={Learning distributed representations of concepts},
  author={Hinton, Geoffrey E and others},
  booktitle={Proceedings of the eighth annual conference of the cognitive science society},
  volume={1},
  pages={12},
  year={1986},
  organization={Amherst, MA}
}

@article{korpelevich1976extragradient,
  title={The extragradient method for finding saddle points and other problems},
  author={Korpelevich, Galina M},
  journal={Matecon},
  volume={12},
  pages={747--756},
  year={1976}
}

@misc{dubey2024llama3herdmodels,
      title={The Llama 3 Herd of Models}, 
      author={{Llama Team} and others},
      year={2024},
      eprint={2407.21783},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2407.21783}, 
}

@misc{rolinek2019variationalautoencoderspursuepca,
      title={Variational Autoencoders Pursue PCA Directions (by Accident)}, 
      author={Michal Rolinek and Dominik Zietlow and Georg Martius},
      year={2019},
      eprint={1812.06775},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1812.06775}, 
}

@misc{ma2023finetuningllamamultistagetext,
      title={Fine-Tuning LLaMA for Multi-Stage Text Retrieval}, 
      author={Xueguang Ma and Liang Wang and Nan Yang and Furu Wei and Jimmy Lin},
      year={2023},
      eprint={2310.08319},
      archivePrefix={arXiv},
      primaryClass={cs.IR},
      url={https://arxiv.org/abs/2310.08319}, 
}

@misc{ioffe2015batchnormalizationacceleratingdeep,
      title={Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift}, 
      author={Sergey Ioffe and Christian Szegedy},
      year={2015},
      eprint={1502.03167},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1502.03167}, 
}


@misc{li2024wmdpbenchmarkmeasuringreducing,
      title={The WMDP Benchmark: Measuring and Reducing Malicious Use With Unlearning}, 
      author={Nathaniel Li and Alexander Pan and Anjali Gopal and Summer Yue and Daniel Berrios and Alice Gatti and Justin D. Li and Ann-Kathrin Dombrowski and Shashwat Goel and Long Phan and Gabriel Mukobi and Nathan Helm-Burger and Rassin Lababidi and Lennart Justen and Andrew B. Liu and Michael Chen and Isabelle Barrass and Oliver Zhang and Xiaoyuan Zhu and Rishub Tamirisa and Bhrugu Bharathi and Adam Khoja and Zhenqi Zhao and Ariel Herbert-Voss and Cort B. Breuer and Samuel Marks and Oam Patel and Andy Zou and Mantas Mazeika and Zifan Wang and Palash Oswal and Weiran Lin and Adam A. Hunt and Justin Tienken-Harder and Kevin Y. Shih and Kemper Talley and John Guan and Russell Kaplan and Ian Steneker and David Campbell and Brad Jokubaitis and Alex Levinson and Jean Wang and William Qian and Kallol Krishna Karmakar and Steven Basart and Stephen Fitz and Mindy Levine and Ponnurangam Kumaraguru and Uday Tupakula and Vijay Varadharajan and Ruoyu Wang and Yan Shoshitaishvili and Jimmy Ba and Kevin M. Esvelt and Alexandr Wang and Dan Hendrycks},
      year={2024},
      eprint={2403.03218},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2403.03218}, 
}

@misc{hyvarinen2016unsupervisedfeatureextractiontimecontrastive,
      title={Unsupervised Feature Extraction by Time-Contrastive Learning and Nonlinear ICA}, 
      author={Aapo Hyvarinen and Hiroshi Morioka},
      year={2016},
      eprint={1605.06336},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/1605.06336}, 
}

@article{HYVARINEN1999429,
title = {Nonlinear independent component analysis: Existence and uniqueness results},
journal = {Neural Networks},
year = {1999},
author = {Hyvärinen, A. and Pajunen, P.}
}

@misc{lesswrong_featuresuppression202,
  title = {Addressing Feature Suppression in SAEs},
  author = {{LessWrong Community}},
  year = {2024},
  url = {https://www.lesswrong.com/posts/3JuSjTZyMzaSeTxKk/addressing-feature-suppression-in-saes#Fine_tuning_Reduces_Feature_Suppression},
  note = {Accessed: 2024-10-01},
}


@article{bricken2023monosemanticity,
       title={Towards Monosemanticity: Decomposing Language Models With Dictionary Learning},
       author={Bricken, Trenton and Templeton, Adly and Batson, Joshua and Chen, Brian and Jermyn, Adam and Conerly, Tom and Turner, Nick and Anil, Cem and Denison, Carson and Askell, Amanda and Lasenby, Robert and Wu, Yifan and Kravec, Shauna and Schiefer, Nicholas and Maxwell, Tim and Joseph, Nicholas and Hatfield-Dodds, Zac and Tamkin, Alex and Nguyen, Karina and McLean, Brayden and Burke, Josiah E and Hume, Tristan and Carter, Shan and Henighan, Tom and Olah, Christopher},
       year={2023},
       journal={Transformer Circuits Thread},
       url={https://transformer-circuits.pub/2023/monosemantic-features/index.html}
    }

@article{gao2024scaling,
  title={Scaling and evaluating sparse autoencoders},
  author={Gao, Leo and la Tour, Tom Dupr{\'e} and Tillman, Henk and Goh, Gabriel and Troll, Rajan and Radford, Alec and Sutskever, Ilya and Leike, Jan and Wu, Jeffrey},
  journal={arXiv preprint arXiv:2406.04093},
  year={2024}
}

@misc{gallegoPosada2022cooper,
    author={Gallego-Posada, Jose and Ramirez, Juan},
    title={{Cooper: a toolkit for Lagrangian-based constrained optimization}},
    howpublished={\url{https://github.com/cooper-org/cooper}},
    year={2022}
}

@misc{lin2022truthfulqameasuringmodelsmimic,
      title={TruthfulQA: Measuring How Models Mimic Human Falsehoods}, 
      author={Stephanie Lin and Jacob Hilton and Owain Evans},
      year={2022},
      eprint={2109.07958},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2109.07958}, 
}