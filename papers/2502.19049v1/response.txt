\section{Related Work}
\label{sec:related-work}

As we noted in the Introduction, most available solutions tackling the 
data-driven drift and diffusion estimation problem proceed primarily through Bayesian non-parametrics and symbolic regression.
%
In general, when the observed data is noisy, sparse in time, or both, one faces uncertainty not only in determining the drift and diffusion of the putative SDE, but also in the state of the system itself.
%
Therefore, a Bayesian treatment requires the estimation of the posterior distribution over these states, conditioned on the noisy data (\textit{i.e.}~the so-called smoothing problem).
%
Starting with the seminal works of Rasmussen, "Gaussian Processes for Machine Learning", 
 %
most proposals have mainly focused on devising different strategies to infer the smoothing distribution --- \textit{while assuming prior parametric forms for the drift and diffusion functions}. See \textit{e.g.} Kalman, "A New Approach to Linear Filtering and Prediction Problems",  Arulampalam, "A Tutorial on Particle Filters for Online Nonlinear/Non-Gaussian Bayesian Tracking" , or the recent work by Fox, "Bayesian Methods for Chaotic Systems".
%
A notable exception is the proposal of Opper, "Variational Inference for Non-Parametric Gaussian Process Models",  which extended the variational trick of Rasmussen by imposing a non-parametric prior over the drift of the prior process.
%
However, Snelson's trick has been shown to suffer from significant convergence issues Snelson, "Deep Kalman Filters" , which are inherited by Opper's model.
%
In contrast, Bertoluzzo framed the drift-diffusion estimation problem from uncorrupted (\textit{i.e.}~clean and dense) data as a Gaussian process regression problem,
%
and extended this (non-parametric) approach to observations that are sparse in time, by leveraging Orstein-Uhlenbeck bridges optimized with an expectation maximization algorithm.
%
Nevertheless, this extension can only deal with non-parametric drifts (\textit{i.e.}~the diffusion is restricted to parametric forms) and clean data, and is highly sensitive to the choice of prior hyperparameters.
%
% We will compare our methodology against that of Papamakarios, "The Normalising Flows are Non-Reversible".
% Despite the recent proliferation of symbolic regression methods, only few of these tackle function estimation for SDEs (see \textit{e.g.}~the reviews by Raissi, "Physics-Informed Neural Networks: A Deep Learning Framework for Solving Forward and Inverse Problems Involving Nonlinear Partial Differential Equations"  and Wilson, "Deep Learning and Process Control")
%
Symbolic regression methods for drift and diffusion estimation mainly extend the SINDy algorithm Brunton, "Discovering governing equations from data: a review of the physics-informed neural network approach" --- which performs sparse linear regression on a predefined library of candidate nonlinear functions --- to SDEs.
%
The first of these extensions corresponds to the work of Guglielmi, "A Variational Approach to State Estimation in Stochastic Differential Equations",  which sets the regression problem by approximating the local values of the drift and diffusion functions with the empirical expectations of Eqs~\ref{eq:drift-definition} and \ref{eq:diffusion-definition} of Section~\ref{sec:preliminaries}.  
%
However, the calculation of these local expectations generally requires significant amounts of data, even for one-dimensional systems.
%
To (somewhat) alleviate this issue, Chen and Li  recently resorted to sparse Bayesian learning, but their solutions still require sizable dataset sizes and, most problematically, assume access to clean and dense observations.
%
In response to this limitations, Ma proposed a hybrid solution that leveraged the variational trick of Rasmussen, while allowing the drift of the prior process to be approximated by a sparse, linear combination of known basis functions.
%
However, their model inherits the slow convergence problems of the variational approximation. 
%
What is more, all SINDy-like methods are limited by construction to linear combination of the functions in their library, 
%
which therefore makes their performance highly dependent on the preselected functions within the library.

Finally, most neural network models for SDEs rely on black-box parameterizations of the drift and diffusion functions for path (\textit{i.e.}~state) generation. 
%
Prominent examples include the works by Chen, "Neural Ordinary Differential Equations",  Li, "Physics-informed neural networks: a deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations",  Qin, "A Variational Approach to State Estimation in Stochastic Differential Equations" , and  Chen, "Deep Learning of Dynamical Systems from Partial Measurements".
%
To the best of our knowledge, our proposal is the first neural network and non-parametric solution to the problem of SDE function estimation --- \textit{and the first to do so in a zero-shot fashion}.

% adjoint computation introduces a high computational burden, leading to stiffness and limited parallelization, which heavily constrains the success of these approaches.
%
%
% In the context of approximate Bayesian inference, the problem of learning the drift and diffusion functions from data has been tackle with the use of Gaussian process priors. In the seminal work of , variational inference is performed for diffusion processes, where the approximating variational family consists of time-variant linear diffusion process. Improvements of this approach include moment based methodologies . Improvement of learning from fixed point ____. Wishard diffusion ____. Interpretability conditioning on a set of learned fixed points and local jacobian dynamics . For non linear diffusions, regression was been achieved via EM algorithms using Orstein Uhlenbeck bridges as data likelihoods ____. All this approaches are based on non parametric estimates based on GPs approximations, and requires a different training per dataset i.e. these are non amortized methods. Furthermore, they typically required if not adjoint methods, forward and backwards paths that are computationally costly.
%
% In the realm of deep neural networks, the seminal paper by Chen introduced Neural ODEs (NODE), where the vector field is parameterized using a neural network and trained via backpropagation through the solver using adjoint methods. This approach enabled them to handle continuous dynamics and irregularly sampled time series ____. Attempts to incorporate stochastic dynamics have explored neural processes ____, variational autoencoders ____, and the training of SDE operators as a form of infinite-dimensional GANs ____ or diffusion models ____. This methodologies however requiere adjoint estimates as well as large amount fo data for \textit{one generator}.