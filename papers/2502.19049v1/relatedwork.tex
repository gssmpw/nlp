\section{Related Work}
\label{sec:related-work}

As we noted in the Introduction, most available solutions tackling the 
data-driven drift and diffusion estimation problem proceed primarily through Bayesian non-parametrics and symbolic regression.
%
In general, when the observed data is noisy, sparse in time, or both, one faces uncertainty not only in determining the drift and diffusion of the putative SDE, but also in the state of the system itself.
%
Therefore, a Bayesian treatment requires the estimation of the posterior distribution over these states, conditioned on the noisy data (\textit{i.e.}~the so-called smoothing problem).
%
Starting with the seminal works of \citet{archambeau2007gaussian, archambeau2007variational}, which approximated the posterior over the states with a variational and inhomogeneous Gaussian process, 
 %
most proposals have mainly focused on devising different strategies to infer the smoothing distribution --- \textit{while assuming prior parametric forms for the drift and diffusion functions}. See \textit{e.g.}~\citet{VRETTAS20111877}, \citet{wildner2021moment}, or the recent work by \citet{verma2024variational}.
%
A notable exception is the proposal of~\citet{duncker2019learning},  which extended the variational trick of~\citet{archambeau2007gaussian} by imposing a non-parametric prior over the drift of the prior process.
%
However, \citet{archambeau2007gaussian}'s trick has been shown to suffer from significant convergence issues \cite{verma2024variational}, which are inherited by~\citet{duncker2019learning}'s model.
%
In contrast, \citet{batz2018approximate} framed the drift-diffusion estimation problem from uncorrupted (\textit{i.e.}~clean and dense) data as a Gaussian process regression problem,
%
and extended this (non-parametric) approach to observations that are sparse in time, by leveraging Orstein-Uhlenbeck bridges optimized with an expectation maximization algorithm.
%
Nevertheless, this extension can only deal with non-parametric drifts (\textit{i.e.}~the diffusion is restricted to parametric forms) and clean data, and is highly sensitive to the choice of prior hyperparameters.
%
% We will compare our methodology against that of~\citet{batz2018approximate}.

% Despite the recent proliferation of symbolic regression methods, only few of these tackle function estimation for SDEs (see \textit{e.g.}~the reviews by \citet{la2021contemporary} and~\citet{makke2024interpretable})
%
Symbolic regression methods for drift and diffusion estimation mainly extend the SINDy algorithm~\cite{brunton2016discovering} --- which performs sparse linear regression on a predefined library of candidate nonlinear functions --- to SDEs.
%
The first of these extensions corresponds to the work of~\citet{boninsegna2018sparse}, which sets the regression problem by approximating the local values of the drift and diffusion functions with the empirical expectations of Eqs~\ref{eq:drift-definition} and \ref{eq:diffusion-definition} of Section~\ref{sec:preliminaries}.  
%
However, the calculation of these local expectations generally requires significant amounts of data, even for one-dimensional systems.
%
To (somewhat) alleviate this issue, \citet{huang2022sparse} and~\citet{WANG2022244} recently resorted to sparse Bayesian learning, but their solutions still require sizable dataset sizes and, most problematically, assume access to clean and dense observations.
%
In response to this limitations, \citet{course2023state} proposed a hybrid solution that leveraged the variational trick of~\citet{archambeau2007gaussian}, while allowing the drift of the prior process to be approximated by a sparse, linear combination of known basis functions.
%
However, their model inherits the slow convergence problems of the variational approximation. 
%
What is more, all SINDy-like methods are limited by construction to linear combination of the functions in their library, 
%
which therefore makes their performance highly dependent on the preselected functions within the library.

Finally, most neural network models for SDEs rely on black-box parameterizations of the drift and diffusion functions for path (\textit{i.e.}~state) generation. 
%
Prominent examples include the works by~\citet{li2020scalable}, \citet{kidger2021neural}, \citet{bilovs2023modeling} and~\citet{zeng2024latent}. 
%
To the best of our knowledge, our proposal is the first neural network and non-parametric solution to the problem of SDE function estimation --- \textit{and the first to do so in a zero-shot fashion}.

% adjoint computation introduces a high computational burden, leading to stiffness and limited parallelization, which heavily constrains the success of these approaches.
%
%
% In the context of approximate Bayesian inference, the problem of learning the drift and diffusion functions from data has been tackle with the use of Gaussian process priors. In the seminal work of , variational inference is performed for diffusion processes, where the approximating variational family consists of time-variant linear diffusion process. Improvements of this approach include moment based methodologies . Improvement of learning from fixed point \cite{verma2024variational}. Wishard diffusion \cite{jorgensen2020stochastic}. Interpretability conditioning on a set of learned fixed points and local jacobian dynamics . For non linear diffusions, regression was been achieved via EM algorithms using Orstein Uhlenbeck bridges as data likelihoods \cite{batz2018approximate}. All this approaches are based on non parametric estimates based on GPs approximations, and requires a different training per dataset i.e. these are non amortized methods. Furthermore, they typically required if not adjoint methods, forward and backwards paths that are computationally costly.
%
% In the realm of deep neural networks, the seminal paper by \cite{chen18} introduced Neural ODEs (NODE), where the vector field is parameterized using a neural network and trained via backpropagation through the solver using adjoint methods. This approach enabled them to handle continuous dynamics and irregularly sampled time series \cite{rubanova2019latent, kidger2020neural, li2020learning}. Attempts to incorporate stochastic dynamics have explored neural processes \cite{norcliffe2021neural}, variational autoencoders \cite{yildiz2019ode2vae}, and the training of SDE operators as a form of infinite-dimensional GANs \cite{kidger2021neural} or diffusion models \cite{bilovs2023modeling}. This methodologies however requiere adjoint estimates as well as large amount fo data for \textit{one generator}.