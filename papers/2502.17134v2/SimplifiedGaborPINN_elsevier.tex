\documentclass[authoryear, preprint, 12pt]{elsarticle}


% Required Packages
\usepackage{amsmath, amssymb}   % Math symbols
\usepackage{graphicx, subcaption} % Figures and subfigures
\usepackage{listings}           % Code formatting
\usepackage{tabularx, booktabs} % Tables
\usepackage{url}                % URLs
\usepackage{xcolor}             % Color formatting
\usepackage{natbib}
\usepackage[margin=1in]{geometry} 


% Title
\title{Gabor-Enhanced Physics-Informed Neural Networks for Fast Simulations of Acoustic Wavefields}

% Authors and Affiliations
%\author[bcam]{Mohammad Mahdi Abedi \corref{cor1}}
%\author[upv]{David Pardo}
%\author[kaust]{Tariq Alkhalifah}
%
%\address[bcam]{Basque Center for Applied Mathematics, Bilbao, Spain}
%\address[upv]{University of the Basque Country, Department of Mathematics, Spain \& Ikerbasque, Basque Foundation for Science, Bilbao, Spain}
%\address[kaust]{King Abdullah University of Science and Technology, Thuwal 23955-6900, Saudi Arabia}
%
%\cortext[cor1]{Corresponding author. Email: \texttt{mabedi@bcamath.org}}


\author{
	\normalsize Mohammad Mahdi Abedi\textsuperscript{1,*}\corref{cor1},  
	David Pardo\textsuperscript{2,1,3},  
	Tariq Alkhalifah\textsuperscript{4} \\  
	\footnotesize  
	\textsuperscript{1}Basque Center for Applied Mathematics, Bilbao, Spain \\  
	\footnotesize  
	\textsuperscript{2}University of the Basque Country, Department of Mathematics, Spain \\  
	\footnotesize  
	\textsuperscript{3}Ikerbasque, Basque Foundation for Science, Bilbao, Spain \\  
	\footnotesize  
	\textsuperscript{4}King Abdullah University of Science and Technology, Thuwal 23955-6900, Saudi Arabia \\  
	\footnotesize  
	\textit{Emails:} mabedi@bcamath.org, david.pardo@ehu.es, tariq.alkhalifah@kaust.edu.sa  
}

\cortext[cor1]{Corresponding author}

\begin{document}
	
	% Front Matter
	\begin{frontmatter}
		
		\begin{abstract}
			Physics-Informed Neural Networks (PINNs) have gained increasing attention for solving partial differential equations, including the Helmholtz equation, due to their flexibility and mesh-free formulation. However, their low-frequency bias limits their accuracy and convergence speed for high-frequency wavefield simulations. To alleviate these problems, we propose a simplified PINN framework that incorporates Gabor functions, designed to capture the oscillatory and localized nature of wavefields more effectively. Unlike previous attempts that rely on auxiliary networks to learn Gabor parameters, we redefine the network's task to map input coordinates to a custom Gabor coordinate system, simplifying the training process without increasing the number of trainable parameters compared to a simple PINN. We validate the proposed method across multiple velocity models, including the complex Marmousi and Overthrust models, and demonstrate its superior accuracy, faster convergence, and better robustness features compared to both traditional PINNs and earlier Gabor-based PINNs. Additionally, we propose an efficient integration of a Perfectly Matched Layer (PML) to enhance wavefield behavior near the boundaries. These results suggest that our approach offers an efficient and accurate alternative for scattered wavefield modeling and lays the groundwork for future improvements in PINN-based seismic applications.
		\end{abstract}
		
		\begin{keyword}
			Physics-Informed Neural Networks \sep Helmholtz Equation \sep Gabor Functions \sep Perfectly Matched Layer \sep Seismic Wavefield 
		\end{keyword}
		
	\end{frontmatter}
	
	% Set figure path
	\graphicspath{{figures/}}
	
	\section{Introduction}
	


Wavefield simulations governed by the Helmholtz equation, the frequency-domain form of the wave equation, are critical for understanding wave propagation phenomena in geophysics and acoustics. The Helmholtz equation is particularly relevant for seismic applications such as full waveform inversion (FWI) and imaging \citep{pratt1999, sirgue2004}, as well as for emerging fields like offshore wind farm site characterization, where accurate subsurface imaging is essential for foundation design and geohazard assessment. However, solving it numerically poses significant challenges due to its highly oscillatory solutions, which require high-resolution grids, leading to substantial computational costs \citep{Virieux1986, Marfurt1984}. Applications in wind farm site characterization typically require higher frequencies to resolve fine-scale near-surface features, further increasing computational demands. Traditional numerical methods suffer from issues such as numerical dispersion and the curse of dimensionality \citep{Hesthaven2007, Ainsworth2004}. Although dimensionality reduction offers more efficient computations, the inversion of the stiffness matrix associated with the Helmholtz equation remains computationally prohibitive, particularly for high-frequency applications or complex velocity models \citep{wu2023}.


Physics-Informed Neural Networks (PINNs) have demonstrated the ability to learn functional solutions to partial differential equations \citep{raissi2019}, including the Helmholtz equation \citep{song2021}. PINNs learn a continuous function that satisfies the given partial differential equation (PDE), boundary conditions, and any additional constraints. These methods use the governing conditions in the loss function, enabling unsupervised training and flexible domain representation \citep{llanas2006}. They are advantageous for irregular geometries, high-dimensional problems, and can also solve PDEs with problem-dependent parameters \citep{lu2021learning,zou2024seismic,baharlouei2025least}, making them adaptable for a wide range of applications.

However, PINNs face limitations in training cost and accuracy for complex functions \citep{wu2018a, song2023}. High training costs stem from the inherent low-frequency bias of neural networks, making high-frequency solutions challenging to achieve \citep{neal2019, wang2021eigenvector, Alkhalifah2024}. To mitigate these issues, recent works have proposed enhancements to the PINN framework, including modified activation functions \citep{alsafwan2021, waheed2022kronecker}, improved input representations \citep{tancik2020fourier, huang2024}, advancements in optimizers \citep{li2022gradient}, alternative architectures \citep{yang2023fwigan}, and frequency upscaling and feature mapping \citep{chai2024overcoming}.

A potential approach involves augmenting PINNs with so-called \textit{explicit basis functions} suited to wavefield characteristics. Gabor functions \citep{gabor1946}, known for their efficacy in representing localized wave phenomena \citep{pinto2014}, have been integrated into neural networks to improve their ability to capture complex wavefield features. By incorporating Gabor basis functions into PINNs, researchers have achieved significant gains in accuracy and efficiency \citep{huang2023b, Alkhalifah2024}. \cite{huang2023b} used the Gabor function as a multiplicative activation function for all hidden layers throughout the network and realized that proper initialization of its wavelength parameter is crucial to its stability. \cite{Alkhalifah2024} used Gabor functions as explicit basis functions in the penultimate layer, and found their magnitude and centers from two NNs. The performance and final accuracy of their method highly depend on a proper selection of hyperparameters, including the number of neurons in their auxiliary network, which requires a trial-and-error procedure.

In this study, we build upon this concept, proposing a PINN model augmented with simplified Gabor basis functions. We show that the properties of Gabor functions allow the definition of a single parameter to absorb the effects of original Gabor centers and magnitudes. By redefining the network’s task into learning a mapping from the input to a custom Gabor coordinate system, the learned function becomes smoother, as the oscillatory part of the wavefield solution is handled by the Gabor functions. Consequently, a smaller neural network can be employed, and shown to be more robust to hyperparameter selection. In addition, we suggest an efficient incorporation of Perfectly Matched Layers (PMLs) for scattered wavefield modeling to mimic absorbing boundary conditions.

The rest of this paper is organized as follows. In Section 2, we introduce the mathematical formulation of the problem. In Section 3, we define the loss function, describe the incorporation of a Perfectly Matched Layer (PML), and explain the implementation of our proposed integration of Gabor basis functions into the PINN framework. In Section 4, we present numerical experiments on various velocity models and frequencies, including comparisons with other PINN approaches, an analysis of robustness with respect to weight initialization, and the effect of PML. Finally, we summarize our findings and suggest potential directions for future work.

	
	\section{Mathematical Formulation}
	
	\subsection{Helmholtz Equation for Wavefield Simulation}
	
	The wavefield simulation in this study focuses on solving the Helmholtz equation, which represents the wave equation in the frequency domain. For an isotropic medium with wave velocity \( v(\mathbf{x}) \), the Helmholtz partial differential equation (PDE) is given by:
	\begin{equation}
		\left(\nabla^2 + \frac{\omega^2}{v(\mathbf{x}^2)}\right) u(\mathbf{x}) = f(\mathbf{x}),
		\label{eq:helmholtz}
	\end{equation}
	where \( \nabla^2 \) is the Laplacian operator, \( \omega \) is the angular frequency, and \( f(\mathbf{x}) \) represents the source function.
	
	The solution $u(\mathbf{x}) = u^r(\mathbf{x}) + \mathrm{i} u^i(\mathbf{x})$  is a complex-valued wavefield, where \( \mathbf{x} = \{x, z\} \) denotes the spatial coordinates in a 2D domain. Since $f(\mathbf{x})$ is modeled as a point source (Dirac's delta function), this generates a point singularity in the solution. To address this, the wavefield $u(\mathbf{x})$ is decomposed into a background wavefield $u_0(\mathbf{x})$ and a scattered wavefield $u_s(\mathbf{x}) = u(\mathbf{x}) - u_0(\mathbf{x})$. The background wavefield \( u_0(\mathbf{x}) \) is analytically calculated in a homogeneous medium with a constant background velocity \( v_0 \), defined as the velocity at the source location \( \mathbf{x}_f \), i.e., \( v_0 = v(\mathbf{x}_f) \). The analytical background wavefield \( u_0(\mathbf{x}) \) satisfies the Sommerfeld radiation condition.
	
	Substituting this decomposition into equation~\ref {eq:helmholtz} yields the Helmholtz PDE for the scattered wavefield:
	\begin{equation}
	\left(\nabla^2 + \frac{\omega^2}{v^2(\mathbf{x})}\right) u_s(\mathbf{x}) = -\omega^2 \delta m(\mathbf{x}) u_0(\mathbf{x}),
\end{equation}
where $\delta m(\mathbf{x}) = \frac{1}{v^2(\mathbf{x})} - \frac{1}{v_0^2}$ represents the perturbation in slowness squared.

\subsection{Perfectly Matched Layer}  
Absorbing boundary conditions, commonly used in classical numerical simulations to model wave propagation in infinite media, allowing us to limit the size of the computational domain without inducing reflections from the boundary. The Perfectly Matched Layer (PML) technique achieves a similar effect by introducing an artificial absorbing layer around the boundaries of the simulation domain. Within this layer, wave energy is progressively damped as it propagates, effectively preventing reflections from the boundaries \citep{berenger1994perfectly}.  

The scattered Helmholtz equation with PML reads (see Appendix A):  
\begin{equation}
	\frac{\partial}{\partial x} \left( \frac{e_z}{e_x} \frac{\partial u_s}{\partial x} \right) +
	\frac{\partial}{\partial z} \left( \frac{e_x}{e_z} \frac{\partial u_s}{\partial z} \right) +
	e_x e_z \omega^2 m(x) u_s = - e_x e_z \omega^2 \delta m(x) u_0(x),
	\label{eq:scattered_helmholtz_PML}
\end{equation}
where scaling factors for the complex coordinate stretching are defined as  
\begin{equation}
	\begin{aligned}
		e_x &= 1 - i c l_x^2, \\
		e_z &= 1 - i c l_z^2,
	\end{aligned}
	\label{eq:ex_ez}
\end{equation}
and the boundary distance profiles are: 
\begin{equation}
	\begin{aligned}
		l_x &= \max(0, x_{bl} - x) + \max(0, x - x_{br}), \\
		l_z &= \max(0, z_{bu} - z) + \max(0, z - z_{bd}). 
	\end{aligned}
	\label{eq:lx_lz}
\end{equation}
The decay condition we employ reads as  
\begin{equation}
	c = a_0 \frac{\omega_0}{\omega L_{\text{PML}}^2},
	\label{eq:c}
\end{equation}
where \( L_{\text{PML}} \) is the PML thickness, \( a_0 \) is a constant, and \( x_{bl}, x_{br}, z_{bu}, z_{bd} \) are the positions of the left, right, upper, and lower boundaries, respectively.  

We derive an analytical approximation for the background wavefield with the PML implementation (see Appendix A):  

		\begin{equation}
		u_0(\mathbf{x}) = \frac{i}{4} H_0^{(2)}\left(\frac{\omega}{v_0}  |\mathbf{x} - \mathbf{x}_s|\right) e^{-\omega c \frac{(l_x^2+l_z^2)}{3 v_0}^{(3/2)}},
	\end{equation}
	where $H_0^{(2)}\left(.\right)$ is the zero-order Hankel function of the second kind. This equation is exact within the domain of interest, and an approximation in the PML region. 
	
\section{Methodology}
	\subsection{Loss Function}
	
	The solution for $u_s(\mathbf{x})$ is obtained using a PINN framework. The loss function combines the PDE loss ($\mathcal{L}_{\text{PDE}}$) and a soft constraint term ($\mathcal{L}_{\text{C}}$):
	\begin{equation}
		\mathcal{L} = \mathcal{L}_{\text{PDE}} + \beta \mathcal{L}_{\text{C}}.
		\label{eq:loss_total}
	\end{equation}
	
	Without the PML, and when the velocity model is real-valued (non-attenuative), \( \mathcal{L}_{\text{PDE}} \) is the loss enforcing the real and imaginary parts of the solution to satisfy the scattered Helmholtz equation \citep{Alkhalifah2021}:
	\begin{equation}
		\begin{aligned}
			\mathcal{L}_{\text{PDE}} = \frac{1}{N} \sum_{j=1}^{N} \left( 
			\left| \nabla^2 u_s^r(\mathbf{x}_j) + \frac{\omega^2}{v^2(\mathbf{x}_j)} u_s^r(\mathbf{x}_j) + \omega^2 \delta m(\mathbf{x}_j) u_{0}^r(\mathbf{x}_j) \right|^2 
			\right. + \\
			\left. \left| \nabla^2 u_s^i(\mathbf{x}_j) + \frac{\omega^2}{v^2(\mathbf{x}_j)} u_s^i(\mathbf{x}_j) + \omega^2 \delta m(\mathbf{x}_j) u_{0}^i(\mathbf{x}_j) \right|^2 \right),
		\end{aligned}
		\label{eq:loss_pde}
	\end{equation}
	where $N$ is the number of collocation points. Note that the two terms in the above equation share the background wavefield $u_0$, which links the real and imaginary components of the scattered field.
	
Our numerical experiments with PINNs show that classical boundary reflections do not appear even without explicitly implementing an absorbing boundary condition. This aligns with observations reported by other researchers (e.g., \cite{Alkhalifah2021, Rasht-Behesht2022, wu2023}). We believe this is related to the spectral bias of PINNs and the fact that reflections generate solutions with higher oscillations. Nonetheless, in our applications, PMLs can be beneficial because they explicitly impose the coupling of the real and imaginary parts of the scattered wavefield, and enforce physical boundary conditions on the wavefield at the domain boundaries \citep{ wu2023}.
	
		Incorporating the PML in the scattered wavefield PDE (equation~\ref {eq:scattered_helmholtz_PML}), and defining the $\mathcal{L}_{\text{PDE}}$ as the mean squared difference between the two sides of that equation, the loss function takes the form (see Appendix A):  
\begin{equation}
	\mathcal{L}_{\text{PDE}} = \frac{1}{N} \sum \left( L_r(u_s^r, u_s^i)^2 + L_i(u_s^r, u_s^i)^2 \right),
	\label{eq:loss_pml}
\end{equation}
	where \( L_r \) and \( L_i \) represent the real and imaginary components of the loss, respectively, as functions of the real (\( u_s^r \)) and imaginary (\( u_s^i \)) parts of the scattered field. For implementation efficiency, we explicitly calculate the real and imaginary parts of the loss, as shown in Appendix A, to avoid using complex numbers during training. With the addition of the PML, each term of the loss depends on both the real and imaginary components of the neural network output.  
	
	
To prevent the trivial solution \(u_s = -u_0 \), the soft constraint term \( \mathcal{L}_{\text{C}} \) minimizes the solution close to the source where $ u_0 $ has a large magnitude \citep{huang2023b}. The term is calculated as:
	\begin{equation}
		\mathcal{L}_{\text{C}} = \frac{1}{N_{\text{C}}} \sum_{j=1}^{N_{\text{C}}} \left|u_s(\mathbf{x}_{\text{C},j}) \cdot \gamma(\mathbf{x}_{\text{C},j})\right|^2,
	\end{equation}
	where \( \mathbf{x}_{\text{C},j} \) are the \(N_{\text{C}}\) collocation points within the soft constraint  radius, and \( \gamma(\mathbf{x}_{\text{C},j}) \) is a scaling factor based on the distance from the source:
	\begin{equation}
		\gamma(\mathbf{x}_{\text{C},j}) =\sqrt{ \max\left(0, \frac{\lambda^2}{4} - r_f^2 \right)},
	\end{equation}
	where \(r_f\) represents the distance from the source, and \( \lambda \) is the wavelength. 


	
	

	\subsection{Explicit Gabor Basis Functions}
	
When the output layer of a neural network is linear, the resulting wavefield is a linear combination of the outputs from the penultimate layer. By analogy, these outputs can be viewed as \textit{learned basis functions} that capture complex spatial patterns of the solution, and we will therefore employ the term \textit{basis functions} in this work. Notice, however, that they do not strictly meet the mathematical definition of basis functions, as they can be linearly dependent.

	\begin{figure}[ht]
	\centering
	\includegraphics[width=0.4\textwidth]{Gabor_schematic.pdf}
	\caption{Real part of a 10 Hz Gabor function displayed as a function of \( (d_x, d_z) \). }
	\label{fig:Gabor_schematic}
\end{figure}
	
	\textbf{Gabor Function:} Gabor functions combine a Gaussian envelope with a sinusoidal carrier and present both local and oscillatory behaviors of the solution \citep{pinto2014}. Therefore, they are well-suited for wavefield simulations if used as explicit basis functions. 
	
	
	A conventional two-dimensional Gabor function centered at \( (\mu_x, \mu_z) \) is given by \citep{gabor1946}:
	\begin{equation}
		G(x, z) = A \exp \left( -\frac{x'^2 + z'^2}{2\sigma^2} \right) 
		\exp \left( i \left( \frac{\omega}{v} x' + \psi \right) \right),
	\end{equation}
	where \( (x', z') \) are rotated coordinates defined as:
	\begin{align}
		x' &= (x - \mu_x) \cos(\theta) + (z - \mu_z) \sin(\theta), \\
		z' &= -(x - \mu_x) \sin(\theta) + (z - \mu_z) \cos(\theta).
	\end{align}
	Here, \( \sigma \) controls the spatial spread of the Gaussian envelope, \( \theta \) is the rotation angle of the Gabor function in the spatial domain, \( A \) is its peak magnitude, and \( \psi \) is the phase offset. The sinusoidal carrier represents the oscillatory behavior, while the Gaussian envelope ensures spatial localization.  

	
	In \cite{Alkhalifah2024}, Gabor functions are used at the output of the penultimate layer in a multiplicative form, which means their main network learns \( A_p(\mathbf{x}) \), representing the magnitude of the \( p \)-th basis function at a given input location. To allow each basis function to contribute differently across spatial coordinates, the Gabor centers are also modeled as functions of the input coordinates, i.e., \( \mu(\mathbf{x}) \). They train an auxiliary network to output \( \mu(\mathbf{x}) \) for given collocation points.  
	
	We propose defining  
	\begin{equation}
		d(\mathbf{x}) = \mathbf{x} - \mu(\mathbf{x}),
	\end{equation}
	where \( d_x \) and \( d_z \) represent the horizontal and vertical distances from the Gabor function center in a local coordinate system (Figure \ref{fig:Gabor_schematic}). Using the existing Gaussian envelope, \( d(\mathbf{x}) \) contains the effects of both \( A_p(\mathbf{x}) \) and \( \mu_p(\mathbf{x}) \).  
	Further simplifying the formulation by removing the phase offset, the real and imaginary components of our Gabor basis functions are defined as:
	\begin{align}
		G_{\text{real}}(d_x, d_z) &= \cos\left( \frac{\omega}{v} d_{x_\theta} \right) 
		\exp\left( -\frac{1}{2\sigma^2} \left( d_{x_\theta}^2 + d_{z_\theta}^2 \right) \right), \\
		G_{\text{imag}}(d_x, d_z) &= \sin\left( \frac{\omega}{v} d_{x_\theta} \right) 
		\exp\left( -\frac{1}{2\sigma^2} \left( d_{x_\theta}^2 + d_{z_\theta}^2 \right) \right),
	\end{align}
	where the transformed coordinates are given by:
	\begin{align}
		d_{x_\theta} &= d_x \cos(\theta) + d_z \sin(\theta), \\
		d_{z_\theta} &= -d_x \sin(\theta) + d_z \cos(\theta).
	\end{align}
	

	\begin{figure}[tb]
		\centering
		\begin{minipage}{0.59\textwidth}
			\centering
			\includegraphics[width=\textwidth]{dnn_schematic_gabor.pdf}  % Part (b)
		\end{minipage}
		\caption{Schematic representation of the neural network architecture. Dashed connections have no associated weight. The network learns a mapping from the input \((x,z)\) to \((d_x,d_z)\) in the Gabor functions that produce the oscillatory behavior of the wave field \(u_s\).	}
		\label{fig:NN_architecture}
	\end{figure}
	
	\begin{figure}[htb]
		\centering
		\includegraphics[width=0.7\textwidth]{gabor_neuron_input_output.pdf}
		\caption{(a) Real part of the Gabor function plotted as a function of \( (d_x, d_z) \). The superimposed paths represent the trajectories produced by the network for vertical and horizontal sets of input collocation points \( (x, z) \). (b) Output values of this Gabor basis function, illustrating how the learned mapping from \( (x, z) \) to \( (d_x, d_z) \) enables the Gabor basis function to contribute to the wavefield solution.}
		\label{fig:gabor_mapping}
	\end{figure}
	

	\textbf{Implementation of Gabor Basis Functions: }A feed-forward neural network (NN) is trained to minimize the loss function.  The network’s task is to learn the mapping from the input collocation points \( (x, z) \)  to the corresponding \( (d_x, d_z) \) values for each Gabor basis function. To implement it, pairs of neurons from the penultimate layer are used as \( d_x \) and \( d_z \), as shown in Figure \ref{fig:NN_architecture}. The other parameters of the Gabor functions are set as global parameters, meaning they do not vary with each input. This allows the network to learn only the transformation from the input coordinates \( (x, z) \) to the appropriate \( (d_x, d_z) \) coordinates corresponding to the required wavefield behavior. This method of adding Gabor basis functions does not change the number of trainable parameters compared to a simple PINN with the same number of layers and neurons per layer.
	
	The angular frequency \( \omega \) is fixed to its value in the objective Helmholtz equation, as it dictates the oscillatory behavior of the wavefield. We fix the standard deviation \( \sigma = 0.1 \), and constrain the range of \( d_x \) and \( d_z \) to the interval \( [0, 1] \), to ensure that the Gabor function decays to zero within the possible range of \( d_x \) and \( d_z \).
	
	Since the relationship between the \(( d_x, d_z) \) coordinates and the original spatial coordinates \( (x, z) \) is nonlinear, there is no need for the parameters \( \theta \) (the rotation angle) or \( \frac{\omega}{v} \) (the wavelength) to directly match those in the target wavefield solution. Instead, these parameters are treated as trainable variables, initialized to the same values in all neurons. We initialize $\theta$ to $-\pi/4$, and $v$ to $v_0$. 
	
	When for a given input location \( (x, z) \), the Gabor function \( G \) is nonzero for the calculated \(( d_x, d_z) \), that basis function contributes to the solution at that point in the wavefield. Since the Gabor functions already incorporate the oscillatory behavior of the wavefield, the mapping from \( (x, z) \) to \( (d_x, d_z )\) learned by the network can be smooth and easily learned, allowing for efficient training and accurate representation of the wavefield. Figure \ref{fig:gabor_mapping} illustrates the mapping between the physical coordinates \( (x, z) \) and the corresponding Gabor function coordinates \( (d_x, d_z) \), highlighting how the neural network utilizes the Gabor basis function to represent the wavefield.
	
	In Figure \ref{fig:gabor_mapping}a, the real part of a Gabor function is plotted as a function of \( (d_x, d_z) \), with all other parameters fixed. The surface represents the value of the Gabor function across different points in the \( (d_x, d_z) \) space. Superimposed are two paths, which represent the \( (d_x, d_z) \) trajectories that the neural network produced for two sets of input collocation points; one set corresponding to vertical points and the other to horizontal points in the \( (x, z) \) domain. Figure \ref{fig:gabor_mapping}b shows the corresponding values produced by the Gabor basis function for these two sets of input collocation points in the physical space. The network learns to map each input \( (x, z) \) to a specific trajectory in the \( (d_x, d_z) \) space, allowing the Gabor basis function to contribute appropriately to the wavefield solution for each location point.
	
	The activation function used in hidden layers of the NN is chosen as,
	\begin{equation}
		\phi(\mathbf{h}) = \sin(\mathbf{h}),
	\end{equation}
	which is well-suited for modeling oscillatory wavefields \citep{huang2021,wong2022learning,song2022versatile,buzaev2024hybrid}.
	
	To enhance the representation of the input spatial coordinates \( (x, z) \), we use positional encoding to map the input into a higher-dimensional space, enabling the network to better capture oscillatory behaviors \citep{Vaswani2017, huang2021}. We use a simple sinusoidal encoding function:
	\begin{equation}
		\mathbf{E}(\mathbf{x}) = \left[x,z, \sin(2^k x), \cos(2^k x), \sin(2^k z), \cos(2^k z) \right]_{k=0}^{K},
		\label{eq:encoder}
	\end{equation}
	where \( \mathbf{E}(\mathbf{x}) \in \mathbb{R}^{2(K+1)} \) and \( K \) is the maximum frequency used for the encoding. A schematic representation of the encoder layer is shown in Figure \ref{fig:NN_architecture}. 
	
	\begin{figure}[]
		\centering
		\includegraphics[width=\textwidth]{10Hz_FD_velocity.pdf}  
		\caption{A simple velocity model and the finite difference (FD) modeled 10Hz wave field using a fine grid spacing. We use this simulation result to calculate the error of different PINN predictions. }
		\label{fig:10Hz_fd_wavefoeld}
	\end{figure}
	


\begin{figure}[tp]
\centering
\begin{minipage}{0.32\textwidth}
	\centering
	\includegraphics[width=\textwidth]{EAGE_Loss_simplePINN.pdf}
\end{minipage}
\begin{minipage}{0.32\textwidth}
	\centering
	\includegraphics[width=\textwidth]{EAGE_Loss_Gabor_PINN.pdf}
\end{minipage}
\begin{minipage}{0.32\textwidth}
	\centering
	\includegraphics[width=\textwidth]{EAGE_errors.pdf}
\end{minipage}

\caption{Comparison of the convergence behavior of the simple PINN and the proposed Gabor-PINN for simulating a 10 Hz wavefield (Test 1) in the simple velocity model shown in Figure~\ref{fig:10Hz_fd_wavefoeld}. (a) Loss evolution for the simple PINN, (b) loss evolution for the proposed Gabor-PINN, (c) evolution of prediction errors on validation points (relative to the finite difference result) for both methods. The proposed Gabor-PINN demonstrates faster convergence and higher accuracy.}
\label{fig:loss_10Hz}
\end{figure}
	
		\begin{figure}[tp]
		\centering
		\begin{minipage}{0.30\textwidth}
			\centering
			\includegraphics[width=\textwidth]{10Hz_PINN_500.pdf}
		\end{minipage}
		\begin{minipage}{0.30\textwidth}
			\centering
			\includegraphics[width=\textwidth]{10Hz_PINN_10000.pdf}
		\end{minipage}
		\begin{minipage}{0.30\textwidth}
			\centering
			\includegraphics[width=\textwidth]{10Hz_PINN_100000.pdf}
		\end{minipage}
		\\
		\begin{minipage}{0.30\textwidth}
			\centering
			\includegraphics[width=\textwidth]{10Hz_Gabor_500.pdf}
			\caption*{(a) 500 epochs}
		\end{minipage}
		\begin{minipage}{0.30\textwidth}
			\centering
			\includegraphics[width=\textwidth]{10Hz_Gabor_10000.pdf}
			\caption*{(b) 10,000 epochs}
		\end{minipage}
		\begin{minipage}{0.30\textwidth}
			\centering
			\includegraphics[width=\textwidth]{10Hz_Gabor_100000.pdf}
			\caption*{(c) 100,000 epochs}
		\end{minipage}
\caption{Real part of the predicted 10 Hz wavefield from the proposed simple PINN and the proposed Gabor-PINN at different training epochs (500, 10,000, and 100,000) for Test 1. Both methods share the same training settings, number of trainable parameters, and network architecture, except for the inclusion of Gabor functions in the Gabor-PINN. The Gabor-PINN exhibits faster convergence. The reference finite difference (FD) simulated wavefield is shown in Figure~\ref{fig:10Hz_fd_wavefoeld}.}
\label{fig:predictions_10Hz}

	\end{figure}

\section{Numerical Results}

We evaluate different aspects of the proposed method through five numerical experiments, using three velocity models at various frequencies, and compare the results with other approaches.

	\subsection{Test 1: Convergence}
	
	The first test is performed using a simple velocity model, shown in Figure~\ref{fig:10Hz_fd_wavefoeld}, where we also present the 10 Hz scattered wavefield obtained from finite difference (FD) modeling. The source is located at a depth of 26 meters and it is positioned horizontally at the center of the model. This FD numerical solution will serve as a reference data for comparing the outputs of different PINN approaches, and is provided on a \(200 \times 200\) grid. The FD modeling is performed on a grid with four times finer spacing on each axis and a large perfectly matched layer (PML) to minimize boundary reflections.
	

\textbf{Simple PINN: }We first implement a deep neural network (DNN) with three hidden layers, each containing 64 neurons, and use the encoder layer (defined in equation \ref{eq:encoder}) with \(K=3\). During training, we randomly select 2,601 varying collocation points from the computational domain at each epoch. The network is trained using the loss function based on Equations~\eqref{eq:loss_total} and \eqref{eq:loss_pde} for 100,000 epochs using an exponentially decaying learning rate (starting at 0.001 and decreasing to about 0.0003). The learning rate decay helps improve stability, and has been shown to enhance training in PINNs (e.g., \cite{bihlo2024improving, xu2024preprocessing}).

The evolution of the training and validation losses is shown in Figure~\ref{fig:loss_10Hz}a, where the validation loss is computed over the same \(200 \times 200\) grid used for the FD reference. The simple PINN starts to converge after approximately 20,000 epochs.
	

	\textbf{Gabor-PINN: }We then apply the proposed Gabor-PINN method, incorporating 32 explicit Gabor basis functions in the NN, as schematically  illustrated in Figure~\ref{fig:NN_architecture}. The number of neurons and layers, collocation points, learning rate, weight initialization, and other parameters remain the same as in the simple PINN implementation. Note that our implementation of Gabor functions results in the same total number of trainable parameters as the simple PINN. The loss evolution for this method is shown in Figure~\ref{fig:loss_10Hz}b, in which the Gabor-PINN begins converging almost immediately and consistently achieves lower losses than the simple PINN.
	
	The validation errors for both methods are shown in Figure~\ref{fig:loss_10Hz}c, where the FD-simulated wavefield is used as the reference. The lowest error achieved by the simple PINN after 100,000 epochs is reached in just 7,000 epochs using the proposed Gabor-PINN. This highlights the substantial speed-up and improved accuracy gained by incorporating Gabor basis functions. 
	
	Figure~\ref{fig:predictions_10Hz} compares the predictions of both models at epochs 500, 10000, and 100000, illustrating the evolution of their predictions as training progresses. The results show that the Gabor-PINN achieves significantly better predictions in far fewer epochs compared to the simple PINN.
	
	
	\subsection{Test 2: Comparison to Alkhalifah and Huang  (2024)}
	
	In this test, we compare our proposed Gabor-PINN with the implementation of Gabor functions in PINNs by \cite{Alkhalifah2024}, demonstrating the higher accuracy and robustness of our method with respect to random initializations of the neural network weights. The test is conducted for a 4 Hz wavefield using the simple velocity model shown in Figure~\ref{fig:10Hz_fd_wavefoeld}.
	
	For the method of \cite{Alkhalifah2024}, we use the architecture and hyperparameters they optimized for their method on this test. For our Gabor-PINN, we employ similar parameters, incorporating 16 Gabor basis functions and a network with three hidden layers of 32 neurons each. 
	
	Figure~\ref{fig:4Hz_test} compares the training and validation losses, as well as the validation errors, for three different random initializations of weights. The method of \cite{Alkhalifah2024} exhibits higher sensitivity to weight initialization, leading to varying loss and error curves. In contrast, our proposed Gabor-PINN demonstrates greater resilience to random weight initialization, as reflected in the consistently converging graphs. Furthermore, the proposed Gabor-PINN achieves more accurate results than the method of \cite{Alkhalifah2024}.
	
	Figure~\ref{fig:4Hz_test}d-e further compares the real parts of wavefield predictions from \cite{Alkhalifah2024}, our proposed Gabor-PINN, and a reference wavefield obtained using finite difference (FD) modeling.
	
	\begin{figure}[h]
		\centering
		\begin{subfigure}[b]{0.32\textwidth}
			\includegraphics[width=\textwidth]{Comparison_Alkhalifah_4Hz__randinitialization_PINN.pdf}
%			\caption{\cite{Alkhalifah2024} method}
		\end{subfigure}
		\begin{subfigure}[b]{0.32\textwidth}
			\includegraphics[width=\textwidth]{Comparison_Alkhalifah_4Hz__randinitialization_alkhalifahLS.pdf}
%			\caption{Loss: Proposed Gabor-PINN}
		\end{subfigure}
		\begin{subfigure}[b]{0.32\textwidth}
			\includegraphics[width=\textwidth]{Comparison_Alkhalifah_4Hz_MSE__randinitialization_error.pdf}
%			\caption{Validation errors}
		\end{subfigure}
		
		\begin{subfigure}[b]{0.32\textwidth}
			\includegraphics[width=\textwidth]{Alkhalifah_Huang4Hz.pdf}
%			\caption{Prediction: \cite{Alkhalifah2024} method}
		\end{subfigure}
		\begin{subfigure}[b]{0.32\textwidth}
			\includegraphics[width=\textwidth]{ProposedGabor_4Hz.pdf}
%			\caption{Prediction: Proposed Gabor-PINN}
		\end{subfigure}
		\begin{subfigure}[b]{0.32\textwidth}
			\includegraphics[width=\textwidth]{4Hz_FD.pdf}
%			\caption{Reference wavefield (FD)}
		\end{subfigure}
		
\caption{Comparison of training losses, validation errors, and scattered wavefield predictions for Test 2 (4 Hz) using three different initializations of weights. Panels (a) and (b) show the loss curves for \cite{Alkhalifah2024} and our proposed Gabor-PINN, respectively, (c) presents the validation errors for both methods. (d)-(f) display the wavefield predictions from \cite{Alkhalifah2024}, our Gabor-PINN, and the finite difference (FD) reference solution.}
\label{fig:4Hz_test}

	\end{figure}
	
\subsection{Test 3:  Higher-frequency wavefield}

In this experiment, we simulate a 20 Hz wavefield using the same velocity model from previous tests. The higher frequency introduces greater oscillatory behavior, making this test more challenging, and the scattered solution more complex. We design the neural network with three hidden layers, each containing 128 neurons. In the proposed Gabor-PINN method, this results in 64 Gabor basis functions. Additionally, we employ an encoder layer with \( K = 5 \) for positional encoding to better capture the high-frequency features of the wavefield. During training, we use 90,601 randomly varying collocation points in each step.

Figure \ref{fig:20Hz_test}a and b show the evolution of the training and validation losses for the simple PINN and the proposed Gabor-PINN methods, using Equation \ref{eq:loss_total} as the loss function. The Gabor-PINN demonstrates superior performance, achieving the fastest convergence and consistently lower losses. The validation error shown in Figure \ref{fig:20Hz_test}c further confirms its accuracy, with the Gabor-PINN achieving significantly lower errors at an earlier stage of training.

To evaluate the quality of the modeled wavefield, Figures \ref{fig:20Hz_test}d-e present the real part of the wavefield for both PINN methods and the finite difference simulation (reference) on a $300 \times 300$ grid.

	
	
	\begin{figure}[h]
		\centering
		\begin{subfigure}[b]{0.32\textwidth}
			\includegraphics[width=\textwidth]{20Hz_Loss_simplePINN.pdf}
			%			\caption{\cite{Alkhalifah2024} method}
		\end{subfigure}
		\begin{subfigure}[b]{0.32\textwidth}
			\includegraphics[width=\textwidth]{20Hz_Loss_Gabor_PINN.pdf}
			%			\caption{Loss: Proposed Gabor-PINN}
		\end{subfigure}
		\begin{subfigure}[b]{0.32\textwidth}
			\includegraphics[width=\textwidth]{20Hz_errors.pdf}
			%			\caption{Validation errors}
		\end{subfigure}
		
		\begin{subfigure}[b]{0.32\textwidth}
			\includegraphics[width=\textwidth]{20Hz_prediction_SimplePINN.pdf}
			%			\caption{Prediction: \cite{Alkhalifah2024} method}
		\end{subfigure}
		\begin{subfigure}[b]{0.32\textwidth}
			\includegraphics[width=\textwidth]{20Hz_prediction_Gabor.pdf}
			%			\caption{Prediction: Proposed Gabor-PINN}
		\end{subfigure}
		\begin{subfigure}[b]{0.32\textwidth}
			\includegraphics[width=\textwidth]{20Hz_FD.pdf}
			%			\caption{Reference wavefield (FD)}
		\end{subfigure}
		
\caption{Testing the proposed Gabor-PINN method and a simple PINN for simulating a 20 Hz scattered wavefield (Test 3) in the velocity model shown in Figure~\ref{fig:10Hz_fd_wavefoeld}.}
\label{fig:20Hz_test}
	\end{figure}
	
\subsection{Test 4: Effect of PML in the Marmousi Model}

In this test, we evaluate the effect of the perfectly matched layer (PML) implementation using a portion of the benchmark Marmousi velocity model for a 10 Hz wavefield modeling. The relatively nonsmooth Marmousi model introduces high-contrast velocity variations, providing a challenging scenario for wavefield modeling. Figure~\ref{fig:Marmousi_prediction}a shows the velocity model, while Figure~\ref{fig:Marmousi_prediction}b presents the reference finite difference (FD) wavefield, presented on a 100 by 150 grid (using four times finer grid spacing for computation). The neural network architecture consists of 4 hidden layers, each with 128 neurons, and an encoder layer with \( K = 3 \). The network is trained for 100,000 epochs with an exponentially decaying learning rate, starting at 0.002 and decaying to 0.0007.

We train both the simple PINN and our Gabor-PINN with and without the PML implementation. First, the networks are trained using the loss function based on equations \eqref{eq:loss_total} and \eqref{eq:loss_pde}, which do not include the PML. During training, we use 15,251 randomly varying collocation points in each step. Figure~\ref{fig:Marmousi_losses} shows the evolution of training and validation losses, as well as the validation errors compared to the reference FD wavefield. The simple PINN starts to converge after approximately 22,000 epochs, while the Gabor-PINN converges almost immediately and achieves higher accuracy.

Next, we define the loss using equations \eqref{eq:loss_total} and \eqref{eq:loss_pml}, incorporating the proposed definition for \( u_0 \) to include the PML, using $a_0=1$. We add a PML with the thickness of 0.5 km, and to account for the larger domain due to the PML region, we increase the number of collocation points proportionally, using 30,351 collocation points in each step. Our implementation does not increase the training time, except for the additional time required due to the larger number of collocation points. With the inclusion of PML, the convergence of the simple PINN is delayed until approximately 44,000 epochs (Figure~\ref{fig:Marmousi_losses}), whereas the Gabor-PINN's convergence is hardly affected. Both methods, however, achieve higher final accuracies with the PML included.

Figure~\ref{fig:Marmousi_prediction} also shows the Gabor-PINN predictions with and without the PML. The wavefield decays successfully within the PML region, confirming the effectiveness of the PML implementation.


	
		\begin{figure}[h]
		\centering
		\begin{subfigure}[b]{0.32\textwidth}
			\includegraphics[width=\textwidth]{Marmousi_velocity.pdf}
			%			\caption{\cite{Alkhalifah2024} method}
		\end{subfigure}
		\begin{subfigure}[b]{0.32\textwidth}
			\includegraphics[width=\textwidth]{Marmousi_FD.pdf}
			%			\caption{Loss: Proposed Gabor-PINN}
		\end{subfigure}
		\\
		\begin{subfigure}[b]{0.32\textwidth}
			\includegraphics[width=\textwidth]{Marmousi_prediction_GaborPINN.pdf}
			%			\caption{Validation errors}
		\end{subfigure}
		\begin{subfigure}[b]{0.32\textwidth}
			\includegraphics[width=\textwidth]{Marmousi_prediction_GaborPML.pdf}
			%			\caption{Prediction: \cite{Alkhalifah2024} method}
		\end{subfigure}
		
\caption{The Marmousi velocity model (Test 4) shown in panel (a) and the reference finite difference (FD) wavefield in panel (b). Panels (c) and (d) show the Gabor-PINN wavefield predictions without and with PML, respectively.}
\label{fig:Marmousi_prediction}
	\end{figure}
	
	\begin{figure}[h]
		\centering
		\begin{subfigure}[b]{0.32\textwidth}
			\includegraphics[width=\textwidth]{Marmousi_loss_Simple_PINN_constraint.pdf}
			%			\caption{\cite{Alkhalifah2024} method}
		\end{subfigure}
		\begin{subfigure}[b]{0.32\textwidth}
			\includegraphics[width=\textwidth]{Marmousi_loss_Gabor-PINN_constraint.pdf}
			%			\caption{Loss: Proposed Gabor-PINN}
		\end{subfigure}
		\\
		\begin{subfigure}[b]{0.32\textwidth}
			\includegraphics[width=\textwidth]{Marmousi_loss_Simple_PINN_PML.pdf}
			%			\caption{Validation errors}
		\end{subfigure}
		\begin{subfigure}[b]{0.32\textwidth}
			\includegraphics[width=\textwidth]{Marmousi_loss_Gabor-PINN_PML.pdf}
			%			\caption{Prediction: \cite{Alkhalifah2024} method}
		\end{subfigure}
%		\begin{subfigure}[b]{0.32\textwidth}
%			\includegraphics[width=\textwidth]{20Hz_prediction_Gabor.pdf}
%			%			\caption{Prediction: Proposed Gabor-PINN}
%		\end{subfigure}
\\
		\begin{subfigure}[b]{0.55\textwidth}
			\includegraphics[width=\textwidth]{Marmousi_PML_errors.pdf}
			%			\caption{Reference wavefield (FD)}
		\end{subfigure}
		
\caption{Comparison of training and validation losses for the simple PINN and proposed Gabor-PINN methods with and without PML (Test 4, related to Figure~\ref{fig:Marmousi_prediction}). The Gabor-PINN demonstrates superior convergence and accuracy, while the simple PINN shows a significant delay in convergence. Validation errors for both methods are shown, illustrating the improved final accuracy when PML is applied.}
\label{fig:Marmousi_losses}
	\end{figure}
	
	
	\subsection{Test 5: Overthrust Velocity Model}  
	In this test, we evaluate the performance of our Gabor-PINN on the complex Overthrust velocity model. This model, depicted in Figure~\ref{fig:test3_OVE_results}a, has similarities to the subsurface in the Canadian foothills, featuring two overthrust faults. It is larger and more challenging than the previous models.  
	
	We design a neural network with 4 hidden layers, each containing 128 neurons (resulting in 64 Gabor basis functions for our Gabor-PINN) and positional encoding with \( K = 3 \). The loss function is defined based on equations \eqref{eq:loss_total} and \eqref{eq:loss_pml}, which include the PML. The network is trained using the same learning rate schedule as in the previous test.  
	
	Figures~\ref{fig:test3_OVE_results}b and c show the evolution of training loss and validation error for the simple PINN and the proposed Gabor-PINN. The simple PINN becomes unstable during training, resulting in frequent jumps in the loss and validation error. In contrast, the proposed Gabor-PINN demonstrates stable convergence.  
	
	Figures~\ref{fig:test3_OVE_results}d and e show the wavefield predictions from the simple PINN and Gabor-PINN on the validation grid points. Panels (f) and (g) present the difference between the predictions and the finite difference (FD) reference wavefield. The proposed Gabor-PINN prediction is significantly more accurate than the simple PINN. However, the prediction still misses several fine details due to the low-frequency bias of the learned function. The sharpness of the solution may be improved by implementing a hybrid optimization scheme to reduce the spectral bias, which will be the focus of our future studies.  
\begin{table}[h]
	\centering
	\renewcommand{\arraystretch}{1.2} % Increase row spacing for readability
	\begin{tabular}{lccc p{4cm}} 
		\toprule
		\textbf{Test} & \textbf{Layers} & \textbf{Neurons} & \textbf{\( K \)} & \textbf{Collocation Points} \\ 
		\midrule
		Test 1 (10 Hz) & 3 & 64 & 3 & 2,601 \\ 
		Test 2 (4 Hz) & 3 & 32 & 3 & 5,041 \\ 
		Test 3 (20 Hz) & 3 & 128 & 5 & 90,601 \\ 
		Test 4 (Marmousi) & 4 & 128 & 3 & 15,251 (without PML) \\ 
		&   &     &   & 30,351 (with PML) \\  
		Test 5 (Overthrust) & 4 & 128 & 3 & 108,741 (with PML) \\  
		\bottomrule
	\end{tabular}
	\caption{Summary of hyperparameters for each test, including the number of hidden layers, neurons per layer, positional encoding parameter \( K \), and collocation points. The number of Gabor basis functions is half the number of neurons per layer.}
	\label{tab:hyperparameters}
\end{table}

	
	
	\begin{figure}[htbp]
		\centering
		\begin{minipage}{0.48\textwidth}
			\centering
			\includegraphics[width=\textwidth]{OVE_velocity.pdf}
			\caption*{(a)}
		\end{minipage}
		\begin{minipage}{0.25\textwidth}
			\centering
			\includegraphics[width=\textwidth]{OVE_PML_Loss.pdf}
			\caption*{(b)}
		\end{minipage}
				\begin{minipage}{0.25\textwidth}
			\centering
			\includegraphics[width=\textwidth]{OVE_PML_errors.pdf}
			\caption*{(c)}
		\end{minipage}
		\\
		\begin{minipage}{0.48\textwidth}
			\centering
			\includegraphics[width=\textwidth]{OVE_10Hz_prediction_PINN_PML.pdf}
			\caption*{(d)}
		\end{minipage}
		\begin{minipage}{0.48\textwidth}
			\centering
			\includegraphics[width=\textwidth]{OVE_10Hz_prediction_GaborPINN_PML.pdf}
			\caption*{(e)}
		\end{minipage}
				\\
		\begin{minipage}{0.49\textwidth}
			\centering
			\includegraphics[width=\textwidth]{OVE_10Hz_difference_PINN_PML.pdf}
			\caption*{(f)}
		\end{minipage}
		\begin{minipage}{0.49\textwidth}
			\centering
			\includegraphics[width=\textwidth]{OVE_10Hz_difference_GaborPINN_PML.pdf}
			\caption*{(g)}
		\end{minipage}
		\caption{(a) The Overthrust velocity model. (b) Training loss and (c) validation error evolution for the simple PINN and proposed Gabor-PINN. The simple PINN exhibits unstable training, while the Gabor-PINN shows stable convergence. (d) and (e) show wavefield predictions on validation grid points for the simple PINN and Gabor-PINN, respectively. (f) and (g) present the prediction differences with the finite difference reference wavefield. The Gabor-PINN prediction is more accurate but still affected by low-frequency bias.}
		\label{fig:test3_OVE_results}
	\end{figure}


\section*{Conclusion}

We proposed a  Physics-Informed Neural Network (PINN) framework that incorporates explicit Gabor basis functions for simulating scattered wavefields governed by the Helmholtz equation. Unlike previous approaches that used Gabor functions as multiplicative activation functions throughout the network or introduced auxiliary networks to learn both the Gabor centers and magnitudes, our method significantly reduces the complexity of the network architecture. We achieved this by redefining the task of the network to learning a mapping from input coordinates to a custom Gabor coordinate system. Since the Gabor functions already incorporate the oscillatory behavior of the wavefield, the mapping learned by the network can be smooth and easily learned. This approach allowed us to simplify the Gabor basis functions by eliminating unnecessary parameters, absorbing the effects of both Gabor centers and magnitudes into a single coordinate transformation. 

As demonstrated by numerical experiments, our simplified Gabor-PINN offers several key advantages: improved robustness with respect to hyperparameter selection, faster convergence, and enhanced accuracy compared to previous Gabor-based PINN implementations and a simple PINN with the same architecture and number of trainable parameters. Additionally, we derived the required equations and demonstrated how the Perfectly Matched Layer (PML) can be effectively integrated into our framework, further improving accuracy and boundary behavior without additional computational cost except the unavoidable cost added due to the extra domain size.

The results across different velocity models and frequencies confirmed the superiority of our method, particularly in terms of stability and accuracy in handling high-frequency and complex wavefield simulations. Future work will explore hybrid optimization strategies to reduce the remaining low-frequency bias and enhance the resolution of finer wavefield details.

\section*{CRediT authorship contribution statement}

\textbf{Mohammad Mahdi Abedi:} Conceptualization, Methodology, Writing – original draft, Data curation, Investigation, Software, Visualization.  
\textbf{David Pardo:} Conceptualization, Funding acquisition, Supervision, Writing – review \& editing.  
\textbf{Tariq Alkhalifah:} Conceptualization, Supervision, Writing – review \& editing.

\section*{Declaration of competing interest}
The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.

\section*{Code Availability}  
The implementation of the proposed Gabor-Enhanced Physics-Informed Neural Network (PINN) for solving the Helmholtz equation is available as an open-source repository. The code, along with instructions for usage and reproducibility, can be accessed at:  
\begin{center}
	\url{https://github.com/mahdiabedi/Gabor-Enhanced-PINN}
\end{center}  

\section*{Acknowledgments}
This research was supported by th following Research Projects/Grants: European Union’s Horizon Europe research and innovation programme under the Marie Sklodowska-Curie grant agreement No 101119556.TED2021-132783B-I00 funded by MICIU/AEI /10.13039/501100011033 and by FEDER, EU; PID2023-146678OB-I00 funded by MICIU/AEI /10.13039/ 501100011033 and by the European Union Next Generation EU/ PRTR; “BCAM Severo Ochoa” accreditation of excellence CEX2021-001142-S funded by MICIU/AEI/ 10.13039/ 501100011033; Basque Government through the BERC 2022-2025 program; BEREZ-IA (KK-2023/00012) and RUL-ET(KK-2024/00086), funded by the Basque Government through ELKARTEK; Consolidated Research Group MATHMODE (IT1456-22) given by the Department of Education of the Basque Government; BCAM-IKUR-UPV/EHU, funded by the Basque Government IKUR Strategy and by the European Union Next Generation EU/PRTR. 



%	
%	\subsection{Effect of Hyperparameters on Model Performance}
%	
%	To further evaluate the robustness and efficiency of the proposed Gabor LS-PINN, we analyze the effects of key hyperparameters on model performance. Specifically, we investigate:
%	\begin{enumerate}
%		\item The value of \( K \) in the encoder layer.
%		\item The number of layers in the network.
%		\item The number of neurons per layer.
%		\item The random initialization of neural network weights.
%	\end{enumerate}
%	
%	In each test, we vary one hyperparameter while keeping all other settings fixed. The tests involve modeling the 10Hz wavefield in the velocity model presented in Figure \ref{fig:10Hz_fd_wavefoeld}. For each hyperparameter, three different values are tested to assess its impact on the training loss and prediction error.
%	
%	Figure \ref{fig:hyperparam_analysis} summarizes the results. Each row corresponds to one hyperparameter, and the columns display, The evolution of traning loss in the simple PINN,  Gabor LS-PINN, and the prediction errors for validation collocation points (computed with respect to the finite difference modeled wavefield in Figure \ref{fig:10Hz_fd_wavefoeld}).
%	
%	
%	The results reveal that the convergence and final accuracy of the simple PINN are highly sensitive to changes in hyperparameters. In contrast, the Gabor LS-PINN consistently exhibits robust performance, with minimal variation in both convergence and accuracy across all tests. This robustness highlights the efficacy of the proposed method in addressing the challenges posed by hyperparameter sensitivity in traditional PINNs.
%	
%	
%	\begin{figure}[htb]
%		\centering
%		% Row 1: Effect of K value
%		\begin{minipage}{0.32\textwidth}
%			\centering
%			\includegraphics[width=\textwidth]{parametertest_1_encoder_PINN.pdf}
%			\caption*{(a.i)}
%		\end{minipage}
%		\begin{minipage}{0.32\textwidth}
%			\centering
%			\includegraphics[width=\textwidth]{parametertest_1_encoder_GaborLS.pdf}
%			\caption*{(a.ii)}
%		\end{minipage}
%		\begin{minipage}{0.32\textwidth}
%			\centering
%			\includegraphics[width=\textwidth]{parametertest_1_encoder_error.pdf}
%			\caption*{(a.iii)}
%		\end{minipage}
%		\\
%		% Row 2: Effect of Number of Layers
%		\begin{minipage}{0.32\textwidth}
%			\centering
%			\includegraphics[width=\textwidth]{parametertest_4_layers_PINN.pdf}
%			\caption*{(b.i)}
%		\end{minipage}
%		\begin{minipage}{0.32\textwidth}
%			\centering
%			\includegraphics[width=\textwidth]{parametertest_4_layers_GaborLS.pdf}
%			\caption*{(b.ii)}
%		\end{minipage}
%		\begin{minipage}{0.32\textwidth}
%			\centering
%			\includegraphics[width=\textwidth]{parametertest_4_layers_error.pdf}
%			\caption*{(b.iii)}
%		\end{minipage}
%		\\
%		% Row 3: Effect of Neurons per Layer
%		\begin{minipage}{0.32\textwidth}
%			\centering
%			\includegraphics[width=\textwidth]{parametertest_2_neuron_PINN.pdf}
%			\caption*{(c.i)}
%		\end{minipage}
%		\begin{minipage}{0.32\textwidth}
%			\centering
%			\includegraphics[width=\textwidth]{parametertest_2_neuron_GaborLS.pdf}
%			\caption*{(c.ii)}
%		\end{minipage}
%		\begin{minipage}{0.32\textwidth}
%			\centering
%			\includegraphics[width=\textwidth]{parametertest_2_neuron_error.pdf}
%			\caption*{(c.iii)}
%		\end{minipage}
%		\\
%		% Row 4: Effect of Initialization
%		\begin{minipage}{0.32\textwidth}
%			\centering
%			\includegraphics[width=\textwidth]{parametertest_3_randinitialization_PINN.pdf}
%			\caption*{(d.i)}
%		\end{minipage}
%		\begin{minipage}{0.32\textwidth}
%			\centering
%			\includegraphics[width=\textwidth]{parametertest_3_randinitialization_GaborLS.pdf}
%			\caption*{(d.ii)}
%		\end{minipage}
%		\begin{minipage}{0.32\textwidth}
%			\centering
%			\includegraphics[width=\textwidth]{parametertest_3_randinitialization_error.pdf}
%			\caption*{(d.iii)}
%		\end{minipage}
%		\caption{Effect of hyperparameters on model performance: (a) \( K \) value in the encoder layer, (b) number of layers, (c) number of neurons per layer, and (d) random initialization of weights. Each row corresponds to one hyperparameter, and the columns show: (i) training losses for the simple PINN, (ii) training losses for the Gabor LS-PINN, and (iii) prediction errors for both methods. The dotted lines correspond to simple PINN and the solid line correspond to Gabor LS-PINN. The Gabor LS-PINN demonstrates consistent robustness and accuracy regardless of hyperparameter variations, while the simple PINN is significantly influenced.}
%		\label{fig:hyperparam_analysis}
%	\end{figure}
%	
%	
\appendix
\section*{Appendix A: Scattered Wave Equation with PML}
	\renewcommand{\theequation}{A.\arabic{equation}}
	\setcounter{equation}{0} % Reset equation counter for appendix
	
	 For an isotropic medium with wave velocity $v(\mathbf{x})$, the Helmholtz equation with PML is given by \citep{berenger1994perfectly,chen2013optimal,song2022versatile}:
	\begin{equation}
		\frac{\partial}{\partial x}\left(\frac{e_z}{e_x} \frac{\partial u}{\partial x} \right)+ \frac{\partial}{\partial z}\left(\frac{e_x}{e_z} \frac{\partial u}{\partial z} \right)+ {e_z}{e_x} \omega^2 m(\mathbf{x}) u(\mathbf{x}) = f(\mathbf{x}),
		\label{eq:helmholtz_pml}
	\end{equation}
	where $m(\mathbf{x}) = \frac{1}{v^2(\mathbf{x})}$ is the slowness squared. The terms $e_x$ and $e_z$ represent the complex stretching along the $x$ and $z$ directions, respectively, in the PML regions, defined in Equation \ref{eq:ex_ez}. The solution $u(\mathbf{x}) = u^r(\mathbf{x}) + \mathrm{i} u^i(\mathbf{x})$ is a complex-valued wavefield, where $\mathbf{x} = \{x, z\}$ denotes the spatial coordinates in a 2D domain.
		
		To derive the scattered Helmholtz equation, we decompose the total wavefield into a background wavefield \( u_0(\mathbf{x}) \) and a scattered wavefield \( u_s(\mathbf{x}) \):
		
		\begin{equation*}
			u(x) = u_0(\mathbf{x}) + u_s(\mathbf{x}), 
		\end{equation*}
and substitute this into Equation \eqref{eq:helmholtz_pml}, to obtain:
		
		\begin{equation}
			\frac{\partial}{\partial x} \left( \frac{e_z}{e_x} \frac{\partial (u_0 + u_s)}{\partial x} \right) +
			\frac{\partial}{\partial z} \left( \frac{e_x}{e_z} \frac{\partial (u_0 + u_s)}{\partial z} \right) +
			e_x e_z \omega^2 m(x) (u_0 + u_s) = f(x).
			\label{eq:A8}
		\end{equation}
	The background wavefield \( u_0(x) \) should satisfy the Helmholtz equation with PML \eqref{eq:helmholtz_pml} in a homogeneous medium:
		
		\begin{equation}
			\frac{\partial}{\partial x} \left( \frac{e_z}{e_x} \frac{\partial u_0}{\partial x} \right) +
			\frac{\partial}{\partial z} \left( \frac{e_x}{e_z} \frac{\partial u_0}{\partial z} \right) +
			e_x e_z \omega^2 m_0 u_0 = f(x),
				\label{eq:A9}
		\end{equation}
where \( m_0 = \frac{1}{v_0^2} \) is the background slowness squared.
		
The amplitude decay inside the PML in $x$ direction is given by:
\begin{equation}
	e^{-\int \frac{\omega c l_x^2}{v_0 (1 + c^2 l_x^4)} \, dx}.
\end{equation}
Using the analytical solution for the background wavefield in a homogeneous medium and approximating the amplitude decay for small $c$, we find the analytical approximation of $u_0(\mathbf{x})$ that satisfies \eqref{eq:A9}:

	\begin{equation}
u_0(\mathbf{x}) = \frac{i}{4} H_0^{(2)}\left(\frac{\omega}{v_0}  |\mathbf{x} - \mathbf{x}_s|\right) e^{-\omega c \frac{(l_x^2+l_z^2)}{3 v_0}^{(3/2)}},
	\end{equation}
where $H_0^{(2)}\left(.\right)$ is the zero-order Hankel function of the second kind.

Subtracting  \eqref{eq:A9}  from the total wave equation  \eqref{eq:A8} results in the scattered Helmholtz equation with PML:
		
		\begin{equation}
			\frac{\partial}{\partial x} \left( \frac{e_z}{e_x} \frac{\partial u_s}{\partial x} \right) +
			\frac{\partial}{\partial z} \left( \frac{e_x}{e_z} \frac{\partial u_s}{\partial z} \right) +
			e_x e_z \omega^2 m(x) u_s = - e_x e_z \omega^2 \delta m(x) u_0(x).
			\label{eq:scattered_helmholtz_pml}
		\end{equation}
		
	
	Using the mean squared difference between the two sides of the above equation over $N$ collocation points to define loss function $\mathcal{L}$, we obtain:
	\begin{equation}
		\mathcal{L} = \frac{1}{N} \sum \left( {L}_r^2 + {L}_i^2 \right),
	\end{equation}
	where ${L}_r$ and ${L}_i$ are the real and imaginary parts of the loss. For implementation efficiency, we explicitly calculate the real and imaginary parts of the loss to avoid using complex numbers during training. 
	
	The terms ${L}_r$ and ${L}_i$ are calculated as:
	\begin{align}
		{L}_r &= F_{1}^r + F_{2}^r +\left( 1 - c^2 l_x^2 l_z^2 \right) \omega^2 \left( m  u_s^r +   u_0^r \delta m \right) 
		+ c \omega^2 \left( l_x^2 + l_z^2 \right) \left( m  u_s^i +   u_0^i \delta m\right), \\
		{L}_i &= F_{1}^i + F_{2}^i  +\left( 1 - c^2 l_x^2 l_z^2 \right) \omega^2 \left( m  u_s^i +  u_0^i \delta m\right) 
		- c \omega^2 \left( l_x^2 + l_z^2 \right) \left( m  u_s^r +   u_0^r \delta m\right),
	\end{align}
using the following definitions for the auxiliary terms,
\begin{equation}
	\begin{aligned}
		C_1 &= \frac{1 + c^2 l_x^2 l_z^2}{1 + c^2 l_x^4}, \quad
		C_2 = \frac{c (l_x^2 - l_z^2)}{1 + c^2 l_x^4}, \\
		C_3 &= \frac{1 + c^2 l_x^2 l_z^2}{1 + c^2 l_z^4}, \quad
		C_4 = \frac{c ( l_z^2-l_x^2 )}{1 + c^2 l_z^4}.
	\end{aligned}
\end{equation}
	and calculating the derivatives of these auxiliary terms,
	\begin{equation}
	\begin{aligned}
		\frac{\partial C_1}{\partial x} &= \frac{2 c^2 l_x \big(-2 l_x^2 + l_z^2 - c^2 l_x^4 l_z^2 \big) }{\big(1 + c^2 l_x^4\big)^2}\frac{\partial l_x}{\partial x}, \\
		\frac{\partial C_2}{\partial x} &= \frac{2 c l_x \big(1 - c^2 l_x^2 (l_x^2 - 2 l_z^2)\big) }{\big(1 + c^2 l_x^4\big)^2}\frac{\partial l_x}{\partial x}, \\
		\frac{\partial C_3}{\partial z} &= \frac{2 c^2 l_z \big(-2 l_z^2 + l_x^2  - c^2 l_z^4 l_x^2\big) }{\big(1 + c^2 l_z^4\big)^2}\frac{\partial l_z}{\partial z}, \\
		\frac{\partial C_4}{\partial z} &= \frac{2 c l_z \big(1 - c^2 l_z^2 (-2 l_x^2 + l_z^2)\big) }{\big(1 + c^2 l_z^4\big)^2}\frac{\partial l_z}{\partial z}.
	\end{aligned}
	\end{equation}
	we find:
	\begin{equation}
	\begin{aligned}
		F_{1}^r &= \frac{\partial C_1}{\partial x} \frac{\partial u_s^r}{\partial x} 
		+ C_1 \frac{\partial^2 u_s^r}{\partial x^2}
		- \frac{\partial C_2}{\partial x} \frac{\partial u_s^i}{\partial x}
		- C_2 \frac{\partial^2 u_s^i}{\partial x^2}, \\
		F_{2}^r &= \frac{\partial C_3}{\partial z} \frac{\partial u_s^r}{\partial z} 
		+ C_3 \frac{\partial^2 u_s^r}{\partial z^2}
		- \frac{\partial C_4}{\partial z} \frac{\partial u_s^i}{\partial z}
		- C_4 \frac{\partial^2 u_s^i}{\partial z^2}, \\
		F_{1}^i &= \frac{\partial C_1}{\partial x} \frac{\partial u_s^i}{\partial x} 
		+ C_1 \frac{\partial^2 u_s^i}{\partial x^2}
		+ \frac{\partial C_2}{\partial x} \frac{\partial u_s^r}{\partial x}
		+ C_2 \frac{\partial^2 u_s^r}{\partial x^2}, \\
		F_{2} ^i &= \frac{\partial C_3}{\partial z} \frac{\partial u_s^i}{\partial z} 
		+ C_3 \frac{\partial^2 u_s^i}{\partial z^2}
		+ \frac{\partial C_4}{\partial z} \frac{\partial u_s^r}{\partial z}
		+ C_4 \frac{\partial^2 u_s^r}{\partial z^2}.
	\end{aligned}
\end{equation}

	From the above equations, we can see that with the addition of PML, each term of the loss depends on both the real and imaginary components of the NN output. The first-order derivatives of $l$ and one extra hyper-parameter $a_0$ also contribute to the added complexity.   

%\bibliographystyle{elsarticle}
\bibliographystyle{apalike}
\bibliography{references}

\end{document}
