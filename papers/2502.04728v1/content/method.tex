\section{The Proposed Test-time Compute Scaling Approach}
Our proposed methods aim to explore the following key questions:
\vspace{2mm}
\begin{itemize}[leftmargin=*,nosep]
\setlength\itemsep{0.4em}
    \item \textbf{How can PDDL serve as a good world representation for planning?}
    Natural language task planning faces significant challenges in state estimation, constraint-based plan generation, and plan validation. How can we create explicit, unambiguous world models? (see Section~\ref{swm})
    \item \textbf{How can we effectively generate PDDL-based world models?}
    Generating symbolic world models requires not only natural language understanding but also sophisticated deductive reasoning to maintain logical consistency across all model components.
Without such formal modeling sophistication, models risk generating inconsistent state transitions and producing suboptimal plans.
To address this challenge, we enhance LLMs' reasoning capabilities through an instance verbalized machine learning algorithm (see Section ~\ref{text:ivml}), initializing it with good candidates generated by best-of-N sampling.
\end{itemize}


\subsection{PDDL-based World Model Representation}
\label{swm}
Classical planning problems are inherently complex.
Even determining plan satisfiability~\cite{russell2016artificial}—whether any solution exists for a given planning problem—is NP-hard. 
Planning problems that involve optimization under constraints pose even greater challenges for natural language planners.
Our approach, which utilizes the PDDL-based representation, offers several advantages:
(1) PDDL employs a logical system to express atoms and predicates derived from STRIPS~\cite{fikes1971strips} (\ie, Stanford Research Institute Problem Solver), and therefore it provides a formal and unambiguous syntax for representing world models. 
PDDL-based world modeling employs explicit representations that not only enrich the description of actions and states but also ensure precise and straightforward validation, thereby eliminating ambiguity.
(2) Natural language plan prediction often reduces to an n-gram task of ungrounded token generation. 
We provide the description of the Termes problem in both natural language (see Figure~\ref{o1-plan}) and PDDL (see the text box below) to illustrate the difference.
By adopting PDDL as representation, we transform this task into explicit classical planning. 
This formulation enables the use of search algorithms such as A$\ast$. With the help of the PDDL representation, a planning graph (\eg, Figure~\ref{fig:graph})  can be used to provide improved heuristic estimates when searching through the state space.


\begin{figure}[h!]
\centering
\begin{tcolorbox}[title = {Termes in Planning Domain Definition Language},
   fonttitle = \bfseries, fontupper = \sffamily\tiny, fontlower = \sffamily\tiny, colframe=c1, colback=green2!5]
\textbf{Domain}:\\
\vspace{-3mm}
    \begin{lstlisting}
(define (domain termes)
    (:requirements :typing :negative-preconditions)
    (:types
        numb - object
        position - object)
    (:predicates
        (height ?p - position ?h - numb)
        (at ?p - position)
        (has-block)
        (SUCC ?n1 - numb ?n2 - numb)
        (NEIGHBOR ?p1 - position ?p2 - position)
        (IS-DEPOT ?p - position))
    (:action move
        :parameters (?from - position ?to - position ?h - numb)
        :precondition (and (at ?from)(NEIGHBOR ?from ?to)(height ?from ?h)(height ?to ?h))
        :effect (and (not (at ?from))(at ?to) ) )
    (:action move-up
        :parameters (?from - position ?hfrom - numb ?to - position ?hto - numb)
        :precondition (and(at ?from)(NEIGHBOR ?from ?to)(height ?from ?hfrom)(height 
        ?to ?hto)(SUCC ?hto ?hfrom))
        :effect (and(not (at ?from))(at ?to)))

    (:action move-down ...
    (:action place-block ...
    (:action remove-block ...
    (:action create-block ...
    (:action destroy-block ...
)
\end{lstlisting}

\textbf{Problem}:\\
\vspace{-3mm}
\begin{lstlisting}
(define (problem termes-00038-0036-4x3x3-random_towers_4x3_3_1_3)
(:domain termes)
; termes-00038-0036-4x3x3-random_towers_4x3_3_1_3
; Initial state:
;  0   0  R0D  0
;  0   0   0   0
;  0   0   0   0
; Goal state:
;  0   0   0   0
;  0   0   0   0
;  0   3   0   0
; Maximal height: 3
(:objects
    n0 - numb......
    pos-0-0 - position......
)
(:init
    (height pos-0-0 n0)......
    (at pos-2-0)
    (SUCC n1 n0)......
    (NEIGHBOR pos-0-0 pos-1-0)......
    (IS-DEPOT pos-2-0)
)
(:goal
(and (height pos-0-0 n0) ...... (not (has-block)))))
\end{lstlisting}

\label{tbox:termes_pddl} 
\end{tcolorbox}
\end{figure}

\subsection{Best-of-N Sampling for PDDL Initialization}
~\label{text:bon}
\vspace{-4mm}

Our test-time scaling approach adopts a two-stage coarse-to-fine optimization process. The first stage is to coarsely search a good initial solution, and the second stage is to iteratively refine this solution in a fine-grained manner.
To efficiently get a diverse set of plausible solutions, before LLM iteratively the internal logic with iVML, we adopt Best-of-N sampling to find a good solution as the initial PDDL-based world model. 
For each problem, the LLM generates $ N $ candidate solutions in parallel and retains the $ K $ samples with the highest log-likelihoods.
This process involves three main steps: candidate generation, scoring, and selection.
During sampling, a high temperature parameter is used to add more randomness and diversity to the solution space. 
Each candidate $ c_i $ is assigned a score $ S_i $ based on the sum of the log-likelihoods of its generated tokens:
\begin{align} \label{eq:bon}
S_i = \sum_{t=1}^{L_i} \log p_t\left(w_t^{(i)}\right), \quad \forall i \in \{1, 2, \dots, N\}
\end{align}
where:$ L_i $ is the length of candidate $ c_i $, $ w_t^{(i)} $ is the $ t $-th token of candidate $ c_i $, $ p_t\left(w_t^{(i)}\right) $ is the probability of token $ w_t^{(i)} $ at position $ t $.
During the selection phase, the top $ K $ candidates with the highest scores are chosen as the initialization points to be optimized by iVML. 

\subsection{iVML: Instance Verbalized Machine learning} 
~\label{text:ivml}
With the BoN sampling to select the candidates as the initial solution, we introduce instance verbalized machine learning to refine both the generated PDDL domain and the natural language chain of thought. iVML is an adaptation of verbalized machine learning~\cite{xiao2024verbalized} to the instance optimization setting, where the goal is to optimize and refine a single instance (\ie, PDDL domains in this paper).
In iVML, functions are parameterized using natural language rather than numerical values.
Viewing an LLM as the inference engine, we can evaluate such a natural language parameterized function, and optimize its model parameters in the natural language space.
In our setting, we are given a description $\mathcal{G}$ from the planning domain either in natural language for NL2Domain or in problem code for Prob2Domain, and we aim to generate an accurate corresponding PDDL domain description $\mathbf{D}^*$, \ie,
\begin{align} \label{eq:loss}
    \mathbf{D}^* = \arg\min_\mathbf{D} \; \mathcal{L}(\mathcal{G}, \mathbf{D})
\end{align}
where $\mathcal{L}(\cdot)$ is a loss function defining the closeness between $\mathcal{G}$ and $\mathbf{D}$.
Solving \Cref{eq:loss} is difficult as both $\mathcal{G}$ and $\mathbf{D}$ are text, and $\mathcal{L}(\cdot)$ is hard to define unless abstractly using natural language.
Using the iVML framework, we can approximately solve \Cref{eq:loss} with an iterative algorithm that alternates between two natural language parameterized functions at the iteration $i$:
\begin{align}
    \mathbf{F}_{i} & = f_\mathrm{opt}(\mathcal{L}, \mathcal{G}, \mathbf{T}_{i-1}, \mathbf{D}_{i-1}), \\
    \mathbf{T}_{i}, \mathbf{D}_{i} & = f_\mathrm{update}(\mathbf{F}_{i}, \mathbf{T}_{i-1}, \mathbf{D}_{i-1}),
\end{align}
where $\mathbf{T}_{i-1}$ and $\mathbf{D}_{i-1}$ correspond to the current thoughts and the current PDDL domain, $\mathbf{F}_{i}$ is the feedback from the optimizer function $f_\mathrm{opt}(\cdot)$, $\mathbf{T}_{i}$ and $\mathbf{D}_{i}$ are the updated thoughts and PDDL domain output from the update function $f_\mathrm{update}(\cdot)$.
$\mathbf{D}_{0}$ is initialized from the best-of-N sampleing.
These two functions are evaluated through separate LLMs calls.
We show the prompt templates for $f_\mathrm{opt}(\cdot)$ and $f_\mathrm{update}(\cdot)$ below:

\begin{tcolorbox}[title = {Prompt template for $f_\mathrm{opt}(\cdot)$},
  fonttitle = \bfseries, fontupper = \sffamily\small, fontlower = \sffamily\small, colframe=c1, colback=green2!5]
You will be provided a natural language description of a planning domain, and its corresponding PDDL domain code with intermediate thoughts explaining each predicate and action. Your task is to generate critical feedback on the PDDL domain code based on the natural language description. 
You should evaluate the grammar and logic of the PDDL domain codes, and the logic error in the intermediate thoughts.

PDDL synthesis problem: \{$\mathcal{G}$\}\\
natural language chain of thoughts: \{$\mathbf{T}_{i-1}$\}\\
Generated PDDL domain: \{$\mathbf{D}_{i-1}$\}

\end{tcolorbox}

\begin{tcolorbox}[title = {Prompt template for $f_\mathrm{update}(\cdot)$},
  fonttitle = \bfseries, fontupper = \sffamily\small, fontlower = \sffamily\small, colframe=c1, colback=green2!5]
You will be provided a PDDL domain code and critical feedback on the PDDL domain code based on the natural language description.
Your task is to generate a new PDDL domain code that is more consistent with the natural language description.

PDDL synthesis problem: \{$\mathcal{G}$\}\\
Natural language chain of thoughts at the previous turn: \{$\mathbf{T}_{i-1}$\}\\
Generated PDDL domain at the previous turn: \{$\mathbf{D}_{i-1}$\}\\
The error of the PDDL domain \{$\mathbf{F}_{i-1}$\}

\end{tcolorbox}
\textbf{iVML with BoN initialization effectively balances exploration and exploitation.}
BoN employs a broad exploration strategy, maintaining diverse candidate solutions to probe distinct regions of the combinatorial search space. While this approach mitigates initialization bias, it suffers from diminishing returns: beyond a critical sample size, the probability of discovering novel valid solutions decays due to redundant model generations.
In contrast, iVML performs verbalized in-context exploitation by iteratively refining BoN-selected candidates based on objectives defined in natural language. This closed-loop process enables precise error correction (\eg, resolving precondition conflicts in PDDL actions) but remains susceptible to local minima—a fundamental challenge in non-convex optimization~\cite{sharony2024learning}.
Our approach combines BoN and iVML to achieve an effective balance between exploration and exploitation, addressing the limitations of each method alone through a two-phase optimization framework:
(1) BoN initialization that generates multiple diverse and high-quality initializations $\{\mathbf{D}_{(i)}^{0}\}_{i=1}^k$, and 
(2) iVML refinement that optimizes these initial solutions with verbalized machine learning. 








