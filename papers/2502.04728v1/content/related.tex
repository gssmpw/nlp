\section{Related Work}
\subsection{LLMs for Task Planning}

\begin{figure}[!t]
    \centering
    \includegraphics[width=1\textwidth]{content/imgs/o1_v10.pdf} 
    \vspace{-7mm}
    \caption{\small OpenAI-o1 plans for Termes: o1 frequently exhibits hallucination during the planning process. Specifically, in steps three and four, the LLM violates predefined rules when selecting and leveraging actions. Additionally, step four hallucinates the achievement of the goal, leading to incorrect or unrealistic outcomes. Even when using o1 itself to evaluate the hallucinated plan, it incorrectly identifies the plan as valid. 
    }
    \label{o1-plan}
\end{figure}
 a 
Recent advances in large language models (LLMs), such as OpenAI-o1~\cite{zeng2024scaling} and Qwen~\cite{yang2024qwen2}, have shown promise in handling common reasoning tasks such as GSM8K~\cite{cobbe2021gsm8k} and HumanEval~\cite{chen2021evaluating}.
The gain of reasoning and planning capabilities of LLMs can be attributed to (but not limited to) several factors:
(1) Extensive training on reasoning datasets: \cite{yu2024distilling} fine-tunes LLMs with distilled chain-of-thought datasets to improve plausible reasoning, and recent models have scaled this approach using larger and more diverse datasets, which has enhanced performance on tasks such as mathematical reasoning, coding, and logical reasoning. 
However, such an approach raises concerns about whether the observed gains are attributable to data contamination~\cite{stroebl2024inference, Mirzadeh2024}.
(2) Self-improvement during inference time: Some approaches incorporate verifiers that provide synthetic feedback during inference, using self-critique~\cite{xiao2024verbalized}, process reward models~\cite{zhang2024generative} or simple sparse objective reward~\cite{zheng2023judging} guide improvement.
However, \cite{stroebl2024inference} shows that imperfect verifiers increase false positive samples during scaling up reasoning at inference time. 
Despite these plausible advancements in common benchmarks, LLMs face complex reasoning and planning challenges.
Results from constraint-heavy and spatially complex planning tasks, for example, Termes (see Figure~\ref{o1-plan}) demonstrate that LLMs continue to struggle with planning tasks requiring intricate multi-step logical reasoning, simultaneous management of multiple constraints, and manipulation of spatial relationships~\cite{valmeekam2024planbench, wang2024planning,qiu2025sgpbench}. 

These challenges often lead to inconsistent or suboptimal outcomes or worse, hallucinations (see Figure~\ref{o1-plan}).
Although LLMs can approximate state transitions~\cite{hao2023reasoning}, they do so by probabilistically predicting subsequent tokens based on patterns learned from vast datasets, rather than through logical deduction or structured inference~\cite{kambhampati2024can}.
Inspired by traditional model-based reinforcement learning approaches \cite{agostinellilearning}, which solve decision-making problems by predicting discrete world models for heuristic search, our method significantly enhances LLMs' planning capabilities through a two-stage process. 
First, we predict a symbolic Planning Domain Definition Language (PDDL)-based world model using an instance verbalized machine learning approach. 
This stage transforms the problem into a structured, symbolic representation, enabling more logical and systematic reasoning. Second, we leverage heuristic search methods, such as A$\ast$, to efficiently find optimal solutions within this structured framework.

\begin{table}[t]
\centering
\setlength{\abovecaptionskip}{6pt}
\setlength{\belowcaptionskip}{-5pt}
\setlength{\tabcolsep}{3pt}
\renewcommand{\arraystretch}{1.4}
\small
\begin{tabular}{c|c|c|c}\textbf{Methods} & \textbf{Synthesis Objective} & \textbf{Benchmarking} & \textbf{Technical Method} \\
\shline
\cite{guan2023leveraging} & 
\begin{tabular}{c}
Each action separately \\[-0.85mm]
within PDDL domains
\end{tabular} & 
\begin{tabular}{c}
3 domains:\\[-0.85mm]
Household, Logistics, Tyreworld
\end{tabular} & 
Human experts \\[5mm]
\cite{zuo2024planetarium}& 
\begin{tabular}{c} Whole PDDL problems \\[-0.85mm]
(initial state and goal)
\end{tabular} & 
\begin{tabular}{c}
2 domains:\\[-0.85mm]
Gripper, Blockworld
\end{tabular} & 
Finetuned on a large dataset %
\\[5mm]\rowcolor{Gray}
Ours & 
\begin{tabular}{c}
Whole PDDL domains
\end{tabular} & 
\begin{tabular}{c}
283 IPC domains (NL2Domain)\\[-0.85mm]
332 IPC domains (Prob2Domain)
\end{tabular} & 
\begin{tabular}{c}
Test-time scaling without\\[-0.85mm]
model training \& human experts
\end{tabular}\\[3mm]
\end{tabular}
\caption{\small Comparison between current PDDL synthesis methods and ours.}
\label{tab:pddl_synthesis_methods}
\end{table}

\subsection{World Model Generation}
However, automatically generating scalable PDDL-based world models by LLMs is still challenging.
Current LLMs rely heavily on either human-in-the-loop or extensive training data to plausibly be superior in generating PDDL on limited scenarios.
For example, \cite{guan2023leveraging} leverages multiple human experts to refine the logical error in individual PDDL action expressions generated by LLMs.
\cite{zuo2024planetarium} collect more than 132,027 SFT data to train LLM on limited simple planning scenarios, (\eg, BlockWorld and Grippers). 
In contrast, our approach focuses on automation and scalability across diverse planning scenarios without additional training (no training data is needed). 
We compare our method and current works in Table~\ref{tab:pddl_synthesis_methods}. 

Several methods have also explored using LLMs to generate world model representations other than PDDL.
For example, GIF-MCTS~\cite{dainese2024generating} and World-coder~\cite{tang2024worldcoder} translate natural language descriptions of world models into Gym-like Python code~\cite{1606.01540}, and using pre-collected trajectories to validate and providing feedback for wrong state-transition predictions iteratively. 
Our work, however, focuses on more general planning scenarios without pre-collected validation datasets to provide critique feedback and ensuring the correctness of world modeling.

\subsection{Adaptation of LLMs}
In the absence of pre-collected validation datasets, the adaptation of LLMs for the PDDL-based world model presents unique challenges.
Parameter-efficient finetuning methods (\eg, \cite{hu2022lora,qiu2023controlling,liu2024boft,ding2023parameter}) enable effective adaptation of LLMs to downstream tasks, they still require high-quality training data. 
If there lacks a sufficient amount of training data, in-context learning~\cite{brown2020language,wei2022emergent,dong2022survey} offers an alternative adaptation approach. 
Prompt optimization techniques~\cite{zhou2022large,pryzant2023automatic,yang2024large} further enhance adaptation performance by deriving improved instruction prompts from limited training data. 
Recently, \cite{xiao2024verbalized} proposes an iterative framework to update model-characterizing text prompts with LLMs. 
\cite{pryzant2023automatic,yuksekgonul2024textgrad} introduce the concept of textual gradients as criteria for updating text prompts. 
Although finetuning methods can achieve effective adaptation, they risk causing catastrophic forgetting in pretrained LLMs, potentially compromising their general instruction-following capabilities. 
Therefore, test-time adaptation of LLMs to downstream tasks has emerged as a practical solution.
In our paper, we introduce a simple yet effective test-time adaptation method for PDDL-based world model generation that scales efficiently with test-time computing and requires no model finetuning.
