


\section{Introduction}

Enabling large language models (LLMs) to plan in complex scenarios like Barman, Floortile, and Termes remains an open problem. 
While recent LLMs like OpenAI-o1 excel at complex reasoning tasks, including coding and mathematics, they still struggle with deductive reasoning and principled planning that requires the consideration of optimality, constraints, and complex state transitions.
This limitation persists in o1 even using self-critique techniques and multiple answer re-sampling strategies.
A natural solution is translating the world abstraction from natural language into Planning Domain Definition Language (PDDL), which utilizes first-order logic (FOL) to explicitly describe states and relationships. 
Compared to natural language, the formal nature of PDDL simplifies verification and enables the precise specification of constraints and objectives, facilitating the seamless integration of off-the-shelf planning algorithms.,
However, it remains a huge challenge to translate natural language descriptions into PDDL domains with satisfactory accuracy.
Current LLMs perform poorly in this translation task due to two key challenges: the scarcity of high-quality PDDL training data and the complexity of maintaining logical consistency across predicates and actions.
Traditionally, the translation process has heavily relied on human expertise and manual refinement, making it difficult to automate and scale~\cite{guan2023leveraging}.

\begin{figure}[t]
    \centering
    \includegraphics[width=1\linewidth]{content/imgs/vml_pipeline_v7.pdf}
    \vspace{-5mm}
    \caption{\small An overview of the proposed method. Our test-time compute scaling approach consists of two main steps:
    (1) Best-of-N Sampling for PDDL Initialization (see Section~\ref{text:bon}): We start by running a parallel sampling process to generate multiple chain-of-thought responses that are composed of the formalized PDDL-based world model representation $\mathbf{D}_i$ and the natural language thought $\mathbf{T}_i$. 
    (2) Closed-loop Iteration with iVML (see Section~\ref{text:ivml}): We use Instance Verbalized Machine Learning (iVML) to iteratively improve the solutions. 
    The iVML incorporates: (1) An optimizer LLM $f_\mathrm{opt}$ that evaluates the solutions from the previous iteration, and (2) A learner LLM $f_\mathrm{learner}$ that learns from the feedback and updates the PDDL-based world model $\mathbf{D}_i$.
    The most optimal PDDL-based world model would be sent to the systematic search engine for planning.
    }
    \label{fig:pipeline}
\end{figure}


To address these challenges, our work leverages LLMs to generate PDDL-based symbolic world models for task planning without requiring model finetuning. We achieve this through a simple yet effective strategy to scale the test-time computation of LLMs. Specifically, we start by generating multiple PDDL domains from the input text query using Best-of-N (BoN) sampling. This step aims to find a good initial solution that is error-free and logically correct. Then we refine the best initial solution by optimizing the text with instance verbalized machine learning (iVML) such that the generated PDDL domain can gradually fit the input query (\eg, natural language descriptions, PDDL problems).

Our method is guided by the insight that effective test-time scaling can elicit stronger reasoning capabilities over formal languages like PDDL, thereby compensating for the scarcity of high-quality PDDL training data. We start by applying BoN sampling to explore the solution space and select the best initial solution, and then iVML aims to exploit the solution space around this initialization such that the solution gets gradually improved. VML~\cite{xiao2024verbalized} is a test-time training approach designed to iteratively refine a learner LLM's text prompt based on feedback from an optimizer LLM, which takes into account the training data and learning objectives. The learner LLM's text prompt characterizes data patterns to perform inductive inference tasks such as regression and classification. In our paper, we apply VML to PDDL domain refinement and propose the instance VML (iVML) framework that reformulates VML for instance learning. Unlike the original tasks in \cite{xiao2024verbalized}, PDDL domain refinement is an instance learning task without training data to produce verbalized gradients. Therefore, rather than requiring training data, we use LLMs to verify the validity of PDDL domains and generate critiques that serve as verbalized gradients. Specifically, the learner LLM receives an initial PDDL domain and generates a refined version based on critiques from an optimizer LLM, iteratively eliminating logical inconsistencies and grammatical errors.
However, the performance of iVML is highly dependent on the quality of initial solutions, as poor initialization can result in slow convergence or suboptimal solutions.
While BoN sampling emphasizes exploration by independently generating diverse solutions, it does not leverage past predictions from LLMs, hence limiting its ability to exploit the solution space.
iVML, in contrast, emphasizes exploitation by iteratively refining solutions based on the optimizer LLM's feedback. To achieve a good balance between exploration and exploitation, we propose to combine these complementary strategies for generating high-quality PDDL domains. This hybrid approach leverages the strengths of both methods by applying iVML to refine solutions initially generated through BoN sampling. Our contributions are listed below:


\vspace{2mm}
\begin{itemize}[leftmargin=*,nosep]
\setlength\itemsep{0.4em}
    \item \textbf{Scalable PDDL domain generation}: We propose an effective test-time scaling approach for automatic and scalable PDDL domain generation without additional model training.
    Using Qwen2.5-Coder-7B as the base model, our approach achieves state-of-the-art performance with an 85.2\% success rate on the NL2Domain task and 71.4\% on Prob2Domain, substantially surpassing o1's performance (41.7\% and 33.7\% respectively).
    \item \textbf{Application of VML to instance learning}. We introduce the iVML framework to adapt VML to instance learning, where there is no training data available. We use LLMs to check the validity of PDDL domains and generate textual critiques as gradients to iteratively update PDDL domains.
    \item \textbf{Efficient test-time compute scaling}: We enhance VML with BoN sampling initialization, effectively balancing exploration and exploitation to achieve faster convergence and obtain better solutions.
    \item \textbf{Robust planning through PDDL abstraction}: We demonstrate that PDDL-based formal abstraction enables more robust planning compared to direct LLM-based approaches. Our method successfully handles complex domains such as Barman and Termes, where existing LLM-based planners fail.
\end{itemize}









