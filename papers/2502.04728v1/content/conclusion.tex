\section{Concluding Remarks and Current Limitations}
\vspace{1mm}


Our work introduces a test-time scaling framework for automated PDDL synthesis that integrates best-of-N sampling with instance verbalized machine learning. This approach demonstrates that effectively scaling test-time computation of open-source LLMs can outperform state-of-the-art closed-source LLM planners, including OpenAI's o1-mini.Our hybrid method employs a two-phase optimization paradigm:
(1) BoN initialization which generates diverse candidate solutions to explore critical regions of the search space, addressing the cold-start problem in formal language synthesis.
(2) iVML refinement which iteratively improves the BoN initial solutions through self-critique and natural language feedback, resolving logical inconsistencies and syntactic errors.
Leveraging BoN's stochastic search to initialize iVML's refinement process, our method achieves faster convergence and higher-quality PDDL domains.
The effectiveness of iVML in PDDL problem synthesis even surpasses models specifically fine-tuned for this task. 
The results show that our proposed test-time compute scaling approach can enhance LLMs' formal reasoning and planning capabilities.
By generating PDDL-based symbolic world models, we enable explicit model-based planning with classical search algorithms (e.g., A*), avoiding the error-prone state transitions inherent in direct LLM-as-planner approaches. 
Beyond PDDL synthesis, our work provides a general framework for scaling up test-time compute of LLMs for formal language synthesis.


The limitations of our work include:
(1) \emph{challenges in semantic verification for autoformalization}:
Consistent with prior work in PDDL synthesis (\eg, \cite{guan2023leveraging, zuo2024planetarium, valmeekam2024planbench}), our evaluation relies on VAL~\cite{howey2003val} for syntax validation and plan verification. While VAL ensures syntactic correctness (\eg, predicate arity, type consistency) and plan executability (\eg, action preconditions and effects), it cannot detect semantic inconsistencies that violate domain intent or commonsense logic. This limitation parallels broader challenges in autoformalization, where even formal mathematical proof~\cite{zheng2021minif2f} struggles to verify semantic alignment between informal specifications and formal outputs through compiler checking.
(2) \emph{simulation assumptions}:
Our evaluation relies on an idealized simulation environment with two key assumptions. First, actions execute perfectly, with no execution misalignment. Second, the state space is fully observable, with no sensor noise or occlusions. These idealized conditions differ significantly from real-world robotic manipulation scenarios.













