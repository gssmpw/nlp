\section{Related Work}
\vspace{-3mm}

\subsection{Long-Tailed Object Detection}
\label{sec2.1}
\vspace{-3mm}

In the research of long-tailed object recognition, the main approaches include data re-sampling, specialized loss function design, architectural improvements, decoupled training, and data augmentation.
Data re-sampling is a common method to address imbalanced datasets by increasing the sampling frequency of tail class samples to balance the data distribution. Common re-sampling strategies include Class-aware sampling \cite{Class_aware_sampling} and Repeat factor sampling (RFS) \cite{lvis}. These methods can be employed at different stages of training to achieve a multi-stage training process. Specialized loss function design is another technical approach to tackling long-tailed challenges. For instance, EQL \cite{eql} reduces suppression on tail classes by truncating the negative gradients from head classes. The subsequent EQLv2 \cite{eqlv2} further improves this approach through a gradient balancing mechanism. Other methods, such as Seesaw Loss \cite{seesaw}, Equalized Focal Loss \cite{efl}, ACSL \cite{acsl}, and LOCE \cite{loce}, reduce excessive suppression of tail classes by dynamically adjusting classification logits or suppressing overconfident scores. C2AM \cite{c2am} observed that the severe imbalance in weight norms across classes leads to pathological decision boundaries, and therefore proposes learning fairer decision boundaries by adjusting the ratio of weight norms. 

Current research mainly focuses on these two directions. In addition, module improvement emphasizes modifying the structure of detectors to address long-tailed distribution issues. For example, BAGS \cite{bags} and Forest R-CNN \cite{forest} mitigate the impact of head classes on tail classes by grouping all classes based on valuable prior knowledge. Decoupled training \cite{decoupling} has found that long-tailed distributions do not significantly affect the learning of high-quality features, thus some methods freeze the feature extractor parameters during the classifier learning phase, adjusting only the classifier \cite{fdc,number4,zhang2021distribution}. Data augmentation, as a means of introducing additional sample variability, has been shown to provide further improvements in long-tailed detection tasks. Recently proposed methods such as Simple Copy-Paste \cite{copy_paste}, FDC \cite{fdc}, FASA \cite{fasa}, and FUR \cite{fur} supplement the insufficiency of tail-class samples by performing data augmentation in both image and feature spaces.
RichSem \cite{RichSem} and Step-wise Learning \cite{dong} introduce Transformer-based object detection architectures, with the former relying on external data and adding new network branches, while the latter incorporates multiple modules and multi-stage training. The core advantage of our proposed IGAM lies in its simplicity and efficiency.

\vspace{-3mm}
\subsection{Methods for Measuring Class Difficulty}
\label{sec2.2}
\vspace{-3mm}

\textbf{The study of class difficulty is most relevant to our work.} Most research addressing class bias has focused on scenarios with sample imbalance, where rebalancing strategies based on sample size can be somewhat effective. However, recent studies have reported that even when sample sizes are perfectly balanced, classification models still exhibit significant performance disparities across different classes. Investigating the root causes of model bias in scenarios where sample sizes are balanced is crucial for improving model fairness and understanding learning mechanisms. However, research on this issue is still limited. From a geometric perspective, DSB \cite{ma2023delving}, CR \cite{ma2023curvature}, and IDR \cite{ma2024unveiling} conceptualize the data classification process as the disentangling and separating of different perceptual manifolds. These three studies respectively reveal that the geometric properties of perceptual manifolds—volume, curvature, and intrinsic dimensionality—are significantly correlated with class performance. \cite{icml2024balanced} discovered that differences in the spectral features of classes could be a source of class bias. Unfortunately, in the field of object detection, there has been no research exploring the underlying causes of model bias.
Our work is the first to directly report on the widespread bias present in object detection models and to attempt to explore the potential mechanisms underlying this bias.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%  第三章 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{-2mm}