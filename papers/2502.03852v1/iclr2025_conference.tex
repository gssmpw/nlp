
\documentclass{article} % For LaTeX2e
\usepackage{iclr2025_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}
\usepackage{times}
\usepackage{array}

\usepackage{colortbl}  % 或者 colortbl 包

\usepackage{amsmath,amsfonts}
\usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
\usepackage{textcomp}
\usepackage{stfloats}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}
%\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{multirow}
%\usepackage{color,xcolor}
\usepackage{bbding}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{enumitem}
\usepackage{multirow}
%\usepackage[table]{xcolor}
\usepackage{setspace} % 需要导入 setspace 宏包


%\usepackage{marvosym}
\usepackage[misc]{ifsym} %\Letter
%
% These are are recommended to typeset listings but not required. See the subsubsection on listing. Remove this block if you don't have listings in your paper.
\usepackage{newfloat}


\usepackage{listings}
%\iffalse
\DeclareCaptionStyle{ruled}{labelfont=normalfont,labelsep=colon,strut=off} % DO NOT CHANGE THIS
\lstset{
    language=Python,            % 语言为Python
    backgroundcolor=\color{pink!10}, % 设置背景色
    basicstyle=\ttfamily\footnotesize, % 设置字体和大小
    keywordstyle=\color{blue}\bfseries, % 关键字样式
    commentstyle=\color{green!50!black}\itshape, % 注释样式
    stringstyle=\color{red!70!black}, % 字符串样式
    %frame=single,               % 单线框
    rulecolor=\color{gray},      % 边框颜色
    showstringspaces=false,      % 去掉字符串中的空格标记
    breaklines=true,             % 自动换行
    tabsize=4,                   % Tab 键宽度
    numbers=none                 % 去掉行号
}

\usepackage[colorlinks,
            linkcolor=red,       %%修改此处为你想要的颜色
            anchorcolor=blue,  %%修改此处为你想要的颜色
            citecolor=blue,        %%修改此处为你想要的颜色，例如修改blue为red
            ]{hyperref}

% Some very useful LaTeX packages include:
% (uncomment the ones you want to load)
\usepackage{orcidlink}








\usepackage{hyperref}
\usepackage{url}
\usepackage{wrapfig}
\usepackage{graphicx}
\usepackage{graphicx} 
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{enumitem}
\usepackage{amsmath}
\usepackage{amsthm}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{lemma}{Lemma}
\newtheorem{definition}{Definition}






\title{Pursuing Better Decision Boundaries for Long-Tailed Object Detection via Category Information Amount}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{Yanbiao Ma, Wei Dai \& Jiayi Chen  \\
Xidian University, Xi’an, 710071, China\\
\texttt{ybmamail@stu.xidian.edu.cn} \\
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

%\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\iclrfinalcopy
\begin{document}


\maketitle

\vspace{-5mm}
\begin{abstract}
In object detection, the instance count is typically used to define whether a dataset exhibits a long-tail distribution, implicitly assuming that models will underperform on categories with fewer instances. This assumption has led to extensive research on category bias in datasets with imbalanced instance counts. However, models still exhibit category bias even in datasets where instance counts are relatively balanced, clearly indicating that instance count alone cannot explain this phenomenon. In this work, we first introduce the concept and measurement of category information amount. We observe a significant negative correlation between category information amount and accuracy, suggesting that category information amount more accurately reflects the learning difficulty of a category. Based on this observation, we propose Information Amount-Guided Angular Margin (IGAM) Loss. The core idea of IGAM is to dynamically adjust the decision space of each category based on its information amount, thereby reducing category bias in long-tail datasets. IGAM Loss not only performs well on long-tailed benchmark datasets such as LVIS v1.0 and COCO-LT but also shows significant improvement for underrepresented categories in the non-long-tailed dataset Pascal VOC. Comprehensive experiments demonstrate the potential of category information amount as a tool and the generality of our proposed method.
\end{abstract}



\iffalse
\begin{figure}[h]
\centering
	\begin{minipage}{0.48\linewidth}
		\centering
		\includegraphics[width=1\linewidth]{fig1}
		%\caption{chutian1}
		%\label{chutian1}%文中引用该图片代号
	\end{minipage}
	\begin{minipage}{0.48\linewidth}
		\centering
		\includegraphics[width=1\linewidth]{fig2}
		%\caption{chutian2}
		%\label{chutian2}%文中引用该图片代号
	\end{minipage}
\caption{The left vertical axis represents the number of instances per class. The right vertical axis shows the performance of Faster R-CNN with an R-50-FPN backbone across all classes, trained on the Pascal VOC dataset. The model was trained using the settings described in Section \ref{sec4.2}. The red text box displays the Pearson correlation coefficient between class performance and the number of instances.}
\label{fig1}
%\vskip -0.2in
\end{figure}
\fi

%\iffalse
\begin{wrapfigure}[23]{r}{16.5em} % 纵向8行，图片靠右，宽度12.5em
\begin{center}
\vskip -0.3in
\includegraphics[width=1.03\linewidth]{fig1.pdf}
%\caption{chutian1}
%\label{chutian1}%文中引用该图片代号
\vskip -0.15in
\caption{The left vertical axis represents the number of instances per class. The right vertical axis represents the performance of Faster R-CNN trained with cross-entropy loss using R-50-FPN as the backbone across all classes, trained on the Pascal VOC. The model was trained using the settings described in Section \ref{sec4.2}. The red text box displays the Pearson correlation coefficient between class performance and the number of instances.}
\label{fig1}
\end{center}
\end{wrapfigure}
%\fi

%\vspace{-6mm}
\section{Introduction}

In object detection tasks, long-tailed distribution is a common phenomenon, where most instances are concentrated in a few categories, while other categories have relatively few instances \cite{jiao2019survey,ijcvsurvey,zou2023object,TPAMIimbalance}. A widely accepted perspective is that the imbalance in the number of instances causes the model to be more biased towards frequent categories during training, ignoring the less frequent ones, leading to significant category bias during testing \cite{ecm,number1,number2,number3,number4}. However, recent research in image classification suggests that category bias is not only caused by the imbalance in sample numbers but may also be closely related to the complexity of intra-category features \cite{ma2023delving,ma2023curvature,icml2024balanced}. This is evidenced in datasets with perfectly balanced samples, where models still exhibit bias. Out of curiosity, we examined the correlation between category average precision (AP) and the number of instances on Pascal VOC, a target detection dataset with a relatively balanced number of instances (see Figure \ref{fig1}), and found that the correlation between the two was very low. This indicates that in object detection, model bias may also originate from the complexity of intra-category features.

\begin{wraptable}[12]{r}{0.36\textwidth} % 表格放在右侧，宽度为页面宽度的60%
\vskip -0.15in
\caption{Pearson correlation coefficient between category information amount and class average precision on long-tailed datasets. The model is Faster R-CNN with R-50-FPN backbone.}
\label{tab0}
\vskip 0.1in
\centering  
\begin{small}
\renewcommand\arraystretch{1}
\setlength{\tabcolsep}{5.4pt} % 修改边距
\begin{tabular}{l|lll}
\hline \toprule
         Dataset                & \multicolumn{3}{c}{LVIS v1.0}                                   \\ \hline
\multicolumn{1}{c|}{}   & \multicolumn{1}{l|}{CE}    & \multicolumn{1}{l|}{SeeSaw} & Focal \\ \hline
\multicolumn{1}{c|}{IA} & \multicolumn{1}{l|}{-0.68} & \multicolumn{1}{l|}{-0.66}  & -0.70 \\ \hline
       Dataset                  & \multicolumn{3}{c}{COCO-LT}                                     \\ \hline
\multicolumn{1}{c|}{IA}    & \multicolumn{1}{l|}{-0.66} & \multicolumn{1}{l|}{-0.65}  & -0.69 \\ \bottomrule \hline
\end{tabular}
\end{small}
%\vskip -0.1in
\end{wraptable}

Traditional long-tailed object detection methods mainly alleviate this issue by re-weighting low-frequency categories \cite{Class_aware_sampling,lvis,number1,number3}, adjusting gradients \cite{focal,seesaw,efl,eql,eqlv2}, and employing data augmentation techniques \cite{copy_paste,fdc,fasa,fur}. However, these approaches primarily focus on the impact of the number of instances while ignoring the complexity within categories. As a result, the model may fail to focus on some disadvantaged categories, limiting its overall performance. To better reveal and mitigate category bias, we introduce the concept of category information amount and its corresponding measurement. Category information amount quantifies the diversity of instances within a category, and a straightforward hypothesis is that the greater the information amount of a category, the more difficult it is for the model to learn and memorize that category. We calculated the correlation between category information amount and AP on the relatively balanced Pascal VOC dataset and the long-tailed dataset (LVIS v1.0 and COCO-LT), as shown in Table \ref{tab0} and Figure \ref{fig2}. Category information amount, compared to the number of instances, better reflects the difficulty of a category. Therefore, we aim to leverage category information amount to direct the model's attention to truly challenging categories rather than simply focusing on categories with fewer instances.

\begin{wrapfigure}[14]{r}{17em} % 纵向8行，图片靠右，宽度12.5em
\begin{center}
\vskip -0.5in
\includegraphics[width=1\linewidth]{fig2}
%\caption{chutian1}
%\label{chutian1}%文中引用该图片代号
\vskip -0.15in
\caption{Pearson correlation coefficients between category information amount and category average precision and between category instance count and category average precision, under two backbone networks and three loss function settings.}
\label{fig2}
\end{center}
\end{wrapfigure}


Consider the following scenario: if a head category has very little category information amount, the model can more easily learn and abstract the patterns of that category \cite{number3}. From an information compression perspective, the decision space required for that category need not be very large. However, recent studies \cite{c2am,qi2023balanced} have shown that the severe imbalance in data volume leads to pathological decision boundaries, where the decision space for tail categories is significantly compressed, while head categories have disproportionately large decision spaces. This unreasonable allocation of decision space represents a waste of model capacity. Can we directly equalize decision spaces?

Differences in category information amount are inherent \cite{ma2023delving,xuelong}. For a recognition task, forcibly equalizing decision spaces means that for categories with higher information amounts, the model needs to learn stronger invariant representations to compress such categories into a decision space of the same size as those with lower information amounts. However, the model does not receive additional constraints or support to improve its learning for categories with higher information amounts. Therefore, we propose dynamically adjusting the proportion of decision space allocated to each category based on its information amount. Specifically, we design a novel loss function—Information Amount-Guided Angular Margin (IGAM) Loss, which aims to reduce model bias by dynamically adjusting decision spaces according to the information amount of each category. To enable dynamic updates to the information amount, we also designed a low-cost, end-to-end training strategy. Comprehensive experiments on long-tailed benchmark datasets LVIS v1.0, COCO-LT, and Pascal VOC demonstrate that our method surpasses most existing approaches in both overall performance and reducing model bias.

The main contributions of this paper are as follows: \textbf{(1)} We propose the concept of category Information amount and define its measurement (Section \ref{sec3.1}). A surprising finding is that in object detection tasks, category informativeness, compared to the number of instances, better reflects the difficulty of learning each category. This provides a useful tool for future research on improving performance on challenging categories. \textbf{(2)} We introduce the Information Amount-Guided Angular Margin (IGAM) Loss (Section \ref{sec3.2}), which adjusts the decision space of categories based on their information amount, encouraging the model to focus more on challenging categories. To dynamically update the information amount of categories, we propose an end-to-end training framework for applying IGAM at a low cost (Section \ref{sec3.3}). \textbf{(3)} On long-tail benchmark datasets LVIS v1.0 and COCO-LT, our method achieves the best performance in most cases, particularly in improving the model's accuracy on rare categories (Sections \ref{sec4.4} and \ref{sec4.5}). On the relatively balanced Pascal VOC dataset, our method significantly outperforms other approaches on challenging categories.

\iffalse
\vspace{-6mm}
\begin{itemize}[]
\item[] We propose the concept of category Information amount and define its measurement (Section \ref{sec3.1}). A surprising finding is that in object detection tasks, category informativeness, compared to the number of instances, better reflects the difficulty of learning each category. This provides a useful tool for future research on improving performance on challenging categories.

\item[] We introduce the Information Amount-Guided Angular Margin (IGAM) Loss (Section \ref{sec3.2}), which adjusts the decision space of categories based on their information amount, encouraging the model to focus more on challenging categories. To dynamically update the information amount of categories, we propose an end-to-end training framework for applying IGAM at a low cost (Section \ref{sec3.3}).

\item[] On long-tail benchmark datasets LVIS v1.0 and COCO-LT, our method achieves the best performance in most cases, particularly in improving the model's accuracy on rare categories (Section \ref{sec4.4}). On the relatively balanced Pascal VOC dataset, our method significantly outperforms other approaches on challenging categories.
\end{itemize}
\fi



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%  第二章 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\iffalse
\vspace{-3mm}
\section{Related Work}
\vspace{-3mm}

\subsection{Long-Tailed Object Detection}
\label{sec2.1}
\vspace{-3mm}

In the research of long-tailed object recognition, the main approaches include data re-sampling, specialized loss function design, architectural improvements, decoupled training, and data augmentation.
Data re-sampling is a common method to address imbalanced datasets by increasing the sampling frequency of tail class samples to balance the data distribution. Common re-sampling strategies include Class-aware sampling \cite{Class_aware_sampling} and Repeat factor sampling (RFS) \cite{lvis}. These methods can be employed at different stages of training to achieve a multi-stage training process. Specialized loss function design is another technical approach to tackling long-tailed challenges. For instance, EQL \cite{eql} reduces suppression on tail classes by truncating the negative gradients from head classes. The subsequent EQLv2 \cite{eqlv2} further improves this approach through a gradient balancing mechanism. Other methods, such as Seesaw Loss \cite{seesaw}, Equalized Focal Loss \cite{efl}, ACSL \cite{acsl}, and LOCE \cite{loce}, reduce excessive suppression of tail classes by dynamically adjusting classification logits or suppressing overconfident scores. C2AM \cite{c2am} observed that the severe imbalance in weight norms across classes leads to pathological decision boundaries, and therefore proposes learning fairer decision boundaries by adjusting the ratio of weight norms. 

Current research mainly focuses on these two directions. In addition, module improvement emphasizes modifying the structure of detectors to address long-tailed distribution issues. For example, BAGS \cite{bags} and Forest R-CNN \cite{forest} mitigate the impact of head classes on tail classes by grouping all classes based on valuable prior knowledge. Decoupled training \cite{decoupling} has found that long-tailed distributions do not significantly affect the learning of high-quality features, thus some methods freeze the feature extractor parameters during the classifier learning phase, adjusting only the classifier \cite{fdc,number4,zhang2021distribution}. Data augmentation, as a means of introducing additional sample variability, has been shown to provide further improvements in long-tailed detection tasks. Recently proposed methods such as Simple Copy-Paste \cite{copy_paste}, FDC \cite{fdc}, FASA \cite{fasa}, and FUR \cite{fur} supplement the insufficiency of tail-class samples by performing data augmentation in both image and feature spaces.
RichSem \cite{RichSem} and Step-wise Learning \cite{dong} introduce Transformer-based object detection architectures, with the former relying on external data and adding new network branches, while the latter incorporates multiple modules and multi-stage training. The core advantage of our proposed IGAM lies in its simplicity and efficiency.

\vspace{-3mm}
\subsection{Methods for Measuring Class Difficulty}
\label{sec2.2}
\vspace{-3mm}

\textbf{The study of class difficulty is most relevant to our work.} Most research addressing class bias has focused on scenarios with sample imbalance, where rebalancing strategies based on sample size can be somewhat effective. However, recent studies have reported that even when sample sizes are perfectly balanced, classification models still exhibit significant performance disparities across different classes. Investigating the root causes of model bias in scenarios where sample sizes are balanced is crucial for improving model fairness and understanding learning mechanisms. However, research on this issue is still limited. From a geometric perspective, DSB \cite{ma2023delving}, CR \cite{ma2023curvature}, and IDR \cite{ma2024unveiling} conceptualize the data classification process as the disentangling and separating of different perceptual manifolds. These three studies respectively reveal that the geometric properties of perceptual manifolds—volume, curvature, and intrinsic dimensionality—are significantly correlated with class performance. \cite{icml2024balanced} discovered that differences in the spectral features of classes could be a source of class bias. Unfortunately, in the field of object detection, there has been no research exploring the underlying causes of model bias.
Our work is the first to directly report on the widespread bias present in object detection models and to attempt to explore the potential mechanisms underlying this bias.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%  第三章 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{-2mm}
\section{Pursuing Better Decision Boundaries with the Help of Category Information Amount}
\label{sec3}
\vspace{-2mm}

In this section, we first define and compute the category information amount (Section \ref{sec3.1}), then gradually derive how to dynamically adjust the decision space of categories based on their information amount (Section \ref{sec3.2}). Finally, we propose a low-cost, end-to-end training strategy to enable the dynamic update of the category information amount (Section \ref{sec3.3}).

\subsection{Definition and Measurement of Category Information Amount}
\label{sec3.1}
\vspace{-2mm}

Recent studies have shown that the response of deep neural networks to images is similar to human vision, following the manifold distribution hypothesis, where the embeddings of images lie near a low-dimensional perceptual manifold embedded in high-dimensional space \cite{nc,pnas}. Continuous sampling along a dimension of this manifold corresponds to continuous changes in physical features. Therefore, the volume of the perceptual manifold mapped by a deep neural network can effectively measure the information amount of a category. Based on this theory, we define the information amount \( I_i \) of category \( i \) as the volume of its perceptual manifold: $I_i = Vol(X_i)$, where \( X_i = [x_1, x_2, \dots, x_m] \) represents the set of embeddings for instances in category \( i \), $m$ denotes the number of instances. \( \text{Vol}(X_i) \) measures the volume of the perceptual manifold, reflecting the information amount of the category. It is important to note that the embeddings used to calculate the information amount should be extracted from the classification module of the object detection model, not the regression module. Below is the method for calculating \( \text{Vol}(X_i) \).

\begin{wrapfigure}[17]{r}{18.5em} % 纵向8行，图片靠右，宽度12.5em
\begin{center}
\vskip -0.3in
\begin{minipage}{0.49\textwidth} % 定义代码块的宽度
    \begin{lstlisting}[language=Python, basicstyle=\scriptsize\ttfamily, caption=Category Information Amount, label={lst:listing}]
import numpy as np
# Function to compute Information Amount.
def compute_instance_diversity(embeddings):
    m, p = embeddings.shape
    mean_embedding=np.mean(embeddings,axis=0)
    centered_embeddings = embeddings - mean_embedding
    sample_cov = np.dot(centered_embeddings.T, centered_embeddings) / m
    eigvals, eigvecs = np.linalg.eigh(sample_cov)
    c = p / m
    lambda_minus = (1 - np.sqrt(c))**2
    lambda_plus = (1 + np.sqrt(c))**2
    d = np.maximum(eigvals, lambda_minus)
    shrunk_cov = np.dot(eigvecs, np.dot(np.diag(d), eigvecs.T))
    Information = 0.5 * np.log2(np.linalg.det(np.eye(p) + shrunk_cov))
    return Information
    \end{lstlisting}
    \end{minipage}
\end{center}
\end{wrapfigure}

Given the embedding set \( X_i = [x_1, x_2, \dots, x_m] \in \mathbb{R}^{p \times m} \), where each instance embedding \( x_j \in \mathbb{R}^p \), $p$ denotes the dimension of the embedding. We first compute the covariance matrix of the embedding set \( X_i \): $\Sigma(X_i) = \frac{1}{m} \sum_{j=1}^m (x_j - \bar{x})(x_j - \bar{x})^T$, where \( \bar{x} \) is the mean vector of the embedding set \( X_i \): $\bar{x} = \frac{1}{m} \sum_{j=1}^m x_j$.

The covariance matrix \( \Sigma(X_i) \) captures the distribution characteristics of the category \( i \) in the high-dimensional embedding space, and its determinant can be used to calculate the volume of the perceptual manifold. To enhance the accuracy of the covariance matrix estimation, we employ the Ledoit-Péché nonlinear shrinkage method \cite{shrinkage_method}, which improves robustness in high-dimensional spaces through eigenvalue transformation. The covariance matrix \( \Sigma(X_i) \) is reconstructed as follows:
%\begin{small}
\begin{equation}
\setlength\abovedisplayskip{2pt} 
\setlength\belowdisplayskip{2pt}
\begin{split}
\Sigma(X_i) = V \text{diag}(\lambda_1, \lambda_2, \dots, \lambda_p) V^T,
%\nonumber
\end{split}
\end{equation}
%\end{small}
where \( V \) is the matrix of eigenvectors, and \( \lambda_i = \max(\lambda_i, \lambda_-) \), with \( \lambda_- = (1 - \sqrt{p/m})^2 \) as the nonlinearly transformed minimum eigenvalue. Finally, the information amount of category \( i \) is formally defined as: 
%\begin{small}
\begin{equation}
\setlength\abovedisplayskip{2pt} 
\setlength\belowdisplayskip{2pt}
\begin{split}
I_i = \text{Vol}(X_i) = \frac{1}{2} \log_2 \det(\Sigma(X_i) + I),
%\nonumber
\end{split}
\end{equation}
%\end{small}
where \( \det(\Sigma(X_i) + I) \) is the determinant of the matrix \( \Sigma(X_i) + I \), and \( I \) is the identity matrix. The determinant reflects the spread of the category’s embeddings, representing the volume of the perceptual manifold. In this way, we can quantify the information amount of category \( i \).


\subsection{Information Amount-Guided Angular Margin (IGAM) Loss}
\label{sec3.2}

Cross-entropy loss is widely used in deep learning, especially in classification tasks. It measures the difference between the model's predicted probability distribution and the true label distribution. For the classification component in object detection tasks, given a feature vector \( x \) and a label \( i \), the cross-entropy loss is typically defined as:
%\begin{small}
\begin{equation}
\setlength\abovedisplayskip{2pt} 
\setlength\belowdisplayskip{2pt}
\begin{split}
L = -\log \left( \frac{e^{W_i^T x}}{\sum_{j=1}^C e^{W_j^T x}} \right),
%\nonumber
\end{split}
\end{equation}
%\end{small}
where \( W_j \) is the \( j \)-th column of the final fully connected layer, corresponding to the weight vector for category \( j \). In long-tailed scenarios, it has been observed that the norm of the weight vectors corresponding to each category is extremely unbalanced, making it more difficult to recognize tail categories. For example, in a binary classification task, when \( W_1^T x = W_2^T x \), we have:
%\begin{small}
\begin{equation}
\setlength\abovedisplayskip{2pt} 
\setlength\belowdisplayskip{2pt}
\begin{split}
\|W_1\|_2 \cdot \cos(\theta_1) = \|W_2\|_2 \cdot \cos(\theta_2), \quad 0 \leq \theta_1, \theta_2 \leq \frac{\pi}{2}. 
%\nonumber
\end{split}
\end{equation}
%\end{small}
If \( \|W_1\|_2 > \|W_2\|_2 \), then \( \theta_1 > \theta_2 \) must hold for the Equation (4) to be true. Clearly, when \( \|W_1\|_2 \gg \|W_2\|_2 \), the decision space for category 2 is pathologically compressed. To address this, a simple solution is to directly equalize the decision space, ignoring the norm of the weight vectors for each category, resulting in:
%\begin{small}
\begin{equation}
\setlength\abovedisplayskip{2pt} 
\setlength\belowdisplayskip{2pt}
\begin{split}
L = -\log \left( \frac{e^{s \cdot \cos(\theta_i)}}{\sum_{j=1}^C e^{s \cdot \cos(\theta_j)}} \right),
%\nonumber
\end{split}
\end{equation}
%\end{small}
where \( \cos(\theta_i) = \frac{W_i^T x}{\|W_i\|_2 \cdot \|x\|_2} \), and \( s \) is a hyperparameter introduced to stabilize training. The optimization goal can be understood as minimizing the angle between \( W_i \) and \( x \). Although this approach addresses the pathological decision space allocation, recent studies show that absolute equal allocation among all categories is not the optimal solution. A straightforward explanation is that even with the same number of samples for each category, their learning difficulties vary. Thus, absolute equality restricts the model's sensitivity and attention to different categories.

From an information compression perspective, if the information amount of a category is significantly larger than others, but its decision space is required to be compressed to the same extent, this is clearly unreasonable. Therefore, we propose using each category's information amount to dynamically adjust the decision boundaries, allowing categories with larger information amounts to have larger decision spaces. Specifically, we introduce an angular margin \( m_{ij} \) based on the information amount of each category into Equation (5). The final optimization objective is expressed as:
%\begin{small}
\begin{equation}
\setlength\abovedisplayskip{2pt} 
\setlength\belowdisplayskip{2pt}
\begin{split}
L = -\log \left( \frac{e^{s \cdot \cos(\theta_i)}}{e^{s \cdot \cos(\theta_i)} + \sum_{j=1, j \neq i}^C e^{s \cdot \cos(\theta_j + m_{ij})}} \right),
%\nonumber
\end{split}
\end{equation}
%\end{small}
where: $m_{ij} = \max \left( 0, \frac{1}{\pi} \cdot \log(\frac{I_i'}{I_j'}) \right)$, and: $I_i' = \frac{e^{e^{I_i / (\bar{I} \cdot \sqrt{C})}}}{\sum_{j=1}^C e^{I_j / (\bar{I} \cdot \sqrt{C})}} \cdot C + 1, \quad \bar{I} = \sum_{i=1}^C I_i$.
In this formula, \( I_i' \) represents the normalized information amount of category \( i \), with the normalization method adopted from \cite{ma2023delving}. The term \( m_{ij} \) is based on the ratio of the information amounts of the category \( i \) and \( j \). If the information amount of category \( i \) is larger (i.e., \( I_i' > I_j' \)), then \( m_{ij} \) is positive, meaning the decision space for that category should be expanded. Conversely, if \(I_i' < I_j'\), then \(m_{ij}\) is negative, and the decision space for class \(i\) is compressed. By incorporating information amount, IGAM Loss can more accurately reflect the internal complexity of categories, rather than solely relying on instance numbers. This allows the model to allocate more decision space to complex categories, improving detection accuracy for these categories.

In practical training, we face an engineering challenge: the information amount of categories changes as the model parameters evolve, necessitating dynamic updates. However, calculating the information amount requires the covariance matrix of all instance embeddings, and extracting embeddings for the entire dataset in each iteration would lead to excessive memory and time costs, interrupting training. We propose a novel training framework to address this issue.



\vspace{-3mm}
\subsection{Low-Cost Dynamic Update of Information Density}
\label{sec3.3}
\vspace{-3mm}

\subsubsection{Dynamic Update Strategy}
\label{sec3.3.1}
\vspace{-2mm}

The phenomenon of \textbf{feature slow drift} \cite{wang2020cross,ma2023delving} indicates that as training progresses, the distance between the embeddings of the same sample at different training stages becomes increasingly smaller, to the extent that the previous version of an embedding can approximate the latest version. Inspired by this, we propose the most straightforward approach: store the embeddings of all instances generated during training in a queue, with the queue length equal to the total number of instances in the dataset. After each training epoch, all embeddings in the queue can be updated once. At the end of each epoch, the embeddings in the queue are used to calculate and update the information amount for all categories. In the following, we refer to this approach as the original strategy. Although the original strategy avoids repeatedly extracting embeddings for the entire dataset, it increases the demand for storage space.
Considering that the essence of calculating the information amount lies in obtaining the covariance matrix of the instance embeddings, we propose a new strategy that significantly reduces storage space. The core idea of this new strategy is to calculate the global covariance matrix of the samples using multiple local sample covariance matrices. The specific steps are as follows:
\begin{itemize}[]
\item[(1)] Initialize a queue to store instance embeddings. The length of the queue can be adjusted according to the GPU memory size. Suppose the object detection dataset contains \( C \) categories with a total of \( N \) instances, and the queue length is \( d \). If \( d < N \), it means the queue cannot hold all instance embeddings. In this work, we set the queue length to 50,000, which can store 50,000 instance embeddings.

\item[(2)] At the beginning of a training epoch, first store the instance embeddings generated in each batch into the queue until it is full (i.e., storing \( d \) instance embeddings). Then, use the embeddings in the queue to calculate the local covariance matrix and mean for each category. Continuously update the queue, and once all old embeddings in the queue are updated, calculate the local covariance matrix and mean for each category again. By the end of an epoch, we can calculate \( \lfloor N/d \rfloor + 1 \) local sample covariance matrices \( \Sigma_i^k \) and means \( \mu_i^k \) for each category \( i = 1, \ldots, C \), \( k = 1, \ldots, \lfloor N/d \rfloor + 1 \).

\item[(3)] At the end of an epoch, use the stored local covariance matrices to calculate and update the information amount for each category. Taking category \( i \) as an example, first calculate the global mean:
\begin{equation}
\setlength\abovedisplayskip{2pt} 
\setlength\belowdisplayskip{2pt}
\begin{split}
\mu_i = \frac{1}{N_i} \sum_{k=1}^{\lfloor N/d \rfloor + 1} n_i^k \mu_i^k,
%\nonumber
\end{split}
\end{equation}
where \( N_i \) is the total number of instances in category \( i \), and \( n_i^k \) is the number of instances in the local sample. Then, calculate the global covariance matrix:
\begin{small}
\begin{equation}
\setlength\abovedisplayskip{2pt} 
\setlength\belowdisplayskip{2pt}
\begin{split}
\Sigma_i = \frac{1}{N_i} \left( \sum_{k=1}^{\lfloor N/d \rfloor + 1} n_i^k \Sigma_i^k + \sum_{k=1}^{\lfloor N/d \rfloor + 1} n_i^k (\mu_i^k - \mu_i)(\mu_i^k - \mu_i)^T \right). 
%\nonumber
\end{split}
\end{equation}
\end{small}
The proof of this formula is provided in Appendix \ref{app1}. By integrating local covariance matrices to obtain the global covariance matrix, we significantly reduce the additional storage space required to update the information amount. Further, the information amount of category \( i \) is estimated as \( \text{Vol}_i = \frac{1}{2} \log_2 \det(I + \Sigma_i) \).

\end{itemize}

\iffalse
\begin{algorithm}[t]
\setstretch{1} % 将行间距设置为1.5倍
\caption{Dynamic Update of Information Density}
\label{alg:dynamic_info_density}
\begin{algorithmic}[1]
\STATE \textbf{Input:} Number of classes $C$, total number of instances $N$, queue length $d$ (e.g., 50,000), number of epochs $E$
\STATE Initialize queue $Q$ with maximum length $d$
\FOR{each epoch $e = 1$ to $E$}
    \FOR{each batch of data}
        \STATE Extract instance embeddings for each class and store in queue $Q$
        \IF{Queue $Q$ is full}
            \STATE Compute local covariance matrix $\Sigma_i^k$ and mean $\mu_i^k$ for each class $i$
            \STATE Update queue $Q$ with new embeddings
        \ENDIF
    \ENDFOR
    \STATE After processing all batches, compute additional local covariance matrices and means as needed
    \FOR{each class $i = 1$ to $C$}
        \STATE Compute global mean $\mu_i$:
        \[
        \mu_i \gets \frac{1}{N_i} \sum_{k=1}^{\lfloor N/d \rfloor + 1} n_i^k \mu_i^k
        \]
        \STATE Compute global covariance matrix $\Sigma_i$:
        \begin{align*}
        \Sigma_i \gets & \frac{1}{N_i} \bigg( \sum_{k=1}^{\lfloor N/d \rfloor + 1} n_i^k \Sigma_i^k \\
        & + \sum_{k=1}^{\lfloor N/d \rfloor + 1} n_i^k (\mu_i^k - \mu_i)(\mu_i^k - \mu_i)^T \bigg)
        \end{align*}
        \STATE Estimate the information density $I_i$ using Algorithm 1.
    \ENDFOR
\ENDFOR
\STATE \textbf{Output:} Information densities $I_i$ for each class $i$
\end{algorithmic}
\end{algorithm}
\fi


\subsubsection{Storage Space Comparison}
\label{sec3.3.2}

Assume the object detection dataset contains \( N \) instances, each with an embedding dimension of \( p \), and there are \( C \) categories. The queue length is set to \( d \). The storage space required by the original strategy is: $S_{\text{original}} = N \times p$.
The storage space required by the new strategy is:
\begin{equation}
\begin{split}
S_{\text{new}} = d \times p + C \times (\lfloor N/d \rfloor + 1) \times p^2.
%\nonumber
\end{split}
\end{equation}
where \( C \times (\lfloor N/d \rfloor + 1) \times p^2 \) represents the space needed to store the local covariance matrices. To analyze when the new strategy saves more space, we define the storage space ratio \( R \):
\begin{equation}
\begin{split}
R = \frac{S_{\text{new}}}{S_{\text{original}}} = \frac{d \times p + C \times (\lfloor N/d \rfloor + 1) \times p}{N}.
%\nonumber
\end{split}
\end{equation}
\begin{wrapfigure}[12]{r}{19em} % 纵向8行，图片靠右，宽度12.5em
\begin{center}
\vskip -0.27in
	\begin{minipage}{0.49\linewidth}
		\centering
		\includegraphics[width=1\linewidth]{fig4}
		%\caption{chutian1}
		%\label{chutian1}%文中引用该图片代号
	\end{minipage}
	\begin{minipage}{0.49\linewidth}
		\centering
		\includegraphics[width=1\linewidth]{fig5}
		%\caption{chutian2}
		%\label{chutian2}%文中引用该图片代号
	\end{minipage}
\vskip -0.1in
\caption{The function of the storage space ratio $R$ as it varies with the queue length $d$ on the Pascal VOC and MS COCO datasets.}
\label{fig3}
\end{center}
\end{wrapfigure}
When \( R < 1 \), the new strategy saves more storage space. To visually compare the storage space requirements of the new and original strategies, we take the Pascal VOC and MS COCO datasets as examples and plot the function graph of the storage space ratio \( R \) as it varies with the queue length \( d \) in Figure \ref{fig3}. It can be observed that in most cases, the storage space required by the new strategy is less than that of the original strategy. By visualizing our proposed storage space ratio, it becomes easy to choose the optimal queue length \( d \), thereby saving approximately 60\% of the storage space.
Given two examples \(\{N=55800, p=128, C=20\}\) and \(\{N=605638, p=128, C=80\}\), corresponding to the Pascal VOC and MS COCO datasets respectively, we use Listing \ref{lst:listing} to select the optimal queue lengths \( d \) of 11517 and 68182. For Example 1, the original strategy requires an additional 27.25 MB of memory, while the new strategy only requires 11.87 MB, saving approximately $\textbf{56.44\%}$ of memory. For Example 2, the new strategy can reduce memory usage from 295.72 MB to 78.29 MB, saving approximately $\textbf{73.52\%}$ of memory.

\begin{wrapfigure}[9]{r}{19em} % 纵向8行，图片靠右，宽度12.5em
\begin{center}
\vskip -0.35in
\begin{minipage}{0.49\textwidth} % 定义代码块的宽度
    \begin{lstlisting}[language=Python, basicstyle=\scriptsize\ttfamily, caption=Code for selecting the optimal $d$, label={lst:listing}]
import matplotlib.pyplot as plt
import numpy as np
# Define parameters
N = 55800  # Total number of instances
p = 128     # Embedding dimension
C = 20      # Number of categories
# Define the range of queue lengths
d_values = np.linspace(1000, N, 100)
R_values = list((d_values + C * (np.floor(N / d_values) + 1) * p) / N)
min_R_values = min(np.abs(R_values))
optimal_d = d_values[R_values.index(min_R_values)]
print('The optimal d is:', optimal_d)
    \end{lstlisting}
    \end{minipage}
\end{center}
\end{wrapfigure}

The new training framework significantly reduces storage space utilization by merging the local sample covariance matrices, ensuring that the calculated value of information density remains unchanged. This innovative strategy not only provides an efficient solution for dynamically updating information density but also offers a low-cost storage solution for future research. 

\iffalse
\begin{table*}[t]
    \centering
    \caption{Accuracy for each class using different methods with ResNet-50 and ResNet-101 as the backbone}
    \label{tab:combined_accuracy}
\begin{small}
\renewcommand\arraystretch{1.1}
\setlength{\tabcolsep}{4.5pt} % 修改边距
\begin{tabular}{lccccccccccccccc}
    \hline  \toprule
    \textbf{Class} & \multicolumn{6}{c}{\textbf{ResNet-50}} & \multicolumn{6}{c}{\textbf{ResNet-101}} \\
    \cmidrule(lr){2-7} \cmidrule(lr){8-13}
    & Seesaw & ID-Seesaw & EFL & ID-EFL & C2AM & ID-C2AM & Seesaw & ID-Seesaw & EFL & ID-EFL & C2AM & ID-C2AM \\
    \midrule
    cat           & 89.5 & 89.0 & 88.3 & 88.7 & 88.1 & 87.8 & 91.4 & 91.8 & 91.2 & 90.5 & 89.8 & 88.0 \\
    car           & 79.8 & 82.5 & 78.9 & 80.5 & 80.8 & 81.3 & 82.2 & 83.0 & 81.1 & 82.4 & 82.0 & 83.3 \\
    horse         & 85.9 & 86.7 & 86.3 & 85.5 & 87.0 & 88.1 & 86.5 & 85.6 & 86.6 & 87.1 & 87.6 & 87.1 \\
    bus           & 84.8 & 85.8 & 82.9 & 81.9 & 84.5 & 85.2 & 83.6 & 84.9 & 84.4 & 85.7 & 85.3 & 86.5 \\
    bicycle       & 86.1 & 87.5 & 84.5 & 83.8 & 85.2 & 84.6 & 87.0 & 88.3 & 85.8 & 85.5 & 85.5 & 86.2 \\
    dog           & 88.4 & 88.2 & 85.3 & 86.0 & 86.5 & 87.4 & 90.8 & 90.6 & 90.2 & 90.3 & 87.1 & 86.7 \\
    person        & 79.1 & 79.7 & 77.1 & 80.1 & 77.8 & 79.5 & 84.9 & 84.6 & 82.8 & 84.6 & 78.2 & 79.8 \\
    train         & 84.2 & 85.9 & 82.9 & 84.4 & 85.3 & 87.0 & 86.4 & 88.0 & 85.4 & 85.9 & 85.5 & 86.1 \\
    motorbike     & 83.6 & 84.7 & 80.1 & 82.7 & 82.0 & 83.2 & 87.4 & 87.7 & 84.1 & 85.0 & 82.6 & 83.6 \\
    cow           & 82.9 & 83.5 & 79.8 & 81.6 & 80.7 & 82.1 & 81.8 & 82.0 & 80.1 & 81.9 & 81.0 & 81.9 \\
    \rowcolor{purple!10} % 设置淡蓝色填充
    aeroplane     & 69.7 & 72.8 \textcolor{red}{$\uparrow$3.1} & 72.1 & 74.4 \textcolor{red}{$\uparrow$2.3} & 71.8 & 74.3 \textcolor{red}{$\uparrow$2.5} & 73.1 & 75.5 \textcolor{red}{$\uparrow$2.4} & 70.0 & 72.6 \textcolor{red}{$\uparrow$2.6} & 72.5 & 74.7 \textcolor{red}{$\uparrow$2.2} \\
    tvmonitor     & 75.8 & 79.3 & 76.4 & 77.2 & 74.1 & 75.7 & 75.2 & 76.8 & 69.9 & 72.2 & 77.0 & 78.5 \\
    sheep         & 76.5 & 78.7 & 74.8 & 76.9 & 75.6 & 76.5 & 80.4 & 81.7 & 78.2 & 80.9 & 75.5 & 77.4 \\
    bird          & 79.3 & 81.5 & 75.2 & 76.8 & 79.7 & 81.0 & 74.1 & 76.4 & 72.3 & 74.1 & 80.4 & 81.2 \\
    diningtable   & 74.0 & 76.0 & 71.3 & 73.2 & 73.4 & 75.1 & 75.5 & 75.9 & 73.3 & 75.1 & 74.3 & 76.1 \\
    sofa          & 78.7 & 79.1 & 72.7 & 70.5 & 79.5 & 80.8 & 77.2 & 78.0 & 72.5 & 74.8 & 79.7 & 80.6 \\
    \rowcolor{purple!10} % 设置淡蓝色填充
    boat          & 66.4 & 68.2 \textcolor{red}{$\uparrow$1.8} & 62.4 & 64.9 \textcolor{red}{$\uparrow$2.5} & 64.2 & 66.5 \textcolor{red}{$\uparrow$2.2} & 63.3 & 65.3 \textcolor{red}{$\uparrow$2.0} & 64.2 & 67.2 \textcolor{red}{$\uparrow$3.0} & 65.2 & 67.9 \textcolor{red}{$\uparrow$2.7} \\
    \rowcolor{purple!10} % 设置淡蓝色填充
    bottle        & 53.6 & 57.9 \textcolor{red}{$\uparrow$4.3} & 50.8 & 54.6 \textcolor{red}{$\uparrow$3.8} & 54.8 & 58.2 \textcolor{red}{$\uparrow$3.4} & 52.4 & 55.6 \textcolor{red}{$\uparrow$3.2} & 51.5 & 54.0 \textcolor{red}{$\uparrow$2.5} & 55.6 & 58.6 \textcolor{red}{$\uparrow$3.0} \\
    \rowcolor{purple!10} % 设置淡蓝色填充
    chair         & 65.8 & 68.5 \textcolor{red}{$\uparrow$2.7} & 61.1 & 63.7 \textcolor{red}{$\uparrow$2.6} & 62.3 & 65.1 \textcolor{red}{$\uparrow$2.8} & 62.3 & 65.6 \textcolor{red}{$\uparrow$3.3} & 61.9 & 64.2 \textcolor{red}{$\uparrow$2.3} & 63.5 & 66.0 \textcolor{red}{$\uparrow$2.5} \\
    \rowcolor{purple!10}  % 设置淡蓝色填充
    pottedplant   & 52.7 & 57.3 \textcolor{red}{$\uparrow$4.6} & 49.1 & 53.2 \textcolor{red}{$\uparrow$4.1} & 51.5 & 54.8 \textcolor{red}{$\uparrow$3.3} & 53.9 & 57.8 \textcolor{red}{$\uparrow$3.9} & 50.0 & 53.5 \textcolor{red}{$\uparrow$3.5} & 54.0 & 57.3 \textcolor{red}{$\uparrow$3.3} \\ \midrule

\rowcolor{gray!20}  % 设置淡蓝色填充
    Total   & 52.7 & 57.3 \textcolor{red}{$\uparrow$4.6} & 49.1 & 53.2 \textcolor{red}{$\uparrow$4.1} & 51.5 & 54.8 \textcolor{red}{$\uparrow$3.3} & 53.9 & 57.8 \textcolor{red}{$\uparrow$3.9} & 50.0 & 53.5 \textcolor{red}{$\uparrow$3.5} & 54.0 & 57.3 \textcolor{red}{$\uparrow$3.3} \\
    \bottomrule \hline
\end{tabular}
\end{small}
\end{table*}
\fi


%\vspace{-10 \in}
\section{Experiments}
\label{sec4}

We conducted a comprehensive evaluation of the effectiveness of IGAM on long-tailed and relatively balanced object detection benchmark datasets. The experiments are divided into two parts: the first part is carried out on the long-tailed large-vocabulary datasets LVIS v1.0 and COCO-LT, while the second part is conducted on the relatively balanced Pascal VOC dataset.

\subsection{Datasets and Evaluation Metrics}
\label{sec4.1}
\textbf{LVIS v1.0} \cite{lvis} contains 1,203 categories, with the training set consisting of 100k images (approximately 1.3M instances) and the validation set containing 19.8k images. Based on the frequency of occurrence in the training set, all categories are divided into three groups: rare (1$\sim$10 images), common (11$\sim$100 images), and frequent (more than 100 images). In line with EFL \cite{efl}, we report not only the widely used object detection metric $AP^b$ across IOU thresholds (from 0.5 to 0.95) but also the bounding box AP for frequent ($AP_f$), common ($AP_c$), and rare ($AP_r$) categories separately.
The \textbf{COCO-LT} \cite{number4} dataset is a long-tailed subset of MS COCO \cite{MSCOCO}, and they share the same validation set. Consistent with previous work \cite{seesaw}, we divided the 80 classes in COCO-LT into four groups based on the number of training instances per class: fewer than 20 images, 20$\sim$400 images, 400$\sim$8000 images, and 8000 or more images. 
\textbf{Pascal VOC} \cite{VOC} includes two versions, 2007 and 2012, comprising a total of 20 classes. Following standard practice \cite{tong2023rethinking}, we trained on the train+val sets of VOC2007 and VOC2012 and tested on the test set of VOC2007.
We report the \emph{Average Precision (AP)} for each class on Pascal VOC, and on COCO-LT, we report the accuracy of the four groups as $AP_1^b$, $AP_2^b$, $AP_3^b$, and $AP_4^b$. The mean average precision is reported as $mAP^b$.


\subsection{Implementation Details}
\label{sec4.2}

Consistent with previous studies \cite{qi2023balanced}, we implemented the Faster R-CNN \cite{faster} detector using the MMDetection \cite{mmdetection} toolbox and adopted ResNet-50 and ResNet-101 \cite{resnet} with an FPN \cite{size2} structure as the backbone networks. During training, we set the batch size to 16 and the initial learning rate to 0.02, consistent with EFL \cite{efl} and C2AM \cite{c2am}. We trained the model using an SGD optimizer with a momentum of 0.9 and a weight decay rate of 0.0001 for 24 epochs. The learning rate was reduced to 0.002 and 0.0002 at the end of the 16th and 22nd epochs, respectively. In all experiments, we applied random horizontal flipping and multi-scale jittering for data augmentation. We did not use any test-time augmentations. We did not use any test-time augmentations.


\subsection{Main Results: Effectiveness of IGAM Loss}
\label{sec4.3}

\begin{wraptable}[10]{r}{0.36\textwidth} % 表格放在右侧，宽度为页面宽度的60%
\vskip -0.4in
\caption{Results of IGAM Loss with different hyper-parameter $s$ on LVIS v1.0. The model is Faster R-CNN with R-50-FPN backbone.}
\label{tab1}
\vskip 0.1in
\centering  
\begin{small}
\renewcommand\arraystretch{1}
\setlength{\tabcolsep}{5.4pt} % 修改边距
\begin{tabular}{l|c|ccc}
\hline \toprule 
s &$mAP^b$ &$AP_r$  &$AP_c$  &$AP_f$     \\ \hline

10   &16.8 &0.8  &12.6  &27.8 \\ 
 20  &26.2  &17.6  &23.8  &31.5 \\

 30  &\textbf{26.8} &19.0  &25.2  &31.4 \\ 
 40 &25.9  &18.4  &23.5  &30.9 \\
  50 &24.6  &16.9  &22.7  &30.5 \\

\bottomrule \hline
\end{tabular}
\end{small}
%\vskip -0.1in
\end{wraptable}

Recalling Section \ref{sec3.2}, we introduce the hyperparameter \(s\) in the IGAM Loss, which scales the cosine value to ensure it falls within an appropriate range, thereby stabilizing the optimization process. This is a standard practice in cosine-based classifiers and has been widely adopted. We refer to \cite{c2am} for rigorous determination of \(s\) and tested the model's performance under different settings. The experimental results, summarized in Table \ref{tab1}, show that when \(s = 30\), the model trained with IGAM Loss achieves optimal performance. After determining the hyperparameter, we compare IGAM Loss with the baseline model. Since our method is derived from cross-entropy loss, the baseline model is trained using cross-entropy loss.



To validate the effectiveness of IGAM Loss, we employed Faster R-CNN and Cascade Faster R-CNN as detection frameworks, with ResNet-50 and ResNet-101 as backbone networks. The baseline models were trained using cross-entropy loss. As shown in Table \ref{tab2}, the four baseline models trained with cross-entropy loss achieve an average precision (APr) of only 1.1\%, 1.0\%, 1.5\%, and 2.6\% on tail classes, making it nearly impossible to recognize tail instances. In contrast, IGAM significantly improves the detection accuracy for tail classes. For example, with Mask R-CNN as the framework and ResNet-50-FPN as the backbone, IGAM raises APr by \textbf{17.9\%}. Moreover, IGAM also brings a \textbf{9.1\%} performance gain in the APc metric. Beyond the substantial improvements in tail-class performance, IGAM surpasses the baseline model in overall performance across the four different detection frameworks and backbone configurations by \textbf{7.5\%}, \textbf{7.1\%}, \textbf{6.4\%}, and \textbf{5.2\%}, respectively.

\begin{table}[t]
\vskip -0.25in
\caption{Evaluation results on LVIS v1.0. The $mAP^b$, $AP_r$, $AP_c$, and $AP_f$ (\%) for each method are reported, with \textcolor[RGB]{0,201,87}{green arrows} indicating performance improvements.}
\label{tab2}
\vskip 0.05in
\centering  
\begin{small}
\renewcommand\arraystretch{0.95}
\setlength{\tabcolsep}{3.1pt} %修改边距
\begin{tabular}{l|l|c|c|ccc}
\hline \toprule 
Framework  &Backbone &Loss &$mAP^b$ &$AP_r$  &$AP_c$  &$AP_f$     \\ \hline

\multirow{6}{*}{Faster R-CNN} &\multirow{2}{*}{ResNet-50-FPN}  &Cross-Entropy (CE) &19.3 &1.1  &16.1  &30.9 \\ 
  &   & IGAM Loss  &26.8 \textcolor[RGB]{0,201,87}{$\uparrow$7.5}  &19.0 \textcolor[RGB]{0,201,87}{$\uparrow$17.9}  &25.2  &31.4 \\ \cline{2-7}

   &\multirow{2}{*}{ResNet-101-FPN}  &Cross-Entropy (CE) &20.9 &1.0  &18.2 &32.7 \\ 
   &  & IGAM Loss & 28.0 \textcolor[RGB]{0,201,87}{$\uparrow$7.1} &20.1 \textcolor[RGB]{0,201,87}{$\uparrow$19.1}  &26.8  &32.5  \\  \cline{2-7}

   &\multirow{2}{*}{Swin-T}  &Cross-Entropy (CE) &25.4 &6.2  &24.5 &35.3 \\ 
   &  & IGAM Loss & 31.7 \textcolor[RGB]{0,201,87}{$\uparrow$6.3} &21.4 \textcolor[RGB]{0,201,87}{$\uparrow$15.2}  &30.8  &37.1  \\ \hline 


\multirow{6}{*}{Cascade Mask R-CNN} &\multirow{2}{*}{ResNet-50-FPN}  &Cross-Entropy (CE) &22.7 &1.5  &20.6 &34.4 \\ 
  &   & IGAM Loss  &29.1 \textcolor[RGB]{0,201,87}{$\uparrow$6.4}  &21.5 \textcolor[RGB]{0,201,87}{$\uparrow$20.0}  &27.7  &33.9  \\ \cline{2-7}

   &\multirow{2}{*}{ResNet-101-FPN}  &Cross-Entropy (CE-FPN) &24.5 &2.6 &23.1  &35.8  \\ 
   &  & IGAM Loss  & 29.7 \textcolor[RGB]{0,201,87}{$\uparrow$5.2} &21.9 \textcolor[RGB]{0,201,87}{$\uparrow$19.3}  &28.5  &34.6  \\  \cline{2-7}

   &\multirow{2}{*}{Swin-T}  &Cross-Entropy (CE) &31.3 &6.8  &30.2 &39.4 \\ 
   &  & IGAM Loss & 37.9 \textcolor[RGB]{0,201,87}{$\uparrow$6.6} &25.2 \textcolor[RGB]{0,201,87}{$\uparrow$18.4}  &35.5  &38.7  \\ \hline 


\multirow{7}{*}{DETR} &\multirow{2}{*}{ResNet-50-FPN}  &Cross-Entropy (CE) &21.8 &3.3  &21.2 &30.5 \\ 
  &   & IGAM Loss  &27.6 \textcolor[RGB]{0,201,87}{$\uparrow$5.8}  &18.5 \textcolor[RGB]{0,201,87}{$\uparrow$15.2}  &27.0  &32.7  \\ \cline{2-7}

   &\multirow{2}{*}{ResNet-101-FPN}  &Cross-Entropy (CE) &23.1 &3.7 &23.4  &32.2  \\ 
   &  & IGAM Loss  & 30.4 \textcolor[RGB]{0,201,87}{$\uparrow$7.3} &20.7 \textcolor[RGB]{0,201,87}{$\uparrow$17.0}  &30.0  &35.5  \\  \cline{2-7}

   &\multirow{3}{*}{Swin-T}  &Cross-Entropy (CE) &30.2 &6.3  &28.9 &38.2 \\ 
   &   & RichSem \cite{RichSem} &34.9  & 26.0  & 32.6   &41.3  \\
   &  & IGAM Loss & 37.3 \textcolor[RGB]{0,201,87}{$\uparrow$7.1} &24.8 \textcolor[RGB]{0,201,87}{$\uparrow$18.5}  &34.8  &38.3  \\ 



\bottomrule \hline
\end{tabular}
\end{small}
%\vskip -0.18in
\end{table}


\begin{table}[t]
\vskip -0.3in
    \centering
    \caption{Performance comparison with state-of-the-art methods on LVIS {\em val} set. The ResNet-50-FPN and ResNet-101-FPN are adopted as backbones for Faster R-CNN. All methods are trained with a 2x schedule, {\em i.e.}, 24 epochs in total. The $mAP^b$, $AP_r$, $AP_c$, and $AP_f$ (\%) for each method are reported. The best and second-best results are shown in \underline{\textbf{underlined bold}} and \textbf{bold}, respectively.}
\vskip 0.05in
    \renewcommand\arraystretch{0.95}
%\setlength{\tabcolsep}{3.5pt} %修改边距
    \resizebox{\textwidth}{!}{%
    \begin{tabular}{c|l|cccccccc}
\hline \toprule 
        \multirow{3}{*}{Strategy}    & \multirow{3}{*}{Methods}                                              & \multicolumn{8}{c}{LVIS   v1.0}                                                                                                                                                                \\ \cline{3-10} 
                                     &                               & \multicolumn{4}{c|}{ResNet-50-FPN}                                                                      & \multicolumn{4}{c}{ResNet-101-FPN}                                                 \\ \cline{3-10} 
                                     &                               &\multicolumn{1}{c|}{$mAP^b$}   &$AP_r$   &$AP_c$   &\multicolumn{1}{c|}{$AP_f$}    &\multicolumn{1}{c|}{$mAP^b$}   &$AP_r$   &$AP_c$   &$AP_f$ \\ \hline\hline
        \multirow{14}{*}{End-to-end} %& SCE\cite{ren2015faster}                & \multicolumn{1}{c|}{19.3}          & 1.1           & 16.1          & \multicolumn{1}{c|}{30.9}          & \multicolumn{1}{c|}{20.9}          & 1.0           & 18.2          & 32.7          \\
                                    % & BCE\cite{ren2015faster}                  & \multicolumn{1}{c|}{19.5}          & 1.6           & 16.6          & \multicolumn{1}{c|}{30.6}          & \multicolumn{1}{c|}{21.4}          & 2.0           & 19.3          & 32.3          \\
                                  %   & BCE*\cite{ren2015faster}                  & \multicolumn{1}{c|}{21.6}          & 2.6           & 19.7          & \multicolumn{1}{c|}{32.1}          & \multicolumn{1}{c|}{23.1}          & 3.7           & 21.4          & \textbf{33.5} \\
                                     & RFS \cite{lvis}                     & \multicolumn{1}{c|}{24.2}          & 14.2          & 22.3          & \multicolumn{1}{c|}{30.6}          & \multicolumn{1}{c|}{25.7}          & 15.9          & 23.7          & 32.2          \\
                                     & EQL \cite{eql}                    & \multicolumn{1}{c|}{21.8}          & 3.6           & 21.1          & \multicolumn{1}{c|}{30.5}          & \multicolumn{1}{c|}{23.4}          & 4.5           & 22.9          & 32.3          \\
                                     & DropLoss \cite{droploss}             & \multicolumn{1}{c|}{21.8}          & 5.2           & 21.8          & \multicolumn{1}{c|}{29.1}          & \multicolumn{1}{c|}{23.5}          & 5.9           & 23.9          & 30.7          \\
                                     & RIO \cite{number5}                & \multicolumn{1}{c|}{23.4}          & 15.3          & 21.2          & \multicolumn{1}{c|}{29.4}          & \multicolumn{1}{c|}{25.5}          & 17.2          & 23.7          & 31.2          \\
                                     & Forest R-CNN \cite{forest}         & \multicolumn{1}{c|}{-}             & -             & -             & \multicolumn{1}{c|}{-}             & \multicolumn{1}{c|}{-}             & -             & -             & -             \\
                                     & BALMS \cite{number2}                    & \multicolumn{1}{c|}{24.1}          & 15.2          & 23.0          & \multicolumn{1}{c|}{29.4}          & \multicolumn{1}{c|}{26.9}          & 18.5 & 25.2          & 32.4          \\
                                     & De-confound-TDE \cite{tang2020long}         & \multicolumn{1}{c|}{23.7}          & 10.0          & 22.4          & \multicolumn{1}{c|}{31.2}          & \multicolumn{1}{c|}{-}             & -             & -             & -             \\
                                     & EQLv2 \cite{eqlv2}                & \multicolumn{1}{c|}{25.4}          & 15.8          & 23.5          & \multicolumn{1}{c|}{\underline{\textbf{31.7}}}          & \multicolumn{1}{c|}{26.8}          & 17.1          & 24.9          & \textbf{33.1}          \\
                                     & Seesaw \cite{seesaw}              & \multicolumn{1}{c|}{24.8}          & 14.8          & 22.7          & \multicolumn{1}{c|}{\textbf{31.6}}          & \multicolumn{1}{c|}{26.6}          & 14.9          & 25.2          &  \underline{\textbf{33.3}}          \\
                                     & FASA \cite{fasa}                       & \multicolumn{1}{c|}{21.5}          & 7.4           & 19.2          & \multicolumn{1}{c|}{30.2}          & \multicolumn{1}{c|}{22.9}             & 9.0             & 20.6             & 31.6             \\
                                   %  & PCB\cite{he2022relieving}                   & \multicolumn{1}{c|}{23.0}          & 6.2           & 21.5          & \multicolumn{1}{c|}{\textbf{32.2}} & \multicolumn{1}{c|}{24.6}          & 8.0           & 23.1          & \textbf{33.5} \\ 
                                 &EFL \cite{efl}                   &  \multicolumn{1}{c|}{26.0} &\textbf{16.6} &25.1  & \multicolumn{1}{c|}{30.8}    &  \multicolumn{1}{c|}{26.3}  &\textbf{18.5}  &23.9 & \multicolumn{1}{c}{32.6} \\     
                                 &C2AM \cite{c2am}                 &  \multicolumn{1}{c|}{25.4}  &15.6  &24.2  & \multicolumn{1}{c|}{30.9}    &  \multicolumn{1}{c|}{26.5}  &18.1  &25.5 &31.2 \\   \hline\hline
        \multirow{5}{*}{Decoupled}   & SimCal \cite{number4}                & \multicolumn{1}{c|}{-}             & -             & -             & \multicolumn{1}{c|}{-}             & \multicolumn{1}{c|}{-}             & -             & -             & -             \\
                                     & BAGS \cite{bags}                    & \multicolumn{1}{c|}{23.7}          & 14.2          & 22.2          & \multicolumn{1}{c|}{29.6}          & \multicolumn{1}{c|}{25.4}          & 14.9          & 25.2          & 31.4          \\
                                     & ACSL \cite{acsl}             & \multicolumn{1}{c|}{22.2}          & 9.9           & 21.3          & \multicolumn{1}{c|}{28.5}          & \multicolumn{1}{c|}{23.7}          & 11.0          & 23.0          & 30.2          \\
                                     & DisAlign \cite{zhang2021distribution}        & \multicolumn{1}{c|}{20.9}          & 3.9           & 20.4          & \multicolumn{1}{c|}{29.0}          & \multicolumn{1}{c|}{25.5}          & 13.3          & 24.5          & 32.0          \\
                                     & LOCE \cite{loce}         & \multicolumn{1}{c|}{25.1}          & 15.7          & 24.2          & \multicolumn{1}{c|}{30.1}          & \multicolumn{1}{c|}{26.7}          & 18.4          & 25.5          & 31.7          \\  
    & BACL \cite{qi2023balanced}                     & \multicolumn{1}{c|}{\textbf{26.1}} &16.0 &\underline{\textbf{25.7}} & \multicolumn{1}{c|}{30.9}          & \multicolumn{1}{c|}{\textbf{27.8}} & 18.1          & \underline{\textbf{27.3}} &32.6    \\ \hline
             End-to-end                        & IGAM                     & \multicolumn{1}{c|}{\underline{\textbf{26.8}}} & \underline{\textbf{19.0}} & \textbf{25.2} & \multicolumn{1}{c|}{31.4}          & \multicolumn{1}{c|}{\underline{\textbf{28.0}}} & \textbf{20.1}          & \textbf{26.8} & 32.5          \\ \bottomrule \hline
        \end{tabular}}
    \label{tab3}
\vskip -0.2in
\end{table}

IGAM demonstrates remarkable generalization capabilities across various configurations, and its performance leap over the baseline models highlights the effectiveness and versatility of our proposed method, which refines decision space partitioning dynamically using information amount.



\iffalse
\begin{table}[t]
%\vskip -0.05in
\caption{Evaluation results on LVIS v1.0. The $mAP^b$, $AP_r$, $AP_c$, and $AP_f$ (\%) for each method are reported, with red arrows indicating performance improvements.}
\label{tab4}
%\vskip -0.07in
\centering  
\begin{small}
\renewcommand\arraystretch{1.1}
\setlength{\tabcolsep}{5pt} %修改边距
\begin{tabular}{l|l|ccc|l|ccc}
\hline \toprule 
\multirow{3}{*}{Dataset}   &\multicolumn{8}{c}{LVIS v1.0}     \\ \cline{2-9}
  &\multicolumn{4}{c}{ResNet-50-FPN}   &\multicolumn{4}{c}{ResNet-101-FPN}  \\ \cline{2-9}
   &$mAP^b$   &$AP_r$   &$AP_c$   &$AP_f$    &$mAP^b$   &$AP_r$   &$AP_c$   &$AP_f$    \\ \hline


SCE \cite{faster} & 19.3  &1.1 &16.1 &\underline{\textbf{30.9}}   \\ 
BCE \cite{faster} & 19.5  &1.6 &16.6 &30.6   \\ 
EQL \cite{eql} & 21.8  &3.6 &21.1 &30.5   \\ 
DropLoss \cite{droploss} & 21.8  &5.2 &21.8 &29.1   \\  
Seesaw \cite{seesaw}   & 24.8  &14.8  &22.7 &31.6    \\ 
EFL \cite{efl} & 26.0 &18.6 &24.5  &30.8    \\ 
C2AM \cite{c2am}     & 25.4  &15.6  &24.2  &30.9    \\  



SCE \cite{faster} & 20.9  &1.0 &18.2 &32.7   \\ 
BCE \cite{faster} & 21.4  &2.0 &19.3 &32.3   \\ 
EQL \cite{eql} & 23.4  &4.5 &22.9 & 32.3  \\ 
DropLoss \cite{droploss} & 23.5  &5.9 &23.9 &30.7   \\  \hline
Seesaw \cite{seesaw}   & 26.6  &14.9  &25.2 &33.3     \\ 
EFL \cite{efl} & 26.3  &19.3  &23.2 &32.6   \\ 
C2AM \cite{c2am}     & 26.5  &18.1  &25.5 &31.2  \\  

\bottomrule \hline
\end{tabular}
\end{small}
%\vskip -0.18in
\end{table}
\fi

\subsection{Comparison with State-of-the-Arts}
\label{sec4.4}

Table \ref{tab3} presents the experimental results on LVIS v1.0. IGAM outperforms the current state-of-the-art methods on both ResNet-50-FPN and ResNet-101-FPN backbones, achieving overall performances of \textbf{26.8\%} and \textbf{28.0\%}, respectively. Notably, for rare categories, IGAM surpasses the second-best method by \textbf{2.4\%} and \textbf{1.6\%} on the two backbones, respectively. It is worth mentioning that despite BACL incorporating a series of techniques, including foreground-balanced loss, synthetic hallucination samples, and decoupled training, our method still exhibits strong competitiveness.
For the most frequent categories, although EQLv2 and Seesaw exhibit exceptional performance, they significantly lack attention to rare categories. In contrast, IGAM demonstrates superior performance on rare categories while maintaining competitive results on the most frequent categories. On the ResNet-50-FPN backbone, IGAM's $AP_f$ is only \textbf{0.3\%} and \textbf{0.2\%} lower than that of EQLv2 and Seesaw, respectively.
We attribute IGAM's superior overall performance to its accurate measurement of category learning difficulty through information amount, allowing IGAM not to impair the performance of frequent categories by focusing too much on rare categories.

\begin{table}[h]
\vskip -0.1in
\caption{Evaluation results on COCO-LT. The $mAP^b$, $AP_1^b$, $AP_2^b$, $AP_3^b$, and $AP_4^b$ (\%) for each method are reported. An asterisk (*) indicates results reproduced by our implementation. The best and second-best results are shown in \underline{\textbf{underlined bold}} and \textbf{bold}, respectively.}
\label{tab4}
\vskip 0.05in
\centering  
\begin{small}
\renewcommand\arraystretch{1.1}
\setlength{\tabcolsep}{4.5pt} %修改边距
\begin{tabular}{l|l|cccc|l|cccc}
\hline \toprule 
\multirow{3}{*}{Methods}   &\multicolumn{10}{c}{COCO-LT}     \\  \cline{2-11}
       &\multicolumn{5}{c|}{ResNet-50-FPN}    &\multicolumn{5}{c}{ResNet-101-FPN}   \\ \cline{2-11}
   &$mAP^b$   &$AP_1^b$   &$AP_2^b$   &$AP_3^b$ &$AP_4^b$    &$mAP^b$   &$AP_1^b$   &$AP_2^b$   &$AP_3^b$ &$AP_4^b$    \\ \hline
%\multicolumn{2}{l}{\textbf{R-50-FPN}} \\ \hline


Cross-Entropy (CE) & 24.5  &0 &14.6 &\textbf{29.6}  &\textbf{32.9}       & 26.0  &0 &16.4 &\underline{\textbf{31.4}} &\underline{\textbf{34.2}}    \\ 
Seesaw \cite{seesaw}   & 23.9  &3.0  &14.5  &28.4  &32.3          & 24.9  &3.2  &14.5 &30.0 &33.4      \\ 
EQLv2 \cite{eqlv2}   & \textbf{25.7}  &3.8  &\underline{\textbf{18.1}}  &29.6  &\underline{\textbf{33.0}}          & \textbf{26.8}  &3.2  &\underline{\textbf{19.4}} &\textbf{30.8} &\textbf{34.1}      \\ 
EFL* \cite{efl} & 25.0 &\textbf{3.8} &16.3  &29.5  &32.5          & 25.4  &\textbf{3.6}  &16.5 &30.2 &32.8     \\ 
C2AM* \cite{c2am}     & 24.7  &2.8  &15.6  &29.4  &32.3      & 25.1  &2.9  &15.6 &30.2 &32.7    \\  \hline

IGAM    & \underline{\textbf{25.8}} &\underline{\textbf{6.1}}   &\textbf{18.0}  &\underline{\textbf{29.7}}  &32.5    & \underline{\textbf{27.0}}  &\underline{\textbf{6.6}} &\textbf{19.0} &30.5 &33.4    \\ 


\bottomrule \hline
\end{tabular}
\end{small}
\vskip -0.15in
\end{table}

\begin{table*}[t]
\vskip -0.1in
    \centering
    \caption{Evaluation results on Pascal VOC. $AP^b (\%)$ for each class using different methods with ResNet-50 and ResNet-101 as the backbone. We selected the five most challenging classes, with the best and second-best results are shown in \underline{\textbf{underlined bold}} and \textbf{bold}, respectively. \textcolor[RGB]{0,201,87}{Green arrows and values} indicate the difference between the best and second-best results.}
    \label{tab5}
\vskip 0.05in
\begin{small}
\renewcommand\arraystretch{0.9}
\setlength{\tabcolsep}{2.8pt} % 修改边距
\begin{tabular}{l|c|ccccc|c|ccccc}
    \hline  \toprule
    \textbf{Class} & \multicolumn{6}{c}{\textbf{ResNet-50}} & \multicolumn{6}{c}{\textbf{ResNet-101}} \\
    \cmidrule(lr){2-7} \cmidrule(lr){7-13}
    &CE  & Seesaw & EFL  & C2AM &BACL & IGAM &CE  & Seesaw  & EFL   & C2AM &BACL     & IGAM \\
    \midrule
    cat           &84.8     & 89.5  & 88.3 & 88.1 &89.6        & 87.8        &88.9         & 91.4  & 91.2  & 89.8 &91.8         & 88.0 \\
    car           & 75.7    & 79.8  & 78.9 & 80.8 &80.3       & 81.3       &79.2         & 82.2  & 81.1  & 82.0 &82.7         & 83.3 \\
    horse       & 81.6      & 85.9  & 86.3  & 87.0 &85.5       & 88.1     &83.6         & 86.5  & 86.6 & 87.6 &86.7         & 87.1 \\
    bus           &80.2     & 84.8  & 82.9  & 84.5 &84.1       & 85.2      &82.4         & 83.6  & 84.4 & 85.3 &86.1         & 86.5 \\
    bicycle     &82.1       & 86.1  & 84.5 & 85.2 &87.8       & 84.6      & 84.0        & 87.0  & 85.8 & 85.5 &88.1         & 86.2 \\
    dog           &81.7     & 88.4  & 85.3  & 86.5 &88.0       & 87.4      &88.9         & 90.8  & 90.2  & 87.1 &91.3         & 86.7 \\
    person     &75.8        & 79.1 & 77.1  & 77.8 &80.5       & 79.5      &81.1         & 84.9 & 82.8  & 78.2 &85.6         & 79.8 \\
    train         &80.4     & 84.2  & 82.9  & 85.3 &85.6       & 87.0      &83.9         & 86.4  & 85.4 & 85.5 &87.7         & 86.1 \\
    motorbike  &79.4        & 83.6  & 80.1 & 82.0 &82.6       & 83.2    &83.2         & 87.4  & 84.1 & 82.6 &86.8         & 83.6 \\
    cow           &80.3     & 82.9  & 79.8  & 80.7 &82.8       & 82.1      &78.4         & 81.8 & 80.1  & 81.0 &81.5         & 81.9 \\
    \rowcolor{purple!5} % 设置淡蓝色填充
    aeroplane    &64.4      & 69.7  & \textbf{72.1}  & 71.8 &70.1       & \underline{\textbf{74.3}} \textcolor[RGB]{0,201,87}{$\uparrow$2.2}          &69.0         & \textbf{73.1}  & 70.0  & 72.5 &71.3         & \underline{\textbf{74.7}} \textcolor[RGB]{0,201,87}{$\uparrow$1.6} \\
    tvmonitor     &71.3     & 75.8  & 76.4 & 74.1 &75.9       & 75.7 &68.0         & 75.2  & 69.9 & 77.0 &71.6         & 78.5 \\
    sheep       &73.4       & 76.5  & 74.8  & 75.6 &78.8       & 76.5 &75.9         & 80.4  & 78.2  & 75.5 &79.6         & 77.4 \\
    bird         &75.5      & 79.3  & 75.2 & 79.7 &77.3       & 81.0 & 69.5        & 74.1  & 72.3  & 80.4 &72.7         & 81.2 \\
    diningtable   &70.8     & 74.0  & 71.3  & 73.4 &76.3       & 75.1 &68.7         & 75.5  & 73.3  & 74.3 &74.7         & 76.1 \\
    sofa         &75.2      & 78.7 & 72.7  & 79.5 &76.6       & 80.8 &70.6         & 77.2  & 72.5  & 79.7 &73.5         & 80.6 \\
    \rowcolor{purple!5} % 设置淡蓝色填充
    boat          &61.8     & \textbf{66.4}  & 62.4  & 64.2 & 65.2      & \underline{\textbf{67.7}} \textcolor[RGB]{0,201,87}{$\uparrow$1.3} &61.5         & 63.3  & 64.2  & \textbf{65.2} &64.2         & \underline{\textbf{67.9}} \textcolor[RGB]{0,201,87}{$\uparrow$2.7} \\
    \rowcolor{purple!5} % 设置淡蓝色填充
    bottle        &50.6     & 53.6  & 50.8  & \textbf{54.8} &54.2       & \underline{\textbf{58.2}} \textcolor[RGB]{0,201,87}{$\uparrow$3.4} & 50.0        & 52.4  & 51.5  & \textbf{55.6} &53.6         & \underline{\textbf{58.6}} \textcolor[RGB]{0,201,87}{$\uparrow$3.0} \\
    \rowcolor{purple!5} % 设置淡蓝色填充
    chair      &61.5        & \textbf{65.8}  & 61.1  & 62.3 &61.7       & \underline{\textbf{66.5}} \textcolor[RGB]{0,201,87}{$\uparrow$0.7} &58.9         & 62.3  & 61.9  & \textbf{63.5} &63.1         & \underline{\textbf{66.0}} \textcolor[RGB]{0,201,87}{$\uparrow$2.5} \\
    \rowcolor{purple!5}  % 设置淡蓝色填充
    pottedplant  &49.3      & \textbf{52.7} & 49.1  & 51.5 &51.1       & \underline{\textbf{54.8}} \textcolor[RGB]{0,201,87}{$\uparrow$2.1} &48.2         & 53.9  & 50.0  & \textbf{54.0} &52.2         & \underline{\textbf{57.3}} \textcolor[RGB]{0,201,87}{$\uparrow$3.3} \\ \midrule

\rowcolor{gray!20}  % 设置淡蓝色填充
    Total   & 72.8    & \textbf{76.9}  &74.6 & 76.2 &76.8       & \underline{\textbf{77.7}} \textcolor[RGB]{0,201,87}{$\uparrow$0.8} &73.5         &  \textbf{77.5} & 75.8  &77.0 &77.3         & \underline{\textbf{78.6}} \textcolor[RGB]{0,201,87}{$\uparrow$1.1} \\
    \bottomrule \hline
\end{tabular}
\end{small}
\vskip -0.1in
\end{table*}



\subsection{Evaluation results on COCO-LT}
\label{sec4.5}

The COCO-LT dataset is not yet a widely recognized benchmark in the field of long-tailed object detection, and thus, there are relatively few studies validating methods on it. We trained baseline models on COCO-LT using cross-entropy loss and compared Seesaw and EQLv2 following \cite{qi2023balanced}. Additionally, we independently implemented EFL and C2AM. The experimental results are summarized in Table \ref{tab4}. It can be observed that IGAM achieves the best overall performance on both backbone networks, with \textbf{25.8\%} and \textbf{27.0\%}, respectively. Notably, IGAM surpasses the second-best method in the $AP_1^b$ metric by \textbf{2.3\%} and \textbf{3.0\%} on the two backbones, respectively, highlighting the significant advantage of our method on rare categories. While cross-entropy loss and EQLv2 perform well on the most frequent categories, they exhibit a clear gap in performance on rare categories compared to IGAM. Furthermore, IGAM does not fall behind cross-entropy loss and EQLv2 in the $AP_4^b$ metric, demonstrating that our method effectively balances performance across both rare and frequent categories.



\subsection{Evaluation results on Pascal VOC}
\label{sec4.6}

%\textbf{More importantly for this work}, our methods significantly improve the performance of poorly performing categories. We selected five representative underperforming categories for observation: aeroplane, boat, bottle, chair, and potted plant. When the backbone is R-50-FPN, our improvements led to performance gains of \textbf{3.1\%}, \textbf{1.8\%}, \textbf{4.3\%}, \textbf{2.7\%}, and \textbf{4.6\%} for Seesaw loss in these five categories, respectively. When using R-101-FPN as the backbone, our improvements enhanced Seesaw loss performance by \textbf{2.4\%}, \textbf{2.0\%}, \textbf{3.2\%}, \textbf{3.3\%}, and \textbf{3.9\%} in these categories, respectively. Particularly, for the two worst-performing categories, bottle, and potted plant, our improvements were the most pronounced. The experimental results demonstrate that although Seesaw, EFL, and C2AM losses are designed to address model bias, our methods can further focus on and identify the underperforming categories. This further proves that the proposed information density can more accurately measure the learning difficulty of categories.


We trained models using Seesaw, EFL, and C2AM losses on the relatively balanced Pascal VOC dataset for comparison. Table \ref{tab5} presents the performance of each method across all categories as well as the overall performance. It can be observed that IGAM outperforms the other methods in terms of overall performance on both backbone networks, achieving \textbf{77.7\%} and \textbf{78.6\%}, respectively, surpassing the second-best method by 0.8\% and 1.1\%.

\textbf{More importantly}, our method significantly improves the performance of underperforming categories. We selected five representative poorly performing categories for observation: aeroplane, boat, bottle, chair, and pottedplant. With the ResNet-50 backbone, IGAM achieved the best performance across all five categories, surpassing the second-best method by \textbf{2.2\%}, \textbf{1.3\%}, \textbf{3.4\%}, \textbf{0.7\%}, and \textbf{2.1\%}, respectively. Similarly, with the ResNet-101 backbone, IGAM also achieved the best performance across all five categories, exceeding the second-best method by \textbf{1.6\%}, \textbf{2.7\%}, \textbf{3.0\%}, \textbf{2.5\%}, and \textbf{3.3\%}, respectively. Notably, for the two worst-performing categories, bottle, and pottedplant, IGAM consistently demonstrated the most significant improvements.
The experimental results indicate that even on relatively balanced datasets, our method can effectively focus on and enhance the performance of the underrepresented categories. This further validates that the category information amount we proposed more accurately measures the learning difficulty of each category.


\iffalse
\begin{figure*}[t]
\centering
	\begin{minipage}{0.497\linewidth}
		\centering
		\includegraphics[width=1\linewidth]{fig7_3}
		%\caption{chutian1}
		%\label{chutian1}%文中引用该图片代号
	\end{minipage}
	\begin{minipage}{0.497\linewidth}
		\centering
		\includegraphics[width=1\linewidth]{fig7_4}
		%\caption{chutian2}
		%\label{chutian2}%文中引用该图片代号
	\end{minipage}
\caption{The effectiveness of our method in mitigating model bias on COCO-LT and LVIS v1.0.}
\label{fig6}
%\vskip -0.2in
\end{figure*}
\fi

\subsection{Effectiveness in Reducing Model Bias}
\label{sec4.7}

%\iffalse
\begin{wrapfigure}[15]{r}{16.5em} % 纵向8行，图片靠右，宽度12.5em
\begin{center}
\vskip -0.35in
\includegraphics[width=1 \linewidth]{fig6}
%\caption{chutian1}
%\label{chutian1}%文中引用该图片代号
\vskip -0.15in
\caption{Model bias from models trained with different methods on LVIS v1.0.}
\label{fig4}
\end{center}
\end{wrapfigure}
%\fi

To more clearly demonstrate the effectiveness of our method in mitigating model bias, we use the variance of class-wise average precision (AP) as a measure of model bias. The comparison results on LVIS v1.0 are shown in Figure \ref{fig4}. It can be observed that the models trained with our method exhibit lower bias compared to Seesaw, EFL, and C2AM, across two different backbones. Notably, compared to Seesaw, our method reduces model bias by approximately \textbf{50\%}. These results are attributed to the accurate reflection of learning difficulty through category information amount. We encourage other researchers to explore additional potential factors influencing model bias, aiming to design more equitable object detection models.


\section{Conclusion}
\label{sec7}

This work addresses the issue that instance count fails to explain the generalized bias present in deep learning models for object detection tasks. We propose using information amount to measure the detection difficulty of categories, and experiments reveal a significant negative correlation between a category's information amount and its accuracy. Based on this finding, we propose dynamically adjusting the decision boundaries of categories using their information amount. Comprehensive empirical studies demonstrate that information amount helps the model focus more on learning challenging categories, both in long-tailed and non-long-tailed datasets.



















\bibliographystyle{iclr2025_conference}
\bibliography{TKDE2023}

\newpage

\appendix
\section{Appendix}\label{app1}
%You may include other additional sections here.

\begin{proof}[Proof 1: Integrating Local Covariance Matrices to Obtain the Global Covariance Matrix.]
Assume we have a dataset containing \( N \) instances, and we divide these instances into \( K \) batches, each containing \( n_k \) instances. For the \( k \)-th batch, let the instances be \(\{x_{k1}, x_{k2}, \ldots, x_{kn_k}\}\). The mean vector and local covariance matrix for this batch are defined as follows:
\begin{equation}
\mu_k = \frac{1}{n_k} \sum_{i=1}^{n_k} x_{ki},
\nonumber
\end{equation}

\begin{equation}
\Sigma_k = \frac{1}{n_k} \sum_{i=1}^{n_k} (x_{ki} - \mu_k)(x_{ki} - \mu_k)^T.
\nonumber
\end{equation}

The global covariance matrix is the covariance matrix of all batches, defined as:
\begin{equation}
\mu = \frac{1}{N} \sum_{k=1}^K \sum_{i=1}^{n_k} x_{ki},
\nonumber
\end{equation}

\begin{equation}
\Sigma = \frac{1}{N} \sum_{k=1}^K \sum_{i=1}^{n_k} (x_{ki} - \mu)(x_{ki} - \mu)^T.
\nonumber
\end{equation}

First, calculate the global mean \( \mu \):
\begin{equation}
\mu = \frac{1}{N} \sum_{k=1}^K \sum_{i=1}^{n_k} x_{ki} = \frac{1}{N} \sum_{k=1}^K n_k \mu_k.
\nonumber
\end{equation}

Then, split \((x_{ki} - \mu)\) in the global covariance matrix \( \Sigma \) into \((x_{ki} - \mu_k)\) and \((\mu_k - \mu)\):
\begin{equation}
\Sigma = \frac{1}{N} \sum_{k=1}^K \sum_{i=1}^{n_k} [(x_{ki} - \mu_k + \mu_k - \mu)(x_{ki} - \mu_k + \mu_k - \mu)^T].
\nonumber
\end{equation}
Expanding this, we get:
\begin{equation}
\begin{split}
\Sigma = \frac{1}{N} \sum_{k=1}^K &\sum_{i=1}^{n_k}[(x_{ki} - \mu_k)(x_{ki} - \mu_k)^T + (x_{ki} - \mu_k)(\mu_k - \mu)^T \\
& + (\mu_k - \mu)(x_{ki} - \mu_k)^T + (\mu_k - \mu)(\mu_k - \mu)^T].
\nonumber
\end{split}
\end{equation}

According to the properties of the covariance matrix, the first term is the local covariance matrix \( \Sigma_k \), and the expectation values of the second and third terms are zero. The fourth term can be calculated as:
\begin{equation}
\sum_{i=1}^{n_k} (\mu_k - \mu)(\mu_k - \mu)^T = n_k (\mu_k - \mu)(\mu_k - \mu)^T.
\nonumber
\end{equation}
Finally, the expression for the global covariance matrix is:
\begin{equation}
\begin{split}
\Sigma = \frac{1}{N} \left( \sum_{k=1}^K n_k \Sigma_k + \sum_{k=1}^K n_k (\mu_k - \mu)(\mu_k - \mu)^T \right).
\nonumber
\end{split}
\end{equation}

This formula demonstrates that the global covariance matrix can be calculated by taking a weighted sum of the local covariance matrices and adding the difference terms between local means and the global mean. This integration method effectively utilizes the unbiasedness and independence of the local covariance matrices, ensuring the accuracy of the global covariance matrix.
\label{proof1}
\end{proof}

\newpage
\section{Supplementary experiments on the Pascal VOC dataset}\label{app2}

We have also included the improvement brought by IGAM to the baseline methods when using Cascade Mask R-CNN and DETR as target detection frameworks. The experimental results are shown in the Table \ref{tab101}, and it can be observed that IGAM significantly improves the performance of the baseline methods in all four cases.

\begin{table*}[h]
\vskip -0.1in
    \centering
    \caption{Evaluation results on Pascal VOC.}
    \label{tab101}
\vskip 0.05in
\begin{small}
\renewcommand\arraystretch{0.9}
\setlength{\tabcolsep}{5.5pt} % 修改边距
\begin{tabular}{l|c|cc}
\hline \toprule 
Framework &Backbone &Loss & $mAP^b$     \\ \hline

\multirow{4}{*}{Cascade Mask R-CNN}   &\multirow{2}{*}{ResNet-50-FPN} &Cross-Entropy (CE)  &74.1   \\ 
  &  &IGAM Loss  &78.7   \\  \cline{2-4}
 &\multirow{2}{*}{ResNet-101-FPN} &Cross-Entropy (CE)  &75.6  \\
&  &IGAM Loss  &80.2   \\ \hline

\multirow{4}{*}{DETR}   &\multirow{2}{*}{ResNet-50-FPN} &Cross-Entropy (CE)  &75.8   \\ 
  &  &IGAM Loss  &80.5   \\ \cline{2-4}
 &\multirow{2}{*}{ResNet-101-FPN} &Cross-Entropy (CE)  &76.5  \\
&  &IGAM Loss  &81.0   \\ 


\bottomrule \hline
\end{tabular}
\end{small}
\vskip -0.1in
\end{table*}


\end{document}
