\subsubsection{Conditioned Diffusion Models}

By operating the data in latent space instead of pixel space, conditioned diffusion models have gained promising development \cite{rombach2022latentDiff}. MM-Diffusion \cite{ruan2023mmdi} designed for joint audio and video generation took advantage of coupled denoising autoencoders to generate aligned audio-video pairs from Gaussian noise. Extending the scalability of diffusion models, diffusion Transformers treat all inputs, including time, conditions, and noisy image patches, as tokens, leveraging the Transformer architecture to process these inputs \cite{bao2023ViTDiff}. In DiT \cite{peebles2023DiT}, William et al. emphasized the potential for diffusion models to benefit from Transformer architectures, where conditions were tokenized along with image tokens to achieve in-context conditioning. 

\subsubsection{Diffusion Models in Robotics}

Recently, a probabilistic multimodal action representation was proposed by Cheng Chi et al. \cite{chi2023diffusionpolicy}, where the robot action generation is considered as a conditional diffusion denoising process. Leveraging the diffusion policy, Ze et al. \cite{ze20243d} conditioned the diffusion policy on compact 3D representations and robot poses to generate coherent action sequences. Furthermore, GR-MG combined a progress-guided goal image generation model with a multimodal goal-conditioned policy, enabling the robot to predict actions based on both text instructions and generated goal images \cite{li2025grmg}. BESO used score-based diffusion models to learn goal-conditioned policies from large, uncurated datasets without rewards. Score-based diffusion models progressively add noise to the data and then reverse this process to generate new samples, making them suitable for capturing the multimodal nature of play data \cite{reuss2023md}. RDT-1B employed a scalable Transformer backbone combined with diffusion models to capture the complexity and multimodality of bimanual actions, leveraging diffusion models as a foundation model to effectively represent the multimodality inherent in bimanual manipulation tasks \cite{liu2024rdt-1b}. NoMaD exploited the diffusion model to handle both goal-directed navigation and task-agnostic exploration in unfamiliar environments, using goal masking to condition the policy on an optional goal image, allowing the model to dynamically switch between exploratory and goal-oriented behaviors \cite{sridhar2023nomad}. The aforementioned insights grounded the significant advancements of diffusion models in robotic tasks.

\subsubsection{VLM-based Autonomous Driving}

End-to-end autonomous driving introduces policy learning from sensor data input, resulting in a data-driven motion planning paradigm \cite{chen2024vadv2}. As part of the development of VLMs, they have shown significant promise in unifying multimodal data for specific downstream tasks, notably improving end-to-end autonomous driving systems\cite{ma2024dolphins}. DriveMM can process single images, multiview images, single videos, and multiview videos, and perform tasks such as object detection, motion prediction, and decision making, handling multiple tasks and data types in autonomous driving \cite{huang2024drivemm}. HE-Drive aims to create a human-like driving experience by generating trajectories that are both temporally consistent and comfortable. It integrates a sparse perception module, a diffusion-based motion planner, and a trajectory scorer guided by a Vision Language Model to achieve this goal \cite{wang2024hedrive}. Based on current perspectives, a differentiable end-to-end autonomous driving paradigm that directly leverages the capabilities of VLM and a multimodal action representation should be developed. 







