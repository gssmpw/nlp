\subsection{Motivation}

Over time, diffusion model-based approaches have proven their value in robotic policy learning tasks \cite{chi2023diffusionpolicy, yang2024diff-es, yu2024ldp}. Dating back to the advancement of diffusion models, they have gained recognition as a cornerstone in the field of generative modeling \cite{peebles2023DiT}. Conditioned diffusion models extend vanilla diffusion models by incorporating additional information during the generation process, while latent diffusion models improve computational efficiency and sample quality by operating in a compressed latent space \cite{rombach2022latentDiff}. As shown above, diffusion models have exhibited promising potential in generating high-quality data across various modalities and improving the representation of complex data structures \cite{yang2023diffrep}. \par In robotic applications, multisensory data often includes rich and heterogeneous sources, such as camera images, LiDAR point clouds, etc. Diffusion models, through their ability to condition on various modalities, can generate coherent and contextually relevant outputs. This capability is particularly advantageous for robotic state-action mapping, where accurate interpretation and synthesis of multisensory inputs are crucial for effective decision-making and action execution \cite{li2025grmg}. \par Regarding autonomous driving, where the end-to-end paradigm has evolved vigorously \cite{sun2024sparsedrive}, state-action mapping is a core principle that enables vehicles to learn effective decision-making policies directly from raw sensor inputs \cite{liao2024diffusiondrive}. This process involves mapping the current state of the vehicle and its environment to appropriate control and planning actions. \par To enrich the state-understanding capacity of end-to-end autonomous driving systems, Visual Language Models (VLMs) have exhibited an outstanding impact, significantly improving the systems' interpreting capability of complex driving scenarios \cite{tian2024drivevlm, guo2024vlm-auto}. Accordingly, it is essential to enhance the decision-making capabilities as the improvement of state understanding by proposing adaptive and context-aware actions tailored to various driving scenarios \cite{li2024hydra}. 

\begin{figure}[t]
    \centering
    \includegraphics[width=0.98\linewidth]{Figs/cover.png}
    \begin{subfigure}{0.48\textwidth}
        \caption{VLM's path proposals from  \ \  (b) Conditional sampled paths \\ the continuous frames based on  \ \ by our diffusion Transformers. \\ a consistent scenario.}
        \label{fig:vlm_path_proposals}
    \end{subfigure}
    
    \caption{We conduct the experiments with our VDT-Auto on unseen real-world driving dataset in a zero-shot way. The VLM is fine-tuned on our processed nuScenes dataset while the path proposals from the fine-tuned VLM are able to provide contextual approximation across the unseen continuous frames. Subsequently, our diffusion Transformers sample the path proposals based on the geometric and contextual conditions.}
    \label{fig:cover}
\vspace{-0.5cm}
\end{figure}

\par With these insights, we propose VDT-Auto, an end-to-end paradigm that bridges states and actions via VLM and diffusion Transformers. For state understanding, images from the surrounding cameras are encoded into birdâ€™s-eye view (BEV) features. In addition, a front image among the surrounding images is passed to a supervised fine-tuned VLM for contextual interpretation by the description of the detection, the advice of ego vehicle's behavior and the proposal of a path. Meanwhile, the designed diffusion Transformers encode both the BEV features from the BEV backbone and the contextual embeddings from the VLM as the states to predict the optimized path, where the added noise in the diffusion process is sampled from the VLM's proposed path. Our contributions in this paper are summarized as follows: 
\begin{itemize}
    \item We introduce a novel pipeline, VDT-Auto, which employs a BEV encoder and a VLM to geometrically and contextually parse the environment. The parsed information is then used to condition the diffusion process of our diffusion Transformers to generate the optimized actions of the ego vehicle.
    \item VDT-Auto is differentiable, where we use a processed nuScenes dataset to train our BEV encoder for perception and fine-tune our VLM for conditioning the diffusion Transformers. The constructed and processed dataset will be publicly available. 
    \item In the nuScenes open-loop planning evaluation, our VDT-Auto achieved $0.52$ m on average L2 errors and $21\%$ on average collision rate. In our real-world driving dataset, VDT-Auto showed promising performance on the unseen data in a zero-shot way.
\end{itemize}

\subsection{Related Work}

\input{Related}


