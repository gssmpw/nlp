\begin{table*}[]
\caption{The open-loop planning results of our VDT-Auto on nuScenes validation set.}
\centering
\resizebox{0.66\linewidth}{!}{
    \begin{tabular}{l|l|c|c|c|c|c|c|c|ccc}
        \toprule
        \multirow{2}{*}{No.} & \multirow{2}{*}{Methods} & \multicolumn{4}{c|}{L2 (m) $\downarrow$} & \multicolumn{4}{c}{Collision Rate (\%) $\downarrow$} \\
         & & 1s & 2s & 3s & Avg. & 1s & 2s & 3s & Avg. \\
        \midrule
        1 & FF~\cite{hu2021safeff} & 0.55 & 1.20 & 2.54 & 1.43 & 0.06 & 0.17 & 1.07 & 0.43 \\
        2 & EO~\cite{kh2022differentiable} & 0.67 & 1.36 & 2.78 & 1.60 & 0.04 & 0.09 & 0.88 & 0.33 \\
        3 & ST-P3~\cite{hu2022st-p3} & 1.33 & 2.11 & 2.90 & 2.11 & 0.23 & 0.62 & 1.27 & 0.71 \\
        4 & UniAD~\cite{hu2023uniAD} & 0.48 & 0.96 & 1.65 & 1.03 & 0.05 & 0.17 & 0.71 & 0.31 \\
        5 & GPT-Driver~\cite{mao2023gpt} & 0.27 & 0.74 & 1.52 & 0.84 & 0.07 & 0.15 & 1.10 & 0.44 \\
        6 & VLP-UniAD~\cite{pan2024vlp} & 0.36 & 0.68 & 1.19 & 0.74 & 0.03 & 0.12 & 0.32 & 0.16 \\
        7 & RDA-Driver~\cite{huang2024rda} & 0.23 & 0.73 & 1.54 & 0.80 & \textbf{0.00} & 0.13 & 0.83 & 0.32 \\
        8 & DriveVLM~\cite{tian2024drivevlm} & \textbf{0.18} & \textbf{0.34} & \textbf{0.68} & \textbf{0.40} & 0.10 & 0.22 & 0.45 & 0.27 \\
        9 & HE-Drive-B~\cite{wang2024hedrive} & 0.30 & 0.56 & 0.89 & 0.58 & \textbf{0.00} & \textbf{0.03} & \textbf{0.14} & \textbf{0.06} \\
        
        \midrule
        10 & \textbf{Ours} & 0.20 & 0.47 & 0.88 & 0.52 & 0.05 & 0.18 & 0.40 & 0.21 \\
     
    
        \midrule
       
    \end{tabular}
}
\label{tab:L2}
\vspace{-0.3cm}
\end{table*}

\subsection{Experimental Setup}

We conducted our training and open-loop experiments on the nuScenes dataset that consists of $1,000$ street scenes collected from Boston and Singapore, known for their dense traffic and challenging driving conditions \cite{caesar2020nuscenes}. In addition, we evaluated our pipeline on a real-world driving dataset in a zero-shot manner. The real-world driving dataset was recorded by our experimental car shown in Fig. \ref{fig:sdc} \cite{guo2024hawkdrive}. \par In our experiments, we first trained our BEV encoder and fine-tuned Qwen2-VL-7B on nuScenes to obtain the BEV features and the VLM responses from their inference. Then we cached the BEV features and VLM responses and constructed the training set for the training of our diffusion Transformers.

\subsection{Comparison with Other State-of-the-Art Methods on nuScenes}

During the evaluation, on the nuScenes validation set \cite{caesar2020nuscenes}, we compared our VDT-Auto with other methods by the L2 error in meters and the collision rate in percentage in Table \ref{tab:L2}. The average L2 error is determined by calculating the distance between each waypoint in the planned trajectory and the corresponding waypoint in the ground truth trajectory. This metric indicates how closely the planned trajectory aligns with a human-driven trajectory. The collision rate is assessed by positioning an ego-vehicle bounding box at each waypoint along the planned trajectory and subsequently checking for any intersections with the ground truth bounding boxes of other objects. Our VDT-Auto achieved state-of-the-art performance in open-loop planning tasks.

\subsection{Experiments on Real-world Driving Dataset}

In our real-world driving dataset, we demonstrate the potentials of our VDT-Auto on unseen data in a zero-shot way, where our BEV encoder is adjusted to obtain the extracted features from a single front image, and the fine-tuned VLM analyzes the front image to provide the contextual information. In Fig. \ref{fig:cover}, we show the VLM's path proposals from the continuous frames based on a consistent scenario in the left column (a), while the conditional sampled paths are shown in the right column (b).

\subsection{Ablation Study}

To verify the effectiveness of our design, in Table \ref{tab:ablation}, we show the results of the ablation experiments of our VDT-Auto with timestep embedding (TSE), cross-attention-based fusion of geometric and contextual embedding (CAF), contextual average pooling (CAP), and BEV feature compression (BFC) in nuScenes validation set. In TSE, we embed the noise scheduler timesteps for the preparation of a cross-attention-based fusion of geometric and contextual embedding, while CAP and BFC are the dimensionality reduction of the BEV features $G_t$ and contextual embeddings $S_t$ for the stability of training and inference.




