\begin{figure*}
    \centering
    \includegraphics[width=0.95\linewidth]{Figs/Overview.png}
    \caption{\textbf{Framework overview of VDT-Auto.} At each time step, the surrounding images are encoded by the BEV encoder to provide the geometric feature grids of the scenario. A front image from the surrounding images is analyzed by our fine-tuned VLM to provide the contextual information of the conditions. Based on the BEV feature grids and VLM output, we construct the conditional latents for our diffusion Transformers, where the BEV feature grids and VLM's detection and advice are embedded and VLM's path proposal is sampled for conditioning. In Section \Romannum{3}, we introduce our noise sampling approach in details. Finally, our diffusion Transformers denoise the VLM's path proposal, conditioning on the geometric feature grids of the scenario and the contextual information from our fine-tuned VLM.}
    \label{fig:overview}
\vspace{-0.3cm}
\end{figure*}

\subsection{BEV Encoder}

In Fig. \ref{fig:overview}, our BEV encoder is based on LSS \cite{philion2020lss, hu2021fiery}, where the surrounding camera images from the $T$ time steps are lifted into the BEV feature grids. $F^{k}_{t}\in\mathbb{R}^{(C_{f} + D_{d}) \times H \times W}$ represents the extracted features of the $k$-th camera at time $t$ from the image backbone, where $F^{k}_{t,C_{f}} \in \mathbb{R}^{C_{f} \times H \times W}$ is the contextual features and $F^{k}_{t,D_{d}} \in \mathbb{R}^{D_{d} \times H \times W}$ represents the estimated depth distribution. Then the contextual feature map in height dimension $F'^{k}_{t}$ is computed as $F^{k}_{t,C_{f}} \otimes F^{k}_{t,D_{d}}$. According to the nuScenes camera setup \cite{caesar2020nuscenes}, with the intrinsics and extrinsics of the cameras, $F'^{k}_{t}$ is then aggregated and weighted along the height dimension into the ego-centered coordinate system to obtain the BEV feature grids $G_{t} \in \mathbb{R}^{C_\text{state} \times H \times W}$ at time $t$, where $C_\text{state}$ is the number of state channels.

\subsection{VLM Module}

For our work, Qwen2-VL-7B is used to bridge the input of sensory data and the output of contextual conditions \cite{wang2024qwen2vl}. In Qwen2-VL, Multimodal Rotary Position Embedding (M-RoPE) is applied to process multimodal input by decomposing rotary embedding into temporal, height, and width components, which equips Qwen2-VL with powerful multimodal data handling capabilities. In our VDT-Auto, the supervised fine-tuning of Qwen2-VL-7B is carried out by feeding a front image of surrounding cameras and system prompts, expecting the output of the description of the detection, the structured advice of the behavior of the ego vehicle, and the proposal of a path. To achieve supervised fine-tuning, we constructed our fine-tuning dataset by extracting ground truth information from the nuScenes dataset \cite{caesar2020nuscenes}. In Section \Romannum{3}, we will introduce more details about our dataset construction and supervised fine-tuning.

\subsection{Diffusion Prerequisites}

In Fig. \ref{fig:overview}, we show our entire VDT-Auto pipeline, where the feature grids $G_{t} \in \mathbb{R}^{C_\text{state} \times H \times W}$ of the BEV encoder and the contextual output $S_t$ of Qwen2-VL-7B including the description of the detection and structured advice on the behavior of the ego vehicle are encoded as state conditions for the diffusion process. Thus, in our designed diffusion Transformers, the conditioned policy $\pi_{\theta}(A_t | G_t, S_t)$ predicts the denoised path $A_t = (a^0_t, a^1_t, \ldots,a^n_t)$ of length $n$, conditioned on both the current BEV features $G_t$ and contextual embeddings $S_t$ \cite{bao2023onemmdi, han2024emma, reuss2024mdt}. During training, the proposal of a path based on supervised fine-tuning of Qwen2-VL-7B at time $t$ pairing with current BEV features $G_t$ and contextual embeddings $S_t$ to form the training set, where our diffusion Transformers aim to maximize log-likelihood $\ell_{\text{training}}$ throughout the training set,

\vspace{-0.3cm}
\begin{equation}
    \ell_{\text{training}} = \underset{\theta}{\arg\max}{}_{(a_t^i, g_t^i, s_t^i) \in (A'_t, G_t, S_t)} \log {\pi_\theta}({a_t^i | g_t^i, s_t^i}),
\end{equation}
where $a_t^i, g_t^i, s_t^i$ are sampled from our constructed training set. We extract the noise distribution $\sigma_{\text{VLM}}$ from the path output of the supervised fine-tuned Qwen2-VL-7B to construct the noisy path dataset $A'_t$ by adding the sampled noise from the extracted noise distribution $\sigma_{\text{VLM}}$ to the ground truth path $A_{gt}$ of nuScenes.

\par In Section \Romannum{3}, we demonstrate that the noise distribution $\sigma_{\text{VLM}}$ of the path proposal from our fine-tuned Qwen2-VL-7B is treated as a normal distribution, where we examine the extracted noise using One-Sample Kolmogorov-Smirnov test for both the $x$ and $y$ coordinates of the paths \cite{Kstest}. 

\subsection{Loss Functions}

Our diffusion Transformers predict the denoised path $A_t$ conditioned on current BEV features $G_t$ and contextual embeddings $S_t$. Therefore, the loss function is defined as follows.

\vspace{-0.3cm}
\begin{equation}
\begin{split}
    \mathcal{L}_{\text{train}} = \mathcal{L}_{\text{MSE}} (\pi_{\theta}(A_t | g_t, s_t, \boldsymbol{\epsilon}), A_{gt}) + \\
    \mathcal{L}_{\text{MSE}}(\sum_{j=1}^{n} a^j, \sum_{j=1}^{n} a_{gt}^j),
\end{split}
\end{equation}
where $\pi_{\theta}$ is our trained diffusion Transformers. Under the conditions of encoded BEV features $g_t \in G_t$, contextual embeddings $s_t \in S_t$, and added noise $\boldsymbol{\epsilon} \in \sigma_{\text{VLM}}$, the first part of our loss function is the mean squared error between the path prediction $A_t$ and the ground truth path from nuScenes $A_{gt}$. Besides, the second part of our loss function is the mean squared error between the cumulative sum of the waypoints $a^j \in A_t$ and $a_{gt}^j \in A_{gt}$.

