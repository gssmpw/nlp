\section{Related Works}
\subsection{Image Segmentation}\label{sec:image_segmentation}

The SAM 2 architecture is based on the original Segment Anything Model ____ (SAM) for image segmentation. When given an input image and a prompt, the model predicts segmentation masks for the associated object, providing precise outlines that can be adjusted or refined based on user input or specific criteria. It operates by leveraging a large dataset of images and their corresponding segmentation masks, allowing it to learn a variety of object shapes and boundaries during an iterative process of annotation and refinement ____. SAM employs a transformer-based architecture, which enables it to process images in a way that captures both local and global context. SAM architecture uses a heavy-weight image encoder and a light-weight prompt attention mechanism to attend to regions of an image or natural language prompts. SAM returns a set of segmentation masks given the cross-attention of the prompts with the image embeddings, passed through a mask decoder (Figure \ref{fig:architecture}).

\subsection{Video Segmentation}

Until recently, state-of-the-art VOS models, including XMem ____ and Cutie ____, relied on the relationship between current and previous frames to predict the relationship between the masks, a process known as frame-to-frame affinity. The semi-supervised VOS structure relies on a single mask prompt and its previous predictions to make the full list of mask predictions. SAM 2 ____ introduced an architecture that is not dependent on affinity, fundamentally changing the way VOS is explored as a problem. Rather than using frame-to-frame affinity for mask prediction, SAM 2 solely utilizes attention between the frame embeddings and the memory. Masks, bounding boxes, and points can all be used as inputs at any point through a video, and prompt encodings are used to decode the mask prediction. SAM 2 applies SAM iteratively and includes the attention of previous frames as a self-prompting mechanism.\\

\noindent A simplified architecture diagram for SAM 2 is shown in Figure \ref{fig:architecture}. First, an input image is preprocessed and passed through an image encoder to find the image embeddings. These embeddings undergo two cross-attentional layers, first with the memory of previous mask/frame relations and second with prompt inputs, such as masks, bounding boxes, and point clicks. The mask predictions are decoded as a function of these cross-attentional layers. A 256-dimensional object pointer represents the current state of the object in relation to previous frames. The mask prediction and object pointer are encoded into memory and used for future predictions.

\subsection{Datasets}

Existing datasets explore the segmentation of partially or fully obscured objects. DAVIS ____ is a dataset of multi-object videos which have been segmented for ground truth masks. It is a highly popular benchmark on which models can compare performance. Although these data contain limited object obscurations, they provide a good baseline for VOS. The DAVIS dataset will be used to generate the data used in this paper.\\

\noindent MOSE ____ is a popular video dataset similar to DAVIS that emphasizes object obscuration. In many samples, obscured objects will become partially or fully obscured in a section of the video to test the robustness of the VOS model. State-of-the-art models, particularly Cutie and SAM 2, have emphasized this dataset when developing their architectures and have great performance on these obscurations. Our datasets contain synthetic obscurations that are unreliable when the clean data contains natural obscurations. For this reason, we use DAVIS to create a dataset of videos with the necessary annotated long-term interjections.