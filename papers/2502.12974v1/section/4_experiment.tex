\section{Experimental Methodology}
In this section, we describe the datasets, baselines, and implementation details for our experiments.

\input{table/overall}
\textbf{Datasets.} 
We train all \method{} models using the public portion of the E5 dataset~\cite{wang2023improving,springer2024repetition}, which comprises approximately 1.5M samples. 
The retrieval effectiveness of \method{} is evaluated on the BEIR benchmark~\cite{thakur2021beir}, which includes 18 datasets that span a variety of domains.
Our evaluation focuses on the 14 publicly available datasets used for the zero-shot retrieval task. 
Additional details about the training dataset and evaluation benchmark can be found in Appendix~\ref{app:dataset}.

\textbf{Evaluation Metrics.}
To evaluate the retrieval effectiveness of \method{}, we use nDCG@10, the standard metric for the BEIR benchmark. The metric implementation follows the \texttt{pytrec-eval} toolkit~\cite{van2018pytrec_eval}, which is consistent with prior work~\cite{zhu2023large}.

\textbf{Baselines.}
We compare \method{} with several baseline retrievers implemented with different language models. GTR~\cite{ni2021large} employs large dual encoder-only models to build a dense retriever, while SGPT~\cite{muennighoff2022sgpt} trains dense retrieval models using decoder-only architectures. Emb-V3\footnote{\url{https://cohere.com/blog/introducing-embed-v3}} is a commercial text retrieval model provided by Cohere. PromptReps~\cite{zhuang2024promptreps} directly prompts LLMs to generate dense representations without supervision. RepLLaMA~\cite{ma2024fine} and E5-Mistral~\cite{wang2024multilingual} fine-tune LLMs as dense retrievers, using the hidden state of an additional end-of-sequence token to represent the input context. Notably, E5-Mistral is trained on the same dataset as \method{} but leverages a larger foundational model.

\textbf{Implementation Details.}
We initialize the \method{} models with MiniCPM-2.4B and MiniCPM-4B~\cite{hu2024minicpm}. All \method{} models are trained for 1,000 steps using the AdamW optimizer with a batch size of 256. The learning rate follows a cosine decay schedule, with a warm-up phase covering the first 3\% of the total iterations, peaking at 2e-4. We train \method{} using hybrid negatives, including one hard negative from the E5 dataset and seven in-batch negatives. The CoD length for all \method{} models is set to 8. \method{} is implemented using the OpenMatch toolkit~\cite{yu2023openmatch}, with flash-attention~\cite{NEURIPS2022_67d57c32} and LoRA~\cite{hu2021lora} enabled to mitigate memory constraints and improve computational efficiency. %All \method{} models are trained on 4$\times$ NVIDIA A100-40 GB GPUs.

