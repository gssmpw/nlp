\section{Appendix}
\subsection{Licenses}
The E5 dataset and BEIR benchmark are released under the Apache License 2.0.
The terms of use can be found on their Github pages.
This license allows users to freely use, modify, and distribute the data, permitting academic use of their dataset.

\subsection{More Details of Datasets Used in Experiments}
In this section, we provide a detailed description of the training dataset and the evaluation benchmark for our \method{}.

\textbf{E5 Dataset}.
We employ the E5 dataset~\cite{wang2023improving, springer2024repetition} for training, comprising a collection of publicly available datasets. The E5 dataset is carefully curated to encompass a wide range of retrieval scenarios and tasks. We present the statistics of the E5 dataset in Table~\ref{tab:train_dataset_details}. The instructions used in the E5 dataset are shown in Table~\ref{tab:dataset_descriptions}. The full dataset can be available at their website\footnote{\url{https://github.com/jakespringer/echo-embeddings}}.

\textbf{BEIR Benchmark}.
We evaluate our \method{} models using the BEIR benchmark, which includes 18 datasets from 9 heterogeneous retrieval tasks. We focus on the 14 publicly available datasets and use the standard instructions in MTEB~\cite{muennighoff2022mteb}. The statistics for these datasets are provided in Table~\ref{tab:dataset_stats}, and the instructions can be found in Table~\ref{tab:retrieval_dataset}.

\input{table/Train_dataset}
\input{table/datasets}
\input{table/instruction}
\input{table/inference_instruction}

\label{app:dataset}


\subsection{Case Study}
\label{app:case_study}
\input{table/case}
%\input{figure/case_hot}
In this subsection, we present two case studies to demonstrate the effectiveness of \method{}. Specifically, we show the top 1 retrieved document comes from both MiniCPM and \method{} in the FiQA and Trec-COVID datasets. The retrieval cases are shown in Table~\ref{tab:case_study}.
%Then, we plot a detailed similarity relationship for one case to explore how embeddings at various stages affect the final representation used for retrieval.
%\textbf{Retrieval Cases.}

For the query ``In the US, is it a good idea to hire a tax consultant for doing taxes?'', while the retrieval result of MiniCPM suggests hiring a professional, it fails to mention the specific context of ``In the US''. 
In contrast, the document retrieved by \method{} aligns more closely with the query, addressing all the relevant aspects. This highlights that \method{} effectively activates the reasoning capabilities of LLMs, enabling a more fine-grained document representation.

For the query ``What is the mechanism of inflammatory response and pathogenesis of COVID-19 cases?'', MiniCPM retrieves documents containing more pathological terms, such as ``RAGE transactivation'' and ``the ACE/Ang II/ATR1 pathway''. 
However, these retrieved documents cannot accurately address the inflammatory response and pathogenesis of COVID-19. This suggests that MiniCPM lacks a fine-grained understanding of document content, leading to suboptimal retrieval performance. In contrast, \method{} provides accurate retrieval results, demonstrating the importance of deliberate thinking before searching.

%\textbf{Embedding Analysis.} 
%We show a heatmap of detailed similarity relationships for one case in Figure~\ref{fig:case_hot}. The similarity relationships between neighboring thinking steps indicating stepwise learning behavior of Chain-of-Deliberation. 


\subsection{Comparison with Additional Baseline Retrievers}
\label{app:comparison}
In this subsection, we conduct additional comparisons with baseline retrievers not discussed in the main section, including several advanced retrievers from the BEIR benchmark.

Specifically, we use CPT~\cite{neelakantan2022text}, Udever~\cite{zhang2023languagemodelsuniversalembedders}, ULLME, Ada2, and Promptriever as our additional retrievers to compare with \method{}.
CPT refers to a series of large encoder models developed by OpenAI \cite{brown2020language}, which are pre-trained on large-scale and unsupervised data. We use the CPT-text-175B model as our baseline model.   
Udever contrastively trains the BLOOM~\cite{workshop2023bloom176bparameteropenaccessmultilingual} to build dense retrievers, enabling it to effectively generate aligned multilingual embeddings for both text and code. 
ULLME~\cite{man-etal-2024-ullme} enables bidirectional attention between LLMs and enhances them for text embedding using Generative Reinforcement Learning (GRL).
Ada2 is a versatile text embedding model introduced by OpenAI. The evaluation result for Ada2 comes from~\citet{kamalloo-etal-2023-evaluating-embedding}. 
Promptriever~\cite{weller2025promptriever} is trained on an instance-level instruction dataset from MS MARCO, enhancing its ability to follow instructions for retrieval tasks.

The performance comparison results on the BEIR benchmark are shown in Table~\ref{tab:other_baseline}. 
Compared with these baseline models, \method{} still exhibits competitive performance, demonstrating its effectiveness. Compared to Udever and ULLME, \method{} shows significant improvement on FEVER and Climate-FEVER, indicating its effectiveness across diverse fact-checking scenarios. 
When compared to Promptriever, \method{} demonstrates similar retrieval performance. However, unlike Promptriever, \method{} does not rely on synthetic data for training its retrieval models.

\input{table/other_baseline}
