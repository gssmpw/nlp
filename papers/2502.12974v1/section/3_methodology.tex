\section{Methodology}
In this section, we introduce our Deliberate Thinking based Dense Retriever (\method{}).
We first introduce the preliminary of LLM-based dense retrieval~(Sec.~\ref{method:preliminary}). Then we describe our deliberation thinking based embedding learning method used by \method{}~(Sec.~\ref{method:method}).

\subsection{Preliminary of Dense Retrieval with Large Language Models as Foundations}
\label{method:preliminary}
Given a query $q$ and a document collection $\mathcal{D}$, the goal of the retrieval task is to identify a subset of documents most relevant to the query.

LLM-based dense retrievers typically map both the query $q$ and document $d$ into a shared latent space for retrieval, where the query embedding $h^q$ and document embedding $h^d$ are defined as:
\begin{equation}\small
\begin{aligned} 
\label{eq:preliminary}
h^q &= \text{LLM}( q, \text{\texttt{</s>}})[-1],\\
h^d &= \text{LLM}( d, \text{\texttt{</s>}})[-1].
\end{aligned} 
\end{equation}
The ranking score $f(q, d)$ between the query embedding $h^q$ and the document embedding $h^d$ is calculated as:
\begin{equation}\small
f(q, d) = sim(h^q, h^d),
\end{equation}
where \textit{sim} denotes the similarity function. In \method{}, we use cosine similarity to measure the similarity between queries and documents, which is also employed in previous works~\cite{wang2023improving, behnamghader2024llm2vec}. Subsequently, we train the LLMs contrastively to maximize the probability of retrieving the positive document $d^+$ over the negative document $d^-$:
\begin{equation}\small
\begin{aligned} 
\label{eq:prob} 
p(d^{+}|q, {d^{+}}\cup\mathcal{D}^{-}) = \frac{e^{f(q, d^+)}}{e^{f(q, d^+)} + \sum_{d^-\in \mathcal{D}^-} e^{f(q, d^-)}}, \end{aligned} 
\end{equation}
where $\mathcal{D}^{-}$ denotes the set of negative documents, typically obtained via in-batch negative sampling~\cite{karpukhin2020dense}. 

Current LLM-based dense retrievers typically use the last hidden state corresponding to the end-of-sequence token (\texttt{</s>}) as the dense representation. However, they do not fully exploit the reasoning capabilities of LLMs, which helps conduct more effective representations by learning information from diverse views of documents.

\subsection{Enhancing Dense Retriever through Deliberate Thinking}
\label{method:method} 
In this subsection, we introduce the Deliberate Thinking based Dense Retriever (\method{}), which aims to unleash the reasoning ability of LLMs and generate more fine-grained document representations. As shown in Figure~\ref{fig:main-architecture}, \method{} consists of two modules to enhance the LLM-based dense retriever: Chain-of-Deliberation (CoD) and Self Distillation (SD).

\textbf{Chain-of-Deliberation.}
To enhance these LLM-based dense retrievers, \method{} introduces the Chain-of-Deliberation (CoD) approach, which delays the computation of document embeddings by performing several steps of reasoning.

Specifically, CoD incorporates a sequence of prompt tokens $\{t_1, t_2, \dots, t_m \}$ to stimulate the reasoning capability of LLMs when representing the document $d$. These tokens $\{t_1, t_2, \dots, t_{m-1} \}$ serve as intermediate thinking steps, encouraging the model to think step-by-step before producing the final document embedding at the $m$-th step:
\begin{equation}\small
\label{eq:cod}
h_{m}^d = \text{LLM}( X,t_1,t_2, \dots, t_{m-1}, t_m),
\end{equation}
where $m$ is a hyperparameter to control the thinking depth. An appropriate choice of $m$ is crucial to avoid overthinking or under-optimization.


During training, we first calculate the similarity score between query representation $h^q$ and the document representation $h^d_i$ at the $i$-th thinking step:
\begin{equation}\small
f(q, d(t_i)) = sim (h^q, h^d_i).
\end{equation}
Next, we gather all similarity scores using the decoded document representations $\{h^d_1, ..., h^d_m\}$. We then select the most useful thinking step from CoD and use the corresponding embedding as the document representation to compute the training loss. The relevance scores $f_\text{max}(q, d)$ between the query and the document are computed as:
\begin{equation}\small 
f_\text{max}(q, d) = \max_{1 \leq i \leq m} sim(h^q, h^d_i),
\label{eq:relevance}
\end{equation} 
The LLM is optimized by minimizing the contrastive training loss:
\begin{equation}\small
\mathcal{L}_c=-\text{log}\frac{e^{f_\text{max}(q, {d^+})}}{e^{f_\text{max}(q, {d^+})}+ \sum_{d^-\in \mathcal{D}^-}e^{f_\text{max}(q, {d^-})}}.
\end{equation} 


\textbf{Self Distillation.}
Although the final token of the Chain-of-Deliberation aggregates information from all thinking steps through autoregressive decoding, it may overlook crucial reasoning cues presented in embeddings decoded at earlier steps.

To address this, we introduce Self Distillation (SD), a strategy for distilling knowledge from different thinking steps into the final document representation $h^d_m$. Specifically, we use the most informative thinking step as the teacher to guide the representation learning of the final token in CoD, thereby enhancing the document representation.

For the query $q$, we compute the ranking probability of the $i$-th document $d_i$ in the document collection $\Tilde{\mathcal{D}} = \{d^+\} \cup\mathcal{D}^{-}$ as: 
\begin{equation}\small
P(d_i | q) = \frac{e^{f_\text{max}(q, d_i)}}{\sum_{d_j \in \Tilde{\mathcal{D}}} e^{f_\text{max}(q, d_j)}},
\end{equation}
where $|\Tilde{\mathcal{D}}| = k$. This yields a probability distribution $P(\Tilde{\mathcal{D}}|q)$ over the $k$ documents: 
\begin{equation}\small
P(\Tilde{\mathcal{D}}|q) = \left[ P(d_1 | q), P(d_2 | q), \dots, P(d_k | q) \right].
\end{equation}
Each value $P(d_i | q)$ represents the ranking probability of the $i$-th document $d_i$ using the document representations from all thinking steps $\{h^d_1, ..., h^d_m\}$ of CoD that yield a higher similarity with the query. Concurrently, we compute the rank probability of $d_i$ using the last-token embedding $h^d_m$ from CoD:
\begin{equation}\small
Q(d_i(t_m) | q) = \frac{e^{f(q, d_i(t_m))}}{\sum_{d_j \in \{d^+\} \cup\mathcal{D}^{-}} e^{f(q, d_j(t_m))}}.
\end{equation}
Then we can obtain the ranking probability distribution $Q(\Tilde{\mathcal{D}}|q)$ as well:
\begin{equation}\small
Q(\Tilde{\mathcal{D}}|q) = \left[ Q(d_1 | q), Q(d_2 | q), \dots, Q(d_k | q) \right].
\end{equation}
We then minimize the Kullback-Leibler (KL) divergence between two probability distributions $P(\Tilde{\mathcal{D}}|q)$ and $Q(\Tilde{\mathcal{D}}|q)$:
\begin{equation}\small
\small
\mathcal{L}_{t}={P(\Tilde{\mathcal{D}}|q) \cdot \log  \frac{P(\Tilde{\mathcal{D}}|q)}{Q(\Tilde{\mathcal{D}}|q)}},
\end{equation}
where the Self Distillation loss $\mathcal{L}_{t}$ optimizes the document representation $h_m^d$ by capturing more crucial matching signals from all thinking steps.

\textbf{Training.}
Finally, we train our \method{} models by minimizing the following loss $\mathcal{L}$:
\begin{equation}\small
\mathcal{L} = \mathcal{L}_c + \mathcal{L}_t,
\end{equation}
where $\mathcal{L}_c$ optimizes the CoD, and $\mathcal{L}_t$ is used to distill crucial information from the thinking steps into the final dense representation of the document. This combined loss allows \method{} to leverage both thinking depth and self-knowledge distillation to improve retrieval performance.
