\section{Introduction}
Dense retrieval models encode both queries and documents into a dense embedding space and measure their similarity to retrieve relevant documents~\cite{karpukhin2020dense, zhao2024dense, xiong2020approximate}, demonstrating strong effectiveness in various downstream NLP tasks, such as open-domain question answering~\cite{chen2020open}, fact verification~\cite{liu2019fine}, and web search~\cite{chen2024ms}. However, recent findings have shown that dense retrievers suffer from significant performance degradation when applied to new tasks or domains~\cite{su2022one}, raising concerns about their versatility~\cite{luo2024large, khramtsova2024leveraging}.

\input{figure/intro}
Large Language Models (LLMs), such as ChatGPT~\cite{achiam2023gpt} and LLaMA~\cite{touvron2023llama}, have demonstrated extraordinary emergent capabilities~\cite{wei2022emergent, zhao2023survey}, inspiring researchers to leverage them to enhance the task and domain generalization of dense retrievers~\cite{zhu2023large, khramtsova2024leveraging}. In particular, existing work has focused on prompting LLMs to generate dense representations for retrieval~\cite{zhuang2024promptreps}. These methods typically use task-specific instructions or in-context demonstrations to guide LLMs in generating task- and domain-aware embeddings. To learn more tailored representations for dense retrieval, researchers further focus on optimizing LLM-based retrievers using relevance labels~\cite{ma2024fine, neelakantan2022text, li2025making}. These methods exploit the superior reasoning abilities of LLMs, achieving impressive performance across various retrieval tasks~\cite{wang2023improving, zhu2023large, luo2024large}. Recent studies suggest that LLMs pose strong reasoning capability, particularly implemented by their step-by-step thinking~\cite{kudo2024think,wei2022chain}. LLM-based retrievers typically rely on the hidden state of the end-of-sequence token as both query and document representations. Nevertheless, only relying on one embedding usually shows less effectiveness in representing documents from different views that can match queries~\cite{zhang2022multi,khattab2020colbert}. 

In this paper, we propose a \textbf{D}\textbf{e}li\textbf{b}er\textbf{a}te \textbf{T}hinking based Dens\textbf{e} \textbf{R}etriever (\method{}) model to learn more effective document representations through deliberately thinking step-by-step before retrieval. As shown in Figure~\ref{fig:intro}, our method stimulates LLMs to conduct the reasoning process, enabling them to generate more fine-grained document representations for retrieval. Specifically, \method{} introduces the Chain-of-Deliberation mechanism to encourage LLMs to conduct deliberate thinking by autograssively decoding the document representations. Then \method{} utilizes the Self Distillation mechanisms to gather all information from previous steps and compress them into the document embedding at the last step.

Our experiments show that \method{} achieves comparable or even better retrieval performance than the baseline methods implemented by larger-scale LLMs, highlighting its effectiveness. Our further analyses show that both Chain-of-Deliberation and Self Distillation play important roles in \method{} and appropriately increasing the thinking steps can benefit these LLM-based dense retrieval models. Thriving on autograssively decoding different document representations during thinking, the document representations can be gradually refined to be more effective. By incorporating our Self Distillation, LLMs show the ability to capture different key information at different thinking steps and gather all crucial semantics from different steps to the final document representations. 

