\section{Evaluation Results}
In this section, we first evaluate the retrieval effectiveness of
\method{} and then conduct ablation studies to show the roles of different modules in \method{}. Then we analyze the characteristics of learned embeddings during thinking step by step.


\input{table/ablation}

\subsection{Overall Performance}
The overall performance of \method{} and the baseline retrievers is shown in Table~\ref{tab:overall}. Additional experimental results comparing \method{} with other baseline retrievers are provided in Appendix~\ref{app:comparison}.

Overall, \method{} outperforms all baseline retrievers in terms of average retrieval accuracy on BEIR, achieving more than a 2\% improvement. This highlights its effectiveness in enhancing the representation capability of LLMs for retrieval. Compared to the prompt-based method PromptReps, these fine-tuned LLM-based methods consistently show improvements, indicating that LLMs also benefit from supervised training to learn more tailored embeddings for retrieval.
When compared to E5-Mistral-7B, which is trained on the same E5 corpus as \method{}, \method{} significantly improves retrieval performance on Trec-COVID, NQ, and FEVER, demonstrating its capability across diverse question-answering scenarios.
Notably, when implemented with MiniCPM-2.4B, \method{} achieves retrieval performance comparable to that of larger 7B-scale LLM-based dense retrievers while utilizing only 35\% of the parameters. This demonstrates that \method{} can enhance the representation learning capabilities of smaller-scale LLMs, rather than relying on larger foundational LLMs. 
Furthermore, when implemented with MiniCPM-4B,  the retrieval performance of \method{} is improved by 1\%,  demonstrating that larger models effectively enhance the retrieval capabilities of \method{}.


\subsection{Ablation Study}

As shown in Table~\ref{tab:ablation}, we conduct ablation studies to further investigate the roles of Chain-of-Deliberation (CoD) and Self Distillation (SD) modules in \method{}.

We compare our \method{} with three variations, using MiniCPM-2.4B and MiniCPM-4B as the foundations for building dense retrievers. Both vanilla LLM and MiniCPM w/ CoD models represent documents using the hidden state of the last token and train query and document representations using contrastive training. The key difference between them lies in that MiniCPM w/ CoD performs additional CoD steps before obtaining the document representation. Besides, MiniCPM w/ SD is identical to \method{} but removes the CoD steps when generating the document representation.

Compared to vanilla LLM, MiniCPM w/ SD shows almost identical retrieval performance, indicating that relying solely on a few last tokens in the input sequence does not effectively enhance the document representations. This suggests that the special tokens used in CoD serve as prompts that stimulate LLMs to produce more meaningful embeddings.
On the other hand, MiniCPM w/ CoD still yields a limited improvement over the vanilla LLM, demonstrating that directly incorporating CoD in representing documents fails to enhance the representation ability of LLMs. After incorporating the Self Distillation mechanism, MiniCPM w/ CoD achieves further improvements, demonstrating its importance in capturing semantics from the different deliberative steps of CoD to optimize the last token as the document representation.
Additionally, when using contrastive training to optimize LLMs, the 4B-scale retrieval model performs worse than the 2.4B-scale model. Notably, \method{} not only mitigates this performance degradation but also leads to an additional 1.3\% improvement, highlighting the effectiveness and robustness of \method{}.


\subsection{Effectiveness of Chain-of-Deliberation with Different Thinking Depths}
\label{5_3_cod}
In this subsection, we explore how thinking depth affects the effectiveness of \method{}. Specifically, we vary the length of the Chain-of-Deliberation (CoD) to train several \method{}-2.4B and \method{}-4B models and evaluate their retrieval performance on TREC-COVID and FiQA. 

\input{figure/different_tokens}
As illustrated in Figure~\ref{fig:different_tokens}, both \method{}-2.4B and \method{}-4B exhibit significant and consistent improvements in retrieval performance as the thinking depth increases to 4. 
This indicates that an appropriate thinking depth effectively activates the reasoning capabilities of LLM-based retrievers, enabling them to generate finer-grained representations of documents. 
When the thinking depth is further extended to 8, \method{}-2.4B reaches a plateau, indicating that it may be nearing its capacity to process more complex or prolonged deliberations.
In contrast, \method{}-4B continues to show incremental improvements when the length of CoD extends to 8, indicating that larger models benefit more from extended reasoning due to their stronger ability to integrate and retain detailed intermediate steps. 
Nonetheless, further increasing the CoD beyond a certain point (e.g., 12) may lead to overthinking and result in performance degradation for both model sizes. These observations demonstrate that while moderate depths effectively boost retrieval accuracy, excessively long chains can dilute the benefits and introduce unnecessary computational overhead. Overall, these findings underscore the importance of carefully tuning the thinking depth for LLM-based retrievers.

\input{figure/think_step}
\subsection{Retrieval Performance of CoD-Generated Document Representations} \label{5_4_CoD}

In this subsection, we investigate how the Chain-of-Deliberation (CoD) enhances the representation capability of LLM-based retrievers. Specifically, we evaluate the quality of embeddings produced at different thinking steps in CoD and assess their retrieval performance individually.

As shown in Figure~\ref{fig:think_step}, during the early stages of reasoning (e.g., steps 1 and 2), retrieval performance is relatively low and even decreases. This suggests that initial embeddings, based on minimal deliberation, may lack the nuanced understanding required for effective retrieval. However, as the number of thinking steps increases, performance generally improves, indicating that more deliberation leads to more refined embeddings for retrieval tasks. These improvements demonstrate that \method{} helps LLMs better capture relevant information from the tokens of documents and previous decoded representations in the thinking steps.
On the other hand, \method{}'s performance gradually reaches a plateau as the thinking steps increase, suggesting that after a certain point, the embeddings become sufficiently fine-grained, and further deliberation may result in limited improvements and unnecessary computational complexity.


\subsection{Characteristics of the Embeddings Generated by \method{}} 
In this subsection, we analyze the embeddings learned by \method{} from CoD. Specifically, we compute the average cosine similarity scores of embeddings generated at different positions in FiQA to understand how embeddings at various stages affect the final representation used for retrieval.

 \input{figure/hot}

\textbf{Learning Patterns of CoD.} As shown in Figure~\ref{fig:hot_fig}, we present the average similarity scores among the first five embeddings generated by \method{} to explore how \method{} refines document representations step by step during CoD.

The results reveal a clear pattern in the similarity relationships: each embedding is most similar to its immediate neighbors, with similarity gradually decreasing as the distance between embeddings increases. This indicates that each embedding heavily relies on the previously decoded representations to generate more refined embeddings, which likely results from the autoregressive decoding mechanism of LLMs. Comparing this with the \method{} w/o SD model (Figure~\ref{fig:hot_fig_wosdd}), we observe that the \method{} model shows higher similarity scores with representations from more recent steps during CoD. This suggests that our Self Distillation method effectively encourages LLMs to learn more diverse representations at different thinking steps and to gather more relevant information from nearby steps, which leads to finer-grained document representations. %Additional experimental results on the similarity scores of all embeddings at different thinking steps can be found in Appendix~\ref{app:case_study}.

\textbf{Contributions of CoD Steps to Document Representations.} Figure~\ref{fig:embed_similarity} illustrates the similarity relationship between embeddings at intermediate thinking steps of CoD and the final document representation generated at the last thinking step. This helps us explore the contributions of different thinking steps to the final document representations.

In general, both \method{} w/o SD and \method{} models exhibit a trend of gradually increasing similarity to the final embedding as the thinking steps progress. As shown in Figure~\ref{fig:sim_emb7_wosdd}, the \method{} w/o SD model tends to produce similar similarity scores with the final step, showing that relying solely on CoD may degrade the performance of \method{}. It may lie in that all CoD generated embeddings are supervised with the same training loss and optimized to match the same query, making them become homogeneous. In contrast, \method{} (Figure~\ref{fig:sim_emb7}) shows a more significant increase in similarity, indicating that these thinking steps contribute more variably to the final document representation. Notably, the information generated at each CoD step is gradually compressed into the last embedding, which further demonstrates the effectiveness of \method{} in leveraging the thinking capacity of LLMs to generate more effective document representations for retrieval. 