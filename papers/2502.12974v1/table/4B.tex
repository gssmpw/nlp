\begin{table}[ht]
    \centering
    % Adjust the table width to fit the page
    %\captionsetup{justification=justified} % Set the justification for the caption
    \scriptsize
    \label{tab:performance_comparison}
    \scalebox{1.24}{
    \begin{tabular}{lcc}
        \toprule
        \cmidrule(lr){2-6}
        \textbf{Method ($\rightarrow$)} & MiniCPM-\method{} &MiniCPM-\method{}\\
        \textbf{Model Size ($\rightarrow$)} & 2.4B & 4B\\
        \midrule
        TREC-COVID & 0.795&0.836 \\
        NFCorpus & 0.378&0.399 \\
        NQ & 0.560 &  \\
        HotpotQA &  0.678& \\
        FiQA-2018 & 0.434&0.462 \\
        ArguAna &  0.567&0.562 \\
        Touch√©-2020 & 0.210&0.250 \\
        Quora &   0.886& 0.886  \\
        DBPedia  &  0.430 &\\
        SCIDOCS &   0.197&0.212 \\
        FEVER & 0.859& \\
        Climate-FEVER & 0.303& \\
        SciFact &  0.735&0.743 \\
        \midrule
        \textbf{Average} & 0.541& \\
        \bottomrule
\end{tabular}
}
    
\vspace{1ex}
\caption{Retrieval performances on BEIR datasets. The scores of benchmarks are from their papers, and other scores are from us. $^{\dagger}$indicaate statistically significant improvement over Vanilla model.}
\end{table}