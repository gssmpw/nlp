\begin{table}[ht]
    \centering
    % Adjust the table width to fit the page
    %\captionsetup{justification=justified} % Set the justification for the caption
    \scriptsize
    \label{tab:performance_comparison}
    \scalebox{1.24}{
    \begin{tabular}{lcc}
        \toprule
        \textbf{Training($\rightarrow$)}& \multicolumn{2}{c}{Supervised}\\
        \cmidrule(lr){2-6}
        \textbf{Method ($\rightarrow$)} & MiniCPM-Vinalla &MiniCPM-ours\\
        \textbf{Model Size ($\rightarrow$)} & 2.4B & 2.4B\\
        \midrule
        TREC-COVID & 0.728 & 0.795\\
        NFCorpus &0.368 & 0.378\\
        NQ & 0.545 & 0.560 \\
        HotpotQA & 0.670 & 0.678\\
        FiQA-2018 & 0.406& 0.434\\
        ArguAna & 0.561 & 0.567\\
        Touch√©-2020 & 0.202& 0.210\\
        Quora & 0.880  & 0.886 \\
        DBPedia  & 0.414 & 0.430\\
        SCIDOCS & 0.191  & 0.197\\
        FEVER & 0.837& 0.859\\
        Climate-FEVER & 0.277& 0.303\\
        SciFact & 0.715 & 0.735\\
        CQADupStack&&
        \midrule
        \textbf{Average} & 0.523& 0.541$^{\dagger}$\\
        \bottomrule
\end{tabular}
}
    
\vspace{1ex}
\caption{Retrieval performances on BEIR datasets. The scores of benchmarks are from their papers, and other scores are from us. $^{\dagger}$indicaate statistically significant improvement over Vanilla model.}
\end{table}