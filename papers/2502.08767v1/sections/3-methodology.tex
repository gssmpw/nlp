\section{\se}\label{sec:met}

\se is an inference-time context augmentation framework to enhance generative LM's capability to rely on relevant information present in the context in order to reduce hallucinations and provide factually correct grounded response. \se requires \textbf{no additional training}, is \textbf{efficient at inference time} and provides \textbf{significant performance improvement} across models and tasks.

\subsection{Self-guided Contextual Evidence Eliciting}
\label{sec:met-elicit}

\begin{figure*}[t]
\vspace{-5pt}
    \centering
    \includegraphics[width=\linewidth]{figs/layer_att_large.pdf}
    \caption[evidence reading layers]{
        Relative attention\footnotemark[3] to the evidence/non-evidence sections (y-axis) across the layers (x-axis) for different LM families on HotpotQA.
        Deeper layers pay much greater attention to crucial evidence (green lines) in the context, even when LM responds incorrectly (dashed lines).
        Best viewed in color.
    }
    \label{fig:layeratt}
\vspace{-5pt}
\end{figure*}

The first step in \se is to leverage the LM's internal representations to identify the relevant evidence sentences in the context. Consider the context-based QA setup described in Sec.~\ref{sec:bac}. Denote the number of tokens in input $\tau(\vc, \vq)$ by $n$ and the number of sentences\footnote{The sentences in a document can be extracted with many natural language processing packages, in this paper we use spaCy~\cite{honnibal2017spacy}.} in the context $\vc$ by $m$. Define $\gS: \{\vs_1, \vs_2, \cdots, \vs_m\}$ where $\vs_i$ denotes $i$-th sentence. Let $(t_{\vs_i}^\text{start}, t_{\vs_i}^\text{end})$ denote the start and end token index of $i$-th sentence. Define sentence-level attention vector $\bva^{(\ell)} \in \R^m$ from token level attention $\va^{(\ell)}$ as follows
% \vspace{-5pt}
\begin{equation}
\label{eq:att-sent}
    \begin{aligned}
    &\bva^{(\ell)} := [\bar{a}_1^{(\ell)}, \bar{a}_2^{(\ell)}, \cdots, \bar{a}_m^{(\ell)}] \in \R^m, \\
    \text{where} \quad & \bar{a}_i^{(\ell)} = \frac{1}{|t_{\vs_i}^\text{end} - t_{\vs_i}^\text{start} + 1|}\sum\limits_{j=t_{\vs_i}^\text{start}}^{t_{\vs_i}^\text{end}} \va^{(\ell)}_j.    
    \end{aligned}
\end{equation}
Intuitively $\bar{a}_i^{(\ell)}$ tells the relative importance of each context sentence $\vs_i$ at layer $\ell$. In Fig.~\ref{fig:layeratt} we plot how $\bar{a}_i^{(\ell)}$ varies across layers for evidence vs non-evidence sentences in the context when generating the first response token. We can clearly see that, regardless of whether the model responds correctly or not, the deeper layers of the LM pay significantly higher attention to the relevant evidence\footnote{In Appendix~\ref{sec:apd-res-attlayer} we explain how we obtain the ground truth for relevant evidence in the context.} in the context. This observation holds across models families and datasets, see Appendix~\ref{sec:apd-res-attlayer}.

We leverage this insight to define the \textbf{sentence evidence scores} $e_i$ for each sentence $\vs_i$ in $\gS$ , which we use at the inference time to predict the evidence containing sentences in the context, as follows: We use a pre-selected subset of layers, denoted by $\gL_\text{ER}$ and referred to as \textbf{evidence-reading} layers, and aggregate $\bar{a}_i^{(\ell)}$ to get $e_i$, i.e., 
\begin{equation}
% \vspace{-5pt}
\label{eq:evd-score}
    \begin{aligned}
    &\ve := [e_1, e_2, \cdots, e_m] \in \R^m, \\
    \text{where} \quad & e_i =  \frac{1}{|\gL_{ER}|} \sum\limits_{\ell \in \gL_{ER}} \bar{a}_i^{(\ell)}
    \end{aligned}
\end{equation}
In Section~\ref{sec:exp-analysis} (RQ4) we study different choices of $\gL_\text{ER}$ for their ability to identify the relevant evidence. We find that using the last 50\% of the layers as $\gL_\text{ER}$ performs consistently well across models and datasets so we use it as default for experiments.

\footnotetext[3]{
Relative attention to the evidence/non-evidence section is defined as the ratio of section-average attention per token (APT) to the context-average APT, e.g., a 600\% evidence relative attention means that LM is paying 6x attention to each token in evidence sentences compared to context average.
}

Next, we introduce an evidence thresholding parameter $\alpha \in [0, 1]$ and use it in combination $e_i$ to predict evidence sentences $\gS_\text{SE}$
\begin{equation}
\label{eq:threshold}
    \gS_\text{SE} = \{\vs_i | \vs_i \in \gS; e_i \geq \alpha \cdot \text{max}(\ve)\},
\end{equation}
A higher value of $\alpha$ will result in fewer sentences being highlighted, and vice versa. It can be seen as a trade-off parameter between precision and recall in evidence elicitation, which can be tuned based on task requirements. \se is generally robust to $\alpha$, see Section~\ref{sec:exp-analysis} (RQ5), with $\alpha=0.5$ yielding significant performance improvement over baseline across models and tasks. Therefore, We use $\alpha=0.5$ as our default setting for experiments. 

We note that similar analyses can be presented at a token level to identify evidence tokens instead of sentences. However, as discussed in Appendix~\ref{sec:apd-res-token}, augmenting the original context by highlighting complete sentences is semantically more meaningful and beneficial than individual tokens.
\subsection{Contextual Evidence Highlighting}
\label{sec:met-highlight}
The second step of \se is to modify the original context to highlight the evidence sentences identified in Sec.~\ref{sec:met-elicit} and to modify the original instructions in Sec.~\ref{sec:bac} to guide the model to use the highlighted evidence. Prioritizing simplicity and efficiency, we introduce a prompt augmentation strategy to achieve this. Specifically for highlighting, in the raw context passage $\vc$ we place text markers "<start\_important>" and "<end\_important>" before and after each sentence in $\gS_\text{SE}$, resulting in the new context $\vc^*$. Furthermore, we update the task instructions to guide the LM's attention towards the highlighted sentences. The response is generated using the mark-highlighted $\vc^*$ and the updated prompt template $\tau_\texttt{SEQA}$.
\begin{tcolorbox}[title={\footnotesize \se Prompt Template $\tau_\texttt{SEQA}$},top=1mm,bottom=1mm]
\scriptsize
\{Original QA Instructions\}
Within the context, <start\_important> and <end\_important> are used to mark the important evidence sentences, read carefully. Do not include the markers in the output.\\
Context: \{\texttt{elicited\_context}\}\\
Question: \{\texttt{question}\}
\end{tcolorbox}
\se is summarized in Algorithm~\ref{alg:se}.
Note that \se is computationally efficient as it only generates one additional token (i.e., line 1 in Alg.~\ref{alg:se}) for identifying $\gS_\text{SE}$ during inference.

\begin{algorithm}[H]
\caption{\se}
\label{alg:se}
\renewcommand{\algorithmicrequire}{\textbf{Input}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\begin{algorithmic}[1]
    \REQUIRE{Language model $\Phi$, question $\vq$, context $\vc$, prompt templates $\tau_\text{QA}$ and $\tau_\text{SEQA}$, evidence-reading layers $\gL_\text{ER}$, eliciting threshold $\alpha$.}
    \STATE Generate one token with $\Phi(\tau_\text{QA}(\vc, \vq))$;
    \STATE Compute $\bva^{(\ell)}$ for all $l \in \gL_\text{ER}$; \hfill $\rhd$ \eqref{eq:att-sent}
    \STATE Get evidence score $\ve$ with $\gL_\text{ER}$; \hfill $\rhd$ \eqref{eq:evd-score}
    \STATE Select evidence sentences $\gS_\text{SE}$; \hfill $\rhd$ \eqref{eq:threshold}
    \STATE Derive new context $\vc^*$ by highlighting $\gS_\text{SE}$;
    \STATE Generate answer $\vg_\text{SE} \gets \Phi(\tau_\text{SEQA}(\vc^*, \vq))$
    \ENSURE {The final answer $\vg_\text{SE}$}
\end{algorithmic}
\end{algorithm}
\vspace{-10pt}




