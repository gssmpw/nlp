\section{Preliminaries}\label{sec:bac}
% \vspace{-5pt}
\paragraph{Problem description.}

Given an LM $\Phi$, question $\vq$, context $\vc$ and QA prompt template $\tau_\text{QA}$, we obtain the generated answer $\vg$ for the question by combining context and question as input: $\vg \gets \Phi(\tau_\text{QA}(\vc, \vq))$.
We use the following template as baseline to test different LMs' base ability in leveraging contextual evidence for QA tasks:
\begin{tcolorbox}[title={\footnotesize A direct prompt template $\tau_\texttt{QA}$ for context-based QA},top=1mm,bottom=1mm]
\scriptsize
Directly answer the question based on the context passage, no explanation is needed. If the context does not contain any evidence, output ``I cannot answer based on the given context."\\
Context: \{\texttt{context}\}\\
Question: \{\texttt{question}\}
\end{tcolorbox}

\paragraph{Notation.}
We only outline the notation and details of Transformer models needed for this work, please see \cite{transformers2017} for more detailed exposition on transformer architecture. Given an input sequence with $n$ tokens, a decoder only transformer LM generates next token by adaptively attending to all previous n tokens. Denote the attention probability vector for attention head $h$ and layer $\ell$ by $\va^{(\ell,h)}\in \R^n$. Then we define
\begin{equation}
\label{eq:att-layer}
\va^{(\ell)} = \frac{1}{H}\sum\limits_{h=1}^{H} \va^{(\ell, h)}
\end{equation}
where $H$ is the total number of heads in a layer and $L$ is the number of layers in the transformer model. Intuitively, $\va^{(\ell)}$ provides a holistic view of layer $\ell$'s attention distribution over the input sequence, summarizing which tokens the model deems most informative at this stage of processing.