\section{Experiments}\label{sec:exp}
We conduct comprehensive experiments across six LMs from different families with varying sizes and on four single- and multi-hop reasoning open-book QA tasks from various domains to investigate:
\vspace{-2mm}
\begin{itemize}[leftmargin=*]
    \setlength{\itemsep}{-3pt}
    \item \textbf{RQ1:} How does \se perform in terms of improving answer quality and factuality?
    \item \textbf{RQ2:} How do the evidence scores and the context sentences highlighted by \se correlate with relevant evidence?
    \item \textbf{RQ3:} Robustness to noise in the context?
    \item \textbf{RQ4\&5:} How do different choices of evidence-reading layers \& threshold $\alpha$ affect \se?
\end{itemize}
\vspace{-2mm}


\begin{table*}[t]
% \small
\centering
\caption{
    Results of applying different evidence-eliciting methods to 6 LMs across 4 context-based QA tasks. 
    We report EM and Token F1 scores (in $\times10^{-2}$) with the gains over direct QA.
    The "average" columns present the average QA performance and the inference time (per sample) across all datasets.
    The best results are \textbf{bolded}.
}
\label{tab:main}
\vspace{-5pt}
\resizebox{\textwidth}{!}{%
\begin{tabular}{ccl|cccccccc|ccc}
\hline
\multicolumn{2}{c}{\multirow{3}{*}{\textbf{Model}}} & \multicolumn{1}{c|}{\multirow{3}{*}{\textbf{Method}}} & \multicolumn{8}{c|}{\textbf{Dataset}} & \multicolumn{3}{c}{\textbf{Average}} \\ \cline{4-14} 
\multicolumn{2}{c}{} & \multicolumn{1}{c|}{} & \multicolumn{2}{c|}{\textbf{HotpotQA}} & \multicolumn{2}{c|}{\textbf{NewsQA}} & \multicolumn{2}{c|}{\textbf{TQA}} & \multicolumn{2}{c|}{\textbf{NQ}} & \multicolumn{2}{c|}{\textbf{Ranking}} & \textbf{Inference} \\
\multicolumn{2}{c}{} & \multicolumn{1}{c|}{} & \textbf{EM} & \multicolumn{1}{c|}{\textbf{Token F1}} & \textbf{EM} & \multicolumn{1}{c|}{\textbf{Token F1}} & \textbf{EM} & \multicolumn{1}{c|}{\textbf{Token F1}} & \textbf{EM} & \textbf{Token F1} & \textbf{EM} & \multicolumn{1}{l|}{\textbf{Token F1}} & \textbf{Time (ms)} \\ \hline
\multirow{10}{*}{\textbf{\rotatebox{90}{Llama-3.1}}} & \multicolumn{1}{c|}{\multirow{5}{*}{\textbf{\rotatebox{90}{8B}}}} & \textsc{Base} & 58.9 & \multicolumn{1}{c|}{57.7} & 64.3 & \multicolumn{1}{c|}{56.2} & 72.8 & \multicolumn{1}{c|}{66.1} & 59.7 & 61.6 & 4.75 & \multicolumn{1}{c|}{4.38} & 224.1 \\
 & \multicolumn{1}{c|}{} & \textsc{Cot} & 60.4 & \multicolumn{1}{c|}{58.6} & 64.9 & \multicolumn{1}{c|}{55.8} & 74.4 & \multicolumn{1}{c|}{67.4} & 59.6 & 62.1 & 4.00 & \multicolumn{1}{c|}{3.75} & 224.8 \\
 & \multicolumn{1}{c|}{} & \textsc{FullElicit} & 60.7 & \multicolumn{1}{c|}{59.0} & 65.9 & \multicolumn{1}{c|}{56.6} & 72.8 & \multicolumn{1}{c|}{66.3} & 61.1 & 62.5 & 3.25 & \multicolumn{1}{c|}{3.12} & 226.3 \\
 & \multicolumn{1}{c|}{} & \textsc{PromptElicit} & 66.3 & \multicolumn{1}{c|}{66.3} & 62.8 & \multicolumn{1}{c|}{56.7} & 76.0 & \multicolumn{1}{c|}{69.6} & 61.8 & 65.6 & 2.00 & \multicolumn{1}{c|}{2.75} & 1672.0 \\
 & \multicolumn{1}{c|}{} & \cem{\bf\se} & \cem\bs{68.5} & \multicolumn{1}{c|}{\cem\bs{69.5}} & \cem\bs{66.9} & \multicolumn{1}{c|}{\cem\bs{60.8}} & \cem\bs{79.4} & \multicolumn{1}{c|}{\cem\bs{72.7}} & \cem\bs{64.0} & \cem\bs{67.8} & \cem\bs{1.00} & \multicolumn{1}{c|}{\cem\bs{1.00}} & \cem264.1 \\ \cline{2-14} 
 & \multicolumn{1}{c|}{\multirow{5}{*}{\textbf{\rotatebox{90}{70B}}}} & \textsc{Base} & 71.8 & \multicolumn{1}{c|}{74.2} & 66.7 & \multicolumn{1}{c|}{57.4} & 78.0 & \multicolumn{1}{c|}{71.2} & 59.3 & 63.2 & 3.00 & \multicolumn{1}{c|}{3.25} & 1389.8 \\
 & \multicolumn{1}{c|}{} & \textsc{Cot} & 72.4 & \multicolumn{1}{c|}{74.0} & 67.2 & \multicolumn{1}{c|}{56.4} & 77.0 & \multicolumn{1}{c|}{69.9} & 60.3 & 63.1 & 4.50 & \multicolumn{1}{c|}{3.12} & 1394.2 \\
 & \multicolumn{1}{c|}{} & \textsc{FullElicit} & 71.3 & \multicolumn{1}{c|}{73.8} & 66.2 & \multicolumn{1}{c|}{56.8} & 77.5 & \multicolumn{1}{c|}{70.7} & 58.2 & 61.8 & 4.50 & \multicolumn{1}{c|}{4.25} & 1408.1 \\
 & \multicolumn{1}{c|}{} & \textsc{PromptElicit} & 73.4 & \multicolumn{1}{c|}{76.2} & 63.1 & \multicolumn{1}{c|}{58.2} & 77.0 & \multicolumn{1}{c|}{72.3} & 64.0 & 68.5 & 2.00 & \multicolumn{1}{c|}{3.38} & 8124.0 \\
 & \multicolumn{1}{c|}{} & \cem{\bf\se} & \cem\bs{75.9} & \multicolumn{1}{c|}{\cem\bs{79.0}} & \cem\bs{69.2} & \multicolumn{1}{c|}{\cem\bs{62.1}} & \cem\bs{80.0} & \multicolumn{1}{c|}{\cem\bs{74.4}} & \cem\bs{65.4} & \cem\bs{68.7} & \cem\bs{1.00} & \multicolumn{1}{c|}{\cem\bs{1.00}} & \cem1566.9 \\ \hline
\multirow{10}{*}{\textbf{\rotatebox{90}{Mistral}}} & \multicolumn{1}{c|}{\multirow{5}{*}{\textbf{\rotatebox{90}{7B}}}} & \textsc{Base} & 70.4 & \multicolumn{1}{c|}{45.6} & 61.4 & \multicolumn{1}{c|}{32.6} & 81.8 & \multicolumn{1}{c|}{47.9} & 65.7 & 29.9 & 4.25 & \multicolumn{1}{c|}{3.75} & 538.4 \\
 & \multicolumn{1}{c|}{} & \textsc{Cot} & 71.4 & \multicolumn{1}{c|}{44.0} & 60.4 & \multicolumn{1}{c|}{31.4} & 82.2 & \multicolumn{1}{c|}{48.2} & \bs{67.5} & 29.3 & 4.75 & \multicolumn{1}{c|}{2.25} & 560.8 \\
 & \multicolumn{1}{c|}{} & \textsc{FullElicit} & 70.6 & \multicolumn{1}{c|}{47.6} & \bs{62.0} & \multicolumn{1}{c|}{34.0} & 81.3 & \multicolumn{1}{c|}{51.5} & 66.8 & 30.5 & 3.00 & \multicolumn{1}{c|}{3.00} & 541.4 \\
 & \multicolumn{1}{c|}{} & \textsc{PromptElicit} & 71.0 & \multicolumn{1}{c|}{\bs{64.1}} & 57.7 & \multicolumn{1}{c|}{\bs{42.2}} & 81.9 & \multicolumn{1}{c|}{\bs{62.3}} & 65.6 & 42.8 & \bs{1.25} & \multicolumn{1}{c|}{4.00} & 1877.8 \\
 & \multicolumn{1}{c|}{} & \cem{\bf\se} & \cem\bs{74.3} & \multicolumn{1}{c|}{\cem61.6} & \cem60.6 & \multicolumn{1}{c|}{\cem41.7} & \cem\bs{83.6} & \multicolumn{1}{c|}{\cem61.3} & \cem66.4 & \cem\bs{43.4} & \cem1.75 & \multicolumn{1}{c|}{\cem\bs{2.00}} & \cem431.3 \\ \cline{2-14} 
 & \multicolumn{1}{c|}{\multirow{5}{*}{\textbf{\rotatebox{90}{12B}}}} & \textsc{Base} & 59.2 & \multicolumn{1}{c|}{65.9} & 51.9 & \multicolumn{1}{c|}{51.6} & 72.9 & \multicolumn{1}{c|}{68.7} & 54.8 & 61.6 & 4.75 & \multicolumn{1}{c|}{5.00} & 281.9 \\
 & \multicolumn{1}{c|}{} & \textsc{Cot} & 62.0 & \multicolumn{1}{c|}{69.0} & 53.0 & \multicolumn{1}{c|}{52.6} & 75.6 & \multicolumn{1}{c|}{71.1} & 55.3 & 62.0 & 3.25 & \multicolumn{1}{c|}{3.25} & 284.6 \\
 & \multicolumn{1}{c|}{} & \textsc{FullElicit} & 59.8 & \multicolumn{1}{c|}{65.8} & 53.1 & \multicolumn{1}{c|}{51.9} & 73.7 & \multicolumn{1}{c|}{68.8} & \bs{55.5} & 62.6 & 4.00 & \multicolumn{1}{c|}{2.88} & 283.9 \\
 & \multicolumn{1}{c|}{} & \textsc{PromptElicit} & 62.8 & \multicolumn{1}{c|}{\bs{73.1}} & 52.2 & \multicolumn{1}{c|}{56.6} & 79.9 & \multicolumn{1}{c|}{77.6} & \bs{55.5} & 65.7 & 1.75 & \multicolumn{1}{c|}{2.38} & 1455.0 \\
 & \multicolumn{1}{c|}{} & \cem{\bf\se} & \cem\bs{63.6} & \multicolumn{1}{c|}{\cem72.9} & \cem\bs{54.9} & \multicolumn{1}{c|}{\cem\bs{58.6}} & \cem\bs{82.6} & \multicolumn{1}{c|}{\cem\bs{79.9}} & \cem55.3 & \cem\bs{66.0} & \cem\bs{1.25} & \multicolumn{1}{c|}{\cem\bs{1.50}} & \cem339.1 \\ \hline
\multirow{10}{*}{\textbf{\rotatebox{90}{Qwen2.5}}} & \multicolumn{1}{c|}{\multirow{5}{*}{\textbf{\rotatebox{90}{7B}}}} & \textsc{Base} & 65.2 & \multicolumn{1}{c|}{65.8} & 58.3 & \multicolumn{1}{c|}{45.4} & 77.4 & \multicolumn{1}{c|}{66.1} & 62.2 & 59.9 & 3.75 & \multicolumn{1}{c|}{3.75} & 245.2 \\
 & \multicolumn{1}{c|}{} & \textsc{Cot} & \bs{70.7} & \multicolumn{1}{c|}{37.9} & 59.2 & \multicolumn{1}{c|}{31.9} & \bs{78.6} & \multicolumn{1}{c|}{41.6} & 63.8 & 32.3 & 5.00 & \multicolumn{1}{c|}{2.00} & 421.5 \\
 & \multicolumn{1}{c|}{} & \textsc{FullElicit} & 65.5 & \multicolumn{1}{c|}{65.7} & 57.5 & \multicolumn{1}{c|}{48.2} & 77.1 & \multicolumn{1}{c|}{67.3} & 64.4 & 60.6 & 2.75 & \multicolumn{1}{c|}{3.50} & 249.6 \\
 & \multicolumn{1}{c|}{} & \textsc{PromptElicit} & 64.7 & \multicolumn{1}{c|}{67.7} & 54.9 & \multicolumn{1}{c|}{46.3} & 75.4 & \multicolumn{1}{c|}{66.8} & 64.5 & \bs{65.0} & 2.25 & \multicolumn{1}{c|}{4.25} & 1165.1 \\
 & \multicolumn{1}{c|}{} & \cem{\bf\se} & \cem69.1 & \multicolumn{1}{c|}{\cem\bs{71.4}} & \cem\bs{59.6} & \multicolumn{1}{c|}{\cem\bs{50.8}} & \cem78.1 & \multicolumn{1}{c|}{\cem\bs{67.8}} & \cem\bs{65.0} & \cem64.7 & \cem\bs{1.25} & \multicolumn{1}{c|}{\cem\bs{1.50}} & \cem289.4 \\ \cline{2-14} 
 & \multicolumn{1}{c|}{\multirow{5}{*}{\textbf{\rotatebox{90}{32B}}}} & \textsc{Base} & 71.8 & \multicolumn{1}{c|}{68.4} & 60.0 & \multicolumn{1}{c|}{44.7} & 79.0 & \multicolumn{1}{c|}{69.3} & 62.7 & 59.3 & 3.25 & \multicolumn{1}{c|}{3.12} & 928.2 \\
 & \multicolumn{1}{c|}{} & \textsc{Cot} & 71.3 & \multicolumn{1}{c|}{67.1} & 60.0 & \multicolumn{1}{c|}{43.5} & 79.5 & \multicolumn{1}{c|}{66.8} & 59.5 & 55.3 & 5.00 & \multicolumn{1}{c|}{3.62} & 998.6 \\
 & \multicolumn{1}{c|}{} & \textsc{FullElicit} & 71.3 & \multicolumn{1}{c|}{68.2} & 61.6 & \multicolumn{1}{c|}{45.7} & 78.5 & \multicolumn{1}{c|}{68.8} & 63.2 & 58.1 & 3.75 & \multicolumn{1}{c|}{3.25} & 936.3 \\
 & \multicolumn{1}{c|}{} & \textsc{PromptElicit} & 71.3 & \multicolumn{1}{c|}{74.5} & 59.0 & \multicolumn{1}{c|}{51.5} & 78.0 & \multicolumn{1}{c|}{69.9} & 64.8 & 68.1 & 2.00 & \multicolumn{1}{c|}{4.00} & 5109.8 \\
 & \multicolumn{1}{c|}{} & \cem{\bf\se} & \cem\bs{73.3} & \multicolumn{1}{c|}{\cem\bs{75.0}} & \cem\bs{65.6} & \multicolumn{1}{c|}{\cem\bs{57.3}} & \cem\bs{82.1} & \multicolumn{1}{c|}{\cem\bs{74.8}} & \cem\bs{66.8} & \cem\bs{69.8} & \cem\bs{1.00} & \multicolumn{1}{c|}{\cem\bs{1.00}} & \cem980.7 \\ \hline
\end{tabular}%
}
\vspace{-10pt}
\end{table*}



\begin{table*}[t]
\scriptsize
\centering
\caption{
Examples of how \se helps LM, with \textev{blue} text highlighting the evidence selected by \se. Due to space limitation we only display a small portion of the full context, please see Appendix~\ref{sec:apd-res-exa} for full context. 
}
\label{tab:example}
\vspace{-5pt}
\resizebox{\textwidth}{!}{%
\begin{tabular}{p{0.01\textwidth} p{0.75\textwidth} p{0.22\textwidth}}
\toprule
{\bf\small} & {\bf\small (Partial) Context Passage} & {\bf\small Question \& Answers}\\
\midrule
\multirow{7}{*}{\rotatebox{90}{\bf True or False}}
& 
\uline{\textbf{(Displaying 123 of 794 Words)}}
\textev{Tiger Please is an Indie / Alternative five-piece band from Cardiff, Wales. The band formed in August 2008.} The band's influences are U2, Sigur Rós, Kings of Leon, John Mayer and Counting Crows. They signed with Walnut Tree Records in 2009 and released their debut EP "They Don't Change Under Moonlight". "Kerrang!" magazine, "Rock Sound" magazine, and "Classic Rock" magazine praised the EP and featured the band on the "Rock Sound" and "Classic Rock" cover-mount albums. ... ... \textev{Black Rebel Motorcycle Club (often abbreviated as BRMC) is an American rock band from San Francisco, California.} The group consists of Peter Hayes (vocal, guitar, harmonica), Robert Levon Been (vocal, bass, guitar), and Leah Shapiro (drums). Former drummer Nick Jago left the band in 2008 ... ...
& 
{\scriptsize
    \textbf{Question:} Are the bands Tiger Please and Black Rebel Motorcycle Club from the same country?\vspace{1mm}
    
    \textbf{True Answer:} No. \vspace{1mm}
    
    \uline{\textbf{\texttt{Base:}}}
    Yes. \xmark \vspace{1mm}
    
    \uline{\textbf{\texttt{+\se:}}}
    No. \cmark \vspace{1mm}
}
\\ \midrule
\multirow{8}{*}{\rotatebox{90}{\bf Comparison}}
& 
\uline{\textbf{(Displaying 129 of 227 Words)}}
\textev{Home Monthly was a monthly women's magazine published in Pittsburgh, Pennsylvania in the late 19th century.} "The Strategy of the Were-Wolf Dog" is a short story by Willa Cather. \textev{It was first published in "Home Monthly" in December 1896.} The Count of Crow's Nest is a short story by Willa Cather. It was first published in "Home Monthly" in October 1896. \textev{Mirabella was a women's magazine published from June 1989 to April 2000.} It was created by and named for Grace Mirabella, a former "Vogue" editor in chief, in partnership with Rupert Murdoch. "Nanette: An Aside" is a short story by Willa Cather. It was first published in "Courier" on 31 July 1897 and one month later in "Home Monthly". "The Prodigies" is a short story by Willa Cathe ... ...
& 
{\scriptsize
    \textbf{Question:} Which women's magazine was published first, Mirabella or Home Monthly?\vspace{1mm}
    
    \textbf{True Answer:} Home Monthly. \vspace{1mm}
    
    \uline{\textbf{\texttt{Base:}}}
    Mirabella. \xmark \vspace{1mm}
    
    \uline{\textbf{\texttt{+\se:}}}
    Home Monthly. \cmark \vspace{1mm}
}
\\ \midrule
\multirow{8}{*}{\rotatebox{90}{\bf Fact Retrieval}}
& 
\uline{\textbf{(Displaying 153 of 1014 Words)}}
... ... Lars Lunde (born 21 March 1964) is a Danish former professional football player, who played in the striker position. Lunde got his breakthrough with Brøndby IF in 1983, and he made his debut for the Denmark national football team in October 1983. He was sold to Young Boys Bern in Switzerland, before moving to German club Bayern Munich in 1986. \textev{He was a part of the Bayern team which won the German Bundesliga championship in 1987, and he came on as a late substitute when Bayern lost the 1987 European Cup Final to FC Porto.} He played the last of his three matches for the Danish national team in April 1987, before leaving Bayern during the 1987–88 season. He went on to play for a number of smaller clubs, ending his career with FC Baden in Switzerland. ... ...
& 
{\scriptsize
    \textbf{Question:} Which team did Lars Lunde play for when defeated for the 1987 European Cup Final?\vspace{1mm}
    
    \textbf{True Answer:} Bayern Munich \vspace{1mm}
    
    \uline{\textbf{\texttt{BASE:}}}
    FC Porto. \xmark \vspace{1mm}
    
    \uline{\textbf{\texttt{+\se:}}}
    Bayern Munich. \cmark \vspace{1mm}
}
\\\midrule
\multirow{8}{*}{\rotatebox{90}{\bf Multi-hop Reasoning}}
& 
\uline{\textbf{(Displaying 146 of 532 Words)}}
... ... This sent Olympic down to play in the Premier League in 2007. Adelaide City won the title with games to spare after being runaway leaders, finishing the season unbeaten. \textev{Norwood is a suburb of Adelaide, about 4 km east of the Adelaide city centre.} The suburb is in the City of Norwood Payneham \& St Peters, the oldest South Australian local government municipality, with a city population over 34,000. Whyalla railway station was the terminus station of the Whyalla line serving the South Australian city of Whyalla. \textev{Walter Frank Giffen (20 September 1861 in Norwood – 28 June 1949 in Adelaide) was an Australian cricketer who played in 3 Tests between 1887 and 1892.} He was the brother of the great all-rounder George Giffen. The City of Burnside is a local government area with an estimated population of 44,300 people in the South Australian city of Adelaide. ... ... 
& 
{\scriptsize
    \textbf{Question:} Walter Giffen is from a suburb of which South Australian city?\vspace{1mm}
    
    \textbf{True Answer:} Adelaide\vspace{1mm}
    
    \uline{\textbf{\texttt{Base:}}}
    Norwood. \xmark \vspace{1mm}
    
    \uline{\textbf{\texttt{+\se:}}}
    Adelaide. \cmark \vspace{1mm}
}
\\
\bottomrule
\end{tabular}
}
\vspace{-5pt}
\end{table*}


\subsection{Experimental Setup}
\vspace{-0.5em}

\paragraph{Datasets and Metrics.}
We test 4 datasets: HotpotQA~\cite{yang2018hotpotqa} and the MRQA version \cite{fisch2019mrqa} of NewsQA~\cite{trischler2017newsqa}, TriviaQA (TQA)~\cite{joshi2017triviaqa}, and Natural Questions (NQ)~\cite{kwiatkowski2019natural}.
These datasets feature context passages from diverse sources (e.g., web/Wikipedia/news reports), requiring the model to reason over a single or multiple pieces of evidence within the context. 
This provides a comprehensive test of \se in real-world applications.
For all datasets, we use the official validation split on HuggingFace for testing.
We apply greedy decoding to get deterministic answers. 
Exact Match (EM) and Token-level F1 scores are used for QA performance evaluation.

\paragraph{Models and Baselines.}
We test six open-source instruction fine-tuned models: Llama-3.1 (8B, 70B)~\cite{dubey2024llama}, Mistral (7B, 12B)~\cite{jiang2023mistral}, and Qwen2.5 (7B, 32B)~\cite{yang2024qwen}.
We compare \se to the following prompting/evidence-eliciting approaches:
\begin{itemize}
    \item \co~\cite{wei2022chain}: Chain-of-thought prompting encourages the model to reason through intermediate steps before reaching a final answer. 
    While CoT promotes step-by-step reasoning based on the evidence in the context, it does not explicitly highlight important information within the context. 
    Therefore, we use it as a natural baseline for validating the advantage of explicit evidence elicitation in \se.
    We implement it by adding the \co prompt "Think step by step to provide the answer." at the end of the instruction.
    \item \fe: A naive approach that highlights the entire context as important. Comparing with it demonstrates the necessity of fine-grained, sentence-level evidence elicitation.
    \item \pe: This method leverages the LM itself for generative evidence extraction. 
    It involves two steps: first, the LM is prompted to select the most relevant evidence sentences from the context passage that can help answer the question. 
    Then, we highlight the extracted evidence and get new context for QA following Section~\ref{sec:met-highlight}. 
    Note that this method involves generative evidence extraction in iterative prompting, which requires the model to generate a large amount of additional tokens.
    This serves as a strong baseline for evaluating whether the quality of evidence selected by \se can match that of evidence extracted through generative approach. We use the following prompt for generative evidence extraction step:
\end{itemize}
\vspace{-5pt}
\begin{tcolorbox}[title={\footnotesize \pe Prompt Template $\tau_\texttt{PE}$},top=1mm,bottom=1mm]
\scriptsize
Please find the supporting evidence sentences from the context for the question, then copy-paste the original text to output. Template for output: `- [sentence1] - [sentence2] ...'\\
Context: \{\texttt{context}\}\\
Question: \{\texttt{question}\}
\end{tcolorbox}
Since our focus is on analyzing and improving the inherent ability of the generative LM to provide factually grounded responses based on relevant information provided in the context, we do not compare to any dataset specific and task specific fine-tuning methods. For a fair comparison, we do not tune the hyper-parameters for \se. They are fixed as $\gL_\text{ER}$ being the last 50\% layers and $\alpha=0.5$.
% \paragraph{Implementation Details.}
% Use of PyTorch, HuggingFace transformers. Experiment run on NVIDIA A100 GPUs.

\subsection{Main Results}\label{sec:exp-main}
\vspace{-0.5em}

\paragraph{RQ1: \se consistently improves grounded factuality.}
Table~\ref{tab:main} compares \se to the other methods. We observe that
\begin{itemize}
    \item As expected \fe does not provide meaningful and consistent improvement since it doesn't elicit fine-grained evidence. 
    \item \co also does not provide a meaningful and consistent gain which highlights the fact that asking the model to reason carefully does not improve its ability to leverage the relevant evidence while generating the response. \co results also show variation across LM families where we don't observe meaningful gains for Llama and Mistral models. For Qwen models \co generates longer answers with intermediate steps. 
    While this results in a slight increase in EM scores, it also leads to a large drop (up to 27.9) in F1 scores due to the inclusion of redundant information, as well as longer inference times (e.g., 171.9\% more inference time for Qwen2.5-7B)
    \item \se significantly and consistently improves the performance across all datasets and models of different sizes (5.0\%-11.7\% gain over baseline). Even when compared to computationally expensive (average inference time increase of 878\%/939\% for Llama3.1-8B/70B) iterative prompting approach \pe, \se outperforms for 40 out of 48 model-task metric pairs while incurring a fraction of the computational cost increment (only $\sim$3-5\% increase when compared to \pe).  
    

\end{itemize}
Appendix~\ref{sec:apd-res-eff} provides a more detailed discussion of computational efficiency of the approaches.


\paragraph{RQ2: \se highlights relevant evidence.}
We demonstrate both qualitatively and quantitatively how the context highlighted by \se is the relevant evidence for the task which leads to the performance gains reported in Table~\ref{tab:main}.

For qualitative illustration, in Table~\ref{tab:example} we show various types of examples (e.g., true/false, comparison, fact retrieval, multi-hop reasoning) from the HotpotQA dataset. \se identifies the most relevant supporting facts across different QA tasks. For instance, in the 2$^\text{nd}$ "comparison" example that asks which magazine was published first, \se highlights the information stating the publication dates of the magazines and related articles. In the 4$^\text{th}$ "multi-hop reasoning" example, it emphasizes the overlooked 2$^\text{nd}$-hop evidence “Norwood is a suburb of Adelaide”, which helps the model arrive at the correct answer “Adelaide”.

\begin{figure*}[t]
  \subfigure[
    Performance of base and \se-augmented LM with/without distracting context information, evaluated by Exact Match and Token F1 score.
  ]{
  \label{fig:noise-qa}
    \centering
    \includegraphics[width=0.58\linewidth]{figs/noise-hl-2.pdf}
  }
\hfill
  \subfigure[
    Context elicitation ratio of \se with/without distracting context information.
  ]{
  \label{fig:noise-elicit-ratio}
    \centering
    \includegraphics[width=0.35\linewidth]{figs/noise-hr.pdf}
  }
  \vspace*{-5pt}
  \caption{
    \se demonstrates robust advantage even in the presence of noisy context (Fig.~\ref{fig:noise-qa}).
    When the context passages contain more distracting information, \se tends to select a significantly smaller portion of text as evidence (Fig.~\ref{fig:noise-elicit-ratio}) to prevent the LM from being distracted by irrelevant contexts.
  }
  \label{fig:noise}
\end{figure*}


\begin{table}[h]
\footnotesize
\centering
\caption{\se accurately identifies contextual evidence sentences across different datasets and models.}
\label{tab:elicit}
\vspace{-5pt}
\resizebox{0.8\columnwidth}{!}{%
\begin{tabular}{@{}cccc@{}}
\toprule
\multicolumn{1}{c|}{\multirow{2}{*}{\textbf{Dataset}}} & \multicolumn{3}{c}{\textbf{Model}} \\
\multicolumn{1}{c|}{} & \textbf{Llama3.1} & \textbf{Mistral} & \textbf{Qwen2.5} \\ \midrule
\multicolumn{4}{c}{\textbf{Metric: Sentence-level AUROC}} \\ \midrule
\multicolumn{1}{c|}{HotpotQA} & 91.24 & 85.35 & 88.21 \\
\multicolumn{1}{c|}{NewsQA} & 92.68 & 88.68 & 91.54 \\
\multicolumn{1}{c|}{TQA} & 73.27 & 68.89 & 70.59 \\
\multicolumn{1}{c|}{NQ} & 90.87 & 85.51 & 87.43 \\ \midrule
\multicolumn{4}{c}{\textbf{Metric: Sentence-level NDCG}} \\ \midrule
\multicolumn{1}{c|}{HotpotQA} & 91.36 & 82.45 & 87.05 \\
\multicolumn{1}{c|}{NewsQA} & 82.79 & 70.65 & 82.37 \\
\multicolumn{1}{c|}{TQA} & 66.41 & 63.32 & 67.19 \\
\multicolumn{1}{c|}{NQ} & 91.45 & 86.45 & 87.65 \\ \bottomrule
\end{tabular}%
}
\vspace{-10pt}
\end{table}

For quantitative evaluation, we assess the accuracy of evidence elicitation by checking whether \se assigns higher evidence scores to \textit{ground-truth} evidence sentences. 
Specifically, for HotpotQA, we use the "supporting\_facts" annotations to derive ground-truth evidence labels, while for other datasets, a sentence is treated as ground truth evidence if it contains at least one of the correct answers.
Since \se computes \textit{continuous} evidence scores for each sentence (see \eqref{eq:evd-score}), we utilize AUROC and NDCG@all to assess the score for accurately classifying/ranking contextual evidence.
Table~\ref{tab:elicit} shows that SE can accurately locate evidence sentences across models and datasets, with 80-95 AUROC or NDCG scores in most cases. We note that results on the TQA dataset are relatively lower because TQA often has multiple candidate correct answers and thus a larger set of evidence. During inference, the model typically focuses on evidence for only one of the answers, which reduces the evidence score of other (candidate) evidence sentences. However, this does not hinder \se's ability to help LM respond better, as demonstrated by the results in Table~\ref{tab:main}.

\subsection{Additional Analysis}\label{sec:exp-analysis}
\vspace{-0.5em}

\paragraph{RQ3: \se is effective in presence of context noise.}
To explore the impact of real world noise on \se we study it's performance for the "distractor" variant of HotpotQA dataset containing additional distracting information, retrieved from Wikipedia~\cite{yang2018hotpotqa}, in the context. The distractor setting increases the average length of the context by 1443\% by introducing irrelevant information. Figure~\ref{fig:noise-qa} shows that \se maintains the advantage over the baseline even in the presence of substantial context noise. 

However, similar to the baseline, the performance of \se for "distractor" variant worsens meaningfully when compared to \se performance for the gold context setup, also shown in Figure~\ref{fig:noise-qa}. To investigate this we do a deep dive into the evidence elicitation capability of \se in the presence of substantial noise. Figure~\ref{fig:noise-elicit-ratio} contrasts the elicit ratio (i.e., the proportion of elicited evidence within the original context) under gold and distractor settings. With gold context, \se tends to select larger proportion as evidence since most contextual information provided is relevant. By contrast, in the distractor setting where most of the context is irrelevant, the proportion of evidence selected decreases substantially (typically <10\%) for same $\alpha=0.5$. Furthermore the proportion of elicited evidence coming from the additional distractor information is only TK\% even though the distractor information constitutes a major portion of the context. Therefore, even though \se highlights the relevant evidence without being distracted by noise, there is still potential to improve how the LM can utilize this highlighted evidence to maintain similar performance as in the case of gold context, a topic for future research. 

\begin{table}[h]
\centering
\caption{Effect of different evidence-reading (ER) layer choices on elicit accuracy and QA performance.}
\label{tab:layer-choice}
\vspace{-5pt}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{@{}c|cc|cc@{}}
\toprule
\multirow{2}{*}{\textbf{ER Layer Span}} & \multicolumn{2}{c|}{\textbf{Elicit Accuracy}} & \multicolumn{2}{c}{\textbf{QA Performance}} \\
 & \textbf{AUROC} & \textbf{NDCG} & \textbf{EM} & \textbf{Token F1} \\ \midrule
0\%-100\% & 89.02 & 75.50 & 62.57 & 63.33 \\ \midrule
0\%-50\% & 70.38 & 44.99 & 62.14 & 62.65 \\
50\%-100\% & \sbs{91.55} & \bs{80.11} & \bs{64.86} & \bs{65.23} \\ \midrule
0\%-25\% & 59.01 & 37.55 & 61.86 & 61.83 \\
25\%-50\% & 74.82 & 48.80 & 62.57 & 62.73 \\
50\%-75\% & \bs{91.66} & \sbs{79.96} & \sbs{63.57} & \sbs{64.14} \\
75\%-100\% & 91.02 & 78.72 & 63.43 & 64.10 \\ \bottomrule
\end{tabular}%
}
\end{table}

\paragraph{RQ4: Deeper Layers are better for $\gL_\text{ER}$.}
We already saw in Fig.~\ref{fig:layeratt} that the deeper layers of all the model families exhibit higher attention scores and a clear ability to distinguish relevant evidence within the context.  In Table~\ref{tab:layer-choice} we verify this further empirically by comparing the evidence elicit accuracy and QA performance of seven different choices of $\gL_\text{ER}$ for Llama3.1-8B evaluated on HotpotQA with $\alpha=0.5$. We see that, consistent with the observation in Fig.~\ref{fig:layeratt}, choosing last 50\% of the layers as $\gL_\text{ER}$ leads to (close to) best metrics in terms of both evidence elicitation and task performance. Based on the qualitative observation in Fig.~\ref{fig:layeratt}, this choice also looks robust across model families which is then further verified by the results in Table~\ref{tab:main} where we see a consistent significant improvement across all model families and tasks for this choice of $\gL_\text{ER}$. While there may be other choices of $\gL_\text{ER}$ which lead to better performance gains for a specific model-dataset pair, finding them requires model and task specific hyper-parameter tuning unlike universally applicable default setting.
\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{figs/analysis_threshold_gain.pdf}
    \vspace{-15pt}
    \caption{
        Impact of elicit threshold $\alpha$ (x-axis) on the QA performance gain (blue bars, left y-axis) and evidence elicit ratio (orange lines, right y-axis) of \se on four QA tasks.
        Best viewed in color.
    }
    \label{fig:threshold}
\end{figure*}

\paragraph{RQ5: Balancing elicitation precision and comprehensiveness in choice of $\alpha$.}
The choice of $\alpha$ in \se acts as a proxy for trade-off between evidence elicitation precision vs coverage. For example, for $\alpha=1$ we only select the sentence with highest $e_i$ whereas for $\alpha=0$ we have the whole context selected, analogue to \fe. In Figure~\ref{fig:threshold} we show the impact of $\alpha$ on the Token F1 score and evidence elicitation ratio for Llama3.1-8B on four QA datasets. We observe
\begin{itemize}
    \item For all datasets the performance quickly rises and the evidence elicitation ratio quickly drops with increasing $\alpha$ for smaller values of $\alpha$ indicating clearly that evidence elicitation upto a threshold helps all datasets. 
    \item Beyond $\alpha=0.5$ there is relatively minor variation in the performance with change in $\alpha$, illustrating robustness of \se to choice of $\alpha \in [0.5, 1]$ across datasets. The minor variation is dependent on the nature of the dataset. For datasets that require multi-hop reasoning, relying on multiple pieces of relevant evidence, we achieve optimal performance for $\alpha$ closer to $0.5$ whereas for NQ requiring simpler reasoning we achieve the best performance for $\alpha=1$. 
\end{itemize}
To strike a balanced trade-off between evidence elicitation precision and coverage, we choose $\alpha=0.5$ as default for experiments which leads to consistent gains across models and datasets (Table~\ref{tab:main}).