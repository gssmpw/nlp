

\begin{table*}[t]
\centering
\caption{Detailed runtime and efficiency results. we report the average inference time and number of generated tokens for each sample (i.e., context-question pair) for each dataset.}
\label{tab:efficiency}
\vspace{-10pt}
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}ccllllllllllc@{}}
\toprule
\multicolumn{2}{c}{\multirow{3}{*}{\textbf{Model}}} & \multicolumn{1}{c}{\multirow{3}{*}{\textbf{Method}}} & \multicolumn{8}{c}{\textbf{Dataset}} & \multicolumn{2}{c}{\textbf{Average}} \\ \cmidrule(l){4-13} 
\multicolumn{2}{c}{} & \multicolumn{1}{c}{} & \multicolumn{2}{c|}{\textbf{HotpotQA}} & \multicolumn{2}{c|}{\textbf{NewsQA}} & \multicolumn{2}{c|}{\textbf{TQA}} & \multicolumn{2}{c}{\textbf{NQ}} & \multicolumn{1}{c}{\textbf{Inference}} & \textbf{\#Generated} \\
\multicolumn{2}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{\textbf{Time (ms)}} & \multicolumn{1}{c|}{\textbf{\#Tokens}} & \multicolumn{1}{c}{\textbf{Time (ms)}} & \multicolumn{1}{c|}{\textbf{\#Tokens}} & \multicolumn{1}{c}{\textbf{Time (ms)}} & \multicolumn{1}{c|}{\textbf{\#Tokens}} & \multicolumn{1}{c}{\textbf{Time (ms)}} & \multicolumn{1}{c}{\textbf{\#Tokens}} & \multicolumn{1}{c}{\textbf{Time/sample (ms)}} & \textbf{Tokens} \\ \midrule
\multirow{10}{*}{\textbf{\rotatebox{90}{Llama-3.1}}} & \multicolumn{1}{c|}{\multirow{5}{*}{\textbf{\rotatebox{90}{8B}}}} & \multicolumn{1}{l|}{\textsc{Base}} & 161.46 & \multicolumn{1}{l|}{6.68} & 290.76 & \multicolumn{1}{l|}{11.79} & 230.18 & \multicolumn{1}{l|}{6.72} & 213.18 & \multicolumn{1}{l|}{10.42} & \multicolumn{1}{l|}{223.90\di{+0.0\%}} & 8.90\di{+0.0\%} \\
 & \multicolumn{1}{c|}{} & \multicolumn{1}{l|}{\textsc{Cot}} & 163.14 & \multicolumn{1}{l|}{6.52} & 290.86 & \multicolumn{1}{l|}{11.77} & 230.21 & \multicolumn{1}{l|}{6.76} & 213.85 & \multicolumn{1}{l|}{10.36} & \multicolumn{1}{l|}{224.51\di{+0.3\%}} & 8.85\di{-0.6\%} \\
 & \multicolumn{1}{c|}{} & \multicolumn{1}{l|}{\textsc{FullElicit}} & 163.55 & \multicolumn{1}{l|}{6.49} & 290.79 & \multicolumn{1}{l|}{11.83} & 236.30 & \multicolumn{1}{l|}{6.97} & 213.64 & \multicolumn{1}{l|}{10.47} & \multicolumn{1}{l|}{226.07\di{+1.0\%}} & 8.94\di{+0.4\%} \\
 & \multicolumn{1}{c|}{} & \multicolumn{1}{l|}{\textsc{PromptElicit}} & 1419.08 & \multicolumn{1}{l|}{75.78} & 1718.02 & \multicolumn{1}{l|}{85.18} & 2412.91 & \multicolumn{1}{l|}{116.81} & 1117.64 & \multicolumn{1}{l|}{61.40} & \multicolumn{1}{l|}{1666.91\di{+644.5\%}} & 84.79\di{+852.5\%} \\
 & \multicolumn{1}{c|}{} & \multicolumn{1}{l|}{\cem{\bf\se}} & \cem204.65 & \multicolumn{1}{l|}{\cem6.98} & \cem318.74 & \multicolumn{1}{l|}{\cem10.54} & \cem297.71 & \multicolumn{1}{l|}{\cem7.44} & \cem233.78 & \multicolumn{1}{l|}{\cem9.83} & \multicolumn{1}{l|}{\cem263.72\di{+17.8\%}} & \cem8.70\di{-2.3\%} \\ \cmidrule(l){2-13} 
 & \multicolumn{1}{c|}{\multirow{5}{*}{\textbf{\rotatebox{90}{70B}}}} & \multicolumn{1}{l|}{\textsc{Base}} & 910.68 & \multicolumn{1}{l|}{5.17} & 1850.04 & \multicolumn{1}{l|}{11.32} & 1397.99 & \multicolumn{1}{l|}{5.95} & 1421.32 & \multicolumn{1}{l|}{11.18} & \multicolumn{1}{l|}{1395.01\di{+0.0\%}} & 8.40\di{+0.0\%} \\
 & \multicolumn{1}{c|}{} & \multicolumn{1}{l|}{\textsc{Cot}} & 917.37 & \multicolumn{1}{l|}{5.15} & 1856.47 & \multicolumn{1}{l|}{10.61} & 1398.79 & \multicolumn{1}{l|}{5.88} & 1423.61 & \multicolumn{1}{l|}{10.88} & \multicolumn{1}{l|}{1399.06\di{+0.3\%}} & 8.13\di{-3.2\%} \\
 & \multicolumn{1}{c|}{} & \multicolumn{1}{l|}{\textsc{FullElicit}} & 915.09 & \multicolumn{1}{l|}{5.41} & 1851.00 & \multicolumn{1}{l|}{11.13} & 1448.56 & \multicolumn{1}{l|}{6.32} & 1434.97 & \multicolumn{1}{l|}{10.31} & \multicolumn{1}{l|}{1412.41\di{+1.2\%}} & 8.29\di{-1.3\%} \\
 & \multicolumn{1}{c|}{} & \multicolumn{1}{l|}{\textsc{PromptElicit}} & 8545.64 & \multicolumn{1}{l|}{69.39} & 7712.69 & \multicolumn{1}{l|}{55.77} & 8283.76 & \multicolumn{1}{l|}{56.14} & 7139.59 & \multicolumn{1}{l|}{60.06} & \multicolumn{1}{l|}{7920.42\di{+467.8\%}} & 60.34\di{+618.1\%} \\
 & \multicolumn{1}{c|}{} & \multicolumn{1}{l|}{\cem{\bf\se}} & \cem1198.53 & \multicolumn{1}{l|}{\cem6.05} & \cem1854.49 & \multicolumn{1}{l|}{\cem9.48} & \cem1672.19 & \multicolumn{1}{l|}{\cem6.15} & \cem1398.81 & \multicolumn{1}{l|}{\cem9.88} & \multicolumn{1}{l|}{\cem1531.00\di{+9.7\%}} & \cem7.89\di{-6.1\%} \\ \midrule
\multirow{10}{*}{\textbf{\rotatebox{90}{Mistral}}} & \multicolumn{1}{c|}{\multirow{5}{*}{\textbf{\rotatebox{90}{7B}}}} & \multicolumn{1}{l|}{\textsc{Base}} & 468.26 & \multicolumn{1}{l|}{21.60} & 564.08 & \multicolumn{1}{l|}{24.60} & 479.97 & \multicolumn{1}{l|}{18.67} & 647.02 & \multicolumn{1}{l|}{31.38} & \multicolumn{1}{l|}{539.83\di{+0.0\%}} & 24.06\di{+0.0\%} \\
 & \multicolumn{1}{c|}{} & \multicolumn{1}{l|}{\textsc{Cot}} & 489.55 & \multicolumn{1}{l|}{23.31} & 568.91 & \multicolumn{1}{l|}{25.27} & 502.71 & \multicolumn{1}{l|}{19.59} & 688.56 & \multicolumn{1}{l|}{34.02} & \multicolumn{1}{l|}{562.43\di{+4.2\%}} & 25.55\di{+6.2\%} \\
 & \multicolumn{1}{c|}{} & \multicolumn{1}{l|}{\textsc{FullElicit}} & 477.01 & \multicolumn{1}{l|}{19.74} & 567.08 & \multicolumn{1}{l|}{23.57} & 479.99 & \multicolumn{1}{l|}{18.61} & 647.23 & \multicolumn{1}{l|}{31.78} & \multicolumn{1}{l|}{542.83\di{+0.6\%}} & 23.42\di{-2.7\%} \\
 & \multicolumn{1}{c|}{} & \multicolumn{1}{l|}{\textsc{PromptElicit}} & 1444.44 & \multicolumn{1}{l|}{70.58} & 2043.11 & \multicolumn{1}{l|}{97.09} & 2230.07 & \multicolumn{1}{l|}{102.03} & 1788.88 & \multicolumn{1}{l|}{89.59} & \multicolumn{1}{l|}{1876.62\di{+247.6\%}} & 89.82\di{+273.3\%} \\
 & \multicolumn{1}{c|}{} & \multicolumn{1}{l|}{\cem{\bf\se}} & \cem330.42 & \multicolumn{1}{l|}{\cem12.74} & \cem493.56 & \multicolumn{1}{l|}{\cem18.33} & \cem440.59 & \multicolumn{1}{l|}{\cem13.32} & \cem462.43 & \multicolumn{1}{l|}{\cem21.00} & \multicolumn{1}{l|}{\cem431.75\di{-20.0\%}} & \cem16.34\di{-32.1\%} \\ \cmidrule(l){2-13} 
 & \multicolumn{1}{c|}{\multirow{5}{*}{\textbf{\rotatebox{90}{12B}}}} & \multicolumn{1}{l|}{\textsc{Base}} & 205.88 & \multicolumn{1}{l|}{5.62} & 380.22 & \multicolumn{1}{l|}{10.46} & 287.35 & \multicolumn{1}{l|}{5.32} & 251.74 & \multicolumn{1}{l|}{7.97} & \multicolumn{1}{l|}{281.30\di{+0.0\%}} & 7.34\di{+0.0\%} \\
 & \multicolumn{1}{c|}{} & \multicolumn{1}{l|}{\textsc{Cot}} & 208.52 & \multicolumn{1}{l|}{5.57} & 381.90 & \multicolumn{1}{l|}{10.09} & 287.36 & \multicolumn{1}{l|}{5.35} & 258.02 & \multicolumn{1}{l|}{7.41} & \multicolumn{1}{l|}{283.95\di{+0.9\%}} & 7.11\di{-3.2\%} \\
 & \multicolumn{1}{c|}{} & \multicolumn{1}{l|}{\textsc{FullElicit}} & 208.39 & \multicolumn{1}{l|}{5.65} & 380.59 & \multicolumn{1}{l|}{10.54} & 289.87 & \multicolumn{1}{l|}{5.41} & 254.23 & \multicolumn{1}{l|}{7.90} & \multicolumn{1}{l|}{283.27\di{+0.7\%}} & 7.37\di{+0.4\%} \\
 & \multicolumn{1}{c|}{} & \multicolumn{1}{l|}{\textsc{PromptElicit}} & 1273.57 & \multicolumn{1}{l|}{46.77} & 1560.04 & \multicolumn{1}{l|}{52.11} & 1749.14 & \multicolumn{1}{l|}{54.64} & 1226.14 & \multicolumn{1}{l|}{46.18} & \multicolumn{1}{l|}{1452.22\di{+416.3\%}} & 49.93\di{+580.0\%} \\
 & \multicolumn{1}{c|}{} & \multicolumn{1}{l|}{\cem{\bf\se}} & \cem278.07 & \multicolumn{1}{l|}{\cem6.29} & \cem428.54 & \multicolumn{1}{l|}{\cem9.02} & \cem385.85 & \multicolumn{1}{l|}{\cem5.45} & \cem260.08 & \multicolumn{1}{l|}{\cem7.47} & \multicolumn{1}{l|}{\cem338.14\di{+20.2\%}} & \cem7.06\di{-3.9\%} \\ \midrule
\multirow{10}{*}{\textbf{\rotatebox{90}{Qwen2.5}}} & \multicolumn{1}{c|}{\multirow{5}{*}{\textbf{\rotatebox{90}{7B}}}} & \multicolumn{1}{l|}{\textsc{Base}} & 143.78 & \multicolumn{1}{l|}{6.36} & 303.72 & \multicolumn{1}{l|}{13.53} & 247.26 & \multicolumn{1}{l|}{8.26} & 286.31 & \multicolumn{1}{l|}{15.49} & \multicolumn{1}{l|}{245.27\di{+0.0\%}} & 10.91\di{+0.0\%} \\
 & \multicolumn{1}{c|}{} & \multicolumn{1}{l|}{\textsc{Cot}} & 449.74 & \multicolumn{1}{l|}{25.10} & 407.30 & \multicolumn{1}{l|}{20.06} & 370.10 & \multicolumn{1}{l|}{15.93} & 460.24 & \multicolumn{1}{l|}{26.51} & \multicolumn{1}{l|}{421.85\di{+72.0\%}} & 21.90\di{+100.7\%} \\
 & \multicolumn{1}{c|}{} & \multicolumn{1}{l|}{\textsc{FullElicit}} & 145.24 & \multicolumn{1}{l|}{6.18} & 306.38 & \multicolumn{1}{l|}{12.89} & 255.94 & \multicolumn{1}{l|}{6.85} & 291.09 & \multicolumn{1}{l|}{16.35} & \multicolumn{1}{l|}{249.66\di{+1.8\%}} & 10.57\di{-3.1\%} \\
 & \multicolumn{1}{c|}{} & \multicolumn{1}{l|}{\textsc{PromptElicit}} & 1109.49 & \multicolumn{1}{l|}{62.77} & 1227.45 & \multicolumn{1}{l|}{64.27} & 1312.63 & \multicolumn{1}{l|}{64.84} & 1005.11 & \multicolumn{1}{l|}{58.32} & \multicolumn{1}{l|}{1163.67\di{+374.4\%}} & 62.55\di{+473.3\%} \\
 & \multicolumn{1}{c|}{} & \multicolumn{1}{l|}{\cem{\bf\se}} & \cem194.21 & \multicolumn{1}{l|}{\cem6.90} & \cem357.74 & \multicolumn{1}{l|}{\cem12.92} & \cem292.41 & \multicolumn{1}{l|}{\cem7.59} & \cem313.18 & \multicolumn{1}{l|}{\cem16.05} & \multicolumn{1}{l|}{\cem289.38\di{+18.0\%}} & \cem10.87\di{-0.4\%} \\ \cmidrule(l){2-13} 
 & \multicolumn{1}{c|}{\multirow{5}{*}{\textbf{\rotatebox{90}{32B}}}} & \multicolumn{1}{l|}{\textsc{Base}} & 624.64 & \multicolumn{1}{l|}{6.55} & 1271.39 & \multicolumn{1}{l|}{15.35} & 773.30 & \multicolumn{1}{l|}{6.51} & 1044.09 & \multicolumn{1}{l|}{14.50} & \multicolumn{1}{l|}{928.35\di{+0.0\%}} & 10.73\di{+0.0\%} \\
 & \multicolumn{1}{c|}{} & \multicolumn{1}{l|}{\textsc{Cot}} & 625.01 & \multicolumn{1}{l|}{7.46} & 1447.84 & \multicolumn{1}{l|}{18.24} & 871.12 & \multicolumn{1}{l|}{8.13} & 1048.22 & \multicolumn{1}{l|}{14.59} & \multicolumn{1}{l|}{998.05\di{+7.5\%}} & 12.10\di{+12.8\%} \\
 & \multicolumn{1}{c|}{} & \multicolumn{1}{l|}{\textsc{FullElicit}} & 636.89 & \multicolumn{1}{l|}{6.63} & 1275.19 & \multicolumn{1}{l|}{15.06} & 780.95 & \multicolumn{1}{l|}{6.80} & 1052.68 & \multicolumn{1}{l|}{14.21} & \multicolumn{1}{l|}{936.43\di{+0.9\%}} & 10.67\di{-0.5\%} \\
 & \multicolumn{1}{c|}{} & \multicolumn{1}{l|}{\textsc{PromptElicit}} & 5556.76 & \multicolumn{1}{l|}{82.86} & 5468.44 & \multicolumn{1}{l|}{76.23} & 5290.17 & \multicolumn{1}{l|}{69.83} & 4096.52 & \multicolumn{1}{l|}{62.62} & \multicolumn{1}{l|}{5102.97\di{+449.7\%}} & 72.89\di{+579.5\%} \\
 & \multicolumn{1}{c|}{} & \cem{\bf\se} & \cem738.98 & \cem6.32 & \cem1220.44 & \cem12.29 & \cem1011.58 & \cem7.02 & \cem948.09 & \cem11.47 & \cem979.77\di{+5.5\%} & \cem9.28\di{-13.5\%} \\ \bottomrule
\end{tabular}%
}
\vspace{-10pt}
\end{table*}

\newpage
\appendix

\section{Reproducibility Details}
\label{sec:app-rep}

\paragraph{QA pipeline details.}
We use PyTorch~\cite{paszke2019pytorch} and HuggingFace Transformers~\cite{wolf2020transformers} to implement all involved language models (LMs).
Specifically, the LMs used in the paper are the instruct fine-tuned version of Llama3.1 8B\footnote{\url{https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct}} and 70B\footnote{\url{https://huggingface.co/meta-llama/Llama-3.1-70B-Instruct}}, Mistral 7B (instruct-v0.3)\footnote{\url{https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3}} and 12B (Nemo)\footnote{\url{https://huggingface.co/mistralai/Mistral-Nemo-Instruct-2407}}, as well as Qwen2.5 7B\footnote{\url{https://huggingface.co/Qwen/Qwen2.5-7B-Instruct}} and 32B\footnote{\url{https://huggingface.co/Qwen/Qwen2.5-32B-Instruct}}.
We use the official \texttt{apply\_chat\_template} function to get the input tokens from text input ($\tau(\vc, \vq)$) and let the model generate the answer based on it.
We use greedy decoding with a temperature of 0 to get deterministic answers.
For all the datasets used, we test the LM on the official public validation split, with 7,405/4,212/7,785/12,836 context-question pairs for HotopotQA/NewsQA/TQA/NQ.

\paragraph{\se implementation details.}
Unless otherwise specified, all \se results in this paper use the default settings stated in the paper: the last 50\% of layers are designated as evidence-reading layers, with an elicitation threshold $\alpha = 0.5$. 
We use SpaCy~\cite{honnibal2017spacy} to segment the context passages into sentences, and \se performs the sentence-level evidence score computation and elicitation based on this segmentation.
Note that this is only for \se, for \textsc{PromptElicit}, we directly search for exact text-level matches of the generatively extracted evidence to ensure that all extracted information is utilized.

\begin{figure*}[t]
\vspace{-5pt}
    \centering
    \includegraphics[width=\linewidth]{figs/layer_att_full.pdf}
    \caption[evidence reading layers]{
        Across different LM families and datasets, the deep attention layers highlight the crucial evidence sentences within the context, even when the LM gives incorrect answers (dashed lines).
        The X-axis is the depth of attention layers, and the Y-axis is the ratio of the average attention per token (APT) in the evidence/non-evidence sections to the APT across the entire context.
        For the HotpotQA~\cite{yang2018hotpotqa} dataset, We leverage the ``\texttt{supporting\_facts}'' annotations to differentiate evidence and non-evidence sentences within the context.
        For other datasets, we treat a context sentence as evidence if it contains at least one of the correct answers.
    }
    \label{fig:layeratt-full}
\vspace{-5pt}
\end{figure*}

\section{Additional Results and Analysis}
\label{sec:apd-res}

\subsection{Computational efficiency}\label{sec:apd-res-eff}
Due to space constraints, we report only the average running time per sample across all datasets in Table~\ref{tab:main}. Table~\ref{tab:efficiency} provides a detailed breakdown of efficiency results for each dataset, including the average inference time per sample (i.e., context-question pair) and the total number of tokens generated during inference.
We observe that: 
(i) While \fe is highly efficient as a naive method, it barely helps with the QA performance, as shown in Table~\ref{tab:main}. 
(ii) \pe, based on generative evidence extraction, requires the model to generate a large volume of extra tokens during inference, up to 852\% more than direct QA, resulting in considerably slower inference times. 
(iii) As discussed before, the response to \co is not consistent for different LMs. Generally, for Llama and Mistral models, \co have little impact on the QA performance, and inference time as well. For Qwen-series models, \co obtains slightly better EM by generating the intermediate reasoning steps before answering, but this also leads to a great drop in F1 score and significantly higher inference time (e.g., +72.0\% for Qwen2.5-7B) due to the generation of intermediate step tokens.
(iv) \se, by helping the model focus on contextual evidence, generates shorter but more accurate answers. Additionally, as \se does not involve generative evidence extraction, it remains computationally efficient, with only 3-5\% of the inference time overhead compared to \pe.



\subsection{More layer-wise attention visualization}\label{sec:apd-res-attlayer}
In Figure~\ref{fig:layeratt}, we present the differences in attention paid by different layers of the model to evidence versus non-evidence context information on the HotpotQA~\cite{yang2018hotpotqa} dataset. We chose HotpotQA because it provides human-annotated sentence-level "supporting\_facts" annotations.
In this section, we extend our analysis to other datasets. Since human annotations are unavailable for these datasets, we use a simple rule to distinguish evidence from non-evidence sentences: any sentence containing at least one correct answer is considered evidence. While this is not a rigorous approach for labeling evidence sentences, it still allows us to visualize patterns and roughly validate whether our observations hold across different datasets.
As shown in Figure~\ref{fig:layeratt-full}, deeper layers of various LMs consistently demonstrate a strong ability to differentiate evidence across datasets.

\begin{table}[h]
\centering
\caption{Sentence-level versus token-level eliciting.}
\label{tab:sent-vs-token}
\vspace{-10pt}
\resizebox{0.9\columnwidth}{!}{%
\begin{tabular}{@{}ccccc@{}}
\toprule
\textbf{Elicit} & \multicolumn{4}{c}{\textbf{Dataset}} \\ \cmidrule(l){2-5} 
\textbf{Level} & \textbf{HotpotQA} & \textbf{NewsQA} & \textbf{TQA} & \textbf{NQ} \\ \midrule
\multicolumn{5}{c}{Metric: EM} \\ \midrule
\multicolumn{1}{c|}{Token} & 66.5 & 63.5 & 74.0 & 57.5 \\
\multicolumn{1}{c|}{Sentence} & \textbf{68.5} & \textbf{66.9} & \textbf{79.4} & \textbf{64.0} \\ \midrule
\multicolumn{5}{c}{Metric: Token F1} \\ \midrule
\multicolumn{1}{c|}{Token} & 68.6 & 58.4 & 70.9 & 61.6 \\
\multicolumn{1}{c|}{Sentence} & \textbf{69.5} & \textbf{60.8} & \textbf{72.7} & \textbf{67.8} \\ \bottomrule
\end{tabular}%
}
\vspace{-10pt}
\end{table}

\subsection{Sentence-level versus token-level eliciting}\label{sec:apd-res-token}
We now show that sentence-level evidence eliciting holds a significant advantage over token-level eliciting due to its ability to highlight evidence with better semantic coherence. 
Table~\ref{tab:sent-vs-token} presents the experimental results using the llama3.1-8B model across four datasets.
We compare the normal version \se (sentence) and a new token-level variant \se (token) by treating each token as a separate "sentence".
It is evident that token-level eliciting performs worse than sentence-level eliciting on all datasets. 
The underlying reason is that tokenization does not ensure each token represents a complete entity or concept, resulting in highlighted evidence that lacks full semantic meaning.
For example, we notice that a year within the context, say "2002", can be tokenized into two tokens "200" and "2" and token-level eliciting could end up highlighting only the first token "200.".
Similar "partial highlighting" issues also occur with names, locations, and other uncommon phrases. 
In contrast, sentence-level eliciting highlights the entire sentence, maintaining semantic continuity in the processed context, which better aids the model in utilizing context information for QA.


\begin{table*}[t]
\scriptsize
\centering
\caption{
Full context of example \#1 \& \#2 from Table~\ref{tab:example} demonstrating how our method assists the LM in context-based QA, with \textev{blue} text highlighting the evidence selected by \se. The base LM used is Llama3.1-8B-Instruct.
}
\label{tab:example-full1}
\vspace{-5pt}
\resizebox{\textwidth}{!}{%
\begin{tabular}{p{0.01\textwidth} p{0.75\textwidth} p{0.22\textwidth}}
\toprule
{\bf\small} & {\bf\small Full Context Passage} & {\bf\small Question \& Answers}
\\ \midrule
\multirow{7}{*}{\rotatebox{90}{\bf True or False}}
& 
\textev{Tiger Please is an Indie / Alternative five-piece band from Cardiff, Wales. The band formed in August 2008.} The band's influences are U2, Sigur Rós, Kings of Leon, John Mayer and Counting Crows. They signed with Walnut Tree Records in 2009 and released their debut EP "They Don't Change Under Moonlight". "Kerrang!" magazine, "Rock Sound" magazine, and "Classic Rock" magazine praised the EP and featured the band on the "Rock Sound" and "Classic Rock" cover-mount albums. The band toured with Kids In Glass Houses, InMe, Twin Atlantic and Funeral For A Friend. Taxi Violence is a South African rock band from Cape Town. The group consists of George van der Spy (vocals), Jason Ling (bass), Louis Nel (drums), Rian Zietsman (guitar) and Loedi van Renen (guitar/bass). They have released five studio albums: "Untie Yourself" (2006), "The Turn" (2009), "" (2011), "Soul Shake" (2013), and "Tenfold" (2014). They are influenced by bands such as: Black Rebel Motorcycle Club, Queens of the Stone Age, Led Zeppelin, Jimi Hendrix, Nirvana, and Pearl Jam. This is discography of the American rock band Black Rebel Motorcycle Club \textev{Black Rebel Motorcycle Club (often abbreviated as BRMC) is an American rock band from San Francisco, California.} The group consists of Peter Hayes (vocal, guitar, harmonica), Robert Levon Been (vocal, bass, guitar), and Leah Shapiro (drums). Former drummer Nick Jago left the band in 2008 to focus on his solo project. Skybombers is a rock band from Melbourne. They were formed as Collusion by Scotch College students Hugh Gurney, Ravi Sharma, Scott McMurtrie and Sam Bethune. They later changed to Skybombers, a name inspired by an icy-pole. Their placing a demo song "It Goes Off" on MySpace brought them their first TV appearances. They had early international attention when "It Goes Off" of their EP "Sirens" made the most-played list on L.A.'s Indie 103.1 and played a showcase gig at The Viper Room. They have toured Australia, Japan and USA. and their debut album "Take Me To Town" was recorded in L.A. with Rick Parker (Black Rebel Motorcycle Club). The band made their way on video game media in 2007 when "It Goes Off" appeared on the soundtrack for "Burnout Dominator", the song later reappeared on "Burnout Paradise" in early 2008. Black Carousel was recorded in LA, again with Rick Parker at the helm. Masaki Liu, sometimes referred to as "Saki", is the engineer and producer operating One Way Studio, a digital recording studio in Benicia, California. Masaki has recorded and produced music for many bands, including Five Iron Frenzy, Black Rebel Motorcycle Club, The Echoing Green, The W's and Yellow Second. Dan Russell is a musician and songwriter in addition to an artist manager and advocate, musician, songwriter, concert promoter, record producer and music supervisor for television and film. A graduate of Walpole High School in Massachusetts and later Barrington College, Russell is known for managing both the American rock band The Call and songwriter Michael Been and has worked in various capacities with such artists as Black Rebel Motorcycle Club, Sam Philips, Mark Heard, U2 and Robin Lane, Ramona Silver, Vigilantes of Love, among others. Black Rebel Motorcycle Club: Live is a DVDs of Black Rebel Motorcycle Club concert footage captured from three sold out shows in Berlin, Dublin and Glasgow, and chronicles the end of the band's 2007 tour in support of "Baby 81". Additionally, it includes intimate, behind-the-scenes footage, glimpses into the making of 2005's Howl and is rounded out with a bonus live album featuring 14 songs. KAV (Kav Sandhu) is a British musician from Leicester UK, based in Los Angeles. KAV played guitar with British band Happy Mondays for 4 years after helping reform the band with frontman Shaun Ryder in 2004. He launched his solo project under moniker "KAV" in 2008 with long-time friend and drummer Jim (James) Portas. His solo material has been compared by the media to everyone from Iggy \& The Stooges, Black Rebel Motorcycle Club, Primal Scream, Kasabian, The Rolling Stones \& Bob Dylan. He plays live with a full live band, which sometimes features guest musicians from various bands. Vagrant Records is an American record label based in California. It was founded in 1995 by Rich Egan and Jon Cohen. The label focuses on rock but features artists in a variety of other genres including folk, soul, electronic, and pop. It is home to artists such as The 1975, Death Spells, Eels, Bad Suns, Edward Sharpe and the Magnetic Zeroes, CRUISR, Active Child, PJ Harvey, School of Seven Bells, Black Rebel Motorcycle Club, James Vincent McMorrow, Black Joe Lewis, Wake Owl, Blitzen Trapper, and Bombay Bicycle Club. Originally, Vagrant Records was mostly focused on emo bands such as Dashboard Confessional, Saves the Day, The Get Up Kids, and Alkaline Trio.
& 
{\scriptsize
    \textbf{Question:} Are the bands Tiger Please and Black Rebel Motorcycle Club from the same country?\vspace{1mm}
    
    \textbf{True Answer:} No. \vspace{1mm}
    
    \uline{\textbf{\texttt{Base:}}}
    Yes. \xmark \vspace{1mm}
    
    \uline{\textbf{\texttt{+\se:}}}
    No. \cmark \vspace{1mm}
}
\\ \midrule
\multirow{8}{*}{\rotatebox{90}{\bf Comparison}}
& 
\textev{Home Monthly was a monthly women's magazine published in Pittsburgh, Pennsylvania in the late 19th century.}
"The Strategy of the Were-Wolf Dog" is a short story by Willa Cather. \textev{It was first published in "Home Monthly" in December 1896.}
The Count of Crow's Nest is a short story by Willa Cather. It was first published in "Home Monthly" in October 1896.
\textev{Mirabella was a women's magazine published from June 1989 to April 2000.} It was created by and named for Grace Mirabella, a former "Vogue" editor in chief, in partnership with Rupert Murdoch.
"Nanette: An Aside" is a short story by Willa Cather. It was first published in "Courier" on 31 July 1897 and one month later in "Home Monthly".
"The Prodigies" is a short story by Willa Cather. It was first published in "Home Monthly" in July 1897.
"A Resurrection" is a short story by American writer Willa Cather. It was first published in "Home Monthly" in April 1897.
Tommy, the Unsentimental is a short story by Willa Cather. It was first published in "Home Monthly" in August 1896.
The Princess Baladina is a short story by Willa Cather. It was first published in "Home Monthly" in 1896 under the pseudonym of Charles Douglass.
"The Way of the World" is a short story by Willa Cather. It was first published in "Home Monthly" in April 1898.
& 
{\scriptsize
    \textbf{Question:} Which women's magazine was published first, Mirabella or Home Monthly?\vspace{1mm}
    
    \textbf{True Answer:} Home Monthly. \vspace{1mm}
    
    \uline{\textbf{\texttt{Base:}}}
    Mirabella. \xmark \vspace{1mm}
    
    \uline{\textbf{\texttt{+\se:}}}
    Home Monthly. \cmark \vspace{1mm}
}
\\
\bottomrule
\end{tabular}
            }
\vspace{-10pt}
\end{table*}

\begin{table*}[t]
\scriptsize
\centering
\caption{
Full context of example \#3 \& \#4 from Table~\ref{tab:example} demonstrating how our method assists the LM in context-based QA, with \textev{blue} text highlighting the evidence selected by \se. The base LM used is Llama3.1-8B-Instruct.
}
\label{tab:example-full2}
\vspace{-5pt}
\resizebox{\textwidth}{!}{%
\begin{tabular}{p{0.01\textwidth} p{0.75\textwidth} p{0.22\textwidth}}
\toprule
{\bf\small} & {\bf\small Full Context Passage} & {\bf\small Question \& Answers}
\\ \midrule
\multirow{7}{*}{\rotatebox{90}{\bf Fact Retrieval}}
& 
The 1987 European Cup Final was a football match held at the Prater Stadium, Vienna, on 27 May 1987, that saw Porto of Portugal defeat Bayern Munich of West Germany 2–1. Both sides were missing key players: the Portuguese were without their injured striker Fernando Gomes, while the Germans were missing their sweeper, and captain, Klaus Augenthaler, who was suspended, along with striker Roland Wohlfarth and midfield player Hans Dorfner, who were both injured. The Portuguese side fought back from 1–0 down to win their first European Cup, with the goals coming from a back heel by Rabah Madjer and a volley from Juary, after a Ludwig Kögl header had given Bayern the lead in the first half. The final was the first European Cup final that Bayern, and their captain Lothar Matthäus would lose to successive late goals, repeated 12 years later in the 1999 UEFA Champions League Final against Manchester United.
The 1993 European Cup Winners' Cup Final was a football match contested between Parma of Italy and Royal Antwerp of Belgium. The final was held at Wembley Stadium in London, England on 12 May 1993. It was the final match of the 1992–93 European Cup Winners' Cup and the 33rd European Cup Winners' Cup Final. Parma beat Antwerp 3–1 and in doing so became the eighth different Italian team to win a European trophy.
The 1970 European Cup Winners' Cup Final was a football match between Manchester City of England and Górnik Zabrze of Poland on 29 April 1970 at Prater Stadium in Vienna, Austria. It was the final match of the 1969–70 European Cup Winners' Cup and the tenth European Cup Winners' Cup final. Both sides made their first appearance in a European final. Manchester City won the match 2–1 thanks to goals by Neil Young and Francis Lee. The victory was City's only European trophy.
The 1977 European Cup Final was an association football match between Liverpool of England and Borussia Mönchengladbach of Germany on 25 May 1977 at the Stadio Olimpico in Rome, Italy (the venue was decided in Bern by the UEFA Executive Committee on 17 September 1976). The showpiece event was the final match of the 1976–77 season of Europe's premier cup competition, the European Cup. Both teams were appearing in their first European Cup final, although the two sides had previously met in the 1973 UEFA Cup Final, which Liverpool won 3–2 on aggregate over two legs.
The 1988 European Cup Winners' Cup Final was a football match contested between Mechelen of Belgium and the defending champions, Ajax of Netherlands. It was the final match of the 1987–88 European Cup Winners' Cup and the 28th European Cup Winners' Cup Final. The final was held at Stade de la Meinau in Strasbourg, France. Mechelen won the match 1–0 thanks to a goal by Piet den Boer.
The 1961 European Cup Winners' Cup Final was a football match contested between Fiorentina of Italy and Rangers of Scotland. It was the final of the 1960–61 European Cup Winners' Cup the first UEFA Cup Winners' Cup final. It was the only time that the final was played over two legs. The first leg was played at Ibrox Stadium, Glasgow and the second leg at the Stadio Comunale in Florence. It was Rangers first European final and in doing so became the first British team to reach the final of a European football competition. It was Fiorentina's second European final having previously reached the 1957 European Cup final.
The 1987 European Cup Winners' Cup Final was a football match contested between Ajax of Netherlands and Lokomotive Leipzig of East Germany. It was the final match of the 1986–87 European Cup Winners' Cup and the 27th European Cup Winners' Cup Final. The final was held at Olympic Stadium in Athens, Greece. Ajax won the match 1–0 with a 20th-minute header from Marco van Basten.
Lars Lunde (born 21 March 1964) is a Danish former professional football player, who played in the striker position. Lunde got his breakthrough with Brøndby IF in 1983, and he made his debut for the Denmark national football team in October 1983. He was sold to Young Boys Bern in Switzerland, before moving to German club Bayern Munich in 1986. \textev{He was a part of the Bayern team which won the German Bundesliga championship in 1987, and he came on as a late substitute when Bayern lost the 1987 European Cup Final to FC Porto.} He played the last of his three matches for the Danish national team in April 1987, before leaving Bayern during the 1987–88 season. He went on to play for a number of smaller clubs, ending his career with FC Baden in Switzerland.
The 1978 European Cup Final was an association football match between Liverpool of England and Club Brugge of Belgium on 10 May 1978 at Wembley Stadium, London, England (the venue was decided in Bern by the UEFA Executive Committee on 20 September 1977). It was the final match of the 1977–78 season of Europe's premier cup competition, the European Cup. Liverpool were the reigning champions and were appearing in their second European Cup final. Club Brugge were appearing in their first European Cup final. The two sides had met once before in European competition, when they contested the 1976 UEFA Cup Final, which Liverpool won 4–3 on aggregate.
The 1985 European Cup Winners' Cup Final was a football match contested between Everton of England and Rapid Wien of Austria. It was the final match of the 1984–85 European Cup Winners' Cup and the 25th European Cup Winners' Cup Final. The final was held at Feijenoord Stadion in Rotterdam, Netherlands, on 15 May 1985. Everton, which dominated throughout, won the match 3–1 thanks to goals by Andy Gray, Trevor Steven and Kevin Sheedy. Everton were unable to defend the trophy: as league champions they would have entered the 1985–86 European Cup, but they were not permitted to play in either competition following the actions of rival Liverpool fans at the Heysel Stadium, which saw all English clubs banned from European competitions.
& 
{\scriptsize
    \textbf{Question:} Which team did Lars Lunde play for when defeated for the 1987 European Cup Final?\vspace{1mm}
    
    \textbf{True Answer:} Bayern Munich \vspace{1mm}
    
    \uline{\textbf{\texttt{BASE:}}}
    FC Porto. \xmark \vspace{1mm}
    
    \uline{\textbf{\texttt{+\se:}}}
    Bayern Munich. \cmark \vspace{1mm}
}
\\ \midrule
\multirow{8}{*}{\rotatebox{90}{\bf Multi-hop Reasoning}}
& 
Mount Barker Junction railway station is a disused station on the Adelaide to Wolseley line serving the South Australian city of Mount Barker. Mount Gambier railway station was the junction station for the Naracoorte–Millicent and Mount Gambier-Heywood lines in the South Australian city of Mount Gambier. Frewville is a small suburb in the South Australian city of Adelaide. It is three kilometres south-east of Adelaide's central business district (CBD). The 2006 South Australian Super League was the first season of the South Australian Super League, the new top division of association football in South Australia, replacing the South Australian Premier League, which became the second division. It was also the first year that football in South Australia was run by the Football Federation of South Australia, which replaced the South Australian Soccer Federation. The season came down to a final round relegation battle between White City Woodville and Adelaide Olympic. Olympic lost 3–1 at Modbury while White City went down 1–0 away to Cumberland. This sent Olympic down to play in the Premier League in 2007. Adelaide City won the title with games to spare after being runaway leaders, finishing the season unbeaten. \textev{Norwood is a suburb of Adelaide, about 4 km east of the Adelaide city centre.} The suburb is in the City of Norwood Payneham \& St Peters, the oldest South Australian local government municipality, with a city population over 34,000. Whyalla railway station was the terminus station of the Whyalla line serving the South Australian city of Whyalla. \textev{Walter Frank Giffen (20 September 1861 in Norwood – 28 June 1949 in Adelaide) was an Australian cricketer who played in 3 Tests between 1887 and 1892.} He was the brother of the great all-rounder George Giffen. The City of Burnside is a local government area with an estimated population of 44,300 people in the South Australian city of Adelaide. Burnside was founded in August 1856 as the District Council of Burnside, and was classed as a city in 1943. It is named after the property of an early settler and stretches from the Adelaide Parklands into the Adelaide foothills. It is bounded by Adelaide, Adelaide Hills Council, Campbelltown, Mitcham, Norwood Payneham and St Peters and Unley. The city has an area of 27.53 km². Glenunga is a small southern suburb of 2,539 people in the South Australian city of Adelaide. It is located five kilometres southeast of the Adelaide city centre. The name Glenunga is taken from an Aboriginal language "unga" meaning near and "glen" because of its proximity to Glen Osmond (see Manning's places of South Australia by Geoffrey H. Manning published in 1990). Bounded on the north by Windsor Road, the east by Portrush Road, the south-west by Glen Osmond Road and the west by Conyngham Street, the leafy suburb forms a rough triangular layout. It is close by to other Burnside council suburbs of Toorak Gardens and Glenside. Collina is a suburb of the Australian city of Griffith in the Riverina region of New South Wales. The suburb is in the City of Griffith local government area. Collina is 4 km northwest of the Griffith city centre and reflects the city's rapid growth in the early 2000s.
& 
{\scriptsize
    \textbf{Question:} Walter Giffen is from a suburb of which South Australian city?\vspace{1mm}
    
    \textbf{True Answer:} Adelaide\vspace{1mm}
    
    \uline{\textbf{\texttt{Base:}}}
    Norwood. \xmark \vspace{1mm}
    
    \uline{\textbf{\texttt{+\se:}}}
    Adelaide. \cmark \vspace{1mm}
}
\\
\bottomrule
\end{tabular}
}
\vspace{-10pt}
\end{table*}



\subsection{Examples with full context}\label{sec:apd-res-exa}
Finally, we present the full context of examples from Table~\ref{tab:example} to illustrate \se's ability to identify supporting facts/evidence within noisy long contexts. The results are shown in Tables~\ref{tab:example-full1} and \ref{tab:example-full2}.
These examples are from HotpotQA's distractor setting, where the context passage contains distracting information retrieved from Wikipedia using the question as a query~\cite{yang2018hotpotqa}.
Note that these distractors are not completely irrelevant random noise that can be easily filtered out by retrieval systems. 
Instead, they appear related to the question but do not actually support answering it, serving as "hard negatives."
This scenario is quite common in practice and significantly impacts the performance of retrieval-augmented generation~\cite{wu2024ragirrelevant,cuconasu2024ragnoise,he2024llm,liu2024logic}.
As shown in Tables~\ref{tab:example-full1} and \ref{tab:example-full2}, the LM struggles to effectively use contextual evidence to provide correct answers under such a situation. 
Despite this, \se accurately highlights the critical evidence within the noisy long context, helping the model focus on the most relevant facts and thus arrive at the correct answer.

\section{Additional Discussions}

\subsection{Potential Risks}

Generative AI tools such as language models have an increasing impact on our daily lives in the era of big data and AI~\cite{yan2024pacer,xuslog,ban2021ee,lin2024bemap,lin2024backtime,guo2023taming}, such as finance~\cite{chan2024group,liu2024aim} and healthcare~\cite{ye2023web,liu2024class}, especially with the recent trends of foundation models \cite{DBLP:conf/kdd/ZhengJLTH24, DBLP:journals/corr/abs-2410-12126, DBLP:journals/corr/abs-2412-21151, DBLP:journals/corr/abs-2412-08174}.
This study focuses on enhancing language models' ability to effectively utilize information from contextual documents. However, retrieved documents from the internet may contain unethical or discriminatory content, which the model might read and incorporate into its outputs. While addressing ethical concerns in Retrieval-Augmented Generation (RAG) or fact-based question-answering tasks~\cite{cornnet,binet,kompare} is beyond the scope of this work, such issues can typically be mitigated by using detectors to filter harmful information from the context documents provided to the language model, or other general techniques such as ensemble multi-model answers~\cite{liu2021imbens,liu2020self,liu2020mesa} or watermarking~\cite{chen2024wapiti} AI-generated contents.

\subsection{Usage of Artifacts and AI Assistants}

All models and datasets used in this study are publicly available on HuggingFace, and we adhered to their respective licenses and terms of use, limiting our work to non-commercial academic research. These models and datasets have been reviewed by their developers/creators to minimize the inclusion of personally identifiable information or offensive content and are widely adopted by the research community. The datasets primarily consist of English-language content and focus on fact-based question-answering tasks. We used AI tools to assist with language refinement during the writing process, but the paper contains no AI-generated paragraphs. All material has been carefully reviewed to ensure accuracy and adherence to ethical standards.