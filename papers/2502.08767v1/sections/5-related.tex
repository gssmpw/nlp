\section{Related Works}\label{sec:rel}
\vspace{-5pt}
\paragraph{Context-based question answering.}
Furnishing LMs with relevant context is an effective way of providing up-to-date external and/or private knowledge~\cite{ji2023survey,selfrag} to help mitigate hallucination and improve response accuracy~\cite{hallucination_survey}. 
Retrieval-augmented Generation (RAG) is a widely adopted paradigm for this purpose~\cite{gao2023ragsurvey,fan2024ragsurvey}.
Despite its popularity, recent studies have pointed out that context retrieved from external sources often contains noise and irrelevant information, leading to confusion for LM~\cite{cuconasu2024ragnoise,wu2024ragirrelevant}.
Motivated by this, we explore how to leverage contextual information more effectively at a finer granularity by highlighting critical information within the context.
To the best of our knowledge, we are the first to investigate automated contextual evidence highlighting based on LM internal representations.

\paragraph{Factuality and internal representation.}
Recent studies have explored ways to understand QA factuality by analyzing the internal representations to identify important attention layers~\cite{yuksekgonul2023attention,chen2024sharpness} or heads~\cite{halawi2024overthinking} that are crucial for generation correctness or hallucination.
However, they primarily focus on how LMs utilize their parametric knowledge in controlled generation~\cite{yuksekgonul2023attention,halawi2024overthinking} or closed-book QA~\cite{chen2024sharpness}, with little discussion on the role of internal representations when utilizing external non-parametric knowledge in context-based QA.
Our work focuses on investigating LM's internal attention patterns for the context and on how to leverage these patterns to identify critical evidence in the context.
