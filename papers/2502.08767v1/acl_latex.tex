% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

\usepackage[table]{xcolor}
% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[preprint]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

% \newcommand{}{}

\usepackage{ulem}
\usepackage{enumitem}

\usepackage{longtable}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{xspace}
\usepackage{amsmath}
\usepackage{pifont}
\usepackage{cleveref}
\usepackage{multirow}
\usepackage{lipsum}
\usepackage{booktabs}
\usepackage{tcolorbox}
\usepackage{subfigure}  % for subfigures
\usepackage[prependcaption,textsize=scriptsize,color=pink]{todonotes}
% \usepackage[disable]{todonotes}

\input{mathdef}

\newcommand{\se}{\textsc{SelfElicit}\xspace}
\newcommand{\co}{\textsc{Cot}\xspace}
\newcommand{\fe}{\textsc{FullElicit}\xspace}
\newcommand{\pe}{\textsc{PromptElicit}\xspace}

\newcommand{\hh}[1]{{\textcolor{red}{[HH: #1]}}}
\newcommand{\zn}[1]{{\textcolor{cyan}{[ZN: #1]}}}
\newcommand{\raa}[1]{{\textcolor{orange}{[RAA: #1]}}}
\newcommand{\fixed}[1]{{\textcolor{green}{#1}}}

% \newcommand{\todo}[1]{{\textcolor{violet}{TODO: #1}}}
\newcommand{\cmark}{\textcolor{teal}{\bf\ding{51}}}
\newcommand{\xmark}{\textcolor{red}{\bf\ding{55}}}
\newcommand{\textev}[1]{\textcolor{blue}{\bf #1}}
\definecolor{em}{gray}{0.9}
\newcommand{\cem}{\cellcolor{em}}
\newcommand{\best}[1]{{\textbf{#1}}}
\newcommand{\bs}[1]{{\textbf{#1}}}
\newcommand{\sbs}[1]{{\uline{#1}}}
\newcommand{\di}[1]{{ (\small{#1})}}
\newcommand{\gain}[1]{{\textcolor{teal}{\ ($\uparrow$#1)}}}
\newcommand{\loss}[1]{{\textcolor{violet}{\ ($\downarrow$#1)}}}
\renewcommand{\paragraph}[1]{\vspace{0.3em}\noindent\textbf{#1}\hspace{0.5em}}

\title{\se: Your Language Model Secretly Knows Where is the Relevant Evidence}

\author{Zhining Liu\textsuperscript{1}, Rana Ali Amjad\textsuperscript{2}, Ravinarayana Adkathimar\textsuperscript{2}, \textbf{Tianxin Wei\textsuperscript{1}, Hanghang Tong\textsuperscript{1}}\\
  \textsuperscript{1}University of Illinois Urbana-Champaign, \textsuperscript{2}Amazon Science\\
  \textsuperscript{1}\texttt{\{liu326, twei10, htong\}@illinois.edu} \textsuperscript{2}\texttt{\{raamjad,adkathi,ani\}@amazon.com}\\
}

\begin{document}
\maketitle
\begin{abstract}
Providing Language Models (LMs) with relevant evidence in the context (either via retrieval or user-provided) can significantly improve their ability to provide factually correct grounded responses. 
However, recent studies have found that LMs often struggle to fully comprehend and utilize key evidence from the context, especially when it contains noise and irrelevant informationâ€”an issue common in real-world scenarios.
To address this, we propose \se, an inference-time approach that helps LMs focus on key contextual evidence through self-guided explicit highlighting.
By leveraging the inherent evidence-finding capabilities of LMs using the attention scores of deeper layers, our method automatically identifies and emphasizes key evidence within the input context, facilitating more accurate and factually grounded responses \textit{without} additional training or iterative prompting.
We demonstrate that \se brings consistent and significant improvement on multiple evidence-based QA tasks for various LM families while maintaining computational efficiency.
Our code and documentation are available at \url{https://github.com/ZhiningLiu1998/SelfElicit}.
\end{abstract}

\input{sections/1-introduction}
\input{sections/2-background}
\input{sections/3-methodology}
\input{sections/4-experiment}
\input{sections/5-related}
\input{sections/6-conclusion}

\section*{Limitations}
We evaluated the effectiveness of \se across several open-source LMs. However, we can not assess \se on proprietary LMs due to the need to access attention scores. 
On the computational efficiency front, although \se generates only one additional token in the first pass and already demonstrates good computational efficiency in practice, there is potential for further acceleration by avoiding re-encoding prompt. 
This redundancy arises because the input sequence is encoded twice during the first and second passes, despite the only difference being a few text markers inserted into the context passage for evidence highlighting. 
Exploring methods for caching and reusing shared input content encoding process, thereby avoiding the repeated encoding of the input content is a promising future direction for further reducing computational overhead.

\normalem
% Bibliography entries for the entire Anthology, followed by custom entries
%\bibliography{anthology,custom}
% Custom bibliography entries only
\bibliography{custom}

\input{sections/appendix}

\end{document}
