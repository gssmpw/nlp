\section{Related Work}
Grid computing environments have been extensively studied due to their potential to leverage distributed resources for high-performance computing tasks. A major challenge in these settings is optimizing job scheduling and data allocation, both crucial to improving resource utilization and reducing the makespan.

\subsection{Job Scheduling}
Job scheduling has been a central focus of grid computing research over the past few decades. Given its NP-hard nature, numerous heuristics have been developed to efficiently provide high-quality approximate solutions \citep{BRAUN2001810}. Notable examples include the MinExe \citep{ranganathan2002decoupling}, Min-min, and Max-min algorithms \citep{armstrong1998mapping, freund1998scheduling}.  These heuristics primarily focus on job execution time and machine availability, but often overlook the impact of data transfer time prior to job execution. To address this gap, context-aware methods have been proposed \citep{mcclatchey2007data, LI2016119, sahni2019data}, explicitly incorporating data transfer time into scheduling optimization. Despite this progress, these methods still assume static data availability, missing the optimization of the data allocation policy.

\subsection{Data Allocation}
Effective dynamic data allocation strategies have been shown to be significant in improving the efficiency of grid computing systems \citep{ALLCOCK2002749, bell2002simulation, tang2006impact, Venugopal2004grid}. Classical heuristics, such as Least Recently Used (LRU), Least Frequently Used (LFU) \citep{podlipnig2003cache}, and Latest Access Largest Weight (LALW) \citep{Ruay2008dynamic}, base data replication decisions on historical access patterns. In practice, data allocation strategies also need to balance storage overhead with access time, employing methods like adaptive replication \citep{fadaie2012replica} and hierarchical replication \citep{shorfuzzaman2010adaptive}. Additional strategies could also be developed to dynamically determine the number of data copies \citep{ranganathan2002improve}. In our study, we focus on a two-tier hierarchy of remote and local storage nodes, emphasizing full data replication with a single copy for each data object.


\subsection{Joint Optimization of Job Scheduling and Data Allocation}
Despite significant advances in optimizing job scheduling and data allocation separately, their joint optimization remains challenging due to their interdependence. Traditional approaches often address one aspect with simplifying assumptions and then fix that strategy when optimizing the other \citep{CHANG2007846,7874167}. This sequential approach can create a chicken-and-egg problem, leading to suboptimal performance of the initially optimized strategy. To overcome these limitations, meta-heuristics such as genetic algorithms \citep{Phan05Co}, artificial bee colony optimization \citep{TAHERI20131564}, and particle swarm optimization \citep{liu2013swarm} have been used for joint optimization. Recent studies have also begun to explore deep reinforcement learning for this purpose \citep{wei2021joint1, wei2021joint2, ZENG2024121}. Our work is closely related to Ko el al. \citep{Ko2019MIQP} and Govardhan et al. \citep{TAHERI20131885}. The former utilizes MIQP, 
which clearly suffers from scaling issues when the problems are large. The latter relies on a Hopfield neural network to optimize job scheduling and data allocation alternatively, similar to our approach in using the decomposition strategy. But either its way of decomposition or the optimization method is less efficient than ours in terms of optimization computation within a fixed budget.  In our experiments, we have found that \name{} significantly outperforms both methods, as shown in Table \ref{tab:main_results}.