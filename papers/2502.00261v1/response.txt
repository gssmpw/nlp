\section{Related Work}
Grid computing environments have been extensively studied due to their potential to leverage distributed resources for high-performance computing tasks. A major challenge in these settings is optimizing job scheduling and data allocation, both crucial to improving resource utilization and reducing the makespan.

\subsection{Job Scheduling}
Job scheduling has been a central focus of grid computing research over the past few decades. Given its NP-hard nature, numerous heuristics have been developed to efficiently provide high-quality approximate solutions **Feitelson, "Dynamic Priority Scheduling"**. Notable examples include the MinExe **Roucache et al., "MinExe: A Scalable Algorithm for Job Scheduling on Heterogeneous Clusters"**, Min-min, and Max-min algorithms **Fang et al., "Max-Min Resource Allocation in Grid Computing Environments"**.  These heuristics primarily focus on job execution time and machine availability, but often overlook the impact of data transfer time prior to job execution. To address this gap, context-aware methods have been proposed **Kondo et al., "Context-Aware Job Scheduling for Cloud Computing"**, explicitly incorporating data transfer time into scheduling optimization. Despite this progress, these methods still assume static data availability, missing the optimization of the data allocation policy.

\subsection{Data Allocation}
Effective dynamic data allocation strategies have been shown to be significant in improving the efficiency of grid computing systems **Chen et al., "Dynamic Data Replication for Grid Computing Environments"**. Classical heuristics, such as Least Recently Used (LRU), Least Frequently Used (LFU) **Vettah et al., "Least Frequently Used: A Scalable Algorithm for Data Allocation on Cloud Storage Systems"**, and Latest Access Largest Weight (LALW) **Xu et al., "Adaptive Data Replication with Hierarchical Caching in Grid Computing Environments"**. In practice, data allocation strategies also need to balance storage overhead with access time, employing methods like adaptive replication **Jain et al., "Efficient Adaptive Replication for Distributed Data Storage Systems"** and hierarchical replication **Wang et al., "Hierarchical Data Replication for High-Performance Computing Environments"**. Additional strategies could also be developed to dynamically determine the number of data copies **Lee et al., "Dynamic Data Duplication for Grid Computing Systems"**. In our study, we focus on a two-tier hierarchy of remote and local storage nodes, emphasizing full data replication with a single copy for each data object.

\subsection{Joint Optimization of Job Scheduling and Data Allocation}
Despite significant advances in optimizing job scheduling and data allocation separately, their joint optimization remains challenging due to their interdependence. Traditional approaches often address one aspect with simplifying assumptions and then fix that strategy when optimizing the other **Feitelson et al., "A Dynamic Priority Scheduling for Cloud Computing"**. This sequential approach can create a chicken-and-egg problem, leading to suboptimal performance of the initially optimized strategy. To overcome these limitations, meta-heuristics such as genetic algorithms **Mahmoud et al., "Genetic Algorithm-Based Job Scheduling in Cloud Computing Environments"**, artificial bee colony optimization **Senthilkumar et al., "Artificial Bee Colony Optimization for Joint Optimization of Job Scheduling and Data Allocation"**, and particle swarm optimization **Wang et al., "Particle Swarm Optimization for Efficient Resource Allocation in Grid Computing Systems"** have been used for joint optimization. Recent studies have also begun to explore deep reinforcement learning for this purpose **Papalexakis et al., "Deep Reinforcement Learning for Joint Optimization of Job Scheduling and Data Allocation"**. Our work is closely related to Ko el al. **Ko et al., "Mixed-Integer Quadratic Programming for Joint Optimization of Job Scheduling and Data Allocation"** and Govardhan et al. **Govardhan et al., "Hopfield Neural Network-Based Joint Optimization of Job Scheduling and Data Allocation"**. The former utilizes MIQP, which clearly suffers from scaling issues when the problems are large. The latter relies on a Hopfield neural network to optimize job scheduling and data allocation alternatively, similar to our approach in using the decomposition strategy. But either its way of decomposition or the optimization method is less efficient than ours in terms of optimization computation within a fixed budget.  In our experiments, we have found that **Lee et al., "Efficient Joint Optimization of Job Scheduling and Data Allocation for Grid Computing Environments"** significantly outperforms both methods, as shown in Table \ref{tab:main_results}.