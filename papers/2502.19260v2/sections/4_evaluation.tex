\section{Evaluation}
\label{sec:eval_protocol}

This section presents the experimental results conducted on all three task-specific datasets. For each set of experiments, we provide a description of the evaluated models, specify the evaluation protocol, outline the implementation details, and report the results. 

\begin{table*}[t!]
\centering
\caption{Off-shelf vs Fine tuned YOLOX-L detection results (the best results are highlighted in bold)}
\begin{tabular}{|p{3cm}|p{2cm}|c|c|c|c|c|}
\hline
\multirow{2}{*}{\textbf{Detector}} & \multirow{2}{*}{\textbf{Object Class}} & \multicolumn{5}{c|}{\textbf{Evaluation Metrics}} \\
\cline{3-7}
& & \textbf{Precision↑} & \textbf{Recall↑} & \textbf{F1Score ↑} & \textbf{FP↓} & \textbf{FN↓} \\
\hline
\multirow{4}{3cm}{\textit{YoloX Off-shelf}} 
& Vehicle & \textbf{90.14} & 23.59 & 37.39 & 2,669 & 79,002  \\
& Pedestrian & 65.59 & 10.33 & 17.84 & 255  & 4,221 \\
& Motorbike &  \textbf{96.15} &  4.61 &  8.79 & 4 & 2,071  \\
& Cyclist & 50.00 & 0.65 & 1.28 & 1 & 153 \\
\cline{2-7}
& \textbf{overall} & 89.50 & 22.62 & 36.11 & 2,929 & 85,447 \\ 
\cline{1-7}

\multirow{4}{3cm}{\textit{YoloX fine-tunned}}
& Vehicle & 85.53 & \textbf{66.87} & \textbf{75.06} & 11,692 & \textbf{34,257} \\
& Pedestrian & \textbf{82.76} & \textbf{73.13} & \textbf{77.64} & 717 & \textbf{1,265}  \\
& Motorbike & 93.12 & \textbf{56.70} & \textbf{70.48} & 91 & \textbf{940}  \\
& Cyclist & \textbf{93.63} &  \textbf{95.45} & \textbf{94.53} & 10 & \textbf{7} \\
\cline{2-7}
& \textbf{overall} & 85.53 &\textbf{66.97} & \textbf{75.12} & 12,510 & \textbf{36,469}   \\ 
% \hline
\hline 
\end{tabular}
\label{tab:detection-eval}
\end{table*}

\begin{table*}[t!]
\centering
\caption{Tracking Evaluation Results for six Detector-Tracker Settings (the best results for fine-tuned setting between two trackers are highlighted in bold)}
\begin{tabular}{|p{0.5cm}|p{3cm}|p{2cm}|c|c|c|c|c|c|}
\hline
\multirow{2}{*}{\textbf{Det.}} & \multirow{2}{*}{\textbf{Tracker}} & \multirow{2}{*}{\textbf{Object Class}} & \multicolumn{6}{c|}{\textbf{Evaluation Metrics}} \\
\cline{4-9}
& & & \textbf{MOTA↑} & \textbf{HOTA↑} & \textbf{IDF1↑} & \textbf{FP↓} & \textbf{FN↓} & \textbf{IDs↓} \\
\hline

\multirow{8}{2.5cm}{\rotatebox{90}{\textbf{Ground Truth}}} & \multirow{4}{3cm}{\textit{BoT-SORT}} 
& Vehicle & 92.0 & 73.9 & 74.8 & 117,353 & 147,168 & 12,992 \\
& & Pedestrian & 84.9 & 70.9 & 76.8 & 4,158  & 6,674& 1,156 \\
& & Motorbike &  86.8 &  77.1 &  79.6 & 1,542 & 2,800 & 200 \\
& & Cyclist & 93.9 & 77.02 & 89.0  & 50 & 79 &  7\\
\cline{2-9}


& \multirow{4}{3cm}{\textit{ByteTrack}}
& Vehicle & 85.4 & 66.9 & 72.5 & 124,039 & 162,065 & 14,044 \\
& & Pedestrian & 70.6 & 59.8 & 68.0 & 4,676 & 9,493 & 1,742 \\
& & Motorbike & 83.0 & 71.4 & 76.8 & 1,723 & 3,180 & 208 \\
& & Cyclist & 73.9 &  60.3 & 75.8 & 61 & 194 &  16 \\
\hline

\multirow{8}{2.5cm}{\rotatebox{90}{\textbf{YoloX Off-shelf}}} & \multirow{4}{3cm}{\textit{BoT-SORT}}& Vehicle & 20.2 & 25.9 & 29.4 & 20,924 & 443,214 & 1,077 \\
& & Pedestrian & 6.6 & 14.1 & 14.0 & 757  & 22,656 &  53 \\
& & Motorbike & 3.7 & 8.3 & 5.9 & 135 & 10,944 &  10 \\
& & Cyclist & 0 & 6.5 & 6.4 & 40 & 573 & 1 \\


\cline{2-9}
& \multirow{4}{3cm}{\textit{ByteTrack}}
& Vehicle & 19.8 & 25.4 & 29.6 &  25,507 & 441,511 & 1,757\\
& & Pedestrian & 6.9 & 13.7 & 13.8 & 721 & 22,689 & 67  \\
& &  Motorbike & 3.7 & 8.5 & 5.9 & 157 & 10,940 & 13 \\
& & Cyclist & 0 & 3.6 & 3.5 & 27 & 583 & 0 \\
\hline 

\multirow{8}{2.5cm}{\rotatebox{90}{\textbf{YoloX fine-tunned}}} & \multirow{4}{3cm}{\textit{BoT-SORT}}& Vehicle & \textbf{72.7} & \textbf{57.6} &  \textbf{65.8} & \textbf{130,451} & \textbf{211,305} &  \textbf{11,009} \\
& & Pedestrian & \textbf{58.7} & \textbf{50.4} & \textbf{63.0} & \textbf{4,909} & \textbf{11,011} & \textbf{960} \\
& & Motorbike & \textbf{63.0} & \textbf{58.1} &  \textbf{68.2} & \textbf{1,255} & \textbf{4,804} & \textbf{127} \\
& & Cyclist & \textbf{64.8} & \textbf{54.5} & \textbf{69.9} & 74 & \textbf{235} & \textbf{6} \\
\cline{2-9}


& \multirow{4}{3cm}{\textit{ByteTrack}}
& Vehicle & 67.4 & 55.0 & 64.7 & 132,892 & 217,837 & 12,055 \\
& & Pedestrian & 48.5 & 44.8 & 55.4 & 5,175 & 13,171 & 1,361 \\
& & Motorbike & 60.7 & 56.2 & 65.9 & 1,377 & 5,063 & 113 \\
& & Cyclist & 45.8 & 40.0 & 56.3 & \textbf{57} & 339 & 14 \\

\hline 
\end{tabular}
\label{tab:tracking-eval}
\end{table*}

\subsection{Multi-Agent Tacking}
Multi-agent tracking experiments are conducted using Kalman filter-based trackers in a tracking-by-detection setup. Two SOTA trackers are evaluated to assess their ability to handle occlusions and detection errors, particularly when dealing with far objects.

\subsubsection{Trackers}
\begin{itemize}
     \item \textbf{ByteTrack} \cite{bytetrack} is a lightweight real-time multi-object tracking model employing a two-level data association method. ByteTrack processes both high-confidence and low-confidence detections, enabling more robust tracking while maintaining computational efficiency. The method first associates tracklets with high-confidence detections, then matches unmatched tracklets with low-confidence detections to recover lost tracks. 

    \item \textbf{BoT-SORT} \cite{BoT-SORT} is a multi-object tracker that enhances motion-based tracking through camera-motion compensation and an improved Kalman filter state representation. Like ByteTrack, it employs a two-level association strategy. The tracker employs a cascade matching approach that prioritizes motion information for data association, with an optional ReID extension that adds appearance features as a secondary matching cue.
\end{itemize}



\subsubsection{Evaluation Protocol}
We evaluate the trackers using three different detector configurations:
\begin{itemize}
\item  ground-truth detections assuming the presence of perfect detector, providing an upper bound on performance;
\item  off-the-shelf YOLO detector, representing readily available solutions;
\item fine-tuned YOLO detector, optimized for our specific use case.
\end{itemize}
Each detector configuration serves a distinct analytical purpose. Ground-truth detections allow us to evaluate the tracker's association algorithm under ideal conditions, isolating its core tracking capabilities from detection errors. The off-the-shelf YOLO detector helps assess tracker robustness under challenging conditions with missing and erroneous detections. Finally, the fine-tuned YOLO detector represents a more realistic production scenario, where the detector is optimized for the specific domain but still maintains some inherent errors.
We conduct the evaluation using F1-score, Identity Switches and Higher Order Tracking Accuracy (HOTA):
\begin{itemize}
\item \textit{Multi-Object Tracking Accuracy (MOTA)}: Evaluates overall tracking performance by accounting for false positives, false negatives, and identity switches.
\item \textit{False Positives (FP)}: Instances where the tracker incorrectly identifies an object that is not present in the ground truth (ghost detections).
\item \textit{False Negatives (FN)}: Cases where the tracker fails to identify an object present in the ground truth due to detection or tracking errors (missed detections).
\item \textit{Identity Switches (IDs)}: The total number of instances where a tracker incorrectly reassigns identities between objects.
\item \textit{Identity F1 Score (IDF1)}: Measures the tracker's ability to maintain consistent object identities, considering identity switches and fragmentation.
\item \textit{Higher Order Tracking Accuracy (HOTA)}: A comprehensive metric that balances detection and association accuracy, decomposed into detection accuracy (DetA) and association accuracy (AssA) components.

\end{itemize}
For evaluation purposes, we group detection classes into four superclasses: pedestrian, motorbike, cyclist, and vehicle. The vehicle superclass encompasses all other classes in the dataset, including cars, buses, small motorized vehicles, medium and large vehicles, and emergency vehicles. Tracking performance metrics are broke down for each superclass to provide a detailed analysis of tracker behavior across different object categories. The evaluation was done on the full dataset, i.e., including train and test splits.

\subsubsection{Implementation Details}
As an off-the-shelf detector, the YOLOX-L model from ByteTrack was utilized. For the fine-tuned setting, YOLOX-L was trained on a train-test split with an input image resolution of 1280×1280. Since the original implementations of both trackers were designed for single-class pedestrian tracking, we extended their capabilities to support multi-class MOT. Our implementation maintains separate tracking instances for each class, ensuring that object association occurs only between detections of the same class. This preserves the core tracking logic while enabling simultaneous tracking of multiple object categories.

All experiments followed our evaluation protocol using standard tracking metrics. To ensure reproducibility, we maintained consistent tracking parameters throughout testing. ByteTrack was configured with a detection confidence threshold of 0.5, a track buffer size of 10 frames, and an IoU matching threshold of 0.8. BOT-SORT used these same core parameters while incorporating additional high and low confidence thresholds of 0.6 and 0.1, respectively. These configurations were based on the trackers' original implementations and optimized for our specific tracking scenario while preserving their fundamental operational characteristics.


\subsubsection{Results}
After fine-tuning YOLOX-L on our dataset, we evaluated both the fine-tuned and off-the-shelf models as detection backends for tracking. Table \ref{tab:detection-eval} presents the per-class detection results. All evaluations used a detection confidence threshold of 0.4 and an NMS threshold of 0.5. While NMS is typically set higher, the densely packed objects and frequent bounding box overlaps in our dataset required a lower threshold to prevent excessive suppression. The fine-tuned YOLOX-L model significantly improved recall across all object classes, leading to higher F1 scores, particularly for pedestrians, motorbikes, and cyclists. The F1 score increased by approximately 2.5x across classes compared to the off-the-shelf model. While for vehicles, the precision dropped from 90.14 to 85.53, this trade-off resulted in a substantial reduction in false negatives (FN), cutting overall missed detections from 85,447 to 36,469. This improvement is crucial for tracking, where recall is essential for maintaining track consistency. Error analysis highlights a key limitation of the off-the-shelf model. Its high precision comes at the cost of poor recall, leading to frequent missed detections. This performance gap is particularly pronounced in challenging scenarios, such as high object density and adverse conditions like rainy nights. Fine-tuning on the train split enabled better detection robustness, especially for underrepresented classes.

Table \ref{tab:tracking-eval} presents tracking results across three detection settings and two tracking models, yielding six result combinations. BoT-SORT consistently outperforms ByteTrack in HOTA and MOTA across all settings, except for motorbike HOTA and pedestrian MOTA in the off-the-shelf setting. The largest performance gap is observed in the ground-truth setting for cyclists, where BoT-SORT achieves a MOTA improvement of 20 and HOTA improvement of 16.7 over ByteTrack. The low false negatives (FN) and high false positives (FP) in the off-the-shelf setting stem from the detector’s low recall, making it overly permissive and generating excessive detections with poor precision. For instance, in ByteTrack, FP for vehicles drops from 124,039 (ground-truth) to 20,924 (off-the-shelf), but FN increases more than 2.5×, from 162,065 to 441,511, confirming that the model struggles with false detections despite detecting more objects. 

This imbalance impacts ID assignment, as seen in BoT-SORT’s pedestrian IDs, which drop from 960 (fine-tuned) to 55 (off-the-shelf), indicating that the tracker aggressively associates detections by leveraging high recall. However, this comes at the cost of tracking false objects and ghost tracklets, leading to inconsistent ID management. The low IDF1 scores in BoT-SORT (68.2 and 69.9 in fine-tuned vs. 5.9 and 6.4 in off-the-shelf) further confirm this issue, as the tracker frequently initializes new tracklets instead of maintaining consistent tracking IDs. While the fine-tuned setting significantly improves over the off-the-shelf model, the performance gap between fine-tuned and ground-truth settings underscores tracking sensitivity to detector errors. For instance, in BoT-SORT, pedestrian MOTA drops from 84.9 (ground truth) to 58.7 (fine-tuned), and motorbike IDF1 declines from 76.8 to 58.1, showing that fine-tuning is beneficial but does not fully close the gap. These results suggest that further improvements in both detection and tracking are needed to enhance overall tracking stability, reduce fragmentation, and maintain consistent object identities.

\begin{table*}[t]
\centering
\caption{Trajectory Predictors Evaluation (Sequential)}
\resizebox{\textwidth}{!}{
\begin{tabular}{|c|p{3cm}|cc|cc|cc|cc|cc|cc|cc|}
\hline

& \multirow{2}{*}{\textbf{Predictor}} & \multicolumn{2}{|c|}{\textbf{10/10, 1s/1s}} &  \multicolumn{2}{|c|}{\textbf{20/10, 2s/1s}} &  \multicolumn{2}{|c|}{\textbf{10/20, 1s/2s}} &  \multicolumn{2}{|c|}{\textbf{20/20, 2s/2s}} &  \multicolumn{2}{|c|}{\textbf{20/30, 2s/3s}}  &  \multicolumn{2}{|c|}{\textbf{20/60, 2s/6s}} \\ \cline{3-14} 
& & \textbf{ADE↓} & \textbf{FDE↓} & \textbf{ADE↓} & \textbf{FDE↓} & \textbf{ADE↓} & \textbf{FDE↓} & \textbf{ADE↓} & \textbf{FDE↓} & \textbf{ADE↓} & \textbf{FDE↓} & \textbf{ADE↓} & \textbf{FDE↓}  \\ \hline

\multirow{3}{*}{\rotatebox{90}{\makecell{sliding\\window=1}}} & \textit{LSTM} & \textbf{10.49} & \textbf{19.63} & 9.52 & 17.62 & 19.20 & 42.43 & 17.46 & 38.21 &  25.86 & 59.84 & 46.27 &  105.68\\ 
& \textit{Transformer} & 10.66 & 20.49 & 9.23 & 17.29 & 19.48 & 43.05  &  18.45 & 40.70 & 27.86 & 64.40 &  46.88 &  105.47 \\ 
& \textit{Transformer+GMM} & 10.76 & 20.22 & 9.68 & 17.88 & 19.48 & \textbf{41.71}  & 18.36 & 39.14&26.36& 57.48 &\textbf{44.94} & 96.89 \\ \hline

\multirow{3}{*}{\rotatebox{90}{\makecell{sliding\\window=3}}} &  \textit{LSTM} & 10.55 & 19.87 &  9.50 & 17.40 & \textbf{19.18} & 42.10 & 17.08 & 37.09 & 24.94 &  57.58 & 45.81 & 103.67\\  
&  \textit{Transformer}  &11.13&21.49 &10.19  &19.24  &20.66 &47.53  &18.49  &40.53  &28.21 &64.63  & 46.06 & 101.31 \\ 
&  \textit{Transformer+GMM} & 10.77 & 20.24 & 9.74 & 17.97 & 19.80 & 41.99 & 17.61 & 37.50 & 26.86 & 57.12 & 45.46 & \textbf{93.28} \\   \hline

\multirow{3}{*}{\rotatebox{90}{\makecell{sliding\\window=5}}} &  \textit{LSTM} & 10.77 & 20.12 & \textbf{9.20} &  \textbf{16.84} & 19.54 & 43.10 & \textbf{17.04}  & 37.02 &  \textbf{24.91} & 57.06 & 45.92 & 104.84  \\ 
&  \textit{Transformer}  & 11.30 & 21.71  & 9.72  & 18.47 & 21.92  & 52.03  & 19.86  & 45.57 & 27.64 & 63.30  & 47.31 & 105.54  \\  %23.27 43.92 -> seed 42 -> 24.78 48.42 
&  \textit{Transformer+GMM} & 10.99 & 20.55 & 9.85 & 18.02 & 20.05 & 42.68 & 17.40 & \textbf{36.47} & 25.79 & \textbf{56.54} &  45.67& 93.95  \\   \hline
\end{tabular}
}
\label{tab:prediction_results}
\end{table*}



\begin{table*}[t]
\centering
\caption{Trajectory Predictors Evaluation (Multimodal, Sequential)}
\resizebox{\textwidth}{!}{
\begin{tabular}{|c|p{3cm}|cc|cc|cc|cc|cc|}
\hline

& \multirow{2}{*}{\textbf{Predictor}} & \multicolumn{2}{|c|}{\textbf{10/10, 1s/1s}} &  \multicolumn{2}{|c|}{\textbf{10/20, 1s/2s}} &  \multicolumn{2}{|c|}{\textbf{20/30, 2s/3s}}  &  \multicolumn{2}{|c|}{\textbf{20/60, 2s/6s}} \\ \cline{3-10} 
& & \textbf{MinADE$_5$↓} & \textbf{MinFDE$_5$↓} & \textbf{MinADE$_5$↓} & \textbf{MinFDE$_5$↓} & \textbf{MinADE$_5$↓} & \textbf{MinFDE$_5$↓} & \textbf{MinADE$_5$↓} & \textbf{MinFDE$_5$↓}  \\ \hline

\makecell{sliding\\window=1} & \textit{Transformer+GMM} & \textbf{7.19} & \textbf{12.01} & \textbf{13.11} & \textbf{24.75}  & 19.93 & 41.42  & \textbf{32.30} & \textbf{62.23}   \\ \hline
\makecell{sliding\\window=3} & \textit{Transformer+GMM}  &  7.27&12.19  &14.11  &27.82 & 20.42& 40.44  &35.61 &67.69  \\  \hline
\makecell{sliding\\window=5} &  \textit{Transformer+GMM} &7.49 &12.72 &13.19 &25.56 & \textbf{17.32} & \textbf{34.24} &35.49 &66.41  \\  \hline
\end{tabular}
}
\label{tab:prediction_results_multimodal}
\end{table*}



\begin{table*}[t]
\centering
\caption{Trajectory Predictors Evaluation (Frame-based)}
\resizebox{0.8\textwidth}{!}{%
\begin{tabular}{|c|p{3cm}|cc|cc|cc|cc|cc|}
\hline
& \multirow{2}{*}{\textbf{Predictor}} & \multicolumn{2}{|c|}{\textbf{video\_122233 }} &  \multicolumn{2}{|c|}{\textbf{video\_155425}} &  \multicolumn{2}{|c|}{\textbf{video\_160325}} &  \multicolumn{2}{|c|}{\textbf{video\_125233}} &  \multicolumn{2}{|c|}{\textbf{video\_204347}}  \\ \cline{3-12} 
& & \textbf{ADE↓} & \textbf{FDE↓} & \textbf{ADE↓} & \textbf{FDE↓} & \textbf{ADE↓} & \textbf{FDE↓} & \textbf{ADE↓} & \textbf{FDE↓} & \textbf{ADE↓} & \textbf{FDE↓} \\ \hline
\multirow{6}{*}{\rotatebox{90}{prediction horizon=10}} 
 & \textit{LSTM} & \textbf{7.55} & \textbf{13.94} &  \textbf{30.92} &  \textbf{56.57} & 25.16  & 44.83  & \textbf{9.04}  & \textbf{17.1}  & \textbf{17.73}   & \textbf{34.73} \\ 
 & \textit{Transformer}  &  8.28 & 15.77 &  31.88 & 58.12  & \textbf{24.68} & \textbf{44.48} & 9.71 & 18.69 & 23.28 & 47.82 \\
 & \textit{GCN} & 9.57 & 16.18 & 31.92 & 56.93 & 27.62 & 47.95 & 11.7 &  21.28  &  24.51 & 46.13 \\
 & \textit{GCN$_{temporal}$}  & 10.2 & 19.1 & 31.62 & 58.72 & 26.32 & 46.58 & 12.5 &  23.64 &  23.88 & 47.30  \\ 
 & \textit{GAT} & 10.13 & 17.91 & 33.18 & 59.62 & 26.98 & 47.32 & 11.98 & 22.02 & 25.28 & 49.46  \\
 & \textit{GAT$_{temporal}$}  & 10.54 & 19.77 & 32.11  & 59.43 & 26.14 & 47.16 & 13.1 & 24.75 & 24.76 & 49.08  \\ \hline
\multirow{6}{*}{\rotatebox{90}{prediction horizon=20}} 
 & \textit{LSTM} &  \textbf{14.5} & \textbf{32.63} &  60.83 & 125.53 & 47.15 & 96.83  & \textbf{18.18}  & \textbf{40.69} &  \textbf{35.99}  & \textbf{85.21} \\ 
 & \textit{Transformer}  & 17.3 & 40.5 & 65.59 & 132.95 & 50.44  & 103.07 & 19.46 & 43.43 &  48.51 & 116.02 \\
 & \textit{GCN} & 17.94 & 35.22 & 66.05 & 132.18 &  53.9 & 102.78  & 23.74  & 47.89  &  49.69 & 108.02  \\
 & \textit{GCN$_{temporal}$}  & 18.88  & 41.38 & 65.55 & 133.08 & 50.18 & 101.55 & 24.07 & 51.98 & 46.13 & 105.42 \\ 
 & \textit{GAT} & 17.17 & 38.33 & \textbf{58.6} & \textbf{119.8} & \textbf{44.29} & \textbf{88.44} & 21.77  & 46.03 & 48.2 & 112.45\\
 & \textit{GAT$_{temporal}$}  & 19.62 & 42.94 & 65.27 & 133.09 & 51.57& 104.84 & 24.29 &  53.46 & 47.31 & 109.49  \\ \hline
\end{tabular}%
}
\label{tab:prediction_results_frame}
\end{table*}




\subsection{Trajectory Prediction}
\label{sec:Prediction}

For trajectory prediction, three deep-learning architectures are evaluated, each differing in their ability to capture temporal dependencies and interaction dynamics. 

\subsubsection{Predictors}
\begin{itemize}
    \item \textbf{LSTM-based models} assume that motion follows temporally causal dependencies, making recurrent networks a common choice for trajectory prediction. By leveraging hidden states to incorporate past information, LSTMs capture dynamic motion properties and temporal correlations in prediction sequences.
    
    \item \textbf{Transformer-based models} employ self-attention layers within an encoder-decoder architecture to capture dependencies comprehensively. The encoder processes input trajectories with positional encodings to preserve temporal order, while self-attention mechanisms compute pairwise relationships across timesteps. We adopt the Transformer configuration proposed by \cite{Murad}, which integrates encoder-decoder Transformer layers to model sequential dependencies from observed trajectories, combined with a Mixture Density Network (MDN) to estimate Gaussian Mixture Model (GMM) parameters. For generating a deterministic unimodal trajectory, the mean ($\mu$) of the mixture component with the highest mixing coefficient ($\pi$) at each timestep is selected. For multimodal predictions, trajectory points can either be sampled from the mixtures or taken directly as their mean values ($\mu$), considering only mixtures with mixing coefficients $\pi > \tau$. The trajectory module then generates paths by connecting these points across time steps, where each point at time $t+1$ is connected to its closest neighbor at time $t$.
    
    \item \textbf{Graph Neural Networks (GNNs)} model complex spatial and temporal interactions by operating on graphs constructed from each scene, where nodes represent traffic agents and edges encode pairwise relationships, such as proximity. Each node contains information about an agent's past trajectory. We evaluate two message-passing strategies: \textbf{Graph Convolutional Networks (GCNs)}, which uniformly aggregate information from neighboring nodes, and \textbf{Graph Attention Networks (GATs)}, which dynamically compute attention weights to prioritize more influential interactions. Both GNNs are tested in two configurations: (1) \textit{sequence-to-sequence}, where the past trajectory is flattened and directly used for future trajectory prediction, and (2) \textit{temporal}, where a GNN encoder first captures spatial interactions, followed by an LSTM encoder-decoder to model temporal dynamics and generate future trajectories in an autoregressive manner.
  
\end{itemize}


\begin{figure*}[h!]
\centering
\includegraphics[width=1.0\textwidth]{figures/prediction_samples.pdf}
\caption{Samples of predicted trajectories: ground truth trajectories are shown in red, while predictions are depicted in green. The predicted trajectories accurately capture the agent's heading but exhibit lower accuracy in predicting speed, which in turn affects the precise estimation of future trajectory locations.}
\label{fig:prediction_samples}
\end{figure*}


\subsubsection{Evaluation Protocol}
Each predictor is trained on ground truth past trajectories. To compare performance, we compute the average and final displacement errors for each prediction setting in a unimodal mode, as well as for the Transformer coupled with the GMM model in a multimodal output setting. Let $\zeta_{i,t}$ and $\hat{\zeta}_{i,t}$ denote the predicted and ground truth trajectories for the $i$-th agent at time step $t$, respectively.

\begin{itemize}
  \item \textit{Average Displacement Error (ADE)} measures the average $\ell_2$ distance between predicted and ground truth trajectories over the entire prediction horizon $\Delta t$:
    \begin{equation*}
        ADE = \frac{1}{n \times \Delta t} \sum_{i=1}^{n} \sum_{t=1}^{\Delta t} ||\zeta_{i,t} - \hat{\zeta}_{i,t}||
    \end{equation*}

    For multimodal predictions:
    \begin{equation*}
        MinADE_k = \frac{1}{n \times \Delta t} \min_{k} \sum_{i=1}^{n} \sum_{t=1}^{\Delta t} ||\zeta^k_{i,t} - \hat{\zeta}_{i,t}||
    \end{equation*}

  \item \textit{Final Displacement Error (FDE)} measures the average $\ell_2$ distance between predicted and ground truth trajectories at the final time step $\Delta t$:
    \begin{equation*}
        FDE = \frac{1}{n} \sum_{i=1}^{n} ||\zeta_{i, \Delta t} - \hat{\zeta}_{i, \Delta t}||
    \end{equation*}

    For multimodal predictions:
    \begin{equation*}
        MinFDE_k = \frac{1}{n} \min_{k} \sum_{i=1}^{n} ||\zeta^k_{i, \Delta t} - \hat{\zeta}_{i, \Delta t}||
    \end{equation*}
\end{itemize}

For multimodal evaluation, the metric computes the $\ell_2$ distance between the ground truth trajectory and the closest prediction among $k$ possible trajectories, with $k$ set to 5.
\subsubsection{Implementation Details}
All predictors operate on normalized relative distances, computed as differences between absolute positions and standardized using the mean and standard deviation from the training dataset. This normalization strategy mitigates the impact of varying resolutions and improves training convergence. Both LSTM and Transformer models process sequences in the format \([past\_trajectory]\ [future\_trajectory]\).

The LSTM implementation uses two layers with a hidden dimension of 128, trained with a batch size of 128 for 50 epochs. Our transformer architecture features three encoder-decoder layers, each employing four attention heads and an embedding dimension of 128. To maintain temporal causality during training, the decoder employs a masked attention mechanism on shifted target trajectories, while the encoder processes the past trajectory input. At inference time, both models autoregressively generate predictions using outputs from previous time steps, maintaining the normalized relative position representation. All models are trained using Mean Squared Error (MSE) loss. For the transformer, we adapt the learning rate scheduler parameters and warmup period to accommodate significantly different dataset sizes resulting from various sliding window configurations. Gradient clipping ensures stable training across these configurations. The final predicted trajectories are denormalized to absolute positions before computing the ADE and FDE metrics.

The transformer-GMM implementation extends the standard transformer with a specialized Gaussian Mixture Model component for probabilistic trajectory prediction. The architecture features three encoder-decoder layers with four attention heads and an embedding dimension of 128. The GMM layer is implemented through three parallel networks, each structured with three fully-connected layers using ELU activations and progressively decreasing widths (from model dimension to a hidden size of 32, then to 24, and finally to 16 units). These networks specialize in generating different GMM parameters: mixture weights ($\pi$), means ($\mu$), and covariance matrices ($\sigma$) for six two-dimensional Gaussians. During training, a masked attention mechanism in the decoder is employed while passing zeros (zero conditioning) as inputs rather than shifted targets. The model is trained for 100 epochs using negative log-likelihood (NLL) loss, with adaptive learning rate scheduling and gradient clipping to ensure stability. At inference time, the model produces complete future trajectories in a single forward pass, with the GMM outputs fed into a future trajectory building algorithm that prioritizes high-probability components while maintaining alternative paths.


For GNN models, the input is reformatted into a frame-centered structure. For each frame, the data is rearranged such that all objects contained within the frame are aggregated along with their past and future trajectories. If the available past or future trajectories are shorter than the predefined observation length and future horizon, the corresponding object is disregarded for that frame. To feed frame-based data into GNNs, the parameter \texttt{max\_num\_nodes} is set to 50. This is necessary since PyTorch's GNN implementation operates with a static adjacency matrix. To handle the varying number of agents across frames, masking is implemented. Each sample consists of a fixed number of nodes representing agents in a scene and includes past trajectory features, an adjacency matrix, future trajectory targets, and a mask indicating valid nodes.

To construct the mask, given a batch with a variable number of agents \( n \) (up to \texttt{max\_num\_nodes}), the mask is computed as:

\[
M_i =
\begin{cases}
1, & \text{if } i \leq n, \\
0, & \text{otherwise}.
\end{cases}
\]

This mask is applied during loss computation to prevent gradient updates on padded entries. Edges in the adjacency matrix are established if the pairwise distance between nodes is below a predefined threshold, which is set to 100. To ensure stable training and effective message passing, the adjacency matrix is symmetrically normalized:

\[
\hat{A} = D^{-\frac{1}{2}} (A + I) D^{-\frac{1}{2}},
\]

where \( D \) is the degree matrix. This normalization prevents numerical instability caused by scale differences between nodes and ensures effective information propagation across the graph. The sequence-to-sequence models consist of two GNN encoder and decoder layers with a hidden dimension of 256, trained with a batch size of 32 for 50 epochs. The temporal variation of the models consists of two GNN encoder layers with a hidden dimension of 256, two LSTM encoders and decoders  with a hidden dimension of 2128. A masked Mean Squared Error (MSE) loss is used to ensure that missing or padded nodes do not contribute to the loss function:

\[
\mathcal{L} = \frac{1}{\sum_{b,n} M_{b,n}} \sum_{b,t,n} M_{b,n} \| \hat{V}_{b,t,n} - V_{b,t,n} \|^2,
\]

where \( M_{b,n} \) is a mask indicating valid nodes, \( \hat{V}_{b,t,n} \) is the predicted velocity, and \( V_{b,t,n} \) is the ground-truth velocity.



\subsubsection{Results}

Table \ref{tab:prediction_results} presents the evaluation results of LSTM, Transformer, and Transformer coupled with GMM on prediction datasets with varying prediction horizons, observed trajectory lengths, and sliding window settings. The best results for each column are highlighted in bold, indicating which model and sliding window configuration performed best. For longer prediction horizons (i.e., 20, 30, and 60), the Transformer coupled with GMM outperforms the other models in terms of final displacement error (FDE). In contrast, for an observed trajectory of 10 and a prediction horizon of 1, LSTM achieves the best performance. The first two settings in the table (10/10 and 20/10) test the hypothesis that a longer observed trajectory improves prediction accuracy. This hypothesis is supported by the results, as using an observed trajectory of 20 achieves an ADE of 9.20 compared to 10.49 for an observed trajectory of 10. Similarly, the FDE improves from 19.63 to 16.84. A similar trend is observed in the third and fourth columns for a prediction horizon of 20 with observed trajectories of 10 and 20. The best settings demonstrate an FDE improvement of 5.36 and an ADE improvement of 2.04 when increasing the observed trajectory length from 10 to 20.

Table \ref{tab:prediction_results_multimodal} presents the results of the multimodal output evaluation, with the best results highlighted in bold across different sliding window settings. The model performed best when trained using a sliding window of 1. Compared to the unimodal setting, the multimodal approach demonstrates a clear advantage. For instance, in the 20/60 split, the best ADE in the unimodal setting is 44.94, whereas the multimodal model achieves 32.30. Similarly, FDE improves from 93.28 (unimodal) to 62.23 (multimodal), representing a reduction of over 30. In the simplest setting (10/10), the multimodal model outperforms LSTM, achieving a gain of 3.3 in ADE and 7.6 in FDE. Further, Table \ref{tab:prediction_results_frame} presents the results of frame-based evaluation for prediction horizons of 10 and 20. To better reflect real-world applications, objects without a sufficiently long past trajectory, matching the model's required observation length, are padded with zeros to simulate newly detected objects entering the scene. For a prediction horizon of 10, LSTM demonstrates the most robust performance across all test videos, except for the third sample, where the Transformer achieves a better score by 0.48 in ADE and 0.35 in FDE. In the case of a longer prediction horizon, GAT outperforms LSTM in the second and third samples, achieving more tangible improvements, with an average gain of 2.5 in ADE and 7.06 in FDE. Figure \ref{fig:prediction_samples} illustrates examples of predicted trajectories, where LSTM-generated predictions in the 10/10 setting are shown with green arrows, while ground truth trajectories are depicted in red. The predictions accurately capture the agent's heading but show lower accuracy in precise location estimation, particularly struggling to adapt to varying agent speeds.


\subsection{Intention Prediction}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Intention Evaluation

\begin{table*}[t]
\centering
\caption{Intention Prediction Evaluation (I - vanilla setting and II - autoregressive mode).}
\resizebox{\textwidth}{!}{
\begin{tabular}{|p{1.3cm}|p{2cm}|c|c|c|c|c|c|}
\hline
\multirow{2}{*}{\makecell{\textbf{Prediction} \\ \textbf{Setting}}} & \multirow{2}{*}{\textbf{Intention}} & \multicolumn{2}{|c|}{\textbf{10/10, 1s/1s}} &  \multicolumn{2}{|c|}{\textbf{15/10, 1.5s/1s}} &  \multicolumn{2}{|c|}{\textbf{20/10, 2s/1s}}  \\ \cline{3-8}
 & & \textbf{F1-score$_{all}$(\%)↑} & \textbf{F1-score$_{last}$(\%)↑} & \textbf{F1-score$_{all}$(\%)↑} & \textbf{F1-score$_{last}$(\%)↑} & \textbf{F1-score$_{all}$(\%)↑} & \textbf{F1-score$_{last}$(\%)↑}  \\ \hline
 
\multirow{11}{*}{\textit{LSTM}$_{I}$} & reversing & 4.73 & 4.98 & 54.72 & 50.27 & \textbf{75.76} & \textbf{78.22} \\ 
& turn-right & 22.28 & 22.48 & 57.94 & 56.66 & \textbf{78.33} & \textbf{76.76} \\ 
& turn-left & 17.22 &  17.17 & 57.99 & 56.79 & \textbf{76.56} & \textbf{73.36}  \\ 
& merge-right & 11.57 & 11.31 & 52.95 & 51.54 & \textbf{76.07} & \textbf{74.41} \\ 
& merge-left & 20.37 &  19.07 & 58.46 & 56.54& \textbf{78.81} & \textbf{76.63} \\   
& braking & 1.66 & 1.76 & 20.39 & 19.11 & \textbf{34.75} & \textbf{34.49} \\ 
& stopped & 68.66 & 68.18 & 84.08 & 83.61 & \textbf{90.62} & \textbf{90.07} \\ 
& lane-keeping & 89.99 & 89.92 & 94.74 & 94.61 & \textbf{96.95} & \textbf{96.77}  \\
& walking & 32.82 &  33.33 & 70.74 & 70.86 & \textbf{86.44} & \textbf{86.48}  \\
& crossing & 38.59 & 37.43 & 69.06 & 67.29 & \textbf{85.96} & \textbf{85.74} \\ 
& waiting\_to\_cross & 24.29 &  23.41 & 63.87 & 63.3 & \textbf{83.55} & \textbf{83.24}  \\ \cline{2-8}
& \textbf{overall score} & 30.2 & 29.91 & 62.27 & 60.96 & \textbf{78.53} & \textbf{77.83} \\ \cline{2-8}
& \textbf{$D_{L_{norm}}$} & \multicolumn{2}{|c|}{0.1708} &  \multicolumn{2}{|c|}{0.09} & \multicolumn{2}{|c|}{\textbf{0.0517}} \\ \hline

\multirow{11}{*}{\textit{LSTM}$_{II}$}  & reversing  & 12.70 & 10.53 & 50.75 & 47.61 & 72.82 & 73.31 \\ 
& turn-right & 18.23 & 17.91 & 59.33 & 58.67 & 75.62 & 73.58  \\ 
& turn-left & 15.65 & 15.12 & 56.29 & 54.92 & 73.08 & 72.26 \\ 
& merge-right & 12.58 & 12.37 & 52.66 & 51.60 & 73.67 & 72.27 \\ 
& merge-left & 20.21 & 19.46 & 57.92 & 56.73 & 77.01& 74.77 \\ 
& braking & 2.99 & 3.36 & 16.05 & 16.22 & 33.48 & 32.53 \\
& stopped & 68.47 & 67.92 & 83.55 & 83.06 & 90.26 & 89.84\\ 
& lane-keeping & 89.93 & 89.85 & 94.66 & 94.55 & 96.83& 96.69\\
& walking & 31.93 & 32.51 & 69.44 & 69.91 & 83.76 & 83.92 \\ 
& crossing & 36.03 & 35.16 & 69.88 & 68.42 & 82.55 & 81.86 \\ 
& waiting\_to\_cross & 23.91 & 23.83 & 61.29 & 60.55 & 79.01 & 80.03\\ \cline{2-8}
& \textbf{overall score} & 30.24 & 29.82 & 61.08 & 60.20 & 76.19 & 75.55 \\ \cline{2-8}
& \textbf{$D_{L_{norm}}$} & \multicolumn{2}{|c|}{0.1716} & \multicolumn{2}{|c|}{0.0917} & \multicolumn{2}{|c|}{0.0544} \\ \hline
\end{tabular}
}
\label{tab:intention_results}
\end{table*}
 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{figure*}[h!]
\centering
\includegraphics[width=1.0\textwidth]{figures/intention_prediction_samples_day.pdf}
\caption{Samples of predicted intentions during daytime. The ground truth intention for the next timestamp is shown in orange, while predictions are in purple. Red rectangles highlight misclassifications, such as predicting "keep\_lane" for walking and crossing, failing to predict "merge," and incorrectly assigning "stop" to a reversing vehicle. Green indicates correctly predicted intentions, including accurate "keep\_lane" predictions on the main road, successful "merge" predictions from the right road, and correct stopping behavior at crossings.}
\label{fig:intention_day}
\end{figure*}

\begin{figure*}[h!]
\centering
\includegraphics[width=1.0\textwidth]{figures/intention_prediction_samples_night.pdf}
\caption{Samples of predicted intentions during nighttime. The ground truth intention for the next timestamp is shown in orange, while predictions are in purple. Red rectangles highlight misclassifications, such as predicting "stop" for moving vehicles and failing to predict "merge" and "turn." Green indicates correctly predicted intentions, including accurate stopping behavior of vehicles at intersections.}
\label{fig:intention_night}
\end{figure*}


For intention prediction, the experiments involve providing the model with a past trajectory and generating future intentions at each timestamp over a prediction horizon of 10. The core evaluation model is an LSTM trained in a manner to classify and predict the most probable intention from a set of 11 classes. The model is evaluated in two settings: \textit{vanilla}, where predicted intentions are assumed to be independent, and \textit{autoregressive}, where dependencies between consecutive predictions are considered. For instance, if a vehicle has stopped at a junction, it is less likely to change lanes immediately upon resuming movement; instead, it is more probable that it will continue with a lane-keeping intention. Similarly, for pedestrians, a "waiting to cross" intention is likely to be followed by "crossing".

\subsubsection{Predictors}
\begin{itemize}
    \item \textbf{LSTM$_{I}$ model} corresponds to the vanilla setting, where the decoder generates the entire sequence of future intentions in a single forward pass. Predictions are evaluated against the ground truth using CrossEntropyLoss.
    \item \textbf{LSTM$_{II}$ model} operates in an autoregressive mode with the capability of teacher forcing during training. The decoder predicts intentions one timestep at a time, appending the previous output as input for the next step. During inference, the model relies entirely on its own predictions, with teacher forcing set to zero.
\end{itemize}

\subsubsection{Evaluation Protocol}
Intention prediction is formulated as a classification task, where the intention is predicted over a 10-timestep horizon and the evaluation is conducted using 5-fold cross-validation. To assess the impact of the observed trajectory length on prediction accuracy, we evaluate the model under three settings, each with a fixed future horizon of 10 frames: using the past \textit{10}, \textit{15}, and \textit{20} frames as input. The models' performance is evaluated using the following metrics:

\begin{itemize}
    \item \textit{F1-score for all timestamps:} This score is computed over every token in the predicted sequence by flattening all timesteps. The F1-scores are computed both per class and overall using macro averaging, which treats each class equally. This ensures that the performance on underrepresented classes is adequately reflected.

    \item \textit{F1-score for the last token:} In addition to the overall F1-score, we evaluate the robustness of the prediction by computing the F1-score for the last token (i.e., the final intention) of the sequence. This score is computed by comparing the final token in  predicted intention sequence to the corresponding token in the ground truth intention sequence, with macro averaging applied in the same manner as for the overall F1-score.

    \item \textit{Average Normalized Levenshtein Distance \cite{4160958}:} To assess the performance at the sequence level, we compute the Levenshtein distance between the ground truth sequence (\( s^{gt} \)) and the predicted sequence (\( s^{pr} \)) . The Levenshtein distance, denoted \( D_L \), is defined as the minimum number of single-token edits (insertions, deletions, or substitutions) required to transform the grounf truth sequence to predicted. A normalized Levenshtein Distance ($D_{L_{\text{norm}}}$), when equals 0 indicates a perfect match, while values closer to 1 mean larger discrepancies. To compute the distance, let \( D(i, j) \) denote the Levenshtein distance between the first \( i \) tokens of \( s^{gt} \) and the first \( j \) tokens of \( s^{pr} \). The recursive formulation is given by
    \begin{equation*}
        \resizebox{0.93\columnwidth}{!}{$
        D(i, j) =
        \begin{cases}
            \max(i, j), & \text{if } \min(i, j) = 0, \\[8pt]
            \min \begin{cases}
                D(i-1, j) + 1, \\
                D(i, j-1) + 1, \\
                D(i-1, j-1) + \delta(s^{gt}_i, s^{pr}_j)
            \end{cases}, & \text{otherwise}.
        \end{cases}
            $}
    \end{equation*}, where \( s^{gt}_i \) as the \( i \)-th token of the ground truth sequence, \( s^{pr}_j \) is the \( j \)-th token of the predicted sequence and $\delta$ is defined as:  
    \begin{equation*}
        \delta(s^{gt}_i, s^{pr}_j) =
        \begin{cases}
            0, & \text{if } s^{gt}_i = s^{pr}_j \\
            1, & \text{if } s^{gt}_i \neq s^{pr}_j
        \end{cases}
    \end{equation*}
    
    The computed distance is then normalized by dividing by the length of the ground truth sequence \( |s^{gt}| \):
    \begin{equation*}
        D_{L_{\text{norm}}}(s^{gt}, s^{pr}) = \frac{D_L(s^{gt}, s^{pr})}{|s^{gt}|}.
    \end{equation*}
\end{itemize}


\subsubsection{Implementation Details}
Both models were trained with a batch size of 128, using two hidden layers of size 128, over 50 epochs. Two evaluation settings are implemented: (1) a train/test split, similar to that used in trajectory prediction, and (2) \(k\)-fold cross-validation. Cross-validation was conducted to provide a comprehensive assessment of model performance, given the diversity of the dataset and the fact that the models rely solely on positional data. For a more thorough evaluation and testing generalization of prediction models, especially when transferring models trained on other intention datasets to EMT, the train/test split option is also provided and recommended. Additionally, for the LSTM autoregressive mode, a teacher forcing ratio can be set, allowing for experimentation with the degree of ground truth feedback introduced during training.


\subsubsection{Results}
Table~\ref{tab:intention_results} presents the results of the conducted experiments, detailing the performance across both model settings, intention classes, and past trajectory settings. The highest performance is achieved by the vanilla model configuration, surpassing the autoregressive setting by nearly 2\% in F1-score across all timestamps. The results indicate that when only relative displacement is utilized for prediction, the model fails to capture temporal dependencies between intentions. Intentions such as "waiting\_to\_cross" and "crossing" should exhibit sequential dependencies. However, as shown in Figure~\ref{fig:intention_day}, the model incorrectly predicts the "keep\_lane" intention for crossing scenarios. This misclassification occurs due to the similarity between lane-keeping motion patterns and the vehicle-relative displacement of crossing pedestrians. Additionally, since the model does not incorporate object type information or associated intention priors, it is unable to distinguish between different entities. 

A similar issue is observed for "waiting\_to\_cross," as shown in Figure~\ref{fig:intention_night}, where the model does not differentiate between a pedestrian standing at a crossing and one waiting at a bus stop. This limitation arises from the absence of visual cues in the feature set. Additionally, since the model primarily relies on trajectory patterns, an object may appear to be moving due to shifts in its bounding box caused by the camera perspective, despite being stationary in the real world. As a result, the model incorrectly classifies the object as dynamic. A clear trend observed in the results is the significant improvement in performance with increasing past trajectory length. The most frequent intention, "keep\_lane," benefits from an improvement of approximately 7\%, while "reversing" shows a performance gain of up to 60\%. This suggests that maneuver-based intentions strongly depend on extended historical context. Furthermore, the reduction in normalized Levenshtein distance indicates that increasing the length of observed trajectory improves sequence-level accuracy.

Future research should investigate augmenting the feature vector beyond velocity-based inputs, incorporating visual cues, and balancing the dataset to improve performance for underrepresented classes, e.g., "braking" and "reversing."



