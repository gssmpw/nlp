\begin{table}[h!]
    \centering
    % \setlength\extrarowheight{-1pt}
    \setlength\tabcolsep{9 pt}
    \begin{tabular}{c|c|c|c}
    \toprule
    \multicolumn{4}{c}{Common} \\
    \midrule
    State Normalization & Z-Score & Action Normalization & Min-Max \\
    
    Batch size & 2048 & Epochs & 100 \\
    
    Optimizer & AdamW & Learning Rate & 1e-4 \\
    
    Weight Decay & 0.1 & LR Scheduler & Cosine \\
    
    \midrule
    \multicolumn{2}{c|}{ResNet} & \multicolumn{2}{c}{LSTM+GMM} \\
    
    \midrule
    MLP Dimensions & [4096x3, 2048x7, 1024]  & 
    History Length & 10 \\
    
    Activation Function & GELU & 
    LSTM Dimensions & [1470x4] \\
    
    Loss & L2 Norm & 
    Last Layer Dimension & 2048 \\
    
    & & 
    Activation Function & GELU \\
    
    & & 
    Number of GMM Modes & 10 \\
    
    & & 
    Loss & NLL \\
    \midrule
    \multicolumn{2}{c|}{cVAE} & \multicolumn{2}{c}{Transformer} \\
    \midrule
    Encoder Dimensions & [1024, 1024, 2048] & 
    Embedding Dimension & 1024 \\
    Decoder Dimensions & [1024, 1024, 2048] & 
    Number of Layers & 6 \\
    Prior Dimensions & [1024, 1024, 2048] & 
    Number of Heads & 8 \\
    Latent Dimensions & 14 & 
    Dropout & 0.1 \\
    Loss & VAE & 
    Activation & GELU \\
    KL Loss Weight & 1.0 & 
    Loss & L2 \\
    \bottomrule
    \end{tabular}
    \caption{Hyperparameters of imitation learning architectures}
    \label{table:IL_ablation_hyper}
\end{table}