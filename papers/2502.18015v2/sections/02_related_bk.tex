\subsection{Learning prehensile and non-prehensile skills}
% Prehensile skill
% Grasp prediction + motion planning: DexNet, contact-graspnet, Anygrasp.
% End-to-end
% Qt-opt, Joshi et al., 
There are two common approaches to learning prehensile skills. The first approach involves incorporating grasp pose prediction and motion planning \cite{fang2023anygrasp, mahler2017dex, sundermeyer2021contact}. Grasp pose prediction modules output a grasp pose given an RGB-D or point cloud observation. These grasp pose predictions are typically trained on a synthesized dataset in a supervised manner, and low-level robot joint actions are generated by a motion planner. The second approach involves training an end-to-end policy using RL \cite{joshi2020robotic, kalashnikov2018scalable, wang2022goal}. RL policy learns to grasp objects of diverse shapes by optimizing a policy through trial and error, using rewards to enhance stable grasps while penalizing failures. For a more comprehensive review of prehensile skills, we encourage readers to refer to the survey by \citet{xie2023learning}. In this work, we train prehensile skills with predefined grasps using RL which can leverage GPU-parallelization to offer faster computation than a planner.

% Non-prehensile skill
% Hacman, CRM, CORN
% Recently, several works have proposed to leverage learning to directly map sensory inputs to actions in nonprehensile manipulation tasks, circumventing the computational cost of planning, inaccuracy of hybrid dynamics models, and difficulty in system identification from high dimensional sensory inputs. In Kimet al. (2023), they propose an approach that outputs diverse motions and effectively performs time-varying hybrid force and position control (Bogdanovic et al., 2020), by using the end-effector target pose and controller gains as action space. However, since they represent object geometry via its bounding box, their approach has a limited generalization capability across shapes. Similarly, other approaches only handle simple shapes such as cuboids (Yuan et al., 2018; 2019; Ferrandis et al., 2023; Zhou & Held, 2023), pucks (Peng et al., 2018), or cylinders (Lowrey et al., 2018).

% explain how previous works using RL or IL to train the Non-prehensile policies

% yuan2018rearrangement -> cube, planar push, tabletop
% yuan2019end -> cube, planar push, tabletop
% ferrandis2023nonprehensile -> cube, planar push, tabletop
% peng2018sim -> pucks, planar push, tabletop
% lowrey2018reinforcement -> pucks, planar push, tabletop
% zhou2023learning -> variety objects. move to grasp the object, 
%  \citet{kim2023pre} -> diverse cuboids, not only tabletop, not only push (flip the object using environment)

% To construct a set of skills for our problem, our non-prehensile skills should be trained to manipulate a fixed object, regardless of its shape. Also, the skills manipulate the object into the desired pose within fixed environments, which can differ from just the tabletop. % add not only the planar push
% do not reuquire generalization on diverse objects and envirnomnet
% Along with HACMan, other methods use simple primitives (Yuan et al., 2019; Zhou et al., 2023). But for problems we are contemplating, use of such open-loop primitives precludes dense feedback motions. This is problematic for highly unstable (e.g. rolling or pivoting) objects that move erratically, which require the robot to continuously adjust the contact
% jiang2024hacmanpp

Recent studies use learning-based approaches for non-prehensile manipulation, directly mapping sensory inputs to actions while avoiding the computational burden of planning, inaccuracies in hybrid dynamics models, and challenges in system identification from high-dimensional inputs. Several works \cite{ferrandis2023nonprehensile, yuan2018rearrangement, yuan2019end} focus on planar in pushing using simple cuboid objects on a tabletop, while \citet{peng2018sim} train policies for pushing pucks. \citet{zhou2023learning} train a policy for various object shapes, where the goal is to reposition the object to enable grasping, a sub-problem in PNP problems. HACMan \cite{zhou2023hacman} proposes object-centric action representations, which consist of a target point on the object point cloud and a poking direction, enabling generalization to diverse object shapes. Most of these approaches are limited to short-horizon problems, and have a restricted motion repertoire, relying primarily on planar pushing or hand-crafted primitives.

Recently, \citet{kim2023pre} have proposed an RL-based approach for non-prehensile manipulation based on a general action space that automatically discovers various skills such as toppling, sliding, pushing, pulling, etc. Moreover, this work has several features that make it suitable for PNP problems. More concretely, it decomposes the policy into a pre-contact policy, which outputs the robot configuration for contacting the object for manipulation, and a post-contact policy, which performs the manipulation. This decomposition naturally aligns with our approach to defining connectors: we train connectors to transition the robot to the pre-contact configuration, allowing skills to chain. We adopt this approach for training our skills.



%To construct a set of skills for our problem, we need an algorithm that can train non-prehensile skills with various object shapes, extending beyond planar pushing on a flat tabletop to include tasks such as manipulating a cup in a sink. We apply the approach from \citet{kim2023pre} to construct a robust skill set for NP manipulation.

\subsection{Reinforcement learning with given skills}
% One way to address PNP problems is to formulate the problem as Parameterized Action Markov Decision Processes (PAMDPs)~\cite{masson2016reinforcement} where each action is a skill represented by a pair of skill index and continuous parameters and use RL. For instance, PADDPG~\cite{hausknecht2015deep} treats the hybrid action space as continuous by selecting skills via probabilities, with a single policy generating both probabilities and parameters simultaneously, but neglecting their dependency. PDQN~\cite{xiong2018parametrized} combines DQN~\cite{mnih2013playing} for discrete actions and DDPG for parameters but introduces redundancy by modeling all parameters jointly. MAHHQN~\cite{fu2019deep} and PATRPO~\cite{wei2018hierarchical} adopt hierarchical policies, conditioning low-level parameter policies on high-level skill outputs, but require fixed parameter dimensions. \citet{li2021hyar} address this limitation by leveraging a latent action space to adapt parameters for each skill, enabling dimension flexibility. While these methods focus on simple domains like robot soccer, we target complex, high-dimensional robotic tasks.

% RAPS \cite{dalal2021accelerating} and MAPLE \cite{nasiriany2022augmenting} propose similar approaches, where a high-level policy learns to predict the logits of skill indices and the mean and variance of parameters using RL, and introduces low-level primitives as the provided primitives are often insufficient to solve tasks. MAPLE specifically suggests an affordance reward function that helps evaluate the utility of skills in specific states, facilitating exploration by providing additional rewards that guide the system towards states satisfying the skill's preconditions. HACMan++~\cite{jiang2024hacmanpp} computes skill parameters for each skill and object point in the point cloud. The critic network generates Q-values for every skill-parameter-point combination. The policy then selects the skill index and its parameter by sampling from this Q-value map using a softmax distribution. While these methods are applicable to our problems, RL often struggles in long-horizon problems with sparse rewards. We instead use a planning algorithm to solve problems, and use IL to learn a policy.

One way to address PNP problems is to formulate them as parameterized action Markov decision processes (PAMDPs), where each action is a skill represented by a skill index and continuous parameters, and solve them using RL. General RL algorithms, designed for purely discrete or continuous action spaces, require modifications to handle parameterized action spaces. PADDPG~\cite{hausknecht2015deep} converts the parameterized action space into a continuous space for DDPG, using a single policy to generate both skill probabilities and continuous parameters simultaneously, regardless of the selected skill. Similarly, PDQN~\cite{xiong2018parametrized} combines DQN for discrete skill selection and DDPG for continuous parameter generation. However, both PADDPG and PDQN are unstable because the action-value \( Q \)-function is influenced by parameters generated for unselected skills~\cite{li2021hyar}. This introduces noise and disrupts stability in learning. To address this, MAHHQN~\cite{fu2019deep} and PATRPO~\cite{wei2018hierarchical} adopt hierarchical policies, where low-level parameter policies are conditioned on high-level skill outputs.

Our problem also can be addressed using PAMDPs, where a skill index is selected, and its continuous desired object pose is generated. However, we also need low-level actions to connect the skills. To address this, RAPS~\cite{dalal2021accelerating} and MAPLE~\cite{nasiriany2022augmenting} introduce a low-level primitive that directly controls single-timestep end-effector motions, in addition to the set of skills which are multi-timestep hard-coded closed-loop controllers. They then learn a high-level policy that uses these to solve long-horizon problems. However, exploration remains challenging, particularly in determining skill parameters such as where to grasp or place. MAPLE addresses this by introducing an affordance reward function, which evaluates the utility of skills in specific states and guides exploration by rewarding skill's applicability. % However, generating an affordance reward function for each skill individually makes it challenging to define the utility of a skill, making the approach difficult to scale.
HACMan++~\cite{jiang2024hacmanpp} also solves PNP problems using a PAMDP formulation. The critic network generates Q-values for each combination, and the policy selects the skill index and its parameter by sampling from the Q-value map using a softmax distribution. However, they assume that skills can chain.

In contrast to these approaches, we train connectors to guide the robot to a state where skills can be applied. Moreover, RL often struggles in long-horizon tasks with sparse reward settings. Instead, we use a planning algorithm to solve these problems and apply imitation learning (IL) to train a policy.

%However, the issue with RL is that they struggle to address long-horizon problems due to sparse rewards and the credit assignment problem, particularly in scenarios that require temporarily moving away from the goal to ultimately achieve it. In contrast, we utilize a planning algorithm, \texttt{Skill-RRT}, which efficiently explores the search space.

% Like our setup, there are RL-based approaches for using pre-defined skills. In particular, reinforcement Learning for Parameterized Action Markov Decision Processes (PAMDPs) \cite{hausknecht2015deep, masson2016reinforcement} focus on sequencing skills and their continuous parameters. \citet{masson2016reinforcement} use DDPG~\cite{lillicrap2015continuous} to train a single policy that outputs both skill probabilities and parameters, but it does not consider dependencies between discrete actions and continuous parameters. To address this, \citet{xiong2018parametrized} propose a hybrid approach, using DQN~\cite{mnih2013playing} to select discrete actions and DDPG to determine continuous parameters, though it remains flat-level training with dependent continuous parameters. \citet{fu2019deep} introduce a hierarchical framework where a high-level policy selects discrete actions, and a low-level policy generates continuous parameters conditioned on the discrete action. However, this method struggles with varying skill parameter dimensions across discrete actions. \citet{li2021hyar} address this limitation by using a latent action space to generate continuous parameters specific to each discrete action, enabling better handling of dimension variations.


% To apply RL, we can formulate the PNP problem as  Parameterized Action Markov Decision Processes (PAMDPs)~\cite{hausknecht2015deep,jain2020generalization, jiang2024hacmanpp, masson2016reinforcement,  nasiriany2022augmenting, wei2018hierarchical, xiong2018parametrized, li2021hyar, dalal2021accelerating} where each action is a skill represented by a pair of skill index and continuous parameters, and use RL to train a high-level policy for selecting skills and their parameters. RAPS \cite{dalal2021accelerating} and MAPLE \cite{nasiriany2022augmenting} propose similar approaches, where a high-level policy learns to predict the logits of skill indices and the mean and variance of parameters using RL, and introduces low-level primitives as the provided primitives are often insufficient to solve tasks. MAPLE specifically suggests an affordance reward function that helps evaluate the utility of skills in specific states, facilitating exploration by providing additional rewards that guide the system towards states satisfying the skill's preconditions. 

% The issue with RL is that they struggle to address long-horizon problems due to sparse rewards and the credit assignment problem, particularly in scenarios that require temporarily moving away from the goal to ultimately achieve it. In contrast, we utilize a planning algorithm, \texttt{Skill-RRT}, which efficiently explores the search space.



\subsection{Planning with given skills}

% Sampling based method (dogar, barry, mao)
% Optimization based planning method (STAP, LSP)
% Train skill dynamic model + planning (Liang, 
    % DLPA: DLPA~\cite{zhang2024model} trains a parameterized-action-conditioned dynamics model, which includes a transition predictor, a reward predictor, and a continue predictor to model task termination in PAMDPs. It generates actions by planning with a modified Cross-Entropy Method, maximizing cumulative returns using trajectory rollouts predicted by the dynamics model over a planning horizon.

Given a set of skills and their preconditions and effects, Task and Motion Planning (TAMP) performs integrated discrete task planning and continuous motion planning for long-horizon problems~\cite{garrett2021integrated}. There are two common methods for solving TAMP: sampling-based and optimization-based approaches. Sampling-based methods~\cite{garrett2018ffrob, garrett2020pddlstream,kaelbling2011hierarchical, ren2024extended} typically work by finding a task plan using a symbolic planner and using sampling to find a feasible set of continuous parameters. Optimization-based approaches, such as Logic-Geometric Programming (LGP) \cite{migimatsu2020object, toussaint2015logic, toussaint2018differentiable} first solve symbolic task planning, and leverages optimization to find continuous parameters with the constraints imposed by the symbolic plan. While these methods have shown impressive performance in long-horizon problems \cite{du2023video, kim2022representation, mendez2023embodied, vu2024coast, zhu2021hierarchical}, using TAMP requires defining the preconditions and effects of skills, which are not available in PNP problems.
% to use TAMP, you need to define preconditions and effects of skills, which we do not have in PNP problems.

%. These are descriptions of state sets that ensure a skill, when executed in a state satisfying its preconditions, will deterministically lead to a state satisfying its effects. However, in PNP problems, such information is not readily available. While an applicability checker can determine whether a skill can be applied in a given state, it does not ensure the skill's success in moving the object to the desired pose. Further, even when the skill succeeds, the object is not positioned precisely at the desired pose but rather within a specified distance threshold. These uncertainties render the effect states undefined. 

Similar to our \texttt{Skill-RRT}, \citet{barry2013hierarchical} propose an RRT-based hierarchical skill planning algorithm that first generates a collision-free object path and realizes it by identifying a sequence of skills that could achieve that object path. It is similar to our algorithm in that it extends a tree toward randomly sampled configurations through skills using RRT. The key differences are that we utilize RL-based skills, which we assume to be independently trained, and use connectors to bridge gaps between the skills. In general, the core problem with pure planners, including \texttt{Skill-RRT}, is that they are computationally expensive and prone to frequent re-planning when unexpected transitions occur, particularly in contact-rich tasks where modeling contact dynamics is inherently challenging. We solve this by distilling \texttt{Skill-RRT} using IL.

%formulates TAMP as a joint optimization problem over discrete symbolic actions with task constraints, often expressed as a logic program, and continuous motion parameters with a geometric model of the robot and environment. 

%generating symbolic action plans with corresponding continuous parameters, sampling and refining trajectories to satisfy both the plan's discrete constraints and motion feasibility. 

%Logic-Geometric Programming (LGP) \cite{toussaint2015logic, toussaint2018differentiable, migimatsu2020object} formulates TAMP as a joint optimization problem over discrete symbolic actions with task constraints, often expressed as a logic program, and continuous motion parameters with a geometric model of the robot and environment. It leverages optimization techniques to find feasible solutions that satisfy both logical and geometric constraints. 


% Leaning skill dynamic models and planning
\iffalse
%BK: this could potentially be useful
To acquire the effects of skills, several works \cite{liang2022search, shi2023skill, zhang2024model, lidexdeform} train skill dynamics models and utilize them in planning. \citet{liang2022search} train GNN-based skill effect models, which take as input the current state and a skill parameter and predict the terminal state reached as well as the heuristic-based execution cost of the skills. They then integrate these skill effect models with a graph search by constructing a graph based on their skill effect predictions. \citet{shi2023skill} and \citet{lidexdeform} jointly train skill dynamics models and skills during both the pre-training and downstream adaptation phases and employ the skill dynamics models in model-based RL, which simulates the outcomes of candidate skills and selects the best skill. DLPA~\cite{zhang2024model} trains a parameterized-action-conditioned dynamics model, comprising a transition predictor, a reward predictor, and a continuation predictor, to model task termination in PAMDPs. It generates actions by planning with a modified Cross-Entropy Method, maximizing cumulative returns through trajectory rollouts predicted by the dynamics model over a planning horizon. However, small prediction errors from learned skill dynamics models are critical for our tasks, as the feasible state of the next skill is narrow. For instance, when predicting the terminal state of pushing a card to the edge of a table, even a few millimeters of prediction error could incorrectly determine the feasibility of the prehensile skill. As a result, this could render the plan infeasible and lead to inefficiencies in finding a feasible plan. In contrast, we obtain the terminal states of skills by simulating the skills in parallel.


% Optimizing value function.
% Problem undefined
% The key to generalization is planning actions that maximize the probability of long-horizon task success, which we model using the product of learned Q-values.

Another line of work focuses on skill planning to find an optimal sequence of skills and their parameters that maximize the probability of long-horizon task success. To do so, they optimize a skill sequence and its parameters based on the total product or summation of the value functions of each skill with its corresponding parameters in the sequence \cite{agia2023stap, xue2024logic}. STAP \cite{agia2023stap} performs sampling-based optimization on skill parameters to maximize the product of the Q-values of a plan skeleton which can be integrated with a task planner, such as PDDLStream \cite{garrett2020pddlstream}. LSP \cite{xue2024logic} generates a skill plan using Monte Carlo Tree Search (MCTS) \cite{browne2012survey} and optimizes skill parameters with the Cross-Entropy Method (CEM) \cite{de2005tutorial} to maximize the sum of the value functions. For better value function approximation, skills are trained based on Generalized Policy Iteration using Tensor Train (TTPI). While these methods require careful reward function design to prevent the dominance of one skill, we can utilize skills regardless of how the reward function is designed.
\fi


\subsection{Imitation learning for robotics}
There are three challenges in applying IL to PNP problems: (1) addressing action multi-modality that stems from different choices of object intermediate poses, (2) how to generate a large dataset, and (3) how to generate a high-quality dataset. 

To cope with action multi-modality, \citet{lee2024behavior} and \citet{shafiullah2022behavior} introduce methods combining discrete action mode predictors with continuous offset correctors to identify and refine distinct modes in the action distribution. ACT~\cite{Zhao-RSS-23} uses conditional variational autoencoders (CVAE) to handle multimodality within a transformer architecture to generate a fixed-length action sequence, called action chunks. OPTIMUS~\cite{dalal2023imitating} uses a Gaussian Mixture Model (GMM) with five modes to model diverse paths, grasps, and placements, outperforming unimodal approaches based on Mean Squared Error (MSE) loss. More recently, Diffusion Policy~\cite{chi2023diffusion} addresses action multi-modality using diffusion models, capturing diverse strategies like left or right end-effector movements to push a block. Their policy demonstrates strong performance in simulations and real-world settings. Our method also leverages the diffusion policy to capture multi-modal behaviors.

To gather a large amount of data, many studies rely on teleoperation systems to collect robotic demonstration data \cite{handa2020dexpilot, heo2023furniturebench, mandlekar2018roboturk, mandlekar2019scaling, mandlekar2020human, stepputtis2022system, Zhao-RSS-23}. However, teleoperation-based data collection is labor-intensive and time-consuming, creating challenges in scaling datasets for diverse tasks, such as varying initial and goal object poses \cite{brohan2022rt, dalal2023imitating, jang2022bc, xiang2019task, yang2021trail}. 

Alternatively, recent approaches automate data collection for robotics tasks using simulators and planning algorithms, such as TAMP, to generate large-scale datasets and train a policy with IL. \citet{driess2021learning} use Logic Geometric Programming (LGP)~\cite{toussaint2018differentiable} to generate high-level action sequences, with Model Predictive Control (MPC) producing robot joint controls.  \citet{mcdonald2022guided} train the high-level policy to imitate action types and their parameters from TAMP solutions. Their parallelized Modular TAMP approach uses FastForward~\cite{hoffmann2001ff} to generate initial action sequences, refined into executable trajectories by motion planners. OPTIMUS ~\cite{dalal2023imitating} uses a single-level IL policy to directly imitate end-effector control from generated data. Their method generates TAMP solutions using the PDDLStream planning framework~\cite{garrett2020pddlstream} with an adaptive, sampling-based algorithm, incorporating samplers for grasp generation, placement, inverse kinematics, and motion planning. We take much inspiration from these works, but \texttt{Skill-RRT} generates data from a planner tailored for PNP problems.

Training with high-quality demonstrations is essential for IL \cite{mandlekar2022matters}, as low-quality data can lead to high-risk states, causing failures or out-of-distribution (OOD) states. To mitigate this, OPTIMUS~\cite{dalal2023imitating} imposes a containment constraint within a bounding box, pruning trajectories where the end-effector exits the workspace. This ensures the learned policy avoids OOD states but is limited to a setup where end-effector location defines problem characteristic. In our work, we evaluate the quality of plans generated by \texttt{Skill-RRT} replaying plans under simulator stochasticity, and keeping only the ones that are robust under disturbance.