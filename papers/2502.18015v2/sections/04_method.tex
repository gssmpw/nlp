% how constructed methodology
% \input{tables/notations/state}
% \input{tables/notations/skill}
% \input{tables/notations/skill-rrt}
% \input{tables/notations/connector}
% \input{tables/notations/IL}
\begin{figure*}[ht!] % Force the figure at the top of the page
\centering
\vspace{-20mm}
\resizebox{\textwidth}{!}{
    \includegraphics{figures/cpnp_overview_v3.png}
}
\caption{Overview of our method: (a) A skill library $\mathcal{O}$ that consists of $n$ number of non-prehensile (NP) and $m$ number of prehensile (P) skills are given. (b) We first train connector skills. To do this, we use \texttt{Abstract Skill-RRT} to collect pairs of states that the connector needs to connect, based on the preconditions and effects of the skills in $\mathcal{O}$, and the connectors are trained using reinforcement learning (RL). (c) We then generate data for imitation learning by running \texttt{Skill-RRT} with the skills in $\mathcal{O}$ and connector skill. Given an initial node $q_{\text{init}}$ (red circle) and a goal object pose $q^{\text{obj}}_{\text{goal}}$ (green circle), we generate a skill plan $\tau_{\text{skill}}$, which includes a sequence of skills (blue lines), connectors (yellow lines), and their associated subgoals (black circle). (d) We filter out low-quality data by replaying each skill plan N times, and filtering skill plans with low replay success rate based on a pre-defined threshold $\theta$. The filtered trajectories are used to train diffusion policy. The final diffusion policy is deployed in the real world in a zero-shot manner. }\label{fig:CPNP_overview} % Place the label after the caption
\end{figure*}

\subsection{Problem formulation}\label{method:PF}



We aim to efficiently solve contact-rich, long-horizon robotics problems using the skill library $\mathcal{O}$, comprising a set of independently-trained skills. Each problem involves manipulating an object from an initial object pose $q^\text{obj}_\text{init}$ to a goal object pose $q^\text{obj}_\text{goal}$ within a fixed environment. We assume each skill consists of a policy, a pre-contact robot configuration sampler, and its preconditions and effects. All the skills are variants of either prehensile or non-prehensile manipulation skills. We also assume we have a physics engine that simulates real-world physics, albeit with low accuracy. Our objective is to acquire a policy that generalizes to different pairs of $(q^\text{obj}_\text{init}, q^\text{obj}_\text{goal})$ without requiring significant computation time. Generalization across environments, objects, and tasks is not considered.

To solve the problem, we design a three-phase approach: (1) use our planner called \texttt{Skill-RRT} to train additional connector skills to enable chaining skills in $\mathcal{O}$, (2) generate state-action trajectories data using \texttt{Skill-RRT}, and (3) use imitation learning based on diffusion model to train an efficient policy. Figure \ref{fig:CPNP_overview} overviews our proposed framework.

\subsection{Skill Definition}
Defining the feasible set of object poses for manipulation is crucial to manipulating an object effectively. For example, an object pose in \( SE(3) \) that lies below the table or beneath the ground plane (which is physically unrealizable) or a pose positioned too high for the robot to reach (which is beyond the robot's workspace) would be considered invalid. Therefore, for each environment, we define a set of regions \( \{\mu^i\}_{i=1, \dots, n} \) to restrict the search space of object poses. This restriction involves reducing the space from \( SE(3) \) to subsets of \( SE(2) \), where \( SE(2) \) describes rigid transformations in a 2D plane. These regions correspond to subsets of the parametric representation of \( SE(2) \), expressed as tuples \((x, y, \theta) \in \mathbb{R}^2 \times [0, 2\pi)\). Each \( \mu^i \subset \text{SE}(2) \) is further constrained based on the environment's geometry, structure, and layout, such as the presence of shelves or sinks. Also, each region \( \mu^i \) represents a set of stable object poses capable of maintaining their configuration under the influence of gravity, assuming static conditions without external disturbances. The regions \( \{\mu^i\} \) are defined to be mutually exclusive to ensure that each object pose belongs to exactly one region. For example, in a kitchen setting:
\begin{itemize}
    \item \( \mu^1 \): ``on the sink,'' capturing the stable poses of objects located on the sink.
    \item \( \mu^2 \): ``on the shelf,'' representing the stable poses of objects on the shelf.
\end{itemize}
By taking the union of all regions, \( \mu = \bigcup_{i=1}^n \mu^i \), we define the overall set of feasible object poses in the environment.

\input{algos/skill_def}

Based on the definition of the regions, we define two variant types of skills: non-prehensile (NP) and prehensile (P). The \( i \)-th NP skill \( o_{\text{NP}_i} \) is associated with a specific region \( \mu^i \). This skill manipulates an object from a pose \( q^\text{obj} \in \mu^i \) to a subgoal pose \( q^\text{obj}_\text{sg} \in \mu^i \). However, due to the mutually exclusive nature of each region \( \mu^i \), it is impossible to manipulate an object directly from \( \mu^i \) to \( \mu^j \), where \( i \neq j \), using any NP skill \( o_{\text{NP}} \). To enable manipulation between different regions, we define the prehensile skill \( o_{\text{P}} \), which allows the manipulation of an object from \( \mu^i \) to \( \mu^j \), where \( i \neq j \). By combining all the skills, we construct the skill library \(\mathcal{O}=\{o_{\text{NP}_1},...,o_{\text{NP}_n},o_{\text{P}}\}\).

% skill definition
A skill $o$ is defined by four key components: (1) a policy function \( \pi \), (2) a pre-contact robot configuration sampler \( \pi^{\text{pre}} \), and (3) success condition. % and (4) feasibility rejection rule.

The policy \( \pi \) generates an action distribution conditioned on the current state and a subgoal object pose, expressed as \( \pi(a \mid s, q^{\text{obj}}_\text{sg}) \). The input (state) and output (action) configuration for each skill policy is outlined in Tables \ref{table:NP_state}, \ref{table:P_state}, \ref{table:NP_action} and \ref{table:P_action} in Appendix \ref{Appendix:Skill}.

The sampler $\pi^{\text{pre}}$ computes a pre-contact robot configuration $q^{\text{robot}}_{\text{pre}} = \pi^{\text{pre}}(q^{\text{obj}}, q^{\text{obj}}_{\text{sg}})$ based on the current and subgoal object poses such as robot end-effector grasp pose for the P skill.

The success condition of a skill determines whether the skill execution in simulation is classified as a success or failure. A skill is considered successful if the policy manipulates the object such that the distance between the object's current pose and the subgoal pose, denoted as \( d^\text{obj}(q^\text{obj}_\text{sg}, q^\text{obj}) \), is less than a threshold parameter \( \epsilon^\text{obj} \). Here, \( d^\text{obj} \) represents a metric that quantifies the difference between the object's subgoal pose \( q^\text{obj}_\text{sg} \) and its current pose \( q^\text{obj} \). Since the object's pose lies in a continuous space, achieving an exact match between the subgoal pose and the current pose is practically impossible. Instead, the success condition allows for a small margin of error, defined by the threshold \( \epsilon^\text{obj} \), to account for this inherent limitation in precision.

% TAMP's action, due to the task's nature (such as droping the card during the ), 
% % explain our skill does not success at every situation. and we cannot know before skill's success before simulation

% Therefore, we should check skill's success after simulation of the skill.

% Simulating a skill in every possible situation is highly inefficient. For example, consider the \( i \)-th NP skill, which manipulates an object within the region \( \mu^i \). If a subgoal object pose is sampled from \( \mu^i \), but the initial object pose is located in a different region \( \mu^j \) (where \( i \neq j \)), the simulation of this skill becomes wasteful. Because the NP skill is not designed to operate across regions, it is unlikely to succeed in such situations.

% To address this inefficiency, we introduce a feasibility rejection rule—a guideline for determining whether a simulation should be performed for a given skill. This rule evaluates whether the transition from the initial state to the subgoal object pose aligns with the skill’s definition. For instance, NP skills require both the initial object pose and the subgoal pose to lie within the same region \( \mu^i \). By filtering out invalid transitions, the feasibility rejection rule ensures that simulations focus only on potentially successful scenarios, avoiding unnecessary computational overhead.

\subsection{Train the Connector Skills}
\input{algos/skill-RRT/skill_rrt}
\input{algos/skill-RRT/lazy_skill_rrt}
\input{algos/skill-RRT/concrete_skill_rrt}

\input{algos/skill-RRT/sampleskillandsubgoal}
\input{algos/skill-RRT/nearest}

\input{algos/skill-RRT/extend_abstract}
\input{algos/skill-RRT/extend}

To train the connector, we first collect problem instances that define the gap between one skill final state and the subsequent skill initial state using simulation. These problem instances are generated using Algorithm \ref{algo:lazy-skill-rrt}, referred to as \texttt{Lazy-Skill-RRT}. The algorithm produces a path \(\texttt{Path}_\text{skill} = [v_0, v^c_1, v_1, ..., v^c_K, v_K]\), which is the sequence of nodes. Each node represents the simulated skill with its subgoal $(o, q^\text{obj}_\text{sg})$, consist of four configurations simulated skill with its subgoal $(o, q^\text{obj}_\text{sg})$, simulation's result; object pose $q^\text{obj}$ and robot configuration $q^\text{robot}$. The algorithm takes problem $(q^\text{obj}_\text{init}, q^\text{obj}_\text{goal})$, skill library $\mathcal{O}$, goal sampling probability $p_g$ and metric function between object poses $d^\text{obj}(\cdot, \cdot)$ as inputs.

In \texttt{Lazy-skill-RRT}, the tree is initialized with a root node $v_0$ containing $q^\text{obj}_\text{init}$. The tree is extended by simulating the skill and its subgoal in the IsaacGym simulator, followed by iteratively performing three processes to add new nodes that progress towards the goal object pose $q^\text{obj}_\text{goal}$: 1) \texttt{SampleSkillSubgoal}, 2) \texttt{GetFeasibleNearestNode}, and 3) \texttt{LazyExtend}.

The algorithm \texttt{SampleSkillAndSubgoal}, described in Algorithm \ref{algo:SampleSkillAndPose}, samples a skill $o$ with a subgoal object pose $q^{\text{obj}}_{\text{sg}}$ from given $\mathcal{O}$ and $\mathcal{M}$, respectively, which will be attempted in simulator. The process begins by uniformly sampling a skill from the skill library \( \mathcal{O} \). If the NP skill $o_{\text{NP}_i}$ is sampled, then the $q^{\text{obj}}_{\text{sg}}$ is sampled from associated region \( \mu^i \). If the P skill $o_{\text{P}}$ is sampled, then $q^{\text{obj}}_{\text{sg}}$ is sampled from $\cup^n_{i=1} \mu^i$. Additionally, with a goal sampling probability \( p_g \), $q^{\text{obj}}_{\text{sg}}$ is replaced by the problem's goal object pose \( q^{\text{obj}}_{\text{goal}} \), ensuring that the planning process remains goal-directed while maintaining diversity in subgoal generation.

To simulate the sampled skill with its subgoal \((o, q^\text{obj}_\text{sg})\), a node from the tree is required to provide the state (the robot and object configuration contained in the node) to initiate the simulation of \((o, q^\text{obj}_\text{sg})\). In traditional RRT \cite{kuffner2000efficient}, the node is selected based on the minimum distance between the sampled configuration and the existing nodes in the tree, as determined by a distance metric. Once the nearest node is identified, the algorithm attempts to \texttt{Extend} from the nearest node to the newly sampled configuration. In our approach, we also can similarly identify the nearest node, such as based on the distance between the object pose contained in the node and the sampled subgoal object pose. 

In our case, the distance metric alone does not guarantee the successful execution of a skill. For example, consider an NP skill designed to manipulate objects in the unflipped region \( \mu^\text{unflip} \). Suppose the subgoal object pose is sampled as an unflipped card positioned at the center of the table. However, the nearest node, determined by its object pose information, contains a pose with the same position but a flipped orientation. In the case, the initial object pose does not belong to \( \mu^\text{unflip} \) and, therefore, the skill lacks the ability to manipulate the object toward \( q^\text{obj}_\text{sg} \) from that state. Attempting to simulate the skill under these circumstances is not only inefficient but also impractical, as it violates the skill’s characteristic. Thus, relying solely on the nearest node for simulation in such cases is insufficient and counterproductive. 

\input{algos/skill_feasibility_reject}

To address this inefficiency, we introduce a feasibility rejection rule—a guideline for determining whether a simulation should be performed for a given $(o, q^\text{obj}_{sg})$. This rule evaluates whether the transition from the initial state to the subgoal object pose aligns with the skill’s definition. For instance, NP skills require both the initial object pose and the subgoal pose to lie within the same region \( \mu^i \). By filtering out invalid transitions, the feasibility rejection rule ensures that simulations focus only on potentially successful scenarios, avoiding unnecessary computational overhead.

The preconditions for NP skill $\text{pre}^\text{[level]}_{\text{NP}i}$ and P skill $\text{pre}^\text{[level]}_{\text{P}}$ are categorized based on these abstraction levels as follows:

\begin{itemize}
    \item $\text{pre}^{[0]}_{\text{NP}_i}(q^\text{obj}, q^\text{obj}_\text{sg}): $ Whether the current object pose and subgoal object pose, $(q^{\text{obj}}, q^{\text{obj}}_{\text{sg}})$, lie within the initial and subgoal regions, $(\mu^i_{\mathcal{I}}, \mu^i_\beta)$.
    \item $\text{pre}^{[1]}_{\text{NP}_i}(q^\text{obj}, q^\text{robot}, q^\text{obj}_\text{sg}): $ Whether the robot configuration satisfies $q^{\text{robot}} == f^{\text{pre}}_{\text{NP}_i}(q^{\text{obj}}, q^{\text{obj}}_{\text{sg}})$.
    \vspace{1em}
\end{itemize}

\begin{itemize}
    \item $\text{pre}^{[0]}_{\text{P}}(q^\text{obj}, q^\text{obj}_\text{sg}): $ Whether an inverse kinematically feasible and collision-free relative grasp pose exists between the current object pose and the subgoal object pose.
    \item $\text{pre}^{[1]}_{\text{P}}(q^\text{obj}, q^\text{robot}, q^\text{obj}_\text{sg}): $ Whether the robot configuration satisfies $q^{\text{robot}} == f^{\text{pre}}_{\text{P}}(q^{\text{obj}}, q^{\text{obj}}_{\text{sg}})$.
    \vspace{1em}
\end{itemize}

The algorithm \texttt{CheckPreAndNearestNode}, described in Algorithm \ref{algo:NearestNode} in Appendix \ref{Appendix:Skill-RRT Details}, produces the node $v_{\text{near}}$ to be extended. It takes as inputs the tree $T$, the sampled skill $o$, the level 0 precondition $\text{pre}^{[0]}$ of the sampled skill, the sampled subgoal object pose $q^\text{obj}_\text{sg}$, and the distance metric function $d$. The algorithm evaluates nodes in the tree for selection \( T \) for selecting $v_\text{near}$ by two criteria: satisfaction of the Level 0 precondition of the sampled skill $\text{pre}^{[0]}$, and proximity based on the distance from the sampled subgoal object pose based on the metric function \( d (v.q^\text{obj}, q^{\text{obj}}_{\text{sg}}) \).

If the sampled skill \( o \) is an NP skill \( o_{\text{NP}_i} \), the algorithm evaluates the Level 0 precondition \( \mathrm{pre}^{0}_{\text{NP}_i}(v.q^{\text{obj}}, q^{\text{obj}}_{\text{sg}}) \) for each node \( v \) in the tree. For nodes where the precondition is satisfied, the distance \( d(v.q^{\text{obj}}, q^{\text{obj}}_{\text{sg}}) \) is calculated, and the node with the smallest distance is selected as \( v_\text{near} \). If the sampled skill \( o \) is a P skill \( o_{\text{P}} \), checking its level 0 precondition, \( \mathrm{pre}^{0}_{\text{P}} \), is computationally expensive due to the need for inverse kinematics (IK) feasibility and collision checks. Therefore, the algorithm first identifies the nearest node based on the metric \( d \) and then verifies whether \( \mathrm{pre}^{0}_{\text{P}}(v.q^{\text{obj}}, q^{\text{obj}}_{\text{sg}}) \) is satisfied. If the precondition is unmet, the algorithm repeats this process for up to 5 nearest nodes due to computation cost. This approach ensures the selected node is close and satisfies the necessary preconditions.

The algorithm \texttt{LazyExtend}, described in Algorithm \ref{algo:AbstractExtend}, updates the tree \( T \) by adding a new node if the simulation of a sampled skill with its subgoal \( (o, q^\text{obj}_\text{sg}) \) is successful. It takes as inputs the tree \( T \), the nearest node \( v_\text{near} \), the skill \( o \), the skill's pre-contact configuration sampler \( f^\text{pre} \), and the subgoal object pose \( q^\text{obj}_\text{sg} \). However, if the state in \( v_\text{near} \) might not satisfy the skill \( o \)'s precondition at level 1, \( \text{pre}^{[1]} \) (e.g., NP skill's effect robot configuration and P skill's initial grasp robot configuration). Therefore, the execution of the skill is highly likely to fail.

Therefore, we create a connector node \( v_\text{near}^c \), which is a copy of \( v_\text{near} \), but with its robot configuration replaced by the pre-contact robot configuration \( q^{\text{robot}}_{\text{pre}} \), computed using \( f^\text{pre}(v_{\text{near}}.q^{\text{obj}}, q^{\text{obj}}_{\text{sg}}) \). With this update, the object pose and robot configuration of \( v^c_\text{near} \), along with the subgoal object pose \( q^\text{obj}_\text{sg} \), satisfy both the abstract and full-level preconditions of the skill \( o \). The skill \( o \) with subgoal \( q^\text{obj}_\text{sg} \) is then executed in simulation starting from the state defined by \( v^c_\text{near}.q^{\text{obj}} \) and \( v^c_\text{near}.q^{\text{robot}} \), ignoring the gap between the effect of \( v_\text{near}.o \) and the precondition of the sampled skill \( o \). If the execution is successful, a new skill node \( v_\text{new} \), containing the simulation result \( (q^\text{obj}, q^\text{robot}) \), is added to the tree as a child of \( v^c_\text{near} \). This new edge, \( e_\text{new} \), also stores the skill execution details \( (o, q^\text{obj}_\text{sg}) \). Additionally, \( v^c_\text{near} \), which includes the updated robot configuration \( q^{\text{robot}}_{\text{pre}} \), is added to the tree as a child of \( v_\text{near} \). This connection is established through a new edge \( e^c_\text{new} \), with edge type bypass. If the execution fails, the tree remains unchanged.

These processes are repeated until a node with a subgoal that matches the task's goal object pose is added to the tree. Once such a node is added, the process terminates, and the tree is backtracked from the node to the root node to extract $\tau_\text{skill}^\text{abstract} = [v_0, v_1, v^c_1, ..., v_{K-1}, v_{K}]\), where the subscript of node $v$ is the skill timestep of the plan.

For every consecutive pair of skill node and connector node \( (v_{i}, v^c_i) \) in the plan, we generate a connector problem. The connector \( c \) is defined for the skill \( o = v_{i+1}.\text{edge}.o \). The problem instance is formulated as follows: the parent node's state \( (v_{i}.q^{\text{obj}}, v_{i}.q^{\text{robot}}) \) serves as the initial state, while the child node's pre-contact robot configuration \( v^c_i.q^{\text{robot}}\) is treated as the subgoal configuration \( q^{\text{robot}}_{\text{sg}} \). 

Using these problem instances, the connector policy \( \pi^c \) is trained to smoothly transition the robot from the parent node's configuration to the child node's configuration, ensuring the seamless execution of the skill plan. The connector policies are trained using Proximal Policy Optimization (PPO) \cite{schulman2017proximal}. Appendix \ref{Appendix:Connector} details the MDP setting for training and connector policy network architecture. For each skill \(o\), we create a connector \(c\) that generates low-level robot actions to move the robot toward its pre-contact configuration \(q^\text{robot}_\text{pre}\). By grouping these connectors, we construct a connector library \(C\).

\subsection{Data Generation for the Diffusion Policy Using \texttt{Skill-RRT}}
With the connector library (\(C\)), we can bridge the gap in robot configurations between \(v_i\) and \(v^c_i\) in the abstract skill plan (\(\tau^\text{abstract}_\text{skill}\)). By leveraging both the connector library (\(C\)) and the skill library (\(\mathcal{O}\)), our planner, \texttt{Skill-RRT}, generates a complete skill plan (\(\tau^\text{full}_\text{skill}\)). This plan is represented as a sequence of nodes, where each node corresponds to a simulated skill or connector, along with its associated subgoal \((o, q^\text{obj}_\text{sg})\) or \((c, q^\text{robot}_\text{sg})\), and the resulting object pose (\(q^\text{obj}\)) and robot configuration (\(q^\text{robot}\)). This approach enables the manipulation of the object from \(q^\text{obj}_\text{init}\) to \(q^\text{obj}_\text{goal}\) without bypassing any intermediate robot or object configurations. The key differences between \texttt{Skill-RRT} and \texttt{Lazy-skill-RRT} are highlighted with a gray background in Algorithm~\ref{algo:skill-rrt}.

In \texttt{Skill-RRT}, a tree is initialized with a root node that contains the initial object pose. The tree is then updated by iteratively performing the following three processes: 1) \texttt{SampleSkillAndSubgoal}, 2) \texttt{CheckPreAndNearestNode}, and 3) \texttt{Extend}. The process \texttt{SampleSkillAndSubgoal}, and \texttt{CheckPreAndNearestNode} are performed the same as in the \texttt{Lazy-skill-RRT}. Among these two processes, we can have the nearest node $v_\text{near}$ and the sampled skill $o$ and its subgoal $q^\text{obj}_\text{sg}$.

The algorithm \texttt{Extend}, described in Algorithm \ref{algo:Extend}, updates the tree \( T \) by adding new nodes if the execution of a sampled skill and its corresponding subgoal \( (o, q^\text{obj}_\text{sg}) \) succeeds. The process begins by computing the pre-contact robot configuration \( q^{\text{robot}}_{\text{pre}} \) using the function \( f^\text{pre}(v_{\text{near}}.q^{\text{obj}}, q^{\text{obj}}_{\text{sg}}) \). Subsequently, the connector \( c \) associated with the skill \( o \) is executed, starting from the state defined in the nearest node \( (v_\text{near}.q^\text{obj}, v_\text{near}.q^\text{robot}) \), along with its subgoal \( q^\text{robot}_\text{pre} \). If the execution of \( c \) is successful (i.e., it satisfies the precondition \( \text{pre}^{[1]} \)), the skill \( o \) is executed directly, starting from the terminal state of \( c \). If the skill execution is successful, two new nodes are added to the tree: \( v_{\text{new}} \), which contains the terminal state (object pose and robot configuration) of the executed skill \( o \), and \( v^c_{\text{new}} \), which contains the terminal state (object pose and robot configuration) of the executed connector \( c \). The node \( v^c_{\text{new}} \) is added as a child of \( v_{\text{near}} \) via a new edge \( e^c_\text{new} \), which stores the connector execution information \( (c, q^{\text{robot}}_{\text{pre}}) \). Similarly, \( v_{\text{new}} \) is added as a child of \( v^c_{\text{new}} \) via a new edge \( e_\text{new} \), which stores the skill execution information \( (o, q^{\text{obj}}_{\text{sg}}) \). If either the connector \( c \) or the skill \( o \) execution fails, the tree remains unchanged.

These processes are repeated until a node with a subgoal that matches $q^\text{obj}_\text{goal}$ is added to the tree $T$. Once such a node is added, the process terminates, and the tree is backtracked from the node to the root node to extract $\tau^\text{full}_\text{skill}=[ v_0, ..., v_1, ..., v_K ]$.

By generating a skill plan \(\tau^\text{full}_\text{skill} = [v_i]_{i=0}^{K}\), where \( K \) represents the length of the plan, we define \(\tau_\text{skill} =[(v_i.o, v_i.q_\text{sg})]_{i=1}^K\) as the sequence of skills and their subgoals extracted from the nodes \( v_i \) in the plan. By executing this sequence step by step, the object can be manipulated from its initial pose \( q^\text{obj}_\text{init} \) to the goal pose \( q^\text{obj}_\text{goal} \) in simulation. This sequence specifies the required skills (or connectors) and their associated subgoals, providing a complete solution to the manipulation problem \( (q^\text{obj}_\text{init}, q^\text{obj}_\text{goal}) \).

\subsection{Imitation Learning}

% Problem
% Mimic a given s.
% Action is multimodal as 
% a1 \pi(s, sg1)
% a2 \pi(s, sg2)

% Directly applying a skill plan \(\tau_{\text{skill}}\), generated via \texttt{Skill-RRT}, to real-world tasks is inefficient due to the sim-to-real gap: after executing each skill \((o, q^{\text{obj}}_{\text{sg}})_k\) in \(\tau_{\text{skill}}\), the resulting state may deviate from the preconditions required by the subsequent skill \((o, q^{\text{obj}}_{\text{sg}})_{k+1}\). These deviations necessitate re-planning, which introduces delays and makes real-time execution impractical. To address this issue, we propose using imitation learning (IL) to train a policy that enables immediate robot responses to observations, ensuring smooth transitions between skills without re-planning. To train an effective policy, we focus on two critical aspects: (1) collecting a robust and diverse dataset and (2) designing a model architecture capable of handling the multimodal nature of the task.

Directly applying a skill plan \(\tau_{\text{skill}}\), generated via \texttt{Skill-RRT}, to real-world tasks is inefficient for two primary reasons. The first reason is that planning requires significant online computation time and the second reason is the sim-to-real gap. After executing each skill \((o, q_{\text{sg}})\) in \(\tau_{\text{skill}}\), the resulting state will deviate from the planned state due to large dynamic uncertainty in contact-rich manipulations. These deviations often necessitate re-planning, which introduces delays and makes real-time execution impractical. To address these issues, we leverage imitation learning (IL) to train a single distillation policy $\pi:s \rightarrow a$ that enables immediate robot responses to observations.

% We randomly sample \( q^{\text{obj}}_{\text{init}} \) and \( q^{\text{obj}}_{\text{goal}} \) from the environment's problem distribution, while setting \( q^{\text{robot}}_{\text{init}} \) to the pre-contact configuration of the initially feasible skill $o$ (computes by $f^\text{pre}(q^{\text{obj}}_{\text{init}}, q^{\text{obj}}_{\text{init}})$). The skill plan \( \tau_{\text{skill}} \) is generated using Algorithm \ref{algo:skill-rrt}, and diverse state-action pairs are collected by replaying \( \tau_{\text{skill}} \) multiple times, coming from the stochastic GPU simulator \cite{nvidia2020physx} and the domain randomization. To enhance the robustness of the imitation-trained policy and facilitate sim-to-real transfer in the presence of perception errors, as well as to adapt to unknown physical parameters (e.g., uncertainties in table and card friction), we also apply domain randomization during replay.

% BC -- MLP
% Multi-modal data -- GMM, Transformer
% Fast

% The goal of imitation learning (IL) is to train a policy that replicates actions from an offline dataset. Given state-action trajectories \(\tau = \{(s_0, a_0), (s_1, a_1), ..., (s_T, a_T)\}\), the policy learns to predict \(a=\pi_{\text{IL}}(s)\) by optimizing an objective function, such as the L2 loss. 
The simplest approach would be to train a deterministic policy using the L2 loss. However, this method would perform poorly if applied to a dataset generated by \texttt{Skill-RRT}. This is because the distillation policy lacks access to each subgoal  \(sg\) while the actions in the dataset are generated by subgoal-conditioned skills, i.e., \(a=\pi_o(s,sg)\). As a result, actions can vary significantly for the same state \(s\), depending on the subgoal; for example, \(a'=\pi_o(s, sg') \not= a''=\pi_o(s,sg'')\). This introduces multimodality in the dataset, making it challenging for the imitation learning (IL) policy to learn effectively \cite{robomimic2021}. To address this issue, a Gaussian Mixture Model (GMM) can be employed to handle multimodal data. While GMMs achieve moderate performance, they are sensitive to the number of modes and require careful tuning for each domain, which limits their generalizability. Instead, generative models such as Variational Autoencoders (VAEs) \cite{kingma2013auto}, Generative Adversarial Networks (GANs) \cite{goodfellow2014generative}, and diffusion models \cite{ho2020denoising} can be considered, as these are better suited for multimodal data. Due to the smoothing effect observed in VAEs and the training instability associated with GANs, we choose to use diffusion policy \cite{chi2023diffusion}.

Despite their high performance, diffusion models have slow inference speeds due to the iterative denoising process. Inference speed is especially critical in our contact-rich tasks because the robot must react to state transitions as quickly as possible. To achieve faster inference, we use a U-Net \cite{ronneberger2015u} architecture as the backbone instead of Transformers \cite{vaswani2017attention}, as Transformers incur high computational costs due to attention mechanisms. Additionally, we further reduce inference time by removing action chunking and state history from the diffusion policy. As a result, the policy is able to output actions at a frequency of 75 Hz on an Nvidia 4090 GPU.

% Inference time is a critical factor for real-world deployment because the IL policy must respond quickly to stochastic state transitions arising from contact-rich manipulations. To ensure fast inference without significantly degrading performance, it is essential to carefully select the model architecture. Transformers \cite{vaswani2017attention} are currently the most powerful architecture for many tasks, but their high computational cost due to attention mechanisms makes them unsuitable for real-time applications. Recurrent Neural Networks (RNNs) are a potential alternative to Transformers, but they are still slow for practical use in real-world scenarios. As a result, a Multi-Layer Perceptron (MLP) with generative modeling emerges as the most suitable option for real-world applications, balancing inference speed and performance.

% Finally, we choose to use a diffusion policy with a U-Net backbone \cite{chi2023diffusion}, as it effectively handles multi-modal data while maintaining reasonable inference time. To further enhance inference speed, we remove action chunking and state history, which are originally included in the diffusion policy. In this configuration, the IL policy is trained to predict a single action based on a single time-step observation. Importantly, we observe no degradation in performance despite these adjustments. Detailed descriptions of the state and action spaces are provided in Table \ref{table:IL_state} and Table \ref{table:IL_action}.

The state space and action space of the distillation policy are shown in Table \ref{table:IL_state} and Table \ref{table:IL_action}. For the state space, we found that joint velocity introduces a significant sim-to-real gap, even though robot joint velocity is included in the skills' state space during RL training in simulation. To address this issue, we replace joint velocity with the joint position at the previous timestep in the state space. The gripper of the Franka Research 3 cannot be controlled until the previous gripper command is completed, which takes approximately 1.2 seconds. To mitigate this issue, we include a binary input indicating whether the gripper is executable. Additionally, we add the previous gripper target position to the state space so the policy can track the gripper's current movement. The action space consists of joint position targets, their gains and damping values, enabling the policy to perform torque control for non-prehensile manipulation, as well as gripper target positions.

%Though sim-to-real gap in joint velocity is large, it is important information for the distillation policy because it tells where the robot is moving toward. For example, consider the state when the robot place a card at the edge of table. If the joint velocity is large, the card will be highly fall down, so the policy will keep gripper close. If the joint velocity is small, the policy will open the gripper and move on to the next skill like NP skill. To replace the joint velocity, we add joint position at the previous timestep to the state space.

%
% The gripper of Franka Research 3 robot cannot be controled by real-time.

\input{tables/IL_state}
\input{tables/IL_action}


% These are judged based on three factors: 1) training time, 2) inference time, 3) capability of handling multi-modal data.

\subsubsection{Data Filtering}

% robust subgoal
Using entire trajectories generated by \texttt{Skill-RRT} can degrade the performance of the imitation learning (IL) policy. Even if a skill plan is successfully generated, it may have been achieved with low probability due to uncertainties in state transitions. To ensure high-quality data, we evaluate each skill plan by replaying it $N$ times. When a skill plan $\tau_\text{skill}=\{ q^{\text{obj}}_{\text{init}}, q^{\text{obj}}_{\text{goal}}, \{(c,q^{\text{robot}}_{\text{pre}})_k, (o,q^{\text{obj}}_{\text{sg}})_k\}_{k=0}^K) \}$ is found, its skills and connectors are re-executed N times. The replay success rate of the skill plan is then measured based on whether all skills and connectors succeed during replay. Skill plans with a replay success rate lower than a predefined threshold $\theta$ (e.g. 90\%) are filtered out. A high replay success rate indicates that the skill plan consistently succeeds in reaching the goal despite uncertainties in state transitions and domain randomizations. Training the IL policy with such high-quality trajectories improves its overall performance.

%To address this, we retain only skill plans with a high success rate, ensuring the policy performs reliably across trials and generalizes better during deployment. Specifically, only skill plans with a success rate exceeding a predefined threshold \( \theta \) are included in the dataset \( \mathcal{D}_{\text{imitate}} \). The dataset is curated by replaying skill plans on randomly selected tasks and retaining only successful trajectories, resulting in high-quality training data.

% Once the dataset \(\mathcal{D}_{\text{imitate}}\) is collected, we employ a U-Net-based diffusion model \cite{ho2020denoising, chi2023diffusion} to train the imitation learned policy \(\pi_{\text{IL}}\). Diffusion models are particularly suitable for our task, as they excel in handling multimodal and high-dimensional data. In our dataset, the same state may lead to different actions depending on the skill or subgoal, making multimodality a key challenge.

% The policy is trained to predict a single action for each state at a single time step. Since our task is contact-rich, the policy performs better with frequent observation feedback. Also we use only the single time-step observation during training for reducing computational complexity. The input and output configuration of the policy $\pi_\text{IL}$ is outlined in Table \ref{table:IL_state} and \ref{table:IL_action}.

The trained IL policy is directly deployed in real-world manipulation tasks in a zero-shot manner. By leveraging the diversity and quality of trajectories, along with the generalization capabilities of the diffusion model, the policy is able to handle complex manipulation tasks without requiring additional fine-tuning.

\input{tables/experiment/exp_setup}