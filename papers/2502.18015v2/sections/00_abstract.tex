Current robots face challenges in manipulation tasks that require a long sequence of prehensile and non-prehensile skills. This involves handling contact-rich interactions and chaining multiple skills while considering their long-term consequences. This paper presents a framework that leverages imitation learning to distill a planning algorithm, capable of solving long-horizon problems but requiring extensive computation time, into a policy for efficient action inference. We introduce \texttt{Skill-RRT}, an extension of the rapidly-exploring random tree (RRT) that incorporates skill applicability checks and intermediate object pose sampling for efficient long-horizon planning. To enable skill chaining, we propose \textit{connectors}, goal-conditioned policies that transition between skills while minimizing object disturbance. Using lazy planning, connectors are selectively trained on relevant transitions, reducing the cost of training. High-quality demonstrations are generated with \texttt{Skill-RRT} and refined by a noise-based replay mechanism to ensure robust policy performance. The distilled policy, trained entirely in simulation, zero-shot transfer to the real world, and achieves over 80\% success rates across three challenging manipulation tasks. In simulation, our approach outperforms the state-of-the-art skill-based reinforcement learning method, \texttt{MAPLE}, and \texttt{Skill-RRT}. Project website: \href{https://sites.google.com/view/skill-rrt}{sites.google.com/view/skill-rrt}. % Our code for training, testing, and real-world experiments will be available in the camera-ready version.

%, showcasing the capability to efficiently perform intricate long-horizon multi-skill manipulation tasks.

%High-quality demonstrations are generated with \texttt{Skill-RRT} and refined through a noise-based replay mechanism to ensure robust policy performance. The distilled policy, trained entirely in simulation, achieves zero-shot transfer to the real world, demonstrating over 80\% success rates across three contact-rich PNP tasks. In simulation, we show that our approach outperforms state-of-the-art skill-based reinforcement learning algorithm, \texttt{MAPLE}, and pure planning algorithm


%In this paper, we address the problem of finding a sequence of robot actions (e.g., torques) for contact-rich, long-horizon tasks, such as flipping a thin card, given an initial state, a goal, and a set of non-prehensile (NP) and prehensile (P) manipulation skills. Our problem involves three primary challenges: (1) mismatched preconditions and effects between NP and P skills, such as the end state of a pushing skill not aligning with the state of grasping an object required for a P skill, which prevents seamless skill chaining, (2) designing an algorithm capable of effectively chaining NP and P skills, and (3) minimizing exhaustive planning and re-planning times to enable efficient real-world deployment. Traditional approaches to this problem, including reinforcement learning with Parameterized Action MDPs and Task and Motion Planning, often rely on task-specific dense reward functions or significant computational resources, and all of them assume that all necessary skills are given. To address these challenges, we propose a three-stage framework: (1) connector acquisition, (2) skill chaining, and (3) imitation learning. To bridge the gap between existing skills, we first train intermediate skills, referred to as connectors, which transition the effects of preceding skills to the preconditions of subsequent skills like moving the robot from the end state of a pushing skill to the state required for grasping an object. To train these connectors, we collect pairs of states that need to be connected using the Abstract GPU-accelerated Skill-RRT. Next, we search for a skill plan—a sequence of skills and their subgoals—across diverse initial states and goals using GPU-accelerated Skill-RRT, which is based on the rapidly-exploring random tree (RRT) algorithm and enhanced by parallel GPU computation. Since planning is time-consuming and re-planning is often necessary due to stochastic state transitions in real-world scenarios, we generate trajectories consisting of low-level actions using GPU-accelerated Skill-RRT and train a single distillation policy to enable one-step action prediction. Additionally, we utilize a diffusion policy to address multi-modal actions in trajectories, which arise from different subgoals even under the same state. However, the distillation policy's performance degrade when trained on trajectories from skill plans that achieve the goal with low probability due to dynamic uncertainties in contact-rich manipulation and observation noise. To mitigate this issue, we replay skill plans multiple times and filter out those with low replay success rates based on a predefined threshold. We demonstrate that the final distillation policy achieves high performance in three contact-rich, long-horizon tasks in the real world without requiring additional fine-tuning.

% Problems we solve (given, unknown, constarints).
% Previous works.
% (Challenges)
% Explain how we solve.
% problem - method

% In this paper, we address the challenge of solving contact-rich, long-horizon tasks, such as flipping a thin card, by chaining non-prehensile (NP) and prehensile (P) manipulation skills. Traditional methods, including Parameterized Action MDPs and Task and Motion Planning, often require either task-specific reward functions or significant computational resources. The challenges in our problem include the absence of skills that can flexibly connect NP and P skills, deciding on an appropriate skill and subgoal, and enabling real-time execution for real-world deployment. To address these challenges, we propose a three-stage framework: (1) connector acquisition, (2) skill chaining, and (3) imitation learning. First, we train connector skills to bridge the effects of one skill and the preconditions of the next using an abstracted version of a skill-chaining algorithm called the Abstract GPU-accelerated Skill-RRT. Using these connector skills, we generate demonstrations through a simple yet effective skill-chaining algorithm called GPU-accelerated Skill-RRT. These demonstrations are then filtered to ensure high quality and finally distilled into a single imitation learning policy for real-time execution. We validate our framework on three contact-rich tasks—flipping a card, rearranging a book, and organizing a cup—using only simulation data for training. Experimental results show that our final distillation policy achieves high performance when deployed in the real-world in a zero-shot manner.