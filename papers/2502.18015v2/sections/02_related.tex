% Planning
% Reinforcement Learning
% Imitation Learning



% \subsection{Contact-rich Long Horizon Robotics Manipulation} % broad Contact-rich manipulation Long ()
% Previous works on solving contact-rich, long-horizon robotics manipulation either tackle the entire task as a whole or decompose it into sub-tasks. For tasks addressed as a whole, \cite{fu2024mobile} used imitation learning with an action chunk transformer, while \cite{chi2023diffusion} employed a diffusion model with human demonstrations. However, these methods require a large amount of human demonstrations to solve the initial task goal and face challenges in generalizing to new, unseen scenarios.

% Another approach to solving long-horizon problems involves decomposing complex tasks into smaller, reusable sub-tasks. Various methods have been proposed, such as learning sub-policies through RL \cite{lee2019learning} or utilizing a predefined skill library \cite{wang2022generalizable}. Once the task is decomposed, the focus shifts to sequencing prehensile skills (e.g., pick-and-place) along with non-prehensile skills like pushing, pulling, and toppling. Recent studies have investigated long-horizon manipulation tasks requiring prehensile and non-prehensile skills \cite{nasiriany2022augmenting, agia2023stap, xue2024logic}. However, these approaches are often tested with non-prehensile skills, such as planar pushing performed exclusively within the confines of a flat table, where success is nearly guaranteed. In contrast, real-world scenarios introduce higher levels of uncertainty. These algorithms do not account for situations where non-prehensile skills, when sequenced with prehensile skills, are more prone to failure, as the success of the non-prehensile skill cannot be guaranteed, such as pushing a card to a table edge or toppling a book from a bookshelf.

% Training robots for complex, long-horizon manipulation tasks from scratch using RL and IL presents significant challenges. These include difficulties in credit assignment over extended task horizons for RL, as well as the scarcity of sufficient demonstrations required to generalize across diverse pairs of initial and goal object poses for IL.

% A common approach to addressing complex manipulation tasks is to decompose them into reusable sub-tasks \cite{sutton1999between}. These approaches often utilize sub-policies obtained through various methods, such as learning from demonstrations \cite{lu2021learning, mishra2023generative}, training each skill for each sub-task via RL \cite{ahn2022can, lee2019learning} or leveraging a predefined skill library \cite{wang2022generalizable}. However, these methods have typically only been successful in handling prehensile manipulation tasks, which are limited to pick-and-place operations. % which are limited to prehensile manipulation. % Consequently, they fail to address the unique challenges posed by long-horizon, contact-rich environments that require the integration of NP manipulation. 

% % NP manipulation is often required in scenarios where an object is either ungraspable or, while graspable, is positioned in a pose that makes it inaccessible for direct grasping.
% Unlike prehensile manipulation, where specific parts of the object are grasped and controlled, NP manipulation does not maintain a fixed relative pose between the object and the robot's end-effector. Consequently, even when a NP manipulation successfully moves the object to the desired subgoal object pose, it becomes challenging to precisely predict or know the result of the manipulation. For instance, the resulting robot configuration (e.g, joint position) after the skill is executed might be unknown. This inherent uncertainty significantly complicates the sequencing of NP skills, as the lack of precise effect estimation hinders smooth transitions and sequencing with prehensile skills. % planning is difficult due to this reason, RL is difficult to
% % Existing methods, which primarily focus on prehensile-only scenarios, fail to provide robust strategies to address these challenges in contact-rich tasks.

% Some methods have attempted to sequence skills that include non-prehensile (NP) skills. However, these approaches often utilize NP manipulation even in scenarios where the object is already graspable, diminishing the practicality of their solutions \cite{nasiriany2022augmenting}. Furthermore, in cases where NP manipulation is genuinely required, the experiments are typically conducted in overly controlled and safe environments, where the success of NP skills is nearly guaranteed \cite{xue2024logic}. % easy to retry 
% This creates a situation where the manipulation process involves moving an object from a graspable position to an ungraspable one, often placing it in a precarious, unstable position, which is unrealistic and impractical for real-world tasks. In contrast, our approach successfully sequences NP skills in scenarios where the risk of failure and object instability is much higher, demonstrating a more robust and realistic solution for handling such tasks safely and effectively in practical environments.
% % rethink how to re-write 
% % talk about the skills first challnening 한 task에서 demonstration을 보여주지 않았다.

% There are three approaches to addressing long-horizon robotic manipulation problems.

% \subsection{Skill Sequencing with Skill Skeleton}
% % fine-tuning skills and skill-skeleton
% Several approaches rely on a predefined skill skeleton to guide the solution process, fine-tuning skills from the skill library to ensure smooth transitions between tasks. \cite{lee2021adversarial, chen2024scar} refines the initiation and termination sets of each skill so that the termination set of one skill becomes a subset of the initiation set of the next, ensuring the successful execution of skill sequences. Another method involves selecting appropriate skill parameters based on the predefined skeleton. \cite{agia2023stap} maximizes the product of Q-functions to ensure the joint success of all skills in a sequenced plan. \cite{mishra2023generative} used a diffusion model to generate the full sequence of skill parameters, considering both the executability of skill pairs and goal achievement. However, these approaches assume the existence of a predefined skill skeleton for every task, which can be an overly restrictive assumption.

\subsection{Skill-based RL}
\iffalse
% There are several approaches to automatically discover and sequence skills to solve tasks. One of method is unsupervised skill discovery methods \cite{eysenbach2018diversity, sharma2019dynamics, lee2019learning, park2022lipschitz, park2023controllability}, a subset of unsupervised RL, which obtain skills by maximizing .

Loosely speaking, skill-based RL algorithms are learning-based approaches for long-horizon problems with temporally abstracted actions. These algorithms are largely divided into ones that discover skills, and ones that assume the skills are given. We begin with the description of the former. 

\subsubsection{Methods that discover skills}
There are two approaches for discovering skills: unsupervised skill discovery, and supervised skill discovery via IL from off-line data.
In unsupervised skill discovery~\cite{eysenbach2018diversity, sharma2019dynamics, lee2019learning, park2022lipschitz, park2023controllability}, skills are represented as a latent vector $z$ with a latent-variable-conditioned policy $\pi(a|s,z)$ and are discovered by maximizing mutual information (MI) between skills $z$ and states $s$. In the second approach, skills are defined as goal-conditioned policies \cite{mandlekar2020iris, gupta2020relay}, latent-conditioned policies \cite{pertsch2021accelerating, rosete2023latent}, or a set of discrete primitives \cite{lee2019composing} and are discovered through imitation learning (IL) from offline data. For both approaches, once skills are obtained, a high-level policy is then trained to solve downstream tasks using these skills via online RL \cite{pertsch2021accelerating, zhang2024extract}, IL \cite{mulling2013learning, mandlekar2020iris, rosete2023latent, garrett2024skillmimicgen}, or a combination of both \cite{gupta2020relay}. 

Notably, Lee et al. \cite{lee2019composing} propose transition policies that learn to facilitate transitions between skills for effective skill chaining, similar to our connectors. They train proximity predictors, which output the proximity to the initiation set of the next skill, from successful and failed transition trajectories. The proximity predictors then provide dense rewards to train transition policies with RL. However, it is infeasible to collect successful transitions for the next skills in our tasks. For example, after pushing a card to the edge of a table, there is no opportunity to succeed in the prehensile skill, where the card needs to be grasped. Therefore, we propose \texttt{Lazy Skill-RRT} to collect pairs of initial states and goals for training connectors.


In contrast to these approaches, we take a more bottom-up approach to solve PNP problems. We make the observation that humans use prehensile and non-prehensile skills in our daily lives to manipulate objects, and build on existing work~\cite{kim2023pre} for learning non-prehensile manipulation policy to acquire these skills.

\subsubsection{Methods that use the given skills}
\fi
Like our setup, there are RL-based approaches for using a discrete set of pre-defined skills. In particular, reinforcement Learning for Parameterized Action Markov Decision Processes (PAMDPs) \cite{hausknecht2015deep, masson2016reinforcement} focus on sequencing skills and their continuous parameters in robotics applications~\cite{dalal2021accelerating, nasiriany2022augmenting, jiang2024hacmanpp}. RAPS \cite{dalal2021accelerating} and MAPLE \cite{nasiriany2022augmenting} propose similar approaches, where a high-level policy learns to predict the logits of skill indices and the mean and variance of parameters using RL, and introduce low-level primitives as the provided primitives are often insufficient to solve tasks. MAPLE specifically suggest an affordance reward function that helps evaluate the utility of skills in specific states, facilitating exploration by providing additional rewards that guide the system towards states satisfying the skill's preconditions. However, RL often struggles to address long-horizon problems due to sparse rewards and the credit assignment problem, particularly in scenarios that require temporarily moving away from the goal to ultimately achieve it. In contrast, we utilize an RRT-based planning algorithm, which explores the state space by building a tree that grows towards unexplored areas through Voronoi bias.


%While these methods assume access to offline data, we focus on utilizing well-studied prehensile and non-prehensile manipulation skills and integrating them to solve long-horizon tasks. A high-level policy then learns to compose these skills to solve downstream tasks using online RL with environment interaction.  However, these methods prioritize discovering diversity or novelty of skills over utility for tasks (e.g., how effectively skills change object configurations). 


%In the second approach, skills are defined as goal-conditioned policies \cite{mandlekar2020iris, gupta2020relay}, latent-conditioned policies \cite{pertsch2021accelerating, rosete2023latent}, or a set of discrete primitives \cite{lee2019composing} and are discovered through imitation learning (IL) from offline data. Once skills are obtained, a high-level policy is then trained to solve downstream tasks using these skills via online RL with environment interaction \cite{pertsch2021accelerating, zhang2024extract}, IL \cite{mulling2013learning, mandlekar2020iris, rosete2023latent, garrett2024skillmimicgen}, or a combination of both \cite{gupta2020relay}. While these methods assume access to offline data, we focus on utilizing well-studied prehensile and non-prehensile manipulation skills and integrating them to solve long-horizon tasks.


% The option framework~\cite{sutton1999between, precup2000temporal} is also used to discover skills (referred to as options within their framework) for solving long-horizon problems. The option framework, which models temporally extended actions, is widely applied to automate the learning of sub-behaviors (skills) by formulating the problem as a Semi-Markov Decision Process (Semi-MDP)~\cite{sutton1999between}. The Option-Critic Architecture~\cite{bacon2017option} introduces an end-to-end framework for jointly learning intra-option policies and termination functions through gradient-based optimization \cite{sutton1984temporal}. This approach enables RL agents to autonomously discover and develop options without using prior knowledge. \citet{klissarov2017learnings} extend the Option-Critic to continuous action spaces by employing Proximal Policy Optimization (PPO)~\cite{schulman2017proximal} to train the options. Additionally, \citet{zhang2019dac} reformulate the option framework as two parallel augmented MDPs, allowing the application of off-the-shelf policy optimization algorithms for both option learning and master policy learning in an intra-option manner. \citet{bagaria2019option} combine skill chaining with deep neural networks to discover options, explicitly learning initiation set classifiers that allow each option to specialize in distinct regions of the state space. This approach reduces the burden on options to learn representations for states far outside their initiation regions and enables each skill to construct its own skill-specific state abstraction. However, the option framework differs from our skill definition in several key aspects. Unlike options, we do not train a termination condition for each skill. Instead, the termination of our skills is inherently linked to the parameters of the skill. Additionally, while options are not parameterized, our skills are explicitly parameterized by the target object's pose, enabling adaptability. For instance, moving an object to positions $(1,0)$ and $(0,1)$ would be treated as independent skills in the option framework, despite their similarities. Such limitations hinder the generalization and efficiency of the option framework in our environments.

The option framework~\cite{sutton1999between, precup2000temporal} models temporally extended actions to automate skill discovery for long-horizon tasks by formulating the problem as a Semi-Markov Decision Process (Semi-MDP). The Option-Critic Architecture~\cite{bacon2017option} provides an end-to-end framework for jointly learning intra-option policies and termination functions through gradient-based optimization, enabling RL agents to autonomously develop options. Extensions include adapting the framework to continuous action spaces using PPO~\cite{klissarov2017learnings} and reformulating it as two parallel augmented MDPs for off-the-shelf policy optimization~\cite{zhang2019dac}. \citet{bagaria2019option} enhance skill specialization by combining skill chaining with deep neural networks, learning initiation set classifiers to reduce the burden of representing states outside initiation regions. % However, the option framework differs from our skill definition. We do not train termination conditions; instead, skill termination is determined by skill parameters. Unlike non-parameterized options, our skills are explicitly parameterized by the target object’s pose, allowing greater adaptability. For example, moving an object to $(1,0)$ and $(0,1)$ is treated as a single skill in our framework, compared to separate options, addressing limitations in generalization and efficiency within our environments.
However, the option framework is not directly applicable to our skills. In the option framework, termination conditions for each option are explicitly trained as part of the policy. In contrast, the termination of our skills is inherently determined by the skill parameters, such as the target object poses. Unlike non-parameterized options, our skills require a policy that selects the skill and specifies its parameters, which implicitly define the termination condition. This fundamental difference makes the direct application of option frameworks unsuitable for our problem.

% Another works perform imitation learning with fixed time step from offline data. A high-level policy is then trained to use the discovered skills to solve to downstream tasks via online RL with environment interaction \cite{eysenbach2018diversity, sharma2019dynamics, lee2019learning, park2022lipschitz, park2023controllability, pertsch2021accelerating, zhang2024extract}, imitation learning (IL) \cite{mulling2013learning, mandlekar2020iris, rosete2023latent, garrett2024skillmimicgen}, or a combination of both \cite{gupta2020relay}. Also, some works train skill dynamics models and incorporate them into the reward function by proximity to the next initiation set \cite{lee2019composing} or use them with model-based RL \cite{shi2023skill, lidexdeform}. The problem with these skill-discovery methods is that they struggle to discover a useful set of manipulation skills. Instead, we make the observation that humans use prehensile and non-prehensile skills in our daily lives to manipulate objects, and build on existing work~\cite{kim2023pre} for learning non-prehensile manipulation policy to acquire these skills.

%Instead, we utilize independently trained prehensile and non-prehensile manipulation skills and integrate them to solve long-horizon tasks.

% Continuous Discretization
% Discrete continualization
% Discret policy + discrete conditioned parameter policy: wei2018hierarchical
% xiong2018parametrized: hybrid version of the DQN and DDPG (discrete action -> DQN, continuosu action -> DDPG)
% 
% jain2020generalization: ???
% li2021hyar: different continuous parameter dimensions

% Like our setup, there are RL-based approaches for using pre-defined skills. In particular, RL for PAMDP \cite{hausknecht2015deep, masson2016reinforcement} aims to find a sequence by of skills and their corresponding continuous parameters. Earlier works in this area address handling discrete-continuous hybrid action space.

% Parameterized Action
% Like our setup, there are RL-based approaches for using pre-defined skills. In particular, RL for PAMDP \cite{hausknecht2015deep, masson2016reinforcement} aims to find a sequence by of skills and their corresponding continuous parameters. Earlier works in this area uses specialized RL algorithms capable of handling both discrete and continuous action spaces~\cite{wei2018hierarchical, xiong2018parametrized, fu2019deep, jain2020generalization, li2021hyar}. While these works primarily focus on simple domains, such as robot soccer, we consider complex and high-dimensional robotic domains. 

% Like our setup, there are RL-based approaches for using pre-defined skills. In particular, reinforcement Learning for Parameterized Action Markov Decision Processes (PAMDPs) \cite{hausknecht2015deep, masson2016reinforcement} focus on sequencing skills and their continuous parameters. \citet{masson2016reinforcement} use DDPG~\cite{lillicrap2015continuous} to train a single policy that outputs both skill probabilities and parameters, but it does not consider dependencies between discrete actions and continuous parameters. To address this, \citet{xiong2018parametrized} propose a hybrid approach, using DQN~\cite{mnih2013playing} to select discrete actions and DDPG to determine continuous parameters, though it remains flat-level training with dependent continuous parameters. \citet{fu2019deep} introduce a hierarchical framework where a high-level policy selects discrete actions, and a low-level policy generates continuous parameters conditioned on the discrete action. However, this method struggles with varying parameter dimensions across discrete actions. \citet{li2021hyar} address this limitation by using a latent action space to generate continuous parameters specific to each discrete action, enabling better handling of dimension variations.


%More recently, HACMan++ \cite{jiang2024hacmanpp} extends primitives to spatially grounded primitives, enabling the robot to generalize across diverse object shapes. 
% How they train high-level policy?
% What components are needed to train high-level policy?
% 1. Option
% 2. Simple RL (or PAMDP?)
% 3. Learning transition model
% 4. From demonstration

%-------------------------------------------------------------------------------------
% option, https://dl.acm.org/doi/10.1145/3453160,  
% explain what is option & how it consists
% explain how option developed, and what is the limitation of them.
% The options framework, introduced by Sutton et al.~\cite{sutton1999between}, extends the MDP by introducing temporally extended actions called \textit{options}. An option represents a sequence of decisions or actions the agent executes over multiple timesteps. The framework is formalized using the Semi-Markov Decision Process (SMDP)~\cite{baykal2010semi}, which generalizes the MDP by allowing actions to take variable durations (multiple timesteps). An option $\omega$ is defined by three components $(I_\omega, \pi_\omega, \beta_\omega)$. The initiation set $I_\omega \subseteq S$ specifies the states where the option can be initiated. The policy $\pi_\omega: S \times A \rightarrow [0, 1]$ defines the actions taken while the option is active. The termination condition $\beta_\omega: S \rightarrow [0, 1]$ provides the probability of terminating the option in a given state $s \in S$.
% how the optiosn are traiend

% Option framework~\cite{sutton1999between, precup2000temporal} provides a framework to model temporally extended actions, called option, in a more generic way so that automatically learning sub-behaviors and their composition becomes more feasible in multiple settings, using the same learning algorithm. The key difference from ours is 

% However, the frameworks assume dense reward signals, which makes training difficult in sparse reward settings. It does not explicitly represent subgoals, leading to task inefficiencies where similar goals are treated as separate skills. For example, moving an object to positions $(1, 0)$ and $(0, 1)$ are treated as independent skills, even though they share similarities. These limitations affect the generalization and efficiency of the framework in our environments.

% The option framework, introduced by \citet{sutton1999between}, extends the MDP by introducing temporally extended actions called \textit{options}. An option $\omega$ is defined by three components $(I_\omega, \pi_\omega, \beta_\omega)$. The initiation set $I_\omega \subseteq S$ specifies the states where the option can be initiated. 
% Options are trained using methods such as policy gradient algorithms. Option-Critic \cite{bacon2017option} proposed policy gradient theorem derived for online learning of options in an end-to-end fashion in discrete action space, extends the option critic the continuous actions space \cite{klissarov2017learnings},  use any off-the-shelf policy optimization \cite{zhang2019dac} for option learning. However, the frameworks assume dense reward signals, which makes training difficult in sparse reward settings. It does not explicitly represent subgoals, leading to task inefficiencies where similar goals are treated as separate skills. For example, moving an object to positions $(1, 0)$ and $(0, 1)$ are treated as independent skills, even though they share similarities. These limitations affect the generalization and efficiency of the framework in our environments.

% 1. train the termination condition
% 2. initation set is almost all states
% 3. when the option is trained, that might not have the exact meaning.
% 4. there is no parameter policy for skills

\subsection{Planning with skills}

% Sampling based method (dogar, barry, mao)
% Optimization based planning method (STAP, LSP)
% Train skill dynamic model + planning (Liang, 
    % DLPA: DLPA~\cite{zhang2024model} trains a parameterized-action-conditioned dynamics model, which includes a transition predictor, a reward predictor, and a continue predictor to model task termination in PAMDPs. It generates actions by planning with a modified Cross-Entropy Method, maximizing cumulative returns using trajectory rollouts predicted by the dynamics model over a planning horizon.

Task and Motion Planning (TAMP) is a problem that combines discrete task planning and continuous motion planning \cite{garrett2020pddlstream, garrett2021integrated}, which has been shown to be effective in solving long-horizon tasks. There are two common methods for solving TAMP: sampling-based and optimization-based approaches. Sampling-based methods \cite{garrett2018ffrob, garrett2020pddlstream, ren2024extended} address task and motion planning problems by generating symbolic action plans with corresponding continuous parameters, sampling and refining trajectories to satisfy both the plan's discrete constraints and motion feasibility. Logic-Geometric Programming (LGP) \cite{toussaint2015logic, toussaint2018differentiable, migimatsu2020object} formulates TAMP as a joint optimization problem over discrete symbolic actions with task constraints, often expressed as a logic program, and continuous motion parameters with a geometric model of the robot and environment. It leverages optimization techniques to find feasible solutions that satisfy both logical and geometric constraints. However, both approaches require preconditions and effects of symbolic actions because these define the logical constants and transitions necessary for the planner to generate feasible high-level plans and corresponding low-level motions. In our case, the preconditions and effects of skills are not available because preconditions cannot guarantee that the skill will succeed, making the effects undefined.

% Leaning skill dynamic models and planning
To acquire the effects of skills, several works \cite{liang2022search, shi2023skill, zhang2024model, lidexdeform} train skill dynamics models and utilize them in planning. \citet{liang2022search} train GNN-based skill effect models, which take as input the current state and a skill parameter and predict the terminal state reached as well as the heuristic-based execution cost of the skills. They then integrate these skill effect models with a graph search by constructing a graph based on their skill effect predictions. \citet{shi2023skill} and \citet{lidexdeform} jointly train skill dynamics models and skills during both the pre-training and downstream adaptation phases and employ the skill dynamics models in model-based RL, which simulates the outcomes of candidate skills and selects the best skill. DLPA~\cite{zhang2024model} trains a parameterized-action-conditioned dynamics model, comprising a transition predictor, a reward predictor, and a continuation predictor, to model task termination in PAMDPs. It generates actions by planning with a modified Cross-Entropy Method, maximizing cumulative returns through trajectory rollouts predicted by the dynamics model over a planning horizon. However, small prediction errors from learned skill dynamics models are critical for our tasks, as the feasible state of the next skill is narrow. For instance, when predicting the terminal state of pushing a card to the edge of a table, even a few millimeters of prediction error could incorrectly determine the feasibility of the prehensile skill. As a result, this could render the plan infeasible and lead to inefficiencies in finding a feasible plan. In contrast, we obtain the terminal states of skills by simulating the skills in parallel.

% Optimizing value function.
Another line of work focuses on skill planning by optimizing a skill sequence and its parameters with respect to the total sum of the value functions \cite{agia2023stap, xue2024logic}. STAP \cite{agia2023stap} performs sampling-based optimization on skill parameters to maximize the summation of the Q-values of a plan skeleton which can be integrated with a task planner, such as PDDLStream \cite{garrett2020pddlstream}. LSP \cite{xue2024logic} generates a skill plan using Monte Carlo Tree Search (MCTS) \cite{browne2012survey} and optimizes skill parameters with the Cross-Entropy Method (CEM) \cite{de2005tutorial} to maximize the sum of the value functions. For better value function approximation, skills are trained based on Generalized Policy Iteration using Tensor Train (TTPI). While these methods require careful reward function design to prevent the dominance of one skill, we can utilize skills regardless of how the reward function is designed.

Similar to our \texttt{Skill-RRT} algorithms, \citet{barry2013hierarchical} propose an RRT-based hierarchical skill planning algorithm that first generates a collision-free object path and realizes it by identifying a sequence of skills. This approach resembles our \texttt{Skill-RRT} algorithm in that it extends a tree toward randomly sampled configurations through skills using RRT. The key differences are that we utilize RL-based skills and can discover new skills (connectors) to bridge gaps between existing skills through \texttt{Lazy Skill-RRT}. Our algorithm is advantageous because it avoids the expensive computation of motion planning and does not assume that skills can be chained directly.

\subsection{Imitation Learning for robotics}
% why imitation is well used, explain about is robustness at robotics task (compared to RL briefly)
%Imitation Learning (IL) \cite{hussein2017imitation} is a machine learning approach where an agent learns to perform tasks by mimicking demonstrations, such as mapping observed states to corresponding actions. 

% To effectively apply IL to robotics tasks, we must address three factors of demonstration data in robotics: action multi-modality, quantity of data, and quality of data \cite{mandlekar2022matters}. Action multi-modality arises from the variability in demonstrations, where the same task can be performed using different action sequences depending on how the goal is achieved. 
% %For example, when flipping a thin card on a table, one might drag it to the left edge to lift it. In contrast, another might drag it to the right edge, achieving the same goal through different intermediate actions. 
% Prior approaches aim to deal with multi-modality by developing new policy architectures such as action discretization \cite{lee2024behavior, shafiullah2022behavior}, applying advanced generative model \cite{chi2023diffusion}, and selecting appropriate pair of network architecture with its training objective function \cite{dalal2023imitating, Zhao-RSS-23}. 

% % automatic large amounts of data generation
% Training IL policy with large amounts of data is important to get high state coverage for preventing distributional shift \cite{mandlekar2022matters} caused by compounding error. Many works have leveraged teleoperation systems \cite{handa2020dexpilot, heo2023furniturebench, mandlekar2018roboturk, mandlekar2019scaling, mandlekar2020human, stepputtis2022system, Zhao-RSS-23} to collect robotic demonstration data. However, collecting large amounts of data via the system is labor-intensive and time-consuming \cite{brohan2022rt, heo2023furniturebench, jang2022bc,  xiang2019task, yang2021trail}, posing a bottleneck in generalization across diverse problems, such as varying initial and goal object poses \cite{dalal2023imitating}.

% Recent approaches automates data collection by leveraging a simulator and planner, such as TAMP algorithms, for reducing reliance on human effort. Using automatically collected data, Driess et al.~\cite{driess2021learning} predicts the feasibility of a given action sequence, based on the initial state, McDonald et al.\cite{mcdonald2022guided} imitates the skill with its parameter given the current state, and dalal et al.~\cite{dalal2023imitating} imitates low-level control of robot given the current state. They show how to transfer the knowledge from large amount of automatically collected data  reducing reliance on human operators for supervision. However, TAMP-based data generation is infeasible in our task because we lack explicit knowledge of a skill's outcome before simulating it, not only the resulting state but also whether the skill will succeed or fail. Therefore, we propose a data generation method incorporating skill simulation as a core component. Our algorithm accelerates planning using GPU-based parallelization to ensure computational efficiency during simulation.

To effectively apply Imitation Learning (IL) to robotics tasks, it is essential to address three critical factors of demonstration data: action multi-modality, data quantity, and data quality \cite{mandlekar2022matters}.

% Action multi-modality refers to the variability in demonstrations, where different action sequences can accomplish the same task depending on the intermediate states. Several works have proposed addressing action multi-modality in demonstration data by developing advanced policy architectures. \citet{lee2024behavior} and \citet{shafiullah2022behavior} introduce methods that integrate discrete action mode predictors to identify distinct modes within the action distribution with continuous action offset correctors to refine predictions for each mode. \citet{Zhao-RSS-23} tackle the challenge of action multi-modality in human demonstrations, which often exhibit subtle variations, by introducing Action Chunking with Transformers (ACT). This approach leverages conditional variational autoencoders (CVAE) integrated within a transformer architecture~\cite{vaswani2017attention} to generate action sequences conditioned on current observations, showcasing robust performance in real-world experiments. \citet{dalal2023imitating} focus on TAMP generated demonstrations, which exhibit significant multi-modality due to the diversity in planned paths and variations in grasps and placements for each object. Their approach leverages Gaussian Mixture Models (GMMs) with five modes, training the model using a log-likelihood objective. This method successfully captures multi-modal action distributions, overcoming the limitations of traditional Mean Squared Error (MSE) loss, which typically results in unimodal policies. Recently, \citet{chi2023diffusion} use diffusion models to address action multi-modality, such as choosing between left or right movements to push a block. Their diffusion policy shows strong imitation learning performance in simulation and real-world settings. Our method addresses demonstration data with identical initial states and goals but differing intermediate object poses, resulting in varied actions at each state. Building on these advances, we adopt the diffusion policy proposed by \citet{chi2023diffusion}, leveraging its robustness in capturing multi-modal action behaviors across simulations and real-world experiments.

Action multi-modality arises in demonstrations where different action sequences achieve the same task due to variations in intermediate states. To address this, advanced policy architectures have been proposed. \citet{lee2024behavior} and \citet{shafiullah2022behavior} introduce methods combining discrete action mode predictors with continuous offset correctors to identify and refine distinct modes in the action distribution. \citet{Zhao-RSS-23} address subtle variations in human demonstrations using Action Chunking with Transformers (ACT). Their approach leverages conditional variational autoencoders (CVAE) within a transformer architecture~\cite{vaswani2017attention} to conditionally generate action chunks (sequences). By utilizing action chunks, this method reduces the task horizon, mitigates compounding errors, captures temporally correlated behaviors, and smoothens action execution through overlapping predictions. \citet{dalal2023imitating} focus on TAMP-generated demonstrations, using Gaussian Mixture Model (GMM) with five modes to model diverse paths, grasps, and placements, outperforming unimodal approaches based on Mean Squared Error (MSE) loss. Recently, \citet{chi2023diffusion} address action multi-modality using diffusion models, capturing diverse strategies like left or right end-effector movements to push a block. Their policy demonstrates strong performance in simulations and real-world settings. Our method builds on these advances, leveraging the diffusion policy by \citet{chi2023diffusion} to handle demonstrations with identical start and goal states but varied intermediate object poses, capturing multi-modal behaviors effectively across different environments.
% instead of paper name -> write the model name

% automatic large amounts of data generation
% IL policies often suffer from distributional shift, which occurs when the policy encounters states during deployment that significantly differ from those observed during training and lack action supervision in the dataset. To address this issue, training IL policies with large and diverse datasets is essential to ensure high state coverage, thereby reducing the risk of distributional shift \cite{mandlekar2022matters}. Many studies have utilized teleoperation systems \cite{handa2020dexpilot, heo2023furniturebench, mandlekar2018roboturk, mandlekar2019scaling, mandlekar2020human, stepputtis2022system, Zhao-RSS-23} to collect robotic demonstration data for this purpose. However, gathering large-scale datasets through teleoperation is both labor-intensive and time-consuming \cite{brohan2022rt, heo2023furniturebench, jang2022bc, xiang2019task, yang2021trail}, creating a bottleneck for generalizing IL policies to diverse problems, such as variations in initial and goal object poses \cite{dalal2023imitating}.

Imitation learning (IL) policies often face distributional shift, which occurs when states differ significantly from those seen during training and lack action supervision \cite{mandlekar2022matters}. Training with large, diverse datasets can mitigate this issue by increasing state coverage.
% need to think about the above sentence, diverse initial goal
To achieve this, many studies rely on teleoperation systems to collect robotic demonstration data \cite{handa2020dexpilot, heo2023furniturebench, mandlekar2018roboturk, mandlekar2019scaling, mandlekar2020human, stepputtis2022system, Zhao-RSS-23}. However, teleoperation-based data collection is labor-intensive and time-consuming, creating challenges in scaling datasets for diverse tasks, such as varying initial and goal object poses \cite{brohan2022rt, heo2023furniturebench, jang2022bc, xiang2019task, yang2021trail, dalal2023imitating}.

% Recent approaches automate data collection for robotics tasks by leveraging simulators and planning algorithms, such as TAMP frameworks, to generate large-scale datasets for training IL policies. \citet{driess2021learning} use Logic Geometric Programming (LGP)~\cite{toussaint2018differentiable}, a TAMP solver, to generate high-level action sequences, with Model Predictive Control (MPC) producing low-level control transitions. Their method employs a two-level hierarchical policy to imitate the TAMP solution: the high-level policy, informed by observations and action sequences, predicts the feasibility and provides parameters to the low-level policy from LGP solutions. The low-level policy imitates control transitions from MPC's output with inputs the parameter from high-level and the robot's current joint configurations. % Instead of evaluating feasibility, \citet{mcdonald2022guided} directly train the high-level policy to imitate action types and their parameters from TAMP solution. Their parallelized Modular TAMP approach utilizes FastForward~\cite{hoffmann2001ff} to generate initial action sequences, which motion planners refine into executable trajectories. The approach employs a two-level hierarchical policy: the high-level policy, trained on symbolic planner outputs, predicts action types (e.g., "place") and parameters (e.g., $obj1$) using one-hot encodings. The low-level policy, trained via imitation learning on successful motion planning trajectories, uses an attention module to map continuous observations and discrete parameters into continuous geometric frames, such as determining the placement of $(obj1, targ2)$. To predict task-level actions, the learner requires a fixed set of ground actions, limiting the policy's adaptability to tasks with varying numbers of objects. \citet{dalal2023imitating} address this by leveraging the PDDLStream planning framework~\cite{garrett2020pddlstream} to model their TAMP domain with an adaptive, sampling-based algorithm. Their method includes samplers for grasp generation, placement, inverse kinematics, and motion planning. Unlike hierarchical approaches, they adopt a single-level imitation learning policy that directly learns low-level end-effector control from the generated data, bypassing the need to imitate high-level skills or parameters. However, TAMP-based data generation is infeasible in our task due to the lack of prior knowledge about the resulting state and the skill’s success or failure. To address this, we propose a data generation method that integrates skill simulation as a fundamental component. Our approach accelerates planning through GPU-based parallelization, ensuring computational efficiency during simulation.

Recent approaches automate data collection for robotics tasks using simulators and planning algorithms, such as TAMP frameworks, to generate large-scale datasets for training IL policies. \citet{driess2021learning} use Logic Geometric Programming (LGP)~\cite{toussaint2018differentiable} to generate high-level action sequences, with Model Predictive Control (MPC) producing robot joint controls. Their method employs a two-level hierarchical policy to imitate TAMP solutions: the high-level policy predicts the feasibility of action sequences from LGP solutions and provides parameters to the low-level policy, which then imitates control transitions from MPC using these parameters and the robot's current joint configurations. Instead of evaluating feasibility, \citet{mcdonald2022guided} train the high-level policy to imitate action types and their parameters from TAMP solutions. Their parallelized Modular TAMP approach uses FastForward~\cite{hoffmann2001ff} to generate initial action sequences, refined into executable trajectories by motion planners. The two-level hierarchical policy imitates symbolic planner outputs for high-level action selection, predicting action types (e.g., "place") and parameters (e.g., $obj1$) using one-hot encodings. The low-level policy, trained on successful motion planning trajectories, employs an attention module to map continuous observations and discrete parameters into continuous geometric frames, such as determining the placement of $(obj1, targ2)$. To predict task-level actions, a fixed set of ground actions is required, limiting the policy's adaptability to tasks with varying object numbers. \citet{dalal2023imitating} use a single-level IL policy to directly imitate end-effector control from generated data, bypassing the need to imitate high-level skills or parameters. Their method generates TAMP solutions using the PDDLStream planning framework~\cite{garrett2020pddlstream} with an adaptive, sampling-based algorithm, incorporating samplers for grasp generation, placement, inverse kinematics, and motion planning. However, TAMP-based data generation is infeasible in our task due to the lack of prior knowledge about the resulting state and the skill’s success or failure. To address this, we propose a data generation method that integrates skill simulation as a fundamental component. Our approach accelerates planning through GPU-based parallelization, ensuring computational efficiency during simulation.

% data quality
% Training with high-quality demonstrations is crucial for IL \cite{mandlekar2022matters}, as low-quality data can lead the robot into high-risk states, potentially causing task failure (e.g., dropping the card onto the table) or getting stuck in out-of-distribution states not covered by the demonstration data. To address this, \citet{dalal2023imitating} evaluate demonstration data by imposing a containment constraint as a bounding box within the workspace, pruning trajectories where the end-effector pose exits the defined box. This approach helps ensure that the imitation-learned policy avoids encountering out-of-distribution (OOD) states. While restricting the workspace is an efficient method to train an IL policy to remain within in-distribution data and improve robustness, it requires additional domain knowledge about the environment. Unlike the method, we evaluate the quality of skill plans (sequence of skill with its parameters) generated by \texttt{Skill-RRT} rather than the quality of demonstrations (sequence of state and actions). The evaluation is achieved by replaying the plans using the simulator's stochasticity without prior domain knowledge. Demonstrations generated from the replays of high-quality plans are then used to train the IL policy.

Training with high-quality demonstrations is essential for IL \cite{mandlekar2022matters}, as low-quality data can lead to high-risk states, causing failures or out-of-distribution (OOD) states. To mitigate this, \citet{dalal2023imitating} impose a containment constraint within a defined bounding box, pruning trajectories where the end-effector exits the workspace. This ensures the learned policy avoids OOD states but requires additional domain knowledge. In contrast, we evaluate the quality of skill plans (sequences of skills with parameters) generated by \texttt{Skill-RRT}, not demonstrations (state-action sequences). The evaluation relies on replaying plans under simulator stochasticity without domain knowledge, with high-quality replays used to train the IL policy.
% robomimic

% For creating high-quality dataset, prior approach filters out low-quality data based on criteria such as trajectory length to mitigate compounding errors and staying within the constrained workspace by additional domain knowledge about the environment to reduce distributional shift \cite{dalal2023imitating}. Unlike previous methods, we evaluate the quality of skill plans (sequence of skill with its parameters) generated by \texttt{Skill-RRT} rather than the quality of low-level state action demonstrations. The evaluation is achieved by replaying the plans using the simulator's stochasticity without prior environment information. Demonstrations that successfully replay high-quality plans are then used to train the IL policy. 