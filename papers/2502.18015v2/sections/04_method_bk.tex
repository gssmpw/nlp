% how constructed methodology
% \input{tables/notations/state}
% \input{tables/notations/skill}
% \input{tables/notations/skill-rrt}
% \input{tables/notations/connector}
% \input{tables/notations/IL}
\begin{figure*}[ht] % Force the figure at the top of the page
\centering
\vspace{-10mm}
\resizebox{\textwidth}{!}{
    \includegraphics{figures/cpnp_overview_v7.png}
}
% \caption{Overview of our method: (a) A skill library $\mathcal{O}$ that consists of $n$ number of non-prehensile (NP) and $m$ number of prehensile (P) skills are given. (b) We first train connector skills. To do this, we use \texttt{Abstract Skill-RRT} to collect pairs of states that the connector needs to connect, based on the preconditions and effects of the skills in $\mathcal{O}$, and the connectors are trained using reinforcement learning (RL). (c) We then generate data for imitation learning by running \texttt{Skill-RRT} with the skills in $\mathcal{O}$ and connector skill. Given an initial node $q_{\text{init}}$ (red circle) and a goal object pose $q^{\text{obj}}_{\text{goal}}$ (green circle), we generate a skill plan $\tau_{\text{skill}}$, which includes a sequence of skills (blue lines), connectors (yellow lines), and their associated subgoals (black circle). (d) We filter out low-quality data by replaying each skill plan N times, and filtering skill plans with low replay success rate based on a pre-defined threshold $\theta$. The filtered trajectories are used to train diffusion policy. The final diffusion policy is deployed in the real world in a zero-shot manner. }\label{fig:CPNP_overview} % Place the label after the caption
\caption{Overview of our method: (a) Examples of given skills in the bookshelf domain~(Figure~\ref{fig:CPNP_overview}, row 2). $K_\text{P}$ is a prehensile skill, $K_{\text{NP}_1}$ is a toppling skill that topples a book, and $K_{\text{NP}_n}$ is a pushing skill that pushes an object at a bottom shelf. (b) We first use \texttt{Lazy Skill-RRT} to collect a set of $(v.s, v_{\text{connect}}.s, K)$ triplets which defines the connections the connectors need to make, and $v$ denotes a node of RRT. The middle of (b) shows an example where the connector has to move the gripper from the end of the prehensile skill's state, $v.s$, to the beginning of the pushing skill, $v_{\text{connect}}.s$. We use RL to train connectors. (c) \texttt{Skill-RRT} is run with the trained connectors $\mathcal{C}$ and the set of skills $\mathcal{K}$ to generate a skill plan $\tau_{\text{skill}}$. Starting from the initial node $s_{0}$ (red circle) and the goal object pose $q_{\text{obj}}^{g}$ (green circle), the skill plan consists of a sequence of skills, connectors, and their associated desired object poses, or desired robot configuration (black circles). (d) We use IL to train skills using skill plans. To filter data, we replay each skill plan $N$ times, and those with a replay success rate below a predefined threshold $m$ are filtered out. The remaining high-quality trajectories are used to train a diffusion policy, which is deployed in the real world in a zero-shot manner.}\label{fig:CPNP_overview}
\vspace{-2mm}
\end{figure*}

\subsection{Problem formulation for prehensile-and-non-prehensile (PNP) manipulation problems}\label{method:PF}
\newcommand{\skillset}{\mathcal{K}}
\newcommand{\skill}{K}
\newcommand{\npskill}{K_{\text{NP}}}
\newcommand{\pskill}{K_{\text{P}}}
\newcommand{\applicabilitychecker}{\phi}
\newcommand{\policy}{\pi}
\newcommand{\qobj}{q_\text{obj}}
\newcommand{\qobjg}{q_\text{obj}^g}
\newcommand{\vobj}{\dot{q}_\text{obj}}
\newcommand{\Qobj}{Q_\text{obj}}
\newcommand{\qrobot}{q_\text{r}}
\newcommand{\vrobot}{\dot{q}_\text{r}}
\newcommand{\fsim}{f_{\text{sim}}}
\newcommand{\skillplan}{\tau_{\text{skill}}}

\newcommand{\node}{v}
\newcommand{\pipre}{\pi_{\text{pre}}}
\newcommand{\pipost}{\pi_{\text{post}}}


We denote the state space as $S$, action space as $A$, and space of stable and relevant object configurations as $\Qobj \subset SE(3)$, which only includes regions where the object can occupy, determined by environmental constraints like the presence of shelves. A state $s \in S$ includes object pose and velocity, robot joint positions, and velocities, denoted $\qobj, \vobj, \qrobot$ and $\vrobot$ respectively. An action $a \in  A$ consists of the target end-effector pose, the gain and damping values for a low-level differential inverse kinematics (IK) controller, and the target gripper width.

We assume we are given a set of manipulation skills, $\skillset=\{\pskill,K_{\text{NP}_1},\cdots,K_{\text{NP}_n}\}$, one of which is prehensile and the rest are non-prehensile (NP) skills. For instance, in the bookshelf domain, we have a \textit{topple} and \textit{push} non-prehensile skills, and one prehensile skill for placing the book in a desired region. We assume that an NP skill has an associated region defined on $\Qobj$; for instance, the topple skill is associated with object poses where the object is nearly upright with respect to the shelf. 

Each skill $K$ is a tuple of two functions, $K =\{ \applicabilitychecker, \pi\}$, where $\pi: S \times \Qobj \rightarrow A$ is a goal-conditioned policy that maps a state $s$ and a desired object pose $\qobj$ to an action\footnote{More accurately, instead of using $s$ directly, we use quantities derived from $s$. See Appendix~\ref{Appendix:NP_Skill},~\ref{Appendix:P_Skill} and~\ref{Appendix:Connector}}. We train our skills using RL, using the same policy computation structure from~\cite{kim2023pre}, where our policy $\pi$ consists of $\pipre$ and $\pipost$\footnote{$\pipre$ uses a different action space to ensure contact. See Appendix~\ref{Appendix:NP_Skill} and~\ref{Appendix:P_Skill}.}. $\pipre$ is a \emph{pre-contact} policy that predicts the robot configuration prior to executing the \emph{post-contact} policy that actually manipulates the object. For instance, for our prehensile skill, $\pipre$ puts the robot at a pre-grasp configuration, and $\pipost$ places the object. 

% The function $\applicabilitychecker:S \times \Qobj \rightarrow \{0,1\}$ is an applicability checker that tests whether it is possible to execute the skill policy in $s$ with $\qobj \in \Qobj$ as its goal. This could be an IK solver that checks the existence of a feasible grasp for both the current and target object poses for a prehensile skill, or a simple check for whether the desired object pose and the current object pose belong to the same region for an NP skill. Note that $\phi$ only checks whether skill \textit{can} be applied in the current state, and not whether the skill would succeed. For a more detailed skill examples, see experiment section, Table ~\ref{tab:skill_defn}.


The function $\applicabilitychecker:S \times \Qobj \rightarrow \{0,1\}$ is an applicability checker that tests whether it is possible to execute the skill policy in $s$ with $\qobj \in \Qobj$ as its goal. This could be an IK solver that checks the existence of a feasible grasp for both the current and target object poses for a prehensile skill, or a simple check for whether the desired object pose and the current object pose belong to the same region for an NP skill. Note that $\phi$ only checks whether skill \textit{can} be applied in the current state, and not whether the skill would succeed. For a more detailed skill examples, see experiment section, Table ~\ref{tab:skill_defn}.

Lastly, we assume we have a simulator $\fsim$ that takes in state $s$ and $\pi(\cdot;\qobj)$, a policy conditioned on desired object pose $\qobj$, and simulates the policy for $N_{sim}$ number of time steps, and returns the next state, $\fsim: S \times \Pi \rightarrow S$ where $\Pi$ is the set of policies of skills defined in $\skillset$. 

Given a pair of an initial state $s_0 \in S$ and the ultimate goal object pose $\qobj^g \in \Qobj$, the objective of a PNP problem is to find a sequence of actions for achieving $\qobj^g$. Because our policies output actions, this problem reduces to finding the sequence of skills and associated intermediate object poses, that we call a \textit{skill plan}, $\skillplan = \{\pi^{(t)},q^{(t)}\}_{t=1}^{T}$, such that when we simulate the sequence of $T$ policies from $s_0$, the resulting state would have $\qobj^g$ as its object pose.

At a high-level, our method uses \texttt{Skill-RRT} to collect data, and then distill it to a policy using IL. But to find the trajectories using \skillrrt, we first need connectors, and to do that, we need \lazyskillrrt~to collect relevant skills and states for which to train connectors. See Figure~\ref{fig:CPNP_overview} for the overview. We first introduce \texttt{Skill-RRT}, from which you can define \lazyskillrrt.

% i think we need to introduce the notion of sim state, and that we store everything inside the node
% to simulate the state. I am tempted to write Skill RRT to take as input the initial simulation state, and formulate the entire problem with this initial sim state as given.

%TODO: I need to mention the GPU acceleration

\subsection{Skill-RRT}
\newcommand{\extend}{\texttt{Extend}}
\newcommand{\connectorskillset}{\mathcal{C}}

\begin{algorithm}[h]
\caption{\skillrrt($s_0, \qobj^g, \skillset, \connectorskillset,\Qobj$)}
\label{algo:skill-rrt}
\begin{algorithmic}[1]
\State $T=\emptyset$   
\State $v_{0} \gets \{(\emptyset, \emptyset), s_0\}$ 
\State $T$.\texttt{AddNode}($parent=\emptyset, child=v_{0}$)

\For{$i = 1$ \textbf{to} $N_{\text{max}}$}
    \State ${K}, \qobj \gets $\hyperref[algo:UnifSmplSkillAndSubgoal]{\texttt{UnifSmplSkillAndSubgoal}}($\skillset,\Qobj$)

    \State ${v}_{\text{near}} \gets$ \hyperref[algo:GetFeasibleNearestNode]{\texttt{GetApplicableNearestNode}}($T, K, \qobj$)
    \If{$v_\text{near}$ is $\emptyset$} 
        \State \textbf{continue}
    \EndIf
    %TODO 
    %   A bit of complication here; what do we do with connector policies? 
    %   Ideally, we would pass in the connector set, and if that is empty, we would declare this as a lazy RRT. How about the following fix?
    \State \hyperref[algo:extend]{\texttt{Extend}}$\big(T, v_{\text{near}}, K, \qobj, \connectorskillset\big)$
    \If{\texttt{Near}($\qobj^g,T$)}
        \State \textbf{Return} \texttt{Retrace}($\qobj^g, T$)
    \EndIf
\EndFor
\State \textbf{Return} None

% \Statex
% \hrulefill
\end{algorithmic}
\label{algo:skillrrtbackbone}
\end{algorithm}
Algorithm~\ref{algo:skillrrtbackbone} provides the pseudocode for \skillrrt. The algorithm takes as input an initial state $s_0$, goal object configuration $\qobjg$, set of skills $\skillset$, set of connectors $\connectorskillset$, and object regions $\Qobj$. As we will soon explain, setting $\connectorskillset$ to an empty set makes it \lazyskillrrt.

The algorithm begins by initiating the tree, $T$, with an empty set, defining the root node, and adding it to the tree (L2-3). A node $v$ consists of a state $s$ and a pair of a skill policy $\pi$ and its desired object pose $\qobj$ that have been applied to the parent state to achieve the current state. For the root node, the policy and pose are set to an empty set (L2). We then begin the main for-loop. We first uniform-randomly sample a skill $K$ and the desired object pose for the skill, $\qobj$, using the function \texttt{UnifSmplSkillAndSubgoal} (L5), and then compute the nearest node from the tree among the nodes where $K$ can be applied (L6). Specifically, \texttt{GetApplicableNearestNode} function returns the nearest node $v$ where $K.\phi(v.s, \qobj)$ is true, and an empty set if no such node exists. If the function returns an empty set, we discard the sample, and move to the next iteration (L7-8). Otherwise, we use \texttt{Extend} function with $v_{near}$ (L9). Lastly, at every iteration, we check if $\qobjg$ is close enough to any of the nodes in the tree, and if it is, return the path by calling the \texttt{Retrace} function that computes a sequence of $v$ from root node to that node (L10-11). If no such node can be found after $N_{max}$ number of iterations, we return None. 

\iffalse
\begin{algorithm}[H]
\caption{\texttt{GetFeasibleNearestNode}($T, K, \qobj,\texttt{Dist})$}\label{algo:NearestNode}
\begin{algorithmic}[1]
    \State $V_{\text{sorted}} \gets \texttt{SortBy}(T.V, \qobj, \texttt{Dist})$
    \For{$v$ in $V_\text{near}$}
        \If{$K.\phi(v.s, \qobj)$} \Comment{Checks applicability}
            \State \Return $v$
        \EndIf
    \EndFor
    \State \Return $\emptyset$
\end{algorithmic}
\end{algorithm}
\fi

\newcommand{\connectingnode}{v_{\text{connect}}}
Algorithm~\ref{algo:extend} shows the $\texttt{Extend}$ function. The function takes in the tree $T$, skill to be simulated $K$, node $v$ that we will use $K$ from, desired object pose we will extend to, $\qobj'$, and set of connectors $\connectorskillset$. The algorithm begins by computing a pre-contact robot configuration for $K$, $\qrobot'$, using $K$'s pre-contact policy at state $v.s$ with $\qobj'$ as a goal (L1). It then creates a connecting node, using $\texttt{ComputeConnectingNode}$ shown in Algorithm~\ref{algo:computeconnectingnode}. This algorithm first checks if $\connectorskillset$ is empty, in which case this will instantiate \lazyskillrrt. In this case, we create a connecting node $\connectingnode$ with empty skill and object pose, with the same state as $v$, except the robot joint configuration is teleported to $\qrobot'$ (L2-4). Otherwise, we retrieve the connector $\pi_C$ for the given skill $K$ (L6), simulate it from $v.s$ with pre-contact configuration $\qrobot'$ as a goal (L7), and create $\connectingnode$ using $\pi_C$, $\qrobot'$, and the resulting state $s'$ (L8). We then return the connecting node $\connectingnode$, to the \texttt{Extend} function. From the connecting node's state, $\connectingnode.s$, we simulate the post-contact policy $\pipost$  with $\qobj'$ as its goal. If the skill fails to achieve $\qobj'$, then we return without modifying the tree (Algorithm~\ref{algo:extend}, L3-5).
Otherwise, we create a new node $\node'$ with the resulting state $s'$, simulated policy $\skill.\pipost$, and the desired object pose $\qobj'$ (L7). We add $\connectingnode$ with its parent as $\node$, and $\node'$ with $\connectingnode$ as its parent.

%For \lazyskillrrt, we pass in empty set for $\connectorskillset$ to \skillrrt, 
%We use this backbone to define \lazyskillrrt~that teleports the robot to the pre-contact configuration in simulation. Algorithm~\ref{algo:lazyextend} describes the algorithm. The algorithm takes as input the tree $T$, skill that will be simulated, $K$, the node to extend from, $v$, and the target object pose for the skill, $\qobj'$. The algorithm begins by computing the pre-contact robot configuration for the skill, $\qrobot'$ (L1). It then creates the node that \textit{would have} resulted \emph{if} we had executed a connector policy from $v$, $\connectingnode$, which practically have the same state as $\node$ except the robot configuration at $\qrobot'$ and $K.\pipre$ as its policy (L2-3). We then simulate the post-contact policy from the state of $\connectingnode$ with $\qobj'$ as its goal to get the resulting state $s'$, and check if the skill has achieved $\qobj'$. If not, we return without modifying the tree (L4-6). Otherwise, we create a new node $\node'$ with the resulting state $s'$, simulated policy $\skill.\pipost$, and the target object pose (L7). We add $\connectingnode$ with its parent as $\node$, and also $\node'$ with $\connectingnode$ as its parent. \lazyskillrrt~is defined as an instantiation of \skillrrtbackbone~with \texttt{LazyExtend} as the extend function.

\begin{algorithm}[h]
\caption{\texttt{Extend}($T, \skill, \node, \qobj', \connectorskillset$)}\label{algo:extend}
\begin{algorithmic}[1]
\State $\qrobot' \gets \skill.\pipre(\node.s.\qobj;\qobj')$
\State $\connectingnode \gets \hyperref[algo:computeconnectingnode]{\texttt{ComputeConnectingNode}}(\qrobot', \node, \skill, \connectorskillset)$
\State $s' \gets \fsim(\connectingnode.s, \skill.\pipost(\connectingnode.s;\qobj'))$
\If{$\hyperref[algo:Failed]{\texttt{Failed}}(s', \qobj')$}
\State \Return
\EndIf
\State $v' \gets (\skill.\pipost, \qobj',s')$
\State $T.\texttt{Add}(parent=\node, child=\connectingnode)$
\State $T.\texttt{Add}(parent=\connectingnode, child=\node')$
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[h]
\caption{\texttt{ComputeConnectingNode}($\qrobot',\node,\skill,\connectorskillset$)}\label{algo:computeconnectingnode}
\begin{algorithmic}[1]
\State IsLazy $\gets \connectorskillset == \emptyset$
\If{IsLazy}
\State $\connectingnode \gets (\emptyset, \emptyset, \node.s)$
\State $\connectingnode.s.\qrobot \gets \qrobot'$ \Comment{Teleport the robot's config}
\Else
\State $\pi_C \gets \texttt{GetConnectorForSkill}(\connectorskillset,\skill) $
\State $s' \gets \fsim(\node.s, \pi_C(v.s;\qrobot'))$
\State $\connectingnode \gets (\pi_C,\qrobot', s')$
\EndIf
\Return $\connectingnode$
\end{algorithmic}
\end{algorithm}

Our description of \skillrrt~gives only a high-level description of some of the subroutines such as \texttt{UnifSmplSkillAndSubgoal}, \texttt{GetFeasibleNearestNode} or \texttt{Failed}~for brevity and clarity. Their detailed pseudocodes are in Appendix~\ref{Appendix:Skill-RRT Details}. Also, we use GPU-parallelized version of \texttt{Skill-RRT} in our experiments, which is described in Appendix~\ref{Appendix:Skill-RRT Details}.

\subsection{Training the connectors}\label{method:train_connector}
To train a set of connectors $\connectorskillset$, we collect problems using \lazyskillrrt{} by first creating a PNP problem that consists of a pair $(s_0,\qobjg)$, and then running Algorithm~\ref{algo:skill-rrt} with $\connectorskillset=\emptyset$. This will return a solution node sequence, some of which will be $\connectingnode$. We then collect a set of $(\node.s,\connectingnode.s,K)$ triplet from the node sequence, where $\node$ is the parent of $\connectingnode$, and $K$ is the skill that was used from $\connectingnode$ to get its child node.

% There is something odd about the definition of K above.
% Why is this a skill used from v_connect? Because we need to know what skill came after the connecting node, because we need the skill associated with the connector that got us to v_connect

We then train a connector $\pi_C$ for each $K$, whose goal is to go from $\node.s$ to the connecting node's robot configuration $\connectingnode.s.\qrobot$, using PPO~\cite{schulman2017proximal}, with minimum disturbance to the object pose in $\node.s$. This is a goal conditioned policy that maps a state to an action, $\pi_C(\cdot;\qrobot): S \rightarrow A$. The reward consists of four main components,
\[
    r_{\text{connector}} = r_{\text{ee}} + r_{\text{tip}} + r_{\text{obj-move}} + r_{\text{success}},
\]
where $r_{\text{ee}}$ and $r_{\text{tip}}$ are dense rewards based on the distances between the current and target end-effector poses and gripper tip positions, respectively. $r_{\text{obj-move}}$ penalizes an action if it changes the object pose from previous object pose, and $r_{\text{success}}$ gives a large success reward for achieving the target end-effector pose and gripper width. More details about the implementation of these reward terms can be found in Appendix~\ref{Appendix:Connector}.




%%% Continue from here

\subsection{Distilling \skillrrt~to a policy using IL}\label{method:IL}
%Directly applying a skill plan \(\tau_{\text{skill}}\), generated via \skillrrt, in the real world is impractical because planning requires significant online computation time, and frequent re-planning is necessary because of the modeling error. So, we distill \skillrrt~into a policy using IL. In particular, we use diffusion policy~\cite{chi2023diffusion}, which is able to handle multi-modal data without having to specify the number of modes, as in Gaussian Mixture Models~\cite{robomimic2021}, are easier to train than Generative Adversarial Nets (GANs)~\cite{goodfellow2014generative}, and is able to generate much sharper outputs than Variational Autoencoders (VAEs)~\cite{kingma2013auto} .


We distill \skillrrt~into a policy using Diffusion Policy~\cite{chi2023diffusion}, which have shown promising results in robotics, by imitating the solutions generated by \skillrrt. Despite their high performance, diffusion models have slow inference speeds due to the iterative denoising process. To achieve faster inference, we use a U-Net \cite{ronneberger2015u} architecture as the backbone instead of Transformers \cite{vaswani2017attention}, as Transformers incur high computational costs due to attention mechanisms. Additionally, we further reduce inference time by removing action chunking and state history from the diffusion policy. As a result, the policy is able to output actions at 75 Hz on an Nvidia 4090 GPU. Details of imitation learning such as hyperparameters are described in Appendix~\ref{Appendix:imitation_learning}.
%Inference speed is especially critical in our tasks because the robot must react quickly based on perceptual feedback. 

An action space of our diffusion policy uses target robot joint positions instead of the target end-effector pose, as used in the skill's policy. This eliminates the need for IK computation and enables faster inference. The policy uses a slightly different state space from the skill's, as shown in Table~\ref{table:IL_state}, to accommodate the simulation-to-reality gap. For instance, we found that the joint velocity of our robot (Franka Research 3) has a significant sim-to-real gap, so we use joint position from the previous time step, $q^{(t-1)}_r$, instead of using velocity directly. We also found that Franka's gripper cannot accept another command until the previous command is completed, which takes approximately 1.2 seconds. So, we include a binary input indicating whether that time has elapsed and is ready for another gripper command, $\mathbbm{1}_\text{gripper-executable}$.

Further, we use key points instead of poses for end-effector, object, and gripper tip, denoted $p_\text{ee}, p_\text{obj}$ and $p_\text{tip}$ respectively. The key points are the locations of eight corners of a bounding box for the gripper and object, and the locations of the tip of two fingers on the gripper for the gripper tip. We use key points rather than poses because poses involve orientations, which are known to be discontinuous and difficult for a neural network to learn~\cite{zhou2019continuity}.
%On the Continuity of Rotation Representations in Neural Networks
\input{tables/IL_state}


To generate training data, we create multiple PNP problems and solve them using \skillrrt. To improve the success rate, we filter the generated data based on quality, ensuring that the dataset remains robust to perception and modeling errors. Specifically, we prioritize solutions that are more likely to reach the goal despite these uncertainties. For example, in the card flip domain, placing the card at the far end of the table might technically solve the problem, but even a slight perception error could cause the object to fall, making such solutions less reliable.

To create such a robust dataset we evaluate each skill plan, $\skillplan$, by executing it $N$ times while introducing noise into both the state and the torque output of the low-level controller. For object pose, we apply different scales of noise in different domains. We also add random noise to the commanded torque to reflect real-world noise effects in robot controller. The noises for each domain are summarized in Table \ref{table:DR_params}. We only accept skill plans whose success rate exceeds a threshold, $m$, i.e., if $N_{success}/N > m$. A high success rate during these replays indicates that the plan is reliable even under disturbances. 



%For example, when a book is in between two other books in the bookshelf domain, the noise in the object pose is larger because the book is only partially visible. 



 %Additionally, we randomize the friction of the environment since real-world friction is unknown and

% The noise injection process is described in detail in Appendix~\ref{Appendix:DR}.


\input{tables/DR_params}

\iffalse
\begin{algorithm}[H]
\caption{Data filtering}\label{algo:Collect_data}
\begin{algorithmic}[1]
\State \textbf{Input:} Skill library ${\mathcal{O}} = \{o_{\text{NP}_1}, o_{\text{NP}_2}, \ldots, o_{\text{NP}_n}, o_{\text{Place}}\}$
\State \textbf{Initialize:} Counter $n \leftarrow 0$, dataset $\mathcal{D} \leftarrow \emptyset$
\While{$n < 500$}
    \State Randomly sample initial $q^\text{obj}_{\text{init}}$ and goal object pose $q^\text{obj}_{\text{goal}}$
    \State Generate skill plan $\tau_{\text{skill}} = \text{Skill-RRT}(q^\text{obj}_{\text{init}}, q^\text{obj}_{\text{goal}}, {\mathcal{O}})$
    \State Replay the skill plan $\tau_{\text{skill}}$ 400 times to obtain full trajectories $\tau_{\text{full}}^{(i)}$ for $i = 1, \ldots, 400$
    \State Compute replay success rate $r = \frac{1}{400} \sum \text{success}(\tau_{\text{full}}^{(i)})$
    \If{$r \geq 0.9$}
        \State $\mathcal{D} \leftarrow \mathcal{D} \cup \{\text{selected 30 successful trajectories}\}$
        \State $n \leftarrow n + 1$
    \EndIf
\EndWhile
\end{algorithmic}
\end{algorithm}
\fi
%Using entire trajectories generated by \texttt{Skill-RRT} can degrade the performance of the distillation policy. Even if a skill plan is successfully generated, it may not consistently achieve the task due to uncertainties in state transitions caused by contact-rich dynamics. For example, if a subgoal is positioned very close to the edge of the table, it might solve the task but risk dropping the card due to dynamic uncertainties in the real world. Therefore, we need a metric to measure whether the generated skill plan is robust enough to reliably solve the given task and to filter out those with poor performance.

% TODO this needs to be moved to the related work
%Previous works \cite{agia2023stap, xue2024logic} propose a metric for measuring the optimality of skill plans as the sum of value functions. However, these approaches require either additional modules, such as skill dynamics models and uncertainty quantification of skills, or specific forms of RL training to accurately approximate the value function. Instead, we propose a simpler metric for measuring the robustness of skill plans by replaying them $N$ times in simulation. Specifically, for a given skill plan, we sequentially replay the skills and their subgoals $N$ times. We then calculate the replay success rate, defined as the ratio of successful replays. A skill plan replay is considered successful if all the skills are executed successfully and reach the given goal, while it is considered a failure if any skill fails to achieve its subgoal (e.g., due to timeout or dropping an object). The replay success rate of all skill plans is measured, and plans with a replay success rate lower than a predefined threshold $\theta$ (e.g. 90\%) re filtered out. Trajectories consisting of state-action data pairs generated by replaying skill plans with high replay success rates are then used to train the distillation policy.

% we evaluate each skill plan by replaying it $N$ times in simulation. When a skill plan $\tau_\text{skill}=\{ q^{\text{obj}}_{\text{init}}, q^{\text{obj}}_{\text{goal}}, \{(c,q^{\text{robot}}_{\text{pre}})_k, (o,q^{\text{obj}}_{\text{sg}})_k\}_{k=0}^K) \}$ is found, its skills and connectors are re-executed N times. The replay success rate of the skill plan is then measured based on whether all skills and connectors succeed during replay. Skill plans with a replay success rate lower than a predefined threshold $\theta$ (e.g. 90\%) are filtered out. A high replay success rate indicates that the skill plan consistently succeeds in reaching the goal despite uncertainties in state transitions and domain randomizations. Training the IL policy with such high-quality trajectories improves its overall performance.

%To address this, we retain only skill plans with a high success rate, ensuring the policy performs reliably across trials and generalizes better during deployment. Specifically, only skill plans with a success rate exceeding a predefined threshold \( \theta \) are included in the dataset \( \mathcal{D}_{\text{imitate}} \). The dataset is curated by replaying skill plans on randomly selected tasks and retaining only successful trajectories, resulting in high-quality training data.

% Once the dataset \(\mathcal{D}_{\text{imitate}}\) is collected, we employ a U-Net-based diffusion model \cite{ho2020denoising, chi2023diffusion} to train the imitation learned policy \(\pi_{\text{IL}}\). Diffusion models are particularly suitable for our task, as they excel in handling multimodal and high-dimensional data. In our dataset, the same state may lead to different actions depending on the skill or subgoal, making multimodality a key challenge.

% The policy is trained to predict a single action for each state at a single time step. Since our task is contact-rich, the policy performs better with frequent observation feedback. Also we use only the single time-step observation during training for reducing computational complexity. The input and output configuration of the policy $\pi_\text{IL}$ is outlined in Table \ref{table:IL_state} and \ref{table:IL_action}.

%The trained distillation policy is directly deployed in real-world manipulation tasks in a zero-shot manner. By leveraging the diversity and quality of trajectories, along with the generalization capabilities of the diffusion model, the policy is able to handle complex manipulation tasks without requiring additional fine-tuning.

