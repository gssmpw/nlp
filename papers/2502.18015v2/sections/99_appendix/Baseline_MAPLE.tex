Baseline \texttt{MAPLE} is a hierarchical reinforcement learning method that includes the task policy \( \pi_\text{tsk} \) and the parameter policy \( \pi_\text{parameter} \). The algorithm is implemented based on Soft Actor-Critic~\cite{haarnoja2018soft}, consistent with the original implementation. However, we modify the \texttt{MAPLE} implementation to incorporate a parallelized environment, Isaac Gym, and to adapt \texttt{MAPLE} to our PNP problem. First, we remove the affordance reward, which in \texttt{MAPLE} guides the high-level policy toward the desired manipulation region. Instead, we integrate the connectors and skills, incorporating an applicability checker \(\applicabilitychecker\). The connector, which moves the robot to a state where the corresponding skill is applicable, is executed only when the predicted desired object pose is applicable (i.e., when the corresponding \(\applicabilitychecker\) holds true); otherwise, it is not executed. This replaces the need for an affordance reward. Furthermore, we replace the explicit initial end-effector position parameter, \( x_{\text{reach}} \), which serves as an input to skills in \texttt{MAPLE}, with the output of the pre-contact policy \( \pi_\text{pre} \) of each skill. Therefore, the initial robot position is computed by \( \pi_\text{pre} \), and the connector policy moves the robot to this computed position before skill execution. Additionally, we eliminate atomic primitives, low-level actions used to fill in gaps that cannot be fulfilled by skills, since our connectors are already trained to handle these gaps.

To train the policies, we outline: (1) the initial problem setup, (2) the state space \( S \), the action space \( A \), and the reward function \( R \) and, (3) hyperparameters used for training. 

The problem for training the policies, \( \pi_\text{tsk} \) and \( \pi_\text{parameter} \), consists of: (1) the initial object pose \( q^\text{init}_\text{obj} \), and (2) the target object pose \( q^\text{g}_\text{obj} \). We randomly sample the initial and target object poses, the same as in Skill-RRT in Appendix~\ref{Appendix:Problem}.

\begin{itemize}
    \medskip
    \item \textbf{State (\( S \)):} The components of the state space for the \texttt{MAPLE} policies are identical to those of the distillation policy. However, the \texttt{MAPLE} policies utilize the robotâ€™s joint velocity $\dot{q}^\text{(t)}_r$ instead of the previous joint position $q^\text{(t-1)}_r$ and omits both the gripper action executability $\mathbbm{1}_\text{gripper-execute}$ and previous timestep's robot gripper width action $a^\text{(t-1)}_\text{width}$.
    % The state represents the current configuration of the robot and the environment. It includes:
    % \input{tables/appendix/Baseline_MAPLE_state}

    \medskip
    \item \textbf{Action (\( A \)):} The action \( (K, \hat{q}^{\text{obj}}_{\text{sg}}) \in A \) represents skills $K$ and its corresponding normalized desired object pose $\hat{q}^{\text{obj}}_{\text{sg}}$ for the control module \( \pi_{\text{skill}}(\cdot;q^{\text{obj}}_{\text{sg}})\). The normalized desired object pose is unnormalized to align with the region of the corresponding skill.
    
    % \input{tables/appendix/Baseline_MAPLE_action}
    \medskip
    
    \item \textbf{Reward (\( R \)):} The reward function \( R(s, K, \hat{q}^{\text{obj}}_{\text{sg}}, s') \) is designed as a sparse reward to encourage the robot to manipulate the object to the goal object pose \( q^{\text{obj}}_\text{g} \), as defining a dense reward for this task is challenging. The reward consists of :

    \begin{enumerate}

        \item \textbf{Feasible Skill Reward:} Encourages the policy to choose feasible skills.
        \[
        \begin{aligned}
        r^\text{(t)}_{\text{feasible}} = {\epsilon_0^{\text{feasible}}} \cdot \mathbbm{1}[K.\phi (s, q^{\text{obj}}_{\text{sg}}) ]
        \end{aligned}
        \]
        
        \item \textbf{Success Reward:} The reward $r^\text{(t)}_\text{success}$ computation is identical to the success reward used in non-prehensile post-contact policy training. % Provides a success reward, $r^{\text{succ}}$, when the object is successfully manipulated to the target goal pose $q^{\text{obj}}_\text{g}$.
    \end{enumerate}

    The overall reward is defined as:
    \[
    r^\text{(t)}_{\texttt{MAPLE}} =  r^\text{(t)}_{\text{feasible}} + r^\text{(t)}_{\text{success}}
    \]
    The hyperparameters of the reward function, $\epsilon_0^{\text{feasible}} = 0.1$, $\delta_\text{obj} = 0.005$ and $r_{\text{succ}} = 100$, are the same for all tasks.
\end{itemize}


Hyperparameters related to training \texttt{MAPLE} are summarized in Table ~\ref{tab:baseline_maple_hyperparams}. These parameters remain consistent across all the tasks. 
\input{tables/appendix/Baseline_MAPLE_hyperparameter}