To train the connector policy \( \pi_C\), we introduce (1) the initial problem setup and (2) the state space $S$, the action space $A$, and the reward function $R$ used for training. The problem setup for connector policy is provided in Section~\ref{method:train_connector}.
% \Romannum{3}.C.
%~\ref{method:train_connector}.

For training connector policy $\pi_C$, the state space, the action space, and the reward function are defined as follows. 

\begin{itemize}
    \item \textbf{State (\( S \)):} % The state space of connector policy $\pi_C$ consist of variables listed in Table~\ref{table:Connector_state}.
    \input{tables/appendix/connector_state}
    The state for the skill's connector policy, \( K.\pi_C \), consists of ten components outlined in Table~\ref{table:Connector_state}. 
    % The state for the Connector Skill Policy \( \pi_C \) consists of nine components: robot joint position \( q^\text{(t)}_r \), robot joint velocity \( \dot{q}^\text{(t)}_r \), robot tool pose \( T^\text{(t)}_\text{tool} \), robot end-effector keypoint positions \( p^\text{(t)}_\text{ee} \), robot gripper tip positions \( p^\text{(t)}_\text{tip} \), object keypoint positions \( p^\text{(t)}_\text{obj} \), previous action \( a^\text{(t-1)} \), target end-effector keypoint positions \( p^\text{g}_\text{ee} \), and target gripper tip positions \( p^\text{g}_\text{tip} \).
    % The robot information includes the joint position \( q^\text{(t)}_r \), joint velocity \( \dot{q}^\text{(t)}_r \), tool pose \( T^\text{(t)}_\text{tool} \), end-effector keypoint positions \( p^\text{(t)}_\text{ee} \), and gripper tip positions \( p^\text{(t)}_\text{tip} \), which together provide a representation of the robot's state.
    % The object information is provided by the object keypoint positions \( p^\text{(t)}_\text{obj} \). The previous timestep action \( a^\text{(t-1)} \) is included to provide sufficient statistics.
    \(\mathbbm{1}^\text{(t)}_\text{gripper-execute}\), provided by the simulator, represents a binary indicator of the executability of the gripper action, where a value of 1 indicates that the action is executable. The gripper action is executable every 1.2 seconds because the Franka Research 3 gripper cannot accept new commands until the previous gripper command is completed. The goal is represented by the target end-effector keypoint positions \( p^\text{g}_\text{ee} \) and the target gripper tip positions \( p^\text{g}_\text{tip} \), which define the desired configurations for the robot to achieve. The remaining state components are identical to the state components of the prehensile skill post-contact policy.

    \medskip

    \item \textbf{Action (\( A \)):} The action \( a \in A \) represents the control inputs applied to the robot. The action consists of four components as show in Table \ref{table:Connector_action}. The delta end-effector pose, the proportional gain for robot joints, and joint damping correspond to the action components of the NP skill's post-contact policy. The fourth component, $q_\text{width}$, is the target gripper tip width. For the robot joints (excluding the gripper tip), the control process follows the same procedure as the post-contact policy control process used in the non-prehensile skill. For the gripper tip, the gripper width is adjusted to \( q_\text{width} \).
    \input{tables/appendix/connector_action}

    \medskip
    \item \textbf{Reward (\( R(s_t, a_t, s_{t+1}) \)):} 
    \begin{enumerate}
        \item \textbf{End-Effector Distance Reward:} Encourages the end-effector to move closer to its target position. Here, $p^\text{(t)}_\text{ee}$ represents the end-effector keypoint positions at timestep $t$, and $p^\text{g}_\text{ee}$ denotes the target end-effector keypoint positions:
        \[
        \begin{aligned}
        r^\text{(t)}_{\text{ee}} = \epsilon^{\text{ee}}_0 \big( &\exp(-\epsilon^{\text{ee}}_1 \| p^\text{(t)}_\text{ee} - p^\text{g}_\text{ee} \|_2) - \exp(-\epsilon^{\text{ee}}_1 \| p^\text{(t-1)}_\text{ee} - p^\text{g}_\text{ee} \|_2) \big)
        \end{aligned}
        \]
    
        \item \textbf{Gripper Tip Position Reward:} Aligns the gripper tips with their target positions. Here, $p^\text{(t)}_\text{ee}$ represents the gripper tip positions at timestep $t$, and $p^\text{g}_\text{tip}$ denotes the target tip positions:
        \[
        \begin{aligned}
        r^\text{(t)}_{\text{tip}} = \epsilon^{\text{tip}}_0 \big( &\exp(-\epsilon^{\text{tip}}_1 \| p^\text{(t)}_\text{ee} - p^\text{g}_\text{tip} \|_2)
        - \exp(-\epsilon^{\text{tip}}_1 \| p^\text{(t-1)}_\text{ee} - p^\text{g}_\text{tip} \|_2) \big)
        \end{aligned}
        \]
    
        \item \textbf{Object Movement Penalty:} Penalizes unnecessary object movement to ensure the preconditions of subsequent skills remain valid. Here, $p^\text{(t)}_\text{obj}$ represents the object keypoint positions at timesteps $t$:
        \[
        r^\text{(t)}_{\text{obj-move}} = -w^{\text{move}} \| p^\text{(t)}_\text{obj} - p^\text{(t-1)}_\text{obj} \|_2
        \]
    
        \item \textbf{Success Reward:} A success reward, \( r_{\text{succ}} \), is given when both the end-effector and gripper tip reach their respective target positions, \( p^\text{g}_\text{ee} \) and \( p^\text{g}_\text{tip} \), within the allowable error thresholds \( \delta_\text{ee} \) and \( \delta_\text{tip} \).
    \[
    r^\text{(t)}_{\text{success}} =
    \begin{cases} 
    r_{\text{succ}} & \text{if } ||p^\text{(t)}_\text{ee} - p^\text{g}_\text{ee}||_2 < \delta_\text{ee}, \text{ and }  ||p^\text{(t)}_\text{tip} - p^\text{g}_\text{tip}||_2 < \delta_\text{tip}, \\
    0 & \text{otherwise}
    \end{cases}
    \]
    \end{enumerate}

    The overall reward is defined as:
    \[
    r^\text{(t)}_{\text{connector}} = r^\text{(t)}_{\text{ee}} + r^\text{(t)}_{\text{tip}} + r^\text{(t)}_{\text{obj-move}} + r^\text{(t)}_{\text{success}}
    \]
    The hyperparameters of the reward function, $\epsilon_0$, $\epsilon_1$, $\omega$, $\epsilon_\text{vel}$, $\delta_\text{ee}$, and $\delta_\text{tip}$, vary depending on the specific connector being trained. The values of these reward hyperparameters for each connector are provided in Table~\ref{table:connector_reward}.
    
\end{itemize}

\input{tables/appendix/Connector_reward}

Connector policies utilize a multilayer perceptron (MLP) architecture to generate low-level robot actions based on the state and target robot configuration. Each connector policy \( \pi_C \) employs a five-layer MLP with an input dimension of 131 and an output dimension of 21. Other components of the network architecture are identical to those of the NP skill post-contact policy's network architecture.
% The MLP has hidden dimensions of $[512, 256, 256, 128]$ and an output dimension of 21. ELU is used as the activation function for the hidden layers, while Identity is applied as the final activation function.
\input{tables/appendix/connector_network_architecture}