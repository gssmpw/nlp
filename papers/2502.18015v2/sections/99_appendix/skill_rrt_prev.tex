This section provides the algorithms used in the \texttt{Skill-RRT}.

% sampling Rule
\textbf{SampleSkillAndSubgoal} Our sampling process in \texttt{Skill-RRT} outputs both a skill $o_\alpha$ and a subgoal object pose $q^{\text{obj}}_{sg}$ given the skill library \( \mathcal{O} \) and the goal object pose sampling probability \( p_g \), as shown in Algorithm \ref{algo:SampleSkillAndPose}.

First, a skill is uniformly sampled from the skill library \( \mathcal{O} \). For the subgoal, we initially sample a subogal region \( \mu_{\beta} \) from the set of subgoal object pose regions \( \mathcal{M} \), and then the subgoal object pose \( q^{\text{obj}}_{\text{sg}} \) is sampled from the region \( \mu_{\beta} \). With a certain goal sampling probability \( p_g \), the goal pose \( q^{\text{obj}}_{\text{goal}} \) is selected as the subgoal.
\input{algos/skill-RRT/sampleskillandsubgoal}

% find the nearest node rule
\textbf{CheckPreAndNearestNode} The process of finding the nearest node in \texttt{Skill-RRT} outputs the nearest node \( v_{\text{near}} \) in the tree \( T \), given the tree \( T \), the sampled skill \( o_\alpha \), the sampled subgoal object pose \( q^{\text{obj}}_{\text{sg}} \), and the metric function \( d \) as shown in Algorithm \ref{algo:NearestNode}.

For NP skills \( o_{{\text{NP}_i}} \), we check if the precondition at level 0, \( \mathrm{pre}^{0}_{\text{NP}_i}(v.q^{\text{obj}}, v.q^{\text{robot}}, q^{\text{obj}}_{\text{sg}}) \), is satisfied for each node \( v \) in the tree. If met, the distance \( d(v.q^{\text{obj}}, q^{\text{obj}}_{\text{sg}}) \) is calculated, and the nearest valid node is selected. For the P skill, we first find the nearest node based on the metric \( d \) and then verify the precondition \( \mathrm{pre}^{0}_{\text{Place}}(v.q^{\text{obj}}, v.q^{\text{robot}}, q^{\text{obj}}_{\text{sg}}) \). If the precondition is not met, we check up to 5 nearest nodes due to computational cost, such as inverse kinematics (IK) feasibility and collision checks.

\input{algos/skill-RRT/nearest}

\input{algos/skill-RRT/extend_abstract}

\textbf{Extend} The Extend process in \texttt{Skill-RRT} outputs an updated tree \( T \), given the tree \( T \), the nearest node \( v_{\text{near}} \), the skill \( o_\alpha \), the connector \( c_\alpha \), the sampled subgoal object pose \( q^{\text{obj}}_{\text{sg}} \), and the pre-contact configuration sampler \( f^\text{pre}_\alpha \), as shown in Algorithm \ref{algo:Extend}.

The algorithm extends the tree towards a sampled subgoal by simulating the skill \( o_\alpha \). First, the pre-contact sampler \( f^{\text{pre}}_\alpha \) computes a robot configuration \( q^{\text{robot}}_{\text{pre}} \), which serves as the subgoal \( q^{\text{robot}}_{\text{sg}} \) for the connector policy \( \pi^c_\alpha \). The connector tries to move the robot end-effector and its gripper tip position to \( q^{\text{robot}}_{\text{pre}} \). If the connector succeeds (i.e., the precondition \( \mathrm{pre}^1_\alpha \) is satisfied), the skill \( o_\alpha \) is executed. After the skill is executed, the object pose \( q^{\text{obj}} \) is checked to see if it reaches the subgoal \( q^{\text{obj}}_{\text{sg}} \). If successful, a new node \( v_{\text{new}} \) is created, consisting of the resulting simulation state \( q^{\text{obj}} \) and \( q^{\text{robot}} \). This new node is connected to \( v_{\text{near}} \) with an edge \( e_{\text{new}} \), which includes the connector \( c_\alpha \), the pre-contact configuration \( q^{\text{robot}}_{\text{pre}} \), skill \( o_\alpha \), and subgoal \( q^{\text{obj}}_{\text{sg}} \). The updated tree is then returned. 

During the simulations in this procedure, domain randomization is applied for a sim-to-real transfer. This randomization introduces variations in environmental parameters (e.g., friction, object mass, etc.) and adds noise to the state configurations (as shown in Table \ref{table:NP_state}, \ref{table:P_state}, and \ref{table:Connector_state}), which serve as inputs to \( \pi^c_\alpha \) and \( \pi_\alpha \). Additionally, noise is introduced to the controllerâ€™s commanded torques, simulating real-world robot motors uncertainties.

\input{algos/skill-RRT/extend}
