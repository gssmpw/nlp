In this section, we study how the design choices of imitation learning (IL) affect the performance of the distillation policy in simulation. We compare the diffusion policy \cite{chi2023diffusion} with the following architectures: 1) ResNet \cite{he2016deep}, a simple IL model with large parameters; 2) LSTM+GMM, which has been shown to be effective for multimodal data in RoboMimic \cite{robomimic2021}; 3) cVAE \cite{kingma2013auto}, another conditional generative model; and 4) Transformer \cite{vaswani2017attention}, a widely used architecture for multimodal data such as language. Each architecture is trained on the same dataset, with the similar model parameter size ($\approx$70M) and the same training duration of 100 epochs, across three different seeds. 

For training, the codebase for imitation learning is adapted from RoboMimic~\cite{robomimic2021}. We revise the MLP architecture to ResNet for a fair comparison with other large-sized models. We do not modify the original code of LSTM+GMM, cVAE, or Transformer in RoboMimic, other than adjusting hyperparameters such as model size. The hyperparameters for each architecture are identical across the three domains and are summarized in Table~\ref{table:IL_ablation_hyper}. Each architecture is trained with the same number of training epochs (100).

\input{tables/appendix/IL_ablation_hyper}
\input{tables/ablation/ablation_IL_arch}

To measure the success rate, we test each architecture on 100 test problems, consisting of the initial object configuration $s_0.q_{\text{obj}}$, the initial robot configuration $s_0.q_{\text{r}}$, and the goal object configuration $q^g_{\text{obj}}$. The success rate is measured at every training epoch, and the performance of each architecture is assessed based on the maximum success rate achieved during training across three seeds. The success rates for each architecture are summarized in Table \ref{table:IL_arch_ablation}. 

In the card flip domain, the diffusion policy significantly outperforms the other architectures. We hypothesize that this is because of the high diversity of intermediate target poses in this domain—for instance, an object pose can be at either the left or right edge of the table. In the bookshelf and kitchen domains, ResNet, LSTM+GMM, and Transformer achieve success rates similar to the diffusion policy, which might because the level of intermediate object poses diversity is lower than in the card flip domain. cVAE shows poor performance across all domains, indicating that the smoothing effect of VAE negatively impacts imitation learning.

% % previous ablation appendix
% In this section, we provide the training settings for the imitation learning design ablation, including ResNet, LSTM+GMM, cVAE, and Transformer. The codebase for imitation learning is adapted from RoboMimic \cite{robomimic2021}. We revise the MLP architecture to ResNet for a fair comparison with other large-sized models. We do not modify the original code of LSTM+GMM, cVAE, or Transformer in RoboMimic, other than adjusting hyperparameters such as model size. The hyperparameters for each architecture are identical across the three domains and are summarized in Table \ref{table:IL_ablation_hyper}. Each architecture has a similar model size ($\approx$70M) with the same number of training epochs (100).

% To measure the success rate, we test each architecture on 100 test problems, consisting of the initial object configuration $s_0.q_{\text{obj}}$, the initial robot configuration $s_0.q_{\text{r}}$, and the goal object configuration $q^g_{\text{obj}}$. The success rate is measured at every training epoch, and the performance of each architecture is assessed based on the maximum success rate achieved during training across three seeds.


% % at previous ablation main paper.
% In this section, we study how the design choices of imitation learning (IL) affect the performance of the distillation policy in simulation. We compare the diffusion policy \cite{chi2023diffusion} with the following architectures: 1) ResNet \cite{he2016deep}, a simple IL model with large parameters; 2) LSTM+GMM, which has been shown to be effective for multimodal data in RoboMimic \cite{robomimic2021}; 3) cVAE \cite{kingma2013auto}, another conditional generative model; and 4) Transformer \cite{vaswani2017attention}, a widely used architecture for multimodal data such as language. Each architecture is trained on the same dataset, with the similar model parameter size ($\approx$70M) and the same training duration of 100 epochs, across three different seeds. % Detailed training settings such as hyperparameters are described in Appendix~\ref{Appendix:IL_ablation}. 
% The success rate is measured at every training epoch on 100 fixed test problems and we report the maximum success rates achieved during the training epochs. The success rates for each architecture are summarized in Table \ref{table:IL_arch_ablation}. 

% In the card flip domain, the diffusion policy significantly outperforms the other architectures. We hypothesize that this is because of the high diversity of intermediate target poses in this domain—for instance, an object pose can be at either the left or right edge of the table. In the bookshelf and kitchen domains, ResNet, LSTM+GMM, and Transformer achieve success rates similar to the diffusion policy, which might because the level of intermediate object poses diversity is lower than in the card flip domain. cVAE shows poor performance across all domains, indicating that the smoothing effect of VAE negatively impacts imitation learning.

% \input{tables/ablation/ablation_IL_arch}