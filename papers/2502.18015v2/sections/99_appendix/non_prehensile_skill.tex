
\medskip
This section outlines the training details for the non-prehensile (NP) skills, as described in Table~\ref{tab:skill_defn}. Each skill's policy consists of \( \pi_\text{pre} \) and \( \pi_\text{post} \), as mentioned in Section~\ref{method:PF}. \( \pi_\text{post} \) computes low-level robot actions that manipulate the object toward the desired object pose, using the state and desired object pose as inputs. \( \pi_\text{pre} \) computes the robot configuration needed to execute \( \pi_\text{post} \), with the current and desired object poses as inputs.

First we describe how we train \( \pi_\text{post} \) and then explain about \( \pi_\text{pre} \). \( \pi_\text{post} \) is trained using the Proximal Policy Optimization (PPO) algorithm, which demonstrates successful non-prehensile manipulation in \citet{kim2023pre}. To train the policy \( \pi_\text{post} \), we introduce (1) the initial problem setup and (2) the state space $S$, the action space $A$, and the reward function $R$ used for training. The problem for training post-contact policy \( \pi_\text{post} \), consist of (1) initial object pose $q^\text{init}_\text{obj}$, (2) initial robot configuration $q^\text{init}_r$, and (3) desired object pose $q^\text{g}_\text{obj}$. Initial and desired object poses are sampled from each region $K_\text{NP}.R$. The initial robot configuration is computed from the pre-contact policy with inputs consist of initial and desired object poses $q^\text{init}_r = K.\pi_\text{pre}(q^\text{init}_\text{obj}, q^\text{g}_\text{obj})$.

For training NP skill's post-contact policy $K_\text{NP}.\pi_\text{post}$, the state space, the action space, and the reward function are defined as follows. 

\begin{itemize}
    \medskip
    \item \textbf{State (\( S \)):}    \input{tables/appendix/NP_state}
    The state for the NP skill's post-contact policy, \( K_\text{NP}.\pi_\text{post} \), consists of six components outlined in Table~\ref{table:NP_state}.
    % The state consists of six variables: robot joint position $q^\text{(t)}_r$, robot joint velocity $\dot{q}^\text{(t)}_r$, robot tool pose $T^\text{(t)}_\text{tool}$, object keypoint positions $p^\text{(t)}_\text{obj}$, previous timestep action $a^\text{(t-1)}$, and desired object keypoint positions $p^\text{g}_\text{obj}$.
    The robot information includes the robot joint position $q^\text{(t)}_r$, joint velocity $\dot{q}^\text{(t)}_r$ , and tool pose $T^\text{(t)}_\text{tool}$ (The tool pose is computed from the robot joint positions $q^\text{(t)}_r$). The previous timestep action $a$ provides sufficient statistics \cite{powell2012ai}. 
    
    The object information is represented by the object keypoint positions $p^\text{(t)}_\text{obj}$ (computed from object pose $q^\text{(t)}_\text{obj}$), which represents the object's geometry. From the pre-defined relative object keypoints and the object pose $q^\text{(t)}_\text{obj}$, the object keypoint positions $p^\text{(t)}_\text{obj}$ are computed. We pre-define eight relative object keypoints (\(x, y, z\) positions) on each object's surface. For cuboid-shaped objects like a card or a book, the eight keypoints correspond to the vertices of the cuboid. For a cup, six keypoints are located on the body, and two keypoints are positioned on the handle.
    %the eight keypoints are pre-defined along the surface of the cup. 
    These keypoint positions are used to calculate the distances between two different object poses, both in the reward function and as state components of the skill policy. The desired object pose information is represented by desired object keypoint positions $p^\text{g}_\text{obj}$, which are also computed from the relative object keypoints and desired object pose $q^\text{g}_\text{obj}$.

    \medskip
    \item \textbf{Action (\( A \)):} The action \( a \in A \) represents the control inputs applied to the robot.
    \input{tables/appendix/NP_action}
    
    The action consists of three components. The first component, \( \Delta q_\text{ee} \), represents the delta end-effector pose, with a dimension of 6. It indicates the change in the end-effector pose, defined by the position \( (x, y, z) \) and orientation angles \( (\theta_{x}, \theta_{y}, \theta_{z}) \). The second component, \( k_p \), is the proportional gain for robot joints, which has a dimension of 7. It refers to the gain values for the robot's joints, excluding the two gripper joints. The third component, \( \rho \), represents the joint damping, and it also has a dimension of 7. It specifies the damping values for the robot's joints, excluding the gripper joints.

    For the robot joints (except gripper tip), the control process begins by computing the target end-effector pose using the end-effector pose $q^\text{(t)}_\text{ee}$ and the change of end-effector pose \( \Delta q_\text{ee} \). This target pose $q^\text{(t)}_\text{ee} + \Delta q_\text{ee}$ is then used to solve the inverse kinematics (IK) problem, yielding the target joint positions \( q_r^\text{target} \). The derivative gain \( k_d \) is calculated from the proportional gain \( k_p \) and the damping ratio \( \rho \) using the relation \( k_d = \rho \cdot \sqrt{k_p} \). Finally, the PD controller computes the torque $\tau^\text{(t)}$ at timestep $t$ as \( \tau^\text{(t)} = k_p \cdot (q_r^\text{target} - q^\text{(t)}_r) - k_d \cdot \dot{q}^\text{(t)}_r \). The computed torque $\tau$ is applied to the joints at a frequency of 100 Hz.

    For the gripper tip, no additional control is applied. The gripper width, computed by \( \pi_\text{pre} \), is maintained.

    \medskip
    \item \textbf{Reward (\( R(s_t, a_t, s_{t+1}) \)):}
    \begin{enumerate}
        \item \textbf{Object Keypoint Distance Reward:} Encourages moving the object closer to its desired object pose. Here, $p_\text{obj}^\text{(t)}$ % $(s_t.q_\text{obj})^\text{keypoint}$ 
        represents the object keypoint positions at timestep $t$ computed from $q^\text{(t)}_\text{obj}$, $p^\text{g}_\text{obj}$ % $(q'_\text{obj})^\text{keypoint}$
        denotes the keypoint positions of the desired object pose computed from $q^\text{g}_\text{obj}$, and $\epsilon^{\text{obj}}_0$, $\epsilon^{\text{obj}}_1$ are reward hyperparameters:
        \[
        \begin{aligned}
        r^\text{(t)}_{\text{obj}} = {\epsilon_0^{\text{obj}}\over{\|p^\text{(t)}_\text{obj} - p^\text{g}_\text{obj}\| + \epsilon^{\text{obj}}_1}}
        - {\epsilon_0^{\text{obj}}\over{\|p^\text{(t-1)}_\text{obj} - p^\text{g}_\text{obj}\| + \epsilon^{\text{obj}}_1}}
        \end{aligned}
        \]
    
        \item \textbf{Tip Contact Reward:} Encourages maintaining contact between the gripper tips and the object. Here, $p^\text{(t)}_\text{tip}$ represents the robot gripper tip positions at timestep $t$, $q^\text{(t)}_\text{obj}.\text{pos}$ represents the object position at timestep $t$, and $\epsilon^{\text{tip-obj}}_0$, $\epsilon^{\text{tip-obj}}_1$ are reward hyperparameters:
        \[
        \begin{aligned}
        r^\text{(t)}_{\text{tip-contact}} = {\epsilon_0^{\text{tip-obj}}\over{\|p_\text{tip}^\text{(t)} - q^\text{(t)}_\text{obj}.\text{pos}\| + \epsilon^{\text{tip-obj}}_1}}
        - {\epsilon_0^{\text{tip-obj}}\over{\|p^\text{(t-1)}_\text{tip} - q^\text{(t-1)}_\text{obj}.\text{pos}\| + \epsilon^{\text{tip-obj}}_1}}
        \end{aligned}
        \]
    
    \item \textbf{Success Reward:} A success reward, \( r_{\text{succ}} \), is given when the object is successfully manipulated to the desired object pose \( q^\text{g}_\text{obj} \); otherwise, the reward is 0. The distance function between two object poses, \( q^1_\text{obj} \) and \( q^2_\text{obj} \), is defined as \( d_{SE(3)}(q^1_\text{obj}, q^2_\text{obj}) = \Delta T(q^1_\text{obj}, q^2_\text{obj}) + \alpha \Delta\theta(q^1_\text{obj}, q^2_\text{obj}) \), where \( \Delta T \) represents the positional difference, \( \Delta\theta \) denotes the orientational difference, and \( \alpha \) is a weighting factor for the orientation difference, set to \( 0.1 \). The object manipulation is considered successful if the distance between the current and desired object poses is less than \( \delta_\text{obj} \).

    \[
    r^\text{(t)}_{\text{success}} =
    \begin{cases} 
    r_{\text{succ}} & \text{if } d_{SE(3)}(q^\text{(t)}_{\text{obj}}, q^\text{g}_{\text{obj}}) < \delta_\text{obj}, \\
    0 & \text{otherwise}.
    \end{cases}
    \]

    \end{enumerate}

    The overall reward is defined as:
    \[
    r^\text{(t)}_{\text{NP}} = r^\text{(t)}_{\text{obj}} + r^\text{(t)}_{\text{tip-contact}} + r^\text{(t)}_{\text{success}}
    \]
    The hyperparameters of the reward function, $\epsilon_0^{\text{obj}}$, $\epsilon_1^{\text{obj}}$, $\epsilon_0^{\text{tip-obj}}$, $\epsilon_1^{\text{tip-obj}}$, $r_{\text{succ}}$, and $\delta_\text{obj}$, vary depending on the specific NP skill being trained. The values of these reward hyperparameters for each NP skill \( K_{\text{NP}_i} \) are provided in Table~\ref{table:NP_reward}.

\end{itemize}

\input{tables/appendix/NP_reward}
For training pre-contact policy $K_\text{NP}.\pi_\text{pre}$, the state space consist of two object poses, initial object pose $q_\text{obj}$, and desired object pose $ q^\text{g}_\text{obj}$. The pre-contact policy outputs $q'_r$, which is the robot joint positions for initiating the $K_\text{NP}$. The pre-contact policy is jointly trained with $\pi_\text{post}$ following the algorithm in \citet{kim2023pre}.

% \medskip
% \subsubsection{NP Skill Policy architecture}
NP skill policies utilize a multilayer perceptron (MLP) architecture to generate low-level robot actions based on the state and desired object pose information. The post-contact policy $\pi_\text{post}$ employs a five-layer MLP. The MLP has hidden dimensions of $[512, 256, 256, 128]$, input dimension 93, and an output dimension of 20. ELU is used as the activation function for the hidden layers, while Identity is applied as the final activation function. Pre-contact policy $\pi_\text{pre}$ employs a five-layer MLP. The MLP has hidden dimensions of $[512, 256, 256, 128]$, input dimension 14, and an output dimension of 9. ELU is used as the activation function for the hidden layers, while Identity is applied as the final activation function. We summarize the hyperparamters of architecture in Table \ref{table:NP_arch}

\input{tables/appendix/skill_network_architecture}