%Flat RL
% This section describes the baselines: PPO and \texttt{MAPLE}.

Baseline PPO is a flat reinforcement learning method whose training pipeline follows the same process as skill training. To train the policy \( \pi_\text{post} \), we detail: (1) the initial problem setup, (2) the state space \( S \), the action space \( A \), and the reward function \( R \), and (3) a policy architecture used for training.

The problem for training the post-contact policy, \(\pi_\text{post} \), consists of: (1) the initial object pose \( q^\text{init}_\text{obj} \), (2) the initial robot configuration \( q^\text{init}_r \), and (3) the target object pose \( q^\text{g}_\text{obj} \). We randomly sample the initial and desired object poses, the same as in Skill-RRT in Appendix~\ref{Appendix:Problem}.
%We randomly sample the initial and target object poses from \( \Qobj \). 
Subsequently, we compute \( q^\text{init}_r \) by \( \pi_\text{pre}(q^\text{init}_\text{obj},q^\text{g}_\text{obj}) \). If the computed \( q^\text{init}_r \) does not cause collisions, either between the robot and the environment or between the robot and the object, the problem is generated. Otherwise, it is excluded from the dataset.


\begin{itemize}
    \medskip
    \item \textbf{State (\( S \)):} The components of the state space for the PPO policy are identical to those of the distillation policy. However, the PPO policy uses the robotâ€™s joint velocity $\dot{q}^\text{(t)}_r$ instead of the previous joint position $q^\text{(t-1)}_r$ and excludes the gripper action executability $\mathbbm{1}_\text{gripper-execute}$. Additionally, it incorporates not only the previous gripper width action $ a^\text{(t-1)}_\text{width}$ but also all previous actions $a^\text{(t-1)}$ as components of the state space.
    % The state for the PPO policy consists of nine components outlined in Table~\ref{table:PPO_state}.
    % It includes:
    %\input{tables/appendix/Baseline_PPO_state}
    \medskip
    \item \textbf{Action (\( A \)):}
    The action components are identical to action components of connector policy. % The action \( a \in A \) represents the control inputs applied to the robot.
    % \input{tables/appendix/Baseline_PPO_action}

    \medskip
    \item \textbf{Reward (\( R \)):} The reward function \( R(s_t, a_t, s_{t+1}) \) is designed to encourage the robot to manipulate object to the goal object pose \( q_{\text{obj}}^\text{g} \). The reward consists of three main components:
    \begin{enumerate}

        \item \textbf{Object Keypoint Distance Reward:} The reward $r^\text{(t)}_\text{obj}$ computation is identical to the object keypoint distance reward used in non-prehensile skill post-contact policy training. % Encourages moving the object closer to its target object pose. Here, $p^\text{(t)}_\text{obj}$ represents the object's keypoint position at timestep $t$, and $p^\text{g}_\text{obj}$ denotes the keypoint position of the target object pose:
        % \[
        % \begin{aligned}
        % r^\text{(t)}_{\text{obj}} = {\epsilon_0^{\text{obj}}\over{\|p^\text{(t)}_\text{obj} - p^\text{g}_\text{obj}\| + \epsilon^{\text{obj}}_1}}
        % - {\epsilon_0^{\text{obj}}\over{\|p^\text{(t-1)}_\text{obj} - p^\text{g}_\text{obj}\| + \epsilon^{\text{obj}}_1}}
        % \end{aligned}
        % \]

        \item \textbf{Tip Contact Reward:} The reward $r^\text{(t)}_\text{tip-contact}$ computation is identical to the tip contact reward used in non-prehensile post-contact policy training. % Encourages maintaining contact between the gripper tips and the object. Here, $(u_\text{tip})_t$ represents the robot gripper tip positions at timestep $t$, $q^\text{(t)}_\text{obj}.\text{pos}$ represents the object position at timestep $t$, and $\epsilon^{\text{tip-obj}}_0$, $\epsilon^{\text{tip-obj}}_1$ are reward hyperparameters:
        % \[
        % \begin{aligned}
        % r^\text{(t)}_{\text{tip-contact}} = {\epsilon_0^{\text{tip-obj}}\over{\|p^\text{(t)}_\text{tip} - q^\text{(t)}_\text{obj}.\text{pos}\| + \epsilon^{\text{tip-obj}}_1}}
        % - {\epsilon_0^{\text{tip-obj}}\over{\|p^\text{(t-1)}_\text{obj} -     q^\text{(t-1)}_\text{obj}.\text{pos}\| + \epsilon^{\text{tip-obj}}_1}}
        % \end{aligned}
        % \]

        \item \textbf{Domain-Oriented Reward:} Encourages the successful completion of domain-specific objectives. The exact reward varies depending on the domain as shown in Table \ref{table:PPO_reward_condition}:

        \[
        \begin{aligned}
        r^\text{(t)}_{\text{domain}} = {\epsilon_0^{\text{domain}}} \cdot \mathbb{I}[\text{domain-specific conditions}]
        \end{aligned}
        \]

        \input{tables/appendix/Baseline_PPO_reward_condition}

        % \begin{itemize}
        %     \item 
        %     For the Card Flip domain, $r^{\text{domain}} = {\epsilon_0^{\text{domain}}} \cdot \mathbb{I}[\text{object is flipped}]$
        %     \item
        %     For the Bookshelf domain, $r^{\text{domain}} = {\epsilon_0^{\text{domain}}} \cdot \mathbb{I}[\text{object is placed on the box}]$
        %     \item
        %     For the Kitchen domain, $r^{\text{domain}} = {\epsilon_0^{\text{domain}}} \cdot \mathbb{I}[\text{object is placed on the shelf}]$

        % \end{itemize}

    
        \item \textbf{Success Reward:} The reward $r^\text{(t)}_\text{success}$ computation is identical to the success reward used in non-prehensile post-contact policy training.% Provides a success reward, $r^{\text{succ}}$, when the object is successfully manipulated to the target goal pose $q^{\text{obj}}_\text{g}$.
    \end{enumerate}

    The overall reward is defined as:
    \[
    r^\text{(t)}_{\text{PPO}} = r^\text{(t)}_{\text{obj}} + r^\text{(t)}_{\text{tip-obj}} + r^\text{(t)}_{\text{domain}} + r^\text{(t)}_{\text{success}}
    \]
    The hyperparameters of the reward function, $\epsilon_0^{\text{obj}}$, $\epsilon_1^{\text{obj}}$, $\epsilon_0^{\text{tip-obj}}$, $\epsilon_1^{\text{tip-obj}}$, $\epsilon_0^{\text{domain}}$ and $r_{\text{succ}}$, are provided in Table~\ref{table:baseline_ppo_reward}.
    \input{tables/appendix/Baseline_PPO_reward}
\end{itemize}

% \subsubsection{Baseline PPO Policy architecture}
The baseline PPO policies utilize a multilayer perceptron (MLP) architecture to generate low-level robot actions based on the state and goal object pose information. Each policy, $\pi_{\text{pre}}$ and $\pi_{\text{post}}$, employs a five-layer MLP. $\pi_{\text{pre}}$ and $\pi_{\text{post}}$ have input dimensions of 14 and 147, respectively. $\pi_{\text{pre}}$ and $\pi_{\text{post}}$ have output dimensions of 9 and 21, respectively. The other components of the architecture are identical to those of the non-prehensile skill post-contact policy's network architecture.
\input{tables/appendix/Baseline_PPO_network_architecture}

