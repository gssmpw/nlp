This section outlines the definition and training details for the prehensile (P) skill, $K_\text{P}$. The prehensile skill consists of a goal-conditioned policy $ \pi: S \times \Qobj \rightarrow A $ and an applicability checker $ \applicabilitychecker:S \times \Qobj \rightarrow {0,1} $, similar to the NP skill. The goal-conditioned policy includes two components: a pre-contact policy $ \pi_{\text{pre}} $ and a post-contact policy $ \pi_{\text{post}} $, where $ \pi_{\text{pre}} $ is a heuristic function, while $ \pi_{\text{post}} $ is a learned policy trained using RL.

$ \pi_{\text{pre}} $ takes the initial object pose and the desired object pose and outputs the robot configuration prior to executing $ \pi_{\text{post}} $, where the robot configuration corresponds to the grasp configuration of the initial object pose. For each object, there are predefined grasps, including relative grasp poses in the object frame and grasp width.

We describe how we create predefined grasps for the card, book, and cup. For cuboid objects (card and book), the poses of predefined grasps are calculated as follows: (1) form multiple contact points on a rectangle positioned slightly away from the edge on the objectâ€™s broad surface (2) set grasp directions from the contact points toward the object's center of mass (3) calculate the grasp pose based on the contact points and grasp direction. The grasp width is set to the thickness of the cuboid object. For a cup, we follow the grasp generation method from ACRONYM \cite{eppner2021acronym} due to the unstructured shape of the cup. The grasp generation procedure follows: (1) sample a random grasp pose (2) check for collisions between the cup and the object (3) close and shake the gripper (4) if the object is still grasped, save the grasp pose and width.

Once the grasps are generated, $ \pi_{\text{pre}} $ operates as follows. It first converts all predefined grasp poses from the object frame to the world frame for both the initial and desired object poses. Then, it performs inverse kinematics (IK) on the set of grasp poses, and if an IK solution exists, $ \pi_{\text{pre}} $ returns the IK solution randomly sampled from the set of feasible grasp poses. Otherwise, it returns null values.

The applicability checker $ \phi(s, q_{\text{obj}}) $ returns true if the robot joint position from $ \pi_{\text{pre}} $ is not null and there is no collision between the environment and the robot joint position. Otherwise, it returns false.

% Each domain has a single P skill, which manipulates its respective object (card, book, or cup) while maintaining a grasp. The prehensile skill begins by grasping the object at the relative grasp pose with respect to object and then places it in the desired object pose. The robot grasps the object based on the relative grasp poses for each object type, as described in Appendix~\ref{Appendix:Object}. 

% For each domain, the prehensile skill $K_\text{P}$ of each domain defined as below.
% \begin{itemize}
%     \item Card Flip Domain: A P skill that moves a card within \( \Qobj \) while maintaining a grasp on it. 
%     \item Bookshelf Domain: A P skill that moves a book from the upper bookshelf to the lower bookshelf while maintaining a grasp on it. 
%     \item Kitchen Domain: A P skill that moves a cup from the sink and places it into a cupboard while maintaining a grasp on it.
% \end{itemize}

% As mentioned in Section~\ref{method:PF}, each skill has two functions: an applicability condition \( \phi \) and a goal-conditioned policy \( \pi \). The P skill follows this definition as well.

% of P skill, we first compute the grasp poses in the world frame. We derive two sets of grasp poses: the first set is based on the current object pose and the relative grasp poses, while the second set is based on the desired object pose and the relative grasp poses. Next, we solve the inverse kinematics (IK) for each set of grasp poses using the IK-fast solver~\cite{diankov_thesis}. For each set, we generate a set of feasible grasp poses that are both IK-feasible and collision-free within each domain. If both sets contain feasible grasp poses computed from the same relative grasp pose, the applicability condition is satisfied. If either condition is not met, the applicability condition is not satisfied.

% For the post-contact policy of the P skill, \( \pi_\text{post} \), which is consist of $\pi_\text{pre}$ which computes low-level robot actions based on the state and desired object pose to place the object. 
The post-contact policy $\pi_\text{post}$ are trained using RL for each domain. To train \( \pi_\text{post} \), we introduce: (1) the initial problem setup, and (2) the state space \( S \), the action space \( A \), and the reward function \( R \) used for training. The problem consists of: (1) the initial object pose \( q^\text{init}_\text{obj} \), (2) the initial robot configuration \( q^\text{init}_r \), and (3) the desired object pose \( q^\text{g}_\text{obj} \). We randomly sample the initial and desired object poses from \( \Qobj \). Subsequently, we compute \( q^\text{init}_r \) by \( \pi_\text{pre}(q^\text{init}_\text{obj},q^\text{g}_\text{obj}) \). If the computed \( q^\text{init}_r \) does not cause collisions, either between the robot and the environment or between the robot and the object, the problem is generated. Otherwise, it is excluded.

For training the post-contact policy $\pi_\text{post}$, the state space, the action space, and the reward function are defined as follows. 

\begin{itemize}
    \medskip
    \item \textbf{State (\( S \)):}
    \input{tables/appendix/P_state}
    
    The state space consists of eight components outlined in Table~\ref{table:P_state}. %: robot joint position \( q^\text{(t)}_r \), robot joint velocity \( \dot{q}^\text{(t)}_r \), robot tool pose \( T^\text{(t)}_\text{tool} \), robot end-effector keypoint positions \( p^\text{(t)}_\text{ee} \), robot gripper tip positions \( p^\text{(t)}_\text{tip} \), object keypoint positions \( p^\text{(t)}_\text{obj} \), previous action \( a^\text{(t-1)} \),  and desired object keypoint positions $p^\text{g}_\text{obj}$.
     The end-effector pose is computed from the robot joint positions \( q^\text{(t)}_r \). Eight keypoints are defined at the vertices of the end-effector. Using these keypoints and the end-effector pose, the end-effector keypoint positions \( p^\text{(t)}_\text{ee} \) are computed. Similarly, the gripper tip positions \( p^\text{(t)}_\text{tip} \) are computed from the robot joint positions \( q^\text{(t)}_r \). The remaining state components are identical to the non-prehensile skill's post-contact policy's state components.
    
    % The robot information includes the joint position \( q^\text{(t)}_r \), joint velocity \( \dot{q}^\text{(t)}_r \), tool pose \( T^\text{(t)}_\text{tool} \), end-effector keypoint positions \( p^\text{(t)}_\text{ee} \) (computed from $q^\text{(t)}_r$), and gripper tip positions \( p^\text{(t)}_\text{tip} \) (computed from $q^\text{(t)}_r$), which together provide a representation of the robot's state.
    % The object information is provided by the object keypoint positions \( p^\text{(t)}_\text{obj} \). The previous timestep action \( a^\text{(t-1)} \) is included to provide sufficient statistics. The desired object pose information is represented by desired object keypoint positions $p^\text{g}_\text{obj}$, which are computed from the relative object keypoints and desired object pose $q^\text{g}_\text{obj}$.

    \medskip
    
    \item \textbf{Action (\( A \)):} The action space of the prehensile skill is identical to that of the NP skill. For the gripper tip, control is applied with a width of 0 to maintain the grasp on the object.

    \medskip
    
    \item \textbf{Reward (\( R(s_t, a_t, s_{t+1}) \)):}
    \begin{enumerate}
        \item \textbf{Object Keypoint Distance Reward:} % Encourages moving the object closer to its desired object pose. Here, $p^\text{(t)}_\text{obj}$ represents the object's keypoint position at timestep $t$, and $p^\text{g}_\text{obj}$ denotes the keypoint position of the desired object pose:
        % \[
        % \begin{aligned}
        % r^\text{(t)}_{\text{obj}} = {\epsilon_0^{\text{obj}}\over{\|p^\text{(t)}_\text{obj} - p^\text{g}_\text{obj}\| + \epsilon^{\text{obj}}_1}}
        % - {\epsilon_0^{\text{obj}}\over{\|p^\text{(t-1)}_\text{obj} - p^\text{g}_\text{obj}\| + \epsilon^{\text{obj}}_1}}
        % \end{aligned}
        % \]
        The reward $r^\text{(t)}_\text{obj}$ computation is identical to the object keypoint distance reward used in non-prehensile post-contact policy training.
    
        \item \textbf{Object Rotation Reward:} Encourages aligning the object orientation with its desired object orientation. Here, $q^\text{(t)}_\text{obj}.\theta$ represents the object pose orientation at timestep $t$, and $q^\text{g}_\text{obj}.\theta$ denotes the desired object pose orientation:
        \[
        \begin{aligned}
        r^\text{(t)}_{\text{rot}} = {\epsilon_0^{\text{rot}}\over{\|q^\text{(t)}_\text{obj}.\theta - q^\text{g}_\text{obj}.\theta\| + \epsilon^{\text{rot}}_1}}
        - {\epsilon_0^{\text{rot}}\over{\|q^\text{(t-1)}_\text{obj}.\theta - q^\text{g}_\text{obj}.\theta\| + \epsilon^{\text{rot}}_1}}
        \end{aligned}
        \]
    
        \item \textbf{Relative Grasp Reward:} Ensures the gripper tips maintain a consistent grasp on the object. Here, \( p^\text{(t)}_\text{ee-rel} \) represents the relative position of the robot end-effector keypoints. It is computed from the absolute end-effector keypoints \( p^\text{(t)}_\text{ee} \) with respect to the object pose \( q^\text{(t)}_{\text{obj}} \) at timestep \( t \):
        \[
        \begin{aligned}
        r^\text{(t)}_{\text{grasp}} = w^{\text{grasp}}\|p^\text{(t)}_\text{ee-rel}- p^\text{(t-1)}_\text{ee-rel}\| 
        \end{aligned}
        \]
    
    \item \textbf{Success Reward:}
    The reward $r^\text{(t)}_\text{success}$ computation is identical to the success reward used in non-prehensile post-contact policy training. % A success reward, \( r_{\text{succ}} \), is given when the object is manipulated to the target pose \( q^\text{g}_\text{obj} \) within the error threshold \( \delta_\text{obj} \); otherwise, the reward is 0. The same distance function is used to compute the success reward for the non-prehensile skill post-contact policy training.

    % \[
    % r^\text{(t)}_{\text{success}} =
    % \begin{cases} 
    % r_{\text{succ}} & \text{if } d_{SE(3)}(q^\text{(t)}_{\text{obj}}, q^\text{g}_{\text{obj}}) < \delta_\text{obj} \\
    % 0 & \text{otherwise}
    % \end{cases}
    % \]
    
    \end{enumerate}

    The overall reward is defined as:
    \[
    r_{\text{P}} = r^\text{(t)}_{\text{obj}} + r^\text{(t)}_{\text{rot}} + r^\text{(t)}_{\text{grasp}} + r^\text{(t)}_{\text{success}}
    \]

    The hyperparameters of the reward function, $\epsilon_0^{\text{object}}$, $\epsilon_1^{\text{object}}$, $\epsilon_0^{\text{rotation}}$, $\epsilon_1^{\text{rotation}}$, $w^{\text{grasp}}$, $r_{\text{succ}}$, and $\delta_\text{obj}$, vary depending on the specific P skill being trained. The values of these reward hyperparameters for each P skill \( K_\text{P} \) are provided in Table~\ref{table:P_reward}.

\end{itemize}

\input{tables/appendix/P_reward}

We train the P skill policy with Proximal Policy Optimization (PPO) \cite{schulman2017proximal}.

\medskip
% \subsubsection{P SKill Policy Architecture}
P skill policies utilize a multilayer perceptron (MLP) architecture to generate low-level robot actions based on the state and desired object pose information. \( K_{\text{P}}.\pi_{\text{P}} \) employs a five-layer MLP with an input dimension of 123. Other components of the network architecture are identical to the NP skill post-contact policy's network architecture. The hyperparameters of network is summarized in Table \ref{table:P_arch}
 % The MLP has hidden dimensions of $[512, 256, 256, 128]$ and an output dimension of 20. ELU is used as the activation function for the hidden layers, while Identity is applied as the final activation function.
\input{tables/appendix/P_skill_architecture}