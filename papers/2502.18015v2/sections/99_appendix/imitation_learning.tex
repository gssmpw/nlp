
% \input{tables/IL_state}
\input{tables/IL_action}

We train the diffusion policy with a U-Net backbone from the diffusion policy \cite{chi2023diffusion} codebase. To achieve faster inference times, we remove action chunking and state history.

For the state components, as shown in Table \ref{table:IL_state}, we use the previous robot joint position \( q^\text{(t-1)}_r \) instead of the robot joint velocity $\dot{q}^\text{(t)}_r$ due to the large sim-to-real gap in joint velocity. The relative positions of the robot's end-effector keypoints $p^\text{(t)}_\text{ee-rel}$ and gripper tip $p^\text{(t)}_\text{tip-rel}$, with respect to the object pose $q_\text{obj}$, are used to explicitly represent the relationship between the object and the robot. We also use \( a^\text{(t-1)}_\text{width} \) to identify the previous width action in order to provide previous gripper commands for efficient execution. Other state components (\( q^\text{(t)}_r, p^\text{(t)}_\text{ee}, p^\text{(t)}_\text{tip}, p^\text{(t)}_\text{obj}, \) and \( p^\text{g}_\text{obj} \)) correspond to the same components in the state components of the prehensile skill post-contact policy. The state component \( \mathbbm{1}^\text{(t)}_\text{gripper-execute} \) corresponds to the connector policy's state component.

% Additionally, we include a binary indicator to specify whether the gripper is currently controllable, as the gripper of the Franka Research 3 cannot accept new commands until the previous gripper command is completed.

For the action components, as described in Table \ref{table:IL_action}, it consists of four components to perform torque control for non-prehensile manipulation. Their gains and damping values correspond to the same components in the action components of the non-prehensile skill post-contact policy. The distillation policy uses changes in the robot joint positions $\Delta q_\text{joint}$ instead of changes in the end-effector $\Delta q_\text{ee}$, eliminating the need for inverse kinematics (IK) computation and enabling faster inference. Hyperparameters related to training diffusion policies are summarized in Table \ref{table:IL_hyper}. These parameters remain consistent across the card flip, bookshelf, and kitchen domains.