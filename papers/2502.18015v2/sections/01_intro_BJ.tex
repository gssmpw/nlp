% Humans have a remarkable capability to use a long sequence of prehensile and non-prehensile skills to manipulate objects. To flip a thin card on a table and reposition it, we drag the card to the table’s edge to enable grasping, pick it up, flip it, and drag it again to the desired spot. Similarly, taking a book from a crowded bookshelf and placing it in a narrow space involve toppling the book for grasping, moving it to the edge of the target space, and then pushing it into place. These are extremely intricate and complex tasks: they require considering not only the feasibility of the skills, but also their long-term consequences in a way that they \textit{chain}, such as toppling a book to enable a grasp. 

Humans have a remarkable capability to use a long sequence of prehensile and non-prehensile skills to manipulate objects. To take a book from a crowded shelf and place it in a tight space where our entire hand would not fit, we topple the book to enable grasping, move it to the edge of the target space, and then push it into place using just our fingers. Such problems are extremely intricate and complex: they require considering not only the feasibility of the skills, but also their long-term consequences in a way that they chain, such as toppling a book to enable a grasp.

Our goal is to enable robots to solve such \textit{Prehensile-Non-Prehensile (PNP)} problems — tasks that require long sequences of prehensile and non-prehensile skills to move an object to a target pose, like those shown in Figure \ref{fig:CPNP_tasks}. We assume access to a set of independently trained skills, where their training processes do not assume the knowledge of other skills. Each skill consists of a \emph{goal-conditioned policy} that takes as an input a desired object pose and outputs a sequence of joint torques to achieve that pose, and an \emph{applicability checker} which, given a state, checks whether the skill can be applied in that state. We also assume access to a physics engine that can simulate these skills.
%The objective is to find a sequence of joint torques to achieve the ultimate goal object pose.

%One approach to solve PNP problems is to use Task and Motion Planning (TAMP) algorithms that have been shown to be effective for long-horizon problems~\cite{garrett2021integrated}. TAMP combines discrete task planning with continuous motion planning, analogous to selecting a sequence of skills from a discrete set and the associated intermediate object poses in PNP problems. However, the issue with applying TAMP to PNP problems is that TAMP requires preconditions and effects of a skill. These are descriptions of state sets that ensure a skill, when executed in a state satisfying its preconditions, will deterministically lead to a state satisfying its effects. However, in PNP problems, such information is not readily available. While an applicability checker can determine whether a skill can be applied in a given state, it does not ensure the skill's success in moving the object to the desired pose. Further, even when the skill succeeds, the object is not positioned precisely at the desired pose but rather within a specified distance threshold. These uncertainties render the effect states undefined. Another problem is that TAMP algorithms are generally computationally expensive and prone to frequent re-planning when unexpected transitions occur, particularly in contact-rich tasks where modeling contact dynamics is inherently challenging.

One approach to PNP problems is to use Task and Motion Planning (TAMP) algorithms that are effective for long-horizon problems~\cite{garrett2021integrated}. However, the issue with applying TAMP to PNP problems is that TAMP requires 
preconditions and effects of a skill, which are state sets that explicitly define the deterministic transition model of the skill. However, in PNP problems, such information is not readily available. While an applicability checker can determine whether a skill \emph{can} be applied in a given state, it does not ensure the skill's success in moving the object to the desired pose. Further, even when the skill succeeds, we have a stochastic transition where the object is not positioned precisely at the desired pose but rather at a random pose within a specified distance threshold.  Another problem is that TAMP algorithms, as with any planning algorithms, are generally computationally expensive, as they involve exploring different sequences of actions and expensive feasibility checks, such as the existence of a motion plan.


%TAMP combines discrete task planning with continuous motion planning, analogous to selecting a sequence of skills from a discrete set and the associated intermediate object poses in PNP problems. However, the issue with applying TAMP to PNP problems is that TAMP requires preconditions and effects of a skill. These are descriptions of state sets that ensure a skill, when executed in a state satisfying its preconditions, will deterministically lead to a state satisfying its effects. However, in PNP problems, such information is not readily available. While an applicability checker can determine whether a skill can be applied in a given state, it does not ensure the skill's success in moving the object to the desired pose. Further, even when the skill succeeds, the object is not positioned precisely at the desired pose but rather within a specified distance threshold. These uncertainties render the effect states undefined. Another problem is that TAMP algorithms are generally computationally expensive and prone to frequent re-planning when unexpected transitions occur, particularly in contact-rich tasks where modeling contact dynamics is inherently challenging.

Alternatively, learning-based methods, such as reinforcement learning (RL) and imitation learning (IL), offer faster decision-making by leveraging neural network inference to compute actions.  However, RL often faces challenges in long-horizon tasks due to sparse rewards and the credit assignment problem. These difficulties are particularly pronounced in our tasks where we often have to temporarily move away from the goal to ultimately achieve it, such as positioning a card near the edge of a table, away from a goal pose, to facilitate grasping (e.g. Figure~\ref{fig:CPNP_tasks}, top row). IL has shown impressive performance in robotics \cite{chi2023diffusion, fu2024mobile, Reuss-RSS-23, wang2023mimicplay,Zhao-RSS-23}, but is typically limited to acquiring short-horizon skills as it is expensive to collect large-scale long-horizon demonstrations.


%To apply RL, we can formulate the PNP problem as  Parameterized Action Markov Decision Processes (PAMDPs)~\cite{hausknecht2015deep,jain2020generalization, jiang2024hacmanpp, masson2016reinforcement,  nasiriany2022augmenting, wei2018hierarchical, xiong2018parametrized, li2021hyar} where each action is a skill represented by a pair of skill index and continuous parameters, and use RL to train a high-level policy for selecting skills and their parameters. 


Instead, we propose a two-stage framework: (1) generate high-quality, large-scale demonstrations for PNP problems using a novel planner, and (2) use IL to reduce the online computation time. To efficiently generate demonstrations, we propose \texttt{Skill-RRT}, which extends RRT~\cite{lavalle1998rapidly}, a time-tested method for computing a collision-free motion to a goal, to object poses and skills. RRT efficiently explores the search space by randomly sampling a robot configuration, extending from the nearest node in the tree if the configuration is collision-free, and rejecting it otherwise. \texttt{Skill-RRT} has the same structure, but introduces two key differences. First, instead of sampling a robot configuration, it samples object pose and skill from the space of object poses and the discrete set of skills. Second, it extends from the nearest node in the tree only if the sampled skill is both applicable and succeeds when simulated with the sampled object pose as its goal. If these conditions are not met, the sampled pose and skill are rejected.

\newcommand{\skillrrt}{\texttt{Skill-RRT}}
\newcommand{\lazyskillrrt}{\texttt{Lazy Skill-RRT}}

%\skillrrt{} enables efficient planning in the space of skills and intermediate object poses by adopting the exploration strategy of RRT. However, one problem with \skillrrt{} is that the resulting state of one skill might not align with the applicable states of the next skill as skills are not necessarily made to chain \cite{konidaris2009skill}. For instance, in the card domain shown in Figure~\ref{fig:CPNP_tasks}, a sliding skill ends when the robot achieves the given object pose, with its gripper at the top of the card, closed. To pick the object, the robot must transition to a pre-grasp configuration at the side of the card with the gripper opened, as that is the applicable state of the prehensile skill, but this is a motion that belongs neither to the sliding skill nor the prehensile skill. A naive solution is to use a collision-free motion planner to move the gripper, but we found this to be unworkable: transitioning to the next skill’s applicable state often involves breaking and making contacts, and the stiff motions from motion planners tend to disturb the object, causing undesirable events such as card falling off the table.

\skillrrt{} facilitates efficient planning in the space of skills and intermediate object poses by leveraging the exploration strategy of RRT. However, a key challenge with \skillrrt{} is that the resulting state of one skill may not align with the applicable states required by the next skill, as skills are not inherently designed to chain~\cite{konidaris2009skill}. For example, in the card domain, a sliding skill ends when the robot positions the object at the desired pose, with the gripper closed and positioned on top of the card (Figure~\ref{fig:CPNP_tasks}, third column). To transition to a prehensile skill, the robot must move to a pre-grasp configuration at the side of the card with the gripper opened, as this is the applicable state of the prehensile skill (Figure~\ref{fig:CPNP_tasks}, fourth column). However, this motion belongs to neither the sliding nor the prehensile skill. One naive solution is to use a collision-free motion planner to move the gripper, but we found the stiff motions generated by motion planners tend to disturb the object, causing undesirable outcomes such as the card falling off the table.

To address this, we introduce \textit{connectors} -- goal-conditioned policies that, given the current state and goal robot configuration, breaks the contact at the current configuration and remake the contact at a new configuration with the minimum disturbance to the object’s pose. For each skill, we have an associated connector that moves the robot to a state where the skill is applicable, similar to Mason's idea of ``funnels''~\cite{mason1985mechanics}. 

To train connectors, we use RL. However, the challenge is defining the right distribution of problems: if we, say, uniform-randomly generate initial state and goal configurations for connectors, we might end up training connectors on states that are irrelevant to the given PNP problem. If on the other hand, we manually design these problems, then the connector might end up facing out-of-distribution problems.

Instead, we propose \texttt{Lazy Skill-RRT}, which tentatively assumes connectors exist, and teleport the robot to the next skill's applicable state. We use \texttt{Lazy Skill-RRT} to solve PNP problems, while logging the states where we teleported the robot, hence requires a connector. We then train the connector on these logged problems, which allows us to focus our connectors only on states that are likely to be encountered on a solution path.

With the connectors, \texttt{Skill-RRT} can generate complete skill plans for PNP problems. We use \texttt{Skill-RRT} to create solution dataset for diverse initial and goal object poses and distill it to a policy via IL to eliminate expensive online planning. Because the choice of intermediate object poses results in multi-modal trajectories, we use Diffusion Policy \cite{chi2023diffusion}. However, as already observed in the previous paper \cite{dalal2023imitating}, not all data from a planner is useful, as low-quality trajectories may degrade policy performance by leading the robot to a high-risk state. To mitigate this, we filter data by replaying the \texttt{Skill-RRT} plans with noise in simulation, and discard those whose success rate is lower than a threshold. 
%For example, unstable card poses positioned very close to the edge of the table might solve the task but risk dropping the card. 

We generate data and train all policies entirely in simulation, and zero-shot transfer to the real world. In three contact-rich, real-world long-horizon PNP problems (Figure \ref{fig:CPNP_tasks}), our policy achieves more than 80\% success rates across all domains. We also show that the distilled policy outperforms pure planner, \texttt{Skill-RRT}, and \texttt{MAPLE}~\cite{nasiriany2022augmenting}, the state-of-the-art skill-based RL algorithm, in terms of computation time, and success rate.

