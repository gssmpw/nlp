% \textbf{Evaluate Connector in Bridging the Gap Between Effect and Preconditions}
% % purpose of this section
% We examine the effectiveness of the connector in bridging the gap between the effect of one skill and the precondition of the next. To evaluate its performance, we compared our trained connector policies against a motion planner \cite{sucan2012the-open-motion-planning-library}, which generates a sequence of long-horizon robot joint actions to reposition the robot and satisfy the precondition of the subsequent skill.

% % How did we implement connector and motion planner
% We replace the connector policy with the Open Motion Planning Library (OMPL) \cite{sucan2012the-open-motion-planning-library} and collect test problems similarly for connector training. Each problem consists of the effect of the skill, the skill's terminated state \( s_{k-1} \), the subsequent skill \( o_k \), and its subgoal \( (q^\text{obj}_\text{sg})_k \). The target robot configuration, \( q^\text{robot}_\text{pre} \), is determined using the subsequent skill's pre-contact robot configuration sampler \( f^\text{pre}_{k} \), the terminated state's object pose \( s_{k-1}.q^\text{obj} \), and \( (q^\text{obj}_\text{sg})_k \).

% The connector computes low-level actions based on the current state \( s \) and the target robot configuration \( q^\text{robot}_\text{pre} \). For the motion planner, a trajectory of robot joint positions is computed from the current robot joint positions \( s_{k-1}.q^\text{robot} \) to \( q^\text{robot}_\text{pre} \). If the motion planning computation is successful, the robot executes the trajectory by following the sequence of joint positions generated by the motion planner.

% % The measurement, result and analysis
% We evaluate the effectiveness of each method using three metrics: 1) execution time to assess practicability with skill-RRT, 2) the rate of satisfying the subsequent skill's precondition, and 3) the success rate of the subsequent skill in manipulating the object to \( q^\text{obj}_\text{sg} \), as shown in Figure.

% \begin{figure}[ht!] % Adjust to fit within a single column
% \centering
% % \vspace{-10mm}
% \includegraphics[width=\columnwidth]{figures/results/Ablation_subsequent_success_rate.png}
% \caption{Ablation Result:} % Adjust caption text if needed
% \label{fig:ablation_subsequent_success} % Place the label after the caption
% \end{figure}


% % \input{tables/ablation/ablation_connector}
% \textcolor{red}{result and analysis needed}
% % However, motion planners primarily focus on the robot's movement, often neglecting the object's motion during the connector phase.

% textbf{Effect of Replay Success Rate Threshold ($m$)}
\subsection{Impact of our data filtering mechanism}

% what we want to ablate
%High-quality demonstrations are essential for imitation learning (IL) \cite{mandlekar2022matters}, as low-quality data can lead to high-risk states. We collect state-action trajectories for training a distillation policy by replaying skill plans $\tau_{\text{skill}}$ that exceed a replay success rate threshold $m$ during the replay process. We investigate the performance of the distillation policy by training with data collected from state-action trajectories with different replay success rates threshold $m$.

% How to set other baselines
We evaluate the impact of $m$ and the use of the data filtering mechanism. Figure \ref{fig:ablation_filtering_success} shows the result. Baseline Without Replay uses 15,000 skill plans to train a diffusion policy, without filtering, with a trajectory per skill plan. On the other hand, Replay $(m=0.1)$ and Replay $(m=0.9)$ filter out skill plans that do not meet their respective replay success rate threshold $m$. A skill plan's replay success rate is measured by
\[
\frac{\text{Number of successful replays of }\skillplan: N_{success}}{\text{Total number of replays of }\skillplan: N}
\]
as mentioned in Section~\ref{method:IL}. To measure the success rate, we replay each skill plan \( N = 400 \) times and collect the successful trajectories for each skill plan. If a skill plan's \( N_{success} \) exceeds 40, it is included in the Replay with \( m = 0.1 \). If a skill plan's \( N_{success} \) exceeds 360, it is included in the Replay with \( m = 0.9 \). Both methods collect 500 skill plans. From these, we randomly select 30 trajectories from the successful executions for each skill plan. While all successful trajectories can be used, each method collects a total of 15,000 trajectories to ensure a fair comparison when training the distillation policy. Across all domains, we see that $m=0.9$ with the filtering mechanism achieves the highest success rate, by preferring desired object poses that prevents risky states, where object might drop. Detailed characteristics of skill plans under each filtering method, including the distribution of desired object poses, are provided in Appendix~\ref{Appendix:data_qual}.

% Replay $(m=0.1)$ uses data filtering with $(m=0.1)$ on 500 skill plans, and collect 30 successful trajectories per each skill plan. Replay $(m=0.9)$ uses the same setup, but uses a high success rate threshold $(m=0.9)$. All methods collect 15,000 trajectories in total to train the distillation policy, ensuring a fair comparison. Across all domains, we see that $m=0.9$ with the filtering mechanism achieves the highest success rate, by preferring subgoal object poses that prevents risky states, where object might drop. Detailed characteristics of skill plans under each filtering method, including the distribution of desired object poses, are provided in Appendix~\ref{Appendix:data_qual}.

%We evaluate the effectiveness of each method using the distillation policy's success rate in simulation as shown in . %\ref{}.

\begin{figure}[ht!] % Adjust to fit within a single column
\centering
% \vspace{-10mm}
\includegraphics[width=\columnwidth]{figures/results/success_rates_across_environments.png}
\caption{Success rate for each data filtering method.}

%We provide a more comprehensive presented in Appendix~\ref{Appendix:IL_ablation}}%Table \ref{table:ablation_dataQ}.} % Adjust caption text if needed
\label{fig:ablation_filtering_success} % Place the label after the caption
\end{figure}

%Our filtering rule ensures data quality by restricting skill plans to those with a high replay success rate (Replay \(m=0.9\)). This approach consistently outperforms replaying low-success-rate plans (Replay \(m=0.1\)) and collecting data without replay (Without Replay), which includes plans with lower success rates and leads to decreased performance. While collecting 15,000 plans without replay improves generalization (initial and goal object poses generalization), the lack of filtering low-success-rate plans results in inferior performance. High replay success rate plans are characterized by intermediate desired object poses within skills that minimize risky states. For instance, in the card flip domain, NP skill target poses positioned slightly inward from the table's edge facilitate grasping and maintain stability, whereas lower success rate plans place the card too close to the edge, increasing the risk of falling. Detailed characteristics of skill plans under each filtering method, including the distribution of desired object poses, are provided in Appendix~\ref{Appendix:data_qual}.

\subsection{Design choices for IL}

In this section, we study how the design choices of imitation learning (IL) affect the performance of the distillation policy in simulation. We compare the diffusion policy \cite{chi2023diffusion} with the following architectures: 1) ResNet \cite{he2016deep}, a simple IL model with large parameters; 2) LSTM+GMM, which has been shown to be effective for multimodal data in RoboMimic \cite{robomimic2021}; 3) cVAE \cite{kingma2013auto}, another conditional generative model; and 4) Transformer \cite{vaswani2017attention}, a widely used architecture for multimodal data such as language. Each architecture is trained on the same dataset, with the similar model parameter size ($\approx$70M) and the same training duration of 100 epochs, across three different seeds. Detailed training settings such as hyperparameters are described in Appendix~\ref{Appendix:IL_ablation}. 
The success rate is measured at every training epoch on 100 fixed test problems and we report the maximum success rates achieved during the training epochs. The success rates for each architecture are summarized in Table \ref{table:IL_arch_ablation}. 

In the card flip domain, the diffusion policy significantly outperforms the other architectures. We hypothesize that this is because of the high diversity of intermediate target poses in this domainâ€”for instance, an object pose can be at either the left or right edge of the table. In the bookshelf and kitchen domains, ResNet, LSTM+GMM, and Transformer achieve success rates similar to the diffusion policy, which might because the level of intermediate object poses diversity is lower than in the card flip domain. cVAE shows poor performance across all domains, indicating that the smoothing effect of VAE negatively impacts imitation learning.

\input{tables/ablation/ablation_IL_arch}
