\subsection{Experiment Setup}
We evaluate our framework and baselines across three domains. For each domain, we provide a description, including the space of stable and relevant object poses $\Qobj$, a set of skills $\skillset=\{\pskill,K_{\text{NP}_1},\cdots,K_{\text{NP}_n}\}$, initial state $s_0$, and goal object pose $q_{\text{obj}}^g$.

We conduct experiments across three distinct domains: card flip, bookshelf, and kitchen. In the card flip domain, the task involves manipulating a card from its initial pose, \( s_0.q_{\text{obj}} \), to a goal pose, \( q_{\text{obj}}^g \). Since one pose faces the sky and the other faces the ground, the task requires a flipping motion to achieve successful manipulation. In the bookshelf domain, the task involves manipulating a book from its initial pose, \( s_0.q_{\text{obj}} \), to a goal pose, \( q_{\text{obj}}^g \). Here, the initial pose of the book is on the upper shelf, while the goal pose is on the lower shelf. In the kitchen domain, the task involves manipulating a cup from its initial pose, \( s_0.q_{\text{obj}} \), to a goal pose, \( q_{\text{obj}}^g \). The cup starts in the sink, and the goal pose is on a shelf, which is randomly sampled between the left or right shelf.

The object pose space $\Qobj$ for each domain is outlined in Table~\ref{table:exp_setup}. In the card flip domain, there are two regions: $R^\text{up}$, where the object lies on the table with its upper surface facing the sky, and $R^\text{down}$, where the object lies on the table with its upper surface facing the ground. In the bookshelf domain, there are two regions: \(R^\text{uppershelf}\), which corresponds to the upper shelf where the book is fully placed on the bookshelf between other books, and \(R^\text{lowershelf}\), which represents the lower shelf where the book lies on a box. In the kitchen domain, there are three regions: \( R^\text{sink} \), representing the sink where the cup is fully in the book; \( R^\text{rightshelf} \), representing the right shelf where the cup is placed; and \( R^\text{leftshelf} \), representing the left shelf where the cup is also placed. The detailed region descriptions are outlined in Appendix~\ref{Appendix:region}.

The skill library $\mathcal{K}$ for each domain is outlined in Table~\ref{table:exp_setup}. In the card flip domain, the skill library includes \(K_\text{P}\), which moves the card across regions (\(R^\text{up}\) and \(R^\text{down}\)), as well as two non-prehensile (NP) skills: \(K_{\text{NP}_\text{up}}\) and \(K_{\text{NP}_\text{down}}\), which manipulate the card within \(R^\text{up}\) and \(R^\text{down}\), respectively. In the bookshelf domain, the library consists of \(K_\text{P}\), which moves a book between the upper and lower shelves, \(K_{\text{NP}_\text{topple}}\), which topples a book on the upper shelf, and \(K_{\text{NP}_\text{push}}\), which pushes a book inward on the lower shelf. In the kitchen domain, the skill library includes \(K_\text{P}\), which moves a cup from the sink to the shelf, as well as \(K_{\text{NP}_\text{sink}}\), \(K_{\text{NP}_\text{leftshelf}}\), and \(K_{\text{NP}_\text{rightshelf}}\), which manipulate the cup within the sink, left shelf, and right shelf regions, respectively. The detailed skill descriptions and the training methods for the skills are outlined in Appendix~\ref{Appendix:P_Skill} and~\ref{Appendix:NP_Skill}.

% region and skill library  outlined in the table, and briefly explain each one

% \begin{itemize}\label{Exp:setup_Skills}
%     \item card flip
%     \begin{itemize}
%         \item \textbf{Description}: manipulate the card from its initial pose $s_0.q_{\text{obj}}$ to the goal configuration $q_{\text{obj}}^g$.
%         \item Environment region: $\Qobj = R^\text{up} \cup R^\text{down} $. The detailed description of $R$ is outlined in Appendix \ref{env:card_flip_region_descript}.
%         \item Skill library $\mathcal{K}$: $\{K_\text{P}, K_{\text{NP}_{\text{up}}}, K_{\text{NP}_{\text{down}}}\}$. The detailed skill description is outlined in Appendix \ref{env:card_down_skill_descript}
%     \end{itemize}
    
%     \item bookshelf
%     \begin{itemize}
%         \item \textbf{Description}: manipulate the book from its initial pose $s_0.q_{\text{obj}}$ on the upper bookshelf to the goal configuration $q_{\text{obj}}^g$ on the lower bookshelf.
%         \item Environment region: $\Qobj = R^{\text{uppershelf}} \cup R^{\text{lowershelf}}$. The detailed description of $R$ is outlined in Appendix \ref{env:bookshelf_region_descript}.
%         \item Skill library $\mathcal{K}$: $\{K_\text{P}, K_{\text{NP}_{\text{topple}}}, K_{\text{NP}_{\text{push}}}\}$. The detailed skill description is outlined in Appendix \ref{env:bookshelf_skill_descript}.
%     \end{itemize}

%     \item kitchen
%     \begin{itemize}
%         \item \textbf{Description}: manipulate the cup from its initial pose $s_0.q_{\text{obj}}$ in the sink to the goal configuration $q_{\text{obj}}^g$ on the shelf.
%         \item Environment region: $\Qobj = R^{\text{sink}} \cup R^{\text{shelf}}_\text{left} \cup R^{\text{shelf}}_\text{right}$. The detailed description of $R$ is outlined in Appendix \ref{env:kitchen_region_descript}.
%         \item Skill library $\mathcal{K}$: $\{K_\text{P}, K_{\text{NP}_\text{sink}}, K_{\text{NP}_\text{left-shelf}},  K_{\text{NP}_\text{right-shelf}}\}$. The detailed skill description is outlined in Appendix \ref{env:kitchen_skill_descript}.
%     \end{itemize}
% \end{itemize}

\subsection{Baselines}
We compare our method with the following baselines:

\begin{itemize}
    % \item PPO \cite{schulman2017proximal}: A RL method to train the low-level actions (residual robot joint position) based on the current state and the goal object pose information. %  Haecheol A model free on-policy RL method that learns a policy that output the actions (residual robot joint position) based on the current state and the goal object information from a reward.

    \item PPO \cite{schulman2017proximal}: An end-to-end RL policy that outputs joint torque commands with joint gain and damping values directly based on the current state and goal object pose, without utilizing a set of skills $\mathcal{K}$. % , relying solely on an end-to-end reward-based approach.

    \item MAPLE \cite{nasiriany2022augmenting}: A state-of-the-art RL method for PAMDPs. We use sparse rewards, where rewards are provided when the high-level policy selects a feasible skill or successfully completes the task.
        
    \item \texttt{Skill-RRT} (low-level action): It executes a sequence joint torques given by \texttt{Skill-RRT} (Algorithm \ref{algo:skillrrtbackbone}) in open-loop manner.
    
    \item \texttt{Skill-RRT} (skill plan): It executes a skill plan, a sequence of skills with their intermediate goal poses, generated from \texttt{Skill-RRT} (Algorithm \ref{algo:skillrrtbackbone}). It is semi-open loop in that skills execute closed-loop policies but we do not change the skill plan based on states.

    \item Ours: Filtered data with replay success rate threshold $m=0.9$ and $500$ skill plans to train each model for each task.
\end{itemize}
The summary of these baselines are shown in the first two columns of Table~\ref{table:main_exp}. To adapt MAPLE to our PNP problem, we apply several modifications. First, we omit the affordance reward, which in MAPLE guides the high-level policy toward the desired manipulation region. Instead, we integrate the connector and skill, using a applicability checker $\applicabilitychecker$. The connector activates only feasible skills, thereby eliminating the need for an affordance reward. Additionally, we remove the explicit target location parameter, $x_\text{reach}$ used in MAPLE, computing it instead through the pre-contact policy $\pi_\text{pre}$. We also remove atomic primitives, low-level actions used to fill in gaps that cannot be fulfilled by skills, since our connectors are already trained to handle these gaps. Detailed description for training MAPLE can be found in Appendix~\ref{Appendix:baseline}.

For \texttt{Skill-RRT} (low-level action), given an initial state and goal, we first store the low-level actions from \texttt{Skill-RRT}. Then, we re-execute the stored low-level actions starting from the same initial state and evaluate whether the object reaches the goal.  For \skillrrt (skill action), each skill in the skill plan is executed sequentially, moving to the next skill if the current skill satisfies its success condition.

For training our method, we collect 500 skill plans with a replay success rate of \( m = 0.9 \), filtered from 13,900, 2900, and 4400 skill plan samples for the card flip, bookshelf, and kitchen domains, respectively. From these 500 skill plans, 2.9M, 2.8M, and 3.2M state-action pairs are used to train our IL policy for the card flip, bookshelf, and kitchen domains.

% We evaluate the baselines using two metrics: (1) success rate, which is the number of successfully executed trajectories divided by the total number of problems solved, (2) Runtime, which measures the total time the algorithm takes to solve the problem. For planning-based algorithms, runtime includes both the planning and execution phases, while for other methods, it primarily corresponds to the execution phase.
We evaluate the baselines using two metrics: (1) success rate, which is the number of successfully executed trajectories divided by the total number of problems solved, (2) computation time, which measures the total elapsed time for computing the sequence of actions. For planning-based algorithms, computation time includes both the planning and action computation, while for other methods, it primarily corresponds to the action computation in execution phase.

\subsection{Result and Analysis}
\input{tables/baselines_exp}

In this paper, we make the following claims.

\begin{itemize}
    \item Claim 1: We leverage planning to overcome challenges like local minima and inefficient exploration, particularly in tasks where rewards are undefined or dense rewards for each timestep are unavailable.
    % compare open-loop versus closed-loop
    \item Claim 2: Feedback control, which is typically unavailable in planning algorithms, is crucial for contact-rich PNP problems to handle highly uncertain state transitions.
    \item Claim 3: Combining planning (\texttt{Skill-RRT}) with
    imitation learning enhances execution efficiency and robustness by refining plans based on the current state, enabling faster and more successful skill execution.
    % split
\end{itemize}

% To support Claim 1, we compare our framework with MAPLE \cite{nasiriany2022augmenting}. As shown in Table \ref{table:main_exp}, MAPLE achieves a zero success rate in the card and kitchen domains. This outcome is caused by the difficulty of sampling a target object pose that is applicable for the next skill under sparse rewards. For example, in the card domain, the probability of MAPLE outputting the desired card pose at the end of the table through random actions is low, reducing the applicability of the P skill. In contrast, in the bookshelf domain, the P skill becomes applicable once the book is toppled, making skill chaining more easy.
To support Claim 1, we compare our framework with MAPLE \cite{nasiriany2022augmenting}. As shown in Table \ref{table:main_exp}, MAPLE achieves a success rate of zero in both the card flip and kitchen domains. This poor performance is mainly due to the challenge of predicting an object target pose that enables the successful execution of subsequent skills through random exploration. For instance, in the card domain, task success depends on releasing the gripper and moving it to the card without falling the card after completing the prehensile skill. However, the region where the subsequent non-prehensile skill can successfully execute is narrow, making it challenging for RL-based methods, which rely on random exploration, to find a viable solution without dense reward signals to guide the search.

In contrast, our framework achieves success by leveraging more structured exploration strategies, such as Rapidly-exploring Random Trees (RRT), which are better suited for navigating narrow spaces. RRT is capable of exploring the entire space, ensuring a more comprehensive search. This approach systematically explores the environment, leading to higher success rates even in challenging scenarios where RL struggles due to issues with random exploration and credit assignment. Additionally, the bookshelf domain exhibits a broader region in which the subsequent skill could succeed. Specifically, the prehensile skill becomes feasible to execute if the topple skill is successful, enabling a higher success rate with the PAMDP method in this domain.
% bookshelf -> we can generate the high success rate plan 
% It is hard to sample target object pose that is feasible for the next skill with random exploration under sparse reward setting. (except bookshelf) 
% need shield -> bookshelf domain's characteristic

To support claim 2, we compare our distillation policy with \texttt{Skill-RRT} (low-level actions). As shown in Table \ref{table:main_exp}, this approach achieves almost zero success rates across all domains. The poor performance is due to the contact-rich nature of our PNP tasks and simulation stochasticity of GPU-based simulation, which leads to increased dynamic uncertainty. Executing actions in an open-loop manner cannot actively respond to stochastic state transitions because it lacks state feedback, whereas our distillation policy can adapt its action to the current state immediately.

To support Claim 3, we compare our method with \skillrrt (skill action). However, this method still fails to achieve high success rates. This is because there are risky target object poses where the skills do not succeed 100\%, even though these target poses succeed during the planning stage. For example, pushing a card to the very edge of a table might succeed during planning but is likely to result in the card falling off in execution. In contrast, our method (\texttt{Skill-RRT} + IL) filters out skill plans with low replay success rates by replaying them multiple times and trains the distillation policy on trajectories generated from such robust skill plans. As a result, our method demonstrates higher success rates compared to simply executing skill plans.

Another benefit of distillation is that the distillation policy can solve tasks much faster than planning because it requires significantly less online computation time compared to planning. To validate our argument, we compare ''computation time'', defined as the total elapsed time for computing the sequence of actions. Note that only successful episodes are measured, as failure episodes, such as those involving object falling or timeout, result in extremely short or extremely long computation time. The results shown in Table \ref{table:main_exp} demonstrate that our distillation policy significantly reduces the computation time by up to 120 seconds compared to our planning method, \texttt{Skill-RRT}. This is because the distillation policy does not require the computationally expensive planning time needed to find a skill plan, unlike \texttt{Skill-RRT}.

% Action Type: By executing the skill plan, if it fails, we cannot revise the skill with the current setting (as there is no function $\bets$ for termination the skill). Therefore, state-action imitation provides a better approach to address the problem.

% \textcolor{red}{result analysis, not yet}

\subsection{Real World Experiments}
We evaluate the final distillation policy through real-world experiments on card-flipping, book arrangement in a bookshelf, and cup arrangement in kitchen tasks. The policy is deployed in the real world in a zero-shot manner, enabled by training data collected with domain randomization. For each experiment, we use a fixed set of test problems consisting of diverse initial object poses, initial robot configurations, and goal object poses. The shapes of the real environments are identical to those in the simulation, and the objects used in each domain—such as cards, books, and cups—are fixed. The policy achieves a success rate of over 80\% for all domains, as shown in Table \ref{table:real_world_result}. The main failure cases across domains include torque limit violations of the robot hardware caused by impacts between the robot and the object, and object dropping due to unstable object placement poses. Detailed setups for the real-world experiments are described in Appendix~\ref{Appendix:real_exp}.

\input{tables/real_world_result}